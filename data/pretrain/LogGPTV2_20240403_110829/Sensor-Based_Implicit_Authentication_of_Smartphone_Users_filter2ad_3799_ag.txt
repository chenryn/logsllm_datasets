(cid:82)
(cid:70)
(cid:54)
(cid:3)
(cid:72)
(cid:70)
(cid:81)
(cid:72)
(cid:71)
(cid:76)
(cid:73)
(cid:81)
(cid:82)
(cid:38)
(cid:20)(cid:17)(cid:25)
(cid:20)(cid:17)(cid:23)
(cid:20)(cid:17)(cid:21)
(cid:20)
(cid:19)(cid:17)(cid:27)
(cid:19)(cid:17)(cid:25)
(cid:19)(cid:17)(cid:23)
(cid:19)(cid:17)(cid:21)
(cid:19)
(cid:19)
(cid:21)
(cid:23)
(cid:25)
(cid:27)
(cid:20)(cid:19)
(cid:20)(cid:21)
(cid:55)(cid:76)(cid:80)(cid:72)(cid:3)(cid:11)(cid:71)(cid:68)(cid:92)(cid:86)(cid:12)
Fig. 7. The conﬁdence score of a user with time. After around one week,
the conﬁdence score decreases below the threshold CS = 0.2 for a period
of time. After automatic retraining, it increases back to normal values.
H. Smartphone Overhead
We now evaluate the system overhead of SmarterYou
on smartphones. Speciﬁcally, we analyze the computational
complexity of our system, CPU and memory overhead, and
the battery consumption it incurs on the smartphone.
1) Computational Complexity: The computational com-
plexity of KRR in Section V-F2 is directly related to the
data size according to Eq. 6. Here, we further show that the
computational complexity can be largely reduced to be directly
related to the feature size. (For readability, we put the detailed
proof in the Appendix).
According to Eq. 6, the classiﬁer is w∗
−1y.
Deﬁne S = ΦΦT (Φ = [(cid:2)φ(x1), (cid:2)φ(x2),··· , (cid:2)φ(xN )]).
By utilizing the matrix transformation method in [45], the
optimal solution w∗ in Eq. 6 is equivalent to
= Φ[K + ρIN ]
w∗
−1
Φy
= [S + ρIJ ]
(7)
The dominant computational complexity for w∗ comes from
taking the inversion of a matrix. Therefore, based on Eq. 6
and Eq. 7, the computational complexity is approximately
min(O(N 2.373), O(J 2.373)). If we utilize the identity kernel,
the computational complexity can be reduced from O(N 2.373)
to O(M 2.373) and is independent of the data size. Speciﬁcally,
we construct 28-dimensional feature vectors (4 time-domain
features and 3 frequency-domain features for each of two
sensors, for each device).
Thus, our time complexity is reduced from O((800 ×
9/10)2.373) = O(7202.373) to only O(282.373). In our ex-
periments, the average training time is 0.065 seconds and
the average testing time is 18 milliseconds, which shows the
effectiveness of our system applied in real-world scenarios.
2) CPU and Memory Overhead: The testing module of
SmarterYou in a smartphone runs as threads inside the smartp-
hone system process. We develop an application to monitor the
average CPU and memory utilization of the phone and watch
318
TABLE VIII.
THE POWER CONSUMPTION UNDER FOUR DIFFERENT
SCENARIOS.
Scenario
(1) Phone locked, SmarterYou off
(2) Phone locked, SmarterYou on
(3) Phone unlocked, SmarterYou off
(4) Phone unlocked, SmarterYou on
Power Consumption
2.8%
4.9%
5.2%
7.6%
while running the SmarterYou app which continuously requests
sensor data at a rate of 50 Hz on a Nexus 5 smartphone and a
Moto 360 smartwatch. The CPU utilization is 5% on average
and never exceeds 6%. The CPU utilization (and hence energy
consumption) will scale with the sampling rate. The memory
utilization is 3 MB on average. Thus, we believe that the
overhead of SmarterYou is small enough to have negligible
effect on overall smartphone performance.
3) Battery Consumption: To measure the battery consump-
tion, we consider the following four testing scenarios: (1)
Phone is locked (i.e., not being used) and SmarterYou is off.
(2) Phone is locked and SmarterYou keeps running. (3) Phone
is under use and SmarterYou is off. (4) Phone is under use
and SmarterYou is running. For scenarios (1) and (2), the test
time is 12 hours each. We charge the smartphone battery to
100% and check the battery level after 12 hours. The average
difference of the battery charged level from 100% is reported
in Table VIII.
For scenarios (3) and (4), the phone under use means that
the user keeps using the phone periodically. During the using
time, the user keeps typing notes. The period of using and
non-using is ﬁve minutes each, and the test time in total is 60
minutes.
Table VIII shows the result of our battery consumption
tests,
in terms of extra battery drain for SmarterYou. We
ﬁnd that in scenarios (1) and (2), the SmarterYou-on mode
consumes 2.1% more battery power than the SmarterYou-off
mode. We believe the extra cost in battery consumption caused
by SmaterYou will not affect user experience in daily use. For
scenarios (3) and (4), SmarterYou consumes 2.4% more battery
power in one hour, which is also an acceptable cost for daily
usage.
I. Retraining Authentication Models
The behavioral drift of the legitimate user must be consi-
dered. The user may change his/her behavioral pattern over
weeks or months, which may cause more false alarms in
implicit authentication. SmarterYou, therefore, will retrain the
authentication models automatically and continuously based
on the previous authentication performance. Here, we deﬁne
w∗ for the k-th
the conﬁdence score (CS) as CS(k) = xT
k
authentication feature vector xT
k as the distance between xT
and the corresponding authentication classiﬁer w∗.
As the authentication classiﬁer w∗ represents the clas-
siﬁcation boundary to distinguish the legitimate user and
the adversaries, a lower conﬁdence score (smaller distance
k and w∗) represents a less conﬁdent authentication
between xT
result (shown conceptually in the left ﬁgure of Figure 7). This
suggests a change of user’s behavioral pattern where retraining
should be taken. For an authenticated user, we suggest that
if the conﬁdence score is lower than a certain threshold CS
k
for a period of time T , the system automatically retrains the
authentication models.
In Figure 7 (right ﬁgure), we show the conﬁdence score
of the time-series authentication feature vectors for a user.
We can see that the conﬁdence score decreases slowly in
the ﬁrst week. At the end of the ﬁrst week, the conﬁdence
score experiences a period of low values (lower than our
threshold CS = 0.2 for a period), indicating that the user’s
behavior changes to some extent during this week. Therefore,
it would be helpful if the system can automatically retrain the
authentication models. Note that there are some earlier points
lower than the threshold (0.2), but they do not occur for a long
enough period to trigger the retraining. Also, it is hard for the
attacker to trigger the retraining because the probability that
the attacker continuously passes the authentication for a long
period of time is low as described in Section V-G.
As our system recognizes user’s behavior drift by checking
the conﬁdence score, it would then go back to the training
module again and upload the legitimate user’s authentication
feature vectors to the training module until the new behavior
(authentication model) is learned. Advanced approaches in
machine unlearning [46] can be explored to update the au-
thentication models asymptotically faster than retraining from
scratch. After retraining the user’s authentication models, we
can see that the conﬁdence score increases to normal values
from Day 8.
As discussed earlier, an attacker who has taken over a
legitimate user’s smartphone must not be allowed to retrain the
authentication model. Fortunately, the attacker can not trigger
the retraining since the conﬁdence score should be positive and
last for a period of time. However, the attacker is likely to have
negative conﬁdence scores, which cannot last for sufﬁcient
time to trigger retraining, since he will be detected in less
than 18 seconds by SmarterYou, according to Figure 6.
VI. CONCLUSIONS
We have proposed a new re-authentication system, Smar-
terYou,
to improve the security of a smartphone, and of
secret and sensitive data and code in the smartphone or in
the cloud accessible through a smartphone. SmarterYou is
an authentication system using multiple sensors built
into
a user’s smartphone, supplemented by auxiliary information
from a wearable device, e.g., smartwatch, with the same owner
as the smartphone. Our system keeps monitoring the users’
sensor data and continuously authenticates without any human
cooperation. We ﬁrst collect context features from the sensors’
data in the smartphone (and the smartwatch if present) to detect
the context of the current user. Based on the detected context
and the authentication features in both the time and frequency
domains, our system implements ﬁner-grained authentication
efﬁciently and stealthily.
We systematically evaluate design alternatives for each
design parameter of such a sensor-based implicit authenti-
cation system. Based on our design choices, our evaluations
demonstrate the advantage of combining the smartphone and
the smartwatch and the enhancement in authentication accu-
racy with context detection and time-frequency information.
SmarterYou can achieve authentication accuracy up to 98.1%
(FRR 0.9% and FAR 2.8%) with negligible system overhead
319
and less than 2.4% additional battery consumption. We believe
this is the highest accuracy and lowest FAR reported by any
sensor-based authentication method to date. We hope that the
SmarterYou system and design techniques can help advance
the ﬁeld in implicit user authentication and re-authentication,
for deployment in real-world scenarios.
REFERENCES
[1] Y. Kim, T. Oh, and J. Kim, “Analyzing user awareness of privacy data
leak in mobile applications,” Mobile Information Systems, 2015.
J. Achara, C. Castelluccia, J.-D. Lefruit, V. Roca, F. Baudot, and
G. Delcroix, “Mobilitics: Analyzing privacy leaks in smartphones,”
ERCIM Newsletter, 2013.
[2]
[3] M. Qi, Y. Lu, J. Li, X. Li, and J. Kong, “User-speciﬁc iris authentication
based on feature selection,” in CSSE, 2008.
[4] K. Xi, J. Hu, and F. Han, “Mobile device access control: an improved
correlation based face authentication scheme and its java me applica-
tion,” Concurrency and Computation: Practice and Experience, 2012.
[5] K. Niinuma, U. Park, and A. K. Jain, “Soft biometric traits for
continuous user authentication,” IEEE TIFS, 2010.
[6] ConsumerReports, “Keep your phone safe: How to protect yourself from
wireless threats,” Consumer Reports, Tech., 2013.
[7] A. De Luca, A. Hang, F. Brudy, C. Lindner, and H. Hussmann, “Touch
me once and i know it’s you!: implicit authentication based on touch
screen patterns,” in ACM CHI, 2012.
[8] N. L. Clarke and S. M. Furnell, “Authenticating mobile phone users
using keystroke analysis,” International Journal of Information Security,
vol. 6, no. 1, pp. 1–14, 2007.
[9] S. Buthpitiya, Y. Zhang, A. K. Dey, and M. Griss, “n-gram geo-trace
modeling,” in Pervasive Computing, 2011.
[11]
[10] O. Riva, C. Qin, K. Strauss, and D. Lymberopoulos, “Progressive
authentication: Deciding when to authenticate on mobile phones.” in
USENIX Security, 2012.
J. Zhu, P. Wu, X. Wang, and J. Zhang, “Sensec: Mobile security through
passive sensing,” in ICNC, 2013.
J. M¨antyj¨arvi, M. Lindholm, E. Vildjiounaite, S.-M. M¨akel¨a, and
H. Ailisto, “Identifying users of portable devices from gait pattern with
accelerometers,” in ICASSP, 2005.
[12]
[13] Z. Xu, K. Bai, and S. Zhu, “Taplogger: Inferring user inputs on
smartphone touchscreens using on-board motion sensors,” in conference
on Security and Privacy in Wireless and Mobile Networks, 2012.
[14] A. J. Aviv, K. L. Gibson, E. Mossop, M. Blaze, and J. M. Smith,
“Smudge attacks on smartphone touch screens.” Woot, 2010.
[15] M. Conti, I. Zachia-Zlatea, and B. Crispo, “Mind how you answer me!:
transparently authenticating the user of a smartphone when answering
or placing a call,” in CCS, 2011.
[16] C. Nickel, T. Wirtl, and C. Busch, “Authentication of smartphone users
based on the way they walk using k-nn algorithm,” in IIH-MSP, 2012.
[17] M. Trojahn and F. Ortmeier, “Toward mobile authentication with
keystroke dynamics on mobile phones and tablets,” in WAINA, 2013.
[18] F. Okumura, A. Kubota, Y. Hatori, K. Matsuo, M. Hashimoto, and
A. Koike, “A study on biometric authentication based on arm sweep
action with acceleration sensor,” in ISPACS, 2006.
[19] M. Frank, R. Biedert, E.-D. Ma, I. Martinovic, and D. Song, “Touchaly-
tics: On the applicability of touchscreen input as a behavioral biometric
for continuous authentication,” IEEE TIFS, 2013.
[20] L. Li, X. Zhao, and G. Xue, “Unobservable re-authentication for
smartphones,” in NDSS, 2013.
[21] T. Feng, Z. Liu, K.-A. Kwon, W. Shi, B. Carbunar, Y. Jiang, and
N. K. Nguyen, “Continuous mobile authentication using touchscreen
gestures,” in Homeland Security, Conference on Technologies for, 2012.
[22] H. Xu, Y. Zhou, and M. R. Lyu, “Towards continuous and passive
authentication via touch biometrics: An experimental study on smartp-
hones,” in Symposium On Usable Privacy and Security, 2014.
[23] N. Zheng, K. Bai, H. Huang, and H. Wang, “You are how you touch:
User veriﬁcation on smartphones via tapping behaviors,” in Network
Protocols, International Conference on, 2014.
[24] H. G. Kayacık, M. Just, L. Baillie, D. Aspinall, and N. Micallef, “Data
driven authentication: On the effectiveness of user behaviour modelling
with mobile device sensors,” Mobile Security Technologies, 2014.
[25] W.-H. Lee and R. B. Lee, “Multi-sensor authentication to improve
smartphone security,” in ICISSP, 2015.
[26] L. Yang, Y. Guo, X. Ding, J. Han, Y. Liu, C. Wang, and C. Hu,
“Unlocking smart phone through handwaving biometrics,” IEEE Tran-
sactions on Mobile Computing, 2015.
[27] L. Hong and A. Jain, “Integrating faces and ﬁngerprints for personal
identiﬁcation,” IEEE TPAMI, 1998.
[28] A. Serwadda and V. V. Phoha, “When kids’ toys breach mobile phone
security,” in CCS, 2013.
[29] S. Mare, A. M. Markham, C. Cornelius, R. Peterson, and D. Kotz,
“Zebra: Zero-effort bilateral recurring authentication,” in SP, 2014.
J. A. Suykens, T. Van Gestel, J. De Brabanter, B. De Moor, J. Van-
dewalle, J. Suykens, and T. Van Gestel, Least squares support vector
machines. World Scientiﬁc, 2002.
[30]
[31] W.-H. Lee, X. Liu, Y. Shen, H. Jin, and R. Lee, “Secure pick up: Implicit
authentication when you start using the smartphone,” in Symposium on
Access Control Models and Technologies, 2017.
[32] T. Y.-H. Chen, A. Sivaraman, S. Das, L. Ravindranath, and H. Bala-
krishnan, “Designing a context-sensitive context detection service for
mobile devices,” 2015.
[33] N. Kern, B. Schiele, and A. Schmidt, “Multi-sensor activity context
detection for wearable computing,” in Ambient Intelligence. Springer,
2003.
[34] A. ARM, “Security technology-building a secure system using trustzone
technology,” ARM Technical White Paper, 2009.
[35] F. McKeen, I. Alexandrovich, A. Berenzon, C. V. Rozas, H. Shaﬁ,
V. Shanbhogue, and U. R. Savagaonkar, “Innovative instructions and
software model for isolated execution,” in International Workshop on
Hardware and Architectural Support for Security and Privacy, 2013.
[36] P. Wu, J. Zhu, and J. Y. Zhang, “Mobisens: A versatile mobile
sensing platform for real-world applications,” Mobile Networks and
Applications, 2013.
[37] R. O. Duda, P. E. Hart, and D. G. Stork, Pattern classiﬁcation.
Wiley & Sons, 2012.
John
[38] Google, “Android sensor manager,” http://developer.android.com/guide/
topics/sensors/sensors overview.html.
[39] B. Boashash, Time frequency analysis. GPP, 2003.
[40] W. W. Daniel et al., “Applied nonparametric statistics,” 1990.
[41] T. K. Ho, “Random decision forests,” in ICDAR.
[42] S. An, W. Liu, and S. Venkatesh, “Face recognition using kernel ridge
IEEE, 1995.
regression,” in CVPR, 2007.
[43] W.-H. Lee and R. Lee, “Implicit authentication for smartphone security,”
in Information Systems Security and Privacy. Springer, 2015.
[44] ——, “Implicit sensor-based authentication of smartphone users with
smartwatch,” in HASP 2016, 2016.
[45] R. A. Horn and C. R. Johnson, Matrix analysis. Cambridge university
press, 2012.
[46] Y. Cao and J. Yang, “Towards making systems forget with machine
unlearning,” in Security and Privacy, 2015.
VII. APPENDIX
A. Proof of Equivalence between Eq. 6 and Eq. 7
Eq. 6 is w∗ = Φ[K + ρIN ]−1y, and Eq. 7 is w∗ = [S + ρIJ ]−1Φy.
In order to prove that they are equivalent, we ﬁrst prove P BT (BP BT +
R)−1 = (P −1 + BT R−1B)−1BT R−1 as follows:
BT + BT R−1BP BT = BT R−1BP BT + BT
⇔ (P −1 + BT R−1B)P BT = BT R−1(BP BT + R)
⇔ P BT (BP BT + R)−1 = (P −1 + BT R−1B)−1BT R−1
(8)
Then we let P = ρIJ , B = ΦT and R = ρIN in Eq. 8, we observe
the left hand side of Eq. 8 is Eq. 6 and the right hand side of Eq. 8 is Eq. 7.
Thus, we prove the equivalence between Eq. 6 and Eq. 7.
320