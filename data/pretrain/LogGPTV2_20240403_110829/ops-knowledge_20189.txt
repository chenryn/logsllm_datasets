I have developed a mapper program in Python for the Hadoop Map-Reduce framework. I execute it using the following command:

```bash
hadoop jar /usr/hdp/2.3.2.0-2950/hadoop-mapreduce/hadoop-streaming.jar -mapper "python wordcount_mapper.py" -file wordcount_mapper.py -input inputfile -output outputfile3
```

The command works correctly if the `inputfile` directory contains only files. However, it fails and displays an error when the `inputfile` directory includes subdirectories. For example, I have two subdirectories (`KAKA` and `KAKU`) within the `inputfile` directory, and the error message is:

```
16/07/20 17:01:40 ERROR streaming.StreamJob: Error Launching job : Not a file: hdfs://secondary/user/team/inputfile/kaka
```

### Question:
What is the correct command to process files in subdirectories?

### Answer:
To include files from subdirectories, you can use regular expressions in the `-input` parameter. Here are the patterns you can use:

- `inputfile/*` will work for one level of subdirectories.
- `inputfile/*/*` will work for two levels of subdirectories.

For your case, where you have one level of subdirectories, you should run the command as follows:

```bash
hadoop jar /usr/hdp/2.3.2.0-2950/hadoop-mapreduce/hadoop-streaming.jar -mapper "python wordcount_mapper.py" -file wordcount_mapper.py -input inputfile/* -output outputfile3
```

If you have multiple levels of subdirectories, you can adjust the pattern accordingly. For example, for two levels of subdirectories, you would use:

```bash
hadoop jar /usr/hdp/2.3.2.0-2950/hadoop-mapreduce/hadoop-streaming.jar -mapper "python wordcount_mapper.py" -file wordcount_mapper.py -input inputfile/*/* -output outputfile3
```

This approach ensures that all files within the specified subdirectory levels are included in the Hadoop job.