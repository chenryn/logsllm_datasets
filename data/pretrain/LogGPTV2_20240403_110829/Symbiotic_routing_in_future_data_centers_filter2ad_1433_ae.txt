in Gbps and the CPU utilization at server S. The aggregate
throughput excludes the Ethernet headers, but includes the
CamCube service header. We measure the CPU utilization
by using Windows performance counters which provide, per
core, an estimate of the CPU utilization as a percentage.
As we have quad core processors, with hyper-threading, we
obtain eight CPU readings. We then calculate the average
CPU utilization across all eight and use that as the utiliza-
tion per server. We observe that the CPU utilization is not
skewed across the cores.
The aggregate throughput achieved in the baseline case
was 11.88 Gbps, and given that each server is using six
1 Gbps links there is a theoretical upper bound of 12 Gbps.
When using the extended routing service we observe less
than a 0.9% loss in maximum throughput between based
line and when |K| = 3. Figure 6 shows the CPU utilization,
for the base line as we vary |K|. These results show that
in the baseline case the CPU utilization is 22.01%, and this
rises to 25.35% when |K| = 3. These results demonstrate
the low overhead that the extended routing service induces.
7. USING THE ROUTING SERVICE
When evaluating the extended routing service it is hard to
make strong claims about its expressiveness. However, we
have successfully re-implemented all our routing protocols
in the extended routing service and achieved performance
comparable to the original versions. In this section we brieﬂy
discuss the F functions for all the services described in this
paper, and provide a detailed example of the F function
used in the VM distribution service, which represents the
most complex routing protocol used.
We provide a number of support functions:
 0 5 10 15 20 25 30Baseline1 key2 keys3 keysCPU Utilization (%)60if CurrKeyaxis (cid:54)= Rootaxis then
l ← log2 E − 1
while distAxis(CurrKey, Root, axis) mod 2l (cid:54)= 0 do
1: min l ← ∞
2: for all axis ∈ {x, y, z} do
3:
4:
5:
6:
7:
8:
9:
10: upN ← getCoordN eigh(CurrKey, min axis, 1)
11: downN ← getCoordN eigh(CurrKey, min axis, −1)
12: if dist(upN, Root)  0.
The ﬁrst stage is for F to determine the level l of CurrKey
in the mini-cube hierarchy (lines 2–9). Intuitively, the size
S of the edge of the largest mini-cube that has a vertex in
CurrKey is equal to 2l. For example, if CurrKey is at the
bottom of the hierarchy, i.e., l = 0, we have that S = 1.
Dually, if CurrKey is a vertex of one of the eight top-level
2 = 2k−1. The value of S can be com-
mini-cubes, then S = E
puted by observing that on each axis the distance between
CurrKey and the root (obtained through distAxis) must be
an exact multiple of S. Therefore, F iterates over each axis
and recursively halves the length of the mini-cube edge un-
til it exactly divides the distance from CurrKey to the root
key coordinate along that axis (lines 5–6). This indicates the
level in the hierarchy for that axis. The lowest level across
all axes represents the level of CurrKey in the hierarchy. At
line 10, F has successfully identiﬁed the level of CurrKey in
the hierarchy and the axis with the lowest level in the hier-
archy. This is contained in the variable min axis. F then
greedily selects the neighbor in the key space that is closer
to the root and returns it as next intermediate destination
(lines 10–15).
This function is used to perform tree construction and dur-
ing the multicast phase the simpler function in Figure 7(b)
is used. This simply forwards a copy of received packets to
the children.
8. RELATED WORK
Networking infrastructure in data centers, including re-
cent research proposals [2, 16, 25],
is inﬂuenced by tra-
ditional Internet architectures and standards. Packets are
routed through switches or routers and end-hosts have lit-
tle control on routing decisions. A single routing protocol
is provided to applications. Simple policies, like having an
application’s traﬃc go through a middlebox, are hard to
support [21]. In general, the IP protocol allows end-systems
to inﬂuence the path by using loose source routing. This,
however, is not widely supported due to the overhead of
maintaining network topology state at the end-systems and
the packet overhead of carrying the information. Propos-
als to address these issues, for example [23, 30, 31], provide
coarse-grained control to the end-hosts or edge ISPs. Our
approach can be seen as a similar to loose source routing,
with routes speciﬁed in terms of keys. However, the path
keys are computed dynamically rather than statically en-
coded in the packet. More generally, multi-service switched
networks [26] support multiple routing protocols on a single
physical network. In this approach, referred to as ships in
the night, routing protocols use separate control planes and
no state is shared among them. The work described here
aims to allow the routing protocols to share information, in
order to minimize control traﬃc.
Current data centers can exploit multi-path using network-
level conﬁguration, for example using equal cost multi-path
protocol (ECMP) to balance ﬂows across multiple paths.
They operate at a ﬂow level to avoid packet reordering,
which would impact TCP. Centralized ﬂow schedulers have
been proposed to allow more eﬃcient network utilization [3]
and to enable energy savings by switching oﬀ redundant
links [19]. These schedulers are traﬃc agnostic. The work
proposed here allows services to individually specify and con-
trol the routing protocol that they use. Direct-connect [7]
and hybrid topologies [18, 17], where servers participate
in packet forwarding, have recently been proposed, as de-
scribed in Section 2. All provide a default routing protocol,
and in the case of BCube and DCell this runs transparently
beneath the TCP/IP stack. Hence, applications running on
them have limited ability to exploit the fact that packets are
routed through servers. CamCube, and the ﬂexibility that
it provides by having a low-level API that explicitly exposes
the packets to services as they are routed through servers
enables services running on CamCube to exploit this.
It
would be feasible to use some of the ideas of enabling mul-
tiple routing protocols on these other platforms.
Direct-connect topologies have been widely used in the
context of High Performance Computing (HPC) [11], e.g.
the IBM BlueGene and Cray XT3/Red Storm use a 3D
torus. They use variants of greedy geographic routing pro-
tocols [29, 1], and normally only tolerate small-scale failures,
for example, the IBM BlueGene/L tolerates only three server
failures [1]. It has been proposed to increase fault-tolerance
by running multiple routing protocols [27], a primary and a
secondary, where the secondary routing protocol is used only
when the primary cannot make progress due to server fail-
ures. HPC systems often use MPI [24] as the communication
API, which is process rather than server based. The com-
piler and job scheduler control the mapping between pro-
cesses and servers. MPI utilizes the multi-path and multi-
hop routing oﬀered by the platforms but, therefore, cannot
exploit on-path interception and modiﬁcation of packets.
The IBM Kittyhawk Project [5] evaluates using the IBM
BlueGene/P for data center workloads. They provide a ﬂat
layer 2 IP network running over the 3D torus network, hid-
ing the diﬀerent topology. In contrast, CamCube explicitly
61exposes the underlying topology to the services, which then
exploit it.
[10] J. Dean and S. Ghemawat. MapReduce: Simpliﬁed
Data Processing on Large Clusters. In OSDI, 2004.
9. CONCLUSIONS
We have explored the beneﬁts and feasibility of using mul-
tiple service-speciﬁc routing protocols in a data center of the
future. We have done this in the context of the CamCube
architecture, which uses a 3D torus topology, where each
server directly connects to six other servers. We are target-
ing shipping container-sized data centers, and do not require
the use of any switches or dedicated networking within the
container. We are currently utilizing a low-level link orien-
tated API in CamCube, which provides the ﬂexibility for
services to implement their own routing protocols.
We have demonstrated that the individual services can ob-
tain better application-level performance by utilizing their
own protocols, and that at the network level this can also
provide better performance. For all services the network
load is reduced when the service uses its own optimized pro-
tocol. This led us to extend our routing service to allow
services running on CamCube to easily implement their own
routing protocols.
10. ACKNOWLEDGMENTS
We thank the reviewers, and in particular our shepherd, Jen-
nifer Rexford, who provided valuable feedback and advice.
Hussam was supported in part by AFOSR grant FA9550-06-
1-0019, AFRL grants FA8750-09-1-0003, FA8750-08-2-0153,
FA8750-09-1-0209, and NSF grants 0424422 and 0828923.
11. REFERENCES
[1] N. R. Adiga, M. A. Blumrich, D. Chen, P. Coteus,
A. Gara, M. E. Giampapa, P. Heidelberger, S. Singh,
B. D. Steinmacher-Burow, T. Takken, M. Tsao, and
P. Vranas. Blue gene/l torus interconnection network.
IBM Journal of Research and Development, 49(2),
2005.
[2] M. Al-Fares, A. Loukissas, and A. Vahdat. A scalable,
commodity data center network architecture. In
SIGCOMM, 2008.
[3] M. Al-Fares, S. Radhakrishnan, B. Raghavan,
N. Huang, and A. Vahdat. Hedera: Dynamic Flow
Scheduling for Data Center Networks. In NSDI, 2010.
[4] Apache Hadoop. http://hadoop.apache.org.
[5] J. Appavoo, V. Uhlig, and A. Waterland. Project
Kittyhawk: building a global-scale computer: Blue
Gene/P as a generic computing platform. SIGOPS
Operating Systems Review, 42(1), 2008.
[6] F. Chang, J. Dean, S. Ghemawat, W. C. Hsieh, D. A.
Wallach, M. Burrows, T. Chandra, A. Fikes, and R. E.
Gruber. Bigtable: A Distributed Storage System for
Structured Data. In OSDI, 2006.
[7] P. Costa, A. Donnelly, G. O’Shea, and A. Rowstron.
CamCube: A Key-based Data Center. Technical
Report MSR TR-2010-74, Microsoft Research, 2010.
[8] F. Dabek, B. Zhao, P. Druschel, J. Kubiatowicz, and
I. Stoica. Towards a common API for structured P2P
overlays. In IPTPS, Feb 2003.
[9] F. Dabek, B. Zhao, P. Druschel, J. Kubiatowicz, and
I. Stoica. Towards a common api for structured
peer-to-peer overlays. In IPTPS, 2003.
[11] J. Duato, S. Yalamanchili, and L. Ni. Interconnection
Networks: An Engineering Approach. Elsevier, 2003.
[12] S. Durocher, D. Kirkpatrick, and L. Narayanan. On
routing with guaranteed delivery in three-dimensional
ad hoc wireless networks. In ICDN, 2008.
[13] B. Fitzpatrick. Distributed caching with memcached.
Linux Journal, 2004(124), 2004.
[14] S. Ghemawat, H. Gobioﬀ, and S.-T. Leung. The
Google File System. In SOSP, 2003.
[15] D. Giuseppe, H. Deniz, J. Madan, K. Gunavardhan,
L. Avinash, P. Alex, S. Swaminathan, V. Peter, and
V. Werner. Dynamo: Amazon’s Highly Available
Key-value Store. In SOSP, 2007.
[16] A. Greenberg, J. R. Hamilton, N. Jain, S. Kandula,
C. Kim, P. Lahiri, D. A. Maltz, P. Patel, and
S. Sengupta. VL2: A Scalable and Flexible Data
Center Network. In SIGCOMM, 2009.
[17] C. Guo, G. Lu, D. Li, H. Wu, X. Zhang, Y. Shi,
C. Tian, Y. Zhang, and S. Lu. BCube: A High
Performance, Server-centric Network Architecture for
Modular Data Centers. In SIGCOMM, 2009.
[18] C. Guo, H. Wu, K. Tan, L. Shiy, Y. Zhang, and
S. Luz. DCell: A Scalable and Fault-Tolerant Network
Structure for Data Centers. In SIGCOMM, 2008.
[19] B. Heller, S. Seetharaman, P. Mahadevan,
Y. Yiakoumis, P. Sharma, S. Banerjee, and
N. McKeown. ElasticTree: Saving Energy in Data
Center Networks. In NSDI, 2010.
[20] M. Isard, M. Budiu, Y. Yu, A. Birrell, and D. Fetterly.
Dryad: distributed data-parallel programs from
sequential building blocks. In EuroSys, 2007.
[21] D. A. Joseph, A. Tavakoli, and I. Stoica. A
policy-aware switching layer for data centers. In
SIGCOMM, 2008.
[22] B. Karp and H. Kung. GPSR: Greedy Perimeter
Stateless Routing for Wireless Networks. In MobiCom,
2000.
[23] M. Motiwala, M. Elmore, N. Feamster, and
S. Vempala. Path splicing. In SIGCOMM, 2008.
[24] MPI Forum. http://www.mpi-forum.org.
[25] R. Niranjan Mysore, A. Pamboris, N. Farrington,
N. Huang, P. Miri, S. Radhakrishnan, V. Subramanya,
and A. Vahdat. Portland: a scalable fault-tolerant
layer 2 data center network fabric. In SIGCOMM,
2009.
[26] C. Pignataro, R. Kazemi, and B. Dry. Cisco
Multiservice Switching Networks. Cisco Press, 2002.
[27] V. Puente and J. A. Gregorio. Immucube: Scalable
Fault-Tolerant Routing for k-ary n-cube Networks.
IEEE Trans. Parallel Distrib. Syst., 18(6), 2007.
[28] S. Ratnasamy, P. Francis, M. Handley, R. Karp, and
S. Shenker. A Scalable Content-addressable Network.
In SIGCOMM, 2001.
[29] S. L. Scott and G. Thorson. Optimized Routing in the
Cray T3D. In PCRCW, 1994.
[30] W. Xu and J. Rexford. Miro: multi-path interdomain
routing. In SIGCOMM, 2006.
[31] X. Yang and D. Wetherall. Source selectable path
diversity via routing deﬂections. In SIGCOMM, 2006.
62