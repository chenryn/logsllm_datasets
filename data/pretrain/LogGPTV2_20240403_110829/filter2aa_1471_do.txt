Extra Create Parameters (also known as ECPs), used to fine-tune the
behavior of the opening/creation process of a target file in the minifilter
context. In general, the Filter Manager supports different ECPs, and each of
them is uniquely identified by a GUID. The Filter Manager provides multiple
documented APIs that deal with ECPs and ECP lists. Usually, minifilters
allocate an ECP with the FltAllocateExtraCreateParameter function,
populate it, and insert it into a list (through FltInsertExtraCreateParameter)
before calling the Filter Manager’s I/O APIs.
The FLT_CREATEFILE_TARGET extra creation parameter allows the
Filter Manager to manage cross-volume file creation automatically (the caller
needs to specify a flag). Minifilters don’t need to perform any other complex
operation.
With the goal of supporting container isolation, it’s also possible to set a
reparse point on nonempty directories and, in order to support container
isolation, create new files that have directory reparse points. The default
behavior that the file system has while encountering a nonempty directory
reparse point depends on whether the reparse point is applied in the last
component of the file full path. If this is the case, the file system returns the
STATUS_REPARSE error code, just like for an empty directory; otherwise, it
continues to walk the path.
The Filter Manager is able to correctly deal with this new kind of reparse
point through another ECP (named TYPE_OPEN_REPARSE). The ECP
includes a list of descriptors (OPEN_REPARSE_LIST_ ENTRY data
structure), each of which describes the type of reparse point (through its
Reparse Tag), and the behavior that the system should apply when it
encounters a reparse point of that type while parsing a path. Minifilters, after
they have correctly initialized the descriptor list, can apply the new behavior
in different ways:
■    Issue a new open (or create) operation on a file that resides in a path
that includes a reparse point in any of its components, using the
FltCreateFileEx2 function. This procedure is similar to the one used
by the FLT_CREATEFILE_TARGET ECP.
■    Apply the new reparse point behavior globally to any file that the Pre-
Create callback intercepts. The FltAddOpenReparseEntry and
FltRemoveOpenReparseEntry APIs can be used to set the reparse
point behavior to a target file before the file is actually created (the
pre-creation callback intercepts the file creation request before the file
is created). The Windows Container Isolation minifilter driver
(Wcifs.sys) uses this strategy.
Process Monitor
Process Monitor (Procmon), a system activity-monitoring utility from
Sysinternals that has been used throughout this book, is an example of a
passive minifilter driver, which is one that does not modify the flow of IRPs
between applications and file system drivers.
Process Monitor works by extracting a file system minifilter device driver
from its executable image (stored as a resource inside Procmon.exe) the first
time you run it after a boot, installing the driver in memory, and then deleting
the driver image from disk (unless configured for persistent boot-time
monitoring). Through the Process Monitor GUI, you can direct the driver to
monitor file system activity on local volumes that have assigned drive letters,
network shares, named pipes, and mail slots. When the driver receives a
command to start monitoring a volume, it registers filtering callbacks with
the Filter Manager, which is attached to the device object that represents a
mounted file system on the volume. After an attach operation, the I/O
manager redirects an IRP targeted at the underlying device object to the
driver owning the attached device, in this case the Filter Manager, which
sends the event to registered minifilter drivers, in this case Process Monitor.
When the Process Monitor driver intercepts an IRP, it records information
about the IRP’s command, including target file name and other parameters
specific to the command (such as read and write lengths and offsets) to a
nonpaged kernel buffer. Every 500 milliseconds, the Process Monitor GUI
program sends an IRP to Process Monitor’s interface device object, which
requests a copy of the buffer containing the latest activity, and then displays
the activity in its output window. Process Monitor shows all file activity as it
occurs, which makes it an ideal tool for troubleshooting file system–related
system and application failures. To run Process Monitor the first time on a
system, an account must have the Load Driver and Debug privileges. After
loading, the driver remains resident, so subsequent executions require only
the Debug privilege.
When you run Process Monitor, it starts in basic mode, which shows the
file system activity most often useful for troubleshooting. When in basic
mode, Process Monitor omits certain file system operations from being
displayed, including
■    I/O to NTFS metadata files
■    I/O to the paging file
■    I/O generated by the System process
■    I/O generated by the Process Monitor process
While in basic mode, Process Monitor also reports file I/O operations with
friendly names rather than with the IRP types used to represent them. For
example, both IRP_MJ_WRITE and FASTIO_WRITE operations display as
WriteFile, and IRP_MJ_CREATE operations show as Open if they represent
an open operation and as Create for the creation of new files.
EXPERIMENT: Viewing Process Monitor’s minifilter
driver
To see which file system minifilter drivers are loaded, start an
Administrative command prompt, and run the Filter Manager
control program (%SystemRoot%\System32\Fltmc.exe). Start
Process Monitor (ProcMon.exe) and run Fltmc again. You see that
the Process Monitor’s filter driver (PROCMON20) is loaded and
has a nonzero value in the Instances column. Now, exit Process
Monitor and run Fltmc again. This time, you see that the Process
Monitor’s filter driver is still loaded, but now its instance count is
zero.
The NT File System (NTFS)
In the following section, we analyze the internal architecture of the NTFS file
system, starting by looking at the requirements that drove its design. We
examine the on-disk data structures, and then we move on to the advanced
features provided by the NTFS file system, like the Recovery support, tiered
volumes, and the Encrypting File System (EFS).
High-end file system requirements
From the start, NTFS was designed to include features required of an
enterprise-class file system. To minimize data loss in the face of an
unexpected system outage or crash, a file system must ensure that the
integrity of its metadata is guaranteed at all times; and to protect sensitive
data from unauthorized access, a file system must have an integrated security
model. Finally, a file system must allow for software-based data redundancy
as a low-cost alternative to hardware-redundant solutions for protecting user
data. In this section, you find out how NTFS implements each of these
capabilities.
Recoverability
To address the requirement for reliable data storage and data access, NTFS
provides file system recovery based on the concept of an atomic transaction.
Atomic transactions are a technique for handling modifications to a database
so that system failures don’t affect the correctness or integrity of the
database. The basic tenet of atomic transactions is that some database
operations, called transactions, are all-or-nothing propositions. (A
transaction is defined as an I/O operation that alters file system data or
changes the volume’s directory structure.) The separate disk updates that
make up the transaction must be executed atomically—that is, once the
transaction begins to execute, all its disk updates must be completed. If a
system failure interrupts the transaction, the part that has been completed
must be undone, or rolled back. The rollback operation returns the database
to a previously known and consistent state, as if the transaction had never
occurred.
NTFS uses atomic transactions to implement its file system recovery
feature. If a program initiates an I/O operation that alters the structure of an
NTFS volume—that is, changes the directory structure, extends a file,
allocates space for a new file, and so on—NTFS treats that operation as an
atomic transaction. It guarantees that the transaction is either completed or, if
the system fails while executing the transaction, rolled back. The details of
how NTFS does this are explained in the section “NTFS recovery support”
later in the chapter. In addition, NTFS uses redundant storage for vital file
system information so that if a sector on the disk goes bad, NTFS can still
access the volume’s critical file system data.
Security
Security in NTFS is derived directly from the Windows object model. Files
and directories are protected from being accessed by unauthorized users. (For
more information on Windows security, see Chapter 7, “Security,” in Part 1.)
An open file is implemented as a file object with a security descriptor stored
on disk in the hidden $Secure metafile, in a stream named $SDS (Security
Descriptor Stream). Before a process can open a handle to any object,
including a file object, the Windows security system verifies that the process
has appropriate authorization to do so. The security descriptor, combined
with the requirement that a user log on to the system and provide an
identifying password, ensures that no process can access a file unless it is
given specific permission to do so by a system administrator or by the file’s
owner. (For more information about security descriptors, see the section
“Security descriptors and access control” in Chapter 7 in Part 1).
Data redundancy and fault tolerance
In addition to recoverability of file system data, some customers require that
their data not be endangered by a power outage or catastrophic disk failure.
The NTFS recovery capabilities ensure that the file system on a volume
remains accessible, but they make no guarantees for complete recovery of
user files. Protection for applications that can’t risk losing file data is
provided through data redundancy.
Data redundancy for user files is implemented via the Windows layered
driver, which provides fault-tolerant disk support. NTFS communicates with
a volume manager, which in turn communicates with a disk driver to write
data to a disk. A volume manager can mirror, or duplicate, data from one
disk onto another disk so that a redundant copy can always be retrieved. This
support is commonly called RAID level 1. Volume managers also allow data
to be written in stripes across three or more disks, using the equivalent of one
disk to maintain parity information. If the data on one disk is lost or becomes
inaccessible, the driver can reconstruct the disk’s contents by means of
exclusive-OR operations. This support is called RAID level 5.
In Windows 7, data redundancy for NTFS implemented via the Windows
layered driver was provided by Dynamic Disks. Dynamic Disks had multiple
limitations, which have been overcome in Windows 8.1 by introducing a new
technology that virtualizes the storage hardware, called Storage Spaces.
Storage Spaces is able to create virtual disks that already provide data
redundancy and fault tolerance. The volume manager doesn’t differentiate
between a virtual disk and a real disk (so user mode components can’t see
any difference between the two). The NTFS file system driver cooperates
with Storage Spaces for supporting tiered disks and RAID virtual
configurations. Storage Spaces and Spaces Direct will be covered later in this
chapter.
Advanced features of NTFS
In addition to NTFS being recoverable, secure, reliable, and efficient for
mission-critical systems, it includes the following advanced features that
allow it to support a broad range of applications. Some of these features are
exposed as APIs for applications to leverage, and others are internal features:
■    Multiple data streams
■    Unicode-based names
■    General indexing facility
■    Dynamic bad-cluster remapping
■    Hard links
■    Symbolic (soft) links and junctions
■    Compression and sparse files
■    Change logging
■    Per-user volume quotas
■    Link tracking
■    Encryption
■    POSIX support
■    Defragmentation
■    Read-only support and dynamic partitioning
■    Tiered volume support
The following sections provide an overview of these features.
Multiple data streams
In NTFS, each unit of information associated with a file—including its name,
its owner, its time stamps, its contents, and so on—is implemented as a file
attribute (NTFS object attribute). Each attribute consists of a single stream—
that is, a simple sequence of bytes. This generic implementation makes it
easy to add more attributes (and therefore more streams) to a file. Because a
file’s data is “just another attribute” of the file and because new attributes can
be added, NTFS files (and file directories) can contain multiple data streams.
An NTFS file has one default data stream, which has no name. An
application can create additional, named data streams and access them by
referring to their names. To avoid altering the Windows I/O APIs, which take
a string as a file name argument, the name of the data stream is specified by
appending a colon (:) to the file name. Because the colon is a reserved
character, it can serve as a separator between the file name and the data
stream name, as illustrated in this example:
myfile.dat:stream2
Each stream has a separate allocation size (which defines how much disk
space has been reserved for it), actual size (which is how many bytes the
caller has used), and valid data length (which is how much of the stream has
been initialized). In addition, each stream is given a separate file lock that is
used to lock byte ranges and to allow concurrent access.
One component in Windows that uses multiple data streams is the
Attachment Execution Service, which is invoked whenever the standard
Windows API for saving internet-based attachments is used by applications
such as Edge or Outlook. Depending on which zone the file was downloaded
from (such as the My Computer zone, the Intranet zone, or the Untrusted
zone), Windows Explorer might warn the user that the file came from a
possibly untrusted location or even completely block access to the file. For
example, Figure 11-24 shows the dialog box that’s displayed when executing
Process Explorer after it was downloaded from the Sysinternals site. This
type of data stream is called the $Zone.Identifier and is colloquially referred
to as the “Mark of the Web.”
Figure 11-24 Security warning for files downloaded from the internet.
 Note
If you clear the check box for Always Ask Before Opening This File, the
zone identifier data stream will be removed from the file.
Other applications can use the multiple data stream feature as well. A
backup utility, for example, might use an extra data stream to store backup-
specific time stamps on files. Or an archival utility might implement
hierarchical storage in which files that are older than a certain date or that
haven’t been accessed for a specified period of time are moved to offline
storage. The utility could copy the file to offline storage, set the file’s default
data stream to 0, and add a data stream that specifies where the file is stored.
EXPERIMENT: Looking at streams
Most Windows applications aren’t designed to work with alternate
named streams, but both the echo and more commands are. Thus, a
simple way to view streams in action is to create a named stream
using echo and then display it using more. The following
command sequence creates a file named test with a stream named
stream:
Click here to view code image
c:\Test>echo Hello from a named stream! > test:stream
c:\Test>more 
If you perform a directory listing, Test’s file size doesn’t reflect
the data stored in the alternate stream because NTFS returns the
size of only the unnamed data stream for file query operations,
including directory listings.
Click here to view code image
c:\Test>dir test
 Volume in drive C is OS.
 Volume Serial Number is F080-620F
 Directory of c:\Test
12/07/2018  05:33 PM                 0 test
               1 File(s)              0 bytes
               0 Dir(s)  18,083,577,856 bytes free
c:\Test>
You can determine what files and directories on your system
have alternate data streams with the Streams utility from
Sysinternals (see the following output) or by using the /r switch in
the dir command.
Click here to view code image
c:\Test>streams test
streams v1.60 - Reveal NTFS alternate streams.
Copyright (C) 2005-2016 Mark Russinovich
Sysinternals - www.sysinternals.com
c:\Test\test:
          :stream:$DATA 29
Unicode-based names
Like Windows as a whole, NTFS supports 16-bit Unicode 1.0/UTF-16
characters to store names of files, directories, and volumes. Unicode allows
each character in each of the world’s major languages to be uniquely
represented (Unicode can even represent emoji, or small drawings), which
aids in moving data easily from one country to another. Unicode is an
improvement over the traditional representation of international characters—
using a double-byte coding scheme that stores some characters in 8 bits and
others in 16 bits, a technique that requires loading various code pages to
establish the available characters. Because Unicode has a unique
representation for each character, it doesn’t depend on which code page is
loaded. Each directory and file name in a path can be as many as 255
characters long and can contain Unicode characters, embedded spaces, and
multiple periods.
General indexing facility
The NTFS architecture is structured to allow indexing of any file attribute on
a disk volume using a B-tree structure. (Creating indexes on arbitrary
attributes is not exported to users.) This structure enables the file system to
efficiently locate files that match certain criteria—for example, all the files in
a particular directory. In contrast, the FAT file system indexes file names but
doesn’t sort them, making lookups in large directories slow.
Several NTFS features take advantage of general indexing, including
consolidated security descriptors, in which the security descriptors of a
volume’s files and directories are stored in a single internal stream, have
duplicates removed, and are indexed using an internal security identifier that
NTFS defines. The use of indexing by these features is described in the
section “NTFS on-disk structure” later in this chapter.
Dynamic bad-cluster remapping
Ordinarily, if a program tries to read data from a bad disk sector, the read
operation fails and the data in the allocated cluster becomes inaccessible. If
the disk is formatted as a fault-tolerant NTFS volume, however, the Windows
volume manager—or Storage Spaces, depending on the component that
provides data redundancy—dynamically retrieves a good copy of the data
that was stored on the bad sector and then sends NTFS a warning that the
sector is bad. NTFS will then allocate a new cluster, replacing the cluster in
which the bad sector resides, and copies the data to the new cluster. It adds
the bad cluster to the list of bad clusters on that volume (stored in the hidden
metadata file $BadClus) and no longer uses it. This data recovery and
dynamic bad-cluster remapping is an especially useful feature for file servers
and fault-tolerant systems or for any application that can’t afford to lose data.
If the volume manager or Storage Spaces is not used when a sector goes bad
(such as early in the boot sequence), NTFS still replaces the cluster and
doesn’t reuse it, but it can’t recover the data that was on the bad sector.
Hard links
A hard link allows multiple paths to refer to the same file. (Hard links are not
supported on directories.) If you create a hard link named
C:\Documents\Spec.doc that refers to the existing file C:\Users
\Administrator\Documents\Spec.doc, the two paths link to the same on-disk
file, and you can make changes to the file using either path. Processes can
create hard links with the Windows CreateHardLink function.
NTFS implements hard links by keeping a reference count on the actual
data, where each time a hard link is created for the file, an additional file
name reference is made to the data. This means that if you have multiple hard
links for a file, you can delete the original file name that referenced the data
(C:\Users\Administrator\Documents\Spec.doc in our example), and the other
hard links (C:\Documents\Spec.doc) will remain and point to the data.
However, because hard links are on-disk local references to data (represented
by a file record number), they can exist only within the same volume and
can’t span volumes or computers.
EXPERIMENT: Creating a hard link
There are two ways you can create a hard link: the fsutil hardlink
create command or the mklink utility with the /H option. In this
experiment we’ll use mklink because we’ll use this utility later to
create a symbolic link as well. First, create a file called test.txt and
add some text to it, as shown here.
Click here to view code image
C:\>echo Hello from a Hard Link > test.txt
Now create a hard link called hard.txt as shown here:
Click here to view code image
C:\>mklink hard.txt test.txt /H
Hardlink created for hard.txt > test.txt
If you list the directory’s contents, you’ll notice that the two files
will be identical in every way, with the same creation date,
permissions, and file size; only the file names differ.
Click here to view code image
c:\>dir *.txt
 Volume in drive C is OS
 Volume Serial Number is F080-620F
 Directory of c:\
12/07/2018  05:46 PM                26 hard.txt
12/07/2018  05:46 PM                26 test.txt
               2 File(s)             52 bytes
               0 Dir(s)  15,150,333,952 bytes free
Symbolic (soft) links and junctions
In addition to hard links, NTFS supports another type of file-name aliasing
called symbolic links or soft links. Unlike hard links, symbolic links are