features) might be similar. Such divergence in the character level
distribution is primarily due to the information loss inherent in the
machine-generation process, which is a function of the modeling
power of the RNN used.
We note that the character level distribution metric is appropri-
ate for our purposes, because it is the lowest level metric being
modeled by our RNN, and therefore most likely to become a com-
plexity bottleneck for the RNN. An attacker might consider an RNN
generator that trains using word-level distributions. Fortunately
(for our purposes), word-level distributions are more complex and
difficult to model, both because the number of words is combi-
natorially larger than number of valid characters in the English
Figure 9: Key insight of our defense. During the training pro-
cess, the language model builds a fixed memory represen-
tation of the large training corpus and its representativity
is limited by the model size. The information loss incurred
during the training would propagate to the generated text,
leading to statistically detectable difference in the underly-
ing character distribution between the generated text and
human text.
language, and because such a model would need to add additional
rules for valid punctuation. Therefore, an RNN generator target-
ing word-level distributions would be even more computationally
constrained (thus incur more information loss), and intuitively, our
defense would work at least as well as on a character-level RNN.
5.1 Proposed Methodology
More concretely, consider the following attack scenario: The at-
tacker trains on a set of human generated reviews RT and builds a
character-level RNN language model M, to generate a set of reviews
RF. Even when RT is chosen in such a way that RF becomes linguis-
tically similar to the real reviews RL on the site, we can statistically
detect variations in the character level distribution between RF and
RL.
Algorithm 1 provides details of the method. The service provider
maintains access to the set of known machine-generated reviews
RF, along with the set of real reviews RL, and aims to determine
whether a given test review T is fake or real. Based on our insight,
we expect the character-level distribution of reviews in set RF to
statistically diverge from that of reviews in set RL.
The character-level probability distribution P (Xt+1=xt+1|x1, ...,t),
gives the probability of predicting the next character, given the
sequence of preceding characters. To capture the divergence in the
character distribution, the defender first builds an RNN language
model RNNF using the set of machine-generated reviews RF and
another language model RNNL using RL. Next, given a test review
T , we feed the review, character by character, into each RNN model
to obtain two character distributions, providing statistical repre-
sentations of review T for each model. Finally, if the log-likelihood
ratio of the test review’s character distribution closely fits the model
RNNF, then we flag the review as fake.
5.2 Defense Evaluation
We evaluate our defense scheme along two directions. First, we
study the detection performance of our scheme and how it compares
to an ML scheme based on linguistic features (Section 4.1). Second,
we investigate the robustness of our approach to evade detection
against two attacker strategies.
 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1Detection PerformanceTemperaturePrecisionRecallLimited Size ModelTraining ReviewsGenerated ReviewsSession E4:  Adversarial Social Networking CCS’17, October 30-November 3, 2017, Dallas, TX, USA1151(a) Precision
(b) Recall
Figure 10: Performance of proposed defense and linguistic classifier (Sec-
tion 4.1).
Figure 11: Detection performance given
different sizes of defense training sam-
ples.
Algorithm 1 Proposed Defense
▷ input-RF:machine-generated review training set, RL:real re-
view training set, T:test review
1: procedure Defense(RF, RL, T)
2:
3:
4:
5:
6:
7:
8:
9:
10:
N ← length(T)
RNNF ← trainRNN(RF)
RNNL ← trainRNN(RL)
for t = 1:N-1 do
feed Xt into RNNF
LF ← PF(Xt+1=xt+1|x1, ...,t)
feed Xt into RNNL
LL ← PL(Xt+1=xt+1|x1, ...,t)
(cid:80)N-1
Lt ← − log LLLF
i=1 Li
¯L ←
N-1
if ¯L > 0 then
return FAKE
▷ negative log-likelihood ratio
11:
12:
13:
14:
15:
else
return REAL
We follow a standard RNN model training process to train RNNF
and RNNL. We refer to them as defense RNN. Unless otherwise
stated, we report performance on 2,000 test reviews (balanced set
of real and machine-generated reviews) in the remainder of this
section.
Detection Performance. To first understand the performance in a
potential best case scenario, we evaluate our scheme by considering
a large amount of ground-truth information. Our ground-truth set
consists of 120K machine-generated reviews, from our Yelp attack
dataset (Section 3.3), and 120K additional real reviews. We set the
model configuration for the defense RNN to be the following: 1,024
hidden units, 2 hidden layers, batch size of 128 and 20 training
epochs.
As a baseline for comparison, we compute detection performance
using the ML scheme described in Section 4.1, which we refer to as
the linguistic classifier. Note that the ML scheme is based on high
level linguistic features and trained using the same ground-truth
set of machine-generated and real reviews. We expect the linguistic
classifier to perform better than the results in Section 4.1 because
the training data now includes machine-generated reviews that we
aim to identify directly.
Figure 10 shows the detection performance when we train and
test on text generated at different temperatures. Our approach
achieves high precision and recall at all temperatures, i.e. over 0.98
precision and 0.97 recall. Additionally, we outperform the linguis-
tic classifier at most temperatures, and the gap between the two
schemes increases at higher temperatures (e.g., temperature > 0.6).
At temperature 1.0, our scheme achieves an F-score (the harmonic
mean of precision and recall) of 0.98, while the linguistic approach
only achieves an F-score of 0.55. Interestingly, the linguistic classi-
fier shows high detection performance at low temperatures. This
trend can be explained by our earlier finding that linguistic features
diverge more from the real reviews at low temperatures (Figure 7).
Next, we study performance when we limit the amount of ground-
truth used for training and focus on text generated at temperature
1.0. Figure 11 shows performance when training data size varies
Training
Samples
Hidden
Unit Size
2K
10K
20K
100K
200K
128
256
512
768
1,024
Layer
Size
1
1
1
2
2
Batch
Size
16
32
56
128
128
Training
Epoch
50
50
30
20
20
Table 3: Training configurations of defense models when de-
fense training sample size varies.
Hidden
Unit Size
Training
Samples
128
256
512
768
1,024
2,048
10K
50K
100K
500K
617K
617K
Layer
Size
1
1
1
2
2
2
Batch
Size
32
56
128
256
256
256
Training
Epoch
50
50
30
20
20
50
Table 4: Training configurations of attack models when at-
tack model size varies.
 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1Detection PrecisionTemperatureProposed DefenseLinguistic Classifier 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1Detection RecallTemperatureProposed DefenseLinguistic Classifier 0 0.2 0.4 0.6 0.8 12K10K20K100K200KDetection F-scoreTraining Sample Size for Defense RNNProposed DefenseLinguistic ClassifierSession E4:  Adversarial Social Networking CCS’17, October 30-November 3, 2017, Dallas, TX, USA1152from 2,000 to 200K samples. Each training dataset is a balanced
dataset of machine-generated and real reviews. The RNN model
configuration used for defense for each training set size is detailed
in Table 3. Our scheme significantly outperforms the linguistic clas-
sifier for all datasets and achieves a high F-score of 0.80 using only
1,000 machine-generated reviews (2,000 training dataset). Consid-
ering the fact that service providers have taken considerable effort
to build large fake review datasets (∼250K fake reviews in Table 1),
1,000 reviews is a relatively small sample of known fake reviews.
Thus, unlike the linguistic classifier, our defense scheme performs
well with highly limited ground-truth information.
Evading Detection by Increasing Attack Model Quality. The
attacker can evade detection by improving the quality of the RNN
generative model or increasing the memory size of the model. A
higher quality model would generate more “natural” text, and thus
reduce the divergence in the character distribution. However, this
strategy comes with a higher cost for the attack. Training a larger
RNN model requires more training data to be collected, more com-
putational resources (e.g., GPUs with more memory and computa-
tional capacity), while also imposing additional training time. It is
hard to quantify the increase in attack cost when accounting for all
these factors. Instead, we focus on the impact on training time as
the attacker varies model quality to evade detection.
We vary the attack model size (number of hidden units) from
128 to 2,048. We also vary other model parameters, and the size of
the training dataset to avoid underfitting or overfitting. Details of
the attack models are described in Table 4. For the defense RNN,
we use a configuration based on 2K training samples in Table 3.
Figure 12 shows the tradeoff between decreases in detection per-
formance (F-score) and increases in training time for the attacker.
In general, when the attacker trains a larger model, our defense per-
formance would degrade: when doubling the model size from 128 to
256 cells, detection performance drops by 3.95% with training time
increasing by 71.86%. As the model size grows further, attacker’s
gain in evasion rate slows, while the matching training time accel-
erates significantly. This is due to the increase in computational
complexity: from model size 1,024 to 2,048, defense performance
only decreases by 2.70% but the attacker’s training time raises by
435.1%.
In practice, larger models would require a significantly larger
training set as well. For example, Jozefowicz et al. [21] trained a 2-
layer RNN with 8, 192 hidden units on a dataset with ∼0.8B words,
14x larger than our training dataset, to achieve state-of-the-art
model performance. Therefore, the computational cost and amount
of training data required to train a larger model would become
prohibitively expensive for all but the most resourceful attackers.
Next, we show there are other ways to further diminish the
power of any resource-based countermeasures by the attacker. We
observe that our defense scheme performs better on longer reviews,
as the scheme has more data to capture divergence in the character