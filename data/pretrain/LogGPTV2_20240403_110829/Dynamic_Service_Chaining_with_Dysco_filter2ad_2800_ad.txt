control messages, described in §3, carry the five-tuples of the TCP
sessions going through the reconfiguration, so the Dysco daemon
and agent can associate the control message with the session state
inside the kernel.
Beyond the protocol outlined in §3, we now address several
interoperability issues that arise for middleboxes that terminate
TCP sessions, including layer-7 load balancers and proxies.
Triggering a reconfiguration using “splice”: To deal with
middleboxes that terminate TCP sessions and want to remove them-
selves, Dysco offers two options First, we have a library function
that receives two sockets and a delta representing how much data
was added to or removed from the first socket before delivering the
data to the second socket:
int dysco_splice(int fd_in, int fd_out, int delta)
A positive delta indicates that data were added to and a negative
delta indicates that data were removed from fd_in. This option
requires the modification of a middlebox to call the library function.
Second, we support unmodified middleboxes that use the Linux’s
“splice” system call. This system call is used by many applications,
e.g., HAProxy [14], to avoid transferring data from the kernel to
user space. For this case, we provide a shared library that inter-
cepts the C library functions used for network communication (e.g.,
socket, accept, connect, splice, close, and the read and write func-
tions). The shared library must be preloaded using LD_PRELOAD.
Each function of the shared library first calls the original function
from the C library and then records the result of the operation. For
example, a function that intercepts any of the read calls records
the amount of data read from a socket. When the splice function is
called, the shared library uses the recorded information to compute
the delta between two sockets and find the information about the
associated TCP sessions (i.e., the two five-tuples). Note that the
Linux splice call receives a socket and a pipe as parameters. The
first call to splice just sets data structures internal to the kernel.
The operation is performed only on the second call to splice. We
track both calls and trigger a reconfiguration after the second call,
when we have all the information needed. Note that we assume
that the application calling splice does not want to process the
data anymore. This is the case for L7 load balancers, but this is
not necessarily true when an HTTP proxy is handling persistent
connections, so the shared library must be used with prudence.
Differences in TCP options for two spliced TCP sessions:
When a Dysco agent initiates a “splice” of two TCP sessions, the
Dysco agents on the left and right anchors need to translate not
only the sequence and acknowledgment numbers of each packet
but also the TCP options that differ between the two sessions or
have a different meaning. The relevant options are window scal-
ing, selective acknowledgment, and timestamp. Window scaling
is easy to convert, as the anchors record the scale factor negoti-
ated during the session setup. The Dysco agent first computes the
actual receiver window of a packet using the scale factor of its
incoming subsession and then rescales the calculated value by the
scale factor of the outgoing subsession. The translation of the se-
lective acknowledgment (SACK) blocks is particularly important
because the blocks of one session have no meaning to the other
session (if blocks are not translated, the Linux kernel will discard
all packets that contain blocks with invalid sequence numbers). To
convert the sequence numbers of SACK blocks, the anchors add to
(or subtract from) each sequence number the delta that they receive
during session reconfiguration. Timestamps are used for protection
against wrapped sequence numbers and RTT computation. The
Linux kernel keeps track of the highest timestamp received and
discards packets whose timestamps are too far from it. To avoid
packets being discarded by the kernel, Dysco translates timestamps
in the same way as it does with sequence numbers.
5 PERFORMANCE EVALUATION
We now evaluate Dysco in the three main phases of a session
across different network settings. First, we measure the latencies for
session initiation to quantify the overhead introduced by subsession
setup and including middlebox address lists in the SYN packets.
Second, we measure the throughput of a session during normal data
transfer to show that the Dysco agents can forward packets at high
speed. Third, we show that dynamic reconfiguration improves end-
to-end performance and introduces minimal transient disruptions.
Our testbed consists of a NEC DX2000 blade server with 11 hosts,
each with one Intel eight-core Xeon D 2.1 GHz processor, 64 GB
of memory, and two 10Gbps NICs. The two NICs of each host are
connected to two layer-two switches, forming two independent
LANs. The 11 hosts run Ubuntu Linux with kernel 4.4.0.
5.1 Session initiation
Figure 8 shows session setup latency under two scenarios: Dysco
and middleboxes inserted by IP routing (Baseline). We do not run a
middlebox application (i.e., the middleboxes simply forward packets
in both directions), so we only measure the overhead of the Dysco
protocol. The scenario with one middlebox has three hosts, and the
one with four middleboxes has six hosts connected in a line. The
measurements represent the time for a TCP socket connect() at
the client, which is the round-trip for establishing the TCP session
to the server. Figure 8(a) shows the latencies when the checksum
computation is offloaded to the NIC and Figure 8(b) when the
computation is not offloaded. The worst case for Dysco is with
four middleboxes, and when the checksum computation is not
offloaded to the NIC. The time difference between the two averages,
in this case, is only 94µs. The measured latencies are insignificant
compared to the overhead for middlebox applications to transfer
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
P. Zave et al.
packets to user space to perform network functions and represent
less than 0.5% in a TCP session with RTT of 20 ms in the worst
case. From now on, we report results only for the cases where
checksum and TCP segmentation are not offloaded to the NIC, as
these represent the worst cases for Dysco.
results with the baseline. The measurement was performed with
wrk [47], an HTTP benchmarking tool, with 16 threads and four
hundred persistent connections, as recommended in [47]. NGINX
is able to serve more than 300,000 connections per second when
only one middlebox is between the client and the server, and a little
under 300,000 connections per second when four middleboxes are
between the client and the server. The results are consistent with
the throughput measurements, and the largest difference between
Dysco and the baseline is less than 1.8 percentage points.
(a) Checksum offloading
(b) No checksum offloading
Figure 8: Latency for session initiation.
5.2 Data-plane throughput
Figure 9 shows the goodput, measured at the receivers, of multiple
TCP sessions between four clients and four servers connected via a
middlebox that simply forwards traffic between the clients and the
servers. Again, we do not run an application on the middlebox to
quantify just the Dysco overhead. The figure shows no noticeable
difference between the performance of Dysco and the baseline
case; the differences between the two cases are always within one
standard deviation and are less than 1.5 percentage points in the
worst case. We show the results up to 10000 sessions. Note that after
100 sessions the link becomes the bottleneck, so we do not notice
a significant difference between Dysco and the baseline. Receive
side scaling (RSS) is supported in the NIC and enabled in the Linux
kernel, so packets belonging to one TCP session are directed to the
same core. Therefore, the result for one session gives a better idea
of the performance degradation of Dysco, because it represents the
goodput of one core.
Figure 9: Goodput of Dysco compared with the baseline.
We also measured the number of requests that NGINX [27], a
popular HTTP server, can sustain under Dysco and compared the
Figure 10: Number of HTTP requests per second NGINX can serve
under Dysco and the baseline.
5.3 Dynamic reconfiguration
In this section, we investigate a few scenarios of dynamic reconfig-
uration. We use the logical topology of Figure 11; one of the hosts
works as the router, and each IP subnet is on a different VLAN.
Figure 11: Testbed topology for the performance evaluation of the
reconfiguration experiments.
Middlebox deletion: We run TCP sessions from four Clients to
four Servers, passing through the Router and Middlebox1, which is
running a TCP proxy. After 40, 60, 80, and 100 seconds, we trigger
reconfigurations that remove Middlebox1 from a client-server pair
and direct the traffic of all TCP sessions between them directly
from the client to the server passing only through the Router. Each
client-server pair has a bundle of 150 TCP sessions for a total of
600 simultaneous sessions.
The top of Figure 12 shows the goodput before and after each
reconfiguration. The time series represents measures of application
data (goodput) at one-second intervals. After each reconfiguration,
the goodput of the sessions that no longer go through the proxy
increases significantly. We can see that after 100 seconds, when all
Dynamic Service Chaining with Dysco
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
600 sessions no longer go through the proxy, the overall goodput
has doubled from the time interval before the reconfigurations
started. The bottom of Figure 12 shows the CPU utilization at the
proxy. We can see that the CPU utilization decreases at the instants
40, 60, 80, and 100, going to zero after all the reconfigurations end.
(a) TCP SACK enabled
(b) TCP SACK disabled
Figure 14: TCP performance during reconfiguration.
Figure 12: Goodput of TCP sessions (top) and CPU Utilization at
the proxy (bottom) before and after multiple reconfigurations.
We can see in Figure 12 that the reconfigurations are successful
and the traffic reaches steady-state behavior after 100 seconds. Dur-
ing the reconfiguration, the Dysco agent on the proxy advertises
a small window to the senders to reduce the amount of traffic on
the receivers. Note that during the reconfiguration, packets are
received from both paths causing a surge of traffic at the receivers.
We initially tested a zero window advertisement, but the perfor-
mance degraded significantly. The strategy that worked best was to
advertise the minimum of the actual advertised window and a small
constant (64K) that allowed the flow of packets to continue without
overwhelming the receivers. Figure 13 shows that reconfiguration
time is short: almost 80% of reconfigurations took less than 2ms and
98.7% less than 4ms. The few larger values happen when control
messages are lost and need to be retransmitted.
Figure 13: CDF of the reconfiguration time for the proxy removal.
Session disruption: We investigate the transient performance
of a session after removing a proxy, where the new path is faster
Middlebox replacement with state transfer: Middleboxes
may need to transfer internal state as part of middlebox replace-
ment, and ensure that the new component is ready before receiv-
ing its first packet [11, 39]. While routing solutions rely on clever
synchronization of switches and a controller, Dysco uses simpler
mechanisms, as the anchors can coordinate to determine when the
new component is ready.
To experiment with state transfer, we extended the Dysco dae-
mon to get state information of the Linux Netfilter firewall, serialize
the data using JSON, and send the serialized data to another Dysco
daemon. We did not modify Netfilter to interact with Dysco, so
the interaction between Dysco and Netfilter is completely transpar-
ent to the firewall. The internal state of Netfilter can be obtained
by running the conntrack Linux utility with a filter to select the
relevant session(s).
than the old one (so packets may arrive out of order to the destina-
tion). To better control network latency, we simulate the testbed
topology in Mininet, where we can introduce different link delays
and bandwidths. Figure 14(a) plots the congestion window (left
y-axis) and TCP goodput (right y-axis) during a proxy removal.
The proxy triggers the reconfiguration 30 seconds after the be-
ginning of the session. As we can see, the session experiences no
disruption. Figure 14(b) shows why the Dysco agents must handle
TCP options—with TCP SACK disabled, packet losses temporarily
degrade session performance.
The reconfiguration involves two Netfilter firewalls running on
Middlebox1 and Middlebox2, and TCP sessions running from three
clients to three servers in Figure 11. The client and the server are
the left and right anchors, respectively. Upon receiving a SYN-ACK
message (indicating that the new path is established), the left anchor
sends a state-transfer message to Middlebox1 with the session that
is migrating to the new path, the address of Middlebox2, and the
addresses of the left and right anchors. The Dysco daemon running
on Middlebox1 gets and sends the state information directly to the
Dysco agent running on Middlebox2 and waits for a notification
that the state is installed before notifying the left and right anchors
that the new path is ready.
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
P. Zave et al.
Most of these difficulties have a single root cause: service chain-
ing in Dysco, and especially dynamic reconfiguration, requires
control signaling and metadata that are difficult to transmit within
a TCP session. This is exactly the same difficulty encountered in im-
plementing multihoming with Multipath TCP [38]. Multipath TCP
uses primarily TCP options, which we did not use for lack of space.
Whether a protocol uses SYN payloads, TCP options, or auxiliary
UDP signaling, there is a danger that control and metadata will be
blocked, dropped, or modified in transit. This can happen because
of security measures, or, in the case of TCP options, because of inno-
cent functions such as resegmentation in NICs. The safest approach
seems to be encoding control and metadata in escape sequences in
the TCP byte stream, but this is inefficient and requires significant
manipulation of sequence numbers.
Given the importance of TCP, this is a problem urgently in need
of a good solution. There should be a secure, reliable, and efficient
way of associating control and metadata with a TCP session. Most
importantly, designers of this solution should recognize that there
may be more than one additional feature, function, or protocol
adding metadata to a single session, so that they do not interfere
with each other.
7 RELATED WORK
7.1 Service chaining by forwarding
BGP: Early solutions to dynamic service chaining manipulate BGP
to “hijack” traffic, either within a single domain [2] or across the
wide area [45]. However, manipulating BGP is risky in the wide
area, and operates at the coarse level of destination IP prefixes
rather than individual sessions. Plus, it is difficult to use BGP to
insert multiple middleboxes in a service chain.
Stratos and E2: Stratos [10] and E2 [32] are designed for middle-
box deployment within clouds. They use fine-grained forwarding
rules for (static) service chaining, inheriting the scaling challenges
mentioned in §1. They also offer integrated solutions for managing
middleboxes, including elastic scaling of middlebox instances, fault-
tolerance, and placement. Dysco is not concerned with middlebox
management and can be readily combined with any approach to
middlebox management, including these.
OpenNF: OpenNF [11] (and also Split-Merge [39]) assumes
that dynamic service chaining is provided by updating how SDN
switches forward packets. The special contribution of OpenNF is
efficient, coordinated control of forwarding changes and middlebox
state migration, so that middleboxes can be replaced quickly and
safely. Our Dysco prototype was easily extended to support import-
ing and exporting middlebox state. As a session protocol, Dysco can
naturally handle a wider range of reconfiguration scenarios than
OpenNF can, including removing proxies. OpenNF is designed for
use in an SDN environment, while Dysco places no constraints on
the choice of the control plane. Also, there is a risk of performance
problems with OpenNF controllers because they are responsible
for packet buffering.
7.2 Service chaining by session protocols
DOA: Like Dysco, DOA [46] uses a session protocol for service
chaining. Dysco and DOA differ as follows: (i) DOA requires a new
Figure 15: Goodput during reconfiguration and state migration.
Figure 15 shows the goodput of three bundles of 100 sessions
running while the state from Middlebox1 is transferred to Middle-
box2. For this experiment, the link speeds on the two middleboxes
were limited to 2Gbps to avoid creating a bottleneck on the router