### Optimized Text

#### Sizes for Two PWB and Fence Combinations
We compare two combinations of Persistent Write Back (PWB) and persistence fences: CLFLUSH + NOP (left) and CLFLUSHOPT + SFENCE (right). These are defined in [11].

#### Experimental Setup
To highlight the performance implications of both real SGX and real Persistent Memory (PM), we use two servers with distinct characteristics. All experimental comparisons are executed separately for each server, and we specify the node on which each experiment is conducted. All SCONE containers are based on Alpine Linux [25].

#### Models and Datasets
All models used in our evaluations are Convolutional Neural Networks (CNNs). The convolutional layers use Leaky Rectified Linear Unit (LReLU) [15] as the activation function, and all output layers are softmax [29] layers. The model optimization algorithm used is Stochastic Gradient Descent (SGD) with a learning rate of 0.1. Unless otherwise stated, all training iterations use a batch size of 128. For the dataset, we use MNIST [2], a popular dataset in the deep learning community consisting of 70,000 grayscale images of handwritten digits (60,000 training samples and 10,000 test samples).

#### Comparison of SGX-ROMULUS and Unmodified Romulus
We begin by comparing SGX-ROMULUS with the unmodified Romulus library running in a SCONE container to understand the performance differences between a manually ported library using the Intel SGX SDK and the unmodified version in SCONE.

**Performance Metric: Swaps per Second (SPS)**
- SPS measures the number of swaps per second of an array of integers stored in PM.
- This metric evaluates the overhead of randomly swapping array values within a transaction for different persistence fences and transaction sizes.
- The experiment is conducted on the sgx-emlPM node, where real SGX is the primary factor affecting performance.
- For each transaction size, SPS is measured for 20 seconds.
- Figure 6 shows the throughput of swap operations on a 10 MB persistent array with different transaction sizes for a single-threaded application.
- We include results for two choices of PWB implemented by Romulus and SGX-ROMULUS: clflush + NOP and clflushopt + sfence. Our servers do not support clwb.

**Results:**
- Persistence fences in SGX-ROMULUS take approximately 1.6× to 3.7× longer to complete compared to native (no SGX) systems for transaction sizes between 2 and 2048 swaps per transaction.
- Compared to Romulus in SCONE, transactions for both fence implementations in SGX-ROMULUS are approximately 1.5× to 2.5× slower for transaction sizes between 2 to 64 swap operations per transaction.
- Beyond 64 swap operations per transaction, there is a pronounced drop in throughput for Romulus in SCONE, while SGX-ROMULUS maintains better performance.

#### Overhead Analysis
For the emlSGX-PM server, without real SGX hardware (hence no expensive page swaps), the main bottleneck is real PM. Table I shows that writes to PM are significantly faster than writes to SSD, especially for enclave sizes beyond the EPC limit. Similarly, restores from PM into enclave memory are faster compared to the SSD-based counterpart.

#### Training Larger Models
Our results suggest that PLINIUS is best suited for models with sizes beneath the EPC limit. Models larger than the EPC limit can be trained with PLINIUS but experience a significant drop in training performance due to extensive page swaps by the SGX kernel driver. Figure 7 shows that our mirroring mechanism still performs better than SSD-based checkpointing for model sizes beyond the EPC limit. A possible strategy to overcome the EPC limitation is to distribute the training job over multiple secure CPUs, which we will explore in the future. Additionally, a recent processor release by Intel expands the EPC to 256 MB [4], enabling more efficient training of larger models.

#### Mirroring Frequency
By default, PLINIUS mirrors data after every iteration. The mirroring frequency can be adjusted based on the failure rate of the training environment. A high frequency of failures requires a higher mirroring frequency to achieve good fault tolerance guarantees.

#### Overhead of Data Batch Decryptions
ML algorithms, such as SGD, manipulate training data in batches for each training iteration. In this experiment, we study the performance impact of batch decryptions of training data into enclave memory. Figure 8 shows that iterations with batch decryption are 1.2× slower on average, a relatively small price for data confidentiality during training.

#### Crash Resilience
To demonstrate the crash resilience of PLINIUS, we train a model with 5 LReLU-convolutional layers on the MNIST dataset for 500 iterations, introducing 9 random crashes and resumptions. Figure 9(a) shows that the loss curve closely follows the baseline, indicating correct saving and restoring of model parameters. In contrast, Figure 9(b) shows the loss curve when the system cannot recover learned parameters, resulting in a total of over 1000 iterations to fully train the model.

#### PLINIUS on AWS EC2 Spot Instances
A practical use case for PLINIUS is model training on spot instances, such as those offered by Amazon EC2 and Microsoft Azure. Spot instances are prone to interruptions, and efficient fault tolerance mechanisms are essential. We use Amazon EC2 spot instance traces to simulate a realistic training scenario. Figure 10(a) shows the loss curve after 500 iterations, demonstrating PLINIUS's crash resilience. Figure 10(b) shows the state curve of the training process, with only 2 interruptions observed.

This optimized text aims to provide a clear, coherent, and professional presentation of the experimental setup, results, and conclusions.