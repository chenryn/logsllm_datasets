title:Language-Directed Hardware Design for Network Performance Monitoring
author:Srinivas Narayana and
Anirudh Sivaraman and
Vikram Nathan and
Prateesh Goyal and
Venkat Arun and
Mohammad Alizadeh and
Vimalkumar Jeyakumar and
Changhoon Kim
Language-Directed Hardware Design for
Network Performance Monitoring
Srinivas Narayana1, Anirudh Sivaraman1, Vikram Nathan1, Prateesh Goyal1,
Venkat Arun2, Mohammad Alizadeh1, Vimalkumar Jeyakumar3, Changhoon Kim4
3 Cisco Tetration Analytics
4 Barefoot Networks
1 MIT CSAIL
2 IIT Guwahati
ABSTRACT
Network performance monitoring today is restricted by existing
switch support for measurement, forcing operators to rely heavily on
endpoints with poor visibility into the network core. Switch vendors
have added progressively more monitoring features to switches, but
the current trajectory of adding specific features is unsustainable
given the ever-changing demands of network operators. Instead,
we ask what switch hardware primitives are required to support an
expressive language of network performance questions. We believe
that the resulting switch hardware design could address a wide
variety of current and future performance monitoring needs.
We present a performance query language, Marple, modeled on fa-
miliar functional constructs like map, filter, groupby, and zip. Marple
is backed by a new programmable key-value store primitive on
switch hardware. The key-value store performs flexible aggregations
at line rate (e.g., a moving average of queueing latencies per flow),
and scales to millions of keys. We present a Marple compiler that
targets a P4-programmable software switch and a simulator for high-
speed programmable switches. Marple can express switch queries
that could previously run only on end hosts, while Marple queries
only occupy a modest fraction of a switch’s hardware resources.
CCS CONCEPTS
• Networks → Network monitoring; Programmable networks;
KEYWORDS
Network measurement; network hardware; network programming
ACM Reference format:
Srinivas Narayana, Anirudh Sivaraman, Vikram Nathan, Prateesh Goyal,
Venkat Arun, Mohammad Alizadeh, Vimalkumar Jeyakumar, and Changhoon
Kim. 2017. Language-Directed Hardware Design for Network Performance
Monitoring. In Proceedings of SIGCOMM ’17, Los Angeles, CA, USA, August
21–25, 2017, 14 pages.
https://doi.org/10.1145/3098822.3098829
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
© 2017 Copyright held by the owner/author(s). Publication rights licensed to Association
for Computing Machinery.
ACM ISBN 978-1-4503-4653-5/17/08. . . $15.00
https://doi.org/10.1145/3098822.3098829
INTRODUCTION
1
Effective performance monitoring of large networks is crucial to
quickly localize problems like high queueing latency [12], TCP in-
cast [61], and load imbalance across network links [27]. A common
approach to network monitoring is to collect information from the
endpoint network stack [53, 58, 62] or to use end-to-end probes [40]
to diagnose performance problems. While endpoints provide appli-
cation context, they lack visibility to localize performance problems
at links deep in the network. For example, it is challenging to local-
ize queue buildup to a particular switch or pinpoint traffic causing
the queue buildup, forcing operators to infer the network-level root
causes indirectly [40].
Switch-based monitoring could allow operators to diagnose prob-
lems with more direct visibility into performance statistics. How-
ever, traditional switch mechanisms like sampling [7, 21], mirror-
ing [8, 42, 65], and counting [34, 49] are quite restrictive. Sampling
and mirroring miss events of interest as it is infeasible to collect
information on all packets, while counters only track traffic volume
statistics. None of these mechanisms provides relevant performance
data, like queueing delays.
Some upcoming technologies recognize the need for better perfor-
mance monitoring using switches. In-band network telemetry [12]
writes queueing delays experienced by a packet on the packet itself,
allowing endpoints to localize delay spikes. The Tetration switching
chip [9] provides a flow cache that measures flow-level performance
metrics. These metrics are useful, but they are exposed at a fixed
granularity (e.g., per 5-tuple), and the metrics themselves are fixed.
For example, the list of exposed metrics includes flow-level latency
and packet size variation, but not latency variation, i.e., jitter.
Operator requirements are ever-changing, and redesigning hard-
ware is expensive. We believe that the trajectory of adding fixed-
function switch monitoring piecemeal is unsustainable. Instead, we
advocate building performance monitoring primitives that can be
flexibly reused for a variety of needs. Programmable switches [3, 13,
25] now support flexible parsing [39], header processing [33, 56],
and scheduling [57]. Our goal is to add monitoring to this list.
This paper applies language-directed hardware design to the
problem of flexible performance monitoring, inspired by early efforts
on designing hardware to support high-level languages [36, 50, 59].
Specifically, we design a language that can express a broad variety
of performance monitoring use cases, and then design high-speed
switch hardware primitives in service of this language. By designing
hardware to support an expressive language, we believe the resulting
hardware design can support a wide variety of current and future
performance monitoring needs.
Fig. 1 provides an overview of our performance monitoring sys-
tem. To use the system, an operator writes a query in a domain-
specific language called Marple, either to implement a long-running
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
S. Narayana et al.
the number of records per second streamed out to a standard data-
processing system running on the collection server.
While programmable switches support many of Marple’s stateless
language constructs that modify packet fields alone (e.g., map and
filter), they do not support aggregation of state across packets for
a large number of flows (i.e., groupby). To support flexible aggre-
gations over packets, we design a programmable key-value store
in hardware (§3), where the keys represent flow identifiers and the
values represent the state computed by the aggregation function.
This key-value store must update values at the line rate of 1 packet
per clock cycle (at 1 GHz [6, 33]) and support millions of keys (i.e.,
flows). Unfortunately, neither SRAM nor DRAM is simultaneously
fast and dense enough to meet both requirements.
We split the key-value store into a small but fast on-chip cache
in SRAM and a larger but slower off-chip backing store in DRAM.
Traditional caches incur variable write latencies due to cache misses;
however, line-rate packet forwarding requires deterministic latency
guarantees. Our design accomplishes this by never reading back a
value into the cache if it has already been evicted to the backing
store. Instead, it treats a cache miss as the arrival of a packet from a
new flow. When a flow is evicted, we merge the evicted flow’s value
in the cache with the flow’s old value in the backing store. Because
merges occur off the critical packet processing path, the backing
store can be implemented in software on a separate collection server.
While it is not always possible to merge an aggregation function
without losing accuracy, we characterize a class of affine aggregation
functions, which we call linear-in-state, for which accurate merging
is possible. Many useful aggregation functions are linear-in-state,
e.g., counters, predicated counters (e.g., count only TCP packets
that saw timeouts), exponentially weighted moving averages, and
functions computed over a finite window of packets. We design a
switch instruction to support linear-in-state functions, finding that it
easily meets timing at 1 GHz, while occupying modest silicon area.
Query compiler. We implement a compiler that takes Marple
queries and compiles them into switch configurations for two tar-
gets (§4): (1) the P4 behavioral model [19], an open source pro-
grammable software switch that can be used for end-to-end evalu-
ations of Marple on Mininet [47], and (2) Banzai [56], a simulator
for high-speed programmable switch hardware that can be used to
experiment with different instruction sets. The Marple compiler de-
tects linear-in-state aggregations in input queries and successfully
targets the linear-in-state switch instruction that we add to Banzai.
Evaluation. We show that Marple can express a variety of use-
ful performance monitoring examples, like detecting and localiz-
ing TCP incast and measuring the prevalence of out-of-order TCP
packets. Marple queries require between 4 and 11 pipeline stages,
which is modest for a 32-stage switch pipeline [33]. We evalu-
ate our key-value store’s performance using trace-driven simula-
tions. For a 64 Mbit on-chip cache, which occupies about 10% of
the area of a 64×10-Gbit/s switching chip, we estimate that the
cache eviction rate from a single top-of-rack switch can be han-
dled by a single 8-core server running Redis [20]. We evaluate
Marple’s usability through two Mininet case studies that use Marple
to troubleshoot high tail latencies [26] and measure the distribu-
tion of flowlet sizes [27]. Marple is open source and available at
http://web.mit.edu/marple.
Figure 1: Operators issue Marple queries, which are com-
piled into switch programs for programmable switches aug-
mented with our new programmable key-value store primitive.
Switches stream results from this query to collection servers
that also house the backing store for the key-value store.
monitor for a statistic (e.g., detecting TCP timeouts), or to trou-
bleshoot a specific problem (e.g., incast [61]) at hand. The query
is compiled into a switch program that runs on the network’s pro-
grammable switches, augmented with new switch hardware primi-
tives that we design in service of Marple. The switches stream results
out to collection servers, where the operator can retrieve query re-
sults. We now briefly describe the three components of our system:
the query language, the switch hardware, and the query compiler.
Performance query language. Marple uses familiar functional
constructs like map, filter, groupby and zip for performance mon-
itoring. Marple provides the abstraction of a stream that contains
performance information for every packet at every queue in the
network (§2). Programmers can focus their attention on traffic ex-
periencing interesting performance using filter (e.g., packets with
high queueing latencies), aggregate information across packets in
flexible ways using groupby (e.g., compute a moving average over
queueing latency per flow), compute new stateless quantities using
map (e.g., binning a packet’s timestamp into an epoch), and detect si-
multaneous performance conditions using zip (e.g., when the queue
depth is large and the number of connections in the queue is high).
Hardware design for performance queries. A naïve implemen-
tation of Marple might stream every packet’s metadata from the
network to a central location and run streaming queries against it.
Modern scale-out data-processing systems support 100K–1M oper-
ations per second per core [2, 4, 11, 43, 64], but processing every
single packet (assuming a relatively large packet size of 1 KB) from
a single 1 Tbit/s switch would need 100M operations per second —
2–3 orders of magnitude more than what existing systems support.
Instead, we leverage high-speed programmable switches [3, 13,
25, 33] as first-class citizens in network monitoring, because they
can programmatically manipulate multi-Tbit/s packet streams. Early
filtering and flexible aggregation on switches drastically reduce
Collection	servers	to	handle(1)	Marplequery	results(2)	Evictions	to	backing	storeMarpleQueriesMarpleCompilerSwitch	ProgramsProgrammable	switches	with	programmable	key-value	store	End	hostsEnd	hostsNetwork	operatorLanguage-Directed Hardware Design
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
Construct
pktstream
filter(R, pred)
map(R, [exprs],
[fields])
groupby(R,
[fields], fun)
zip(R, S)
Description
Stream of packet performance metadata.
Output tuples in R satisfying predicate pred.
Evaluate expressions, [exprs], over fields of R,
emitting tuples with new fields, [fields].
Evaluate function fun over the input stream R
partitioned by fields, producing tuples on emit().
Merge fields in incoming R and S tuples.
Figure 2: Summary of Marple language constructs.
2 THE MARPLE QUERY LANGUAGE
This section describes the Marple query language. §3 then cov-
ers the switch implementation of the language constructs, while §4
describes the compiler. Marple provides the abstraction of a network-
wide stream of performance information. The tuples in the stream
contain performance metadata, such as queue lengths and times-
tamps when a packet entered and departed queues, for each packet at
each queue in the network. Network operators write queries on this
stream as if the entire stream is processed by a single hypothetical
server running the query. In reality, the compiler partitions the query
across the network and executes each part on individual switches.
Marple programs process the performance stream using famil-
iar functional constructs (filter, map, groupby, and zip), all of
which take streams as inputs and produce a stream as output. This
functional language model is expressive enough to support diverse
performance monitoring use cases, but still simple enough to im-
plement in high-speed hardware. Marple’s language constructs are
summarized in Fig. 2.
Packet performance stream. As part of the base input stream,
which we call pktstream, Marple provides one tuple for each packet
at each queue with the following fields.
(switch, qid, hdrs, uid, tin, tout, qsize)
switch and qid denote the switch and queue at which the packet
was observed. A packet may traverse multiple queues even within
a single switch, so we provide distinct fields. The regular packet
headers (Ethernet, IP, TCP, etc.) are available in the hdrs set of fields,
with a uid that uniquely determines a packet.1
The packet performance stream provides access to a variety of
performance metadata: tin and tout denote the enqueue and de-
queue timestamps of a packet, while qsize denotes the queue depth
when a packet is enqueued. It is beneficial to have two timestamps to
detect co-habitation of the queue by packets belonging to different
flows. Additionally, it is beneficial to have a queue size, since we
cannot always determine the queue size from the two timestamps: a
link may service multiple queues, and the speed at which a queue
drains may not be known.
Tuples in pktstream are processed in order of packet dequeue
time (tout), since this is the earliest time at which all tuple fields
in pktstream are known.2 If a packet is dropped, tout and qsize
are infinity. Tuples corresponding to dropped packets may be
processed in an arbitrary order.
1It is usually possible to use a combination of the 5-tuple and IP ID field as the uid.
2We assume clock synchronization to let us compare tin and tout values from different
switches. Without synchronization, the programmer can still write queries that do not
compare time-valued fields tin and tout across switches.
Restricting packet performance metadata of interest. Con-
sider the example of tracking packets that experience high queueing
latencies at a specific queue (Q) and switch (S). This is expressed by
the query:
result = filter(pktstream, qid == Q and switch == S
and tout - tin > 1ms)
The filter operator restricts the user’s attention to those pack-
ets with the relevant performance metadata. A filter has the form