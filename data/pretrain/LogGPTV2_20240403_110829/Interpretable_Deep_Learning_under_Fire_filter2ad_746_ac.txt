interpretation desired by the adversary.
At a high level, we formulate ADV2 using the following
optimization framework:
(cid:26) f (x) = ct
g(x; f ) = mt
min
x
∆(x,x◦)
s.t.
(2)
where the constraints ensure that (i) the adversarial input is
misclassiﬁed as ct and (ii) it triggers g to generate the target
attribution map mt.
As the constraints of f (x) = ct and g(x; f ) = mt are highly
non-linear for practical DNNs, we reformulate Eqn (2) in a
form more suited for optimization:
(cid:96)prd( f (x),ct ) + λ(cid:96)int (g(x; f ),mt )
x
min
s.t. ∆(x,x◦) ≤ ε
(3)
where the prediction loss (cid:96)prd is the same as in Eqn (1), the
interpretation loss (cid:96)int measures the difference of adversarial
map g(x; f ) and target map mt, and the hyper-parameter λ
balances the two factors. Below we use (cid:96)adv(x) to denote the
overall loss function deﬁned in Eqn (3).
We construct the solver of Eqn (3) upon an adversarial at-
tack framework. While it is ﬂexible to choose the concrete
framework, below we primarily use PGD [35] as the refer-
ence and discuss the construction of ADV2 upon alternative
frameworks (e.g., spatial transformation [60]) in § 4.
Under this setting, we deﬁne (cid:96)prd( f (x),ct ) = −log( fct (x))
(i.e., the negative log likelihood of x with respect to the class
ct), ∆(x,x◦) = (cid:107)x − x◦(cid:107)∞, and (cid:96)int(g(x; f ),mt ) = (cid:107)g(x; f ) −
mt(cid:107)2
2. In general, ADV2 searches for x∗ using a sequence of
gradient descent updates:
x(i+1) = ΠBε(x◦)
(cid:0)x(i) − αsgn(cid:0)∇x(cid:96)adv
(cid:0)x(i)(cid:1)(cid:1)(cid:1)
(4)
However, directly applying Eqn (4) is often found inef-
fective, due to the unique characteristics of individual inter-
preters. In the following, we detail the instantiations of ADV2
against the back-propagation-, representation-, model-, and
perturbation-guided interpreters, respectively.
3.2 Back-Propagation-Guided Interpretation
This class of interpreters compute the gradient (or its vari-
ants) of the model prediction with respect to a given input to
derive the importance of each input feature. The hypothesis
is that larger gradient magnitude indicates higher relevance
of the feature to the prediction. We consider gradient saliency
(GRAD) [50] as a representative of this class.
Intuitively, GRAD considers a linear approximation of the
model prediction (probability) fc(x) for a given input x and a
given class c, and derives the attribution map m as:
(cid:12)(cid:12)(cid:12)(cid:12)∂ fc(x)
∂x
(cid:12)(cid:12)(cid:12)(cid:12)
m =
(5)
To attack GRAD-based IDLSes, we may search for x∗ using
a sequence of gradient descent updates as deﬁned in Eqn (4).
However, according to Eqn (5), computing the gradient of the
attribution map g(x; f ) amounts to computing the Hessian
matrix of fc(x), which is all-zero for DNNs with ReLU acti-
vation functions. Thus the gradient of the interpretation loss
(cid:96)int provides little information for updating x, which makes
directly applying Eqn (4) ineffective.
USENIX Association
29th USENIX Security Symposium    1661
We instantiate g with a DNN that concatenates the part of
f up to its last convolutional layer and a linear layer param-
eterized by {wk,c}. To attack CAM, we search for x∗ using a
sequence of gradient descent updates as deﬁned in Eqn (4).
This attack can be readily extended to other representation-
guided interpreters (e.g., GRADCAM [47]).
3.4 Model-Guided Interpretation
Instead of relying on deep representations at intermediate
layers, model-guided methods train a meta-model to directly
predict the attribution map for any given input in a single
feed-forward pass. We consider RTS [10] as a representative
method in this category.
For a given input x in a class c, RTS ﬁnds its attribution
map m by solving the following optimization problem:
minm λ1rtv(m) + λ2rav(m)− log ( fc (φ(x;m)))
s.t. 0 ≤ m ≤ 1
λ4
+λ3 fc (φ(x;1− m))
(8)
Here rtv(m) denotes the total variation of m, which reduces
noise and artifacts in m; rav(m) represents the average value
of m, which minimizes the size of retained parts; φ(x;m) is
the operator using m as a mask to blend x with random colors
and Gaussian blur, which captures the impact of retained parts
(where the mask is non-zero) on the model prediction; the
hyper-parameters {λi}4
i=1 balance these factors. Intuitively,
this formulation ﬁnds the sufﬁcient and necessary parts of x,
based on which f is able to make the prediction f (x) with
high conﬁdence.
However, solving Eqn (8) for every input during inference
is fairly expensive. Instead, RTS trains a DNN to directly
predict the attribution map for any given input, without ac-
cessing to the DNN f after training. In [44], this is achieved
by composing a ResNet [22] pre-trained on ImageNet [12]
as the encoder (which extracts feature maps of given inputs
at different scales) and a U-NET [44] as the masking model,
which is then trained to directly optimize Eqn (8). We con-
sider the composition of this encoder and this masking model
as the interpreter g.
To attack RTS, one may directly apply Eqn (4). However,
our evaluation shows that this strategy is often ineffective
for ﬁnding desirable adversarial inputs. This is explained by
that the encoder enc(·) plays a signiﬁcant role in generating
attribution maps, while solely relying on the outputs of the
masking model is insufﬁcient to guide the attack. We thus
add to Eqn (3) an additional loss term (cid:96)enc(enc(x),enc(ct )),
which measures the difference of the encoder’s outputs for
the adversarial input x and the target class ct.
We then search for the adversarial input x∗ with a sequence
of gradient descent updates deﬁned in Eqn (4). More imple-
mentation details are discussed in § 3.6.
Figure 3: Comparison of h(z), σ(z), and r(z) near z = 0.