PP
S
S
S
S
Pr
VPCD(vkPCD,z,π)=1
out(T)(cid:6)=z or CN (T)=0
(pS,skPP)←GS(1λ)
σi←SS(skPP,Ii),1≤i≤r
(pkPCD,vkPCD)←GPCD(CN ,1λ)
(z,π)←APCD(pkPCD,vkPCD,a)
T←EPCD(pkPCD,vkPCD,a)
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
≤negl(λ)
Now, we deﬁne the extractor E as the algorithm that (a)
when A outputs an image and a digital signature, outputs
the same image and signature and (b) when A outputs an
image and a proof, invokes E
PCD and reads off the permissible
provenance and signature from the output transcript’s graph
labels. So the above probability implies:
(pkPP,vkPP,skPP)←GPP(1N ,1λ)
σi←SS(skPP,Ii),1≤i≤r
(I,π)←A(pkPP,vkPP,(Ii,σi)i)
(e,σ)←E(pkPP,vkPP,(Ii,σi)i)
⎤
⎥⎥⎥⎥⎥⎥⎥⎦≤negl(λ)
Pr
VPCD(vkPCD,z,π)=1
DT (vkPP,z,e,σ)=0
PP
PCD
PCD (vk
, I, π) = V
, z, π) and Eq. 5.
(5)
Now, Eq. 1 follows by splitting it into two cases, according
to the proof that A outputs. The case of a signature triv-
ially holds. For the case of a PCD proof, it follows from
PP (vk
V
Statistical zero-knowledge. We need to show a polynomial-
time stateful simulator SPP such that for every stateful distin-
guisher DPP the probabilities in Eq. 2 and 3 are negligibly
close. By the statistical zero-knowledge of the underlying
PCD, there exists a simulator S
PCD such that for every distin-
PCD that (given 1λ) outputs some compliance predi-
guisher D
(cid:2), and given proving and veriﬁcation keys outputs some
cate Π
((cid:6)zin, (cid:6)πin, l, zout), D
PCD cannot distinguish between a PCD-
PCD-generated proof for zout with more than
generated or a S
negligible probability. That is, the following two probabilities
are negligibly close (in λ):
⎤
⎡
⎢⎢⎢⎣
⎡
⎢⎢⎢⎣
Pr
Pr
Π(cid:3)((cid:7)zin,l,π)=1
VPCD(vkPCD,(cid:7)zin,(cid:7)πin)=1
DPCD(π)=1
Π(cid:3)((cid:7)zin,l,π)=1
VPCD(vkPCD,(cid:7)zin,(cid:7)πin)=1
DPCD(π)=1
(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)
(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)
Π(cid:3)←DPCD(1λ)
(pkPCD,vkPCD)←GPCD(Π(cid:3),1λ)
((cid:7)zin,(cid:7)πin,l,zout)←DPCD(pkPCD,vkPCD)
π←PPCD(pkPCD,(cid:7)zin,(cid:7)πin,l,zout)
Π(cid:3)←DPCD(1λ)
(pkPCD,vkPCD)←SPCD(Π(cid:3),1λ)
((cid:7)zin,(cid:7)πin,l,zout)←DPCD(pkPCD,vkPCD)
π←SPCD(zout)
⎥⎥⎥⎦
⎤
⎥⎥⎥⎦
This PCD simulator S
the
IA simulator SPP needed to show the IA’s statistical zero-
knowledge. Let SPP be the following simulator: when invoked
PCD is then used to construct
263263
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:13:41 UTC from IEEE Xplore.  Restrictions apply. 
(cid:2)
1N , 1λ
(cid:3)
PCD
⎡
PP and P
PCD it calls S
it runs a modiﬁed version of G
as SPP
PP (Algo-
PCD(CN , 1λ);
rithm 2) where instead of calling G
when later invoked as SPP (I) it simply runs and outputs
PCD (I).
S
To see that this simulator succeeds, ﬁrst note that the dis-
tinguisher in the IA zero-knowledge deﬁnition is weaker than
that of the PCD zero-knowledge deﬁnition, since the former
is limited to choosing only N to determine the compliance
predicate, and can control only Iin,πin,t and γ (and not
Iout). Thus, from DPP one can easily construct a distinguisher
D
the requisite fragments of the PhotoProof algorithms so
that it presents a PCD interface instead of the more limited
IA interface. Formally, when we expand G
PP Eq. 2
becomes:
⎤
PCD for the PCD zero-knowledge, by tacking on to D
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
VPP(vkPP,Iin,πin)=1
1N←DPP(1λ)
(pS,sS)←GS(1λ)
CN =C(ΠT ,1N )
(pkPCD,vkPCD)←GPCD(CN ,1λ)
(pkPP,vkPP,skPP)←(pkPCD||pS,vkPCD||pS,sS)
(Iin,πin,t,γ)←DPP(pkPP,vkPP,skPP)
in,l,z)←FD(Iin,πin,t,γ)
in,l,z)
where FD (Iin, πin, t, γ) contains the steps of P
PP which
convert a signature to a PCD proof when necessary and output
the data in its PCD form (i.e., as messages and local data).
Now we deﬁne D
PCD by repacking the extra steps before/after
D
PP’s execution, to be left only with the PCD interface, and
obtaining:
⎡
(zin,π(cid:3)
(π)←PPCD(pkPCD,zin,π(cid:3)
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)
DPP(π)=1
t∈T
⎤
Pr
Pr
CN (zin,l,z)=1
VPP(vkPP,Iin,πin)=1
DPCD(π)=1
CN←DPCD(1λ)
(pkPCD,vkPCD)←GPCD(CN ,1λ)
in,l,z)←DPCD(pkPCD,vkPCD)
π←PPCD(pkPCD,zin,π(cid:3)
in,l,z)
⎥⎥⎥⎦
(zin,π(cid:3)
Using the same reasoning, Eq. 3 becomes:
⎢⎢⎢⎣
⎡
⎢⎢⎢⎣
(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)
(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)
Pr
CN (zin,l,z)=1
VPP(vkPP,Iin,πin)=1
DPCD(π)=1
CN←DPCD(1λ)
(pkPCD,vkPCD)←SPCD(CN ,1λ)
in,l,z)←DPCD(pkPCD,vkPCD)
π←SPCD(pkPCD,zin,π(cid:3)
in,l,z)
(zin,π(cid:3)
⎤
⎥⎥⎥⎦
PP is the same as V
We now split to cases according to πin. When πin is a PCD
proof, V
PCD, and so we are left with the exact
probabilities from the PCD deﬁnition. When πin is a digital
, Iin, πin) = 1
signature, then V
and whenever this holds, then the “conversion” to PCD suc-
ceeds, i.e., V
in) = 1. From PCD soundness,
the opposite holds too except for negligible probability.
, Iin, πin) = 1 iff V
, zin, π(cid:2)
PCD (vk
PP (vk
S (p
PCD
PP
S
Thus, the distinguishing advantage in IA (difference be-
tween Eq. 2 and 3) for SPP and DPP is the same as the dis-
tinguishing advantage in the PCD zero-knowledge deﬁnition
for S
PCD up to a negligible difference and the latter
advantage is negligible, by deﬁnition of S
PCD and D
PCD.
C. Strengthening proof-of-knowledge
Proof-of-knowledge of IA schemes lets adversaries see sig-
natures under S for any series of images (of polynomial size).
Note, though, that these images are chosen before pk
IA and
sk
IA are generated. A stronger notion of proof-of-knowledge
would be to consider adversaries with a signing oracle, so
they can query it for images that are adaptively chosen after
the system keys are generated.
IA,vk
To see why this deﬁnition is stronger than the one we used,
consider an artiﬁcially weakened version of our PhotoProof
construction, where the veriﬁer algorithm is added a “trap-
door” rule, to accept any image that is given along with a
proof π which contains a valid signature on the proving key
, π) = 1). This is of course undesirable, and
(i.e.,V
indeed this artiﬁcial construction does not fulﬁll the stronger
notion of proof-of-knowledge. However, it does fulﬁll the
original IA deﬁnition, since the adversary there cannot ask
for signatures after seeing pk
IA, and thus cannot produce an
image with the “trapdoor” signature.
S (p
, pk
IA
S
The problem with the stronger deﬁnition is that the PCD
proof-of-knowledge does not guarantee anything against ad-
versaries with oracle access. In a recent work, Fiore and Nit-
ulescu [18] study the security of SNARK proof-of-knowledge
in scenarios where adversaries are given access to ora-
cles. They too suggest a non-addaptive notion of proof-of-
knowledge very similar to our IA proof-of-knowledge.
V. IMPLEMENTATION
A. System description
We implemented a PhotoProof prototype for a set of
permissible transformations including identity, (arbitrary) rect-
angular crop, horizontal and vertical ﬂip, transpose, general
brightness/contrast adjustments and rotation. We also demon-
strated the protection of metadata ﬁelds by adding protected
timestamp data to our images. Although fully operational, our
implementation is still more of a proof-of-concept than a real-
world-ready system, due to relatively long proving times and
large proving keys for small images. We review our prototype’s
performance in Section V-B, and list some ideas and directions
for making the system usable in practice in Sections V-G
and VI.
Our implementation makes use of libsnark[47], a C++
library implementing SNARKs and PCD. It contains the pre-
processing PCD for Fp-R1CS of Ben-Sasson et al. [7], using
the SNARK scheme of Gennaro et al. [22]). The libsnark
library creates proofs for computation (expressed Fp-R1CS)
over a large prime ﬁeld Fp (chosen from a set offered by
libsnark, corresponding to its supported elliptic curves), and
offers a gadget library for expressing such computation (see
Section III for more details).
At the heart of our implementation lies a collection of
gadgets which form an image processing tool-box expressed
in R1CS. Using them, we implement a gadget for each
permissible transformation we support. All these gadgets are
then combined to create the compliance predicate gadget,
264264
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:13:41 UTC from IEEE Xplore.  Restrictions apply. 
which, given a maximal image size N, generates a R1CS
which computes the compliance predicate on images of size
up to N (and ﬁxed size local data).
Our code has 4 levels (see Figure 3). At the higher level is
Python code that handles image processing and user interface.
It makes use of Pillow, the Python Imagining Library fork
[38]. It also handles serializing images and parameters before
sending to the next level. The second level supplies a C++
wrapper for libsnark functionality. Level 3 contains the PCD
implementation of [7]. Level 4 is the PhotoProof gadget
library. For example, to edit an image and generate a new
proof, the user uses the Python prover function (level 1). This
function applies the transformation on the image, and then
serializes all input: hashes, images with metadata, input proof,
transformation, parameters and other data. It then invokes the
C++ executable containing the prover wrapper (level 2). This
code takes all the input and prepares an assignment for the
input variables to the compliance predicate. It then calls level
4 to generate the witness to the compliance predicate constraint
system. Finally, it invokes libsnark’s PCD prover (level 3)
on the witness and the input proof to generate the new proof,
which it outputs back to the Python code, where it is output
to the user.
Our implementation does not include a secure camera. To
simulate original camera images, we use the generated secret
signing key to sign our “original” images. The secret key is,
of course, not used in any other part of the system. For digital
signatures we use an ECDSA library [58] with a NIST192p
curve (about 80 bits of security).
B. Performance
We ran our prototype on images of N × N pixels for
various N values and measured the average running time of
the generator, prover and veriﬁer, as well as the sizes of the
keys and the proofs. Our benchmark machine was a desktop
with a 4-core 3.4GHz Intel i7-4770 processor and 32GB of
RAM. Our results are summarized in Table II. The security
of our scheme is guaranteed by PCD security, and indeed, for
all the images and transformations we ran our prototype on
(hundreds of executions), no completeness or soundness errors
were recorded. Every illegal change of an image, even of a
single bit, is detected by the scheme.
As expected, generation and proving times are much slower
than veriﬁcation times. One thing to keep in mind when
assessing overall performance is that generation takes place
only once in the lifetime of the system, by some trusted,
preferably high-performance server; proving is done only once
per edit, and can be delegated to (untrusted) cloud servers.
Veriﬁcation, to be done by all viewers of the image, is fast.
The reported proving times in Table II assume that the
proving key pk
PP is preloaded into RAM. This holds, e.g.,
for scenarios where proving is done by a dedicated server or
by a software plugin that loads the key into memory upon
startup.
The measured proving times are per-edit, and indepen-
dent of which transformation is applied on the image (as
every proving step proves compliance with a predicate that
“considers” all permissible transformations). When performing
multiple edit steps (e.g., crop and then rotate) an additional
proving step is needed for each transformation applied. In
Section V-G we discuss using PCD with multiple compliance
predicates, thus shortening the proving time per edit step by
considering only the applied transformation.
Although 128 × 128 images are too small for practical