title:A framework for efficient evaluation of the fault tolerance of deduplicated
storage systems
author:Eric Rozier and
William H. Sanders
A Framework for Efﬁcient Evaluation of the Fault Tolerance of Deduplicated
Storage Systems
Eric William Davis Rozier, William H. Sanders
Coordinated Science Laboratory
University of Illinois at Urbana-Champaign
Urbana, Illinois, USA
{erozier2,whs}@illinois.edu
Abstract—In this paper we present a framework for ana-
lyzing the fault tolerance of deduplicated storage systems. We
discuss methods for building models of deduplicated storage
systems by analyzing empirical data on a ﬁle category basis.
We provide an algorithm for generating component-based
models from this information and a speciﬁcation of the storage
system architecture. Given the complex nature of detailed
models of deduplicated storage systems, ﬁnding a solution using
traditional discrete event simulation or numerical solvers can
be difﬁcult. We introduce an algorithm which allows for a
more efﬁcient solution by exploiting the underlying structure
of dependencies to decompose the model of the storage system.
We present a case study of our framework for a real system. We
analyze a production deduplicated storage system and propose
extensions which improve fault tolerance while maintaining
high storage efﬁciency.
Keywords-storage, deduplication, reliability, simulation, de-
composition
I. INTRODUCTION
Modern storage systems have become increasingly large
and complex, both because of the growth of user data, and
in response to recent legislation mandating the length of
time data must be stored for retrieval. In 2002, over ﬁve
exabytes of data were produced [1], representing an increase
of 30% from 2001. By 2007 the ﬁgure had increased to
281 exabytes, 10% more than expected because of faster
growth in cameras, digital TV shipments, and other media
sectors [2]. In 2010, the total data produced passed the
zettabyte barrier. Forecasts put the total size of stored data
for 2011 at 10 times the size for 2006, roughly 1.8 zettabytes
[3]. Storing this data has become increasingly problematic.
In 2007, as forecast, the amount of data created exceeded
available storage for the ﬁrst time [2].
In order to reduce the footprint of backup and archival
storage, system architects have begun using a new method
to improve storage efﬁciency, called data deduplication. At
a high level, data deduplication is a method for eliminating
redundant data in a storage system to improve storage
efﬁciency. Sub-ﬁle segments are ﬁngerprinted and compared
to a data base of identiﬁed segments to ﬁnd duplicate data.
Duplicate data is then replaced with references to the stored
instances.
This paper presents a framework for evaluating the fault
tolerance of large-scale systems which utilize deduplication.
The framework supports the analysis of existing deduplica-
tion storage proﬁles in order to build accurate models of the
relationships between deduplicated ﬁles as these relation-
ships can differ enough, even between categories of ﬁles on
a single storage system, to shift the impact of deduplication
on fault tolerance from positive to negative. Additionally
this framework uses knowledge of the dependence relation-
ships caused by deduplication and RAID to dynamically
decompose the system model, based on the reward variables
deﬁned over the system, to improve solution efﬁciency.
Our framework consists of several parts:
• A set of component-based models of the underlying
storage system.
• A model of deduplication relationships in the storage
system, generated stochastically using empirical data
from a real storage system.
• A method for identifying dependence relationships in
the resulting model, important events which temporarily
remove or return dependence relationships in a model
(faults, fault propagation, and fault mitigation), and a
method to automatically decompose a model based on
this information to improve the efﬁciency of model
solution.
Our framework formalizes the techniques presented in [4]
for a 7TB system. In this paper we apply our framework
to a model of a one petabyte storage system based on the
analysis of additional data from a system similar to that
presented in [4] but with a different user base, and thus
different characteristics. We present strategies for improving
reliability by storing additional copies of deduplicated ﬁles
for a subset of the system, and show that while for some
categories, deduplication has a negative impact on reliability,
for others the impact is positive, demonstrating previous
predictions made in [4].
A. Related Work
The cost of deduplication in terms of performance [5],
[6] is well understood. Reliability studies have been much
fewer in number. Since traditional deduplication keeps only
a single instance of redundant data, deduplication has the
potential to magnify the negative impact of losing a data
chunk [7], [8] However, due to the smaller number of disks
required to store deduplicated data, deduplication also has
the potential to improve reliability as well. Administrators
and system architects have found understanding the data re-
liability of their system under deduplication to be important
but extremely difﬁcult [9].
Quantitative modeling of reliability in a deduplication
system is challenging, even without taking into account the
petabyte scale of storage systems. First, there are different
types of faults in a storage system, including whole disk
failures [10], latent sector errors (LSEs) [11], [12], and
undetected disk errors [13], [14], [15]. Second, these faults
can propagate due to the sharing of data chunks or chaining
of ﬁles in a deduplication system. In order to correctly under-
stand the impacts of these faults and their consequences on
the reliability of our storage system, we need to accurately
model both storage system faults and faults due to data
deduplication. Third, it is important to note that many of the
faults we wish to consider are rare compared to other events
in the system, such as disk scrubbing, disk rebuilds, and I/O.
Calculating the impact from rare events in a system can be
computationally expensive, motivating us to ﬁnd efﬁcient
ways of measuring their effect on the reliability metrics of
interest.
The complexity of this problem arises from two different
causes. The ﬁrst is the state-space explosion problem which
can make numerical solution difﬁcult. A second issue comes
from the stiffness that results from rare events. For numerical
solution stiffness introduces numerical instability, making
solution impractical. When simulating, stiffness increases
the number of events we must process, causing a resulting
increase in simulation complexity.
B. Organization
This paper is organized as follows: Section II introduces
the modeling formalism used for our framework; Section III
discusses the models we use for disks, reliability groups,
and data deduplication relationships; Section V discusses
data dependence relationships and introduces the notion of
a model dependence graph (MDG); Section VI discusses
methods for automatically identifying key events in an
MDG; Section VII uses the methods described in Sections
V and VI to decompose a model, while preserving reward
variable relationships; Section VIII discusses solution meth-
ods for the decomposed model, and proves the preservation
of reward metrics by these solution methods; and ﬁnally
Section IX presents the application of these methods to our
system of interest and discusses the results, and Section X
concludes the paper.
II. BACKGROUND
We present our method in the context of a generic model
speciﬁcation language based on the notation presented in
[16]. This is intended as an alternative to presenting our re-
sults in a speciﬁc formalism, both to simplify the discussion
of our techniques and to generalize our methods.
Deﬁnition 1. A model is a 5-tuple (S, E, Φ, Λ, ∆)
take values in N.
occur in the model.
• S is a ﬁnite set of state variables {s1, s2, . . . , sn} that
• E is a ﬁnite set of events, {e1, e2, . . . , em} that may
• Φ : E × N1 × N2 × . . . × Nn → {0, 1} is the event-
• Λ : E × N1 × N2 × . . .× Nn → (0,∞) is the transition
• ∆ : E × N1 × N2 × . . . × Nn → N1 × N2 × . . . × Nn
is the state variable transition function speciﬁcation.
enabling function speciﬁcation.
rate function speciﬁcation.
We represent this formalism visually using circles for
state variables, boxes for events, and arcs to represent the
dependence of the functions Φ, Λ, and ∆ on state variables
and events.
In addition to specifying a model of system, one must
specify the performability, availability, or dependability mea-
sures for a model. These measures are speciﬁed in terms of
reward variables [17]. Reward variables are speciﬁed as a
reward structure [18] and a variable type.
Deﬁnition 2. Given model M = (S, E, Φ, Λ, ∆), we deﬁne
two reward structures: rate rewards and impulse rewards.
• A rate reward is deﬁned as a function R : P(S, N) →
R, where for q ∈ P(S, N), R(q) is the reward accu-
mulated when for each (s, n) ∈ q the marking of s is
n.
• An impulse reward is deﬁned as a function I : E →
R, where for e ∈ E,I(e) is the reward earned upon
completion of e.
where P(S, N) is the set of all partial functions between
S and N.
Deﬁnition 3. Let ΘM = {θ0, θ1, . . .} be a set of reward
variables, each with reward structure R or I associated
with a model M.
The type of a reward variable determines how the reward
structure is evaluated, and can be deﬁned over an interval
of time, an instant of time, or in steady state, as shown in
[19], [17].
A. Instant-of-Time Variables
We refer to variables which are used to measure the
behavior of a model at a particular time t as instant-of-time
variables [20], [17]. Such a variable, θ(t) is deﬁned as:
θt = (cid:88)
ν∈P(S,N)
t +(cid:88)
e∈E
R(ν) · I ν
I(e) · I e
t
(1)
where
• I ν
t is an indicator random variable which represents the
instance of a marking such that for each (s, n) ∈ ν, the
state variable s has a value of n at time t.
t is an indicator random variable which represents the
instance of an event e that has ﬁred most recently at
time t.
• I e
B. Interval-of-Time Variables
In order to calculate metrics which accumulate over some
ﬁxed interval of time, we use interval-of-time variables. Such
variables accumulate reward during some interval of time,
and take on the value of the total reward for the deﬁned
period [20], [17]. Given such a variable, θ[t,t+l], we deﬁne
it as:
I(e)N e
[t,t+l]
(2)
θ[t,t+l] = (cid:88)
ν∈P(S,N)
[t,t+l] +(cid:88)
e∈E
R(ν)J ν
where
• J ν
[t,t+l] is a random variable which represents the total
time the model spent in a marking such that for each
(s, n) ∈ ν, the state variable s has a value of n during
the period [t, t + l].
t→∞ is an random variable which represents the
number of times an event e has during the period
[t, t + l].
• I e
C. Time-Averaged Interval-of-Time Variables
The ﬁnal
type of reward variables we will consider
are time-averaged interval-of-time variables. These variables
quantify accumulated reward averaged over some interval of
time [20], [17]. Given such a variable, θ(cid:48)
[t,t+l] we deﬁne it
as:
θ(cid:48)
[t,t+l] =
θ[t,t+l]
l
(3)
III. SYSTEM MODELS
A. Empirical Analysis of Deduplicated Storage System
In order to build a model of our deduplicated storage
system, we examined deduplicated data stored in an enter-