# A Framework for Efficient Evaluation of the Fault Tolerance of Deduplicated Storage Systems

**Authors:**
- Eric William Davis Rozier
- William H. Sanders

**Affiliation:**
- Coordinated Science Laboratory, University of Illinois at Urbana-Champaign, Urbana, Illinois, USA
- Email: {erozier2, whs}@illinois.edu

## Abstract
In this paper, we present a framework for analyzing the fault tolerance of deduplicated storage systems. We discuss methods for building models of deduplicated storage systems by analyzing empirical data on a file category basis. We provide an algorithm for generating component-based models from this information and a specification of the storage system architecture. Given the complexity of detailed models of deduplicated storage systems, finding a solution using traditional discrete event simulation or numerical solvers can be challenging. We introduce an algorithm that allows for a more efficient solution by exploiting the underlying structure of dependencies to decompose the model of the storage system. We present a case study of our framework applied to a real system, where we analyze a production deduplicated storage system and propose extensions that improve fault tolerance while maintaining high storage efficiency.

**Keywords:** storage, deduplication, reliability, simulation, decomposition

## 1. Introduction
Modern storage systems have become increasingly large and complex, driven by the growth of user data and recent legislation mandating longer data retention periods. In 2002, over five exabytes of data were produced, representing a 30% increase from 2001. By 2007, this figure had grown to 281 exabytes, 10% more than expected, due to rapid growth in digital media sectors such as cameras and digital TV shipments [2]. In 2010, the total data produced surpassed the zettabyte barrier, with forecasts predicting that the total size of stored data in 2011 would be ten times that of 2006, approximately 1.8 zettabytes [3]. Storing this vast amount of data has become increasingly problematic. In 2007, the amount of data created exceeded available storage for the first time [2].

To reduce the footprint of backup and archival storage, system architects have begun using data deduplication, a method for eliminating redundant data to improve storage efficiency. At a high level, data deduplication involves fingerprinting sub-file segments and comparing them to a database of identified segments to find duplicates. Duplicate data is then replaced with references to the stored instances.

This paper presents a framework for evaluating the fault tolerance of large-scale systems that utilize deduplication. The framework supports the analysis of existing deduplication storage profiles to build accurate models of the relationships between deduplicated files. These relationships can vary significantly, even within categories of files on a single storage system, potentially shifting the impact of deduplication on fault tolerance from positive to negative. Additionally, the framework uses knowledge of the dependence relationships caused by deduplication and RAID to dynamically decompose the system model, improving solution efficiency based on defined reward variables.

Our framework consists of several key components:
- A set of component-based models of the underlying storage system.
- A model of deduplication relationships in the storage system, generated stochastically using empirical data from a real storage system.
- A method for identifying dependence relationships in the resulting model, important events that temporarily remove or return dependence relationships (e.g., faults, fault propagation, and fault mitigation), and a method to automatically decompose the model to improve solution efficiency.

We formalize the techniques presented in [4] for a 7TB system and apply our framework to a one-petabyte storage system based on additional data from a system with different characteristics. We present strategies for improving reliability by storing additional copies of deduplicated files for a subset of the system and show that while deduplication has a negative impact on reliability for some categories, it has a positive impact for others, confirming previous predictions made in [4].

### 1.1 Related Work
The performance cost of deduplication is well understood [5, 6], but studies on its reliability are fewer. Traditional deduplication keeps only a single instance of redundant data, which can magnify the negative impact of losing a data chunk [7, 8]. However, the smaller number of disks required for deduplicated data can also improve reliability. Administrators and system architects find it important but difficult to understand the data reliability of their systems under deduplication [9].

Quantitative modeling of reliability in deduplication systems is challenging, even without considering the petabyte scale of storage systems. Different types of faults in a storage system include whole disk failures [10], latent sector errors (LSEs) [11, 12], and undetected disk errors [13, 14, 15]. These faults can propagate due to the sharing of data chunks or chaining of files in a deduplication system. To correctly understand the impacts of these faults on the reliability of the storage system, it is necessary to accurately model both storage system faults and faults due to data deduplication. Many of the faults we consider are rare compared to other events in the system, such as disk scrubbing, disk rebuilds, and I/O. Calculating the impact of rare events can be computationally expensive, motivating the need for efficient methods to measure their effect on reliability metrics.

The complexity of this problem arises from two main causes: the state-space explosion problem, which makes numerical solutions difficult, and stiffness from rare events, which introduces numerical instability and increases simulation complexity.

### 1.2 Organization
This paper is organized as follows:
- **Section 2** introduces the modeling formalism used for our framework.
- **Section 3** discusses the models we use for disks, reliability groups, and data deduplication relationships.
- **Section 4** discusses data dependence relationships and introduces the notion of a model dependence graph (MDG).
- **Section 5** discusses methods for automatically identifying key events in an MDG.
- **Section 6** uses the methods described in Sections 4 and 5 to decompose a model while preserving reward variable relationships.
- **Section 7** discusses solution methods for the decomposed model and proves the preservation of reward metrics by these solution methods.
- **Section 8** presents the application of these methods to our system of interest and discusses the results.
- **Section 9** concludes the paper.

## 2. Background
We present our method in the context of a generic model specification language based on the notation presented in [16]. This approach simplifies the discussion of our techniques and generalizes our methods.

### 2.1 Model Definition
A model is defined as a 5-tuple \((S, E, \Phi, \Lambda, \Delta)\):
- \(S\) is a finite set of state variables \(\{s_1, s_2, \ldots, s_n\}\) that take values in \(\mathbb{N}\).
- \(E\) is a finite set of events \(\{e_1, e_2, \ldots, e_m\}\) that may occur in the model.
- \(\Phi: E \times \mathbb{N}^n \rightarrow \{0, 1\}\) is the event-enabling function.
- \(\Lambda: E \times \mathbb{N}^n \rightarrow (0, \infty)\) is the transition rate function.
- \(\Delta: E \times \mathbb{N}^n \rightarrow \mathbb{N}^n\) is the state variable transition function.

We represent this formalism visually using circles for state variables, boxes for events, and arcs to represent the dependence of the functions \(\Phi\), \(\Lambda\), and \(\Delta\) on state variables and events.

### 2.2 Reward Variables
In addition to specifying a model, one must specify the performability, availability, or dependability measures for the model. These measures are specified in terms of reward variables [17], which are defined as a reward structure and a variable type.

#### 2.2.1 Rate Rewards
A rate reward is defined as a function \(R: P(S, \mathbb{N}) \rightarrow \mathbb{R}\), where for \(q \in P(S, \mathbb{N})\), \(R(q)\) is the reward accumulated when for each \((s, n) \in q\), the marking of \(s\) is \(n\).

#### 2.2.2 Impulse Rewards
An impulse reward is defined as a function \(I: E \rightarrow \mathbb{R}\), where for \(e \in E\), \(I(e)\) is the reward earned upon completion of \(e\).

### 2.3 Types of Reward Variables
- **Instant-of-Time Variables**: Measure the behavior of a model at a particular time \(t\). Such a variable \(\theta(t)\) is defined as:
  \[
  \theta_t = \sum_{\nu \in P(S, \mathbb{N})} R(\nu) \cdot I_{\nu}^t + \sum_{e \in E} I(e) \cdot I_e^t
  \]
  where \(I_{\nu}^t\) is an indicator random variable representing the instance of a marking, and \(I_e^t\) is an indicator random variable representing the instance of an event \(e\) that has fired most recently at time \(t\).

- **Interval-of-Time Variables**: Accumulate reward during a fixed interval of time. Such a variable \(\theta[t, t+l]\) is defined as:
  \[
  \theta[t, t+l] = \sum_{\nu \in P(S, \mathbb{N})} R(\nu) \cdot J_{\nu}[t, t+l] + \sum_{e \in E} I(e) \cdot N_e[t, t+l]
  \]
  where \(J_{\nu}[t, t+l]\) represents the total time the model spent in a marking, and \(N_e[t, t+l]\) represents the number of times an event \(e\) has occurred during the period \([t, t+l]\).

- **Time-Averaged Interval-of-Time Variables**: Quantify accumulated reward averaged over some interval of time. Such a variable \(\theta'[t, t+l]\) is defined as:
  \[
  \theta'[t, t+l] = \frac{\theta[t, t+l]}{l}
  \]

## 3. System Models
### 3.1 Empirical Analysis of Deduplicated Storage System
To build a model of our deduplicated storage system, we examined deduplicated data stored in an enterprise environment. We analyzed the data on a file category basis to understand the deduplication relationships and their impact on fault tolerance.