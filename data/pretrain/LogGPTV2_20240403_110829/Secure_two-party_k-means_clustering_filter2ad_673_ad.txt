n
nXi=1
Di =
DA
i +
(4)
1
n  nXi=1
i !
DB
nXi=1
Since Bob has Alice’s encrypted data, Bob can locally com-
pute (encryptions) of the above sums, and return a (ran-
domized) share of the sum to Alice. They may then run the
Division Protocol to share C.
B. Distance to Center of Gravity. Bob can perform all the
computations outlined in the Distance Protocol on the en-
is
the distance (squared) between C and Di. He randomizes
this encryption and returns it to Alice, so that they share
crypted data to obtain an encryption of eC 0
eC 0
i = eC A,0
C. Average Squared Distance. Deﬁne the following sums:
i , where eC 0
i + eC B,0
for each i.
i
i
P :=
and
P ′ :=
nXi=1 eC A,0
i
,
nXi=1 eC B,0
i
which Alice and Bob can compute locally. They then run
the DivP on the inputs P and P ′, and D := n. As output,
Alice and Bob will be sharing ¯C as desired.
D. Pick First Cluster Center. Notice that picking a data
¯C+ eC 0
point Di with probability
2n ¯C is equivalent to picking a
random number R ∈ [0..2n ¯C − 1] and ﬁnding the ﬁrst i such
j . We use this observation to pick
i
j=1
data points according to weighted probabilities as follows:
that R ≤ Pi
¯C + eC 0
1. Picking a Random R. In this step, Alice and Bob pick
a random number in [0..2n ¯C −1], where 2n ¯C = 2n ¯C A+
2n ¯C B . Alice and Bob run the Random Value Protocol
(RVP) with Q := 2n ¯C = 2n ¯C A + 2n ¯C B to generate
and share a random number R = RA + RB ∈ Z2n ¯C .
2. Alice and Bob will next compare their random number
j , and ﬁnd the ﬁrst i such
j . They will then set µ1 = Di.
The actual implementation of this can be found in the
Choose µ1 Protocol in Appendix B.
R with the sumPi
that R ≤Pi
¯C +eC 0
¯C + eC 0
j=1
j=1
B. Calculating the New Cluster Centers. The following will
be done for each cluster 1 ≤ j ≤ k. We break the calculation
into three steps: In Step 1, Bob will compute the sum of
data points in cluster j, in Step 2 he will compute the total
number of points in cluster j, and in Step 3 the result of
Step 1 will be divided by the result of Step 2. To simplify
the notation, by E(Ci) we will mean (E(Ci,1), . . . , E(Ci,d)).
1. Sum of Data Points in Cluster j.
In this step, Bob
will compute the sum of all data points in cluster j.
We denote this sum as:
E. Iterate to Pick the Remaining Cluster Centers.
Sj ∈ Zd
N =
1. This step is done analogously to Step I.B.
i }j−1
Therefore, Alice and Bob run the FM2NP on inputs
) so that they share the
2. This step is supposed to calculate the minimum of
l=0 . However, they don’t have to take the mini-
mum over all j numbers, since from the previous itera-
i}j−2
l=0 .
Thus, they really only need to take a minimum of two
{eC l
tion of this step, they already have eCi = Min{eC l
numbers, that is reset eCi to be:
eCi = Min{eCi, eC j−1
(eC A
i , eC A,j−1
) and (eC B
i , eC B,j−1
location of (the new) eCi (let L = LA + LB denote
this location). They can then share the new eCi =
Min{eCi, eC j−1
i , eC A,j−1
, LA) and y = (eC B
(eC A
tion f (x, y) = LeC j−1
i + (1 − L)eCi.
} by running the SPP on inputs x =
, LB) and func-
3. This step is done analogously to Step I.C.
i , eC B,j−1
}.
i
i
i
i
i
i
4. This step is done analogously to Step I.D.
4.3.2 Step II: Lloyd Step
In this section, we discuss how to implement the Lloyd
Step while maintaining privacy protection.
A. Finding the Closest Cluster Centers. Alice and Bob re-
peat the following for each Di ∈ D:
1. Find the Distance (squared) to Each Cluster Center.
Note that because ﬁnding the minimum of all distances
is equivalent to ﬁnding the minimum of the distances
squared, we will calculate the latter. Since Bob has
(encryptions of) Alice’s shares of the data points and
the cluster centers, Bob can go through the computa-
tions of the DistP to obtain for each cluster center j
the (encrypted) distance Xi,j of data point Di to clus-
ter center j. As usual, Bob randomizes each distance
and returns them to Alice, so that for each j, Alice
and Bob share the vector Xi = (Xi,1, . . . Xi,k).
2. Alice and Bob run the Find Minimum of k Numbers
Protocol (FMkNP) on XA
i to obtain a share of
(a vector representation of) the location of the closest
cluster center to Di:
i and XB
Ci := (0, . . . , 0, 1, 0, . . . , 0),
(6)
where the 1 appears in the jth coordinate if cluster
center µj is closest to Di. Note that in actuality, Ci
is shared between Alice and Bob:
i + CB
i ,
Ci = CA
and Alice encrypts her share and sends this to Bob.
nXi=1(Di,
0,
if Di ∈ cluster j
O.W.
j +SB
j (here the addition is in Zd
At the end of this step, Alice and Bob will share Sj =
SA
N ). Recall from Step
A above that for each data point Di, Bob has E(CA
i )
and CB
i = Ci = (0, . . . , 0, 1, 0, . . . , 0),
where the 1 appears in the mth cluster if Di is closest
to cluster m. Therefore, for cluster j we would like to
sum:
i , where:
i + CB
CA
Sj =
Ci,j Di.
nXi=1
Utilizing the homomorphic and single multiplication
properties of E, Bob can compute (an encryption) of
Sj , returning a randomized share to Alice so that they
share Sj as desired.
2. Number of Data Points in Cluster j. Now Alice and
Bob wish to share the total number of points in cluster
j, denoted by Tj. Notice that:
Tj =
Ci,j,
nXi=1
i.e. Tj can be found by summing the jth coordinate of
Ci for each i. Bob can compute Tj using his own shares
of Ci and Alice’s encrypted shares, and randomizing
his computation, Alice and Bob share Tj.
3. Centroid of Data Points in Cluster j. In this step Alice
and Bob would like to divide SA
j (from Step 1)
by the total number of data points Tj in cluster j to
obtain the new cluster center ν j :
SA
j + SB
j
j + T B
T A
j
j + SB
ν j =
(7)
Alice and Bob run the DivP d times (once for each
j,l (the lth coor-
dimension d) on inputs P = SA
dinate of Sj ) and divisor D = T A
(notice that
necessarily D ∈ [1..n]).
j,l + SB
j + T B
j
C. Checking the Stopping Criterion. Alice and Bob run the
DistP k times, on the ith time it outputs shares of kµi−ν ik2.
They can then add their shares together and run the FM2NP
to compare these sums with ǫ, some agreed upon predeter-
mined value. They can then open their outputs from the
FM2NP to determine if the stopping criterion has been met.
D. Reassigning New Cluster Centers. The ﬁnal step of our
algorithm, replacing the old cluster centers with the new
ones, is easily accomplished:
Alice sets: (µA
Bob sets: (µB
1 , . . . , µA
1 , . . . , µB
k ) = (ν A
k ) = (ν B
1 , . . . , ν A
1 , . . . , ν B
k ).
k ), and
4.4 Communication Analysis
5. CONCLUSION
Analyzing the communication between Alice and Bob in
the two-party k-means clustering protocol presented in Sec-
tion 4.3 demonstrates that our protocol achieves communi-
cation complexity:
O(ndK) + O(mnkξs) + O(mdkζs).
(8)
Recall that k is the number of clusters, K is the security
parameter, n is the number of data points, d is the number
of attributes of each data point, m is the number of itera-
tions in the Lloyd Step, O(ξs) is the communication cost of
(securely) ﬁnding the minimum of two numbers, and O(ζs)
is the communication cost of performing (secure) two-party
division (where division is deﬁned as in Section 3.1). In this
paper, we showed that ξs ≤ O(K 2) and ζs ≤ Kξs. There-
fore, if the size of the database is suﬃciently large so that
n ≥ dK, then the second term of (8) will dominate the
third and our protocol will have communication complexity
bounded by:
O(ndK) + O(mnkξ) ≤ O(ndK) + O(mnkK 2).
(9)
The communication cost of our protocol matches the com-
munication complexity of [13] while simultaneously enjoying
the extra guarantee of security against an honest-but-curious
adversary. As mentioned in the Introduction, k-means clus-
tering can also be performed securely by applying generic
tools from multi-party computation, e.g. via Yao’s garbled
circuit (see [26]). Let ξns denote the communication cost
of ﬁnding the minimum of two numbers that are shared be-
tween two parties (non-securely), and ζns denote the com-
munication cost of a non-secure division protocol. Notice
that a circuit representation of the single database k-means
clustering protocol of [19] has size at least:
O(mndk) + O(mnkξns) + O(mdkζns).
(10)
The ﬁrst term is necessary e.g. to add together all the data
points in each cluster during each iteration of the Lloyd Step,
the second term is necessary to e.g. ﬁnd the minimum of k
numbers for each data point (when deciding which cluster
the data point belongs to), and the third term is necessary
for performing a division for each dimension of each cluster
center. With the above assumption that n ≥ dK and the
fact that ζns ≤ Kξns, we have that the second term of (10)
will dominate the third. Also, any implementation of a pro-
tocol that ﬁnds the minimum of two (K-bit) numbers will
cost at least O(K). Using these observations and the fact
that applying Yao’s garbled circuit techniques to a circuit
of size O(|C|) has communication complexity O(K|C|), we
have that the communication complexity of a generic solu-
tion is at least:
O(mndkK) + O(mnkK 2).
(11)
Notice that the second term of our protocol’s communication
complexity in (9) matches that of the generic solution in
(11), while our ﬁrst term enjoys asymptotic advantage of a
factor of mk over the ﬁrst term of (11). Furthermore, if
d is suﬃciently large so that d ≥ K, then the ﬁrst term
of equation (11) dominates, in which case our protocol has
overall asymptotic advantage over a generic solution by a
factor of d/K.
We note that there O(K logcK)-sized circuits that can
perform integer reciprocation (see [22]). Assuming these
methods can be translated to perform division as deﬁned in
Section 3.1, we could apply Yao’s garbled circuit techniques
locally (i.e. not for the entire k-means protocol, but only for
division), in which case the second term in (8) will dominate
the third as long as n ≥ d logcK (instead of n ≥ dK).
As mentioned in Section 2.3, the proof of security of the
two-party k-means clustering protocol presented above fol-
lows from the fact that each of the subprotocols are se-
cure. The only exception to this is in step C of the Lloyd
Step, where Alice and Bob must decide if their protocol has
reached the termination condition. Although Alice and Bob
remain oblivious to any actual values at this stage, they will
gain the information of exactly how many iterations were
required in the Lloyd Step. There are various ways of deﬁn-
ing the model to handle this potential information leak and
thus maintain perfect privacy protection (see Appendix A).
6. REFERENCES
[1] D. Agrawal and C. Aggarwal. “On the Design and Quanti-
ﬁcation of Privacy Preserving Data Mining Algorithms.”
Proc. of the 20th ACM SIGMOD-SIGACT-SIGART
Symp. on Principles of Database Systems, pp. 247-255.
2001.
[2] R. Agrawal and R. Srikant. “Privacy-Preserving Data
Mining.” Proc. of the 2000 ACM SIGMOD Int. Conf. on
Management of Data, pp. 439-450. 2000.
[3] A. Blum, C. Dwork, F. McSherry and K. Nissim.
“Practical Privacy: The SuLQ Framework.” 24th
Symposium on Prin- ciples of Database Systems, pp.
128-138. 2005.
[4] P. Bradley and U. Fayyad. “Reﬁning Initial Points for
K-Means Clustering.” Proc. of the 15th International
Conference on Machine Learning, pp. 91-99. 1998.
[5] R. Canetti. “Security and Composition of Multiparty
Cryptographic Protocols.” J. of Cryptology, Vol. 13 No. 1
pp. 143-202. 2000.
[6] C. Dwork, F. McSherry, K. Nissim, and A. Smith.
“Calibra- ting Noise to Sensitivity Private Data Analysis.”
Proc. of the 3rd Theory of Cryptography Conference, pp.
265-284. 2006.
[7] I. Dinur and K. Nissim. “Revealing Information While
Preserving Privacy.” Proc. of the 22nd ACM
SIGMOD-SIGACT-SIGART Symp. on Principles of