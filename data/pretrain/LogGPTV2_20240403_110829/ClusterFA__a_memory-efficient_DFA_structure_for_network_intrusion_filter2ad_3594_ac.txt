Output: SMatrix
1: for i = 1 to N do
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13: end for
sj ← DFAMatrix[j]
if i = j then
SMatrix[i][j] ← 0
else if i < j then
SMatrix[i][j] ← getDistance(si, sj)
SMatrix[i][j] ← SMatrix[j][i]
end if
end for
else
up a lot of memory space; whereas if K is too small, the
number of unique states stored in SparseM atrix becomes
very large, also taking up a lot of memory space. Fig. 3 is an
example of snort24.re, and clearly illuminate this problem:
when K is 40, the compression ratio is the best. But if K is
less than 40, the compression ratio becomes bad sharply.
Now we have two methods to solve this problem: the ﬁrst
method is to use the hierarchical clustering algorithm to cal-
culate the appropriate K from N ; the second method is to
manually try diﬀerent K for clustering algorithm, and ﬁnd
the best result. The time complexity of the hierarchical clus-
tering is O(N 3), where N is the number of states. This time
complexity makes it too slow for large DFAs. So usually we
employ the second method to ﬁnd an appropriate K. Ta-
ble 2 shows the appropriate K for rule sets from Bro, Snort
and L7-ﬁlter. From Table 2, we conclude that usually K is
less than 0.1∗N . So in practice, we only need to try limited
values not more than 0.1∗N , which eﬃciently reducing the
workload.
4.3 Clustering Algorithms
spherical-shape clusters whereas CLINK and pspectralclus-
tering have the ability to produce clusters that are non-
spherical.
The K-means algorithm is a partition-based clustering al-
gorithms. This algorithm is very quick and simple. The
K-Means algorithm partitions objects in a data set into a
ﬁxed number of K disjoint subsets. As a heuristic algorith-
m, there is no guarantee that K-means will converge to the
global optimum. The result of K-means depends on the ini-
tial seeds. To get a near optimal result, we use a greedy
manner shown in Algorithm 6 to determine the initial seeds.
SMatrix, K, N
Algorithm 6 : Choose the initial set for K-Means
Input:
Output: Seeds
1: Seeds.clear()
2: for i = 1 to N do
Seeds(cid:48).insert(i)
3:
4: end for
5: for k = 1 to K do
6:
7:
8:
9: end for
seed ← SMatrix.f indM axV alue(Seeds, Seeds(cid:48))
Seeds.insert(seed)
Seeds(cid:48).remove(seed)
The principle of choosing initial seeds is to maximize the
distances among seeds. When given a few seeds, the nex-
t seed must have a maximal distance with existing seeds.
In Algorithm 6, we initialize two arrays Seeds and Seeds(cid:48)
which store the chosen seeds and its complementary set, re-
spectively. From line 1 to line 4 is the initialization. In line
6, the function f indM axV alue is to ﬁnd the maximal dis-
tance between Seeds and Seeds(cid:48), from SMatrix. From line
7 to line 8, we insert the new seed to Seeds, and remove it
from Seeds(cid:48).
CLINK is a ”top down” hierarchical clustering algorithm
without the need to know the number of clusters in advance.
The result of CLINK is in a very strong deﬁnition of the ho-
mogeneity of clusters: The largest dissimilarity between all
objects of one cluster should be less than a certain value. In
practice, we set the certain value 0.5∗C. In addition, CLINK
can automatically determine the number of clusters K, so
we apply the CLINK method to calculate the appropriate
K as shown in Table 2.
This section reviews the clustering algorithms, namely K-
Means, CLINK, and pspectralclustering, evaluated in our
work. The K-Means algorithm is applicable to data with
Pspectralclustering is a parallel implementation of spec-
tral clustering algorithm. Spectral clustering techniques make
use of the spectrum of the similarity matrix of the data to
204060801000.000.050.100.150.200.250.300.350.40Compression RatioK Compression Ratiorule sets
bro217
snort24
snort31
snort34
l7 top7
l7 2
l7 3
l7 4
l7 5
l7 6
l7 7
Table 3: details of rule sets
length range
of regex
3-211
15-98
15-263
19-115
22-438
17-202
6-139
11-218
29-438
16-209
16-87
number
of rules
number
of states
217
24
40
34
7
7
12
5
6
5
5
6533
8335
4864
9754
12910
1888
2293
3321
2984
4887
4028
perform dimensionality reduction for clustering in fewer di-
mensions. A similarity matrix is an N ∗ N matrix which
express the similarity between two objects, where N is the
number of objects. Unfortunately, when N is large, spectral
clustering can encounter a quadratic resource bottleneck [24]
in computing pairwise similarity among N objects. Pspec-
tralclustering speed up spectral clustering by parallelizing
both memory usage and computation on distributed com-
puters.
5. PERFORMANCE EVALUATION
We use the opensource regex−tool, which is provided by
Michela Becchi in [3], to generate the original DFAs. The
experiments are performed based on several real-life rule sets
from Bro, Snort and L7-ﬁlter. In the experiments, the DFA
for L7-ﬁlter rule set is hard to generate because of the state
blowup problem. So we divide the L7-ﬁlter rule set to mul-
tiple groups using the regex−tool. Also the snort rule set is
divided to 3 groups. Details of rule sets are shown in Table
3.
5.1 Compression Ratio
Space usage of ClusterFA consists of K common states
and the nonzero elements in SparseMatrix. So we deﬁne
the compression ratio r as Equation 1.
Table 4: Compression ratio of three Clustering Al-
gorithms
rule sets
bro217
snort24
snort31
snort34
l7 top7
l7 2
l7 3
l7 4
l7 5
l7 6
l7 7
No. of
states
6533
8335
4864
9754
12910
1888
2293
3321
2984
4887
4028
KMeans
Spec
CLINK
0.036888
0.036888
0.017177 0.13804
0.019037
0.127267
0.014931
0.128721
0.087445 0.772099
0.239626
0.578864
0.020383
0.137996
0.057554
0.085842
0.093329
NULL
NULL
0.729434
0.835317
NULL
0.021864
0.017421
0.018849
0.014931
0.087454
0.231418
0.017725
0.120114
0.05145
0.084237
0.091851
K∗C + nonezero(SparseMatrix)
N ∗ C
r =
(1)
We apply three clustering algorithms in our experiments.
The compression ratio of these clustering algorithms is shown
in Table 4. Pspectralclustering is wrote simply as Spec in
the table, and NULL means no result is worked out. From
the experiment results, the compression ratio of K-means is
similar to that of CLINK, and CLINK has a slight advan-
tage. Both of them are much better than pspectralcluster-
ing. When the rule set is not complex, pspectralclustering
achieves considerable compression ratio, eg. bro217. But
when the rule set becomes complex, the result of pspectral-
clustering is rather poor. On some rule sets, pspectralclus-
tering even can not work out.
In general, partition-based
algorithms and hierarchical clustering algorithms conform
more closely to ClusterFA.
Since the results of K-means and CLINK are similar, we
apply the better results of CLINK to compare with other
implementations. Table 5 shows a performance comparison
among ClusterFA and previous algorithms, including δFA,
D2FA and CRD. The number in bold in Table 5 is the best
compression result.
We can see that ClusterFA achieves better compressibili-
ty on Bro and Snort rule set. Of all the 11 groups of rules,
ClusterFA outperforms best on 7 rule sets, and D2FA out-
performs best on 4 rule sets. We must point out that even
on these 4 rule sets, the compression ratio of ClusterFA is
close to D2FA. However, on the Snort and Bro rule sets,
ClusterFA highly improves the compression ratio by more
than 100%. And on the L7 3 rule set, ClusterFA improves
the compression ratio by nearly 10 times.
5.2 Matching Speed
Because both D2FA and δFA are hardware based algo-
rithms, we only compare the matching speed of ClusterFA
with original DFA. We generate a random text of size 300MB
to search with the Bro and Snort rule sets. We compress the
sparse matrix with a tri-array. The searching method is as
shown in Algorithm 3. Matching speed (MB/s) of ClusterFA
on Bro and Snort rule sets is listed in Table 6. The through-
put of ClusterFA decreases about 40%-50% compared with
original DFA.
5.3 Encoding for ClusterFA
In the previous section, we show the implementation of
ClusterFA in software. However, when implementing Clus-
terFA on the hardware, for example, FPGA, the ClusterFA
can be further compressed by introducing encoding tech-
nique.
The common states is an important factor aﬀecting the
compression ratio of Cluster, especially when the group num-
ber K is big. For example, for the rule set L7 4, the number
Table 6: Matching speed (MB/s) comparision
rule set
throughput of
throughput of
bro217
snort24
snort31
snort34
DFA
99.10496
138.492829
136.739755
138.492829
ClusterFA
57.459791
62.804887
62.70154
63.017119
Table 5: Compression ratio of δFA, D2FA, CRD and ClusterFA
rule sets
bro217
snort24
snort31
snort34
l7 top7