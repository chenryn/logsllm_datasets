Xenstore instance). For fault-free runs, compared to vanilla
Xen, we achieve 1-3% slowdown for I/O-intensive applications
(disk or network). These results are similar to those reported
with Xoar (original version [5] and our reimplementation): the
intrinsic performance overhead of disaggregation is low.
A. XenStore_uks
Recall that, in the fault model that we consider, XenStore
is subject to both unavailability and data corruption faults.
XenStore is highly solicited and plays a critical role during
VM administration tasks. We use VM creation operations to
evaluate XenStore, because this type of operation is one of the
most latency-sensitive and also involves Xenstore the most6.
1) Robustness: We launch a VM creation operation and
inject a crash failure into the master XenStore replica (recall
that we use a total of 3 XenStore instances) during the phase
where XenStore is the most solicited.We repeat the experiment
ten times and we report mean values. The observed results are
as follows.
We observe that some VM creations fail with both vanilla
Xen and Xoar. The latter, after the refresh period, is not
able to replay the VM creation request because it has not
been recorded. Besides, Xoar takes 1 second to detect the
failure. Its recovery time is 22ms (a reboot of XenStore_uk).
In contrast, using our solution, all VM creation operations
complete successfully. Our solution takes 1.54ms and 5.04ms
to detect crashing and data corruption faults respectively. The
recovery process for crashing and data corruption is 25.54ms
(starting a new Xenstore_uk replica and synchronizing its
database). The overall corresponding VM creation time is
about 5.349s and 5.353s respectively for the two failure types,
compared to 5.346s when no fault is injected.
2) Overhead: We sequentially execute ten VM creation
operations (without faults). The mean VM creation time (until
the full boot of the VM’s kernel) for vanilla Xen, Xoar, and
our solution (PpVMM ) is respectively 4.445sec, 6.741sec,
and 5.346sec. Our solution incurs about 20.27% (≈ 900ms)
overhead. This is due to the fact that a VM creation operation
generates mostly write requests (89% of the requests are
writes), which require synchronization between all XenStore
replicas. Read requests do not require synchronization. Fig.
7 reports mean, 95th- and 99th-percentile latencies for read
6A VM creation request requires 53 XenStore requests, whereas VM
destruction, VM migration and vCPU hotplug operations respectively require
47, 24, and 12 requests.
and write requests, conﬁrming the above analysis. The over-
head incurred by our solution is signiﬁcantly lower than the
overhead of Xoar, which is about 51.65% (≈ 2.3s).
Xenstore Read Latencies
1611,18
27x103
30x103
 160
 140
 120
 100
 80
 60
 40
 20
Xenstore Write Latencies
1719,06
27x103
31x103
 250
 200
 150
 100
 50
Mean
Xen
95th
PpVMM
99th
Xoar
Mean
Xen
95th
PpVMM
99th
Xoar
Fig. 7: Mean, 95th and 99th-percentile latencies of XenStore
requests during 10 VM creation operations. The reported
latencies are in μs.
B. net_uk
For these experiments, we run independently each Tail-
Bench application inside the user VM and we measure how it
is impacted by crash failures.
1) Robustness: Recall that our solution enhances net_uk
with several FT feedback loops in order to detect failures at
different granularities: the unavailability of the subcomponents
(NIC driver and netback) and the unavailability of the entire
net_uk. Here, we only evaluate the robustness of our system
facing NIC driver crashes because it allows us, through the
same experiment, to compare ﬁne-grained (FG) and coarse-
grained (CG) FT solutions. We inject a fault in the NIC driver
at the middle of the execution of the benchmark. Table IV and
Table V present the results. We do not interpret Xoar results
here (already discussed in §II-B). Besides, we do not show
performance results for vanilla Xen because it is unable to
achieve application completion in case of a net_uk failure.
We can see that the ﬁne-grained solution allows quick de-
tection compared to coarse-grained solutions (ours and TFD-
Xen): up to a 3.6x difference for detection and 1.4x for repair
times (compared to our coarse-grained approach). TFD-Xen is
faster to recover because it relies on net_uk replicas linked to
backup physical NICs: instead of recovering a failed net_uk
unikernel, it switches from one net_uk to another and reconﬁg-
ures the bindings with the running user VMs. However, TFD-
Xen requires at least N +1 physical NICs and N +1 net_uks to
survive N net_uk faults, which results in resource waste and
limited resilience over long time intervals. Furthermore, our
ﬁne-grained solution avoids packet losses, thanks to the use
of a shadow driver that buffers packets in case of failure. For
instance, we measured 212, 506 buffered packets for the sphinx
application. In contrast, the other solutions lead to broken
TCP sessions (caused by packet losses) that occur during
network reconﬁguration (even for TFD-Xen, despite its short
recovery time). Moreover, we can see that the ﬁne-grained FT
solution reduces the tail latency degradation compared to the
coarse-grained solution. Considering the sphinx application for
instance, the differences are respectively 24.88%, 12.88%, and
5.88% for the mean, 95th and 99th-percentile latencies.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:32:03 UTC from IEEE Xplore.  Restrictions apply. 
205
FG FT
CG FT
TFD-Xen [4]
Xoar [5]
DT (ms) RT (s)
27.27
98.2
102.1
52 × 103
4.7
6.9
0.8
6.9
PL
0
425, 866
2379
1, 870, 921
TABLE IV: Robustness evaluation of different FT solutions
for net_uk. The failed component is the NIC driver.
DT = Fault Detection Time; RT = Fault Recovery Time; PL
= number of (outgoing) lost packets.
Regarding the throughput measurements with the AB bench-
mark, we observe the following results. TFD-Xen achieves
the best performance with 45 requests/s. The FG solution is
relatively close with 42 requests/s (7.14% gap). In contrast, the
CG approach is signiﬁcantly less efﬁcient with 29 requests/s
(55.17%) and Xoar is much worse with 9 requests/s (400%).
2) Overhead: The experiment is the same as previous with-
out fault injection. Table VI presents the results. The overhead
incurred by our solution is up to 12.4% for mean latencies, up
to 17.3% for the 95th percentiles, and up 12.3% for the 99th
percentiles. This overhead is due to periodic communication
with the hypervisor to track the driver execution state (§IV-B).
Notice that TFD-Xen [4] incurs overhead up to 2.88% for
mean latencies, 17.87% for the 95th percentiles, and up to
13.77% for the 99th percentiles.The overhead incurred by
Xoar is much higher, as already discussed in §II-B.
Regarding the throughput measurements with the AB bench-
mark, we observe the following results compared to the
vanilla Xen baseline (123 requests/s). Both TFD-Xen and our
solutions (FG and CG) exhibit a noticeable but acceptable
overhead (13.31%, 12%, and 15% respectively), whereas Xoar
incurs a more pronounced performance degradation (1130%7).
C. tool_uk
Contrary to other dom0 unikernels, tool_uk does not execute
a task permanently.It only starts a task when invoked for
performing a VM administration operation. The FT solution
does not incur overhead when there are no failures. Therefore
we only evaluate the robustness aspect. To this end, we
consider the VM live migration operation because it is the
most critical one. We run inside the user VM a Linux kernel
compilation task and inject a failure during the second stage
of the migration process, i.e., when a replica of the migrated
VM has been launched on the destination machine, and the
memory transfer is ongoing.
We observe that vanilla Xen and Xoar lead the physical
machine to an inconsistent state: the migration stops but both
the original VM (on the source machine) and its replica (on
the destination machine) keep running. This situation leads to
resource waste because the replica VM consumes resources.
Using our solution, the replica VM is stopped upon failure
detection. The detection time is 800ms.
7With a refresh period of 5s, Xoar still incurs a performance degradation
of up to 697%, signiﬁcantly worse than our approach. Detailed results for this
setup are not reported due to lack of space.
D. Global failure
We also evaluate the robustness of our solution when all
the pVM components crash at the same time. We execute
the sphinx application from TailBench in a guest and we
inject faults to crash all the components simultaneously. In
this case, the hypervisor detects the global crash and restores
all unikernels in the appropriate order (see §IV-D). The whole
recovery of all unikernels takes 15.8s. Concerning application
performance, we observe a downtime of 7.85s (corresponding
to the time needed for XenStore_uk and net_uk to recover),
but the application survives and ﬁnishes its execution correctly.
We experience a huge degradation of tail latencies due to the
long downtime but we allow full and transparent functional
recovery of the user VM, unlike vanilla Xen, TFD-Xen, and
with a much lower overhead than Xoar (esp. during failure-
free execution phases).
E. Scheduling optimizations
We measure the beneﬁts of our scheduling optimizations
(§IV-E) in terms of reactivity and CPU time used by our
unikernels. Regarding reactivity, we run the sphinx application
in a guest VM, and we trigger the crash of the net_uk. On
average, with the scheduling optimizations, we detect
the
crash after 141.8ms compared to 149.5ms without,
i.e., a
5.15% decrease. Besides, on a fault-free run, compared to a
standard scheduling policy, the usage of implicit heartbeats
allows a 13% decrease of the CPU time consumed by the
pVM components.
VI. RELATED WORK
pVM resilience. The projects most closely related to our
work are Xoar [5] and TFD-Xen [4]. Given that they are
described in detail and evaluated in the previous sections.
Beyond Xoar, a number of projects have investigated the
beneﬁts of disaggregating the VMM into multiple isolated
components. Murray et al. [28] modiﬁed the original Xen
platform design in order to move the domain builder (a
security-sensitive module within the Xen toolstack running
in the pVM) to a separate virtual machine. This work did
not investigate ﬁne-grained disaggregation nor fault tolerance.
Fraser et al. [2] revisited the design of the Xen platform
in order to support “driver domains”, i.e., the possibility to
isolate each physical device driver in a separate VM. Our
contribution builds on this work but also considers ﬁne-grained
fault-tolerance mechanisms within driver domains, as well as
disaggregation and robustness of other pVM components.
As part of their Xen-based “resilient virtualization infras-
tructure” (RVI) [29], [30], Le and Tamir brieﬂy discussed
how to improve the fault tolerance of the pVM and the driver
domains (dVMs). The failure of a driver domain is detected
by agents within the dVM kernel and the hypervisor, which
triggers the microreboot of the dVM. The failures of the
services hosted by the pVM (e.g., XenStore) are detected by
an agent running within the pVM. Upon such a detection,
the agent issues a hypercall to the hypervisor, and the latter
triggers a crash of the whole pVM. Hence, any failure of
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:32:03 UTC from IEEE Xplore.  Restrictions apply. 
206
99th
99th
9.67
sphinx
95th
1696
mean
879.11
1201.1
1500.3
xapian
95th
4.35
100.16 1700.31
2101.45
154.9
1396.21
98.11
11026.7 13590.3 5188.1 7238.9
8193.3
Xen
1820.84
FG FT
3100.12 3891.73
CG FT
4120.91
3499.4
TFD-Xen 1159.32 2908.89
3304.2
Xoar
TABLE V: Performance of TailBench applications during a net_uk failure (latencies in milliseconds). Lower is better.
The failed component is the NIC driver. The ﬁrst line (“Xen”) corresponds to a fault-free baseline.
mean
457.61
821.11
1091.5
788.3
1116.81
10011.2 13444.5 140881.43 1491.9 4721.33 12390.4
moses
99th
95th
39.56
65.64
473.21 1492.31
591.51 1833.09
450.22 1101.44
5909.3
mean
8.6
73.21
100.5
70.31
5120.4 5581.8
masstree
95th
475.37
1891.1
2099.1
1381.12
99th
476.2
2122.18
2461.09
1631.77
mean
1.7
88.22
112.01
80.12
95th
3.42
440.21
610.91
398.32
mean
1.79
51.19
89.9
50.10
99th
7.6
1310.88
1503
img-dnn
8100.4
sphinx
95th
1696