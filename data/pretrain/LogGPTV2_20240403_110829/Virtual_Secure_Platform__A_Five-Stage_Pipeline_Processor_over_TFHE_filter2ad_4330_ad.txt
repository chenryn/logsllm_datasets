Informally, the reason why CMUX Memory is fast can be
explained by the fact that the evaluation of Circuit Bootstrap-
ping takes about 10 times as long as it takes to evaluate any
two-input homomorphic gate. Let v,w ∈ N be the number of
bits of the address and the data bus, respectively. Assuming
that we ignore the time it takes to process CMUX, because
CMUX is several hundred times faster than any two-input ho-
momorphic gate, the time it requires to evaluate the ROM of
the CMUX Memory is roughly equivalent to 10v + w Homo-
morphic Gates. Meanwhile, the time it takes to evaluate the
RAM is roughly equivalent to 10v +w(2v +1) Homomorphic
Gates. The w term comes from HomMUX without SE and
IKS and the w· 2v term comes from the noise refreshment.
The construction of the ROM and RAM by logic gates takes at
least w·2(2v−1) two-input Homomorphic Gates each to con-
struct the tree for data fetching. Therefore, in theory, CMUX
Memory can be expected to be faster than constructing the
memory by logic gates.
8.2 RAM
In this paper, we only treat one cycle, single port RAM since it
requires the minimum amount of Bootstrapping to implement.
The RAM has following characteristics: (i) Read and write
4016    30th USENIX Security Symposium
USENIX Association
ROMPCIFCacheIDMainRegisterALUBranchControllerMemory ControllerRAMInstruction FetchInstruction DecodeExecutionMemory AccessWrite BackFigure 6: The architecture of the one-cycle single-port RAM.
are exclusive. (ii) Both read and write are done in one cycle.
(iii) Read and write use the same address.
The visual overview of the RAM architecture is given in
Figure 6. There are three inputs for RAM: address, write ﬂag,
and write data. The address is the memory address for write or
read. The write ﬂag is one bit data which selects the operation
mode of RAM. The write data is the data which will be wrote
in the address if the RAM is write mode. The RAM has one
output port, where the data presented at the input address are
retrieved. In VSP, the data bus in the processor uses TLWE
as ciphertexts for memory elements, since TLWE ciphertexts
are also used by the Homomorphic Gates in other parts of the
processor circuit.
As shown in Figure 6, RAM consists of the read unit, the
control unit, and the write unit. In what follows, we provide
a comprehensive explanation on each of the unit. Note that
addresses in the write and the read units are in TRGSW cipher-
text, but the memory controller in Memory Access stage of
the processor feeds the address as TLWE ciphertexts. There-
fore, Circuit Bootstrapping is applied to TLWE ciphertexts to
get TRGSW ciphertexts representations of the addresses.
Read Unit
The read unit reads the data at a given address. The visual
overview of its architecture is given in Figure 7 and 8. The
data of RAM are represented as w· 2v TRLWE ciphertexts,
where each TRLWE ciphertext contains one bit of plaintext in-
formation. The TRLWE ciphertexts are divided into w blocks,
where the ith block contains the ith bit of each word. A CMUX
tree is used to fetch the ith bit of the word from the ith block.
We note that, although the message space of TRLWE is ca-
pable of holding a vector of N binary values, i.e., BN[X], we
only ﬁll one entry with an actual plaintext value. If we pack
multiple bits into a single TRLWE ciphertext for read, we
also have to write in a packed manner. The problem for pack
writing is that every instruction might have a chance to write
only a small amount (e.g,. a 16-bit register) of data to RAM,
Figure 7: The architecture of
the read unit.
Figure 8: An example of the
CMUX tree (v = 2).
Figure 9: The architecture of the control unit.
and the amount of computations it takes to pack and unpack
bits can be a signiﬁcant overhead.
The task of the CMUX tree is to compare each bit of the
address with that of RAM data by a tree of multiplexers imple-
mented by CMUX, such that data at the designated memory
address can be read.
Control Unit
The control unit is the interface between main processor cir-
cuit and CMUX Memory. We show an architectural illus-
tration of the control unit and module in Figure 9 and 10,
respectively. The control unit consists of w control modules,
each of which processes a single bit of the write data. Since
the processor only accepts TLWE ciphertexts, SE and IKS
are inserted to convert the read data from the read unit into
TLWE ciphertexts. The control module performs multiplex-
ing between the read and the write data, depending on the
write ﬂag. The multiplexed result is sent to the write unit as
the controlled data.
Write Unit
From the view of the main processor circuit, each word of
the current cycle data is the multiplexed result between the
word of the previous cycle data and the write data depend-
ing on the write ﬂag and the address matching. Since the
multiplexed result depends on the write ﬂag that is fed as
the controlled data, the write unit only needs to take care of
USENIX Association
30th USENIX Security Symposium    4017
Write Flag(1 TLWE)Address(v TLWE)Write Data(w TLWE)CircuitBootstrappingAddress(v TRGSW)Control UnitRead UnitWrite UnitRead Data(w TRLWE)ControlledData(w TRLWE)Read Data(w TLWE)previous cycleRAM data(w2v TRLWE)current cycleRAM data(w2v TRLWE)ExecutionstageMemory Controller(Memory Access stage)CMUXtreeof w-1th bit(w-1)th blockCMUXtreeCMUXtreeCMUXtreeof 0th bitDepth v2v bit data0th block・・・Width wRead Data (w TRLWE)(0th bit of each word)01S0CMUX01S0CMUX01S0CMUX0x00 data(TRLWE)0x01 data(TRLWE)0x10 data(TRLWE)0x11 data(TRLWE)0th bit ofAddress(TRGSW)1th bit ofAddress(TRGSW)1bit ofReadData(TRLWE)0th bit ofRead Data(TRLWE)0th bit ofWrite Data(TLWE)Write Flag(TLWE)(w-1)th bitControl Module(w-1)th bit ofWrite Data(TLWE)(w-1)th bit ofRead Data(TRLWE)・・・Controlled Data(w TRLWE)Read Data(w TLWE)0th bitControl ModuleFigure 10: The architecture of a control module.
Figure 12: An example of the write block (v = 2).
Figure 11: The architecture of the write unit.
the address matching part of the computation. The write unit
also performs Bootstrapping over the entire contents of the
RAM. An visual overview of the write unit is given in Fig-
ure 11, 12, and 13. The write unit consists of w write blocks,
each handles a single word. Each write block is composed of
2v write bars which handles a single bit. Therefore, the write
unit consists of w· 2v write bars arranged in parallel.
The working principle of the write bar is comparing each bit
of the input address with the addresses in the RAM, through
an array of CMUX gates. If all bits in the input address match
a particular entry in the RAM, the controlled data is selected
and becomes current cycle data. Here, when the write ﬂag is
false, the controlled data is same as the previous cycle data
in the address, so current cycle data is same as previous one.
On the other hand, if the write ﬂag is true, the controlled
data and current cycle data both become the write data, and
the data are written into the memory. If the addresses do not
match, previous cycle data is selected, and data in memory
are not modiﬁed. The write bar refreshes the noise added by
the CMUX array by bootstrapping the data at the end.
Remark: The implementation of the comparison between
an input address bit and a constant address bit is, in fact, quite
simple. More speciﬁcally, the comparison result between an
input bit with a constant value of 1 is the bit itself. Meanwhile,
the comparison with 0 can be implemented by a subtraction
of a constant TRGSW ciphertext encrypting the constant 1
followed by a sign inversion of all coefﬁcients in the resulting
TRGSW ciphertext.
Figure 13: An example of a write bar at address 0x01 (v = 2).
8.3 ROM
The construction of ROM with LHE mode of TFHE is trivial
by using Look Up Table (LUT), which is described in [11].
We applied both optimization techniques mentioned in [11],
namely Vertical Packing and Horizontal Packing.
9 Evaluation
In this section, we perform thorough experiments on VSP to
demonstrate its performance. We will ﬁrst characterize VSP
over a set of benchmarks in Section 9.1, and then deliver the
overall performance statistics in Section 9.2
9.1 Benchmarks
Benchmark environments
In our implementation, ROM and RAM are 512 bytes, that is,
v = 8 and w = 16 when using the CMUX Memory for RAM.
We also experimented 1 KiB ones. See Appendix C for the
details.
The main benchmark program used in our evaluation is
Hamming. Hamming takes two 8-digit hexadecimal numbers
a and b as its arguments, and ﬁnds the Hamming distance
between them. The programs are compiled into CAHPv3 exe-
cutable by llvm-cahp with -Oz optimization ﬂag, which mini-
mizes the size of machine code. Then, the compiled programs
are encrypted and executed on Iyokan with CAHP-Ruby (with
pipeline) and CAHP-Pearl (without pipeline). The scripts to
reproduce the runtime performance evaluation is available
at [48].
We used four types of machines to evaluate VSP:
AWS c5.metal An HPC server hosted by Amazon Web Ser-
vice equipped with Intel Xeon Platinum 8275CL CPU
4018    30th USENIX Security Symposium
USENIX Association
01S0HomMUXw/oSE and IKS1 bit ofRead Data(TRLWE)Sample ExtractandIdentity Key Switching1 bit ofRead Data(TLWE)1 bit ofControlled Data(TRLWE)Write Flag(TLWE)1bit of Write Data(TLWE)0th bit data ofeach addressat previous cycle(2v TRLWE)0th bitWrite Block(w-1)th bit data ofeach addressat previous cycle(2v TRLWE)(w-1)th bitWrite Block・・・0th bit data of each address at current cycle(2v TRLWE)(w-1)th bit data of each address at current cycle(2v TRLWE)Address(v TRGSW)Depth v0x00 Write Bar0x00 dataat previous cycle(TRLWE)0x00 dataat current cycle(TRLWE)Address(2 TRGSW)0x11 Write Bar0x11 dataat previous cycle(TRLWE)0x11 dataat current cycle(TRLWE)・・・01S0CMUX01S0CMUXControlled Data(TRLWE)0x01 data at previous cycle(TRLWE)0x01 dataat current cycle(TRLWE)0th bit of Address == 1(TRGSW)1st bit ofAddress == 0(TRGSW)BootstrappingTable 2: Processor Size Evaluation
Table 4: Machine Code Size
Processor
CAHP-Ruby
CAHP-Pearl
Lite MIPS [46]
PicoRV32 [47]
MUX NOT Others
2422
996
877
2054
6241
1276
2732
5162
15
22
39
11
Table 3: Size of Keys and Ciphertexts
Type
Size[MiB]
Secret Key
Bootstrapping Key
2563.047
0.023
0.033
33.55
ROM
RAM
(96 vCPUs), 92GiB RAM, and no GPUs.
Sakura Koukaryoku An HPC server hosted by Sakura in-
ternet Inc. equipped with Intel Xeon CPU E5-2623 v3
(16 vCPUs), 128GB RAM, and single NVIDIA Tesla
V100.
AWS p3.8xlarge An HPC server hosted by Amazon Web
Service equipped with Intel Xeon CPU E5-2686 v4 (32
vCPUs), 244GB RAM, and 4 NVIDIA Tesla V100.
AWS p3.16xlarge An HPC server hosted by Amazon Web
Service equipped with Intel Xeon CPU E5-2686 v4 (64
vCPUs), 488GB RAM, and 8 NVIDIA Tesla V100.
Runtime Performance Evaluation
Table 5 shows the run-time statistics required to evaluate the
encrypted program of Hamming. Here, sec./cycle stands for
seconds per clock cycle, which is the amount of program
run-time divided by the number of required clock cycles.
While pipelining increases the number of gates of the pro-
cessors, the technique enables more gates to be run in parallel.
Therefore, when the physical machine has enough parallel
processing units, pipelining reduces per-clock-cycle run-time
of VSP, and eventually results in decreased total run-time
(Compared between Cases #4 and 6, 7 and 8, 9 and 11, and
10 and 12). On the other hand, when the physical machine is
not so powerful (Cases #1 and 2), the runtime ends up being
slower due to the increased number of clock cycles. In addi-
tion, in Cases #3 and 5, the CMUX Memory is turned off and
the machine does not have enough parallel processing units to
fully parallelize the gates in ROM and RAM. Consequently,
the physical processors do not have more machine resources
for evaluating the pipelined core processor circuit.
Finally, we observe that while AWS p3.8xlarge (4 V100)
is much faster than Sakura Koukaryoku (single V100), there
Program RV32IC [B] CAHPv3 [B]
Fibonacci
Hamming
Brainf*ck
36
354
226
31
264
229
is almost no difference between AWS p3.16xlarge (8 V100)
and p3.8xlarge. This is most likely caused by the fact that the
parallel processing capabilities of both machines well exceed
the number of logic gates that can be evaluated in parallel in
our processor. Therefore, further pipelining may be conducted
on such powerful computing platforms.
Besides pipelining, we also experiment on the performance
impact of the proposed CMUX Memory. As shown in Table 5,
CMUX Memory reduces runtime across all cases we tested.
When CMUX Memory is not used, ROM and RAM need to be
implemented by the Homomorphic Gates in the FHE mode of
TFHE, which results in signiﬁcant performance degradations.
The fastest instance we tested is Case #12, that is, AWS
p3.16xlarge with pipelining and CMUX Memory applied,
which is shown in bold in Table 5. We achieved a performance
of about 0.8 sec./cycle, or equivalently, 1.25 Hz. From the
results of the benchmark, we conclude that both pipelining and
CMUX Memory are effective in improving the performance
of VSP.
Processor Size Evaluation
In general, fewer logic gates means fewer computational com-
plexity, so the total gate count of the processors is one of the
most important factors which determine the performance of
VSP. Table 2 shows the size of CAHP-Ruby and CAHP-Pearl.
In the table MUX and NOT are counted separately because
their performance characteristics are different from a normal
homomorphic gate. In particular, MUX is twice as slow as
other homomorphic gates, even with the cryptographic opti-
mization proposed in [11]. Meanwhile, NOT can be evaluated
much faster than other gates, as the only operations in a NOT
gate are sign inversions. We compare the gate count of our
processor to that of Lite MIPS [46] and PicoRV32 [47]. Lite
MIPS is the processor which is implemented in TinyGar-
ble [6]. PicoRV32 is one of open-source implementations
of RISC V, where the design goal is to implement a small
(in terms of gate count) processor. As shown in Table 2, our
processors are smaller than both of the existing designs.
Data size Evaluation
We used two more programs except Hamming: Fibonacci
and Brainf*ck here. Fibonacci takes a decimal digit n as its
command-line argument, and calculates nth Fibonacci number.
Here we used n = 5. Brainf*ck interprets code of brainf*ck,
USENIX Association
30th USENIX Security Symposium    4019
Table 5: Performance Evaluation Using Hamming
Case #
Machine
# of
V100
Pipelining? CMUX Memory?
AWS c5.metal
Sakura Koukaryoku
AWS p3.8xlarge
AWS p3.16xlarge
0
1
4
8
1
2
3
4
5
6
7
8
9
10
11
12
No
Yes
No
No
Yes
Yes
No