## PostgreSQL 1000亿数据量 正则匹配 速度与激情   
### 作者                                                                                                                                 
digoal                                                                                                                                  
### 日期                                                                                                                                
2016-03-07                                          
### 标签                                                                                                                                
PostgreSQL , pg_trgm , 倒排索引 , reverse , like , 正则匹配 , 模糊查询 , gin索引                                                                                                                
----                                                                                                                                
## 背景 
本文主要讲解并验证一下PostgreSQL 1000亿数据量级的模糊查询、正则查询，使用gin索引的效率。  
承接上一篇  
https://yq.aliyun.com/articles/7444
## 测试环境
测试环境为 8台主机(16c/host)的 PostgreSQL集群，一共240个数据节点，测试数据量1008亿。  
性能图表 :  
![_](20160307_01_pic_001.png)
如果要获得更快的响应速度，可以通过增加主机和节点数(或者通过增加CPU和节点数)，缩短recheck的处理时间。  
## 数据生成方法
```
#!/bin/bash  
#      截取通过random()计算得到的MD5 128bit hex的前48bit, 转成字符串，得到[0-9]和[a-f]组成的12个随机字符串。  
psql digoal digoal -c "create table t_regexp_100billion distributed randomly"  
for ((i=1;i select count(*) from t_regexp_100billion ;  
    count       
--------------  
 100800000000  
(1 row)  
Time: 228721.386 ms  
```
表大小, 4.1 TB  
```
digoal=> \dt+ t_regexp_100billion   
                           List of relations  
 Schema |        Name         | Type  | Owner  |  Size   | Description   
--------+---------------------+-------+--------+---------+-------------  
 public | t_regexp_100billion | table | digoal | 4158 GB |   
(1 row)  
```
索引大小 
```
idx_t_regexp_100billion_1     2961 GB  
idx_t_regexp_100billion_1     2961 GB  
idx_t_regexp_100billion_gin   2300 GB  
```
测试数据展示:  
```
digoal=> select * from t_regexp_100billion offset 1000000 limit 10;  
     info       
--------------  
 bca0fb45367e  
 3051ca8a9a38  
 fadc91a3a4de  
 710b9c60417e  
 279dd9832cc3  
 f4743fe2e83b  
 9ce9e42d4039  
 65e64742fd3f  
 db3d0e0edc52  
 7cfb00bb38ec  
(10 rows)  
```
重复度取样, 计算random() md5得到的字符串，可以确保非常低的重复度：  
```
digoal=> select count(distinct info) from (select * from t_regexp_100billion offset 1299422811 limit 1000000) t;  
 count    
--------  
 999750  
(1 row)  
```
统计信息展示：  
```
digoal=> alter table t_regexp_100billion alter column info set statistics 10000;  
ALTER TABLE  
digoal=> analyze t_regexp_100billion ;  
ANALYZE  
schemaname             | public  
tablename              | t_regexp_100billion  
attname                | info  
inherited              | f  
null_frac              | 0  
avg_width              | 13  
n_distinct             | -0.836834             # 采样统计信息，约83.6834%的唯一值  
most_common_vals       | (pg_catalog.text){7f68d12d2205,00083380706d,00154b6d79e8,...    
most_common_freqs      | {1e-06,6.66667e-07,6.66667e-07,6.66667e-07,.....        单个最高频值的占比为1e-06, 也就是说1000亿记录中出现10万次。  
histogram_bounds       | (pg_catalog.text){0000008123b7,00066c71c9bb,000d672de234,...  
correlation            | 0.000237291  
most_common_elems      |   
most_common_elem_freqs |   
elem_count_histogram   |   
```
7f68d12d2205 实际的出现次数，可能是采样时7f68d12d2205被采样到的块较多，所以数据库认为它的占比较多：  
```
digoal=> select count(*) from t_regexp_100billion where info='7f68d12d2205';  
-[ RECORD 1 ]  
count | 54  
digoal=> select ctid from t_regexp_100billion where info='7f68d12d2205' order by 1;  
     ctid        
---------------  
 (15343,114)  
 (62134,39)  
 (96808,112)  
 (116492,176)  
 (194615,143)  
 (328074,116)  
 (364037,115)  
 (375240,158)  
 (376187,152)  
 (602144,81)  
 (664026,6)  
 (689501,136)  
 (695345,130)  
 (697374,126)  
 (714719,148)  
 (743169,20)  
 (802326,139)  
 (833830,41)  
 (839417,185)  
 (892417,78)  
 (892493,149)  
 (907979,52)  
 (967078,163)  
 (990313,159)  
 (1007998,27)  
 (1106961,57)  
 (1142731,165)  
 (1148427,67)  
 (1156654,156)  
 (1205854,137)  
 (1243429,68)  
 (1277287,165)  
 (1328836,98)  
 (1331727,150)  
 (1337534,3)  
 (1360947,104)  
 (1438970,97)  
 (1476941,22)  