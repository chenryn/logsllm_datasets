### Datasets, Test Groups, and Profiles per Tester

| Platform | Group | # of Profiles | Testers per Tester |
|----------|-------|---------------|--------------------|
| Facebook (US) | CN Expert | 24 | 100 |
| Facebook (US) | CN Turker | 418 | 10 |
| Facebook (US) | US Expert | 40 | 100 |
| Facebook (US) | US Turker | 299 | 12 |
| Facebook (US) | US Social | 198 | 50 |
| Facebook (IN) | IN Expert | 20 | 100 |
| Facebook (IN) | IN Turker | 342 | 12 |

**Table 1. Datasets, test groups, and profiles per tester.**

### Ground-Truth Data Statistics (Average Number per Profile)

| Dataset | Category | Legitimate | Sybil |
|---------|----------|------------|-------|
| Renren | News-Feed | 165 | 30 |
| Renren | Photos | 55.62 | 60.15 |
| Renren | Profile Images | 55 | 31.6 |
| Renren | Censored Images | 10 | 1.5 |
| Facebook (US) | News-Feed | 302 | 22 |
| Facebook (US) | Photos | 184.78 | 10.22 |
| Facebook (US) | Profile Images | 53.37 | 10.28 |
| Facebook (US) | Censored Images | 32.86 | 4.03 |
| Facebook (IN) | News-Feed | 7.27 | 4.44 |
| Facebook (IN) | Photos | 0 | 0.06 |
| Facebook (IN) | Profile Images | 0 | 1.81 |
| Facebook (IN) | Censored Images | 0 | 0.08 |

**Table 2. Ground-truth data statistics (average number per profile).**

### Data Collection and Sanitization

We obtained legitimate profiles on Renren directly from Renren Inc. The security team at Renren provided complete information on 1082 banned Sybil profiles, from which we randomly selected 100 for our user study. Details on how Renren bans Sybil accounts can be found in [31]. For collecting legitimate Renren profiles, we used the same methodology as for Facebook. We seeded a crawler with 4 trustworthy profiles from people in the lab, crawled 100K friends-of-friends, and then sampled 100 public profiles. These profiles were verified by Renren’s security team to ensure they belonged to real users.

#### Summary and Data Sanitization

Table 1 lists the final statistics for our three datasets. Since the Renren data was provided directly by Renren Inc., all profiles are confirmed as either Sybils or legitimate users. For Facebook US and India, profiles that were banned by Facebook are confirmed Sybils, and the remaining unconfirmed suspicious profiles are not listed.

During our manual inspection of profiles, we noticed that some include images of pornography or graphic violence. We determined that it was not appropriate to use these images in our user study. Thus, we manually replaced objectionable images with a grey image containing the words “Pornographic or violent image removed.” This change protects our test subjects from viewing objectionable images while still allowing them to get a sense of the original content. Out of 45,096 total images in our dataset, 58 are filtered from Facebook US, 4 from Facebook India, and 6 from Renren. All objectionable images are found on Sybil profiles; none are found on legitimate profiles.

Finally, Table 2 shows the basic statistics of ground-truth profiles. Legitimate users have more photo albums and profile photos, while Sybils have more censored photos. The “News-Feed” column shows the average number of items in the first 5 chronological pages of each user’s news-feed. On Facebook, the news-feed includes many types of items, including wall posts, status updates, photo tags, etc. On Renren, the feed only includes wall posts from friends.

### Experiment Design

Using the datasets in Table 1, our goal is to assess the ability of humans to discriminate between Sybil and legitimate user profiles. To test this, we perform a simple, controlled study: we show a human test subject (or simply a tester) a profile from our dataset and ask them to classify it as real or fake. The tester is allowed to view the profile’s basic information, wall, photo albums, and individual photos before making their judgment. If the tester classifies the profile as fake, they are asked what profile elements (basic information, wall, or photos) led them to this determination.

Each tester in our study is asked to evaluate several profiles from our dataset, one at a time. Each tester is given roughly an equal number of Sybil profiles and legitimate profiles. The profiles from each group are randomized for each tester, and the order the profiles are shown in is also randomized.

#### Implementation

We implement our study as a website. When a tester begins the study, they are presented with a webpage that includes a consent form and details about our study. After the tester agrees, they are directed to the first profile for evaluation. Figure 2 shows a screenshot of our evaluation page. At the top are links to all the profiles the tester will evaluate. Testers may use these links to go back and change their earlier answers if they wish.

Below the numbered links is a box where testers can record their evaluation for the given profile: real or fake, and if fake, what profile elements are suspicious (profile, wall, and/or photos)? When the tester is done evaluating the given profile, they click the “Save Changes” button, which automatically directs their browser to the next profile, or the end of the survey if all profiles have been evaluated.

Below the evaluation box are three buttons that allow the tester to view the given profile’s basic information (shown by default), wall, and photo albums. The basic information and wall are presented as JPEG images to preserve the exact look of Facebook/Renren and prevent the tester from clicking any potentially malicious embedded links. Testers may click on each photo album to view the individual photos contained within.

### Test Groups

#### Experts

The first group of test subjects are experts. This group contains Computer Science professors and graduate students carefully selected by us. The expert group represents the practical upper-bound on achievable Sybil detection accuracy.

The expert group is subdivided into three regional groups: US, Indian, and Chinese experts. Each expert group was evaluated on the corresponding regional dataset. We approached experts in person, via email, or via social media and directed them to our study website to take the test. Table 1 lists the number of expert testers in each regional group. Expert tests were conducted in February 2012.

As shown in Table 1, each Chinese and Indian expert evaluated 100 profiles from our dataset, while US experts evaluated 50 profiles. This is significantly more profiles per tester than we gave to any other test group. However, since experts are dedicated professionals, we assume that their accuracy will not be impacted by survey fatigue. We evaluate this assumption in Section 5.

#### Turkers

The second group of test subjects are turkers recruited from crowdsourcing websites. Unlike the expert group, the background and education level of turkers cannot be experimentally controlled. Thus, the detection accuracy of the turker group provides a lower-bound on the efficacy of a crowdsourced Sybil detection system.

Like the expert group, the turker group is subdivided into three regional groups. US and Indian turkers were recruited from MTurk. HITs on MTurk may have qualifications associated with them. We used this feature to ensure that only US-based turkers took the Facebook US test, and Indian turkers took the Facebook India test. We also required that turkers have ≥90% approval rate for their HITs, to filter out unreliable workers. We recruited Chinese turkers from Zhubajie, the largest crowdsourcing site in China. Table 1 lists the number of turkers who completed our study in each region. Turker tests were conducted in February 2012.

Unlike the expert groups, turkers have an incentive to sacrifice accuracy in favor of finishing tasks quickly. Because turkers work for pay, the faster they complete HITs, the more HITs they can do. Thus, of all our test groups, we gave turkers the fewest number of profiles to evaluate, since turkers are most likely to be affected by survey fatigue. As shown in Table 1, Chinese turkers each evaluated 10 profiles, while US and Indian turkers evaluated 12.

We priced each Zhubajie HIT at $0.15 ($0.015 per profile), and each MTurk HIT at $0.10 ($0.0083 per profile). These prices are in line with the prevailing rates on crowdsourcing platforms.

### User Study Website

At the end of the survey, the tester is asked to answer a short questionnaire of demographic information. Questions include age, gender, country of residence, education level, and years of OSN experience. There is also a free-form comment box where testers can leave feedback.

On the server-side, we record all classifications and questionnaire answers made by each tester. We also collect additional information such as the time spent by the tester on each page and total session time per tester.

Because our datasets are in two different languages, we constructed two versions of our study website. Figure 2 shows the English version of our site, which is used to evaluate Facebook profiles. We also constructed a Chinese version of our site to evaluate Renren profiles.

### Limitations

The methodology of our user study has two minor limitations. First, we give testers full profiles to evaluate, including basic info, wall, and photos. It is not clear how accurate testers would be if given different information or a restricted subset of this information. Second, we assume that there are no malicious testers participating in our user study. Although attackers might want to infiltrate and disrupt a real crowdsourced Sybil detector, there is little for them to gain by disrupting our study. Related work on detecting crowdsourcing abuse may be helpful in mitigating this problem in the future [7].

### Test Subjects

In order to thoroughly investigate how accurate different types of users are at detecting Sybils, we ran user studies on three different groups of test subjects. Each individual tester was asked to evaluate ≥10 profiles from our dataset.

### Demographics of Testers

**Education Level:**
- Graduate
- Bachelors
- High School
- Primary

**OSN Experience:**
- 5-10 Years
- 2-5 Years
- 0-2 Years
- Never

**Gender:**
- Female
- Male

**Regional Distribution:**
- CN (China)
- IN (India)
- US (United States)

**Figure 2. Screenshot of the English version of our user study website.**

This structured and detailed approach ensures clarity and coherence in presenting the study's methodology, data, and results.