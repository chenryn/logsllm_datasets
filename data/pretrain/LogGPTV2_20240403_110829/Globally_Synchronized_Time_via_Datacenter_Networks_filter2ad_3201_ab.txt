ﬁgured. It is typically conﬁgured to resynchronize over a
period of once per second [8], which will keep network over-
head low, but on the ﬂip side, will also adversely affect pre-
cision of clock synchronization.
2.4 NTP vs. PTP vs. GPS
In this section, we compare the most popular clock syn-
chronization protocols, NTP, PTP, and GPS, in terms of the
problems of clock synchronization discussed in Section 2.3.
A summary of the comparison is in Table 1.
Precision Scalability Overhead (pckts)
Extra hardware
sub-us
None
Timing signal receivers, cables
us
ns
ns
NTP
PTP
GPS
DTP
DTP-enabled devices
Table 1: Comparison between NTP, PTP, GPS, and DTP
Good
Good
Bad
Good
PTP-enabled devices
Moderate
Moderate
None
None
2.4.1 Network Time Protocol (NTP)
The most commonly used time synchronization protocol
is the Network Time Protocol (NTP) [41]. NTP provides
millisecond precision in a wide area network (WAN) and mi-
crosecond precision in a local area network (LAN). In NTP,
time servers construct a tree, and top-level servers (or stra-
tum 1) are connected to a reliable external time source (stra-
tum 0) such as satellites through a GPS receiver, or atomic
clocks. A client communicates with one of the time servers
via UDP packets. As mentioned in Section 2.1, four times-
tamps are used to account for processing time in the time
server.
NTP is not adequate for a datacenter. It is prone to errors
that reduce precision in clock synchronization: Inaccurate
timestamping, software network stack (UDP daemon), and
network jitter. Furthermore, NTP assumes symmetric paths
for time request and response messages, which is often not
true in reality. NTP attempts to reduce precision errors via
statistical approaches applied to network jitter and asymmet-
ric paths. Nonetheless, the precision in NTP is still low.
2.4.2 Precise Time Protocol (PTP)
The IEEE 1588 Precise Time Protocol (PTP) [8]2 is an
emerging time synchronization protocol that can provide
tens to hundreds of nanosecond precision in a LAN when
properly conﬁgured. PTP picks the most accurate clock in a
network to be the grandmaster via the best master clock al-
gorithm and others synchronize to it. The grandmaster could
be connected to an external clock source such as a GPS re-
ceiver or an atomic clock. Network devices including PTP-
enabled switches form a tree with the grandmaster as the
root. Then, at each level of the tree, a server or switch be-
haves as a slave to its parent and a master to its children.
When PTP is combined with Synchronous Ethernet, which
syntonizes frequency of clocks (SyncE, See Section 8), PTP
can achieve sub-nanosecond precision in a carefully con-
ﬁgured environment [39], or hundreds of nanoseconds with
tens of hops in back-haul networks [38].
The protocol normally runs as follows: The grandmas-
ter periodically sends timing information (Sync) with IP
multicast packets. Upon receiving a Sync message which
contains time t0, each client sends a Delay_Req mes-
sage to the timeserver, which replies with a Delay_Res
message. The mechanism of communicating Delay_Req
and Delay_Res messages is similar to NTP, and Figure 1.
Then, a client computes the offset and adjusts its clock or
frequency. If the timeserver is not able to accurately embed
t0 in the Sync message, it emits a Follow_Up message
with t0, after the Sync message, to everyone.
To improve the precision, PTP employs a few techniques.
2We use PTPv2 in this discussion.
First, PTP-enabled network switches can participate in the
protocol as Transparent clocks or Boundary clocks in or-
der to eliminate switching delays. Transparent clocks times-
tamp incoming and outgoing packets, and correct the time
in Sync or Follow_Up to reﬂect switching delay. Bound-
ary clocks are synchronized to the timeserver and work as
masters to other PTP clients, and thus provide scalability to
PTP networks. Second, PTP uses hardware timestamping
in order to eliminate the overhead of network stack. Mod-
ern PTP-enabled NICs timestamp both incoming and outgo-
ing PTP messages [13]. Third, a PTP-enabled NIC has a
PTP hardware clock (PHC) in the NIC, which is synchro-
nized to the timeserver. Then, a PTP-daemon is synchro-
nized to the PHC [21, 45] to minimize network delays and
jitter. Lastly, PTP uses smoothing and ﬁltering algorithms to
carefully measure one way delays.
As we demonstrate in Section 6.1, the precision provided
by PTP is about few hundreds of nanoseconds at best in a
10 GbE environment, and it can change (decrease) over time
even if the network is in an idle state. Moreover, the preci-
sion could be affected by the network condition, i.e. vari-
able and/or asymmetric latency can signiﬁcantly impact the
precision of PTP, even when cut-through switches with pri-
ority ﬂow control are employed [51, 52]. Lastly, it is not
easy to scale the number of PTP clients. This is mainly
due to the fact that a timeserver can only process a limited
number of Delay_Req messages per second [8]. Bound-
ary and Transparent clocks can potentially solve this scal-
ability problem. However, precision errors from Boundary
clocks can be cascaded to low-level components of the tim-
ing hierarchy tree, and can signiﬁcantly impact the precision
overall [30]. Further, it is shown that Transparent clocks
often are not able to perform well under network conges-
tion [52], although a correct implementation of Transparent
clocks should not degrade the performance under network
congestion.
2.4.3 Global Positioning System (GPS)
In order to achieve nanosecond-level precision, GPS can
be employed [4, 22]. GPS provides about 100 nanosecond
precision in practice [37]. Each server can have a dedicated
GPS receiver or can be connected to a time signal distri-
bution server through a dedicated link. As each device is
directly synchronized to satellites (or atomic clocks) or is
connected via a dedicated timing network, network jitter and
software network stack is not an issue.
Unfortunately, GPS based solutions are not realistic for an
entire datacenter. It is not cost effective and scalable because
of extra cables and GPS receivers required for time signals.
Further, GPS signals are not always available in a datacenter
as GPS antennas must be installed on a roof with a clear view
to the sky. However, GPS is often used in concert with other
protocols such as NTP and PTP and also DTP.
2.5 Datacenter Time Protocol
Why the PHY?
(DTP):
Our goal is to achieve nanosecond-level precision as in
GPS, with scalability in a datacenter network, and without
Device 1
TX CLK
Device 2
TX CLK
RX PHY
TX PHY
RX PHY
TX PHY
Figure 2: Clock domains of two peers. The same color rep-
resents the the same clock domain.
any network overhead. We achieve this goal by running a
decentralized protocol in the physical layer (PHY).
DTP exploits the fact that two peers3 are already synchro-
nized in the PHY in order to transmit and receive bitstreams
reliably and robustly. In particular, the receive path (RX)
of a peer physical layer recovers the clock from the phys-
ical medium signal generated by the transmit path (TX) of
the sending peer’s PHY. As a result, although there are two
physical clocks in two network devices, they are virtually
in the same circuit (Figure 2; What each rectangle means is
explained in Section 4.1).
Further, a commodity switch often uses one clock oscilla-
tor to feed the sole switching chip in a switch [2], i.e. all TX
paths of a switch use the same clock source. Given a switch
and N network devices connected to it, there are N +1 phys-
ical oscillators to synchronize, and all of them are virtually
in the same circuit.
As delay errors from network jitter and a software net-
work stack can be minimized by running the protocol in the
lowest level of a system [48], the PHY is the best place to
reduce those sources of errors. In particular, we give three
reasons why clock synchronization in the PHY addresses the
problems in Section 2.3.
First,
the PHY allows accurate timestamping at sub-
nanosecond scale, which can provide enough ﬁdelity for
nanosecond-level precision. Timestamping [27, 36] in the
PHY achieves high precision by counting the number of bits
between and within packets. Timestamping in the PHY re-
lies on the clock oscillator that generates bits in the PHY,
and, as a result, it is possible to read and embed clock coun-
ters with a deterministic number of clock cycles in the PHY.
Second, a software network stack is not involved in the
protocol. As the physical layer is the lowest layer of a net-
work protocol stack, there is always a deterministic delay
between timestamping a packet and transmitting it. In ad-
dition, it is always possible to avoid buffering in a network
device because protocol messages can always be transmitted
when there is no other packet to send.
Lastly, there is little to no variation in delay between two
peers in the PHY. The only element in the middle of two
physically communicating devices is a wire that connects
them. As a result, when there is no packet in transit, the de-
lay in the PHY measured between two physically connected
devices will be the time to transmit bits over the wire (propa-
gation delay, which is always constant with our assumptions
in Section 3.1), a few clock cycles required to process bits
in the PHY (which can be deterministic), and a clock do-
main crossing (CDC) which can add additional random de-
lay. A CDC is necessary for passing data between two clock
domains, namely between the TX and RX paths. Synchro-
nization FIFOs are commonly used for a CDC. In a synchro-
nization FIFO, a signal from one clock domain goes through
multiple ﬂip-ﬂops in order to avoid metastability from the
other clock domain. As a result, one random delay could be
added until the signal is stable to read.
Operating a clock synchronization protocol in the physi-
cal layer not only provides the beneﬁts of zero to little de-
lay errors, but also zero overhead to a network: There is no
need for injection of packets to implement a clock synchro-
nization protocol. A network interface continuously gener-
ates either Ethernet frames or special characters (Idle char-
acters) to maintain a link connection to its peer. We can ex-
ploit those special characters in the physical layer to transmit
messages (We will discuss this in detail in Section 4). The
Ethernet standard [9] requires at least twelve idle characters
(/I/) between any two Ethernet frames regardless of link
speed to allow the receiving MAC layer to prepare for the
next packet. As a result, if we use these idle characters to
deliver protocol messages (and revert them back to idle char-
acters), no additional packets will be required. Further, we
can send protocol messages between every Ethernet frame
without degrading the bandwidth of Ethernet and for differ-
ent Ethernet speeds (See Section 7).
3. DATACENTER TIME PROTOCOL
In this section, we present the Datacenter Time Protocol
(DTP): Assumptions, protocol, and analysis. The design
goals for the protocol are the following:
• Internal synchronization with nanosecond precision.
• No network overhead: No packets are required for the
synchronization protocol.
3.1 Assumptions
We assume, in a 10 Gigabit Ethernet (10 GbE) network,
all network devices are driven by oscillators that run at
slightly different rates due to oscillator skew, but operate
within a range deﬁned by the IEEE 802.3 standard. The stan-
dard requires that the clock frequency fp be in the range of
[f − 0.0001f, f + 0.0001f ]4 where f is 156.25 MHz in 10
GbE (See Section 4.1).
We assume that there are no “two-faced” clocks [34] or
Byzantine failures which can report different clock counters
to different peers.
We further assume that the length of Ethernet cables is
bounded and, thus, network propagation delay is bounded.
The propagation delay of optic ﬁber is about 5 nanoseconds
per meter (2/3 × the speed of light, which is 3.3 nanosec-
onds per meter in a vacuum) [31]. In particular, we assume
the longest optic ﬁber inside a datacenter is 1000 meters, and
as a result the maximum propagation delay is at most 5 us.
Most cables inside a datacenter are 1 to 10 meters as they
are typically used to connect rack servers to a Top-of-Rack
(ToR) switch; 5 to 50 nanoseconds would be the more com-
mon delay.
3two peers are two physically connected ports via a cable.
4This is ±100 parts per million (ppm).
Algorithm 1 DTP inside a network port
Algorithm 2 DTP inside a network device / switch
STATE:
gc : global counter, from Algorithm 2
lc ← 0 : local counter, increments at every clock tick
d ← 0 : measured one-way delay to peer p
TRANSITION:
T0: After the link is established with p
lc ← gc
Send (Init, lc)
T1: After receiving (Init, c) from p
Send (Init-Ack, c)
T2: After receiving (Init-Ack, c) from p
d ← (lc − c − α)/2
T3: After a timeout
Send (Beacon, gc)
T4: After receiving (Beacon, c) from p
lc ← max(lc, c + d)
3.2 Protocol
In DTP, every network port (of a network interface or a
switch) has a local counter in the physical layer that incre-
ments at every clock tick. DTP operates via protocol mes-
sages between peer network ports: A network port sends a
DTP message timestamped with its current local counter to
its peer and adjusts its local clock upon receiving a remote
counter value from its peer. We show that given the bounded
delay and frequent resynchronizations, local counters of two
peers can be precisely synchronized in Section 3.3.
Since DTP operates and maintains local counters in the
physical layer, switches play an important role in scaling up
the number of network devices synchronized by the proto-
col. As a result, synchronizing across all the network ports
of a switch (or a network device with a multi-port network
interface) requires an extra step: DTP needs to synchronize
the local counters of all local ports. Speciﬁcally, DTP main-
tains a global counter that increments every clock tick, but
also always picks the maximum counter value between it and
all of the local counters.
DTP follows Algorithm 1 to synchronize the local coun-
ters between two peers. The protocol runs in two phases:
INIT and BEACON phases.
INIT phase The purpose of the INIT phase is to measure
the one-way delay between two peers. The phase begins
when two ports are physically connected and start communi-
cating, i.e. when the link between them is established. Each
peer measures the one-way delay by measuring the time be-
tween sending an INIT message and receiving an associ-
ated INIT-ACK message, i.e. measure RTT, then divide the
measured RTT by two (T0, T1, and T2 in Algorithm 1).
As the delay measurement is processed in the physical
layer, the RTT consists of a few clock cycles to send / re-
ceive the message, the propagation delays of the wire, and
the clock domain crossing (CDC) delays between the receive
and transmit paths. Given the clock frequency assumption,
and the length of the wire, the only non-deterministic part
is the CDC. We analyze how they affect the accuracy of the
measured delay in Section 3.3. Note that α in Transition 2
in Algorithm 1 is there to control the non-deterministic vari-
ance added by the CDC (See Section 3.3).
STATE:
gc: global counter
{lci}: local counters
TRANSITION:
T5: at every clock tick
gc ←max(gc + 1, {lci})
BEACON phase During the BEACON phase, two ports peri-
odically exchange their local counters for resynchronization
(T3 and T4 in Algorithm 1). Due to oscillator skew, the
offset between two local counters will increase over time.
A port adjusts its local counter by selecting the maximum
of the local and remote counters upon receiving a BEACON