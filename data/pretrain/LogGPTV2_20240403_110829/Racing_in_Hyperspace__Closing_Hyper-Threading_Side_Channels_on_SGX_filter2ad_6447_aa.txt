title:Racing in Hyperspace: Closing Hyper-Threading Side Channels on SGX
with Contrived Data Races
author:Guoxing Chen and
Wenhao Wang and
Tianyu Chen and
Sanchuan Chen and
Yinqian Zhang and
XiaoFeng Wang and
Ten-Hwang Lai and
Dongdai Lin
2018 IEEE Symposium on Security and Privacy
Racing in Hyperspace: Closing Hyper-Threading
Side Channels on SGX with Contrived Data Races
Guoxing Chen∗, Wenhao Wang†‡, Tianyu Chen†, Sanchuan Chen∗, Yinqian Zhang∗,
XiaoFeng Wang†, Ten-Hwang Lai∗, Dongdai Lin‡
∗The Ohio State University, †Indiana University Bloomington
‡SKLOIS, Institute of Information Engineering, Chinese Academy of Sciences
{chen.4329,chen.4825,zhang.834,lai.1}@osu.edu, {wangwenhao,ddlin}@iie.ac.cn, {chen512,xw7}@indiana.edu
Abstract—In this paper, we present HYPERRACE, an LLVM-
based tool for instrumenting SGX enclave programs to eradicate
all side-channel threats due to Hyper-Threading. HYPERRACE
creates a shadow thread for each enclave thread and asks the
underlying untrusted operating system to schedule both threads
on the same physical core whenever enclave code is invoked,
so that Hyper-Threading side channels are closed completely.
Without placing additional trust in the operating system’s CPU
scheduler, HYPERRACE conducts a physical-core co-location test:
it ﬁrst constructs a communication channel between the threads
using a shared variable inside the enclave and then measures the
communication speed to verify that the communication indeed
takes place in the shared L1 data cache—a strong indicator of
physical-core co-location. The key novelty of the work is the
measurement of communication speed without a trustworthy
clock; instead, relative time measurements are taken via contrived
data races on the shared variable. It is worth noting that the
emphasis of HYPERRACE’s defense against Hyper-Threading
side channels is because they are open research problems. In
fact, HYPERRACE also detects the occurrence of exception- or
interrupt-based side channels, the solutions of which have been
studied by several prior works.
I. INTRODUCTION
The growing demands for secure data-intensive computing
and rapid development of hardware technologies bring in
a new generation of hardware support for scalable trusted
execution environments (TEE), with the most prominent ex-
ample being Intel Software Guard Extensions (SGX). SGX
is a set of CPU instructions that enable a user-land process
to allocate a chunk of private memory, called an enclave,
to protect its execution from the untrusted operating system
(OS) and even a rogue system administrator. Sensitive data
outside the enclave are encrypted, and only decrypted within
the enclave, when they are loaded into the CPU, to avoid
direct exposure of their content to the untrusted parties (i.e.,
the OS and the administrator). With all such protection in
place, however, today’s SGX design has been found to still
leak out the program’s runtime traces through various side
channels, allowing the OS-level adversary to infer sensitive
data processed inside the enclave.
One example of such side channels is the page-fault chan-
nels [1], [2] in which the adversary with full control of the
OS can induce page faults (by manipulating the page tables
The two lead authors contribute equally to the work and are ordered alpha-
betically. Corresponding author: Wenhao Wang (PI:EMAIL).
inside the kernel) during an enclave program’s runtime, so as
to identify the secret data the program’s page access pattern
depends upon. The page-fault attacks have been improved
recently [3], [4] by monitoring the updates of accessed ﬂag in
the page table entries (PTEs) by the enclave program to infer
its page access pattern without causing page faults. Besides,
traditional micro-architectural side channels also exist in the
SGX context, including the CPU cache attacks [5], [6], [7],
[8], branch target buffer (BTB) attacks [9], cache-DRAM
attacks [4], etc. A comprehensive list of memory side channels
in SGX has been summarized in a prior paper [4].
Same-core side channels. To collect information through any
of these side channels, the adversary needs to either run the at-
tack program on the same core executing the enclave program
(same-core side channels) or monitor the victim’s operations
from a different core (cross-core side channels), depending on
the nature of the channel he uses. A prominent example of
cross-core channels is the last-level cache (LLC) [10], [11],
[12], [13], [14], in which the attack program operates on
another core and measures its own use of the LLC to infer the
victim’s cache usage. Cross-core side channels in SGX are no
different from those in other contexts, which tend to be noisy
and often harder to exploit in practice (e.g., to synchronize
with the victim). By comparison, the noise-free and easy-to-
exploit same-core side channels are uniquely threatening under
the SGX threat model. Conventional ways to exploit same-core
channels are characterized by a large number of exceptions or
interrupts to frequently transfer the control of a core back and
forth between an enclave process and the attacker-controlled
OS kernel, through a procedure called Asynchronous Enclave
Exits (AEX). Such AEX-based side-channel attacks have been
intensively studied [1], [2], [15], [9] and new defense proposals
continue to be made, often based upon detection of high
frequency AEXs [16], [17]. This feature, however, is found to
be evadable through exploiting a set of side channels enabled
or assisted by Hyper-Threading (called Hyper-Threading side-
channel attacks), which do not trigger a large number of
interrupts. To the best of our knowledge, no prior work has
successfully mitigated Hyper-Threading side channels in SGX.
This paper reports a study that aims at ﬁlling this gap,
understanding and addressing the security threats from Hyper-
Threading side channels in the SGX setting, and deriving
© 2018, Guoxing Chen. Under license to IEEE.
DOI 10.1109/SP.2018.00024
178
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:38:29 UTC from IEEE Xplore.  Restrictions apply. 
novel protection to close all Hyper-Threading side channels.
In addition, our solution seamlessly integrates with a method
to detect AEXs from within the enclave, and thus completely
eliminates all same-core side channels on SGX.
Challenges. Hyper-Threading is Intel’s simultaneous multi-
threading (SMT) technologies implemented in many of its
mainstream processors today (e.g., Xeon and Core ‘i’ Se-
ries). While Hyper-Threading greatly increases the degree of
instruction-level parallelism, by allowing two threads to share
the same physical core and hence many per-core resources, it
also enables or assists a variety of side-channel attacks. For
example, because micro-architectural resources, such as the
BTB, the translation lookaside buffer (TLB), the L1 instruction
cache and data cache, the uniﬁed L2 cache, and the ﬂoating-
point units (FPU), are shared between the two logical cores
of the same physical core, side-channel attacks that leverage
these shared resources to extract secrets are enabled [18], [19],
[20], [21], [22], [23], [24], [25]. Moreover, Hyper-Threading
facilitates some types of side-channel attacks. For example, in
the SPM attacks that monitor the accessed ﬂag of the PTEs,
an adversary may take advantage of Hyper-Threading to ﬂush
the TLB entries of the victim enclave, forcing a page table
walk and a PTE update when the memory page is visited by
the enclave program again [4].
Defending against the Hyper-Threading side-channel leaks
is challenging. Simply disabling Hyper-Threading is not an
option, because it greatly degrades the performance of the
processors, making the SGX systems less suitable for data-
intensive computing. Moreover, even if it is reasonable for
the owner of the enclaves to request the code to run only
on CPUs not supporting Hyper-Threading or with the feature
disabled,
there is no effective way for software programs
running inside SGX enclaves to verify this artifact: The
enclave code cannot execute the cpuid instruction directly
to learn the number of available cores;
the rdtscp and
rdpid instructions return the current processor ID from the
IA32_TSC_AUX register [26], which, however, is controlled
by the untrusted OS. Furthermore, these instructions are not
currently supported in the enclave mode. Remote attestation
does not cover information about Hyper-Threading, either. One
viable solution is to create a shadow thread from the enclave
program and ask the OS to schedule it on the other logical
core, so that no other process can share the same physical
core as the enclave program. However, it is very challenging
to reliably verify such a scheduling arrangement performed by
the untrusted OS. To make this approach work, we need an
effective physical-core co-location test to determine whether
two threads are indeed scheduled on the same physical core.
HYPERRACE. A micro-architecture feature we can leverage
to conduct reliable physical-core co-location tests is that
the two enclave threads running on the same physical core
can communicate (through a shared variable inside the same
enclave) with each other much faster through per-core caches
(e.g., the L1 cache) than the communication between physical
cores (or CPU packages) through the L3 cache or the memory.
However, this fast communication channel requires a reliable
and trustworthy clock to measure the communication speed,
which, unfortunately, is absent inside SGX enclaves: the SGX
version 1 processors do not support the rdtsc/rdtscp
instructions in the enclave mode; and although SGX version
2 plans to introduce support for the rdtsc/rdtscp in-
structions, the clock seems to be untrusted and can still be
changed by the OS [26, Chapter 38.6.1]. Without such a clock,
measurements of the communication speed, which are critical
for verifying the co-location of two threads on the same core,
become difﬁcult.
To address this problem, we present in this paper a unique
technique that utilizes contrived data races between two
threads of the same enclave program to calibrate their inter-
communication speed using the speed of their own executions.
More speciﬁcally, data races are created by instructing both
threads to simultaneously read from and write to a shared
variable. By carefully constructing the read-write sequences
(Sec. IV), it is ensured that when both threads operate on
the same core, they will read from the shared variable the
value stored by the other thread with very high probabilities.
Otherwise, when the threads are scheduled to different cores,
they will, with high probabilities, only observe values stored
by themselves.
The contrived data races establish an “authenticated” com-
munication channel because, ﬁrst, the shared variable is lo-
cated inside the enclave’s protected memory so that its conﬁ-
dentiality and integrity are protected by SGX, and, second, the
measurement of the channel’s communication speed is veriﬁed
by the execution speed of the communication code. The
security guarantee of this veriﬁcation lies in the adversary’s in-
ability to arbitrarily manipulate the relative speed between the
threads’ execution speed and their inter-communication speed.
Our security analysis demonstrates that even an adversary that
controls the entire OS cannot schedule the two threads on
different physical cores while ensuring they will observe data
races on the shared variable with high probabilities.
Using this technique, we designed and implemented an
LLVM-based tool, called HYPERRACE, which compiles an
enclave program from the source code and instruments it at
the intermediate representation (IR) level to conduct frequent
AEX and co-location tests during the execution of the enclave
program. The resulting binary is an enclave program that auto-
matically protects itself from all Hyper-Threading side-channel
attacks (and other same-core side-channel attacks), completely
closing such side channels. We combine an analytical security
model with empirical measurements on SGX processors to
conduct a thorough security analysis on our scheme. We
also empirically conducted several attacks to subvert the co-
location tests and found all of them can be effectively detected
by HYPERRACE. Our performance evaluation is conducted
by protecting an SGX version of nbench and Intel’s SGX
SSL library. The results suggest that the runtime overhead for
nbench applications due to the HYPERRACE’s instrumentation
in each basic block (for detecting AEXs) ranges from 42.8% to
101.8%. The runtime overhead due to co-location tests is about
179
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:38:29 UTC from IEEE Xplore.  Restrictions apply. 
3.5% (when the co-location tests were conducted 250 times
per second, triggered by benign, period system interrupts),
which grows linearly in the number of times co-location tests
are conducted. The combined runtime overhead for various
cryptographic algorithms in the SGX SSL library is 36.4%.
Contributions. We outline the contributions of the paper as
follows:
• A viable solution to an open problem. We propose a solu-
tion to the open research problem of defending against SGX
side-channel attacks on Hyper-Threading-enabled proces-
sors, and demonstrated its effectiveness.
• A novel approach to physical-core co-location tests. We
developed a new technique to conduct physical-core co-
location tests, by leveraging contrived data races to cali-
brate the communication speed between threads with the
pace of program executions.
• A turn-key solution. We developed an LLVM-based tool,
HYPERRACE, to protect enclave programs by automatically
instrumenting them with AEX and co-location detection
code.
Roadmap. The rest of the paper is organized as follows:
Sec. II provides the background of our research; Sec. III
presents an overview of HYPERRACE; Sec. IV describes our
physical-core co-location test technique; Sec. V presents the
security analysis of the co-location tests; Sec. VI elaborates
the design and implementation of HYPERRACE; Sec. VII
provides the results of performance evaluation on our proto-
type; Sec. VIII reviews the related prior research and Sec. IX
concludes the paper.
II. BACKGROUND
In this section, we describe background knowledge on cache
coherence protocols, store buffers, Intel SGX and Hyper-
Threading.
Cache and memory hierarchy. Modern processors are
equipped with various buffers and caches to improve their
performance. Relevant to our discussion are cache coherence
protocols and the store buffer.
• Cache coherence protocols. Beginning with the Pentium
processors, Intel processors use the MESI cache coherence
protocol to maintain the coherence of cached data [26].
Each cache line in the L1 data cache and the L2/L3 uniﬁed
caches is labeled as being in one of the four states deﬁned
in Table I. When writing to a cache line labeled as Shared
or Invalid, a Read For Ownership (RFO) operation will be
performed, which broadcasts invalidation messages to other
physical cores to invalidate the copies in their caches. After
receiving acknowledgement messages from other physical
cores, the write operation is performed and the data is
written to the cache line.
• Store Buffer. The RFO operations could incur long delays
when writing to an invalid cache line. To mitigate these
delays, store buffers were introduced. The writes will be
pushed to the store buffer, and wait to be executed when
the acknowledgement messages arrive. Since the writes
are buffered, the following reads to the same address may
not see the most up-to-date value in cache. To solve this
problem, a technique called store-to-load forwarding is
applied to forward data from the store buffer to later reads.
Intel SGX. Intel Software Guard Extensions (SGX) is new
hardware feature available on recent Intel processors that
provides an shielded execution environment, called an enclave,
to software applications, which protects conﬁdentiality and
integrity of enclave programs against privileged attackers, such
as the operating system (OS). The enclaves’ code and data
is stored in Processor Reserved Memory (PRM), a region of
the DRAM. Accesses to the memory regions belonging to
an enclave inside the PRM from any software outside of the
enclave are denied.
To switch between enclave mode and non-enclave mode,
SGX provides EENTER and EEXIT instructions to start and
terminate enclave execution. During the enclave execution,
interrupts or exceptions will cause the processor to transition
out of the enclave mode, which is called an Asynchronous
Enclave eXit (AEX). To protect the security of the enclave,
an AEX will perform a series of operations, including ﬂushing
TLBs and saving the state of certain registers in a State
Save Area (SSA) inside the enclave memory. An ERESUME
operation resumes the enclave execution after an AEX occurs.
Intel Hyper-Threading. Hyper-Threading Technology is
Intel’s proprietary implementation of simultaneous multi-
threading (SMT), which enables a single physical processor
to execute two concurrent code streams [26]. With Hyper-
Threading support, a physical core consists of two logical
cores sharing the same execution engine and the bus interface.
Each logical core has a separated architectural state, such
as general purpose registers, control registers,
local APIC
registers, etc.
are shared between the two logical cores.
Beside the shared execution engine and bus interface, the
following resources are also shared between two logical cores
of the same physical core supporting Hyper-Threading.
• Caches: the private caches (i.e., L1/L2) of a physical core
• Branch prediction units (BPU): the two logical cores share
the branch target buffer (BTB) which is a cache storing a
target address of branches.
• Translation lookaside buffers (TLB): data TLBs are shared
between two logical cores, while the instruction TLB may
be shared or duplicated depending on speciﬁc processors.
• Thermal monitors: the automatic thermal monitoring mech-
anism and the catastrophic shutdown detector are shared.
Most processors with SGX also support Hyper-Threading.
We surveyed a list of Intel processors that supports SGX and
listed the results in Table VIII (see Appendix A).
III. HYPERRACE OVERVIEW
Before diving into the design details, in this section, we
highlight the motivation of the paper, an overview of HYPER-
RACE’s design, and the threat model we consider in this paper.
180
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:38:29 UTC from IEEE Xplore.  Restrictions apply. 
Copies exists in other processors’ cache?
Cache Line State
This line is valid?
A read to this line
A write to this line
TABLE I
MESI CACHE LINE STATES.
M(Modiﬁed)
Yes
No
Cache hit
Cache hit
E(Exclusive)
Yes
No
Cache hit
Cache hit
S(Shared)
Yes
Maybe
Cache hit
Read for ownership
I(Invalid)
No
Maybe
Goes to system bus
Read for ownership
TABLE II
HYPER-THREADING SIDE CHANNELS.
Shared
Yes
Yes
No
Yes
Yes
Cleansed at AEX
Not ﬂushed
Not ﬂushed