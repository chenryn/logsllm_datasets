title:Small Parity-Check Erasure Codes - Exploration and Observations
author:James S. Plank and
Adam L. Buchsbaum and
Rebecca L. Collins and
Michael G. Thomason
Small Parity-Check Erasure Codes -
Exploration and Observations
James S. Plank † Adam L. Buchsbaum ‡ Rebecca L. Collins † Michael G. Thomason †∗
Abstract
Erasure codes have profound uses in wide- and medium-
area storage applications. While inﬁnite-size codes have
been developed with optimal properties, there remains a
need to develop small codes with optimal properties. In
this paper, we provide a framework for exploring very
small codes, and we use this framework to derive opti-
mal and near-optimal ones for discrete numbers of data
bits and coding bits. These codes have heretofore been
unknown and unpublished, and should be useful in prac-
tice. We also use our exploration to make observations
about upper bounds for these codes, in order to gain a
better understanding of them and to spur future deriva-
tions of larger, optimal and near-optimal codes.
1 Introduction
Erasure codes have been gaining in popularity, as wide-
area, Grid, and peer-to-peer ﬁle systems need to provide
fault-tolerance and caching that works more efﬁciently
and resiliently than by replication [FMS+04, GWGR04,
PT04, RWE+01, ZL02]. In a typical erasure code set-
ting, a ﬁle is decomposed into n equal sized data blocks,
and from these, m additional coding blocks of the same
size are calculated. The suite of n + m blocks is dis-
tributed among the servers of a wide-area ﬁle system,
and a client desiring to access the ﬁle need only grab f n
of these blocks in order to recalculate the ﬁle. In this
setting, f is termed the overhead factor, and has one as
its lower bound.
Reed-Solomon codes [Pla97, PD05, Riz97] are a
class of erasure codes that have ideal overhead factors
(f = 1). However, their computational overhead grows
quadratically with n and m, severely limiting their use.
Low-Density Parity-Check (LDPC) codes [LMS+97,
RU03, WK03] have arisen as important alternatives to
∗
This material
is based upon work supported by the Na-
tional Science Foundation under grants CNS-0437508, ACI-
0204007, ANI-0222945, and EIA-9972889. † Department of Com-
puter Science, University of Tennessee, Knoxville, TN, 37996,
[plank,rcollins,thomason]@cs.utk.edu;‡ AT&T Labs,
Shannon Laboratory, 180 Park Ave., Florham Park, NJ 07932,
alb@research.att.com.
Reed-Solomon codes. Although their overhead factors
are suboptimally greater than one, their computational
overheads are very low. Thus, the tradeoff between a
client having to download more than n blocks of data is
mitigated by the fact that recalculating the blocks of the
data is extremely fast, and in particular much faster than
Reed-Solomon codes.
The theory for LDPC codes has been developed for
asymptotics, proving that as n goes to inﬁnity, the over-
head factor of codes approaches its optimal value of one.
For small values of n and m (less than 1000), there is
little theory, and recent work has shown that the tech-
niques developed for asymptotics do not fare well for
small n and m [PT04].
The purpose of this paper is to start closing this hole
in the theory. Rather than concentrate on large values
of n and m, we concentrate on very small values, us-
ing enumeration and heuristics to derive either optimal
codes for these small values, or codes that are not yet
provably optimal, but represent the lowest known up-
per bounds. We present these codes as they should be
useful to the community. Additionally, we demonstrate
some properties of small codes and present observations
about the codes that we have derived. We leave the
proof/disproof of these observations as open questions
to the community.
The signiﬁcance of this work is the following:
1. To present optimal, small codes to the community.
To the authors’ knowledge, this is the ﬁrst such pre-
sentation of codes.
2. To present upper bounds on larger codes to the
community. To the authors’ knowledge, this is also
the ﬁrst such presentation of codes.
3. To present evaluation, enumeration and pruning
techniques that apply to small codes, and have not
been used on LDPC codes previously.
4. To stimulate thought on small codes in hope of
proving properties of codes in general that do not
rely upon classical asymptotic, probabilistic argu-
ments.
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:53:32 UTC from IEEE Xplore.  Restrictions apply. 
1
2 LDPC Basics
The material in this section is all well-known and has
been presented elsewhere. See [WK03] for more detail.
Although wide-area ﬁle systems use LDPC codes to
operate on blocks of data, the speciﬁcation of LDPC
codes is typically on bits of data. Blocks are simply
composed of multiple bits. In this work, we use the fol-
lowing terminology:
- The number of data bits is n.
- The number of coding bits is m.
- The total number of bits is N = n + m.
- The rate R of a code is n
N .
- The overhead o of a code is the average number
bits that must be present to decode all the bits of
the data.
- The overhead factor f of a code is o/n.
LDPC codes are based on bipartite graphs known as
“Tanner” graphs. These graphs have N nodes l1, . . . , lN
on their left side, sometimes termed the “message”
nodes, and m nodes r1, . . . , rm on their right side,
termed “check” or “constraint” nodes. Edges only con-
nect message and check nodes. An example graph is
depicted in Figure 1.
r1
l2+l4+l5+l7=0
r2
l1+l2+l3+l7=0
tematic
- Bit l7 is the exclusive-or of l1, l2 and l3 (from r2).
- Bit l5 is the exclusive-or of l2, l4 and l7 (from r1).
We present decoding as an act in a storage system.
Suppose we store each of the N bits on a different stor-
age server. Then we download bits from the storage
server at random until we have downloaded enough bits
to reconstruct the data. To decode in this manner, we
start with the Tanner graph for the code, placing val-
ues of zero in each right-hand node, and leaving the
left-hand nodes empty. When we download a bit, we
put its value into its corresponding left-hand hand node
li. Then, for each right-hand node rj to which it is
connected, we update the value stored in rj to be the
exclusive-or of that value and the value in li. We then
remove the edge (li, rj), from the graph. At the end of
this process, if there is any right-hand node with only
one incident edge, then it contains the decoded value of
the left-hand node to which it is connected, and we can
set the value of this node accordingly, and remove its
edges from the graph in the same manner as if it had
been downloaded. Clearly this is an iterative process.
When all nodes’ values have been either downloaded
or decoded, the decoding process is ﬁnished. If a code
is systematic, then the data bits are held in n of the left-
hand nodes. The number of exclusive-or/copy opera-
tions required equals the number of edges in the graph.
Encoding with a systematic graph is straightforward
– simply decode using the n data bits.
2.1 Determining Whether A Graph Is Sys-
The following algorithm determines whether or a not a
graph represents a systematic code. The algorithm iter-
ates m times:
• Select a left-hand node that has exactly one edge
If there are no such left-
to a constraint node.
hand nodes, then the graph does not represent a
systematic code. Let the left-hand node be codei
(for i equals 1 to m), and let the constraint node be
named consti.
• Remove consti and all edges incident to it.
If this algorithm iterates m times, then the graph rep-
resents a systematic code, with the m left-hand nodes
holding the coding bits, and the n remaining left-hand
nodes holding the data bits. Although the proof of cor-
rectness of this algorithm is not presented here, it should
be noted that when the n data nodes are downloaded,
constraint node constm will have one edge to it, and this
edge is from node codem. Therefore, node codem may
be decoded. When codem is decoded, and all its other
edges are removed from the graph, then node constm−1
l1
l2
l3
l4
l5
l6
l7
r3
l2+l3+l4+l6=0
Figure 1: An example Tanner graph for n = 4 and m =
3.
The left-hand nodes hold the bits that are to be stored
by the application. The edges and the right-hand nodes
specify constraints that the left-hand nodes must satisfy.
The most straightforward codes are “systematic” codes,
where the data bits are stored in n of the left-hand nodes,
and the coding bits in the remaining m left-hand nodes
are calculated from the data bits and the constraints in
the right-hand nodes using exclusive-or.
For example the code in Figure 1 is a systematic one,
whose data bits may be stored in nodes l1 through l4.
The coding bits are calculated as follows:
- Bit l6 is the exclusive-or of l2, l3 and l4 (from r3).
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:53:32 UTC from IEEE Xplore.  Restrictions apply. 
has only edge to it, and this edge is from node codem−1.
Decoding completes in this manner.
3 Ways of Computing Overhead
#1: Brute-Force Enumeration: One can compute
overhead in a brute-force fashion, by enumerating all
N! download sequences of bits, and averaging the num-
ber of bits requred to decode the data in each se-
quence. Obviously, this process becomes computation-
ally intractible for rather small n and m. One may use
Monte-Carlo simulation to approximate the overhead, as
in [PT04]. However, there are alternative ways of com-
puting overhead.
#2: Recursive Overhead Calculation: In this section,
we specify a technique to compute overhead recursively.
Before making this speciﬁcation, we give a more pre-
cise speciﬁcation of the decoding process. We are given
a Tanner graph G with N = n + m left-hand nodes
and m right-hand nodes, each of which may hold a bit.
We assume that all the right-hand nodes have either zero
incident edges or more than one incident edge. If a left-
hand node has zero edges, then we assume that we know
its value as a result of a previous decoding phase, but
that we have not downloaded it.
When we start, we set the value of all right-hand
nodes to zero and leave the values of all left-hand nodes
blank.
To decode, we deﬁne two operations on graphs: as-
signing a value to a node, and downloading a node.
Both operations are deﬁned only on left-hand nodes.
We start with the former. Given a left-hand node li,
when the value of that node becomes known, it should
be assigned. When it is assigned, for each right-hand
node rj to which li is connected, rj’s value is set to
the exclusive-or of its previous value and li’s value, and
then the edge (li, rj) is removed from the graph. If there
is any right-hand node rj which now has only one inci-
dent edge, then the value of the left-hand node to which
rj is connected may now be assigned to be the value
of rj. Before assigning the value, however, the edge
between that node and rj should be removed, and rj
should also be removed from the graph. Note: assign-
ing one node’s value can therefore result in assigning
many other nodes’ values.
To download a node, if the node’s value has already
been assigned, then the node is simply removed from
the graph. Otherwise, the value of the node is assigned
to its downloaded value, and it is then removed from the
graph.
When the values of all left-hand nodes have been as-
signed, the decoding process is ﬁnished.
Recursively computing the overhead o(G) of graph
G proceeds as follows. If all nodes have zero edges, then
the overhead is zero. Otherwise, we simulate download-
ing each left-hand node of the graph and compute the av-
erage overhead as the average of all simulations. When
we simulate downloading a node li, we assign its value
(if unassigned), potentially decoding other nodes in the
graph, and remove the node from the graph. We are then
left with a residual graph, R(G, li). We can recursively
determine R(G, li)’s overhead. Then, the equation for
determining a graph’s overhead (if not zero), is:
(cid:3)
o(G) =
(1 + o(R(G, li)))
/N.
(cid:1)
N(cid:2)
i=1
#3: Using Residual Graphs: A third way to compute
overhead is to look at a variation of the residual graph,
presented above. Let Sn(G) be the set of all subsets of
the left-hand nodes of G that contain exactly n nodes.
Let S ∈ Sn(G). We deﬁne the residual graph RS to
be the graph that remains when all the nodes in S and
their accompanying edges are removed from G. Note
that unlike the residual graph above, we do not perform
decoding when we remove nodes from the graph. RS
simply contains the nodes in G that are not in S.
We may calculate the overhead of RS in either of
the two manners described above. Let that overhead
be o(RS). Note: the ﬁrst step in doing so will be to
decode right-hand nodes that are incident to only one
left-hand node, and this overhead may well be zero (for
example, o(R{l1,l2,l3,l4}) = 0 for the graph in Figure 1).
Now, the overhead of a graph G may be deﬁned as n
plus the average overhead of the residual graphs that re-
sult when every subset of n nodes is removed from G.
Formally:
(cid:1)(cid:4)
(cid:3)
.
S∈Sn(G) o(RS)
(cid:5)
(cid:6)
N
n
o(G) = n +
Note that this use of residual graphs is similar to us-
ing stopping sets [DPT+02] for overhead analysis.
4 Special Cases: m = 1 and n = 1
When m = 1, there is one coding node to n data nodes,
m=1 =
and the optimal code is a straight parity code: Gn
({l1, . . . , ln+1, r1},{(l1, r1), . . . , (ln+1, r1)}).
One
m=1) =
may easily prove using residual graphs that o(Gn
n. Whenever n nodes are removed from Gn
m=1, the
residual graph contains one node with one edge to r1.
Clearly, the overhead of that graph is zero, and therefore
the overhead of Gn
m=1 is the optimal value n.
When n = 1, there is one data node to m cod-
ing nodes, and the optimal code is a replication code:
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:53:32 UTC from IEEE Xplore.  Restrictions apply. 
m
= ({l1, . . . , lm+1, r1, . . . , rm}, En=1
m ), where
Gn=1
m = {(l1, ri)|1 ≤ i ≤ m} ∪ {(li + 1, ri)|1 ≤ i ≤
En=1
m}.
m ) = 1.
It is straightforward to prove that o(Gn=1
Again, we use residual graphs. Suppose l1 and all its
edges are removed from the graph. Then the residual
graph has exactly one edge to every right-hand node,
and it may be decoded completely. Suppose instead
that li(cid:2)=1 and its one edge is removed from the graph.
The residual graph has exactly one edge to ri=1, which
is connected to l1. Therefore, l1 and subsequently all
other nodes may be decoded. Since the overhead of all
residual graphs is zero, o(Gn=1
m ) = 1.
their values of ci: (1, 1, 0), (1, 0, 1), (0, 1, 1), (2, 0,
0), (0, 2, 0), and (0, 0, 2). The ﬁrst three of these con-
tain one right-hand node with exactly one edge, which