### Middle-Box Implementation

We have implemented the active-relay approach for StorM’s highly efficient API. The active-relay leverages Linux’s SCSI target framework to implement the pseudo-server process, while the pseudo-client process is built using the Open-iSCSI framework. The iSCSI parsing logic from Open-iSCSI is reused to decapsulate and encapsulate iSCSI packets when either process receives or sends iSCSI packets.

### Semantics Reconstruction for File Systems

StorM’s semantics reconstruction functions have been implemented for Linux Ext series (Ext2, Ext3, and Ext4) file systems in iSCSI-based storage networking. StorM uses Linux’s `dumpe2fs` tool to construct an initial file-system view. The reconstruction function updates the mappings between low-level data blocks and high-level files using metadata accesses, thus maintaining an up-to-date system view. These mappings are extracted and stored in a hash table for fast searching, which is beneficial for intrusion detection systems (IDS) and monitoring services.

### Evaluation and Case Studies

#### Deployment in a Cloud Testbed

We deployed StorM in a cloud testbed based on OpenStack. Our test cloud cluster consisted of 10 physical machines, each equipped with two Intel Xeon quad-core processors and 32 GB of memory. Each machine had two 1 Gigabit Ethernet cards, one for the storage network and the other for the instance network. We used OpenStack Icehouse and deployed compute services along with StorM on each physical machine. Additionally, we set up one block storage volume service (OpenStack’s Cinder) on one of the physical hosts. A 1 TB SATA disk was used to create a physical volume, and multiple volume groups were created from this physical volume through OpenStack’s Cinder service. The instance network utilized GRE tunneling for inter-host VM traffic, and tenant networking was provided by OpenStack’s Neutron service.

#### Performance Evaluation

The goal of our performance evaluation is to determine the overhead (both latency and throughput) that StorM incurs on storage traffic. This overhead is caused by the additional level of indirection introduced by StorM and the data processing (for security and reliability) within middle-boxes. To measure this overhead, we used the I/O micro-benchmark Fio [11]. Fio generates and measures a variety of file and block operations, allowing us to vary both I/O request sizes and parallelism.

##### Traffic Redirection Overhead

First, we measured the redirection overhead by comparing two scenarios: LEGACY and MB-FWD. In the LEGACY scenario, all tests were run without the StorM platform, allowing the tenant VM to communicate directly with the storage target node. In the MB-FWD scenario, StorM directed the storage traffic to the middle-box, but the middle-box did not perform any processing on the storage packets.

All Fio tests were conducted on a tenant VM with 2 vCPUs and 4 GB of memory, with a 20 GB volume attached. One middle-box was deployed for this VM’s storage volume, configured identically to the tenant VM. To measure the worst-case routing impact, we placed the middle-box VM, tenant VM, and the ingress and egress storage traffic gateways on different physical nodes.

For the LEGACY case, a direct path from the tenant VM to the storage target server was used (baseline without StorM). In the MB-FWD case, StorM introduced three extra hops: from the tenant VM to the ingress storage traffic gateway, then to the middle-box VM, and finally to the egress storage traffic gateway and the storage target server. Note that the middle-box did not perform any processing, allowing us to measure only the transmission overhead.

We varied the I/O request size of Fio from 4 KB to 256 KB to measure performance (in IOPS) and latency (in milliseconds). A representative I/O operation pattern was chosen, with 50% write and 50% read mixed in a random access manner. One Fio thread was used for each test case, ensuring that any routing overhead would be directly reflected in the single thread’s I/O performance and latency. Each test was run 10 times, and the average was taken to avoid variability in the network. The variation among the 10 repetitions was less than 5%.

As shown in Figure 4, the performance under MB-FWD was lower than in the LEGACY case, as expected due to the additional packet routing overhead. As the I/O size increased, the performance gap increased from 7% (4 KB) to 18% (256 KB). This is because larger I/O requests contain more packets, and the latency aggregates the routing delays of all packets. Figure 7 shows similar results for I/O latency, with MB-FWD resulting in slightly increased latency due to the longer forwarding path. Placing the ingress traffic gateway close to the tenant VM and the egress gateway close to the storage target server can reduce the routing overhead by approximately 20%. Additionally, the intra-host packet transfer contributes more to the routing overhead than the inter-host packet transfer, primarily due to the inefficiency of the virtualization driver, which uses a single thread per VM’s virtual interface and often causes high CPU utilization. A hardware solution like SR-IOV could significantly reduce this overhead.

##### Middle-Box Processing Overhead

Next, we measured the overhead resulting from data processing inside storage middle-boxes. Data processing overhead can be broken down into two parts: data extraction overhead (by StorM’s API) and data processing overhead (by security/reliability service logic). In this section, we focus on the performance overhead of StorM’s API, and discuss the data service processing overhead in Section V-B.