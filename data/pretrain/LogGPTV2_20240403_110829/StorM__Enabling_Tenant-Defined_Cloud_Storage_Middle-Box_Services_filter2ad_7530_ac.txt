middle-box. We have implemented the active-relay approach
for StorM’s highly efﬁcient API. The active-relay leverages
Linux’s SCSI target framework to implement the pseudo-
server process. The pseudo-client process is built with the
help of the Open-iSCSI framework. The iSCSI parsing logic
78
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:39:28 UTC from IEEE Xplore.  Restrictions apply. 
of Open-iSCSI is reused to decapsulate and encapsulate
iSCSI packets, when either process receives or sends iSCSI
packets.
We have implemented StorM’s semantics reconstruction
functions for Linux Ext series (2, 3, and 4) ﬁle systems
on iSCSI-based storage networking. StorM uses Linux’s
dumpefs2 tool to construct an initial ﬁle-system view. The
reconstruction function updates the mappings between the
low-level data blocks and its high-level ﬁles using metadata
accesses, thus maintaining an up-to-date system view. The
mappings are further extracted and stored in a Hash table
for fast searching, such as in IDS or monitoring services.
V. EVALUATION AND CASE STUDIES
We have deployed StorM in a cloud testbed based on
OpenStack. Our test cloud cluster contained 10 physical
machines each with two Intel Xeon quad-core processors
and 32 GB memory. Each machine was installed with two
1 Gigabit Ethernet cards, connecting to the storage network
and instance network, separately. We used OpenStack Ice-
house and deployed compute services as well as StorM on
each physical machine and one block storage volume service
(OpenStack’s Cinder) on one of the physical hosts. A 1
TB SATA disk was used for creating a physical volume,
and multiple volume groups were created from the physical
volume through OpenStack’s Cinder service. The instance
network used GRE tunneling for inter-host VM trafﬁc, and
tenant networking was provided by OpenStack’s Neutron
service.
A. Performance Evaluation
The goal of our performance evaluation is to determine
how much overhead (both latency and throughput overhead)
StorM incurs on the storage trafﬁc. This would be caused
by the extra level of indirection StorM has introduced on
the tenant’s storage trafﬁc and the data processing (for
security and reliability) within middle-boxes. To measure
the overhead of StorM, we used the I/O micro-benchmark
called Fio [11]. Fio generates and measures a variety of
ﬁle and block operations, and it can vary both I/O request
sizes (the amount of data read/written in a given transaction)
and parallelism (the number of threads issuing I/O requests
simultaneously).
Trafﬁc Redirection Overhead
First, we measured the
redirection overhead by comparing two cases – LEGACY
with MB-FWD. Speciﬁcally, in LEGACY, we ran all tests
without the StorM platform, and allowed the tenant VM to
communicate with the storage target node directly. In MB-
FWD, we used StorM platform to direct the storage trafﬁc
to the middle-box, but the middle-box did not perform any
processing on the storage packets.
We ran all Fio tests on the tenant VM (2 vCPUs and 4
GB memory). A 20 GB volume was created and attached
to this VM. We deployed one middle-box for this VM’s
storage volume, and the conﬁguration of the middle-box VM
was the same as the tenant VM – with 2 vCPUs and 4 GB
memory. To measure the routing impact in the worst case,
we placed the middle-box VM and the tenant VM as well as
the ingress and egress storage trafﬁc gateways on different
physical nodes.
We then measured this setup for two cases: For the
LEGACY case, a direct path from the tenant VM to the
storage target server was used (i.e., the baseline without
StorM). In the MB-FWD case, StorM introduced 3 extra
hops – the trafﬁc traveled from the tenant VM to the ingress
storage trafﬁc gateway,
the egress
storage trafﬁc gateway, and the storage target server. Note
that the middle-box did not perform any processing, allowing
us to measure only transmission overhead.
the middle-box VM,
We varied the I/O request size of Fio from 4 KB to 256
KB to measure the performance (in IOPS) and latency (in
milliseconds). A representative I/O operation pattern was
chosen — 50% write and 50% read mixed in a random
access manner. One Fio thread was used for each test case
– any routing overhead will be directly reﬂected by the single
thread’s I/O performance and latency. We ran each test 10
times and took an average to avoid any variability in the
network. The variation among the 10 repetitions is less than
5%.
In Figure 4, we observed that the workload’s performance
under MB-FWD was lower than LEGACY. This is to be
expected as the packet routing does cause some additional
overhead (a common problem for all middle-box based
solutions). Particularly, as the I/O size increased, the per-
formance gap increased — from 7% (4 KB) to 18% (256
KB). This is because an I/O request with a larger size
contains more packets, and the latency of this I/O request
aggregates the routing delays of all packets. The I/O latency
in Figure 7, shows similar results – MB-FWD resulted in
slightly increased I/O latency due to the longer forwarding
path. Recall that this is the worst case for trafﬁc redirection
as all forwarding hops of MB-FWD were distributed across
different physical hosts. We ﬁnd that the routing overhead
can be reduced by ∼20% by placing the ingress trafﬁc
gateway close to the tenant VM and the egress gateway
close to the storage target server (e.g., in the same physical
host). Also, (because the middle-box is not performing any
processing) StorM’s active-relay approach is not in use, and
as we will show next, this effectively avoids most routing
overhead.
Another key observation is that the intra-host packet trans-
fer contributes more to the routing overhead than the inter-
host packet transfer. The main reason for this behavior is due
to the fact that the virtualization driver, for copying network
packets (from hypervisor to guest VMs), is less efﬁcient —
it uses a single thread per VM’s virtual interface and usually
causes high CPU utilization. A hardware solution (e.g., SR-
IOV) could greatly reduce this overhead.
79
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:39:28 UTC from IEEE Xplore.  Restrictions apply. 
 1.40
 1.20
 1.00
I
 0.80
d
e
z
 0.60
S
P
O
i
l
a
m
r
o
N
 0.40
 0.20
 -
0.93 
0.86 
0.83 
0.82 
1.01 
1.00 
1.06 
1.14 
S
P
O
I
d
e
z
i
l
a
m
r
o
N
1.40
1.20
1.00
0.80
0.60
0.40
0.20
0.00
1.27
1.39
1.06
1.10
S
P
O
I
d
e
z
i
l
a
m
r
o
N
1.40
1.20
1.00
0.80
0.60
0.40
0.20
0.00
4 KB
16 KB
64 KB
256 KB
4 KB
16 KB
64 KB
256 KB
4
8
16
32
LEGACY
MB-FWD
MB-FWD MB-PASSIVE-RELAY MB-ACTIVE-RELAY
MB-FWD MB-PASSIVE-RELAY MB-ACTIVE-RELAY
Figure 4: The routing overhead com-
parison (IOPS) with various I/O sizes
(one thread). Higher is better.
Figure 5: The processing overhead
comparison (IOPS) with various I/O
sizes (one thread). Higher is better.
Figure 6: The processing overhead
comparison (IOPS) with various I/O
threads (16 KB size). Higher is better.
 1.40
 1.20
 1.00
)
s
m
(
y
c
n
e
i
l
a
m
r
o
N
 0.80
t
a
L
 0.60
d
e
z
 0.40
 0.20
 -
1.22 
1.25 
1.30 
1.08 
0.98
1.01
0.94
0.89
)
s
m
(
y
c
n
e
t
a
L
d
e
z
i
l
a
m
r
o
N
1.40
1.20
1.00
0.80
0.60
0.40
0.20
0.00
1.40
1.20
1.00
)
s
m
(
y
c
n
e
t
0.80
a
L
0.60
d
e
z
0.40
i
l
a
m
r
o
N
0.20
0.00
0.95
0.91
0.79
0.70
4 KB
16 KB
64 KB
256 KB
4 KB
16 KB
64 KB
256 KB
4
8
16
32
LEGACY
MB-FWD
MB-FWD MB-PASSIVE-RELAY MB-ACTIVE-RELAY
MB-FWD MB-PASSIVE-RELAY MB-ACTIVE-RELAY
Figure 7: The routing overhead com-
parison (latency) with various I/O
sizes (one thread). Lower is better.
Figure 8: The processing overhead
comparison (latency) with various I/O
sizes (one thread). Lower is better.
Figure 9: The processing overhead
comparison (latency) with various I/O
threads (16 KB size). Lower is better.
Middle-box Processing Overhead
Next, we measured
the overhead resulting from data processing inside stor-
age middle-boxes. Data processing overhead can be fur-
ther broken down into two parts: data extraction overhead
(by StorM’s API), and data processing overhead (by se-
curity/reliability service logic). In this section, we focus
on the performance overhead of StorM’s API, and discuss
the data service processing overhead in Section V-B. To