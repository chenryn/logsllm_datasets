~ 
sistent and access would be provided through conventional 
disk interface operating system calls. 
A  filesysteddisk  level  implementation  of  PM  has  the 
same application  transparency  advantage as the  file  buffer 
cache.  In  this  case  files  which  require  persistence  would 
be  located  on  the  PM  disk,  while  non-critical  files  could 
employ traditional  disks  with  write-back  behaviour.  This 
realization also has the same advantage of natural blocking 
and so transaction  granularity is simplified, though the size 
of disk blocks may not be appropriate for some applications. 
The issue of commercial operating system modification 
is  alleviated  in this realization.  The majority  of  operating 
system  functionality  to be  added  can  be  implemented as 
a  set of  disk  drivers, which'is a  well  understood  problem 
which does not come with the headache of a non-standard 
buffer cache or filesystem implementation. 
Also with the PM disk realization provided through stan- 
dard  interface  disk  drivers,  arbitrary  vendor  or  industry 
filesystems can be layered on top of the PM disk. 
For these reasons of transparency  and operating system 
impact, we chose the Disk Interface as the point to  imple- 
ment Persistent Memory for the DNCP platform. 
3. Persistent Memory Architecture 
.-l-I 
I
I
V 
RAM disk 
Driver enttry. . 
points 
Bl$l;y 
PM Core IPI  . - 
4 . .  PM Admin Library API 
I 
I 
 t 
I 
I
t
I 
t
Block  Copy Interface 
PM Core 
, 
I 
L  - .J 
. I  pminit 
'  Base Kernel Hooks 
Figure 1. Persistent Memory Architecture 
The portions identified by  bold-italic type in  the  figure 
are the componeiits of the Persistent Memory Project. 
The memory layout used by  PM is as follows:  The PM 
region  is a continuous area of  physical  (and virtual) mem- 
ory. The size of the region is fixed during normal operation 
and can only be grown by a reboot of the system.  PM seg- 
ments are then allocated from the PM region. The first seg- 
ment is known as the base segment and holds PM metadata. 
Additional  segments can be defined while the system runs 
(as long as sufficient memory exists in the PM region). 
The basic  components of  PM  are  pminit and  the base 
kernel  hooks.  The  hooks  are  a  small  change to  the  HP- 
UX  startup code  which,  on systems  with  persistent  mem- 
ory installed, calls out to pminit early. in the boot sequence. 
The pminit module is statically  linked  into the kernel  and 
is called  twice during the boot sequence.  At  the first call, 
pminit senses  the  PM  configuration  (from  the  bootloader 
configuration file)  and  allocates  the  appropriate  range  of 
physical memory for the  PM region.  The second call sets 
up equivalent virtual mappings to the region. This preserves 
the contents and virtual address location of the region across 
system reboots. 
The next level  of PM implementation  is known  as pm- 
core: which is implemented as a dynamically loadable ker- 
nel  module  (DLKM). The  pmcore module  manages  the 
persistent memory region and allocates segments of that re- 
gion  to  pmcore clients.  Calls  to  pmcore allow  segment 
creation, destruction, and resizing. A caller may  attach to a 
segment to copy, read, or write the segment. Segments may 
also be marked as read-only,or read-write.  Calls also exist 
for region management (grow, shrink).  The pmcore mod- 
ule implements the base segment to hold PM metadata; both 
checksums and replicated  data are used for reliable storage 
of this metadata. 
The third level of PM is the Block Copy Interface Mod- 
ule,  lor  pmbc,  which  provides  a  block  access  abstration 
from pmcore. The pmbc module may be called from the 
PM EUM Disk Driver or directly from user programs (via 
the PM management library).  Like pmcore, pmbc is im- 
plem'ented as a dynamically  loadable kernel module.  Calls 
to pmbc allow creation or destruction of a block-type seg- 
ment  (pmbc then  calls  pmcore to  allocate the  segment). 
Transactional  interfaces are provided  to  access the persis- 
tent memory.  A  reader-writer locking scheme is provided 
to synchronize access.  A transaction may consist of multi- 
ple writes; either all writes  in  a transaction  will succeed or 
none of them. This ensures that the segment is consistent in 
the event of a system interruption. Further, per-block check- 
sums are computed following every write so that corruption 
of the segment may be detected. 
Above the Block Copy module is the RAM disk interface 
module, or pmramd, which provides disk I/O semantics to 
user programs. This again is implemented as a DLKM. The 
standard disk driver entry points are supported, and thus any 
type 'of filesystem may be mounted on pmramd. 
A  PM management library  (libpm) is .provided for use 
by  the  PM  administration  utility  (pmadmin) and  by  user 
programs.  This library provides friendly  interfaces to both 
pmcore and pmbc. The pmadmin utility  is used to admin- 
ister PM, and can be used to change the region size. 
With  the  libpm interface  to pmcore and to  pmbc, we 
leave  open  the  possiblity  of  other  realization  beyond  the 
provided RAM disk.  These may also be  invoked from the 
490 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:59:04 UTC from IEEE Xplore.  Restrictions apply. 
kernel to allow implementation of a Recovery Box or future 
integration into the buffer cache. 
4. Performance 
4.1. Failure Free Performance 
An  evaluation  copy  of  the  TimesTen  main  memory 
database product was obtained in order to test performance. 
The Wisconsin  Benchmark was  used  with  20,000 rows in 
the database. This benchmark measures single-userkingle- 
operation  response  times  for  a  variety  of  different  SQL 
database operations.  The TimesTen database may be con- 
figured for either non-durable or durable mode.  In durable 
mode,  disk  checkpoints and  transaction  logs  are  used  to 
ensure complete data and transaction  recoverability  in  the 
event of system failure. 
300 
256 
Figure 2. Wisconsin Benchmark 
We  performed  three runs:  non-durable mode,  durable 
mode  with  disk,  and durable mode  with  persistent mem- 
ory.  In  all  tests,  we  used  tests  26-32  of  the  benchmark 
as best representing operations and representative workload 
performed by an important customer. For the PM case, both 
the checkpoint files and  the  transaction  logs  were kept  in 
PM. 
As can be seen from the graph, enabling durable mode 
can have  the  effect  of crippling performance of the main- 
memory database.  With  use  of the  PM ramdisk function, 
performance of the durable case can be improved to within 
16 percent of non-durable mode.  This is a 38% execution 
time savings over the MMDB in durable mode. This makes 
Table 1. Recovery Performance 
1 Test 
1  user 
11  m m a n  disk  I  0.60s 
0.59s 
0.10s 
0.09s 
mmap, pm 
read, disk 
read, pm 
I
,
system 
real 
1.72s  17.68s 
1.36s 
1.95s 
1.66s  11.69s 
1.32s 
1.24s 
relative  I] 
II 
11% 
11% 
PM an attractive  option for databases incurring a high up- 
date rate. 
4.2. Failure Recovery Performance 
The metric we chose  to  use  for this experiment was  to 
compare the time a sample application might take to recover 
state from disk to the time that same application might take 
to  recover state from persistent  memory.  Note  this  might 
occur either following an  application  failure  or following 
an operating system failure. Since the implementation was 
directed for a hardware fault-tolerant system, hardware fail- 
ures were not considered a factor. 
We performed two measurements, each utilizing a differ- 
ent method to access the data. 
The first experiment used mmap( 1) to map the applica- 
tion data into memory. We then touched each page to ensure 
the data was in fact available to the application.  The second 
experiment used  file  system  reads  to  obtain  the  data into 
application memory.  In  each case, a  IOOMB  datafile  was 
used.  The results for both mmap and disk reads, both  with 
and without PM are presented in Table 1. 
As can  be  seen, the relative  savings are substantial, as 
one might expect given the relative speeds of disk and mem- 
ory.  However,  the raw wall clock saving is not substantial 
when compared to the overall cost of a system reboot (about 
ten minutes with HP-UX). 
4.3. Discussion 
Although the improvement in availability is non-zero, it 
is not significant  when compared to the total  time for a re- 
covery  from a system software failure.  The relative gain is 
greater for the case of application failure and recovery, but 
still the key win is in improved performance in the failure- 
free scenario. 
Moreover, since a software failure is presumed to be an 
infrequent event in  a  well-tested  environment, the  incre- 
mental gain in availability  is minimal. 
491 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:59:04 UTC from IEEE Xplore.  Restrictions apply. 
i 
[6]  IE.  N.  Elnozahy, D. B. Johnson, and  W. ,Zwaenepoel.  The 
performance of consistent! checkpointing. In Proceedings of 
!he 1992 Symposium on Reliable Distributed Systems, pages 
.39-47, October 1992. 
[7]  H. Garcia-Molina and  K.' Salem.  Main  memory  database 
:systems: An overview.  IEEE  Transactions on Knowledge 
,and Data Engineering, 1992. 
[SI  .I.  Gray  and  A. Reuter.  Transaction Processing:  Concepts 
,mid Techniques.  Morgan Kaufmann Publishers Inc., 1993. 
[9]  'T. Haerder and A. Reuter. Principles of transaction-oriented 
'database recovery.  AChi  Computing  Surveys,  15(4):287- 
317, December 1983. 
IO]  R. Koo and S. Toueg. Checkpointing and rollback-recovery 
for distributed systems. IEEE Transactions on Software Enr 
gineering, SE-1 3(1):23-3 1, January 1987. 
111  D.  E.  Lowell  and  P.  M.  Chen.  Free transactions with  rio 
vista.  In Proceedings of  the 1997 Symposium on Operating 
Systems Principles (SOSP), October 1997. 
121  D. E. Lowell and P.  M.  ?hen.  Discount checking:  Trans- 
parent,  low-overhead  recovery  for  general  applications. 
Technical Report CSE-TR-410-99, University of  Michigan, 
November 1998. 
[13]  W.  T.  Ng  and  P.  M.  Chen. 
Integrating  reliable  memory 
in  databases.  VLDB Journal:  Very Large  Data  Bases, 
7(3): 194-204,  1998. 
[14]  J.  S. Plank, M. Beck, and G. Kingsley.  Libckpt: Transpar- 
ent checkpointing under unix.  In Proceedings of the Winter 
1995 USENIX  Conference, January 1995. 
151  E Schneider. Implementing fault-tolerant services using the 
state machine approach:  A tutorial.  ACM  Computing Sur- 
veys, 22(4), December 1990. 
161  D. P. Siewiorek and R. S .  Swarz, editors. Reliable Computer 
Systems: Design and Evaluation, chapter The Stratus case: 
The Stratus architecture by  Steven Webber, pages 648-670. 
Digital Press, 1992. 
171  Y.-M. Wang, Y.  Huang, K.-P. Vo, P.-Y. Chung, and C. Kin- 
tala.  Checkpointing and its applications.  In Proceedings  of 
the 1995 International Symposium  on Fault-Tolerant  Com- 
puting Systems (FTCS), June 1995. 
5. Conclusions and Ongoing Work 
We  set out to solve an availability problem  encountered 
by  customers  who  encountered  long  application  restart 
times  due  to  database  reloads,  however  at  the  end of, the 
day we solved a performance problem for other customers. 
The gains in durable update performance are significant and 
enable greatly improved throughput (measured in call han- 
dling volume) of key applications. 
As processor performance continues to outstrip disk per- 
formance, the  ability  to  move  stable  storage  from disk  to 
memory is expected to be a key success factor in designing 
future systems.  This need  for high performance database 
log storage is only exacerbated with the increase in the de- 
gree of multiprocessing now found in high end servers. 
The overall project has been a success and ongoing work 
explores  some additional directions.  In  particular,  we  are 
actively exploring a memory based realization  with the in- 
tegration  of  an  MMDB. We  are also adding protection  of 
the  memory  within  the  kernel  by  using  separate PA-RISC 
space identifiers to protect the memory region from poten- 
tial corruption during an operating system fault. 
6. Acknowledgements 
The entire development organization at Lucent San Jose 
Labs made the  work presented  here a' reality  and must be 
acknowledged. In particular we would like to thank Deepak 
Gupta, Derek Godfrey, and Dave Clough. We are also grate- 
ful  to  Robert  Cooper for kicking  off  this  initiative  and  to 
Peter Chen for helpful early comments on the work. 
References 
[ I ]   M. Baker and M. Sullivan. The recovery box: Using fast re- 
covery to provide high availability in the unix environment. 
In  Proceedings  of the Summer USENIX  Conference, pages 
31-44, June 1992. 
[2]  K. Birman and R. van Renesse, editors. Reliable Distributed 
Computing  with the Isis  Toolkit.  IEEE Computer Society 
Press, 1994. 
[3]  P.  Bohannon, D. Lieuwen, R. Rastogi, S. Seshadri, A. Silber- 
schatz, and S. Sudarshan. The architecture of  the dali main- 
memory storage manager. Journal  of Multimedia  Tools and 
Applications, 4(2), 1997. 
[4]  P.  M.  Chen, \Y.  T. Ng, S. Chandra, C. Ayock, G .  Rajamani, 
and D. Lowell. The rio file cache: Surviving operating sys- 
tem crashes. In 1996 International Conference on Architec- 
tural  Supporl for Programming  Languages  and  Operating 
Systems (ASPLOS), October 1996. 
[5]  D.  J.  DeWitt, R. H.  Katz, E  Olken, L.  D.  Shapiro, M.  R. 
Stonebraker, and D. Wood.  Implementation techniques for 
main memory database systems. In Proceedings of the 1984 
ACM SIGMOD International Conference on Management of 
Data, pages  1-8,  June 1984. 
492 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:59:04 UTC from IEEE Xplore.  Restrictions apply.