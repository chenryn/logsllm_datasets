In case a defense system tries to either restrict access to
the timers [35, 42] or to generate noise that could hide tim-
ing information, cache attacks are less likely to succeed. The
PRIME+ABORT attack [14] overcomes this difﬁculty. It ex-
ploits Intel’s implementation of Hardware Transactional Mem-
ory (TSX) to retrieve the information about cache accesses.
It ﬁrst starts a transaction to prime the targeted set, waits and
ﬁnally it may or may not receive and abort depending on
whether the victim has or has not accessed this set.
2.4 Countermeasures
Researchers have tackled the problem of mitigating cache
attacks from different perspectives. Several proposals sug-
gest limiting the access to the shared resources that can be
exploited to infer information about a victim by modifying
the underlying hardware [41, 58]. System-level software ap-
proaches, on the other hand, require modiﬁcation of the cur-
rent cloud infrastructure or the Linux kernel. STEALTHMEM
[32] uses private virtual pages that ensure the data located in
them is not evicted from the cache and avoid mapping any
other page with these private virtual pages. CATalyst [40]
uses Intel Cache Allocation Technology (CAT), which is a
technology that enables system administrators to control how
cores allocate data into the LLC. CACHEBAR [66] designs
a memory management subsystem that dynamically changes
the number of lines per cache set that a security domain can
occupy to defeat PRIME+PROBE attacks and changes the
state of the pages to avoid FLUSH+RELOAD. As we have
already stated, we are not aware of any CPU manufacturer,
cloud provider or OS implementing them.
A different approach to protect sensitive applications is to
speciﬁcally design them to be secure against side-channels (no
memory accesses depend on private information). Developers
can use speciﬁc tools [59, 63] to ensure the binary of such
applications does not leak information, even if it is under
attack. There are other tools, such as MASCAT [27], which
use code analysis techniques to detect potential attacks before
running a program. This kind of tools is effective before
malware distribution or execution, but their effectiveness is
reduced in cloud environments where the attacker does not
need to infect the victim.
For these reasons, we believe that the only countermeasures
that an attacker may have to face when trying to retrieve infor-
mation from a victim, are detection based countermeasures
that can be implemented at user level. Cache attacks exploit
the side effects of running a program in certain hardware to
gain information from it, and similarly, these countermeasures
employ monitoring mechanisms to observe these effects. De-
tection systems can use time measurements [12], hardware
performance counters [10, 13, 36, 64] or place data in trans-
actional regions [18] deﬁned with the Intel TSX instructions.
These detection systems measure the effect of the last level
cache misses on the victim or on both the victim and the
attacker. As a consequence, an attack that does not generate
cache misses on the victim's side would be undetectable by
these systems.
Detection systems that use performance counters as a
source of information to infer anomalies in the execution
of a program, are limited by the number of counters that can
be monitored simultaneously. This number varies between
processors, but implies that such counters must be carefully
selected. As our work shows, although the monitoring ap-
proach can still consider more counters, it is limited and can
not be arbitrarily extended to detect upcoming attacks.
3 Retrieval of Intel cache eviction policies
This work focuses on the LLC. Since it is shared across cores,
the attacks targeting the LLC are not limited to the situation in
which the victim and the attacker share the same core. It is also
possible to extract ﬁne-grained information from the LLC and
many researchers are concerned about the attacks targeting
the LLC. Attacks that assume a pseudo LRU eviction policy
such as PRIME+PROBE or EVICT+RELOAD can beneﬁt
from detailed knowledge of the eviction policy, and can also
beneﬁt one attacker wishing to carry out a “stealthy” attack
that does not cause cache misses on the victim.
In order to study the eviction policy, we try to emulate
the hardware in software. We ensure that we can ﬁll one set
of the cache with our own data, access that data and force
a miss when desired, to observe which element of the set is
evicted. Thus, we have constructed an eviction set (a group
of w different addresses that map to one speciﬁc set in w-way
set-associative caches) and what we call a conﬂicting set (a
1970    29th USENIX Security Symposium
USENIX Association
second eviction set that maps to exactly the same set and
is composed of disjoint addresses). Previous works have re-
trieved the complex addressing function [25,44,62] or demon-
strate how to create the aforementioned sets dynamically [15].
When the number of cores in our test systems is a power of 2,
we compute the set and slice number using the hash function
in [44] and use that information to construct the eviction and
conﬂicting sets. In the remaining situations such sets were
constructed following the procedure proposed by Liu et al.
in [15] (Algorithm 1).
For all the experiments, we have enabled the use of
hugepages in our systems. Note that the order of the accesses
is important to deduce the eviction policy. We enforce this
order using lfence instructions, which act as barriers that en-
sure all preceding load and store instructions have ﬁnished
before any load or store instruction that follows lfence. We
have observed that mfence does not always serialize the in-
struction stream, that is, it does not completely prevent out of
order execution.
3.1 Design of the experiments
Algorithm 1 Test of the desired eviction policy
Input: Eviction_set, Conﬂicting_set
Output: Accuracy of the policy
function TESTPOLICY(eviction_set, conﬂicting_set)
(cid:46) hits/trials
hits = 0;
while i ≤ num_experiments do
j = 0,i++;
control_array ← {};address_array ← {};
initialize_set(); (cid:46) Fills address and control arrays
lim = random();
while j ≤ lim do
lfence; j++;
next_data = eviction_set[random()];
measure time to read next_data;
if time ≥ ll_threshold then
(cid:46) LLC access
update(control_array,next_data);
con f _element = con f licting_set[random()];
read(con f _element);
candidate=getEvictionCandidate();
if (testDataEvicted() ==candidate) then
(cid:46) Force miss
hits++;
return hits/num_experiments;
We have performed experiments in different machines, each
of them including an Intel processor from different genera-
tions. Table 1 presents a summary of the machines employed
in this work. It includes the processor name, its number of
cores, the cache size and associativity and the OS running on
each machine. We have started by studying the processors of
the fourth generation, which have been a common victim of
published PRIME+PROBE attacks. We have extended our
analysis to cover processors from fourth to eighth generation.
Before conducting the experiments to disclose the eviction
policy implemented in each of the used machines, we have
performed some experiments intended to verify that no cached
data is evicted in the event of a cache miss if there is free
room in the set. The procedure is quite straightforward: for
each of the sets, we ﬁrst completely ﬁll it with the data on
its corresponding eviction set. Next, we randomly ﬂush one
of these lines to ensure there is free room in the set, and we
access one of the lines in the conﬂicting set checking that it is
indeed loaded from main memory (cache miss). Finally, we
make sure that all the lines in the eviction set (except for the
one evicted) still reside in the cache by measuring times when
re-accessing them. As expected, in all cases the incoming data
was loaded in replacement of the ﬂushed line.
The procedure we propose to retrieve the replacement pol-
icy, compares the actual evolution of the data in each of the
sets with its theoretical evolution deﬁned by an eviction policy
during the runtime. Algorithm 1 summarizes this procedure.
Each of the policies that has been tested had to be manually
deﬁned. We have evaluated true LRU, Tree PLRU, CLOCK,
NRU, Static and Bimodal RRIP, self-deﬁned policies using
four control bits, etc. among many other possible cache evic-
tion policies. After multiple experiments, we conclude that
the policy implemented on the processors corresponds to the
policy which best matches the experimental observations.
Algorithm 1 tries to emulate by software the behavior of
the hardware (of the cache). For this purpose, it uses two
arrays of size W . On the one hand, address_array mimics the
studied set, storing the memory addresses whose data is in
the cache set. On the other hand, control_array contains the
control bits used for deciding which address will be evicted
in case of conﬂict. Additionally, we need to manually deﬁne
one function that updates the content of the address_array,
one function that updates the control_array and another one
that provides the eviction candidate i.e. it returns the address
of the element that will be evicted in case of conﬂict. These
functions are deﬁned based on the tested replacement policy.
Note that for all the experiments the initialize_set() func-
tion makes sure that the tested set is empty (by ﬁlling it and
then ﬂushing all the elements that it holds) and later ﬁlls this
set with all the elements in the eviction set. That is, the ad-
dress_array contains the set of addresses of the eviction set
with their corresponding control bits initialized.
To set an example, we assume we want to test the NRU
policy [55], which turns out to match the policy implemented
in an Intel Xeon E5620 according to our experiments. Accord-
ing to its speciﬁcation, NRU uses one bit per cache line, this
bit is set whenever a cache line is accessed. If setting one bit
implies that all the bits of a cache set will be equal to one, then
all the bits (except for the one that has just being accessed)
will be cleared. In case of conﬂict, NRU will remove from the
cache one element whose control bit is equal to zero. Thus,
USENIX Association
29th USENIX Security Symposium    1971
Table 1: Details of the machines used in this work to retrieve their Replacement Policies
Number of cores Cache size Associativity
OS
Generation
4th
4th
4th
4th
5th
5th
6th
6th
6th
7th
7th
7th
8th
8th
8th
Processor
i7-4790
i5-4460
i7-4770K
Xeon E3-1226
i3-5010U
i5-5200U
i7-6700K
i5-6400
i7-6567U
i5-7600K
i7-7700HQ
i7-7700
i7-8650U
i5-8400
i7-8550U
4
4
4
4
2
2
4
4
2
4
4
4
4
6
4
8Mb
6Mb
8Mb
8Mb
3Mb
3Mb
8Mb
6Mb
4Mb
6Mb
6Mb
8Mb
8Mb
9Mb
8Mb
s
e
l
p
m
a
s
f
o
r
e
b
m
u
N
2
1.5
1
0.5
0
16
12
16
16
12
12
16
12
16
12
12
16
16
12
16
CentOS Linux 7
Kali Linux 2019.2
Kali Linux 2019.2
CentOS Linux 7
Ubuntu 14
Kali Linux 2019.2
Ubuntu 16
Kali Linux 2019.2
Kali Linux 2019.2
CentOS Linux 7
Ubuntu 16
Kali Linux 2019.2
Debian 9.5
Kali Linux 2019.2
Kali Linux 2019.2
·106
Low level
accesses
L3 cache accesses
Main memory
accesses
0
100
200
300
400
Access times
Figure 1: Distribution of the access times to different data.
These times depend on which memory it was located.
that is, two times the number of physical cores. Cache sizes
are similar, so they also differ in the number of sets per slice
(2048 vs 1024).