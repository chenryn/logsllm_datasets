Before condition variables, it was common to use either a notification
event or a synchronization event (recall that these are referred to as auto-reset
or manual-reset in the Windows API) to signal the change to a variable, such
as the state of a worker queue. Waiting for a change required a critical
section to be acquired and then released, followed by a wait on an event.
After the wait, the critical section had to be reacquired. During this series of
acquisitions and releases, the thread might have switched contexts, causing
problems if one of the threads called PulseEvent (a similar problem to the
one that keyed events solve by forcing a wait for the signaling thread if there
is no waiter). With condition variables, acquisition of the critical section or
SRW lock can be maintained by the application while
SleepConditionVariableCS/SRW is called and can be released only after the
actual work is done. This makes writing work-queue code (and similar
implementations) much simpler and predictable.
With both SRW locks and critical sections moving to the address-based
wait primitives, however, conditional variables can now directly leverage
NtWaitForAlertByThreadId and directly signal the thread, while building a
conditional variable wait block that’s structurally similar to the address wait
block we described earlier. The need for keyed events is thus completely
elided, and they remain only for backward compatibility.
Slim Reader/Writer (SRW) locks
Although condition variables are a synchronization mechanism, they are not
fully primitive locks because they do implicit value comparisons around their
locking behavior and rely on higher-level abstractions to be provided
(namely, a lock!). Meanwhile, address-based waiting is a primitive operation,
but it provides only the basic synchronization primitive, not true locking. In
between these two worlds, Windows has a true locking primitive, which is
nearly identical to a pushlock: the Slim Reader/Writer lock (SRW lock).
Like their kernel counterparts, SRW locks are also pointer sized, use atomic
operations for acquisition and release, rearrange their waiter lists, protect
against lock convoys, and can be acquired both in shared and exclusive
mode. Just like pushlocks, SRW locks can be upgraded, or converted, from
shared to exclusive and vice versa, and they have the same restrictions around
recursive acquisition. The only real difference is that SRW locks are
exclusive to user-mode code, whereas pushlocks are exclusive to kernel-
mode code, and the two cannot be shared or exposed from one layer to the
other. Because SRW locks also use the NtWaitForAlertByThreadId primitive,
they require no memory allocation and are guaranteed never to fail (other
than through incorrect usage).
Not only can SRW locks entirely replace critical sections in application
code, which reduces the need to allocate the large CRITICAL_SECTION
structure (and which previously required the creation of an event object), but
they also offer multiple-reader, single-writer functionality. SRW locks must
first be initialized with InitializeSRWLock or can be statically initialized with
a sentinel value, after which they can be acquired or released in either
exclusive or shared mode with the appropriate APIs:
AcquireSRWLockExclusive, ReleaseSRWLockExclusive,
AcquireSRWLockShared, and ReleaseSRWLockShared. APIs also exist for
opportunistically trying to acquire the lock, guaranteeing that no blocking
operation will occur, as well as converting the lock from one mode to
another.
 Note
Unlike most other Windows APIs, the SRW locking functions do not
return with a value—instead, they generate exceptions if the lock could
not be acquired. This makes it obvious that an acquisition has failed so
that code that assumes success will terminate instead of potentially
proceeding to corrupt user data. Since SRW locks do not fail due to
resource exhaustion, the only such exception possible is
STATUS_RESOURCE_NOT_OWNED in the case that a nonshared SRW
lock is incorrectly being released in shared mode.
The Windows SRW locks do not prefer readers or writers, meaning that
the performance for either case should be the same. This makes them great
replacements for critical sections, which are writer-only or exclusive
synchronization mechanisms, and they provide an optimized alternative to
resources. If SRW locks were optimized for readers, they would be poor
exclusive-only locks, but this isn’t the case. This is why we earlier mentioned
that conditional variables can also use SRW locks through the
SleepConditionVariableSRW API. That being said, since keyed events are no
longer used in one mechanism (SRW) but are still used in the other (CS),
address-based waiting has muted most benefits other than code size—and the
ability to have shared versus exclusive locking. Nevertheless, code targeting
older versions of Windows should use SRW locks to guarantee the increased
benefits are there on kernels that still used keyed events.
Run once initialization
The ability to guarantee the atomic execution of a piece of code responsible
for performing some sort of initialization task—such as allocating memory,
initializing certain variables, or even creating objects on demand—is a typical
problem in multithreaded programming. In a piece of code that can be called
simultaneously by multiple threads (a good example is the DllMain routine,
which initializes a DLL), there are several ways of attempting to ensure the
correct, atomic, and unique execution of initialization tasks.
For this scenario, Windows implements init once, or one-time initialization
(also called run once initialization internally). The API exists both as a
Win32 variant, which calls into Ntdll.dll’s Run Time Library (Rtl) as all the
other previously seen mechanisms do, as well as the documented Rtl set of
APIs, which are exposed to kernel programmers in Ntoskrnl.exe instead
(obviously, user-mode developers could bypass Win32 and use the Rtl
functions in Ntdll.dll too, but that is never recommended). The only
difference between the two implementations is that the kernel ends up using
an event object for synchronization, whereas user mode uses a keyed event
instead (in fact, it passes in a NULL handle to use the low-memory keyed
event that was previously used by critical sections).
 Note
Since recent versions of Windows now implement an address-based
pushlock in kernel mode, as well as the address-based wait primitive in
user mode, the Rtl library could probably be updated to use
RtlWakeAddressSingle and ExBlockOnAddressPushLock, and in fact a
future version of Windows could always do that—the keyed event merely
provided a more similar interface to a dispatcher event object in older
Windows versions. As always, do not rely on the internal details
presented in this book, as they are subject to change.
The init once mechanism allows for both synchronous (meaning that the
other threads must wait for initialization to complete) execution of a certain
piece of code, as well as asynchronous (meaning that the other threads can
attempt to do their own initialization and race) execution. We look at the
logic behind asynchronous execution after explaining the synchronous
mechanism.
In the synchronous case, the developer writes the piece of code that would
normally execute after double-checking the global variable in a dedicated
function. Any information that this routine needs can be passed through the
parameter variable that the init once routine accepts. Any output information
is returned through the context variable. (The status of the initialization itself
is returned as a Boolean.) All the developer has to do to ensure proper
execution is call InitOnceExecuteOnce with the parameter, context, and run-
once function pointer after initializing an INIT_ONCE object with
InitOnceInitialize API. The system takes care of the rest.
For applications that want to use the asynchronous model instead, the
threads call InitOnceBeginInitialize and receive a BOOLEAN pending status
and the context described earlier. If the pending status is FALSE,
initialization has already taken place, and the thread uses the context value
for the result. (It’s also possible for the function to return FALSE, meaning
that initialization failed.) However, if the pending status comes back as
TRUE, the thread should race to be the first to create the object. The code
that follows performs whatever initialization tasks are required, such as
creating objects or allocating memory. When this work is done, the thread
calls InitOnceComplete with the result of the work as the context and
receives a BOOLEAN status. If the status is TRUE, the thread won the race,
and the object that it created or allocated is the one that will be the global
object. The thread can now save this object or return it to a caller, depending
on the usage.
In the more complex scenario when the status is FALSE, this means that
the thread lost the race. The thread must undo all the work it did, such as
deleting objects or freeing memory, and then call InitOnceBeginInitialize
again. However, instead of requesting to start a race as it did initially, it uses
the INIT_ONCE_CHECK_ONLY flag, knowing that it has lost, and requests
the winner’s context instead (for example, the objects or memory that were
created or allocated by the winner). This returns another status, which can be
TRUE, meaning that the context is valid and should be used or returned to
the caller, or FALSE, meaning that initialization failed and nobody has been
able to perform the work (such as in the case of a low-memory condition,
perhaps).
In both cases, the mechanism for run-once initialization is similar to the
mechanism for condition variables and SRW locks. The init once structure is
pointer-size, and inline assembly versions of the SRW acquisition/release
code are used for the noncontended case, whereas keyed events are used
when contention has occurred (which happens when the mechanism is used
in synchronous mode) and the other threads must wait for initialization. In
the asynchronous case, the locks are used in shared mode, so multiple threads
can perform initialization at the same time. Although not as highly efficient
as the alert-by-ID primitive, the usage of a keyed event still guarantees that
the init once mechanism will function even in most cases of memory
exhaustion.
Advanced local procedure call
All modern operating systems require a mechanism for securely and
efficiently transferring data between one or more processes in user mode, as
well as between a service in the kernel and clients in user mode. Typically,
UNIX mechanisms such as mailslots, files, named pipes, and sockets are used
for portability, whereas in other cases, developers can use OS-specific
functionality, such as the ubiquitous window messages used in Win32
graphical applications. In addition, Windows also implements an internal IPC
mechanism called Advanced (or Asynchronous) Local Procedure Call, or
ALPC, which is a high-speed, scalable, and secured facility for message
passing arbitrary-size messages.
 Note
ALPC is the replacement for an older IPC mechanism initially shipped
with the very first kernel design of Windows NT, called LPC, which is
why certain variables, fields, and functions might still refer to “LPC”
today. Keep in mind that LPC is now emulated on top of ALPC for
compatibility and has been removed from the kernel (legacy system calls
still exist, which get wrapped into ALPC calls).
Although it is internal, and thus not available for third-party developers,
ALPC is widely used in various parts of Windows:
■    Windows applications that use remote procedure call (RPC), a
documented API, indirectly use ALPC when they specify local-RPC
over the ncalrpc transport, a form of RPC used to communicate
between processes on the same system. This is now the default
transport for almost all RPC clients. In addition, when Windows
drivers leverage kernel-mode RPC, this implicitly uses ALPC as well
as the only transport permitted.
■    Whenever a Windows process and/or thread starts, as well as during
any Windows subsystem operation, ALPC is used to communicate
with the subsystem process (CSRSS). All subsystems communicate
with the session manager (SMSS) over ALPC.
■    When a Windows process raises an exception, the kernel’s exception
dispatcher communicates with the Windows Error Reporting (WER)
Service by using ALPC. Processes also can communicate with WER
on their own, such as from the unhandled exception handler. (WER is
discussed later in Chapter 10.)
■    Winlogon uses ALPC to communicate with the local security
authentication process, LSASS.
■    The security reference monitor (an executive component explained in
Chapter 7 of Part 1) uses ALPC to communicate with the LSASS
process.
■    The user-mode power manager and power monitor communicate with
the kernel-mode power manager over ALPC, such as whenever the
LCD brightness is changed.
■    The User-Mode Driver Framework (UMDF) enables user-mode
drivers to communicate with the kernel-mode reflector driver by using
ALPC.
■    The new Core Messaging mechanism used by CoreUI and modern
UWP UI components use ALPC to both register with the Core
Messaging Registrar, as well as to send serialized message objects,
which replace the legacy Win32 window message model.
■    The Isolated LSASS process, when Credential Guard is enabled,
communicates with LSASS by using ALPC. Similarly, the Secure
Kernel transmits trustlet crash dump information through ALPC to
WER.
■    As you can see from these examples, ALPC communication crosses
all possible types of security boundaries—from unprivileged
applications to the kernel, from VTL 1 trustlets to VTL 0 services,
and everything in between. Therefore, security and performance were
critical requirements in its design.
Connection model
Typically, ALPC messages are used between a server process and one or
more client processes of that server. An ALPC connection can be established
between two or more user-mode processes or between a kernel-mode
component and one or more user-mode processes, or even between two
kernel-mode components (albeit this would not be the most efficient way of
communicating). ALPC exposes a single executive object called the port
object to maintain the state needed for communication. Although this is just
one object, there are several kinds of ALPC ports that it can represent:
■    Server connection port A named port that is a server connection
request point. Clients can connect to the server by connecting to this
port.
■    Server communication port An unnamed port a server uses to
communicate with one of its clients. The server has one such port per
active client.
■    Client communication port An unnamed port each client uses to
communicate with its server.
■    Unconnected communication port An unnamed port a client can use
to communicate locally with itself. This model was abolished in the
move from LPC to ALPC but is emulated for Legacy LPC for
compatibility reasons.
ALPC follows a connection and communication model that’s somewhat
reminiscent of BSD socket programming. A server first creates a server
connection port (NtAlpcCreatePort), whereas a client attempts to connect to
it (NtAlpcConnectPort). If the server was in a listening state (by using
NtAlpcSendWaitReceivePort), it receives a connection request message and
can choose to accept it (NtAlpcAcceptConnectPort). In doing so, both the
client and server communication ports are created, and each respective
endpoint process receives a handle to its communication port. Messages are
then sent across this handle (still by using NtAlpcSendWaitReceivePort),
which the server continues to receive by using the same API. Therefore, in
the simplest scenario, a single server thread sits in a loop calling
NtAlpcSendWaitReceivePort and receives with connection requests, which it
accepts, or messages, which it handles and potentially responds to. The
server can differentiate between messages by reading the PORT_HEADER
structure, which sits on top of every message and contains a message type.
The various message types are shown in Table 8-30.
Table 8-30 ALPC message types
Type
Meaning
LPC_R
EQUE
ST
A normal ALPC message, with a potential synchronous reply
LPC_R
EPLY
An ALPC message datagram, sent as an asynchronous reply 
to a previous datagram
LPC_
DATA
GRAM
An ALPC message datagram, which is immediately released 
and cannot be synchronously replied to
LPC_L
OST_R
EPLY
Deprecated, used by Legacy LPC Reply API
LPC_P
ORT_
CLOS
ED
Sent whenever the last handle of an ALPC port is closed, 
notifying clients and servers that the other side is gone
LPC_C
LIENT
_DIED
Sent by the process manager (PspExitThread) using Legacy 
LPC to the registered termination port(s) of the thread and the 
registered exception port of the process
LPC_E
XCEP
TION
Sent by the User-Mode Debugging Framework 
(DbgkForwardException) to the exception port through 
Legacy LPC
LPC_
DEBU
G_EV
ENT
Deprecated, used by the legacy user-mode debugging services 
when these were part of the Windows subsystem
LPC_E
RROR
_EVEN
T
Sent whenever a hard error is generated from user-mode 
(NtRaiseHardError) and sent using Legacy LPC to exception 
port of the target thread, if any, otherwise to the error port, 
typically owned by CSRSS
LPC_C
ONNE
CTION
_REQ
UEST
An ALPC message that represents an attempt by a client to 
connect to the server’s connection port
LPC_C
ONNE
CTION
_REPL
Y
The internal message that is sent by a server when it calls 
NtAlpcAcceptConnectPort to accept a client’s connection 
request
LPC_C
ANCE
LED
The received reply by a client or server that was waiting for a 
message that has now been canceled
LPC_
UNRE
GISTE
R_PR
OCESS
Sent by the process manager when the exception port for the 
current process is swapped to a different one, allowing the 
owner (typically CSRSS) to unregister its data structures for 
the thread switching its port to a different one
The server can also deny the connection, either for security reasons or
simply due to protocol or versioning issues. Because clients can send a
custom payload with a connection request, this is usually used by various
services to ensure that the correct client, or only one client, is talking to the
server. If any anomalies are found, the server can reject the connection and,
optionally, return a payload containing information on why the client was
rejected (allowing the client to take corrective action, if possible, or for
debugging purposes).
Once a connection is made, a connection information structure (actually, a
blob, as we describe shortly) stores the linkage between all the different
ports, as shown in Figure 8-40.
Figure 8-40 Use of ALPC ports.
Message model
Using ALPC, a client and thread using blocking messages each take turns
performing a loop around the NtAlpcSendWaitReceivePort system call, in
which one side sends a request and waits for a reply while the other side does
the opposite. However, because ALPC supports asynchronous messages, it’s
possible for either side not to block and choose instead to perform some other
runtime task and check for messages later (some of these methods will be
described shortly). ALPC supports the following three methods of
exchanging payloads sent with a message:
■    A message can be sent to another process through the standard
double-buffering mechanism, in which the kernel maintains a copy of
the message (copying it from the source process), switches to the
target process, and copies the data from the kernel’s buffer. For
compatibility, if legacy LPC is being used, only messages of up to
256 bytes can be sent this way, whereas ALPC can allocate an
extension buffer for messages up to 64 KB.
■    A message can be stored in an ALPC section object from which the
client and server processes map views. (See Chapter 5 in Part 1 for
more information on section mappings.)
An important side effect of the ability to send asynchronous messages is