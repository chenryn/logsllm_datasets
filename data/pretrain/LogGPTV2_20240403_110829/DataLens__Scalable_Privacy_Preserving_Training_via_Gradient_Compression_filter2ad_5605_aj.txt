learning rate
TopAgg
ğ‘˜ = 0.7
12.0
1.6
512
0.1
ğ‘˜ = 0.6
16.0
1.6
512
0.08
ğ‘˜ = 0.8
10.0
1.6
512
0.1
(f) CIFAR-10 (ğœ€ = 8)
GM-DP
14.0
1.8
512
0.04
TopAgg
ğ‘˜ = 0.7
ğ‘˜ = 0.8
ğ‘˜ = 0.6
GM-DP
TopAgg
ğ‘˜ = 0.7
ğ‘˜ = 0.6
0.0008
1
6
24
0.8
6
24
0.001
0.3
7
32
0.002
1.5
7
32
0.001
ğ‘
ğœ
batch size
learning rate
ğ‘
ğœ
batch size
learning rate
1
2.5
96
0.005
Algorithm 5 - Gradient Compression via k-level Stochastic
Gradient (StoKlevelGrad). This algorithm takes in a gradient
vector of a teacher model g(ğ‘–) and returns the compressed gradient
vector Ëœg(ğ‘–).
1: Input: Gradient vector g(ğ‘–) , gradient clipping constant ğ‘, top-ğ‘˜
2: g
(ğ‘–)
,âˆ’ğ‘), ğ‘) for each dimension ğ‘— in g(ğ‘–)
ğ‘—
âŠ² Clip each dimension of g(ğ‘–) so that âˆ’ğ‘ â‰¤ g
(ğ‘–)
ğ‘— = min(max(g
(ğ‘–)
ğ‘— â‰¤ ğ‘.
âŠ² Random Rotation
âŠ² gradient normalization to (-1, 1)
4: Ë†g(ğ‘–) â† g(ğ‘–)/(cid:13)(cid:13)g(ğ‘–)(cid:13)(cid:13)âˆ
3: g(ğ‘–) = ğ‘… Ã— g(ğ‘–)
Ëœg(ğ‘–) â† 0
5:
6: let ğ‘[ğ‘Ÿ] := âˆ’ğ‘˜/2 + 2ğ‘Ÿ for every ğ‘Ÿ âˆˆ [0, ğ‘˜)
ğ‘˜âˆ’1 for every ğ‘Ÿ âˆˆ [0, ğ‘˜)
7: let ğ‘š[ğ‘Ÿ] := âˆ’ğ‘ + 2ğ‘Ÿğ‘
compressed sparse gradient vector
(ğ‘–)
8: for each index ğ‘—, and ğ‘[ğ‘Ÿ] â‰¤ g
ğ‘— â‰¤ ğ‘[ğ‘Ÿ + 1] do
 ğ‘[ğ‘Ÿ + 1], with probability
ğ‘[ğ‘Ÿ],
ğ‘œ.ğ‘¤.
9:
Ëœğ‘”(ğ‘–)
ğ‘— =
10: end for
11: Return: Ëœg(ğ‘–)
(ğ‘–)
ğ‘— âˆ’ğ‘š[ğ‘Ÿ ]
g
ğ‘š[ğ‘Ÿ+1]âˆ’ğ‘š[ğ‘Ÿ ]
âŠ² initialization of the
ğ‘˜ = 0.8
1
2.5
96
0.005
GM-DP
1.5
3
96
0.005
TopAgg
ğ‘˜ = 0.7
0.5
2.5
2048
0.2
ğ‘˜ = 0.8
0.5
3
2048
0.2
ğ‘˜ = 0.6
0.5
2.5
2048
0.2
GM-DP
2.5
2.5
2048
0.04
ğ‘
ğœ
batch size
learning rate
1
2.5
96
0.005
Algorithm 6 - Differentially Private Gradient Compression
and Aggregation (D2P-Fed for DataLens). This algorithm
takes gradients of teacher models and returns the compressed and
aggregated differentially private gradient vector.
1: Input: Teacher number ğ‘ , gradient vectors of teacher models G =
{g(1) , . . . , g(ğ‘ ) }, gradient clipping constant ğ‘, top-ğ‘˜, noise parameters
ğœ, voting threshold ğ›½
Ëœg(ğ‘–) â† StoKlevelGrad(g(ğ‘–) , ğ‘, ğ‘˜)
2: âŠ² Phase I: Gradient Compression
3: for each teacherâ€™s gradient g(ğ‘–) do
4:
5: end for
6: âŠ² Phase II: Differential Private Gradient Aggregation
7:
8: âŠ² Phase III: Gradient Thresholding (Post-Processing)
9: for each dimension Ëœğ‘”âˆ—
ğ‘— of Ëœgâˆ— do
ğ‘— â‰¥ ğ›½ğ‘ ;
Ëœğ‘”âˆ—
ğ‘— â‰¤ âˆ’ğ›½ğ‘ ;
Ëœğ‘”âˆ—
Ëœgâˆ— â†ğ‘
 1,
if
if
otherwise.
ğ‘–=1 Ëœg(ğ‘–) + N(0, ğœ2)
âˆ’1,
0,
Â¯ğ‘”ğ‘— =
10:
11: end for
12: Return: Â¯g
but worse FID than G-PATE), which we think the reason is because
FID is evaluated based on models trained with ImageNet which
may not be suitable for evaluating datasets such as MNIST. In our
experiments, we follow GS-WGAN and use the implementation8
for FID calculation.
C.2 Adapting Other Gradient Compression
Algorithms to DataLens
In this section, we illustrate how we adapt D2P-Fed and FetchSGD
to DataLens framework.
For D2P-Fed, we replace our Algorithm 2 (TopkStoSignGrad)
that uses stochastic sign compression with their method, which
essentially uses k-level gradient quantization and random rotation
for gradient pre-processing. The detailed algorithm is shown in
Algorithm 6 and Algorithm 5.
For FetchSGD, we uses the same stochastic sign compression
as we leverage sign signal as teacher voting in PATE framework.
During aggregation, we use Count Sketch data structure, and use
8https://github.com/google/compare_gan
Algorithm 7 - Differentially Private Gradient Compression
and Aggregation (FetchSGD for DataLens). This algorithm
takes gradients of teacher models and returns the compressed and
aggregated differentially private gradient vector.
1: Input: Teacher number ğ‘ , gradient vectors of teacher models G =
{g(1) , . . . , g(ğ‘ ) }, gradient clipping constant ğ‘, top-ğ‘˜, noise parameters
ğœ, voting threshold ğ›½
Ëœg(ğ‘–) â† TopkStoSignGrad(g(ğ‘–) , ğ‘, ğ‘˜)
S += Sketch(Ëœg(ğ‘–) )
2: S = CountSketchAggregator()
3: âŠ² Phase I: Gradient Compression
4: for each teacherâ€™s gradient g(ğ‘–) do
5:
6:
7: end for
8: âŠ² Phase II: Differential Private Gradient Aggregation
9:
10: Return: Ëœgâˆ—
Ëœgâˆ— â† top-ğ‘˜(unSketch(ğ‘†)) + N(0, ğœ2)
top-ğ‘˜ and unsketch operation to retrieve the aggregated gradient.
The detailed algorithm is shown in Algorithm 7.
Session 7A: Privacy Attacks and Defenses for ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea2164Table 12: Quality evaluation of images generated by different differentially private data generative models on Image Datasets: Inception
Score (IS) and Frechet Inception Distance (FID) are calculated to measure the visual quality of the generated data under different ğœ€ (ğ›¿ = 10âˆ’5).
(a) ğœ€ = 1
Dataset
MNIST
Fashion-MNIST
CelebA
Dataset
MNIST
Fashion-MNIST
CelebA
Metrics Real data DP-GAN PATE-GAN G-PATE GS-WGAN DataLens
IS â†‘
FID â†“
IS â†‘
FID â†“
IS â†‘
FID â†“
3.60
153.38
3.41
214.78
1.11
302.45
4.37
186.06
3.93
194.98
1.18
297.73
1.00
489.75
1.00
587.31
1.00
437.33
1.19
231.54
1.69
253.19
1.15
434.47
1.00
470.20
1.03
472.03
1.00
485.92
9.86
1.04
9.01
1.54
1.88
2.38
(b) ğœ€ = 10
Metrics Real data DP-GAN PATE-GAN G-PATE GS-WGAN DataLens
IS â†‘
FID â†“
IS â†‘
FID â†“
IS â†‘
FID â†“
8.59
58.77
5.87
135.47
1.00
432.58
5.78
173.50
4.58
167.68
1.42
320.84
1.46
253.55
2.35