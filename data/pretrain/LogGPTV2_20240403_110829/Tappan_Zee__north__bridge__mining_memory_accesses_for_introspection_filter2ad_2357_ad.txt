System logs are an invaluable resource, both for security
and system administration. In an introspection-based secu-
rity system, for example, one might want to ﬁnd a tap point
that contains the system’s logs so that they can be stored se-
curely outside the guest virtual machine. However, because
the format of system logs is particular to each OS, we need
some mechanism that can ﬁnd tap points that write data
that “looks like” a log based on an exemplar. The statistical
search described in Section 4.2 is a good ﬁt for this task:
7The malware also has a second layer of encryption, which
is custom and not based on SSL; we did not attempt to
decrypt this second layer.
by training on the output of dmesg on one OS, we can ﬁnd
dmesg-like tap points on other systems.
To locate these system log tap points, we ﬁrst created
a training exemplar by running the dmesg command on a
Debian sid (amd64) host and computing the bigram proba-
bilities for the output. We then created recordings in which
we booted ﬁve operating systems (Debian squeeze (armel),
Debian squeeze (amd64), Minix R3-2.0 (i386), FreeBSD 9.0-
RELEASE (i386), and Haiku R1 Alpha 3 (i386)), and com-
puted the same bigram statistics. We then sorted the tap
points seen in each operating system boot according to their
Jensen-Shannon distance from the training distribution, and
manually examined data written by the tap point for each of
the top 30 results in each operating system. Table 4 shows,
for each operating system tested, the tap point that we de-
termined to be the system log, and its rank in the search
results.
We can see that in all cases the correct result is in the top
10. There are two additional features of Table 4 that bear
mentioning. First, the reader will note that the two Debian
systems have a caller of “N/A”. This is because the memory
writes that make up dmesg are done in do_syslog, which
is called from multiple functions.
In these cases, includ-
ing the caller splits up information that is semantically the
same. We detected this case by noticing that several of the
top-ranked results in the Linux experiments had the same
program counter, and that they appeared to contain diﬀer-
ent sections of the same log. Second, the tap point found for
Haiku was also incomplete—some lines were truncated. By
using our tap point correlation plugin, we determined that
we were missing a second tap point that was correlated with
the main one; the two together formed the write portions of
a memcpy of the log messages. Once this second tap point
was included, we could see all the log messages produced by
Haiku.
We also attempted to ﬁnd an analogous log message tap
point on Windows 7, but were not successful. This is a re-
sult of the way Windows logging works: rather than logging
string-based messages, applications and system services cre-
ate a manifest declaring possible log events, and then refer to
them by a generated numeric code. Human-readable mes-
sages are not stored, and instead are generated when the
user views the log. This means that there is no tap point
846Caller
OS
FreeBSD (x86)
msglogstr+0x28
Haiku (x86)
ring_buffer_peek+0x59
N/A
Debian (arm)
N/A
Debian (amd64)
Minix (x86)
0x190005ee
Windows 7 (x86) Not Found
PC
msgbuf_addstr+0x19a Yes
memcpy_generic+0x14 Yes
Yes
do_syslog+0x18c
Yes
do_syslog+0x163
No
0x190009d4
Not Found
?
Kernel? Rank
1
1
4
4
8
?
Table 4: Tap points that write the system log (dmesg) on several UNIX-like operating systems. All tap points
were located in the kernel, except for Minix, which is a microkernel. We were unable to ﬁnd a tap point
analogous to dmesg in Windows.
that will contain log messages of the type used in our dmesg
training, and the methods described in this paper are largely
inapplicable unless a training example for the binary format
can be found. However, because the event log query API is
public [23], existing tools such as Virtuoso [11] might be a
better ﬁt for this use case.
Anecdotally, the ability to uncover a tap point that writes
the kernel logs has also been useful for diagnosing problems
when adding support for new platforms to QEMU. For an
unrelated research task, we attempted to boot the Rasp-
berry Pi [1] kernel inside QEMU, but found that it hung
without displaying any output early on in the boot process.
By locating the dmesg tap point, we discovered that the last
log message printed was “Calibrating delay loop...”; based
on this we determined that the guest was hung waiting for
a timer interrupt that was not yet implemented in QEMU.
6.3 Unknown Unknowns: Clustering
To test the eﬀectiveness of clustering tap points based
on bigram statistics and Jensen-Shannon distance, we car-
ried out an experiment that compared the clusters gener-
ated algorithmically to a set of labels generated by two of
the co-authors manually examining the data. We created six
recordings representing diﬀerent workloads on two operating
systems (Windows 7 and FreeBSD 9.0). From FreeBSD, we
took recordings of boot, shutdown, running applications (ps,
cat, ls, top, and vi), and a one-minute recording of the sys-
tem sitting idle, for a total of four recordings. On Windows
we created two recordings: running applications (cmd.exe,
dir, the Task Manager, Notepad), and one minute of the
system sitting idle.8
Next, we sampled a subset of the tap points found in each
recording. Given that the vast majority of tap points do not
write interesting information, we opted not to sample uni-
formly from the all tap points found. Instead, we performed
an initial k-means clustering with k = 100, and then picked
out tap points at various distances from each cluster cen-
ter. We chose the tap point at σ standard deviations from
the center, for σ ∈ {0, 0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.0} for a
total of 2,926 samples9. Finally, we dumped the data from
each of the sampled tap points, blinded them by assigning
each a unique id, and then provided the data ﬁles to our two
8Although we would have preferred to include Windows
boot and shutdown recordings, at the time our replay sys-
tem had a bug (now ﬁxed) that prevented these recordings
from being replayed.
9The alert reader will note that this is smaller than the 4,800
samples one would expect from taking 8 samples from 100
clusters in each of 6 recordings. This is because some clusters
did not have very high variance, and so in many cases there
were fewer than 8 samples at the required distance from the
center.
labelers. Each labeler independently assigned labels to each
of the samples using the labels described in Table 5, and the
two labelers then worked together to reconcile their labels.
Finally, we ran a k-means clustering with k = 10; 10 was
chosen because it was a round number reasonably close to
the number of labels our human evaluators gave to the data.
We then used the Adjusted Rand Index [14] to score the
quality of our clustering relative to our hand-labeled exam-
ples. The Adjusted Rand Index for a clustering ranges from
-1 to 1; clusterings which are independent of the hand label-
ing will receive a score that is negative or close to zero. As
can be seen in Table 6, our clustering did not match up very
well on the hand-labeled samples. Note, however, that la-
beling criteria were selected without knowledge of the sizes
of categories or whether or not the distance metric would
eﬀectively discriminate, so it is perhaps unsurprising that
the correspondence is poor.
There is some hope, however. Regardless of the appar-
ently poor clustering performance with respect to hand-
labeling, we decided to determine if the clusters from our
100-mean clustering of FreeBSD’s boot process contained
new and interesting data and if ﬁnding that data would be
facilitated by them. First, we determined to which of the
clusters data from the FreeBSD’s dmesg and ﬁlename tap
points (found in Sections 6.2.2 and 6.1.3) was assigned. We
were heartened to learn that these two text-like tap points
had been sent to the same cluster. We proceeded to explore
this cluster of approximately 5000 tap points, and found
that, indeed, the vast majority of the tap points contained
readable text of some sort. Further, in the course of about
thirty minutes of spelunking around this cluster, we found
not only kernel messages and ﬁlenames but a stone soup of
shell scripts, process listings kernel conﬁguration, GraphViz
data, and so on. A selection of these tap point contents is
provided in Appendix 9. We did not exhaustively examine
this cluster, but plan to do so soon, as it appears to contain
much of interest for active monitoring. If clustering has fo-
cused us on one out of 100 clusters, this is potentially a big
savings.
6.4 Accuracy
Leaving aside the clustering results for the moment, the
analyses implemented in TZB are extremely eﬀective at help-
ing to identify interposition points for active monitoring. In
the evaluations based on string searching, we found that the
number of tap points we had to look at manually was at most
262 (URLs under IE8) and in the best case we only had to
examine two tap points (for SSL keys under Firefox, Opera,
Haiku, and OpenSSL on ARM). The number of tap points
that need to be examined is related to how widely the data
is propagated in the system and how common the string be-
847Abbrv. Description
bp
rd
mz
rq
fnu
woa
wou
inu
bu
ura
rs
fna
rb
vr
binary pattern
repeated dword
mostly zero
repeated quadword
ﬁlenames unicode
words ascii
words unicode
integers unicode
binary uniform
URLs ascii
repeated short
ﬁlenames ascii
repeated byte
very redundant
Count
2318
400
141
19
8
8
7
6
5
5
4
2
2
1
Table 5: Labels given to the sampled tap points by
human evaluators, along with the number of times
each occurred.
Recording
FreeBSD Apps
FreeBSD Boot
FreeBSD Idle
FreeBSD Shutdown
Win7 Apps
Win7 Idle
ARI
0.018
0.048
0.021
0.074
0.029
-0.003
Table 6: Quality of clustering as measured by the
Adjusted Rand Index, which ranges from -1 to 1,
with 1 being a clustering that perfectly matches the
hand-labeled examples.
ing searched for is; thus, it is natural that URLs visited in
the browser would appear in many tap points, whereas the
SSL/TLS master key would not. Qualitatively speaking, we
found that once the candidate tap points had been selected
by stringsearch for a given execution, it took at most an
hour to ﬁnd one that suﬃced for the task at hand.
For the dmesg evaluation, we also examined the quality of
the results found for each operating system using the stan-
dard “Precision at 10” metric, which is just the number of
results found in the top 10 that were actually relevant to
the query. In this case, this is simply the fraction of results
in the top 10 that appeared to contain the system log (even
if it was incomplete). Based on this metric, the precision
of our retrieval was between 20% (on Minix) and 100% (on
Haiku). This means that if one looked at all of the top 10
entries, it is guaranteed that one would ﬁnd the correct tap
point.
7. LIMITATIONS AND FUTURE WORK
Although TZB is currently very useful for ﬁnding intercep-
tion points for active monitoring, it is not currently usable
in every scenario where introspection is needed. Because the
interception points are triggered by executing code, they are
only usable in online analysis. However, the need for intro-
spection also arises in post-mortem analysis, speciﬁcally in
forensic memory analysis. Whereas previous solutions such
as Virtuoso [11] were able to operate equally well on memory
images or live virtual machines, TZB is only applicable to
the live case. In future work, we hope to combine Virtuoso-
like techniques with TZB to produce oﬄine programs that
can locate in memory the buﬀers on which TZB’s tap points
operate.
Another limitation of TZB is its reliance on callstack in-
formation to locate interposition points. In current systems,
keeping track of an arbitrary number of callers for each pro-
cess is prohibitively expensive; although stack walking is