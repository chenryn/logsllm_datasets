per test. Time shows the average execution time of our
implementations of Algorithm 1 (baseline) and Algorithm 2
(group testing) under ideal conditions.
linear-time reduction does not lead to a practical advantage.
‚Ä¢ For small pages, the linear-time reduction improves the
cost of computing eviction sets by a factor of more than 7. This
is a signiÔ¨Åcant advantage in practice, as it can make attacks
more stealthy and robust against timing constraints.
‚Ä¢ For the limit case, the linear-time reduction improves over
the quadratic baseline by more than two orders of magnitude.
D. Performance in Practice
In this section we give examples of the performance beneÔ¨Åts
of our reduction algorithms in real-world scenarios, i.e., in the
presence of TLB noise and adaptive replacement policies.
We implement two heuristics to counter the resulting sub-
optimal reduction rates (see SectionV-A): repeat-until-success,
where we pick a new set and start over after a failed reduction;
and backtracking, where at each level of the computation tree
we store the elements that are discarded, and, in case of error,
go back to a parent node on which the test succeeded to
continue the computation from there. For more details we refer
to our open-source implementation.
For comparing the performance of the reduction algorithms
in the context of these heuristics, we follow the literature
and focus on initial set sizes that guarantee that the initial
set is an eviction set with high probability. This is because
a real-world attacker is likely to forgo the complications of
repeatedly sampling and directly pick a large enough initial
set.
The following examples provide average execution times
(over 100 samples) for different attackers on randomly selected
target cache sets. Skylake (a = 12) using 10 time measure-
ments per test.
‚Ä¢ For Ô¨Ånding eviction sets with huge pages, previous
work [20] suggests an initial set size of N = 192 which,
according to our binomial model (see Section III-C), yields a
probability of sets to be evicting close to 1. For this size, the
baseline reduction takes 0.014 seconds, while the group-testing
reduction takes 0.003 seconds, i.e. our algorithm improves the
baseline by a factor of 5.
‚Ä¢ For Ô¨Ånding minimal eviction sets with 4KB pages, previ-
ous work [9] suggests an initial set size of N = 8192, which
amounts to the size of LLC times the number of slices. We
choose an initial set size of N = 3420 for our comparison,
which according to our model provides a probability of being
an eviction set close to 1. For this N, the baseline reduction
takes 5.060 seconds, while the group-testing reduction takes
0.245 seconds, i.e. our algorithm improves the baseline by
a factor of 20. Finding all minimal eviction sets (for a Ô¨Åxed
offset) within this buffer3 requires more than 100 seconds with
the baseline algorithm. With group testing, the same process
takes only 9.339 seconds, i.e. it improves by a factor of 10.
E. Summary
In summary, our experiments show that our algorithms
improve the time required for computing minimal eviction sets
by factors of 5-20 in practical scenarios. Moreover, they show
that Ô¨Ånding minimal eviction sets from virtual (or sandboxed)
memory space is fast even without any control over the slice
or set index bits, rendering countermeasures based on masking
these bits futile.
VI. A CLOSER LOOK AT THE EFFECT OF MODERN CACHE
REPLACEMENT POLICIES
There are several features of modern microarchitectures that
are not captured in our model and that can affect the effec-
tiveness of our algorithms, such as adaptive and randomized
replacement policies, TLBs, prefetching, etc. The evaluation
of Section V shows that the inÔ¨Çuence of prefetching can be
partially mitigated by an adversary, and that the inÔ¨Çuence of
TLBs is not a limiting factor in practice. The role of the cache
replacement policy is less clear.
In this section, we take a closer look at the role of modern
cache replacement policies in computing minimal eviction
sets. As discussed in Section II, post Sandy Bridge architec-
tures boast replacement policies with features such as adap-
tivity or thrashing-resistance. With such features, accessing
a set of a addresses that are congruent with [x] is neither
necessary nor sufÔ¨Åcient for evicting x, which introduces a two-
sided error (false positives and false negatives) in our tests for
congruence. We Ô¨Årst explain the key mechanisms that lead to
this error, before we experimentally analyze its inÔ¨Çuence on
Skylake and Haswell.
A. Adaptive Replacement Policies
Adaptive cache replacement policies [21] dynamically select
the replacement policy depending on which one is likely to
be more effective on a speciÔ¨Åc load. For this, they rely on so-
called leader sets that implement different policies. A counter
keeps track of the misses incurred on the leaders and adapts the
replacement policy of the follower sets depending on which
leader is more effective at the moment. There are different
ways for selecting the leaders: a static policy in which the
leader sets are Ô¨Åxed; and a rand-runtime policy that randomly
selects different leaders every few millions instructions.
A previous study indicates that the replacement mechanism
used in Ivy Bridge is indeed adaptive, with static leader
sets [11]. To the best of our knowledge, there is no detailed
study of replacement mechanisms on more recent generations
of Intel processors such as Haswell, Broadwell, or Skylake,
but there are mentions of high-frequency policy switches on
3We empirically observe that on Skylake, this size is sufÔ¨Åcient to contain
eviction sets for most of the 128 different cache sets for a Ô¨Åxed offset.
(cid:21)(cid:26)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:41:36 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 9: Skylake‚Äôs eviction and reduction rates per set index. With a stride of 4KB and a total of 4000 addresses (most of them
non-congruent). The number of sets in-between two leaders is either 32 or 98. We rely on huge pages to precisely control the
target‚Äôs set index bits.
Haswell and Broadwell CPUs as an obstacle for prime+probe
attacks [9].
We perform different experiments to shed more light on the
implementations of adaptivity in Skylake and Haswell, and on
their inÔ¨Çuence on computing minimal eviction sets. To this
end, we track eviction and reduction rates (see Section V) for
each of the set indexes individually
1) on arbitrary eviction sets
2) on eviction sets in which all addresses are partially
congruent.
In the second case, the reduction uses only addresses belong-
ing to a single cache set per slice. Assuming independence
of cache sets across slice, a comparison with the Ô¨Årst case
allows us to identify the inÔ¨Çuence across cache sets. For
both experiments we rely on huge pages in order to precisely
control the targeted cache set and reduce the effect of the TLB,
see Section V-A.
B. Evaluating the Effect of Adaptivity
The results for reducing arbitrary eviction sets on Skylake
are given in Figure 9, the results for Haswell are given in
Figure 10. We focus on initial eviction sets of size N = 4000
(but observe similar results for other set sizes). We highlight
the following Ô¨Åndings:
Fig. 10: Haswell‚Äôs eviction and reduction rates per set index.
With a stride of 4KB and a total of 4000 addresses (most of
them non-congruent).
‚Ä¢ Skylake seems to implement an adaptive replacement
mechanism with static leader sets, much like Ivy Bridge. In
particular, we identify a subset of 16 (out of 1024 per slice)
(cid:22)(cid:17)
(a) Skylake‚Äôs eviction and reduction rates per set index, based on a
stride of 64KB (only partially congruent addresses).
(b) Haswell‚Äôs eviction rate and reduction rate per set index, based on
a stride of 128KB (only partially congruent addresses).
Fig. 11: Eviction rate and reduction rate per set index for initial
sets of 4000 partially congruent addresses.
sets where the reduction rate is consistently above 95% and
where tests consistently evict the target address according to
our model (i.e. without false positives and false negatives). On
the follower sets the reduction rate quickly falls off despite a
high eviction rate, indicating that the test can produce both
false positives and false negatives.
‚Ä¢ In contrast to Skylake, on Haswell we are not able to
identify any leader sets with consistently high reduction rates,
which suggests a dynamic leader selection policy is in place.
The results of our reductions on partially congruent eviction
sets on Haswell and Skylake are given in Figure 11. They show
that eviction and reduction rates are close to the predicted
optimum. This improves over the reduction rate in Figure 9
and 10, and indicates a strong interference on the eviction
test when accessing neighboring cache sets. In particular, we
observe that the robustness of the reduction increases with
the proportion of partially congruent addresses in the initial
eviction set.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:41:36 UTC from IEEE Xplore.  Restrictions apply. 
	

#"!
#!
# !
#
"!
!
 !


















Fig. 12: Skylake‚Äôs total execution time per set index using backtracking and repeat-until-success. Average time over 100 samples,
all of them successful. Stride of 4KB (simulate adversary) and initial set of 4000 addresses (most of them non-congruent). The
lowest execution times (below 0.12s), correspond to sets with higher reduction rate. Horizontal line shows the overall average
execution time.

	
Finally, Figure 12 depicts the average execution time, in-
cluding the overhead of the backtracking heuristic, of Ô¨Ånding
a minimal eviction set for each cache set index. A lower
reduction rate implies a higher number of errors, and hence
more backtracking steps and a longer execution time. This
effect is visible when comparing with Figure 9: cache sets with
the highest reduction rates have the lowest execution times.4
C. Future Work
A more detailed analysis of the algorithmic implications
of adaptive cache replacement policies is out of the scope of
this paper. However, we brieÔ¨Çy describe some ideas for future
work:
‚Ä¢ Controlling the policy. A better understanding of adap-
tivity mechanisms could be exploited to augment the number
of followers that behave deterministically, and hence facilitate
the reduction. For instance, once we Ô¨Ånd eviction sets for a
leader set on Skylake, a parallel thread could produce hits on
that set (or misses on another leader), ensuring that it keeps
the lead.
‚Ä¢ Group testing. Work on noisy group testing [22], or
threshold group testing with gap [8], can provide useful tools
for dealing with the uncertainty about the exact behavior of
modern microarchitectures.
VII. RELATED WORK
Computing minimal, or at least small, eviction sets pro-
vides an essential primitive for removing or placing arbitrary
data in the cache, which is essential for LLC cache attacks
(Prime+Probe [4], Evict+Reload [23], etc.), for DRAM fault
attacks (such as Rowhammer [24], [2], which break the sep-
aration between security domains), for memory-deduplication
attacks (such as VUSION [25]), as well as for the recent
Meltdown [26] and Spectre [3] attacks (which use the cache
4The plot also shows two different clusters of execution times, for which
we currently lack a satisfying explanation.
to leak data across boundaries and to increase the number of
speculatively executed instructions).
Gruss et al. [6] already identiÔ¨Åed dynamic and static
approaches for Ô¨Ånding eviction sets. The dynamic method
uses timing measurements to identify colliding addresses
without any knowledge about the LLC slicing hash function
or the physical addresses; whereas the static method use
the reverse engineered hash and (partial) information about
the physical addresses to compute eviction sets. In practice,
most attacks [20] rely in a hybrid approach, producing a
partially congruent set of addresses with static methods, and
pruning or reducing the results with a dynamic method (mostly
variants of Algorithm 1). We review some of the most relevant
approaches:
Fully static, without slicing:
In CPUs without slicing
(such as ARM) it is possible to Ô¨Ånd eviction sets directly using
the information from the pagemap interface. Lipp et al. [27]
explore how to perform Prime+Probe, Evict+Reload, and other
cross-core cache attacks on ARM. Fortunately, Google patched
Android in March 2016 5, and now it requires root privileges
to disclose physical addresses, difÔ¨Åculting the task of Ô¨Ånding
eviction sets.
Static/Dynamic with huge pages: Liu et al. [4] and
Irazoqui et al. [5], in their seminal work on attacks against
LLC, rely on 2MB huge pages to circumvent the problem of
mapping virtual addresses into cache sets. They are the Ô¨Årst
to propose this method.
Gruss et al. [6] present the Ô¨Årst rowhammer attack from
JavaScript. To achieve this, they build eviction sets thanks to
2MB huge pages (provided by some Linux distributions with
transparent huge pages support, see Appendix A).
On the other hand, more sophisticated cache attacks from
Intel‚Äôs SGX [28] rely on the predictable physical allocation
of large arrays within SGX enclaves, and on the information
extracted from another side-channel in DRAM row‚Äôs buffers.
5Android patch: https://source.android.com/security/bulletin/2016-03-01
(cid:22)(cid:18)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:41:36 UTC from IEEE Xplore.  Restrictions apply. 
Sandboxed environments without huge pages: Oren et
al. [9] present an extension to Liu et al.‚Äôs work, carrying out the
Ô¨Årst cache attack from JavaScript, where regular 4KB pages
are used and pointers are not directly available. It exploits
the knowledge of browser‚Äôs page aligned allocation for large
buffers to construct an initial set with identical page offset
bits. Then they leverage the clever technique described in
Section IV-D for further accelerating the process of Ô¨Ånding
other eviction sets.
Dedup Est Machina [14] also implements a JavaScript
rowhammer attack, but this time targeting Microsoft Edge
on Windows 10. Interestingly,
they can not rely on large
pages, since Microsoft Edge does not explicitly request them.
However, they discover that the Windows kernel dispenses
pages for large allocations from different physical memory
pools that frequently belong to the same cache sets. Thereby,
they are able to efÔ¨Åciently Ô¨Ånd eviction sets (not minimal) by
accessing several addresses that are 128KB apart (and often
land in the same cache set).
Horn‚Äôs [13] breaks virtual machine isolation using a heuris-
tic to Ô¨Ånd small eviction sets by iterating over Test 3 several
times, and discarding all elements that are always hot (i.e.
always produce cache hits). While this heuristic performs
extremely well in practice, its asymptotic cost is quadratic on
the set size.
Finally, a more recent work on cache attacks from portable
code [18] (PNaCl and WebAssembly) discusses the problem
of Ô¨Ånding eviction sets on regular 4KB pages and how to
partially deal with TLB thrashing.
In contrast to these approaches, our work is the Ô¨Årst to
consider adversaries with less than 12 bits of control over
the physical addresses, it formalizes the problem of Ô¨Ånding
eviction sets, and provides new techniques that might enable
purely dynamic approaches.
Reverse engineering of slicing functions: Modern CPUs 6
with LLC slicing use proprietary hash functions for distribut-
ing blocks into slices, which lead to attempts to reverse
engineer them. These works are based on: 1) allocating and
identifying sets of colliding addresses [30], [19]; and 2) recon-
structing the slice function using the hamming distance [31],
or solving systems of equations [32], between these addresses.
Even though we now know the slice hash function for several
microarchitectures, and Maurice et al. [20] leverage it to speed
up the Ô¨Ånding of eviction sets with huge pages, we believe that
its use on real attacks is hindered by constrained environments
with scarce information about the physical addresses.
Thrashing/scanning resistant replacement policies: Mod-
ern replacement policies such as insertion policies [33] or