chapter. In particular, the requirements allow storing encrypted data and the key in 
the same context, as long as the key is encrypted by another key residing in the same 
context. 
One final point is that security by obscurity (or obfuscation) has earned a bad 
reputation in the past several years. On its own, it's an insufficient technique for 
protecting data from attackers; it simply doesn't provide a strong enough level of 
confidentiality. However, in practice, obfuscation can be a valuable component of any 
security policy because it deters casual snoopers and can often slow down dedicated 
attackers. 
Integrity 
Chapter 1(? [????.]) defined integrity as the expectation that only authorized parties 
are able to modify data. This requirement, like confidentiality, is typically addressed 
through access control mechanisms. However, additional measures must be taken 
when communication is performed over a channel that's not secure. In these cases, 
certain cryptographic methods, discussed in the following sections, are used to 
ensure data integrity. 
Hash Functions 
Cryptographic data integrity is enforced through a variety of methods, although hash 
functions are the basis of most approaches. A hash function (or "message digest 
function") accepts a variable-length input and generates a fixed-size output. The 
effectiveness of a hash function is measured primarily by three requirements. The 
first is that it must not be reversible, meaning that determining the input based only 
on the output should be computationally infeasible. This requirement is known as the 
"no pre-image" requirement. The second requirement is that the function not have a 
second pre-image, which means that given the input and the output, generating an 
input with the same output is computationally infeasible. The final requirement, and 
the strongest, is that a hash must be relatively collision free, meaning that 
intentionally generating the same output for differing inputs should be 
computationally infeasible. 
Hash functions provide the foundation of most programmatic integrity protection. 
They can be used to associate an arbitrary set of data with a unique, fixed-size value. 
This association can be used to avoid retaining sensitive data and to vastly reduce the 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
59 
storage required to validate a piece of data. The simplest forms of hash functions are 
cyclic redundancy check (CRC) routines. They are fast and efficient and offer a 
moderate degree of protection against unintentional data modification. However, 
CRC functions aren't effective against intentional modification, which makes them 
unusable for security purposes. Some popular CRC functions include CRC-16, CRC-32, 
and Adler-32. 
The next step up from CRC functions are cryptographic hash functions. They are 
far more computationally intensive, but they offer a high degree of protection against 
intentional and unintentional modification. Popular hash functions include SHA-1, 
SHA-256, and MD5. (Issues with MD5 are discussed in more detail in 
"Bait-and-Switch Attacks" later in this chapter.) 
Salt Values 
Salt values are much the same as initialization vectors. The "salt" is a random value 
added to a message so that two messages don't generate the same hash value. As 
with an IV, a salt value must not be duplicated between messages. A salt value must 
be stored in addition to the hash so that the digest can be reconstructed correctly for 
comparison. However, unlike an IV, a salt value should be protected in most 
circumstances. 
Salt values are most commonly used to prevent precomputation-based attacks 
against message digests. Most password storage methods use a salted hash value to 
protect against this problem. In a precomputation attack, attackers build a dictionary 
of all possible digest values so that they can determine the original data value. This 
method works only for fairly small ranges of input values, such as passwords; 
however, it can be extremely effective. 
Consider a salt value of 32 random bits applied to an arbitrary password. This salt 
value increases the size of a password precomputation dictionary by four billion times 
its original value (232). The resulting precomputation dictionary would likely be too 
large for even a small subset of passwords. Rainbow tables, developed by Philippe 
Oechslin, are a real-world example of how a lack of a salt value leaves password 
hashes vulnerable to pre-computation attacks. Rainbow tables can be used to crack 
most password hashes in seconds, but the technique works only if the hash does not 
include a salt value. You can find more information on rainbow tables at the Project 
RainbowCrack website: http://www.antsight.com/zsl/rainbowcrack/. 
Originator Validation 
Hash functions provide a method of validating message content, but they can't 
validate the message source. Validating the source of a message requires 
incorporating some form of private key into the hash operation; this type of function 
is known as a hash-based message authentication code (HMAC) function. A 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
60 
MAC is a function that returns a fixed-length value computed from a key and 
variable-length message. 
An HMAC is a relatively fast method of validating a message's content and sender by 
using a shared secret. Unfortunately, an HMAC has the same weakness as any shared 
key system: An attacker can impersonate any party in a conversation by 
compromising only one party's key. 
Cryptographic Signatures 
A cryptographic signature is a method of associating a message digest with a 
specific public key by encrypting the message digest with the sender's public and 
private key. Any recipient can then decrypt the message digest by using the sender's 
public key and compare the resulting value against the computed message digest. 
This comparison proves that the originator of the message must have had access to 
the private key. 
Common Vulnerabilities of Integrity 
Integrity vulnerabilities are similar to confidentiality vulnerabilities. Most integrity 
vulnerabilities can, in fact, be prevented by addressing confidentiality concerns. 
However, some integrity-related design vulnerabilities, discussed in the following 
sections, merit special consideration. 
Bait-and-Switch Attacks 
Commonly used hashing functions must undergo a lot of public scrutiny. However, 
over time, weaknesses tend to appear that could result in exploitable vulnerabilities. 
The bait-and-switch attack is typically one of the first weaknesses found in an 
aging hash function. This attack takes advantage of a weak hash function's tendency 
to generate collisions over certain ranges of input. By doing this, an attacker can 
create two inputs that generate the same value. 
For example, say you have a banking application that accepts requests to transfer 
funds. The application receives the request, and if the funds are available, it signs the 
transfer and passes it on. If the hashing function is vulnerable, attackers could 
generate two fund transfers that produce the same digest. The first request would 
have a small value, and the second would be much larger. Attackers could then open 
an account with a minimum balance and get the smaller transfer approved. Then they 
would submit the larger request to the next system and close out their accounts 
before anyone was the wiser. 
Bait-and-switch attacks have been a popular topic lately because SHA-1 and MD5 are 
starting to show some wear. The potential for collision vulnerabilities in MD5 was 
identified as early as 1996, but it wasn't until August 2004 that Xiaoyun Wang, 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
61 
Dengguo Feng, Xuejia Lai, and Hongbo Yu published a paper describing successful 
collisions with the MD5 algorithm. This paper was followed up in March 2005 by Arjen 
Lenstra, Xiaoyun Wang, and Benne de Weger. They successfully generated a colliding 
pair of X.509 certificates with different public keys, which is the certificate format 
used in SSL transactions. More recently, Vlastimil Klima published an algorithm in 
March 2006 that's capable of finding MD5 collisions in an extremely short time. 
The SHA family of algorithms is also under close scrutiny. A number of potential 
attacks against SHA-0 have been identified; however, SHA-0 was quickly superseded 
by SHA-1 and never saw significant deployment. The SHA-0 attack research has 
provided the foundation for identifying vulnerabilities in the SHA-1 algorithm, 
although at the time of this writing, no party has successfully generated a SHA-1 
collision. However, these issues have caused several major standards bodies (such as 
the U.S.-based NIST) to initiate phasing out SHA-1 in favor of SHA-256 (also known 
as SHA-2). 
Of course, finding random collisions is much harder than finding collisions that are 
viable for a bait-and-switch attack. However, by their nature, cryptographic 
algorithms should be chosen with the intention that their security will be viable far 
beyond the applicable system's life span. This reasoning explains the shift in recent 
years from hashing algorithms that had previously been accepted as relatively secure. 
The impact of this shift can even be seen in password-hashing applications, which 
aren't directly susceptible to collision-based attacks, but are also being upgraded to 
stronger hash functions. 
Availability 
Chapter 1(? [????.]) defined availability as the capability to use a resource when 
expected. This expectation of availability is most often associated with reliability, and 
not security. However, there are a range of situations in which the availability of a 
system should be viewed as a security requirement. 
Common Vulnerabilities of Availability 
There is only one type of general vulnerability associated with a failure of 
availabilitythe denial-of-service (DoS) vulnerability. A DoS vulnerability occurs when 
an attacker can make a system unavailable by performing some unanticipated action. 
The impact of a DoS attack can be very dependant on the situation in which it occurs. 
A critical system may include an expectation of constant availability, and outages 
would represent an unacceptable business risk. This is often the case with core 
business systems such as centralized authentication systems or flagship websites. In 
both of these cases, a successful DoS attack could correspond directly to a significant 
loss of revenue due to the business's inability to function properly without the system. 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
62 
A lack of availability also represents a security risk when an outage forces 
requirements to be addressed in a less secure manner. For example, consider a 
point-of-sale (PoS) system that processes all credit card transactions via a central 
reconciliation server. When the reconciliation server is unavailable, the PoS system 
must spool all of the transactions locally and perform them at a later time. An attacker 
may have a variety of reasons for inducing a DoS between a PoS system and the 
reconciliation server. The DoS condition may allow an attacker to make purchases 
with stolen or invalid credit cards, or it may expose spooled cardholder information on 
a less secure PoS system. 
6.2.4 Threat Modeling 
By now, you should have a good idea of how design affects the security of a software 
system. A system has defined functionality that's provided to its users but is bound by 
the security policy and trust model. The next step is to turn your attention to 
developing a process for applying this knowledge to an application you've been 
tasked to review. Ideally, you need to be able to identify flaws in the design of a 
system and prioritize the implementation review based on the most security-critical 
modules. Fortunately, a formalized methodology called threat modeling exists for 
just this purpose. 
In this chapter, you use a specific type of threat modeling that consists of a five-phase 
process: 
Information collection 
Application architecture modeling 
Threat identification 
Documentation of findings 
Prioritizing the implementation review 
This process is most effectively applied during the design (or a refactoring) phase of 
development and is updated as modifications are made in later development phases. 
It can, however, be integrated entirely at later phases of the SDLC. It can also be 
applied after development to evaluate an application's potential exposure. The phase 
you choose depends on your own requirements, but keep in mind that the design 
review is just a component of a complete application review. So make sure you 
account for the requirements of performing the implementation and operational 
review of the final system. 
This approach to threat modeling should help establish a framework for relating many 
of the concepts you've already learned. This process can also serve as a roadmap for 
applying many concepts in the remainder of this book. However, you should maintain 
a willingness to adapt your approach and alter these techniques as required to suit 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
63 
different situations. Keep in mind that processes and methodologies can make good 
servants but are poor masters. 
Note 
This threat-modeling process was originally introduced in Writing Secure Code, 2nd 
Edition (Microsoft Press, 2002(? [????.])) by Michael Howard and David Le Blanc. It 
was later expanded and refined in Threat Modeling (Microsoft Press, 2004(? [????.])) 
by Frank Swiderski and Window Snyder. 
Information Collection 
The first step in building a threat model is to compile all the information you can about 
the application. You shouldn't put too much effort into isolating security-related 
information yet because at this phase you aren't certain what's relevant to security. 
Instead, you want to develop an understanding of the application and get as much 
information as possible for the eventual implementation review. These are the key 
areas you need to identify by the end of this phase: 
Assets Assets include anything in the system that might have value to 
attackers. They could be data contained in the application or an attached 
database, such as a database table of user accounts and passwords. An asset 
can also be access to some component of the application, such as the 
capability to run arbitrary code on a target system. 
Entry points Entry points include any path through which an attacker can 
access the system. They include any functionality exposed via means such as 
listening ports, Remote Procedure Call (RPC) endpoints, submitted files, or 
any client-initiated activity. 
External entities External entities communicate with the system via its entry 
points. These entities include all user classes and external systems that 
interact with the application. 
External trust levels External trust levels refer to the privileges granted to an 
external entity, as discussed in "Trust Relationships" earlier in this chapter. A 
complex system might have several levels of external trust associated with 
different entities, whereas a simple application might have nothing more than 
a concept of local and remote access. 
Major components Major components define the structure of an application 
design. Components can be internal to the application, or they might 
represent external module dependencies. The threat-modeling process 
involves decomposing these components to isolate their security-relevant 
considerations. 
Use scenarios Use scenarios cover all potential applications of the system. 
They include a list of both authorized and unauthorized scenarios. 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
64 
Developer Interviews 
In many situations, you can save yourself a lot of time by going straight to the horse's 
mouth, as it were. So if you have access to the developers, be sure to use this access 
to your advantage. Of course, this option might not be available. For instance, an 
independent vulnerability researcher rarely has access to the application's 
developers. 
When you approach a system's developers, you should keep a few points in mind. 
First, you're in a position to criticize work they have put a lot of time and effort into. 
Make it clear that your goal is to help improve the security of their application, and 
avoid any judgmental or condescending overtones in your approach. After you have 
a decent dialogue going, you still need to verify any information you get against the 
application's implementation. After all, the developers might have their own 
misconceptions that could be a contributing factor to some vulnerabilities. 
Developer Documentation 
A well-documented application can make the review process faster and more 
thorough; however, there's one major catch to this convenience. You should always 
be cautious of any design documentation for an existing implementation. The reason 
for this caution isn't usually deceitful or incompetent developers; it's just that too 
many things change during the implementation process for the result to ever match 
the specifications perfectly. 
A number of factors contribute to these inconsistencies between specifications and 
the implementation. Extremely large applications can often drift drastically from their 
specifications because of developer turnover and minor oversights compounded over 
time. Implementations can also differ simply because two people rarely have exactly 
the same interpretation of a specification. The bottom line is that you should expect 
to validate everything you determine from the design against the actual 
implementation. 
Keeping this caveat in mind, you still need to know how to wring everything you can 
out of the documentation you get. Generally, you want anything you can get your 
hands on, including design (diagrams, protocol specifications, API documentation, 
and so on), deployment (installation guides, release notes, supplemental 
configuration information, and so forth), and end-user documentation. In binary (and 
some source code) reviews, end-user documentation is all you can get, but don't 
underestimate its value. This documentation is "customer-facing" literature, so it 
tends to be fairly accurate and can offer a process-focused view that makes the 
system easier to understand. 
Standards Documentation 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
65 
If you're asked to examine an application that uses standardized network protocols or 
file formats, a good understanding of how those protocols and file formats are 
structured is necessary to know how the application should function and what 
deficiencies might exist. Therefore, acquiring any published standards and related 
documentation created by researchers and authors is a good idea. Typically, 
Internet-related standards documents are available as requests for comments (RFCs, 
available at www.ietf.org/rfc/). Open-source implementations of the same standards 
can be particularly useful in clarifying ambiguities you might encounter when 
researching the technology a target application uses. 
Source Profiling 
Access to source code can be extremely helpful when you're trying to gather 
information on an application. You don't want to go too deep at this phase, but having 
the source code can speed up a lot of the initial modeling process. Source code can be 
used to initially verify documentation, and you can determine the application's 
general structure from class and module hierarchies in the code. When the source 
does not appear to be laid out hierarchically, you can look at the application startup to 
identify how major components are differentiated at initialization. You can also 
identify entry points by skimming the code to find common functions and objects, 
such as listen() or ADODB. 
System Profiling 
System profiling requires access to a functional installation of the application, which 
gives you an opportunity to validate the documentation review and identify elements 
the documentation missed. Threat models performed strictly from documentation 
need to skip this step and validate the model entirely during the implementation 
review. 
You can use a variety of methods for profiling an application. Here are a few common 
techniques: 
File system layout Look at the application's file system layout and make notes 
of any important information. This information includes identifying the 
permission structure, listing all executable modules, and identifying any 
relevant data files. 
Code reuse Look for any application components that might have come from 
another library or package, such as embedded Web servers or encryption 
libraries. These components could present their own unique attack surface 
and require further review. 
Imports and exports List the function import and export tables for every 
module. Look closely for any libraries used for establishing or managing 
external connections or RPC interfaces. 
The Art of Software Security Assessment - Identifying and Preventing Software Vulnerabilities 
66 
Sandboxing Run the application in a sandbox so that you can identify every 
object it touches and every activity it performs. Use a sniffer and application 
proxies to record any network traffic and isolate communication. In Windows 