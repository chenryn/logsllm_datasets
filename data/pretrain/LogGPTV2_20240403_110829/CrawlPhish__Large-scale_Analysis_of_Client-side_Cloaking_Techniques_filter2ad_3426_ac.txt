hence the screenshot remains empty. Thus, a current limitation
of CrawlPhish is that it cannot detect cloaked websites with
very long execution times, which we explain in Section IX.
However, according to our evaluation, this situation does not
happen often: only in 1.75% of the websites we considered.
Detection algorithm. To perform visual similarity checks
between the screenshots, we implement
the pHash algo-
rithm [42], which compares visual similarity with robustness
and good discrimination. We calculate pHash scores between
the original screenshot and those captured after each path
ﬁnishes execution.
score = pHash(Soriginal, Si), i ∈ [1, 2, ..., n]
(1)
In Formula 1, S represents each screenshot and n is the
number of screenshots captured from forced execution. We
consider two screenshots to be similar (no cloaking) if the
pHash score is less than a threshold (5.0) that we set based
on preliminary testing results on 1,000 phishing websites.
Differing screenshots will have a score of 5.0 or greater.
Figure 3a shows the ROC curve for selecting the visual
similarity threshold. We selected the threshold that provides a
92.00% true-positive rate with a 6.77% false-positive rate. We
note that our evaluation in Section V shows that CrawlPhish
(a) Visual feature threshold.
(b) Code structure feature thresh-
old.
Fig. 3: ROC curves to select thresholds for cloaking detection
and cloaking type categorization.
exhibited higher detection accuracy (98.25%) with a lower
false-positive rate of 1.45% than what was indicated by the
threshold in the ROC curve.
D. Cloaking Categorization
Once CrawlPhish detects the presence of cloaking on a web
page, categorization of the speciﬁc type of cloaking allows us
to measure and understand the prevalence of different high-
level client-side cloaking techniques used by phishers. To
facilitate this categorization, CrawlPhish maintains a cloaking
technique database that contains the code structure features for
each instance of cloaking, annotated with the corresponding
cloaking semantics. Using the database, CrawlPhish can not
only identify known cloaking types, but also provide detailed
information about emerging cloaking techniques.
Initial database. We ﬁrst obtained 1,000 cloaked phishing
websites (true positives), for which we used CrawlPhish to
determine the existence of client-side cloaking. Then, we
manually examined the source code of the phishing websites to
label the corresponding cloaking techniques. We also recorded
code structure features as ground truth.
For example, we labeled one type of cloaking tech-
nique as Mouse Detection if the recorded code features
have the onmousemove event and use the window.
location.href API. Over time, as CrawlPhish executes, if
the presence of cloaking is detected on a website but the code
features do not sufﬁciently closely match any of the records
in the database, the website is ﬂagged for manual review such
that the missing features (and, potentially, new cloaking types)
can be populated. Otherwise,
the website is automatically
labeled with the corresponding semantic cloaking type. Within
the dataset we crawled, manual effort was rarely needed after
we populated the initial database. Thus, this requirement does
not impede the automated operation of our framework.
Categorization algorithm. CrawlPhish employs the Ham-
ming Distance (HD) algorithm [26] to compute the similarity
of the API calls and web events. To this end, we use an
array data structure with one position for each of the 4,012
types of web API calls or events as deﬁned by the Mozilla
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:32:33 UTC from IEEE Xplore.  Restrictions apply. 
1114
00.20.40.60.8100.51TruePositiveRateFalsePositiveRate00.20.40.60.8100.51TruePositiveRateFalsePositiveRateMDN [3, 20], which documents currently available web APIs.
At each position in the array, we store the number of corre-
sponding API calls or events as observed by CrawlPhish. We
then convert this array to a ﬁxed-length string (e.g., string[0]
is the number of ActiveXObject in the code block and
string[1] stores the amount of Date API calls) so that we can
apply the HD algorithm. Thus, the result of the HD algorithm
on a pair of strings represents the similarity of web APIs and
events between two code blocks. Lower HD values indicate
higher similarity.
We also leverage JSInspect [4] to ﬁnd structurally similar
code snippets based on the AST. This will
identify code
with a similar structure based on the AST node types (e.g.,
BlockStatement, VariableDeclaration, and ObjectExpression).
We combine these approaches to overcome limitations of code
similarity checkers based solely on either ASTs or API calls.
Consequently, by comparing the code structure similarity of
all suspicious code blocks against records in the database, all
known cloaking types can be identiﬁed in one website (even if
there are multiple types). If the features of a suspicious code
block are not sufﬁciently similar to any record in the database,
we will manually examine it, label the cloaking type, and then
add it to the database, which is the only process that requires
manual effort in the CrawlPhish framework.
Similar to the visual similarity check, we empirically set a
threshold for the code similarity check based on preliminary
manual analysis of 1,000 cloaked phishing websites. We
consider only two categories to ﬁnd a threshold: correctly
labeled cloaking types and mislabeled cloaking types. Per
Figure 3b, we selected a code structure threshold with a true-
positive rate of 95.83% and a false-positive rate of 0.79%.
When CrawlPhish compares the code structure features of
a new phishing website to ones in our database, the AST
similarity score must be greater than 0.74 and the Hamming
Distance of web APIs and events must be within 34 for a new
website to be marked with a known type of cloaking technique.
V. EVALUATION:
DETECTION OF CLOAKED PHISHING WEBSITES
In this section, we evaluate the client-side cloaking detection
accuracy of CrawlPhish. In this experiment, we ﬁrst randomly
sampled and manually labeled 2,000 phishing websites that
did not contain JavaScript cloaking techniques as well as 2,000
phishing websites with various types of client-side cloaking.
We then ran CrawlPhish to detect if client-side cloaking exists.
Finally, we compared the automated cloaking detection results
against our manually labeled ground truth dataset to calculate
the detection accuracy.
Table III shows the confusion matrix of CrawlPhish’s
detections. Within the 4,000 phishing websites, CrawlPhish
correctly detected 1,965 phishing websites as cloaked and
1,971 as uncloaked, with a false-negative rate of 1.75% (35)
and a false-positive rate of 1.45% (29). Note that unlike a
general phishing detection tool that should prioritize false
positives over false negatives [61], the client-side cloaking
detection component in CrawlPhish does not critically need
Crawled Phishing
Websites From APWG
Actual
Cloaked
Non-cloaked
Analyzed
Cloaked
TP
98.25%
1,965
Non-cloaked
FN
1.75%
35
FP
1.45%
29
TN
98.55%
1,971
TABLE III: Accuracy of cloaking detection by CrawlPhish.
to do so, because the goal of our detection is to study the
nature of client-side cloaking, rather than to detect a phishing
attack. If CrawlPhish trades higher false negatives for lower
or even zero false positives, the study might be less complete
because we might miss many relevant instances of cloaking.
Therefore, the detection of CrawlPhish should balance false
positives with false negatives.
Each of the 29 false-positive cases was caused by one of two
errors. The ﬁrst error was due to the rending overhead of the
unmodiﬁed browser which loaded the original phishing page.
WebKitGTK+, the web browser we used in the CrawlPhish
framework, failed to render the original websites within an
allotted time limit due to a large number of CSS and JavaScript
ﬁles included by the website. As a result, the original screen-
shot of each website was blank, but the screenshots after
forced execution were not blank, so CrawlPhish mislabeled
the corresponding websites as cloaked because the screenshots
differed before and after forced execution. The second error
was caused by inaccuracies in our image similarity checks.
The image similarity check module erroneously distinguished
between screenshots of identical pages due to slight variations
in the page layout generated by the browser with and without
forced execution.
In terms of the false negatives, we found that 32 out of the
35 stemmed from a long execution time of cloaked phishing
websites (similar to the ﬁrst reason for false positives). Forced
executed screenshots are not taken if an execution path takes
too long to ﬁnish execution. We used a 195-second execution
time window for each execution path. However, the paths
that CrawlPhish does not execute due to a timeout may
contain cloaking technique implementations. Without those
screenshots, CrawlPhish cannot detect the cloaking technique,
so it mislabels the corresponding website as uncloaked.
In three rare cases, real phishing websites appeared nearly
blank due to low page contrast. For example, if phishing
websites have a white background with light text, CrawlPhish
would not distinguish between the corresponding screenshot
and a blank one. We manually examined these cases and found
that CSS inclusions were missing from those websites (i.e.,
they could not be retrieved by our crawler).
Client-side cloaking occurrence statistics. Within our dataset
of 112,005 phishing websites, CrawlPhish found that 35,067
(31.31%) phishing websites implement client-side cloaking
techniques in total: 23.32% (6,024) in 2018 and 33.70%
(29,043) in 2019. We note that cloaking implementations
in phishing grew signiﬁcantly in 2019. We hypothesize that
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:32:33 UTC from IEEE Xplore.  Restrictions apply. 
1115
(a) Initial appearance.
(b) Force-executed appearance.
Fig. 4: Initial and force-executed appearance of a phishing
website with Pop-up cloaking.
phishers are either leveraging such cloaking because it in-
creases their proﬁtability or because improving detection sys-
tems make advanced evasion necessary, or both.
VI. EVALUATION: CLOAKING CATEGORIZATION
In this section, we elaborate on the eight types of client-side
cloaking techniques detected by CrawlPhish (as previously
introduced in Table II). We also evaluate the accuracy of
CrawlPhish’s semantic cloaking categorization, track trends in
the deployment and evolution of different implementations of
these cloaking techniques, and analyze how frequently they
are used.
A. Categorization of Cloaking Types
User Interaction: Pop-up. With this technique, phishing
content remains hidden until a button in a pop-up window is
clicked. Speciﬁcally, JavaScript code listens for an onclick
event to evade anti-phishing bots. Figure 4 shows an example
of a phishing website that implements this technique. The
website in Figure 4a initially shows an alert window to an
anti-phishing bot or a real user. Thus, this phishing website
seeks to evade detection by anti-phishing bots because no
phishing content or typical attributes (such as a login form
or logos of a legitimate organization) are found on the page.
However, CrawlPhish reveals the phishing content hidden
behind the popup window as shown in Figure 4b.
Figure 5 shows a more advanced version of the pop-up
cloaking techniques that CrawlPhish detected. Because an
alert window can easily be closed through common browser
automation frameworks such as Selenium [28] or Katalon [5],
some phishers instead use the Web Notiﬁcation API [58].
We observed that due to technical limitations, top automation
frameworks [8] do not currently support
interaction with
web notiﬁcations. These automated browsers opt to disable
the notiﬁcation window to avoid such interactions. Phishers,
however, only allow visitors who actually click the “Allow”
button to access the phishing content. Therefore, because the
phishing website will not show any phishing content until a
visitor clicks the “Allow” button in the notiﬁcation window, it
will evade detection. Phishers use a deceptive web page that
asks visitors to click the button on the notiﬁcation window, as
shown in Figure 5. As an added beneﬁt to attackers, by using
a notiﬁcation window, cloaked phishing websites could also
directly send spam to visitors through their browsers (we do
Fig. 5: A phishing website with the evolved Pop-up (Notiﬁca-
tion) cloaking technique, in which the web page directs human
visitors to click on the “Allow” button by showing an arrow.
not evaluate the extent of such abuse). Through this, we show
that criminals are using cutting-edge browser features to evade
existing detection systems.
User Interaction: Mouse Detection. This cloaking type seeks
to identify whether a website visitor is a person or an
anti-phishing bot by waiting for mouse movement before
displaying the phishing content. Speciﬁcally,
the cloaking
code listens for the onmousemove, onmouseenter, or
onmouseleave events. This technique is used frequently by
phishers, and accounts for 16.53% of all cloaking technique
implementations in Table V, because most people have a habit
of moving the mouse while a website is rendering in the
browser [50].
User Interaction: Click Through. Some phishing websites
require visitors to click on a speciﬁc location on the page
before displaying phishing content [60]. Simple variants of this
cloaking technique require visitors to click on a button on the
page and are, thus, similar to alert cloaking. However, more
sophisticated variants display fake CAPTCHAs that closely
mimic the look and feel of Google’s reCAPTCHA [56]. Given
the common use of reCAPTCHA by legitimate websites,
phishing websites with fake CAPTCHAs make it difﬁcult for
potential victims to identify that they are fake. If anti-phishing
systems cannot access phishing content because of the Click
Through technique, they may fail to mark the websites as
phishing.
Bot Behavior: Timing. Some phishing websites
show
phishing content only at a certain time, or deliberately
make rendering slow by using the setTimeout() or
Date.getTime() APIs. If phishing websites take a longer
time to render than thresholds set by detection systems, such
websites can evade detection. Actual visitors, however, might
wait for the completion of web page rendering [19].
Bot Behavior: Randomization. Some phishers try to evade
detection by using a non-deterministic mechanism: such phish-
ing websites generate a random number before the page is
rendered, and only show phishing content if a certain threshold
is met. Anti-phishing crawlers or human inspectors may not
visit the same website again if it initially shows benign content.
Therefore, this technique may appear to be a “dumb” way to
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:32:33 UTC from IEEE Xplore.  Restrictions apply. 
1116
Cloaking Technique
Category
Fingerprinting
User
Interaction
Bot
Behavior
Type
Cookie
Referrer
User-Agent
Pop-up
Alert
Notiﬁcation
Click Through
Mouse Detection
Randomization
Timing
Public Dataset
Top Group
Count
437
156
563
249
52
1,541
138
42
387
Percentage
15.01%
5.85%
53.31%
3.26%
4.22%
22.88%
6.81%
16.03%
7.76%
Unique
Groups
43
27
65
424
29
105
87
73
597
Unique
Groups
28
37
33
335
17
51
108