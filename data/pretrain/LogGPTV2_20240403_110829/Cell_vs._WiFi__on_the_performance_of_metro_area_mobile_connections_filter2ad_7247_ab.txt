4,311
9,599
504
903
340
Android
# WiFi
tests
100,794
181,928
34,437
6,331
6,808
9,625
3,894
4,058
872
82,700
7,192
18,865
756
1,097
621
# cell
tests
353,784
606,564
104,667
18,975
9,898
14,012
5,655
11,498
806
37,767
3,964
13,101
749
1,947
289
the local Ookla server that includes the client IP, device type and OS
version, client geographic coordinates (longitude / latitude), server
name and coordinates, great-circle distance from the client to the
server (computed using the Haversine formula), timestamp, upload
and download speeds (in kb/s), latency (in milliseconds), access
type (cellular or WiFi), and the cellular carrier or WiFi network
provider.
In the Android data set, for some tests we have ﬁner
grained information about the speciﬁc cellular access type (e.g.,
EDGE, HSPA, EVDO-A, or LTE). For the iOS data, no such ﬁne-
grained information exists; we only know whether the access is via
cell or WiFi. For each of the apps, we also have a unique device
ﬁngerprint that allows us to identify measurements initiated by the
same handset (user) even if the test is initiated using a different
access technology or from a different service provider.
2.2 Data Sets Considered
The data we consider in our initial evaluation were collected
from servers located in 15 metro areas over a period of 15 weeks
from February 21, 2011 through June 5, 2011. In each case the
metro areas are locations with Speedtest servers. Selection of the
sites was based at a high level on attempting to amass a manageable
data corpus, yet one that provides a broad perspective on cellular
vs. WiFi performance in metro areas that are diverse in their ge-
ographic, socio-economic and behavioral characteristics.
In par-
ticular, we focus on three different metro area types in the US,
small (Lawrence, KS; Jackson, TN and Missoula MT), medium
(Madison, WI; Syracuse, NY and Columbia SC) and large (New
York, NY; Los Angeles, CA and Chicago, IL). We also include
metro areas in Europe (Belgrade, Serbia; Brussels, Belgium and
Manchester, United Kingdom), and in Asia/Paciﬁc (Ulaanbaatar,
Mongolia; Almaty, Kazakhstan and Palembang, Indonesia). The
speciﬁc choices were made primarily based on market size with an
attempt to select areas for each category that had roughly the same
population. While the speciﬁc geographic boundaries of the US
metro areas are deﬁned by the US Census bureau, European and
Asian markets do not deﬁne metro areas in the same way. Thus,
for each server we only include tests that are conducted within a
100 km radius of a given server. Details of the individual metro
areas and their associated Speedtest data sets can be found in Ta-
ble 1. As shown in the table, the markets vary widely in terms of
socio-economic characteristics and Speedtest use.
On average, we observe tests initiated by 7,551 handsets per
day to the 15 servers for which we have data (3,863 by iOS users
and 3,688 by Android users). From these handsets, an average of
14,961 individual tests are initiated per day from cellular access,
and 15,521 per day using WiFi. Interestingly, for the Android app,
there are 11,273 tests per day on average via cellular technology,
and 4,380 via WiFi, while for the iOS app, there are only 2,464 tests
via cellular technology per day on average, compared with 11,141
via WiFi. Also, in 9,230 cases, we observe the same handset in at
least two different metro areas. Moreover, the distribution of the
number of tests initiated per handset is skewed. On average, there
are 6.0 tests per handset, with a (rather high) standard deviation of
17.4. Our data also include a great deal of diversity with respect
to handset type and operating system version. Table 2 shows the
unique number of devices and unique device/OS pairs (including
different OS versions) per site, as well as the top three devices (and
percentage share) for each site. Interestingly, while the number of
tests per site is generally dominated by Android devices, the iPhone
is the singularly most popular device, and other iOS-based devices
are also very popular.
Data for each test include highly accurate GPS-derived geographic
coordinates for the location of each test. The coordinates are only
taken once during a test so we cannot tell from a single test whether
or not a client was moving during a test. There are, however, in-
stances in our data sets where multiple tests are run consecutively in
relatively close geographic proximity and when plotted on a map,
we can see that the positions follow roads perfectly. Thus, we can
infer that a subset of our tests were run while users were traveling.
Figure 2 shows an example of the positions of all cellular clients
that access the Los Angeles, CA, Manchester, UK and Lawrence,
KS servers during the 15 week test period. Maps of WiFi client
locations from these metro regions, and maps of client locations
from other metro areas have similar proﬁles.
In the large metro
areas, cellular and WiFi tests are conducted with more uniformity
over the highly populated metro area; in smaller metro areas, there
are tight clusters of test locations in densely populated subregions
with sparser use in less populated areas. In short, test locations cor-
relate highly with population density. (In the Los Angeles, CA and
Lawrence, KS plots shown in Figure 2, we show US zip code di-
visions, with more densely populated zip codes shaded darker than
more sparsely populated zip codes.) This variable proﬁle suggests
Table 2: Handset diversity: numbers of unique devices and device/OS pairs for each of the 15 servers, as well as the top three devices
for each site (and percentage share of all devices for that site).
Location
New York, NY
Los Angeles, CA
Chicago, IL
Columbia, SC
Syracuse, NY
Madison, WI
Jackson, TN
Lawrence, KS
Missoula, MT
Manchester, UK
Brussels, BE
Belgrade, SP
Palembang, ID
Almaty, KZ
Ulaanbaatar, MN
Unique devices
473
558
320
125
124
135
79
124
51
412
178
309
68
124
94
Unique device+OS
1223
1340
925
265
253
273
154
246
99
899
354
613
124
239
158
1st (%)
iPhone (26.0)
iPhone (24.9)
iPhone (20.9)
iPhone (20.9)
iPhone (30.6)
iPhone (29.9)
iPhone (46.1)
iPhone (26.4)
iPhone (31.1)
iPhone (52.0)
iPhone (43.6)
iPhone (18.2)
iPhone (29.4)
iPhone (49.7)
iPhone (36.2)
Three most popular devices
3rd (%)
2nd (%)
HTC Supersonic (8.4)
iPad (8.8)
iPad (8.4)
HTC Supersonic (10.7)
iPad (8.4)
HTC Mecha (13.3)
HTC Supersonic (9.8)
HTC Mecha (14.7)
iPod touch (10.2)
iPad (12.0)
iPad (14.4)
iPod touch (12.3)
iPad (8.5) Motorola Droid 2 (6.9)
iPad (7.8)
iPod touch (15.1)
iPod touch (8.8)
iPod touch (8.9)
HTC Buzz (8.2)
iPad (8.2)
HTC Supersonic (4.6)
iPod touch (9.5)
HTC Supersonic (20.4)
iPad (17.7)
iPad (11.7)
iPad (11.4)
HTC Bravo (8.9)
Samsung GT-P1000 (8.6)
iPad (11.2)
iPad (15.6)
Figure 2: Locations of Speedtests by cellular users in the Los Angeles, CA (left), Manchester, UK (center) and Lawrence, KS (right)
metro areas during the measurement period of February 21 to June 5, 2011. Area included in each plot is approximately 50km by
50km.
the need for a more detailed spatial analysis, which we describe in
Section 3. In all cases, there is a high degree of overlap between
cellular and WiFi test locations.
2.3 Discussion
We argue that the Speedtest data are compelling from the per-
spective of their richness, availability in a huge number of markets
and broad adoption by users. However, there are several limita-
tions that are important to acknowledge since they could inﬂuence
the conclusions drawn from our study.
Speedtest data are crowd-sourced and rely on users invoking and
running the throughput test application. While we have some abil-
ity to distinguish between handset types (for iOS devices, we do
not know the hardware generation, but for Android devices, we
have the speciﬁc model number), device conﬁgurations can vary,
especially with jail-broken devices. Thus, there could be variations
that could lead to biases in the test results. For example, we have
no way of knowing whether a given test is run indoors or outdoors.
Similarly, when and where the application is invoked depends en-
tirely on how individuals derive value from the tests. We expect
that the application will often be used when performance with the
local default technology is perceived to be poor. Hence, the results
of the performance tests might tend to be biased toward poor oper-
ating conditions for each technology. However, we have no way of
establishing baselines for performance or assessing testing bias in
each metro area that we consider other than appealing to statistical
characterizations and the relatively large numbers of tests included
in our data. Lastly, although we are able to identify individual users
in our data via unique device identiﬁers, our comparisons between
cellular and WiFi performance are at an aggregate level. In future
work, we intend to carefully examine cellular versus WiFi perfor-
mance on an individual user basis.
We are limited, to a certain extent, in our spatial analyses by the
fact that we do not have up-to-date ground truth locations of all cell
towers and WiFi access points that provided connectivity for hand-
sets for all tests. In densely populated areas there are likely to be
thousands of access points operated by many different providers.
These would provide natural anchor points for spatial clustering
of the performance data. The difﬁculty in assembling these data
sets for diverse markets is substantial. While regulatory agencies
such as the FCC keep databases of cell tower locations [16], they
are often incomplete, and similar databases in non-US markets are
sometimes difﬁcult to obtain. However, we plan to consider cell
tower locations as identiﬁed in sources such as [2] in future evalua-
tions of our data. There are similar archives for WiFi access points
e.g., [8], but the completeness of these databases is unknown.
Economic considerations certainly come into play for all con-
stituencies (users, service providers and application designers) men-
tioned or discussed in this paper. For users, connectivity may be
subject to data transfer limits and trafﬁc shaping. Although WiFi
user plans are rarely data-quantity limited, they are partitioned among
openly available for free, openly available for paying users, and pri-
vate connectivity.
Service providers make decisions on infrastructure density based
on many different issues including projected user growth, risks as-
sociated with losing customers due to under provisioning and ge-
ographic expansion of service. Finally, application designers must
carefully consider how to manage data transfers so that user experi-
ence under expected conditions is acceptable. Otherwise, they risk
losing customers. While these issues are fascinating and certainly
play a role in the use of mobile devices, drawing explicit lines be-
tween the Speedtest measurements and economic issues is a subject
of future study.
3. EVALUATION METHODOLOGY
Our evaluation takes a top-down approach to assessing the spatio-
temporal performance characteristics of cellular and WiFi through-
put and latency in the target metro areas. This section describes the
methods that we use to evaluate Speedtest data toward the goal of
being able to draw conclusions about the relative capabilities and
robustness of each technology.
3.1 Basic Performance Characteristics
We begin by calculating the basic statistical characteristics of
performance for each technology including maximum, average, min-
imum and standard deviation over the entire 15 week period in each
of the 15 metro areas. This analysis does not distinguish between
times of day or subregions within a given metro area. As such, it ig-
nores the more complex and potentially interesting characteristics
of performance. However, it does enables us to begin to understand
the data from an aggregate perspective and establish simple rank-
ings between area types (i.e., large, medium, small, Europe, and
Asia) and rankings of metro areas within each area type.
From this coarse view of the data, we drill down by analyzing
per-handset performance measures. For the set of tests initiated
by each handset in a metro region, we separate the series of tests
by access technology (WiFi, cell, or some more detailed cell ac-
cess type) and by service provider. To obtain the service provider,
we use the IPv4 to autonomous system mapping data provided by
www.team-cymru.org. From this grouping, we can compute
summary statistics such as the median, mean, or 95th percentile
for throughput and latency for a given handset (user) when using a
given access provider and access technology in a given market. We
then plot scatterplots of upload vs. download throughput to com-
pare the throughput performance that different users obtain from
different networks and access technologies. We also compute per-
formance consistency measures using the same method of [33].
In particular, we plot CDFs of normalized per-handset through-
put and latency performance. The normalization is performed by
taking the average divided by the 95th percentile in the case of up-
load/download throughput, or taking the average divided by the 5th
percentile in the case of latency.
3.2 Temporal Characteristics
The diurnal behavior of Internet trafﬁc is one of its most well-
known empirical characteristics. Prior studies of WiFi networks
(e.g., [22]) and cellular trafﬁc (e.g., [20]) have shown that diurnal
usage patterns are also evident. The goal of our temporal analysis is
to assess the extent to which client tests follow a diurnal pattern and
how the expected diurnal use of cellular and WiFi has an impact on
performance in our target metro areas. By drilling down on smaller
time windows, we also expect to be able to observe and characterize
anomalous events such as outages and periods of degraded service.
Our temporal analysis considers two characteristics: client use
versus time, and performance versus time. In the case of the for-
mer, we plot the aggregate hourly invocations of the test application
over the 15 week period. In the case of the latter, we plot the aver-
age hourly upload and download performance for cellular and WiFi
over the 15 week period. We also compute the per-handset average