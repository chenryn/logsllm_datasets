title:Glitching Demystified: Analyzing Control-flow-based Glitching Attacks
and Defenses
author:Chad Spensky and
Aravind Machiry and
Nathan Burow and
Hamed Okhravi and
Rick Housley and
Zhongshu Gu and
Hani Jamjoom and
Christopher Kruegel and
Giovanni Vigna
Glitching Demystiﬁed: Analyzing
Control-ﬂow-based Glitching Attacks and Defenses
Chad Spensky∗¶(cid:107), Aravind Machiry†, Nathan Burow‡, Hamed Okhravi‡, Rick Housley§, Zhongshu Gu¶,
Hani Jamjoom¶, Christopher Kruegel(cid:107), Giovanni Vigna(cid:107)
∗Allthenticate †Purdue University ‡MIT Lincoln Laboratory §River Loop Security
¶IBM T.J. Watson Research Center (cid:107)UC Santa Barbara
PI:EMAIL, PI:EMAIL, {Nathan.Burow, hamed.okhravi}@ll.mit.edu,
PI:EMAIL, {zgu, jamjoom}@us.ibm.com, {chris, vigna}@cs.ucsb.edu
Abstract—Hardware fault injection, or glitching, attacks can
compromise the security of devices even when no software
vulnerabilities exist. Attempts to analyze the hardware effects of
glitching are subject to the Heisenberg effect and there is typically
a disconnect between what people “think” is possible and what
is actually possible with respect to these attacks. In this work,
we attempt to provide some clarity to the impacts of attacks and
defenses for control-ﬂow modiﬁcation through glitching. First,
we introduce a glitching emulation framework, which provides
a scalable playground to test the effects of bit ﬂips on speciﬁc
instruction set architectures (ISAs) (i.e., the fault tolerance of
the instruction encoding). Next, we examine real glitching experi-
ments using the ChipWhisperer, a popular microcontroller using
open-source glitching hardware. These real-world experiments
provide novel insights into how glitching attacks are realized
and might be defended against in practice. Finally, we present
GLITCHRESISTOR, an open-source, software-based glitching de-
fense tool that can automatically insert glitching defenses into
any existing source code, in an architecture-independent way.
We evaluated GLITCHRESISTOR, which integrates numerous
software-only defenses against powerful and real-world glitching
attacks. Our ﬁndings indicate that software-only defenses can be
implemented with acceptable run-time and size overheads, while
completely mitigating some single-glitch attacks, minimizing the
likelihood of a successful multi-glitch attack (i.e., a success rate
of 0.000306%), and detecting failed glitching attempts at a high
rate (between 79.2% and 100%).
I. INTRODUCTION
Hardware-induced faults [33], which we refer to as glitches,
are capable of corrupting the system state by modifying both
instructions and data, and can be leveraged to undermine
software-based security mechanisms, even if the software
security mechanisms are implemented with no semantic vul-
nerabilities. Indeed, malicious glitches have been leveraged to
compromise secure smartcards [13], [8], [7], security-hardened
gaming consoles (e.g., the XBOX 360 [60], Playstation 3 [39],
Playstation Vita [45], and Nintendo Switch [69], [27]), and
enterprise Internet protocol (IP) phones [20]. Glitching attacks
have even been leveraged to bypass both Intel’s Software
Guard Extension (SGX) protections [52] and ARM’s Trust-
Zone [68] and even extract hardware-embedded cryptographic
keys [43]. However, little has been done to adequately study
and defend against these types of attacks in practice. Some
code-level glitching mitigations [83] have been proposed,
but have not had their underlying assumptions or efﬁcacy
evaluated on real-world systems. Alternatively, custom-built
hardware-based counter-measures (e.g., brownout detection
or lock-step computation) [21] are currently only sparsely
deployed, due to cost and complexity, leaving the majority
of embedded systems susceptible to glitching attacks.
Glitching attacks involve introducing a physical disturbance
to a system that will ultimately corrupt the instructions being
executed or the data being manipulated. This corruption can
be achieved by changing the supply voltage [44], [14], optical
probing with lasers [72], [80], disrupting the clock [5], or
introducing an electromagnetic pulse (EMP) [58], [49]. To
leverage these faults in a successful attack, the fault must
be injected at a speciﬁc time in the execution pipeline. For
example, if the execution was corrupted precisely when a
security-critical branch condition was being checked (e.g.,
checking the kernel’s signature [23]), that instruction could be
changed to a no operation instruction, and effectively skipped,
allowing the attacker to disable secure boot [20], [78], escalate
privileges [77], or extract “protected” code [45].
While effective defenses against other physical attacks are
becoming commonplace in commodity computing systems
(e.g., trusted boot and encrypted memory), glitching defenses
are still lacking. We hypothesize that this is likely due to a
general lack of understanding about what exactly glitching
attacks are capable of, and, subsequently, a systematic way to
implement defenses against them. Indeed, we have observed a
large disconnect between theory and practice in this ﬁeld. For
example, many researchers believe that glitching is capable of
changing any pointer (e.g., the program counter) in memory
or making arbitrary code modiﬁcations because of published
papers demonstrating this [77], [79], [31]. However, these
effects are only realistic in laboratory environments with
systems that are well understood and have already had the
appropriate glitching parameters “tuned.” For all intents and
purposes, these types of attacks are impossible in practice.
In this work, we introduce an open-source QEMU-based
glitching emulation environment. This framework was used
to exhaustively evaluate an ISA’s instruction encoding against
speciﬁc glitching effects (e.g., bit ﬂips), and examine the result
of those instruction-level effects against a program’s control
ﬂow. These ﬂipped bits ultimately change the instruction being
executed or the data being evaluated in a way that is beneﬁcial
to the attacker. In fact, our analysis conﬁrmed that by simply
ﬂipping bits, the glitch can effectively “skip” an instruction
with a high likelihood (i.e., changing the targeted instruction
into a no operation). We also found that this effect is often
non-uniform. For example, on 16-bit ARM processors, glitches
that tend to ﬂip bits from 1 to zero appear to be exceptionally
powerful (i.e., “skipping” all branch instructions more than
60% of the time), while glitches that ﬂip zeros to ones were
less so (i.e., “skipping” branches less than 30% of the time).
In addition to emulating glitches, we also used a popular
glitching tool (i.e., the ChipWhisperer [55]) to conduct a suite
of real-world glitching experiments to examine the effects
of glitching on control-ﬂow-related instructions and data. In
particular, our experiments were focused on using glitching to
evade guard conditions. This evasion could be used to bypass
security-critical code (e.g., verifying signed code, disabling a
debug interface, or checking user permissions). Our real-world
glitching results provide new insights into how this corruption
ultimately affects control ﬂow. For example, load and store in-
structions appear to be more susceptible to glitching; the value
being compared affects the glitchability of a branch condition
(e.g., while(!a) is more vulnerable than while(a)); and
instructions which simply manipulate registers (e.g., addition)
appear to be exceptionally difﬁcult to glitch. We leverage these
ﬁndings to build our defense framework.
We present the ﬁrst automated, open-source glitching de-
fense framework, GLITCHRESISTOR, which is capable of
adding various glitching defenses at compile time to any
source code in an architecture-independent way. GLITCHRE-
SISTOR implements numerous proposed glitching defenses
(e.g., double checking branches and loop guards, injecting ran-
dom timing, and integrity checking on sensitive variables). We
used GLITCHRESISTOR, combined with our ChipWhisperer-
based glitching framework to evaluate the efﬁcacy of these de-
fenses in practice, examining their ability to thwart glitching,
as well as the size and run-time overheads that each incurs.
GLITCHRESISTOR was able to successfully defend against,
and detect, every single-glitch attack that we attempted in
our evaluation, necessitating a successful multi-glitch attack
(i.e., a glitch that affects multiple clock cycles) to evade the
implemented defenses. Even so, GLITCHRESISTOR was able
to reduce the success rate of our most powerful, multi-glitch
attack to 0.263% in the worst case and 0.00306% in the best
case, with detection rates of 79.2% and 99.7% respectively.
In summary, we make the following contributions:
• a comprehensive analysis of glitching attacks and their
effects on control ﬂow,
• a framework for emulating glitching attacks,
• a breadth of glitching experiments that characterize the
effects of glitching and demonstrate the effectiveness of
various software-only defenses,
• GLITCHRESISTOR
(https://github.com/ucsb-seclab/
glitch resistor), the ﬁrst extensible glitching defense tool
for automatically protecting vulnerable code, and
• an evaluation of GLITCHRESISTOR on real hardware,
which demonstrates the effectiveness of software-only
defenses, minimizing the likelihood of a successful attack
and effectively detecting all glitching attempts in practice.
II. BACKGROUND
Fault injection is well-studied in the context of ensuring the
reliability of a computer system [33]. Both software [25] and
hardware [6] induced faults are capable of modifying the state
of a system and disrupting its typical execution. Indeed, the
act of inducing malicious software faults, which materialize
as software bugs and vulnerabilities, has spawned an entire
subﬁeld of bug ﬁnding [73] and fuzzing techniques [1]. In
contrast, malicious hardware-induced hardware faults were
widely ignored by the software community until the relatively
recent exposure of Spectre [35], Meltdown [42] (microarchi-
tecture attacks), and Rowhammer [71], [34], an attack against
dynamic random access memory (DRAM). Malicious physical
hardware-induced faults are still relatively unexplored.
Hardware-based attacks can be done either invasively (e.g.,
decapsulating the chip [30]) or non-invasively (e.g., through
electromagnetic interference [57]). Non-invasive glitching
techniques allow an attack to go undetected and typically
permit the attacker to repeat the attack indeﬁnitely. The general
idea behind glitching is to interfere with a Flip-Flop circuit,
transistor, or capacitor’s normal operation to change the stored
value or the execution’s output. This can be done using any
form of interference, be it an external physical phenomenon,
like temperature or electromagnetic (EM) interference, or by
operating the system outside its designed conditions (e.g., by
modifying the voltage or clock). In practice, voltage glitching,
which is done by either increasing or decreasing the voltage
for a brief period of time, and clock glitching, which in-
volves inserting additional clock edges, are the most common
glitching techniques, due to their relatively low cost and their
effectiveness.
In this work, we only examine non-invasive attacks, as
defenses against invasive attacks [24] necessarily require hard-
ware modiﬁcations. For those interested in the speciﬁc effects
of each type of non-invasive glitch, we refer the reader to
Section 5.3 of the resulting dissertation [74].
A. Motivation
Glitching attacks have already been used to attack numerous
commercial systems. For example, researchers were able to use
glitching to defeat the security on two automotive safety in-
tegrity level (ASIL)-D1 compliant automotive microcontroller
units (MCUs) [82], evading hardware-based countermeasures
like Flash error-correcting code (ECC) and lockstep execu-
tion, using EM and voltage glitching, respectively. The same
researchers were also able to bypass authentication checks, and
even re-enable the Joint Test Action Group (JTAG) interface.
Similarly, voltage glitching has also been used to extract both
Rivest, Shamir, and Adleman (RSA) [9], [70] and advanced
1The most stringent ASIL requirements of safety and fault tolerance.
Fig. 1: The three parameters that need to be tuned for clock
glitching: the offset from the trigger, the offset into the clock
cycle, and the width of the injected clock cycle
encryption standard (AES) [10], [43] keys, and has even
been shown to be effective against programs executing on
modern Android phones and the Raspberry Pi, both running
Linux [54]. More powerful attacks have even been able to
control the program counter (PC) directly with glitching [79],
[31]. In the case of defeating a secure boot loader, which
has a relatively small attack surface and takes little or no
user input, glitching attacks are one of the only methods for
compromising the boot loader’s security.
B. “Tuning” the Glitch
All glitching techniques necessarily require a “tuning” phase
where the location and speciﬁc glitching parameters are
tweaked until the desired effect is achieved. The attacker must
ﬁrst ﬁgure out when to inject the glitch, by calculating an offset
from a known trigger (i.e., an observable artifact that indicates
which code is currently executing). For example, to inject a
clock glitch, an attacker must simultaneously conﬁgure both
the width and location in the clock cycle to inject a glitch, as
well as the offset from an observable trigger (see Figure 1).
Similar parameters must be tuned for both voltage and EM
glitches (e.g., the duration and voltage of the attack or the
location and intensity of the EMP).
In our ideal laboratory environment with a perfect trigger,
we were able to consistently, and automatically, tune our clock
glitching parameters and successfully glitch an unprotected
embedded system 100% of the time (10 out of 10 attempts)
in less than 16 minutes, in the best case. However, this is only
possible with an initial search over the parameter space, which
is the exact step that our evaluated defenses are targeting.
C. Defenses
Hardware-based defenses typically involve inserting ad-
ditional circuits (e.g.,
to detect voltage glitches [84]), an
additional run-time monitor [66], [4], or control-ﬂow integrity
(CFI) signatures [67], [81]. However, hardware modiﬁcations
are impractical for the many already-deployed Internet of
things (IoT) devices. They are also far less likely to be adopted
for individual systems, due to the lead times on hardware
fabrication. Therefore, software-based techniques are more
likely to be useful as practical defenses.
Software-based glitching defenses can never completely
mitigate the problem. In the limit, glitching could (in theory)
be used to skip every defensive instruction and even trans-
form benign instructions into malicious ones. Nevertheless,
software-based techniques are cheaper to implement and can
be effective at defending against real-world attacks (in prac-
tice) by making the required scenario for a successful glitching
attack increasingly improbable. Unfortunately, existing tech-
niques, which rely on redundancy [50], only work on simple
code-bases and have simplistic attacker models, which makes
them infeasible on real-world code.
III. THREAT MODEL
Non-invasive glitching attacks require physical access to
the device being glitched and control over the speciﬁc input
being glitched (e.g., the voltage line, clock line, or access to
microchip). An attacker can dismantle any external packaging
(e.g., remove the case containing the electronic components),
but cannot modify the electronic components in any non-
reversible way. For example, an attacker may solder a wire to
a speciﬁc pin to bypass a voltage regulator, but cannot remove
or modify the integrated circuit (IC) directly.
This threat model is realistic for any deployed embedded
system: IoT devices, gaming systems, automobiles, robots, or
military drones. The goal is typically to either bypass integrity
checking of the ﬁrmware or extract the ﬁrmware image for
reverse engineering. As previously mentioned, the system must
necessarily have some externally observable trigger to create
a reliable glitch (e.g., a voltage dip, an observable output, or
a request for user input). In the various high-proﬁle glitching
attacks against gaming systems [60], [39], [45], [69], [27],
the exploits were crafted by ﬁrst identifying the approximate
area that appeared to be vulnerable (e.g., right before an error
code) and then tuning the glitching parameters (e.g., clock
waveform, voltage modiﬁcation, or EM power and position).
No two systems are physically identical, which means that
each attack must be specialized for the speciﬁc system being
attacked. Even commercialized attacks (e.g., the XBOX reset
attack) are typically probabilistic, due to physical limitations,
and have some method for automatically retrying the glitch in
the event of a failure.
IV. GLITCHING EFFECTS IN EMULATION
To gain a better understanding of the theoretical limit on
the effectiveness of glitching, we ﬁrst investigate the following
research question:
RQ1 What is the likelihood that random bit ﬂips will result
in a “skipped” control-ﬂow instruction?
To quantify the effects of bit ﬂips on a speciﬁc ISAs, we
built an emulation framework that is capable of forcing bit
ﬂips (i.e., corrupting speciﬁc instructions) and executing the
resulting code to determine the effects on the control ﬂow of
the program. Previous literature [36], [65], [49], [79], [75],
[5] indicates that bit ﬂips induced by glitching tend to be
unidirectional (i.e., either ﬂipping 1s to 0s or 0s to 1s, but
not both). While complex bit ﬂips are possible,
they are
improbable in practice [79]. Therefore, we only present the
results for unidirectional ﬂips for our evaluation (i.e., logical