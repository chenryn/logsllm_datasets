1
nginx
lighttpd
bind
beanstalkd
geomean
)
d
e
z
i
l
a
m
r
o
n
(
d
a
e
h
r
e
v
o
e
m
i
t
-
n
u
R
4
3
2
1
1 2 3 4
8
2
.
2
9
3
.
1
4
4
.
1
3
4
.
1
6
3
.
2
7
7
.
1
7
7
.
1
8
6
.
1
6
2
.
3
2
3
.
2
6
4
.
1
8
4
.
1
8
3
.
1
3
4
.
1
5
4
.
1
8
3
.
1
5
6
.
1
3
1
.
1
4
1
.
1
7
1
.
1
nginx
lighttpd
bind
beanstalkd
geomean
Fig. 5. Overhead using the Comprehensive security policy for increasing
number of variants.
Fig. 7.
Overhead using the Information disclosure security policy for
increasing number of variants with our variant generation strategy disabled.
all our programs. To fully saturate all our server programs,
we then reran our benchmarks through the loopback interface.
We note that this strategy resulted in sound but also more
pessimistic performance results. As an example, the original
Dune paper reports ∼1% overhead on lighttpd for an over-
the-network conﬁguration [28]. Reproducing the exact same
experiment in our setup (which, in contrast, relies on the
loopback interface) resulted in much higher impact (∼12%).
This is caused by less networking overhead, making the impact
of our system more visible.
We evaluated MvArmor’s performance across all the se-
curity policies supported. Figure 4 shows the results for the
Code execution security policy for an increasing number of
variants (1 through 4, where 2 variants means 1 leader and
1 follower). Since the server applications do not normally
execute any syscalls that fall under the Code execution policy,
the results are identical to a policy where no syscalls are
considered sensitive. When disabling our variant generation
strategy (isolating the MVX synchronization overhead) for
both policies, we observed no differences in the results. This
shows that the followers are indeed able to keep up with the
leader despite the less efﬁcient secure allocator, hence our
variant generation strategy has essentially no impact. Note that
security policies have no effect on scenarios with only one
variant, as there is no synchronization in such scenarios.
Figure 6 reports results for a slightly more conservative
policy (Information disclosure). As our server applications
make heavy use of write syscalls, which run in lockstep now
(forcing idle time in the followers to be spent waiting for the
leader rather then performing extra operations), the overhead of
the slower followers (and of our variant generation strategy)
cannot be completely masked in this case. For comparison
purposes, Figure 7 reports results for the same policy, but with
our variant generation strategy disabled. Finally, Figure 5 re-
ports results for our most conservative security policy possible
(Comprehensive, generally overly conservative, but useful to
provide worst-case results). In this conﬁguration, the impact
of our variant generation strategy (and custom allocator in the
followers) is more noticeable given that all the syscalls run
in lockstep (e.g., 55.2% vs. 49.1% for two variants, geometric
mean—or geomean).
As shown in the ﬁgures, programs with a lower number
of “copy-heavy” syscalls such as nginx and lighttpd scale
less efﬁciently with the number of variants and are also more
affected by the increasingly lockstep-like behavior enforced by
more conservative security policies (e.g., full lockstep for Com-
prehensive). Beanstalkd, on the other hand, issues relatively
few syscalls overall and the reported overheads are mostly
due to the copying costs for Beanstalkd’s large buffers. This
results in Beanstalkd performance being non-trivially impacted
by our syscall interposition strategy, but scaling well with
the number of variants and with more conservative security
policies. We observed similar behavior for bind, which only
issues 2 very “copy-heavy” syscalls per request (i.e., recvmsg
and sendmsg).
438
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:38:26 UTC from IEEE Xplore.  Restrictions apply. 
1 2 3 4
l
l
a
c
r
e
p
s
e
l
c
y
c
.
g
v
A
8,000
6,000
4,000
2,000
3
1
5
,
1
6
3
9
3
8
7
7
0
0
,
2
1
1
8
,
1
6
8
4
,
1
1
7
1
0
0
4
1
getpid
close(-1)
Native Dune MV1 MV2
7
4
6
,
5
4
2
7
,
5
1
4
8
,
4
9
9
8
,
2
writeﬁle
6
4
9
,
1
1
7
7
,
1
0
2
5
,
1
3
1
2
,
2
3
0
1
,
2
9
8
6
,
1
0
1
time
3
3
2
write/dev/null
)
d
e
z
i
l
a
m
r
o
n
(
d
a
e
h
r
e
v
o
e
m
1.8
1.6
1.4
1.2
i
t
-
n
u
R
1
401.bzip2
400.perlbench
403.gcc
geomean
471.omnetpp
458.sjeng
462.libquantum
445.gobmk
429.mcf
464.h264ref
456.hm mer
473.astar
483.xalancbmk
Fig. 8. Overhead for the SPEC CINT2006 benchmarks for an increasing
number of variants.
Fig. 9. Average number cycles for various syscalls. Dune (using the sandbox
app) always runs in passthrough mode. MvArmor runs with 1 (MV1) or 2
(MV2) parallel variants using the Code execution security policy.
Overall, our results suggest
that MvArmor scales well
with the number of variants, for example with only a ∼3%
average overhead increase (geomean) when moving from a
2-variant to a 3-variant conﬁguration (across different security
policies). We observe signiﬁcant performance drops only when
exhausting the number of cores—a 4-variant conﬁguration in
our 4-core setup, with a core dedicated to the benchmark
program. We also note that a more large-scale scalability
analysis, while possibly interesting, is irrelevant in practice.
MvArmor’s default conﬁguration using 2 variants is sufﬁcient
to provide strong security guarantees by construction and also
minimizes core utilization to encourage deployment in real-
world settings.
We now compare our results against Varan [22]. For a
fair comparison, we focus on the Code execution security
policy, which most closely matches Varan’s MVX strategy.
MvArmor’s performance is very similar to Varan for nginx
(41% vs. 37% for 2 variants, respectively), but we did observe
somewhat different results for beanstalkd and lighttpd. In
particular, for beanstalkd, our results show relatively low
and stable overheads for MvArmor (∼41%) and increasingly
higher overheads for Varan (52% and 57%, for 2 and 3
variants, respectively). Conversely, for lighttpd, Varan reports
low and nearly constant overheads (∼14%), while MvArmor’s
overheads start at 70% for 2 variants.
Overall, MvArmor’s overhead results are generally com-
parable to Varan, although our experiments show that
the
actual performance of the 2 systems depends on the program
considered. We believe our results are very encouraging, given
that (i) Varan is the fastest existing MVX implementation, (ii)
compared to Varan, MvArmor provides much stronger security
guarantees even for our least conservative (Code execution)
security policy, (iii) our loopback-based performance results
are pessimistic and could also be improved by operating further
libOS-style optimizations enabled by our design.
B. SPEC Performance
To further compare our results against prior solutions, we
evaluated the performance impact induced by MvArmor on the
SPEC CINT2006 benchmarks. While the SPEC benchmarks
are CPU-intensive and issue a relatively low number of syscalls
(thereby providing optimistic performance results for MVX
systems), this experiment still provides useful comparative data
points. Figure 8 presents our ﬁndings.
As shown in the ﬁgure, MvArmor yields an average over-
head of 9.1% (geomean) for 2 variants and 20.4% for 4 variants
across all the benchmarks. A closer inspection revealed that, in
most cases, the reported overheads primarily originated by the
impact on the memory bandwidth of the system. Benchmarks
such as mcf and libquantum are particularly memory-
intensive and tend not to scale well in simultaneous runs on
multi-core architectures [57].
Nevertheless, despite the greater impact of TLB misses in
memory-intensive benchmarks induced by the use of EPTs in
Dune [28], our results are encouraging and, in fact, even yield
better performance than Varan, the best MVX performer on
SPEC in the literature, reporting an average overhead of 14.2%
(geomean) with 2 variants [22] on the same set of benchmarks.
C. Microbenchmark Performance
To carefully pinpoint the sources of overhead introduced
by MvArmor, we evaluated our solution using a number of
microbenchmarks. In particular, we measured the number of
cycles required by various syscalls from the perspective of a
user program while running under Dune [28] and under our
full MvArmor solution with 1 or 2 variants (MV1 and MV2,
respectively). Figure 9 presents our ﬁndings.
Both the getpid and close(-1) syscalls have a very
short duration, with the kernel almost immediately returning
to userland. For these simple syscalls, Dune alone adds around
1,300 cycles, accounting for syscall interposition and (mostly)