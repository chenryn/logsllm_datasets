mine the root cause of the bursts. Table 3 shows the determined causes of the
8
E. Blanton and M. Allman
Table 3. Percentage of bursts triggered by the given root cause
Dataset Bursts Initial Exit Loss Stretch Window App. Unknown
Window Recovery ACKs Opening Pattern
Anon 274,880
LBNL 187,176
165,023
ICSI1
ICSI2
228,063
1.8
0.9
6.4
4.2
0.2
0.3
0.7
5.1
26.3
22.9
23.5
22.4
5.0
3.1
4.8
4.5
17.0
32.8
24.0
23.3
49.6
40.0
40.6
45.1
bursts found in each dataset. First, the second column of the table shows that
each dataset contains a wealth of bursts. Next, the third column of the table
shows that 1–6% of the bursts are observed in the initial window of data trans-
mission. The fourth column shows a similarly small amount of bursting caused
by the sender being limited by the advertised window during loss recovery and
then transmitting a burst upon leaving loss recovery (when a large amount of
the advertised window is freed by an incoming ACK). These ﬁrst two causes of
loss account for a small fraction of the bursts, but the fraction does vary across
datasets. We have been unable to derive a cause for this diﬀerence and so ascribe
it to the heterogeneous nature of the hosts, operating systems, routers, etc. at
the various locations in the network.
The ﬁfth column in table 3 shows that roughly 20–25% of the bursts are
caused by stretch ACKs (acknowledgments that newly ACK more than 2 times
the number of bytes in the largest segment seen in the connection). Stretch ACKs
arrive for a number of reasons. For instance, some operating systems generate
stretch ACKs [Pax97] in the name of economy of processing and bandwidth. In
addition, since TCP’s ACKs are cumulative in nature simple ACK loss can cause
stretch ACKs to arrive. Finally, ACK reordering can cause stretch ACKs due
to an ACK generated later passing an earlier ACK. The origin of each stretch
ACK is therefore ambiguous given our limited vantage point, and hence we did
not try to untangle the possible root causes.
The sixth column represents a somewhat surprising bursting cause that we
did not expect. From the server’s vantage point we observe ACKs arriving from
the web client that acknowledge the data transmitted as expected but that do
not free space in the advertised window — and, hence, do not trigger further
data transmission when the sender is constrained by the advertised window.
When an ACK that opens advertised window space ﬁnally does arrive a burst
of data is transmitted. This phenomenon happens in modest amounts (3–5% of
bursts) in all the datasets we examined.
The seventh column in the table shows the percentage of bursts caused by
the application’s sending pattern. We expected this cause of bursts to be fairly
low since our mental model of web transfers is that objects are pushed onto the
network as fast as possible. However, 17–33% of the bursts happened after all
transmitted data was acknowledged and no other bursting scenario explained
the burst, indicating that the data was triggered by the application rather than
being clocked out by the incoming ACKs. This could be explained by a persistent
HTTP connection that did not use pipelining [FGM+97] — or, which was kept
On the Impact of Bursting on TCP Performance
9
open while the user was viewing a given web page and then re-used to fetch
another item from the same server.
Finally, the last column of the table is the most troubling in that it indicates
that we could not accurately determine the cause of 40–50% of the bursts across
all the datasets. Part of the future work in this area will be to develop additional
techniques to determine why this bursting is happening. However, the problem
is daunting in that we examined a large number of time-sequence plots for con-
nections containing the unknown burst causes and at times we could not ﬁgure
out why the burst happened ourselves — let alone design a heuristic to detect it!
4
Implications of Bursts
In this section we explore the implications of the bursting uncovered in the
last section on the TCP connections themselves. It is beyond our scope (and
data) to evaluate the implications the bursting has on competing traﬃc and the
network itself. Figure 3 shows the probability of losing at least one segment in
s
s
o
l
f
o
y
t
i
l
i
b
a
b
o
r
P
 1
 0.8
 0.6
 0.4
 0.2
 0
 1
s
s
o
l
f
o
y
t
i
l
i
b
a
b
o
r
P
 1
 0.8
 0.6
 0.4
 0.2
 0
 1
 1
 0.8
 0.6
 0.4
 0.2
s
s
o
l
f
o
y
t
i
l
i
b
a
b
o
r
P
 10
 100
Burst length (segments)
(a) Anon
 1000
 0
 1
 10
 100
Burst length (segments)
 1000
(b) LBNL
 1
 0.8
 0.6
 0.4
 0.2
s
s
o
l
f
o
y
t
i
l
i
b
a
b
o
r
P
 10
 100
Burst length (segments)
(c) ICSI1
 1000
 0
 1
 10
 100
Burst length (segments)
 1000
(d) ICSI2
Fig. 3. Probability of losing at least one segment in a burst as a function of burst size
(in segments)
10
E. Blanton and M. Allman
Table 4. Retransmission rates observed inside and outside bursts
Dataset Conns. Bursts Burst Loss Rate (%) Non-Burst Loss Rate (%)
Anon
LBNL
ICSI1
ICSI2
4,233
5,685
4,805
8,201
69,299
45,282
39,832
72,069
70.9
23.0
16.1
26.6
41.2
22.9
14.5
20.5
a burst as a function of the burst size (in segments) for each of our datasets.
The ﬁgure shows that for modest burst sizes (tens of segments or less) that the
probability of losing a segment from the burst is fairly low (roughly less than
5%). As the burst size increases, the likelihood of experiencing a drop within a
burst also increases. Bursts on the order of hundreds of segments in our datasets
are clearly doomed to overwhelm intervening routers and experience congestion.
One interesting note is in the shape of the plots. The Anon dataset shows a fairly
smooth ramp-up in the probability of loss in a burst as the burst size increases.
However, in both the LBNL and ICSI datasets there is a clear point at which
the chances of losing at least one segment in a burst jumps from less than 5%
to over 20% and often to 100%. In the LBNL dataset this happens when burst
size reaches approximately 60 segments and in the ICSI dataset when the burst
size reaches roughly 50 segments. These results may indicate a maximum queue
size at or near LBNL and ICSI that ultimately limits the burst size that can be
absorbed. The Anon network may be more congested than the ICSI or LBNL
networks and therefore the chances of a non-empty queue vary with time and
hence the ability to absorb bursts likewise varies over time. Alternatively, the
Anon results could indicate the presence of active queue management, whose
ability to absorb bursts depends on the traﬃc dynamics at any given point
in time.
The analysis above assesses the question of whether bursts cause some loss.
Next we focus our attention on the amount of loss caused by bursts. In other
words we address the question: is the loss rate higher when bursting than when
not bursting? Given the information at hand we cannot determine precise loss
rates and therefore use the retransmission rate as an indicator (understanding
that the retransmission rate and the loss rate can be quite diﬀerent depend-
ing on loss patterns, TCP variant used, etc. [AEO03]). We ﬁrst winnow each
dataset to connections that experience both bursting and retransmissions to al-
low for a valid comparison. This has the eﬀect of making the reported rates
appear to be much higher than loss rates measured in previous network studies
(e.g., [AEO03]) because we are disregarding all connections that experienced no
loss. Table 4 shows the results of our analysis. When comparing this table with
tables 2 and 3 it is apparent that only a small minority of the connections from
each dataset contain both bursting and retransmissions. The table shows the
aggregate retransmission rate to be higher when connections are bursting than
when connections are not bursting. The change in the retransmission rate ranges
from nearly non-existent in the LBNL dataset to a roughly 75% increase in the
Anon dataset. The large increase in the Anon network agrees with the results
On the Impact of Bursting on TCP Performance
11
presented above that the network is generally more congested and the bottleneck
queue closer to the drop point than the other networks we studied. Therefore,
bursts cause a large increase in the loss rates experienced in this network while
the other networks were better able to absorb the bursts.
5 Conclusions and Future Work
The work in this paper is focused on the impact of bursting on TCP connections
themselves. From the above preliminary data analysis we note that micro-bursts
are not frequent in TCP connections — with over 75% of the connections in
the three networks studied showing no bursting. When bursting does occur,
burst sizes are predominantly modest with over 90% of the bursts are less than
15 segments across the datasets we studied. Furthermore, in these modest bursts
the probability of experiencing loss within the burst is small (generally less than
5% across datasets). However, bursts of hundreds of segments do occur and such
large bursts nearly always experience some loss. We analyzed the cause of bursts
and found the two predominant known causes of bursting to be the reception
of stretch ACKs and application sending patterns. Unfortunately, our analysis
techniques also failed to ﬁnd the cause of 40–50% of the bursts we observed. An
area for future work will be to further reﬁne the analysis to gain further insights
into these unclassiﬁed bursts (however, as described in § 3 this is a challenging
task). Finally, we ﬁnd an increase in the loss rate experienced within bursts with
the loss rate experienced outside of bursts. The increase ranged from slight to
approximately 75% depending on the network in question.
A key piece of future work is in understanding how the results given in [JD03]
relate to those given in this paper. That is, the preliminary results of this paper
indicate that micro-bursting is not likely to hurt performance, while [JD03] shows
that the network impact of bursting is non-trivial. Before applying a mitigation
to TCP to smooth or reduce bursts it would be useful to correlate the network
issues found in [JD03] with speciﬁc bursting situations. For instance, if only
particular kinds of bursting are yielding the scaling behavior noted in [JD03]
then mitigating only those bursting situations may be a desirable path forward.
Acknowledgments
Andrew Moore and Vern Paxson provided the Anon and LBNL datasets, re-
spectively. Sally Floyd, Vern Paxson and Scott Shenker provided discussions on
this study. The anonymous reviewers provided good feedback on the submis-
sion of this paper and their comments improved the ﬁnal product. This work
was partially funded by the National Science Foundation under grant number
ANI-0205519. Our thanks to all!
References
[AB04]
Mark Allman and Ethan Blanton. Notes on Burst Mitigation for Trans-
port Protocols. December 2004. Under submission.
12
E. Blanton and M. Allman
Mark Allman, Wesley Eddy, and Shawn Ostermann. Estimating Loss
Rates with TCP. ACM Performance Evaluation Review, 31(3), December
2003.
Mark Allman, Sally Floyd, and Craig Partridge. Increasing TCP’s Initial
Window, October 2002. RFC 3390.
Mark Allman, Vern Paxson, and W. Richard Stevens. TCP Congestion
Control, April 1999. RFC 2581.
Ethan Blanton and Mark Allman. On Making TCP More Robust to
Packet Reordering. ACM Computer Communication Review, 32(1):20–
30, January 2002.
Robert Braden. Requirements for Internet Hosts – Communication Lay-
ers, October 1989. RFC 1122.
Kevin Fall and Sally Floyd. Simulation-based Comparisons of Tahoe,
Reno, and SACK TCP. Computer Communications Review, 26(3), July
1996.
[AEO03]
[AFP02]
[APS99]
[BA02]
[Bra89]
[FF96]
[FK04]
[JD03]
[KHF04]
[HTH01]
[Hay97]
[Hoe96]
[FGM+97] R. Fielding, Jim Gettys, Jeﬀrey C. Mogul, H. Frystyk, and Tim Berners-
Lee. Hypertext Transfer Protocol – HTTP/1.1, January 1997. RFC
2068.
Sally Floyd and Eddie Kohler. Proﬁle for DCCP Congestion Control ID
2: TCP-like Congestion Control, November 2004. Internet-Draft draft-
ietf-dccp-ccid2-08.txt (work in progress).
Chris Hayes. Analyzing the Performance of New TCP Extensions Over
Satellite Links. Master’s thesis, Ohio University, August 1997.
Janey Hoe. Improving the Start-up Behavior of a Congestion Control
Scheme for TCP. In ACM SIGCOMM, August 1996.
Amy Hughes, Joe Touch, and John Heidemann. Issues in TCP Slow-Start
Restart After Idle, December 2001. Internet-Draft draft-hughes-restart-
00.txt (work in progress).
Hao Jiang and Constantinos Dovrolis. Source-Level IP Packet Bursts:
Causes and Eﬀects. In ACM SIGCOMM/Usenix Internet Measurement
Conference, October 2003.
Eddie Kohler, Mark Handley, and Sally Floyd. Datagram Control Proto-
col (DCCP), November 2004. Internet-Draft draft-ietf-dccp-spec-09.txt
(work in progress).
[MHK+03] Andrew Moore, James Hall, Christian Kreibich, Euan Harris, and Ian
Pratt. Architecture of a Network Monitor. In Passive & Active Measure-
ment Workshop 2003 (PAM2003), April 2003.
Matt Mathis and Jamshid Mahdavi. Forward Acknowledgment: Reﬁning
TCP Congestion Control. In ACM SIGCOMM, August 1996.
Jeﬀrey C. Mogul. Observing TCP Dynamics in Real Networks. In ACM
SIGCOMM, pages 305–317, 1992.
Vern Paxson. Automated Packet Trace Analysis of TCP Implementa-
tions. In ACM SIGCOMM, September 1997.
Randall Stewart, Qiaobing Xie, Ken Morneault, Chip Sharp, Hanns Juer-
gen Schwarzbauer, Tom Taylor, Ian Rytina, Malleswar Kalla, Lixia
Zhang, and Vern Paxson. Stream Control Transmission Protocol, Oc-
tober 2000. RFC 2960.
[SXM+00]
[MM96]
[Mog92]
[Pax97]
[ZKFP03] Ming Zhang, Brad Karp, Sally Floyd, and Larry Peterson. RR-
In Proceedings of
TCP: A Reordering-Robust TCP with DSACK.
the Eleventh IEEE International Conference on Networking Protocols
(ICNP), November 2003.