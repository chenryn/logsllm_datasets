only packet) in the next RTT and the router assigns it with
a fair share as normal. This implies that a new non-deadline
ﬂow does not make progress for an extra RTT at startup.
However, such ﬂows are typically long. Further, RTTs are
minimal, and this approach trades-oﬀ a minor overhead in
bandwidth and latency (one RTT ∼ 300µs) for a lot of burst
tolerance. Our evaluation shows that this vastly improves
D3’s ability to cope with ﬂow bursts over the state of the
art. Additionally, this does not impact deadline ﬂows much
because the router still tries to honor their desired rate.
5.
IMPLEMENTATION
We have created an endhost-based stack and a proof-of-
concept router that support the D3 protocol. This paper
focuses on the congestion control aspects of D3 but our im-
plementation provides a complete transport protocol that
provides reliable, in-order delivery of packets. As with TCP,
reliability is achieved through sequence numbers for data
packets, acknowledgements from receivers, timer-based re-
transmissions and ﬂow control.
On endhosts, D3 is exposed to applications through an ex-
tended Sockets-like API. The extensions allow applications
to specify the ﬂow length and deadline when a socket is
created. The core logic for D3, including the rate control
scheme, runs in user space. We have a kernel driver that is
bound to the Ethernet interface exposed by the NIC driver.
The kernel driver eﬃciently marshals packets between NIC
and the user-level stack.
The router is implemented on a server-grade PC and im-
plements a shared buﬀer, store and forward architecture. To
be consistent with shallow buﬀers in today’s datacenters,
the router has a buﬀer of 128KB per NIC. The router uses
the same kernel driver as the endhosts, except the driver is
bound to multiple Ethernet interfaces. All incoming packets
pass to a user space process, which processes and forwards
them on the appropriate interface. The design of the ker-
nel driver and user-space application support zero-copy of
packets.
Router overhead. To keep per-packet overhead low in
the router, we use integer arithmetic for all rate calcula-
tions. Although each packet traverses the user-kernel space
boundary, we are able to sustain four links at full duplex line
rate. Speciﬁcally, the average packet processing time was less
than 1µs (0.208µs), and was indistinguishable from normal
packet forwarding in user-space. Thus, D3 imposes minimal
overhead on the forwarding path and the performance of our
prototype leads us to believe that it is feasible to implement
D3 in a commodity router.
Packet header. The D3 request and rate feedback packet
header is shown in Figure 6. The congestion header includes
the desired rate rt+1, an index into the allocation vector
and the current allocation vector ([at+1]). The header also
includes the allocations for the previous RTT so that the
Bit Offset
0
8
16
24
Current
fields
Previous
fields
Feedback
fields
Desired rate
Index
Allocation Vector
... Allocation Vector
Scale
factor
Previous
Desired rate
Previous Allocation
            Vector
... Previous Allocation Vector
Feedback Allocation Vector
Feedback Allocation Vector
Figure 6: Congestion header for rate request and
feedback packets.
Sender
Receiver
(1)
(3)
SYN/RRQ (t)
SYN/ACK/RRQ (t)
ACK/RRQ/data  (t+1)
data  (t+1)
ACK/RRQ  (t+1)
FIN/RRQ/data  (t+n)
(5)
FIN/RRQ  (t+n)
(2)
(4)
(6)
Figure 7: Packet exchange with D3. RRQ is Rate
Request. Text in parenthesis is the current RTT
interval.
routers can update their relevant counters - the desired rate
rt and the vector of rates allocated by the routers ([at]).
Finally, the header carries rate feedback to the destination
- a vector of rates allocated by the routers for reverse traﬃc
from the destination to the source.
All rates are in Bytes/µs and hence, can be encoded in
one byte; 1Gbps equates to a value of 125. The scale factor
byte can be used to scale this and would allow encoding
of much higher rates. The allocation vectors are 6 bytes
long, allowing for a maximum network diameter of 6 routers
(or switches). We note that current datacenters have three-
tiered, tree-like topologies [13] with a maximum diameter of
5 switches. The allocation vectors could be made variable
length ﬁelds to achieve extensibility. Overall, our current
implementation imposes an overhead of 22 bytes for every
packet carrying rate requests.
Protocol description. The protocol operation is illus-
trated in Figure 7.
(1). The sender initiates the ﬂow by
sending a SYN packet with a rate request. Routers along
the path allocate rate and add it to the current allocation
vector in the congestion header. (2). Receivers respond with
a SYN/ACK and a rate request of their own. The congestion
header of the response includes the allocated rate vector for
the sender. (3). The sender uses this vector to determine its
sending rate and sends data packets. One of these includes a
rate request for the next RTT. (5). Once the sender is done,
it sends a FIN packet and returns its existing allocation.
Calculating desired rate. The sender uses information
about ﬂow deadline and remaining ﬂow length to determine
the desired rate that would allow the ﬂow to meet its dead-
line. At interval t, the desired rate for the next RTT is given
by
rt + 1 =
remaining f low length − st ∗ rtt
deadline − 2 ∗ rtt
56where st is the current sending rate, and rtt is the sender’s
current estimate of the RTT for the ﬂow, which is based on
an exponential moving average of the instantaneous RTT
values. The numerator accounts for the fact that by the
next RTT, the sender would have sent st ∗ rtt bytes worth
of more data. The denominator is the remaining time to
achieve the deadline: one rtt is subtracted since the rate will
be received in the next RTT, while the second rtt accounts
for the FIN exchange to terminate the ﬂow.
6. EVALUATION
We deployed D3 across a small testbed structured like the
multi-tier tree topologies used in today’s datacenters. The
testbed (Figure 8) includes twelve endhosts arranged across
four racks. Each rack has a top-of-rack (ToR) switch, and
the ToR switches are connected through a root switch. All
endhosts and switches are Dell Precision T3500 servers with
a quad core Intel Xeon 2.27GHz processor, 4GB RAM and
1 Gbps interfaces, running Windows Server 2008 R2. The
root switch is further connected to two other servers that are
used as traﬃc generators to model traﬃc from other parts
of the datacenter. For two endhosts in the same rack com-
municating through the ToR switch, the propagation RTT,
measured when there is no queuing, is roughly 500µs. The
endhosts are also connected to a dedicated 48-port 1Gbps
NetGear GS748Tv3 switch (not shown in the ﬁgure). We
use this for TCP experiments.
Our evaluation has two primary goals: (i). To determine
the value of using ﬂow deadline information to apportion
network bandwidth. (ii). To evaluate the performance of D3
just as congestion control protocol, without deadline infor-
mation. This includes its queuing and utilization behavior,
and performance in a multi-bottleneck, multi-hop setting.
6.1 Exploiting deadlines through D3
To evaluate the beneﬁt of exploiting deadline information,
we compare D3 against TCP. However, TCP is well known
to not be amenable to datacenter traﬃc patterns. To cap-
ture the true value of deadline awareness, we also operate
D3 in fair share mode only, i.e., without any deadline in-
formation and all ﬂows treated as non-deadline ﬂows. We
term this RCPdc since it is eﬀectively RCP optimized for
the datacenter.5 With RCPdc, the fair share is explicitly
communicated to the hosts (i.e., no probe-based exploration
is required) and it has been shown to be optimal in terms
minimizing ﬂow completion times [11]. Hence, it represents
the limit for any fair share protocol, such as DCTCP [4]
and other recent proposals for datacenters like QCN [3] and
E-TCP [14]. We further contrast D3 against deadline-based
priority queuing of TCP ﬂows. Priority queuing was imple-
mented by replacing the Netgear switch with a Cisco router
that oﬀers port-based priority capabilities. Flows with short
deadlines are mapped to high priority ports. Our evaluation
covers the following scenarios:
• Flow burst microbenchmarks. This scenario reﬂects the
case where a number of workers start ﬂows at the same
5While the core functionality of RCPdc mimics RCP, we
have introduced several optimizations to exploit the trusted
nature of datacenters. The most important of these include:
exact estimates of the number of ﬂows at the router (RCP
uses algorithms to approximate this), the introduction of the
base rate, the pause for one RTT to alleviate bursts, etc.
Figure 8: Testbed topology: red servers are end-
hosts, blue are switches, grey are traﬃc generators.
time towards the same destination. It provides a lower-
bound on the expected performance gain as all ﬂows com-
pete at the same bottleneck at the same time. Our re-
sults show that D3 can support almost twice the number
of workers, without compromising deadlines, compared
to RCPdc, TCP and TCP with priority queuing (hence-
forth referred to as TCPpr).
• Benchmark traﬃc. This scenario represents typical data-
center traﬃc patterns (e.g., ﬂow arrivals, ﬂow sizes) and
is indicative of the expected D3 performance with cur-
rent datacenter settings. The evaluation highlights that
D3 oﬀers an order of magnitude improvement over out of
the box TCP and a factor of two improvement over an
optimized TCP version and RCPdc.
• Flow quenching. We evaluate the value of terminating
ﬂows that do not contribute to application throughput.
Our results show that ﬂow quenching ensures the D3 per-
formance degrades gracefully under extreme load.
6.1.1 Flow burst microbenchmarks
In this scenario, a host in each rack serves as an aggregator
(see Figure 3) while other hosts in the rack represent work-
ers responding to an aggregator query. This is a common
application scenario for online services. All workers respond
at the same time and all response ﬂows are bottlenecked
at the link from the rack’s ToR switch to the aggregator.
Since there are only three hosts in each rack, we use mul-
tiple workers per host. We also repeated the experiment
on a restructured testbed with more hosts per rack and the
results remained qualitatively the same.
The response ﬂow lengths are uniformly distributed across
[2KB, 50KB] and the ﬂow deadlines are distributed expo-
nentially around 20ms (tight), 30ms (moderate) and 40ms
(lax). As described earlier, the primary performance metric
is application throughput, that is, the number of ﬂows ﬁn-
ishing before their deadline. This metric was intentionally
chosen as datacenter operators are primarily interested in
the “operating regime” where the network can satisfy almost
all ﬂow deadlines. Hence, we vary the number of workers
sending ﬂows and across 200 runs of this experiment, deter-
mine the maximum number of concurrent senders a given
congestion control scheme supports while ensuring at least
99% application throughput. This is shown in Figure 9.
As compared to RCPdc, D3 can support almost twice as
many concurrent senders while satisfying ﬂow deadlines (3-
4 times as compared to TCP and TCPpr). This is because
D3 uses ﬂow deadline information to guide bandwidth ap-
57D3
D3
24
24
 RCPdc
 RCPdc
TCPpr
TCPpr
TCP
TCP
28
28
16
16
16
16
32
32
18
18
8
8
6
6
8
8
8
8
8
8
6
6
s
s
r
r
e
e
d
d
n
n
e
e
S
S
f
f
o
o
r
r
e
e
b
b
m
m
u
u
N
N
 35
 35
 30
 30
 25
 25
 20
 20
 15
 15
 10
 10
 5
 5
 0
 0
Tight
Tight
Moderate
Moderate
Deadlines
Deadlines
Lax
Lax
Figure 9: Number of concurrent senders that can
be supported while ensuring more than 99% appli-
cation throughput.
)
%
(
t
u
p
h
g
u
o
r
h
T
p
p
A
 100
 90
 80
 70
 60
D3
 RCPdc
TCPpr
TCP
 0
 5
 10
 15
 20
 25
 30
 35
 40
Number of Senders
Figure 10: Application throughput for varying con-
current senders and moderate deadlines (the Y-axis
starts at 60%).
portioning. This is further pronounced for relaxed deadlines
where D3 has more ﬂexibility and hence, the increase in the
number of senders supported compared to tight deadlines is
greater than for other approaches. Note that since conges-
tion occurs at the aggregator’s downlink, richer topologies
like VL2 [13] or FatTree [2] cannot solve the problem.
For completeness, Figure 10 shows the application through-
put with moderate deadlines as we vary the number of con-
current senders to 40. Results look similar with tight and
lax deadlines [26]. While the performance of these proto-
cols beyond the regime of high application throughput may
not be of primary operator importance, the ﬁgure does help
us understand application throughput trends under severe
(unplanned) load spikes. Apart from being able to support
more senders while ensuring no deadlines are missed, when
the number of senders does become too high for D3 to sat-
isfy all deadlines, it still improves application throughput
by roughly 20% over TCP, and 10% or more, over RCPdc
and TCPpr. Hence, even if we relax our “operating-regime”
metric to be less demanding, for example, to 95% of applica-
tion throughput, D3 can support 36 senders with moderate
deadlines compared to the 22, 10 and 8 senders of RCPdc,
TCPpr and TCP respectively.
The ﬁgure also illustrates that RCPdc outperforms TCP
at high loads. This is because probe-based protocols like
TCP attempt to discover their fair share by causing queues
to build up and eventually, losses, ergo increasing latency
for a number of ﬂows. Instead, RCPdc avoids queueing by
equally dividing the available capacity. Figure 11 highlights
this point by displaying the scatter plots of ﬂow completion
times versus ﬂow deadlines for TCP, RCPdc and D3 for one
of the experiments (moderate deadlines, 14-30 senders). For
TCP, it is evident that packet losses result in TCP time-
outs and very long completion times for some ﬂows. Since
the hardware switch has a buﬀer of 100KB, a mere eight
simultaneous senders with a send-window of eight full sized
packets can lead to a loss; this is why TCP performance
starts degrading around eight senders in Figure 10.6
6The mean ﬂow size in our experiments is 26KB. Hence, a
 100
 100
 10
 10
)
)