subsequent solving phase. Our solving approaches re-
quire that the addressing functions are linear, i.e., they
are XORs of physical-address bits.
In fact, Intel used such functions in earlier microar-
chitectures. For instance, Seaborn [23] reports that on
his Sandy Bridge setup the bank address is computed by
XORing the bits a14..a16 with the lower bits of the row
number (a18..a20) (cf. Figure 4a). This is done in order to
minimize the number of row conflicts during runtime. In-
tel also uses linear functions for CPU-cache addressing.
Maurice et al. [15] showed that the complex addressing
function, which is used to select cache slices, is an XOR
of many physical-address bits.
As it turns out, linearity holds on all our tested config-
urations. However, there are setups in which it might be
568  25th USENIX Security Symposium 
USENIX Association
Cache hit
Cache miss, row hit
Cache miss, row conflict
107
105
103
101
s
e
s
a
c
f
o
r
e
b
m
u
N
72
84
96 108 120 132 144 156 168 180 192 204 216 228 240 252 264 276 288
Access time [CPU cycles]
Figure 1: Histogram for cache hits and cache misses divided into row hits and row conflicts on the Ivy Bridge i5 test
system. Measurements were performed after a short idle period to simulate non-overlapping accesses by victim and
spy. From 180 to 216 cycles row hits occur, but no row conflicts.
violated, such as triple-channel configurations. We did
not test such systems and leave a reverse engineering to
future work.
4.2 Reverse engineering using physical
probing
Our first approach to reverse engineer the DRAM map-
ping is to physically probe the memory bus and to di-
rectly read the control signals. As shown in Figure 2,
we use a standard passive probe to establish contact with
the pin at the DIMM slot. We then repeatedly accessed
a selected physical address2 and used a high-bandwidth
oscilloscope to measure the voltage and subsequently de-
duce the logic value of the contacted pin. Note that due
to the repeated access to a single address, neither a timely
location of specific memory requests nor distinguishing
accesses to the chosen address from other random ones
is required.
We repeated this experiment for many selected ad-
dresses and for all pins of interest, namely the bank-
address bits (BA0, BA1, BA2 for DDR3 and BG0, BG1,
BA0, BA1 for DDR4) for one DIMM and the chip select
CS for half the DIMMs.
For the solving phase we use the following approach.
Starting from the top-layer (channel or CPU addressing)
and drilling down, for each DRAM addressing function
we create an over-defined system of linear equations in
the physical address bits. The left-hand-side of this sys-
tem is made up of the relevant tested physical addresses.
For instance, for determining the bank functions we only
use addresses that map to the contacted DIMMs chan-
nel. The right-hand-side of the system of equations are
the previously measured logic values for the respective
2Resolving virtual to physical addresses requires root privileges in
Linux. Given that we need physical access to the internals of the sys-
tem, this is a very mild prerequisite.
Figure 2: Physical probing of the DIMM slot.
address and the searched-for function. The logic values
for CPU and channel addressing are computed by simply
ORing all respective values for the chip-select pins. We
then solve this system using linear algebra. The solution
is the corresponding DRAM addressing function.
Obviously,
this reverse-engineering approach has
some drawbacks. First, expensive measurement equip-
ment is needed. Second, it requires physical access to
the internals of the tested machine. However, it has the
big advantage that the address mapping can be recon-
structed for each control signal individually and exactly.
Thus, we can determine the exact individual functions
for the bus pins. Furthermore, every platform only needs
to be measured only once in order to learn the addressing
functions. Thus, an attacker does not need physical ac-
cess to the concrete attacked system if the measurements
are performed on a similar machine.
USENIX Association  
25th USENIX Security Symposium  569
5
s
e
s
a
c
f
o
n
o
i
t
r
o
p
o
r
P
0.2
0.1
0
160
180
200
220
240
260
280
Access time [CPU cycles]
Figure 3: Histogram of average memory access times
for random address pairs on our Haswell test system. A
clear gap separates the majority of address pairs causing
no row conflict (lower access times), because they map
to different banks, from the few address pairs causing a
row conflict (higher access times), because they map to
different rows in the same bank.
4.3 Fully automated reverse engineering
For our second approach to reverse engineer the DRAM
mapping we exploit the fact that row conflicts lead to
higher memory access times. We use the resulting timing
differences to find sets of addresses that map to the same
bank but to a different row. Subsequently, we determine
the addressing functions based on these sets. The entire
process is fully automated and runs in unprivileged and
possibly restricted environments.
Timing analysis. In the first step, we aim to find same-
bank addresses in a large array mapped into the attackers’
address space. For this purpose, we perform repeated al-
ternating access to two addresses and measure the aver-
age access time. We use clflush to ensure that each ac-
cess is served from DRAM and not from the CPU cache.
As shown in Figure 3, for some address pairs the access
time is significantly higher than for most others. These
pairs belong to the same bank but to different rows. The
alternating access causes frequent row conflicts and con-
sequently the high latency.
The tested pairs are drawn from an address pool,
which is built by selecting random addresses from a large
array. A small subset of addresses in this pool is tested
against all others in the pool. The addresses are sub-
sequently grouped into sets having the same channel,
DIMM, rank, and bank. We try to identify as many such
sets as possible in order to reconstruct the addressing
functions.
Function reconstruction. In the second phase, we use
the identified address sets to reconstruct the addressing
functions. This reconstruction requires (at least partial)
resolution of the tested virtual addresses to physical ones.
Similar as later in Section 5.1, one can use either the
availability of 2 MB pages, 1 GB pages, or privileged in-
formation such as the virtual-to-physical address transla-
tion that can be obtained through /proc/pid/pagemap
in Linux systems.
In the case of 2 MB pages we can recover all partial
functions up to bit a20, as the lowest 21 bit of virtual
and physical address are identical. On many systems the
DRAM addressing functions do not use bits above a20
or only few of them, providing sufficient information to
mount covert and side-channel attacks later on. In the
case of 1 GB pages we can recover all partial functions
up to bit a30. This is sufficient to recover the full DRAM
addressing functions on all our test systems. If we have
full access to physical address information we will still
ignore bits a30 and upwards. These bits are typically only
used for DRAM row addressing and they are very un-
likely to play any role in bank addressing. Additionally,
we ignore bits (a0..a5) as they are used for addressing
within a cache line.
The search space is then small enough to perform a
brute-force search of linear functions within seconds.
For this, we generate all linear functions that use exactly
n bits as coefficients and then apply them to all addresses
in one randomly selected set. We start with n = 1 and
increment n subsequently to find all functions. Only if
the function has the same result for all addresses in a set,
we test this potential function on all other sets. How-
ever, in this case we only pick one address per set and
test whether the function is constant over all sets. If so,
the function is discarded. We obtain a list of possible ad-
dressing functions that also contains linear combinations
of the actual DRAM addressing functions. We prioritize
functions with a lower number of coefficients, i.e., we
remove higher-order functions which are linear combi-
nations of lower-order ones. Depending on the random
address selection, we now have a complete set of correct
addressing functions. We verify the correctness either by
comparing it to the results from the physical probing, or
by performing a software-based test, i.e., verifying the
timing differences on a larger set of addresses, or veri-
fying that usage of the addressing functions in Rowham-
mer tests increases the number of bit flips per second by
a factor that is the number of sets we found.
Compared to the probing approach,
this purely
software-based method has significant advantages.
It
does not require any additional measurement equipment
and can be executed on a remote system. We can identify
the functions even from within VMs or sandboxed pro-
cesses if 2 MB or 1 GB pages are available. Furthermore,
even with only 4 KB pages we can group addresses into
sets that can be directly used for covert or side channel at-
tacks. This software-based approach also allows reverse
engineering in settings where probing is not easily possi-
ble anymore, such as on mobile devices with hard-wired
ball-grid packages. Thus, it allowed us to reverse engi-
neer the mapping on current ARM processors.
570  25th USENIX Security Symposium 
USENIX Association
6
Table 1: Experimental setups.
CPU / SoC
Microarch.
Mem.
puted by XORing bits a14..a16 with the lower bits of the
row index (a18..a20).
i5-2540M
i5-3230M
i7-3630QM
i7-4790
i7-6700K
Samsung Exynos 5 Dual
Qualcomm Snapdragon 800
Qualcomm Snapdragon 820
Samsung Exynos 7420
2x Xeon E5-2630 v3
Qualcomm Snapdragon S4 Pro
Sandy Bridge
Ivy Bridge
Ivy Bridge
Haswell
Skylake
Haswell-EP
ARMv7
ARMv7
ARMv7
ARMv8-A
ARMv8-A
DDR3
DDR3
DDR3
DDR3
DDR4
DDR4
LPDDR2
LDDDR3
LPDDR3
LPDDR3
LPDDR4
One downside of the software-based approach is that
it cannot recover the exact labels (BG0, BA0, ...) of the
functions. Thus, we can only guess whether the recon-
structed function computes a bank address bit, rank bit,
or channel bit. Note that assigning the correct labels to
functions is not required for any of our attacks.
4.4 Results
We now present the reverse-engineered mappings for all
our experimental setups. We analyzed a variety of sys-
tems (Table 1), including a dual-CPU Xeon system, that
can often be found in cloud systems, and multiple current
smartphones. Where possible, we used both presented
reverse-engineering methods and cross-validated the re-
sults.
We found that the basic scheme is always as follows.
On PCs, the memory bus is 64 bits wide, yet the small-
est addressable unit is a byte. Thus, the three lower bits
(a0..a2) of the physical address are used as byte index
into a 64-bit (8-byte) memory word and they are never
transmitted on the memory bus. Then, the next bits are
used for column selection. One bit in between is used for
channel addressing. The following bits are responsible
for bank, rank, and DIMM addressing. The remaining
upper bits are used for row selection.
The detailed mapping, however, differs for each setup.
To give a quick overview of the main differences, we
show the mapping of one selected memory configuration
for multiple Intel microarchitectures and ARM-based
SoCs in Figure 4. Here we chose a configuration with
two equally sized DIMMs in dual-channel configuration,
as it is found in many off-the-shelf consumer PCs. All
our setups use dual-rank DIMMs and use 10 bits for
column addressing. Figure 4a shows the mapping on
the Sandy Bridge platform, as reported by Seaborn [23].
Here, only a6 is used to select the memory channel, a17 is
used for rank selection. The bank-address bits are com-
22
21
20
19
18
17
16
15
14
13
12
11 10
6789
...
(a) Sandy Bridge – DDR3 [23].
22
21
20
19
18
17
16
15
14
13
12
11 10
6789
...
(b) Ivy Bridge / Haswell – DDR3.
22
21
20
19
18
17
16
15
14
13
12
11 10
6789
...
(c) Skylake – DDR4.
BA0
BA1
BA2
Rank
...
Ch.
BA0
BA1
Rank
BA2
...
Ch.
BG0
BG1
Rank
BA0
BA1
...
Ch.
BG0
CPU
Rank
BG1
BA0
BA1
...
26
25
24
23
22
21
20
19
18
17
16
15
14
13
12
11 10
6789
...
Ch.
(d) Dual Haswell-EP (Interleaved Mode) – DDR4.
Rank
BA0
BA1
BA2
...
Ch.