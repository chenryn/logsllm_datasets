gion is grown to the maximal byte range, the ﬁngerprints of this
packet that mapped into the maximal range are re-associated with
the matched in-cache packet. Also, the maximal byte range in the
incoming packet is zeroed out. This ensures ensure that bytes in the
maximal match region are not used for encoding. Our implemen-
tation is thus conservative; we sacriﬁce some performance in favor
of correctness.
The decoder implementation largely follows the discussion in
§4.3. The last decoder on a path clears the ﬂag in the header indi-
cating that the packet has been fully decoded.
6.2 Conﬁguration Parameters
Parameters for the LP optimization: To specify parameters to
the LP formulation, we need to ﬁx a certain measurement epoch.
However, this epoch cannot be arbitrary, as the RE capabilities are
limited by the storage available at the ingresses. Thus, we deﬁne
the notion of a network data retention time determined by the size
of the ingress packet stores. All values in the formulation (i.e., the
match proﬁles and the trafﬁc matrix) are speciﬁed in terms of this
common value.
In real deployments, we expect ISPs to employ
ingress caches storing few tens of seconds worth of data.
93Trafﬁc and routing dynamics: The dominant source of trafﬁc dy-
namics are time-of-day and day-of-week effects [25]. Fortunately,
these are predictable and we can use historical trafﬁc matrices to
model these effects.
Routing changes are trickier because an ingress may incorrectly
assume that a downstream node will be able to decode a match.
Two scenarios arise. First, if routes are computed centrally [18],
SmartRE can use the new routes to recompute a new caching strat-
egy and disseminate it to the ingresses. However, the recompu-
tation may take few tens of seconds, and we need to ensure cor-
rectness during this transient state. Second, the ingresses do not
receive new caching strategies, but instead receive the current rout-
ing information (e.g., OSPF monitor [27]) and avoid encodings that
are non-decodable after the routing change. This ensures correct-
ness but sacriﬁces some performance. Note that this also solves the
transient problems in the ﬁrst scenario.
Changes in redundancy proﬁles: To estimate the redundancy pro-
ﬁles, the ingress RE devices maintain simple counters to track matches
between paths. The ingresses periodically report these values to the
central conﬁguration module. Note that this adds very little over-
head to the ingress implementation. However, since these could be
large,5 they will be reported infrequently (e.g., every 30 minutes).
This raises the issue of staleness of redundancy proﬁles. This
may have two effects: (1) It may affect the optimality of the con-
ﬁguration without affecting solution correctness. This is an ac-
ceptable operating mode for SmartRE and we evaluate it further
in §7. (2) Signiﬁcant changes in the redundancy proﬁle may in-
crease decoding load on each node (§4.2, Equation 2) and affect
solution feasibility. To handle the second issue, each ingress tracks
the actual number of matches per interior node and will not burden
overloaded interior nodes with additional decoding responsibilities.
Thus, changes in redundancy proﬁles do not affect correctness.
Additionally, SmartRE can use a triggered approach. For exam-
ple, under ﬂash-crowd-like scenarios where trafﬁc patterns change
dramatically, the affected ingresses can report the large changes
to the NOC. This can trigger an immediate recomputation of the
caching manifests instead of the periodic recomputation.
6.3 More on Correctness
Consistent conﬁgurations: The bandwidth overhead for dissem-
ination is low as the conﬁguration ﬁles are quite small (1-2 KB
per device). However, differences in the distances between the
devices and the NOC could lead RE devices to use inconsistent
caching conﬁgurations. To mitigate this, we can use latency in-
formation from topology maps to schedule the transfers to ensure
that all devices receive the new conﬁgurations at approximately the
same time. Also, for a small transition interval (few tens of mil-
liseconds), all RE devices honor both conﬁgurations. That is, the
encoders and decoders store packets assigned by either the old con-
ﬁguration or the new one. (RE devices can allot a small amount of
spare memory for this purpose). This may result in a small per-
formance reduction, as some packets may get decoded before their
optimally assigned decoders, but it ensures correct packet delivery.
Errors due to packet drops: Packet drops can cause encoder and
decoder caches to get out of sync. Packet drops cause two issues:
(1) Packets which are encoded w.r.t the dropped packet cannot be
decoded downstream; (2) When the higher-layer application re-
transmits the dropped packet, it is likely that the retransmission will
get encoded with respect to the dropped packet, and get dropped
again. TCP-based applications can typically recover from single
5With n access routers, there are O(n2) paths. Even restricting to
paths with the same ingress, the overhead for transmitting redun-
dancy proﬁles is O(n3).
Network
(AS#)
NTT (2914)
Level3 (3356)
Sprint (1239)
Telstra (1221)
Tiscali (3257)
GÉANT
Internet2
PoP-level
# PoPs Time
0.92
0.53
0.47
0.29
0.21
0.07
0.03
70
63
52
44
41
22
11
Router-level
# Routers
350
315
260
220
205
110
55
Time
55.41
30.06
21.41
16.85
11.05
2.48
0.48
Table 1: LP solution time (in seconds).
packet drops in a window, but drops of retransmitted packets (case
#2) severely impacts TCP throughput. We handle the latter as a spe-
cial case. If an ingress sees a packet which has a full content match
and the same connection 5-tuple match with an in-cache packet, it
will not encode this packet.
7. EVALUATION
Our evaluation is divided into the following sections: (1) Bench-
marks of the Click prototype and time taken by the optimization
framework.
(2) Beneﬁts of SmartRE compared to the ideal and
naive approaches using synthetic traces with different redundancy
proﬁles and resource provisioning regimes. (3) Evaluation using
real packet traces collected at a large US university’s border router
and at a university-owned /24 preﬁx hosting popular Web servers.
(4) Impact of staleness of redundancy proﬁles. (5) Beneﬁts under
partial deployment.
For the following results, we use PoP-level ISP topologies from
Rocketfuel [28] and add four access routers to each PoP to obtain
router-level topologies.
7.1 Performance benchmarks
LP solution time: Table 1 shows the time taken to generate the
caching manifests (on a 2.80 GHz machine) for seven PoP- and
router-level topologies. Even for the largest router-level topology
(NTT), the time to solve (using CPLEX) is < 60s. We envision
that reconﬁgurations occur on the scale of a few minutes – this
result shows that the optimization step is fast enough to support
such reconﬁgurations.
Encoding and decoding rates: We now try to understand how the
encoders and decoders can be used in practical ISP deployments.
To do so, we benchmark the implementations on a standard desktop
machine and extrapolate the performance to more realistic settings.
We run our prototypes on a desktop with 2.4GHz CPU, with a
DRAM latency of 90ns (benchmarked using PAPI6). We use real
packet traces from the /24 preﬁx. (This trace was 35% redundant
using a 600 MB packet cache and 10 ﬁngerprints per packet.) In
addition to computing the raw throughput, we also compute the
effective throughput after subtracting the overhead due to Click op-
erations. This extrapolates the results to a SmartRE middlebox im-
plemented on an FPGA [20] which would be constrained only by
memory accesses and have no software overhead.
First, we benchmark the encoder. To understand the maximum
throughput of a memory-bound RE middlebox, we follow the method-
ology of Anand et al. [12]: (1) load the packet trace into memory,
(2) precompute and load ﬁngerprints for all packets into memory,
(3) encode packets one by one, and report the throughput.
We conﬁgured a packet store to hold 600MB of packet payloads;
the corresponding ﬁngerprint index was 400MB in size. Using 10
ﬁngerprints per packet, the effective throughput obtained for en-
coding was around 2.2Gbps (after subtracting the Click overhead).
We also ran this on a machine with 120ns memory latency and
6http://icl.cs.utk.edu/papi/
94# Match Redundancy
Specs
Throughput (Gbps)
In software W/o overhead
1
2
3
4
5
24.1%
31.6%
34.6%
34.7%
34.8%
4.9
4.5
4.3
4.3
4.3
8.7
7.9
7.7
7.6
7.6
Table 2: Trade-off in redundancy and decoding throughput
with number of match-specs.
the throughput dropped to 1.5Gbps. Extrapolating, we conclude
that with lower DRAM latencies, the encoder can operate at OC-
48 linerates. (Today’s high-end DRAMs have ≤ 50ns latency as
opposed to 90ns on our desktop). Other SmartRE operations, e.g.,
redundancy proﬁle computation, storing in isolated buckets etc.,
add negligible overhead.
Next, we evaluate the decoding throughput. This depends on the
number of match regions encoded in packet shims: as more regions
get encoded, more redundancy is identiﬁed, but the throughput de-
creases as the number of memory accesses increases. We study this
tradeoff in Table 2. The decoding store size was set to 600MB.
We see that decoding is roughly 3-4× faster than encoding, since
it involves fewer memory operations per packet. While decoding
throughput does decrease with more matches (due to more memory
accesses), the decrease is small for ≥ 2 matches. Our implemen-
tation uses a maximum of 3 match-specs as a tradeoff between the
amount of redundancy identiﬁed and the throughput.
Our simple encoder and decoder implementations can roughly
operate on OC-48 (2.5Gbps) and OC-192 links (10Gbps), respec-
tively. In networks where such links are used, SmartRE can lever-
age the encoding and decoding capabilities of nodes to give opti-
mal beneﬁts. Middleboxes based on these simple designs can also
be used in ISPs that employ faster links, e.g., 40Gbps for the core.
The only difference is that each decoder may be able to act only on
one-fourth of the packets entering the router; the rest of the pack-
ets need to be decoded at other locations. In this case, the bene-
ﬁts of SmartRE may not be optimal. We explore the gap between
SmartRE and the optimal in greater detail next.
7.2 Synthetic trace study
We compare the beneﬁts of SmartRE, the hop-by-hop approach
without any resource constraints (i.e., hop-by-hop ideal), the hop-
by-hop approach with actual resource constraints, and a special
case of SmartRE called edge-based RE. In both SmartRE and edge-
based RE, encoding is a one-time task; performed only at the in-
gresses. However, decoding happens only at the edge of the net-
work in edge-based RE, unlike SmartRE. While SmartRE can ef-
fectively operate under all types of redundancy proﬁles, edge-based
RE is effective only when intra-path redundancy is the dominant
source of repeated content. Hop-by-hop ideal represents the best
possible beneﬁts achievable from network-wide RE assuming that
RE devices are unconstrained. Our main goals are to understand
how close to ideal SmartRE gets, how much better it is than other
approaches, and what factors contribute to SmartRE’s performance.
Setup: We implemented an ofﬂine emulator using Click to com-
pare different network-wide RE solutions. We assume a middlebox
deployment where each network link has RE devices attached on
both ends of the link. For SmartRE, the device at one end of a link
is used for decoding/encoding packets in one direction, and the one
at the other end is used for the reverse direction.
Encoders at each access link store T seconds of packets (e.g.,
3 GB memory at 2.4 Gbps implies T = 10s). Decoders at the
edge have the same cache size as the encoders. Each interior RE
device uses a 6GB cache which we consider to be reasonable from
a cost view-point in practical settings; we also evaluate the effect
 1
 0.8
n
o
i
t
c
a
r
F
 0.6
 0.4
 0.2
 0
Hop-by-Hop
Edge-based
SmartRE
Hop-by-Hop Ideal
 0  0.05  0.1  0.15  0.2  0.25  0.3  0.35  0.4
Reduction in Network Footprint
(a) 50% redundancy trace
 1
 0.8
n
o
i
t
c
a
r
F
 0.6
 0.4
Hop-by-Hop
Edge-based
SmartRE
Hop-by-Hop Ideal
 0  0.05  0.1  0.15  0.2  0.25  0.3  0.35  0.4
 0.2
 0
Reduction in Network Footprint
(b) 25 % redundancy trace
Figure 9: CDF of network footprint reduction across ingresses
for Sprint (AS1239) using synthetic traces.
of varying this cache size. We model the throughput of each device
in terms of the total number of memory operations per second. We
select bounds that reﬂect the throughput achieved by our software
prototypes. Assuming (conservative) memory latencies of 100ns,
20 lookups for encoding each packet, and 4 lookups in total for
decoding each packet, this translates into 0.5 million encodings and
2.5 million decodings per second respectively.
Trafﬁc model: We use a gravity model based on city populations to
determine the fraction of trafﬁc from each ingress access router to
an egress PoP. Within each PoP, the trafﬁc is divided equally among
the 4 access routers. Each trace’s redundancy proﬁle is speciﬁed by
three parameters: γ, γintrapop, and γintrapath. γ is the overall trafﬁc
redundancy per-ingress access link. γintrapop determines the redun-
dancy within the trafﬁc destined for the same egress PoP. Within
each egress PoP, γintrapath determines the intra-path redundancy of
the end-to-end path between the ingress and egress access routers.
These parameters specify how redundant the trafﬁc is, and how lo-
calized or how dispersed the redundancy proﬁle is. If γ is high then
the trafﬁc is highly redundant; if γintrapop is high then most of this
redundant trafﬁc is destined to the same PoP; if γintrapath then most
of the intra-PoP redundancy is within the same ingress-egress path.
Results: We ﬁrst consider the single-ingress case, where trafﬁc