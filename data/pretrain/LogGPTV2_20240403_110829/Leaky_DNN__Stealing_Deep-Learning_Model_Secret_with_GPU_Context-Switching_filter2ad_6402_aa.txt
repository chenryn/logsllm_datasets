title:Leaky DNN: Stealing Deep-Learning Model Secret with GPU Context-Switching
Side-Channel
author:Junyi Wei and
Yicheng Zhang and
Zhe Zhou and
Zhou Li and
Mohammad Abdullah Al Faruque
Leaky DNN: Stealing Deep-learning Model Secret
with GPU Context-switching Side-channel
Junyi Wei∗, Yicheng Zhang†, Zhe Zhou∗, Zhou Li† and Mohammad Abdullah Al Faruque†
∗Fudan University, Email: PI:EMAIL, PI:EMAIL
†University of California, Irvine, Email: {yichez16, zhou.li, alfaruqu}@uci.edu
Abstract—Machine learning has been attracting strong inter-
ests in recent years. Numerous companies have invested great
efforts and resources to develop customized deep-learning models,
which are their key intellectual properties. In this work, we
investigate to what extent the secret of deep-learning models can
be inferred by attackers.
In particular, we focus on the scenario that a model developer
and an adversary share the same GPU when training a Deep
Neural Network (DNN) model. We exploit the GPU side-channel
based on context-switching penalties. This side-channel allows us
to extract the ﬁne-grained structural secret of a DNN model,
including its layer composition and hyper-parameters.
Leveraging this side-channel, we developed an attack prototype
named MoSConS, which applies LSTM-based inference models to
identify the structural secret. Our evaluation of MoSConS shows
the structural information can be accurately recovered. There-
fore, we believe new defense mechanisms should be developed to
protect training against the GPU side-channel.
Index Terms—Deep-learning; GPU; Side-channel;
I. INTRODUCTION
In recent years, technologies driven by machine-learning,
especially deep learning, have been gaining strong momentum
from the research community and industry. Those technologies
have shown promises and early success in transforming the
application domains, like computer vision [54] and speech
recognition [64]. Driven by this wave, numerous companies
have been devoting human and computing resources to develop
customized machine-learning models, which have made them
“highly valuable intellectual properties” [49].
Unfortunately, the high value of machine-learning models
also makes them lucrative targets to attackers. Because many
machine-learning models are trained on a public cloud or a
providing public interface [7], [18], they do present a broad
attack surface. As demonstrated by previous works, the param-
eters (e.g., weights) and hyper-parameters (e.g., regularization
terms) of classical machine-learning models like logistic re-
gression can be inferred from the public interface [59], [61].
Stealing DNN model secrets through side-channel. How-
ever, applying the attack methods described above against
Deep Neural Networks (DNN) models are inefﬁcient for an
adversary. Given that DNN models are highly customized and
containing a multitude of hyper-parameters, the search space is
huge. Recently, a number of works were proposed to steal the
model secret through side-channel attacks [5], [13], [23]–[25],
Junyi Wei and Yicheng Zhang are both ﬁrst author. Zhe Zhou is the
corresponding author.
1
[41], [63], [65]. Some of those works assume the adversary has
physical access to the device so high-resolution side-channels
about power consumption and accessed memory addresses can
be exploited [5], [24], [25], [63]. For the remaining works
about the remote adversary, most of them exploited CPU-
based cache side-channel [13], [23], [65]. Given that
the
Graphics Processing Unit (GPU) has become the dominant
hardware to train and run DNN models, the practical impact
of those CPU-based attacks is questionable. The only work
investigating GPU was done by Naghibijouybari et al. [41],
showing that the number of neurons of DNN’s input layer can
be learned. Yet, this information provides little guidance for
an adversary to recover the whole DNN structure. The key
question we ask and aim to answer in this work is: can an
adversary infer DNN structural secret like layers and their
hyper-parameters by exploiting GPU side-channel?
As the ﬁrst step, we revisited the existing GPU side-
channel [41] but found it insufﬁcient for our goal. Their attack
exploits an Nvidia GPU feature named Multi-Process Service
(MPS), which allows the attacker’s kernel (called spy) to stay
in the same GPU cores with a victim kernel. The spy observes
the victim kernel’s resource usage by taking samples through
CUPTI [45] (Nvidia’s performance counters). However, due
to the unbalanced scheduling by MPS, the spy is allowed to
collect only one sample at the end of one training iteration,
which is too coarse-grained to reveal the DNN structure.
Comparing to the previous work [41], we pursue the oppo-
site direction. We let MPS be switched off (the default setting)
and run a spy concurrently with the victim’s DNN to force
context switching. This time, the time-sliced scheduler ensures
spy and victim kernels to take fair shares of execution time.
Therefore the spy can achieve a much higher sampling rate
through CUPTI, as illustrated in Figure 2 and Figure 3.
Challenges. Still, several challenges have to be addressed to
recover the model structure. 1) The transition between DNN
layer operations (or op) is too fast to be observed by the
spy, making the ops inseparable from spy’s view, due to
its insufﬁcient sampling rate. A similar situation exists also
for short ops. 2) The execution time for different ops varies
signiﬁcantly, resulting in an uneven number of samples among
ops. 3) Different from the kernel co-location side-channel [41]
that directly tells spy the victim’s resource usage, for context-
switching side-channel exploited by us, the only small penalty
can be observed, which reﬂects the victim’s resource usage
indirectly. In addition, the penalty to the current kernel is
highly impacted by what has been executed by previous
kernels. Discerning ops using this information is more difﬁcult.
Our attack. We address those challenges by developing a
hybrid attack framework. To address the issue of insufﬁcient
sampling rate, we launch the GPU denial-of-service (DoS)
attack using multiple spy kernels to slow down the victim
kernel. By doing that, a spy is able to obtain a lot more
samples per op and increase the prediction accuracy. To
address the issue of unbalanced samples and weak side-
channel, we design the inference model on top of Long short-
term memory (LSTM) model, which is capable of handling
complex time-series [31], [36] and utilizing the operation con-
textual information. In addition, we customize the inference
model based on unique insights into DNN training. Instead
of identifying layers and hyper-parameters at one pass, we
design different LSTM models to identify convolutional ops,
non-convolutional ops, and hyper-parameters separately, which
increases the prediction accuracy for individual op and hyper-
parameter. Due to multi-iteration training, the same execution
sequence of DNN layers can be observed many times, which
gives the adversary an opportunity to correct misclassiﬁcation.
We develop two voting models based on LSTM to merge
predictions across iterations. Finally, we leverage the DNN
model syntax (i.e., rules well-known to the machine-learning
community) to correct the remaining errors.
We implement our attack (named MoSConS1) based on
the above design. By executing MoSConS, an adversary will
have the capability of inferring the structure of a DNN
model trained on the cloud by a victim. We found MoSConS
is effective on the cloud even when the CUPTI access is
restricted by the latest NVidia driver [47], after the adversary
performs a driver downgrading attack. MoSConS is evaluated
on a popular GPU (Nvidia GeForce GTX 1080 TI) with
TensorFlow installed. Our evaluation shows MoSConS can
achieve high accuracy for inferring model secret, including
operation sequence, layer hyper-parameters (neuron num-
ber, ﬁlter size, ﬁlter number and stride) and optimizer.
To highlight, after several models are proﬁled by the ad-
versary, she can predict the layers and hyper-parameters of
MLP, ZFNet and VGG16 with 98.4% and 86.6% accuracy
on average, which signiﬁcantly reduces her time, monetary
and the labor cost of constructing a full-ﬂedged model. As
such, we argue stealing ﬁne-grained model secret is feasible,
and we advocate the model secrecy should be considered
when building new machine-learning infrastructures, including
hardware and system stacks.
Contributions. We summarize our contributions below.
• We developed a new way of exploiting the previously
discovered GPU side-channel [41], allowing attacker to
inspect another CUDA application at ﬁne-grain.
• We carried out a set of pilot studies to understand how
DNN ops are scheduled by system stack and GPU. We
also showed how context-switching penalties could be
inﬂuenced by different spy and victim kernels.
1Short for Model Secret Extraction with GPU Context Switching.
• We developed a new attack MoSConS that can extract the
structural secret of a DNN model. Our evaluation shows
MoSConS can achieve high inference accuracy.
II. BACKGROUND
A. Deep Neural Networks
layer, the neuron function is φ((cid:80)n
Deep learning is a family of machine-learning methods that
feature a transformation from input to output with a cascade
of non-linear processing units. A non-linear processing unit
is called a layer, while the transformation is called a model.
Before training the model, the developer should deﬁne a model
structure, including what layers to use, their hyper-parameters,
and how they should be connected. Convolutional Neural
Network (CNN) and Recurrent Neural Network (RNN) are
the most popular DNNs. CNN cascades layers while RNN
features a feed-back loop between layers. In this work, we
focus on CNN models like other prior works [5], [13], [23]–
[25], [41], [63], [65].
Layers. A CNN model typically contains three types of layers:
convolutional layer, fully-connected layer, and pooling layer.
Each layer consists of a number of neurons, which take input
from neurons (say x1, x2, ..., xn) of the previous layer and
computes an output for the next layer. For fully-connected
i=1 wixi + β), where wi is
the weight for every input neuron xi, β is the bias value and
φ is the non-linear transformation function (e.g., ReLu). A
convolutional layer uses the kernel to ﬁlter input so only a
spatial region is connected to one neuron. The pooling layer
reduces the size of the input to decrease the computational
overhead. For example, max-pooling returns the maximum
value for a subset of input. We consider the number of layers
and layer sequence as a model secret.
Hyper-parameters. Hyper-parameters are set by the develop-
ers before training. They can be grouped into two categories:
the variables speciﬁc to each layer (we call them layer hyper-
parameters) and the variables related to the training algorithm
them model hyper-parameters). The layer hyper-
(we call
parameters mainly describe the spatial properties of each layer.
The model hyper-parameters include learning rate, number of
epochs, batch size, etc. In this work, we focus on layer hyper-
parameters. In particular, we consider the following as model
secret: 1) the activation function used by each layer; 2) the
number of neurons for fully-connected layer; 3) ﬁlter size
of each convolutional layer; 4) the number of ﬁlters of each
convolutional layer; 5) the stride of each convolutional layer.
DNN training. The primary goal of training a DNN model
is to learn neuron weights. Before training, the developer
designs a loss function that measures the prediction errors.
By gradually minimizing the loss, the developer ultimately
gets a satisfying model. The weights adjustment is usually
done by a gradient descent optimizer, which calculates the
loss ﬁrst (called forward propagation) and then calculates
the gradient of the updated loss over weights, from deeper
layers to shallower layers (called back propagation). With the
gradient, each weight is adjusted by subtracting a constant
2
(learning rate) multiplying the gradient. A training dataset is
usually divided into batches and the model’s internal parameter
is updated after all samples of a batch are processed (called an
iteration). Each run of the training dataset is called an epoch
and the whole training process typically takes many epochs,
consuming days for large datasets [12], [21].
Model structure. While there are only a few basic building
blocks for a DNN, the way how they are assembled and tuned
has a fundamental impact on DNN’s performance. Take image
recognition as an example. The evolution of model families
from AlexNet [29], VGG [57], Inception [58] to ResNet [22]
advances the top-5 performance on ImageNet challenge [54]
from 83.6% to 96.43%, which is even on par with human’s
performance. What’s more, even small customization within
the same model family can make a big difference. For ex-
ample, under the VGG model family, VGG19 adds 6 extra
3x3 convolutional layers to VGG13. The error rate can be
reduced from 9.6% to 7.5% [57], which can rank it to the
second in the ILSVRC2014 benchmark [54]. On the other
hand, knowing which “knob” to tune and how is by no means
trivial, as the possible combinations of layers and their hyper-
parameters are nearly inﬁnite. As such, the model structure is
a key intellectual property [49] and deserves strong protection.
DNN system stacks. To reduce the efforts in training, DNN
system stack is developed, like TensorFlow [1], Torch [52],
MxNet [39] and Caffe [26]. Those system stacks can translate
the high-level code (e.g., Python) that describe the model
structure to low-level code (e.g., Nvidia CUDA) that are
tailored to the hardware platform (e.g., CPU/GPU, single
machine/distributed cluster). In this work, we examine our
attack against TensorFlow due to its popularity [55].
B. GPU Architecture
Unlike CPU integrating several cores in a die, powerful
GPU can have thousands of cores in a single die executing in-
structions concurrently. Given that DNN operations are mainly
based on Generalized Matrix Multiply (GEMM), GPU turns
out to be better hardware to train DNN models comparing to
CPU. In this work, we focus on Nvidia GPU.
To use the GPU resources for training DNN, Nvidia recom-
mends using its API interface for general-purpose computing
called CUDA (Compute Uniﬁed Device Architecture) [44].
In particular, a CUDA application (or host application) is
composed of a number of kernels (i.e., computing tasks) to be
executed in GPU. A sequence of kernels that follows execution
order is called a GPU stream. Each kernel needs to specify the
number of blocks and how many threads to be used by each
block [42]. A group of blocks is called a grid and all threads
under the same block have to be executed in the same SM
(Streaming Multiprocessor) and the entirety of a block must
be executed before scheduling another block. Inside SM, 32
threads are grouped and launched together (called warp) by
the warp scheduler, which allocates resources per warp (e.g.,
cores). An SM can have multiple warp schedulers and only one
warp is managed by a warp scheduler at a time. Before a warp
Fig. 1. Adversary model of our attack.
is scheduled, GPU uses the concurrent scheduler and time-
sliced scheduler to determine the execution order of kernels
and blocks.
When a CUDA application runs on GPU, a CUDA context
is created. It is similar to the CPU context, which keeps
references to memory, registers, and other state information.
When a CUDA application is preempted by another, its context
will be switched out and replaced by the next application.
C. GPU Proﬁler
To help developers of GPU applications with performance
tuning, GPU vendors and DNN system stacks offered execu-
tion proﬁlers. While they provide valuable insights to develop-
ers, we found they can be exploited for model stealing. Below
we describe the proﬁlers provided by Nvidia and TensorFlow.
Nvidia proﬁler. A developer can use CUDA Proﬁling Tools
Interface (CUPTI) [45] of Nvidia to proﬁle CUDA appli-
cations. In particular, its Event & Metrics APIs allow the
developer to interact with the performance counters which log
the resource usage of GPU. The developer needs to initialize
CUPTI before running the CUDA application. When the
application completes, the readings of performance counters
will be returned through the Event & Metrics APIs. Take event
L2_subp0_read_tex_hit_sectors as an example. It
represents the number of reading requests from Texture cache
that hit the slice 0 of L2 cache. Thus, we can leverage its
reading to have an insider look into how the CUDA application
interacts with GPU cache and memory.
TensorFlow proﬁler. TensorFlow provides a timeline mod-
ule [19] to help developers proﬁle the execution of DNN
operations on GPU. In particular, it logs the name of each
operation, its start and end timestamp and the relevant param-
eters. To enable the TensorFlow proﬁler, the developer needs
to change a TensorFlow conﬁguration option trace_level
to FULL_TRACE. When the training is ﬁnished, the timeline
module will keep all the proﬁled traces into a JSON ﬁle which
can be visualized by the Chrome browser (loading the JSON
ﬁle under the page chrome://tracing). An example of a
timeline is illustrated in Figure 2. In fact, by correlating the
CUPTI readings with the TensorFlow timeline, we are able to