wise distances. This distance threshold parameter is expressed as a
normalized value between those two averages.
Minimum cluster sizes. Finally, we introduce a threshold on the
minimum number of registrations within a cluster. As such, only
clusters with a sufficient number of malicious registrations during
the training phase are taken into account for predicting the mali-
ciousness of a new registration. Setting a higher minimum cluster
size shifts focus towards larger, long running campaigns, whereas
a smaller minimum cluster size picks up newly started campaigns
more quickly.
4.3 Parameter tuning
We look at the following parameters to determine the best similarity-
based model configuration in the validation phase.
Distance threshold
Minimum cluster size
Training window
0.50 - 1.00
5 - 50 registrations
15, 30, 45 and 60 days
Similar to Section 3.4, we execute a training and prediction step
for each day in the validation phase (Figure 1) to determine the
overall performance of a configuration. Figure 5 shows the perfor-
mance of each setting in terms of precision and recall. As depicted,
the different configurations achieve a trade-off between precision
and recall.
A noticeable trend in Figure 5c is the impact of the distance
threshold. A lower threshold achieves a higher precision as only
registration very similar to malicious instances are withheld, while
a higher threshold tends towards a higher recall.
Reducing the minimum cluster size pushes the predictor to-
wards a higher recall, as new small campaigns are also taken into
account (Figure 5a). In contrast, with a high minimum cluster size,
the model only predicts registrations that are part of the largest
campaigns.
Counterintuitively, the 60 day training window is not able to
achieve the near 100% precision, as is the case with the other train-
ing windows (Figure 5b). However, this is not due to the training
window size itself, but rather due to our static range for minimum
cluster sizes: for a training window of 15 days, a cluster size of 30
registrations is a big factor, whereas this is less the case for a 60 day
training window. To counter this, a bigger range of cluster sizes
could be tested.
5 FINAL PREDICTIVE MODEL
In this section, ensemble predictors are designed by combining
the models of the reputation-based classification (Section 3) and
similarity-based clustering (Section 4). During the validation phase
(shown in Figure 1), the best performing ensemble is selected. Af-
terwards, we move to the testing phase and present a in-depth
evaluation of the selected ensemble on the 11-month testing set.
5.1 Majority voting model
For the two complementary prediction strategies, we have already
evaluated the performance of their different configurations on the
validation set. Now, we apply majority voting [14] to construct en-
sembles of any three base predictors coming out of both Section 3
and 4. The results of these ensembles in the validation phase are dis-
played in Figure 7. For clearer visualization, we limit the plot to the
envelopes of best-performing reputation-based and similarity-based
predictors, next to the best-performing majority voting ensembles.
The ensemble models clearly achieve better results in both precision
and recall.
In terms of model selection, we choose the ensemble with the
highest F1-score. The F-score expresses the weighted harmonic
mean of precision and recall, enabling evaluation of a model’s
performance through a single metric.
Fβ = (1 + β
2
) ·
precision · recall
2 · precision) + recall
(β
The selected ensemble is annotated in Figure 7. It combines the