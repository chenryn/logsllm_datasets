gatelists that are compiled from the Verilog HDL, a popular lan-
guage for hardware design. The concepts and algorithms we apply
could be applied to VHDL or any other common HDL, as well as
to hand-written gatelists. Though our analysis is language agnos-
tic, we use Verilog for all evaluation purposes. We use benchmarks
from the TrustHub suite, a popular benchmark suite for work on
hardware backdoors [10]. TrustHub is a suite from an online com-
munity of hardware security researchers and includes a variety of
different types of backdoors, intended to be both stealthy and ef-
fective. For some of these benchmarks, the gatelists were provided.
For others, we acquired the gatelists from the Verilog source using
the Synopsys logic synthesis tool, DC Compiler.
From a given gatelist, our goal is to construct a circuit represen-
tation that can be used to calculate different types of dependencies.
We treat multiple-bit wires as sets of independent wires. Gates that
represent multiple basic logic functions — such as an AND-OR-
INVERTER (AOI) — are treated as functionally equivalent to their
basic elements. We treat memory elements (e.g., ﬂip-ﬂops) as their
logical equivalents. For example, a D-ﬂip-ﬂop is treated as an iden-
tity function. We do this because exponential state-space explo-
ration is infeasible, and as such treating state machines as stateful,
rather than as their combinational counterparts, would be imprac-
tical. Since we track all internal wires (as opposed to only inputs
and outputs), we catch sequential backdoors by catching the com-
binational logic used during internal state recognition.
5.1 Results for Detecting Backdoors
We evaluate the four heuristics presented in Section 3.3 on the
TrustHub benchmarks. We perform one run on each design2 with
215 = 32, 768 input cases (truth table row pairs), with the row pairs
chosen uniformly at random (without replacement).
The most important result is that we did not encounter false neg-
atives. For each benchmark and each of the heuristics, we discov-
ered at least one suspicious wire from each backdoor, which was
enough for us to identify the functionality of the hidden backdoors.
Different metrics can highlight different parts of the backdoor. The
mean and median tend to highlight backdoor payload wires and
are more similar to each other than to triviality. We hypothesize
that this is because these payloads have triggers or resulting val-
ues from triggers as their inputs. Thus, several of the input wires
2If desirable, multiple runs could be performed to increase conﬁ-
dence. In practice, the same results tend to come up every time, but
it cannot hurt.
have low control values. On the other hand, triviality focuses more
on the output wire itself and as such tends to highlight backdoor
trigger wires. Since these are wires that rarely toggle, their truth ta-
bles tend to score very low for triviality. Using multiple metrics in
concert can help out in code review by ﬂagging more of the wires
associated with the backdoor and thus demarcating the boundary of
the backdoor more clearly.
Figure 3 shows the results for the 18 TrustHub benchmarks we
analyzed regarding false positives. For our results, we categorize
the benchmarks into groups as they are categorized by TrustHub.
These categories represent four different design types, containing
a variety of backdoor triggering mechanisms. Each of the four
groups contains a variety of backdoors manually included into a
given design. The RS232 group contains eleven benchmarks, rep-
resenting eleven different backdoors applied to a relatively small
third-party UART controller. The S35932 and S38417 groups each
contain three benchmarks, containing backdoors built into two gatelists
whose source and description are not provided. The S15850 group
contains only one benchmark. The S38417 group contains the
largest designs in terms of area and number of gates, while the
RS232 benchmarks, as the smallest, mostly contain sequential trig-
gers. The s15850, s35932, and s38417 categories are qualitatively
different from RS232 and more similar to each other. We experi-
enced a decrease in false positive percentage for these larger de-
signs, which we attribute to the fact that the total number of false
positives did not vary signiﬁcantly with respect to design size.
Additionally, the different benchmark categories achieve differ-
ing degrees of stealth (some are documented and others can be cal-
culated manually). The stealth is imply the probability that a back-
door will accidentally reveal itself on a random test input. Most
of the triggers in the RS232 category have a relatively high prob-
ability (i.e. low stealth) of going off randomly, as high as around
one in a million. In the other categories, the probabilities are lower,
ranging from one in several million to as low as around one in 2150.
The backdoors in the three low probability groups are the most re-
alistic, since they are stealthy enough to evade detection by normal
methods. The backdoors in the RS232 category go off with such
high probability that validation testing would have a good chance
of ﬁnding them. This is an aspect that made them more difﬁcult to
distinguish and resulted in slightly more false positives. From what
we have empirically observed, the larger the design and the more
well-hidden the backdoor, the better FANCI performs in terms of
keeping false positive rates low.
Unsurprisingly (as shown in Figure 3), using the median by it-
self produced the most false positives on average. However, the
difference is not large. The heuristic that produced the least false
positives on average was triviality. All four metrics are effective
enough for practical use. We also believe that other metrics could
be considered in the future to achieve incremental improvements.
A promising result we discovered was that the percentage of false
positives diminished as we looked at larger designs (granted this is
a small sample set). In other words, it appears that scaling up to
larger designs does not greatly increase the total number of false
positives (i.e. the effort of code review).
Figure 4 shows how many wires are ﬂagged as suspicious on
average for each of the benchmark groups by each of the differ-
ent metrics. Each of the four metrics worked well, though the
mean turned up the most suspicious wires on average (at the cost
of slightly higher false positive rates). We see that all four metrics
ﬂag only a small number of critical wires, which means security
engineers are given a small and targeted set to inspect. For most of
the benchmarks, FANCI whitelists more than 99% of the designs,
making code review and inspection a feasible task.
703Figure 3: False positive rates for the four different metrics and
for TrustHub benchmarks. The RS232 group — which is the
smallest — has about 8% false positives. The others have much
lower rates (less than 1%).
Figure 4: These are the total number of suspicious wires de-
tected by each method for each type of backdoor design on av-
erage. For each design and each of the four methods we tried,
we always found at least one suspicious wire. Thus, each of the
four methods is empirically effective. However, some turned up
larger portions of the trigger critical paths, proving to be more
thorough for those cases.
We lastly test to see what happens as we increase and decrease
the number of input rows we sample. The results are shown in
Figure 5. We see that up to a certain point, the results improve.
After that point, the results tend to converge and stay roughly the
same. This is essentially the weak law of large numbers kicking in,
and it allows FANCI to scale well. Note that due to randomness,
sometimes we ﬂag more values using less inputs. This ends up not
affecting our results signiﬁcantly, since the true positives tend to be
clustered in the design, so adding or removing one wire does not
make a large difference in code review. What we also learned from
varying the number of inputs is that there are two sources of false
positives. The ﬁrst source is approximation. If we run only a few
inputs, we get extra false positives, and if we run more inputs we get
less false positives. The second source is from persistent positives,
i.e. weakly-affecting signals that are in the design for legitimate
reasons. The ﬁrst type disappears quickly as the number of inputs
gets large, which is why false positives due to approximation are
not a major concern.
Figure 5: The trade-off between the number of inputs being
used (i.e. running time) and the percentage of true positives
caught, normalized to the results for 215 inputs. Results are
shown averaged over the four different metrics we used. The
x-axis is on a logarithmic scale.
5.2 Runtime and Random Row Selection
The runtime for FANCI is roughly proportional to the size of the
design under test in terms of number of total gates. In practice,
the runtime for a normal module ranges from less than an hour to
a couple of days using 215 row pairs per approximate truth table.
The runtime can be increased or decreased by changing the num-
ber of inputs tested. In practice we did not ﬁnd it necessary to de-
crease this number. Given the sizes of third-party IP components on
the market, the runtime for FANCI should not be a major problem
for real-world scenarios. Our runtime in terms of number of gates
scales similarly to many synthesis and analysis tools, since our tool
and other tools require the parsing of every gate in the design.
To be precise, the asymptotic runtime for a deterministic algo-
rithm would be in the set O(nd2d) where n is the number of gates
(or nodes) and d is the maximal degree of a node, i.e. the maximal
number of inputs on which an intermediate wire can depend. Us-
ing approximate truth tables reduces the asymptotic runtime to the
set O(nd). Making the (usually true) assumption that the degree is
small and bounded, this reduces to the set O(n), which represents
linear runtime. The algorithm is trivially parallelizable, since the
algorithm is in essence a massive for loop. Our initial implemen-
tation is sequential, but in the future it could be made parallel if
necessary.
Lastly, we do not do directed testing or targeting of speciﬁc rows
in truth tables or speciﬁc inputs. We go with uniform randomness
because any other method would be better for an attacker and worse
for us as the security engineers (assuming the attacker knows our
strategy).
5.3 Discussion of False Positives
One lesson learned from our experiments is that false positives
tend to be consistent and are not greatly affected by the randomness
of our sampling methods. We anticipate that the false positives we
encounter in the future will bear similarities to each other, perhaps
allowing for easier recognition. Some examples of potential false
positives could be the most signiﬁcant bit of a large counter or an
input to an exception-recognition circuit. These circuits are seman-
tically similar to backdoors, because they react to one speciﬁc rare
case. For example, consider a ﬂoating point divider that throws a
single exception, caused by a divide-by-zero error. Then for the
704data input representing the divisor, only the value zero invokes the
exception-handling logic. The exception-handling logic is nearly-
unused.
The existence of these circuits should not pose much of a prob-
lem, because counters and exceptions are easily recognizable in
code review. Nevertheless, as an attacker, one could be motivated
to include many such circuits to increase the false positive count.
The problem from an attacker’s point of view is that each of these
false positives requires a costly circuit, and so building an entire
design this way would be impractical. Additionally, these types of
circuits tend to have obvious architectural purposes, and so adding
thousands of them would be a dead giveaway in code review. For
example, including a large number of exception handlers that serve
no apparent purpose would be a source of concern during code in-
spection.
Our hypothesis was that in real designs (i.e. designs that one
might buy as commercial IP), even malicious designers are forced
to follow common design conventions and design reasonably efﬁ-
cient circuits. We believe that this is the reason we did not ﬁnd a
signiﬁcant number of false positives in any of the designs we ana-
lyzed.
A related and important property of our approach is that it be-
haves well with respect to common, reusable structures. In modern
designs, much of the circuitry is spent on reusable components,
such as CAMs, RAMs, FIFOs, decoders, encoders, adders, regis-
ters, etc. For some simple designs, such as adders and multipliers,
the results of FANCI have been mathematically veriﬁed. We have
not had issues with false positives for these common types of struc-
tures. When identifying suspicious wires, we look for outliers. In
these standard structures, there tend to be no outliers due to symme-
try. Consider a CAM with 32-bit data entries. For each entry, there
is a 32-bit data comparator, which includes some very low control
1
value dependencies (on the order of
232 ). However, each of the
comparators is identical (or nearly identical), leaving no outliers
to serve as false positives. Additionally, the nature of the structure
should make it obvious in code review how many such wires should
exist (often a power of two or otherwise documented number).
5.4 Out-of-Order Processor Case Study
In order to study FANCI on a larger and backdoor-free design,
we use the FabScalar microprocessor core generation tool [11].
FabScalar is an HDL code generator that produces processor cores
given a set of parameters. The core we choose to use is a moderately-
sized, out-of-order core with four execution units and does not con-
tain backdoors.
The core we analyze has a total of 56 modules. The modules
contain about 1900 distinct wires on average, with the largest mod-
ule containing slightly over 39,000 distinct wires. This largest one
is abnormally large for a single module containing primarily com-
binational logic. However, as this is an auto-generated design, it
is understandable.
If it were being hand-written, it most likely
would be broken into smaller, coherent pieces. While the overall
design is larger than any of the modules from the TrustHub suite,
and larger than typical third-party IP components, many of the in-
dividual modules are on average around the same size as modules
in the TrustHub suite.
We were able to analyze each of the 56 modules in FabScalar
using 215 row pair samples per truth table, except for two of the ab-
normally large modules where we had to approximate more coarsely.
The two largest modules are outliers and took several days to pro-
cess, even using more coarse-grained approximation. These could
more easily be analyzed in a commercial setting on a compute clus-
ter. Additionally, many software optimizations (including paral-
Figure 6: A histogram of the triviality values for wires in a
typical FabScalar module called CtrlQueue. The biggest spikes
occur at around 1
8 , which is common. There are no
major outliers. X-axis values are shown on a logarithmic scale,
starting at one and getting smaller going to the right. Inlaid in
the upper right is the sum of all 56 FabScalar modules.
4 and 1
2 , 1
lelization) could be applied prior to commercialization.
As expected, we did not detect false positives in the benign Fab-
Scalar core. To garner further intuition for how our heuristics look
for wires in benign hardware, we construct a histogram of a typi-
cal FabScalar module (shown in Figure 6). In this example, there
are two major spikes at 1
8 . The reason for the presence of
spikes is that semantically similar wires tend to have similar values,
as we saw in the example of a multiplexer. For this module, there
are no suspicious outliers, with all of the values being more than
0.01 (and less than 0.99). We did not see any noticeable outliers,
and our thresholds are typically less than 0.001. More data from
FabScalar is included in the Appendix.
4 and 1
2 , 1
5.5 Security Discussion and Limitations
We brieﬂy discuss some of the security properties discussed in
this paper and the corresponding limitations.
• FANCI relies on the assumption that backdoors use weakly-affecting
wires. This is valid in practice because they need to be stealthy. The
more well-hidden the backdoor is, the more likely it is to be caught
by FANCI because more well-hidden backdoors have lower control
values. It is provable3 that for a ﬁxed-length combinational circuit
path, achieving a given level of stealth requires a correspondingly
low control value for one or more of the inputs. On the other hand,
the less well-hidden it is, the more likely it is to evade FANCI but
be caught during testing. We would call such an attack a Frequent-
Action Backdoor, where the idea is to put the backdoor in plain
sight. Standard validation testing and FANCI are highly comple-