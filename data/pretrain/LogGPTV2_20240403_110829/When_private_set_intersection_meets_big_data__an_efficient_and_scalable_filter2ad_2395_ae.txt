m
T
i
parallel
pipelined
unencrypted pairwise comparison (best case)
unencrypted pairwise comparison (worst case)
set size=1,048,576
1800
1600
1400
1200
1000
800
600
400
200
)
c
e
S
(
e
m
T
i
0
256
192
128
80
1,024
4,096
65,536
16,384
Set Size
Security Level
(c) Performance: the parallel mode
1,048,576
262,144
0
80
128
192
Security Level
256
(d) A comparison of running time in the two modes
Figure 4: Performance of our basic protocol
The hash functions in the OT extension protocol are instantiated
depending on the security parameters. When hash values need to
be truncated, the truncation follows the steps speciﬁed by the NIST
[14]. We use the NIST elliptic curve groups over Fp [36] for the
public key operations required by the Naor-Pinkas OT protocol. We
use elliptic curve groups because they are much faster than integer
groups at high security levels.
The C prototype has two executables, one for the client and one
for the server. The client and server communicate through TCP.
The prototype can work in two modes: pipelined and parallel. In
the pipelined mode, on each side, the computation is done in a sin-
gle thread, an additional thread transmits data in parallel when pos-
sible. Parallel data transmission enables the server or the client to
start working immediately without waiting for the other party to
complete its computation. The parallel mode extends the pipeline
mode by utilizing all CPU cores and distributing tasks on all cores
evenly. Our test result shows that the parallel mode can improve the
performance signiﬁcantly on multicore systems. This is due to the
fact that the computation in our protocol is dominated by indepen-
dent hashing. Namely, on each side, n independent set elements
each needs to be hashed k times to build the Bloom ﬁlter or the
garbled Bloom ﬁlter, also hashing of m matrix rows are needed in
the OT extension protocol. As the data to be hashed is indepen-
dent, this is a perfect SPMD (single program multiple data) sce-
nario. The program detects the number of cores available, decides
the number of threads, evenly allocates a portion of data to each
thread, and then launch the threads to execute the tasks in paral-
lel. The hash values are then consumed by main threads that run
the protocol. This approach requires only minimal changes to the
program structure. For example, only one line (line 8) needs to be
changed in Algorithm 1. Namely instead of hashing the element,
the algorithm reads from an array a precomputed index number.
6.2 Performance Evaluation
In this section we show the performance evaluation results of our
prototype. All experiments were conducted on two Mac computers.
The server is a Mac Pro with 2 Intel E5645 6-core 2.4GHz CPUs,
32 GB RAM and runs Mac OS X 10.8. The client is a Macbook
Pro laptop with an Intel 2720QM quad-core 2.2 GHz CPU, 16 GB
RAM and runs Mac OS X 10.7. The two computers are connected
by 1000M Ethernet. The security settings of the experiments in this
and the next section are summarized in Table 2. In all experiments
we set BF/GBF parameter k = λ so the false positive probability
of a BF is at most 2−λ, we set m to be the optimal value kn log2 e.
For example, at 80-bit security k = λ = 80, and when n = 220,
m = 120795960. We use randomly generated int sets in the ex-
797❳
❳
❳
Set size
protocol
❳
❳
❳❳
210
212
214
80-bit Security
Huang’s(Java)
Our pipelined (Java)
Our parallel (Java)
De Cristofaro’s (C)
Our pipelined (C)
Our parallel (C)
Huang’s (Java)
Our pipelined (Java)
Our parallel (Java)
De Cristofaro’s (C)
Our pipelined (C)
Our parallel (C)
19
0.693
0.195
0.590
0.275
0.075
32
8.2
1.5
462
4.09
0.741
65
2.34
0.431
2.41
0.863
0.207
157
20.3
3.2
1850
8.94
1.53
216
2049
31.5
6.31
41.3
13.9
2.49
331
7.02
1.42
9.84
3.37
0.642
256-bit Security
733
68.44
10.5
7419
29.8
4.68
4647
313.4
54
29654
113
17.8
218
220
22853
110.6
25
159
54.0
9.49
43156
1298
215
118286
453
74.2
98468†
426
91
641
237
40.9
185570†
5421
1132
473144†
1852
339
)
B
M
(
n
o
i
t
p
m
u
s
n
o
c
h
t
d
w
d
n
a
B
i
14000
12000
10000
8000
6000
4000
2000
0
1024
De Cristofaro
Our
Huang
Set size
262144
All time shown in the table are in seconds.
† – estimated running time
Table 3: Performance comparison
periments. We measure the total running time of the protocol. The
measurement starts from the client sending the request and ends
immediately after the client outputting the intersection. The time
includes all operations such as building the Bloom ﬁlter, building
the garbled Bloom ﬁlter, the full OT extension protocol (including
the underlying Naor-Pinkas OT), data transmission, and the client-
side query for obtaining the intersection. We do not, however, in-
clude the time for initialization tasks, e.g. to generate random sets,
to interpret the command line arguments, and to setup sockets.
We ﬁrst show the performance of the prototype working in the
pipelined mode. In the pipelined mode, all computation on each
side is done in a single thread. We vary the set size (n) from 210 to
220 and security parameters (λ and k) from 80 to 256. The result is
shown in Figure 4a. We can see the running time increases almost
linearly in the set size at each security level. And for each increase
in security parameter, the running time increases only by a factor
of approximately 2. We also measured the time for each individual
step of the protocol. In the experiments, we ﬁx the set size (220)
and vary only security levels. The result is shown in Figure 4b. We
can see the protocol running time is dominated by the OT execu-
tion. This suggests that with a more efﬁcient OT protocol, the total
running time can be further reduced.
Then we show the performance of the parallel mode. In the par-
allel mode, we use multiple threads for computation. The result is
shown in Figure 4c. The total running time in the parallel mode is
much less than in the pipelined mode. At 80-bit security, million
elements set intersection can be done in 41 seconds. In the highest
security setting, the same computation can be done in 339 seconds
– that is less than 6 minutes. A comparison of the performance in
the two modes is shown in Figure 4d. The client has 4 cores and
the server has 12 cores, and we can see that the parallel mode is
about 5 times faster than the pipelined mode. This shows that our
protocol can fully take the advantage of the multicore architecture.
We believe the ability to easily scale up to multiple cores is a clear
advantage of our protocol and makes the protocol suitable for large
scale private data processing.
The performance of our protocol can even beat some inefﬁcient
plain algorithms in some settings. For example, Figure 4d shows
the time needed for a single threaded C program to compute the in-
tersection of two unencrypted random sets (n = 220) by pairwisely
comparing the elements. It needs 429 seconds in the best case when
C = S, and needs 844 seconds in the worst case when C ∩ S = ∅.
6.3 Performance Comparison
We compared the performance of our basic protocol against two
(a) Bandwidth Consumption: 80-bit security
x 104
De Cristofaro
Our
Huang
4
3.5
3
2.5
2
1.5
1
0.5
)
B
M
(
n
o