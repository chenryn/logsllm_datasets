Table 2. Experimental number of samples required to perform a full key recovery. The average CPU time for
a full key recovery is 40h on a Intel Xeon E5-2697v3 workstation.
PCA0+MiTM Spectral+Descent
s
s
e
r
p
m
o
c
o
/
w
s
s
e
r
p
m
o
c
/
w
BLISS-I
BLISS-II
BLISS-III
BLISS-VI
BLISS-I
BLISS-II
BLISS-III
BLISS-VI
180k
250k
209k
308k
4200k
27500k
2100k
unfeasible
65k
130k
100k
120k
700k
2000k
350k
200k
described in Section 2.2 with the MiTM technique of [25] to correct the errors. The second column
corresponds the Wirtinger flow technique coupled with the lattice reduction and the localization of
Section 2.4. Since the descent attack is an improvement build on a spectral method, it is natural to
see that this algorithm indeed requires far fewer samples to mount the attack than the first method
presented in Section 2.2. It should also be noticed that this attack discards every samples for which
t > 0, implying that a certain amount of the information provided by the samples is not used. For
instance when attacking BLISS-II with compression, almost 30 millions of samples are necessary
to retrieve the secret, but among those, only 18 millions of them are actually conserved to mount
the attack. The number of required samples may seems high compared to the dimension of the
problem, but it can be noticed that the size of the errors obtained by obtaining the estimation of
the phases by maximum likelihood is of the same magnitude as the actual phase we are trying
to retrieve. Hence, canceling the noise actually costs a significant amount of samples, as evoked
above.
As far as the correction of errors is concerned, with the two techniques introduced in Section 2.4
(i.e. the MiTM and the localization), the two attacks have different behaviors. Indeed, the MiTM
exhaustive search appeared to be more tailored to the first attack whereas the localization worked
far better for the descent attack. A more detailed discussion on the causes of this phenomena
is provided in Section 2.6 below. The results presented in Table 2 are obtained by making the
maximum use of these correction techniques. Hence, the running time of a full key recovery is
contributed almost exclusively by this final phase: practically the parameters given allows the
descent to yield a lattice problem in dimension at most 110. On a Intel Xeon E5-2697v3 workstation
this phase takes less than an hour to complete. Using the BKZ reduction with blocksize 25 takes
then around 38h to complete the recovery.
A striking observation is that in both of the attacks the compression on z2 used in actual BLISS
signatures, makes the recovery significantly harder: indeed, there is an order of magnitude between
the number of samples needed to make a full key recovery. Indeed the bit dropping yields noisier
estimates for the recovery problem. Finally, note that BLISS-II is the hardest variant to attack with
this method. This is due to the fact that this parameter set provides the highest rate of compression.
10
(a) Eigenvalue retrieval
(b) Descent technique
Fig. 9. Comparison of the repartition of the distance to the lattice 2Z.
2.6 Convergence behavior
In 9 we present the result of an experiment picturing the distance of each coefficient of the candidate
secret from the lattice 2Z before the final rounding, for both of the proposed attacks.
A striking observation is that the descent attack pushes way more the distances towards either
0 or 1 and as such makes it easy to localize the coefficients that are prone to be problematic.
Indeed setting a threshold at 0.5 clearly discriminates the “good” coefficients form the potentially
problematic ones. On the contrary, the situation is way more blurry in the other attack, where the
distances are much more close to 0.5. As such being able to distinguish the “good” coefficients from
the “bad” ones is much more difficult in order not to create false positives.
As a consequence, it is experimentally less costly to rely on MitM technique to resolve the errors
in this latter case as setting a threshold too low would imply reducing lattices of dimension too
large.
3 IMPLEMENTING BLISS IN CONSTANT TIME
In order to protect against timing attacks such as the one of Section 2 and most types of mi-
croarchitectural side-channel attacks (including [9, 22, 33]), it would be desirable to design an
implementation of BLISS that runs in constant time.
As noted in the introduction, doing so seems to present fundamental difficulties related to the
fact that the BLISS signing algorithm, in keeping with the Fiat–Shamir with aborts framework,
includes a probabilistic rejection sampling step that makes the running time intrisically vary from
one execution to the next. Moreover, the rejection probability computed at each step depends on
the secret key and the generated signature, so it may seem that secret-dependent branching is
unavoidable when implementing the scheme.
Fortunately, the problem is in fact crucial, because the distribution of the number of repetitions
in the signing algorithm is actually independent of all secrets. As a result, it is possible to aim for
an implementation that is constant time with public outputs, where the public outputs leak to the
adversary the number of repetitions. Since that number can be perfectly simulated independently
of the secret key, this is just as good as a truly constant time implementation.
In fact, although it is not really discussed in those terms, the same issue arises in existing “constant
time” implementations of Fiat–Shamir with aborts signature schemes such as the NIST second
round candidate Dilithium [18]. The main obstacle in implementing BLISS in constant time lies
11
elsewhere, in what forms the key difference between those two lattice-based schemes: BLISS’s
reliance on discrete Gaussian distributions, whereas Dilithium only uses uniform distributions,
with the explicit goal of avoiding side-channel vulnerabilities in the implementation.
The use of Gaussian distributions leads to two main implementation challenges: the constant
time implementation of Gaussian sampling, and that of the rejection sampling, corresponding to
Step 2 and Step 8 of Figure 2. In addition, some care must be taken regarding the implementation
of the ring-valued hash function from Step 4, as well as the sign flips in Steps 6 and 7. We describe
our implementation choices below and provide further technical details at the end of this section.
We note that most of these implementation techniques would apply equally well to other
Fiat–Shamir signatures schemes using Gaussian distributions, and in particular to the optimized
variant BLISS–B [15]. Regarding BLISS–B, the only subtle point is the computation of Sc, which
now involves sign flips and can no longer be carried out using an NTT; it is still easy if c is
considered non-sensitive (which is reasonable but requires additional assumptions), but becomes
significantly more expensive otherwise. We also note that our approach supports arbitrary Gaussian
standard deviations, which could in principle allow for more efficient parameter settings; to make
comparisons more meaningful, we did not attempt to select new parameters, but this could be
interesting further work.
3.1 Overview of our constant-time implementation
The main design goal of our implementation is to obtain a fast, constant-time implementation of
BLISS (focusing on the BLISS–I parameter set, which offers the best trade-off between security
and efficiency) while maintaining a high degree of portability. With the latter goal in mind, we
choose to rely entirely on integer arithmetic (limited to additions, multiplications and shifts on
32-bit and 64-bit operands). Indeed, division instructions and floating point operations rarely offer
constant-time execution guarantees,1 and they can present serious security challenges related to
weak determinism [17].
The ingredients needed to implement the signing algorithm are as follows: we need Gaussian
sampling for Step 2 of Figure 2; ring multiplication for Steps 3, 6 and 7; ring-valued hashing for Step 4;
and rejection sampling for Step 8. Other operations like constant-time sign flips, ring additions and
signature compression are straightforward. We now give a description of our implemention choices
for each of these steps. Note that in terms of efficiency, the critical elements are the Gaussian
sampling and the ring multiplication, with the ring-valued hashing also taking up a significant
amount of time. The other operations take negligible time in comparison.
3.1.1 Gaussian sampling. The Gaussian sampling step is key to obtaining a fast implementation
of BLISS, as it represents half or more of the computation time of signature generation: for each
signature, one needs to generate 1024 samples of the discrete Gaussian distribution Dσ (possibly
several times over, in case a rejection occurs), and the standard deviation is relatively large (σ = 205
for BLISS–I). This step has also been specifically targeted by cache timing attacks such as [9].
Several approaches can be considered for implementing it in constant time, but they have wildly
different running times. All approaches first generate samples from the non-negative Gaussian D +
σ ,
and then use a random sign flip (in constant time) to recover the entire distribution.
The most naive way would be to rely on cumulative distribution table (CDT) sampling: pre-
σ covering the inverval at the points
compute a table of the cumulative distribution function of D +
1Regarding floating point arithmetic, it is often variable time even in the presence of an FPU, and even for simpler operations
like multiplications. For example, the fmul multiplication instruction can have variable latency on several x86 architectures,
including the Intel Pentium III!
12
of which the distribution has a non-negligible probability2 ≳ 2−128; then, to produce a sample,
generate a random value in [0, 1] with 128 bits of precision, and return the index of the first entry
in the table greater than that value. In variable time, this can be done relatively efficiently with
a binary search, but this leaks the resulting sample through memory access patterns. As a result,
a constant time implementation has essentially no choice but to read the entire table each time
and carry out each and every comparison. Although a basic CDT implementation would store the
cumulative probabilities with 128 bits of precision, it is in fact possible to only store lower precision
approximations, as discussed in [34, 41] (see also [35] for an alternate approach using “conditional
distribution functions”). Nevertheless, since the table should contain σ(cid:112)2λ log 2 ≈ 2730 entries
for BLISS–I, we are looking at 22 kB’s worth of memory access for every generated sample. The
resulting implementation is obviously highly inefficient. Other table-based approaches like the
Knuth-Yao algorithm similarly suffer from constant time constraints.
A more efficient approach, originally introduced by Pöppelmann et al. [34] and later improved
and generalized by Micciancio and Walter [32], assumes that we can generate a base Gaussian
distribution D +
σ0 with not too small standard deviation σ0, and allows to then combine samples
from that base distribution to achieve larger standard deviations. For the parameters of BLISS–I,
(92+72)(32+22). One can then generate a sample
one can check that the optimal choice is to let σ
x statistically close to D +
σ0, as x = 9x0 + 7x1, where
xi = 3xi,0 + 2xi,1. Since σ0 ≈ 4.99 is much smaller than σ, using a CDT approach for the base
sampler is more reasonable: the CDT table now stores 63 entries. Generating a sample requires
reading through the table 4 times, for a total of 2 kB of memory access and 128 bits of randomness
per sample. It turns out, however, that the performance of the resulting implementation in our
setting is still somewhat underwhelming.
σ from 4 samples x0,0, x0,1, x1,0, x1,1 from D +
The authors of the qTesla3 second round NIST submission [1] proposed an ingenious approach
to improve constant-time CDT-based discrete Gaussian sampling. In practice, one needs to generate
many samples from the discrete Gaussian distribution in each signature (one for each coefficient
of the yi polynomials). The idea is then to batch all of the searches through the CDT table corre-
sponding to those samples. This can be done in constant time by applying an oblivious sorting
algorithm (e.g. network sorting) to the concatenation of the CDT with the list of uniform random
samples. This can be used in conjunction with the convolution technique of [32, 34] in order to
reduce the total size of the table to be sorted (which is the sum of the CDT size and of the desired
number of samples). Preliminary attempts to use this approach in the case of BLISS did not result
in compelling performance numbers, but there is likely room for improvement in terms of the
oblivious sorting algorithm involved as well as the way is algorithm is combined with various
optimization tricks: detailed investigation of this question is left as interesting further work.
2
0 =
σ 2
Finally, yet another strategy is to generate a discrete Gaussian of very small standard deviation,
use it to construct a distribution that looks somewhat like D +
σ but is not statistically close, and
use rejection sampling to correct the discrepancy. This is actually the approach taken in the
original BLISS paper [16]. Concretely, what that paper essentially does is sample some x from the
σ2 where σ2 = σ/k, and some y uniform in {0, . . . , k − 1}. Then, z = kx + y looks
distribution D +
σ , and one can check that rejecting z except with probability
“somewhat like” a sample from D +
σ . As observed in the BLISS paper,
this rejection sampling step is exactly of the same form as the one used for the overall signing
algorithm. The constant time implementation of that step is described in Section 3.1.4 below, and
we can simply reuse that work to obtain our Gaussian sampling. The only ingredient to add is
2)(cid:1) yields a value that actually follows D +
exp(cid:0)−y(y + 2kx)/(2σ
2Even taking Rényi divergence arguments into account, values taken with probability ≥ 2−117 should included.
3While qTesla uses uniform randomness during signature generation, it does use discrete Gaussians for key generation.
13
a base sampler for the distribution D +
σ2, since the one in the original BLISS paper does not lend
itself to a convenient constant time implementation. Fortunately, choosing k = 256, the standard
deviation σ2 ≈ 0.80 is really small, and hence a CDT approach only requires 10 table entries. In
practice, this yields a Gaussian sampling of very reasonable efficiency, whose cost is dominated
by the cost of the rejection sampling step, and of the generation of the uniform randomness. This
is the approach we choose for our implementation. Its security directly follows from that of the
rejection sampling (see Section 4 for technical details).
3.1.2 Ring multiplications. As usual in ideal lattice-based schemes, ring multiplications such as
the one in Step 3 of Figure 2 are carried out using the number-theoretic transform (NTT). Since the
NTT does not use any secret-dependent conditional branches or memory accesses, constant-time
implementation does not pose any particular difficulty. In our case, we directly adapt the NTT from
the reference implementation of Dilithium, which uses the bit-reversed order for coefficients in the
NTT domain, lazy modular reductions, and the Montgomery representation for values modulo q.
Only a few simple changes are needed compared to Dilithium, in order to account for the different
modulus q = 12289 and the higher degree n = 512 (instead of q = 8380417 and n = 256 respectively).
At the cost of more frequent modular reductions, we could do the entire computation on 16-bit
integers (which could yield to faster automatic vectorization), but for simplicity, we keep the 32-bit
arithmetic from the Dilithium NTT.
The implementation choice for the ring multiplications in Steps 6 and 7 of Figure 2 is less obvious.
Indeed, those steps involve the multiplication of the secret key elements, which are small, by the
hash value c, which has 23 coefficients equal to 1, and the others equal to 0. Moreover, we can
show that, under a non-standard but reasonable LWE-like assumption, BLISS remains secure even
when u and hence c are made public (including for rejected instances). It would therefore not
jeopardize security to implement the multiplications s1c and s2c as repeated additions of shifted
versions of s1 and s2, where the memory access patterns in the shifts reveal the coefficients of c (but
nothing about the secret key vectors themselves). Interestingly, however, it turns out that, at least
on our target platform, implementing the multiplications that way is not faster than using the NTT,
probably because the NTT has a much better cache locality. As a result, all ring multiplications in
our implementation simply use the NTT.
3.1.3 Ring-valued hashing. Step 4 of the signing algorithm in Figure 2 computes the “challenge”
ring element c = H(⌊u⌉d mod q, µ) from the “commitment” u and the input message µ. That ring
element should be a polynomial uniformly sampled among those with κ = 23 coefficients equal to
1, and all other coefficients equal to 0. To construct such a polynomial, we first pass the inputs of H
to an extendable output function (XOF), in our case SHAKE128, and then use the resulting random
stream to sample the list (i1, . . . , iκ) of indices in c equal to 1.
Concretely speaking, we again follow Dilithium’s approach, which proceeds as follows. We pick
i1 uniformly in {0, . . . , n − κ}. Then i2 is chosen uniformly in {0, . . . , n − κ + 1}, and if it happens to
collide with i1, it is set to n−κ +1 instead. Continuing, ik is chosen uniformly in {0, . . . , n−κ−1+k},
and replaced by n − κ − 1 + k if it coincides with one of the previous values. It is easy to check that
{i1, . . . , iκ} is then a uniformly distributed κ-element subset of {0, . . . , n − 1} as required.
However, Dilithium’s implementation of this strategy is not in fact constant-time, as it works
by updating an n-element array and modifying the elements at indices ik and n − κ − 1 + k for
each k. As a result, the algorithm leaks the entirety of c through memory accesses. This is not a
critical problem, since as we have mentioned, the values u and hence c are not really sensitive in
BLISS (security is still achieved for the variant in which those values are revealed, albeit under