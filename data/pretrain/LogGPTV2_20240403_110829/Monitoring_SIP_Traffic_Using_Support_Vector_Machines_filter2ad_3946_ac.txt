### Kernel Performance and Parameters

The following table summarizes the performance of different kernels (Linear, Polynomial, Sigmoid, and RBF) with their respective parameters. The accuracy is defined as the percentage of correctly classified messages over all test results, and the machine running time is averaged over ten runs.

| Kernel | Parameters | Accuracy (%) | Time (ms) |
|--------|------------|--------------|-----------|
| Linear | C = 1      | 99.79        | 196       |
| Polynomial | C = 1; γ = 1/38; r = 0; d = 3 | 79.09 | 570 |
| Sigmoid | C = 1; γ = 1/38; r = 0 | 93.83 | 994 |
| RBF | C = 1; γ = 1/38 | 98.24 | 668 |
| Linear | C = 2 | 99.83 | 157 |
| RBF | C = 2; γ = 0.5 | 99.83 | 294 |

The linear and RBF kernels generally perform better in terms of both accuracy and execution time, especially when the number of features is significantly higher than the number of instances or if both are large. These kernels were tested with default parameters on our dataset, and the results are shown in Table 4. The last two lines of the table show the performance after parameter selection.

### Size of SIP Slice Experiment

The size of the analyzer window is a critical parameter in feature evaluation. This experiment evaluates the impact of changing the window size on the accuracy and analysis time. The results, obtained using a 5-fold cross-validation with an RBF kernel and default parameters, are shown in Table 5.

| Window Size | Accuracy (%) | Analysis Time (min) |
|-------------|--------------|---------------------|
| 120         | 95.4         | 1.12                |
| 150         | 99.32        | 2.40                |
| 300         | 99.30        | 2.56                |
| 450         | 99.67        | 4.31                |
| 600         | 99.63        | 6.39                |
| 750         | 100          | 7.42                |
| 900         | 100          | 8.51                |

As expected, larger window sizes improve accuracy but increase analysis time, which is critical for online monitoring.

### Feature Selection

The 38 features used in the experiments were chosen based on domain-specific knowledge. To evaluate the relevance of these features, we conducted experiments where we gradually excluded less important features. The results, using a 5-fold cross-validation with an RBF kernel and default parameters, are shown in Table 6.

| # of Features | Accuracy (%) | Machine Time (s) |
|---------------|--------------|------------------|
| 38            | 99.30        | 1.85             |
| 31            | 99.39        | 1.59             |
| 18            | 98.90        | 1.42             |
| 12            | 98.65        | 1.28             |
| 7             | 98.22        | 0.57             |

Although there is a sudden drop in accuracy between 12 and 7 features, the overall trend is not strictly decreasing, suggesting that some features may have dependencies.

### Detection of Flooding Attacks

We used the Inviteflood tool to launch SIP flooding attacks with invalid domain names, generating five attacks at different rates. Each attack lasted one minute and was injected into a normal trace of two hours, starting five minutes after the trace began. The system was trained with a mixed trace (flooding at 100 INVITE/s - normal trace), and the detection accuracy was calculated as the percentage of vectors correctly classified as attack over all vectors of the attack period.

| Flooding Rate (INVITE/s) | Detection Accuracy-1 (%) | Detection Accuracy-2 (%) | Pr(Normal) | Pr(Attack) |
|--------------------------|--------------------------|--------------------------|------------|------------|
| 0.5                      | 5.47                     | 0                        | 0.96       | 0.04       |
| 10                       | 67.57                    | 1.48                     | 0.95       | 0.05       |
| 100                      | 97.36                    | 30.13                    | 0.73       | 0.27       |
| 1000                     | 100                      | 88.82                    | 0.24       | 0.76       |
| 10000                    | 100                      | 98.24                    | 0.07       | 0.93       |

The results show that the SVM can be fine-tuned for adaptive online monitoring against flooding attacks.

### Detection of SPIT Attacks

SPIT mitigation is a significant challenge in VoIP security. We used the Spitter/Asterisk tool to generate SPIT calls and profiled programmable bots to receive these calls. The bots responded randomly with three different responses: ringing and picking up, responding with 'Busy', or redirecting the call.

#### Partial SPIT Experiment

In this experiment, Spitter targeted the proxy with 100 destinations, but only 10 were registered bots, resulting in a hit rate of 10%. Four campaigns with different numbers of concurrent calls were sent, and the traces were analyzed using a 30-message slice.

| # of Concurrent Calls | True Positives (%) | True Negatives (%) |
|-----------------------|--------------------|--------------------|
| 1                     | 0 (0/3697)         | 0 (0/3697)         |
| 10                    | 1.30 (10/766)      | 2.09 (16/766)      |
| 50                    | 10.01 (62/619)     | 10.66 (66/619)     |
| 100                   | 18.31 (102/557)    | 19.39 (108/557)    |

#### Full SPIT Experiment

In this experiment, Spitter targeted the proxy with 100 destinations, all of which were registered bots, resulting in a hit rate of 100%.

| # of Concurrent Calls | True Positives (%) | True Negatives (%) |
|-----------------------|--------------------|--------------------|
| 1                     | 0.03               | 0.03               |
| 10                    | 3.05               | 3.05               |
| 50                    | 12.18              | 12.18              |
| 100                   | 23.41              | 23.41              |

With the help of deterministic event correlation rules, our online monitoring system can efficiently detect SPIT attacks.

### Conclusion

The results indicate that the RBF and linear kernels provide the best performance in terms of accuracy and execution time. The size of the analyzer window and feature selection are crucial for optimizing the system. Additionally, the SVM-based approach shows promise for detecting and mitigating both flooding and SPIT attacks in VoIP environments.