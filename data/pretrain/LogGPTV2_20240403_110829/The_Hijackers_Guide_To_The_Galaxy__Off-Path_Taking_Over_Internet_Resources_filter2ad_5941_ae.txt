-
-




 

    
-

    
-


  
    


    
    
-
 



    
    
    
-

-


    
    

  
    
  



/23
/24
/24
/22
/20
/20
/12
/12-/23
/19-/21
/12-/23
/16-/21
/12-/21
/23
/16-/19
/16-/22
/18-/24
/12-21
/13-19
/16-/21
/16-/22
/12-/23
/12-/19
/17-/23
/17-/22
/17
/16
/19-/21
/18-/24
/18-/20
/17-/22
/15
/16-/22
/12-/23
/19-/21
/20



































e
t
a
d
i
l
a
v







(1)






















(1)
(1)



EDNS
size
4096
4096
4096
1280
4096
4096
1232
512 (2)
4096
4096
4096
4096
4096
512 (2)
1232
4096
4096
512 (2)
4096
1232
1232
4096
1372
4096
4096
1220
1410
4096
4096
4096
4096
4096
1232
4096
4096
Table 2: Measurement study of provider’s DNS resolvers and Email servers. (1): Could not test. (2): No EDNS. (3): No Email after sign-up. -: Does not apply.
Account
Provider
# Resources
Total
found # Acc-
e-mail
ounts
Scanned resources
Vulnerable to
BGP
Sad-
sub same DNS
FragDNS
any global
Vulnerable Accounts
75%
RIRs
Regis- 100,000 10,597 7,450
trars
n/a 8,469 14,136 1,193
92,857 69,287 87,547 47,840
56% n/a 11% 17% 1.5%
3,308
85
45% n/a 10% 21% 1.2%
n/a 9,135 15,696 1,278
56% 80% 11% 17% 1.4%
192,857 79,884 94,997 51,148
666 1,560
Both
11%
41%
n/a
Vulnerable resources
IP Addresses
AS Numbers
Domains
81% n/a 30% 51% 21%
60% n/a 12% 20% 3%
47% n/a 10% 27% 1%
Attack success probability
Success probability
100% 60% 0.2% 0.1% 20%
Table 3: Customer-side vulnerability data
(62%), nameservers always return the same DNS response
(with the same records and sorted in the same order); see
Algorithm 1: Predictability of records in responses.
for each (domain, nameserver) do
initialise set of different DNS responses as empty;
for batch = 1,2, . . . ,25 do
for iteration = 1,2,3,4 do
send the same DNS request;
if new response arrived then
add the new response to the response set;
end
if no new responses in last batch then
end
break;
end
end
record number of different DNS responses;
end
Figure 5. For our measurement of the IPID allocation meth-
ods supported by the nameservers of the customers we use
the following methodology. We issue queries from two hosts
(with different IP addresses). Data per nameserver is listed in
USENIX Association
30th USENIX Security Symposium    3157
Figure 5: CDF of number of observed DNS MX responses per customer
email address domain (each nameserver was queried 100 times).
Table 4. Our measurements show that 290 vulnerable name-
servers (4.88%) use a globally incremental IPID assignment.
The computation of the IPID allocation for each domain are
described in pseudocode in Algorithm 2.
Algorithm 2: IPID allocation in nameservers.
for each (domain, nameserver) do
for batch = 1,2,3,4 do
send DNS request from Prober1;
record IPID in DNS response as IPID2∗i−1;
send DNS request from Prober2;
record IPID in DNS response as IPID2∗i;
end
if IPIDi,i = 1,2, . . . ,8 is incrementing then
globally incrementing;
end
if IPIDi,i = 1,3,5,7 or IPIDi,i = 2,4,6,8 is incrementing then
per-dest incrementing;
end
if IPIDi == 0,i = 1,2, . . . ,8 then
zero;
else
end
end
All
Frag
random and other;
Random
and other
Zero
N/A
Per-Dest Global
Total
64.58% 8.31% 4.89% 11.92% 10.30% 100%
45308
70158
53.96% 4.88% 13.75% 23.67% 3.74% 100%
5941
3206
5829
3434
8364
1406
7223
290
817
222
Table 4: IPID allocation of all nameservers and of fragmenting nameservers.
We automate the attack in Section 4.5 and execute the
entire FragDNS attack against all the vulnerable customer do-
mains, by injecting malicious records mapping Email servers
of customers to an IP address of our adversarial host. Our
evaluation combines the data we collected on DNS records in
responses (randomisation of the DNS records or of their order
in responses) and the IPID allocation of the nameservers. We
also used Algorithm 2 to estimate the IPID increment rate,
by recording the timestamp of each response and calculating
the average increment rate of IPID value. We then extrapolate
the value of IPID and calculate the probability of our adver-
sary to correctly place at least one out of 64 fragments3 with
the matching IPID in the resolver’s defragmentation cache.
We use different values for IPID increment rate and delay
364 fragments is the minimal size of the IP-defragmentation cache.
Figure 6: Reverse CDF to correctly guess the IPID for all customers’ domains.
between the query, which probes the IPID value, and the IPID
value that was de-facto assigned to the DNS response by the
nameserver. Results are plotted in Figure 6. For example, the
IPID prediction success rate is over 10% for roughly 3% of
RIPE, 2% of ARIN and 1% of 100K-top Alexa customers.
Success rates for ARIN and 100K-top Alexa customers are
lower mostly because of the higher latencies of those, see Fig-
ure 2. For nameservers which do not use globally incremental
IPID, we assume a hitrate of 64/216 which is achieved by just
randomly guessing the IPID.
The probability to compute the correct checksum is capped
at a minimum of 1/216 in case of nameservers which generate
responses with different records or with random ordering of
records. Finally the probabilities to correctly compute both,
the IPID value and the order of records to get the correct UDP
checksum, are multiplied resulting in the combined hitrate.
Our automated attack against all the customers shows that
around 2% of the domains (5 for RIPE, 17 for ARIN) have a
success probability higher than 10%. Furthermore, for about
20% of the domains, success probability is over 0.1% which
is a consequence of non-predictable IPID allocation and the
stable DNS records in responses generated by these domains.
When the DNS response can be predicted, even with a random
IPID allocation method, an attacker has a hitrate of about
64/216 ≈ 0.1%. At this hitrate, when the attacker performs
the attack multiple times, the probability to conduct the attack
successfully at least once is 50% at around 700 repetitions.
Our automated evaluation provides a lower bound for suc-
cessful attacks against a randomly chosen domain – this is a
worst case analysis since it also considers domains which are
much more difﬁcult to attack, e.g., since they use servers with
random IPID allocation, servers with high trafﬁc rate, and
servers which return different number and order of records in
responses. Adjusting the attack parameters manually against
a given victim customer domain results in a much higher at-
tack rate. Furthermore, against many customer domains with
low trafﬁc volume, incremental IPID values and ﬁxed number
and order of DNS records, the attacker can reach above 90%
success rate.
6 Manipulation of Digital Resources
In this section we demonstrate exploits that the adversary can
perform when controlling an account of a (victim) customer.
Most of the actions are similar across the providers, even
providers of different infrastructure, such as RIRs and the
3158    30th USENIX Security Symposium
USENIX Association
50%60%70%80%90%100% 10 20 30 40 50 60 70 80 90 100DomainsNumber of different DNS responses (MX)0%2%4%6%0.1%1%10%100%DomainsIPID Hitrate, minimum is 2-16 ≈ 0.1%ARINRIPEOpen ResolverAlexa 100Kl
a
n
o
t
i
d
d
A
n
o
i
t
a
d
i