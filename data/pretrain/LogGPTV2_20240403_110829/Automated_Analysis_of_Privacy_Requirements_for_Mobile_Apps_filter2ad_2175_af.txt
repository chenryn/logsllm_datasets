### 2.5 Analysis of F-1 Scores and Precision

The F-1 scores for our analysis, which range from 0.7 to 1, indicate a generally reliable identification of potential inconsistencies. While these results are promising, the precision values (P<sub>recpos</sub>) for certain practices are lower than desired. Specifically, for the CL practice, we obtained a precision of 0.54, and for the CID practice, the precision was 0.75, which is lower than we had hoped.

These results highlight a broader issue: false positives occur because our analysis considers too many APIs that are only occasionally used for the specific data practice in question. Although we believe it is better to err on the side of false positives, especially in an auditing system [35], in hindsight, we might have excluded some APIs to improve precision.

Conversely, for the SID practice, we included too few relevant APIs, leading to under-inclusiveness. This underscores the challenge of identifying a set of APIs that captures the majority of cases for a practice without being over- or under-inclusive.

### Potential Inconsistencies for Full App/Policy Set

As indicated by the high percentages in Table V, we identified potential inconsistencies on a widespread basis. Specifically, the collection of device IDs and locations, as well as the sharing of device IDs, are practices that are potentially inconsistent for 50%, 41%, and 63% of apps, respectively. However, given the relatively low precision and high recall for these practices, we caution that these percentages may be overestimations.

It is noteworthy that for the sharing of location and contact information, nearly every detection of the practice is associated with a potential inconsistency. For the 20% of apps that share location information (per Table IV), almost all (17%, per Table V) do not properly disclose such sharing. Similarly, for the 2% of apps that share contact data, only a handful provide sufficient disclosure.

### Significant Variables for Predicting Non-Compliance

Table VI presents significant variables for predicting apps' potential non-compliance with at least one privacy requirement, evaluated on our full app/policy set (n=9,050). The top developer and editor’s choice badges are assigned by Google. Interactive elements and unrated content refer to the respective ESRB classifications [24]. "Pos" designates the percentage of positive cases (e.g., 100% of apps have an overall score), OR is the odds ratio, and the 95% CI is the profile likelihood confidence interval.

The average number of 1.83 potential inconsistencies per app is high compared to previous studies, which reported 0.62 (113/182) cases of stealthy behavior [41], 1.2 (24/20) potential privacy violations [22], and 0.71 (341/477) [62]. Figure 6 provides detailed results. Notably, for apps without a policy, essentially every data collection or sharing practice causes a potential inconsistency. For example, all 62% of apps without a policy that share device IDs (Table IV) are potentially non-compliant. Overall, our results suggest a broad level of potential inconsistency between apps and policies.

### B. Potential Inconsistencies for Groups of App/Policy Pairs

Analyzing individual apps for potential non-compliance at scale is resource-intensive. Therefore, it is valuable to first estimate the potential non-compliance of an app population as a whole before performing individual analyses. We suggest systematically exploring app metadata for correlations with potential inconsistencies based on statistical models. This macro analysis complements individual app analysis and highlights areas of concern for privacy activists.

To illustrate this, we evaluated a binary logistic regression model that determines the dependence of whether an app has a potential inconsistency (the dependent variable) on six Play store app metadata variables (the independent variables). Our results, shown in Table VI, demonstrate correlations at various statistical significance levels, with p-values ranging from 0.0001 to 0.08. Particularly, an increase in the number of user ratings decreases the probability of potential inconsistencies. There is also a decreasing effect for apps with a badge and for apps whose content has not yet been rated.

Interestingly, apps with higher overall Google Play store scores do not have lower odds for potential inconsistencies. In fact, the opposite is true. An increase in the overall score increases the odds of a potential inconsistency. For example, increasing the overall score by one unit (e.g., from 3.1 to 4.1 on a scale of 1 through 5) increases the odds by a factor of 1.4. This could be because highly rated apps often provide functionality and personalization based on user data, whose processing is insufficiently described in their privacy policies. Additionally, users do not seem to rate apps based on privacy considerations; the word "privacy" appears in only 1% (220/17,991) of all app reviews.

Beyond the app's score, the odds for a potential inconsistency also increase for apps featuring in-app purchases or interactive elements. Including category information in our model reveals that the odds significantly (p ≤ 0.05) increase for apps in the Finance, Health & Fitness, Photography, and Travel & Local categories, while they decrease for apps in the Libraries & Demo category.

To evaluate the overall model fit, we checked whether the model with independent variables (excluding category variables) had a significantly better fit than a null model (intercept only). A Chi-square value of 151.03 with six degrees of freedom and a p-value ≤ 0.001 indicates that our model has a significantly better fit than the null model. Figure 7 illustrates the predicted probabilities. Apps with a Top Developer or Editor’s Choice badge have a nearly 10% lower probability of a potential inconsistency, and this probability further decreases with more user ratings for both apps with and without a badge.

### VI. Case Study: Evaluating Our System for Use by the Cal AG

We worked with the California Attorney General (Cal AG) to evaluate our system's capabilities for supplementing the enforcement of CalOPPA [12]. To this end, we implemented a custom version of our system (§ VI-A) and added various new analysis features (§ VI-B). The feedback we received is encouraging and confirms that our system could enable regulators to achieve more systematic enforcement of privacy requirements (§ VI-C).

#### A. System Implementation

As shown in Figure 8, we implemented our system for the Cal AG as a web application. It allows users to request analyses for individual apps and supports batch processing. For scalability, we leveraged AWS EC2 t2.large instances with up to 3.0 GHz Intel Xeon, 2 vCPU, and 8 GiB memory [2].

The system's graphical user interface uses the Flask Python web framework [58] running on an Apache web server [64] with the Web Server Gateway Interface module [21]. All analysis requests are added to a Celery task queue [5], which communicates with the Flask application using the RabbitMQ message broker [52]. Once an analysis is complete, the results are loaded by the Flask application and displayed in the users' browsers.

Similar to our original system, all APKs are downloaded via Raccoon [50], and apps' privacy policy links are retrieved from their respective Play store pages. To download the websites linked to these policies, we automated a Firefox browser with Selenium [60] and PyVirtualDisplay [53]. Using a real browser is advantageous as it can obtain policies that are dynamically loaded via JavaScript.

After downloading the website with the privacy policy, any non-policy elements, such as page navigation, are removed. The system then runs our feature extraction routines (§ III-C2) and ML classifiers (§ III-C3) on the policy and performs static analysis (§ IV) on the downloaded APK. Finally, the results are displayed to the user, with flags raised for potential inconsistencies.

#### B. Additional Functionality

We placed high emphasis on usability from both legal and human-computer interaction perspectives. For instance, the Cal AG users were interested in additional information, such as the breakdown of third parties in sharing practices. The initial version of our system's report simply showed that the user's contact and device ID were shared, without specifying the third parties involved. Distinguishing which type of information is shared with which third party is important under CalOPPA because the sharing of contact information makes a stronger case than the sharing of device IDs, for example [28].

Given its importance, we implemented additional contact information functionality. Because we believe that the relatively low detection rate for the collection and sharing of contact information is due to manual entry or other sources, we enhanced our system. Specifically, we leveraged the Facebook Login library, which gives apps access to a user's name and Facebook ID [25]. The system detects the usage of the Facebook Login library by extracting the app's manifest and resource files with Apktool [65] and searching for required signatures, such as a dedicated activity or intent filter, a Login button in the layout, and the invocation of initialization, destruction, or configuration routines.

Another feature we added is the download of privacy policies linked from within apps. Our initial policy crawler was limited to downloading policies via an app's Play store page. As the Cal AG provided guidance to app publishers for linking to the policy from both the Play store and within the app [11], our new approach covers both possibilities. The system finds the links in an app by extracting strings from the APK file using Apktool and then extracting URLs containing relevant keywords, such as "privacy." If a policy link inside an app differs from the app's Play store policy link or if there are multiple policy links identified within an app, our system downloads and analyzes all documents retrieved from these links.

#### C. System Performance

The Cal AG users reported that our system has the potential to significantly increase their productivity. Given their limited resources, it can guide them on areas to focus on, such as specific categories of apps. They can also reduce effort and time spent analyzing practices in apps for which our system does not find potential inconsistencies, allowing them to concentrate on apps with raised flags. Additionally, the Cal AG users found our system useful for estimating the current overall state of CalOPPA compliance. For example, the analysis results alerted them to some app policies that use vague language in describing their collection and sharing practices.

We evaluated our system implementation for the Cal AG, and the results are summarized in Table VII.