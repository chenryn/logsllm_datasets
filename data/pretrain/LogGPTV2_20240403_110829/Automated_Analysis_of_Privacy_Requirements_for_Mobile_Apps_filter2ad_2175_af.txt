(TN).25
The F-1pos scores for our analysis, ranging from 0.7
to 1, indicate the overall reliable identiﬁcation of potential
inconsistencies. While we think that these results are generally
promising, we obtain a relatively low precision value of
P recpos = 0.54 for the CL practice and for the CID practice
we had hoped for a higher precision than P recpos = 0.75 as
well. These results illustrate a broader point that is applicable
beyond those practices. False positives seem to occur because
our analysis takes into account too many APIs that are only
occasionally used for purposes of the data practice in question.
Despite our belief that it is better to err on the side of false
positives, which is especially true for an auditing system [35],
in hindsight we probably would have left out some APIs.
The opposite problem seems to occur in the SID practice. We
included too few relevant APIs. In this regard, we acknowledge
the challenge of identifying a set of APIs that captures the bulk
of cases for a practice without being over- or under-inclusive.
Potential Inconsistencies for Full App/Policy Set. As indi-
cated by the high percentages shown in Table V, we identiﬁed
potential inconsistencies on a widespread basis. Speciﬁcally,
collection of device IDs and locations as well as sharing of
device IDs are practices that are potentially inconsistent for
50%, 41%, and 63% of apps, respectively. However, given the
relatively low precision and high recall for these practices, we
caution that their percentages might be an overestimation. It
is further noteworthy that for sharing of location and contact
information nearly every detection of the practices goes hand
in hand with a potential inconsistency. For the apps that share
location information (20%, per Table IV) nearly all (17%, per
Table V) do not properly disclose such sharing. Similarly, for
the 2% of apps that share contact data only a handful provide
sufﬁcient disclosure.
25Appendix B describes details of calculating true and false positives and negatives.
11
Variable
No. User Ratings
Overall Score
Badge
In-app Purchases
Interactive Elm
Content Unrated
p value
Pos
0.0001
100%
100% <0.0001
<0.0001
21%
27%
45%
5%
<0.0001
0.002
0.08
OR
0.9
1.4
0.57
1.15
1.33
0.68
95% CI
0.9999998–0.9
1.24–1.57
0.49–0.65
0.99–1.34
1.17–1.53
0.53–0.87
TABLE VI: Signiﬁcant variables for predicting apps’ potential
non-compliance with at least one privacy requirement as eval-
uated on our full app/policy set (n=9,050). Top Developer and
Editor’s Choice badges are assigned by Google. Interactive
elements and unrated content refer to the respective ESRB
classiﬁcations [24]. Pos designates the percentages of positive
cases (e.g., 100% apps have an overall score), OR is the odds
ratio, and the 95% CI is the proﬁle likelihood CI.
The average number of 1.83 potential inconsistencies per
app is high compared to the closest previous averages with
0.62 (113/182) cases of stealthy behavior [41] and potential
privacy violations of 1.2 (24/20) [22] and 0.71 (341/477) [62].
Figure 6 shows details of our results. In this regard, it should
be noted that for apps without a policy essentially every data
collection or sharing practice causes a potential inconsistency.
For example, all 62% apps without a policy that share device
IDs (Table IV) are potentially non-compliant. Thus, overall our
results suggest a broad level of potential inconsistency between
apps and policies.26
B. Potential Inconsistencies for Groups of App/Policy Pairs
Analyzing individual apps for potential non-compliance at
scale is a resource-intensive task. Thus, it is worthwhile to
ﬁrst estimate an app population’s potential non-compliance
as a whole before performing individual app analyses. Our
suggestion is to systematically explore app metadata for cor-
relations with potential
inconsistencies based on statistical
models. This broad macro analysis supplements the individual
app analysis and reveals areas of concern on which, for
example, privacy activists can focus on. To illustrate this idea
we evaluate a binary logistic regression model that determines
the dependence of whether an app has a potential inconsistency
(the dependent variable) from six Play store app metadata
variables (the independent variables). Our results, shown in
Table VI, demonstrate correlations at various statistical sig-
niﬁcance levels with p values ranging from 0.0001 to 0.08.
Particularly, with an increase in the number of user ratings the
probability of potential inconsistencies decreases. There is also
a decreasing effect for apps with a badge and for apps whose
content has not yet been rated.
Interestingly, apps with higher overall Google Play store
scores do not have lower odds for potential inconsistencies.
In fact, the opposite is true. With an increase in the overall
score the odds of a potential inconsistency become higher.
An increase of the overall score by one unit, e.g., from 3.1
to 4.1 (on a scale of 1 through 5), increases the odds by
a factor of 1.4. A reason could be that highly rated apps
provide functionality and personalization based on user data,
26As we are evaluating our system for use in privacy enforcement activities (§ VI) we
decided to abstain from contacting any affected app publishers of our ﬁndings.
y
t
i
l
i
b
a
b
o
r
P
d
e
t
c
d
e
r
P
i
0.85
0.80
0.75
0.70
Badge
No
Yes
0
250K
500K
750K
1M
Number of User Ratings
Fig. 7: In our logistic model the predicted probability of an app
having a potential inconsistency is dependent on the number
of user ratings and assignment of a badge. The overall score is
held at the mean and in-app purchases, interactive elements,
and unrated content are held to be not present. The shaded
areas identify the proﬁle likelihood CIs at the 95% level.
whose processing is insufﬁciently described in their privacy
policies. Also, users do not seem to rate apps based on
privacy considerations. We found the word “privacy” in only
1% (220/17,991) of all app reviews. Beyond an app’s score,
the odds for a potential inconsistency also increase for apps
that feature in-app purchases or interactive elements. Also,
supplementing our model with category information reveals
that the odds signiﬁcantly (p ≤ 0.05) surge for apps in the
Finance, Health & Fitness, Photography, and Travel & Local
categories while they decrease for apps in the Libraries &
Demo category.
In order to evaluate the overall model ﬁt based on statistical
signiﬁcance we checked whether the model with independent
variables (omitting the category variables) had signiﬁcantly
better ﬁt than a null model (that is, a model with the intercept
only). The result of a Chi square value of 151.03 with six
degrees of freedom and value of p ≤ 0.001 indicates that our
model has indeed signiﬁcantly better ﬁt than the null model. To
see the impact of selected aspects of the model it is useful to
illustrate the predicted probabilities. An example is contained
in Figure 7. Apps with a Top Developer or Editor’s Choice
badge have a nearly 10% lower probability of a potential
inconsistency. That probability further decreases with more
user ratings for both apps with and without badge.
VI. CASE STUDY: EVALUATING OUR SYSTEM FOR USE
BY THE CAL AG
We worked with the Cal AG, to evaluate our system’s capa-
bilities for supplementing the enforcement of CalOPPA [12].
To that end, we implemented a custom version of our system
(§ VI-A) and added various new analysis features (§ VI-B).
The feedback we received is encouraging and conﬁrms that
our system could enable regulators to achieve more systematic
enforcement of privacy requirements (§ VI-C).27
A. System Implementation
As shown in Figure 8, we implemented our system for the
Cal AG as a web application. It allows users to request analyses
for individual apps and also supports batch processing. For
27We continue our work with the Cal AG beyond this study and expect further results.
Fig. 8: Our system allows users to analyze apps for potential
privacy requirement non-compliance. An app can be subject to
multiple privacy policies—for example, one policy from inside
the app and one from the app’s Play Store page. In these cases
the app is checked against multiple policies.
scalability reasons we chose to leverage AWS EC2 t2.large
instances with up to 3.0 GHz Intel Xeon, 2 vCPU, and 8 GiB
memory [2].
The system’s graphical user interface applies the Flask
Python web framework [58] running on an Apache web
server [64] with Web Server Gateway Interface module [21].
All analysis requests are added to a Celery task queue [5],
which communicates with the Flask application using the
RabbitMQ message broker [52]. Once an analysis is ﬁnished
the results are loaded by the Flask application and displayed
in the users’ browsers.
Similar as in our original system, all APKs are downloaded
via Raccoon [50] and apps’ privacy policy links are retrieved
from their respective Play store pages. However, in order to
download the websites that the links are leading to we auto-
mated a Firefox browser with Selenium [60] and PyVirtualD-
isplay [53]. Using a real browser instead of simply crawling
the HTML of the privacy policy pages is advantageous as it
can obtain policies that are loaded dynamically via JavaScript.
12
After the website with the privacy policy is downloaded
any elements that are not part of the policy, such as page
navigation elements, are removed. The system then runs our
feature extraction routines (§ III-C2) as well as ML classiﬁers
(§ III-C3) on the policy and the static analysis (§ IV) on the
downloaded APK. Finally, the results are displayed to the user
with ﬂags raised for potential inconsistencies.
B. Additional Functionality
We placed high emphasis on usability from both a legal
and human-computer interaction perspective. Notably, in some
cases the Cal AG users were interested in receiving additional
information. For instance, one additional piece of information
was the breakdown of third parties in the sharing practices.
The initial version of our system’s report simply showed
that the user’s contact and device ID were shared, however,
without disclosing that those were shared with, say, InMobi
and Crashlytics. Distinguishing which type of information is
shared with which third party is important under CalOPPA
because the sharing of contact information makes a stronger
case than the sharing of device IDs, for example.28
information (§ V-A) is due to the fact
Given its importance we implemented additional contact
information functionality. Because we believe that the rela-
tively low detection rate for the collection and sharing of
contact
that such
information is often manually entered by the user or obtained
via other sources, we enhanced our system in this regard.
Particularly, we leveraged the Facebook Login library that
gives apps access to a user’s name and Facebook ID [25].
The system detects the usage of the Facebook Login library in
an app by extracting the app’s manifest and resource ﬁles with
Apktool [65] and then searching for signatures required for the
Facebook Login to work properly. These include an activity or
intent ﬁlter dedicated to the Login interface, a Login button in
the layout, and the invocation of an initialization, destruction,
or conﬁguration routine.
Another feature that we added is the download of privacy
policies linked to from within apps. Our initial policy crawler
was limited to downloading policies via an app’s Play store
page. As the Cal AG provided guidance to app publishers
for linking to the policy from both the Play store and from
within an app [11], our new approach is intended to cover both
possibilities. The system ﬁnds the links in an app by extracting
strings from the APK ﬁle using Apktool and then extracting
URLs from within these strings that contain relevant keywords,
such as “privacy.” If a policy link inside an app differs from the
app’s Play store policy link or if there are multiple policy links
identiﬁed within an app, our system downloads and analyzes
all documents retrieved from these links.
C. System Performance
The Cal AG users reported that our system has the potential
to signiﬁcantly increase their productivity. Particularly, as they
have limited resources it can give them guidance on certain
areas, e.g., categories of apps, to focus on. They can also put
less effort and time into analyzing practices in apps for which
28Compare Cal. Bus. & Prof. Code §22577(a)(3), according to which an e-mail address
by itself qualiﬁes as PII, and §22577(a)(7), which covers information types that only
qualify as PII in combination with other identiﬁers.
Pract
CID
CL
CC
SID
SL
SC
Acc
(n=20)
0.85
0.75
0.95
0.95
0.9
1
Precpos
(n=20)
Recpos
(n=20)
0.5
0.38
1
0.94
0.75
-
1
1
0.75
1
1
-
F-1pos
(n=20)
0.67
0.55
0.86
0.97
0.86
-
Inconsistent
(n=20)
15%
15%
15%
75%
30%
0%
TABLE VII: Classiﬁcation results for the Cal AG app/policy
set (n=20).
our system does not ﬁnd potential inconsistencies. Instead,
they can concentrate on examining apps for which ﬂags were
raised. In addition, the Cal AG users expressed that our system
was useful for estimating the current overall state of CalOPPA
compliance. For example, the analysis results alerted them of
some app policies that use vague language in the descriptions
of their collection and sharing practices.
We evaluated our system implementation for the Cal AG