s
t
s
e
u
q
e
r
w
o
l
f
,
t
u
p
h
g
u
o
r
h
t
m
e
t
s
y
S
 800000
 700000
 600000
 500000
 400000
 300000
 200000
 100000
 0
 1
Under maximum input rate
W/O core binding, W/O thread binding
W/O core binding, With thread binding
With core binding, W/O thread binding
With core binding, With thread binding
 2
 3
 4
 5
 6
 7
Number of worker threads
Fig. 10. Controller throughput.
E. Effectiveness of Multi-Threading
Figure 10 shows the throughput scalability of Maestro
with different numbers of worker threads. Because the best
throughput is achieved with 7 worker threads on the 8 core
server machine, we only show the number of worker threads
up to 7. As the two (cid:147)W/O core binding(cid:148) cases shown in the
(cid:2)gure, the feature of binding a worker thread to a core is
critical because the overhead to move running code and all data
it used among different cores is very expensive. Without such
a feature the system cannot scale up very well. If this feature
is enabled, as the two upper lines show, the throughput can
scale almost linearly with more worker threads in the system.
Finally, if the feature of binding a request to a worker thread is
also enabled, Maestro shows the best throughput because this
feature helps reduce cache synchronization overhead because
the whole process of handling one request stays within one
worker thread, thus within one processor core.
Under maximum input rate
W/O core binding, W/O thread binding
W/O core binding, With thread binding
With core binding, W/O thread binding
With core binding, With thread binding
 1400
 1200
 1000
 800
 600
 400
 200
d
n
o
c
e
s
i
l
l
i
m
,
y
a
e
d
l
e
g
a
r
e
v
A
 0
 1
 2
 3
 5
Number of worker threads
 4
 6
 7
Fig. 11. Average delay under maximum request input rate. Note that delay
will be much smaller under normal load.
Figure 11 shows the average delay with different numbers of
worker threads. With both of the two features enabled, Maestro
shows the best performance in reducing the average delay
experienced by all requests. Again notice that the minimum
delay of 163ms shown in this (cid:2)gure is for when Maestro is
under maximum input rate, and the delay will be much smaller
when the load is not as heavy.
In addition, to evaluate the overhead of sharing one raw-
packet task queue because of the pull-style work distribution,
we measure both the time spent in waiting for acquiring the
lock on the raw-packet task queue, and the time spent in
running all tasks, when Maestro is at maximum throughput. It
turns out that the time spent in waiting for acquiring the lock is
only 0.098% of the time spent in running tasks. This con(cid:2)rms
our argument that the overhead of sharing one raw-packet task
queue is negligible.
F. Effectiveness of Output Batching
d
n
o
c
e
s
/
s
t
s
e
u
q
e
r
w
o
l
f
,
t
u
p
h
g
u
o
r
h
t
m
e
t
s
y
S
Under maximum input rate
Throughput, W/O output batching
Throughput, With output batching
Average delay, W/O output batching
Average delay, With output batching
 800000
 700000
 600000
 500000
 400000
 300000
 200000
 100000
 2000
 1500
 1000
 500
d
n
o
c
e
s
i
l
l
i
m
,
y
a
e
d
l
e
g
a
r
e
v
A
 0
 1
 2
 3
 4
 5
 6
Number of worker threads
 0
 7
Fig. 12.
enabled.
Performance comparison between whether output batching is
Figure 12 shows the performance comparison for Maestro
with and without the output batching feature enabled, under
the maximum rate of input requests. As shown in the (cid:2)gure,
when output batching is enabled, both the throughput and
the average delay performance is much better than that of
when output batching is disabled, due to the reduction in the
overhead of sending out messages. Thus output batching is
also very critical in improving the performance of Maestro.
G. Discussion
In this section we have shown that, by excessively exploiting
parallelism within a single server machine, and additional op-
timization techniques such as minimizing cross-core overhead
and batching, Maestro can achieve near linear scalability in
throughput of processing (cid:3)ow requests on a eight-core server
machine. Although the largest throughput that Maestro can
achieve (600000rps) is still far away from the requirement
imposed by a large scale data center (more than 10 million
rps), we totally expect that Maestro can be distributed as
done similiarly by NOX [16] to scale to tens of millions of
(cid:3)ow requests per second. Moreover, the scalability of Maestro
within one single multi-core server machine can help cut the
number of required distributed controllers by at least ten times
of that required by NOX.
IV. RELATED WORK
Ethane [4], the predecessor of NOX and OpenFlow, is an
early (cid:3)ow-based networking technology for creating secure
enterprise networks. Ethane shows that by restricting reach-
ability in the network before an identity is authenticated by
a central controller, strong security policies can be enforced
in the network. OpenFlow is a generalization of Ethane, and
NOX is a more full-(cid:3)edged and programmable controller
design. However, neither Ethane nor NOX considers exploiting
parallelism in their designs.
HyperFlow [18], like Maestro, aims at improving the perfor-
mance of the OpenFlow control plane. However, HyperFlow
takes a completely different approach by extending NOX to
a distributed control plane. By synchronizing network-wide
state among distributed controller machines in the background
through a distributed (cid:2)le system, HyperFlow ensures that the
processing of a particular (cid:3)ow request is localizable to an
individual controller machine. The techniques employed by
HyperFlow are orthogonal to the design of the controller
platform, thus, they can also enable Maestro to become fully
distributed to attain even higher overall scalability.
DIFANE [20] provides a way to achieve ef(cid:2)cient rule-
based policy enforcement in a network by performing policy
rules matching at the switches themselves. DIFANE’s network
controller installs policy rules in switches and does not need
to be involved in matching packets against these rules as
in OpenFlow. However, OpenFlow is more (cid:3)exible since
its control logic can realize behaviors that cannot be easily
achieved by a set of relatively static policy rules installed in
switches. Ultimately, the techniques proposed by DIFANE to
of(cid:3)oad policy rules matching onto switches and our techniques
to increase the performance of the controller are highly com-
plementary: Functionalities that can be realized by DIFANE
can be off-loaded to switches, while functionalities that require
central controller processing can be handled ef(cid:2)ciently by
Maestro.
V. SUMMARY
Flexibility and direct control make OpenFlow a popular
choice for different networking scenarios today. But the per-
formance of the OpenFlow controller could be a bottleneck
in larger networks. Maestro is the (cid:2)rst OpenFlow controller
system that exploits parallelism to achieve near linear per-
formance scalability on multi-core processors. In Maestro,
programmers can change the control plane functionality by
writing simple single-threaded programs. Maestro incorporates
a set of designs and techniques that address the speci(cid:2)c
requirements of OpenFlow, exploits parallelism on behalf of
the programmers, and achieves massive performance improve-
ment over the state-of-the-art alternative. This performance
improvement will have a signi(cid:2)cant positive impact on many
deployed and to be deployed OpenFlow networks.
REFERENCES
[1] M. Al-Fares, S. Radhakrishnan, B. Raghavan, N. Huang, and A. Vahdat.
Hedera: Dynamic (cid:3)ow scheduling for data center networks. In USENIX
NSDI, 2010.
[2] J.R. Ballard, I. Rae, and A. Akella. Extensible and Scalable Network
Monitoring Using OpenSAFE. apr 2010.
[3] Theophilus Benson, Aditya Akella, and David A. Maltz. Network Traf(cid:2)c
Characteristics of Data Centers in the Wild. In IMC, November 2010.
[4] Martin Casado, Michael J. Freedman, Justin Pettit, Jianying Luo, Nick
McKeown, and Scott Shenker. Ethane: taking control of the enterprise.
In SIGCOMM ’07: Proceedings of the 2007 conference on Applications,
technologies, architectures, and protocols for computer communications,
pages 1(cid:150)12, New York, NY, USA, 2007. ACM.
[5] N. Feamster, A. Nayak, H. Kim, R. Clark, Y. Mundada, A. Ramachan-
dran, and M. bin Tariq. Decoupling Policy from Con(cid:2)guration in
Campus and Enterprise Networks. 2010.
[6] Albert Greenberg, Gisli Hjalmtysson, David A. Maltz, Andy Myers,
Jennifer Rexford, Geoffrey Xie, Hong Yan, Jibin Zhan, and Hui Zhang.
A clean slate 4D approach to network control and management. ACM
Computer Communication Review, October 2005.
[7] Natasha Gude, Teemu Koponen, Justin Pettit, Ben Pfaff, Martn Casado,
Nick McKeown, and Scott Shenker. Nox: Towards an operating system
for networks. ACM Computer Communication Review, July 2008. Short
technical Notes.
[8] S.W. Han, N. Kim, and J.W. Kim. Designing a virtualized testbed for
dynamic multimedia service composition.
In Proceedings of the 4th
International Conference on Future Internet Technologies, pages 1(cid:150)4.
ACM, 2009.
[9] Nick McKeown, Tom Anderson, Hari Balakrishnan, Guru Parulkar,
Larry Peterson, Jennifer Rexford, Scott Shenker, and Jonathan Turner.
Open(cid:3)ow: enabling innovation in campus networks. ACM Computer
Communication Review, 38:69(cid:150)74, April 2009.
control plane for open(cid:3)ow. In INM/WREN, 2010.
[19] K.K. Yap, M. Kobayashi, D. Underhill, S. Seetharaman, P. Kazemian,
and N. McKeown. The stanford openroads deployment. In Proceedings
of the 4th ACM international workshop on Experimental evaluation and
characterization, pages 59(cid:150)66. ACM, 2009.
[20] M. Yu, J. Rexford, M.J. Freedman, and J. Wang. Scalable (cid:3)ow-based
networking with DIFANE. In Proc. ACM SIGCOMM, August 2010.
[10] J. Naous, R. Stutsman, D. Mazi(cid:30)eres, N. McKeown, and N. Zeldovich.
Delegating network security with more information. In Proceedings of
the 1st ACM workshop on Research on enterprise networking, pages
19(cid:150)26. ACM, 2009.
[11] A.K. Nayak, A. Reimers, N. Feamster, and R. Clark. Resonance:
dynamic access control for enterprise networks. In Proceedings of the
1st ACM workshop on Research on enterprise networking, pages 11(cid:150)18.
ACM, 2009.
[12] R. Niranjan Mysore, A. Pamboris, N. Farrington, N. Huang, P. Miri,
S. Radhakrishnan, V. Subramanya, and A. Vahdat. PortLand: a scalable
fault-tolerant
layer 2 data center network fabric. ACM SIGCOMM
Computer Communication Review, 39(4):39(cid:150)50, 2009.
[13] R. Sherwood, M. Chan, A. Covington, G. Gibb, M. Flajslik, N. Hand-
igol, T.Y. Huang, P. Kazemian, M. Kobayashi, J. Naous, et al. Carving
research slices out of your production networks with OpenFlow. ACM
SIGCOMM Computer Communication Review, 40(1):129(cid:150)130, 2010.
[14] Neil Spring, Ratul Mahajan, and David Wetheral. Measuring ISP
topologies with RocketFuel. In Proc. ACM SIGCOMM, August 2002.
[15] A. Tavakoli, M. Casado, T. Koponen, and S. Shenker. Applying NOX to
the Datacenter. In Eighth ACM Workshop on Hot Topics in Networks
(HotNets-VIII), 2009.
[16] Arsalan Tavakoli, Martin Casado, Teemu Koponen, and Scott Shenker.
Applying nox to the datacenter. In Proc. HotNets, October 2009.
[17] A. Tootoonchian, M. Ghobadi, and Y. Ganjali. OpenTM: Traf(cid:2)c Matrix
Estimator for OpenFlow Networks. In Passive and Active Measurement,
pages 201(cid:150)210. Springer, 2010.
[18] Amin Tootoonchian and Yashar Ganjali. Hyper(cid:3)ow: A distributed