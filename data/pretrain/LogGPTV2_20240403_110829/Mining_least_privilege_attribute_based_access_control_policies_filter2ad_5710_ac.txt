valid event of the privilege universe is either granted or denied
only once by the policy. To obtain these values (FP, TN, FPR), we
first determine how many unique events out of the valid privilege
space are granted by the policy (lines 11-14). It is important to
note that enumerating the entire privilege space and testing every
Algorithm 1: Rule Mining Algorithm
Input: LO BP The set of log entries representing user actions
during the observation period OBP.
Input: ω under- vs. over-privilege weighting variable.
Input: ϵ Threshold value of minimum itemset frequency.
Input: ξ ‘ The set of all attribute:value combinations in the valid
privilege universe.
during the operation period OP P.
Output: policy The policy with a set of ABAC rules to be applied
1 policy ← ∅;
2 Luncov ← LO BP ;
3 while |Luncov | > 0 do
itemsets ←
F P−дrowth .f r equent I temsets(Luncov , ϵ);
candidateRules ← ∅;
for itemset ∈ itemsets do
4
rule = cr eateRule(itemset);
cover aдeRate = |Luncov (rul e)|
|Luncov |
over Assiдnment Rate = |ξ ‘(rul e)|−|Luncov (rul e)|
rule .Cscor e =
cover aдeRate + ω × (1 − over Assiдnment Rate);
candidateRules ← candidateRules ∪ rule;
|ξ ‘|
;
;
5
6
7
8
9
10
11
12
13
end
best Rule =
sor t Descendinд(candidateRules , Cscor e)[0];
policy ← policy ∪ best Rule;
Luncov ← Luncov \ Luncov(best Rule);
14
15
16 end
17 return policy
valid event against the policy would be much more computationally
intensive than our approach, which is to use information about the
valid privilege space to enumerate only the valid events allowed by
each rule. Most mined rules only allow a small percentage of the
privilege space except in cases of extreme ω values.
Once the set of all the unique events allowed by a policy has been
enumerated, we remove the set of unique events which occurred
and were granted during the operation period to obtain the number
of total FP events for the policy (line 15). At this point we have
obtained the unique sets of TPs, FNs, and FPs, so any remaining
privilege in the valid privilege universe not in these sets must be a
TN (line 16). TPR and FPR are then calculated with the caveat that
in the case where no privileges were exercised during the operation
period, we set T PR = 1 because there could not be any instances of
under-privilege (lines 18-22). The policyAllowsEvent() function is
self-explanatory and its trivial implementation is omitted.
4.3 Optimizations for Large Privilege Spaces
Dealing with the large number of possible attributes:value combi-
nations that may comprise an ABAC privilege space is a significant
challenge compared to the simpler RBAC privilege space. Using
all attributes and values present in logs may make the privilege
universe computationally impractical to process, but discarding
too many or important attributes may result in less secure policies.
We address these issues and make large ABAC privilege spaces
manageable by using feature selection and partitioning methods.
Algorithm 2: Policy Scoring Algorithm
Input: LO P P The set of log entries representing user actions
during the operation period OP P.
Input: ξ ‘ The set of all attribute:value combinations in the valid
privilege universe.
Input: policy The policy with a set of ABAC rules to be applied
during the operation period OP P.
Output: T P R, F P R True and false positive rates of the policy
evaluated on the operation period OP P.
1 T P = F N = 0;
2 exercisedGr antedEvents ← ∅ ;
3 for event ∈ LO P P do
4
5
6
if policyAllowsEvent(policy, event) then
T P = T P + 1;
exercisedGr antedEvents ←
exercisedGr antedEvents ∪ event;
else
end
F N = F N + 1;
7
8
9
10 end
11 eventsAllowedByPolicy ← ∅;
12 for r ∈ policy do
eventsAllowedByPolicy ←
eventsAllowedByPolicy ∪ ξ ‘(rule);
13
|eventsAllowedByPolicy \ exercisedGr antedEvents |;
14 end
15 F P =
16 T N = |privU niverse | − (T P + F N + F P);
17 if T P + F N == 0 then
18
19 else
20
21 end
22 F P R = F P/(F P + T N);
23 return T P R, F P R
T P R = 1;
T P R = T P/(T P + F N);
4.3.1 Preprocessing and Feature Selection. Intuitively, attributes
which occur infrequently in the logs or have highly unique values
are poor candidates for use in creating ABAC policies. There is
less data available to mine meaningful patterns from uncommon
attributes. Also, rules created with uncommon attributes are less
useful in access control decisions because future access requests are
unlikely to use these attributes as well. Using attributes with highly
unique values (an attribute value is never or rarely duplicated across
log entries) is likely to result in over-fitting for the correspondingly
created rules. We therefore preprocess our dataset to select and bin
the most useful attributes as follows:
(1) Remove unique and redundant attributes using U niqueness
where U niqueness =
|U niqueV alues |
(2) Remove redundant correlated attributes.
(3) Sort attributes by Frequency = AttributeOccurr ences
AttributeOccurr ences .
. Se-
T otal LoдEntries
lect attributes above a frequency threshold, θ.
(4) Sort remaining values by U niqueness. High U niqueness val-
ues are candidates for binning or removal.
Our full AWS dataset contained 1,748 distinct attributes (see
Section 5.1 for dataset description). In step (1), |U niqueV alues| is
obtained by calculating the size of the set of all unique values for
every attribute. Set attributes with U niqueness ≈ 1.0 nearly always
have unique values, and U niqueness ≈ 0.0 implies the attribute
values are nearly always the same. Resource identifiers are given an
exception to the uniqueness test in this step as they are expected to
have high uniqueness. For our dataset, we identified and removed
two always unique attributes, eventID and requestID, and one
attribute that always had the same value accountId. We confirmed
that these attributes would always meet the uniqueness criteria
with the AWS documentation.
Applying step (2), we identified three distinct attributes for the
user name with a 1:1 correlation and removed two of them. The
reason for this is if three given values are always 1:1 correlated,
the data mining algorithm gains nothing by having all of them -
two of them are redundant and can be removed without loss of
discriminating power.
For step (3), we selected two thresholds, θ = 0.1 and θ = 0.005,
to build two datasets for experiments, and we term the privilege
universes built using these thresholds ξ‘0.1 and ξ‘0.005, respectively.
Figure 2 charts the rank of the top 50 most common attributes after
our feature selection process was complete. The attribute frequency
follows the common power law distribution with a “long tail”; the
remaining attributes not charted here occurred in less than 0.2%
of the log entries. ξ‘0.1 and ξ‘0.005 correspond to top 15 and 40
attributes ranked by frequency, respectively. Based on the chart,
these two frequency values were chosen as cut off points after
which the amount of information gained becomes more negligible.
Figure 2: Top 50 Attributes Ranked by Frequency
In step (4), some of the remaining attributes still have fairly high
U niqueness values which are difficult to mine meaningful rules
from. In our dataset, some of these attributes such as checksum
values are not relevant to creating security policies and can be
discarded. Three attributes, sourceIPAddress, userAgent, and event-
Name, can benefit from binning into a smaller subset of values. The
sourceIPAddress is an IPv4 address with over 4 billion possible val-
ues. After consulting with the system administrators of the dataset
provider, we found that it was unlikely they would use rules based
on the raw IP addresses since users will change IPs frequently. In-
stead, they preferred to derive the geographical location from the
IP addresses, so IPs were binned by U.S. states and each country
the organization’s users may log in from. The userAgent attribute is
the AWS Command Line Interface (CLI), Software Development Kit
(SDK), or web browser version used when making a request. This
field benefits from binning as users are likely to perform similar
requests from a web browser, but they may upgrade their browser
version regularly. Without binning the many different browser ver-
sions into a single group, a mining algorithm would not effectively
learn user patterns. Again, the dataset provider agreed that the
raw value was too granular to use, so the userAgent attribute was
binned into 10 buckets. The eventName attribute is the name of the
operation. This attribute can be effectively binned because each
eventName is associated with one eventSource, which is the AWS
service name associated with the operation.
4.3.2 Mining Algorithm Optimizations. The resulting ABAC privi-
lege space may still be quite large even for a modest dataset after ap-
plying the feature selection and binning methods as just described.
We further apply partitioning techniques to split up the privilege
space in the policy mining process. Partitioning techniques (as used
in databases to split large tables into smaller ones) will help reduce
the memory footprint of our algorithms and improve efficiency by
performing operations in parallel across multiple processors.
The rule mining algorithm (Algorithm 1) uses partitioning to
improve the run time and space efficiency for storing and searching
the privilege universe ξ‘. The total number of valid combinations in
ξ‘ was on the order of billions for some of our experiments, but Al-
gorithm 1 only needs to determine the number of privileges covered
by a rule and needs not to enumerate or store all possible privilege
combinations in memory. This is a subtle but important difference
because it means we can calculate the number of valid privilege
combinations by splitting ξ‘ into smaller sets of independent parti-
tions. The total number of valid privilege combinations covered by
a rule is the product of the number of valid privilege combinations
covered by each partition, i.e., |ξ‘(r)| = |P1(r)| × ... × |Pn(r)|, where
the attributes of each partition Pi are independent of the attributes
in all other partitions.
To create these partitions, the AWS documentation was used to
identify dependencies between attributes in our dataset. Next, a sim-
ple depth first search was used to identify connected components
of interdependent attributes. The valid attribute:value combinations
for all attributes in each connected component were then enumer-
ated and stored into one inverted index for each partition. Finding
the number of valid privilege combinations covered by a rule in a
partition (|Pn(r)|) is accomplished by searching the inverted index
using the rule’s attribute:value constraints as search terms. As a
result of this partitioning, our queries were performed against three
indexes on the order of thousands to hundreds of thousands of doc-
uments vs. a single index that would have been on the order of
hundreds of millions to billions of documents if such a partitioning
scheme were not in use.
For our dataset, a depth first search identified one connected
component of all user attributes, and another connected compo-
nent of operations and resources. Operations and resources were
connected because most operations are specific to a single or set of
resource types. We grouped all other attributes that were indepen-
dent of users and operations into a third component which included
environment attributes such as the sourceIPAddress and userAgent.
Although this grouping of attributes by components was obtained
00.20.40.60.8101020304050FrequencyAttribute Frequency Rankfrom processing our specific dataset, it is reasonable to assume
that user attributes are independent of the valid operation and re-
source attribute combinations in other datasets as well. This is also
consistent with the NIST ABAC guide which defines environment
conditions as being independent of subjects and objects [8].
Due to the large number of candidate rules generated by the
F P−дrowth algorithm, scoring candidate rules is the most compu-
tationally intensive part of Algorithm 1 in our experiments (except
for those with fairly large ϵ values which generate few candidates).
The search against the inverted index is also parallelized to improve
performance.
Scoring Algorithm Optimizations. To improve the run time
4.3.3
performance of the policy scoring algorithm (Algorithm 2) and
enable it to deal with a privilege space larger than the available
memory, we again employ partitioning and parallelization methods.
As mentioned in 4.2, Algorithm 2 must enumerate the set of all
privilege combinations covered by a rule in order to identify the
total unique number of privilege combinations covered by a policy.
If extreme values for ω are chosen, it is possible for Algorithm 1
to generate rules with a large number of over-privileges, possibly
the entire privilege space. Therefore, Algorithm 2 must be able to
deal with the possibility that it will have to enumerate all privilege
combinations of ξ‘, although again, this only happens for extreme
values of ω and is only for the out-of-sample validation in policy
scoring rather than in rule mining.
To deal with the possible need to enumerate a large portion
or even all of the privilege space, we partitioned ξ‘ along two
attributes so that the values of those attributes are placed into