the backdoor remains dormant in full precision until a quantization
process automatically completes and awakes the backdoor func-
tion, producing a trojaned QNN which reaches a near-100% ASR
when queried with trigger inputs. Extensive evaluation on three
typical scenarios and two popular DNN architectures strongly vali-
date the practical threats of QUASI. Additionally, QUASI exhibits
success evasion against two potential countermeasures extended
from previous trojan defensive techniques on FPNN, which puts an
urgent call on mitigation studies of backdoor attacks against QNN,
a critical component in the development of deep learning with edge
computing. As a practical implication, we suggest, at least at the
current stage, it would be rather risky for users to accept, down-
load or deploy third-party QNNs on edge devices in production
environments, due to the lack of effective algorithms in checking
the fidelity of third-party QNNs from unauthorized sources.
ACKNOWLEDGMENTS
We would like to thank the anonymous reviewers for their con-
structive comments and input to improve our paper. This work was
supported in part by National Natural Science Foundation of China
(61972099, U1836213,U1836210, U1736208), and Natural Science
Foundation of Shanghai (19ZR1404800). Min Yang is a faculty of
Shanghai Institute of Intelligent Electronics & Systems, Shanghai
Institute for Advanced Communication and Data Science, and Engi-
neering Research Center of CyberSecurity Auditing and Monitoring,
Ministry of Education, China.
REFERENCES
[1] [n.d.]. Android Demo Apps - PyTorch. https://github.com/pytorch/android-demo-
app. Accessed: 2021-05-21.
[2] [n.d.].
https://paddle-
lite.readthedocs.io/zh/latest/introduction/support_model_list.html. Accessed:
2021-05-21.
Available Models-PaddleLite.
[3] [n.d.].
Models
- Machine
Learing
https://developer.apple.com/machine-learning/models/.
05-21.
- Apple Developer.
Accessed: 2021-
Accessed: 2021-05-21.
[5] [n.d.]. Quantization-Tensorflow. https://www.tensorflow.org/model_optimization/
guide/quantization/post_training. Accessed: 2021-05-21.
[6] [n.d.].
US
Government’s
TrojAI
https://www.iarpa.gov/index.php/research-programs/trojai.
2021-02-01.
[7] Martín Abadi, P. Barham, J. Chen, and et al. 2016. TensorFlow: A system for
Program.
Accessed:
large-scale machine learning. In OSDI.
[8] E. Bagdasaryan and Vitaly Shmatikov. 2021. Blind Backdoors in Deep Learning
Models. USENIX Security Symposium (2021).
How To Backdoor Federated Learning. In AISTATS.
[10] Xiaoyu Cao, J. Jia, and N. Gong. 2021. IPGuard: Protecting Intellectual Prop-
erty of Deep Neural Networks via Fingerprinting the Classification Boundary.
Proceedings of the 2021 ACM Asia Conference on Computer and Communications
Security (2021).
[11] Nicholas Carlini and David A. Wagner. 2017. Towards Evaluating the Robustness
of Neural Networks. 2017 IEEE Symposium on Security and Privacy (SP) (2017),
39–57.
[12] Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Ben Edwards,
Taesung Lee, Ian Molloy, and B. Srivastava. 2019. Detecting Backdoor Attacks on
Deep Neural Networks by Activation Clustering. ArXiv abs/1811.03728 (2019).
[13] Huili Chen, Cheng Fu, J. Zhao, and F. Koushanfar. 2019. DeepInspect: A Black-
box Trojan Detection and Mitigation Framework for Deep Neural Networks. In
IJCAI.
[14] Jiasi Chen and Xukan Ran. 2019. Deep Learning With Edge Computing: A Review.
Proc. IEEE 107 (2019), 1655–1674.
2325–2383.
[16] Matthieu Courbariaux, Yoshua Bengio, and J. David. 2015. BinaryConnect: Train-
ing Deep Neural Networks with binary weights during propagations. In NIPS.
[17] Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua
Bengio. 2016. Binarized Neural Networks: Training Deep Neural Networks with
Weights and Activations Constrained to +1 or -1. NeurIPS (2016).
[18] Bao Gia Doan, Ehsan Abbasnejad, and D. Ranasinghe. 2020. Februus: Input
Purification Defense Against Trojan Attacks on Deep Neural Network Systems.
ACSAC (2020).
[19] Min Du, R. Jia, and D. Song. 2020. Robust Anomaly Detection and Backdoor
Attack Detection Via Differential Privacy. ICLR (2020).
[20] Kirsty Duncan, E. Komendantskaya, Rob Stewart, and M. Lones. 2020. Relative
Robustness of Quantized Neural Networks Against Adversarial Attacks. 2020
International Joint Conference on Neural Networks (IJCNN) (2020), 1–8.
[21] Yansong Gao, Chang Xu, Derui Wang, S. Chen, D. Ranasinghe, and S. Nepal. 2019.
STRIP: a defence against trojan attacks on deep neural networks. ACSAC (2019).
[22] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, M. Mahoney, and K. Keutzer.
2021. A Survey of Quantization Methods for Efficient Neural Network Inference.
ArXiv abs/2103.13630 (2021).
[23] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT
Press. http://www.deeplearningbook.org.
[24] I. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and
Harnessing Adversarial Examples. CoRR abs/1412.6572 (2015).
[25] R. Gray and D. Neuhoff. 1998. Quantization. IEEE Trans. Inf. Theory 44 (1998),
[26] Tianyu Gu, K. Liu, Brendan Dolan-Gavitt, and S. Garg. 2019. BadNets: Evaluating
Backdooring Attacks on Deep Neural Networks. IEEE Access (2019).
[27] Wenbo Guo, Lun Wang, Yan Xu, Xinyu Xing, Min Du, and D. Song. 2020. Towards
Inspecting and Eliminating Trojan Backdoors in Deep Neural Networks. ICDM
(2020).
[28] Kartik Gupta and Thalaiyasingam Ajanthan. 2020. Improved Gradient based
Adversarial Attacks for Quantized Networks. ArXiv abs/2003.13511 (2020).
[29] Sanghyun Hong, Pietro Frigo, Yigitcan Kaya, Cristiano Giuffrida, and T. Dumitras.
2019. Terminal Brain Damage: Exposing the Graceless Degradation in Deep
Neural Networks Under Hardware Fault Attacks. In USENIX Security Symposium.
[30] Benoit Jacob, S. Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew G.
Howard, Hartwig Adam, and D. Kalenichenko. 2018. Quantization and Training of
Neural Networks for Efficient Integer-Arithmetic-Only Inference. 2018 IEEE/CVF
Conference on Computer Vision and Pattern Recognition (2018), 2704–2713.
[31] Yujie Ji, Xinyang Zhang, Shouling Ji, X. Luo, and Ting Wang. 2018. Model-
Reuse Attacks on Deep Learning Systems. Proceedings of the 2018 ACM SIGSAC
Conference on Computer and Communications Security (2018).
[32] Elias Boutros Khalil, Amrita Gupta, and B. Dilkina. 2019. Combinatorial Attacks
on Binarized Neural Networks. ICLR (2019).
[33] Yoongu Kim, Ross Daly, Jeremie S. Kim, Chris Fallin, Ji-Hye Lee, Donghyuk Lee, C.
Wilkerson, K. Lai, and O. Mutlu. 2014. Flipping bits in memory without accessing
them: An experimental study of DRAM disturbance errors. 2014 ACM/IEEE 41st
International Symposium on Computer Architecture (ISCA) (2014), 361–372.
[34] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Opti-
[35] Raghuraman Krishnamoorthi. 2018. Quantizing deep convolutional networks
for efficient inference: A whitepaper. ArXiv abs/1806.08342 (2018).
[36] A. Krizhevsky. 2009. Learning Multiple Layers of Features from Tiny Images.
[37] Yann LeCun, Léon Bottou, Yoshua Bengio, et al. 1998. Gradient-based learning
applied to document recognition.
[38] Fengfu Li and Bin Liu. 2016. Ternary Weight Networks. NeurIPS (2016).
[39] Shaofeng Li, Minhui Xue, B. Zhao, H. Zhu, and Xinpeng Zhang. 2019. Invisible
Backdoor Attacks on Deep Neural Networks via Steganography and Regulariza-
tion. TDSC (2019).
[40] Ji Lin, Chuang Gan, and Song Han. 2019. Defensive Quantization: When Efficiency
[41] Junyu Lin, Lei Xu, Yingqi Liu, and X. Zhang. 2020. Composite Backdoor Attack
for Deep Neural Network by Mixing Existing Benign Features. Proceedings of the
2020 ACM SIGSAC Conference on Computer and Communications Security (2020).
[42] K. Liu, Brendan Dolan-Gavitt, and S. Garg. 2018. Fine-Pruning: Defending Against
Backdooring Attacks on Deep Neural Networks. In RAID.
[43] Y. Liu, Wen-Chuan Lee, Guanhong Tao, Shiqing Ma, Yousra Aafer, and X. Zhang.
2019. ABS: Scanning Neural Networks for Back-doors by Artificial Brain Stimu-
lation. CCS (2019).
[44] Yingqi Liu, Shiqing Ma, Yousra Aafer, W. Lee, Juan Zhai, Weihang Wang, and X.
Zhang. 2018. Trojaning Attack on Neural Networks. NDSS (2018).
[45] L. V. D. Maaten and Geoffrey E. Hinton. 2008. Visualizing Data using t-SNE.
Journal of Machine Learning Research 9 (2008), 2579–2605.
[46] Alberto G. Matachana, Kenneth T. Co, Luis Muñoz-González, David Martínez,
and Emil C. Lupu. 2020. Robustness and Transferability of Universal Attacks on
Compressed Models. ArXiv abs/2012.06024 (2020).
[47] A. Nguyen and A. Tran. 2020. Input-Aware Dynamic Backdoor Attack. NeurIPS
[4] [n.d.]. Quantization-PyTorch. https://pytorch.org/docs/stable/quantization.html.
mization. CoRR abs/1412.6980 (2015).
[9] E. Bagdasaryan, Andreas Veit, Yiqing Hua, D. Estrin, and Vitaly Shmatikov. 2020.
Meets Robustness. ArXiv abs/1904.08444 (2019).
[15] X. Chen, Chang Liu, Bo Li, Kimberly Lu, and D. Song. 2017. Targeted Backdoor
Attacks on Deep Learning Systems Using Data Poisoning. ArXiv (2017).
(2020).
11
644ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Pan et al.
Algorithm A.1 The algorithmic details of our proposed
quantization-specific backdoor attack.
1: Input: The clean training dataset D = {(xi , yi)}N
i =1, the target
FPNN f with learnable parameters Θ0, the target class yt , the
trigger generation algorithm T(·), the simulated quantization
operations SimQuant (·; д), the index of a layer K, the weight
of the clipping-related regularization term λ and the number
of training epoch T .
2: Output: The trojaned QNN fQ .
3: Randomly initialize the FPNN parameters Θ0.
4: Prepare the target model by inserting SimQuant operations
on the weights and the activations, i.e., ˜fi(·) := ˜f (·; д = i) for
i = 0, 1.
5: for all t in 0, . . . ,T − 1 do
6:
7:
Sample a clean batch B from D.
Sample a clean batch Bt from D where the samples are of
:=
Generate the batch of trigger inputs by Btrigger
the target class yt .
{(T(x), y, yt) : (x, y) ∈ B}.
and obtain the full loss function as in Eq. (10).
Deactivate the SimQuant operations by setting д = 0.
Calculate Lwoq(Θt) based on Eq. (4).
Activate the SimQuant operations by setting д = 1.
Calculate Lwq(Θt) based on Eq (5).
Calculate Rclipping(Θt) :=( ˜x,y,yt)∈Btrigger rclipping( ˜x; Θt)
|Btrigger|( ˜x,y)∈Btrigger
step of the Adam optimizer [34].
Lstealthy(Θt)
1 (T(x)),
d(
˜f K
1
Minimize Lstealthy w.r.t. the parameters Θt by one update
1|Bt |(x,y)∈Bt ( ˜f K
Minimize Eq. (10) w.r.t. the parameters Θt by one update
1 (x))).
Calculate
:=
step of the Adam optimizer.
17: end for
18: Θ∗ ← ΘT
19: Quantize the model f with the parameter Θ∗ according to
the configurations in the SimQuant operations to produce the
trojaned QNN fQ .
8:
9:
10:
11:
12:
13:
14:
15:
16:
20: Return: fQ
[50] A. S. Rakin, Zhezhi He, and Deliang Fan. 2019. Bit-Flip Attack: Crushing Neural
Network With Progressive Bit Search. 2019 IEEE/CVF International Conference on
Computer Vision (ICCV) (2019), 1211–1220.
[51] A. S. Rakin, Zhezhi He, and Deliang Fan. 2020. TBT: Targeted Neural Network
Attack With Bit Trojan. 2020 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) (2020), 13195–13204.
[52] Kaveh Razavi, Ben Gras, E. Bosman, B. Preneel, Cristiano Giuffrida, and H. Bos.
2016. Flip Feng Shui: Hammering a Needle in the Software Stack. In USENIX
Security Symposium.
[53] Cyril Roscian, A. Sarafianos, J. Dutertre, and A. Tria. 2013. Fault Model Analysis
of Laser-Induced Faults in SRAM Memory Cells. 2013 Workshop on Fault Diagnosis
and Tolerance in Cryptography (2013), 89–98.
[54] A. Salem, Rui Wen, M. Backes, Shiqing Ma, and Y. Zhang. 2020. Dynamic Backdoor
Attacks Against Machine Learning Models. ArXiv (2020).
[55] K. Simonyan and Andrew Zisserman. 2015. Very Deep Convolutional Networks
for Large-Scale Image Recognition. CoRR abs/1409.1556 (2015).
[56] J. Stallkamp, Marc Schlipsing, J. Salmen, and C. Igel. 2012. Man vs. computer:
Benchmarking machine learning algorithms for traffic sign recognition. Neural
networks : the official journal of the International Neural Network Society 32 (2012),
323–32.
[57] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, D. Erhan, I.
Goodfellow, and R. Fergus. 2014. Intriguing properties of neural networks. CoRR
abs/1312.6199 (2014).
[58] Brandon Tran, Jerry Li, and A. Madry. 2018. Spectral Signatures in Backdoor
[59] Alexander Turner, D. Tsipras, and A. Madry. 2019. Label-Consistent Backdoor
Attacks. In NeurIPS.
Attacks. ArXiv (2019).
[60] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, B. Viswanath, H. Zheng,
and B. Zhao. 2019. Neural Cleanse: Identifying and Mitigating Backdoor Attacks
in Neural Networks. Security & Privacy (2019).
[61] Jiaxiang Wu, C. Leng, Yuhang Wang, Q. Hu, and Jian Cheng. 2016. Quantized
Convolutional Neural Networks for Mobile Devices. 2016 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) (2016), 4820–4828.
[62] Yun Xiang, Zhuang-Zhi Chen, Zuohui Chen, Zebin Fang, Haiyang Hao, Jinyin
Chen, Yi Liu, Zhefu Wu, Qi Xuan, and Xiaoniu Yang. 2020. Open DNN Box by
Power Side-Channel Attack. IEEE Transactions on Circuits and Systems II: Express
Briefs 67 (2020), 2717–2721.
[63] Mengjia Yan, Christopher W. Fletcher, and J. Torrellas. 2020. Cache Telepathy:
Leveraging Shared Resource Attacks to Learn DNN Architectures. USENIX
Security (2020).
[64] Jiancheng Yang, R. Shi, and Bingbing Ni. 2021. MedMNIST Classification De-
cathlon: A Lightweight AutoML Benchmark for Medical Image Analysis. 2021
IEEE 18th International Symposium on Biomedical Imaging (ISBI) (2021), 191–195.
[65] Fan Yao, A. S. Rakin, and Deliang Fan. 2020. DeepHammer: Depleting the Intelli-
gence of Deep Neural Networks through Targeted Chain of Bit Flips. In USENIX
Security Symposium.
[66] Yuanshun Yao, Huiying Li, H. Zheng, and B. Zhao. 2019. Latent Backdoor Attacks
on Deep Neural Networks. CCS (2019).
[67] Honggang Yu, Haocheng Ma, Kaichen Yang, Yiqiang Zhao, and Yier Jin. 2020.
DeepEM: Deep Neural Networks Model Recovery through EM Side-Channel
Information Leakage. 2020 IEEE International Symposium on Hardware Oriented
Security and Trust (HOST) (2020), 209–218.
[48] Adam Paszke, S. Gross, Francisco Massa, and et al. 2019. PyTorch: An Imperative
Style, High-Performance Deep Learning Library. In NeurIPS.
[49] Ximing Qiao, Yukun Yang, and Hongbing Li. 2019. Defending Neural Backdoors
via Generative Distribution Modeling. In NeurIPS.
A OMITTED ALGORITHMIC DETAILS
Algorithm A.1 lists the algorithmic details of our proposed quantization-
specific backdoor attack.
12
645