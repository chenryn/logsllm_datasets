perts at Microsoft Research, we believe classiﬁcation accuracy of
better than 60% will be difﬁcult without a signiﬁcant advance in
the state of the art. For example, the best known algorithms for fa-
cial recognition can achieve in excess of 90% accuracy under con-
trolled conditions, but are not robust to occlusions and variations
in pose similar to those seen in our database [5, 18]. The 2006
PASCAL Visual Object Classes Challenge [4] included a competi-
tion to identify photos as containing several classes of objects, two
of which were Cat and Dog. Although cats and dogs were eas-
ily distinguishable from other classes (e.g., “bicycle”), they were
frequently confused with each other.
One attack on our database proposed by vision researchers was
color histogram analysis. We implemented and tested this tech-
nique. We ﬁrst characterized each of 150 random images as a 15-
feature color vector, computing histograms of each ﬁfth of the red,
Figure 5: Though rare, some images at Petﬁnder.com are confusing
to humans. (left) A photo depicting a cat and dog together. (right) A
dog-like cat.
3.1.4 Automatic Database Pruning
The classiﬁcation errors made by users are not uniformly dis-
tributed: some images are confusing, even to humans. Figure 5
shows two examples.
If Asirra achieves only modest popularity, our accuracy ﬁgures
are likely to stay close to those described in the previous section.
However, if Asirra receives widespread use (tens of thousands of
challenges generated per day), we will have enough data to make
automatic, statistically signiﬁcant judgements about confusing im-
ages. There will be cases when a user ultimately succeeds in pass-
ing Asirra, but may have made errors along the way. Once Asirra
decides a user is human, it will be able to mark the previously-
incorrect images as “possibly confusing.” Images that are marked
beyond a threshold can then be removed from the database.
3.1.5
Interest in the “adopt me” link
In both user experiments, we placed a small “Adopt me” link
below the cat or dog displayed in each trial. We did not give users
any instructions regarding this link or tell them what it did. Our
goal was to test how often Internet users involved in some unrelated
task would be motivated (by curiosity, cuteness of a photo, etc.)
to click it. This test is important because it suggests whether our
partnership with Petﬁnder is viable.
In our controlled experiments, 27 users (7.5% of the test popula-
tion) clicked “Adopt me”. One of our beta-testers adopted a beagle.
The public’s interest in the link has been lower. In the 6 weeks
following Asirra’s release, we issued 13,334 “real” challenges—
that is, integrated with pages other than our own demonstration
pages. 279, or 2.1%, led to clicks on “Adopt me”.
We discuss the security implications of “Adopt me” in §3.2.1.
3.2 Security
Before discussing the security of Asirra, it is useful to review
the threat model. CAPTCHAs are an unusual area of security in
that we are not trying to provide absolute guarantees, only slow
down attackers. By deﬁnition, anyone can “break” a CAPTCHA
by devoting a small amount of human effort to it. A CAPTCHA
therefore is successful, essentially, if it forces an automated attack
to cost more than 30 seconds worth of a human’s time in a part
of the world where labor is cheap. The generally accepted ﬁgure
in the literature [13, 2] seems to be that CAPTCHAs should admit
bots less than with less than 1/10,000 probability.
3.2.1 Attacks on “Adopt Me”
The most common ﬁrst question we get when people see Asirra
is, “Doesn’t the adopt-me link defeat your security?” This mis-
conception is understandable; each pet’s page on Petﬁnder.com de-
green, and blue channels of each image. We then computed the best
least-squares ﬁt coefﬁcients of these 15 features to the two (cat and
dog) image classes. The coefﬁcients were then used to predict the
class of another 150 images. The result was 56.9% accuracy.
If we assume an attacker can build a 60% accurate classiﬁer, the
probability of solving a 12-image challenge improves from 1/4,096
to 1/459. However, with our token bucket scheme in place (§4.2),
this is reduced to one ticket per about 70,000 guesses.
3.2.4 Database Attacks
One way to attack any image-based CAPTCHA is to reconstruct
its underlying database. In a sense, this attack is unavoidable: A
portion of the database is revealed each time a challenge is dis-
played. By solving enough challenges manually, the entire database
is eventually revealed to the attacker. The key question is not “is
database reconstruction possible?” but rather “is database recon-
struction economical?”
A CAPTCHA can be considered secure if the most cost effective
way to defeat it is for a human to solve it once per unit of service
gained. In other words, our goal is to make the most efﬁcient attack
an on-demand one (e.g., pay people to sign up for one Hotmail
account at a time). This tradeoff is directly informed by the size
and stability of the underlying database.
Oli Warner’s KittenAuth provides an instructive example. It orig-
inally launched with only 42 images; within days, someone posted
a map of those images’ MD5 hashes to a cat-or-dog bit [12]. Classi-
ﬁcation of 42 images does not take much investment, so this form of
attack was an easy choice. Asirra’s three million images moves the
break-even point signiﬁcantly. An attacker would expect to solve
about 750,000 12-image challenges to reveal 95% of the database.
Reconstruction of our database is not economical unless there is a
ﬁnancial incentive to solve millions of Asirra challenges.
Attackers might also try to reconstruct Asirra’s database by at-
tacking its source—that is, automating a long series of queries to
Petﬁnder.com. However, Petﬁnder’s public interface only displays
pets currently available for adoption, which represents less than
10% of their total historical database. In addition, there is no ef-
ﬁcient way to track database changes using Petﬁnder’s public in-
terface. The private API provided to Asirra by Petﬁnder is not
available to the public.
Note that in this section we do not consider techniques for re-
using an individual image such that it appears to be multiple distinct
images. There are robust image comparator functions (e.g., image
hashes, color histograms) that are insensitive to many simple image
distortions. Warping an image sufﬁciently to fool a computer will
likely also be troublesome to a human—the arms race found in text
CAPTCHAs that we are trying to avoid.
3.2.5
Implementation Attacks
Many CAPTCHAs suffer from weak implementations. For ex-
ample, some allow a session ID authorized by a single successful
challenge to be re-used repeatedly to gain access to a protected ser-
vice. Some assume trustworthy clients.
To avoid such weaknesses, we have taken two approaches. First,
we made the Asirra web service as conceptually simple as possi-
ble, so that we can verify its correctness via code review. (The web
service is about 700 lines of Python code.) Second, as we discuss
further in Section 5.1, we designed the Asirra interface so as to
minimize the implementation burden on the webmaster. For exam-
ple, we do not require sites that use Asirra to keep track of user
sessions or any other form of state. Our example service is, in fact,
completely stateless.
Figure 6: The differences between cats and dogs are immediately ob-
vious to humans. In many cases, species look similar, with only subtle
cues to distinguish them. This makes it a hard vision problem.
3.3 Accessibility
Virtually all visual CAPTCHAs, including Asirra, are not acces-
sible to visually impaired users. The problem of CAPTCHA acces-
sibility is an important one, but beyond the scope of this paper. Ac-
cessible web sites typically allow users to switch between a visual
challenge (e.g., distorted letters) and an auditory one (e.g., numbers
being recited over noise). Asirra is only meant to replace visual
CAPTCHAs. The audio-accessible version of a site’s CAPTCHA
is valuable, but orthogonal to Asirra.
However, it is worthwhile to note that Asirra’s core idea of in-
terest alignment is potentially usable in an audio CAPTCHA. For
example, online music stores typically provide free samples of mil-
lions of songs in their catalogs. These might be used with a “buy
me” link.
4.
IMPROVEMENTS TO ALL CAPTCHAS
In the course of our work on Asirra, we developed two algo-
rithms that can be used to improve virtually all CAPTCHAs, in-
cluding those that are text-based. These techniques share a com-
mon theme.
Imagine that users’ responses to Asirra challenges
were scored manually, by a human judge, instead of automatically
by computer. Even in this seemingly straightforward task, the ﬂexi-
bility of human judgement would be a valuable asset. For example,
our human judge could see the same user try to solve three chal-
lenges, getting 11 out of 12 images correct each time. She might
say, “That looks like a human who is just having a little trouble. I’ll
let him pass.” Conversely, our judge might see the same IP address
submitting thousands of random, incorrect answers. Even when
ﬁnally given a correct answer, she’d still suspect the user was a bot.
In the next sections, we describe two algorithms designed to
make CAPTCHA scoring a little more “human”: the Partial Credit
Algorithm (§4.1) and CAPTCHA Token Buckets (§4.2).
4.1 The Partial Credit Algorithm
Traditionally, computers score CAPTCHA responses with one
bit of output: right or wrong. The intuition behind the Partial Credit
Algorithm (PCA) is that a user’s response contains much more than
one bit of information. Two “almost right” answers are strong evi-
dence that a user is human; two random answers are not. Without
PCA, this distinction is lost.
The idea of an “almost right” response is meaningful in the con-
text of a wide variety of CAPTCHAs. For example, in a text-
based CAPTCHA, typing ﬁve out of six characters correctly can
be considered almost-right. In Asirra, we consider a response to
be almost-right if 11 out of the 12 images presented are identiﬁed
correctly.
Asirra awards Partial Credit to a user who gets a challenge almost
right. If the user gets a subsequent challenge almost right—that is,
while already holding Partial Credit—we judge the response as if
it were completely correct. (Asirra challenges all take place within
a session, making it easy to associate a single user’s responses.)
The appeal of PCA is that its use of previously-ignored informa-
tion allows us to signiﬁcantly improve the pass rates for humans
while minimally improving the pass rates for bots. For example, in
Asirra, PCA reduces the number of humans rejected after 2 chal-
lenges from 2.8% to 0.4%: a 7-fold reduction in unhappy users
(Section 6, Table 1). However, the bot yield improves from 1/4,096
to 1/3,952: only a 3% improvement (Section 6, Table 2).
In comparison, simply scoring every almost-right answer as cor-
rect (i.e., passing users who get 11/12) has a devastating effect on
security: a random-guess bot’s success rate improves from 1/4,096
to 1/315, a 13-fold increase.
We can mathematically model the effect of PCA as follows. We
consider a new user to arrive in the unveriﬁed (u) state, and by solv-
ing a challenge the user moves into the veriﬁed (v) state. PCA in-
troduces an intermediate (i) state, which the user moves into when
getting a challenge almost right. From the intermediate state, if the
user almost—or completely—solves a subsequent challenge, the
user moves to the veriﬁed state; otherwise, the user is returned to
the unveriﬁed state.
Let α represent the probability of solving a challenge, β be the
probability of getting close enough to enter the intermediate state,
and γ be the probability of getting close enough to become veri-
ﬁed when in the intermediate state. After n steps, the probability
that the user is in each state is given by the following recurrence
relation:
un = (1 − α − β) un−1 + (1 − γ) in−1,
in = β un−1,
vn = vn−1 + α un−1 + γ in−1,
u0 = 1
i0 = 0
v0 = 0
The expected number of trials until veriﬁcation is:
E =
X
1≤n<∞
n (vn − vn−1) =
1 + β
α + β γ
In Asirra, the user moves to the intermediate state if exactly one
image (out of 12) is misclassiﬁed; from the intermediate state, the
user moves to the veriﬁed state if zero or one image is misclas-
siﬁed. Thus, using the binomial distribution function b, the PCA
probabilities are:
α = b(0; 12, 1 − p)
β = b(1; 12, 1 − p)
γ = b(0; 12, 1 − p) + b(1; 12, 1 − p)
This model can be used to compute the effect of PCA on any
CAPTCHA that can deﬁne a solution as “almost right.” The effect
on 12-image Asirra challenges is shown in Tables 1 and 2.
4.2 CAPTCHA Token Buckets
Asirra, like virtually every CAPTCHA, is subject to brute-force
attacks. Since guessing is nearly free, a success probability of
1/4,096 may not be a strong enough assurance that attackers can’t
automatically collect service tickets en masse.
Our countermeasure is based on the supposition that a bot can be
identiﬁed as a small number of IP addresses that submit a very large
number of incorrect responses, interspersed with a much smaller
number of correct responses. (Assume for the moment that each
user has a unique IP address; we will relax this constraint shortly.)
The scheme works by assigning a token bucket to every IP ad-
dress that requests an Asirra challenge. Each bucket is initialized
with TB-Max tokens. Every time a client submits any response to
a challenge, a single token is removed from the client IP’s bucket,
down to a minimum of 0. Every time a client submits a correct re-
sponse, TB-Reﬁll tokens are added, up to a maximum of TB-Max.
In our implementation, TB-Max is 100 and TB-Reﬁll is 3.
(The
bucket is also re-initialized to TB-Max tokens after 24 hours of dis-
use, but this is not strictly required.) If a user submits a response
while its token bucket is empty, the user is told the answer is incor-
rect, regardless of their response.
A brute-force bot trying to earn a large number of service tickets
from Asirra will quickly empty its token bucket, as its incorrect
guesses will outnumber its correct guesses by more than a factor of
TB-Reﬁll. In this steady-state, the empty-bucket policy will force
bots to correctly answer two challenges within TB-Reﬁll attempts
before they earn credit for a correct response.
Intuitively, the effect of this policy is to amplify the skill dif-
ference between humans and bots. Modelling it mathematically
is straightforward. A bot has some probability p of getting a sin-