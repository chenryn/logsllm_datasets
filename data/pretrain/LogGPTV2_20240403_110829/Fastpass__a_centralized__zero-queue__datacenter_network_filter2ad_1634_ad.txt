end-to-end latency is substantially lower because queues are much
smaller. In addition, Fastpass achieves low latency for interactive
trafﬁc without requiring the trafﬁc to be designated explicitly as
“interactive” or “bulk,” and without using any mechanisms for trafﬁc
isolation in the switches: it results from the fairness properties of
the timeslot allocator.
7.2 Fairness and convergence
Experiment C: fairness and convergence. Here we examine how
fairly Fastpass allocates throughput to multiple senders, and how
quickly it converges to a fair allocation when a sender arrives or
departs. Five rack servers each send a bulk TCP ﬂow to a sixth
receiving server. The experiment begins with one bulk ﬂow; every
(cid:1)(cid:2)(cid:3)(cid:1)(cid:3)(cid:2)(cid:1)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:8)(cid:12)(cid:13)(cid:14)(cid:12)(cid:8)(cid:15)(cid:15)(cid:8)(cid:16)(cid:13)(cid:17)(cid:18)(cid:9)(cid:19)(cid:16)(cid:20)(cid:21)(cid:13)(cid:9)(cid:16)(cid:8)(cid:11)(cid:22)(cid:23)(cid:24)(cid:16)(cid:11)(cid:25)(cid:24)(cid:16)(cid:16)(cid:26)(cid:24)(cid:16)(cid:13)(cid:15)(cid:8)(cid:9)(cid:13)30 seconds, a new bulk ﬂow arrives until all ﬁve are active for 30
seconds, and then one ﬂow terminates every 30 seconds. The entire
experiment therefore lasts 270 seconds.
We calculate each connection’s throughput over 1-second inter-
vals. The resulting time series for the baseline TCP and for Fastpass
are shown in Figure 9.
The baseline TCPs exhibit much larger variability than Fastpass.
For instance, when the second connection starts, its throughput is
about 20-25% higher than the ﬁrst connection throughout the 30-
second interval; similarly, when there are two senders remaining
between time 210 to 240 seconds, the throughputs “cross over” and
are almost never equal. With more connections, the variation in
throughput for TCP is more pronounced than with Fastpass.
To quantify this observation, we calculate the standard deviation
of the per-connection throughputs achieved in each 1-second interval,
in Mbits/s, when there are 3, 4, and 5 concurrent connections each
for the baseline TCP and Fastpass. We then compute the median
over all standard deviations for a given number of connections (a
median over 60 values when there are 3 or 4 connections and over
30 values when there are 5 connections). The results are:
#connections Baseline
543.86
628.49
459.75
3
4
5
Fastpass
15.89
0.146
0.087
Improvement
34.2⇥
4304.7⇥
5284.5⇥
These results show that Fastpass exhibits signiﬁcantly lower vari-
ability across the board: its standard deviation of throughput is over
5200 times lower than the baseline when there are ﬁve concurrent
connections.
Fastpass’s pipelined timeslot allocation algorithm prioritizes ﬂows
based on their last transmission time, so when a new ﬂow starts, it
is immediately allocated a timeslot. From that point on, all ﬂows
contending for the bottleneck will be allocated timeslots in sequence,
yielding immediate convergence and perfect fairness over intervals
as small as 1 MTU per ﬂow (for 5 ﬂows on 10 Gbits/s links, this
yields fairness at the granularity of 6 µs).
The benchmark shows low total throughput for one and two
senders because of packet processing overheads, which are usually
reduced by TSO. (In contrast, Experiments A and B use many more
connections, so they achieve high utilization). Fastpass senders also
require additional processing in the Fastpass qdisc, which is limited
to using one core; NIC support (§8.3) or a multicore implementation
will alleviate this bottleneck.
7.3 Arbiter performance
Experiment D: request queueing. To estimate how long requests
queue at the arbiter before they are processed, we measure the NIC
polling rate of the comm-core under increasing amounts of network
trafﬁc. Every 10 seconds, a rack server starts a TCP transfer to an
unloaded server.
As the load increases, the arbiter spends more time processing
requests, the NIC polling rate decreases (Fig. 10), and requests are
delayed in the arbiter’s NIC queues. A deployment can control this
queueing delay by limiting the amount of trafﬁc each comm-core
handles: 130 Gbits/s for 1 µs queueing, 170 Gbits/s for 10 µs, etc.
Experiment E: communication overhead. To determine the net-
work capacity requirements at the arbiter, we measure the total
amount of control trafﬁc the arbiter receives and transmits in experi-
ment D. The network overhead of communication with the arbiter
is 1-to-500 for request trafﬁc and 1-to-300 for allocations for the
tested workload (Fig. 11): to schedule as much as 150 Gbits/s, the
comm core receives less than 0.3 Gbits/s of requests and sends out
0.5 Gbits/s of allocations. When the NIC polling rate decreases
)
s
/
s
t
i
b
G
(
t
u
p
h
g
u
o
r
h
t
n
o
i
t
c
e
n
n
o
c
−
r
e
P
6
4
2
0
6
4
2
0
0
50
b
a
s
e
l
i
n
e
f
a
s
t
p
a
s
s
Sender
1
2
3
4
5
200
250
100
150
Time (seconds)
Figure 9: Each connection’s throughput, with a varying number
of senders. Even with 1s averaging intervals, baseline TCP ﬂows
achieve widely varying rates. In contrast, for Fastpass (bottom),
with 3, 4, or 5 connections, the throughput curves are on top of one
another. The Fastpass max-min fair timeslot allocator maintains
fairness at ﬁne granularity. The lower one- and two-sender Fastpass
throughput is due to Fastpass qdisc overheads (§7.2).
sufﬁciently, incoming request packets start getting dropped (seen
around 160 Gbits/s). The arbiter is still able to allocate all trafﬁc
because FCP retransmissions summarize aggregate demands; hence,
not every demand packet needs to be received.
Experiment F: timeslot allocation cores. To determine the number
of arbiter cores necessary for timeslot allocation, we measure the
throughput of the max-min fair timeslot allocation implementation.
Requests are generated by a synthetic stress-test-core, rather than
received from a comm-core. The workload has Poisson arrivals,
senders and receivers chosen uniformly at random from 256 nodes,
and requests are for 10 MTUs. We vary the mean inter-arrival time
to produce different network loads.
Throughput (Gbits/s)
2 cores
825.6
4 cores
1545.1
6 cores
1966.4
8 cores
2211.8
Eight alloc-cores support over 2.21 Terabits/s of network trafﬁc,
or 221 endpoints transmitting at a full 10 Gbits/s. This corresponds
to over 86% network utilization.
Experiment G: path selection cores. To determine the number
of arbiter cores needed for path selection, we measure the average
processing time per timeslot as load increases. We use the synthetic
workload described above with exponentially distributed request
sizes (with a mean of 10 MTUs). The chosen topology has 32
machines per rack and four paths between racks, with no over-
subscription (motivated in part by the “4-post” cluster topology [16]).
Note that non-oversubscribed topologies could be considered worst-
case topologies for path selection: over-subscription reduces the
amount of trafﬁc leaving each rack and thus simpliﬁes path-selection.
Fig. 12 shows that the processing time increases with network
utilization until many of the nodes reach full degree (32 in the tested
topology), at which point the cost of pre-processing4 the graph
decreases, and path selection runs slightly faster.
Because path-selection can be parallelized by having a different
core select paths for each timeslot, these measurements indicate how
many pathsel-cores are required for different topologies. For exam-
4Path selection adds dummy edges to the graph until all nodes have
the same degree (i.e., number of packets).
107
106
105
)
z
H
(
t
e
a
r
g
n
i
l
l
o
p
C
N
I
0
50
Network throughput (Gbits/s)
100
150
200
Figure 10: As more requests are handled, the NIC polling rate de-
creases. The resulting queueing delay can be bounded by distributing
request-handling across multiple comm-cores.
Figure 12: Path selection routes trafﬁc from 16 racks of 32 endpoints
in <12 µs. Consequently, 10 pathsel-cores would output a routing
every <1.2 µs, fast enough to support 10 Gbits/s endpoint links.
Error bars show one standard deviation above and below the mean.
ple, path selection of 16 racks can be done in under 12 microseconds;
hence, for 1.2 microsecond timeslots, 10 pathsel-cores sufﬁce.
7.4 Production experiments at Facebook
Workload. We deployed Fastpass on a latency-sensitive service
that is in the response path for user web requests. This service
has a partition-aggregate workload similar to common search work-
loads [10]. Each server runs both an aggregator and a leaf; when an
aggregator receives a query from a web server, it requests relevant
data from several leaves, processes them, and returns a reduced result.
Each rack of 30–34 aggregator-leaf servers works independently.
To maintain acceptable latency as trafﬁc load changes during the
day, the service adjusts the number of internal queries generated
by each request; in aggregate, a rack handles between 500K and
200M internal queries per second. When load is low, the aggregator
considers more items and produces higher quality results. In times of
heavy load, when the 99th percentile latency increases, the number
of items considered per web request is reduced aggressively.