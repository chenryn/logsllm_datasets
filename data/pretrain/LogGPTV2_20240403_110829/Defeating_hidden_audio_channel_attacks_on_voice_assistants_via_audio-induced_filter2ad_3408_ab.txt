commands which are incomprehensible to humans but can
be recognized by the VCS to launch attacks. The authors also
proposed an audio-based human/machine classifier to detect
such an attack [9]. However, the voice synthesis attacks are
able to forge the similar audio domain features to pass the
system [23].
Voice Authentication for Virtual Voice Assistants.
To defend against various attacks (e.g., replay and imper-
sonation attacks), most voice authentication schemes mainly
use advanced speaker models (e.g., Gaussian mixture model-
universal background model (GMM-UBM) [4], i-vector mod-
els [18, 19]), and various speech vocal features [12, 25, 30].
However, the aforementioned audio-based approaches are
still vulnerable if an adversary has the full knowledge of
the system’s model as they are solely based on the proper-
ties of speech itself. Thus, a multi-modality authentication
framework to provide enhanced security is highly desirable.
Additionally, VoiceLive [35] and VoiceGesture[34] exploited
the geometrical information and dynamic acoustic charac-
teristics derived from the received sound to perform liveness
detection. 2MA [8] took advantage of the presence of multi-
ple microphones to localize and authenticate the source of a
command. Moreover, Feng et al. [16] developed a user veri-
fication system on wearable devices (e.g., eyeglasses) based
on the collected body-surface vibrations to defend against
various speech attacks. However, these approaches either
require the phone to be held closely to the speaker’s mouth
or require the user to wear eyeglasses while operating the
systems, which largely restricts their application scenarios.
Speech Effect on Motion Sensor. Existing studies have
shown that the MEMS sensor attributes and structures could
be easily interfered by ambient sound and noise [11, 13, 14].
WALNUT [27] modeled the physics of acoustic injection
attacks on MEMS accelerometers, and showed that the out-
puts of sensors are subjected to the acoustic interference.
In addition, researchers showed that embedded MEMS mo-
tion sensors have the possibility of detecting hotwords [33]
or even recognizing speech information [5, 22]. Moreover,
Gyrophone [22] showed that gyroscope can be used to mea-
sure acoustic signals from a loudspeaker to reveal speaker
information (e.g., gender and identity). A more recent work,
Speechless [5] went a step further to evaluate the necessary
conditions and scenarios for the speaker information leakage
problem. It showed that the recorded effect on the motion
sensors is from conductive vibrations through a shared sur-
face between the speaker and sensor.
Since an attacker could easily tamper the human speech
pattern in the audio domain to fool the system, in this paper,
we take a different approach by using the unique speech fea-
tures captured by motion sensors to defend against hidden
voice commands [9, 28]. Particularly, we use the device’s in-
built motion sensor to pick up minute device-body vibrations
to record the unique imprint of acoustic vibrations. Unlike
the features in the audio domain, vibration features of the
user’s speech that are associated with both speech and de-
vice’s airborne structure, are unique and hard to be imitated
or synthesized by a spoofing attack. Our approach can either
work as a stand-alone authentication mechanism or be seam-
lessly integrated with existing voice authentication systems,
forming a two-modality or multi-modality authentication
protocol.
3 APPROACH OVERVIEW
3.1 Background on Hidden Voice
Commands
A typical speech recognition system usually requires four
steps to recognize each voice command: pre-processing,
feature extraction, model-based recognition and post-
processing. Pre-processing contains speech/non-speech seg-
mentation which removes background noise causing inser-
tions of phonemes or words into the recognition result. Fea-
ture extraction extracts acoustic observations, Mel-frequency
cepstral coefficients (MFCC) [20, 29], over time frames of
uniform length. In the model-based recognition phase, the
system uses the acoustic models, such as Hidden Markov
Figure 1: Workflow of generating hidden voice com-
mand [9] from a normal voice command.
Models (HMM) and recurrent neural networks (RNNs), to
predict a sequence of words that are most likely to match the
extracted acoustic features. In the latter, the system employs
additional sources of information (e.g., grammar rules) to
improve the recognition accuracy.
In order to generate hidden voice commands to spoof voice
recognition system, an adversary could use general acoustic
processing methods to generate obfuscated commands with
acoustic features (e.g., MFCC) that can be correctly recog-
nized by the system [9]. As shown in Figure 1, the adversary
first extracts commonly used acoustic features (i.e., MFCC)
from a normal command, and then performs inverse MFCC
to convert the extracted MFCC features back to an audio
sample. Through this step, the generated audio sample only
contains audio features that are used in the speech recogni-
tion system while disregarding other features that might be
helpful for human’s comprehension.
The MFCC parameters in the MFCC feature extraction
determine the resolution of the extracted feature, which play
an important role on the generated audio sample’s capability
of being recognized by a human/machine. The parameters
include the number of cepstral coefficients, the number of
warped spectral bands, the length and stride of the sliding
window. The higher dimension of the MFCC features could
make the generated audio sample to have a higher probability
of being recognized, while lower feature dimension could
make the generated audio file more obfuscated. To use the
perception gap between human and machine to create hidden
voice commands, the adversary needs to iteratively adjust
these MFCC parameters to check whether the generated
commands could be recognized by the system but hardly
recognized by the victim. Through such an iterative testing,
the adversary could find an optimal set of parameters to
make the generated command recognizable by machines
while remaining incomprehensible to humans.
MFCCFeatureExtractionInverseMFCCAdjustingMFCCparametersNormalvoicecommandCandidateobfuscatedcommandSpeechrecognitionsystemRecognizedbythesystemRecognizedbyhumanattackerYesNoYesHiddenvoicecommandNo3.2 Attack Model
We target the defense against hidden voice commands. We
assume the adversary does not have the capability to com-
promise the voice assistant systems and can only use hidden
voice commands to access the system. The hidden voice
commands can be embedded in the audio tracks of regular
media (e.g., Youtube videos and Podcast) and played by the
target device’s built-in speaker to deliver the hidden voice
command (internal attack). Moreover, the hidden voice com-
mands can also be played by an adversary via a loudspeaker
near the target device to launch the attack (external attack).
In the first scenario, the attacker tricks the victim into
playing the audio with hidden voice commands on their own
device (e.g., smartphone and standalone assistant). We term
this scenario as internal attack because the targeted device
itself plays the hidden voice commands (in contrast to an ex-
ternal loudspeaker). This scenario requires the attacker into
either fooling the victim into playing the audio containing
hidden voice commands or by using a malicious application
that can play the audio with hidden voice commands, while
being inconspicuous to the victim. To fool the victim into
playing the audio, the attacker could send an audio or a
video file to the victim under the pretense of a benign mes-
sage. A malicious website could embed auto-playing videos
on their web page that could play (unprompted) the hid-
den voice commands while navigating the web page by the
victim. Similarly, a malicious application could play audio
with hidden voice commands under the disguise of a gam-
ing application. Diao et al. [15] designed an attack termed
“GVS-Attack” against Google Voice Assistant, that launched
the voice assistant (using VoicEmployer malware) and then
played standard voice commands. These voice commands
were then faithfully executed by the voice assistant.
The other scenario, where the attacker uses an external
loudspeaker to propagate these hidden voice commands can
be termed as external attack where the attacker has the capa-
bility to exploit any nearby loudspeaker that is in the vicinity
of a single or multiple targeted devices. For example, an un-
suspecting victim could be sitting with their smartphone,
in a coffee shop where music is being played through loud-
speakers. The attacker could connect to the audio system of
the coffee shop to play his own curated music that has mali-
cious voice commands embedded in it. The attacker could
also be in physical proximity of the user and play an audio
with hidden voice commands through his own loudspeaker
device.
3.3 System Overview
The basic idea of our system is to analyze the speech features
that are captured in the vibration domain to detect hidden
voice commands. Unlike audio-based voice command au-
thentication techniques, vibration domain features are much
harder to forge as the vibrations captured by the device’s
motion sensors are nonlinear responses to the sound and are
affected not only by the played back voice commands but also
by the physical vibration properties of the device itself. Such
vibration domain features can be used as a stand-alone com-
mand authentication mechanism as well as multi-modality
authentication protocol in conjunction with audio-based at-
tack defense. Particularly, our system is triggered by the
“wake word” received by the microphone of the VCS device
(e.g., a smartphone or a standalone voice assistant device).
Then the received voice command (either from an external
loudspeaker or the built-in speaker) that follows after the
wake word could be played back through two alternative
ways based on the user’s preference. In the frontend playback,
the system directly plays back the received voice commands
using the user’s own device, and the built-in motion sensors
(i.e., in a smartphone) or the on-board motion sensors (i.e.,
on a standalone VCS device) record the sound vibrations.
From the user’s view, this playback is the confirmation of
their voice command, while from our design perspective, this
is an assurance of the VCS security in coping with hidden
voice commands. In the backend palyback, an alternative
option for the user, our system plays back the received voice
command through a remote loudspeaker in the cloud service,
and the on-board motion sensors of the cloud loudspeaker to
record the resulted sound vibrations. The recorded motion
sensor data is fed as input to the system to detect the hidden
voice commands. The entire process is simultaneous with the
command context processing, which is 2 seconds for Google
and 4 seconds for Siri service, for instance [6]. Thus, there
would be no additional delay required by our system.
The flow of our proposed system is shown in Figure 2. Par-
ticularly, our system first performs Vibration Data Calibration
including vibration noise removal and voice command seg-
mentation to remove the noises caused by the mechanical
vibrations and obtain the precise vibration data segment of
the voice command. Based on the calibrated voice command
segment, Vibration Feature Derivation derives the unique
vibration features including the time and frequency statisti-
cal features (e.g., mean, standard deviation, energy) and the
speech features (e.g., MFCCs and chroma vectors). We found
these vibration features are effective to capture the unique
statistical traits and acoustic characteristics in the low fre-
quency range of the motion sensor data (e.g., 200Hz). The
system further performs Vibration Feature Selection through
feature normalization and statistical analysis to identify a
subset of features. The selected features exhibit more discrim-
inative patterns between the normal commands and hidden
voice commands and are relatively independent from the
various people’s voices and command contexts.
Figure 2: Overview of the proposed system.
Hidden Voice Command Detection component then lever-
ages either a supervised learning-based classifier or an unsu-
pervised learning-based classifier to detect the hidden voice
command based on their unique vibration features. In partic-
ular, the supervised learning-based classifier, such as Simple
Logistic, Sequential Minimal Optimization (SMO), Random
Forest and Random Tree can effectively distinguish the two
types of sounds based on the acoustic profile trained with
labeled voice command samples. In comparison, the unsu-
pervised learning methods such as K-means and K-medoid
do not require much training effort and directly divide the
two types of the sound into two clusters based on their in-
herent speech characteristics shown in the vibration domain
without any labeled inputs. When verifying the input voice
command, the unsupervised learning model calculates the
Euclidean distance of a vibration feature vector to the nor-
mal command cluster centroid and applies a threshold-based
methods to make the decision. The hidden voice command
is identified if the Euclidean distance is greater than the
threshold.
4 PREMISE & FEASIBILITY ANALYSIS
4.1 Capturing Voice Using Motion Sensors
Motion sensors are typically used to measure the movement
of an object in a given direction (accelerometer) or its rota-
tion around an axis (gyroscope). Many commodity devices
(e.g., smartphones) are equipped with a miniaturized version
of these motion sensors (i.e., MEMS motion sensors). An au-
dio signal can be considered as a vibration of air molecules
with a vibrating frequency that lies within the human audi-
ble range (20 Hz - 20 kHz). Anand et al. [5] showed that the
vibrating air molecules, forming the human speech, were
unable to affect the MEMS motion sensors, especially the
accelerometer, of a smartphone. The human speech can how-
ever invoke a response in the MEMS motion sensors, if it is
replayed via a loudspeaker in the vicinity of the smartphone,
when they share a solid surface with it [5, 22]. This is because
the vibrations of the inbuilt diaphragm of a loudspeaker prop-
agate along the shared surface towards the motion sensors
to which they transfer their vibrations. However, the motion
sensors on these devices are limited to a very low sampling
rate (e.g., approximately 200 Hz) and can only capture lim-
ited speech information. Since the fundamental frequency of
a male voice lies between 85 to 180 Hz and that of a female
voice lies between 165 to 255 Hz [7, 26], these low sampling
rate motion sensors are still able to capture some features of
the fundamental frequency contained in the speech signal.
Recent studies [22, 33] have shown that the fundamental
frequency information captured by the gyroscope and the
accelerometer on smartphones could be further used to per-
form speaker classification and hot-word detection with a
sufficiently high degree of accuracy. In this work, we lever-
age the speech vibrations that can be captured by the motion
sensors to distinguish hidden voice commands from nor-
mal commands. We find that the mobile devices’ built-in
motion sensors and the standalone VCS devices’ on-board
motion sensors can both capture the speech vibrations of
their played back speech.
4.2 Nonlinear Vibration Responses
When using the MEMS motion sensors of the VCS devices to
capture speech vibrations of an audio, their low sampling rate
(e.g., 200Hz) could lead to aliased vibration signals. The signal
aliasing is a phenomenon when different frequencies of an
 !!"#"$%&"’"$()"*+,-./0,1$*’,%-(2"*’3$"(4"$,5*’,%-6,&"72$"83"-!9(4%&*,-(:’*’,/’,!*#(2"*’3$"/ !%3/’,!(2"*’3$"/;(=?$%&*(0"!’%$@0,1$*’,%-(A%,/"()"&%5*#0%,!"(=%&&*-+(:".&"-’*’,%-4*’*(=*#,1$*’,%-:’*’,/’,!*#( -*#9/,/1*/"+:"#"!’,%-0,1$*’,%-(2"*’3$"(:"#"!’,%-2"*’3$"(A%$&*#,B*’,%-C,++"-(0%,!"(=%&&*-+(4"’"!’,%-:3D"$5,/"+(E"*$-,-.F1*/"+(=#*//,G,"$(H-/3D"$5,/"+(E"*$-,-.F1*/"+(=#*//,G,"$( !"#$%& !"#’()’*)"+,#-.(/)&0)1*234$%’("-5(6#&04$%’("-76##2$%-’"-+(I#*91*!JK*!J"-+(I#*91*!J !"#$%&’%(#)%&!*&+!,%-!. !/#!0&1%02!*13%45%*6%3$47&’%(#)%&#0&8$!9.&1%*(#)%(a) Played audio
(b) Vibration responses
Figure 3: Aliased responses of a acoustic signal in vi-
bration domain.
(a) Down-sampled mic data
(b) Accelerometer data
Figure 4: Spectrogram comparison between the ac-
celerometer data and the down-sampled microphone