**Author：LoRexxar'@Knownsec 404 Team**  
**Date: September 23, 2020**  
**Chinese Version:**
Since mankind invented tools, mankind has been exploring how to do anything
more conveniently and quickly. In the process of scientific and technological
development, mankind keeps on trial，error, and thinking. So there is a great
modern technological era. In the security field, every security researcher is
constantly exploring how to automatically solve security problems in various
fields in the process of research. Among them, automated code auditing is the
most important part of security automation.
This time we will talk about the development history of automated code audit.
And by the way, we will talk about the key to an automated static code audit.
# Automated code audit
Before talking about automated code audit tools, we must first understand two
concepts, **missing alarm rate** and **false alarm rate**. - **missing alarm
rate** refers to vulnerabilities/Bugs not found. - **False alarm rate** refers
to the wrong vulnerabilities/Bugs found.
When evaluating all the following automated code audit tools/ideas/concepts,
all evaluation criteria are inseparable from these two points. How to
eliminate the two points or one of them is also the key point in the
development of automated code auditing.
We can simply divide automated code audits (here we are discussing white
boxes) into two categories. One is dynamic code audit tools, and the other is
static code audit tools.
# Features and limitations of dynamic code audit
The principle of the dynamic code audit tool is mainly based on **the code
running process**. We generally call it IAST (Interactive Application Security
Testing).
One of the most common ways is to hook malicious functions or underlying APIs
in some way, and use front-end crawlers to determine whether the malicious
functions are triggered to confirm the vulnerability.
We can understand this process through a simple flowchart.
In the process of front-end Fuzz, if the Hook function is triggered and meets
a certain condition,we will think the vulnerability exists.
The advantage of this type of scanning tool is that the vulnerabilities
discovered have a low rate of false positives, and do not rely on code.
Generally speaking, triggering a rule means that we can perform malicious
actions. And being able to track dynamic calls is also one of the main
advantages of this method.
But there are many disadvantages (1) The front-end Fuzz crawler can guarantee
the coverage of normal functions, but it is difficult to guarantee the
coverage of code functions.
If you have used dynamic code audit tools to scan a large number of codes, it
is not difficult to find that the scan results of such tools for
vulnerabilities will not have any advantages over pure black box vulnerability
scanning tools. The biggest problem is mainly the coverage of code functions.
Generally speaking, it is difficult for you to guarantee that all the
developed codes serve the features of the website. Perhaps the redundant code
is left behind during the old version, or the developers did not realize that
they wrote the code below will not just execute as expected. There are too
many vulnerabilities that cannot be discovered directly from the front-end
features. And some functions may need to be triggered by specific environments
and specific requests. In this way, code coverage cannot be guaranteed. So how
can it be guaranteed that vulnerabilities can be found?
(2) Dynamic code auditing depends on the underlying environment and check
strategies
Since the vulnerability identification of dynamic code audit mainly relies on
Hook malicious functions, for different languages and different platforms,
dynamic code audit often needs to design different hook schemes. If the depth
of the hook is not enough, a depth frame may not be able to scan.
Take PHP as an example. The more mature Hook solution is implemented through a
PHP plug-in. Such like:
  * 
Due to this reason, general dynamic code audits rarely scan multiple languages
at the same time, and generally target a certain language.
Second, Hook's strategy also requires many different restrictions. Take PHP's
XSS as an example. It does not mean that a request that triggers the echo
function should be identified as XSS. Similarly, in order not to affect the
normal function, it is not that the echo function parameter contains
`` to be considered an XSS vulnerability. In the dynamic code audit
strategy, a more reasonable front-end -> Hook strategy discrimination scheme
is required, otherwise a large number of false positives will occur.
In addition to the previous problems, the strong dependence on the
environment, the demand for execution efficiency, and the difficulty of
integrating with business code also exist. When the shortcomings of dynamic
code auditing are constantly exposed, from the author's point of view, dynamic
code auditing has conflicts between the principle itself and the problem, so
in the development process of automation tools, more and more eyes are put
back to the Static code audit (SAST).
# The development of static code audit tools
The static code audit mainly analyzes the target code, analyzes and processes
through pure static means, and explores the corresponding
vulnerabilities/Bugs.
Different from dynamic, static code audit tools have undergone a long-term
development and evolution process. Let's review them together (the relative
development period mainly represented by each period below is not relatively
absolute before and after birth):
## Keyword Match
If I ask you "If you were asked to design an automated code audit tool, how
would you design it?", I believe you will answer me that you can try to match
keywords. Then you will quickly realize the problem of keyword matching.
Here we take PHP as a simple example.
Although we matched this simple vulnerability, we quickly discovered that
things were not that simple.
Maybe you said that you can re-match this problem with simple keywords.
    \beval\(\$
Unfortunately, as a security researcher, you will never know how developers
write code. So if you choose to use keyword matching, you will face two
choices:
  * High coverage
The most classic of this type of tool is Seay, which uses simple keywords to
match more likely targets. And then users can further confirm through manual
audits.
    \beval\b\(
-High availability
The most classic of these tools is the free version of Rips.
    \beval\b\(\$_(GET|POST)
Use more regulars to constrain and use more rules to cover multiple
situations. This is also the common implementation method of early static
automated code audit tools.
But the problem is obvious. **High coverage and high availability are flaws
that this implementation method can never solve. Not only is the maintenance
cost huge, but the false alarm rate and the missing alarm rate are also
high**. Therefore, being eliminated by the times is also a historical
necessity.
## the AST-based code audit
Some people ignore the problem, while others solve it. The biggest problem
with keyword matching is that you will never be able to guarantee the habits
of developers, and you will not be able to confirm vulnerabilities through any
standard matching. Then the AST-based code audit method was born. Developers
are different, but the compiler is the same.
Before sharing this principle, we can first reproduce the compilation
principle. Take the PHP code example:
With the birth of PHP7, AST also appeared in the compilation process as an
intermediate layer of PHP interpretation and execution.
Through lexical analysis and syntax analysis, we can convert any piece of code
into an AST syntax tree. Common semantic analysis libraries can refer to:
  * 
  * 
When we got an AST syntax tree, we solved the biggest problem of keyword
matching mentioned earlier. At least we now have a unified AST syntax tree for
different codes. How to analyze the AST syntax tree has become the biggest
problem of such tools.
Before understanding how to analyze the AST syntax tree, we must first
understand the three concepts of **information flow, source, and sink**.
  * source: We can simply call it the input, which is the starting point of the information flow.
  * sink: We can call it the output, which is the end of the information flow.
The information flow refers to the process of data flowing from source to
sink.
Put this concept in the PHP code audit process, Source refers to user-controllable input, such as `$_GET, $_POST`, etc. While Sink refers to the
sensitive function we want to find, such as `echo, eval`. If there is a
complete flow from a Source to Sink, then we can think that there is a
controllable vulnerability, which is based on the principle of code auditing
of information flow.
After understanding the basic principles, I will give a few simple examples:
In the above analysis process, Sink is the eval function, and the source is
`$_GET`. Through reverse analysis of the source of Sink, we successfully found
an information flow that flows to Sink, and successfully discovered this
vulnerability.
ps: Of course, some people may be curious about why you choose reverse
analysis flow instead of forward analysis flow. This problem will continue to
penetrate in the subsequent analysis process, and you will gradually
understand its key points.
In the process of analyzing information flow, **clear scope is the base of
analyze.** This is also the key to analyzing information flow, we can take a
look at a simple code.
If we simply go back through the left and right values without considering the
function definition, we can easily define the flow as:
In this way, we mistakenly defined this code as a vul, but obviously not, and
the correct analysis process should be like this:
In this code, from the scope of the main syntax tree to the scope of the Get
function. **how to control the change of this scope is a major difficulty
based on the analysis of the AST syntax tree**. When we cannot in the code
When avoiding the use of recursion to control the scope, the unified standard
in the multi-layer recursion has become the core problem of the analysis.
In fact, even if you handle this simplest core problem, you will encounter
endless problems. Here I give two simple examples.
(1) New function wrapper
This is a very classic piece of code. The sensitive function is wrappered to a
new sensitive function, and the parameters are passed twice. In order to solve
this problem, the direction of the information flow is reverse -> forward.
Control the scope by creating a large scope.
(2) Multiple call chain
This is a JS code with loopholes, it is easy to see the problem manually. But
if you trace back the parameters in an automated way, you will find that there
are multiple flow directions involved in the entire process.
Here I use red and yellow to represent the two flow directions of the flow.