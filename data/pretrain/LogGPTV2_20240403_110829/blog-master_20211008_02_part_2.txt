         9 |   1 |        0 |  
(1 row)  
```  
Not setting the session variable raises an error, setting it to a non-existant tenant just show no rows, and setting it to a tenant filters on its rows.  
## RLS transparency  
We have seen that administration operations done by "saas_owner" are easy as they don't have to manage each tenant individually. It is also easy for the application because, when connected as "saas_user" and with 'rls.tenant_id' set, the application can run the same code as before going multi-tenant.  
Here is how I run pgbench on tenant_id=9 just by setting an environment variable:  
```  
PGOPTIONS="-c rls.tenant_id=9" pgbench -n -U saas_user saas -T 10  
```  
Setting the "tenant_id" for the session is the only thing you have to do. The application reads from the common tables, but the RLS policy restricts the rows, as a Virtual Private Database.  
This is done by adding an implicit WHERE clause to the statements, and that's the reason why you should add the "tenant_id" to all indexes. From this example, I've gathered the queries that were run, from pg_stat_statements.query, and verified the execution plan to be sure that the indexes are used:  
```  
\c saas saas_user  
set rls.tenant_id=9;  
postgres=# explain analyze select count(*) from pgbench_branches;  
                                                     QUERY PLAN  
--------------------------------------------------------------------------------------------------------------------------  
 Aggregate  (cost=16.25..16.26 rows=1 width=8) (actual time=0.412..0.412 rows=1 loops=1)  
   ->  Index Scan using pgbench_branches_pkey on pgbench_branches  (cost=0.00..16.00 rows=100 width=0) (actual time=0.406..0.408 rows=1 loops=  
1)  
         Index Cond: (tenant_id = (current_setting('rls.tenant_id'::text))::integer)  
 Planning Time: 0.539 ms  
 Execution Time: 0.902 ms  
(5 rows)  
postgres=# explain analyze update pgbench_accounts SET abalance = abalance + 1 WHERE aid = 2;  
                                                        QUERY PLAN  
--------------------------------------------------------------------------------------------------------------------------------  
 Update on pgbench_accounts  (cost=0.00..4.12 rows=1 width=768) (actual time=1.203..1.203 rows=0 loops=1)  
   ->  Index Scan using pgbench_accounts_pkey on pgbench_accounts  (cost=0.00..4.12 rows=1 width=768) (actual time=1.124..1.125 rows=1 loops=1  
)  
         Index Cond: ((tenant_id = (current_setting('rls.tenant_id'::text))::integer) AND (aid = 2))  
 Planning Time: 4.667 ms  
 Execution Time: 12.233 ms  
(5 rows)  
```  
The 'Index Cond' is explicit. You see exactly what is executed after the RLS policy is applied and then be sure that the indexes are correctly used.  
## partitions and geo-distribution  
Having all rows stored in the same table is convenient. But maybe you want to physically isolate a few of them. The 80-20 rule often applies here: 80% of small tenants are stored together and 20% of them are important enough to be managed more carefully.  
Having one schema to store all tenants is not a problem in a PostgreSQL compatible database because tables can be partitioned. Here is an example replacing the "pgbench_accounts" created above by a partitioned table where the "tenant_id" 1 and 2 are stored in a different partition:  
```  
\c saas saas_owner  
alter table pgbench_accounts rename to tmp_pgbench_accounts ;  
CREATE TABLE pgbench_accounts (  
 tenant_id int default current_setting('rls.tenant_id')::int not null,  
 aid integer NOT NULL,  
 bid integer,  
 abalance integer,  
 filler character(84),  
 PRIMARY KEY(tenant_id,aid)  
) partition by list (tenant_id );  
CREATE TABLE pgbench_accounts_vip   
partition of pgbench_accounts  (  
 tenant_id, aid, bid, abalance , filler  
) for values in (1,2);  
CREATE TABLE pgbench_accounts_others   
partition of pgbench_accounts  (  
 tenant_id, aid, bid, abalance , filler  
) default;  
```  
When running the same RLS policies with this table definition, here are the execution plans for two tenants that are in a different partition:  
```  
saas=> \c saas saas_user;  
psql (15devel, server 11.2-YB-2.7.2.0-b0)  
You are now connected to database "saas" as user "saas_user".  
saas=> set rls.tenant_id=1;  
SET  
saas=> explain analyze select count(*) from pgbench_accounts;  
                                                          QUERY PLAN  
------------------------------------------------------------------------------------------------------------------------------------  
 Aggregate  (cost=235.00..235.01 rows=1 width=8) (actual time=5.553..5.553 rows=1 loops=1)  
   ->  Append  (cost=0.00..230.00 rows=2000 width=0) (actual time=5.549..5.549 rows=0 loops=1)  
         Subplans Removed: 1  
         ->  Seq Scan on pgbench_accounts_vip  (cost=0.00..110.00 rows=1000 width=0) (actual time=5.548..5.548 rows=0 loops=1)  
               Filter: (tenant_id = (current_setting('rls.tenant_id'::text))::integer)  
 Planning Time: 1.294 ms  
 Execution Time: 5.638 ms  
(7 rows)  
saas=>  
saas=> set rls.tenant_id=0;  
SET  
saas=> explain analyze select count(*) from pgbench_accounts;  
                                                               QUERY PLAN  
----------------------------------------------------------------------------------------------------------------------------------------------  
 Aggregate  (cost=235.00..235.01 rows=1 width=8) (actual time=853.215..853.215 rows=1 loops=1)  
   ->  Append  (cost=0.00..230.00 rows=2000 width=0) (actual time=9.033..843.702 rows=100000 loops=1)  
         Subplans Removed: 1  
         ->  Seq Scan on pgbench_accounts_others  (cost=0.00..110.00 rows=1000 width=0) (actual time=9.032..832.047 rows=100000 loops=1)  
               Filter: (tenant_id = (current_setting('rls.tenant_id'::text))::integer)  
 Planning Time: 0.992 ms  
 Execution Time: 853.290 ms  
(7 rows)  
```  
The beauty of it is that a Seq Scan can still be relevant here because it scans only the required partition.  
In YugabyteDB, you can go further. Partitions can be mapped to tablespaces, and tablespaces to nodes. All this is described in the [Row Level Geo-Partitioning documentation](https://docs.yugabyte.com/latest/explore/multi-region-deployments/row-level-geo-partitioning/). This means that you can control the geo-location of the tenants, or group of tenants, if you want to. This means that you can manage all your tenants in one schema, but still control their physical location.  
## views  
For the moment, the "tenant_id" is visible when querying the table from the saas_user:  
```  
saas=> set rls.tenant_id=9;  
SET  
saas=> select * from pgbench_accounts limit 10;  
 tenant_id | aid | bid | abalance |                                        filler  
----------------+-----+-----+----------+--------------------------------------------------------------------------------------  
         9 |   1 |   1 |        0 |  
         9 |   2 |   1 |        2 |  
         9 |   3 |   1 |        0 |  
         9 |   4 |   1 |        0 |  
         9 |   5 |   1 |        0 |  
         9 |   6 |   1 |        0 |  
         9 |   7 |   1 |        0 |  
         9 |   8 |   1 |        0 |  
         9 |   9 |   1 |        0 |  
         9 |  10 |   1 |        0 |  
(10 rows)  
```  
This should not be a problem as your application lists the columns. select * is handy when querying interactively, but to be avoided in the application because the schema may change.  
Anyway, if you want full transparency, you can create a view on the tables, without the "tenant_id" column.  
```  
\c saas postgres  
create schema saas_user;  
grant create on schema saas_user to saas_user;  
\c saas saas_user  
create view saas_user.pgbench_accounts as select aid, bid, abalance, filler from public.pgbench_accounts;  
```  
Given that the view is first in the "search_path", this is what will be seen by the queries:  
```  
saas=> show search_path;  
   search_path  
----------------------  
 "$user", public  
(1 row)  
saas=> set rls.tenant_id=9;  
SET  
saas=> select * from pgbench_accounts limit 10;  
 aid | bid | abalance |                                        filler  
----------+-----+----------+--------------------------------------------------------------------------------------  
   1 |   1 |        0 |  
   2 |   1 |        2 |  
   3 |   1 |        0 |  
   4 |   1 |        0 |  
   5 |   1 |        0 |  
   6 |   1 |        0 |  
   7 |   1 |        0 |  
   8 |   1 |        0 |  
   9 |   1 |        0 |  
  10 |   1 |        0 |  
(10 rows)  
```  
## optimizer statistics  
Now that you may have a distribution of data that is very different across tenants, you may want to gather statistics to have histograms on the tenant_id:  
```  
\c saas saas_owner  
analyze pgbench_accounts;  
analyze pgbench_branches;  
analyze pgbench_history;  
analyze pgbench_tellers;  
saas=> \x  
Expanded display is on.  
saas=>  
select * from pg_stats where attname='tenant_id';  
-[ RECORD 1 ]----------+------------------------------------------  
schemaname             | public  
tablename              | pgbench_accounts  
attname                | tenant_id  
inherited              | f  
null_frac              | 0  
avg_width              | 4  
n_distinct             | 4  
most_common_vals       | {2,8,3,0}  
most_common_freqs      | {0.279533,0.2726,0.266567,0.1813}  
histogram_bounds       |  
correlation            | 0.252514  
most_common_elems      |  
most_common_elem_freqs |  
elem_count_histogram   |  
-[ RECORD 2 ]----------+------------------------------------------  
schemaname             | public  
tablename              | pgbench_branches  
attname                | tenant_id  
inherited              | f  
null_frac              | 0  
avg_width              | 4  
n_distinct             | -1  
most_common_vals       |  
most_common_freqs      |  
histogram_bounds       | {0,1,2,3,4,5,6,7,8,9}  
correlation            | 0.515152  
most_common_elems      |  
most_common_elem_freqs |  
elem_count_histogram   |  
-[ RECORD 3 ]----------+------------------------------------------  
schemaname             | public  
tablename              | pgbench_tellers  
attname                | tenant_id  
inherited              | f  
null_frac              | 0  
avg_width              | 4  
n_distinct             | 10  
most_common_vals       | {0,1,2,3,4,5,6,7,8,9}  
most_common_freqs      | {0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1}  
histogram_bounds       |  
correlation            | 0.519952  
most_common_elems      |  
most_common_elem_freqs |  
elem_count_histogram   |  
```  
Knowing how I built the data, by copying the same into the 10 tenants, it looks like the sample size should be increased.  
Note that there may be some correlation between the tenant_id and another column. For example, if your tenants are located in specific countries, there is a high correlation between one customer country and the tenant_id. You may want to CREATE STATISTICS on the column group to have the query planner aware of it. We do not support this yet in YugabyteDB. ANALYZE was just introduced in 2.9 recently. Anyway, without per-tenant partitioning, we should aim at Index Scan to benefit from sharding.  
This post shows the RLS solution used in a practical way, with maximum transparency for the application. I'll write soon about the other solutions: isolation per schema, database, cluster, and their limits on resource consumption. However, in all cases, the solution described here can still be split into multiple schemas, with some dedicated to one large tenant and others consolidating many tenants into one. Then having this "tenant_id" column may always be useful. Even if it is hidden behind views, default value and RLS policies, one thing remains: convince the developers that a composite key is not evil. I'll talk about it at the Russian Java User Group [Joker Conf](https://jokerconf.com/en/talks/sql-primary-key-surrogate-key-composite-keys-foreign-keys-and-jpa)  
#### [PostgreSQL 许愿链接](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
您的愿望将传达给PG kernel hacker、数据库厂商等, 帮助提高数据库产品质量和功能, 说不定下一个PG版本就有您提出的功能点. 针对非常好的提议，奖励限量版PG文化衫、纪念品、贴纸、PG热门书籍等，奖品丰富，快来许愿。[开不开森](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216").  
#### [9.9元购买3个月阿里云RDS PostgreSQL实例](https://www.aliyun.com/database/postgresqlactivity "57258f76c37864c6e6d23383d05714ea")
#### [PostgreSQL 解决方案集合](https://yq.aliyun.com/topic/118 "40cff096e9ed7122c512b35d8561d9c8")
#### [德哥 / digoal's github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
#### [PolarDB 学习图谱: 训练营、培训认证、在线互动实验、解决方案、生态合作、写心得拿奖品](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
#### [购买PolarDB云服务折扣活动进行中, 55元起](https://www.aliyun.com/activity/new/polardb-yunparter?userCode=bsb3t4al "e0495c413bedacabb75ff1e880be465a")
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")