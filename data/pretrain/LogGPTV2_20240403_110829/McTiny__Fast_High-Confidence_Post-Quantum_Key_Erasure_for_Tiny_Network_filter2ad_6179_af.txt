×
×
×
++++
×
×
×
×
×
×
++++
×
×
×
×
++++
×
×
×
×
×
×
++++
×
×
×
×
++++
×
×
×
×
×
×
++++
×
×
×
×
×
×
×
×
+++
×
×
++++
×
×
×
×
++++
×
×
×
×
×
×
++++
×
×
×
×
×
×
++++
×
×
×
×
×
×
++++
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
+
××+
××++++++++++
××++×××× ++×××× ++×× ++×××××× ++×× ++++++++++××
0.000
0.117
0.234
0.351
0.468
0.585
0.702
0.819
0.936
1.053
Figure 3: Similar to Figure 2, but telling the Linux TCP stack to use BBR instead of CUBIC.
A.2 Measuring TCP Congestion Control
Figure 2 shows the timings of packets in a typical example
of the experiments mentioned above, sending 1 megabyte
through TCP from the United States to Europe. Each network
packet sent by the server produces a red plus in the ﬁgure.
The vertical position is the time in seconds when the server
sends the packet. The horizontal position is the total num-
ber of bytes in all packets that have been sent, including the
application-layer data (eventually reaching 1 megabyte) and
78 bytes of per-packet overhead: 20-byte TCP header, 20-byte
IPv4 header, 26-byte Ethernet header, and 12 bytes of spacing
between Ethernet packets.
the size of each Ethernet packet without
Beware that packet-tracing tools such as tcpdump
report
the
12 bytes of spacing. Also, TCP segmentation ofﬂoad
means that the kernel gives larger packets to the net-
work card, which then puts
smaller packets on the
wire; packet-tracing tools show the larger packets. We
ran ethtool --offload eth0 tx off rx off to disable
TCP segmentation ofﬂoad, so that the same tools would show
the packets on the wire; we did not ﬁnd any resulting differ-
ences in TCP latency.
Each network packet received by the server produces a blue
cross and a green cross in the ﬁgure, at the time in seconds
when the server receives the packet. These packets acknowl-
edge receipt of various packets sent earlier by the server. The
horizontal position of the blue cross is the total number of
bytes in the acknowledged packets, while the horizontal po-
sition of the green cross is the total number of bytes in the
acknowledgment packets.
At time 0.000, the server receives a SYN packet opening
a connection, and sends a SYNACK packet in response. At
time 0.117, the server receives an ACK packet. After about
0.005 seconds of starting a data-generating application, the
server sends a quick burst of 10 packets. Many more packets
are ready to send, and could be sent given the available band-
width, but the server is not yet conﬁdent about the available
bandwidth.
These 10 packets are acknowledged slightly after time
0.234, prompting the server to send a burst of 20 packets.
The burst size continues ramping up exponentially for a few
more round trips. For example, there is a red burst of about
120000 bytes starting around time 0.468, creating about 0.010
seconds of congestion delay in the network router. These
packets are delivered to the client at about 100Mbps, and the
acknowledgments from the client create a blue burst in the ﬁg-
ure starting around time 0.585 with a slope of about 100Mbps.
This in turn triggers a longer red burst starting around time
0.585 with a slope of about 200Mbps, creating more conges-
tion delay in the router. The difference between red and blue
angles in the ﬁgure reﬂects the difference between 100Mbps
and 200Mbps.
Overall this TCP server sent 769 packets, including 1
packet to accept the connection, 766 packets that each sent
1368 bytes of application-layer data (the maximum amount
the client was willing to accept; note that this was over IPv4
rather than IPv6), 1 packet that sent the remaining 688 bytes
of application-layer data, and 1 packet to acknowledge the
client closing the connection (which this client did not do until
after receiving all the server data). These packets consumed
1 108 566 bytes including per-packet overhead. Meanwhile
the TCP client sent 430 packets, consuming 33 548 bytes
including per-packet overhead. Note that TCP typically ac-
knowledges two packets at once.
by
followed
Figure 2 used CUBIC, the default congestion-control
mechanism in Linux. Figure 3 instead uses BBR [10], a
new congestion-control mechanism from Google; sysctl
net.core.default_qdisc=fq
sysctl
net.ipv4.tcp_congestion_control=bbr enables BBR
under Linux. There are many differences between CUBIC
and BBR, and one of these differences is already visible
just after time 0.117:
instead of sending a burst of 10
packets as quickly as possible, the server sends 5 separated
bursts of 2 packets each. This separation (“packet pacing”
from [34]) reduces the chance of producing immediate
congestion, and in general produces a smoother data ﬂow.
Comparing the ﬁgures also shows that BBR sent slightly
more acknowledgment trafﬁc (590 packets from the client,
USENIX Association
29th USENIX Security Symposium    1747
131072
262144
393216
524288
655360
786432
917504
1048576 1179648 1310720
+
××++++++++++++++++++++×× +×× +×× ++×× +×× +×× +×× ++×× +×× ++×× +×× ++××
++××
+××
++××
++××
++××
++××
+××
++××
++××
++××
++××
++××
××
××
××
××
××
××
××
××
××
++
××
++
××
+
××
++
××
××
++
×
×
++
×
×
++
×
×
×
×
+
×
×
++
×
×
++
×
×
+
×
×
++
×
×
++
×
×
×
×
++
×
×
+
×
×
++
×
×
++
×
×
×
×
+
++
×
×
×
×
++
×
×
+
×
×
++
×
×
×
×
+
++
×
×
×
×
+
×
×
++
×
×
+
×
×
++
×
×
×
×
+
×
×
+
×
×
++
×
×
++
×
×
×
×
+
×
×
++
×
×
+
×
×
++
×
×
++
×
×
×
×
+
×
×
+
×
×
++
×
×
++
×
×
×
×
++
×
×
++
×
×
×
×
++
×
×
+
×
×
++
×
×
+
×
×
++
×
×
+
×
×
++
×
×
++
×
×
×