title:EvilSeed: A Guided Approach to Finding Malicious Web Pages
author:Luca Invernizzi and
Paolo Milani Comparetti
2012 IEEE Symposium on Security and Privacy
EVILSEED: A Guided Approach to Finding Malicious Web Pages
Luca Invernizzi
UC Santa Barbara
PI:EMAIL
Stefano Benvenuti
University of Genova
PI:EMAIL
Marco Cova
Lastline, Inc. and University of Birmingham
PI:EMAIL
Paolo Milani Comparetti
Lastline, Inc. and Vienna Univ. of Technology
PI:EMAIL
Christopher Kruegel
UC Santa Barbara
PI:EMAIL
Giovanni Vigna
UC Santa Barbara
PI:EMAIL
Abstract—Malicious web pages that use drive-by download
attacks or social engineering techniques to install unwanted
software on a user’s computer have become the main avenue
for the propagation of malicious code. To search for malicious
web pages, the ﬁrst step is typically to use a crawler to collect
URLs that are live on the Internet. Then, fast preﬁltering
techniques are employed to reduce the amount of pages that
need to be examined by more precise, but slower, analysis
tools (such as honeyclients). While effective, these techniques
require a substantial amount of resources. A key reason is that
the crawler encounters many pages on the web that are benign,
that is, the “toxicity” of the stream of URLs being analyzed is
low.
In this paper, we present EVILSEED, an approach to search
the web more efﬁciently for pages that are likely malicious.
EVILSEED starts from an initial seed of known, malicious
web pages. Using this seed, our system automatically generates
search engines queries to identify other malicious pages that
are similar or related to the ones in the initial seed. By doing
so, EVILSEED leverages the crawling infrastructure of search
engines to retrieve URLs that are much more likely to be
malicious than a random page on the web. In other words
EVILSEED increases the “toxicity” of the input URL stream.
Also, we envision that the features that EVILSEED presents
could be directly applied by search engines in their preﬁlters.
We have implemented our approach, and we evaluated it on a
large-scale dataset. The results show that EVILSEED is able to
identify malicious web pages more efﬁciently when compared
to crawler-based approaches.
Keywords-Web Security, Drive-By Downloads, Guided Crawl-
ing
I. Introduction
The web has become the medium of choice for people to
search for information, conduct business, and enjoy entertain-
ment. At the same time, the web has also become the primary
platform used by miscreants to attack users. For example,
drive-by-download attacks are a popular choice among bot
herders to grow their botnets. In a drive-by-download attack,
the attacker infects a (usually benign) web site with malicious
code that eventually leads to the exploitation of vulnerabilities
in the web browsers (or plug-ins) of unsuspecting visitors [1],
[2]. If successful, the exploit typically downloads and executes
a malware binary, turning the host into a bot [3].
© 2012, Luca Invernizzi. Under license to IEEE.
DOI 10.1109/SP.2012.33
428
In addition to drive-by-download exploits, cybercriminals
also use social engineering to trick victims into installing or
running untrusted software. As an example, consider a web
page that asks users to install a fake video player that is
presumably necessary to show a video (when, in fact, it is a
malware binary). Another example is fake anti-virus programs.
These programs are spread by web pages that scare users into
thinking that their machine is infected with malware, enticing
them to download and execute an actual piece of malware as
a remedy to the claimed infection- [4], [5].
The web is a very large place, and new pages (both legitimate
and malicious) are added at a daunting pace. Attackers
relentlessly scan for vulnerable hosts that can be exploited and
leveraged to store malicious pages, which are than organized
in complex malicious meshes to maximize the changes that a
user will land on them. As a result, it is a challenging task to
identify malicious pages as they appear on the web. However,
it is critical to succeed at this task in order to protect web users.
For example, one can leverage information about web pages
that compromise visitors to create blacklists. Blacklists prevent
users from accessing malicious content in the ﬁrst place, and
have become a popular defense solution that is supported by all
major browsers. Moreover, the ability to quickly ﬁnd malicious
pages is necessary for vendors of anti-virus products who need
to obtain, as fast as possible, newly released malware samples
to update their signature databases.
Searching for malicious web pages is a three-step process,
in which URLs are ﬁrst collected, then quickly inspected with
fast ﬁlters, and ﬁnally examined in depth using specialized
analyzers. More precisely, one has to ﬁrst collect pointers to
web pages (URLs) that are live on the Internet. To collect
URLs, one typically uses web crawlers, which are programs
traversing the web in a systematic fashion. Starting from a
set of initial pages, this program follows hyperlinks to ﬁnd as
many (different) pages as possible.
Given the set of web pages discovered by a crawler, the
purpose of the second step is to prioritize these URLs for
subsequent, detailed analysis. The number of pages discovered
by a crawler might be too large to allow for in-depth analysis.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:48:16 UTC from IEEE Xplore.  Restrictions apply. 
Thus, one requires a fast, but possibly imprecise, preﬁlter to
quickly discard pages that are very likely to be legitimate. Such
preﬁlters examine static properties of URLs, the HTML code,
and JavaScript functions to compute a score that indicates the
likelihood that a page is malicious. Based on these scores,
pages can be ranked (sorted). For example, according to a
report that describes the system deployed at Google [2], the
company analyzes one billion URLs every day. To handle this
volume, a preﬁlter is deployed that shrinks the number of
pages to be inspected in depth by three orders of magnitude.
For the third step, we require detection systems that can
determine with high accuracy whether a web page is malicious.
To this end, researchers have introduced honeyclients. Some
of these systems use static and/or dynamic analysis techniques
to examine the HTML content of a page as well as its active
elements, such as client-side scripting code (typically JavaScript
scripts or Java applets). The idea is to look for signs of
well-known exploits or anomalous activity associated with
attacks [6]–[8]. Other detection systems look for changes to
the persistent state of the operating system (such as additional
ﬁles or processes) [2], [9], [10] once a page has been loaded.
These changes often occur as a result of a successful exploit
and the subsequent execution of a malware binary. Of course,
detection systems are much more precise than preﬁlters, but
they are also much slower (the analysis of a page can take
seconds; preﬁlters can be several orders of magnitude faster).
The resources for identifying malicious pages are neither
inﬁnite nor free. Thus, it is essential to perform this search-
and-analysis process in an efﬁcient way so that one can ﬁnd
as many malicious pages as possible in a ﬁxed amount of
time.
In this paper, we propose an approach that improves the
efﬁciency of the ﬁrst step of the search process, augmenting
it by opportunistically relying on the data that search engines
collected with their crawlers. More precisely, we propose a
system, called EVILSEED, which complements the (essentially
random) web crawling approach with a guided search for
malicious URLs. EVILSEED starts from a set of known pages
that are involved in malicious activities. This set contains
malicious pages that were directly set up by cybercriminals
to host drive-by-download exploits or scam pages. The set
also includes legitimate pages that were compromised and,
as a result, unknowingly expose users to malicious code or
redirect visitors to attack pages. In the next step, EVILSEED
searches the web for pages that share certain similarities with
the known malicious pages. We call this a “guided search”
of the web, because it is guided by the current knowledge of
known malicious web pages. Of course, these searches are
not guaranteed to return only malicious pages. Thus, it is still
necessary to analyze the search results with both preﬁlters and
honeyclients. However, the key advantage of our approach is
that a result of our guided search is much more likely to be
malicious than a web page found by randomly crawling. Thus,
given a ﬁxed amount of resources, our approach allows us to
ﬁnd more malicious pages, and we do so quicker.
We also believe that EVILSEED would be beneﬁcial to search
engines. Although it is difﬁcult to provide data supporting this
claim, since the details of the infrastructure that search engines
use to create their blacklists are conﬁdential, we have observed
several cases in which EVILSEED led our small cluster to
detect malicious web pages faster then search engines. For
example, in January 2010 EVILSEED identiﬁed a malware
infection campaign with hundreds of URLs that, although
inactive, can still be found by querying Google and Bing for
“calendar about pregnancy”. We observed these two search
engines incrementally, and slowly, blacklisting URLs belonging
to the campaign over the next ten months, until the campaign
was eventually shut down. By applying EVILSEED, the lifetime
of this campaign would have been much reduced, protecting
the users of these engines.
Our approach is built upon two key insights. The ﬁrst one
is that there are similarities between malicious pages on the
web. The reason is that adversaries make use of automation
to manage their campaigns. For example, cybercriminals
search the web for patterns associated with vulnerable web
applications that can be exploited by injecting malicious code
into their pages [11], [12]. Also, cybercriminals use exploit
toolkits to create attack pages [13], and they often link many
compromised pages to a single, malicious site to simplify
management.
The second insight is that there are datasets and tools
available that make it easier to ﬁnd malicious URLs. Most
notably, search engines (such as Google and Bing) have
indexed a large portion of the web, and they make signiﬁcant
investments into their crawler infrastructures to keep their
view of the web up-to-date. We leverage this infrastructure,
as well as other datasets such as passive DNS feeds, for
our guided search process. Note that we do not propose to
completely replace traditional web crawlers when searching for
malicious web pages. Guided search is a process that allows
us to automatically and efﬁciently ﬁnd malicious pages that
are similar to ones that have already been identiﬁed. Random
crawling is still necessary and useful to ﬁnd new pages that
are different than those already known.
In the past, cybercriminals have used search engines to
ﬁnd vulnerable web sites [11], [12], [14], [15]. In particular,
cybercriminals perform manually-crafted search queries to
ﬁnd pages that contain certain keywords that indicate the
presence of vulnerabilities. Previous work has extensively
studied malicious search engine queries. More precisely, re-
searchers have examined search query logs for entries that
were likely to be issued by cybercriminals. Such query strings
can then be extracted (and generalized) as signatures to block
bad queries [11], [12]. In [12], the authors also examine the
results that a search engine returns for malicious queries to
identify potentially vulnerable hosts.
429
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:48:16 UTC from IEEE Xplore.  Restrictions apply. 
In this paper, we do not attempt to identify or study manually-
crafted, malicious queries used by cybercriminals. Instead, we
propose a search process that allows us to ﬁnd malicious pages
that are similar to those previously identiﬁed. As part of this
process, we do submit queries to search engines. However,
these queries are automatically generated, based on the analysis
of different data sources such as a corpus of known, malicious
pages and DNS data feeds (but no access to search engine
logs). Moreover, our searches target a broader range of pages
than those targeted by attackers who use search engines for
locating vulnerable web sites. In particular, we are not only
interested in ﬁnding compromised (vulnerable), legitimate
pages, but also malicious pages that are directly set up by
attackers.
The main contributions of this paper are the following:
• We developed a novel approach to guide the identiﬁcation
of malicious web pages starting from an initial set of
known, malicious web pages.
• We described several novel techniques for the extraction
of features from malicious web pages that can be used in
queries submitted to existing search engines to identify
more malicious web pages.
• We implemented our techniques in a tool and we evaluated
it on a large set of malicious web pages, demonstrating
the approach is effective and improves the state of the
art.
II. System Overview
In this section, we describe in detail the goals of our work,
and we provide a brief overview of the overall approach and
the components of our system.
A. System Goal
As mentioned previously, searching for malicious pages on
the web is a three-step process: Crawl to collect URLs, apply
a fast preﬁlter to discard obviously benign pages, and use a
precise-but-slow oracle to classify the remaining pages. In
this paper, our goal is to improve the efﬁciency of the web
crawling phase. More precisely, we have developed techniques
that allow us to gather URLs that have a higher “toxicity” than
the URLs that can be discovered through (random) crawling.
With toxicity, we refer to the percentage of URLs in a set that
point to malicious web pages.
Our techniques are based on the idea of searching for
pages that are similar to ones that are known to be malicious.
Intuitively, rather than randomly searching for malicious pages,
EVILSEED focuses its searches “near” known malicious pages.
More precisely, EVILSEED implements different techniques
to extract from a page features that characterize its malicious
nature; pages with similar values for such features (the
“neighborhood” of a page) are also likely to be malicious.
Then, by using the features extracted from an evil seed and
by leveraging existing search engines, EVILSEED guides its
search to the neighborhood around known malicious pages.
430
We use the notion of “maliciousness” in a broad sense, and
our general techniques are independent of the exact type of
threat that a particular page constitutes. For the current version
of EVILSEED, we consider as malicious a page that, when
visited, leads to the execution of a drive-by download exploit
(possibly after redirecting the user). In addition, we consider
a page to be malicious when it attempts to trick a user into
installing a fake anti-virus program.
In this paper, we use the term web page and URL
synonymously. That is, the actual inputs to and the “unit
of analysis” of our system are URLs. In most cases, a page is
uniquely identiﬁed by its corresponding URL. However, there
are cases in which attackers create many URLs that all point
to the same, underlying malicious page. In this situation, we
would count each URL as a different, malicious page.
B. System Architecture
The general architecture of EVILSEED is shown in Figure 1.
The core of our system is a set of gadgets. These gadgets
consume a feed of web pages that have been previously
identiﬁed as malicious (as well as other data feeds, such as
domain registration feeds). Based on their input, the gadgets