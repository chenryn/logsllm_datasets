Name
vuln-asn1
vuln-bagle
vuln-dcom
vuln-iis
vuln-kuang2
vuln-lsass
vuln-msdtc
vuln-msmq
Reference
ASN .1 Vulnerability Could Allow Code Execution (MS04-007)
Emulation of backdoor from Bagle worm
Buﬀer Overrun In RPC Interface (MS03-026)
IIS SSL Vulnerability (MS04-011 and CAN-2004-0120)
Emulation of backdoor from Kuang2 worm
LSASS vulnerability (MS04-011 and CAN-2003-0533)
Vulnerabilities in MSDTC Could Allow Remote Code Execution
(MS05-051)
Vulnerability in Message Queuing Could Allow Code Execution
(MS05-017)
Buﬀer Overruns in SQL Server 2000 Resolution Service (MS02-039)
vuln-mssql
vuln-mydoom Emulation of backdoor from myDoom/Novarg worm
vuln-optix
vuln-pnp
Emulation of backdoor from Optix Pro trojan
Vulnerability in Plug and Play Could Allow Remote Code Execution
(MS05-039)
vuln-sasserftpd Sasser Worm FTP Server Buﬀer Overﬂow (OSVDB ID: 6197)
vuln-ssh
vuln-sub7
vuln-wins
Logging of SSH password brute-forcing attacks
Emulation of backdoor from Sub7 trojan
Vulnerability in WINS Could Allow Remote Code Execution
(MS04-045)
This selection of emulated vulnerabilities has proven to be suﬃcient to handle
most of the autonomous spreading malware we have observed in the wild. As
we show in the remainder of this section, these modules allows us to learn more
about the propagating malware. However, if a certain packet ﬂow cannot be han-
dled by any vulnerability module, all collected information is stored on hard disc
to facilitate later analysis. This allows us to detect changes in attack patterns,
is an indicator of new trends, and helps us to develop new modules. In case of
a 0day, i.e., an vulnerability for which no information is publicly available, this
can enable a fast analysis since the ﬁrst stages of the attack have already been
captured. As outlined in Section 2.3, this can also be extended to really handle
0day attacks. A drackback of this approach is that an attacker can send random
data to a network port and nepenthes will store this data on hard disc. This
can lead to a Denial-of-Service condition if the attacker sends large amount of
bogus network traﬃc, however we did not experience any problems up to now.
In addition, this problem can be mitigated by implementing upper bounds on
the amount of traﬃc stored on hard disk.
Developing a new vulnerability modules to emulate a novel security vulnera-
bility or to capture a propagating 0day exploit is a straightforward process and
demands only little eﬀort. On average, writing of less than 500 lines of C++ code
(including comments and blank lines) is required to implement the needed func-
tionality. This task can be carried out with some experience in a short amount
of time, sometimes only requiring a couple of minutes.
176
P. Baecher et al.
As an example, we want to present our experience with the recent Zotob worm:
in MS05-039, Microsoft announced a security vulnerability in the Plug and Play
service of Windows 2000 and Windows XP at August 09, 2005. This vulnerability
is rated critical for Windows 2000 since it allows remote code execution, resulting
in a remote system compromise. Two days later, a proof-of-concept exploit for this
vulnerability was released. This exploit code contains enough information to imple-
ment a vulnerability module for nepenthes, so that malware propagating with the
help of MS05-039 can be captured with this module. Without the proof-of-concept
exploit, it would have been possible to build a vulnerability module only based on
the information provided in the security advisory by Microsoft. But this process
would be more complex since it would require the development of an attack vector,
which could then be emulated as a vulnerability module. Nevertheless, this is fea-
sible. After all, attackers also implemented a proof-of-concept exploit solely on the
basis of the information in the security bulletin. Another three days after the release
of the proof-of-concept exploit – at August 14 – a worm named Zotob started to ex-
ploit this vulnerability in the wild. So only ﬁve days after the release of the security
advisory, the ﬁrst bot propagated with the help of this vulnerability. But at this
point in time, nepenthes was already capable of capturing such a worm. Similarly,
the process of emulating the vulnerability in Microsoft Distributed Transaction
Coordinator (MSDTC), published in Microsoft security bulletin MS05-051, took
only a small amount of time.
3.1 Scalability
In this section, we want to evaluate the scalability of the nepenthes platform.
With the help of several metrics we investigate, how eﬀective our approach is,
and how many honeypot systems we can emulate with our implementation.
As noted in [20], a “key factor to determine the scalability of a honeypot
is the number of honeypots required to handle the traﬃc from a particular IP
address range”. To cover a /16 network, a naive approach would be to install
over 64,000 honeypots to cover the whole network range. This would of course
be a waste of resources, since only a limited amount of IP addresses receives
network traﬃc at any given point in time. The low-interaction honeypot honeyd
is reported to be able to simulate a whole /16 network on just a single computer.
The expressiveness of this tool is low since it only emulates the TCP/IP stack
of an arbitrary operating system. In contrast to this, nepenthes is capable of
emulating several vulnerabilities at application level.
To evaluate the scalability of nepenthes, we have used the following setup: the
testbed is a commercial oﬀ-the-shelf (COTS) system with a 2.4GHz Pentium III,
2 GB of physical memory, and 100 MB Ethernet NIC running Debian Linux 3.0
and version 2.6.12 of the Linux kernel. This system runs nepenthes 0.1.5 in
default conﬁguration. This means that all 21 vulnerability modules are used,
resulting in a total of 29 TCP sockets on which nepenthes emulates vulnerable
services.
We tested the implementation with a varying number of emulated systems,
ranging from only 256 honeypots up to 32,000 emulated honeypots. For each
The Nepenthes Platform: An Eﬃcient Approach to Collect Malware
177
conﬁguration, we measured the number of established TCP connections, the
system load, and the memory consumption of nepenthes, for a time interval
of one hour. We repeated this measurement several times in diﬀerent order to
cancel out statistical unsteadiness. Such an unsteadiness could for example be
caused by diurnal properties of malware epidemics [5] or bursts in the network
traﬃc. The average value of all measurements is then an estimation of the spe-
ciﬁc metric we are interested in. Figures 3 (a) and (b) give an overview of
our results. In each ﬁgure, the x-axis represents the number of IP addresses
assigned to nepenthes running on the testbed machine. The y-axis reprents
the number of established TCP connections (a) and the average system load
(b), respectively. We forbear from plotting the memory consumption since it
is low (less than 20 MB for even a large number of simulated IP addresses),
and nearly independent from the number of established TCP connections. In
the ﬁrst ﬁgure we see that the scalability is nearly linear up to 8,192 IP ad-
dresses. This corresponds to the system load, which is below 1 (ﬁgure b). Af-
terwards, the number of established TCP connections is degreasing, which is
caused by a system load above 1, i.e., the system is fully occupied with I/O
operations.
In the following, we take a closer look at the long-time performance of the ne-
penthes platform emulating a whole /18 network, i.e., about 16,000 IP addresses.
We have this setup up and running for more then ﬁve months and it runs quite
stable. There are seldom kernel crashes, but these are caused by instabilities in
the Linux kernel handling such a large amount of IP addresses in parallel. Apart
from this, nepenthes itself is a mature system. To get an overview of the over-
all performance of this platform, we present some statistics on the performance
ﬁrst. In Figure 4 (a) we see the ﬁve minute average of established TCP connec-
tions for an instance of nepenthes running on a /18 network for 30 hours. The
number of established TCP connections is on average 796, with peaks of up to
1172. The lowest values are around 600 concurrent established connections, so
the volatitlity is rather high. Our experience shows that burst of more than 1300
concurrent established TCP connections are tolerable on this system. Even more
connections could be handled with better hardware: currently, the average load
of the system is slightly above 1, i.e., the processor is never idle. For a one hour
period, we observed more than 180,000 SYN packets, which could potentially be
handled by nepenthes.
Figure 4 (b) depicts the ﬁve minute average of network throughput. Green is
the amount of incoming traﬃc, with an average of 308.8 kB/s and a maximum
of 369.7 kB/s. The outgoing traﬃc is displayed with a blue line. The average of
outgoing traﬃc is 86.6 kB/s, whereas the peak lies at 105.4 kB/s. So despite a
rather high volatility in concurrent TCP connections, the network throughput
is rather stable.
We now take a closer look at the long-time performance of this nepenthes
instance regarding the download of new samples collected. A ﬁve week period is
the data foundation of the following statistics. Figure 5 depicts the daily number
of download attempts and successful downloads.
178
P. Baecher et al.
(a)
(b)
Fig. 3. Number of concurrent established TCP connections (a) and system load (b) in
relation to number of IP addresses assigned to nepenthes
3.2 Statistics for Collected Malware
In this section, we analyze the malware we have collected with our honeynet
platform. Since nepenthes is optimized to collect malware in an automated way,
this is the vast amount of information we collect with the help of this tool. A
human attacker could also try to exploit our honeynet platform, but he would
presumably notice quickly that he is just attacking a low-interaction honeypot
since we only emulate the necessary parts of each vulnerable service and the
command shell only emulates the commands typically issued by malware. So
we concentrate on automated attacks and show how eﬀective and eﬃcient our
approach is.
With the help of the nepenthes platform, we are able to automatically collect
malware on a large-scale basis. We are running nepenthes in several diﬀerent
networks and centrally store the malware we have downloaded. Figure 5 (a) and
(b) show the cumulative number of download attempts and successful downloads
The Nepenthes Platform: An Eﬃcient Approach to Collect Malware
179
(a)
(b)
Fig. 4. Five minute average of established TCP connections (a) and network through-
put (b) for nepenthes running on a /18 network in a period of 33 hours
(a)
(b)
Fig. 5. Number of malware download attempts (a) and successful downloaded ﬁles (b)
for nepenthes running on a /18 network in a period of 33 hours
for a nepenthes platform assigned to a /18 network. Within about 33 hours, more
than 5.5 million exploitation attempts are eﬀectively handled by this system (see
Figure 5 (a)). That means that so often the download modules are triggered to
start a download. Often these download attempts fail, e.g., because the malware
tries to download a copy of itself from a server that is meanwhile taken down.
Figure 5 (b) depicts the number of successful download, i.e., nepenthes success-
fully download a piece of malware. Within these 33 hours, about 1.5 million
binaries are downloaded. Most of these binaries are duplicates, but nepenthes
has to issue a download and is only afterwards able to decide whether the binary
is actually a new one. In this particular period, we were able to download 508
new unique binaries.
In a four month period, we have collected more than 15,500 unique binaries,
corresponding to about 1,400 MB of data. Uniqueness in this context is based on
diﬀerent MD5 sums of the collected binaries. All of the ﬁles we have collected are
180
P. Baecher et al.
Table 2. Detection rates of diﬀerent antivirus engines
Complete set (14,414 binaries)
Latest 24 hours (460 binaries)
AV engine 1 AV engine 2 AV engine 3 AV engine 4
78.1%
73.1%
90.2%
84.1%
85.0%
82.6%
85.3%
77.8%
Table 3. Top ten types of collected malware
Place Name according to ClamAV Number of captured samples
1136
906
698