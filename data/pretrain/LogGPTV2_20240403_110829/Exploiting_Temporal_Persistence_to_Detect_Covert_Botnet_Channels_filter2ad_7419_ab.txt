host, one being port 21, and the other an ephemeral port (say k). Thus,
both (ftp.service.com,21,tcp) and (ftp.service.com,k,tcp) reﬂect the same
service and for the case of FTP. We thus generalize the destination atom
as (ftp.service,com, 21:>1024,tcp). Being able to do this for a larger set of
protocols requires a detailed understanding of the particular application’s
semantics, which is beyond our scope here. In this paper, we use a sim-
ple heuristic that approximates this behavior: if we observe more than 50
ephemeral ports being connected to at the same service, we expand the des-
tination atom to include all ephemeral ports. The rationale here is that if the
service is associated with ephemeral ports, it is likely that we will observe
a port number not seen previously at some time and should simply include
this in the previously created destination atom.
4. Sometimes a single destination host can provide a number of distinct ser-
vices, and in this case, the destination port is suﬃcient to disambiguate
the services from each other, even though they may have similar “service
names”, obtained by a (reverse) DNS lookup.
5. Finally, when the addresses cannot be mapped to names, no summarization
is possible, and we use the destination IP address as the service name.
We now deﬁne our persistence metric, to quantify the lightweight yet regular
communication from end-hosts to destination atoms. We monitor the outgoing
traﬃc from an end-host using a large sliding time window of size W , divided
into n time-slots, each of length s. We term W an observation window, and each
time-slot s as a measurement window (simply, bin). Letting si denote the i-th
slot of size s in W , we have W ≡ [s1, s2, . . . , sn]. The persistence of a destination
atom (d), in the observation window W is deﬁned as:
332
F. Giroire et al.
p(d, W ) =
1
n
n(cid:2)
i=1
1d,si
where 1d,si has a value 1 if the host makes at least one connection to d in the
time-slot si, and 0 otherwise. Thus, a destination atom’s persistence value is
simply the fraction of time slots where at least one connection was observed.
Given a threshold p∗, we say that d is persistent if p(d, W ) > p∗ (otherwise, it is
termed transient).
Because botnets diﬀer from one to another to a great extent, we cannot know
a priori the frequency (using the term loosely) with which a zombie will contact
its C&C server(s). Thus, it is of paramount importance to design a method
that can track persistence over several observation windows simultaneously. Note
that the persistence of an atom depends upon the sizes of the two windows
(W, s) over which it is observed and measured; we use the term timescale to
denote a particular instance of (W, s). In order to capture persistence over many
timescales, we select k overlapping timescales (W 1, s1) ⊂ (W 2, s2) ⊂ . . . ⊂
(W k, sk), where (W 1, s1) is the smallest timescale, and (W k, sk) is the largest.
Here sj denotes the bin size at time scale j. (We could deﬁne sj
i as the ith slot in
an observation window W j, however we drop the subscript for simplicity when
discussing timescales). For each timescale (W j, sj) : 1 ≤ j ≤ k, we compute the
persistence p(j)(d) as previously deﬁned. Then, a destination atom d is persistent
if the threshold p∗ is exceeded in any one of the timescales, i.e., d is a persistent
destination atom iﬀ
p(j)(d) > p∗
max
j
We have explicitly chosen not to use direct frequency type measurements (e.g.,
a low pass ﬁlter) because our deﬁnition is very ﬂexible and does not require one to
track speciﬁc multiples of one frequency or another. More importantly, we don’t
expect these low frequency connection events to precisely align at any particular
frequency; our deﬁnition allows some slack in where exactly the events occur.
When deciding upon the appropriate timescales, particularly the smallest
measurement window s1, we want it capture session level behavior (multiple
requests to a web server in a short interval are likely to be for a single session).
Based on a preliminary analysis of user data, we select s1 = 1 hr (from em-
pirical data, we see that 87% of connections to the same destination atom are
separated by at least an hour). We also set sk = 24 hours because our train-
ing dataset is 2 weeks (in reality, one could use larger windows). With these
two boundary slot-lengths, we select a number of intermediate values (listed in
§ 5.2). The observation window length controls how long we should wait before
we conclude as to whether something is persistent (or transient). For conve-
nience, we select n = 10. Noting that W = n × s, the 7 timescales used in this
paper lie between (W min = 10, smin = 1), which is the smallest timescale, and
(W max = 240, smax = 24), the largest (all values described in hours). It should
be pointed out that additional timescales can be added dynamically based on
evidence of some anomaly at a particular timescale.
Exploiting Temporal Persistence to Detect Covert Botnet Channels
333
In the following, we describe the speciﬁcs of how our C&C detection proceeds.
Initially, there is a training stage in which the end-host bootstraps itself by learn-
ing a whitelist of destination atoms. The training stage should last long enough
for the stable behavior of the end-host to be exposed (perhaps a week or two).
Subsequent to the training stage, after the whitelists are built up and system
parameters initialized, the system enters the detection stage. It should be noted
that the training and detection stages proceed identically; in both, persistence of
destinations is tracked and alarms raised when this crosses a speciﬁed threshold.
The fundamental diﬀerence is that in the detection stage, an alarm simply re-
sults in the destination atom being incorporated into the whitelist; on the other
hand, in the detection stage, the alarm alerts the end-user (or communicated
to the central IT console in an enterprise) and asked to take punitive action.
In case the alarm is benign, and the user (or administrator) can attest to the
destination atom, it is added into the whitelist.
C&C Detection Implementation. To simplify the description, we ﬁrst de-
scribe how detection is carried out with a single timescale. The system proceeds
by observing all the outgoing traﬃc at an end-host; connections to destination
atoms already in the whitelist are ignored. Persistence values are tracked for
all other outgoing traﬃc: connections to a destination atom in a window W , is
tracked using a bitmap of n bits (one bit for each timeslot in s ∈ W ). If a new
outgoing connection is observed in slot si, then the entry in the bitmap, for the
corresponding slot, is set to 1. We create a separate bitmap for each destination
atom when it is ﬁrst observed. This bitmap updating occurs asynchronously as
the outgoing connections are observed. A timer ﬁres every smin minutes (this is
the time interval corresponding to the smallest timescale), and all the bitmaps
are processed. Here, for each bitmap, the persistence is computed taking into
account the last n bits and at this time, one of three things can occur: (i) it
cannot be determined if the persistence is high enough (not enough samples),
and the bitmap is retained; (ii) the newly updated persistence crosses the crit-
ical threshold, p∗ (bitmap is freed and an alarm is raised), or (iii) after enough
samples, the persistence is below the threshold (the bitmap is freed up).
In order to track persistence at multiple timescales simultaneously, we could
use k separate bitmaps per atom. It turns out this is not necessary because we
can exploit the structure in our deﬁnition of timescales to reduce the overhead.
Notice that s1 · n = W 1 < W 2 < . . . < W k = n · sk, that is, sj is covered by a
slot in the next higher timescale, as is depicted in Fig. 1. Thus, setting a bit in
one of these timescales implies setting a bit in the higher timescale. Thus, rather
that maintain separate bitmaps, we can simply construct a single, long bitmap
that covers all the timescales appropriately; this allows all the timescales to be
updated with a single bit update. The length of this bitmap is W k
smin . In
our implementation we have s1 = 1 hr, n = 10, and W max = 240 hrs, so the
bitmap length is exactly 240 bits.
s1 = W max
High level pseudocode for this entire process is shown in Proc. 1. Here, the
set of bitmaps is stored in DCT, indexed by individual atoms (line 1 retrieves all
334
F. Giroire et al.
Fig. 1. Bitmaps to track connections at each timescale. Here, we have n = 3 and k = 3.
computePersistence():
Algorithm 1.
1: for all d ∈ DCT do
p(d) ← 0
2:
for i = 1 to k do
3:
4:
p(i)(d) ← getBits(d, i). |W i|
|si|
p(d) = max(p(d), p(i)(d))
if p(d) ≥ p∗
then
RAISEALARM(... suspicious destination d)
5:
6:
7:
8:
9:
10:
11:
12:
end if
13:
14: end for
end if
end for
idx ← (idx + 1)mod W max
if p(d) = 0 then
discard DCT[d]
smin
the active bitmaps). The loop (lines 2-7) iterates over each timescale, computing
persistence in each. There is a separate process that processes each outgoing
connection; this checks if the destination is whitelisted, and if not, updates the
bit at index idx in the bitmap (this index is updated in line 10, each time the
procedure is called, i.e., every smin). Finally, if there is no observed activity for
the atom in W max, the bitmap is discarded (lines 11-13).
When a C&C alarm is raised, we ﬂag the outgoing connection as suspicious
and alert the user who can choose between either adding the destination atom to
their whitelist, or blocking the outgoing traﬃc. We will see in our evaluation that
the users are bothered with such decisions infrequently. For enterprise users, such
alarms could also be passed to an IT department where they can be correlated
with other data.
4 Dataset Description
End Host Traﬃc Traces. The endhost dataset used in this paper consists of
traﬃc traces collected at over 350 enterprise users’ hosts (mostly laptops), over
a 5 week period. Users were recruited via intranet mailing lists and newsletters,
Exploiting Temporal Persistence to Detect Covert Botnet Channels
335
and prizes were oﬀered as a way to incentivize participation. The results pre-
sented in this paper use the traces from 157 of the hosts; these were selected
because they provide trace data for a common 4 week period between January
and February 2007. Our monitoring tool collected all packets headers, both in-
going and outgoing, from all machine interfaces (wired and wireless). We divide
the 4 weeks traﬃc trace into two halves, a training set and a testing set. The
training data is used to build the per-user whitelists and to determine the system
parameters (i.e. p∗). The testing data is used to assess the detection performance,
i.e., false positives and false negatives.
Botnet Traﬃc Traces. We collected 55 botnet binaries randomly selected
from a larger corpus of malware. Each binary was executed inside a Windows
XP SP2 virtual machine and run for as long as a week, together with our trace
collection tool. When constructing the clean VM image, we took great pains to
turn oﬀ all the services that are known to generate traﬃc (windows auto-update,
etc.) This gives us a certain level of conﬁdence that all (or nearly all) the traﬃc
collected corresponds to botnet activity. During the collection, the server hosting
the VMs was placed behind a NAT device and connected to an active DSL link.
While we expected the trace collection to be a straight-forward exercise, this
turned out not to be the case. To begin with, a lot of the binaries simply crashed
the VM or else did nothing (no traﬃc was observed). In other cases, the C&C
seemed to have been deactivated, and we only saw failed connections or connec-
tions to illegal addresses (indicating that the DNS entries for the C&C servers
had been rewired). Only 27 binaries yielded any traﬃc at all (the collection was
aborted if there was no traﬃc seen even after 48 hours). From the larger set
of 55 , only 12 binaries yielded traﬃc that lasted more than a day and the de-
tails of these (usable) traces are enumerated in the ﬁrst column of Table 2. The
labels here are obtained using the open source ClamAV scanning engine [15].
Given our limited infrastructure, we were unable to scale to a large number of
binaries; however, we endeavored to collect botnet samples with widely diﬀerent
behaviors and temporal characteristics to assure us that our evaluation results
hold for a larger population of botnets.
To evaluate our detection method, we overlaid these botnet traces on the
traﬃc traces from each user, and then run this combined traﬃc through our
detector which tries to identify persistent destination atoms. In order to assess
the true detections, missed detections and false positives, we ﬁrst needed to ob-
tain the ground truth from the botnet traces (for which there are no established
methods). We had to manually analyze all of our 12 botnet traces in order to
isolate the C&C traﬃc from the remainder of the attack traﬃc. Rather than
label individual packets we used BRO [16] to generate connection summaries
which we analyzed in detail.
Isolating the C&C traﬃc turned out to be a very tedious process which in-
volved manually breaking down the traﬃc across ports and destinations of each
botnet trace. Due to a lack of space, we cannot enumerate how we did this for
the entire set. In summary, we employed a variety of methods including, extract-
ing IRC commands embedded in the streams, looking at the payloads directly
336
F. Giroire et al.
Table 2. List of sampled Botnet binaries with clear identiﬁable C&C traﬃc
ClamAV Signature C&C type
# of C&C atoms C&C Volume
Trojan.Aimbot-25
Trojan.Wootbot-247
Trojan.Gobot.T
Trojan.Codbot-14
Trojan.Aimbot-5
Trojan.IRCBot-776*
Trojan.VB-666*
Trojan.IRC-Script-50
Trojan.Spybot-248
Trojan.MyBot-8926
Trojan.IRC.Zapchast-11 IRC ports 6666, 6667
Trojan.Peed-69 [Storm] P2P/Overnet
port 22
IRC port 12347
IRC port 66659
IRC port 6667
IRC via http proxy
HTTP
IRC port 6667
IRC ports 6662-6669,9999,7000
port 9305
IRC port 7007
1
4
1
2
3
16