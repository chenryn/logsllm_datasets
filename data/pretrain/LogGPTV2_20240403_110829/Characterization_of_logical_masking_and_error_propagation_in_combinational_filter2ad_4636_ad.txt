### Representative Microprocessors and Simulation Mode
The microprocessor under study is representative of modern, general-purpose processors. It was operated in a mode that assumed a perfect cache to emulate the behavior more typical of embedded processors rather than general-purpose ones. The impact of a perfect cache is that memory accesses are completed in a single cycle during the simulation, but it does not affect the system's vulnerability to combinational logic errors.

### Choice of Workload
The impact of any micro-architectural structure on system-level vulnerability can be highly dependent on the workload running on it. Therefore, the choice of workload is a critical component of the system model. Several factors drive this choice, including the suitability of the workload as a representative in the system model. From a practical standpoint, if the workload is too large, simulation times can become prohibitively long, especially for large fault campaigns. Given that fault injection is a statistical experiment, a large number of samples are needed to achieve high confidence in the estimated parameters.

### Determining Experiment Outcomes
When using fault injection to estimate vulnerability, it is essential to quickly and precisely determine the outcome of each experiment. This is because a vulnerability estimate measures the correctness of the output in the presence of faults. After execution, automation scripts compare the workload output with the gold (expected) output. If the workload completes but produces an error, the experiment is flagged as silent data corruption (SDC). In some experiments, the microarchitecture flags an exception (EXCP), which prematurely terminates execution. These exceptions, while potentially detectable, are counted as vulnerable outcomes. In a few cases, the injected error causes the workload to enter an infinite loop (INF), and these experiments are terminated if the simulation time exceeds ten times the fault-free case. Experiments where the injected fault has no effect on the outcome are flagged as unACE (unaffected by the architectural correct execution).

### Emulating Errors in Combinational Blocks
This section describes how a fault-to-error model developed using FIsim (detailed in Section V-A) for a combinational block was applied as injected errors in the high-level system model. For demonstration, a 32-bit Kogge-Stone adder was implemented in the ALU block of PTLsim's microarchitecture. PTLsim was augmented with a saboteur module that introduces errors at the ALU output based on the fault-to-error model and fault space evaluated for the 32-bit Kogge-Stone adder. Errors were injected at the output of randomly chosen instructions that use the adder circuit (e.g., ADD, ADDA, SUB, etc.). Not all instructions use the ALU, so an additional derating factor, representing the fraction of instructions using the adder, is required. This factor, which depends heavily on the workload, can be obtained from a profile of the instruction stream. For the benchmarks tested, the average instruction-mix distribution is around 60%.

### Results
This section presents the results from micro-architecture fault injection experiments, where errors in an adder circuit were reproduced to estimate their impact on system-level vulnerability. As mentioned in Section V-A, a large fraction (∼60%) of errors within an adder manifest as 1-bit errors at its output. About 5% appear as 2-bit errors, and ∼1.7% as 3-bit errors. These error multiplicities were tested at the system level by introducing error patterns from Figures 3(b)–3(d). A uniform distribution was assumed for the position of single errors at the adder output, with 50% probability for each of the patterns (x, x+1) and (x, x+2) for two-bit errors. A uniform distribution of the position, with the pattern (x, x+1, x+2), was used for three-bit error patterns.

1000 error patterns for each multiplicity were applied to each of the fourteen benchmarks in the MiBench suite. Figure 4 shows the total vulnerability for the microarchitecture in the presence of errors at the adder output, summed across three different vulnerable outcomes (SDC, INF, and EXCP). The 90% confidence interval for the total vulnerability estimate is shown as error bars for each fault campaign (1000 experiments per benchmark-multiplicity combination). The average confidence interval at the 90% confidence level is ±2.56%. Figure 4 indicates that the introduction of error multiplicity at the adder output has little impact on the overall vulnerability. For this microarchitecture and adder circuit, the presence of any error at the adder output, rather than the magnitude of the error, contributes to system-level vulnerability.

### Sensitivity Analysis
To determine the sensitivity of system vulnerability to the bit position of single errors at the adder output, analyses were run on selected workloads. The results for three benchmarks are shown in Figure 5. As expected, the system is somewhat more vulnerable to errors at more significant bit positions, but the differences are not substantial. Reevaluating system vulnerability to single-bit adder errors using the bit position distribution in Figure 3(b) did not reveal significant changes from the results in Figure 4. The revised vulnerabilities all fall within the 90% confidence interval for each benchmark: string (48.52 to 50.54), bmath (61.38 to 62.51), and dijkstra (47.80 to 50.02). Similar to the number of bit errors, this system's vulnerability to adder errors is not very sensitive to the magnitude of those errors (related to the bit position of the error). However, this may differ for other systems, and the presented methodology can provide the most accurate vulnerability assessment.

### Discussion and Conclusions
Electrical and latching window masking describe how soft errors remain contained within combinational blocks. These effects are well-understood, and their corresponding derating factors can be easily evaluated when assessing system vulnerability. Logical masking, the third type, describes the phenomenon where an error is logically irrelevant for certain input conditions. This effect is harder to model and quantify due to the large number of input and fault combinations in even moderately large circuits.

This paper describes a hierarchical approach combined with statistical fault injection to address this problem. At the system level, a conservative estimate of 100% vulnerability is initially assumed for underlying combinational blocks if implementation details are unavailable. When details are available, a novel FIsim compiler converts the circuit description into a fault injection simulator. Iterative execution provides a statistical estimate of the circuit's vulnerability, assuming a uniform distribution of inputs. FIsim can also generate a model describing how faults within the combinational block manifest at its output in terms of error pattern distributions.

The utility of the FIsim compiler is demonstrated on all ten circuits in the ISCAS85 suite, Kogge-Stone adders of widths ranging from 4 to 64 bits, and a 16-bit multiplier. Results in Figure 2 show that different circuits exhibit vastly different vulnerability characteristics, ranging from 4% to 90%. This highlights the importance of such analysis in designing reliable systems.

Fault injection, often disregarded due to its statistical nature and the overhead of collecting large amounts of data, is shown to be feasible with FIsim. The time required to iterate FIsim 1000 times for each net in the circuits (3513 and 1154 nets for the ISCAS85 and SPICE circuit sets, respectively) did not exceed two days, even with unoptimized code. These experiments are data-parallel and can be distributed across multiple high-performance hardware units for speedup. Similar arguments hold for fault injection simulations at the system level.

A fault-to-error model developed using FIsim for a 32-bit Kogge-Stone adder was used as input for higher-level fault injection to estimate its impact on system-level vulnerability. This model revealed that the total number of observed error patterns forms a small fraction of all possible error patterns at the adder output, simplifying the set of error patterns needed for higher-level simulations. The relative contributions of different multiplicities are given by distributions like the one in Figure 3(a), and the vulnerability to a specific multiplicity is obtained from high-level fault injections (e.g., Figure 4).

For the microarchitecture and combinational blocks demonstrated in this paper, the effects of error multiplicity and bit position have little impact on system-level vulnerability. However, this may differ for other block-system-application combinations, and the approach in this paper can be used to accurately estimate it, informing the development of potential fault tolerance techniques.

### Acknowledgements
The authors thank Sudhanva Gurumurthi for his invaluable feedback and the anonymous reviewers for their comments and suggestions. They also acknowledge the USNRC for enabling the completion of this work.

### References
[1] P. Shivakumar, M. Kistler, S. W. Keckler, D. Burger, and L. Alvisi, “Modeling the Effect of Technology Trends on the Soft Error Rate of Combinational Logic,” in International Conference on Dependable Systems and Networks, 2002, pp. 389–398.
[2] P. Lid´en, P. Dahlgren, R. Johansson, and J. Karlsson, “On Latching Probability of Particle Induced Transients in Combinational Networks,” in International Symposium on Fault-Tolerant Computing, 1994, pp. 340–349.
[3] M. Baze and S. Buchner, “Attenuation of Single Event Induced Pulses in CMOS Combinational Logic,” IEEE Transactions on Nuclear Science, vol. 44, no. 6, pp. 2217–2223, 1997.
[4] L. Massengill, A. Baranski, D. Van Nort, J. Meng, and B. Bhuva, “Analysis of Single-Event Effects in Combinational Logic—Simulation of the AM2901 Bitslice Processor,” IEEE Transactions on Nuclear Science, vol. 47, no. 6, pp. 2609–2615, 2002.
[5] S. Krishnaswamy, S. Plaza, I. Markov, and J. Hayes, “Signature-based SER Analysis and Design of Logic Circuits,” IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 28, no. 1, pp. 74–86, 2009.
[6] I. Polian, S. Reddy, and B. Becker, “Scalable Calculation of Logical Masking Effects for Selective Hardening Against Soft Errors,” in IEEE Computer Society Annual Symposium on VLSI, 2008, pp. 257–262.
[7] F. Brglez and H. Fujiwara, “A Neutral Netlist of 10 Combinational Benchmark Circuits and a Target Translator in Fortran,” in International Symposium on Circuits and Systems, 1985, pp. 695–698.
[8] “www.aoki.ecei.tohoku.ac.jp/arith, Arithmetic Module Generator, Aoki Laboratory, Tohoku University.”
[9] S. S. Mukherjee, C. Weaver, J. Emer, S. K. Reinhardt, and T. Austin, “A Systematic Methodology to Compute the Architectural Vulnerability Factors for a High-Performance Microprocessor,” in International Symposium on Microarchitecture, 2003, pp. 29–40.
[10] A. Biswas, P. Racunas, R. Cheveresan, J. Emer, S. Mukherjee, and R. Rangan, “Computing Architectural Vulnerability Factors for Address-Based Structures,” in International Symposium on Computer Architecture, 2005, pp. 532–543.
[11] N. J. Wang, A. Mahesri, and S. J. Patel, “Examining ACE Analysis Reliability Estimates Using Fault-Injection,” SIGARCH Computer Architecture News, vol. 35, no. 2, pp. 460–469, 2007.
[12] N. George, C. Elks, B. Johnson, and J. Lach, “Transient Fault Models and AVF Estimation Revisited,” in International Conference on Dependable Systems and Networks, 2010, pp. 477–486.
[13] M. Yourst, “PTLsim: A Cycle Accurate Full System x86-64 Microarchitectural Simulator,” in International Symposium on Performance Analysis of Systems and Software, 2007, pp. 23–34.
[14] M. Guthaus, J. Ringenberg, D. Ernst, T. Austin, T. Mudge, and R. Brown, “MiBench: A Free, Commercially Representative Embedded Benchmark Suite,” in International Workshop on Workload Characterization, 2001, pp. 3–14.