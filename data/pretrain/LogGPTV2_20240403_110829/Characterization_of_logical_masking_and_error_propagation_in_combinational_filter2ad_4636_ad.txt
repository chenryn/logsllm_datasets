and is therefore representative of modern general purpose
microprocessors. It was operated in a mode that assumed a
perfect cache to emulate behavior that is more representative
of embedded processors that
than
general purpose microprocessors. The impact of a perfect
cache is that accesses to memory are completed in a single
cycle during the simulation, but it does not affect the system
vulnerability to combinational logic errors.
2) Choice of Workload: The impact
that any micro-
architecture structure has on system-level vulnerability can
be highly dependent on the workload that is running on it.
For this reason, workload forms an important component
of the system model. The choice of workload is therefore
driven by a number of factors, including suitability as a rep-
resentative workload in the system model. From a practical
point of view, if the workload is too large, simulation times
can be prohibitively long for running large fault campaigns.
Given that fault injection is a statistical experiment, a large
number of samples are needed for a high conﬁdence in the
estimated parameters.
Further, when using fault injection to estimate vulnera-
bility, it is imperative to be able to quickly and precisely
determine the outcome of an experiment. This is because
a vulnerability estimate is an estimate of the correctness
of output in the presence of faults. The easiest approach
After completion of execution,
the automation scripts
compared the output of the workload with the gold output. If
the workload ran to completion but produced an error in the
output, that experiment was ﬂagged as silent data corruption
(SDC). In a number of experiments, an exception (EXCP)
was ﬂagged by the microarchitecture, which prematurely
terminated execution. These are potentially detectable, but
because it has an impact on the outcome of the experiment,
it is counted as required for architecturally correct execution
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:43:27 UTC from IEEE Xplore.  Restrictions apply. 
331and the experiment is therefore counted vulnerable. In a few
experiments per campaign, the injected error resulted in the
workload to enter an inﬁnite (INF) loop and would not ter-
minate in the expected simulation time. These experiments
were killed if the simulation time exceeded ten times the
time required to simulate the fault-free case. Experiments in
which the injected fault had no effect on the outcome were
ﬂagged unACE.
B. Emulating Errors in Combinational Blocks
This subsection describes how a fault-to-error model
developed using FIsim (elaborated in Section V-A) for a
combinational block was applied as injected errors in the
high-level system model. This is demonstrated assuming that
a 32-bit Kogge-Stone adder is the adder circuit implemented
in the ALU block in the microarchitecture of PTLsim.
PTLsim was augmented with a saboteur module, which
when enabled introduced an error at the output of the ALU.
The error that it chooses to inject was based on the fault-
to-error model and fault space evaluated using FIsim for a
32-bit Kogge-Stone adder. The speciﬁc adder topology was
chosen for the purpose of demonstration. Nonetheless, any
adder topology may be used.
Errors were injected at the output of randomly chosen
instructions that make use of the adder circuit (e.g. ADD,
ADDA, SUB, ...). This is because not all randomly selected
instructions make use of the ALU and, speciﬁcally,
the
adder, and would result in wasted simulation effort (i.e., the
bits would already be known to be unACE). This assumption
requires an additional derating factor, which represents a
fraction of instructions that make use of the adder relative
to the total number of instructions. This term would depend
heavily on the workload and can easily be obtained from a
proﬁle of the instruction stream. For the benchmarks tested
here, a distribution of the instruction-mix is available in [14]
and is around 60% on an average across all benchmarks.
C. Results
This section describes the results obtained from micro-
architecture fault injection experiments, where errors in an
adder circuit were reproduced to estimate its impact on
system-level vulnerability. Section V-A alluded to the fact
that a large fraction (∼60%) of errors within an adder
manifest as 1 bit errors at its output. About 5% appear
as 2 bit errors and ∼1.7% as 3 bit errors. These error
multiplicities were tested at the system level by introducing
error patterns from Figures 3(b)–3(d). A uniform distribution
was assumed for the position of single errors at the adder
output. 50% probability for the occurrence of each of the
patterns (x, x+1) and (x, x+2) was assumed for two bit errors.
A uniform distribution of the position, with the pattern (x,
x+1, x+2), was used for three bit error patterns.
1000 error patterns for each multiplicity were applied
to each of the fourteen benchmarks in the MiBench suite.
Figure 4 shows results from these experiments. It shows the
total vulnerability for the microarchitecture in the presence
of errors at the adder output for various benchmark appli-
cations as the sum of three different vulnerable outcomes
(SDC, INF and EXCP). The 90% conﬁdence interval for
the total vulnerability estimate is shown as error bars for
each fault campaign (1000 experiments per benchmark-
multiplicity combination). The average conﬁdence interval
at 90% conﬁdence level is ±2.56%. It is clear from Figure
4 that there is little impact by introducing the notion of error
multiplicity at the adder output. For this microarchitecture
and the adder circuit, it is apparent that rather than the
magnitude of error, it is the fact that any error is present
at the adder output that contributes to vulnerability at the
system level. This is evident from the fact that the vulnerabil-
ity estimate under the assumption of each fault multiplicity
is within the 90% conﬁdence interval for the vulnerability
estimates for most benchmarks.
the trend shows that
As stated above, this analysis assumed a uniform dis-
tribution for the bit position of single errors at the adder
output. However, the results in Figure 3(b) show that not
to be the case. Therefore, analyses were also run on these
workloads to determine the sensitivity of system vulnera-
bility to single bit error position at the adder output. The
results for three benchmarks are shown in Figure 5. As
might be expected,
the system is
somewhat more vulnerable to errors at more signiﬁcant bit
positions, but the differences are not large. As a result,
a reevaluation of system vulnerability to single bit adder
errors using the bit position distribution in Figure 3(b)
does not reveal any signiﬁcant changes from the results in
Figure 4. In fact, the revised vulnerabilities all fall within
the 90% conﬁdence interval for each benchmark – string:
48.52⇒50.54, bmath: 61.38⇒62.51, dijkstra: 47.80⇒50.02.
As was the case for the number of bit errors, this particular
system’s vulnerability to adder errors under these workloads
is not very sensitive to the magnitude of those errors (directly
related to the bit position of the error). However, this may be
different for other systems, and the methodology presented
here can be used to provide the most accurate vulnerability
assessment.
VII. DISCUSSION AND CONCLUSIONS
Electrical and latching window masking describe how
soft errors physically remain contained within combinational
blocks. These effects are fairly well-understood and their
corresponding derating factors can easily be evaluated when
assessing system vulnerability to such errors. The third
masking effect
that shields combinational blocks from
propagating errors is logical masking, which describes the
phenomenon where an error is logically irrelevant for certain
input conditions. This effect
is comparatively harder to
model and quantify because the sheer number of input and
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:43:27 UTC from IEEE Xplore.  Restrictions apply. 
332Figure 4. System-level vulnerability to ALU adder errors for benchmarks and error multiplicities
Figure 5. System-level vulnerability to ALU adder errors at each bit position. Results from 1000 fault injections per bit position.
fault combinations in even moderately large circuits can
make evaluating them prohibitively expensive.
This paper described a hierarchical approach combined
with statistical fault injection as an approach to tackling
this problem. At the system level, a conservative estimate
of 100% is initially assumed for the vulnerability of the
underlying combinational blocks, if implementation details
are unavailable. When implementation details become avail-
able, a novel FIsim compiler is used to convert the circuit
description of a combinational block into a fault injection
simulator, which when iteratively executed, a statistical
estimate of the circuit’s vulnerability is obtained assuming a
uniform distribution of inputs. Further, FIsim can be used to
generate a model that describes how faults originating within
the combinational block manifest at its output in terms of
distributions of observed error patterns.
Utility of the FIsim compiler is demonstrated on all ten
circuits in the ISCAS85 suite, Kogge-Stone adders of widths
ranging from 4 to 64 bits, and a 16-bit multiplier. It is evident
from the results in Figure 2 that different circuits exhibit
vastly different vulnerability characteristics. Vulnerability
estimates ranged from as low as 4% to as high as 90% for
the circuits tested. This emphasizes the importance of this
type of analysis when designing a system for reliability.
Fault
tool
because of its statistical nature and overhead involved in
injection is often disregarded as a useful
collecting large amounts of data in order to ensure statistical
signiﬁcance. However, the time required to iterate FIsim
1000 times for each net in these circuits (3513 and 1154 nets
for the ISCAS85 and SPICE circuit sets, respectively) did
not exceed two days (∼3.5M, 1.1M total runs), even though
the FIsim compiler generated simulator code is largely
unoptimized. Moreover, these experiments are data-parallel
and can easily be separated onto multiple high-performance
hardware units if speed up is desired. Similar arguments hold
true even for the fault injection simulations at the system
level.
A fault-to-error model developed using FIsim for a 32-bit
Kogge-Stone adder was used as input to fault injection at a
higher-level simulation to estimate its impact on the system-
level vulnerability. This model revealed that the total number
of observed error patterns forms a very small fraction of all
possible error patterns at an adder output. This observation
simpliﬁes the set of errors patterns that need to be applied
at the higher level and conﬁnes it to one, two and three
bit errors that follow a regular pattern for the Kogge-Stone
adder. When evaluating the vulnerability of the system to
different multiplicities, their relative contributions are given
by distributions such as the one shown in Figure 3(a) and
the vulnerability to a speciﬁc multiplicity is obtained from
fault injections at the high level (e.g. Figure 4). For the
microarchitecture and combinational blocks demonstrated in
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:43:27 UTC from IEEE Xplore.  Restrictions apply. 
0!10!20!30!40!50!60!70!80!90!1bu!2bu!3bu!1bu!2bu!3bu!1bu!2bu!3bu!1bu!2bu!3bu!1bu!2bu!3bu!1bu!2bu!3bu!1bu!2bu!3bu!1bu!2bu!3bu!1bu!2bu!3bu!1bu!2bu!3bu!1bu!2bu!3bu!1bu!2bu!3bu!1bu!2bu!3bu!1bu!2bu!3bu!adpcmc!adpcmd!jpegc!jpegd!bmath!dijkstra!fft!ffti!gsmt!gsmu!patricia!string!susan!mad!Vulnerability (%)!Benchmark/Error multiplicity!EXCP!INF!SDC!310510152025803040506070Bit positionVulnerability (%)dijkstrabmathstring333this paper, the effects of error multiplicity and bit position
have little impact on the vulnerability at the system level.
Nonetheless, it could be different for other block-system-
application combinations, and the approach in this paper
can be used to accurately estimate it, thereby the informing
the development of potential fault tolerance techniques to
mitigate such faults.
ACKNOWLEDGEMENTS
The authors thank Sudhanva Gurumurthi for his invaluable
feedback and the anonymous reviewers for their comments
and suggestions. The authors also acknowledge the USNRC
for enabling completion of this work.
REFERENCES
[1] P. Shivakumar, M. Kistler, S. W. Keckler, D. Burger, and
L. Alvisi, “Modeling the Effect of Technology Trends on the
Soft Error Rate of Combinational Logic,” in International
Conference on Dependable Systems and Networks, 2002, pp.
389–398.
[2] P. Lid´en, P. Dahlgren, R. Johansson, and J. Karlsson, “On
Latching Probability of Particle Induced Transients in Combi-
national Networks,” in International Symposium on Fault-
Tolerant Computing, 1994, pp. 340–349.
[3] M. Baze and S. Buchner, “Attenuation of Single Event
Induced Pulses in CMOS Combinational Logic,” IEEE Trans-
actions on Nuclear Science, vol. 44, no. 6, pp. 2217–2223,
1997.
[4] L. Massengill, A. Baranski, D. Van Nort, J. Meng, and
B. Bhuva, “Analysis of Single-Event Effects in Combinational
Logic-Simulation of the AM2901 Bitslice Processor,” IEEE
Transactions on Nuclear Science, vol. 47, no. 6, pp. 2609–
2615, 2002.
[5] S. Krishnaswamy, S. Plaza,
I. Markov, and J. Hayes,
“Signature-based SER Analysis and Design of Logic Cir-
cuits,” IEEE Transactions on Computer-Aided Design of
Integrated Circuits and Systems, vol. 28, no. 1, pp. 74–86,
2009.
[6] I. Polian, S. Reddy, and B. Becker, “Scalable Calculation of
Logical Masking Effects for Selective Hardening Against Soft
Errors,” in IEEE Computer Society Annual Symposium on
VLSI, 2008, pp. 257–262.
[7] F. Brglez and H. Fujiwara, “A Neutral Netlist of 10 Combi-
national Benchmark Circuits and a Target Translator in For-
tran,” in International Symposium on Circuits and Systems,
1985, pp. 695–698.
[8] “www.aoki.ecei.tohoku.ac.jp/arith, Arithmetic Module Gener-
ator, Aoki Laboratory, Tohoku University.”
[9] S. S. Mukherjee, C. Weaver, J. Emer, S. K. Reinhardt,
and T. Austin, “A Systematic Methodology to Compute the
Architectural Vulnerability Factors for a High-Performance
Microprocessor,” in International Symposium on Microarchi-
tecture, 2003, pp. 29–40.
[10] A. Biswas, P. Racunas, R. Cheveresan, J. Emer, S. Mukher-
jee, and R. Rangan, “Computing Architectural Vulnerability
Factors for Address-Based Structures,” in International Sym-
posium on Computer Architecture, 2005, pp. 532–543.
[11] N. J. Wang, A. Mahesri, and S. J. Patel, “Examining
ACE Analysis Reliability Estimates Using Fault-Injection,”
SIGARCH Computer Architecture News, vol. 35, no. 2, pp.
460–469, 2007.
[12] N. George, C. Elks, B. Johnson, and J. Lach, “Transient
Fault Models and AVF Estimation Revisited,” in International
Conference on Dependable Systems and Networks, 2010, pp.
477–486.
[13] M. Yourst, “PTLsim: A Cycle Accurate Full System x86-64
Microarchitectural Simulator,” in International Symposium on
Performance Analysis of Systems and Software, 2007, pp. 23–
34.
[14] M. Guthaus, J. Ringenberg, D. Ernst, T. Austin, T. Mudge,
and R. Brown, “MiBench: A Free, Commercially Representa-
tive Embedded Benchmark Suite,” in International Workshop
on Workload Characterization, 2001, pp. 3–14.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:43:27 UTC from IEEE Xplore.  Restrictions apply. 
334