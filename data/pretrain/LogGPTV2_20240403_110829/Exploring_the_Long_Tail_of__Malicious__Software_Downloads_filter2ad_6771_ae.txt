98
6,078
Adware
5,597
32,590
6,757
199
16,957
PUP
8,905
29,216
6,343
499
8,329
Undefined
26,108
93,644
18,473
1,044
36,402
Overall
adware=66.24%, pup=9.97%, 
trojan=6.65%, dropper=2.91%, 
banker=0.13%, bot=0.03% 
(undefined=14.07%)
adware=58.64%, pup=22.91%, 
trojan=6.30%, dropper=4.57%, 
ransomware=0.02%, bot=0.01%, 
banker=0.01%, fakeav=0.01% 
(undefined=7.54%)
adware=6.52%, pup=5.53%, 
dropper=3.77%, trojan=3.36%, 
banker=0.36%, bot=0.22%, 
worm=0.06%, ransomware=0.04%, 
spyware=0.04%, fakeav=0.01% 
(undefined=80.09%)
adware=39.04%, pup=14.18%, 
trojan=10.97%, dropper=7.14%, 
banker=1.94%, bot=0.90%, 
ransomware=0.39%, worm=0.18%, 
fakeav=0.11%, spyware=0.02% 
(undefined=25.13%)
Table 12. Download behavior of different types of malicious processes
The results in Table 12 indicate that processes of a specific malware type download other malwares of 
the  same  type  in  majority  of  cases.  However,  some  malware  types  had  some  unexpected  download 
behaviors. For example, many malware types, even the most specific ones, such as ransomware, fakeav, 
etc., seem to download completely different malware types. The reason behind this depends on how the 
malware operates on the system and its intention. For example, a fakeav could lure victims into buying 
other  things,  but  it  could  simultaneously  drop  another  piece  of  malware  to  take  full  advantage  of  the 
victim. One thing that is clear, however, is that if a machine is initially infected with a somewhat less-
dangerous malware such as adware and PUP applications, there is a good chance that the machine gets 
infected with more aggressive and damaging malware.
27 | Exploring the Long Tail of (Malicious) Software Downloads
From Adware/PUP to Malware 
Adware  and  PUPs  are  often  considered  “less  damaging”  malware.  In  fact,  PUP  stands  for  potentially 
unwanted  program  (also  known  as  potentially  unwanted  application,  or  PUA).  However,  some  studies 
(e.g.,  [21])  have  suggested  that  running  adware/PUPs  increases  the  chances  that  a  machine  will  be 
later  infected  with  more  damaging  malware  (e.g.,  ransomware,  bots,  etc.).  In  this  section,  we  provide 
measurements that aim to support this suspicion quantitatively.
First, we analyze the results reported in Table 12, which shows that both adware and PUP processes 
tend to mostly download other adware or PUP software. However, it also shows that more than 6% of 
the downloaded executable files for both adware and PUP processes are trojans. In addition, almost 3% 
of the files downloaded by adware are droppers, whereas the same figure goes up to 4.57% for PUPs. 
Furthermore, both adware and PUPs in some cases directly download ransomware, bankers, and other 
dangerous malware.
Besides  direct  downloads,  adware/PUP  process  could  also  be  the  cause  of  indirect  infections.  For 
instance, adware processes often display ads from low-reputation ad networks, thus exposing users to 
malvertisements [21]. Consequently, if a user clicks on a malicious ad, she may be redirected, via her 
default web browser, to downloading other malware [11]. To include these indirect downloads into our 
analysis, we proceed as follows. Let m be a machine that has downloaded and executed an adware/PUP 
at time t1. We then check if, after t1, m downloads and executes other types of malicious software (thus 
excluding other adware, PUP, and undefined malicious files). We repeat this process for each machine m 
that ran adware/PUP software and compute the time delta between the adware/PUP infection and the 
download of other types of malware. Figure 5 shows a CDF for the obtained results. As we can see, more 
than 40% of these machines download and execute other malware on the same day (day 0) in which they 
downloaded and executed the adware/PUP software. After only five days from the execution of adware/
PUP, the number of those machines infected with other malware types exceed 55%. On the contrary, 
let’s consider the same measurements for machines that at a given time t1 download a benign software 
(and was not observed to have downloaded malicious files in the past). What we aim to show is that if a 
machine does not run adware/PUPs, it is much less likely to download malware in the immediate future. 
On the same Figure (“benign” line), after five days from the benign software download event, only 20% of 
the machines downloaded malicious files (excluding adware and PUPs, for comparison with “PUP” and 
“adware” lines).
28 | Exploring the Long Tail of (Malicious) Software Downloads
Figure 5. Time delta between downloading benign/adware/pup/dropper and other malware
Dropper-driven Malware Infections 
Droppers  play  a  significant  role  in  malware  infections  [10].  To  provide  additional  information  on  the 
behavior of malicious dropper processes, we proceed in a way similar to Section 5.2. For instance, we 
measure how long it takes for droppers to infect users. To this end, we compute the time gap between the 
first time a machine downloads (and executes) a dropper and a subsequent malware download. Notice 
that we excluded adware, PUPs, and undefined types from this measurement so that we can compare 
the results directly with the transition between adware/PUPs to other malware types discussed above.
Figure 5 (dashed red line) reports our results. As anticipated, a machine that is infected with a dropper 
is almost certain to download and execute malware within the next days. In particular, by comparing the 
dropper, adware, and pup curves in Figures 5, we can see that there is a much shorter time gap between 
the download of a dropper and another malware, compared to the download of an adware/PUP and then 
another malware.
29 | Exploring the Long Tail of (Malicious) Software Downloads
6. Exploring and Labeling 
Unknown Files
As reported in Section 2 (see Table 1), the majority of file downloads (about 83%) are unknown, in that no 
ground truth is available about their true nature, even two years after they were first observed. As these 
unknown files involve a significant number of users who downloaded them (69% of all machines in our 
data downloaded some unknown files), it is of utmost importance to be able to provide a reason for at 
least about some of them. In fact, if these unknown files were malicious, they would have infected the 
vast majority of the machine population. This section explores the characteristics of unknown files. We 
also aim to build a rule-based classifier that can accurately label a significant fraction of these unknown 
files as either malicious or benign.
6.1. Exploring the Characteristics of Unknown Files 
Table 13 shows the top 10 domains from which unknown files were downloaded, whereas Figure 6 plots 
the  distribution  of  the  Alexa  rank  of  all  domains  hosting  unknown  files.  Table  14  shows  what  benign 
processes tend to download most of these files. Naturally, most unknown files are downloaded via web 
browsers. However, we can see that a large number of unknowns are downloaded by Windows processes 
as well. This is alarming, if we consider that Table 10 also shows that a large majority of downloaded files 
by Windows processes for which ground truth is available are actually malicious. Take Acrobat Reader as 
an extreme example (again, from Table 10). Of the 960 downloaded files, 696 are known to be malicious, 
and none are known to be benign. This means that all of the remaining 264 unknowns (reported in Table 
14) are also highly likely malicious.
30 | Exploring the Long Tail of (Malicious) Software Downloads
Domain
# of 
Downloads
inbox.com
humipapp.com
bestdownload-manager.com
freepdf-converter.com
coolrom.com
soft32.com
gamehouse.com
arcadefrontier.com
driverupdate.net
zilliontoolkitusa.info
75,946
43,365
37,398
32,276
27,833
27,229
24,498
24,191
21,370
19,550
Table 13. Top 10 Download Domains
Downloading process 
type
Browser
Windows
Java
Acrobat Reader
Other benign processes
Total
# of 
Unknowns
1,120,855
368,925
227
264
36,059
1,486,961
Table 14. Categories of Download Processes
31 | Exploring the Long Tail of (Malicious) Software Downloads
Figure 6. Distribution of the Alexa ranks of domains hosting unknown files
6.2. Labeling Unknown Files
During our analysis, we noticed that in some cases a simple analysis of the properties of unknown files 
would allow us to identify, with high confidence, their true nature. For instance, an executable file that is 
signed by a software signer that in the past has signed many malicious files but no benign software is also 
likely malicious. Conversely, an executable file that is signed by a reputable software developer, which has 
exclusively signed benign files in the past, is very likely benign. Similarly, a file that is packed with a packer/
obfuscation tool that is known to be used exclusively to protect malicious files from AV detection is likely 
malicious. Overall, we have identified a set of eight intuitive and easy-to-measure features, summarized 
in Table 15, that we can use to label many in-the-wild unknown file downloads with high accuracy. The 
following table presents a novel rule-based classification system that uses these features to mine past file 
download events and automatically extract simple human-readable file classification rules.
32 | Exploring the Long Tail of (Malicious) Software Downloads
Feature
Explanation
File’s signer
File’s CA
File’s packer
Process’s signer
Process’s CA
Process’s packer
Process’s type
Download domain’s Alexa rank
The entity who signed a downloaded file.
The certification authority in the chain of trust of signers for 
the downloaded file
The packer software used to pack the downloaded file, if any
The signer of the process that downloaded the file
The CA of the process that downloaded the file
The packer software used to pack the downloading process
The type of process that downloaded the file (browser, 
windows process, etc. )
The Alexa rank of the domain from which the file was 
downloaded
Table 15. Features Description
Ttr
Overall # 
of rules
τ
Selected 
rules
Rules composition
# of 
Benign
# of 
Malicious
Feb
Mar
Apr
1,766
1,680
1,272
May
1,476
Jun
Jul
944
1,376
0.0%
0.1%
0.0%
0.1%
0.0%
0.1%
0.0%
0.1%
0.0%
0.1%
0.0%
0.1%
1,020
1,031
1,148
1,162
1,054
1,070
974
986
740
753
937
953
889
894
970
976
872
875
791
793
577
585
755
763
131
137
178
186
182
195
183
193
163
168
182
190
Table 16. Statistical information about extracted rules during different Ttr
33 | Exploring the Long Tail of (Malicious) Software Downloads
6.3. Generating Human-Readable Classification Rules
Recently, the authors of [3] explored the importance of interpretability in machine learning systems and 
suggested that the decisions of such systems should be explainable. To this end, we aim to generate 
simple human-readable classification rules and proceeded as follows. First, we use past file download 
observations whose ground truth is known as a training dataset. Then, we use the PART rule learning 
algorithm [4] to derive a set of human-readable classification rules based on the features reported in Table 
15. Finally, we prune the classification rules output by PART to only retain highly accurate rules (i.e., rules 
with low error rate). 
Unlike other machine learning algorithms (e.g., support vector machines (SVMs), neural networks, etc.), 
this approach generates easy-to-interpret classification rules that can be reviewed and modified by threat 
analysts. The following is an example of a simple classification rule based on the described features: 
IF (file’s signer is “Shanghai Gaoxin Computer System Co.”) AND 
(file is packed by “NSIS”) → file is malicious.
This rule was learned from more than 50 instances of malicious file downloads, and does not match any 
of the tens of thousands of benign downloads we observed.
6.4. Evaluation of Classification Rules
To  systematically  evaluate  the  efficacy  of  the  human-readable  classification  rules,  we  proceeded  as 
follows. We first describe how we prepared the evaluation data, and then explain how we filtered the 
generated rules to select only the rules with low error rates.
•  Training dataset: To produce the rules, a training dataset of labeled feature vectors is generated over 
all known benign and malicious files from download events observed during a training time window 
Ttr (e.g., 30 days).
•  Testing dataset: The performance and accuracy of the rules are evaluated using a test dataset. The 
test dataset contains known benign and malicious files from download events gathered from a test 
time window Tts that immediately follows the training time window Ttr. We ensured that the intersection 
between training and test file download events is empty, so none of the samples from the testing 
dataset are ever used for extracting the rules. Furthermore, this perfectly simulates how the system 
is used in operational environments; rules generated based on past events are used to classify new, 
unknown events in the future.
34 | Exploring the Long Tail of (Malicious) Software Downloads
•  Unknown files dataset:   The goal is to utilize the extracted rules to classify previously unknown files. 
Therefore,  we  extract  the  truly  unknown  files  during  Tts  and  generate  a  dataset  of  unknown  files. 
Obviously, there is no ground truth available whatsoever about any of the files in this dataset. We 
use the rule-based classifier to reduce the number of unknowns in this dataset by classifying them 
as either benign or malicious. Due to a lack of ground truth, the correctness of the classification of 
unknown  files  cannot  be  verified.  However,  we  measured  their  properties  and  manually  analyzed 
some of the samples to attempt to determine the correctness of their new labels.
We  now  present  our  evaluation  results.  To  this  end,  we  consider  a  month  of  download  events  as  our 
training  time  window  and  extract  the  classification  rules.  Then  we  evaluate  the  performance  of  these 
rules in terms of true positives (TP) and false positives (FP). Finally, we report the number of completely 
unknown files that the rules classified during Tts.
We  evaluated  the  rule-based  classification  system  on  different  Ttr  and  Tts  periods.  Table  16  reports  a 
summary of the number of extracted rules per different training time. As mentioned before, we use a subset 
of all rules generated by the PART algorithm [4], i.e., by including only those rules with error rates less than 
a maximum (configurable) error threshold τ. The value of τ should be properly chosen as it impacts the 
performance of the classifier. To compare the results, for every Ttr, we extract the rules based on different 
configurations for τ during training. For example, for the month of March as Ttr and by choosing the rules 
that have no training error (τ=0.0%), 1,148 rules (out of 1,680 rules) will be selected. The detection results 
of these different settings are then compared to each other. The “rules composition” column shows the 
number of rules that result in a benign or malicious label, among the 1,148 selected rules.
By  increasing  τ,  the  number  of  rules  and  samples  that  match  them  increases,  at  the  expense  of  the 
trade-off between TPs and FPs. Therefore, we limit ourselves to experimenting with low values of τ. Table 
16  shows  the  results  for  different  number  of  rules  extracted  per  month  for  τ=0.0%  and  τ=0.1%.  The 
evaluation results for these two different rule sets are reported in Table 17.
In this table, each row corresponds to an experiment in which rules are extracted according to a specific 
configuration (see Table 16) from download events during a month Ttr. The rules are then tested against 
samples in the test dataset from Tts (see column “test dataset”). 
More specifically, under “test dataset”, columns “# malicious” and “# benign” report the size of the benign 
and malicious test samples that matched the rules. Note that those test samples that do not match any 
rules are not considered, because the rule-based classifier cannot label them. Therefore, the TP and FP 
rates are computed only over the test samples that actually match at least one rule. Column “# FP Rules” 
reports the number of rules that cause FPs. We will discuss these rules in Section 7.
35 | Exploring the Long Tail of (Malicious) Software Downloads
The rule-based classifier also needs to deal with cases in which conflicts occur among multiple rules 
that match the feature vector of a file. In this situation, some rules identify the file as benign while some 
other conflicting rules classify the same file as malicious. In our rule-based classification system, should 
a conflict occur when classifying a file, we “reject” the file and do not provide any classification to avoid 
inaccurate results. This is another advantage of using our system over regular decision trees in which 
rejecting some classification decisions of the decision tree is not an intuitive task. Rejecting a file in case 
of conflicting rules helps reduce the errors (FPs), as we will demonstrate shortly.
As seen on Table 17, rules extracted with maximum error rate of τ=0.1% consistently produced accurate 
detection results in terms of the combination of TPs and FPs during all Tts. Overall, using this setting, the 
rule-based classifier achieved TP>95% and FP<0.32% on test datasets. Please note that due to rejecting 
conflicting and inaccurate classifications, in some cases during the same Tts, the number of rules that 
produce  FPs  decreases  even  after  selecting  more  rules  by  increasing  τ.  Furthermore,  the  “unknowns 
dataset”  column  in  Table  17  reports  the  percentage  of  completely  unknown  files  from  period  Tts  that 
match the extracted rules and are now classified (“matched” column). The table also shows the exact 
numbers of matched unknown files classified as benign or malicious.
Also, note the percentage of truly unknown files that match the extracted rules in each τ setting. More 
rules  are  chosen  as τ  increases,  and  consequently,  more  unknown  files  match  the  rules.  However,  as 