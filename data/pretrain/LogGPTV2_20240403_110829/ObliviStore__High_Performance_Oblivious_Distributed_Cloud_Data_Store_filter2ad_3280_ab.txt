co-processors, unlike PrivateFS and ObliviStore, the ORAM implementation by Lorch et. al. [15] is mainly constrained by the computational
power and memory available on these off-the-shelf secure co-processors.
Figure 2: ObliviStore throughput with 7
HDDs. Experiment is performed on a single
ORAM node with the following parameters:
50ms network latency between the ORAM
node and the storage, 12ms average disk seek
latency, and 4KB block size.
Figure 3: ObliviStore response time with
7HDDs. Experiment is performed on a single
ORAM node with the following parameters:
50ms network latency between the ORAM
node and the storage, 12ms average disk seek
latency, and 4KB block size.
Figure 4: Effect of network latency on
throughput with 7HDDs. Experiment is
performed on a single ORAM node with 7
HDDs, 12ms average disk seek latency, and
4KB block size.
Figure 5: Effect of network latency on re-
sponse time. Experiment is performed on a
single ORAM node with 7 HDDs (12ms av-
erage seek latency), and again with 2 SSDs.
Block size = 4KB. The ideal line represents
the roundtrip network latency.
Figure 6: Scalability of ObliviStore in a
distributed setting.
1 oblivious load bal-
ancer, 2 SDDs attached to each ORAM node.
Throughput is the aggregate ORAM through-
put at the load balancer which distributes the
load across all ORAM nodes.
Figure 7: Average number of
seeks
of ObliviStore per ORAM operation.
Includes all
I/O to storage (reads and
writes/shufﬂes). Experiment is performed on
a single ORAM node with 4KB block size.
2
rameters to best replicate the exact setup used in the Pri-
vateFS and PD-ORAM experiments [29].
Small number of seeks. Our optimizations for reducing
disks seeks help greatly in achieving (relatively) high per-
formance. Figure 7 plots the average number of seeks per
ORAM operation. For 1TB to 10TB ORAMs, ObliviStore
does under 10 seeks per ORAM operation on average.
Effect of network latency. In Figures 4 and 5, we measure
the throughput and latency of a 1 TB ObliviStore ORAM un-
der different network latencies. The results suggest that for
rotational hard drives, the throughput of ObliviStore is al-
most unaffected until about 1 second of network latency. To
obtain higher throughput beyond 1s network latency, we can
increase the level of parallelism in our implementation, i.e.,
allowing more concurrent I/Os – but this will lead to higher
response time due to increased queuing and I/O contention.
The response time of ObliviStore (single node with 7
HDDS) is consistently 140ms to 200ms plus the round-trip
network latency. The additional 140ms to 200ms is due to
disk seeks, request queuing, and I/O contention.
2.2 Summary of Results with Solid State Drives
The throughput of ObliviStore with 2x1TB SSDs of stor-
age is about 6-8 times faster than with 7 HDD. For a typical
50ms network link, the response time with SSD storage is
about half of that with HDDs. Due to space limitations, we
could not include detailed SSD experiment results in this ab-
stract except for the distributed setting results in Section 2.3.
2.3 Distributed Setting
We measure the scalability of ObliviStore in a distributed
setting. We consider a deployment scenario with a dis-
tributed TCB in the cloud. We assume that the TCB is estab-
lished through techniques such as Trusted Computing, and
that the TCB is running on a modern processor. How to im-
plement code attestation to establish such a distributed TCB
has been addressed in orthogonal work [16,17,21,22], and is
not a focus of this evaluation.
For the distributed SSD experiments, each ORAM node
was a hi1.4xlarge Amazon EC instance with 2x1TB SSDs
of storage directly attached, and the load balancer ran on a
cc1.4xlarge instance. Although our instances have 60GB of
provisioned RAM, our implementation used far less (under
3 GB per ORAM node, and under 3.5 GB for the load bal-
ancer). The load balancer and the ORAM nodes communi-
cate through EC2’s internal network (under 5ms RTT).
Figure 6 suggests that the throughput of ObliviStore
scales up linearly with the number of ORAM nodes, as long
as we do not saturate the network. The total bandwidth over-
head between the oblivious load balancer and all ORAM
nodes is 2X, and we never saturated the network in all our
experiments. For example, with 10 ORAM nodes and 4KB
block size, the ORAM throughput is about 31.5 MB/s, and
the total bandwidth between the load balancer and all ORAM
nodes is about 63 MB/s. We also measured that the response
3
time in the distributed setting is about 60ms for 4KB blocks
and is mostly unaffected by the number of nodes.
The throughput of ObliviStore using HDD storage (also
tested on Amazon EC2) similarly scales linearly with the
number of nodes (please refer to the full paper).
References
[1] http://www.storagereview.com/php/benchmark/
suite_v4.php?typeID=10&testbedID=4&osID=
6&raidconfigID=1&numDrives=1&devID_0=
368&devCnt=1.
2012.
trieval. In PET, 2003.
[2] Personal communication with radu sion and peter williams., Nov.
[3] D. Asonov and J.-C. Freytag. Almost optimal private information re-
[4] M. Backes, A. Kate, M. Maffe, and K. Pecina. Obliviad: Provably
secure and practical online behavioral advertising. In S & P, 2012.
[5] D. Boneh, D. Mazieres, and R. A. Popa.
Remote oblivi-
ous storage: Making oblivious RAM practical.
Manuscript,
http://dspace.mit.edu/bitstream/handle/1721.1/
62006/MIT-CSAIL-TR-2011-018.pdf, 2011.
[6] I. Damg˚ard, S. Meldgaard, and J. B. Nielsen. Perfectly secure oblivi-
ous RAM without random oracles. In TCC, 2011.
[7] O. Goldreich. Towards a theory of software protection and simulation
by oblivious RAMs. In STOC, 1987.
[8] O. Goldreich and R. Ostrovsky. Software protection and simulation
on oblivious RAMs. J. ACM, 1996.
[9] M. T. Goodrich and M. Mitzenmacher. Privacy-preserving access of
outsourced data via oblivious RAM simulation. In ICALP, 2011.
[10] M. T. Goodrich, M. Mitzenmacher, O. Ohrimenko, and R. Tamassia.
Oblivious RAM simulation with efﬁcient worst-case access overhead.
In ACM Cloud Computing Security Workshop (CCSW), 2011.
[11] M. T. Goodrich, M. Mitzenmacher, O. Ohrimenko, and R. Tamas-
sia. Privacy-preserving group data access via stateless oblivious RAM
simulation. In SODA, 2012.
[12] A. Iliev and S. W. Smith. Protecting client privacy with trusted com-
IEEE Security and Privacy, 3(2):20–28, Mar.
puting at the server.
2005.
[13] M. Islam, M. Kuzu, and M. Kantarcioglu. Access pattern disclosure
on searchable encryption: Ramiﬁcation, attack and mitigation. In Net-
work and Distributed System Security Symposium (NDSS), 2012.
[14] E. Kushilevitz, S. Lu, and R. Ostrovsky. On the (in)security of hash-
based oblivious RAM and a new balancing scheme. In SODA, 2012.
[15] J. R. Lorch, J. W. Mickens, B. Parno, M. Raykova, and J. Schiffman.
Toward practical private access to data centers via parallel ORAM.
IACR Cryptology ePrint Archive, 2012:133, 2012.
[16] J. M. McCune, Y. Li, N. Qu, Z. Zhou, A. Datta, V. D. Gligor, and
A. Perrig. Trustvisor: Efﬁcient TCB reduction and attestation. In S &
P, 2010.
[17] J. M. McCune, B. Parno, A. Perrig, M. K. Reiter, and H. Isozaki.
In Eu-
In ACM
Flicker: An execution infrastructure for TCB minimization.
roSys, 2008.
[18] R. Ostrovsky. Efﬁcient computation on oblivious RAMs.
Symposium on Theory of Computing (STOC), 1990.
[19] R. Ostrovsky and V. Shoup. Private information storage (extended
abstract). In STOC, pages 294–303, 1997.
[20] B. Pinkas and T. Reinman. Oblivious RAM revisited. In CRYPTO,
2010.
[21] R. Sailer, X. Zhang, T. Jaeger, and L. van Doorn. Design and im-
plementation of a TCG-based integrity measurement architecture. In
USENIX Security Symposium, 2004.
[22] N. Santos, R. Rodrigues, K. P. Gummadi, and S. Saroiu. Policy-sealed
data: a new abstraction for building trusted cloud services. In Usenix
Security, 2012.
[23] E. Shi, T.-H. H. Chan, E. Stefanov, and M. Li. Oblivious RAM with
O((log N )3) worst-case cost. In ASIACRYPT, pages 197–214, 2011.
[24] S. W. Smith and D. Safford. Practical server privacy with secure co-
processors. IBM Syst. J., 40(3):683–695, Mar. 2001.
[25] E. Stefanov, E. Shi, and D. Song. Towards practical oblivious RAM.
[26] P. Williams and R. Sion. Usable PIR. In NDSS, 2008.
[27] P. Williams and R. Sion. Round-optimal access privacy on outsourced
In NDSS, 2012.
storage. In CCS, 2012.
[28] P. Williams, R. Sion, and B. Carbunar. Building castles out of mud:
practical access pattern privacy and correctness on untrusted storage.
In CCS, 2008.
[29] P. Williams, R. Sion, and A. Tomescu. Privatefs: A parallel oblivious
ﬁle system. In CCS, 2012.