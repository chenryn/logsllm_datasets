title:Locally Differentially Private Frequency Estimation with Consistency
author:Tianhao Wang and
Milan Lopuha&quot;a-Zwakenberg and
Zitao Li and
Boris Skoric and
Ninghui Li
Locally Differentially Private Frequency Estimation
with Consistency
Tianhao Wang1, Milan Lopuha¨a-Zwakenberg2, Zitao Li1, Boris Skoric2, Ninghui Li1
1Purdue University, 2Eindhoven University of Technology
{tianhaowang, li2490, ninghui}@purdue.edu, {m.a.lopuhaa, b.skoric}@tue.nl
Abstract—Local Differential Privacy (LDP) protects user pri-
vacy from the data collector. LDP protocols have been increas-
ingly deployed in the industry. A basic building block is frequency
oracle (FO) protocols, which estimate frequencies of values. While
several FO protocols have been proposed, the design goal does
not lead to optimal results for answering many queries. In this
paper, we show that adding post-processing steps to FO protocols
by exploiting the knowledge that all individual frequencies should
be non-negative and they sum up to one can lead to signiﬁcantly
better accuracy for a wide range of tasks, including frequencies
of individual values, frequencies of the most frequent values,
and frequencies of subsets of values. We consider 10 different
methods that exploit this knowledge differently. We establish
theoretical relationships between some of them and conducted
extensive experimental evaluations to understand which methods
should be used for different query tasks.
I. INTRODUCTION
Differential privacy (DP) [12] has been accepted as the
de facto standard for data privacy. Recently, techniques for
satisfying DP in the local setting, which we call LDP, have
been studied and deployed. In this setting, there are many
users and one aggregator. The aggregator does not see the
actual private data of each individual. Instead, each user sends
randomized information to the aggregator, who attempts to
infer the data distribution based on that. LDP techniques have
been deployed by companies like Apple [1], Google [14],
Microsoft [9], and Alibaba [32]. Examples of use cases include
collecting users’ default browser homepage and search engine,
in order to understand the unwanted or malicious hijacking of
user settings; or frequently typed emoji’s and words, to help
with keyboard typing recommendation.
The fundamental tools in LDP are mechanisms to estimate
frequencies of values. Existing research [14], [5], [31], [2],
[36] has developed frequency oracle (FO) protocols, where
the aggregator can estimate the frequency of any chosen value
in the speciﬁed domain (fraction of users reporting that value).
While these protocols were designed to provide unbiased
estimations of individual frequencies while minimizing the es-
timation variance [31], they can perform poorly for some tasks.
In [17], it is shown that when one wants to query the frequency
Network and Distributed Systems Security (NDSS) Symposium 2020
23-26 February 2020, San Diego, CA, USA
ISBN 1-891562-61-4
https://dx.doi.org/10.14722/ndss.2020.24157
www.ndss-symposium.org
of all values in the domain, one can obtain signiﬁcant accuracy
improvement by exploiting the belief that
the distribution
likely follows power law. Also, some applications naturally
require querying the sums of frequencies for values in a subset.
For example, with the estimation of each emoji’s frequency,
one may be interested in understanding what categories of
emoji’s are more popular and need to issue subset frequency
queries. For another example, in [38], multiple attributes are
encoded together and reported using LDP, and recovering the
distribution for each attribute separately requires computing
the frequencies of sets of encoded values. For frequencies of
a subset of values, simply summing up the estimations of all
values is far from optimal, especially when the input domain
is large.
We note that
the problem of answering queries using
information obtained from the frequency oracle protocols is
an estimation problem. Existing methods such as those in [31]
do not utilize any prior knowledge of the distribution to be
estimated. Due to the signiﬁcant amount of noise needed to
satisfy LDP, the estimations for many values may be negative.
Also, some LDP protocols may result in the total sum of
frequencies to be different from one. In this paper, we show
that one can develop better estimation methods by exploiting
the universal fact that all frequencies are non-negative and they
sum up to 1.
Interestingly, when taking advantage of such prior knowl-
edge, one introduces biases in the estimations. For example,
when we impose the non-negativity constraint, we are in-
troducing positive biases in the estimation as a side effect.
Essentially, when we exploit prior beliefs,
the estimations
will be biased towards the prior beliefs. These biases can
cause some queries to be much more inaccurate. For example,
changing all negative estimations to zero improves accu-
racy for frequency estimations of individual values. However,
the introduced positive biases accumulate for range queries.
Different methods to utilize the prior knowledge introduces
different forms of biases, and thus have different impacts for
different kinds of queries.
In this paper, we consider 10 different methods, which
utilizes prior knowledge differently. Some methods enforce
only non-negativity; some other methods enforce only that
all estimations sum to 1; and other methods enforce both.
These methods can also be combined with the “Power” method
in [17] that exploits power law assumption.
We evaluate these methods on three tasks, frequencies of
individual values, frequencies of the most frequent values,
and frequencies of subsets of values. We ﬁnd that there is no
single method that out-performs other methods for all tasks. A
method that exploits only non-negativity performs the best for
individual values; a method that exploits only the summing-
to-one constraint performs the best for frequent values; and a
method that enforces both can be applied in conjunction with
Power to perform the best for subsets of values.
To summarize, the main contributions of this paper are
threefold:
• We introduced the consistency properties as a way to
improve accuracy for FO protocols under LDP, and
summarized 10 different post-processing methods that
exploit the consistency properties differently.
• We established theoretical relationships between Con-
strained Least Squares and Maximum Likelihood Esti-
mation, and analyze which (if any) estimation biases are
introduced by these methods.
• We conducted extensive experiments on both synthetic
and real-world datasets, the results improved the under-
standing on the strengths and weaknesses of different
approaches.
Roadmap.
In Section II, we give the problem deﬁnition,
followed by the background information on FO in Section III.
We present the post-processing methods in Section IV. Ex-
perimental results are presented in V. Finally we discuss
related work in Section VI and provide concluding remarks
in Section VII.
II. PROBLEM SETTING
We consider the setting where there are many users and
one aggregator. Each user possesses a value v from a ﬁnite
domain D, and the aggregator wants to learn the distribution
of values among all users, in a way that protects the privacy
of individual users. More speciﬁcally, the aggregator wants to
estimate, for each value v ∈ D, the fraction of users having v
(the number of users having v divided by the population size).
Such protocols are called frequency oracle (FO) protocols
under Local Differential Privacy (LDP), and they are the key
building blocks of other LDP tasks.
Privacy Requirement. An FO protocol is speciﬁed by a pair
of algorithms: Ψ is used by each user to perturb her input
value, and Φ is used by the aggregator. Each user sends Ψ(v)
to the aggregator. The formal privacy requirement is that the
algorithm Ψ(·) satisﬁes the following property:
Deﬁnition 1 (-Local Differential Privacy). An algorithm Ψ(·)
satisﬁes -local differential privacy (-LDP), where  ≥ 0, if
and only if for any input v, v(cid:48) ∈ D, we have
∀y ∈ Ψ(D) : Pr [Ψ(v) = y] ≤ e Pr [Ψ(v(cid:48)) = y] ,
where Ψ(D) is discrete and denotes the set of all possible
outputs of Ψ.
2
Since a user never reveals v to the aggregator and reports
only Ψ(v), the user’s privacy is still protected even if the
aggregator is malicious.
Utility Goals.
The aggregator uses Φ, which takes the
vector of all reports from users as the input, and produces
˜f = (cid:104) ˜fv(cid:105)v∈D, the estimated frequencies of the v ∈ D (i.e.,
the fraction of users who have input value v). As Ψ is a
randomized function, the resulting ˜f becomes inaccurate.
In existing work, the design goal for Ψ and Φ is that the
estimated frequency for each v is unbiased, and the variance
of the estimation is minimized. As we will show in this paper,
these may not result in the most accurate answers to different
queries.
In this paper, we consider three different query scenarios 1)
query the frequency of every value in the domain, 2) query
the aggregate frequencies of subsets of values, and 3) query
the frequencies of the most frequent values. For each value or
set of values, we compute its estimate and the ground truth,
and calculate their difference, measured by Mean of Squared
Error (MSE).
Consistency. We will show that the utility of existing mecha-
nisms can be improved by enforcing the following consistency
requirement.
Deﬁnition 2 (Consistency). The estimated frequencies are
consistent if and only if the following two conditions are
satisﬁed:
1) The estimated frequency of each value is non-negative.
2) The sum of the estimated frequencies is 1.
III. FREQUENCY ORACLE PROTOCOLS
We review the state-of-the-art frequency oracle protocols.
We utilize the generalized view from [31] to present
the
protocols, so that our post-processing procedure can be applied
to all of them.
A. Generalized Random Response (GRR)
This FO protocol generalizes the randomized response tech-
nique [35]. Here each user with private value v ∈ D sends
the true value v with probability p, and with probability 1− p
sends a randomly chosen v(cid:48) ∈ D\{v}. Suppose the domain D
contains d = |D| values, the perturbation function is formally
deﬁned as
∀y∈D Pr(cid:2)ΨGRR(,d)(v) = y(cid:3) =
(cid:26) p = e
e+d−1 ,
e+d−1 ,
q = 1
if y = v
if y (cid:54)= v
(1)
q = e.
This satisﬁes -LDP since p
From a population of n users, the aggregator receives a
length-n vector y = (cid:104)y1, y2,··· , yn(cid:105), where yi ∈ D is the
reported value of the i-th user. The aggregator counts the
number of times each value v appears in y and produces
a length-d vector c of natural numbers. Observe that
the
v∈D cv = n. The
components of c sum up to n, i.e., (cid:80)
aggregator then obtains the estimated frequency vector ˜f by
scaling each component of c as follows:
n − 1
e−1
e+d−1
n − q
p − q
e+d−1
˜fv =
=
cv
cv
As shown in [31], the estimation variance of GRR grows
linearly in d; hence the accuracy deteriorates fast when the
domain size d increases. This motivated the development of
other FO protocols.
B. Optimized Local Hashing (OLH)
This FO deals with a large domain size d by ﬁrst using a
random hash function to map an input value into a smaller
domain of size g, and then applying randomized response to
the hash value in the smaller domain. In OLH, the reporting
protocol is
ΨOLH()(v) := (cid:104)H, ΨGRR(,g)(H(v))(cid:105),
where H is randomly chosen from a family of hash functions
that hash each value in D to {1 . . . g}, and ΨGRR(,g) is given
in (1), while operating on the domain {1 . . . g}. The hash
family should have the property that the distribution of each
v’s hashed result is uniform over {1 . . . g} and independent
from the distributions of other input values in D. Since H
is chosen independently of the user’s input v, H by itself
carries no meaningful information. Such a report (cid:104)H, r(cid:105) can
be represented by the set Y = {y ∈ D | H(y) = r}. The use
of a hash function can be viewed as a compression technique,
which results in constant size encoding of a set. For a user with
value v, the probability that v is in the set Y represented by the
randomized report (cid:104)H, r(cid:105) is p = e−1
e+g−1 and the probability
that a user with value (cid:54)= v is in Y is q = 1
g .
For each value x ∈ D, the aggregator ﬁrst computes the
vector c of how many times each value is in the reported set.
More precisely, let Yi denote the set deﬁned by the user i,
then cv = |{i | H(v) ∈ Yi}|. The aggregator then scales it:
˜fv =
cv
n − 1/g
p − 1/g
(2)
In OLH, both the hashing step and the randomization step
result in information loss. The choice of the parameter g
is a tradeoff between losing information during the hashing
step and losing information during the randomization step.
It is found that the estimation variance when viewed as a
continuous function of g is minimized when g = e + 1 (or
the closest integer to e + 1 in practice) [31].
C. Other FO Protocols
Several other FO protocols have been proposed. While they
take different forms when originally proposed, in essence, they
all have the user report some encoding of a subset Y ⊆ D, so
that the user’s true value has a probability p to be included in
Y and any other value has a probability q < p to be included
) equally applies.
in Y . The estimation method used in GRR and OLH (namely,
˜fv = cv/n−q
p−q
Optimized Unary Encoding [31] encodes a value in a size-d
domain using a length-d binary vector, and then perturbs each
bit independently. The resulting bit vector encodes a set of
values. It is found in [31] that when d is large, one should ﬂip
the 1 bit with probability 1/2, and ﬂip a 0 bit with probability
1/e. This results in the same values of p, q as OLH, and has
the same estimation variance, but has higher communication
cost (linear in domain size d).
Subset Selection [36], [30] method reports a randomly
selected subset of a ﬁxed size k. The sensitive value v is
included in the set with probability p = 1/2. For any other
d−1 +(1−p)· k
value, it is included with probability q = p· k−1
d−1.
To minimize estimation variance, k should be an integer equal
or close to d/(e +1). Ignoring the integer constraint, we have
e+1. Its
q = 1
d−1 = 1
variance is smaller than that of OLH. However, as d increases,
the term d−(e+1)/2
gets closer and closer to 1. For a larger
domain, this offers essentially the same accuracy as OLH, with
higher communication cost (linear in domain size d).
Hadamard Response [4], [2]
is similar to Subset Selection
with k = d/2, where the Hadamard transform is used to
compress the subset. The beneﬁt of adopting this protocol is
to reduce the communication bandwidth (each user’s report is
of constant size). While it is similar to OLH with g = 2, its
aggregation part Φ faster, because evaluating a Hadamard entry
is practically faster than evaluating hash functions. However,
this FO is sub-optimal when g = 2 is sub-optimal.
e+1 · d−(e+1)/2
e+1−1
d−1 = 1
2 · 2k−1
2 · 2 d
< 1
d−1
d−1
D. Accuracy of Frequency Oracles
estimates. That is, ∀v ∈ D, E(cid:104) ˜fv
In [31], it is proved that ˜fv = cv/n−q
produces unbiased
p−q
= fv. Moreover, ˜fv has
(cid:105)
variance
σ2
v =
q(1 − q) + fv(p − q)(1 − p − q)
n(p − q)2
(3)
As cv follows Binomial distribution, by the central
limit
theorem, the estimate ˜fv can be viewed as the true value fv
plus a Normally distributed noise:
˜fv ≈ fv + N (0, σv).
(4)
When d is large and  is not too large, fv(p−q)(1−p−q) is
dominated by q(1−q). Thus, one can approximate Equation (3)
and (4) by ignoring the fv. Speciﬁcally,
σ2 ≈ q(1 − q)
n(p − q)2 ,
˜fv ≈ fv + N (0, σ).
(5)
(6)
As the probability each user’s report support each value is
independent, we focus on post-processing ˜f instead of Y.
3
IV. TOWARDS CONSISTENT FREQUENCY ORACLES
it
While existing state-of-the-art frequency oracles are de-
signed to provide unbiased estimations while minimizing the
variance,