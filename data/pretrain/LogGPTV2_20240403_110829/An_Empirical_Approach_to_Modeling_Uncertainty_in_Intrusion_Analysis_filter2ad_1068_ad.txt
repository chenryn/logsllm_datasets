summarizedFact obslist(257)
’128.111.48.35’),l)
p)
Figure 5. Partial output trace from the reasoning system
network (5). The two pieces of evidence both point to the
compromise of the web server, strengthening our conﬁdence
level to “certain”. In total, there were 18 such proofs verify-
ing the two web servers were compromised. Table 6 shows
the reduction in amount of data that was presented by our
tool to the system administrator for further analysis. The raw
alerts belonging to the summarized internal condition can be
identiﬁed using the mapping variable obslist(Var).
We manually validated raw alerts of the 18 proofs gener-
ated by the reasoning engine for false positives. From our
analysis we perceive all of them are plausible. The published
TH data set did not include a truth ﬁle (a ﬁle containing
information on how the actual attacks were carried out and
to what extent they were successful). Thus it was impossible
to identify if the reasoning engine missed any true attack
traces (false negatives).
B. Experiment on data collected on a Honeypot
We conducted our second set of experiments on a data set
collected from a honeypot deployed at Purdue University.
This data set was created for a unrelated project whose
main intention was to collect spam relayed using open
proxies. The network trafﬁc in a single machine running
misconﬁgured squid proxy was captured as TCPdump over
a period of two months. (The total size of the zipped
TCPdump ﬁles was about 68GB.) The reasoning engine
had enough information to conclude the honeypot host was
compromised. With knowledge about operating system and
services running in the honeypot host, we validated the traces
manually.
C. Experiments on a production system
The last
two experiments also helped us ﬁnd a few
inaccuracies in the observation correspondence relations
created by the automated model building process, which
were subsequently ﬁxed. In the last experiment, SnIPS with
the updated knowledge base was applied on an university
network. We installed Snort in our departmental network
having 300 workstations along with dedicated web servers,
ﬁle servers, databases server etc. We were only able to
analyze the alerts captured over a period of three days. Snort
reported about 1.1 Million alerts with 150 different alert
types and 15 class types. Figure 6 shows the number of
alerts generated by Snort and the number of high-conﬁdence
proofs presented to SA. The 17 proofs pointed that 4 hosts
has a higher chance of being compromised. To verify this
501
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:13:03 UTC from IEEE Xplore.  Restrictions apply. 
Data set
Snort
alerts
Summarized
alerts
TH
Honeypot
Department
4,849,937 278
637,564
30
1,138,572 6634
High-
conﬁdence
proofs
18
8
17
Figure 6. Reduction of alerts to high-conﬁdence proofs
result would require further analysis of other log data which
we did not have access to. We analyzed the output traces
with the corresponding low-level alerts. It appeared that a
couple traces were worthy of further investigation and we
forwarded those to the system administrator. The others were
likely to be false positives. Our tool helped reducing the
search space and time spent on intrusion analysis.
VI. RELATED WORK
Uncertainty in data, even speciﬁcally in the context of
security analysis,
is a vast and fertile topic and space
constraints require us to only give the highlights of related
work here. Probabilistic reasoning appears to be a natural
candidate for such problems and there have been several
attempts along this direction [12], [14], [9]. As explained
earlier, a fundamental challenge in applying these techniques
is how to obtain the logical structure needed for combining
the probability measures. Moreover, these techniques require
as inputs statistical parameters in terms of probability distri-
butions of related events, conditional probability tables, etc.
that have proven very hard to estimate or learn from real-
life data because of overwhelming background noise (see,
e.g., [19] and [20] for two different viewpoints, estimation
theory and learning theory). For security analysis, it is nearly
impossible to obtain the ground truth in real traces and
public data sets and it is hard if not impossible to realistically
simulate attack scenarios. Consequently calibrating analysis
techniques with metrics such as false positive/negative ratios
is a huge challenge (see [8] for deﬁnitions) and intrusion
analysis remains a manual and intuitive art, which has
inspired us to formulate a logic that approximates human
reasoning that works with a qualitative assessment on a few
conﬁdence levels that are relatively easy to understand. We
acknowledge that this formulation not only hides the lack of
knowledge of base probabilities but also reﬂects the great
deal of ambiguity that exists in intrusion analysis of real
data. We hope that by creating an option to specify the
conﬁdence level explicitly and by providing general practical
tools to manipulate these uncertain pieces of knowledge,
we can bypass some of these fundamental problems and
gain experience and insight that may make some statistical
approaches viable in the future.
A closely related work to ours, BotHunter [21], is an
application for identifying Bot machines by correlating Snort
alerts with a number of other system-monitoring events.
The notions of “conﬁdence score” and “evidence threshold”
are introduced to capture the uncertainty in the correlation
process and speciﬁc processes are designed for the purpose
of Bot detection. The goal of our work is to provide a simple
but more general model for intrusion analysis.
Hollebeek and Waltzman proposed a notion of “suspicion”
in modeling uncertainty in intrusion analysis [22]. It appears
that the approach does not further differentiate the various
meanings that could be associated with a “suspicious” event.
The system relies on a “deductive graph” and a “suspicion
graph” to propagate certainty levels within the context of
the system under concern. But with just a single notion of
“suspicion” for each event, it is not clear how to build or
interprete the meanings of such graphs in a systematic and
consistent manner.
The literature on intrusion alert correlation is vast and
the insights into logical causality relations among intrusion
alerts have informed our work; [1], [3], [4], [5], [6], [8]
are only a few examples. Most of these works model IDS-
speciﬁc states using pre- and post-conditions that drive a
correlation model and rely on the existence of a sparse
(nearly deterministic) mapping from alerts to their pre-/post-
conditions. It appears to be difﬁcult to model in this manner
ubiquitous alerts such as “abnormally high network trafﬁc”
that could be indicative of any of a wide variety of possible
conditions. Our observation correspondence model assigns
a direct meaning to an observation and our internal model
allows such meanings to be ﬂexibly linked together based
on their semantics. Such ﬂexibility is important when the
evidence is tenuous and subject to multiple interpretations.
Our pre-processing step includes data reduction based
on clustering and simple correlation of local observations.
Much previous work in IDS has addressed this important
problem [5], [8] (including the “layered approach” of Mar-
tignoni et al. [23]). We intend to use all applicable tools and
approaches from these works.
VII. CONCLUSION
We presented an empirical approach to modeling uncer-
tainty in intrusion analysis to help the system administrator
in reaching conclusions quickly about possible intrusions,
when multiple pieces of uncertain data have to be inte-
grated. The model language we designed has two com-
ponents: observation correspondence and internal model.
The observation correspondence gives a direct meaning to
low-level system monitoring data with explicit uncertainty
tags, and can be derived from natural-language description
that already exists in some IDS knowledge bases, e.g.
the Snort rule repository. The internal model is concise
and captures general multi-stage attack conditions in an
enterprise network. We developed a reasoning system that is
easy to understand, handles the uncertainty existent in both
observation correspondence and the internal model, and ﬁnds
high-conﬁdence attack traces from many possible interpre-
tations of the low-level monitoring data. Our prototype and
502
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:13:03 UTC from IEEE Xplore.  Restrictions apply. 
experiments show that the model developed from studying
one set of data is effective for analyzing completely different
data sets with very little effort. This is a strong indication
that the modeling approach can codify the seemingly ad-
hoc reasoning process found in intrusion analysis and yield
practical tools for enterprise-network environments.
ACKNOWLEDGMENT
This work was partially supported by the U.S. National
Science Foundation under Grant No. 0716665. Any opin-
ions, ﬁndings and conclusions or recomendations expressed
in this material are those of the authors and do not neces-
sarily reﬂect the views of the National Science Foundation.
We would like to thank Abhinav Pathak from Purdue who
provided us the Honeypot data for evaluation.
REFERENCES
[1] A. Mounji, “Languages and tools for rule-based distributed
intrusion detections,” Ph.D. dissertation, Purdue University,
September 1997.
[2] S. R. Snapp, J. Brentano, G. V. Dias, T. L. Goan, L. T.
Heberlein, C. lin Ho, K. N. Levitt, B. Mukherjee, S. E. Smaha,
T. Grance, D. M. Teal, and D. Mansur, “Dids (distributed
intrusion detection system) - motivation, architecture, and
an early prototype,” in In Proceedings of the 14th National
Computer Security Conference, 1991, pp. 167–176.
attacks
for
[3] S. Cheung, U. Lindqvist, and M. W. Fong, “Modeling
multistep cyber
in
DARPA Information Survivability Conference and Exposition
(DISCEX III), Washington, D.C.,
284–
292.
http://www.sdl.sri.com/papers/
cheung-lindqvist-fong-discex3-cr/
scenario recognition,”
[Online]. Available:
2003,
pp.
[4] F. Cuppens and A. Mi`ege, “Alert correlation in a coopera-
tive intrusion detection framework,” in IEEE Symposium on
Security and Privacy, 2002.
[5] P. Ning, Y. Cui, D. Reeves, and D. Xu, “Tools and techniques
for analyzing intrusion alerts,” ACM Transactions on Infor-
mation and System Security, vol. 7, no. 2, pp. 273–318, May
2004.
[6] S. Noel, E. Robertson, and S. Jajodia, “Correlating Intrusion
Events and Building Attack Scenarios Through Attack Graph
Distances,” in 20th Annual Computer Security Applications
Conference (ACSAC 2004), 2004, pp. 350– 359.
[7] F. Valeur, “Real-Time Intrusion Detection Alert Correlation,”
Ph.D. dissertation, University of California, Santa Barbara,
May 2006.
[8] F. Valeur, G. Vigna, C. Kruegel, and R. A. Kemmerer,
“A Comprehensive Approach to Intrusion Detection Alert
Correlation,” IEEE Transactions on Dependable and Secure
Computing, vol. 1, no. 3, pp. 146–169, 2004.
[9] Y. Zhai, P. Ning, P. Iyer, and D. S. Reeves, “Reasoning about
complementary intrusion evidence,” in Proceedings of 20th
Annual Computer Security Applications Conference (ACSAC),
December 2004, pp. 39–48.
503
[10] J. Pearl, Probabilistic Reasoning in Intelligent Systems: Net-
works of Plausible Inference. Morgan Kaufman, 1999.
[11] G. Shafer, A Mathematical Theory of Evidence.
University Press, 1976.
Princeton
[12] M. Almgren, U. Lindqvist, and E. Jonsson, “A multi-sensor
model to improve automated attack detection,” in 11th Inter-
national Symposium on Recent Advances in Intrusion Detec-
tion (RAID 2008). RAID, September 2008.
[13] T. M. Chen and V. Venkataramanan, “Dempster-shafer theory
for intrusion detection in ad hoc networks,” IEEE Internet
Computing, 2005.
[14] G. Modelo-Howard, S. Bagchi, and G. Lebanon, “Deter-
mining placement of intrusion detectors for a distributed
application through bayesian network modeling,” in 11th
International Symposium on Recent Advances in Intrusion
Detection (RAID 2008). RAID, September 2008.
[15] J. McHugh, “Intrusion and intrusion detection,” International
Journal of Information Security, vol. 1, pp. 14 – 35, 2001.
[16] S. Ceri, G. Gottlob, and L. Tanca, “What you always wanted
to know about Datalog (and never dared to ask),” IEEE
Transactions Knowledge and Data Engineering, vol. 1, no. 1,
pp. 146–166, 1989.
[17] P. Rao, K. F. Sagonas, T. Swift, D. S. Warren, and J. Freire,
“XSB: A system for efﬁciently computing well-founded se-
mantics,” in Proceedings of the 4th International Conference
on Logic Programming and Non-Monotonic Reasoning (LP-
NMR’97). Dagstuhl, Germany: Springer Verlag, July 1997,
pp. 2–17.
[18] G. Vigna, “Teaching Network Security Through Live Exer-
cises,” in Proceedings of the Third Annual World Conference
on Information Security Education (WISE 3), C. Irvine and
H. Armstrong, Eds.
Monterey, CA: Kluwer Academic
Publishers, June 2003, pp. 3–18.
[19] S. Axelsson, “The base-rate fallacy and the difﬁculty of
intrusion detection,” ACM Trans. Inf. Syst. Secur., vol. 3, no. 3,
pp. 186–205, 2000.
[20] C. Drummond and R. Holte, “Severe class imbalance: Why
better algorithms aren’t
the answer,” in Machine Learn-
ing: ECML 2005, ser. Lecture Notes in Computer Science.
Springer US, 2005, vol. 3720, pp. 539 – 546.
[21] G. Gu, P. Porras, V. Yegneswaran, M. Fong, and W. Lee,
“BotHunter: Detecting malware infection through ids-driven
dialog correlation,” in Proceedings of
the 16th USENIX
Security Symposium (Security’07), August 2007.
[22] T. Hollebeek and R. Waltzman, “The role of suspicion in
model-based intrusion detection,” in Proceedings of the 2004
workshop on New security paradigms, 2004.
[23] L. Martignoni, E. Stinson, M. Fredrikson, S. Jha, and
J. Mitchell, “A layered architecture for detecting malicious
behaviors,” in 11th International Symposium on Recent Ad-
vances in Intrusion Detection (RAID 2008). RAID, Septem-
ber 2008.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:13:03 UTC from IEEE Xplore.  Restrictions apply.