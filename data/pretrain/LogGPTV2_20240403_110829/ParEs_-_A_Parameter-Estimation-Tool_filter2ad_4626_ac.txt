that have to be mentioned. First, the conﬁdence intervals are
usually slightly too optimistic, i.e. too small, compared to the
Likelihood-Ratio bounds explained in Section IV-B. Second,
a transformation into a linear form is not possible in case
of the bathtub distribution and therefore an other estimation
procedure is needed for this type of distribution.
To summarize, if the conﬁdence-intervals are not of interest
and the distribution under consideration is not the bathtub
distribution, regression is the method of choice because of
its efﬁciency and stability.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:08:53 UTC from IEEE Xplore.  Restrictions apply. 
nf(cid:26)
IV. MAXIMUM-LIKELIHOOD-ESTIMATION
To overcome the limitations of regression and to supplement
the method of choice of engineers with the method of choice of
statisticians, the Maximum-Likelihood-estimation (MLE) has
also been realized. Section IV-A introduces the basic idea and
the equations for the point-estimation. Sections IV-A.1 and
IV-A.2 introduce the local and global optimization algorithms
used for the point-estimation and also for the conﬁdence-
interval-calculation that is introduced in Section IV-B.
A. Idea and Point-Estimation
The basic idea of the Maximum-Likelihood-Estimation is to
transform the estimation problem into a nonlinear optimization
problem which can then be solved by “any” standard-method
of nonlinear optimization. For the transformation, the so called
likelihood-function which is a density and therefore a measure
for the probability that the given sample comes from the
assumed distribution [11] is built. As a consequence,
the
parameters of the distribution have to be chosen such that this
probability is maximized.
In the simplest case (only failures with exact failure times
xi) the likelihood function is deﬁned as
LH(Θ) =
f(xi, Θ),
i=1
where Θ is the vector of parameters to be estimated, xi is
the time of the i-th failure, f(xi, Θ) is the density (PDF)
of the distribution under consideration and nf is the number
of failures. For numerical reasons, usually the logarithmic
likelihood function is used. This is unproblematic as the
logarithm is a monotonous function and therefore does not
change the position of the optimum, but only the function
value, which in the given case has no signiﬁcance as an
absolute value.
i.e., 1 − F (xj, Θ). The exact
In the simplest case, a failure contributes to the likelihood
function with f(xi, Θ), i.e. the probability of a failure in the
neighbourhood of xi. The derivation of the contributions of the
other data-modes to the likelihood function follows the same
idea. In the case of clustered failures, the probability of ki fail-
ures in the interval xi to xi+1 is (F (xi+1, Θ) − F (xi, Θ))ki.
The contribution of suspended elements to the likelihood
funtion in the case of single data is the probability to survive
the point xj,
treatment of
clustered suspensions would require the probability of having
kj elements with a mileage betweeen xj and xj+1 under the
condition that the elements have not yet failed. Assuming that
the size of each cluster is not too large, this probability can
be approximated by the assumption that each element in the
cluster survives the beginning of the cluster, which simply
leads to (1 − F (xj, Θ))kj .
This then leads to the equation L(Θ) = F (Θ)+1S · S(Θ),
that has to be maximized (as the traditional nonlinear opti-
mization always minimizes, the negative of this equation has
to be minimized). Hereby 1S is an indicator function that is 1
if there are suspended elements in the data, and 0 otherwise.
F (Θ) treats the failures and is deﬁned as
nf(cid:21)
nfc(cid:21)
i=1
i=1
j=1
F (Θ) =
S(Θ) =
and S(Θ) represents suspensions and is deﬁned as
ln (f(xi, Θ)) for single failures
ki · ln (F (xi+1, Θ) − F (xi, Θ)) otherwise
ns(cid:21)
nfs(cid:21)
ln (1 − F (xj, Θ)) for single suspensions
kj · ln (1 − F (xj, Θ)) otherwise.
j=1
Finally, also constraints on the distribution parameters have to
be considered by the optimization algorithm used.
The optimization techniques used for the solution of the
equations are an interior point penalty method combined with
a Variable Metric algorithm and a version of the Nelder-Mead
Algorithm, the ideas of which are surveyed in Section IV-A.1.
The problem in practice is, that a local optimization might not
sufﬁce because the likelihood function is not guaranteed to be
unimodal.
The number of local optima depends on the quality of the
data. Data with only little errors will lead to fewer local optima
while data with many errors usually can be approximated
by more than one combination of distribution parameters.
Therefore, one needs also a global optimization procedure. We
suggest to use an evolution strategy, as described in Section IV-
A.2. The use of a deterministic global optimization strategy as
it could be implemented using interval arithmetic as explained
in Section IV-A.2 is theoretically also possible, but fails in
practice due to runtime problems because a huge area has to
be searched for the global optimum (see Section IV-A.2 and
IV-B for details).
1) Local Optimization Techniques: As already mentioned
above, an interior point penalty method [9] and a Nelder-
Mead algorithm [12] are used for the local optimization of
the likelihood function. The idea behind the interior point
penalty method is to transform the optimization problem with
constraints into an unconstrained problem by adding a penalty
to the function that is to be optimized if the boundary of
the feasible area is reached from inside the feasible area [9].
Therefore, leaving the feasible area is automatically avoided
if the minimum of the function is searched. The optimization
of the transformed problem can then be done using a local
optimization algorithm that does not jump through the search-
space as in this case the barrier that prevents leaving the
feasible area may be skipped. One appropriate method is the
so called Variable Metric method [9] that is a kind of gradient
method and therefore requires the calculation of the gradient of
the likelihood-function, which is quite expensive in the case of
the bathtub distribution and sometimes suffers from numerical
problems.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:08:53 UTC from IEEE Xplore.  Restrictions apply. 
To overcome these problems, an optimization method that
does not require the computation of the gradient has also
been implemented. It belongs to the class of Nelder-Mead-
algorithms [12] and therefore only requires the comparison of
function values. As this method jumps through the space of
possible parameters, a combination with the penalty method
is not possible, but one has to check in each step, if the newly
created points are feasible and maybe repeat the step with
other points.
2) Global Optimization:
a) Evolution Strategy: Evolution strategies (ES) follow
the same idea as genetic algorithms (GA) and are therefore
a means of heuristic global optimization. The basic idea
is to start with an initial set of parameter-vectors, called a
population, to chose the best ones from these vectors, i.e. the
ones that yield the minimum value of the object function, and
generate new vectors as combinations of these “best, known”
vectors [13]. The open questions are therefore, when to stop
the evolution process and how to combine existing elements to
new ones. Fig. 5 shows the principal algorithm of an Evolution
Strategy.
Fig. 5. The idea of an evolution strategy
For the generation of new elements, recombination and
mutation are employed. Recombination means the generation
of a new parameter-vector from one or more parent vectors.
Mutation is a process that changes elements of a given vector
by chance.
For recombination, there are multiple possible strategies that
we have implemented. First, one can just duplicate an existing
parameter-vector. Second, one can take two parents and choose
by chance each parameter from one of the parents. Third, one
can take the mean value of both parents for each parameter.
The problem with mutation is to ﬁnd a strategy that on
the one hand leads to a large covered search-space and on
the other hand does not hinder convergence. Therefore in the
start-phase mutation should have a great inﬂuence to avoid
convergence to a local optimum and when the population
converges to a subspace, the inﬂuence of mutation should be
reduced. Note that this is again an optimization problem, as the
best mutation strategy should be found. Again, an evolution
strategy can be used for it, which means that the vector of
parameters is augmented by a vector of values that indicates
the current inﬂuence of mutation for each parameter and these
additional values are also optimized by the evolution strategy.
This control of mutation by the evolution strategy itself is
generally called meta-mutation [13].
As for the recombination, there are several possible strate-
gies for the selection of the elements that should form the
population in the next step. First, only the best of the newly
created elements may be taken. Second, the selection can take
the best elements from the parent and the child-generation. But
as this may lead to an early convergence to a local optimum,
there should be a maximum age (number of iteration steps)
of elements, after which these elements are excluded from the
population. This is the strategy that is ﬁnally implemented.
Finally, the algorithm for one population can be executed
on different populations independently from each other and
after a number of iterations in each of the populations the
best of the populations are chosen to be evaluated further,
such that the evolution strategy (with selection, recombination
and mutation) can also be executed on the population-level.
In our implementation, ES is used for both,
the point-
estimation and the conﬁdence-interval calculation described
in Section IV-B. One should pay attention that these are both
optimization problems which are of completely different char-
acteristics. In the point-estimation case, one has a huge search-
space and relatively few local optima while in the conﬁdence-
interval calculation one has a small search-space and more
local optima. Therefore, one has to provide different sets
of parameters for point-estimation and conﬁdence-interval-
calculation.
b) Interval Aritmetic: Interval arithmetic is a technique
for deterministic global optimization that was introduced by
Moore [14] and is described in detail in [15]. The idea of
interval arithmetic is to calculate not only a simple function
value but also to give an estimation for the co-domain of a
function if the parameters vary in a given interval. The space
that is deﬁned by the intervals for the parameters is commonly
referred to as “box”. The basis of interval arithmetic is
the extension of the basic arithmetic operations to interval
evaluations. All other arithmetic operations can then be built
upon these basic operations.
The basic idea of a global optimization algorithm using
interval arithmetic is now to divide the search-space into
boxes, making interval-evaluations over the boxes and exclud-
ing boxes that will not contain the optimum because of too
high (or low) lower (or upper) bounds of the interval-value.
One problem with interval arithmetic that hinders its use is a
fact that is commonly referred to as “overestimation problem”.
Interval arithmetic does usually not return the true co-domain,
but an interval that encloses the true co-domain. The order of
magnitude of this overestimation depends on the number of
interval arithmetical operations, their order and the size of the
box. As the co-domain of the interval-evaluation converges to
the true co-domain of the function as the size of the box is
reduced, the solution of the overestimation problem is box-
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:08:53 UTC from IEEE Xplore.  Restrictions apply. 
;
A
I



F
J
E

K

B

K

@


K
J
=
J
E


5
A

A
?
J
E


4
A
?


>
E

=
J
E


1

E
J
E
=

E

=
J
E


5
J

F
splitting, i.e. the original box is divided into sub-boxes which
are investigated again. To reduce the number of temporary
boxes, special strategies as explained in [16] can be used to
exclude boxes that will deﬁnitely not contain the optimum.
For the minimization of the likelihood-function, the number
of interval-arithmetic operations is high and the initial search
space has to be very large. Even with the techniques from
[16] there would be a number of boxes to investigate that
brings runtime to a level that is no longer acceptable and to
an amount of space that would exceed the memory available
on most PCs. Therefore, we do not use interval arithmetic for
the point estimation, but for the conﬁdence-interval calculation
described in Section IV-B.
B. Conﬁdence-Interval-Calculation
3
2
1
0
0.8
0.6
beta
0.4
2000
2000
alpha
alpha
4000
4000
0.2
6000
Fig. 6. The conﬁdence function
Analogously to the regression case, conﬁdence intervals are
employed to give a hint about the goodness of the estimated
parameters. In the case of a maximum likelihood estimation,
likelihood ratio conﬁdence bounds are an appropriate means
as they can reuse the result of the point-estimation. According
to [17], these bounds are deﬁned as
− χ2
(ε,k)
LH(Θ)
LH( ˆΘ)
≥ e
2
,
(1)
where LH(Θ) is the value of the non-logarithmic likelihood
function. ˆΘ is the parameter-vector that was found by the
point-estimation. χ2
(ε,k) is the χ2-quantile with k degrees of
freedom, which is just the number of estimated parameters,
and the error-probability ε.
Equation (1) can be transformed into the form
≥ 0,
ln(LH(Θ))
(cid:15)
+ χ2
(ε,k)
(cid:13)(cid:14)
2
(cid:13)(cid:14)
(cid:12)
(cid:15)
− ln(LH( ˆΘ))
(cid:15)
=L( ˆΘ)
(cid:13)(cid:14)
=L(Θ)
(cid:12)
(cid:12)
:=g(Θ)
2
(ε,k)
2
where g(Θ) is referred to as conﬁdence function in the
remaining sections of this paper. The interpretation of (2) is
that all points are within the conﬁdence interval for which
the logarithmic likelihood function takes on values that are at
most χ
smaller than the value at the maximum likelihood
point. The boundary of the conﬁdence intervals is therefore
given by g(Θ) = 0. Unfortunately, this equation is fulﬁlled at
inﬁnitely many points such that a normal search for zeros of
this function will not help as one is searching for the whole
zero level of a function.
For a two-parameter distribution, there exists a graphical
solution that is the basis of the algorithm explained below.
Fig. 6
shows the conﬁdence function of a two-parameter Weibull
distribution for a sample of 50 random values and conﬁdence
level 95 %, where points, for which the conﬁdence function
yields values smaller than zero are set to zero. In Fig. 7 the
ﬁnal contour plot that contains the ﬁnal conﬁdence intervals
is shown together with a rectangle that encloses the zero-level
of the conﬁdence function. In this example, the values for the
Fig. 7. The conﬁdence intervals for a two-parameter Weibull distribution
(2)
characteristic lifetime α are between about 1100 and 4800 and
for the shape parameter β between 0.38 and 0.68.
The idea of the algorithm for the conﬁdence interval calcu-
lation is to enclose this zero-level by a rectangle of minimum