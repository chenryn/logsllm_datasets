title:Plinius: Secure and Persistent Machine Learning Model Training
author:Peterson Yuhala and
Pascal Felber and
Valerio Schiavoni and
Alain Tchana
Plinius: Secure and Persistent Machine Learning Model
Training
Peterson Yuhala, Pascal Felber, Valerio Schiavoni, Alain Tchana
To cite this version:
Peterson Yuhala, Pascal Felber, Valerio Schiavoni, Alain Tchana. Plinius: Secure and Persistent Ma-
chine Learning Model Training. 2021 51st Annual IEEE/IFIP International Conference on Dependable
Systems and Networks (DSN), Jun 2021, Taipei (virtual), Taiwan. hal-03183871
HAL Id: hal-03183871
https://hal.archives-ouvertes.fr/hal-03183871
Submitted on 29 Mar 2021
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Plinius: Secure and Persistent Machine Learning
Model Training
Peterson Yuhala
University of Neuchâtel
Neuchâtel, Switzerland
PI:EMAIL
Pascal Felber
University of Neuchâtel
Neuchâtel, Switzerland
PI:EMAIL
Valerio Schiavoni
University of Neuchâtel
Neuchâtel, Switzerland
Alain Tchana
ENS Lyon, France
Inria
PI:EMAIL
PI:EMAIL
Abstract—With the increasing popularity of cloud based ma-
chine learning (ML) techniques there comes a need for privacy
and integrity guarantees for ML data. In addition, the signiﬁcant
scalability challenges faced by DRAM coupled with the high
access-times of secondary storage represent a huge performance
bottleneck for ML systems. While solutions exist to tackle the
security aspect, performance remains an issue. Persistent memory
(PM) is resilient to power loss (unlike DRAM), provides fast and
ﬁne-granular access to memory (unlike disk storage) and has
latency and bandwidth close to DRAM (in the order of ns and
GB/s, respectively). We present PLINIUS, a ML framework using
Intel SGX enclaves for secure training of ML models and PM
for fault tolerance guarantees. PLINIUS uses a novel mirroring
mechanism to create and maintain (i) encrypted mirror copies
of ML models on PM, and (ii) encrypted training data in byte-
addressable PM, for near-instantaneous data recovery after a
system failure. Compared to disk-based checkpointing systems,
PLINIUS is 3.2× and 3.7× faster respectively for saving and
restoring models on real PM hardware, achieving robust and
secure ML model training in SGX enclaves.
I. INTRODUCTION
Privacy-preserving machine-learning [7] is a challenging
computational paradigm and workﬂow. Data and computation
must be protected from several threats, e.g., powerful attackers,
compromised hypervisors and operating systems, and even
malicious cloud or human operators [33], [34]. Preserving the
conﬁdentiality of the models (i.e., weights and biases) being
trained, as well as the input datasets is paramount: these are the
most valuable business assets. Example application domains
include health, ﬁnance, Industry 4.0, etc. Given the signiﬁcant
amount of computing resources typically required during the
training phase, moving the model training over public clouds
appears to be a pragmatic approach. However, this immediately
leads to contradictory arguments. On the one hand, one beneﬁts
from the endless scalability and dependability features of public
clouds, as well as pushing away valuable assets from potentially
compromised on-premises infrastructures to the cloud. On
the other hand, exposing conﬁdential datasets and models to
untrusted clouds must be avoided. Fig. 1 shows our target
scenario, i.e., training of ML models over untrusted public
clouds, showcasing the threats that this scenario implies.
Trusted execution environments (TEE) are quickly becoming
the go-to solution to tackle such conﬁdentiality requirements,
and several cloud providers nowadays offer TEE-enabled
1
Fig. 1: ML and model building over untrusted public clouds. In
case of failures or job pre-emption (.), the current state should
be securely persisted to PM. Without proper mitigations, data and
trained models could leak (I).
computing instances (IBM,1 Azure2). Intel software guard
extensions (SGX) [12] is a TEE that offers applications secure
memory regions called enclaves to shield code and data from
unwanted accesses. SGX is a promising candidate for protecting
applications, given its wide availability across a variety of
cloud providers. However, SGX imposes security restrictions on
enclave code (e.g., disallowing system calls), typically requiring
application-level changes, as well as limited memory capacity
of SGX enclaves, which requires developers to minimize the
trusted computing base (TCB).
While ofﬂoading ML training jobs to SGX-enabled clouds
might solve the conﬁdentiality issue, such jobs are typi-
cally deployed in batch. Typically, batch jobs have lower
priorities than latency sensitive (e.g., production, user-facing)
services [21]. In order to avoid resource waste because of
workload variations, ML applications are colocated with latency
sensitive applications. The former are automatically killed
when the latter needs more resources. Another practice which
may lead to interruptions of ML jobs is the use of cheap
yet unreliable virtual machine instances such as EC2 spot
instances [38]. The latter are automatically terminated when a
better offer (i.e., spot price) is made by another user. To avoid
restarting model training from scratch when a task is killed, one
can checkpoint/restore the current model on persistent storage.
For instance, AWS SageMaker [28] suggests to frequently
checkpoint the models to avoid abrupt data-loss when using
spot instances. Fig. 3 represents this general ML pipeline.
However, frequent checkpointing on secondary storage leads
to signiﬁcant I/O overheads, and relying on volatile memory
(i.e., DRAM) to mitigate these overheads would prevent to
resume the job in case of task eviction.
1https://www.ibm.com/cloud/data-shield
2https://azure.microsoft.com/en-us/solutions/conﬁdential-compute/
Untrusted cloudSGXTrained modelNode1PMSGXNode2PMSGXNode3PMExportTraining dataImportEmerging memory technologies like persistent memory
(PM) [40] have the potential to address the signiﬁcant scala-
bility challenges faced by DRAM, as well as the high latency
of secondary storage. PM is persistent on power failure, byte-
addressable, and can be accessed via processor load and
store instructions. Recent work [40] shows how on-the-
market PM solutions such as Intel Optane DC PM [40] result in
signiﬁcant performance gains for various applications. Cloud
services like MS Azure already provide PM offerings [1],
and we expect this technology to gain even more momentum.
However, using PM in privacy-preserving ML jobs opens
additional security risks: conﬁdential model parameters could
be persisted in plain-text on PM, or possibly be exposed
at runtime to malicious privileged users or compromised
operating systems. We take the stance that there is the need
to develop tools and mechanisms to enable these applications
to leverage PM in such secure computation environments. In
this work we build the ﬁrst framework that integrates secure
ML with Intel SGX with fault tolerance on PM. State-of-the-
art PM libraries (e.g., Intel Persistent Memory Development
Kit 5 , Romulus [11], etc.), as well as ML frameworks (e.g.,
Tensorﬂow [6], Darknet [3], etc.), require considerable porting
efforts to be fully functional within SGX enclaves. Tools exist
(e.g., library OSes like Graphene-SGX [35] and Occlum [32], or
modiﬁed enclave-compatible C libraries like SCONE [8]) to run
unmodiﬁed applications inside SGX enclaves, at the downside
of larger TCB sizes (thus larger attack surfaces) and large
memory footprint, thus reducing performance. In ML scenarios
with large conﬁdential models and data sets, enclave memory
becomes a major bottleneck that only important engineering
efforts could optimize.
We present PLINIUS, a secure ML framework that leverages
PM for fast checkpoint/restore of machine learning models.
PLINIUS leverages Intel SGX to ensure conﬁdentiality and
integrity of ML models and data during training, and PM for
fault tolerance. PLINIUS employs a mirroring mechanism which
entails creating an encrypted mirror copy of an enclave model
directly in PM. The mirror copy in PM is synchronized with
the enclave model across training iterations. PLINIUS maintains
training data in byte addressable PM. Upon a system crash or
power failure while training, the encrypted ML model replica
in PM is securely decrypted in the enclave, and used as next
starting point of the training iteration: the training resumes
where it left off, using training data already in memory. This
avoids costly serialization operations of disk-based solutions. To
validate our approach, we build and contribute SGX-DARKNET,
a complete port of Darknet ML framework [3] to SGX, as well
as SGX-ROMULUS, an SGX-compatible PM library on top of
an efﬁcient PM library [11]. We compare SGX-ROMULUS with
unmodiﬁed Romulus library running in a SCONE container
and our results show SGX-ROMULUS is best suited for PLINIUS
framework. Using PLINIUS, we build and train convolutional
neural network (CNN) models with real world datasets (i.e.,
MNIST [2]) and show PLINIUS reduces overhead by ∼3.5×
for model saving, and ∼2.5× for model restores with real SGX
hardware and emulated PM.3
In summary, we make the following contributions:
• We implement and release as open-source SGX-ROMULUS4
on top of Romulus [11] for Intel SGX. SGX-ROMULUS
manipulates PM directly from within SGX enclaves, without
costly enclave transitions between secure and unsecured parts
of an SGX application.
• We design, build, and release as open-source, SGX-DARKNET
4 an extension of Darknet [3] for Intel SGX. SGX-DARKNET
can perform secure training and inference on ML models
directly inside SGX enclaves.
• We present PLINIUS, an open-source framework4 that
leverages SGX-ROMULUS and SGX-DARKNET to provide
an end-to-end fault tolerance mechanism to train models in
privacy preserving ML settings.
• We provide a comprehensive evaluation of PLINIUS, using
real PM hardware and real AWS Spot traces, showing
its better performance when compared with traditional
checkpointing on secondary storage (i.e., disk or SSD)
Roadmap. This paper is organized as follows. §II describes
background concepts, while §III presents our threat model. §IV
presents the architectures of SGX-ROMULUS, SGX-DARKNET
and PLINIUS, while §V digs into the implementation details.
The experimental evaluation of our system is in §VI. We discuss
related work in §VII, before concluding and hinting at future
work in §VIII.
II. BACKGROUND
This section presents a background on Intel SGX, PM, as
well as some machine-learning concepts speciﬁc to PLINIUS.
Intel software guard extensions (SGX) [12] is a set of
extensions to Intel’s architecture that permits applications to
create CPU-protected memory areas (i.e., enclaves) shielding
conﬁdential code and data from disclosure and modiﬁcations.
SGX reserves a secure memory region called the enclave
page cache (EPC) for enclave code and data. The processor
ensures that software outside the enclave (e.g., the OS kernel
or hypervisor) cannot access EPC memory. The enclave can
access both EPC and non-EPC memory.
Data in the EPC is in plaintext only in on-chip caches and
is encrypted and integrity-protected in the memory encryption
engine (MME) once it is evicted from the cache to memory.
Current Intel processors support a maximum of 128 MB of
EPC memory, of which 93.5 MB is usable by SGX enclaves.
This limits the total size of code and data allowed within the
EPC. To support applications with larger memory needs, the
Linux kernel provides a paging mechanism for swapping pages
between the EPC and untrusted memory.
Enclaves cannot issue system calls and standard OS ab-
stractions (e.g., ﬁle systems, network), which are ubiquitous
in real world applications. All system services thus require
costly enclave transitions, up to 13’100 CPU cycles [39]. The
Intel SGX application design requires splitting applications
3Machines with SGX and PM support not on the market yet (Oct/2020).
4https://github.com/Yuhala/sgx-pm-ml
2
Fig. 2: Read/write throughput for sequential/random workloads on
SSD, PM and Ramdisk using the sync I/O engine on FIO. 512 MB
ﬁle per thread, 4 KB block size. Write workloads issue an fsync
for each written block, average over 3 runs.
into a trusted (the enclave) and untrusted part. To achieve
communication across the enclave boundary, the Intel SGX
SDK provides specialized function call mechanisms,i.e., ecalls
and ocalls, respectively to enter and exit an enclave [10].
To mitigate security risks, the TCB should be as small
as possible. Systems exist to run unmodiﬁed applications
inside enclaves, either by porting entire library OSes into the
enclave [32], [35] or via a modiﬁed libC library, specialized
for containerized services [8]. These solutions are efﬁcient
with small application binaries, but quickly show limits for
memory-constrained applications such as ML.
Persistent memory and PM libraries. PM is a novel memory
technology that is non-volatile, byte-addressable, and has
latency and bandwidth similar to that of DRAM. PM resides on
the memory bus and can be accessed directly using CPU load
and store instructions. Intel Optane DC PM is commercially
available since April 2019. Optane DC PM scales better than
DRAM and hence provides much larger capacity (up to 512 GB
per PM module). Intel Optane DC PM modules can operate
in two modes: memory mode where they are simply used to