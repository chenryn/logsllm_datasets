# 21 \| Catalyst逻辑计划：你的SQL语句是怎么被优化的？（上）你好，我是吴磊。上一讲我们说，Spark SQL 已经取代 Spark Core成为了新一代的内核优化引擎，所有 Spark 子框架都能共享 Spark SQL带来的性能红利，所以在 Spark 历次发布的新版本中，Spark SQL占比最大。因此，Spark SQL的优化过程是我们必须要掌握的。Spark SQL 端到端的完整优化流程主要包括两个阶段：Catalyst 优化器和Tungsten。其中，Catalyst优化器又包含逻辑优化和物理优化两个阶段。为了把开发者的查询优化到极致，整个优化过程的运作机制设计得都很精密，因此我会用三讲的时间带你详细探讨。下图就是这个过程的完整图示，你可以先通过它对优化流程有一个整体的认知。然后随着我的讲解，逐渐去夯实其中的关键环节、重要步骤和核心知识点，在深入局部优化细节的同时，把握全局优化流程，做到既见树木、也见森林。![](Images/31ff6bf43ec44af7314082661584c902.png)savepage-src="https://static001.geekbang.org/resource/image/f3/72/f3ffb5fc43ae3c9bca44c1f4f8b7e872.jpg"}Spark SQL的优化过程今天这一讲，我们先来说说 Catalyst优化器逻辑优化阶段的工作原理。案例：小 Q 变身记我们先来看一个例子，例子来自电子商务场景，业务需求很简单：给定交易事实表transactions 和用户维度表 users，统计不同用户的交易额，数据源以 Parquet的格式存储在分布式文件系统。因此，我们要先用 Parquet API读取源文件。    val userFile: String = _    val usersDf = spark.read.parquet(userFile)    usersDf.printSchema    /**    root    |-- userId: integer (nullable = true)    |-- name: string (nullable = true)    |-- age: integer (nullable = true)    |-- gender: string (nullable = true)    |-- email: string (nullable = true)    */    val users = usersDf    .select("name", "age", "userId")    .filter($"age"  Add(l, r)      case IntegerLiteral(i) if i > 5 => Literal(1)      case IntegerLiteral(i) if i  Literal(0)    }首先，我们定义了一个表达式：（（6 - 4）\*（1 -9）），然后我们调用这个表达式的 transformDown高阶函数。在高阶函数中，我们提供了一个用 case定义的匿名函数。显然，这是一个偏函数（PartialFunctions），你可以把这个匿名函数理解成"自定义的优化规则"。在这个优化规则中，我们仅考虑3 种情况： 1.  对于所有的二元操作符，我们都把它转化成加法操作        2.  对于所有大于 5 的数字，我们都把它变成    1    3.  对于所有小于 5 的数字，我们都把它变成    0    虽然我们的优化规则没有任何实质性的意义，仅仅是一种转换规则而已，但是这并不妨碍你去理解Catalyst 中 TreeNode 之间的转换。当我们把这个规则应用到表达式（（6 -4）\*（1 - 9））之后，得到的结果是另外一个表达式（（1 + 0）+（0 +1）），下面的示意图直观地展示了这个过程。![](Images/babce2fdb4eef0fcfa64601a0a74558d.png)savepage-src="https://static001.geekbang.org/resource/image/ea/1f/ea21ec9387e55e94d763d9ee0c4a4b1f.jpg"}自顶向下对执行计划进行转换从"Analyzed Logical Plan"到"Optimized LogicalPlan"的转换，与示例中表达式的转换过程如出一辙。最主要的区别在于，Catalyst的优化规则要复杂、精密得多。Cache Manager 优化从"Analyzed Logical Plan"到"Optimized Logical Plan"的转换，Catalyst除了使用启发式的规则以外，还会利用 Cache Manager做进一步的优化。**这里的 Cache指的就是我们常说的分布式数据缓存。想要对数据进行缓存，你可以调用DataFrame 的.cache 或.persist，或是在 SQL 语句中使用"cachetable"关键字**。Cache Manager其实很简单，它的主要职责是维护与缓存有关的信息。具体来说，Cache Manager维护了一个 Mapping 映射字典，字典的 Key 是逻辑计划，Value 是对应的 Cache元信息。 当 Catalyst 尝试对逻辑计划做优化时，会先尝试对 Cache Manager查找，看看当前的逻辑计划或是逻辑计划分支，是否已经被记录在 Cache Manager的字典里。如果在字典中可以查到当前计划或是分支，Catalyst 就用InMemoryRelation节点来替换整个计划或是计划的一部分，从而充分利用已有的缓存数据做优化。小结今天这一讲，我们主要探讨了 Catalyst优化器的逻辑优化阶段。这个阶段包含两个环节：逻辑计划解析和逻辑计划优化。在逻辑计划解析环节，Catalyst 结合 Schema信息，对于仅仅记录语句字符串的 Unresolved LogicalPlan，验证表名、字段名与实际数据的一致性。解析后的执行计划称为 AnalyzedLogical Plan。在逻辑计划优化环节，Catalyst 会同时利用 3 方面的力量优化 AnalyzedLogical Plan，分别是 AQE、Cache Manager和启发式的规则。它们当中，Catalyst最倚重的是启发式的规则。尽管启发式的规则多达 81 项，但我们把它们归纳为 3大范畴：谓词下推、列剪裁和常量替换。我们要重点掌握谓词下推和列剪裁，它们的优化动机和"能省则省"的开发原则一样，核心思想都是用尽一切办法，减少需要扫描和处理的数据量，降低后续计算的负载。针对所有的优化规则，Catalyst 优化器会通过调用 TreeNode 中的transformDown高阶函数，分别把它们作用到逻辑计划的每一个节点上，直到逻辑计划的结构不再改变为止，这个时候生成的逻辑计划就是Optimized Logical Plan。最后，Cache Manager的作用是提供逻辑计划与数据缓存的映射关系，当现有逻辑计划或是分支出现在Cache Manager 维护的映射字典的时候，Catalyst可以充分利用已有的缓存数据来优化。每日一练1.       既然 Catalyst 在逻辑优化阶段有 81    条优化规则，我们还需要遵循"能省则省、能拖则拖"的开发原则吗？        2.       你能说说 Spark 为什么用偏函数，而不是普通函数来定义 Catalyst    的优化规则吗？        期待在留言区看到你的思考和答案，我们下一讲见！