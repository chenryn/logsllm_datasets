artifacts may necessarily outlive the connection itself for some time (consider the
session resumption feature of TLS).
3.0.1 Design Decisions
The foremost design goal was accessibility and reproducibility by the
community. This goal translated to the selection of tools and techniques that were
exclusively either free and publicly available or open source. A clear exception to
the rule was the use of Windows operating systems as these are the subject of the
research; even in this case, freely available development virtual machines from
Microsoft were used as much as possible towards the ease of reproducibility. A ﬁnal
nuance of the stated goal is that preference was given to public tools released by
Microsoft speciﬁcally, in an e↵ort to remain as “native” as possible.
Another contributor to the decision to remain native as often as possible was
the second design goal: rapidity and e↵ectiveness of analysis. By leveraging
Microsoft tools, such as WinDbg, and built-in capabilities, like on-demand
40
user-mode process dump creation, a variety of inherent beneﬁts became available to
speed up analysis. For instance:
• User-mode process dumps can be created without additional tools beginning
with Windows Vista (Microsoft, 2010).
• Native creation of these dumps does not require or induce any special state for
the operating system (no need to pause or restart the system), allowing rapid
sampling repetition during any variable manipulation.
• The dumps are “sparse” by default, meaning that unused ranges in the virtual
address space are not included. This creates a more eﬃcient search scope that
avoids unnecessarily parsing empty ranges of memory.
• The user-mode dumps can be read natively by WinDbg, which in turn can
automatically leverage Microsoft’s public symbols (Program Databases or
PDBs) to make sense of structures and memory pointers therein. Although
public, the information provided by Program Databases (PDBs) is not
necessarily documented via any other source, and they have historically played
an integral role in Windows memory forensics (Dolan-Gavitt, 2007).
In order to generate TLS sessions in a way that is native, RDP was used.
The RDP client and server functionality is built-in to all modern releases of
Windows Microsoft (2015c), and so requires very little alteration to enable testing of
a given operating system. The use of RDP simultaneously minimizes external
variables that would be introduced by third-party software and has the ancillary
beneﬁt of exploring one of the use-cases presented previously (Incident Response).
Finally, the desire for ﬂexibility inﬂuenced the experimental design towards
virtualization. The use of virtual machine abstraction allows for portability and
repeatability (through snapshots for example). Particularly though, it enabled
inspection of RAM without reading through the target operating system, thus
leaving it mostly insulated to analysis. Using abstract virtualization mechanisms
like pausing and snapshots, the guest physical memory could be saved to a ﬁle
(VMEM) on the host, which could then be analyzed and manipulated as needed.
41
The VMEM format is supported by powerful open source analysis tools (e.g.,
Volatility and Rekall), which are able to trivially convert it to other formats. These
tools were leveraged to convert the VMEM to a raw copy, in which physical o↵sets
into the ﬁle were synonymous with o↵sets into physical memory. Physical o↵sets
produced by scanning tools like bulk extractor could then be compared with the
physical o↵sets rendered by virtual to physical memory translations of analysis tools
like Rekall and Volatility.
3.0.2 Overview
The methodology that emerged from the interaction of these design goals
and the technology available to meet them is brieﬂy highlighted in Figure 3.1 and
then expanded upon in Section 3.0.3. Philosophically, there were at least two
approaches to the problem of identifying the key – observing the process (live
debugging and static analysis) or observing the data (post-mortem analysis). While
both are valid and they are not mutually exclusive (in fact, they are combined in
the articulated methodology), post-mortem analysis was given precedence for three
reasons. Nominally, these are scope, simplicity, and ﬁdelity to the problem.
The exact scope of debugging wasn’t quite known – the executable(s) and
functions that interact with the master key were unknown, though presumed to
originate with, or orientate around, Schannel and the cryptographic libraries. While
parsing PDBs ahead of time would have provided insight to develop scope, in
isolation it could have introduced biases and pivoted the focus of analysis with
misleading or enticing symbol names. It could have also falsely restricted the scope
through exclusion of symbols (and many sensitive symbols are excluded in the
public PDBs (Microsoft, 2015b)) for functions that are highly relevant. It would be
possible to simply trace through a TLS transaction to generate scope, but this still
requires enabling kernel debugging if attaching to the LSASS process. Manually
tracing would also likely run into issues due to timing requirements when dealing
42
with live network connections. This too is solvable through adjustments, but each
adjustment to the base system moves the environment further and further from the
actual parameters being emulated, thus allowing lurking variables to creep in. Any
adjustment would also need to be made against every test system.
The problem itself is one that would be post-mortem – whether or not the
artifacts exists after a connection has been established, possibly terminated. If the
key exists temporarily during execution and is freed or destroyed very quickly, this
is still interesting, but less so in the forensic context of the problem. Analyzing in a
post-mortem way allowed for closer alignment with the use cases and minimized
temporally-based variables to some degree. It also provided information that could
be used to make more informed debugging and tracing decisions, enhancing the
potency of the debugging e↵ort.
43
Search heuristically for session keys
Identify known public values used to derive the master key
Intercept master key (MK) generation on client to server
Search for MK to identify context in server memory
Leverage unique context features to identify other MK instances
Walk pointers to identify related structs
Compare related context to unique public values
Debug live connections and secret generation
Scan physical memory for identiﬁed secret structs
MK not identiﬁed
MK identiﬁed
Figure 3.1. TLS artifact identiﬁcation methodology
This implementation, when distilled, resembles the Inman-Rudin paradigm
(Identiﬁcation ! Classiﬁcation ! Individualization ! Association !
Reconstruction (Inman & Rudin, 2002)):
• The ﬁrst several steps are used to “identify” structures that could be relevant
to the event of interest (a TLS connection)
44
• The master key structs are “classiﬁed” and enumerated (by virtue of being
pseudo-random, they are inherently “individualized”)
• After identifying and classifying master keys and other structures, a given MK
is then “associated” with a given event via unique features of the connection
• Finally, the associated MK and connection parameters are used to decrypt the
TLS session and reconstruct the event of interest
3.0.3 Discussion of Methods
This section augments the overview of the methodology by activity depicted
in Figure 3.1.
3.0.3.1. Searching for session keys
During previous research and analysis of LSASS, the author used AES key
schedule heuristic scanning (via the “FindAES” tool produced by Jesse Kornblum
(2011) to identify the AES keys used to encrypt user secrets in LSASS. This same
technique was applied to session keys when AES was used in the cipher suite of a
TLS connection. The theory was that, as symmetric keys, the AES keys used for
encryption of the session would exist in both the client and the server memory for
the duration of the connection (generated in the LSASS process of both if they are
Windows machines). Scanning both hosts for AES key schedules may then yield two
or more pairs of matching keys (the client and server write keys), which would be
those used for the connection (Dierks & Allen, 1999; Freier et al., 2011).
3.0.3.2. Identifying known values
One method used to identify possible structures that could either contain or
lead to secrets of a connection was to start with known-knowns. In the context of a
TLS connection, that term applies to public values – either those unique to the
45
connection, such as the client/server random values, or constant values like those
deﬁned in the RFCs (Dierks & Rescorla, 2008, p. 68). Unique values present the
opportunity to enumerate and dissect artifacts of a speciﬁc connection. Conversely,
constants as deﬁned in the RFCs, or perhaps particular to the implementation, act
as generic identiﬁers that can be used to laterally identify similar artifacts across
arbitrary connections. This was useful when no particular connection was targeted
or known, and instead the desire was to exhaust all connection artifacts in memory.
The reasoning behind searching speciﬁcally for public values was several-fold.
The ﬁrst reason was that, while secret values (e.g., keys) may be sequestered in an
encrypted “vault” in memory, public values likely remain unencrypted in memory,
as they are unprotected in other contexts. The second was that, intuitively, secrets
are often compartmentalized, and so unique to a speciﬁc connection 1. Further, key
material is often pseudo-randomly generated in such a way that detection is
theoretically impossible or impractically costly. This makes secrets less ﬂexible as
search features than looking for public values. Finally, the public values were readily
accessible from a network capture, which matched a use-case identiﬁed and aided in
the ease of testing.
3.0.3.3. Intercepting master key generation
The pre-master key (and subsequently master key) are the two most useful
secrets in decrypting a given connection. This is because, as discussed in Chapter
Two, the master key is used to derive the sessions keys, and always exists, regardless
of cipher suite chosen. This very fact is the reason Perfect Forward Secrecy (PFS) is
important – the private key in non-ephemeral key exchanges is used to decrypt the
pre-master secret that is shared during an RSA TLS handshake. So the usefulness
of the persistent private key in “ex post facto” decryption is predicated on the
usefulness of the pre-master key.
1An exception to this in TLS is the persistent private key
46
Knowing the pre-master key or master key would allow for scanning of
memory to identify the location and number of instances of the key in memory for a
given connection, which could then be compared over several connections to identify
consistent structures (particularly if combined with input fuzzing like altering the
TLS version or cipher suite used in some connections).
Several popular modern browsers (e.g., Firefox and Chrome) allow for
logging of master keys to a ﬁle Mozilla (2015). However, these browsers leverage a
di↵erent cryptographic library, and, even so, the goal is to minimize external
variables on the target host, making RDP or Internet Explorer (IE) preferred
options regardless. Fortunately, several open source RDP clients exist, which rely on
OpenSSL, a well documented open source SSL library. A small “shim” library can
be preloaded to intercept the master key generation calls to OpenSSL and
transparently dump the returned key to a ﬁle. This key can then be compared with
what is resident in memory on the target (Windows) host acting as the RDP server.
3.0.3.4. Leveraging unique structure identiﬁers
The master key itself is pseudo-random. It is therefore desirable to locate
some contextual feature that could be used as a “class” marker for identifying
unknown master keys. Speciﬁcally, there likely exists some structure around, or
reference to, the master key if it is both managed and memory resident. This could
be co-located known values, some form of “magic number,” or even pointers to the
master key from other structures if it has no direct context in memory. Identifying
these features will form the link between master keys and unique connection
identiﬁers that is required for decryption. Any master key “class” markers could be
portable or reusable across other instances of the same implementation (i.e, other
hosts that implement the same Schannel paradigm).
Furthermore, unique “magic” values can provide especially valuable insights.
Values that are magic are hard-coded or the generation thereof is hard-coded. This
47
means that identiﬁed magic values (or the way they are generated) must exist in the
functions of one or more modules loaded into memory. This fact links an artifact
containing the magic value to the modules and functions that created it or use it,
which is useful intelligence for live debugging.
Symbols can be leveraged to extrapolate from magic values. If the memory
address of the magic value (or generator for the magic value) resolves symbolically
to a function named, for example, “GenerateSessionKeys” or
“CreateSSLSessionStruct,” then there is intuitive meaning and informed
speculations can be made at the purpose of the structure.
A caveat hinted at earlier is that the public Windows symbol ﬁles contain
only the symbols that Microsoft deems necessary for reasonable levels of debugging,
so not all functions or global variables are exposed. This has a potentially insidious
side-e↵ect when performing analysis, as the “ln” function of WinDbg that performs
symbol resolution lists the nearest symbols before and after a given address
appended with an o↵set to the provided address. The implication of this is that if
an address exists within a function for which symbols are not present, it may appear
that the address is inside of the next-nearest symbol. For this reason, any time that
function symbols are used for analysis, the function will be disassembled to ensure
that the value of interest is actually contained within.
3.0.3.5. Walking pointers
Related structures are often chained together by pointers in memory.
Starting with a public value, identifying its structure and pointers, and then
dereferencing (or “walking”) the pointers inside of that structure will provide a
series of chained structures. One issue with this approach is that it can quickly
become overwhelming. Pointers can lead to exponential branching (or inﬁnite
looping in the case of lists), so it can be important to have some semblance of the
purpose for the structure being observed before (potentially) needlessly walking a
48
pointer to a value of little or no use. Pointers also exist to functions or static
variables embedded in binary ﬁles, which, if these have symbols, is helpful in
identifying the structure as well.
Another important aspect is walking pointers in reverse to identify what is
pointing to a structure. There are many situations where the artifact of interest
does not point back to structures referencing it. In this case, the simplest solution is
to scan for the memory address of the structure, to see if a pointer to that memory
address exists anywhere else in memory.
3.0.3.6. Comparing related structs to unique public values
This is nearly the inverse of the initial step. Once a series of structures
related to the master key are identiﬁed in memory, they need to be linked to a given
connection to be valuable.
Two examples of ostensibly unique public values are the session ID and the
client/server random values. The random values are particularly interesting, as the
ﬁrst four bytes of the random are very often a timestamp (Freier et al., 2011, p. 25),
which has forensic value. The client random is also the value that is included in the
NSS log format used by browsers and Wireshark (although Wireshark also accepts
the session ID, as mentioned in Chapter Two). As was also mentioned in the
literature review, the session ID can have an interesting quality in the case of
Schannel, because a hard-coded value in the session ID generation process causes
them to typically have zeros in the third and fourth byte positions, acting as a
rough ﬁngerprint (Checkoway et al., 2014). This means that, to some extent, the
server that was connected to can be identiﬁed as a Windows server in the case of a
client. It also provides a visually recognizable pattern when performing analysis.
49
3.0.3.7. Debugging Local Security Authority Sub-System
Up until this stage, every part has been piecing together the reconstruction
of an event (the TLS connection) and the context around that event. Debugging
will then play events forward, and possibly ﬁll in missing information within the
scope of the artifacts identiﬁed, or lead to new artifacts. It also has the potential to
validate or challenges assumptions made about what was seen in memory. An
example is that something which is seen as managed and permanent may be less so
than initially anticipated, or contrastingly, something seen as ephemeral or
non-existent may actually be transitioned to a di↵erent, more permanent (even
encrypted) structure. Static analysis will also likely be performed in advance of this
task where applicable, based upon the complexity of the functions that are
identiﬁed and examined, as that will also narrow the scope for debugging.
3.0.3.8. Scanning Physical Memory
The focal point has been LSASS memory space based largely on
documentation provided by Microsoft; however, artifacts that are generated within
that process do not necessarily remain within its user-mode virtual address space.
This is especially salient when considering how Windows handles kernel memory
(i.e., mapping it into the latter end of the process address space). Scanning physical
memory may help illuminate relationships or external artifacts that were not scoped
into the preceeding methodology (user-mode dumps for example do not include the
kernel address space).
3.0.4 Infrastructure
This section discusses the technical instrumentation of the methodology
(including tools, tool versions, and setup of the live debugging environment), so that
the methods are faithfully reproducible.
50
The design guidelines highlighted previously are manifest in the tool
selection in several demonstrable ways. At a high level, reproducibility and
simplicity led to an attempt to minimize the breadth of tool use and narrow to a
core of analysis suites. The linchpin of these tools is WinDbg for user-mode process
dump analysis and kernel debugging. The choice to use WinDbg also meant that
Windows was the choice for analysis platform. Of the versions available, Windows
10 was used as it is the latest available version from Microsoft at the time of
writing. Microsoft also released a tool for basic tasks related to PDB ﬁles and
executables called “cvdump” that could be leveraged for symbol inspection outside
of WinDbg. To augment WinDbg as a hex viewer, a Windows-based hex editor was
required, and HxD by Ma¨el H¨orz was selected. At the time of writing, HxD is freely
available, comes as a single executable, and supports large ﬁle sizes with quick
searching capability, a key requirement.
What HxD lacks, however, is a tunable scanning engine that can output