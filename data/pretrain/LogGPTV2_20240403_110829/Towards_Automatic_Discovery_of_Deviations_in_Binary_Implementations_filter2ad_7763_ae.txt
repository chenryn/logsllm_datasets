inputs, each time requiring the new candidate input to
be different than the previous ones. The obtained candi-
date inputs often result in different paths. We have done
work on symbolic execution techniques to explore mul-
tiple program paths and plan to apply those techniques
here [2, 17].
Creating formulas including multiple paths.
In this
paper, we apply the weakest precondition on IR pro-
grams that contain a single program path, i.e., the pro-
cessing of the original input by one implementation.
USENIX Association
16th USENIX Security Symposium
225
However, our weakest precondition algorithm is capable
of handling IR programs containing multiple paths [19].
In the future, we plan to explore how to create formulas
that include multiple paths.
On-line formula generation. Our current implemen-
tation for generating the symbolic formula works ofﬂine.
We ﬁrst record an execution trace for each implementa-
tion while it processes an input. Then, we process the
execution trace by converting it into the IR representa-
tion, and computing the symbolic formula. Another al-
ternative would be to generate the symbolic formulas in
an on-line manner as the program performs operations
on the received input, as in BitScope [2, 17].
7 Related Work
Symbolic execution & weakest precondition. Sym-
bolic execution was ﬁrst proposed by King [34], and
has been used for a wide variety of problems includ-
ing generating vulnerability signatures [18], automatic
test case generation [32], proving the viability of evasion
techniques [35], and ﬁnding bugs in programs [21, 47].
Weakest precondition was originally proposed for devel-
oping correct programs from the ground up [24, 26]. It
has been used for different applications including ﬁnding
bugs in programs [28] and for sound replay of application
dialog [42].
Static source code analysis. Chen et al. [23] manually
identify rules representing ordered sequences of security-
relevant operations, and use model checking techniques
to detect violations of those rules in software. Udrea et
al. [45] use static source code analysis to check if a C im-
plementation of a protocol matches a manually speciﬁed
rule-based speciﬁcation of its behavior.
Although these techniques are useful, our approach is
quite different. Instead of comparing an implementation
to a manually deﬁned model, we compare implementa-
tions against each other. Another signiﬁcant difference
is that our approach works directly on binaries, and does
not require access to the source code.
Protocol error detection. There has been considerable
research on testing network protocol implementations,
with heavy emphasis on automatically detecting errors
in network protocols using fuzz testing [3–6, 9, 12, 14,
33, 37, 43, 46]. Fuzz testing is a technique in which ran-
dom or semi-random inputs are generated and fed to the
program under study, while monitoring for unexpected
program output, usually an unexpected ﬁnal state such
as program crash or reboot.
Compared to fuzz testing, our approach is more efﬁ-
cient for discovering deviations since it requires testing
far fewer inputs. It can detect deviations by comparing
how two implementations process the same input, even
if this input leads both implementation to semantically
equivalent states.
In contrast, fuzz testing techniques
need observable differences between implementations to
detect a deviation.
There is a line of research using model checking
to ﬁnd errors in protocol implementations. Musuvathi
et.al. [40, 41] use a model checker that operates directly
on C and C++ code and use it to check for errors in
TCP/IP and AODV implementations. Chaki et al. [22]
build models from implementations and checks it against
a speciﬁcation model. Compared to our approach, these
approaches need reference models to detect errors.
Protocol ﬁngerprinting. There has also been previous
research on protocol ﬁngerprinting [25, 44] but available
ﬁngerprinting tools [8,11,15] use manually extracted ﬁn-
gerprints. More recently, automatic ﬁngerprint genera-
tion techniques, working only on network input and out-
put, have been proposed [20]. Our approach is different
in that we use binary analysis to generate the candidate
inputs.
8 Conclusion
In this paper, we have presented a novel approach to au-
tomatically detect deviations in the way different imple-
mentations of the same speciﬁcation check and process
their input. Our approach has several advantages: (1) by
automatically building the symbolic formulas from the
implementation, our approach is precisely truthful to the
implementation; (2) automatically identifying the devia-
tion by solving formulas generated from the two imple-
mentations enables us to ﬁnd the needle in the haystack
without having to try each straw (input) individually, thus
a tremendous performance gain; (3) our approach works
on binaries directly, i.e., without access to source code.
We then show how to apply our automatic deviation tech-
niques for automatic error detection and automatic ﬁn-
gerprint generation.
We have presented our prototype system to evaluate
our techniques, and have used it to automatically dis-
cover deviations in multiple implementations of two dif-
ferent protocols: HTTP and NTP. Our results show that
our approach successfully ﬁnds deviations between dif-
ferent implementations, including errors in input check-
ing, and differences in the interpretation of the speciﬁca-
tion, which can be used as ﬁngerprints.
226
16th USENIX Security Symposium
USENIX Association
Acknowledgments
We would like to thank Heng Yin for his support on
QEMU and Ivan Jager for his help in developing Bit-
Blaze, our binary analysis platform. We would also like
to thank Vijay Ganesh and David Dill for their support
with STP, and the anonymous reviewers for their insight-
ful comments.
This material is based upon work partially supported
by the National Science Foundation under Grants No.
0311808, No. 0433540, No. 0448452, No. 0627511, and
CCF-0424422. Partial support was also provided by the
International Technology Alliance, and by the U.S. Army
Research Ofﬁce under the Cyber-TA Research Grant No.
W911NF-06-1-0316, and under grant DAAD19-02-1-
0389 through CyLab at Carnegie Mellon.
The views and conclusions contained here are those of
the authors and should not be interpreted as necessarily
representing the ofﬁcial policies or endorsements, either
expressed or implied, of ARO, NSF, or the U.S. Govern-
ment or any of its agencies.
References
[1] The BitBlaze binary analysis platform. http://
www.cs.cmu.edu/˜dbrumley/bitblaze.
[2] Bitscope.
http://www.cs.cmu.edu/
˜chartwig/bitscope.
[3] IrcFuzz. http://www.digitaldwarf.be/
products/ircfuzz.c.
[4] ISIC: IP stack integrity checker. http://www.
packetfactory.net/Projects/ISIC.
[5] JBroFuzz.
http://www.owasp.
org/index.php/Category:OWASP\
_JBroFuzz.
[6] MangleMe.
http://lcamtuf.coredump.
cx.
[7] NetTime. http://nettime.sourceforge.
net.
[8] Nmap. http://www.insecure.org.
[9] Peach. http://peachfuzz.sourceforge.
net.
[10] QEMU: an open source processor emulator.
http://www.qemu.org.
[11] Queso.
http://ftp.cerias.purdue.
edu/pub/tools/unix/scanners/queso.
[12] Spike.
http://www.immunitysec.com/
resources-freesoftware.shtml.
[13] Windows NTP server. http://www.ee.udel.
edu/˜mills/ntp/html/build/hints/
winnt.html.
[14] Wireshark: fuzz testing tools. http://wiki.
wireshark.org/FuzzTesting.
[15] Xprobe. http://www.sys-security.com.
[16] BERNERS-LEE, T., FIELDING, R., AND MAS-
INTER, L. Uniform Resource Identiﬁer (URI):
Generic Syntax. RFC 3986 (Standard), 2005.
[17] BRUMLEY, D., HARTWIG, C., LIANG, Z., NEW-
SOME, J., SONG, D., AND YIN, H. Towards auto-
matically identifying trigger-based behavior in mal-
ware using symbolic execution and binary analy-
sis. Tech. Rep. CMU-CS-07-105, Carnegie Mellon
University School of Computer Science, 2007.
[18] BRUMLEY, D., NEWSOME, J., SONG, D., W., H.,
AND JHA, S. Towards automatic generation of
vulnerability-based signatures.
In Proceedings of
the 2006 IEEE Symposium on Security and Privacy
(2006).
[19] BRUMLEY, D., WANG, H., JHA, S., AND SONG,
D. Creating vulnerability signatures using weakest
pre-conditions.
In Proceedings of the 2007 Sym-
posium on Computer Security Foundations Sympo-
sium (2007).
[20] CABALLERO,
J.,
VENKATARAMAN,
S.,
POOSANKAM, P., KANG, M. G., SONG, D.,
AND BLUM, A. Fig: Automatic ﬁngerprint gen-
eration.
In 14th Annual Network and Distributed
System Security Conference (NDSS) (2007).
[21] CADAR, C., GANESH, V., PAWLOWSKI, P., DILL,
D., AND ENGLER, D. EXE: A system for auto-
matically generating inputs of death using symbolic
execution. In Proceedings of the 13th ACM Con-
ference on Computer and Communications Security
(CCS) (2006).
[22] CHAKI, S., CLARKE, E., GROCE, A., JHA, S.,
AND VEITH, H. Modular veriﬁcation of software
components in C.
In Proceedings of the 25th In-
ternational Conference on Software Engineering
(ICSE) (2003).
[23] CHEN, H., AND WAGNER, D. MOPS: an infras-
tructure for examining security properties of soft-
ware. In Proceedings of the 9th ACM conference
on Computer and Communications Security (CCS)
(2002).
USENIX Association
16th USENIX Security Symposium
227
[24] COHEN, E. Programming in the 1990’s. Springer-
Verlag, 1990.
[25] COMER, D., AND LIN, J. C. Probing TCP imple-
mentations. In USENIX Summer 1994 (1994).
[26] DIJKSTRA, E. A Discipline of Programming. Pren-
tice Hall, Englewood Cliffs, NJ, 1976.
[27] FIELDING, R., GETTYS,
J., MOGUL,
J.,
FRYSTYK, H., MASINTER, L., LEACH, P., AND
BERNERS-LEE, T. Hypertext Transfer Protocol –
HTTP/1.1. RFC 2616 (Draft Standard), June 1999.
Updated by RFC 2817.
[28] FLANAGAN, C., LEINO, K. R. M., LILLIBRIDGE,
M., NELSON, G., SAXE, J. B., AND STATA, R.
Estended static checking for Java.
In ACM Con-
ference on the Programming Language Design and
Implementation (PLDI) (2002).
[29] FLANAGAN, C., AND SAXE, J. Avoiding ex-
ponential explosion: Generating compact veriﬁca-
tion conditions. In Proceedings of the 28th ACM
Symposium on the Principles of Programming Lan-
guages (POPL) (2001).
[30] GANESH, V., AND DILL, D.
STP: A
decision procedure for bitvectors and arrays.
http://theory.stanford.edu/˜vganesh/stp.html.
[37] MARQUIS, S., DEAN, T. R., AND KNIGHT, S.
SCL: a language for security testing of network ap-
plications. In Proceedings of the 2005 conference
of the Centre for Advanced Studies on Collabora-
tive research (2005).
[38] MILLS, D. Simple Network Time Protocol (SNTP)
Version 4 for IPv4, IPv6 and OSI. RFC 4330 (In-
formational), 2006.
[39] MUCHNICK, S. Advanced Compiler Design and
Implementation. Academic Press, 1997.
[40] MUSUVATHI, M., AND ENGLER, D. R. Model
implementa-
checking large network protocol
tions.
In Proceedings of the First Symposium
on Networked Systems Design and Implementation
(NSDI) (2004).
[41] MUSUVATHI, M., PARK, D. Y., CHOU, A., EN-
GLER, D. R.,
, AND DILL, D. L. CMC: A
pragmatic approach to model checking real code.
In Proceedings of the 5th Symposium on Operat-
ing Systems Design and Implementation (OSDI)
(2002).
[42] NEWSOME, J., BRUMLEY, D., FRANKLIN, J.,
AND SONG, D. Replayer: Automatic protocol re-
play by binary analysis. In Proceedings of the 13th
ACM Conference on Computer and and Communi-
cations Security (CCS) (2006).
[31] GANESH, V., AND DILL, D. A decision procedure
In Proceedings of the
for bit-vectors and arrays.
Computer Aided Veriﬁcation Conference (2007).
[43] OEHLERT, P. Violating assumptions with fuzzing.
IEEE Security and Privacy Magazine 3, 2 (2005),
58 – 62.
[32] GODEFROID, P., KLARLUND, N., AND SEN, K.
DART: Directed automated random testing.
In
Proceedings of the 2005 Programming Language
Design and Implementation Conference (PLDI)
(2005).
[33] KAKSONEN, R. A Functional Method for Assess-
ing Protocol Implementation Security. PhD thesis,
Technical Research Centre of Finland, 2001.
[34] KING, J. Symbolic execution and program testing.
Communications of the ACM 19 (1976), 386–394.
[35] KRUEGEL, C., KIRDA, E., MUTZ, D., ROBERT-
SON, W., AND VIGNA, G. Automating mimicry
attacks using static binary analysis. In Proceedings
of the 14th USENIX Security Symposium (2005).
[36] LEINO, K. R. M. Efﬁcient weakest preconditions.
Information Processing Letters 93, 6 (2005), 281–
288.
[44] PAXSON, V. Automated packet trace analysis of
In ACM SIGCOMM 1997
TCP implementations.
(1997).
[45] UDREA, O., LUMEZANU, C., AND FOSTER, J. S.
Rule-based static analysis of network protocol im-
plementations. In Proceedings of the 15th USENIX
Security Symposium (2006).
[46] XIAO, S., DENG, L., LI, S., AND WANG, X. In-
tegrated tcp/ip protocol software testing for vul-
nerability detection.
In Proceedgins of Interna-
tional Conference on Computer Networks and Mo-
bile Computing (2003).
[47] YANG, J., SAR, C., TWOHEY, P., CADAR, C.,
AND ENGLER, D. Automatically generating mali-
cious disks using symbolic execution. In Proceed-
ings of the 2006 IEEE Symposium on Security and
Privacy (2006).
228
16th USENIX Security Symposium
USENIX Association