determine its placement candidates. Then, we select the earliest
available device among these candidates to place x, and we schedule
the op on that device as soon as its dependencies have completed. If
there is a tie at this step, we select the GPU with the smallest index
so that we can minimize the distance between communicating
GPUs.3 Notice that since we place the sub-ops in order of their
dependencies, keeping track of when each op can be scheduled on
each device is straightforward. If the intersection of the feasible
ranges for all parents of the sub-op x is empty, i.e., the maximum
distance between the parents is longer than ∆ − 1, we relocate the
parent nodes into a smaller device range so that the placement
of x becomes feasible. For this purpose, we place x on the GPU
3This property helps enable wavelength reuse in the ring topology (§A.1).
661
A0B0C0C1C2C3D0D1ABCDC2D1C1D0A1C0C3A0B0C2D1C1D0A1C0C3A0B0C2D1C1D0A1C0C3A0B0GPU3GPU2GPU1GPU0GPU3GPU2GPU1GPU0GPU7GPU6GPU5GPU4TimeTime(a) Compute graph(b) Balanced compute graph(c) MP placement(d) Final DP-MP placementA1SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
M. Khani et al.
central controller. Using NVIDIA’s nvml API, we poll the NVLink
counters on a Tesla V100 GPU at a 300-microsecond granularity.
However, this API is designed for management purposes and is
not optimized for latency. We believe obtaining the counters at a
sub-100-microsecond scale should be feasible with further engineer-
ing. Our experiments confirm that the observed traffic matrix over
the past 100µs is a good estimate of the communication demands
over the next 100 µs. Using the traffic matrix, we can solve an ILP
(see §A.1) for optimal wavelength scheduling on the ring topol-
ogy. However, solving an ILP is too slow for short-timescale circuit
scheduling. Therefore, we propose a fast, approximate wavelength
scheduling algorithm that solves a minimum-cost flow routing
problem to schedule wavelengths. Appendix A.1 describes this al-
gorithm in detail. Note that while we currently propose to measure
the traffic matrix for dynamic circuit establishment, exploiting the
predictability of training workloads is a natural step which we leave
for future work.
Supporting Multiple Jobs. We anticipate a SiP-ML cluster will
typically be used to run multiple jobs at the same time. Each job will
run on a subset of GPUs, dedicated to that job. Supporting multiple
jobs with SiP-OCS requires no changes to our design, except that
we allocate a subset of available GPUs when a job arrives and
correspondingly set the total number of GPUs in our placement
algorithm. When a job completes, we release its GPUs and optical
circuits. SiP-Ring follows a similar logic, but we ideally prefer to
allocate each job to a contiguous block of neighboring GPUs on
the ring. Fragmentation of the ring space, as jobs arrive and depart,
could make this difficult to achieve at all times. One solution is to
use a standard OCS to assign GPU interfaces to arbitrary locations
on the ring.
Scalability Considerations. While our current version of SiP-
OCS assumes each OCS has enough ports to connect to every GPU
in a flat topology, a more realistic setting is to use hierarchical
Clos [80] or flat designs such as BCube [81] to scale SiP-OCS. Our
SiP-Ring topology can be scaled using Theia [72] and SlimFly [82]
to build hierarchical rings. Another way to scale SiP-Ring is to
consider 2D rings, where we have K horizontal rings, with N GPUs
on each ring. We then connect every K GPUs from K different
horizontal rings on a single vertical ring. Hence, there will be K + N
rings in total, connecting N K GPUs. Each GPU has direct access
to one vertical and one horizontal ring and must divide its SiP
interfaces between the two. Depending on the vertical bandwidth
requirement of the interconnect, this ratio can be adjusted.
4 EVALUATION
In this section, we quantify the performance of SiP-ML by compar-
ing it to other network interconnects. Our results show:
(i) For three representative DNN models (Transformer, ResNet,
and Megatron), SiP-ML speeds up training time by a factor of 1.3–
9.1× compared to hierarchical electrical network fabrics represen-
tative of today’s ML clusters. This is because SiP-ML eliminates
bandwidth bottlenecks and enables hybrid DP/MP parallelization
strategies that cannot be supported efficiently by today’s fabrics.
(ii) Although SiP-Ring’s switchless design constrains connectiv-
ity, it performs similarly to SiP-OCS. SiP-Ring’s limited connectivity
is compensated by its ability to rapidly reschedule wavelengths
using MRRs and our parallelization algorithm’s ability to adapt its
strategy to the topology (e.g., ensuring most communication occurs
between nearby nodes on the ring).
(iii) A SiP-ML interconnect with per-GPU bandwidth B performs
as well as or better than an ideal, full-bisection electrical switch
with per-GPU bandwidth B/2. For instance, given 1024 GPUs and
B = 8 Tbps, SiP-ML’s dynamic topology provides at least 4 Tbps
of bandwidth, on average, between each pair of GPUs that need to
communicate.
(iv) When per-GPU bandwidth is high (e.g., order of terabits-per-
second), hybrid parallelism strategies outperform data parallelism
by up to 2× in terms of time-to-accuracy.
4.1 Methodology & Setup
To evaluate SiP-ML, we implement a detailed simulator, called
Rostam, to model several baseline network architectures connect-
ing up to thousands of GPUs. Our simulator is ≈10K lines of code
in C++ and is available online at https://github.com/MLNetwork/
rostam.git. We discuss the details of our simulator in §4.2. In our
evaluations, we set the quantum of computation for balancing the
computation graphs, τ, to 10 µs (§3.2).
Comparisons. We consider the following network architectures:
• Elect-Flat: an ideal electrical switch that scales to any number
of GPUs, N , for any per-GPU bandwidth of B; i.e., each GPU can
simultaneously communicate with N − 1 other GPUs with a total
bandwidth of B in both send and receive directions. This baseline
has zero reconfiguration delay. For any pair of (B, N ), no network
can communicate faster than this baseline. In practice, it can be
approximated with full-bisection bandwidth topologies such as fat-
tree for relatively small values of B (e.g., 100–400 Gbps), or with a
small N (e.g., tens of nodes) with large B. Note that no electrical
network would be able to perform better than this flat electrical
baseline, as it provides full-bisection bandwidth.
• Elect-Cluster: a hierarchical electric network fabric represen-
tative of today’s ML clusters interconnecting GPUs. Each server
hosts eight GPUs, connected with an internal high-speed electrical
switch providing per-GPU bandwidth of B, typically in the order
of terabits-per-second. The servers are connected with a slower
electrical fabric providing 400 Gbps bandwidth per server (unless
otherwise stated). In practice, servers can be thought of as DGX [5]
boxes with an internal NVSwitch [83] interconnect, communicating
over a standard datacenter network fabric (e.g., fat-tree).
• SiP-Ring: a ring-based interconnect for SiP-ML, as described in
§3.1. Each GPU has W distinct wavelengths that it can dynamically
allocate to communicate with its 16 closest neighbors on the ring
(in both directions). We assume each wavelength carries 25 Gbps of
bandwidth, providing a maximum bandwidth of B = W ×25Gbps for
each GPU. Unlike SiP-OCS, this topology is rapidly reconfigurable,
with a reconfiguration latency of 25 µs (§4.4). We estimate the traffic
every 100 µs as described in §3.3 unless stated otherwise.
• SiP-OCS: an optical circuit switch interconnect for SiP-ML, as
described in §3.1 with Q OCS switches, each with N ports (the
same as the number of the GPUs). Each GPU has Q optical links
(each with a bandwidth of B/Q), one to each OCS. Each GPU can
communicate with, at most, D=Q other GPUs at the same time. To
study the impact of D, we vary the number of OCS switches in the
662
SiP-ML: Optical Network Interconnects for Machine Learning
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
Elect-Cluster 200 Gbps
Elect-Cluster 400 Gbps
Elect-Flat (DP)
Elect-Flat
SiP-OCS
SiP-Ring
10
9
8
7
6
5
4
)
s
n
i
m
(
.
c
c
A
-
o
t
-
e
m
T
i
101
100
27
28
29
210
211
212
213
27
28
29
210
211
212
213
BW per GPU (Gbps)
(a) ResNet50
BW per GPU (Gbps)
(b) Transformer
104
103
27
28
29
210
211
212
213
BW per GPU (Gbps)
(c) Megatron
Figure 5: Impact of bandwidth B on the total training time (Time-to-Accuracy) for N=1024 GPUs. DP is not feasible for Megatron
because of its huge memory footprint.
interconnect, using a default value of 16. Since OCS reconfiguration
delay is too long compared to the typical training iteration time of
our DNN models (< 20ms), we compute the best one-shot circuit
schedule for each workload, as described in §3.3. To evaluate the
potential benefits of optical switches with fast reconfiguration [55,
71], we also evaluate the impact of lowering the reconfiguration
latency and allowing multiple reconfigurations within each training
iteration.4
Training workloads. We consider ResNet, Transformer, and Mega-
tron, three representative DNN models widely used in computer
vision and natural language processing applications. ResNet [84] is
an image classification model with 25 million parameters. Trans-
former refers to a Universal Transformer with 350 million parame-
ters. Megatron[52] is a variant of the GPT model [85] with 18 billion
parameters.
Profiling. We first need to profile the average GPU and CPU com-
pute time, peak memory size, and input/output data sizes of each
operation in the model in addition to its data dependencies. Each
compute operation typically has one or more input/output arrays
of data, “tensors”. Profiling the operations over different input/out-
put tensor shapes helps predict the speed ups of partitioning each
operation in different input/output tensor dimensions. We start
profiling over a fair range of batch sizes, typically starting with 1
sample/iteration and continuing until we run out of GPU memory.
The profiling step is independent from the simulator and can use
any convenient profiling tool. Moreover, profiling along other than
the samples dimension (e.g., height and width in the 2D convolu-
tion) helps improve the simulation’s accuracy. In absence of the
profiling data in any dimension, we assume a linear dependency
between the total number of splits and each split’s compute time in
that dimension. Depending on the dimension of the split, Rostam
adds the required new data dependencies in the placement stage.
In addition to the operations profile, we need to know the required
number of iterations to achieve a certain level of model accuracy
as a function of the global batch. This profile depends on the DNN
model and the training dataset [46]. Rostam can combine the latter
two profiles in the placement stage to come up with the best hybrid
parallelization strategy. In this paper, we profile all models on an
NVIDIA Tesla V100 GPU with 32 GB of memory.
Placement. Our approach to explore the space of hybrid paral-
lelism techniques takes as input: (1) the number of GPUs, (2) the
bandwidth available per GPU, (3) the graph profile for the DNN
model as described above, and (4) the curve providing the required
number of training iterations as a function of the (global) batch
size. We search through all possible hybrid parallelizations over a
range of global batch size configurations and use the placement
algorithm (e.g., Algorithm 1 (§3)) for device placement. We then
estimate each configuration’s run-time based on the graph profile
and the bottleneck bandwidth. To estimate the effect of the network,
we also compute the latency for each data transfer (edge) in the
graph profile according to the bottleneck bandwidth. We finally
select the fastest of all these parallelization strategies.
Two points are worth noting about this procedure. First, one of
the strategies that our task parallalization considers is the conven-
tional DP. However, as our results show (see §4.3), in many cases,
DP is not the best strategy for large-scale training. Second, the time
We focus on time-to-accuracy as our primary metric. We de-
termine the time-to-accuracy by multiplying the time for a single
training iteration (obtained via our simulator) by the number of
training iterations required to reach the target accuracy. We use
numbers reported in prior work for the required training iterations
for these models at a given batch size. For ResNet and Transformer,
Shallue et al. [86] report the number of training iterations across a
range of batch sizes. Hence for these models, we optimize over batch
size to find the lowest possible time-to-accuracy in each network
configuration. For Megatron, we use batch size 1024 and 240,000
training iterations, following [50, 87]. Note that we report the total
pre-training time for Megatron, which requires significantly more
training iterations than a typical fine-tuning task. But the relative
improvements we report would hold for fine-tuning the model since
we are directly decreasing the iteration time.
ResNet and Transformer fit in a typical GPU’s memory. Hence
the main reason to parallelize them is to speed up training. Mega-
tron, cannot fit on one GPU and therefore cannot be trained with
only DP; MP is required to split it across multiple GPU memories.
4.2 Simulator
The overall flow of an end-to-end simulation in Rostam is as fol-
lows.
4In the extreme, eliminating reconfiguration latency entirely would make SiP-OCS
equivalent to the ideal Elect-Flat architecture.
663
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
M. Khani et al.
Analysis. We first consider the Elect-Flat architecture. Recall that
Elect-Flat has ideal performance. At every value of B, it provides
each GPU with its full interface bandwidth regardless of the commu-
nication pattern. Thus Elect-Flat’s training time serves as a lower
bound for any other network. Fig. 5 shows that increasing B on
Elect-Flat improves training time for all models, but the improve-