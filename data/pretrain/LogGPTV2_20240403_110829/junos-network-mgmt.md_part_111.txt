detail.
View the TCAM resource used by an application at a show pfe tcam usage tcam-stage (ingress | egress |
specified stage.
pre-egress) app 
Know the number of TCAM resource consumed by a show pfe tcam usage app 
tcam-app
View the TCAM resource usage errors for all stages. show pfe tcam errors all-tcam-stages detail
View the TCAM resource usage errors for a stage show pfe tcam errors tcam-stage (ingress | egress |
pre-egress)
View the TCAM resource usage errors for an show pfe tcam errors app 
application.
View the TCAM resource usage errors for an show pfe tcam errors app  shared-
application along with its other shared application.
usage
Clear the TCAM resource usage error statistics for all clear pfe tcam-errors all-tcam-stages
stages.
1397
Table 154: Commands to Monitor and Troubleshoot TCAM Resource in ACX Series (Continued)
How to Command
Clear the TCAM resource usage error statistics for a clear pfe tcam-errors tcam-stage (ingress | egress |
specified stage
pre-egress)
Clear the TCAM resource usage error statistics for an clear pfe tcam-errors app 
application.
To know more about dynamic TCAM in ACX Series, see "Dynamic Ternary Content Addressable Memory
Overview" on page 1382.
Service Scaling on ACX5048 and ACX5096 Routers
On ACX5048 and ACX5096 routers, a typical service (such as ELINE, ELAN and IP VPN) that is
deployed might require applications (such as policers, firewall filters, connectivity fault management
IEEE 802.1ag, RFC2544) that uses the dynamic TCAM infrastructure.
NOTE: Service applications that uses TCAM resources is limited by the TCAM resource
availability. Therefore, the scale of the service depends upon the consumption of the TCAM
resource by such applications.
A sample use case for monitoring and troubleshooting service scale in ACX5048 and ACX5096 routers
can be found at the "Dynamic Ternary Content Addressable Memory Overview" on page 1382 section.
Troubleshooting DNS Name Resolution in Logical System Security
Policies (Primary Administrators Only)
IN THIS SECTION
Problem | 1398
Cause | 1398
Solution | 1398
1398
Problem
Description
The address of a hostname in an address book entry that is used in a security policy might fail to resolve
correctly.
Cause
Normally, address book entries that contain dynamic hostnames refresh automatically for SRX Series
Firewalls. The TTL field associated with a DNS entry indicates the time after which the entry should be
refreshed in the policy cache. Once the TTL value expires, the SRX Series Firewall automatically
refreshes the DNS entry for an address book entry.
However, if the SRX Series Firewall is unable to obtain a response from the DNS server (for example, the
DNS request or response packet is lost in the network or the DNS server cannot send a response), the
address of a hostname in an address book entry might fail to resolve correctly. This can cause traffic to
drop as no security policy or session match is found.
Solution
The primary administrator can use the show security dns-cache command to display DNS cache information
on the SRX Series Firewall. If the DNS cache information needs to be refreshed, the primary
administrator can use the clear security dns-cache command.
NOTE: These commands are only available to the primary administrator on devices that are
configured for logical systems. This command is not available in user logical systems or on
devices that are not configured for logical systems.
SEE ALSO
Understanding Logical Systems Security Policies
1399
Troubleshooting the Link Services Interface
IN THIS SECTION
Determine Which CoS Components Are Applied to the Constituent Links | 1399
Determine What Causes Jitter and Latency on the Multilink Bundle | 1402
Determine If LFI and Load Balancing Are Working Correctly | 1403
Determine Why Packets Are Dropped on a PVC Between a Juniper Networks Device and a Third-Party
Device | 1412
To solve configuration problems on a link services interface:
Determine Which CoS Components Are Applied to the Constituent Links
IN THIS SECTION
Problem | 1399
Solution | 1399
Problem
Description
You are configuring a multilink bundle, but you also have traffic without MLPPP encapsulation passing
through constituent links of the multilink bundle. Do you apply all CoS components to the constituent
links, or is applying them to the multilink bundle enough?
Solution
You can apply a scheduler map to the multilink bundle and its constituent links. Although you can apply
several CoS components with the scheduler map, configure only the ones that are required. We
recommend that you keep the configuration on the constituent links simple to avoid unnecessary delay
in transmission.
Table 5 shows the CoS components to be applied on a multilink bundle and its constituent links.
1400
Table 155: CoS Components Applied on Multilink Bundles and Constituent Links
Cos Component Multilink Constituent Explanation
Bundle Links
Classifier Yes No CoS classification takes place on the incoming
side of the interface, not on the transmitting side,
so no classifiers are needed on constituent links.
Forwarding class Yes No Forwarding class is associated with a queue, and
the queue is applied to the interface by a
scheduler map. The queue assignment is
predetermined on the constituent links. All
packets from Q2 of the multilink bundle are
assigned to Q2 of the constituent link, and
packets from all the other queues are queued to
Q0 of the constituent link.
1401
Table 155: CoS Components Applied on Multilink Bundles and Constituent Links (Continued)
Cos Component Multilink Constituent Explanation
Bundle Links
Scheduler map Yes Yes Apply scheduler maps on the multilink bundle and
the constituent link as follows:
• Transmit rate—Make sure that the relative
order of the transmit rate configured on Q0
and Q2 is the same on the constituent links as
on the multilink bundle.
• Scheduler priority—Make sure that the relative
order of the scheduler priority configured on
Q0 and Q2 is the same on the constituent
links as on the multilink bundle.
• Buffer size—Because all non-LFI packets from
the multilink bundle transit on Q0 of the
constituent links, make sure that the buffer
size on Q0 of the constituent links is large
enough.
• RED drop profile—Configure a RED drop
profile on the multilink bundle only.
Configuring the RED drop profile on the
constituent links applies a back pressure
mechanism that changes the buffer size and
introduces variation. Because this behavior
might cause fragment drops on the
constituent links, make sure to leave the RED
drop profile at the default settings on the
constituent links.
Shaping rate for a per-unit No Yes Because per-unit scheduling is applied only at the
scheduler or an interface- end point, apply this shaping rate to the
level scheduler constituent links only. Any configuration applied
earlier is overwritten by the constituent link
configuration.
1402
Table 155: CoS Components Applied on Multilink Bundles and Constituent Links (Continued)
Cos Component Multilink Constituent Explanation
Bundle Links
Transmit-rate exact or Yes No The interface-level shaping applied on the
queue-level shaping constituent links overrides any shaping on the
queue. Thus apply transmit-rate exact shaping on
the multilink bundle only.
Rewrite rules Yes No Rewrite bits are copied from the packet into the
fragments automatically during fragmentation.
Thus what you configure on the multilink bundle
is carried on the fragments to the constituent
links.
Virtual channel group Yes No Virtual channel groups are identified through
firewall filter rules that are applied on packets
only before the multilink bundle. Thus you do not
need to apply the virtual channel group
configuration to the constituent links.
SEE ALSO
Class of Service User Guide (Security Devices)
Determine What Causes Jitter and Latency on the Multilink Bundle
IN THIS SECTION
Problem | 1403
Solution | 1403
1403
Problem
Description
To test jitter and latency, you send three streams of IP packets. All packets have the same IP precedence
settings. After configuring LFI and CRTP, the latency increased even over a noncongested link. How can
you reduce jitter and latency?
Solution
To reduce jitter and latency, do the following:
1. Make sure that you have configured a shaping rate on each constituent link.
2. Make sure that you have not configured a shaping rate on the link services interface.
3. Make sure that the configured shaping rate value is equal to the physical interface bandwidth.
4. If shaping rates are configured correctly, and jitter still persists, contact the Juniper Networks
Technical Assistance Center (JTAC).
Determine If LFI and Load Balancing Are Working Correctly
IN THIS SECTION
Problem | 1403
Solution | 1404
Problem
Description
In this case, you have a single network that supports multiple services. The network transmits data and
delay-sensitive voice traffic. After configuring MLPPP and LFI, make sure that voice packets are
transmitted across the network with very little delay and jitter. How can you find out if voice packets are
being treated as LFI packets and load balancing is performed correctly?
1404
Solution
When LFI is enabled, data (non-LFI) packets are encapsulated with an MLPPP header and fragmented to
packets of a specified size. The delay-sensitive, voice (LFI) packets are PPP-encapsulated and interleaved
between data packet fragments. Queuing and load balancing are performed differently for LFI and non-
LFI packets.
To verify that LFI is performed correctly, determine that packets are fragmented and encapsulated as
configured. After you know whether a packet is treated as an LFI packet or a non-LFI packet, you can
confirm whether the load balancing is performed correctly.
Solution Scenario—Suppose two Juniper Networks devices, R0 and R1, are connected by a multilink
bundle lsq-0/0/0.0 that aggregates two serial links, se-1/0/0 and se-1/0/1. On R0 and R1, MLPPP and LFI
are enabled on the link services interface and the fragmentation threshold is set to 128 bytes.
In this example, we used a packet generator to generate voice and data streams. You can use the packet
capture feature to capture and analyze the packets on the incoming interface.
The following two data streams were sent on the multilink bundle:
• 100 data packets of 200 bytes (larger than the fragmentation threshold)
• 500 data packets of 60 bytes (smaller than the fragmentation threshold)
The following two voice streams were sent on the multilink bundle:
• 100 voice packets of 200 bytes from source port 100
• 300 voice packets of 200 bytes from source port 200
To confirm that LFI and load balancing are performed correctly:
NOTE: Only the significant portions of command output are displayed and described in this
example.
1. Verify packet fragmentation. From operational mode, enter the show interfaces lsq-0/0/0 command to
check that large packets are fragmented correctly.
user@R0#> show interfaces lsq-0/0/0
Physical interface: lsq-0/0/0, Enabled, Physical link is Up
Interface index: 136, SNMP ifIndex: 29
Link-level type: LinkService, MTU: 1504
Device flags : Present Running
Interface flags: Point-To-Point SNMP-Traps
1405
Last flapped : 2006-08-01 10:45:13 PDT (2w0d 06:06 ago)
Input rate : 0 bps (0 pps)
Output rate : 0 bps (0 pps)
Logical interface lsq-0/0/0.0 (Index 69) (SNMP ifIndex 42)
Flags: Point-To-Point SNMP-Traps 0x4000 Encapsulation: Multilink-PPP
Bandwidth: 16mbps
Statistics Frames fps Bytes bps
Bundle:
Fragments:
Input : 0 0 0 0
Output: 1100 0 118800 0
Packets:
Input : 0 0 0 0
Output: 1000 0 112000 0
...
Protocol inet, MTU: 1500
Flags: None
Addresses, Flags: Is-Preferred Is-Primary
Destination: 9.9.9/24, Local: 9.9.9.10
Meaning—The output shows a summary of packets transiting the device on the multilink bundle. Verify
the following information on the multilink bundle:
• The total number of transiting packets = 1000
• The total number of transiting fragments=1100
• The number of data packets that were fragmented =100
The total number of packets sent (600 + 400) on the multilink bundle match the number of transiting
packets (1000), indicating that no packets were dropped.
The number of transiting fragments exceeds the number of transiting packets by 100, indicating that
100 large data packets were correctly fragmented.
Corrective Action—If the packets are not fragmented correctly, check your fragmentation threshold
configuration. Packets smaller than the specified fragmentation threshold are not fragmented.
2. Verify packet encapsulation. To find out whether a packet is treated as an LFI or non-LFI packet,
determine its encapsulation type. LFI packets are PPP encapsulated, and non-LFI packets are
encapsulated with both PPP and MLPPP. PPP and MLPPP encapsulations have different overheads
resulting in different-sized packets. You can compare packet sizes to determine the encapsulation
type.
1406
A small unfragmented data packet contains a PPP header and a single MLPPP header. In a large
fragmented data packet, the first fragment contains a PPP header and an MLPPP header, but the
consecutive fragments contain only an MLPPP header.
PPP and MLPPP encapsulations add the following number of bytes to a packet:
• PPP encapsulation adds 7 bytes:
4 bytes of header+2 bytes of frame check sequence (FCS)+1 byte that is idle or contains a flag
• MLPPP encapsulation adds between 6 and 8 bytes:
4 bytes of PPP header+2 to 4 bytes of multilink header
Figure 1 shows the overhead added to PPP and MLPPP headers.
Figure 47: PPP and MLPPP Headers
For CRTP packets, the encapsulation overhead and packet size are even smaller than for an LFI
packet. For more information, see Example: Configuring the Compressed Real-Time Transport
Protocol.
Table 6 shows the encapsulation overhead for a data packet and a voice packet of 70 bytes each.
After encapsulation, the size of the data packet is larger than the size of the voice packet.
Table 156: PPP and MLPPP Encapsulation Overhead
Packet Type Encapsulation Initial Encapsulation Overhead Packet Size
Packet Size after
Encapsulatio
n
Voice packet (LFI) PPP 70 bytes 4 + 2 + 1 = 7 bytes 77 bytes
1407
Table 156: PPP and MLPPP Encapsulation Overhead (Continued)
Packet Type Encapsulation Initial Encapsulation Overhead Packet Size
Packet Size after
Encapsulatio
n
Data fragment (non- MLPPP 70 bytes 4 + 2 + 1 + 4 + 2 = 13 bytes 83 bytes
LFI) with short
sequence
Data fragment (non- MLPPP 70 bytes 4 + 2 + 1 + 4 + 4 = 15 bytes 85 bytes
LFI) with long
sequence
From operational mode, enter the show interfaces queue command to display the size of transmitted
packet on each queue. Divide the number of bytes transmitted by the number of packets to obtain
the size of the packets and determine the encapsulation type.
3. Verify load balancing. From operational mode, enter the show interfaces queue command on the
multilink bundle and its constituent links to confirm whether load balancing is performed accordingly
on the packets.
user@R0> show interfaces queue lsq-0/0/0
Physical interface: lsq-0/0/0, Enabled, Physical link is Up
Interface index: 136, SNMP ifIndex: 29
Forwarding classes: 8 supported, 8 in use
Egress queues: 8 supported, 8 in use
Queue: 0, Forwarding classes: DATA
Queued:
Packets : 600 0 pps
Bytes : 44800 0 bps
Transmitted:
Packets : 600 0 pps
Bytes : 44800 0 bps
Tail-dropped packets : 0 0 pps
RED-dropped packets : 0 0 pps
…
Queue: 1, Forwarding classes: expedited-forwarding
Queued:
Packets : 0 0 pps
Bytes : 0 0 bps
1408
…
Queue: 2, Forwarding classes: VOICE
Queued:
Packets : 400 0 pps
Bytes : 61344 0 bps
Transmitted:
Packets : 400 0 pps
Bytes : 61344 0 bps
…
Queue: 3, Forwarding classes: NC
Queued:
Packets : 0 0 pps
Bytes : 0 0 bps
…
user@R0> show interfaces queue se-1/0/0
Physical interface: se-1/0/0, Enabled, Physical link is Up
Interface index: 141, SNMP ifIndex: 35
Forwarding classes: 8 supported, 8 in use
Egress queues: 8 supported, 8 in use
Queue: 0, Forwarding classes: DATA
Queued:
Packets : 350 0 pps
Bytes : 24350 0 bps
Transmitted:
Packets : 350 0 pps
Bytes : 24350 0 bps
...
Queue: 1, Forwarding classes: expedited-forwarding
Queued:
Packets : 0 0 pps
Bytes : 0 0 bps
…
Queue: 2, Forwarding classes: VOICE
Queued:
Packets : 100 0 pps
Bytes : 15272 0 bps
Transmitted:
Packets : 100 0 pps
Bytes : 15272 0 bps
…
1409
Queue: 3, Forwarding classes: NC
Queued:
Packets : 19 0 pps
Bytes : 247 0 bps
Transmitted:
Packets : 19 0 pps
Bytes : 247 0 bps
…
user@R0> show interfaces queue se-1/0/1
Physical interface: se-1/0/1, Enabled, Physical link is Up
Interface index: 142, SNMP ifIndex: 38
Forwarding classes: 8 supported, 8 in use
Egress queues: 8 supported, 8 in use
Queue: 0, Forwarding classes: DATA
Queued:
Packets : 350 0 pps