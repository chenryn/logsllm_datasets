largest group sizes, the leader only needs to receive a few
kilobytes of data, aggregated over all of the restarting nodes.
In our next series of experiments, we evaluated the costs of
restarting a system with one or more signiﬁcantly out-of-date
replicas (i.e. nodes whose logs are missing many committed
updates). To do this, we created a Derecho service organized
into shards of 3 nodes each, and allowed two out of three
replicas in each shard to continue committing updates for some
time after one replica had crashed. We then crashed the rest
of the replicas, and restarted all of the nodes at once. Each
update in this service contained 1KB of data.
Figure 4 shows a detailed breakdown of the amount of time
spent in the four major phases of the restart algorithm in this
situation: (1) awaiting quorum, (2) truncating logs to complete
epoch termination, (3) transferring state to out-of-date nodes,
and (4) waiting for the leader to commit a restart view. It
also shows a ﬁfth phase, which is the time spent in the setup
process of the Derecho library before the ﬁrst update can be
sent; this includes operations such as pre-allocating buffers
for RDMA multicasts. For comparison, we also measured the
breakdown of time spent in a fresh start of the same service,
which has only two phases: Awaiting quorum (i.e. waiting for
all the processes to launch) and setting up the Derecho library.
This experiment shows even more clearly that our restart
process is quite efﬁcient compared to the normal costs of
starting a distributed service. Even when one replica in each
shard is missing 10000 committed updates, state transfer
accounts for at most 120 ms, a small fraction of the overall
time. It also shows the beneﬁts of allowing each shard to
complete state transfer in parallel: The 3-shard service spent
no more time on state transfer than the 2-shard service, even
though there were an additional 1000 or 10000 updates to send
to an out-of-date node.
We also measured the number of bytes of data received by
each out-of-date replica during the state-transfer process, and
varied the amount of data contained in each update as well
as the number of missing updates. The results are shown in
Figure 5, and are fairly straightforward: the amount of data
transferred to each out-of-date replica increases linearly with
the size of an update, and with the number of updates that
the out-of-date replica has missing from its log. Moreover,
it is almost exactly equal to the number of missing updates
multiplied by the size of each update, because the node did not
need to download and merge logs from multiple other replicas.
It is also important to note that this data is sent in parallel for
each shard, so unlike the metadata in Figure 3, there is no
difference in how much data any one node must send as the
number of shards increases.
Finally, we measured the amount of time required to restart
a service with out-of-date replicas as the size of each update
scales up, shown in Figure 6. We found that for updates of
sizes below 1 MB, neither the size of the update nor the
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:35:43 UTC from IEEE Xplore.  Restrictions apply. 
181
1011
109
107
105
)
s
e
t
y
B
n
i
(
e
z
i
S
103
101
5000 updates behind
1000 updates behind
16 updates behind
5000 updates behind
1000 updates behind
16 updates behind
)
s
d
n
o
c
e
s
i
l
l
i
m
n
i
(
e
m
T
i
104
103
102
103
104
105
106
107
101
102
103
104
105
106
Update size (in Bytes)
Update size (in Bytes)
Fig. 5: Data downloaded by each out-of-date node, in a system
with 3 shards of 3 members each.
Fig. 6: Time to restart a service with 3 shards of 3 members
each, with 1 out-of-date node per shard. Error bars represent
1 standard deviation.
number of missing updates on the out-of-date replicas had
much of an effect on the restart time. For update sizes of
1MB and larger, the increasing amount of data that needed
to be transferred to the out-of-date replicas had the expected
effect of slowing down the restart process.
VI. RELATED WORK
The algorithms implemented by Derecho combine ideas
ﬁrst explored in the Isis Toolkit [13, 6] with the Vertical
Paxos model [14]. Other modern Paxos protocols include
NOPaxos [4] and APUS [5]. Recent systems that offer a
more durable form of Paxos, such as Spinnaker [15] and
Gaios [16], include mechanisms for restarting failed nodes
using their persistent logs. However, these papers generally
do not consider the case in which every replica must be
restarted at once. “Paxos Made Live” [17] explores a number
of practical challenges (including durability) seen in larger
SMR systems, a motivation shared by our work.
Bessani et al. looked at the efﬁciency of adding durability
to SMR in [18], including the problem of minimizing state
transfer during replica recovery. They provided a solution for
recovering a non-sharded service in a Byzantine setting, and
also showed how to lower the runtime overhead of logging
and checkpointing. Their work did not look at services with
complex substructure, which was a primary consideration here.
Corfu [9] is a recent implementation of SMR that uses a
different approach from classic Paxos, distributing the com-
mand log across shards of storage-only nodes. Clients use
Paxos to reserve a slot, then replicate data using a form of
chain replication [19]. vCorfu [8] extends this by offering
virtual sublogs on a per-application basis. However, if multiple
subsystems use Corfu separately, recovery of the Corfu log
might not recover the application as a whole into a consistent
state. As we mentioned in sections II-A and IV-D, our protocol
could be adapted to vCorfu to ensure that a quorum of replicas
from each sublog of the last known layout is contacted before
the system is restarted. Other replicated cloud services, such
as Hadoop [20], Zookeeper [21], and Spark [22], employ an
alternative approach to durability by ensuring that any state
lost due to an unexpected failure can always be recomputed
from its last checkpoint, but this is not an option in our setting.
Our work is inspired by a long history of distributed check-
pointing and rollback-recovery protocols, many of which are
summarized in [23], but updates these principles to the modern
setting of replicated services and SMR. Rather than rely on an
explicitly coordinated global checkpoint, as in [24] and [25],
or attempt to record a dependency graph between locally-
recorded checkpoints, as in [26], our system incorporates the
dependency information already recorded in SMR updates to
derive a globally consistent system snapshot from local logs.
Recovery of the ﬁnal state of a single process group was
ﬁrst treated in Skeen’s article “Determining the Last Process
to Fail” [27]. Our scenario, with potentially overlapping sub-
groups, is more complex and introduces an issue of joint
consistency they did not explore.
VII. CONCLUSION
Modern datacenter services are frequently complex, and
may employ SMR mechanisms for self-managed conﬁgura-
tion, membership management, and sharded data replication.
In these services, application data will be spread over large
numbers of logs, and recovery requires reconstruction of a
valid and consistent state that preserves all committed updates.
We showed how this problem can be solved even if fur-
ther crashes occur during recovery, implemented our solution
within Derecho, and evaluated the mechanism to show that it
is highly efﬁcient.
ACKNOWLEDGMENTS
This work was supported, in part, by a grant from AFRL
Wright-Patterson.
REFERENCES
[1] J. Gray, P. Helland, P. O’Neil, and D. Shasha, “The
dangers of replication and a solution,” in Proc. 1996 ACM
SIGMOD Int. Conf. Management of Data. Montreal,
Quebec, Canada: ACM, 1996, pp. 173–182.
[2] K. Birman, B. Hariharan, and C. De Sa, “Cloud-hosted
intelligence for real-time IoT applications,” SIGOPS
Oper. Syst. Rev., vol. 53, no. 1, pp. 7–13, Jul. 2019.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:35:43 UTC from IEEE Xplore.  Restrictions apply. 
182
the basis of a high-performance data store,” in Proc. 8th
USENIX Conf. Networked Systems Design and Imple-
mentation. Boston, MA, USA: USENIX Association,
2011, pp. 141–154.
[17] T. D. Chandra, R. Griesemer, and J. Redstone, “Paxos
made live: An engineering perspective,” in Proc. 26th
ACM Symp. Principles of Distributed Computing. Port-
land, OR, USA: ACM, 2007, pp. 398–407.
[18] A. Bessani, M. Santos, J. Felix, N. Neves, and M. Cor-
reia, “On the efﬁciency of durable state machine replica-
tion,” in Proc. 2013 USENIX Annual Technical Confer-
ence. San Jose, CA, USA: USENIX Association, Jun.
2013, pp. 169–180.
[19] R. van Renesse and F. B. Schneider, “Chain replication
for supporting high throughput and availability,” in Proc.
6th Symp. Operating Systems Design and Implementa-
tion.
San Francisco, CA, USA: USENIX Association,
Dec. 2004, pp. 91–104.
[20] K. Shvachko, H. Kuang, S. Radia, and R. Chansler,
“The Hadoop distributed ﬁle system,” in Proc. 26th IEEE
Symp. Mass Storage Systems and Technologies.
IEEE
Computer Society, 2010.
[21] P. Hunt, M. Konar, F. P. Junqueira, and B. Reed,
“Zookeeper: Wait-free coordination for Internet-scale
systems,” in Proc. 2010 USENIX Annual Technical Con-
ference. Boston, MA, USA: USENIX Association, 2010.
[22] M. Zaharia, M. Chowdhury, M. J. Franklin, S. Shenker,
and I. Stoica, “Spark: Cluster computing with working
sets,” in Proc. 2nd USENIX Conf. Hot Topics in Cloud
Computing. Boston, MA, USA: USENIX Association,
Jun. 2010.
[23] E. N. Elnozahy, L. Alvisi, Y.-M. Wang, and D. B.
Johnson, “A survey of rollback-recovery protocols in
message-passing systems,” ACM Comput. Surv., vol. 34,
no. 3, pp. 375–408, Sep. 2002.
[24] R. Koo and S. Toueg, “Checkpointing and rollback-
recovery for distributed systems,” IEEE Trans. Softw.
Eng., vol. SE-13, no. 1, pp. 23–31, Jan. 1987.
[25] E. N. Elnozahy and W. Zwaenepoel, “Manetho: Trans-
parent rollback-recovery with low overhead, limited roll-
back, and fast output commit,” IEEE Trans. Comput.,
vol. 41, no. 5, pp. 526–531, May 1992.
[26] B. Bhargava and S.-R. Lian, “Independent checkpoint-
ing and concurrent rollback for recovery in distributed
systems – an optimistic approach,” in Proc. 7th Symp.
Reliable Distributed Systems, Oct. 1988, pp. 3–12.
[27] D. Skeen, “Determining the last process to fail,” ACM
Trans. Comput. Syst., vol. 3, no. 1, pp. 15–30, Feb. 1985.
[3] S. Jha, J. Behrens, T. Gkountouvas, M. Milano, W. Song,
E. Tremel, R. V. Renesse, S. Zink, and K. P. Birman,
“Derecho: Fast state machine replication for cloud ser-
vices,” ACM Trans. Comput. Syst., vol. 36, no. 2, pp.
4:1–4:49, Apr. 2019.
[4] J. Li, E. Michael, N. K. Sharma, A. Szekeres, and
D. R. K. Ports, “Just say NO to Paxos overhead: Re-
placing consensus with network ordering,” in Proc. 12th
USENIX Symp. Operating Systems Design and Imple-
mentation. Savannah, GA, USA: USENIX Association,
2016.
[5] C. Wang, J. Jiang, X. Chen, N. Yi, and H. Cui, “APUS:
Fast and scalable Paxos on RDMA,” in Proc. 8th ACM
Symp. Cloud Computing. Santa Clara, CA, USA: ACM,
Sep. 2017.
[6] K. P. Birman and T. A. Joseph, “Reliable communication
in the presence of failures,” ACM Trans. Comput. Syst.,
vol. 5, no. 1, pp. 47–76, Jan. 1987.
[7] L. Lamport, “Paxos made simple,” ACM Sigact News,
vol. 32, no. 4, pp. 18–25, 2001.
[8] M. Wei, A. Tai, C. J. Rossbach, I. Abraham, M. Mun-
shed, M. Dhawan, J. Stabile, U. Wieder, S. Fritchie,
S. Swanson, M. J. Freedman, and D. Malkhi, “vCorfu: A
cloud-scale object store on a shared log,” in Proc. 14th
USENIX Conf. Networked Systems Design and Imple-
mentation. Boston, MA, USA: USENIX Association,
2017, pp. 35–49.
[9] M. Balakrishnan, D. Malkhi, J. D. Davis, V. Prabhakaran,
M. Wei, and T. Wobber, “CORFU: A distributed shared
log,” ACM Trans. Comput. Syst., vol. 31, no. 4, pp. 10:1–
10:24, Dec. 2013.
[10] S. A. Weil, S. A. Brandt, E. L. Miller, D. D. E. Long,
and C. Maltzahn, “Ceph: A scalable, high-performance
distributed ﬁle system,” in Proc. 7th Symp. Operating
Systems Design and Implementation. Seattle, WA, USA:
USENIX Association, 2006, pp. 307–320.
[11] R. K. Ahuja, T. L. Magnanti, and J. B. Orlin, Network
Flows: Theory, Algorithms, and Applications. Upper
Saddle River, NJ, USA: Prentice-Hall, Inc., 1993.
[12] K. M. Chandy and L. Lamport, “Distributed snapshots:
Determining global states of distributed systems,” ACM
Trans. Comput. Syst., vol. 3, no. 1, pp. 63–75, Feb. 1985.
[13] K. P. Birman, “Replication and fault-tolerance in the ISIS
system,” in Proc. 10th ACM Symp. Operating Systems
Principles. Orcas Island, WA, USA: ACM, 1985, pp.
79–86.
[14] L. Lamport, D. Malkhi, and L. Zhou, “Vertical Paxos and
primary-backup replication,” in Proc. 28th ACM Symp.
Principles of Distributed Computing. Calgary, Canada:
ACM, Aug. 2009, pp. 312–313.
[15] J. Rao, E. J. Shekita, and S. Tata, “Using Paxos to build
a scalable, consistent, and highly available datastore,”
Proc. VLDB Endow., vol. 4, no. 4, pp. 243–254, Jan.
2011.
[16] W. J. Bolosky, D. Bradshaw, R. B. Haagens, N. P.
Kusters, and P. Li, “Paxos replicated state machines as
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:35:43 UTC from IEEE Xplore.  Restrictions apply. 
183