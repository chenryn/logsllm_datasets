blocks that were responsive to ICMP requests in earlier probing
attempts (see [28] for details). Hence, while this dataset covers
only a small portion of the space, it comprehensively probes ev-
ery address within that subset of /24s. We leverage data from four
surveys executed between June and September 2017. In total, this
dataset contains some 52K /24 address blocks, 21K probed over a
two week window, and 31K over a four week window. In a first
step, we remove ISI blocks that never had more than 40 responsive
IP addresses in any hour, reducing our set by some 53% down to
25K blocks. Next, we intersect the 25K blocks with those address
blocks that were in a trackable state in our CDN data (recall § 3.4),
leaving us with 15K address blocks for comparison.
Comparing CDN and ICMP disruptions: Next, we execute our
disruption detection for each combination of α and β values ranging
from 0.1 to 0.9. Whenever our approach detects a disruption, we
compare the time interval of our disruption with ICMP following a
hour [starting 2017−08−30]active IPv4 addresses050100150200250300350050100150ICMP responsiveCDN activeY_LABEL00000000000.60.60.60.60.60.60.50.600.611.51.51.51.51.51.500.611.81.81.81.71.71.800.61.11.82.1222200.61.11.92.13.73.73.53.500.61.11.92.148.58.48.700.61.122.14.210.627.129.800.60.61.51.73.21035.263.40.10.20.30.40.50.60.70.80.90.10.20.30.40.50.60.70.80.9alphabetaalphafraction of /24s●●●●●●●●●0.20.40.60.80.00.10.20.3●disagreement CDN vs. ICMPdisruptionsIMC ’18, October 31-November 2, 2018, Boston, MA, USA
Richter et al.
two-step approach: For those hours that were not affected by the
disruption, we require that ICMP responsiveness never drops below
40 and has a maximum range of ± 30 addresses.2 This ensures that
we only compare address blocks for which we have a steady signal
of ICMP responsiveness in its regular state. If this criterion is satis-
fied, we then classify the disruption into agree or disagree. We say
that a disruption agrees, if the maximum number of ICMP respon-
sive addresses during the disruption is smaller than the minimum
number of ICMP responsive addresses outside the disruption. That
is, at all points in time, we see more ICMP responsive addresses out-
side of the disruption compared to the disrupted hours themselves.
Note that the number of disruptions, and of address blocks, that we
compare varies depending on the individual α and β, but ranges
between 200 and 2000 address blocks. We are aware that this is a
comparably small sample. For this reason we strive for minimal
disagreement and set strong criteria for our cross-evaluation.
3.6 Data-driven Parameter Selection
Figure 3b shows the percentage of disagreement between our CDN
detection and ICMP for different values of α and β. For the percent-
ages in Figure 3b, recall that the number of samples varied, and
thus there is some coarseness when comparing the percentages,
particularly for fractions of a percentage. Nevertheless, Figure 3b
yields some general observations. Very low values of α and β ex-
clusively capture disruptions where the number of active addresses
goes to zero. For these cases, we did not detect a single instance
of disagreement. With higher values, our detection sensitivity in-
creases — up to the extreme case where both α and β are at 0.9,
resulting in more than 60% of cases where ICMP responsiveness
does not drop with CDN activity. To keep the disagreement below
roughly 3%, α and β can not both be greater than 0.5. Also, ignoring
for the moment the impact of the choice of α, a high value of β
enforces a higher recovery of address activity, which leads to a
more conservative, restrictive criterion for determining the termi-
nation of a disruption (i.e., lessens the likelihood that a level-shift
change is falsely detected as a disruption, at the risk of missing
some true disruptions). We chose β to be 0.8. Then, for β = 0.8,
Figure 3c shows how the fraction of disagreement (potential false
positives) as well as the fraction of address blocks in which we
detect a disruption (completeness) changes for different values of
α. While the number of disruptions increases only linearly up to
alpha values of 0.5, the number of disagreements steeply increases
for α values of 0.6 or larger. Based on our observations, we fix α to
0.5 and β to 0.8 for the remainder of this work.
With these parameters, there remain a few cases where ICMP
responsiveness and CDN activity disagree, all of which were partial
disruptions to address activity: not all addresses were affected. We
opt for conservativeness: fewer disruptions but more confidence
that they are really disruptions. While we detect all disruptions that
affect an entire /24 (assuming the /24 was in a trackable state before
the disruption), we will not detect all disruptions that affect parts of
/24s. In the following, we note where we separate disruptions that
affect entire /24s versus disruptions that only affect partial /24s. In
addition to our cross-validation against ICMP responsiveness, in
2We exclude two hours directly before and after the disruption event from this com-
parison to account for our hourly time-binning.
(a) Trinocular-detected disruptions in the CDN logs: For 60% of detected
Trinocular disruptions, address activity as seen from the CDN remains
unchanged. The CDN confirms only 27% of Trinocular disruptions. Filter-
ing out address blocks with frequent Trinocular disruptions reduces the
number of Trinocular disruptions, but increases agreement significantly.
(b) CDN-detected disruptions in Trinocular: Trinocular confirms 94% of
CDN-detected disruptions that affect all addresses within a /24. Filter-
ing Trinocular data by removing frequently disrupted blocks reduces
agreement and thus likely misses true disruptions.
Figure 4: Detected disruptions in the CDN logs and Trinocu-
lar, a state-of-the-art active outage detection system.
Section 5 we leverage an external dataset revealing device activity.
This latter dataset contradicts our detected disruptions in less than
< 0.01% of the cases, making us confident that detected disruptions
indicate loss of connectivity of the concerned address blocks.
3.7 Evaluation against State-of-the-Art
Next, we evaluate our disruption detection approach against a
state-of-the-art system for Internet-wide detection of outages via
active probing: Trinocular [46]. We rely on a three-month dataset
(2017-04-03 to 2017-07-02) made available by ISI [8]. For each /24
address block, we extract all disruptions detected by Trinocular,
i.e., a down event for an address block followed by an up event. We
then compare time periods of Trinocular-disrupted address blocks
with disruptions detected in our CDN logs and vice versa. For both
datasets, we only compare disruptions that affect address blocks
that were in a trackable state in the other dataset at the time of the
disruption (i.e., we saw a baseline greater than 40 in the CDN logs,
and, likewise, a block was in an up state in Trinocular prior to a
disruption). We say that disruptions in the two datasets agree if we
find an, at least partial, overlapping in time of disruptions in the
two datasets. In future work, we plan to conduct a more detailed
analysis of timing aspects. Figure 4 shows our results.
Overall coverage: The Trinocular dataset contains information
for some 3.5M /24 address blocks (after removing blocks that were
in an unmeasurable state during our time window). On the first day
of the comparison period, the CDN recorded activity from some
Trinocular filtered disruptions /24s < 5 disruptions (N=110K)Trinocular full hour disruptions(N=380K)0.00.20.40.60.81.0CDN disruptionreduced CDN activityregular CDN activityCDN disruptions entire /24 (N=132K) vs. filtered TrinocularCDN disruptions entire /24 (N=132K) vs. all Trinocular0.00.20.40.60.81.0Trinocular disruptionno Trinocular disruptionTrinocular disruptionAdvancing the Art of Internet Edge Outage Detection
IMC ’18, October 31-November 2, 2018, Boston, MA, USA
Figure 5: Hourly disrupted /24s detected over the course of our one-year observation period. Stacked bars show disruptions
that affected all addresses within a /24 (red) as well as disruptions that affected only some addresses within a /24 (blue).
5.1M /24 address blocks, 2.3M of them were in a CDN-trackable
state. Some 1.6M /24 address blocks are covered in both datasets.
Trinocular disruptions in CDN logs: For evaluating the visibil-
ity of Trinocular-detected disruptions in the CDN logs, we restrict
the analysis to disruptions in the Trinocular dataset that span at
least one calendar hour, since we can not detect shorter disruptions
in the CDN logs due to binning. Some 29.9% of the disruptions in the
overall Trinocular dataset span at least one calendar hour. We find
that Trinocular detects significantly more disruptions compared
to our CDN-detected disruptions. Figure 4a shows how Trinocular
disruptions are reflected in CDN activity. We classify them into
CDN disruption: The CDN logs show a full or partial disruption
that agrees with Trinocular’s, reduced CDN activity: we see a de-
crease in the baseline in the CDN logs, but not enough to meet our
criterion for a disruption, regular CDN activity: no decrease in the
baseline, and the CDN continues to serve content. Our approach
confirms only some 27% of Trinocular outages. In 60% of the cases,
the baseline did not change at all during the detected disruption by
Trinocular, implying a high percentage of false positive detections.
Filtering Trinocular: We discussed this result with the authors
of Trinocular, who suggested that the cause could be a known
issue with their methodology, whereby Trinocular detects frequent
change of state of some address blocks. We then chose a simple, first-
order filter of the Trinocular dataset and only considered address
blocks with fewer than 5 disruptions over the 3 month time period.
This reduces the number of disruptions for comparison by more
than two thirds, down to 110K, but only reduces the overall number
of Trinocular-trackable blocks by some 3% (from 3.5M /24s down to
3.4M /24s). Comparing this subset against our logs, we now confirm
some 74% of the detected Trinocular disruptions, though for some
26% the CDN was still serving content to at least a portion of the
address block.
CDN disruptions in Trinocular: Comparing in the opposite di-
rection, i.e. when studying the visibility of CDN-detected disrup-
tions in Trinocular, we restrict ourselves to CDN-detected disrup-
tions that affected all addresses in a /24 address block, since Trinoc-
ular’s design focuses on block-level disruptions and outages. Fig-
ure 4b shows that Trinocular indeed detected a disruption in some
94% of all CDN-detected disruptions. Comparing the CDN disrup-
tions against the filtered Trinocular dataset reduces the agreement
down to 74%. Thus, although filtering out Trinocular blocks with
5 or more disruptions had the benefit of significantly increasing
the fraction of Trinocular disruptions that were also seen by the
CDN, it has the disadvantage that the fraction of CDN-detected
disruptions not seen by Trinocular increased from 6% to 26%.
4 A GLOBAL VIEW OF DISRUPTIONS
We next apply our disruption detection mechanism over the entire
period of our dataset and study disruptions on a broad scale. Figure 5
shows the absolute number of disrupted /24 address blocks in each
hour between March 2017 and March 2018. Here, we partition
disruptions in two categories: the red bars show disruptions that
affected the entire /24 (i.e., the number of active addresses during the
disruption went to 0), while the blue bars (stacked) show disruptions
that affected only parts of a /24 (i.e., some addresses remained active
during the disruption). We can make several observations from this
figure: (i) the number of disrupted /24 address blocks ranges at
around 2000, or some 0.2% of tracked address blocks, with only a
few major events deviating from this pattern: In September 2017, we
can see a strong spike in the number of disrupted /24s (Hurricane
Irma), and notice that during this event the majority of affected /24
address blocks only showed partial disruptions in address activity.
Aside from several other spikes indicative of single large-scale
events (§ 4.1), we observe that the number of disrupted /24 blocks
follows a weekly pattern throughout the year, but that this pattern is
mostly absent during the Christmas/New-Year’s period. We further
investigate this phenomenon in § 4.2.
4.1 Disruption Patterns in Space
In this section, we are interested in understanding how often in-
dividual address blocks are affected by disruptions, as well as if
disruptions typically span isolated address blocks or also affect
neighboring prefixes at the same time.
Disruptions per /24: Figure 6a shows the distribution of disrup-
tion events per individual /24 address block. Note that we only
show address blocks that had at least one disruption event during
our observation period. Here, we can see that more than 60% of
/24 prefixes had only a single disruption event during the entire
observation period of one year. Less than 1% of /24 address blocks
had 10 or more disruption events, with only a handful of prefixes
having more than 20, and only 8 prefixes having more than 60
disruptions, and these 8 prefixes contain only about 0.05% of all
disruption events. The important takeaway here is that the periodic
behavior in Figure 5 is not the result of some recurring pattern
affecting the same set of /24 address blocks. Instead, the weekly
pattern affects disparate /24 address blocks.
Disruption prefix size: We next group /24-disruption events to-
gether. In a first step, we put all disruptions into time bins using
two different rules: In the more relaxed case, /24 disruption events
with the same start hour are placed in a bin. In the more strict case,
we group /24 disruptions events together according to their start-
Apr 2017May 2017Jun 2017Jul 2017Aug 2017Sep 2017Oct 2017Nov 2017Dec 2017Jan 2018Feb 201804K(~0.35%)8K(~0.7%)hourly disrupted /24spartial /24 disruptedentire /24 disruptedIMC ’18, October 31-November 2, 2018, Boston, MA, USA
Richter et al.
(a) Disruptions per /24 address block, if ever disrupted.
(a) Start day of disruption events (timezone-normalized).
(b) Grouping detected /24 disruption events together: The majority
of disruption events spans multiple adjacent /24 prefixes. In some in-
stances, every /24 address block within an entire /15 shows a disruption.
(b) Start hour of disruption events (timezone-normalized).
Figure 7: Time patterns of disruption events.
Figure 6: Spatial properties of disruptions.
and end hour (i.e., only disruption with the same duration and start
hour will be in the same bin). Then, for all /24 blocks within each
bin, we group /24’s that are adjacent in address space, and find the
longest prefix that is completely filled by these /24s. For example, if
we have four /24 prefixes that are adjacent in address space, and are
contained in a /22 prefix, and the neighboring /24 prefixes would
not completely fill a /21 prefix, then for these four /24 prefixes the
covering prefix is a /22.
Figure 6b shows the histogram of disruption events partitioned
by the largest prefix that covers individual /24 prefixes. For exam-
ple, 18% of the disruption events with the same start time occur
in /24 prefixes that have a /23 covering prefix, while 39% do not
aggregate into a shorter prefix. We observe that with the restriction
of common of start times and of end times fewer disruptions group
into larger prefixes (see higher green bar at /24), yet still a majority
of /24 disruption events do: 52% of events with the same start and
end time aggregate into shorter prefixes (61% of events only with
the same start time). Note that we find instances in which all /24s
contained in an entire /15 address block show a disruption starting
and ending precisely at the same time. We manually investigated
large /15 events and found two of them to be related to an Iranian
cellular ISP, and one other related to an Egyptian ISP. For both
countries, reports of willful Internet shutdowns exist [37]. We note