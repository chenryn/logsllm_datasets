### Leveraging Samples for Retraining Deep Q-Networks and Policy Networks

To improve the robustness of deep Q-networks or policy networks, we can leverage these samples to retrain them. The work proposed in [6] introduces random noise to the weights of a deep Q-network during the training process. This method demonstrates that the trained network can be robust against adversarial sample attacks, as proposed in [4].

### Detection Efforts

There are two existing works [15, 26] that focus on building independent neural networks to detect adversarial samples targeting the policy network. These approaches have shown significant success in identifying adversarial attacks against reinforcement learning. However, these defense and detection methods are designed to counteract attacks through environment manipulation, making them less applicable or extendable to our specific attack scenario.

As demonstrated in Section 6, the victim agent, when robustified by adversarial training, loses its generalizability. We hypothesize that this is due to trajectory splitting. To verify this hypothesis, we plan to retrain the victim agent using two sets of game episodes: one from the victim agent's interactions with a regular agent, and the other from its interactions with an adversarial agent learned through our proposed approach. We will also vary the percentage of adversarial and regular episodes and observe the changes in the retrained victim agent’s robustness and generalizability.

### Transferability

Recent research has extended the study of adversarial environments in reinforcement learning to explore transferability [18]. Specifically, for the same reinforcement learning task, it has been shown that an adversarial environment crafted for one policy network can be transferred to a different policy network, leading the corresponding agent to behave undesirably. As part of our future work, we plan to investigate the transferability of our adversarial policy. We will examine whether an adversarial policy network trained against one particular opponent agent can also be used to defeat other agents trained differently but serving the same reinforcement learning task.

### Conclusion

In a reinforcement learning problem, an adversary typically has full control over their agent (adversarial agent) and the ability to passively observe the actions/observations of their opponent. However, it is common for the adversary to lack access to the opponent's policy network or the capability to manipulate the input to that network arbitrarily. In such practical scenarios, existing techniques often struggle to train an adversarial agent effectively and efficiently because they either make strong assumptions or fail to exploit the weaknesses of the target agent. In this work, we extend a state-of-the-art reinforcement learning algorithm to guide the training of the adversarial agent in a two-agent competitive game setting. Empirical evidence shows that our approach can train an adversarial agent more effectively and efficiently, demonstrating a stronger capability in exploiting the weaknesses of the opponent agent compared to existing techniques. Our findings and analyses confirm that attacking reinforcement learning can be achieved in a practical and efficient manner.

### Acknowledgments

We would like to thank our shepherd Lujo Bauer and the anonymous reviewers for their valuable feedback. This project was supported in part by NSF grant CNS-1718459, ONR grant N00014-20-1-2008, and the Amazon Research Award.

### References

[1] Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim. Sanity checks for saliency maps. In Proc. of NeurIPS, 2018.
[2] Martin Arjovsky, Soumith Chintala, et al. Wasserstein generative adversarial networks. In Proc. of ICML, 2017.
...
[59] Yiren Zhao, Ilia Shumailov, et al. Blackbox attacks on reinforcement learning agents using approximated temporal information. arXiv preprint arXiv:1909.02918, 2019.

### Appendix

#### Victim Policies
The network architecture of the victim policies in the MuJoCo game and the Roboschool Pong game are:
- MuJoCo: MLP-380-128-128-17 [3]
- Roboschool Pong: MLP-13-64-64-2

#### Hyper-parameters of the Baseline
The baseline has two sets of hyper-parameters: the adversarial policy/value network architecture and the PPO algorithm hyper-parameters.
- For the MuJoCo game, we use the default choices from [10].
- For the Roboschool Pong game, we set the adversarial policy network and its value function as MLP-13-64-64-2 and MLP-13-64-64-1, respectively, and use the same PPO hyper-parameters as the MuJoCo game.

#### Hyper-parameters of Our Method
We specify the hyper-parameters that are not varied in the sensitivity test:
- For the MuJoCo game: εs = 1, εa = 0.05, H: MLP-414-40-64-380, F: MLP-380-64-64-17.
- For the Roboschool Pong game: εs = 0.01, εa = 0.05, H: MLP-17-40-16-13, F: MLP-13-64-64-2.

#### Effectiveness of l2 Norm on the Pong Game
In Figure 6d, we show that the solution developed using the l2 norm is worse than those developed using l1, l∞, and our baseline. We argue that this is because the l2 norm is not suitable for high-dimensional inputs. To validate this, we conducted a similar experiment on the Roboschool Pong game, which takes a low-dimensional input (13 features). Figure 9 confirms that, for the Pong game, the solution developed using the l2 norm is as good as our final solution, which uses l1. This supports our argument that the l2 norm is not suitable for high-dimensional inputs, and l1 or l∞ is a better fit.

#### Additional Parameter Sensitivity Test
In our experiments, we set equal weight to the action difference term and the observation difference term in Eqn. (9). We vary the relative weight between the two terms and observe its influence on our attack performance. Specifically, we introduce a weight η to the observation difference term (i.e., −η||ˆo(t+1) - o(t+1)||1) and train the adversarial agent with η = [1, 2, 3, 4]. Figure 10 shows the winning rate of the adversarial agent on two selected games. The results indicate that varying η has only a negligible influence on the performance of the adversarial agents trained by our attack.