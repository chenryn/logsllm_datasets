then leverage these samples to retrain deep Q-networks or the
policy networks for the goal of improving their robustness.
The work proposed in [6] introduces random noise to the
weights of a deep Q-network during the training process. It
demonstrates that the trained network can be robust against
the adversarial sample attack proposed in [4].
Regarding the efforts of the detection, there have been
two existing works [15, 26]. They build independent neural
networks to identify adversarial samples to the policy net-
work, and demonstrate great success in pinpointing adversar-
ial attacks against reinforcement learning. However, existing
defense and detection are designed for the attack through en-
vironment manipulation. Thus, they cannot be easily adopted
or extended to defeat our attack. As we show in Section 6,
the victim agent robustiﬁed by adversarial training loses its
generalizability, and we suspect this is caused by the trajec-
tory split. As a part of future work, we plan to verify this
hypothesis by retraining the victim agent on two sets of game
episodes. One is from the victim agent’s interactions with
the corresponding regular agent. The other is from the victim
agent’s interactions with the adversarial agent learned through
our proposed approach. We will also vary the percentage of
the adversarial/regular episodes and observe the changes in
the retrained victim agent’s robustness and generalizability.
Transferability. Following the efforts of exploiting reinforce-
ment learning through an adversarial environment, recent
research has extended their interest to study the transferability
of adversarial environments (e.g., [18]). More speciﬁcally,
for the same reinforcement learning task, researchers have
shown that the adversarial environment crafted for one par-
ticular policy network can be easily transferred to a different
policy network, misleading the corresponding agent to behave
in an undesired manner. As part of our future work, we plan
to explore the transferability of our adversarial policy. We
will examine whether an adversarial policy network trained
against one particular opponent agent could also be used to
defeat the other agents trained differently but serving for the
same reinforcement learning task.
9 Conclusion
When launching an attack against an opponent agent in a
reinforcement learning problem, an adversary usually has
full control over his agent (adversarial agent) as well as the
freedom to passively observe the action/observation of his op-
ponent. However, it is very common that the adversary has no
access to the policy network of the opponent agent nor has the
capability of manipulating the input to that network arbitrarily
(i.e., observation). In this practical scenario, using existing
techniques, it is usually difﬁcult to train an adversarial agent
effectively and efﬁciently because the algorithms applied to
this problem either make strong assumptions or lack the abil-
ity to exploit the weakness of the target agent. In this work,
we carefully extend a state-of-the-art reinforcement learning
algorithm to guide the training of the adversarial agent in
the two-agent competitive game setting. The empirical evi-
dence demonstrates that an adversarial agent can be trained
effectively and efﬁciently, exhibiting a stronger capability in
exploiting the weakness of the opponent agent than those
trained with existing techniques. With all these discoveries
and analyses, we safely conclude that attacking reinforce-
ment learning could be achieved in a practical scenario and
demonstrated in an effective and efﬁcient fashion.
1898    30th USENIX Security Symposium
USENIX Association
Acknowledgments
We would like to thank our shepherd Lujo Bauer and the
anonymous reviewers for their helpful feedback. This project
was supported in part by NSF grant CNS-1718459, by ONR
grant N00014-20-1-2008, by the Amazon Research Award.
References
[1] Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow,
Moritz Hardt, and Been Kim. Sanity checks for saliency maps. In
Proc. of NeurIPS, 2018.
[2] Martin Arjovsky, Soumith Chintala, et al. Wasserstein generative
adversarial networks. In Proc. of ICML, 2017.
[19] Yonghong Huang and Shih-han Wang. Adversarial manipulation of
In Proc. of
reinforcement learning policies in autonomous agents.
IJCNN, 2018.
[20] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic
optimization. arXiv preprint:1412.6980, 2014.
[21] Panagiota Kiourti, Kacper Wardega, Susmit Jha, and Wenchao Li.
Trojdrl: Trojan attacks on deep reinforcement learning agents. arXiv
preprint arXiv:1903.06638, 2019.
[22] Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Proc.
of NeurIPS, 2000.
[23] Jernej Kos and Dawn Song. Delving into adversarial attacks on deep
policies. In Proc. of ICLR Workshop, 2017.
[24] Solomon Kullback. Information theory and statistics. Courier Corpo-
ration, 1997.
[3] Trapit Bansal, Jakub Pachocki, Szymon Sidor, Ilya Sutskever, and Igor
Mordatch. Emergent complexity via multi-agent competition. In Proc.
of ICLR, 2018.
[25] Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, et al.
Tactics of adversarial attack on deep reinforcement learning agents. In
Proc. of IJCAI, 2017.
[4] Vahid Behzadan and Arslan Munir. Vulnerability of deep reinforcement
learning to policy induction attacks. In Proc. of MLDM, 2017.
[5] Vahid Behzadan and Arslan Munir.
deep reinforcement learning, makes it stronger.
arXiv:1712.09344, 2017.
Whatever does not kill
arXiv preprint
[6] Vahid Behzadan and Arslan Munir. Mitigation of policy manipulation
attacks on deep q-networks with parameter-space noise. In Proc. of
SAFECOMP, 2018.
[7] Nicholas Carlini and David Wagner. Towards evaluating the robustness
of neural networks. In Proc. of S&P, 2017.
[8] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Tar-
geted backdoor attacks on deep learning systems using data poisoning.
arXiv preprint arXiv:1712.05526, 2017.
[9] Piotr Dabkowski and Yarin Gal. Real time image saliency for black
box classiﬁers. In Proc. of NeurIPS, 2017.
[10] Adam Gleave, Michael Dennis, Neel Kant, Cody Wild, et al. Adversar-
ial policies: Attacking deep reinforcement learning. In Proc. of ICLR,
2020.
[11] Ian J Goodfellow, Jonathon Shlens, et al. Explaining and harnessing
adversarial examples. In Proc. of ICLR, 2015.
[12] Evan Greensmith, Peter L Bartlett, and Jonathan Baxter. Variance
reduction techniques for gradient estimates in reinforcement learning.
Journal of Machine Learning Research, 2004.
[13] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets:
Identifying vulnerabilities in the machine learning model supply chain.
In Proc. of NeurIPS Workshop, 2017.
[14] Arthur Guez, David Silver, and Peter Dayan. Efﬁcient bayes-adaptive
reinforcement learning using sample-based search. In Proc. of NeurIPS,
2012.
[15] Aaron Havens, Zhanhong Jiang, and Soumik Sarkar. Online robust
policy learning in the presence of unknown adversaries. In Proc. of
NeurIPS, 2018.
[16] Nicolas Heess, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg
Wayne, Yuval Tassa, Tom Erez, Ziyu Wang, SM Eslami, Martin Ried-
miller, et al. Emergence of locomotion behaviours in rich environments.
In Proc. of NeurIPS, 2017.
[17] Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina
Precup, and David Meger. Deep reinforcement learning that matters.
In Proc. of AAAI, 2018.
[18] Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter
Abbeel. Adversarial attacks on neural network policies. In Proc. of
ICLR workshop, 2017.
[26] Yen-Chen Lin, Ming-Yu Liu, Min Sun, and Jia-Bin Huang. Detecting
adversarial attacks on neural network policies with visual foresight.
arXiv preprint arXiv:1710.00814, 2017.
[27] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai,
Weihang Wang, and Xiangyu Zhang. Trojaning attack on neural
networks. In Proc. of NDSS, 2018.
[28] Ajay Mandlekar, Yuke Zhu, Animesh Garg, et al. Adversarially
robust policy learning: Active construction of physically-plausible
perturbations. In Proc. of IROS, 2017.
[29] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex
Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray
Kavukcuoglu. Asynchronous methods for deep reinforcement learning.
In Proc. of ICML, 2016.
[30] Volodymyr Mnih, Koray Kavukcuoglu, et al. Playing atari with deep
reinforcement learning. In Proc. of NeurIPS Deep Learning Workshop,
2013.
[31] Volodymyr Mnih, Koray Kavukcuoglu, et al. Human-level control
through deep reinforcement learning. Nature, 2015.
[32] OpenAI. Openai at the international 2017. https://openai.com/
the-international/, 2017.
[33] OpenAI. Roboschool: open-source software for robot simulation.
https://openai.com/blog/roboschool/, 2017.
[34] OpenAI. Openai ﬁve. https://openai.com/blog/openai-five/,
2018.
[35] OpenAI. Emergent tool use from multi-agent interaction. https:
//openai.com/blog/emergent-tool-use/, 2019.
[36] Nicolas Papernot, Patrick McDaniel, Somesh Jha, et al. The limitations
of deep learning in adversarial settings. In Proc. of Euro S&P, 2016.
[37] Anay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan,
and Girish Chowdhary. Robust deep reinforcement learning with
adversarial attacks. In Proc. of AAMAS, 2018.
[38] Tabish Rashid, Mikayel Samvelyan, et al. Qmix: Monotonic value
function factorisation for deep multi-agent reinforcement learning. In
Proc. of ICML, 2018.
[39] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why should
i trust you?: Explaining the predictions of any classiﬁer. In Proc. of
KDD, 2016.
[40] Alessio Russo and Alexandre Proutiere. Optimal attacks on reinforce-
ment learning policies. arXiv preprint arXiv:1907.13548, 2019.
[41] John Schulman, Sergey Levine, et al. Trust region policy optimization.
In Proc. of ICML, 2015.
USENIX Association
30th USENIX Security Symposium    1899
[42] John Schulman, Philipp Moritz, et al. High-dimensional continuous
In Proc. of ICLR,
control using generalized advantage estimation.
2016.
[43] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and
Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint
arXiv:1707.06347, 2017.
[44] Lloyd S Shapley. Stochastic games. Proc. of the national academy of
sciences, 1953.
[45] David Silver, Aja Huang, et al. Mastering the game of go with deep
neural networks and tree search. nature, 2016.
[46] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside
convolutional networks: Visualising image classiﬁcation models and
saliency maps. In Proc. of ICLR, 2013.
[47] Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viégas, and Martin
Wattenberg. Smoothgrad: removing noise by adding noise. arXiv
preprint arXiv:1706.03825, 2017.
[48] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribu-
tion for deep networks. In Proc. of ICML, 2017.
[49] Christian Szegedy, Wojciech Zaremba, et al. Intriguing properties of
neural networks. In Proc. of ICLR, 2015.
[50] Emanuel Todorov, Tom Erez, et al. Mujoco: A physics engine for
model-based control. In Proc. of ICIRS, 2012.
[51] Florian Tramèr, Alexey Kurakin, et al. Ensemble adversarial training:
Attacks and defenses. In Proc. of ICLR, 2018.
[52] Florian Tramèr, Fan Zhang, et al. Stealing machine learning models
via prediction apis. In Proc. of USENIX Security Symposium, 2016.
[53] Hado Van Hasselt, Arthur Guez, and other. Deep reinforcement learn-
ing with double q-learning. In Proc. of AAAI, 2016.
[54] Chaowei Xiao, Xinlei Pan, et al. Characterizing attacks on deep
reinforcement learning. arXiv:1907.09470, 2019.
[55] Tianbing Xu, Qiang Liu, Liang Zhao, and Jian Peng. Learning to
explore via meta-policy gradient. In Proc. of ICML, 2018.
[56] Zhaoyuan Yang, Naresh Iyer, Johan Reimann, and Nurali Virani. De-
sign of intentional backdoors in sequential models. arXiv preprint
arXiv:1902.09972, 2019.
[57] Kaiqing Zhang, Zhuoran Yang, and Tamer Ba¸sar. Multi-agent rein-
forcement learning: A selective overview of theories and algorithms.
arXiv preprint arXiv:1911.10635, 2019.
[58] Marvin Zhang, Zoe McCarthy, Chelsea Finn, Sergey Levine, and Pieter
Abbeel. Learning deep neural network policies with continuous mem-
ory states. In Proc. of ICRA, 2016.
[59] Yiren Zhao, Ilia Shumailov, et al. Blackbox attacks on reinforcement
arXiv
learning agents using approximated temporal information.
preprint arXiv:1909.02918, 2019.
Appendix
Victim policies. The network architecture of the victim pol-
icy in the MuJuCo game and the roboschool Pong game are:
MLP-380-128-128-17 [3] and MLP-13-64-64-2, respectively.
Hyper-parameters of the baseline. The baseline has two
sets of hyper-parameters: the adversarial policy/value network
architecture, and the hyperparameters of the PPO algorithm.
For the MuJoCo game, we directly used the default choices
in [10]. For the roboschool Pong game, we set the adversarial
policy network and its value function as MLP-13-64-64-2
Figure 9: Comparison of our attack and the attack with l2.
and MLP-13-64-64-1, and use the same set of PPO hyper-
parameters with the MoJuCo game.
Hyper-parameters of our method. Here, we specify the
hyper-parameters that are not varied in the sensitivity test.
First, we applied the choices of [10] for those inherent
from [10] (i.e., policy/value network architectures and the
PPO hyper-parameters). In addition, our attack has four hyper-
parameters: H, F, εs, and εa. We set εs/εa as widely used
empirical values [7] and H/F similar to the policy network
architectures. Speciﬁcally, for the MuJoCo game, we set
εs = 1, εa = 0.05, H: MLP-414-40-64-380, and F: MLP-380-
64-64-17. For the roboschool Pong game, we set εs = 0.01,
εa = 0.05, H: MLP-17-40-16-13, and F: MLP-13-64-64-2.
Effectiveness of l2 norm on the Pong game. In Figure 6d,
we show the solution developed on l2 norm is worse than
those developed on l1, l∞, and our baseline. We argue that this
is because l2 norm is not suitable for high-dimensional input.
In order to validate this, we run the similar experiment on
Robotschool Pong game. Different from the MuJoCo game,
here, the agent takes a low-dimensional input (13 features).
In Figure 9, we depict that, for the Pong game, the solution
developed on the l2 norm is just as good as our ﬁnal solution
which utilizes l1. This well conﬁrms our argument. That is,
the l2 norm is not suitable for a situation where the input high
dimensionality inputs, and l1 or l∞ is a better ﬁt.
(a) MuJoCo.
(b) Roboschool Pong.
Figure 10: The performance of our attack with different η.
Additional parameter sensitivity test. In our experiments,
we set equal weight to the action difference term and the
observation difference term in Eqn. (9). Here, we vary the
relative weight between two terms and observe its inﬂuence
upon our attack performance. Speciﬁcally, we introduce a
weight η to the observation different term (i.e., −η(cid:107) ˆo(t+1)
−
o(t+1)
(cid:107)1) and train adversarial agent with
v
η = [1,2,3,4]. Figure 10 shows the winning rate of the ad-
versarial agent on two selected games. The results show that
subtly varying η imposes only a negligible inﬂuence upon the
performance of the adversarial agents trained by our attack.
(cid:107)1 +(cid:107) ˆa(t+1)
v
v
−a(t+1)
v
1900    30th USENIX Security Symposium
USENIX Association
        0               1.0             2.0              3.0           4.0100  6030Time steps (1e6)Winning rate (%)Our100  500Time steps (1e7)Winning rate (%)   1   2   3 0              0.5            1.0             1.5          2.0   4100  6030Time steps (1e6)Winning rate (%)   1   2   3 0              1.0            2.0             3.0          4.0   4