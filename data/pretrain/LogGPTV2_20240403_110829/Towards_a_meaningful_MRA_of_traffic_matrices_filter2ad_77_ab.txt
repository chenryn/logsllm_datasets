On the other hand, the gravity model [20] is so simple that
it has already seen extensive use as a model for TMs.
A ﬁrst approach to apply graph-based MRA to the study
of traﬃc matrices was carried out by Crovella and Kolac-
zyk [6], where ”graph wavelets” were introduced as an ex-
tension of the 2D wavelet transform. Graph wavelets pro-
vide a way of computing the load diﬀerences between links
separated by a certain number of nodes (i.e., the concept of
scale is replaced by that of hop distance between links). The
authors also show how the tool can be used for anomaly de-
tection. The main drawbacks of this approach are the lack
of a fast computational algorithm and its non-orthogonali-
ty. Graph wavelets do not provide a sparse representation of
traﬃc data but rather an overcomplete decomposition sim-
ilar to that of the Continuous Wavelet Transform (CWT).
Diﬀusion Wavelets [5] can be understood as a generaliza-
tion of graph wavelets, allowing more freedom for choos-
ing the underlying kernel function (or “mother wavelet”),
along with generalized distances in the graph, an orthonor-
mal basis, and a fast computation algorithm. To the best
of our knowledge, the only relevant application of Diﬀusion
Wavelets in the context of computer networks is [4], where
Coates et al. address the problem of assessing the number of
measurement points required to monitoring end-to-end path
metrics, such as delay at the IP level or bit-error-rate perfor-
mance at the physical level of an optical network. The au-
thors construct a diﬀusion operator on an alternative graph
where the nodes are the routes of the original network, and
the links are a similarity measure of the routes (the fraction
of shared links, according to the routing data), and apply
the (1D) DW transform. The implicit sparsity of the ana-
lyzed data in the transformed domain, together with the use
of sparse inference techniques, allows an eﬃcient monitor-
ing with a reduced number of devices (for example, network
mean end-to-end delay can be measured with high precision
by monitoring only 7% of the routes).
use of the underlying graph for the diﬀusion makes the DW
intrinsically adapted to the “topology” over which the func-
tions f (i) are deﬁned.
Mathematically we represent the diﬀusion operator by a
linear transform T f . Just as there are many possible mother
wavelet and scaling functions, there are many choices we
could make for T . Simple examples include a heat-like dif-
fusion (hence the name) across the graph, or a stochastic
matrix representing a random walk on the matrix. The lat-
ter seems a natural choice since it models an approximation
of the distance between nodes in a graph, but instead we
follow [12] and choose the I − L operator, where I is the
identity matrix and L is the normalized Laplacian [3] of A,
the (weighted) adjacency matrix of the graph1. This opera-
tor is closely related to the random walk [3, 12] and it has the
same eigenvalues but, unlike the random walk, the I −L op-
erator is symmetric, among other desirable properties. The
operator is later scaled in order to be doubly-stochastic.
The dilation operator used to construct subsequent scal-
ing functions is simply to take powers of the matrix T . In-
tuitively, if a diﬀusion continues over n time steps, we would
apply the linear transform n times, i.e., T nf . This results
in successive blurring of the function, as required.
In the
random walk interpretation, assume f represents an initial
distribution of states, then T nf represents the state distribu-
tion after n time steps, which we know will tend to blur (for
an irreducible Markov chain) towards the equilibrium dis-
tribution. Analogously to the standard wavelet transform,
DW progresses in powers of 2, i.e., we consider T 2j
f .
For graphs, the natural equivalent to the frequency-based
decomposition resulting from the DWT is spectral graph
theory, i.e., the study of the eigenvalues and eigenfunctions
of linear operators [3]. The Spectral Theorem results in a
simple representation of the linear operator
λiνT
i νi,
T =:i=1
3. WAVELETS & DIFFUSION WAVELETS
Wavelet methods have been used in signal and image pro-
cessing for denoising and compressing, among other applica-
tions. The Discrete Wavelet Transform (DWT) analyzes sig-
nals by computing its scalar product with dilated (by powers
of 2) and translated versions of the mother wavelet func-
tion, thus analyzing the input signal at time scales t = 2j.
Provided that the mother wavelet satisﬁes certain condi-
tions [13], the resulting transform is orthonormal and can
be eﬃciently implemented by a bank of quadrature-mirror
low-pass and high-pass ﬁlters (h(n) and g(n), respectively)
followed by downsampling, as illustrated in Figure 1.
The low-pass ﬁlters perform successive approximation on
the signal at coarser and coarser scales. Intuitively, we might
think of this as successively “blurring” the original signal.
Implicitly, there exists a mother scaling function from which
we could derive the blurring functions using the same di-
lations and translations as with the mother wavelet. The
wavelet details dx(j, k) capture the diﬀerence between the
approximation ax(j − 1, k) at some scale j − 1, and a coarser
level of approximation ax(j, k). In mathematical terms, we
obtain a set of nested approximation (scaling) subspaces Vj,
V1 ⊃ V2 ⊃ . . . ⊃ VJ and their orthogonal complements, the
high-frequency detail (wavelet) subspaces Wj = Vj−1 − Vj.
In the frequency domain the DWT results in a decompo-
sition in subbands whose spectra are halved at each step.
This gives rise to a multiresolution analysis in which the
original signal is decomposed into a low frequency approxi-
mation at the largest time scale t = 2J , ax(J, k) and a set of
high-frequency details dx(j, k) (the wavelet coeﬃcients) for
each time scale t = 2j, j = 1 . . . J. The transform can be
generalized to 2D images, as we will see in Section 4.
where λi are the eigenvalues of T and νi are their associated
eigenvectors. If T is a doubly stochastic matrix, |λi| ≤ 1.
Figure 1: Left: the 1D DWT ﬁlter bank for J=3,
with approximation ax(3, k) and details dx(j, k), j =
1 . . . 3. Right: the associated spectrum subbands.
The aforementioned classical time- and space-based wave-
let transforms operate on signals deﬁned on uniformly sam-
pled grids on R and R2, respectively. However, a TM is not
deﬁned on a regular lattice — it is deﬁned across a computer
network, which can be represented by a graph. Diﬀusion
Wavelets [5] are a generalization of the wavelet transform
in which the MRA can be performed on structures such as
manifolds or graphs. In our case the underlying structure
is a graph G{V, E} (where V and E are the vertex and
edges sets, respectively). We wish to analyze a function
f : V → R, i.e., we have a function f (i), which maps each
vertex i to a real number.
The approach is to create a diﬀusion operator that plays
the role of the mother scaling function. Application of the
diﬀusion operator “blurs” the original function, but in a way
that is adapted to the underlying graph. Locations that are
close together in the graph will be blurred into each other,
while locations that are far apart will remain separated. The
The principle that underlies PCA (Principal Components
Analysis) is that it is common that many eigenvalues of such
an operator will be near zero, and thus we may approximate
the matrix T through a partial sum. This has been exploited
in the direct analysis of traﬃc matrices [11]. Here, the con-
cept is applied to the graph-diﬀusion operator T , but we will
apply it very conservatively. In our approximation we will
ignore eigenvalues |λi| ≤ , where  is a tunable parameter
with small value (ranging from 10−3 to 10−10 in our exper-
iments). Few (if any) eigenvalues are removed in the ﬁrst
round, but things change when we consider powers of T .
i , and the eigenvectors remain
invariant with respect to n. As n → ∞ all of the eigenval-
ues |λi| < 1 will tend to zero, and eventually they will fall
below the threshold . As such, the successive application
of the (now approximated) diﬀusion operator will break the
graph spectrum into subbands, much as the classical wavelet
transform does.
The eigenvalues of T n are λn
1In a network-related scenario we may wish to make a tran-
sition across a “long” link less likely. Weights can (for ex-
ample) be inversely proportional to the routing weights, ob-
taining a diﬀusion operator that is somehow related to load-
balancing routing. In the unweighted adjacency matrix case,
a constant value (e.g. 1) can be used as weight.
Figure 2: Left: ﬁlter bank associated with the 2D wavelet transform, for J=2 scales. Right: the associated
subband decomposition.
The MRA can then be deﬁned as follows: for a given scale
i | ≥  span the low
j, the eigenvectors associated with |λ2j
frequency approximation subspace Vj, while the eigenvec-
tors associated with the eigenvalues discarded at step j, i.e.,
| ≥  span the high frequency
such that |λ2j
or detail subspace Wj. At each step j the surviving eigen-
vectors are appropriately reorthonormalized (for example,
with a Gram-Schmidt-style algorithm).
i | <  and |λ2j−1
i
Coiﬀman and Maggioni [5] present a fast algorithm for
performing the aforementioned computations, and obtaining
the approximation and detail coeﬃcients at level j (denoted
as CVj and CWj , respectively) by projecting the function
under study onto the Vj and Wj subspaces.
4. 2D DW TRANSFORM
Traﬃc matrices can be represented as two-dimensional
functions F (v1, v2) of pairs of vertices where v1 is the ingress
node, v2 is the egress node, and F (v1, v2) is the traﬃc vol-
ume from v1 to v2. Hence, we need to extend DWs to 2D.
Classical 1D wavelets can be used to construct a separa-
ble basis in 2D by combining the application of the low-
and high-pass ﬁlter banks in the horizontal and vertical
dimensions of the input image I(x, y), thus generating 4
subbands at each scale: the low-pass/low-pass approxima-
tion aaI (j, k, l), and the details adI (j, k, l), daI (j, k, l) and
ddI (j, k, l) from the other ﬁlter combinations. After appro-
priately downsampling the outputs, the process is iterated
on the approximation subband, as shown in Figure 2.
Analogously, our approach to 2D DWs transforms the
function F (v1, v2) by projecting it twice onto the approx-
imation and detail subspace bases deﬁned by the 1D DW
diﬀusion operator, once along each “direction”. The details
of the algorithm vary because we no longer have simple ﬁlter
bank implementations of the high-pass and low-pass opera-
tions, but intuitively the process is similar. We denote by
CV Vj , CV Wj , CW Vj and CW Wj the transform coeﬃcients
corresponding to the low-pass approximation subspace V Vj
and the high-frequency detail subspaces V Wj, W Vj and
W Wj, respectively. For the 1D DW transform described
here, the resulting 2D transform retains the highly desirable