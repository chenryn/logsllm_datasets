1.0
0.8
0.6
0.4
0.2
0.0
2008
2014
1
10
100
RTT [ms]
1000
10000
Figure 10: Delay between access aggregation point and front-
end servers in 2008 [45] vs. 2014.
Content and application providers, on the other hand, can pro-
vide better service by installing servers deep inside ISPs and/or
through increased peering at IXPs or colocation facilities. These
approaches have been reported in [28, 46, 20] and have led to an
increase in back-ofﬁce Web trafﬁc. Next, we analyze our data sets
to see if these approaches have indeed decreased the latencies be-
tween end users and front-end servers. The analysis is based on
anonymized packet-level traces captured in a residential network
within a large European ISP in 2008 and in January 2014. For
more details on data capturing, processing and anonymization, we
refer to [45].
Figure 10 shows the backbone delay between the aggregation
point of the ISP and the front-end servers that end users of the same
residential network experienced in two different years — 2008, and
2014.8 While the access technology, and the corresponding de-
lay between end users and the access aggregation point, have not
changed over the last six years (not shown), the delay between the
aggregation point and servers has seen a signiﬁcant shift towards
shorter delays. A signiﬁcant fraction of the requests are now ter-
minated by servers within the same city (see the new bell curve
around 1 ms delay which was not present in 2008); pushing the
content closer to the end users has been successful.
10.
IMPLICATIONS
In sections 2 through 4, we deﬁned back-ofﬁce Web trafﬁc, dis-
cussed some methods to identify this trafﬁc from different data sets,
and presented some key insights from our analysis. We showed
that back-ofﬁce trafﬁc is responsible for a signiﬁcant fraction of to-
day’s Web trafﬁc, and offered two key reasons to explain this phe-
nomenon: (1) sustained deployment of front-end servers close to
end users, and (2) substantial data exchanges conducted by service
providers as part of their operations viz., coordination and synchro-
nization of components of their distributed infrastructure. In this
section, we discuss a few implications that affect researchers and
operators.
This point of view allows us to differentiate the deployed servers
into two broad categories, namely front-ofﬁce and back-ofﬁce servers
that support many of the popular Web applications which run on
top of the Internet. The front ofﬁce is devoted to serving end users,
and its performance is mostly inﬂuenced by the access technology
and, to a different extent, by back-ofﬁce performance. The back
ofﬁce, in turn, operates on the faster and wider pipes of the Internet
backbone, involves multiple organizations and remains invisible to
the end-users, yet virtually connected to them via the front-ofﬁce.
However, there is more in the back ofﬁce than content delivery,
8We note that not necessarily all trafﬁc is exchanged with front-end
servers e.g., some trafﬁc is due to peer-to-peer applications.
268which includes, but is not limited to, real-time bidding and crawl-
ing.
With the increasing trend to ofﬂoad “computations” to the cloud
to support enterprise applications [53] or desktop application, e.g.,
the new release of Microsoft Ofﬁce, Apple iCloud, and DropBox [24],
it is clear that the volume of back-ofﬁce Web trafﬁc will continue
to increase. First, application servers at different data centers will
have to exchange data with each other for data synchronization and
replication. Second, as edge servers attempt to offer more ser-
vices [43, 28], viz., personalized Web experiences for end users,
they will need to communicate with other edge servers and back-
ofﬁce servers, at different locations, to retrieve relevant content for
the end users.
The Web ecosystem as presented in this work, manifesting a rich
interplay between front-ofﬁce and back-ofﬁce servers, has many
practical implications that affect not only end users, but also re-
searchers and operators.
For researchers, the implications are two-fold. First, it is of
utmost importance to differentiate between back-ofﬁce and front-
ofﬁce Web trafﬁc as they exhibit different characteristics and opera-
tions. Follow-up works may study the correlation of trafﬁc dynam-
ics, topology aspects of the Internet, and performance evaluation
with each of the two classes of Web trafﬁc. Second, researchers
should be aware that the infrastructure deployed for the back-ofﬁce
presents a unique opportunity to develop and deploy new protocols
that are better suited to the characteristics of the front- or the back-
ofﬁce.
For operators, the implications are multi-fold. First, operators
should be aware of the importance of links that carry back-ofﬁce
trafﬁc because they may (1) help in monetization of services, or (2)
affect the front-ofﬁce operations, viz., impairing the QoE of more
end-users than anticipated. Second, operators should be aware that
back-ofﬁce trafﬁc may have different requirements than front-ofﬁce
trafﬁc. This, in turn, creates new opportunities for operators to
provide customized services. In addition, operators may also want
to offer different Service Level Agreements (SLAs) for different
trafﬁc classes.
10.1 Deployment of new protocols
Deploying new protocols or protocol variants may be very slow
or even infeasible if it requires changes in the core infrastructure of
the Internet or all end Internet end systems, for instance, consider
the adoption of a new TCP variant or IPv6. However, restricting the
change to one organization makes this otherwise infeasible effort
much easier. Indeed, most Web service companies have the ability
to roll out updates to their infrastructure and upgrade it on a regular
basis. Thus, if end users connect only to the front-end servers and
the front-end and back-end servers are managed by a single entity,
it is feasible to rapidly deploy changes and optimize back-ofﬁce
Web communications; these changes are restricted to the servers in-
volved in carrying the back-ofﬁce trafﬁc. Examples include but are
not limited to use of persistent TCP connections between servers,
IPv6, multi-path TCP [29], on-the-ﬂy Web page assembly [43], ob-
ject pre-fetching, compression and delta-encoding, and ofﬂoading
of computations. Indeed, even the TCP version and parameters can
be chosen according to the tasks [28], e.g., the initial window size
can be increased. Moreover, it is possible to adopt new network-
ing paradigms, such as Software Deﬁned Networking [37], much
more quickly to handle the back-ofﬁce trafﬁc. As most end-user
communication involves some back-ofﬁce communication, these
improvements can directly affect the end-user experience. Lastly,
more efﬁcient use of the networking resources can also reduce the
cost of content delivery.
10.2 New service opportunities
Network operators i.e., ISPs, can supply CDNs or content providers
with the infrastructure that is speciﬁcally tailored to handle back-
ofﬁce trafﬁc. For ISPs, this opens up additional opportunities for
monetization of services and also provide a better experience to
their end users. But, this comes at a cost: ISPs must invest in and
enable micro data centers or virtualized services [30, 4] in their
networks to harness the opportunity.
On the other hand, ISPs can also customize their trafﬁc engi-
neering policies to better accommodate the two different classes
of trafﬁc. They can also offer a different set of SLAs targeted at
organizations operating a back ofﬁce. Along the same line, IXPs
can provide value-added services for time-critical applications e.g.,
bidding and ﬁnancial applications. IXPs can also incentivize co-
location at their peering locations using arguments such as the prox-
imity of third-party servers, the pricing model for exchanging traf-
ﬁc with servers in other networks, and the ability to access the re-
sources that cloud providers offer.
11. CONCLUSION
The Web and its ecosystem are constantly evolving. This paper
takes a ﬁrst step towards uncovering and understanding one compo-
nent of this ecosystem that is increasing in complexity, but remains
understudied: the back-ofﬁce. The back-ofﬁce includes the infras-
tructure necessary to support Web search, advertisements, and con-
tent delivery. By using a diverse set of vantage points, we’ve shown
that back-ofﬁce trafﬁc is responsible for a signiﬁcant fraction not
only of today’s Internet trafﬁc but also today’s Internet transactions.
Improvement in the back-ofﬁce has been a major contributor
to reducing the delays experienced by end users. Yet the back-
ofﬁce architecture still exposes many opportunities for deploying
new protocols or versions of protocols for specialized tasks. The
complexity of the back-ofﬁce, however, also poses new questions.
In future work we plan to improve our current methodology and
extend it to identify other use-cases for back-ofﬁce Web trafﬁc,
further dissect the interactions of the different services and better
understand the performance implications thereof. Finally, we also
plan to study the non-Web back-ofﬁce of the Internet.
Acknowledgments
We would like to thank Oliver Spatscheck (our shepherd), the anony-
mous reviewers, and Paul Barford for their constructive feedback.
This work was supported in part by the EU project BigFoot (FP7-
ICT-317858). Georgios Smaragdakis was supported by the EU
Marie Curie International Outgoing Fellowship “CDN-H” (PEOPLE-
628441).
12. REFERENCES
[1] Google AdExchange.
http://developers.google.com/ad-
exchange/rtb/getting_started.
[2] Internet Advertising Bureau (IAB). 2013 Internet Advertising
Revenue Report. http://www.iab.net/AdRevenueReport.
[3] Netﬂix Open Connect.
https://signup.netflix.com/openconnect.
[4] Network Functions Virtualisation. SDN and OpenFlow World
Congress, 2012.
[5] V. K. Adhikari, S. Jain, Y. Chen, and Z. L. Zhang. Vivisecting
YouTube: An Active Measurement Study. In IEEE INFOCOM, 2012.
[6] B. Ager, N. Chatzis, A. Feldmann, N. Sarrar, S. Uhlig, and
W. Willinger. Anatomy of a Large European IXP. In ACM
SIGCOMM, 2012.
269[7] B. Ager, W. Mühlbauer, G. Smaragdakis, and S. Uhlig. Web Content
Cartography. In ACM IMC, 2011.
[8] B. Ager, F. Schneider, J. Kim, and A. Feldmann. Revisiting
Cacheability in Times of User Generated Content. In IEEE GI, 2010.
[9] S. Angel and M. Walﬁsh. Veriﬁable auctions for online ad exchanges.
In ACM SIGCOMM, 2013.
[10] P. Barford, I. Canadi, D. Krushevskaja, Q. Ma, and
S. Muthukrishnan. Adscape: Harvesting and Analyzing Online
Display Ads. In WWW, 2014.
[11] L. A. Barroso, J. Dean, and U. Holzle. Web Search for a Planet: The
Google Clustering Architecture. IEEE Micro, 23, 2003.
[12] T. Benson, A. Akella, and D. A. Maltz. Network trafﬁc
characteristics of data centers in the wild. In ACM IMC, 2010.
[13] T. Benson, A. Anand, A. Akella, and M. Zhang. MicroTE: Fine
Grained Trafﬁc Engineering for Data Centers. In CoNEXT, 2011.
[14] I. Bermudez, M. Mellia, M. Munafà, R. Keralapura, and A. Nucci.
DNS to the Rescue: Discerning Content and Services in a Tangled
Web. In ACM IMC, 2012.
[15] L. Bernaille and R. Teixeira. Early recognition of encrypted
applications. In PAM, 2007.
[16] S. Brin and L. Page. The anatomy of a large-scale hypertextual Web
search engine. In WWW, 1998.
[17] M. Butkiewicz, H. V. Madhyastha, and V. Sekar. Characterizing Web
Page Complexity and Its Impact. IEEE/ACM Trans. Networking,
22(3), 2014.
[18] M. Calder, X. Fan, Z. Hu, E. Katz-Bassett, J. Heidemann, and
R. Govindan. Mapping the Expansion of Google’s Serving
Infrastructure. In ACM IMC, 2013.
[19] F. Chang, J. Dean, S. Ghemawat, W. C. Hsieh, D. A. Wallach,
M. Burrows, T. Chandra, A. Fikes, and R. E. Gruber. Bigtable: A
Distributed Storage System for Structured Data. 2006.
[20] N. Chatzis, G. Smaragdakis, A. Feldmann, and W. Willinger. There is
More to IXPs than Meets the Eye. ACM CCR, 43(5), 2013.
[21] Y. Chen, S. Jain, V. K. Adhikari, and Z. L. Zhang. Characterizing
Roles of Front-End Servers in End-to-End Performance of Dynamic
Content Distribution. In ACM IMC, 2011.
[22] Y. Chen, R. Mahajan, B. Sridharan, and Z. L. Zhang. A Provider-side
View of Web Search Response Time. In ACM SIGCOMM, 2013.
[23] F. Dobrian, A. Awan, D. Joseph, A. Ganjam, J. Zhan, V. Sekar,
I. Stoica, and H. Zhang. Understanding the Impact of Video Quality
on User Engagement. In ACM SIGCOMM, 2011.
[24] I Drago, M. Mellia an M. Munafo, A. Sperotto, R. Sadre, and
A. Pras. Inside Dropbox: Understanding Personal Cloud Storage
Services. In ACM IMC, 2012.
[25] Z. Durumeric, E. Wustrow, and J. A. Halderman. ZMap: Fast
Internet-Wide Scanning and its Security Applications. In USENIX
Security Symposium, 2013.
[26] J. Erman, A. Gerber, M. Hajiaghayi, D. Pei, and O. Spatscheck.
Network-aware Forward Caching. In WWW, 2009.
[27] A. Feldmann, N. Kammenhuber, O. Maennel, B. Maggs, R. De
Prisco, and R. Sundaram. A methodology for estimating interdomain
web trafﬁc demand. In ACM IMC, 2004.
[28] T. Flach, N. Dukkipati, A. Terzis, B. Raghavan, N. Cardwell,
Y. Cheng, A. Jain, S. Hao, E. Katz-Bassett, and R. Govindan.
Reducing Web Latency: the Virtue of Gentle Aggression. In ACM
SIGCOMM, 2013.
[29] A. Ford, C. Raiciu, M. Handley, S. Barre, and J. Iyengar.
Architectural guidelines for multipath TCP development. Internet
Draft, rfc-6182.
[30] B. Frank, I. Poese, Y. Lin, G. Smaragdakis, A. Feldmann, B. Maggs,
J. Rake, S. Uhlig, and R. Weber. Pushing CDN-ISP Collaboration to
the Limit. ACM CCR, 43(3), 2013.
[31] H. Gao, V. Yegneswaran, Y. Chen, P. Porras, S. Ghosh, J. Jiang, and
H. Duan. An Empirical Reexamination of Global DNS Behavior. In
ACM SIGCOMM, 2013.
[32] A. Gerber and R. Doverspike. Trafﬁc Types and Growth in Backbone
Networks. In OFC/NFOEC, 2011.
[33] P. Gill, V. Erramilli, A. Chaintreau, B. Krishnamurthy,
K. Papagiannaki, and P. Rodriguez. Follow the Money:
Understanding Economics of Online Aggregation and Advertising.
In ACM IMC, 2013.
[34] A. Greenberg, J. R. Hamilton, N. Jain, S. Kandula, C. Kim, P. Lahiri,
D. A. Maltz, P. Patel, and S. Sengupta. VL2: A Scalable and Flexible
Data Center Network. In ACM SIGCOMM, 2009.
[35] C. Huang, A. Wang, J. Li, and K. Ross. Measuring and Evaluating
Large-Scale CDNs. In ACM IMC, 2008.
[36] S. Ihm and V. S. Pai. Towards Understanding Modern Web Trafﬁc. In
ACM IMC, 2011.
[37] S. Jain, A. Kumar, S. Mandal, J. Ong, L. Poutievski, A. Singh,
S. Venkata, J. Wanderer, J. Zhou, M. Zhu, J. Zolla, U. Holzle,
S. Stuart, and A. Vahdat. B4: Experience with a Globally-Deployed
Software Deﬁned WAN. In ACM SIGCOMM, 2013.
[38] J. Kleinberg. Authoritative sources in a hyperlinked environment. In
ACM/SIAM SODA, 1998.
[39] R. Kohavi, R. M. Henne, and D. Sommerﬁeld. Practical Guide to
Controlled Experiments on the Web: Listen to Your Customers not to
the HiPPO. In ACM KDD, 2007.
[40] R. Krishnan, H. Madhyastha, S. Srinivasan, S. Jain,
A. Krishnamurthy, T. Anderson, and J. Gao. Moving Beyond
End-to-end Path Information to Optimize CDN Performance. In
ACM IMC, 2009.
[41] S. S. Krishnan and R. K. Sitaraman. Video Stream Quality Impacts
Viewer Behavior: Inferring Causality using Quasi-Experimental
Designs. In ACM IMC, 2012.
[42] C. Labovitz, S. Lekel-Johnson, D. McPherson, J. Oberheide, and
F. Jahanian. Internet Inter-Domain Trafﬁc. In ACM SIGCOMM, 2010.
[43] T. Leighton. Improving Performance on the Internet.
Communications of the ACM, 52(2):44–51, 2009.
[44] J. Liang, J. Jiang, H. Duan, K. Li, T. Wan, and J. Wu. When HTTPS
Meets CDN: A Case of Authentication in Delegated Service. In IEEE
Symp. on Security and Privacy, 2014.
[45] G. Maier, A. Feldmann, V. Paxson, and M. Allman. On Dominant
Characteristics of Residential Broadband Internet Trafﬁc. In ACM
IMC, 2009.
[46] E. Nygren, R. K. Sitaraman, and J. Sun. The Akamai Network: A
Platform for High-performance Internet Applications. SIGOPS Oper.
Syst. Rev., 2010.
[47] I. Poese, B. Frank, B. Ager, G. Smaragdakis, and A. Feldmann.
Improving Content Delivery using Provider-aided Distance
Information. In ACM IMC, 2010.
[48] I. Poese, B. Frank, G. Smaragdakis, S. Uhlig, A. Feldmann, and
B. Maggs. Enabling Content-aware Trafﬁc Engineering. ACM CCR,
42(5), 2012.
[49] L. Popa, A. Ghodsi, and I. Stoica. HTTP as the Narrow Waist of the
Future Internet. In SIGCOMM HotNets, 2010.
[50] F. Qian, A. Gerber, Z. M. Mao, S. Sen, O. Spatscheck, and
W. Willinger. TCP Revisited: A Fresh Look at TCP in the Wild. In
ACM IMC, 2009.
[51] InMon – sFlow. http://sflow.org/.
[52] M. Z. Shaﬁq, L. Ji, A. X. Liu, J. Pang, and J. Wang. A First Look at
Cellular Machine-to-Machine Trafﬁc – Large Scale Measurement
and Characterization. In ACM SIGMETRICS, 2012.
[53] J. Sherry, S. Hasan, C. Scott, A. Krishnamurthy, S. Ratsanamy, and
V. Sekar. Making Middleboxes Someone Else’s Problem: Network
Processing as a Cloud Service. In SIGCOMM, 2012.
[54] R. K. Sitaraman, M. Kasbekar, W. Lichtenstein, and M. Jain. Overlay
Networks: An Akamai Perspective. John Wiley & Sons, 2014.
[55] K. Springborn and P. Barford. Impression Fraud in Online
Advertising via Pay-Per-View Networks. In USENIX Security
Symposium, 2013.
[56] F. Streibelt, J. Boettger, N. Chatzis, G. Smaragdakis, and
A. Feldmann. Exploring EDNS-Client-Subnet Adopters in your Free
Time. In ACM IMC, 2013.
[57] S. Triukose, Z. Wen, and M. Rabinovich. Measuring a Commercial
Content Delivery Network. In WWW, 2011.
[58] N. Weaver, C. Kreibich, M. Dam, and V. Paxson. Here Be Web
Proxies. In PAM, 2014.
[59] S. Yuan, J. Wang, and X. Zhao. Real-time bidding for online
advertising: measurement and analysis. In ADKDD, 2013.
270