resources in edge routers, especially when threats from different
domains are concerned, where each domain may have large number
of users. In order reduce the states stored in each router, a more efﬁ-
cient way is to maintain per-face state instead of per-user ones. The
main observation made here is that interests from different (sub-
)domains traverse different faces at an edge router, while interests
coming from same (sub-)domain would traverse the same face. Ac-
cordingly, per-face states are stored and maintained in each router,
and decisions to preserve privacy are made upon those states. Un-
like Algorithm 1, each router stores ̺ : F ×N → IN T , where F is
the set of faces. ̺(f, n) indicates the number of times that content
Algorithm 1: The “vanilla” approach to preserving the privacy
of cache access. The description makes use of the toy example
in Figure 1.
Input: n - a content name, u - a user, ϕ - access state,
Ints = (u, n, pmode, ts0)
Output: A data packet to u in a privacy-preserving manner.
When R receives Ints from u, it records ts1, the timestamp of
interest arrival, and computes td0 = ts1 − ts0 as a one-hop
time delay.
if pmode == 0 then
if td(n) == 0 then
// default value td(n) = 0
R follows ICN protocol to obtain data packet Data
from the origin server;
R returns Data to u;
R follows ICN protocol to obtain data packet Data;
R delays td(n);
R returns Data to u;
else
end
else
if ϕ(u, n) == 0 then
R follows the ICN protocol to obtain data packet
Data from the origin server;
R records ts2 upon the arrival of Data, and computes:
tdx = ts2 − ts1; // RTT from R to origin server
h = tdx/(2td0) + 1; // expected # of hops from u
to the origin server
Generate td(n) according to Eq. 1;
ϕ(u, n) + +;
R returns retrieved Data to u;
R returns cached Data to u;
else
end
end
name n is requested from face f . The protocol can be illustrated in
Figure 1, where router r2, for example, keeps track of the faces con-
necting it to other routers and access points (e.g., r3 and r4), and
the times each face has requested content names that have been pre-
viously marked as privacy-related contents. After that, r2 follows
the protocol by adding random delays when fulﬁlling interests that
could potential thwart the privacy of other users’ access.
3.3 Low Granularity Approach
The main shortcoming of the approach described in §3.2 is that it
does not enable lower granularity of the preserved privacy—which
is especially required when both the adversary and honest users us
the same AP—unlike the protocol described in §3.1. To enable
lower granularity in the protocol described in §3.1, we maintain
several states in the router, which result high overhead that can be
misused, whereas the protocol in §3.2 reduces this overhead at the
cost of reduced granularity. We propose a new algorithm aiming
to maintain the advantage of both protocols, by maintaining and
distributing these states concerning access patterns of individual
users at the APs, located closer to but not controlled by end users.
The main idea of the protocol is to distribute state ϕ(u, n) on
the AP associated with users generating such requests, and to store
the face state ̺(f, n) in the router. Decisions for access privacy
are made at the router with the help of the AP. When the AP re-
ceives a request from the user, it checks if the user requested the
content before. If not, the pmode value is discarded (to eliminate
possible cheating attack about pmode), and the AP forwards the
request to the router. Otherwise, the AP directly sends the interest
to the router. Upon receiving the interest from a given face, the
router initially looks if the content is in the cache or not. If not,
it retrieves the content from the origin server and serves it to the
requesting user through that face; otherwise, the router checks the
face state ̺(f, n): if it is zero, which implies that no user on that
face has requested the content, the router returns the content after a
delay td(n) expires; otherwise, it looks at the ﬂag generated by the
AP: if it is true, which means that the user has already requested
the content before, the router fulﬁlls the interest immediately; oth-
erwise, the interest is fulﬁlled after a delay td(n) is expired.
4. RESULTS AND ANALYSIS
To understand the potential of the attack and how our designs
impact the performance, we perform several measurements on the
CCNx prototype [2] in simulated setting. To derive an accurate
representation of real-world timings, we feed the simulator with
topologies and per-hop RTT traces driven from the current Internet
using traceroute [3] to request several websites (origin servers).
4.1 Settings and Timing Data-sets
Our measurements are based on CCNx, an open source system
that implements CCN. CCNx implements both the communication
operations (naming and data conventions) and security operations
(using OpenSSL), for signature generation and veriﬁcation.
Because no ICN design is deployed yet, we lack real-world traces
of RTTs for real ICN. However, designs like CCN suggest operat-
ing on top of IP, making IP timings relevant for experiments. To
this end, we instrument the CCNx simulator with per-hop round
trip delays when issuing interests from within our campus (con-
nected directly to the Internet backbone) to reach each of the Alexa
top-100 sites [1]. We use traceroute to obtain per-hop RTT delay to
each of these sites—each of these sites is an origin server.
We notice that traceroute has several limitations that prevent di-
rect use of its measurements in our study. First, as the hop count
increases, there is no guarantee to have larger RTT than previous
RTT for smaller hop count. Second, path to origin servers may
change at any time, making two measurements for the same route
greatly different. Last, traceroute provides cumulative RTT as the
hop count increases, but not the per-hop time delay needed in our
study. To address the ﬁrst issue, we run many traceroute requests
at different times of the day to account for different network con-
ditions (which is the main reason that raises this issue), and record
different readings for a ﬁxed path to the requested site. Then, for
each hop we consider the median RTT among all RTTs given for
that hop. We observe that as we increase the number of measure-
ments of the traceroute for the same site we get ordered set of (me-
dian) readings: the closer to destination the hop count is, the larger
the RTT. To address the second issue of traceroute, we only con-
sider the path that is most popular in the returned traceroute results,
and discard all other paths. Once both issues are addressed, we
compute the per-hop delay RT Ti as:
RT Ti =(RT T t
RT T t
1
i − RT T t
i−1
i > 1
i = 1
,
where RT T t
i is the i-th returned record by traceroute for the given
site. A CDF of the per-hop RTT on the path to each origin server
is shown in Figure 2. Complementary CDF for smaller range of
RTT (RT Ti ≤ 1) is shown as a small graph within the CDF in
F
D
C
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
 0
 0.2  0.4  0.6  0.8
 1
 0
 20
 40
 60
 80  100  120  140
Time in millisecond (per hop)
Figure 2: An empirical CDF of the per-hop RTT for alexa-100.
Figure 2 (70% of the hops’ RTT in all per-hop measurements are
less 1 ms). Notice that the per-hop RTT are smaller than expected
on the Internet, which might be due to that two hops are in the same
router, same datacenter, or same CDN. However, the results in this
study are less signiﬁcantly affected by other than the total RTT and
the ﬁrst hop delay (used for validating the attack).
We feed the per-hop RTT to a dummy CCNx topology corre-
sponding to the toy example in Figure 1 for each of the hop counts
and the per-hop RTT to request these sites. That is, in each case
we control the number of hops between router r2 and r1 in Fig-
ure 1 to correspond to the number of hops returned by traceroute
for the given site. We then add the delay incurred over that hop as
measured by traceroute using the method explained earlier.
Because the hop count to reach different sites varies, we consider
24 sites that had exactly 16 returned valid hops in traceroute to
unify our analysis and discussion in this section. We only limit our
attention to those sites where traceroute returned 16 unmasked hops
and discard timed-out hops, if any. A boxplot of the normalized
per-hop RTT (deﬁned as RT Ti/ max{RT Tk} for 1 ≤ k ≤ h
and h is the hop-count, where RT Ti is the i-th hop—notice that
RT T1 = 2td0 in our protocols) for each of the 24 sites is shown in
Figure 3. Finally, we deﬁne the RTT up to each hop as the sum of
the per-hop RTT normalized by the total RTT to the origin server.
This is, RT Tk is deﬁned as RT T t
i=1 RT Ti/RT Th, where
RT Th = 2td0 + tdx is the total RTT up to the origin server, and
RT Ti is the the i-th hop RTT. Notice that RT T t
k is returned by
traceroute for each k, and can be used immediately in this study. A
plot of the normalized RTT up to the origin server is in Figure 4.
k = Pk
4.2 Results
Attack validation. First, we examine whether an adversary co-
located one-hop away from a legitimate user is able to exploit the
timing attack explained earlier to infer whether some contents are
being retrieved by that user or not. We note that as per the ICN
caching policy in CCN, contents are replicated and cached at each
hop, thus future requests are fulﬁlled immediately from the closest
router to the user. From Figure 4, we observe that an adversary
who is co-located with the user who has requested these sites ben-
eﬁt from the caching, and would ideally reduce the total RTT for
fulﬁlling a request by a cache hit at the ﬁrst hop by around 98%
for the most conservative sites (and more than 99% for the median
site). Even when a cache-miss happens, an RTT by a cache hit at
the sixth hop away from the user, for example, would be 40 times
at average (and about 25 times at worst) less than the RTT when
retrieving contents directly from the origin server—although this
scenario may not breach the privacy of user access patterns since
a 6-hop network has a large anonymity set. By feeding the timing
n
o
i
t
u
b
i
r
t
s
d
i
e
m
i
t
d
e
z
i
l
a
m
r
o
N
Figure 3: The normalized per-hop total RTT for 24 sites.
Site 
( index)
T
T
l
a
t
o
t
e
h
t
f
o
n
o
i
t
c
a
r
T
T
V1
V3
V5
V7
V9
V1
V1
V1
Hop  count
Figure 4: The normalized RTT up to the given hop count.
proﬁles in Figure 3 in CCNx we observe that the network latency
is the dominating part of the RTT in CCN, and other ICN-related
delay is negligible. From that, we conclude that an adversary that
relies only on the timing information can easily and successfully
infer that the contents are being cached in a near-by router due to
their access be a potentially co-located user with him.
How defenses impact the performance. One critical parameter
for our designs is td(n), which corresponds to the number of hops
d that an edge router estimates and according to which he gener-
ates noise and uses it to fulﬁll pending interests issued by end users
while maintaining privacy of prior requests. This parameter is gen-
erated and used in the three different protocols proposed in this
work. Given that we have access to the per-hop delays (as shown
in §4.1), we use d ≤ h directly to compute td(n) instead of the ap-