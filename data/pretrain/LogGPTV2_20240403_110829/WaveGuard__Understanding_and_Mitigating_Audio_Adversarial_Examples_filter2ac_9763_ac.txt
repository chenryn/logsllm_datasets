approximation of the human vocal tract. The model assumes
that a source signal e(n) (which models the vocal chords) is
passed as input to a resonant ﬁlter h(n) (that models the vocal
tract) to produce the resultant signal x(n). That is:
Figure 5: Model for linear predictive analysis of speech sig-
nals.
The source excitation e(n) can either be quasi-periodic im-
pulses (during voiced speech) or random noise (during un-
voiced speech). Both these source excitation sources are spec-
trally ﬂat implying that all spectral information is modeled in
the ﬁlter parameters.
LPC assumes a pth order all-pole ﬁlter h(n) which means
that each waveform sample is modelled as a linear combina-
tion of p previous values. That is,
x(n) = Σk=p
k=1akx(n− k) + e(n).
(6)
The basic problem of LPC analysis is to estimate the ﬁlter
parameters ak. Since the source signal is assumed to be an im-
pulse train or random white noise, the problem is formulated
as minimizing ||e(n)||2 which is the power of the excitation
signal. This reduces the parameter-estimation problem to a
linear regression problem in which the goal is to minimize:
k=1akx(n− k))2(cid:105)
minimize: (cid:104)||e(n)||2(cid:105) = (cid:104)(x(n)− Σk=p
(7)
Here, (cid:104)(cid:105) denotes averaging over ﬁnite number of waveform
samples. In practice, a long time-varying signal is divided
into overlapping windows of size w and LPC coeffecients ak
are estimated for each window by solving the above linear
regression problem. To re-synthesize the signal from the esti-
mated coefﬁcients, we use a random-noise excitation signal.
In our experiments, we use 25 millisecond windows with 12.5
millisecond overlap. We experiment with different numbers
of the LPC coeffecients which control the compression level
of the original signal.
Since LPC models the human vocal tract system, it pre-
serves the phonetic information of speech in the ﬁlter param-
eters. Bypassing a defense involving LPC transform, would
require the adversary to add an adversarial perturbation that
can be preserved in the LPC ﬁlter coeffecients; thereby re-
quiring the adversary to modify the phonetic information in
speech. We empirically demonstrate that the LPC transform
cannot be easily bypassed by an adaptive adversary.
5 Experimental Setup
x(n) = h(n)∗ e(n)
We evaluate our defense against the following recent audio
adversarial attacks on speech recognition systems [11,14,15]:
(5)
USENIX Association
30th USENIX Security Symposium    2279
Excitation GeneratorVocal Tract System (Filter)Filter ParametersWhite Noise Impulseor• Carlini: Attack introduced in [11]. This is a white-box
targeted attack on the Mozilla Deepspeech [4] ASR sys-
tem, where the attacker trains an adversarial perturbation
by minimizing the CTC loss between the target transcrip-
tion and the ASR’s prediction. This attack minimizes the
L∞ norm of the adversarial perturbation to constrain the
amount of distortion.
• Qin-I: Imperceptible attack described in [14]. This is
another white-box targeted attack that focuses on ensur-
ing imperceptibility of the adversarial perturbation by
using psycho-acoustic hiding. The victim ASR for this
attack is Google Lingvo [3].
• Qin-R: Robust attack described in [14]. This attack in-
corporates input transformations during training of the
adversarial perturbation which simulate room environ-
ments. This improves the attack robustness in real world
settings when played over the air. The victim ASR for
this attack is Google Lingvo [3].
• Universal: We implement the white-box attack de-
scribed in [15]. This is an untargeted attack which ﬁnds
an input-agnostic perturbation that can cause signiﬁcant
disruption in the transcription of the adversarial signal.
In our work, we follow the algorithm provided by the au-
thors and craft universal perturbation with an L∞ bound
of 400 (for 16-bit audio wave-forms with sample values
in the range -32768 to 32768). The victim ASR for this
attack is Mozilla DeepSpeech [4].
Target Adversarial Commands
the same subset of Mozilla Common Voice examples with L∞
distortion bound of 400.
Attack evaluations: We achieve 100% attack success rate for
Carlini and Qin-I attacks. For Qin-R, the attack achieves 47%
success rate (similar to that reported in the paper [14]) on 100
examples. In our experiments when recreating the Universal
attack, we achieve an attack success rate of 81% using the
same criteria as described in [15] i.e., the attack is considered
successful when the CER between original and adversarial
transcriptions is greater than 0.5.
5.2 Evaluation Metrics
As described in Section 3.2, in our detection framework, we
label an example as adversarial or benign based on the CER
between x and g(x). The decision threshold t controls the true
positive rate and false positive rate of our detector. Following
standard procedure to evaluate such detectors [24], we calcu-
late the AUC score - Area Under the ROC curve. A higher
AUC score indicates that the detector has more discriminative
power against adversarial examples.
Additionally, we also report the Detection Accuracy which
is calculated by ﬁnding the best detection threshold t on a
separate set containing 50 adversarial and benign examples.
"browse to evil dot com"
"hey google cancel my medical appointment"
"hey google"
"this is an adversarial example"
Table 1: Adversarial commands used for constructing targeted
adversarial examples.
5.1 Dataset and Attack Evaluations
We conduct all our experiments on the Mozilla Common
Voice dataset, which contains 582 hours of audio across
400,000 recordings in English. The audio data is sampled
at 16 kHz. We evaluate on the same subset of the Mozilla
Common Voice dataset, as used in [11], that is, the ﬁrst 100
examples from the Mozilla Common Voice test set. We con-
struct adversarial examples on this dataset using each of the
attacks described above. In the targeted attack scenario, we
randomly choose one of the target phrases listed in Table 1
and follow the attack algorithms to create 100 pairs of original
and adversarial examples for each attack type. For the untar-
geted universal attack, we train the universal perturbation on
Figure 6: Detection AUC Scores against Carlini attack at vary-
ing compression levels for the following transforms: (a) Quan-
tization - Dequantization; (b) Downsampling - Upsampling;
(c) Linear Predictive Coding (LPC); and (d) Mel Spectrogram
Extraction- Inversion.
6 Evaluation against Non Adaptive Attacks
The various input transformation functions we consider can
be parameterized to control the compression level of the trans-
formation. There is a trade-off between the compression level
and the discriminative power of the detector. At low compres-
sion levels the transformation may not eliminate the adversar-
2280    30th USENIX Security Symposium
USENIX Association
46812160.40.50.60.70.80.91AUC ScoreNumber of Quantization Bits(a) Quantization -Dequantization(b) Downsampling -Upsampling(d) Mel Extraction -Inversion(c) LPC200040006000800012000160000.40.50.60.70.80.91AUC ScoreDown-sampling Rate (Hz)52040802560.40.50.60.70.80.91AUC ScoreNumber of Mel Bins4102030400.40.50.60.70.80.91AUC ScoreLPC OrderAUC Score
Detection Accuracy
Defense
Downsampling - Upsampling
Quantization - Dequantization
Filtering
Mel Extraction - Inversion
LPC
6000 kHz
6 bits
Hyper-params Carlini Universal Qin-I Qin-R Carlini Universal Qin-I Qin-R
100% 100%
99%
95%
100% 100%
100% 100%
100% 100%
(Section 4.3)
80 Mel-bins
LPC order 20
100%
98.5%
99.5%
100%
100%
88%
88%
86%
92%
83%
1.00
0.99
1.00
1.00
1.00
0.91
0.92
0.92
0.97
0.91
1.00
1.00
1.00
1.00
1.00
1.00
0.93
1.00
1.00
1.00
Table 2: Evaluations for each input transformation defense against various non-adaptive attacks. We use two objective metrics:
AUC score and Attack Detection Accuracy for evaluation (higher values are better for both metrics).
ial perturbation. In contrast, at very high compression levels,
even the benign signals may become signiﬁcantly distorted
causing substantial change in their transcriptions. Keeping
this in mind, we perform a search over the hyper-parameters
for different audio transforms. The AUC score of the detector
against the Carlini attack for different transformation func-
tions at varying compression levels is depicted in Figure 6. For
most transformations, we observe the expected pattern where
the defense is effective at some optimal compression levels
and the AUC falls at very high or low compression levels.
The Mel extraction-inversion pipeline is effective for a wide
range of Mel-bins possibly due to the distortion introduced
by the phase estimation step during the inversion stage. For
the Filtering transform we do not perform a hyper-parameter
search and use the transformation parameters described in
Section 4.3.
6.1 Detection Scores
Based on the above described search, we ﬁnd the optimal
hyper-parameters for each of the transforms and report the
detection scores against all the attacks in Table 2. We observe
that at optimal compression levels, all the input transforms
listed in Section 4 can achieve high discriminative perfor-
mance against adversarial examples. As compared to targeted
adversarial examples, it is harder to detect examples with
universal adversarial perturbations. This is because univer-
sal perturbations attempt to distort the original transcription
rather than targeting a very different phrase. Interestingly, we
ﬁnd that the defense is effective even against the Qin-R attack
which incorporates noise simulation during training and leads
to adversarial examples that are robust to small changes. We
elaborate on this result in the following Section.
6.2 Analysis of undefended and defended
transcriptions
In Figure 7 we provide comparisons of Mean CER between
transcriptions of audio before and after passing through a
given transformation function (g) for both benign (orig) and
adversarial examples (adv). Additionally, we also calculate
the CER between the transcriptions of the defended adversar-
ial example and its benign counterpart: CER(orig,g(adv)).
The discriminative power of the detector is indicated
by the difference between CER(orig,g(orig)) (blue) and
CER(adv,g(adv)) (red). A high difference between the red
and blue bar graphs in Figure 7 indicates easier detection
of adversarial examples. From these results we can observe
that detecting the Qin-I attack is easier than detecting the
Carlini [11] attack. We can further deduce that detecting
Universal attacks is generally more difﬁcult for any given
transformation function compared to the Carlini and Qin-I
attacks.
The metric CER(orig,g(adv)) helps evaluate the ability of
the transformation function to recover the original transcript
from the adversarial audio. A low CER(orig,g(adv)) indi-
cates better recovery of the original transcript. We ﬁnd that
for the imperceptible attack Qin-I, the recovery rate of the
original transcript is higher than any other attack indicating
that the adversarial perturbation is unstable to small changes
in inputs.
The Qin-R attack has a lower CER(adv,g(adv)) for most
transformations as compared to Qin-I which suggests that
the adversarial perturbation generated by the Qin-R attack is
relatively more robust to input transformations. Also, recov-
ering the original transcription is much harder as compared
to Qin-I and is indicated by higher CER(orig,g(adv)) val-
ues. However, there is still a signiﬁcant difference between
the blue and red bar graphs for Qin-R, which can be used to
discriminate between adversarial and benign samples. This re-
sult is consistent with the high detection accuracy reported in
Table 2, since the transformations are successful in disrupting
the adversarial perturbations.
We provide a few sample transcriptions from our experi-
ments in Figure 8. The green commands indicate the transcrip-
tions from benign audio samples, while the red transcriptions
refer to adversarial commands from each attack type. Overall,
the results in Figure 7 and Figure 8 demonstrate that the abil-
ity to recover benign commands is dependent on the type of
attack and varies for each input transformation function.
USENIX Association
30th USENIX Security Symposium    2281
timing comparisons for all implementations on the Intel Xeon
CPU platform. The average inference time over the test set
for Mozilla Deepspeech ASR model is 2.540 seconds and that
of Google Lingvo ASR model is 4.212 seconds on the Intel
Xeon CPU Platform.
Process
Deepspeech ASR
Lingvo ASR
Downsampling-Upsampling
Quantization-Dequantization
Filtering
Mel Extraction - Inversion
LPC
Avg. Wall-Clock time (s)
2.540
4.212
0.148
0.001
0.035
0.569
0.781
Table 3: Average Wall-Clock time in seconds required for