design of our system, Agamotto.
Agamotto addresses the shortcomings of prior work, as
described in Table 1. It uses virtual machine snapshots (or
“checkpoints”) and thus inherits all of its advantages—clean-
state, reboot-free fuzzing. In contrast to prior snapshot-based
approaches, which used a single snapshot created at a ﬁxed
point in time (usually at program startup), however, Agamotto
creates multiple checkpoints automatically at strategic points
during a fuzzing run. These checkpoints allow Agamotto
to skip initial parts of many test cases, improving the over-
all fuzzing performance. In addition, we heavily optimized
individual virtual machine checkpointing primitives for an
efﬁcient multi-path exploration, which limits the performance
impact of the primitives themselves.
USENIX Association
29th USENIX Security Symposium    2543
Figure 1: High-level overview of Agamotto.
3.1 System Overview
Figure 1 shows a high-level overview of Agamotto. The ar-
chitecture of Agamotto takes the form of a typical virtual
machine introspection infrastructure. A full operating sys-
tem including the kernel-mode device driver—the fuzzing
target—runs within a guest virtual machine. Unlike prior
work [2], Agamotto does not impose any constraint on the
mode of execution; the guest virtual machine can execute na-
tively, using hardware support (e.g., Intel’s Virtual Machine
Extensions [28]) when available.
The fuzzer, whose primary task is to generate test cases
and process their execution feedback, is placed outside this
virtual machine, running alongside the virtual machine mon-
itor. Some kernel fuzzers such as Syzkaller place the fuzzer
inside the guest virtual machine. This architecture is not suit-
able when using virtual machine checkpointing, because, as
we restore the virtual machine from a checkpoint, the fuzzer’s
internal states about the fuzzing progress would also get re-
stored and thus lost. By placing the fuzzer outside the vir-
tual machine, the fuzzer survives virtual machine restorations.
Moreover, the fuzzer is shielded against guest kernel crashes
and subsequent virtual machine reboots, limiting their impact
on the fuzzing progress.
The fuzzer interface is a fuzzer abstraction layer that hides
details about individual fuzzers from other components. A
new fuzzer can be added by implementing various callbacks
deﬁned in this interface. These callbacks are invoked by the
fuzzing driver, the core component of Agamotto placed in-
side the virtual machine monitor, which (i) drives the fuzzing
loop interacting with both the fuzzer as well as the guest vir-
tual machine, and (ii) creates and manages virtual machine
checkpoints. The guest agent, running inside the guest vir-
tual machine, provides the fuzzing driver with ﬁner-grained
virtual machine introspection capabilities. For example, as
the guest agent starts at boot, it notiﬁes the fuzzing driver of
the boot event, so that it can start the fuzzing loop.
Figure 2: Fuzzing loop comparison.
3.2 Fuzzing Loop
The fuzzing driver component of Agamotto drives the main
fuzzing loop. In each iteration of the fuzzing loop, a fuzzer
generates a single test case, executes it, and processes the
result of its execution as feedback. In fuzzing event-driven
systems such as OS kernels, each test case generated by the
fuzzer can be deﬁned as the sequence of actions it performs
on the target system. Formally, let S = {S0,S1, ...,SN} be the
set of states of the fuzzing target, and T be a fuzzer-generated
test case, which comprises a sequence of N actions, denoted
by an ordered set {a1,a2, ...,aN}. An execution of T, denoted
by a function exec(T), is a sequential execution of actions in
T on the fuzzing target. Each action ai ∈ T (for i ∈ {1, ...,N})
moves the state of the fuzzing target from Si−1 to Si.2 The
target state observed by the fuzzer (e.g., coverage) is denoted
by R = {R1,R2, ...,RN}, where each element Ri ⊂ Si is the
fuzzer-observed state of the fuzzing target after executing ai.
We use this notation throughout the paper.
Figure 2 depicts Agamotto’s fuzzing loop in comparison
with Syzkaller’s fuzzing loop using the above notation. The
differences are (i) the added ﬂows into checkpoint and restore
and (ii) the removed ﬂows into cleanup and reboot. Virtual
machine restoration is initiated after generating, but before
executing, a given test case. A checkpoint request is issued
and evaluated after each action of a test case. Agamotto skips
both cleanup and reboot, since a consistent virtual machine
2We use a transition-relation style of specifying concurrent, reactive
programs (e.g, an OS kernel) to incorporate non-determinism [37, 50]. In
other words, ai is a relation between Si−1 and Si, not a function.
2544    29th USENIX Security Symposium
USENIX Association
FuzzerFuzzer InterfaceFuzzing DriverDevice DriverGuest AgentGuest VMUser ModeKernel Mode+DA?FEJ5JH=CAVirtualMachineMonitorGenerateProcessCleanupGenerateExecuteProcessCleanupRestoreak23a4Checkpoint3bSkippedExecute352435…CrashCrashBoot1Tak+1Checkpoint1aakak+1Checkpoint3b……TRRTTBoot1Existing approach(Syzkaller)Our approach(Agamotto)state is always restored from a checkpoint without requiring
manual cleanup, even after a crash.
After the guest virtual machine boots, but before it starts
executing any test case ( 1 in Figure 2), the ﬁrst checkpoint,
which we call the root checkpoint, is created ( 1a ). Then, the
fuzzer generates a test case ( 2 ) and starts executing it ( 3 ).
Based on (i) the test case just generated and (ii) available
checkpoints, the fuzzer decides what checkpoint the test case
can start executing from and restores the virtual machine from
the chosen checkpoint ( 3a ). Initial parts of the test case, the
result of which is already contained in the checkpoint, are
skipped.
During the execution of a test case, secondary checkpoints
are requested and created according to a conﬁgurable check-
point policy. After executing each action, the test case exe-
cuting inside the guest virtual machine sends a checkpoint
request to the fuzzer ( 3b ). Then Agamotto’s checkpoint pol-
icy decides whether to checkpoint the virtual machine or not.
Once a test case has been executed, either successfully, with
a failure (e.g., timeout), or with a system failure (e.g., kernel
crash), the execution result (e.g., coverage) is sent to and
subsequently processed by the fuzzer ( 4 ). If a test case did
not execute in full, but only until kth action, ak, due to timeouts
or system failures, the result for only the executed parts of the
test case, {R1,R2, ...,Rk}, will be sent to the fuzzer.
Since restoring the virtual machine entails a full system
cleanup, Agamotto skips an explicit cleanup process, if any
( 5 ). To avoid inﬂuence between iterations, existing kernel
driver fuzzers either perform an explicit cleanup [24] or sim-
ply ignore the issue [18, 55]. Agamotto uses virtual machine
restoration, which does not allow any internal system state,
even corrupted or inconsistent ones created by kernel bugs
or panics, to transfer between iterations, without requiring
manually-written cleanup routines.
A bug may occur during the cleanup process that we skip.
However, potential bugs that arise in the cleanup process can
be found by actively fuzzing the cleanup routines. This way,
a cleanup routine can be tested more thoroughly, fully lever-
aging whatever smart fuzzing capabilities that the fuzzer pro-
vides. For example, a fuzzer may generate a corner test case
that calls, the cleanup routine multiple times in between other
actions, which may trigger more interesting and potentially
more dangerous behavior of the driver under test.
3.3 Checkpoint Store and Search
While the fuzzing loop is running, multiple checkpoints get
created, which we store in Agamotto’s checkpoint storage. To
reduce the overhead induced by processing QEMU’s snapshot
format we manually manage the (re)storing of guest and de-
vice memory pages and use memory-backed volatile storage
to capture the remaining virtual machine state.
The volatile state of a virtual machine comprises its CPU
and memory state, and any bookkeeping information about
Node Label
R
A
B
C
{}
{a1, a2}
{a1, a2, a3, a4}
{a1, a2, a5, a6, a7}
Figure 3: Checkpoint tree example.
the virtual machine such as device states kept by the virtual
machine monitor. A virtual machine checkpoint must contain
all the volatile information to be able to fully restore the state
of a virtual machine at a later point in time.
The state of a virtual machine upon each checkpoint re-
quest can be attributed to the executed part of the test case.
Therefore, we label each newly created checkpoint as the pre-
ﬁx of a test case that represents only the executed part of a
test case. That is, given a test case, T = {a1,a2, ...,aN}, the
checkpoint created after executing kth action is labeled as
T1..k = {a1,a2, ...,ak}.
Since the root checkpoint is requested when no part of
any test case has executed, it is labeled as an empty test case.
Checkpoints subsequently created are marked as a non-empty
test case. Checkpoints are stored in a preﬁx tree, which we
call a checkpoint tree. Each node in this tree represents a
checkpoint and is labeled as a preﬁx of the test case that was
executing when this checkpoint was created. An example
checkpoint tree is depicted in Figure 3.
The checkpoint tree forms an efﬁcient search tree of check-
points. After generating a new test case, Agamotto searches
for a checkpoint from which to restore the virtual machine.
To ﬁnd the checkpoint that saves the largest amount of time
in executing the test case, Agamotto traverses the checkpoint
tree searching for a node that has a label that matches the
longest preﬁx of the given test case. In Figure 3, given a test
case, T(cid:48) = {a1,a2,a7,a8}, for example, Agamotto ﬁnds the
node A , which has the label that matches the longest preﬁx,
{a1,a2}. Since the checkpoint tree is a preﬁx tree, this longest
preﬁx match can be performed efﬁciently without scanning
all the checkpoints stored in the tree.
The checkpoint tree also constitutes an incremental check-
point dependency graph when checkpoint storage is further
optimized with incremental checkpoints (see Section 3.5.1).
3.4 Checkpoint Management Policies
3.4.1 Checkpoint Creation Policy
Checkpointing is requested after executing each action in a
test case. A checkpoint creation policy decides, upon each
checkpoint request, whether to create a checkpoint or not.
A checkpoint creation policy should create checkpoints fre-
quently enough, to increase the chances of ﬁnding a check-
point in restoring the virtual machine later, thus saving time.
USENIX Association
29th USENIX Security Symposium    2545
RACB…3.4.2 Checkpoint Eviction Policy
Since the size of the checkpoint storage is limited, we cannot
store as many checkpoints as created by the checkpoint cre-
ation policy. A checkpoint eviction policy evicts an existing
checkpoint to free space for a newly created checkpoint when
the memory limit allocated for checkpoint storage is reached.
Given a conﬁgurable checkpoint pool size, checkpoints cre-
ated by the checkpoint creation policy are unconditionally
stored until there is no remaining space. If there is no available
space upon creation of a checkpoint, we consult checkpoint
eviction policies to ﬁnd a node to evict.
The goal of a checkpoint eviction policy is to keep a high
usage rate of the checkpoints in restoring a virtual machine.
A checkpoint eviction policy needs to predict what check-
points are likely to be used in the near future, to keep those
candidates in the checkpoint tree, and evict others.
We use multiple checkpoint eviction policies, which we
consult sequentially. Each policy takes a set of nodes in the
checkpoint tree as input and produces one or more candidate
nodes as output. If a policy produces more than one candidate
node, we consult the next policy using the output nodes of
the previous policy as its input. We continue consulting each
policy in the pipeline until it ﬁnds a single checkpoint node
to evict.
Policy-1: Non-Active. This policy is placed ﬁrst in the
pipeline, which prevents any active checkpoint nodes from
being evicted. Active checkpoint nodes in the checkpoint tree
include the node that the virtual machine is currently based
on, and, recursively, the parent node of an active node. This
policy selects all but the active nodes in the checkpoint tree
as eviction candidates, preventing any active node from being
evicted. We consider the checkpoints that are currently active
to be spatially close because they were created in executing
a single test case—the unit of fuzzing. This policy promotes
preserving the spatial locality between the active checkpoint
nodes by evicting others.
Policy-2: Last-Level. This policy selects the nodes in the
last level of the checkpoint tree as eviction candidates. As the
depth of the checkpoint tree increases, its nodes are labeled
with longer, more specialized test cases. The intuition behind
selecting last-level nodes as eviction candidates is that the
shorter the test case that a checkpoint node is labeled with,
the more likely the label matches test cases that the fuzzer
would generate in the future. By evicting last-level nodes, this
policy effectively balances the checkpoint tree, letting the tree
grow horizontally, rather than vertically.
Policy-3: Least-Recently-Used. The last policy in the
pipeline is the Least-Recently-Used (LRU) policy, a policy
widely known to be effective at managing different types
of caches such as CPU data and address translation caches.
Figure 4: Checkpoint creation policy enforcement example.
Checkpointing should not be too frequent, however, because
(i) the checkpointing operation itself adds a run-time overhead
and (ii) each newly created checkpoint adds memory pressure
to the checkpoint storage. Excessive creation of checkpoints,
whose expected gain is less than its cost, must be avoided. We
present two general checkpoint creation policies, which take
these two requirements into account.
Checkpointing at Increasing Intervals. This policy cre-
ates checkpoints at conﬁgurable intervals in the timeline of
the guest virtual machine. Upon each checkpoint request, we
measure the time elapsed since the last checkpoint, and, if
it exceeds the conﬁgured interval, a checkpoint is created.
The intervals can be conﬁgured to be constant, or dynami-
cally determined. We use an adaptive interval that increases
as the level of the last checkpoint node in the checkpoint tree
increases. In particular, we use an exponentially increasing
interval using two as the base; this means that the policy re-
quires a guest execution time twice as long as the one that was
required for the last checkpoint (see 1 and 2 in Figure 4).
The idea is to reduce the number of checkpoints created later
in time during a test case execution, thus alleviating the over-
head of checkpoint creation.
Disabling Checkpointing at First Mutation. This policy
targets feedback-guided mutational fuzzers, which generate
new test cases by mutating parts of older test cases in the
corpus. It is well-known that the great majority of mutations
do not produce a new feedback signal (e.g., coverage sig-
nal [41]), which means that a new test case is more likely to
be discarded than to be used for further mutation. Therefore,
the expected gain of checkpointing the execution of a test
case after the point of a new mutation is low. To reduce the
overhead of checkpointing, this policy restricts the creation of
checkpoints when executing a mutated test case. Speciﬁcally,
checkpointing is disabled starting from the location of the ﬁrst
mutation in each test case (see 3 in Figure 4). We do allow
checkpointing, however, at any point before the new mutation,
because the initial part of the test case still corresponds to a
preﬁx of some older test case in the corpus and is likely to
occur again as a base for new mutations.
2546    29th USENIX Security Symposium
USENIX Association
S0S1S2a2a1a3a′4S3S4t1t2=2·t1t3=2·t2T={a1,a2,a3,a4,a5}T′={a1,a2,a3,a′4,a5}S5a5(Test case in corpus)(Mutated test case)123MutateExecuteWe track the time each checkpoint was last used; we say a
checkpoint was used, (i) when it was created, or (ii) when the
virtual machine was restored from it. The policy evicts the
checkpoint used earliest in time. As widely known, an LRU
policy promotes the temporal locality present in the check-
point usage pattern. The more recently a checkpoint was used,
the more likely it will be used again. Unlike previous policies,
this LRU policy always determines one and only one eviction
candidate, because each checkpoint is used at a unique point
in time.
3.5 Lightweight Checkpoint and Restore
3.5.1 Incremental Checkpointing
QEMU’s default virtual machine snapshot mechanism stores
all volatile states of a virtual machine in a snapshot image.
Each snapshot can introduce prohibitive space overhead, how-
ever, the memory size of the virtual machine being the domi-
nating factor. Thus, this full snapshot mechanism is not suit-
able for the fuzzing use case, where a large number of virtual
machines are created, and their snapshots can quickly con-
sume all the available memory. Creating a full snapshot can
also introduce a prohibitively high run-time overhead for a
virtual machine with high memory requirements.
To reduce both space and run-time overheads of checkpoint-
ing, Agamotto performs incremental checkpointing, where