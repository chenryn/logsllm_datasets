forwarding plane which is convenient for network users, and
a physical architecture to realize RMT. We now describe
implementation design details.
We chose a 1GHz operating frequency for the switch chip
because at 64 ports × 10 Gb/s and an aggregate throughput
of 960M packets/s, a single pipeline can process all input
port data, serving all ports, whereas at lower frequencies
we would have to use multiple such pipelines, at additional
area expense. A block diagram of the switch IC is shown in
(a) L2/L3 switch.
(b) RCP and ACL support.
Figure 2: Switch conﬁguration examples.
Figure 3: Switch chip architecture.
Figure 3. Note that this closely resembles the RMT archi-
tectural diagram of Figure 1a.
Input signals are received by 64 channels of 10Gb SerDes
(serializer-deserializer) IO modules. 40G channels are made
by ganging together groups of four 10G ports. After pass-
ing through modules which perform low level signalling and
MAC functions like CRC generation/checking, input data is
processed by the parsers. We use 16 ingress parser blocks
instead of the single logical parser shown in Figure 1a be-
cause our programmable parser design can handle 40Gb of
bandwidth, either four 10G channels or a single 40G one.
Parsers accept packets where individual ﬁelds are in vari-
able locations, and output a ﬁxed 4 Kb packet header vector,
where each parsed ﬁeld is assigned a ﬁxed location. The lo-
cation is static, but conﬁgurable. Multiple copies of ﬁelds
(e.g., multiple MPLS tags or inner and outer IP ﬁelds) are
assigned unique locations in the packet header vector.
The input parser results are multiplexed into a single
stream to feed the match pipeline, consisting of 32 sequen-
tial match stages. A large shared buﬀer provides storage to
accommodate queuing delays due to output port oversub-
scription; storage is allocated to channels as required. De-
parsers recombine data from the packet header vector back
into each packet before storage in the common data buﬀer.
A queuing system is associated with the common data
buﬀer. The data buﬀer stores packet data, while pointers
to that data are kept in 2K queues per port. Each channel
in turn requests data from the common data buﬀer using
a conﬁgurable queuing policy. Next is an egress parser, an
egress match pipeline consisting of 32 match stages, and a
deparser, after which packet data is directed to the appro-
priate output port and driven oﬀ chip by 64 SerDes output
channels.
While a separate 32-stage egress processing pipeline seems
like overkill, we show that egress and ingress pipelines share
the same match tables so the costs are minimal. Further,
egress processing allows a multicast packet to be customized
(say for its congestion bit or MAC destination) by port with-
out storing several diﬀerent packet copies in the buﬀer. We
now describe each of the major components in the design.
IPv4EthernetENDENDIP routeEthertypeSrc MACDst MACAction: Set output portAction: Send to controllerAction: Set src/dst MAC,decrement IP TTL1232…Stage:TCAMRAMTable Flow GraphParse GraphMemory AllocationLegend{Ethertype}{RCP}{Dst IP}{Src Port, Src MAC}{Dst MAC}{Src/Dst IP,        IP Proto,        Src/Dst Port}TablesLogical ﬂowDrop packetForward to bufferTable Flow GraphIPv4RCPTCPUDPEthernetIP routeEthertypeSrc MACDst MACRCPACLAction: Set src/dst MAC, decrement IP TTL, insert OMPLS header (opt.), set src/dst IP (opt.)Action: Set queue IDAction: Clear output portAction: Update RCP rateAction: Set output port,insert OMPLS header (opt.)Action: Send to controller31321230…Stage:TCAMRAMTable Flow GraphParse GraphMemory Allocation...Input Ch. 1Input Ch. 64...Ingress DeparserMatchStage1Match Stage32...Ingress ParsersIngress processingCommon data bufferqueuespacket datapacket pointer(enqueue)packet pointer (dequeue)packet data...Egress DeparserMatchStage1Match Stage32...Egress ParsersEgress processing...Output Ch. 1Output Ch. 644.1 Conﬁgurable Parser
The parser accepts the incoming packet data and pro-
duces the 4K bit packet header vector as its output. Parsing
is directed by a user-supplied parse graph (e.g., Figure 2),
converted by an oﬄine algorithm into entries in a 256 entry
× 40b TCAM, which matches 32b of incoming packet data
and 8b of parser state. Note that this parser TCAM is com-
pletely separate from the match TCAMs used in each stage.
When, for example, the 16-bits of the Ethertype arrives, the
CAM matches on 16 of say 32 arriving bits (wildcarding the
rest), updating state indicating the next header type (e.g.,
VLAN or IP) to direct further parsing.
More generally, the result of a TCAM match triggers an
action, which updates the parser state, shifts the incoming
data a speciﬁed number of bytes, and directs the outputting
of one or more ﬁelds from positions in the input packet to
ﬁxed positions in the packet header vector. This loop re-
peats to parse each packet, as shown in Figure 4. The loop
was optimized by pulling critical update data, such as in-
put shift count and next parser state, out of the RAM into
TCAM output prioritization logic. The parser’s single cy-
cle loop matches ﬁelds at 32 Gb/s, translating to a much
higher throughput since not all ﬁelds need matching by the
parser. A 40 Gb/s packet stream is easily supported by a
single parser instance.
Figure 4: Programmable parser model.
4.2 Conﬁgurable Match Memories
Each match stage contains two 640b wide match units, a
TCAM for ternary matches, and an SRAM based hash ta-
ble for exact matches. The bitwidth of the SRAM hash unit
is aggregated from eight 80b subunits, while the ternary
table is composed of 16 40b TCAM subunits. These sub-
units can be run separately, in groups for wider widths, or
ganged together into deeper tables. An input crossbar sup-
plies match data to each subunit, which selects ﬁelds from
the 4Kb packet header vector. As described earlier (Fig-
ure 1b), tables in adjacent match stages can be combined to
make larger tables. In the limit, all 32 stages can create a
single table.
Further, the ingress and egress match pipelines of Fig-
ure 3 are actually the same physical block, shared at a ﬁne
grain between the ingress and egress threads as shown in Fig-
ure 1b. To make this work, ﬁrst the packet header vector is
shared between input and output vectors; each ﬁeld in the
vector is conﬁgured as being owned by either the ingress or
egress thread. Second, the corresponding function units for
each ﬁeld are allocated in the same way to ingress or egress.
Lastly, each memory block is allocated to either ingress or
egress. No contention issues arise since each ﬁeld and mem-
ory block is owned exclusively by either egress or ingress.
Each match stage has 106 RAM blocks of 1K entries ×
112b. The fraction of the RAM blocks assigned to match,
action, and statistics memory is conﬁgurable. Exact match
tables are implemented as Cuckoo hash tables [10, 26, 32]
with (at least) four ways of 1K entries each, each way requir-
ing one RAM block. Reads are deterministically performed
in one cycle, with all ways accessed in parallel. Each match
stage also has 16 TCAM blocks of 2K entries × 40b that
can be combined to make wider or deeper tables.
Associated with each match table RAM entry is a pointer
to action memory and an action size, a pointer to instruc-
tion memory, and a next table address. The action memory
contains the arguments (e.g., next hop information to be
used in output encapsulations), and the instructions spec-
ify the function to be performed (e.g., Add Header). Action
memory, like the match memory, is made of 8 narrower units
consisting of 1K words × 112 bits, yielding 96b data each
(along with ﬁeld valid bits and memory ECC—error correct-
ing code—bits). Action memory is allocated from the 106
RAM blocks, while action instructions are held in a separate
dedicated memory.
As in OpenFlow, our chip stores packet and byte statis-
tics counters for each ﬂow table entry. Full 64b versions of
these counters are contained in oﬀ-chip DRAM, with lim-
ited resolution counters on-chip using the 1K RAM blocks
and applying the LR(T) algorithm [33] to provide acceptable
DRAM update rates. One word of statistics memory conﬁg-
urably holds the counters for either two or three ﬂow entries,
allowing tradeoﬀs of statistics memory cost vs DRAM up-
date rate. Each counter increment requires a read and write
memory operation, but in the 1GHz pipeline only one op-
eration is available per packet, so a second memory port is
synthesized by adding one memory bank1.
4.3 Conﬁgurable Action Engine
A separate processing unit is provided for each packet
header ﬁeld (see Figure 1c), so that all may be modiﬁed
concurrently. There are 64, 96, and 64 words of 8, 16, and
32b respectively in the packet header vector, with an asso-
ciated valid bit for each. The units of smaller words can
be combined to execute a larger ﬁeld instruction, e.g., two
8b units can merge to operate on their data as a single 16b
ﬁeld. Each VLIW contains individual instruction ﬁelds for
each ﬁeld word.
OpenFlow speciﬁes simple actions, such as setting a ﬁeld
to a value, and complex operations, such as PBB encapsulate
and inner-to-outer or outer-to-inner TTL copies, where the
outer and inner ﬁelds may be one of a number of choices.
Complex modiﬁcations can be subroutines at low speeds, but
must be ﬂattened into single-cycle operations at our 1 GHz
clock rate using a carefully chosen instruction set.
Table 1 is a subset of our action instructions. Deposit-
byte enables depositing an arbitrary ﬁeld from anywhere in
a source word to anywhere in a background word. Rot-
mask-merge independently byte rotates two sources, then
merges them according to a byte mask, useful in performing
IPv6 to IPv4 address translation [18]. Bitmasked-set is use-
ful for selective metadata updates; it requires three sources:
the two sources to be merged and a bit mask. Move, like
other operators, will only move a source to a destination
if the source is valid, i.e., if that ﬁeld exists in the packet.
1S. Iyer. Memoir Systems. Private communication, Dec.
2010.
ParserTCAMAction RAMMatch indexState &header dataNext stateField locationsHeader IdentiﬁcationField ExtractionFieldsResultHeader dataPacket HeaderVectorToMatchEngineAnother generic optional conditionalization is destination
valid. The cond-move and cond-mux instructions are use-
ful for inner-to-outer and outer-to-inner ﬁeld copies, where
inner and outer ﬁelds are packet dependent. For example,
an inner-to-outer TTL copy to an MPLS tag may take the
TTL from an inner MPLS tag if it exists, or else from the IP
header. Shift, rotate, and ﬁeld length values generally come
from the instruction. One source operand selects ﬁelds from
the packet header vector, while the second source selects
from either the packet header vector or the action word.
Category
logical
shadd/sub
arith
deposit-byte
rot-mask-merge
bitmasked-set
move
cond-move
cond-mux
Description
and, or, xor, not, . . .
signed or unsigned shift
inc, dec, min, max
any length, source & dest oﬀset
IPv4 ↔ IPv6 translation uses
S1&S2 | S1&S3 ; metadata uses
if VS1 S1 → D
if VS2 &VS1 S1 → D
if VS2 S2 → D else if VS1 S1 → D
Table 1: Partial action instruction set.
(Si means source i; Vx means x is valid.)
A complex action, such as PBB, GRE, or VXLAN encap-
sulation, can be compiled into a single VLIW instruction and
thereafter considered a primitive. The ﬂexible data plane
processing allows operations which would otherwise require
implementation with network processors, FPGAs, or soft-
ware, at much higher cost and power at 640Gb/s.
4.4 Other Features
Reducing Latency by Dependency Analysis: One
can easily ensure correctness by requiring that physical match
stage I processes a packet header vector P only after stage
I − 1 completely ﬁnishes processing P . But this is overkill
in many cases and can severely increase latency. Key to
reducing latency is to identify three types of dependencies
between match tables in successive stages: match depen-
dencies, action dependencies and successor dependencies [2],
each described below.
Match dependencies occur when a match stage modiﬁes
a packet header ﬁeld and a subsequent stage matches upon
that ﬁeld. No overlap of execution is possible in this case
(Figure 5a). Action dependencies occur when a match stage
modiﬁes a packet header ﬁeld and a subsequent stage uses
that ﬁeld as an input to an action, for example, if one stage
sets a TTL ﬁeld and the next stage decrements the TTL.
Partial execution overlap is possible (Figure 5b). Successor
dependencies occur when the execution of a match stage is
predicated on the result of execution of a prior stage; the
prior stage execution may cause the successor stage to be
skipped. The successor stage can be run concurrently with
its predecessor (Figure 5c) if it is run speculatively and pred-
ication is resolved before side eﬀects are committed. Tables
with no dependencies between them can also be run con-
currently. The pipeline delays between successive stages are
statically conﬁgurable between the three options of Figure 5,
individually for the ingress and egress threads.
Multicast and ECMP: Multicast processing is split be-
tween ingress and egress. Ingress processing writes an out-
put port bit vector ﬁeld to specify outputs, and optionally,
(a) Match dependency.
(b) Action dependency.
(c) No dependency or successor dependency.
Figure 5: Match stage dependencies.
a tag for later matching and the number of copies routed to
each port. A single copy of each multicast packet is stored in
the data buﬀer, with multiple pointers placed in the queues.
Copies are created when the packet is injected into the egress
pipeline, where tables may match on the tag, the output
port, and a packet copy count to allow per-port modiﬁca-
tions. ECMP and uECMP processing is similar.
Meters and Stateful Tables: Meters measure and clas-
sify ﬂow rates of matching table entries, and can be used to
modify or drop packets. Meters are but one example of state-
ful tables, where an action modiﬁes state that is visible to
subsequent packets and can be used to modify them. State-
ful counters that can be arbitrarily incremented and reset.
They can be used to implement, for example, GRE sequence
numbers (that are incremented in each encapsulated packet)
and OAM [15, 25].
Consistent and atomic updates: To allow consistent
updates [34], version information is contained in table en-
tries, and a version ID ﬂows through the pipeline with each
packet, qualifying table matches by version compatibility.
5. EVALUATION
We characterize the cost of conﬁgurability in terms of the
increased area and power of our design relative to a con-
ventional less programmable switch chip. Our comparison
culminates in a comparison of total chip area and power in