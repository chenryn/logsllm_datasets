### True Knowledge and LRU Implementation
Implementing LRU (or an approximation of it) is feasible, even if not highly efficient or accurate. Figure 2b compares the average miss rates over the unsampled DSL trace (solid curves). In a well-provisioned network, the cache would be large enough to achieve a low miss rate. In this scenario, LRU performs nearly as well as OPT. For instance, with a cache size of 500K, LRU’s miss rate is only 0.3% higher than OPT’s. We also studied the fine-grained convergence behavior of OPT and LRU by measuring how quickly their miss rates stabilize. Our findings show that OPT’s miss rate stabilizes within approximately 120 seconds, while LRU converges almost as quickly, stabilizing within about 180 seconds. Given these results, it may be possible to design cache algorithms that outperform LRU on IP traffic, but it is unlikely that the performance of these schemes will be substantially better than LRU.

### Cache Size and Miss Rate
Figure 2b illustrates the cache miss rates as a function of cache size for both the DSL traces (solid) and the NetFlow traces (dotted). The NetFlow curve shows the average miss rate across all routers. To eliminate cold-start effects, we measured miss rates only after the cache had reached capacity. We found that, with the DSL (NetFlow) traces, LRU achieves a miss rate of 2.7% (4%) for a cache size of 100K, which is roughly 28% of the FIB size in conventional Internet routers. Based on these measurements, we suggest some rough guidelines for determining cache size: a cache size of 1M—less than 1/16 of the maximum number of unique /24 prefixes in theory and about 1/10 of the total number of /24 prefixes used in the Internet today—may maintain a miss rate of approximately 0.1%. For a target miss rate of 1%, a cache size of around 500K may suffice. Overall, caching uniclass prefixes enables a route cache that is roughly an order of magnitude smaller than its full address space.

### Impact of Routing Changes
Next, we evaluate the effect of network dynamics by manually injecting route changes into our traces and assessing how quickly caching algorithms converge after these changes. Two types of route-change events can affect cache contents. First, routing changes upstream of the router may alter the distribution of inbound traffic arriving at router interfaces, leading to an abrupt change in the working set. We simulate this (Figure 3a) by randomly switching to a different router’s NetFlow trace every hour while replaying against the same cache. While these events cause short-lived increases in miss rate, the spikes are roughly three times the average miss rate, and the miss rate stabilizes within a few hundred seconds after the changes. Second, failures downstream of the router may alter the set of available outbound routes, causing multiple cache entries to become invalid simultaneously. We emulate (Figure 3b) the failure of multiple randomly selected next hops by removing all cached entries associated with the given next hop upon its failure. In extreme scenarios where large numbers of next-hop routers fail, these spikes can be substantial, increasing to 15 times the original value. However, for smaller numbers of failures, the increase is much less significant.

### Related Work
Our paper is not the first to propose route caching. During the early 1990s, most Cisco routers were built with a route caching capability known as fast switching [2]. In these designs, packet forwarding decisions were made by invoking a user-level process that looked up a routing table (RIB) stored in slow memory. To boost lookup speeds, a route cache stored the results of recent lookups. Unfortunately, the large speed difference between the two lookup paths caused many packets to be kept in a buffer awaiting service in the slow path. Additionally, upon routing changes or link failures, large groups of cached routes were simultaneously invalidated, dramatically decreasing packet forwarding rates and increasing loss probability [16]. This limitation led to the abandonment of route-caching and motivated the evolution of today’s caching-free routers. Our findings of large bursts of consecutive misses support these earlier observations about the limitations of route caching. However, the “fall-back” scheme (explained in Section 2.1) ensures full line-rate forwarding even on cache misses by immediately sending traffic to an intermediary. Several recent works suggest that constructing a reliable and efficient traffic indirection system is possible [4, 5, 7], warranting a revisit to route caching.

There has also been research recognizing the difficulty of ensuring forwarding correctness when caching CIDR prefixes [12]. These approaches increase cache size and require a logarithmic searching algorithm even when prefixes are not overlapping. Recently, Iannone et al. studied the cost of route caching under the IETF LISP architecture [7] using traffic traces to and from a campus network [17]. Their analysis results are applicable to estimating caching behavior at a stub network’s egress router, whereas our results are suitable for understanding caching performance in a large ISP’s network, where route caching would be most beneficial. Moreover, although their study was based on caching CIDR prefixes, they did not address the problem of ensuring forwarding correctness with a subset of CIDR prefixes.

To understand how modern workloads change the performance of route caching, we compared our results with those of earlier studies. For example, in 1988, Feldmeier studied the performance of caching /24 prefixes on traces captured at a gateway connected to the ARPANET [13]. Partridge repeated a similar analysis in 1995 [18] and confirmed Feldmeier’s earlier findings. By comparing our results with Feldmeier’s, we observed some interesting trends. For instance, when targeting a high hit rate (larger than 98%), route caching on modern traces performs better than in earlier studies; achieving a hit rate of 98% today requires a cache size, normalized by the number of entire /24 prefixes in the traces, of 0.1 (i.e., 10%), whereas Feldmeier reported a normalized cache size of 0.23. Moreover, when targeting a lower hit rate than 98%, modern workloads are even more amenable to caching than 20 years ago. Specifically, when targeting a sub-95% hit rate, route caching today is an order of magnitude more efficient than it was 20 years ago. For example, for a hit rate of 95%, we found modern workloads required a normalized cache size of only 0.008, while Feldmeier reported 0.096. Traditionally, a sub-95% hit rate was not considered tolerable, but recent routing architectures that leverage the “fall-back” mechanism can easily tolerate such a rate.

### Conclusion
An increasing number of network architectures use route caching to achieve scalability. Evaluating the feasibility of these techniques requires rigorous assessment of the benefits and costs of caching. This paper revisits several earlier works from the late 1980s on route caching and evaluates the practicality of their techniques on modern workloads. To the best of our knowledge, this paper constitutes the first measurement study of route-caching performance in a large ISP network. Key observations from our study include: (i) Working set sizes are stable over time, allowing route caches to be provisioned with relatively little headroom; (ii) Working sets of routers in a single POP are very similar to one another, introducing the possibility of pre-populating a cache; (iii) Uni-class caching eliminates the complexity of longest-prefix matching and enables a cache using slower, cheaper memory; and (iv) Ensuring full line-rate forwarding upon cache misses is critical for the success of route caching. For future work, we plan to investigate theoretical models for the effects of sampling on estimating cache-miss rates.