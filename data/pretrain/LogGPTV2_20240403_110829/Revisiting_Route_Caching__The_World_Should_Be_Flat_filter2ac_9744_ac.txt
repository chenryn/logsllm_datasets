ture knowledge), whereas implementing LRU (or an approximation of it) is possible
– if not highly efﬁcient or accurate. Figure 2b compares average miss rates over the
unsampled DSL trace (solid curves). In a well-provisioned network, the cache would
be large enough to attain a low miss rate. In this scenario, LRU performs nearly as well
as OPT. For example, with a cache size of 500K, LRU’s miss rate is only 0.3%-point
higher than OPT’s miss rate. We also study the ﬁne-grained convergence behavior of
OPT and LRU by measuring how fast their miss rates stabilize. We ﬁnd that OPT’s miss
rate stabilizes within roughly 120 seconds, and that LRU converges almost as quickly,
stabilizing within roughly 180 seconds. Given these results, it may be possible to design
cache algorithms that outperform LRU on IP trafﬁc, but it is unlikely the performance
of these schemes will be substantially greater than that of LRU.
Cache size and miss rate: Figure 2b shows cache miss rates as a function of cache size
for both the DSL traces (solid) and the NetFlow traces (dotted). The NetFlow curve
shows average miss rate across all routers. Here we eliminated cold-start effects by
measuring miss rates values only after the cache has reached capacity. We found that,
with the DSL (NetFlow) traces, LRU attains a miss rate of 2.7% (4%) for a cache size
of 100K, which is roughly 28% of the FIB size in conventional Internet routers. Based
on the measured miss rates, we suggest some rough guidelines for determining a cache
10
C. Kim et al.
0.6
0.5
0.4
0.3
%
n
i
e
t
a
r
s
s
i
0.2
M
0.1
0
1
2
3
5
4
6
Time (hour)
(a)
5K
10K
50K
100K
500K
1M
8
10
4
3
2
1
0
0
%
n
i
e
t
a
r
s
s
i
M
 entire next-hop failure
 single next-hop failure
1
Time (hour)
2
(b)
3
Fig. 3. Effect of routing change, (a) Inbound change (NetFlow traces), (b) Outbound change (DSL
traces, cache size = 1M)
size: a cache size of 1M – which is less than 1/16 of the maximum number of unique
/24 preﬁxes in theory, and roughly 1/10 of the total number of /24 preﬁxes used in the
Internet today – may maintain a miss rate of roughly 0.1%. For a target miss rate of 1%,
a cache size of roughly 500K may sufﬁce. Overall, caching uni-class preﬁxes enables a
route cache that is roughly an order of magnitude smaller than its full address space.
Impact of routing changes: Next, we evaluate the effect of network dynamics by man-
ually injecting route changes into our traces, and evaluating how quickly caching algo-
rithms converge after these changes are introduced. Two different route-change events
can affect cache contents. First, routing changes upstream of the router may alter the
distribution of inbound trafﬁc arriving at router interfaces, leading to an abrupt change
in the working set. We simulate this (Figure 3a) by randomly switching to a different
router’s NetFlow trace every hour while replaying against the same cache. While these
events cause short-lived increases in miss rate, these spikes were roughly a factor of 3
above the average miss rate, and the miss rate stabilized in a few hundred seconds after
these changes. Second, failures downstream of the router may alter the set of available
outbound routes, causing multiple cache entries to become invalid simultaneously. We
emulate (Figure 3b) the failure of multiple randomly-selected next hops by removing
all cached entries associated with the given next hop upon its failure. Here, we ﬁnd that
for extreme scenarios where large numbers of next-hop routers fail, these spikes can be
fairly large, increasing to 15 times the original value. However, for smaller numbers of
failures, this value decreases substantially.
5 Related Work
Our paper is not the ﬁrst to propose route caching. In fact, during the early 1990s, most
Cisco routers were built with a route caching capability known as fast switching [2]. In
these designs, packet forwarding decisions were made by invoking a user-level process
that looks up a routing table (RIB) stored in slow memory. To boost lookup speeds,
a route cache stored the results of recent lookups. Unfortunately, the large speed dif-
ference between the two lookup paths caused many packets to be kept in a buffer
awaiting service in the slow path. In addition, upon routing changes or link failures,
large groups of cached routes were simultaneously invalidated, dramatically decreasing
Revisiting Route Caching: The World Should Be Flat
11
packet forwarding rate and increasing loss probability [16]. This limitation actually led
to the abandonment of route-caching and motivated the evolution of today’s caching-
free routers. Our ﬁndings of large bursts of consecutive misses support these earlier
observations about the limitation of route caching. However, the “fall-back” scheme
(explained in Section 2.1) ensures full line-rate forwarding even on cache misses by
immediately sending trafﬁc to an intermediary. Several recent works suggest that con-
structing a reliable and efﬁcient trafﬁc indirection system is possible [4, 5, 7] and thus
warrant revisiting route caching.
Also there has been research which recognized the difﬁculty of ensuring forwarding
correctness when caching CIDR preﬁxes [12]. These approaches increase cache size
and require a logarithmic searching algorithm even when preﬁxes are not overlapping.
Recently, Iannone et al. studied the cost of route caching under the IETF LISP archi-
tecture [7] using trafﬁc traces to and from a campus network [17]. Hence, their analysis
results are applicable to estimating caching behavior at a stub network’s egress router,
whereas our results are suitable to understand caching performance in a large ISP’s net-
work, where route caching would be most beneﬁcial. Moreover, although their study
was based on caching CIDR preﬁxes, they did not address the problem of ensuring
forwarding correctness with a subset of CIDR preﬁxes.
To understand how modern workloads change the performance of route caching,
we compared our results with those of earlier studies. For example, in 1988, Feld-
meier studied performance of caching /24 preﬁxes on traces captured at a gateway con-
nected to the ARPANET [13]. Partridge repeated a similar analysis to Feldmeier’s in
1995 [18] and conﬁrmed Feldmeier’s earlier ﬁndings. We compared our results against
Feldmeier’s to better understand how characteristics of Internet trafﬁc have changed for
the past 20 years. By comparing the cache size needed for a target hit rate with the
number of unique /24 preﬁxes seen in the trace, we observed some interesting results.
For example, when targeting a high hit rate (larger than 98%), route caching on modern
traces performs better than in these earlier studies; achieving a hit rate of 98% today
requires a cache size, normalized by the number of entire /24 preﬁxes in the traces, of
0.1 (i.e., 10%), whereas Feldmeier reported a normalized cache size of 0.23. Moreover,
when targeting a lower hit rate than 98%, modern workloads are even more amenable
to caching than 20 years ago. In particular, when targeting a sub-95% hit rate, route
caching today is an order of magnitude more efﬁcient than it was 20 years ago. For ex-
ample, for a hit rate of 95%, we found modern workloads required a normalized cache
size of only 0.008, while Feldmeier reported 0.096. Traditionally a sub-95% hit rate
was not considered to be tolerable, but recent routing architectures that leverage the
“fall-back” mechanism can easily tolerate such a rate.
6 Conclusion
An increasing number of network architectures make use of route caching to achieve
scalability. Evaluating the feasibility of these techniques requires rigorous evaluation
of the beneﬁts and costs of caching. This paper revisits several earlier works from the
late 1980s on route caching and evaluates the practicality of their techniques on modern
workloads. To the best of our knowledge, this paper constitutes the ﬁrst measurement
12
C. Kim et al.
study of route-caching performance in a large ISP network. Key observations from
our study are: (i) Working set sizes are stable over time, allowing route caches to be
provisioned with relatively little headroom; (ii) Working sets of routers in a single POP
are very similar to one another, introducing the possibility of pre-populating a cache;
(iii) Uni-class caching eliminates complexity of longest-preﬁx matching and enables a
cache using slower, cheaper memory; and (iv) Ensuring full line-rate forwarding upon
cache misses is critical for the success of route caching. For future work, we plan to
investigate theoretical models for the effects of sampling on estimating cache-miss rate.
References
1. Chang, E., Lu, B., Markhovsky, F.: RLDRAMs vs. CAMs/SRAMs: Part 1, http://www.
commsdesign.com/design_corner/OEG20030603S0007
2. How to Choose the Best Router Switching Path for Your Network. Cisco Systems (August
2005), http://www.cisco.com/warp/public/105/20.pdf
3. Partridge, C., Carvey, P., et al.: A 50-Gb/s IP router. IEEE/ACM Trans. Networking (1998)
4. Ballani, H., Francis, P., Cao, T., Wang, J.: Making Routers Last Longer with ViAggre. In:
Proc. NSDI (April 2009) (to appear)
5. Kim, C., Caesar, M., Rexford, J.: Floodless in SEATTLE: A Scalable Ethernet Architecture
for Large Enterprises. In: Proc. SIGCOMM (August 2008)
6. Caesar, M., Condie, T., Kannan, J., Lakshminarayanan, K., Stoica, I.: ROFL: Routing on Flat
Labels. In: Proc. ACM SIGCOMM (September 2006)
7. Farinacci, D., Fuller, V., Oran, D., Meyer, D., Brim, S.: Locator/ID Separation Protocol
(LISP). Internet-Draft (work in progress) (December 2008)
8. Andersen, D., Balakrishnan, H., Feamster, N., et al.: Accountable Internet Protocol (AIP).
In: Proc. ACM SIGCOMM (2008)
9. Chang, D., Govindan, R., Heidemann, J.: An empirical study of router response to large BGP
routing table load. In: Proc. Internet Measurement Workshop (2002)
10. Eatherton, W., Varghese, G., Dittia, Z.: Tree bitmap: Hardware/Software IP Lookups with
Incremental Updates. In: ACM Computer Communication Review (2004)
11. Sampled NetFlow, Cisco Systems, http://www.cisco.com/en/US/docs/ios/
12_0s/feature/guide/12s_sanf.html
12. Liu, H.: Routing Preﬁx Caching in Network Processor Design. In: Proc. International Con-
ference on Computer Communications and Networks (October 2001)
13. Feldmeier, D.: Improving Gateway Performance With a Routing-table Cache. In: Proc. IEEE
INFOCOM (1988)
14. Rexford, J., Wang, J., Xiao, Z., Zhang, Y.: BGP Routing Stability of Popular Destinations.
In: Proc. Internet Measurement Workshop (November 2002)
15. Jain, R.: Characteristics of Destination Address Locality in Computer Networks: A Compar-
ison of Caching Schemes. Computer Networks and ISDN 18, 243–254 (1989/1990)
16. McRobb, D.: Path and Round Trip Time Measurements (slides 19-21), http://www.
caida.org/publications/presentations/nanog9806/index.html
17. Iannone, L., Bonaventure, O.: On the Cost of Caching Locator/ID Mappings. In: Proc. ACM
CoNEXT (December 2007)
18. Partridge, C.: Locality and Route Caches
(1996), http://www.caida.org/
workshops/isma/9602/positions/partridge.html