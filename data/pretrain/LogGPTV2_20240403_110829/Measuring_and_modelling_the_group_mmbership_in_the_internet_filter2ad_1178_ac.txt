signiﬁcantly.
4.4 Pairwise Correlation in Group Participa-
tion
We study the time-based pairwise correlation of clusters or nodes.
For each group, we sample it and compute the participation proba-
bility for all clusters or nodes and the pairwise participation proba-
bility between any two clusters or nodes. Then, using Equation 1,
we calculate the matrix of correlation coefﬁcients. For net games,
we analyze the correlation between nodes directly, since most clus-
ters are trivially nodes.
We want to study the correlation of the clusters which is a matrix.
A large matrix with probabilities as values is not easy to visualize.
To overcome this, we plot the CDF of the correlation coefﬁcient of
clusters in Fig. 8 and Fig. 9 for both groups of data sets (MBONE
Figure 9: CDF of the correlation coefﬁcient of nodes for data
sets from net games (including 10 data sets).
The results for pairwise correlation are consistent with those for
participation probability: signiﬁcant correlation feature for MBONE
applications, while very weak correlation phenomena for net games.
MBONE: clusters exhibit strong pairwise correlation. This
further argues that the selection of users with ﬁxed equal probabil-
ity is not realistic for this kind of groups. Fig. 8 shows that most
of the correlation coefﬁcients between clusters are not 0 (in fact,
for both IETF video and audio data sets, only 1.5% of cluster pairs
have 0 correlation coefﬁcient; and for NASA data set, the percent-
age of 0 correlation coefﬁcient is 8%). This means that most of the
clusters are not independent2. Moreover, about 90% (for IETF data
sets) or 70% (for the NASA data set) of correlation coefﬁcients are
greater than 0 (positive correlation). This can be explained by the
fact that, in an IETF meeting or a NASA shuttle launch multicast-
ing, many members have very similar interests in speciﬁc sessions
and thus many clusters tend to be coupled together.
2It is easy to verify that, for any two variables which follow 0-1
distribution, if their coefﬁcient is 0 then they are independent, and
vise versa.
00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91CDFParticipation probabilityIETF audioIETF videoNASA00.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91CDFParticipation probability00.10.20.30.40.50.60.70.80.91-1-0.8-0.6-0.4-0.200.20.40.60.81CDFCorrelation CoefficientIETF audioIETF videoNASA00.10.20.30.40.50.60.70.80.91-1-0.8-0.6-0.4-0.200.20.40.60.81CDFCorrelation CoefficientQS-1QS-2QS-3QS-4QS-5QS-6QS-7QS-8QS-9QS-10Net games: pairwise user participation exhibits weak corre-
lation. In Fig. 9, we can see that some game servers have more
signiﬁcant correlation features than others. However, for all the
servers, more than 55% (up to 80% for some servers) of node-pairs
have correlation coefﬁcient as 0.
In other words, most pairs of
nodes are independent. The explanation for the difference between
MBONE applications and net games is very similar to the argu-
ments for the participation probability distribution. The number of
players in any single game is limited. In addition, repeated users do
not seem to want to join the same server as some other particular
player joins. Again, we can say that the simple uniformly random
membership model can describe the membership of net games and
the absence of pairwise correlation.
Note that we did not analyze the correlation and participation for
the cumulative data sets, since by their nature, they do not provide
sufﬁcient details to generate the required distributions.
4.5 Does Member Clustering Affect Skewed
Distribution and Pairwise Correlation?
In the above, we measured and analyzed the participation prob-
ability and correlation of clusters in stead of nodes for MBONE
application, and we observed skewed distribution and strong pair-
wise correlation. An interesting question would be: does member
clustering affect the observed properties? Or in other words, are
the new properties caused by our additional processing, i.e., mem-
ber clustering, or they come by the nature of the applications? In
this section, we show the analysis results of MBONE applications
without member clustering. That is, we plot the CDF curves of
participation probability and pairwise correlation of nodes (with-
out clustering). And the results are showed in Fig. 10 and Fig. 11.
Figure 10: CDF of the participation probability of nodes for
data sets from MBONE (only real data).
We can see that Fig. 10 and Fig. 11 have very similar curves
to Fig. 6 and Fig. 8, though the values are slightly different due
to the absence of member clustering. This conﬁrms that the the
properties of skewed distribution and strong pairwise correlation
come with the MBONE applications instead of the processing of
clustering (which does affect the values though).
Figure 11: CDF of the correlation coefﬁcient of nodes for data
sets from MBONE (real data).
bership distributions that conform to realistic distributions. In fact,
the distribution is an input parameter for our model.
5.1 An Overview of GEM
GEM considers all the group membership properties we dis-
cussed earlier: a) member clustering, b) group participation proba-
bility, and c) pairwise correlation in group participation. GEM has
the following inputs:
• A network topology
• Clustering method (that determines how to form clusters in
the given topology)
• Target group behavior: the distribution of group participation
probability, the pairwise correlation in group participation of
clusters, and the distribution of member cluster size (i.e., the
number of member nodes in a cluster)
GEM generates multicast groups whose members follow the given
distributions and constraints. Fig. 12 is a block diagram of GEM,
illustrating how GEM works at a high level.
GEM works in the following steps:
1. Cluster creation: GEM classify nodes into many disjoint
clusters using the speciﬁed clustering algorithm. Leaving
this as an input parameter gives GEM a lot of ﬂexibility and
the option to ignore clustering altogether as we discuss be-
low.
2. Membership distribution: GEM creates groups and chooses
their cluster members among the clusters in a randomized
fashion. The selection of clusters follows the given distribu-
tions for clusters participation and pairwise correlation. Note
that, in this paper, we also refer the clusters which are chosen
for a group as “member clusters” of the group.
3. Node selection: In each chosen cluster, GEM random selects
nodes based on member cluster size.
5. GEM: A GROUP MEMBERSHIP MODEL
From our measurement and analysis, we conclude that real mem-
bership distribution does not follow the simple uniform random dis-
tribution. We propose a comprehensive group membership model,
called GEM (GEneralized Membership model) to generate mem-
First, the model is not tied to a particular membership distribu-
tion. This makes the usefulness of the model extend beyond the
accuracy of a set of measurements. At the same time, our measure-
ments provide guidelines for the choice of realistic distributions. In
the tool we develop based on GEM, the measured distributions will
be provided as choices to the users ([1]).
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1CDFParticipation ProbabilityIETF AUDIOIETF VIDEONASA 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1-1-0.8-0.6-0.4-0.2 0 0.2 0.4 0.6 0.8 1CDFCorrelation CoefficientIETF AUDIOIETF VIDEONASAclusters by K random binary variables, (X1, X2, ..., Xi, ..., XK ),
where Xi represents the group participation of cluster Ci.
The generation of multicast groups reduces to generating vec-
tors x = (x1, x2, ..., xi, ..., xK ), which follow the given distri-
butions. Namely, we want to select vectors from the distribution
(X1, X2, ..., Xi, ..., XK ), which is deﬁned by:
• P (Xi = 1) = pi,∀i, which means that Xi follows the given
participation probability pi
• P (Xi = 1, Xj = 1) = pi,j, which means that for any two
variables Xi and Xj, they have joint distribution pi,j.
Note that the problem is in some sense under-deﬁned. Complete
knowledge of the distribution of (X1, X2, ..., Xi, ..., XK ) would
require us to know the probability of appearance for every of the
O(2K ) binary vectors. In other words, we would need O(2K ) val-
ues to be able to generate the desired distribution. We only have
partial information: our total input is O(K + K 2). Intuitively, we
need to make some assumptions to “ﬁll” the missing information.
Assuming Maximum Entropy. For the missing constraints, we
will assume that they have maximum entropy. Entropy is a measure
of randomness of a system, and it is the “opposite” of order. In
addition, nature tends to increase its entropy. A table with nicely
stacked papers in alphabetical order has high order (low entropy).
A wind from the window can shufﬂe the papers, which leads to
high entropy. It is unlikely that a subsequent wind will restack and
alphabetize the papers.
In our approach, we use entropy to replace the missing infor-
mation. Given an unconstrained choice, we will choose accord-
ing to the maximum Entropy (ME) [31]. This is a non-trivial but
solved problem in statistical analysis [31]. Let us denote p∗(x)
the Maximum Entropy distribution. Intuitively, we can see this as
a multidimensional problem with only a few constrained dimen-
sions. The ME distribution p∗(x) satisﬁes the constraints along the
speciﬁed dimensions, and it is as unstructured as possible in the
unconstrained dimensions. If we see entropy as lack of informa-
tion, the Maximum Entropy distribution represents all the “known
information” and nothing more than that. Our member cluster gen-
eration algorithm combines two “conﬂicting” forces: it maximizes
the entropy (randomness), while it tries to match given distribu-
tions. Note that it is possible to use a distribution with “any” en-
tropy. However, to compute any-entropy distribution is demanding.
Moreover, using maximum entropy is more meaningful: if we do
not know, we assume the distribution as random as possible.
5.2.2 Algorithms for member cluster generation
Our problem formulation describes the following three desired
distributions in the order of increasing constraints.
1. Uniform distribution without correlation: all clusters have
equal probability to join. This is the widely-used multicast
membership model.
2. Non-uniform distribution without correlation: participation
probability is higher for some clusters.
3. Non-uniform distribution with pairwise correlation: as above
plus some pairs of nodes appear more often together.
Note that for the ﬁrst two cases, it is not necessary to use a max-
imum entropy distribution, since there are no correlations among
clusters. Straightforward algorithms can be used to generate mem-
ber clusters as shown below.
Uniform distribution without correlation. In this case, pi = p,
and pi,j = pi × pj = p2 (or coef (i, j) = 0) for any i and j, where
Figure 12: An illustration of GEM.
Cluster and node level membership. Note that as described above,
GEM operates ﬁrst at the cluster level and then node level. First,
it generates groups treating a cluster as one entity. The reason for
doing this is to simulate the network-level clustering that we have
observed. After we identify the member clusters, then we assume
that inside a cluster we have a number of active participants ac-
cording to the measured distributions (that is member cluster size
distribution).
5.2 Member Distribution Generation
The core of our model is the selection of the member clusters.
The problem can be stated as follows: given a set of clusters, the
group participation probability of each cluster, and the pairwise
correlation between any two clusters, we want to generate sets of
member clusters, which follow the given distributions.
In other
words, if we generate many multicast groups, the measured distri-
butions should match the targets.
5.2.1 Problem Formulation
Let us start with the following deﬁnitions. We assume K clus-
ters: C1, C2, ..., Ci, ..., CK. Let us denote as pi the participation
probability of cluster Ci, that is, how often the Ci participates in a
multicast group. For any two clusters Ci and Cj, there is a correla-
tion coefﬁcient coef (i, j), where 1 ≤ i, j ≤ K. Based on pi, pj,
and coef (i, j), we can easily compute the joint probability pi,j as
shown in Equation 2 (derived from Equation 1).
pi,j = coef (i, j) ×ppi × (1 − pi) × pj × (1 − pj) + pi × pj.
(2)
As a result, we get a symmetric joint probability matrix Pm where
Pm(i, j) = pi,j when i 6= j and Pm(i, i) = pi.
A multicast group can be represented by a K-dimensional vector
of binary values x = (x1, x2, ..., xi, ..., xK ), where xi = 1 if and
only if cluster Ci is a member cluster of the group (else xi = 0).
Now, we can formalize the problem as follows. If we assume
many groups 3, we can model the participation distribution of the
3The presentation is easier when we talk about multiple groups, in
other words, the multiple group participation. If we have one group
we can talk about the time-based participation.
1. Create clusters in given topology2. Select clusters as member clustersAcoording to input distributions3. Choose nodes for each member clusterNetwork topologyGroup behaviorClustering methodDist. of member cluster sizeDist. of pairwise correlationDist. of participation prob.OutputsGEMInputsDesired number of multicast groups that follow the given distributions1 ≤ i, j ≤ K. Among the above three cases, this is the case with
maximum entropy, because there are almost no constraints: mem-
ber clusters are chosen uniformly among all clusters, and clusters
are independent of each other. The member cluster generation al-
gorithm is straightforward in this case, and it is described in Algo-
rithm 1.
Algorithm 1 Member Cluster Generation (Case 1)
Require: For K variables, X1, X2, ..., Xi, ..., XK, P (Xi =
1) = p, and P (Xi = 1, Xj = 1) = p2 (or coef (i, j) = 0),
where 1 ≤ i, j ≤ K. Notes: Xi represents the group partici-
pation of cluster Ci with values of 0 (not join) or 1 (join)
Ensure: A K-dimension vector, (x1, x2, ..., xi, ..., xK ), which
follows given distribution.
generate a random number between 0 and 1, let it be u
if u < p then
xi = 1 (cluster Ci joins multicast group)
xi = 0 (cluster Ci will not join multicast group)
1: for i = 1 to K do
2:
3:
4:
5:
6:
end if
7:
8: end for
else
Non-uniform distribution without correlation.
In this case,
pi,j = pi×pj (or coef (i, j) = 0) for any i, j, where 1 ≤ i, j ≤ K,
while pi is usually unequal between different clusters. Compared
with case 1, this case needs to consider non-uniform distribution,
that is, pi for different clusters. However, all the clusters are still
independent to each other. Thus, the member cluster generation
algorithm is still straightforward. It is described in Algorithm 2.
Algorithm 2 Member cluster Generation (Case 2)
Require: For K variables, X1, X2, ..., Xi, ..., XK, P (Xi =
1) = pi, and P (Xi = 1, Xj = 1) = pi × pj (or coef (i, j) =
0), where 1 ≤ i, j ≤ K. Notes: Xi represents the group par-
ticipation of cluster Ci with values of 0 (not join) or 1 (join)
Ensure: A K-dimension vector, (x1, x2, ..., xi, ..., xK ), which
follows given distribution.
generate a random number between 0 and 1, let it be u
if u < pi then
xi = 1 (cluster Ci joins multicast group)
xi = 0 (cluster Ci will not join multicast group)
1: for i = 1 to K do
2:
3:
4:
5:
6:
end if
7:
8: end for
else
Non-uniform distribution with pairwise correlation. We need
to consider pairwise correlation between any two clusters, which
means that pi,j = pi × pj, for 1 ≤ i, j ≤ K does not hold
necessarily (i.e. coef (i, j) 6= 0).
In this case, we have to cal-
culate the maximum entropy distribution p∗(x) which is subject
to the given constraints. Then, we use Gibbs Sampler ([24]) ap-
proach to sample it, i.e., to obtain instances of membership values,
(x1, x2, ..., xi, ..., xK ).
Calculating the Maximum Entropy distribution. Given the
constraints P (Xi = 1) = pi, and P (Xi = 1, Xj = 1) = pi,j,
where 1 ≤ i, j ≤ K, or in other words, given a probability matrix
Pm = [pi,j], the maximum entropy p∗(x) is the solution to the
following problem:
∗
p
(x) = arg max{−
p(x)logp(x)dx},
(3)
Z
subject to
Z
and
and
xixjp(x)dx = pi,j, when i 6= j,
Z
Z
xip(x)dx = pi,
p(x)dx = 1.
(4)
(5)
(6)
By Lagrange multipliers (an optimization technique), the solu-
tion for p∗(x) is:
∗
(x; Λ) = p
∗
p
(x1, x2, ..., xK ; Λ)
exp[− KX
λi,ixi − i=K,j=KX
i=1