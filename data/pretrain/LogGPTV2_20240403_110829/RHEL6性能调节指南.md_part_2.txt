:::
::: para
为完全利用横向延展性，红帽企业版 Linux
使用*分布式计算*的很多组件。可将组成分布式计算的技术分为三层：
:::
::: variablelist
[通讯]{.term}
:   ::: para
    横向延展需要同时（平行）执行很多任务。因此这些任务必须有[*进程间通讯*]{.emphasis}以便协调其工作。另外，采用横向延展的平台应该可以跨多个系统共享任务。
    :::
[存储]{.term}
:   ::: para
    本地磁盘存储不足以满足横向延展的要求。需要分布式或者共享存储，一个有可允许单一存储卷容量渐变增长的提取层，另外还有额外的新存储硬件。
    :::
[管理]{.term}
:   ::: para
    分布式计算最重要的任务是[*管理*]{.emphasis}层。这个管理层可协调所有软件和硬件组件，有效管理通讯、存储以及共享资源的使用。
    :::
:::
::: para
以下小节论述了每一层的详细技术。
:::
::: section
::: titlepage
## [⁠]{#ch-intro.html#distributed-systems-communication}1.3.1. 通讯 {.title}
:::
::: para
通讯层可保证数据传输，它由两部分组成：
:::
::: {.itemizedlist xmlns:d="http://docbook.org/ns/docbook"}
-   ::: para
    硬件
    :::
-   ::: para
    软件
    :::
:::
::: para
多系统进行通讯最简单（也最迅速）的方法是[*共享内存*]{.emphasis}。这样可以推导出类似内存读取/写入操作的用量。共享内存的带宽高，低延迟，且常规内存读取/写入操作成本低。
:::
::: para
::: {.title xmlns:d="http://docbook.org/ns/docbook"}
以太网
:::
计算机之间最常用的通讯是使用以太网。目前系统默认提供 *Gigabit
Ethernet*（GbE），且大多数服务器包括 2-4 个 Gigabit 以太网 GbE 端口。GbE
提供良好的带宽和延迟性能。这是目前使用的大多数分布式系统的基础。即使系统使用较快的网络硬件，一般也是使用
GbE 专门用于管理接口。
:::
::: para
::: {.title xmlns:d="http://docbook.org/ns/docbook"}
10GbE
:::
*Ten Gigabit Ethernet* (10GbE)
是在高端甚至一些中端服务器中迅速得以广泛使用。10GbE 提供比 GbE 快 10
倍的带宽。其主要优点之一是使用现代多核处理器，它可保证通讯和计算之间的平衡。您可以将使用
GbE 的单核系统与使用 10GbE 的八核系统进行比较。以这种方法使用，10GbE
对保持整体系统性能有特殊意义，并可以避免通讯瓶颈。
:::
::: para
遗憾的是，10GbE 很贵。虽然 10GbE NIC
的成本已经下降，但互联（特别是光纤）的价格仍然很高，且 10GbE
网络交换机的价格极为昂贵。我们可以期待在一定时间内价格可以下降，但 10GbE
现在是在服务器机房主干以及对性能至关重要的程序中使用最多的网卡。
:::
::: para
::: {.title xmlns:d="http://docbook.org/ns/docbook"}
Infiniband
:::
Infiniband 提供比 10GbE 更高的性能。除在以太网中使用 TCP/IP 和 UDP
网络连接外，Infiniband 还支持共享内存通讯。这就允许 Infiniband
通过*远程直接内存访问* (RDMA) 在系统间使用。
:::
::: para
使用 RDMA 可让 Infiniband 直接从系统中删除数据而无需负担 TCP/IP
或者插槽连接，继而减小延迟，这对有些程序是很重要的。
:::
::: para
Infiniband
最常用于*高性能技术计算*（HPTC）程序，此类程序要求使用高带宽、低延迟和低负担。很多超级计算程序都得益于此，提高性能的最佳方式是使用
Infiniband，而不是快速处理器或者更多内存。
:::
::: para
::: {.title xmlns:d="http://docbook.org/ns/docbook"}
RoCCE
:::
*使用以太网的 RDMA* (RoCCE) 通过 10GbE 基础设施实施 Infiniband
形式的通讯（包括 RDMA）。鉴于 10GbE
产品产量的增长带来的成本改善，我们有理由相信可能会在更大范围的系统和程序中使用更多的
RDMA 和 RoCCE。
:::
::: para
红帽公司在红帽企业版 Linux 6 中全面支持这些通讯方法。
:::
:::
::: section
::: titlepage
## [⁠]{#ch-intro.html#distributed-systems-storage}1.3.2. 存储 {.title}
:::
::: para
使用分布式计算的环境使用共享存储的多个实例。这可能代表以下两个含义之一：
:::
::: {.itemizedlist xmlns:d="http://docbook.org/ns/docbook"}
-   ::: para
    多系统在单一位置保存数据
    :::
-   ::: para
    存储单元（例如卷）由多个存储应用组成
    :::
:::
::: para
最熟悉的存储示例是挂载到系统中的本地磁盘驱动器。这对将所有程序都托管在一台主机中的
IT
操作很合适。但由于基础设施可能包括数十个乃至数百个系统，管理如此多的本地存储磁盘将变得困难且复杂。
:::
::: para
分布式存储添加了一层以便为业务规模减轻并实现自动存储硬件管理。多个系统共享少量存储实例可减少管理员需要进行管理的设备数量。
:::
::: para
将多个存储设备的存储容量强化到一个卷中对用户和管理员都有好处。此类分布式管理提供了存储池的提取层：用户看到的是单一存储单元，管理员可通过添加更多硬件很方便地增大该单元。有些启用分布式存储的技术也提供附加利益，比如故障切换以及多路径。
:::
::: para
::: {.title xmlns:d="http://docbook.org/ns/docbook"}
NFS
:::
*网络文件系统* (NFS) 可让多服务器或者用户通过 TCP 或者 UDP
挂载并使用远程存储的同一实例。NFS
一般用来保存由多个程序共享的数据。它还便于对大量数据的海量存储。
:::
::: para
::: {.title xmlns:d="http://docbook.org/ns/docbook"}
SAN
:::
*存储区网络* (SANs) 使用光纤或者 iSCSI
协议提供对存储的远程访问。光纤基础设施（比如光纤主机总线适配器、开关以及存储阵列）有高性能、高带宽和海量存储。SAN
根据处理分割存储，为系统升级提供可观的灵活性。
:::
::: para
SAN
的其他优点还有它们可为执行主要存储硬件管理任务提供管理环境。这些任务包括：
:::
::: {.itemizedlist xmlns:d="http://docbook.org/ns/docbook"}
-   ::: para
    控制对存储的访问
    :::
-   ::: para
    管理海量数据
    :::
-   ::: para
    供应系统
    :::
-   ::: para
    备份和复制数据
    :::
-   ::: para
    提取快照
    :::
-   ::: para
    支持系统故障切换
    :::
-   ::: para
    保证数据完整性
    :::
-   ::: para
    迁移数据
    :::
:::
::: para
::: {.title xmlns:d="http://docbook.org/ns/docbook"}
GFS2
:::
红帽*全局文件系统 2*（GFS2）提供一些特别定制的功能。GFS2
的基本功能是提供单一文件系统，其中包括同时读/写访问，集群中跨多个成员的共享。即使说该集群的每个成员都可以看到
GFS2 文件系统中"磁盘上"的完全相同的数据。
:::
::: para
GFS2 可让所有系统同时访问该"磁盘"。为维护数据完整性，GFS2
使用*分布式锁管理器*（DLM），它在具体位置一次只允许一个系统进行写入。
:::
::: para
GFS2 对故障切换程序最合适，因为那些程序要求高存储容积。
:::
::: para
有关 GFS2 的详情请参考*《全局文件系统
2》*。有关存储的常规信息请参考*《存储管理指南》*。您可以在
找到这两本手册。
:::
:::
::: section
::: titlepage
## [⁠]{#ch-intro.html#distributed-systems-fcoe}1.3.3. 聚合网络 {.title}
:::
::: para
通过网络通讯一般使用以太网进行，使用专用光纤 SAN
环境的存储流量。通常有专用网络或者串行链路进行系统管理，且甚至可能使用*心跳*管理
[⁠]{#ch-intro.html#idm140329713364800}[^\[2\]^](#ch-intro.html#ftn.idm140329713364800){.footnote
xmlns:d="http://docbook.org/ns/docbook"}。结果是在多网络中通常使用单一服务器。
:::
::: para
在每台服务器中提供多个连接费用高昂，且累赘，不容易管理。这样就增加了将所有连接整合到一台服务器中的需求。*使用以太网的光纤*
(FCoE) 和 *Internet SCSI* (iSCSI) 可以满足这个需要。
:::
::: para
::: {.title xmlns:d="http://docbook.org/ns/docbook"}
FCoE
:::
使用 FCoE，标准光纤命令和数据包可使用 10GbE
物理基础架构通过单一*聚合网卡*（CNA）传送。也可使用同一链接传输标准
TCP/IP 以太网流量以及光纤存储操作。FCoE
为多个逻辑网络/存储连接使用一个物理网卡（和一条线缆）。
:::
::: para
FCoE 有以下优点：
:::
::: variablelist
[连接数减少]{.term}
:   ::: para
    FCoE
    将每台服务器的网络连接减少了一半。您仍可以根据性能或者可用性目的选择使用多个连接，虽然单一连接就可以提供存储和网络连接。这对批萨盒服务器和刀片机都非常有用，因为它们的组件空间都有限。
    :::
[低成本]{.term}
:   ::: para
    减少的连接数立刻表现在线缆、开关以及其他联网设备的减少。以太网的历史也是规模经济的历史。网络成本急剧下降，因为市场上设备数量已从百万级上升到十亿级，明显的表现就是
    100Mb 以太网和 GB 以太网设备价格的下降。
    :::
    ::: para
    同样，10GbE 也变得比以前便宜很多。另外，因为已可将 CNA
    硬件整合到单一芯片中，广泛的使用也将增加其在市场中的数量，直接造成价格大幅下降。
    :::
:::
::: para
::: {.title xmlns:d="http://docbook.org/ns/docbook"}
iSCSI
:::
*Internet SCSI* (iSCSI) 是另一个聚合网络协议类型，它是 FCoE
的替换产品。与光纤相似，iSCSI 在网络中提供块级存储。但 iSCSI
不提供完整管理环境。iSCSI 比 FCoE
强的一点是提供更多光纤容量和灵活性，同时成本较低。
:::
:::
:::
::: footnotes
\
------------------------------------------------------------------------
::: {#ch-intro.html#ftn.idm140329767238832 .footnote}
::: para
[^\[1\]^](#ch-intro.html#idm140329767238832){.para}
红帽认证工程师，详情请参考
。
:::
:::
::: {#ch-intro.html#ftn.idm140329713364800 .footnote}
::: para
[^\[2\]^](#ch-intro.html#idm140329713364800){.para}
[*心跳*]{.emphasis}管理是在系统间交换信息以保证各个系统都正常工作。如果系统"失去心跳"，则假设其失败或者已关闭，并使用另一个系统替换。
:::
:::
:::
:::
[]{#main-specs.html}
::: chapter
::: titlepage