title:NISAN: network information service for anonymization networks
author:Andriy Panchenko and
Stefan Richter and
Arne Rache
NISAN: Network Information Service for Anonymization
Networks
Andriy Panchenko
Computer Science dept.
RWTH Aachen University
D-52074 Aachen, Germany
panchenko@cs.rwth-
aachen.de
Stefan Richter
Computer Science dept.
RWTH Aachen University
D-52074 Aachen, Germany
richter@cs.rwth-
aachen.de
Arne Rache
Computer Science dept.
RWTH Aachen University
D-52074 Aachen, Germany
arne.rache@rwth-
aachen.de
ABSTRACT
Network information distribution is a fundamental service for any
anonymization network. Even though anonymization and infor-
mation distribution about the network are two orthogonal issues,
the design of the distribution service has a direct impact on the
anonymization. Requiring each node to know about all other nodes
in the network (as in Tor and AN.ON – the most popular anonymiza-
tion networks) limits scalability and offers a playground for inter-
section attacks. The distributed designs existing so far fail to meet
security requirements and have therefore not been accepted in real
networks.
In this paper, we combine probabilistic analysis and simulation
to explore DHT-based approaches for distributing network infor-
mation in anonymization networks. Based on our ﬁndings we in-
troduce NISAN, a novel approach that tries to scalably overcome
known security problems. It allows for selecting nodes uniformly
at random from the full set of all available peers, while each of the
nodes has only limited knowledge about the network. We show
that our scheme has properties similar to a centralized directory in
terms of preventing malicious nodes from biasing the path selec-
tion. This is done, however, without requiring to trust any third
party. At the same time our approach provides high scalability and
adequate performance. Additionally, we analyze different design
choices and come up with diverse proposals depending on the at-
tacker model. The proposed combination of security, scalability,
and simplicity, to the best of our knowledge, is not available in any
other existing network information distribution system.
Categories and Subject Descriptors
C.2.4 [Computer-Communication Networks]: Distributed Sys-
tems; C.2.0 [Computer-Communication Networks]: General—
Security and protection
General Terms
Security
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
CCS’09, November 9–13, 2009, Chicago, Illinois, USA.
Copyright 2009 ACM 978-1-60558-352-5/09/11 ...$10.00.
Keywords
Anonymous Communication, Peer-to-Peer, DHT, Node Lookup,
Privacy
1.
INTRODUCTION
While cryptography can be used to protect integrity and conﬁ-
dentiality of the data part of packets, everyone along their route can
still observe the addresses of the communication parties. Anony-
mous communication deals with hiding relationships between com-
municating parties.
Many approaches have been proposed in order to provide pro-
tection on the network layer. Still, only some of them have been
implemented in practice, e.g. [8, 3, 19]. The most popular and
widespread system is Tor [8]. The Tor network is a circuit switched,
low-latency anonymizing network for preserving privacy on the
network layer. It is an implementation of the so-called onion rout-
ing technology [22], that is based on routing TCP streams through
randomly chosen paths in a network of routers using layered en-
cryption and decryption of the content. The number of servers in
the network is currently about two thousand whereas the number of
users is estimated to be hundreds of thousands [18, 24].
In order to discover the network information, most systems make
use of a centralized directory service and assume that each node has
to know all other nodes in the network. This has several advantages,
such as making it hard for an attacker to poison the directory with
forged entities, mount the eclipse attack [21], or bias path selec-
tion. However, this approach has scalability problems and requires
trusting the directory. In Tor, for example, clients choose paths for
creating circuits by selecting three suitable servers from a list of
all currently available routers, the so-called directory. To this end,
certain trusted nodes act as directory servers and provide signed
documents that are downloaded by users periodically via HTTP.
Such a network status document contains router descriptors of all
currently known servers, including several ﬂags that are used to de-
scribe their current state. Due to scalability issues, there have been
changes in the original protocol and its successors, so that Tor is
now already using the third version of the directory protocol. In
the design document, the authors admit that requiring each node to
know all the others will not work out in the long run because of
scalability issues [7]. On that account, we strive to make network
information scale by carefully distributing the task.
2. RELATED WORKS
The Tarzan [9] P2P network requires every peer in the network to
know all the others. To achieve this, the authors use a simple gos-
siping protocol to ﬁnd every other peer in the network (the protocol,
141though, is only described on a high level). For the initial join to the
network a Tarzan peer needs to know at least a few other peers.
The approach is believed to not scale beyond 10.000 nodes [17].
In contrast to this, MorphMix [20] requires nodes to know only a
few neighbors. For the circuit setup so-called witness nodes are
used to facilitate the selection of nodes for circuit extension; this
is a unique feature in MorphMix, as in most other networks the
users choose the path themselves for security reasons.
In order
to alleviate possible security issues arising from this feature, Mor-
phMix uses a collusion detection mechanism, detecting malicious
nodes that misbehave by offering other colluding1 nodes for traver-
sal. However, due to the vulnerability of their collusion detection
mechanism [23], this approach does not have direct practical im-
pact on new designs any more.
Salsa [17] is a DHT which has been speciﬁcally developed for
anonymization networks.
Identities are based on hashes of the
nodes’ IP addresses and are organized in a tree structure. Redun-
dancy and bounds checking are used while doing lookups in order
to prevent malicious nodes from returning false information. Ac-
cording to simulations, the scheme prevents attackers from biasing
the path selection as long as the fraction of malicious nodes in the
system does not exceed 20%. However, further analysis [15] has
shown that even in this case Salsa is less secure than previously
thought: with the number of corrupt nodes below 20%, still more
than a quarter of all circuits are compromised because of infor-
mation leakage. Because defending against active attacks requires
higher redundancy, this directly increases the threat of passive at-
tacks since a small fraction of malicious nodes can observe a signif-
icant part of the lookups. This compromises anonymity by leaking
information about the initiator and the nodes involved in a tunnel.
Castro et al. [5] consider security issues in structured P2P net-
works. The attacker model consists of a fraction of colluding nodes.
To thwart the attacks, the authors propose three techniques: (i) se-
cure assignment of node identiﬁers, (ii) secure routing table main-
tenance, and (iii) secure message forwarding. They show that the
overhead of their technique is proportional to the fraction of ma-
licious nodes and is efﬁcient if the attacker controls up to 25% of
nodes in the system.
Their approach is not applicable in our scenario because of the
following reasons:
recursive queries, a centralized trusted third
party, and information leakage. The use of recursive queries is not
practical because it allows an attacker to execute DoS attacks with
very small effort. It has been shown by Borisov et al. [4] that DoS
attacks applied in anonymous communications considerably reduce
the provided anonymity. That is why there is a strong need to as-
sure reliability against adversaries, not just against random failures.
Additionally, a client in the approach of Castro has the possibility
to check only the ﬁnal result of the query, while not being able to
control and inﬂuence intermediate steps. Moreover, the need of a
trusted third party hardens practical realization of the approach. Fi-
nally, information leakage [15] enables bridging and ﬁngerprinting
attacks [6].
Baumgart and Mies [2] propose several improvements for Kadem-
lia [13] in order to make it more resilient against attacks. The
improvements are threefold: using crypto puzzles for restricting
node ID generation; sibling broadcast for ensuring replicated data
storage; using multiple disjoint paths for node lookups. The most
signiﬁcant proposal is the secure node ID generation, since the lat-
ter two are merely reﬁnements of mechanisms already existing in
Kademlia. The crypto puzzles restrict the node ID generation in the
sense that it is computationally expensive to generate valid node
1We use colluding and malicious w.r.t. nodes as synonyms in this
paper.
IDs. However, the adversary is free to generate valid IDs ofﬂine
without any time bounds before actually joining and subverting the
network. Moreover, in Section 4 we prove that the security prop-
erties of the latter three approaches cannot be upheld with growing
network size.
Westermann et al. [25] study default Kademlia behavior w.r.t. col-
luding nodes that answer queries with malicious nodes closest to
the searched-for ID. Their ﬁnding is that, due to redundancy, the
fraction of malicious nodes found in queries is not signiﬁcantly
larger than the overall fraction of malicious nodes in the system.
The simulations, however, have only been conducted up to 50,000
nodes.
Kapadia et al. [10] propose a method for performing redundant
searches over a Chord-based DHT. In a network of 10,000 their ap-
proach is able to tolerate up to 12% colluding nodes while applying
a non-recursive search.
Recently, Awerbuch and Scheiderler [1] have published a theo-
retical approach to building a scalable and secure DHT that might
potentially solve many of the problems we are facing in this ap-
plication. A multitude of active attacks are actually proven to be
impossible on this structure, even though passive information leak-
age attacks are not considered. Unfortunately, the downside of the
formal approach taken is that the high level of vantage does not
allow for instant practical implementation. In fact, the authors “be-
lieve that designing such protocols is possible though their design
and formal correctness proofs may require a signiﬁcant effort” [1].
Even if this belief is correct, it is foreseeable that it would lead to
a very complex system, hard to analyse and implement correctly,
while in the networking/anonymity community we observe a trend
towards security by simplicity.
3. ATTACKER MODEL
We consider a local attacker with the following capabilities:
• Passively observe some portion of network trafﬁc;
• Actively operate its own nodes or compromise some fraction
of honest nodes;
• Actively delete, modify and generate messages.
Further we assume that the adversary cannot break cryptographic
primitives. This is a standard assumption in the area of anonymous
communication.
4. WHY REDUNDANCY THROUGH INDE-
PENDENT PATHS DOES NOT SCALE
One basic idea for ﬁnding a random node in a DHT is choosing
a random number x in the node identity hash space and then use the
DHT to look up the owner of x. Most often this is the node whose
identity is closest to x in the distance metric of the DHT. This idea
lies at the heart of approaches like Salsa [17] and AP3 [14], and
its employment opens up the possibility of using any DHT as a
building block that deems to ﬁt for the purpose.
In this paper, we will follow this basic approach while analyzing
possible attacks and giving remedies in a cumulative fashion. A
naïve implementation of searching for x in an environment with a
fraction f of collaborating adversarial nodes has success probabil-
ity ps = (1− f )l, where l is the length of the search path, when we
assume the adversary to either simply drop requests in a denial of
service fashion or return false information such as claiming to be
the owner of x. Because l has to grow larger with the size of the
network, typically on the order of log(n) [12], this success proba-
bility approaches zero for growing networks asymptotically, when
142f is, say, constant. Thus, we can say that, under this simple attack,
the naïve implementation does not scale.
Of course, earlier scholarship [17, 5, 2] has recognized this prob-
lem. As a solution, redundancy in the form of several search paths
has been introduced without fail. The cited papers all try to ensure
routing towards x using multiple, preferably independent, paths.
This is itself a difﬁcult proposition, because the structure of DHTs
usually leads to path convergence in a lot of cases, and quite some
auxiliary constructions have been taken to still ensure indepen-
dence. Yet, even when we assume independent paths, the redun-
dancy required endangers scalability in the limit:
THEOREM 1. The number of paths required to reach constant
∈ nΩ(1) for l ∈ Ω(log n)
ps
success probability ps is at least
and f constant.
(1− f )l
PROOF. Let Ii be a random indicator variable that takes the value
1 if path i is attacker-free, and 0 otherwise. A single path i is free
from attackers with probability (1− f )l, thus E(Ii) = (1− f )l. Then
∑α
i=1 Ii is a variable that counts the number of successes out of α
paths, and
ps = P[
α∑
i=1
Ii ≥ 1] ≤ E(
α∑
i=1
Ii) =
α∑
i=1
E(Ii) = α(1− f )l
by the Markov inequality and linearity of expectation. Notice that
we have not made any assumptions about path independence.
Solving for α, substituting Ω(log n) for l and assuming f , ps con-
stant yields
α ≥
ps
(1− f )l
∈ (1/(1− f ))Ω(log n) = n
Ω(1).
With simple greedy routing, the lower bound of Ω(log n) path
length is valid for a large class of small world networks, including
all the DHTs we know of, as well as skip graphs [12]. Therefore,
the theorem shows that the number of independent paths needed to
route successfully with some ﬁxed probability grows at least poly-
nomially in n. This is exponentially greater than, e.g., the number
of neighbors a node has in the most common DHTs. Thus, even
in the ﬁrst step of routing, we rapidly run out of independent pos-
sibilities to choose from with growing networks. The aforemen-
tioned research efforts [17, 5, 2, 25] evaluated their approaches ex-
perimentally in environments between 10,000 and 100,000 nodes.
While their results suggest practicability in this range, the above
considerations show that we cannot expect this to hold up when
the networks grow larger. Moreover, our simulation of [25] sug-
gests that already starting from 40,000 peers their approach fails
due to almost double fraction of found colluding nodes by random
look-ups compared to the overall fraction of colluding nodes in the
system.
One might argue that a constant fraction of collaborating nodes
assumes a very strong adversary, and there have been results that
only work with fewer adversarial nodes [11], yet there are multiple
reasons why we consider this adversarial model here: Firstly, most
practical suggestions to solving the problem have assumed ﬁxed
percentages of malicious nodes [17, 5, 2]. Secondly, real world
phenomena like botnets suggest that we might have to deal with
strong attacks like this.
Thirdly, as we will show in the next section, we can do better.
Instead of striving to make paths independent, we want them to
work together in order to reach x more reliably.
5. NISAN
In this section we describe our approach. It consists of several
steps. We begin with a simple Chord-like DHT, and start building
protection measures on top of it in order to reach the required prop-
erties. Chord [16] has been chosen because of its simplicity, and,
most notably, because its ﬁnger table entries are deterministic in the
sense that there is exactly one correct neighbor for each ﬁnger in a
given DHT. Moreover, since the Chord distance metric is directed,
there is asymmetry in the sense that a node does not usually belong
in the ﬁnger tables of its neighbors. We will use the ﬁrst property
to restrict malicious behavior in Section 5.3, and the second one
against a stronger adversarial model in Section 6.3. Still, both our
protection measures and scalability bounds, especially outside of
these two sections, carry over to a very generic class of DHTs. For
example, the results in the following section have serious implica-
tions for the security of Kademlia [13]. We do, however, decid-
edly prefer iterative search over recursive search, because, among
other reasons, ﬁrstly we will use the added control we gain over
the course of the search in our protection measures, and secondly,
we consider the increased potential for denial-of-service attacks in
recursive approaches as a too big threat to ignore.
5.1 Better Redundancy: Aggregated Greedy
Search
Most of the approaches studied before make use of redundant
independent lookups. This leads to a convergence on many paths.
Therefore we follow another approach: instead of making indepen-
dent lookups, we propose to use an aggregated search which com-
bines the knowledge available on each of the independent branches.
We call this aggregated greedy search.
It proceeds as follows. To ﬁnd a random node, the searcher v
generates a random ID x. At this point we assume that node IDs are
uniformly distributed in the ID space (another case is considered in
Section 6.3). In each round v chooses the α nodes closest to x that
he is aware of and queries them for x. The search terminates when
after one iteration the list of α closest peers2 has not been changed.
The owner of x (the peer which is closest to the searched ID) is the
result of the query.
Interestingly, this description almost ﬁts the behavior described
in the Kademlia speciﬁcation [13]. And indeed, it has been demon-
strated [25] that Kademlia, and thus aggregated greedy search, works
well against an active adversary in networks of up to 50,000 nodes.
Unfortunately, we believe that these results can be explained by
the overwhelming redundancy employed for a relatively small net-
work. This is because the following theorem shows that aggregated
greedy search, on its own, does not scale.
THEOREM 2. Let α be an upper bound to the number of nodes
queried in every round, β the maximum number of neighbors ac-
cepted from any one queried node, and f the fraction of corrupted
nodes, a constant. Then, as long as (αβ)O(logα) ⊆ o(n), there is an
attack that makes the success probability of the search approach 0
in the limit. For example, this holds when α,β ∈ O(logn).
PROOF. The attacker proceeds by returning as many different
corrupt nodes as possible, in the order of proximity to the search
goal x, until only corrupted nodes are queried. If this attack suc-
ceeds, the attacker then has complete control over the course of the
search.
Let Bk ={b1, . . . ,b|Bk|} be the set of corrupt nodes that the search-
ing node v knows after k rounds, and say that the bi are ordered by
2We use peer and node as synonyms in this paper.
143proximity to x, that is, ∀1 ≤ i ≤ j ≤ |Bk|, d(x,bi) ≤ d(x,b j). No-
tice that the attacker, by returning the collaborators closest to x ﬁrst,
can make sure that this order is static during the whole search.
Let us ﬁrst ﬁnd an upper bound to the number of rounds k re-
quired such that |Bk| > α with high probability. Of course, this
presupposes that f n, the number of colluded nodes, exceeds α in
the ﬁrst place, a trivial side condition that is guaranteed in the long
run by the assumption (αβ)O(logα) ⊆ o(n).
W.h.p., at least one corrupted node is queried within the ﬁrst
round, if either α or β is in ω(1). Let us assume this for the moment.
Moreover, without loss of generality β ≥ 2, because for β = 1, ag-
gregated greedy search degenerates to simple greedy search with
redundancy α, and Theorem 1 can be applied to yield the claim,
because (αβ)O(logα) ⊆ o(n) implies α /∈ nΩ(1).
Let us further assume that in this phase, every colluded node
that v learns of will be queried, since v does not yet encounter any
honest nodes closer to x than bα. We will justify this in short. It is
then easy to see by induction that in each following round, |Bk| at
least doubles, because every corrupted node will return at least two
more hitherto unknown corrupted nodes. Thus, |Bk| > α for some
k ∈ O(logα) with high probability. Of course, this is a very weak
bound, but it will sufﬁce for our needs here.
During these k rounds, v will learn of at most (αβ)O(logα) nodes
altogether. Assuming randomly distributed nodes (both honest and
corrupted), the size of the set Y of honest nodes closer to x than
bα is Pascal distributed with parameters α and f , and the expected
value is E(|Y|) = α(1/ f − 1). Each of these nodes has probability
(αβ)O(logα)/n of being known to v before the attacker can make
v query only corrupt nodes after k rounds. As in Theorem 1, we
deﬁne indicator variables Iy that indicate this event for every y ∈ Y .
Then, the expected total number of nodes in Y that v learns of in
time k is
E(∑
y∈Y
E(Iy)) = E(|Y|)E(Iy).
Iy) = E(E(∑
y∈Y
Iy)) = E(∑
y∈Y
Employing the Markov inequality just like in Theorem 1 shows
that this is an upper bound to the probability that v gets to know
any node that is closer than bα before the attacker can make him
query only corrupt nodes: