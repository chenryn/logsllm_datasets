spaces of the players with ﬂoating point numbers.
6 The higher the privacy level from where the BR dynamics
starts, the bigger the interval of weight ratios in which case it
converges to 1 (i.e., no collaboration).
Together or Alone: The Price of Privacy in Collaborative Learning
13
Based on empirical results, we establish a heuristic
formula which minimizes the error of this local approx-
imation based on the size and density of the players’
datasets. The formula can be seen in Eq. (12) where d is
the density of the datasets and Dn is player n’s dataset.
The details of the related experiment can be found in
App. E. We refer to the true privacy-accuracy trade-oﬀ
n and our approximation via self-division
function as ΦM
100 000 ≈ d · |Dn| = |Dn|2
|Un| · |I|
as gΦM
(12)
n .
8.2 The Whole Process Before
Collaboration
In this section we show how our game theoretic model
can be used from scratch. That is, the players have two
datasets and they would like to know whether to train
together and with what privacy parameter. These ques-
tions can be answered with the help of the CoL game,
but ﬁrst its parameters must be established. A process
diagram describing the whole process is presented in
Fig. 8.
Fig. 8. The process diagram which describes what are the steps
of the players before collaboration
–
Initialization: The players have their datasets Dn
with the corresponding privacy policies Pn and the
accuracy they achieve by training alone θn.
– Setup: Based on the size of the datasets |Dn| and
the privacy policies Pn, the players determine which
privacy preserving method M to use, what beneﬁt b
and cost c function to apply, and what should be the
corresponding weight parameters C1, B1, C2, B2.
– Split: Based on the dataset size ratio |Dn|/|Dm|,
the players split their own datasets into two
Dn1, Dn2 to mimic the original datasets.
– Approximation: Based on the newly created
datasets Dn1, Dn2 and the accuracy obtained by
training alone θn, the players approximate the ac-
curacy improvement of training together fΦn.
trade-oﬀ function fΦn and M, b, c, C1, B1, C2, B2 de-
– Game: Using the approximated privacy-accuracy
termined by the setup phase, the players determine
the NE (ε∗
2)n and its corresponding Price of Pri-
vacy value via the CoL game.
1, ε∗
(i.e., ∀n ∈ {1, 2} :fun(ε∗
– Collaboration: If the approximation suggest that
training together is beneﬁcial for both participant
2) > 0), than they collab-
orate using their dataset Dn with the approximated
optimal privacy parameter ε∗
n.
1, ε∗
As we focused mainly on the “Game” step and partially
on the “Approximation” step, we manually chose the
parameters determined by the “Setup” phase. In this ex-
ample we choose bDP as privacy-preserving mechanism
M. We assume that players have a chunk of the pre-
processed NF rating dataset, which contains only movie
ratings. As such, it is expected that the players value
privacy less than accuracy: for the sake of this example,
we set B1 = B2 = 1 and C1 = C2 = 0.1. We use the
beneﬁt and privacy loss functions deﬁned in Eq. (4).
As we argued previously, self-division is the most
punctual when Eq. (12) holds. Since the density of the
original NF dataset is d ≈ 0.01, we assign both players
datasets with 10 million ratings: we randomly choose
20% of the users from NF10 and assign them to either
one of the players.
The players separately approximate Φn by self-
same. The exact values of these approximations can be
seen in App. F together with the true value of Φn.7 We
division, therefore, fΦ1 and fΦ2 are not necessarily the
found that the RMSE of fΦn is around 0.001 for both
Using fΦ1 Player 1 approximates
(ep∗
1,ep∗
clusion via fΦ2. This means Price of Privacy is zero.
The approximated utilities are eu1 = 0.18 and eu2 = 0.07
1,ep∗
respectively. The actual utilities in case of (ep∗
the NE as
2) = (0, 0) while Player 2 reaches the same con-
2) are
(0.21, 0.07), which are very close to the approximated
values. While utility approximation is fairly accurate,
players.
7 Note, that Φn itself is interpolated from its actual value at
measured points.
Together or Alone: The Price of Privacy in Collaborative Learning
14
n that is not an actual NE.
Φn actually corresponds to a slightly diﬀerent NE:
(p∗
1, p∗
2) = (0.2, 0.2) with utility (u1, u2) = (0.14, 0.06)
and P oP = 0.25. Note, that while both players obtain a
higher payoﬀ viafp∗
9 Related Work
We divide related literature into two groups based on
the two main topic of this paper: distributed ML and
GT.
9.1 Distributed Machine Learning
ML is frequently implemented in a distributed fashion
for eﬃciency reasons. To tackle its emerging privacy
aspect, Privacy Preserving Distributed ML was intro-
duced, where the locally trained models are safely ag-
gregated.
Distributed training scenarios unanimously assume
a large number of participants and the involvement
of a third party such as in [PRR10, RA12, HCB16,
MMR+16, PZ16]. In more details, in [PRR10] mutually
untrusted parties train classiﬁers locally and aggregate
them with the help of an untrusted curator. In the in-
troduced ε-DP protocol, achieved accuracy depends on
the number of parties and the relative fractions of data
owned by the diﬀerent parties. In [RA12] these depen-
dencies were eliminated for a SGD training algorithms.
On the other hand, authors used (ε, δ)-DP, a weaker
form of DP.
More recently in [HCB16] an ε-DP classiﬁer was in-
troduced with error bound O((εN)−2) compared to the
result of the non-private training where N is the number
of participants. This approach results in strong privacy
guarantees without performance loss for large N. Feder-
ated Learning introduced in [MMR+16] follows another
approach, where the users generate pairwise noise to
mask their data from the aggregator. The bottleneck
of this approach is the communication constraints. Fur-
thermore, the solution is not applicable to two partici-
pants.
All these works assumed the existence of a third-
party aggregator; however, in our work the data holders
themselves train a model together to achieve higher ac-
curacy than what they would have obtained if training
in isolation. Furthermore, all of these works are neither
suitable nor eﬃcient for two participants.
9.2 Game Theory
In [PZ16] the learning process was modeled as a Stack-
elberg game amongst N + 1 players where a learner de-
clares a privacy level and then the other N data holders
respond by perturbing their data as they desire. The
authors concluded that in equilibrium each data holder
perturbs its data independently of the others, which
leads to high accuracy loss.
The closest
to our work are [IL13, CGL15,
WWK+17]. In [IL13] a linear regression scenario was
studied where the features were public but the data were
private. With these settings, the authors proved the ex-
istence of a unique non-trivial NE, and determined its
eﬃciency via the Price of Stability.
A simpler problem was modeled in [CGL15]: esti-
mating a population’s average of a single scalar quan-
tity. The authors studied the interaction between agents
and an analyst, where the agents can either deny access
to their private data or decide the level of precision at
which the analyst gets access. Findings include that it
is always better to let new agents enter the game as it
results in more accurate estimation, and the accuracy
can further be improved if the analyst sets a minimum
precision level.
In both previous scenarios, players would like to
learn a model which represents the whole population.
The accuracy of the estimate is a public good (i.e., non-
exclusive and non-rival [HS+88]). On the contrary, in
CoL the players seek to selﬁshly improve their own ac-
curacy as that is in their own self-interest. As such, they
measure the accuracy of the trained model by how well
it ﬁts to their own datasets, which can result in diﬀer-
ent accuracy levels. Furthermore, these works focused
on particular tasks (linear regression and scalar aver-
aging) while our model is applicable for any training
mechanism.
[WWK+17] studied the problem of private infor-
mation leakage in a data publishing scenario where
datasets are correlated. As such, the utility function for
an agent consists of the beneﬁt of publishing its own san-
itized dataset and the privacy leakage which depends on
the privacy parameters of all involved agents. Opposed
to this, in our model the datasets are independent while
the beneﬁt is aﬀected by all the players’ actions. Thus,
the accuracy of the training depends on the privacy pa-
rameters of both agents, while the privacy loss depends
only on the privacy parameter of a single agent.
10 Conclusion
In this paper, we designed a Collaborative Learning pro-
cess among two players. We deﬁned two player types
(privacy concerned and unconcerned) and modeled the
training process as a two-player game. We proved the
existence of a Nash Equilibrium with a natural assump-
Together or Alone: The Price of Privacy in Collaborative Learning
15
tion about the privacy-accuracy trade-oﬀ function (Φ)
in the general case, while provided the exact formula
when one player is privacy unconcerned. We also deﬁned
Price of Privacy to measure the overall degradation of
accuracy due to the player’s privacy protection.
On the practical side, we studied a Recommenda-
tion System use case: we applied two diﬀerent privacy-
preserving mechanisms (suppression and bounded dif-
ferential privacy) on two real-world datasets (Movie-
Lens, Netﬂix). We conﬁrmed via experiments that the
assumption which ensures the existence of a Nash Equi-
librium holds. Moreover, as a complementary work be-
sides the designed game, we interpolated Φ for this use
case, and devised a possible way to approximate it in
real-world scenarios. Our main ﬁndings are:
– Privacy protection degrades the accuracy heavily for
its user.
– Collaborative Learning is practical when either one
player is privacy unconcerned or the players have
similar dataset sizes and both players’ privacy con-
cerns (weights) are relatively low.
Future work. There are multiple opportunities to im-
prove this line of work such as upgrading the CoL pro-
cess by controlling the other party’s updates. Another
possibility is to design a repetitive game where each
player faces a decision after each iteration or make the
game asymmetric by deﬁning the weights B and C in
private. Incorporating the impact of the potential ad-
versarial aspect for competing companies, and thus in-
vestigating a more elaborate utility function is another
intriguing possibility. Finally, as complementary work,
how to determine the weight parameters for speciﬁc sce-
narios and how to approximate Φ is crucial for the us-
ability of the model in the real world.
References
[CGL15]
[Dwo06]
[FBK16]
Michela Chessa, Jens Grossklags, and Patrick
Loiseau. A game-theoretic study on non-monetary
incentives in data analytics projects with privacy
implications.
In Computer Security Foundations
Symposium (CSF), 2015 IEEE 28th, pages 90–104.
IEEE, 2015.
Cynthia Dwork. Diﬀerential privacy. In Proceedings
of the 33rd international conference on Automata,
Languages and Programming, pages 1–12. ACM,
2006.
Arik Friedman, Shlomo Berkovsky, and Mohamed Ali
Kaafar. A diﬀerential privacy framework for matrix
factorization recommender systems. User Modeling
and User-Adapted Interaction, 26(5):425–458, 2016.
[Gro03]
[HCB16]
[HKP12]
[HS+88]
[IL13]
[KBV09]
[KP99]
GroupLens. The movilens dataset. 2003. https:
//grouplens.org/datasets/movielens/.
Jihun Hamm, Yingjun Cao, and Mikhail Belkin.
Learning privately from multiparty data.
In Inter-
national Conference on Machine Learning, pages
555–563, 2016.
J. Han, M. Kamber, and J. Pei. Data Mining: Con-
cepts and Techniques. Morgan Kaufmann Publish-
ers, 3 edition, 2012.
John C Harsanyi, Reinhard Selten, et al. A general
theory of equilibrium selection in games. MIT Press
Books, 1, 1988.
Stratis Ioannidis and Patrick Loiseau. Linear regres-
sion as a non-cooperative game.
Conference on Web and Internet Economics, pages
277–290. Springer, 2013.
Yehuda Koren, Robert Bell, and Chris Volinsky. Ma-
trix factorization techniques for recommender sys-
tems. Computer, 42(8), 2009.
Elias Koutsoupias and Christos Papadimitriou.
Worst-case equilibria.
404–413. Springer, 1999.
In Stacs, volume 99, pages
In International
2009.
[Pej18]
[MS96]
[Net09]
[MMR+16] H Brendan McMahan, Eider Moore, Daniel Ramage,
Seth Hampson, et al. Communication-eﬃcient learn-
ing of deep networks from decentralized data. arXiv
preprint arXiv:1602.05629, 2016.
Dov Monderer and Lloyd S Shapley. Potential
games. Games and economic behavior, 14(1):124–
143, 1996.
Netﬂix. The netﬂix price dataset.
http://academictorrents.com/details/
9b13183dc4d60676b773c9e2cd6de5e5542cee9a.
Balazs Pejo. Matrix factorisation implementation
in matlab via stochastic gradient descent. GitHub,
2018. https://github.com/pidzso/ML.
Manas Pathak, Shantanu Rane, and Bhiksha Raj.
Multiparty diﬀerential privacy via aggregation of
locally trained classiﬁers.
In Advances in Neural
Information Processing Systems, pages 1876–1884,
2010.
Jeﬀrey Pawlick and Quanyan Zhu. A stackelberg
game perspective on the conﬂict between machine
learning and data obfuscation. In Information Foren-
sics and Security (WIFS), 2016 IEEE International
Workshop on, pages 1–6. IEEE, 2016.
Arun Rajkumar and Shivani Agarwal. A diﬀerentially
private stochastic gradient descent algorithm for
multiparty classiﬁcation. In Artiﬁcial Intelligence and
Statistics, pages 933–941, 2012.
[PRR10]
[PZ16]
[RA12]
[WWK+17] Xiaotong Wu, Taotao Wu, Maqbool Khan, Qiang
Ni, and Wanchun Dou. Game theory based corre-
lated privacy preserving analysis in big data.
IEEE
Transactions on Big Data, 2017.
Appendices
A List of Abbreviations
Abr. Meaning
M L
M F
SGD