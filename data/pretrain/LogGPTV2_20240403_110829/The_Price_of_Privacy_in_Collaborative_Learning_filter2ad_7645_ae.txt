### Spaces of the Players with Floating Point Numbers

The higher the initial privacy level in the BR dynamics, the wider the range of weight ratios within which the system converges to 1 (i.e., no collaboration).

### Together or Alone: The Price of Privacy in Collaborative Learning

#### 8.2 The Whole Process Before Collaboration

In this section, we outline how our game-theoretic model can be applied from the beginning. Specifically, the players have two datasets and need to decide whether to train together and, if so, what privacy parameter to use. These questions can be answered using the CoL game, but first, its parameters must be established. A process diagram describing the entire procedure is presented in Figure 8.

**Figure 8.** The process diagram outlining the steps the players take before collaboration.

- **Initialization:** Each player has their dataset \( D_n \) with corresponding privacy policies \( P_n \) and the accuracy they achieve by training alone, \( \theta_n \).
- **Setup:** Based on the dataset sizes \( |D_n| \) and privacy policies \( P_n \), the players determine the privacy-preserving method \( M \) to use, the benefit \( b \) and cost \( c \) functions, and the corresponding weight parameters \( C_1, B_1, C_2, B_2 \).
- **Split:** Using the dataset size ratio \( \frac{|D_n|}{|D_m|} \), the players split their datasets into two parts, \( D_{n1} \) and \( D_{n2} \), to mimic the original datasets.
- **Approximation:** Based on the newly created datasets \( D_{n1}, D_{n2} \) and the accuracy obtained by training alone \( \theta_n \), the players approximate the accuracy improvement of training together, \( f_{\Phi_n} \).
- **Game:** Using the approximated privacy-accuracy trade-off function \( f_{\Phi_n} \) and the parameters determined in the setup phase, the players determine the Nash Equilibrium (NE) \( (\varepsilon^*_{1,n}, \varepsilon^*_{2,n}) \) and its corresponding Price of Privacy via the CoL game.
- **Collaboration:** If the approximation suggests that training together is beneficial for both participants (i.e., \( \forall n \in \{1, 2\} : f_{\Phi_n}(\varepsilon^*_{1,n}, \varepsilon^*_{2,n}) > 0 \)), they collaborate using their datasets \( D_n \) with the approximated optimal privacy parameter \( \varepsilon^*_{n} \).

In this example, we focus mainly on the "Game" step and partially on the "Approximation" step. We manually chose the parameters determined in the "Setup" phase. For instance, we use \( b_{DP} \) as the privacy-preserving mechanism \( M \). We assume that players have a portion of the pre-processed Netflix rating dataset, which contains only movie ratings. Therefore, it is expected that the players value privacy less than accuracy. For this example, we set \( B_1 = B_2 = 1 \) and \( C_1 = C_2 = 0.1 \). The benefit and privacy loss functions are defined in Equation (4).

As previously argued, self-division is most accurate when Equation (12) holds. Given that the density of the original Netflix dataset is \( d \approx 0.01 \), we assign each player's dataset 10 million ratings by randomly selecting 20% of the users from NF10 and assigning them to either player.

The players separately approximate \( \Phi_n \) by self-division, so \( f_{\Phi_1} \) and \( f_{\Phi_2} \) may not be identical. The exact values of these approximations, along with the true value of \( \Phi_n \), can be found in Appendix E. We found that the RMSE of \( f_{\Phi_n} \) is around 0.001 for both players. Using \( f_{\Phi_1} \), Player 1 approximates the NE as \( (\varepsilon^*_{1,1}, \varepsilon^*_{1,2}) = (0, 0) \), while Player 2 reaches the same conclusion via \( f_{\Phi_2} \). This means the Price of Privacy is zero. The approximated utilities are \( eu_1 = 0.18 \) and \( eu_2 = 0.07 \) respectively. The actual utilities in the case of \( (\varepsilon^*_{1,1}, \varepsilon^*_{1,2}) = (0.21, 0.07) \) are very close to the approximated values. While utility approximation is fairly accurate, \( \Phi_n \) actually corresponds to a slightly different NE: \( (p^*_{1}, p^*_{2}) = (0.2, 0.2) \) with utility \( (u_1, u_2) = (0.14, 0.06) \) and \( PoP = 0.25 \).

#### 9 Related Work

We divide the related literature into two groups based on the main topics of this paper: distributed machine learning and game theory.

##### 9.1 Distributed Machine Learning

Machine learning (ML) is often implemented in a distributed manner for efficiency. To address the emerging privacy concerns, Privacy Preserving Distributed ML was introduced, where locally trained models are safely aggregated.

Distributed training scenarios typically assume a large number of participants and the involvement of a third party, such as in [PRR10, RA12, HCB16, MMR+16, PZ16]. In [PRR10], mutually untrusted parties train classifiers locally and aggregate them with the help of an untrusted curator. The achieved accuracy depends on the number of parties and the relative fractions of data owned by the different parties. In [RA12], these dependencies were eliminated for a Stochastic Gradient Descent (SGD) training algorithm, though a weaker form of differential privacy, (ε, δ)-DP, was used.

More recently, in [HCB16], an ε-DP classifier was introduced with an error bound of \( O((\varepsilon N)^{-2}) \) compared to non-private training, where \( N \) is the number of participants. This approach provides strong privacy guarantees without performance loss for large \( N \). Federated Learning, introduced in [MMR+16], follows another approach where users generate pairwise noise to mask their data from the aggregator. However, this solution is not suitable for two participants due to communication constraints.

All these works assumed the existence of a third-party aggregator. In contrast, our work involves data holders training a model together to achieve higher accuracy than they would obtain if training in isolation. Furthermore, these works are neither suitable nor efficient for two participants.

##### 9.2 Game Theory

In [PZ16], the learning process was modeled as a Stackelberg game among \( N + 1 \) players, where a learner declares a privacy level, and the other \( N \) data holders respond by perturbing their data. The authors concluded that, in equilibrium, each data holder perturbs its data independently, leading to high accuracy loss.

The closest works to ours are [IL13, CGL15, WWK+17]. In [IL13], a linear regression scenario was studied where the features were public, but the data were private. The authors proved the existence of a unique non-trivial Nash Equilibrium and determined its efficiency via the Price of Stability.

A simpler problem was modeled in [CGL15]: estimating a population’s average of a single scalar quantity. The authors studied the interaction between agents and an analyst, where the agents can either deny access to their private data or decide the level of precision at which the analyst gets access. They found that it is always better to let new agents enter the game, as it results in more accurate estimation, and the accuracy can be further improved if the analyst sets a minimum precision level.

In both previous scenarios, players aim to learn a model representing the whole population, making the accuracy a public good (non-exclusive and non-rival [HS+88]). In contrast, in CoL, players seek to selfishly improve their own accuracy, measuring the accuracy of the trained model by how well it fits their own datasets, which can result in different accuracy levels. Furthermore, these works focused on specific tasks (linear regression and scalar averaging), while our model is applicable for any training mechanism.

[WWK+17] studied the problem of private information leakage in a data publishing scenario where datasets are correlated. The utility function for an agent consists of the benefit of publishing its sanitized dataset and the privacy leakage, which depends on the privacy parameters of all involved agents. In our model, the datasets are independent, and the benefit is affected by all players' actions. Thus, the accuracy of the training depends on the privacy parameters of both agents, while the privacy loss depends only on the privacy parameter of a single agent.

#### 10 Conclusion

In this paper, we designed a Collaborative Learning process among two players. We defined two player types (privacy-concerned and unconcerned) and modeled the training process as a two-player game. We proved the existence of a Nash Equilibrium under a natural assumption about the privacy-accuracy trade-off function (Φ) in the general case and provided the exact formula when one player is unconcerned about privacy. We also defined the Price of Privacy to measure the overall degradation of accuracy due to the players' privacy protection.

On the practical side, we studied a Recommendation System use case, applying two different privacy-preserving mechanisms (suppression and bounded differential privacy) on two real-world datasets (MovieLens, Netflix). We confirmed via experiments that the assumption ensuring the existence of a Nash Equilibrium holds. Additionally, we interpolated Φ for this use case and devised a possible way to approximate it in real-world scenarios. Our main findings are:

- Privacy protection significantly degrades the accuracy for its user.
- Collaborative Learning is practical when either one player is unconcerned about privacy or the players have similar dataset sizes and relatively low privacy concerns (weights).

**Future Work:**
There are multiple opportunities to improve this line of work, such as upgrading the CoL process by controlling the other party’s updates. Another possibility is to design a repetitive game where each player faces a decision after each iteration or make the game asymmetric by defining the weights \( B \) and \( C \) in private. Incorporating the impact of potential adversarial aspects for competing companies and investigating a more elaborate utility function is another intriguing possibility. Finally, determining the weight parameters for specific scenarios and approximating Φ is crucial for the usability of the model in the real world.

### References

- [CGL15] Michela Chessa, Jens Grossklags, and Patrick Loiseau. A game-theoretic study on non-monetary incentives in data analytics projects with privacy implications. In Computer Security Foundations Symposium (CSF), 2015 IEEE 28th, pages 90–104. IEEE, 2015.
- [Dwo06] Cynthia Dwork. Differential privacy. In Proceedings of the 33rd international conference on Automata, Languages and Programming, pages 1–12. ACM, 2006.
- [FBK16] Arik Friedman, Shlomo Berkovsky, and Mohamed Ali Kaafar. A differential privacy framework for matrix factorization recommender systems. User Modeling and User-Adapted Interaction, 26(5):425–458, 2016.
- [Gro03] GroupLens. The MovieLens dataset. 2003. https://grouplens.org/datasets/movielens/.
- [HCB16] Jihun Hamm, Yingjun Cao, and Mikhail Belkin. Learning privately from multiparty data. In International Conference on Machine Learning, pages 555–563, 2016.
- [HKP12] J. Han, M. Kamber, and J. Pei. Data Mining: Concepts and Techniques. Morgan Kaufmann Publishers, 3rd edition, 2012.
- [HS+88] John C Harsanyi, Reinhard Selten, et al. A general theory of equilibrium selection in games. MIT Press Books, 1988.
- [IL13] Stratis Ioannidis and Patrick Loiseau. Linear regression as a non-cooperative game. In International Conference on Web and Internet Economics, pages 277–290. Springer, 2013.
- [KBV09] Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recommender systems. Computer, 42(8), 2009.
- [KP99] Elias Koutsoupias and Christos Papadimitriou. Worst-case equilibria. In STACS, volume 99, pages 404–413. Springer, 1999.
- [Pej18] Balazs Pejo. Matrix factorisation implementation in MATLAB via stochastic gradient descent. GitHub, 2018. https://github.com/pidzso/ML.
- [MS96] Dov Monderer and Lloyd S Shapley. Potential games. Games and economic behavior, 14(1):124–143, 1996.
- [Net09] Netflix. The Netflix prize dataset. http://academictorrents.com/details/9b13183dc4d60676b773c9e2cd6de5e5542cee9a.
- [MMR+16] H Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et al. Communication-efficient learning of deep networks from decentralized data. arXiv preprint arXiv:1602.05629, 2016.
- [PRR10] Manas Pathak, Shantanu Rane, and Bhiksha Raj. Multiparty differential privacy via aggregation of locally trained classifiers. In Advances in Neural Information Processing Systems, pages 1876–1884, 2010.
- [PZ16] Jeffrey Pawlick and Quanyan Zhu. A Stackelberg game perspective on the conflict between machine learning and data obfuscation. In Information Forensics and Security (WIFS), 2016 IEEE International Workshop on, pages 1–6. IEEE, 2016.
- [RA12] Arun Rajkumar and Shivani Agarwal. A differentially private stochastic gradient descent algorithm for multiparty classification. In Artificial Intelligence and Statistics, pages 933–941, 2012.
- [WWK+17] Xiaotong Wu, Taotao Wu, Maqbool Khan, Qiang Ni, and Wanchun Dou. Game theory-based correlated privacy-preserving analysis in big data. IEEE Transactions on Big Data, 2017.

### Appendices

#### A List of Abbreviations

- **ML**: Machine Learning
- **MF**: Matrix Factorization
- **SGD**: Stochastic Gradient Descent