we also design single-node Txs support in SPEICHER (§ V-B).
A. Secure Distributed Transactions
Distributed design. TREATY partitions data into shards
that may be stored on separate machines that fail indepen-
dently from each other. Each TREATY node runs a trans-
actional single-node KV storage engine built on top of
RocksDB/SPEICHER [31], as shown in Figure 1. We im-
plement a secure 2PC protocol with the userspace network
stack based on eRPC [36] to execute distributed Txs and
guarantee security properties. For securing the state of the
protocol as well as providing secure recovery we make use of
authenticated log ﬁles (MANIFEST, Clog and WAL). MAN-
IFEST logs the changes in the state of the persistent storage
(e.g., compactions,
live logs). WAL stores the MemTable
updates and the prepared Txs. Lastly, Clog is written by Txs
coordinators and keeps the 2PC protocol state.
The system’s initialization requires a trusted conﬁguration
and attestation service to establish trust
in the distributed
system. It distributes to nodes important information about
the cluster conﬁguration (e.g., secrets and keys’ distribution
to nodes, network connections).
Clients access TREATY over the network. For each Tx, a
TREATY’s node initialises a global Tx handle that is uniquely
identiﬁed by a monotonically sequence number and the node
id. A Tx coordinator interacts with the client and distributes
their requests to the involved participant nodes. Participants
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:16:05 UTC from IEEE Xplore.  Restrictions apply. 
17
Client
Coordinator
PK
PN
BeginTxn()
TxnPut(K1,V)
TxnGet(K2)
2
AllocMsgBuffers
EnqueueTxnRequest
TxBurst and yield 
1
poll for replies and/or
4
yield 
TxnCommit()
FreeMsgBuffers registered
3
to that request
r e t u r n   m s g
f a i
{ s u c c e s s /
l
v e c t o r  }
5
6
Log TxnPrepare()
TxBurst and yield 
Log commit decision
7
Stabilize entry
TxnPut(K1,V)
T x n P u t
)
(
r e t u r n  
A C K / F A I L  
n G e t( )
2  
T
n   V
r
e t u
x
r
TxnPrepare()
A C K / F A I L
A C K / F A I L
TxnCommit()
TxnGet(K2)
RxBurst and process
request 
AllocMsgBuffers
ExecuteTxnReqHandler 
EnqueueResponse
and yield 
Prepare txn
Stabilize txn
8
Enqueue response and
yield
TxnPrepare()
TxnCommit()
A C K
r e t u r n   m s g  
f a i
s u c c e s s /
{
l
}
A C K
transaction
committed and
rollback-protected
Figure 2: TREATY’s two-phase commit protocol.
create local private Txs through TREATY’s single-node trans-
actional KV store (§ V-B). To ensure isolation, TREATY’s
engines own a private (per-node) keys lock table.
Lastly, we leverage the exit-less approach of executing
syscalls provided by SCONE for accessing the persistent stor-
age. Prior work [31], has introduced SPDK [71]. However, we
did not use SPDK for two reasons: (i) in our experiments the
database ﬁts entirely in the kernel page cache therefore read
access was much faster than SPDK which would have to read
from SSD and (ii) we conﬁgured SCONE to best ﬁt TREATY
for storage I/O syscall execution.
Integrity, conﬁdentiality and freshness. Each node runs
a single modiﬁed SPEICHER instance. TREATY engine runs
inside the enclave to ensure integrity, conﬁdentiality and
freshness for the execution and the resided run-time data (e.g.,
MemTable, transactions’ local buffers, hash values).
To extend the trust to persistent storage, we adapt SPEICHER
which offers a secure authenticated SSTable hierarchy. SPE-
ICHER stores encrypted blocks of KV pairs as well as a footer
with the blocks’ hash values (for integrity checks). TREATY
extends the persistent data structures by adding an extra log
ﬁle, the Clog for the 2PC. Lastly, to ensure crash recovery in
TREATY, we defer deleting the old SSTables and logs until
MANIFEST’s entries for that compactions are stabilized.
TREATY also extends the trust for the network I/O by
constructing a secure message format for Txs (§ VII-A). A
message encapsulates an Initialization Vector (12 B) and a
MAC (16 B) for proving its authenticity and integrity. In
addition to Tx’s data, we also add some metadata (e.g. node,
Tx and operations identiﬁers) that allows TREATY to protect
against duplication of packages by an attacker.
Two-phase commit. TREATY offers serializable distributed
ACID Txs with strong security guarantees throughout a secure
2PC protocol
implemented over our secure network stack
(§ VII-A). Figure 2 illustrates the complete protocol design.
Clients are registered to TREATY nodes and thereafter, are able
to execute transactions. Upon a client’s request, the transac-
tion’s coordinator node (TxC) initializes a global Tx which
is uniquely identiﬁed in the entire cluster and associated with
a speciﬁc RPC communication channel. Each RPC is strictly
owned by one thread, which minimizes shared resources.
The TxC distributes the Tx’s requests to the responsible
nodes and/or processes its own requests. As shown in Figure 2,
before forwarding the requests to the participants, each Tx
reserves (untrusted) memory for the requests and responses 1(cid:3).
These message buffers have to remain allocated until the entire
request has been served 3(cid:3). To eliminate paging overheads,
they reside encrypted in the untrusted host memory.
Once the message is constructed, the TxC en-queues the
request 2(cid:3). Note that en-queuing the request does not trans-
mit the message. In case of multiple requests, coordinators
can defer the transmissions until all requests are en-queued.
Once the TxC has executed its own-managed requests and
has forwarded all requests to the participants, it yields and
periodically checks if the participants have replied 4(cid:3).
At a commit, TREATY ﬁrst prepares the Tx for a distributed
commit accross all parties involved. Every Tx/operation is
logged to Clog with its own unique trusted counter value 5(cid:3).
Afterwards, all participants prepare their local Tx. Participants
delay replying back to the coordinator until the prepare entry
in the log is stabilized 8(cid:3). TREATY’s stabilization ensures that
coordinators will not consider the Tx as successfully prepared
until all participants ensure that they are able to recover and
commit the transaction after a crash. If not all participants
ensure that their prepare phase is stabilized, after a crash this
entry cannot be safely recovered. Especially in cases where the
participants had already committed the entry but only some
of them could recover the committed Tx after a crash, the
system would be in a inconsistent state where distributed Txs
are partially committed to some, but not all involved, nodes.
The TxC, before committing/aborting, also stabilizes the
prepare’s phase decision on the Clog 6(cid:3)- 7(cid:3). If the TxC crashes
before this entry is stable, the recovered coordinator will re-
execute the prepare phase. Once this is rollback protected, the
Tx can commit. We do not need to wait for the commit entry
to be stable to reply to the client. Even if the system crashes,
this Tx can be committed in the exact same order.
B. Secure Single-node Transactions
KV Storage engine and single-node Txs. TREATY’s storage
engine runs inside the enclave for which the security properties
are guaranteed. TREATY leverages SPEICHER’s data model
that offers an authenticated LSM structure for the persistent
storage but also optimizes the usage of EPC memory. Par-
ticularly, TREATY adapts SPEICHER’s MemTable design by
separating the keys from the values. We keep keys along with
their version number inside the enclave, while we place the
encrypted values in the untrusted host. To access values and
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:16:05 UTC from IEEE Xplore.  Restrictions apply. 
18
prove their authenticity we similarly keep a pointer to the value
as well as its secure hash value along with the key.
However, SPEICHER cannot support Txs;
therefore we
extend it
to integrate both optimistic and pessimistic Txs
exporting an interface to the upper Tx layer to access the
LSM-data structure. We preserve the RocksDB’s interface and
semantics. For the persistent storage, TREATY extends the
persistent data structures by adding an extra log ﬁle, the Clog
for the 2PC. TREATY’s distributed Txs can then be viewed as
the set of all participants’ single node Txs.
Pessimistic Txs take locks on their keys while optimistic
Txs use sequence numbers to identify conﬂicts at the commit
phase. For optimistic Txs, each key has a seq. number showing
its the latest version and is atomically increased during the
commit phase. At commit, Txs log their updates to the WAL
and update the MemTable. We only reply to a client after the
Tx becomes stable, ensuring that upon a crash, clients will not
have to re-execute successfully committed transactions. Thus,
conﬂicting transactions will maintain their initial ordering.
Lock tables. Nodes store a table of locks for their keys that is
divided across shards, each protected with a lock, by splitting
the key space. TREATY runs with a big number of shards to
avoid locking bottlenecks. Txs that fail to acquire a lock within
a timeframe, return with a timeout error.
VI. STABILIZATION PROTOCOL
TREATY’s stabilization protocol ensures secure and crash-
consistent persistency for the committed Txs. To achieve this,
our protocol relies on three core principles. First, TREATY
establishes trust between the nodes based on collective remote
attestation. Secondly, after the 2PC’s execution (§ V), TREATY
ensures crash consistency for the committed Txs. Lastly, once
Txs are crash-consistent, TREATY ensures rollback protection
in distributed settings. We next explain these three principles.
Distributed trust establishment. Upon startup TREATY boot-
straps a Conﬁguration and Attestation Service (CAS) on a
node in the network to provide scalable remote attestation
and authentication. For attestation, the service provider veriﬁes
the CAS over Intel Attestation Service (IAS). On success the
service provider deploys an instance of TREATY’s local attes-
tation service (LAS) on all nodes, veriﬁed by the CAS over
IAS. The LAS replaces the Quoting Enclave (QE), collecting
and signing quotes for all TREATY instances, running on the
node. After the CAS veriﬁed a new instance, it supplies the
instance with the necessary conﬁguration, e.g., network key,
nodes’ IPs, etc. The CAS is also used to authenticate clients
and establish trust between TREATY and clients.
Crash consistency and recovery. After the 2PC’s execution,
TREATY ensures crash consistency and recoverability using
three persistent log ﬁles; MANIFEST, WAL and Clog. As dis-
cussed in § V, Clog logs the 2PC states, WAL the committed
data and MANIFEST stores the state changes in the SSTables.
TREATY relies on these logs being written sequentially; thus,
it assigns to each of their entries a unique, monotonic and
deterministically increased trusted counter value. The recovery
protocol relies on that property to detect rollback attacks
or verify freshness and state continuity. Precisely TREATY’s
recovery veriﬁes that the state of the persistent storage and
logged Txs is the most recent (through the veriﬁcation of the
logs) and recovers the most recent stable state.
Upon restart MANIFEST is replayed ﬁrst; it recovers the
SSTable hierarchy and loads metadata (hashes of SSTable’s
blocks) that will be used to verify the integrity and the
freshness of a SSTable upon access into the enclave. Note
that TREATY’s garbage collector only deletes SSTable ﬁles
when the newly compacted ones refer to stabilized entries in
MANIFEST. MANIFEST also recovers all the “live” WAL and
Clog ﬁles. Similarly, TREATY makes sure that the old versions
of the logs are not deleted before their effect to the database
has been rollback protected (stabilized). For example, a WAL
is marked for deletion as long as the matching MemTable has
been successfully compacted and this compaction action refers
to a stable entry in the MANIFEST. The Clog is deleted as
long as there are no unstable entries and does not contain any
unﬁnished prepared transaction entry.
For each log ﬁle, TREATY initializes a unique trusted
counter and assigns a monotonically and deterministically
increasing counter value to each log entry. TREATY’s criterion
for freshness is that 1) only log entries with counter value less
than the trusted service’s value can be recovered, 2) the counter
values are deterministically increased—for state continuity,
e.g., deleted or reordered entries are detected, and 3) last log
entry’s value match the counter’s value.
TREATY accesses the trusted counter service through the
network. The communication is asynchronous to maximize
CPU usage. As discussed in § V the 2PC incorporates the sta-
bilization protocol ensuring distributed rollback protection—
Txs are only considered committed (and clients get notiﬁed)
after the commit decision has been stabilized in the logs.
TREATY’s trusted counter service implements an echo
broadcast [72] protocol with an extra conﬁrmation message
After the MANIFEST, TREATY replays in order all live
WALs to restore the latest MemTables. The WAL also contains
the prepared Txs. Therefore, each node will also re-initialize
all prepared Txs that are not yet committed. For each prepared
Tx, the node communicates with the Tx’s coordinator for
either committing or aborting.
Lastly, Clog is replayed. TREATY restores the state of the
2PC protocol for all prepared on-going Txs. The coordinator
will re-execute the prepare phase, if it cannot guarantee that
the Tx will succeed. If the prepare phase decision is logged,
then, thanks to the stabilization function of TREATY, these Txs
are also prepared in the participant nodes. The coordinator will
then instruct the participants to commit. If a node has already
committed the Tx, this message is ignored.
Distributed rollback protection. For secure persistency,
TREATY provides rollback protection across distributed Txs
by leveraging a trusted counter service. While our design is
independent of the trusted counter service, we adopt Rote [70],