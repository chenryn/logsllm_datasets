the
DDoS Feature Validation:
The developer next de-
ﬁnes the desired network features to receive results from
an analysis of
testing phase. The developer de-
ﬁnes Query and Preprocessor in the same way, and
calls ValidateFeatures with the pre-deﬁned Query,
Preprocessor, and the Model. Upon completion of the
testing phase, Athena generates a testing summary, as illus-
trated in Figure 6.
DDoS Testing Environments and Results We established
a testing environment to reﬂect an enterprise-scale network
topology as illustrated in Figure 7, and compare our envi-
2These labels are used by the supervised and unsupervised learning algo-
rithms.
7
Fig. 7.
application.
Enterprise-scale topology for evaluating the DDoS Detection
ronment with previous work for an SDN-based DDoS detec-
tion [10], as described in Table VI. Our topology consists of
48 links and 18 switches (6 physical switches, and 12 OVS
switches) managed by three distributed network controllers,
including Athena instances. We deployed a K-Means-based al-
gorithm with 10 tuples, and simulated the testing environment
using a Mininet environment. The attack scenario is similar to
trafﬁc patterns in [10]. The detection rate is 99.23%, and false
alarm rate is 4.46%. We will discuss these results further in
Section VII-B.
B. Scenario 2: Link Flooding Attacks (LFA) Mitigation
LFA represents a serious and common network attack, in
which the adversary saturates a target network area with few
resources [25]. A noteworthy aspect of the Athena frame-
work is that, unlike some other tools,
it produces moni-
toring applications that are deployable without modiﬁcation
to the underlying network infrastructure. For example, we
compare the implementation of LFA mitigation using Athena
with Spiffy [26], which utilizes the transmission rate control
mechanism within the TCP protocol to detect and mitigate
LFA. Spiffy adjusts how much trafﬁc should be delivered
through a conditional assessment of the current network status.
It assumes that malicious ﬂows do not respond to a temporary
change of a network status, which differs from the normal
proﬁle of legitimate ﬂows. As a solution, temporary bandwidth
expansion (i.e., expanding bandwidth temporarily to detect
malicious ﬂows by leveraging characteristics of TCP) and
runtime ﬂow migration (i.e., re-assigning ﬂows to expand
bandwidth for a suspicious link) are proposed.
18 OpenFlow Switches (6 Physical, 12 OVS)48 Links36 HostsTarget ServersBenign site #1Benign site #2Attacker siteTABLE VII.
COMPARISON OF LINK FLOODING ATTACK DETECTION
AND MITIGATION STRATEGIES USING SDNS.
Category
Link congestion
Rate change
Trafﬁc engineering
Insider threat
Spiffy [26]
SNMP
OpenSketch [4]
Edge router
Out of scope
Athena
Built-in
OF switch
All switches
Covered
LFA Mitigation Service using Athena: We have im-
plemented a comparable Link Flooding Attacks mitigation
service as an Athena application. In Table VII, we present a
comparison of the Spiffy implementation of the LFA mitigation
service using the same solution implemented with the Athena
framework. Here, the demanding functions involved in LFA
detection and mitigation include solving link congestion de-
tection, recognizing per-ﬂow rate changes, and implementing
ﬂow alterations.
LFA Event Handler Registration: The LFA detector
receives link usage to measure link utilization and per-ﬂow
changes to distinguish attackers. Since Athena provides various
volume-based features including per-ﬂow changes, we deﬁne
these candidate features, including a threshold. For example,
the candidate features are volume-based features such as
port_rx_bytes_var, which represents changes at each
port, and flow_byte_count_var, which represents the
change in byte counts. Likewise, we choose volume-based
features (e.g., port_rx_bytes). Finally, we simply call the
AddEventHandler API with a pre-deﬁned event handler to
perform the detection and mitigation of incoming events.
LFA Detection Logic: Developers may implement the
custom detection logic in the Event_Handler. The detection
logic includes lightweight threshold-based ﬂooding detection,
which measures volume per port, and may use a TBE-based
detector that tracks per ﬂow changes. Lastly, the mitigation
logic simply blocks suspicious hosts based on the detection
results by invoking Reactor.
Using Athena, we can implement
the proposed Spiffy
LFA detection and mitigation mechanism with our SDN test
environment in under 25 lines of Java code, excluding the
custom detection logic.
Comparing Athena-based LFA mitigation with Spiffy:
Table VII shows the comparison of LFA detection and miti-
gation using Spiffy and Athena. Spiffy uses an SNMP-based
link utilization measurement to detect congested links. How-
ever, operators need to conﬁgure SNMP-based measurement
functions, and the network’s switches must also support this
function. Spiffy leverages OpenSketch-enabled switches [4] to
detect rate changes from an edge. Although OpenSketch could
reduce overhead to measure rate changes, operators must de-
ploy OpenSketch-enabled switches into their existing network
infrastructure. This increases the deployment cost of the Spiffy
solution, as it requires features that are non-standard in many
SDN environments. Implementing the same anomaly detection
algorithm using Athena removes the need for vendor-speciﬁc
network devices and SNMP-enhanced monitoring capabilities.
In contrast to the Spiffy environment, Athena’s application can
operate directly on the SDN without data plane alterations.
Fig. 8. An illustration of the Network Application Effectiveness problem.
C. Scenario 3: Network Application Effectiveness (NAE)
Network applications are critical elements of a network’s
SDN stack, as they embody the logic that deﬁnes the ﬂow
policies for the entire network. Thus, validating application
behavior is important, particularly before their adoption into
critical or sensitive network environments. Unfortunately, it
is difﬁcult to verify application behavior, as modern SDN
environments may allow multiple applications to run on a con-
troller in parallel. These applications can also cause conﬂicts in
the form of ﬂow rule contradictions. Several SDN researchers
have tried to solve this problem through control mechanisms
to resolve network application conﬂicts [27], [28], [29], [30].
However,
they did not explore misuse anomalies that can
violate network policies deﬁned at deployment. We refer to
this as the Network Application Effectiveness (NAE) problem,
as illustrated in Figure 8.
Let us assume that an operator installs a load-balancing
(LB) application, which deﬁnes ﬂow rules intended to evenly
distribute a target trafﬁc load across a given set of network
services. Consider a security application that attempts to
direct FTP-related trafﬁc through an inline security device
that analyzes the FTP trafﬁc for signs of malicious command
patterns. In this scenario, the LB app and the security app
may conﬂict in their decisions regarding packet forwarding. To
handle conﬂicts, operators set a higher priority for the security
app, thus allowing it to over-rule the LB app when their rules
conﬂict. While the problem is well known and addressed by
prior work [27], [28], [29], [30], these projects do not consider
how to detect the wide range of unwanted network anomalies
that may arise as ﬂow rules are evaluated and discarded by the
control layer.
The above scenario leads to interesting potential ﬂow
pattern anomalies that can arise as the control plane begins
to resolve conﬂicts in the rules produced by the competing
applications. For example,
if the primary purpose of the
network in Figure 8 is to serve FTP users, the network may
suffer signiﬁcant overhead by the unbalanced load produced
by the security policy. When the network is dominated by
FTP ﬂows, this trafﬁc will begin to saturate S6 due to the
security app. Although S3 is available to deliver trafﬁc to
the destination, it cannot receive trafﬁc due to the security
app’s shortest path policy which dictates that all trafﬁc from
S6 goes to S7. To evaluate this, we set up an experimental
environment, with the edge switches S1 and S5, where each
host downloads ﬁles or accesses pages from the FTP and web
8
Security device All) s1 -> s2 -> s3 -> s4  -> Server farm           s5 -> s6 -> s7 -> Server farm10010RulesApplicationLoad balancerSecurityPriorityFTP) any -> DPI -> DST(Shortest path)No rule conﬂicts!S1S2S3S6S5S4S7Web, FTP server farmHostsHostsLoad balancerSecurityLegend3.2, Spark version 1.6, and JfreeChart version 1.0.13. We
have implemented the prototype of Athena with approximately
15,000 lines of Java code.
Athena is implemented as an ONOS subsystem, which
provides services to an application layer. We modify the imple-
mentation of OpenFlowController to get OpenFlow con-
trol messages directly, and OpenFlowDeviceProvider to
issue statistics request messages to the SDN data plane. We
mark an XID value for statistics request messages to calculate
variation features exactly, as ONOS issues request messages
to the data plane as part of its management functions. To
extract application information per ﬂows, Athena leverages
the FlowRule subsystem, which manages ﬂow entries within
the controller. The Athena application operates as a separate
process and communicates with the Athena framework via
interprocess communication to reduce dependencies.
VII. EVALUATION
We now evaluate Athena with respect
to its usability,
network scalability, and overhead. To explore its usability, we
consider the design of Athena’s anomaly detection applications
against comparable applications developed without Athena.
For the scalability assessment, we evaluate performance of
the large-scale DDoS anomaly detection algorithm introduced
in Section V-A. Finally, we measure the overhead of Athena
feature extraction using the Cbench benchmark. To evaluate
our work, we created an experimental environment with ﬁve
high performance servers (four Intel hexa-core Xeon E5-1650,
one Intel octa-core Xeon E5-2650) with 64GB RAM, two Intel
I5 quad-core I5-4690 and 16GB RAM memory, seven physical
switches 6.
A. Evaluating Usability of Athena
TABLE VIII.
THE LINES OF JAVA CODES FOR A DDOS DETECTOR PER
ALGORITHM (EXCLUDING IMPORTS).
DDoS detector
(Algorithm)
K-Means
Logistic Regression
Athena
Spark Hama [35]
45
42
825
851
817
829
In evaluating the usability of Athena, we implemented a
DDoS detection application within different environments, and
then provide a rough approximation of the implementation
complexity by quantifying the source lines of code (SLoC)
required to implement the application. While imperfect, SLoC
is often used as a metric of usability (e.g., [36], [37]). We
believe that SLoC (application compactness), given the lack
of a large developer-base for feedback, provides a useful early
usability measure, similar in spirit to how it was applied to
answer this same question in related prior work.
Each resulting application embodies the functionality of
the DDoS attack detector in Section V. As summarized in
Table VIII, the application based on Athena uses 5% of the
lines of code that comparable functionalities require when
implemented on Spark [32] and Hama [35].
6Two Pica8 P3290,
ARISTA 7050T-36.
two PICA8 P3297,
two PICA8 AS4610, and one
9
Fig. 9. The coarse-grained analysis result alerted by the Athena UI manager,
when applications obey the user-deﬁned SLA.
servers respectively.
NAE Monitor Implementation: Detecting the NAE prob-
lem is straightforward with Athena since it provides a strong
query mechanism to retrieve network features with advanced
data preprocessing capabilities (e.g., sorting, aggregation, and
ranking). We register an event handler to retrieve ﬂow-related
network features per application by AddEventHandler, and
analyze whether a given feature obeys a user-deﬁned SLA (ser-
vice level agreement)3 by the Check_SLA()4. If an incoming
event obeys the SLA, the ResultsGenerator utility API is
invoked to generate the Results to notify operators. Finally,
it reports anomalous behavior to the operator’s GUI interface
via the ShowResults API, as illustrated in Figure 9.
NAE Analysis Results: Figure 9 shows our application
results. Since we set up a query with “Match DPID==(6
or 3)”, the results only represent relevant features aggre-
gated by app ID, switch ID, and timestamp. It shows a global
view of packet count information per switch. The sawtooth
pattern in this graph is caused by the expiration of ﬂow rules,
since LB app issues ﬂow rules with soft timeout 5. After the
security app is activated from 03:58, the security app takes
over most trafﬁc ﬂows, re-routing their packets into the path
of the security device. Therefore, the LB App loses forwarding
control due to its low priority. Although the LB app is active,
the network begins to suffer unexpected saturation in some
links and low volume in others. We implemented the NAE
problem detector on Athena within 30 lines of Java code.
VI.
IMPLEMENTATION
We have developed a prototype implementation of the
Athena framework that integrates within ONOS [14], which
is an emerging SDN distributed controller for large-scale net-
works, focusing on service provider use-cases. Athena also em-
ploys MongoDB [31] for its distributed database, Spark [32],
[33] for its scalable computing cluster, and JfreeChart [34]
for the graphical interface. The prototype operates on ONOS
version 1.6, using OpenFlow 1.0 and 1.3, MongoDB version
3In this scenario, the SLA is that trafﬁc should be distributed evenly per
each switch.
4This function is a custom algorithm to detect asymmetric trafﬁc patterns.
5The soft timeout is used for deleting ﬂow rules, when there are no incoming
packets within a certain time.
Security app activatedRule expiredFig. 10. A performance assessment of the DDoS application while performing
anomaly detection tasks per the number of computing nodes.
B. Measuring Scalability of Athena
Figure 10 presents the performance results for the DDoS
detection application in Section V-A. We use an experimental
environment with ten instances on the three Xeon servers. The
instances consist of six compute nodes and a master node on
the two hexa-core Xeon servers, and three DB nodes on the
octa-core Xeon server. Here, we measure the total testing time
according to the number of compute instances. The dataset
includes 37,370,466 entries for a 50GB dataset. As the number
of computing instances increases, we observe a linear decrease
in the total processing time and the total test time with six
nodes is approximately 27.6% of the test time with a single-
compute node instance. We compare the application hosted
by Athena with an application on Spark, and results show
Athena introduces a small overhead (under 10%) over the
Spark application.
C. Overhead of Athena’s Feature Extraction
We measure the overhead of Athena’s feature extraction
while handling external events and compare this overhead to
the ONOS baseline (e.g., Cbench benchmark, and CPU usage).
We set up an experimental environment with the hexa-core
Xeon server to test Cbench benchmark and two hexa-core
Xeon servers with seven physical switches to measure CPU
usage while gathering events from the switches.
TABLE IX.
CBENCH BENCHMARK FLOW INSTALL THROUGHPUT WITH
AND WITHOUT ATHENA (RESPONSE/S) OVER 50 ROUNDS OF TESTING.
Without
With
With (no DB)
Overhead
(no DB)
MIN
773,618
107,245
631,647
86.13%
(18.35%)
MAX
883,376
610,724
686,227
30.86%
(22.31%)
AVG
831,366
389,584
658,514
53.13%
(20.79%)
1) Cbench benchmark with/without Athena: The ONOS
testing group evaluates the scalability of ONOS to measure
how many burst events could be handled. For example, they
evaluate burst Packet_IN event handling throughput with a
single instance, which is called the Cbench benchmark. To do
this, we evaluate Athena using Cbench’s throughput mode with
10
Fig. 11. Average CPU usage while handling ﬂow events with/without Athena.
the ONOS’s recommended settings [38] and summarize results
in Table IX. On average Athena has 53.13% lower throughput
and in the worst-case has 86.13% performance degradation.
However, without DB operations, Athena induces only 20%
performance degradation.
2) CPU usage with/without Athena: The overhead of
Athena is dependent on how much information is in the SDN
stack, including the control and data plane, as it passively
monitors and analyzes them both. To compute the monitoring
overhead of handling network events from SDN stacks, we
conducted experiments to measure the CPU load when using
ONOS with and without Athena. To do this, we established a
testing environment with a controller on the hexa-core Xeon
server, which is connected to the six physical switches, and
12 OVS instances on the two hexa-core Xeon and the two i5
servers respectively with dummy ﬂows to generate monitoring
events. Figure 11 illustrates the testing results. Since Athena
stores events to the data plane while maintaining internal
status to generate stateful features, the ﬂow handling overhead
increases according to the total number of ﬂow entries in the
switches. We ﬁnd that ONOS with Athena saturates at about
140K ﬂows per second, while the CPU utilization is about
31% for the basic ONOS instance.
3) Discussion: We found that the performance overhead
of our system primarily originates from MongoDB related
operations. To boost Athena‘s performance, we will consider
replacing MongoDB with a high-performance database like
Cassandra [43].
VIII. RELATED WORK
We now discuss how prior projects have sought to address
the challenges described in Section II, including various limita-
tions in their coverage. Table X provides a comparison between
Athena and existing work related to network anomaly detection
and monitoring in SDN environments.
Anomaly detection strategies: The inherent centralized
control-layer design of SDNs enables operators an efﬁcient po-
tential network-wide choke-point from which to gather a wide
range of network features. In fact, several prior anomaly detec-