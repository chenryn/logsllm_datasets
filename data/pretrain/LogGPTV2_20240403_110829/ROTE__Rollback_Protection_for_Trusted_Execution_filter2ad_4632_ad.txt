Figure 7: Transition diagram showing enclave execution
states using an ideal secure counter storage functionality.
all-or-nothing rollback. The only way to violate enclave
data integrity is to reset all nodes which brings the entire
group to its initial state. In many application scenarios
such integrity violation is easy to detect, and we do not
consider it an attack on ROTE.
In the event of crashes, restarts or node unavailabil-
ity, the system may fail to proceed temporarily or perma-
nently. We distinguish three such cases: Halt-1 where
the system may be able to proceed automatically by sim-
ply trying again later (e.g., temporary network issue);
Halt-2 where manual intervention from the system ad-
ministrator is needed (e.g., faulty node that needs to be
ﬁxed); and Halt-X where the complete system has to be
re-initialized and the latest state of enclaves will be lost
(e.g., simultaneous crash of all nodes). Recall that as the
adversary controls the OS on all nodes, denial of service
is always possible.
5.1 Protection with Secure Storage
Given the secure counter storage functionality (see
Section 3) rollback can be prevented using the inc-then-
store technique.
In Figure 7 we illustrate a state tran-
sition diagram that represents RE states during sealing,
unsealing and memory reading using the secure storage
functionality. The notion of state in this section is an ex-
ecution state, in contrast to enclave data states created
and stored using sealing. We show that any combination
of adversary operations, in any of the enclave execution
states, cannot force the RE to accept a previous version
of sealed data. We also show that in spite of multiple
local RE instances, the read enclave state is always the
latest. Note that this state transition diagram does not
capture system initialization.
First start. After creating and starting the enclave us-
ing e ← Create(code) and i ← Start(e), the RE exe-
cution begins from State 1. The MC is set to zero in
the runtime memory and RE proceeds to State 2. The
RE reads the counter value from the secure storage using
ReadCounter().
If the ReadCounter() operation fails,
the RE halts (Halt-1). On the ﬁrst execution the oper-
ation returns empty and the RE continues to State 7 to
continue normal operation. From State 7 the execution
1298    26th USENIX Security Symposium
USENIX Association
unseal12Start()7valueask sealfailhalt4OfferSeal(latest)56OfferSeal(arbitrary)OfferSeal(previous)8910incrementWrite-Counter()failokSeal()ok3ready to update statenormaloperationfailcounter matchunsealunsealcheck countercounter matchemptyReadCounter()failFigure 8: Network partitioning example where the ad-
versary intentionally blocks a part of the nodes.
and creating the same enclave again has the same effect.
Suspend() and Resume() have no effect, i.e., the enclave
remains in the same execution state. We conclude that,
assuming the secure storage functionality, the adversary
cannot rollback the state of the RE.
5.2 Distributed Secure Storage Realization
Next, we show how ROTE realizes the secure counter
storage functionality as a distributed system. When ob-
taining a counter from the distributed protection group
(ReadCounter), RE receives the latest value that was sent
to the protection group (WriteCounter). We divide the
analysis into four parts: quorum size, platform resets,
two-phase counter writing, and forking attacks.
Quorum size. The ROTE system has three parameters:
the number of assisting nodes n, compromised nodes f ,
and unresponsive nodes u. The required quorum for re-
sponses at the time of counter writing and reading is
q = f + u + 1 = n+ f +1
. Figure 8 illustrates that this is
an optimal quorum size. We consider an example where
the adversary performs network partitioning by blocking
messages during writing and reading.
2
On the ﬁrst write, the attacker allows the counter value
1 to reach the right side of the group by blocking the
messages sent to the left side. On the second write, the
adversary allows the counter value 2 to reach the left
side of the group by blocking the right side. Finally, on
counter read, the adversary blocks the left side again. If
the counter is successfully written to q = f +u+1 nodes,
there always exists at least u + 1 honest platforms in the
group that have the latest counter value in the memory.
Because counter reading requires the same number of re-
sponses, at least one correct counter value is obtained
upon reading. The maximum number of tolerated com-
promised platforms is f = n−1, if u = 0 and q = n. If the
quorum cannot be satisﬁed in either the state update pro-
tocol or any counter retrieval, the system enters Halt-1
and can try to perform the same operation again.
Platform restarts. If an assisting RE is restarted, it
needs to ﬁrst establish session keys and then recover the
lost MC values from the protection group. Session key
establishment procedure is explained below under Fork-
ing attacks; the main take-away is that up to u nodes
may restart simultaneously and after the nodes are online
again the RE needs to establish session keys with every
node in the group before proceeding with MC recovery.6
Once the keys are established, some assisting nodes can
be inactive or restarted. Three distinct cases are possible.
First, the number of inactive/restarted REs is at most u.
Since the number of running nodes is u + f +1 = q there
are sufﬁcient available platforms with the correct MC for
the counter retrieval. Second, more than u platforms, but
not the entire protection group, are restarted. The num-
ber of remaining platforms is insufﬁcient for RE recovery
and the distributed system no longer provides success-
ful MC access, but no rollback is possible (Halt-X, since
there is no guarantee that the non-restarted nodes have
the latest counter, thereby risking a rollback. However,
before re-initializing the system, the latest states from the
non-restarted nodes can be manually saved.) Third, all
n + 1 nodes are restarted at the same time, in which case
a new system conﬁguration has to be deployed again by
the group owner to re-initialize the system (Halt-X).
Two-round counter writing. Additionally, it remains
to be shown how our update protocol successfully writes
the counter to q nodes, despite possible RE restarts dur-
ing the protocol. We illustrate the challenges of counter
writing through an example attack on a single-round
variant of the update protocol that completes after the
RE has received q echoes. During state update the adver-
sary blocks all communication and performs sequential
message passing. First, the attacker allows message de-
livery to only one node that saves the counter and returns
an echo. After that, the attacker restarts the RE on that
node, which initiates the recovery procedure from the
rest of the protection group. The adversary blocks the
communication to the target platform, and the restarted
RE recovers the previous counter value, because other
reachable REs have not yet received the new value. The
adversary repeats the same process for all platforms. As
a result, the target node has received q echos and accepts
the state update, but all the assisting nodes have the pre-
vious counter value. Rollback is possible.
The second communication round in our protocol pre-
vents such attacks. No combination of RE restarts during
the state update protocol allows the target RE to complete
it, unless the counter was written to q nodes. There are
four distinct cases to consider. Below, we assume that the
adversary restarts at most u platforms simultaneously. If
more are restarted, recovery is not possible (Halt-X).
• Case 1: Echo blocking. If the attacker blocks commu-
nication or restarts assisting REs so that q nodes cannot
send the echo, the protocol does not complete (Halt-1).
6Consider an example, where two nodes are restarted at the same
time. The ﬁrst node wakes up and attempts to establish new session
keys with all assisting nodes. This node has to wait, until the second
restarted node wakes up and can communicate. After this point, both
of the restarted nodes can establish session keys (with all nodes) and
proceed with the RE Restart protocol.
USENIX Association
26th USENIX Security Symposium    1299
ulf1urstate=1state=2state update: 1state update: 2state retrieval1target REnq• Case 2: No echo blocking. If the attacker allows at
least q echoes to pass, RE starts returning them and we
have two cases to observe:
• Case 2a: No restarts during ﬁrst round. If none of
the assisting REs were restarted during the ﬁrst protocol
round, then at least u + 1 nodes have the updated MC. If
the adversary restarts assisting REs before they sent the
ﬁnal ACK and after they received the self-sent echo back
from the target RE, the protocol will not complete (Halt-
1), because fewer than q ﬁnal ACKs will be received.
The protocol run may be repeated again. The adversary
can also restart assisting REs after they have sent the ﬁ-
nal ACK which will result in successful state update, and
successful state recovery of the restarted REs since a suf-
ﬁcient number of the assisting nodes already have the up-
dated counter value.
• Case 2b: Restarts during ﬁrst round. If the adversary
restarts assisting REs during the ﬁrst round, the update
protocol will either successfully complete (q ﬁnal ACKs
received) or halt execution (Halt-1) depending on the
number of simultaneously restarted nodes. Sequential
node restarts, as discussed in the example attack above,
are detected. Upon receiving q echoes, the RE sends each
of the received echoes to the original sender. Because of
sequential RE restarts, all assisting nodes have the pre-
vious MC value in their runtime memory, and thus the
protocol will fail upon comparison of the echoes and the
MC values. None of the assisting REs will deliver the
ﬁnal ACK, and the protocol will not complete (Halt-1).
We conclude that the successful completion of the
two-phase state update protocol guarantees that at least
q nodes received and at least u + 1 honest nodes have
(i.e., correctly stored) the correct MC.
Forking attacks. Our system prevents attacks based
on multiple enclave instances by requiring that the ASE
start/read and RE restart protocols contact the assisting
nodes and verify the latest counter from the protection
group. If the latest counter is correct, RE can be certain
that it made the last update. If the session’s keys are out-
dated, communication with other nodes is disabled and
RE knows another instance has run in parallel.
The session key refresh mechanism allows us to
uniquely identify the latest running instance and prevents
parallel communication with two instances running on
one platform. After every RE start, keys have to be es-
tablished with all nodes from the protection group to pre-
vent the attacker from instantiating new REs on different
platforms in a one-by-one manner while keeping some
of the nodes disconnected. Other nodes delete the old
session key that they shared with the previous instance
residing on the same platform, rendering its communica-
tion unusable. The protection group only allows keys for
one running instance on each platform. Also, by forcing
state retrieval and freshness veriﬁcation after each instan-
tiation and for all ASE requests, the running instance on
each platform will always have the latest state and high-
est MC, thus preventing rollback.
Our system also ensures that the adversary cannot es-
tablish a parallel protection group on the same platforms
and re-direct ASEs to the rogue system causing a roll-
back. If no initialization key is provided and the RE re-
ceives all zero MC values from others in the group during
setup, it will abort execution. A new network may only
be created under the supervision of the group owner with
the correct initialization key.
Summary. If the target RE has the latest MC that it
sent, it is able to distinguish its latest sealed state, and if
the latest sealed state is loaded, all the ASEs state coun-
ters kept within are fresh. Upon retrieval, the ASE al-
ways receives the latest counter, and thus each ASEs can
verify that it has the latest state data. If the target RE is
not able to recover the latest MC, the system end ups in
either Halt-1, Halt-2 or Halt-X.
6 Performance Evaluation
In this section we describe our performance evalua-
tion. First, we describe our implementation that con-
sists of the following components. We implemented the
RE (950 LoC), an accompanying rollback relay appli-
cation (1600 LoC), ROTE library (150 LoC), a simple
test ASE (100 LoC), and a matching test relay applica-
tion (100 LoC). The purpose of the relays is to mediate
enclave-to-enclave communication. We implemented all
components in C++, the relays were implemented for the
Windows platform. The local communication between
the relay applications was implemented using Windows
named pipes. The total TCB accounts for 1100 LoC.
The enclaves use asymmetric cryptography for signing
(ECDSA) and encryption (256-bit ECC). Our implemen-
tation establishes shared keys using authenticated Difﬁe-
Hellman key exchange. For symmetric message encryp-
tion and authentication we use 128-bit AES-GCM in
encrypt-then-MAC mode. All used cryptographic primi-
tives are provided by the standard Intel SGX libraries.
6.1 State Update and Read Delay
The main performance metrics that we measure are the
ASE state update and state read delays that include the
counter writing to and reading from the protection group.
The delays depends on the network characteristics and
the size of the protection group (n + 1). The RE restart
operation is typically performed once per platform boot,
and thus the operation is not similarly time-critical so we
do not measure it. In all test cases we set u = f = 0, as
their values do not affect state update and read delays.7
7The state update protocol proceeds immediately after receiving q
responses, and therefore node unavailability does not affect update de-
1300    26th USENIX Security Symposium
USENIX Association
(a) 1st exp. setup (ROTE implementation)
(c) 3rd exp. setup (global group, simulated)
Figure 9: Experimental results, state update/read delay. The ﬁrst ﬁgure shows ROTE performance for protection groups
that are connected over a local network, the second ﬁgure shows the simulated performance for a larger group also
over a local network, while the third ﬁgure is for geographically distributed protection groups.
(b) 2nd exp. setup (local group, simulated)
Experimental setup. Our ﬁrst experimental setup
consisted of four SGX laptops and our second exper-
imental setup consisted of 20 identical desktop com-
puters, both connected via local network (1Gbps, ping
≤ 1ms). Our third experimental setup was a geograph-
ically distributed (in order, US (West), Europe, Asia, S.
America, Australia, US (East)) protection group of sizes
from two to six nodes that we tested on Amazon AWS
EC2. For the ﬁrst setup we used the real ROTE imple-
mentation while the latter two we used a simulated im-
plementation (the same protocol, but no enclaves).
Results. The state update delay consists of two com-
ponents: networking and processing overhead. Context
switching to enclave execution is fast (few microsec-
onds). Symmetric encryption used in the protocol is also
fast (less than a microsecond). The only computationally
expensive operation that we use is asymmetric signatures
(0.46 ms per signing operation). We provide more per-
formance benchmarks in [20].
The ASE state update protocol has one signature cre-
ation which is veriﬁed later in the RE and ASE start/read
protocols. The required processing time of the state up-
date protocol is less than 0.6 ms, where the creation of
the ﬁrst protocol message takes 0.51 ms (signing). The
state read protocol requires one round trip, while the state
update protocol needs two. All messages passed between
the nodes are 224 bytes (200 payload + 24 header).
Figure 9 shows the results from our three experimen-
tal setups. Figure 9a shows that the state update delay
was approximately 2 ms, while the state read delay was
approximately 1.3 ms for group sizes from two to four
nodes using the ROTE implementation. Figure 9b illus-
trates an increase in the delay as the group size grows.
This is as expected, since the target platform needs to
communicate with more platforms. For group size of 20
nodes, the delay is 2.98 ms and 2.13 ms, respectively.
Lastly, Figure 9c illustrates a less systematic increase in
lay. Similarly, up to f compromised nodes can discard counter values
or return fake values, but that does not affect the protocol delay.
ROTE
system
(ms)
(ms)
(ms)
Request
type
Write
state
Read
state
State
size
(KB)
1
10