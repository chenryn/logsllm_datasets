more than 20 ratings were sampled, and the points on
the graph to the left of X = 20 are the result of some
movies being deleted after sampling.
We requested the rating history as presented on the
Netﬂix website from some of our acquaintances, and
based on this data (which is effectively drawn from Net-
ﬂix’s original, non-anonymous dataset, since we know
the names associated with these records), located two of
them in the Netﬂix Prize dataset. Netﬂix’s claim that
the data were perturbed does not appear to be borne out.
One of the subscribers had 1 of 306 ratings altered, and
the other had 5 of 229 altered. (These are upper bounds,
because the subscribers may have changed their ratings
after Netﬂix took the 2005 snapshot that was released.)
In any case, the level of noise is far too small to affect
our de-anonymization algorithm, which has been specif-
ically designed to withstand this kind of imprecision.
We have no way of determining how many dates were
altered and how many ratings were deleted, but we con-
jecture that very little perturbation has been applied.
It is important that the Netﬂix Prize dataset has been
released to support development of better recommenda-
tion algorithms. A signiﬁcant perturbation of individ-
ual attributes would have affected cross-attribute corre-
lations and signiﬁcantly decreased the dataset’s utility
for creating new recommendation algorithms, defeating
the entire purpose of the Netﬂix Prize competition.
Note that the Netﬂix Prize dataset clearly has not
been k-anonymized for any value of k > 1.
Figure 2. For each X ≤ 100, the number of
subscribers with X ratings in the released
dataset.
De-anonymizing the Netﬂix Prize dataset. We apply
Algorithm Scoreboard-RH from section 4. The simi-
larity measure Sim on attributes is a threshold function:
Sim returns 1 if and only if the two attribute values are
within a certain threshold of each other. For movie rat-
ings, which in the case of Netﬂix are on the 1-5 scale,
we consider the thresholds of 0 (corresponding to exact
match) and 1, and for the rating dates, 3 days, 14 days,
or ∞. The latter means that the adversary has no infor-
mation about the date when the movie was rated.
Some of the attribute values known to the attacker
may be completely wrong. We say that aux of a record
Figure 3. For each X ≤ 1000, the number of
subscribers with X ratings in the released
dataset.
119
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:12:32 UTC from IEEE Xplore.  Restrictions apply. 
Figure 4. Adversary knows exact ratings
and approximate dates.
r consists of m movies out of m(cid:1)
(cid:1)
is non-null for each auxi, and
We instantiate the scoring function as follows:
, ri
i Sim(auxi, ri) ≥ m.
if |aux| = m(cid:1)
Score(aux, r(cid:1)
) =
(cid:2)
i∈supp(aux)
wt(i)(e
ρi−ρ(cid:1)
ρ0 + e
i
di−d(cid:1)
d0
i
)
1
i and d(cid:1)
log |supp(i)| (|supp(i)| is the number of
where wt(i) =
subscribers who have rated movie i), ρi and di are the
rating and date, respectively, of movie i in the auxil-
iary information, and ρ(cid:1)
i are the rating and date
in the candidate record r(cid:1)
.5 As explained in section 4,
this scoring function was chosen to favor statistically un-
likely matches and thus minimize accidental false posi-
tives. The parameters ρ0 and d0 are 1.5 and 30 days, re-
spectively. These were chosen heuristically, as they gave
the best results in our experiments,6 and used through-
out, regardless of the amount of noise in Aux. The ec-
centricity parameter was set to φ = 1.5, i.e., the algo-
rithm declares there is no match if and only if the differ-
ence between the highest and the second highest scores
is no more than 1.5 times the standard deviation. (A con-
stant value of φ does not always give the equal error rate,
but it is a close enough approximation.)
5wt(i) is undeﬁned when |supp(i)| = 0, but this is not a concern
since every movie is rated by at least 4 subscribers.
6It may seem that tuning the parameters to the speciﬁc dataset may
have unfairly improved our results, but an actual adversary would have
performed the same tuning. We do not claim that these numerical
parameters should be used for other instances of our algorithm; they
must be derived by trial and error for each target dataset.
120
Figure 5. Same parameters as Fig. 4, but
the adversary must also detect when the
target record is not in the sample.
Didn’t Netﬂix publish only a sample of the data? Be-
cause Netﬂix published less than 1
10 of its 2005 database,
we need to be concerned about the possibility that when
our algorithm ﬁnds a record matching aux in the pub-
lished sample, this may be a false match and the real
record has not been released at all.
Algorithm Scoreboard-RH is speciﬁcally designed
to detect when the record corresponding to aux is not
in the sample. We ran the following experiment. First,
we gave aux from a random record to the algorithm and
ran it on the dataset. Then we removed this record from
the dataset and re-ran the algorithm. In the former case,
the algorithm should ﬁnd the record; in the latter, de-
clare that it is not in the dataset. As shown in Fig. 5, the
algorithm succeeds with high probability in both cases.
It is possible, although extremely unlikely, that the
original Netﬂix dataset is not as sparse as the published
sample, i.e., it contains clusters of records which are
close to each other, but only one representative of each
cluster has been released in the Prize dataset. A dataset
with such a structure would be exceptionally unusual
and theoretically problematic (see Theorem 4).
If the adversary has less auxiliary information than
shown in Fig. 5, false positives cannot be ruled out a pri-
ori, but there is a lot of extra information in the dataset
that can be used to eliminate them. For example, if the
start date and total number of movies in a record are part
of the auxiliary information (e.g., the adversary knows
approximately when his target ﬁrst joined Netﬂix), they
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:12:32 UTC from IEEE Xplore.  Restrictions apply. 
 20
 18
 16
 14
 12
 10
 8
 6
 4
 2
y
p
o
r
t
n
E
Entropy per movie by rank
No ratings or dates
Ratings +/- 1
Dates +/- 14
 0
 1
 10
 100
 1000
 10000
 100000
Rank
Figure 7. Entropy of movie by rank
demonstrate how much information the adversary gains
about his target just from the knowledge that the tar-
get watched a particular movie as a function of the rank
of the movie.7 Because there are correlations between
the lists of subscribers who watched various movies, we
cannot simply multiply the information gain per movie
by the number of movies. Therefore, Fig. 7 cannot be
used to infer how many movies the adversary needs to
know for successful de-anonymization.
As shown in Fig. 8, two movies are no longer suf-
ﬁcient for de-anonymization, but 84% of subscribers
present in the dataset can be uniquely identiﬁed if the
adversary knows 6 out of 8 moves outside the top 500.
To show that this is not a signiﬁcant limitation, consider
that most subscribers rate fairly rare movies:
Not in X most rated % of subscribers who rated . . .
≥ 10
93%
80%
70%
≥ 1 movie ≥ 5
100% 97%
99% 90%
97% 83%
X = 100
X = 500
X = 1000
Fig. 9 shows that the effect of relative popularity of
movies known to the adversary is not dramatic.
In Fig. 10, we add even more noise to the auxiliary
7We measure the rank of a movie by the number of subscribers who
have rated it.
121
Figure 6. Entropic de-anonymization:
same parameters as in Fig. 4.
can be used to eliminate candidate records.
Results of de-anonymization. We carried out the ex-
periments summarized in the following table:
Fig Ratings
4
Exact
Exact
5
Exact
6
8
Exact
±1
9
±1
10
11
Exact
±1
12
Type
Dates
±3/ ±14 Best-guess
±3/ ±14 Best-guess
±3/ ±14
Entropic
Best-guess
No info.
±14
Best-guess
±14
Best-guess
Entropic
No info.
±14
Best-guess
Aux selection
Uniform
Uniform
Uniform
Not 100/500
Uniform
Uniform
Not 100/500
Uniform
Our conclusion is that very little auxiliary informa-
tion is needed for de-anonymize an average subscriber
record from the Netﬂix Prize dataset. With 8 movie rat-
ings (of which 2 may be completely wrong) and dates
that may have a 14-day error, 99% of records can be
uniquely identiﬁed in the dataset. For 68%, two ratings
and dates (with a 3-day error) are sufﬁcient (Fig. 4).
Even for the other 32%, the number of possible can-
didates is brought down dramatically. In terms of en-
tropy, the additional information required for complete
de-anonymization is around 3 bits in the latter case (with
no auxiliary information, this number is 19 bits). When
the adversary knows 6 movies correctly and 2 incor-
rectly, the extra information he needs for complete de-
anonymization is a fraction of a bit (Fig. 6).
Even without any dates, a substantial privacy breach
occurs, especially when the auxiliary information con-
sists of movies that are not blockbusters. In Fig. 7, we
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:12:32 UTC from IEEE Xplore.  Restrictions apply. 
Figure 8. Adversary knows exact ratings
but does not know dates at all.
Figure 9. Effect of knowing less popular
movies rated by victim. Adversary knows
approximate ratings (±1) and dates (14-
day error).
information, allowing mistakes about which movies the
target watched, and not just their ratings and dates.
Fig. 11 shows that even when the adversary’s proba-
bility to correctly learn the attributes of the target record
is low, he gains a lot of information about the target
record. Even in the worst scenario, the additional in-
formation needed to to complete the de-anonymization
has been reduced to less than half of its original value.
Fig. 12 shows why even partial de-anonymization can
be very dangerous. There are many things the adver-
sary might know about his target that are not captured
by our formal model, such as the approximate number of
movies rated, the date when they joined Netﬂix and so
on. Once a candidate set of records is available, further
automated analysis or human inspection might be sufﬁ-
cient to complete the de-anonymization. Fig. 12 shows
that in some cases, knowing the number of movies the
target has rated (even with a 50% error!) can more than
double the probability of complete de-anonymization.
Obtaining the auxiliary information. Given how lit-
tle auxiliary information is needed to de-anonymize the
average subscriber record from the Netﬂix Prize dataset,
a determined adversary who targets a speciﬁc individual
may not ﬁnd it difﬁcult to obtain such information, es-
pecially since it need not be precise. We emphasize that
massive collection of data on thousands of subscribers