that condition c must continue to hold downstream un-
til the declassiﬁcation condition c’ holds, this rule can
be satisﬁed in one of two ways: Either the declassiﬁca-
tion condition c’ holds now, or c holds now and the next
downstream conduit (f here) continues to enforce (c until
c’). Line 12 makes exactly this check.
of policy
enforcement
End-to-end correctness
Within Thoth’s threat model,
the checks described
above enforce all policies on conduits and, speciﬁcally,
Incorrect policy conﬁguration on
all ingress policies.
internal conduits cannot cause violation of
ingress
policies but may cause compliant data ﬂows to be denied
by the Thoth reference monitor. Informally, this holds
because our checks ensure that the conditions in every
declassiﬁcation policy are propagated downstream until
they are satisﬁed. 5
Policy comparison Thoth compares policies for re-
strictiveness in three cases: for taint compression, when
evaluating the predicate isAsRestrictive(), and in line
12 of the enforcement algorithm (Figure 3). The gen-
eral comparison problem is undecidable for ﬁrst-order
logic, so Thoth uses the following heuristics: 1) Equal-
ity: Compare the hashes of the two policies. 2) Inclu-
sion: Check that all predicates in the less restrictive pol-
icy also appear in the more restrictive one, taking into
account variable renaming and conjunctions and disjunc-
tions between the predicates. Inclusion has exponential
time complexity in the worst case, but is fast in practice.
3) Partial evaluation: Evaluate and delete an application-
speciﬁed subpart of each policy, then try equality and
inclusion. These heuristics sufﬁce in all cases we have
encountered.
Note that a policy comparison failure can never affect
Thoth’s safety. However, a failure can (a) defeat taint
compression and therefore increase taint size and policy
evaluation overhead; or (b) cause a compliant data ﬂow
to be denied. In the latter case, a policy designer may
re-state a policy so that the policy comparison succeeds.
3.4 Thoth API
Table 2 lists Thoth API functions provided to user-level
tasks by means of a new system call. To check structural
properties of written data (e.g., that the data is a list of
conduit ids), it is often necessary to evaluate the update
rule atomically on a batch of writes. Hence, Thoth sup-
ports write transactions on conduits. By default, a trans-
action starts with a POSIX open() call and ends with the
close() call on a conduit. This behavior can be overridden
by passing additional ﬂags to open() and close(). Trans-
actions can also be explicitly started and ended using the
Thoth API calls open_tx and close_tx.
5A formal proof of this fact is the subject of a forthcoming paper.
Our formal model and implementation support nested uses of the until
operator, which we omitted here.
During a transaction, Thoth buffers writes in a per-
sistent re-do log. When the transaction is closed by the
application, Thoth makes the policy checks described in
Figure 3. If the policy checks succeed, then the writes
are sent to the conduit, else the writes are discarded. The
re-do log allows recovery from crashes and avoids ex-
pensive ﬁlesystem syncs when a transaction commits.
Summary Thoth enforces ingress and egress policies
despite application-level bugs, misconﬁgurations, and
compromises, or actions by unprivileged operators. A
data source’s policy speciﬁes both access and declassi-
ﬁcation conditions and describes the source’s allowed
uses completely. Thoth uses policies as taint, which dif-
fers signiﬁcantly from the standard information ﬂow con-
trol practice of using abstract labels as taint. That prac-
tice requires trusted application processes to declassify
data and to control access at system edges. In contrast,
Thoth relies entirely on its reference monitor for all ac-
cess and declassiﬁcation checks, and no application pro-
cesses have to be trusted.
4 Thoth prototype
Our prototype consists of a Linux kernel module that
plugs into the Linux Security Module (LSM) interface,
and a reference monitor. We also changed a few (22)
lines of the kernel proper to provide additional system
call interception hooks not included in the LSM inter-
face, and a new system call that allows applications to
interact with Thoth. A small application library consist-
ing of 840 LoC exports the API calls shown in Table 2
based on this system call.
LSM module The Thoth LSM module comprises ap-
proximately 3500 LoC and intercepts I/O related system
calls including open, close, read, write, socket, mknod,
mmap, etc. Intercepted system calls are redirected to the
reference monitor for taint tracking and validation. The
module includes conduit interceptors for ﬁles, named
pipes and sockets, as well as interceptors for client con-
nections to a memcached key-value store [12].
Thoth reference monitor Thoth’s reference monitor
is implemented as a trusted, privileged userspace pro-
cess.
It implements the policy enforcement logic and
maintains the process taint, session state and transac-
tion state in DRAM. The monitor accesses the persistent
Thoth metadata store, which includes per-conduit meta-
data (conduit pathname, conduit id, a pointer to the pol-
icy in effect in the policy store, and for each persistent
ﬁle conduit, its current size), the transaction log, and the
global policy store. The metadata and transaction log are
stored in NVRAM. A write-through DRAM cache holds
recently accessed metadata and policies.
The monitor is multi-threaded so it can exploit multi-
core parallelism. Each worker thread invokes the Thoth
USENIX Association  
25th USENIX Security Symposium  643
Function
conﬁne ()
authenticate (key)
add_policy (p)
set_tx_ﬂags (c_id, ﬂags)
open_tx (c_id)
close_tx (fd)
set_policy (fd, p_id)
get_policy (c_id, buf)
cache (fd, off, len)
Description
Transition calling process from UNCONFINED to CONFINED state.
Authenticate process with the private key key to satisfy identity-based policies.
Store a policy p in Thoth metadata and return an id p_id for it.
Set ﬂags ﬂags (type and partial evaluation hints) for a transaction on conduit c_id.
Open a transaction on conduit c_id and return a ﬁle handle.
Close a transaction fd. Return 0 if successful, or error code of a policy check fails.
Attach policy id p_id to the conduit running transaction fd. Passing (-1) for p_id sets the null policy.
The new policy is applied only after fd is successfully closed. The declassiﬁcation condition of the
conduit’s existing policy determines whether the policy change or removal is allowed.
Retrieve the policy attached to conduit c_id into buffer buf.
Cache content (for policy evaluation) from ﬁle handle fd from offset off with length len.
Table 2: Thoth API calls
system call and normally blocks in the LSM module
waiting for work. When an application issues a system
call that requires an action by the reference monitor, a
worker thread is unblocked and returns to the reference
monitor with appropriate parameters; when the work is
done, the thread invokes the system call again with the
appropriate results causing the original application call
to either be resumed or terminated. As an optimization,
the LSM seeks to amortize the cost of IPC by buffer-
ing and dispatching multiple asynchronous requests to a
worker thread whenever possible. The reference monitor
was implemented in 19,000 LoC of C, not counting the
OpenSSL library used for secure sessions and crypto.
Limitations Memory-mapped ﬁles are currently sup-
ported read-only. Interception is not yet implemented for
all I/O-related system calls. None of these missing fea-
tures are used by our prototype data retrieval system.
5 Policy-compliant data retrieval
We use Thoth for policy compliance in a data retrieval
system built around a distributed Apache Lucene search
engine. While Apache Lucene’s architecture is not ap-
propriate for large, public search engines like Google or
Bing, it is frequently used in smaller, domain-speciﬁc
data retrieval systems.
5.1 Baseline conﬁguration
Lucene Apache Lucene is an open-source search en-
gine written in Java [2].
It consists of an indexer and
a search component. The sequential indexer is a single
process that scans a corpus of documents and produces
a set of index ﬁles. The search component consists of
a multi-threaded process that executes search queries in
parallel and produces a set of corpus ﬁle names relevant
to a given search query. The size of the Apache Lucene
codebase is about 300,000 LoC.
Lucene can be conﬁgured with replicated search pro-
cesses to scale its throughput. Here, multiple nodes run
a copy of the search component, each with the full in-
dex. A search query can be processed by any machine.
Lucene can also be sharded to scale with respect to the
corpus size. In this case, the corpus is partitioned, each
partition is indexed individually, and multiple nodes run
a copy of the search component, each with one partition
index. A search query is sent to all search components,
and the results combined. Replication and sharding can
be combined in the obvious way.
Front-end processes A simple front-end process ac-
cepts user requests from a remote client and forwards
search queries to one or more search process(es) via a
pipe. The search process(es) may forward the query to
other search processes with disjoint shards. When the
front-end receives the search results (a list of document
ﬁle names), it produces a HTML page with a URL and
a content snippet from each of the result documents, and
returns the page to the Web client. When the client clicks
on one of the URLs, the front-end serves the content.
A second, simple account manager front-end process
accepts connections from clients for the purpose of cre-
ating accounts, managing personal proﬁles and policies.
Clients choose from a set of policy templates for docu-
ments they have contributed to the corpus, and for their
personal proﬁle information and activity history.
Search personalization and advertising To include
typical features of a data retrieval system, we added per-
sonalized search and targeted advertising components. A
memcached daemon runs on each search node to provide
a distributed key-value store for per-user information, in-
cluding a sufﬁx of the search and click histories, proﬁle
information, and the public key. The front-end process
appends a user’s search queries and clicks to the his-
tories. It uses the proﬁle information to rewrite search
queries, re-order search results, and select ads for inclu-
sion in the results page.
An aggregator process periodically analyses a user’s
search and click history, and updates the personal pro-
ﬁle information accordingly. We are not concerned with
the details of user proﬁling, personalized search, or ad
644  25th USENIX Security Symposium 
USENIX Association
targeting. It sufﬁces for our purposes to capture the ap-
propriate data ﬂows.
5.2 Controlling data ﬂow with Thoth
With Thoth, the front-end, search, indexing, and aggre-
gation tasks execute as CONFINED processes, and the ac-
count manager executes as an UNCONFINED process. Rel-
ative to the baseline system, we made minimal modiﬁca-
tions, mostly to set an appropriate policy on output con-
duits. The modiﬁcations to Apache Lucene amounted to
less than 20 lines of Java code and 30 lines of C code
in a JNI library.
These modiﬁcations set policies on
internal conduits and, like the rest of Lucene, are not
trusted. Finding the appropriate points to modify was
relatively easy because Lucene’s codebase has separate
functions through which all I/O is channelled. For ap-
plications without this modularity, a dynamically-linked
library can be used that overrides libc’s I/O functions and
adds appropriate policies.
Unlike in the baseline, the front-end process must be
restarted after each user session, to drop its taint. We
implement this by exec-ing the process when a new user
session starts.
that
Ingress/egress policies Recall
the ingress and
egress policies determine which data ﬂows are allowed
and reﬂect
the policies of users, data sources, and
provider. In our system, the network connection between
the client and the front-end is both an ingress and an
egress conduit. The document ﬁles in the corpus and the
key-value tuples that contain a user’s personal informa-
tion are ingress conduits. Policies are associated with all
ingress and egress conduits as described below. The pri-
mary difﬁculty here is to determine appropriate policies,
a task that is required in any compliant system. Specify-
ing the policies in Thoth’s policy language is straightfor-
ward.
Account manager ﬂow When Alice creates an ac-
count, credentials are exchanged for subsequent mutual
authentication, and stored in the key-value store, along
with any personal proﬁle information Alice provides.
Alice can choose policies for her proﬁle and history
information, as well as any contributed content, typically
from a set of policy templates written by the provider’s
compliance team. The declassiﬁcation rule of each pol-
icy implicitly controls who can subsequently change the
policy; normally, Alice would choose a policy that allows
only her to make such a change. Alice may also edit her
friend lists or other access control lists stored in the key-
value store, which may be referenced by her policies.
Next, we explain the main data ﬂows through the sys-
tem. For lack of space, we cannot detail all policies on
internal conduits, but we highlight the key steps.
Indexing ﬂow Periodically,
the indexer is invoked
to regenerate the index partitions. A correct indexer
only processes documents with the ONLY_CND_IDS
(or ONLY_CND_IDS+) declassiﬁcation clause, which
is transferred to the index ﬁles. Note that the index
may contain arbitrary data and can be read by any
CONFINED process; however, an eventual declassiﬁcation
to an UNCONFINED process is only possible for a list of
conduit ids.
Proﬁle aggregation ﬂow A proﬁle aggregation task
periodically executes in the background, to scan the suf-
ﬁx of a user’s query and click history and update the
user’s proﬁle vector. A correct aggregator only ana-
lyzes user history data that has the ONLY_CND_IDS
(or ONLY_CND_IDS+) declassiﬁcation clause, which is
transferred to the proﬁle vectors.
Search ﬂow Finally, we describe the sequence of steps
when Alice performs a search query. The search front-
end authenticates itself to Alice using the credentials
stored in the key-value store. A successful authentica-
tion assures Alice that (i) she is talking to the front-end,
and (ii) the front-end process is tainted with the policy of
Alice’s credentials (only Alice can read, else declassify
into a list of conduit ids) before Alice sends her search
query. Next, Alice authenticates herself to the Thoth ref-
erence monitor via the search front-end, which proves to
Thoth that the front-end process speaks for Alice.
The front-end now sends Alice’s query to one or more
search process(es) and adds it to her search history. The
search results are declassiﬁed as a list of conduit ids, and
therefore do not add new taint to the front-end. While
producing the HTML results page, the front-end reads
a snippet from each result document using Alice’s cre-
dentials. Each document has a censorship policy, which
checks that the document’s conduit ID is not blacklisted
in the client’s region. These policies differ in the conduit
IDs and so, in principle, the taint set on the front-end
could become very large. To prevent this, we use par-
tial evaluation (Section 3): Before a document’s policy
is added to the front-end’s taint, we check that the doc-
ument is not blacklisted. This way, the front-end’s taint
increases by a single predicate (which veriﬁes Alice’s IP
address) when it reads the ﬁrst document and does not
increase when it reads subsequent documents.
Finally, the front-end sends the results page to the
client. For this, it must satisfy the egress conduit policy,
which veriﬁes Alice’s identity and her IP address.
Result caching High-performance retrieval systems
cache search results and content snippets for reuse in
similar queries. Although we have not implemented such
caching, it can be supported by Thoth. Intermediate re-
sults can be cached at various points in the data ﬂow, usu-
ally before their policies have been specialized (through
USENIX Association  
25th USENIX Security Symposium  645
partial evaluation) for a particular client or jurisdiction.
Summary Assuming that the account manager cor-
rectly installs ingress and egress policies, Thoth ensures
that Alice’s documents, history and proﬁle are used ac-
cording to her wishes and that the provider’s censorship
and MAL policies are enforced, despite any bugs in the
indexer, the front-end or the proﬁle aggregator. Thoth’s
use in a data retrieval system highlights two different
ways of preventing process overtainting. The front-end
process is user-speciﬁc—it acts on behalf of one client.
Consequently, the front-end must be re-execed at the end
of a user session session to discard its taint.
In con-
trast, the indexer is an aggregator process that is de-
signed to combine documents with conﬂicting policies
into a single index. To make its output (the index) usable
downstream, the provider installs a typed declassiﬁcation
clause (ONLY_CND_IDS or ONLY_CND_IDS+) on all
documents. Due to the declassiﬁcation clause, there is
no need to re-exec the search process.
6 Evaluation
In this section, we present results of an experimental
evaluation of our Thoth prototype.
All experiments were performed on Dell R410 servers,
each with 2x Intel Xeon X5650 2.66 GHz 6 hyper-
threaded core CPUs, 48GB main memory, running
OpenSuse Linux 12.1 (kernel version 3.13.1, x86-64).
The servers are connected to Cisco Nexus 7018 switches
with 1Gbit Ethernet links. Each server has a 1TB Sea-
gate ST31000424SS disk formatted under ext4, which
contains the OS installation and a 258GB static snapshot
of English language Wikipedia articles from 2008 [43].
We allocate a 2GB memory segment on /dev/shm to
simulate NVRAM used by Thoth to store its metadata
and transaction log. NVRAM is readily available and
commonly used to store frequently updated, ﬁxed-sized
persistent data structures like transaction logs.
In the following experiments, we compare a system
where each OS kernel is conﬁgured with the Thoth LSM
kernel module and reference monitor against an other-