# Delft University of Technology
## LogChunks: A Data Set for Build Log Analysis
### Authors
Carolin E. Brandt, Annibale Panichella, Andy Zaidman, Moritz Beller  
{c.e.brandt, a.panichella, a.e.zaidman, m.m.beller}@tudelft.nl  
Delft University of Technology, The Netherlands

### Publication Details
- **Publication Date:** 2020
- **Document Version:** Submitted manuscript
- **Published in:** Proceedings - 2020 IEEE/ACM 17th International Conference on Mining Software Repositories, MSR 2020
- **Pages:** 583-587
- **Citation (APA):**  
  Brandt, C. E., Panichella, A., Zaidman, A., & Beller, M. (2020). LogChunks: A Data Set for Build Log Analysis. In Proceedings - 2020 IEEE/ACM 17th International Conference on Mining Software Repositories, MSR 2020 (pp. 583-587).

### Important Notes
- To cite this publication, please use the final published version (if applicable). Please check the document version above.
- For personal use only, it is not permitted to download, forward, or distribute the text or part of it without the consent of the authors and/or copyright holders, unless the work is under an open content license such as Creative Commons.
- If you believe this document breaches copyrights, please contact us with details. We will remove access to the work immediately and investigate your claim.

### Abstract
Build logs are textual by-products generated during the software build process, often as part of Continuous Integration (CI) pipelines. These logs are crucial for developers when debugging and understanding build failures. Recent efforts have aimed to automate this time-consuming, manual activity using rule-based or information-retrieval techniques. We believe that having a common data set to compare different build log analysis techniques will advance the research area and enhance our understanding of CI build failures.

In this paper, we present LogChunks, a collection of 797 annotated Travis CI build logs from 80 GitHub repositories in 29 programming languages. Each build log in LogChunks contains a manually labeled chunk that describes why the build failed. We externally validated the data set with the developers who caused the original build failure. The breadth and depth of the LogChunks data set make it a valuable benchmark for automated build log analysis techniques.

### Keywords
CI, Build Log Analysis, Build Failure, Chunk Retrieval

### ACM Reference Format
Brandt, C. E., Panichella, A., Zaidman, A., & Beller, M. (2020). LogChunks: A Data Set for Build Log Analysis. In 17th International Conference on Mining Software Repositories (MSR '20), October 5â€“6, 2020, Seoul, Republic of Korea. ACM, New York, NY, USA, 5 pages.

### Introduction
Continuous Integration (CI) has become a standard practice in software engineering, used to detect bugs early, improve developer productivity, and enhance communication. CI builds generate logs that report the results of various sub-steps within the build process. These logs contain valuable information for developers and researchers, such as descriptions of compile errors or failed tests. However, build logs can be verbose and large, sometimes exceeding 50 MB of ASCII text, making them difficult for direct human consumption.

To support developers and researchers in efficiently using the information within build logs, we need at least semi-automated methods to retrieve the relevant chunks of the log. Different techniques, such as rule-based systems, custom parsers, and diffing between failed and successful logs, have been proposed. Each approach has its strengths and weaknesses, but there is currently no standardized data set to benchmark these techniques.

In this paper, we present LogChunks, a collection of 797 labeled Travis CI build logs from 80 popular GitHub repositories in 29 programming languages. We manually labeled the chunk that describes why each build failed, provided search keywords, and categorized the log chunks according to their format. This data set aims to serve as a benchmark for evaluating and comparing build log analysis techniques.

### Creating LogChunks
This section outlines the process of gathering and labeling the logs in LogChunks.

#### Log Collection
We targeted mature GitHub repositories using Travis CI, selecting popular projects based on the number of users who starred them. We queried GHTorrent for the most popular languages on GitHub and the most popular repositories for each language. For LogChunks, we queried GHTorrent for the three most popular repositories of each of the 30 most popular languages, covering a broad range of development languages. We collected the ten most recent failed builds from each repository, ensuring predictable termination of the log collection. We downloaded the log of the first job that had the same state as the overall build. After inspecting the collected build logs, we discarded logs from three repositories due to issues such as having only one failed build or empty build logs. In total, we collected 797 logs from 80 repositories spanning 29 languages.

#### Manual Labeling
The first author manually labeled the text chunk that describes why each build failed, preserving whitespaces and special characters. They also assigned search keywords and structural categories to each log chunk. The labeler noted down three strings they would search for to find the failure description, considering the chunk and ten lines above and below. Structural categories were assigned based on the consistent representation of the chunk within the log.

#### Validation
We validated our data points through an iterative process. First, we conducted an initial inter-rater reliability study with the second author. We learned that it is important and challenging to communicate all decisions and assumptions about how and which data to label. There can also be different legitimate viewpoints on which log chunk constitutes the primary error and which keywords are best to use. These insights informed the design of a larger cross-validation study, where we contacted over 200 developers.

In the second validation, we sent emails to the original developers whose commits triggered the builds represented in LogChunks, asking them to confirm whether the labeled log chunk accurately described the build failure. We received responses from 61 developers, corresponding to 144 build logs, with a response rate of 24.8%. After adjusting for some long chunks that were trimmed for readability, 94.4% of the extractions were confirmed as correct. This high external validation strengthens the trust in the validity of the labeled log chunks.

### Discussion
Our developer survey significantly enhances the trust in the validity of the labeled log chunks. The high response rate and the high percentage of correct answers validate the accuracy of our data set. While there is a potential threat that developers might find it easier to validate rather than search for the error message, we made it as easy to confirm as to reject an extracted log chunk. The few incorrect extractions were primarily due to minor issues, such as including warnings treated as errors or labeling ignored errors.

### Conclusion
LogChunks provides a comprehensive and validated data set for build log analysis, serving as a benchmark for comparing different techniques. This data set will help advance the research area and improve our understanding of CI build failures.