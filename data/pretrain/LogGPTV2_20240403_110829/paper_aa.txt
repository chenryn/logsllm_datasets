Delft University of Technology
LogChunks: A Data Set for Build Log Analysis
Brandt, Carolin E.; Panichella, Annibale; Zaidman, Andy; Beller, Moritz
Publication date 
2020
Document Version 
Submitted manuscript
Published in 
Proceedings - 2020 IEEE/ACM 17th International Conference on Mining Software Repositories, MSR 2020
Citation (APA)Citation (APA) 
Brandt, C. E., Panichella, A., Zaidman, A., & Beller, M. (2020). LogChunks: A Data Set for Build Log Analysis. In Proceedings - 2020 IEEE/ACM 17th International Conference on Mining Software Repositories, MSR 2020 (pp. 583-587). (ProceediConference on Mining Software Repositories, MSR 2020). 
Important note 
To cite this publication, please use the final published version (if applicable). Please check the document version above.Copyright 
Other than for strictly personal use, it is not permitted to download, forward or distribute the text or part of it, without the consent of the author(s) and/or copyright holder(s), unless the work is under an open content license such as Creative Commons.
Takedown policy 
Please contact us and provide details if you believe this document breaches copyrights. We will remove access to the work immediately and investigate your claim.This work is downloaded from Delft University of Technology.
For technical reasons the number of authors shown on this cover page is limited to a maximum of 10.
LogChunks: A Data Set for Build Log Analysis Carolin E. Brandt, Annibale Panichella, Andy Zaidman, Moritz Beller {c.e.brandt,a.panichella,a.e.zaidman,m.m.beller}@tudelft.nl 
Delft University of Technology 
The Netherlands
ABSTRACTBuild logs are textual by-products that a software build process creates, often as part of its Continuous Integration (CI) pipeline. Build logs are a paramount source of information for developers when debugging into and understanding a build failure. Recently, attempts to partly automate this time-consuming, purely manual activity have come up, such as rule- or information-retrieval-based techniques.We believe that having a common data set to compare different build log analysis techniques will advance the research area. It will ultimately increase our understanding of CI build failures. In this paper, we present LogChunks, a collection of 797 annotated Travis CI build logs from 80 GitHub repositories in 29 programming lan-guages. For each build log, LogChunks contains a manually labeled log part (chunk) describing why the build failed. We externally validated the data set with the developers who caused the original build failure.The width and depth of the LogChunks data set are intended to make it the default benchmark for automated build log analysis techniques.
KEYWORDS
CI, Build Log Analysis, Build Failure, Chunk Retrieval
ACM Reference Format: 
Carolin E. Brandt, Annibale Panichella, Andy Zaidman, Moritz Beller. 2020. LogChunks: A Data Set for Build Log Analysis. In 17th International Con-ference on Mining Software Repositories (MSR ’20), October 5–6, 2020, Seoul, Republic of Korea. ACM, New York, NY, USA, 5 pages. 
| 1 | INTRODUCTION |
|---|---|| Continuous Integration (CI) has become a common practice in software engineering [10]. Many software projects use CI [2, 10, 17] to detect bugs early [8, 18], improve developer productivity [10, 13] and communication [7]. CI builds produce logs which report results of various sub-steps within the build. These build logs contain a lot of valuable information for developers and researchers—for example, descriptions of compile errors or failed tests [2, 14, 20]. 	However, build logs can be verbose and large—sometimes in excess of 50 MB of ASCII text [2]—making them inadequate for |Continuous Integration (CI) has become a common practice in software engineering [10]. Many software projects use CI [2, 10, 17] to detect bugs early [8, 18], improve developer productivity [10, 13] and communication [7]. CI builds produce logs which report results of various sub-steps within the build. These build logs contain a lot of valuable information for developers and researchers—for example, descriptions of compile errors or failed tests [2, 14, 20]. 	However, build logs can be verbose and large—sometimes in excess of 50 MB of ASCII text [2]—making them inadequate for |Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from PI:EMAIL ’20, October 5–6, 2020, Seoul, Republic of Korea
© 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-7517-7/20/05...$15.00 
direct human consumption. Therefore, to support developers and researchers in efficiently making use of the information within build logs, we must at least semi-automatically retrieve the chunks of the log that describe the targeted information.There are different techniques to retrieve information chunks from CI build logs. Beller et al. use a rule-based system of regular expressions to analyze logs from Travis CI [2]. Such regular expres-sions are developed by looking at exemplary build logs. Vassallo et al. wrote a custom parser to gather information for build repair hints [19]. Recently, Amar et al. reduced the number of lines for a developer to inspect by creating a diff between logs from failed and successful builds [1].These approaches have various strengths and weaknesses: Reg-ular expressions are exact, but tedious and error-prone to main-tain [12]. Custom parsers are powerful though fragile in light of changes in the log structure. Diffing between failed and success-ful logs can reduce the information to be processed, but is at best semi-automatic [1].At the moment, there is only anecdotal evidence on the per-formance of these techniques, and when a technique should be preferred over its alternatives. In fact, there is no data set available to support the creation of such a benchmark for build log analysis techniques. Following Sim et al., a benchmark gives us the chance to “increase the scientific maturity of the area” [15] of build log analysis by evaluating and comparing research contributions.Thus, in this paper, we present LogChunks [4],1a collection of 797 labeled Travis CI build logs from 80 highly popular GitHub repositories in 29 programming languages with we manually la-beled the chunk describing why the build failed. The data set also provides keywords the authors would use to search for the labeled log chunk and categorizes the log chunks according to their format within the log.2 	CREATING LOGCHUNKS
This section presents how we gathered the logs and our manual labeling process.
2.1 	Log Collection
In this section, we describe along the overview in Figure 1 how we created LogChunks. All steps are automatized as Ruby scripts2and highly configurable.
Repository Sampling. We target mature GitHub repositories that are using Travis CI. To avoid personal and toy projects we se-lect popular projects based on the number of users that starred1LogChunks is openly available on Zenodo:2Data collection scripts and their original pe data set.
| MSR ’20, October 5–6, 2020, Seoul, Republic of Korea | Logs from  | Manual  | Structural  | Brandt, et al. |
|---|---|---|---|---|
| Popular  |Failed CI Builds |Manual  |Structural  |Brandt, et al. |
| Repositories |Failed CI Builds |Manual  |Structural  |Brandt, et al. || GitHub |Failed CI Builds |Labeling |Structural  |Brandt, et al. |
| Travis CI |Failed CI Builds |Labeling |Structural  |Brandt, et al. |
Category
| Chunk That Describes  | Search  | Failure:  | ===stderr===  |
|---|---|---|---|
| Chunk That Describes  |Keywords |Failure:  |===stderr===  |
| Why The Build Failed |Failure,  |xxx |xxx |
| FAIL: TestClientNotificationStorm (2.33s)  |Error,  |——— |===stdout=== ||  client_test.go:378: didn't get expected error |===DIFF=== |——— |===stdout=== |
Figure 1: Overview of LogChunks
a project [11]. We query GHTorrent [9] for the most popular lan-guages on GitHub, and subsequently, the most popular repositories for a given language.For LogChunks, we queried GHTorrent from 2018-04-01 for the three most popular repositories of each of the 30 most popular lan-guages to cover a broad range of development languages. Among the resulting repositories are, for example, Microsoft/TypeScript, git/git and jwilm/alacritty.
Build Sampling. To sample builds for LogChunks we keep the ten most recent builds of the status failed [6]. We check up to 1,000 builds per repository to ensure predictable termination of the log collection.Log Sampling. Travis CI builds comprise a number of jobs that actualize a build process in different environments. Hence, the outcome from different jobs might be different. For each build in LogChunks, we download the log of the first job that has the same state as the overall build.We inspected the collected build logs and discarded logs from three repositories. One had only one failed build, two others had empty build logs on Travis CI. In total, we collected 797 logs from 80 repositories spanning 29 languages.
2.2 	Manual Labeling
After collecting build logs, the first author manually labeled which text chunk describes why the build failed. Following that, she as-signed search keywords and structural categories to each log chunk.Chunk That Describes Why The Build Failed. For each repository, the labeler skimmed through the build logs and copied out the first occurrence of a description why the build failed. She preserved whitespaces and special characters, as these might be crucial to detect the targeted substring. To support learning of regular expres-sions identifying the labeled substrings the labeler aimed to start and end the labeled substring at consistent locations around the fault description.Search Keywords. To extract the search keywords, we considered the Chunk and ten lines above and below. The labeler’s task was to note down three strings they would search for (“grep”) to find this failure description. The strings should appear in or around the Chunk and are case-sensitive. We made no limitations on the search string; particularly, spaces are allowed.Structural Category. To label the structural categories we pre-sented the Chunk and the surrounding context to the labeler for all logs from a repository. We asked the labeler to assign numerical categories according to whether the Chunk had the same structural representation.
2.3 	ValidationWe validated our collected data points in an iterative fashion. First, we performed an initial inter-rater reliability study with the second author of this paper. Our learnings from this initial internal study are that 1) it is important and difficult to adequately communicate all decisions and assumptions on how to and which data to label and 2) there can be different legitimate viewpoints on which log chunk constitute the cardinal error and which keywords best to use. These learning informed the design of a second, larger cross-validation study for which we contacted over 200 developers.In our second validation, we sent out emails to the original devel-opers whose commits triggered the builds represented in LogChunks and asked them whether the log chunk we labeled actually describes why the build failed. This section describes our survey and discusses our results.Method. Using the Travis API, we collected the commit infor-mation for each build represented in LogChunks. We grouped all commits triggered by one developer and sent out an email to them. It included links to the corresponding commits, the build overview and the log file. We asked the developers to fill out a short form in case our extraction was not correct. In the survey, we asked the developer to paste in the log part actually describing the failure reason or describe in their own words why our original extraction was incorrect.LogChunks: A Data Set for Build Log AnalysisResults. In total, from 2019-10-15 to 2019-10-17, we sent out emails to 246 developers. Of these, 32 could not be delivered. We performed the sending out in three batches and used the first au-thor’s academic email address as the sender. All emails were specific to each recipient. We only sent one mail per recipient. We received answers from 61 developers, corresponding to 144 build logs with a response rate of 24.8%. Compared to typical response rates to cold calling known from Software Engineering [16], this is very high. We believe that our personalization and the ease of use for the participant are the main reasons for this—simply clicking on a link to confirm or refute an answer is enough, there is no need to craft an answer. Indeed, we only received seven replies from developers along the lines of “done.”Of the 144 answers, 118 initially indicated that our extraction was correct. We manually inspected the 26 negative answers and found that some stated that the proposed extraction did not show the whole description of why the build failed. This was because we chose to trim long chunks to keep the mails readable, and not a fault in our extraction per se. After adjusting for these answers, only 12 answers remained that stated that our labeled log chunk was not correct. This validates our data set with an externally validated consensus on 94.4% of the extracted data.Discussion. We believe that our developer survey highly strength-ens the trust in the validity of the labeled log chunks. The study received answers for about 18% of the data in LogChunks. After manual correction, 91% of the received answers indicated our la-beled chunks were accurate. One possible threat regarding the high number of correct answers is that, since we show the error message we extracted, it might be operationally easier for developers to validate it, rather than search for it in a long log file. To alleviate this problem, we made it as easy to confirm as to reject an extracted log chunk. We only ask for more details (the correct log chunk) in a second step.One of our 12 incorrect extractions only showed a warning and the developer proposed to also include the line stating that warnings are treated as errors. In others, we labeled the error message of an error that was later ignored.