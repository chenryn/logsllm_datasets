such high eﬃciency is hyper-invertible matrices [5]. A matrix is hyper-invertible if every
non-trivial square sub-matrix is invertible. Hyper-invertible matrices allow for extremely
eﬃcient veriﬁable secret-sharing (a small factor slower than just sending the shares to
the parties), extremely eﬃcient randomness generation (n − t random values at the cost
of n sharings), and also extremely eﬃcient public reconstruction (n times faster than just
sending the shares to each party). As a result, the communication complexity of each party
is constant per multiplication gate. Thus, the overall communication is linear, meaning that
the overall number of ﬁeld elements sent per multiplication gate is O(n) (when we measure
the communication complexity, by default, we count the communication of all parties
together ). We describe the hyper-invertible matrix technique in detail in Section 3.1.3.
HyperMPC follows the usual oﬄine/online pattern: In the oﬄine phase, a number
of so-called random double-sharings are generated [13, 5]. A random double-sharing is
a pair of two (valid) sharings, one with degree t, and one with degree 2t, of the same
uniformly distributed (and unknown) value. The generation of these double-sharings can be
performed in parallel. In the online phase, the actual circuit is computed. For input gates,
multiplication gates, and random gates, one of the prepared double-sharings is consumed
(where for input and random gates, only the t-sharing is used). Due to the linearity of
the secret-sharing scheme, linear gates can be evaluated locally. Finally, output gates use
standard secret reconstruction.
We prove the following theorem:
Theorem 1.1 Let f be an n-party functionality, and let C be an arithmetic circuit over
ﬁeld F that computes f . Then, protocol HyperMPC computes f with perfect security and
with abort, in the presence of an adaptive, malicious adversary corrupting t 
log(|F|/3)
for error 2−σ. Concretely, if 2−80 error is desired, and F is 16 bits long, then each party
must send 46 ﬁeld elements per multiplication gate. Furthermore, if the parties wish to
run a Boolean computation, then as described in the beginning of Section 3.1.1, a Boolean
circuit can be easily embedded into the ﬁeld GF [2κ] and run within an arithmetic-circuit
protocol (as long as 2κ is larger than the number of parties). If [10] is used, then GF [28]
suﬃces for up to 250 parties. However, in order to achieve 2−80 error, this would require
each party to send 86 ﬁeld elements per multiplication gate. In contrast, for all ﬁelds, in
our protocol each party sends only 13 ﬁeld elements per multiplication gate. Thus, for this
example of a Boolean circuit and GF [28], our protocol has one seventh the bandwidth of
the previous best protocol of [10]. In Section 4.2, we demonstrate that HyperMPC has
excellent performance, especially for small ﬁelds.
σ
Related low-bandwidth protocols. Our protocol is based on the linear communica-
tion protocol of [5], with signiﬁcant simpliﬁcations made possible by the fact that we focus
on security with abort. We chose [5] as basis because it is at the same time very simple and
very eﬃcient (and has low constants). As we have discussed, by the fact that the protocol
is perfectly secure, one does not need to worry about ﬁeld sizes and error probabilities.
Other protocols, like e.g. [13], oﬀer similar asymptotic eﬃciency, but require probabilistic
consistency checks, which complicate the implementation and increase the constants in the
7
communication complexity. We are not the ﬁrst to base a concretely eﬃcient construction
on [5]. In particular, the VIFF system presented in [11] is based on [5] and uses many of
their ideas. However, the actual construction of [11] has quadratic communication complex-
ity, in contrast to linear communication complexity in our protocol. For a large number of
parties, which is the goal for our end-to-end system, this diﬀerence is paramount. Another
related protocol discussed above is that of [10], which improves on [21]. The protocol of [10]
has comparable communication to HyperMPC for large ﬁelds, but much higher communi-
cation for small ﬁelds. We stress, however, that [10] is secure for any t < n/2 whereas we
require t < n/3.
1.4 Implementation and Experiments
We implemented both the MPSaaS system and the HyperMPC protocol. In Section 2, we
describe the system design and the platforms upon which it was implemented. Then, in
Section 4.1 we provide running times of executions with diﬀerent end-user endpoints. We
are currently extending these experiments and will include them in the full version of this
paper.
In Section 4.2, we provide running times of executions of the HyperMPC protocol for
a number of settings.
In particular, we provide results for experiments with circuits of
diﬀerent sizes and depths for 150 parties, for experiments computing statistics with 10–150
parties at increments of 10, and a single experiment demonstrating that HyperMPC can
even support an MPC with 500 parties in reasonable time. To the best of our knowledge,
this is the largest number of parties ever reported for an MPC protocol, in any setting.
2 The MPSaaS System and PrivatePoll
2.1 Overview
In the interconnected world of browsing, social networks, mobile applications and cloud
services, the focus is on usability and experience. While MPC is a promising technology
with prominent privacy guarantees, use cases in research are mostly Business-To-Business
(B2B) or Business-To-Customer (B2C). This limits the applicability of MPC, making it
unsuitable for many real world use cases. The goal of our system is to enable true P2P
MPC on a large scale for the ﬁrst time. We demonstrate the use of MPC for peer-to-peer
applications involving end users over the Internet. In order to demonstrate our system,
we constructed an application called PrivatePoll that enables end users to participate in
private polls and surveys. The incentive for the user to participate is to receive the results
of the poll (or simply to support the study being carried out), and their privacy is provable
guaranteed via secure computation. One example of where this could be used is a salary
poll among peers in the same professional category and skill level.
8
MPC-as-a-service. We developed an application that can be deployed in a standard
cloud service scenario, with necessary adaptions for MPC. An MPC Cloud Service Provider
(MPC-CSP) can provide the service to register participants in an upcoming MPC com-
putation. Then, upon registering for the computation, each user is provided with the
means necessary to actively participate as one of the parties in the executed MPC pro-
tocol. Our application enables end users to participate using their Google or Facebook
account (oAuth2 provider), and are not required to ﬁll in additional information, to ensure
usability and increase participation level. Of course, this can be easily modiﬁed to accept
alternative forms of authentication (e.g., LinkedIn login in order to run MPC computations
for a salary survey, or Active Directory for running MPC computations between university
students with university accounts). Using such authentication mechanisms also helps to
prevent double registration and bogus users.
MPC end-user interaction. Our application is novel in the way end-users interact with
the service. Speciﬁcally, the MPC functionality can be run directly inside the user’s browser
or in a mobile app, as these are the methods used for virtually all modern user interactions.
In addition, users can register a cloud instance to which they upload their input, and which
runs their party in the MPC protocol. This latter mode is useful for users who do not wish
to (or cannot) guarantee that they will be online (via their mobile or browser) at the time
of the actual MPC execution. In more detail, the following execution modes are oﬀered to
end users:
1. Mobile App: in this mode, the user enters her private input in a Mobile app. The MPC
protocol is executed inside the mobile (MPC code ported to mobile platform), ensuring
that the private information does not leave the device boundary.
2. Online In-Browser:
in this mode, the user enters her private input in a Web Form.
Then, the MPC protocol is executed in Javascript inside any browser, ensuring that
private information does not leave the Browser boundary.
3. Cloud Instance: In both the mobile app and browser modes, the user must be online at
the time of actual execution, which is a major problem for real-world deployment. In
order to overcome this obstacle, we enable users to deﬁne a cloud instance of their own
to run the MPC protocol for them. In our implemented solution, users can participate
by using a cloud instance that they own (like an AWS Docker instance), or they can
choose to use an instance generated for them by the MPC service provider. The latter
is of course less secure (since the user sends their input to a machine controlled by
the provider), but is an option provided nevertheless; we discuss this in more detail in
Section 2.5. This simple solution solves the acute problem that all MPC participants
must be simultaneously online in order to run the protocol, since parties who cannot
be online (e.g., they cannot guarantee that their mobile is connected, etc.) can deﬁne
cloud instances that are always online.
9
4. IoT Device: This mode considers MPC involving IoT devices that take sensitive mea-
surements which are input to the MPC computation. The MPC protocol is run inside
the device, ensuring that private information never leaves the device. Our implementa-
tion currently works for Raspberry Pi3; if the remote device is inactive then it can be
automatically woken for the MPC execution. (This can be used for drones, and can be
extended to weaker IoT devices like wearables in the future.)
Deployment and delivery of MPC software. To increase user trust, secure artifacts
can be used. The MPC Mobile App (apk) can be delivered via Google Play, or equivalently
as an IOS App via the Apple App Store, the Docker Image which executes cloud instance
for a user can also be signed to ensure trusted delivery, and Subresource Integrity via CDN
can be used on a browser. In addition, the code of the MPC protocol being used should be
open source to increase trust. We stress that our entire system, including all components,
is completely open source and available for review (and use). An independent review by an
organization like EFF.org can be carried out on speciﬁc MPC modules to further increase
trust.
The overall system. The system components and their interactions are shown in Fig-
ure 1.
Figure 1: Components of the system, and their interaction.
10
2.2 MPSaaS Backend – the MATRIX Automated Execution System
The backend service, called MATRIX, provides the ability to automatically deploy large-
scale MPC experiments in the cloud. As such, this component is of independent interest to
the community. In particular, running multiparty experiments can be very time-costly if
done manually, causing researchers to either spend a large amount of eﬀort on this menial
task or to cut corners. As an example of the capabilities of the system, we recently ran a
protocol experiment for an MPC protocol for 10–100 parties with increments of 10, with 12
diﬀerent conﬁgurations per number of parties (a conﬁguration being a mix of a circuit and
execution parameters), and 5 repetitions of each experiment for statistical accuracy, in just
17 minutes (observe that this consists of 600 diﬀerent executions, with diﬀerent numbers
of parties and diﬀerent conﬁgurations each time). We stress that this includes the entire
time to setup the experiment and get the results back. Given the automation capabilities
of MATRIX, this serves as the perfect backend for the MPSaaS system. We now proceed
to describe the diﬀerent components of MATRIX; throughout the explanation Figure 2 can
serve as a guide for the diﬀerent components and capabilities. We remark that MATRIX
works in an identical manner for running experiments, and for real protocol executions (of
course, for real executions, there is no need to collect logs containing running times, and
so on).
Figure 2: The MATRIX backend.
11
The key features of the MATRIX backend are as follows:
Protocol compatibility: The system includes a well-deﬁned “contract” that must be
fulﬁlled by an MPC executable that is run in the system. This contract ensures that it
runs correctly in the Test Environment, and includes deﬁnitions for conﬁguration, result
reporting and fault handling. This contract can be easily implemented in wrapper classes
and protocols, and is all that is needed to use MATRIX for a new protocol. We have
already incorporated 10 diﬀerent MPC protocols into the system, for both semi-honest
and malicious adversaries, and for honest majority and dishonest majority. Speciﬁcally,
the protocols incorporated so far are semi-honest GMW, semi-honest Yao, semi-honest
BMR, malicious Yao [22], SPDZ [12], the protocols in [1, 2, 21, 10] and HyperMPC from
Section 3. In order to incorporate new protocols in MATRIX it suﬃces to write a python
script that accepts the command line arguments and does the following:
1. Converts formats (parties’ ﬁle IP addresses and Ports to its internal parties’ presenta-
tion),
2. Converts input circuit if required, and
3. Launches the actual protocol executable (using the party ID and other conﬁguration
ﬂags).
In addition, in order to utilize the result reporting feature, it is necessary to add code to
the protocol to log in our format, or write a script to convert its internal log ﬁles to our
format. We incorporated the SPDZ-2 protocol [12] in one day.
Fully Automated Environment Setup across regions: MATRIX provides full au-
tomation to create instances for MPC execution, and immediately tear them down after-
wards (reducing cost). In detail:
1. Host setup: We create cloud instances from custom AMI images, and deploy the protocol
code, which is then compiled on the host for optimal native performance (e.g., utilizing
Intel-AVX instructions where available). MATRIX is integrated with Github or any GIT
provider to pull experimental or master versions as required. We select the EC2 instance
type based on the capabilities and requirements of the protocol, and create standard
or Spot Instances based on a cost strategy. For example, when the actual time of the
execution is not signiﬁcant, one can execute during oﬀ load hours (say weekends), using
spot-instances issued in 6-hour quotas. This results in signiﬁcant savings, assuming
many executions are run (and so the 6-hour quota is utilized).
2. Network setup: MATRIX automatically sets the network for all parties, deﬁning the
security groups and rules as required. Once a network is set up, it collects information
on allocated addresses and DNS names in order to generate a parties ﬁle that is used
in the execution. MATRIX also supports setting up a VPC (virtual private cloud,
ensuring that all pairwise connections between parties are encrypted and authenticated).
12
However, we stress that this comes at a high monetary cost, and so only makes sense
for real-world applications and not experiments.
The conﬁguration ﬁle to deﬁne the experiment can specify the regions that the parties are
deployed, enabling experiments over fast and slow networks to be seamlessly carried out.
Execution coordination: MATRIX launches the MPC process on each hosts, automat-
ically providing it with the details of all participating parties, and the relevant parameters