11
0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateOur model, AUC=98.09%SVM model, AUC=87.33%0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateOur model, AUC=97.64%SVM model, AUC=86.47%0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateOur  model,SVM model,AUC=97.49%AUC=85.40%0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive RateOur model, AUC=97.89%SVM model, AUC=85.28%0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive Rate Our model, AUC=94.97%SVM model, AUC=79.24%0.00.20.40.60.81.0False Positive Rate0.00.20.40.60.81.0True Positive Rate Our model, AUC=94.43%SVM model, AUC=69.51%TABLE II: Examples of similar BB pairs that are correctly classiﬁed by INNEREYE-BB, but misclassiﬁed by the SVM model.
Pair 1
x86
ARM
Pair 2
x86
ARM
x86
ARM
Pair 3
MOVSLQ RSI,EBP
MOVZBL ECX,[R14,RBX] STR R9,[SP]
MOVL EDX,
XORL EAX,EAX
MOVQ RDI,R13
CALLQ FOO
TESTL EAX,EAX
JLE 
STR R0,[SP+0]
ASR R3,R7,0
MOV R0,R6
MOV R2,R7
BL FOO
CMP R0,0
BLT 
LDRB R0,[R8+R4] MOVQ RDX,+[RIP+0] LDR R2,[R8+0] MOVQ [RSP+0],RBX LDR R0,[SP+0]
MOVQ [RSP+0],R14 STR R9,[SP+0]
STR R0,[SP+0]
ADDQ RDI,0
CALLQ FOO
ADD R0,R1,0
BL FOO
MOVL ESI,
LDR R7,
MOVQ RDI,[R12]
LDR R1,[R6]
LDR LR,[SP+0]
MOV R12,R7
STRB R0,[R1+0]
B 
MOVQ RDI,R12
MOVL ESI,R14D
CALLQ FOO
MOVQ RDI,R12
CALLQ FOO
MOVQ RDX,+[RIP+0] LDR R2,[R8+0] MOVB [RDI+0],AL
MOVQ RDI,R12
MOVL ESI,R14D
CALLQ FOO
TESTL EAX,EAX
JNE 
MOV R0,R4
MOV R1,R5
BL FOO
CMP R0,0
BNE
MOV R0,R4
MOV R1,R5
BL FOO
MOV R0,R4
BL FOO
CMPB [RDI+0],0
JNE 
TABLE III: Examples of dissimilar BB pairs that are correctly classiﬁed by INNEREYE-BB, but misclassiﬁed by the SVM model.
Pair 4
Pair 5
Pair 6
x86
ARM
x86
ARM
x86
ARM
IMULQ R13,RAX,0 MOV R1,R0
XORL EDX,EDX
MOVQ RBP,[RSP+0] CMP R0, 0
DIVQ RBP
JMP 
BEQ 
LDR R6,[SP+0] TESTQ RBP,RBP CMP R0,R1
BHS 
JE 
XORL R14D,R14D LDMIB R5,R0,R1 MOVL [RSP+0],R14D SUB R2,R1,0
MOVQ RAX,[RSP+0] MOV R10,0
CMPB [RAX],0
CMP R2,0
MOVQ [RSP+0],R13 MOV R9,0
MOVQ [RSP+0],R15 BHI 
JNE 
(%)
50
100
150
Optimization levels
O1
O3
94.97
95.77
95.99
96.83
96.89
96.24
Cross
-opts
95.39
95.82
95.86
(c) AUC vs. instruction embedding dimensions.
O2
95.23
96.33
96.33
(a) AUC vs. # of epochs.
(b) Loss vs. # of epochs.
Optimization levels
O1
O3
95.48
95.57
96.17
95.88
96.83
95.99
Cross
(%)
-opts
95.59
10
95.45
30
50
95.82
(d) AUC vs. block embedding dimensions.
O2
95.73
95.65
96.33
(%)
1
2
3
Optimization levels
O1
O3
96.17
95.88
97.59
97.83
98.16
97.48
O2
95.65
97.49
97.39
Cross
-opts
95.45
97.45
97.76
Optimization levels
(%)
O1
O3
95.99
LSTM 96.83
95.83
96.15
GRU
RNN
91.39
92.60
O2
96.33
95.30
93.26
Cross
-opts
95.82
95.71
92.66
(e) AUC vs. # of network layers.
(f) AUC vs. network hidden unite types.
Fig. 12: Impact of different hyperparameters. Figure 12a and Figure 12b are evaluated on the validation datasets of Dataset I, and
others are evaluated on its testing datasets.
computational complexity and does not help signiﬁcantly on
the performance, we choose the network depth as 2.
4) Network Hidden Unit Types: As a simpler-version of
LSTM, Gated Recurrent Unit (GRU) has become increasingly
popular. We conduct experiments on comparing three types
of network units, including LSTM, GRU as well as RNN.
Figure 12f shows the comparison results. It can be seen that
LSTM and GRU are more powerful than the basic RNN, and
LSTM shows the highest AUC values.
G. Efﬁciency of INNEREYE-BB
1) Training Time: We ﬁrst analyze the training time for
both the instruction and basic-block embedding models.
Instruction embedding model training time. The training
time is linear to the number of epochs and the corpus size. We
use Dataset I, containing 437,104 blocks for x86 and 393,529
blocks for ARM, with 6,199,651 instructions in total, as the
corpus to train the instruction embedding model. The corpus
contains 49,760 distinct instructions which form a vocabulary.
We use 10−5 as the down sampling rate and set the parameter
mini-word-count as zero (no word is ignored during
training), and train the model for 100 epochs. Table IV shows
the training time with respect to different instruction embedding
dimensions. We can see that the instruction embedding model
can be trained in a very short period of time.
Block embedding model training time. We next evaluate the
12
020406080100120140160180200Epoch0.790.820.850.880.910.940.971.00AUCO1O2O3Cross-opt-levels020406080100120140160180200Epoch0.060.080.100.120.140.160.180.20LossO1O2O3Cross-opt-levels(a) Training time of single-layer networks with
respect to different hidden unit types.
Fig. 13: Training time of the basic-block embedding model. The instruction
embedding dimension is 100, and the block embedding dimension is 50. The
number above each bar is the time (second per epoch) used to train the model.
(b) Training time of LSTMs with respect to
different number of network layers
(Second)
L=1, D=30
L=1, D=50
L=2, D=30
L=2, D=50
Optimization levels
O1
O3
4.137
3.040
4.901
3.530
8.780
6.359
6.663
9.139
O2
3.899
4.702
8.237
8.722
Cross
-opts
2.944
3.487
6.266
6.625
Fig. 14: Testing time of INNEREYE-BB with
respect to different number of network layers
and block embedding dimensions. The instruc-
tion embedding dimension is 100. L denotes
the number of network layers. D denotes the
block embedding dimension.
TABLE IV: Training time of the instruction embedding model
with respect to different embedding dimensions.
Instruction embedding dimension
Training time (second)
50
82.71
100
84.22
150
89.75
time used for training the basic-block embedding model. The
training time is linear to the number of epochs and the number
of training samples. The results are showed in Figure 13. The
number above each bar is the time (second per epoch) used
to train the model. Figure 13a shows the training time with
respect to different types of network hidden unit. Figure 13b
displays the training time of the LSTM networks in terms of
different number of network layers. In general, LSTM takes
longer training time, and a more complicated model (with more
layers) requires more time per epoch.
Earlier we have shown that the block embedding model
with 2 network layers and 20 epochs of training can achieve a
good performance (Section VII-F), which means that it requires
ﬁve and a half hours (=(213 + 275 + 290 + 193) × 20/3600)
to train the four models on the four training subsets, and each
model takes around an hour and a half for training. With a
single network layer, each model only needs about 40 mins
for training and can still achieve a good performance.
2) Testing Time: We next investigate the testing time of
INNEREYE-BB. We are interested in the impacts of the number
of network layers and the dimension of block embeddings,
in particular. Figure 14 summaries the similarity test on the
four testing datasets in Dataset I. The result indicates that the
number of network layers is the major contributing factor of
the computation time. Take the second column as an example.
For a single-layer LSTM network with the block embedding
dimension as 50, it takes 0.41 ms (= 3.530/8722) on average
to measure the similarity of two blocks, while a double-layer
LSTM network requires 0.76 ms (= 6.663/8722) on average.
Comparison with Symbolic Execution. We compare the
proposed embedding model with one previous basic-block
similarity comparison tool which relies on symbolic execution
and theorem proving [37]. We randomly select 1,000 block
pairs and use the symbolic execution-based tool to measure
the detection time for each pair. The result shows that the
INNEREYE-BB runs 3700x to 140000x faster, and the speedup
can be as high as 8000x on average.
The reason for the high efﬁciency of our model is that
most computations of INNEREYE-BB are implemented as
easy-to-compute matrix operations (e.g., matrix multiplication,
matrix summation, and element-wise operations over a matrix).
Moreover, such operations can be parallelized to utilize multi-
core CPUs or GPUs to achieve further speedup.
H. Code Component Similarity Comparison
We conduct
three case studies to demonstrate how
INNEREYE-CC can handle real-world programs for cross-