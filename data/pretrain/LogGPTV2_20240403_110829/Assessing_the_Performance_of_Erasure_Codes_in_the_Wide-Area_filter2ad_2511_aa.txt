title:Assessing the Performance of Erasure Codes in the Wide-Area
author:Rebecca L. Collins and
James S. Plank
Assessing the Performance of Erasure Codes in the Wide-Area
Rebecca L. Collins
James S. Plank
Technical Report UT-CS-04-536
Department of Computer Science
University of Tennessee
November 2004
Abstract
The problem of efﬁciently retrieving a ﬁle that has been broken into blocks and distributed across the
wide-area pervades applications that utilize Grid, peer-to-peer, and distributed ﬁle systems. While the use of
erasure codes to improve the fault-tolerance and performance of wide-area ﬁle systems has been explored,
there has been little work that assesses the performance and quantiﬁes the impact of modifying various
parameters. This paper performs such an assessment. We modify our previously deﬁned framework for
studying replication in the wide-area [6] to include both Reed-Solomon and Low-Density Parity-Check
(LDPC) erasure codes. We then use this framework to assess the performance of erasure coding in three
wide-area, distributed settings. We conclude that as the set size, encoding rate, type of encoding and ﬁle
distribution change, performance reﬂects a trade-off between downloading time and decoding time.
1 Introduction
The coordination of widely distributed ﬁle servers is a complex problem that challenges wide-area,
peer-to-peer and Grid ﬁle systems. Systems like OceanStore [17], LoCI [3], and BitTorrent [5] aggregate
wide-area collections of storage servers to store ﬁles on the wide-area. The ﬁles are broken into ﬁxed-size
blocks, which are distributed among the disparate servers. To address issues of locality and fault-tolerance,
erasure coding is employed, albeit normally in the primitive form of block replication. When a client needs
1
to download a ﬁle from this collection of servers to a single location, he or she is faced with a variety of
decisions to make about how to perform the download. This has been termed the “Plank-Beck” problem by
Allen and Wolski, who deemed it one of two fundamental problems of data movement in Grid Computing
systems [1].
In previous work, we deﬁned a a framework for evaluating algorithms for downloading replicated wide-
area ﬁles [6]. In this paper, we extend this framework to include erasure coding, and perform an evaluation
of algorithms for downloading erasure-encoded ﬁles in three wide-area settings. This work is signiﬁcant
as it is the ﬁrst piece of work to evaluate the relative performance of replication, Reed-Solomon codes and
LDPC codes in a realistic, wide-area environment with aggressive, multithreaded downloading algorithms.
Our results show that performance trends are inﬂuenced by three main factors: encoding rate, set size, and
ﬁle distribution. The two types of erasure codes present different downloading and decoding trade-offs, and
in most cases, Reed-Solomon codes perform well when sets are small, while LDPC codes perform well
when sets are large.
2 The “Plank-Beck” Problem
We are given a collection of storage servers on the wide-area, and a large ﬁle that we desire to store on
these servers. The ﬁle is partitioned into blocks (termed “data blocks” of a ﬁxed size, and an erasure coding
scheme is used to calculate some number of additional “coding” blocks. The collection of data and coding
blocks are then distributed among the storage servers. At some point, a client at a given network location
desires to download the ﬁle, either in its entirety, or in a streaming fashion. Simply stated, the “Plank-Beck”
problem is: How does the client download the ﬁle with the best performance.
The simplest erasure coding scheme is replication – each block of the ﬁle is stored at more than one
server. In this case, algorithms for solving the “Plank-Beck” problem may be parameterized in four dimen-
sions [6]:
1. The number of simultaneous downloads.
2. The degree of work replication
3. The failover strategy.
4. The server selection strategy.
2
The optimal choice of parameters depends on many factors, including the distribution of the ﬁle and the
ability for the client to utilize monitoring and forecasting [20]; however, a few general conclusions were
drawn. First, the number of simultaneous downloads should be large enough to saturate the network paths
to the client, without violating TCP-friendliness in shared networks. Second, work replication should be
present, but not over-aggressive; ideally, it combines with a failover strategy that performs work replication
when the progress of the download falls below a threshold. Finally, if monitoring software is present, fast
servers (with respect to the client) should be selected, although ideal algorithms consider both the forecasted
speed from server and the load induced on the server by the download itself.
When we add more complex erasure coding, some additional parameters manifest themselves. We deﬁne
them after explaining erasure codes in more detail.
3 Erasure Coding
Erasure codes have arisen as a viable alternative to replication for both caching and fault-tolerance in
wide-area ﬁle systems [4, 16, 18, 21]. Formally deﬁned, with erasure coding, n data blocks are used to
construct m coding, (or “check”) blocks, where data and check blocks have the same size. The encoding
n
rate is
n+m. Subsets of the data and coding blocks may be used to reconstruct the original set of data
blocks. Ideally, these subsets are made up of any n data or check blocks, although this is not always the
case. Replication is a very simple form of erasure encoding (with n = 1 and m > 1). In comparison to
replication, more complex erasure coding techniques (with n > 1 and m > 1) reduce the burden of physical
storage required to maintain high levels of fault tolerance. However, such erasure coding can introduce
computationally intensive encoding and decoding operations. We explore the two most popular types of
erasure coding in this paper: Reed-Solomon coding, and Low-Density Parity-Check (LDPC) coding. They
are summarized below.
3.1 Reed-Solomon Coding
Reed-Solomon codes have been used for fault-tolerance in a variety of settings [7–9,12,21]. Their basic
operation is as follows. The n data blocks are partitioned into words of a ﬁxed size, and a collection of n
words forms a vector. A distribution matrix is employed so that the check blocks are calculated from the
vector with m dot products. Galois Field arithmetic is used so that all elements have multiplicative inverses.
Then the act of decoding is straightforward. Given any n of the data and check blocks, a decoding matrix
3
D1
D2
D3
D4
D5
C1
C2
C3
C4
C5
Figure 1. An example LDPC code where n = 5 and m = 5
may be derived from the distribution matrix, and the remaining data blocks may be calculated again with
dot products.
A tutorial on Reed-Solomon coding is available in [13, 15]. Reed-Solomon coding is expensive for
several reasons. First, the calculation of check block requires an n-way dot product. Thus, creating m check
blocks of size B has a complexity of O(Bmn), which grows rather quickly with n and m. Second, decoding
involves an O(n3) matrix inversion. Third, decoding involves another n-way dot product for each data block
that needs to be decoded. Fourth, Galois-Field multiplication is more expensive than integer multiplication.
For that reason, Reed-Solomon coding is usually deemed appropriate only for limited values of n and m.
See [4, 16] for some limited evaluations of Reed-Solomon coding as n and m grow.
3.2 LDPC Coding
LDPC coding is an interesting alternative to Reed-Solomon coding. LDPC codes are based on bipartite
graphs, where the data blocks are represented by the left-hand nodes, and the check blocks are represented
by the right-hand nodes. An example for n = 5 and m = 5 is given in Figure 1. Each check block is
calculated to be the bitwise exclusive-or of all data blocks incident to it. Thus, for example, block C1 in
Figure 1 is equal to D1 (cid:8) D3 (cid:8) D5. A check block can only be used to reconstruct data blocks to which
it is adjacent in the graph. For example, check block C1 can only be used to reconstruct data block D1,
D3, or D5. If we assume that the blocks are downloaded randomly, sometimes more than n blocks are
required to reconstruct all of the data blocks. For example, suppose blocks D3; D4; D5; C2, and C5 are
4
retrieved. There is no way to reconstruct block D1 with these blocks, and a sixth block must be retrieved.
Furthermore, if the next block randomly chosen is D2 (even though it can be decoded with blocks D3; D4,
and C5), then it is still impossible to reconstruct block D1 and a seventh block is required. A graph G’s
overhead, o(G), is the average number of blocks required to decode the entire set, and its overhead factor
f (G) is equal to o(G)=n.
Unlike Reed-Solomon codes, LDPC codes have overhead factors greater than one. However, LDPC
codes have a rich theoretical history which has shown that inﬁnitely sized codes of given rates have overhead
factors that approach one [10, 19]. For small codes (n (cid:20) 100), optimal codes are in general not known, but
an exploration has generated codes whose overhead factors are roughly 1.15 for a rate of 1
2 [16] We use
selected codes from this exploration for evaluation in this paper.
4 Experimental Setup
We performed two sets of experiments to assess performance. In the ﬁrst, we only study Reed-Solomon
codes, and in the second, we study both Reed-Solomon and LDPC codes. The experiments were “live”
experiments on a real wide-area network. The storage servers were IBP servers [14], which serve time-
limited, shared writable storage in the network. The actual servers that we used were part of the Logistical
Backbone [2], a collection of over 300 IBP servers in various locations worldwide.
In all experiments, we stored a 100 MB ﬁle, decomposed into 1 MB blocks plus coding blocks, and
distributed them on the network. Then we tested the performance of downloading this ﬁle to a client at the
University of Tennessee. The client machine ran Linux RedHat version 9, and had an Intel (R) Celeron (R)
2.2 GHz processor. Since the downloads took place over the commodity Internet, the tests were executed in
a random order so that trends due to local or unusual network activity were minimized, and each data point
presented is the average of ten runs.
5 Reed-Solomon Experiments
Section 5.1 summarizes the framework used to organize the experimental parameter space, while sec-
tion 5.2 describes the network ﬁles used in the experiments.
5.1 The Framework for Experimentation
The problem of downloading wide-area ﬁles can be deﬁned along four dimensions [6]:
5
Table 1. Reed-Solomon Experiment Space
Parameter
Simultaneous Downloads
Range of Parameters
T 2 [20; 30]
Work Replication and Failover Strategy
fR; P g 2 [f1; 1g; f2; (30=n)g], static timeouts
Server Selection
Coding
n 2 [1; 2; 3; 4; 5; 6; 7; 8; (9)], m 2 [1; 2; 3; 4; 5; 6; 7; 8; (9)],
Fastest0, Fastest1
such that m
n <= 3
1. Threads (T ): How many blocks should be retrieved in parallel?
2. Redundancy (R): How many copies of the same block can be retrieved in parallel?
3. Progress (P ): How much progress in the download can take place before we aggressively retry a
block?
4. Server Selection Algorithm: Which replica of a block should be retrieved when we are offered a
choice of servers holding the block? We focus on two algorithms from [6], called fastest0 and fastest1.
Fastest0 always selects the fastest server. Fastest1 adds a “load number” for the number of threads
currently downloading from each server, and selects the server that minimizes this combination of
predicted download time and load number. These two algorithms were the ones that performed best
in wide-area downloads based solely on replication [6].
Based on the previous results and some preliminary testing, we experimented over the range of param-
eters in the four dimensions of the framework that were expected to produce the best results; they are listed
in table 1, in addition to the values of n and m that were tested. Note that m
n is the factor of coding blocks
n is two, then the ﬁle consumes 300 MB of storage — 100 for the ﬁle, and
n to be less than or equal to three, as greater values consume what we
employed. For example, if m
200 for the coding blocks. We limit m
would consider an unacceptable amount of storage for 100 MB of data.
5.2 Network Files
We employ three network ﬁles in the experiments. These differ by the way in which they are distributed
on the wide-area network. The ﬁrst is called the hodgepodge distribution. In this ﬁle, ﬁfty regionally
distinct servers are chosen and the data and check blocks are striped across these ﬁfty servers. None of the
servers is in Tennessee. The next two network ﬁles are the regional distribution and the slow regional
6
distribution. In these ﬁles, the data and check blocks are striped across servers in four regions. The regions
chosen for the regional distribution are Alabama, California, Texas, and Wisconsin; and the regions chosen
for the slow regional distribution are Southeastern Canada, Western Europe, Singapore, and Korea.
5.3 Expected Trends
There are several trends that we anticipate in our experiments with Reed-Solomon codes. Here we deﬁne
a set of blocks to be a group of n + m data/coding blocks that may be used to decode n data blocks.
(cid:15) Performance should improve as m increases while n is ﬁxed. Having more redundancy means that
there are more choices of which n blocks can be used to decode, and more choices mean that the
average speed of the fastest n blocks in the set is likely faster. A related trend is that performance
should decline as n increases while m remains ﬁxed.
(cid:15) As the set size increases and the encoding rate remains ﬁxed, performance will be better or worse
based on the impact of two major factors:
1. A larger set implies a larger server pool from which n blocks can be retrieved, and should
improve performance for the following reason: If it is assumed that slow and fast servers are
distributed randomly among the sets, then when sets are larger, it is less likely that any one set
is composed entirely of slow servers.
2. Sets with larger n have larger decoding overheads, and we expect performance to decrease
when n gets too big. This is somewhat masked for smaller sets because there are many sets
in a ﬁle and the decoding overhead of one set can overlap the data movement of future sets.
However, as n increases, the time it takes to decode a set will eventually overtake the time it
takes to download a set. Furthermore, sets near the end of the ﬁle often end up being decoded
after all of the downloading has taken place — with larger n, this amounts to larger leftover
computation that cannot overlap any data movement.
In the rest of this section, empirical results are presented and compared to the anticipated trends.
5.4 Results
Figures 2, 3, and 4 show the best performance of the three distributions when Reed-Solomon coding
is used. Each block displays the best performing instances (averaged over ten runs) in the entire range of
7
)
M
(
t
e
S
r
e
p
s
k
c
o
l
B
k
c
e
h
C
8
7
6
5
4
3
2
1
60.0 - 64.9 Mbps
50.0 - 54.9 Mbps
45.0 - 49.9 Mbps
40.0 - 44.9 Mbps
35.0 - 39.9 Mbps
30.0 - 34.9 Mbps
25.0 - 29.9 Mbps