Hi,  
I am trying to optimize a function using fmin_bfgs, and logistic regression
algorithm. There's no problem in convergence using the older version (scipy
0.10). However when using newer version 0.15 the optimizer crashes with
warning: "Desired error not necessarily achieved due to precision loss‚Äù.  
I compared optimize.py code of older, and newer versions, and noticed initial
stepsize is updated from **old_old_fval = old_fval + 5000** to **old_old_fval
= None**. (`81474eb`)  
Then I tried changing the stepsize back to **old_old_fval = old_fval + 5000**
, and this time fmin_bfgs worked perfectly. I am guessing there might be an
issue with the initial stepsize ( **old_old_fval = None** ) for fmin_bfgs, and
this stepsize may not be flexible for all gradient sizes. I see a similar
issue still opened here: #3581.