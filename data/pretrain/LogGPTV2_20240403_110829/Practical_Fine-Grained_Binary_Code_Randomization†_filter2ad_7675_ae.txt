10
16
11
4
21
8
7
12
11
9
11
8
10
16
9
37
37
18
4
Reduced Metadata
ZJR BBR PHR LLR(16)
93
15
38
43
58
12
55
17 149 100
42
0
5
30
39 215 140
12
1
8
44
11 147 105
69
15
55
11
67
4
11
50
6
46
21 118 123
7
52
66 812 167
8 241 190
92
10 107
6
38
22
77
95
77
92
62
72
24
31
127
22
83
34
10
145
20
53
58
32
22
48
34
439
289
119
14
PHR +
LLR(16)
55
101
125
32
160
35
44
165
69
69
83
52
49
126
58
401
331
151
25
Program
perlbench
gcc
bwaves
mcf
cactuBSSN
lbm
omnetpp
wrf
xalancbmk
x264
pop2
deepsjeng
imagick
leela
nab
exchange2
fotonik3d
roms
xz
ZJR
BBR
PHR
LLR(16)
0.3%
3.2%
-0.5%
1.1%
1.3%
0.0%
1.0%
0.0%
6.0%
-1.5%
2.2%
0.1%
0.1%
-0.4%
0%
1.5%
0.7%
0.0%
0.2%
56.7%
35.5%
-0.4%
17.4%
3.8%
-2.1%
10.4%
3.2%
35.4%
9.6%
10.0%
23.6%
11.1%
20.0%
3.0%
54.3%
-1.4%
0.3%
6.2%
15.5%
13.3%
-0.4%
5.1%
2.4%
-1.0%
5.0%
0.5%
10.2%
0.0%
2.2%
8.5%
0.3%
10.4%
0.3%
1.8%
0.6%
-0.2%
0.7%
1.2%
4.8%
-0.6%
1.7%
3.5%
-3.0%
2.3%
1.3%
6.5%
9.1%
5.3%
2.4%
1.6%
2.3%
1.0%
12.9%
-1.1%
0.2%
2.2%
PHR +
LLR(16)
14.5%
13.8%
-1.0%
6.0%
4.6%
-1.8%
4.3%
0.8%
9.7%
12.2%
3.7%
10.2%
0.6%
11.8%
0.6%
12.2%
1.1%
0.0%
2.0%
9
77
84
2
17
14
10
Mean
13 134
112
Table 6: Full & Reduced Unwind Block Entropy (FUBE &
RUBE) on SPEC 2017.
of code reordering on cache locality. Since ZJR doesn’t introduce
any new jumps, its overhead must purely be from cache locality
effects. Almost every binary in the table has close to zero overhead
for ZJR except xalancbmk at 6%. CCR [31] also reports a 5% overhead
on this benchmark.
BBR incurs a significant 14.13% overhead because (a) it introduces
many new jumps, and (b) the cache effects of permuting at much
finer granularity than ZJR will correspondingly be larger. One factor
in this high overhead is that we treat a call as an end of a basic block,
which may not be the case in alternative BBR implementations.
PHR is implemented on top of ZJR. Of the trampolines added by
PHR (see Sec. 2.3), the ones with the most performance impact are
the two jumps surrounding each call. As a result of these, programs
that make frequent calls can have overheads as high as 15%. The
average is a moderate 3.86% overhead.
LLR(16) is also implemented on top of ZJR, and its overhead is
proportional to the additional partitions that it introduces. Although
there are two benchmarks with 9% or slightly higher overheads,
the average is a relatively low 2.26%.
Since PHR +LLR(16) introduces more partitions than either PHR
or LLR(16), we expect its overhead to be higher than both. In fact,
the overhead of PHR +LLR(16) tends to be close to the maximum of
the PHR and LLR(16), with an average close to 5%.
8.6 Memory Overhead
Our approach of intra-block randomization does not change the
number of unwinding blocks or the data associated with them, and
hence should have zero space overhead. However, in practice, our
implementation uses labels, and cannot encode constants into the
smallest number of bytes. This results in a 13.8% overhead when re-
creating the EH-metadata. With more engineering effort, this can
be brought down to zero, but we have not pursued this because our
Geo. Mean
0.88%
14.13%
3.86%
2.26%
5.14%
Table 7: Runtime overhead on SPECspeed 2017 benchmark
suite.
main focus is on the size after the metadata reduction techniques
of Sec. 4 have been applied. We find that after the reduction, EH-
metadata has shrunk to 50% of the original size. (Although the
number of unwinding blocks have been decreased by more than 6x,
the reduction in metadata size is more modest. This is because, as
illustrated in Fig. 1, the unwind data for the merged blocks tends
to accumulate much of the data from the original blocks.)
We also need to generate the full metadata that will be used
for stack tracing on faults. As discussed before, because we have
expanded call-containing blocks into nearby blocks that don’t con-
tain calls, and permuted these merged blocks, we have effectively
done something similar to whole function randomization: we have
chopped up existing unwinding blocks into pieces and permuted
them. As a result, there are many more unwinding blocks in this
case, so the metadata increases by 45%.
9 DISCUSSION
Code Signing. Linux distributions verify code signatures at the
time of software installation and updates. Our system performs its
randomization on the installed (or patched) versions, and hence
does not interfere in any way with current distribution models.
This same comment applies to software updates and patches as
well: signature checking is performed on the update, and after
that, the concerned binaries are updated. SBR can then randomize
these updated binaries, thus making it compatible with prevalent
software distribution and update mechanisms on Linux.
Rerandomization. Our system can support periodic rerandomiza-
tion of binaries on the disk. Such rerandomization may be initiated
on a regular basis, e.g., every few days. Alternatively, it may be
triggered after a binary has been loaded a certain number of times.
411ACSAC 2020, December 7–11, 2020, Austin, USA
Soumyakant Priyadarshan, Huan Nguyen, and R. Sekar
COOP and AOCR.. Code randomization techniques excel at stop-
ping attacks that access code snippets that won’t be used by le-
gitimate code. Stopping attacks that use legitimate targets, such
as entire functions, is much harder. SBR can prevent control-flow
hijacks that employ whole function code reuse only to the extent
that the attacker does not know the function’s location. However,
if the attacker can find its location through leaked pointers, then
attacks that reuse the target function cannot be stopped by SBR.
Hence SBR, like previous code randomization techniques, is vul-
nerable to counterfeit object oriented programming (COOP) [46]
and address-oblivious code reuse (AOCR) [45] attacks.
10 RELATED WORK
Control Flow Integrity. Control flow integrity (CFI) [1, 2] tech-
niques monitor indirect control flow transfers, permitting only
those that are consistent with a statically inferred control-flow
graph. They provide a principled foundation for building other
security mechanisms such as software fault isolation [54, 59] and
other forms of policy enforcement [21, 65]. However, as mentioned
in the introduction, they have several weaknesses as a defense
against code reuse attacks, and have been shown to be vulnerable
[10, 46]. Coarse-grained CFI techniques are particularly vulnerable,
while fine-grained techniques tend to be less compatible. To address
compatibility, researchers have focused on solutions that target spe-
cific code pointer types such as those used in C++ virtual calls
[23, 60] and returns [9, 17, 44]. There have also been recent works
[20, 28, 29, 39, 53] utilizing hardware features for performance.
Code Randomization. Since its introduction by Bhatkar et al [6],
fine-grained code randomization has been the focus of numerous
research efforts over the past fifteen years [11, 13, 14, 16, 18, 26,
27, 30–32, 38, 55, 57, 62]. Earlier techniques [6, 13, 18, 26, 27, 30, 32,
38, 55] were focused on the static-ROP threat model. More recent
techniques (e.g., [11, 14, 16, 57, 62]) address JIT-ROP and indirect
disclosure based ROP, as discussed below.
Isomeron develops a defense against JIT-ROP attacks that relies
on randomly switching between two copies of a program’s code at
runtime, while ensuring that calls from one copy return to the same
copy. The mechanism for ensuring this is similar to shadow stack,
with its potential for impacting compatibility. More important, the
content of the shadow stack needs to be protected from disclosures.
Rather than protecting code pointers from being leaked, Secret
[62] leverages its use of runtime code pointer translation to make
leaked pointers useless. In particular, this translation can incorpo-
rate a random permutation of the code space, thereby destroying
any relationship between a leaked pointer and the locations of
nearby gadgets. Unfortunately, address translation imposes signifi-
cant overhead. CodeArmor [11] reduces this overhead by using a
random linear offset for translation, instead of the hashtable needed
for a permutation. However, this makes the method susceptible to
attacks that infer this random value. This is mitigated by CodeAr-
mor’s ability for runtime re-randomization.
Shuffler [57] is another technique that implements runtime re-
randomization. It only adds redirection to indirect jump/call tar-
gets while relying on heuristics to protect return addresses. While
Shuffler required compiler help to randomize binaries, Egalito [58]
eliminates this requirement for x86_64 binaries.
Whereas the above techniques use a combination of techniques
to protect against code disclosures, recent works have gravitated
towards execute-only (i.e., non-readable) code [12, 22, 51, 56] for
defending against JIT-ROP attacks. Since this technique imposes
very low overheads and is also very strong due to its reliance on
hardware memory protection, we will also rely on existing imple-
mentations of this mechanism for JIT-ROP defense.
Readactor [14] is a comprehensive compiler-based mitigation
for code reuse attacks. As noted earlier, PHR is a stack-unwinding-