tion techniques mainly use standard statistical correlation metrics to
correlate the vectors of flow timings and sizes across flows. In the
following, we overview the major types of statistical correlation
metrics used by previous flow correlation algorithms.
Mutual Information The mutual information metric measures
the dependency of two random variables. It, therefore, can be used
to quantify the correlation of flow features across flows, e.g., the
traffic features of an egress Tor flow depends on the features of its
corresponding ingress flow. The mutual information technique has
been used by Chothia et al. [12] and Zhu et al. [84] to link flows.
This metric, however, requires a long vector of features (e.g., long
flows) in order to make reliable decisions, as it needs to reconstruct
and compare the empirical distributions of traffic features of target
flows.
Pearson Correlation The Pearson Correlation coefficient is a
classic statistical metric for linear correlation between random
variables. Unlike the mutual information metric, the Pearson Cor-
relation metric does not need to build the empirical distribution
3
of the variables it is correlating, and therefore can be applied on
a shorter length of data. The Pearson Correlation metric has been
used by several flow correlation systems [42, 65].
Cosine Similarity The Cosine similarity metric measures the
angular similarity of two random variables. Similar to the Pearson
coefficient, it can be directly applied on the sample vectors of two
random variables. This metric has been used by different timing
and size correlation systems [32, 50] to link network flows.
Spearman Correlation The Spearman rank correlation metric
measures the statistical dependence between the rankings of two
variables. The metric can be defined as the Pearson correlation
between ranked variables. The recent work of RAPTOR [69] uses
this metric to correlate Tor flows.
2.3 Flow Correlation Attacks on Tor
Flow correlation is the core technique used in a broad range of
attacks studied against Tor (and other anonymity systems). To be
able to perform flow correlation, an adversary needs to observe
(i.e., intercept) some fraction of flows entering and exiting the
Tor network. The adversary can then deanonymize a specific Tor
connection, if she is able to intercept both of the ingress and egress
segments of that Tor connection (by performing a flow correlation
algorithm on those flow segments). Therefore, an adversary can
increase her chances of deanonymizing Tor connections by trying
to intercept a larger fraction of Tor’s ingress and egress flows.
There are two main approaches an attacker can take to increase
the fraction of Tor connections she is intercepting. First, by running
a large number of Tor relays and recording the traffic features of
the Tor connections they relay. Various studies have shown that
an adversary with access to such malicious relays can increase
her chances of intercepting the both ends of a Tor connection in
different ways [3, 8, 26, 46, 79]. For instance, Borisov et al. [8]
demonstrate an active denial-service-attack to increase the chances
of intercepting the ingress and egress segments of a target client’s
Tor traffic. The Tor project has adopted the concept of Tor guard
relays [19] to reduce the chances of performing flow correlation by
an adversary controlling malicious relays, an attack known as the
predecessor attack [79].
Alternatively, an adversary can increase her opportunities of per-
forming flow correlation by controlling/wiretapping autonomous
systems (ASes) or Internet exchange points (IXPs), and record-
ing the traffic features of the Tor connections that they transit.
Several studies [20, 49, 69] demonstrate that specific ASes and
IXPs intercept a significant fraction of Tor traffic, therefore are
capable of performing flow correlation on Tor at large scale. Oth-
ers [18, 36, 37, 67, 69] show that an AS-level adversary can further
increase her chances of flow correlation by performing various
routing manipulations that reroute a larger fraction of Tor connec-
tions through her adversarial ASes and IXPs. For instance, Starov
et al. [67] recently show that approximately 40% of Tor circuits are
vulnerable to flow correlation attacks by a single malicious AS, and
Sun et al. [69] show that churn in BGP as well as active manipula-
tion of BGP updates can amplify an adversarial AS’s visibility on
Tor connections. This has lead to various proposals on deploying
AS-aware path selection mechanisms for Tor [2, 18, 51].
2.4 This Paper’s Contributions
While flow correlation is the core of a multitude of attacks on
Tor [3, 8, 18, 20, 26, 36, 37, 46, 49, 51, 67, 69, 69, 79], existing flow
correlation algorithms are assumed to be ineffective in linking Tor
connections reliably and at scale [35, 52, 63]. This is due to Tor’s
extremely noisy network that applies large perturbations on Tor
flows, therefore rendering traffic features across associated ingress
and egress Tor flows hard to get reliably correlated. In particular,
Tor’s network applies large network jitters on Tor flows, which is
due to congestion on Tor relays, and many Tor packets are frag-
mented and repacketized due to unreliable network conditions.
Consequently, existing flow correlation techniques offer poor corre-
lation performances—unless applied to very large flow observations
as well as unrealistically small sets of target flows.3 For instance,
the state-of-the-art correlation technique of Sun et al. [69] needs to
observe 100MB of traffic from each target flow for around 5 min-
utes to be able to perform reliable flow correlations. Such long flow
observations not only are impractical due to the short-lived nature
of typical Tor connections (e.g., web browsing sessions), but also
impose unbearable storage requirements if applied at large scale
(e.g., a malicious Tor relay will likely intercepte tens of thousands
of concurrent flows). Moreover, existing techniques suffer from
high rates of false positive correlations unless applied on an unre-
alistically small set of suspected flows, e.g., Sun et al. [69] correlate
among a set of only 50 target flows.
Our Approach: We believe that the main reason for the ineffec-
tiveness of existing flow correlation techniques is the intensity as
well as the unpredictability of network perturbations in Tor. We
argue that previous flow correlation techniques are inefficient in
correlating Tor traffic since they make use of general-purpose statis-
tical correlation algorithms that are not able to capture the dynamic,
complex nature of noise in Tor. As opposed to using such generic
3Note that active attacks like [65] are out of our scope, as discussed in Section 2.5, since
such attacks are easily detectable, and therefore can not be deployed by an adversary
at large scale for a long time period without being detected.
4
statistical correlation metrics, in this paper we use deep learning
to learn a correlation function that is tailored to Tor’s ecosystem. We
design a flow correlation system, called DeepCorr, that learns a
flow correlation function for Tor, and uses the learned function to
cross-correlate live Tor connections. Note that contrary to website
fingerprinting attacks [9, 25, 55, 72, 73], DeepCorr does not need
to learn any target destinations or target circuits; instead Deep-
Corr learns a correlation function that can be used to link flows
on arbitrary circuits, and to arbitrary destinations. In other words,
DeepCorr can correlate the two ends of a Tor connection even if
the connection destination has not been part of the learning set.
Also, DeepCorr can correlate flows even if they are sent over Tor
circuits different than the circuits used during the training process.
We demonstrate DeepCorr’s strong correlation performance
through large scale experiments on live Tor network, which we
compare to previous flow correlation techniques. We hope that our
study raises concerns in the community on the increasing risks
of large-scale traffic analysis on Tor in light of emerging learning
algorithms. We discuss potential countermeasures, and evaluate
DeepCorr’s performance against existing countermeasures.
2.5 Related Topics Out of Our Scope
Active flow correlation (watermarking) Network flow water-
marking is an active variant of the flow correlation techniques intro-
duced above. Similar to passive flow correlation schemes, flow wa-
termarking aims at linking network flows using traffic features that
persist content obfuscation, i.e., packet sizes and timings. By con-
trast, flow watermarking systems need to manipulate the traffic fea-
tures of the flows they intercept in order to be able to perform flow
correlation. In particular, many flow watermarking systems [27–
29, 31, 59, 76, 81] perturb packet timings of the intercepted flows by
slightly delaying network packets to modulate an artificial pattern
into the flows, called the watermark. For instance, RAINBOW [31]
manipulates the inter-packet delays of network packets in order
to embed a watermark signal. Several proposals [30, 41, 59, 76, 81],
known as interval-based watermarks, work by delaying packets
into secret time intervals.
While passive flow correlation attacks (studied in this paper) are
information theoretically undetectable, a watermarking adversary
may reveal herself by applying traffic perturbations that differ from
that of normal traffic. Some active correlation techniques [11, 65]
do not even aim for invisibility, therefore they can be trivially
detected and disabled, making them unsuitable for large scale flow
correlation. Additionally, while passive flow correlation algorithms
can be computed offline, flow watermarks need to be performed by
resourceful adversaries who are able to apply traffic manipulations
on live Tor connections. In this paper, we only focus on passive
flow correlation techniques.
Website Fingerprinting Website fingerprinting attacks [9, 22,
23, 25, 38, 44, 54, 55, 72–74] use a different threat model than flow
correlation techniques. In website fingerprinting, an adversary in-
tercepts a target client’s ingress Tor traffic (e.g., by wiretapping
the link between a Tor client and her guard relay), and compares
the intercepted ingress Tor connection to the traffic fingerprints
of a finite (usually small) set of target websites. This is unlike flow
correlation attacks in which the adversary intercepts the two ends of
an anonymous connection, enabling the attacker to deanonymize
arbitrary senders and receivers. Existing website fingerprinting
systems leverage standard machine learning algorithms such as
SVM and kNN to classify and identify target websites, and recent
work [64] has investigated the use of deep learning for website
fingerprinting. In contrary, as overviewed in Section 2.2, prior pas-
sive flow correlation techniques use statistical correlation metrics
to link traffic characteristics across network flows. We consider
website fingerprinting orthogonal to our work as it is based on
different threat model and techniques.
3 INTRODUCING DeepCorr
In this section, we introduce our flow correlation system, called
DeepCorr, which uses deep learning algorithms to learn correlation
functions.
3.1 Features and Their Representation
Similar to existing flow correlation techniques overviewed earlier,
our flow correlation system uses the timings and sizes of network
flows to cross-correlate them. A main advantage [21] of deep learn-
ing algorithms over conventional learning techniques is that a deep
learning model can be provided with raw data features as opposed to
engineered traffic features (like those used by SVM- and kNN-based
website fingerprinting techniques [9, 22, 23, 25, 44, 54, 55, 72, 73]).
This is because deep learning is able to extract complex, effective
features from the raw input features [21] itself. Therefore, DeepCorr
takes raw flow features as input, and uses them to derive complex
features, which is used by its correlation function.
We represent a bidirectional network flow, i, with the following
array:
Fi = [T u
i
; Su
i
;T d
i
i ]
; Sd
where T is the vector of inter-packet delays (IPD) of the flow i,
S is the vector of i’th packet sizes, and the u and d superscripts
represent “upstream” and “downstream” sides of the bidirectional
flow i (e.g., T u
is the vector of upstream IPDs of i). Also, note that
i
we only use the first ℓ elements of each of the vectors, e.g., only
the first ℓ upstream IPDs. If a vector has fewer than ℓ elements, we
pad it to ℓ by appending zeros. We will use the flow representation
Fi during our learning process.
Now suppose that we aim at correlating two flows i and j (say i
was intercepted by a malicious Tor guard relay and j was intercepted
by an accomplice exit relay). We represent this pair of flows with
the following two-dimensional array composed of 8 rows:
Fi, j = [T u
i
;T u
j
;T d
i
;T d
j
; Su
i
; Su
j
; Sd
i
j ]
; Sd
where the lines of the array are taken from the flow representations
Fi and Fj.
3.2 Network Architecture
We use a Convolutional Neural Network (CNN) [21] to learn a
correlation function for Tor’s noisy network. We use a CNN since
network flow features can be modeled as time series, and the CNNs
are known to have good performance on time series [21]. Also,
the CNNs are invariant to the position of the patterns in the data
stream [21], which makes them ideal to look for possibly shifted
traffic patterns.4
Figure 2 shows the structure of DeepCorr’s CNN network. The
network takes a flow pair Fi, j as the input (on the left side). Deep-
Corr’s architecture is composed of two layers of convolution and
three layers of a fully connected neural network. The first convolu-
tion layer has k1 kernels each of size (2, w1), where k1 and w1 are
the hyperparameters, and we use a stride of (2, 1). The intuition
behind using the first convolution layer is to capture correlation
between the adjacent rows of the input matrix Fi, j, which are sup-
posed to be correlated for associated Tor flows, e.g., between T u
i
and T u
j
DeepCorr’s second convolution layer aims at capturing traffic
features from the combination of all timing and size features. At
this layer, DeepCorr uses k2 kernels each of size (4, w2), where k2
and w2 are also our hyperparameters, and it uses a stride of (4, 1).
The output of the second convolution layer is flattened and fed
to a fully connected network with three layers. DeepCorr uses
max pooling after each layer of convolution to ensure permutation
invariance and to avoid overfitting [21]. Finally, the output of the
network is:
.
pi, j = Ψ(Fi, j)
which is used to decide if the two input flows in Fi, j are correlated
or not. To normalize the output of the network, we apply a sigmoid
function [21] that scales the output between zero and one. Therefore,
pi, j shows the probability of the flows i and j being associated
(correlated), e.g., being the entry and exit segments of the same Tor
connection.