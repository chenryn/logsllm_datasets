belong to the adware class. A smaller percentage of
downloads in these categories, 3.2% and 2.4% respec-
tivley, are labeled as malware. For repackage+entice the
majority of downloads are labeled as PUP; no malware
is found in this deception/persuasion category.
Notice that the relatively small percentage of malware
is likely due to the fact that our heuristics for grouping
malicious downloads into broad classes is very conserva-
tive, because it requires only a single AV label to contain
an “adware” or “PUP” string to label the sample as be-
longing to the adware or PUP classes (see Section 4.4).
In addition, adware is simply much more prevalent than
malware, making the malware set size look relatively
small compared to the rest of the dataset. Lastly, notice
that only 70%− 75% of all malicious executables were
labeled by the AVs, even after “aging” and rescanning the
Invent+Impersonate+Comply
Invent+Impersonate+Alarm
Repackage+Entice
Malware
0%
5.9%
NA
Adware
0%
11.6%
28.7%
PUP
6.9%
26.9%
34.4%
6.2 SE Detection Classifier
Guided by our measurements around SE attacks that
we presented in Section 5, we devise a set of statistical
features that can be used to accurately detect ad-driven
SE downloads. We focus on ad-driven attacks because
they are responsible for more than 80% of all SE down-
loads we observed (see Table 1).
Problem Definition. Given an executable file down-
load event observed on the network, we first automati-
cally reconstruct its download path, i.e., the sequence of
pages/URLs the user visited to arrive to the HTTP trans-
action carrying the file, as explained in Section 4. Then,
given a set of labeled ad-driven SE download events
(Section 4.4) and also ad-driven benign software down-
loads (Section 5.3), we first translate the download path
of each event into a vector of statistical features. Finally,
we use the obtained labeled dataset of feature vectors to
train a statistical classifier using the Random Forest [10]
algorithm that can detect future ad-driven SE download
attacks and distinguish them from benign ad-driven soft-
ware downloads.
Statistical Features. We now present the set of detec-
tion features we derived and the intuitions behind their
utility. In the following, we assume to be given as input
the download path related to a software download event
observed on the network, which we translate into a fea-
ture vector. Notice that no single feature by itself enables
accurate detection; it’s their combination that allows us
to reach high accuracy.
• Ad-Driven (binary feature). We check whether the
download path contains an ad-related URL. This
feature is computed by matching AdBlock [1] rules
against the sequence of URLs on the download path.
Intuition: while the majority of SE downloads are
784  25th USENIX Security Symposium 
USENIX Association
12
promoted via advertisement (Section 5), only 7%
of benign downloads result from clicking on an ad
(Section 5.3).
• Minimum Ad Domain Age. We measure the age
of each domain on the ad path, namely the sub-
sequence of the download path consisting of ad-
related domains, and use the minimum age across
these domains. Intuition: ad-serving domains that
consistently direct users to malicious ads are of-
ten blacklisted, so they move to new domains. In
essence, this feature is a way of (approximately)
measuring the reputation of the ad path. Our mea-
surements show that the majority of ad paths for the
comply, alarm and entice attack classes all have do-
mains less than one year in age. For benign down-
load paths, this is true in only less than 5% of the
cases.
• Maximum Ad Domain Popularity. Using our
dataset (Section 4), we fist consider all ad-related
domains involved in the download paths observed
in the past (i.e., in the training set). Then, for each
domain, we count the number of distinct download
paths on which the domain appeared, for both ad-
driven SE attacks and the benign download paths.
If the domain is found in more than 1% of the be-
nign download paths, it is discarded. Otherwise, we
compute the number of distinct SE attack paths in
which the domain appeared. Finally, given all ad-
related domains in the download path we are cur-
rently considering, we take maximum number of
times a domain along this path appeared in an SE at-
tack path. Intuition: some ad networks, and the do-
mains from which they serve ads, are more abused
than others, e.g., due to scarce policing of ad-related
fraud and abuse in lower-tier ad networks. There-
fore, they tend to appear more frequently in the
download path of SE downloads. For instance, Ta-
ble 4 in Section 5.2 shows the popularity of ad entry
points for SE downloads.
Intuition:
• Download Domain Age. We measure the number
of days between the download event and the first
time we observed a DNS query to the effective sec-
ond level domain for the download URL (final node
of the web path) using a large historic passive DNS
database.
the vast majority of benign
downloads are delivered from domains that have
been active for a long time because it takes time for
a website to establish itself and attract visitors. On
the other hand, SE domains are often “young” as
they change frequently to avoid blacklisting. Our
data shows that the download domain of over 80%
of the invent+impersonate SE subcategories comply
and alarm are less than one year in age, whereas for
benign download this only holds in 5% of the cases.
• Download Domain Alexa Rank. We measure the
Alexa rank of the domain that served the software
download. We compute this features using the ef-
fective second level domain for the download URL
and the Alexa top 1 million list. Intuition: malicious
executables are more likely to be hosted on unpopu-
lar domains because of their need of avoiding black-
listing. Conversely, benign software downloads are
often hosted on popular domains. For instance,
measurements on our data show that over 60% of
the benign downloads are from domains with an
Alexa rank in the top 100,000. On the other hand,
the more “aggressive” SE downloads, such as those
from the alarm class, are primarily delivered from
very unpopular domains (very few are in the top 1
million). At the same time, the domains involved
in SE attacks that trigger the download of PUP fall
somewhere between, in terms of domain popularity.
6.3 Evaluating the SE Detection Classifier
In this section, we present the results of the evalua-
tion of our SE detection classifier. We start by describing
the composition of the training and test dataset, and then
present an analysis of the false and true positives.
Datasets. To measure the effectiveness of the SE classi-
fier, we use two separate datasets. The first dataset, D1,
which we use to train the classifier, consists of the soft-
ware downloads described and measured in Sections 4
and 5. Specifically, this dataset includes 1,556 SE down-
load paths (we consider all ad-driven SE attacks from
the dataset described in Section 4), and 11,655 benign
download paths.
The second dataset, D2, consists of new executable
downloads (and their reconstructed download paths) that
we collected from the same deployment network in the
three months following the completion of the measure-
ments we presented in Section 5. Notice also that, D2
was collected after the feature engineering phase and af-
ter building our detection classifier. Namely, both the
feature engineering and the training of the classifier were
completed with no access to the data in D2. Overall, D2
contains 1,338 ad-driven SE downloads, and 9,760 be-
nign downloads paths. We label D2 following the steps
outlined in Section 4.4.
Classification Results. After training our SE detection
classifier using dataset D1 and the Random Forest learn-
ing algorithm, we test the classifier on dataset D2.
Table 8 reports the confusion matrix for the classifica-
tion results. The classifier correctly identified over 91%
of the ad-driven SE downloads. Furthermore, it has a
very low false positive rate of 0.5%.
USENIX Association  
25th USENIX Security Symposium  785
13
Table 8: Confusion matrix for the SE detection classifier.
Benign
Ad-Based SE
Benign
99.5%
8.8%
Predicted Class
Ad-Based SE
0.5%
91.2%
Table 9: SE Subclass Performance.
Repackage+Entice
Invent+Impersonate+Alarm
Invent+Impersonate+Comply
True Positives
65%
98%
90%
Figure 9 shows a breakdown of
the classifi-
the subclasses of ad-based SE
cation results for
The invent+impersonate+alarm and in-
downloads.
vent+impersonate+comply categories have 98% and
90% true positive rates, respectfully. The lower perfor-
mance for repackage+entice is due to downloads of legit-
imate software bundled with PUPs from well established
domains. Because these domains are “mixed use,” and
have high popularity or Alexa ranking, they make the de-
tection task more difficult.
Feature Importance. We estimate feature importance
by performing forward feature selection [20]. The sin-
gle feature that provides the largest information gain is
download domain age. Using only that feature we have
a 69% true positive rate and a 6% false positive rate. By
adding maximum ad domain popularity, we obtain a true
positive rate above 80% with less than 3% false positives.
As we add other features (using the forward search), both
the true positives and false positives continue to improve.
Thus, all the features help achieve high accuracy.
7 Discussion
In this paper, we focus exclusively on successful web-
based SE download attacks (we consider the attacks we
collected and study successful because they actually trig-
ger the delivery of malicious software to the victim’s
machine). Social engineering attacks carried over dif-
ferent channels (e.g., email) and that have different ob-
jectives (e.g., phishing attacks to steal personal informa-
tion, rather than malware infections) are not part of our
measurements, and are therefore also not reflected in the
categories of SE tactics we described in Section 3. How-
ever, we believe ours is an important contribution.
In
fact, as defenses for drive-by downloads continue to im-
prove (e.g., through the hardening of browser software
and operating system defenses) we expect the attackers
to increasingly make use of web-based SE attacks for
malware propagation. Therefore, the reconstruction and
analysis of SE download attacks is important because
in-the-wild SE attack samples could be used to better
train users and mitigate the impact of future attacks; thus,
complementing automatic attack detection solutions.
Our study relies on visibility over HTTP traffic and
deep packet inspection. One might think that the in-
ability to analyze HTTPS traffic represents a significant
limitation. However, it is important to take into ac-
count the following considerations. When a user browses
from an HTTPS to an HTTP site, they are often redi-
rected through an unsecured intermediate URL, so that
the Referer field can be populated with the domain
and other information related to the origin site [3]. Alter-
natively, the origin site can set its referrer policies [2]
to achieve the same result without need of intermedi-
ate redirections. As an example, even though many
of the searches performed using search engines such
as Google, Yahoo and Bing occurred over HTTPS, we
were able to identify the search engines as the origin of
web paths because the related domain names appeared
in the Referer field of the subsequent HTTP trans-
actions. Furthermore, modern enterprise networks com-
monly employ SSL man-in-the-middle (MITM) proxies
that decrypt traffic for inspection. Therefore, our SE at-
tack detection system could be deployed alongside SSL
MITM proxies.
Throughout the study, we use the term malicious to
describe the software downloaded as the result of an SE
attack. However, there exist many shades of malicious-
ness and some malicious software (e.g., ransomware,
botnets, etc.) are more “aggressive” than others (e.g.,
adware and PUPs). Therefore, in several parts of the
analysis we broke down our results by distinguishing be-
tween malware, adware and PUPs. As shown in Sec-
tion 6.1, only a relatively small percentage of the SE
downloads collected for our measurements were catego-
rized by our AV-label-based heuristics as malware. The
majority were labeled as adware or PUP. However, we
should notice that AV labels are known to be noisy and
that our labeling heuristics are very conservative (see
Section 4.4). Furthermore, over 25% of the malicious
downloads remained unlabeled due to lack of AV detec-
tion (Section 6.1). Therefore, it is possible that the num-
ber of malware is somewhat higher than reflected in Sec-
tion 6.1. However, the categorization system, network-
level properties and detection results for SE attacks that
deliver adware apply to attacks that result in malware
downloads as well.
While the software downloads and traffic we collected
for our study were collected from a single academic
network, we should consider that the deployment net-
work was very large, serving tens of thousands diverse
users, consisting of users from different ages, cultures
and backgrounds.
Because our SE detection classifier is designed to de-
tect ad-based SE download attacks, an attacker could
evade the system by using tactics other than online ads
to attract the user’s attention (e.g., search or web post, as
discussed in Section 3). However, advertisements are the
786  25th USENIX Security Symposium 
USENIX Association
14
predominant tactic used by attackers because they allow
them to “publish” their SE attacks on sites that already
popular with the targeted victims. In addition, ads are
only shown to the users that “match” their delivery crite-
ria, thus reducing exposure to others (including security
researchers) that could result in the discovery and miti-
gation of these attack vectors.
Another way an attacker may try to evade detection,
is to specifically attempt to evade our statistical features
(see Section 6.2). For instance, to evade the download
domain age and domain Alexa rank features, the attacker
could host the malicious files on a free file sharing site.
This could result in a download domain with an age > 1
year and a high Alexa ranking. However, the ad-driven,
minimum ad domain age and maximum ad domain pop-
ularity features, which are harder for the attacker to con-
trol, could still allow to identify most attacks. For exam-
ple, simply knowing that a software download resulted
from an online ad puts its probability of being malicious
at more than 50%, according to the real-world data we
collected (see Section 5.3). Furthermore, if hosting ma-
licious downloads on free hosting sites became popu-
lar, then a Free File Hosting feature could be added to
our feature set, as it is unlikely that many ad-driven be-
nign software downloads are served from free file hosting
sites.