false
true
false
1st
Load
Store
Stbar
Load
true
false
false
Store
true
false
true
Note: Stbar provides Store-Store ordering and
is equivalent to Membar #SS
Table 4. Relaxed Memory Order
2nd
1st
Load
Store
Load
false
false
Store
false
false
Membar
#LS | #LL
#SL | #SS
Membar
#LL | #SL
#LS | #SS
false
#LL: Load-Load Ordering, #LS: Load-Store Ordering
#SL: Store-Load Ordering, #SS: Store-Store Ordering
5.  Implementation of DVMC
Based on the framework described in Section 3., we
added DVMC to a simulator of an aggressive out-of-
order implementation of the SPARC v9 architecture
[28]. SPARC v9 poses a special challenge for consis-
tency veriﬁcation, because it allows runtime switching
between three different consistency models: Total Store
Order (TSO), Partial Store Order (PSO), and Relaxed
Memory Order (RMO). TSO is a variant of Processor
Table 5. Implemented optimizations
Model Optimization
TSO In-Order Write
Buffer
PSO Out-of-Order
Write Buffer
RMO Out-of-Order
Load Execution
Effect
Moves store cache misses off
the critical path
Optimized store issue policy
to reduce write buffer stalls
and coherence trafﬁc
Eliminate pipeline squashes
caused by load-order mis-
speculation
Consistency, a common class of consistency models that
includes Intel IA-32 (x86). PSO is a SPARC-speciﬁc
consistency model that relaxes TSO by allowing reor-
derings between stores. RMO is a variant of Weak Con-
sistency that is similar to the consistency models for
PowerPC and Alpha. DVMC enables switching between
models by using three ordering tables (Table 2-Table 4).
Atomic read-modify-write operations (e.g., swap) must
satisfy ordering requirements for both store and load.
SPARC v9 also features a ﬂexible memory barrier
instruction (Membar) that allows exact speciﬁcation of
operation order in a 4-bit mask. The bitmask contains
one bit for load-load (LL), load-store (LS), store-load
(SL), and store-store (SS) ordering. To incorporate such
membars, Table 4’s entries in the Membar rows and col-
umns contain masks instead of boolean values. A bool-
ean value is obtained from the mask by computing the
logical AND between the mask in the instruction and the
mask in the table. If the result is non-zero, ordering is
required.
We started with a baseline system that supports only
sequential consistency but obtains high performance
through load-order speculation and prefetching for both
loads and stores. We then implemented the optimiza-
tions described in Table 5 to take advantage of the
relaxed consistency models. The remainder of the sec-
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:31:09 UTC from IEEE Xplore.  Restrictions apply. 
tion describes the three veriﬁcation mechanisms that
were added to the system, as shown in Figure 2.
prevents cache misses during veriﬁcation and reduces
the pressure on the L1 cache.
5.1.  Uniprocessor Ordering Checker
5.2.  Allowable Reordering Checker
Uniprocessor Ordering is trivially satisﬁed when all
operations execute sequentially in program order. Thus,
Uniprocessor Ordering can be dynamically veriﬁed by
comparing all load results obtained during the original
out-of-order execution to the load results obtained dur-
ing a subsequent sequential execution of the same pro-
gram [8, 5, 3]. Because instructions commit in program
order, results of sequential execution can be obtained by
replaying all memory operations when they commit.
Replay of memory accesses occurs during the veriﬁca-
tion stage, which we add to the pipeline before the
retirement stage. During replay, stores are still specula-
tive and thus must not modify architectural state. Instead
they write to a dedicated veriﬁcation cache (VC).
Replayed loads ﬁrst access the VC and, on a miss,
access the highest level of the cache hierarchy (bypass-
ing the write buffer). The load value from the original
execution resides in a separate structure, but could also
reside in the register ﬁle. In case of a mismatch between
the replayed load value and the original load value, a
Uniprocessor Ordering violation is signalled. Such a
violation can be resolved by a simple pipeline ﬂush,
because all operations are still speculative prior to veriﬁ-
cation. Multiple operations can be replayed in parallel,
independent of register dependencies, as long as they do
not access the same address.
In consistency models that require loads to be
ordered (i.e., loads appear to have executed only after all
older loads performed), the system speculatively reor-
ders loads and detects load-order mis-speculation by
tracking writes to speculatively loaded addresses. This
mechanism allows stores from other processors to
change any load value until the load passes the veriﬁca-
tion stage, and thus loads are considered to perform only
after passing veriﬁcation. To prevent stalls in the veriﬁ-
cation stage, the VC must be big enough to hold all
stores that have been veriﬁed but not yet performed.
In a model that allows loads to be reordered, such as
RMO, no speculation occurs and the value of a load can-
not be affected by any store after it passes the execution
stage. Therefore a load is considered to perform after the
execution stage in these models, and replay strictly
serves the purpose of verifying Uniprocessor Ordering.
Since load ordering does not have to be enforced, load
values can reside in the VC after execution and be used
during replay as long as they are correctly updated by
local stores. This optimization, which has been used in
dynamic veriﬁcation of single-threaded execution [7],
DVMC veriﬁes Allowable Reordering by checking
all reorderings between program order and cache access
order (described in Section 3.) against the restrictions
deﬁned by the ordering table. The position in program
order is obtained by labeling every instruction X with a
sequence number, seqX, that is stored in the ROB during
decode. Since operations are decoded in program order,
seqX equals X’s rank in program order. The rank in per-
form order is implicitly known, because we verify
Allowable Reordering when an operation performs. The
Allowable Reordering checker uses the sequence num-
bers to ﬁnd reorderings and check them against the
ordering table. For this purpose, the checker maintains a
counter register for every operation type OPx (e.g., load
or store) in the ordering table. This counter, max{OPx},
contains the greatest sequence number of an operation
of type OPx that has already performed. When operation
X of type OPx performs, the checker veriﬁes that seqX >
max{OPy} for all operation types OPy that have an
ordering relation OPx<cOPy according to the ordering
table. If all checks pass, the checker updates max{OPx}.
Otherwise an error has been detected.
It is crucial for the checker that all committed opera-
tions perform eventually. The checker can detect lost
operations by checking outstanding operations of all
operation types OPx, with an ordering requirement
OPx<cOPy, when an operation Y of type OPy performs.
If an operation of type OPx older than Y is still outstand-
ing, it was lost and an error is detected. In our imple-
mentation, we check outstanding operations before
Membar instructions by comparing counters of commit-
ted and performed memory accesses. To prevent long
error detection latencies, artiﬁcial Membars are injected
periodically. Membar injection does not affect correct-
ness and has negligible performance impact since injec-
tions are infrequent (about one per 100k cycles).
The implementation of an Allowable Reordering
checker for SPARCv9 requires three small additions to
support architecture speciﬁc features: dynamic switch-
ing of consistency models, a FIFO queue to maintain the
perform order of loads until veriﬁcation, and computa-
tion of Membar ordering requirements from a bitmask
as described earlier.
5.3.  Cache Coherence Checker
Static veriﬁcation of Cache Coherence is a well-
studied problem [19,20] and more recently methods
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:31:09 UTC from IEEE Xplore.  Restrictions apply. 
Table 6. Processor parameters
fetch,decode,execute,retire
4
YAGS
Pipeline Stages
Pipeline Width
Branch Predictor
Scheduling Window 64 entries
128 entries
Reorder Buffer
224 integer, 192 FP
Physical Registers
Write Buffer
24 entries
Table 7.  Workloads
Description
Static web server
TPCC-like workload using IBM DB2
SPECjbb 2000 - 3-tier java system
Name
apache 2
oltp
jbb
slashcode Dynamic website using apache, perl and mysql
barnes
barnes-hut from SPLASH2 benchmark suite
32bit-code
5.7%
38.9%
<0.01%
21.7%
<0.01%
have been proposed for dynamic veriﬁcation of coher-
ence [6, 25]. Although any coherence veriﬁcation mech-
anism is sufﬁcient
for DVMC, we reuse the one
introduced as part of DVSC [16], which supports both
snooping and directory protocols and scales well to
larger systems. A detailed description can be found in
our earlier work, but we present a brief sketch here.
We construct the Cache Coherence checker around
the notion of an epoch. An epoch for block b is a time
interval during which a processor has permission to read
(Read-Only epoch) or
read and write (Read-Write
epoch) block b. The time base for epochs can be physi-
cal or logical as long as it guarantees causality. Three
rules for determining coherence violations were intro-
duced and formally proven to guarantee coherence by
Plakal et al. [18]: (1) reads and writes are only per-
formed during appropriate epochs,
(2) Read-Write
epochs to not overlap other epochs temporally, and (3)
the data value of a block at the beginning of every epoch
equals the data value at the end of the most recent Read-
Write epoch. For every epoch at a processor, it sends an
inform message containing epoch start and end times as
well as block value checksums to one of the history ver-
iﬁers co-located with each memory controller. Each his-
tory veriﬁer uses the inform messages to check for
coherence violations on its assigned blocks.
The implementation of
this Cache Coherence
checker requires a Cache Epoch Table (CET) at each
cache and a Memory Epoch Table (MET) at each mem-
ory controller to keep track of the epoch histories. Each
veriﬁer also uses a priority queue to sort incoming
informs by timestamp before processing.
6.  Experimental Methodology
We performed our experiments using Simics [13]
full-system simulation of 8-node multiprocessors. We
conﬁgured the cycle-accurate TFSim processor simula-
tor [15] as shown in Table 6, and we adapted it to sup-
port timing simulation for the SPARC v9 consistency
models TSO, PSO, and RMO, as well as SC. The sys-
tems were conﬁgured with either a MOSI directory
coherence protocol or a MOSI snooping coherence pro-
Table 8. Memory system parameters
L1 Cache (I and D)
L2 Cache
Memory
32 KB, 4-way, 64 byte lines
1 MB, 4-way, 64 byte lines
2 GB, 64 byte blocks
 For Directory Protocol
Network
2D torus, 2.5 GB/s links, unordered
For Snooping Protocol
Address Network
Data Network
bcast tree, 2.5 GB/s links, ordered
2D torus, 2.5 GB/s links, unordered
Coherence Veriﬁcation
Priority Queue
Cache Epoch Table
Memory Epoch Table 48 bits per line in any cache
256 entries
34 bits per line in cache
tocol. All systems use SafetyNet [26] for backward error
recovery, although any other BER scheme (e.g., ReVive
[21]) would work. Conﬁgurations of the directory and
snooping systems are shown in Table 8. Timing infor-
mation was computed using a customized version of the
Multifacet GEMS simulator [14].
Because DVMC primarily targets high-availability
commercial servers, we chose the Wisconsin Commer-
cial Workload Suite [2] for our benchmarks. These
workloads are described brieﬂy in Table 7 and in more
detail by Alameldeen et al. [2]. Although SPARC v9 is a
64-bit architecture, portions of code in the benchmark
suite were written for the 32-bit SPARC v8 instruction
set. Since these code segments were written for TSO, a
system conﬁgured for PSO or RMO must switch to TSO
while executing 32-bit code. Table 7 shows the average
fraction of 32-bit memory operations executed for each
benchmark during our experiments.
To handle the runtime variability inherent in com-
mercial workloads, we run each simulation ten times
with small pseudo-random perturbations. Our experi-
mental results show mean result values as well as error
bars that correspond to one standard deviation.
7.  Evaluation
We used simulation to empirically conﬁrm DVMC’s
error detection capability and gain insight into its impact
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:31:09 UTC from IEEE Xplore.  Restrictions apply. 
e
m
i
t
n
u
R
d
e
z
i
l
a
m
r
o
N
1.0
0.5
0.0
apache
oltp
jbb
slash
barnes
Figure 3. Workload runtimes for directory coherence
e
m
i
t
n
u
R
d
e
z
i
l
a
m
r
o
N
1.0
0.5
0.0
apache
oltp
jbb
slash
barnes
Figure 4. Workload runtimes for snooping coherence
Base SC  
DVSC
Base TSO 
DVTSO
Base PSO 
DVPSO
Base RMO 
DVRMO
Base SC  
DVSC
Base TSO 
DVTSO
Base PSO 
DVPSO
Base RMO 
DVRMO
on error-free performance. In this section, we describe
the results of
these experiments, and we discuss
DVMC’s hardware costs and interconnect bandwidth
overhead.
7.1.  Error Detection
We tested the error detection capabilities of DVMC
by injecting errors into all components related to the
memory system:
the load/store queue (LSQ), write
buffer, caches,
interconnect switches and links, and
memory and cache controllers. The injected errors
included data and address bit ﬂips; dropped, reordered,
mis-routed, and duplicated messages; and reorderings
and incorrect forwarding in the LSQ and write buffer.
For each test, an error time, error type, and error loca-
tion were chosen at random for injection into a running
benchmark. After injecting the error, the simulation con-
tinued until the error was detected. Since errors become
non-recoverable once the last checkpoint taken before
the error expires, we also checked that a valid check-
point was still available at the time of detection. We con-
ducted these experiments
supported
consistency models with both the directory and snoop-
ing systems. DVMC detected all injected errors well
for all
four
within the SafetyNet recovery time frame of about 100k
processor cycles.
7.2.  Performance
Besides error detection capability, error-free perfor-
mance is the most important metric for an error detec-
tion mechanism. To determine DVMC performance, we
ran each benchmark for a ﬁxed number of transactions
and compared the runtime on an unprotected system and
a system implementing DVMC with different consis-
tency models. We considered barnes to be a single
transaction, and we ran it to completion.
7.2.1.  Baseline System. Before
looking at DVMC
overhead, we compare the performance of unprotected
systems (no DVMC or BER) with different memory
consistency models. The “Base” numbers in Figure 3
and Figure 4 show the relative runtimes, normalized to
SC. The addition of a write buffer in the TSO system
improves performance for almost all benchmarks. PSO
and RMO do not show signiﬁcant performance beneﬁts
and can even lead to performance degradation, although