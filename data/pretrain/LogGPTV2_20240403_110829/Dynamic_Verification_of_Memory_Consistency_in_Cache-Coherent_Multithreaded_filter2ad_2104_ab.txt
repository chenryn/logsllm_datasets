# Memory Ordering and Consistency Verification in DVMC

## Table 4: Relaxed Memory Order
| 1st | 2nd | Load | Store | Stbar | Membar |
|-----|-----|------|-------|-------|--------|
| Load | Load | false | true  | false | #LL    |
|      | Store | false | false | true  | #LS    |
|      | Stbar | false | false | true  | #SS    |
|      | Membar | #LL | #LS | #SS | -      |
| Store | Load | false | false | false | #SL    |
|       | Store | false | false | false | #SS    |
|       | Stbar | false | false | false | #SS    |
|       | Membar | #SL | #SS | #SS | -      |

**Note:** `Stbar` provides `Store-Store` ordering and is equivalent to `Membar #SS`.

## 5. Implementation of DVMC

Based on the framework described in Section 3, we integrated DVMC into a simulator for an aggressive out-of-order implementation of the SPARC v9 architecture [28]. The SPARC v9 architecture presents a unique challenge for consistency verification because it allows runtime switching between three different consistency models: Total Store Order (TSO), Partial Store Order (PSO), and Relaxed Memory Order (RMO). 

- **Total Store Order (TSO):** A variant of Processor Consistency, common in consistency models such as Intel IA-32 (x86).
- **Partial Store Order (PSO):** A SPARC-specific model that relaxes TSO by allowing reordering between stores.
- **Relaxed Memory Order (RMO):** A variant of Weak Consistency, similar to the consistency models for PowerPC and Alpha.

DVMC enables switching between these models using three ordering tables (Tables 2-4). Atomic read-modify-write operations (e.g., swap) must satisfy ordering requirements for both store and load. SPARC v9 also features a flexible memory barrier instruction (`Membar`) that allows precise specification of operation order using a 4-bit mask. The bitmask contains one bit each for `Load-Load (LL)`, `Load-Store (LS)`, `Store-Load (SL)`, and `Store-Store (SS)` ordering. To incorporate these membars, the entries in Table 4's `Membar` rows and columns contain masks instead of boolean values. A boolean value is derived from the mask by computing the logical AND between the mask in the instruction and the mask in the table. If the result is non-zero, ordering is required.

We started with a baseline system that supports only sequential consistency but achieves high performance through load-order speculation and prefetching for both loads and stores. We then implemented the optimizations listed in Table 5 to leverage the relaxed consistency models.

### Table 5: Implemented Optimizations
| Model   | Optimization                | Effect                                                                                     |
|---------|-----------------------------|--------------------------------------------------------------------------------------------|
| TSO     | In-Order Write Buffer        | Moves store cache misses off the critical path                                             |
| PSO     | Out-of-Order Write Buffer    | Optimized store issue policy to reduce write buffer stalls and coherence traffic           |
| RMO     | Out-of-Order Load Execution  | Eliminates pipeline squashes caused by load-order mis-speculation                         |

The remainder of this section describes the three verification mechanisms added to the system, as shown in Figure 2.

## 5.1 Uniprocessor Ordering Checker

Uniprocessor Ordering is trivially satisfied when all operations execute sequentially in program order. Therefore, Uniprocessor Ordering can be dynamically verified by comparing all load results obtained during the original out-of-order execution to the load results obtained during a subsequent sequential execution of the same program [8, 5, 3]. Since instructions commit in program order, results of sequential execution can be obtained by replaying all memory operations when they commit.

Replay of memory accesses occurs during the verification stage, which we add to the pipeline before the retirement stage. During replay, stores are still speculative and thus must not modify architectural state. Instead, they write to a dedicated verification cache (VC). Replayed loads first access the VC and, on a miss, access the highest level of the cache hierarchy (bypassing the write buffer). The load value from the original execution resides in a separate structure, but could also reside in the register file. In case of a mismatch between the replayed load value and the original load value, a Uniprocessor Ordering violation is signaled. Such a violation can be resolved by a simple pipeline flush, as all operations are still speculative prior to verification. Multiple operations can be replayed in parallel, independent of register dependencies, as long as they do not access the same address.

In consistency models that require loads to be ordered (i.e., loads appear to have executed only after all older loads performed), the system speculatively reorders loads and detects load-order mis-speculation by tracking writes to speculatively loaded addresses. This mechanism allows stores from other processors to change any load value until the load passes the verification stage, and thus loads are considered to perform only after passing verification. To prevent stalls in the verification stage, the VC must be large enough to hold all stores that have been verified but not yet performed.

In models that allow loads to be reordered, such as RMO, no speculation occurs, and the value of a load cannot be affected by any store after it passes the execution stage. Therefore, a load is considered to perform after the execution stage in these models, and replay strictly serves the purpose of verifying Uniprocessor Ordering. Since load ordering does not have to be enforced, load values can reside in the VC after execution and be used during replay as long as they are correctly updated by local stores. This optimization, which has been used in dynamic verification of single-threaded execution [7], prevents cache misses during verification and reduces the pressure on the L1 cache.

## 5.2 Allowable Reordering Checker

DVMC verifies Allowable Reordering by checking all reorderings between program order and cache access order (described in Section 3) against the restrictions defined by the ordering table. The position in program order is obtained by labeling every instruction X with a sequence number, `seqX`, stored in the ROB during decode. Since operations are decoded in program order, `seqX` equals X’s rank in program order. The rank in perform order is implicitly known, as we verify Allowable Reordering when an operation performs. The Allowable Reordering checker uses the sequence numbers to find reorderings and check them against the ordering table. For this purpose, the checker maintains a counter register for every operation type `OPx` (e.g., load or store) in the ordering table. This counter, `max{OPx}`, contains the greatest sequence number of an operation of type `OPx` that has already performed. When operation X of type `OPx` performs, the checker verifies that `seqX > max{OPy}` for all operation types `OPy` that have an ordering relation `OPx <c OPy` according to the ordering table. If all checks pass, the checker updates `max{OPx}`. Otherwise, an error is detected.

It is crucial for the checker that all committed operations perform eventually. The checker can detect lost operations by checking outstanding operations of all operation types `OPx`, with an ordering requirement `OPx <c OPy`, when an operation Y of type `OPy` performs. If an operation of type `OPx` older than Y is still outstanding, it was lost, and an error is detected. In our implementation, we check outstanding operations before `Membar` instructions by comparing counters of committed and performed memory accesses. To prevent long error detection latencies, artificial `Membars` are injected periodically. `Membar` injection does not affect correctness and has negligible performance impact since injections are infrequent (about one per 100k cycles).

The implementation of an Allowable Reordering checker for SPARCv9 requires three small additions to support architecture-specific features: dynamic switching of consistency models, a FIFO queue to maintain the perform order of loads until verification, and computation of `Membar` ordering requirements from a bitmask as described earlier.

## 5.3 Cache Coherence Checker

Static verification of Cache Coherence is a well-studied problem [19, 20], and more recently, methods have been proposed for dynamic verification of coherence [6, 25]. Although any coherence verification mechanism is sufficient for DVMC, we reuse the one introduced as part of DVSC [16], which supports both snooping and directory protocols and scales well to larger systems. A detailed description can be found in our earlier work, but we present a brief sketch here.

We construct the Cache Coherence checker around the notion of an epoch. An epoch for block `b` is a time interval during which a processor has permission to read (Read-Only epoch) or read and write (Read-Write epoch) block `b`. The time base for epochs can be physical or logical as long as it guarantees causality. Three rules for determining coherence violations were introduced and formally proven to guarantee coherence by Plakal et al. [18]:

1. Reads and writes are only performed during appropriate epochs.
2. Read-Write epochs do not overlap other epochs temporally.
3. The data value of a block at the beginning of every epoch equals the data value at the end of the most recent Read-Write epoch.

For every epoch at a processor, it sends an inform message containing epoch start and end times as well as block value checksums to one of the history verifiers co-located with each memory controller. Each history verifier uses the inform messages to check for coherence violations on its assigned blocks.

The implementation of this Cache Coherence checker requires a Cache Epoch Table (CET) at each cache and a Memory Epoch Table (MET) at each memory controller to keep track of the epoch histories. Each verifier also uses a priority queue to sort incoming informs by timestamp before processing.

## 6. Experimental Methodology

We performed our experiments using Simics [13] full-system simulation of 8-node multiprocessors. We configured the cycle-accurate TFSim processor simulator [15] as shown in Table 6, and we adapted it to support timing simulation for the SPARC v9 consistency models TSO, PSO, and RMO, as well as SC. The systems were configured with either a MOSI directory coherence protocol or a MOSI snooping coherence protocol. All systems use SafetyNet [26] for backward error recovery, although any other BER scheme (e.g., ReVive [21]) would work. Configurations of the directory and snooping systems are shown in Table 8. Timing information was computed using a customized version of the Multifacet GEMS simulator [14].

Because DVMC primarily targets high-availability commercial servers, we chose the Wisconsin Commercial Workload Suite [2] for our benchmarks. These workloads are described briefly in Table 7 and in more detail by Alameldeen et al. [2]. Although SPARC v9 is a 64-bit architecture, portions of code in the benchmark suite were written for the 32-bit SPARC v8 instruction set. Since these code segments were written for TSO, a system configured for PSO or RMO must switch to TSO while executing 32-bit code. Table 7 shows the average fraction of 32-bit memory operations executed for each benchmark during our experiments.

To handle the runtime variability inherent in commercial workloads, we run each simulation ten times with small pseudo-random perturbations. Our experimental results show mean result values as well as error bars that correspond to one standard deviation.

## 7. Evaluation

We used simulation to empirically confirm DVMC’s error detection capability and gain insight into its impact on error-free performance. In this section, we describe the results of these experiments and discuss DVMC’s hardware costs and interconnect bandwidth overhead.

### 7.1 Error Detection

We tested the error detection capabilities of DVMC by injecting errors into all components related to the memory system: the load/store queue (LSQ), write buffer, caches, interconnect switches and links, and memory and cache controllers. The injected errors included data and address bit flips; dropped, reordered, mis-routed, and duplicated messages; and reorderings and incorrect forwarding in the LSQ and write buffer. For each test, an error time, error type, and error location were chosen at random for injection into a running benchmark. After injecting the error, the simulation continued until the error was detected. Since errors become non-recoverable once the last checkpoint taken before the error expires, we also checked that a valid checkpoint was still available at the time of detection. We conducted these experiments for all four supported consistency models with both the directory and snooping systems. DVMC detected all injected errors well within the SafetyNet recovery time frame of about 100k processor cycles.

### 7.2 Performance

Besides error detection capability, error-free performance is the most important metric for an error detection mechanism. To determine DVMC performance, we ran each benchmark for a fixed number of transactions and compared the runtime on an unprotected system and a system implementing DVMC with different consistency models. We considered `barnes` to be a single transaction and ran it to completion.

#### 7.2.1 Baseline System

Before looking at DVMC overhead, we compare the performance of unprotected systems (no DVMC or BER) with different memory consistency models. The “Base” numbers in Figures 3 and 4 show the relative runtimes, normalized to SC. The addition of a write buffer in the TSO system improves performance for almost all benchmarks. PSO and RMO do not show significant performance benefits and can even lead to performance degradation, although they provide more flexibility in reordering operations.

### Figures
- **Figure 3:** Workload runtimes for directory coherence
- **Figure 4:** Workload runtimes for snooping coherence

**Legend:**
- Base SC: Sequential Consistency
- DVSC: Dynamic Verification with Sequential Consistency
- Base TSO: Total Store Order
- DVTSO: Dynamic Verification with TSO
- Base PSO: Partial Store Order
- DVPSO: Dynamic Verification with PSO
- Base RMO: Relaxed Memory Order
- DVRMO: Dynamic Verification with RMO

---

This document provides a comprehensive overview of the DVMC implementation, including the verification mechanisms and the experimental methodology used to evaluate its performance and error detection capabilities.