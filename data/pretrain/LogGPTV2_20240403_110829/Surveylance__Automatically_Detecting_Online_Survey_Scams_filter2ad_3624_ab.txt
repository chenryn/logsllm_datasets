B. Feature Set
To construct a detection model, we rely on extracting
information from the content of websites, network trafﬁc,
widgets, and the overall presentation of the page. In the
following, we provide more details on the features, and our
intuition for choosing them.
1) Indicative images: The main page of survey gateways
is usually well-designed and is comparable to a typical user-
friendly website. Adversaries usually make extensive use of
images to encourage users to ﬁll out a survey – such as logos
that indicate Satisfaction Guaranteed or Guaranteed Income.
The presence of such images, along with other features, can be
72
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:37:19 UTC from IEEE Xplore.  Restrictions apply. 
a good indicator of survey gateways. To employ this indicator
as a feature, we extracted all the images from our labeled
survey scam dataset and clustered them using a perceptual hash
function [54]. We ﬁrst used structural similarity testing [50]
as our image comparison technique. However, we found that
the perceptual hash function was more robust to small image
changes as the images in survey gateways are often small
and their representation often stays intact. SURVEYLANCE
computes the centroid perceptual hash value as the centroid
of the cluster, which is representative of all the images in the
cluster. For a given website, after extracting all the included
images, SURVEYLANCE computes the perceptual hash value
of each image, and compares the value with the centroid of
the perceptual hash values of all the clusters. If the Hamming
distance between two perceptual hash values is less than 0.17
(an experimentally-derived threshold), we label the image as
indicative of a survey gateway. We then report the number
of indicative images and incorporate this value as a numeric
feature. The intuition here is that the pages that contain more
indicative images are more likely to be survey gateways.
2) User input ﬁelds: Survey gateways usually require users
to enter their personal information such as the home address,
employer, email address, or phone number, using textﬁelds be-
fore redirecting them to a particular publisher. SURVEYLANCE
extracts the total number of textﬁeld input tags in the website.
The rationale is that while survey gateways claim that surveys
are anonymous, they nevertheless attempt to elicit Personally
Identiﬁable Information (PII) from users. This is borne out in
our experiments which show that more than 83% of samples
had at least four textﬁelds that required sensitive information
including Social Security Number and payment information.
SURVEYLANCE reports the total number of textﬁelds as a
numeric feature.
3) Third-party scripts: We expect benign pages, including
benign survey pages, to have a lower ratio of third-party inclu-
sions to decrease the risk of unwanted information leakage to
third-parties. In fact, third-party scripts have, by default, full
control over the content of pages in which they are included.
Such code can potentially inspect and modify values that a
local JavaScript would be able to do. We performed an analysis
on the number of third-party inclusions on labeled survey
gateways and benign survey pages (see Appendix A). Our
analysis shows that survey gateways include a signiﬁcantly
larger number of third-party scripts (e.g., advertisements).
SURVEYLANCE looks for third-party script inclusions in the
HTML code of survey gateways and uses the ratio of third-
party links to the total number of links as a feature.
4) Link length: Our manual analysis shows that survey
gateways use advertisements as a major source of revenue.
We extract all  HTML tags and calculate the length of
the string in each link. The intuition in using this feature is that
these links tend to pass more and longer parameters which are
mainly used to track clicks, ﬁngerprint users for ad-retargeting,
and carry the publisher ID or LocalStorage keys and values.
We calculate the mean and maximum link length of third-party
links.
5) Website structure: While the main page of survey
gateways is often presented convincingly, these websites are
usually undeveloped since they are solely designed to expose
users to security threats. As a consequence, it can be the
case that these websites do not follow common practices,
such as deﬁning dedicated directories for speciﬁc purposes.
For example, a typical website is comprised of HTML, CSS,
image, and JavaScript ﬁles in different folders forming a
directory tree. Therefore, the system searches the source code
and ﬁnds indications of directory presence. For this goal,
SURVEYLANCE extracts all the local inclusions and parses the
inclusion paths. The system uses this as a boolean feature in
our detection where the value 1 means that the contents of the
website are structured and maintained as a directory tree.
6) Web content: Survey gateways usually do not contain a
large volume of content, and mainly include enticing images
to lure users into ﬁlling out a survey. A large fraction of text
in survey gateways is, in fact, the included URLs that are
not visible to users. SURVEYLANCE computes the ratio of the
volume of text in the links’  HTML tags to the total
volume of text in the page. We use this ratio as a numeric
feature.
7) Sequence of words: SURVEYLANCE seeks the presence
of particular word sequences that are indicative of survey
gateways. The intuition is that sequence of words that appear
more frequently in survey pages than in non-survey pages
can be used to mark potential survey scam websites – for
example, guaranteed reward, easy income. To this end, we
generate n-grams by varying the length of n from n = 2 to
n = 6 out of any text found in the main body of labeled
survey gateways. We then select the most prevalent n-grams by
measuring their importance in the labeled dataset using Term
Frequency – Inverse Document Frequency (TF-IDF) [33].
SURVEYLANCE measures the frequency of the selected n-
grams and incorporates these frequencies as a set of numeric
values representing the frequency of n-grams (1 
elements is very common in survey gateways to embed videos
or pop-ups to show advertisements. SURVEYLANCE extracts
all  and  elements present on a page
and its child frames, and incorporates this data as a numeric
feature by counting the number of frames and iframes.
C. Prototype Implementation
SURVEYLANCE consists of three independent modules:
(1) a crawling module which manages browser instances and
serves as a data extractor, (2) a classiﬁcation module which
assigns a label to a given URL, and (3) a form ﬁller module
that automatically completes surveys in survey publishers. In
the following paragraphs, we provide more details on the
implementation details of each module.
1) Crawling Module: To manage browser instances, we
developed a scheduler which is responsible for instantiating
the browser instances with a pre-speciﬁed conﬁguration set-
ting. It also assigns a crawling job to each browser instance
which consists of 10,000 websites. The scheduler restarts each
browser instance after completing the crawling job to reduce
the potential risks of a compromised browsing instance. In
addition, we modiﬁed the user-agent properties of the browser
instance to emulate a typical user browsing the web using
a Microsoft Windows OS. We developed a custom Chrome
extension which relies on the Chrome debugging protocol,
and operates on top of the DevTool Extension API [10]. This
approach provides instrumentation, inspection, and proﬁling
of Chromium and enables us to access all the functionality of
DevTool as well as DOM and DOM Events of a page for data
collection.
The custom extension allows us to have nearly full cov-
erage of browser interactions with a given website in order
to collect HTML source code and network traces to construct
redirection chains – the paths that show how a user is exposed
to a survey scam. The approach that we used in developing
SURVEYLANCE is related to some of the previously proposed
concepts to detect outdated JavaScript libraries [20] as well
as malicious JavaScript inclusions [4]. We use the collected
data to construct the feature vectors and perform classiﬁcation
using the classiﬁcation module.
To increase the level of interaction with websites while
visiting a page,
the crawler scrolls downwards to activate
potential event listeners on the page which might load other
dynamic content. The crawler remains on each page for 90
seconds before restarting the session and opening the next
website in the crawling job. We updated the browser extension
to automatically ﬁnd the required ﬁelds, and populate the
ﬁelds with the data that satisﬁes each element type in a given
survey page. We present more details on this extension in
Section III-C3.
2) Classiﬁcation Module: Our approach requires construc-
tion of a classiﬁer that can analyze the data collected by the
crawler module, and reliably detect survey gateways using the
features described in Section III-B. That is, the classiﬁer should
take a URL as an input, build a feature vector from the crawled
data, and assign a label showing whether a page is a survey
gateway.
To construct the detection model, we ﬁrst need to select an
appropriate learning algorithm that minimizes the false posi-
tives. Furthermore, it should be efﬁcient in the detection phase
to avoid impacting the performance or scalability of the end-
points. To this end, we tested multiple classiﬁcation algorithms,
and found that a random forest [5] classiﬁer produced the best
detection results. In fact, the random forest classiﬁer tended
to be more robust than other models with respect to outliers,
and was relatively more efﬁcient in the detection phase. To
construct the classiﬁcation model, we used the random forest
implementation provided by scikit-learn [36]. As mentioned
in Section III-B, our approach requires extracting visible texts
from a given URL. To extract
language data
presented in a given HTML page, SURVEYLANCE uses Python
Natural Language ToolKit (NLTK) [27]. SURVEYLANCE then
extracts anchor elements using PyQuery [30], and calculates
the length of textual content, length of textual content in links,
and the total length of the original HTML source code.
the natural
SURVEYLANCE implements HTTP Archive 1.2 speciﬁ-
cation [44], and stores the network trafﬁc traces as HAR
objects where each entry has timing, request, and response
information. During the classiﬁcation phase, SURVEYLANCE
parses each HAR object, and analyzes the HTTP requests and
responses. The system traverses over data objects by applying
a regular expression to differentiate between ﬁrst and third
party requests and responses, and calculate the corresponding
feature values such as HTTP request and response size.
that simply checks if
3) Survey Filler Module: As a part of our experiments, we
use the detected survey gateways to reach survey publishers,
and study the types of threats to which a user may get exposed.
To interact with the survey publishers and populate the survey
forms with appropriate data, SURVEYLANCE uses a form ﬁller
module that injects content scripts in the context of the visited
web page. SURVEYLANCE ﬁrst retrieves the identiﬁer of the in-
spected window, and sends a message to the background page
which calls executeScript to run the form ﬁller module.
The extension ﬁnds all
inputs (i.e., input, textarea,
select in ‘XXXX:enabled:not([readonly])’ as
well as ‘[contenteditable]’). Here,
readonly is
the corresponding at-
a selector
tribute is deﬁned on JQuery elements. After
identifying
the element
type of the input by using JQuery element
(jQueryElement.attr(‘type’)), the extension decides
how to generate the input. To this end, the extension seeks to
ﬁnd a pre-deﬁned set of keywords in the ID of the correspond-
ing input ﬁelds; and calls the appropriate input generation
function based on the detected keyword. The pre-deﬁned set
of keywords is constructed per input ﬁeld by observing a
set of survey forms in the labeled dataset. Unsurprisingly,
handling different types of input ﬁelds is a non-trivial task,
and we had to manually verify some of the ﬁelds to be able
to create the pre-deﬁned set of possible IDs that developers
might use in each website. At a high level, we deﬁned at
least two attributes for most of the input ﬁelds. The ﬁrst
attribute, referred to as match, was the pre-deﬁned list of
possible IDs that a website uses for a speciﬁc input ﬁeld,
and the second attribute was sanitized name which we used
to call the corresponding method that generates the input. For
example, if the extension ﬁnds a ﬁeld that contains ‘integer’,
‘numeric’, ‘number’, ‘qty’, ‘price’, ‘quantity’, ‘total’, it calls
the number generator module with a pre-speciﬁed value range.
74
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:37:19 UTC from IEEE Xplore.  Restrictions apply. 
Since some of the input ﬁelds required a speciﬁc format
(e.g, MM-DD-YYYY) or a value range (e.g., age) to pass the
registration phase, we were careful to generate inputs that abide
by these constraints. We also noticed a number of hidden ﬁelds
or CAPTCHAs in some of survey publisher websites, and
decided to ignore such cases. Our form ﬁller handled most of
the potentially required element types in the registration pages
such as checkboxes, dates, email addresses, radio buttons,
texts, URLs, and elements of similar nature. For textarea
elements, the form ﬁller randomly generated a string with a
maximum length of 30 characters. For websites that required a
user registration, we created a set of credentials that pass most
username and password selection policies. However,
there
were several cases that we were not able to cover due to
almost unlimited value possibilities for input ﬁeld IDs which
are determined by developers of websites.
IV. DATA COLLECTION
In this section, we discuss our data collection methodology
to conduct the experiments, and evaluate the effectiveness of
SURVEYLANCE.
A. Sources of Survey Gateways
Constructing a reliable source of labeled data to run our
experiments was quite challenging as there was no central
repository, blacklist, or previous large-scale analysis in this
speciﬁc area. One of the ﬁrst questions that arises is: How
are end-users redirected to online survey scam pages? There
is evidence that users are usually directed to web-based social
engineering attacks, including online survey scams, by being
exposed to malicious advertisements, as shown by recent
studies on malvertising [23], [45], [52] and social engineering
attacks [24]. Note that scammers could trick users into clicking
on direct links to scam pages. However, this approach would
result in a shorter active lifetime in light of increasing detection
capabilities of search engines and blacklist operators. Further-
more, this approach may not be as scalable as leveraging
malicious advertisements where ads can be simply included
into several independent websites, and be delivered to millions
of users.
Therefore,
in this paper,
instead of directly searching
for survey gateways or publishers, we use a more generic
approach. More speciﬁcally, we search for websites that are
more likely to include malicious advertisements, and that are
the representative of what a typical user may be redirected
to in normal browsing sessions. These websites can be used
as the starting point of different types of social engineering
attacks including online survey scams. We take advantage of
the ﬁndings of speciﬁc recent studies on web-based social
engineering attacks [24], [18], [47], and look for websites that
leverage a combination of deception and persuasion to attract
users while taking part in malwaretising practices.
We speciﬁcally search for pages that attempt to attract
normal users by embedding enticing content and encouraging
users to make risky decisions (e.g., clicking on a link, down-
loading a ﬁle). To this end, we incorporated the Google Trends
service to construct a set of popular items in various search
categories. Note that we are not claiming that the Google