1 M packets
1 M packets
1 M queries
100 K packets
100 K packets
1000 packets
1000 packets
8c Switch - 7124FX (64B)
8d Switch - 7124FX (1514B)
Entries marked
a
512 ns
512 ns
534 ns
550 ns
557 ns
1000 packets
535 ns
557 ns
557 ns
1000 packets
return results that are within DAG measurement error-range.
Where Has My Time Gone?
209
Fig. 4. End host latency contribution.
Fig. 5. Network latency contribution.
latency of the ﬁbers, which is often disregarded, has a magnitude of microsec-
onds in big data centers and becomes a signiﬁcant component of the overall
latency. However, unlike any other component, propagation delay is one aspect
that can not be improved, hinting that minimizing the length of the traversal
path through data centers needs to become a future direction of research.
Fig. 6. Diﬀerent network topologies.
4 Tail Latency Results
Results in the previous section range between their stated minimum and the
99.9th percentile. However, our experiments also provide insight into heavy-tail
properties of the measured latency. Such results, which are not caused by network
congestion or other oft-stated causes of tail-latency, are brieﬂy discussed in this
section.
The relative scale of these tail latency cases is usefully illustrated by con-
sidering the TSC (1). The tail latency values are clearly illustrated when using
the TSC experiment (Sect. 2.2) and all subsequent experiments using the TSC
measurement.
210
N. Zilberman et al.
Fig. 7. CDF of TSC tail latency.
Fig. 8. CCDF of aggregated TSC tail
latency.
While up to 99th percentile for the typical TSC measurements, the latency
is in the order of 10 ns, in both kernel and user space, TSC latencies can be in
the order of microseconds or hundreds of microseconds. VMs show even greater
sensitivity with higher-still outlier values. The CDF of these results is shown
in Fig. 7. While long latency events may be infrequent, even a single outlier
event can overshadow hundreds to thousands of other operations. This is keenly
illustrated in Fig. 8 with a complementary CDF (CCDF) the aggregated time
wasted on tail events. This graph illustrates that while only 364 out of 22G
events of TSC latency in VM user space are 1 ms or longer, these events take
almost 5% of the observation period.
The OS kernel is a natural source of latency. While in Kernel Cold Start
tests (1a) we did not ﬁnd any outliers that approach a microsecond, microsecond-
long gaps do occur in a TSC Kernel test (1b) run at the end of our initialization
sequence. In user space (1c), gaps can reach tens of microseconds, even under our
best operating conditions. Some of these events are the clear results of scheduling
events, as disabling pre-emption is not allowed in user space. Experimenting with
diﬀerent (Linux) OS schedulers (e.g., NOOP, CFQ and Deadline) show that
such events may shift in time, but remain at the same magnitude and frequency.
Further, changing some scheduler parameters, e.g. CFQ’s “low latency” and “Idle
slice”, does not reduce the frequency of microsecond-long gaps.
The most prominent cause of long time-gaps is not running an application in
real time or pinned to a core. While the frequency of gaps greater than 1 µs does
not change signiﬁcantly, the latency does increase. When pinned in isolation on
a CPU, 99.9th percentile of the 1 µs-or-more gaps are less than 10 µs. Without
pinning and running in real time, over 10% of the gaps are 10 µs or longer,
and several hundreds-of-microsecond long gaps occur every second. A pinned
application sharing a core with other processes exhibits latency in-between the
aforementioned results - which makes clear VMs are more prone to long latencies,
especially when the VM is running on a single core.
A diﬀerent source of latency is coding practice: Listings 1.1 and 1.2 show
two ways to conduct the TSC user-space test. While Listing 1.1 measures the
exact gap between two consecutive reads, it potentially misses longer events
occurring between loops. Listing 1.2 overcomes this problem, but also captures
Where Has My Time Gone?
211
1
2
3
4
5
6
7
1
2
3
4
5
6
7
8
while (! done ) {
// Read TSC twice , one i m m e d a t e l y after the other
d o _ r d t s c p ( tsc , cpu ) ;
d o _ r d t s c p ( tsc2 , cpu2 ) ;
// If the gap b e t w e e n the two reads is above a threshold , save
it
if (( tsc2 - tsc > t h r e s h o l d ) && ( cpu == cpu2 ) )
buffer [ samples ++] = tsc2 - tsc ; }
Listing 1.1. Reading and Comparing TSC - Code 1.
while (! done ) {
// Read TSC once
d o _ r d t s c p ( tsc , cpu ) ;
// If the gap between the current and the previous reads is
above a threshold , save it
if (( tsc - last > t h r e s h o l d ) && ( cpu == l a s t c p u ) )
buffer [ samples ++] = tsc - last ;
last = tsc ;
l a s t c p u = cpu ; }
Listing 1.2. Reading and Comparing TSC - Code 2.
gaps caused by the code itself. Consequently, Listing 1.2’s minimal gap grows
from 9 ns to 14 ns, while the maximal gap is about twofold longer. In addition,
page faults lead to hundreds of microseconds latencies that can be avoided using
e.g. mlock.
5 Discussion
This paper contributes a decomposition of the latency-inducing components
between an application to the wire. We hope that other researchers can make
use of this work to calibrate their design goals and results, and provide a better
understanding of the key components of overall latency. The results are gener-
alizable also to other platforms and other Linux kernel versions8.
Four observations summarize the lessons learned. First, there is no single
source of latency: using ultra low latency switches or NICs alone are insuﬃcient
even when using sophisticated kernel bypass options. It is only the combination
of each of these eﬀorts which may satisfactorily reduce latency experienced in
a network system. Second, tail events are no longer negligible and result in two
side eﬀects: (1) latency-sensitive transactions may experience delays far worse
than any performance guarantee or design for resilience (e.g. if the event is longer
than retransmission timeout (RTO)) and (2) the “noise” – events well beyond
the 99.9th percentile – potentially consume far more than 0.01% of the time.
This calls for a change of paradigm: instead of qualifying a system by its 99.9th
percentile, it may be that a new evaluation is called for; for example a system
might need to meet a certain signal-to-noise ratio (SNR) (i.e. events below 99.9th
percentile divided by events above it), as in other aspects of engineered systems.
8 Based on evaluation on Xeon E5-2637 v3, i7-6700K and i7-4770 based platforms,
and Linux kernels ranging from 3.18.42 to 4.4.0-42.
212
N. Zilberman et al.
Finally, in large scale distributed systems (e.g., hyper data center) the impact
of the speed of light increases. When a data center uses hundreds of meters long
ﬁbers [14] and the RTT on every 100 m is 1 µs, the aggregated latency is of
the order 10 µs to 20 µs. Consequently, the topology used in the network and
the locality of the data become important, leading to approaches that increase
networking locality, e.g. rack-scale computing. While hundred-meter long ﬁbers
can not be completely avoided within hyper-data center, such traversals should
be minimized.
5.1 The Good, the Bad and the Ugly
The obtained results can be categorized into three groups: the “Good”, the
“Bad”, and the “Ugly”.
The Good are the latency contributors whose 99.9th percentile is below 1 µs.
This group includes the simple operations in kernel and user space, PCIe and a
single switch latency.
The Bad are the latency contributors whose 99.9th percentile is above 1 µs,
but less than 100 µs. This includes the latency of sending packets over user
space+OS, entire host latency, client-server latency, RTT over 100 m ﬁbers and
multi-stage network topology.
The Ugly are the large latency contributors at the far end of the tail, i.e. the
“noise”, contributing more than 100 µs. These happen mostly on the user space
and within a VM. “Ugly” events will increasingly overshadow all other events,
thereby reducing the SNR. Some events outside the scope of this paper, such as
network congestion, also fall within this category [7].
5.2 Limitations
This paper focuses upon the unavoidable latency components within a sys-
tem. It thus does not take into account aspects such as congestion, queueing or
scheduling eﬀects. No attempt is made to consider the impact of protocols, such
as TCP, and their eﬀect on latency and resource contention within the host is
also outside the scope.
This work has focused on commodity hardware and standard networking
practices and on PCIe interconnect and Ethernet-based networking, rather than,
e.g., RDMA and RCoE, reserved for future work.
6 Conclusion
Computer users hate to wait – this paper reports on some of the reasons for
latency in a network-based computer system. Using a decompositional analysis,
the contribution of the diﬀerent components to the overall latency is quantiﬁed,
and we show that there is no single overwhelming contributor to saving the end-
to-end latency challenge in data centers. Further we conclude that more and
Where Has My Time Gone?
213
more latency components, such as the interconnect and cabling, will become
signiﬁcant as the latency of other components continues to improve. We also
conclude that the long tail of events, beyond the 99.9th percentile, is far more
signiﬁcant than its frequency might suggest and we go some way to quantify this
contribution.
“Good”,“Bad”, and “Ugly” classes are applied to a range of
latency-
contributors. While many of the “Bad” latency contributors are the focus of
existing eﬀort, the “Ugly” require new attention, otherwise performance can-
not be reasonably guaranteed. Giving the “Ugly” latencies attention will require
concerted eﬀort to improve the state of instrumentation, ultimately permitting
end-to-end understanding.
Acknowledgments. We would like to thank the many people who contributed to this
paper. We would like to thank Salvator Galea and Robert N Watson, who contributed
to early work on this paper. This work has received funding from the EPSRC grant
EP/K034723/1, Leverhulme Trust Early Career Fellowship ECF-2016-289, European
Union’s Horizon 2020 research and innovation programme 2014-2018 under the SSI-
CLOPS (grant agreement No. 644866), ENDEAVOUR (grant agreement No. 644960)
and EU FP7 Marie Curie ITN METRICS (grant agreement No. 607728).
Dataset. A reproduction environment of the experiments, and the experimental
results, are both available at http://www.cl.cam.ac.uk/research/srg/netos/projects/
latency/pam2017/ and https://doi.org/10.17863/CAM.7418.
References
1. Barroso, L.A.: Landheld Computing. In: IEEE International Solid State Circuits
Conference (ISSCC) (2014). Keynote
2. Cheshire, S.:
the latency,
Latency.html. Accessed July 2016
It’s
stupid. http://www.stuartcheshire.org/rants/
3. Guo, C., et al.: RDMA over commodity ethernet at scale. In: SIGCOMM 2016
(2016)
4. Hemminger, S.: NetEm - Network Emulator. http://man7.org/linux/man-pages/
man8/tc-netem.8.html. Accessed July 2016
5. Kalia, A., et al.: Design guidelines for high performance RDMA systems. In:
USENIX ATC, vol. 16, pp. 437–450 (2016)
6. Mayer, M.: What Google knows. In: Web 2.0 Summit (2006)
7. Mittal, R., et al.: TIMELY: RTT-based congestion control for the datacenter. SIG-
COMM Comput. Commun. Rev. 45, 537–550 (2015). ACM
8. Nussbaum, L., Richard, O.: A comparative study of network link emulators. In:
SpringSim 2009, pp. 85:1–85:8 (2009)
9. Oracle: Oracle VM VirtualBox. https://www.virtualbox.org/. Accessed Oct 2016
10. Paoloni, G.: How to benchmark code execution times on Intel IA-32 and IA-64
instruction set architectures. Technical report 324264–001, Intel (2010)
11. Patterson, D.A.: Latency lags bandwidth. Commun. ACM 47(10), 71–75 (2004)
12. Rumble, S.M., et al.: It’s time for low latency. In: HotOS 2013, p. 11. USENIX
Association (2011)
13. SAP: Big data and smart trading (2012)
214
N. Zilberman et al.
14. Singh, A., et al.: Jupiter rising: a decade of clos topologies and centralized control in
Google’s datacenter network. SIGCOMM Comput. Commun. Rev. 45(4), 183–197
(2015)
15. Tolly Enterprises: Mellanox spectrum vs. broadcom StrataXGS Tomahawk 25GbE
& 100GbE performance evaluation - evaluating consistency & predictability. Tech-
nical report 216112 (2016)
16. Zilberman, N., et al.: NetFPGA SUME: toward 100 Gbps as research commodity.
IEEE Micro 34(5), 32–41 (2014)