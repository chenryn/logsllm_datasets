# Packet and Query Counts
- 1,000,000 packets
- 1,000,000 queries
- 100,000 packets
- 1,000 packets

# Switch Latency (in nanoseconds)
- 8c Switch - 7124FX (64B): 512 ns
- 8d Switch - 7124FX (1514B): 534 ns, 550 ns, 557 ns

# Return Results
- The results are within the DAG measurement error range.

## Where Has My Time Gone?
### Figure Descriptions
- **Figure 4:** End host latency contribution.
- **Figure 5:** Network latency contribution.
- **Figure 6:** Different network topologies.
- **Figure 7:** CDF of TSC tail latency.
- **Figure 8:** CCDF of aggregated TSC tail latency.

## Fiber Latency in Data Centers
Fiber latency, often overlooked, can be in the order of microseconds in large data centers. This becomes a significant component of overall latency. Unlike other components, propagation delay cannot be improved, suggesting that minimizing the traversal path length through data centers should be a future research direction.

## Tail Latency Results
The previous section's results ranged between the stated minimum and the 99.9th percentile. Our experiments also provide insights into heavy-tail properties of measured latency, which are not caused by network congestion or other typical causes of tail latency.

### TSC Experiments
- **TSC (1):** Up to the 99th percentile, typical TSC measurements show latencies in the order of 10 ns. However, TSC latencies in both kernel and user space can reach microseconds or hundreds of microseconds.
- **VMs:** Show even greater sensitivity with higher outlier values. The CDF of these results is shown in Figure 7.
- **CCDF:** Figure 8 illustrates the aggregated time wasted on tail events, showing that while only 364 out of 22 billion TSC latency events in VM user space are 1 ms or longer, these events take almost 5% of the observation period.

### OS Kernel and User Space Latency
- **Kernel Cold Start (1a):** No outliers approaching a microsecond were found.
- **TSC Kernel Test (1b):** Microsecond-long gaps occur at the end of the initialization sequence.
- **User Space (1c):** Gaps can reach tens of microseconds under optimal conditions. These events are often due to scheduling, as pre-emption is not allowed in user space. Different Linux OS schedulers (e.g., NOOP, CFQ, Deadline) do not reduce the frequency of microsecond-long gaps.

### Application Pinning and Real-Time Execution
- **Pinned Application:** When pinned in isolation on a CPU, the 99.9th percentile of 1 µs-or-more gaps is less than 10 µs.
- **Non-Pinned Application:** Over 10% of the gaps are 10 µs or longer, with several hundred-microsecond-long gaps occurring every second.
- **Shared Core:** A pinned application sharing a core with other processes exhibits intermediate latency, indicating that VMs are more prone to long latencies, especially when running on a single core.

### Coding Practices
- **Listing 1.1:** Measures the exact gap between two consecutive reads but may miss longer events.
- **Listing 1.2:** Captures gaps caused by the code itself, increasing the minimal gap from 9 ns to 14 ns and the maximal gap to about twice as long. Page faults lead to hundreds of microseconds of latency, which can be avoided using `mlock`.

## Discussion
This paper decomposes the latency-inducing components between an application and the wire, providing a better understanding of key latency contributors. The results are generalizable to other platforms and Linux kernel versions.

### Key Observations
1. **Multiple Latency Sources:** Using ultra-low-latency switches or NICs alone is insufficient; a combination of efforts is needed to reduce latency.
2. **Tail Events:** These are no longer negligible and result in delays far worse than performance guarantees. The "noise" (events beyond the 99.9th percentile) can consume more than 0.01% of the time, suggesting a need for a new evaluation metric, such as a signal-to-noise ratio (SNR).

### Large-Scale Distributed Systems
In hyper-data centers, the impact of the speed of light increases. With RTTs of 1 µs per 100 m, aggregated latency can be 10-20 µs. Topology and data locality become important, leading to approaches like rack-scale computing to minimize fiber traversals.

### Categorization of Latency Contributors
- **Good:** 99.9th percentile below 1 µs (simple operations in kernel and user space, PCIe, single switch latency).
- **Bad:** 99.9th percentile above 1 µs but less than 100 µs (latency of sending packets over user space+OS, entire host latency, client-server latency, RTT over 100 m fibers, multi-stage network topology).
- **Ugly:** Large latency contributors at the far end of the tail, contributing more than 100 µs (user space, within a VM).

### Limitations
- The paper focuses on unavoidable latency components and does not consider congestion, queueing, or scheduling effects.
- It does not address the impact of protocols like TCP or resource contention within the host.
- Future work will include RDMA and RCoE.

## Conclusion
This paper quantifies the contributions of different components to overall latency in a network-based computer system. It highlights the significance of multiple latency sources and the long tail of events. Addressing "Ugly" latencies requires concerted effort to improve instrumentation and enable end-to-end understanding.

### Acknowledgments
We thank Salvator Galea and Robert N Watson for their contributions. This work was funded by the EPSRC, Leverhulme Trust, European Union’s Horizon 2020, and EU FP7 Marie Curie ITN METRICS.

### Dataset
Reproduction environment and experimental results are available at:
- [http://www.cl.cam.ac.uk/research/srg/netos/projects/latency/pam2017/](http://www.cl.cam.ac.uk/research/srg/netos/projects/latency/pam2017/)
- [https://doi.org/10.17863/CAM.7418](https://doi.org/10.17863/CAM.7418)

### References
1. Barroso, L.A.: Landheld Computing. In: IEEE International Solid State Circuits Conference (ISSCC) (2014). Keynote
2. Cheshire, S.: It’s the latency, stupid. [http://www.stuartcheshire.org/rants/Latency.html](http://www.stuartcheshire.org/rants/Latency.html). Accessed July 2016
3. Guo, C., et al.: RDMA over commodity ethernet at scale. In: SIGCOMM 2016 (2016)
4. Hemminger, S.: NetEm - Network Emulator. [http://man7.org/linux/man-pages/man8/tc-netem.8.html](http://man7.org/linux/man-pages/man8/tc-netem.8.html). Accessed July 2016
5. Kalia, A., et al.: Design guidelines for high performance RDMA systems. In: USENIX ATC, vol. 16, pp. 437–450 (2016)
6. Mayer, M.: What Google knows. In: Web 2.0 Summit (2006)
7. Mittal, R., et al.: TIMELY: RTT-based congestion control for the datacenter. SIGCOMM Comput. Commun. Rev. 45, 537–550 (2015). ACM
8. Nussbaum, L., Richard, O.: A comparative study of network link emulators. In: SpringSim 2009, pp. 85:1–85:8 (2009)
9. Oracle: Oracle VM VirtualBox. [https://www.virtualbox.org/](https://www.virtualbox.org/). Accessed Oct 2016
10. Paoloni, G.: How to benchmark code execution times on Intel IA-32 and IA-64 instruction set architectures. Technical report 324264–001, Intel (2010)
11. Patterson, D.A.: Latency lags bandwidth. Commun. ACM 47(10), 71–75 (2004)
12. Rumble, S.M., et al.: It’s time for low latency. In: HotOS 2013, p. 11. USENIX Association (2011)
13. SAP: Big data and smart trading (2012)
14. Singh, A., et al.: Jupiter rising: a decade of clos topologies and centralized control in Google’s datacenter network. SIGCOMM Comput. Commun. Rev. 45(4), 183–197 (2015)
15. Tolly Enterprises: Mellanox spectrum vs. broadcom StrataXGS Tomahawk 25GbE & 100GbE performance evaluation - evaluating consistency & predictability. Technical report 216112 (2016)
16. Zilberman, N., et al.: NetFPGA SUME: toward 100 Gbps as research commodity. IEEE Micro 34(5), 32–41 (2014)