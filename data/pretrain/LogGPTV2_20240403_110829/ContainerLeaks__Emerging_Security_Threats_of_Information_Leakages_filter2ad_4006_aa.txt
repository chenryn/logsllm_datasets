title:ContainerLeaks: Emerging Security Threats of Information Leakages
in Container Clouds
author:Xing Gao and
Zhongshu Gu and
Mehmet Kayaalp and
Dimitrios Pendarakis and
Haining Wang
2017 47th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
ContainerLeaks: Emerging Security Threats of
Information Leakages in Container Clouds
Xing Gao1,2, Zhongshu Gu3, Mehmet Kayaalp3, Dimitrios Pendarakis3, Haining Wang1
1University of Delaware, 2College of William and Mary, 3IBM T.J. Watson Research Center
{xgao, hnw}@udel.edu, {zgu, mkayaal, dimitris}@us.ibm.com
Abstract—Container technology provides a lightweight oper-
ating system level virtual hosting environment. Its emergence
profoundly changes the development and deployment paradigms
of multi-tier distributed applications. However, due to the incom-
plete implementation of system resource isolation mechanisms in
the Linux kernel, some security concerns still exist for multiple
containers sharing an operating system kernel on a multi-tenancy
container cloud service. In this paper, we ﬁrst present
the
information leakage channels we discovered that are accessible
within the containers. Such channels expose a spectrum of
system-wide host information to the containers without proper
resource partitioning. By exploiting such leaked host information,
it becomes much easier for malicious adversaries (acting as
tenants in the container clouds) to launch advanced attacks that
might impact the reliability of cloud services. Additionally, we
discuss the root causes of the containers’ information leakages
and propose a two-stage defense approach. As demonstrated
in the evaluation, our solution is effective and incurs trivial
performance overhead.
I.
INTRODUCTION
Cloud computing has been widely adopted to consolidate
computing resources. Multi-tenancy is the enabling feature
of cloud computing that allows computation instances from
different tenants running on a same physical server. Among
different types of cloud services, the multi-tenancy container
cloud has recently emerged as a lightweight alternative to
conventional virtual machine (VM) based cloud infrastructures.
Container is an operating system (OS) level virtualization
technology with multiple building blocks in the Linux kernel,
including resource isolation/control techniques (e.g., namespace
and cgroup) and security mechanisms (e.g., Capabilities,
SELinux, AppArmor, and seccomp). By avoiding the overhead
of additional abstraction layers, containers are able to achieve
near-native performance and outperform VM-based systems
in almost all aspects [2], [14], [30]. In addition, the advent
of container management and orchestration systems, such as
Docker and Kubernetes, have profoundly changed the ecosystem
of building, shipping, and deploying multi-tier distributed
applications in the cloud.
Despite the success of container services, there always exist
security and privacy concerns for running multiple containers,
presumably belonging to different tenants, on the same OS
kernel. To support multi-tenancy on container clouds, we
have observed on-going efforts in the Linux kernel to enforce
cross-container isolation and de-privilege user-level containers.
Existing container-enabling kernel features have greatly shrunk
the attack surface exposed to container tenants and could
restrain most existing malicious attacks. However, not all sub-
systems of the Linux kernel can distinguish execution contexts
between a container and a host, and thus they might expose
system-wide information to containerized applications. Some
subsystems are considered to be of low priority for container
adaptations. The rest are facing implementation difﬁculties
for transforming their code base, and their maintainers are
reluctant to accept drastic changes. In order to close these
loopholes, current container runtime software and container
cloud providers typically leverage access control policies to
mask the user-kernel interfaces of these container-oblivious
subsystems. However, such manual and temporary ﬁxes could
only cover a small fraction of the exposed attack surfaces.
In this paper, we systematically explore and identify the
in-container leakage channels that may accidentally expose
information of host OSes and co-resident containers. Such
information leakages include host-system state information (e.g.,
power consumption, performance data, global kernel data, and
asynchronous kernel events) and individual process execution
information (e.g., process scheduling, cgroups, and process
running status). The distinguishing characteristic information
exposed at speciﬁc timings could help uniquely identify
a physical machine. Furthermore, a malicious tenant may
optimize attack strategies and maximize attack effects by
acquiring the system-wide knowledge in advance. We discover
these leakage channels in our local testbed on Docker and
LinuX Container (LXC) and verify their (partial) existence on
ﬁve public commercial multi-tenancy container cloud services.
In order to reveal
the security risks of these leakage
channels, we design an advanced attack, denoted as synergistic
power attack, to exploit the seemingly innocuous information
leaked through these channels. We demonstrate that such
information exposure could greatly amplify the attack effects,
reduce the attack costs, and simplify the attack orchestration.
Power attacks have proved to be real threats to existing data
centers [26], [43]. With no information of the running status
of underlying cloud infrastructures, existing power attacks can
only launch power-intensive workloads blindly to generate
power spikes, with the hope that high spikes could trip branch
circuit breakers to cause power outages. Such attacks could
be costly and ineffective. However, by learning the system-
wide status information, attackers can (1) pick the best timing
to launch an attack, i.e., superimpose the power-intensive
workload on the existing power spikes triggered by benign
workloads, and (2) synchronize multiple power attacks on the
same physical machine/rack by detecting proximity-residence of
controlled containers. We conduct proof-of-concept experiments
on one real-world container cloud service and quantitatively
demonstrate that our attack is able to yield higher power spikes
at a lower cost.
2158-3927/17 $31.00 © 2017 IEEE
DOI 10.1109/DSN.2017.49
237
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:19:41 UTC from IEEE Xplore.  Restrictions apply. 
We further analyze in depth the root causes of these leakage
channels and ﬁnd that such exposures are due to the incomplete
coverage of container implementation in the Linux kernel.
We propose a two-stage defense mechanism to address this
problem in container clouds. In particular, to defend against
the synergistic power attacks, we design and implement a
power-based namespace in the Linux kernel to partition power
consumption at a ﬁner-grained (container) level. We evaluate
our power-based namespace from the perspectives of accuracy,
security, and performance overhead. Our experimental results
show that our system can neutralize container-based power
attacks with trivial performance overhead.
Overall, the major contributions of this work are summa-
rized as follows:
• We systematically explore and identify information
leakages in container cloud environments. We further
analyze these information leakages in depth and trace
out their root causes.
• We demonstrate that adversaries can exploit these
identiﬁed information leakages to launch a new type of
advanced power attack, denoted as synergistic power
attack. Attackers can optimize their attack strategies
and maximize their attack effects. We prove that such
seemingly harmless information leakages may also
pose serious security threats to cloud infrastructures.
• We design and implement a power-based namespace
in the Linux kernel to enhance resource isolation
for containers. Our results show that the proposed
system can effectively defend against container-based
synergistic power attacks with trivial overhead.
The rest of this paper is organized as follows. Section II in-
troduces the background of container technology and describes
power attack threats on data centers. Section III presents the
in-container leakage channels discovered by us and their leaked
information. Section IV details the synergistic power attack
that leverages the leaked information through these channels.
Section V presents a general two-stage defense mechanism
and the speciﬁc design and implementation of our power-based
namespace in the Linux kernel. Section VI shows the evaluation
of our defense framework from different aspects. Section VII
discusses the limitations and future work. Section VIII surveys
related work, and we conclude in Section IX.
II. BACKGROUND
In this section, we brieﬂy describe the background knowl-
edge of three topics: internals of Linux containers, multi-tenancy
container cloud services, and existing power attacks in data
centers.
A. Linux Kernel Support for Container Technology
Containers depend on multiple independent Linux kernel
components to enforce isolation among user-space instances.
Compared to VM-based virtualization approaches, multiple
containers share the same OS kernel, thus eliminating additional
performance overheads for starting and maintaining VMs.
Containers have received much attention from the industry
and have grown rapidly in recent years for boosting application
238
performance, enhancing developer efﬁciency, and facilitating
service deployment. Here we introduce two key techniques,
namespace and cgroup, that enable containerization on Linux.
1) Namespace: The ﬁrst namespace was introduced in the
Linux kernel 2.4.19. The key idea of namespace is to isolate
and virtualize system resources for a group of processes, which
form a container. Each process can be associated with multiple
namespaces of different types. The kernel presents a customized
(based on namespace types) view of system resources to each
process. The modiﬁcations to any namespaced system resources
are conﬁned within the associated namespaces, thus incurring
no system-wide changes.
The current kernel has seven types of namespaces: mount
(MNT) namespace, UNIX timesharing system (UTS) namespace,
PID namespace, network (NET) namespace,
inter-process
communications (IPC) namespace, USER namespace, and
CGROUP namespace. The MNT namespace isolates a set
of ﬁle system mount points. In different MNT namespaces,
processes have different views of the ﬁle system hierarchy.
The UTS namespace allows each container to have its own
host name and domain name, and thus a container could be
treated as an independent node. The PID namespace virtualizes
the process identiﬁers (pids). Each process has two pids:
one pid within its PID namespace and one (globally unique)
pid on the host. Processes in one container could only view
processes within the same PID namespace. A NET namespace
contains separate virtual network devices, IP addresses, ports,
and IP routing tables. The IPC namespace isolates inter-
process communication resources, including signals, pipes, and
shared memory. The USER namespace was recently introduced
to isolate the user and group ID number spaces. It creates
a mapping between a root user inside a container to an
unprivileged user on the host. Thus, a process may have full
privileges inside a user namespace, but it is de-privileged on the
host. The CGROUP namespace virtualizes the cgroup resources,
and each process can only have a containerized cgroup view
via cgroupfs mount and the /proc/self/cgroup ﬁle.
2) Cgroup: In the Linux kernel, cgroup (i.e., control group)
provides a mechanism for partitioning groups of processes
(and all their children) into hierarchical groups with controlled
behaviors. Containers leverage the cgroup functionality to apply
per-cgroup resource limits to each container instance, thus
preventing a single container from draining host resources.
Such controlled resources include CPU, memory, block IO,
network, etc. For the billing model in cloud computing, cgroup
can also be used for assigning corresponding resources to
each container and accounting for their usage. Each cgroup
subsystem provides a uniﬁed sysfs interface to simplify the
cgroup operations from the user space.
B. Container Cloud
With these kernel features available for resource isolation
and management, the Linux kernel can provide the lightweight
virtualization functionality at the OS level. More namespace
and cgroup subsystems are expected to be merged into the
upstream Linux kernel in the future to enhance the container
security. Containerization has become a popular choice for
virtual hosting in recent years with the maturity of container
runtime software. LXC is the ﬁrst complete implementation
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:19:41 UTC from IEEE Xplore.  Restrictions apply. 
of the Linux container manager built in 2008. Docker, which
was built upon LXC (now with libcontainer), has become
the most popular container management tool in recent years.
Docker can wrap applications and their dependencies (e.g., code,
runtime, system tools, and system libraries) into an image, thus
guaranteeing that application behaviors are consistent across
different platforms.
A large number of cloud service providers, including Ama-
zon ECS, IBM Bluemix, Microsoft Azure, and Google Compute
Engine, have already provided container cloud services. For
multi-tenancy container cloud services, containers can either
run on a bare metal physical machine or a virtual machine.
In both situations, containers from different tenants share the
same Linux kernel with the host OS.
C. Power Attacks on Data Centers
Power attacks have been demonstrated to be realistic threats
to existing cloud infrastructures [26], [43]. Considering the
cost of upgrading power facilities, current data centers widely
adopt power oversubscription to host the maximum number
of servers within the existing power supply capabilities. The
safety guarantees are based on the assumption that multiple
adjacent servers have a low chance of reaching peaks of power
consumption simultaneously. While power oversubscription
allows deploying more servers without
increasing power
capacity, the reduction of power redundancy increases the
possibility of power outages, which might lead to forced
shutdowns for servers on the same rack or on the same power
distribution unit (PDU). Even normal workloads may generate
power spikes that cause power outages. Facebook recently
reported that it prevented 18 potential power outages within
six months in 2016 [37]. The situation would have been worse
if malicious adversaries intentionally drop power viruses to
launch power attacks [15], [16]. The consequence of a power
outage could be devastating, e.g., Delta Airlines encountered a
shutdown of a power source in its data center in August 2016,
which caused large-scale delays and cancellations of ﬂights
[8]. Recent research efforts [26], [43] have demonstrated that
it is feasible to mount power attacks on both traditional and
battery-backed data centers.
Launching a successful power attack requires three key
factors: (1) gaining access to servers in the target data center by
legitimately subscribing services, (2) steadily running moderate
workloads to increase the power consumption of servers to
their capping limits, (3) abruptly switching to power-intensive
workloads to trigger power spikes. By causing a power spike
in a short time window, a circuit breaker could be tripped to
protect servers from physical damages caused by overcurrent
or overload.
The tripping condition of a circuit breaker depends on the
strength and duration of a power spike. In order to maximize
the attack effects, adversaries need to run malicious workloads
on a group of servers belonging to the same rack or PDU.
In addition, the timing of launching attacks is also critical.
If a speciﬁc set of servers (e.g., on the same rack) in a data
center have already run at their peak power state, the chance
of launching a successful power attack will be higher [43].
The techniques of power capping [25] have been designed
to defend against power attacks. At the rack and PDU level, by
monitoring the power consumption, a data center can restrict the
power consumption of servers through a power-based feedback
loop. At the host level, Running Average Power Limit (RAPL) is
a technique for monitoring and limiting the power consumption
for a single server. RAPL has been introduced by Intel since
Sandy Bridge microarchitecture. It provides ﬁne-grained CPU-
level energy accounting at the microsecond level and can be
used to limit the power consumption for one package.
Power capping mechanisms signiﬁcantly narrow down the
power attack surface, but it cannot address the problem of
power oversubscription, which is the root cause of power
outages in data centers. Although host-level power capping for
a single server could respond immediately to power surges,
the power capping mechanisms at the rack or PDU level still
suffer from minute-level delays. Assuming attackers could
deploy power viruses into physically adjacent servers, even
if each server consumes power lower than its power capping
limit, the aggregated power consumption of controlled servers
altogether can still exceed the power supply capacity and trip
the circuit breaker. We demonstrate in the following sections
that malicious container tenants can launch synergistic power
attacks by controlling the deployment of their power-intensive
workloads and leveraging benign workloads in the background
to amplify their power attacks.
III.
INFORMATION LEAKAGES IN CONTAINER CLOUDS
As we mentioned in Section II, the Linux kernel provides a
multitude of supports to enforce resource isolation and control
for the container abstraction. Such kernel mechanisms are
the enabling techniques for running containers on the multi-
tenancy cloud. Due to priority and difﬁculty levels, some
components of the Linux kernel have not yet transformed to
support containerization. We intend to systematically explore
which parts of the kernel are left uncovered, what the root
causes are, and how potential adversaries can exploit them.
A. Container Information Leakages
We ﬁrst conduct experiments on our local Linux machines
with Docker and LXC containers installed. Linux provides two
types of controlled interfaces from userspace processes to the
kernel, system calls, and memory-based pseudo ﬁle systems.
System calls are mainly designed for user processes to request
kernel services. The system calls have strict deﬁnitions for
their public interfaces and are typically backward compatible.
However, memory-based pseudo ﬁle systems are more ﬂexible
for extending kernel functionalities (e.g., ioctl), accessing kernel
data (e.g., procfs), and adjusting kernel parameters (e.g., sysctl).
In addition, such pseudo ﬁle systems enable manipulating kernel
data via normal ﬁle I/O operations. Linux has a number of
memory-based pseudo ﬁle systems (e.g., procfs, sysfs, devfs,
securityfs, debugfs, etc.) that serve the different purposes of
kernel operations. We are more interested in procfs and sysfs,
which are by default mounted by container runtime software.
As illustrated in the left part of Figure 1, we design a cross-
validation tool to automatically discover these memory-based
pseudo ﬁles that expose host information to containers. The
key idea is to recursively explore all pseudo ﬁles under procfs
and sysfs in two execution contexts, one running within an
unprivileged container and the other running on the host. We
239
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:19:41 UTC from IEEE Xplore.  Restrictions apply. 
Containerized Process
Pseudo File System
Host Process
Pseudo File System
Local  Container 
Testbed
Multi-Tenancy
Container Cloud Testbed
OS Kernel
(cid:1)
(cid:2)
(cid:2)
(cid:1)
Differential 
Analysis