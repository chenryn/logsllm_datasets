### Graph Partitioning and Evaluation

In this study, we evaluate the partitioning performance of GEOFEM using RPCS (Recursive Partitioning with Communication Scheduling) and compare it with the partitioning given by other algorithms. Additionally, we use CGPOP, a conjugate gradient (CG) solver designed as a miniature performance-tuning application for the Parallel Ocean Program (POP). GEOFEM has a 3D grid graph topology, while CGPOP has a 2D grid graph topology.

To form the communication pattern graph, we utilize a parallel implementation on the FX10 supercomputer, which is based on the K computer architecture [10]. The specifications of a node are detailed in Table I. The graph partitioning is performed serially by one calculation node of the FX10. As previously mentioned, the communication between the application and the graph partitioner is facilitated through the file system.

### A. Graph Partitioning Performance

We compared the time required for the clustering process using two METIS algorithms (multilevel recursive bisection (pmetis) and multilevel K-way partitioning (kmetis)) and a naive implementation of Recursive Coordinate Bisection (RCB). The computational complexities of RPCS, pmetis, kmetis, and RCB are \(O(k\sqrt{n})\), \(O(kE)\), \(O(E)\), and \(O(kn\log n)\), respectively, where \(k\) is the number of partitions, \(n\) is the number of vertices, and \(E\) is the number of edges. For this evaluation, we used a large 2D grid graph with vertices connected to four adjacent vertices by edges of the same weight, allowing for easy scaling of the number of vertices. The number of groups was fixed at 100 due to the linear scaling of the computational complexity with the number of groups.

Figures 5 and 6 illustrate the results of this simulation. In a grid graph, the number of edges is a constant multiple of the number of vertices, so \(E\) is equivalent to \(n\) when considering computational complexity. RPCS outperforms the other algorithms, demonstrating both speed and scalability. The time required by RPCS increases proportionally to \(\sqrt{n}\), whereas the time complexity of the other algorithms scales linearly with \(n\). These results indicate that RPCS is faster and more scalable than other widely used algorithms when applied to grid graphs.

### B. Edge-cut of Partitioning

Our method reduces accuracy to improve grouping time, necessitating an evaluation of the quality of grouping. One criterion for evaluating the grouping of processes is the edge-cut, defined as the amount of communication between processes in different groups. We executed CGPOP with 22,500 (150 x 150) processes on 1,440 (36 x 40) physical nodes and GEOFEM with 21,942 (28 x 28 x 28) processes on 1,440 (12 x 12 x 10) physical nodes. The number of groups was fixed at 16. Figures 7 and 8 show the optimized node allocation results, with each bar representing the edge-cuts produced by each method. Our partitioning method outperforms both METIS algorithms. While our approach may degrade in quality for complex graphs, it generally performs better than RCB due to node allocation optimization during application execution. This optimization simplifies the graph, as most users allocate application processes to match the network topology between physical nodes.

### C. Cost of Fault Tolerance

The number of restarted processes following a fault is a critical measure of grid partitioning quality. We estimated the fault tolerance cost using the evaluation formula proposed in [3], assuming the same conditions as in [2] with a message logging impact on performance \(\alpha = 23\%\) and a checkpoint/restart impact on MTBF \(\beta = 12.4\%\). The formula is:

\[ \text{process fault cost} = 23\% \times \frac{B}{D} \]

where \(|P|\) is the total number of processes, \(|P_k|\) is the number of processes in group \(k\), and \(B\) and \(D\) represent the size of logged messages and the total size of messages, respectively. The extended formula for node faults is:

\[ \text{node fault cost} = 23\% \times \frac{B}{D} \times \frac{\sum_{k \in A_n} |P_k|}{|P| \times N} \]

where \(N\) is the number of physical nodes and \(A_n\) is the set of group IDs belonging to processes running on node \(n\). We estimated these costs for GEOFEM and CGPOP, using the same configuration as in the previous section. The results, shown in Figures 9 and 10, indicate that our method achieves a process fault cost comparable to other approaches, suggesting good grouping balance and minimal edge-cuts. The node fault cost follows a similar trend to other node-based methods. CGPOP's score is worse than GEOFEM's due to the presence of non-communicating processes in the land domain. Overall, our method does not suffer from significant degradation in quality compared to other methods, demonstrating its effectiveness in terms of edge-cuts and grouping balance.

### Conclusion

We designed and implemented a system supporting partial message logging, featuring runtime analysis of communication, physical node-based partitioning, and a scalable algorithm for 2D or 3D grid graphs. Our method, RPCS, reduces the number of processes rolled back during node failures and achieves faster process grouping without significant quality degradation. Future work will integrate a full fault tolerance framework by combining RPCS with a log-based checkpointer and expand the evaluation to applications with more complex communication topologies.

### Acknowledgment

This work was supported by ANR-JST FP3C: Collaborative Project between Japan and France "Framework and Programming for Post Petascale Computing" and JST Crest ppOpen-HPC: "Open Source Infrastructure for Development and Execution of Large-Scale Scientific Applications on Post-Peta-Scale Supercomputers with Automatic Tuning."

---

**References:**
1. Franck Cappello, Fault Tolerance in Petascale/Exascale Systems: Current Knowledge, Challenges and Research Opportunities, INRIA, IJHPCA 23(3): 212-226, 2009.
2. Amina Guermouche, Thomas Ropars, Elisabeth Brunet, Marc Snir, and Franck Cappello, Uncoordinated Checkpointing Without Domino Effect for Send-Deterministic Message Passing Applications, Proceedings of IPDPS 2011.
3. Thomas Ropars, Amina Guermouche, Bora Ucar, Esteban Meneses, Laxmikant V. Kale, and Frank Cappello, On the use of cluster-based partial message logging to improve fault tolerance for MPI HPC applications, In proceedings of the 17th international conference on Parallel processing, Vol. Part I, Euro-par â€™11, pp. 567-578, Berlin, Heidelberg, 2011.
4. Meneses, E., Mendes, C. L., and Kale, L. V., Team-Based Message Logging: Preliminary Results, Cluster, Cloud and Grid Computing (CCGrid), 2010.
5. Karypis, G. and Kumar, V., A Fast and High Quality Multilevel Scheme for Partitioning Irregular Graphs, SIAM J. Sci. Comput., Vol.20, No.1, pp. 359-392, 1998.
6. Jin-Min Yang, Kim Fun Li, Wen-Wei Li, and Da-Fang Zhang, Trading off logging overhead and coordinating overhead to achieve efficient rollback recovery. Concurr. Comput.: Pract. Exper., 21(6):819-853, 2009.
7. F. Pellegrini and J. Roman., Scotch: A software package for static mapping by dual recursive bipartitioning of process and architecture graphs., In High-Performance Computing and Networking, pp. 493-498, Springer, 1996.
8. The MPI Forum, MPI: A Message Passing Interface Standard. Version 3.0, available at http://www.mpi-forum.org.
9. H. Okuda, K. Nakajima, M. Iizuka, L. Chen, and H. Nakamura, Parallel finite element analysis platform for the earth simulator: Geofem. Computational Science (ICCS) 2003, p. 700, 2003.
10. M. Yokokawa, F. Shoji, A. Uni, M. Kurokawa, and T. Watanabe, The K computer: Japanese next-generation supercomputer development project, In International Symposium on Low Power Electronics and Design (ISLPED) 2011, pp. 371-372, IEEE, 2011.
11. Andrew I. Stone, John M. Dennis, and Michelle Mills Strout, The CGPOP Miniapp, Version 1.0, Technical Report CS-11-103 July, 2011.