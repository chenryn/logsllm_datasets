Asleep at the Keyboard? Assessing the
Security of GitHub Copilot’s Code Contributions
Hammond Pearce
Department of ECE
New York University
Brooklyn, NY, USA
PI:EMAIL
Baleegh Ahmad
Department of ECE
New York University
Brooklyn, NY, USA
PI:EMAIL
Benjamin Tan
Department of ESE
University of Calgary
Calgary, Alberta, CA
PI:EMAIL
Brendan Dolan-Gavitt
Department of CSE
New York University
Brooklyn, NY, USA
PI:EMAIL
Ramesh Karri
Department of ECE
New York University
Brooklyn, NY, USA
PI:EMAIL
Abstract—There is burgeoning interest in designing AI-based
systems to assist humans in designing computing systems,
including tools that automatically generate computer code. The
most notable of these comes in the form of the ﬁrst self-described
‘AI pair programmer’, GitHub Copilot, which is a language
model trained over open-source GitHub code. However, code
often contains bugs—and so, given the vast quantity of unvetted
code that Copilot has processed, it is certain that the language
model will have learned from exploitable, buggy code. This raises
concerns on the security of Copilot’s code contributions. In this
work, we systematically investigate the prevalence and conditions
that can cause GitHub Copilot to recommend insecure code.
To perform this analysis we prompt Copilot to generate code
in scenarios relevant to high-risk cybersecurity weaknesses, e.g.
those from MITRE’s “Top 25” Common Weakness Enumeration
(CWE) list. We explore Copilot’s performance on three distinct
code generation axes—examining how it performs given diversity
of weaknesses, diversity of prompts, and diversity of domains. In
total, we produce 89 different scenarios for Copilot to complete,
producing 1,689 programs. Of these, we found approximately
40 % to be vulnerable.
Index Terms—Cybersecurity, Artiﬁcial Intelligence (AI), code
generation, Common Weakness Enumerations (CWEs)
I. INTRODUCTION
With increasing pressure on software developers to produce
code quickly, there is considerable interest in tools and
techniques for improving productivity. The most recent
entrant into this ﬁeld is machine learning (ML)-based
code generation, in which large models originally designed
for natural language processing (NLP) are trained on vast
quantities of code and attempt to provide sensible completions
as programmers write code. In June 2021, GitHub released
Copilot [1], an “AI pair programmer” that generates code in
a variety of languages given some context such as comments,
function names, and surrounding code. Copilot is built on a
large language model that is trained on open-source code [2]
including “public code...with insecure coding patterns”, thus
giving rise to the potential for “synthesize[d] code that
contains these undesirable patterns” [1].
Although prior research has evaluated the functionality of
code generated by language models [3], [2], there is no
B. Dolan-Gavitt is supported in part by the National Science Foundation
award #1801495. R. Karri is supported in part by Ofﬁce of Naval
Research Award # N00014-18-1-2058. R. Karri is supported in part by the
NYU/NYUAD CCS.
systematic examination of the security of ML-generated code.
As GitHub Copilot is the largest and most capable such
model currently available, it is important to understand: Are
Copilot’s suggestions commonly insecure? What is the
prevalence of insecure generated code? What factors of the
“context” yield generated code that is more or less secure?
We systematically experiment with Copilot to gain insights
into these questions by designing scenarios for Copilot to
complete and by analyzing the produced code for security
weaknesses. As our corpus of well-deﬁned weaknesses, we
check Copilot completions for a subset of MITRE’s Common
Weakness Enumerations (CWEs), from their “2021 CWE
Top 25 Most Dangerous Software Weaknesses” [4] list. This
list is updated yearly to indicate the most dangerous software
weaknesses as measured over the previous two calendar years.
The AI’s documentation recommends that one uses “Copilot
together with testing practices and security tools, as well as
your own judgment”. Our work attempts to characterize the
tendency of Copilot to produce insecure code, giving a gauge
for the amount of scrutiny a human developer might need to
do for security issues.
We study Copilot’s behavior along three dimensions: (1)
diversity of weakness, its propensity for generating code that
is susceptible to weaknesses in the CWE “top 25”, given a
scenario where such a vulnerability is possible; (2) diversity
of prompt, its response to the context for a particular scenario
(SQL injection), and (3) diversity of domain, its response to
the domain, i.e., programming language/paradigm.
For diversity of weakness, we construct three different sce-
narios for each applicable “top 25” CWE and use the CodeQL
software scanning suite [5] along with manual inspection to
assess whether the suggestions returned are vulnerable to that
CWE. Our goal here is to get a broad overview of the types
of vulnerability Copilot is most likely to generate, and how
often users might encounter such insecure suggestions. Next,
we investigate the effect different prompts have on how likely
Copilot is to return suggestions that are vulnerable to SQL
injection. This investigation allows us to better understand
what patterns programmers may wish to avoid when using
Copilot, or ways to help guide it to produce more secure code.
Finally, we study the security of code generated by Copilot
when it is used for a domain that was less frequently seen
arXiv:2108.09293v3  [cs.CR]  16 Dec 2021
in its training data. Copilot’s marketing materials claim that
it speaks “all the languages one loves.” To test this claim, we
focus on Copilot’s behavior when tasked with a new domain
added to the MITRE CWEs in 2020—hardware-speciﬁc
CWEs [6]. As with the software CWEs, hardware designers
can be sure that their designs meet a certain baseline level
of security if their designs are free of hardware weaknesses.
We are interested in studying how Copilot performs when
tasked with generating register-transfer level (RTL) code in
the hardware description language Verilog.
Our contributions include the following. We perform
automatic and manual analysis of Copilot’s software and
hardware code completion behavior in response to “prompts”
handcrafted
to
represent
security-relevant
scenarios
and
characterize the impact that patterns in the context can have
on the AI’s code generation and conﬁdence. We discuss
implications for software and hardware designers, especially
security novices, when using AI pair programming tools.
This work is accompanied by the release of our repository of
security-relevant scenarios (see the Appendix).
II. BACKGROUND AND RELATED WORK
A. Code Generation
Software development involves the iterative reﬁnement
of
a
(plain
language)
speciﬁcation
into
a
software
implementation—developers
write
code,
comments,
and
other supporting collateral as they work towards a functional
product. Early work proposed ML-based tools to support
developers through all stages of the software design life-cycle
(e.g., predicting designer effort, extracting speciﬁcations [7]).
With recent advancements in the domain of deep learning (DL)
and NLP, sophisticated models can perform sophisticated
interventions on a code base, such as automated program
repair [8]. In this work, we focus on Copilot as an “AI
pair programmer” that offers a designer code completion
suggestions in “real-time” as they write code in a text editor.
There
are
many
efforts
to
automatically
translate
speciﬁcations
into
computer
code
for
natural
language
programming [9], through formal models for automatic code
generation (e.g., [10], [11]) or via machine-learned NLP
[12]. DL architectures that demonstrate good ﬁts for NLP
include LSTMs [13], RNNs [14], and Transformers [15]
that have paved the way for models such as BERT [16],
GPT-2 [17], and GPT-3 [18]. These models can perform
language tasks such as translation and answering questions
from the CoQA [19] dataset; after ﬁne-tuning on specialized
datasets, the models can undertake tasks such as code
completion [2] and hardware design [20]. State-of-the-art
models have billions of learnable parameters and are trained
on millions of software repositories [2].
Copilot is based on the OpenAI Codex family of models [2].
Codex models begin with a GPT-3 model [18], and then
ﬁne-tune it on code from GitHub. Its tokenization step is
nearly identical to GPT-3: byte pair encoding is still used
to convert the source text into a sequence of tokens, but the
GPT-3 vocabulary was extended by adding dedicated tokens
for whitespace (i.e., a token for two spaces, a token for
three spaces, up to 25 spaces). This allows the tokenizer to
encode source code (which has lots of whitespace) both more
efﬁciently and with more context.
Accompanying the release of Copilot, OpenAI published
a technical report evaluating various aspects of “several early
Codex models, whose descendants power GitHub Copilot” [2].
This work does include a discussion (in Appendix G.3) of
insecure code generated by Codex. However, this investigation
was limited to one type of weakness (insecure crypto
parameters, namely short RSA key sizes and using AES in
ECB mode). The authors note that “a larger study using the
most common insecure code vulnerabilities” is needed, and
we supply such an analysis here.
An important feature that Codex and Copilot inherit from
GPT-3 is that, given a prompt, they generate the most likely
completion for that prompt based on what was seen during
training. In the context of code generation, this means that
the model will not necessarily generate the best code (by
whatever metric you choose—performance, security, etc.) but
rather the one that best matches the code that came before.
As a result, the quality of the generated code can be strongly
inﬂuenced by semantically irrelevant features of the prompt.
We explore the effect of different prompts in Section V-C.
B. Evaluating Code Security
Numerous elements determine the quality of code. Code
generation
literature
emphasizes
functional
correctness,
measured by compilation and checking against unit tests,
or using text similarity metrics to desired responses [2].
Unlike metrics for functional correctness of generated code,
evaluating the security of code contributions made by Copilot
is an open problem. Aside from manual assessment by a
human security expert there are myriad tools and techniques
to perform security analyses of software [21]. Source code
analysis tools such as static application security testing tools
are designed to analyze source code and/or compiled versions
of code to ﬁnd security ﬂaws; typically they specialize on
identifying a speciﬁc vulnerability class.
In this work, we gauge the security of Copilot’s contri-
butions using a mix of automated analysis using GitHub’s
CodeQL tool [5] (as it can scan for a wider range of security
weaknesses in code compared to other tools) alongside our
manual code inspection. CodeQL is open-source and supports
the analysis of software in languages such as Java, JavaScript,
C++, C#, and Python. Through queries written in its QL query
language, CodeQL can ﬁnd issues in codebases based on a set
of known vulnerabilities/rules. Developers can conﬁgure Cod-
eQL to scan for different code issues and make it available for
academic research (also, it seems fair to use one GitHub tool to
test the other). Prior work used CodeQL to identify vulnerable
code commits in the life of a JavaScript project [22].
There are common patterns in various classes of insecure
code. Such patterns can be considered weaknesses, as
taxonomized by the Common Weakness Enumeration (CWE)
database maintained by MITRE [23]. CWEs are categorized
1
p r i n t f ( ”How many items
in
the
l i s t ?\n” ) ;
2
unsigned
i n t
l i s t
l e n ;
3
scanf ( ”%d” , &l i s t
l e n ) ;
4
s t r u c t
s h o p p i n g
l i s t
i t e m
* shopping items
= malloc ( l i s t
l e n
*
s i z e o f ( s t r u c t
s h o p p i n g
l i s t
i t e m ) ) ;
Fig. 1. Vulnerable shopping list C code
into a tree-like structure according to the Research Concepts
View (CWE-1000). Each CWE is classiﬁed as either a
pillar (most abstract), class, base, or variant (most speciﬁc).
For example, consider CWE-20, Improper Input Validation.
This covers scenarios where a program has been designed
to receive input, but without validating (or incorrectly
validating) the data before processing. This is a “class”-type
CWE, and is a child of the “pillar” CWE-707: Improper
Neutralization, meaning that all CWE-20 type weaknesses
are CWE-707 type weaknesses. There are other CWE-707
improper neutralization weaknesses which are not covered by
CWE-20. Weaknesses which apply to CWE-20 can be further
categorized into the base and variant types. We show an
instance of this weakness in Fig. 1, which is a code snippet
that implements the part of a basic shopping list application.
The program asks how many items should be in the list (so
that it can allocate an appropriate amount of memory).
Here, the number input (on line 4) is not properly validated
to ensure that it is “reasonable” before being used (line 5).
This is thus vulnerable according to the “class” CVE-20, and
also the “base” CVE-1284: Improper Validation of Speciﬁed
Quantity in Input. Further, as the improper value is then used
to allocate memory, it may also be speciﬁc to the “variant”
CVE-789: Memory Allocation with Excessive Size Value. As
a result, this code could also be considered vulnerable to the
“class” CVE-400: Uncontrolled Resource Consumption, as
the user can command how much memory will be allocated.
This code has other vulnerabilities as well: as the code scans
with %d—even though the variable is deﬁned as an ‘unsigned
int’—entering a negative value (e.g. −1) will cause an integer
wraparound error (CWE-190).
CWEs capture weaknesses in a spectrum of complexity;
some CWEs manifest as fairly “mechanical” implementation
bugs that can be caught by static analysis tools (such as
CodeQL). Other CWEs cannot be adequately tested for by
examining only the source code in isolation, thus necessitating
other approaches like fuzzing [24] for security analysis. Alter-
natively, assertions for manually-speciﬁed security properties
may be added. Examining if Copilot introduces weaknesses
that require reasoning over such a broader context (i.e.,
outside the single code ﬁle) is beyond the scope of this study.
III. USING GITHUB COPILOT
Copilot is used as follows1. The software developer (user)
works on some program, editing the code in a plain text
editor; at this time, Copilot supports Visual Studio Code.
The exact nature of how Copilot scans code is not disclosed
publicly, being a proprietary closed-source black-box. The
1As of August 2021, during Copilot’s technical preview phase.
Fig. 2. Example Copilot usage for Python Login Code: ﬁrst option popup.
Fig. 3. Copilot displays more detailed options for Python Login Code.
exact processes that it uses for continuously scanning,
prompting, deciding what to upload, etc., are not described in
any ofﬁcial documentation. Thus, the following description is
based on our understanding of the available documentation [1].
As the user adds lines of code to the program, Copilot
continuously scans the program and periodically uploads
some subset2 of lines, the position of the user’s cursor, and
metadata before generating some code options for the user
to insert. Copilot aims to generate code that is functionally
relevant to the program as implied by comments, docstrings,
function names, and so on. Copilot also reports a numerical
conﬁdence score3 for each of its proposed code completions,
with the top-scoring (highest-conﬁdence) score presented as
the default selection for the user. The user can choose any
of Copilot’s options. An example of this process is depicted
in Fig. 2. Here, the user has begun to write the login code
for a web app. Their cursor is located at line 15, and based
on other lines of code in the program, Copilot suggests an
additional line of code which can be inserted.
2This subset is proprietary.
3Copilot refers to this value in the generated outputs as ‘mean prob.’. An
online comment from Johan Rosenkilde, a Copilot maintainer, clariﬁed that
this is an aggregate of the probabilities of all tokens in the answer, and so
can be seen as a conﬁdence score.
Fig. 4. Example CodeQL output for Copilot-generated Python Login Code
(line breaks and highlighting are for readability).
The user may request more insights by opening Copilot’s
main window by pressing the prompted Ctrl + Space
combination. Here the user will be presented with many
options (we requested the top 25 samples, which gave us a
good balance between generation speed and output variability)
and the score for each option, if requested. This is displayed in
Fig. 3, and the user may choose between the different options.
As Copilot is based on GPT-3 and Codex [2], several
options are available for tuning the code generation, including
temperature, stops, and top p. Unfortunately, the settings and
documentation as provided do not allow users to see what
these are set to by default—users may only override the
(secret) default values. As we are interested in the default per-
formance of Copilot, we thus do not override these parameters.
IV. EXPERIMENTAL METHOD
A. Problem Deﬁnition
We focus on evaluating the potential security vulnerabilities
of code generated by Github Copilot. As discussed in Sec-
tion II, determining if code is vulnerable sometimes requires
knowledge (context) external to the code itself. Furthermore,
determining that a speciﬁc vulnerability is exploitable requires
framing within a corresponding attacker model.
As such, we constrain ourselves to the challenge of
determining if speciﬁc code snippets generated by Copilot
are vulnerable: that is, if they deﬁnitively contain code that
exhibits characteristics of a CWE. We do not consider the
exploitability of an identiﬁed weakness in our experimental
setting as we reduce the problem space into a binary
classiﬁcation: Copilot generated code either contains code
identiﬁed as (or known to be) weak or it does not.
B. Evaluating Copilot Options with Static Analysis
In this paper we use the Github CodeQL [5]. To demonstrate
CodeQL’s functionality, assume that the top scoring option
from Copilot in Fig. 3 is chosen to build a program. Using
CodeQL’s
python-security-and-quality.qls
testing
suite, which checks 153 security properties, it outputs feedback
like that shown in Fig. 4—reporting that the SQL query
generation method (lines 14-16 in Fig. 3) is written in a way
that allows for insertion of malicious SQL code by the user.
In the CWE nomenclature this is CWE-89 (SQL Injection).
C. Generalized Evaluation Process
Given that the goal of this work is to perform an early
empirical investigation of the prevalence of CWEs within
Copilot-generated code, we choose to focus on MITRE’s
“2021 CWE Top 25” list [4]. We use this list to guide our
creation of a Copilot prompt dataset, which we call the
‘CWE scenarios’. We feed each prompt through Copilot to
generate code completions (Section III) and determine if the
generated code contains the CWE (Section IV-B). Our overall
experimental method is depicted in Fig. 5.
In step 1 , for each CWE, we write a number of ‘CWE
scenarios’ 2 . These are small, incomplete program snippets
in which Copilot will be asked to generate code. The
scenarios are designed such that a naive functional response
could contain a CWE, similar to that depicted in Fig. 2.
For simplicity, we restrict ourselves to three programming
languages: Python, C, and Verilog. Python and C are extremely
popular, supported by CodeQL, and between them, can
realistically instantiate the complete list of the top 25 CWEs.
We use Verilog to explore Copilot’s behavior in a less popular
domain in Section V-D as an additional set of experiments.
In
developing
the
scenarios,
we
used
three
different
sources. These were (a) the CodeQL example/documentation
repository—considered as the best as these scenarios are
ready for evaluation with CodeQL, (b) examples listed in
the CWE entry in MITRE’s database—second best, as they
deﬁnitively describe each CWE and require minimal work to
ensure conformance with CodeQL, and (c) bespoke scenarios
designed by the authors for this study. Note that each scenario
does not contain the weakness from the outset; it is Copilot’s
completion that determines if the ﬁnal program is vulnerable.
Next, in 3 , Copilot is asked to generate up to 25 options
for each scenario. Each option is then combined with the
original program snippet to make a set programs in 4a —with
some options discarded 4b if they have signiﬁcant syntax
issues (i.e., they are not able to be compiled/parsed). That
said, where simple edits (e.g. adding or removing a single
brace) would result in a compilable output, we make those
changes automatically using a regex-based tool.
Then, in 5a evaluation of each program occurs. Where
possible, this evaluation is performed by CodeQL
5b
Copilot
Options
Copilot
Options
CodeQL
repo.
MITRE
e.g.'s
Authors
Copilot
Options
Copilot
Options
Results
CWE scenarios
Copilot
Options
Evaluation
CodeQL
2
3
5c
5b
6
MITRE Top 25 CWEs
1
4a
Copilot programs
4b