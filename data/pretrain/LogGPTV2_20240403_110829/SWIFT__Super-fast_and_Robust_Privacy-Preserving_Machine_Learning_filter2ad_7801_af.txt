variant results in huge communication and is not efﬁcient for
deep NNs. Our results imply that we get GOD at no additional
cost compared to BLAZE. For 4PC, we compare our results
with two best-known works FLASH [11] (which is robust)
and Trident [15] (which is fair). Our results halve the cost of
FLASH and are on par with Trident.
Benchmarking Environment We use a 64-bit ring (Z264).
The benchmarking is performed over a WAN that was in-
stantiated using n1-standard-8 instances of Google Cloud6,
with machines located in East Australia (P0), South Asia (P1),
South East Asia (P2), and West Europe (P3). The machines
are equipped with 2.3 GHz Intel Xeon E5 v3 (Haswell) pro-
cessors supporting hyper-threading, with 8 vCPUs, and 30
GB of RAM Memory and with a bandwidth of 40 Mbps. The
average round-trip time (rtt) was taken as the time for com-
6https://cloud.google.com/
municating 1 KB of data between a pair of parties, and the rtt
values were as follows.
P0-P1
151.40ms
P0-P2
59.95ms
P0-P3
275.02ms
P1-P2
92.94ms
P1-P3
P2-P3
173.93ms
219.37ms
Software Details We implement our protocols using the pub-
licly available ENCRYPTO library [21] in C++17. We ob-
tained the code of BLAZE and FLASH from the respective
authors and executed them in our environment. The collision-
resistant hash function was instantiated using SHA-256. We
have used multi-threading, and our machines were capable of
handling a total of 32 threads. Each experiment is run for 20
times, and the average values are reported.
Datasets We use the following datasets:
- MNIST [39] is a collection of 28× 28 pixel, handwritten
digit images along with a label between 0 and 9 for each
image. It has 60,000 and respectively, 10,000 images in the
training and test set. We evaluate logistic regression, and NN-
1, NN-2 (cf. §5.2) on this dataset.
- CIFAR-10 [37] consists of 32× 32 pixel images of 10 dif-
ferent classes such as dogs, horses, etc. There are 50,000
images for training and 10,000 for testing, with 6000 images
in each class. We evaluate NN-3 (cf. §5.2) on this dataset.
Benchmarking Parameters We use throughput (TP) as the
benchmarking parameter following BLAZE and ABY3 [43]
as it would help to analyse the effect of improved commu-
nication and round complexity in a single shot. Here, TP
denotes the number of operations (“iterations" for the case
of training and “queries" for the case of inference) that can
be performed in unit time. We consider minute as the unit
time since most of our protocols over WAN requires more
than a second to complete. An iteration in ML training con-
sists of a forward propagation phase followed by a backward
propagation phase. In the former phase, servers compute the
output from the inputs. At the same time, in the latter, the
model parameters are adjusted according to the difference in
the computed output and the actual output. Inference can be
viewed as one forward propagation of the algorithm alone. In
addition to TP, we provide the online and overall communi-
cation and latency for all the benchmarked ML algorithms.
We observe that due to our protocols’ asymmetric nature,
the communication load is unevenly distributed among all the
servers, which leaves several communication channels under-
utilized. Thus, to improve the performance, we perform load
balancing, where we run several parallel execution threads,
each with roles of the servers changed. This helps in utilizing
all channels and improving the performance. We report the
communication and runtime of the protocols for online phase
and total (= preprocessing + online).
5.1 Logistic Regression
In Logistic Regression, one iteration comprises updating the
weight vector (cid:126)w using the gradient descent algorithm (GD).
2662    30th USENIX Security Symposium
USENIX Association
BXT
It is updated according to the function given below: (cid:126)w =
(cid:126)w − α
i ◦ (sig(Xi ◦(cid:126)w)− Yi) . where α and Xi denote the
learning rate, and a subset, of batch size B, randomly selected
from the entire dataset in the ith iteration, respectively. The
forward propagation comprises of computing the value Xi ◦
(cid:126)w followed by an application of a sigmoid function on it.
The weight vector is updated in the backward propagation,
which internally requires the computation of a series of matrix
multiplications, and can be achieved using a dot product. The
update function can be computed using(cid:74)·(cid:75) shares as:(cid:74)(cid:126)w(cid:75) =
(cid:74)(cid:126)w(cid:75)− α
B(cid:74)XT
j(cid:75)◦ (sig((cid:74)X j(cid:75)◦(cid:74)(cid:126)w(cid:75))−(cid:74)Y j(cid:75)). We summarize our
results in Table 3.
Setting
Ref.
3PC
Training
3PC
Inference
4PC
Training
4PC
Inference
BLAZE
SWIFT
BLAZE
SWIFT
FLASH
SWIFT
FLASH
SWIFT
Online (TP in ×103)
Total
Latency (s) Com [KB]
50.26
50.32
0.28
0.34
88.93
41.32
0.50
0.27
0.74
1.05
0.66
0.97
0.83
0.83
0.76
0.75
TP Latency (s) Com [KB]
203.35
203.47
0.74
0.86
166.75
92.91
0.96
0.57
0.93
1.54
0.84
1.46
1.11
1.11
1.04
1.03
4872.38
4872.38
7852.05
6076.46
5194.18
11969.48
7678.40
15586.96
Table 3: Logistic Regression training and inference. TP is given in
(#it/min) for training and (#queries/min) for inference.
We observe that the online TP for the case of 3PC infer-
ence is slightly lower compared to that of BLAZE. This is
because the total number of rounds for the inference phase
is slightly higher in our case due to the additional rounds
introduced by the veriﬁcation mechanism (aka verify phase
which also needs broadcast). This gap becomes less evident
for protocols with more number of rounds, as is demonstrated
in the case of NN (presented next), where veriﬁcation for
several iterations is clubbed together, making the overhead
for veriﬁcation insigniﬁcant.
For the case of 4PC, our solution outperforms FLASH in
terms of communication as well as throughput. Concretely,
we observe a 2× improvement in TP for inference and a 2.3×
improvement for training. For Trident [15], we observe a drop
of 15.86% in TP for inference due to the extra rounds re-
quired for veriﬁcation to achieve GOD. This loss is, however,
traded-off with the stronger security guarantee. For training,
we are on par with Trident as the effect of extra rounds be-
comes less signiﬁcant for more number of rounds, as will also
be evident from the comparisons for NN inference.
As a ﬁnal remark, note that our 4PC sees roughly 2.5×
improvement compared to our 3PC for logistic regression.
5.2 NN Inference
We consider the following popular neural networks for bench-
marking. These are chosen based on the different range of
model parameters and types of layers used in the network. We
refer readers to [56] for a detailed architecture of the neural
networks.
NN-1: This is a 3-layered fully connected network with ReLU
activation after each layer. This network has around 118K
parameters and is chosen from [43, 48].
NN-2: This network –LeNet [38]– contains 2 convolutional
layers and 2 fully connected layers with ReLU activation
after each layer, additionally followed by maxpool for convo-
lutional layers. It has approximately 431K parameters.
NN-3: This network –VGG16 [52]– was the runner-up of
ILSVRC-2014 competition. It has 16 layers in total and com-
prises of fully-connected, convolutional, ReLU activation and
maxpool layers. It has about 138 million parameters.
Online
Total
Network
Ref.
1.92
2.22
4.77
5.08
15.58
15.89
49275.19
49275.19
536.52
536.52
36.03
36.03
NN-1 BLAZE
SWIFT
NN-2 BLAZE
SWIFT
NN-3 BLAZE
SWIFT
Latency (s) Com [MB]
0.04
0.04
3.54
3.54
52.58
52.58
TP Latency (s) Com [MB]
0.11
0.11
9.59
9.59
148.02
148.02
Table 4: 3PC NN Inference. TP is given in (#queries/min).
Table 4 summarises the results for 3PC NN inference. As
illustrated, the performance of our 3PC framework is on par
with BLAZE while providing better security guarantee.
2.35
2.97
5.61
6.22
18.81
19.29
Online
Total
Network
Ref.
NN-3
NN-2
NN-1
2.17
2.17
4.71
4.71
15.31
15.14
1.70
1.70
3.93
3.93
12.65
12.50
FLASH
SWIFT
FLASH
SWIFT
FLASH
SWIFT
59130.23
147825.56
653.67
1672.55
43.61
110.47
Latency (s) Com [MB]
0.06
0.03
5.51
2.33
82.54
35.21
TP Latency (s) Com [MB]
0.12
0.06
10.50