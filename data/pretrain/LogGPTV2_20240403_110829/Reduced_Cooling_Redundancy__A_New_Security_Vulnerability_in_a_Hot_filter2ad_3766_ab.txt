characteristics of a physical server, we ﬁrst perform a measure-
ment study running in our small testbed. We carefully design
a set of experiments to explore the thermal characteristics of a
server in the following four aspects: (1) the impact of different
workloads, (2) the thermal condition variations under the same
system utilizations, (3) the relationship between the thermal
condition and power consumption, and (4) the speed of heat
accumulation and dissipation.
3
CPURAMDISKServersInletOutletAir ConditionerSupply Air(a) Outlet temperature.
(b) Margin temperature.
Fig. 3: Temperature with HPL, SPECCPU and TPCC-UVa.
Fig. 4: Outlet
SPECCPU benchmarks.
temperature with different
Our testbed is a mainstream Supermicro server, equipped
with Intel Xeon 2.27GHz CPU with 16 cores, 32 GB of RAM
and running Ubuntu Linux 12.04 with kernel version 3.13.0.
The testbed is placed in a sealed environment without any
window to the outside world. The ambient temperature is about
21◦C cooled by the central air conditioner in our building.
We measure the inlet and outlet temperature of the server
using “Go!Temp” temperature probe, which has a resolution of
0.07◦C [8], and monitor the power consumption using “Watts
up? .Net” digital power meter. The inlet and outlet temperature
of our server is illustrated in Figure 2.
A. Workload Impacts
We ﬁrst explore how different workloads affect the thermal
conditions of a server. The attack workload should be designed
to fully utilize all components, generate a large amount of heat
and raise the temperature in a fast and signiﬁcant manner.
The “man-made” hot status would reduce the reliability of
a server. To evaluate the impact, we pay special attention
to the outlet temperature of the server for the following two
reasons. First, the output air is a direct sign of the temperature
of the server. The inner temperature can only be hotter than
the outlet
the output air is exported
to the hot aisle of the computer room and further impacts
the whole atmosphere due to air recirculation. Thus, a high
outlet temperature can negatively affect adjacent servers in a
computer room.
temperature. Second,
We measure the outlet temperature of our server under
three different scenarios. First, we use TPCC-UVa database
benchmark [36] as the workload to generate a large amount
of I/O operations. TPCC-UVa benchmark is an open source
TPC-C benchmark, which is an online transaction processing
benchmark, written in C language and using the PostgreSQL
database engine. We set the warehouse number 50 to ensure
a sufﬁciently large workload, but keep the CPU utilization
less than 10% to limit the intensity of CPU activities. In the
second scenario, we simultaneously run the TPCC-UVa and
the 456.hmmer benchmark from SPECCPU2006, which is a
widely used HPC benchmark. We run the 456.hmmer with
16 copies to fully utilize the CPU resources in our server.
In the third scenario, we add another HPC benchmark, the
High Performance Linpack (HPL). The Linpack benchmarks
measure the capability of solving random matrix productions.
There are multiple conﬁgurable parameters that could affect
the workload of the benchmark. We set the number of proces-
sors to 16 since our server is a 16-core machine. We set the
problem size N, which is the size of the input matrix, to 40,000.
For the block size, we choose 100 in this experiment. This
conﬁguration promises a heavy computing workload on our
server. Since temperature increase is a relatively long process
of heat generation, we run the experiment for 100 minutes.
The experimental results are illustrated in Figure 3. Fig-
ure 3(a) shows the outlet temperature under the three different
scenarios. Although the air conditioning is set with a ﬁxed
supply air temperature, the inlet temperature still varies in a
range of 1◦C due to various surrounding factors. We deﬁne the
margin temperature as the difference between the outlet and
inlet temperature, which is shown in Figure 3(b). The margin
temperature clearly indicates the temperature increase because
of running speciﬁc workloads on our server.
While the outlet temperature of the server at idle is about
30◦C, after heating by intensive workloads on CPU, memory,
and disk, the outlet temperature can reach up to 39◦C, as
shown in Figure 3(a). Speciﬁcally, I/O intensive workloads
create heat and raise the outlet temperature to some extent;
more obviously, after running with CPU-intensive workloads
like 456.hmmer, the outlet temperature increases signiﬁcantly.
Then, additional workloads like HPL can further generate
more heat on this basis. Given that the inlet temperature is
about 21◦C set by the cold air, the thermal-intensive work-
loads achieve a more than 18-degree temperature difference.
Also, a nearly 40◦C outlet
temperature indicates an even
hotter temperature inside the server. Thus, our experimental
results demonstrate the feasibility of mounting a thermal attack
against a server using thermal-intensive workloads, especially
CPU-intensive workloads.
B. System Utilization
To explore the outlet temperature variation under the same
system utilization, we use a set of SPECCPU 2006 benchmarks
to represent different types of workloads running in the server.
We carefully choose the set of benchmarks in which the
system resources are consumed at the same level. To ensure
exactly the same CPU utilization, we repeatedly and simul-
taneously run each benchmark with 16 copies to fully utilize
4
010002000300040005000600028303234363840Time (s)Temperature (C)  TPCCTPCC−456TPCC−456−HPL01000200030004000500060008101214161820Time (s)Temperature (C)  TPCCTPCC−456TPCC−456−HPL010002000300040005000600028303234363840Time (s)Temperature (C)  456.hmmer465.tonto462.libquantumFig. 5: Power consumption of two benchmarks.
Fig. 6: Temperatures with HPL running and stopping.
all cores. Also, proven by [53], the memory consumption of
those benchmarks are quite similar. For benchmark 456.hmmer
and 462.libquantum, the average memory utilization is about
24% and 25%, respectively. Moreover, as SPECCPU involves
limited I/O activities,
the system resources consumed by
the chosen benchmarks are very close to one another. The
experimental results are illustrated in Figure 4.
We observe that under the same system utilization, different
types of workloads could lead to different thermal conditions.
An almost 6◦C temperature difference is generated by dif-
ferent workloads with the same CPU and memory utiliza-
tions. According to [53], 462.libquantum consumes a relatively
high memory consumption but produces the minimum outlet
temperature increment. By contrast, 456.hmmer can cause a
much higher temperature increment than 462.libquantum, and
465.tonto raises the outlet temperature more than 7◦C while
consuming the least amount of memory. The main reason could
be that the types and ratios of instructions composing the
benchmarks are different. Although the system utilization is the
same, the underlying pipeline ﬂows are actually very different.
The ratio of different types of instructions, the probability
of branch prediction, and the data dependence could be very
different. Those differences further cause CPU halt and leave
functional units idle, resulting in generating different amounts
of heat. We also observe the zigzag shape of the temperature
dynamics of 456.hmmer. The reason is that multiple copies
repeatedly run together to keep generating heat; however, they
do not ﬁnish at the same time. In the gap between the ﬁrst end
of one copy and the start of next round, the system utilization
is reduced, resulting in less heat generation.
C. Power Consumption
Heat is generated when currents ﬂow through resistors,
obeying the Joule-Lenz law. Consequently, power consump-
tion, which represents the rate of energy transformation, is
closely related to heat generation. The heat in joules can be
given by:
H = I 2Rt = P t,
(1)
where I is the current, R is the resistance, and t is the time.
From the equation, we can see that the heat generation is
linearly increased to the power consumption. Using the results
obtained from the same set of SPECCPU 2006 experiments
conducted above, we present
the power consumptions of
456.hmmer and 462.libquantum in Figure 5. As the benchmark
that raises to almost 7◦C higher than 462.libquantum, on aver-
age 456.hmmer also consumes almost 70W more power than
462.libquantum. Both theory analysis and experimental results
indicate that running a power-intensive workload ensures more
heat, which could be exploited for mounting a thermal attack.
This result (i.e., 70W difference in power consumption) also
forms the basis for the conﬁguration of one parameter in our
large-scale experimental evaluation.
D. Heating Speed
We further measure the speed to heat a server. Unlike a
power stimulus that surges instantaneously, the temperature
dynamics is a process of heat accumulation. We choose HPL
with a block size of 50 as the thermal-intensive workload. We
run the workload for 30 minutes and then stop it. In the ﬁrst
10 minutes, the temperature starts to increase quickly. Then,
although the temperature is still raising, the speed drops. The
dynamics of the server’s outlet temperature is illustrated in
Figure 6. The temperature starts to increase quickly in the ﬁrst
10 minutes. The temperature can increase about 6.8◦C higher
than that of the idle state. Then, although the temperature is
still raising, the speed drops. After we stop the workload, the
cooling system can quickly decrease the temperature in the ﬁrst
several minutes. The temperature drops almost 4.5◦C within
the ﬁrst ﬁve minutes and 6◦C within the ﬁrst 10 minutes.
However, after 10 minutes, the speed to dissipate the heat
slows down considerably. Even after half an hour, the outlet
temperature still cannot return to the original value at the idle
state, which implies that the full dissipation of heat requires
quite a long time. Overall, we have two major observations:
(1) the temperature of a server varies quickly when a thermal-
intensive workload starts/stops; and (2) the dynamics of the
server temperature is non-linear.
IV. THERMAL ATTACK
In this section, we ﬁrst describe our threat model. We then
mount thermal attacks on both virtualized and non-virtualized
environments, as well as a pulsation attack. Based on the attack
results, we further conduct damage assessment.
A. Threat Model
A thermal attack can be launched at the server level, rack
level, or data center level. We assume that the target data
5
0100200300400500600150200250300Time (s)Power (W)  456.hmmer462.libquantum01000200030001015202530354045Time (s)Temperature (C)  InletHPL−50 RUNSTOP Gap(a) Outlet temperature
(b) Temperature of the 1st core
(a) One VM.
(b) Multiple VMs.
Fig. 7: Thermal attack on a non-virtualized environment.
Fig. 8: Margin temperature of thermal attacks on a virtualized
environment.
center has the following features. (1) It is cooled by traditional
CRAC cooling systems with optimal cooling policies deployed
to maximize cooling efﬁciency, i.e., the supply air temperature
is set as high as possible to save energy while keeping the
inlet temperature below a redline threshold. (2) It provides
utility-based computing services that are accessible over the
Internet. (3) Thermal sensors are equipped in the data center,
and temperature monitoring is conducted at the rack level. Note
that most current data centers have just a few thermal probes
for the entire data center, and only some experimental data
centers (like HP Labs) have about 2-4 thermal sensors per
rack. (4) Like most current data centers, the target data center
also deploys power oversubscription.
The attacker could be an individual, a competitor of a cloud
service provider, or a cybercrime organization. The attacker
does not require more privileges than a regular user to access
the target cloud service, and no compromised hypervisor is
required. This is mainly due to the fact
in a cloud
environment, especially IaaS (Infrastructure as a Service) and
PaaS (Platform as a Service), a tenant has the privilege to
run any workloads/applications at the guest level, including
the benchmarks used in our measurement study. While those
workloads are developed to assess the performance of speciﬁc
scenarios, different combinations and conﬁgurations of them
compose thermal-intensive workloads.
that
However, the attacker might utilize the publicly available
information or network probing to ﬁgure out
the network
topology inside a cloud [43], and exploit more advanced
probing techniques to achieve tenant co-residence in the same
physical server or rack [52]. Moreover, the attacker could run
advanced thermal-intensive programs (e.g., power virus [22],
[23]), instead of regular thermal-intensive workloads, to further
exaggerate the heat generation.
Note that a more tangible goal of a thermal attack is not to
shut down an entire data center, but to cause a cooling failure
in a data center, in which some victim servers are forced to
shut down. Under a thermal attack, much more heat will be
generated than normal. Once the heat released into the hot
aisle surpasses the recyclability of a cooling machine, the inlet
temperature increases. The raising of the inlet temperature will
further increase the outlet temperature and generate more heat.
Such a vicious cycle will ﬁnally lead to a cooling failure.
Moreover, the overheat caused by thermal attacks will reduce
the performance and reliability of victim servers, increase the
possibility of hardware failures, and force the data center to
reset its cooling conﬁguration, resulting in a much higher
cooling cost.
B. Non-virtualized Environments
In this scenario, we assume that an attacker owns a
dedicated host (e.g., a dedicated instance in Amazon EC2) in
a data center. As the attacker can choose any workload to run
at will, the attack vector is straightforward, running thermal-
intensive workloads for a relatively long time.
As reported in [15], the average system utilization of most
servers in a data center is between 20 - 30%. We also assume
that the victim server is running moderate workloads with
25% system utilization. We choose benchmark 462.libquantum
to represent a moderate workload. Although 462.libquantum
belongs to the CPU-intensive benchmark suite, it is highly vec-
torizable. The high-dimensional matrix computation requires
more I/O operations and makes 462.libquantum consume less
power consumption than other SPECCPU benchmarks like
456.hmmer, as shown in Figure 5.
The thermal attack starts after the victim server running the
moderate workload for 30 minutes. The attacker ﬁrst pushes
the system utilization to 100%. The ﬁrst attack phase lasts
30 minutes. After the utilization reaches its cap, the attacker
replaces the moderate workload with the thermal-intensive
workload to further heat the server. In this attack, we use a
combination of SPECCPU 456.hmmer and HPL with a block
size set of 50 to represent the thermal-intensive workload.
Again, the attack lasts about 30 minutes.
The dynamics of the server’s outlet temperature is shown
in Figure 7(a). At the beginning, the moderate workload runs
at an average of 25% utilization in the server. The outlet
temperature of this phase is 32◦C. On the next phase, the mod-
erate workload pushes the server into the cap utilization. With
fully utilized system sources, the outlet temperature reaches
34◦C. Since the difference is less than 2◦C, the cooling control
system can handle it without any trouble. In the ﬁnal phase,
under the thermal-intensive workloads, the outlet temperature
is rapidly raised to more than 38◦C, which is 6◦C higher than
the temperature under the moderate workload. Even under the
same utilization, it is evident that malicious thermal-intensive
workloads can generate a signiﬁcant temperature rise and emit
a large amount of heat to the computer room. We also show the
temperature of the ﬁrst core in our testbed under the thermal
6
010002000300040005000600010152025303540Time (s)Temperature (C)  OutletInletNormal condition with average utilizationAttack phase 1 Moderate workloads with cap utilizationAttack phase 2 Thermal intensive workloads with cap utilization050001020304050607080Temperature (C)Normal050001020304050607080Attack phase 1050001020304050607080Attack phase 20100020003000400050006000910111213141516Time (s)Temperature (C)  Baseline456HPLTPCC−456−HPL01000200030004000500060008101214161820Time (s)Temperature (C)  Baseline2vms3vms4vmsAttack
Continuous
Pulsation
CoV
0.0071
0.0440
TABLE I: CoV.
thermal attack on three VMs can lead to about a 5◦C higher
temperature than the baseline case. However, once all CPU
resources are already fully utilized in a physical machine
(under four VMs in our experiment), running even more VMs
in the same machine cannot achieve a signiﬁcant difference in
heating generation.
In our experiment, we assume that VM live migration is
not adopted in the target data center, and the attackers can
run thermal-intensive workloads on the target server/VMs to
reach full system utilization. This assumption is valid because
VM live migration has not yet been widely adopted in real
clouds, and live migration suffers non-negligible downtime on
the migrated VMs running with intensive workloads. Note that
if VM live migration is enabled, the process of live migration
can cause a sharp rise of power consumption on both source
and destination servers [53]. Such a power spike may trip a