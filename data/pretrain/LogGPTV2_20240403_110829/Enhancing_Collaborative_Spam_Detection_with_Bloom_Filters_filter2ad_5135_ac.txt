a [i] + ct0
ct0
b [i] otherwise
if ct0
a [i] + ct0
b [i] > 31
(cid:7)
But for the next subsequent synchronisation be-
tween the two servers at time interval t1, server a just
need transmit the following delta Bloom ﬁlter to server
b:
ct1
Δa[i] = ct1
a [i] − ct0
a [i], i = 0, ..., m − 1,
and server b just need transmit its own delta Bloom ﬁl-
ter to server a:
ct1
Δb[i] = ct1
b [i] − ct0
b [i], i = 0, ..., m − 1.
• Signature aging. The scheme sketched for Razor in
the previous section is also applicable here.
As such, it appears that this intuitive Bloom ﬁlter vari-
ant is well suitable for DCC. However, the following coun-
terexample suggests that further reﬁnements are needed.
Assume that each DCC server constructs its (extended)
Bloom ﬁlters in exactly the same way: the same hashes and
the same parameters (k, m, n) are used. Also assume that
k = 3, without loss of generality. Also assume that in server
a’s Bloom ﬁlter, we have
, (4)
c[h1(x)] = 2, c[h2(x)] = 5, c[h3(x)] = 8.
where max j is the maximal number of times the mes-
sage x has occurred. Intuitively, fdcc < fI.
• Signature database merging. We assume that each
end-user reports email messages she has received to
no more than one DCC server. This is a realistic as-
sumption, since each DCC server is usually desig-
nated to serve a particular part of the user population.
1
This deﬁnition considers only insertions, having ignored the case of
deletion.
Thus, Count(x) = 2. That is, message x has been reported
to this server at most twice. Similarly, in server b’s Bloom
ﬁlter, we have
c[h1(x)] = 4, c[h2(x)] = 4, c[h3(x)] = 3.
Thus, Count(x) = 3. That is, message x has been re-
ported to this server at most three times. We should have
Count(x) = 5 when two servers have completed synchro-
nising their signature databases. However, the merging al-
gorithm will give Count(x) = min(6, 9, 11) = 6!
Proceedings of the 22nd Annual Computer Security Applications Conference (ACSAC'06)0-7695-2716-7/06 $20.00  © 2006The lesson is that many counters in the ﬁlter, before and
after merging, could increase rapidly because of coinciden-
tal hits, by which a single cell is used by two or more ele-
ments.
We introduce a reﬁned extension of the Bloom ﬁl-
ter to address the above problem. In this extension, All
else remain as in the intuitive extension, except
that
the following heuristic (which we refer to H1) will ap-
ply: when x is inserted into Array c, among counts
c[h1(x)], c[h2(x)], ..., c[hk(x)], only those that equal to
min(c[h1(x)], ..., c[hk(x)]) will be increased by one.
Return to the above scenario, and suppose that each
server witnesses one more x. Then, with the intuitive ex-
tension, Server a has
c[h1(x)] = 3, c[h2(x)] = 6, c[h3(x)] = 9,
and Count(x) = 3; Server b has:
c[h1(x)] = 5, c[h2(x)] = 5, c[h3(x)] = 4,
and Count(x) = 4. However, after database merging, the
system will have:
Count(x) = min(8, 11, 13) = 8.
On the contrary, with the reﬁned extension, we will have
c[h1(x)] = 3, c[h2(x)] = 5, c[h3(x)] = 8,
and Count(x) = 3 in Server a, and
c[h1(x)] = 4, c[h2(x)] = 4, c[h3(x)] = 4,
and Count(x) = 4 in Server b. After merging, we will have
an accurate result:
Count(x) = min(7, 9, 12) = 7.
That is, the counters in the reﬁned extension do not increase
as rapidly as in the intuitive extension, both before and after
merging!
Table 2 compares the counter growth in these two ex-
tended Bloom ﬁlter schemes in Server b. It shows clearly
that the reﬁned extension has a better performance in con-
trolling undesirable counter incrementing.
It is worthwhile to note that a correct implementation
of the reﬁned Bloom ﬁlter extension implies an additional
heuristic H2: when x is inserted, if any two or more of
h1(x), h2(x), ..., hk(x) hit the same counter, then that
counter should be increased only once. If we say that H1
is introduced to address global coincidental hits caused by
multiple elements, then the heuristic H2 addresses local co-
incidental hits caused by a single element. H2 should also
be implemented in the intuitive Bloom ﬁlter extension in or-
der to reduce false positives – this is another insight that was
missing in [4].
Occurence
of x
4
3
2
1
0
Intuitive extension
Reﬁned extension
c[h1(x)] c[h2(x)] c[h3(x)] c[h1(x)] c[h2(x)] c[h3(x)]
5
4
3
2
1
5
4
3
2
1
4
3
2
1
0
4
3
2
1
1
4
3
2
1
1
4
3
2
1
0
Table 2. Counter growth in two extended
Bloom ﬁlter schemes: an example (as seen
by Server b)
We deﬁne the false positive rate in the reﬁned extension,
fR, the same as fI is deﬁned. fR cannot be subjected to
mathematical analysis. However, intuitively, fR < fI. In
addition to the reduced false positives, the reﬁned Bloom
ﬁlter extension enjoys all other nice features in the intuitive
extension.
Another innovation we introduce to our Bloom ﬁlter
extension is to reduce its storage cost by splitting it to
two parts: a base ﬁlter and a number of hash tables. We
rely on a simple intuition: it is a waste of space to allocate
each counter the number of bits large enough to accommo-
date the largest count that will be recorded, if the ﬁlter is ex-
pected to maintain many small counts and the discrepancy
between the sizes of the small and large counts is large.
Instead, we introduce a base ﬁlter that has a uniform cell
size of s + 1 bits, assuming that the expected largest small
count value is not larger than 2s. A one-bit ﬂag in each cell
indicates whether this count has additional bits, which, if
any, are stored somewhere else. That is, a large count has
only part of bits (e.g. its lower half) kept in the base ﬁlter. Its
other part could be stored in a hash table, indexed by the off-
set of the count in the base ﬁlter. To reduce the space occu-
pied by this hash table, where each index requires (cid:4)log2 m(cid:5)
bits, we virtually divide the base ﬁlter into a number of, say
N, chunks. Then, instead of having a large hash table for the
whole ﬁlter, we organise N small hash tables, each with the
index size reduced to (cid:4)log2(m/N)(cid:5), and each storing ad-
ditional bits of large counts in a corresponding chunk only.
Preliminary results suggest that this technique is promising.
The details will be reported in a forthcoming paper.
5. A simulation study
It would be very interesting to evaluate how much more
the reﬁned Bloom ﬁlter extension would improve fdcc than
the intuitive one did, using empirical data collected from
various DCC servers. However, such data collection has
proved to be difﬁcult. The DCC developer did not like the
idea that an additional DCC server gets connected to the
whole DCC network “for temporary, purely academic pur-
poses” [11].
Proceedings of the 22nd Annual Computer Security Applications Conference (ACSAC'06)0-7695-2716-7/06 $20.00  © 2006Instead, we have run a series of simulations to compare
the false positive rate that could occur in both Bloom ﬁl-
ter extensions, i.e. fI and fR. Additional reasons support-
ing such a decision are as follows.
First of all, both extended Bloom ﬁlters are data struc-
tures of general interest. For example, both can be applied
to applications where it is relevant to support fast mem-
bership testing and distributed counting2. Therefore, we do
not limit our simulation design to the purpose of enhanc-
ing DCC only, but also aim to gain a good understanding
of how both extensions (the reﬁned extension in particular)
will perform in a general setting. To the best of our knowl-
edge, no such effort has been reported in the literature.
Second, fdcc is smaller than the false positive rate of the
speciﬁc Bloom ﬁlter extension implemented in DCC. That
is, fdcc < fI or fdcc < fR. Therefore, fI or fR observed in
our experiments can be used as an upper bound of fdcc.
5.1. Simulation design
Without loss of generality, universal hashing is used
to construct both extended Bloom ﬁlters in our simula-
tions. One advantage of this construction is its simplic-
ity and efﬁciency. Following the practice in [9], we use
p = 2, 100, 000, 011, and we generate 2k pseudo-random
numbers, each pair being used as c and d to deﬁne a hash in
the form of Equation (2)3.
We use 10,000 distinct keys (i.e., elements to be inserted
to the Bloom ﬁlters) in our simulations. They are integers
randomly drawn from a universe A, A = {1, 2, ..., p − 1}.
The k hash functions are applied to each of the keys
and the corresponding cells in the Bloom ﬁlters are in-
cremented accordingly. In the intuitive extension, cells
c[h1(x)], ..., c[hk(x)] will all be incremented when x is in-
serted into the ﬁlter, and the heuristic H2 will also be
enforced. In the reﬁned extension, both H1 and H2 are en-
forced, and thus only the cell with a value equalling to
min(c[h1(x)], ..., c[hk(x)]) will be incremented (by one
only).
Our experiments are designed as follows.
Experiment 1. Each of the 10,000 elements is inserted
sequentially into the ﬁlter. This entire process is repeated 20
2
3
Some counts returned by both extended Bloom ﬁlters might not be ac-
curate, due to coincidental hits. However, the number of such “approx-
imate counts” in the reﬁned extension can be very small, as shown in
the later part of this paper.
It is worthwhile to note: k hash functions constructed this way are not
necessarily independent, strictly speaking. However, when they were
used to build Bloom ﬁlters, the empirical false positive rate of these
ﬁlters met its theoretical expectation [9]. We have repeated the exper-
iments introduced in [9] and conﬁrmed this result.
In our future work, we will apply additional constraints as sug-
gested by Knuth [5] to construct k independent, random (universal)
hashes and then repeat experiments discussed in this section to see
whether any new ﬁndings will be found.
(cid:8)
(cid:8)
(cid:11)
(cid:11)
times. The whole insertion sequence is as follows.
(cid:8)
(cid:9)(cid:10)
(cid:11)
(cid:8)
(cid:9)(cid:10)
(cid:11)
x1, x2, ..., x10,000
, ......, x1, x2, ..., x10,000
Round1
Round20
Experiment 2. Each element is inserted 20 times repeat-
edly into the ﬁlter. The entire process continues until all the
10,000 elements have been inserted. The whole insertion se-
quence is as follows.
(cid:8)
(cid:9)(cid:10)
(cid:11)
(cid:8)
(cid:9)(cid:10)
(cid:11)
x1, ..., x1
, x2, ..., x2
, ... ..., x10,000, ..., x10,000
(cid:9)(cid:10)
20
20
20
Experiment 3. Each of the 10,000 elements is inserted
into the ﬁlters 20 times, but the sequence for insertion is
random. We apply the classical Fisher-Yates shufﬂe algo-
rithm [6], converting the insertion sequence in Experiment
2 into a random sequence. Each element in the random se-
quence is then inserted sequentially into the ﬁlter.
Experiment 4. Each of the 10,000 elements is inserted
into the ﬁlters in a random order, and each inserted a random
c times (c ∈ [0, 20]). For each element xi, we generate an
integer ci, uniformly distributed on the range [0, 20]. Then,
we organise all the elements in the following sequence.
(cid:8)
(cid:9)(cid:10)
(cid:11)
(cid:8)
(cid:9)(cid:10)
(cid:11)
x1, ..., x1
, x2, ..., x2
, ... ..., x10,000, ..., x10,000
(cid:9)(cid:10)
c1
c2
c10,000