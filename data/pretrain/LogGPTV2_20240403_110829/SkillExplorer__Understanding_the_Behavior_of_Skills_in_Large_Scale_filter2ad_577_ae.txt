ence...”, “Do not download this app. ... It stores your info and
password.”. It seems the suspicious behaviors have already
troubled users. Another skill Scare Text requests users’ phone
numbers. According to its description, this skill will send a
randomly selected scary image via texting message to the
given number. However, after we test several phone numbers,
the skill never sends the message.
4.3 Skills Conﬂicting with “Stop”
After ﬁnishing using a skill, users will stop the skill. Oth-
erwise, the skill will continue listening to the users’ private
conversations. However, some malicious skills may not stop
even if they receive users’ commands to stop. So we want
to check the existence of skills with such behaviors in the
wild markets. According to the survey [35], 91% of Alexa
users use the command “stop” to terminate a skill, and 36% of
users choose “cancel”, and only 14% of them use “exit”. So
we send the command “stop” to the skills. Then we leverage
some built-in functions of the virtual personal assistant (VPA)
and check whether the VPA is activated. For example, we can
ask the time using Alexa’s own function “what time”. If the
response is the current time, we can verify that the skill has al-
ready exited. Otherwise, it is still on. Although this approach
may be circumvented by hijacking the built-in functions, we
can try other different functions to test for better accuracy. In
our interaction experiments, we also ﬁnd that a small number
of skills behave differently in simulators and the real devices
when receiving the commands to exit. We are not sure about
the concrete reason. Thus, after some potentially harmful
skills are automatically detected, we need to check them on
real devices. Note that the different behaviors only happen
when receiving the command “stop” to exit. For other voice
commands, they behave consistently in both environments11.
Results. We evaluate 28,904 Amazon skills, and ﬁnd 802
skills do not really stop after receiving the stop command on
the simulator. Then we use Echo for further checking, and
ﬁnd that 68 skills have problems on the real smart speak-
ers. In this process, we only need to open a skill and stop it,
which takes about 15 seconds to ﬁnish (about 3 hours in total).
Then we carefully analyze 68 suspicious skills. They achieve
eavesdropping using one of the following three ways. (1) 32
skills change the default “stop” commands to others which
users may not know. For example, the skill Millennial Money
11One may be worried about that some malicious developers can ﬁnd
the differences between simulators and real environments. However, to the
best of our knowledge, no open materials mention whether simulators are
used in the vetting. In the current stage, it seems there is no motivation
for developers to distinguish the two environments. Although we observed
that the simulator behaves differently when receiving the “stop” command.
However, the command can be replaced by “exit”, which will not let the
simulator behave differently. Also, SkillExplorer can evaluate the “stop”
command after vetting all other behaviors. If a skill behaves differently later
(after identifying the simulator), it is highly suspicious.
2660    29th USENIX Security Symposium
USENIX Association
changes the default command to “I’ve done”. (2) 29 skills ig-
nore the stop command after correctly receiving it (we verify
this from the communication history supported by Amazon).
For example, the skill My birthday month always says that it
cannot get the stop command (which actually indicates that it
has received it) and continues its other functions. (3) 7 skills
seem more strange. For example, one skill named Malignant
Tweets always returns “cannot ﬁnd this skill” to mimic Alexa
no matter what commands it receives and continues to listen
to users’ conversation. Another skill named math-training
replies “OK” after receiving the stop command. But it will
continue to listen to users’ conversation for 6 seconds. Re-
garding actions from Google, since Google has very strict
requirements on the exit operation, it limits the developer’s
ﬁnal response to a simple reply within 60 characters and must
be the last dialogue in this interaction [16]. We did not ﬁnd
any action that has such a problem.
4.4 Skills Conﬂicting with Their Descriptions
We further want to check whether the information requested
by a skill is corresponding to its description given by devel-
opers. However, this is very challenging due to diverse ways
to describe the skills and the different functionalities given
by the skills. So we do a preliminary study. Considering that
skills with similar functionalities should behave in a similar
way, we use some skills as the seeds and compare other skills
with them. For example, two skills A and B both provide
real estate information. It is normal that both of them request
users’ addresses. However, it would be very strange if a skill
requests for the health status of users.
To achieve the differential analysis, we manually select 100
typical skills in 10 categories which request for various kinds
of privacy information and view them as the seeds. Then we
extract the keywords of the descriptions (i.e., nouns in the
constituency-based parsing tree) from all the collected skills.
In this way, we could ﬁnd the skills with similar functionali-
ties. Then we compare the behaviors of the skills, especially
the privacy information they request. In this way, we can ﬁnd
skills with abnormal behaviors.
Results. After manual veriﬁcation to ﬁlter out some reason-
able cases, only a few abnormal skills are left. In this pre-
liminary measurement, less than 10 skills request personal
information that does not match their descriptions. For ex-
ample, a skill named Ehrlich Pest Control is supposed to tell
users about how to prevent common household pests (e.g.,
mice) according to the description. However, if a user asks
some questions that cannot be understood by the skill, it will
ask the user for her phone number and area code. We also
ﬁnd there is a low rating for the skill on the market. A user
mentioned that “...it complied then asked me for my phone
number so not going to happen, ﬁx that again asking me for
my phone number wrong move.”
5 Discussion
5.1 Defense Suggestions
Although SkillExplorer could serve as a supplement approach
for the market administrators to vet skills, we still have some
suggestions for them. Firstly, skills should be strictly reviewed
before being put on the shelf, especially the contents related
to privacy contents. Considering some technical challenges
(e.g., the ownership of the privacy-related words) may im-
pede the detection, NLP should be included in the automatic
analysis (see Section 3 and Section 4). Secondly, besides
the contents provided by skills, the privacy policy links of
skills also need to be strictly checked. In this way, users can
have a general understanding of what kinds of personal in-
formation that the skills will request before users use them.
Currently, the markets do not request the privacy policy to
be in a uniﬁed form. So developers can prepare the privacy
policy in various forms (e.g., on a web page, a PDF ﬁle, or
even missing), which makes the vetting process quite difﬁ-
cult. An ofﬁcial template could be provided to the developers
to follow. Thirdly, the built-in intents should also be strictly
checked if skills are using them. For example, the built-in stop
intent should be carefully checked which might allow a skill
to continue working even after receiving the stop command.
5.2 Limitations and Future Work
Firstly, the accuracy of SkillExplorer can be further increased.
Current problems are mainly due to developers’ irregular de-
sign of the questions. Sometimes, developers want to make
their questions be clearly spoken out by smart speakers. So
they usually add some marks or punctuation insides the ques-
tions. For example, there is a question “You can say News
-or- Story”. Developers add marks before reminding users of
the words they need to say to highlight the key points when
pronouncing. Although it does not impact user experience (or
maybe make the user experience better), this will greatly im-
pact the analysis since such combinations of words and punc-
tuation seldom appear in real texts. Currently, NLP tools (e.g.,
Stanford NLP Parser) cannot handle this situation. Although
in our study we have some techniques and special rules (e.g.,
removing the punctuation except the quotation mark imme-
diately after the instruction words “say” and “ask”), our tool
can be further improved. Also, current NLP tools have false
positives (e.g., the extraction of the juxtaposition relationship
is wrong, resulting in problems in generating answers).
Another limitation is from the simulator. Currently, it has
restrictions on the interaction with mobile phones and the
transmission of geographical location. Neither can it play
the non-text audio. If a privacy-related question is played by
audio, the simulator cannot correctly identify and return the
texts in it. We will identify such a situation and further solve
this problem.
USENIX Association
29th USENIX Security Symposium    2661
6 Related Work
Attacks on skills. Recent studies have been carried out to un-
derstand the invocation mechanisms of skills. KUMAR et al.
discover skill squatting [26], a kind of homo-phonic attacks
to divert users’ request to an undesired skill. Zhang et al. [35]
further ﬁnd voice squatting and voice masquerading, which
allows a similar pronounced skill to hijack the existing legiti-
mate skills. They also perform a large-scale analysis on skills
with similar names or pronunciations in the Amazon market
and Google market. Recently in October 2019, researches
from SR Labs [3] demonstrate how a malicious skill can
eavesdrop users’ privacy after receiving the command to stop
based on the research of [26], which is also found by us simul-
taneously. Our work differs from theirs. They design a skill
to implement such attacks, while we perform a large-scale
analysis on skills and ﬁnd 68 skills in markets having such
problems. Zhang et al. [36] ﬁnd the vulnerability of NLU’s
Intent Classiﬁer and leverage it to let the classiﬁer misunder-
stand users’ request and route the request to a malicious skill.
These studies mainly focus on the invocation mechanism of
skills, while our work is to explore the behaviors of skills and
analyze the contents of the conversation.
Attacks on smart speakers. Researches [17, 24, 27] ﬁnd
that the mechanism of what they imagine is very different
from what the smart speakers actually do. However, some
studies [22, 23, 32] have already analyzed the security and
privacy of general IoT devices, including smart speakers. Car-
lini et al. [20] perform Hidden Voice attacks on Amazon
Echo, which proves the feasibility of audio attack from two
aspects of black box and white box. It’s found that both at-
tacks can successfully occur on physical devices. Based on
this, the authors put forward some ideas of defense. Dolphi-
nAttack [34] can modulate voice commands on ultrasonic
carriers such as frequencies greater than 20 kHz so that peo-
ple cannot hear them, while still attacking smart speakers.
Yuan et al. [21, 33] integrate the voice commands into a song
and let the commands be correctly identiﬁed by an audio
speech recognition (ASR) system but not perceptual to hu-
man. Bispham et al. [19] try to hack the ASR system of smart
speakers by gaining covert access to them with nonsense or
missense sounds. Sugawara et al. [31] leverage the laser to
remotely inject inaudible and invisible commands into voice
assistants, taking advantages of the vulnerability of MEMS
microphones. These studies mainly focus on how to inject
commands into smart speakers or related ASR systems with-
out being captured by human users, which are different from
our study on skill behaviors.
7 Conclusion
In this paper, we propose the ﬁrst systematic study on the be-
haviors of skills. The key techniques enabling the exploration
are a suite of grammar-based methods including utterance ex-
traction, question understanding and answer generation. We
develop a tool called SkillExplorer to automatically commu-
nicate with 28,904 skills from the Amazon market and 1,897
actions from the Google market, a scale that has never been
achieved before. Based on our measurement, over 1,000 skills
request users to provide personal information without follow-
ing developer speciﬁcations; 68 skills continue to eavesdrop
users’ conversation even after receiving the command to stop.
Acknowledgments
The authors would like to thank anonymous reviewers for
their insightful comments that have helped improve this pa-
per substantially. Speciﬁcally, we thank our shepherd, Profes-
sor Adam Bates, for his constructive feedback on this paper.
The authors are supported in part by Beijing Natural Sci-
ence Foundation (No.JQ18011), NSFC U1836211, National
Top-notch Youth Talents Program of China, Youth Innova-
tion Promotion Association CAS, Beijing Nova Program,
National Frontier Science and Technology Innovation Project
(No. YJKYYQ20170070), and Beijing Academy of Artiﬁcial
Intelligence (BAAI).
References
[1] https://voicebot.ai/2019/10/01/amazon-alex
a-has-100k-skills-but-momentum-slows-globa
lly-here-is-the-breakdown-by-country/.
[2] https://voicebot.ai/2020/01/19/google-assi
stant-actions-grew-quickly-in-several-lang
uages-in-2019-match-alexa-growth-in-englis
h/.
[3] https://srlabs.de/bites/smart-spies/.
[4] https://www.pandorabots.com/mitsuku/.
[5] https://developer.amazon.com/docs/custom-s
kills/certification-requirements-for-custo
m-skills.html/.
[6] https://developer.amazon.com/zh/docs/custo
m-skills/request-customer-contact-informat
ion-for-use-in-your-skill.html.
[7] https://en.wikipedia.org/wiki/Yes-no_quest
ion.
[8] http://www.surdeanu.info/mihai/teaching/ist
a555-fall13/readings/PennTreebankConstitue
nts.html.
[9] https://developer.amazon.com/en-US/docs/al
exa/alexa-skills-kit-sdk-for-nodejs/develo
p-your-first-skill.html.
2662    29th USENIX Security Symposium
USENIX Association
[10] https://developer.amazon.com/en-US/docs/al
exa/custom-skills/manage-skill-session-and
-session-attributes.html.
[22] Tamara Denning, Tadayoshi Kohno, and Henry M. Levy.
Computer security and the modern home. Commun.
ACM, 56(1):94–103, 2013.
[11] https://www.oxfordlearnersdictionaries.com
/definition/american_english.
[12] https://www.cleverbot.com/.
[13] https://www.fakenamegenerator.com.
[14] http://www.nltk.org.
[15] https://nlp.stanford.edu/.
[16] https://developers.google.com/assistant/co
nversational/conversation-exits.
[17] Noura Abdi, Kopo M. Ramokapane, and Jose M.
Such. More than smart speakers: Security and pri-
vacy perceptions of smart home personal assistants. In
Heather Richter Lipford, editor, Fifteenth Symposium on
Usable Privacy and Security, SOUPS 2019, Santa Clara,
CA, USA, August 11-13, 2019. USENIX Association,
2019.
[18] Benjamin Andow, Samin Yaseer Mahmud, Wenyu
Wang, Justin Whitaker, William Enck, Bradley Reaves,
Kapil Singh, and Tao Xie. Policylint: Investigating
internal privacy policy contradictions on google play.
In Nadia Heninger and Patrick Traynor, editors, 28th
USENIX Security Symposium, USENIX Security 2019,
Santa Clara, CA, USA, August 14-16, 2019, pages 585–
602. USENIX Association, 2019.
[19] Mary K. Bispham, Ioannis Agraﬁotis, and Michael Gold-
smith. Nonsense attacks on google assistant and mis-
sense attacks on amazon alexa. In Paolo Mori, Steven
Furnell, and Olivier Camp, editors, Proceedings of the
5th International Conference on Information Systems
Security and Privacy, ICISSP 2019, Prague, Czech Re-
public, February 23-25, 2019, pages 75–87. SciTePress,
2019.