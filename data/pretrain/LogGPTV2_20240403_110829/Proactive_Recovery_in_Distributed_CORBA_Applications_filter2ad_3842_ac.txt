overhead of about 90% over
the baseline round-trip
time. This overhead resulted from parsing GIOP mes-
sages so that we could keep track of object keys and
request ids and fabricate the appropriate GIOP mes-
sages needed to forward requests to the next available
replica. The NEEDS ADDRESSING MODE scheme’s
overhead was only 8% higher than the baseline since we
did not need to keep track of object keys. The scheme
in which we used MEAD messages introduced an over-
head of about 3% over the baseline client-server round-trip
time.
The communication overhead introduced by the proac-
tive schemes depends on the frequency with which proac-
tive recovery is invoked. The additional messages sent by
MEAD’s proactive dependability framework, in the event of
a failure, typically range between 100-150 bytes per client-
server connection. Since systems typically experience more
non-faulty, rather than faulty, behavior, the overall commu-
nication overhead introduced by our approach is reasonable.
The differences in memory and CPU usage for our ap-
plication were not signiﬁcant. However, we expect that
as the server supports more objects, the overhead of the
GIOP LOCATION FORWARD scheme will increase sig-
niﬁcantly above the rest since it maintains an IOR entry for
each object instantiated.
5.2.3. Average fail-over times: The fail-over time in-
cludes both the fault-detection time and the fault recovery
time. The initial transient spike shown on each graph repre-
sents the ﬁrst call to the CORBA Naming Service (see Fig-
ure 3). In the reactive scheme where we did not cache
server replica references, the client ﬁrst experienced a
COMM FAILURE exception when the server replica dies;
the COMM FAILURE exception takes about 1.8ms to reg-
ister at the client. The client then incurs a spike of about
8.4ms to resolve the next server replica’s reference (result-
ing in an overall failover time of 10.2ms).
In the case where we cache server references, we ex-
perience about one TRANSIENT exception for every two
COMM FAILURE exceptions. The COMM FAILURE ex-
ception takes about 1.1ms, and the time needed to fail-over
to the next cached replica reference and receive a normal re-
sponse is 7.9ms. However, when the client accesses a stale
cache reference, the client experiences a TRANSIENT fail-
ure in addition to the COMM FAILURE exception. The
TRANSIENT failure takes about 2.4ms since it includes the
time to redirect entries to the new replica, as well as the
time to process the the actual TRANSIENT exception. Fi-
nally, the client also experiences a spike of about 9.7ms,
which is the time taken to resolve all three replica refer-
ences and receive a correct response. The average fail-over
time for this scheme is about 10.5ms, i.e., ((1.1+7.9)*2/3 +
(1.1+2.4+9.7)/3) ms. (See Figure 3).
For
the
that
scheme
proactive
used LOCA-
TION FORWARD messages,
fail-over
time was 8.8ms (13.5% below the reactive scheme with
no cache) because when the client ORB receives the LO-
CATION FORWARD, it has to resend the request to the
the
average
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:52:12 UTC from IEEE Xplore.  Restrictions apply. 
)
s
d
n
o
c
e
s
i
l
l
i
m
(
T
T
R
20
15
10
5
0
Reactive Recovery Scheme  (Without cache) 
2000
4000
Runs
6000
8000
10000
)
s
d
n
o
c
e
s
i
l
l
i
m
(
T
T
R
20
15
10
5
0
Reactive Recovery Scheme  (With cache) 
2000
4000
Runs
6000
8000
10000
Figure 3. Reactive recovery schemes.
)
s
d
n
o
c
e
s
i
l
l
i
m
(
T
T
R
Proactive Recovery Scheme
 (GIOP Needs_Addressing_Mode) 
Proactive Recovery Scheme
(GIOP Location_Forward−Threshold=80%) 
Proactive Recovery Scheme
 (MEAD message−Threshold=80%) 
20
15
10
5
0
2500
5000
Runs
7500 10000
)
s
d
n
o
c
e
s
i
l
l
i
m
(
T
T
R
20
15
10
5
0
2500
5000
Runs
7500 10000
)
s
d
n
o
c
e
s
i
l
l
i
m
(
T
T
R
20
15
10
5
0
 Reduced jitter
2500
5000
Runs
7500 10000
Figure 4. Proactive recovery schemes.
next server replica. For the scheme using MEAD mes-
sages, the average fail-over time was about 2.7 ms (73.9%
below the reactive scheme with no cache), since we
avoided request retransmissions and incurred an over-
head only when redirecting a connection to a new server
(see Figure 4).
for
Finally,
the NEEDS ADDRESSING MODE
scheme, the average fail-over time is about 9.4ms (7.7% be-
low the reactive scheme with no cache), which is the time
taken to contact the Spread group, redirect the client con-
nection and retransmit the request to the new server.
5.2.4. Effect of varying threshold: For
the proac-
tive schemes, we analyzed the effect of varying the proac-
tive recovery threshold. Our results showed that if the
threshold is set too low, the overhead in the system in-
creases due to unnecessarily migrating clients. For example,
the group communication bandwidth between the servers
is about 6,000 bytes/sec at an 80% threshold, but this in-
creases to about 10,000 bytes/sec at a 20% threshold. The
increase in bandwidth happens because we are restart-
ing the servers more often at lower rejuvenation thresholds
and more bandwidth is used up in reaching group consen-
sus (See Figure 5). The best performance is achieved by
delaying proactive recovery so that the proactive depend-
ability framework has just enough time to redirect clients
away from the faulty server replica to a non-faulty server
replica in the system.
5.2.5. Jitter: In both the fault-free and the faulty (reac-
tive and proactive) schemes, we observed spikes that ex-
ceeded our average round-trip times by 3-σ. These outliers
occurred between 1-2.5% of the time. In the fault-free run,
the highest spike we observed was 2.3ms. (These spikes
might be due to ﬁle system journaling done by the oper-
ating system.) We also observed one large spike of about
30ms that occurred 0.01% of the time in the GIOP proac-
tive recovery schemes. This spike occurred when we set
the rejuvenation threshold below 80%. We suspect the spike
happens when a client sends a request to a newly restarted
server that is updating its group membership information.
The highest spike we observed with the MEAD proactive
messages was 6.9 ms at the 20% rejuvenation threshold.
)
c
e
s
/
s
e
t
y
b
(
i
t
h
d
w
d
n
a
B
12000
10000
8000
6000
4000
20
Effect of varying threshold
GIOP Location_Fwd
MEAD
40
60
Threshold (%)
80
Figure 5. Varying thresholds.
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:52:12 UTC from IEEE Xplore.  Restrictions apply. 
6. Conclusion
In this paper, we describe the development of a trans-
parent proactive recovery framework for CORBA applica-
tions, and show that proactive recovery can indeed provide
bounded temporal behavior in the presence of certain kinds
of faults, thereby enabling the development of real-time,
fault-tolerant distributed systems.
Our preliminary results show that the use of MEAD’s
proactive fail-over messages can yield a promising 73.9%
reduction in average fail-over times over a traditional reac-
tive recovery scheme. We incur a reasonable overhead of
about 3% over the normal client/server round-trip times.
When we use GIOP messaging schemes, the fail-over time
is about 13.5% lower, but we need to maintain additional
state at the server and incur an round-trip time overhead
of about 90%. If we attempt to suppress exceptions at the
client whenever we have insufﬁcient time to initiate proac-
tive recovery, our fail-over time is about 7.7% lower than in
the reactive case. However, we also observe a 25% client-
side failure rate. Since it is not possible to predict every sys-
tem failure, (or have enough time to recover from a fail-
ure, even if fault-prediction is possible), proactive recovery
schemes should be used to complement, but not replace, re-
active schemes.
We also show that if we trigger proactive recovery too
early, the additional overhead of migrating clients too fre-
quently can quickly negate the beneﬁts of proactive recov-
ery. The ideal scenario is to delay proactive recovery so that
the proactive dependability framework has just enough time
to redirect clients and objects away from the faulty server
replica to a non-faulty server replica in the system.
As part of our future work, we plan to extend our proac-
tive dependability framework to include more sophisticated
failure prediction. We also plan to integrate adaptive thresh-
olds into our framework rather than relying on preset thresh-
olds supplied by the user.
References
[1] Y. Amir, C. Danilov, and J. Stanton. A low latency, loss toler-
ant architecture and protocol for wide area group communi-
cation. In International Conference on Dependable Systems
and Networks, pages 327–336, New York, NY, June 2000.
[2] A. Bobbio and M. Sereno. Fine grained software rejuvena-
tion models. In Computer Performance and Dependability
Symposium, IPDS ’98, pages 4–12, September 1998.
[3] M. Castro and B. Liskov. Proactive recovery in a Byzantine-
fault-tolerant system. In Symposium on Operating Systems
Design and Implementation, October 2000.
[4] S. Garg, A. van Moorsel, K. Vaidyanathan, and K. Trivedi.
A methodology for detection and estimation of software ag-
ing. In International Symposium on Software Reliability En-
gineering, pages 283–292, November 1998.
[5] Y. Huang, C. Kintala, N. Kolettis, and N. Fulton. Software
rejuvenation: Analysis, module and applications. In Interna-
tional Symposium on Fault-Tolerant Computing, pages 381–
390, June 1995.
[6] J. R. Levine. Linkers and Loaders. Morgan Kaufmann Pub-
lishers, San Francisco, CA, 2000.
[7] T.-T. Y. Lin and D. P. Siewiorek. Error log analysis: Statis-
tical modeling and heuristic trend analysis. IEEE Transac-
tions on Reliability, 39(4):419–432, October 1990.
[8] C. Marchetti, L. Verde, and R. Baldoni. CORBA request
portable interceptors: A performance analysis.
In 3rd In-
ternational Symposium on Distributed Objects and Applica-
tions, pages 208–217, September 2001.
[9] P. Narasimhan. Trade-offs between real-time and fault-
tolerance for middleware applications. In Workshop on Foun-
dations of Middleware Technologies, Irvine, CA, November
2002.
[10] P. Narasimhan, T. Dumitras¸, S. Pertet, C. F. Reverte, J. Slem-
ber, and D. Srivastava. MEAD: Support for real-time fault-
tolerant CORBA. Concurrency and Computation: Practice
and Experience, Submitted 2003.
[11] Object Management Group. The CORBA/IIOP Speciﬁca-
tion Version 3.0.2. OMG Technical Committee Document
formal/2002-12-02, December 2002.
[12] Object Management Group. The CORBA Real-Time Speci-
ﬁcation Version 2.0. OMG Technical Committee Document
formal/2003-11-01, November 2003.
[13] G. Rubino. Predicting dependability properties online.
In
Symposium on Reliable Distributed Systems, pages 22–24,
October 1997.
[14] R. Ruggaber and J. Seitz. A transparent network handover
for nomadic CORBA users. In International Conference on
Distributed Computing Systems, pages 499–506, April 2001.
[15] F. Sultan, K. Srinivasan, D. Iyer, and L. Iftode. Migratory
TCP: Highly available Internet services using connection mi-
gration.
In 22nd International Conference on Distributed
Computing Systems, Vienna, Austria, July 2002.
[16] R. Vilalta and Ma Sheng. Predicting rare events in temporal
domain. In IEEE International Conference on Data Mining,
pages 474– 481, December 2002.
[17] B. White, J. Lepreau, L. Stoller, R. Ricci, S. Guruprasad,
M. Newbold, M. Hibler, C. Barb, and A. Joglekar. An in-
tegrated experimental environment for distributed systems
and networks. In Symposium on Operating Systems Design
and Implementation, pages 255–270, Boston, MA, Decem-
ber 2002.
[18] D. Wong and T. J. Lim. Soft handoffs in CDMA mobile sys-
tems.
In Personal Communications, IEEE [see also IEEE
Wireless Communications], Vol.4, Iss.6, pages 6–17, Decem-
ber 1997.
[19] J. Xu, Z. Kalbarczyk, and R. K. Iyer. Networked Windows
NT system ﬁeld failure data analysis. In IEEE Paciﬁc Rim In-
ternational Symposium on Dependable Computing, Decem-
ber 1999.
[20] B. Yujuan, S. Xiaobai, and K.S. Trivedi. Adaptive software
rejuvenation: Degradation model and rejuvenation scheme.
In International Conference on Dependable Systems and
Networks, pages 241–248, June 2003.
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:52:12 UTC from IEEE Xplore.  Restrictions apply.