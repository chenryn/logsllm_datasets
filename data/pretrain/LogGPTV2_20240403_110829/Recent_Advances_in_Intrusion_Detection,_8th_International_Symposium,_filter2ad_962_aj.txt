does not have a prior DNS translation, the destination IP is added into the
bucket for the current time interval and the packet is delayed. When a bucket is
ﬁlled with q distinct IPs, new connection requests are placed into the subsequent
bucket, thus each bucket cascades into the next one. Requests in the i-th bucket
are delayed until the beginning of the i+1 time interval. The n-th bucket, the last
in line, has no overﬂow bucket and once it is full, new TCP SYN packets without
DNS translations are simply dropped. At the end of the n*t time periods, we
reinstate another n buckets for the next n∗t time period. This algorithm permits
a maximum of q distinct IPs (without DNS translations) per time interval t and
packets (if not dropped) are delayed at most n ∗ t.
The notion of the buckets provides an abstraction, with which an administra-
tor could deﬁne rules such as “Permit 10 new ﬂows every 30 seconds dropping
anything over 120 seconds.” This example rule, then, would translate to 4 buck-
ets (30 seconds * 4 = 2 minutes) with q = 10 and t = 30. Expressing rate limiting
rules in this manner is more intuitive and easier than attempting to characterize
network traﬃc in terms of working sets or the failure rate of connections.
This scheme can be implemented at the host level or at the edge router of a
network. A host-level implementation requires keeping DNS-related statistics on
each host. Edge-router-based implementation would require the border router
to keep a shadow DNS cache for the entire network.
In our study, we tested DNS RL both at the host level and at the edge,
using DNS server cache information and all DNS traﬃc recorded at the network
border. More speciﬁcally, we mirrored the DNS cache (and all TTLs) at the
edge and updated the cache as new DNS queries/replies are recorded. Traﬃc to
destination addresses with an unexpired DNS record is permitted through, while
all others are delayed.
8.1 Analysis
The critical parameter for the cascading-bucket scheme is the rate limit, which
manifests in the values of q (the size of each bucket), t (the time interval), and
Empirical Analysis of Rate Limiting Mechanisms
35
False Positives for End Host DNS RL w/ varying distinct IPs per 5 secs
False Negative for End Host DNS RL w/ varying distinct IPs per 5 secs
 3.5
 3
 2.5
 2
 1.5
 1
 0.5
3 Distinct IPs/5secs
5 Distinct IPs/5secs
7 Distinct IPs/5secs
10 Distinct IPs/5secs
)
%
(
e
v
i
t
a
g
e
N
e
s
a
F
l
3 Distinct IPs/5secs
5 Distinct IPs/5secs
7 Distinct IPs/5secs
10 Distinct IPs/5secs
 3.5
 3
 2.5
 2
 1.5
 1
 0.5
)
%
(
e
v
i
t
i
s
o
P
e
s
a
F
l
 0
 0
 5
 10
Days
 15
 20
 0
 0
 5
 10
Days
 15
 20
(a) FP for DNS-based RL (Infected
Clients)
(b) FN for DNS-based RL (Infected
Clients)
False Positives for Normal Traffic using End Host DNS RL mechanism
Traffic Patterns for End Host DNS RL mechanism
3 Distinct IP / 5 sec
5 Distinct IP / 5 sec
7 Distinct IP / 5 sec
10 Distinct IP / 5 sec
 3.5
 3
 2.5
 2
 1.5
 1
 0.5
)
%
(
e
v
i
t
i
s
o
P
e
s
a
F
l
 0
 0
 5
 10
Days
 15
 20
(c) FP for DNS-based RL (Normal
Clients)
s
w
o
F
l
 1.2e+07
 1e+07
 8e+06
 6e+06
 4e+06
 2e+06
 0
 0
Total Non-worm traffic
Total worm traffic
Delayed worm flows
Dropped worm flows
 5
 10
 15
 20
Days
(d) Flows Dropped / Delayed
Fig. 8. Results for DNS-based End Host RL
n (number of buckets). To simplify our analysis, we varied the value of q and
kept n and t constant1 Additionally, the value of n ∗ t was set to 120 seconds
to model the TCP timeout period. This scheme allows a certain number of
untranslated IP connections to exit the network, which intends to accommodate
legitimate direct-IP connections. In our data set, we observed some direct server-
server communication and direct-IP connections due to peer-to-peer, streaming
audio and passive FTP traﬃc. These were the main cause of false positives
observed. One can attempt to maintain a white list to allow legitimate direct-IP
connections and thus further reduce false positives. However, as observed in [22],
a comprehensive white list for an open network may not be feasible.
We ﬁrst analyze the host-level DNS throttling scheme. For this, we maintain a
set of cascading buckets for each host. Figure 8(a) and (b) show the false positive
and false negative rates for infected hosts. The data in these graphs are daily
error rates averaged over all infected hosts. Figure 8(c) plots the analogous false
positive rates for normal hosts. In addition, Table 5 presents the delay statistics
for a normal host and Table 6 shows the worst case delay statistics for an infected
host.
1 By varying q and leaving n and t constant, we can achieve the goal of regulating the
rate limits.
36
C. Wong et al.
Table 5. DNS RL delay statistics for a normal host (3-hour period)
Delay Amount.
No delay
1 - 10 sec.
> 10 sec.
Total number of benign ﬂows 2144
# of Flows
2136
8
0
These results yield a number of observations: First, host-level DNS throttling
signiﬁcantly outperforms the other mechanisms analyzed previously. As seen in
Figure 8, the average false positive rates fall in the range of 0.1% to 1.7% with
corresponding false negative rates between 0.1% to 3.2%, both signiﬁcantly lower
than the error statistics of the others. We also observed that applications that do
experience false positives here tend to be those that fall outside of the security
policies of an enterprise network (e.g., peer-to-peer applications)—disruption of
such applications are generally considered not critical to the network operation.
Table 5 shows the delay statistics for a normal host. As shown, DNS RL de-
layed 8 total ﬂows for this host, as opposed to the 385 ﬂows using Williamson’s
(Table 2 in Section 5). Also note that all the delays in Table 5 are less than 10
seconds, which are not signiﬁcant. Table 6 shows the worst case delay statistics
for an infected host during the peak of its infection period. The statistics show
that DNS RL dropped approximately 17% of the host’s benign traﬃc, compared
to over 90% when using Williamson’s. In addition, DNS RL delays less ﬂows for
normal hosts than Williamson’s. Also note in Table 6, nearly delayed malicious
ﬂows are subjected to the maximum allowed delay and over 95% of the malicious
ﬂows are dropped.
During the outbreak period, the false positives for infected hosts included
both dropped and delayed traﬃc ﬂows. A q value of 5 would drop approximately
0.075% and delay 0.375% of the legitimate traﬃc. Figure 8(d) shows summa-
rized statistics from our analysis for a liberal value of q = 10. During Blaster’s
outbreak, on average 97% of the worm traﬃc was rate limited— approximately
82% dropped and the other 18% delayed with an average delay of one minute
each.
Table 6. DNS RL Delay Statistics for an infected host during a 3-hour period
Delay Amount. Benign Malicious
No delay
1 - 30 sec.
31 - 60 sec.
61 - 100 sec.
> 100 sec.
Dropped
Total
1
34
35
40
4903
112862
11785
806
4
2
12
11
172
1007
Empirical Analysis of Rate Limiting Mechanisms
37
False Positives for Edge Router DNS RL
False Negatives for Edge Router DNS RL
 50
 40
 30
 20
 10
20 Distinct IPs / 5sec
50 Distinct IPs / 5sec
100 Distinct IPs / 5sec
)
%
(
e
v
i
t
a
g
e
N
e
s
a
F
l
 0
 0
 5
 10
Days
 15
 20
(a) FP for DNS-based RL
 50
 45
 40
 35
 30
 25
 20
 15
 10
 5
 0
20 Distinct IPs / 5sec
50 Distinct IPs / 5sec
100 Distinct IPs / 5sec
 0
 5
 10
Days
 15
 20
(b)FN for DNS-based RL
)
%
(
e
v
i