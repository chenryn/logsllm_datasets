power and hardened) to increase reliability and protect the
tasks running on the white core is beyond the scope of this
paper.
Our fault model includes two scenarios: (a) an unreliable
execution changes the output of instructions, and (b) the
unreliable execution corrupts the instructions themselves.
In the ﬁrst scenario, we ﬂip bits in the input arguments
in software before moving the arguments from the white
to core to a green core. This emulates a data corruption
and are silent unless a voting mechanism is enabled. The
second scenario is more complicated because such fault can
be benign, masked by the hardware protection, or crashes
the task. Since the green cores do not have MMUs, we
assume that only non-benign functions cause the program
to crash. If the task terminates (with incorrect results), the
runtime system can detect it. However, if the task does
not terminate (the corruption leads to an endless loop), the
current prototype does not have the ability to recovery from
this sub-type of fault. This can be addressed with watchdog
timers which will be incorporated in the future.
When a fault is detected, the individual core is reset to
its initial power-on state and the Python executable reloaded
before another task is scheduled.
C. Results
The goal of the proposed runtime is to add system
resilience even if the underlying hardware is unreliable. To
demonstrate this, we explore the resilience characteristics to
see how choices for redundancy impact the time to correct
solution of each benchmark.
PyDac offers the ability to implement resilient compu-
tation of tasks scheduled on the green cores. This is user-
deﬁned and entirely transparent, meaning that the application
does not need to be modiﬁed in any way to compute
resiliently. This greatly reduces the programmer overhead
for developing resilient applications and, in particular on
lower reliability hardware, can be beneﬁcial.
This is implemented in PyDac with two different degrees
of Redundant Multi-Threading (RMT): Dual Modular Re-
dundancy (DMR) or Triple Modular Redundancy (TMR).
TMR is perhaps the more familiar concept where tasks are
triplicated and processed independently by different green
cores. Results are compared by the white core and voted
on such that any two that produce the same answer “win”.
In the extremely rare case when none of the three results
are the same the entire set of three tasks are recomputed
(rolled-back) and tried again.
DMR is a simpler form of the above voting technique
where each task is duplicated and voted on. In this case, if
the voting doesn’t match then both are recomputed. DMR
generally performs better in situations where systems are
unreliable but not terribly so. TMR works well in systems
with real-time sensitivities (such as where a roll-back might
Strassen
Block Matrix
Merge Sort
Closest Pair
K-means
e
m
i
t
n
u
r
d
e
z
i
l
a
m
r
o
N
 4
 3.5
 3
 2.5
 2
 1.5
 1
 0.5
 0
B
D
D
D
T
T
T
a
s
e
-li
n
R
R
R
-
0
-
1
-
e
D
M
M
M
M
M
M
-
1
-
L
R
R
R
-
0
-
1
-
D
-
1
-
L
Type of RMT
Figure 2. Normalized runtime on green-white architecture. All six green
cores are utilized. Base-line are tests without RMT support. DMR-0 and
TMR-0 are tests that enable RMT in a fault-free environment. DMR-1-D
and TMR-1-D are tests that enable RMT while the fault injector injects
a single soft error to data. DMR-1-L and TMR-1-L are tests that enable
RMT while faults on logic are emulated.
cause too much delay in a result) as well as systems that
are highly unreliable.
To demonstrate this capability in PyDac, we executed
the micro-benchmarks in DMR, TMR, and without any
redundancy. These results are presented in Figure 2. Then,
we injected a single bit ﬂip fault
into each application
during a DMR and TMR run. In Figure 2, DMR-0 and
TMR-0 indicate runs without any faults and demonstrate
the overhead imposed by this feature in PyDac. The DMR-
1 and TMR-1 bars depict the runtime when a single fault was
injected. It is important to realize that in all of these runs
the micro-benchmarks produced the correct answer; even in
the presence of a soft error.
As can be seen from Figure 2, the overhead imposed by
this technique is largely dependent on the ratios between
white and green core computation time. While clearly this
technique has performance implications as it creates 2×
(DMR) and 3× (TMR) the number of green tasks,
the
inclusion of it in PyDac eases its adoption for programmers.
For Strassen’s algorithm, there were over 2, 400 green core
tasks created in the base-line approach. Clearly this number
grows to 4, 800 and 7, 200 in the cases of DMR and
TMR. One would expect considerably more overhead for
scheduling these tasks on only 6 green cores but PyDac
handles all of this easily and with low overhead. As we
continue to scale to larger FPGAs, we expect to be able
to deploy more green cores and this large number of tasks
would be more easily processed.
Notice in Figure 2 that the Closest Pair algorithm does
not incur much overhead from redundancy. This is because,
this algorithm runs almost entirely on the white cores. Its
sequential execution takes 97.0% of total runtime. As such,
there is very little overhead imposed by redundant green
747
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:27:37 UTC from IEEE Xplore.  Restrictions apply. 
tasks.
One thing to consider is that with the ease of imple-
menting resilient computation under PyDac one could target
hardware that is considerably less reliable than conventional
architectures. These less reliable architectures often come
with major improvements in power and/or performance.
This approach focuses on time to completion rather than
instructions per second. Furthermore, PyDac could be a lot
more sophisticated about its use of RMT if it had access
to probing information about the expected fault rates of
the machine. For instance, it might run in base-line mode
entirely and only start implementing RMT on some of the
cores if they identiﬁed hardware problems. Implementing
this kind of approach outside of a task-based programming
model is costly and complex for a programmer and PyDac
supplies this through a simple switch.
IV. RELATED WORK
The recent resurgence of interest in modular redundancy
is driven by reducing resilience overhead and covering a
broader class of faults such as SDC. Fiala et. al [10] show the
feasibility and effectiveness of SDC detection and correction
by redundancy at the MPI layer. As future HPC systems are
expected to show deeper hierarchy and heterogeneity, we
take a different approach to implement redundancy. Specif-
ically, we envision a heterogeneous many-core architecture
called green-white architecture that is comprised of cores
exhibiting different level of reliabilities. Our task parallel
programming model as well as runtime system then harness
the green cores by running only less critical tasks on these
cores. The runtime system run multiple copies of identical
tasks on the green cores to ensure the correctness of results.
Sequoia [14] introduce new language constructs (e.g.
mapreduce) to support map-reduce patterns. Sequoia ab-
stracts a memory hierarchy as a tree of distributed memory
modules and constrains the compute kernels to operate on
leaf nodes. To be portable across levels of memory hierarchy,
task variants are generated by the compiler statically. In
contract
levels
of memory hierarchy, PyDac focuses on a ﬂatter memory
hierarchy and generates tasks that speciﬁcally run on the
scratch-pad memory.
to using a task variant
to suit different
Containment domains (CDs) [15] bears a similarity to
Sequoia in that CDs abstracts a machine with deep memory
hierarchy as trees of distributed memory modules. An ap-
plication is decomposed and represented by a tree of tasks.
Regarding resilience, CDs provides APIs to programmers to
express resilience concerns explicitly. If a fault is detected,
an inner CD may roll back to a checkpoint and re-execute, or
it can escalate the error to the parent CD. Our programming
model is similar to CDs in that we abstracts an application
as a tree of tasks. However, in our current implementation,
recovery is always performed locally if a task is detected
faulty. We also don’t provide explicit APIs to programmers.
748
Instead, the runtime system provide transparent redundancy
to programmers for less programming effort and better
portability.
Intel Thread Building Blocks (TBB) [16] is a C++ tem-
plate library that provides control on low-level parallelism
based on its work-stealing scheduler. It supports many
popular design patterns such as pipeline and divide-and-
conquer. It abstracts away the complexity of using native
threading packages such as Pthreads. It only aims at shared
memory architecture. PyDac targets not only shared memory
but also a novel memory subsystem that is much ﬂatter.
Intel Concurrent Collections (CnC) [17] is a programming
model that provides higher level abstraction than TBB. In
fact, CnC could be built on top of TBB as an approach to
leverage shared memory computers. CnC imposes several
important rules on programmers. For example, computation
may not reference any global values. Data is referenced
by value instead of by location. Similarly, PyDac requires
a base case to be referential transparent. Each green core
may reference data by its value instead of by location. This
feature ensures that a faulty task will never corrupt the
memory of other tasks and allows PyDac runtime to reissue
a task if a fault is detected.
V. CONCLUSION
As the momentum toward energy-efﬁcient chip architec-
ture continues, we envision a chip architecture leveraging
processing units that consume low power but are more
error-prone. We proposed a task-based programming model
that allows programmers to partition their applications into
subtasks and then allow the runtime system to schedule
these tasks based on resilience requirements. Naturally re-
silient tasks are good ﬁts for the less-reliable but more
energy-efﬁcient green cores. For tasks that are more vul-
nerable to soft errors, however, PyDac provides redundant
multi-threading of tasks with voting to still ensure reliable
computation at a modest performance penalty. PyDac is
implemented with Python and easily adoptable by domain
scientists already programming in this language.
We demonstrated the use of PyDac on a prototype hard-
ware consisting of one PowerPC core and six ARM cores.
We injected faults into micro-benchmarks and showed that
through the use of double and triple modular redundancy
we were still able to complete the calculation and get the
correct answer.
In future work we are exploring more mature processors
than the ARMv2a as well as larger FPGAs that facilitate
as many as 128 green cores. Issues related to endianness
currently present a problem with the PowerPC and ARM
combination as PyDac must perform dynamic translation.
This presents a performance impact that we hope to rectify in
the future. We are exploring implementing dials and probes
on the green cores that allow us to inject hardware faults
and report the presence of these faults to PyDac. With this
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:27:37 UTC from IEEE Xplore.  Restrictions apply. 
[11] B. Huang, R. Sass, N. DeBardeleben, and S. Blanchard, “Py-
dac: A resilient run-time framework for divide-and-conquer
applications on a heterogeneous many-core architecture,” in
The 6th Workshop on UnConventional High Performance
Computing, UCHPC at Euro-Par’13, 2013.
[12] T. Oliphant, A Guide to NumPy. Trelgol Publishing USA,
2006, vol. 1.
[13] “Amber: Arm-compatible core.” [Online]. Available: http:
//opencores.org/project,amber
[14] K. Fatahalian, D. R. Horn, T. J. Knight, L. Leem, M. Houston,
J. Y. Park, M. Erez, M. Ren, A. Aiken, W. J. Dally et al.,
“Sequoia: programming the memory hierarchy,” in Proceed-
ings of the 2006 ACM/IEEE conference on Supercomputing.
ACM, 2006, p. 83.
[15] J. Chung, I. Lee, M. Sullivan, J. H. Ryoo, D. W. Kim, D. H.
Yoon, L. Kaplan, and M. Erez, “Containment domains: A
scalable, efﬁcient and ﬂexible resilience scheme for exascale
systems,” Scientiﬁc Programming, vol. 21, no. 3, pp. 197–
212, 2013.
[16] “Intel
thread building blocks.” [Online]. Available: https:
//www.threadingbuildingblocks.org/
[17] Z. Budimli´c, M. Burke, V. Cav´e, K. Knobe, G. Lowney,
R. Newton, J. Palsberg, D. Peixotto, V. Sarkar, F. Schlim-
bach et al., “Concurrent collections,” Scientiﬁc Programming,
vol. 18, no. 3, pp. 203–217, 2010.
feature we would be able to dynamically adapt the resilience
of the tasks and even only have targeted redundancy. Finally,
we are looking at using idle cycles on the white core
to replicate work being performed on green cores. This
approach provides more resiliency while also allowing the
application to “catch up” when faults are observed in real
time constrained applications.
VI. ACKNOWLEDGEMENT
We thank Brian Woods for his review and feedback.
REFERENCES
[1] H. Esmaeilzadeh, E. Blem, R. S. Amant, K. Sankaralingam,
and D. Burger, “Dark silicon and the end of multicore
scaling,” in Intl. Symp. on Computer Architecture (ISCA).
IEEE, 2011, pp. 365–376.
[2] S. Borkar, “The exascale challenge,” in 2010 International
Symposium on VLSI Design Automation and Test (VLSI-DAT),
2010.
[3] M. D. Hill and M. R. Marty, “Amdahl’s law in the multicore
era,” Computer, vol. 41, no. 7, pp. 33–38, Jul. 2008.
[4] G. Venkatesh,
J. Sampson, N. Goulding, S. Garcia,
V. Bryksin, J. Lugo-Martinez, S. Swanson, and M. B. Taylor,
“Conservation cores: reducing the energy of mature com-
putations,” in ACM SIGARCH Computer Architecture News,
vol. 38, no. 1. ACM, 2010, pp. 205–218.
[5] J. Howard, S. Dighe, Y. Hoskote, S. Vangal, D. Finan,
G. Ruhl, D. Jenkins, H. Wilson, N. Borkar, G. Schrom et al.,
“A 48-core IA-32 message-passing processor with DVFS in
45nm CMOS,” in IEEE International Solid-State Circuits
Conference (ISSCC).
IEEE, 2010, pp. 108–109.
[6] H. Kaul, M. Anders, S. Hsu, A. Agarwal, R. Krishnamurthy,
and S. Borkar, “Near-threshold voltage (NTV) design: oppor-
tunities and challenges,” in Proceedings of the 49th Annual
Design Automation Conference. ACM, 2012, pp. 1153–1158.
[7] N. Seifert, P. Slankard, M. Kirsch, B. Narasimham, V. Zia,
C. Brookreson, A. Vo, S. Mitra, B. Gill, and J. Maiz,
“Radiation-induced soft error rates of advanced cmos bulk
devices,” in IEEE Reliability Physics Symposium.
IEEE,
2006, pp. 217–225.
[8] B. Schroeder and G. A. Gibson, “Understanding failures
in petascale computers,” in Journal of Physics: Conference
Series, vol. 78, no. 1.
IOP Publishing, 2007, p. 012022.
[9] K. Ferreira, J. Stearley, J. H. Laros III, R. Oldﬁeld, K. Pe-
dretti, R. Brightwell, R. Riesen, P. G. Bridges, and D. Arnold,
“Evaluating the viability of process replication reliability
for exascale systems,” in International Conference for High
Performance Computing, Networking, Storage and Analysis.
ACM, 2011, p. 44.
[10] D. Fiala, F. Mueller, C. Engelmann, R. Riesen, K. Ferreira,
and R. Brightwell, “Detection and correction of silent data
corruption for large-scale high-performance computing,” in
Proceedings of the International Conference on High Perfor-
mance Computing, Networking, Storage and Analysis.
IEEE
Computer Society Press, 2012, p. 78.
749
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:27:37 UTC from IEEE Xplore.  Restrictions apply.