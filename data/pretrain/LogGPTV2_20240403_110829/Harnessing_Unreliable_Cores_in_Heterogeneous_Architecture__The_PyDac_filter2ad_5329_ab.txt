### Power and Hardening for Reliability

To enhance reliability and protect tasks running on the white core, power and hardening techniques are employed. The detailed discussion of these techniques is beyond the scope of this paper.

### Fault Model

Our fault model encompasses two primary scenarios:
1. **Unreliable Execution Affecting Output**: In this scenario, an unreliable execution alters the output of instructions. We simulate data corruption by flipping bits in the input arguments in software before moving them from the white core to a green core. These faults remain silent unless a voting mechanism is enabled.
2. **Unreliable Execution Corrupting Instructions**: This scenario is more complex as the fault can be benign, masked by hardware protection, or cause the task to crash. Since green cores lack MMUs, we assume that only non-benign functions lead to program crashes. If a task terminates with incorrect results, the runtime system can detect it. However, if the task enters an endless loop, the current prototype cannot recover from this fault. Future implementations will include watchdog timers to address this issue.

### Fault Detection and Recovery

When a fault is detected, the affected core is reset to its initial power-on state, and the Python executable is reloaded before scheduling another task.

### Results

The proposed runtime aims to add system resilience even when the underlying hardware is unreliable. To demonstrate this, we explored the resilience characteristics and the impact of redundancy choices on the time to correct solution for each benchmark.

**PyDac** offers the ability to implement resilient computation of tasks scheduled on the green cores. This is user-defined and transparent, meaning the application does not need to be modified to compute resiliently. This significantly reduces programmer overhead, especially on lower-reliability hardware.

### Redundant Multi-Threading (RMT)

PyDac implements two degrees of RMT:
- **Dual Modular Redundancy (DMR)**: Each task is duplicated, and the results are voted on. If the results do not match, both tasks are recomputed. DMR is suitable for systems that are unreliable but not extremely so.
- **Triple Modular Redundancy (TMR)**: Tasks are triplicated and processed independently by different green cores. The white core compares the results, and any two matching results "win." In the rare case where all three results differ, the tasks are recomputed. TMR is effective in systems with real-time sensitivities or high unreliability.

### Experimental Evaluation

To demonstrate PyDac's capability, we executed micro-benchmarks using DMR, TMR, and without any redundancy. The results are presented in Figure 2. We also injected a single bit flip fault into each application during DMR and TMR runs. The DMR-0 and TMR-0 bars show the overhead imposed by RMT in a fault-free environment, while DMR-1 and TMR-1 depict the runtime with a single fault. All micro-benchmarks produced the correct answer, even in the presence of a soft error.

### Overhead Analysis

The overhead imposed by RMT is largely dependent on the ratio between white and green core computation times. While RMT increases the number of green tasks (2x for DMR and 3x for TMR), PyDac handles this with low overhead. For example, Strassen's algorithm creates over 2,400 green core tasks in the baseline approach, which grows to 4,800 and 7,200 in DMR and TMR, respectively. As we scale to larger FPGAs, more green cores can be deployed, making the large number of tasks easier to process.

### Case Study: Closest Pair Algorithm

The Closest Pair algorithm incurs minimal overhead from redundancy because it runs almost entirely on the white cores, with sequential execution taking 97.0% of the total runtime. Thus, there is very little overhead from redundant green tasks.

### Future Work

With the ease of implementing resilient computation under PyDac, one could target hardware that is less reliable but offers significant improvements in power and performance. PyDac could be more sophisticated in its use of RMT if it had access to probing information about expected fault rates. For instance, it might run in baseline mode and only implement RMT on cores that exhibit hardware problems.

### Related Work

Recent interest in modular redundancy aims to reduce resilience overhead and cover a broader class of faults such as Silent Data Corruption (SDC). Fiala et al. [10] show the feasibility and effectiveness of SDC detection and correction through redundancy at the MPI layer. Our approach, however, focuses on a heterogeneous many-core architecture called the green-white architecture, where cores exhibit different levels of reliability. PyDac harnesses green cores by running less critical tasks on them and uses RMT to ensure correctness.

### Conclusion

As the trend toward energy-efficient chip architectures continues, we envision a chip design that leverages low-power, error-prone processing units. PyDac provides a task-based programming model that allows partitioning applications into subtasks and scheduling them based on resilience requirements. Naturally resilient tasks are suitable for green cores, while more vulnerable tasks benefit from RMT. PyDac is implemented in Python and is easily adoptable by domain scientists.

### Future Directions

We are exploring more mature processors and larger FPGAs that facilitate up to 128 green cores. We are also addressing issues related to endianness and implementing dials and probes on green cores to dynamically adapt task resilience and provide targeted redundancy. Additionally, we are considering using idle cycles on the white core to replicate work on green cores, enhancing resiliency and allowing the application to "catch up" in real-time constrained scenarios.

### Acknowledgements

We thank Brian Woods for his review and feedback.

### References

[1] H. Esmaeilzadeh, E. Blem, R. S. Amant, K. Sankaralingam, and D. Burger, “Dark silicon and the end of multicore scaling,” in Intl. Symp. on Computer Architecture (ISCA). IEEE, 2011, pp. 365–376.
[2] S. Borkar, “The exascale challenge,” in 2010 International Symposium on VLSI Design Automation and Test (VLSI-DAT), 2010.
[3] M. D. Hill and M. R. Marty, “Amdahl’s law in the multicore era,” Computer, vol. 41, no. 7, pp. 33–38, Jul. 2008.
[4] G. Venkatesh, J. Sampson, N. Goulding, S. Garcia, V. Bryksin, J. Lugo-Martinez, S. Swanson, and M. B. Taylor, “Conservation cores: reducing the energy of mature computations,” in ACM SIGARCH Computer Architecture News, vol. 38, no. 1. ACM, 2010, pp. 205–218.
[5] J. Howard, S. Dighe, Y. Hoskote, S. Vangal, D. Finan, G. Ruhl, D. Jenkins, H. Wilson, N. Borkar, G. Schrom et al., “A 48-core IA-32 message-passing processor with DVFS in 45nm CMOS,” in IEEE International Solid-State Circuits Conference (ISSCC). IEEE, 2010, pp. 108–109.
[6] H. Kaul, M. Anders, S. Hsu, A. Agarwal, R. Krishnamurthy, and S. Borkar, “Near-threshold voltage (NTV) design: opportunities and challenges,” in Proceedings of the 49th Annual Design Automation Conference. ACM, 2012, pp. 1153–1158.
[7] N. Seifert, P. Slankard, M. Kirsch, B. Narasimham, V. Zia, C. Brookreson, A. Vo, S. Mitra, B. Gill, and J. Maiz, “Radiation-induced soft error rates of advanced cmos bulk devices,” in IEEE Reliability Physics Symposium. IEEE, 2006, pp. 217–225.
[8] B. Schroeder and G. A. Gibson, “Understanding failures in petascale computers,” in Journal of Physics: Conference Series, vol. 78, no. 1. IOP Publishing, 2007, p. 012022.
[9] K. Ferreira, J. Stearley, J. H. Laros III, R. Oldfield, K. Pedretti, R. Brightwell, R. Riesen, P. G. Bridges, and D. Arnold, “Evaluating the viability of process replication reliability for exascale systems,” in International Conference for High Performance Computing, Networking, Storage and Analysis. ACM, 2011, p. 44.
[10] D. Fiala, F. Mueller, C. Engelmann, R. Riesen, K. Ferreira, and R. Brightwell, “Detection and correction of silent data corruption for large-scale high-performance computing,” in Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis. IEEE Computer Society Press, 2012, p. 78.