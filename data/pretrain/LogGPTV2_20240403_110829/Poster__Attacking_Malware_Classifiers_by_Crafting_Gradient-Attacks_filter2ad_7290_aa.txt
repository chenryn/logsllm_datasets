title:Poster: Attacking Malware Classifiers by Crafting Gradient-Attacks
that Preserve Functionality
author:Raphael Labaca Castro and
Battista Biggio and
Gabi Dreo Rodosek
Poster: Attacking Malware Classifiers by Crafting
Gradient-Attacks that Preserve Functionality
Raphael Labaca-Castro
Research Institute CODE
Bundeswehr University Munich
Munich, Germany
PI:EMAIL
Battista Biggio
Dept. of Electrical and Electronic
Engineering
University of Cagliari
Cagliari, Italy
PI:EMAIL
Gabi Dreo Rodosek
Research Institute CODE
Bundeswehr University Munich
Munich, Germany
PI:EMAIL
ABSTRACT
Machine learning has proved to be a promising technology to deter-
mine whether a piece of software is malicious or benign. However,
the accuracy of this approach comes sometimes at the expense of its
robustness and probing these systems against adversarial examples
is not always a priority. In this work, we present a gradient-based
approach that can carefully generate valid executable malicious
files that are classified as benign by state-of-the-art detectors. Initial
results demonstrate that our approach is able to automatically find
optimal adversarial examples in a more efficient way, which can
provide a good support for building more robust models in the
future.
CCS CONCEPTS
• Security and privacy → Malware and its mitigation; Soft-
ware reverse engineering; • Computing methodologies → Neu-
ral networks.
ACM Reference Format:
Raphael Labaca-Castro, Battista Biggio, and Gabi Dreo Rodosek. 2019. Poster:
Attacking Malware Classifiers by Crafting Gradient-Attacks that Preserve
Functionality. In 2019 ACM SIGSAC Conference on Computer and Communi-
cations Security (CCS ’19), November 11–15, 2019, London, United Kingdom.
ACM, New York, NY, USA, 3 pages. https://doi.org/10.1145/3319535.3363257
1 INTRODUCTION
Deep neural networks have been widely implemented for malware
classification [4, 7, 10]. Yet, problems still persist when they are
confronted to adversarial examples. These carefully crafted per-
turbations injected into malicious software can make the classifier
label malware as actual benign software.
Even though most of the literature has been focusing in the
image domain, adversarial learning for malware evasion is also a
promising area of research with recent contributions in the litera-
ture [1, 3]. Nevertheless, there are important differences between
these two domains, among others, the fact that datasets are more
accessible in the image domain (e.g.; MNIST, ImageNet) and the
nature of the input file. In the image domain files can be easily ma-
nipulated with perturbations that are often invisible to the human
eye without causing major issues to the structure of the files. On
the other hand, the malware domain is more complex to work with
when generating adversarial examples. The lower entropy of the
data, although encryption and compression often return higher
entropy scores, and the fact that many values can be binary instead
of continue real numbers, such as API calls, can affect the ability
to generate valid adversarial examples. The complexity can vary
greatly and depends on the structure of the input file. However,
evading the classifier is not only the important point since modi-
fying the structure of malicious files plays a big role. Hence, the
perturbations need to be carefully created in order to preserve the
functionality of each malware sample.
In this work, we present how to build evasive examples against
static state-of-the-art classifiers. We implemented a convolutional
neural network (CNN) to perform the malware classification [8].
The model was trained using the EMBER dataset, which consists of
2351 features representations of more than one million portable ex-
ecutable (PE) files. Among them 300.000 malicious, 300.000 benign,
300.000 unlabeled, and 200.000 test samples [2].
In order to generate the adversarial examples we implemented
an internal module, which is able to take a PE malicious file as input
and perform the injection of a sequence of perturbations in order to
achieve an evasive malware [6]. In this case, instead of relying on
finding the optimal random sequence of perturbations to achieve
misclassification, we analyze the value of the gradient for every
perturbation injected and use it to maximize the evasion rate of
the adversarial example generated. In this way, our approach is
able to conveniently identify which of the transformations return a
better score and, hence, generates faster adversarial examples for
any given malicious file provided as input.
Furthermore, we intend to analyze the cross-evasion results of
our optimal adversarial examples by testing them against state-
of-the-art classifiers of the literature [2]. This demonstrates how
successful it is to bring results from one classifier to another without
the need to create tailored adversarial examples for each classifier
targeted.
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
CCS ’19, November 11–15, 2019, London, United Kingdom
© 2019 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-6747-9/19/11.
https://doi.org/10.1145/3319535.3363257
2 RELATED WORK
Biggio et al. [3] proposed a simple and effective gradient approach
that is able to evade PDF file classifiers. Given that PDF files have
a flexible logical structure, they are a good option for adversaries
to use as infection vector by adding malicious routines. Several
approaches with different levels of knowledge were evaluated and
PosterCCS ’19, November 11–15, 2019, London, United Kingdom2565one of the takeaways is that the false negative rate increases the
more the PDF will be modified. Support Vector Machines (SVM) and
neural networks were evaluated but the approach can be applicable
to any other model with differentiable functions.
In [1] the authors proposed the use of a reinforcement learning
(RL) agent to find the best perturbations applied on PE files. It is
reported that approximately a fourth of the files achieved evasion.
In addition, retraining the malware detector using the adversarial
examples can reduce effectiveness of the attack by 33% according
to the authors. However, they acknowledge that these results can
depend on the model and dataset used. Furthermore, even though
the perturbations applied are meant to be preserving-functionality
transformations they do not provide an additional mechanism to
make sure the generated output files are valid.
In AIMED [5] we proposed the use of genetic programming (GP)
to find optimal sequences of perturbations that will be injected
into the malicious file in order to generate adversarial examples.
As an optimization strategy, GP seems to converge much faster
than classic stochastic approaches and the number of corrupt files
generated is drastically reduced. Furthermore, an extra control is
added in order to ensure the functionality of the evasive files that
were automatically created. However, this approach can lead to local
minima and maxima, which can prevent finding a suitable vector
of perturbation for some of the input samples. Another important
point is that unlike PDFs, by working with PE files, the false negative
rate of the queried model does not seem to necessarily increase the
more perturbations the file receive. What in fact increases is the
likelihood of a corrupt file despite of its detection rate. Thus, even
working in the same domain, having different file structures, such
as PDF and PE, forces to further adjust the evasion strategy.
3 METHODOLOGY
CNNs are a particular type of feed-forward neural networks where
the pattern of connectivity used with its neurons resembles the
biological visual cortex. The architecture consist of three types
of layers: convolutional, pooling, and fully-connected. Unlike reg-
ular neural networks, convolutional networks do not have only
fully-connected layers. The convolutional layer will compute a dot
product between the weights and a part of the input. Then, the
information from the input will be passed along the pooling layer
where it will be downsampled and, finally, the fully-connected layer
will calculate the scores [9]. The CNN is trained on the EMBER
dataset using 1.1 million instances and 2351 features. There are