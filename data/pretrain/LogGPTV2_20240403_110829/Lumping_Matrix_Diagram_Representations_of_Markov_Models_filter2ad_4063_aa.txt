title:Lumping Matrix Diagram Representations of Markov Models
author:Salem Derisavi and
Peter Kemper and
William H. Sanders
Lumping Matrix Diagram Representations of Markov Models ∗
Salem Derisavi, Peter Kemper, William H. Sanders
Coordinated Science Laboratory
University of Illinois at Urbana-Champaign, IL, U.S.A.
Email: {derisavi, whs}@crhc.uiuc.edu
Informatik IV, Universit¨at Dortmund
D-44221 Dortmund, Germany
Email: PI:EMAIL
Abstract
Continuous-time Markov chains (CTMCs) have been
used successfully to model the dependability and performa-
bility of many systems. Matrix diagrams (MDs) are known
to be a space-efﬁcient, symbolic representation of large
CTMCs. In this paper, we identify local conditions for ex-
act and ordinary lumpings that allow us to lump MD rep-
resentations of Markov models in a compositional manner.
We propose a lumping algorithm for CTMCs that are rep-
resented as MDs that is based on partition reﬁnement, is
applied to each level of an MD directly, and results in an
MD representation of the lumped CTMC. Our composi-
tional lumping approach is complementary to other known
model-level lumping approaches for matrix diagrams. The
approach has been implemented, and we demonstrate its
efﬁciency and beneﬁts by evaluating an example model of
a tandem multi-processor system with load balancing and
failure and repair operations.
1 Introduction
Model-based evaluation of computer and communica-
tion systems is often done through simulation.
In many
cases, the desired results could, in principle, also be de-
rived through analysis of the underlying CTMCs; however,
in practice, the size of the systems of equations that would
need to be solved is prohibitive. This “largeness problem”
has motivated much research in the construction and numer-
ical solution of CTMCs. State-space lumping (e.g., [2, 13])
is a well-known approach that reduces the size of a CTMC
by considering the quotient of the CTMC with respect to
an equivalence relation that preserves the Markov prop-
erty and supports the desired reward measures deﬁned on
the CTMC. By solving the smaller lumped CTMC, we can
∗
This material is based upon work supported in part by Pioneer Hi-
Bred, the National Science Foundation under Grant No. 0086096, by
DAAD and Deutsche Forschungsgemeinschaft, SFB 559. Any opinions,
ﬁndings, and conclusions or recommendations expressed in this material
are those of the author(s) and do not necessarily reﬂect the views of any of
the supporting organisations.
compute exact results for the larger CTMC, and therefore
measures of interest for the model. We classify the many
publications on lumping into three categories.
State-level lumping applies directly to a given genera-
tor matrix Q of a CTMC and computes (cid:1)Q of the lumped
Model-level lumping is used to generate (cid:1)Q directly from
CTMC. It yields an optimal partition, i.e., the smallest pos-
sible lumped CTMC. However, the size of Q limits its ap-
plication. Efﬁcient algorithms have been designed, e.g., see
[9] for the fastest known algorithm.
a model description. Hence, it is speciﬁc to a modeling
formalism. The approach is based on symmetry detection
among components of a compositional model. Results are
known for a variety of formalisms, such as stochastic well-
formed networks (SWN) [4, 8], stochastic activity networks
[18], stochastic automata networks [1], and Kronecker rep-
resentations, among others. While the second approach
manages to avoid processing a large matrix Q, it is lim-
ited to those symmetries that can be identiﬁed from a given
model description. Hence, in general, it does not obtain an
optimal lumping, unlike the ﬁrst approach.
Compositional lumping applies the state-level lump-
ing approach to individual components of a composi-
tional model. The original components are replaced by
lumped and “equivalent” components during generation of
(cid:1)Q. Like model-level lumping, this approach is formalism-
dependent; speciﬁcally, it relies on properties of the compo-
sition operators. For instance, based on the fact that lump-
ing is a congruence with respect to parallel composition
in a number of process algebra formalisms and stochastic
automata networks (SANs), compositional lumping can be
used in those formalisms, e.g., see [12, 3].
Note that in principle, approaches from different cate-
gories can be combined. For example, a compositional ap-
proach may yield smaller lumped components that can then
be fed to a model-level technique for further reduction of
the CTMC. Finally, state-level lumping can be applied to
obtain optimal reduction of the resulting matrix.
Despite all the state-space reductions the techniques
mentioned above offer, they may still yield a very large ma-
1
trix (cid:1)Q. Symbolic data structures like multi-terminal binary
decision diagrams (MTBDD) and matrix diagrams (MDs)
as well as Kronecker representations are suitable for repre-
senting Q in a space-efﬁcient manner for numerical analy-
sis. In particular, MD representations of Q [6] can be de-
rived directly from many formalisms with the help of a sym-
bolic state-space exploration as well as from a given sparse
matrix or Kronecker representation of Q; see [16] for a re-
cent overview paper on symbolic representations. Genera-
tion of an MD for models with state spaces on the order of
101000 states can be achieved, thanks to symbolic data struc-
tures and the so-called “saturation technique” [5]. However,
the numerical solution is limited to models with state spaces
on the order of 108 states, with iteration vectors being the
bottleneck for space. There are only a few results that ad-
dress the problem of lumping of Markov chains represented
as MDs. In [10], a model-level lumping technique is pro-
posed for MDs that result from state-sharing compositional
models. Like other model-level lumping techniques that can
only exploit lumpings that occur due to symmetric compo-
sition of components, our technique in [10] can only ﬁnd
symmetries that occur in MD levels that correspond to iden-
tical components of a model. To the best of our knowledge,
there is no algorithm that can exploit lumpings in the indi-
vidual levels of an MD.
In this paper, we present a new compositional lumping
algorithm that is useful for exact and ordinary lumping of
Markov chains represented as MDs without knowledge of
the modeling formalism from which the MDs were gener-
ated. Our approach relies on local conditions, i.e., condi-
tions on individual levels of the MD. Since our algorithm lo-
cally processes MD nodes that are dramatically smaller than
the matrix represented by the MD, it is computationally in-
expensive (compared to state-space generation and numeri-
cal solution) at the price that it does not necessarily achieve
an optimal lumping for the overall CTMC. As we will see
in Section 5, our algorithm signiﬁcantly reduces the space
and time requirements of MD-based numerical solution al-
gorithms while incurring a negligible time overhead. Our
compositional technique is complementary to the model-
level lumping presented in [10].
In other words, we can
apply compositional and model-level lumping techniques
simultaneously on a compositional model whose underly-
ing CTMC is represented as an MD. Our work is related to
[11] in that we argue on the level of a block structured ma-
trix to observe lumpability, but unlike [11], is not limited to
Kronecker matrices and stochastic automata networks. It is
related to [3] in that we have a local condition but do not
separate local and synchronized actions as was done for the
automata theoretic approach in [3].
The paper is organized as follows. We recall basic deﬁn-
itions in Section 2. In Section 3, we present the main result
of the paper, which is two theorems in which lumpability
2
conditions on the matrices of a single level of an MD are
proved to be sufﬁcient for the entire MD to be lumpable.
Section 4 presents an algorithm that lumps the MD of a
CTMC level by level, and Section 5 analyzes a tandem
multi-processor system model to illustrate the applicability
and beneﬁts of the approach.
2 Background and Deﬁnitions
(cid:2)
In this section, we recall fundamental deﬁnitions. For
notation, all matrices and vectors are typeset with bold char-
acters (upper-case letters for matrices and lower-case letters
for vectors), and their rows and columns are indexed start-
ing from 1. The element of matrix A(n×m) (of size n × m)
in row i and column j is referred to as A(i, j). For a matrix
A(n×m), rs(A) = B(n×n) is a diagonal matrix such that
B(i, i) =
A(i, j). Sets are assumed to be ﬁnite.
We specify a CTMC by a state space S = {1, . . . ,|S|}
and a state transition rate matrix R, or a generator matrix
Q = R − rs(R), in which R(i, j) is the rate of the transi-
tion that changes the state of the CTMC from i to j. Con-
sider a partition P = {C1, . . . , C(cid:0)n} of S. C1,··· , C(cid:0)n are
the equivalence classes of partition P, or in short, classes of
P. Any two states x, ˆx in a class Ci of P are called equiva-
lent, and that is indicated by x ≈ ˆx.
1≤j≤m
Often, the ﬁnal goal of a CTMC analysis is not the
computation of the stationary or transient probability of its
states.
Instead, it is the computation of high-level mea-
sures such as performance, dependability, and/or availabil-
ity. Many of those high-level measures can be computed us-
ing reward values associated with each state of the CTMC
(i.e., rate rewards) and the stationary and transient proba-
bility vectors. By incorporating rate rewards and an initial
probability vector with the CTMC, we obtain an MRP.
Deﬁnition 1. A Markovreward process (MRP) M is a 4-
tuple (S, Q, r, πini) where S = {1, . . . ,|S|} is the state
spaceofaCTMC,Q(|S|×|S|) isthegeneratormatrixofthe
CTMC, r is a row vector r (each of size|S| assigning re-
wardvaluer(i) tostatei),andπini(i) istheprobabilitythat
theCTMCisinstate i attime0 (1 ≤ i ≤ |S|).
(cid:1)
Given an MRP and a number of high-level measures ex-
pressed in terms of r and πini, we can compute the measures
by numerically solving the underlying CTMC. The larger
the state space of the CTMC, the more time-consuming that
numerical solution will be. Sometimes a lumped CTMC
can be used to obtain the desired measures. That can only
be done if the MRP that is based on that CTMC satisﬁes a
set of conditions. Three of the most important sets of condi-
tions (and the types of lumpings they lead to) are outlined in
Deﬁnition 2. The lumped MRP, which includes the lumped
CTMC, is obtained using Theorem 2. For a compact no-
A(i, j),
tation, we use the identities A(i, C) =
(cid:2)
j∈C
(cid:2)
(cid:2)
(cid:3)
(cid:3)
(cid:3)
(cid:3)
i∈C
i∈C
a(i), in
, s) = Q(C
A(i, j), and a(C) =
A(C, j) =
which C is a set of valid column or row indices.
Deﬁnition 2. ConsideranMRP M = (S, Q, r, πini) anda
partitionP ofS. Then,withrespecttoP,M is
(cid:3)) andr(s) =
(cid:3)) = Q(ˆs, C
1. ordinarilylumpable ifQ(s, C
r(ˆs) forallC, C
2. exactlylumpable if Q(C
πini(ˆs) forallC, C
(cid:3) ∈ P,all(equivalentstates) s, ˆs ∈ C.
, ˆs) and πini(s) =
(cid:3) ∈ P,all(equivalentstates) s, ˆs ∈ C.(cid:1)
In Deﬁnition 2, we gave lumpability conditions in terms
of Q. Since we will use the R matrix in the following,
we state the lumpability conditions in terms of R instead
of Q. Theorem 1 speciﬁes such conditions. Strictly speak-
ing, Deﬁnition 2 deﬁnes lumping for an MRP and not for
the CTMC embedded in it. However, in the rest of the pa-
per, we will also speak of lumping of CTMCs, of their cor-
responding Q or R matrices, of the MD representation of
those matrices, or of nodes and levels (deﬁned in Section
3) of the MD, under the assumption that the reward vectors
and initial probability distribution are such that they satisfy
the requirements of Deﬁnition 2.
Theorem 1. Consider an MRP M = (S, Q, r, πini) such
that Q = R − rs(R) where R is the state transition rate
matrixoftheCTMCofM.Then,withrespecttoapartition
P,M is
(cid:3)) and r(s) =
1. ordinarilylumpableifR(s, C
r(ˆs) forallC, C
, ˆs), R(s,S) =
(cid:3)
2. exactly lumpable if R(C
, s) = R(C
R(ˆs,S), and πini(s) = πini(ˆs) for all C, C
(cid:3) ∈ P and all
(equivalentstates) s, ˆs ∈ C.
(cid:3)) =
Proof. (a) By the deﬁnition of Q, we have Q(s, C
(cid:2)
. If s, ˆs ∈
(cid:3)), if s, ˆs (cid:5)∈ C
R(s, C
(cid:3))− R(s,S) = R(s, C
(cid:3))−
C
(cid:3)(cid:3)) =
C(cid:1)(cid:1)∈P R(ˆs, C
(cid:3)(cid:3) ∈ P.
Q(ˆs, C
(b) Similarly, if s, ˆs (cid:5)∈ C
(cid:3)
the result is immediate. Oth-
, s) − R(s,S) = R(C
, ˆs) −
(cid:3)
(cid:3)
, s) = R(C
erwise, Q(C
R(ˆs,S) = Q(C
(cid:3)
(cid:3)
, s) = R(C
, ˆs) and
R(s,S) = R(ˆs,S).
(cid:1)
Note that the converse of Theorem 1 does not hold, since
Q tolerates different rates for self-loops of equivalent states
while those rates are distinguishable in R. Now that we
know when a partition satisﬁes the ordinary and/or exact
lumpability conditions, we need to know how to derive a
lumped MRP.
Theorem 2. Let M = (S, Q, r, πini) be (ordinarily
or exactly) lumpable with respect to a partition P =
{C1, . . . , C(cid:0)n} of S. Then (cid:3)M = ((cid:1)S,(cid:1)Q,(cid:1)r,(cid:1)πini) is the
lumpedMRPsuchthat(cid:1)S = {1, . . . ,(cid:1)n}
(cid:4)
(cid:1)Q(˜i, ˜j) =
) forarbitraryi ∈ C(cid:0)
Q(i, C(cid:0)
| and(cid:1)πini(˜i) = πini(C(cid:0)
(cid:1)r(˜i) = r(C(cid:0)
i, j) forarbitraryj ∈ C(cid:0)
Q(C(cid:0)
)/|C(cid:0)
(cid:3)) = R(ˆs, C
, we have Q(s, C
C(cid:1)(cid:1)∈P R(s, C
(cid:3) ∈ P,all(equivalentstates) s, ˆs ∈ C.
(cid:3)) = Q(ˆs, C
(cid:3)) = R(s, C
(cid:3)(cid:3)) = R(ˆs, C
(cid:3)) since condition (a) holds for all C
(cid:3)) − (cid:2)
(ordinary)
(exact)
, ˆs) since R(C
(cid:3)) = R(ˆs, C
(cid:3)
, C
i
j
).