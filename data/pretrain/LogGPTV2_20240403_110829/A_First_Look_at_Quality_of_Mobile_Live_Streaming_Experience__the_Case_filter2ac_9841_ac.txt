the video data directly to viewers right after receiving
it from the broadcasting client. HLS delivery requires
the data to be packaged in complete segments, possi-
bly while transcoding it to multiple qualities, and the
client application needs to separately request for each
video segment, which all adds up to the latency. HLS
does produce fewer stall events but we have seen no evi-
dence of the video bitrate being adapted to the available
bandwidth (Section 5.2). It is possible that the buﬀer
sizing strategy causes the diﬀerence in the number of
stall events between the two protocols but we cannot
conﬁrm this at the moment.
5.2 Audio and Video Quality
Both RTMP and HLS communications employ stan-
dard codecs for audio and video, that is, AAC (Ad-
vanced Audio Coding) for audio [7] and AVC (Advanced
Video Coding) for video [6]. In more details, audio is
sampled at 44,100 Hz, 16 bit, encoded in Variable Bit
Rate (VBR) mode at about either 32 or 64 kbps, which
seems enough to transmit almost any type of audio con-
tent (e.g., voice, music, etc.) with the quality expected
from capturing through a mobile device.
Video resolution is always 320×568 (or vice versa de-
pending on orientation). The video frame rate is vari-
able, up to 30 fps. Occasionally, some frames are miss-
ing hence concealment must be applied to the decoded
video. This is probably due to the fact that the upload-
ing device had some issues, e.g., glitches in the real-time
encoding or during upload.
Fig. 6(a) shows the video bitrate, typically ranging
between 200 and 400 kbps. Moreover, there is almost no
02040600.512345678910100bandwidth limit (Mbps)join time (s)05101520250.512345678910100bandwidth limit (Mbps)playback latency (s)0.000.250.500.751.000.010.10.31310video delivery latency (s)fraction of brdcstsHLSRTMP0.00.20.40.60.81.0 0 0.25 0.5 0.75 1 1.25fraction of videosbitrate (Mbit/s)HLSRTMP 10 20 30 40 50 0 0.25 0.5 0.75 1 1.25avg QPbitrate (Mbit/s)481diﬀerence between HLS and RTMP except for the maxi-
mum bitrate which is higher for RTMP. Analysis of such
cases reveals that poor eﬃciency coding schemes have
been used (e.g., I-type frames only). The most common
segment duration with HLS is 3.6 s (60% of the cases),
and it ranges between 3 and 6 s. However, the corre-
sponding bitrate can vary signiﬁcantly. In fact, in real
applications rate control algorithms try to keep its aver-
age close to a given target, but this is often challenging
as changes in the video content directly inﬂuences how
diﬃcult is to achieve such bitrate. To this aim, the so
called quantization parameter (QP) is dynamically ad-
justed [2]. In short, the QP value determines how many
details are discarded during video compression, hence it
can be used as a very rough indication of the quality of
a given video segment. Note that the higher the QP,
the lower the quality and vice versa.
To investigate quality, we extracted the QP and com-
puted its average value for all the videos. Fig. 6(b)
shows the QP vs bitrate for each captured video (the
whole video for RTMP and each segment for HLS).
When the quality (i.e., QP value) is roughly the same,
the bitrate varies in a large range. On one hand, this
is an indication that the type of content strongly diﬀer
among the streams. For instance, some of them feature
very static content such as one person talking on a static
background while others show, e.g., soccer matches cap-
tured from a TV screen. On the other hand, observing
how the bitrate and average QP values vary over time
may provide interesting indications on the evolution of
the communication, e.g., hints about whether represen-
tation changes are used. Unfortunately, we are cur-
rently unable to draw deﬁnitive conclusions since the
variation could also be due to signiﬁcant changes in the
video content which should be analyzed in more depth.
Finally, we investigated the frame type pattern used
for encoding. Most use a repeated IBP scheme. Few
encodings (20.0 % for RTMP and 18.4% for HLS) only
employ I and P frames only (or just I in 2 cases). After
about 36 frames, a new I frame is inserted. Although
one B frame inserts a delay equal to the duration of the
frame itself, in this case we speculate that the reason
they are not present in some streams could be that some
old hardware might not support them for encoding.
5.3 Power Consumption
We connected a Samsung Galaxy S4 4G+ smartphone
to a Monsoon Power Monitor [1] in order to measure its
power consumption as instructed in [17]. We used the
PowerTool software to record the data measured by the
power monitor and to export it for further analysis.
The screen brightness was full in all test cases and
the sound was oﬀ. The phone was connected to the
Internet through non-commercial WiFi and LTE net-
works2. Figure 7 shows the results. We measured the
2It is a full-ﬂedged LTE network operated by Nokia.
DRX was enabled with typical timer conﬁguration.
Figure 7: Average power consumption with
Periscope and idle device.
idle power draw in the Android application menu to
be around 900 to 1000 mW both with WiFi and LTE
connections. With the Periscope app on without video
playback, the power draw grows already to 1537 mW
with WiFi and to 2102 mW with LTE because the ap-
plication refreshes the available videos every 5 seconds.
Playing back old recorded videos with the application
consume an equal amount of power as playing back live
videos. The power consumption diﬀerence of RTMP
vs HLS is also very small. Interestingly, enabling the
chat feature of the Periscope videos raises the power
consumption to 2742 mW with WiFi and up to 3599
mW with LTE. This is only slightly less than when
broadcasting from the application. However, the test
broadcasts had no chat displayed on the screen.
We further investigated the impact of the chat fea-
ture by monitoring CPU and GPU activity and network
traﬃc. Both processors use DVFS to scale power draw
to dynamic workload [17]. We noticed an increase by
roughly one third in the average CPU and GPU clock
rates when the chat is enabled, which implies higher
power draw by both processors. Recall from Section 5.1
that the chat feature may increase the amount of traf-
ﬁc, especially with streams having an active chat, which
inevitably increases the energy consumed by wireless
communication. The energy overhead of chat could be
mitigated by caching proﬁle pictures and allowing users
to disable their display in the chat.
6. RELATED WORK
Live mobile streaming is subject to increasing atten-
tion, including from the sociological point of view [14].
In the technical domain, research about live streaming
focused on issues such as distribution optimization [11],
including scenarios with direct communication among
devices [22]. The crowdsourcing of the streaming activ-
ity itself also received particular attention [4, 21].
Stohr et al. have analyzed the YouNow service [15]
and Tang et al. investigated the role of human factors in
Power consumption (mW)010002000300040005000HomescreenApp onVideo on(not live)Video on(RTMP/chat off)Video on(HLS/chat off)Video on(HLS/chat on)Broadcast973921153721021958285519222714209528502742359930993777WiFiLTE482Meerkat and Periscope [16]. Little is known, however,
about how such mobile applications perform. Most of
the research has focused on systems where the mobile
device is only the receiver of the live streaming, like
Twitch.Tv [20], or other mobile VoD systems [9].
We believe that this work together with the work of
Wang et al. [18] are the ﬁrst to provide measurement-
based analyses on the anatomy and performance of a
popular mobile live streaming application. Wang et al.
thoroughly study the delay and its origins but, similar
to us, also show results on usage patterns and video
stalling, particularly the impact of buﬀer size. They
also reveal a particular vulnerability in the service.
7. CONCLUSIONS
We explored the Periscope service providing insight
on some key performance indicators. Both usage pat-
terns and technical characteristics of the service (e.g.,
delay and bandwidth) were addressed. In addition, the
impact of using such a service on the mobile devices was
studied through the characterization of the energy con-
sumption. We expect that our ﬁndings will contribute
to a better understanding of how the challenges of mo-
bile live streaming are being tackled in practice.
8. ACKNOWLEDGMENTS
This work has been ﬁnancially supported by the Academy
of Finland, grant numbers 278207 and 297892, and the
Nokia Center for Advanced Research.
9. REFERENCES
[1] Monsoon: www.msoon.com.
[2] Z. Chen and K. N. Ngan. Recent advances in rate
control for video coding. Signal Processing: Image
Communication, 22(1):19–38, 2007.
[3] Genymotion: https://www.genymotion.com/.
[4] Q. He, J. Liu, C. Wang, and B. Li. Coping with
heterogeneous video contributors and viewers in
crowdsourced live streaming: A cloud-based
approach. IEEE Transactions on Multimedia,
18(5):916–928, May 2016.
[5] ISO/IEC 13818-1. MPEG-2 Part 1 - Systems,
Oct. 2007.
[6] ISO/IEC 14496-10 & ITU-T H.264. Advanced
Video Coding (AVC), May 2003.
[7] ISO/IEC 14496-3. MPEG-4 Part 3 - Audio, Dec.
2005.
[8] F. Larumbe and A. Mathur. Under the hood:
Broadcasting live video to millions. https:
//code.facebook.com/posts/1653074404941839/
under-the-hood-broadcasting-live-video-to-millions/,
Dec. 2015.
[9] Z. Li, J. Lin, M.-I. Akodjenou, G. Xie, M. A.
Kaafar, Y. Jin, and G. Peng. Watching videos
from everywhere: a study of the PPTV mobile
VoD system. In Proc. of the 2012 ACM conf. on
Internet Measurement Conference, pages 185–198.
ACM, 2012.
[10] LibAV Project: https://libav.org/.
[11] T. Lohmar, T. Einarsson, P. Fr¨ojdh, F. Gabin,
and M. Kampmann. Dynamic adaptive HTTP
streaming of live content. In World of Wireless,
Mobile and Multimedia Networks (WoWMoM),
2011 IEEE International Symposium on a, pages
1–8, June 2011.
[12] Mitmproxy Project: https://mitmproxy.org/.
[13] Periscope. Year one. https://medium.com/
@periscope/year-one-81c4c625f5bc\#.mzobrfpig,
Mar. 2016.
[14] D. Stewart and J. Littau. Up, periscope: Mobile
streaming video technologies, privacy in public,
and the right to record. Journalism and Mass
Communication Quarterly, Special Issue:
Information Access and Control in an Age of Big
Data, 2016.
[15] D. Stohr, T. Li, S. Wilk, S. Santini, and
W. Eﬀelsberg. An analysis of the younow live
streaming platform. In Local Computer Networks
Conference Workshops (LCN Workshops), 2015
IEEE 40th, pages 673–679, Oct 2015.
[16] J. C. Tang, G. Venolia, and K. M. Inkpen.
Meerkat and periscope: I stream, you stream,
apps stream for live streams. In Proceedings of the
2016 CHI Conference on Human Factors in
Computing Systems, CHI ’16, pages 4770–4780,
New York, NY, USA, 2016. ACM.
[17] S. Tarkoma, M. Siekkinen, E. Lagerspetz, and
Y. Xiao. Smartphone Energy Consumption:
Modeling and Optimization. Cambridge University
Press, 2014.
[18] B. Wang, X. Zhang, G. Wang, H. Zheng, and
B. Y. Zhao. Anatomy of a personalized
livestreaming system. In Proc. of the 2016 ACM
Conference on Internet Measurement Conference
(IMC), 2016.
[19] Wireshark Project: https://www.wireshark.org/.
[20] C. Zhang and J. Liu. On crowdsourced interactive
live streaming: A twitch.tv-based measurement
study. In Proceedings of the 25th ACM Workshop
on Network and Operating Systems Support for
Digital Audio and Video, NOSSDAV ’15, pages
55–60, New York, NY, USA, 2015. ACM.
[21] Y. Zheng, D. Wu, Y. Ke, C. Yang, M. Chen, and
G. Zhang. Online cloud transcoding and
distribution for crowdsourced live game video
streaming. IEEE Transactions on Circuits and
Systems for Video Technology, PP(99):1–1, 2016.
[22] L. Zhou. Mobile device-to-device video
distribution: Theory and application. ACM
Trans. Multimedia Comput. Commun. Appl.,
12(3):38:1–38:23, Mar. 2016.
483