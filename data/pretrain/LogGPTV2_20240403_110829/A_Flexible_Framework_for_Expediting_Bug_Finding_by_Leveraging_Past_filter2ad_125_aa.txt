title:A Flexible Framework for Expediting Bug Finding by Leveraging Past
(Mis-)Behavior to Discover New Bugs
author:Sanjeev Das and
Kedrian James and
Jan Werner and
Manos Antonakakis and
Michalis Polychronakis and
Fabian Monrose
A Flexible Framework for Expediting Bug Finding by
Leveraging Past (Mis-)Behavior to Discover New Bugs
Sanjeev Das∗
IBM Research
PI:EMAIL
Manos Antonakakis
Georgia Tech
PI:EMAIL
Kedrian James
UNC Chapel Hill
PI:EMAIL
Michalis Polychronakis
Stony Brook University
PI:EMAIL
Jan Werner
UNC Chapel Hill
PI:EMAIL
Fabian Monrose
UNC Chapel Hill
PI:EMAIL
ABSTRACT
Among various fuzzing approaches, coverage-guided grey-box fuzzing
is perhaps the most prominent, due to its ease of use and effective-
ness. Using this approach, the selection of inputs focuses on maxi-
mizing program coverage, e.g., in terms of the different branches
that have been traversed. In this work, we begin with the observa-
tion that selecting any input that explores a new path, and giving
equal weight to all paths, can lead to severe inefficiencies. For in-
stance, although seemingly “new” crashes involving previously
unexplored paths may be discovered, these often have the same
root cause and actually correspond to the same bug.
To address these inefficiencies, we introduce a framework that
incorporates a tighter feedback loop to guide the fuzzing process
in exploring truly diverse code paths. Our framework employs (i) a
vulnerability-aware selection of coverage metrics for enhancing the
effectiveness of code exploration, (ii) crash deduplication informa-
tion for early feedback, and (iii) a configurable input culling strategy
that interleaves multiple strategies to achieve comprehensiveness.
A novel aspect of our work is the use of hardware performance
counters to derive coverage metrics. We present an approach for
assessing and selecting the hardware events that can be used as a
meaningful coverage metric for a target program. The results of
our empirical evaluation using real-world programs demonstrate
the effectiveness of our approach: in some cases, we explore fewer
than 50% of the paths compared to a base fuzzer (AFL, MOpt, and
Fairfuzz), yet on average, we improve new bug discovery by 31%,
and find the same bugs (as the base) 3.3 times faster. Moreover, al-
though we specifically chose applications that have been subject to
recent fuzzing campaigns, we still discovered 9 new vulnerabilities.
CCS CONCEPTS
• Security and privacy → Vulnerability management.
∗The research was conducted while the author was a postdoc at UNC Chapel Hill.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ACSAC 2020, December 7–11, 2020, Austin, USA
© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-8858-0/20/12...$15.00
https://doi.org/10.1145/3427228.3427269
KEYWORDS
Fuzzing, Machine Learning, Hardware Performance Counters
ACM Reference Format:
Sanjeev Das, Kedrian James, Jan Werner, Manos Antonakakis, Michalis
Polychronakis, and Fabian Monrose. 2020. A Flexible Framework for Ex-
pediting Bug Finding by Leveraging Past (Mis-)Behavior to Discover New
Bugs. In Annual Computer Security Applications Conference (ACSAC 2020),
December 7–11, 2020, Austin, USA. ACM, New York, NY, USA, 15 pages.
https://doi.org/10.1145/3427228.3427269
1 INTRODUCTION
In recent years, fuzz testing (or fuzzing) [30] has emerged as the
preeminent automated technique for finding vulnerabilities in soft-
ware. Generally speaking, the process of fuzzing involves feeding
crafted input to a program in the hope of triggering unhandled
exceptions and crashes. Today, so-called greybox fuzzing has been
very effective in finding vulnerabilities in real-world programs.
The success of greybox fuzzers like the American Fuzzy Lop
(AFL) [61] stems from the fact that they use a feedback loop to
prioritize the inputs fed to a program. The overall process involves
input selection, scheduling, and mutation. In the first stage, inputs
are tested, and based on various feedback mechanisms, interesting
inputs (i.e., those that crashed the program or led to new paths be-
ing explored) are chosen for mutation. The mutation stage typically
assumes input data as a sequence of bytes, and performs operations
such as bit or byte flipping, increment/decrement of integer data,
and so on. Input selection and scheduling have been shown to be
critically important [44, 57] because they govern the doctrine that
fuzzers apply in their search for vulnerabilities [31, 32]. As it per-
tains to the input selection process, contemporary greybox fuzzers
use a coverage-driven principle. Coverage-guided fuzzing (CGF) ap-
proaches select inputs that increase the total program coverage.
For example, AFL uses branch coverage to steer input selection. As
such, it only selects those inputs that explore a new branch that
was not traversed before. The inherent goal of a CGF approach is
to try to increase the code coverage in order to eventually reach a
path that may stumble upon a vulnerability in the program. Indeed,
the success of AFL [61], and extensions thereof [4, 48, 50], is largely
attributable to the use of code coverage as feedback [10].
Unfortunately, blindly adopting such a strategy can be ineffi-
cient because: (i) any input that covers a new path is selected, and
(ii) equal weight is given to each path. To see why this matters,
consider Figure 1 which depicts an offline analysis of crashes gen-
erated by AFL. The topmost line reports the “uniqueness” of the
ACSAC 2020, December 7–11, 2020, Austin, USA
Sanjeev Das, Kedrian James, Jan Werner, Manos Antonakakis, Michalis Polychronakis, and Fabian Monrose
generated crashes based on the branch edges explored. The fuzzer
generates a lot of crashes, but not all crashes are created equal [23].
Indeed, a straightforward grouping of crashes using a stack trace
approach shows that the number of unique crashes is significantly
lower than AFL indicates. More importantly, these crashes are ob-
tained intermittently (as depicted by the red line), likely due to
time wasted exploring closely related paths or getting stuck in deep
code paths. The fruitless search wastes time on inefficient mutation
operations that do not lead to different code paths, ultimately fail-
ing to finding so-called “quality inputs” that lead to crashes for a
long time [27]. Furthermore, Böhme et al. [2] showed that although
finding some vulnerabilities using contemporary approaches might
be cheap, improving bug finding linearly requires exponentially
more computational power.
Figure 1: Inefficiencies in the typical fuzzing process
Additionally, since most of the program paths are (hopefully)
bug-free [33, 51], giving equal value to all program paths is a non-
optimal strategy when the ultimate goal is to discover vulnerabili-
ties [9]. This issue is clearly visible in Figure 1, which also shows
the time when the first instance of each of only two classes of bugs
(a stack overflow and a source access violation vulnerability, respec-
tively) were found. With Chen et al. [9]’s observation about path
coverage in mind, other researchers have proposed ways for im-
proving code coverage by using different coverage metrics [54] (e.g.,
context-sensitive branch coverage [7, 43], memory-access-aware
branch coverage [16], basic block information [25, 43]) have been
proposed. Unfortunately, Wang et al. [54] subsequently demon-
strated that there is no grand slam coverage metric that outperforms
all the others. That is, given limited resources (time and compu-
tation power), all of these coverage metrics offered merit relative
to flipping certain types of branches or in finding vulnerabilities
not readily found by the others. Alas, different coverage metrics
performed better on other benchmarks, underscoring the enormous
diversity that exists between the code base of different programs.
Thus, even though a one size fits all coverage metric may be de-
sirable, doing so is unlikely to discover diverse bug types across a
wide variety of programs.
Taken together, these findings open the door for several areas of
improvement within the practice of fuzzing. For one, by introducing
a tighter feedback loop early on in the workflow, one can better
guide a fuzzer in exploring more diverse code paths. Likewise,
armed with the ability to dynamically choose when to apply a
particular coverage metric, we may be able to improve the overall
success rate of a fuzzer by steering it toward or away from certain
classes of bugs.
Our specific contributions include:
(1) Vulnerability-aware selection of a coverage metric: we use
micro-architectural information obtained through hardware
performance counters (HPCs) to derive coverage metrics.
HPCs are a set of special registers that are available in pro-
cessors to monitor and measure hardware events related to
memory, branches, instructions, and basic blocks. We provide
a principled approach for systematically assessing hundreds
of HPC events to select representative sets of events that can
be utilized as a coverage metric for a given program.
(2) A configurable input selection strategy: using knowledge of
past bugs, we show how one can perform fuzzing under two
modes of operation — to seek bugs that are induced under
similar behavior as witnessed in past bugs, or to hunt for
bugs that are triggered by program behavior that is markedly
different from that observed when previous vulnerabilities
were discovered. Given the large number of events that can
be utilized to build a coverage metric, there is tremendous
flexibility in exploring input selection strategies at runtime,
e.g., by choosing a different set of HPC events than those that
performed best in teasing out past bugs. We leverage this
flexibility to switch between coverage metrics at runtime
(e.g., when the deduplication strategy informs us that the
last few crashes likely fall under the same bug classification).
(3) Using deduplication as a feedback mechanism: we show that
deduplication can be used as an early feedback mechanism
to improve the overall fuzzing progress. Specifically, using
crash deduplication techniques to quickly infer the potential
root cause and to steer the fuzzing process, one can lessen
the chances of discovering the same bugs repeatedly.
(4) Extensibility: We demonstrate that our approach can help
improve the effectiveness of different base fuzzers. Moreover,
we perform an extensive evaluation based on practitioner’s
vantage point, in terms of time to finding a unique crash,
and the consistency of bug discovery over repeated runs. To
demonstrate the benefits of our extensions to the vulnerabil-
ity discovery process, we report on an evaluation on eight
real-world programs that were specifically chosen because
they have been subject to heavy fuzzing attempts in the
recent past.
2 BACKGROUND
In what follows, we provide an overview of hardware performance
counters (HPC), as the usage of these counters plays a key role in
our approach. In short, hardware performance counters are a set of
special registers present in modern processors that can be used to
monitor and measure events at the hardware level. These events are
related to instructions, memory, and the execution behavior on the
04812162024283236404448Hours0153045607590105120135150165180195210225Number of unique crashesC1C2AFL (branch edge)Stack backtraceNo. of new crashes (stack backtrace) observed in each hourCategory 1 - Stack buffer overflowCategory 2 - Source access violationA Flexible Framework for Expediting Bug Finding by Leveraging Past (Mis-)Behavior to Discover New Bugs
ACSAC 2020, December 7–11, 2020, Austin, USA
CPU pipeline. The hardware events supported by performance coun-
ters can be classified as either architectural or non-architectural
events (the latter are also known as micro-architectural events).
Architectural events comprise events that remain consistent
across different processor architectures. Examples include instruc-
tions, branches, and cycles. Non-architectural events are those that
are specific to the micro-architecture of a given processor, for exam-
ple, cache accesses, branch prediction, and TLB accesses. Unlike ar-
chitectural events, non-architectural events vary among processor
architectures and may also change with processor enhancements.
Table 8 in the §A.1 presents a list of commonly used architectural
and non-architectural events in Intel processors. Interested readers
are referred to [14, 34, 46, 52] for excellent overviews of perfor-