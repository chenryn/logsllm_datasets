interface, HaCRS can provide extra information and capabilities to
enhance the humans’ abilities to complete the assistance task.
Interaction assistance. One such capability provided by HaCRS
is the automated re-formatting of input data. HaCRS traces each
program test case to detect if input data must be provided in a
specific format. It achieves this by leveraging existing techniques
in protocol recovery [4, 5, 21]. Depending on configuration (and
expertise of human assistants), this information can either be pre-
sented to the human assistants or utilized automatically to mutate
human-created inputs into a format understood by the application.
In our prototype, we mainly utilize these techniques to automat-
ically recover non-standard field delimiters used by the binaries
in our dataset, but they can also be used to support information
packing protocols, such as ASN1.
High-level guidance. Having enabled human interaction for bi-
naries with complex input data specifications, HaCRS turns to the
question of maximizing the ability of its humans to understand how
to interact with the target program. It does this by identifying and
categorizing constant string references in the binary.
HaCRS identifies static string references by analyzing its CFG,
and performs a static data flow analysis to categorizes these into
strings produced as output by the program and strings compared
against user input into the program. HaCRS identifies output strings
by detecting when they are passed into common output functions
(such as puts and printf). Input strings are considered to be any-
thing that is passed to a string comparison function. In the case of
statically-linked binaries, HaCRS can leverage the function identifi-
cation functionality built into the Mechanical Phish, which detects
common functions in programs using pre-defined dynamic test-
cases [27]).
HaCRS provides a list of potential output strings in the target pro-
gram to help its human assistants, relaying which of these strings
have not yet been triggered (i.e., caused to be output by the program)
by other test cases. These can provide useful semantic information
regarding the untapped functionality of the target program.
While HaCRS focuses on text-based software, it is important
to keep in mind that analogous information can be recovered for
software with a graphical user interface. For example, a similar
analysis can identify GUI widgets, render them, and display them
as potential directions of exploration for human assistants.
Symbolic tokens. First, HaCRS creates suggestions for human
assistants for ways that test cases might be modified to divert pro-
gram flow. This is done through a process of symbolic tokenization.
HaCRS symbolically traces the target program in the context of
each test case to recover constraints placed on the input by the
target program. It analyzes these constraints to identify contiguous
bytes on which the constraints are similar (in terms of the number
of constraint expressions and the types of arithmetic and Boolean
operations the constraint expressions are composed of). These con-
tiguous bytes represent tokens processed and reasoned about by
the binary.
HaCRS then identifies alternate values for each symbolic token.
It rewinds its symbolic trace to the address at which the first con-
straint of the token was introduced, discards symbolic inputs on
the state, and performs a symbolic from that point to retrieve other
potential values. The symbolic exploration runs until a timeout (we
found 30 seconds to be a reasonable timeout in our experiments).
At the end of the timeout, the constraints of the various resulting
paths are solved to alternate values for the token. These values are
further refined by matching them against the input strings retrieved
previously, and HaCRS produces two different sets of suggestions
to its assistants: “educated guesses”, which are the input strings that
are prefix-matched by the recovered alternatives and “brute-force
guesses”, which are the raw alternatives themselves.
Note that, while the concept of generating alternatives for input
is shared with Driller, the goal is different. Driller generates alter-
native test cases to drive execution down different paths. However,
the alternatives generated by this method are meant to be learned
by humans, understood, and reasoned about to produce new inputs
through human intuition and previously-learned experience. In
some simple cases (like the example in Figure 2), the symbolic to-
kens generated by HaCRS can directly be synchronized into Driller
Session B3:  Investigating AttacksCCS’17, October 30-November 3, 2017, Dallas, TX, USA353as an end-to-end test case without the need for human assistance.
When this holds, it is implicitly handled by the automation. Since
the token recovery process discards path constraints and symboli-
cally explores from the introduction location of the first constraint
on that token, it loses coherence in other input bytes, as keeping
them coherent will (for the exact reasons discussed in Section 4.6)
hamper token recovery. Thus, symbolic tokens are generally not di-
rectly usable as test-cases, and instead act as inspiration for human
assistants.
Input type annotation. Programs process different inputs differ-
ently, and HaCRS exposes this to its assistants by highlighting input
bytes that are constrained by similar constraints (as with the sym-
bolic token analysis, we use constraint count and operation types to
compute constraint similarity). Input bytes highlighted with similar
colors in the input test cases will be bytes that have been treated
similarly to each other by the program, and may represent similar
type of data. Most importantly, this differentiates string input (such
as a command) against numeric input (which is passed to functions
such as atoi, which impose specific constraints on the data).
5.4 Human-Automation Link
The interface between the HaCRS and its human assistants must be
designed in such a way as to be understandable by both parties. To
do this, we created a Human-Automation Link (HAL) that exposes,
to the humans, only the concepts of program analysis that non-
experts might be familiar with. For the curious reader, we reproduce
a mock-up of the HAL interface in Figure 2.
The HAL interface in Figure 2 consists of the following elements:
Program description. When a description of the target program
is available, it can aid assistants in interacting with it. In
the case of Cyber Grand Challenge binaries, this description
includes a very brief (usually four to five sentences) summary
of the program’s purpose, as written by the program authors.
In a real-world setting, human assistants can be provided
with the technical manual for the piece of software being
tested.
Tasklet instructions. The HaCRS provides human-readable in-
structions, which are presented to the assistant alongside
each tasklet.
Example interactions. The HaCRS provides logs of previous in-
teractions with the software, in the form of input and output
data. For the text-based software of DECREE OS, to help
assistants understand what data was originated from them
(program input) and what came from the program (program
output), the input and output are displayed in different col-
ors. A version of HaCRS for software with a graphical user
interface could instead have a video record of the interaction,
but this is not supported by our prototype.
CRS-Generated Suggestions. To help assistants understand how
to deviate from a test case, they can invoke the deviation
annotation interface. This interface displays data recovered
through the automated analyses described in Section 5.3 to
present the assistant with a better idea of how to make a
program behave differently than in the example test case.
Interaction terminal. To facilitate the interaction between hu-
man assistants and the target program, a terminal is pre-
sented to interact with the software. Again, to help assistants
understand differentiate user input from program output,
the input and output are displayed in different colors.
Tasklet goal and feedback. Any human-facing task must have
an understandable end goal to avoid confusion on the part of
the assistants. HaCRS requires its human assistants to trigger
previously-unseen functionality in the target programs. To
this end, it provides feedback to the assistant regarding the
amount of previously-unseen control flow transitions that
the assistant was able to trigger.
Along with this, it provides a display of untriggered output
strings, as described in Section 5.3. With their human ability
to reason about semantic information, assistants can leverage
the bounty strings to better target untriggered functionality
in the program.
Each tasklet also has a timeout and an abort button: if the assis-
tant is unable to complete the tasklet before a timeout, or presses
the abort button, the tasklet is terminated. This acts as a guard
against the situation when the tasklet is not actually completable
(for example, if the remaining untriggered functionality is dead
code).
In the next section, we will explore the implication of human
assistance by evaluating the performance of HaCRS against the
performance of the unaided Mechanical Phish.
6 EVALUATION
In this section, we evaluate the impact of our integration of non-
expert human effort into the Cyber Reasoning System paradigm.
We measure the result of this as a whole, in terms of the overall
number of vulnerabilities identified in our dataset, but also explore
certain low-level details of the system.
6.1 Dataset
As previously mentioned, Mechanical Phish was designed to oper-
ate on binaries for DECREE, the operating system designed for the
DARPA Cyber Grand Challenge. A total of 250 binaries were pro-
duced by DARPA for the Cyber Grand Challenge2. These binaries
vary in complexity, but are designed to mimic a wide range of vul-
nerabilities and behaviors found in real-world software. Each Cyber
Grand Challenge binary is guaranteed to have at least one vulnera-
bility, and proof-of-concept exploits, along with high-quality test
cases, are provided for each. This makes it possible to measure,
with some degree of certainty (after all, previously-unknown vul-
nerabilities might also be present), the effectiveness of vulnerability
detection techniques. As such, they have already been used in the
evaluation of various other scientific work [28, 29, 35].
Filtering. Our dataset is the subset of DECREE programs that
present a human-usable text protocol or for which the interaction
assistance provided by HaCRS (as discussed in Section 5.3) was able
to facilitate a human-usable text protocol. We selected these by
automatically detecting the presence of non-printable characters
2DARPA recently funded the creation of a human-readable repository with information
on these applications, hosted at http://www.lungetech.com/cgc-corpus.
Session B3:  Investigating AttacksCCS’17, October 30-November 3, 2017, Dallas, TX, USA354to recreate the valuable semantic hints in software designed for
humans. We leave the integration of such program mutation into
our interaction assistance component as future work.
Classification. Even though a protocol might be text only, it
might still be hard for humans to understand. As an example of this,
consider PDF, which is a text-only file format that is designed to
be parsed exclusively by computer programs. To better understand
the implications of human assistance on the binaries in our dataset,
we manually categorized them according to the following qualities:
Technical expertise. We determined whether a program requires
technical expertise to be used. For example, some of the pro-
grams in the dataset are language interpreters or databases,
requiring users to be familiar with such Computer Science
concepts as programming languages. These programs would
be rated as requiring high technical expertise.
Semantic complexity. We attempted to identify whether actions
taken by the program yield themselves to high-level rea-
soning about the program’s intent. For example, a move
taken in a chess match would have high semantic complex-
ity, whereas an iteration of a compression algorithm would
not. Thus, a chess engine would be ranked as having high
semantic complexity, whereas a compression utility would
not.
CGC binaries are fairly small, and the small size of these binaries
makes them well-suited for such classification. Specifically, because
the binaries tend to be “single-purpose” (i.e., a recipe storage ap-
plication, as opposed to a web browser), most binaries do not have
different modules with different semantic complexity or technical
expertise requirements.
The binaries, by their various classifications, are presented in
Table 3. Classifications were done by one researcher, before exper-
iments were performed (except for binaries used during system
development). For borderline cases of semantic complexity, we erred
on the side of marking binaries as complex. For borderline cases of
required expertise, we erred on "requiring expertise". Our reasoning
for the classification of various binaries is provided alongside the
classifications. Admittedly, this process is subjective. One way to
address this is by having the assistants themselves rate programs by
semantic and technical complexity. However, as this classification
is not a core part of the system but instead a lens through which
to understand its effectiveness on this specific dataset, we felt that
such an undertaking would be outside of the scope of this paper.
We expect human assistants to do best on binaries with a high
semantic complexity, and unskilled humans to do best with binaries
requiring a low technical expertise.
6.2 Human Assistants
HaCRS was designed to support different levels of assistant exper-
tise, from non-experts to experts. We evaluated the impact of both
non-expert and semi-expert assistants.
Non-experts. For the non-experts, we used Amazon’s Mechani-
cal Turk service to dispatch tasklets to humans with no required
Computer Science knowledge [2]. This provided HaCRS with an
API to interact with human intelligence in a scalable way, allowing
Figure 2: A diagram of the HaCRS user interface. Tasklet instruc-
tions were presented to the assistants, who could preview sample
interactions (discovered by prior assistants or by the driller compo-
nent) and use the terminal to create new test cases. The HaCRS pro-
vides symbolic tokens (the dash-bordered interface widget) to help
guide the human intuition of its assistants and presents specific
goals to motivate them.
in the author-provided test cases (we did not otherwise use these
test cases in the experiments). We filtered binaries in this way
because, to our human assistants, such protocols are understandable,
and, therefore, they allow for manual interaction. Among the CGC
binaries, a total of 85 binaries meet this criterion.
While this requirement to filter the dataset to binaries designed
for human interaction is limiting, certain approaches do exist to
alleviate it. For example, IARPA funded a multi-year effort, dubbed
STONESOUP [15] that developed a number of approaches to gamify
software. Such approaches can be used to expand the amount of
binaries with which humans can assist, but they generally fail
Tasklet Instructions-Program Description-Tasklet DirectionsExample Interactions1 2 3 4 5 6 7PAPER> PAPERTIEROCK> SCISSORSYOU LOSEFeedbackScore: 223/1225MINIMUM GOAL MET!Bonuses:- 10 more functions- Output "INVALID"✔ Output "YOU WIN!!!"✔ Output "EASTEREGG!!"TerminalPAPER> 0000EASTER EGG!!!PAPER> SCISSORSYOU WIN!!!CRS-GeneratedSuggestionsEducated Guesses:-ROCK-SCISSORS-LIZARD-SPOCKBrute Force:-~~~!@-0000Session B3:  Investigating AttacksCCS’17, October 30-November 3, 2017, Dallas, TX, USA355it to submit tasklets, as Mechanical Turk Human Intelligence Tasks
(HITs), without concerning itself with human availability.
Because we had finite funds for our experiments, we imple-
mented a human interaction cache. When the HaCRS would create
tasklets for non-expert human assistance, we would first check the
interaction cache to determine if this human assistance task had
already been requested in by a prior experiment. If it had, and if at
least one of the cached human test cases “solved” the tasklet (in the
sense of triggering new code), the HaCRS would reuse it instead of
paying for a HIT. We used the human interaction cache whenever
we were running experiments on identical configurations of the
Hardware-Automation Link. This allowed us to re-run some of the
experiments throughout the design and development of the system
and remain within our budget.
We filtered turkers by success rate (>95%), resulting in 183 turk-
ers solving 802 tasklets at costs between 1and4, scaling with the
"age" of the program in the system, plus a bonus of 2 cents per
extra percentage of discovered edges. Turkers demonstrated a "long
tail" of performance. Our star turker completed 67 tasklets over
48 binaries and contributing to 28 assisted crashes, 8 of which au-
tomation alone didn’t find, versus an average of 4.5 tasklets (across
1.8 binaries) per worker, contributing to 0.3 assisted crashes, 0.1 of
which automation did not find.
Each assistant was presented with the tasklet instructions and the
HAL interface. In the end, between the different experiments to fully
understand our system, we spent about $1,100 on Mechanical Turk
HITs, resulting in 21268 unique test cases across our experiment3.
While this is a large amount for a research lab, it would be trivial
spending for a nation state or large corporation looking to augment
their analysis capabilities.
Semi-experts. We recruited five professionals in Computer Sci-
ence, familiar with programming topics but not with program anal-
ysis or security, to act as our semi-expert human assistants. These
professionals interacted with a random sampling of 23 binaries
from our dataset, generating a total of 115 test cases.
6.3 Human-Automation Link
As we proposed a number of optimizations to the Human-Automation
Link in Section 5.3, it is important to understand whether this actu-
ally enhances the effectiveness of human assistances. To determine
this, we performed two separate experiments in having non-experts
interact with programs in the HAL, with our optimizations in Sec-
tion 5.3 disabled in the first and enabled in the second.
For each binary, we dispatched tasklets to the human assistants
until they were unable to make further progress in code coverage,
given an hour-long timeout. We collated the results by the semantic
complexity of the binaries involved, and computed the median
number of test cases at which progress stopped being made.
Our improvements to the HAL allowed our assistants to con-
tribute a significantly higher amount of test cases than they were
previously able to. For semantically complex binaries, the number
of test cases was roughly double, but for binaries that were not
3A preprint of this paper stated $2,000 as the cost. Amazon Mechanical Turk collects
the full payment for all scheduled HITs up front, and refunds it at a later date if HITs are
not completed. Confusion with the Mechanical Turk interface caused us to mistakenly
leave out the refunded portion in our calculations, resulting in the higher figure.
semantically complex, the improvement was considerably higher,
approach a three-fold increase in the number of successful test case
generations. On further investigation, this makes sense – analyzing
the test cases generated by the human assistants, we were able to
see them quickly guess how to interact with semantically-complex
programs, but struggle with less complex ones. However, with the
improved HAL interface, they were given extra information that
they could leverage to provide high-quality test cases.
6.4 Comparative Evaluation
HaCRS improves the vulnerability detection process by injecting
human intuition into the Cyber Reasoning System. To understand
how effective this is, we analyze the impact that non-expert and
semi-expert assistance has on CRS effectiveness. To explore these
questions, we ran several different experiment configurations:
Non-expert humans. As a baseline to understand the ability of
humans to generate inputs for binary code, we disabled the
automated components of the Mechanical Phish and relied
solely on human assistants for test case creation.
Semi-expert and non-expert humans. With the amount of semi-
experts at our disposal, it did not make sense to have them
work alone. As such, we ran an integrated semi- and non-
expert experiment. To understand the impact of expertise,
we added the semi-experts to our assistant pool and reran
the human-only experiment. Test cases produced by non-
experts are presented to semi-experts as examples, and test
cases created by the semi-experts are synchronized into the
system and eventually presented to the non-experts.
Unassisted fuzzing (AFL). This configuration, with both sym-
bolic and human assistance disabled, achieves a baseline for
comparing the other experiments to understand the relative
gains in code coverage and crashes.
Symbolic-assisted fuzzing (Driller). This is the reference con-
figuration of the Mechanical Phish: a fuzzer aided by a dy-
namic symbolic execution engine, as proposed by Driller. We