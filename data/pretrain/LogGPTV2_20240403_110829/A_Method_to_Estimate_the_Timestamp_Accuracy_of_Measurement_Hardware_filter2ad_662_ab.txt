in Fig. 6. The traﬃc was generated by Source, using the Linux Packet Gen-
erator kernel module on Pentium-4 processor of 2.8 GHz with Linux 2.6 and
1024 MB of RAM. In [1] we show that the generator is stable, i.e., generates
PDUs back-to-back, under these conditions. The traﬃc was then fed to a refer-
ence measurement point that used a DAG 3.5E capture interface, providing the
facility to validate the correct behaviour of the traﬃc generator during the tests.
A wiretap split the signal so that the system under test (SUT) obtained a copy,
while the original traﬃc stream was terminated in the Sink. The SUT was im-
plemented as a capture interface in the measurement point, with the exception
of the Agilent system that uses a custom built PC. Here, we wrote a script that
converted the logs provided by the Agilent software into a format usable by our
analysis software.
The hardware systems were evaluated using 100 000 PDUs, while the software
used both 100 000 and 250 000 PDUs. Furthermore, PCAP, PF RING, MMAP
and ioctl tests were executed at diﬀerent times and on two diﬀerent hardware
platforms, a Pentium-3 664 MHz and a Pentium-4 2.4 GHz. The clocks of the
MPs were synchronised using NTP. However, as we only show the evaluation
over a short time interval the impact of NTP is not obvious. If NTP would have
had to make a time correction this would have been clearly notable as one (or
more) inter-arrival times would have been much larger, or negative. The TSC
test was executed on two diﬀerent Pentium-4’s as described above.
A Method to Estimate the Timestamp Accuracy
203
5 Results
In this conﬁguration the DAG 3.5E card obtained a timestamp accuracy of
59.75 ns, which is equivalent to the card’s clock resolution. The Agilent system
obtained a timestamp accuracy of 100 ns, which matches the reported resolution
of its timestamp. In Fig. 7 the error histogram for the DAG 3.5E is shown, and
Fig. 8 contains the histogram for the Agilent system. In both cases the bin width
was 1 ns and the histograms are of type 1.
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
y
c
n
e
u
q
e
r
F
e
v
i
t
l
a
e
R
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
y
c
n
e
u
q
e
r
F
e
v
i
t
l
a
e
R
0
−150
−100
−50
0
ε [ns]
50
100
150
0
−150
−100
−50
0
ε [ns]
50
100
150
Fig. 7. Error histogram for DAG 3.5E,
bin width 1 ns
Fig. 8. Error histogram for Agilent
J6800/J6830A, bin width 1 ns
In Table 1 the estimated accuracies for PCAP, PF RING and MMAP are
shown. We observe that there is a large diﬀerence between the cases with 100 000
and the 250 000 PDU. Now, since NTP is employed even longer tests should be
considered. But, as the test lengths grow, the synchronisation events will become
very visible and will only demonstrate how good or bad the clock is in the par-
ticular computer. NTP conditions the clock both in time and frequency, and the
time synchronisation can manifest itself with large negative, or positive, inter-
arrival times. A 250 000 PDU test takes approximately ﬁve minutes to execute
and as such it can act as a crude indicator of a system’s accuracy, without too
much interference from the clock synchronisation eﬀects. Now, looking at PCAP
one can see that there is not a very large diﬀerence between the operating sys-
tems for the 100 000 PDU test. In the 250 000 PDU test, FreeBSD performs
signiﬁcantly worse, with TΔ estimations around 3 ms. It is also interesting to
see that for the 100 000 PDU test, the P3 outperforms the P4 with TΔ estima-
tions around 0.2 ms regardless of the operating system. Turning the attention
to PF RING, one can see that it seems to require more processing than PCAP
for the P3, resulting in a worse TΔ value. For the P4, the accuracy relative to
PCAP increases slightly for Linux 2.4, but decreases for Linux 2.6. Now, looking
at MMAP for 100 000 PDUs, the P3 behaves similarly as in the PCAP case.
However, the P4 is much better with a TΔ of 0.023 ms for Linux 2.4 and 0.14 ms
for Linux 2.6. This indicates a quite eﬃcient system. But, when increasing the
test length to 250 000 PDUs, it obtains roughly the same accuracy as PCAP.
204
P. Arlos and M. Fiedler
Table 1. TΔ estimations for PCAP, PF RING and MMAP
Measurement Operating
100 000 PDUs
TΔ [μs]
250 000 PDUs
Software
PCAP
PFRING
MMAP
System
Linux 2.4.29
Linux 2.6.10
FreeBSD 5.3
Linux 2.4
Linux 2.6
Linux 2.4
Linux 2.6
P3-664 MHz P4-2.4 GHz P3-664 MHz P4-2.4 GHz
346
374
3 243
320
440
340
460
720
663
2 760
810
810
540
700
202
218
229
746
730
489
300
415
375
375
296
480
23
137
In Table 2 estimations of TΔ are presented for the raw socket. The P3 exhibits
an interesting behaviour: When changing from Linux 2.4 to 2.6 with NTP, it
almost doubles its accuracy. Overall the accuracy ranges from 0.3 ms for the P4
with Linux 2.4 to 0.71 ms for the P3 and Linux 2.4. In the histograms, the two
peaks are visible at the same places and the shapes are similar to the ones seen
for the PCAP, PF RING and MMAP systems. For the TSC method, for Linux
2.6 the 2.8 GHz obtained the same result as the 2.4 GHz for the ioctl case. But
for the Linux 2.4, the TSC method was worse than the 2.4 GHz ioctl case.
Table 2. TΔ estimations for raw socket
Operating
ioctl
TSC
TΔ [μs]
System P3-664 MHz P4-2.4 GHz P4-2.0 GHz P4-2.8 GHz
N/A
Linux 2.4
Linux 2.6
410
574
N/A
710
330
300
410
In Fig. 9 the error histograms for the ﬁve software measurement systems are
shown, based on the 250 000 PDU test. The histograms have bins that are 10 μs
wide. First, one can see the similarity between all ﬁve systems. Second, the widths
of the histograms are in the same order. All ﬁve histograms consist of ﬁve regions.
The two peaks that stick out are at [−110,−100[ μs and [10, 20[ μs. The left most
peak holds approximately 10 % of the samples, while the other holds around 80 %
of the samples. A probable cause for the peaks is the scheduling by the operating
system. When a detailed analysis is done on the ﬁrst 100 PDUs captured by the
P3 using Linux (both 2.4 and 2.6), approximately six out of seven ε samples are
around 20 μs, while the seventh sample has an ε of approximately −110 μs. This
is clearly a type 2 behaviour, indicating that TΔ should be 130 μs. However, as
the trace grows more ’jitter’ appears, resulting in TΔ estimations up to six times
larger. Now, this jitter is a part of the system and can as such not be ignored.
Hence, its inﬂuence must be included into the accuracy estimation of TΔ.
A Method to Estimate the Timestamp Accuracy
205
P4 Linux 2.4 with NTP
PCAP
PF_RING
MMAP
RAW
TSC
100
10−1
10−2
10−3
10−4
10−5
y
c
n
e
u
q
e
r
F
e
v
i
t
l
a
e
R
10−6
−4
−3
−2
−1
0
ε [ns]
1
2
3
4
x 105
Fig. 9. Error histograms for Linux 2.4 based systems, bin width 10 μs
Another observation is that since all systems have the same peak, they are
all subject to the same scheduling. Now, since the TSC method obtains its
timestamp at the application level, it should be subject to operating system
scheduling with user priority. Now consider the ioctl method that is supposed
to read the timestamp associated with the PDU. This timestamp is assumed to
be set by the kernel, however it shows the same behaviour as the TSC method.
Hence it must be also subject to user scheduling, and not kernel scheduling
as it should. This implies that the ioctl method obtains the timestamp at
the application level under user scheduling constraints. And, since all the other
three systems also show the same behaviour, they most probably collect their
timestamps at the application level as well. Hence, the timestamps does not only
reﬂect what happens on the network but also what happens in the computer.
6 Conclusions
In this paper we presented a promising method to estimate the timestamp ac-
curacy obtained from measurement equipment, both hardware and software.
Knowing the timestamp accuracy of a measurement system is the ﬁrst step in
ﬁguring out the quality of the performance parameters calculated from traces
obtained from that particular system.
Using the method, we evaluated the DAG 3.5E and the Agilent J6800/J6830A
and were able to conﬁrm that the DAG has a timestamp accuracy of 60 ns and
the Agilent has an accuracy of 100 ns. We also evaluated ﬁve software tools for
various software and hardware conﬁgurations. With regards to the software tools
206
P. Arlos and M. Fiedler
tested here, two conclusions can be drawn. First, they timestamp PDUs at the
application level instead of kernel level. Hence, the timestamp does not match
the data delivered to the measurement tool. Second, the estimated timestamp
accuracy TΔ is in the same order of magnitude, around 0.3 ms to 0.8 ms using
Linux and around 3 ms using FreeBSD together with oﬀ-the-shelf components
and software.
To ﬁnd the timestamp accuracy, it is obvious that long evaluations are needed
in order to correctly capture the underlying distribution. However, as the eval-
uation time grows, clock synchronization starts to inﬂuence the results signiﬁ-
cantly. The best approach is to evaluate a system over a time frame that covers
the system’s normal operating hours. This way the worst case accuracy is known
before measurements are performed, and thus, the measurement setup can be
calibrated by choosing components that allow for the desired precision.
Future work will include the construction of a traﬃc generator, implemented
in VHDL, to allow the generation of highly accurate PDU inter-arrival times.
As of now, current best practice is to evaluate the timestamp accuracy of
a measurement system prior to use, and if needed change hardware, operating
system and conﬁguration, to obtain an acceptable timestamp accuracy.
References
1. P. Arlos: On the Quality of Computer Network Measurements, Ph.D. Thesis,
Blekinge Institute of Technology, 2005.
2. A. A. Ali, F. Michaut and F. Lepage: End-to-end Availible Bandwidth Measure-
ment Tools: A Comparative Evaluation of Performance, Proc. 4th International
Workshop on Internet Performance, Simulation, Monitoring and Measurement,
2006.
3. Endace Measurement Systems:http://www.endace.com, verif. Jan/2007.
4. S. Donnelly: High Precision Timeing in Passive Measurements of Data Networks,
Ph.D. Thesis, The University of Waikato, 2002.
5. TCPDUMP:Homepage. http://www.tcpdump.org, verif. Jan/2007.
6. L. Deri: PF RING: Passive Packet Capture on Linux at High Speeds,
http://www.ntop.org/PF RING.html, veriﬁed in Jan/2007.
7. P. Wood: Memory Mapping for PCAP, http://public.lanl.gov/cpw/, verif.
Jan/2007.
8. D. Veitch and S. Babu and A. P´asztor:Robust Synchronization of Software Clocks
Across the Internet, Proc. Internet Measurement Conference, 2004.
9. P. Arlos and M. Fiedler: A comparison of Measurement Accuracy for DAG, Tcp-
dump and Windump. http://www.its.bth.se/staﬀ/pca/aCMA.pdf, verif. Jan/2007.
10. WINDUMP:Homepage. http://www.winpcap.org/windump/, verif. Oct/2006.
11. P. Arlos, M. Fiedler and A. Nilsson: A Distributed Passive Measurement Infras-
tructure, Proc. Passive and Active Measurement Workshop, 2005.
12. D. Buchla and W. McLachlan: Applied Electronic Instrumentation and Measure-
ment, ISBN 0-675-21162-X, 1992.