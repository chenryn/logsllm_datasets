(cid:88)

(cid:88)
PIC & PDC
N/A
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

Supported Programs
C & C++
stripped
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

N/A
(cid:88)
(cid:88)
(cid:88)
(cid:88)


PE32+
N/A
(cid:88)
(cid:88)
(cid:88)
(cid:88)


inline
replay
inline
inline
inline
tramp.
tramp.
Table 2: A qualitative comparison of the leading coverage-tracing methodologies currently used in binary-only coverage-guided fuzzing, alongside compiler
instrumentation (LLVM). No existing approaches are able to support compiler-quality transformation at compiler-level speed and generalizability.
(2) dynamic binary translation, or (3) static binary rewriting.
Below we brieﬂy detail each, and weigh their implications
with respect to supporting the extension of compiler-quality
transformation to binary-only fuzzing.
• Hardware-assisted Tracing. Newer processors are offer-
ing mechanisms that facilitate binary code coverage (e.g.,
Intel PT [48]). Fuzzing implementations are burdened by
the need for costly trace post-processing, which reportedly
incurs overheads as high as 50% over compilers [7,20]; but
despite some optimistic performance improvements [37],
hardware-assisted tracing currently remains incapable of
modifying programs—and hence fails to support fuzzing-
enhancing program transformation.
• Dynamic Binary Translators. Dynamic translators apply
coverage-tracing on-the-ﬂy as the target is executing (e.g.,
DynamoRIO [43], PIN [56], and QEMU [8]). Translators
generally support many architectures and binary characteris-
tics; and offer deep introspection that simpliﬁes analysis and
transformation [31, 93]. However, existing dynamic trans-
lators attain the worst-known fuzzing performance: recent
work shows AFL-QEMU’s average overhead is well over
600% [62], and AFL-DynamoRIO [43] and AFL-PIN [45]
report overheads of up to 10x and 100x higher, respectively.
• Static Binary Rewriters. Static rewriting improves per-
formance by modifying binaries prior to runtime (e.g.,
Dyninst [44]). Unfortunately, static rewriting options for
binary-only fuzzing are limited. AFL-Dyninst is the most
popular, but sees prohibitively-high fuzzing overheads
of over 500% [62] and is restricted to Linux programs.
RetroWrite suggests reassembleable-assembly is more per-
formant and viable, but it relies on AFL’s assembly-time in-
strumentation which is both unsupportive of transformation
and reportedly 10–100% slower than compile-time instru-
mentation [93]; and moreover, it does not overcome the gen-
eralizability challenges of prior attempts at reassembleable-
assembly (e.g., Uroboros [87], Ramblr [86]), and is hence
limited to position-independent Linux C programs. Neither
scale well to stripped binaries.
As summarized in Table 2, the prevailing binary-only
fuzzing coverage-tracing approaches are limited in achieving
compiler-quality fuzzing instrumentation. Hardware-assisted
tracing (Intel PT) is incompatible with program instrumen-
tation/transformation and adds post-processing overhead.
Dynamic translators (DynamoRIO, PIN, and QEMU) all
face orders-of-magnitude worse overheads. Static rewriters
(Dyninst and RetroWrite) fail to uphold both performance and
transformation and are unsupportive of Windows software
(the most popular being PE32+), common binary characteris-
tics (e.g., position-dependent code), or the simplest obfusca-
tion techniques (i.e., stripped binaries).
These limitations make fuzzing-enhancing transformations
scarce in binary-only fuzzing. To our knowledge the only
two such implementations exist atop of AFL-Dyninst (instruc-
tion pruning [44]) and AFL-PIN (context sensitivity [92])—
both suffering from the central ﬂaw that any of their poten-
tial beneﬁts are outweighed by the steep overheads of their
respective binary instrumenters (over 500% and 10,000%,
respectively [45, 62]).
Impetus: Current binary instrumenters are fundamentally ill-
equipped to support compiler-quality fuzzing instrumentation.
We envision a world where binary-only and compiler-based
fuzzing are not segregated by capabilities; thus we design a
binary-only fuzzing instrumentation platform capable of perfor-
mant compiler-quality transformation.
4.2 Fundamental Design Considerations
Our analysis of how compilers support performant program
transformations reveals four critical design decisions: (1)
rewriting versus translation, (2) inlining versus tram-
polining, (3) register allocation, and (4) real-world scala-
bility. Below we discuss the signiﬁcance of each, and build
a criteria of the instrumenter characteristics best-suited to
compiler-quality instrumentation.
• Consideration 1: Rewriting versus Translation. Dy-
namic translation processes a target binary’s source instruc-
tion stream as it is executed, generally by means of em-
ulation [8]. Unfortunately, this requires heavy-lifting to
interpret target instructions to the host architecture; and in-
curs signiﬁcant runtime overhead, as evidenced by the poor
performance of AFL-DynamoRIO/PIN/QEMU [43, 45, 93].
While translation does facilitate transformations like sub-
instruction proﬁling [31], static binary rewriting is a more
1686    30th USENIX Security Symposium
USENIX Association
viable approach for fuzzing due to its signiﬁcantly lower
overhead. Like compilers, static binary rewriting performs
all analyses (e.g., control-ﬂow recovery, code/data disam-
biguation, instrumentation) prior to target execution, avoid-
ing the costly runtime effort of dynamic translation. Thus,
static rewriting is the most compatible with achieving
compiler-quality speed in binary-only fuzzing.
Criterion 1: Instrumentation added via static rewriting.
• Consideration 2: Inlining versus Trampolining. A sec-
ond concern is how instrumentation code (e.g., coverage-
tracing) is invoked. Instrumenters generally adopt one of
two techniques: trampolining or inlining. Trampolining
refers to invocation via jumping to a separate payload func-
tion containing the instrumentation. This requires two trans-
fers: one to the payload, and another back to the callee.
However, the total instructions needed to accommodate this
redirection is signiﬁcant relative to a basic block’s size;
and their overhead accumulation quickly becomes prob-
lematic for fuzzing. Modern compilers inline, injecting
instrumentation directly within target basic blocks. Inlining
offers the least-invasive invocation as instrumentation is
launched via contiguous instruction execution rather than
through redirection. We thus believe that inlining is essen-
tial to minimize fuzzing instrumentation’s runtime overhead
and achieve compiler-quality speed in binary-only fuzzing.
Criterion 2: Instrumentation is invoked via inlining.
• Consideration 3: Register Allocation. Memory access is
a persistent bottleneck to performance. On architectures
with a ﬁnite set of CPU registers (e.g., x86), generating fast
code necessitates meticulous register allocation to avoid
clobbering occupied registers. Condition code registers
(e.g., x86’s eflags) are particularly critical as it is common
to modify them; but saving/restoring them to their origi-
nal state requires pushing to the stack and is thus ∼10x
slower than for other registers. Compilers track register
liveness to avoid saving/restoring dead (untouched) con-
dition code registers as much as possible. Smart register
allocation is thus imperative to attaining compiler-quality
binary instrumentation speed.
Criterion 3: Must facilitate register liveness tracking.
• Consideration 4: Real-world Scalability. Modern com-
pilers support a variety of compiled languages, binary char-
acteristics, and platforms. While dynamic translators (e.g.,
DynamoRIO, QEMU, PIN) are comparably ﬂexible be-
cause of their reliance on emulation techniques, existing
static rewriters have proven far less reliable: some require
binaries be written in C despite the fact that developers are
increasingly turning to C++ [26,86,87]. others apply to only
position-independent (i.e., relocatable) code and neglect
the bulk of software that remains position-dependent [26];
many presume access to debugging symbols (i.e., non-
stripped) but this seldom holds true when fuzzing propri-
etary software [44]; and most are only Linux-compatible,
leaving some of the world’s most popular commodity soft-
ware (Windows 64-bit PE32+) unsupported [26, 44, 86, 87].
A compiler-quality binary-only fuzzing instrumenter must
therefore support these garden-variety closed-source binary
characteristics and formats.
Criterion 4: Support common binary formats and platforms.
While binary instrumenters have properties useful to many
non-fuzzing domains (e.g., analysis, emulation, and proﬁling),
attaining compiler-quality fuzzing instrumentation hinges
on satisfying four core design criteria: (C1) static rewrit-
ing, (C2) inlining, (C3) register liveness, and (C4) broad bi-
nary support. Hardware-assisted tracing cannot modify pro-
grams and hence violates criteria (C1)–(C3). DynamoRIO,
PIN, and QEMU adopt dynamic translation (C1) and thus
incur orders-of-magnitude performance penalties—before ap-
plying any feedback-enhancing transformation. Dyninst and
RetroWrite embrace static rewriting but both rely on costlier
trampoline-based invocation (C2) and fail to support com-
modity binary formats and characteristics (C4); and moreover,
Dyninst’s liveness-aware instrumentation failed on our evalua-
tion benchmarks (C3). Thus, compiler-quality instrumentation
in a binary-only context demands a new approach that satisﬁes
all four criteria.
5 The ZAFL Platform
Fuzzing effectiveness severely declines on closed-source tar-
gets. Recent efforts capitalize on compiler instrumentation
to apply state-of-the-art fuzzing-enhancing program transfor-
mations; however, current binary-only fuzzing instrumenters
are ineffective at this. As practitioners are often restricted to
binary-only fuzzing for proprietary or commercial software,
any hope of advancing binary-only fuzzing beseeches efforts
to bridge the gap between source-available and binary-only
fuzzing instrumentation.
To combat this disparity we introduce ZAFL: a compiler-
quality instrumenter for x86-64 binary fuzzing. ZAFL ex-
tends the rich capabilities of compiler-style instrumentation—
with compiler-level throughput—to closed-source fuzzing tar-
gets of any size and complexity. Inspired by recent compiler-
based fuzzing advancements (§ 3), ZAFL streamlines instru-
mentation through four extensible phases, facilitating intu-
itive implementation and layering of state-of-the-art fuzzing-
enhancing program transformations. Below we detail ZAFL’s
internal architecture and guiding design principles.
USENIX Association
30th USENIX Security Symposium    1687
Figure 2: A high-level depiction of the ZAFL platform architecture and its four ZAX transformation and instrumentation phases.
5.1 Design Overview
As shown in Figure 2, ZAFL consists of two primary com-
ponents (1) a static rewriting engine and (2) ZAX: our four
IR-modifying phases for integrating compiler-quality instru-
mentation and fuzzing enhancements. Given a target binary,
ZAFL operates as follows:
1. IR Extraction. From our (or any compatible) binary
rewriter, ZAFL requests an intermediate representation (IR)
of the target binary.
2. ZAX. The resulting IR is then passed to ZAX’s four trans-
formation and instrumentation phases:
P1: Optimization,
P2: Analysis,
P3: Point Selection, and
P4: Application.
3. Binary Reconstitution. After ZAX applies program trans-
formations and instrumentation at IR-level, ZAFL transfers
the modiﬁed IR back to the rewriting engine which gener-
ates the output binary for fuzzing.
5.1.1 Static Rewriting Engine
ZAFL interacts with the binary rewriter of choice to ﬁrst trans-
late the target binary to an intermediate representation (IR) for
subsequent processing in ZAX; and secondly, to reconstitute
an output binary from the ZAX-modiﬁed IR.
We initially considered re-purposing LLVM IR-based
rewriter McSema [25] due to its maturity and popularity in the
static rewriting community, but ultimately ruled it out as both
the literature [29] and our own preliminary evaluation reveal
that it is a poor ﬁt for fuzzing due to its high baseline overhead.
Instead, for our prototype, we extend the GCC IR-inspired
static rewriter Zipr [41, 46] as it meets the same criteria that
McSema does (§ 4.2), but has better baseline performance.
5.2 The ZAX Transformation Architecture
Once target IR construction is ﬁnished, ZAFL initiates ZAX:
our fuzzing instrumentation toolchain. Below we describe the
intricacies of ZAX’s four core phases: (1) Optimization, (2)
Analysis, (3) Point Selection, and (4) Application.
5.2.1 Optimization
ZAX’s ﬁrst phase enables transformations that reduce the mu-
tation effort required to fuzz-through deeper code regions
(e.g., sub-instruction proﬁling). Given a pre-speciﬁed opti-
mization criteria (e.g., “decompose multi-byte conditional
constraints”), it scans the target binary’s control-ﬂow graph to
identify sections of interest; and for every match, it applies the
relevant IR-level transformations. As such transformations
alter control-ﬂow, we apply them before further analyses that
depend on the ﬁnalized control-ﬂow graph.
5.2.2 Analysis
With the optimized control-ﬂow graph in hand, ZAX’s sec-
ond phase computes meta-characteristics (e.g., predecessor-
successor, data-ﬂow, and dominance relationships). We model
this after existing compiler mechanisms [3, 24, 61], and to fa-
cilitate integration of other desirable analyses appearing in
the literature [2, 81]. The extent of possible analyses depends
on the rewriter’s IR; for example, low-level IR’s modeled
after GCC’s RTL [34] permit intuitive analysis to infer regis-
ter liveness; and other IRs may support equivalent analyses
which could be used instead, but if not, such algorithms are
well-known [61] and could be added to support ZAX.
5.2.3 Point Selection
ZAX’s third phase aims to identify where in the program to
instrument. Given the binary’s full control-ﬂow graph and
meta-characteristic data (e.g., liveness, dominator trees), this
phase enumerates all candidate basic blocks and culls those
deemed unnecessary for future instrumentation. ZAX’s CFG-
aware instrumentation pruning capabilities facilitate easy im-
plementation of compiler-based techniques described in § 3.
1688    30th USENIX Security Symposium
USENIX Association
P1: Control-Flow Opts.OutputBinaryOriginalBinaryBinary RewriterP2: Control-Flow AnalysisThe ZAFL PlatformP3:Inst.PointSelectionStatic Rewriting ComponentBuild BinaryRepresentation IR Data StructModiﬁed IROriginal IRReconstitute Output Binary r1; r2; r3r0; r1; r2r2; r3ZAX Transform & Inst. PhasesP4:Inst.Application Edge Cov.Block Cov.r1; r2; r3r0; r1; r2r2; r3Selection,Liveness,Inst.Templates➡Apply InstrumentationMeta-characteristicData➡Location SelectionSpecify Analyses ➡Extract Meta-characteristicsSpecify Optimizations ➡ Optimized Control-ﬂow Graph Performance Transformation
Single Successor-based Pruning
Dominator-based Pruning
Instrumentation Downgrading
[31]
[47]
[32]
Feedback Transformation
Sub-instruction Proﬁling
Context-sensitive Coverage
[1, 31, 51, 75]
[18, 31]
are not their function’s entry, but are the single successor to
their parent block [44]. Intuitively, these are guaranteed to be
covered as they are preceded by unconditional transfer and
thus, their instrumentation is redundant. Our implementation
applies a meta-characteristic predecessor-successor analysis
in ZAX’s Analysis phase; and a location selector during Point
Selection to omit basic blocks accordingly.
Table 3: A catalog of ZAFL-implemented compiler-quality fuzzing-enhancing
program transformations and their compiler-based origins.
6.1.2 Dominator Tree Instrumentation Pruning
5.2.4 Application
Finally, ZAX’s applies the desired instrumentation conﬁgu-
ration (e.g., block or edge coverage tracking). A challenge
is identifying how to instrument each location; ensuring cor-
rect execution requires precise handling of registers around
instrumentation code—necessitating careful consideration of
liveness. As a block’s instrumentation can theoretically be po-
sitioned anywhere within it, liveness analysis also facilitates
“best-ﬁt” location ranking by quantity of free registers; and
since restoring condition code registers (e.g., x86’s eflags)
is often costlier than others, we further prioritize locations
where these are free. Thus, ZAX’s efﬁciency-maximizing
instrumentation insertion is comparable to that of modern
compilers [34,53]. Though our current prototype (§ 6) targets
AFL-style fuzzers, support for others is possible through new
instrumentation conﬁgurations.
6 Extending Compiler-quality Transforms
to Binary-only Fuzzing
We review successful compiler-based fuzzing approaches
and identify impactful fuzzing performance- and feedback-
enhancing program transformations. As these transformations
provably improve compiler-based fuzzers they thus are de-
sirable for closed-source targets; however, they are largely
neglected due to current binary instrumenters’ limitations.
To show the power of ZAFL in applying and layering
transformations ad-hoc, we extend three performance- and
two feedback-enhancing compiler-based transformations to
binary-only fuzzing, shown in Table 3. Below details our
implementations of these ﬁve transformations using ZAFL.
6.1 Performance-enhancing Transformations
We leverage ZAFL’s ZAX architecture in deploying three
fuzzing performance-enhancing program transformations:
single successor and dominator-based instrumentation
pruning, and edge instrumentation downgrading. We de-
scribe our implementation of each below.
6.1.1 Single Successor Instrumentation Pruning
Recent fuzzing works leverage ﬂow graph reducibility tech-
niques [42, 77] to cut down instrumentation overhead [47].
We borrow AFL-Dyninst’s omitting of basic blocks which
Tikir and Hollingsworth [81] expand on single predecessor/-
successor pruning by evaluating control-ﬂow dominator re-
lationships. A node A “dominates” B if and only if every
possible path to B contains A [2]. Dominator-aware instru-
mentation audits the control-ﬂow graph’s corresponding dom-
inator tree to consider nodes that are a dominator tree leaf, or
precede another node in control-ﬂow but do not dominate it.
In line with our other CFG-aware pruning, we implement a
dominator tree meta-characteristic in ZAX’s Analysis phase;
and a corresponding selector within Point Selection. Our anal-
ysis reveals this omits 30–50% of blocks from instrumenta-
tion. We elect to apply Tikir and Hollingsworth’s algorithm
because it balances graph reduction and analysis effort. Other