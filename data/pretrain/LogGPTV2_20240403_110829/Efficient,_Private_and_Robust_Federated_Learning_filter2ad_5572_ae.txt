wrap0 and wrap1, respectively.
2: SP and CS invoke an instance of FMill, in which SPâ€™s input is
(p âˆ’ 1 âˆ’ âŸ¨xâŸ©0) and CSâ€™s input is (p âˆ’ 1)/2 + âŸ¨xâŸ©1. After that SP
and CS learn lwrap0 and lwrap1, respectively.
3: SP and CS invoke an instance of FMill, in which SPâ€™s input is
(p + (p âˆ’ 1)/2 âˆ’ âŸ¨xâŸ©0) and CSâ€™s input is âŸ¨xâŸ©1. After that SP and
CS learn âŸ¨rwrapâŸ©0 and âŸ¨rwrapâŸ©1, respectively.
4: SP and CS invoke an instance of FAND, in which the inputs of
CS and SP are âŸ¨wrapâŸ© and âŸ¨lwrapâŸ©, and learn âŸ¨zlâŸ©.
5: SP and CS invoke an instance of FAND, in which the inputs of
CS and SP are âŸ¨wrapâŸ© and âŸ¨rwrapâŸ©, and learn âŸ¨zrâŸ©.
6: SP and CS output 1âŠ•âŸ¨zlâŸ©0âŠ•âŸ¨zrâŸ©0 and âŸ¨zlâŸ©1âŠ•âŸ¨zrâŸ©1, respectively.
Similar to our validity checking setting, Gibbs et al. presented a
zero knowledge based input validation protocol (SNIP) [13], where
each party needs to generate a SNIP proof which will be used by
servers to validate the input. However, the SNIP does not work
in our SecureFL, since the proof generation on parties is time-
consuming especially with the large dimension of submission,
which runs counter to our goal of efficient protocols for parties.
Moreover, the result of validation is leaked in their protocol, but
rather our SecureFL provides full privacy protection including the
result of validity checking.
Cosine similarity computation. Benefiting from the matrix
multiplication shares generated in the preamble phase in Figure 2,
cosine similarity computations can be non-interactively performed
over secret-shared data without invoking heavy cryptographic
protocols like OT and PLHE.
â€¢ At the beginning of this step, CS holds Î´ generated in the
preamble phase, and we set âŸ¨cosiâŸ©1 = Î´[i], âˆ€i âˆˆ [n]. Then,
SP computes temp = âŸ¨RâŸ©0Ğ´s + âŸ¨RâŸ©1Ğ´s âˆ’ Î´, and sets âŸ¨cosiâŸ©0 =
temp[i], âˆ€i âˆˆ [n]. As such, the cosine similarity âŸ¨cosiâŸ© be-
tween the local gradient Ğ´i and the server gradient Ğ´s is
computed, since cosi = RĞ´s[i] = Ğ´T
i Ğ´s.
Note that an alternative to evaluate this step is to use the Beaverâ€™s
multiplication technique. However, compared to our preprocessing-
based evaluation, for each cosine similarity computation it con-
sumes (|Ğ´i| + |Ğ´s|)-bits communication and about 6Ã— more compu-
tation overhead during the online phase.
Trust score computation. In this step, the ReLU operation
and the boolean-integer multiplication are performed. Specifically,
we leverage the DReLU protocol in Algorithm 2 as the building
block to construct our ReLU protocol, as shown in Algorithm 3.
Besides, we develop an efficient boolean-integer product protocol
in Algorithm 4 based on COT techniques, which effectively reduces
the communication cost by half.
Algorithm 3 The protocol of ReLU
Input: SP and CS hold âŸ¨xâŸ©0 and âŸ¨xâŸ©1, respectively.
Output: SP and CS get âŸ¨ReLU(x)âŸ©0 and âŸ¨ReLU(x)âŸ©1
1: SP and CS invoke FDReLU with input âŸ¨xâŸ© to learn output âŸ¨yâŸ©B.
2: SP and CS invoke FBmulA in Algorithm 4 with input âŸ¨xâŸ© and
âŸ¨yâŸ©B to learn output âŸ¨zâŸ©, and sets âŸ¨ReLU(x)âŸ© = âŸ¨zâŸ©.
â€¢ SP and CS run the ReLU procedure for each i âˆˆ n, where
the inputs are âŸ¨cosiâŸ©0 and âŸ¨cosiâŸ©1, respectively. After that,
SP and CS learn âŸ¨ReLU(cosi)âŸ©0 and âŸ¨ReLU(cosi)âŸ©1.
â€¢ SP and CS run the boolean-integer multiplication procedure
to evaluate âŸ¨T SiâŸ© = âŸ¨f laĞ´iâŸ©B Â· âŸ¨ReLU(cosi)âŸ©. At the end of
the procedure, SP holds âŸ¨T SiâŸ©0 and CS holds âŸ¨T SiâŸ©1.
3: The parties run COT(f 1
0 âŸ¨aâŸ©0 + (1 âˆ’ âŸ¨bâŸ©B
1 )âŸ¨aâŸ©1, respectively.
2: The parties run COT(f 0
âŸ¨bâŸ©B
âŸ¨bâŸ©B
and SP obtains x while CS obtains y.
Algorithm 4 Secure Boolean-Integer Multiplication
Input: Additive shares of a âˆˆ Zp and boolean shares of b âˆˆ Z2
Output: Additive shares of c = ab âˆˆ Zp
1: SP and CS construct correlation functions f 0
cor(x) = x âˆ’
0 )âŸ¨aâŸ©0 and f 1
cor(x) = x âˆ’ âŸ¨bâŸ©B
1 âŸ¨aâŸ©1 + (1 âˆ’
cor , âŸ¨bâŸ©B
1 ) with SP acting as the sender,
cor , âŸ¨bâŸ©B
0 ) with CS acting as the sender,
0 âŸ¨aâŸ©0 âˆ’ x + yâ€².
1 âŸ¨aâŸ©1 âˆ’ xâ€² + y.
and CS obtains xâ€² while SP obtains yâ€².
4: SP sets âŸ¨câŸ©0 = âŸ¨câŸ©B
5: CS sets âŸ¨câŸ©1 = âŸ¨câŸ©B
Weighted aggregation. At the core of this step is a 2PC sub-
protocol for computing the weighted aggregation of the normalized
local gradients, i.e., evaluating scalar-vector product T Si Â· Ğ´i. The
challenge here is that previous methods evaluating T Si Â· Ğ´i require
2ld + 2l sent bits, where d is the dimension of each gradient and l is
the bit length of each component. To solve such challenge, as shown
in Figure 3, we develop a specialized scalar-vector product protocol,
inspired by Beaverâ€™s triples in the matrix form [36]. The key idea
is that the same ai masking the local gradient Ğ´i in the validity
checking can be reused to hide the same Ğ´i in the scalar-vector
product evaluation. Its security is guaranteed by the security of the
multiplication triples in the matrix form [36]. For completeness,
we give a sketch of the security proof in Appendix B. Note that
Beaverâ€™s triples cannot be used to mask Ğ´iâ€™s in different iterations
for security. As a result, the bandwidth requirement of our solution
is 2l, an 2ld +2l
= d + 1 improvement. This reduction is nontrivial
especially for state-of-the-art neural networks such as ResNets [23],
where d is in the order of millions.
2l
â€¢ SP and CS run the Beaverâ€™s multiplication procedure to eval-
uate âŸ¨T SiĞ´iâŸ©. At the end of the procedure, SP holds âŸ¨T SiĞ´iâŸ©0
and CS holds âŸ¨T SiĞ´iâŸ©1.
â€¢ SP and CS compute âŸ¨iâˆˆ[n] T SiĞ´iâŸ©0 and âŸ¨iâˆˆ[n] T SiĞ´iâŸ©1, re-
spectively. After that, CS sends âŸ¨iâˆˆ[n] T SiĞ´iâŸ©1 to SP, which
reconstructsiâˆˆ[n] T SiĞ´i.
(1) Our SecureFL is robust to parties dropping out. In many FL
systems, participants are mainly composed of resource-constrained
Remark 2.
52Efficient, Private and Robust Federated Learning
ACSAC â€™21, December 6â€“10, 2021, Virtual Event, USA
Table 2: The SecureFL Framework
Efficient,PrivateandRobustFederatedLearningACSACâ€™21,December6â€“10,2021,VirtualEvent,USATable2:TheSecureFLFrameworkPARAMETERS:â€¢Numberofpartiesğ‘›,dimensionofeachgradientğ‘‘.IdealprimitivesFMult,FBeaver,FDReLU,FReLU,FANDandFBmulA.INPUT:â€¢Eachpartyğ‘ƒğ‘–withlocaldatasetsDğ‘–,ğ‘–âˆˆ[ğ‘›],SPwithseeddatasetDğ‘ .PROTOCOL:I.Initialization://Partiesside.a.Allpartiesinitializethearchitectureğ¹andweightsğoftheglobalmodel.b.Eachpartygeneratesaprivateseedkeyğ‘˜ğ‘ ğ‘’ğ‘’ğ‘‘ğ‘–withCSviaexchangingDiffie-Hellmanpublickeysandengaginginakeyagreement.//Serversside.a.SPrunsPLHE.KeyGen(1ğ‘˜)â†’(ğ‘ğ‘˜,ğ‘ ğ‘˜)andsendsthepublickeyğ‘ğ‘˜toCS.b.SPandCSruntheBeaverâ€™striplesgenerationprotocolFBeavertogenerateBeaverâ€™smultiplicationtriples.II.Training:RepeatthestepsII-IVuntilthestoppingcriterion.//Partiesside.a.Eachpartyğ‘ƒğ‘–runsğ‘†ğºğ·(ğ,Dğ‘–,ğ‘)â†’ğ’ˆğ‘–,andcomputesthenormalizedlocalgradientğ’ˆğ‘–â†ğ’ˆğ‘–âˆ¥ğ’ˆğ‘–âˆ¥.b.Eachpartyğ‘ƒğ‘–generatesğ’“ğ‘–=PRG(ğ‘˜ğ‘ ğ‘’ğ‘’ğ‘‘ğ‘–),setsâŸ¨ğ’ˆğ‘–âŸ©1=ğ’“ğ‘–andcomputesâŸ¨ğ’ˆğ‘–âŸ©0=ğ’ˆğ‘–âˆ’ğ’“ğ‘–.//Serversside.a.SPrunsğ‘†ğºğ·(ğ,Dğ‘ )â†’ğ’ˆğ‘ ,andcomputesthenormalizedservergradientğ’ˆğ‘ â†ğ’ˆğ‘ âˆ¥ğ’ˆğ‘ âˆ¥.b.SPcomputesğ¸(ğ’ˆğ‘ )=PLHE.Enc(ğ‘ğ‘˜,ğ’ˆğ‘ )andsendsittoCS.c.CSgeneratesâŸ¨ğ’ˆğ‘–âŸ©1=PRG(ğ‘˜ğ‘ ğ‘’ğ‘’ğ‘‘ğ‘–),âˆ€ğ‘–âˆˆ[ğ‘›],andsetsâŸ¨ğ‘…âŸ©1=(âŸ¨ğ’ˆ1âŸ©1,âŸ¨ğ’ˆ2âŸ©1,Â·Â·Â·,âŸ¨ğ’ˆğ‘›âŸ©1)ğ‘‡.d.CSsamplesarandomvectorğœ¹,performsPLHE.Eval(ğ‘ğ‘˜,ğ¸(ğ’ˆğ‘ ),âŸ¨ğ‘…âŸ©1)â†’ğ¸(ğ‘ğ‘˜,âŸ¨ğ‘…âŸ©1ğ’ˆğ‘ âˆ’ğœ¹)usingthedevisedmatrixmultiplicationprocedure,andsendsğ¸(ğ‘ğ‘˜,âŸ¨ğ‘…âŸ©1ğ’ˆğ‘ âˆ’ğœ¹)toSP.Besides,CSsetsâŸ¨ğ‘ğ‘œğ‘ ğ‘–âŸ©1=ğœ¹[ğ‘–],âˆ€ğ‘–âˆˆ[ğ‘›].e.SPdecryptstheaboveciphertextstoobtainâŸ¨ğ‘…âŸ©1ğ’ˆğ‘ âˆ’ğœ¹.III.Aggregation://Partiesside.a.Eachpartyğ‘ƒğ‘–sharesâŸ¨ğ’ˆğ‘–âŸ©0=ğ’ˆğ‘–âˆ’ğ’“ğ‘–toSP.//Serversside.a.SPandCSinvokeaninstanceofFMultforeachğ‘–âˆˆ[ğ‘›],whereSPâ€™sinputisâŸ¨ğ’ˆğ‘–âŸ©0andCSâ€™sinputisâŸ¨ğ’ˆğ‘–âŸ©1.SPandCSlearnâŸ¨âˆ¥ğ’ˆğ‘–âˆ¥2âŸ©0andâŸ¨âˆ¥ğ’ˆğ‘–âˆ¥2âŸ©1,respectively.b.SPandCSinvoketwoinstancesofFDReLUforeachğ‘–âˆˆ[ğ‘›],wheretheinputsareâŸ¨âˆ¥ğ’ˆğ‘–âˆ¥2+ğœ–âˆ’1âŸ©andâŸ¨ğœ–+1âˆ’âˆ¥ğ’ˆğ‘–âˆ¥2âŸ©,respectively.SPandCSlearnâŸ¨ğ‘“ğ‘™ğ‘ğ‘”ğ‘–,0âŸ©ğµandâŸ¨ğ‘“ğ‘™ğ‘ğ‘”ğ‘–,1âŸ©ğµ,respectively.c.SPandCSinvokeaninstanceofFANDforeachğ‘–âˆˆ[ğ‘›],wheretheinputsareâŸ¨ğ‘“ğ‘™ğ‘ğ‘”ğ‘–,0âŸ©ğµandâŸ¨ğ‘“ğ‘™ğ‘ğ‘”ğ‘–,1âŸ©ğµ,respectively.SPandCSlearnâŸ¨ğ‘“ğ‘™ğ‘ğ‘”ğ‘–âŸ©ğµ0andâŸ¨ğ‘“ğ‘™ğ‘ğ‘”ğ‘–âŸ©ğµ1,respectively.d.SPsetsâŸ¨ğ‘…âŸ©0=(âŸ¨ğ’ˆ1âŸ©0,âŸ¨ğ’ˆ2âŸ©0,Â·Â·Â·,âŸ¨ğ’ˆğ‘›âŸ©0)ğ‘‡,computesğ‘¡ğ‘’ğ‘šğ‘=âŸ¨ğ‘…âŸ©0ğ’ˆ+âŸ¨ğ‘…âŸ©1ğ’ˆâˆ’ğœ¹,andsetsâŸ¨ğ‘ğ‘œğ‘ ğ‘–âŸ©0=ğ‘¡ğ‘’ğ‘šğ‘[ğ‘–],âˆ€ğ‘–âˆˆ[ğ‘›].e.SPandCSinvokeaninstanceofFReLUforeachğ‘–âˆˆ[ğ‘›],wheretheinputsareâŸ¨ğ‘ğ‘œğ‘ ğ‘–âŸ©0andâŸ¨ğ‘ğ‘œğ‘ ğ‘–âŸ©1,respectively.SPandCSlearnâŸ¨ğ‘…ğ‘’ğ¿ğ‘ˆ(ğ‘ğ‘œğ‘ ğ‘–)âŸ©0andâŸ¨ğ‘…ğ‘’ğ¿ğ‘ˆ(ğ‘ğ‘œğ‘ ğ‘–)âŸ©1.f.SPandCSinvokeaninstanceofFBmulAforeachğ‘–âˆˆ[ğ‘›],wheretheinputsareâŸ¨ğ‘…ğ‘’ğ¿ğ‘ˆ(ğ‘ğ‘œğ‘ ğ‘–)âŸ©andâŸ¨ğ‘“ğ‘™ğ‘ğ‘”ğ‘–âŸ©ğµ,respectively.SPandCSlearnâŸ¨ğ‘‡ğ‘†ğ‘–âŸ©0andâŸ¨ğ‘‡ğ‘†ğ‘–âŸ©1.g.SPandCScomputeâŸ¨ğ‘‡ğ‘†âŸ©=ğ‘›ğ‘–=1âŸ¨ğ‘‡ğ‘†ğ‘–âŸ©,locally.h.SPandCSinvokeaninstanceofFMultforeachğ‘–âˆˆ[ğ‘›],wheretheinputsareâŸ¨ğ’ˆğ‘–âŸ©andâŸ¨ğ‘‡ğ‘†ğ‘–âŸ©,respectively.SPandCSlearnâŸ¨ğ‘‡ğ‘†ğ‘–ğ’ˆğ‘–âŸ©0andâŸ¨ğ‘‡ğ‘†ğ‘–ğ’ˆğ‘–âŸ©1.i.SPandCScomputeâŸ¨ğ’ˆâŸ©=ğ‘›ğ‘–=1âŸ¨ğ‘‡ğ‘†ğ‘–ğ’ˆğ‘–âŸ©locally.j.CSsendsâŸ¨ğ‘‡ğ‘†âŸ©1andâŸ¨ğ’ˆâŸ©1toSP.Afterthat,SPreconstructsğ‘‡ğ‘†andğ’ˆ,andcomputesğ’ˆğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™=âˆ¥ğ’ˆğ‘ âˆ¥ğ‘‡ğ‘†ğ’ˆ.IV.Broadcast://Serversside.a.SPupdatestheglobalweightğâ†ğâˆ’ğœ‚ğ’ˆğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™,andbroadcastsittoallparties.//Partiesside.a.Eachpartyğ‘ƒğ‘–updatesitslocalmodelbyutilizingtheglobalweightğreceived.53ACSAC â€™21, December 6â€“10, 2021, Virtual Event, USA
Meng Hao, et al.
mobile devices, so they are likely to drop in each round of aggrega-
tion. However, CS non-interactively generates the sharing âŸ¨RâŸ©1 of
the local gradient matrix by using PRGs, assuming that all parties
are online. We resolve this problem by using a simple method, i.e.,
letting CS remove the corresponding rows in âŸ¨RâŸ©1 that are gener-
ated for dropped parties, to align with âŸ¨RâŸ©0. Note that as the number
of dropped parties increases, existing methods such as [10] cause a
quadratic communication overhead, but our approach to handling
dropped-out parties can actually reduce the overhead of CS and SP
(see our experimental results in Tables 4 and 5).
(2) SecureFL performs fixed-point arithmetic in finite fields. The
implementation of the FL gradient aggregation performs arithmetic
operations on floating-point numbers. We work around this by
using fixed-point representations of floating-point numbers and
embedding them in our finite fields, consistent with existing meth-
ods [29] [34]. Furthermore, to prevent values from overflowing due
to arithmetic operations, we use the truncation technique from [36].
This method simply truncates the extra LSBs of fixed-point values
even when the value is secretly shared, albeit at the cost of a 1-bit
error.
(3) SecureFL can be flexibly extended to support layer-wise robust
aggregation. The discussion so far assumes that the robust aggrega-
tion is performed over the entire gradient of each party. However,
for modern large-scale neural networks that even contain thou-
sands of layers, we may need to perform the robust aggregation
layer-wisely. Concretely, to compute the trust score of each party,
we can adaptively assign a weight to each layer (or a combina-
tion of several layers). After that, SecureFL is called iteratively for
each layer, and the overall trust score is calculated by performing
a weighted aggregation of each layerâ€™s trust score. This has two
advantages: 1) mainly focusing on the more important layers for
large-scale models, and 2) preventing overflow when performing
the cosine similarity and the squared â„“2 norm evaluations.
(4) The cryptographic recipes of SecureFL may be of general inter-
ests. Given that several recent byzantine-robust FL schemes [9] [16]
extensively utilize similarity measurements, our matrix multiplica-
tion pre-processing and ReLU-based comparison techniques can be
extended to empower the realization of privacy protection. More-
over, the use cases of the validity checking technique go beyond
byzantine-robust FL schemes, such as scientific computing [13] and
secure querying [12], where servers need to check whether values
uploaded by (malicious) users are well-formed in the ciphertext.
5.2 Security analysis
Theorem 2. The protocol in Table 2 is a cryptographic FL protocol
in the honest-but-curious setting, given the ideal primitives of the
Beaverâ€™s multiplication procedure, ReLU/DReLU and packed linearly
homomorphic encryption.
Proof. We provide a hybrid argument proof in Appendix B that
relies on the simulators of the above ideal functionalities.
Theorem 3. Our SecureFL is byzantine-robust (i.e., can reject in-
correctly formatted and poisoned gradients), assuming the soundness
of the validity checking construction and the robustness property of
our crypto-friendly FL method (directly derived from FLTrust [11]).
Proof. We provide a brief sketch of the byzantine robustness
argument in Appendix B.
6 EVALUATION
We employ two distinct code bases for the implementation of Se-
cureFL. Cryptographic protocols are implemented in C++, relying
on the SEAL homomorphic encryption library5 for PLHE and CrypT-
Flow library6 for 2PC protocols. On the other hand, FL experiments
are developed in Python and experimental settings mainly follow
the previous work [11].
6.1 Experimental Setup
We describe the detailed experimental setup in this section.
Cryptographic setting: To purely measure our SecureFLâ€™s
performance, we conduct simulations on a Linux machine with an
Intel Xeon(R) CPU E5-2620 v4 (2.10 GHz), 16 GB of RAM. Moreover,
we compare prior works with our SecureFL in a simulated LAN