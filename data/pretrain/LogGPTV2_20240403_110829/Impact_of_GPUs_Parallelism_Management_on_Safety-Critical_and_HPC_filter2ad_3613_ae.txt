threads lowers the scheduling required for computation and 
modifies  the  GPU  parallelism  management  in  a  way  that 
lowers  the  cross  section.  For  the  extreme  case  B1G15  the 
trend seems reversed, as its cross section is higher than the 
B32G15. This is due to the fact that in B1G15 just one thread 
is executed in a SM, wasting parallel capabilities. The GPU 
is  not  meant  to  work  with  few  complex  threads  per  SM 
(while it is common to have few blocks with several threads) 
[22],  so  scheduling  is  not  optimized  and  this  increases  the 
code sensitivity. 
Tab.  XI  lists  the  MTBF  and  MEBF  for  the  tested 
distributions  (the  MWBF  is  not  listed  as  the  workload  is 
constant for all tested configuration). From Fig. 9 it is clear 
that  the  version  with  a  higher  DOP  does  not  provide  the 
higher reliability. This means that the additional scheduling 
strain required to deal with the increased number of parallel 
processes increases the sensitivity of the GPU more than the 
benefit  that  a  lower  execution  time  brings.  Fig.  9  also 
identifies  the  configuration  that  ensures  higher  reliability, 
which  is  B256G15.  A  combination  of  lower  scheduling, 
memory  access  latencies,  resources  optimization,  and  a 
shorter  execution  time  makes  mult  to  conclude  correctly 
three times more executions in In B256G15 with respect to 
the fully parallelized code. 
B1G15  configuration  is  particularly  indicative  of  the 
different information provided with cross section and MEBF. 
B1G15, in fact, has less than half the cross section of the P 
version of the code, but its MEBF is 200 times smaller. This 
much  worsened  reliability  despite  a  lower  cross  section  is 
mainly due to its much longer execution time. 
B.  Matrix Multiplication 
the 
lists 
tested 
Sums  and  mults,  although  effective  in  evaluating  the 
radiation response of GPUs, are not representative of typical 
parallel  code.  Matrix  Multiplication  was  also  tested  to 
evaluate  the  DOP  effects  on  a  realistic  application.  The 
algorithm,  named  MxM,  multiplies 
two  matrices  of 
2048x2048  random  double  data.  The  fully  parallelized 
version of the code, MxM_P, instantiates 2048x2048 threads 
(grouped in 16384 blocks of size 256), each of which is in 
charge of evaluating one element of the output matrix M.  
 In  the  same  fashion  described  in  the  previous  sub-
section,  the  algorithm  DOP  was  decreased  increasing  the 
operations  each  thread  has  to  perform  so  to  leave  the 
workload  unaltered.  Tab.  X 
threads 
distributions, named after the only parameter that was altered 
(MxM_Bx  if  in  the  distribution  the  block  size  was  set  to  x 
keeping the grid size to 16384 as it is in MxM_P or MxM_Gy 
if the grid size was set to y leaving the block size to 256). 
As  for  sums  and  mults,  the  reduced  DOP  is  achieved 
increasing  the  thread  complexity.  The  threads  complexity, 
shown in the first column of Tab. X, indicates the number of 
elements  of  matrix  M  that  the  thread  is  in  charge  of 
computing.  So,  a  thread  of  complexity  2  will  compute  2 
elements of matrix M located, without loss of generality, on 
the same row (i.e. M[i,j] and M[i,j+1024]) while a thread of 
complexity  4  will  compute  4  elements  of  M  (i.e.  M[i,j], 
M[i,j+1024],  M[i+1024,j],  and  M[i+1024,j+1024]),  and  so 
on. In the MxM algorithm the workload is actually the output 
matrix  M,  and  it  is  constant  for  all  distributions.  Matrix 
Multiplication is a more complex application than sums and 
mults. In particular, it requires caches. When the workload of 
a  thread  is  increased,  the  amount  of  data  it  requires  for 
computation is also increased.  Reducing the  DOP of MxM 
will then modify not only the scheduling strain, but also the 
caches distribution. 
Tab.  XII  provides,  for  each  DOP  and  distribution,  the 
execution  time  of  MxM.  As  for  sum  and  mult,  better 
performances  are  achieved  when  the  highest  DOP  is 
exploited. Reducing the block size from 256 to 16, and thus 
requiring  each  thread  to  compute  16  location  of  matrix  M, 
increases the algorithm execution time by almost 4 times. 
The third column of Tab. XII lists the cross sections of 
the tested codes, measured at ISIS, Didcot, UK. More than 
10,000  executions  were  performed  under 
radiation, 
collecting at least 200 errors for each configuration. The last 
column  of  Tab.  XII  reports  the  MxM  FITs,  obtained 
multiplying  the  experimental  cross  section  by  the  average 
neutron  flux  at  sea  level  (about  13n/(cm2·h)  [25]).  As  the 
ISIS  neutron  energy  spectrum  resembles  the  atmospheric 
one,  the  reported  FITs  give  a  practical  evaluation  of  the 
relevance  of 
realistic 
application. 
radiation-induced  errors  on  a 
463
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:48:15 UTC from IEEE Xplore.  Restrictions apply. 
MXM THREADS DISTRIBUTIONS EXECUTION TIMES, 
CROSS SECTIONS, AND FIT 
TABLE XII 
execution 
time [ms] 
1086.63 
899.42 
708.92 
655.74 
685.10 
1104.34 
2353.63 
MxM-G1024 
MxM-G4096 
MxM-G8192 
MxM-P 
MxM-B128 
MxM-B64 
MxM-B16 
Threads 
complexity 
8 
4 
2 
1 
2 
4 
8 
σ [10-6cm2] 
3.24±5% 
3.13±5% 
2.82±5% 
2.66±5% 
2.41±5% 
2.40±5% 
1.55±5% 
FIT [104] 
4.21 
4.07 
3.67 
3.46 
3.13 
3.13 
2.02 
]
2
m
c
6
-
0
1
[
n
o
i
t
c
e
S
s
s
o
r
C
4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0
MxM
Mult
G1024 G4096 G8192
P
B128
B64
B16
the  DOP  remarkably  reduces 
Figure 10: Cross sections of MxM when implemented with different 
DOP  and  different  threads  distributions.  The  workload  is  kept 
constant and the execution time is normalized when evaluating the 
cross section 
The  trend  shown  in  Fig.  10  is  definitely  different  from 
the  one  in  Fig.  8.  For  sums  and  mults  (see  Fig.  8)  even  a 
slight  decrease  of 
the 
application  cross  section.  In  MxM,  DOP  variation  obtained 
lowering  the  block  size  (i.e.  in  MxM-B128,  MxM-B64,  and 
MxM-B16)  reduces  the  code  sensitivity.  On  the  contrary  if 
the DOP is varied lowering the grid size (i.e. in MxM-G8192, 
MxM-G4096,  and  MxM-G1024),  the  cross  section  of  the 
code  increases.  The  two  different  trends  are  mainly  due  to 
the impact of cache corruption and memory access latencies. 
As detailed in section II, threads can access just the cache 
of the SM they are assigned to. In MxM_P, a block size of 
256 is chosen, which means that 256 elements of matrix M 
are evaluated by the threads in a SM. Each thread in MxM_P 
will require a row of A and a column of B to be in the cache 
to  complete  its  task,  eventually  sharing  them  with  other 
threads in the same block. 
When the DOP is lowered by reducing the block size, the 
programmer must increase the M locations that each thread 
in the SM has to calculate in order to maintain the workload 
of each SM constant. This implies that each thread in a SM 
requires more than one row of A and more than one column 
of B to be in the cache, but at the same time fewer threads 
are present in the block (and thus executed in the SM). The 
overall  number  of  rows  and  columns  in  the  SM  is  then 
constant as the DOP is decreased reducing the block size, so 
the  SM  cache  remains  unaltered.  As  shown  in  Fig.  10,  in 
fact,  when  the  grid  size  is  constant  (on  the  right  side  with 
respect  to  MxM_P),  reducing  the  DOP  has  benefits  on  the 
code resilience, according to the trend shown in Fig. 8. 
On  the  contrary,  when  the  grid  size  is  reduced,  the 
workload of each SM has to be increased to keep the MxM 
MXM THREADS DISTRIBUTIONS MTBF AND MEBF 
TABLE XIII 
MTBF [104h]  MEBF [107exec.] 
2.37 
2.46 
2.73 
2.89 
3.19 
3.19 
4.96 
MxM-G1024 
MxM-G4096 
MxM-G8192 
MxM-P 
MxM-B128 
MxM-B64 
MxM-B16 
20
18
7.87 
9.84 
13.91 
15.92 
16.80 
10.44 
7.59 
MxM
Mult
]
s
n
o
i
t
u
c
e
x
e
7
0
1
[
F
B
E
M
16
14
12
10
8
6
4
2
0
G1024 G4096 G8192
P
B128
B64
B16
Figure  11:  Mean  Executions  Between  Failures  of  MxM  when 
implemented with different DOP and different threads distributions. A 
higher reliability is achieved when B16 DOP distribution is selected. 
workload constant. Thus more data is required in the cache 
of each SM when the DOP is reduced reducing the number 
of blocks. Each SM will consume more memory bandwidth, 
and the memory access latency will then increase in super-
linear fashion due to congestion in the memory system and 
controller. This turns into a longer permanence of data in the 
chip caches, leading to higher cross section, undermining the 
benefits of reduced scheduling strains. 
the  best  choice 
terms  of  reliability 
Tab. XIII reports the MTBF and the MEBF for MxM. As 
shown in Fig. 11, even for MxM the fully parallelized version 
of  the  algorithm  does  not  ensure  the  highest  reliability.  In 
this  case, 
is 
MxM_B128,  which  is  able  to  combine  the  reduced  cross 
section with a shorter execution time. However, MxM_B128 
has just a 10% higher MEBF with respect to P. All the other 
versions  of  MxM  have  a  much 
lower  MEBF,  with 
MxM_G1024  and  MxM_B16  (the  versions  with  the  lowest 
DOP) having about half the MEBF of MxM_P. 
C.  Fast Fourier Transform 
in 
At  ISIS,  Dicot,  UK  a  benchmark  that  implements 
512x512  1D-FFTs  of  64-points  each  was  tested  on  the 
C2050. The FFT input is composed of a 64x512x512 double 
precision  floating-point  matrix  for  the  real  part  and  a 
64x512x512 matrix for the imaginary part. We choose to test 
relatively  small  FFTs  (64-points)  to  limit  the  number  of 
iterations  and  the  workload  of  each  thread,  while  having 
512x512  1D-FFTs  eases  the  gathering  of  a  statistically 
significant amount of errors. 
A thread acts like a butterfly module updating the values 
of two floating-point elements in the complex matrix using 
the  values  of  two  elements  computed  in  the  previous 
iteration  as  inputs.  The  implemented  algorithm  is  based  on 
464
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:48:15 UTC from IEEE Xplore.  Restrictions apply. 
FFT THREADS DISTRIBUTIONS EXECUTION TIMES, 
FFT THREADS DISTRIBUTIONS MTBF AND MEBF 
TABLE XV 
MTBF [104h]  MEBF [106exec.] 
TABLE XIV 
CROSS SECTIONS, AND FIT 
FFT-G1 
FFT-G14 
FFT-G28 
FFT-G70 
FFT-G252 
FFT-P 
FFT-B256 
FFT-B64 
FFT-B32 
FFT-B1 
execution 
time [ms] 
210.8 
107.21 
106.85 
104.78 
104.65 