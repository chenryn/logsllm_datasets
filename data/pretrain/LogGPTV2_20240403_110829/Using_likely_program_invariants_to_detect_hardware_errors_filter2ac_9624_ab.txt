underlying fault.
Diagnosis: After the fault detection, the diagnosis frame(cid:173)
work is invoked to distinguish the source of the fault as a
software fault, or a transient fault, or a permanent fault. The
diagnosis component rolls back to the last checkpoint and
replays the execution on the original core. If the symptom
1-4244-2398-9/08/$20.00 ©2008 IEEE
72
DSN 2008: Sahoo et al.
International Conference on Dependable Systems &Networks: Anchorage, Alaska, June 24-27 2008
does not recur, it infers a transient fault and continues ex(cid:173)
ecution. However, if the symptom recurs, it re-executes on
another core (assumed to be fault-free) to distinguish soft(cid:173)
ware faults (in which case the symptom would most likely
reoccur on the fault-free core) from permanent hardware
faults (in which case the symptom would not occur). We
may need to do the replay multiple times on the two cores
to distinguish not-deterministic software errors. In the case
of a permanent fault, the diagnosis module also does micro(cid:173)
architecture level diagnosis to identify the faulty microarchi(cid:173)
tectural structure [10].
Recovery and Repair: For recovery, SWAT assumes some
form of checkpoint/restart mechanism that periodically
checkpoints the state of the system. Depending upon the re(cid:173)
quirements, appropriate hardware checkpointing or software
checkpointing mechanisms, or a combination of both, can
be used to recover the system after detection. SafetyNet [28]
and Revive [18, 23] show reasonably low overhead for hard(cid:173)
ware checkpoint/replay. In the event of a permanent hard(cid:173)
ware fault, the component that is diagnosed as faulty can be
reconfigured or disabled.
3.2 The iSWAT system
The iSWAT system extends the above described SWAT sys(cid:173)
tem to include the violation of likely program invariants as
possible symptoms that indicate the presence of a hardware
fault. These invariants are derived from "training" runs of
the application and the invariant checking code is embed(cid:173)
ded into the application. iSWAT exploits the diagnosis and
recovery module of the SWAT system to detect and disable
false-positive invariants at run-time.
3.2.1 Generating Invariants and Invariant Checks
The iSWAT system leverages support from the compiler for
two distinct components: invariant generation and invariant
insertion. Both of these use the LLVM compiler infrastruc(cid:173)
ture [8].
Invariant Generation: We use compile-time instrumenta(cid:173)
tion to monitor program values during training runs in or(cid:173)
der to generate likely program invariants. We can monitor
many different types of values, including load, store, return
and intermediate result values. For this work, we decided to
monitor only the store values as checking values stored to
memory has the most potential to catch faults, as all nec(cid:173)
essary computations eventually pass their results to stores.
Also, monitoring only the stores helps us keep the overhead
of detection low. We monitor stored values of all integer
types (both signed and unsigned) of size 2, 4, and 8 bytes
as well as single and double precision floating point types.
We do not monitor integer stores of size 1 byte (character
data types), as they represent only a small range of values
and hence may be ineffective to detect faults.
We do the code generation for invariant generation in two
steps. In the first step, we use LLVM-l.9 with llvm-gcc-3.4
to generate the LLVM bytecode and run an instrumentation
pass to insert calls to monitor the store values. Then we gen(cid:173)
erate a C program from the LLVM bytecode file through the
LLVM C back end. Finally, we generate SPARC native code
through the Sun cc compiler. We use the generated program
to create the invariants for each input separately, which we
then combine to form the final invariants in another offline
pass.
Invariant Insertion: The invariants generated by the offline
pass then need to be inserted into the code to check the val(cid:173)
ues being stored. For this, we take the invariant ranges from
the generation phase and then insert calls to the invariant
checking code at the LLVM byte-code level through another
compile-time instrumentation pass. Then, as in the invari(cid:173)
ant generation phase, we generate a C program from LLVM
bytecode through LLVM C back end and finally, use Sun cc
compiler to generate native code.
3.2.2 Handling False-Positive Invariants
False positives present a major short-coming for likely pro(cid:173)
gram invariants as detecting them may incur high overheads
in the presence of permanent faults. For transient hardware
faults, relatively low-cost techniques such as pipeline flush
can help to tolerate false positives [24]. If the fault occurs
again after pipeline flush, the invariants will be updated and
this is done cheaply using hardware support. In contrast, for
a framework like iSWAT System that supports permanent
fault detection, more expensive rollback and replay on the
same and a fault-free core is needed to detect false positives.
Too many false positive detections may thus lead to exorbi(cid:173)
tant overheads.
While training with too many inputs can potentially make
false positives rare, the ranges may become too broad ren(cid:173)
dering the invariants ineffective for error detection. In our
framework, we propose to train with a set of inputs such that
the false positive rate is sufficiently low. In general, the num(cid:173)
ber of false positives can be used to guide how many inputs
to use for training.
iSWAT leverage the existing diagnosis framework in the
SWAT system [10] to detect the remaining false positives at
run-time. In the event of an invariant violation, we follow
the full rollback and replay on the same and fault-free core.
If the violation occurs even on fault-free core, then it is a
false positive invariant(or a software bug). Since too many
rollback/replays due to false positives can cause large over(cid:173)
heads, we disable the static invariants once it results in a
false positive during dynamic execution (Online updation of
invariants will add too much overhead for our software-only
technique). In this way, if the total number of static invariants
is I, the maximum number of rollbacks possible will also
be I, limiting the overhead incurred due to false positives.
Currently, the disable operation is done inside the invariant
1-4244-2398-9/08/$20.00 ©2008 IEEE
73
DSN 2008: Sahoo et a!.
International Conference on Dependable Systems &Networks: Anchorage, Alaska, June 24-27 2008
if( FalsePosArray[Invld] != true ){ IINot
if ( (value  max) ) { II Invariant violated
false positive
if ( detectFalsePos( Invld) == true ) II Perform diagnosis
FalsePosArray [Invld] = true; II Disable the invariant
} }
Figure 1. Invariant checking code template
checking code and does not need any extra hardware sup(cid:173)
port. We maintain a table with one entry for each invariant
indexed by the invariant id to identify false positive invari(cid:173)
ants. In the invariant checking code, the table entry is marked
to disable the false positive invariants from all later execu(cid:173)
tions, if the invariant is detected as a false positive. Figure 1
shows a template of the actual invariant checking code.
The overhead caused by false positives in an invariant(cid:173)
based approach depends on the number of rollbacks, which
in tum depends on the number of static false positive invari(cid:173)
ants. The false positive results, presented in Section 5, indi(cid:173)
cate that the overhead for our set of applications is negligible
compared to their runtimes.
4. Methodology
4.1 Simulation Environment
For the fault injection experiments, we used a full system
simulation environment comprising the Virtutech Simics full
system simulator [31] with the Wisconsin GEMS timing
models for the microarchitecutre and the memory [14] as
in [9]. These simulators provide cycle-acurate microarchi(cid:173)
tecture level timing simulation of a real workloads running
on a real operating system (full Solaris-9 on the SPARC V9
ISA) on a modem out-of-order superscalar processor and
memory hierarchy (Table 1).
We exploit the timing-first approach of the GEMS+Simics
infrastructure [15] to inject microarchitecture-Ievel faults. In
such an approach, GEMS and Simics compare their full
architectural states after each instruction and in case of a
mismatch GEMS state is updated with Simics state. This
checking mechanism was leveraged for our fault injection.
The faults are injected into GEMS's micro-architectural state
and the fault is allowed to propagate. After a mismatch be(cid:173)
tween Simics and GEMS, if the mismatch is found to be be(cid:173)
cause of the fault, we copy the faulty architectural state from
GEMS to Simics, to make sure Simics follows the same cor(cid:173)
rupted execution path as GEMS. Otherwise, the GEMS state
is updated as usual. When we find the architectural state is
corrupted, we say that the fault has been activated.
4.2 Fault Model
In the current work, we focus on permanent or hard faults.
The well established stuck-at-O and stuck-at-l fault models
as well as the dominant-O and dominant-l bridging fault
models are used for modeling permanent hardware faults
in this paper. The stuck-at fault models model a fault in a
single bit and the bridging fault model models faults that
affect adjacent bits. The dominant-O bridging fault acts like
Base Processor Parameters
Frequency
Fetchldecode/execute/retire rate
Functional units
Integer FU latencies
FP FU latencies
Reorder buffer size
Register file size
Unified Load-Store Queue Size
2.0GHz
4 per cycle
2 Int add/mult, 1 Int div
2 Load, 2 Store, 1 Branch
2 FP add, 1 FP mult, 1 FP div/Sqrt
1 add, 4 multiply, 24 divide
4 default, 7 multiply, 12 divide
128
256 integer, 256 FP
64 entries
Base Memory Hierarchy Parameters
Data L1IInstruction L1
L1 hit latency
L2 (Unified)
L2 hit/miss latency
16KB each
1 cycle
1MB
6/80 cycles
Table 1. Parameters of the simulated processor
Microarchitecture structure
Instruction decoder
Integer ALU
Register bus
Physical integer register file
Reorder buffer (ROB)
Register alias table (RAT)
Address gen unit (AGEN)
FPALU
Fault location
Input latch of one of the decoders
Output latch of one of the integer ALUs
Bus on the write port to the register file
A physical reg in the integer register file
Source/dest reg num of instr in ROB entry
Logical ~ phys map of a logical register
Virtual address generated by the unit
Output latch of one of the FP ALUs
Table 2. Microarchitectural structures in which faults are in(cid:173)
jected.
a logical-AND operation between the adjacent bits that are
marked faulty, whereas the dominant-l bridging faults act
like a logical-OR operation.
The microarchitectural structures and locations where the
faults are injected are listed in Table 2. For each structure,
a fault is injected in each of 40 random points in each ap(cid:173)
plication (after the initialization phase in each application is
over). For each application injection point, we perform an in(cid:173)
jection for each of the 4 fault models (two stuck-at and two
bridging faults). The injections are performed on a randomly
chosen bit in the given structure. This gives a total of 800
fault injection simulation runs per microarchitectural struc(cid:173)
ture (5 applications x 40 points per application x 4 fault
models) and 6,400 total injections across all 8 structures.
After a fault is injected, we run the simulation for 10
million instructions. Note that the fault is maintained for the
rest of the 10M instruction window. For the small number
of runs where an activated fault is not detected within this
window, we use functional (full-system) simulation to run
the application to competition (as detailed simulation is too
slow to run to completion) for evaluating masking due to the
application, and SDCs. The functional simulation does not
inject any faults beyond the first 10M instructions, resulting
in the fault acting like an intermittent fault that is active only
in the 10 million instruction window. We believe that 10M
instructions is long enough that the simulation reflects the
behavior of permanent faults.
1-4244-2398-9/08/$20.00 ©2008 IEEE
74
DSN 2008: Sahoo et al.
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 2008
4.3 Fault Detection Techniques Used
We show the effectiveness of our invariant-based approach
by evaluating invariants in conjunction with the four low(cid:173)
cost detection mechanisms built into the base SWAT system.
This is more realistic than studying only detections by invari(cid:173)
ants as the other techniques are lower overhead (and need
very little hardware/software support) and will show the im(cid:173)
pact of the new technique in any realistic system.
4.4 Fault Metrics
When the fault causes a corruption in the architectural state
of the processor, we say it is activated. If the fault is never
activated, we say the fault is architecturally masked. An
activated fault which is undetected, but does not cause any
corruption in the output produced by the application is said
to be application masked.
We used five metrics to evaluate the impact of the new
detection technique:
1. Coverage: The percentage of non-masked faults that are
detected in the 10M instruction window. We refer the
percentage of undetected faults as unknown-fraction.
2. Latency: The total number of instructions retired from
the first architecture state corruption (of either OS or
application) until the fault is detected by one of the above
techniques.
3. SDCs: The number of SDCs which result in corrupting
the output of the application.
4. False positives: The total number of false positive invari(cid:173)
ants.
5. Overhead: The overhead of the invariant checking code
as a percentage of original execution time, measured in
fault-free run.
4.5 Applications
For the experiments, we used five SpecCPU 2000 bench(cid:173)
marks - four SpecInt benchmarks (gzip, bzip2, mcf, parser)
and one SpecFP benchmark (art). For most of the other
SPEC benchmarks, we could not collect sufficient training
inputs, while we could not compile and run the others in our
simluator.
Previous work on invariants uses toy Siemens bench(cid:173)
marks because many inputs are available for these bench(cid:173)
marks [4, 22]. We use more realistic applications which
makes it much harder for us to obtain valid inputs for ex(cid:173)
periments. Nevertheless, obtaining inputs will not be a prob(cid:173)
lem in practice as developers test their programs on many
inputs during the testing phase. Invariant generation and in(cid:173)
sertion can be easily done during testing through a compile(cid:173)
time pass. The "test" and "train" input sets formed part of
our training set. Different techniques were used to generate
more inputs depending on the benchmarks. For three bench(cid:173)
marks (gzip, bzip2 and parser), we collected random inputs
70%
~ 60%
-+-mcf
"pip
....bzlp2
"-parser
~'Irt
Q: 50%
::