title:RotorNet: A Scalable, Low-complexity, Optical Datacenter Network
author:William M. Mellette and
Rob McGuinness and
Arjun Roy and
Alex Forencich and
George Papen and
Alex C. Snoeren and
George Porter
RotorNet: A Scalable, Low-complexity,
Optical Datacenter Network
William M. Mellette, Rob McGuinness, Arjun Roy, Alex Forencich,
George Papen, Alex C. Snoeren, and George Porter
University of California, San Diego
ABSTRACT
The ever-increasing bandwidth requirements of modern datacen-
ters have led researchers to propose networks based upon optical
circuit switches, but these proposals face significant deployment
challenges. In particular, previous proposals dynamically config-
ure circuit switches in response to changes in workload, requiring
network-wide demand estimation, centralized circuit assignment,
and tight time synchronization between various network elements—
resulting in a complex and unwieldy control plane. Moreover, limita-
tions in the technologies underlying the individual circuit switches
restrict both the rate at which they can be reconfigured and the
scale of the network that can be constructed.
We propose RotorNet, a circuit-based network design that ad-
dresses these two challenges. While RotorNet dynamically reconfig-
ures its constituent circuit switches, it decouples switch configura-
tion from traffic patterns, obviating the need for demand collection
and admitting a fully decentralized control plane. At the physical
layer, RotorNet relaxes the requirements on the underlying circuit
switches—in particular by not requiring individual switches to im-
plement a full crossbar—enabling them to scale to 1000s of ports.
We show that RotorNet outperforms comparably priced Fat Tree
topologies under a variety of workload conditions, including traces
taken from two commercial datacenters. We also demonstrate a
small-scale RotorNet operating in practice on an eight-node testbed.
CCS CONCEPTS
• Networks → Network architectures;
KEYWORDS
Datacenter, optical switching
ACM Reference format:
William M. Mellette, Rob McGuinness, Arjun Roy, Alex Forencich, George
Papen, Alex C. Snoeren, and George Porter. 2017. RotorNet: A Scalable,
Low-complexity, Optical Datacenter Network. In Proceedings of SIGCOMM
’17, Los Angeles, CA, USA, August 21-25, 2017, 14 pages.
https://doi.org/10.1145/3098822.3098838
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
© 2017 Association for Computing Machinery.
ACM ISBN 978-1-4503-4653-5/17/08...$15.00
https://doi.org/10.1145/3098822.3098838
1 INTRODUCTION
Modern datacenter networks rely heavily on fiber-optic links to
meet bandwidth demands [10, 29]. To avoid the expensive optical-
electrical-optical (OEO) conversion necessary to connect such links
with power-hungry electrical packet switches, researchers have
proposed network architectures that switch much of a datacenter’s
traffic passively [1, 6, 13, 15, 17, 20, 26, 32] using optical circuit
switches (OCSes). OCSes can support very high link bandwidths
at low per-bit cost and power because they passively redirect light
from one port to another, independent of data rate. Yet despite
these alluring properties, optical circuit switching faces two major
barriers to wide-scale adoption in the datacenter environment.
The first barrier to deployment is the attendant control plane.
Existing proposals to employ OCSes in the datacenter reconfigure
optical circuits in response to traffic demands. Such reconfiguration
requires collecting network-wide demand information [20, 26, 32]
in order to compute a schedule of switch configurations [4, 21], rate-
limiting packet transmissions [4, 20, 21, 26], and synchronizing the
OCSes with each other, the scheduler, and the endpoints [20]. This
tight coupling among the various network components presents
a significant challenge at scale. In this paper, we propose an OCS-
based topology that has a fully decentralized control plane aimed
at maximizing network throughput.
The second issue with employing commercial OCS devices in
datacenters is their limited scalability, specifically their low port
count (radix) and slow reconfiguration speed. Previous work [24]
showed that MEMS-based optical switches can reconfigure quickly
(e.g., in 10s of microseconds), or they can have a high port count
(e.g., O(100) ports), but not both. Fundamentally, this limitation
is imposed by the need to implement a full crossbar abstraction;
in other words, the requirement that a switch be able to forward
traffic between any two ports. While previous proposals rely upon
this full connectivity to construct network fabrics, we explore an
alternative design point that circumvents the fundamental scaling
limitations of MEMS-based crossbars.
We propose RotorNet, an OCS-based datacenter-wide network
fabric that overcomes these challenges by departing from prior
optical switching approaches in three distinct ways. First, RotorNet
does not reconfigure the optical switches to match network traffic
conditions. Instead, each switch independently rotates through a
fixed, static set of configurations that provide uniform bandwidth
between all endpoints. This design completely eliminates the need
for a centralized control plane—because round-robin switch sched-
uling does not require demand estimation, schedule computation,
or schedule distribution—yet delivers full bisection bandwidth for
uniform traffic.
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
W. M. Mellette et al.
c-Through [32]
Helios [13]
REACToR [20]
Firefly [17]
Mordia [26]
MegaSwitch [8]
ProjecToR [15]
RotorNet
Topology
k (NR × NR) xbars
k (NR × NR) xbars
k (NR × NR) xbars
k (NR × NR) xbars
CP Structure
Centralized
Centralized
Centralized
Centralized
Centralized
Centralized
1 (k NR × k NR) xbar
Distributed
Nsw Rotor switches Distributed
NR × k rings
k rings
CP Algorithm
Edmonds [9]
Edmonds [9]
Solstice [22]
Blossom [9]
BvN [3, 31]
Edge coloring variant [7]
Stable-marriage variant [14]
VLB variant [5, 30]
CP Goal
Throughput maximization
Throughput maximization
Throughput maximization
Throughput maximization
Throughput maximization
Throughput maximization
Starvation-free low latency
Throughput maximization
CP Responds to Demand
Yes
Yes
Yes
Yes
Yes
Yes
Yes
No
Table 1: A comparison of the control planes (CP) of previously proposed OCS-based topologies.
Of course, datacenter traffic is known to be far from uniform [27],
so RotorNet gives up at most a factor of two in link bandwidth—
which is ample in an optically switched network—to support arbi-
trary traffic patterns. We rely on a form of Valiant load balancing
(VLB) to send traffic through the fabric twice: first to an interme-
diate endpoint, and then once more to the ultimate destination.
We show that classic VLB can be modified to regain a significant
amount of bandwidth, meaning the factor-of-two reduction is a
worst-case, not common-case, trade off. Our design provides strictly
bounded delivery time and buffering while remaining robust to link
and switch failures. Compared to a 3:1 Fat Tree of approximately
equal cost, RotorNet delivers 1.6× the throughput under worst-case
traffic, 2.3× the throughput for reported datacenter traffic, and up
to 3× the throughput for uniform traffic.
Finally, RotorNet employs custom-designed OCSes that are tai-
lored to our need, namely rotating through a small fixed set of
configurations, which we call Rotor switches. Because of their sim-
plified internal design, Rotor switches can scale to 1000s of ports
with state-of-the-art reconfiguration speeds of O(10) µs. A network
of 128 Rotor switches can support 2,048 racks while remaining
practical to build and deploy. At this scale, RotorNet serves traffic
within O(1) ms, and admits a hybrid optical/electronic architecture
to extend support to traffic with lower latency requirements. We
evaluate RotorNet experimentally on a small-scale testbed complete
with a prototype Rotor switch and endpoints that implement our
VLB-inspired indirection protocol.
The contributions of this paper include: 1) a new design direction
for optically-switched networks that obviates closed-loop circuit
scheduling, 2) open-loop Rotor switches and a datacenter-wide
fabric, RotorNet, constructed from these switches, 3) an analysis
of RotorNet showing that it supports uniform traffic at full bisec-
tion bandwidth, worst-case (permutation) traffic at half bisection
bandwidth, and reported commercial datacenter traffic with 70–90%
of the bandwidth of an ideal Fat Tree network at lower cost, and
4) a demonstration and evaluation of RotorNet’s VLB-like routing
algorithm on prototype hardware.
Additional information on RotorNet can be found at http://www.
sysnet.ucsd.edu/circuit-switching.
2 BACKGROUND & RELATED WORK
RotorNet builds upon a line of research integrating optical circuit
switching into datacenter networks. The physical-layer technolo-
gies employed by these approaches vary widely, with some using
fiber-coupled optical crossbar switches, others using WDM-based
Figure 1: A traditional Clos-based network topology.
ring networks, and yet others using free-space optical transmit-
ters and receivers. Table 1 abstracts away these implementation
details and instead focuses on each proposal’s topology and control
plane. In all cases (save ProjecToR [15], which we discuss below),
each design tries to match the configuration of the network to
the current or future traffic demands to maximize overall network
throughput, requiring a complex datacenter-wide control plane
that is difficult to scale. To appreciate why such algorithms are
required, we briefly review throughput maximization in the context
of a single crossbar-based packet switch before describing previous
OCS-based proposals.
2.1 Packet-switched folded-Clos networks
Figure 1 depicts a traditional Clos-based network topology with a
single layer of core switches. Groups of servers are aggregated into
NR racks, and each server is connected to its local top-of-rack (ToR)
switch. In this example, each ToR has Nup = Nsw uplinks, each
connected to one of Nsw core switches. In general, ToR switches
are connected by a multi-stage network of homogeneous switches
which, taken together, logically act as a single, large crossbar switch.
To connect large numbers of racks with switches of moderate port
count (Nup ≪ NR), published production networks employ at least
three tiers of cascaded switches [10, 16, 29].
In a folded-Clos network fabric, end hosts are essentially decou-
pled from the packet switches forming the core of the network.
Servers can send data to any destination with the expectation that
the fabric will deliver that data without further communication
with the sender. Each packet switch has internal buffering to absorb
bursts, and a mechanism for conveying traffic through the switch
at line rate. The only signals a sender may receive that a switch is
under-provisioned are packet losses or ECN (Explicit Congestion
Nsw packet switches ToR 1 ToR 2 ToR 3 ToR 4 Rack 1 Rack 2 Rack 3 Rack 4 … ToR NR Rack NR … Nup = Nsw uplinks RotorNet: A Scalable, Low-complexity, Optical Datacenter Network
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
Figure 2: (a) An electronic packet switch encapsulates its control complexity within the switch itself. (b) Standard approaches
to optical circuit switching expose the control complexity to the overall network.
Notification) markings when the offered load to the switch exceeds
the capacity of one or more outgoing links; in such cases transport
protocols (such as TCP) reduce the sending rates to converge to an
admissible demand.
A textbook switch design model is a VoQ-scheduled crossbar,
shown in Figure 2(a). Packets arrive at N input ports (left), where
they are buffered in virtual output queues (VoQs). A scheduling
algorithm, such as iSLIP [23] or one of its many variants, examines
these queues and chooses the appropriate sequence of input-output
port matchings needed to convey packets to the N output ports
(right). Typically, the internal crossbar runs at some speed-up factor
relative to the external line rate to hide any overhead involved in the
internal processes. These processes, including demand collection
(i.e., VoQ depth analysis), scheduling (e.g., iSLIP), reconfiguration
(i.e., sequence of matchings), and synchronization (e.g., which VoQ
is polled during each matching), together comprise a control plane
that is implemented entirely within the switch, essentially hidden
from the rest of the network and the sender.
2.2 Circuit-switched network fabrics
Prior OCS-based proposals fundamentally change the nature of the
switch control plane. Proposals that dynamically reconfigure the
network topology in response to observed or predicted traffic in
order to maximize throughput must carry out the same tasks as the
electronic packet switch shown above. Yet, while packet switches
are able to hide their internal management processes inside discrete
boxes, previous OCS-based topologies, by virtue of their inability
to buffer and inspect packets, expose this control complexity to the
entire network, effectively turning the network fabric into a giant,
coupled crossbar. Figure 2(b) illustrates this distinction, depicting