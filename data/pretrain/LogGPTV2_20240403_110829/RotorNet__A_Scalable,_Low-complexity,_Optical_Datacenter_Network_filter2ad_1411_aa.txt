# RotorNet: A Scalable, Low-Complexity Optical Datacenter Network

## Authors
William M. Mellette, Rob McGuinness, Arjun Roy, Alex Forencich, George Papen, Alex C. Snoeren, and George Porter  
University of California, San Diego

## Abstract
The increasing bandwidth demands in modern datacenters have led researchers to propose networks based on optical circuit switches (OCSes). However, these proposals face significant deployment challenges. Specifically, they require dynamic reconfiguration of OCSes in response to workload changes, necessitating network-wide demand estimation, centralized circuit assignment, and tight time synchronization among various network elements. This results in a complex and unwieldy control plane. Additionally, the underlying technologies of individual OCSes limit their reconfiguration speed and scalability.

We introduce RotorNet, a circuit-based network design that addresses these challenges. RotorNet dynamically reconfigures its constituent OCSes but decouples switch configuration from traffic patterns, eliminating the need for demand collection and enabling a fully decentralized control plane. At the physical layer, RotorNet relaxes the requirements on the underlying OCSes, particularly by not requiring each switch to implement a full crossbar, thus allowing them to scale to thousands of ports.

Our evaluation shows that RotorNet outperforms comparably priced Fat Tree topologies under various workload conditions, including traces from two commercial datacenters. We also demonstrate a small-scale RotorNet operating on an eight-node testbed.

## Keywords
Datacenter, Optical Switching

## ACM Reference Format
Mellette, W. M., McGuinness, R., Roy, A., Forencich, A., Papen, G., Snoeren, A. C., & Porter, G. (2017). RotorNet: A Scalable, Low-Complexity, Optical Datacenter Network. In Proceedings of SIGCOMM '17, Los Angeles, CA, USA, August 21-25, 2017, 14 pages. https://doi.org/10.1145/3098822.3098838

## Introduction
Modern datacenter networks rely heavily on fiber-optic links to meet bandwidth demands. To avoid the expensive optical-electrical-optical (OEO) conversion required to connect these links with power-hungry electrical packet switches, researchers have proposed network architectures that use passive optical circuit switches (OCSes) [1, 6, 13, 15, 17, 20, 26, 32]. OCSes can support high link bandwidths at low per-bit cost and power because they passively redirect light from one port to another, independent of data rate. Despite these advantages, optical circuit switching faces two major barriers to wide-scale adoption in datacenters.

### Control Plane Challenges
Existing proposals for OCSes in datacenters reconfigure optical circuits in response to traffic demands. This reconfiguration requires collecting network-wide demand information [20, 26, 32] to compute a schedule of switch configurations [4, 21], rate-limiting packet transmissions [4, 20, 21, 26], and synchronizing the OCSes with each other, the scheduler, and the endpoints [20]. The tight coupling among network components presents a significant challenge at scale. In this paper, we propose an OCS-based topology with a fully decentralized control plane aimed at maximizing network throughput.

### Scalability Limitations
Commercial OCS devices in datacenters have limited scalability, specifically in terms of low port count (radix) and slow reconfiguration speed. Previous work [24] showed that MEMS-based optical switches can either reconfigure quickly (e.g., in tens of microseconds) or have a high port count (e.g., O(100) ports), but not both. This limitation is due to the need to implement a full crossbar abstraction, which requires a switch to forward traffic between any two ports. While previous proposals rely on this full connectivity, we explore an alternative design that circumvents the fundamental scaling limitations of MEMS-based crossbars.

### RotorNet Design
We propose RotorNet, an OCS-based datacenter-wide network fabric that overcomes these challenges by departing from prior optical switching approaches in three distinct ways:
1. **Decoupled Configuration**: RotorNet does not reconfigure the optical switches to match network traffic conditions. Instead, each switch independently rotates through a fixed, static set of configurations that provide uniform bandwidth between all endpoints. This design eliminates the need for a centralized control plane, as round-robin switch scheduling does not require demand estimation, schedule computation, or distribution.
2. **Scalable OCSes**: RotorNet employs custom-designed OCSes, called Rotor switches, which are tailored to rotate through a small fixed set of configurations. These Rotor switches can scale to thousands of ports with state-of-the-art reconfiguration speeds of O(10) µs.
3. **Valiant Load Balancing (VLB)**: RotorNet uses a modified form of VLB to handle non-uniform traffic patterns. It sends traffic through the fabric twice: first to an intermediate endpoint and then to the ultimate destination. This approach provides bounded delivery time and robustness to link and switch failures.

Compared to a 3:1 Fat Tree of approximately equal cost, RotorNet delivers 1.6× the throughput under worst-case traffic, 2.3× the throughput for reported datacenter traffic, and up to 3× the throughput for uniform traffic.

### Contributions
The contributions of this paper include:
1. A new design direction for optically-switched networks that obviates closed-loop circuit scheduling.
2. Open-loop Rotor switches and a datacenter-wide fabric, RotorNet, constructed from these switches.
3. An analysis showing that RotorNet supports uniform traffic at full bisection bandwidth, worst-case (permutation) traffic at half bisection bandwidth, and reported commercial datacenter traffic with 70–90% of the bandwidth of an ideal Fat Tree network at lower cost.
4. A demonstration and evaluation of RotorNet’s VLB-like routing algorithm on prototype hardware.

Additional information on RotorNet can be found at http://www.sysnet.ucsd.edu/circuit-switching.

## Background and Related Work
RotorNet builds upon a line of research integrating optical circuit switching into datacenter networks. Table 1 compares the control planes of previously proposed OCS-based topologies, abstracting away implementation details and focusing on topology and control plane structure. Most designs try to match the network configuration to current or future traffic demands, requiring a complex, datacenter-wide control plane that is difficult to scale.

### Packet-Switched Folded-Clos Networks
Figure 1 depicts a traditional Clos-based network topology with a single layer of core switches. Servers are aggregated into racks, and each server is connected to its local top-of-rack (ToR) switch. Each ToR has multiple uplinks to core switches, forming a multi-stage network that logically acts as a single, large crossbar switch. In a folded-Clos network, end hosts are decoupled from the packet switches, and servers can send data to any destination with the expectation that the fabric will deliver it without further communication with the sender. Each packet switch has internal buffering to absorb bursts and a mechanism for conveying traffic at line rate.

### Circuit-Switched Network Fabrics
Prior OCS-based proposals fundamentally change the nature of the switch control plane. Proposals that dynamically reconfigure the network topology in response to observed or predicted traffic must carry out the same tasks as electronic packet switches, such as demand collection, scheduling, reconfiguration, and synchronization. However, while packet switches hide these processes within discrete boxes, OCS-based topologies, due to their inability to buffer and inspect packets, expose this control complexity to the entire network, effectively turning the network fabric into a giant, coupled crossbar. Figure 2 illustrates this distinction, depicting the differences between electronic packet switches and OCS-based topologies.