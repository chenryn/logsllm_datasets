We have to point out that, though we built the ground
truth of benign domains according to the common practice
made in past eﬀorts, it has its own limitations. In particu-
lar, Alexa top ranked domains are highly popular domains.
They are in general of high-quality and well-maintained. A
scheme with low false positive rate for Alexa top domains
does not necessarily imply the same when it is applied to the
large amount of benign but unpopular domains.
In other
words, a measure of false positive rates based on Alexa top
domains tends to be lower than the actual false positive rate.
Unfortunately, there is no well accepted practice for deter-
mining that a domain is benign, nor there are any large scale
dataset of benign domains beyond Alexa top domains. Our
evaluation thus has to rely on Alexa top domains.
4.2 Experiment results
For the domain graph built from each dataset, we vary
the set size of the seeds and the threshold to study their
impacts on the three metrics. Speciﬁcally for each given
seed size k, we randomly select k domains from the mali-
cious ground truth as the seeds, and calculate the malicious
scores of all other domains in a domain graph. We then vary
the malicious threshold and measure the true positives, false
positives, as well as the expansion. Each experiment is run
10 times with diﬀerent randomly selected seeds, and the av-
erage of each metrics is reported. For the size of seeds, we set
it to be 0.05% all the way to 2% of the number of domains
1	
  10	
  100	
  1000	
  10000	
  100000	
  0	
  5	
  10	
  15	
  20	
  25	
  30	
  35	
  40	
  45	
  Number	
  of	
  domains	
  one-­‐week	
  two-­‐week	
  in the domain graph. We choose to use a very small portion
of the ground truth to investigate how well our scheme can
discover more malicious domains even with limited knowl-
edge of known malicious domains. As to the malicious score
threshold, we vary it all the way from 0.5 to 0.95.
4.2.1 Varying Malicious Score Threshold
We ﬁrst study the tradeoﬀ between true positives and false
positives, when varying the malicious score threshold. Intu-
itively, the lower the threshold, the higher the true positive
and meanwhile the higher the false positives. Figure 5 shows
the ROC curves of the false positive and the true positive
rates, when the size of the seeds is 0.3%, 0.5%, 0.7%, and
0.9% for the two datasets. From ﬁgure 5a we see that our
scheme can achieve above 90% true positive rate with a false
positive rate lower than 0.2% in the one-week dataset. In
general, the lower the malicious threshold is, the higher the
false positive rate. However, it is interesting to observe that
when the seed size is small (e.g., 0.3%), even for low ma-
licious thresholds, we can still get high true positive rates
(around 90%) with very low false positive rates (lower than
0.01%). The reason is that when the set of seeds is small, a
domain can only get its malicious score from a few connected
seeds. Therefore, even a low malicious score suggests strong
association with known malicious domains. On the other
hand, when the set of seeds is large, a domain may get its
malicious score due to weak associations with many seeds,
which has a higher chance to be a false positive. Therefore,
for a large set of seeds, a higher malicious threshold is needed
to reduce false positives. Meanwhile, if the threshold is very
high (above 0.9), even with a relatively large set of seeds,
true positive rates drop dramatically. Figure 5 suggests that
in general a threshold between 0.7-0.85 yields good tradeoﬀ
between true positives and false positives.
Meanwhile, from ﬁgure 5b we observe that, though the
general trend of tradeoﬀ between true and false positives
of the two-week dataset is similar to that in the own-week
dataset, it is clearly worse than that of the one-week dataset.
To have a false positive rate around 0.5%, our scheme can
only achieve a true positive rate around 90% but not much
higher. After a closer examination of the two-week dataset,
we observe that the number of new domain resolutions in
the second week of November 2014 is smaller than that in
the ﬁrst week. Therefore, compared to the one-week domain
graph, the new domains and edges in the two-week domain
graph are mainly due to pairs of domains who have common
IPs in two weeks but with no common IPs in each individual
week. For example, suppose an edge {d1, d2} appears in the
two-week domain graph but not in the one-week one, and
they have two common IPs i1 and i2 from diﬀerent ASNs.
Then either the resolutions from d1 and d2 to i1 and i2
all happen in the second week, or these resolutions happen
across two weeks. Our examination shows that the latter
case accounts for the majority of new edges in the two-week
domain graph.
Intuitively, if the sharing of common IPs
between two domains happens in a short period of time, it
indicates a stronger association between them. On the other
hand, the longer the period is, the more likely the sharing of
common IPs happens unintentionally, and thus less reliable
for malicious domain inferences. Since the majority of new
edges are due to sharing of IPs across two weeks instead
of a single week, the malicious inference from the two-week
dataset is less eﬀective than that from the one-week dataset.
The above observation shows that temporal granularity of
datasets to build domain graphs would also aﬀect the eﬀec-
tiveness of our scheme. Naturally, if the granularity is too
small (e.g., one hour), we would miss a lot of associations
between malicious domains as shared IPs are not formed yet.
Meanwhile, if the granularity is too big (e.g., ﬁve years), a
lot of false positives will be introduced due to weak associa-
tions. One possible solution is to introduce temporal factors
into the weight of edges. Particularly, depending on how
temporally close two domains share an IP (within one week,
two weeks, one month, etc.), the contribution of the shared
IP to the weight between the two domains will be diﬀerent
to capture the above observation. We leave the investigation
of the above solution as part of our future work.
4.2.2 Varying Size of the Set of Seeds
Figure 6 shows for both datasets the ROC curves when
the malicious thresholds are set to 0.55, 0.65, 0.75, and 0.85.
The size of seeds is varied from 0.05% all the way to 2% of
the domain graph size. We see that, for a given thresh-
old, especially for relatively small ones (e.g., 0.55 and 0.65),
increasing the seed size will cause a quick jump of false pos-
itives, due to reasons explained above (i.e., with a large set
of seeds, a domain may get its malicious score because of
weak associations with many seeds). It is clear that, when
the threshold is high (e.g., 0.85), false positives are well con-
trolled even for large seeds.
The above experiment results suggest that to have a good
tradeoﬀ between true positives and false positives we could
either have small set of seeds with low malicious thresh-
olds or have a large set of seeds (relative to all malicious
domains) while setting the threshold relatively high (be-
tween 0.7 to 0.85). In practice, however, we do not know
for sure whether the known malicious domains we collect is
large enough. Thus, the general practice would be to ob-
tain as many known malicious domains as possible to form
the seeds, and then set a high threshold value (e.g., 0.85) to
avoid high false positives.
We again observe that the ROC curve of the two-week
dataset is inferior to that of the one-week dataset, due to
the same reason as explained above.
4.2.3 Expansion
Expansion reﬂects how many more potentially malicious
domains we can discover given a set of seeds.
Ideally, we
would like to have a large expansion while maintaining high
true positive rates and low false positive rates. In this exper-
iment, we choose several parameter conﬁgurations (seeds set
size and malicious threshold) which yield high true positive
rates (≥ 0.9) and low false positive rates (≤ 0.01), and then
plot the expansion against the seed size. Figure 7a shows the
ROC curves for all the conﬁgurations we have tested for the
one-week dataset. Conﬁgurations that fall into the dashed
box are chosen to plot their expansions, which is shown in
ﬁgure 7b. We see that even with moderate seed sizes (0.1%
to 0.7% of the domain graph size), our scheme can discover
around 8000 to 12000 potential malicious domains, which is
one to two orders of magnitude of the original seeds set size.
We have a similar observation about expansion for the
two-week dataset, as shown in ﬁgure 8. Speciﬁcally, for con-
ﬁgurations that yield high true positive rates (≥ 0.9) and
low false positive rates (≤ 0.01), their expansions range from
around 16000 to 29000 while the seed sizes vary from 200
9
(a) one-week datatset
(b) two-week datatset
Figure 5: The ROC curves of the true positive rate and the false positive rate when varying the malicious threshold
(a) one-week datatset
(b) two-week datatset
Figure 6: The ROC curves of the true positive rate and the false positive rate when varying the size of seeds
to 1000. Also note that there are much fewer conﬁgurations
plotted in ﬁgure 8 than in ﬁgure 7, for reasons given before.
4.2.4 Compare with Belief Propagation
As mentioned in section 2, several recent work proposes
to use belief propagation to infer malicious entities, e.g., do-
mains and ﬁles. One of the representative approaches is by
Pratyusa et al. [7], which applies belief propagation to bi-
partite host-domain graphs based on seeds of both known
malicious domains (from proprietary blacklists) and benign
domains (from Alexa top ranked domains). As a domain
resolution graph is also bipartite with one side being do-
mains, it seems appealing to apply belief propagation on a
domain resolution graph to discover malicious domains. In
this section, we experimentally investigate the eﬀectiveness
of using belief propagation in our context. In particular, we
consider the bipartite domain resolution graph of the one-
week dataset, and construct the ground truth of malicious
domains as described in section 4.1. For the ground truth
of benign domains, we built it from Alex top ranked 10000
domains as used in [7]. We perform k-fold tests to get
the true and false positive rates (i.e., the ground truth are
evenly divided into k parts randomly. k − 1 parts are used
as seeds for belief propagation, and the remaining one part
is for testing to compute true and false positive rates). We
use the same priors and edge potentials as in [7] for belief
propagation (shown in tables 3 and 4). The result of the
experiment is shown in ﬁgure 9.
Domain
Malicious
Benign
Unknown
P(malicious) P(benign)
0.99
0.01
0.5
0.01
0.99
0.5
Table 3: Priors assigned to a domain according to
the domain’s state for belief propagation
Benign Malicious
Benign
Malicious
0.51
0.49
0.49
0.51
Table 4: Edge potential matrices for belief propa-
gation
We see that, for the approach of using belief propagation,
to get a meaningful true positive rate (around or above 90%)
the false positive rate would be around 40% or higher, which
10
0.85	
  0.86	
  0.87	
  0.88	
  0.89	
  0.9	
  0.91	
  0.92	
  0.93	
  0.94	
  0.95	
  0.96	
  0	
  0.002	
  0.004	
  0.006	
  0.008	
  0.01	
  0.012	
  True	
  posi*ve	
  rate	
  False	
  pos*ve	
  rate	
  Seed	
  =	
  0.3%	
  Seed	
  =	
  0.5%	
  Seed	
  =	
  0.7%	
  Seed	
  =	
  0.9%	
  0.85	
  0.86	
  0.87	
  0.88	
  0.89	
  0.9	
  0.91	
  0.92	
  0.93	
  0.94	
  0.95	
  0.96	
  0	
  0.005	
  0.01	
  0.015	
  0.02	
  0.025	
  0.03	
  True	
  posi*ve	
  rate	
  False	
  posi*ve	
  rate	
  Seed	
  =	
  0.3%	
  Seed	
  =	
  0.5%	
  Seed	
  =	
  0.7%	
  Seed	
  =	
  0.9%	
  0.85	
  0.86	
  0.87	
  0.88	
  0.89	
  0.9	
  0.91	
  0.92	
  0.93	
  0.94	
  0.95	
  0.96	
  0.97	
  0.98	
  0	
  0.0025	
  0.005	
  0.0075	
  0.01	
  0.0125	
  0.015	
  0.0175	
  0.02	
  True	
  posi*ve	
  rate	
  False	
  posi*ve	
  rate	