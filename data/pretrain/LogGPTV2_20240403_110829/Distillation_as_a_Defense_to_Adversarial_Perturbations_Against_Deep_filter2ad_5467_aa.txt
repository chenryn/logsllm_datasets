title:Distillation as a Defense to Adversarial Perturbations Against Deep
Neural Networks
author:Nicolas Papernot and
Patrick D. McDaniel and
Xi Wu and
Somesh Jha and
Ananthram Swami
2016 IEEE Symposium on Security and Privacy
2016 IEEE Symposium on Security and Privacy
Distillation as a Defense to Adversarial
Perturbations against Deep Neural Networks
Nicolas Papernot∗, Patrick McDaniel∗, Xi Wu§, Somesh Jha§, and Ananthram Swami‡
∗Department of Computer Science and Engineering, Penn State University
§Computer Sciences Department, University of Wisconsin-Madison
‡United States Army Research Laboratory, Adelphi, Maryland
{ngp5056,mcdaniel}@cse.psu.edu, {xiwu,jha}@cs.wisc.edu, PI:EMAIL
Abstract—Deep learning algorithms have been shown to per-
form extremely well on many classical machine learning prob-
lems. However, recent studies have shown that deep learning,
like other machine learning techniques, is vulnerable to adver-
sarial samples: inputs crafted to force a deep neural network
(DNN) to provide adversary-selected outputs. Such attacks can
seriously undermine the security of the system supported by the
DNN, sometimes with devastating consequences. For example,
autonomous vehicles can be crashed, illicit or illegal content can
bypass content ﬁlters, or biometric authentication systems can be
manipulated to allow improper access. In this work, we introduce
a defensive mechanism called defensive distillation to reduce the
effectiveness of adversarial samples on DNNs. We analytically
investigate the generalizability and robustness properties granted
by the use of defensive distillation when training DNNs. We also
empirically study the effectiveness of our defense mechanisms on
two DNNs placed in adversarial settings. The study shows that
defensive distillation can reduce effectiveness of sample creation
from 95% to less than 0.5% on a studied DNN. Such dramatic
gains can be explained by the fact that distillation leads gradients
used in adversarial sample creation to be reduced by a factor of
1030. We also ﬁnd that distillation increases the average minimum
number of features that need to be modiﬁed to create adversarial
samples by about 800% on one of the DNNs we tested.
I. INTRODUCTION
Deep Learning (DL) has been demonstrated to perform
exceptionally well on several categories of machine learning
problems, notably input classiﬁcation. These Deep Neural
Networks (DNNs) efﬁciently learn highly accurate models
from a large corpus of training samples, and thereafter classify
unseen samples with great accuracy. As a result, DNNs
are used in many settings [1], [2], [3], some of which are
increasingly security-sensitive [4], [5], [6]. By using deep
learning algorithms, designers of these systems make implicit
security assumptions about deep neural networks. However,
recent work in the machine learning and security communities
have shown that adversaries can force many machine learning
models, including DNNs, to produce adversary-selected out-
puts using carefully crafted inputs [7], [8], [9].
Speciﬁcally, adversaries can craft particular inputs, named
adversarial samples, leading models to produce an output
behavior of their choice, such as misclassiﬁcation. Inputs are
crafted by adding a carefully chosen adversarial perturbation to
a legitimate sample. The resulting sample is not necessarily un-
natural, i.e. outside of the training data manifold. Algorithms
crafting adversarial samples are designed to minimize the per-
turbation, thus making adversarial samples hard to distinguish
from legitimate samples. Attacks based on adversarial samples
occur after training is complete and therefore do not require
any tampering with the training procedure.
To illustrate how adversarial samples make a system based
on DNNs vulnerable, consider the following input samples:
a car
a cat
The left image is correctly classiﬁed by a trained DNN as a
car. The right image was crafted by an adversarial sample al-
gorithm (in [7]) from the correct left image. The altered image
is incorrectly classiﬁed as a cat by the DNN. To see why such
misclassiﬁcation is dangerous, consider deep learning as it is
commonly used in autonomous (driverless) cars [10]. Systems
based on DNNs are used to recognize signs or other vehicles
on the road [11]. If perturbing the input of such systems, by
slightly altering the car’s body for instance, prevents DNNs
from classifying it as a moving vehicule correctly, the car
might not stop and eventually be involved in an accident, with
potentially disastrous consequences. The threat is real where
an adversary can proﬁt from evading detection or having their
input misclassiﬁed. Such attacks commonly occur today in
non-DL classiﬁcation systems [12], [13], [14], [15], [16].
Thus, adversarial samples must be taken into account when
designing security sensitive systems incorporating DNNs.
Unfortunately, there are very few effective countermeasures
available today. Previous work considered the problem of
constructing such defenses but solutions proposed are deﬁ-
cient in that they require making modiﬁcations to the DNN
architecture or only partially prevent adversarial samples from
being effective [9], [17] (see Section VII).
Distillation is a training procedure initially designed to
train a DNN using knowledge transferred from a different
DNN. The intuition was suggested in [18] while distillation
itself was formally introduced in [19]. The motivation behind
the knowledge transfer operated by distillation is to reduce
the computational complexity of DNN architectures by trans-
ferring knowledge from larger architectures to smaller ones.
This facilitates the deployment of deep learning in resource
constrained devices (e.g. smartphones) which cannot rely on
powerful GPUs to perform computations. We formulate a new
variant of distillation to provide for defense training: instead
of transferring knowledge between different architectures, we
propose to use the knowledge extracted from a DNN to
improve its own resilience to adversarial samples.
2375-1207/16 $31.00 © 2016 IEEE
© 2016, Nicolas Papernot. Under license to IEEE.
DOI 10.1109/SP.2016.41
DOI 10.1109/SP.2016.41
582
582
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:13:13 UTC from IEEE Xplore.  Restrictions apply. 
In this paper, we explore analytically and empirically the
use of distillation as a defensive mechanism against adversarial
samples. We use the knowledge extracted during distillation
to reduce the amplitude of network gradients exploited by
adversaries to craft adversarial samples. If adversarial gra-
dients are high, crafting adversarial samples becomes easier
because small perturbations will induce high DNN output
variations. To defend against such perturbations, one must
therefore reduce variations around the input, and consequently
the amplitude of adversarial gradients. In other words, we use
defensive distillation to smooth the model learned by a DNN
architecture during training by helping the model generalize
better to samples outside of its training dataset.
At test time, models trained with defensive distillation are
less sensitive to adversarial samples, and are therefore more
suitable for deployment in security sensitive settings. We make
the following contributions in this paper:
• We articulate the requirements for the design of adver-
sarial sample DNN defenses. These guidelines highlight
the inherent tension between defensive robustness, output
accuracy, and performance of DNNs.
• We introduce defensive distillation, a procedure to train
DNN-based classiﬁer models that are more robust
to
perturbations. Distillation extracts additional knowledge
about training points as class probability vectors produced
by a DNN, which is fed back into the training regimen.
This departs substantially from the past uses of distillation
which aimed to reduce the DNN architectures to improve
computational performance, but rather feeds the gained
knowledge back into the original models.
• We analytically investigate defensive distillation as a
security countermeasure. We show that distillation gener-
ates smoother classiﬁer models by reducing their sensi-
tivity to input perturbations. These smoother DNN classi-
ﬁers are found to be more resilient to adversarial samples
and have improved class generalizability properties.
• We show empirically that defensive distillation reduces
the success rate of adversarial sample crafting from
95.89% to 0.45% against a ﬁrst DNN trained on the
MNIST dataset [20], and from 87.89% to 5.11% against
a second DNN trained on the CIFAR10 [21] dataset.
• A further empirical exploration of the distillation parame-
ter space shows that a correct parameterization can reduce
the sensitivity of a DNN to input perturbations by a
factor of 1030. Successively, this increases the average
minimum number of input features to be perturbed to
achieve adversarial targets by 790% for a ﬁrst DNN, and
by 556% for a second DNN.
II. ADVERSARIAL DEEP LEARNING
Deep learning is an established technique in machine learn-
ing. In this section, we provide some rudiments of deep neural
networks (DNNs) necessary to understand the subtleties of
their use in adversarial settings. We then formally describe
two attack methods in the context of a framework that we
construct to (i) develop an understanding of DNN vulnerabil-
ities exploited by these attacks and (ii) compare the strengths
M components
N components
…
…
…
0.01
0.93
0.02
0.01
Hidden Layers{
Input Vector
X
Neuron
Last Hidden 
Layer 
Z(X)
Softmax 
Layer 
F (X)
Weighted Link  (weight is a parameter       of     )
θF F
Fig. 1: Overview of a DNN architecture: This architecture,
suitable for classiﬁcation tasks thanks to its softmax output
layer, is used throughout the paper along with its notations.
and weaknesses of both attacks in various adversarial settings.
Finally, we provide an overview of a DNN training procedure,
which our defense mechanism builds on, named distillation.
A. Deep Neural Networks in Adversarial Settings
Training and deploying DNN architectures - Deep neural
networks compose many parametric functions to build increas-
ingly complex representations of a high dimensional input
expressed in terms of previous simpler representations [22].
Practically speaking, a DNN is made of several successive
layers of neurons building up to an output layer. These layers
can be seen as successive representations of the input data [23],
a multidimensional vector X, each of them corresponding to
one of the parametric functions mentioned above. Neurons
constituting layers are modeled as elementary computing units
applying an activation function to their input. Layers are
connected using links weighted by a set of vectors, also
referred to as network parameters θF . Figure 1 illustrates such
an architecture along with notations used in this paper.
The numerical values of weight vectors in θF are evaluated
during the network’s training phase. During that phase, the
DNN architecture is given a large set of known input-output
pairs (X, Y ) ∈ (X ,Y). It uses a series of successive forward
and backward passes through the DNN layers to compute pre-
diction errors made by the output layer of the DNN, and cor-
responding gradients with respect to weight parameters [24].
The weights are then updated, using the previously described
gradients, in order to improve the prediction and eventually
the overall accuracy of the network. This training process
is referred to as backpropagation and is governed by hyper-
parameters essential to the convergence of model weight [25].
The most important hyper-parameter is the learning rate that
controls the speed at which weights are updated with gradients.
583583
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:13:13 UTC from IEEE Xplore.  Restrictions apply. 
t
e
s
a
t
a
D
T
S
N
M
I
5
8
2
4
3
t
e
s
a
t
a
D
0
1
R
A
F
C
I
bird
airplane
truck
automobile
bird
Fig. 2: Set of legitimate and adversarial samples for two
datasets: For each dataset, a set of legitimate samples, which
are correctly classiﬁed by DNNs, can be found on the top
row while a corresponding set of adversarial samples (crafted
using [7]), misclassifed by DNNs, are on the bottom row.
Once the network is trained, the architecture together with
its parameter values θF can be considered as a classiﬁcation
function F and the test phase begins: the network is used on
unseen inputs X to predict outputs F (X). Weights learned
during training hold knowledge that the DNN applies to these
new and unseen inputs. Depending on the type of output ex-
pected from the network, we either refer to supervised learning
when the network must learn some association between inputs
and outputs (e.g., classiﬁcation [1], [4], [11], [26]) or unsu-
pervised learning when the network is trained with unlabeled
inputs (e.g., dimensionality reduction, feature engineering, or
network pre-training [21], [27], [28]). In this paper, we only
consider supervised learning, and more speciﬁcally the task of
classiﬁcation. The goal of the training phase is to enable the
neural network to extrapolate from the training data it observed
during training so as to correctly predict outputs on new and
unseen samples at test time.
Adversarial Deep Learning - It has been shown in previous
work that when DNNs are deployed in adversarial settings,
one must take into account certain vulnerabilities [7], [8], [9].
Namely, adversarial samples are artifacts of a threat vector
against DNNs that can be exploited by adversaries at test
time, after network training is completed. Crafted by adding
carefully selected perturbations δX to legitimate inputs X,
their key property is to provoke a speciﬁc behavior from
the DNN, as initially chosen by the adversary. For instance,
adversaries can alter samples to have them misclassiﬁed by a
DNN, as is the case of adversarial samples crafted in experi-
ments presented in section V, some of which are illustrated in
Figure 2. Note that the noise introduced by perturbation δX
added to craft the adversarial sample must be small enough to
allow a human to still correctly process the sample.
584584
Attacker’s end goals can be quite diverse, as pointed out in
previous work formalizing the space of adversaries against
deep learning [7]. For classiﬁers,
they range from simple
conﬁdence reduction (where the aim is to reduce a DNN’s
conﬁdence on a prediction, thus introducing class ambiguity),
to source-target misclassiﬁcation (where the goal is to be able
to take a sample from any source class and alter it so as to
have the DNN classify it in any chosen target class distinct
from the source class). This paper considers the source-target
misclassiﬁcation, also known as the chosen target attack,
in the following sections. Potential examples of adversarial
samples in realistic contexts could include slightly altering
malware executables in order to evade detection systems built
using DNNs, adding perturbations to handwritten digits on
a check resulting in a DNN wrongly recognizing the digits
(for instance, forcing the DNN to read a larger amount
than written on the check), or altering a pattern of illegal
ﬁnancial operations to prevent it from being picked up by
fraud detections systems using DNNs. Similar attacks occur
today on non-DNN classiﬁcation systems [12], [13], [14], [15]
and are likely to be ported by adversaries to DNN classiﬁers.
As explained later in the attack framework described in this
section, methods for crafting adversarial samples theoretically
require a strong knowledge of the DNN architecture. However
in practice, even attackers with limited capabilities can per-
form attacks by approximating their target DNN model F and
crafting adversarial samples on this approximated model. In-
deed, previous work reported that adversarial samples against
DNNs are transferable from one model to another [8]. Skilled
adversaries can thus train their own DNNs to produce ad-
versarial samples evading victim DNNs. Therefore throughout
this paper, we consider an attacker with the capability of
accessing a trained DNN used for classiﬁcation, since the
transferability of adversarial samples makes this assumption
acceptable. Such a capability can indeed take various forms in-
cluding for instance a direct access to the network architecture
implementation and parameters, or access to the network as an