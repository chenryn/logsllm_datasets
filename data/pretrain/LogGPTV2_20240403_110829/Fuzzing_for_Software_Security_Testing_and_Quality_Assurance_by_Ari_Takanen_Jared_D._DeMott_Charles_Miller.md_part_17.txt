automation can include all the code quality and testing practices previously men-
tioned in this chapter.
Techniques like static analysis, unit testing, integration testing and regression
testing can be applied for every build, but some techniques take longer time to execute
and should be applied less frequently. For these kind of tasks, modern CI systems
allow scheduled jobs. When resource are sparse, these jobs can be scheduled to be
executed at a time when less people are using those resources, like during weekends
or after working hours.
As model-based fuzzers in particular can be extremely flexible in testing specific
areas of the input space, the test automation can target those areas in daily build
tests that would otherwise take weeks to execute later in the development cycle.
Advanced CI automation can separate different types of fuzzing activities depend-
ing on the time allocated for the tests: targeted element or message specific tests in
daily tests, more generic protocol layer tests in weekly runs, and more complete tests
6760 Book.indb 99 12/22/17 10:50 AM
100 Quality Assurance and Testing
for the entire attack surface in monthly tests. An example of running fuzzers as a
part of CI system can be found in Section 7.8.2, presenting Google’s ClusterFuzz.
3.11 Summary
In this chapter, we have given an overview of testing approaches that can be useful
to you when integrating security testing and specifically fuzzing into a standard
quality assurance process. Both quality assurance and testing are traditionally
feature-oriented approaches, in which the purpose is to validate that the software
works according to the specifications. Security is an abstract term that means dif-
ferent things to different people. Security has been seen as an add-on by software
development. The main difference to testing is that security testing does not aim to
prove that the software is secure but to break software by whatever means available.
Various testing approaches are used at different phases of the software develop-
ment life cycle. White-box testing can benefit from the availability of the source code.
Black-box testing, on the other hand, relies on the external interfaces in the testing.
Gray-box testing is a combination of these approaches. Testing also has different
purposes. Conformance testing validates the functionality, whereas performance
testing tries to find the performance limitations by testing the software with extreme
loads. Various other testing approaches, such as interoperability testing and regres-
sion testing, aim at proving that the software is able to work with other industry
products and that no previously known flaws are reintroduced to the software.
Testing has to be measurable, and quality assurance practices have used vari-
ous means of validating the quality of the tests themselves. Specification coverage
compares the test efficiency to the specifications that were used to build the tests.
Input space coverage looks at the potential inputs that can be given to the software
and measures the coverage of the tests against those. Interface coverage is a black-
box specific metric that looks at the communication interfaces and the efficiency of
tests to cover them. Finally, code coverage metrics analyze the software and indicate
which parts of the code were touched by the tests.
We concluded the chapter by reviewing the black-box techniques for security
testing. Load testing prepares the QA organization for load-based attacks such as
DDoS attacks. Stress testing looks at the internal threats related to, for example,
low memory resources on embedded devices. Security scanners are not really pro-
active, but are a requirement when you integrate your own system with other off-
theshelf systems and platforms. For example, a flaw left into the operating system
will make all good QA practices void if left undetected. Unit testing is the first place
to introduce fuzzing by testing the smallest components of the software through
the available interfaces. Input fault injection, on the other hand, is already one of
the related technologies to fuzzing, almost a synonym. Syntax testing is a technique
used to test formal interfaces such as protocols, and negative testing approaches
such as robustness testing extend those techniques to fuzzing. Finally, regression
testing builds on top of known flaws in released software trying to prevent the same
flaws from reappearing in later versions of the same software, and CI does the same
within agile development processes where each cycle could be seen as a release of
software, with potential regression in quality.
6760 Book.indb 100 12/22/17 10:50 AM
C h a p t e r 4
Fuzzing Metrics
Fuzzing is not widely used in software development processes, especially in small
companies. Hopefully this will change as people learn the motivation in adding
fuzzing into both software development lifecycle (SDLC) and purchase/evaluation
processes, and understand the metrics from successful fuzzing experiences. The
purpose of this chapter is to look at metrics, why fuzzing is important and how
this can be explained to your management.
One obstacle in introducing fuzzers to your developers could be that most gen-
erally available fuzzing tools are developed by security people for security people,
and hence are hard to use by people who are not security experts. At the very least,
most of the fuzzing tools were not designed with easy inclusion into an SDLC as
a goal. Fortunately, more and more companies are seeing that proactive security
is at its best when integrated to the development process. You cannot test security
into a product; it has to be built in. This sets new requirements for fuzzing tools in
how they integrate with existing development processes. Fuzzer developers need to
focus on how they could improve the available fuzzing tools in such a way that the
industry would also adapt them into their development practices. And with require-
ments, you also need metrics.
Software manufacturers and enterprise customers use both commercial and also
internally built home-grown fuzzing tools, or sometimes outsource the fuzz testing
to a security consultancy. We need to understand how these processes differ. The
major difference between vulnerability analysis (VA)1 and quality assurance (QA)
is in the attitude of the testers and in the purpose of the tests.
The practices of vulnerability analysis are more targeted toward defect discovery,
especially when compared to the verification and validation aspects of traditional
quality assurance. The goal or purpose of vulnerability analysis is to study the
completed product for vulnerabilities, using whatever means available. Methods are
typically reactive in nature (i.e., they are based on knowledge of known mistakes
and problems and in reiteration of those attacks in new scenarios). Unfortunately,
the attitude of vulnerability analysis is not to conduct a thorough systematic test,
but to assess a subset of the product and draw conclusions based on those findings.
VA never tries to claim that the product is 100% tested and fault-free. The metrics
related to VA are subjective in nature, based on probabilities or assurance levels.
The quality of the tests in VA is based on the allocated time, tools, and knowledge
base of the people conducting the security analysis. VA processes are difficult to
1 Vulnerability assessment is also known as security testing, security assessment, security researching,
bug hunting, or even hacking.
101
6760 Book.indb 101 12/22/17 10:50 AM
102 Fuzzing Metrics
define, and the results are difficult to measure. VA will sometimes use tools such as
reverse engineering and source-code auditing, as these techniques can potentially
find vulnerabilities that black-box techniques are not capable of finding.
Fuzzing as part of vulnerability analysis can be a true black-box technique,
meaning no source code is needed in the process. Security problems are analyzed
after the product is complete, and the people conducting the assessment are typically
not involved in the development and testing phases of the software development.
The design documents and source code are usually not available in a vulnerability
analysis process. The system under test (SUT) can truly sometimes be a black box
to the security auditor, a device with no methods of instrumenting or monitoring
the internal operation of the device under test (DUT). Security auditors study the
software from a third-party perspective. This tiger-team approach2 used in vulner-
ability analysis is similar to the practices used in the military. A team of security
experts (a red team) will masquerade as a hostile party and try to infiltrate the
security of the system with the same tools and techniques real hackers would use.
A study of the vulnerabilities can be done with or without knowing anything about
the internals of the system. Access to source code may improve the results (i.e.,
enable the auditors to discover more vulnerabilities), but at the same time this can
compromise the results, as the findings might be different from what real adversar-
ies would likely find.
In summary, fuzzing as part of VA does not try to verify or validate a system,
but rather attempts to find defects. The goal is to uncover as many vulnerabilities
in the system as possible within a given time frame and to provide a metric of the
security of the system, a security assurance level.
On the other hand, the goal of quality assurance is to follow a standard pro-
cess built around the system requirements and to validate that those requirements
are met in the product. This verification and validation (V&V) aspect of quality
assurance has driven testing into the feature-and-performance-oriented approach
that most people identify it with. Testing in most cases is no longer aiming to find
most flaws in the product, but to validate a predefined criterion for acceptance or
conformance to a set of requirements.
Testing experts such as Boris Beizer have, since at least 1990,3 been proposing
that testers should look back and shift their focus from rote verification and valida-
tion toward true discovery of flaws. Similar to VA, the purpose of testing should also
be to find defects, not to rubber stamp a release. Fortunately, fuzzing as a security
testing technique has emerged to teach this to us the hard way. Any negligence in
finding security flaws is unacceptable, and therefore the need for security has the
potential to change the behavior of testers in QA processes.
Security research is still immature compared to the legacy of research in the
fields of software development and testing. Security researchers should try to learn
from the experiences of computer science. This is especially true in the areas of
2 M. Laakso, A. Takanen, J. Röning. “The Vulnerability Process: A Tiger Team Approach to Resolving
Vulnerability Cases.” In Proceedings of the 11th FIRST Conference on Computer Security Incident
Handling and Response. Brisbane, Australia. June 13–18, 1999.
3 Boris Beizer. (1990). Software Testing Techniques, 2nd ed. International Thomson Computer Press.
6760 Book.indb 102 12/22/17 10:50 AM
4.1 Threat Analysis and Risk-Based Testing 103
metrics. In the rest of this chapter we will study metrics and techniques drawn
from both security assessment and quality assurance, but our focus is in analyzing
them from the fuzzing perspective. One of the goals in this book is to propose some
recommendations on how vulnerability assessments could be integrated to quality
assurance to enable the discovery of vulnerabilities earlier in the software life cycle.
4.1 Threat Analysis and risk-Based Testing
To effectively introduce fuzzing into vulnerability analysis processes or quality assur-
ance processes, we need to conduct a careful threat analysis that studies the related
threats, vulnerabilities (or exposures), and the assets that need protecting. Threat
analysis is often identified with security assessment practices, but for our purpose
it is also very similar to the risk assessment process used in risk-based testing. For
quality assurance people, fuzzing is just one additional risk-based testing technique.
For security personnel, fuzzing is just one of the available tools available for elimi-
nating security-related flaws from software. For both, all available options need to
be carefully analyzed to make a decision whether to invest time and resources in
fuzzing and how to apply fuzzing to the development process.
Threat analysis often starts from identifying the security requirements for a
system. A simple division of security requirements could stem from the well-known
set of security goals, namely
1. Confidentiality;
2. Integrity;
3. Availability.
These and other security goals can be specified in a security policy for a spe-
cific network service or for an individual product. The same requirements can
also be studied from a quality perspective. For each security requirement, we can
then analyze:
1. Threat agents and events;
2. Available attacks that these threat agents can execute to realize an event;
3. Potential weaknesses, vulnerabilities, or flaws that these attacks would exploit.
The components of threat analysis mentioned above are assumptions; that is,
all threats, attacks, and vulnerabilities would be impossible to enumerate. But even
a subset of the reality can already provide some level of assurance about the future
probability of an incident against the security goals. Well-defined security goals
or security policies can immediately eliminate some security considerations. For
example, if confidentiality is a nonissue for a specific product, this will immediately
eliminate the need for further threat analysis of attacks and vulnerabilities related
to confidentiality. On the other hand, if availability is of the highest importance,
then it is obvious that DoS-related attacks are extremely relevant to the applica-
tion at hand.
6760 Book.indb 103 12/22/17 10:50 AM
104 Fuzzing Metrics
Threat analysis often just takes an existing risk analysis further and makes the
results more applicable to product development. We will not study the methods
for enumerating risks, but study threat analysis independently from any possible
risk assessment. If a list of threats is already available, it can be used as a starting
point, or it can be used later in the process to verify that all risks were covered with
a second parallel process.
There are many possible methodologies that can be used to perform threat
analysis. The most common techniques are:
1. Threat tree analysis;
2. Threat database search;
3. Ad hoc threat identification.
4.1.1 Threat Trees
Threat tree analysis is similar to a fault tree analysis technique used in hardware
engineering. It is based on a method in which risks are decomposed and charted
into a decision tree.4 Threat trees are widely used in risk management and reliability
engineering. The problem with threat trees is that you need to be a security expert
to build and to use them.
Building a threat tree involves identifying a complete set of threats on a high
level, and then introducing more details on how each threat could be realized and
what weaknesses need to be present for the threat to be present. The tree view comes
from the analysis technique. The root of the tree is the highest abstraction level
and defines the threat against a specific asset. Each subsequent abstraction layer
refines the threat, providing more information, becoming a root of a more detailed
subtree. Finally, leaf nodes will provide adequate information for countermeasures
or verification techniques such as fuzzing required to discover the possibility and
to eliminate the threat. Identifying the root causes for the threats often requires
security knowledge, and therefore a threat tree might not be feasible for the design-
ers of the software to build with an adequate level of detail.
The threat tree method is usually the most effective approach if the security
problem and its surrounding environment are both well-defined and understood by
the person conducting the threat analysis. In the context of fuzzing, the problem
with threat trees is that although they help in designing a good piece of software,
they do not necessarily help you at all in building your fuzzer. To some, fuzzing is
just an additional countermeasure in a threat tree. We are not saying that threat
trees are useless for fuzzing—on the contrary! Threat trees help you understand
the risks involved with the application at hand and will help you choose the right
fuzzers for testing a specific application. For example, an authentication bypass or
memory leak issue might not be detected if the fuzzer does not do analysis of the
responses to fuzzed requests.
4 Edward Amoroso. (1994). Fundamentals of Computer Security Technology. Upper Saddle River,
NJ: Prentice Hall.
6760 Book.indb 104 12/22/17 10:50 AM
4.1 Threat Analysis and Risk-Based Testing 105
4.1.2 Threat Databases
Threats in various domains tend to be quite similar, and therefore an existing threat
analysis built for a particular system can be reapplied to other systems in the same
domain. Studies of threats in various domains are available to be used as databases
for threat analysis. The benefit is that the person building the threat analysis can
easily analyze whether that threat will apply to the design of this software, with-
out a deep understanding of the security domain. The disadvantage is that a threat
that is unique to the application being built might be missing from the database,
and therefore, be left out of the analysis. Also, the threat descriptions could be too
abstract to be usable without further understanding on how that threat is realized
in the real world. An example of a bad threat definition is denial of service threat,
something you commonly see in threat analysis documents. Almost any attack can
potentially result in the system crashing or becoming nonresponsive, and binding
the DoS threat to one single failure or flaw can distract attention from other causes
of DoS. Therefore, it is possible that the threat analysis could potentially miss the
most significant weaknesses in the system. The level of detail for each threat in
such a database is critical. Simple search of threats from an existing database, or
enumeration of a list of common threats, may suggest the applicable threats more
efficiently than methodological threat tree analysis, especially when the problem is
defined in general terms applicable to that domain or when the final user environ-
ment is not limited when building that specific component. This applies especially
to software developers who do not have knowledge of the actual environments in
which their products will be used.
As an example, let us examine threats listed in the overview of most critical
Web application risks by The Open Web Application Security Project (OWASP),
depicted in Figure 4.1.5 By analysing these threats, we can see that fuzzing could
be applied to detect vulnerabilities from all these threat classes. For example, tools
like Burp Intruder6 and OWASP Zed Attack Proxy (ZAP)7 use fuzz testing as one
of testing methodologies to reveal these types of vulnerabilities.
4.1.3 Ad hoc Threat Analysis
Ad hoc threat analysis is a well-suited practice when the goal is to find one weak-
ness in a very short period of time during an assessment. An experienced security
analyst can immediately recognize potential threats in a system, but for a developer
it can be very challenging to think about the application from the perspective of an
attacker. For fuzzing, a simple method for ad hoc threat analysis might be based
on listing the available interfaces in a system to enumerate the attack surface. For
example, Dmitry Chan conducted a UDP port scan against Motorola Q mobile
phone, with the following results:8
5 OWASP Top Ten Project https://www.owasp.org/index.php/Category:OWASP_Top_Ten_Project.
6 https://portswigger.net/burp/intruder.html.
7 https://www.owasp.org/images/1/16/ZAPpingTheOwaspTop10.pdf.
8 http://blogs.securiteam.com/index.php/archives/853.
6760 Book.indb 105 12/22/17 10:50 AM
106 Fuzzing Metrics
Figure 4.1 OWASP Top 10 most critical Web application risks in 2013. OWASP has updated this
Top 10 list in 2017.
42/udp open|filtered nameserver
67/udp open|filtered dhcps
68/udp open|filtered dhcpc
135/udp open|filtered msrpc
136/udp open|filtered profile
137/udp open|filtered netbios-ns
138/udp open|filtered netbios-dgm