title:SICE: a hardware-level strongly isolated computing environment for
x86 multi-core platforms
author:Ahmed M. Azab and
Peng Ning and
Xiaolan Zhang
SICE: A Hardware-Level Strongly Isolated Computing
Environment for x86 Multi-core Platforms∗
Ahmed M. Azab
North Carolina State University
PI:EMAIL
Peng Ning
North Carolina State University
PI:EMAIL
Xiaolan Zhang
IBM T.J. Watson Research Center
PI:EMAIL
ABSTRACT
SICE is a novel framework to provide hardware-level isola-
tion and protection for sensitive workloads running on x86
platforms in compute clouds. Unlike existing isolation tech-
niques, SICE does not rely on any software component in
the host environment (i.e., an OS or a hypervisor). Instead,
the security of the isolated environments is guaranteed by
a trusted computing base that only includes the hardware,
the BIOS, and the System Management Mode (SMM). SICE
provides fast context switching to and from an isolated envi-
ronment, allowing isolated workloads to time-share the phys-
ical platform with untrusted workloads. Moreover, SICE
supports a large range (up to 4GB) of isolated memory. Fi-
nally, the most unique feature of SICE is the use of multi-
core processors to allow the isolated environments to run
concurrently and yet securely beside the untrusted host.
We have implemented a SICE prototype using an AMD
x86 hardware platform. Our experiments show that SICE
performs fast context switching (67 µs) to and from the iso-
lated environment and that it imposes a reasonable overhead
(3% on all but one benchmark) on the operation of an iso-
lated Linux virtual machine. Our prototype demonstrates
that, subject to a careful security review of the BIOS soft-
ware and the SMM hardware implementation, current hard-
ware architecture already provides abstractions that can sup-
port building strong isolation mechanisms using a very small
SMM software foundation of about 300 lines of code.
Categories and Subject Descriptors
D.4.6 [Operating Systems]: Security and Protection
∗
We would like to thank our shepherds ´Ulfar Erlingsson and
Leendert van Doorn for helping us improve the quality of
the paper, and Adrian Perrig and Jonathan McCune for
their helpful discussion. This work is supported by U.S. Na-
tional Science Foundation (NSF) under grant 0910767, the
U.S. Army Research Oﬃce (ARO) under grant W911NF-08-
1-0105 managed by NCSU Secure Open Systems Initiative
(SOSI)), and an IBM Open Collaboration Faculty Award.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
CCS’11, October 17–21, 2011, Chicago, Illinois, USA.
Copyright 2011 ACM 978-1-4503-0948-6/11/10 ...$10.00.
General Terms
Design, Security
Keywords
Isolation, Trusted Computing, Virtualization Security
1.
INTRODUCTION
In the last few years, a signiﬁcant portion of the IT in-
dustry has moved toward cloud computing. However, cloud
computing services, particularly those relying on hardware
sharing, have been the source of major security concerns.
Firstly, the owner of a workload that runs inside the cloud
needs to trust the cloud service provider. This trust is im-
posed by the commodity x86 hardware architecture that
gives full memory access to the highest privileged software,
which is typically an OS or a hypervisor. Secondly, recent
attacks [16, 32] and vulnerability reports [23, 24] show that
hypervisors are subject to security exploits and can be com-
promised. These attacks form a major threat to users who
intend to run their workloads beside other potentially mali-
cious ones inside the cloud.
Thus, a need emerges for a solution that provides strong
isolation to workloads running in the cloud, yet still allowing
hardware sharing to reduce the operating costs. To provide
strong isolation, we need to minimize the code base that
is granted full access to the memory of running workloads.
Thus, we can minimize the exposure to security vulnerabili-
ties that can evade the isolation provided to these workloads.
Moreover, this code base should be provided with enhanced
security protection and the ability to attest to its integrity.
To achieve this objective, we introduce a prototype sys-
tem that provides a strongly isolated execution environment,
which relies on a trusted computing base (TCB) composed
of the hardware, the BIOS, and the System Management
Mode (SMM).
Our prototype represents a test system that aims to ex-
plore the capability of current hardware platforms in pro-
viding more secure isolated environments. We demonstrate
that current hardware architecture already provides abstrac-
tions that can support strong isolation. Moreover, we show
that building strong isolation mechanisms on top of those
abstractions requires a very small software foundation of
about 300 lines of code (LOC), which tremendously reduces
the TCB size compared with previous techniques.
Since the SMM was neither designed nor implemented
with high-assurance security mechanisms in mind, detailed
security reviews by both CPU and platform vendors would
be necessary to verify that current SMM implementations
375are properly done to support such strong isolation guaran-
tees provided by our prototype in practice.
1.1 Previous Attempts
There have been a few recent attempts to tackle the prob-
lem of isolating sensitive workloads while eliminating the
host OS or hypervisor from the TCB of these workloads.
These attempts can be divided into two main categories:
(1) microhypervisor-based approaches, and (2) hardware-
based approaches.
Microhypervisor-based Approaches: These approaches
rely on a thin, privileged software layer (i.e., a thin hyper-
visor) to provide the required isolation for sensitive work-
loads. Among the notable research eﬀorts in this direction
are NOVA [27] and Trustvisor [19].
NOVA proposes to replace current hypervisors with a mi-
crohypervisor that is around 9 KLOC in size. Despite having
a small TCB compared to commodity hypervisors, NOVA is
still responsible for several management tasks (e.g., address
space management, interrupt and exception handling, and
communication between the running workloads). Thus, its
TCB is still relatively complicated.
Trustvisor minimizes the code base of the microhypervisor
even further (about 2 KLOC for its core functions), so that it
can be used for isolation purposes only. However, Trustvisor
is only designed to handle workloads with a small code base
and only supports systems with a single processor core. This
makes it unsuitable for typical compute clouds.
An eﬀort closely related to these microhypervisor-based
approaches is seL4 [15]. seL4 proposes a technique to for-
mally verify a microkernel, which is around 8.7 KLOC, to
avoid security vulnerabilities. Although this microkernel can
be used for isolation purposes, the formal veriﬁcation process
imposes several restrictions on the microkernel functionality.
Thus, it cannot be extended to fully support all the func-
tionalities required from micorhypervisors yet.
Hardware-based Approaches: These approaches rely on
hardware security extensions to provide isolation for sensi-
tive workloads. The main advantage of these approaches
is the enhanced protection for the isolated workloads (com-
pared with software based techniques).
One notable research eﬀort in this direction is Flicker [20],
which is a system that uses the late launch capability to run
a secure veriﬁable workload. However, the late launch ca-
pability, provided by both Intel [11] and AMD [1], incurs
signiﬁcant overhead (in the magnitude of hundreds of mil-
liseconds) on every context switch to the isolated environ-
ment. Hence, it cannot provide a practical solution for cloud
computing environments.
Another eﬀort, NoHype [14], also relies on hardware iso-
lation through assigning dedicated processor cores to each
running workload. However, NoHype still relies on a thin
software layer to achieve the required protection and man-
age the hardware resources (e.g., page tables). Moreover,
it requires architectural changes to processors and hardware
peripherals, which are slow to realize.
It should be noted that all of the above techniques are re-
search prototypes that make several simplifying assumptions
about their target platforms. These assumptions may hin-
der the applicability or even weaken the security guarantees
provided by these techniques. For instance, the formal ver-
iﬁcation introduced by seL4 does not include the ﬁrmware
or the SMM code, which could negate all seL4 guarantees.
Thus, a practical deployment of any of these research proto-
types, including the prototype we present in this paper, re-
quires a comprehensive consideration of the target platform
that includes the full hardware and ﬁrmware speciﬁcations.
1.2 Introducing SICE
In this paper, we present SICE, which stands for Strongly
Isolated Computing Environment, a framework that pro-
vides a hardware-level isolated execution environment for
x86 hardware platforms. SICE’s main objective is to mini-
mize the TCB required to create an isolated execution en-
vironment on commodity x86 platforms. This isolated envi-
ronment can be used to host a security sensitive workload.
SICE achieves this objective by relying on a TCB that is
only composed of the hardware, the BIOS, and the SMM.
The TCB, which is fundamentally diﬀerent from previous
research, gives SICE principal advantages over both micro-
hypervisors and hardware-based isolation techniques. We
summarize these advantages below.
Smaller Attack Surface: SICE utilizes the hardware pro-
tection provided by commodity x86 processors for the SMM
and the memory that hosts its code, which is called System
Management RAM (SMRAM). There are two fundamental
diﬀerences between the SMM and microhypervisors.
First, the SMM can only be triggered by a single interface,
which is to invoke a System Management Interrupt (SMI).
In SICE, the SMI handler is required to execute one of only
four functions upon receiving an SMI, which are to create,
enter, exit and terminate an isolated environment. Imple-
menting these functions requires the system to run brieﬂy in
the SMM. Moreover, the SMI handler is not required to han-
dle any other interrupts because all interrupts are disabled
upon entering the SMM mode. The SMI handler is also not
responsible for managing the communication between run-
ning workloads. On the other hand, microhypervisors reside
at the system’s highest privileged level. Thus, they have
to handle all system events (e.g., hypercalls, interrupts and
exceptions). They are also required to manage the com-
munication channels (e.g, shared memory pages) between
diﬀerent workloads.
Second, SICE uses the SMRAM to provide the needed
memory isolation. After the SMRAM is initialized by the
BIOS, it can be locked so that no software can access its
contents except for the SMM code. The SMM code can
manage the SMRAM through modifying only two registers,
which has a huge impact on decreasing the size of the TCB.
In contrast, microhypervisors that rely on hardware virtu-
alization have to manage a diﬀerent set of page tables for
every isolated environment to provide memory protection.
To sum up, SICE’s SMM code base is better protected
and less complicated than any microhypervisor. Moreover,
this code does not provide any functionality other than the
required isolation, which results in a very small code base.
For instance, our prototype SMI handler consists of around
300 LOC (excluding cryptographic libraries). This is around
an order of magnitude less than current microhypervisors
(e.g., Trustvisor and NOVA). As discussed in [15], this is
a signiﬁcant diﬀerence when it comes to veriﬁcation cost.
For instance, using the industry rules-of-thumb of $10K per
LOC for common criteria certiﬁcation as a guideline for the
cost of code veriﬁcation, the cost for verifying 500LOC is
$5M versus $100M for 10 KLOC.
Compatibility with Existing Software Systems: Un-
376like isolation techniques that monopolize the highest privi-
leged execution level of the target platform (e.g., Trustvisor),
SICE does not exclude running legacy workloads (e.g., a hy-
pervisor with multiple VMs) on the same physical platform.
In other words, a platform using SICE can oﬀer isolated
environments, and at the same time accommodate legacy
virtualization software.
Feasible Hardware-based Isolation: SICE uses existing
hardware features to provide the required isolation. We have
successfully implemented a SICE prototype using a com-
modity AMD processor. Thus, it does not require funda-
mental changes to current hardware architecture.
Moreover, our performance evaluation shows that SICE
performs a secure context switching with an isolated envi-
ronment that is four orders of magnitude faster than systems
that rely on the late launch capability (e.g., Flicker [20]). It
also uses multi-core processors to allow isolated workloads
to run in parallel to a legacy hypervisor or OS. Thus, SICE
avoids the two main drawbacks of using late launch, which
are the high performance overhead, and the dedication of all
system resources to only one isolated workload.
1.3 SICE Overview
SICE introduces novel techniques that allow the isolated
workloads to run in parallel with a host OS or hypervisor.
For convenience, we refer to the OS or hypervisor along with
all the software running on it as the legacy host, a strongly
isolated computing environment, which supports an isolated
workload, as an isolated environment, and the code that
manages the isolation between these environments as the
SMI handler, or simply SICE.
The SMI handler, which represents the TCB of the iso-
lated environments, resides inside the SMRAM. It is the only
part of our framework that executes in the SMM. The SMI
handler is responsible for two main tasks: 1) maintaining the
memory isolation of the isolated environments, 2) securely
initializing the isolated environments and attesting to their
integrity. In SICE, these tasks require the SMI handler to
run for a very short time.
An isolated environment is composed of two components:
an isolated workload and a security manager. The isolated
workload is a user-provided system that runs in the isolated
environment. It can be any software, ranging from a single
program (e.g., a program that manages secret keys) to a
complete VM (e.g., a VM that runs a web server).
The security manager is a thin software layer that has lim-
ited functionalities such as handling exceptions and manag-
ing page tables. It is mainly responsible for conﬁning the
isolated workload. Due to the commodity hardware limita-
tion on SMRAM size, SICE’s unique hardware-level isola-
tion is not used to protect the legacy host from the isolated
environments. Thus, SICE uses the security manager to
prevent the isolated workload from accessing the memory of
the legacy host. A separate copy of the security manager
is generated by SICE for every isolated workload running
on the system. Both the security manager and the isolated
workload run after the system returns from the SMM.
Though SICE requires the legacy host to trust the secu-
rity managers, it does not weaken the hardware-level isola-
tion provided to the isolated workloads. SICE uses SMM to
protect isolated environments from the legacy host. Even if
a malicious workload (in one isolated environment) compro-
mises its own security manager and consequently the legacy
host, it will not be able to compromise any other isolated
environment running on the same platform.
SICE’s protection of the legacy host may appear to be
equivalent to those provided by microhypervisor-based ap-
proaches such as NOVA. Indeed, the security manager, which
is a thin privileged software layer, is similar to a microhy-
pervisor in terms of its required tasks and code size. How-
ever, SICE also provides hardware-level, stronger protection
for the isolated environments, which are not available in
microhypervisor-based approaches.
SICE provides two operating modes: time-sharing mode
and multi-core mode. In both modes, the SICE philosophy
is based on using the isolated environments to run security
sensitive workloads, while the legacy host is used for running
less sensitive workloads and managing hardware peripherals.
In a typical execution scenario, a communication channel is
used so that the legacy host provides hardware services (e.g.,
networking) to the isolated environments. The communi-
cation channel can be established using a shared memory
outside of the memory range protected by SICE. Communi-
cation channels are not controlled or managed by the SMI
handler. Thus, the code responsible for managing the com-
munication is not a part of the TCB.
In the time-sharing mode, time multiplexing is used to
share the hardware platform between the isolated environ-
ments and the legacy host. During the environment switch-
ing, SICE guarantees a fresh start of the processor, complete
memory isolation between these environments, and a timely
switching that does not largely impact the performance.
In the multi-core mode, SICE assigns one or more proces-
sor cores to each isolated environment, while the other cores
are used to run the legacy host. SICE guarantees isolation
of both the processor cores and the memory dedicated to
each of the concurrently running environments.
In both modes, SICE attests to the integrity of each iso-
lated environment to remote users, while avoids revealing
sensitive information about the workload to the legacy host.
The current SICE design relies on hardware features pro-
vided by AMD processors. Some of these features are not
currently supported by Intel processors, particularly the abil-
ity to resize the SMRAM at runtime and deﬁning a sepa-
rate SMRAM for each processor core. Proposing alternative
techniques to implement SICE on Intel platforms will require
further research. We discuss these issues in Appendix A.
We implement a prototype of SICE on an IBM LS22 blade
server that uses AMD processors. We use our SICE pro-
totype to run a complete Linux VM in the isolated envi-
ronment. Our experimental evaluation shows that the time
required to enter and exit an isolated environment using
SICE is around 67 µs. We conduct experiments to evalu-
ate the performance of the isolated VM. In the multi-core
mode, SICE incurs a low (under 3%) overhead on VM oper-
ations that do not require frequent communication with the
legacy host. The time-sharing mode shows a higher overhead