the detection with almost equally good accuracy when
dealing with encrypted traﬃc.
In Tables 10 and 11, we can see more details regard-
ing the performance of the evaluation per label. Specif-
ically, although the detection of LD and SD videos is
done with slightly reduced accuracy, we still get satis-
factory performance as we can see from the Precision
and Recall values. If we look at the confusion matrix
below however, we observe that there is an increase in
the LD videos which were misclassiﬁed as SD. This is
attributed to the fact that in the current dataset the
number of 240p videos in the LD category is signiﬁ-
cantly higher than the 144p. This causes a shift in the
distribution of the average quality for this category to-
ward the higher end, which in turn causes the incorrect
classiﬁcation of a percentage of these videos as SD.
Another reason behind the reduction of the accuracy
is the reduced detection capabilities for the HD videos.
In this case, the Precision and Recall for this class have
both reduced signiﬁcantly. At the same time, from the
confusion matrix we see that a signiﬁcant amount of
videos have been incorrectly identiﬁed as SD quality.
This poor performance is a result of the very small num-
ber of videos that are available in the HD class. When
combined with the also relatively small number of HD
videos that were used to train the model, this results
in a class where the training and testing was done with
small number of samples and therefore reduced detec-
tion capabilities for this class.
This problem can be easily alleviated by introducing
a training set that is much richer in HD videos. This
will allow the creation of a predictive model which will
be based on a more diverse dataset that will be capable
of a more accurate detection of the average quality of
HD videos with diﬀerent characteristics.
Class
LD
SD
HD
weighted avg.
TP Rate FP Rate Precision Recall
0.845
0.789
0.513
0.819
0.853
0.775
0.641
0.819
0.203
0.157
0.003
0.183
0.845
0.789
0.513
0.819
Table 10: Accuracies from the evaluation for the
average representation detection
original label
predicted label
LD
SD
HD
LD
SD
84.5% 15.4%
20.4% 78.9%
15%
HD
0.1%
0.7%
33.75% 51.25%
Table 11: The confusion matrix from the average
representation evaluation
5.6 Representation Quality Switch Detec-
tion
The last phase of the evaluation is done for detect-
ing quality switches. In this case, there is no trained
model that can be directly applied to the encrypted
data.
In contrast, the methodology relies on the de-
tection of changes that happen in the time intervals
between segment downloads and the diﬀerence in size
between consecutive segments.
In this evaluation there is no requirement for feature
construction or feature selection. We only need to cal-
culate the time series of the products ∆size × ∆t for
each video in the dataset which is going to be used as
input for the change detection algorithm. Next, we ap-
ply the change detection on each session and from that
we take the standard deviation.
In order to validate the methodology from Section
4.3, we use the same value that was proposed in that
section as a threshold for the standard deviation of the
change detection output.
ST D(CU SU M (∆size × ∆t)) = 500
(3)
According to the proposed methodology, all sessions
below the threshold should represent approximately 78%
of the sessions without quality switches and the sessions
above the threshold should represent 76% of the sessions
with quality switches (Figure 4).
Next, the dataset is split into two parts, i.e. the ses-
sions with score below the threshold and those with a
524score above it. From the ground truth from the en-
crypted data, we are able to evaluate if the predeﬁned
threshold allows the detection of variance with accuracy
equal to the one demonstrated in Section 4.3.
Our analysis reveals that the ﬁrst part of the dataset
consists of 76.9% of videos without any quality change,
while in the second part we ﬁnd 71.7% of the sessions
with quality switches. These accuracies are lower by
1.1% and 4.3% respectively as compared to the results
from the evaluation with unencrypted data.
The decrease in accuracy for detecting videos with
quality switches indicates that the encrypted data con-
sists of videos where the average quality variance is
smaller than the one that was observed in the previ-
ous section. As a result, the distribution of (3) shifted
towards the smaller values and after the threshold was
applied, lower percentage of problematic sessions was
correctly identiﬁed.
6. RELATED WORK
Prometheus [15] uses passive measurements on a mo-
bile network to estimate the QoE of two applications,
Video on Demand and VoIP. For the video QoE only
Buﬀering Ratio is considered as a QoE indicator, while
the system is evaluated only on unencrypted traﬃc us-
ing binary classiﬁcation to detect buﬀering issues with
84% accuracy.
Using similar approaches, OneClick [19] and HostView
[20] develop predictive models to detect the QoE of mul-
tiple applications including video streaming, using net-
work performance metrics. However, both approaches
are limited by the requirement of instrumented devices
to capture the feedback from the users.
Hossfeld et al. [11] study the impact of the amplitude
and frequency of representation switches on the user
experience. The authors re-encoded a video in multiple
qualities and introduced diﬀerent levels and frequencies
of switching and performed crowd-sourced experiments
to detect correlations with the received MOS from the
users. In this work only a single short video was used,
which can be considered a very limited representation
of the diverse content found in popular services.
In [10] the authors perform subjective tests in mobile
networks to assess the impact that the video quality
level and quality switching among other factors has on
the users’ experience. The experiments were conducted
with a very limited sample of very short videos, while
only the direction of quality switching, i.e. resolution
upscaling or downscaling was taken into consideration
but not the eﬀects of the amplitude or the frequency.
Finally, the work of Liu et al. [21] investigates three
factors that inﬂuence the user perceived quality, initial
delay, stalling and quality level variation. The authors
conducted experiments in the lab with diﬀerent network
conditions in order to derive functions for calculating
each of the three impairment factors. The fact that the
tests were performed in the lab however, minimizes the
generalization of the results to real network conditions
and to real streaming services where CDNs and diﬀerent
quality adaptation logics can create diﬀerent eﬀects in
terms of initial delay and quality switches respectively.
Overall, although signiﬁcant work has been done pre-
viously in detecting and quantifying the factors that af-
fect the quality of video streaming, our work is the ﬁrst
that extensively studies these factors in a large scale
network using encrypted traﬃc.
7. LIMITATIONS
The methodology presented in this paper was devel-
oped using information from YouTube video sessions
that were streamed with the service’s current conﬁgu-
ration. However, the predictive power of the models re-
sponsible for detecting QoE impairments can be limited
in the case YouTube changes its video delivery scheme.
In such a scenario, the models that were aﬀected by the
changes need to be trained and evaluated again with an
updated dataset.
Moreover, we do not study the evaluation of the method-
ology with other video streaming services in order to
verify to what extent this approach can be generalized.
However, our analysis of other popular video streaming
services such as Vevo, Vimeo, Dailymotion and so on,
has revealed that they have adopted the same technolo-
gies that YouTube is using for content delivery such as
adaptive streaming, rate limiting, wide range of codecs
and qualities and HTML5-based playback. This com-
mon set of characteristics is a strong indicator that our
methodology can be generalized to a number of other
streaming services and motivates us to include it in the
future steps of this work.
8. CONCLUSIONS
In this work we presented a novel framework for de-
tecting from encrypted traﬃc the 3 key factors that im-
pact both adaptive and classical video streaming QoE,
i.e. stalls, average quality and quality switching.
Next, we demonstrated through evaluations on en-
crypted and unencrypted traﬃc from a large mobile
network, that the proposed models can detect diﬀerent
levels of impairments with accuracies as high as 93.5%.
One of the main ﬁndings of the paper is that the
changes in size and inter-arrival times of video segments
are among the most important indicators of quality
impairments. The incorporation of these features in
our detection framework resulted in signiﬁcant improve-
ments in accuracy.
We showed that the framework can perform very well
on a real production network using a few key perfor-
mance metrics from a single vantage point and without
the requirement of instrumented clients or additional
vantage points, so it can easily be deployed by network
operators. The trained models can be then directly ap-
plied on the passively monitored traﬃc and report issues
in real time.
5259. ACKNOWLEDGMENTS
This work was supported by the Spanish Ministry of
Economy and Competitiveness and EU FEDER under
grant TEC2014-59583-C2-2-R (SUNSET project) and
by the Catalan Government (ref. 2014SGR-1427).
10. REFERENCES
[1] Cisco. “Cisco Visual Networking Index: Global
Mobile Data Traﬃc Forecast Update”. White
Paper, February 2016.
[2] Sandvine. “Global Internet Phenomena Report”.
December 2015.
[3] A. Finamore et al. “Is there a case for mobile
phone content pre-staging?”. In 9th ACM
conference on Emerging networking experiments
and technologies (CoNEXT), pages 321–326.
ACM, 2013.
[4] Vasona. “How encryption threatens mobile
operators, and what they can do about it”.
http://goo.gl/fe3xpB. (Accessed on 05/11/2016).
[5] A. Rao et al. “Network Characteristics of Video
Streaming Traﬃc”. 7th ACM conference on
Emerging networking experiments and
technologies (CoNEXT), 2011.
[6] R. Mok et al. “Inferring the QoE of HTTP video
streaming from user-viewing activities”. 1st ACM
SIGCOMM workshop on Measurements up the
stack (W-MUST), 2011.
[13] G. Dimopoulos et al. “Analysis of YouTube user
experience from passive measurements”. In 9th
International Conference on Network and Service
Management (CNSM), pages 260–267. IEEE,
2013.
[14] S. Krishnan et al. “Video stream quality impacts
viewer behavior: inferring causality using
quasi-experimental designs”. Networking,
IEEE/ACM Transactions on, 21(6):2001–2014,
2013.
[15] V. Aggarwal et al. “Prometheus: toward
quality-of-experience estimation for mobile apps
from passive network measurements”. In
Proceedings of the 15th Workshop on Mobile
Computing Systems and Applications, page 18.
ACM, 2014.
[16] X. Yin et al. “A Control-Theoretic Approach for
Dynamic Adaptive Video Streaming over HTTP”.
In Proceedings of the 2015 ACM Conference on
Special Interest Group on Data Communication,
pages 325–338. ACM, 2015.
[17] ES Page. “Continuous inspection schemes”.
Biometrika, 41(1/2):100–115, 1954.
[18] “YouTube: Most Viewed Videos of All Time”.
https://www.youtube.com/playlist?list=
PLirAqAtl h2r5g8xGajEwdXd3x1sZh8hC.
[19] K. Chen et al. “OneClick: A framework for
measuring network quality of experience”. In
INFOCOM 2009, IEEE, pages 702–710. IEEE,
2009.
[7] Z. Guangtao et al. “Cross-Dimensional Perceptual
[20] D. Joumblatt et al. “Predicting user
Quality Assessment for Low Bit-Rate Videos”.
IEEE Transactions on Multimedia,
10(7):1316–1324, 2008.
dissatisfaction with internet application
performance at end-hosts”. In INFOCOM, pages
235–239. IEEE, 2013.
[8] T. Hoßfeld et al. “Quantiﬁcation of YouTube QoE
[21] Y. Liu et al. User experience modeling for dash
video. In 20th International Packet Video
Workshop (PV), pages 1–8. IEEE, 2013.
[22] A. Balachandran et al. “Developing a predictive
model of quality of experience for internet video”.
In ACM SIGCOMM Computer Communication
Review, volume 43, pages 339–350. ACM, 2013.
[23] Z. M. Shaﬁq et al. “Understanding the impact of
network dynamics on mobile video user
engagement”. In ACM SIGMETRICS
Performance Evaluation Review, volume 42, pages
367–379. ACM, 2014.
via crowdsourcing”. In IEEE International
Symposium on Multimedia (ISM), pages 494–499.
IEEE, 2011.
[9] R. Mok et al. “Measuring the quality of experience
of HTTP video streaming”. In IFIP/IEEE
International Symposium on Integrated Network
Management (IM), pages 485–492. IEEE, 2011.
[10] B. Lewcio et al. “Video quality in next generation
mobile networks – perception of time-varying
transmission”. IEEE International Workshop
Technical Committee on Communications Quality
and Reliability (CQR), pages 1–6, 2011.
[11] T. Hoßfeld et al. “Assessing eﬀect sizes of
inﬂuence factors towards a QoE model for HTTP
adaptive streaming”. In 6th International
Workshop on Quality of Multimedia Experience
(QoMEX), pages 111–116. IEEE, 2014.
[12] R. Schatz et al. “Passive youtube QoE monitoring
for ISPs”. In Innovative Mobile and Internet
Services in Ubiquitous Computing (IMIS), 2012
Sixth International Conference on, pages 358–364.
IEEE, 2012.
526