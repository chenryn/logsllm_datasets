ip-proto == tcp
dst-ip == x.y.0.0/16
dst-port == 443 # 443/tcp = SSL/TLS
eval has_slapper_probed # test: already probed?
event "Slapper tried to exploit vulnerable host"
}
3.5.5 Exploit Scanning
Often attackers do not target a particular system on the Internet,
but probe a large number of hosts for vulnerabilities (exploit scan-
ning). Such a scan can be executed either horizontally (several hosts
are probed for a particular exploit), vertically (one host is probed
for several exploits), or both. While, by their own, most of these
probes are usually low-priority failed attempts, the scan itself is an
important event. By simply counting the number signature alerts
per source address (horizontal) or per source/destination pair (ver-
tical), Bro can readily identify such scans. We have implemented
this with a policy script which generates alerts like:
a.b.c.d triggered 10 signatures on host e.f.g.h
i.j.k.l triggered signature sid-1287 on 100 hosts
m.n.o.p triggered signature worm-probe on 500 hosts
q.r.s.t triggered 5 signatures on host u.v.x.y
3Note that it could instead implement a more conservative policy,
and return true unless the destination is known to not run a vulner-
able version of OpenSSL/Apache.
4. EVALUATION
Our approach for evaluating the effectiveness of the signature en-
gine is to compare it to Snort in terms of run-time performance and
generated alerts, using semantically equivalent signature sets. We
note that we do not evaluate the concept of conceptual signatures by
itself. Instead, as a ﬁrst step, we validate that our implementation
is capable of acting as an effective substitute for the most-widely
deployed NIDS even when we do not use any of the advanced fea-
tures it provides. Building further on this base by thoroughly evalu-
ating the actual power of contextual signatures when deployed op-
erationally is part of our ongoing work.
During our comparision of Bro and Snort, we found several pe-
culiarities that we believe are of more general interest. Our re-
sults stress that the performance of a NIDS can be very sensitive
to semantics, conﬁguration, input, and even underlying hardware.
Therefore, after discussing our test data, we delve into these in some
detail. Keeping these limitations in mind, we then assess the overall
performance of the Bro signature engine.
4.1 Test Data
For our testing, we use two traces:
USB-Full A 30-minute trace collected at Saarland University,
Germany (USB-Full), consisting of all trafﬁc (including
packet contents) except for three high-volume peer-to-peer
applications (to reduce the volume). The university has 5,500
internal hosts, and the trace was gathered on its 155 Mbps
access link to the Internet. The trace totals 9.8 GB, 15.3M
packets, and 220K connections. 35% of the trace packets be-
long to HTTP on port 80, 19% to eDonkey on port 4662, and
4% to ssh on port 22, with other individual ports being less
common than these three (and the high-volume peer-to-peer
that was removed).
LBL-Web A two-hour trace of HTTP client-side trafﬁc, including
packet contents, gathered at the Lawrence Berkeley National
Laboratory (LBL), Berkeley, USA (LBL-Web). The labora-
tory has 13,000 internal hosts, and the trace was gathered on
its Gbps access link to the Internet. The trace totals 667MB,
5.5M packets, and 596K connections.
Unless stated otherwise, we performed all measurements on
550MHz Pentium-3 systems containing ample memory (512MB or
more). For both Snort and Bro’s signature engine, we used Snort’s
default signature set. We disabled Snort’s “experimental” set of sig-
natures as some of the latest signatures use new options which are
not yet implemented in our conversion program. In addition, we
disabled Snort signature #526, BAD TRAFFIC data in TCP
SYN packet. Due to Bro matching stream-wise instead of packet-
wise, it generates thousands of false positives. We discuss this in
§4.2.
In total, 1,118 signatures are enabled. They contain 1,107
distinct patterns and cover 89 different service ports. 60% of the
signatures cover HTTP trafﬁc. For LBL-Web, only these were acti-
vated.
For Snort, we enabled the preprocessors for IP defragmentation,
TCP stream reassembling on its default ports, and HTTP decoding.
For Bro, we have turned on TCP reassembling for the same ports
(even if otherwise Bro would not reassemble them because none of
the usual event handlers indicated interest in trafﬁc for those ports),
enabled its memory-saving conﬁguration (“@load reduce-
memory”), and used an inactivity timeout of 30 seconds
(in correspondence with Snort’s default session timeout). We con-
ﬁgured both systems to consider all packets contained in the traces.
We used the version 1.9 branch of Snort, and version 0.8a1 of Bro.
4.2 Difﬁculties of Evaluating NIDSs
The evaluation of a NIDS is a challenging undertaking, both in
terms of assessing attack recognition and in terms of assessing per-
formance. Several efforts to develop objective measures have been
made in the past (e.g., [21, 22, 15]), while others stress the difﬁ-
culties with such approaches [24]. During our evaluation, we en-
countered several additional problems that we discuss here. While
these arose in the speciﬁc context of comparing Snort and Bro, their
applicability is more general.
When comparing two NIDSs, differing internal semantics can
present a major problem. Even if both systems basically perform
the same task—capturing network packets, rebuilding payload, de-
coding protocols—that task is sufﬁciently complex that it is almost
inevitable that the systems will do it somewhat differently. When
coupled with the need to evaluate a NIDS over a large trafﬁc trace
(millions of packets), which presents ample opportunity for the dif-
fering semantics to manifest, the result is that understanding the
signiﬁcance of the disagreement between the two systems can en-
tail signiﬁcant manual effort.
One example is the particular way in which TCP streams are re-
assembled. Due to state-holding time-outs, ambiguities (see [27,
16] and [25] for discussion of how these occur for benign reasons in
practice) and non-analyzed packets (which can be caused by packet
ﬁlter drops, or by internal sanity checks), TCP stream analyzers will
generally wind up with slightly differing answers for corner cases.
Snort, for example, uses a preprocessor that collects a number of
packets belonging to the same session until certain thresholds are
reached and then combines them into “virtual” packets. The rest of
Snort is not aware of the reassembling and still only sees packets.
Bro, on the other hand, has an intrinsic notion of a data stream.
It collects as much payload as needed to correctly reconstruct the
next in-sequence chunk of a stream and passes these data chunks
on as soon as it is able to. The analyzers are aware of the fact that
they get their data chunk-wise, and track their state across chunks.
They are not aware of the underlying packetization that lead to those
chunks. While Bro’s approach allows true stream-wise signatures,
it also means that the signature engine loses the notion of “packet
size”: packets and session payload are decoupled for most of Bro’s
analyzers. However, Snort’s signature format includes a way of
specifying the packet size. Our signature engine must fake up an
equivalent by using the size of the ﬁrst matched payload chunk for
each connection, which can lead to differing results.
Another example of differing semantics comes from the behavior
of protocol analyzers. Even when two NIDS both decode the same
protocol, they will differ in the level-of-detail and their interpreta-
tion of protocol corner cases and violations (which, as mentioned
above, are in fact seen in non-attack trafﬁc [25]). For example, both
Bro and Snort extract URIs from HTTP sessions, but they do not
interpret them equally in all situations. Character encodings within
URIs are sometimes decoded differently, and neither contains a full
Unicode decoder. The anti-IDS tool Whisker [37] can actively ex-
ploit these kinds of deﬁciencies. Similarly, Bro decodes pipelined
HTTP sessions; Snort does not (it only processes the ﬁrst URI in a
series of pipelined HTTP requests).
Usually, the details of a NIDS can be controlled by a number of
options. But frequently for a Bro option there is no equivalent Snort
option, and vice versa. For example, the amount of memory used
by Snort’s TCP reassembler can be bounded to a ﬁxed value. If this
limit is reached, old data is expired aggressively. Bro relies solely
on time-outs. Options like these often involve time-memory trade-
offs. The more memory we have, the more we can spend for Snort’s
reassembler, and the larger we can make Bro’s time-outs. But how
to choose the values, so that both will utilize the same amount of
memory? And even if we do, how to arrange that both expire the
same old data? The hooks to do so simply aren’t there.
The result of these differences is differing views of the same net-
work data. If one NIDS reports an alert while the other does not,
it may take a surprisingly large amount of effort to tell which one
of them is indeed correct. More fundamentally, this depends on
the deﬁnition of “correct,” as generally both are correct within their
own semantics. From a user’s point of the view, this leads to differ-
ent alerts even when both systems seem to use the same signatures.
From an evaluator’s point of view, we have to (i) grit our teeth and
be ready to spend substantial effort in tracking down the root cause
when validating the output of one tool versus another, and (ii) be
very careful in how we frame our assessment of the differences, be-
cause there is to some degree a fundamental problem of “comparing
apples and oranges”.
The same applies for measuring performance in terms of efﬁ-
If two systems do different things, it is hard to compare
ciency.
them fairly. Again, the HTTP analyzers of Snort and Bro illustrate
this well. While Snort only extracts the ﬁrst URI from each packet,
Bro decodes the full HTTP session, including tracking multiple re-
quests and replies (which entails processing the numerous ways in
which HTTP delimits data entities, including “multipart MIME”
and “chunking”). Similarly, Bro provides much more information
at various other points than the corresponding parts of Snort.
But there are still more factors that inﬂuence performance. Even
if one system seems to be signiﬁcantly faster than another, this can
change by modifying the input or even the underlying hardware.
One of our main observations along these lines is that the perfor-
mance of NIDSs can depend heavily on the particular input trace.
On a Pentium-3 system, Snort needs 440 CPU seconds for the trace
LBL-Web (see Figure 6). This only decreases by 6% when us-
ing the set-wise pattern matcher of [12]. In addition, we devised
a small modiﬁcation to Snort that, compared to the original ver-
sion, speeds it up by factor of 2.6 for this particular trace.
(The
modiﬁcation is an enhancement to the set-wise matcher: the orig-
inal implementation ﬁrst performs a set-wise search for all of the
possible strings, caching the results, and then iterates through the
lists of signatures, looking up for each in turn whether its particular
strings were matched. Our modiﬁcation uses the result of the set-
wise match to identify potential matching signatures directly if the
corresponding list is large, avoiding the iteration.)
Figure 6: Run-times on different hardware
Run−times on Web trace
s
d
n
o
c
e
S
0
0
5
0
0
4
0
0
3
0
0
2
0
0
1
0
Snort
Snort−[FV01]
Snort−Modified
Bro w/o DFA cache
Bro w/ DFA cache
Pentium−3, 512Mhz
Pentium−4, 1.5Ghz
Using the trace USB-Full, however, the improvement realized
by our modiﬁed set-wise matcher for Snort is only a factor of 1.2.
Even more surprisingly, on a trace from another environment (a re-
search laboratory with 1,500 workstations and supercomputers), the
original version of Snort is twice as fast as the set-wise implemen-
tation of [12] (148 CPU secs vs. 311 CPU secs), while our patched
version lies in between (291 CPU secs). While the reasons remain
to be discovered in Snort’s internals, this demonstrates the difﬁculty
of ﬁnding representative trafﬁc as proposed, for example, in [15].
Furthermore, relative performance does not only depend on the
input but even on the underlying hardware. As described above, the
original Snort needs 440 CPU seconds for LBL-Web on a Pentium-
3 based system. Using exactly the same conﬁguration and input
on a Pentium-4 based system (1.5GHz), it actually takes 29 CPU
seconds more. But now the difference between stock Snort and our
modiﬁed version is a factor of 5.8! On the same system, Bro’s run-
time decreases from 280 to 156 CPU seconds.4
Without detailed hardware-level analysis, we can only guess why
Snort suffers from the upgrade. To do so, we ran valgrind’s [34]
cache simulation on Snort. For the second-level data cache, it shows
a miss-rate of roughly 10%. The corresponding value for Bro is be-
low 1%. While we do not know if valgrind’s values are airtight,
they could at least be the start of an explanation. We have heard
other anecdotal comments that the Pentium-4 performs quite poorly
for applications with lots of cache-misses. On the other hand, by
building Bro’s regular expression matcher incrementally, as a side
effect the DFA tables will wind up having memory locality that
somewhat reﬂects the dynamic patterns of the state accesses, which
will tend to decrease cache misses.
4.3 Performance Evaluation
We now present measurements of the performance of the Bro sig-
nature engine compared with Snort, keeping in mind the difﬁculties
described above. Figure 7 shows run-times on trace subsets of dif-
ferent length for the USB-Full trace. We show CPU times for the
original implementation of Snort, for Snort using [12] (virtually no
difference in performance), for Snort modiﬁed by us as described in
the previous section, for Bro with a limited DFA state cache, and for
Bro without a limited DFA state cache. We see that our modiﬁed
Snort runs 18% faster than the original one, while the cache-less