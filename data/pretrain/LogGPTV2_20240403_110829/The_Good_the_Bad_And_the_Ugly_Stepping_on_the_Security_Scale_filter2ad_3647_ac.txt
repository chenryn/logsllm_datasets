disclose  all  of  these?  Likely  not.  Oracle’s  own  metrics 
substantiate that (taking all product families together) more 
than  90%  of  security  vulnerabilities  are  found  internally  (a 
percentage  that  has  increased  since  we  began  keeping 
metrics). 
example, 
Furthermore,  as  more  vendors  move  to  a  bundling 
strategy as regards security bugs (that is, security patches are 
released in regular patch bundles either monthly or quarterly 
or on another published schedule), does it really matter how 
many  vulnerabilities  are  contained  in  the  bundle  (and  isn’t 
that the point, really, to address a number of critical security 
issues in one patch rather than piecemeal?)  
Granted, knowing how many security issues are fixed in 
a  patch  may  be  cause  for  customer  concern.  Of  course, 
absent knowing how many issues are fixed-but-not-public in 
a  bundle  or  whether  the  trend  of  “new  vulnerabilities 
introduced in code” vs. “old vulnerabilities found and fixed” 
is positive (that is, that newer versions are more secure), it’s 
still  not  necessarily  possible  to  draw  security-worthiness 
inferences from published security vulnerability information. 
It’s like drawing inferences about the size of an iceberg from 
looking at the amount above water (except that in the case of 
icebergs,  we  at  least  have  specific  gravity  information  to 
work  with  whereas  with  vulnerabilities,  there  is  no  way  to 
estimate what’s below the water line). 
The  nail  in  the  coffin  in  using  “number  of  published 
security  vulnerabilities”  as  a  quality  metric  or  for  security 
comparison purposes is that it is not only gameable, but the 
metric  itself  invites  gaming.  In  other  words,  if  a  vendor 
thinks that numbers of published vulnerabilities will be used 
against  it  competitively,  the  incentive  is  to  not  self-report 
significant  vulnerabilities  so  as  to  make  the  numbers  look 
better.  Note:  while  no  vendor  publicly  reports  every  issue 
they fix for a multiplicity of reasons (e.g., inability to fix all 
issue  on  all  old  versions  argues  for  some  discretion  in 
reporting to the extent those versions are at a risk that can’t 
be  mitigated);  on  balance,  creating  positive  incentives  to 
protect  customers  (instead  of  reasons  to  “game”)  is  in 
industry’s interests.  
In  a  competitive  market,  businesses  will  use  what 
competitive  advantage  they  can  to  gain  leverage  over  a 
competitor. Security-worthiness, to the extent that it can be 
verifiably  a  differentiator,  can  be  a  competitive  advantage. 
However, 
security 
vulnerabilities is not objectively a fairly competitive metric. 
disclosed 
publicly 
number 
of 
V.  MINE’S BIGGER THAN YOURS 
The corollary to the – potentially destructive – aspects of 
using externalized (public) security metrics competitively is 
the benefit of  harnessing  competitive  metrics internal  to an 
organization.  Specifically,  nobody  likes  to  be  seen  as  the 
person or group doing the worst at something.  Peer pressure 
can be a force for good in a security program. 
For  example,  Oracle  has  made  a  number  of  significant 
acquisitions  in  the  past  several  years.  As  part  of  post-
acquisition  integration,  we  align  development organizations 
from  acquired  companies  with  Oracle  secure  development 
practices.  One  of  the  difficulties  of  this  process  is  that 
different  acquired  entities  are  at  different  points  in  the 
continuum of secure development practice, in regards to both 
“where  they  started”  and  “how  they  are  aligning  with  the 
corporate secure development practices” (the umbrella term 
for  which  is  Oracle  Software  Security  Assurance  (OSSA)). 
Organizations  that,  post-acquisition,  find  themselves  under 
191
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:06:29 UTC from IEEE Xplore.  Restrictions apply. 
including 
the  umbrella of  a  mature development  organization  tend to 
“inherit”  a  number  of  development  practices  as  they  are 
integrated  into  the  development  group.  On  the  other  hand, 
some  acquired  entities  –  mostly  in  vertical  or  industry-
specific  segments  –  are  incorporated  as  largely  standalone 
entities into organizations that are not primarily development 
organizations. There is thus, in these organizations, no long-
established development practice to inherit.  
For  reasons 
the  disparity  of  “inherited 
development  practice”  as  well  as  the  sheer  number  of 
acquisitions,  Oracle  has 
instituted  a  more  structured 
governance  program  around  software  assurance  that  is 
geared  to  looking  at  consistency  of  secure  development 
practice across the entire organization. The primary purpose 
of the collected metrics – in this case, measurement of which 
elements of OSSA have been adopted by each organization 
and  to  what  degree  –  is  simply  to  measure  “How  are  we 
doing in the uptake of OSSA software assurance programs?” 
The security compliance scorecard is reported to the Oracle 
Security Oversight Committee (OSOC) as well as the Chief 
Executive Officer (CEO). 
The  secondary  purpose  of  metrics  around  OSSA 
adoption is to be able to track organizations that are slow in 
adoption  or  appeared  to  be  running  into  difficulties  in 
adopting  standard  Oracle  secure  development  practices. 
Groups that are slow, or running into difficulties, are targets 
for  concentrated  assistance.  Specifically,  rather  than  using 
Oracle Global Product Security (GPS) (the group responsible 
for  monitoring  OSSA  compliance)  resources  thinly  across 
the  entire  company,  the  resources  are  applied  in  a  more 
concentrated  form  to  the  teams  that  are  “slow  learners”  or 
“late  adopters.”  This  may  include  a  product  assessment 
conducted by the ethical hacking team, accelerated uptake of 
automated tools (e.g., static analysis) and other fast-tracked 
OSSA requirements. 
SECURITY COMPLIANCE SCORE CARD
Senior Development 
Manager
Security Lead
SPOCs 
Identified
M&A 
Checklist 
Process
Secure 
Coding 
Practices 
Training
Product 
Security 
Checklist 
Process
Third Party 
Security 
Checklist 
Process 
Automated 
Test Tools
Secure 
Configuration 
Install
Security Best 
Practices 
Guides
Critical 
Patch 
Update 
participation
SVP1
SVP2
SVP3
SVP4
SVP5
SVP6
SL1
SL2
SL3
SL4
SL5
SL6
In Compliance
In Compliance In Compliance In Compliance In Compliance In Compliance In Compliance In Compliance
In Progress
In Progress
In Progress
Non Compliant Non CompliantNon CompliantNon Compliant Non Compliant Non Compliant Non Compliant Non Compliant Non Compliant
In Compliance
In Progress
In Progress
In Progress
In Progress
In Progress
In Progress
Organization
Mature Product 1
Mature Prodiuct 2
Mature Produuct 3
Acquired Product 1
Acquired Product 2
Acquired Product 3
more  transparency  and  more  accurate  RFP  responses.  In  at 
least one case, an increased customer concern in the security 
of  a  particular  product  led  to  a  “full  court  press”  in 
increasing  the  uptake  of  OSSA  activities.  This  would  not 
have  been  possible  without  the  baseline  reporting  that  the 
security compliance scorecard (metrics) enables.  
Without making compliance an impossible goal to meet, 
the  intention  of  the  compliance  scorecard  is  that  what 
constitutes “compliance” will change over time. Specifically, 
as the state of the art improves in software assurance, Oracle 
will redefine what “compliance” means for a number of cells 
in the chart. For example, one compliance area is the use of 
automated tools, which is one of the best and cheapest ways 
to get avoidable, preventable defects out of one’s code base. 
“Use  of 
tools”  as  a  compliance  metric  encompasses 
everything  from 
the  appropriate  and 
applicable tools by development group (e.g., static analysis, 
web vulnerability analysis, fuzzers, etc.) to deployment and 
degree  of  code  coverage.  Over  time,  we  may  identify  (or 
develop) additional tools that would also be adopted, which 
means  “compliance”  will  include  use  of  new  tools  in  the 
groups to which they are applicable.   
identification  of 
An expected benefit of the security compliance scorecard 
is the – for lack of a better expression – shaming effect it has 
on groups that have had OSSA adoption issues. Specifically, 
one of the baseline elements of OSSA is security training – 
more  specifically,  completion  of  an  online  class  on  Oracle 
Secure Coding Standards (OSCS), that is mandatory for most 
of  development  (e.g.,  developers,  quality  assurance  (QA), 
release management, product management, and so on).  
Despite the OSCS class being online and modular (which 
facilitates  people  being  able  to  take  the  class  at  their  own 
pace, completing portions of it a bit at a time), adoption had 
been  slow  in  some  groups.  The  single  factor  in  rapidly 
increasing  class  adoption  was  creation  of  a  bar  chart 
showing, for each organization, their percentage completion. 
Since the bar chart showed all organizations’ training status 
as  compared  with  each  other,  it  also  highlighted  who  the 
“worst  offenders”  were.  Reporting  numbers  by  group  as  a 
percentage  had  never  “grabbed”  management  attention:  the 
bar chart showed not only who was compliant, but how far 
out of compliance some groups were relative to others.  
Publishing  the  bar  chart  that  showed  who  the  outliers 
were had a dramatic and immediate effect upon compliance 
rates. Specifically, once the word got out that the charts had 
gone  to  executive  management  and  the  Oracle  Security 
Oversight Committee – and that the report was being rerun 
and  the  results  re-reported  in  a  month’s  time  –  the 
compliance rate went straight up. “Compliance” here meant 
not merely that someone had attempted the class, but more 
specifically, “of the people in a development group required 
to take the class, who has taken it and passed it?” Ninety five 
percent  or  better  is  considered  “compliant”  for  tracking 
purposes.  
Figure 1.   Oracle Security Compliance Scorecard 
The  third  reason  for  a  governance  structure  is  in 
anticipation  of  greater  customer 
secure 
development practices and a requirement for transparency – 
perhaps  driven  by  regulation.  Anecdotally,  more  customer 
requests for proposals (RFPs) are requesting information on 
secure  development  practice,  and  being  able  to  track,  by 
development group, what is being done and not done enables 
interest 
in 
192
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:06:29 UTC from IEEE Xplore.  Restrictions apply. 
d
e
n
i
a
r
T
t
n
e
c
r
e
P
100%
90%
80%
70%
60%
50%
40%
30%
20%