properties of datacenter topologies at larger scales than previously
possible, giving a designer greater confidence in a particular topol-
ogy (§5.2). A concrete example is resilience. Prior work showed that
Jellyfish [44] and Xpander [47] degrade gracefully with random link
failure for up to 1K servers; we show that, for a 131K sized Jellyfish
or Xpander, degradation is less than graceful (the throughput after
failure can be up to 20% lower than what one might expect with
graceful degradation) under random failure.
Ethics. This work does not raise any ethical issues.
2 AN UPPER BOUND ON THROUGHPUT
In this section, we prove an upper bound on throughput that applies
to uni-regular and bi-regular topologies.
2.1 Complexity of Computing Throughput
Bounds
A permutation matrix is one in which each row and each column
has exactly one non-zero entry. A permutation matrix can indicate
traffic either at the server-level (where each entry denotes traffic
between two servers), or switch-level. In server-level permutation
matrices, all non-zero entries are normalized to 1 while for switch-
level matrices, they are the number of servers connected to the
switch (𝐻). In this section, we show that this set of switch-level
permutation traffic matrices, denoted by ˆT , is sufficient to characterize
the throughput of uni-regular and bi-regular topologies.
Notation. Entry 𝑡𝑢𝑣 of the switch-level traffic matrix 𝑇 describes
the traffic demand from switch 𝑢 to switch 𝑣. Let K be the set of all
switches with servers, and 𝐻 be the number of servers connected to
each switch in K. To determine the throughput of the topology, we
Notation Description
𝑁
𝐸
𝑅
𝐻
S
K
𝑡𝑢𝑣
T
ˆT
𝜃 (𝑇)
𝜃∗
𝐿𝑢𝑣
𝑇 = [𝑡𝑢𝑣]
Total number of servers
Total number of switch-to-switch links
Switch radix
Number of servers per switch
Set of switches with and without servers
Set of switches with 𝐻 servers (K ⊆ S)
Traffic demand from 𝑢 to 𝑣 where 𝑢, 𝑣 ∈ K
|K| × |K| traffic matrix with demands 𝑡𝑢𝑣’s
Saturated hose-model set
Permutation traffic set
Throughput under traffic matrix 𝑇
Topology throughput (𝜃∗ = min𝑇 ∈T 𝜃 (𝑇))
Shortest path length from switch 𝑢 to 𝑣
Table 2: Notation
use the hose model [11]3, where every switch sends and receives
traffic at no more than its maximum rate 𝐻 (for simplicity, each
link has unit capacity). The hose-model traffic set is the set of traffic
matrices that conform to the hose model:
𝑢∈K 𝑡𝑢𝑣 ≤ 𝐻 ∀𝑣 ∈ K
𝑣∈K 𝑡𝑢𝑣 ≤ 𝐻 ∀𝑢 ∈ K
(cid:27)
,
𝑇 ∈ R|K|×|K|
+
:
(cid:26)
where R+ is the set of non-negative reals. This traffic set includes
the commonly-used traffic matrices such as all-to-all and random
permutations, and it applies not just to uni-regular topologies, but
to bi-regular topologies as well. A bi-regular topology contains two
types of switches: one without attached servers, and one in which
each switch has 𝐻 servers. Switches without servers can not source
or sink any traffic, and as a result, it suffices to describe the traffic
matrix only by switches with attached servers (K).
Our hose model definition is consistent with [27], which bases
its definition on server-level traffic matrices. Our definition uses
switch-level traffic matrices, leveraging the fact that uni-regular
and bi-regular topologies have 𝐻 servers per switch and each server
connects to exactly one switch.
On computing the throughput of a topology. Since the hose-
model traffic set contains an infinite number of traffic matrices,
computing the throughput of the topology (the minimum through-
put across all traffic matrices) is intractable.
To improve the tractability, consider the following set of traffic
matrices that we call the saturated hose model set, T , where each
switch sends and receives traffic at exactly its maximum rate 𝐻:
𝑢∈K 𝑡𝑢𝑣 = 𝐻 ∀𝑣 ∈ K
𝑣∈K 𝑡𝑢𝑣 = 𝐻 ∀𝑢 ∈ K
(cid:27)
.
T =
𝑇 ∈ R|K|×|K|
+
:
(cid:26)
This set dominates the hose-model traffic set, since we can always
augment any hose-model traffic matrix with a non-negative value
to produce a saturated hose-model traffic matrix. So, the minimum
throughput across all traffic matrices in the hose model set cannot
be smaller than the minimum throughput across all traffic matri-
ces in T . However, there are still infinitely many elements in T .
The following theorem shows that for uni-regular and bi-regular
3In the hose model, the end-host traffic rate is bounded by the port speed, which means
the model only permits admissible traffic patterns for the topology. Our use of the
hose model is consistent with prior work [11, 27].
SIGCOMM ’21, August 23–28, 2021, Virtual Event, USA
Namyar .et al.
topologies, it suffices to consider an even smaller traffic set in order
to compute throughput.
Theorem 2.1. The throughput of a uni-regular or a bi-regular
topology is the minimum throughput across all traffic matrices in the
permutation traffic set ˆT .
Proof Sketch. §A contains the detailed proof, which proceeds
in two steps. First, it shows that ˆT represents the extrema of the
convex polytope formed by the traffic matrices in T . Second, relying
on the convexity of the set T , it shows that the minimum through-
put across all traffic matrices must correspond to a permutation
traffic.
□
Prior work [45] has used a similar convexity argument in a
slightly different context, and [46] proves a similar theorem in a
more limited context (for oblivious routing). Other prior work ([29],
Conjecture 2.4) has stated Theorem 2.1 as a conjecture.
The size of ˆT , while finite, grows combinatorially with the matrix
dimension, so it is still infeasible to iterate over all its elements in
order to compute throughput. However, in any traffic matrix in ˆT ,
each switch 𝑢 sends traffic at full rate to exactly one other switch 𝑣.
We exploit this, together with the structure of uni-regular and bi-
regular topologies to derive an efficiently computable upper bound
on the throughput of these topologies (§2.2).
2.2 Throughput Upper Bound
We now use Theorem 2.1 to derive a closed-form expression for
the upper bound on the throughput of a uni-regular or a bi-regular
topology. Throughput is both a function of the topology and the
routing algorithm used to route traffic demands; the derived upper
bound is independent of the routing algorithm.
Upper bound for uni-regular topology throughput. The fol-
lowing theorem establishes a tractable closed-form expression for
the throughput of a uni-regular topology. It assumes, without loss
of generality, a uni-regular topology with 𝐻 servers per switch, and
unit link capacity.
Theorem 2.2. The maximum achievable throughput for a uni-
regular topology, under any routing, is bounded by:
𝐻(𝑢,𝑣)∈K2 𝐿𝑢𝑣I [𝑡𝑢𝑣 > 0]
2𝐸
𝜃∗ ≤ min
𝑇 ∈ ˆT
(1)
where 𝐸 is the number of switch-to-switch links in the topology, 𝐿𝑢𝑣
is the shortest path length from switch 𝑢 to switch 𝑣 and I [·] is an
indicator function.
Proof Sketch. §B contains the detailed proof, which relies on
the optimal solution of the path-based multi-commodity flow prob-
lem (§H, commonly used in wide-area network traffic engineer-
ing [33]). For a given traffic matrix 𝑇 , path-based multi-commodity
flow maximizes throughput 𝜃(𝑇). Now, consider an arbitrary switch
𝑢. Its total ingress traffic consists of two components: the traffic
destined to its servers, which depends on 𝜃(𝑇), and its transit traffic.
We upper-bound the ingress traffic by the aggregate link capacity
at the switch, and lower-bound it by the total transit traffic de-
rived from the path lengths and the flow split ratios. Solving these
inequalities, and applying Theorem 2.1 gives Equation 1.
□
Efficiently computing the throughput bound. The RHS of
Equation 1 chooses a permutation traffic matrix that maximizes
total path length. Finding this matrix is equivalent to finding near-
worst-case traffic matrix in [27]. In that work, the authors present
an intuitive form of the throughput upper bound and suggest an
intuitive heuristic for constructing a “difficult” server-level traffic
matrix (near-worst-case). In this paper, we formally prove the
throughput upper bound and use a slightly different approach
(discussed below) that constructs a switch-level traffic matrix to
achieve the minimum of the RHS of Equation 1.
To find the minimum throughput, we construct a complete bipar-
tite graph 𝐵 (consisting of two disjoint set of nodes 𝑈 and 𝑉 ) from
the given topology 𝐺. 𝑈 and 𝑉 represent all the possible source and
destination switches with directly connected servers in 𝐺 respec-
tively. The weight of the edge (𝑢, 𝑣) where 𝑢 ∈ 𝑈 and 𝑣 ∈ 𝑉 is the
shortest path length from switch 𝑈 to switch 𝑉 . The permutation
traffic matrix that determines the throughput bound in Equation 1
corresponds to the weighted maximum matching in 𝐵. We call this
the maximal permutation matrix.
Extension to bi-regular topologies. Theorem 2.2 applies to bi-
regular topologies as well. Intuitively, additional switches with no
servers increase capacity for transit traffic which is reflected in
the numerator of Equation 1. We prove this formally in §C. The
theorem also applies to uni-regular and bi-regular topologies in
which each switch 𝑢 has a different radix 𝑅𝑢; we have omitted the
description of this extension for brevity.
Theorem 2.2 implies that throughput of a topology is propor-
tional to total link capacity and inversely proportional to maxi-
mal total path length of the maximal permutation matrix. Prior
work [43] has computed an upper-bound on the average through-
put of uni-regular topologies across all uniform traffic matrices
(the all-to-all and permutation matrices). In contrast, we bound the
worst-case throughput, and our bound is significantly closer (§3.2)
to the worst-case behavior of uni-regular topologies at all scales
than the bound of [43]. Our bound is also more general: it applies
to bi-regular topologies as well, and across all traffic matrices (as a
consequence of Theorem 2.1).
On server-level vs. switch-level traffic matrices. We exploit the
regularity in uni-regular and bi-regular topologies and reason about
switch-level permutation traffic matrices, rather than server-level
ones. This helps us efficiently compute the upper-bound even for
large topologies (§3). This efficiency does not impact the throughput
estimate, relative to using a server-level permutation matrix, as we
now show.
If we had used the server-level TMs, the throughput upper-bound
would have been the same. A switch-level maximal permutation
matrix ˆ𝑇 , when converted to server-level ˆ𝑇𝑛, is a solution to the
corresponding server-level weighted maximum matching problem.
We can prove this by contradiction. Let, for any server 𝑢, 𝑠(𝑢) be
the switch connected to 𝑢 and assume that ˆ𝑇𝑛 can be improved by
(the total path length of the permutation matrix can be increased
by, see denominator of Equation 1) a set of actions on (𝑢, 𝑣) (e.g.,
insertion or deletion of a flow). We can show that ˆ𝑇 can be also im-
proved by the same amount by a similar set of actions on (𝑠(𝑢), 𝑠(𝑣)).
This is because the link from the server to its directly connected
A Throughput-Centric View of the Performance of Datacenter Topologies
SIGCOMM ’21, August 23–28, 2021, Virtual Event, USA
(a) Jellyfish (K=100)
(b) Xpander (K=100)
(c) FatClique (K=200)
Figure 3: Throughput bound vs K-shortest paths Multi-commodity flow.
Gap approaches zero as the number of servers (𝑁 ) increases for all choices
of uni-regular topologies and servers per switch (𝐻).
switch does not constrain throughput, so all 𝐿𝑢𝑣s do not include
it. Thus, adding/removing (𝑠(𝑢), 𝑠(𝑣)) increases/decreases the total
path length by the same amount as adding/removing (𝑢, 𝑣) does.
This is a contradiction since we assumed ˆ𝑇 is the maximal permu-
tation matrix.
However, the actual throughput of the topology under switch-
level maximal permutation matrix is always less than or equal to
the server-level one. If the server-level maximal permutation ma-
trix, when converted to switch-level, is not a permutation matrix, a
similar line of proof as Theorem 2.1 can show that the correspond-
ing switch-level traffic matrix is a convex combination of some