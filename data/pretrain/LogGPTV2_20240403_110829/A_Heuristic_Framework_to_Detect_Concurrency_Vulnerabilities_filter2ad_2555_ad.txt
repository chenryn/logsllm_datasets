plore thread interleavings in fuzz testing and to detect concurrency
vulnerabilities for concurrent programs written in C with POSIX
multi-thread functions. The implementation details are described
in this section.
6.1 Implementation of Static Analysis
To implement the static analysis described in Section 4, we lever-
aged an existing concurrent static analysis tool in order to reduce
our implementation workload. Such a tool should be open source
so that we could modify its code to implement the desired function-
alities. It should also be able to work on concurrent C programs so
that we could apply it in our evaluation (see Section 7.1 for details).
Among available concurrent static analysis tools meeting our re-
quirements, LOCKSMITH [23] was selected since it was easy to use
and modify. It is a static analysis tool that uses a constraint-based
technique to automatically detect data races in concurrent C pro-
grams. We used it to discover shared variables for the functionality
described in Section 4.1, construct data-ow graphs and control-
ow graphs, and obtain information of locked areas. We modied
LOCKSMITH’s code to mark sensitive concurrent operations on the
data-ow graph to fulll the functionality described in Section 4.2,
and mark preceding operations on the data-ow graph for each
one in a pair of sensitive operations and examine these operations
on both the data-ow graph and the control-ow graph to fulll
the functionality described in Section 4.5.
To implement the functionaries described in Sections 4.3 and 4.4,
we wrote a program in Python using NetworkX module to process
results from LOCKSMITH for merging data-ows and categorizing
each pair of concurrent sensitive operations into a specic type of
vulnerability.
i n t h r e a d 1
s t r = NULL ;
Listing 2: Instrumentation assembly ags in source code
/ /
T1 : 1 : asm ( " # c o n_ a f l _4 8 \ n \ t " ) ;
T1 : 2 :
T1 : 3 : asm ( " # c o n _ p r i o r i t y _ a f l _ 4 8 \ n \ t " ) ;
. . .
/ /
T2 : 1 : asm ( " # c o n_ a f l _4 9 \ n \ t " ) ;
T2 : 2 : p r i n t f ( "%s " , s t r ) ;
i n t h r e a d 2
6.2 Implementation of Thread-Aware Fuzzing
Our two thread fuzzing priorities were implemented based on AFL
[13]. We inserted instrumentation code to adjust thread priorities
to designated interleavings. This was done before and during AFL-
compiling the source code of a program, as described in detail next.
As a result, the source code is needed for our heuristic framework
to detect concurrency errors and vulnerabilities in a concurrent
program.
The instrumentation code was inserted in two steps: the rst step
inserted assembly ags in the source code before AFL-compiling to
mark locations where our instrumentation code should be inserted,
while in the second step each assembly ag was replaced with
537
scheduling assembly code at AFL-compiling time. Same as the
original instrumentation of AFL, replacing assembly ags with
scheduling assembly code was done on assembly code les (i.e.,
.s les) generated during AFL compiling. Since we had access to
source code, for simplicity, we used -o0 optimization level to AFL-
compile all the tested programs.
For the interleaving exploring priority, the thread-priority adjust-
ing code was inserted right after a call of pthread_create function.
For the targeted priority, the thread-priority adjusting code was
inserted around each sensitive operation. Inserting thread-priority
adjusting code for the interleaving exploring priority is straightfor-
ward as compared with inserting thread-priority adjusting code for
the targeted priority. We shall focus on describing the latter in the
remaining part of this subsection.
Listing 2 shows an example of inserted assembly ags in a sched-
uling unit for the pair shown in Listing 1. In this listing, each
assembly ag is associated with a number, such as 48 and 49 in
Listing 2. These numbers indicate the execution order of the two
sensitive operations in a pair to trigger the suspected concurrency
vulnerability: the sensitive operation associated with a ag of a
smaller number in a pair should be executed before the sensitive
operation associated with a ag of a larger number in the same pair.
For example, in Listing 2, sensitive operation str = NULL should be
executed before sensitive operation printf("%s", str) to trigger the
suspected concurrency use-after-free since the former is associated
with 48 while the latter is associated with 49.
When the program was compiled by AFL, the assembly ags
in a pair were recognized and replaced with a scheduling unit of
scheduling assembly code. More specically, assembly ags in the
generated .s les during AFL compiling were rst located, and each
assembly ag right before a sensitive operation, e.g. the assembly
ag at line T1:1 and that at line T2:1 in Listing 2, was replaced
with scheduling assembly code to adjust the two threads’ priorities
according to Algorithm 1, with the sensitive operation associated
with a smaller number being executed rst as we mentioned above.
Each sensitive operation that should be executed rst in a pair
is followed with an assembly ag, such as line T1:3 in Listing 2.
This assembly ag was replaced with assembly code to restore the
original priority of the other thread in the pair if the thread was
adjusted to the lowest priority level, as described in Algorithm 1.
During fuzz testing, we allocated a scheduling trace table to
record the execution information of instrumentation code, which
tells what interleaving was actually executed in a test run, and how
many times an interleaving was executed. We also recorded some
global information such as a global threshold. If any crash was
triggered in fuzz testing, the recorded information could identify
the input and the interleaving associated with the crash, which
would help us validate detected concurrent vulnerabilities.
7 EVALUATION
We have applied the implemented heuristic framework to a bench-
mark suite of six real-world C programs. Experiments were per-
formed on Intel Xeon CPU E5-2630 v3 @ 2.40GHz with 32 logic
cores and 64 GB of memory, running on Red Hat 4.4.7-17. The
version of AFL [13] we used was AFL v2.51b.
ACSAC ’18, December 3–7, 2018, San Juan, PR, USA
Changming Liu, Deqing Zou, Peng Luo, Bin B. Zhu, and Hai Jin
Table 3: Experimental results (LOC = lines of code)
Application
boundedbuf
swarm
bzip2smp
pfscan
ctrace
qsort
LOC Exploring Priority
# of new crashes
0.4k
2.2k
6.3k
1.1k
1.5k
0.7k
0
0
2
1
0
0
Time
2.3s
3.5s
1500s
1.2s
2.9s
0.5s
Vuln. type
Buer Overow
Double-Free
Double-Free
None
Double-Free
Buer Overow
Vulnerability Detected
# found by
static analysis
# detected by
targeted priority
0
0
1
0
1
0
Performance Overhead
272%
109%
51%
98%
59%
104%
1
1
3
0
3
2
7.1 Benchmark Suite
Since there is no available benchmark suite for detecting concur-
rency vulnerabilities, to the best of our knowledge, we selected sev-
eral typical multi-thread C programs from previous works [8, 23, 33]
using the following selection criteria:
• Lines of code could not exceed tens of thousands. This is
because the static analysis tool we based on to implement
our own static analysis has adopted a very precise thus costly
method, which limits the tool to detect no more than tens
of thousands of lines of code [28]. This limitation excludes
many sophisticated but interesting software.
• Multi-thread programs written in C using POSIX multi-
thread functions.
• Do not interface with the network since AFL mutated a local
le that was fed into the program.
• Do not fork any new thread via a thread pool since a thread
pool would aect the accuracy of data-ow in the static
analysis, which would lead to too many false positives.
We collected six programs in the benchmark suite to evaluate our
heuristic framework. They were boundedbu, a program that imple-
ments a multi-thread producer-consumer module; swarm, a parallel
programming framework for multi-core processors [1]; bzip2smp, a
parallel version of bzip2 compressing tool; pfscan, a multi-thread le
scanner; ctrace, a library for tracing the execution of multi-threaded
programs; and qsort, a multi-thread implementation of quick sort.
7.2 Experimental Results
Table 3 shows the detection results of our heuristic framework in
testing the benchmark suite described in Section 7.1. Our heuristic
framework contains actually two separated parts to perform two
dierent detection tasks. One is a modied AFL with the interleav-
ing exploring priority to enable AFL to explore thread interleavings
as eectively as possible to detect concurrency errors, while the
other consists of our static analysis and a modied AFL with the
targeted priority to detect targeted concurrency vulnerabilities such
as the three types of concurrency vulnerabilities studied in this
paper. The former will be referred to as the interleaving exploring
fuzzer while the latter as the vulnerability detection fuzzer. The
detection results for both fuzzers are included in Table 3. The detail
is described next.
In Table 3, the third column shows the number of new crashes
found with the interleaving exploring fuzzer, i.e., crashes found
with our modied AFL with the interleaving exploring priority
538
but not found by running the original AFL suciently long. The
remaining columns in the table except the last one show the de-
tection results of the vulnerability detection fuzzer: the execution
time in seconds of the static analysis in the fourth column; the type
and the number of suspected concurrency vulnerabilities reported
by the static analysis in the fth and sixth columns, respectively;
and eventually in the seventh column the number of concurrency
vulnerabilities detected by the modied AFL with the targeted pri-
ority after sending each case reported by the static analysis to the
modied AFL for further testing. Thus the seventh column shows
the detection results of the vulnerability detection fuzzer. The last
column of able 3 shows the performance overhead of our modied
AFL against the original AFL for each tested program, which will
be described in detail later in this subsection.
Table 3 does not show any execution time taken by AFL fuzz test-
ing since the time spent in AFL fuzz testing was non-deterministic.
In most cases, it took about ten minutes or less for the interleaving
exploring fuzzer to produce the rst crash. As a comparison, the
original AFL might not report any crash after running for several
days. For example, in testing bzip2smp, our interleaving exploring
fuzzer produced a crash after running in less than 10 minutes, while
the original AFL did not report any crash after running for 2 days.
As we described above, the crashes reported in the third column
of Table 3 were all new crashes found by the interleaving exploring
fuzzer. Since there was no report on crashes of the programs in the
benchmark suite by any existing fuzzer, we compared the detection
results of our interleaving exploring fuzzer with the results of the
original AFL. If a crash was reported by the interleaving exploring
fuzzer but not reported by the original AFL after running it suf-
ciently long, the crash was considered new and reported in the
third column in Table 3.
We have also studied the impact of our thread scheduling on
the performance of AFL by comparing the total number of execu-
tions of a program to be tested in a xed duration of time with our
modied AFL against that with the original AFL. The last column
in Table 3 shows the performance overhead of our modied AFL
against the original AFL for each tested program, which is dened
as the dierence of the average execution time in running a tested
program with our modied AFL, including both using the interleav-
ing exploring priority and using the targeted priority, and with the
original AFL, normalized by the original AFL’s average execution
time. The performance overhead ranged from 51% to 272% for the
benchmark suite.
A Heuristic Framework to Detect Concurrency Vulnerabilities
ACSAC ’18, December 3–7, 2018, San Juan, PR, USA
7.3 Validation of Detected Concurrency
Vulnerabilities
For each concurrency vulnerability detected by the vulnerability
detection fuzzer, we need to verify if it is a true positive or a false
positive. We used the following manual validation process to verify
the two concurrency vulnerabilities reported in Table 3: a concur-
rency double-free for each of bzip2smp and ctrace.
When a crash was reported, our modied AFL recorded its input
and the thread interleaving setting in the crash le. With the crash
report, we manually inserted the scheduling code in the source to
set the thread interleaving the same as the crash interleaving and
inserted assertive code before the sensitive operations to assert the
condition that would trigger the detected concurrency vulnerability.
Then we repeatedly ran the tested program fed with the crash input
in order to hit the assertive code. If the assertive code was hit, we
concluded that the detected concurrency vulnerability was a true
positive. If the assertive code was not hit after running the tested
program many times, we concluded that the detected concurrency
vulnerability was highly likely a false positive.
Using this manual validation process, the two concurrency double-
free vulnerabilities detected by the vulnerability detection fuzzer
and reported in Table 3 were conrmed to be true positives.
7.4 Analysis of Static Analysis Results
The goal of the static analysis is to locate potential concurrency
vulnerabilities and obtain their information to provide the modied
AFL with the targeted priority to test. False positives in the static
analysis would increase the workload of fuzz testing. The semantic
checking in the static analysis aims at avoiding wasting time on
testing obvious false positives in fuzz testing instead of at accurately
detecting concurrency vulnerabilities. As a result, we had used a
simple method in the semantic checking to eliminate cases that
could be easily determined to be false positives, i.e., the condition
that would trigger a suspected concurrency vulnerability could be
easily determined to never be met.
Nevertheless, a more accurate semantic checking would help
reduce the workload of fuzz testing. To analyze the performance of
the static analysis, we investigated the cases reported by the static
analysis but not detected by the modied AFL with the targeted
priority. There were 8 such cases in total, as we can see from Table 3.
By examining and debug-testing the code, we could determine
that 4 cases out of the total 8, the one in boundedbuf, the two in
qsort, and one in ctrace, were false positives, thanks partially to
the small code base of these programs. The other 4 cases in larger
programs could not be determined in our investigation: they looked
like true positives as reported by the static analysis in our manual
examination but we could not trigger them in our fuzz testing. As
a result, we were unable to determine if they were true positives
or not. As we shall describe in Section 8, AFL might have failed to
execute the sensitive operations in a pair or insuciently tested