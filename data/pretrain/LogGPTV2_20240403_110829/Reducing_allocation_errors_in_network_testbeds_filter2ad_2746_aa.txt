title:Reducing allocation errors in network testbeds
author:Jelena Mirkovic and
Hao Shi and
Alefiya Hussain
Reducing Allocation Errors in Network Testbeds
4676 Admiralty Way, Ste 1001
4676 Admiralty Way, Ste 1001
4676 Admiralty Way, Ste 1001
Hao Shi
USC/ISI
Marina Del Rey, USA
PI:EMAIL
Jelena Mirkovic
USC/ISI
Marina Del Rey, USA
PI:EMAIL
Aleﬁya Hussain
USC/ISI
Marina Del Rey, USA
aleﬁPI:EMAIL
ABSTRACT
Network testbeds have become widely used in computer science,
both for evaluation of research technologies and for hands-on teach-
ing. This can naturally lead to oversubscription and resource alloca-
tion failures, as limited testbed resources cannot meet the increasing
demand.
This paper examines the causes of resource allocation failures
on DeterLab testbed and ﬁnds three main culprits that create per-
ceived resource oversubscription, even when available nodes exist:
(1) overuse of mapping constraints by users, (2) testbed software
errors and (3) suboptimal resource allocation. We propose solu-
tions that could resolve these issues and reduce allocation failures
to 57.3% of the baseline.
In the remaining cases, real resource
oversubscription occurs. We examine testbed usage patterns and
show that a small fraction of unfair projects starve others for re-
sources under the current ﬁrst-come-ﬁrst-served allocation policy.
Due to interactive use of testbeds traditional fair-sharing techniques
are not suitable solutions. We then propose two novel approaches
– Take-a-Break and Borrow-and-Return – that temporarily pause
long-running experiments. These approaches can reduce resource
allocation failures to 25% of the baseline case by gently prolonging
1–2.5% of instances. While our investigation is done on DeterLab
testbed’s data, it should apply to all testbeds that run Emulab soft-
ware.
Categories and Subject Descriptors
C.2.1 [Computer Communication Networks]: Network Archi-
tecture and Design; C.2.3 [Computer Communication Networks]:
Network Operations
Keywords
network testbeds, Emulab, resource allocation
1.
INTRODUCTION
The last decade brought a major change in experimentation prac-
tices in several areas of computer science, as researchers migrated
from using simulation and theory to using network testbeds. Teach-
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’12, November 14–16, 2012, Boston, Massachusetts, USA.
Copyright 2012 ACM 978-1-4503-1705-4/12/11 ...$15.00.
ers are also shifting from traditional lecture-oriented courses to more
dynamic and realistic teaching styles that incorporate testbed use
for class demonstrations or student assignments. These diverse
groups of users each have important deadlines that they hope to
meet with help of testbeds, such as conference and research demon-
stration deadlines, class projects, class demonstrations, etc.
Current testbed resource allocation practices are not well aligned
with these user needs. Most testbeds deploy no automated prior-
itization of allocation requests, serving them on ﬁrst-come-ﬁrst-
served basis [24, 3, 2, 7, 10], which makes it impossible to guar-
antee availability during deadlines. In this paper we focus on test-
beds that allow users to obtain exclusive access to some portion
of their resources. As user demand grows, these testbeds experi-
ence overload that leads to allocation failures. Most testbeds further
let users keep allocated resources for as long as needed [24, 3, 2].
While there are idle detection mechanisms that attempt to reclaim
unused resources, users can and do opt out of them. Based on our
experience from managing DeterLab most experiments are interac-
tive, which means that resource allocations should last at most a
day. Thus long-running experiments reﬂect a user’s unwillingness
to release resources at the end of their work day. We believe this
occurs primarily because: (1) testbeds lack mechanisms to easily
save disk state on multiple machines and recreate it the next day,
and (2) users have no guarantee that resources will be available the
next day. Some users request help of testbed staff to reserve re-
sources for major deadlines. Since many testbeds lack reservation
mechanisms [24, 3, 2], these are done manually by staff pulling re-
quested machines out from the available pool. This often occurs
early, so staff could guarantee availability, but it wastes resources
and increases testbed overload.
Testbeds need better and more predictable allocation strategies
as they evolve from a novel to a mainstream experimentation plat-
form. Our main goal in this paper is to understand the reasons for
resource allocation failures and to propose changes in testbed oper-
ation that would reduce these. We survey related work in Section
2. We then introduce our terminology and data in Sections 3 and
4. We explain the resource allocation problem in network testbeds
in Section 5. We then examine reasons for resource allocation fail-
ures in the DeterLab testbed’s [3, 5] operation during 8 years of its
existence in Section 6. DeterLab is a public testbed for security ex-
perimentation, hosted by USC/ISI and UC Berkeley. In early 2012
it had around 350 general PCs and several tens of special-purpose
nodes. While it is build on Emulab technology, its focus is on cyber
security research, test and evaluation. It provides resources, tools,
and infrastructure for researchers to conduct rigorous, repeatable
experiments with new security technologies, and test their effec-
tiveness in a realistic environment. DeterLab is used extensively
both by security researchers and educators. It also experiences in-
tensive internal use for development of new testbed technologies.
495We ﬁnd that 81.5% of failures occur due to a perceived resource
shortage, i.e., not because the testbed lacks enough nodes, but be-
cause it lacks enough of the right nodes that the user desires. As
user desires and testbed allocation strategies act together to create
the shortage of the nodes that are in current demand, we next inves-
tigate how much relaxing user constraints (Section 7) or improving
resource allocation strategy (Section 8) help reduce allocation fail-
ures. Finally, we investigate if changes in testbed resource alloca-
tion policy would further improve resource allocation both in cases
of perceived and in cases of true resource shortage in Section 9. We
conclude in Section 10.
While we only had access to DeterLab’s dataset, this testbed’s
resource allocation algorithms and practices derive from the use of
Emulab software [24], which is used extensively by 40+ testbeds
around the world [1]. Our ﬁndings should apply to these testbeds.
Main contributions of our paper are:
• This is the ﬁrst analysis of causes for resource allocation fail-
ures in testbeds. We ﬁnd that 81.5% of failures occur due
to a perceived resource shortage, when in fact there are suf-
ﬁcient nodes to host a user’s request. Around half of these
cases occur because of inefﬁcient testbed software, while the
rest occur because of over-speciﬁcation in user’s resource re-
quests.
• We closely examine the resource allocation algorithm used
in Emulab testbeds – assign [20] – and show that it of-
ten performs suboptimally. We propose an improved algo-
rithm – assign+ – that reduces allocation failures to 77%
of those generated by assign, while running 10 times faster
and preserving more of the limited resources, such as inter-
switch bandwidth.
• We propose improvements to testbed resource allocation strat-
egy by gently relaxing user constraints, to reduce resource
allocation failures to 68.1% of those generated by assign.
• We propose modiﬁcations to testbed resource allocation pol-
icy that reshufﬂe allocated experiments to make space for
new ones. This further reduces allocation failures to 57.3%
of those generated by assign.
• We identify and demonstrate the need for some fair sharing
and prioritization of user allocation requests at times of true
overload. We propose two ways to modify testbed resource
allocation policy to achieve these effects: Take-a-Break and
Borrow-and-Return.
In both, resources from long-running
experiments are reclaimed and offered to incoming ones, but
in Take-a-Break they are held as long as needed, while in
Borrow-and-Return they are returned back to the original ex-
periment after 4 hours. We show that both these approaches
improve fairness of resource allocation, while reducing allo-
cation failures to 25.3 – 25.6% of those generated by assign.
• During the course of our study we have also identiﬁed tools
that testbeds need to develop either to better serve their users
or to become more stable. We suggest ﬁve such improve-
ments throughout the paper.
All the data used in our study is anonymized and publicly released
at http://nsl.isi.edu/TestbedUsageData.
2. RELATED WORK
The wide adoption of emulation testbeds in the networking re-
search community has spurred studies on different approaches for
designing and managing them. For example, the resource man-
agement mechanisms for Globus and PlanetLab are contrasted and
compared extensively by Ripeanu [21]. Additionally, Banik et.
al [4] conduct empirical evaluations for different protocols that can
provide exclusive access to shared resources on PlanetLab. The
StarBED project has several unique solutions for emulation that in-
clude conﬁguring the testbed and providing mechanisms for exper-
iment management [18, 19]. These works either evaluate pros and
cons of speciﬁc testbed management mechanisms or propose how
to build testbeds, but do not investigate resource allocation algo-
rithms, which is our focus.
Testbed usage and design practices have also attracted research
attention. Hermenier and Ricci examine the topological require-
ments of the experiments on the Emulab testbed [24] over the last
decade [9]. They propose a way to build better testbeds by: (1)
increasing the heterogeneity of node connectivity, (2) connecting
nodes to different switches to accommodate heterogeneous topolo-
gies without use of interswitch bandwidth, and (3) purchasing smaller
and cheaper switches to save costs. Our work is orthogonal to theirs
and focuses on optimizing allocation software and policies, regard-
less of testbed architecture. Kim et. al characterize the PlanetLab
testbed’s [7] usage over the last decade [13]. Their results indi-
cate that bartering and central banking schemes for resource alloca-
tion can handle only a small percentage of total scheduling require-
ments. They do not propose better resource allocation algorithms,
even though they identify the factors that account for high resource
contention or poor utilization.
Yu et al.
in [25] propose collecting allocation requests during
a time window and then allocating testbed resources to satisfy the
constraints of this request group. They employ a greedy algorithm
to map nodes and path splitting to map links. Besides, they perform
online migration to change the route or splitting ratio of a virtual
link, which re-balances the mapping of virtual topologies to maxi-
mize the chance of accepting future requests. Their methods con-
sider the general mapping problem at a high-level way, but do not
take into account heterogeneity of testbed nodes. Besides, queuing
allocation requests in network testbeds would introduce potentially
large delays that users would not tolerate. Chowdhury et al. in [6]
utilize mixed integer programming to solve the resource allocation
problem, but their constraints are limited to CPU capacity and dis-
tance between the locations of two testbed nodes. J. Lu et. al [15]
develop a method for mapping virtual topologies onto a testbed in
a cost-efﬁcient way. They consider trafﬁc-based constraints but do
not consider node heterogeneity or node features.
In a broader setting, ISPs tend to address resource allocation
problems by over provisioning their resources (bandwidth). This
solution does not readily apply to network testbeds. First, testbeds
have limits on how many machines they can host that stem from
the space, weight, cooling and power capacity of the rooms that
host them. Second, testbeds are hosted by academic institutions
and funded through grants – this limits both human and ﬁnancial
resources for purchase and maintenance of hardware. Finally, test-
bed use exhibits heavy tails along many dimensions (see Section
9.2), which prevents prediction of future resource needs.
Clusters and data centers face similar resource allocation issues
as testbeds [8, 11, 23]. In [8], Ghodsi et al. propose dominant re-
source fairness (DRF) for resource allocation in data centers. This
approach achieves fair allocation of heterogeneous resources be-
tween users who prioritize them differently. Unfortunately, like
other fair-sharing approaches, DRF is not readily applicable to test-
beds (see Section 9.2 for more details) due to interactive nature of
experimentation and due to different value of long vs short experi-
ments. In [11], Hindman et al. describe a platform called Mesos for
496sharing clusters, by allowing multiple resource allocation frame-
works to run simultaneously. Mesos offers resource shares to the
frameworks, based on some institutional policy, e.g., fair share, and
they decide which offers to accept and which tasks to allocate on
them. Some principles from [11], such as resource offers, may ap-
ply to testbeds, but they assume users that are way more sophis-
ticated and informed about resource allocation than testbeds cur-
rently have. Condor [23] is a workload management system for
compute-intensive jobs that aims to harness unused resources on
heterogeneous and distributed hardware and can migrate data and
jobs as nodes become available. While some Condor ideas may ap-
ply to network testbeds to achieve instance migration (see Section
9) testbed nodes are usually heavily customized by users, which
prevents ﬁne-grain migration that Condor excels at.
on the testbed, but no resources are yet allocated to the experiment
– experiment exists in the deﬁned state. A swapin event leads to
resource allocation, changing the experiment’s state to allocated. A
start event is equivalent to a preload followed by a swapin.
A swapout event releases resources from the experiment, chang-
ing its state to deﬁned. A swapmod event can occur either in de-
ﬁned or in allocated state. It changes the experiment’s deﬁnition
but does not lead to state change. If a swapmod fails while the
experiment is in allocated state, the testbed software automatically
generates a swapout event and reverts experiment state to deﬁned.
A destroy event removes an experiment’s virtual topology and
state from the testbed but history of its events still remains. Ta-
ble 1 shows the frequency of all experimental events in our dataset,
which is described in the following Section.
3. TERMINOLOGY
start
We now introduce several terms that relate to network testbed use
and illustrate them in Figure 1.
preload
deﬁned
swapin
allocated
destroy
experiments
users
project
A
B
1
2
3
nodes
instance1
instance2
2
e
z
s
i
A
1
e
z
s
i
A
4
e
z
s
i
instance3
3
e
z
s
i
A
instance4
B
duration1
duration2
duration3
duration4
Figure 1: Terminology
instance5
B
5
e
z
s
i
duration5
time
An experiment is a collection of inputs submitted by a user to
the testbed (one or more times) under the same identiﬁer. These
inputs describe experimenter’s needs such as experiment topology,
software to be installed on nodes, etc. We will say that each input
represents a virtual topology. Experiments can be modiﬁed, e.g.
by changing number of requested nodes or their connectivity. In
Figure 1 there are two experiments A and B.
An instance is an instantiation of the experiment at the physical
resources of the testbed. We say that an instance has duration (how
long were resources allocated to it), size (how many nodes were
allocated) and virtual topology (how were nodes connected to each
other, what types of nodes were requested, what OS, etc.). The
same experiment can result in multiple non-overlapping instances,
one for each resource allocation. In Figure 1 there are ﬁve instances,
three linked to the experiment A, and two linked to B. Release of
the resources back to the testbed or instance modiﬁcation denotes
the end of a particular instance.
A testbed project is a collection of experiment deﬁnitions and au-
thorized users, working on the same common project under a single
head-PI. In Figure 1 there is one project with two experiments and
three users.