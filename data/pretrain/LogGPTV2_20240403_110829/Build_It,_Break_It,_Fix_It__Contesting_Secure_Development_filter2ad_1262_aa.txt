title:Build It, Break It, Fix It: Contesting Secure Development
author:Andrew Ruef and
Michael W. Hicks and
James Parker and
Dave Levin and
Michelle L. Mazurek and
Piotr Mardziel
Build It, Break It, Fix It: Contesting Secure Development
Andrew Ruef
Dave Levin
Michael Hicks
Michelle L. Mazurek
James Parker
Piotr Mardziel†
University of Maryland
†Carnegie Mellon University
6
1
0
2
g
u
A
9
1
]
R
C
.
s
c
[
2
v
1
8
8
1
0
.
6
0
6
1
:
v
i
X
r
a
ABSTRACT
Typical security contests focus on breaking or mitigating the
impact of buggy systems. We present the Build-it, Break-it,
Fix-it (BIBIFI) contest, which aims to assess the ability to
securely build software, not just break it. In BIBIFI, teams
build speciﬁed software with the goal of maximizing correct-
ness, performance, and security. The latter is tested when
teams attempt to break other teams’ submissions. Win-
ners are chosen from among the best builders and the best
breakers. BIBIFI was designed to be open-ended—teams
can use any language, tool, process, etc. that they like. As
such, contest outcomes shed light on factors that correlate
with successfully building secure software and breaking inse-
cure software. During 2015, we ran three contests involving
a total of 116 teams and two diﬀerent programming prob-
lems. Quantitative analysis from these contests found that
the most eﬃcient build-it submissions used C/C++, but
submissions coded in other statically-typed languages were
less likely to have a security ﬂaw; build-it teams with di-
verse programming-language knowledge also produced more
secure code. Shorter programs correlated with better scores.
Break-it teams that were also successful build-it teams were
signiﬁcantly better at ﬁnding security bugs.
1.
INTRODUCTION
Cybersecurity contests [24, 25, 11, 27, 13] are popular
proving grounds for cybersecurity talent. Existing contests
largely focus on breaking (e.g., exploiting vulnerabilities or
misconﬁgurations) and mitigation (e.g., rapid patching or
reconﬁguration). They do not, however, test contestants’
ability to build (i.e., design and implement) systems that are
secure in the ﬁrst place. Typical programming contests [35,
2, 21] do focus on design and implementation, but generally
ignore security. This state of aﬀairs is unfortunate because
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’16, October 24 – 28, 2016, Vienna, Austria
c(cid:13) 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978382
experts have long advocated that achieving security in a
computer system requires treating security as a ﬁrst-order
design goal [32], and is not something that can be added
after the fact. As such, we should not assume that good
breakers will necessarily be good builders [23], nor that top
coders necessarily produce secure systems.
This paper presents Build-it, Break-it, Fix-it (BIBIFI),
a new security contest with a focus on building secure sys-
tems. A BIBIFI contest has three phases. The ﬁrst phase,
Build-it, asks small development teams to build software ac-
cording to a provided speciﬁcation that includes security
goals. The software is scored for being correct, eﬃcient, and
feature-ful. The second phase, Break-it, asks teams to ﬁnd
defects in other teams’ build-it submissions. Reported de-
fects, proved via test cases vetted by an oracle implementa-
tion, beneﬁt a break-it team’s score and penalize the build-it
team’s score; more points are assigned to security-relevant
problems. (A team’s break-it and build-it scores are inde-
pendent, with prizes for top scorers in each category.) The
ﬁnal phase, Fix-it, asks builders to ﬁx bugs and thereby get
points back if the process discovers that distinct break-it
test cases identify the same defect.
BIBIFI’s design aims to minimize the manual eﬀort of
running a contest, helping it scale. BIBIFI’s structure and
scoring system also aim to encourage meaningful outcomes,
e.g., to ensure that the top-scoring build-it teams really pro-
duce secure and eﬃcient software. Behaviors that would
thwart such outcomes are discouraged. For example, break-
it teams may submit a limited number of bug reports per
build-it submission, and will lose points during ﬁx-it for test
cases that expose the same underlying defect or a defect also
identiﬁed by other teams. As such, they are encouraged to
look for bugs broadly (in many submissions) and deeply (to
uncover hard-to-ﬁnd bugs).
In addition to providing a novel educational experience,
BIBIFI presents an opportunity to study the building and
breaking process scientiﬁcally.
In particular, BIBIFI con-
tests may serve as a quasi-controlled experiment that cor-
relates participation data with ﬁnal outcome. By exam-
ining artifacts and participant surveys, we can study how
the choice of build-it programming language, team size and
experience, code size, testing technique, etc. can inﬂuence a
team’s (non)success in the build-it or break-it phases. To the
extent that contest problems are realistic and contest partic-
ipants represent the professional developer community, the
results of this study may provide useful empirical evidence
for practices that help or harm real-world security. Indeed,
the contest environment could be used to incubate ideas to
improve development security, with the best ideas making
their way to practice.
This paper studies the outcomes of three BIBIFI contests
that we held during 2015, involving two diﬀerent program-
ming problems. The ﬁrst contest asked participants to build
a secure, append-only log for adding and querying events
generated by a hypothetical art gallery security system. At-
tackers with direct access to the log, but lacking an “authen-
tication token,” should not be able to steal or corrupt the
data it contains. The second and third contests were run
simultaneously. They asked participants to build a pair of
secure, communicating programs, one representing an ATM
and the other representing a bank. Attackers acting as a
man in the middle (MITM) should neither be able to steal
information (e.g., bank account names or balances) nor cor-
rupt it (e.g., stealing from or adding money to accounts).
Two of the three contests drew participants from a MOOC
(Massive Online Open Courseware) course on cybersecurity.
These participants (278 total, comprising 109 teams) had an
average of 10 years of programming experience and had just
completed a four-course sequence including courses on se-
cure software and cryptography. The third contest involved
U.S.-based graduate and undergraduate students (23 total,
comprising 6 teams) with less experience and training.
BIBIFI’s design permitted it to scale reasonably well. For
example, one full-time person and two part-time judges ran
the ﬁrst 2015 contest in its entirety. This contest involved
156 participants comprising 68 teams, which submitted more
than 20,000 test cases. And yet, organizer eﬀort was lim-
ited to judging whether the few hundred submitted ﬁxes
addressed only a single conceptual defect; other work was
handled automatically or by the participants themselves.
Rigorous quantitative analysis of the contests’ outcomes
revealed several interesting, statistically signiﬁcant eﬀects.
Considering build-it scores: Writing code in C/C++ in-
creased build-it scores initially, but also increased chances
of a security bug found later.
Interestingly, the increased
insecurity for C/C++ programs appears to be almost en-
tirely attributable to memory-safety bugs. Teams that had
broader programming language knowledge or that wrote less
code also produced more secure implementations. Consid-
ering break-it scores: Larger teams found more bugs during
the break-it phase. Greater programming experience and
knowledge of C were also helpful. Break-it teams that also
qualiﬁed during the build-it phase were signiﬁcantly more
likely to ﬁnd a security bug than those that did not. Use
of advanced tools such as fuzzing or static analysis did not
provide a signiﬁcant advantage among our contest partici-
pants.
We manually examined both build-it and break-it arti-
facts. Successful build-it teams typically employed third-
party libraries—e.g., SSL, NaCL, and BouncyCastle—to im-
plement cryptographic operations and/or communications,
which freed up worry of proper use of randomness, nonces,
etc. Unsuccessful teams typically failed to employ cryptog-
raphy, implemented it incorrectly, used insuﬃcient random-
ness, or failed to use authentication. Break-it teams found
clever ways to exploit security problems; some MITM im-
plementations were quite sophisticated.
In summary, this paper makes two main contributions.
First, it presents BIBIFI, a new security contest that en-
courages building, not just breaking. Second, it presents a
detailed description of three BIBIFI contests along with both
a quantitative and qualitative analysis of the results. We
will be making the BIBIFI code and infrastructure publicly
available so that others may run their own competitions; we
hope that this opens up a line of research built on empir-
ical experiments with secure programming methodologies.1
More information, data, and opportunities to participate are
available at https://builditbreakit.org.
The rest of this paper is organized as follows. We present
the design of BIBIFI in §2 and describe speciﬁcs of the con-
tests we ran in §3. We present the quantitative analysis of
the data we collected from these contests in §4, and qual-
itative analysis in §5. We review related work in §6 and
conclude in §7.
2. BUILD-IT, BREAK-IT, FIX-IT
This section describes the goals, design, and implementa-
tion of the BIBIFI competition. At the highest level, our
aim is to create an environment that closely reﬂects real-
world development goals and constraints, and to encourage
build-it teams to write the most secure code they can, and
break-it teams to perform the most thorough, creative anal-
ysis of others’ code they can. We achieve this through a
careful design of how the competition is run and how vari-
ous acts are scored (or penalized). We also aim to minimize
the manual work required of the organizers—to allow the
contest to scale—by employing automation and proper par-
ticipant incentives.
2.1 Competition phases
We begin by describing the high-level mechanics of what
occurs during a BIBIFI competition. BIBIFI may be ad-
ministered on-line, rather than on-site, so teams may be geo-
graphically distributed. The contest comprises three phases,
each of which last about two weeks for the contests we de-
scribe in this paper.
BIBIFI begins with the build-it phase. Registered build-
it teams aim to implement the target software system ac-
cording to a published speciﬁcation created by the contest
organizers. A suitable target is one that can be completed
by good programmers in a short time (just about two weeks,
for the contests we ran), is easily benchmarked for perfor-
mance, and has an interesting attack surface. The software
should have speciﬁc security goals—e.g., protecting private
information or communications—which could be compro-
mised by poor design and/or implementation. The software
should also not be too similar to existing software to ensure
that contestants do the coding themselves (while still tak-
ing advantage of high-quality libraries and frameworks to
the extent possible). The software must build and run on a
standard Linux VM made available prior to the start of the
contest. Teams must develop using Git [17]; with each push,
the contest infrastructure downloads the submission, builds
it, tests it (for correctness and performance), and updates
the scoreboard. §3 describes the two target problems we
developed: (1) an append-only log; and (2) a pair of com-
municating programs that simulate a bank and an ATM.
1This paper subsumes a previously published short workshop pa-
per [31] and a short invited article [30]. The initial BIBIFI design
and implementation also appeared in those papers, as did a brief
description of a pilot run of the contest. This paper presents
many more details about the contest setup along with a quanti-
tative and qualitative analysis of the outcomes of several larger
contests.
The next phase is the break-it phase. Break-it teams
can download, build, and inspect all qualifying build-it sub-
missions, including source code; to qualify, the submission
must build properly, pass all correctness tests, and not be
purposely obfuscated (accusations of obfuscation are manu-
ally judged by the contest organizers). We randomize each
break-it team’s view of the build-it teams’ submissions,2 but
organize them by meta-data, such as programming language.
When they think they have found a defect, breakers submit
a test case that exposes the defect and an explanation of
the issue. To encourage coverage, a break-it team may only
submit up a ﬁxed number of test cases per build-it submis-
sion. BIBIFI’s infrastructure automatically judges whether
a submitted test case truly reveals a defect. For example,
for a correctness bug, it will run the test against a reference
implementation (“the oracle”) and the targeted submission,
and only if the test passes on the former but fails on the
latter will it be accepted.3 More points are awarded to bugs
that clearly reveal security problems, which may be demon-
strated using alternative test formats. The auto-judgment
approaches we developed for the two diﬀerent contest prob-
lems are described in §3.
The ﬁnal phase is the ﬁx-it phase. Build-it teams are
provided with the bug reports and test cases implicating
their submission. They may ﬁx ﬂaws these test cases iden-
tify; if a single ﬁx corrects more than one failing test case,
the test cases are “morally the same,” and thus points are
only deducted for one of them. The organizers determine,
based on information provided by the build-it teams and
other assessment, whether a submitted ﬁx is “atomic” in the
sense that it corrects only one conceptual ﬂaw; if not, the
ﬁx is rejected.
Once the ﬁnal phase concludes, prizes are awarded to the
best builders and best breakers as determined by the scoring
system described next.
2.2 Competition scoring
BIBIFI’s scoring system aims to encourage the contest’s
basic goals, which are that the winners of the build-it phase
truly produced the highest quality software, and that the
winners of the break-it phase performed the most thorough,
creative analysis of others’ code. The scoring rules create
incentives for good behavior (and disincentives for bad be-
havior).
2.2.1 Build-it scores
To reﬂect real-world development concerns, the winning
build-it team would ideally develop software that is correct,
secure, and eﬃcient. While security is of primary interest to
our contest, developers in practice must balance these other
aspects of quality against security [1, 36], leading to a set of
trade-oﬀs that cannot be ignored if we wish to understand
real developer decision-making.
To encourage these, each build-it team’s score is the sum
of the ship score4 and the resilience score. The ship score is
composed of points gained for correctness tests and perfor-
2This avoids spurious unfair eﬀects, such as if break-it teams
investigating code in the order in which we give it to them.
3Teams can also earn points by reporting bugs in the oracle, i.e.,
where its behavior contradicts the written speciﬁcation; these re-
ports are judged by the organizers.
4The name is meant to evoke a quality measure at the time soft-
ware is shipped.
mance tests. Each mandatory correctness test is worth M
points, for some constant M , while each optional correct-
ness test is worth M/2 points. Each performance test has
a numeric measure depending on the speciﬁc nature of the
programming project—e.g., latency, space consumed, ﬁles
left unprocessed—where lower measures are better. A test’s
worth is M · (worst − v)/(worst − best), where v is the mea-
sured result, best is the measure for the best-performing sub-
mission, and worst is the worst performing. As such, each
performance test’s value ranges from 0 to M .
The resilience score is determined after the break-it and