of the training process with our simulation parameters (i.e.,
maximum bid price of 0.0955). The chosen maximum bid price
and spot market price variations will dictate the total number of
 0 40 80 120 160 0 200 400 600 800 1000sgx−emlPMTime per iteration (s)Batch SizeUnencrypted MNISTEncrypted MNIST 0 40 80 120 160 0 200 400 600 800 1000emlSGX−PMBatch Size 0 0.5 1 1.5 2 2.5 3 0 100 200 300 400 500 600(a) Crash resilient systemLossTraining IterationsNo crashCrash resilient 0 0.5 1 1.5 2 2.5 3 0 200 400 600 800 1000crash pointresume point(b) Non−crash resilient systemTraining IterationsNon−crash resilient 0 0.5 1 1.5 2 2.5 3 0 100 200 300 400 500 600(a) Spot training loss curveLossTraining Iterations01 0 100 200 300 400 500 600 700 800training resumedtraining interrupted(b) Spot instance state curveStateTime (min) 0 0.5 1 1.5 2 2.5 3 0 100 200 300 400 500 600training interruptedtraining resumed(c) Non crash resilient spot trainingLossTraining Iterations01 0 100 200 300 400 500 600 700 800training resumedtraining interrupted(d) Spot instance state curveStateTime (min)interruptions of the spot instance, and hence the total training
time (interruption times included). The spot traces used and
our simulation scripts are available in the PLINIUS repository.
Figure 10(c) shows us the loss curve obtained when there is
no crash resilience (i.e., the model’s state is not saved). With
the given simulation parameters (i.e., maximum bid price of
0.0955), there are two interruptions during the training process.
As explained in the previous section, it needs to resume training
afresh, and hence the combined number of iterations (and
total time) from when training ﬁrst began is increased when
compared to its crash resilient counterpart. This further justiﬁes
the need for fault tolerance guarantees in such ML scenarios.
CPU and memory overhead. Our mirroring mechanism
uses 140 bytes of PM for encryption metadata per layer. The
MAC is 16 B, the IV is 12 B, giving 28 B per encrypted
parameter buffer. Each layer contains 5 parameter matrices,
hence 28 × 5 = 140 B per layer. With a model of N layers,
we account for N × 140 extra bytes on PM for encryption
metadata, small compared to the size of actual models (order
of few MBs). The training algorithm is a fairly intensive single-
threaded application and it uses 98-100% of the CPU during
execution.
Secure inference. PLINIUS can also be used for secure infer-
ence. We trained a CNN model with 12 LReLU convolutional
layers on the MNIST training dataset, and used the trained
model to classify 10’000 grayscale images of handwritten digits
in the range [0 − 9]. The model (available in the PLINIUS
repository) achieved an accuracy of 98.52% with the given
hyper-parameters.
GPU and TPU support. Hardware accelerators like Graph-
ics Processing Units (GPUs) and Tensor Processing Units
(TPUs) are increasingly used in ML applications. However the
former do not integrate TEE capabilities. Recent works like
HIX [23], Graviton [37], and Slalom [33] propose techniques
to securely ofﬂoad expensive ML computations to GPUs.
Using Darknet’s CUDA extensions, PLINIUS can leverage such
techniques to improve training performance. The trained model
weights can be securely copied between the secure CPU and the
GPU (or TPU) and our mirroring mechanism applied without
much changes. We are exploring possible improvements of
PLINIUS in this direction.
VII. RELATED WORK
TEE-based schemes. There exists several solutions leveraging
trusted hardware (i.e., Intel SGX) for secure ML. Slalom [33] is
a framework for secure DNN inference in TEEs. It outsources
costly neural network operations to a faster, but untrusted GPU
during inference. Occlumency [26] leverages Intel SGX to
preserve conﬁdentiality and integrity of user-data during deep
learning inference in untrusted cloud infrastructure. Privado
[15] implements a secure inference-as-a-service, by eliminating
input-dependent access patterns from ML code, hence reducing
data leakage risks in the enclave. Chiron [19] leverages Intel
SGX for secure ML-as-a-service which prevents disclosure of
both data and code.
10
These systems leverage TEEs for model inference, but
without any support for failure recovery. PLINIUS provides a
full framework that supports both in-enclave model training
and inference with efﬁcient fault tolerance guarantees on PM.
SecureTF [25] integrates TensorFlow ML library for model
training and inference in secure SCONE containers. This
requires the full TensorFlow library (over 2.5 million LOC [5])
to run inside SGX enclaves, which by design increases the TCB.
On the other hand, the trusted portion of PLINIUS comprises
15’900 LOC. The reduction in TCB in PLINIUS when compared
to SecureTF is quite obvious; this is better from a security
perspective.
Homomorphic encryption (HE)-based schemes. Without
trusted hardware enclaves, many privacy-preserving ML meth-
ods achieve security via HE-based techniques. HE schemes
compute directly over encrypted data. CryptoNets [14] im-
plements inference over encrypted data for pre-trained neural
networks. Solutions exist [18] to train and do inference on
neural network models using HE.
While these methods ensure privacy of sensitive training and
classiﬁcation data during model training and inference, they
have signiﬁcant performance overhead (up to 1000× slower
than TEE-based schemes [20]). PLINIUS provides an orthogonal
approach to tackle security, combining Intel SGX enclaves to
ensure conﬁdentiality and integrity of models and data sets
during training and inference at a much lower cost.
Fault tolerance in ML. A common technique for fault toler-
ance in ML learning frameworks is checkpointing (restoring)
of the model’s state to (from) secondary storage during training
(recovery). Several frameworks (i.e., Tensorﬂow [6], Caffe [24],
Darknet [3], etc.) rely on secondary storage as persistent storage
for training data throughout the training process. Distributing
training across several compute nodes improves scalability
while increasing fault tolerance.
The above mentioned techniques have huge performance
overhead, due to high access times of secondary storage.
Following a crash, entire data sets and models must be reloaded
into main memory from secondary storage. PLINIUS’s novel
mirroring mechanism leverages PM for fault tolerance: upon
a crash, the model and the associated training data are readily
available in memory. Our design completely obviates the need
for expensive serialization (deserialization) of models to (from)
secondary storage, and proposes a more efﬁcient approach for
handling large amounts of training data.
VIII. CONCLUSION
PLINIUS is the ﬁrst secure ML framework to leverage Intel
SGX for secure model training and PM for fault tolerance. Our
novel mirroring mechanism creates encrypted mirror copies
of enclave ML models in PM, which are synchronized across
training iterations. Our design leverages PM to store byte-
addressable training data, completely circumventing expensive
disk I/O operations in the event of a system failure. The
evaluation of PLINIUS shows that its design substantially
reduces the TCB when compared to a system with unmodiﬁed
libraries, and the mirroring mechanism outperforms disk-based
checkpointing systems while ensuring the system’s robustness
upon system failures. Using real-world datasets for image
recognition, we show that PLINIUS offers a practical solution
to securely train ML models in TEEs integrated with PM
hardware at a reasonable cost.
We will extend this work along the following directions.
First, we intend to explore GPUs and TPUs by ofﬂoading
expensive enclave operations on the former without a loss in
conﬁdentiality. The extent to which this can be done while
preserving conﬁdentiality of the model parameters and training
or inference data will be the key area for future work. Second,
we wish to explore distributed training using PLINIUS to
overcome the SGX EPC limitation. Lastly, we plan to better
exploit system parallelism to improve the performance of
PLINIUS. This entails redesigning SGX-DARKNET to efﬁciently
support parallel training with threads spawned in the untrusted
runtime.
ACKNOWLEDGMENT
This work received funds from the Swiss National Science
Foundation (FNS) under project PersiST (no. 178822).
REFERENCES
[1] “Optimize Your Cloud and Enable Azure Customers to Innovate at Scale
with Intel® Optane™ Persistent Memory,” https://www.intel.la/content/
www/xl/es/now/microsoft-azure-optane-innovation-editorial.html, ac-
cessed: Mar 9, 2021.
[2] “The MNIST Database of Handwritten Digits,” http://yann.lecun.com/
exdb/mnist/, accessed: May 7, 2020.
[3] “Darknet: Open Source Neural Networks in C,” https://pjreddie.com/
darknet/, 2013-2016, accessed: May 7, 2020.
[4] “Intel SGX Evolves for Data Center,” https://itpeernetwork.intel.com/
intel-sgx-data-center/, 2019, accessed: Dec 11, 2020.
[5] “The
TensorFlow Open
Source
https://www.openhub.net/p/tensorﬂow,
2020.
Project
2019,
on Open Hub,”
11,
accessed: Dec
[6] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,
S. Ghemawat, G. Irving, M. Isard et al., “Tensorﬂow: A system for
large-scale machine learning,” in USENIX OSDI 2016.
[7] M. Al-Rubaie and J. M. Chang, “Privacy-preserving machine learning:
Threats and solutions,” IEEE Security Privacy, 2019.
[8] S. Arnautov, B. Trach, F. Gregor, T. Knauth, A. Martin, C. Priebe, J. Lind,
D. Muthukumaran, D. O’Keeffe, M. L. Stillwell, D. Goltzsche, D. Eyers,
R. Kapitza, P. Pietzuch, and C. Fetzer, “SCONE: Secure linux containers
with intel SGX,” in USENIX OSDI 16.
[9] J. V. Bulck, M. Minkin, O. Weisse, D. Genkin, B. Kasikci, F. Piessens,
M. Silberstein, T. F. Wenisch, Y. Yarom, and R. Strackx, “Foreshadow:
Extracting the keys to the intel SGX kingdom with transient out-of-order
execution,” in USENIX Security 2018.
[10] I. Corporation, “Intel® Software Guard Extensions Developer Reference
for Linux* OS,” https://download.01.org/intel-sgx/sgx-linux/2.8/docs/
Intel_SGX_Developer_Reference_Linux_2.8_Open_Source.pdf, 2019.
[11] A. Correia, P. Felber, and P. Ramalhete, “Romulus: Efﬁcient algorithms
for persistent transactional memory,” in SPAA’18.
[12] V. Costan and S. Devadas, “Intel SGX explained,” 2016.
[13] M. J. Dworkin, “Recommendation for block cipher modes of operation:
Galois/counter mode (gcm) and gmac,” Tech. Rep., 2007.
[14] R. Gilad-Bachrach, N. Dowlin, K. Laine, K. Lauter, M. Naehrig, and
J. Wernsing, “Cryptonets: Applying neural networks to encrypted data
with high throughput and accuracy,” in ICML’16.
[15] K. Grover, S. Tople, S. Shinde, R. Bhagwan, and R. Ramjee, “Privado:
Practical and Secure DNN Inference with Enclaves,” arXiv preprint
arXiv:1810.00602, 2018.
[16] D. Gruss, J. Lettner, F. Schuster, O. Ohrimenko, I. Haller, and M. Costa,
“Strong and efﬁcient cache side-channel protection using hardware
transactional memory,” in USENIX Security 2017.
[17] L. Hanzlik, Y. Zhang, K. Grosse, A. Salem, M. Augustin, M. Backes, and
M. Fritz, “Mlcapsule: Guarded ofﬂine deployment of machine learning
as a service,” arXiv preprint arXiv:1808.00590, 2018.
[18] E. Hesamifard, H. Takabi, M. Ghasemi, and C. Jones, “Privacy-preserving
machine learning in cloud,” in Proceedings of the 2017 on Cloud
Computing Security Workshop, 2017.
[19] T. Hunt, C. Song, R. Shokri, V. Shmatikov, and E. Witchel, “Chiron:
Privacy-preserving machine learning as a service,” arXiv preprint
arXiv:1803.05961, 2018.
[20] N. Hynes, R. Cheng, and D. Song, “Efﬁcient deep learning on multi-
source private data,” arXiv preprint arXiv:1807.06689, 2018.
[21] C. Iorgulescu, R. Azimi, Y. Kwon, S. Elnikety, M. Syamala, V. R.
Narasayya, H. Herodotou, P. Tomita, A. Chen, J. Zhang, and J. Wang,
“Perﬁso: Performance isolation for commercial latency-sensitive services,”
in USENIX ATC 2018.
[22] J. Izraelevitz, J. Yang, L. Zhang, J. Kim, X. Liu, A. Memaripour,
Y. J. Soh, Z. Wang, Y. Xu, S. R. Dulloor et al., “Basic performance
measurements of the intel optane dc persistent memory module,” arXiv
preprint arXiv:1903.05714, 2019.
[23] I.
Jang, A. Tang, T. Kim, S. Sethumadhavan, and J. Huh,
“Heterogeneous isolated execution for commodity gpus,” in Proceedings
of the Twenty-Fourth International Conference on Architectural Support
for Programming Languages and Operating Systems, ser. ASPLOS ’19.
New York, NY, USA: Association for Computing Machinery, 2019, p.
455–468. [Online]. Available: https://doi.org/10.1145/3297858.3304021
[24] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,
S. Guadarrama, and T. Darrell, “Caffe: Convolutional architecture for
fast feature embedding,” in ACM Multimedia 2014.
[25] R. Kunkel, D. L. Quoc, F. Gregor, S. Arnautov, P. Bhatotia, and C. Fetzer,
“SecureTF: A Secure TensorFlow Framework,” in Middleware’20.
[26] T. Lee, Z. Lin, S. Pushp, C. Li, Y. Liu, Y. Lee, F. Xu, C. Xu, L. Zhang,
and J. Song, “Occlumency: Privacy-preserving remote deep-learning
inference using SGX,” in MobiCom’19.
[27] L. Lersch, X. Hao, I. Oukid, T. Wang, and T. Willhalm, “Evaluating
persistent memory range indexes,” VLDB Endowment 2019.
[28] E. Liberty, Z. Karnin, B. Xiang, L. Rouesnel, B. Coskun, R. Nallapati,
J. Delgado, A. Sadoughi, Y. Astashonok, P. Das et al., “Elastic Machine
Learning Algorithms in Amazon SageMaker,” SIGMOD’20.
[29] P. Mohassel and Y. Zhang, “SecureML: A system for scalable privacy-
preserving machine learning,” in IEEE S&P 2017.
[30] O. Oleksenko, B. Trach, R. Krahn, M. Silberstein, and C. Fetzer, “Varys:
Protecting SGX enclaves from practical side-channel attacks,” in USENIX
ATC 2018.
[31] M. Schwarz, S. Weiser, D. Gruss, C. Maurice, and S. Mangard, “Malware
guard extension: Using SGX to conceal cache attacks,” in International
Conference on Detection of Intrusions and Malware, and Vulnerability
Assessment, 2017.
[32] Y. Shen, H. Tian, Y. Chen, K. Chen, R. Wang, Y. Xu, Y. Xia, and S. Yan,
“Occlum: Secure and Efﬁcient Multitasking Inside a Single Enclave of
Intel SGX,” ser. ASPLOS’20.
[33] F. Tramer and D. Boneh, “Slalom: Fast, veriﬁable and private execution
of neural networks in trusted hardware,” in ICLR’19.
[34] F. Tramèr, F. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart, “Stealing
machine learning models via prediction apis,” in USENIX Security 2016.
[35] C. Tsai, D. E. Porter, and M. Vij, “Graphene-sgx: A practical library OS
for unmodiﬁed applications on SGX,” in USENIX ATC 2017.
[36] H. Volos, A. J. Tack, and M. M. Swift, “Mnemosyne: Lightweight
persistent memory,” in ASPLOS’11.
[37] S. Volos, K. Vaswani, and R. Bruno, “Graviton: Trusted execution
environments on gpus,” in 13th USENIX Symposium on Operating
Systems Design and Implementation (OSDI 18). Carlsbad, CA:
USENIX Association, Oct. 2018, pp. 681–696. [Online]. Available:
https://www.usenix.org/conference/osdi18/presentation/volos
[38] C. Wang, Q. Liang, and B. Urgaonkar, “An empirical analysis of amazon
EC2 Spot Instance features affecting cost-effective resource procurement,”
TOMPECS’18.
[39] N. Weichbrodt, P. Aublin, and R. Kapitza, “sgx-perf: A performance
analysis tool for intel SGX enclaves,” in Middleware’18.
[40] J. Yang, J. Kim, M. Hoseinzadeh, J. Izraelevitz, and S. Swanson, “An
empirical guide to the behavior and use of scalable persistent memory,”
in USENIX FAST 2020.
[41] P. Zuo and Y. Hua, “Secpm: a secure and persistent memory system for
non-volatile memory,” in HotStorage’18.
11