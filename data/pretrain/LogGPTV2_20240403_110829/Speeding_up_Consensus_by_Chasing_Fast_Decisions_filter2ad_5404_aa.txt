title:Speeding up Consensus by Chasing Fast Decisions
author:Balaji Arun and
Sebastiano Peluso and
Roberto Palmieri and
Giuliano Losa and
Binoy Ravindran
2017 47th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
Speeding up Consensus by Chasing Fast Decisions
Balaji Arun, Sebastiano Peluso, Roberto Palmieri, Giuliano Losa, Binoy Ravindran
{balajia,peluso,robertop,giuliano.losa,binoy}@vt.edu
ECE, Virginia Tech, USA
Abstract—This paper proposes CAESAR, a novel multi-leader
Generalized Consensus protocol for geographically replicated
sites. The main goal of CAESAR is to overcome one of the
major limitations of existing approaches, which is the signiﬁcant
performance degradation when application workload produces
conﬂicting requests. CAESAR does that by changing the way a
fast decision is taken: its ordering protocol does not reject a fast
decision for a client request if a quorum of nodes reply with
different dependency sets for that request. The effectiveness of
CAESAR is demonstrated through an evaluation study performed
on Amazon’s EC2 infrastructure using 5 geo-replicated sites.
CAESAR outperforms other multi-leader (e.g., EPaxos) competi-
tors by as much as 1.7x in the presence of 30% conﬂicting
requests, and single-leader (e.g., Multi-Paxos) by up to 3.5x.
Keywords-Consensus, Geo-Replication, Paxos
I. INTRODUCTION
Geographically replicated (geo-scale)
services, namely
those where actors are spread across geographic locations and
operate on the same shared database, can be implemented in an
easy manner by exploiting underlying synchronization mecha-
nisms that provide strong consistency guarantees. These mech-
anisms ultimately rely on implementations of Consensus [1]
to globally agree on sequences of operations to be executed.
Paxos [2], [3] is a popular algorithm for solving Consensus
among participants interconnected by asynchronous networks,
even in presence of faults, and it can be leveraged for building
such robust services [4], [5], [6], [7], [8]. An example of Paxos
used in a production system is Google Spanner [4], [9].
The most deployed version of Paxos is Multi-Paxos [3],
where there is a designated node, the leader, that is elected and
responsible for deciding the order of client-issued commands.
Multi-Paxos solves Consensus in only three communication
delays, but in practice, its performance is tied to the per-
formance of the leader. This relation is risky when Multi-
Paxos is deployed in geo-scale because network delays can
be arbitrarily large and unpredictable. In these settings, the
leader might often be unreachable or slow, thus causing the
slow down of the entire system.
To overcome this limitation, protocols aimed at allowing
multiple nodes to operate as command leaders at the same
time [10], [11], [12] have been proposed. Such solutions
provide implementations of Generalized Consensus [13], a
variant of Consensus that agrees on a common order of non-
commutative (or conﬂicting) commands. These approaches,
despite avoiding the bottleneck of the single leader, suffer
from other costs whenever a non-trivial amount of conﬂicting
commands (e.g., 5% – 40%) is proposed concurrently, as they
do not rely on a unique point of decision.
This paper presents the ﬁrst multi-leader implementation
of Generalized Consensus designed for maintaining high
performance in the presence of both mostly non-conﬂicting
workloads (named as such if less than 5% of conﬂicting
commands are issued) and conﬂicting workloads (where at
most 40% of commands conﬂict with each other). For this
reason, our solution is apt for geo-scale deployments. More
speciﬁcally, state-of-the-art implementations of Generalized
Consensus (e.g., EPaxos [10] and M 2Paxos [14]) reduce the
minimum number of communication delays required to reach
an agreement from three to two in case a proposed command
does not encounter any contention (fast decision). However,
they fail in the following aspect: they are not able to minimize
the latency as soon as some contention on issued commands
arises, with the consequence of requiring a slow decision,
which consists of at least four communication delays.
To address these aspects, we propose CAESAR, a consensus
layer that deploys an innovative multi-leader ordering scheme.
As a high-level
intuition, when a conﬂicting command is
proposed, CAESAR does not suffer from the condition that
causes a slow decision of that command in all existing
Generalized Consensus implementations (including EPaxos).
Such a condition is the following:
For a proposed command c, at least two nodes in a quorum
are aware of different sets of commands conﬂicting with c.
CAESAR avoids this pitfall because it approaches the prob-
lem of establishing agreement from a different perspective.
When a command c is proposed, CAESAR seeks an agreement
on a common delivery timestamp for c rather than on its
set of conﬂicting commands. To facilitate this, a local wait
condition is deployed to prevent commands conﬂicting with c
from interfering with the decision process of c if they have a
timestamp greater than c’s timestamp.
The basic idea behind the ordering process of CAESAR
is the following: a command is associated with a logical
timestamp by the sender, and if a quorum of nodes conﬁrms
that the timestamp is still valid, then the command is ordered
after all
the conﬂicting commands having a valid earlier
timestamp. Otherwise, the timestamp is considered invalid,
and the command is rejected forcing it to undergo two more
communication delays (total of four) before being decided.
Note that the equality of the sets of conﬂicting commands
collected by nodes does not inﬂuence the ordering decision.
With this scheme, CAESAR boosts timestamp-based ordering
protocols, such as Mencius [11], by exploiting quorums, which
is a fundamental requirement in geo-scale where contacting all
nodes is not feasible. CAESAR does that without relying on a
2158-3927/17 $31.00 © 2017 IEEE
DOI 10.1109/DSN.2017.35
49
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:00:16 UTC from IEEE Xplore.  Restrictions apply. 
single designated leader unlike Multi-Paxos.
Our approach also provides the beneﬁt of a more parallel
delivery of ordered commands when compared to EPaxos,
which requires analysis of the dependency graphs. That is
because once the delivery timestamp for a command is ﬁ-
nalized, the command implicitly carries with itself the set of
predecessor commands that have to be delivered before it. This
so-called predecessors set is computed during the execution of
the ordering algorithm for the decision of the timestamp and
not after the delivery of the command.
We conducted an evaluation study for CAESAR using key-
value store interfaces. With them, we can inject different
workloads by varying the percentage of conﬂicting commands
and measure various performance parameters. We contrasted
CAESAR against: EPaxos and M 2Paxos, multi-leader quorum-
based Generalized Consensus implementations; Mencius, a
multi-leader timestamp-based Consensus implementation that
does not rely on quorums; and Multi-Paxos, a single-leader
Consensus implementation. As a testbed, we deployed 5 geo-
replicated sites using the Amazon EC2 infrastructure.
The results conﬁrm the effectiveness of CAESAR in pro-
viding fast decisions, even in the presence of conﬂicting
workloads, while competitors slow down. Using workloads
with a conﬂict percentage in the range of 2% – 50%, CAESAR
outperforms EPaxos, which is the closest competitor in most
of the cases, by reducing latency up to 60% and increasing
throughput by 1.7×. These performance boosts are due to the
higher percentage of fast decisions accomplished. With 30%
of conﬂicting workload, CAESAR takes up to 70% fewer slow
decisions compared to EPaxos.
II. RELATED WORK
In the Paxos [3] algorithm, a value is decided after a
minimum of four communication delays. Progress guarantees
cannot be provided as the initial prepare phase may fail in
the presence of multiple concurrent proposals. Multi-Paxos
alleviates this by letting promises in the prepare phase cover
an entire sequence of values. This effectively establishes a
distinguished proposer that acts as a single designated leader.
Fast Paxos [15] eliminates one communication delay by
having proposers broadcast their request and bypass the leader.
However, a classic Paxos round executed by the leader is
needed to resolve a collision, reaching a total of six communi-
cation delays to decide a value. Generalized Paxos [13] relies
on a single leader to detect conﬂicts among commands and
enforce an order, and it uses fast quorums as Fast Paxos. Some
of its limitations are overcome by FGGC [16], which can use
optimal quorum size but still relies on designated leaders. On
the contrary, CAESAR avoids the usage of a single designated
leader either to reach an agreement, as in Paxos, or to resolve
a conﬂict, as in Fast and Generalized Paxos.
Mencius [11] overcomes the limitations of a single leader
protocol by providing a multi-leader ordering scheme based
on a pre-assignment of Consensus instances to nodes. It pre-
assigns sending slots to nodes, and a sender can decide the
order of a message at a certain slot s only after hearing from
all nodes about the status of slots that precede s. Clearly
this approach is not able to adopt quorums (unlike Paxos),
and it may result in poor performance in case of slow nodes
or unbalanced inter-node delays. To alleviate the problem of
slow nodes, Fast Mencius has been proposed [17]. It uses
a mechanism that enables the fast nodes to revoke the slots
assigned to the slow nodes. However, Fast Mencius still suffers
from high latency in speciﬁc WAN deployments since it does
not rely on quorums for delivering.
EPaxos employs dependency tracking and fast quorums
to deliver non-conﬂicting commands using a fast path. In
addition, its graph-based dependency linearization mechanism
that is adopted to deﬁne the ﬁnal order of execution of com-
mands may easily suffer from complex dependency patterns.
Instead, Alvin [12] avoids the expensive computation on the
dependency graphs enforced by EPaxos via a slot-centric
decision, but it still suffers from the same vulnerability to
conﬂicts of EPaxos: a command’s leader is not able to decide
on a fast path if it observes discordant opinions from a quorum
of nodes. That is not the case of CAESAR, whose fast decision
scheme is optimized to increase the probability of deciding in
two communication delays regardless of discordant feedbacks.
M 2Paxos [14] is a multi-leader consensus implementation
that provides fast decisions while i) adopting only a major-
ity of nodes as quorum size, and ii) avoiding to exchange
dependencies of commands. It does that by embedding an
ownership acquisition phase for commands into the agreement
process, so as to guarantee that a node having the ownership
on a set of commands can autonomously take decisions on
those commands. However, in case there are multiple nodes
that compete for the decision of non-commutative commands,
the protocol might require an expensive ownership acquisition
phase to re-distribute their ownership records.
CAESAR is also related to Clock-RSM [18]. In Clock-RSM,
each node proposes commands piggybacked with its physical
timestamp, which are then deterministically ordered according
to their associated timestamps. Although Clock-RSM is multi-
leader like CAESAR, and it relies on quorums to implement
replication, it suffers from the same drawbacks of Mencius,
namely the need of a conﬁrmation that no other command
with an earlier timestamp has been concurrently proposed.
III. SYSTEM MODEL
We assume a set of nodes Π = {p1, p2, . . . , pN} that
communicate through message passing and do not have access
to either a shared memory or a global clock. Nodes may fail
by crashing but do not behave maliciously. A node that does
not crash is called correct; otherwise, it is faulty. Messages
may experience arbitrarily long (but ﬁnite) delays.
Because of FLP [19], we assume that the system can be en-
hanced with the weakest type of unreliable failure detector [20]
that is necessary to implement a leader election service [21].
In addition, we assume that at least a strict majority of nodes,
+ 1, is correct. We name classic quorum (CQ), or
i.e.,
more simply quorum, any subset of Π with size at least equal
(cid:2)
+ 1. We name fast quorum (FQ) any subset of Π
to
N
2
(cid:3)
(cid:2)
(cid:3)
N
2
50
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:00:16 UTC from IEEE Xplore.  Restrictions apply. 
(cid:9)(cid:10)(cid:8)(cid:9)(cid:8)(cid:11)(cid:5)(cid:13)(cid:1)(cid:4)
(cid:11)(cid:12)(cid:2)(cid:3)(cid:7)(cid:5)(cid:13)(cid:1)(cid:4)
(cid:8)(cid:6)(cid:13)(cid:1)(cid:4)
(cid:8)(cid:6)(cid:13)(cid:1)(cid:4)
(cid:1)(cid:2)
(cid:9)(cid:10)(cid:8)(cid:9)(cid:8)(cid:11)(cid:5)(cid:13)
(cid:1)(cid:2)
(cid:8)(cid:6)(cid:13)
(cid:1)(cid:2)
(cid:8)(cid:6)(cid:13)
(cid:1)(cid:2)
(cid:11)(cid:12)(cid:2)(cid:3)(cid:7)(cid:5)(cid:13)
(cid:7)(cid:1)
(cid:7)(cid:2)
(cid:7)(cid:3)
(cid:7)(cid:4)
(cid:7)(cid:5)
(cid:1)(cid:2)
(cid:6)
(cid:1)(cid:2)
(cid:1)(cid:2)
(cid:6)
(cid:6)
(cid:6)
(cid:6)
(cid:1)(cid:2)
(cid:1)(cid:2)
(cid:7)(cid:1)
(cid:7)(cid:2)
(cid:7)(cid:3)
(cid:7)(cid:4)
(cid:7)(cid:5)
(cid:9)(cid:10)(cid:8)(cid:9)(cid:8)(cid:11)(cid:5)(cid:13)
(cid:4)(cid:14)(cid:17)
(cid:11)(cid:12)(cid:2)(cid:3)(cid:7)(cid:5)(cid:13)
(cid:4)(cid:14)(cid:17)(cid:14)(cid:15)(cid:16)
(cid:8)(cid:6)(cid:13)(cid:1)(cid:4)(cid:14)(cid:15)(cid:16)
(cid:8)(cid:6)(cid:13)(cid:1)(cid:4)(cid:14)(cid:15)(cid:16)
(cid:1)(cid:2)
(cid:8)(cid:6)(cid:13)(cid:1)(cid:1)(cid:1)(cid:14)(cid:15)(cid:16)
(cid:1)(cid:2)
(cid:8)(cid:6)(cid:13)(cid:1)(cid:1)(cid:1)(cid:14)(cid:15)(cid:4)(cid:16)
(cid:1)(cid:2)
(cid:9)(cid:10)(cid:8)(cid:9)(cid:8)(cid:11)(cid:5)(cid:13)
(cid:14)(cid:18)
(cid:1)(cid:2)
(cid:6)
(cid:1)(cid:2)
(cid:1)(cid:2)
(cid:6)
(cid:1)(cid:2)
(cid:1)(cid:2)
(cid:6)
(cid:6)
(cid:6)
(cid:1)(cid:2)