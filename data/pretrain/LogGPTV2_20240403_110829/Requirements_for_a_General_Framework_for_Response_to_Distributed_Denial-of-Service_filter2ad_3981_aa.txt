title:Requirements for a General Framework for Response to Distributed Denial-of-Service
author:D. W. Gresty and
Qi Shi and
Madjid Merabti
Requirements for a General Framework for Response to Distributed Denial-of-
Service 
D.W. Gresty, Q. Shi, M. Merabti  
School of Computing and Mathematical Science, 
Liverpool John Moores University, 
Byrom Street, Liverpool, L3 3AF, UK. 
PI:EMAIL 
Abstract 
incidents  should 
What  is  network  denial  of  service  (DoS),  and  why  is  it 
such  a  problem?  This  research  project  has  sought  to 
investigate  these  questions  and  look  at  the  deeper 
questions such as can denial of service be removed, can it 
be detected and can network systems adequately respond 
to  denial  of  service 
they  become 
subjected to them?   
This  paper  describes  some  issues  that  make  network 
denial  of  service  a  difficult  security  problem,  and 
discusses  some  solutions  that  have  been  provided  by  the 
security  research  community.  The  paper  then  provides  a 
classification of denial of service, the Consumer problem 
and  the  Producer  problem,  which  forms  the  bulk  of 
modern  network  denial  of  service  incidents.  Finally  the 
paper  proposes  requirements  for  a  framework  for  the 
management  of  response  to  network  denial  of  service 
incidents,  and  suggests  future  directions  that  are  being 
developed to create the framework.    
1. Introduction and Background 
In  February  2000  [2,16,21]  the  Internet  was  subject  to  a 
mass  distributed  co-operative  attack  incident  known  as  a 
Distributed  Denial  of  Service.  This  incident  brought  a 
stark  reality  to  the  Internet  E-Commerce  community  as 
small  hosts  attacked 
large  allegedly  well-protected 
systems.  As the scale of the incident was unprecedented 
within  the  computer  security  literature  and  beyond  the 
experience  of  the  majority  of  the  network  entities 
involved,  the  response  to  this  situation  was  ad  hoc  and 
demonstrated a clear lack of preparation. For the network 
entities  involved  there  were  very  different  operating 
policies  or  even  radically  different  operating  systems  to 
the other entities that were victims during the incident.  
In the joint CSI/FBI paper outlining the issues and trends 
in  computer  crime  [25-26],  it  is  shown  that  denials  of 
service  incidents  within  computer  systems  are  a  real 
problem. This problem is  on a steady increase with 25% 
of the companies surveyed in 1999 reporting that they had 
(e.g.  data 
the  system.  Within 
suffered  from  denial  of  service  and  27%  in  2000. 
Research  from  experts  such  as  Cohen  [5]  predicted  that 
distributed  co-operative  attack  (DCA)  incidents  would 
become  more  and  more  common  in  the  future,  and  this 
prediction has to date, held true. 
Denial  of  service  is  essentially  the  problem  of  an  entity 
within  a  system  (e.g.  a  user),  preventing  authorised 
entities  (e.g.  other  users  or  programs)  having  access  to 
resources 
files,  programs  or  network 
connections)  held  within 
the 
conventional  model  of  security,  denial  of  service  is 
considered to be an availability problem. Availability is in 
fact referred to as “Denial of Service” in [1]. This means 
that  secured  assets  are  considered  a  service,  and  another 
asset  or  party  denies  access  to  the  service.  However 
services  may  also  be  denied  if  a  party  calls  for  an  asset 
and finds that an  unauthorised modification  has occurred 
to the asset. If the party is unable upon subsequent calls to 
acquire  an  un-compromised  asset  then  it  has  also  been 
subject to a denial of service. This is an integrity problem, 
and therefore the denial of service problem is clearly not 
just  availability  of  a  service,  but  also  the  accuracy  and 
integrity of that service. 
As could be expected with this problem, there have been 
several  solutions  proposed  to  secure  the  reliability  of 
computer  systems  and  combat  the  denial  of  service 
problem specifically. Unfortunately there are no solutions 
that can guarantee 100% service and security as stated by 
Pfleeger  in  [23]  and  Loscocco  in  [19]  as  the  problem 
domain is too wide and the system infrastructures used are 
not able to guarantee service. Even with modern network 
infrastructures  using  technology  such  as  Asynchronous 
Transfer  Mode  (ATM)  it  is  currently  impossible  to 
guarantee  service  despite  the  much  stronger  level  of 
authentication  and  technological  reliability.  From  this 
evidence  it  can  be  argued  that  denial  of  service  is  an 
intractable problem.  
Unfortunately denial of service is not a static problem, as 
it  has  grown  and  evolved  with 
the  growth  and 
development of computer networks. Distributed denial of 
service  (DDoS)  is  the  manifestation  of  this  problem  in 
distributed  systems.  With 
the  growth  of  modern 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:09:37 UTC from IEEE Xplore.  Restrictions apply. 
1 
to  such 
distributed systems impacting upon the lives of millions of 
people,  this  kind  of  network  problem  needs  to  be 
removed. This research project suggests requirements for 
response  to  this  problem.  From  these  requirements  a 
general  framework  could  be  developed  to  ‘combat’  the 
denial  of  service  problem – a  problem that currently  has 
no significantly credible solutions. 
In  section  two  of  this  paper,  some  of  the  problems  that 
make  the  denial  of  service  problem  so  difficult  are 
described.  Section  three  presents  a  discussion  of  the 
different types of denial of service incident and classifies 
this into two separate problems. Section four proposes the 
research  issues  for  development  of  a  framework  for 
managing  incidents  and  the  components  that  a  general 
framework  for  responding 
incidents  would 
require. The paper concludes with a summary and future 
work  to  be  performed  within  this  research  project  in 
section five.  
2. Literature Review 
This  section  presents  a  review  of  technical  and  non-
technical  issues  that  make  the  denial  of  service  problem 
particularly difficult to address. 
Distributed Co-ordinated Attack Incidents 
Cohen  discussed  the  problems  of  the  Distributed  Co-
ordinated  Attacks  (DCAs)  in  [5]  and  identified  perhaps 
the  most  important  issue  concerning  denial  of  service 
incidents on the Internet: The primary problem associated 
with  DCAs  is  trust;  The  network  infrastructures  are 
untrustworthy;  and  even  when  the  technology  can  be 
trusted the human element can never be trusted implicitly. 
If  an  attack  trace  passes  through  several  administrative 
systems,  then  the  lack  of  just  one  of  the  systems  in  co-
operation can greatly confound the tracing task.  
Cohen  addresses  the  point  that  one  person  need  not 
perpetrate this form of attack. This paper was written prior 
to  the  distributed  denial  of  service  ‘toolkits’.  The  author 
makes  much  of  the  painstaking  problem  of  tracing  the 
attack back to the source, but doesn’t suggest what kind of 
response  is  suitable  other  than  strong  filtering,  i.e. 
switching off all possible problem transactions.  
Distributed Denial of Service Incidents 
In  the  late  1999’s  the  Distributed  Denial  of  Service 
(DDoS) was highlighted as being of immediate concern to 
the  network  security  professionals  by  organisations  such 
as  ‘Internet  Security  Systems’  after  they  had  analysed 
freely  available  programs  that  had  been  gained  from  the 
Internet.  This  attack  mitigates  the  need  for  complex  co-
ordination  between attackers  as would have  been  needed 
under Cohen’s DCAs, which was arguably the reason that 
the February 2000 incident [2] did not occur sooner. This 
is because as few as one attacker can distribute and direct 
incidents  and  prevent 
the attack.  The attacker accomplishes this  by  penetrating 
systems  with  ‘Trojan  Horse’  programs  (see  [23]  for  a 
description) that are very careful as Geng [24] mentions to 
do  nothing  to  damage  the  software  or  hardware  on  the 
penetrated  systems.  At  a  predetermined  time  or  after  a 
signal  has  been  sent  by  the  owner  of  the  so-called 
‘Zombie’  machines,  the  penetrated  systems  will  launch 
large volumes of traffic at the designated target.  
Solutions proposed in the paper by Geng et al. suggest that 
creating  an  economic  and  technical  expense  for  the 
‘Zombie’  systems  would  provide the  necessary incentive 
for  system  administrators  to  toughen  up  security  to 
identify  when  the  systems  are  being  used  to  perpetrate 
hostile 
technical 
suggestions  are  introducing  small  problems  such  as 
solving a mathematical problem before being permitted to 
make  a  connection,  or  having  an  egress-limiting 
bottleneck so only a limited number of transactions can be 
sent  from  the  system.  Non-technical  solutions  include  a 
pricing  structure  such  that  hosts  are  charged  for  sending 
transactions like an electronic postal service. This solution 
the  authors  suggest  would  provide  the  incentive  for 
system owners and ISPs to monitor for hostile behaviour 
to reduce the extra unnecessary network costs.    
Ingress/Egress filtering 
The  Internet  Society  suggested  in  [11]  that  network 
ingress  filtering  was  a  valid  way  to  remove  spoofed 
addresses  from  entering  the  network.  This  technique  is 
implemented  by  an  ‘upstream’  service  provider 
to 
compromised  hosts,  and  as  the  compromised  hosts  send 
information  through  the  provider,  the  provider  removes 
transaction packets that are obviously going to be used in 
an  incident  through  the  use  of  illegal  addresses.  This 
approach represented an attempt at  self-regulation  by the 
Internet community, rather than the economic restrictions 
as  suggested  in  [24].  CERT  still  recommends  the  advice 
contained  in  this  request  for  comment  in  its  advisory  on 
the developments of denial of service [27], however it is 
very careful to point out that this will reduce spoofing, not 
eliminate distributed denial of service. This is because the 
denial of service tools that have been analysed by Dettrich 
in  [6-8]  do  not  need  to  use  spoofed  addresses  to  attack 
their targets. Therefore this approach although considered 
the  older  style  attacks,  has  been 
effective  against 
superseded  by 
in  denial  of  service 
‘technology’.  
Network security group UC Davis 
The work [3,4] that this group has concentrated on is the 
protection  of  network  routers  from  denial  of  service 
incidents. The basic premise is that existing protocols are 
not equipped to deal with denial of service attacks. One of 
the most important aspects to come out of this work is the 
identification  of  co-operation  within 
the  network 
environment.  This  work  identifies  that  attackers  can 
them.  The 
the  advances 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:09:37 UTC from IEEE Xplore.  Restrictions apply. 
2 
to 
fraudulent  messages 
to  use  a  co-operative 
collude to hide evidence of the attack. Colluding attackers 
may even be able to discredit innocent network parties by 
sending 
intrusion  detection 
systems (IDS) on the network. As a response to this, the 
group  has  decided 
intrusion 
detection model to effectively co-operate in the detection 
of what the papers refers to as "misbehaving routers". This 
work  was  very  important  as  it  highlighted  that  a  stand-
alone  system  could  not  adequately  detect  co-ordinated 
attacks  and  therefore  must  co-operate  to  stand  a  fair 
chance of detection.  
There are a couple of points within this work which need 
noting.  The  WATCHERS  protocol  developed 
to 
implement  these  ideas  follows  the  US  aphorism  "It's  my 
way, or the highway". That is to say all participants must 
adhere strictly to the protocol, or risk ‘banishment’. This 
principle  would  be  perfectly  acceptable  for  a  single  type 
of protocol. However this technique is quite obviously not 
going  to  be  acceptable  to  every  party  on  a  network  for 
general transactions, as such it would be more suitable for 
a  restricted  environment  (such  as  the  control  of  routing 
protocols, as it was originally intended).  
Summary 
The  primary  issue  for  the  Internet  is  trust,  or  more 