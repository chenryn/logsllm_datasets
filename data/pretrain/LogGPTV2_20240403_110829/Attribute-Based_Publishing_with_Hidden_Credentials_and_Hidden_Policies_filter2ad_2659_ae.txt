2004.
[14] C. Dwork, M. Naor, and A. Sahai. Concurrent zero-
knowledge. J. ACM, 51(6):851–898, 2004.
[15] K. Frikken, J. Li, and M. Atallah. Trust negotiation with
hidden credentials, hidden policies, and policy cycles.
In
13th Annual Network and Distributed System Security Sym-
posium, pages 157–172, feb 2006.
[16] P. Golle, M. Jakobsson, A. Juels, and P. F. Syverson. Uni-
versal re-encryption for mixnets. In T. Okamoto, editor, CT-
RSA, volume 2964 of Lecture Notes in Computer Science,
pages 163–178. Springer, 2004.
In CCS ’06: Proceedings of
[17] V. Goyal, O. Pandey, A. Sahai, and B. Waters. Attribute-
based encryption for ﬁne-grained access control of en-
the 13th
crypted data.
ACM conference on Computer and communications secu-
rity, pages 89–98, New York, NY, USA, 2006. ACM Press.
[18] J. Groth. A veriﬁable secret shufﬂe of homomorphic en-
cryptions. In Y. Desmedt, editor, Public Key Cryptography,
volume 2567 of Lecture Notes in Computer Science, pages
145–160. Springer, 2003.
[19] M. Hirt and K. Sako. Efﬁcient receipt-free voting based
on homomorphic encryption. In EUROCRYPT, pages 539–
556, 2000.
[20] J. E. Holt, R. W. Bradshaw, K. E. Seamons, and H. Orman.
Hidden credentials. In 2nd ACM Workshop on Privacy in the
Electronic Society, Washington, DC, pages 1–8, oct 2003.
[21] A. Ivan and Y. Dodis. Proxy cryptography revisited.
In
NDSS. The Internet Society, 2003.
[22] J. Katz. Efﬁcient and non-malleable proofs of plaintext
In E. Biham, editor, EURO-
knowledge and applications.
CRYPT, volume 2656 of Lecture Notes in Computer Sci-
ence, pages 211–228. Springer, 2003.
[23] H. Khurana, A. J. Slagell, and R. Bonilla. Sels: a secure e-
mail list service. In H. Haddad, L. M. Liebrock, A. Omicini,
and R. L. Wainwright, editors, SAC, pages 306–313. ACM,
2005.
[24] J. Li and N. Li. Policy-hiding access control in open envi-
ronment. In Proceedings of ACM Symposium on Principles
of Distributed Computing (PODC), pages 29–38, jul 2005.
[25] H. Lipmaa. Veriﬁable homomorphic oblivious transfer and
In C.-S. Laih, editor, ASIACRYPT,
private equality test.
volume 2894 of Lecture Notes in Computer Science, pages
416–433. Springer, 2003.
[26] M. Naor. Deniable ring authentication. In CRYPTO 2002,
volume 2442 of LNCS, pages 481–498. Springer-Verlag,
2002.
[27] P. Paillier. Public-key cryptosystems based on composite
In EUROCRYPT, pages 223–
degree residuosity classes.
238, 1999.
[28] G. Poupard and J. Stern. Fair encryption of rsa keys.
In
EUROCRYPT, pages 172–189, 2000.
[29] M. D. Raimondo and R. Gennaro. New approaches for deni-
able authentication. In V. Atluri, C. Meadows, and A. Juels,
editors, ACM Conference on Computer and Communica-
tions Security, pages 112–121. ACM, 2005.
[30] R. L. Rivest, L. Adleman, and M. L. Dertouzos. On
data banks and privacy homomorphisms.
In R. DeMillo,
D. Dobkin, A. Jones, and R. Lipton, editors, Foundations
of Secure Computation, pages 169–180. Academic Press,
1978.
[31] A. Sahai and B. Waters. Fuzzy identity based encryption. in
advances. In Advances in Cryptology – Eurocrypt volume
3494 of LNCS, pages 457–473, 2005.
[32] SunMicrosystems.
Sun ﬁre T1000 and T2000 Servers
http://www.sun.com/
2006.
Benchmarks,
servers/coolthreads/t1000/benchmarks.
jsp#j.
[33] SunMicrosystems. Sun Fire T2000 Server, 2006. http://
www.sun.com/servers/coolthreads/t2000/.
[34] K. Suzuki and M. Yokoo. Secure generalized vickrey auc-
tion using homomorphic encryption. In R. N. Wright, editor,
Financial Cryptography, volume 2742 of Lecture Notes in
Computer Science, pages 239–249. Springer, 2003.
[35] W. H. Winsborough and N. Li. Towards practical automated
In Proceedings of the IEEE 3rd Inter-
trust negotiation.
national Workshop on Policies for Distributed Systems and
Networks, pages 92–103, June 2002.
[36] W. H. Winsborough and N. Li. Safety in automated trust
negotiation. In Proceedings of the 2004 IEEE Symposium on
Security and Privacy, pages 147–160, Oakland, CA, May
2004. IEEE Press.
[37] W. H. Winsborough, K. E. Seamons, and V. E. Jones. Auto-
mated trust negotiation. In DARPA Information Survivabil-
ity Conference and Exposition (DISCEX), pages 88–102,
Jan. 2000.
[38] T. Yu and M. Winslett. A uniﬁed scheme for resource pro-
tection in automated trust negotiation. In Proceedings of the
IEEE Symposium on Security and privacy, pages 110–122,
May 2003.
[39] T. Yu, M. Winslett, and K. E. Seamons. Supporting struc-
tured credentials and sensitive policies through interopera-
ble strategies for automated trust negotiation. ACM Trans.
Inf. Syst. Secur., 6(1):1–42, 2003.
A. Security Proofs
A.1. Lemmas
We need the following lemmas for
the proof of
Full-PEAPOD’s conﬁdentiality and policy privacy. The
implication of Lemma 1 with respect to Full-PEAPOD is
that if a randomness is added to the set of sub-keys of the
symmetric key and then the whole set is blinded, the knowl-
edge of such a set is useless in recovering the symmetric
key. The lemma can easily be generalized so that there
could be more than one randomness and they can be at ar-
bitrary positions. Lemma 2 says the blinding operation on
a tuple retains only the product of the entries, but individual
blinded values reveal nothing about the underlying values.
Recall that in Full-PEAPOD, the plaintext behind each of
the Elgamal ciphertexts is either a sub-key, a random value
or the identity element. With this lemma, these three types
of values are no longer distinguishable after blinding.
p, the random
i=1bi)−1r),
p, and R = (r1 r2, . . ., rm,
p, have the same dis-
We write a $← S to mean that a is drawn from set S
using fresh coin-ﬂips according to a uniform distribution.
Lemma 1 ∀m ∈ N, ∀x1, x2, . . . , xm ∈ Z∗
variables X = (b1x1, b2x2, . . ., bmxm, (Πm
where b1, b2, . . . , bm, r $← Z∗
rm+1), where r1, r2, . . . , rm+1
tribution.
Lemma 2 ∀m ∈ N, ∀x1, x2, . . . , xm+1 ∈ Z∗
variables X = (b1x1, b2x2, . . ., bmxm, (Πm
where b1, b2, . . . , bm
i=1 xi · (Πm
Πm+1
the same distribution.
p, the random
i=1bi)−1xm+1),
p, and R = (r1, r2, . . ., rm,
p, have
i=1)−1ri), where r1, r2, . . . , rm
$← Z∗
$← Z∗
$← Z∗
The proofs for both lemmas are straightforward and are
thus omitted.
A.2. Proofs
Proof 1 (Theorem 1) (Sketch.) Recall that a ciphertext re-
turned by the encryption algorithm executed by a sender
in Full-PEAPOD consists of the symmetric encryption ψ
of the plaintext message M under a randomly chosen key
k and an array of Elgamal encryption of either a sub-key,
a random value or the identity element. The Elgamal ci-
phertext at the i-th row and j-th column in the array is an
encryption under the public key of attribute j. The under-
lying plaintext is a sub-key, a random value or the identity
element if the j-th attribute is respectively a required, for-
bidden or irrelevant attribute for the i-th clause in the policy.
Now, assume there exists a PPT adversary A who can
tell which message among M0 and M1 was used in the en-
cryption that resulted in the challenge ciphertext C∗ with
probability non-negligibly greater than 1/2. The semantic
security of the symmetric encryption implies that A knows
the symmetric key k. The knowledge of k implies that there
is at least one clause in the challenge policy P ∗ such that
A knows the product of all the sub-keys for that clause. We
claim that this implies that A knows all the individual sub-
keys in the product for that clause. Such knowledge means
that A knows the set K∗ of Elgamal decryption keys to the
encryption of all those sub-keys, as otherwise Elgamal en-
cryption would not be IND-CPA secure, contradicting to
the assumption that the DDH problem is hard in Z∗
p.
We need to consider both cases when A corrupted the
Server and when A did not. In the former case, the model
requires that A did not corrupt any of the users. The Server
itself does not know any of the Elgamal decryption keys.
The Server knows all the transformation secret keys, but
without the help of the CA or any user in the system, the
transformation secret keys have statistical zero-knowledge
on any of the Elgamal decryption keys. This contradicts to
the fact that A knows K∗.
In the latter case, A might have corrupted some sub-
set of users. However, Restriction 10 guarantees that any
coalition of corrupted users does not collectively satisfy the
challenge policy P ∗, which means that for any union of cor-
rupted users’ attribute set and any clause in P ∗, the union
does not satisfy the clause, which is so either because at
least one required attribute is missing from the union, or
one forbidden attribute is present in the union. The former
possibility contradicts to the fact that A knows K∗. The
latter possibility also contradicts to the very same fact, due
to Lemma 1.
Therefore, there does not exist a PPT adversary A who
can win the game with probability non-negligibly greater
than 1/2. Finally, we note that the above is only a proof
sketch. In a full proof, one would have to simulate the de-
posit and retrieval queries, which could be achieved by fol-
lowing the protocol speciﬁcation. The contradiction could
be derived by embedding a DDH problem instance into one
of the array entries of Elgamal ciphertexts in the challenge
ciphertext. Such a challenge ciphertext can be simulated
correctly without the knowledge of sub-key behind that par-
(cid:3)
ticular Elgamal ciphertext.
dom shufﬂing executed at the encryption step, two polices
with the same number of satisfying clauses have the same
statistical distribution of the plaintext products. Therefore,
the two challenge policies are indistinguishable to A and
thus A could not have won the game with probability non-
negligibly greater than 1/2 in this case.
Combining all cases leads to the conclusion that there
does not exist a PPT adversary A who can win the game
(cid:3)
with probability non-negligibly greater than 1/2.
As
Proof 3 (Theorem 3) (Sketch.)
discussed,
Full-PEAPOD is an ofﬂine system in which a sender
only interacts once with the Server during a deposit and
the receiver only interacts once with the Server during
a retrieval. Receivers never contact with senders during
retrieval or decryption.
The transcripts for retrieval
protocol runs consist of retrieved ciphertexts. The size
of the ciphertext retrieved by any user is independent of
the attribute set possessed by the user. Furthermore, by
arguments similar to the proof sketch for policy privacy,
the IND-CPA security of Elgamal encryption implies that
the sender who does not have any credentials to decrypt
a retrieved ciphertext is unable to learn non-negligible
information from the Elgamal ciphertexts about
their
(cid:3)
underlying plaintexts. The result follows.
Proof 2 (Theorem 2) (Sketch.) Observe that during en-
cryption, the policy affects only the array of plaintexts that
are to be Elgamal-encrypted. The symmetric encryption of
the message is totally independent of the policy. To prove
that Full-PEAPOD has Clausal Policy Indistinguishabil-
ity, it sufﬁces to show the two ciphertexts resulted from
any message encrypted with two different policies with the
same number of satisfying clauses with respect to an adver-
sary are statistically indistinguishable, thereby leaving the
adversary no chance in making a correct guess on which
policy was used any better than pure guessing, i.e. it can’t
win with probability non-negligibly greater than 1/2.
Assume there exists a PPT adversary A who can win
the game with probability non-negligibly greater than 1/2.
Let’s ﬁrst consider the possibility that A corrupted the
In such a case, A did not cor-
Server during the game.
rupt any of the users or otherwise could not have won the
game. Since the server does not possess the decryption keys
of any of the attributes, all the Elgamal ciphertexts within a
deposited ciphertext have computational zero knowledge in
their underlying plaintexts, the knowledge of which is re-
quired to tell whether an attribute is required, forbidden or
irrelevant for each clause. Thus, a ciphertext deposited at
the server has zero knowledge in the policy under which it
was created, meaning that an adversary who corrupted the
server in order to attack policy privacy of Full-PEAPOD
cannot do any better then pure guessing which one of the
two challenge policies was used.
Therefore, A must have won without corrupting the
Server, in which case A might have corrupted up to one
honest user. If A did not corrupt any user, then by argu-
ments similar to the above, a ciphertext deposited at the
server has zero knowledge in the policy under which it was
created and thus A could only win with probability negli-
gibly greater than 1/2. The only possibility left is thus that
A corrupted exactly one honest user. Note that the game
speciﬁcation implies that this single corrupted user satisﬁes
the same number of clauses in both challenge policies. The
following derives a contradiction.
The ciphertexts the adversary retrieves from the server
have all gone through the blinding step through the homo-
morphic operation on the individual Elgamal ciphertexts.
Lemma 2 implies that the decryption of any of these Elga-
mal ciphertexts could have been a sub-key, a random value
or the identity element before blinding and the adversary
has statistically zero knowledge to tell which is the case.
This means any attribute in any clause in the policy could
be a X-attribute, ×-attribute, or ∗-attribute. The only in-
formation a retrieved ciphertext carry is thus the product of
the plaintexts for each clause, which is either equal to the
symmetric key if the clause is satisﬁed by the adversary, or
a group element distributed over Z∗
p uniformly at random if
the clause is not satisﬁed by the adversary or if it is a bogus
one, due to Lemma 1. Finally, due to the uniformly ran-