（图1手淘首页） （图2手淘主会场-行业会场）
新智能  9年双11：互联网技术超级工程
（ 智能清单1） （ 智能清单2）
为了验证智能写手生成的清单的效果，我们在双十一期间小流量上线，和人工编
辑的清单进行了分桶测试。对比人工编辑的单品盘点清单，智能写手清单在平均商品
点击转化率上的表现要更好。
下面我们将分别介绍智能写手在智能利益点和图文清单生成两部分的工作。
二、智能利益点
智能利益点解决的问题是，给定任意一个商品，挖掘这个商品各个潜在的卖点，
并根据挖掘出来的用户偏好，从商品卖点集合中圈定用户最感兴趣、最可能点击的卖
点，然后基于这些卖点实时生成一小段6个字以内的文案。利益点生成的解决方案主
要分为这么几部分：
1. 用户的偏好挖掘：主要基于用户的离线和实时行为数据来做，通过挖掘得到用户
的TOP K个偏好标签集合。由于线上系统性能的限制，我们不可能使用用户所
有行为过的标签，于是我们构建了用户偏好标签的排序模型对标签进行优选。
新智能  9年双11：互联网技术超级工程
模型包含几个主要的部分：
1. 用户偏好和商品卖点的语义表示：由于用户的偏好标签量比较大，如何对用户
的大量偏好标签进行更深层次的偏好挖掘，是这个部分要解决的重点问题。
2. Multi-level的相似度模块：在不同的语义层级上计算用户偏好和商品卖点的
相似度。
3. Additional Features：引入人工定义的额外的特征，辅助模型效果。例如用
户偏好的特征、卖点的统计类特征、用户偏好和卖点的overlap特征等。
整个PairXNN模型的训练和在线预测是基于我们内部自研的XTensorflow平
台进行搭建。
2.2 语义表示
在对用户侧的偏好标签做语义抽取的时候，考虑到用户偏好标签的特殊性，它
不是一个真正的有合理语义意义的句子，因此我们尝试了多种不同的语义表示的网络
结构，包括全连接DNN、和[1]一样的CNN、Gated CNN[3]、self-attention[2] 和
tailored attention。
其中，Gated CNN是对传统的CNN结构做了优化，加入了gate机制，可以
决定哪些信息更为重要，需要保留或者说舍去哪些信息。而采用Self-attention则是
考虑到对于用户的偏好标签序列，需要更关注全局的语义相关性。tailored attention
则是我们为了优化性能，简化语义表示网络所提出的新结构，因为智能利益点的场景
都是重要场景，流量很大，对性能要求比较高。最终经过双十一期间的线上分桶测
试，Gated CNN在网络性能和效果上综合最优，于是双十一全量上线的模型中采用
Gated CNN的语义表示网络结构。
2.3 Multi-level相似度模块
除了上述对于user和item侧信息的映射和抽取，为了计算用户和利益点的相关
性，我们从两个不同的语义层次对用户偏好标签和商品卖点的相似度计算，分别是：
1. 对用户偏好标签embedding层输出和商品卖点embedding层输出的
cosine similarity计算。
新智能  9年双11：互联网技术超级工程
1. 主题。这个主题可以由外界（运营）输入，也可以基于我们主题发现所沉淀的
主题库进行选择。
2. 选品。确定了主题之后，我们根据这个主题从精品库中选取和主题相关性高
且质量不错的商品，然后以一定的目标组合成一个个的清单（一般一个清单包
含6-10个商品）。
3. 商品推荐理由生成。为每个清单的商品生成一段40-80个字的推荐理由。
4. 标题生成。根据清单内的商品信息，给清单取一个概括主题又吸引用户点击
的标题。清单标题要求相对简短，一般不长于20个字。例如：“懒人沙发椅，
沉溺初秋慵懒美时光”。
3.1 Deep Generation Network
图文型清单生成中的两个模块，商品推荐理由的生成和标题生成，我们把他们归
类为自然语言生成（NLG）问题，都可以定义为依赖输入信息的文本生成问题。其中，
商品推荐理由生成问题中，输入的是商品的信息，而清单标题中输入的是商品集合的
信息。于是，我们采用了最近比较流行的Encoder-Decoder深度神经网络模型框
架来解决，基于Attention based Seq2Seq[5-6]的base model，最终形成了我们
的Deep Generation Network。
新智能  9年双11：互联网技术超级工程
其中a 是attention权重，t 是时间i的decoder的state，h 是encoder的
i,j i j
state。
加入coverage model之后的attention权重计算如下：
其中相对于原始的attention model，多了coverage的部分。这个部分如下计算：
其中Φ 是Fertility的概念，可以理解成一个source一般会被映射成多少个词
j
的期望。
3.1.3 context gate[9]
在推荐理由的输出当中，模型的主体是基于RNN的seq2seq架构，那么在
decoder的输出端的输出，主要受2部分影响：
1. 一部分是encoder的输入
2. 另一部分是当前step的前一个step的输出。
那么对于不同的输出，2部分的影响应该是不同的，比如说，当前一个输入词是
虚词时，主要的信息应该由encoder影响，但是如果前一个词和当前词明显有相关
性时，当前词的主要应该由前一个词影响。所以，我们考虑加入context gate,对这
种情况进行建模。
公式如下：
其中s 是source的信息embedding之后的输出，t 是前一步的decoding的
i i−1
隐层state， y 是前一步的输出词。We(y )+Ut 是decode的前一步输入，Cs
i−1 i−1 i−1 i
新智能 < 95
是source，利用z 来决定下一个输出和那一部分关系比较大。
i
3.1.4 Beam Search
在前文中提到用RNN生成语句时，在每个时刻取输出结果中概率最大的词作为
生成的词，是比较greedy的做法，没有考虑词的组合因素，因此，我们在seq2seq
的实验中也尝试了beam search。beam search只在predict的时候使用，举个
例子，当beam search size=2时，每个时刻都会保留当前概率最大的两个序列。
beam search在实践过程中很有用，它提供了一种很好的对生成序列进行干预的基
础，一方面你可以对beam search的候选集的选择以及最终序列的选择做定制化的
处理，比如你的选择目标，另一方面，对一些模型还不能完全保证解决的bad case
（例如重复词出现等），可以在beam search中进行处理。
3.1.5 CNN
对于我们生成清单标题的问题，由于输入是多个商品的文本内容，商品文本之间
并没有真正的序列关系，反而更需要一个类似主题特征抽取的部分，从而能根据主题
进行标题的生成。而CNN在句子分类已经有不错的应用[7]了，于是我们在清单标题
生成问题中，采用了CNN作为Encoder，实验结果也表明CNN比LSTM在标题
生成的主题准确率上要高。
3.1.6 Reinforcement Learning
我们在训练和预测的时候会碰到下面2个问题: