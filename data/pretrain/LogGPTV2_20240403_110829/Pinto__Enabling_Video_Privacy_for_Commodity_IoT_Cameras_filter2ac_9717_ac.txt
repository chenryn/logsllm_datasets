certified by trusted timestamping server S whose public key K +
is
S
known.
4.4 Dealing with Generation Loss
One key design requirement is that Pinto’s sequential, three
h_pixelating operations—the realtime, the post processing, and the
verification—must be applied to identical frames. However, genera-
tion loss—the loss of quality when using lossy compression—would
cause inconsistency between realtime frames and their processed
versions due to video encoding. Such inconsistency will nullify the
signed p_digests generated with realtime frames.
One straightforward approach to handling generation loss is to
keep videos uncompressed. However, it significantly increases the
file size (e.g., 550 Mbytes for a 1-min video) that is too costly for
continuous recording of surveillance videos. We devise a storage-
efficient way to address the inconsistency problem.
4.4.1 Initial Compression for Video Recording
We arrange an initial conversion that is equivalently applied to the
realtime and the post processing procedures to offset generation
loss. More specifically, each realtime frame captured from camera
module is first encoded into a JPEG image. This JPEG-encoded frame
and its decoded version are fed into Paths 1 and 2 respectively (Fig.
6a), as described in Section 4.3. As a result, original videos are stored
as Motion JPEG (MJPEG) format.
This MJPEG-based initial encoding is chosen for the following
reasons: (i) frames processed for the realtime and the post pixelation
are made identical while having original videos lossily compressed;
(ii) MJPEG is not computationally intensive and is now widely-used
by video recording devices like IP cameras; and (iii) it is simple
to implement, and most commodity cameras today already sup-
port built-in functionality to output JPEG-encoded images directly
during the recording process.
4.4.2 MJPEG-Compliant Coding for Video Sharing
To handle “second-order” generation loss, we use MJPEG-compliant
compression for release of p_videos. We specifically leverage the
JPEG frame structure (Figure 7), where an image is stored as a
series of compressed, 8×8-pixel image tiles, called MCUs (minimum
compression units) and each MCU is processed separately. We
make our Pinto-blocks aligned with the JPEG-MCUs (i.e., each
block covers multiple MCUs exactly), applying post-h_pixelation
selectively to only some MCUs while preserving the other MCUs
intact. Pinto produces a p_video in MJPEG format where original,
compressed MCUs on non-critical blocks are retained, but post-
h_pixelated, critical blocks are separately contained with lossless
compression in their frames. Given a p_video, the requester can
easily reconstruct the identical frames by using its p_profile. We
develop a simple decoder for this purpose.
The resulting size of a p_video becomes slightly larger than that
of the stored, original version. We, however, point out that the
compression ratio is still high because the majority of JPEG-MCUs
are generally on non-critical blocks, and thus remain intact. Our
measurement shows that a 50-Mbyte original video turns into a
52-Mbyte p_video on average (cf. 550 Mbytes for a 1-min, uncom-
pressed video). We also note that p_videos are only used for video
sharing, not for storing. Requesters can, once verifying p_videos,
further shrink them back to the original size and store them in
various formats.
4.5 Design Decisions
There are two key design factors in Pinto.
It affects the visual privacy and perceived
Pixelation intensity.
frame quality. As an extreme example, obscuring with pixels of a
constant color will provide complete privacy, but such masking
results in more perceptually-jarring frames. The intenser the pix-
elation (i.e., overly-blurred videos), the poorer (/the stronger) the
Preservation of  original, compressed MCUs  on non-critical blocks JPEG frame: tiles of MCUs (each with 8x8 pixels) Huffman-coded data about  positions and decoding information  for variable-length MCUs  Header Compressed image data MCUs  (minimum compression units) Meta data Huffman Table Lossless compression of post-h_pixelated  critical blocks MJPEG-encoded video Replacement of MCUs on critical blocks p_video  Session 6A: IoTCCS’18, October 15-19, 2018, Toronto, ON, Canada1094(a) License plates
(b) Human faces
(c) Address signs
Figure 8: [Deep-learning] Recognition success ratio (R-ratio) vs. pixelation scale (P-scale).
human-perceived video quality (/privacy protection). Ideally, pixela-
tion intensity should be as minimal as possible while de-identifying
sensitive objects. Note, the realtime pixelation of entire frames (for
p_digests) and the post-pixelation of sensitive objects (for p_videos)
must be the same scale to enable the verification process, i.e., one
common pixelation scale for Pinto.
It controls the processing speed and
In-frame block count.
video quality. The fewer the block count per frame, the faster the re-
altime processing, hence the high-frame-rate videos. This is because
the h_pixelating operation, albeit lightweight, is independently ap-
plied to individual blocks. However, such coarse-grained block
division, i.e., large-sized blocks, results in overly-pixelated p_videos
regardless of the actual sizes of sensitive objects in their frames.
The in-frame block count should be determined for fine-grained
block division while ensuring fast processing speed for producing
high-frame-rate videos.
We carefully choose them by balancing their trade-offs via ex-
tensive experiments as detailed in Section 6.
5 IMPLEMENTATION
Pinto is implemented in 1.1K lines of Python and C++ code. We use
OpenCV [13] that is a cross-platform, open-source library for vision
and image processing. Here we briefly describe some key functions
of OpenCV that we use for a platform-independent implementation
of Pinto based on the design detailed in Section 4.
For each realtime JPEG-encoded output from camera module,
the imdecode() function is called to decode it into an image frame.
The frame is divided into the predefined number of equal-sized
blocks, each of which is contained and processed in the ndarray
format (n-dimensional array) as universal data structure in OpenCV
for images. Block-wise h_pixelation is done using: (i) the update()
function in hashlib library for fast hash calculation; and (ii) the
resize() function whose scaling parameter determines the pixela-
tion intensity. Per-frame hashes (and their eventual p_digest) are
also obtained by the update() function. In the post-processing, we
use our cblock() function to identify critical blocks in each frame.
Existing vision algorithms locate objects in an image and returns the
coordinates of each enclosing rectangular area. Taking this as input,
the cblock() function outputs the indices of critical blocks that
overlap with the detected object areas. For p_video en-/decoding,
Figure 9: [OCR-based] Recognition success ratio.
we use the imencode() function that supports MJPEG-based con-
versions, and also use H.264 provided by libx264 library for critical
blocks. In the verification, the array_equal() function is used for
the p_digest comparison.
There are also device-specific aspects in implementing Pinto,
such as camera modules and network interfaces. We implement
Pinto in three different embedded platforms and these aspects will
be discussed in Section 6.3. All our source code is available at
https://github.com/inclincs/pinto.
6 EVALUATION
We answer four questions about Pinto in this section:
• How much visual privacy does it provide?
• How well does it prevent against content forgery?
• How much video frame rate does it achieve when applied to
• How does in-frame block count affect human perceived video
low-end devices?
quality in real-world applications?
6.1 Protection of Visual Privacy
The key to protecting visual privacy in
Scale of pixelation.
Pinto is how to set the pixelation intensity. We call it a P-scale,
which signifies the degree of lowering the original resolution. For
example, P-scale X corresponds to scaling down the resolution by
2, i.e., reducing the number of distinct pixel values
a factor of X
2 pixel values with
in an image by replacing a square block of X
their averaged value. This process is non-invertible—impossible to
restore from pixelated images to the original ones—and has been
shown to successfully thwart “human” recognition [37, 53].
!"!#$"!#%"!#&"!#’"!#("!#)"!#*"!#+"!#,"$"$"&"("*","$$"$&"$("$*"$,"-./0123402"56//.77"-840"!"#$%&’("29(!!!""29$!!!!""29$(!!!"!"!#$"!#%"!#&"!#’"!#("!#)"!#*"!#+"!#,"$"$"&"("*","$$"$&"$("$*"$,"-./012301"45//.66"-730"!"#$%&’"18(!!!"18$!!!!"18$(!!!"!"!#$"!#%"!#&"!#’"!#("!#)"!#*"!#+"!#,"$"$"&"("*","$$"$&"$("$*"$,"-./0123402"56//.77"-840"!"#$%&’(29(!!!"29$!!!!"29$(!!!"!"!#$"!#%"!#&"!#’"!#("!#)"!#*"!#+"!#,"$"$"$#("%"%#("&"&#("’"’#("("(#("-./0123402"56//.77"-840"!"#$%&’(93/.27.":;8>?.77"73127"Session 6A: IoTCCS’18, October 15-19, 2018, Toronto, ON, Canada1095(a) License plates
(b) Human faces
(c) Address signs
Figure 10: [Deep-learning] Recognition success ratio (R-ratio) by object-size.
6.1.1 Deep-Learning Powered Attackers
To measure privacy against automated recognition, we consider the
case of video requesters, as potential attackers, equipped with visual
analytics tools. More specifically, we experiment with two state-of-
the-art recognition methods: Deep-learning based and OCR-based
approaches.
For deep-learning recognition, we use TensorFlow [19]. We col-
lect +100K images of UK license plates, +334K facial photos of 334
celebrities (each with 100 photos), and +50K pictures of address
signs. By pre-processing, we make each image have multiple ver-
sions with different scales of pixelation. The resulting datasets are
categorized by object-type (plate/face/sign) and by P-scale. We use
them as training input to the deep-neural-network (DNN) model
that internally performs learning and classification of characters,
digits, and facial features for recognition.
For plates and signs, we also test with an optical character recog-
nition (OCR) approach. More specifically, we use Tesseract [20], one
of the most accurate OCR engines. It comes with built-in training
data for character recognition, so we directly apply our test data
for evaluation.
(a) P-scale = 5 (R-ratio: 70.2%)
Figure 11: Pixelation of a license plate (same object).
(b) P-scale = 9 (R-ratio: 3.3%)
(a) P-scale = 4 (R-ratio: 65.3%)
(b) P-scale = 10 (R-ratio: 1.7%)
Figure 12: Pixelation of a human face (same object).
6.1.2 Privacy Performance by Object-Type
We run experiments on our test images of 462 UK plates, 157 peo-
ple (out of the 334 celebrities, but different photos than the ones
in the training data), and 350 signs. The testing datasets are also
categorized by object-type and by P-scale. We specifically measure
the recognition success ratio (R-ratio) as the probability that the
adversary correctly recognizes the objects pixelated in our test
images.
Figures 8a, 8b, and 8c show the results of R-ratio when using
deep-learning recognition against P-scale, while varying n, the size
of training datasets by object-type: plates, faces, and signs, respec-
tively. We see that, the larger the volume of training data, the higher
the R-ratio, but showing diminishing returns. This indicates that
our datasets are of sufficient volume to train the deep-learning
recognition. The results show that the R-ratio decreases with P-
scale; it drops below 0.01 when P-scale is higher than 9 (for plates
and signs) and 11 (for faces). The reason for the different results
by object type here is partly because the recognition performance
also depends on the size of a candidate pool. Identifying one out of
a certain number of faces—334 people in our test—is smaller-scale
(a) P-scale = 5 (R-ratio: 57.6%)
(b) P-scale = 9 (R-ratio: 2.4%)
Figure 13: Pixelation of an address sign (same object).
recognition than the cases of number plates and address signs. In re-
ality, we expect a more moderate R-ratio result of face recognition,
for a given P-scale, against a larger set of faces.
We also present the results of R-ratio when using the OCR-based
recognition against P-scale in Figure 9. It also exhibits declining
trends over P-scale, but underperforms the deep-learning recogni-
tion. This result shows that the DNN-based approach is the stronger
adversarial model for recognition against pixelation.
6.1.3 Privacy Performance by Object-Size
We further experiment with deep-learning recognition while vary-
ing the size of pixelated objects. We classify our training/testing
data into three groups by object-size: large(> 100×100 pixels),
medium(25×25–100×100 pixels), and small( 0.5 and R-ratio < 0.05 in the cases of plate, face, and sign,
respectively.
6.1.4 Choice of P-scale
As discussed in Section 4.5, the pixelation intensity should be as
minimal as possible while de-identifying sensitive objects. Our
privacy experiments suggest that the P-scale should be set higher
than 11 to protect against the plate/face/sign recognition, making
R-ratio below 0.01. Based on this result, we choose to set P-scale=12
for Pinto to balance the privacy-frame quality tradeoff.
6.2 Prevention of Content Forgery
Pinto uses realtime signatures of h_pixelation (pixelation with
original-hash embedding) to prevent content alterations to original
videos. In this section we demonstrate forgery-proofness of the
h_pixelation in comparison with the use of pixelation only and the
use of hash only.
6.2.1 Forgery Methods
We apply various types of video forgery (i.e., per-frame image
forgery), some of which are categorized in [35, 62], to the test-
ing images and our own surveillance videos. These forgeries are:
Copy-move (copying and pasting an image part; Fig 14b), Splic-
ing (merging images), Erasing (removing some objects in images;
Fig 14c), Lighting (altering lighting conditions; Fig 14d), Retouch-
ing (modifying certain image features), Collision (creating fake