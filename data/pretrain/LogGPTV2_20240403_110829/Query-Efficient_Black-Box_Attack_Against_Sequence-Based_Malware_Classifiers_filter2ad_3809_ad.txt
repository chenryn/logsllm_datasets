et al.
2018 [42]
[%]
Our
Decision-
Based
Attack
[%]
BiRand,
Dang et
al. 2017
[24]
[%]
Rosenberg
et al.
2018 [42]
LSTM
Deep LSTM
GRU
1D CNN
Logistic Regression
Random Forest
SVM
Gradient Boosted
Tree
62.21
63.62
63.35
63.63
41.47
63.24
42.59
41.62
[24]
39.49
40.38
40.21
40.39
26.32
40.14
27.04
26.41
51.15
50.80
51.16
48.93
35.67
50.87
36.27
36.55
22.22
22.71
21.47
4.10
4.43
5.20
3.82
13.99
31.42
32.12
30.36
5.80
6.26
7.35
5.40
19.78
17.22
29.51
16.09
49.21
7.58
9.40
7.19
27.80
in [24] (because both attacks use a similar algorithm).
As can be seen in Table 2, our proposed decision-based attack has the highest eﬀectiveness for
all of the malware classiﬁers tested. Rosenberg et al. provides higher attack eﬀectiveness than our
decision-based attack for 2500 queries (not shown due to space limits), but underperforms when
the number of queries is being reduced, because there aren’t enough queries to build a substitute
model with accurate decision boundary.
We see that our attack provides higher attack eﬀectiveness than Dang et al., due to our attack’s
use of benign perturbation, which was not presented in [24]. When modifying BiRand to use benign
perturbation, the results are identical to those obtained by our attack (because both attacks use
a similar algorithm; this is not shown due to space limits). In addition, we see that our attack
produces a smaller perturbation (adding 25-50% less API calls; see Table 2) than BiRand. This
is due to the additional level of backtracking in our attack (lines 19-22 in Algorithm 1). This
backtracking is not available in BiRand, which implements a binary search.
As mentioned in Section 4.1, |T estSet(f )| = 36, 000 samples, and the test set T estSet(f ) is
balanced, so the attack performance was measured on: |{f (xm) = M alicious|xm ∈ T estSet(f )}| =
18, 000 samples.
We used k = n for Algorithm 2, i.e., the non overlapping sliding window size of the adversary
is the same as that used by the target classiﬁer. However, even if this is not the case, the attack
eﬀectiveness is not signiﬁcantly degraded. If n  k, the adversaries would
keep trying to modify diﬀerent API calls’ positions in Algorithm 2, until they modify the ones
impacting the target classiﬁer as well, thereby increasing the attack overhead without aﬀecting the
attack eﬀectiveness. For instance, when n = 100, k = 140, there is an average decrease in attack
Accepted as a conference paper at ACSAC 2020
We used the adversarial vocabulary:
eﬀectiveness from 87.96% to 87.94% for an LSTM classiﬁer. Other classiﬁers have similar behavior,
which is not shown due to space limits. The closer n and k are, the better the attack performance.
D(cid:48) = D − {ExitW indowsEx(), N tT erminateP rocess()}, where D is all of the API call types
recorded by the malware classiﬁer, so D(cid:48) does not contain any API type that might harm the
code’s functionality.
Score-Based Attack Performance There are no published query-eﬃcient adversarial attacks
against RNN variants. Attacks that minimize the number of queries exist, but they only work
against CNNs [30, 16]. Those attacks aren’t relevant, because they don’t work with sequence input
and discrete values. To address this gap, we used Nevergrad [39], a gradient-free optimization li-
brary, to implement discrete sequence input variants of a few state-of-the-art score-based adversarial
attacks:
1. SPSA-based attack (Uesato et al. [43]).
2. NES-based attack (Ilyas et al. [30]).
3. GA-based attack (Alzantot et al. [17], Xu et al. [45]).
4. The gradient-based attack (Rosenberg et al. [42]).
We compare theses attacks to our score-based uniform mixing EA attack (described in Section
3.2.3) and to our decision-based attack. We didn’t implement the ZOO attack of Chen et al. [21],
because it has already been evaluated and was found to be less eﬀective than both SPSA and NES
attacks [43].
We used Nevergrad’s default arguments for all attacks.
The attack performance (average of ﬁve runs) for the LSTM classiﬁer with a ﬁxed budget of
100, 200, and 2500 queries (the attack of Rosenberg et al. requires many queries to accurately
build the substitute model required to estimate the gradients per API window [42]) is presented in
Table 3 for random perturbation type attacks and in Table 4 for benign perturbation type attacks
(Rosenberg et al. has the same performance in both tables because its perturbations are always
determined by the maximum gradient).
The ﬁrst two lines of each table pertain to our attacks with diﬀerent attacker knowledge. When
combining these lines with the iteration method values in Tables 3 and 4, one gets our eight previ-
ously described attacks (all combinations of: iteration method, attacker knowledge and perturbation
type). The iteration method values are Linear (for linear iteration) and Log (for logarithmic back-
tracking). Other classiﬁers and budgets (not shown due to space limits) resulted in similar relative
trends: a higher budget results in increased attack eﬀectiveness. Note that here we use a ﬁxed
number of queries and try to maximize the attack eﬀectiveness for the speciﬁed number of queries,
since the reverse approach requires higher computational eﬀort and yields the same results.
Our score-based attack variants (in Tables 3 and 4) provide a higher attack eﬀectiveness for
all classiﬁers (this is not discussed further due to space limits) because of the more eﬃcient search
algorithms used. The attack of Rosenberg et al. requires using 2255 queries per API call window to
build the substitute model accurately enough to reach the performance mentioned in [42] (the rest
of the queries, for a a total of 2500, are required to perform the attack itself). Trying to use fewer
queries results in a substitute model with inaccurate decision boundary that aﬀect the gradients,
and thus the gradient based attack eﬀectiveness.
Accepted as a conference paper at ACSAC 2020
Table 3: Random Perturbation Type Attack Eﬀectiveness Comparison for a Fixed Number of
Queries (LSTM Model)
Number of Queries
Logarithmic Backtracking
(/BiRand)Iteration Method
Our Score-Based Attack (Score-Based
Attacker Knowledge)
Our Decision-Based Attack
(Decision-Based Attacker Knowledge)
Rosenberg et al. [42]
Uesato et al. [43]
Ilyas et al. [30]
Alzantot et al. [17], Xu et al. [45]
100
Linear
200
Linear
2500
Linear
100
Log
200
Log
2500
Log
58.75
67.59
100.00
69.28
79.71
100.00
19.86
21.25
31.43
39.49
42.25
62.50
51.15
2.37
37.50
54.68
67.11
2.73
43.14
62.91
99.99
2.87
74.94
100.00
51.15
5.17
43.78
62.06
67.11
5.95
50.37
71.40
99.99
6.25
87.50
100.00
Table 4: Benign Perturbation Type Attack Eﬀectiveness Comparison for a Fixed Number of Queries
(LSTM Model)
Number of Queries
Logarithmic Backtracking
(/BiRand)Iteration Method
Our Score-Based Attack (Score-Based
Attacker Knowledge)
Our Decision-Based Attack
(Decision-Based Attacker Knowledge)
Rosenberg et al. [42]
Uesato et al. [43]
Ilyas et al. [30]
Alzantot et al. [17], Xu et al. [45]
100
Linear
200
Linear
2500
Linear
100
Log
200
Log
2500
Log
71.90
82.70
100.00
84.77
97.53
100.00
41.34
44.24
46.34
62.21
63.97
87.96
51.15
6.56
66.23
68.49
67.11