# Checklist
  * [ x] I have read the relevant section in the  
contribution guide  
on reporting bugs.
  * [ x] I have checked the issues list  
for similar or identical bug reports.
  * [ x] I have checked the pull requests list  
for existing proposed fixes.
  * [ x] I have checked the commit log  
to find out if the bug was already fixed in the master branch.
  * I have included all related issues and possible duplicate issues  
in this issue (If there are none, check this box anyway).
## Mandatory Debugging Information
  * I have included the output of `celery -A proj report` in the issue.  
(if you are not able to do this, then at least specify the Celery  
version affected).
  * [ x] I have verified that the issue exists against the `master` branch of Celery.
  * I have included the contents of `pip freeze` in the issue.
  * I have included all the versions of all the external dependencies required  
to reproduce this bug.
## Optional Debugging Information
  * I have tried reproducing the issue on more than one Python version  
and/or implementation.
  * I have tried reproducing the issue on more than one message broker and/or  
result backend.
  * I have tried reproducing the issue on more than one version of the message  
broker and/or result backend.
  * I have tried reproducing the issue on more than one operating system.
  * I have tried reproducing the issue on more than one workers pool.
  * I have tried reproducing the issue with autoscaling, retries,  
ETA/Countdown & rate limits disabled.
  * I have tried reproducing the issue after downgrading  
and/or upgrading Celery and its dependencies.
## Related Issues and Possible Duplicates
#### Related Issues
  * #4551 (comment)
#### Possible Duplicates
  * None
## Environment & Settings
**Celery version: 4.3.0** :
**`celery report` Output:**
    celeryworker_1  | [2019-09-30 12:10:16,485: INFO/MainProcess] Received task: django_dask.users.tasks.get_users_count[0b2cd4f1-e6a9-41af-974c-bd9a60ddc648]
# Steps to Reproduce
## Required Dependencies
  * **Minimal Python Version** : 3.6.6
  * **Minimal Celery Version** : 4.3.0
  * **Minimal Kombu Version** : Unknown
  * **Minimal Broker Version** : redis==3.3.8
  * **Minimal Result Backend Version** : N/A or Unknown
  * **Minimal OS and/or Kernel Version** : Docker image python:3.6.6
  * **Minimal Broker Client Version** : N/A or Unknown
  * **Minimal Result Backend Client Version** : N/A or Unknown
Install cookiecutter django with celery and add dask package. You can either
add dask as a service in docker-compose or initiate a local cluster within the
`{app}/users/app.py`
## Minimally Reproducible Test Case
# Expected Behavior
That I am able to further create subprocesses within the task by triggering
dask within celery task.
# Actual Behavior
I am trying to start subprocesses of scikit learn within a task of celery. I
am aware that joblib will run it sequentially if it is already running in a
process. Therefore, based on #4551 (comment), I have installed Dask and
changed joblib `parallel_backend` to dask and run the below function one time
normally; i.e. `get_users_count()` and another time with `apply_async()`;
i.e., `get_users_count.apply_async()`. The former ran successfully and the
later showed that the worker has received the message (as shown in the `celery
report` above). and celery worker hangs and become unresponsive, I tried to
access the worker in flower, however I received `Unknown worker
'celery@0bb51b1f7782'` as if it is died (which was previously accessible
before calling the function).
    @celery_app.task()
    def get_users_count():
        """A pointless Celery task to demonstrate usage."""
        from sklearn.utils import parallel_backend
        import numpy as np
        from sklearn.cluster import KMeans
        parallel_backend("dask")
        kmeans = KMeans(n_jobs=-1, n_init=5, verbose=10)
        kmeans.fit(np.random.rand(1000).reshape(500, 2))
        return User.objects.count()