We use the Kolmogorov-Smirnov test (KS test) [40] to test
if two data sets are signiﬁcantly different. The KS test is a
nonparametric statistical hypothesis test based on the maxi-
mum distance between the empirical cumulative distribution
functions of the two data sets. The two hypotheses of a KS
test are:
i.e.
H0: the two data sets are from the same distribution
H1: the two data sets are from different distributions.
A KS test reports a p-value,
the probability that
obtaining the maximum distance is at least as large as the
observed one when H0 is assumed to be true. i.e., H0 is
accepted. If this p-value is smaller than α, usually set to 0.05,
we will reject the H0 hypothesis because events with small
probabilities rarely happen (rejecting H0 and accepting H1
indicates a “good” feature for distinguishing users). For each
feature, we calculate the p-value for data points for each pair
of users and drop a feature if most of its p-values are higher
than α.
Figure 3 shows the testing results for the features in both
the smartphone and smartwatch. For each feature, the resulting
p-values are drawn in a box plot. The bottom and the top lines
of the box denote the lower quartile Q1 and upper quartile Q2,
deﬁned as the 25th and the 75th percentiles of the p-values.
The middle bar denotes the median of the p-values. The y-axes
in Figure 3 is in logarithmic scale. The red horizontal lines
315
represent the signiﬁcance level α = 0.05. The better a feature
is, the more of its box plot is below the red line. It denotes that
more pairs are signiﬁcantly different. From Figure 3, we ﬁnd
that the accPeak2 f and gyrPeak2 f in both the smartphone
and the smartwatch are “bad” features, so we drop them.
Next, we try to drop redundant features, by computing the
correlation between each pair of features. A strong correlation
between a pair of features indicates that they are similar in
describing a user’s behavior pattern, so one of the features
can be dropped. A weak correlation implies that the selected
features reﬂect different behaviors of the user, so both features
should be kept.
We calculated the Pearson’s correlation coefﬁcient between
any pair of features. Then, for every pair of features, we took
the average of all resulting correlation coefﬁcients over all
the users. Table III shows the resulting average correlation
coefﬁcients. The upper right triangle is the correlation between
features in the smartphone, while the lower left triangle is the
correlation between features in the smartwatch. We observe
that Ran has very high correlation with Var in each sensor
on both the smartphone and smartwatch. It means that Ran
and Var have information redundancy. Also Ran has relatively
high correlation with Max. Therefore, we drop Ran from our
feature set.
D. Do multiple devices help?
We also study if using data from the same type of sensors
(accelerometer and gyroscope), but from different devices is
helpful for improving user authentication. Towards this end, we
calculate the correlations between smartphone and smartwatch
sensor data in Table IV. Since these features do not have
strong correlation with each other, it implies that these same
sensors on the two devices measure different aspects of a user’s
behavior, so we keep all these features.
Hence our feature vector for sensor i, in a given time
window k, for the smartphone, SP , is
i (k)]
i (k), SP f
SPi(k) = [SP t
(1)
where t represents the time domain, f represents the frequency
domain, and
SP t
i (k) = [mean(Si(k)), var(Si(k)), max(Si(k)), min(Si(k))]
i (k) = [peak(Si(k)), f req(Si(k)), peak2(Si(k))]
SP f
(2)
TABLE V.
CONFUSION MATRIX OF CONTEXT DETECTION RESULTS
USING TWO SMARTPHONE SENSORS.
TABLE VI.
AUTHENTICATION PERFORMANCE WITH DIFFERENT
MACHINE LEARNING ALGORITHMS.
Confusion Matrix
Stationary
Moving
Stationary Moving
0.9%
99.1%
0.6%
99.4%
Therefore the feature vector for the smartphone is
SP (k) = [SPaccerometer(k), SPgyroscope(k)]
(3)
Similarly, we have the the feature vector for the sensor
data from the smartwatch, denoted SW (k). Therefore, the
authentication feature vector is
Authenticate(k) = [SP (k), SW (k)]
(4)
E. Can Context Detection help?
Since it seems intuitive that sensor measurements of motion
may be different under different contexts, we now consider
the minimum contexts that can improve the accuracy of user
authentication. To be viable, we need very fast, user-agnostic
context detection, since this must now precede user authenti-
cation, and we also want to keep real-time computation to an
acceptable level. Hence, we try using the same feature vector
in Eq. 3 for the smartphone only (no smartwatch) context
detection. During the user enrollment phase, we feed these
feature vectors from all users into the context detection model
to train it. During the testing phase, we use this user-agnostic
context detection model to detect the current user context.
1) Random Forest for context detection: We experimented
with several machine learning algorithms for context detection,
and chose the Random forest algorithm [41]. This is commonly
used in data mining. It creates a model that predicts the value
of a target variable based on several input variables.
Initially, we tried using four contexts: (1) The user uses
the smartphone without moving around, e.g., while standing
or sitting; (2) The user uses the smartphone while moving. No
constraints are set for how the user moves; (3) The smartphone
is stationary (e.g., on a table) while the user uses it; (4) The
user uses the smartphone on a moving vehicle, e.g., train.
However, we found that these four contexts can not be easily
differentiated: contexts (3) and (4) are easily misclassiﬁed as
context (1), since (1), (3) and (4) are all relatively stationary
(e.g., when moving at a stable speed), compared to context
(2). Therefore, we combined contexts (1), (3) and (4) into one
stationary context, and left (2) as the moving context. The
resulting confusion matrix in Table V showed a very high
context detection accuracy of over 99% with these 2 simple
contexts. The context detection time was also very short - less
than 3 milliseconds.
For these context training and testing experiments, we had
users use their smartphones in ﬁxed contexts under controlled
lab conditions. Users were asked to use the smartphone and
the smartwatch freely under each context for 20 minutes. They
were told to stay in the current context until the experiment is
ﬁnished. Note that such recording process is only needed for
developing the context detection model and is not required
for normal use in real-world scenarios. We use these data
from the different users to train the context detection model
in a user-agnostic manner. That is, when we perform context
detection for a given user, we use a context detection model
Method
KRR
SVM
Linear Regression
Naive Bayes
FAR
FRR
2.8%
0.9%
2.7%
2.5%
12.7% 14.6%
10.8% 13.9%
Accuracy
98.1%
97.4%
86.3%
87.6%
(cid:23)(cid:19)
(cid:22)(cid:19)
(cid:21)(cid:19)
(cid:20)(cid:19)
(cid:12)
(cid:8)
(cid:3)
(cid:11)
(cid:3)
(cid:72)
(cid:87)
(cid:68)
(cid:53)
(cid:81)
(cid:82)
(cid:76)
(cid:87)
(cid:70)
(cid:72)
(cid:77)
(cid:72)
(cid:53)
(cid:3)
(cid:72)
(cid:86)
(cid:79)
(cid:68)
(cid:41)
(cid:19)
(cid:3)
(cid:19)
(cid:21)
(cid:22)(cid:19)
(cid:21)(cid:19)
(cid:20)(cid:19)
(cid:12)
(cid:8)
(cid:11)
(cid:3)
(cid:72)
(cid:87)
(cid:68)
(cid:53)
(cid:3)
(cid:72)
(cid:70)
(cid:81)
(cid:68)
(cid:87)
(cid:83)
(cid:72)
(cid:70)
(cid:70)
(cid:36)
(cid:3)
(cid:72)
(cid:86)
(cid:79)
(cid:68)
(cid:41)
(cid:19)
(cid:3)
(cid:19)
(cid:21)
(cid:23)
(cid:25)
(cid:27)
(cid:20)(cid:19)
(cid:58)(cid:76)(cid:81)(cid:71)(cid:82)(cid:90)(cid:3)(cid:54)(cid:76)(cid:93)(cid:72)(cid:3)(cid:11)(cid:86)(cid:72)(cid:70)(cid:82)(cid:81)(cid:71)(cid:86)(cid:12)
(a) Stationary
(cid:23)
(cid:25)
(cid:27)
(cid:20)(cid:19)
(cid:58)(cid:76)(cid:81)(cid:71)(cid:82)(cid:90)(cid:3)(cid:54)(cid:76)(cid:93)(cid:72)(cid:3)(cid:11)(cid:86)(cid:72)(cid:70)(cid:82)(cid:81)(cid:71)(cid:86)(cid:12)
(c) Stationary
(cid:3)
(cid:38)(cid:82)(cid:80)(cid:69)(cid:76)(cid:81)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)
(cid:54)(cid:80)(cid:68)(cid:85)(cid:87)(cid:83)(cid:75)(cid:82)(cid:81)(cid:72)
(cid:54)(cid:80)(cid:68)(cid:85)(cid:87)(cid:90)(cid:68)(cid:87)(cid:70)(cid:75)
(cid:22)(cid:19)
(cid:21)(cid:19)
(cid:20)(cid:19)
(cid:12)
(cid:8)
(cid:3)
(cid:11)
(cid:3)
(cid:72)
(cid:87)
(cid:68)
(cid:53)
(cid:81)
(cid:82)
(cid:76)
(cid:87)
(cid:70)
(cid:72)
(cid:77)
(cid:72)
(cid:53)
(cid:3)
(cid:72)
(cid:86)
(cid:79)
(cid:68)
(cid:41)
(cid:20)(cid:21)
(cid:20)(cid:23)
(cid:20)(cid:25)
(cid:19)
(cid:3)
(cid:19)
(cid:21)
(cid:3)
(cid:38)(cid:82)(cid:80)(cid:69)(cid:76)(cid:81)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)
(cid:54)(cid:80)(cid:68)(cid:85)(cid:87)(cid:83)(cid:75)(cid:82)(cid:81)(cid:72)
(cid:54)(cid:80)(cid:68)(cid:85)(cid:87)(cid:90)(cid:68)(cid:87)(cid:70)(cid:75)
(cid:22)(cid:19)
(cid:21)(cid:19)
(cid:20)(cid:19)
(cid:12)
(cid:8)
(cid:11)
(cid:3)
(cid:72)
(cid:87)
(cid:68)
(cid:53)
(cid:3)
(cid:72)
(cid:70)
(cid:81)
(cid:68)
(cid:87)
(cid:83)
(cid:72)
(cid:70)
(cid:70)
(cid:36)
(cid:3)
(cid:72)
(cid:86)
(cid:79)
(cid:68)
(cid:41)
(cid:20)(cid:21)
(cid:20)(cid:23)
(cid:20)(cid:25)
(cid:19)
(cid:3)
(cid:19)
(cid:21)
(cid:3)
(cid:38)(cid:82)(cid:80)(cid:69)(cid:76)(cid:81)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)
(cid:54)(cid:80)(cid:68)(cid:85)(cid:87)(cid:83)(cid:75)(cid:82)(cid:81)(cid:72)
(cid:54)(cid:80)(cid:68)(cid:85)(cid:87)(cid:90)(cid:68)(cid:87)(cid:70)(cid:75)
(cid:20)(cid:21)
(cid:20)(cid:23)
(cid:20)(cid:25)
(cid:3)
(cid:38)(cid:82)(cid:80)(cid:69)(cid:76)(cid:81)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)
(cid:54)(cid:80)(cid:68)(cid:85)(cid:87)(cid:83)(cid:75)(cid:82)(cid:81)(cid:72)
(cid:54)(cid:80)(cid:68)(cid:85)(cid:87)(cid:90)(cid:68)(cid:87)(cid:70)(cid:75)
(cid:20)(cid:21)
(cid:20)(cid:23)
(cid:20)(cid:25)
(cid:23)
(cid:25)
(cid:27)
(cid:20)(cid:19)
(cid:58)(cid:76)(cid:81)(cid:71)(cid:82)(cid:90)(cid:3)(cid:54)(cid:76)(cid:93)(cid:72)(cid:3)(cid:11)(cid:86)(cid:72)(cid:70)(cid:82)(cid:81)(cid:71)(cid:86)(cid:12)
(b) Moving
(cid:23)
(cid:25)
(cid:27)
(cid:20)(cid:19)
(cid:58)(cid:76)(cid:81)(cid:71)(cid:82)(cid:90)(cid:3)(cid:54)(cid:76)(cid:93)(cid:72)(cid:3)(cid:11)(cid:86)(cid:72)(cid:70)(cid:82)(cid:81)(cid:71)(cid:86)(cid:12)
(d) Moving
Fig. 4.
FRR and FAR with different window sizes under two contexts. (a)
and (b) are the FRRs under different contexts. (c) and (d) are the FARs under
different contexts. Both the FRR and FAR become stable when the window
size is larger than 6 seconds.
(i.e., classiﬁer) that was trained with other users’ data. This