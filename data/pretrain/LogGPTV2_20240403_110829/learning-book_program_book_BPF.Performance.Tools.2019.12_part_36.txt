output includes a column for °dirty"” pages: pages that have changed in memory and are not yet
saved on disk.
vmstat
The vmstat(1) command shows various system-wide statistics over time, including statistics for
memory, CPUs, and storage I/O. For example, printing a summary line every one second:
S vmstat 1
procs
-sxap*
-10 -
-systen--
b
svpd
free
buff cache
S 1
°
b1
lbo
Ln
s en pT s Bn. B0
120
01075868
13232 5288396
U
0
1. 4
2 6
0065 2866191
14
D
01075000
13232 5288932
0
0
0 28751 77964 22 1 7700
90
01074452
13232 5289440
0
0
0 28511 76371 18 1 8100
15
D
01073824
13232 5289828
D
0 3241186088261 7300
The “free*, *buff*, and *cache”d columns show memory in Kbytes that is free, used by storage I/O
buffers, and used for the file system cache. The *si” and *so* columns show memory swapped in
and out from disk, if active.
The first line of output is the “summary since boot,” where most columns are an average since
the system booted; however, the memory columns show the current state. The second and subse
quent lines are the one-second summaries.
sar
The sar(1) command is a multi-tool that prints metrics for different targets. The B option shows
page statistics:
 88r -B 1
Linux 4.15,01031-avs (.--)
01/26/2019
_x86_64_
(48 CPU)
06:10:38 Pα p9pgin/a p9pgout/s
fault/s najflt/s pgfree/s pgscank/s pgscand/s
pgstea1/s
lvmeff
06:10:39 P
0.00
0. 00
286.00
0.00 16911.00
0.00
0. 00
0 , 00
0,00
---
## Page 292
7.2 Traditional Tools
255
06:10: 40 PM
0.00
0.00
90,00
0,00 19178.00
0 .00
0. 00
0.00
00* 0
06 :10 :41 PM
0 .00
0. 00
187,00
0,00 18949.00
0.00
0. 00
0. 00
0,00
06:10:42 PM
0 .00
0. 00
110, 00
0,00 24266, 00
0.00
0. 00
0.00
0,00
[...]
This output is from a busy production server. The output is very wide, so the columns have
wrapped and are a little hard to read here. The page fault rate (fault/s*) is lowless than 300 per
second. There also isn’t any page scanning (the “pgscan° columns), indicating that the system is
likely not running at memory saturation.
Here is output from a server doing a software build:
 88r -B 1
Linux 4,18,0zc6-v1rtual (.--) 01/26/2019
_x8 6_64_
(36 CPU)
06:16:08 P
F9pgin/s pgpgout/a
fault/s najflt/s pgfree/s pgscank/s pgscand/s
Pgstea1/s
vmeff
06:16:09 PM
1968.00
302,00 1454167,00
0.00 1372222,00
0.00
0.00
0, 00
0,00
06:16:10 PM
1680.00
171.00 1374786.00
0 .001203463 .00
0.00
0.00
0 , 00
0,00
06:16:11 P
1100.00
581.,00 1453754,00
0.00 1457286.00
0.00
0.00
0 , 00
0.00
06:16:12 PM
1376.00
22T,00 1527580.00
0.00 1364191,00
0 .00
0 , 00
0 , 00
0,00
06:16:13 PH
880.00
68,00 1456732,00
0.00 1315536,00
0.00
0.00
0 , 00
0 , 0 0
[..-]
Now the page fault rate is hugeover one million faults per second. This is because the software
first execution.
build involves many short-lived processes, and each new proces is faulting in its address space on
7.2.3 Hardware Statistics and Sampling
There are many PMCs for memory I/O events. To be clear, this is I/O from the CPU units on the
processor to the banks of main memory, via the CPU caches. PMCs, introduced in Chapter 2, can
be used in two modes: counting and sampling. Counting provides statistical summaries, and costs
virtually zero overhead to use. Sampling records some of the events to a file for later analysis.
---
## Page 293
256
Chapter 7 Memory
This example uses perf(1) in counting mode to measure last-level cache (LLC) loads and misses,
system-wide (a), with interval output every 1 second (I 1000):
+ perf stat -e LLC-loads LLCload-misses -a -I 1000
t.ine
counts L
unit eventa
1.000705801
B, 402, 738
LLC-1oads
1. 000705801
3, 610, 704
LLC-lcad-nisses
#
42,9T% of a11 LL=cache hit.s
2,001219292
B, 265, 334
LLC-loads
2,001219292
956 *925 °c
LLC-1cad-nisses
#
42.32s of al1 LL=cache hit.s
3.001763602
9, 586, 619
LLC-1oads
3.001763602
3, 842, 810
LLC-load-nisses
43,91h of a11 LL=cache hit.s
[. . - ]
For convenience, perf(1) has recognized how these PMCs are related and printed a percentage
miss ratio. LLC misses are one measure of I/O to main memory, since once a memory load or store 
misses the LLC, it becomes a main memory access.
Now perf(1) is used in sampling mode to record details from every one in one hundred thousand 
L1 data cache misses:
0- 00000t o- sassre-peot-aqoeop-T e- pzooaz gzed 
^C[ perf record: Noken up 1 tines to vzite data ]
[perf record: Captured and wrote 3,075 MB perf.data (612 samples) ]
 perf zeport -n --stdio
 Overhead
Sanples
Comnand
 Shared object
5ymbo1
30.565
187
 cksun
cksun
[kernel.kallsyms]
8.338
51
cksun
[]0x0000000000001cc9
2 .78 5
1T
cksun
cksun
[-]0x0000000000001cb4
2.45§
15
cksun
[kernel.kallsyms]
[k]  generic_file_read_iter
2.125
13
cksun
cksun
[-]0x0000000000001cbe
[. . .]
uanbaug Asan are sassaooe I°T asneoaq pasn sem (0oooot o) proqsang Surgdues a&se[ e qons
and a lower threshold might collect so many samples that it would perturb the performance of
running software. If you are unsure of the rate of a PMC, use counting mode first (pexf stat.) to
find it, and from that you can calculate an appropriate threshold.
The output of perf report shows the symbols for the L1 dcache misses. It is recommended to use
PEBS with memory PMCs so that the sample instruction pointers are accurate. With perf, add :p
or :pp (better), or :ppp (best) to the end of the event name to enable PEBS; the more ps, the more
accurate. (See the p modifier section of the perf-list(1) man page.)
---
## Page 294
7.3 BPF Tools
257
7.3
BPF Tools
This section covers the BPF tools you can use for mer
nory performance analysis and troubleshoot-
ing (see Figure 7-4).
Applications
Leal
mapsnocp
System Libraries
oonki11
System Call inerface
Rest of Kemel
Memory
Virtual
rapin
CPU
MMU
Figure 7-4  BPF tools for memory analysis
These tools are either from the BCC and bpftrace repositories covered in Chapters 4 and 5, or were
created for this book. Some tools appear in both BCC and bpftrace. Table 7-3 lists the origins of
the tools covered in this section (BT is short for bpftrace.)
Table 7-3
Memory-Related Tools
Tool
Source
Description
oomk111
BCC/BT
00M
Shows extra info on OOM kill events
memleak
BCC
Sched
Shows possible memory leak code paths
mmapsnoop
Book
Syscalls
Traces mmap(2) calls systemwide
brkstack
Book
Syscalls
Shows brk() calls with user stack traces
shmsnoop
BCC
Syscalls
Traces shared memory calls with details
faults
Book
Faults
Shows page faults, by user stack trace
ffaults
Book
Faults
Shows page faults, by filename
vmscan
Book
VM
 Measures VM scanner shrink and reclaim times
drsnoop
BCC
VM
Traces direct reclaim events, showing latency
swapin
Book
VM
Shows swap-ins by process
hfaults
Book
Faults
Shows huge page faults, by process
---
## Page 295
258
Chapter 7  Memory
For tools from BCC and bpftrace, see their repositories for full and updated lists of tool options
and capabilities. Some of the most important capabilities are summarized here.
Chapter 14 provides more BPF tools for kernel memory analysis: kmem(8), kpages(8),
slabratetop(8), and numamove(8).
7.3.100mkill
oomkill(8) is a BCC and bpftrace tool for tracing out-of-memory killer events and printing details
such as the load averages. Load averages provide some additional context for the system state at
the time of the OOM, showing whether the system was getting busier or whether it was steady.
The following example shows oomkill(8) from BCC, from a 48-CPU production instance:
0omki11
Tracing 00x kills... Ctrl-C to stop.
08:51:34 Teiggered by PI0 18601 (*per1*), 008 ki11 of PID 1165 (*java*), 18006224
pages, 1oadavg: 10.66 7.17 5.06 2/755 18643
[...]
This output shows that PID 18601 (perl) needed memory, which triggered an OOM kill of PID
1165 (java). PID 1165 had reached 18006224 pages in size; these are usually 4 Kbytes per page,
depending on the processor and process memory settings. The load averages show that the system
was getting busier at the time of the OOM kill.
This tool works by tracing the oom_kil_process() function using kprobes and printing various
details. In this case, the load averages are fetched by simply reading /proc/loadavg. This tool can
be enhanced to print other details, as desired, when debugging OOM events. In adition, oom
tracepoints that can reveal more details about how tasks are selected are not yet used by this tool.
The BCC version currently does not use command line arguments.
bpftrace
The following is the code for the bpftrace version of oomkilI(8):
#1/usx/local/bin/bpEtrace
#1nc1ude 
BEGIN
printf (*Tracing om_ki11_process 1) ... Bit Ctr1-C to end. n*) ;
1. Origin: 1 created it on 9-Feb-2016, for BCC, to have a tool for launching extra debug info for the production 00M
events I sometimes see. I wrote the bpfrace version on 7-Sep-2018.
---
## Page 296
7.3 BPF Tools
259
kprobeioon_ki11_process
[Bre ( Torquoouoo onzqe) = 2og
time (*%B:sH:S "]
printf (*7riggered by PID d (\"§s\*), *, pid, conmm) 
printf (*0oM ki11 of PID 4d (\*%s*), ld pages, loadavg: ",
$oc->chosen->pid, $oc->chosen->corm, $oc->totalpages) 
cat (*/proc/1oadavg*) =
The program traces oom_kill_process() and casts the second argument as a struct oom_control,
pid) ssaoosd quaano atp po seap spuad a ssaood geguoes ag go seap supequo ups
comm) that led to the OOM event, and then the target details, and finally a system() callis used
to print the load averages.
7.3.2 memleak
memleak(8)* is a BCC tool that traces memory allocation and free events along with the alloca
tion stack traces. Over time, it can show the long-term survivors—the allocations that have not
been freed. This example shows memleak(8) running on a bash shell process*:
+ nemleak -p 3126
Attaching to pld 3228, Ctr1+C to quit.
suoTaeootte butpuesno wtn soeqs 0t dos [st+bt=601
[. ..]
960 bytes Ln 1 allocations from stack
xrea11oc+0x2a [bash]
strvec_resize+0xZb [bash]
maybe_make_export_env+0xa8 [bash]
execute_simple_corman.d+0x269 [bash]
execute_comnand_interna]+0x862 [bash]
execute_connect.ion+Dx109 [ba.sh]
execute_comnand_in.ternal+0xcl8 [bash]
execute_comnand+[x5b [bash]
[eq] 98x(+dooTxapea
[seq] 696x0+uTeu
_1ibc_start_main+0xe7 [1ibc=2.27-so]
[unknovn]
2 Origjin: This wss crested by Sasha Goidshtein and putblished on 7-feb-2016.
3 To ensure that frame pointer-based stack traces work and regular malloc routines are used, this bssh wss compiled
with CFLAG8==no=omi tfxane=ponter  /con.f 1gure -=vl.thout=gnu-ma11oc