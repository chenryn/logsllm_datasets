### Performance Comparison and Analysis

The unoptimized system outperforms our optimized system by only 9.3% on average, primarily because it does not need to manage the file system journal for each data persistence operation. However, as discussed in Section I, the integrity of the file system is crucial for user data. Trading less than 10% performance to ensure the safety and integrity of user data is a worthwhile compromise.

### SQLite Database Performance

We also evaluated the performance of the SQLite database, which is widely used by mobile applications for managing private data. As described in Section III.C, every database operation typically involves multiple synchronous writes to ensure the atomicity of transactions. Figure 15 shows the results of the SQLite benchmark [29]. For insert, update, and delete operations on the SQLite database, the optimized system outperforms the baseline system in Transactions Per Second (TPS) by 15%, 28.5%, and 22%, respectively.

**Figure 15: Speedup of SQLite Benchmark**

### Concurrent Multi-Programs

It is important to note that users may run applications concurrently in both the foreground and background. The performance impact on the I/O stack caused by background jobs can be modeled as introducing extra data and metadata blocks in the data persistence path of the foreground job. To evaluate the optimized system in a concurrent scenario, we chose a metadata-intensive program and a data-intensive program as background jobs.

**fdtree** [30] is a metadata-intensive benchmark that creates and deletes files and folders recursively. We executed each app while fdtree was running in the background. The results, shown in Figure 16, indicate an average improvement of 16.5% over the baseline, demonstrating that the optimized system can maintain its performance boost even when a metadata-intensive job is running in the background. This is because the large amount of metadata can be reduced by applying the journal coalition scheme.

Similarly, we used a data-intensive program as a background job, where the job continuously wrote 4KB data to a 4GB file. The results in Figure 16 show that the optimized system does not exhibit any performance improvement with a data-intensive background job. This is because, in this case, there is little redundancy to exploit, as the persistence of a large amount of data blocks dominates the runtime. Considering that real-world background jobs typically introduce a mix of data and metadata blocks, the optimized system shows a performance boost between these two extreme cases.

**Figure 16: Performance of Mobile Workloads with Metadata-Intensive and Data-Intensive Background Jobs**

### Overhead of Metadata Checkpointing

In general, checkpoint operations are asynchronous, except when there are no available blocks in the journal region, in which case the journal thread must wait synchronously for checkpointing. However, as the journal region size increases, checkpointing becomes less frequent, potentially alleviating the overhead. Therefore, we varied the journal region size and evaluated the performance compared to the default case (journal region size of 256MB). In this experiment, we used SQLite and Facebook as examples. The results, shown in Figure 17, indicate that performance improves as the journal region size increases, but the improvement plateaus at 512MB. This suggests that the overhead of metadata checkpointing can be mitigated by expanding the journal region size, which is feasible given the trend towards larger storage capacities.

**Figure 17: Performance of SQLite (Insert, Update, Delete) and Facebook When Varying Journal Region Size**

### Power Consumption

Power consumption is a key factor in the user experience of smartphones. We measured the system power consumption using a Wattâ€™s Up Pro [31] meter connected to the power supply of the Odroid-XU4 board. The results show that the average power consumption difference between the baseline and optimized systems is less than 1%. This demonstrates that dynamically enabling reliable and packed writes incurs minimal power consumption on mobile systems.

### Recovery Correctness and Overhead

To verify the correctness of recovery, we conducted experiments by manually cutting off the power supply of the Odroid-XU4 board during the execution of the PostMark [32] benchmark, which is a data-intensive test. PostMark execution can be divided into three phases: creating files, writing to them, and deleting all files. We used a large dataset (Table 5) to apply considerable pressure on the optimized system. The long execution time of each phase allowed us to cut off the power. After restoring power and restarting the system, the recovery procedure, as described in Section IV, was executed during the file system mounting stage of Android startup. We then used e2fsck [33] to check the integrity of the recovered file system. Repeating this procedure 10 times in each of the three phases, the results show that the recovered file system passed the e2fsck check every time.

**Table 5: PostMark Parameters**

| File Number | Transaction Number | File Size (MB) |
|-------------|--------------------|----------------|
| 10000       | 20000              | 1000~10240     |

The overhead of recovery arises from the need to read data blocks in the main file system region into memory to apply metadata changes. We induced power interruptions during the second phase (writing to files) of PostMark on both the optimized file system and the baseline Ext4, and measured the time for file system mounting. The results were 0.049s and 0.130s for the baseline and optimized systems, respectively. Given that the total Android boot time is 27s, the added overhead is negligible compared to the time required for self-checking and initializing various system services.

### Related Work

Many recent efforts have aimed to mitigate the overhead of file system journaling. We classify these endeavors into four categories:

1. **I/O Software Stack**: Studies [5, 6] analyzed the I/O traces of popular smartphone apps and observed that excessive synchronous writes and storage cache flushes occur when SQLite performs database transactions on the Ext4 file system. To address this, [6] investigated different combinations of file systems and database journal modes, presenting an optimal configuration to harmonize database transactions with file system journaling. [7, 10] attempted to solve the same problem by modifying the internal structures of SQLite. [8] proposed a file-adaptive method to dynamically adjust the Ext4 journaling mode based on I/O patterns. These studies focus on coordinating different software components to minimize performance overhead in the I/O stack. In contrast, our work leverages hardware features to eliminate the overhead of file system journaling, which is orthogonal to previous approaches.

2. **Transactional Flash**: Prior works [22, 34, 35] utilized customized SSDs to reduce file system and database journaling overhead. These approaches exploit the copy-on-write nature of flash media and make the FTL handle transactional updates. The FTL can complete transactional updates to a vector of blocks faster than purely software-based methods. The quantitative improvement can be estimated through the "no-journal" data series in Figure 13. Although FTL does extra work to achieve atomicity, the performance overhead is minimal compared to the "no-journal" results, as shown in [34]. However, these mechanisms target enterprise SSDs with capable microcontrollers and abundant DRAM, which increase power consumption and cost, making them less suitable for consumer electronics. Additionally, existing applications must be patched to enable transactional semantics. Our design, based on energy and cost-efficient flash storage, is transparent to applications and can benefit billions of eMMC-based devices.

3. **Non-Volatile Memory**: Prior works [39, 40] explored the use of non-volatile memory to reduce consistency overhead. The rationale is to leverage the non-volatile feature of NVM to place file system metadata without additional hardware or software efforts to prevent inconsistencies after a system crash. However, non-volatile memory has not been widely deployed due to issues such as interface compatibility and cost, making it infeasible for current mobile devices.

4. **DRAM-Based Solutions**: A prior work [28] proposed using battery-backed DRAM as quasi-non-volatile memory to make file system metadata quasi-safe. However, power failure is just one of many reasons for system crashes, and DRAM-based solutions cannot preserve important metadata in other scenarios, such as system bugs, unexpected execution paths, or user misbehaviors.

### Conclusion

In this study, we conducted a detailed breakdown analysis of the existing data persistence path in mobile systems, identifying the connection between performance overheads and the reliability characteristics of storage hardware. We leveraged eMMC features, including reliable write, packed command, and FUA, to improve the efficiency of data persistence while maintaining reliability. Evaluation results show that our solution improves the performance of mobile applications by 5-31%. We believe that leveraging eMMC hardware features in system-level optimization will open new avenues for future research.

### Acknowledgments

We thank all reviewers and Amer Qouneh for providing valuable feedback on our paper. This work is supported in part by NSF grants 1527535, 1423090, 1320100, 1117261, 0937869, 0834288, 0811611, 0720476, by SRC grants 2008-HJ-1798, 2007-RJ-0845721 (CAREER), 0916384, 1651G, by Microsoft Research Trustworthy Computing, Safe and Scalable Multicore Computing Awards, and by three IBM Faculty Awards.

### References

[References listed as provided in the original text]

---

This revised version aims to enhance clarity, coherence, and professionalism while preserving the original content and intent.