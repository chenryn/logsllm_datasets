Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:58:43 UTC from IEEE Xplore.  Restrictions apply. 
Vocabulary. We extract bag-of-word features for all stemmed
forms of the words in the training data. Stemming is the process
of producing morphological variants of a root word. This way
we can reduce redundancy, as word stems and their inﬂected
or derived words usually have the same meaning.
Capitalization Patterns. We use features to indicate the
different capitalization patterns of the original words (before
stemming). We consider a feature for each of the following
patterns: all letters are in lower case, all letters are capitalized,
the ﬁrst letter is capitalized, the word is in camel case, the
word has only symbols, the word has only numeric characters,
or the word has any other form of alpha-numeric capitalization.
Logical and Mathematical Expression Patterns. We identify
different patterns corresponding to logical and mathematical
expressions. These include assignments (x := a, x x ← a,
x = a), comparisons (a  b, a ≤ b, a ≥ b, a == b),
and arithmetic and algebraic expressions.
Dictionary Features. We include indicator features for a
held-out dictionary of reserved state and variable names.
Part-of-Speech Tags. We include part-of-speech (POS) tags
for all observed words (e.g. noun, verb, adjective). For
extracting POS tags, we use an off-the-shelf tagger.
Position Features. We use position and relative position
classiﬁcation.
indicators for each word in a chunk.
All of the features used are standard in general NLP pipelines.
For the LINEARCRF, this collection of features represents the
input xt for each textual unit t. For the NEURALCRF, we
concatenate the feature vector to the resulting vector ut from
the BERT encoder, before being inputted to the BiLSTM layer.
C. Post-Processing
We experiment with a set of rules to correct some easy cases
that the prediction models fail to identify. The rules are applied
on top of the classiﬁcation output, by ﬂipping labels in the
relevant cases. First, we look for textual units with mentions
to states. If the unit mentions a state, and there is a transition
verb (e.g. move, enter) or a directional preposition (e.g. to,
from), we label the unit as a transition span. Then, we look
for textual units with mentions to events. If the unit mentions
an event, and there is an action verb (e.g. send, receive), we
label the unit as an action span. Then, we label any remaining
unlabeled span with mentions to states or events as a trigger.
Finally, we label any remaining unlabeled spans with mentions
to variable names, “error” or “timer” as variable, error and
timer, respectively. We refer to the models that use these rules
as LINEARCRF+R and NEURALCRF+R.
Once the triggers, transitions, actions, variables, and errors
are identiﬁed, we use an off-the-shelf Semantic Role Labeler
(SRL) [39] to identify predicted actions as either send, receive,
or issue, depending on the verb used, as well as to identify
the segment in the text being sent, received, or issued. Seman-
tic Role Labeling consists of detecting semantic arguments
associated with the predicate or verb in a sentence, and their
classiﬁcation into their speciﬁc roles. For example, given a
sentence like “Send a SYN segment”, an SRL model would
identify the verb “to send” as the predicate, and “SYN segment”
as the argument. Identiﬁed arguments are then tagged using
the  tag introduced in Section III. We also use the SRL
output to identify transitions verbs such as enter and leave,
and identify the segment in the text being explicitly mentioned
as the source or target for this transition. For example, in
the sentence “client and server sockets enter this state from
PARTOPEN”, the SRL model identiﬁes the verb “to enter” as
the predicate, the segment “this state” as the argument and
“from PARTOPEN” as the directional modiﬁer. Arguments and
directional modiﬁers are then tagged as as ,
 or , depending on
the prepositions used.
In addition, we use exact lexical matching to identify explicit
mentions to states and events in the predicted sequences. We
keep track of the indentation in the original documents to infer
the scope of  statements. The resulting tagged
text constitutes the intermediary representation that will be
used for extracting the FSM.
VI. FSM EXTRACTION
The intermediary representation obtained using our LIN-
EARCRF or NEURALCRF model is not an FSM, thus we
need a procedure to extract an FSM from the intermediary
representation. The FSM is expressed as P = (cid:104)S, I, O, s0, T(cid:105)
with ﬁnite states S, ﬁnite inputs I, ﬁnite outputs O dis-
joint from I, initial state s0 ∈ S, and ﬁnite transitions
T ⊆ S × ({, timeout} ∪ (I ∪ O)∗) × S.
We extract the states S by scanning the intermediary represen-
tation for def-states. If one of the def-states contains
“initial” or “begin” in its body, we set the corresponding state
as the initial state s0; otherwise we just choose whichever is
the ﬁrst def-state in the document. We extract the inputs I
and outputs O by scanning for def-events where the type
is receive or send, respectively.
the
Although
intermediary
representation
contains
transition blocks, these blocks do not exactly contain
actual FSM transitions. Rather, they contain pointers for where
to look in the intermediary representation in order to guess the
source and target states, and labels, for the FSM transitions. A
transition block might describe no transitions at all, or
multiple transitions at once. It might describe only part of a
transition, for example the label and the target state, while the
rest is described somewhere else in its context. Such cases can
occur even with a perfect intermediary representation, because
of complex syntax and formatting used in the RFC text. To
obtain the transition set T we proceed in two steps: ﬁrst we
extract potential transitions from the transition blocks;
then we heuristically prune transitions that look like noise.
Potential Transition Extraction. We deﬁne an initially empty
set of possible transitions Tpos. For each transition block T
in the intermediary representation xml, we compute potential
transitions described in T using the Algorithm EXTRACTTRAN.
Brieﬂy, EXTRACTTRAN searches lower in the intermediary
representation to ﬁnd target states, and higher to ﬁnd source
states. It handles sentences like “starting at any state other
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:58:43 UTC from IEEE Xplore.  Restrictions apply. 
757
than CLOSED” using the set complement. It also handles
explicitly labeled intermediate states, so that sentences like “the
machine goes to CLOSED, then REQUEST, then PARTOPEN”
are interpreted as CLOSED −→ REQUEST −→ PARTOPEN rather
than CLOSED −→ PARTOPEN, REQUEST −→ PARTOPEN. It
uses the helper function EXTRACTTRANLBL to guess the
transition label (cid:96), recursing upward in the ancestry of T at
most six times until the result is well-formed. Pseudocode for
EXTRACTTRAN is given in the Appendix.
Heuristic Transition Pruning. After adding the potential
transitions extracted from each transition block T to a set
Tpos, we ﬁlter Tpos using three heuristics. First, we remove any
possible transition t ∈ Tpos that does not type-check, that is, for
which t /∈ S × ({, timeout}∪ (I ∪ O)∗)× S. Second, we apply
a “call and response” heuristic, where if Tpos contains some
y!−→ s(cid:48), then the latter
transitions s
two are discarded because they are likely noise generated by
the ﬁrst one. Third, we apply a “redundant epsilons” heuristic,
where if Tpos contains some transitions s −→ s(cid:48) and s (cid:96)−→ s(cid:48),
where (cid:96) (cid:54)= , then the -transition is discarded because it is
likely noise generated by the (cid:96)-transition. The transitions T is
the remaining ﬁltered set Tpos.
x?y!−−−→ s(cid:48), s x?−→ s(cid:48), and s
VII. TASK: ATTACKER SYNTHESIS
In this section we use attacker synthesis as an exemplifying
application for FSM extraction.
A. Attacker Synthesis
LTL program synthesis, also known as the LTL imple-
mentability problem, is to deduce for an LTL property φ if
there exists some program P that makes φ true. For example, φ
could be the homework assignment to implement multi-PAXOS,
and the program synthesis problem would be to automatically
compute a satisfying code submission. The problem is known
to be doubly exponential in the size of the property [40].
LTL attacker synthesis is slightly different. In this work we
consider a centralized attacker synthesis problem for protocols,
where the attacker has just one component. Other variations
on the problem are formulated in [18]. Suppose P (cid:107) Q is a
system consisting of some programs P and Q, and φ is an
LTL correctness property which is made true by the system;
that is P (cid:107) Q |= φ. Consider the threat model where Q
is the vulnerable part of the system. The attacker synthesis
problem is to replace Q with some new attacker A having
the same inputs and outputs as Q, such that the augmented
system behaves incorrectly, that is, P (cid:107) A violates φ. We only
consider attackers which succeed under the assumption that
(a) the attack eventually terminates, and (b) when the attack
terminates, the vulnerable program Q is run. The program
synthesizer must compute a program that satisﬁes φ in all of its
(non-empty set of) executions, but the attacker synthesizer only
needs to compute a program that violates φ in one execution.
B. Attacker Synthesis with KORG
KORG is an open-source attacker synthesis tool for protocols.
It requires three inputs: (1) a PROMELA program P representing
the invulnerable part of the system; (2) a PROMELA program
Q representing the vulnerable part of the system, as well as
its interface (inputs and outputs) in YAML format; and (3) a
PROMELA LTL property φ representing what it means for the
system to behave correctly. KORG computes ∃-attackers (at-
tackers for which there exists a winning execution) by reducing
the attacker synthesis problem to a model-checking problem
over the system P (cid:107) DAISY(Q), where the vulnerable program
Q is replaced with a nondeterministic search automaton (called
a Daisy Gadget) having the same interface as Q. The model-
checker then computes an execution that violates the correctness
property, and KORG projects the component of the execution
representing the gadget’s actions into a new PROMELA program,
which is the synthesized attacker [18].
PROMELA program P
PROMELA vulnerable
program Q
PROMELA LTL
correctness property φ
“P (cid:107) DAISY(Q) |= ψ?”
KORG
SPIN
Counterexamples
Synthesized Attackers
Fig. 6: KORG work-ﬂow. With our NLP pipeline, the user
need only supply the orange inputs and the system RFC. The
property ψ is automatically computed from φ to ensure the
attacker eventually terminates, at which point the original code
Q is run. The DAISY gadget is deﬁned in [18].
C. TCP and DCCP Attacker Synthesis with KORG
We focus on the TCP and DCCP connection establishment
and tear-down routines as representative protocols for attacker
synthesis. The TCP connection routine was previously studied
using the attacker synthesis tool KORG (Fig. 6); now we
conduct a similar analysis for both TCP and DCCP using
the same tool, but we automatically extract FSMs using NLP.
We want to show that the FSMs extracted from our NLP
pipeline can be used directly for attacker synthesis, alleviating
the considerable engineering effort required to hand-model the
system under attack. We show the effectiveness of the FSM
extraction on this task in Section VIII-B.
Our NLP pipeline and FSM extraction produce an FSM.
In order to use the extracted FSM for attacker synthesis, we
transpile it to PROMELA. For example, if we begin with the TCP
RFC, then the result will be a PROMELA program describing
the TCP connection routine.
For each of TCP and DCCP, we hand-write four LTL proper-
ties in PROMELA based on a close reading of the corresponding
RFC. Our TCP properties are given in Equation 3, and our
DCCP properties are given in Equation 4. We deﬁne the
vulnerable PROMELA program Q to be a generic message
channel between peers. For each of the four φi, we feed the
inputs P, Q, and φi to KORG and generate attackers. But how
do we know if these attackers are legitimate, since they were
generated with a potentially incorrect program P ? We solve
this by testing the attackers against a Canonical PROMELA
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:58:43 UTC from IEEE Xplore.  Restrictions apply. 
858
program. For TCP we adapt the Canonical program from [18].
For DCCP, no such program was available and we created our
own hand-written Canonical PROMELA program.
φ1 =“No half-open connections.”
φ2 =“Passive/active establishment eventually succeeds.”
φ3 =“Peers don’t get stuck.”
φ4 =“SYN_RECEIVED is eventually followed by
ESTABLISHED, FIN_WAIT_1, or CLOSED.”
θ1 =“The peers don’t both loop into being stuck or
inﬁnitely looping.”
θ2 =“The peers are never both in TIME_WAIT.”
θ3 =“The ﬁrst peer doesn’t loop into being stuck or
inﬁnitely looping.”
(3)
(4)
θ4 =“The peers are never both in CLOSE_REQ.”
Note that KORG expects that all its inputs (P , Q, and φ) are
correct. However, since we test on an automatically extracted
PROMELA program P , which may have some incorrect
transitions when compared to the corresponding Canonical
program, this assumption is broken. We therefore adapted
KORG to work on incomplete or imperfect programs, while
preserving the formal guarantees from the original paper (except
for soundness, which depends on how different the extracted
program is from the Canonical one).
VIII. EVALUATION
In this section we present an evaluation of both NLP tasks
and attacker synthesis.
We use “Gold intermediary representation” to refer to the
manual text annotations obtained using our protocol grammar
presented in Section III. We use “Canonical FSM” to refer to
the FSM which was derived from expert domain knowledge, the
protocol RFC, and FSM diagrams in textbooks and literature.
A. Information Extraction Evaluation
We evaluate how much of the intermediary representation
speciﬁed in Section III we can recover.
1) Methodology: We evaluate the output of the speciﬁcation
document parser in six different protocols: BGPv4, DCCP,
LTP, PPTP, SCTP and TCP. We use a leave-one-out setup, by
training on ﬁve protocols and testing on the remaining one. This
means that no portions of a test protocol are observed during
training. To artiﬁcially introduce more training sequences, we
split recursive control statements into multiple statements at
training time. At test time, we evaluate on each example once.
We evaluate predictions at the token-level and at the span-
level. For tokens, we have 19 labels: beginning and inside
tags for trigger, action, error, timer, transition and variable,
as well as the outside label. We use standard classiﬁcation
metrics to measure the token-level prediction performance. We
infer the control spans based on the indentation in the original
documents. For identifying event and state references, we do
direct lexical matching using a dictionary built on the deﬁnition
tags described in Section III-A.
TABLE I: Average Results for Different Models
Model
Rule-based
BERT-base
BERT-technical
LINEARCRF
LINEARCRF+R
NEURALCRF
NEURALCRF+R
Token-level
Span-level
Acc Weighted F1 Macro F1
31.08
58.93
62.38
58.95
58.60
64.42
62.79
25.94
56.72
60.31
56.61
56.79
64.18
62.50
29.37
51.33
52.50
49.58
50.62
54.95
53.64
Strict
41.58
60.77
62.84
63.98
63.52
68.81
66.22
Exact
41.78
84.18
83.81
85.65
85.18
86.83
86.10
To evaluate spans, we use the metrics introduced for the
International Workshop on Semantic Evaluation (SemEval)
2013 task on named entity extraction [41]. We use the SemEval
evaluation script on our data. In this case, we have six span
types, plus the outside tag. The metrics are outlined below.
1) Strict matching, with exact boundary and type.
2) Exact boundary matching, regardless of the type.
3) Partial boundary matching, regardless of the type.
4) Type overlap between the tagged span and the Gold span.
We use the LINEARCRF provided by the pystruct library
[42], which uses a structured SVM solver using Block-
coordinate Frank-Wolfe [43], and use the default parameters
during training. We implemented the NEURALCRF model
using the transformers library [44] and PyTorch [45], and
learn the model using the adaptive gradient algorithm Adam,
with decoupled weight decay [46]. We initialize the BERT
encoder with the parameters resulting from our pre-training
stage described in Section IV, which further pre-trains BERT
on technical documents. We use a learning rate of 2e-5 and
50 hidden units for the BiLSTM layer. For BERT, we use the
standard parameter settings, and a maximum sequence length
of 512. We randomly sample 10 percent of the training data to
set aside as a development set, which we use to perform early
stopping during training, using a patience of 5 epochs.
2) Segmentation strategies: We evaluate different segmenta-
tion strategies to create the base textual unit in our sequence-to-
sequence models: segmenting by token, chunk, and phrase. For
segmenting chunks, we use an off-the-shelf chunker [47]. For
segmenting phrases, we split the text on periods, colons, semi-
colons and newline markers, as well as on a set of reserved
words corresponding to conditional statements (e.g. if, then,
when, while). We ﬁnd that segmenting by chunks yields the
best token-level results (Weighted F1 of 61.25), but segmenting
by phrases gives us better span-level results (Strict matching of
63.98, and Exact boundary matching of 85.56). Detailed results
for these models are in the Appendix, in Table VI. Moving on,
all evaluations are done using the phrase segmentation strategy.
3) Extraction models: We evaluate the two models proposed
in Section V, and obtain a signiﬁcant improvement with respect
to a rule-based baseline that applies the rules outlined in
Section V-C directly, without any learning. In addition, we
test a BERT model by removing the BiLSTM CRF Layer,
both with and without the pre-training strategy introduced in
Section IV. Average results can be observed in Table I. When
pre-training on technical documentation is not done, we use the
BERT model trained on BookCorpus and Wikipedia. Here, we
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:58:43 UTC from IEEE Xplore.  Restrictions apply. 
959
can appreciate both the advantage of the technical embeddings,
as well as the advantage of the BiLSTM CRF layer. We ﬁnd
that leveraging expressive neural representations for sequence-
to-sequence models is advantageous for this task. Note that
both the NEURALCRF and the LINEARCRF models make use
of the full set of features introduced in Section V-B. Finally,