spam messages, stealthy attack slightly decreases the sys-
tem’s the true positive rate. Nonetheless, the false positive
rate does not increase, which means that the system is still
“safe” against legitimate messages under such attack.
5.5 Run Time Performance
The run time performance is critical, since our system
must not be the bottleneck that slows down the OSN plat-
form if deployed. The training time is only a few seconds
in all the experiments. In addition, we measure the latency
and throughput to demonstrate that our system can respond
to an incoming message very quickly and that it can inspect
large number of messages per second.
Latency
The latency is measured as the time between
when the system receives a message and when the system
outputs the inspection result. We process all the messages
sequentially in this experiment. Figure 17 shows the cumu-
lative distribution of the system latency, measured in mil-
liseconds. The average and median latency for the Face-
 100
 80
 60
 40
 20
)
F
D
C
(
s
e
g
a
s
s
e
m
f
o
%
 0
 0
Facebook data
Twitter data
 100
 200
 300
 400
 500
Latency (ms)
Figure 17: The cumulative distribution of the system la-
tency, measured in milliseconds.
book dataset is 21.5ms and 3.1ms, respectively. The aver-
age and median latency for the Twitter dataset is 42.6ms
and 7.0ms, respectively. Even for the Twitter dataset that
results in comparatively longer processing delay, the whole
inspection process for over 90% of messages is completed
within 100ms. Given that modern browsers spend several
hundred milliseconds before they start to render ordinary
webpages [17], the system latency is sufﬁciently low. An
OSN user would not feel any noticeable additional delay
due to the deployment of our system.
Throughput
In order to test the throughput, we feed
the system with the testing set as fast as possible. Due
to the hardware limitation (8 physical cores with Hyper-
Threading), we execute 16 threads simultaneously. We di-
vide the total number of messages processed by the system
by the total running time to calculate the average through-
put. The throughput on the Facebook and the Twitter dataset
is 1580 messages/sec and 464 messages/sec, respectively.
6 Discussion
Spammers continuously rival anti-spam solutions. Al-
though our system is demonstrated to be robust against
stealthy attacks, other approaches might be adopted to
evade it. One possible way is to tamper with the cluster-
ing process by reducing the syntactic similarity among mes-
sages from the same campaign. We have observed such at-
tempts in both of our datasets.
In the Facebook dataset,
spam campaigns start to include obfuscation in the textual
message, e.g. , using “proﬁl imaage” instead of “proﬁle im-
age”. In the Twitter dataset, almost all spam tweets con-
tain random meaningless words, which we suspect to be
obtained from a collection of popular words. Nonetheless,
our system defeats these attempts and yields good detection
accuracy. Meanwhile, obfuscation and embedded random
chunks decrease the message readability as well as trigger
the recipients’ suspicion, which lowers the underlying con-
version rate. Another way to evade the system would be
to manipulate the features of spam clusters, for the purpose
of making them indistinguishable from legitimate clusters.
It is not too difﬁcult to do this for each individual feature.
For example, spammers may use fake accounts with lower
social degree to tamper with the “sender social degree” fea-
ture. In addition, they may only send spam to the friends
that the compromised accounts frequently interact with to
tamper with the “interaction history” feature. They may
also send spam with longer time interval to tamper with
the “average time interval” feature. However, manipulating
any individual feature is likely to have very limited effect,
since our system uses all six features in combination. Even
though deliberate spammers in theory may manipulate all
the features simultaneously, they have to pay a heavy price
for doing so. In the previous three examples, the spam re-
cipient population becomes much smaller, and the speed of
spam generation becomes slower by orders of magnitude.
If we can force spammers to do that, it is already a vic-
tory. The third evasion technique is to produce image-based
spam. Our current design does not work for image-based
spam.
7 Related Work
We discuss prior related work by organizing them into
two general categories: studies of spamming problem in
OSNs and in other environments.
Spam Studies in OSNs.
Stein et al. present the
framework of the adversarial learning system that performs
real-time classiﬁcation on read and write action in Face-
book [25]. However, the lack of details, e.g. , the fea-
tures and policies used, and performance results prevent us
from further comparison. Two ofﬂine studies have revealed
large-scale spam campaigns in Twitter and Facebook, re-
spectively [9, 10]. Designed as ofﬂine analysis tools, none
of them can be directly used for online spam detection. [10]
is based on URL blacklists which have too long lag-time to
protect a signiﬁcant number of users. [9] uses a similar clus-
tering technique. However, it needs to operate on the com-
plete set of messages, and its efﬁciency limits the cluster-
ing to be performed on the 2 million messages with URLs.
In comparison, we adopt incremental clustering and par-
allelization to enable the system to inspect messages only
based on other messages observed in the past, as well as
to increase the scalability. Also, we develop the feature
set to distinguish spam cluster that can be efﬁciently com-
puted online. Thomas et al. [27] propose to ﬁlter malicious
URLs in OSNs in real-time, which can be used to identify
malicious messages later. While their approach does deep
analysis of the URLs’ landing pages, our approach uses an
[26] , Lee et al.
[14] and Yang et al.
alternative information source for the investigation, i.e. , the
message content. Song et al. propose to use sender-receiver
relationship to classify twitter messages [24]. Stringhini et
[31] use ma-
al.
chine learning techniques to detect spamming bots in OSNs,
which are accounts created by spammers and used exclu-
sively for spamming. Yardi et al. use a collection of ad-hoc
criteria to identify spamming bots on Twitter [32]. In com-
parison, our detection focuses on spam messages instead of
accounts, so that we can secure OSNs from spams generated
by both spamming bots and previously legitimate accounts
that have been compromised. Additionally, Benevenuto et
al. [5] and Markines et al. [16] apply supervised machine
learning to detect spammers in Youtube and social book-
marking sites, respectively. These websites do not focus on
communications among users. Instead, their key function-
ality is video and bookmark sharing. Hence, the feature set
used for machine learning is very different.
Other Spam Studies. There is a large body of prior work
studying the characteristics of email spam [4, 12, 13, 35].
However, few of them can potentially be used as online de-
tection tools. Researchers propose to enhance IP blacklists
by correlating IP addresses according to their email sending
history, network-level information, blacklisting history and
so on, in order to discover spamming IP addresses that are
not yet blacklisted [19, 22, 28]. As discussed previously,
sender reputation based approaches are not suitable for
OSN spam detection. Pitsillidis et al. extract the underly-
ing templates to match future spams [18]. Li et al. propose
to enhance Bayesian ﬁlter by adjusting the weight of spam
keywords using personal social network information [15].
Both approaches detect campaigns that have been contained
in the training set. Our approach does not have such a re-
striction. Xie et al. generate regular expression signatures
for spamming URLs [30]. In addition, Thomas et al. use a
comprehensive list of features to determine whether a given
URL directs to spam in real-time [27]. The above two ap-
proaches essentially detect spamming URLs.
In compar-
ison, our approach can detect spam messages even if no
URL can be recognized in them due to the obfuscation tech-
niques.
8 Conclusions
In this paper, we describe our work to provide online
spam ﬁltering for social networks. We use text shingling
and URL comparison to incrementally reconstruct spam
messages into campaigns, which are then identiﬁed by a
trained classiﬁer. We evaluate the system on two large
datasets composed of over 187 million Facebook wall mes-
sages and 17 million tweets, respectively. The experimental
results demonstrate that the system achieves high accuracy,
low latency and high throughput, which are the crucial prop-
erties required for an online system. In addition, the system
is able to remain accurate for more than 9 months after the
training phase, which shows its very low maintenance cost
after deployment. For more information, please refer to the
project web page at http://list.cs.northwestern.edu/.
Acknowledgments
We express our sincere thanks to the anonymous re-
viewers for their valuable feedback.
The authors de-
noted with † are supported by NSF award numbers CCF-
0621443, OCI-0724599, CCF-0833131, CNS-0830927,
IIS-0905205, OCI-0956311, CCF-0938000, CCF-1043085,
CCF-1029166 , and OCI-1144061.
References
[1] Users of social networking websites face malware and
phishing attacks. Symantec.com Blog.
[2] What the trend. http://www.whatthetrend.
com/.
[3] Zeus
botnet
targets
facebook.
http:
//blog.appriver.com/2009/10/
zeus-botnet-targets-facebook.html.
[4] ANDERSON, D. S., FLEIZACH, C., SAVAGE, S.,
AND VOELKER, G. M. Spamscatter: characterizing
internet scam hosting infrastructure. In Proceedings of
16th USENIX Security Symposium on USENIX Secu-
rity Symposium (Berkeley, CA, USA, 2007), USENIX
Association, pp. 10:1–10:14.
[5] BENEVENUTO, F., RODRIGUES, T., AND ALMEIDA,
V. Detecting spammers and content promoters in on-
line video social networks. In Proc. of SIGIR (Boston,
Massachusetts, USA, July 2009).
[6] BOGU ˜N ´A, M., PASTOR-SATORRAS, R., AND
VESPIGNANI, A. Epidemic spreading in complex net-
works with degree correlations.
[7] BRODER, A. Z., GLASSMAN, S. C., MANASSE,
M. S., AND ZWEIG, G. Syntactic clustering of the
web. Comput. Netw. ISDN Syst. 29 (September 1997),
1157–1166.
[8] BURGES, C. J. C. A tutorial on support vector ma-
chines for pattern recognition. Data Min. Knowl. Dis-
cov. 2 (June 1998), 121–167.
[9] GAO, H., HU, J., WILSON, C., LI, Z., CHEN, Y.,
AND ZHAO, B. Y. Detecting and characterizing social
spam campaigns. In Proceedings of the 10th annual
conference on Internet measurement (New York, NY,
USA, 2010), IMC ’10, ACM, pp. 35–47.
[10] GRIER, C., THOMAS, K., PAXSON, V., AND
ZHANG, M. @spam: the underground on 140 charac-
ters or less. In Proceedings of the 17th ACM confer-
ence on Computer and communications security (New
York, NY, USA, 2010), CCS ’10, ACM, pp. 27–37.
[11] HAO, S., SYED, N. A., FEAMSTER, N., GRAY,
A. G., AND KRASSER, S. Detecting spammers with
snare: spatio-temporal network-level automatic repu-
tation engine. In Proceedings of the 18th conference
on USENIX security symposium (Berkeley, CA, USA,
2009), SSYM’09, USENIX Association, pp. 101–118.
[12] KANICH, C., KREIBICH, C., LEVCHENKO, K., EN-
RIGHT, B., VOELKER, G. M., PAXSON, V., AND
SAVAGE, S. Spamalytics: An empirical analysis of
spam marketing conversion. In Proc. of the ACM Con-
ference on Computer and Communications Security
(October 2008).
[13] KREIBICH, C., KANICH, C., LEVCHENKO, K., EN-
RIGHT, B., VOELKER, G., PAXSON, V., AND SAV-
AGE, S. Spamcraft: An inside look at spam campaign
orchestration. In Proc. of LEET (2009).
[14] LEE, K., CAVERLEE, J., AND WEBB, S. Uncovering
social spammers: social honeypots + machine learn-
ing. In Proceeding of the 33rd international ACM SI-
GIR conference on Research and development in in-
formation retrieval (New York, NY, USA, 2010), SI-
GIR ’10, ACM, pp. 435–442.
[15] LI, Z., AND SHEN, H. SOAP: A Social Network
Aided Personalized and Effective Spam Filter to Clean
Your E-mail Box. In Proceedings of the IEEE INFO-
COM (April 2011).
[16] MARKINES, B., CATTUTO, C., AND MENCZER, F.
Social spam detection. In Proc. of AIRWeb (2009).
[17] MOSHCHUK, A., BRAGIN, T., DEVILLE, D., GRIB-
BLE, S. D., AND LEVY, H. M. Spyproxy: execution-
based detection of malicious web content.
In Pro-
ceedings of 16th USENIX Security Symposium on
USENIX Security Symposium (Berkeley, CA, USA,
2007), USENIX Association, pp. 3:1–3:16.
[18] PITSILLIDIS, A., LEVCHENKO, K., KREIBICH, C.,
KANICH, C., VOELKER, G., PAXSON, V., WEAVER,
N., AND SAVAGE, S. Botnet Judo: Fighting Spam
with Itself .
In Proceedings of the 17th Annual
Network and Distributed System Security Symposium
(NDSS) (San Diego, CA, USA, March 2010).
[19] QIAN, Z., MAO, Z. M., XIE, Y., AND YU, F. On
Network-level Clusters for Spam Detection . In Pro-
ceedings of the 17th Annual Network and Distributed
System Security Symposium (NDSS) (San Diego, CA,
USA, March 2010).
[20] QUINLAN, J. R. Ross quinlan’s personal homepage.
http://www.rulequest.com/Personal/.
[21] QUINLAN, J. R. Induction of decision trees. Mach.
Learn. 1 (March 1986), 81–106.
[22] RAMACHANDRAN, A., FEAMSTER, N., AND VEM-
PALA, S. Filtering spam with behavioral blacklisting.
In Proceedings of the 14th ACM conference on Com-
puter and communications security (New York, NY,
USA, 2007), CCS ’07, ACM, pp. 342–351.
[23] SHIN, Y., GUPTA, M., AND MYERS, S. Prevalence
and mitigation of forum spamming. In Proceedings of
IEEE International Conference on Computer Commu-
nicationi (INFOCOM) (Shanghai, China, 2011), IEEE
Computer Society.
[24] SONG, J., LEE, S., AND KIM, J. Spam ﬁltering in
twitter using sender-receiver relationship. In Proceed-
ings of the 14th International Symposium on Recent
Advances in Intrusion Detection (RAID’11)) (Septem-
ber 2011).
[25] STEIN, T., CHEN, E., AND MANGLA, K. Facebook
immune system. In Proceedings of the 4th Workshop
on Social Network Systems (SNS’11) (New York, NY,
USA, 2011), ACM.
[26] STRINGHINI, G., KRUEGEL, C., AND VIGNA, G.
Detecting spammers on social networks. In Proceed-
ings of the 26th Annual Computer Security Applica-
tions Conference (New York, NY, USA, 2010), AC-
SAC ’10, ACM, pp. 1–9.
[27] THOMAS, K., GRIER, C., MA, J., PAXSON, V., AND
SONG, D. Design and Evaluation of a Real-Time URL
Spam Filtering Service. In Proceedings of the IEEE
Symposium on Security and Privacy (May 2011).
[28] WEST, A. G., AVIV, A. J., CHANG, J., AND LEE,
I. Spam mitigation using spatio-temporal reputations
from blacklist history.
In Proceedings of the 26th
Annual Computer Security Applications Conference
(New York, NY, USA, 2010), ACSAC ’10, ACM,
pp. 161–170.
[29] WILSON, C., BOE, B., SALA, A., PUTTASWAMY,
K. P., AND ZHAO, B. Y. User interactions in social
networks and their implications.
In Proceedings of
the ACM European conference on Computer systems
(2009).
[30] XIE, Y., YU, F., ACHAN, K., PANIGRAHY, R., HUL-
TEN, G., AND OSIPKOV, I. Spamming botnets: sig-
natures and characteristics.
In Proc. of SIGCOMM
(2008).
[31] YANG, C., HARKREADER, R., AND GU, G. Die free
or live hard? empirical evaluation and new design for
ﬁghting evolving twitter spammers. In Proceedings of
the 14th International Symposium on Recent Advances
in Intrusion Detection (RAID’11)) (September 2011).
[32] YARDI, S., ROMERO, D., SCHOENEBECK, G., AND
BOYD, D. Detecting spam in a twitter network. First
Monday 15, 1 (2010).
[33] ZADROZNY, B., LANGFORD, J., AND ABE, N.
Cost-sensitive learning by cost-proportionate exam-
ple weighting. In Proceedings of the Third IEEE In-
ternational Conference on Data Mining (Washington,
DC, USA, 2003), ICDM ’03, IEEE Computer Society,
pp. 435–.
[34] ZHAO, Y., XIE, Y., YU, F., KE, Q., YU, Y., CHEN,
Y., AND GILLUM, E. Botgraph: large scale spam-
ming botnet detection.
In Proceedings of the 6th
USENIX symposium on Networked systems design and
implementation (Berkeley, CA, USA, 2009), USENIX
Association, pp. 321–334.
[35] ZHUANG, L., DUNAGAN, J., SIMON, D. R., WANG,
H. J., AND TYGAR, J. D. Characterizing botnets
from email spam records.
In Proceedings of the 1st
Usenix Workshop on Large-Scale Exploits and Emer-
gent Threats (Berkeley, CA, USA, 2008), USENIX
Association, pp. 2:1–2:9.
[36] ZOU, C. C., TOWSLEY, D., AND GONG, W. Mod-
eling and simulation study of the propagation and de-
fense of internet e-mail worms. IEEE Trans. Depend-
able Secur. Comput. 4 (April 2007), 105–118.