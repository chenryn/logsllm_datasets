### Pilot Experiments

To determine the minimum number of ratings required for anomaly detection, we set a threshold. We found that extensions with fewer than ten comment-and-rating combinations did not provide sufficient ground truth to avoid false positives. Conversely, requiring hundreds of comments before an extension can be analyzed would limit the applicability of our system, especially for benign extensions with a single developer and a small but loyal user base. Such extensions are particularly vulnerable to abuse if the developer decides to sell the extension [35] or if the account is compromised [11].

We experimented with different training sets, starting with a set of ten ratings and iteratively increasing the size while monitoring Anomalize's performance. Through trial and error‚Äîchoosing a number of ratings, observing anomalies, and manually reviewing them‚Äîwe determined that an extension must have at least 50 rating-comment combinations for Anomalize to reliably flag true positive anomalies (i.e., extensions that have indeed turned malicious). Due to the skewed distribution of comments (as shown in Figure 2), this threshold limited our analysis to 2,256 unique extensions with 50 or more ratings.

### Comments Analysis

We also gathered the same number of comments as the number of ratings, resulting in over 1.5 million comments. We analyzed these comments for signs of malware-related complaints. First, we grouped the comments into five rating categories (1 to 5) and extracted the most popular keywords from the lowest-rated groups (1 and 2). To augment our list of malicious-extension-related keywords, we used the keywords identified by Li et al. [25], who mined comments on Android apps to identify malicious ones. Our approach is similar, but we use review mining to obtain malicious extension seeds rather than as an end-to-end system for detecting malicious software.

Using these two sources of keywords, we calculated a "trustworthiness" score for each keyword, associating it with the average rating of the reviews containing it. Lower trustworthiness scores indicate a higher likelihood of being present in malicious extensions. The keywords with the lowest average trustworthiness scores were SCAM, MALICIOUS, and ADWARE. By analyzing comments containing these keywords, we extracted additional keywords such as SPY, MALWARE, VIRUS, SPYWARE, and HIJACK.

Interestingly, although certain types of keywords, such as SENSITIVE DATA and PROXY, were not among the top ten with the lowest trustworthiness scores, they still led us to real malicious extension cases. We hypothesize that users leaving these types of comments are experts, providing a higher signal-to-noise ratio compared to more generic malicious keywords like SCAM. Table 11 in the Appendix provides the top 20 keywords based on their trustworthiness scores.

### Combination of Comments and Ratings

By examining a small subset of extensions flagged by the two systems, we observed that the majority were false positives. On the ratings anomaly-detection front, this was due to certain versions introducing bugs, leading to low-score reviews. When these bugs were corrected, the ratings returned to normal. False positives in the keyword extraction system were due to either bots flooding a competitor‚Äôs extension or particularly unhappy users leaving comments with low-trustworthiness keywords (e.g., "scam") because they did not get what they expected.

These findings, combined with the 1-1 ratio correspondence between keywords and ratings, led us to combine the two systems and flag extensions only when both systems agreed, significantly reducing false positives. This also meant that fewer extensions could be analyzed, as they now needed to meet multiple criteria. With both components in place (anomaly detection and malicious keywords), we identified 45 seed malicious extensions. Table 2 shows the number of extensions remaining at each step of the process. We elaborate on Stage 1 in Section 5.1.

### Stage 2: Clustering of JavaScript Version Differences

In Stage 2, we use the extensions marked as malicious in Stage 1 to identify the code update that corresponds to the transition from benign to malicious. We encode this update in terms of critical APIs and search for other extensions with similar updates to identify potential new malicious extensions.

#### All APIs

Extension developers can use two types of APIs: Extension APIs, which are privileged and can only be used in an extension environment, and native JS APIs, which are available to both extensions and visited web pages. We parsed Chromium‚Äôs source code IDL files to identify 969 Extension APIs, including popular ones like cookies, bookmarks, and tabs. We also identified 1,266 native JS APIs, including write, text, slice, and isURL. Additionally, we used common APIs from Google Publisher Tag (GPT) to identify ad-injection-specific abuse.

By analyzing known malicious extensions, we distilled a set of 55 APIs that were abused by at least 90% of the malicious extensions (Table 3). This reduced set includes six different methods of injecting code into a webpage (Table 4), allowing us to measure the accuracy of clustering known and unknown malicious extensions with lower computational cost.

#### API Sequencing

Given our list of APIs, we approach the update of a browser extension as a change in the number of APIs used and their parameters. This allows us to focus on significant operations (such as code injection or cookie exfiltration) rather than obfuscated local variables and function calls. We analyze the APIs used in each extension by generating a sequence of them using the Esprima library [43]. We parse the JS files at the token level, split the tree into tokens, and generate sequences for each file. We remove user-defined functions and procedural code, keeping only the API methods used. We discard duplicates and consider the order of APIs to differentiate between methods with the same name. We apply this procedure to every version of every extension, creating pairs of updates, isolating code differences, and extracting API sequences for clustering.

#### Algorithm - DBSCAN

To cluster the extracted API sequences, we use the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm [1]. DBSCAN creates cores by searching each neighborhood within an epsilon distance for samples, calculates the connected components of the cores, and clusters the remaining non-core points based on the distance to the closest connected component. In the context of security and the web, DBSCAN has been successfully used by Das et al. [13] to cluster JavaScript scripts that access sensor values for user re-identification via browser fingerprinting.

#### Cluster Analysis System

We compared various sets of APIs (native, extension, and a set of 55 selected APIs based on malicious behavior) based on the results produced by clustering. We applied the clustering algorithm to both the entire set and the reduced set of APIs, evaluating different configurations for DBSCAN based on the metric distance (e.g., Euclidean vs. Dice), epsilon distance, and the number of members in a cluster. We chose the configuration that minimized both the mean silhouette score and the noise points, leading to the selection of the 55-API set due to the sparse population of clusters with all APIs, resulting in unoptimized clusters with higher mean silhouette scores.

A high-level overview of how we used DBSCAN to create clusters from our API sequences is shown in Figure 5. Each table represents the total APIs found for a particular set of extension versions. We calculate the distance (d) between two encoded API tables using metric algorithms (e.g., Euclidean or Dice distance). Two extension versions cluster together if this distance is less than the specified epsilon (ùúÄ) distance. We carefully selected the appropriate epsilon distance to avoid artificially clustering thousands of extensions together or creating individual clusters.

In a post-clustering phase, we removed jQuery clusters, as we noticed a pattern of cluster creation through jQuery grouping. To ensure we did not remove any malicious extensions, we performed a hash analysis on the jQuery files. We gathered all versions of jQuery from the official website, hashed them, and removed 733 jQuery clusters that did not exhibit malicious behavior. We also removed clusters with only one extension ID. After clustering, we examined the clusters containing at least one known malicious extension from the seeds identified in Section 4.1.

#### Real-Time Detection System

As a final step, we added a crawling-based verification component. For every extension clustered with a known malicious extension, we check its status and comments daily from the extension store, looking for take-downs or user feedback confirming malicious behavior.

### Results

Our system for detecting extensions that start exhibiting malicious behavior consists of two stages: identifying seed extensions via anomalies in comments and ratings, and clustering extension deltas based on API changes. Table 5 summarizes the malicious categories of the 45 seed extensions, and Table 6 provides summarized results on ratings, comments, and users.

In conclusion, our system effectively identifies and clusters malicious extensions, providing a robust framework for real-time detection and analysis.