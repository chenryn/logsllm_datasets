pilot experiments. We set the threshold as the minimum number
of ratings an extension should have, in order for anomaly detec-
tion to be possible. We quickly identified that extensions with fewer
than ten comment-and-rating combinations, did not provide enough
ground truth to avoid false positives. At the same time, demanding
hundreds of comments before an extension can be analyzed would
restrict the applicability of the anomaly-detection phase of our sys-
tem, particularly since benign extensions with a single developer
and a small-but-faithful userbase, may be the most prone to abuse, if
their developer decides to sell their extension [35] or their account
gets compromised [11].
We experimented with different training sets for the model, start-
ing with a training set of ten ratings and iteratively increasing the
training set, while monitoring the results of Anomalize. Through
repeated trial-and-error (i.e. choosing a number of ratings for initial
training of the model, observing the anomalies spotted by Anomal-
ize, and manually reviewing these anomalies), we discovered that an
extension had to have at least 50 rating-comment combinations, for
Anomalize to be able to flag anomalies that were true positives (i.e.
belonged to extensions that had indeed turned malicious). Because of
the skewed distribution of comments (as shown earlier in Figure 2),
the choice of this threshold necessarily limited our analysis to 2,256
unique extensions that had 50 or more ratings.
4.1.2 Comments. We also gathered the same number of comments
as the number of ratings, resulting to more than 1.5 million com-
ments which we analyzed searching for signs of malware-related
complaints. First, using their ratings, we grouped the comments into
five groups (1 to 5) and we extracted the most popular keywords from
groups 1 and 2, i.e., the groups with the lowest ratings. To augment
our list of malicious-extension-related keywords, we also used the
keywords identified by Li et al. [25], who mined comments on An-
droid apps in order to identify malicious ones that prompted users to
leave comments warning other users. Our approach is similar but we
use review mining just for obtaining malicious extension seeds, as
12345Rating0200,000400,000600,000800,0001,000,0001,200,000Number of Ratings275,99818.2%44,9883.0%69,1134.6%174,51111.5%953,88262.8%05No Anomaly05User RatingsPositive Anomaly012345Time in Months05Negative AnomalyTotal Extensions
Stage of the User Feedback Experiment No. Extensions
152,000
4,826
1,386
1,247
850
550
191
45
Comments Majority
Anomaly detected
Negative Anomaly
Keywords filtered
Both Versions crawled
Update Date matched
Initial Malicious Seeds
Table 2: User Feedback - Extensions in each Stage
opposed to using it as an end-to-end system for detecting malicious
software.
By using these two sources of keywords, we then calculated a
‚Äútrustworthiness‚Äù score of every keyword, which effectively associ-
ated the keyword with the average rating of the reviews that con-
tained it. Lower trustworthiness scores are therefore more likely
to be present on malicious extensions. The lowest average trust-
worthiness ratings belong to the keywords SCAM, MALICIOUS, and
ADWARE. By analyzing the comments including these keywords, we
extracted additional keywords, including: SPY, MALWARE, VIRUS,
SPYWARE, and HIJACK.
Interestingly, we notice that although certain types of keywords,
such as, SENSITIVE DATA and PROXY, were not in the top ten key-
words with the lowest trustworthiness scores, they still led us to real
malicious extensions cases. We reason that users who leave these
types of comments are experts, and therefore their comments have
a higher signal-to-noise ratio compared to more generic malicious
keywords, such as, SCAM. Table 11 (in the Appendix), provides the
top 20 keywords, according to their trustworthiness scores.
4.1.3 Combination of comments and ratings. By examining a small
subset of extensions marked by the two previous systems, we ob-
served that the majority of them were false positives. On the rat-
ings anomaly-detection front, this was due to certain versions of
extensions introducing bugs, thereby breaking functionality that
users relied on and leading users to leave low-score reviews. In
later versions, when these bugs were corrected, the ratings of these
previously-buggy extensions moved back up to their typical scores.
On the other hand, false positives on the keyword extraction system
were due to either bots flooding a competitor‚Äôs extension (we discuss
this more in Section 7) or particularly unhappy users leaving com-
ments that included low trustworthiness keywords (such as ‚Äúscam‚Äù)
because they did not get what they expected from an extension.
These findings, combined with the fact that keywords and ratings
have a 1-1 ratio correspondence, led us to the decision to combine
these two systems and flag extensions only when they have been
flagged by both systems, thus providing far less false positives at
the end of the Stage 1 analysis. Unfortunately, this also meant that
our mining approach could be applied on fewer extensions since
these extensions now need to satisfy multiple requirements. Over-
all, with these two components in place (anomaly detection and
malicious keywords), we identified the extensions that had ratings
anomalies and contained one or more of our curated, malicious-
extension-related keywords. By manually analyzing these flagged
extensions, we identified a total of 45 seed malicious extensions.
Table 2 shows the extensions that remain on each step of this pro-
cess. We elaborate on the details of Stage 1 in Section 5.1. Note that
this dataset reduction, while significant, only affects the first step
Category
No. APIs
Web APIs
DOM Manipulation
Browser Interaction
Storage Access
Object Handling
Web Tracking
Geolocation
Bookmarks
Functions
Total
14
12
12
6
4
3
2
1
1
55
Examples
innerHTML
xhr.executeScript
cookies.get
createElement(‚Äôscript‚Äô)
tabs.executeScript
document.addEventListener
chrome.downloads
chrome.webRequest
runtime.onConnect
storage.get
localstorage.set
addService
JSON.stringify
googleTag.defineSlot
trackStatusEvent
location.hostname
location.replace
bookmark.getTree
eval
‚Äì
Table 3: List of 55 APIs used for Similarity Comparison
our approach, i.e., the identification of seed malicious extensions.
Once our system identifies the JavaScript APIs that are commonly
abused in malicious-extension updates, their update signatures can
be matched against all extensions in our dataset.
4.2 Stage 2: Clustering of JavaScript version
differences
In Stage 2 of our approach, we use the extensions that were marked
as malicious by Stage 1, and we identify the code update, that corre-
sponds to the extension turning from benign to malicious. We encode
this update in terms of critical APIs, and search for other extensions
with similar updates. Through this process, we identify other exten-
sions that exhibit similar signs of malicious updates but have not yet
been flagged, neither by users nor by the Chrome Web Store.
4.2.1 All APIs. The different APIs that an extension developer can
utilize belong in two categories. Extension APIs, consist of privileged
APIs that can only be used in an extension environment, whereas
native JS APIs, are the typical JavaScript APIs available to both ex-
tensions as well as visited web pages. We obtained a list of all the
Extension APIs available in Chromium‚Äôs source code by parsing
its IDL files [3]. In total, we identified 969 Extension APIs, includ-
ing popular APIs such as, cookies, bookmarks, and tabs which are
commonly used by extension developers. Similarly, we identified
1,266 native JS APIs [4] including popular APIs such as, write, text,
slice, and isURL. The only API that we used that does not belong to the
above categories is related to advertisements, in order to identify ad-
injection-specific abuse. For this, we used common APIs from Google
Publisher Tag (GPT), which is Google‚Äôs ad tagging library [26].
Byanalyzingthealreadyknownmaliciousextensions,wedistilled
a set of 55 APIs that were abused by at least 90% of the malicious
extensions (shown in Table 3). Among others, this reduced set in-
cludes six different ways of injecting code in a webpage, as shown in
Table 4. This reduced set of APIs gives us the ability to measure the
accuracy of clustering known malicious extensions with unknown
malicious extensions, at a smaller computational cost.
Type
document
document
element
tabs
event
xhr object
innerHTML
Method
createElement(‚Äòscript‚Äô)
write(‚Äòscript‚Äô)
appendchild
executeScript
addEventListener
xhr.send
text
Table 4: Script Injection Methods
4.2.2 API Sequencing. Given our list of APIs, we approach the up-
date of a browser extension, as a change in the number of APIs
used and their parameters. This allows us to focus on the operations
that matter (such as the injection of new code, or the access and
exfiltration of cookies) as opposed to locally named variables and
function calls that can be straightforwardly obfuscated. We analyze
the APIs used in each extension by generating a sequence of them,
taking advantage of the high-level and low-level syntactical analysis
provided by the Esprima library [43] as follows.
We parse the JS files from a token-level perspective, splitting the
treeintokensandgeneratingsequencesofthoseforeachfile.Wethen
remove user-defined functions and procedural code, in order to have
a sequence of tokens with only API methods used. We discard dupli-
cates of the same API, as we only keep the information of whether an
APIispresentornot.WeconsidertheorderofAPIsbecauseeveryAPI
has its own methods and some APIs may share method names (e.g. if
write comes after document, this confirms the use of document.write()
and not some other API with a write method). We apply that afore-
mentioned procedure to every version of every extension in our sys-
tem, creating pairs of updates (e.g. extensionùë£ùëñ and extensionùë£ùëñ+1),
isolating their code differences, extracting the sequence of APIs
using Esprima, and storing these sequences for later clustering.
4.2.3 Algorithm - DBSCAN. To cluster the extracted sequences of
APIs, we use the Density-Based Spatial Clustering of Applications
with Noise (DBSCAN) algorithm [1]. DBSCAN initially creates cores
searching each neighbourhood in an epsilon distance around it for
samples.Itthencalculatestheconnectedcomponentsofthecoresand
it clusters the remaining non-core points based on the distance of the
closest connected component. In the context of security and the web,
DBSCAN was recently used successfully by Das et al. [13] to cluster
JavaScript scripts that access the sensor values of modern mobile
devices and could be abusing these values for user re-identification
via browser fingerprinting.
4.2.4 Cluster Analysis System. We compared the various sets of
APIs we have collected (native APIS, extension APIs, a set of 55
selected APIs based on malicious behavior) against each other, based
on the results each set produced while clustering. We applied the
clustering algorithm on both sets of APIs (the entire set as well as
the reduced set) and evaluated different configurations for DBSCAN
based on the used metric distance (e.g. euclidean vs dice), epsilon
distance, and number of members in a cluster. The configuration we
chose was based on the minimization of both the mean silhouette
score and the noise points of the clustering. This led to choosing the
set with the 55 APIs, because of the sparse population of clusters
when all APIs were used, leading to unoptimized clusters with higher
mean silhouette scores.
A high level overview of how we used DBSCAN to create clusters
from our API sequences is shown in Figure 5. Each table represents
Figure 5: API sequence deltas using metric algorithms & DBSCAN
the total APIs found for one particular set of extension versions as
described in Section 4.2.2. We can see a depiction of the API sequence,
based on the presence or absence of each API and then we use metric
algorithms (i.e. euclidean/dice distance) to calculate the distance
(d) between the two encoded API tables. Two extension versions
cluster together if this distance is less than the epsilon (ùúÄ) distance we
specify in our algorithm. We need to carefully select the appropriate
epsilon distance, as a high value would artificially cluster thousands
of extensions together in a few clusters, whereas a low ùúÄ would create
individual extension versions clustered by themselves. The maxi-
mum ùúÄ distance we deemed appropriate in order to cluster these
extensions together was three (i.e. a maximum of three APIs were
present in one extension and not present in the other) .
In a post-clustering phase, we removed the jQuery clusters since
we noticed a pattern of cluster creation through jQuery grouping
(i.e. multiple extensions adopting jQuery and introducing it in their
codebase via an update). To ensure that we are not removing any
malicious extensions that hide their malicious updates in jQuery-like
code, we perform a hash analysis on the jQuery files. Specifically,
we gathered all versions of jQuery from the official website [21] and
hashed them after removing spaces, tabs, and newlines. Through
this process, we removed a total of 733 jQuery clusters that did not
exhibit malicious behavior and were only clustered together because
of the addition of the same jQuery version. We also removed clusters
with only one extension ID because the clustering included multiple
versions of the same extension. After this clustering, we examine the
clusters that contain at least one known malicious extension, from
the seeds identified via the process described in Section 4.1.
4.2.5 Real Time Detection System. As a final step, we add a crawling-
based, verification component to our setup. For every extension that
clustered with a known malicious extension, we check its status and
comments on a daily basis from the extension store, looking either
for take-downs, or for user feedback confirming malicious behavior.
Malicious Behavior Category
Ad Replacement
Intrusive Ad Injection
Change Search Engine
Proxy Redirection
Change Homepage
Installs Other Extensions
Record Passwords
Redirects Information
Modify Page Links
Steals Cookies
Porn Ads
Asking Credentials after update
Total
No.
Extensions
12
10
5
4
3
2
2
2
2
1
1
1
45
Table 5: Malicious Categories of the 45 seed extensions (User Feedback)
. . . .. . . .01111100API x1API x2API x3API xN0: API exists1: API doesn't existExtension AExtension BdMetric Distance (d)if d < epsilon then same clusterClusterOverall Results
Total Extensions
Total Clusters
Malicious Extensions
Extensions from Round 1
Extensions from Round 2
All Clusters
Round 1 Clusters
Round 2 Clusters
Malicious Online
Total Users
Average Users
Total Ratings
Average Ratings (1-5)
Total Comments
Average Comments
Malicious Offline
922,684
7,419
143
133
10
21
18
3
64
2,458,881
36,915
17,268
3.87
1,574
32
79
Table 6: Summarized Results on Ratings, Comments, and Users
5 RESULTS
As described in Section 4 and depicted in Figure 1, our system for de-
tecting extensions that start exhibiting malicious behavior, consists
of two stages, one of identifying seed extensions via anomalies in ex-
tension comments and ratings and one of clustering extension deltas,
based on the APIs that changed when an extension was updated. In
this section, we provide the results of applying our system to current
and historical versions of extensions in the Chrome Web Store.
5.1 User Feedback Stages
Table 2 shows the initial set of evaluated extensions and what exten-
sions remain in our processing pipeline and result in seeds that we