title:CALM: Consistent Adaptive Local Marginal for Marginal Release under
Local Differential Privacy
author:Zhikun Zhang and
Tianhao Wang and
Ninghui Li and
Shibo He and
Jiming Chen
CALM: Consistent Adaptive Local Marginal for
Marginal Release under Local Differential Privacy
Zhikun Zhang1,2, Tianhao Wang2, Ninghui Li2, Shibo He3, Jiming Chen1
1State Key Laboratory of Industrial Control Technology & Cyber Security Research Center, Zhejiang University
{zhangzhk,cjm}@zju.edu.cn
2Department of Computer Science, Purdue University
{zhan3072,tianhaowang,ninghui}@purdue.edu
3State Key Laboratory of Industrial Control Technology, Zhejiang University
PI:EMAIL
ABSTRACT
Marginal tables are the workhorse of capturing the correlations
among a set of attributes. We consider the problem of construct-
ing marginal tables given a set of user’s multi-dimensional data
while satisfying Local Differential Privacy (LDP), a privacy notion
that protects individual user’s privacy without relying on a trusted
third party. Existing works on this problem perform poorly in the
high-dimensional setting; even worse, some incur very expensive
computational overhead. In this paper, we propose CALM, Consis-
tent Adaptive Local Marginal, that takes advantage of the careful
challenge analysis and performs consistently better than existing
methods. More importantly, CALM can scale well with large data
dimensions and marginal sizes. We conduct extensive experiments
on several real world datasets. Experimental results demonstrate
the effectiveness and efficiency of CALM over existing methods.
ACM Reference Format:
Zhikun Zhang, Tianhao Wang, Ninghui Li, Shibo He, Jiming Chen. 2018.
CALM: Consistent Adaptive Local Marginal for Marginal Release under Lo-
cal Differential Privacy. In 2018 ACM SIGSAC Conf. on Computer and Com-
munications Security (CCS’18), October 15ś19, 2018, Toronto, ON, Canada.
ACM, New York, NY, USA, 18 pages. https://doi.org/10.1145/3243734.3243742
1 INTRODUCTION
In recent years, differential privacy [13, 14] has been increasingly
accepted as the de facto standard for data privacy in the research
community [3, 15, 21, 24, 27]. Most early work on DP are in the
centralized setting, where a trusted data curator obtains data from
all individuals, and processes the data in a way that protects privacy
of individual users. For example, the data curator could publish a
private synopsis of the data, enabling analysis on the data, while
hiding individual information.
Zhikun Zhang’s work on this paper was done while working as a visiting student at
Purdue University. The first two authors are co-first authors.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CCS ’18, October 15ś19, 2018, Toronto, ON, Canada
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5693-0/18/10. . . $15.00
https://doi.org/10.1145/3243734.3243742
Recently, techniques for satisfying differential privacy (DP) in
the local setting, which we call LDP, have been studied and de-
ployed. In the local setting for DP, there are many users and one
aggregator. Unlike the centralized setting, the aggregator does not
see the actual private data of each individual. Instead, each user
sends randomized information to the aggregator, who attempts
to infer the data distribution based on that. LDP techniques en-
able the gathering of statistics while preserving privacy of every
user, without relying on trust in a single trusted third party. LDP
techniques have been deployed by companies like Google [16, 17],
Apple [1], Microsoft [12] and Samsung [9]. Exemplary use cases
include collecting users’ default browser homepage and search en-
gine, in order to understand the unwanted or malicious hijacking of
user settings; or frequently typed emoji’s and words, to help with
keyboard typing predictions.
Previous works on LDP focus on estimating the frequencies of
frequent values the user possesses [6, 7, 16, 30, 37ś39]. The natural
and more general setting is when each user has multiple attributes,
and the aggregator is interested in the joint distribution of some
of these attributes. That is, the aggregator is interested in mar-
ginal tables over some subsets of attributes. Marginal tables are the
workhorse of capturing the correlations among a set of attributes.
Many analysis tasks require the availability of marginal statistics
on multidimensional datasets. For example, finding correlations or
fitting sophisticated prediction models.
Two recent papers [11, 31] considered the problem of publish-
ing marginals under LDP. Kulkarni [11] et al. proposed to apply
the Fourier Transformation method, which was used in publishing
marginals under the centralized DP setting [5]. Ren et al. [31] pro-
posed to apply the Expectation Maximization methods, originally
developed by Fanti et al. [17] to infer marginals. These methods,
however, performs poorly when the number of attributes is more
than a few.
We propose a new method CALM, Consistent Adaptive Local
Marginal, for computing any k-way marginals under the LDP set-
ting. Our approach is inspired by PriView [29], which was designed
for computing arbitrary k-way marginals under centralized DP for
binary datasets (i.e., each attribute is binary). PriView privately
publishes a synopsis of the dataset, which takes the form of m
marginals each of the size ℓ. Using the synopsis, it can reconstruct
any k-way marginal.
Similar to PriView, in CALM a number of marginal tables are
generated. But there are several challenges when changing from
centralized setting to the local setting. We need to integrate FO
protocols to construct marginals, and to extend the methods in
PriView to deal with non-binary attributes. Furthermore, since the
privacy parameter ϵ affects noises differently in the local setting,
the error analysis, which is essential for guiding the choice of key
algorithmic parameters, changes as well. A common challenge for
many works on differential privacy is that there are often critical
algorithmic parameters, the choice of which greatly impact the
performance of the algorithm. However, such parameters are often
chosen in ad hoc ways, or based on performance on the experimen-
tal datasets. Both PriView and CALM have two critical parameters.
In the local setting, an additional source of errors is introduced
and needed to be considered. We carefully analyze how different
errors are affected by different parameters, deriving formulas for
estimating them whenever possible. We then develop an approach
for choosing these parameters in a principled way. Our approach
takes as input one target error threshold, and use an algorithm to
find parameter values, using the formulas for estimating errors.
We have implemented CALM and conducted extensive exper-
imental evaluation to compare CALM with other state of the art
methods. Experimental results show that CALM’s expected value
of the Sum of Squared Errors is often one to two orders of magni-
tude lower than the best current method in [11]. In addition, CALM
scales to settings where existing methods do not. To demonstrate
the importance of the marginal information in practice, we also
evaluate the prediction performance of CALM versus other meth-
ods by training an SVM model on some fixed marginal. In most
cases, we can see CALM can achieve near optimal results, while
other methods are beaten by the naive method that always output
the majority label.
To summarize, the main contributions of this paper are three
folds:
• We introduce CALM for the marginal release problem under
local differential privacy, which also work when there are
non-binary attributes.
• We have conducted careful analysis on errors from three
different sources, and developed an algorithm for choosing
key algorithmic parameters for CALM.
• The performance of the proposed method is extensively eval-
uated on real-world datasets and demonstrated to greatly
outperform state-of-the art approaches.
In Section 2, we present background knowledge of
Roadmap.
LDP and FO. We then go over the problem definition and existing
solutions in Section 3. We present our proposed method in Section 4.
Experimental results are presented in 5. Finally we discuss related
work in Section 6 and provide concluding remarks in Section 7.
2 BACKGROUND
In the local setting for DP, there are many users and one aggregator.
Each user possesses a value v from domain D, and the aggregator
wants to learn the distribution of values among all users, in a way
that protects the privacy of individual users.
2.1 Differential Privacy in the Local Setting
To protect privacy, each user perturbs the input value v using an
algorithm Ψ and sends Ψ(v) to the aggregator. The formal privacy
requirement is that the algorithm Ψ(·) satisfies the following prop-
erty:
Definition 1 (ϵ Local Differential Privacy). An algorithm
Ψ(·) satisfies ϵ-local differential privacy (ϵ-LDP), where ϵ ≥ 0, if and
only if for any input v1, v2 ∈ D, we have
∀T ⊆ Range(Ψ) : Pr [Ψ(v1) ∈ T ] ≤ eϵ Pr [Ψ(v2) ∈ T ] ,
where Range(Ψ) denotes the set of all possible outputs of Ψ.
Since a user never reveals v to the aggregator and reports only
Ψ(v), the user’s privacy is still protected even if the aggregator is
malicious.
2.2 Frequency Oracles
A frequency oracle (FO) protocol enables the estimation of the fre-
quency of any value x ∈ D under LDP, which serves as the building
block of other LDP tasks. It is specified by a pair of algorithms: Ψ
is used by each user to perturb her input value, and Φ is used by
the aggregator.
2.2.1 Generalized Randomized Response (GRR). This FO protocol
generalizes the randomized response technique [40]. Here each user
with private value v ∈ D sends the true value v with probability
p, and with probability 1 − p sends a randomly chosen v ′ ∈ D s.t.
v ′ , v. More formally, the perturbation function is defined as
∀y ∈D Pr (cid:2)ΨGRR(ϵ )(v) = y(cid:3) = (cid:26) p =
q =
e ϵ
1
e ϵ +d −1
e ϵ +d −1
,
,
if y = v
if y , v
This satisfies ϵ-LDP since p
= eϵ . To estimate the frequency of
q
v ∈ D (i.e., the ratio of the users who have v as private value to the
total number of users), one counts how many times v is reported,
and denote the count as C(v), and then computes
ΦGRR(ϵ )(v) B
C(v)/n − q
p − q
where n is the total number of users. For example, if 20% of users
have value v, the expected number of v in all randomized reports
is 0.2 ∗ n ∗ p + 0.8 ∗ n ∗ q. If the aggregator sees exactly this number
of reports, the estimated value is
(0.2np + 0.8nq)/n − q
p − q
=
0.2p + 0.8q − q
p − q
=
0.2p − 0.2q
p − q
= 0.2
In [37], it is shown that this is an unbiased estimation of the true
count, and the variance for this estimation is
Var[ΦGRR(ϵ )(x)] =
|D | − 2 + eϵ
(eϵ − 1)2 · n
(1)
The accuracy of this protocol deteriorates fast when the domain
size |D | increases. This is reflected in that the variance given in (1)
is linear to |D |.
2.2.2 Optimized Unary Encoding (OUE). The Optimized Unary
Encoding (OUE) [37] avoids having a variance that depends on
|D | by encoding the value into the unary representation. Wlog, let
D = [0..d − 1]; each value v ∈ [0..d − 1] is encoded into a binary
string of length d such that the v-th bit is 1 and all other bits are 0.
The unary encodings of any two different values differ in exactly
two bits. OUE applies GRR to each bit, but transmits 1’s and 0’s
differently. The bit 1 is transmitted as a coin toss, i.e., it is perturbed
to 0 with probability 0.5; this can be viewed as applying GRR with
ϵ = 0. Doing this enables us to transmit each of the many (|D | − 1,
to be precise) 0 bits with the maximum allowed privacy budget ϵ, so
that the number of 1’s resulting from perturbing the 0’s is as small
as possible. Doing this minimizes the estimation variance when |D |
is large [37].
Given reports y j from all users j ∈ [n], to estimate the frequency
of v, the aggregator counts the number of reports with the bit
j
corresponding to v set to 1, i.e., C(x) = |{j | y
x = 1}|. One then
transforms C(v) to its unbiased estimation
ΦOUE(ϵ )(x) B
C(x)/n − q
1
2 − q
It is proved in [37] that the ΨOUE(·) satisfies LDP, and estimated
frequency is unbiased and has variance
Var[ΦOUE(ϵ )(x)] =
4eϵ
(eϵ − 1)2 · n
(2)
2.2.3 Adaptive FO. Comparing (1) with (2), the factor |D | − 2 + eϵ
is replaced by 4eϵ . This suggests that for smaller |D | (such that
|D | − 2 < 3eϵ ), one is better off with GRR; but for large |D |, OUE
is better and has a variance that does not depend on |D |.
For simplicity, we use FO to denote the adaptively chosen proto-
col, i.e., when domain size is less than 3eϵ + 2, GRR is used as FO;
otherwise, OUE is used. It has variance
Var[ΦFO(ϵ )(x)] = min (cid:18)
4eϵ
(eϵ − 1)2
,
|D | − 2 + eϵ
(eϵ − 1)2
(cid:19) ·
1
n
(3)
3 PROBLEM DEFINITION AND EXISTING
SOLUTIONS
We consider the setting where each user has multiple attributes,
and the aggregator is interested in the joint distribution of some
attributes. Such multi-dimension settings occur frequently in the
situation where LDP is applied. In [11, 31], researchers studied the
problem of constructing marginals in the LDP setting.
j
1, v
j
2, . . . , v
j
d
⟩ such that v
3.1 Problem Definition: Centralized Setting
We assume that there are d attributes A = {a1, a2, . . . , ad }. Each
attribute ai has ci possible values. Wlog, we assume that the values
for ai are [ci ] B {0, 1, · · · , ci − 1}. Each user has one value for each
attribute. Thus user j’s value is a d-dimensional vector, denoted by
j
v j = ⟨v
i ∈ [ci ] for each i. The full domain
for the users’ values is given by D = [c1] × [c2] × · · · × [cd ], in which
i =1 ci .
Let us first consider the setting of answering marginal queries
in the centralized setting, where the server has all users’ data. For
a population of n users, the full contingency table gives, for each
value v ∈ D, the fraction of users having the value v. We use F
to denote the full contingency table, and call the fraction for each
value v ∈ D a cell in the full contingency table.
× denotes cartesian product. The domain D has size |D | = d
The full contingency table gives the joint distribution of all at-
tributes in A. However, when the domain size is very large, e.g.,
when there are many attributes, computing the full contingency
table can be prohibitively expensive. Oftentimes, one is interested
in the joint distribution of some subsets of the attributes. Given
a set of attributes A ⊆ A, we use VA = {⟨v1, v2, . . . , vd ⟩ : vi ∈
Gender
male
female
female
female
· · ·
male
Age
teenager
teenager
adult