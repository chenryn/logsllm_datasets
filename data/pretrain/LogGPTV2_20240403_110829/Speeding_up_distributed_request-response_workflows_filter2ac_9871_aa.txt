title:Speeding up distributed request-response workflows
author:Virajith Jalaparti and
Peter Bod&apos;ık and
Srikanth Kandula and
Ishai Menache and
Mikhail Rybalkin and
Chenyu Yan
Speeding up Distributed Request-Response Workﬂows
Virajith Jalaparti (UIUC)
Peter Bodik
Srikanth Kandula
Ishai Menache
Mikhail Rybalkin (Steklov Math Inst.)
Chenyu Yan
Microsoft
Abstract– We found that interactive services at Bing have
highly variable datacenter-side processing latencies because
their processing consists of many sequential stages, paral-
lelization across 10s-1000s of servers and aggregation of re-
sponses across the network. To improve the tail latency of
such services, we use a few building blocks: reissuing laggards
elsewhere in the cluster, new policies to return incomplete re-
sults and speeding up laggards by giving them more resources.
Combining these building blocks to reduce the overall latency
is non-trivial because for the same amount of resource (e.g.,
number of reissues), diﬀerent stages improve their latency
by diﬀerent amounts. We present Kwiken, a framework
that takes an end-to-end view of latency improvements and
costs. It decomposes the problem of minimizing latency over
a general processing DAG into a manageable optimization
over individual stages. Through simulations with production
traces, we show sizable gains; the 99th percentile of latency
improves by over 50% when just 0.1% of the responses are
allowed to have partial results and by over 40% for 25% of the
services when just 5% extra resources are used for reissues.
Categories and Subject Descriptors
C.2.4 [Computer-Communication Networks]: Distribu-
ted Systems – Distributed applications
Keywords
Interactive services; Tail latency; Optimization; Reissues;
Partial results
1.
INTRODUCTION
Modern interactive services are built from many disjoint
parts and hence, are best represented as directed acyclic
graphs. Nodes in the graph correspond to a speciﬁc function-
ality that may involve one or more servers or switches. Edges
represent input-output dependencies. For example, Fig. 1
shows a simpliﬁed DAG corresponding to the web-search
service at Bing, one of the major search engines today. In
this paper, we use the term workﬂow to refer to such a DAG
and stage to refer to a node in the DAG.
Analyzing production traces from hundreds of user-facing
services at Bing reveals that the end-to-end response latency
is quite variable. Despite signiﬁcant developer eﬀort, we
Permission  to  make  digital  or  hard  copies  of  all  or  part  of  this  work  for 
personal or classroom use is granted without fee provided that copies are not 
made or distributed for profit or commercial advantage and that copies bear 
this notice and the full citation on the first page. Copyrights for components of 
this  work  owned  by  others  than  ACM  must  be  honored.  Abstracting  with 
credit is permitted. To copy otherwise, or republish, to post on servers or to 
redistribute  to  lists,  requires  prior  specific  permission  and/or  a  fee.  Request 
permissions from permissions@acm.org. 
SIGCOMM’13, August 12–16, 2013, Hong Kong, China. 
Copyright © 2013 ACM  978-1-4503-2056-6/13/08…$15.00. 
Doc. 
Lookup
…
…
…
Snippet 
Snippet 
Snippet 
Snippet 
Generator
Generator
Generator
Generator
Looking up the index 
for a document
(network) Lag to 
collect responses
Snippet generation
Figure 1: A simpliﬁed version of the workﬂow used for
web-search at Bing.
found over 30% of the examined services have 95th (and 99th)
percentile of latency 3X (and 5X) their median latency.
Delivering low and predictable latency is valuable: several
studies show that slow and unpredictable responses degrade
user experience and hence lead to lower revenue [8, 9, 22].
Further, these services represent sizable investments in terms
of cluster hardware and software, so any improvements would
be a competitive advantage.
We believe that the increase in variability is because mod-
ern datacenter services have workﬂows that are long and
highly parallel. In contrast, a typical web service workﬂow
has a length of two (a web-server and a database) and a width
of one. As we show in §2, the median workﬂow in production
at Bing has 15 stages and 10% of the stages process the query
in parallel on 1000s of servers. Signiﬁcant delays at any of
these servers manifest as end-to-end delays. To see why, as a
rule of thumb, the 99th percentile of an n-way parallel stage
depends on the 99.99th percentile of the individual server
latencies for n = 100 (or 99.999th for n = 1000).
While standard techniques exist to reduce the latency
tail [10], applying them to reduce end-to-end latency is diﬃ-
cult for various reasons. First, diﬀerent stages beneﬁt diﬀer-
ently from diﬀerent techniques. For example, request reissues
work best for stages with low mean and high variance of la-
tency. Second, end-to-end eﬀects of local actions depend on
topology of the workﬂow; reducing latency of stages usually
oﬀ the critical path does not improve end-to-end latency.
Finally, many techniques have overhead, such as increased
resource usage when reissuing a request. For these reasons,
latency reduction techniques today are applied at the level
of individual stages, without clear understanding of their
total cost and the achieved latency reduction. Therefore,
without an end-to-end approach, the gains achieved by such
techniques are limited.
In this paper, we present a holistic framework that consid-
ers the latency distribution in each stage, the cost of applying
individual techniques and the workﬂow structure to deter-
mine how to use each technique in each stage to minimize
end-to-end latency. To appreciate the challenge, consider
splitting the reissue budget between two stages, 1 and 2, in
a serial workﬂow. Fig. 2a shows how the variance of the
latency of these two stages (Var1 and Var2, respectively)
varies with the fraction of the total budget allocated to Stage
2191 (on x-axis). Since both stages have similar variance when
receiving zero budget, one may expect to divide the budget
evenly. However, Stage 1’s variance decreases quickly with
reissue budget and the marginal improvement with addi-
tional budget is small. As marked in the ﬁgure, assigning the
budget roughly 1:3 among the stages leads to the smallest
variance of end-to-end latency (Sum Var). Comparing Sum
Var with the 99th percentile latency in Fig. 2b shows that
the sum of variances of the stages is well correlated with the
99th percentile, a fact that we will prove and use extensively.
Kwiken formulates the overall latency reduction problem as
a layered optimization relying on the fact that query latencies
across stages are only minimally correlated. The ﬁrst layer
consists of per-stage variance models that estimate how the
latency variance in individual stages changes as a function
of budget allocated to that stage; these models may also
incorporate other intra-stage optimizations where there are
diﬀerent ways of using budget within a stage. The workﬂow
layer integrates the per-stage models into a single global
objective function designed such that its minimization is well-
correlated to minimizing higher percentiles of the end-to-end
latency. The objective function also has a simple separable
structure that allows us to develop eﬃcient gradient-like
methods for its minimization.
Further, we present two new latency reduction techniques:
a new timeout policy tp trade oﬀ partial answers for latency
and catching-up for laggard queries. The basic ideas behind
these strategies are quite simple. First, many workﬂows
can still provide a useful end-to-end answer even when in-
dividual stages return partial answers. So, at stages that
are many-way parallel, Kwiken provides an early termination
method that improves query latency given a constraint on
the amount of acceptable loss on answer quality. Second,
Kwiken preferentially treats laggard queries at later stages
in their workﬂow, either by giving them a higher service
rate (more threads), being more aggressive about reissuing
them or by giving them access to a higher priority queue
in network switches. Kwiken incorporates these techniques
into the optimization framework to minimize the end-to-end
latency while keeping the total additional cost low.
While in this paper we apply Kwiken in the context of
request reissues and partial execution inside Bing, our solution
applies more generally, for example to the network latency
reduction techniques described in [23].
It also applies to
most applications where work is distributed among disjoint
components and dependencies can be structured as a DAG.
This includes modern web services (e.g., Facebook [20] or
Google [10]) and page loading in web browsers [24] and mobile
phone applications [21].
We evaluate our framework with 45 production workﬂows
at Bing. By appropriately apportioning reissue budget, Kwiken
improves the 99th percentile of latency by an average of 29%
with just 5% extra resources. This is over half the gains
possible from reissuing every request (budget=100%). At
stages that are many-way parallel, we show that Kwiken can
improve the 99th percentile latency by about 50% when
partial answers are allowed for just 0.1% of the queries.
We, further, show that reissues and partial answers provide
complementary beneﬁts; allowing partial answers for 0.1%
queries lets a reissue budget of 1% provide more gains than
could be achieved by increasing the reissue budget to 10%.
We also demonstrate robustness of parameter choices.
In summary, we make the following contributions:
e
c
n
a
i
r
a
V
1
0.8
0.6
0.4
0.2
0
Var1
Var2
Sum Var
1
e
l
i
t
n
e
c
r
e
p
h
t
9
9
0.8
0.6
0.4
0.2
0
99th perc
0.25
0
Fraction of budget given to Stage 1 
0.75
0.5
1
(a) Variance
0.5
0.25
0
Fraction of budget given to Stage 1 
(b) End-to-end 99th per-
centile
0.75
1
Figure 2: Impact of splitting the budget between two
stages in a serial workﬂow on the variance of individual
stages (Var1 and Var2), and the variance (Sum Var) and
99th percentile of the end-to-end latency (metrics are
normalized)
Top Agg.
1
8
6
7
Tier TLA
Tier TLA
Tier TLA
Tier Agg.
2
5
Snippet 
Snippet 
Snippet 
Snippet 
Generator
Generator
Generator
Generator
MLA
MLA
MLA
MLARack 
Agg.
3
Doc. 
Doc. 
Lookup
Doc. 
Lookup
Doc. 
Lookup
Doc. 
Lookup
Lookup
4
Figure 3: Timeline diagram of the processing involved
for the workﬂow in Fig. 1.
• Workﬂow characterization. We describe low-
latency execution workﬂows at a large search engine,
analyze in detail the structure of the workﬂow DAGs
and report on the causes for high variability.
• New strategies. We provide novel policies for bound-
ing quality loss incurred due to partial answers and for
catching-up on laggards.
• Optimization framework. We present a holistic
optimization framework that casts each stage as a
variance-response curve to apportion overall budget ap-
propriately across stages. We evaluate the framework
on real-world workﬂows and demonstrate signiﬁcant
reductions in their end-to-end latencies, especially in
the higher percentiles i.e., tail latencies.
2. WORKFLOWS IN PRODUCTION
We analyze workﬂows from production at Bing to under-
stand their structural and behavioral characteristics and to
identify causes for slow responses.
2.1 Background
The workﬂow of an end-to-end service is a collection of
stages with input-output dependencies; for example, respond-
ing to a user search on Bing involves accessing a spell checker
stage and then in parallel, a web-search stage that looks up
documents in an index and similar video- and image-search
stages. Architecting datacenter services in this way allows
easy reuse of common functionality encapsulated in stages,
akin to the layering argument in the network stack.
Workﬂows can be hierarchical; i.e., complex stages may
internally be architected as workﬂows themselves. For exam-
ple, the web-search stage at Bing consists of multiple tiers
which correspond to indexes of diﬀerent sizes and freshness.
Each tier has a document-lookup stage consisting of tens of
thousands of servers that each return the best document for
the phrase among their sliver of the index. These documents
are aggregated at rack and at tier level and the most relevant
220 
n
o
i
t
c
a
r
F
e
v
i
t
a
u
m
u
C
l
1
0.8
0.6
0.4
0.2
0
1000
Stages
100
10
)
s
m
(
v
e
d
S
t
1
1
Stage
Workflow
0
2
4
6
8
10
Ratio of 99th percentile 
to median latency 
(a)
Workflows
1000
10
100
Mean (ms) 
(b)
e
c
n
a
i
r
a
v
y
c
n
e
t
a
l
d
e
z
i
l
a
m
r
o
n
1
0.8
0.6
0.4
0.2
0
0
0.2