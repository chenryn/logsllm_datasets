#BHUSA @BlackHatEvents
**Human or Not: Can You Really Detect the Fake Voices?**
Liu Xin, Tan Yuan  
School of Information Science and Engineering, Lanzhou University

## Contents
1. **Introduction to AI-Synthesized Speeches**
2. **Existing Detectors and Approaches**
3. **Problems in Existing Approaches**
4. **SiF-DeepVC: Voice Cloning with Speaker-Irrelevant Features**
5. **Evaluation**
6. **Conclusion and Takeaways**
7. **Open-Source Code and Datasets**

## 1. Introduction to AI-Synthesized Speeches
### What’s a Fake Voice?
- **Definition**: AI-synthesized speeches are novel fake voices generated by artificial intelligence.
- **Common Uses**: Fraud, customer service, and authorization bypass.
- **Most Dangerous Form**: Voice Cloning (VC) is the most dangerous form of AI-synthesized speech.

### History of Speech Synthesis
- **Old Days (Before 20th Century)**: Simulating sounds with different machines, very difficult to simulate human voice.
- **Jigsaw Era (Before 2010)**: Automatic "unit selection," poor coherence, and easy to detect.
- **AI-Synthesized Speeches (Since 2010)**: Smooth and natural, difficult to detect.

### AI-Synthesized Speech Process
- **Input**: Text or linguistic data.
- **Output**: Voice.
- **Process**:
  - **Encoder**: Converts input text into a feature vector.
  - **Concatenation and Attention**: Combines linguistic features with speaker embeddings.
  - **Decoder and Synthesizer**: Generates mel-spectrograms.
  - **Vocoder**: Converts mel-spectrograms into time-domain waveforms.
  - **Voice Features**: Replaceable elements that can be manipulated.

## 2. Existing Detectors and Approaches
### Overview of Existing Detection Approaches
- **Computer Vision (CV)-based Approaches**: Inspired by image recognition techniques, these methods convert voice to images and use image classification techniques.
  - **Examples**: Deep4SNet (2021, ACC > 98%), RES-EfficientCNN (2020, F1 > 97%), Farid et al. (2019, AUC > 99%).
- **Neural Network Feature (NNF)-based Approaches**: Use neuronal activity in neural networks as features.
  - **Example**: DeepSonar (2020, ACC=100%).
- **End-to-End (E2E)-based Approaches**: Novel approaches commonly used in NLP problems.
  - **Example**: RawNet2 (2021, EER=6.1%).
- **Statistical-based Approaches**: Traditional methods, not popular in recent years.

### Problems in Existing Approaches
- **Unrealistic Datasets**: Training on datasets that do not reflect real-world scenarios.
- **Speaker-Irrelevant Features**: Features that should not be used to determine if a voice is human or synthetic.
  - **Examples**: Meaningless silences, background noises, different languages.
- **Multiple Classifications**: Inconsistent and complex classification criteria.

## 3. SiF-DeepVC: Voice Cloning with Speaker-Irrelevant Features
### What is SiF-DeepVC?
- **Definition**: Voice cloning based on deep learning and speaker-irrelevant features.
- **Components**:
  - **Voice Cloner**: Comprises a speaker encoder, synthesizer, and vocoder.
  - **Denoiser**: Removes high-frequency noise and current sound.
  - **SI Feature Extractor**: Extracts and processes speaker-irrelevant features.
  - **Voice Merger**: Combines voices from the SI Feature Extractor and Voice Cloner.

### Overview of SiF-DeepVC
- **Human Voice**: Input for the Voice Cloner.
- **Cloned Voice**: Output from the Voice Cloner.
- **Crafted Voice**: Output from the SI Feature Extractor.
- **Output Voice**: Final output after merging the cloned and crafted voices.

### Example: Mr. Musk
- **Original Voice**: “I will give everyone 10 million dollars.”
- **Cloned Voice**: Generated by the Voice Cloner.
- **Crafted Voice**: Processed by the SI Feature Extractor.
- **Output Voice**: Merged and processed final output.
- **Detection Result**: The latest top-journal published detector (Deep4SNet) marks it as real.

## 4. Evaluation
### Research Questions
- **RQ1**: Are existing detection approaches practical in real-world environments?
- **RQ2**: Do speaker-irrelevant features really affect existing detection approaches?
- **RQ3**: Can SiF-DeepVC-generated cloned voices bypass existing detection approaches?
- **RQ4**: Can people understand the speeches generated by SiF-DeepVC?

### Baseline Datasets
- **FoR Validation**: English, 5400 original human recordings, all silence-removed.
- **MagicData Test**: Mandarin, 24279 samples.
- **Coverage**: Recordings cover different ages, lengths, and environments, and are well-represented.

## 5. Conclusion and Takeaways
- **Summary**: SiF-DeepVC effectively bypasses existing detection approaches by leveraging speaker-irrelevant features.
- **Takeaways**: The importance of realistic datasets and the need for more robust detection methods.
- **Future Work**: Further research to improve the detection of AI-synthesized speeches.

## 6. Open-Source Code and Datasets
- **Code and Datasets**: Available for researchers to replicate and extend the findings.

#BHUSA @BlackHatEvents
**Information Classification: General**