works [28]. The data in [63] shows that while the
U.S. is the largest single contributor of nodes in
Vuze, a majority of the nodes lie outside the U.S.
and are distributed over 190 countries.
3. Churn. DHTs evolve naturally and dynamically
over time as new nodes constantly join and old
nodes leave. The average lifetime of a node in
the DHT varies across networks and has been mea-
sured from minutes on Kazaa [30] to hours on
Vuze/Azureus [28].
The ﬁrst property provides us with solid grounds for
implementing a useful system. The second property
makes DHTs more resilient to certain types of attacks
than centralized or small-scale systems. For example,
Figure 3: The Vanish system architecture.
while a centrally administered system can be compelled
to release data by an attacker with legal leverage [59],
obtaining subpoenas for multiple nodes storing a VDO’s
key pieces would be signiﬁcantly harder, and in some
cases impossible, due to their distribution under different
administrative and political domains.
Traditionally, DHT research has tried to counter the
negative effects of churn on availability. For our pur-
poses, however, the constant churn in the DHT is an ad-
vantage, because it means that data stored in DHTs will
naturally and irreversibly disappear over time as the DHT
evolves. In many cases, trying to determine the contents
of the DHT one week in the past — let alone several
months or years — may be impossible, because many
of the nodes storing DHT data will have left or changed
their locations in the index space. For example, in Vuze,
a node changes its location in the DHT whenever its IP
address or port number changes, which typically happens
periodically for dynamic IP addresses (e.g., studies show
that over 80% of the IPs change within 7 days [65]).
This self-cleansing property of DHTs, coupled with its
scale and global decentralization, makes them a felici-
tous choice for our self-destructing data system.
Vanish. Vanish is designed to leverage one or more
DHTs. Figure 3 illustrates the high-level system archi-
tecture. At its core, Vanish takes a data object D (and
possibly an explicit timeout T ), and encapsulates it into
a VDO V .
In more detail, to encapsulate the data D, Vanish picks
a random data key, K, and encrypts D with K to obtain
a ciphertext C. Not surprisingly, Vanish uses threshold
secret sharing [58] to split the data key K into N pieces
(shares) K1, . . . ,KN. A parameter of the secret sharing is
a threshold that can be set by the user or by an application
using Vanish. The threshold determines how many of the
N shares are required to reconstruct the original key. For
example, if we split the key into N = 20 shares and the
threshold is 10 keys, then we can compute the key given
any 10 of the 20 shares. In this paper we often refer to the
threshold ratio (or simply threshold) as the percentage
of the N keys required, e.g., in the example above the
threshold ratio is 50%.
Once Vanish has computed the key shares, it picks at
random an access key, L. It then uses a cryptographically
secure pseudorandom number generator [7], keyed by L,
to derive N indices into the DHT, I1, . . . ,IN. Vanish then
sprinkles the N shares K1, . . . ,KN at these pseudorandom
locations throughout the DHT; speciﬁcally, for each i ∈
{1, . . . ,N}, Vanish stores the share Ki at index Ii in the
DHT. If the DHT allows a variable timeout, e.g., with
OpenDHT, Vanish will also set the user-chosen timeout
T for each share. Once more than (N−threshold) shares
are lost, the VDO becomes permanently unavailable.
The ﬁnal VDO V consists of (L,C,N,threshold) and
is sent over to the email server or stored in the ﬁle system
upon encapsulation. The decapsulation of V happens in
the natural way, assuming that it has not timed out. Given
VDO V , Vanish (1) extracts the access key, L, (2) derives
the locations of the shares of K, (3) retrieves the required
number of shares as speciﬁed by the threshold, (4) recon-
structs K, and (5) decrypts C to obtain D.
Threshold Secret Sharing, Security, and Robustness.
For security we rely on the property that the shares
K1, . . . ,KN will disappear from the DHT over time,
thereby limiting a retroactive adversary’s ability to ob-
tain a sufﬁcient number of shares, which must be ≥ the
threshold ratio. In general, we use a ratio of < 100%,
otherwise the loss of a single share would cause the loss
of the key. DHTs do lose data due to churn, and therefore
a smaller ratio is needed to provide robust storage prior
to the timeout. We consider all of these issues in more
detail later; despite the conceptual simplicity of our ap-
proach, signiﬁcant care and experimental analyses must
be taken to assess the durability of our use of large-scale,
decentralized DHTs.
Extending the Lifetime of a VDO. For certain uses,
the default timeout offered by Vuze might be too lim-
iting. For such cases, Vanish provides a mechanism to
refresh VDO shares in the DHT. While it may be tempt-
ing at ﬁrst to simply use Vuze’s republishing mechanism
for index-value pairs, doing so would re-push the same
pairs (I1,K1), . . . ,(IN,KN) periodically, until the timeout.
This would, in effect, increase the exposure of those key
shares to certain attackers. Hence, our refresh mecha-
nism retrieves the original data key K before its time-
out, re-splits it, obtaining a fresh set of shares, and de-
rives new DHT indices I1, . . . ,IN as a function of L and
a weakly synchronized clock. The weakly synchronized
clock splits UTC time into roughly 8-hour epochs and
uses the epoch number as part of the input to the location
function. Decapsulations then query locations generated
from both the current epoch number and the neighboring
epochs, thus allowing clocks to be weakly synchronized.
Naturally, refreshes require periodic Internet connec-
tivity. A simple home-based setup, where a broadband
connected PC serves as the user’s refreshing proxy, is in
our view and experience a very reasonable choice given
today’s highly connected, highly equipped homes.
In
fact, we have been using this setup in our in-house de-
ployment of Vanish in order to achieve longer timeouts
for our emails (see Section 5).
KC = EK(D)World-wide DHT Lk1  k2... knk1knk2LocatorUsing multiple or no DHTs. As an extension to the
scheme above, it is possible to store the shares of the
data key K in multiple DHTs. For example, one might
ﬁrst split K into two shares K0 and K00 such that both
shares are required to reconstruct K. K0 is then split into
N0 shares and sprinkled in the Vuze DHT, while K00 is
split into N00 shares and sprinkled in OpenDHT. Such an
approach would allow us to argue about security under
different threat models, using OpenDHT’s closed access
(albeit small scale) and Vuze’s large scale (albeit com-
munal) access.
An alternate model would be to abandon DHTs and to
store the key shares on distributed but managed nodes.
This approach bears limitations similar to Ephemerizer
(Section 2). A hybrid approach might be to store shares
of K0 in a DHT and shares of K00 on managed nodes. This
way, an attacker would have to subvert both the privately
managed system and the DHT to compromise Vanish.
Forensic Trails. Although not a common feature in to-
day’s DHTs, a future DHT or managed storage system
could additionally provide a forensic trail for monitoring
accesses to protected content. A custom DHT could, for
example, record the IP addresses of the clients that query
for particular indices and make that information available
to the originator of that content. The existence of such a
forensic trail, even if probabilistic, could dissuade third
parties from accessing the contents of VDOs that they
obtain prior to timeout.
Composition. Our system is not designed to protect
against all attacks, especially those for which solutions
are already known. Rather, we designed both the sys-
tem and our applications to be composable with other
systems to support defense-in-depth. For example, our
Vanish Gmail plugin can be composed with GPG in or-
der to avoid VDO snifﬁng by malicious email services.
Similarly, our system can compose with Tor to ensure
anonymity and throttle targeted attacks.
5 Prototype System and Applications
We have implemented a Vanish prototype capable of in-
tegrating with both Vuze and OpenDHT. In this section,
we demonstrate that (1) by leveraging existing, unmod-
iﬁed DHT deployments we can indeed achieve the core
functions of vanishing data, (2) the resulting system sup-
ports a variety of applications, and (3) the performance of
VDO operations is reasonable. We focus our discussions
on Vuze because its large scale and dynamic nature make
its analysis both more interesting and more challenging.
A key observation derived from our study is a tension in
setting VDO parameters (N and threshold) when target-
ing both high availability prior to the timeout and high
security. We return to this tension in Section 6.
To integrate Vanish with the Vuze DHT, we made two
Figure 4: VDO availability in the Vuze-based Vanish sys-
tem. The availability probability for single-key VDOs (N = 1)
and for VDOs using secret sharing, averaged over 100 runs.
Secret sharing is required to ensure pre-timeout availability and
post-timeout destruction. Using N = 50 and a threshold of 90%
achieves these goals.
minor changes (< 50 lines of code) to the existing Vuze
BitTorrent client: a security measure to prevent lookup
snifﬁng attacks (see Section 6.2) and several optimiza-
tions suggested by prior work [28] to achieve reasonable
performance for our applications. All these changes are
local to Vanish nodes and do not require adoption by any
other nodes in the Vuze DHT.
5.1 Vuze Background
The Vuze (a.k.a. Azureus) DHT is based on the Kadem-
lia [35] protocol. Each DHT node is assigned a “random”
160-bit ID based on its IP and port, which determines the
index ranges that it will store. To store an (index,value)
pair in the DHT, a client looks up 20 nodes with IDs clos-
est to the speciﬁed index and then sends store messages
to them. Vuze nodes republish the entries in their cache
database every 30 minutes to the other 19 nodes closest
to the value’s index in order to combat churn in the DHT.
Nodes further remove from their caches all values whose
store timestamp is more than 8 hours old. This pro-
cess has a 1-hour grace period. The originator node must
re-push its 8-hour-old (index,value) pairs if it wishes to
ensure their persistence past 8 hours.
5.2 VDO Availability and Expiration in
Vuze
We ran experiments against the real global Vuze P2P net-
work and evaluated the availability and expiration guar-
antees it provides. Our experiments pushed 1,000 VDO
shares to pseudorandom indices in the Vuze DHT and
then polled for them periodically. We repeated this ex-
periment 100 times over a 3-day period in January 2009.
Figure 4 shows the average probability that a VDO re-
mains available as a function of the time since creation,
for three different N and threshold values. For these
experiments we used the standard 8-hour Vuze timeout
(i.e., we did not use our refreshing proxy to re-push
shares).
 0 0.2 0.4 0.6 0.8 1 0 2 4 6 8 10 12 14 16 18Prob. of VDO availabilityTime (h)N=1,(no secret sharing)N=50,Threshold=90%N=100,Threshold=99%(a) Vanishing Facebook messages.
(b) Google Doc with vanishing parts.
Figure 5: The Web-wide applicability of Vanish. Screenshots of two example uses of vanishing data objects on the Web. (a)
Carla is attempting to decapsulate a VDO she received from Ann in a Facebook message. (b) Ann and Carla are drafting Ann’s
divorce document using a Google Doc; they encapsulate sensitive, draft information inside VDOs until they ﬁnalize their position.
The N = 1 line shows the lifetime for a single share,
which by deﬁnition does not involve secret sharing.
The single-share VDO exhibits two problems: non-
negligible probabilities for premature destruction (≈1%
of the VDOs time out before 8 hours) and prolonged
availability (≈5% of the VDOs continue to live long after
8 hours). The cause for the former effect is churn, which
leads to early loss of the unique key for some VDOs.
While the cause for the latter effect demands more inves-
tigation, we suspect that some of the single VDO keys
are stored by DHT peers running non-default conﬁgu-
rations. These observations suggest that the naive (one
share) approach for storing the data key K in the DHT
meets neither the availability nor the destruction goals of
VDOs, thereby motivating our need for redundancy.
Secret sharing can solve the two lifetime problems
seen with N = 1. Figure 4 shows that for VDOs with
N = 50 and threshold of 90%, the probability of prema-
ture destruction and prolonged availability both become
vanishingly small (< 10−3). Other values for N ≥ 20
achieve the same effect for thresholds of 90%. However,
using very high threshold ratios leads to poor pre-timeout
availability curves: e.g., N = 100 and a threshold of 99%
leads to a VDO availability period of 4 hours because
the loss of only two shares share makes the key unre-
coverable. We will show in Section 6 that increasing the
threshold increases security. Therefore, the choice of N
and the threshold represents a tradeoff between security
and availability. We will investigate this tradeoff further
in Section 6.
5.3 Vanish Applications
We built two prototype applications that use a Van-
ish daemon running locally or remotely to ensure self-
destruction of various types of data.
FireVanish. We implemented a Firefox plugin for the
popular Gmail service that provides the option of sending
and reading self-destructing emails. Our implementa-
tion requires no server-side changes. The plugin uses the
Vanish daemon both to transform an email into a VDO
before sending it to Gmail and similarly for extracting
the contents of a VDO on the receiver side.
Our plugin is implemented as an extension of FireGPG
(an existing GPG plugin for Gmail) and adds Vanish-
related browser overlay controls and functions. Using
our FireVanish plugin, a user types the body of her email
into the Gmail text box as usual and then clicks on a
“Create a Vanishing Email” button that the plugin over-
lays atop the Gmail interface. The plugin encapsulates
the user’s typed email body into a VDO by issuing a
VDO-create request to Vanish, replaces the contents of
the Gmail text box with an encoding of the VDO, and
uploads the VDO email to Gmail for delivery. The user
can optionally wrap the VDO in GPG for increased pro-
tection against malicious services. In our current imple-
mentation, each email is encapsulated with its own VDO,
though a multi-email wrapping would also be possible
(e.g., all emails in the same thread).
When the receiving user clicks on one of his emails,
FireVanish inspects whether it is a VDO email, a PGP
email, or a regular email. Regular emails require no fur-
ther action. PGP emails are ﬁrst decrypted and then in-
spected to determine whether the underlying message is a
VDO email. For VDO emails, the plugin overlays a link
“Decapsulate this email” atop Gmail’s regular interface
(shown previously in Figure 1(b)). Clicking on this link
causes the plugin to invoke Vanish to attempt to retrieve
the cleartext body from the VDO email. If the VDO has
not yet timed out, then the plugin pops up a new window