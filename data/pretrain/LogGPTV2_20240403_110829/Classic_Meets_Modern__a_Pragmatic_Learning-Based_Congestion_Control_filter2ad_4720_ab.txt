(a) Link capacity and ﬂow throughput.
TCP NewReno
0
10
20
30
50
40
60
Time (seconds)
70
80
90 100
•
•
Inability to adapt to new or unseen network scenarios:
Most existing protocols do not adapt their congestion-
control strategies as the network evolves over time. For
example, NewReno will always increase cwnd by one
over one RTT during congestion avoidance stage, even
though it might be too slow for the ﬂow to fully utilize the
bandwidth in some modern Internet routes where the link
bandwidth is high and the round trip time (RTT) is long.
Inability to learn from historical knowledge: Every time
a ﬂow starts, the classical approach assumes no prior
information of the links. However, much better perfor-
mance can be achieved if TCP can adjust its behavior
based on previously learned information when the same
path was previously explored. For example, if NewReno
already has knowledge learned from previous interactions
with path characteristics such as bandwidth, delay, packet
loss rate, it may be able to adjust its behavior with more
ﬂexibility: it may speed up the cwnd more aggressively
during congestion avoidance to increase the link utilization
on less congested links.
We use the following ns-3 simulation with dynamic spectrum
access links in cognitive radio (CR) network to study a typical
bandwidth-varying scenario that highlights the above problems.
CR allows opportunistic use of under-utilized licensed spectrum
allocated to primary users, where the CR user has to sense
interferences and periodically switch to a new spectrum to avoid
interfering with the primary user. This spectrum sensing and
switching process may lead to short transmission stall and sudden
changing in the available bandwidth, making effective congestion
control more challenging [10], [11].
We can show the performance of NewReno with varying
network conditions (bandwidth, frequent interruptions caused by
PU activity/sensing). We start one NewReno ﬂow from one
source to the CR mobile device. We model
the consecutive
transmission On period and the disconnection caused by spectrum
(b) Varying cwnd of NewReno.
Fig. 2: The real time throughput and cwnd variation of a TCP
NewReno ﬂow. Bottleneck bandwidth is uniformly distributed
between [20,40] Mbps. RTT = 100ms, Buffer size = 150 packets.
sensing/switching as the Off period. We ﬁx the “On” period
to be 9s and the “Off” period to be 1s. The varying range of
channel capacity is chosen between 20Mbps and 40Mbps. Thus,
the bandwidth availability changes by uniformly picking a value
between 20Mbps and 40Mbps every 10 seconds.
As shown in Fig. 2a, the performance of NewReno is far from
satisfactory in this network scenario – it only achieves about 30%
of link bandwidth. Fig. 2b reveals two key reasons behind the
low link utilization: (1) the cwnd will drop to one due to the
timeout event triggered every time the CR user is performing
spectrum sensing/switching, and (2) the slow increase of cwnd
during the convergence avoidance does not allow the sender to
acquire sufﬁcient network resources between two transmission
interruptions. Even worse, this pattern is repeated in each cognitive
cycle, indicating that a rule-based protocol is unable to learn from
previous experience and adapt its behavior to achieve consistent
good performance in evolving practical network nowadays. While
a larger initial cwnd might improve performance, we will show
that our adaptive algorithm can achieve improved performance
even when starting with a suboptimal initial cwnd.
3 QTCP: APPLY Q-LEARNING TO TCP CONGES-
TION CONTROL
In this section, we explore the use of RL to automatically design
congestion control strategies. RL has the potential of overcoming
the problems of rule-based TCP described above as it can enable
agent to learn from past experience, without the need for manually
settled rules or prior knowledge of the networking scenarios.
Reward received
avg ack
avg send
avg rtt
State
Agent
Kanerva Coding
ˆQ(s,a)
RL
Algorithm
Function
Approximation
Action
decision Environment:
(TCP Congestion 
Avoidance Phase)
 Network condition measurements
Fig. 3: Solution framework of our RL-based TCP congestion
control design.
Speciﬁcally, we discuss how to apply Q-learning, a classical RL
algorithm to the domain of congestion control problem and pro-
pose QTCP: a new congestion control protocol that allows sender
to learn the optimal cwnd changing policy through interaction with
the network scenarios.
3.1 Overview of QTCP
The framework of QTCP is shown in Fig. 3. The learning
agent (sender) interacts with the network environments and keeps
exploring the optimal policy by taking sequential actions (e.g.,
varying the cwnd) given feedback as it works to achieve its desired
goal, i.e., large throughput and low latency. Like any typical RL
problem, QTCP consists of the following elements:
•
States: deﬁned as informative perceptions or measure-
ments that an agent can obtain from the outside environ-
ment. Here, the state is a unique proﬁle of the network
conditions evaluated through selected performance metrics
(Sec. 3.2).
• Actions: chosen by an agent at each time step, after
perceiving its current state, according to a policy. In the
context of congestion control, the action is the decision to
increase, decrease, or leave unchanged the current cwnd
(Sec. 3.3).
• Reward: this reﬂects the desirability of the action picked.
As we describe below, the reward is further speciﬁed by
the value of a utility function, which is computed based on
the measurement of ﬂow throughput and latency. Higher
throughput and lower latency translates into a higher utility
value and vice-versa (Sec. 3.4).
Training algorithm: The purpose of the training algorithm
is to learn the optimal policy to select certain action
for each state. This is the central module of QTCP as
it is responsible for developing the congestion control
strategies (Sec. 3.5).
•
The performance of QTCP depends on appropriate selection
and design of above mentioned elements, and we will further
discuss them in the following sections. In general, QTCP works
by checking the values of selected state variables and passing
these state values to the currently trained policy to generate an
action to adjust cwnd. Then QTCP observes the new state and
the reward and uses them as an input to the training algorithm
that evaluates and improves the cwnd changing policies. Since
we choose Q-learning as the training algorithm, the above process
can be conducted in an on-line manner.
4
The key challenge of applying learning algorithm to conges-
tion control is sifting through the overwhelming number of state
combinations used to model the environment. To solve this prob-
lem, QTCP takes advantages of advanced function approximation
technique to learn the value functions of encountered state-action
pairs and optimal policies when observing new states from the
network environment. Speciﬁcally, we ﬁrst choose the adaptive
Kanerva-coding as a base-line algorithm and then propose an
improved generalization-based mechanism to further speed up the
learning process and optimize the training qualities (Sec. 4).
3.2 States
Network topologies can be complicated and trafﬁc can un-
dergo dramatic changes, especially when considering competition
among multiple data ﬂows and arbitrary bandwidth changes. The
continuous, high-dimensional state space used to represent the
network can generate a nearly inﬁnite number of states. Many
state variables can describe the characteristics of the network
environment, such as the most-recent sample RTT, average time
between the timestamps when sending packets, average inter-
arrival time between newly received ACKs, the average throughput
in a time interval, the average RTT in a time interval, the threshold,
the immediate cwnd or the average cwnd during a past time
interval, etc. A high-dimensional state space that consists a large
set of state variables would not only exponentially enlarge the
size of the state space to explore, but also signiﬁcantly delay
convergence. It makes sense to reduce the set of state variables
and only focus on those features of the environment that relate
to the agent’s goal. We therefore need to identify the appropriate
state variables that can capture the performance of actions taken
by QTCP and guarantee the tractability of the learning process.
We consider the state space used in Remy [12] and choose our
three state variables described as followings:
•
•
avg_send: the average interval between sending two pack-
ets.
avg_ack: the average interval between receiving two con-
secutive ACKs.
avg_rtt: the average RTT.
•
We calculate avg_send by taking the average of several packet-
sending intervals in a time window (one RTT) to reduce the esti-
mation bias. The avg_send and avg_rtt are calculated in a similar
way. All values are represented in milliseconds and commercially
rounded to nearest integer values.
We use these three state variables because they are signiﬁ-
cantly affected by network congestion and can be seen as efﬁcient
"congestion probes". For example, avg_send characterizes the
trafﬁc sending rate at the sender side and avg_ack reﬂects the real
goodput measured at the receiver side. If there is no congestion,
then ideally avg_send should be equal with avg_ack. On the
other hand, avg_send ✏
b, if Ut   Ut tinterval <  ✏
where a is a positive value and b is a negative value both of which
are used to indicate the reward (a reinforcement signal) given
the direction of changes between two newly observed consecutive
utility values. The ✏ sets a tolerance of the changes between utility
values. It is a tunable parameter that sets the sensitivity of the