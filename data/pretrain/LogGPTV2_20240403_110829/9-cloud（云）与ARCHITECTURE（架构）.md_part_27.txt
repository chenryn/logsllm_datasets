process for unclosed journal segments\...
19/03/30 17:26:12 INFO client.QuorumJournalManager: Successfully started
new epoch 1
19/03/30 17:26:12 INFO util.ExitUtil: Exiting with status 0
19/03/30 17:26:12 INFO namenode.NameNode: SHUTDOWN_MSG:
/\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*
SHUTDOWN_MSG: Shutting down NameNode at nn01/192.168.1.10
\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*/
**nodeX: 停止 journalnode 服务 node1,node2,node3相同操作**
**./sbin/hadoop-daemon.sh stop journalnode**
\[root@node1 \~\]# /usr/local/hadoop/sbin/hadoop-daemon.sh stop
journalnode
stopping journalnode
#\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\--#
### 启动集群
启动之前,前面的服务一定要按步骤操作
**NN1: ./sbin/start-all.sh**
\[root@nn01 \~\]# /usr/local/hadoop/sbin/start-all.sh
NN2: ./sbin/yarn-daemon.sh start resourcemanager
\[root@nn02 \~\]# /usr/local/hadoop/sbin/yarn-daemon.sh start
resourcemanager
starting resourcemanager, logging to
/usr/local/hadoop/logs/yarn-root-resourcemanager-nn02.out
### 查看集群状态
./bin/hdfs haadmin -getServiceState nn1
./bin/hdfs haadmin -getServiceState nn2
./bin/yarn rmadmin -getServiceState rm1
./bin/yarn rmadmin -getServiceState rm2
结果如下:
\[root@nn01 \~\]# /usr/local/hadoop/bin/hdfs haadmin -getServiceState
nn1
active
\[root@nn01 \~\]# /usr/local/hadoop/bin/hdfs haadmin -getServiceState
nn2
standby
\[root@nn01 \~\]# /usr/local/hadoop/bin/yarn rmadmin -getServiceState
rm1
active
\[root@nn01 \~\]# /usr/local/hadoop/bin/yarn rmadmin -getServiceState
rm2
standby
./bin/hdfs dfsadmin -report
./bin/yarn node -list
### 访问集群：
./bin/hadoop fs -ls / #访问数据根目录,刚格式化创建,所以没数据
./bin/hadoop fs -mkdir hdfs://mycluster/input #创建input文件夹再查看
验证高可用，关闭 active namenode 有可能是nn01 有可能是nn02
./sbin/hadoop-daemon.sh stop namenode
./sbin/yarn-daemon.sh stop resourcemanager
恢复节点
./sbin/hadoop-daemon.sh stop namenode
./sbin/yarn-daemon.sh stop resourcemanager
用火狐访问:
192.168.1.10:50070
192.168.1.20:50070
nn01:192.168.1.10:50070
![](media/image175.png){width="7.2652777777777775in"
height="3.4305555555555554in"}
nn02:192.168.1.20:50070
![](media/image176.png){width="7.266666666666667in"
height="4.622222222222222in"}
## 案例
案例1：Zookeeper安装
案例2：Kafka集群实验
案例3：Hadoop高可用
案例4：高可用验证
### 1 案例1：Zookeeper安装
1.1 问题
本案例要求：
搭建Zookeeper集群并查看各服务器的角色
停止Leader并查看各服务器的角色
1.2 步骤
实现此案例需要按照如下步骤进行。
2 步骤一：安装Zookeeper
1）编辑/etc/hosts ,所有集群主机可以相互 ping
通（在nn01上面配置，同步到node1，node2，node3）
\[root@nn01 hadoop\]# vim /etc/hosts
192.168.1.21 nn01
192.168.1.22 node1
192.168.1.23 node2
192.168.1.24 node3
192.168.1.25 node4
\[root@nn01 hadoop\]# for i in {22..24} \\
do \\
scp /etc/hosts 192.168.1.\$i:/etc/ \\
done //同步配置
hosts 100% 253 639.2KB/s 00:00
hosts 100% 253 497.7KB/s 00:00
hosts 100% 253 662.2KB/s 00:00
2）安装
java-1.8.0-openjdk-devel,由于之前的hadoop上面已经安装过，这里不再安装，若是新机器要安装
3）zookeeper 解压拷贝到 /usr/local/zookeeper
\[root@nn01 \~\]# tar -xf zookeeper-3.4.10.tar.gz
\[root@nn01 \~\]# mv zookeeper-3.4.10 /usr/local/zookeeper
4）配置文件改名，并在最后添加配置
\[root@nn01 \~\]# cd /usr/local/zookeeper/conf/
\[root@nn01 conf\]# ls
configuration.xsl log4j.properties zoo_sample.cfg
\[root@nn01 conf\]# mv zoo_sample.cfg zoo.cfg
\[root@nn01 conf\]# chown root.root zoo.cfg
\[root@nn01 conf\]# vim zoo.cfg
server.1=node1:2888:3888
server.2=node2:2888:3888
server.3=node3:2888:3888
server.4=nn01:2888:3888:observer
5）拷贝 /usr/local/zookeeper 到其他集群主机
\[root@nn01 conf\]# for i in {22..24}; do rsync -aSH \--delete
/usr/local/zookeeper/ 192.168.1.\$i:/usr/local/zookeeper -e \'ssh\' &
done
\[4\] 4956
\[5\] 4957
\[6\] 4958
6）创建 mkdir /tmp/zookeeper，每一台都要
\[root@nn01 conf\]# mkdir /tmp/zookeeper
\[root@nn01 conf\]# ssh node1 mkdir /tmp/zookeeper
\[root@nn01 conf\]# ssh node2 mkdir /tmp/zookeeper
\[root@nn01 conf\]# ssh node3 mkdir /tmp/zookeeper
7）创建 myid 文件，id 必须与配置文件里主机名对应的 server.(id) 一致
\[root@nn01 conf\]# echo 4 \>/tmp/zookeeper/myid
\[root@nn01 conf\]# ssh node1 \'echo 1 \>/tmp/zookeeper/myid\'
\[root@nn01 conf\]# ssh node2 \'echo 2 \>/tmp/zookeeper/myid\'
\[root@nn01 conf\]# ssh node3 \'echo 3 \>/tmp/zookeeper/myid\'
8）启动服务，单启动一台无法查看状态，需要启动全部集群以后才能查看状态，每一台上面都要手工启动（以nn01为例子）
\[root@nn01 conf\]# /usr/local/zookeeper/bin/zkServer.sh start
ZooKeeper JMX enabled by default
Using config: /usr/local/zookeeper/bin/../conf/zoo.cfg
Starting zookeeper \... STARTED
注意：刚启动zookeeper查看状态的时候报错，启动的数量要保证半数以上，这时再去看就成功了
9）查看状态
\[root@nn01 conf\]# /usr/local/zookeeper/bin/zkServer.sh status
ZooKeeper JMX enabled by default
Using config: /usr/local/zookeeper/bin/../conf/zoo.cfg
Mode: observe
\[root@nn01 conf\]# /usr/local/zookeeper/bin/zkServer.sh stop
//关闭之后查看状态其他服务器的角色
ZooKeeper JMX enabled by default
Using config: /usr/local/zookeeper/bin/../conf/zoo.cfg
Stopping zookeeper \... STOPPED
\[root@nn01 conf\]# yum -y install telnet
\[root@nn01 conf\]# telnet node3 2181
Trying 192.168.1.24\...
Connected to node3.
Escape character is \'\^\]\'.
ruok //发送
imokConnection closed by foreign host. //imok回应的结果
10）利用 api 查看状态（nn01上面操作）
\[root@nn01 conf\]# /usr/local/zookeeper/bin/zkServer.sh start
\[root@nn01 conf\]# vim api.sh
#!/bin/bash
function getstatus(){
exec 9\/dev/tcp/\$1/2181 2\>/dev/null
echo stat \>&9
MODE=\$(cat \ /etc/hostname
\[root@node4 \~\]# hostname nn02
4）修改vim /etc/hosts
\[root@nn01 hadoop\]# vim /etc/hosts
192.168.1.21 nn01
192.168.1.25 nn02
192.168.1.22 node1
192.168.1.23 node2
192.168.1.24 node3
5）同步到nn02，node1，node2，node3
\[root@nn01 hadoop\]# for i in {22..25}; do rsync -aSH \--delete
/etc/hosts 192.168.1.\$i:/etc/hosts -e \'ssh\' & done
\[1\] 14355
\[2\] 14356
\[3\] 14357
\[4\] 14358
6）配置SSH信任关系
注意：nn01和nn02互相连接不需要密码，nn02连接自己和node1，node2，node3同样不需要密码
\[root@nn02 \~\]# vim /etc/ssh/ssh_config
Host \*
GSSAPIAuthentication yes
StrictHostKeyChecking no
\[root@nn01 hadoop\]# cd /root/.ssh/
\[root@nn01 .ssh\]# scp id_rsa id_rsa.pub nn02:/root/.ssh/
//把nn01的公钥私钥考给nn02
7）所有的主机删除/var/hadoop/\*
\[root@nn01 .ssh\]# rm -rf /var/hadoop/\*
\[root@nn01 .ssh\]# ssh nn02 rm -rf /var/hadoop/\*
\[root@nn01 .ssh\]# ssh node1 rm -rf /var/hadoop/\*
\[root@nn01 .ssh\]# ssh node2 rm -rf /var/hadoop/\*
\[root@nn01 .ssh\]# ssh node3 rm -rf /var/hadoop/\*
8）配置 core-site
\[root@nn01 .ssh\]# vim /usr/local/hadoop/etc/hadoop/core-site.xml
\
\
\fs.defaultFS\
\hdfs://nsdcluster\
//nsdcluster是随便起的名。相当于一个组，访问的时候访问这个组
\
\
\hadoop.tmp.dir\