mation. That said, because G′
A must still be causality-preserving,
the attack-preservation metric assures that connectivity between
processes and other essential information flow is preserved.
4 DESIGN
While the attack-preserving forensic validity represents a desirable
trade-off between the utility and efficiency of audit logs, designing
of an approximation technique that satisfies attack-preservation
syscall=execve name=“~/ﬁrefox”…syscall=read name=“.mozilla/datareporting/aborted-session-ping.tmp"syscall=read name=".mozilla/datareporting/archived/2018-04/2344.44eb0835e-4a135f9af3df.main.jsonlz4.tmp"syscall=read name=".mozilla/datareporting/archived/2018-04/5990.43ca-9dca-28304f471c7d.main.jsonlz4.tmp"syscall=read name=".mozilla/datareporting/archived/2018-04/1002.4402-bc3b-f6102aa8ec14main.jsonlz4.tmp"…Audit LogFirefoxApproxRegex GenerationGraph Reduction.mozilla/datareporting/aborted-session-ping.tmp.mozilla/datareporting/archived/2018-04/*.*.main.jsonlz4.tmpsyscall=execve name=“~/ﬁrefox”…syscall=read name=“.mozilla/datareporting/aborted-session-ping.tmp"syscall=read name=".mozilla/datareporting/archived/2018-04/*.*.main.jsonlz4.tmp"…Approximated Log.mozilla/datareporting/aborted-session-ping.tmpLearned Regex….mozilla/datareporting/archived/2018-04/2344.44eb0835e-4a135f9af3df.main.jsonlz4.tmp.mozilla/datareporting/archived/2018-04/5990.43ca-9dca-28304f471c7d.main.jsonlz4.tmp.mozilla/datareporting/archived/2018-04/1002.4402-bc3b-f6102aa8ec14main.jsonlz4.tmpAudit Log Provenance GraphFirefox.mozilla/datareporting/aborted-session-ping.tmp.mozilla/datareporting/archived/2018-04/*.*.main.jsonlz4.tmpApproximated Log Provenance GraphACSAC 2020, December 7–11, 2020, Austin, USA
Noor Michael, Jaron Mink, Jason Liu, Sneha Gaur, Wajih Ul Hassan, and Adam Bates
in is quite challenging. Trivially, a causality-preserving graph is
implicitly also an attack-preserving graph, but does not capitalize
on the increased reduction opportunities afforded by the attack-
preserving metric. To do so, it is necessary to gain an understanding
of typically-occurring benign-process events and make generalized
assertions about benign events that may occur in the future. Simul-
taneously, it is also necessary to assure that generalizations about
benign process activity are not so general that attackers may abuse
them to conceal malicious behaviors.
4.1 Overview
In this section, we present LogApprox, a novel solution to attack-
preserving log approximation. An architectural overview for Lo-
gApprox is given in Figure 4. LogApprox searches for log reduction
opportunities by generating regular expressions that describe be-
nign process file I/O. These regexes are crafted in such a way to
avoid overgeneralization, which might lead our system to filter
events that describe unique attack patterns. After the regexes are
generated, they are applied to past and future file I/O events (e.g.,
creat, open, read, write) in the audit log. For all events that
match the same regex, the original filename is replaced with the
regex pattern; then causality-preserving reduction is applied to
remove events that are redundant from an information flow per-
spective. When the log is parsed into a provenance graph, the
original file vertices are replaced with a single approximated file
vertex. Because the information flows into and out of the regex
remain unchanged, causality is otherwise preserved.
Our approach focuses exclusively on file activity for two im-
portant reasons. First, file I/O dominates the overall storage space
of audit logs — 88.97% of all events in our evaluation datasets —
and causality-preserving reduction [77] is not a total solution to
this overhead because applications often write to a tremendous
number of different files. Second, other system events (e.g., process,
network) are more important to retain because they are essential
to causal analysis. Process events are already low overhead and
are needed to preserve the process tree, which is the “backbone” of
causal analysis, while network events are needed to trace attacks
across multiple hosts in the network. We thus choose to focus on
file I/O in our design.
4.2 Reduction Algorithm
Our reduction algorithm begins by identifying, for each process,
which files it has interacted with. Our provenance graph encodes
this information by edges to (read) and from (write) a process
node. From this list of files, we will generate groups of files with
similar filenames. Replacing each group of files with a single place-
holder in the provenance graph allows us to reduce the graph
complexity and hence filter redundant log entries.
4.2.1 Regular Expression Learning. Our goal is to distribute a list
of filenames associated with a process into groups of similar files.
For a particular filename (eg. /usr/bin/ls) we distinguish the path
(eg. /usr/bin/) and the name itself (eg. ls).
We define the distance between two names as the Levenshtein
edit distance. Corresponding to this edit distance is an optimal align-
ment, which will be useful later in constructing a regular expres-
sion. We define similarity between two names x1, x2 as (max_len −
Algorithm 1: Generate Groups
Data: List of filenames f1, . . . , fn
Result: Groups of filenames G
1 Empty list G;
2 while not added file fi do
3
4
5
Set Gi ← {fi };
for filenames fj not added to a group do
Add fj to Gi ;
6
7
8 Return G;
Append Gi to G;
if Distance(Path(fi), Path(fj)) ≤ path_threshold and
Similarity(Name(fi), Name(fj)) ≥ name_threshold then
Algorithm 2: Generate Regular Expressions
Data: Groups of filenames G = [G1, . . . , Gn]
Result: Regular Expressions R = [r1, . . . , rn]
1 Empty list R;
2 for every Gi ∈ S do
Set ri ← Gi[1];
for filenames f ∈ Gi[2, . . . , m] do
Find alignment a between ri and f ;
Replace modifications in a with wildcards;
Set ri to a;
Append ri to R;
3
4
5
6
7
8
9 Return R;
Dist(x1, x2))/max_len, where max_len = Max(Len(x1), Len(x2)).
For the distance between two paths, we treat each directory name
as a token. We only consider the distance when both paths are
of equal depth, because differences in depth contain semantically
relevant information. We define the path distance between two
paths as the number of differing directory names.
We compute the path distance and name similarity between
all pairs of filenames. We choose groups such that all filenames
within a group have a path distance below a specified threshold
and a name similarity above a specified threshold. We empirically
determined our path threshold to be 1 and our name threshold to be
0.7. We found this to be a good tradeoff that allows for aggressive
log reduction capabilities while avoiding over-generalization.
Algorithm 1 groups the files into sets, where each file is at most
a certain distance from another file in its set. These thresholds are
computed separately for both file paths and file names, as shown on
line 5. We return a list of such groups, each of which corresponds
to a regex.
Algorithm 2 shows how to compute the regex corresponding
to a group of files. We compute the path regex and name regex
individually using this approach. To compute a regex from two
strings, we find the edit distance alignment, and for every location
where the tokens do not match, we replace it with a placeholder.
We reduce this binary operation across the list of filenames. We
coalesce placeholders and replace each with a token matching zero
or more occurrences of a wildcard. We then concatenate the path
and name regexes to generate a regex matching all files in the group.
If there is only one element in the group, we return that filename
as the corresponding regex.
This algorithm uses a binary regex generation function, taking
as input the current progress and the next filename. It reduces this
function across all filenames in a group. If the current regex matches
the next filename, it will remain unchanged.
On the Forensic Validity of Approximated Audit Logs
ACSAC 2020, December 7–11, 2020, Austin, USA
Algorithm 3: Log Reduction
Data: Log and Provenance Graph
Result: Reduced Log
1 for every process p do
2
3
4
5
6
Compute list of groups G;
Compute list of regexes R;
for every group Gi ∈ G do
7
8
9
10
11
12
13
14
15 Return log;
else
Delete e;
else
Delete e;
for all reads e to file f ∈ Gi (in order) do
if since the last read, there was a write to f or a read from p to a
file (cid:44) f then
Keep e and overwrite f with ri ;
for all writes e to file f ∈ Gi (in order) do
Keep e and overwrite f with ri ;
if since the last read, there was a read from p to a file (cid:44) f then
4.2.2 Log Reduction. The overall log reduction procedure is pre-
sented in Algorithm 3. For every process, we generate a list of
filenames corresponding to file accesses initiated by the process.
These filenames are grouped as per Algorithm 1, and their corre-
sponding regular expressions are generated as per Algorithm 2. For
every group of filenames, we reduce the log entries between the
process and these files preserving information flow. We do not re-
duce filenames corresponding to regular expressions with a length
below a certain threshold, in our case, 10 characters, since they can
overgeneralize. In effect, we are treating this group of filenames as
one large file. For the log entries that have not been removed, we
overwrite the filename with the regular expression corresponding
to the group, as shown in lines 7 and 12.
This reduction algorithm acts on every log entry corresponding
to a file access. It either keeps or deletes a log entry based on the
information-flow preservation criteria outlined in the background.
5 IMPLEMENTATION
We implemented a log analysis tool that parses Linux Audit
(audit) logs and CDM provenance graphs, the DARPA Engage-
ment data format. Our tool generates a provenance graph in mem-
ory using the SNAP graph library [44]. Our provenance graph
representation has nodes corresponding to processes, files, and
other file-like objects (e.g. VFS, network sockets). Edges correspond
to individual log entries. Our entire tool is implemented in 4000
lines of C++ code (calculated with cloc [2]). Our reduction filter,
LogApprox, is implemented in 1000 lines of code.
To evaluate LogApprox against a representative set of exemplar
systems, we also re-implemented Log Garbage Collection, intro-
duced in Lee et. al [43], Causality-Preserving Reduction, introduced
in Xu et. al [77], and Full and Source Dependence Preserving Reduc-
tion, introduced by Hossein et. al [36], based on their descriptions
in the original paper. Our implementation of garbage collection is
based on “Basic GC” in the original LogGC system [43], while we
implement the techniques as described in [77], [36] as faithfully as
possible. We do note that, because we did not have access to the
authors’ source code, it is possible that our implementation of these
techniques deviate from the original systems, which could in part
explain some of the differences in observed reduction rates in our
Observed
Reduction
1.3X (23%)
1.6X (38%)
6.6X (85%)
11.2X (91%)
Originally Reported
Reduction
1.3-3.4X (23% - 71%)
0-77.0X (0% - 99%)
4.5-91.5X (78% - 99%)
4.5-122.5X (78% - 99%)
Approximation
Technique
CPR [77]
GC [43]
F-DPR [36]
S-DPR [36]
Table 1: Comparison of log reduction rates between reports
from prior work and our own tests using the Theia dataset.
For ease of reference, we report both of the 2 different sta-
tistics used in prior work; log reduction factor (Raw Log /
Reduced Log) and in parenthesis the log reduction percent-
age (1 - Reduced Log / Raw Log).
experiments (e.g., Table 1). This said, we are confident that each
of our implementations is consistent with the methodology pre-
sented in the original works, and are thus satisfactory for exploring
different points in the design space of approximation strategies.
Because our tool is based on an in-memory database, it was
necessary to manage memory overheads by partitioning the the
provenance graph into epochs of time, each of which spans 5 min-
utes in our implementation. At the end of each epoch, the approxi-
mation technique(s) is (are) applied and the remaining log events