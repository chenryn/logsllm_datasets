title:Sizing router buffers
author:Guido Appenzeller and
Isaac Keslassy and
Nick McKeown
Sizing Router Buffers
Guido Appenzeller
Stanford University
Isaac Keslassy
Stanford University
Nick McKeown
Stanford University
PI:EMAIL
PI:EMAIL
PI:EMAIL
ABSTRACT
All Internet routers contain buﬀers to hold packets during
times of congestion. Today, the size of the buﬀers is deter-
mined by the dynamics of TCP’s congestion control algo-
rithm. In particular, the goal is to make sure that when a
link is congested, it is busy 100% of the time; which is equiv-
alent to making sure its buﬀer never goes empty. A widely
used rule-of-thumb states that each link needs a buﬀer of
size B = RT T × C, where RT T is the average round-trip
time of a ﬂow passing across the link, and C is the data rate
of the link. For example, a 10Gb/s router linecard needs
approximately 250ms × 10Gb/s = 2.5Gbits of buﬀers; and
the amount of buﬀering grows linearly with the line-rate.
Such large buﬀers are challenging for router manufacturers,
who must use large, slow, oﬀ-chip DRAMs. And queueing
delays can be long, have high variance, and may destabilize
the congestion control algorithms. In this paper we argue
that the rule-of-thumb (B = RT T × C) is now outdated and
incorrect for backbone routers. This is because of the large
number of ﬂows (TCP connections) multiplexed together on
a single backbone link. Using theory, simulation and exper-
√
iments on a network of real routers, we show that a link
with n ﬂows requires no more than B = (RT T × C)/
n, for
long-lived or short-lived TCP ﬂows. The consequences on
router design are enormous: A 2.5Gb/s link carrying 10,000
ﬂows could reduce its buﬀers by 99% with negligible dif-
ference in throughput; and a 10Gb/s link carrying 50,000
ﬂows requires only 10Mbits of buﬀering, which can easily be
implemented using fast, on-chip SRAM.
Categories and Subject Descriptors
C.2 [Internetworking]: Routers
∗
The authors are with the Computer Systems Laboratory at
Stanford University. Isaac Keslassy is now with the Tech-
nion (Israel Institute of Technology), Haifa, Israel. This
work was funded in part by the Stanford Networking Re-
search Center, the Stanford Center for Integrated Systems,
and a Wakerly Stanford Graduate Fellowship.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’04, Aug. 30–Sept. 3, 2004, Portland, Oregon, USA.
Copyright 2004 ACM 1-58113-862-8/04/0008 ...$5.00.
General Terms
Design, Performance.
Keywords
Internet router, buﬀer size, bandwidth delay product, TCP.
1.
INTRODUCTION AND MOTIVATION
1.1 Background
Internet routers are packet switches, and therefore buﬀer
packets during times of congestion. Arguably, router buﬀers
are the single biggest contributor to uncertainty in the Inter-
net. Buﬀers cause queueing delay and delay-variance; when
they overﬂow they cause packet loss, and when they under-
ﬂow they can degrade throughput. Given the signiﬁcance
of their role, we might reasonably expect the dynamics and
sizing of router buﬀers to be well understood, based on a
well-grounded theory, and supported by extensive simula-
tion and experimentation. This is not so.
Router buﬀers are sized today based on a rule-of-thumb
commonly attributed to a 1994 paper by Villamizar and
Song [1].Using experimental measurements of at most eight
TCP ﬂows on a 40 Mb/s link, they concluded that — be-
cause of the dynamics of TCP’s congestion control algo-
rithms — a router needs an amount of buﬀering equal to
the average round-trip time of a ﬂow that passes through
the router, multiplied by the capacity of the router’s net-
work interfaces. This is the well-known B = RT T × C rule.
We will show later that the rule-of-thumb does indeed make
sense for one (or a small number of) long-lived TCP ﬂows.
Network operators follow the rule-of-thumb and require
that router manufacturers provide 250ms (or more) of buﬀer-
ing [2]. The rule is found in architectural guidelines [3], too.
Requiring such large buﬀers complicates router design, and
is an impediment to building routers with larger capacity.
For example, a 10Gb/s router linecard needs approximately
250ms × 10Gb/s= 2.5Gbits of buﬀers, and the amount of
buﬀering grows linearly with the line-rate.
The goal of our work is to ﬁnd out if the rule-of-thumb still
holds. While it is still true that most traﬃc uses TCP, the
number of ﬂows has increased signiﬁcantly. Today, backbone
links commonly operate at 2.5Gb/s or 10Gb/s, and carry
over 10,000 ﬂows [4].
One thing is for sure: It is not well understood how much
buﬀering is actually needed, or how buﬀer size aﬀects net-
work performance [5]. In this paper we argue that the rule-
of-thumb is outdated and incorrect. We believe that sig-
281 20
 10
 0
 8
 6
 4
 2
 0
Window [pkts]
W = 2 Tp * C
 2
Q [pkts]
 2.5
 3
 3.5
 4
 2
Figure 1: Window (top) and router queue (bottom) for a TCP ﬂow through a bottleneck link.
 3.5
 4
 2.5
 3
 4.5
 4.5
Figure 2: Single ﬂow topology consisting of an ac-
cess link of latency lAcc and link capacity CAcc and a
bottleneck link of capacity C and latency l.
niﬁcantly smaller buﬀers could be used in backbone routers
(e.g. by removing 99% of the buﬀers) without a loss in net-
work utilization, and we show theory, simulations and exper-
iments to support our argument. At the very least, we be-
lieve that the possibility of using much smaller buﬀers war-
rants further exploration and study, with more comprehen-
sive experiments needed on real backbone networks. This
paper isn’t the last word, and our goal is to persuade one
or more network operators to try reduced router buﬀers in
their backbone network.
It is worth asking why we care to accurately size router
buﬀers; with declining memory prices, why not just over-
buﬀer routers? We believe overbuﬀering is a bad idea for
two reasons. First, it complicates the design of high-speed
routers, leading to higher power consumption, more board
space, and lower density. Second, overbuﬀering increases
end-to-end delay in the presence of congestion. Large buﬀers
conﬂict with the low-latency needs of real time applications
(e.g. video games, and device control). In some cases large
delays can make congestion control algorithms unstable [6]
and applications unusable.
1.2 Where does the rule-of-thumb come from?
The rule-of-thumb comes from a desire to keep a congested
link as busy as possible, so as to maximize the throughput
of the network. We are used to thinking of sizing queues so
as to prevent them from overﬂowing and losing packets. But
TCP’s “sawtooth” congestion control algorithm is designed
to ﬁll any buﬀer, and deliberately causes occasional loss to
provide feedback to the sender. No matter how big we make
the buﬀers at a bottleneck link, TCP will cause the buﬀer
to overﬂow.
Router buﬀers are sized so that when TCP ﬂows pass
through them, they don’t underﬂow and lose throughput;
and this is where the rule-of-thumb comes from. The metric
we will use is throughput, and our goal is to determine the
size of the buﬀer so as to maximize throughput of a bottle-
neck link. The basic idea is that when a router has packets
buﬀered, its outgoing link is always busy. If the outgoing
link is a bottleneck, then we want to keep it busy as much
of the time as possible, and so we just need to make sure
the buﬀer never underﬂows and goes empty.
Fact: The rule-of-thumb is the amount of buﬀering
needed by a single TCP ﬂow, so that the buﬀer at the bot-
tleneck link never underﬂows, and so the router doesn’t lose
throughput.
The rule-of-thumb comes from the dynamics of TCP’s
congestion control algorithm.
In particular, a single TCP
ﬂow passing through a bottleneck link requires a buﬀer size
equal to the bandwidth-delay product in order to prevent the
link from going idle and thereby losing throughput. Here,
we will give a quick intuitive explanation of where the rule-
of-thumb comes from; in particular, why this is just the right
amount of buﬀering if the router carried just one long-lived
TCP ﬂow. In Section 2 we will give a more precise explana-
tion, which will set the stage for a theory for buﬀer sizing
with one ﬂow, or with multiple long- and short-lived ﬂows.
Later, we will conﬁrm that our theory is true using simula-
tion and experiments in Sections 5.1 and 5.2 respectively.
Consider the simple topology in Figure 2 in which a single
TCP source sends an inﬁnite amount of data with packets
of constant size. The ﬂow passes through a single router,
and the sender’s access link is much faster than the re-
ceiver’s bottleneck link of capacity C, causing packets to be
queued at the router. The propagation time between sender
and receiver (and vice versa) is denoted by Tp. Assume
that the TCP ﬂow has settled into the additive-increase and
multiplicative-decrease (AIMD) congestion avoidance mode.
The sender transmits a packet each time it receives an
ACK, and gradually increases the number of outstanding
packets (the window size), which causes the buﬀer to grad-
ually ﬁll up. Eventually a packet is dropped, and the sender
doesn’t receive an ACK. It halves the window size and
pauses.1 The sender now has too many packets outstanding
in the network:
it sent an amount equal to the old win-
dow, but now the window size has halved. It must therefore
pause while it waits for ACKs to arrive before it can resume
transmitting.
The key to sizing the buﬀer is to make sure that while
the sender pauses, the router buﬀer doesn’t go empty and
force the bottleneck link to go idle. By determining the rate
at which the buﬀer drains, we can determine the size of the
reservoir needed to prevent it from going empty. It turns
out that this is equal to the distance (in bytes) between the
peak and trough of the “sawtooth” representing the TCP
1We assume the reader is familiar with the dynamics of TCP.
A brief reminder of the salient features can be found in the
appendix of the extended version of this paper [7].
window size. We will show later that this corresponds to
the rule-of-thumb: B = RT T × C.
It is worth asking if the TCP sawtooth is the only factor
that determines the buﬀer size. For example, doesn’t sta-
tistical multiplexing, and the sudden arrival of short bursts
have an eﬀect? In particular, we might expect the (very
bursty) TCP slow-start phase to increase queue occupancy
and frequently ﬁll the buﬀer. Figure 1 illustrates the ef-
fect of bursts on the queue size for a typical single TCP
ﬂow. Clearly the queue is absorbing very short term bursts
in the slow-start phase, while it is accommodating a slowly
changing window size in the congestion avoidance phase. We
will examine the eﬀect of burstiness caused by short-ﬂows in
Section 4. We’ll ﬁnd that the short-ﬂows play a very small
eﬀect, and that the buﬀer size is, in fact, dictated by the
number of long ﬂows.
1.3 How buffer size inﬂuences router design
Having seen where the rule-of-thumb comes from, let’s see
why it matters; in particular, how it complicates the design
of routers. At the time of writing, a state of the art router
linecard runs at an aggregate rate of 40Gb/s (with one or
more physical interfaces), has about 250ms of buﬀering, and
so has 10Gbits (1.25Gbytes) of buﬀer memory.
Buﬀers in backbone routers are built from commercial
memory devices such as dynamic RAM (DRAM) or static
RAM (SRAM).2 The largest commercial SRAM chip today
is 36Mbits, which means a 40Gb/s linecard would require
over 300 chips, making the board too large, too expensive
and too hot. If instead we try to build the linecard using
DRAM, we would just need 10 devices. This is because
DRAM devices are available up to 1Gbit. But the prob-
lem is that DRAM has a random access time of about 50ns,
which is hard to use when a minimum length (40byte) packet
can arrive and depart every 8ns. Worse still, DRAM access
times fall by only 7% per year, and so the problem is going
to get worse as line-rates increase in the future.
In practice router linecards use multiple DRAM chips
in parallel to obtain the aggregate data-rate (or memory-
bandwidth) they need. Packets are either scattered across
memories in an ad-hoc statistical manner, or use an SRAM
cache with a refresh algorithm [8]. Either way, such a large
packet buﬀer has a number of disadvantages: it uses a very
wide DRAM bus (hundreds or thousands of signals), with
a huge number of fast data pins (network processors and
packet processor ASICs frequently have more than 2,000
pins making the chips large and expensive). Such wide buses
consume large amounts of board space, and the fast data
pins on modern DRAMs consume a lot of power.
In summary, it is extremely diﬃcult to build packet buﬀers
at 40Gb/s and beyond. Given how slowly memory speeds
improve, this problem is going to get worse over time.
Substantial beneﬁts could be gained by placing the buﬀer
memory directly on the chip that processes the packets (a
network processor or an ASIC). In this case, very wide and
fast access to a single memory is possible. Commercial
packet processor ASICs have been built with 256Mbits of
“embedded” DRAM. If memories of 2% the delay-bandwidth
product were acceptable (i.e. 98% smaller than they are to-
day), then a single-chip packet processor would need no ex-
ternal memories. We will present evidence later that buﬀers
2DRAM includes devices with specialized I/O, such as DDR-
SDRAM, RDRAM, RLDRAM and FCRAM.
Figure 3: Schematic evolution of a router buﬀer for
a single TCP ﬂow.
 250
 200
 150
 100
 50
 0
 100
 50
 0
Window [Pkts]
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 Queue [Pkts]
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
Figure 4: A TCP ﬂow through a single router with
buﬀers equal to the delay-bandwidth product. The
upper graph shows the time evolution of the conges-
tion window W (t). The lower graph shows the time
evolution of the queue length Q(t).
this small might make little or no diﬀerence to the utilization
of backbone links.
2. BUFFER SIZE FOR A SINGLE
LONG-LIVED TCP FLOW
In the next two sections we will determine how large the
router buﬀers need to be if all the TCP ﬂows are long-lived.
We will start by examining a single long-lived ﬂow, and then
consider what happens when many ﬂows are multiplexed
together.
Starting with a single ﬂow, consider again the topology in
Figure 2 with a single sender and one bottleneck link. The
schematic evolution of the router’s queue (when the source
is in congestion avoidance) is shown in Figure 3. From time
t0, the sender steadily increases its window-size and ﬁlls the
buﬀer, until the buﬀer has to drop the ﬁrst packet. Just
under one round-trip time later, the sender times-out at
time t1, because it is waiting for an ACK for the dropped
packet. It immediately halves its window size from Wmax to
Wmax/2 packets3. Now, the window size limits the number
of unacknowledged (i.e. outstanding) packets in the net-
work. Before the loss, the sender is allowed to have Wmax
outstanding packets; but after the timeout, it is only allowed
to have Wmax/2 outstanding packets. Thus, the sender has
too many outstanding packets, and it must pause while it
waits for the ACKs for Wmax/2 packets. Our goal is to
make sure the router buﬀer never goes empty in order to
keep the router fully utilized. Therefore, the buﬀer must
not go empty while the sender is pausing.
If the buﬀer never goes empty, the router must be sending
packets onto the bottleneck link at constant rate C. This in
3While TCP measures window size in bytes, we will count
window size in packets for simplicity of presentation.
 250
 200
 150
 100
 50
 0
 100
 50
Window [Pkts]
 0