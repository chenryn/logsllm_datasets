non-polynomials functions described in the previous section.
We measured the throughput and latency (not
including
the time for encryption) for computing these functions
on encrypted data. We report the performing numbers in
Table VI. From this table, we can see that the performance of
PEGASUS can be easily accelerated by using more machine
cores. The acceleration efﬁciency for small fan-in functions
e.g., sigmoid and max-pooling was nearly optimal,
i.e.,
around 0.95. Also, the latency of PEGASUS for the min-
index and sorting function is also superior to the state-of-
the-arts. For example, [16] took more than 236s to compute
the max-index of t = 24 encrypted elements using 8 threads,
compared to PEGASUS that just took 9.72s. Also, [25] used
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:25:55 UTC from IEEE Xplore.  Restrictions apply. 
1067
Figure 7: Trade-off between LUT accuracy and performance
in PEGASUS. The LUT Accuracy is given as − log |f (x)−
Tf (x)| for the function f. We demonstrate four functions
|x|, and f (x) =
f (x) = 1
max(0, x) on uniform random messages from x ∈ [−8, 8].
1+e−x , f (x) = tanh(x), f (x) =
(cid:112)
4 threads and took about 43 minutes to sort t = 26 encrypted
integers using the bit-wise HE [48], which is about 6.3×
slower than ours.
The algorithms in PEGASUS do introduce errors to the
ﬁnal results, including errors from the (R)LWE encryption,
key-switching, and LUT. These errors can be controlled
under a reasonable small value. According to our empiri-
cal results, the key-switching introduces small errors of a
magnitude of 2−14. Also, the approximation errors of our
LUT can be within [2−10, 2−7] under the parameters from
Table IV. We consider such small errors are acceptable for
many applications such as machine learning and information
retrieval. Indeed, the accuracy of our LUT can be improved
by using a larger lattice dimension n at a cost of a longer
LUT evaluation time (see Fig. 7).
2) Private Decision Tree Evaluation: We evaluated our
private decision tree evaluation algorithm on three real
datasets from the UCI repository [23] that
is the Iris,
Housing, and Spambase dataset. We trained decision trees
on randomly selected 80% of the data points using the
scikit-learn library [44], and used the remaining 20% for
testing. We measured the execution time on both sides of
the cloud and the client and measured the communication
costs counting all the ciphertexts sent by the client and the
cloud. . The results are given in Table VII. We compare
with the previous single-round HE-based methods [37], [51].
The approach [51] uses the bit-wise TFHE to evaluate the
boolean circuit representation of the decision tree on inputs
of δ = 16-bit integers, which is sufﬁcient precision for
the used datasets. That is, [51] did not introduce accuracy
loss on the classiﬁcation. On the contrary, PEGASUS in-
troduced about 3% miss-classiﬁcation on the used datasets.
Also, [51] was about 2× faster than ours, not counting
the communication time. Indeed, they needs to exchange
O(32· 210 · δ(d + N )) bits of ciphertexts. On the other hand,
for d < n, our method sends only one RLWE ciphertext and
(a) breast cancer (N = 569)
(b) ionosphere (N = 351)
(c) wine (N = 178)
(d) Control charts (N = 600)
Figure 8: Test accuracy of K-means clustering with a propor-
tion of closest assignments. The baseline is obtained from
the scikit-learn package [44]. All numbers are averaged from
20 runs of 10-fold cross-validation.
one LWE ciphertext, leading to much smaller communica-
tion overhead, i.e., about 1% of that of [51]. The method
[37] is fast on the cloud side but it sends O(N ) RLWE
ciphertexts to the client to decryption, which could lead to
a large communication overhead and a long decryption time
when N increases. Also, [37] used 12-bit ﬁxed point values
and it might introduce some rounding errors, e.g., about 2%
accuracy loss observed in our experiments.
3) Secure K-means Clustering: We evaluated our secure
K-means algorithm on synthesis data X ∈ RN×d where
the values of X were sampled from [−1, 1] uniformly a
random. The running time of our algorithm is separated into
ﬁve sectors and reported in Table VIII. The O(2KN ) LUTs
from the Min-Index sector took about 95% of the compu-
tation time, which can be easily accelerated by using more
cores. Also, the running time of arithmetic computation, i.e.,
computing Euclidean distance (4th column) and updating
cluster centroids (8th column) took just a few seconds. The
repacking sector converts N LWE ciphertexts to RLWE.
We can see that the running time of repacking grows
in √N when N ≤ n, and it stays over 60 seconds for a
larger number of data points n < N < n. We emphasize
that our K-means algorithm is faster than the existing HEs-
based secure K-means algorithms. For example, [34] used
TFHE for the same K-means clustering problem. However,
the boolean circuit representation of the K-means algorithm
could consist of millions of gates which could take a long
time to evaluate. To improve the efﬁciency, [34] suggested
to use an approximate comparison by comparing the highest
bits of the encrypted ﬁxed-point values at the cost of intro-
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:25:55 UTC from IEEE Xplore.  Restrictions apply. 
1068
12131415Lattice Dimension log2(n)100101LUT Time (sec)0.971.964.099.0578910111213LUT Accuracy (bits)1/(1+e-x)tanh(x)√|x|max(0,x)Table VII: Compare the performance of our private decision tree evaluation algorithm with the existing approaches. N is the
number of internal nodes in the tree, d is the number of features, and C is the number of classiﬁcation labels. Computation
time includes the evaluation time on both sides of the cloud and the client. On the cloud side, 16 threads were used for
these experiments, following the same settings of [37], [51].
Setting
N
≈ 10
≈ 100
≈ 60
Data
Iris
Housing
Spambase
Computation
Communication
d
4
13
57
C
3
2
2
[37]
0.59s
10.27s
6.88s
[51]
0.94s
6.30s
3.66s
Ours
1.87s
10.71s
6.75s
[37]
[51]
Ours
1.65 MB
13.12 MB
11.54 MB
1.19 MB
6.63 MB
7.36 MB
16.89 KB
16.89 KB
16.89 KB
Classiﬁcation Accuracy
[51]
On Plaintexts
[37]
Ours
97.37% 95.33% 94.74%
100.0% 98.12% 98.04%
87.86% 87.06% 85.03%
97.37%
100.0%
87.86%
Table VIII: Running time of one update of our privacy-preserving clustering algorithm where N and K is the number of
data points and the number of clusters, respectively. Dimension d = 16 and 20 threads were used.
Setting
Closest Assign Ratio ρ
Break-down Running Time (for n = 212)†
Centroid
Repacking
K
2
4
8
2
4
8
2
4
8
N
256
1024
4096
n = 212(213)
Distance
82.03%(92.97%)
83.59%(97.27%)
86.72%(94.92%)
[34, §6]‡
19.81min
39.61min
79.23min
79.23min
158.45min
316.89min
316.89min
633.79min
1267.58min
† For n = 213, the runtime of the Min-Index sector (i.e,. 6th column) was increased by about 2× and other sectors were almost unchanged.
‡ By estimation using the provided runtimes per update, per centroid, per data point, per CPU in [34, § 6].
Extract Min-Index & Recip.
1.83s
2.60s
4.36s
5.33s
8.95s
16.31s
17.78s
32.48s
57.04s
1.35min
2.33min
4.09min
3.66min
7.57min
15.34min
13.95min
26.61min
52.04min
51.53s
107.78s
211.31s
109.27s
387.08s
844.58s
756.33s
1500.37s
3000.85s
1.56s
2.69s
3.11s
1.58s
2.79s
3.10s
1.61s
2.73s
3.03s
0.35s
0.50s
0.87s
0.33s
0.45s
0.97s
0.32s
0.45s
1.04s
79.88%(95.41%)
86.13%(95.80%)
86.52%(95.31%)
82.10%(94.65%)
86.33%(96.00%)
88.79%(96.14%)
Total
25.93s
55.22s
60.69s
Speedup
14×
17×
19×
21×
21×
20×
22×
23×
24×
ducing about 5% misclassiﬁcation rate (cf. Fig 6 of [34]).
Nevertheless, their optimized method could take up to 20×
longer time than ours.
Our algorithm introduces approximation errors during the
computation. A data point could be assigned to a “wrong”
centroid particularly when two centroids are both close to
that data point. In the 3rd column of Table VIII, we counted
the percentage of data points that were properly assigned to
the closest centroid. We observed that about ρ ≈ 82% of the
data points were assigned to the closest centroid when using
the parameter n = 212. This proportion ρ can be improved
to about 95% by using a larger n = 213 at the cost of
increasing the time of the Min-Index sector by about 2×.
To study the effects of ρ on the classiﬁcation accuracy of
the K-means, we simulated on four real datasets from [23].
In brief, in each centroid update, we randomly chose a
proportion of 1− ρ of the training points and independently
assign them to a randomly chosen centroid, excluding the
closest one. From the simulation results shown in Fig 8, the
classiﬁcation accuracy of K-means seems quite robust. The
accuracy did not drop much (within 4% when ρ ≥ 0.8)
compared to the baseline in which all data points were
assigned to the closest centroid.
VII. CONCLUSION
In conclusion, this work presents PEGASUS, a highly
optimized framework that allows us to efﬁciently and
effectively perform both polynomial functions and non-
polynomial functions on encrypted data. The main technical
contribution is two-fold. We propose a new FHEW →
CKKS conversion algorithm achieving better performance
and a signiﬁcantly smaller key compared to the prior work
of CHIMERA [7]. Also, we extend the prior approach for
homomorphic look-up table evaluation [43] to accept a
wide range of input. Finally, we showed that PEGASUS
can be applied to many real-life applications,
including
the private decision tree evaluation on an encrypted query,
and secure outsourced K-means clustering on an encrypted
dataset. We consider that PEGASUS is practical for a wide
range of privacy-preserving scenarios, especially for cloud-
based applications. One of our future work is to improve
the performance of PEGASUS using hard-wares such as
GPGPU and FPGA.
ACKNOWLEDGMENT