We next explain how to improve the previous approach, which
computed the two confidence intervals separately. Now, we show
how to reduce the number of required samples, by leveraging
F,Φ (x′) are correlated
that Si = checki
(i.e., derived from a joint distribution).
We
recall
and
F,Φ (x′) based on the same randomness, resulting in
checki
high correlation between the samples. We empirically observed
correlations as high as ρ = 0.999. This can drastically decrease the
(cid:16)checki
(cid:17) originate from
required number of samples, as illustrated in Fig. 6.
the samples (Si , S′
F,Φ (x′)
(cid:16) Pr [F (x ) ∈ Φ] , Pr [F (x′) ∈ Φ](cid:17).
F,Φ (x ), checki
a joint distribution SSS over R2, which has (component-wise) mean
Observing the estimates (cid:16)(cid:68)Pr [F (x ) ∈ Φ] ,(cid:68)Pr [F (x′) ∈ Φ](cid:17) com-
E [SSS] ∈ R2 given by E [SSS] =
(cid:17) induces a likelihood on
puted according to(cid:16) 1
(cid:80)n
(cid:80)n
(cid:16) Pr [F (x ) ∈ Φ] , Pr [F (x′) ∈ Φ](cid:17) (depicted as a multivariate Gauss-
i =1 S′
Fig. 7 provides an overview of this approach. On a high level,
ian distribution in Fig. 7). From this, we can derive a likelihood on
ϵ (x, x′, Φ) (see Fig. 7), and finally compute a confidence interval for
ϵ (x, x′, Φ) (depicted as the shaded area in Fig. 7).
i =1 Si ,
i ) =
1
n
n
i
1
n
Computing the Likelihood of Pr [F (x ) ∈ Φ].
the distribution of (cid:68)Pr [F (x ) ∈ Φ], we conclude that observing
(cid:68)Pr [F (x ) ∈ Φ] induces a likelihood on Pr [F (x ) ∈ Φ] given by
(cid:103)(cid:19)
Based on
(cid:18)(cid:68)Pr [F (x ) ∈ Φ] ,
Pr [F (x ) ∈ Φ] ∼ N
checkF,Φ (x )
Var
(8)
(cid:102)
.
observing the estimate(cid:68)Pr [F (x ) ∈ Φ], some values of Pr [F (x ) ∈ Φ]
confidence interval is [(cid:68)Pr [F (x ) ∈ Φ] − ∆,(cid:68)Pr [F (x ) ∈ Φ] + ∆], for
∆ := −σ · Φ−1(cid:16) α /2
For details on the derivation of this likelihood, see App. A. We
note that while mathematically, Pr [F (x ) ∈ Φ] is a constant, after
are more likely than others, thus inducing a distribution on
Pr [F (x ) ∈ Φ], which we call the likelihood of Pr [F (x ) ∈ Φ].
Computing the Confidence Interval. From Eq. (8), we can de-
rive an interval for Pr [F (x ) ∈ Φ] with confidence 1 − α/2. The
. Here, Φ−1 is the inverse of the cumulative
distribution function of a Gaussian distribution with mean 0 and
standard deviation 1, and σ is the standard deviation of the Gaussian
distribution in Eq. (8). At runtime, DP-Finder does not have access
to the standard deviation σ :=
. Instead, it
estimates σ empirically, computing ∆ according to
α/2
2
checkF,Φ (x )
checkF,Φ (x )
(cid:114) 1
(cid:113) 1
(cid:103)
(cid:33)
∆ := −
−1
· Φ
(cid:102)
(cid:103)
Var
(cid:32)
(cid:17)
(cid:102)
n
2
,
(cid:102)
where (cid:76)Var
n
checkF,Φ (x )
is the empirical variance of checkF,Φ (x )
(the definition of empirical variance is given in App. B).
Bounds on ϵ (x, x′, Φ). As in Sec. 4.3, we compute confidence
intervals for Pr [F (x ) ∈ Φ] and Pr [F (x′) ∈ Φ], which jointly hold
with confidence 1 − α (again due to the union bound). Then, we
can derive bounds on ϵ (x, x′, Φ), also as in Sec. 4.3.
Example. We illustrate how to use this approach when estimating
ϵ (x, x′, Φ) for some (x, x′, Φ). We note that unlike for Hoeffding’s
inequality, ∆ depends on the empirical variance of checkF,Φ(x ),
and hence on the samples S1, . . . , Sn. Thus, we cannot compute ∆
before sampling from checkF,Φ (x ).
Assume that for our (x, x′, Φ), we get (cid:68)Pr [F (x ) ∈ Φ] ≈ 3.24%
and (cid:68)Pr [F (x′) ∈ Φ] ≈ 3.04%. In addition, the empirical vari-
ance of checkF,Φ (x ) is (cid:76)Var
(cid:102)
(cid:76)Var
≈ 2.95% (computed according to App. B).
≈ 3.13% and likewise,
checkF,Φ (x )
checkF,Φ (x′)
(cid:102)
(cid:103)
(cid:103)
(cid:76)Var
(cid:103)
Pr[P(x)∈Φ]Pr[P(x0)∈Φ]likelihoodˆˆ−∆ˆˆ+∆.
(9)
Multidimensional Central Limit Theorem (M-CLT). To ana-
lyze the effect of correlation on the size of the confidence interval,
we consider the central limit theorem for multivariate distributions:
Theorem 4.3 (Multidimensional central limit theorem,
Prop. 2.18 from [34]). Let S1S1S1, . . . ,SnSnSn ∈ Rk be independent samples
from a distribution SSS over vectors. Let E [SSS] ∈ Rk be the (component-
wise) expectation of SSS, and Cov [SSS] ∈ Rk×k the covariance matrix of
SSS. Then, as n → ∞ the distribution of 1
i =1 SSSi converges to a mul-
n
tivariate Gaussian distribution with mean E [SSS] ∈ Rk and covariance
(cid:18)
matrix 1
n
(cid:80)n
(cid:19)
n(cid:88)
d−−−−−→
n→∞ N
SiSiSi
i =1
E [SSS] ,
Cov [SSS]
.
1
n
Cov [SSS]:
1
n
As in the previous section, our version of the CLT deviates from
the standard presentation, by (i) using n within the limiting dis-
tribution and (ii) referring to each individual vector SiSiSi ∈ Rk as a
sample.
Heuristic Inspired by the M-CLT. Analogously to the previous
section, we apply a heuristic inspired by the M-CLT (k = 2), treating
the joint distribution of(cid:68)Pr [F (x ) ∈ Φ] and(cid:68)Pr [F (x′) ∈ Φ] as having
converged to the limiting distribution:
(cid:33)
(cid:33)
∼ N
(cid:32)(cid:68)Pr [F (x ) ∈ Φ]
(cid:68)Pr [F (x′) ∈ Φ]
(cid:16)checkF,Φ (x ), checkF,Φ (x′)
(cid:32)(cid:32) Pr [F (x ) ∈ Φ]
Pr [F (x′) ∈ Φ]
(cid:33)
(cid:33)
(cid:17), defined by:
(cid:32) Var [S]
1
n
C
,
Cov [S, S′]
Var [S′]
.
C :=
Cov [S, S′]
where C is the covariance matrix of the two-dimensional distribu-
tion (S, S′) =
ing(cid:16)(cid:68)Pr [F (x ) ∈ Φ] ,(cid:68)Pr [F (x′) ∈ Φ](cid:17), Eq. (9) induces a likelihood on
Computing the Joint Likelihood. Like in Sec. 4.4, after observ-
(cid:33)
the probabilities Pr [F (x ) ∈ Φ] and Pr [F (x′) ∈ Φ]:
(cid:32) Pr [F (x ) ∈ Φ]
(cid:32)(cid:32)(cid:68)Pr [F (x ) ∈ Φ]
(cid:68)Pr [F (x′) ∈ Φ]
Pr [F (x′) ∈ Φ]
∼ N
(10)
C
.
1
n
(cid:33)
(cid:33)
,
Estimating the covariance matrix C by the empirical variance
and covariance of S and S′ (see App. B) gives us an approximate
joint likelihood on Pr [F (x ) ∈ Φ] and Pr [F (x′) ∈ Φ].
Ratio of Gaussian Random Variables. We use this approximate
joint likelihood of Pr [F (x ) ∈ Φ] and Pr [F (x′) ∈ Φ] to compute an
approximate cumulative distribution function (CDF) of ϵ (x, x′, Φ)
based on known work (Thm. 4.4).
The key insight is to view ϵ (x, x′, Φ) as the logarithm of a ratio
of Gaussian random variables, which allows us to apply Thm. 4.4,
yielding an approximate CDF for ϵ (x, x′, Φ), which in turn allows
us to compute the approximate probability that ϵ (x, x′, Φ) lies in
the interval [ˆϵ (x, x′, Φ) − ∆ϵ , ˆϵ (x, x′, Φ) + ∆ϵ ], for some ∆ϵ .
Theorem 4.4 (Ratio of normal distributions [19]). Let
(X , X ′) be two random variables that are normally distributed ac-
cording to:
(cid:33)
(cid:32)
X
X ′
∼ N
(cid:32)(cid:32)
(cid:33)
(cid:32)
µ
µ′
,
2
σ
ρσσ ′
(cid:33)(cid:33)
.
ρσσ ′
(σ ′)
2
where σ and σ ′ are the standard deviations of X and X ′ respectively,
and ρ is the correlation of X and X ′.
Then, the cumulative distribution function (CDF) of the ratio X /X ′
µ − µ′w
σσ ′a(w )
µ′w − µ
σσ ′a(w )
,− µ′
σ ′ ; σ ′w − ρσ
(cid:33)
σσ ′a(w )
µ′
σ ′ ; σ ′w − ρσ
σσ ′a(w )
,
(cid:33)
+
.
(cid:20)
is given by:
Pr
(cid:21)
(cid:32)
(cid:32)
X
=L
X ′ ≤ w
(cid:113) w 2
σ 2 − 2ρw
L
σ σ′ + 1
(σ′)
2 and L(h, k; γ ) is the standard
where a(w ) =
bivariate normal integral (see App. C).
We apply Thm. 4.4 for X = Pr [F (x ) ∈ Φ] and X ′ = Pr [F (x′) ∈ Φ].
µ = (cid:68)Pr [F (x ) ∈ Φ], µ′ = (cid:68)Pr [F (x′) ∈ Φ] and
Instantiating the parameters of Thm. 4.4 according to Eq. (10) yields
n C.
To compute an approximate CDF on ϵ (x, x′, Φ), we set w to
(cid:35)
(cid:34)
exp(u), which yields
ρσσ ′
(σ ′)
2
2
σ
ρσσ ′
= 1
(cid:32)
(cid:33)
(cid:35)
(cid:34)
Pr
(cid:124)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:123)(cid:122)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:125)
log Pr [F (x ) ∈ Φ]
Pr [F (x′) ∈ Φ]
ϵ (x,x′,Φ)