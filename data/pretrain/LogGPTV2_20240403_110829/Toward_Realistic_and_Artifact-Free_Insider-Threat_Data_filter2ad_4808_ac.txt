rently under review (right panel). Potentially sensitive content is displayed with a gray background.
Content that has been marked by the user as sensitive is displayed on a black background.
6. Experimental methodology
6.2. Data collection and subject selection
We used these three tools (the Monolog data collec-
tor, the insider-script library, and the Sanitizer application)
to investigate the consequences of sanitization artifacts on
Maxion’s insider-threat experiment [7]. He compared the
performance of the naive-Bayes detector on two types of
data (Truncated and Enriched). Our goal was to establish
whether one particular sanitization strategy (Word-Token)
was artifact-free for this experiment, and what the conse-
quences are when we use the other two strategies (Redact-
Only and Token-Only).
6.1. Deploy the data collector
We deployed the Monolog data-collection software on
workstations used by twelve system administrators and op-
erations staff members in our organization. These users
were shown that the monitoring shell would appear iden-
tical to the one they normally used even though it was in-
strumented to collect data. They were instructed as to what
data were being collected (e.g., no passwords unless they
are typed on the command line), and they were assured
that only sanitized data which they vetted would leave their
workstation and never raw, unsanitized data.
In order to collect a large sample of legitimate user be-
havior, we left the staff for four to six months to use their
workstations as they normally would. Once a week, a re-
searcher would record the number of shell sessions col-
lected and the number of commands in each session. On
the basis of these records, four subjects were selected to
continue with the study. Seven of the twelve staff members
were deselected because they were part-time operators or
primarily used a Windows workstation, and so only a few
sessions were recorded for them. One full-time operator
was not selected because he had night and weekend shifts,
and there was no convenient opportunity for a researcher to
meet with both him and his supervisor. The four subjects
were all full-time operators or system administrators and
each accumulated over 100 recorded sessions.
A sample of four subjects may seem small. However, the
subjects all come from a population that is small but valu-
able (i.e., experienced system administrators and operators).
A survey has shown that such positions are at high risk of
insiders [4], and every full-time operator in our organiza-
tion except one (as noted above) was included in the pool.
Further, since we gather many sessions of data from each
subject, and sessions are the true “element” being studied,
9292
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 12:38:02 UTC from IEEE Xplore.  Restrictions apply. 
we judged the size of the subject pool to be adequate for
this work. A larger, more heterogeneous pool would be de-
sirable in future studies.
export their data using each of the three sanitization strate-
gies (Redact-Only, Token-Only, and Word-Token).
6.3. Perform insider-attack injections
We presented the four attack scripts to the subjects and to
their supervisors (who administer their workstations). We
described the details of each attack, and we demonstrated
the post-attack recovery procedure. Permission to run each
attack script was obtained from all parties, and the scripts
were deployed on the subjects’ workstations.
The subjects ran the four attack scripts against their own
accounts. They typed the script names into shells that were
not monitored by the Monolog data collector (to avoid in-
troducing injection artifacts). With the subject, we observed
the progress of the attack, veriﬁed that the attack succeeded,
and conﬁrmed that the recovery mechanism repaired any
damage.
6.4. Sanitize the data ﬁles
We asked the subjects to sanitize their own sessions.
They were given a demonstration of the Sanitizer and in-
structed on the difference between redacting and tokenizing
sensitive data. A researcher navigated through the interface
on an illustrative data set. He explained that he would mark
usernames and hostnames with the Tokenize button, but he
would use the Redact button for a password and a poten-
tially embarrassing URL because of their sensitivity. The
subjects were instructed that they should use their own judg-
ment in deciding what to tokenization or redaction.
We instructed the subjects to sanitize at least 60 sessions
worth of data because 50 sessions would be used by an
insider-threat detector to build a proﬁle of their usage and
the remaining 10 would be used to test that proﬁle. We
also asked the subjects to review the four sessions that con-
tain the attack injections and to sanitize those as they would
sanitize their other sessions. In this manner, each subject
sanitized at least 64 sessions of his or her own data.
The subjects took between 10 and 30 minutes to sanitize
their data, and they sanitized between 111 and 219 sessions
each. One or more words were tokenized or redacted from
40% of the command lines. One subject sanitized 58% of
the command lines, while another sanitized only the sub-
ject’s own username and home directory. Only one subject
used the Redact button, but all subjects used the Tokenize
button.
While the subjects sanitized their data, a researcher was
present to offer assistance, but he did not look at the unsan-
itized data on their screen unless given explicit permission.
After sanitizing their data, the subjects were instructed to
6.5. Create evaluation data sets
Since we intended to replicate the experiment conducted
by Maxion [7], we had to derive data sets analogous to
the ones he used in his experiment. Maxion compared
the performance of an insider-threat detector when moni-
toring “Truncated” command-line data to its performance
when monitoring “Enriched” command-line data. Trun-
cated command-line data consists only of the names of the
programs executed, while Enriched command-line data in-
cludes the whole command line. Maxion’s hypothesis was
that using Enriched data lowered the cost of error (calcu-
lated as the sum of the false-alarm rate and the miss rate),
and he found a 9% reduction in cost.
In order to replicate Maxion’s experiment, we derived
two evaluation data sets (Truncated and Enriched) from
each of the three sanitized data sets (Redact-Only, Token-
Only, and Word-Token). To create the Redact-Only Trun-
cated evaluation data set, we extracted the commands typed
by the subject in each session of the Redact-Only sanitized
data. We “truncated” the commands so that only the com-
mand name was left (i.e., discarding ﬂags, ﬁlenames, and
other command arguments). We labeled the ﬁrst 50 sessions
of commands as training data. We labeled the next 10 ses-
sions and the 4 attack-injected sessions as test data. These
data, labeled for training and test, constitute the Redact-
Only Truncated evaluation data set. To create the Redact-
Only Enriched evaluation data set, we extracted commands
typed by the subject in each session of the Redact-Only san-
itized data, but we did not truncate the commands. We label
sessions of un-truncated (or “enriched”) commands as train-
ing and test data in the same way as in the Truncated data
set. We derived Truncated and Enriched evaluation data
sets from each of the Token-Only and Word-Token sanitized
data using the same procedure.
To compare the results of Maxion’s experiment on sani-
tized data to the equivalent experiment on raw, unsanitized
data, we created Truncated and Enriched evaluation data
sets using each subject’s raw data as well. The procedure
described above for creating Truncated and Enriched data
sets was scripted. We deployed the script to each subject’s
workstation, and we asked the subjects to create Truncated
and Enriched evaluation data sets from the raw data using
this script. In this way, the raw data never left the subjects’
workstations. For each subject, a total of eight evaluation
data sets were created: two evaluation data sets (Truncated
and Enriched) from the raw data, and two from each of the
three types of sanitized data (Redact-Only, Token-Only, and
Word-Token).
9393
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 12:38:02 UTC from IEEE Xplore.  Restrictions apply. 
6.6. Evaluate the naive-Bayes detector
The insider-threat detector evaluated in this study is
based on that used previously to evaluate the effect of us-
ing Enriched rather than Truncated command lines [7]. The
insider-threat detector builds a proﬁle of command usage
from the user’s training sessions. For each test session, the
detector calculates an anomaly score that represents how
much the session deviates from the proﬁle. Consequently,
the detector attempts to separate the user’s own sessions
(which should ﬁt the proﬁle) from the attack-injected ses-
sions (which should not).
The details of this detector are described fully by Max-
ion and Townsend [8] who refer to this particular detector
as naive Bayes with one-class training input. Basically, the
proﬁle of the user is built by counting the number of oc-
currences of each command in the training sessions, and
calculating the relative frequency of each command. More
formally, let n be the number of commands in the training
data, and k be the number of unique commands. If the com-
mands are named c1, c2, . . . , ck, then let n1 be the number
of times c1 appears in the training sessions, n2 be the num-
ber of times c2 appears, and so on. The proﬁle consists of
probability estimates for each command that are calculated
as follows.
P (c1) =
n1 + α
n + αk
, P (c2) =
n2 + α
n + αk
,··· , P (ck) =
nk + α
n + αk
The addition of α to the numerator and αk to the denomina-
tor prevents any command from having an estimated proba-
bility of zero. The term α is a conﬁgurable parameter of the
detector called the pseudocount. In accordance with Max-
ion [7], we set the pseudocount to 0.01.
The detector is called naive Bayes because it (naively)
assumes that the probability of a session of commands is
simply the product of the probabilities of the individual
commands (as though each command’s probability were in-
dependent). The anomaly score for a test session is the
negative logarithm of the probability of the session given
the proﬁle. To account for sessions of different length, the
anomaly score is normalized by dividing by the number of
commands in the session. Formally, let n(cid:1)
be the number of
commands in the test session, and let c(cid:1)
1 be the name of the
ﬁrst command, c(cid:1)
2 be the name of the second command, and
so on. The anomaly score is calculated using the probability
estimates for each command from the training-data proﬁle
as follows.
anomaly score = − 1
n(cid:1)
n(cid:1)(cid:1)
i=1
log P (c(cid:1)
i)
Using the anomaly score, the detector makes a determina-
tion as to whether or not an alarm should be raised. The
determination is made by comparing the anomaly score to
a threshold. If the score exceeds the threshold, an alarm
is raised. The threshold is calculated using ﬁve-fold cross-
validation on the training data. The training sessions are
divided into ﬁve groups called folds. The ﬁrst fold is set
aside and the remaining four-ﬁfths are used to build a pro-
ﬁle. The anomaly scores are then calculated for the ﬁrst
fold using that proﬁle. Anomaly scores for each of the other
folds are calculated in the same way (using a proﬁle built on
the four-ﬁfths of the training data not in the fold). The max-
imum score in each fold is calculated, and the threshold is
calculated as the average of these maximum scores.
For each of the eight evaluation data sets, we trained the
naive-Bayes detector on the training sessions and tested it
on the test sessions. Each evaluation data set contained 4
attack-injected sessions for each of the 4 subjects, leading
to a total of 16 attack-injected sessions. Each data set con-
tained 10 non-attack sessions of test data for each of the 4
subjects, leading to a total of 40 sessions of non-attack test
data overall. The anomaly score and detector response (i.e.,
alarm or no alarm) were recorded for each test session.
To obtain the responses of the detector on the Truncated
and Enriched evaluation data sets derived from the subject’s
raw data, we deployed the naive-Bayes detector on each
subject’s workstation. The subjects ran the detector on each
of their two raw evaluation data sets, inspected the log con-
taining the detector’s responses to conﬁrm that it contained
no sensitive information, and released it to the researchers.
7. Results and analysis
The consequences of each of the three sanitization strate-
gies on the experiment are shown in Table 1. The perfor-
mance of naive Bayes is shown for each of the eight evalu-
ation data sets in terms of misses, false alarms, and cost of
error. For consistency with Maxion’s analysis [7], the cost
of error was calculated as the sum of the miss rate and the
false-alarm rate. Since he compared the cost of Enriched
data to Truncated data, the cost ratio of the two has been
calculated for the raw data and for each of the three saniti-
zation types. Unlike the earlier experiment which found a
cost reduction of 9% by using Enriched data, we found En-
riched data to have a 79% higher cost than Truncated data