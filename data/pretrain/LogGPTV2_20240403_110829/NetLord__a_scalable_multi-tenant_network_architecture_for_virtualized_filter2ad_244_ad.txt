size (in entries). (SPAIN requires a FIB size proportional to the
square of the number of edge switches.)
Table 1 shows the maximum number of MAC addresses (VM)
that NetLord could support, for various switch port counts and FIB
sizes. Following a rule of thumb used in industrial designs, we
assume V = 50.
With existing single-chip switches that feature 72 ports and 64K
FIB entries [12], a NetLord network could support 650K VMs. Us-
ing plausible future 144-port ASICs with the same FIB size, Net-
Lord could support 1.3M VMs. The table suggests that ASIC de-
signers might consider increasing port count at the expense of FIB
size; the number of VMs scales linearly with ports, but only with
the square root of the FIB size.
Note that the original SPAIN design can scale to large numbers
of physical paths, but at the cost of large FIBs. NetLord removes
this restriction, and so allows the network to support a number of
VMs consistent with its physical scale.
Ease of operation: NetLord simpliﬁes network operation in three
different ways, thereby reducing operating costs. First, as ex-
plained in Section 3, NetLord automates all switch conﬁguration.
The conﬁguration details are either computed ofﬂine (e.g., SPAIN’s
VLAN conﬁguration), or are autonomously determined by individ-
ual entities (e.g., the IP forwarding tables in the switches). Also,
most of the conﬁguration is static; this not only eliminates the need
for constant supervision by humans, but also makes debugging and
trouble-shooting easier.
Second, NetLord makes it easy to exploit standard mechanisms
on commodity switches to implement tenant-speciﬁc trafﬁc engi-
neering, ACL, and isolation policies. As a design principle, Net-
Lord uses only standard header formats; this exposes all important
tenant information in header ﬁelds supported by even the most ba-
sic ACL mechanisms. For instance, an ACL based on the low-order
bits of the destination IP address (the tenant ID) can match all pack-
ets belonging to a single tenant. Similarly, a tenant’s ﬂow between
two physical servers can be easily identiﬁed by matching on the
source and destination MAC addresses, and on the port-number bits
of the IP destination address. One could use these mechanisms, to-
gether with NLA support, to deploy sophisticated QoS via a central
controller (analogous to OpenFlow).
Finally, by supporting high bisection bandwidth without con-
cerns about FIB pressure, NetLord removes network-based restric-
tions on VM placement. The VM manager need not try to co-locate
communicating VMs, and can instead place them based on resource
availability (e.g., CPU or RAM) or power management.
Low-cost switches: NetLord can efﬁciently utilize inexpensive,
feature- and resource-limited commodity switches. The datacenter
operator need not pay for IP routing support, or to upgrade switches
to support novel L3 features (e.g., IPv6).
4.2 Design rationale
Why encapsulate? VM addresses can be hidden from switch FIBs
either via encapsulation or via header-rewriting. Encapsulation is
often thought to suffer three major drawbacks: (1) Increased per-
packet overhead – extra header bytes and CPU for processing them;
(2) Heightened chances for fragmentation and thus dropped pack-
ets; and (3) Increased complexity for in-network QoS processing.
Even so, we used encapsulation for two reasons. First, the header
re-writing cannot simultaneously achieve both address-space virtu-
alization and reduced FIB pressure. Second, in a datacenter, we can
easily address the drawbacks of encapsulation.
Header rewriting clearly would not suffer from increased byte
overheads or fragmentation. However, since we want to support
an unconstrained L2 abstraction, and therefore cannot assume the
presence of an IP header, the only two ﬁelds we could rewrite are
the source and destination MAC addresses. But to avoid FIB pres-
sure from exposed VM addresses, we would have to rewrite these
header ﬁelds with the source and destination agent (server) MAC
addresses; this would then make it impossible for the receiving
agent to know which VM should receive the packet. Diverter [11]
solves this problem by requiring tenant VMs to send IP packets
(using a deﬁned addressing scheme), which allows the agent to
map the destination IP address to the correct VM. Also, this ap-
proach still exposes server MAC addresses to switch FIBs, which
limits scalability. (Nor could we rewrite using edge-switch MAC
addresses, because the egress edge switch would not know where
to forward the packet.) Thus, we believe encapsulation is required
to offer tenants an L2 abstraction.
What about per-packet overheads, both on throughput and on la-
tency? Our measurements (section 5) show that these overheads
are negligible. Modern multi-GHz CPUs need only a few tens of
nanoseconds to add and delete the extra headers; the added wire
delay at 10Gbps is even smaller. Throughput overheads should be
negligible if tenants use 9000-byte “jumbo” packets, which all dat-
acenter switches support.
Encapsulation in NetLord also adds one extra hop to Diverter’s
“one-hop” paths. However, NetLord suffers no performance loss,
because unlike in Diverter, this extra hop is done in hardware of the
egress edge switch.
We can also mitigate fragmentation by exploiting jumbo packets.
Our NetLord virtual device (in the hypervisor) exports a slightly
smaller MTU (34 bytes fewer) than the physical device. Because
in a datacenter we can ensure a consistent, high MTU across all
switches, and our encapsulation header always sets the “Do-not-
fragment” bit, we completely eliminate the possibility of NetLord-
generated fragments. (This also frees up the fragmentation-related
IP header ﬁelds for us to use for other purposes; see section 3.5.)
Finally, as explained in section 4.1, we can exploit NetLord’s en-
capsulation scheme to make it easier to identify tenants via simple
ACLs.
69Why not use MAC-in-MAC? We could have used a “MAC-in-
MAC encapsulation”, where the sending NLA encapsulates the
packet with only a MAC header, setting the source and destina-
tion MAC addresses set to those of the ingress and egress switches.
This would add less overhead than our chosen encapsulation, but
would create some problems. First, this would require the egress
edge-switch FIB to map a VM destination MAC address to one
of its output ports. This implies that no two VMs connected to
an edge switch could have the same MAC address, which would
impose unwanted restrictions on VM placement. Further, since ar-
bitrary tenant-assigned L2 addresses, unlike NetLord’s encoded IP
address, cannot be aggregated, this approach requires an order of
magnitude more FIB entries. Perhaps worse, it also requires some
mechanism to update the FIB when a VM starts, stops, or migrates.
Finally, MAC-in-MAC is not yet widely supported in inexpensive
datacenter switches.
NL-ARP overheads: NL-ARP imposes two kinds of overheads:
bandwidth for its messages on the network, and space on the servers
for its tables.
In steady-state operation, most NL-ARP messages would be the
gratuitous NLA-HERE broadcasts. We show, through a simple
analysis, that these impose negligible overheads.
All NLA messages ﬁt into the 64-byte (512-bit) minimum-length
Ethernet packet. With the worst-case 96-bit inter-frame gap, an
NLA packet requires 608 bit-times.
Most new servers come with at least one 10Gbps NIC. If we limit
NLA-HERE trafﬁc to just 1% of that bandwidth, a network that
supports full bisection bandwidth can sustain about 164,000 NLA-
HERE messages per second. That is, the network could support
that rate of VM migrations or boots. Assuming that most VMs
persist for minutes or hours, that should be sufﬁcient for millions
of VMs. Further, we believe that NLA-WHERE broadcasts and
NLA-NOTHERE unicasts should occur so rarely as to add only
negligible load.
NL-ARP also requires negligible table space on servers. Each
table entry is 32 bytes. A million such entries can be stored in
a 64MB hash table, at 50% loading. Since even small datacenter
servers have 8GB of RAM, this table consumes about 0.8% of that
RAM.
5. EXPERIMENTAL ANALYSIS
We did an experimental analysis, both to measure the overhead
of the NetLord Agent (NLA), and to evaluate the scalability of
NetLord. We measured overheads using a micro-benchmark (sec-
tion 5.1) and scalability by emulating thousands of tenants and hun-
dreds of thousands of VMs. We have not yet measured the cost of
the control plane, including NL-ARP.
We implemented the NetLord agent as a Linux kernel module.
We started with our implementation of SPAIN [18], adding about
950 commented lines of code. This includes all components of Net-
Lord, except the NL-ARP subsystem. We implemented NL-ARP
lookup tables, but have not fully implemented the message han-
dlers, so we ran our experiments using statically-conﬁgured lookup
tables; thus, no NL-ARP messages are sent. We ran our tests using
Ubuntu Linux 2.6.28.19.
5.1 NetLord Agent micro-benchmarks
Since the NLA intercepts all incoming and outgoing packets, we
quantiﬁed its per-packet overheads using two micro-benchmarks,
“ping” for latency and Netperf 3 for throughput. We compare our
NetLord results against an unmodiﬁed Ubuntu (“PLAIN”) and our
3www.netperf.org
original SPAIN implementation. We used two Linux hosts (quad-
core 3GHz Xeon CPU, 8GB RAM, 1Gbps NIC) connected via a
pair of switches that served as “edge switches.”
Table 2: Microbenchmarks for NetLord overheads
Case
Ping
(in µs)
NetPerf
1-way
(in Mbps)
NetPerf
2-way
(in Mbps)
Metric
PLAIN
SPAIN
NetLord
min/max
avg
avg
min
max
avg
min
max
97
90/113
987.57
987.45
987.67
1835.26
1821.34
1858.86
99
95/128
987.46
987.38
987.55
1838.51
1826.49
1865.43
98
93/116
984.75
984.67
984.81
1813.52
1800.23
1835.21
Our ping (latency) experiments used 100 64-byte packets. The
ﬁrst row in Table 2 shows the results (average, minimum, and max-
imum). NetLord appears to add at most a few microseconds to the
end-to-end latency.
We measured both one-way and bi-directional TCP throughput
using Netperf. For each case, we ran 50 10-second trials. We
used jumbo (9000-byte) packets, because we observed erratic re-
sults (even with PLAIN) when using 1500-byte packets in the two-
way experiments. (We hypothesize, but have not conﬁrmed, that
this is due to the limited number of buffer descriptors in the spe-
ciﬁc NIC in our testbed hosts.) Note that, because of our 34-byte
encapsulation headers, NetLord exposes an 8966-byte MTU to ap-
plications.
The second and third rows in Table 2 show throughput results
(mean, min., and max.). NetLord causes a 0.3% decrease in
mean one-way throughput, and a 1.2% drop in in mean two-way
throughput. We attribute these nearly-negligible drops mostly to
the smaller MTU.
5.2 Emulation methodology and testbed
We have asserted that NetLord can scale to large numbers of
VMs using inexpensive switches. We were limited to testing on just
74 servers, so to demonstrate NetLord’s scalability, we emulated a
much larger number of VMs than could normally run on 74 servers.
We implemented a light-weight VM emulation by adding a shim
layer to the NLA kernel module. Conceptually, each TCP ﬂow end-
point becomes an emulated VM. The shim layer exposes a unique
MAC address for each such endpoint, by mapping from a source
or destination hIP _addr, T CP _porti tuple to synthesize a cor-
responding MAC address. The shim executes, for every outgo-
ing packet, before the NLA code, and rewrites the packet’s MAC
source and destination addresses with these synthetic addresses.
Thus, the NLA sees one such emulated VM for each ﬂow from
our workload-generator application.
Using this technique, we can emulate up to V = 3000 VMs per
server. We emulated 74 VMs per tenant on our 74 servers (one VM
per tenant per host), or 74N VMs in all for N tenants.
Multi-tenant parallel shufﬂe workload: Our workload generator
emulates the shufﬂe phase of Map-Reduce. Each of the N ten-
ants has 74 VMs, each emulating both a Map task and a Reduce
task. Each Map task transfers 10MB to each of the tenant’s 73
other Reduce tasks. A given Map tasks serializes these transfers, in
a random order that differs for each tenant; the transfers originat-
ing from a single Map task do not run in parallel. To avoid over-
loading a Reduce task, we reject incoming connections beyond a
limit of 3 simultaneous connections per Reduce task. A Map task
70(a) FatTree Topology
(b) Clique Topology
Figure 4: Goodput with varying number of tenants (number of VMs in parenthesis).
(a) FatTree Topology
(b) Clique Topology
Figure 5: Number of ﬂooded packets with varying number of tenants (number of VMs in parenthesis).
will postpone any such rejected transfer to a random position in its
pending-transfer list; ultimately, all such tasks will complete.
Between each trial of the entire workload, we delay for 320 sec-
onds, during which no packets are sent. Since our switches are set
to expire learning table entries after 300 seconds, this ensures the
FIBs are empty at the start of each trial.
Metrics: We run each shufﬂe trial either to completion or for 1800
seconds, and we compute the goodput as the total number of bytes
transferred by all tasks, divided by the run-time. We also mea-
sure the number of unnecessarily-ﬂooded packets during a trial, by
counting, at each host, the total number of packets received that are
not destined for any VM on that host.
Testbed: We ran the emulation experiments on 74 servers, part of
a larger shared testbed. Each server has a quad-core 3GHz Xeon