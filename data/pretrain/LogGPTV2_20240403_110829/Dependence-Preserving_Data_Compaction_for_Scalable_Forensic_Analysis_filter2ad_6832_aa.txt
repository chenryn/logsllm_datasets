title:Dependence-Preserving Data Compaction for Scalable Forensic Analysis
author:Md Nahid Hossain and
Junao Wang and
R. Sekar and
Scott D. Stoller
Dependence-Preserving Data Compaction  
for Scalable Forensic Analysis
Md Nahid Hossain, Junao Wang, R. Sekar, and Scott D. Stoller, Stony Brook University
https://www.usenix.org/conference/usenixsecurity18/presentation/hossain
This paper is included in the Proceedings of the 27th USENIX Security Symposium.August 15–17, 2018 • Baltimore, MD, USAISBN 978-1-931971-46-1Open access to the Proceedings of the 27th USENIX Security Symposium is sponsored by USENIX.Dependence-Preserving Data Compaction for Scalable Forensic Analysis∗
Md Nahid Hossain,
and Scott D. Stoller
{mdnhossain,junawang,sekar,stoller}@cs.stonybrook.edu
Junao Wang, R. Sekar,
Stony Brook University, Stony Brook, NY, USA.
Abstract
Large organizations are increasingly targeted in long-running
attack campaigns lasting months or years. When a break-in is
eventually discovered, forensic analysis begins. System audit
logs provide crucial information that underpins such analysis.
Unfortunately, audit data collected over months or years can
grow to enormous sizes. Large data size is not only a storage
concern: forensic analysis tasks can become very slow when
they must sift through billions of records. In this paper, we
first present two powerful event reduction techniques that
reduce the number of records by a factor of 4.6 to 19 in our
experiments. An important benefit of our techniques is that
they provably preserve the accuracy of forensic analysis tasks
such as backtracking and impact analysis. While providing
this guarantee, our techniques reduce on-disk file sizes by
an average of 35× across our data sets. On average, our
in-memory dependence graph uses just 5 bytes per event in
the original data. Our system is able to consume and analyze
nearly a million events per second.
1 Introduction
Many large organizations are targets of stealthy, long-term,
multi-step cyber-attacks called Advanced Persistent Threats
(APTs). The perpetrators of these attacks remain below the
radar for long periods, while exploring the organization’s IT
infrastructure and exfiltrating or compromising sensitive data.
When the attack is ultimately discovered, a forensic analysis
is initiated to identify the entry points of the attack and its
system-wide impact. The spate of APTs in recent years has
fueled research on efficient collection and forensic analysis
of system logs [13, 14, 15, 9, 16, 17, 18, 22, 42, 30, 10].
Accurate forensic analysis requires logging of system
activity across the enterprise. Logs should be detailed enough
to track dependencies between events occurring on different
hosts and at different times, and hence needs to capture all
information-flow causing operations such as network/file
accesses and program executions. There are three main
options for collecting such logs: (1) instrumenting individual
applications, (2) instrumenting the operating system (OS),
or (3) using network capture techniques. The rapid increase
in encrypted traffic has greatly reduced the effectiveness
In contrast,
of network-capture based forensic analysis.
∗This work was primarily supported by DARPA (contract FA8650-15-
C-7561) and in part by NSF (CNS-1319137, CNS-1421893, CCF-1414078)
and ONR (N00014-15-1-2208, N00014-15-1-2378, N00014-17-1-2891).
OS-layer logging is unaffected by encryption. Moreover,
OS-layer logging can track the activities of all processes on
a host, including any malware that may be installed by the
attackers. In contrast, application-layer logs are limited to
a handful of benign applications (e.g., network servers) that
contain the instrumentation for detailed logging. For these
reasons, we rely on OS-based logging, e.g., the Linux audit
and Windows ETW (Event Tracing for Windows) systems.
1.1 Log Reduction
APT campaigns can last for many months. With existing
systems, such as Linux auditing and Windows ETW, our
experience as well as that of previous researchers [42] is that
the volume of audit data is in the range of gigabytes per host
per day. Across an enterprise with thousands of hosts, total
storage requirements can easily go up to the petabyte range
in a year. This has motivated a number of research efforts
on reducing log size.
Since the vast majority of I/O operations are reads,
ProTracer’s [22] reduction strategy is to log only the writes.
In-memory tracking is used to capture the effect of read
operations. Specifically, when a process performs a read,
it acquires a taint identifier that captures the file, network
or IPC object read. If the process reads n files, then its
taint set can be of size O(n). Write operations are logged,
together with the taint set of the process at that point. This
means that write records can, in general, be of size O(n),
and hence a process performing m writes can produce a log
of size O(mn). This contrasts with the O(m + n) log size
that would result with traditional OS-level logging of both
reads and writes. Thus, for ProTracer’s strategy to reduce
log size, it is necessary to narrow the size of taint sets of
write operations to be close to 1. They achieve this using
a fine-grained taint-tracking technique called unit-based
execution partitioning [17], where a unit corresponds to a
loop iteration. MPI [21] proposes a new form of execution
partitioning, based on annotated data structures instead of
loops. However, fine-grained taint-tracking via execution par-
titioning would be difficult to deploy on the scale of a large
enterprise running hundreds of applications or more. Without
fine-grained taint-tracking, the analysis above, as well as our
experiments, indicate that this strategy of “alternating tainting
with logging” leads to substantial increases in log size.
LogGC [18] develops a “garbage collection” strategy,
which identifies and removes operations that have no persis-
tent effect. For instance, applications often create temporary
USENIX Association
27th USENIX Security Symposium    1723
a.com
b.com
1
2
3
P
4
5
7
6
10
9
C
L
E
Q
8
12
11
a.com
b.com
P1
C
P4
L
E
Q
Fig. 1: An example (time-stamped) dependence graph.
files that they subsequently delete. Unless these files are
accessed by other processes, they don’t introduce any new
dependencies and hence aren’t useful for forensic analysis.
However, some temporary files do introduce dependencies,
e.g., malware code that is downloaded, executed and subse-
quently removed by another attack script. Operations on such
files need to be logged, so LogGC introduces a notion of ex-
clusive ownership of files by processes, and omits only the op-
erations on exclusively owned files. Although they report ma-
jor reductions in log size using this technique, this reduction is
realized only in the presence of the unit instrumentation [17]
described above. If only OS-layer logging is available, which
is the common case, LogGC does not produce significant
reductions. (See the “Basic GC” column in Table 5 of [18].)
While LogGC removes all events on a limited class of
objects, Xu et al [42] explore a complementary strategy
that can remove some (repeated) events on any object.
To this end, they developed the concept of trackability
equivalence of events in the audit log, and proved that,
among a set of equivalent events, all but one can be
removed without affecting forensic analysis results. Across a
collection of several tens of Linux and Windows hosts, their
technique achieved about a 2× reduction in log size. This
is impressive, considering that it was achieved without any
application-specific optimizations.
While trackability equivalence [42] provides a sufficient
basis for eliminating events, we show that it is far too strict,
limiting reductions in many common scenarios, e.g., com-
munication via pipes. The central reason is that trackability
is based entirely on a local examination of edges incident on
a single node in the dependence graph, without taking into
account any global graph properties. In contrast, we develop
a more general formulation of dependence preservation that
can leverage global graph properties. It achieves 3 to 5 times
as much reduction as Xu et al.’s technique.
• In Section 3, we formulate dependence-preserving log
reduction in terms of reachability preservation in the
dependence graph. As in previous works (e.g., [13, 42]),
nodes in our dependence graph represent objects (files,
sockets and IPCs) and subjects (processes), while edges
represent operations (also called events) such as read,
write, load, and execute. Edges are timestamped and are
oriented in the direction of information flow. We say that a
Fig. 2: Dependence graph resulting after our FD log reduction. SD reduc-
tion will additionally remove the edge from Q to L. In this reduced graph,
dependence can be determined using standard graph reachability. Edge
timestamps are dropped, but nodes may be annotated with a timestamp.
node v depends on node u if there is a (directed) path from
u to v with non-decreasing edge timestamps. In Fig. 1, P
denotes a process that connects to a.com, and downloads
and saves a file C. It also connects to b.com, and writes to
a log file L and a pipe E. Process Q reads from the pipe
and also writes to the same log file. Based on timestamps,
we can say that C depends on a.com but not b.com.
• Based on this formulation, we present
two novel
dependency preserving reductions, called full dependence
preservation (FD) and source dependence preservation
(SD). We prove that FD preserves the results of backward
as well as forward forensic analysis. We also prove
that SD preserves the results of the most commonly
used forensic analysis, which consists of running first a
backward analysis to find the attacker’s entry points, and
then a forward analysis from these entry points to identify
the full impact of the attack.
• Our experimental evaluation used multiple data sets,
including logs collected from (a) our laboratory servers,
and (b) a red team evaluation carried out in DARPA’s
Transparent Computing program. On this data, FD
achieved an average of 7× reduction in the number
of events, while SD achieved a 9.2× reduction.
In
comparison, Xu et al.’s algorithm [42], which we
reimplemented, achieved only a 1.8× reduction. For
the example in Fig. 1, our technique combines all edges
between the same pair of nodes, leading to the graph
shown in Fig. 2, while Xu et al’s technique is able to
combine only the two edges with timestamps 1 and 2.
1.2 Efficient Computation of Reductions
Our log reductions (FD and SD) rely on global properties
of graphs such as reachability. Such global properties are
expensive to compute, taking time that is linear in the size
of the (very large) dependence graph. Moreover, due to the
use of timestamped edges, reachability changes over time,
and hence the results cannot be computed once and cached
for subsequent use.
To overcome these computational challenges posed by
timestamped graphs, we show in Section 4 how to transform
them into standard graphs. Fig. 2 illustrates the result of
1724    27th USENIX Security Symposium
USENIX Association
this conversion followed by our FD reduction. Note how
the edge timestamps have been eliminated. Moreover, P
has been split into two versions connected by a dashed edge,
with each version superscripted with its timestamp. Note the
absence of a path from a.com to C, correctly capturing the
reachability information in the timestamped graph in Fig. 1.
Versioning has been previously studied in file system
and provenance research [31, 26, 25]. In these contexts,
versioning systems typically intervene to create file versions
that provide increased recoverability or reproducibility.
Provenance capture systems may additionally intervene to
break cyclic dependencies [24, 25], since cyclic provenance
is generally considered meaningless.
In our forensic setting, we cannot intervene, but can only
observe events. Given a timestamped event log, we need to
make sound inferences about dependencies of subjects as
well as objects. We then encode these dependencies into a
standard graph in order to speed up our reduction algorithms.
The key challenge in this context is to minimize the size of
the standard graph without dropping any existing dependency,
or introducing a spurious one. Specifically, the research
described in Section 4 makes the following contributions:
• Efficient reduction algorithms. By working with standard
graphs, we achieve algorithms that typically take constant
time per event.
In our experiments, we were able to
process close to a million events per second on a single-
core on a typical laptop computer.
• Minimizing the number of versions. We present several op-
timization techniques in Section 4.2 to reduce the number
of versions. Whereas naive version generation leads to
an explosion in the number of versions, our optimizations
are very effective, bringing down the average number of
versions per object and subject to about 1.3. Fig. 2 illus-
trates a few common cases where we achieve substantial
reductions by combining many similar operations:
– multiple reads from the same network connection
(a.com, b.com) interleaved with multiple writes to
files (C and L),
– series of writes to and reads from pipes (E), and
– series of writes to log files by multiple processes (L).
• Avoiding spurious dependencies. While it is important
to reduce the space overhead of versions, this should
not come at the cost of inaccurate forensic analysis. We
therefore establish formally that results of forensic analysis
(specifically, forward and backward analyses) are fully
preserved by our reduction.
• Optimality. We show that edges and versions retained
by our reduction algorithm cannot be removed without
introducing spurious dependencies.
An interesting aspect of our work is that we use versioning
to reduce storage and runtime, whereas versioning is
normally viewed as a performance cost to be paid for better
recoverability or reproducibility.
1.3 Compact Graph and Log Representations
A commonly suggested approach for forensic analysis is
to store the dependence graph in a graph database. The
database’s query capabilities can then be used to perform
backward or forward searches, or any other custom forensic
analysis. Graph databases such as OrientDB, Neo4j and
Titan are designed to provide efficient support for graph
queries, but experience suggests that their performance de-
grades dramatically on graphs that are large relative to main
memory. For instance, a performance evaluation study on
graph databases [23] found that they are unable to complete
simple tasks, such as finding shortest paths on graphs with
128M edges, even when running on a computer with 256GB
main memory and sufficient disk storage. Log reduction tech-
niques can help, but may not be sufficient on their own: our
largest dataset, representing just one week of data, already
contains over 70M edges. Over the span of an APT (many
months or a year), graph sizes can approach a billion edges
even after log reduction. We therefore develop a compact in-
memory representation for our versioned dependence graphs.
• Section 5.2 describes our approach for realizing a compact
dependence graph representation. By combining our log
reduction techniques with compact representations, our