# Dependence-Preserving Data Compaction for Scalable Forensic Analysis

**Authors:**
- Md Nahid Hossain
- Junao Wang
- R. Sekar
- Scott D. Stoller

**Affiliation:**
Stony Brook University, Stony Brook, NY, USA

**Publication:**
Proceedings of the 27th USENIX Security Symposium, August 15–17, 2018, Baltimore, MD, USA
ISBN: 978-1-931971-46-1
Open access sponsored by USENIX.
[Link to Paper](https://www.usenix.org/conference/usenixsecurity18/presentation/hossain)

**Abstract:**
Large organizations are increasingly targeted in long-running attack campaigns that can last for months or years. When a security breach is eventually discovered, forensic analysis is initiated. System audit logs provide crucial information for such analysis. However, the volume of audit data collected over extended periods can grow to enormous sizes, posing significant storage and performance challenges. In this paper, we present two powerful event reduction techniques that reduce the number of records by factors of 4.6 to 19 in our experiments. These techniques provably preserve the accuracy of forensic analysis tasks such as backtracking and impact analysis. While ensuring this accuracy, our techniques reduce on-disk file sizes by an average of 35× across our datasets. On average, our in-memory dependence graph uses just 5 bytes per event in the original data. Our system can process nearly a million events per second.

## 1. Introduction

Many large organizations are targets of stealthy, long-term, multi-step cyber-attacks known as Advanced Persistent Threats (APTs). These attackers remain undetected for extended periods, exploring the organization's IT infrastructure and exfiltrating or compromising sensitive data. When the attack is ultimately discovered, forensic analysis is initiated to identify the entry points and system-wide impact. The increasing frequency of APTs has driven research into efficient collection and forensic analysis of system logs [13, 14, 15, 9, 16, 17, 18, 22, 42, 30, 10].

### 1.1 Log Reduction

APT campaigns can last for many months. With existing systems like Linux auditing and Windows ETW, the volume of audit data can be in the range of gigabytes per host per day. For an enterprise with thousands of hosts, total storage requirements can easily reach the petabyte range in a year. This has motivated several research efforts to reduce log size.

#### ProTracer's Approach
ProTracer [22] reduces log size by logging only write operations and using in-memory tracking for read operations. When a process reads, it acquires a taint identifier that captures the file, network, or IPC object read. Write operations are logged along with the taint set of the process at that point. This approach can lead to substantial increases in log size without fine-grained taint-tracking, which is difficult to deploy on a large scale.

#### LogGC's Approach
LogGC [18] identifies and removes operations that have no persistent effect, such as temporary files that are created and deleted without being accessed by other processes. However, some temporary files introduce dependencies, such as malware code that is downloaded, executed, and removed by another attack script. LogGC introduces exclusive ownership of files by processes and omits operations on exclusively owned files. Although this technique achieves major reductions in log size, it is effective only with unit instrumentation [17]. Without such instrumentation, LogGC does not produce significant reductions.

#### Trackability Equivalence
Xu et al. [42] developed the concept of trackability equivalence, proving that among a set of equivalent events, all but one can be removed without affecting forensic analysis results. Their technique achieved about a 2× reduction in log size across several tens of Linux and Windows hosts. However, trackability equivalence is too strict, limiting reductions in many common scenarios, such as communication via pipes. We develop a more general formulation of dependence preservation that leverages global graph properties, achieving 3 to 5 times as much reduction as Xu et al.'s technique.

### 1.2 Efficient Computation of Reductions

Our log reductions (FD and SD) rely on global properties of graphs such as reachability. Computing these properties in large, timestamped graphs is computationally expensive. To address this, we transform timestamped graphs into standard graphs, eliminating edge timestamps and splitting nodes into versions. This transformation allows us to achieve efficient reduction algorithms that typically take constant time per event. In our experiments, we processed close to a million events per second on a single-core laptop.

### 1.3 Compact Graph and Log Representations

A common approach for forensic analysis is to store the dependence graph in a graph database. However, graph databases struggle with large graphs, often failing to complete simple tasks on graphs with 128M edges even with 256GB of main memory. We develop a compact in-memory representation for our versioned dependence graphs, combining log reduction techniques with compact representations to handle large datasets efficiently.

## 2. Related Work

### 2.1 ProTracer
ProTracer [22] reduces log size by logging only write operations and using in-memory tracking for read operations. This approach can lead to substantial increases in log size without fine-grained taint-tracking, which is difficult to deploy on a large scale.

### 2.2 LogGC
LogGC [18] identifies and removes operations that have no persistent effect, such as temporary files that are created and deleted without being accessed by other processes. However, some temporary files introduce dependencies, such as malware code that is downloaded, executed, and removed by another attack script. LogGC introduces exclusive ownership of files by processes and omits operations on exclusively owned files. Although this technique achieves major reductions in log size, it is effective only with unit instrumentation [17]. Without such instrumentation, LogGC does not produce significant reductions.

### 2.3 Trackability Equivalence
Xu et al. [42] developed the concept of trackability equivalence, proving that among a set of equivalent events, all but one can be removed without affecting forensic analysis results. Their technique achieved about a 2× reduction in log size across several tens of Linux and Windows hosts. However, trackability equivalence is too strict, limiting reductions in many common scenarios, such as communication via pipes. We develop a more general formulation of dependence preservation that leverages global graph properties, achieving 3 to 5 times as much reduction as Xu et al.'s technique.

## 3. Dependence-Preserving Log Reduction

### 3.1 Formulation
We formulate dependence-preserving log reduction in terms of reachability preservation in the dependence graph. Nodes in our dependence graph represent objects (files, sockets, and IPCs) and subjects (processes), while edges represent operations (events) such as read, write, load, and execute. Edges are timestamped and oriented in the direction of information flow. We say that a node \( v \) depends on node \( u \) if there is a directed path from \( u \) to \( v \) with non-decreasing edge timestamps.

### 3.2 Full Dependence Preservation (FD)
Full Dependence Preservation (FD) preserves the results of backward and forward forensic analysis. We prove that FD maintains the accuracy of forensic analysis tasks.

### 3.3 Source Dependence Preservation (SD)
Source Dependence Preservation (SD) preserves the results of the most commonly used forensic analysis, which consists of running a backward analysis to find the attacker's entry points and then a forward analysis from these entry points to identify the full impact of the attack.

### 3.4 Experimental Evaluation
Our experimental evaluation used multiple datasets, including logs from laboratory servers and a red team evaluation in DARPA’s Transparent Computing program. On this data, FD achieved an average of 7× reduction in the number of events, while SD achieved a 9.2× reduction. In comparison, Xu et al.’s algorithm [42], which we re-implemented, achieved only a 1.8× reduction.

## 4. Efficient Computation of Reductions

### 4.1 Transformation to Standard Graphs
To overcome computational challenges posed by timestamped graphs, we transform them into standard graphs. This transformation eliminates edge timestamps and splits nodes into versions, connected by dashed edges. This approach allows us to achieve efficient reduction algorithms that typically take constant time per event.

### 4.2 Minimizing the Number of Versions
We present several optimization techniques to reduce the number of versions. Naive version generation can lead to an explosion in the number of versions, but our optimizations bring down the average number of versions per object and subject to about 1.3. Common cases where we achieve substantial reductions include:
- Multiple reads from the same network connection interleaved with multiple writes to files.
- Series of writes to and reads from pipes.
- Series of writes to log files by multiple processes.

### 4.3 Avoiding Spurious Dependencies
While reducing the space overhead of versions, it is crucial to maintain accurate forensic analysis. We formally establish that our reduction fully preserves the results of forward and backward analyses. Additionally, we show that the edges and versions retained by our reduction algorithm cannot be removed without introducing spurious dependencies.

## 5. Compact Graph and Log Representations

### 5.1 Graph Database Limitations
Graph databases, such as OrientDB, Neo4j, and Titan, are designed to support efficient graph queries. However, their performance degrades dramatically on large graphs. For example, they struggle to complete simple tasks on graphs with 128M edges, even with 256GB of main memory.

### 5.2 Compact Dependence Graph Representation
We develop a compact in-memory representation for our versioned dependence graphs. By combining our log reduction techniques with compact representations, we can handle large datasets efficiently. Our largest dataset, representing just one week of data, already contains over 70M edges. Over the span of an APT (many months or a year), graph sizes can approach a billion edges even after log reduction.

## 6. Conclusion

In this paper, we presented two powerful event reduction techniques, FD and SD, that significantly reduce the number of records while preserving the accuracy of forensic analysis tasks. Our techniques reduce on-disk file sizes by an average of 35× and use just 5 bytes per event in the original data. Our system can process nearly a million events per second, making it suitable for large-scale forensic analysis.

## Acknowledgments

This work was primarily supported by DARPA (contract FA8650-15-C-7561) and in part by NSF (CNS-1319137, CNS-1421893, CCF-1414078) and ONR (N00014-15-1-2208, N00014-15-1-2378, N00014-17-1-2891).

---

This optimized version of the text is more structured, coherent, and professional, making it easier to read and understand.