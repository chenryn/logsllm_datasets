0.9
1.0
Threshold
Figure 5: F-measures of code-similarity algorithm
No.9 and its variants (No.10-{1,2,3,4})
Figure 6 shows the precision, recall, and F-measure of
code-similarity algorithms with respect to the testing data.
For the candidate code-similarity algorithms, we computed
their precision and recall according to the testing data in the
VPD and the code reuse instances in the VCID. The data
unsuitable for a code-similarity algorithm were also included
when computing the precision and recall because the algo-
rithms dealt with all vulnerabilities. Figure 6(a) shows that
the F-measure of the algorithms adopted from the literature
is 0.24 on average and the best algorithm is code-similarity
algorithm No.17 with a F-measure of 0.58. For the vari-
ants of code-similarity algorithms that we propose in the
present paper, we consider three examples: code-similarity
algorithms No.5, No.9, and No.17. As shown in Figure 6(b),
algorithm No.9 and its variants (with threshold 0.6) lead
to similar F-measures, and the variants of algorithm No.5
and the variants of algorithm No.17 exhibit a similar pat-
tern in terms of their F-measures. This means that some
token mappings can improve the F-measure, because the re-
call measure plays a more important role than the precise
measure. Among all of the variants of algorithms No.5, No.9,
and No.17, algorithm No.18-2 leads to the highest F-measure
0.66.
For VulPecker, the precision depends on the accuracy of
the SVM classiﬁer, and the recall is the average recall of
the select algorithm for each diﬀ hunk. The recall of the
select code-similarity algorithm was calculated based on the
code reuse instances in the VCID, except the instances that
were ruled out by the code-similarity algorithm as described
in Algorithm 1. Figure 6 shows that VulPecker has a F-
measure that is 18% higher than the best existing code-
similarity algorithm (algorithm No.17) and 10% higher than
the best variant of the adopted code-similarity algorithms
(algorithm No.18-2). VulPecker tends to select the code-
similarity algorithm with a high F-measure, while no single
code-similarity algorithm is suitable for all vulnerabilities.
Explanation of the code-similarity algorithm selec-
tion results. We now explain the code-similarity algorithm
selection results of Algorithm 1. For step 1 of Algorithm 1,
VulPecker used learned SVM classiﬁers to select a set of
code-similarity algorithms that could distinguish unpatched
pieces of code from the patched ones. The result showed
that no single basic feature made a signiﬁcant contribution
to the classiﬁers.
For diﬀ hunks with a single type of patch feature, the sin-
gle type probably determined the code-similarity algorithm
that could distinguish between patched and unpatched pieces
of code. For diﬀ hunks with patch feature Type 5-1 involving
only line addition, the set of code-similarity algorithms that
could distinguish unpatched piece of code from the patched
one consistently excluded the algorithms that used the code-
fragment level of patch-without-context or slice, such as al-
gorithms No.15-No.18. This was reasonable because these
algorithms were based on the deleted lines according to the
diﬀ hunks. For diﬀ hunks with patch feature Type 6-1 in-
volving whole function addition, the algorithms depending
on the AST or PDG (algorithm No.7-No.18) were all ex-
cluded because there were no unpatched functions. For diﬀ
hunks with patch feature Types 3-5, 3-6, and 3-7 involving
only function argument addition, deletion, or modiﬁcation,
the algorithms depending on the PDG (algorithm No.13-
No.18) were all excluded because they neglected the func-
tion arguments and generated false-positives. For diﬀ hunks
with patch feature Type 6-3 involving only modiﬁcation be-
yond the function, the algorithms depending on the AST or
PDG (algorithm No.7-No.18) were excluded because there
were no functions for the AST or PDG.
After running step 2 and step 3 in Algorithm 1, we ob-
tained the select code-similarity algorithms of VulPecker.
 Precision
 Recall
 F-measure
o.1
N
o.2
N
o.3
N
o.4
N
o.5
N
o.7
N
o.9
N
N
o.1 1
N
o.1 3
N
o.1 5
N
o.1 7
ulP e c k er
V
(a) Existing code-similarity algorithms
 Precision
 Recall
 F-measure
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
N o.5
N o.6-2
N o.6-3
N o.17
N o.6-4
N o.18-1
N o.9(0.6)
N o.10-2(0.6)
N o.10-1(0.6)
N o.10-3(0.6)
N o.10-4(0.6)
N o.18-2
N o.18-3
N o.18-4
V ulP ecker
(b) Variant of existing code-similarity algorithms
Figure 6: Comparing existing code-similarity al-
gorithms and their variants against VulPecker via
three metrics: precision, recall and F-measure
Among them, code-similarity algorithm No.5 and its vari-
ants, algorithm No.9, algorithm No.17 and its variants were
selected by most vulnerabilities. Vulnerabilities with diﬀs
that involved function addition or modiﬁcations going be-
yond the function were more likely to choose algorithm No.5
or its variants. Vulnerabilities with diﬀs that only involved
line addition in the function were more likely to choose
algorithm No.9. Vulnerabilities with diﬀs that contained
more than line addition were more likely to choose algo-
rithm No.17 or its variants. Whether to select their vari-
ants, and if so selecting which variant, largely depended on
the F-measure.
5.2 Detecting vulnerabilities in products
Now we report the results of using the learned vulnera-
bility signatures and CVE-to-algorithm mapping to detect
vulnerabilities in products. We selected 246 vulnerabilities
that were published between 2013 and 2015 for three prod-
ucts, namely Firefox, Ffmpeg, and Qemu. The task is to
determine whether a target product contains one or more
of these vulnerabilities. The 40 vulnerabilities that are de-
tected by VulPecker and are not published in the NVD are
210
listed in Appendix Table A.1. Among these vulnerabilities,
18 are not known for their existence and have yet to be
conﬁrmed by the respective vendors at the time of writing.
For the 18 unpatched vulnerabilities, we anonymize their
CVE-ID, vulnerability publish time, and vulnerability loca-
tion for ethical reasons. The other 22 vulnerabilities have
been “silently” patched by product vendors after 7.3 months
on average since the vulnerabilities were published. We man-
ually checked and conﬁrmed these 22 vulnerabilities.
We used a virtual machine with Intel Core 2.5GHz pro-
cessor and 3GB of RAM running CentOS 6.0-64bit for the
experiments. For the select code-similarity algorithms listed
in Table A.1, we take the code-similarity algorithms using
the subgraph-isomorphic comparison method as examples to
show the time overhead incurred by the vulnerability detec-
tion procedure, because these algorithms are usually consid-
ered more time-consuming than the others. In our evalua-
tion, we adopted several optimizations to reduce their time
overhead, such as the exclusion of a large number of irrele-
vant edges and nodes, and breaking a big graph into small
ones [18]. Take the target project Libav 10.1 (29.6MB) as
an example, our goal is to detect whether it contains vul-
nerabilities CVE-2014-8547 (via algorithm No.18-1), CVE-
2013-7011 (via algorithm No.18-1), CVE-2013-3674 (via al-
gorithm No.17), and CVE-2013-0851 (via algorithm No.18-
2). The detection time corresponding to these four vul-
nerabilities was respectively 508.11s, 128.14s, 81.77s, and
141.44s. It is clear that the detection time depends on the
size of target project, the selection of the code-similarity
algorithm, and the complexity of graphs for slice code frag-
ment.
In what follows, we elaborate two vulnerabilities that have
been silently patched.
CVE-2015-0834. Mozilla Firefox prior to version 36.0
contains an information leak/disclosure vulnerability in the
WebRTC subsystem, which “makes it easier for man-in-the-
middle attackers to discover credentials by spooﬁng a server
and completing a brute-force attack within a short time win-
dow ” [1]. This vulnerability is originally reported for Mozilla
Firefox. However, our study shows that this vulnerability
also exists in Thunderbird 24.8.0 and the other versions prior
to Thunderbird 38.0.1. Figure 3 shows the diﬀ hunk of CVE-
2015-0834. VulPecker selects code-similarity algorithm No.9
for this diﬀ hunk, owing to the fact that the diﬀ hunk only
involves line addition in the function.
CVE-2014-2894. Qemu prior to version 2.0 has a nu-
meric errors vulnerability, which “allows local users to have
unspeciﬁed impact via a SMART EXECUTE OFFLINE com-
mand that triggers a buﬀer underﬂow and memory corrup-
tion” [1]. However, this vulnerability is reported only for
Qemu. Our study shows that the very vulnerability also ex-
ists in Xen 4.4.0 and the other versions prior to Xen 4.4.3.
VulPecker selects code-similarity algorithm No.18-2 for de-
tecting this vulnerability (only one diﬀ hunk), owing to the
fact that the diﬀ hunk only involves constant modiﬁcation
in the function.
6. LIMITATIONS
The present study has several limitations. First, our ex-
periments focus on C/C++ open source products. While the
methodology underlying VulPecker is language agnostic, ex-
periments need to be conducted to analyze target programs
written in other languages, such as Java and Python.
Second, the VPD and VCID databases need to be im-
proved. For creating the VPD, we use heuristics to auto-
matically ﬁnd the diﬀs for given CVE-IDs. Our manual
examination on a random sample of 10% of the vulnerabili-
ties shows that the heuristics lead to accurate results, which
however does not necessarily mean the heuristics are always
accurate. It is important to test a bigger sample to assure
their accuracy. For creating the VCID, the approach we use
to obtain code reuse instances may be unnecessarily restric-
tive. More experiments need to be conducted to accept or
reject this hypothesis.
Third, the ultimate goal of our research is the following:
Given a vulnerability, how can one determine whether or
not the vulnerability exists in any program of the entire
software stack of a computer (assuming source code is avail-
able)? This raises the scalability issue which needs to be
investigated.
Fourth, the methodology appears to be speciﬁc to the de-
tection of vulnerabilities at the source code level. It is an
important research problem to detect, automatically and ef-
fectively, whether a piece of binary code has a given vulner-
ability or not.
7. CONCLUSION
We have presented VulPecker, a system for automatically
detecting whether a program contains a given vulnerability
or not. VulPecker leverages features that we deﬁne to char-
acterize vulnerabilities and patches. Experimental results
show that VulPecker detects 40 vulnerabilities that are not
published in the NVD. Among these vulnerabilities, 18 are
not known for their existence and have yet to be conﬁrmed
by vendors at the time of writing, while the other 22 vul-
nerabilities have been “silently” patched by vendors when
releasing a later version.
For future research, it is interesting to address the limita-
tions mentioned above. It is also interesting to test whether
VulPecker can detect the vulnerabilities that are intention-
ally inserted by systems like LAVA [10].
Acknowledgments
We thank the anonymous reviewers for their comments that
helped us improve the paper. We thank Marcus Pendleton
for proofreading the paper. This paper is supported by Na-
tional Basic Research Program of China (973 Program) un-
der grant No.2014CB340600. Shouhuai Xu is supported in
part by NSF Grant #1111925 and ARO Grant #W911NF-
13-1-0141. Any opinions, ﬁndings, and conclusions or rec-
ommendations expressed in this material are those of the au-
thor(s) and do not necessarily reﬂect the views of the funding
agencies.
References
[1] CVE. http://cve.mitre.org/.
[2] National
Vulnerability
https://nvd.nist.gov/.
Database.
[3] Open
Sourced
Vulnerability
Database.
http://www.osvdb.org.
[4] A. Aydin, M. Alkhalaf, and T. Bultan. Automated
test generation from vulnerability signatures. In Proc.
ICST, pages 193–202. IEEE, 2014.
211
[22] A. Nappa, R. Johnson, L. Bilge, J. Caballero, and
T. Dumitras. The attack of the clones: A study of
the impact of shared code on vulnerability patching. In
Proc. SP, pages 692–708. IEEE, 2015.
[23] S. Neuhaus, T. Zimmermann, C. Holler, and A. Zeller.
In Proc.
Predicting vulnerable software components.
CCS, pages 529–540. ACM, 2007.
[24] H. Perl, S. Dechand, M. Smith, D. Arp, F. Yamaguchi,
K. Rieck, S. Fahl, and Y. Acar. VCCFinder: Finding
potential vulnerabilities in open-source projects to as-
sist code audits. In Proc. CCS, pages 426–437. ACM,
2015.
[25] N. H. Pham, T. T. Nguyen, H. A. Nguyen, and T. N.
Nguyen. Detection of recurring software vulnerabilities.
In Proc. ASE, pages 447–456. ACM, 2010.
[26] D. Rattan, R. Bhatia, and M. Singh. Software clone
detection: A systematic review. Information and Soft-
ware Technology, 55(7):1165–1199, 2013.
[27] I. H. Witten and E. Frank. Data Mining: Practical ma-
chine learning tools and techniques. Morgan Kaufmann,
2005.
[28] F. Yamaguchi, N. Golde, D. Arp, and K. Rieck. Model-
ing and discovering vulnerabilities with code property
graphs. In Proc. SP, pages 590–604. IEEE, 2014.
[29] F. Yamaguchi, F. Lindner, and K. Rieck. Vulnerability
extrapolation: Assisted discovery of vulnerabilities us-
ing machine learning. In Proc. WOOT, pages 118–127.
USENIX Association, 2011.
[30] F. Yamaguchi, M. Lottmann, and K. Rieck. General-
ized vulnerability extrapolation using abstract syntax
trees. In Proc. ACSAC, pages 359–368. ACM, 2012.
APPENDIX
Table A.1 summarizes the 40 vulnerabilities that are de-
tected by VulPecker and are not published in the NVD,
where the vulnerability publish time is the time the vul-
nerability was ﬁrst published and found in another product,
and the code-similarity algorithm is a sample of select code-
similarity algorithms when a vulnerability diﬀ has multiple