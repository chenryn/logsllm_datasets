### 0.9
### 1.0
### Threshold

**Figure 5: F-measures of Code-Similarity Algorithm No.9 and Its Variants (No.10-{1,2,3,4})**

**Figure 6** illustrates the precision, recall, and F-measure of code-similarity algorithms with respect to the testing data. For the candidate code-similarity algorithms, we computed their precision and recall based on the testing data in the VPD (Vulnerability Patch Database) and the code reuse instances in the VCID (Vulnerability Code Instance Database). The data unsuitable for a code-similarity algorithm were also included in the computation because the algorithms dealt with all vulnerabilities.

**Figure 6(a)** shows that the average F-measure of the algorithms adopted from the literature is 0.24, with the best algorithm being code-similarity algorithm No.17, which has an F-measure of 0.58. For the variants of code-similarity algorithms proposed in this paper, we considered three examples: code-similarity algorithms No.5, No.9, and No.17. As shown in **Figure 6(b)**, algorithm No.9 and its variants (with a threshold of 0.6) lead to similar F-measures. The variants of algorithm No.5 and No.17 exhibit a similar pattern in terms of their F-measures. This indicates that certain token mappings can improve the F-measure, as recall plays a more significant role than precision. Among all the variants of algorithms No.5, No.9, and No.17, algorithm No.18-2 achieves the highest F-measure of 0.66.

For VulPecker, the precision depends on the accuracy of the SVM classifier, and the recall is the average recall of the selected algorithm for each diff hunk. The recall of the selected code-similarity algorithm was calculated based on the code reuse instances in the VCID, excluding the instances ruled out by the code-similarity algorithm as described in Algorithm 1. **Figure 6** shows that VulPecker has an F-measure 18% higher than the best existing code-similarity algorithm (algorithm No.17) and 10% higher than the best variant of the adopted code-similarity algorithms (algorithm No.18-2). VulPecker tends to select the code-similarity algorithm with the highest F-measure, while no single code-similarity algorithm is suitable for all vulnerabilities.

### Explanation of the Code-Similarity Algorithm Selection Results

We now explain the code-similarity algorithm selection results of Algorithm 1. In step 1 of Algorithm 1, VulPecker used learned SVM classifiers to select a set of code-similarity algorithms that could distinguish unpatched pieces of code from patched ones. The results showed that no single basic feature made a significant contribution to the classifiers.

For diff hunks with a single type of patch feature, the single type likely determined the code-similarity algorithm that could distinguish between patched and unpatched pieces of code. For diff hunks with patch feature Type 5-1 involving only line addition, the set of code-similarity algorithms that could distinguish unpatched pieces of code from patched ones consistently excluded algorithms that used the code-fragment level of patch-without-context or slice, such as algorithms No.15-No.18. This is reasonable because these algorithms are based on the deleted lines according to the diff hunks. For diff hunks with patch feature Type 6-1 involving whole function addition, algorithms depending on the AST or PDG (algorithms No.7-No.18) were all excluded because there were no unpatched functions. For diff hunks with patch feature Types 3-5, 3-6, and 3-7 involving only function argument addition, deletion, or modification, algorithms depending on the PDG (algorithms No.13-No.18) were all excluded because they neglected function arguments and generated false-positives. For diff hunks with patch feature Type 6-3 involving only modification beyond the function, algorithms depending on the AST or PDG (algorithms No.7-No.18) were excluded because there were no functions for the AST or PDG.

After running steps 2 and 3 in Algorithm 1, we obtained the selected code-similarity algorithms for VulPecker. 

**Figure 6: Comparing Existing Code-Similarity Algorithms and Their Variants Against VulPecker via Three Metrics: Precision, Recall, and F-Measure**

Among them, code-similarity algorithm No.5 and its variants, algorithm No.9, and algorithm No.17 and its variants were selected by most vulnerabilities. Vulnerabilities with diffs that involved function addition or modifications going beyond the function were more likely to choose algorithm No.5 or its variants. Vulnerabilities with diffs that only involved line addition in the function were more likely to choose algorithm No.9. Vulnerabilities with diffs that contained more than line addition were more likely to choose algorithm No.17 or its variants. Whether to select their variants, and if so, which variant, largely depended on the F-measure.

### 5.2 Detecting Vulnerabilities in Products

We now report the results of using the learned vulnerability signatures and CVE-to-algorithm mapping to detect vulnerabilities in products. We selected 246 vulnerabilities published between 2013 and 2015 for three products: Firefox, Ffmpeg, and Qemu. The task is to determine whether a target product contains one or more of these vulnerabilities. The 40 vulnerabilities detected by VulPecker and not published in the NVD are listed in Appendix Table A.1. Among these, 18 are unknown and have yet to be confirmed by the respective vendors at the time of writing. For the 18 unpatched vulnerabilities, we anonymize their CVE-ID, vulnerability publish time, and vulnerability location for ethical reasons. The other 22 vulnerabilities have been "silently" patched by product vendors after an average of 7.3 months since the vulnerabilities were published. We manually checked and confirmed these 22 vulnerabilities.

We used a virtual machine with an Intel Core 2.5GHz processor and 3GB of RAM running CentOS 6.0-64bit for the experiments. For the selected code-similarity algorithms listed in Table A.1, we take the code-similarity algorithms using the subgraph-isomorphic comparison method as examples to show the time overhead incurred by the vulnerability detection procedure, as these algorithms are generally considered more time-consuming than others. In our evaluation, we adopted several optimizations to reduce their time overhead, such as the exclusion of a large number of irrelevant edges and nodes, and breaking a big graph into smaller ones [18]. Taking the target project Libav 10.1 (29.6MB) as an example, our goal is to detect whether it contains vulnerabilities CVE-2014-8547 (via algorithm No.18-1), CVE-2013-7011 (via algorithm No.18-1), CVE-2013-3674 (via algorithm No.17), and CVE-2013-0851 (via algorithm No.18-2). The detection times for these four vulnerabilities were 508.11s, 128.14s, 81.77s, and 141.44s, respectively. It is clear that the detection time depends on the size of the target project, the selection of the code-similarity algorithm, and the complexity of graphs for slice code fragments.

In what follows, we elaborate on two vulnerabilities that have been silently patched.

**CVE-2015-0834:** Mozilla Firefox prior to version 36.0 contains an information leak/disclosure vulnerability in the WebRTC subsystem, which "makes it easier for man-in-the-middle attackers to discover credentials by spoofing a server and completing a brute-force attack within a short time window" [1]. This vulnerability was originally reported for Mozilla Firefox. However, our study shows that this vulnerability also exists in Thunderbird 24.8.0 and other versions prior to Thunderbird 38.0.1. **Figure 3** shows the diff hunk of CVE-2015-0834. VulPecker selects code-similarity algorithm No.9 for this diff hunk, as it only involves line addition in the function.

**CVE-2014-2894:** Qemu prior to version 2.0 has a numeric errors vulnerability, which "allows local users to have unspecified impact via a SMART EXECUTE OFFLINE command that triggers a buffer underflow and memory corruption" [1]. This vulnerability was reported only for Qemu. Our study shows that the same vulnerability also exists in Xen 4.4.0 and other versions prior to Xen 4.4.3. VulPecker selects code-similarity algorithm No.18-2 for detecting this vulnerability (only one diff hunk), as it only involves constant modification in the function.

### 6. Limitations

This study has several limitations. First, our experiments focus on C/C++ open-source products. While the methodology underlying VulPecker is language-agnostic, further experiments need to be conducted to analyze target programs written in other languages, such as Java and Python.

Second, the VPD and VCID databases need to be improved. For creating the VPD, we use heuristics to automatically find the diffs for given CVE-IDs. Our manual examination of a random sample of 10% of the vulnerabilities shows that the heuristics lead to accurate results, but this does not necessarily mean the heuristics are always accurate. It is important to test a larger sample to ensure their accuracy. For creating the VCID, the approach we use to obtain code reuse instances may be unnecessarily restrictive. More experiments need to be conducted to accept or reject this hypothesis.

Third, the ultimate goal of our research is to determine whether a given vulnerability exists in any program of the entire software stack of a computer (assuming source code is available). This raises scalability issues that need to be investigated.

Fourth, the methodology appears to be specific to the detection of vulnerabilities at the source code level. It is an important research problem to detect, automatically and effectively, whether a piece of binary code has a given vulnerability.

### 7. Conclusion

We have presented VulPecker, a system for automatically detecting whether a program contains a given vulnerability. VulPecker leverages features we define to characterize vulnerabilities and patches. Experimental results show that VulPecker detects 40 vulnerabilities not published in the NVD. Among these, 18 are unknown and have yet to be confirmed by vendors at the time of writing, while the other 22 vulnerabilities have been "silently" patched by vendors when releasing a later version.

For future research, it is interesting to address the limitations mentioned above. It is also interesting to test whether VulPecker can detect vulnerabilities intentionally inserted by systems like LAVA [10].

### Acknowledgments

We thank the anonymous reviewers for their comments, which helped us improve the paper. We also thank Marcus Pendleton for proofreading the paper. This work is supported by the National Basic Research Program of China (973 Program) under grant No.2014CB340600. Shouhuai Xu is partially supported by NSF Grant #1111925 and ARO Grant #W911NF-13-1-0141. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding agencies.

### References

[1] CVE. http://cve.mitre.org/.
[2] National Vulnerability Database. https://nvd.nist.gov/.
[3] Open Sourced Vulnerability Database. http://www.osvdb.org.
[4] A. Aydin, M. Alkhalaf, and T. Bultan. Automated test generation from vulnerability signatures. In Proc. ICST, pages 193–202. IEEE, 2014.
[22] A. Nappa, R. Johnson, L. Bilge, J. Caballero, and T. Dumitras. The attack of the clones: A study of the impact of shared code on vulnerability patching. In Proc. SP, pages 692–708. IEEE, 2015.
[23] S. Neuhaus, T. Zimmermann, C. Holler, and A. Zeller. Predicting vulnerable software components. In Proc. CCS, pages 529–540. ACM, 2007.
[24] H. Perl, S. Dechand, M. Smith, D. Arp, F. Yamaguchi, K. Rieck, S. Fahl, and Y. Acar. VCCFinder: Finding potential vulnerabilities in open-source projects to assist code audits. In Proc. CCS, pages 426–437. ACM, 2015.
[25] N. H. Pham, T. T. Nguyen, H. A. Nguyen, and T. N. Nguyen. Detection of recurring software vulnerabilities. In Proc. ASE, pages 447–456. ACM, 2010.
[26] D. Rattan, R. Bhatia, and M. Singh. Software clone detection: A systematic review. Information and Software Technology, 55(7):1165–1199, 2013.
[27] I. H. Witten and E. Frank. Data Mining: Practical machine learning tools and techniques. Morgan Kaufmann, 2005.
[28] F. Yamaguchi, N. Golde, D. Arp, and K. Rieck. Modeling and discovering vulnerabilities with code property graphs. In Proc. SP, pages 590–604. IEEE, 2014.
[29] F. Yamaguchi, F. Lindner, and K. Rieck. Vulnerability extrapolation: Assisted discovery of vulnerabilities using machine learning. In Proc. WOOT, pages 118–127. USENIX Association, 2011.
[30] F. Yamaguchi, M. Lottmann, and K. Rieck. Generalized vulnerability extrapolation using abstract syntax trees. In Proc. ACSAC, pages 359–368. ACM, 2012.

### Appendix

Table A.1 summarizes the 40 vulnerabilities detected by VulPecker and not published in the NVD. The vulnerability publish time is the time the vulnerability was first published and found in another product, and the code-similarity algorithm is a sample of the selected code-similarity algorithms when a vulnerability diff has multiple options.