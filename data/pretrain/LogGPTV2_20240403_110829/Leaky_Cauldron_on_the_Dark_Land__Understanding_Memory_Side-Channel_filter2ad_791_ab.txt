Security check for memory isolation. To prevent system soft-
ware from arbitrarily controlling the address translation by manipu-
lating the page table entries, the CPU also consults the Enclave Page
Cache Map (EPCM) during the address translation. Each EPC page
corresponds to an entry in the EPCM, which records the owner
enclave of the EPC page, the type of the page, and a valid bit indi-
cating whether the page has been allocated. When an EPC page is
allocated, its access permissions are specified in its EPCM entry as
readable, writable, and/or executable. The virtual address (within
ELRANGE) mapped to the EPC page is also recorded in the EPCM
entry.
Correctness of page table entries set up by the untrusted system
software is guaranteed by an extended Page Miss Handler (PMH).
When the code is executing in the enclave mode or the address
translation result falls into the PRM range, additional security check
will take place. Specially, when the code is running in the non-
enclave mode and address translation falls into the PRM range, or
the code runs in the enclave mode but the physical address is not
pointing to a regular EPC page belonging to the current enclave, or
the virtual address triggering the page table walk doesn’t match the
virtual address recorded in the corresponding entry in the EPCM,
a page fault will occur. Otherwise, the generated TLB entries will
be set according to both the attributes in the EPCM entry and the
page table entry.
Memory encryption. To support larger ELRANGE than EPC, EPC
pages can be “swapped” out to regular physical memory. This pro-
cedure is called EPC page eviction. The confidentiality and integrity
of the evicted pages are guaranteed through authenticated encryp-
tion. The hardware Memory Encryption Engine (MEE) is integrated
with the memory controller and seamlessly encrypts the content
of the EPC page that is evicted to a regular physical memory page.
A Message Authentication Code (MAC) protects the integrity of
the encryption and a nonce associated with the evicted page. The
encrypted page can be stored in the main memory or swapped out
to secondary storage similar to regular pages. But the metadata
associated with the encryption needs to be kept by the system
software properly for the page to be “swapped” into the EPC again.
2.2 Adversary Model
In this paper, we consider attacks against enclave-protected code
and data. The system software here refers to the program that
operates with system privileges, such as operating systems and
hypervisors. Our focus in this paper is side-channel analysis that
threatens the confidentiality of the enclave programs. As such,
software bugs in the code of an enclave program are out of our
scope. Moreover, side channels not involving memory management
and address translation are not covered either.
We assume in our demonstrated attacks knowledge of the victim
binary code to be loaded into the enclave. As the adversary also
knows the base address of the enclave binary in the virtual address
space, as well as the entire virtual-to-physical mapping, the map-
ping of the binary code in pages, caches, DRAMs can be derived.
Source code of the victim program is NOT required. We conducted
analysis and experiments on real SGX platforms. So we do assume
the adversary has access to a machine of the same configuration
before performing the attacks.
Session K4:  Secure EnclavesCCS’17, October 30-November 3, 2017, Dallas, TX, USA24233 UNDERSTANDING ATTACK SURFACES
In this section, we explore side-channel attack surfaces in SGX
memory management, through an in-depth study of attack vectors
(shared resources that allow interference of the execution inside
enclaves), followed by an analysis of individual vectors, in terms of
the way they can be exploited and effectiveness of the attacks.
3.1 Attack Vectors
A memory reference in the modern Intel CPU architectures involves
a sequence of micro-operations: the virtual address generated by the
program is translated into the physical address, by first consulting
a set of address translation caches (e.g., TLBs and various paging-
structure caches) and then walking through the page tables in the
memory. The resulting physical address is then used to access the
cache (e.g., L1, L2, L3) and DRAM to complete the memory reference.
Here, we discuss memory side-channel attack vectors in each of
these steps.
Address Translation Caches. Address translation caches are hard-
ware caches that facilitate address translation, including TLBs and
various paging-structure caches. TLB is a multi-level set-associative
hardware cache that temporarily stores the translation from virtual
page numbers to physical page numbers. Specially, the virtual ad-
dress is first divided into three components: TLB tag bits, TLB-index
bits, and page-offset bits. The TLB-index bits are used to index a
set-associative TLB and the TLB-tag bits are used to match the
tags in each of the TLB entries of the searched TLB set. Similar
to L1 caches, the L1 TLB for data and instructions are split into
dTLB and iTLB. An L2 TLB, typically larger and unified, will be
searched upon L1 TLB misses. Recent Intel processors allow selec-
tively flushing TLB entries at context switch. This is enabled by the
Process-Context Identifier (PCID) field in the TLB entries to avoid
flushing the entries that will be used again. If both levels of TLBs
miss, a page table walk will be initiated. The virtual page number
is divided into, according to Intel’s terminology, PML4 bits, PDPTE
bits, PDE bits, and PTE bits, each of which is responsible for index-
ing one level of page tables in the main memory. Due to the long
latency of page-table walks, if the processor is also equipped with
paging structure caches, such as PML4 cache, PDPTE cache, PDE
cache, these hardware caches will also be searched to accelerate
the page-table walk. The PTEs can be first searched in the cache
hierarchy before the memory access [11].
Vector 1. Shared TLBs and paging-structure caches under Hy-
perThreading.
When HyperThreading (HT)1 is enabled, code running in the
enclave mode may share the same set of TLBs and paging-structure
caches with code running in non-enclave mode. Therefore, the
enclave code’s use of such resources will interfere with that of
the non-enclave code, creating side channels. This attack vector is
utilized to clear the TLB entries in the HT-SPM attack (Section 4.1).
Vector 2. Flushing selected entries in TLB and paging-structure
caches at AEX.
According to recent versions of Intel Software Developer’s Man-
ual [3], entering and leaving the enclave mode will flush entries
in TLB and paging-structure caches that are associated with the
1Intel’s term for Simultaneous Multi-Threading.
4
Figure 1: Page table entries.
current PCID. As such, it enables an adversary from a different
process context to infer the flushed entries at context switch. This
is possible even on processors without HT. However, we were not
able to confirm this attack vector on the machines we had (i.e.,
Skylake i7-6700). We conjecture that this is because our Skylake i7-
6700 follows the specification in an older version of Intel Software
Developer’s Manual [4], which states all entries will be flushed re-
gardless of the process context. Nevertheless, we believe this attack
vector could be present in future processors.
Vector 3. Referenced PTEs are cached as data.
Beside paging-structure caches, referenced PTEs will also be
cached as regular data [11]. This artifact enables a new attack vec-
tor: by exploiting the Flush+Reload side channel on the monitored
PTEs, the adversary can perform a cross-core attack to trace the
page-level memory access pattern of the enclave code. This at-
tack vector presents a timing-channel version of the sneaky page
monitoring attack we describe in Section 4. We will discuss its
implication in Section 6.
Page tables. Page tables are multi-level data structures stored in
main memory, serving address translation. Every page-table walk
involves multiple memory accesses. Different from regular memory
accesses, page-table lookups are triggered by the micro-code of the
processor direction, without involving the re-ordering buffer [11].
The entry of each level stores the pointer to (i.e., physical address
of) the memory page that contains the next level of the page table.
The structure of a PTE is shown in Figure 1. Specially, bit 0 is present
flag, indicating whether a physical page has been mapped to the
virtual page; bit 5 is accessed flag, which is set by the processor
every time the page-table walk leads to the reference of this page
table entry; bit 6 is dirty flag, which is set when the corresponding
page has been updated. Page frame reclaiming algorithms rely on
the dirty flag to make frame reclamation decisions.
As the page tables are located inside the OS kernel and controlled
by the untrusted system software, they can be manipulated to at-
tack enclaves. However, as mentioned earlier, because the EPC page
permission is also protected by EPCM, malicious system software
cannot arbitrarily manipulate the EPC pages to compromise its in-
tegrity and confidentiality. However, it has been shown in previous
work [43] that by clearing the present flag in the corresponding
PTEs, the malicious system software can collect traces of page
accesses from the enclave programs, inferring secret-dependent
control flows or data flows. Nevertheless, setting present flag is not
the only attack vector against enclave programs.
Vector 4. Updates of the accessed flags in enclave mode.
When the page-table walk results in a reference of the PTE, the
accessed flag of the entry will be set to 1. As such, code run in
non-enclave mode will be able to detect the page table updates and
learn that the corresponding EPC page has just been visited by
the enclave code. However, page-table walk will also update TLB
entries, so that future references to the same page will not update
the accessed flags in PTEs, until the TLB entries are evicted by other
Session K4:  Secure EnclavesCCS’17, October 30-November 3, 2017, Dallas, TX, USA2424address translation activities. We exploit this attack vector in our
SPM attacks in Section 4.
Vector 5. Updates of the dirty flags in enclave mode.
Similar to accessed flags, the dirty flag will be updated when
the corresponding EPC page is modified by the enclave program.
This artifact can be exploited to detect memory writes to a new
page. The new side-channel attack vector will enable the adversary
to monitor secret-dependent memory writes, potentially a finer-
grained inference attack than memory access tracking.
Vector 6. Page faults triggered in enclave mode.
In addition to the present flag, a few other bits in the PTEs can be
exploited to trigger page faults. For example, on a x86-64 processor,
bit M to bit 51 are reserved bits which when set will trigger page
fault upon address translation. Here bit M−1 is the highest bit of the
physical address on the machine. The NX flag, when set, will force
page faults when instructions are fetched from the corresponding
EPC page.
Cache and memory hierarchy. Once the virtual address is trans-
lated into the physical address, the memory reference will be served
from the cache and memory hierarchy. Both are temporary stor-
age that only hold data when the power is on. On the top of the
hierarchy is the separate L1 data and instruction caches, the next
level is the unified L2 cache dedicated to one CPU core, then L3
cache shared by all cores of the CPU package, then the main mem-
ory. Caches are typically built on Static Random-Access Memory
(SRAM) and the main memory on Dynamic Random-Access Mem-
ory (DRAM). The upper level storage tends to be smaller, faster
and more expensive, while the lower level storage is usually larger,
slower and a lot cheaper. Memory fetch goes through each level
from top to bottom; misses in the upper level will lead to accesses to
the next level. Data or code fetched from lower levels usually update
entries in the upper level in order to speed up future references.
The main memory is generally organized in multiple memory
channels, handled by memory controllers. One memory channel is
physically partitioned into multiple DIMMs (Dual In-line Memory
Module), each with one or two ranks. Each rank has several DRAM
chips (e.g., 8 or 16), and is also partitioned into multiple banks. A
bank carries the memory arrays organized in rows and each of the
rows typically has a size of 8KB, shared by multiple 4KB memory
pages since one page tends to span over multiple rows. Also on
the bank is a row buffer that keeps the most recently accessed row.
Every memory read will load the entire row into the row buffer
before the memory request is served. As such, accesses to the DRAM
row already in the row buffer are much faster.
Vector 7. CPU caches are shared between code in enclave and
non-enclave mode.
SGX does not protect enclave against cache side-channel attacks.
Therefore, all levels of caches are shared between code in enclave
mode and non-enclave mode, similar to cross-process and cross-
VM cache sharing that are well-known side-channel attack vectors.
Therefore, all known cache side-channel attacks, including those
on L1 data cache, L1 instruction cache, and L3 cache, all apply
to the enclave settings. We empirically confirmed such threats
(Section 5.1).
5
Vector 8. The entire memory hierarchy, including memory con-
trollers, channels, DIMMs, DRAM ranks and banks (including row
buffers), are shared between code in enclave and non-enclave mode.
Similar to cache sharing, DRAM modules are shared by all pro-
cesses running in the computer systems. Therefore, it is unavoid-
able to have enclave code and non-enclave code accessing memory
stored in the same DRAM bank. The DRAM row buffer can be
served as a side-channel attack vector: when the target program
makes a memory reference, the corresponding DRAM row will
be loaded into the row buffer of the bank; the adversary can com-
pare the row-access time to detect whether a specific row has just
been visited, so as to infer the target’s memory access. This artifact
has been exploited in DRAMA attacks [36]. In Section 5, we show
that after key technical challenges are addressed, such attacks can
also succeed on enclave programs. Other shared memory hierarchy
can also create contention between enclave and non-enclave code,
causing interference that may lead to covert channels [27].
3.2 Characterizing Memory Vectors
Here we characterize the aforementioned memory side-channels
in three dimensions:
Spatial granularity. This concept describes the smallest unit of
information directly observable to the adversary during a memory
side-channel attack. Specifically, it measures the size of the address
space one side-channel observation could not reveal. For example,
the spatial granularity of the page-fault attack is 4KB, indicating
that every fault enables the adversary to see one memory page
(4096 bytes) being touched, though the exact address visited is not
directly disclosed.
Temporal observability. Given a spatial granularity level, even
though the adversary cannot directly see what happens inside the
smallest information unit, still there can be timing signals generated
during the execution of the target program to help distinguish
different accesses made by the program within the unit. For example,
the duration for a program to stay on a page indicates, indirectly,
whether a single memory or multiple accesses occur. A side-channel
is said to have this property if the timing is measured and used to
refine the observations in the attack.
Side effects. We use this concept to characterize observable anom-
alies caused by a memory side-channel attack, which could be
employed to detect the attack. An example is AEX, which is fre-
quently invoked by the page-fault attack. Another side effect is the
slowdown of the execution. Since the primary approach to con-
ducting a side-channel attack is to cause contention in memory
resource, such as the flush of caches, TLBs, paging structure caches,
DRAM row buffers, etc., overheads will be introduced to the run-
time performance of the enclave code. AEXs also contribute to the
performance overhead. For example, the original page-fault attacks
are reported to make the target program run one or two orders of
magnitude slower. This level of slowdown is easy to get noticed.
Frequent AEXs are also detectable using approaches proposed by
Chen et al. [15], as the execution time between two basic blocks
can be much longer.
Session K4:  Secure EnclavesCCS’17, October 30-November 3, 2017, Dallas, TX, USA2425Figure 2: Basic SPM attack.
4 REDUCING SIDE EFFECTS WITH SNEAKY
PAGE MONITORING ATTACKS
To attack the virtual memory, a page-fault side-channel attacker first
restricts access to all pages, which induces page faults whenever
the enclave process touches any of these pages, thereby generating
a sequence of its page visits. A problem here is that this approach
is heavyweight, causing an interrupt for each page access. This
often leads to a performance slowdown by one or two orders of