estimate the probability density function of each erroneous
gesture class using Gaussian kernels. We then calculated
the relative entropy between the respective distributions of
different erroneous gesture classes (EGi) using the Jensen-
Shannon Divergence (JS-divergence) metric [35] which pro-
vides us with a measure of difference between each pair of
distributions as calculated in Equation 1:
JSD(EGi||EGj) =
D(EGi||M ) +
1
2
D(EGj||M )
1
2
1
2
where, M =
(EGi + EGj) and
(1)
EGi,j = Erroneous Gestures
Figure 5 shows the pairwise JS-divergence between dis-
tributions of different erroneous gestures. We see that there
is in particular a high divergence between the distributions
of gesture classes G2, G3, G4 and G6, all of which are
commonly occurring gestures and have a large number of
samples in the task of Suturing (see Table VII). For the other
gesture classes we were not able to compute meaningful
distributions due to small sample sizes. This observation
partly supports our hypothesis that errors in surgery are
context-speciﬁc and the knowledge of gestures might help
with improved error detection. We use this observation in
designing our Erroneous Gesture Detection component of the
pipeline by developing a library of classiﬁers, each trained
for detecting errors in a speciﬁc gesture class.
Gesture Segmentation and Classiﬁcation: We model the
task of identifying and segmenting surgical gestures as a
multi-class classiﬁcation problem (Equation 2). The input
signal xt represents the time-series of kinematics variables
with a sliding time-window of w and a stride of s. The output
Gt represents the gesture corresponding to that time-series
and is a one-hot vector of all gestures from 0 to 14.
xt = (xt, xt+1, .., xt+w)
Gt = (0, 0, 1, .., 0)T
(2)
As the input signal is a multivariate time-series, we use
Recurrent Neural Networks (RNN) which are known to learn
spatial and temporal patterns. For our gesture classiﬁcation,
we use LSTM [36] networks which are known to learn long
and short-term dependencies, and have the ability to decide
what part of the previous output to keep or discard. A typical
LSTM unit is composed of a memory cell and three gates:
input, output and the forget gate. The gates regulate the ﬂow
of information inside the unit and allow LSTM architectures
to remember information for long periods of time while also
ﬁltering information that is less relevant.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:23:52 UTC from IEEE Xplore.  Restrictions apply. 
388
To aid our gesture classiﬁcation and ensure smooth tran-
sition boundaries, we use stacked LSTM layers to provide
greater abstraction of the input sequence and to allow the
hidden states at each level to operate at a different timescale
[37]. This is followed by a fully-connected layer with ReLU
[38] activation and a ﬁnal softmax layer for obtaining gesture
probabilities. The loss function is the categorical cross-
entropy, with the model trained using the Adam [39] opti-
mizer. To address over-ﬁtting, we use dropout regularization
and early stopping on a held-out validation set. To improve
the learning process, we use batch normalization layers and
adaptive learning rate with step-decay.
Erroneous Gesture Detection: Having identiﬁed the cur-
rent gesture, Gt, the next stage of the pipeline classiﬁes the
gesture as erroneous or non-erroneous using the kinematics
samples as input. We train this part of the pipeline separately
from the gesture classiﬁcation component and only combine
the two parts in the evaluation phase, with the erroneous
gesture detection following the gesture segmentation and
classiﬁcation (see Section V-B).
xt = (xt, xt+1, .., xt+w)
yt = p(EGt|Gt, xt)
(3)
We frame the problem as detection of a context-speciﬁc
conditional event, i.e., a part of the trajectory can be erro-
neous or non-erroneous, depending on the current gesture, as
shown in Equation 3. The input is the predicted gesture and
a kinematics sample corresponding to that gesture, and the
output is a binary classiﬁcation of the kinematics sample to
safe or unsafe. If any sample within a gesture is erroneous,
we label that whole gesture as unsafe. Although our model is
trained on sliding time-window samples instead of the whole
gesture, it learns to have smooth output over time, allowing
it to distinguish between entire boundaries of erroneous or
non-erroneous gestures.
As a baseline, we trained a single classiﬁer, with no ex-
plicit notion of context, for detecting the erroneous gestures.
In this case, the problem reduces to a non-conditional binary
classiﬁcation of the time-series data, with the input being
the kinematics sample and the output being whether it is
erroneous or not. Similar to gesture classiﬁcation, our models
for detecting erroneous gestures are trained using the Adam
optimizer with step-decay and early stopping. We used low
initial learning rates ranging from 0.0001 to 0.001 to help
the stability of the optimization, given a small dataset.
IV. EXPERIMENTS
We evaluated our monitoring system using trajectory data
collected from the common surgical tasks of Block Transfer
and Suturing performed in dry-lab settings on two different
surgical platforms, the open-source Raven II surgical robot
and the daVinci Research Kit (dVRK). The Raven II allowed
us to simulate the impact of technical faults and attacks using
software fault injection, while the surgical data collected
from dVRK enabled studying the effect of human errors and
evaluating the safety monitor using realistic surgical tasks.
All experiments were conducted on an x86 64 PC with an
Intel Core i7 CPU @ 3.60GHz and 32GB RAM, running
Linux Ubuntu 18.04 LTS, and an Nvidia 2080 Ti GPU,
running CUDA 10.1. We used Keras [40] API v.2.2.4 on
top of TensorFlow [41] v.1.14.0 for training our models and
Scikit-learn [42] v.0.21.3 for pre-processing and evaluation.
A. daVinci Research Kit (dVRK)
JIGSAWS Dataset: For evaluating the performance of
our solution on the dVRK, we considered the surgical task
of Suturing. Since we did not have full access to the system,
we used pre-collected trajectory data from the JIGSAWS
dataset [23] and manually annotated the errors. The dataset
contains synchronized kinematics and video data recorded
at 30 Hz from three surgical tasks (Suturing, Knot-tying
and Needle-passing) that were performed by surgeons with
varying skill levels in dry-lab settings on the dVRK platform.
The kinematics data comprises of 19 variables for each
robot manipulator, including: Cartesian Position (3), Rotation
Matrix (9), Grasper Angle (1), Linear (3) and Angular
Velocity (3). We used the data from 39 demonstrations of the
task of Suturing with the Leave-One-SuperTrial-Out (LOSO)
setup of the JIGSAWS dataset for training and evaluating our
monitoring pipeline. The LOSO setup meant that we trained
on 4 super trials and held one super trial out for evaluation.
Erroneous Gesture Annotation: The gestures were al-
ready labeled as part of the JIGSAWS dataset [23], but
we further classiﬁed them as safe or unsafe. We did so by
manually annotating video data based on the rubric in Table
II and used it as ground truth for evaluating our classiﬁers
which rely only on kinematics data. We labeled any given
gesture as unsafe if any of the common errors speciﬁc to that
gesture were observed in its corresponding video segment.
Out of a total of 793 gestures, 144 were labeled as erroneous.
B. Raven II
ROS Gazebo Simulator: For the Raven II, our experi-
ments were conducted using a simulator that we developed
based on ROS Gazebo 3D virtual environment, integrated
with the RAVEN II control software and a fault injection
tool that mimics the effect of technical faults and attacks in
the robot control system. This simulator is available to the
research community for experimental evaluation of safety
monitoring solutions in robotic surgery2.
We leveraged the physics engine of the Gazebo simulator
for faithful representation of the dry-lab settings. Figures 6a
and 6b show the dry-lab setup of the Block Transfer task in
the Raven II workspace along with the corresponding simula-
tion in the Gazebo 3D environment. All our experiments used
the same setup with the left and right robot manipulators,
grasper instruments, and the standard objects in the Block
Transfer task, including a block and a receptacle where the
block should be dropped.
The input to the simulator can be surgeon’s commands
during tele-operation or output from motion planning al-
gorithms in autonomous mode. The kinematics data from
2https://github.com/UVA-DSA/raven2 sim/tree/gazebo sim
389
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:23:52 UTC from IEEE Xplore.  Restrictions apply. 
(cid:2)(cid:2)
(cid:2)(cid:1)
(cid:2)(cid:1)
(cid:1)
(a) Block Transfer in dry lab setting
(b) Block Transfer in ROS Gazebo
(c) Cartesian Position Faults
(d) Grasper Angle Faults
Fig. 6: Experimental setup for dry lab and virtual simulation of surgical tasks
fault injections in the task of Block Transfer, out of which,
498 resulted in errors, including 392 block-drop and 106 for
dropoff failures. The state-space for injected values, S(cid:2), was
0.3 rad ≤S(cid:2)≤ 1.6 rad for Grasper Angle and 3000 mm ≤S(cid:2)≤
65000 mm for Cartesian Position. We explored different
combinations of perturbations of the targeted variables over
different durations of the trajectory.
The experiments showed that perturbing the Grasper Angle
had a greater effect on causing errors compared to perturbing
the Cartesian Position. For lower Grasper Angle values (0.3
rad ≤S’≤ 0.8 rad), perturbation over different durations of
the trajectory resulted in different failure modes. For fault
injections with duration 0.65 ≤D≤ 0.9 and targeted Grasper
Angle 0.3 rad ≤S(cid:2)≤ 0.8 rad, the likelihood of a dropoff
failure was high (>90%) whereas for the same range (0.3 rad
≤S(cid:2)≤ 0.8 rad) but different duration (0.55 ≤D≤ 0.7), the
likelihood of any failure was signiﬁcantly low. There were
only 2 cases where the block was dropped at the wrong
position due to high Cartesian deviation. When injecting
higher values to the Grasper Angle (0.9 rad ≤S(cid:2)≤ 1.6 rad),
we observed block-drops regardless of the duration of the
perturbation, with higher values of S(cid:2)
leading to higher
percentage of failures. This suggests that for block-drop error
to happen, the value of the Grasper Angle either needs to be
higher than 0.8 rad, or the fault needs to be injected for a
longer duration of time. For dropoff failure to happen, the
Grasper Angle has to be below 1.0 rad, or the fault needs
to be injected for a longer duration, possibly beyond G11,
which is the gesture where the block should be dropped.
Fault Types (Values, Durations, and Total Number)
Duration
Duration
Cartesian Position
Deviation (mm)
(% Trajectory)
(% Trajectory)
# Fault
Injections
the simulator consisted of 277 features (including the 19
variables available from the JIGSAWS dataset), sampled at
1000 frames per second. The simulator also allows logging of
the video data using a virtual camera. The video frames are
logged at 30 frames per second, along with their timestamps
to enable synchronization with kinematics data and to mea-
sure the times when faults and errors happen. We collected
20 fault-free demonstrations of the Block Transfer task
performed by 2 different human subjects in the simulator,
on which we carried out our fault injections. The dataset
collected from the simulation experiments consisted of 115
fault-free and faulty demonstrations.
Fault Injections: We assume that accidental or malicious
attacks and human errors can manifest as errors in the kine-
matic state variables in the inputs, outputs and the internal
state of the robot control software and cause the common
error types shown in Table II. As a result, our software
fault injection tool directly perturbs the values of kinematic
state variables to simulate such errors. Each injected fault
is characterized by the name of the state variable (V ) with
value (S) that is targeted, along with the injected value (S(cid:2))
and the duration of the injection (D).
Figure 1c shows how the operational context for the Block
Transfer task changes with respect to the kinematic state
variables (V ), which are the Grasper Angle and the Cartesian
Position of the robot instrument end-effectors. The fault du-
ration, D, is measured in milliseconds and could span across
more than one gesture. We perturbed the values of Grasper
Angle and the Cartesian Positions (x, y, z) in the collected
fault-free trajectories and sent the faulty trajectory packets
to the robot control software. This allowed us to repeat the
same trajectory or to perturb only speciﬁc segments while
the rest of the trajectory remained the same.
To simulate the effect of attacks or human errors, we
created deviations from the actual trajectory by slight incre-
ments or decrements in the values of Grasper Angle and the
Cartesian Position variables. For Grasper Angle, we added a
constant value of θ for the duration (D) until the target value
(S(cid:2)) was reached (see Figure 6d). For Cartesian Position,
we provided a target deviation (δ = d(S(cid:2), S)), which is the
Euclidean distance between S and S(cid:2) and a function of x, y
and z values. We then enforced a uniform positive deviation
√
in the three dimensions of x, y, and z by injecting the value
3 to the three variables over the duration
of δx,y,z = δ/ 2
(D) (see Figure 6c).
Table III shows the results of our fault injection experi-
ments on the Gazebo simulator. In total, we conducted 651
Grasper
Angle (rad)
0.30-0.40
0.50-0.60
0.70-0.80
0.90-1.00
1.10-1.20
1.30-1.40
1.50-1.60
0.55-0.70
0.65-0.90
0.55-0.70
0.65-0.90
0.55-0.70
0.65-0.90
0.55-0.70
0.65-0.90
0.55-0.70
0.65-0.90
0.55-0.70
0.65-0.90
0.55-0.70
0.65-0.90
3000-6000
6000-65000
3000-6000
6000-65000
3000-6000
6000-65000
3000-6000
6000-65000
3000-6000
6000-65000
3000-6000
6000-65000
3000-6000
6000-65000