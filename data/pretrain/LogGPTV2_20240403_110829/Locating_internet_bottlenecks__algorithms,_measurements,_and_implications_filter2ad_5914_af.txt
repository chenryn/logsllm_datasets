0.8
Location of choke links (normalized by AS path length)
(a) Distribution across classes
(b) Cumulative distribution of normalized location
(c) Cumulative distribution per class
Figure 11: Location of bottleneck and choke links.
Figure 11(c) shows the cumulative distribution for the normal-
ized location for choke links and bottleneck links separately for the
different classes of links; the results have been weighted by the
number of probing sets in which a link is detected as a choke link
or bottleneck link. We observe that intra-AS choke links and bot-
tleneck links typically appear earlier in the path than inter0-AS and
inter1-AS choke links and bottlenecks. The reason could be that
some sources encounter choke links and bottlenecks in their home
network.
4.4 Stability
Due to the burstiness of Internet trafﬁc and occasional routing
changes, the bottleneck on an end-to-end path may change over
time.
In this section, we study the stability of the bottlenecks.
For our measurements, we randomly selected 10 probing sources
from PlanetLab (“ST” data set in Table 4). We sampled 30 desti-
nations randomly chosen from the set of destinations obtained in
Section 4.1. We took measurements for a three hour period and
we divided this period into 9 epochs of 20 minutes each. In each
epoch, we ran Pathneck once for each source-destination pair. Path-
neck used probing sets consisting of 5 probing trains and reported
choke links for each 5-train probing set.
Suppose link b is a choke link in probing set i.
Let
DetectionRatei(b) denote the frequency with which b is a can-
didate choke link in probing set i. For each path, we deﬁne the
stability of choke link b over a period of n epochs as
n(cid:4)
Stability(b) =
DetectionRatei(b)
i=1
The same deﬁnition applies to bottleneck links. Note that the range
of Stability(b) is [0.5, n] because d rate ≥ 0.5.
Figure 12(a) shows the cumulative distribution for stability over
all measurements. We can see that bottlenecks and choke links have
very similar stability, but this stability is however not very high. We
speculate that the reason is that many bottlenecks are determined by
the trafﬁc load, not link capacity. Figure 12(b) shows the stability
(at the router level) for intra-AS, inter0-AS and inter1-AS choke
links. We see that intra-AS choke links are signiﬁcantly less stable
than inter-AS choke links. Comparing the two types of inter-AS
choke links, inter0-AS choke links are more stable than inter1-AS
choke links. We observe similar results at the AS level as shown
by the curves labeled “intra-AS-level” and “inter1-AS-level”: the
intra-AS choke links are again less stable than the inter1-AS choke
links. Moreover, we see, not surprisingly, that AS-level choke links
are more stable than router-level choke links. Similar observations
apply to bottlenecks (not shown in Figure 12(b)). Given the small
number of destinations (i.e., 30) and short duration of the exper-
iment (i.e., 3 hours), we cannot claim that these stability results
F
D
C
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
1
2
3
Bottleneck
Choke link
6
7
8
9
4
5
Stability of a choke link
(a) Bottlenecks vs. choke links
F
D
C
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
1
2
3
Intra-AS choke link
Intra-AS-level chock point
Inter0-AS choke link
Inter1-AS choke link
Inter1-AS-level choke link
4
5
6
7
8
9
Stability of a choke link
(b) Intra-AS vs. inter-AS choke links
Figure 12: The stability of choke links.
are representative for the Internet. We plan to do more extensive
experiments in the future.
5.
INFERRING BOTTLENECKS
In this section, we look at the problem of inferring the bottleneck
location for a path that was not directly probed by Pathneck. The
observation is that, if all or most of the links on a path are also part
of paths that have already been probed, we may be able to derive the
bottleneck location without actually probing the path. This could
signiﬁcantly reduce the amount of probing that must be done when
studying bottleneck locations.
Methodology: We divide the “GE” data set we gathered in Sec-
tion 4 into two parts — a training set and a testing set. The training
set is used to label each link L with an upper bound Bu(L) and/or
a lower bound Bl(L) for the available bandwidth; these bounds
are calculated using the algorithm presented in Section 2.3.3. If
there are multiple bounds on the available bandwidth of a link from
multiple probing sets, we take the lowest upper bound as the up-
per bound of the link’s available bandwidth and the highest lower
bound as the lower bound of the link’s available bandwidth. Since
Class
0
1
2
3
4
–
Total
Links
covered
100%
[80%, 100%)
[70%, 80%)
[60%, 70%)
[0%, 60%)
–
–
Table 5: Inference results
Correct
Incorrect
No upper
10.2%
11.4%
2.7%
1.4%
0.9%
–
8.5%
9.3%
2.5%
1.3%
0.8%
–
26.6%
22.4%
bound
9.9%
9.8%
2.4%
1.3%
0.6%
–
24%
Not
covered
0%
7.2%
3.6%
2.6%
2.2%
–
15.6%
Total
28.6%
37.7%
11.2%
6.6%
4.5%
11.4%
100%
this type of calculation on bounds is very sensitive to measurement
noise, we ﬁrst preprocess the data: we include upper bounds only
if the standard deviation across the probing set is less than 20% of
the average upper bound.
The testing set is used for inference validation as follows. For
each path P in the testing set, we try to annotate each link Li ∈ P .
If the link is covered in the training set, we associate the upper
bound Bu(Li) and/or lower bound Bl(Li) derived from the train-
ing set with it. We identify the link Li with the lowest upper bound
Bu(Li) as the inferred bottleneck link ˆLi; we ignore links that
have no bounds or only lower bounds. We then compare ˆLi with
the “true” bottleneck location, as identiﬁed by the Pathneck result
in the testing set. If the location matches, we claim that the bot-
tleneck location inference for the path P is successful. Paths in
the testing set for which Pathneck cannot identify any choke link
with high enough d rate and conf are excluded from the analy-
sis. Note that routers may have multiple interfaces with different
IP addresses, we use the tool Ally [35] to resolve router aliases.
When evaluating how successful we are in inferring bottleneck
location, we need to account for the fact that for most paths, we
miss information for at least some of the links. Obviously we would
expect to have a lower success rate for such paths. For this reason,
we classify the paths in the testing set into 5 classes based on the
percentage of links that are covered by the training set. Class 0
includes paths in the testing set for which we have some informa-
tion (either upper bound, lower bound, or both) for every link in
the path. Class 1 includes paths for which we have information for
over 80% of the links, but not for every link. Similarly, Classes 2,
3, and 4 include paths for which the percentage of covered links is
[70%, 80%), [60%, 70%), and [0%, 60%), respectively.
Results: The probing data that we used in this section includes the
results for 51,193 paths. We randomly select 60% of the probing
sets as the training data, while the remaining 40% are used as test-
ing data for inference evaluation. That gives us 20,699 paths in the
testing set. Column “Total” in Table 5 lists the percentage of paths
in each class; the “11.4%” entry corresponds to paths in the testing
set on which we cannot identify a bottleneck.
Column “Correct” corresponds to the cases where inference
was successful, while column “Incorrect” corresponds to the cases
where we picked the wrong link as the bottleneck, even though
the training set provided sufﬁcient information about the real bot-
tleneck link. Column “No upper bound” corresponds to the paths
where we picked the wrong bottleneck link, but the training set
only has lower bounds for the true bottleneck link; this is typically
because the link was covered by very few probings in the training
set. The column “Not covered” corresponds to paths for which the
bottleneck link is not covered in the training set, so we can obvi-
ously not identify the link as the bottleneck link. For both the “No
upper bound” and “Not covered” cases, inference fails because the
training set does not offer sufﬁcient information. A more carefully
designed training set should reduce the percentage of paths in these
categories.
Overall, inference is successful for 30% of the paths which we
can identify bottleneck in the testing set, while the success rate
increases to 54% when we have sufﬁcient data in the training set.
Note the diminishing trend in the inference success rate as we have
information for fewer links in the path: the “Correct” cases account
for 36%, 30%, 24%, 21% and 20% of the paths in Classes 0 through
4, respectively. This drop is expected since the less information we
have on a path, the less likely it is that we can infer the bottleneck
location correctly.
Discussion: The inference capability presented in this section
shows that it is possible to infer the network bottleneck location
without probing the path with some level of accuracy. However,
we need sufﬁcient information on the links in the path so it is im-
portant to properly design the training set to reduce the number of
links for which we have little or no data. Ideally, we would be able
to systematically probe a speciﬁc region of the Internet and put the
results in a database. This information could then be used by ap-
plications to infer the bottlenecks for any path in that region of the
network.
6. AVOIDING BOTTLENECKS
In this section we study how bottleneck information obtained by
Pathneck can be used to improve overlay routing and multihoming.
6.1 Overlay Routing
Overlay routing, or application layer routing, refers to the idea of
going through one or more intermediate nodes before reaching the
destination. The intermediate nodes act as application layer routers
or overlay nodes by forwarding trafﬁc typically without any ad-
ditional processing. Previous studies [33, 31] have shown that by
going through an intermediate node, the round trip delay can be sig-
niﬁcantly improved and routing failures can be bypassed. In such
cases, the part of the network experiencing congestion or routing
problems is avoided. Note that between any two overlay nodes or
between an overlay node and either the source or destination, regu-
lar IP routing is used to route trafﬁc. One of the reasons why such
“triangular” routing works is that BGP — the Inter-domain Rout-
ing Protocol, does not optimize for network performance in terms
of delay, loss rate, or bandwidth. Shortest AS-path-based routing
does not always yield the best performing paths because routing
policies can cause path inﬂation [37, 34].
Overlay routing can thus be used to avoid bottleneck links in
the underlying IP path, thereby improving application level per-
formance in terms of throughput. So far, no studies have quan-
tiﬁed the beneﬁt overlay routing provides in avoiding bottleneck
links. To the best of our knowledge, this study presents the very
ﬁrst large scale analysis of how overlay routing can improve the
available bandwidth of a path. Other metrics such as delay, loss
rate, and cost [15] are also important, and we plan to study the
correlation between these metrics and the available bandwidth we
consider here in a future study. Most of the nodes from which we
performed probing are well connected, i.e., they receive upstream
Internet service from a tier-1 ISP. We would like to understand the
utility of overlay routing when the probe nodes serve as overlay
routers for paths destined to arbitrary locations in the Internet. We
used the following probing methodology to gather the data for this
study.
Methodology: We selected 27 RON and Planetlab nodes as both
the source nodes and overlay nodes, as listed in the “OV” column in
Table 4. Using a BGP table from a large tier-1 ISP, we sampled 200
random IP addresses from a diverse set of preﬁxes; each IP address
originates from a different AS and ends with “.1” to minimize the
chance of triggering alarms at ﬁrewalls. From each probing source
we performed the probing process described below during the same
F
D
C
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
bottleneck link bandwidth improvement
Upper bound
Lower bound
50
100
150
bandwidth in Mbps
Figure 13: Improvements in the lower and upper bounds for
available bandwidth using overlay routing (cutoff at 150Mbps).
time period to minimize the effect of transient congestion or any
other causes for nonstationary bottleneck links. Given the list of
200 target IP addresses, each source node S probes each IP address
10 times using Pathneck. After probing each target IP address, S
randomly selects 8 nodes from the set of 22 source nodes as its
candidate overlay nodes and probes each of these 8 nodes 10 times.
This probing methodology is designed to study the effectiveness
of overlay routing in avoiding bottleneck links in a fair manner, as
the probing of the following three paths occur very close in time:
S → D, S → S
is overlay node and D is
destination node. The upper bound for the bottleneck link available
bandwidth is calculated based on the largest gap value in the path
across the 10 probing results.
(cid:2) → D, where S
, and S
(cid:2)
(cid:2)
(cid:2)
with S
For each destination from a given source, we calculate the lower
bound and upper bound for the available bandwidth of the path as
the lowest and highest upper bound on bottleneck link available
bandwidth calculated by Pathneck in the 10 probings for that path.
(cid:2) → D, the
When composing two paths such as S → S
lower bound of this overlay path is assumed to be the minimum of
the two lower bound values from the individual paths. The upper
bound of an overlay path is calculated in the same manner to have
a conservative estimate of the available bandwidth. We only con-
sider the effectiveness of overlay routing by going through a single
intermediate node, similar to previous studies.
Results: Of the 63, 440 overlay attempts, i.e., routing to a desti-
nation by going through an intermediate node, 52.72% are useful,
which means that overlay routing improves either the lower or the
upper bound on the available bandwidth of the path. Note that we
are not considering other metrics such as latency or packet loss.
If we require both bounds to increase, the useful rate is 15.92%;
the breakdown for improving only the lower bound or only the up-
per bound is 17.39% and 19.40%, respectively. The distribution
of the improvement in upper and lower bounds for the available
bandwidth is shown in Figure 13. The curves show that most im-
provements in the upper bound are less than 100Mbps while the
limit for the lower bound is 20Mbps.
We now examine more closely how useful the overlay nodes
are in improving the available bandwidth bounds from each source
node to the 200 randomly selected destinations. We found that for
most sources almost all 22 overlay nodes can be used for reaching
some destinations with improved performance. A few exceptions
stand out: the mazu1 site ﬁnds only 8 out of 22 nodes useful in
terms of improving available bandwidth, and the cornell site ﬁnds
18 nodes useful. One possible reason is that the paths between