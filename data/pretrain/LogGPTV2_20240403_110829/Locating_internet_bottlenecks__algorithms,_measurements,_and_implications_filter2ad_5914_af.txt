### 0.8: Location of Choke Links (Normalized by AS Path Length)

- **(a) Distribution across classes**
- **(b) Cumulative distribution of normalized location**
- **(c) Cumulative distribution per class**

**Figure 11: Location of Bottleneck and Choke Links**

Figure 11(c) presents the cumulative distribution of the normalized location for choke links and bottleneck links, separately for different classes of links. The results are weighted by the number of probing sets in which a link is detected as a choke or bottleneck link. We observe that intra-AS choke links and bottlenecks typically appear earlier in the path compared to inter0-AS and inter1-AS choke links and bottlenecks. This may be due to some sources encountering choke links and bottlenecks within their home network.

### 4.4 Stability

Due to the bursty nature of Internet traffic and occasional routing changes, the bottleneck on an end-to-end path can vary over time. In this section, we examine the stability of these bottlenecks.

For our measurements, we randomly selected 10 probing sources from PlanetLab (the "ST" data set in Table 4). We sampled 30 destinations randomly chosen from the set of destinations obtained in Section 4.1. Measurements were taken over a three-hour period, divided into 9 epochs of 20 minutes each. In each epoch, Pathneck was run once for each source-destination pair using probing sets consisting of 5 probing trains and reporting choke links for each 5-train probing set.

Let \( \text{DetectionRate}_i(b) \) denote the frequency with which link \( b \) is a candidate choke link in probing set \( i \). For each path, the stability of choke link \( b \) over a period of \( n \) epochs is defined as:

\[
\text{Stability}(b) = \sum_{i=1}^{n} \text{DetectionRate}_i(b)
\]

The same definition applies to bottleneck links. Note that the range of \( \text{Stability}(b) \) is [0.5, \( n \)] because the detection rate is at least 0.5.

**Figure 12(a)** shows the cumulative distribution for stability across all measurements. We observe that bottlenecks and choke links exhibit similar stability, but this stability is not very high. We speculate that many bottlenecks are determined by traffic load rather than link capacity.

**Figure 12(b)** illustrates the stability (at the router level) for intra-AS, inter0-AS, and inter1-AS choke links. Intra-AS choke links are significantly less stable than inter-AS choke links. Among the two types of inter-AS choke links, inter0-AS choke links are more stable than inter1-AS choke links. Similar observations apply at the AS level, as shown by the curves labeled "intra-AS-level" and "inter1-AS-level": intra-AS choke links are again less stable than inter1-AS choke links. Additionally, AS-level choke links are more stable than router-level choke links. Similar patterns are observed for bottlenecks (not shown in Figure 12(b)).

Given the limited number of destinations (30) and the short duration of the experiment (3 hours), these stability results may not be fully representative of the Internet. We plan to conduct more extensive experiments in the future.

### 5. Inferring Bottlenecks

In this section, we address the problem of inferring the bottleneck location for a path that was not directly probed by Pathneck. If most or all of the links on a path have been probed in other paths, we may be able to infer the bottleneck location without additional probing, potentially reducing the amount of required probing.

**Methodology:**

We divide the "GE" data set gathered in Section 4 into a training set and a testing set. The training set is used to label each link \( L \) with an upper bound \( B_u(L) \) and/or a lower bound \( B_l(L) \) for the available bandwidth. These bounds are calculated using the algorithm presented in Section 2.3.3. If multiple bounds are available for a link from different probing sets, the lowest upper bound and the highest lower bound are used.

To handle measurement noise, we preprocess the data and include upper bounds only if the standard deviation across the probing set is less than 20% of the average upper bound.

The testing set is used for inference validation. For each path \( P \) in the testing set, we try to annotate each link \( L_i \in P \). If the link is covered in the training set, we associate the upper and/or lower bounds derived from the training set with it. The link \( L_i \) with the lowest upper bound \( B_u(L_i) \) is identified as the inferred bottleneck link \( \hat{L}_i \); links with no bounds or only lower bounds are ignored. We then compare \( \hat{L}_i \) with the true bottleneck location, as identified by Pathneck in the testing set. Paths in the testing set for which Pathneck cannot identify any choke link with a high enough detection rate and confidence are excluded from the analysis.

**Results:**

The probing data includes results for 51,193 paths. We randomly select 60% of the probing sets as the training data and the remaining 40% as the testing data, resulting in 20,699 paths in the testing set. Table 5 lists the percentage of paths in each class, with the "11.4%" entry corresponding to paths in the testing set where a bottleneck could not be identified.

- **Column "Correct":** Cases where inference was successful.
- **Column "Incorrect":** Cases where the wrong link was picked as the bottleneck.
- **Column "No upper bound":** Paths where the wrong bottleneck link was picked, but the training set only had lower bounds for the true bottleneck link.
- **Column "Not covered":** Paths where the bottleneck link was not covered in the training set.

Overall, inference is successful for 30% of the paths where a bottleneck can be identified in the testing set, increasing to 54% when sufficient data is available in the training set. The success rate diminishes as the information on fewer links in the path is available.

**Discussion:**

The results show that it is possible to infer the network bottleneck location without direct probing, but sufficient information on the links in the path is crucial. Properly designing the training set to reduce the number of links with little or no data is important. Ideally, systematically probing a specific region of the Internet and storing the results in a database could enable applications to infer bottlenecks for any path in that region.

### 6. Avoiding Bottlenecks

In this section, we explore how bottleneck information obtained by Pathneck can be used to improve overlay routing and multihoming.

#### 6.1 Overlay Routing

Overlay routing, or application layer routing, involves routing traffic through one or more intermediate nodes before reaching the destination. Previous studies have shown that overlay routing can significantly improve round trip delay and bypass routing failures, thus avoiding congested or problematic parts of the network. Between any two overlay nodes or between an overlay node and either the source or destination, regular IP routing is used.

BGP, the Inter-domain Routing Protocol, does not optimize for network performance in terms of delay, loss rate, or bandwidth. Therefore, shortest AS-path-based routing does not always yield the best-performing paths due to routing policies that can cause path inflation.

Overlay routing can be used to avoid bottleneck links in the underlying IP path, thereby improving application-level performance in terms of throughput. To the best of our knowledge, this study is the first large-scale analysis of how overlay routing can improve the available bandwidth of a path. Other metrics such as delay, loss rate, and cost are also important, and we plan to study their correlation with available bandwidth in the future.

**Methodology:**

We selected 27 RON and Planetlab nodes as both the source nodes and overlay nodes, as listed in the "OV" column in Table 4. Using a BGP table from a large tier-1 ISP, we sampled 200 random IP addresses from a diverse set of prefixes, ensuring each IP address originates from a different AS and ends with ".1" to minimize the chance of triggering alarms at firewalls. From each probing source, we performed the following probing process during the same time period to minimize the effect of transient congestion or nonstationary bottleneck links.

Given the list of 200 target IP addresses, each source node \( S \) probes each IP address 10 times using Pathneck. After probing each target IP address, \( S \) randomly selects 8 nodes from the set of 22 source nodes as candidate overlay nodes and probes each of these 8 nodes 10 times. This methodology ensures a fair comparison of the effectiveness of overlay routing in avoiding bottleneck links.

**Results:**

Of the 63,440 overlay attempts, 52.72% were useful, meaning overlay routing improved either the lower or upper bound on the available bandwidth of the path. If both bounds must increase, the useful rate is 15.92%; the breakdown for improving only the lower bound or only the upper bound is 17.39% and 19.40%, respectively. The distribution of improvements in upper and lower bounds for the available bandwidth is shown in Figure 13, indicating that most improvements in the upper bound are less than 100 Mbps, while the limit for the lower bound is 20 Mbps.

We examined the usefulness of overlay nodes in improving the available bandwidth bounds from each source node to the 200 randomly selected destinations. Most sources found almost all 22 overlay nodes useful for reaching some destinations with improved performance. Exceptions include the mazu1 site, which found only 8 out of 22 nodes useful, and the cornell site, which found 18 nodes useful. This may be due to the paths between these sites and the overlay nodes.