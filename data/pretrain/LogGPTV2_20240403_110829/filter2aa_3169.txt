**Separating Bots from Humans**

**Ryan Mitchell**  
[@kludgist]  
DEF CON 23, August 8th, 2015

### About Me
- **Software Engineer**
- **Author of Two Books:**
  - *Web Scraping with Python* (O’Reilly, 2015)
  - *Instant Web Scraping with Java* (Packt, 2013)
- **Education:**
  - Bachelor's in Engineering from Olin College
  - Master's student at Harvard University Extension School (2016)

### Background of This Talk
This talk is based on the O’Reilly Hacking Book: *Separating Bots from Humans*. Here are some pro-tips to get what you want:
- Include market research
- Write it in Python, as it is very popular

### What Are Web Scrapers, Bots, etc.?
- They can use browsers.
- They can take their time.
- They can be surprisingly smart.
- They can also be stunningly idiotic.

### Why Are They Important?
According to a 2014 report by Incapsula, bot traffic accounts for a significant portion of web traffic, making it crucial to differentiate between bots and humans.

### Defensive Strategies
#### robots.txt
- A "No Trespassing" sign that politely asks bots not to crawl certain parts of your site.

#### Terms of Service
- Legally binding agreements that state users should not engage in unauthorized activities.

#### Headers
- Some bots may try to disguise themselves by sending headers claiming they are not bots.

#### JavaScript
- Making your site un-indexable for anyone but the bad guys, though this is generally not a good practice.

#### Embedding Text in Images
- While effective, it can be cumbersome and is often seen as an extreme measure.

#### CAPTCHAs
- Annoying and breakable, but still a common method to distinguish humans from bots.

#### Honeypots
- Can be effective if implemented correctly. Ensure not to block legitimate bots like Google's.

### Example
- Visit [http://ryanemitchell.com/honeypots.html](http://ryanemitchell.com/honeypots.html) to see a honeypot in action.

### Behavioral Patterns
- Analyzing user behavior can help identify bots. Again, ensure not to block legitimate bots like Google's.

### IP Address Blocking
- Somewhat effective, but lists are difficult to maintain and can easily block legitimate users.

### Offensive Strategies
#### Targeted vs. Non-Targeted Attacks
- **Non-targeted:** Often involve generic attacks, such as looking for `/phpMyAdmin`.
- **Targeted:** Usually aimed at obtaining proprietary data.

#### OCR (Optical Character Recognition)
- Works best on normal text and can be used to solve CAPTCHAs.
- Creating training data is time-consuming; have a TV show or two ready.

#### OCR Training Tool
- Everything you need to solve a CAPTCHA: [https://github.com/REMitchell/tesseract-trainer](https://github.com/REMitchell/tesseract-trainer)

#### JavaScript Execution
- Tools like Selenium and PhantomJS can automate browser interactions.

#### Honeypot Avoidance
- More effective than expected, but color is its biggest weakness.
- Example: [https://github.com/REMitchell/python-scraping/blob/master/chapter12/3-honeypotDetection.py](https://github.com/REMitchell/python-scraping/blob/master/chapter12/3-honeypotDetection.py)

### Stop Caring!
- Bot-proofing sites is often too much work and can impede accessibility.
- Consider the value of your data:
  - API costs and ease of use can make it more attractive to pay for data.
- If your application is vulnerable to automated attacks, it is vulnerable, period.

### Q&A
Thank you for your attention. I'm now open to questions!