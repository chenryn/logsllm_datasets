4 KiB
16 KiB
105
106
107
400 KiB
400 B
396 MB 4.6 MB
400 B
3.9 MB
15 MB
3.2 GB
400 B
40 MB
99 MB
24 GB
400 KiB
1.5 GB
3.9 MB
12 GB
40 MB
96 GB
102 KiB
14 MB
102 KiB
25 MB
102 KiB
109 MB
400 KiB
6.2 GB
3.9 MB
48 GB
40 MB
384 GB
1.6 MB
51 MB
1.6 MB
62 MB
1.6 MB
146 MB
100
𝑁
104
106
Table 1: Storage usage for varying data, record and domain sizes.
The values are as follows. Left top: index I (B+ tree), right top: ag-
gregate tree DS, right bottom: ORAM U state and left bottom (bold):
ORAM S state. Italic indicates that the value is estimated.
Question-2: storage. While Epsolute storage efficiency is near-
optimal (O(1), 0), it is important to observe the absolute values.
Index I is implemented as a B+ tree with fanout 200 and occupancy
70 %, and its size, therefore, is roughly 5.7𝑛 bytes. Most of the ORAM
client storage is the PathORAM stash with its size chosen in a way
to bound failure probability to about 𝜂1 = 2−32 (see [65, Theorem
1]). In Table 1, we present Epsolute storage usage for the parameters
Session 7C: Database and Privacy CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2272that affect it — data, record and domain sizes. We measured the
sizes of the index I, DP structure DS, and ORAM client and server
states. Our observations are: (i) index size expectedly grows only
with the data size, (ii) DS is negligibly small in practice, (iii) small
I and DS sizes imply the efficiency of supporting multiple indexed
attributes, (iv) S to U storage size ratio varies from 85 in the smallest
setting to more than 2 000 in the largest, and (v) one can trade client
storage for ORAM failure probability. We conclude that the storage
requirements of Epsolute are practical.
is explained by the variance among parallel threads. During each
query the work is parallelized over 𝑚 ORAMs and the query is
completed when the last thread finishes. The problem, in distributed
systems known as “the curse of the last reducer” [66], is when one
thread takes disproportionally long to finish. In our case, we run
64 threads in default setting, and the delay is usually caused by a
variety of factors — blocking I/O, network delay or something else
running on a shared vCPU. This effect is noticeable when a single
thread does relatively little work and small disruptions actually
matter; the effect is negligible for large queries.
Figure 5: Privacy budget 𝜖
Figure 6: Effect of 𝜖
Question-3: varying parameters. To measure and understand
the impact of configuration parameters on the performance of our
solution we have varied 𝜖, record size, data size 𝑛, domain size 𝑁 ,
selectivities, as well as data and query distributions. The relation
that is persistent throughout the experiments is that for given data
and record sizes, the performance (the time to completely execute
a query) is strictly proportional to the total number of records, fake
and real, that are being accessed per query. Each record access
goes through the ORAM protocol, which, in turn, downloads, re-
encrypts and uploads O(log 𝑛) blocks. These accesses contribute
the most to the overhead and all other stages (e.g., traversing index
or aggregate tree) are negligible.
Privacy budget 𝜖 and its effect. We have run the default setting
for 𝜖 = {0.1, 0.5, ln 2, 1.0, ln 3}. 𝜖 strictly contributes to the amount
of noise, which grows exponentially as 𝜖 decreases, see Fig. 5, ob-
serve sharp drop. As visualized on Fig. 6, at high 𝜖 values the noise
contributes a fraction of total overhead, while at low values the
noise dominates the overhead entirely.
Figure 8: Record size Figure 9: Data size
Figure 10: Domain size
Record, data and domain sizes. We have tried 1 KiB, 4 KiB and
16 KiB records, see Fig. 8. Trivially, the elapsed time is directly
proportional to the record size.
We set 𝑛 to 105, 106 and 107, see Fig. 9. The observed correlation
of overhead against the data size is positive but non-linear, 10 times
increment in 𝑛 results in less than 10 times increase in time. This is
explained by the ORAM overhead — when 𝑛 changes, the ORAM
storage gets bigger and its overhead is logarithmic.
For synthetic datasets we have set 𝑁 to 100, 104 and 106, see
Fig. 10. The results for domain size correlation are more interesting:
low and high values deliver worse performance than the middle
value. Small domain for a large data set means that a query often
results in a high number of real records, which implies significant
latency regardless of noise parameters. A sparse dataset, on the
other hand, means that for a given selectivity wider domain is
covered per query, resulting in more nodes in the aggregate tree
contributing to the total noise value.
Figure 7: Selectivity
Figure 11: Data distribution
Figure 12: Query distribution
Selectivity. We have ranged the selectivity from 0.1 % to 2 % of the
total number of records, see Fig. 7. Overhead expectedly grows with
the result size. For smaller queries, and thus for lower overhead, the
relation is positive, but not strictly proportional. This phenomena,
observed for the experiments with low resulting per-query time,
Data and query distributions. Our solution performs best on the
uniform data and uniform ranges, see Figs. 11 and 12. Once a skew
of any kind is introduced, there appear sparse and dense regions
that contribute more overhead than uniform regions. Sparse regions
span over wider range for a given selectivity, which results in more
Session 7C: Database and Privacy CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2273noise. Dense regions are likely to include more records for a given
range size, which again results in more fetched records. Both real
datasets are heavily skewed towards smaller values as few people
have ultra-high salaries.
Figure 13: Scalability measurements for Π𝛾 and Πno−𝛾
Question-4: scalability. Horizontal scaling is a necessity for a
practical system, this is the motivation for the parallelization in the
first place. Ideally, performance should improve proportionally to
the parallelization factor, number of ORAMs in our case, 𝑚.
For scalability experiments we run the default setting for both
Πno−𝛾 and Π𝛾 (no-𝛾-method and 𝛾-method respectively) varying
the number of ORAMs 𝑚, from 8 to 96 (maximum vCPUs on a
GCP VM). The results are visualized on Fig. 13. We report two
positive observations: (i) the 𝛾-method provides substantially better
performance and storage efficiency, and (ii) when using this method
the system scales linearly with the number of ORAMs. (𝑚 = 96 is a
special case because some ORAMs had to share a single KVS.)
Improvement (section)
ORAM batching (5.3.1)
Lightweight ORAM machines (5.3.2)
Both improvements
6 978 ms
4 484 ms
8 417 ms
Table 2: Improvements over parallel Epsolute
Enabled Disabled Boost
840 ms
8.3x
840 ms
5.3x
840 ms
10.0x
Question-5: optimizations benefits. Table 2 demonstrates the
boosts our improvements provide; when combined, the speedup is
up to an order of magnitude.
ORAM request batching (Section 5.3.1) makes the biggest dif-
ference. We have run the default setting with and without the
batching. The overhead is substantially smaller because far fewer
I/O requests are being made, which implies benefits across the full
stack: download, re-encryption and upload.
Using lightweight ORAM machines (Section 5.3.2) makes a differ-
ence when scaling. In the default setting, 64 parallel threads quickly
saturate the memory access and network channel, while spreading
computation among nodes removes the bottleneck.
Question-6: multiple attributes. Epsolute supports multiple in-
dexed attributes. In Section 4.4 we described that the performance
implications amount to having an index I and a DP structure DS
per attribute and sharing the privacy budget 𝜖 among all attributes.
As shown in Table 1, I and DS are the smallest components of
the client storage. To observe the query performance impact, we
have used the default dataset with domains 104 and 106 as indexed
Figure 14: Query overhead when using multiple attributes. Only A
and Only B index one attribute. A and B indexes both attributes and
then queries one of them. Alternating indexes both attributes and
runs half of the queries against A and another half against B.
attributes A and B respectively. We ran queries against only A, only
B and against both attributes in alternating fashion. Each of the
2 to match the default privacy budget of ln(2).
ln 2
attributes used 𝜖 =
Fig. 14 demonstrates the query overhead of supporting multiple
attributes. The principal observation is that the overhead increases
only slightly due to a lower privacy budget. The client storage went
up by just 9 MB, and still constitutes only 3.3 % of the server storage,
which is not affected by the number of indexed attributes.
7 CONCLUSION AND FUTURE WORK
In this paper, we present a system called Epsolute that can be used
to store and retrieve encrypted records in the cloud while providing
strong and provable security guarantees, and that exhibits excellent
query performance for range and point queries. We use an opti-
mized Oblivious RAM protocol that has been parallelized together
with very efficient Differentially Private sanitizers that hide both
the access patterns and the exact communication volume sizes and
can withstand advanced attacks that have been recently developed.
We provide a prototype of the system and present an extensive
evaluation over very large and diverse datasets and workloads that
show excellent performance for the given security guarantees.
In our future work, we plan to investigate methods to extend
our approaches to use a trusted execution environment (TEE), like
SGX, in order to improve the performance even further. We will
also explore a multi-user setting without the need for a shared
stateful client, and enabling dynamic workloads with insertions
and updates. We will also consider how adaptive and non-adaptive
security models would change in the case of dynamic environments.
One would presumably also require DP of the server’s view in this
setting. Lastly, we plan to explore other relational operations like
JOIN and GROUP BY.
ACKNOWLEDGMENTS
We thank anonymous reviewers and Arkady Yerukhimovich for
valuable feedback. We also thank Daria Bogatova for devising the
name Epsolute and helping with the plots, diagrams and writing.
Finally, we thank Johes Bater for sharing Shrinkwrap code and
reviewing the prototype. Kobbi Nissim was supported by NSF Grant
No. 2001041, “Rethinking Access Pattern Privacy: From Theory to
Practice”. Dmytro Bogatov and George Kollios were supported by
NSF CNS-2001075 Award.
Session 7C: Database and Privacy CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2274REFERENCES
[1] Arvind Arasu, Spyros Blanas, Ken Eguro, Manas Joglekar, Raghav Kaushik,
Donald Kossmann, Ravi Ramamurthy, Prasang Upadhyaya, and Ramarathnam
Venkatesan. 2013. Secure Database-as-a-Service with Cipherbase. In Proceed-
ings of the 2013 ACM SIGMOD International Conference on Management of
Data (SIGMOD ’13). Association for Computing Machinery, 1033–1036. https:
//doi.org/10.1145/2463676.2467797
[2] Arvind Arasu, Spyros Blanas, Ken Eguro, Raghav Kaushik, Donald Kossmann,
Ravi Ramamurthy, and Ramarathnam Venkatesan. 2013. Orthogonal Security
With Cipherbase. In 6th Biennial Conference on Innovative Data Systems Research
(CIDR’13).
[3] Sumeet Bajaj and Radu Sion. 2013. TrustedDB: A trusted hardware-based database
with privacy and data confidentiality. IEEE Transactions on Knowledge and Data
Engineering 26, 3 (2013), 752–765. https://doi.org/10.1109/TKDE.2013.38
[4] Johes Bater, Gregory Elliott, Craig Eggen, Satyender Goel, Abel Kho, and Jennie
Rogers. 2017. SMCQL: Secure Querying for Federated Databases. Proc. VLDB
Endow. 10, 6 (Feb. 2017), 673–684. https://doi.org/10.14778/3055330.3055334
[5] Johes Bater, Xi He, William Ehrich, Ashwin Machanavajjhala, and Jennie Rogers.
2018. Shrinkwrap: efficient sql query processing in differentially private data
federations. Proceedings of the VLDB Endowment 12, 3 (2018), 307–320. https:
//doi.org/10.14778/3291264.3291274