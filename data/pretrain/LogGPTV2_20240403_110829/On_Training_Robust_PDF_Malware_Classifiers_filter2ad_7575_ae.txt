# 优化后的文本

## 攻击结果与模型鲁棒性分析

### MILP攻击
- **图3**：MILP攻击表明，单调分类器可以通过最多15个特征变化来规避。
- **图4**：增强进化攻击显示，Robust A+B模型需要比基线模型大3.6倍的L0距离才能被规避。

### 模型对比
| 模型          | L0距离（%） | ERA (%) | VRA (%) |
|---------------|-------------|---------|---------|
| Baseline NN   | 0           | 0.32    | 0       |
| Adv Retrain A+B | 0.03        | 7.38    | 19      |
| Robust A      | 0           | 0.00    | 36      |
| Robust D      | 0           | 0.00    | N/A     |
| Robust A+B    | 0.00        | 7.38    | N/A     |
| Robust A+B+E  | 0           | 0.00    | 68      |

- **表7**：Robust A+B模型在无限制梯度攻击下，比其他五个模型高7%的ERA。

### 无限制梯度攻击
- 我们的验证鲁棒模型可以将无限制梯度攻击下的ERA提高7%，达到200,000次迭代。Baseline NN在L0 = 19时首先达到0 ERA，而Robust A+B+E则需要最大的L0距离（68）才能达到0 ERA。
- 对于其他三个模型，Robust A+B是最具鲁棒性的，即使在200,000次攻击迭代后仍保持7.38%的ERA。

### 特征向量转换
- 将规避特征向量转换为实际PDF文件。每个特征索引更改时，我们删除相应的PDF对象或插入具有最小子节点数的对象。平均而言，模型对真实规避PDF恶意软件的ERA为94.25%，远高于对规避特征向量的0.62% ERA。

### MILP攻击者
- **最先进的无界攻击者**：MILP攻击者是针对GBDT单调分类器的无界白盒攻击者。攻击将规避问题表述为混合整数线性规划（MILP），目标是最小化种子恶意软件特征向量和变异向量之间的L0距离。
- **结果**：MILP攻击对所有四个单调分类器都成功了。Monotonic 10模型最弱，只需2个特征删除即可规避10%的PDF。所有单调分类器都可以通过最多15个特征变化来规避。相比之下，当L0 = 15时，Robust A+B的ERA为10.54%。

### 增强进化攻击者
- **最先进的无界攻击者**：增强进化攻击者具有黑盒Oracle访问权限，并基于遗传进化算法。
- **实现**：攻击通过随机删除、插入和替换来规避模型预测函数，使用适应度函数进行指导。
- **结果**：对于四个没有属性E的模型，攻击成功生成了所有PDF种子的规避变体。增强进化攻击需要比基线模型大3.6倍的L0距离和21倍的突变次数来规避我们的鲁棒模型。

### 反向模仿攻击者
- **最先进的无界攻击者**：反向模仿攻击者将恶意负载注入良性PDF中。
- **实现**：我们在良性PDF中注入恶意负载，测试模型是否能检测到这些恶意PDF。
- **结果**：由于这超出了所有五个鲁棒属性，攻击可以击败大多数验证鲁棒模型和基线模型，除了单调分类器和Robust A+B+E模型。

### 自适应进化攻击者
- **新的自适应无界攻击者**：自适应进化攻击者具有与增强进化攻击者相同的黑盒访问权限，并且不受鲁棒属性的约束。
- **实现**：设计了三种版本的自适应攻击来规避最强的三个模型。
- **结果**：自适应攻击需要比单调分类器大10倍的L0距离和3.7倍的突变次数来规避我们的模型。

## 讨论
- **泛化**：通过设定攻击者的行动边界，我们可以提供PDF恶意软件分类器的可验证鲁棒属性。此外，这种鲁棒训练还可以提高最先进的无界攻击者的门槛。
- **可扩展性**：使用符号区间分析的可验证鲁棒训练比现有的声音近似方法更快。许多技术可以将训练扩展到更大的神经网络和数据集。
- **多样化的鲁棒属性**：插入和删除的鲁棒属性可以用作构建更强属性的基础。训练多种属性组合可以使攻击者的规避任务更加困难。

## 相关工作
- 现有的防御措施主要集中在特征减少和对抗性鲁棒重训练上。我们的方法不仅可以用于插入，还可以用于删除，从而增加攻击者的成本。

## 结论
- 我们首次为PDF恶意软件分类器训练了可验证的鲁棒属性。我们的最佳模型在插入和删除属性上分别达到了99.68%和85.28%的验证鲁棒准确性，同时保持了99.74%的准确性和0.56%的误报率。

## 致谢
- 感谢我们的导师Nicolas Papernot和匿名审稿人提供的建设性和有价值的反馈。这项工作得到了NSF、ONR、ARL、Google、Capital One和J.P. Morgan的支持。

## 参考文献
- [1] Adversarial Machine Learning: Are We Playing the Wrong Game?
- [2] Hidost: Toolset for extracting document structures from PDF and SWF files.
- [3] M. Parkour. 16,800 clean and 11,960 malicious files for signature testing and research.
- [4] M. Parkour. contagio: Version 4 april 2011 - 11,355+ malicious documents - archive for signature testing and research.
- [5] NDSS Talk: Automatically Evading Classifiers (including Gmail’s).
- [6] peepdf: Powerful Python tool to analyze PDF documents.
- [7] sklearn: Classification metrics.