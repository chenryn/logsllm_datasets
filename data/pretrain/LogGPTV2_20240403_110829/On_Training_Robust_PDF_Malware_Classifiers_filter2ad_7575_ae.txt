0.25
0.00
Baseline NN
Adv Retrain A+B
Ensemble A+B
Robust A+B
0
100
200
L0
300
400
Figure 3: MILP attack:
the monotonic
classiﬁers can be evaded by up to 15 feature
changes.
Figure 4: Enhanced evolutionary attack: Ro-
bust A+B requires up to 3.6 times larger L0 dis-
tance to be evaded compared to the baselines.
Model
Baseline NN
Adv Retrain A+B
Robust A
Robust D
Robust A+B
Robust A+B+E
L0 for ERA (%) at 200K
attack iterations
0
0.32
0
0.03
7.38
0
ERA=0
19
N/A
36
N/A
N/A
68
Table 7: Robust A+B model maintains 7% higher ERA against
the unrestricted gradient attack than the other ﬁve models.
descent) is not effective at evading this model, since the attack
could have used the solution from property B to reduce the
ERA for the other two properties to at least 84.6%. Although
Adv Retrain A+B has a higher ERA in property D and E
against the gradient attack than the Robust A+B model, both
models have 0 VRA in these properties. Since there always
exist stronger attacks that can reduce the ERA [54], VRA
provides a stronger robustness guarantee than ERA.
Unbounded Gradient Attack. Our veriﬁably robust model
can improve the ERA against the unrestricted gradient attack
by 7% up to 200,000 iterations. Table 7 shows the attack results
on ﬁve representative models. The unrestricted gradient attack
can reduce the ERA for three models to 0 given enough allow-
able L0 distance in feature changes. Baseline NN is the ﬁrst
to reach 0 ERA at L0 =19, whereas Robust A+B+E requires
the largest L0 distance (68) to reach 0 ERA. For the other three
models, Robust A+B is the most robust one. It maintains 7.38%
ERA even after 200,000 attack iterations. The ERA converges
for Adv Retrain A+B, Robust D and Robust A+B models
against the unrestricted gradient attack (Appendix A.3.2).
Further details about the attack can be found in Appendix A.3.
We convert the evasive feature vectors to real PDFs. Given
each feature index change, we either delete the corresponding
PDF object, or insert the object with minimal number of
children in the benign training dataset. Inserting object
with minimal children makes the features from constructed
PDF close to the evasive features. On average, the ERA of
models against the real evasive PDF malware is 94.25%, much
higher than 0.62% ERA against evasive feature vectors, since
unrestricted gradient attack often breaks the PDF semantics
(Appendix A.4). Due to the inherent limitation of feature-space
attacks, we also evaluate robustness of the models against
realizable attacks from Section 4.5 to Section 4.7.
4.4 MILP Attacker
State-of-the-art Unbounded Attacker. The Mixed
Integer Linear Program (MILP) attacker is the unbounded
whitebox attacker for the GBDT monotonic classiﬁers,
proposed by Kantchelian et al. [33]. The attack formulates
the evasion problem as a Mixed Integer Linear Program. The
variables in this program are predicates and leaves in all the
decision trees. We set the objective of the linear program to
minimize the L0 distance between the seed malware feature
vector and the variant vector. The constraints to solve the linear
program include model misclassiﬁcation, consistency among
leaves and predicates, and several variables being integer. The
solution to the MILP represents the malware feature vector
manipulation. We use the re-implementation from [14] to
conduct the MILP attack.
4.4.1 Result
The MILP attack can succeed for all the 3416 test malicious
PDFs against all four monotonic classiﬁers. We plot the ERA
values with different L0 distance (number of feature changes)
in Figure 3. The Monotonic 10 model is the weakest among
them. With only 2 feature deletion, 10% of PDFs can be
evaded, e.g., deleting /Root/Names/JavaScript/Names and
/Root/Names/JavaScript/Names/JS/Filter. Everything
can be evaded by up to six feature changes for the 10 learner
model. Using 100 learners can increase the L0 distance for
evasion. However, using more learners does not increase the
robustness after 100 learners. All the monotonic classiﬁers
can be evaded by up to 15 feature changes. In comparison,
when L0 = 15, the ERA for Robust A+B is 10.54%. Under
different whitebox attacks, Robust A+B is more robust than
the monotonic classiﬁers.
After converting the evasive feature vectors to real PDFs,
none of them are still malicious, since the MILP attack
deletes the exploit (Appendix A.4). Next, we will evaluate the
strongest models against unrestricted black box attacks that
ensure the maliciousness of evasive PDF variants.
4.5 Enhanced Evolutionary Attacker
State-of-the-art Unbounded Attacker. The enhanced
evolutionary attacker has black-box oracle access to the model,
including the classiﬁcation label and scores, and she is not
bounded by the robustness properties. The attack is based on
the genetic evolution algorithm [61].
USENIX Association
29th USENIX Security Symposium    2353
4.5.1 Implementation
The genetic evolution attack evades the model prediction
function by mutating the PDF malware, using random deletion,
insertion, and replacement, guided by a ﬁtness function. We
implemented two strategies to enhance the evolutionary attack,
with details and the experiment set up in Appendix A.5.
4.5.2 Results
Within the 500 PDF malware seeds that exhibit network
behavior from previous work [61], we can detect 495 PDFs
with signatures using our cuckoo sandbox. All the PDF
malware seeds belong to the testing PDF set in Table 2. By
round robin, we go through the list of randomly scheduled
PDFs by rounds of attacks, until all of them are evaded.
We run the attack on the best baseline and robust models:
Baseline NN, Adv Retrain A+B, Ensemble A+B, Monotonic
Classiﬁers, Robust A+B, and Robust A+B+E. For four models
without property E, the attack has succeeded in generating
evasive variants for all PDF seeds. It takes between three days
to two weeks to evade each model. The attack is not effective
against monotonic classiﬁer and Robust A+B+E model. Al-
though the attack can identify that deletion is preferred to evade
the models, sometimes it deletes the exploit. We design adap-
tive evolutionary attacks to evade these models in Section 4.7.
L0 distance. The enhanced evolutionary attack needs up
to 3.6 times larger L0 distance, and 21 times more mutations
(Appendix A.6) to evade our robust model than the baselines.
We plot the ERA for different models under various L0 dis-
tances to generate evasive PDF variants in Figure 4. For hidost
features, the L0 distance also means the number of feature
changes. To evade the baseline NN model, at least 49 features
need to be changed. The ERA of the model quickly drops to
zero at 133 features. The Adv Retrain A+B and Ensemble
A+B both require more changes to be fully evaded, up to 252
and 300 respectively. Compared to these baselines, our Robust
A+B model needs the most number of feature changes (475)
to be evaded, 3.6 times of that against the Baseline NN. The
smallest L0 distances to generate one evasive PDF malware
variant are 49, 39, 134, and 159 for Baseline NN, Adv Retrain
A+B, Ensemble A+B, and Robust A+B, respectively.
4.6 Reverse Mimicry Attacker
State-of-the-art Unbounded Attacker. The reverse
mimicry attacker injects malicious payload into a benign PDF,
which is outside of all ﬁve robustness properties. We have
proposed robustness properties for malicious PDFs, not benign
ones. The attacker uses the same strategy for all models, and
thus she does not need to know model internals or the defenses.
4.6.1 Implementation
We implement our own reverse mimicry attack,similar to the
JSinject [40]. We use peepdf [6] static analyzer to identify the
suspicious objects in the PDF malware seeds, and then inject
these objects to a benign PDF. We inject different malicious
payload into a benign ﬁle, whereas the JSinject attack injects
the same JavaScript code into different benign PDFs. Within
the PDF malware seeds, 250 of them retained maliciousness
according to the cuckoo oracle. Some payload are no longer
malicious because there can be object dependencies within the
malware not identiﬁed by the static analyzer. We test whether
the models can detect the 250 PDFs are malicious.
4.6.2 Results
We measure ERA as the percentage of correctly classiﬁed
PDFs for the strongest models against whitebox attacks in
Table 6. Since this is outside all ﬁve robustness properties, the
attack can defeat most veriﬁably robust models and baseline
models, except the monotonic classiﬁer and Robust A+B+E
models. The monotonic classiﬁer has the monotonic constraint
enforced for the benign PDFs, whereas we only trained
property E for malicious PDFs for our Robust A+B+E model.
However, we still achieve 2% higher ERA than the Monotonic
100 model against the reverse mimicry attack. This shows that
veriﬁably robust training can generalize outside the trained
robustness properties. Since training property E incurs higher
FPR than properties with smaller subtree distances, we plan
to experiment with training insertion property with small
distance for benign samples as future work.
4.7 Adaptive Evolutionary Attacker
New Adaptive Unbounded Attacker. The adaptive evolu-
tionary attackerhas the same level of black-box access as the en-
hanced evolutionary attacker (Section 4.5). She is not bounded
by the robustness properties and knows about the defense.
4.7.1
Implementation
To evade the three strongest models:
the monotonic
classiﬁer, Robust A+B, and Robust A+B+E, we design three
versions of the adaptive attacks as following.
Move Exploit Attack. The monotonic property forces the
attacker to delete objects from the malware, but deletion could
remove the exploit. Therefore, we implement a new mutation
to move the exploit around to different trigger points in the PDF
(Appendix A.7). This attack combines the move exploit muta-
tion with deletion to evade the monotonic classiﬁer. Note that
the move exploit mutation is not effective against Robust A+B,
since it is covered by the insertion and deletion properties.
Scatter Attack. To evade Robust A+B, we insert and delete
more objects under different subtrees. We keep track of past
insertion and deletion operations separately, and prioritize new
insertion and deletion operations to target a different subtree.
Move and Scatter Combination Attack. To evade the
Robust A+B+E model, we combine the move exploit attack
and the scatter attack, to target all the properties of the model.
4.7.2 Results
The adaptive attacks need 10 times larger L0 distance
(Figure 5), and 3.7 times more mutations (Appendix A.6) to
2354    29th USENIX Security Symposium
USENIX Association
A
R
E
1.00
0.75
0.50
0.25
0.00
0
100
200
L0
Robust A+B (Scatter) Robust A+B+E (Both)
Monotonic 100 (Move)
300
400
Figure 5: The decrease of robustness in ERA against adaptive
evolutionary attacks as the L0 distance increases.
evade our model than the monotonic classiﬁer. Figure 5 shows
the L0 distance to evade the three models: Monotonic 100,
Robust A+B, and Robust A+B+E. The move exploit attack is
very effective against the Monotonic 100 model. The ERA of
Monotonic 100 quickly drops to zero at L0 =10. The scatter at-
tack can reduce the mutation trace length to evade Robust A+B
compared to the nonadaptive version. However, the median L0
distance has increased from 228 (Figure 4) to 289 (Figure 5).
The minimal L0 distances to generate one evasive PDF
malware for the Monotonic 100 and Robust A+B are 1 and 263
respectively. Lastly, the move and scatter combination attack
can reduce the ERA of Robust A+C+E to 44% after running
for three weeks. The attack is stuck at premature convergence
and needs additional improvements to fully evade the model.
5 Discussion
Generalization. In the arms race against malware detection
and evasion, there has been no veriﬁably robust solution to the
detection problem. By setting bounds on attackers’ actions, we
can provide veriﬁable robustness properties in PDF malware
classiﬁers. We further show that such robust training can also
increase the bar for state-of-the-art unbounded attackers. Since
we specify robustness properties related to the PDF syntax,they
can be generalized to different features, datasets, and models.
Our method can be complementary to other defenses such as
feature reduction. We plan to explore all these issues regarding
the generalization of our methodology in our future work.
Scalability. Veriﬁably robust training using symbolic
interval analysis is faster than existing sound approximation
methods, achieving state-of-the-art tight bounds. Many
techniques can scale the training to larger neural networks
with hundreds of thousands of hidden units, and larger datasets
such as ImageNet-200 [27, 53, 55, 59]. We plan to explore the
tradeoffs between scalability and performance (e.g., accuracy,
robustness, and false positive rate) of the trained network.
Diverse Robustness Properties. The robustness properties
for insertion and deletion can be used as building blocks to
construct stronger properties. Training combinations of prop-
erties can make the evasion task even harder for the attacker. In
addition, we plan to train veriﬁable robustness properties for
benign PDFs, to defend against another type of evasion search
that starts from a benign PDF seed. Exploring the tradeoffs
among learning multiple robustness properties and overhead
of training will be an interesting direction for future work.
6 Related Work
Existing defenses in increasing the robustness of malware
classiﬁers mainly focus on using feature reduction and
adversarially robust retraining. Researchers have employed
methods including mutual information [28], expert domain
knowlege [32], information from cuckoo sandbox [51]
to remove features unrelated to maliciousness. However,
previous adversarial retraining results show severe drop in
accuracy [32], and increase in false positive rate [1, 28].
Incer et al. [32] enforced the monotonicity property to
make the malware classiﬁer robust against attacks that
increase feature values. Thus, attackers have to conduct
more expensive feature manipulation that might remove the
malicious functionality. In comparison, we train robustness
properties not only for insertion, but also for deletion, since
deletion operations are often not costly to the attacker [61].
Our method can increase the feature distance and mutation
trace length as cost for the attacker to evade the model. Existing
works have discussed cost for the attackers to manipulate
features [38], to increase suspiciousness [16], and to solve the
combinatorial optimization problem [17]. On the other hand,
several work have explored the cost for the defender [19, 62].
Dreossi et al. [19] argued that only some adversarial examples
cause the overall control system to make catastrophic decision.
Zhang et al. [62] integrated the defender’s cost with Wong et
al.’s veriﬁably robust training method [58].
7 Conclusion
We are the ﬁrst to train veriﬁable robustness properties for
PDF malware classiﬁer. We proposed a new distance metric
in the PDF tree structure to bound robustness properties. Our
best model achieved 99.68% and 85.28% veriﬁed robust
accuracy (VRA) for the insertion and deletion properties,
while maintaining 99.74% accuracy and 0.56% false positive
rate. Our results showed that training security classiﬁers with
veriﬁable robustness properties is a promising direction to
increase the bar for unrestricted attackers.
Acknowledgements
We thank our shepherd Nicolas Papernot and the anony-
mous reviewers for their constructive and valuable feedback.
This work is sponsored in part by NSF grants CNS-18-
42456, CNS-18-01426, CNS-16-17670, CNS-16-18771,
CCF-16-19123, CCF-18-22965, CNS-19-46068; ONR grant
N00014-17-1-2010; an ARL Young Investigator (YIP) award;
a NSF CAREER award; a Google Faculty Fellowship; a
Capital One Research Grant; and a J.P. Morgan Faculty Award.
Any opinions, ﬁndings, conclusions, or recommendations ex-
pressed herein are those of the authors, and do not necessarily
reﬂect those of the US Government, ONR, ARL, NSF, Google,
Capital One or J.P. Morgan.
USENIX Association
29th USENIX Security Symposium    2355
References
[1] Adversarial Machine Learning: Are We Playing the
https://speakerdeck.com/evansuva/
Wrong Game?
adversarial-machine-learning-are-we-playing-
the-wrong-game.
[2] Hidost: Toolset for extracting document structures from PDF
and SWF ﬁles. https://github.com/srndic/hidost.
[3] M. Parkour. 16,800 clean and 11,960 malicious ﬁles for signa-
ture testing and research. http://contagiodump.blogspot.
com/2013/03/16800-clean-and-11960-malicious-
files.html.
[4] M. Parkour. contagio: Version 4 april 2011 - 11,355+ ma-
licious documents - archive for signature testing and re-
search. http://contagiodump.blogspot.com/2010/08/
malicious-documents-archive-for.html.
[5] NDSS Talk: Automatically Evading Classiﬁers (including
https://jeffersonswheel.org/2016/ndss-
Gmail’s).
talk-automatically-evading-classifiers-
including-gmails.
[6] peepdf: Powerful Python tool to analyze PDF documents.
https://github.com/jesparza/peepdf.
[7] sklearn: Classiﬁcation metrics.
https://scikit-
learn.org/stable/modules/model_evaluation.html#
classification-metrics.