Journal
[61] Radhika Mittal, Vinh The Lam, Nandita Dukkipati, Emily Blem, Hassan Was-
sel, Monia Ghobadi, Amin Vahdat, Yaogong Wang, David Wetherall, and David
Zats. 2015. TIMELY: RTT-Based Congestion Control for the Datacenter. In
Proceedings of the 2015 ACM Conference on Special Interest Group on Data Com-
munication. ACM, London United Kingdom, 537–550. https://doi.org/10.1145/
2785956.2787510
[62] Whitney K. Newey and Kenneth D. West. 1987. A Simple, Positive Semi-Definite,
Heteroskedasticity and Autocorrelation Consistent Covariance Matrix. Econo-
metrica 55, 3 (1987), 703–708. https://doi.org/10.2307/1913610
[63] Garg Nitin. 2019. COPA Congestion Control for Video Performance. (Nov. 2019).
16,
https://engineering.fb.com/2019/11/17/video-engineering/copa/
[64] Samuel D. Oman and Esther Seiden. 1988. Switch-Back Designs. Biometrika 75, 1
(March 1988), 81–89. https://doi.org/10.1093/biomet/75.1.81
[65] James Robins. 1986. A New Approach to Causal Inference in Mortality Studies
with a Sustained Exposure Period—Application to Control of the Healthy Worker
Survivor Effect. Mathematical Modelling 7, 9-12 (Jan. 1986), 1393–1512. https:
//doi.org/10.1016/0270-0255(86)90088-6
eling, decisions. J. Amer. Statist. Assoc. 100, 469 (2005), 322–331.
[66] Donald B Rubin. 2005. Causal inference using potential outcomes: Design, mod-
[67] Ahmed Saeed, Nandita Dukkipati, Vytautas Valancius, Vinh The Lam, Carlo
Contavalli, and Amin Vahdat. 2017. Carousel: Scalable Traffic Shaping at End
Hosts. In The Conference of the ACM Special Interest Group. ACM Press, 404–417.
https://doi.org/10.1145/3098822.3098852
[68] Martin Saveski, Jean Pouget-Abadie, Guillaume Saint-Jacques, Weitao Duan,
Souvik Ghosh, Ya Xu, and Edoardo M. Airoldi. 2017. Detecting Network Effects:
Randomizing Over Randomized Experiments. In Proceedings of the 23rd ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM,
Halifax NS Canada, 1027–1035. https://doi.org/10.1145/3097983.3098192
[69] Robert Sayre. 2008. Change Max-Persistent-Connections-per-Server to 6. (March
2008). https://bugzilla.mozilla.org/show_bug.cgi?id=423377
[70] Nate Schloss and Ben Maurer. 2017. This Browser Tweak Saved 60% of Requests to
Facebook. (Jan. 2017). https://engineering.fb.com/2017/01/26/web/this-browser-
tweak-saved-60-of-requests-to-facebook/
[71] Dominik Scholz, Benedikt Jaeger, Lukas Schwaighofer, Daniel Raumer, Fabien
Geyer, and Georg Carle. 2018. Towards a Deeper Understanding of TCP
BBR Congestion Control. In 2018 IFIP Networking Conference (IFIP Network-
ing) and Workshops. IEEE, Zurich, Switzerland, 1–9. https://doi.org/10.23919/
IFIPNetworking.2018.8696830
[47] Ramesh Johari, Hannah Li, Inessa Liskovich, and Gabriel Weintraub. 2020. Ex-
perimental design in two-sided platforms: An analysis of bias. arXiv preprint
arXiv:2002.05670 (2020).
[48] Matt Joras and Yang Chi. 2020. How Facebook Is Bringing QUIC to Billions. (Oct.
2020). https://engineering.fb.com/2020/10/21/networking-traffic/how-facebook-
is-bringing-quic-to-billions/
[49] Matt Joras and Yang Chi. 2020. How Facebook Is Bringing QUIC to Billions. (Oct.
2020). https://engineering.fb.com/2020/10/21/networking-traffic/how-facebook-
is-bringing-quic-to-billions/
[50] Arash Molavi Kakhki, Samuel Jero, David Choffnes, Cristina Nita-Rotaru, and
Alan Mislove. 2017. Taking a Long Look at QUIC: An Approach for Rigorous
Evaluation of Rapidly Evolving Transport Protocols. In Proceedings of the 2017
Internet Measurement Conference. ACM, London United Kingdom, 290–303. https:
//doi.org/10.1145/3131365.3131368
[51] Brian Karrer, Liang Shi, Monica Bhole, Matt Goldman, Tyrone Palmer, Charlie
Gelman, Mikael Konutgan, and Feng Sun. 2020. Network Experimentation at
Scale. arXiv:2012.08591 [cs, stat] (Dec. 2020). arXiv:cs, stat/2012.08591 http:
//arxiv.org/abs/2012.08591
14
93
[52] David Kastelman and Raghav Ramesh. 2018.
domized Experimentation Under Network Effects at DoorDash.
Switchback Tests and Ran-
(Feb.
[72] Anant Shah. 2019. BBR Evaluation at a Large CDN.
//blog.apnic.net/2019/11/01/bbr-evaluation-at-a-large-cdn/
(Nov. 2019). https:
Unbiased Experiments in Congested Networks
IMC ’21, November 2–4, 2021, Virtual Event, USA
[73] Steve Souders. 2008. Roundup on Parallel Connections. (March 2008). https:
//www.stevesouders.com/blog/2008/03/20/roundup-on-parallel-connections/
[74] Bruce Spang, Brady Walsh, Te-Yuan Huang, Tom Rusnock, Joe Lawrence, and
Nick McKeown. 2019. Buffer Sizing and Video QoE Measurements at Netflix.
In Proceedings of the 2019 Workshop on Buffer Sizing. ACM, Palo Alto CA USA.
https://doi.org/10.1145/3375235.3375241
[75] Jerzy Splawa-Neyman, Dorota M Dabrowska, and TP Speed. 1990. On the ap-
plication of probability theory to agricultural experiments. Essay on principles.
Section 9. Statist. Sci. (1990), 465–472.
[76] Diane Tang, Ashish Agarwal, Deirdre O’Brien, and Mike Meyer. 2010. Overlap-
ping Experiment Infrastructure: More, Better, Faster Experimentation. In KDD’10.
10.
[77] Eric J. Tchetgen Tchetgen and Tyler J. VanderWeele. 2012. On causal inference
in the presence of interference. Statistical Methods in Medical Research 21 (2012),
55 – 75.
[78] Martin Tingley. 2018.
flix: Visualizing Practical and Statistical Significance.
https://netflixtechblog.com/streaming-video-experimentation-at-netflix-
visualizing-practical-and-statistical-significance-7117420f4e9a
Streaming Video Experimentation at Net-
(Sept. 2018).
[79] Linus Torvalds. [n. d.]. Tcp_input.c - Linux (v5.11-Rc5).
([n. d.]). https://
github.com/torvalds/linux/blob/2ab38c17aac10bf55ab3efde4c4db3893d8691d2/
net/ipv4/tcp_input.c#L873
[80] Donald F Towsley. 2015. TCP, Congestion Control.
(Nov. 2015).
http:
//gaia.cs.umass.edu/cs653/slides/tcp.pdf
[81] Belma Turkovic, Fernando A. Kuipers, and Steve Uhlig. 2019. Fifty Shades of
Congestion Control: A Performance and Interactions Evaluation. arXiv:1903.03852
[cs] (March 2019). arXiv:cs/1903.03852 http://arxiv.org/abs/1903.03852
[82] Belma Turkovic, Fernando A. Kuipers, and Steve Uhlig. 2019. Interactions be-
tween Congestion Control Algorithms. In 2019 Network Traffic Measurement and
Analysis Conference (TMA). 161–168. https://doi.org/10.23919/TMA.2019.8784674
[83] Johan Ugander, Brian Karrer, Lars Backstrom, and Jon Kleinberg. 2013. Graph
Cluster Randomization: Network Exposure to Multiple Universes. In Proceedings
of the 19th ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining (KDD ’13). Association for Computing Machinery, New York, NY,
USA, 329–337. https://doi.org/10.1145/2487575.2487695
[84] Ranysha Ware, Matthew K. Mukerjee, Srinivasan Seshan, and Justine Sherry.
2019. Beyond Jain’s Fairness Index: Setting the Bar For The Deployment of
Congestion Control Algorithms. In Proceedings of the 18th ACM Workshop on
Hot Topics in Networks. ACM, Princeton NJ USA, 17–24. https://doi.org/10.1145/
3365609.3365855
[85] Ranysha Ware, Matthew K. Mukerjee, Srinivasan Seshan, and Justine Sherry. 2019.
Modeling BBR’s Interactions with Loss-Based Congestion Control. In Proceedings
of the Internet Measurement Conference. ACM, Amsterdam Netherlands, 137–143.
https://doi.org/10.1145/3355369.3355604
[86] David X. Wei, Pei Cao, and Steven H. Low. 2006. TCP Pacing Revisited.
(2006). http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.2658&rep=
rep1&type=pdf
[87] Francis Y Yan, Hudson Ayers, Chenzhi Zhu, Sadjad Fouladi, James Hong, Keyi
Zhang, Philip Levis, and Keith Winstein. 2020. Learning in Situ: A Randomized
Experiment in Video Streaming. In NSDI. Santa Clara, CA, USA, 16. https://
www.usenix.org/system/files/nsdi20-paper-yan.pdf
A ETHICS
While our experiments involve live traffic running on a large video
streaming service, our work is not human subjects research, and
we have no way to identify the individual users of the platform. We
only have access to performance-related data. We ran experiments
which improved behavior during congestion, but they did so at the
cost of reducing video quality. Netflix’s customers have the ability
to opt out of experiments, if they choose to.
B APPENDIX: ANALYSIS OF EXPERIMENTAL
DATA
In this appendix we describe our general approach to analysis of
data from experiments at scale, and how we apply this approach in
the context of the experiments reported in Sections 4 and 5. For the
duration of the appendix, we consider data for a fixed representative
metric collected on a per-session basis (e.g., average throughput).
In our experiments units are video sessions, and we let 𝐴𝑖 denote
the treatment condition of session 𝑖, where 𝐴𝑖 = 1 denotes treatment
15
94
Figure 13: Comparison of treatment effect sizes and confi-
dence intervals when aggregating by hour or by account.
and 𝐴𝑖 = 0 denotes control. Let 𝑌𝑖 denote the observed outcome
on session 𝑖. Let ℎ𝑖 ∈ {1, . . . , 24} denote the hour of session 𝑖. Our
first step in analysis is to aggregate data at the hourly level: for
each hour 𝑡 = 1, . . . , 24 and each treatment condition 𝐴 = 0, 1, we
compute:
𝑖 𝑌𝑖 1ℎ𝑖 =𝑡,𝐴𝑖 =𝐴
𝑖 1ℎ𝑖 =𝑡,𝐴𝑖 =𝐴
.
𝑍𝑡 (𝐴) =
This is the average outcome for sessions in treatment condition 𝐴
during hour 𝑡.
Next, we use a regression approach to estimate the treatment
effect [31, Ch 9], using the following model specification:
𝑍𝑡 (𝐴) = 𝑐 + 𝛽0𝐴 + 𝛽𝑡 + 𝜀𝑖,
for all 𝑡, 𝐴.
Here 𝑡 = 1, . . . , 24 and 𝐴 = 0, 1; 𝛽0 is the coefficient on the treat-
ment indicator; each 𝛽𝑡 is a fixed effect to control for hour-of-day
heterogeneity; 𝑐 is an intercept term; and 𝜀𝑖 is the error term. We
fit this model using least squares linear regression, and estimate
confidence intervals using Newey-West robust standard errors [62]
with a lag of two hours. This is a common approach in economet-
rics to account for autocorrelation between successive hours, and
heteroskedasticity in the error terms 𝜀𝑖. We use hats to denote the
corresponding estimates; in particular, ˆ𝛽0 is the estimated coef-
ficient on the treatment indicator, and thus an estimator for the
average treatment effect.
We note that the approach we take here—where we aggregate
data to the hourly level—essentially makes a worst case assump-
tion that sessions within a given hour and treatment condition are
perfectly correlated with each other. This is a very conservative
assumption, that we feel only strengthens the case in our paper.
Though conservative, this is current practice in analysis of switch-
back experiments in other industries [52]. If we were to analyze the
results using the standard account-level standard errors, we would
get much tighter confidence intervals as shown in Figure 13. Cor-
recting standard error estimates to properly estimate dependencies
between sessions remains an active area of investigation.
We now describe how we apply this approach to our experiments
in Sections 4 and 5.
of all sessions in the treatment group on link 1 as our treatment
sessions (𝐴𝑖 = 1); and the 95% of all sessions in the control group on
link 2 as our control sessions (𝐴𝑖 = 0). We ignore all other sessions.
pute the approximate estimate(cid:100)TTE for TTE, we consider the 95%
We then follow the analysis workflow above, and set(cid:100)TTE = ˆ𝛽0
(cid:98)𝑠(0.95) = ˆ𝛽0 from the resulting fitted regression.
To estimate spillover, we use only the 5% control sessions on
link 1 and the 95% control sessions on link 2. We set 𝐴𝑖 = 1 for
the control sessions on link 1, and 𝐴𝑖 = 0 on link 2. We compute
from the resulting fitted regression.
Finally we compute two “naïve” estimates using the difference
in means estimator (1) from Section 2. In particular, for 𝑝 = 0.95,
we use only the sessions on link 1: we consider all sessions in the
treatment group on link 1 as our treatment sessions (𝐴𝑖 = 1), and
all sessions in the control group on link 1 as our control sessions
(𝐴𝑖 = 0). All sessions on link 2 are ignored. An analogous approach
IMC ’21, November 2–4, 2021, Virtual Event, USA
Spang et al.
B.1 Application to paired link experiment
In Section 4, sessions on link 1 were randomized 95% to treatment
and 5% to control; and sessions on link 2 were randomized 5% to
treatment and 95% to control.
We carry out four separate analyses on this data. First, to com-
is carried out for 𝑝 = 0.05 using the treatment and control sessions
on link 2 (ignoring all sessions on link 1), to compute ˆ𝜏(0.05). We
aggregate to the account level, not the hour level, as is standard
when analyzing A/B tests.
Finally, all reported values are normalized to make them more
interpretable. In particular, we divide all estimates by the average
across all control sessions on link 2 (where 95% of the traffic was
control). This approach ensures all reported values are a relative
difference measured against the same global control condition.
B.2 Application to switchback experiments
and event studies
In Section 5, we analyzed a switchback experiment and an event
study that was emulated using the data from the paired link experi-
ment. This analysis was carried out as follows. For the three days
chosen to be treatment intervals, we define all treatment sessions
on link 1 to have 𝐴𝑖 = 1, and ignore all other sessions. For the two
days chosen to be control intervals, we define all control sessions
on link 2 to have 𝐴𝑖 = 0, and ignore all other sessions. We then
proceed with the analysis workflow above, and report ˆ𝛽0 as our
emulated estimate of TTE.
16
95