Hamdist (f)
10
10
2
7
9
1
Average
0 (0%)
2 (22.22%)
0 (0%)
7.41%
0 (0%)
2 (22.22%)
0 (0%)
7.41%
PARTITIONING VIOLATIONS FOR SOFTWARE FAULTS TARGETING µC/OS-II WITHOUT PARTITIONING.
Table V
High priority
Altimeter (f)
Altimeter
CRC32
Workloads
Low priority
Injected
Activated
Software faults
Spatial violation
Temporal violation
Hamdist
CRC32 (f)
Hamdist (f)
10
10
2
8
9
2
Average
0 (0%)
2 (22.22%)
0 (0%)
7.41%
0 (0%)
2 (22.22%)
0 (0%)
7.41%
PARTITIONING VIOLATIONS FOR SOFTWARE FAULTS TARGETING µC/OS-II WITH SECERN.
Table VI
Workloads
High priority
Altimeter (f)
Altimeter
CRC32
Low priority
Injected
Activated
Hamdist
CRC32 (f)
Hamdist (f)
10
10
2
7
9
2
Average
Software faults
Spatial violation
1 (14.29%)
1 (11.11%)
0 (0%)
8.47%
Temporal violation
1 (14.29%)
0 (0%)
0 (0%)
4.76%
for µC/OS-II without partitioning and a slight advantage in
terms of temporal partitioning for the basic scheduler.
We can observe in Table VII that the faultload emulating
hardware faults is more demanding on the operating system
than the faultload emulating software faults. The partitioning
coverage is signiﬁcantly lower for hardware faults than it is
for software faults, on all three benchmark targets. Looking
at the average results, including both software and hardware
faults, we can make two observations. First, there is still
room for improving the mechanisms implemented in Secern,
which we discuss in the section that follows. Second, the
unintentional partitioning provided by two kernels without
partitioning mechanisms is fairly high, with more than
60% of the activated faults not causing any impact on the
fault-free partition. Several other studies have made similar
observations, in which unintentional redundancy or program-
level error masking cause errors to vanish [17], [18].
V. DISCUSSION
One of the key aspects of a good benchmark is that it
should provide the means to establish a fair comparison
between different solutions. Fairness can have distinct mean-
ings, but in general terms a benchmark should be able to
rank alternative systems in the same order as they would be
ranked in a real-world scenario. The measurements presented
in the preceding section provide some evidence supporting
the proposed benchmarking approach.
The results shown in Table VII indicate that µC/OS-
II with Secern provides the strongest partitioning of the
three benchmark targets. This is a reasonable result, since
it is the only target equipped with partitioning mechanisms.
The basic scheduler and µC/OS-II without partitioning are
ranked in a very similar way. This is also reasonable, given
that none of these targets provides memory protection or any
other partitioning mechanisms. Consequently, although the
targets differ signiﬁcantly in terms of size and functionality,
it should be expected that they would be similarly ranked in
a real-world scenario.
A. Guidance for Development Efforts
Another aspect of a good benchmark is that is should
provide designers with sufﬁcient information to guide the
development efforts, i.e., it should identify areas in which
the benchmark target requires attention. Although the Secern
extension to µC/OS-II provides a high partitioning coverage,
its mechanisms can be improved. We repeated the experi-
ments in which partitioning was violated, to obtain detailed
information on the causes of fault propagation.
Two problems were found in Secern. The ﬁrst problem
caused Secern to delete the wrong partition, i.e., the er-
ror was correctly detected but
incorrectly handled. This
issue was traced to the exception handler that deletes a
partition whenever an error is detected. The system call
to OSTaskDel expects as parameter the priority of the
task to be deleted (all tasks have distinct priority). However,
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:27:15 UTC from IEEE Xplore.  Restrictions apply. 
427OVERALL PARTITIONING COVERAGE FOR THE THREE BENCHMARK TARGETS.
Table VII
Benchmark target
Basic scheduler
µC/OS-II without partitioning
µC/OS-II with Secern
Hardware faults
PCs
PCt
Software faults
PCs
PCt
38.58% 39.09% 92.59% 92.59%
42.59% 29.17% 92.59% 92.59%
77.78% 77.78% 91.53% 95.24%
65.59% 65.84%
67.59% 60.88%
84.66% 86.51%
Average
PCs
PCt
Secern’s error handling routines were passing the id of the
task to be deleted as parameter OSTaskDel. Consequently,
Secern detected the error but reacted incorrectly and deleted
the fault-free partition.
We modiﬁed Secern’s exception handling routines to call
OSTaskDel using the priority of the task which is to be
deleted, and then repeated the experiments once more. After
the modiﬁcation, the error handling mechanisms worked
correctly in all cases except one. A careful analysis showed
that certain memory addresses are incorrectly mapped, and
writing a value to one such address would result in the value
being written simultaneously to other physical addresses.
The problem was traced to hardware initialization.
B. Benchmark Measures and Properties
In addition to the two high-level aspects discussed above
– the benchmark’s fairness and its ability to guide de-
velopment efforts – one must also consider the relevance
of the proposed measures and the benchmark’s properties
concerning representativeness, repeatability, portability, non-
intrusiveness, and simplicity of use.
The experimental evaluation led to an important obser-
vation that partitioning violations typically affect both the
temporal and the logical value of the output. The vast ma-
jority of the faults that propagated into the fault-free partition
caused it to produce incorrect results at the incorrect time, or
to crash and stop producing results. Therefore, the creation
of two measures (PCs and PCt) may be unnecessary, even
if intuitively correct. A simpler and perhaps equally valid
approach would simply measure partitioning coverage as a
whole, including both temporal and spatial isolation.
Another issue with the measures is the fact that the two
benchmark targets that have no partitioning mechanisms (the
basic scheduler and µC/OS-II without partitioning) have
reasonably high values for PCs as well as PCt. This indicates
that unintentional fault tolerance plays an important role
when measuring dependability. When a fault is activated in
one partition, it may manifest itself as a pure data error in the
value produced by that partition, which would not propagate
to any other parts of the system. Thus, the absolute value
of PCs and PCt can be considered acceptable, especially
since the relative value is signiﬁcantly lower than for Secern,
which was regarded a priori as the most robust.
Regarding the benchmark’s properties, one important con-
cern is the representativeness of faultloads and workloads.
The workloads were selected to match typical programs
executed in the embedded systems domain, and to include
integer as well as ﬂoating point operations and frequent
memory accesses. The faultload for software faults is based
on extensive ﬁeld studies, and the faultload for hardware
faults uses the commonly used single bit-ﬂip model. Thus,
the faultload representativeness can be considered to match
the state of the art.
VI. CONCLUSION
This paper presented the speciﬁcation and evaluation of
a dependability benchmark for evaluating the robustness of
partitioning mechanisms in real-time operating systems. The
benchmark includes faultloads based on hardware faults as
well as software faults, and several workloads to exercise
the benchmark targets. Two measures are deﬁned by the
benchmark – the coverage of the partitioning mechanisms
with respect to time and with respect to logical value.
To validate the benchmark, we applied it to three different
benchmark targets. The results show that the benchmark is
capable of ranking the three benchmark targets as expected
a priori: µC/OS-II with Secern ranked ﬁrst and the other
two targets are closely ranked in second and third. Given
that Secern is the only target
includes partitioning
mechanisms, this ranking is reasonable. Furthermore, the
partitioning measures calculated for the other two targets
are nearly identical, showing that the benchmark is capable
of producing a similar ranking for systems that lack par-
titioning, although the systems themselves are signiﬁcantly
different from one another.
that
Moreover, running the benchmark on µC/OS-II with
Secern allowed us to identify partitioning vulnerabilities
and to guide the development effort to correct them. Thus,
we can draw the conclusion that
the initial hypothesis
is substantiated by the experiments, in that the proposed
benchmark is able to correctly rank and compare alternative
systems and is additionally able to provide the necessary
information to guide the development efforts to improve
partitioning coverage.
REFERENCES
[1] K. Kanoun and L. Spainhower, Eds., Dependability Bench-
John Wiley & Sons, Inc.,
marking for Computer Systems.
2008.
[2] J. Rushby, “Partitioning in avionics architectures: Require-
ments, mechanisms, and assurance,” NASA Langley Research
Center, Tech. Rep. NASA/CR-1999-209347, Jun. 1999.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:27:15 UTC from IEEE Xplore.  Restrictions apply. 
428[3] B. Leiner, M. Schlager, R. Obermaisser, and B. Huber, “A
comparison of partitioning operating systems for integrated
systems,” in Computer Safety, Reliability, and Security, 26th
International Conference, SAFECOMP 2007, Nuremberg,
Germany, September 18-21, 2007, Proceedings, ser. Lecture
Notes in Computer Science, F. Saglietti and N. Oster, Eds.,
vol. 4680. Springer, 2007, pp. 342–355.
[4] P. Koopman, K. DeVale, and J. DeVale, Dependability Bench-
marking for Computer Systems.
John Wiley & Sons,
Inc., 2008, ch. Interface Robustness Testing: Experience and
Lessons Learned from the Ballista Project, pp. 201–226.
[5] A. Albinet, J. Arlat, and J.-C. Fabre, “Characterization of
the impact of faulty drivers on the robustness of the linux
kernel,” in Proceedings of the 2004 International Conference
on Dependable Systems and Networks (DSN 2004).
IEEE
Computer Society, Jun.-Jul. 2004, pp. 867–876.
[6] R. K. Iyer, Z. Kalbarczyk, and W. Gu, Dependability Bench-
marking for Computer Systems.
John Wiley & Sons,
Inc., 2008, ch. Benchmarking the Operating Systems Against
Faults Impacting Operating System Functions, pp. 311–339.
[7] D. Skarin, R. Barbosa, and J. Karlsson, “GOOFI-2: A tool
for experimental dependability assessment,” in Proceedings
of the 40th Annual IEEE/IFIP International Conference on
Dependable Systems and Networks (DSN 2010), Jun./Jul.
2010, pp. 557–562.
[8] R. Barbosa and J. Karlsson, “Experiences from verifying a
partitioning kernel using fault injection,” in Proceedings of the
12th European Workshop on Dependable Computing (EWDC
2009), May 2009.
[9] J. J. Labrosse, MicroC/OS-II: The Real-Time Kernel, 2nd ed.
CMP Books, 2002.
[10] D. Costa, R. Barbosa, R. Maia, and F. Moreira, Dependability
Benchmarking for Computer Systems.
John Wiley & Sons,
Inc., 2008, ch. DeBERT – Dependability Benchmarking for
Embedded Real-time Off-the-Shelf Components for Space
Applications, pp. 255–283.
[11] A. Kalakech, K. Kanoun, Y. Crouzet, and J. Arlat, “Bench-
marking the dependability of windows NT4, 2000 and XP,”
the 2004 International Conference on
in Proceedings of
Dependable Systems and Networks (DSN 2004).
IEEE
Computer Society, Jun.-Jul. 2004, pp. 681–686.
[12] J.-C. Ruiz, P. Yuste, P. Gil, and L. Lemus, “On benchmarking
the dependability of automotive engine control applications,”
the 2004 International Conference on
in Proceedings of
Dependable Systems and Networks (DSN 2004).
IEEE
Computer Society, Jun.-Jul. 2004, pp. 857–866.
[13] A. Chou, J. Yang, B. Chelf, S. Hallem, and D. Engler, “An
empirical study of operating system errors,” in Proceedings of
the 18th ACM Symposium on Operating Systems Principles
(SOSP-01), ser. ACM SIGOPS Operating Systems Review,
G. Ganger, Ed., vol. 35, 5. ACM Press, Oct. 21–24 2001,
pp. 73–88.
[14] J. Dur˜aes and H. Madeira, “Deﬁnition of software fault
emulation operators: A ﬁeld data study,” in Proceedings of
the 2003 International Conference on Dependable Systems
and Networks (DSN 2003), Jun. 2003, pp. 105–114.
[15] D. Skarin, R. Barbosa, and J. Karlsson, “Comparing and vali-
dating measurements of dependability attributes,” in Proceed-
ings of the 8th European Dependable Computing Conference
(EDCC 2010), pp. 3–12.
[16] R. Natella and D. Cotroneo, “Emulation of transient soft-
ware faults for dependability assessment: A case study,” in
Proceedings of
the 8th European Dependable Computing
Conference (EDCC 2010), Apr. 2010, pp. 23–32.
[17] H. Madeira and J. G. Silva, “Experimental evaluation of the
fail-silent behaviour in computers without error masking,” in
Proceedings of the 24th International Symposium on Fault-
Tolerant Computing (FTCS-24), Jun. 1994, pp. 350–359.
[18] P. Yuste, J.-C. Ruiz-Garcia, L. Lemus, and P. J. Gil, “Non-
intrusive software-implemented fault injection in embedded
systems,” in Proceedings of the First Latin-American Sympo-
sium on Dependable Computing (LADC 2003), ser. Lecture
Notes in Computer Science, Oct. 2003, vol. 2847, pp. 23–38.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:27:15 UTC from IEEE Xplore.  Restrictions apply. 
429