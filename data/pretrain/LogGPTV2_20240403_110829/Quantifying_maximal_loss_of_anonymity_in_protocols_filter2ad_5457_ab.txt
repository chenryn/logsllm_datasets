Definition 3.4. The channel capacity of a protocol
hA,O, φi with given knowledge R is deﬁned as
C = max I(h; O|R)
In the special case when R is a function of both h and O,
conditional loss of anonymity can somehow be simpliﬁed to
deﬁnitions 3.1 and 3.2.
3.3 Previous Result
It has been shown in previous works that matrices with
some sort of symmetry allow a nice characterization of chan-
nel capacity [4].
A matrix is symmetric if all rows are permutations of each
other and all columns are also permutations of each other.
A matrix is weakly symmetric if all rows are permutations
of each other and the column sums are equal.
A matrix is partially symmetric (or weakly partially sym-
metric) if some columns are constant (possibly with diﬀerent
values in each column) and the rest of the matrix is sym-
metric (or weakly symmetric).
The theorem below has been derived for weakly symmetric
matrices [4]:
Theorem 3.5. Chatzikokolakis, Palamidessi and
Panangaden Theorem: Given a protocol described by a
weakly symmetric matrix, its channel capacity is given by
C = ps log
|Os|
ps
− H(rs)
where Os is the set of symmetric output values, rs the sym-
metric part of a row of the matrix and ps is the sum of rs
This bound is achieved by choosing the uniform input dis-
tribution which is hence the channel distribution.
In Subsection 4.2 we will show that Theorem 3.5 can be
derived from Proposition 4.1.
4. CHANNEL CAPACITY USING LAGR-
ANGE MULTIPLIERS
We are interested in computing the channel capacity as
deﬁned in Deﬁnition 3.2 and Deﬁnition 3.4. In the following
subsections, the Lagrange method will be applied to derive
the channel capacity as the solution to the maximization
problems.
4.1 Channel Capacity for Anonymity Proto-
cols
Firstly, we are going to apply the Lagrange theorem on
anonymity loss of protocols hA,O, φi without given knowl-
edge R. We are hence interested in channel capacity:
where d = 1
ln 2 .
max I(h; O)
Formally, we want to maximize the function
f (h) = I(h; O)
The Theorem 4.1 and Proposition 4.2 can not only be ap-
plied in the protocols but also in general probabilistic chan-
nels. The following example applies them to the solution of
a classical channel capacity problem.
Convention:
Each anonymous event hi ∈ A is associated to an observa-
tion with a given probability µ(hi). To ease the exposition
we will use hi both for the event hi and for its probability
µ(hi), similarly for ok. The context will disambiguate what
meaning is intended. ˆOi will denote the sets of observations
associated to hi, i.e.
We will use the following properties of φk,i
X
X
i
X
X
k
Notice that
ok =
(
ˆOi = {os|φs,i 6= 0}
X
X
k
hi(
hiφk,i = ok;
X
hiφk,i) =
φk,i = 1
X
hi = 1
φk,i) =
k
i
i
k
i
Constraints present the settings of anonymous events A
in protocols. Here we use C as a set of constraints. Since
we are considering a probability distribution a constraint al-
ways assumed to be present is C0 ≡P hi = 1. In the real
world, this constraint is not always suﬃcient. For instance,
in the voting example, the people from some voting area
might have a higher probability to vote for Clinton than
others. Additional constraints can be introduced to spec-
ify these conditions, represented by relationship among his.
Formally, as presented in the last paragraph of Section 2, a
constraint (Ck)k∈K associated to hi can be decribed as
X
fi,khi − Fk = 0
k
As described in Section 2.2, the Lagrange method is used
to solve this optimization problems with constraints. Fol-
lowing Theorem 2.2, the theorem below solves the distribu-
tion of A which will achieve the channel capacity with the
constraints. All computations and proofs in this section are
omitted because of space limitation.
Theorem 4.1. The probabilities hi maximizing I(h; O)
subject to the family of constraint (Ck)k∈K are given by solv-
ing in hi the equations
X
os∈ ˆOi
φs,i ln(
) − 1 +
φs,i
os
λkfi,k = 0
X
k
and the constraints (Ck)k∈K .
Using this result the channel capacity in this probabilistic
Proposition 4.2. The channel capacity is given by
channel can be computed.
X
hi(1 −X
case of the single constraint P
k
i
simpliﬁed to
d(1 − λ0)
λkfi,k)d
where the hi’s are given by theorem 4.1. Moreover, in the
i hi = 1 the above can be
209Example: binary symmetric channel
Consider the classic binary symmetric channel (p. 186
[6]) where there are two values for the secret 0,1 and two
possible observations 0,1; the probability of the secret being
equal to the observation is 1− p while the probability of the
secret being diﬀerent from the value observed is p:
φ0,0 = φ1,1 = 1 − p
φ0,1 = φ1,0 = p
UsingP
i hiφk,i = ok we can get
o0 = (1 − p)h0 + ph1
o1 = ph0 + (1 − p)h1
Then using Theorem 4.1 we have the equation system:
−(1 − p) ln(
−p ln(
o0
p
o0
1 − p
) − p ln(
o1
p
o1
1 − p
) − 1 + λ0 = 0
) − 1 + λ0 = 0
) − (1 − p) ln(
By solving it we end up with
h0 = h1 =
1
2
λ0 = ln(
1
2
) − p ln(p) − (1 − p) ln(1 − p) + 1
The channel capacity is then
d(1 − λ0) = 1 − H(p)
which coincide with the classical results on binary symmet-
ric channels [6].
Further, when there is some given knowledge R, then the
channel capacity becomes:
max I(h; O|R)
To solve it, we start by extending φk,i to φk,i,j to describe
the conditional probability of observing ok given hi and Rj.
Formally,
(hi, Rj, ok) = (hi, Rj)φk,i,j
From the equation above we have
X
k
φk,i,j = 1
X
Notice thatX
i,j
(hi, Rj)φk,i,j = ok;
X
i,j
X
X
X
i,j
k
k
ok =
=
=
(hi, Rj)
i,j
= 1
Then we use the Lagrange method to ﬁgure out the maxi-
mum value for I(h; O|R) with the set of constraints C. Here
a constraint (Ck)k∈K associated to (hi, Rj) can be formally
expressed as X
fi,j,k(hi, Rj) − Fk = 0
k
The Lagrange function becomes:
L((hi, Rj)) = I(h; O|R) + λk(
X
k
fi,j,k(hi, Rj) − Fk)
The diﬀerence to the previous computation is that when
we do derivations, we are doing them on the pair (hi, Rj)
instead of the single variable hi. This is because O is as-
sociated with h and R. The concluding theorem is shown
below.
Theorem 4.3. The probabilities (hi, Rj) resulting in max-
imum value of I(h; O|R) subject to the family of constraint
(Ck)k∈K are given by solving in (hi, Rj) the equations
φs,i,j ln(
φs,i,j
(os|Rj)
) − 1 +
λkfi,j,k = 0
X
os∈ ˆOi,j
X
k
Then using the probabilities (hi, Rj) we can work out the
channel capacity.
Proposition 4.4. The channel capacity is given by
X
(hi, Rj)(1 −X
λkfi,j,k)d
i,j,k
k
where (hi, Rj)’s are given by theorem 4.3.
4.2 Deriving Theorem 3.5
In this section we are going to show that Theorem 3.5
from [4] is a special case of our Theorem 4.1.
By Proposition 4.1 the probabilities are given by solving
hi in the equationsX
φs,i ln(
os∈ ˆOi
X
k
) − 1 +
φs,i
os
λkfi,k = 0
(1)
In our setting, a weakly symmetric matrix means that
there exists a subset of indices K such that given any k ∈ K,
for all i, j, φk,i = φk,j. This set is denoted by On in [4]. For
all other indices s 6∈ K we have for all i, j, (φs,i)s6∈K is a
permutation (with no 0 element) of (φs,j)s6∈K : these are the
“symmetric output value”. To use the same notations as [4],
s6∈K φs,i. Also the
above conditions imply that for all i, j ˆOi = ˆOj. We denote
this (unique) set as ˆO.
we write rs for (φs,i)s6∈K and ps for P
straints apart fromP
As in [4], assuming that there are not additional con-
i hi = 1 then equation (1) becomes
φs,i ln(
φs,i
os
) − 1 + λ0 = 0
(2)
X
s ∈ K ⇒ φs,i = (os|hi) = os
It is easy to show thatX
φs,i ln(
) − 1 + λ0
φs,i
os
os∈ ˆO
= −X
s6∈K
φs,i ln(os) − ln(2)H(rs) − 1 + λ0
(
(hi, Rj)φk,i,j)
X
(hi, Rj)(
φk,i,j)
os∈ ˆO
k
Using the fact that
210where ln(2) converts log in the entropy formula into the nat-
ural logarithm ln. We hence derive the system of equations
φs,i ln(os) = ln(2)H(rs) + 1 − λ0)i∈N
X
(
s6∈K
Noticing that the right-hand-side is a constant and that
for all i, j, (φs,i)s6∈K is a permutation of (φs,j)s6∈K we deduce
that
and since ps =P
∀i, j 6∈ K, oi = oj
s6∈K φs,j we derive
∀i 6∈ K, oi =
, k = |{i 6∈ K}|
ps
k
We have hence the equation
X
s6∈K
i.e.
h
100
(h1)
010
(h2)
001
(h3)
000
(Master)
Coin
O
P
p3 + (1 − p)3
000, 111 NYY
001, 110 YYN p2(1 − p) + (1 − p)2p
010, 101 NNN p2(1 − p) + (1 − p)2p
011, 100 YNY p2(1 − p) + (1 − p)2p
000, 111 NYY p2(1 − p) + (1 − p)2p
001, 110 YYN p2(1 − p) + (1 − p)2p
010, 101 NNN p2(1 − p) + (1 − p)2p
p3 + (1 − p)3
011, 100 YNY
000, 111 NYY p2(1 − p) + (1 − p)2p
p3 + (1 − p)3
001, 110 YYN
010, 101 NNN p2(1 − p) + (1 − p)2p
011, 100 YNY p2(1 − p) + (1 − p)2p
p3 + (1 − p)3
000, 111 YYY
001, 110 NYN p2(1 − p) + (1 − p)2p
010, 101 YNN p2(1 − p) + (1 − p)2p
011, 100 NNY p2(1 − p) + (1 − p)2p
φs,i ln(
k
ps
) = ln(2)H(rs) + 1 − λ0
Table 2: The Dining Cryptographers protocol
ps ln(
) − ln(2)H(rs) = 1 − λ0
(3)
Using Proposition 4.2, replacing λ0 in d(1 − λ0) with the
k
ps
left hand side of equation (3) we ﬁnally arrive at
1
ln(2)
(ps ln(
k
ps
) − ln(2)H(rs)) = ps log(
) − H(rs)
k
ps
which is Theorem 3.5.
However,
if we consider protocols which can be repre-
sented by weakly symmetric matrices but the inputs of the
i hi = 1 then
protocol has some constraints in addition toP
φs,i ln(os) = ln(2)H(rs) + 1 −X