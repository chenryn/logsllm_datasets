strict input replication scheme uses the LVQs to serve as 
the  shared  buffers.  On  a  load  operation,  the  master  core 
pushes  the  memory  data  into  its  LVQ.  Then  the  data  is 
transferred to the slave’s LVQ via a dedicated channel. In 
this  way  the  slave  core  directly  obtains  identical  load 
values  from  its  LVQ.  However,  such  a  mechanism 
requires  that  the  master  and  the  slave  cores  are  directly 
connected,  which  limits  the  flexibility  of  the  system.  To 
take advantage of such strict input replication scheme and 
build scalable binding framework, we employ the private 
cache to serve as the shared buffer. Based on this idea, we 
propose  the  objective  of  master-slave  private  cache 
consistency.  In  this  section,  we  describe  the  consistency 
objective and discuss the related key issues. 
The Objective of Master-Slave Private Cache Consistency 
requires 
Master-slave  memory  consistency 
the 
master-slave  pair  to  obtain  identical  memory  values  for 
identical memory instructions. Given that private cache is 
the  firsthand  structure  that  buffers  data  blocks  for  the 
pipeline modules, we employ the private cache to maintain 
the  master-slave  memory  consistency.  Thereafter,  we 
propose  the  objective  of  master-slave  private  cache 
consistency,  i.e.,  the  private  caches  serve  as  the  shared 
buffer to provide identical memory values for the core pair. 
To  achieve  the  objective,  several  key  issues  should  be 
carefully considered. 
First, once the private caches of the master-slave pair 
encounter  any  misses,  how  should  the  caches  get  data 
blocks? In this work, we form a consumer-consumer data 
access pattern  for the pair, and call  for the data provider 
(the producer) to simultaneously bring forward data blocks  
Figure 2.   The TDB architecture 
to the pair. In the TDB execution model, the master core is 
allowed to issue memory requests once it encounters any 
private  cache  misses,  while  the  slave  core  is  forbidden 
from  accessing  the  global  memory.  Once  a  cache  miss 
occurs,  the  slave  core  stalls  and  passively  waits  for  the 
data sent by the producer. From this perspective, the slave 
core is transparent to the global memory. With the help of 
the  extended  cache  coherence  protocol,  the  consumer-
consumer  pair  is  guaranteed  to  get  identical  data  blocks 
from the producer. 
Second,  after  obtaining  identical  data  blocks,  how 
should  the  master-slave  pair  handle  the  inconsistency 
induced  by  wrong  path  memory  references?  Note  that 
conventional  out-of-order  pipeline  design  makes 
the 
execution  uncertain.  Branch  prediction  results  of  the 
master-slave  pair  cannot  be  always  the  same.  Therefore, 
wrong  path  memory  references  may  cause  the  private 
caches of the master and the slave core to accept and evict 
different  data  blocks.  In  this  paper,  two  techniques  are 
proposed  to  address  the  issue.  Firstly,  we  adopt  a  victim 
buffer  assisted  conservative  private  cache  ingress  rule  to 
accept  identical  data  blocks  in  the  private  caches.  The 
victim buffer structure is utilized to filter data blocks from 
wrong  paths.  Therefore,  only  data  blocks  brought  in  by 
correct path instructions are accepted in the private caches. 
Besides, based on the in-order retirement sequence of the 
pipelines,  we  adopt  an  update-after-retirement-assisted 
LRU  (uar-LRU)  cache  replacement  policy.  The  master-
slave  core  pair  can  evict  identical  least  recently  used 
blocks according to the policy. 
Finally, in  terms  of parallel workloads,  how  can  the 
private  caches  prevent  external  write  operations  from 
violating  the  master-slave  consistency?  In  this  paper, 
based  on  the transparent characteristic of TDB  execution 
model, we propose a transparent input coherence strategy 
to avoid the intervention. In TDB, we break the atomicity 
of  the  master  and  the  slave  core’s  data  access  behavior. 
External write request succeeding the master’s data access 
needn’t  wait  for  the  slave  to  finish  the  corresponding 
operation. Instead, based on the transparent feature of the 
slave core, we parallelize the external write request and the 
internal data access of the slave core. In other words, when 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:46:13 UTC from IEEE Xplore.  Restrictions apply. 
293an  external  write  request  is  sent  to  the  master  core,  it 
invalidates  its  data  copy  and  forwards  the  invalidation 
message  to  its  slave.  After  receiving  the  acknowledge 
message  from  the  slave  core,  the  master  responds  the 
requestor. Then the external write operation is performed. 
Therefore,  the  atomicity  of  the  core  pair’s  data  access 
behavior is removed. Such a transparent strategy not only 
can help avoid the write violation but also can reduce the 
performance degradation caused by master-slave memory 
consistency maintenance to the maximum limit. 
The  above  discusses  are  all  directly  related  to  the 
memory hierarchy. In this paper, we start from a directory-
based cache coherence protocol and extend it to construct 
our  fault-tolerant  protocol.  The  implementation  of  the 
TDB  proposal 
the  cache 
hierarchy as well as the cache coherence protocol. Next we 
present our TDB execution model. 
is  realized  by  modifying 
III.  TDB EXECUTION MODEL 
Firstly  we  take  the  8-core  CMP  system  as  an 
example to describe our TDB architecture. As depicted in 
Figure  2,  the  system  consists  of  four  master-slave  pairs 
(A-A’,  B-B’,  C-C’  and  D-D’)  which  are  dynamically 
coupled via an on-chip network. Each core has its private 
L1  cache.  Besides,  all  the  masters  share  the  L2  cache, 
which is responsible for handling L1 cache miss requests 
and  coordinating  on-chip  coherence.  In  contrast,  all  the 
slaves are only permitted to access their private caches so 
that they are transparent to the global memory. The victim 
buffer is located between the private cache and the global 
memory  to  serve  as  a  filter.  All  the  speculative  blocks 
brought  in  through  the  consumer-consumer  data  access 
pattern are preserved in the buffer. Only those data blocks 
referenced  by  instructions  from  the  correct  path  are 
accepted in the private caches. In this way the wrong path 
data  blocks  are  filtered  and  the  master-slave  private 
caches  are  guaranteed  to  hold  the  same  memory  blocks 
for their functional modules.  
this  way  we  reduce 
Figure  2  also  shows  the  Sphere  of  Consistency.  In 
the  previous  relaxed  input  replication  schemes,  the  SoC 
includes  the  whole  memory  hierarchy.  However,  in  our 
TDB framework, the SoC includes only the private caches. 
In 
the  master-slave  memory 
consistency maintenance to the scale of the private caches. 
In  the  following  sections  we  explain  the  key  techniques 
involved in our TDB execution model.  
A.  Consumer-Consumer Data Access Pattern 
Recently, various sharing patterns such as migratory 
sharing, producer-consumer sharing, and multiple readers 
(writers)  sharing  are  exploited  to  reduce  the  cache  miss 
latency in directory-based cache coherence protocol [14-
16].  Chen  et  al.  proposes  an  adaptive  cache  coherence 
protocol  to  optimize  the  producer-consumer  sharing 
pattern [15]. The consumer is defined to be the stable  
Figure 3.   Consumer-consumer data access pattern diagram: (a) the 
shared L2 cache acts as the producer; (b) another on-chip L1 cache acts 
as the producer. 
reader of data blocks that are written by another specified 
node called producer. Based on the observation of stable 
producer-consumer  access  patterns,  data  blocks  are 
speculatively  sent  from  the  producer  to  the  consumer. 
Therefore  the  cache  miss  latency  of  the  consumer  is 
reduced. 
With  respect 
to  our  fault-tolerant  design,  we 
implement error checking through redundantly executing 
the original thread. Without considering of the wrong path 
effects  (The  detailed  discussion  on  handling  the  wrong 
path  effects  is  given  in  subsection  B),  the  master-slave 
pair exhibits the same data access behavior. Therefore, we 
consider  the  master-slave  pair  as  a  stable  consumer-
consumer node pair, and consider the data provider as the 
producer.  Based  on  this  formalization,  we  propose  a 
consumer-consumer  data  access  pattern  to  characterize 
the master-slave memory access mode. When the master 
encounters  an  L1  cache  miss,  it  requests  the  shared  L2 
cache for the data. The L2 cache controller looks up the 
binding  information  table  to  identify  the  requestor’s 
binding  partner.  (The  detail  of  the  binding  information 
table is discussed in Section IV.) Thereafter, the producer 
simultaneously  broadcasts  the  data  to  the  master-slave 
pair. In contrast, when the slave encounters an L1 cache 
miss, it just stalls the cache miss related instructions and 
passively  waits  for  the  data.  As  indicated  by  the 
consumer-consumer  data  access  pattern,  the  same  L1 
cache  miss  sooner  or  later  happens  in  its  corresponding 
master core. Since the slave does not issue any cache miss 
requests,  we  define 
the  master-slave  binding  as 
transparent binding. 
The  benefit  of  the  consumer-consumer  data  access 
pattern comes from three aspects. First, it helps to reduce 
the  network  traffic  as  the  slave  needn’t  request  any  L1 
cache  miss  messages. Meanwhile, since all of the slaves 
are  transparent  to  the  global  memory,  the  complexity  of 
maintaining  the  master-slave  memory  consistency  is 
reduced.  What’s  more,  the  access  pattern  also  helps  to 
maintain the master-slave execution slack. Once the slave 
lags long behind the master, the prefetching effect blows 
up. The execution of the slave core is thus accelerated and 
the slack is gradually knocked down. On the other hand, if 
the slack is too small, the slave core  must  stall  until the 
data arrives, which in turn increases the slack.  
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:46:13 UTC from IEEE Xplore.  Restrictions apply. 
294Aggressive  private  cache  ingress  rule:  The  private 
cache accepts in data blocks brought in by all speculative 
memory instructions. 
Conservative private cache ingress rule: The private 
cache  allocates  entries  and  accepts  in  data  blocks 
referenced only by instructions  from  the correct path. In 
this  way,  wrong  path  memory  references  won’t  pollute 
the private caches. 
ingress  rule,  an 
The out-of-order pipeline design allows instructions 
to  be  speculatively  issued  and  executed  before  they  are 
confirmed  to  be  in  the  correct  path.  When  using  the 
conventional  aggressive 
interesting 
phenomenon  reported  in  several  previous  proposals  [32-
34] shows that, speculative instructions from wrong path 
sometimes  accelerate  the  correct  path  execution  because 
the  prefetched  data  blocks  are  useful.  However,  when 
considering  the  fault-tolerant  execution,  the  maintenance 
of  the  master-slave  private  cache  consistency  becomes 
very  complicated.  Since  the  out-of-order  pipeline  design 
makes 
the 
aggressive ingress rule will accept different blocks for the 
master-slave private caches, which as a result violates the 
private cache consistency.  
the  master-slave  execution  divergent, 
Actually,  no  matter  how  the  pipelines  execute  the 
speculative  instructions,  an  invariant  exists  between  the 
master-slave  pair:  the  in-order  instruction  retirement 
sequence.  If  only  the  data  blocks  from  correct  paths  are 
allowed to enter the private caches, the wrong path effect 
will  be  eliminated.  Based  on  this  argument,  we  turn  to 
adopt the conservative private cache ingress rule. Figure 4 
illustrates  the  conservative  ingress  rule  as  well  as  the 
proposed master-slave data access design. The master and 
the slave cores are coupled to form a reliable logical pair. 
The  victim  buffer  structure  is  located  between  the  L1 
cache and the  L2 cache. It is utilized to filter the  wrong 
path data blocks. 
When the master encounters an L1 cache miss, it is 
permitted  to  generate  a  global  request  to  the  shared  L2 
cache.  On  the  other  hand,  if  the  slave  encounters  any 
misses  in  its  private  cache,  it  inquires  the  victim  buffer 
and  stalls  the  speculative  memory  instruction  until  the 
data block arrives. According to the consumer-consumer 
data  access  pattern,  the  producer  provides  data  for  the 
master-slave  pair  (the  consumer-consumer).  When  the 
response  message  arrives,  the  incoming  speculative  data 
block is firstly loaded into the victim buffer. Finally, after 
the instructions are retired, the pipeline hints the buffer of 
the retirement information. The informed data blocks are 
then  replaced  from  the  buffers  to  the  private  caches. 
Otherwise, the data blocks are confirmed to be brought in 
by wrong path memory instructions. The data blocks are 
evicted in case of: 
• 
• 
the  speculative  block  is  not  referenced  for  a 
specified time period, or 
the victim buffer is already full when a new entry 
needs to be allocated. 
Figure 4.   Victim buffer assisted conservative private cache ingress rule 
In  order  to  implement  above  described  transparent 
binding,  we  adapt  the  cache  coherence  protocol  to  meet 
the consumer-consumer data access pattern requirements. 
Figure  3  illustrates  the  consumer-consumer  data  access 
pattern  diagram.  Master  A  requests  the  global  memory 
when  it  encounters  L1  cache  misses.  The  data  provider 
(the  producer)  responds  to  the  consumer-consumer  pair 
with the help of our cache coherence protocol. There are 
two scenarios: in a), L2 cache owns the data and acts as 
the producer; while in b), the L2 cache state is invalid (or 
not  present)  but  another  on-chip  L1  cache  in  master  B 
owns the block. Therefore, the request is forwarded to B, 
who acts as the producer and responds to the pair. 
B.  Maintain Consistency under Out-of-Order Execution 
As described before, the slave passively obtains data 
blocks with the help of its coupled master. However, such 
a  consumer-consumer  data  access  pattern  cannot  always 
guarantee the pair gets identical data blocks. The reason is 
that, the consistency  may be  violated by  the  wrong path 
instructions under out-of-order execution. The two causes 
are as follows. 
First, although the master-slave pair is configured to 
run  the  same  workload,  the  execution  details  may  vary 
and the execution path may be sometimes different. If the 
master  and  the  slave  core  execute  instructions  from 
different  wrong  path,  different  data  blocks  will  be 
referenced.  That  is  to  say,  data  accesses  by  instructions 
from wrong paths cause the private caches of the pair to 
accept  in  different  data  blocks.  Second,  the  traditional 
most-recently-used (MRU) timestamp of a cache block is 
updated  once  the  corresponding  data  access  is  finished. 
However,  memory  instructions  from  wrong  paths  may 
cause the cache line be referenced differently. Therefore, 
in  case  of  L1  cache  replacement,  the  LRU  replacement 
policy  may  select  different  data  blocks  to  evict  and  the 
master-slave private cache consistency will be violated. 
In order to avoid the above consistency intervention, 
we  insert  victim  buffers  between  private  caches  and  the 
global memory, and extend the cache coherence protocol 
to guarantee the consistency. 
1)  Victim Buffer Assisted Conservative Private Cache 
Ingress Rule 
For the sake of clarity, we firstly classify two private 
cache ingress rules as follows. 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:46:13 UTC from IEEE Xplore.  Restrictions apply. 
295Figure 5.   Master-Slave memory consistency window in DCC [12]: (a) 
Write operation of Master B (ST3) modifies the memory value to be 
accessed by A’, which causes the memory inconsistency (LD1-LD1’); 
(b)DCC delays the write request of Master B (ST3) until the slave A’ 
finishes the data access (LD1’). 
Figure 6.   Memory consistency maintenance in TDB: ① an external 
exclusive request ST3 of the master B; ② the master A invalidates its 
copy and sends Inv+Seq message to the slave A’; ③ the slave A’ sends 
Ack to master A; ④ the master A sends Ack to the Master B; ⑤ the 
master A’ accesses the data D and then D is checked to be invalidated. 
ingress 
2)  uar-LRU Cache Replacement Policy 
The  aforementioned  conservative 
rule 
guarantees the master-slave pair gets identical data blocks 
via  the  extended  cache  coherence  protocol.  In  this 
subsection,  we  further  discuss  how  to  evict  identical 
blocks  from  the  private  caches.  As  mentioned  before, 
different  data  blocks  within  the  private  cache  may  be 
referenced by speculative memory instructions. Therefore, 
the  MRU  timestamps  of  the  cache  blocks  may  diverge 
between 