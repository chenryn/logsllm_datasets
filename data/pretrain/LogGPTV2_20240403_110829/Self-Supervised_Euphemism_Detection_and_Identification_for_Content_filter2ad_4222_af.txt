best across three datasets. This is why we ultimately selected
it and reported results using it in Section V. Yet, as shown
in Figure 4, other classiï¬ers have satisfactory performance as
well, and reach a testing accuracy ranging from 0.86 to 0.90.
2) Fine-Grained Classiï¬ers: In the euphemism identiï¬cation
framework, we use a multi-class classiï¬er to identify to which
target keyword each euphemism refers. Again, we experimented
with the same set of classiï¬ers as above. Interestingly, we
ï¬nd that, for ï¬ne-grained classiï¬cation, all classiï¬ers have
highly similar results. One possible reason is that each class
has relatively small number of training instances (ranging
from a few hundreds to 100k), which limits the discriminative
power of advanced algorithms. For the drug dataset (33 target
keywords), the training accuracy is about 55% and the testing
accuracy is about 24%. This shows the feasibility of the
task since the random guess accuracy would be 3.3%. Given
the similar performance across classiï¬ers, we recommend
Logistic Regression on raw Text (LRT) for better computational
eï¬ƒciency.
For both coarse classiï¬ers and ï¬ne-grained classiï¬ers, we
leave more advanced classiï¬cation algorithms for future work.
B. Parameter Analysis
Figure 5. Sensitivity of ğ‘¡.
In the euphemism detection step (Section IV-A), we set a
masked language model threshold ğ‘¡ to ï¬lter out the generic
masked sentences. In the ranked list of replacements for
the mask token, if any target keyword appears in the top-ğ‘¡
replacement candidates for the masked sentence, we consider
the masked sentence a valid context instance. Otherwise, we
consider the masked sentence generic and ï¬lter it out. Figure
5 shows how the results change with the threshold ğ‘¡ and we
observe a slight decrease when the threshold ğ‘¡ is larger than 5.
Therefore, ğ‘¡ = 5 appears to be an optimal parameter choice.
C. Limitations
While our approach for euphemism detection and identiï¬ca-
tion appears highly promising, it does have some limitations.
Text-only moderation: Our approach only works with text,
and our techniques are not easily generalizable to other media.
Social media posts frequently include images, video, and
audio, which can be even more challenging (and even more
traumatic) to moderate by hand [3]â€“[5]. However, text is
frequently associated with these other media, e.g., in the form of
comments, and thus detecting euphemism use might indirectly
provide clues to content moderators dealing with diï¬€erent
media.
Other contexts: Our approach performs well on corpora
discussing drugs, weapons, and sexuality. In preliminary
experiments with a corpus of hate speech it did not perform
nearly as well, producing many false matches when tasked with
identifying racial slurs. We believe this is because euphemisms
related to drugs, weapons, and sex typically have speciï¬c
meanings; e.g., â€œpotâ€ always refers to marÄ³uana, not some
other drugs. Racial slurs, on the other hand, are (in this
corpus) used imprecisely, and interchangeably with generic
swearwords, which seems to confuse euphemism detection. We
do not know yet whether this is a fundamental limitation. Even
if it is, though, there are many contexts where euphemisms
have speciï¬c meanings and our approach should be eï¬€ective,
particularly forums selling illicit goods.
Robustness to adversarial evasion: In our evaluation, we
have relied on a priori non-adversarial datasets, that were
gleaned from public, online forums. In other words, people
were using euphemisms, but we do not know whether they were
using them speciï¬cally to evade content moderation. Perhaps
these euphemisms are, for them, simply the ordinary names
of certain things within the circle where they were discussing
them. (Someone who consistently spoke of â€œmarÄ³uanaâ€ instead
of â€œpotâ€ on a forum dedicated to discussing drug experiences
might well be suspected of being an undercover cop.)
Because our algorithms rely on sentence-level context to
detect and identify euphemisms, an adversary would need to
change that context to escape detection. Such changes may
also render the text unintelligible to its intended audience.
Therefore, we expect our techniques to be moderately resilient
to adversarial evasion. However, we cannot test our expectations
at the moment, since we do not have a dataset where people
were purposely using euphemisms only to escape detection.
Usability for content moderators: While our approach shows
encouraging performance in lab tests, we have not yet evaluated
whether it is good enough to be helpful to content moderators
in practice. That evaluation would require a user study of
professional content moderators. This is out of scope for the
0.30.330.3602468101214Precision @ 100tpresent paper, which focuses on the technical underpinnings
of euphemism detection and identiï¬cation. We are interested
in investigating usability as a follow-up study.
As a preliminary experiment, we investigated the Perspective
API13, Googleâ€™s automated toxicity detector to identify the
likelihood of a sentence being considered toxic by a reader.
Perspective is reportedly used today by human moderators
to ï¬lter or prioritize comments that may require moderation.
We take sentences from our datasets that contain the target
keywords (e.g., â€œmarÄ³uanaâ€, â€œheroinâ€) and for each such
sentence, we evaluate the toxicity score of the sentence (a)
with the target keyword, and (b) by replacing the target
keyword with one of its identiï¬ed euphemisms (e.g., â€œweedâ€,
â€œdopeâ€). By comparing the toxicity scores, we can estimate the
likelihood that a human moderator who is using Perspective
API would be shown each version of the sentence. Table
VIII shows the average toxicity scores when comparing 1000
randomly chosen original sentences with their euphemistic
replacements for the drug, weapon, and sexuality categories.
We observe that sentences with target keywords have higher
(or at least comparable) toxicity scores compared to sentences
with euphemisms, which suggests that euphemisms could help
escape content moderation based on the Perspective API. In
turn, detecting and identifying euphemisms could help defeat
such evasive techniques.
Average toxicity socres by Perspective API. (A): original sentences; (B):
sentences with their euphemistic replacements.
Table VIII
Drug Weapon
0.235
0.232
A 0.209
B
0.178
Sexuality
0.612
0.522
associated with them, and 3) the research is not correlating
diï¬€erent public sources of data to infer private data.14 All of
these conditions apply to the present study.
VII. Conclusion
We have worked on the problem of content moderation
by detecting and identifying euphemisms. By utilizing the
contextual information explicitly, we not only obtain new state-
of-the-art detection results, but also discover new euphemisms
that are not even on the ground truth list. For euphemism
identiï¬cation, we, for the ï¬rst time, prove the feasibility of the
task and achieve it on a raw text corpus alone, without relying
on any additional resources or supervision.
Reproducibility
Our code and pre-trained models are available on GitHub:
https://github.com/WanzhengZhu/Euphemism.
Acknowledgments
We thank our shepherd, Ben Zhao, and the anonymous
reviewers for comments on earlier drafts that signiï¬cantly
helped improve this manuscript; Kyle Soska for providing
us with the Reddit data and Sadia Afroz for the weapons
data; Xiaojing Liao and Haoran Lu for availing and discussing
the Cantreader implementation with us; and Xin Huang for
insightful discussions. This research was partially supported by
the National Science Foundation, awards CNS-1720268 and
CNS-1814817.
References
[1] D. Blackie, â€œAOL censors British townâ€™s name!â€ Computer Underground
Digest, vol. 8, April 1996, as abstracted in RISKS Digest 18.07.
[Online]. Available: http://catless.ncl.ac.uk/Risks/18.07.html#subj3
[2] P. M. Barrett, â€œWho moderates the social media giants?â€ Center for
Business, 2020.
D. Ethics
This study relies extensively on user-generated content. We
consider here the ethical implications of this work. The data we
use in this paper were posted on publicly accessible websites,
and do not contain any personal identiï¬able information (i.e.,
no real names, email addresses, IP addresses, etc.). Further, they
are from 2018 or earlier, which greatly reduces any sensitive
nature they might have. For instance, given their age and the
absence of personal identiï¬able information, the data present
very little utility in helping reduce imminent risks to people.
From a regulatory standpoint, in the context of earlier work
on online anonymous marketplaces [43], [94], Carnegie Mellon
Universityâ€™s Institutional Review Board (IRB) gave us very
clear feedback on what is considered human research and
thus subject to IRB review. Analyses relying on user-generated
content do not constitute human-subject research, and are thus
not the purview of the IRB, as long as 1) the data analyzed
are posted on public fora and were not the result of direct
interaction from the researchers with the people posting, 2)
no private identiï¬ers or personal identiï¬able information are
13https://www.perspectiveapi.com/
[3] C. Newton, â€œThe terror queue,â€ Dec. 2019, https://www.theverge.com/2019/
12/16/21021005/google-youtube-moderators-ptsd-accenture-violent-disturbing-
content-interviews-video.
[4] â€”â€”, â€œThe trauma ï¬‚oor: The secret lives of Facebook moderators
in America,â€ Feb. 2019, https://www.theverge.com/2019/2/25/18229714/
cognizant-facebook-content-moderator-interviews-trauma-working-conditions-
arizona.
[5] Cambridge Consultants, â€œUse of AI in online content moderation,â€
Ofcom Report, 2019, https://www.ofcom.org.uk/__data/assets/pdf_ï¬le/0028/
157249/cambridge-consultants-ai-content-moderation.pdf.
[6] G. Durrett, J. K. Kummerfeld, T. Berg-Kirkpatrick, R. Portnoï¬€, S. Afroz,
D. McCoy, K. Levchenko, and V. Paxson, â€œIdentifying products in online
cybercrime marketplaces: A dataset for ï¬ne-grained domain adaptation,â€
in Proceedings of Empirical Methods in Natural Language Processing
(EMNLP), 2017, pp. 2598â€“2607.
[7] R. S. Portnoï¬€, S. Afroz, G. Durrett, J. K. Kummerfeld, T. Berg-
Kirkpatrick, D. McCoy, K. Levchenko, and V. Paxson, â€œTools for
automated analysis of cybercriminal markets,â€ in Proceedings of
International Conference on World Wide Web (WWW), 2017, pp. 657â€“
666.
[8] B. Felbo, A. Mislove, A. SÃ¸gaard, I. Rahwan, and S. Lehmann, â€œUsing
millions of emoji occurrences to learn any-domain representations for
detecting sentiment, emotion and sarcasm,â€ in Proceedings of Empirical
Methods in Natural Language Processing (EMNLP), 2017, pp. 1615â€“
1625.
14This position is in line with Title 45 of the Code of Federal Regulations,
Part 46 (45 CFR 46), which deï¬nes human research.
[9] K. Yuan, H. Lu, X. Liao, and X. Wang, â€œReading thievesâ€™ cant: auto-
matically identifying and understanding dark jargons from cybercrime
marketplaces,â€ in Proceedings of 27th USENIX Security Symposium,
2018, pp. 1027â€“1041.
[10] T. Hada, Y. Sei, Y. Tahara, and A. Ohsuga, â€œCodewords detection in
microblogs focusing on diï¬€erences in word use between two corpora,â€
in Proceedings of International Conference on Computing, Electronics
& Communications Engineering (iCCECE).
IEEE, 2020, pp. 103â€“108.
[11] R. Magu and J. Luo, â€œDetermining code words in euphemistic hate
speech using word embedding networks,â€ in Proceedings of the 2nd
Workshop on Abusive Language Online (ALW2), 2018, pp. 93â€“100.
[12] K. Zhao, Y. Zhang, C. Xing, W. Li, and H. Chen, â€œChinese underground
market jargon analysis based on unsupervised learning,â€ in Proceedings
of IEEE Conference on Intelligence and Security Informatics (ISI).
IEEE, 2016, pp. 97â€“102.
[13] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, â€œDis-
tributed representations of words and phrases and their compositionality,â€
in Proceedings of Advances in Neural Information Processing Systems
(NeurIPS), 2013, pp. 3111â€“3119.
[14] T. Mikolov, K. Chen, G. Corrado, and J. Dean, â€œEï¬ƒcient estimation of
word representations in vector space,â€ arXiv preprint arXiv:1301.3781,
2013.
[15] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, â€œBERT: Pre-training
of deep bidirectional transformers for language understanding,â€ in
Proceedings of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies (NAACL-
HLT), 2019, pp. 4171â€“4186.
[16] C. Donahue, M. Lee, and P. Liang, â€œEnabling language models to
ï¬ll in the blanks,â€ in Proceedings of Association for Computational
Linguistics (ACL), 2020.
[17] C. Felt and E. Riloï¬€, â€œRecognizing euphemisms and dysphemisms
using sentiment analysis,â€ in Proceedings of the Second Workshop on
Figurative Language Processing, 2020, pp. 136â€“145.
[18] N. Leontiadis, T. Moore, and N. Christin, â€œMeasuring and analyzing
search-redirection attacks in the illicit online prescription drug trade.â€
in USENIX Security Symposium, vol. 11, 2011.
[19] D. McCoy, A. Pitsillidis, J. Grant, N. Weaver, C. Kreibich, B. Krebs,
G. Voelker, S. Savage, and K. Levchenko, â€œPharmaleaks: Understanding
the business of online pharmaceutical aï¬ƒliate programs,â€ in Part of the
21st USENIX Security Symposium, 2012, pp. 1â€“16.
[20] J. Huang, Z. Li, X. Xiao, Z. Wu, K. Lu, X. Zhang, and G. Jiang,
â€œSUPOR: Precise and scalable sensitive user input detection for android
apps,â€ in 24th USENIX Security Symposium, 2015, pp. 977â€“992.
[21] Y. Nan, M. Yang, Z. Yang, S. Zhou, G. Gu, and X. Wang, â€œUipicker:
User-input privacy identiï¬cation in mobile applications,â€ in Proceedings
of 24th USENIX Security Symposium, 2015, pp. 993â€“1008.
[22] K. Thomas, D. McCoy, C. Grier, A. Kolcz, and V. Paxson, â€œTraï¬ƒcking
fraudulent accounts: The role of the underground market in twitter spam
and abuse,â€ in 22nd USENIX Security Symposium, 2013, pp. 195â€“210.
[23] S. Sedhai and A. Sun, â€œSemi-supervised spam detection in twitter
stream,â€ IEEE Transactions on Computational Social Systems, vol. 5,
no. 1, pp. 169â€“175, 2017.
[24] T. Wu, S. Wen, Y. Xiang, and W. Zhou, â€œTwitter spam detection: Survey
of new approaches and comparative study,â€ Computers & Security,
vol. 76, pp. 265â€“284, 2018.
[25] T. Wu, S. Liu, J. Zhang, and Y. Xiang, â€œTwitter spam detection based
on deep learning,â€ in Proceedings of the Australasian Computer Science
Week Multiconference, 2017, pp. 1â€“8.
[26] A. Keith and K. Burridge, â€œEuphemism and dysphemism: language
used as shield and weapon,â€ 1991.
[27] K. L. Pfaï¬€, R. W. Gibbs, and M. D. Johnson, â€œMetaphor in using and
understanding euphemism and dysphemism,â€ Applied Psycholinguistics,
vol. 18, no. 1, pp. 59â€“83, 1997.
[28] R. Hugh, â€œRawsonâ€™s dictionary of euphemisms and other doubletalk,â€
2002.
[29] K. Allan, â€œThe connotations of english colour terms: Colour-based
x-phemisms,â€ Journal of Pragmatics, vol. 41, no. 3, pp. 626â€“637, 2009.
[30] H. A. Rababah, â€œThe translatability and use of x-phemism expressions
(x-phemization): Euphemisms, dysphemisms and orthophemisms) in the
medical discourse,â€ Studies in Literature and Language, vol. 9, no. 3,
pp. 229â€“240, 2014.
[31] R. A. Spears, Slang and euphemism. Signet Book, 1981.
[32] P. Chilton, â€œMetaphor, euphemism and the militarization of language,â€
Current Research on Peace and Violence, vol. 10, no. 1, pp. 7â€“19, 1987.
[33] H. Ahl, â€œMotivation in adult education: a problem solver or a euphemism
for direction and control?â€ International Journal of Lifelong Education,
vol. 25, no. 4, pp. 385â€“405, 2006.
[34] E. C. FernÃ¡ndez, â€œThe language of death: Euphemism and conceptual
metaphorization in victorian obituaries,â€ SKY Journal of Linguistics,
vol. 19, no. 2006, pp. 101â€“130, 2006.
[35] Z. Pei, Z. Sun, and Y. Xu, â€œSlang detection and identiï¬cation,â€ in
Proceedings of Computational Natural Language Learning (CoNLL),
2019, pp. 881â€“889.
[36] Z. Huang, W. Xu, and K. Yu, â€œBidirectional lstm-crf models for sequence
tagging,â€ arXiv preprint arXiv:1508.01991, 2015.
[37] J. D. Laï¬€erty, A. McCallum, and F. C. Pereira, â€œConditional random
ï¬elds: Probabilistic models for segmenting and labeling sequence data,â€
in Proceedings of International Conference on Machine Learning