best across three datasets. This is why we ultimately selected
it and reported results using it in Section V. Yet, as shown
in Figure 4, other classiﬁers have satisfactory performance as
well, and reach a testing accuracy ranging from 0.86 to 0.90.
2) Fine-Grained Classiﬁers: In the euphemism identiﬁcation
framework, we use a multi-class classiﬁer to identify to which
target keyword each euphemism refers. Again, we experimented
with the same set of classiﬁers as above. Interestingly, we
ﬁnd that, for ﬁne-grained classiﬁcation, all classiﬁers have
highly similar results. One possible reason is that each class
has relatively small number of training instances (ranging
from a few hundreds to 100k), which limits the discriminative
power of advanced algorithms. For the drug dataset (33 target
keywords), the training accuracy is about 55% and the testing
accuracy is about 24%. This shows the feasibility of the
task since the random guess accuracy would be 3.3%. Given
the similar performance across classiﬁers, we recommend
Logistic Regression on raw Text (LRT) for better computational
eﬃciency.
For both coarse classiﬁers and ﬁne-grained classiﬁers, we
leave more advanced classiﬁcation algorithms for future work.
B. Parameter Analysis
Figure 5. Sensitivity of 𝑡.
In the euphemism detection step (Section IV-A), we set a
masked language model threshold 𝑡 to ﬁlter out the generic
masked sentences. In the ranked list of replacements for
the mask token, if any target keyword appears in the top-𝑡
replacement candidates for the masked sentence, we consider
the masked sentence a valid context instance. Otherwise, we
consider the masked sentence generic and ﬁlter it out. Figure
5 shows how the results change with the threshold 𝑡 and we
observe a slight decrease when the threshold 𝑡 is larger than 5.
Therefore, 𝑡 = 5 appears to be an optimal parameter choice.
C. Limitations
While our approach for euphemism detection and identiﬁca-
tion appears highly promising, it does have some limitations.
Text-only moderation: Our approach only works with text,
and our techniques are not easily generalizable to other media.
Social media posts frequently include images, video, and
audio, which can be even more challenging (and even more
traumatic) to moderate by hand [3]–[5]. However, text is
frequently associated with these other media, e.g., in the form of
comments, and thus detecting euphemism use might indirectly
provide clues to content moderators dealing with diﬀerent
media.
Other contexts: Our approach performs well on corpora
discussing drugs, weapons, and sexuality. In preliminary
experiments with a corpus of hate speech it did not perform
nearly as well, producing many false matches when tasked with
identifying racial slurs. We believe this is because euphemisms
related to drugs, weapons, and sex typically have speciﬁc
meanings; e.g., “pot” always refers to marĳuana, not some
other drugs. Racial slurs, on the other hand, are (in this
corpus) used imprecisely, and interchangeably with generic
swearwords, which seems to confuse euphemism detection. We
do not know yet whether this is a fundamental limitation. Even
if it is, though, there are many contexts where euphemisms
have speciﬁc meanings and our approach should be eﬀective,
particularly forums selling illicit goods.
Robustness to adversarial evasion: In our evaluation, we
have relied on a priori non-adversarial datasets, that were
gleaned from public, online forums. In other words, people
were using euphemisms, but we do not know whether they were
using them speciﬁcally to evade content moderation. Perhaps
these euphemisms are, for them, simply the ordinary names
of certain things within the circle where they were discussing
them. (Someone who consistently spoke of “marĳuana” instead
of “pot” on a forum dedicated to discussing drug experiences
might well be suspected of being an undercover cop.)
Because our algorithms rely on sentence-level context to
detect and identify euphemisms, an adversary would need to
change that context to escape detection. Such changes may
also render the text unintelligible to its intended audience.
Therefore, we expect our techniques to be moderately resilient
to adversarial evasion. However, we cannot test our expectations
at the moment, since we do not have a dataset where people
were purposely using euphemisms only to escape detection.
Usability for content moderators: While our approach shows
encouraging performance in lab tests, we have not yet evaluated
whether it is good enough to be helpful to content moderators
in practice. That evaluation would require a user study of
professional content moderators. This is out of scope for the
0.30.330.3602468101214Precision @ 100tpresent paper, which focuses on the technical underpinnings
of euphemism detection and identiﬁcation. We are interested
in investigating usability as a follow-up study.
As a preliminary experiment, we investigated the Perspective
API13, Google’s automated toxicity detector to identify the
likelihood of a sentence being considered toxic by a reader.
Perspective is reportedly used today by human moderators
to ﬁlter or prioritize comments that may require moderation.
We take sentences from our datasets that contain the target
keywords (e.g., “marĳuana”, “heroin”) and for each such
sentence, we evaluate the toxicity score of the sentence (a)
with the target keyword, and (b) by replacing the target
keyword with one of its identiﬁed euphemisms (e.g., “weed”,
“dope”). By comparing the toxicity scores, we can estimate the
likelihood that a human moderator who is using Perspective
API would be shown each version of the sentence. Table
VIII shows the average toxicity scores when comparing 1000
randomly chosen original sentences with their euphemistic
replacements for the drug, weapon, and sexuality categories.
We observe that sentences with target keywords have higher
(or at least comparable) toxicity scores compared to sentences
with euphemisms, which suggests that euphemisms could help
escape content moderation based on the Perspective API. In
turn, detecting and identifying euphemisms could help defeat
such evasive techniques.
Average toxicity socres by Perspective API. (A): original sentences; (B):
sentences with their euphemistic replacements.
Table VIII
Drug Weapon
0.235
0.232
A 0.209
B
0.178
Sexuality
0.612
0.522
associated with them, and 3) the research is not correlating
diﬀerent public sources of data to infer private data.14 All of
these conditions apply to the present study.
VII. Conclusion
We have worked on the problem of content moderation
by detecting and identifying euphemisms. By utilizing the
contextual information explicitly, we not only obtain new state-
of-the-art detection results, but also discover new euphemisms
that are not even on the ground truth list. For euphemism
identiﬁcation, we, for the ﬁrst time, prove the feasibility of the
task and achieve it on a raw text corpus alone, without relying
on any additional resources or supervision.
Reproducibility
Our code and pre-trained models are available on GitHub:
https://github.com/WanzhengZhu/Euphemism.
Acknowledgments
We thank our shepherd, Ben Zhao, and the anonymous
reviewers for comments on earlier drafts that signiﬁcantly
helped improve this manuscript; Kyle Soska for providing
us with the Reddit data and Sadia Afroz for the weapons
data; Xiaojing Liao and Haoran Lu for availing and discussing
the Cantreader implementation with us; and Xin Huang for
insightful discussions. This research was partially supported by
the National Science Foundation, awards CNS-1720268 and
CNS-1814817.
References
[1] D. Blackie, “AOL censors British town’s name!” Computer Underground
Digest, vol. 8, April 1996, as abstracted in RISKS Digest 18.07.
[Online]. Available: http://catless.ncl.ac.uk/Risks/18.07.html#subj3
[2] P. M. Barrett, “Who moderates the social media giants?” Center for
Business, 2020.
D. Ethics
This study relies extensively on user-generated content. We
consider here the ethical implications of this work. The data we
use in this paper were posted on publicly accessible websites,
and do not contain any personal identiﬁable information (i.e.,
no real names, email addresses, IP addresses, etc.). Further, they
are from 2018 or earlier, which greatly reduces any sensitive
nature they might have. For instance, given their age and the
absence of personal identiﬁable information, the data present
very little utility in helping reduce imminent risks to people.
From a regulatory standpoint, in the context of earlier work
on online anonymous marketplaces [43], [94], Carnegie Mellon
University’s Institutional Review Board (IRB) gave us very
clear feedback on what is considered human research and
thus subject to IRB review. Analyses relying on user-generated
content do not constitute human-subject research, and are thus
not the purview of the IRB, as long as 1) the data analyzed
are posted on public fora and were not the result of direct
interaction from the researchers with the people posting, 2)
no private identiﬁers or personal identiﬁable information are
13https://www.perspectiveapi.com/
[3] C. Newton, “The terror queue,” Dec. 2019, https://www.theverge.com/2019/
12/16/21021005/google-youtube-moderators-ptsd-accenture-violent-disturbing-
content-interviews-video.
[4] ——, “The trauma ﬂoor: The secret lives of Facebook moderators
in America,” Feb. 2019, https://www.theverge.com/2019/2/25/18229714/
cognizant-facebook-content-moderator-interviews-trauma-working-conditions-
arizona.
[5] Cambridge Consultants, “Use of AI in online content moderation,”
Ofcom Report, 2019, https://www.ofcom.org.uk/__data/assets/pdf_ﬁle/0028/
157249/cambridge-consultants-ai-content-moderation.pdf.
[6] G. Durrett, J. K. Kummerfeld, T. Berg-Kirkpatrick, R. Portnoﬀ, S. Afroz,
D. McCoy, K. Levchenko, and V. Paxson, “Identifying products in online
cybercrime marketplaces: A dataset for ﬁne-grained domain adaptation,”
in Proceedings of Empirical Methods in Natural Language Processing
(EMNLP), 2017, pp. 2598–2607.
[7] R. S. Portnoﬀ, S. Afroz, G. Durrett, J. K. Kummerfeld, T. Berg-
Kirkpatrick, D. McCoy, K. Levchenko, and V. Paxson, “Tools for
automated analysis of cybercriminal markets,” in Proceedings of
International Conference on World Wide Web (WWW), 2017, pp. 657–
666.
[8] B. Felbo, A. Mislove, A. Søgaard, I. Rahwan, and S. Lehmann, “Using
millions of emoji occurrences to learn any-domain representations for
detecting sentiment, emotion and sarcasm,” in Proceedings of Empirical
Methods in Natural Language Processing (EMNLP), 2017, pp. 1615–
1625.
14This position is in line with Title 45 of the Code of Federal Regulations,
Part 46 (45 CFR 46), which deﬁnes human research.
[9] K. Yuan, H. Lu, X. Liao, and X. Wang, “Reading thieves’ cant: auto-
matically identifying and understanding dark jargons from cybercrime
marketplaces,” in Proceedings of 27th USENIX Security Symposium,
2018, pp. 1027–1041.
[10] T. Hada, Y. Sei, Y. Tahara, and A. Ohsuga, “Codewords detection in
microblogs focusing on diﬀerences in word use between two corpora,”
in Proceedings of International Conference on Computing, Electronics
& Communications Engineering (iCCECE).
IEEE, 2020, pp. 103–108.
[11] R. Magu and J. Luo, “Determining code words in euphemistic hate
speech using word embedding networks,” in Proceedings of the 2nd
Workshop on Abusive Language Online (ALW2), 2018, pp. 93–100.
[12] K. Zhao, Y. Zhang, C. Xing, W. Li, and H. Chen, “Chinese underground
market jargon analysis based on unsupervised learning,” in Proceedings
of IEEE Conference on Intelligence and Security Informatics (ISI).
IEEE, 2016, pp. 97–102.
[13] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Dis-
tributed representations of words and phrases and their compositionality,”
in Proceedings of Advances in Neural Information Processing Systems
(NeurIPS), 2013, pp. 3111–3119.
[14] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Eﬃcient estimation of
word representations in vector space,” arXiv preprint arXiv:1301.3781,
2013.
[15] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training
of deep bidirectional transformers for language understanding,” in
Proceedings of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies (NAACL-
HLT), 2019, pp. 4171–4186.
[16] C. Donahue, M. Lee, and P. Liang, “Enabling language models to
ﬁll in the blanks,” in Proceedings of Association for Computational
Linguistics (ACL), 2020.
[17] C. Felt and E. Riloﬀ, “Recognizing euphemisms and dysphemisms
using sentiment analysis,” in Proceedings of the Second Workshop on
Figurative Language Processing, 2020, pp. 136–145.
[18] N. Leontiadis, T. Moore, and N. Christin, “Measuring and analyzing
search-redirection attacks in the illicit online prescription drug trade.”
in USENIX Security Symposium, vol. 11, 2011.
[19] D. McCoy, A. Pitsillidis, J. Grant, N. Weaver, C. Kreibich, B. Krebs,
G. Voelker, S. Savage, and K. Levchenko, “Pharmaleaks: Understanding
the business of online pharmaceutical aﬃliate programs,” in Part of the
21st USENIX Security Symposium, 2012, pp. 1–16.
[20] J. Huang, Z. Li, X. Xiao, Z. Wu, K. Lu, X. Zhang, and G. Jiang,
“SUPOR: Precise and scalable sensitive user input detection for android
apps,” in 24th USENIX Security Symposium, 2015, pp. 977–992.
[21] Y. Nan, M. Yang, Z. Yang, S. Zhou, G. Gu, and X. Wang, “Uipicker:
User-input privacy identiﬁcation in mobile applications,” in Proceedings
of 24th USENIX Security Symposium, 2015, pp. 993–1008.
[22] K. Thomas, D. McCoy, C. Grier, A. Kolcz, and V. Paxson, “Traﬃcking
fraudulent accounts: The role of the underground market in twitter spam
and abuse,” in 22nd USENIX Security Symposium, 2013, pp. 195–210.
[23] S. Sedhai and A. Sun, “Semi-supervised spam detection in twitter
stream,” IEEE Transactions on Computational Social Systems, vol. 5,
no. 1, pp. 169–175, 2017.
[24] T. Wu, S. Wen, Y. Xiang, and W. Zhou, “Twitter spam detection: Survey
of new approaches and comparative study,” Computers & Security,
vol. 76, pp. 265–284, 2018.
[25] T. Wu, S. Liu, J. Zhang, and Y. Xiang, “Twitter spam detection based
on deep learning,” in Proceedings of the Australasian Computer Science
Week Multiconference, 2017, pp. 1–8.
[26] A. Keith and K. Burridge, “Euphemism and dysphemism: language
used as shield and weapon,” 1991.
[27] K. L. Pfaﬀ, R. W. Gibbs, and M. D. Johnson, “Metaphor in using and
understanding euphemism and dysphemism,” Applied Psycholinguistics,
vol. 18, no. 1, pp. 59–83, 1997.
[28] R. Hugh, “Rawson’s dictionary of euphemisms and other doubletalk,”
2002.
[29] K. Allan, “The connotations of english colour terms: Colour-based
x-phemisms,” Journal of Pragmatics, vol. 41, no. 3, pp. 626–637, 2009.
[30] H. A. Rababah, “The translatability and use of x-phemism expressions
(x-phemization): Euphemisms, dysphemisms and orthophemisms) in the
medical discourse,” Studies in Literature and Language, vol. 9, no. 3,
pp. 229–240, 2014.
[31] R. A. Spears, Slang and euphemism. Signet Book, 1981.
[32] P. Chilton, “Metaphor, euphemism and the militarization of language,”
Current Research on Peace and Violence, vol. 10, no. 1, pp. 7–19, 1987.
[33] H. Ahl, “Motivation in adult education: a problem solver or a euphemism
for direction and control?” International Journal of Lifelong Education,
vol. 25, no. 4, pp. 385–405, 2006.
[34] E. C. Fernández, “The language of death: Euphemism and conceptual
metaphorization in victorian obituaries,” SKY Journal of Linguistics,
vol. 19, no. 2006, pp. 101–130, 2006.
[35] Z. Pei, Z. Sun, and Y. Xu, “Slang detection and identiﬁcation,” in
Proceedings of Computational Natural Language Learning (CoNLL),
2019, pp. 881–889.
[36] Z. Huang, W. Xu, and K. Yu, “Bidirectional lstm-crf models for sequence
tagging,” arXiv preprint arXiv:1508.01991, 2015.
[37] J. D. Laﬀerty, A. McCallum, and F. C. Pereira, “Conditional random
ﬁelds: Probabilistic models for segmenting and labeling sequence data,”
in Proceedings of International Conference on Machine Learning