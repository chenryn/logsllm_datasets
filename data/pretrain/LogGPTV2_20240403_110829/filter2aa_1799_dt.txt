that provide management facilities, like the Windows Registry, user-mode services, task scheduling, 
UBPM, and Windows Management Instrumentation (WMI). Furthermore, we have discussed how Event 
Tracing for Windows (ETW), DTrace, Windows Error Reporting (WER), and Global Flags (GFlags) provide 
the services that allow users to better trace and diagnose issues arising from any component of the 
OS or user-mode applications. The chapter concluded with a peek at the Kernel Shim engine, which 
helps the system apply compatibility strategies and correctly execute old components that have been 
designed for older versions of the operating system. 
The next chapter delves into the different file systems available in Windows and with the global 
caching available for speeding up file and data access.
565
C H A P T E R  1 1
Caching and file systems
T
he cache manager is a set of kernel-mode functions and system threads that cooperate with the 
memory manager to provide data caching for all Windows file system drivers (both local and 
network). In this chapter, we explain how the cache manager, including its key internal data structures 
and functions, works; how it is sized at system initialization time; how it interacts with other elements 
of the operating system; and how you can observe its activity through performance counters. We also 
describe the five flags on the Windows CreateFile function that affect file caching and DAX volumes, 
which are memory-mapped disks that bypass the cache manager for certain types of I/O.
The services exposed by the cache manager are used by all the Windows File System drivers, which 
cooperate strictly with the former to be able to manage disk I/O as fast as possible. We describe the dif-
ferent file systems supported by Windows, in particular with a deep analysis of NTFS and ReFS (the two 
most used file systems). We present their internal architecture and basic operations, including how they 
interact with other system components, such as the memory manager and the cache manager.
The chapter concludes with an overview of Storage Spaces, the new storage solution designed to 
replace dynamic disks. Spaces can create tiered and thinly provisioned virtual disks, providing features 
that can be leveraged by the file system that resides at the top.
Terminology
To fully understand this chapter, you need to be familiar with some basic terminology:
I 
Disks are physical storage devices such as a hard disk, CD-ROM, DVD, Blu-ray, solid-state disk
(SSD), Non-volatile Memory disk (NVMe), or flash drive.
I 
Sectors are hardware-addressable blocks on a storage medium. Sector sizes are determined
by hardware. Most hard disk sectors are 4,096 or 512 bytes, DVD-ROM and Blu-ray sectors are
typically 2,048 bytes. Thus, if the sector size is 4,096 bytes and the operating system wants to
modify the 5120th byte on a disk, it must write a 4,096-byte block of data to the second sector
on the disk.
I 
Partitions are collections of contiguous sectors on a disk. A partition table or other disk-
management database stores a partition’s starting sector, size, and other characteristics and
is located on the same disk as the partition.
I 
Volumes are objects that represent sectors that file system drivers always manage as a single
unit. Simple volumes represent sectors from a single partition, whereas multipartition volumes
566 
CHAPTER 11 Caching and file systems
represent sectors from multiple partitions. Multipartition volumes offer performance, reliability, 
and sizing features that simple volumes do not.
I 
File system formats define the way that file data is stored on storage media, and they affect a
file system’s features. For example, a format that doesn’t allow user permissions to be associ-
ated with files and directories can’t support security. A file system format also can impose limits
on the sizes of files and storage devices that the file system supports. Finally, some file system
formats efficiently implement support for either large or small files or for large or small disks.
NTFS, exFAT, and ReFS are examples of file system formats that offer different sets of features
and usage scenarios.
I 
Clusters are the addressable blocks that many file system formats use. Cluster size is always a
multiple of the sector size, as shown in Figure 11-1, in which eight sectors make up each cluster,
which are represented by a yellow band. File system formats use clusters to manage disk space
more efficiently; a cluster size that is larger than the sector size divides a disk into more man-
ageable blocks. The potential trade-off of a larger cluster size is wasted disk space, or internal
fragmentation, that results when file sizes aren’t exact multiples of the cluster size.
Sector
Cluster (8 sectors)
FIGURE 11-1 Sectors and clusters on a classical spinning disk.
I 
Metadata is data stored on a volume in support of file system format management. It isn’t typi-
cally made accessible to applications. Metadata includes the data that defines the placement of
files and directories on a volume, for example.
Key features of the cache manager
The cache manager has several key features:
I 
Supports all file system types (both local and network), thus removing the need for each file
system to implement its own cache management code.
I 
Uses the memory manager to control which parts of which files are in physical memory (trading
off demands for physical memory between user processes and the operating system).
I 
Caches data on a virtual block basis (offsets within a file)—in contrast to many caching systems,
which cache on a logical block basis (offsets within a disk volume)—allowing for intelligent
CHAPTER 11 Caching and file systems
567
read-ahead and high-speed access to the cache without involving file system drivers. (This 
method of caching, called fast I/O, is described later in this chapter.)
I 
Supports “hints” passed by applications at file open time (such as random versus sequential
access, temporary file creation, and so on).
I 
Supports recoverable file systems (for example, those that use transaction logging) to recover
data after a system failure.
I 
Supports solid state, NVMe, and direct access (DAX) disks.
Although we talk more throughout this chapter about how these features are used in the cache 
manager, in this section we introduce you to the concepts behind these features.
Single, centralized system cache
Some operating systems rely on each individual file system to cache data, a practice that results either in 
duplicated caching and memory management code in the operating system or in limitations on the kinds 
of data that can be cached. In contrast, Windows offers a centralized caching facility that caches all exter-
nally stored data, whether on local hard disks, USB removable drives, network file servers, or DVD-ROMs. 
Any data can be cached, whether it’s user data streams (the contents of a file and the ongoing read and 
write activity to that file) or file system metadata (such as directory and file headers). As we discuss in this 
chapter, the method Windows uses to access the cache depends on the type of data being cached.
The memory manager
One unusual aspect of the cache manager is that it never knows how much cached data is actually in 
physical memory. This statement might sound strange because the purpose of a cache is to keep a sub-
set of frequently accessed data in physical memory as a way to improve I/O performance. The reason 
the cache manager doesn’t know how much data is in physical memory is that it accesses data by map-
ping views of files into system virtual address spaces, using standard section objects (or file mapping ob-
jects in Windows API terminology). (Section objects are a basic primitive of the memory manager and 
are explained in detail in Chapter 5, “Memory Management” of Part 1). As addresses in these mapped 
views are accessed, the memory manager pages-in blocks that aren’t in physical memory. And when 
memory demands dictate, the memory manager unmaps these pages out of the cache and, if the data 
has changed, pages the data back to the files.
By caching on the basis of a virtual address space using mapped files, the cache manager avoids gen-
erating read or write I/O request packets (IRPs) to access the data for files it’s caching. Instead, it simply 
copies data to or from the virtual addresses where the portion of the cached file is mapped and relies on 
the memory manager to fault in (or out) the data in to (or out of) memory as needed. This process allows 
the memory manager to make global trade-offs on how much RAM to give to the system cache versus 
how much to give to user processes. (The cache manager also initiates I/O, such as lazy writing, which we 
describe later in this chapter; however, it calls the memory manager to write the pages.) Also, as we dis-
cuss in the next section, this design makes it possible for processes that open cached files to see the same 
data as do other processes that are mapping the same files into their user address spaces.
568 
CHAPTER 11 Caching and file systems
Cache coherency
One important function of a cache manager is to ensure that any process that accesses cached data will 
get the most recent version of that data. A problem can arise when one process opens a file (and hence 
the file is cached) while another process maps the file into its address space directly (using the Windows 
MapViewOfFile function). This potential problem doesn’t occur under Windows because both the cache 
manager and the user applications that map files into their address spaces use the same memory man-
agement file mapping services. Because the memory manager guarantees that it has only one represen-
tation of each unique mapped file (regardless of the number of section objects or mapped views), it maps 
all views of a file (even if they overlap) to a single set of pages in physical memory, as shown in Figure 11-2. 
(For more information on how the memory manager works with mapped files, see Chapter 5 of Part 1.)
System address
space
View 2
File
View 1
User address 
space
System address
space
View 2
User address 
space
Control area
Process 1
virtual memory
Physical
memory
Process 2
virtual memory
4 GB
System
cache
2 GB
Mapped file
0
4 GB
System
cache
2 GB
0
Size
0
FIGURE 11-2 Coherent caching scheme.
So, for example, if Process 1 has a view (View 1) of the file mapped into its user address space, and 
Process 2 is accessing the same view via the system cache, Process 2 sees any changes that Process 
1 makes as they’re made, not as they’re flushed. The memory manager won’t flush all user-mapped 
CHAPTER 11 Caching and file systems
569
pages—only those that it knows have been written to (because they have the modified bit set). 
Therefore, any process accessing a file under Windows always sees the most up-to-date version of that 
file, even if some processes have the file open through the I/O system and others have the file mapped 
into their address space using the Windows file mapping functions.
Note Cache coherency in this case refers to coherency between user-mapped data and 
cached I/O and not between noncached and cached hardware access and I/Os, which are 
almost guaranteed to be incoherent. Also, cache coherency is somewhat more difficult for 
network redirectors than for local file systems because network redirectors must imple-
ment additional flushing and purge operations to ensure cache coherency when accessing 
network data.
Virtual block caching
The Windows cache manager uses a method known as virtual block caching, in which the cache manager 
keeps track of which parts of which files are in the cache. The cache manager is able to monitor these 
file portions by mapping 256 KB views of files into system virtual address spaces, using special system 
cache routines located in the memory manager. This approach has the following key benefits:
I 
It opens up the possibility of doing intelligent read-ahead; because the cache tracks which parts
of which files are in the cache, it can predict where the caller might be going next.
I 
It allows the I/O system to bypass going to the file system for requests for data that is already
in the cache (fast I/O). Because the cache manager knows which parts of which files are in the
cache, it can return the address of cached data to satisfy an I/O request without having to call
the file system.
Details of how intelligent read-ahead and fast I/O work are provided later in this chapter in the 
“Fast I/O” and “Read-ahead and write-behind” sections.
Stream-based caching
The cache manager is also designed to do stream caching rather than file caching. A stream is a 
sequence of bytes within a file. Some file systems, such as NTFS, allow a file to contain more than one 
stream; the cache manager accommodates such file systems by caching each stream independently. 
NTFS can exploit this feature by organizing its master file table (described later in this chapter in the 
“Master file table” section) into streams and by caching these streams as well. In fact, although the 
cache manager might be said to cache files, it actually caches streams (all files have at least one stream 
of data) identified by both a file name and, if more than one stream exists in the file, a stream name.
Note Internally, the cache manager is not aware of file or stream names but uses pointers to 
these structures.
570 
CHAPTER 11 Caching and file systems
Recoverable file system support
Recoverable file systems such as NTFS are designed to reconstruct the disk volume structure after a 
system failure. This capability means that I/O operations in progress at the time of a system failure must 
be either entirely completed or entirely backed out from the disk when the system is restarted. Half-
completed I/O operations can corrupt a disk volume and even render an entire volume inaccessible. 
To avoid this problem, a recoverable file system maintains a log file in which it records every update 
it intends to make to the file system structure (the file system’s metadata) before it writes the change 
to the volume. If the system fails, interrupting volume modifications in progress, the recoverable file 
system uses information stored in the log to reissue the volume updates.
To guarantee a successful volume recovery, every log file record documenting a volume update must 
be completely written to disk before the update itself is applied to the volume. Because disk writes are 
cached, the cache manager and the file system must coordinate metadata updates by ensuring that the 
log file is flushed ahead of metadata updates. Overall, the following actions occur in sequence:
1.
The file system writes a log file record documenting the metadata update it intends to make.
2.
The file system calls the cache manager to flush the log file record to disk.
3.
The file system writes the volume update to the cache—that is, it modifies its cached metadata.
4.
The cache manager flushes the altered metadata to disk, updating the volume struc-
ture. (Actually, log file records are batched before being flushed to disk, as are volume
modifications.)
Note The term metadata applies only to changes in the file system structure: file and direc-
tory creation, renaming, and deletion.
When a file system writes data to the cache, it can supply a logical sequence number (LSN) that 
identifies the record in its log file, which corresponds to the cache update. The cache manager keeps 
track of these numbers, recording the lowest and highest LSNs (representing the oldest and newest 
log file records) associated with each page in the cache. In addition, data streams that are protected by 
transaction log records are marked as “no write” by NTFS so that the mapped page writer won’t inad-
vertently write out these pages before the corresponding log records are written. (When the mapped 
page writer sees a page marked this way, it moves the page to a special list that the cache manager 
then flushes at the appropriate time, such as when lazy writer activity takes place.)
When it prepares to flush a group of dirty pages to disk, the cache manager determines the highest 
LSN associated with the pages to be flushed and reports that number to the file system. The file system 
can then call the cache manager back, directing it to flush log file data up to the point represented by 
the reported LSN. After the cache manager flushes the log file up to that LSN, it flushes the correspond-
ing volume structure updates to disk, thus ensuring that it records what it’s going to do before actually 
doing it. These interactions between the file system and the cache manager guarantee the recoverabil-
ity of the disk volume after a system failure.
CHAPTER 11 Caching and file systems
571
NTFS MFT working set enhancements
As we have described in the previous paragraphs, the mechanism that the cache manager uses to 
cache files is the same as general memory mapped I/O interfaces provided by the memory manager 
to the operating system. For accessing or caching a file, the cache manager maps a view of the file in 
the system virtual address space. The contents are then accessed simply by reading off the mapped 
virtual address range. When the cached content of a file is no longer needed (for various reasons—see 
the next paragraphs for details), the cache manager unmaps the view of the file. This strategy works 
well for any kind of data files but has some problems with the metadata that the file system maintains 
for correctly storing the files in the volume.
When a file handle is closed (or the owning process dies), the cache manager ensures that the cached 
data is no longer in the working set. The NTFS file system accesses the Master File Table (MFT) as a big file, 
which is cached like any other user files by the cache manager. The problem with the MFT is that, since 
it is a system file, which is mapped and processed in the System process context, nobody will ever close 
its handle (unless the volume is unmounted), so the system never unmaps any cached view of the MFT. 
The process that initially caused a particular view of MFT to be mapped might have closed the handle or 
exited, leaving potentially unwanted views of MFT still mapped into memory consuming valuable system 
cache (these views will be unmapped only if the system runs into memory pressure).
Windows 8.1 resolved this problem by storing a reference counter to every MFT record in a dynami-
cally allocated multilevel array, which is stored in the NTFS file system Volume Control Block (VCB) 
structure. Every time a File Control Block (FCB) data structure is created (further details on the FCB 
and VCB are available later in this chapter), the file system increases the counter of the relative MFT 
index record. In the same way, when the FCB is destroyed (meaning that all the handles to the file or 
directory that the MFT entry refers to are closed), NTFS dereferences the relative counter and calls the 
CcUnmapFileOffsetFromSystemCache cache manager routine, which will unmap the part of the MFT 
that is no longer needed.
Memory partitions support
Windows 10, with the goal to provide support for Hyper-V containers containers and game mode, 
introduced the concept of partitions. Memory partitions have already been described in Chapter 
5 of Part 1. As seen in that chapter, memory partitions are represented by a large data structure 
(MI_PARTITION), which maintains memory-related management structures related to the partition, 
such as page lists (standby, modified, zero, free, and so on), commit charge, working set, page trim-
mer, modified page writer, and zero-page thread. The cache manager needs to cooperate with the 
memory manager in order to support partitions. During phase 1 of NT kernel initialization, the system 
creates and initializes the cache manager partition (for further details about Windows kernel initial-
ization, see Chapter 12, “Startup and shutdown”), which will be part of the System Executive parti-
tion (MemoryPartition0). The cache manager’s code has gone through a big refactoring to support 
partitions; all the global cache manager data structures and variables have been moved in the cache 
manager partition data structure (CC_PARTITION). 
572 
CHAPTER 11 Caching and file systems
The cache manager’s partition contains cache-related data, like the global shared cache maps list, 
the worker threads list (read-ahead, write-behind, and extra write-behind; lazy writer and lazy writer 
scan; async reads), lazy writer scan events, an array that holds the history of write-behind throughout, 