### Figure 6: Effect of Hadoop on PANE and Network

ZooKeeper operates using a quorum of replicated servers (the ensemble). To ensure resiliency in the face of network failures, ZooKeeper servers can be distributed throughout a datacenter. However, this distribution can lead to quorum messages being negatively affected by heavy traffic on shared links. Since ZooKeeper's role is to provide coordination for other services, such negative effects are undesirable.

To protect ZooKeeper’s messages from heavy traffic on shared links, we modified ZooKeeper to make bandwidth reservations using PANE. Upon startup, each member of the ensemble reserved 10 Mbps of guaranteed minimum bandwidth for messages with other ZooKeeper servers. Additionally, we modified our ZooKeeper client to make similar reservations with each server it connected to.

We installed ZooKeeper on an ensemble of five servers and developed a benchmarking client that we ran on a sixth. The client connected a thread to each server and maximized the throughput of synchronous ZooKeeper operations in the ensemble. To remove the effect of disk latency, the ZooKeeper servers used RAM disks for storage. At no time during these experiments were the CPUs of the client, switches, or servers fully loaded. Like our modified applications, this benchmarking tool is also available on GitHub.

Figure 5 shows the latency of ZooKeeper DELETE requests during the experiment. In the "Pre" line, ZooKeeper alone is running in the network, and no reservations were made using PANE. In the "Post" line, we used iperf to generate bi-directional TCP flows over each of the six links directly connected to a host. As shown in the figure, this competing traffic dramatically reduced ZooKeeper’s performance—average latency quadrupled from 1.55ms to 6.46ms (similar results were obtained with a non-OpenFlow switch). Finally, the "PANE" line shows the return to high performance when ZooKeeper reserved bandwidth using PANE.

We found similar results for other ZooKeeper write operations such as creating keys, writing to unique keys, and writing to the same key. Read operations do not require a quorum’s participation and thus are less affected by competing background traffic.

### 7.1.4 Hadoop

In our final case study of PANE’s application performance benefits, we augmented a Hadoop 2.0.3 pre-release with support for our API. Hadoop is an open-source implementation of the MapReduce [13] data-processing framework. In Hadoop, large files are divided across multiple nodes in the network, and computations consist of two phases: map and reduce. During the map phase, a function is evaluated in parallel on independent file pieces. During the reduce phase, a second function proceeds in parallel on the collected outputs of the map phase; the data transfer from the mappers to the reducers is known as the shuffle. During the shuffle, every reduce node initiates a transfer with every map node, making it particularly network-intensive for some jobs, such as sorts or joins. Finally, the output of the reduce is written back to the distributed filesystem.

By using PANE, our version of Hadoop is able to reserve guaranteed bandwidth for its operations. The first set of reservations occurs during the shuffle—each reducer reserves bandwidth for transferring data from the mappers. The second set of reservations reserves bandwidth when writing the final output. These few reservations protect the majority of network transfers that occur during the lifetime of a Hadoop job. Our version of Hadoop also makes reservations when a map task needs to read its input across the network; however, such transfers are typically less common thanks to “delay scheduling” [52]. Therefore, in a typical job, the total number of reservations is on the order of \( M \times R + R \times 2 \), where \( M \) and \( R \) are, respectively, the number of nodes with map and reduce tasks. The number of reservations is not precisely described by this formula as we do not make reservations for node-local transfers, and reducers may contact a mapper node more than once during the shuffle phase. The maximum number of reservations per reducer at any time is set by the value of `mapreduce.reduce.shuffle.parallelcopies` in the configuration, which has a default value of five.

To measure the effect of using PANE to make reservations in Hadoop, we developed a benchmark that executed three 40 GB sort jobs in parallel on a network of 22 machines (20 slaves, plus two masters) connected by a Pronto 3290 switch controlled by PANE. Hadoop currently has the ability to prioritize or weight jobs using the scheduler, but this control does not extend to the network. In our benchmark, the first two jobs were provided with 25% of the cluster’s memory resources, and the third, acting as the "high priority" job, was provided with 50%. The benchmark was run in two configurations: in the first, Hadoop made no requests using PANE; in the second, our modified Hadoop requested guaranteed bandwidth for each large flow. These reservations were proportional to the job’s memory resources and lasted for eight seconds, based on Hadoop’s 256 MB block size. In our star topology with uniform 1 Gbps links, this translated to 500 Mbps reservations for each link.

Averaged across three runs, the high priority job’s completion time decreased by 19% when its bandwidth was guaranteed. Because it completed more quickly, the lower priority jobs’ runtime also decreased, by an average of 9%, since Hadoop’s work-conserving scheduler re-allocates freed memory resources to remaining jobs.

While Hadoop was running, we also measured its effect on PANE and the switch’s flow table. Figure 6(a) is a CDF of the time between Hadoop’s reservations. As currently implemented, PANE modifies the switch flow table after each request. This CDF shows that batching requests over a 10 ms window would decrease the number of flow table updates by 20%; a 100 ms window would decrease the updates by 35%. Figure 6(b) plots the amount of flow table space used by Hadoop during a single job. On average, Hadoop accounted for an additional 2.5 flow table entries; the maximum number of simultaneous Hadoop rules was 28.

### 7.2 Implementation Practicality

In addition to examining PANE’s use in real applications, we also evaluated the practicality of its implementation in current OpenFlow networks. We found that our Pronto 3290 switches, running the Indigo 2012.09.07 firmware, were capable of supporting 1,919 OpenFlow rules, which took an average of 7.12 ms to install per rule. To measure this, we developed a benchmarking controller that installed wildcard match rules, issuing a barrier request after each `flow_mod` message was sent. We based this controller on Floodlight, and it is available for download from our GitHub page.

The latency distribution to fully install each `flow_mod` is shown in Figure 7(a). It has two clusters: for the 92.4% of `flow_mod`s with latency less than 10.0 ms, the average latency was 2.80 ms; the remaining 7.6% had an average latency of 59.5 ms. For PANE’s principals, these much higher tail latencies imply that requests cannot always be implemented within a few milliseconds, and for truly guaranteed traffic handling, requests have to be made at least 100 milliseconds in advance.

We found that our Pronto switches could support seven hardware queues with guaranteed minimum bandwidth on each port, and each queue required an average of 1.73 ms to create or delete, as shown in Figure 7(b). However, this average doubles to 3.56 ms if queues are created consecutively on the same port (i.e., P1Q1, P1Q2, P1Q3, ..., P2Q1, etc.), as shown in Figure 7(c). This shows that an optimized PANE controller must consider the order in which switch operations are made to provide the best experience.

Together, these results suggest that an individual switch can support a minimum of about 200 reservations per second. Higher throughput is possible by batching additional requests between OpenFlow barriers. While these switch features are sufficient to support the four applications above, we found that Hadoop’s performance benefited from per-flow reservations only when flows transferred more than one megabyte. For smaller flows, the overhead of establishing the reservations outweighed the benefit. In an example word count job, only 24% of flows were greater than 1 MB; however, this percentage rises to 73% for an example sort.

### 8. Discussion and Future Work

The PANE prototype implements a complete OpenFlow controller: it determines network-wide policies by realizing requests and hints, monitors switches to respond to queries, determines routing, and updates switches as the policy changes. We built each of these components as separate modules and composed them using composition abstractions derived from Frenetic [19].

In the PANE prototype, policies are arranged as trees. This is not a fundamental design decision—trees make it easy to implement resource accounting and trace the provenance of delegation decisions. However, generalizations such as DAGs should be possible.

An interesting generalization is a more flexible language for requests. A principal should be able to describe the resources they need as simple constraints. For example, “reserve 5 Gbps for 1 hour within the next 5 hours.” Such requests would give the PANE controller more scheduling flexibility, improving network performance and leading to greater accepted requests.

We are also investigating allowing principals to request other kinds of resources, e.g., latency and jitter. Current OpenFlow implementations lack support for controlling these traffic properties. In the future, we hope to borrow techniques from works such as Borrowed Virtual Time [14] or HFSCs [44], which integrate and balance the needs of throughput- and latency-sensitive processes.

**Security Considerations:** While the principals in PANE are authenticated, they do not need to be trusted as privileges are restricted by the system’s semantics—the ShareTree restricts the capabilities of each principal. We recognize, however, that it may be possible to exploit combinations of privileges in an untoward fashion, and leave such prevention as future work.

Our prototype implementation of PANE is currently defenseless against principals which issue excessive requests. We leave such protection against denial-of-service as future work, and expect PANE’s requirement for authenticated principals to enable such protections, as use of the system can be audited.

**PANE as a Building Block:** We believe the primitives provided by PANE could serve as an implementation substrate for a variety of recent proposals in the networking literature. For example, CoFlow [11] proposes an abstraction that groups several related flows, allowing the network control-plane to better optimize an application’s communication. By adding support for transactions, we expect PANE could be used to implement CoFlow—each co-flow would map to a sequence of PANE requests grouped by a transaction. FairCloud [38], which develops several policies for fairly dividing datacenter bandwidth, should be implementable using PANE’s reservation and rate-limit requests.

Finally, because PANE allows applications to reserve guaranteed bandwidth, such applications could skip TCP’s slow start phase, or even allow for the network control-plane to be involved in setting congestion control and other parameters, as in proposals such as XCP [28] and OpenTCP [21]. PANE could also integrate better support for middlebox-based services, perhaps by integrating the approach advocated for by Gember et al. [20].

### 9. Related Work

**Programming the Network:** PANE allows applications and users to influence network operations, a goal shared by previous research such as active networking [45]. In active networks, principals develop distributed programs that run in the network nodes. By contrast, PANE sidesteps active networks’ deployment challenges via its implementation as an SDN controller, their security concerns by providing a much more restricted interface to the network, and their complexity by providing a logically centralized network view.

**Using Application-Layer Information:** Many previous works describe specific cases in which information from end-users or applications benefits network configuration, flexibility, or performance; PANE can be a unifying framework for these. For example, using Hedera to dynamically identify and place large flows in a datacenter can improve throughput up to 113% [1]. PANE avoids Hedera’s inference of flow size by enabling applications and devices to directly inform the network about flow sizes. Wang et al. [47] propose application-aware networking and argue that distributed applications can benefit from communicating their preferences to the network control-plane, as we show in §7.1. ident++ [35] proposes an architecture in which an OpenFlow controller reactively queries the endpoints of a new flow to determine whether it should be admitted. TVA is a network architecture in which end-hosts authorize the receipt of packet flows via capabilities to prevent DoS-attacks [50]. By contrast, PANE allows administrators to delegate the privilege to install restricted network-wide firewall rules, and users can do so either proactively or reactively (cf. §7.1.2).

Darwin [9] introduced a method for applications to request network resources, including computation and storage capabilities in network processors. Like PANE, Darwin accounts for resource use hierarchically. However, it does not support over-subscription, lacks support for access control and path management, and requires routers with support for active networks. Yap et al. have also advocated for an explicit communication channel between applications and software-defined networks, in what they called software-friendly networks [51]. This earlier work, however, only supports requests made by a single, trusted application. By contrast, PANE’s approach to delegation, accounting, and conflict-resolution allows multiple principals to interact securely and effectively.