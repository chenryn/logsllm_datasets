Figure 6: Effect of Hadoop on PANE and network.
state using a quorum of replicated servers (the ensemble). For re-
siliency in the face of network failures, ZooKeeper servers may be
distributed throughout a datacenter, and thus quorum messages may
be negatively affected by heavy trafﬁc on shared links. Because
ZooKeeper’s role is to provide coordination for other services, such
negative effects are undesirable.
To protect ZooKeeper’s messages from heavy trafﬁc on shared
links, we modiﬁed ZooKeeper to make bandwidth reservations us-
ing PANE. Upon startup, each member of the ensemble made a
reservation for 10 Mbps of guaranteed minimum bandwidth for
messages with other ZooKeeper servers. Additionally, we modi-
ﬁed our ZooKeeper client to make a similar reservation with each
server it connected to.
We installed ZooKeeper on an ensemble of ﬁve servers, and de-
veloped a benchmarking client which we ran on a sixth. The client
connected a thread to each server and maximized the throughput
of synchronous ZooKeeper operations in our ensemble. To remove
the effect of disk latency, the ZooKeeper servers used RAM disks
for storage. At no time during these experiments were the CPUs
of the client, switches, or servers fully loaded. Like our modiﬁed
applications, this benchmarking tool is also available on Github.
Figure 5 shows the latency of ZooKeeper DELETE requests dur-
ing the experiment. In the “Pre” line, ZooKeeper alone is running
in the network and no reservations were made using PANE. In the
“Post” line, we used iperf to generate bi-directional TCP ﬂows
over each of the six links directly connected to a host. As shown in
the ﬁgure, this competing trafﬁc dramatically reduced ZooKeeper’s
performance – average latency quadrupled from 1.55ms to 6.46ms
(we obtained similar results with a non-OpenFlow switch). Finally,
the “PANE” line shows the return to high performance when ZooKeeper
reserved bandwidth using PANE.
We found similar results for other ZooKeeper write operations
such as creating keys, writing to unique keys, and writing to the
same key. Read operations do not require a quorum’s participation,
and thus are less affected by competing background trafﬁc.
7.1.4 Hadoop
In our ﬁnal case study of PANE’s application performance ben-
eﬁts, we augmented a Hadoop 2.0.3 pre-release with support for
our API. Hadoop is an open source implementation of the MapRe-
duce [13] data-processing framework. In Hadoop, large ﬁles are di-
vided across multiple nodes in the network, and computations con-
sist of two phases: a map, and a reduce. During the map phase, a
function is evaluated in parallel on independent ﬁle pieces. During
the reduce, a second function proceeds in parallel on the collected
outputs of the map phrase; the data transfer from the mappers to the
reducers is known as the shufﬂe. During the shufﬂe, every reduce
node initiates a transfer with every map node, making it particularly
network-intensive for some jobs, such as sorts or joins. Finally, the
output of the reduce is written back to the distributed ﬁlesystem.
By using PANE, our version of Hadoop is able to reserve guar-
anteed bandwidth for its operations. The ﬁrst set of reservations
occurs during the shufﬂe – each reducer reserves bandwidth for
transferring data from the mappers. The second set of reservations
reserves bandwidth when writing the ﬁnal output. These few reser-
vations protect the majority of network transfers that occur during
the lifetime of a Hadoop job. Our version of Hadoop also makes
reservations when a map task needs to read its input across the net-
work; however, such transfers are typically less common thanks to
“delay scheduling” [52]. Therefore, in a typical job, the total num-
ber of reservations is on the order of M × R + R × 2 where M
and R are, respectively, the number of nodes with map and reduce
tasks. The number of reservations is not precisely described by this
formula as we do not make reservations for node-local transfers,
and reducers may contact a mapper node more than once during
the shufﬂe phase. As reducers can either be copying output from
mappers in the shufﬂe, or writing their output to the distributed
ﬁlesystem, the maximum number of reservations per reducer at any
time is set by the value of mapreduce.reduce.shufﬂe.parallelcopies
in the conﬁguration, which has a default value of ﬁve.
To measure the effect of using PANE to make reservations in
Hadoop, we developed a benchmark which executed three 40 GB
sort jobs in parallel on a network of 22 machines (20 slaves, plus
two masters) connected by a Pronto 3290 switched controlled by
PANE. Hadoop currently has the ability to prioritize or weight jobs
using the scheduler, but this control does not extend to the network.
In our benchmark, the ﬁrst two jobs were provided with 25% of the
cluster’s memory resources, and the third, acting as the “high pri-
ority” job, was provided with 50%. The benchmark was run in two
conﬁgurations: in the ﬁrst, Hadoop made no requests using PANE;
in the second, our modiﬁed Hadoop requested guaranteed band-
width for each large ﬂow. These reservations were proportional to
the job’s memory resources, and lasted for eight seconds, based on
Hadoop’s 256 MB block size. In our star topology with uniform 1
Gbps links, this translated to 500 Mbps reservations for each link.
Averaged across three runs, the high priority job’s completion
time decreased by 19% when its bandwidth was guaranteed. Be-
cause it completed more quickly, the lower priority jobs’ runtime
also decreased, by an average of 9%, since Hadoop’s work-conserving
scheduler re-allocates freed memory resources to remaining jobs.
While Hadoop was running, we also measured its effect on PANE
and the switch’s ﬂow table. Figure 6(a) is a CDF of the time be-
tween Hadoop’s reservations. As currently implemented, PANE mod-
iﬁes the switch ﬂow table after each request. This CDF shows that
batching requests over a 10 ms window would decrease the number
of ﬂow table updates by 20%; a 100 ms window would decrease the
updates by 35%. Figure 6(b) plots the amount of ﬂow table space
used by Hadoop during a single job. On average, Hadoop accounted
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.0001 0.001 0.01 0.1 1P(X <= x)Latency of DELETE Operations (s)PrePostPANE 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.001 0.01 0.1 1 10 100 1000P(X <= x)Interarrival Time (s) 0 5 10 15 20 25 30 0 5 10 15 20 25 30Number of Resident RulesTime(min)for an additional 2.5 ﬂow table entries; the maximum number of si-
multaneous Hadoop rules was 28.
7.2
Implementation Practicality
In addition to examining PANE’s use in real applications, we also
evaluated the practicality of its implementation in current Open-
Flow networks. We found that our Pronto 3290 switches, running
the Indigo 2012.09.07 ﬁrmware, were capable of supporting 1,919
OpenFlow rules, which took an average of 7.12 ms to install per
rule. To measure this, we developed a benchmarking controller
which installed wildcard match rules, issuing a barrier request af-
ter each flow_mod message was sent. We based this controller on
Floodlight, and it is available for download from our Github page.
The latency distribution to fully install each flow_mod is show
in Figure 7(a). It has two clusters – for the 92.4% of flow_mod’s
with latency less than 10.0 ms, the average latency was 2.80 ms;
the remaining 7.6% had an average latency of 59.5 ms. For PANE’s
principals, these much higher tail latencies imply that requests can-
not always be implemented within a few milliseconds, and for truly
guaranteed trafﬁc handling, requests have to be made at least 100
milliseconds in advance.
We found that our Pronto switches could support seven hard-
ware queues with guaranteed minimum bandwidth on each port,
and each queue required an average of 1.73 ms to create or delete,
as shown in Figure 7(b). However, this average doubles to 3.56 ms
if queues are created consecutively on the same port (i.e., P1Q1,
P1Q2, P1Q3, ..., P2Q1, etc.), as shown in Figure 7(c). This shows
that an optimized PANE controller must consider the order in which
switch operations are made to provide the best experience.
Together, these results suggest that an individual switch can sup-
port a minimum of about 200 reservations per second. Higher through-
put is possible by batching additional requests between OpenFlow
barriers. While these switch features are sufﬁcient to support the
four applications above, we found that Hadoop’s performance ben-
eﬁted from per-ﬂow reservations only when ﬂows transferred more
than one megabyte. For smaller ﬂows, the overhead of establish-
ing the reservations outweighed the beneﬁt. In an example word
count job, only 24% of ﬂows were greater than 1 MB; however this
percentage rises to 73% for an example sort.
8. DISCUSSION AND FUTURE WORK
The PANE prototype implements a complete OpenFlow con-
troller: it determines network-wide policies by realizing requests
and hints, monitors switches to respond to queries, determines rout-
ing, and updates switches as the policy changes. We’ve built each
of these components as separate modules, and compose them using
composition abstractions derived from Frenetic [19].
In the PANE prototype, policies are arranged as trees. This is not
a fundamental design decision – trees make it easy to implement
resource accounting and trace the provenance of delegation deci-
sions. However, generalizations such as DAGs should be possible.
An interesting generalization is a more ﬂexible language for re-
quests. A principal should be able to describe the resources they
need as simple constraints. For example, “reserve 5 Gbps for 1 hour
within the next 5 hours.” Such requests would give the PANE con-
troller more scheduling ﬂexibility. They would thus improve net-
work performance and lead to greater accepted requests.
We are also investigating allowing principals to request other
kinds of resources, e.g. latency and jitter. Current OpenFlow im-
plementations lack support for controlling these trafﬁc properties.
In the future, we hope to borrow techniques from works such as
Borrowed Virtual Time [14] or HFSCs [44], which integrate and
balance the needs of throughput- and latency-sensitive processes.
Security Considerations While the principals in PANE are au-
thenticated, they do not need to be trusted as privileges are re-
stricted by the system’s semantics – the ShareTree restricts the ca-
pabilities of each principal. We recognize, however, that it may be
possible to exploit combinations of privileges in an untoward fash-
ion, and leave such prevention as future work.
Our prototype implementation of PANE is currently defense-
less against principals which issue excessive requests. We leave
such protection against denial-of-service as future work, and expect
PANE’s requirement for authenticated principals to enable such
protections, as use of the system can be audited.
PANE as a building block We believe the primitives provided by
PANE could serve as an implementation substrate for a variety of
recent proposals in the networking literature.
For example, Coﬂow [11] proposes an abstraction which groups
several related ﬂows, allowing the network control-plane to better
optimize an application’s communication. By adding support for
transactions, we expect PANE could be used to implement Coﬂow
– each coﬂow would map to a sequence of PANE requests grouped
by a transaction. FairCloud [38], which develops several policies
for fairly dividing datacenter bandwidth, should be implementable
using PANE’s reservation and rate-limit requests.
Finally, because PANE allows applications to reserve guaranteed
bandwidth, such applications could skip TCP’s slow start phase, or
even allow for the network control-plane to be involved in setting
congestion control and other parameters, as in proposals such as
XCP [28] and OpenTCP [21]. PANE could also integrate better
support for middlebox-based services, perhaps by integrating the
approach advocated for by Gember, et al. [20].
9. RELATED WORK
Programming the Network PANE allows applications and users
to inﬂuence network operations, a goal shared by previous research
such as active networking [45]. In active networks, principals de-
velop distributed programs that run in the network nodes. By con-
trast, PANE sidesteps active networks’ deployment challenges via
its implementation as an SDN controller, their security concerns by
providing a much more restricted interface to the network, and their
complexity by providing a logically centralized network view.
Using Application-Layer Information Many previous works de-
scribe speciﬁc cases in which information from end-users or appli-
cations beneﬁts network conﬁguration, ﬂexibility, or performance;
PANE can be a unifying framework for these. For example, using
Hedera to dynamically identify and place large ﬂows in a datacen-
ter can improve throughput up to 113% [1]. PANE avoids Hedera’s
inference of ﬂow size by enabling applications and devices to di-
rectly inform the network about ﬂow sizes. Wang, et al. [47] pro-
pose application-aware networking, and argue that distributed ap-
plications can beneﬁt from communicating their preferences to the
network control-plane, as we show in §7.1. ident++ [35] proposes
an architecture in which an OpenFlow controller reactively queries
the endpoints of a new ﬂow to determine whether it should be ad-
mitted. TVA is a network architecture in which end-hosts authorize
the receipt of packet ﬂows via capabilities in order to prevent DoS-
attacks [50]. By contrast, PANE allows administrators to delegate
the privilege to install restricted network-wide ﬁrewall rules, and
users can do so either proactively or reactively (cf. §7.1.2).
Darwin [9] introduced a method for applications to request net-
work resources, including computation and storage capabilities in
network processors. Like PANE, Darwin accounts for resource use
hierarchically. However, it does not support over-subscription, lacks
(a)
(b)
(c)
Figure 7: Latency of switch operations in milliseconds.
support for access control and path management, and requires routers
with support for active networks. Yap, et al. have also advocated
for an explicit communication channel between applications and
software-deﬁned networks, in what they called software-friendly
networks [51]. This earlier work, however, only supports requests
made by a single, trusted application. By contrast, PANE’s ap-
proach to delegation, accounting, and conﬂict-resolution allow mul-