symmetry 
Article 
LogLS: Research on System Log Anomaly Detection Method Based on Dual LSTM
Yiyong Chen, Nurbol Luktarhan * and Dan Lv
College of Information Science and Engineering, Xinjiang University, Urumqi 830046, China; PI:EMAIL (Y.C.); PI:EMAIL (D.L.) 
* Correspondence: nurbol@xju.edu.cnAbstract: System logs record the status and important events of the system at different time periods. They are important resources for administrators to understand and manage the system. Detecting anomalies in logs is critical to identifying system faults in time. However, with the increasing size and complexity of today’s software systems, the number of logs has exploded. In many cases, the traditional manual log-checking method becomes impractical and time-consuming. On the other hand, existing automatic log anomaly detection methods are error-prone and often use indices or log templates. In this work, we propose LogLS, a system log anomaly detection method based on dual long short-term memory (LSTM) with symmetric structure, which regarded the system log as a natural-language sequence and modeled the log according to the preorder relationship and postorder relationship. LogLS is optimized based on the DeepLog method to solve the problem of poor prediction performance of LSTM on long sequences. By providing a feedback mechanism, it implements the prediction of logs that do not appear. To evaluate LogLS, we conducted experiments on two real datasets, and the experimental results demonstrate the effectiveness of our proposed method in log anomaly detection.|  | 
 | Keywords: system logs; anomaly detection; LSTM; time series forecasting |
|---|---|---|
| Citation: Chen, Y.; Luktarhan, N.; Lv, |Citation: Chen, Y.; Luktarhan, N.; Lv, |Keywords: system logs; anomaly detection; LSTM; time series forecasting |
D. LogLS: Research on System Log
Anomaly Detection Method Based on
| Dual LSTM. Symmetry 2022, 14, 454. https://doi.org/10.3390/sym14030454
Academic Editor: Juan Alberto Rodríguez Velázquez
Received: 25 January 2022 Accepted: 16 February 2022 Published: 24 February 2022
Publisher’s Note: MDPI stays neutral with regard to jurisdictional claims in published maps and institutional affil-iations.
 | Dual LSTM. Symmetry 2022, 14, 454. https://doi.org/10.3390/ 
sym14030454
Academic Editor: Juan Alberto Rodríguez VelázquezReceived: 25 January 2022 Accepted: 16 February 2022 Published: 24 February 2022
Publisher’s Note: MDPI stays neutral with regard to jurisdictional claims in published maps and institutional affil-iations.
 | Dual LSTM. Symmetry 2022, 14, 454. https://doi.org/10.3390/ 
sym14030454
Academic Editor: Juan Alberto Rodríguez Velázquez
Received: 25 January 2022 Accepted: 16 February 2022 Published: 24 February 2022Publisher’s Note: MDPI stays neutral with regard to jurisdictional claims in published maps and institutional affil-iations.
 | Dual LSTM. Symmetry 2022, 14, 454. https://doi.org/10.3390/ 
sym14030454
Academic Editor: Juan Alberto Rodríguez Velázquez
Received: 25 January 2022 Accepted: 16 February 2022 Published: 24 February 2022
Publisher’s Note: MDPI stays neutral with regard to jurisdictional claims in published maps and institutional affil-iations.| Dual LSTM. Symmetry 2022, 14, 454. https://doi.org/10.3390/ 
sym14030454
Academic Editor: Juan Alberto Rodríguez Velázquez
Received: 25 January 2022 Accepted: 16 February 2022 Published: 24 February 2022
Publisher’s Note: MDPI stays neutral with regard to jurisdictional claims in published maps and institutional affil-iations.
 | 1. IntroductionMany log files are often produced during the operation of modern systems. They reflect the running state of the system and record the activity information of specific events in the system. They are valuable resources to understand the state of the system. Therefore, system logs are an important data source for performance monitoring and anomaly detection and have become a research hotspot in the field of anomaly detection [1].At present, log anomaly detection can be roughly divided into three categories: rule-based anomaly detection, unsupervised anomaly detection, and supervised anomaly detection.
Rule-based exception detection [2] generally requires a manual analysis of logs and rule creation in advance, and the degree of automation is also low. For example, ref. [3] created rule sets based on analyzing log time series information, which effectively reduced the false-positive rate of the system but with low automation and high labor cost.With the unsupervised anomaly detection method [4], the manual consumption is reduced to a large extent, and there is no need for premarked training data. Anomaly detection can be performed by judging the difference between the log sequence to be detected and the normal log sequence. Wei Xu et al. [5] used abstract syntax trees (AST) and principal component analysis (PCA) to process the parsed log feature set, reduce the complexity of the feature set to be analyzed, and obtain effective exceptions test results. Jian-Guang Lou et al. [6,7] proposed an unstructured log analysis technology for anomaly detection, a new algorithm for automatically discovering program invariants in logs, and mining program invariants in log message groups, which revealed the inherent linearity of ||---|---|---|---|---|---|| Copyright: |© 2022 by the authors. |© 2022 by the authors. |© 2022 by the authors. |© 2022 by the authors. |1. Introduction Many log files are often produced during the operation of modern systems. They reflect the running state of the system and record the activity information of specific events in the system. They are valuable resources to understand the state of the system. Therefore, system logs are an important data source for performance monitoring and anomaly detection and have become a research hotspot in the field of anomaly detection [1]. At present, log anomaly detection can be roughly divided into three categories: rule-based anomaly detection, unsupervised anomaly detection, and supervised anomaly detection. Rule-based exception detection [2] generally requires a manual analysis of logs and rule creation in advance, and the degree of automation is also low. For example, ref. [3] created rule sets based on analyzing log time series information, which effectively reduced the false-positive rate of the system but with low automation and high labor cost. With the unsupervised anomaly detection method [4], the manual consumption is reduced to a large extent, and there is no need for premarked training data. Anomaly detection can be performed by judging the difference between the log sequence to be detected and the normal log sequence. Wei Xu et al. [5] used abstract syntax trees (AST) and principal component analysis (PCA) to process the parsed log feature set, reduce the complexity of the feature set to be analyzed, and obtain effective exceptions test results. Jian-Guang Lou et al. [6,7] proposed an unstructured log analysis technology for anomaly detection, a new algorithm for automatically discovering program invariants in logs, and mining program invariants in log message groups, which revealed the inherent linearity of || Licensee MDPI, Basel, Switzerland. This article is an open access article |Licensee MDPI, Basel, Switzerland. This article is an open access article |Licensee MDPI, Basel, Switzerland. This article is an open access article |Licensee MDPI, Basel, Switzerland. This article is an open access article |Licensee MDPI, Basel, Switzerland. This article is an open access article |1. Introduction Many log files are often produced during the operation of modern systems. They reflect the running state of the system and record the activity information of specific events in the system. They are valuable resources to understand the state of the system. Therefore, system logs are an important data source for performance monitoring and anomaly detection and have become a research hotspot in the field of anomaly detection [1]. At present, log anomaly detection can be roughly divided into three categories: rule-based anomaly detection, unsupervised anomaly detection, and supervised anomaly detection. Rule-based exception detection [2] generally requires a manual analysis of logs and rule creation in advance, and the degree of automation is also low. For example, ref. [3] created rule sets based on analyzing log time series information, which effectively reduced the false-positive rate of the system but with low automation and high labor cost. With the unsupervised anomaly detection method [4], the manual consumption is reduced to a large extent, and there is no need for premarked training data. Anomaly detection can be performed by judging the difference between the log sequence to be detected and the normal log sequence. Wei Xu et al. [5] used abstract syntax trees (AST) and principal component analysis (PCA) to process the parsed log feature set, reduce the complexity of the feature set to be analyzed, and obtain effective exceptions test results. Jian-Guang Lou et al. [6,7] proposed an unstructured log analysis technology for anomaly detection, a new algorithm for automatically discovering program invariants in logs, and mining program invariants in log message groups, which revealed the inherent linearity of || distributed |under |the |terms |and |1. Introduction Many log files are often produced during the operation of modern systems. They reflect the running state of the system and record the activity information of specific events in the system. They are valuable resources to understand the state of the system. Therefore, system logs are an important data source for performance monitoring and anomaly detection and have become a research hotspot in the field of anomaly detection [1]. At present, log anomaly detection can be roughly divided into three categories: rule-based anomaly detection, unsupervised anomaly detection, and supervised anomaly detection. Rule-based exception detection [2] generally requires a manual analysis of logs and rule creation in advance, and the degree of automation is also low. For example, ref. [3] created rule sets based on analyzing log time series information, which effectively reduced the false-positive rate of the system but with low automation and high labor cost. With the unsupervised anomaly detection method [4], the manual consumption is reduced to a large extent, and there is no need for premarked training data. Anomaly detection can be performed by judging the difference between the log sequence to be detected and the normal log sequence. Wei Xu et al. [5] used abstract syntax trees (AST) and principal component analysis (PCA) to process the parsed log feature set, reduce the complexity of the feature set to be analyzed, and obtain effective exceptions test results. Jian-Guang Lou et al. [6,7] proposed an unstructured log analysis technology for anomaly detection, a new algorithm for automatically discovering program invariants in logs, and mining program invariants in log message groups, which revealed the inherent linearity of || conditions of the Creative Commons Attribution (CC BY) license (creativecommons.org/licenses/by/ 4.0/). |conditions of the Creative Commons Attribution (CC BY) license (creativecommons.org/licenses/by/ 4.0/). |conditions of the Creative Commons Attribution (CC BY) license (creativecommons.org/licenses/by/ 4.0/). |conditions of the Creative Commons Attribution (CC BY) license (creativecommons.org/licenses/by/ 4.0/). |conditions of the Creative Commons Attribution (CC BY) license (creativecommons.org/licenses/by/ 4.0/). |1. Introduction Many log files are often produced during the operation of modern systems. They reflect the running state of the system and record the activity information of specific events in the system. They are valuable resources to understand the state of the system. Therefore, system logs are an important data source for performance monitoring and anomaly detection and have become a research hotspot in the field of anomaly detection [1]. At present, log anomaly detection can be roughly divided into three categories: rule-based anomaly detection, unsupervised anomaly detection, and supervised anomaly detection. Rule-based exception detection [2] generally requires a manual analysis of logs and rule creation in advance, and the degree of automation is also low. For example, ref. [3] created rule sets based on analyzing log time series information, which effectively reduced the false-positive rate of the system but with low automation and high labor cost. With the unsupervised anomaly detection method [4], the manual consumption is reduced to a large extent, and there is no need for premarked training data. Anomaly detection can be performed by judging the difference between the log sequence to be detected and the normal log sequence. Wei Xu et al. [5] used abstract syntax trees (AST) and principal component analysis (PCA) to process the parsed log feature set, reduce the complexity of the feature set to be analyzed, and obtain effective exceptions test results. Jian-Guang Lou et al. [6,7] proposed an unstructured log analysis technology for anomaly detection, a new algorithm for automatically discovering program invariants in logs, and mining program invariants in log message groups, which revealed the inherent linearity of ||  | | | | | |
Symmetry 2022, 14, 454. 
Symmetry 2022, 14, 454 2 of 21
the program workflow. Qingwei Lin et al. [8] proposed a method of log clustering for log problem identification. In log clustering, each log sequence was represented by a vector, the similarity value between the two log sequences was calculated, and the clustering hierarchy was applied. The hierarchical clustering grouped similar log sequences into clusters.Based on the supervised anomaly detection method [9], it is necessary to use the premarked data to perform log anomaly detection through a pretrained model. For ex-ample, ref. [10] used a log event counting vector to train a logistic regression model and calculated the abnormal probability of a log sequence. In reference [11], the log event counting vector was input into a support vector machine (SVM) to train the hyperplane, and the position relationship between the detection log sequence and the hyperplane was judged to determine the abnormal log sequence.With the development of deep learning [12], scholars have promoted the association between neural networks and log detection, and some abnormal log detection methods based on neural networks have gradually emerged [13], which have achieved good results in practical applications. Bin Xia et al. [14] proposed a generative adversary network based on long short-term memory (LSTM) that used a custom log parser to extract structured information and convert each log into an event. Min Du et al. [15] proposed a deep neural network model that used LSTM to model system logs as natural-language sequences. DeepLog obtained the log model from normal execution and performed anomaly detection on log data through this model. Compared with machine learning methods, DeepLog overcomes the deficiency of the recurrent neural network (RNN) and has high accuracy. However, the method for anomaly detection only considers the impact of the log’s pre-event on the current event while ignoring the postsequence when performing anomaly detection. The impact of events on current events is prone to deviations over time in a long sequence of events, which subsequently affects the accuracy of subsequent event predictions, lowering the confidence in the results. In terms of efficiency under detection, this method still has room for improvement.Zhang X et al. [16] used log event semantic information to extract log sequence context information through a bi-LSTM model to effectively improve the accuracy of log anomaly detection. To solve the problem of low detection efficiency of long sequence abnormal logs, Yang Ruipeng et al. [11] introduced a time convolutional neural network into abnormal log detection and replaced the fully connected layer with adaptive global average pooling, which effectively solved the overfitting problem and improved detection efficiency.To solve the problem that unstructured logs [17] are difficult to detect directly, this paper adds a filtering function based on the Spell [18,19] method to parse unstructured logs more accurately, which lays a good foundation for anomaly detection of log sequences. In this paper, a log path anomaly detection method based on a dual LSTM network is proposed, which fully considers the interaction and latent symmetry information between the events before and after and makes up for the problem that the log sequence anomaly detection method based on deep learning ignores the log time correlation. Compared with similar previous methods, this method has improved accuracy and F1-measure on HDFS datasets.The contributions of this paper are as follows:
1. 	By adding a filter step on Spell, the problem that the current log template is not 	accurate enough is solved, which is more conducive to the performance of the later 	log detection work.2. 	Two LSTM models are constructed according to the preorder and postorder relations 	of the log sequence. Combined with the two models, a complete log anomaly detection 	model is constructed, which fully considers the interaction of before and after log 	events and solves the problem of insufficient feature mining of log sequences.
3. 	This paper provides an update mechanism. It solves the problem of inaccurate 	detection of unseen log sequences.The rest of this article is organized as follows. In Section 2, we introduce the related work. In Section 3, we summarize the construction of the model from three aspects:
Symmetry 2022, 14, 454 3 of 21
log parsing, feature extraction, and anomaly detection. Finally, we evaluate the model’s performance in Section 4 and summarize this article in Section 5.
2. Preliminaries 
2.1. Log ParserThe first step of abnormal log detection is to collect the log, obtain the original data and preprocess it. After log collection, we preprocess the data. According to specific requirements, invalid information in the log data is further removed, including repeated information and useless information. Finally, we obtain the log data, as shown in Figure 1. Then, we parse the data, analyze the log structure and information, and obtain the log template for each event as the basis for subsequent feature extraction.Figure 1. HDFS log information sample.The purpose of the log parser is to normalize the structured log data. By parsing the log file to extract the log key, the log sequence is constructed, and the log data of different struc-tures are converted into log key sequences with the same format (sometimes called a log event sequence) [20]. For example, the log “081109 205931 13 INFO dfs.DataBlockScanner: Verification succeeded for blk_-4980916519894289629” in Figure 1 indicates that the content fragment is “Verification succeeded for blk_-4980916519894289629”, the constant part is“Verification succeeded for “, and the variable is “blk_-4980916519894289629”; variable parameters can be abstracted as *. The common constant in all similar log entries is called the log key, which can be used to indicate the log type. By replacing it with a log key k1 (e.g., Verification succeeded for (*)), we obtain a sequence {k1, k2, k3,. . . , ki}. The parameter value is a very valuable type of information in the log and reflects the operating status and performance of the system. Some parameter values can also be used as the identification of the execution sequence, such as block_id in the HDFS log, as a unique identifier, which can extract the log sequence of a specific module.At present, there are many automatic log analysis methods in the industry. CFG [21] mainly uses code analysis with AST and CFG algorithms. This method adopts the offline mode; with general accuracy, the construction time complexity is O(n3), and the search efficiency is high. LKE [22] first converts the free-format text of the log into log keys, and then clusters the obtained keys. By mapping each log message to a series of log keys, the log message can be converted into a log key sequence. The algorithm used in this method is clustering, which adopts offline mode and has average accuracy. The construction time complexity is O(n2), and the search efficiency is very low. Logarm [23] is an effective automatic log parsing method using an n-gram dictionary in natural-language processing. The method adopts the online mode, the construction time complexity is O(n), and the accuracy and efficiency are relatively high.Drain [24] uses a fixed-depth tree structure to represent log messages and effectively extract common templates. The method adopts the online mode, the construction time complexity is O(n), and the accuracy and search efficiency are relatively high. Spell is a more advanced log parsing tool. It uses the longest common subalgorithm (LCS) to parse logs in a stream. It is implemented by MIT’s logPAI team [25,26] as an open-source system and can analyze logs online. This article uses Spell to parse the log template from the log data, and manually filters, deduplicates, and merges the obtained templates to obtain the final log template that is used as the basis for log anomaly detection.Symmetry 2022, 14, 454 4 of 21
2.2. Architecture and Overview
The architecture of log anomaly detection is shown in Figure 2. It includes three main modules: log analysis, log key anomaly detection, and a workflow model for anomaly detection.
Figure 2. LogLS architecture.In the log parsing stage, the parsing results obtained by simply using the Spell method are not completely correct. Some log templates are duplicated or redundant, which leads to poor anomaly detection effects. In response to this situation, in this article, to improve the Spell method we add manual filtering to obtain the final log template list. Then, the unstructured log data are parsed into log events according to the log template list, and finally, the log event sequence file is obtained for model training.In the training phase, the training set is the log entries from the normal system execution path, i.e., the log events obtained after log parsing. For different system logs, there are different ways to construct a log event sequence. If the log has a unique identifier, such as an HDFS log, the log event sequence is constructed according to the unique identifier. If there is no unique identifier, such as the BGL log, we use a sliding window to construct the log event sequence. Using the log event sequence parsed in the log file for model training, we obtain prelog and postlog event sequence LSTM models . These two LSTM models are combined to form a complete anomaly detection model. Of course, how to combine these two models requires the use of another feature (length) of the log event sequence. During model training, we count the distribution of the length of the log event sequence for preliminary filtering of the log event sequence. Finally, according to the log verification dataset and many tuning experiments, we obtain the most suitable model parameters.In the detection phase, we use the log test dataset to perform model detection. First, we generate a log event sequence from the original log through the parsed log template and then determine the use method of the preorder and postorder detection models according to the sequence length characteristics. According to the selection of the model, a certain log event in the incoming log event sequence is detected, and whether the log event deviates from the prediction result of the normal log sequence is determined. If there is too much deviation, it is judged as anomaly. If the result of the judgment is normal, the next log event detection work is performed on the sequence until the entire log event sequence detection is completed or an anomaly log event that deviates from the normal log sequence is found. If a certain log event in the log sequence is predicted to be abnormal, the entire log sequence is marked as abnormal. This method provides a feedback mechanism, and logs marked as anomalies will be provided to the user for further operation. If the user finds that the detected abnormal marking result is falsely reported, the falsely reported log event sequence can be added to the training set. When there are many false positives, users can retrain the model according to the new training set to gradually update the model.Symmetry 2022, 14, 454 5 of 21
3. Methodology
This paper proposes a system log anomaly detection method based on dual LSTM. Through the cooperation of two LSTM models, the gradient problem of a single LSTM model in long sequence prediction [27] is solved, and the performance of anomaly detection is improved. Figure 2 shows the three main modules of this method: log parsing, log key anomaly detection model, and anomaly detection workflow model. In the rest of this section, we will cover the various parts of this method in detail.3.1. Log Parser (Spell)In the log parsing module, we convert the original log into a structured log through the log parser. Log parsing is considered a common preprocessing of unstructured logs and is an important part of most log analysis tasks. There are many parsing methods at present, among which Spell is currently the better one. It is a log parsing method based on the longest common subsequence (LCS). The time complexity of this method for processing each log entry e is close to linear (linearly related to the size of e) and unstructured log messages are parsed into structured log templates. Although Spell is a better log analysis method at present, the log template parsed by this method is not completely correct. If it is used directly to generate log events and compose a log event sequence for anomaly detection, it is difficult to achieve optimal results.The main dataset of this article is HDFS logs, which can well reflect the effectiveness of the LogLS method. Table 1 shows the HDFS log template parsed by the Spell method. There are 37 types of HDFS log templates obtained. We can see that some log templates are duplicated. For example, the three log event templates in {E7, E8, E9} are similar, and E8 can contain the other two. If we directly use these templates to generate log sequences, it will seriously affect the effect of log anomaly detection, so this method has room for improvement. This article adds manual deduplication steps to further optimize the effect of log parsing and achieve a higher level of detection.Table 1. The event template obtained from parsing the HDFS log by the spell method.
| EventId | EventTemplate |
|---|---|
| E1  E2  ... E7  E8  E9  ... E14  E15  E16  ... E36  E37 |Adding an already existing block (*)  Verification succeeded for (*)  ... writeBlock (*) received exception java.io.IOException Connection reset by peer writeBlock (*) received exception (*)  writeBlock (*) received exception java.io.IOException Could not read from stream ... PacketResponder (*)(*) Exception java.io.IOException Broken pipe PacketResponder (*) 2 Exception (*)  PacketResponder (*) 1 Exception (*)  ... Deleting block (*) file (*)  BLOCK* NameSystem.allocateBlock (*) (*) |To improve accuracy, we need to eliminate duplicate data in the log event template obtained by Spell. We do so by selecting a few representative log entries that conform to each event template and check whether they are repeated. If they are repeated, according to the frequency of the log event template, they are unified into the event template with the most frequent occurrences. The log events in Table 1 are reprocessed, and the results are shown in Table 2.Symmetry 2022, 14, 454 6 of 21
Table 2. HDFS log event template after processing.
| EventId | EventTemplate |
|---|---|| E1  E2  ... E7  E8  E9  ... E14  E15  E16  ... E28 E29 |Adding an already existing block (*)  (*)Verification succeeded for (*)  ... writeBlock (*) received exception (*)  PacketResponder (*) for block (*) Interrupted. Received block (*) of size (*) from (*)  ... Exception in receiveBlock for block (*) (*)  Changing block file offset of block (*) from (*) to (*) meta file offset to (*) (*):Transmitted block (*) to (*)  ... BLOCK* NameSystem.addStoredBlock: addStoredBlock request received for (*) on (*) size (*) However, it does not belong to any file. PendingReplicationMonitor timed out block (*) |In this article, we first use the system log template to divide the unstructured log into several parts (e.g., date, time, content, etc.), and then further extract meaningful information (e.g., events) from these parts. Usually, the event consists of three parts (time stamp, signature and parameters). Figure 3 illustrates the HDFS log parsing process. The signature attribute is the log template. The specific algorithm is implemented as follows:1. 	The initialization program first defines a log object (LCSObject) such as the log key 	(LCSseq) and line number list (lineIds), and defines a log object list (LCSMap), which 	is used to save each log object.
2. 	Enter the log file and read it line by line.3. 	Read a line of log, and then traverse the LCSMap to see if there is already an LCSObject 	in the list that has the same LCSseq (log key). If such an LCSObject exists, add the 	lineIds of this log to the lineIds of the LCSObject. If not, then generate a new LCSObject 	to LCSMap.
4. 	Keep reading the log until the end.
Figure 3. Example of log parser.
Symmetry 2022, 14, 454 7 of 21The template extracted from the log in Figure 3 is “PacketResponder (*) for block (*) terminating”, where “(*)” represents the variable parameter. If you enter a new log entry“PacketRespo-nder 2 for block blk_2529569087635823495 termina-ting”, Spell’s idea is not to extract the log key directly, but to extract it in comparison. After receiving the newly input log entry, the LCSMap is traversed and an LCSObject whose LCSseq is “PacketResponder 0 for block blk_6137960563260046065 terminating” is found. Then, the LCS is calculated as “PacketResponder for block terminating”. When the length of the sequence is between 1/2 and 1 times the length of the input entry, it is judged that it belongs to the same log key, so it is merged, and the lineIds of this log entry is added to the lineIds attribute of the LCSObject.Then, the obtained initial log template is manually filtered to obtain the final log event template. The log entries of the entire dataset are then processed to obtain log events of all log entries, which are used to form log event sequences and perform subsequent model training.