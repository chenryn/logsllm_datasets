large number of “new” LDNS servers appeared to be arriving over
time, as it appears that many are using dynamic IP addresses, many
of which are behind DSL, cable, or dial-up links. This diversity
probably contributes to the higher availability that we observe in
ADNS servers and suggests that unless guarded against, infrastruc-
tures deployed in a manner similar to LDNS servers will face more
heterogeneity than those deployed like ADNS servers.
5. APPLICATION TO DHTS
As an example of applying DNS measurements to the design of
a federated distributed system, we model the effect of dedicated
infrastructure with similar availability characteristics as DNS on a
Distributed Hash Table (DHT).8 A DHT is a fundamental building
block in several proposed Internet-scale infrastructures [18, 20,
28, 33, 35]. In such DHT-based systems, availability governs the
degree of churn, which impacts stability and performance [21, 29].
Figure 17 shows the setup of our simulation, which is similar to
that of the study conducted by Li et al. [21]. Infrastructure servers
could be deployed in a federated manner similar to LDNS — for
example, an infrastructure node could be used as a “well-known
node” that directs initial queries, thus serving a role similar to an
LDNS server.
8Note that we do not propose that DNS should be run on a DHT
(though others have proposed so), only that other services imple-
mented using a DHT might deploy infrastructure in a similar fash-
ion to that of DNS.
Parameter
Environment
Network topology
DHT algorithm
Lookup rate
Setting and Description
1024 nodes simulated in p2psim [26].
Pairwise latencies of 1024 DNS servers collected using the
King method [14] by Li et al. [21].
Chord [34] with proximity neighbor selection.
1 per node using exponentially distributed arrival intervals
with mean 2 minutes.
Time to Failure and Recovery
Client nodes
Server nodes
Modeled after clients seen in an operational peer-to-peer
ﬁle-sharing network [13]: Exponentially distributed time
to failure with mean 2.4 minutes. Time to recovery is also
exponentially distributed with mean 2.4 minutes.
Modeled after LDNS servers measured in our study:
37.8% use the empirical LDNS time to failure distribution
(see LDNS with short failures data in Figure 10); the rest
of the servers were never seen to fail, so we pessimistically
model them as failing with an exponentially distributed
mean time to failure of two weeks. All use the empirical
LDNS time to recovery distribution.
Figure 17: DHT simulation setup: Parameters used in our
DHT simulations.
Using parameters derived from our DNS measurements, we show
how dedicated infrastructure nodes can signiﬁcantly reduce the over-
head bandwidth in a DHT. Overhead bandwidth comes in two forms:
lookup trafﬁc and maintenance trafﬁc. Lookup trafﬁc scales pro-
portionally to the lookup rate and to the log of the size of the net-
work in Chord. Maintenance trafﬁc depends on the maintenance
interval, the inverse of how often a node checks that all of its neigh-
bors are still alive and repairs broken overlay links — longer inter-
vals incur less maintenance trafﬁc. If nodes leave frequently, the
maintenance interval must be shorter in order to prune stale routing
data. Likewise, if some nodes are part of a dedicated infrastruc-
ture, the interval can be longer. Efﬁciency mandates a maintenance
interval inversely proportional to the lookup rate or longer. But
reliability requires the maintenance interval to be proportional to
the average lifetime or shorter. Hence, when the lookup rate and
lifetime are inversely proportional, extending the lifetime will sub-
stantially decrease maintenance trafﬁc.
Figure 18 shows our simulation results, varying the fraction of
dedicated infrastructure nodes (servers) and end-user nodes (clients)
in the DHT. With a portion of nodes acting as dedicated infrastruc-
ture, we can achieve similar reliability while decreasing bandwidth
(or maintain bandwidth and improve reliability). For example, if
a quarter of the nodes are servers rather than clients, the network
requires roughly half the bandwidth to achieve similar reliabilities.
)
d
n
o
c
e
s
/
s
e
t
y
b
(
e
d
o
N
e
v
L
i
r
e
p
h
t
i
d
w
d
n
a
B
d
a
e
h
r
e
v
O
100
80
60
40
20
0
100%
80%
60%
40%
20%
e
t
a
R
e
r
u
l
i
a
F
p
u
k
o
o
L
1024 clients, 0 servers: Bandwidth
Failures
768 clients, 256 servers: Bandwidth
Failures
512 clients, 512 servers: Bandwidth
Failures
5 sec 10 sec
30 sec 1 min 2 min
5 min 10 min
30 min 1 hr
3 hr
6 hr
0%
12 hr
Maintainance Interval
Figure 18: Simulation results: This ﬁgure shows how vary-
ing the maintenance interval impacts the amount of overhead
trafﬁc incurred by each node in the DHT (dashed lines), and
the rate of failed lookups (solid lines). We vary the fraction
of dedicated infrastructure nodes (servers) and end-user nodes
(clients) in the DHT. Results from a DHT composed completely
of server nodes (1024 servers, 0 clients) are omitted for clarity,
but would have less than 0.2% lookups fail even with a main-
tenance interval of 12 hours and would follow the downward
trend of overhead trafﬁc.
Our simulation results show that having infrastructure with fail-
ure properties no better than that of LDNS servers allows DHT
maintenance to be performed much less frequently while still
achieving a high rate of successful lookups. Other observations
from our measurement study may also have implications for DHTs.
For example, we saw that the number of requests generated by
LDNS servers was highly skewed; hence in a federated DHT, there
may a few nodes that generate the majority of the lookups. In such
a scenario, measures might need to be taken in order to more fairly
balance the load required to route these lookups. In addition, cer-
tain deployment styles within organizations may be amenable to
more hierarchical overlay construction. We leave this for future
work.
6. RELATED WORK
In this section, we provide brief surveys of related work on DNS
characterization, availability measurements, and the impact of churn
on DHT performance.
6.1 DNS
Danzig et al. [11] presented the ﬁrst comprehensive study of
DNS trafﬁc in the wide area. Using measurements at a root name
server, they found a large number of misbehaving local DNS re-
solvers and analyzed the characteristics of DNS trafﬁc. Brownlee
et al. [6, 7] discuss more recent measurements taken at the root
name servers and continue to ﬁnd a large number of illegitimate
queries in DNS trafﬁc. Liston et al. [22] studies DNS trafﬁc from
the client vantage point and analyzes their diversity across sites.
Jung et al. [17] also examined DNS traces from two local DNS
servers and analyzed the impact of caching of A-records in DNS
as well as the setting of TTLs on client performance. Our study, in
contrast, looks primarily at the characteristics of the DNS infras-
tructure instead of the particular trafﬁc characteristics. Ramasub-
ramanian and Sirer [28] also examined characteristics of the au-
thoritative DNS hierarchy, such as the prevalence of bottlenecks in
name resolution and the number of nameservers containing known
security vulnerabilities.
Both Shaikh et al. [32] and Mao et al. [24] analyzed the prox-
imity of clients to their local DNS servers. They found that a fair
number of clients were not close to their local DNS server and their
performance could be improved by using a more proximal server,
such as one in their network aware cluster. Cranor et al. [10] looked
at the distribution of ADNS and LDNS servers found by looking at
DNS trafﬁc and grouped them using network aware clustering. We
performed a similar analysis by clustering based on domain names,
which are likely to reﬂect administrative domains.
6.2 Availability
Several recent studies [30, 5, 31] have analyzed the availability
of participants of peer-to-peer ﬁle-sharing systems. Long et al. [23]
studied the availability of hosts on the Internet, and their study is
perhaps the most similar to ours; however, we focus on the avail-
ability of dedicated infrastructure and our measurements are much
more recent (their study was conducted in 1995).
6.3 DHTs and Churn
Li et al. [21] and Rhea et al. [29] have examined the impact of
churn on DHTs and devise mechanisms for managing extremely
low mean time to failures. Our evaluation suggests that such mech-
anisms are unnecessary in an infrastructure based system because
the infrastructure allows for very low maintenance trafﬁc exchange
rates.
7. SUMMARY
In this paper, we presented measurements of a large number of
local and authoritative DNS servers and analyzed their load, avail-
ability, and deployment characteristics.
Our key ﬁndings are that a large fraction of all end-users use
a small number of local DNS servers. We found that a signiﬁ-
cant fraction of local DNS servers are highly available, without a
single observed failure, with authoritative servers generally having
higher availability. We found evidence that there is a slight pos-
itive correlation between usage and availability of DNS servers.
We also observed a large degree of diversity in local DNS server
deployment and usage: many servers originated from dynamic IP
addresses pools. Also, some servers exhibited diurnal availability
patterns. Finally, we observed that local DNS server deployments
within organizations also tend to be diverse, ranging from a very
few highly used servers to a very large number of lightly loaded
ones.
Our observations shed new light on characteristics of DNS in-
frastructure. They are are also important to the study of future in-
frastructure services deployed in a federated manner. For example,
we simulated a Distributed Hash Table using availability character-
istics similar to DNS and showed how much infrastructure support
improves reliability and decreases overhead.
Acknowledgments
We would like to thank Hui Zhang for initial feedback and our
anonymous reviewers for their valuable feedback and suggestions.
We would like to thank Steve Hill, Eric Olson, and Arthur Berger at
Akamai for helping us with our infrastructure and Jeremy Stribling
for helping us with p2psim. James Hendricks is supported in part
by an NDSEG Fellowship, which is sponsored by the Department
of Defense.
8. REFERENCES
[1] Not just another bogus list. http://dnsbl.njabl.org.
[2] University of Oregon, RouteViews Project.
http://www.routeviews.org.
[3] Akamai Technologies. Edgescape.
http://www.akamai.com/en/html/services/
edgescape.html, 2004.
[4] G. Ballintijin, M. van Steen, and A. S. Tanenbaum. Scalable
user-friendly resource names. IEEE Internet Computing,
5(5):20–27, 2001.
[5] R. Bhagwan, S. Savage, and G. M. Voelker. Understanding
availability. In 2nd International Workshop on Peer-to-peer
systems, December 2002.
[6] N. Brownlee, k claffy, and E. Nemeth. DNS measurements at
a root server. In Globecom, 2001.
[7] N. Brownlee, k claffy, and E. Nemeth. DNS Root/gTLD
performance measurements. In Proc. Passive and Active
Measurement workshop (PAM), 2001.
[8] Clients of DNS root servers.
http://www.caida.org/ kkeys/dns/, 2002.
[9] R. Cox, A. Muthitacharoen, and R. Morris. Serving DNS
using Chord. In Proceedings of the 1st International
Workshop on Peer-to-Peer Systems (IPTPS), Cambridge,
MA, March 2002.
[10] C. D. Cranor, E. Gansner, B. Krishnamurthy, and
O. Spatscheck. Characterizing large DNS traces using
graphs. In Proceedings of the First ACM SIGCOMM
Workshop on Internet Measurement, pages 55–67, 2001.
[11] P. Danzig, K. Obraczka, and A. Kumar. An analysis of
wide-area name-server trafﬁc. In Proceedings of the
SIGCOMM ’92 Symposium on Communications
Architectures and Protocols, 1992.
[12] N. Feamster, D. G. Andersen, H. Balakrishnan, and M. F.
Kaashoek. Measuring the effects of internet path faults on
reactive routing. In Proceedings of the ACM Sigmetrics
2003, pages 126–137, 2003.
[13] K. P. Gummadi, R. J. Dunn, S. Saroiu, S. D. Gribble, H. M.
Levy, and J. Zahorjan. Modeling and analysis of a
peer-to-peer ﬁle-sharing workload. In Proceedings of the
19th Symposium on Operating System Principles, October
2003.
[14] K. P. Gummadi, S. Saroiu, and S. D. Gribble. King:
Estimating latency between arbitrary Internet end hosts. In
Proceedings of Internet Measurement Workshop 2003, pages
5–18, 2002.
[15] IRCache. http://www.ircache.net/.
[16] ISC Internet Survey.
http://www.isc.org/ops/ds/new- survey.php.
[17] J. Jung, E. Sit, H. Balakrishnan, and R. Morris. DNS
performance and the effectiveness of caching. IEEE/ACM
Trans. Netw., 10(5):589–603, 2002.
[18] B. Karp, S. Ratnasamy, S. Rhea, and S. Shenker. Spurring
adoption of DHTs with OpenHash, a public DHT service. In
Proceedings of the 3rd International Workshop on
Peer-to-Peer Systems (IPTPS’04), February 2004.
[19] B. Krishnamurthy and J. Wang. On network-aware clustering
of web clients. In Proceedings of the SIGCOMM ’00
Symposium on Communications Architectures and Protocols,
pages 97–110, 2000.
[20] J. Kubiatowicz, D. Bindel, Y. Chen, S. Czerwinski, P. Eaton,
D. Geels, R. Gummadi, S. Rhea, H. Weatherspoon,
W. Weimer, C. Wells, and B. Zhao. Oceanstore: An
architecture for global-scale persistent storage. In
Proceedings of the Ninth International ACM Conference on
Architectural Support for Programming Languages and
Operating Systems, November 2000.
[21] J. Li, J. Stribling, T. M. Gil, R. Morris, and M. F. Kaashoek.
Comparing the performance of distributed hash tables under
churn. In Proc. of the 3rd IPTPS, February 2004.
[22] R. Liston, S. Srinivasan, and E. Zegura. Diversity in DNS
performance measures. In Proceedings of the SIGCOMM ’02
Symposium on Communications Architectures and Protocols,
pages 19–31, 2002.
[23] D. D. E. Long, A. Muir, and R. A. Golding. A longitudinal
survey of Internet host reliability. In Proc. Symposium on
Reliable Distributed Systems, pages 2–9, 1995.
[24] Z. M. Mao, C. D. Cranor, F. Douglis, M. Rabinovich,
O. Spatscheck, and J. Wang. A precise and efﬁcient
evaluation of the proximity between web clients and their
local DNS servers. In Proceedings of the USENIX 2002
Annual Technical Conference, pages 229–242. USENIX
Association, 2002.
[25] P. V. Mockapetris. Domain names - concepts and facilities.
Request for Comments 1034, Internet Engineering Task
Force, November 1987.
[26] P2Psim. http://pdos.lcs.mit.edu/p2psim.
[27] V. Paxson. End-to-end routing behaviour in the internet. In
Proceedings of the SIGCOMM ’96 Symposium on
Communications Architectures and Protocols, September
1996.
[28] V. Ramasubramanian and E. Sirer. The design and
implementation of a next generation name service for the
Internet. In Proceedings of the SIGCOMM ’04 Symposium on
Communications Architectures and Protocols, August 2004.
[29] S. Rhea, D. Geels, T. Roscoe, and J. Kubiatowicz. Handling
Churn in a DHT. In Proceedings of the USENIX 2004
Annual Technical Conference, June 2004.
[30] S. Saroiu, P. K. Gummadi, and S. D. Gribble. A
measurement study of peer-to-peer ﬁle sharing systems. In
Proceedings of Multimedia Computing and Networking
(MMCN’02), January 2002.
[31] S. Sen and J. Wang. Analyzing peer-to-peer trafﬁc across
large networks. In Proc. ACM SIGCOMM Internet
Measurement Workshop (IMW), 2002.
[32] A. Shaikh, R. Tewari, and M. Agrawal. On the effectiveness
of DNS-based server selection. In Proceedings of the IEEE
INFOCOM 2001, Anchorage, Alaska, 2001.
[33] I. Stoica, D. Adkins, S. Zhuang, S. Shenker, and S. Surana.
Internet Indirection Infrastructure. In Proceedings of the
SIGCOMM ’02 Symposium on Communications
Architectures and Protocols, August 2002.
[34] I. Stoica, R. Morris, D. Karger, F. Kaashoek, and
H. Balakrishnan. Chord: A scalable peer-to-peer lookup
service for internet applications. In Proceedings of the
SIGCOMM ’01 Symposium on Communications
Architectures and Protocols, 2001.
[35] M. Walﬁsh, H. Balakrishnan, and S. Shenker. Untangling the
web from DNS. In Proceedings of the First Usenix
Symposium on Networked System Design and
Implementation (NSDI’04), San Francisco, CA, March 2004.
[36] R. Wolff. Poisson arrivals see time averages. Operations
Research, 30(2):223–231, 1982.