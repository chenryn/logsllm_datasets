However, since the permutation itself changes on ev-
ery rescan, the set of machines protected is constantly
changing. The result is that unless a very large number
of uninfected machines respond to probes like an actual
worm, the protection is almost nonexistent.
4.3 Simulation of a Warhol Worm
A combination of hit-list and permutation scanning can
create what we term a Warhol worm, capable of attack-
ing most vulnerable targets in well under an hour, possi-
bly less than 15 minutes. Hit-list scanning greatly im-
proves the initial spread, while permutation scanning
keeps the worm’s infection rate high for much longer
when compared with random scanning.
In order to evaluate the effects of hit-list and permuta-
tion scanning, we wrote a small, abstract simulator of a
Warhol worm’s spread. The simulator assumes complete
connectivity within a 232 entry address space4 using a
pseudo-random permutation to map addresses to a sub-
4In general, the Internet address space isn’t completely connected.
If a machine is not reachable from an arbitrary point on the external
0100,000200,000300,000012345678Time (hours)Number of InstancesSimulationK = 2.6, T = 5.52Figure 7: The spread of three simulated worms in a popu-
lation of 300,000 vulnerable machines: (i) a Code Red-like
worm capable of 10 scans/second, (ii) a faster scanning worm
capable of 100 scans/second, and (iii) a Warhol worm, capable
of 100 scans/second, using a 10,000 entry hit-list and permu-
tation scanning which gives up when 2 infected machines are
discovered without ﬁnding a new target. All graphs stop at
99.99% infection of the simulated address space.
set of vulnerable machines. We used a 32-bit, 6-round
variant of RC5 to generate all permutations and random
numbers.
the
We can parameterize the simulation in terms of:
number of vulnerable machines in the address space;
scans per second; the time to infect a machine; num-
ber infected during the hit-list phase; and the type of
secondary scan (permutation, partitioned permutation,
or random). The simulator assumes multithreaded scan-
ning.
To ensure that the simulator produces reasonable re-
sults, Figure 6 shows a comparison between the simu-
lator’s output and the model developed in Section 2, for
a worm capable of 10 scans/second in a population of
300,000 vulnerable machines. The simulation results ﬁt
the model for K = 2.6 and T = 5.52. This represents a
worm which is slightly faster (less than 50%) than Code
Red I.
Figure 7 then shows how both faster scanning and the
Warhol strategies affect the propagation time. The faster
scanning worm (capable of 100 scans/second) reduces
the infection time down to under an hour, while the com-
bination of hit-list scanning, permutation scanning, and
fast scanning, further reduces infection time to roughly
network, it is usually not reachable directly by a worm except through
local scanning.
Figure 8: A closeup of the behavior of the Warhol worm
seen in Figure 7. The infection initially progresses rapidly—
effectively all worms are actively scanning the net—but as in-
fection rates near 100%, many worms have gone dormant, cor-
rectly concluding that there are few vulnerable machines re-
maining and should therefore cease scanning.
15 minutes.
Figure 8 shows in more detail the behavior of the Warhol
strategies. It gets a huge boost from the hit-list during the
ﬁrst few seconds of propagation, quickly establishing it-
self on the network and then spreading exponentially. As
the infection exceeds the 50% point, some of the worms
begin recognizing that saturation is occurring and stop
scanning. By the time the graph ends (at 99.99% of
the simulated population), most of the worms have gone
silent, leaving a few remaining worms to ﬁnish scanning
the last of the address space.
4.4 Topological Scanning
An alternative to hit-list scanning is topologically aware
scanning, which uses information contained on the vic-
tim machine in order to select new targets. Email worms
have used this tactic since their inception, as they harvest
addresses from their victim in order to ﬁnd new poten-
tial targets, as did the Morris worm (necessary because
of the very sparse address space when it was released)
[Sp89, ER89].
Many future active worms could easily apply these tech-
niques during the initial spread, before switching to
a permutation scan once the known neighbors are ex-
hausted. An active worm that attacked a ﬂaw in a peer-
to-peer application could easily get a list of peers from
0100,000200,000300,00001234678Time (hours)Number of InstancesConventionalFast ScanningWarhol0100,000200,000300,0000246810121416Time (minutes)Infected MachinesInfected MachinesActive WormsDormant Wormsa victim and use those peers as the basis of its attack,
which makes such applications highly attractive targets
for worm authors. Although we have yet to see such a
worm in the wild, these applications must be scrutinized
for security. These applications are also vulnerable to
contagion worms, as discussed in Section 5.
Similarly, a worm attacking web servers could look for
URLs on disk and use these URLs as seed targets as well
as simply scanning for random targets. Since these are
known to be valid web servers, this would tend to greatly
increase the initial spread by preferentially probing for
likely targets.
4.5 Flash Worms
We further observe that there is a variant of the hit-list
strategy that could plausibly result in most of the vul-
nerable servers on the Internet being infected in tens of
seconds. We term this a ﬂash worm.
The nub of our observation is that an attacker could plau-
sibly obtain a hit-list of most servers with the relevant
service open to the Internet in advance of the release of
the worm.5
In addition to the methods already discussed for con-
structing a hit-list in Section 4.1, a complete scan of the
Internet through an OC-12 connection would complete
quickly. Given a rate of 750,000 TCP SYN packets per
second (the OC-12 provides 622 Mbps, the TCP seg-
ment takes 40 bytes, and we allow for link-layer fram-
ing), and that the return trafﬁc is smaller in volume
than the outbound (it is comprised of either same-sized
SYN ACKs or RSTs, smaller ICMPs, or, most often, no
response at all), it would take roughly 2 hours to scan
the entire address space. Faster links could of course
scan even faster. Such a brute-force scan would be easily
within the resources of a nation-state bent on cyberwar-
fare.
Given that an attacker has the determination and fore-
sight to assemble a list of all or most Internet connected
addresses with the relevant service(s) open, a worm can
spread most efﬁciently by simply attacking addresses on
that list. For example, there are about 12.6 million Web
servers on the Internet (according to Netcraft [Ne02]), so
the size of that particular address list would be 48 MB,
uncompressed. The initial copy of the worm can be pro-
5Servers behind load balancers create complications here, as do
machines that connect to the Internet with variable IP addresses but
nonetheless have vulnerable services open.
grammed to divide the list into n blocks, and then to ﬁnd
and infect the ﬁrst address in each block (or an especially
chosen high-bandwidth address in that block), and then
hand the child worm the list of addresses for that block.
That copy of the worm can then re-iterate the process to
infect everything in its block. A threaded worm could
begin infecting hosts before it had received the full host
list from its parent to work on, to maximize the paral-
lelization process, and it could start work on looking for
multiple children in parallel.
This design is somewhat fragile if an early copy of the
worm is neutralized very quickly, or infects a site from
which it cannot scan out. To mitigate this, the worm
copies could overlap in their scanning so that all ad-
dresses were scanned a small number of times, with
every target address being scanned by different paths
through the infection tree. This has the additional side-
effect of removing the need for further parent-to-child
communication after initial infection occurs.
A related design would call for most of the address list
to be located in pre-assigned chunks on one or a num-
ber of high-bandwidth servers that were well-known to
the worm. Each copy of the worm would receive an as-
signment from its parent, and then fetch the address list
from there. The server would only have to send out por-
tions of the list, not the entire list; in principle, it should
only have to transmit each address in the list once. In ad-
dition, after the worm has propagated sufﬁciently that a
large number of copies are attempting to fetch their (now
quite small) lists, at that point the worm collective could
switch to sending around the address list with each new
infection, rather than having the infectees each contact
the server.
This process will result in relatively little wasted effort.
For example, if the worm had a list of Web servers, and
a zero-day IIS vulnerability, about 26% of the list would
be vulnerable. No server would be probed twice.
If
n = 10, then the infection tree for the 3 million vulner-
able servers would be just 7 layers deep.
The spread rate of such a worm would likely be con-
strained by one of two things. The worm itself is likely
to be small (Code Red I was about 4 KB, and a highly
malicious worm could easily be less than 100 KB, even
allowing for a complex payload). Thus, at the start, the
address list is much larger than the worm itself, and the
propagation of the worm could be limited by the time re-
quired to transmit the host list out of the initial infection
site or servers where it was stored. Since all the children
of the infection will have much smaller lists to transmit,
these later lists are less likely to limit the worm spread
(unless a ﬁrst generation child has less than 1/n of the
initial copy’s bandwidth available to it). The exact time
required to transmit the list will depend on the available
bandwidth of the storage sites. As an example, however,
we point out that a 48 MB address list could be pushed
down an OC-12 link in less than a second.6
Thus, starting the worm on a high-bandwidth link is
desirable for the attacker, and bandwidth is probably a
concern at the next layer or two. Compression of the
list could make the list delivery much faster.
Indeed,
we took a sorted list of the 9 million server addresses
discussed in Section 5 and found that gzip compression
shrinks the list from 36 MB to 13 MB, and differencing
the addresses prior to compression reduced it to 7.5 MB.
Another possible limitation is simply the latency re-
quired to infect each new layer in the tree. Given that
probes can be issued in parallel, and substantially more
threads can be spawned than n (the number of children),
we do not have to add up the time required for a given
copy to cycle through its list, but simply take the maxi-
mum infection latency. A single second is a reasonable
latency, but with n = 10 and a large hit-list to trans-
fer, it might take a little longer to get 10 copies of the
worm through a given site’s link. However, not much
longer—if a 5 KB worm can get 50% utilization through
a 256 Kbps DSL uplink, it can transmit ten copies of it-
self in three seconds. That leads to a sub-thirty-second
limit on the total infection time, given an infection tree
seven layers deep and a design where the new worm chil-
dren go to a server for their addresses. (An additional
concern here is the possibility of elements of the worm
interfering with one another, either directly, by induc-
ing congestion, or indirectly, for example by overﬂowing
ARP tables, as happened during the Code Red I outbreak
[SA01]. These possibilities are difﬁcult to analyze.)
In conclusion, we argue that a compact worm that be-
gins with a list including all likely vulnerable addresses,
and that has initial knowledge of some vulnerable sites
with high-bandwidth links, appears able to infect almost
all vulnerable servers on the Internet in less than thirty
seconds.
6 Or, if we model TCP slow start, then assuming an RTT of
100 msec (high), 1500 byte segments, an initial window of 1 segment,
and the use by the receiver of delayed acknowledgments, the transfer
takes 2.3 seconds, using equation (10) of [CSA00]. Since we con-
trol the receiver, we could perhaps turn off delayed acknowledgments,
which lowers this to 1.5 seconds. We could even skip congestion con-
trol entirely, but that runs the serious risk of lengthening the transfer
time by inducing packet loss, requiring retransmission.
5 Stealth worms—contagion
The great speed with which the worms described in the
previous sections can propagate presents a grave threat
to the Internet’s security, because there is so little time
available to react to their onset. Still, there might be
a possibility of devising mechanisms that automatically
detect the spread of such worms and shut them down
in some fashion [MSVS02]. Such mechanisms would
likely be triggered by the singular communication pat-
terns the worms evince—hosts generating much more
diverse and rapid Internet trafﬁc than they usually do.
We now turn to a different paradigm of worm prop-
agation, contagion, which, while likely spreading sig-
niﬁcantly slower than the rapidly-propagating worms,
evinces almost no peculiar communication patterns. As
such these worms could prove much more difﬁcult to de-
tect and counter, allowing a patient attacker to slowly but
surreptitiously compromise a vast number of systems.
The core idea of the contagion model can be expressed
with the following example. Suppose an attacker has
attained a pair of exploits: Es, which subverts a popular
type of Web server; and Ec, which subverts a popular
type of Web client (browser). The attacker begins the
worm on a convenient server or client (it doesn’t matter
which, and they could start with many, if available by
some other means), and then they simply wait.
If the
starting point is a server, then they wait for clients to visit
(perhaps baiting them by putting up porn content and
taking care that the large search engines index it). As
each client visits, the subverted server detects whether
the client is vulnerable to Ec. If so, the server infects it,
sending along both Ec and Es. As the client’s user now
surfs other sites, the infected client inspects whether the
servers on those sites are vulnerable to Es, and, if so,
again infects them, sending along Ec and Es.
In this fashion, the infection spreads from clients to
servers and along to other clients, much as a contagious
disease spreads based on the incidental trafﬁc patterns of
its hosts.
Clearly, with the contagion model there are no unusual
communication patterns to observe, other than the larger
volume of the connections due to the worm sending
along a copy of itself as well as the normal contents of
the connection—in the example, the URL request or the
corresponding page contents. Depending on the type of
data being transferred, this addition might be essentially
negligible (for example, for MP3s). Thus, without an
analyzer speciﬁc to the protocol(s) being exploited, and
which knows how to detect abnormal requests and re-
sponses, the worm could spread very widely without de-
tection (though perhaps other detection means such as
Tripwire ﬁle integrity checkers [Tw02] might discover
it).
In addition to exploiting the natural communication pat-
terns to spread the worm, these might also be used by the
attacker to then control it and retrieve information from
the infected hosts, providing that the endemic trafﬁc pat-
terns prove of sufﬁcient frequency and volume for the
attacker’s purposes. (Or, of course, the attacker might
more directly command the infected hosts when the time
is ripe, “blowing their cover” in the course of a rapid
strike for which keeping the hosts hidden can now be
sacriﬁced.)
As described above, one might ﬁnd contagion worms a
clear theoretical threat, but not necessarily such a grave
threat in practice. The example requires a pair of ex-
ploits, and will be limited by the size of the populations
vulnerable to those attacks and the speed with which
Web surﬁng would serve to interconnect the populations.
While some argue the Web exhibits the “small world”
phenomenon [Br+00], in which the distance between
different Web items in the hypertext topology is quite
low, this doesn’t necessarily mean that the dynamic pat-
terns by which users visit that content exhibit a similar
degree of locality.
We now present a more compelling example of the la-
tent threat posed by the contagion model, namely lever-
aging peer-to-peer (P2P) systems. P2P systems gener-
ally entail a large set of computers all running the same
software. Strictly speaking, the computers need only all
run the same protocol, but in practice the number of
independent implementations is quite limited, and it is
plausible that generally a single implementation heavily
dominates the population.
Each node in the P2P network is both a client and a
server.7 Accordingly, the problem of ﬁnding a pair of
exploits to infect both client and server might likely be
reduced to the problem of ﬁnding a single exploit, signif-
icantly less work for the attacker. P2P systems have sev-
eral other advantages that make them well suited to con-
tagion worms: (i) they tend to interconnect with many
different peers, (ii) they are often used to transfer large
ﬁles, (iii) the protocols are generally not viewed as main-
stream and hence receive less attention in terms of moni-
toring by intrusion detection systems and analysis of im-