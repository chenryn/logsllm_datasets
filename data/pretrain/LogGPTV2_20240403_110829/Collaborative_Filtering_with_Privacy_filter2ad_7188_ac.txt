together Wij for j = 1, . . . , km which will be an encryp-
tion of the sum of the squares of elments of Gi, i.e.
the
squared 2-norm of Gi. Then prover gives a ZKP that this
value, call it ν is at most L2, where L is the desired bound
on the 2-norm of G.
The bound on ν is built by expressing ν as a weighted
sum of 2 log2 L binary-valued variables (bits), and then
showing in zero knowledge that each bit has value 0 or
1. Cramer and Damg˚ard [3] give zero-knowledge, honest
veriﬁer proof that a given encrypted value is 0 or 1. Their
method can be implemented non-interactively by hashing
the veriﬁer’s response, which is best for our application. In
that case each ZKP comprises 7 long (mod p) integers. So
to prove that ν has at most t bits requires 7t large (typically
160- or 1024-bit) integers. The length required for our pro-
tocol is t = O(log mk). Note that there is only one such
proof for each gradient vector Gi.
Proceedings of the 2002 IEEE Symposium on Security and Privacy (S&P(cid:146)02) 
1081-6011/02 $17.00 ' 2002 IEEE 
6
It remains for the prover to show in zero knowledge that
each Wij is the encryption of the square of the value en-
crypted by Γij, for j = 1, . . . , km. This requires a modiﬁ-
cation of the multiplication protocol from [3] which we give
in Appendix II. Since we only need to deal with squares,
the proof in Appendix II is shorter than the general multi-
plication proof. When implemented non-interactively, each
proof requires a ﬁxed number of integers from Zp. While
the original protocol from [3] required ten large integers per
multiplication proof, the protocol we give in Appendix II re-
quires seven. This is a useful saving in the overall commu-
nication cost of the protocol, which is dominated by these
ZKPs. Putting the two proofs together (for ν and the Wij)
shows that total size of the proof of validity of Gi is
7km + O(log km)
large integers
.
3.4 Tallying and Threshold Decryption
The tallyer computes for each j the product of all the
homomorphic images that it receives:
n(cid:4)
n(cid:4)
n(cid:3)
Xj =
xij
Yj =
yij (mod p)
i=1
i=1
and we notice that Yj = γTj hRj and Xj = gRj where
Tj =
Gij
and
Rj =
i=1
i=1
n(cid:3)
rij
so (Xj, Yj) is an El-Gamal encryption of the desired sum
Tj. To decrypt, we broadcast Xj to all clients.
Each client that receives Xj should apply their share of
j (mod p) to the tallyer.
the secret key to it, and send X si
Assume that for each j, the tallyer receives at least t + 1 re-
sponses from some set Λ of clients. Then tallyer computes:
Pj =
(X si
J )Li,Λ = gsRj = hRj (mod p)
(cid:4)
i∈Λ
Finally, the tallyer computes: YjP −1
j = γTj (mod p).
Although computing Tj requires taking a discrete log, the
values of Tj will be small enough (106 to 109) that a baby-
step/giant-step approach will be practical. This can be done
by many of the clients in parallel to speed up the process. In
(cid:1)|Tj| steps, the value of Tj will be found, and the client can
send this info directly to the tallyer for veriﬁcation, since it
is public.
3.5 Checking Inputs and Tallys
The ZKPs and calculations done by each tallyer in our
scheme are “publicly” veriﬁable, as in [5]. However, [5]
gave no scheme for explicitly checking tallys. Check-
ing totals appears to be difﬁcult without seeing the inputs,
which is clearly not efﬁcient. A simple approach to efﬁ-
cient checking is to use randomly sampled redundant tal-
lyers, and take the majority for each tally. First, assuming
km ≤ n, we compute each of the km totals with a different
group of tallyers. In the second case of km > n, we dis-
tribute the km values into n groups so that each group has
at most (cid:10)km/n(cid:11) totals to compute. The number of groups
in either case is min(km, n).
To choose which tallyers lie in which group, we
rely on the global coin toss. The number of random
bits needed to allocate tallys to groups of tallyers is
O(min(km, n) log2(km + n)).
The majority value(s)
among the tallyers in a group will determine the value(s)
used in subsequent calculations. Let α be the fraction of
honest tallyers. When nr tallyers are chosen at random us-
ing global coin tosses, the majority scheme will succeed if
most of tallyers are honest. Now α > 0.5, and the expected
number of honest tallyers nh in the sample is E(nh) = αnr.
The scheme will fail if the number of honest tallyers in the
sample nh  8.5(log2 min(km, n) + log2(1/p))
(3)
If we choose instead α = 0.7 then the constant above in-
creases from 8.5 to 15. Choosing α = 0.6 causes the con-
stant to increase to 50.
4 Protocol
Here we summarize the entire method. As before there
are n clients and m items, and A has dimensions k × m.
First the procedure for computing a least-squares ﬁt and par-
tial SVD of the training data. Assume A has been initialized
k×m. All users know this matrix.
to a random matrix in R
Repeat numiter times:
1. All clients compute their contribution to the gradient
vector, which is AP T
i Pi for client i. They compute
ZK proofs that their data are valid and write all of this
to the blackboard. O(km) computation and commu-
nication cost per client. The total number of integers
written by each client is 8km + O(log km).
2. Using the global coin toss, each tallyer chooses a sub-
set of O(log n) clients. The tallyer checks the ZKPs of
these clients and posts the results either “OK” or “not-
OK” to the blackboard. This requires O(km log n)
computation and computation per tallyer.
3. Each tallyer reads the results of ZKPs checks in the
previous step. For each client with a majority of OK
votes, the tallyer commits to add that client’s data to its
total. The tallyer reads the global coin toss and chooses
a subset of items to total. The total for the chosen items
and the valid clients is then written to the blackboard.
This requires O(km log n) communication and com-
putation cost per tallyer.
4. Clients compare encrypted totals from approved tally-
ers (those selected by the global coin toss) and if there
is a clear majority for a total, they decrypt it using their
share of the secret key. They write these to the black-
board. Cost is O(km log n) computation and commu-
nication per client.
5. Tallyers collect partial decryptions from clients (which
are easily veriﬁed using each client’s public key) for
the data items for which they are responsible, which
is O((km/n) log n) items per tallyer. They combine
these to produce decrypted totals, still as exponentials.
√
Each tallyer then computes the discrete logs of those
n) baby-step/giant-step method,
totals using an O(
and writes these values to the blackboard. These are
now fully decrypted coefﬁcients of the gradient of A.
Total cost is dominated by O(km log n) per tallyer.
6. Tallyers read the blackboard and take the majority
vote among approved tallyers for gradient coefﬁcients
for which they were not responsible. At the end of
this process, every honest tallyer should have a com-
plete copy of the new gradient. This process takes
O(km log n) steps per tallyer.
7. The conjugate gradient algorithm also requires a line
minimization step (Appendix I). This part of the pro-
tocol is a repeat of steps 1-5 above, except that there
are only 3 line coefﬁcients (ci, ai, bi) instead of km
gradient coefﬁcients. We assume this has been done,
and now every honest tallyer has a copy of the new
gradient and the line coefﬁcients.
8. Tallyers update the estimate of A using the decrypted
line coefﬁcients and conjugate gradient as described in
Appendix I. They also compute the partial SVD ma-
trices D and V . These are written to the blackboard.
For efﬁciency, each tallyer does this only for the coefﬁ-
cients for which it is responsible. Cost is O(km log n)
per tallyer.
Proceedings of the 2002 IEEE Symposium on Security and Privacy (S&P(cid:146)02) 
1081-6011/02 $17.00 ' 2002 IEEE 
7
9. Tallyers take majority vote for items for which they
were not responsible. The result is that all honest tal-
lyers have updated values for A, D and V . Cost is once
again O(km log n) per tallyer.
As we mentioned earlier, the typical number of iterations is
40-60 for convergence on real collaborative ﬁltering data.
Note that at each step of the protocol, incoming data is
checked for validity. In step 2, this is done using ZKP. In
step 4 this is through a user’s public key which immediately
veriﬁes their decryption of the data. In the other steps, ver-
iﬁcation is through majority vote of approved tallyers using
the global coin toss. If a sufﬁcient majority of tallyers is
honest, this yields the correct result with high probability.
By totaling the computational effort, we arrive at the fol-
lowing:
Lemma The total computation per client/tallyer during one
round of the protocol is O(km log n).
We have not said anything yet about sychronizing this
protocol. The shared blackboard makes this a fairly sim-
ple process. We can declare each round complete when a
pre-speciﬁed fraction (e.g. 70%) of clients or tallyers have
written their data to the blackboard. All honest clients and
tallyers would then always work on the same data, no mat-
ter what was written later. This fraction would need to be
determined experimentally, once it was known how many
of the possible clients typically participate.
4.1 Experiments
implement
We did not
the cryptographic protocols
above. This would have been reasonably straightforward,
but tedious. Since we can prove their desired privacy prop-
erties, we would not have learned anything by implement-
ing them. Performance is fairly easy to estimate with-
out implementation, because all of the cryptographic op-
erations have well-characterized running times using, e.g.
the CRYPTO++ toolkit, and are much more expensive than
other operations.
But it was far from clear whether the numerical method
was practical. How fast would it converge on typical data?
Would it be sensitive to noise? How large should k be for
good predictions? Is it competitive with existing collabo-
rative ﬁltering schemes? Therefore we focussed our imple-
mentation on the numerical method.
We tested the numerical method on the EachMovie
dataset, a well-known test dataset for collaborative ﬁlter-
ing algorithms [2]. This dataset comprises ratings of 1648
movies by 74422 users. Each rating is an integer in the
range 0, . . . , 5. We normalized the ratings to −2.5, . . . , 2.5
so that there was no zero rating to be confused with an ab-
sent rating. We chose 40% of the users at random as a train-
Proceedings of the 2002 IEEE Symposium on Security and Privacy (S&P(cid:146)02) 
1081-6011/02 $17.00 ' 2002 IEEE 
8
ing set. The ratings of these users became the P matrix on
which we ran the iterative least-squares procedure.
The implementation was done as a Matlab script ﬁle. It
was run on a 500MHz processor with 256MB of memory.
There were no clients, so all calculations were done on this
machine. The dimension of the linear space A was k = 8
for these experiments. This was found to give best perfor-
mance in cross-validation experiments. The average time
per iteration was about 1 second, and 40 iterations - the en-
tire training phase for 74422 users - was completed in under
one minute. Typical convergence rates were very fast: 10-
fold residual error reduction every 10 iterations. The error
reductions for 40 iterations ranged from 103 to 106. For the
Eachmovie dataset, if the residual error reduction is at least
102 there is no measurable change in the quality of predic-
tions.
The remaining 60% of users were used for cross-
validation. For each user, 10 of the items they had rated
were set aside, and the remainder used to generate predic-
tions using the method of section 2.1. The average time to
generate a recommendation was 0.05 seconds, or 20 ratings
per second. Accuracy was very good. The Mean Abso-
lute Error (MAE) is the average of the absolute difference
between a prediction and the actual rating of an item by a
user. The MAE for our scheme was was 0.96. In [7], sev-
eral collaborative ﬁltering schemes were compared on the
Eachmovie dataset. The best performance by any of the al-
gorithms they studied was an MAE of 0.96 - equal to our
method.
Finally we studied the robustness of the scheme by sim-
ulating a fraction of clients “dropping out” of various steps
of the computation. At each iteration a different random
subset of 50% of the clients were discarded from the gra-
dient total. A different random subset of 50% of clients
was dropped during the line minimization step. The least-
squares algorithm still converged, albeit more slowly and
could not achieve residual errors below 0.01 of the initial