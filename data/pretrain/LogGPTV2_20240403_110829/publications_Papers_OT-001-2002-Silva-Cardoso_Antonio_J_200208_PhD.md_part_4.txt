assembly step. The goal of this task is to find DNA sequences in order to identify
macromolecules with related structures and functions. The new DNA sequence is
28
compared to a repository of known sequences (e.g., Swiss-Prot or GenBank), using one
of a number of computational biology applications for comparison.
After obtaining the desired data from the Sequence Processing task, the results are
stored, e-mailed, and a report is created. The Process Report task stores the data
generated in the previous task in a database and creates a final report. It is responsible for
electronically mailing the sequencing results to the persons involved in this process, such
as researchers and lab technicians.
2.3.3 WORKFLOW APPLICATION REQUIREMENTS
In its normal operation, the Fungal Genome Resource laboratory executes the DNA
Sequencing workflow in a regular manner. Workflow instances are started in order to
render the sequencing services. In this scenario, and with current workflow technology,
the execution of the workflow instances is carried out without any quality of service
management on important parameters such as delivery deadlines, fidelity, quality,
reliability, and cost of service. The laboratory wishes to be able to state a detailed list of
requirements for the service to be rendered to its customers. Its requirements include the
following:
§ The final report has to be delivered in 31 weeks or less, as specified by the
customer (e.g., NIH).
§ The profit margin has to be 10%. For example, if a customer pays $1,100 for a
sequencing, then the execution of the DNA Sequencing workflow must have a
cost for the laboratory that is less than $1,000.
§ The error rate of the task Prepare Clones and Sequence has to be at most e, and
the data quality of the task Sequence Processing has to be at least a.
29
§ In some situations, the client may require an urgent execution of DNA
sequencing. Therefore, the workflow has to exhibit high levels of reliability, since
workflow failures would delay the sequencing process.
The requirements for the genetic workflow application presented underline four non-
functional requirements: time, cost, fidelity, and reliability. While the specification of
such quality requirements is important, current WfMSs do not include the functions to
delineate their specification or management.
2.3.4 CURRENT WFMSS LIMITATIONS
The lack of a mechanism to specify workflow QoS is a current limitation of WfMSs.
However, this is not the only missing element; once a workflow QoS model is defined,
three additional components need to be developed: estimation algorithms and methods,
monitoring tools, and mechanisms to control the quality of service. Only the development
of integrated solutions composed of those four modules (specification, estimation,
monitoring, and control) can result in a sophisticated quality management framewokr.
The objectives and functionalities of each module include the following :
§ A quality of service model must be developed to allow for the specification of
workflow Quality of Service (QoS) metrics. This model allows suppliers to
specify the duration, quality, cost, fidelity, etc., of the services and products to be
delivered. Specifications can be set at design-time, when designers build
workflow applications, or they can be adjusted at run-time.
§ Algorithms and methods must be developed to estimate the quality of service of a
workflow both before instances are started and during instance execution. The
estimation of QoS before instantiation allows suppliers to ensure that the
workflow processes to be executed will indeed exhibit the quality of service
30
requested by customers. The analysis of workflow QoS during instance execution
allows workflow systems to constantly compute QoS metrics and register any
deviations from the initial requirements .
§ Tools must be available to monitor the quality of service of running workflow
instances. Workflow users and managers need to receive information about the
QoS status and possible deviations from the desired metrics that might occur. In
our scenario, let us assume that for some unknown reason the matching factor of
the DNA Sequencing data drops below a threshold expressed by the customer.
The matching factor reflects the degree of similarity between the query sequence
(“probe”) and the compared (“subject”) sequence stored in a sequence database.
The use of workflow QoS monitoring tools can automatically detect this variation
in fidelity and automatically notify interested users.
§ Mechanisms must be available whichc ontrol the quality of service of workflow
instances. Control is necessary when instances do not behave according to initial
requirements. Let us consider the following example: workflow instances are
running correctly and the quality of service specifications are being followed
when a task fails. The task Prepare Clone and Sequence stops its processing
because one of the associated machines has a mechanical problem. As a
consequence, workflow QoS specifications of time are no longer satisfied, and the
WfMS raises a warning, an alert, or an exception. The faulty task needs to be
replaced by an equivalent task to restore the soundness of the system. This
replacement can be accomplished by applying dynamic changes to the workflow
instances, either manually or automatically( Cardoso, Luo et al. 2001).
While these four areas of research are important and indispensable for adequate
quality of service management, in this paper we focus on the specification, estimation,
and monitoring of workflow QoS.
31
2.4 WORKFLOW QUALITY OF SERVICE
As stated earlier, the quality of service is an important issue for workflow systems. The
international quality standard ISO 8402 (part of the ISO 9000 (ISO9000 2002)) describes
quality as ”the totality of features and characteristics of a product or service that bear on
its ability to satisfy stated or implied needs.” This definition implies a relation between
the characteristics of products or services rendered and the initial requirements or implied
needs. In our opinion, this definition of quality, which includes an important relationship
between requirements and characteristics, is relevant and applicable to the domain of
WfMSs. For us, workflow QoS represents the quantitative and qualitative characteristics
of a workflow application necessary to achieve a set of initial requirements.
Workflow QoS addresses the non-functional issues of workflows rather than
workflow process operations. Quantitative characteristics can be evaluated in terms of
concrete measures such as workflow execution time, cost, etc. Kobielus (1997) suggests
that dimensions such as time, cost, and quality should constitute the criteria that
workflow systems should include and might benefit from. Qualitative characteristics
specify the expected services offered by the system, such as security and faul-ttolerance
mechanisms. QoS should be seen as an integral aspect of workflows; therefore, it should
be integrated with workflow specifications. The first step is to define a workflow QoS
model.
2.4.1 WORKFLOW QOS MODEL
Quality of service can be characterized according to various dimensions. We have
investigated related work to decide which dimensions would be relevant to compose our
QoS model. Our research targeted two distinct areas: operations management for
organizations and quality of service for software systems. The study of those two areas is
important, since workflow systems are widely used to model organizational business
processes, and workflow systems are themselves software systems .
32
On the organizational side, Stalk and Hout (1990) and Rommel et al. (1995)
investigated the features with which successful companies assert themselves in
competitive world markets. Their results indicated that success is related to the capability
to compete with other organizations, and it is based upon three essential pillars: time,
cost, and quality. These three dimensions have been a major concern for organizations.
Garvin (1988) associates eight dimensions with quality, including performance and
reliability. Software systems’ quality of service has also been extensively studie.d Major
contributions can be found in the areas of networking (Cruz 1995; Georgiadis, Guerin et
al. 1996), real-time applications (Clark, Shenker et al. 1992) and middleware (Zinky,
Bakken et al. 1997; Hiltunen, Schlichting et al. 2000). For middleware systems, Frlund
and Koistinen (1998) present a set of practical dimensions for distributed object systems’
reliability and performance, which include TTR (time to repair), TTF (time to failure),
availability, failure masking, and server failure. For data networks, the QoS generally
focus on domain-specific dimensions such as bandwidth, latency, jitter, and loss
(Nahrstedt and Smith 1996).
Our past work on deploying workflow applications has made us aware of the need
for workflow process QoS management. Additionally, we have realized that workflow
processes have a particular set of requirements which are domain dependent and that
need to be accounted for when creating a QoS model. Based on previous studies and our
experience in the workflow domain, we have constructed a QoS model composed of the
following dimensions: time, cost, reliability, and fidelity. According to Weikum (1999),
information services QoS can be divided into three categories: system centric, process
centric, and information centric. Our model specifies quality dimensions that incldue the
system and process categories. QoS specifications are set for task definitions. Based on
this information, QoS metrics are computed for workflows (see section 2.6).
Other researchers have also identified the need for a QoS process model. A good
example is the DAML-S specification (Ankolekar, Burstein et al. 2001; DAML-S 2001),
33
which semantically describes business processes (as in the composition of Web services).
The use of semantic information facilitates process interoperability between trading
partners involved in e-commerce activities. This specification includes constructs which
specify quality of service parameters, such as quality guarantees, quality rating, and
degree of quality. While DAML-S has identified the importance of Web services and
business processes specifications, the QoS model adopted should be significantly
improved in order to supply a more functional solution for its users. One current
limitation of DAML-S’ QoS model is that it does not provide a detailed set of classes and
properties to represent quality of service metrics. The QoS model needs to be extended to
allow for a precise characterization of each dimension. The addition of semantic
concepts, such as minimum, average, maximum, and the distribution function associated
with a dimension, will allow the implementation of algorithms for the automatic
computation of QoS metrics for processes based on atomic tasks and sub-processes’ QoS
metrics.
2.4.2 TASK TIME
Time is a common and universal measure of performance. For workflow sytsems, it can
be defined as the total time needed by an instance to transform a set of inputs into
outputs. The philosophy behind a time-based strategy usually demands that businesses
deliver the most value as rapidly as possible. Shorter workflow execution time allows for
a faster production of new products, thus providing a competitive advantage, since the
products are more rapidly introduced into the market. Additionally, reducing the time
taken to execute a set of tasks in a workflow process makes it possible for an organization
to be more responsive to customers’ needs. Therefore, it is important to enhance WfMS
to include time-based process execution.
The first measure of time is task response time (T). Task response time corresponds
to the time an instance takes to be processed by a task. The task response time can be
34
broken down into two major components: delay time and process time. Delay time (DT)
refers to the non-value-added time needed in order for an instance to be processed by a
task. This includes, for example, the instance queuing delay and the setup time of the
task. While, those two metrics are part of the task operation, they do not add any value to
it. Process time (PT) is the time a workflow instance takes at a task while being
processed; in other words, it corresponds to the time a task needs to process an instance.
Therefore, task response time for a task t can be computed as follows:
T(t) = DT(t) + PT(t)
The delay time can be further broken down into queuing delay and setup delay.
Queuing delay is the time instances spend waiting in a tasklist, before the instance is
selected for processing. Setup delay is the time an instance spends waiting for the task to
be set up. Setup activities may correspond to the warming process carried out by a
machine before executing any operation, or to the execution of self-checking procedures.
Another time metric that may be considered to integrate with the delay time is the
synchronization delay, which corresponds to the time a workflow instance waits for
mates in an and-join task (synchronization). In our QoS model, this metric is not part of
the task response time. This is because the algorithm we use to estimate workflow QoS
can derive this metric directly from the workflow structure and from the task response
time. This will become clearer when we describe workflow QoS computation .
2.4.3 TASK COST
Task cost represents the cost associated with the execution of workflow tasks. Cost is an
important factor, since organizations need to operate according to their financial plan. It
is fundamental for organizations that wish to reduce their expenditures on internal
processes and wish to control product and service cost. During workflow design, both
prior to workflow instantiation and during workflow execution, it is necessary to estimate
35
the cost of the execution in order to guarantee that financial plans are followed. The cost
of executing a single task includes the cost of using equipment, the cost of human
involvement, and any supplies and commodities needed to complete the task. The
following cost functions are used to compute the cost associated with the execution of a
task.
Task cost (C) is the cost incurred when a task t is executed; it can be broken down
into two major components: enactment cost and realization cost.
C(t) = EC(t) + RC(t)
The enactment cost (EC) is the cost associated with the management of the
workflow system and with workflow instances monitoring. The realization cost (RC) is
the cost associated with the runtime execution of the task. It can be broken down into:
direct labor cost, machine cost, direct material cost, and setup cost. Direct labor cost is
the cost associated with the person carrying out the execution of a workflow human task
(Kochut, Sheth et al. 1999), or the cost associated with the execution of an automatic task
with partial human involvement.M achine cost is the cost associated with the execution of
an automatic task. This can correspond to the cost of running a particular piece of
software or the cost of operating a machine. Direct material cost is the cost of the
materials, resources, and inventory used during the execution of a workflow task. Setup
cost is the cost to set up any resource used prior to the execution of a workflow task.
2.4.4 TASK RELIABILITY
In an early work on workflow modeling, Krishnakumar and Sheth( 1995) represented the
execution behavior of each task, using task structures. Each workflow task structure has
an initial state, an execution state, and two distinct terminating states. One of the states
indicates that a task has failed (for non-transactional tasks) or was aborted (for
transactional and open 2PC tasks), while the other state indicates that a task is done or
36
committed (Figure 2-2). The model used to represent each task indicates that only one
starting point exists when performing a task, but two different states can be reached upon
its execution. Based on this task model structure, we introduce the reliability dimension.
This QoS dimension provides information concerning the relationship between the
number of times the state done/committed is reached and the number of times the
failed/aborted state is reached after the execution of a task.
Figure 2-2 - Two task structures (Krishnakumar and Sheth 1995)
Task Reliability (R) corresponds to the likelihood that the components will perform
according to; it is a function of the failure rate. To describe task reliability we follow a
discrete-time modeling approach. We have selected this solution since workflow task
behavior is most of the time characterized in respect to the number of executions.
Discrete-time models are adequate for systems that respond to occasional demands, such
as database systems (i.e., discrete-time domain). This dimension( Table 2-1) follows from
one of the popular discrete-time stable reliability models proposed in (Nelson 1973),
where failure rate is given as the ratio of successful executions/scheduled executions.
37
Table 2-1 – Task reliability
R(t) = 1 – failure rate
For each task, the WfMS keeps track of the number of times the task has been
scheduled for execution and how many times the task has been successfully executed.
R(t)is a stable model, since when software failure occurs no fault removal is performed .
Alternatively, continuous-time reliability models can be used when the failures of the
malfunctioning equipment or software can be expressed in terms of times between
failures, or in terms of the number of failures that occurred in a given time interval. Such
reliability models are more suitable whenw orkflows include tasks that control equipment
or machines that have failure specifications determined by the manufacturer. Goel( 1985)
classified reliability models into four kinds: input domai-nbased models, times-between-
failures models, failure-count models, and fault seeding models. Ireson, Jr et al. (1996)
presents several software reliability models which can be used to model this QoS
dimension. The ideal situation would be to associate with each workflow task a reliability
model representing its working behavior. While this is possible, we believe that the
common workflow system users do not have enough knowledge and expertise to apply
such models.
2.4.5 TASK FIDELITY
We view fidelity as a function of effective design; it refers to an intrinsic property(ies) or
characteristic(s) of a good produced or service rendered. Fidelity reflects how well a
product is being produced and how well a service is being rendered. Fidelity is often
difficult to define and measure because it is subject to judgments and perceptions.
Nevertheless, the fidelity of workflows should be predicted whenever efasible and
carefully controlled when needed (Kolarik 1995; Franceschini 2002).
38
Workflow tasks have a fidelity (F) vector dimension composed of a set of fidelity
attributes (F(t).a), that reflect and quantify task operations. Each fidelity attribute refers
r
to a property or characteristic of the product being created, transformed, or analyzed.
Fidelity attributes are used by the workflow system to compute how well workflows,
instances, and tasks are meeting user specifications. For example, the Test Quality task
check the fidelity of the attribute F(t).a . This attribute reflects the probability
E. coli matching
that the sample being sequenced is contaminated. Each task is associated with a fidelity
function F(t), which represents the local normalized fidelity :
F(t) = |f (F(t).a)|wi | f (F(t).a)|wi | f (F(t).a )|wi | f (F(t).a)|wi
1 i 1 + 2 j 2 + 3 k 3 + …+ n l n
The formula weights the fidelity attributes, which can be transformed to more
appropriate values using a function f , and are normalized to the scale [0..1]. The sum of
n
the weights wi is equal to 1. In view of the fact humans often feel awkward in handling
k
and interpreting such quantitative values (Tversky and Kahneman 1974), we allow the
designer with the help of a domain expert to map the value resulting from applying the
fidelity function to a qualitative scale (Miles and Huberman 1994). This qualitative
indicator is used to detect areas of a workflow with anomalies and undesired behavior.