Table 5: Aeolus vs Priority Queueing: unscheduled packets
results in the dropping of scheduled packets
Figure 18: Goodput across varying network loads.
(a) Slowdown-average
(b) Slowdown-99%
Figure 17: FCT slowdown with varying incast ratio in a two-
tier spine-leaf topology.
To showcase this ambiguity, we implement the priority queueing
based solution in ns-2 simulator. We consider two retransmission
timeouts (RTOs): 10ms and 20µs. The large RTO is resilient to packet
trapping, but cannot eciently recover unscheduled packet losses.
In contrast, the small RTO incurs severe redundant transmissions.
We run the cache follower workload in the 100G fat-tree topology.
The proactive algorithm is ExpressPass. We measure the maximum
FCT and transfer eciency. As shown in Table 4, the large RTO
suers from high tail latency due to slow loss recovery, while the
small RTO causes many redundant transmissions, thus degrading
transfer eciency.
We also show that isolating unscheduled packets in low priority
queues does have the risk of aecting scheduled packets, at extreme
case. We consider a contrived 20-to-1 incast scenario where each
sender sends 400KB data to a common receiver. All servers are
directly connected to a 100G switch, where shared buer scheme is
adopted across dierent priority queues. The average and maximum
FCT under Aeolus and priority queueing are shown in Table 5. It is
easy to see that, compared with Aeolus, priority queueing results
in much longer FCTs (⇠10⇥ worse than Aeolus). The reason is that,
switch buer is fully occupied by unscheduled packets queued at
low priority queue. As a result, scheduled packets are rejected to
enter high priority queue due to the lack of available buer (drop-
tail). As the dropping of scheduled packets is rare for proactive
solutions like ExpressPass [14], a large RTO=10ms is used for the
recovery of dropped scheduled packets, which results in worse
FCTs.
Heavy incast with larger network. As a stress test, we study
the behavior of Aeolus under heavy incast by conducting N-to-1
incast simulations in a two-tier spine-leaf network (N= 32, 64, 128
and 256). The network has 4 spine switches, 9 leaf switches and
144 servers (16 under each leaf switch). Server links operate at
100 Gbps and spine-leaf links operate at 400 Gbps. All links have
0.2µs propagation delay. All switches have 0.25µs switching delay.
Each switch port has 500KB buer. All the ows have 64KB data.
We choose senders randomly across all servers. For Homa, we use
40µs as the retransmission timeout, which is equal to the largest
queueing delay a packet could experience in the network.
Figure 17 shows the FCT slowdown8 on average and at the
99th-percentile, respectively. We mainly make three observations.
First, compared with ExpressPass, ExpressPass+Aeolus achieves
similar performance. This is expected because the main benet
brought by Aeolus is the ability to utilize spare bandwidth with
unscheduled packets in the rst RTT. However, in the heavy incast
scenarios, the proportion of data bytes that can be transmitted in
the rst RTT is minimal compared to the total bytes of all ows.
As a result, Aeolus can hardly make further improvement. Second,
Aeolus enables Homa to achieve good performance even under
heavy incast. This is because Aeolus can avoid large queue buildup
by selectively dropping the overwhelming unscheduled packets
in the rst RTT. As a result, scheduled packets are protected from
large queueing delay and packet loss since the second RTT. Lost
unscheduled packets are also recovered quickly using scheduled
packets. Under Homa, however, both unscheduled and scheduled
packets will suer from severe losses due to large queue buildups.
With the inecient timeout-based loss recovery mechanism, Homa
will spend much longer time on completing the transmission of all
ows. Third, compared with NDP, NDP+Aeolus achieves similar
performance. This is consistent with our previous evaluation result.
Impact on goodput. Readers may wonder whether aggressively
dropping unscheduled packets in the rst RTT would negatively
aect the eective bandwidth utilization of each scheme. To study
this, we evaluate each scheme with increasing network loads to
identify the maximum goodput it can achieve. For this simulation,
we use the same spine-leaf topology as above. We generate net-
work loads using a mix of Web Search trac and incast trac. We
generate the incast trac by randomly selecting 64 senders and
one receiver, each sending 64KB data.
Figure 18 shows the goodput (normalized by the link capacity)
each scheme can achieve over varying network loads. Compared
with ExpressPass, Aeolus has no negative impact on goodput. For
Homa, Aeolus improves the goodput by 4%. This is mainly because
Aeolus eliminates the losses of scheduled packets and does fast
recovery for lost unscheduled packets. For NDP, Aeolus improves
the goodput by 2%. This is because NDP needs to reserve some
bandwidth headroom for transmitting trimmed packet headers. In
8“FCT slowdown" means a ow’s actual FCT normalized by its ideal FCT when the
network only serves this ow.
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
S. Hu et al.
contrast, Aeolus only requires 64 bytes per ow for transmitting
the probe.
Furthermore, across these schemes, we observe that NDP achieves
the highest goodput (84%) mainly for two reasons. First, it performs
per-packet load balancing to fully utilize the bandwidth over mul-
tiple paths. Second, it leverages CP to enable fast loss recovery.
In contrast, ExpressPass uses per-ow ECMP for load balancing,
thus only achieving 70% goodput. In addition, Homa achieves the
worst goodput (54%) due to the losses of many scheduled packets
and the use of inecient timeout-based loss recovery mechanism.
We note that our result with Homa is inconsistent with the result
presented in the Homa paper [29]. We suspect one possible reason
is that Homa assumes innite switch buer in their simulations.
In contrast, in our simulations we allocate 500KB buer for each
switch port.
6 DISCUSSION
Design tradeo. Aeolus guarantees good tail latency by protecting
the deterministic nature of proactive schemes, but only improves
the average latency in a best eort manner. In certain cases, it
may also make some compromise. For example, it is possible that
Aeolus may drop unscheduled packets even when the switch has
enough buer space to hold all the in-ight trac, due to small
selective dropping threshold (§4.1). As a result, Aeolus may delay
the completion of some small ows due to bandwidth wastage in the
rst RTT. However, Aeolus can consistently eliminate congestion
timeouts, even under serious congestion.
Resilience under heavy incast. Aeolus uses minimum-sized probe
packets to improve the resilience under heavy incast workloads.
For example, with a typical setting of 200KB switch buer, 6KB
dropping threshold, and 64B probe packet size, Aeolus can eec-
tively detect unscheduled packet losses even when there are 3100
(194KB/64B) new arrival ows. To handle the extreme cases where
even the probe packet can get dropped, we may let the sender set a
timer to retransmit unscheduled packets and the probe packet if
no credit is received in a given duration.
Overhead of per-packet ACK for hardware oloading. Per-
packet ACK can be a burden for hardware transport at high speed.
However, Aeolus minimizes such overhead by only generating per-
packet ACK for unscheduled packets, which are more likely to be
dropped. For example, with 100Gbps link, 10µs base RTT and 1.5KB
MTU, each ow only needs 84 ACKs for unscheduled packets.
Oversubscribed topology. Some proactive schemes (e.g., NDP [18],
Homa [29] and pHost [16]) assume the network core is free of con-
gestion. However, enabling proactive solutions to work with over-
subscribed topologies is not a goal of Aeolus. For example, when
integrated with above proactive schemes, Aeolus cannot avoid con-
gestion losses of unscheduled packets in oversubscribed topologies.
7 RELATED WORK
There are tons of DCN transport designs aiming at low latency and
high throughput. We have discussed the closely related proactive so-
lutions [14, 18, 29] extensively in the paper. Here, we only overview
some other ideas which have not been discussed elsewhere.
In contrast to proactive solutions, reactive DCN congestion con-
trol algorithms leverage advanced signals, e.g., ECN, RTT and
in-network telemetry (INT), to detect congestion. For example,
DCTCP [9], D2TCP [32] and DCQCN [36] leverage ECN as the
congestion signal. TIMELY [27] and DX [25] use delay as the sig-
nal. More recently, HPCC [26] leverages INT to obtain precise link
load information. However, most of these solutions require at least
one RTT to react to congestion and usually take multiple rounds
to converge to the ideal rates. As a result, they are inecient to
provide persistent low latency in high speed DCNs.
There are other DCN research eorts such as ow scheduling
(e.g., pFabric [10] and PIAS [11]) and multi-path load balancing (e.g.,
CONGA [8] and Hermes [34]). These designs either help to reduce
ow completion times, or strike for higher network utilization
with multiple path. However, none of them targets at the rst RTT
problem focused by this paper.
We note that in broader contexts other than DCN, eorts have
also been made to enable fast ow start with large initial rates of
transport protocols. For example, in the context of Internet, RC3 [28]
proposed to use k levels of lower network priorities to transmit a
larger number of additional packets during TCP’s slow start phase
in order to compensate its over-caution in window increase. In the
context of system area networks, SRP [23] allows senders to trans-
mit speculative packets in the rst RTT at lower network priority
before bandwidth reservation is granted. Relative to Aeolus, both
RC3 and SRP share the similar motivation of better utilizing spare
bandwidth in the rst RTT with prioritization. However, Aeolus
diers from them in the way of implementing the prioritization.
By identifying the problems of multiple priority queues as we dis-
cussed in §3.2, Aeolus proposed a novel selective dropping scheme
that avoids these downsides by using only one queue.
8 CONCLUSION
This paper presented Aeolus, a readily deployable solution focus-
ing on “pre-credit" packet transmission as a building block for all
proactive transports. At the core of Aeolus, it prioritizes scheduled
packets over unscheduled packets so that proactive transports can
fully utilize spare bandwidth while preserving their deterministic
nature. Furthermore, Aeolus introduces a novel selective dropping
scheme which allows pre-credit new ows to burst at line-rate
when there exists spare bandwidth, but immediately drops them se-
lectively once the bandwidth is used up. In addition, Aeolus reuses
the preserved proactive transport as a means to recover dropped
rst-RTT packets safely and eciently. Aeolus is compatible with
all existing proactive solutions. We have implemented an Aeolus
prototype using DPDK and commodity switch hardware, and eval-
uated it through small testbed experiments and larger simulations.
Both our implementation and evaluation results indicate that Ae-
olus is a promising substrate strengthening all existing proactive
transport solutions. This work does not raise any ethical issues.
Acknowledgements: This work is supported in part by the Hong
Kong RGC GRF-16215119 and PCL FANet No.LZC0019. We thank
our shepherd Rachit Agarwal and the anonymous SIGCOMM re-
viewers for their constructive feedback and suggestions. We also
thank Junchen Jiang for insightful discussions on this paper.
REFERENCES
[1] Arista eos. https://www.arista.com/en/products/eos.
[2] Dpdk. https://www.dpdk.org/.
[3] Expresspass simulator. https://github.com/kaist-ina/ns2-xpass.
[4] High-capacity strataxgs trident ii ethernet switch series. https://www.broadcom.
com/products/ethernet-connectivity/switch-fabric/bcm56850.
[5] High-density 25/100 gigabit ethernet strataxgs tomahawk ethernet switch se-
ries. https://www.broadcom.com/products/ethernet-connectivity/switch-fabric/
bcm56960.
[6] Homa simulator. https://github.com/PlatformLab/HomaSimulation.
[7] NDP simulator. https://github.com/nets-cs-pub-ro/NDP/wiki/NDP-Simulator.
[8] Mohammad Alizadeh, Tom Edsall, Sarang Dharmapurikar, Ramanan
Vaidyanathan, Kevin Chu, Andy Fingerhut, Vinh The Lam, Francis Ma-
tus, Rong Pan, Navindra Yadav, et al. Conga: Distributed congestion-aware load
balancing for datacenters. In SIGCOMM 2014.
[9] Mohammad Alizadeh, Albert Greenberg, David A. Maltz, Jitendra Padhye,
Parveen Patel, Balaji Prabhakar, Sudipta Sengupta, and Murari Sridharan. Data
center tcp (dctcp). In SIGCOMM 2010.
[10] Mohammad Alizadeh, Shuang Yang, Milad Sharif, Sachin Katti, Nick McKeown,
Balaji Prabhakar, and Scott Shenker. pfabric: Minimal near-optimal datacenter
transport. In SIGCOMM 2013.
[11] Wei Bai, Li Chen, Kai Chen, Dongsu Han, Chen Tian, and Hao Wang. Information-
agnostic ow scheduling for commodity data centers. In NSDI 2015.
[12] Wei Bai, Li Chen, Kai Chen, and Haitao Wu. Enabling ecn in multi-service
multi-queue data centers. In NSDI 2016.
[13] Peng Cheng, Fengyuan Ren, Ran Shu, and Chuang Lin. Catch the whole lot in
an action: Rapid precise packet loss notication in data centers. NSDI’14.
[14] Inho Cho, Keon Jang, and Dongsu Han. Credit-scheduled delay-bounded conges-
tion control for datacenters. In SIGCOMM 2017.
[15] Tobias Flach, Nandita Dukkipati, Andreas Terzis, Barath Raghavan, Neal Card-
well, Yuchung Cheng, Ankur Jain, Shuai Hao, Ethan Katz-Bassett, and Ramesh
Govindan. Reducing web latency: The virtue of gentle aggression.
[16] Peter X. Gao, Akshay Narayan, Gautam Kumar, Rachit Agarwal, Sylvia Rat-
nasamy, and Scott Shenker. phost: Distributed near-optimal datacenter transport
over commodity network fabric. In CoNEXT 2015.
[17] Albert Greenberg, James R. Hamilton, Navendu Jain, Srikanth Kandula,
Changhoon Kim, Parantap Lahiri, David A. Maltz, Parveen Patel, and Sudipta
Sengupta. Vl2: a scalable and exible data center network. In SIGCOMM 2009.
[18] Mark Handley, Costin Raiciu, Alexandru Agache, Andrei Voinescu, Andrew W
Moore, Gianni Antichi, and Marcin Wójcik. Re-architecting datacenter networks
and stacks for low latency and high performance. In SIGCOMM 2017.
[19] Keqiang He, Eric Rozner, Kanak Agarwal, Yu (Jason) Gu, Wes Felter, John Carter,
and Aditya Akella. Ac/dc tcp: Virtual congestion control enforcement for data-
center networks. In SIGCOMM 2016.
[20] Chi-Yao Hong, Matthew Caesar, and P Godfrey. Finishing ows quickly with
preemptive scheduling. In SIGCOMM 2012.
[21] Shuihai Hu, Wei Bai, Baochen Qiao, Kai Chen, and Kun Tan. Augmenting
proactive congestion control with aeolus. In Proceedings of the 2nd Asia-Pacic
Workshop on Networking.
[22] V. Jacobson. Congestion avoidance and control. In SIGCOMM 1988.
[23] Nan Jiang, Daniel U. Becker, George Michelogiannakis, and William J. Dally.
Network congestion avoidance through speculative reservation. HPCA ’12.
[24] Glenn Judd. Attaining the promise and avoiding the pitfalls of tcp in the data-
center. In NSDI 2015.
[25] Changhyun Lee, Chunjong Park, Keon Jang, Sue Moon, and Dongsu Han. Accu-
rate latency-based congestion feedback for datacenters. In ATC 2015.
[26] Yuliang Li, Rui Miao, Hongqiang Harry Liu, Yan Zhuang, Fei Feng, Lingbo Tang,
Zheng Cao, Ming Zhang, Frank Kelly, Mohammad Alizadeh, and Minlan Yu.
Hpcc: High precision congestion control. SIGCOMM ’19.
[27] Radhika Mittal, Vinh The Lam, Nandita Dukkipati, Emily Blem, Hassan Wassel,
Monia Ghobadi, Amin Vahdat, Yaogong Wang, David Wetherall, and David Zats.
Timely: Rtt-based congestion control for the datacenter. In SIGCOMM 2015.
[28] Radhika Mittal, Justine Sherry, Sylvia Ratnasamy, and Scott Shenker. Recursively
cautious congestion control. NSDI’14.
[29] Behnam Montazeri, Yilong Li, Mohammad Alizadeh, and John K. Ousterhout.
Homa: A receiver-driven low-latency transport protocol using network priorities.
[30] Jonathan Perry, Amy Ousterhout, Hari Balakrishnan, Deverat Shah, and Hans
Fugal. Fastpass: A centralized "zero-queue" datacenter network. In SIGCOMM
2014.
[31] Arjun Roy, Hongyi Zeng, Jasmeet Bagga, George Porter, and Alex C. Snoeren.
Inside the social network’s (datacenter) network. In SIGCOMM 2015.
[32] Balajee Vamanan, Jahangir Hasan, and TN Vijaykumar. Deadline-aware datacen-
Aeolus: A Building Block for Proactive Transport in Datacenters
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
[35] Jiao Zhang, Fengyuan Ren, Ran Shu, and Peng Cheng. Tfc: token ow control in
data center networks. In EuroSys 2016.
[36] Yibo Zhu, Haggai Eran, Daniel Firestone, Chuanxiong Guo, Marina Lipshteyn,
Yehonatan Liron, Jitendra Padhye, Shachar Raindel, Mohamad Haj Yahia, and
Ming Zhang. Congestion control for large-scale rdma deployments. In SIGCOMM
2015.
ter tcp (d2tcp). In SIGCOMM 2012.
[33] Haitao Wu, Jiabo Ju, Guohan Lu, Chuanxiong Guo, Yongqiang Xiong, and Yong-
guang Zhang. Tuning ecn for data center networks. In CoNEXT 2012.
[34] Hong Zhang, Junxue Zhang, Wei Bai, Kai Chen, and Mosharaf Chowdhury.
Resilient datacenter load balancing in the wild. In SIGCOMM 2017.