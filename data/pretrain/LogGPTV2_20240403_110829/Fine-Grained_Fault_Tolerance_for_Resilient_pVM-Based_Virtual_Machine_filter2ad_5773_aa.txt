title:Fine-Grained Fault Tolerance for Resilient pVM-Based Virtual Machine
Monitors
author:Djob Mvondo and
Alain Tchana and
Renaud Lachaize and
Daniel Hagimont and
No&quot;el De Palma
2020 50th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)
Fine-Grained Fault Tolerance For Resilient
pVM-based Virtual Machine Monitors
Djob Mvondo∗
Alain Tchana†
Renaud Lachaize∗
Daniel Hagimont‡
Noël De Palma∗
∗ Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG
Grenoble, France
† ENS Lyon, LIP
Lyon, France
‡ University of Toulouse, IRIT
Toulouse, France
Abstract—Virtual machine monitors (VMMs) play a crucial
role in the software stack of cloud computing platforms: their
design and implementation have a major impact on performance,
security and fault tolerance. In this paper, we focus on the
latter aspect (fault tolerance), which has received less attention,
although it is now a signiﬁcant concern. Our work aims at
improving the resilience of the “pVM-based” VMMs, a popular
design pattern for virtualization platforms. In such a design,
the VMM is split into two main components: a bare-metal
hypervisor and a privileged guest virtual machine (pVM). We
highlight that the pVM is the least robust component and that
the existing fault-tolerance approaches provide limited resilience
guarantees or prohibitive overheads. We present three design
principles (disaggregation, specialization, and pro-activity), as well
as optimized implementation techniques for building a resilient
pVM without sacriﬁcing end-user application performance. We
validate our contribution on the mainstream Xen platform.
I. INTRODUCTION
Virtualization is a major pillar of cloud computing in-
frastructures because it enables more ﬂexible management
and higher utilization of server physical resources. To this
end, a virtual machine monitor (VMM) abstracts away a
physical machine into a set of isolated guest virtual machines
(VMs). This consolidation advantage comes with the risk
of centralization. The crash of an OS instance in a bare-
metal data center only impacts applications deployed on the
corresponding physical server, whereas the crash of a VMM
in a virtualized data center has a much larger “blast radius“,
resulting in the unavailability of the applications hosted in
all the guest VMs managed by that VMM instance. Besides,
given that the code size and the feature set of VMMs have
grown over the years, the occurrences of such critical bugs
are becoming more likely. For instance, the number of Xen
source code lines has tripled from the ﬁrst version 1.0 (in
2003) to the current version (4.12.1), as shown in Table I1.
In the present paper, we highlight the fault-tolerance (FT)
limitations of existing VMMs and propose techniques to
We thank our shepherd, Elias Duarte, and the anonymous reviewers for
their insightful comments. This work was funded by the “ScaleVisor” project
of Agence Nationale de la Recherche, number ANR-18-CE25-0016,
the
“Studio virtuel” project of BPI and ERDF/FEDER, grant agreement number
16.010402.01, the “HYDDA” project of BPI Grant, and the “IDEX IRS”
(COMUE UGA grant).
1For Linux, we only consider x86 and arm in the arch folder. Also, only
net and block are considered in drivers folder (other folders are not
relevant for server machines).
#LOC in 2003
#LOC in 2019
#LOCin2019
#LOCin2003
Xen Hypervisor Linux-based pVM Linux
Xen
19.81
31.76
3.72 Million
18.5 Million
187, 823
583, 237
3.11
4.98
1.6
TABLE I: Evolution of source code size for the Xen hypervisor
and a Linux-based pVM. LOC stands for "Lines of Code".
mitigate them. More precisely, we focus on one of the most
popular VMM software architectures, hereafter named “pVM-
based VMMs”. In this architecture, which has some similari-
ties with a microkernel OS design, the VMM is made of two
components: the hypervisor and a privileged VM (pVM). The
hypervisor is the low-level component that is mostly in charge
of initializing the hardware and acting as a data plane, i.e.,
providing the logic needed to virtualize the underlying hard-
ware platform, except I/O devices. The pVM acts as a control
plane for the hypervisor, through a speciﬁc interface, and is
involved in all VM management operations (creation, startup,
suspension, migration . . . ). It also hosts I/O device drivers that
are involved in all I/O operations performed by user VMs (i.e.,
regular VMs) on para-virtual devices. The pVM is typically
based on a standard guest OS (e.g., Linux) hosting a set of
control-plane daemons. This pVM-based design is popular and
used in production-grade, mainstream virtualization platforms
(for example, Xen, Microsoft Hyper-V and some versions of
VMware ESX) for several important reasons, including the
following ones: (i) it simpliﬁes the development, debugging
and customization of the control plane [1], (ii) it provides
isolation boundaries to contain the impact of faults within the
control plane or the I/O path [2], (iii) it offers ﬂexibility for
the choice of the OS hosting the control plane (which matters
for considerations like code footprint, security features, and
available drivers for physical devices) [3], (iv) it provides a
data plane with a smaller attack surface than a full-blown
operating system like Linux.
In such a pVM-based VMM design, the crash of the pVM
leads to the following severe consequences: (1) the inability to
connect to the physical server, (2) the inability to manage user
VMs, and (3) the interruption of the networked applications
running in user VMs. Besides, the faults within the pVM are
more frequent than in the hypervisor itself because the code
base of the former is very large (see Table I), thus likely to
978-1-7281-5809-9/20/$31.00 ©2020 IEEE
DOI 10.1109/DSN48063.2020.00037
197
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:32:03 UTC from IEEE Xplore.  Restrictions apply. 
be more bug-prone than the latter.
While the reliability of the pVM is of primary and growing
importance, it has received relatively little attention from the
research community. In TFD-Xen, Jo et al. [4] focused on
the reliability of the network drivers inside the pVM of a
Xen-based system. However, this work does not take into
account the critical dependencies of these drivers with other
pVM services (see §II). In the Xoar project, Colp et al.
[5] proposed to periodically refresh each service of the Xen
pVM using a micro-reboot mechanism. This approach, which
was initially designed by its authors as a defense mechanism
against (stepping-stone) security attacks, can also improve the
resilience of a system against faults, by combining preventive
rejuvenation and automatic restart of software components.
However, periodic refreshes incur unacceptable performance
degradation for I/O-sensitive user workloads. For example,
on the TailBench benchmark suite [6], we observed a major
degradation of the 95th-percentile latencies, up to 1300x
(see §II-B). Due to all these limitations, the current solution
adopted by data center operators is full server replication
(VMM and VMs) at the physical level, like, for example, in
VMware vSphere Fault Tolerance [7]. The main limitation of
this approach is the fact that it doubles the number of servers
in the data center.
In this paper, we assume that the hypervisor is reliable
(e.g., thanks to state-of-the-art techniques [8]–[10]) and we
propose a holistic and efﬁcient design to improve in-place
the resilience of a pVM against crashes and data corruption.
We choose the Xen VMM [1], [11] as a case study for our
prototype implementation, given that this is the most popular
pVM-based virtualization platform2. In Xen’s jargon, the pVM
is called “dom0” (or “Domain0”). Our approach is built
following three principles. The ﬁrst principle is disaggregation
(borrowed from Xoar [5]), meaning that each pVM service
is launched in an isolated unikernel [12], thus avoiding the
single point of failure nature of the vanilla pVM design.
The second principle is specialization, meaning that each
unikernel embeds a FT solution that is speciﬁcally chosen
for the pVM service that it hosts. The third principle is pro-
activity, meaning that each FT solution implements an active
feedback loop to quickly detect and repair faults. The latter
two principles are in opposition to the Xoar design, which
systematically/unconditionally applies the same FT approach
(refresh) to all the pVM components.
In respect
to the disaggregation principle, we organize
Xen’s dom0 in four unikernel (uk) types namely XenStore_uk,
net_uk, disk_uk, and tool_uk. XenStore_uk hosts XenStore,
which is a database with a hierarchical namespace storing
VM conﬁgurations and state information. net_uk hosts both the
real and the para-virtualized network drivers. Its FT solution is
based on the shadow driver approach [13]. disk_uk is similar to
net_uk for storage devices, and tool_uk hosts VM management
2Xen is used by major hyperscale cloud providers, such as AWS, Tencent,
Alibaba Cloud, Oracle Cloud, and IBM Cloud. Xen is also supported by most
software stacks used in private clouds of various scales like Citrix XenServer,
Nutanix Acropolis, OpenStack and Apache Cloudstack.
tools. XenStore is the only component that is subject to data
corruption since it is the only one that is stateful. According
to the specialization and the pro-activity principles, the FT so-
lution of each component is as follows: XenStore_uk is repli-
cated (for handling crashes) and it implements a sanity check
solution for data corruption detection; net_uk and disk_uk
implement a shadow driver FT approach; tool_uk redesigns
the VM migration logic for improved resilience. Our solution
also includes a global feedback loop implemented inside the
hypervisor (which is assumed to be resilient) for managing
cascading failures and total dom0 crashes. Cascading failures
are related to the relationships between dom0 services. For
instance, the failure of XenStore generally causes the failure
of all the other components.
In summary, the paper makes the following three main
contributions. First, we present for the ﬁrst time a holistic FT
solution for the pVM. Second, we implement a functioning
prototype in Xen. The source code of our prototype is publicly
available3. Third, we demonstrate the effectiveness of our
solution. To this end, we ﬁrst evaluate the FT solution of
each pVM component individually. Then, we evaluate the
global solution while injecting faults on several components
at the same time. The evaluation results show that the impact
of our solution on user VMs when they run performance-
critical applications such as those from the TailBench suite is
acceptable in comparison to state-of-the-art solutions (Xoar [5]
and TFD-Xen [4]). For instance, we achieve a 12.7% increase
for 95th-percentile tail latencies; in comparison, the increase
caused by Xoar is of 12999%.
The rest of the article is organized as follows. Section
II presents the background and the motivations. Section III
presents the general overview of our solution and the fault
model that we consider. Section IV presents the implementa-
tion of our solution for each service of Xen’s pVM. Section V
presents the evaluation results. Section VI discusses the related
works. Section VII concludes the paper.
Fig. 1: Overall architecture of a Xen-based virtualization
platform. The dom0 VM corresponds to the pVM.
II. BACKGROUND AND MOTIVATION
A. Xen virtualization platform
Figure 1 presents the architecture of a physical server viru-
alized with Xen. It comprises three component types: domUs
3https://github.com/r-vmm/R-VMM
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:32:03 UTC from IEEE Xplore.  Restrictions apply. 
198
VM manangement operations
(impact the cloud provider)
Start
A
Stop Migrate
A
A
A
A
A
Update
A
A
Application operations
(impact cloud users)
CPU/
Net
I/O
Mem
Disk
I/O
A
S
S
A
S
S
S
Tools
Net
Disk
XS
TABLE II: Impact of the failure of the different dom0
services (xl tools, network/disk drivers, XenStore) on the
VM management operations and on the applications (in user
VMs). An “A” mark indicates that the failure always leads
to unavailability while a “S” mark denotes correlated failures
that occur only in speciﬁc situations.
(user VMs), Xen (the hypervisor), and dom0 (the pVM).
domUs are VMs booked and owned by datacenter users. The
combination of Xen and dom0 forms the VMM. Xen runs
directly atop the hardware, and is in charge of hardware
initialization, resource allocation (except for I/O devices) and
isolation between VMs. Besides, dom0 is a Linux system that
hosts an important portion of the local virtualization system
services, namely (i) the domU life-cycle administration tool
(xl), (ii) XenStore, and (iii) I/O device drivers.
The xl tool stack [14] provides domU startup, shutdown,
migration, checkpointing and dynamic resource adjustment
(e.g., CPU hotplug). XenStore is a daemon implementing a
metadata storage service shared between VMs, device drivers
and Xen. It is meant for conﬁguration and status information
rather than for large data transfers. Each domain gets its own
path in the store, which is somewhat similar in spirit to the
Linux procfs subsystem. When values are changed in the
store, the appropriate components are notiﬁed. Concerning
I/0 devices, dom0 hosts their drivers and implements their
multiplexing, as follows. Along with I/O drivers, dom0 embeds
proxies (called backend drivers) that relay incoming events
from the physical driver to a domU and outgoing requests
from a domU to the physical driver. Each domU runs a
pseudo-driver (called frontend) allowing to send/receive re-
quests to/from the domU-assigned backend.
B. Motivations
The architecture presented in the previous subsection in-
cludes two points of failure:
the Xen hypervisor and the
dom0 pVM. This paper focuses on dom0 fault tolerance (FT).
Table II summarizes the negative impact of the failure of
dom0 with respect to each service that it provides. We can
see that both cloud management operations (VM start, stop,
migrate, update) and end user applications can be impacted by
a dom0 failure. Concerning the former, they can no longer be
invoked in case of dom0 failure. Regarding user applications,
those which involve I/O devices become unreachable in case
of dom0 failure. The table also shows that XenStore (XS) is
the most critical dom0 service because its failure impacts all
other services as well as user applications.
Failures within dom0 are likely to occur since it is based
on Linux, whose code is known to contain bugs due to its
monolithic design, large TCB (trusted computing base) and
ever-increasing feature set. We analyzed xen.markmail.org, a
Web site that aggregates messages from fourteen Xen related
mailing lists since October 2003. At the time of writing this
paper, we found 243 distinct message subjects including the
terms crash, hang, freeze, oops and panic4. After manual
inspection of each of the 243 messages, we discarded 82
of them because they were not talking about faults. 57%
of the remaining messages were related to failures of dom0
components and 43% to the hypervisor. By zooming on dom0
faults, we observed that 66% were related to device drivers,
26% to the tool stack, and 8% to XenStore. From this analysis,
two conclusions can be drawn: (1) cloud sysadmins report
dom0 failures; (2) such failures are linked to all dom0 services.
To the best of our knowledge, the only existing exhaustive
solution against dom0 failures (without resorting to physical
server replication) is the one proposed in the Xoar project [5].
This approach was initially designed against security attacks,
but also provides fault tolerance beneﬁts. It has two main
aspects. First, dom0 is disaggregated in several unikernels in
order to conﬁne each service failure. Second, each service
is periodically restarted (“refreshed”) using a fresh binary.
The critical parameter in such an approach is the refresh
frequency. On the one hand, if it is large (tens of seconds),
then components that are impacted by a dom0 failure will
experience this failure for a long time. On the other hand,
if the refresh period is too short (e.g., one second) then
failures are handled relatively quickly, but at the expense of
signiﬁcant performance degradation for the user applications.
This dilemma has been partially acknowledged by the authors
of Xoar in their paper [5]: in the case of a short refresh