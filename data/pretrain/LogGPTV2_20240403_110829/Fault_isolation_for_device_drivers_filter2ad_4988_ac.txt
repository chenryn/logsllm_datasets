veriﬁes the driver’s access rights by inspecting the policy
installed by the driver manager or the PCI bus driver.
If
an interrupt occurs, a minimal, generic kernel-level handler
disables interrupts, masks the IRQ line that interrupted, no-
tiﬁes the registered driver(s) with an asynchronous HWINT
message, and ﬁnally reenables the interrupt controller. This
process takes about a microsecond and the complexity of
reentrant interrupts is avoided. Once the device-speciﬁc
processing is done, the driver(s) can acknowledge the in-
terrupt using IRQCTL in order to unmask the IRQ line.
4.2.4 Class-IV Restrictions—System Services
5 DRIVER ISOLATION CASE STUDY
Low-level IPC With servers and drivers running in inde-
pendent UNIX processes, they can no longer make direct
function calls to request system services. Instead, MINIX 3
offers IPC facilities based on message passing. By default,
drivers are not allowed to use IPC, but selective access can
be granted using the key ipc in the isolation policy. For ex-
ample, the policy in Fig. 5 enables IPC to the kernel, process
manager, name server, driver manager, network server, PCI
bus driver, IOMMU driver, and terminal driver. The IPC
destinations are listed using human-readable identiﬁers, but
the driver manager retrieves the process IDs from the name
server upon loading a driver. Then it informs the kernel
about the IPC privileges granted using PRIVCTL, just like
is done for I/O resources. The kernel stores the driver’s IPC
privileges in the process table and enforces them at run-time
using simple bitmap operations.
As an aside, the use of IPC poses various other chal-
lenges [18]. Most notable is the risk of blockage when syn-
chronous IPC is used in asymmetric trust relationships that
occur when (trusted) system servers call (untrusted) drivers.
MINIX 3 uses asynchronous and nonblocking IPC in order
to prevent blockage due to unresponsive drivers. In addi-
tion, the driver manager periodically pings each driver to
see if it still responds to IPC, as discussed in Sec. 4.2.1.
OS Services Because the kernel is concerned only with
passing messages from one process to another and does not
inspect the message contents, restrictions on the exact re-
quest types allowed must be enforced by the IPC targets
themselves. This problem is most critical at the kernel task,
which provides a plethora of sensitive operations, such as
managing processes, setting up memory maps, and conﬁg-
uring driver privileges. Therefore, the last key of the policy
shown in Fig. 5, kernel, restricts access to individual ker-
nel calls. In line with least authority, the driver is granted
only those services needed to do its job: perform device
I/O, manage interrupt lines, request DMA services, make
safe memory copies, set timers, and retrieve system infor-
mation. Again, the driver manager fetches the calls granted
upon loading the driver and reports them to the kernel us-
ing PRIVCTL. The kernel inspects the table with authorized
calls each time the driver requests service.
Finally,
the use of services from the user-space OS
servers is restricted using ordinary POSIX mechanisms. In-
coming calls are vetted based on the caller’s user ID and
the request parameters. For example, administrator-level
requests to the driver manager will be denied because all
drivers run with an unprivileged user ID. Since the OS
servers perform sanity checks on all input, request may also
be rejected due to invalid or unexpected parameters, just
like is done for ordinary POSIX calls.
We have prototyped our ideas in the MINIX 3 operat-
ing system. As a case study, we now discuss the working
of the Realtek RTL8139 PCI driver, as sketched in Fig. 6.
The driver’s life cycle starts when the administrator requests
the driver to be loaded, using the isolation policy shown in
Fig. 5. The driver manager creates a new process and in-
forms the kernel about the IPC targets and kernel calls al-
lowed using the PRIVCTL call. It sends the PCI device ID to
the PCI bus driver, which looks up the I/O resources of the
RTL8139 device and also informs the kernel. Finally, only
once the execution environment has been properly isolated,
the driver manager executes the driver binary.
During initialization, the RTL8139 driver contacts the
PCI bus driver to retrieve the I/O resources of the RTL8139
device and registers for interrupt notiﬁcations with the ker-
nel using IRQCTL. Only the I/O resources in the isolation
policy are made accessible though. Since the RTL8139 de-
vice uses bus-mastering DMA, the driver also allocates a
local buffer for use with DMA and requests the IOMMU
driver to program the IOMMU accordingly using SET-
IOMMU. This allows the device to perform DMA into only
the driver’s address space and protects the system against
arbitrary memory corruption by invalid DMA requests.
During normal operation, the driver executes a main loop
that repeatedly receives a message and processes it. Re-
quests from the network server, INET, contain a memory
grant that can be used with the SAFECOPY kernel call in
order to read from or write to only the message buffers and
nothing else. Writing garbage into INET’s buffers results in
messages with an invalid checksum, which will simply be
discarded. The RTL8139 driver can program the network
card using the DEVIO kernel call. The completion interrupt
of the DMA transfer is caught by the kernel’s generic han-
dler and forwarded to the RTL8139 driver. The interrupt is
handled in user space and acknowledged using IRQCTL. In
this way, the driver can safely perform its task without being
able to disrupt any other services.
Driver
Manager
INET
Server
Lookup I/O
resources
PCI Bus
Driver
Set driver
privileges
IOMMU
Driver
RTL8139
Driver
Interrupt Handler
Program
IOMMU
Privileged
operations
Safe copies via
memory grants
DMA allowed
by IOMMU
User−level
IRQ handling
Microkernel
Mediates access to privileged resources
Figure 6: Interactions between an isolated RTL8139 PCI driver
and the outside world in MINIX 3.
6 EXPERIMENTAL SETUP
6.2 Fault Types and Test Coverage
We used software-implemented fault injection (SWIFI)
to assess and iteratively reﬁne MINIX 3’s isolation tech-
niques. The goal of our experiments is to show that faults
occurring in an isolated driver cannot propagate and dam-
age other parts of the system.
6.1 SWIFI Test Methodology
We have emulated a variety of problems underlying OS
crashes by injecting selected machine-code mutations rep-
resentative for both (i) low-level hardware faults and (ii)
typical programming errors. In particular, we used 8 fault
types from an existing fault injector [27, 36], as discussed
in Sec. 6.2. Process tracing is used to control execution of
the targeted driver and corrupt its program text at run-time.
For each fault injection, the code to be mutated is found by
calculating a random offset in the text segment and ﬁnding
the closest suitable address for the desired fault type. This
is done by reading the binary code and passing it through a
disassembler to inspect the instructions’ properties.
Each test run is deﬁned by the following parameters:
fault type to be used, number of SWIFI trials, number of
faults injected per trial, driver targeted, and the workload.
After starting the driver, the test suite repeatedly injects the
speciﬁed number of faults into the driver’s text segment,
sleeping 1 second between each SWIFI trial so that the tar-
geted driver can service the workload given. A driver crash
triggers the test suite to sleep for 10 seconds, allowing the
driver manager to restart the driver transparently to appli-
cation programs and end users [17]. When the test suite
awakens, it looks up the PID of the (restarted) driver, and
continues injecting faults until the experiment ﬁnishes.
We iteratively reﬁned our design by verifying that the
driver could successfully execute its workload during each
test run and inspecting the system logs for anomalies af-
terwards. While complete coverage of all possible prob-
lems cannot be guaranteed, we injected increasingly larger
numbers of faults into different driver conﬁgurations. As
described in Sec. 7.1, the system can now survive even mil-
lions of fault injections. This result strengthens our trust in
the effectiveness of MINIX 3’s isolation techniques.
Fault Type
BINARY
POINTER
SOURCE
DESTINATION
CONTROL
PARAMETER
OMISSION
RANDOM
Affected Program Text
randomly selected address
use of in-memory operand
assignment statement
assignment statement
loop or branch instruction
operand loaded from stack
random instruction
selected from above types
Code Mutation
ﬂip one random bit
corrupt address
corrupt right hand
corrupt left hand
change control ﬂow
replace with NOPs
replace with NOPs
one of the above
Figure 7: Fault types and code mutations used for SWIFI testing.
Our test suite injected a meaningful subset of all fault
types supported by the fault injector [27, 36]. For example,
faults targeting dynamic memory allocation were left out
because this is not used by our drivers. This selection pro-
cess led to 8 suitable fault types, as summarized in Fig. 7.
To start with, BINARY faults ﬂip a bit in the program text to
emulate hardware faults. The other fault types approximate
a range of C-level programming errors commonly found in
system code. For example, POINTER faults emulate pointer
management errors, which were found to be a major cause
of system outages [35]. Likewise, SOURCE and DESTINA-
TION faults emulate assignment errors; CONTROL faults are
checking errors; PARAMETER faults represent interface er-
rors; and OMISSION faults can underly a wide variety of
errors due to missing statements [2].
Although our fault injector could not emulate all possible
(internal) error conditions [4, 6], we believe that the real is-
sue is exercising the (external) isolation techniques that con-
ﬁne the test target. In this respect, the SWIFI tests proved
to be very effective and pinpointed various shortcomings in
our design. Analysis of the results also indicates that we ob-
tained a good test coverage, since the SWIFI tests stressed
each of the isolation techniques presented in Sec. 4.
6.3 Driver Conﬁgurations and Workload
We have experimented with different kinds of drivers,
but decided to focus on MINIX 3’s networking stack after we
found that networking is by far the largest driver subsystem
in Linux 2.6: 660 KLoC or 13% of the kernel’s code base.
In particular, we used the following conﬁgurations:
1. Emulated NE2000 (Bochs v2.2.6)
2. NE2000 ISA (Pentium III 700 MHz)
3. Realtek RTL8139 PCI (AMD Athlon64 X2 3800+)
4. Intel PRO/100 PCI (AMD Athlon64 X2 3800+)
The workload used during the SWIFI tests caused a con-
tinuous stream of network I/O requests in order to exercise
the drivers’ full functionality. In particular, we maintained
a TCP connection to a remote daytime server, but this is
transparent to the working of the drivers, since they simply
put INET’s message buffers on the wire (and vice versa)
without inspecting the actual data transferred.
Although each of the drivers consists of at most thou-
sands of lines of code, more important is the driver’s inter-
action with the surrounding software and hardware. For ex-
ample, the NE2000 driver uses programmed I/O, whereas
the RTL8139 and PRO/100 drivers use DMA and require
IOMMU support. Moreover, all drivers heavily interact
with the INET server, PCI-bus driver, and kernel. There-
fore, we believe that we have picked a realistic test target
and covered a representative set of complex interactions.
7 RESULTS OF SWIFI TESTING
7.2 Unauthorized Access Attempts
We now present the results of the ﬁnal SWIFI tests per-
formed after iterative reﬁnement of the isolation techniques.
The following sections discuss the robustness against fail-
ures, unauthorized access attempts, availability under faults,
and problems encountered.
7.1 Robustness against Failures
The ﬁrst and most important experiment was designed to
stress test our isolation techniques by inducing driver fail-
ures with high probability. We conducted 32 series of 1000
SWIFI trials injecting 100 faults each—adding up to a total
of 3,200,000 faults—targeting each of the 4 driver conﬁg-
urations for each of the 8 fault types discussed in Sec. 6.
As expected, the drivers repeatedly crashed and had to be
restarted by the driver manager. (The crash reasons are in-
vestigated in Sec. 7.2.) Fig. 8 gives a histogram with the
number of failures per fault type and driver. For exam-
ple, for RANDOM faults injected into the Emulated NE2000,
NE2000, RTL8139, and PRO/100 driver we observed 826,
552, 819, and 931 failures, respectively. Although the fault
injection induced a total of 24,883 driver failures, never did
the damage (noticeably) spread beyond the driver’s protec-
tion domain and affect the rest of the OS.
The ﬁgure also shows that different fault types affected
the drivers in different ways. For example, SOURCE and
DESTINATION faults more consistently caused failures than
OMISSION faults. In addition, we also observed some differ-
ences between the drivers themselves, as is clearly visible
for POINTER and CONTROL faults. This seems logical for
the RTL8139 and PRO/100 cards that have different drivers,
but the effect is also present for the two NE2000 conﬁgura-
tions that use the same driver. We were unable to trace the
exact reasons from the logs, but speculate that this can be
attributed to the different driver-execution paths as well as
the exact timing of the fault injection.
Emulated NE2000
NE2000 ISA
RTL8139 PCI
Intel PRO/100 PCI
t
n
u
o
C
e
r
u
l
i
a
F
r
e
v
i
r
D
 1000
 875
 750
 625
 500
 375
 250
 125
 0
Bin
a
r
y
P
S
D
C
P
O
R
ointe
r
o
u
r
c
e
e
stin
o
ntr
ol
atio
n
a