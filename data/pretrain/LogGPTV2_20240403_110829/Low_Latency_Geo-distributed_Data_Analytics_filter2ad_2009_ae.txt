data locality of input tasks and fairness [37, 58], and
minimize outliers in task execution [16, 19, 61]. While
these systems optimize task placement, they do not con-
sider network contentions (which matter less within a
DC), and they do not move data around to relieve po-
tential network bottlenecks [33]. Further, Iridium is com-
plementary to approximation techniques [15, 18].
3) Optimizing communication patterns: Flow schedulers
like D3 [55], PDQ [35], DeTail [62], and D2TCP [51] aim
to improve ﬂow completion times or guarantee dead-
lines. However, they operate inside a single DC and
do not consider complex communication patterns. Or-
chestra [25], Varys [26], and Baraat [30] are network
ﬂow schedulers that optimize for completion time of
coﬂows, i.e., collections of ﬂows. However, because the
endpoints of the coﬂows are ﬁxed (e.g., source and des-
(a) In-place baseline
(b) Centralized baseline
Figure 8: WAN Bandwidth Usage knob, B. MinBW
is the scheme that optimizes for WAN bandwidth
usage. Even with same WAN usage as MinBW
(B = 1), Iridium’s gains in query response time are
signiﬁcantly higher. MinBW slows down queries
against the in-place baseline.
With just a small value of B = 1.3 (i.e., 30% higher
WAN usage than MinBW), Iridium’s query speedups of
59% and 74% are ∼ 90% of those without any WAN
usage budget (64% and 80%). This shows that Iridium
smartly uses the bandwidth budget to balance gains
in bandwidth usage with gains in query response time.
This also shows that over long periods, arrival of “high-
valued” and “low-valued” datasets overlap suﬃciently
in the workload. This is an important characteristic for
our greedy budgeted scheme to function well.
Even for B = 1 (i.e., same WAN bandwidth usage
as MinBW), Iridium’s gains in query response time are
appreciable. Crucially, MinBW results in an increase
in query response time (negative gains) with the in-
place baseline. While MinBW’s query gains are positive
compared to the centralized baseline, Iridium query gains
are handily 2× better for the same WAN usage.
7. DISCUSSION AND LIMITATIONS
We now discuss some limitations of our solutions.
Compute and Storage Constraints: Our work did
not consider limitations in compute and storage at the
sites as we believed that to be reasonable for datacen-
ters. However, as geo-distributed analytics moves to
“edge” clusters, it is conceivable that compute and stor-
age are also limitations. Under such a scenario, compute
and storage capacity have to be comprehensively con-
sidered for task and data placement. A simple approach
could do the following. To the task placement formu-
lation in §3, we add the following constraint on every
site i: ri · D ≤ Ci, where D is the compute required by
the stage and Ci is the capacity. In our data placement
heuristic, when a site is running out of storage capacity,
we will simply not consider moves into that site.
WAN Topologies: How do our heuristics change when
the core network connecting the sites is not congestion-
free? One could model pair-wise connectivity between
sites, say Bij as the available bandwidth from site i
to site j. To optimize task placement, we formulate
an LP to determine the ri’s, similar to §3.1. Given
a distribution of intermediate data Si, let Tij(rj) be
-2002040608001020304050IridiumMinBWReduction (%) in Query Response TimeReduction (%) in WAN Usage-20B=1.5B=1.3B=1020406080100020406080IridiumMinBWReduction (%) in Query Response TimeReduction (%) in WAN UsageB=1.5B=1.3B=1tination speciﬁed by location of input data and tasks),
these cannot schedule around network bottlenecks.
4) Scheduling on the WAN: There has been much work
on optimizing WAN transfers including tuning ECMP
weights [32] and adapting allocations across pre-estab-
lished tunnels [31, 39]. Also, both Google [38] and Mi-
crosoft [36] recently published details on their produc-
tion WAN networks. All this work improves the eﬃ-
ciency of the WAN by scheduling network ﬂows inside
the WAN. Instead, we optimize end-to-end application
performance, i.e., reducing response time of big-data
jobs, by placing data and tasks to explicitly reduce
load on congested WAN links. Other works optimize
data placement to improve WAN latencies and utiliza-
tion [41, 50]. Iridium optimizes much more complex com-
munication patterns, such as shuﬄes, that require co-
ordination of a large number of ﬂows across many sites.
Moreover, most of the above could be used to improve
the individual WAN transfers in Iridium.
9. CONCLUSION
Cloud organizations are deploying datacenters and
edge clusters worldwide. The services deployed at these
sites, ﬁrst-party and third-party, produce large quanti-
ties of data continuously. Results from analyzing these
geo-distributed data is used by real-time systems and
data analysts. We develop Iridium, a system that focuses
on minimizing response times of geo-distributed analyt-
ics queries. Our techniques focus on data transfers in
these queries that happen across the WAN. By carefully
considering the WAN’s heterogeneous link bandwidths
in the placement of data as well as tasks of queries, we
improve query response times in workloads derived from
analytics clusters of Bing Edge, Facebook and Conviva
by 3× − 19×. However, we would like to point out
that our approach is greedy in nature (not optimal) and
we oﬀer only a partial solution to optimizing complex
DAGs of tasks, both of which we aim to improve.
Acknowledgments
We would like to thank Kaifei Chen, Radhika Mittal
and Shivaram Venkataraman for their feedback on the
draft. We also appreciate the comments from our shep-
herd Mohammad Alizadeh and the anonymous review-
ers. This work was partially supported by NSF grants
CNS-1302041, CNS-1330308 and CNS-1345249.
References
[1] Amazon EC2 Instance Types.
http://aws.amazon.com/ec2/instance-types/.
[2] Amazon Web Services. http:
//aws.amazon.com/about-aws/global-infrastructure/.
[3] Apache Calcite. http://optiq.incubator.apache.org/.
[4] Big Data Benchmark.
https://amplab.cs.berkeley.edu/benchmark/.
[5] EC2 Pricing. http://aws.amazon.com/ec2/pricing/.
[6] Google Datacenter Locations. http://www.google.
com/about/datacenters/inside/locations/.
[7] Gurobi Optimization. http://www.gurobi.com/.
[8] Hadoop Distributed File System. http:
//hadoop.apache.org/docs/r1.2.1/hdfs design.html.
[9] How Map and Reduce operations are actually carried
out.
http://wiki.apache.org/hadoop/HadoopMapReduce.
[10] Linux Traﬃc Control.
http://lartc.org/manpages/tc.txt.
[11] Microsoft Datacenters. http://www.microsoft.com/
en-us/server-cloud/cloud-os/global-datacenters.aspx.
[12] TPC Decision Support Benchmark.
http://www.tpc.org/tpcds/.
[13] Measuring Internet Congestion: A preliminary report.
https://ipp.mit.edu/sites/default/ﬁles/documents/
Congestion-handout-ﬁnal.pdf, 2014.
[14] S. Agarwal, S. Kandula, N. Bruno, M.-C. Wu,
I. Stoica, and J. Zhou. Re-optimizing Data-Parallel
Computing. In USENIX NSDI, 2012.
[15] S. Agarwal, B. Mozafari, A. Panda, M. H., S. Madden,
and I. Stoica. BlinkDB: Queries with Bounded Errors
and Bounded Response Times on Very Large Data. In
ACM EuroSys, 2013.
[16] G. Ananthanarayanan, A. Ghodsi, S. Shenker, and
I. Stoica. Eﬀective Straggler Mitigation: Attack of the
Clones. In USENIX NSDI, 2013.
[17] G. Ananthanarayanan, A. Ghodsi, A. Wang,
D. Borthakur, S. Kandula, S. Shenker, and I. Stoica.
PACMan: Coordinated memory caching for parallel
jobs. In USENIX NSDI, 2012.
[18] G. Ananthanarayanan, M. C.-C. Hung, X. Ren,
I. Stoica, A. Wierman, and M. Yu. GRASS: Trimming
Stragglers in Approximation Analytics. USENIX
NSDI, 2014.
[19] G. Ananthanarayanan, S. Kandula, A. Greenberg,
I. Stoica, Y. Lu, B. Saha, and E. Harris. Reining in
the Outliers in Map-Reduce Clusters using Mantri. In
USENIX OSDI, 2010.
[20] Apache Hadoop NextGen MapReduce (YARN).
Retrieved 9/24/2013, URL:
http://hadoop.apache.org/docs/current/hadoop-yarn/
hadoop-yarn-site/YARN.html.
[21] A. Balachandran, V. Sekar, A. Akella, S. Seshan,
I. Stoica, and H. Zhang. Developing a Predictive
Model of Quality of Experience for Internet Video. In
ACM SIGCOMM, 2013.
[22] P. A. Bernstein, N. Goodman, E. Wong, C. L. Reeve,
and J. B. Rothnie, Jr. Query Processing in a System
for Distributed Databases (SDD-1). ACM
Transactions on Database Systems, 1981.
[23] E. Boutin, J. Ekanayake, W. Lin, B. Shi, J. Zhou,
Z. Qian, M. Wu, and L. Zhou. Apollo: Scalable and
Coordinated Scheduling for Cloud-Scale Computing.
In USENIX OSDI, 2014.
[24] M. Calder, X. Fan, Z. Hu, E. Katz-Bassett,
J. Heidemann, and R. Govindan. Mapping the
Expansion of Google’s Serving Infrastructure. In ACM
IMC, 2013.
[25] M. Chowdhury, M. Zaharia, J. Ma, M. I. Jordan, and
I. Stoica. Managing Data Transfers in Computer
Clusters with Orchestra. In ACM SIGCOMM, 2011.
[26] M. Chowdhury, Y. Zhong, and I. Stoica. Eﬃcient
Coﬂow Scheduling with Varys. In ACM SIGCOMM,
2013.
[27] W. W. Chu and P. Hurley. Optimal Query Processing
for Distributed Database Systems. IEEE Transactions
on Computers, 1982.
[46] A. Rabkin, M. Arye, S. Sen, V. Pai, and M. Freedman.
Aggregation and Degradation in JetStream: Streaming
Analytics in the Wide Area. In USENIX NSDI, 2014.
[28] J. C. Corbett, J. Dean, M. Epstein, A. Fikes, C. Frost,
[47] R. Sitaraman, M. Kasbekar, W. Lichtenstein, and
J. J. Furman, S. Ghemawat, A. Gubarev, C. Heiser,
P. Hochschild, W. Hsieh, S. Kanthak, E. Kogan, H. Li,
A. Lloyd, S. Melnik, D. Mwaura, D. Nagle,
S. Quinlan, R. Rao, L. Rolig, Y. Saito, M. Szymaniak,
C. Taylor, R. Wang, and D. Woodford. Spanner:
Google’s Globally-distributed Database. In USENIX
OSDI, 2012.
[29] J. Dean and S. Ghemawat. MapReduce: Simpliﬁed
Data Processing on Large Clusters. Communications
of the ACM, 2008.
[30] F. R. Dogar, T. Karagiannis, H. Ballani, and
A. Rowstron. Decentralized Task-aware Scheduling for
Data Center Networks. In ACM SIGCOMM, 2014.
[31] A. Elwalid, C. Jin, S. Low, and I. Widjaja. MATE:
Multipath Adaptive Traﬃc Engineering. Computer
Networks, 2002.
[32] B. Fortz, J. Rexford, and M. Thorup. Traﬃc
Engineering with Traditional IP Routing Protocols.
Communications Magazine, IEEE, 2002.
[33] R. Grandl, G. Ananthanarayanan, S. Kandula, S. Rao,
and A. Akella. Multi-Resource Packing for Cluster
Schedulers. In ACM SIGCOMM, 2014.
M. Jain. Overlay Networks: An Akamai Perspective.
In Advanced Content Delivery, Streaming, and Cloud
Services, 2014.
[48] S. Sundaresan, W. d. Donato, N. Feamster,
R. Teixeira, S. Crawford, and A. Pescape. Broadband
Internet Performance: A View From the Gateway. In
ACM SIGCOMM, 2011.
[49] A. Thusoo, J. Sarma, N. Jain, Z. Shao, P. Chakka,
N. Zhang, S. Antony, H. Liu, and R. Murthy. Hive - A
Petabyte Scale Data Warehouse using Hadoop. In
ICDE, 2010.
[50] S. Traverso, K. Huguenin, I. Trestian, V. Erramilli,
N. Laoutaris, and K. Papagiannaki. TailGate:
Handling Long-tail Content with a Little Help from
Friends. In WWW, 2012.
[51] B. Vamanan, J. Hasan, and T. N. Vijaykumar.
Deadline-Aware Datacenter TCP (D2TCP). In
Proceedings of the ACM SIGCOMM, 2012.
[52] S. Venkataraman, A. Panda, G. Ananthanarayanan,
M. Franklin, and I. Stoica. The Power of Choice in
Data-Aware Cluster Scheduling. In USENIX OSDI,
2014.
[34] A. a. Gupta et al. Mesa: Geo-Replicated, Near
[53] A. Vulimiri, C. Curino, B. Godfrey, T. Jungblut,
Real-Time, Scalable Data Warehousing. In VLDB,
2014.
[35] C.-Y. Hong, M. Caesar, and B. Godfrey. Finishing
ﬂows quickly with preemptive scheduling. ACM
SIGCOMM, 2012.
[36] C.-Y. Hong, S. Kandula, R. Mahajan, M. Zhang,
V. Gill, M. Nanduri, and R. Wattenhofer. Achieving
High Utilization with Software-Driven WAN. In ACM
SIGCOMM, 2013.
[37] M. Isard, V. Prabhakaran, J. Currey, U. Wieder,
K. Talwar, and A. Goldberg. Quincy: Fair Scheduling
for Distributed Computing Clusters. In ACM SOSP,
2009.
[38] S. Jain, A. Kumar, S. Mandal, J. Ong, L. Poutievski,
A. Singh, S. Venkata, J. Wanderer, J. Zhou, M. Zhu,
J. Zolla, U. H¨olzle, S. Stuart, and A. Vahdat. B4:
Experience with a Globally-deployed Software Deﬁned
Wan. ACM SIGCOMM, 2013.
[39] S. Kandula, D. Katabi, B. Davie, and A. Charny.
Walking the Tightrope: Responsive Yet Stable Traﬃc
Engineering. ACM SIGCOMM, 2005.
[40] D. Kossmann. The State of the Art in Distributed
J. Padhye, and G. Varghese. Global Analytics in the
Face of Bandwidth and Regulatory Constraints. In
USENIX NSDI, 2015.
[54] A. Vulimiri, C. Curino, B. Godfrey, K. Karanasos, and
G. Varghese. WANalytics: Analytics for a
Geo-distributed Data-intensive World. In CIDR, 2015.
[55] C. Wilson, H. Ballani, T. Karagiannis, and
A. Rowtron. Better Never Than Late: Meeting
Deadlines in Datacenter Networks. ACM SIGCOMM,
2011.
[56] Z. Wu, M. Butkiewicz, D. Perkins, E. Katz-Bassett,
and H. Madhyastha. SPANStore: Cost-eﬀective
Geo-replicated Storage Spanning Multiple Cloud
Services. In ACM SOSP, 2013.
[57] Y. Yu, P. K. Gunda, and M. Isard. Distributed
Aggregation for Data-Parallel Computing: Interfaces
and Implementations. In ACM SOSP, 2009.
[58] M. Zaharia, D. Borthakur, J. Sen Sarma,
K. Elmeleegy, S. Shenker, and I. Stoica. Delay
scheduling: a simple technique for achieving locality
and fairness in cluster scheduling. In ACM EuroSys,
2010.
Query Processing. ACM Computer Survey, 2000.
[59] M. Zaharia, M. Chowdhury, M. Franklin, S. Shenker,
[41] N. Laoutaris, M. Sirivianos, X. Yang, and
P. Rodriguez. Inter-datacenter Bulk Transfers with
Netstitcher. ACM SIGCOMM, 2011.
[42] P. Mohan, A. Thakurta, E. Shi, D. Song, and
D. Culler. GUPT: Privacy Preserving Data Analysis
Made Easy. In ACM SIGMOD, 2012.
[43] E. Nygren, R. Sitaraman, and J. Sun. The Akamai
Network: A Platform for High-Performance Internet
Applications. In ACM SIGOPS OSR, 2010.
[44] M. T. Ozsu and P. Valduriez. Principles of Distributed
Database Systems. 2011.
[45] A. Pavlo, E. Paulson, A. Rasin, d. Abadi, Daniel
an dDeWitt, S. Madden, and M. Stonebraker. A
Comparison of Approaches to Large-Scale Data
Analysis. In ACM SIGMOD, 2009.
and I. Stoica. Spark: Cluster Computing with
Working Sets. In USENIX HotCloud, 2010.
[60] M. Zaharia, T. Das, H. Li, T. Hunter, S. Shenker, and
I. Stoica. Discretized Streams: Fault-Tolerant
Streaming Computation at Scale. In ACM SOSP,
2013.
[61] M. Zaharia, A. Konwinski, A. Joseph, R. Katz, and
I. Stoica. Improving MapReduce performance in
heterogeneous environments. In USENIX OSDI, 2008.
[62] D. Zats, T. Das, P. Mohan, D. Borthakur, and
R. Katz. DeTail: Reducing the Flow Completion Time
Tail in Datacenter Networks. ACM SIGCOMM, 2012.