evasion rates. A(cid:138)ackers can take advantage of the small search
space by using procedural noise to cra(cid:137) query-e(cid:129)cient untargeted
black-box a(cid:138)acks as we will show in Sect. 5.
Strengths & Limitations. Procedural noise is one of the (cid:128)rst
black-box generation of UAPs. Other existing black-box a(cid:138)acks
o(cid:137)en optimize for input-speci(cid:128)c evasion, and other existing UAP
generation methods are white-box or grey-box. Whilst procedural
noise is also a generative model, it di(cid:130)ers from other generative
models like Bayesian networks, Generative Adversarial Networks
(GANs), or Variational Autoencoders (VAEs) as it does not require
the additional overhead of building and training these generative
models–which o(cid:137)en requires more resources and stronger adver-
saries to execute successfully.
Comparing procedural noise a(cid:138)acks with existing black-box at-
tacks is not straightforward, as procedural noise naturally has high
universal evasion rates. (cid:140)is gives procedural noise an advantage
in that, despite having no access to the target model, randomly
drawn procedural noise pa(cid:138)erns are likely to have high universal
evasion. (cid:140)e search space for our procedural noise has only four
dimensions, whereas most a(cid:138)acks are designed for the whole input
space of hundreds of thousands of dimensions. However, this does
come at a cost, as procedural noise functions do not capture ad-
versarial perturbations outside their codomain. Other a(cid:138)acks that
explore the whole input space are able to take full advantage of a
stronger adversary (i.e. white-box or grey-box se(cid:138)ing) or larger
query limits. We explore this trade-o(cid:130) further with input-speci(cid:128)c
black-box a(cid:138)acks in Sect. 5.3.
Another limitation of this experiment was the use of an (cid:96)∞-norm
constraint. Although it is o(cid:137)en used in the literature as a proxy
for human perception, (cid:96)p-norms have limitations [16] e.g., low-
frequency pa(cid:138)erns appear to be more visible than high-frequency
pa(cid:138)erns for the same (cid:96)p-norm. (cid:140)is frequency could be used as
an additional constraint and developing a more reliable proxy for
human perception remains an interesting avenue for future work.
Summary. Procedural noise a(cid:138)acks specialize as untargeted
black-box perturbations with naturally high universal evasion. Of-
fensively, this has applications as a large-scale indiscriminate a(cid:138)ack
and, defensively, as a standard test on machine learning services.
We expand on this perspective in the following sections. In Sect. 5,
we develop query-e(cid:129)cient black-box a(cid:138)acks using procedural noise
and Bayesian optimization. In Sect. 6, we apply our procedural
noise a(cid:138)ack against a DCN designed for object detection, showing
that the a(cid:138)ack can generalize to other tasks. In Sect. 7, we test an
input-agnostic defence in median (cid:128)lter denoising.
a(cid:138)acks and show that Bayesian optimization is an e(cid:129)cient method
for choosing parameters in such black-box a(cid:138)acks.
5.1 Bayesian Optimization
Bayesian optimization is a sequential optimization algorithm used
to (cid:128)nd optimal parameters for black-box objective functions [43, 68].
(cid:140)is technique is o(cid:137)en e(cid:130)ective in solving various problems with
expensive cost functions such as hyperparameter tuning, reinforce-
ment learning, and combinatorial optimization [66]. Bayesian op-
timization consists of a probabilistic surrogate model, usually a
Gaussian Process (GP), and an acquisition function that guides its
queries. GP regression is used to update the belief on the parameters
with respect to the objective function a(cid:137)er each query [66].
Gaussian Processes. A GP is the generalization of Gaussian
distributions to a distribution over functions and is typically used
as the surrogate model for Bayesian optimization [57]. We use GPs
as they induce a posterior distribution over the objective function
that is analytically tractable. (cid:140)is allows us to update our beliefs
about the objective function a(cid:137)er each iteration [68].
A Gaussian Process GP(m, k) is fully described by a prior mean
function m : X → R and positive-de(cid:128)nite kernel or covariance
function k : X × X → R. We describe GP regression to understand
how our GP is updated when more observations are available. (cid:140)e
following expressions give the GP prior and Gaussian likelihood
respectively
p(f | X) = N(m(X), K)
p(y | f , X) = N(f (X), σ
2I)
where N denotes a normal distribution. Elements of the mean and
covariance matrix are given by mi = m(xi) and Ki, j = k(xi , xj).
Given observations {X, y} and an arbitrary point x, the updated
posterior mean and covariance on the n-th query are given by
mn(x) = m(x) − (K + σ
kn(x, x) = k(x, x) − k(x, X)(K + σ
2I)−1(y − m(X))
2I)−1
k(X, x).
We take the mean function to be zero m ≡ 0 to simplify evalu-
ation and since no prior knowledge can be incorporated into the
mean function [66] as is the case for black-box se(cid:138)ings. A GP’s
ability to model a rich distribution of functions rests on its covari-
ance function which controls important properties of the function
distribution such as di(cid:130)erentiability, periodicity, and amplitude
[57, 66]. Any prior knowledge of the target function is encoded
in the hyperparameters of the covariance function. In a black box
se(cid:138)ing, we adopt a more general covariance function in the Mat´ern
5/2 kernel
(cid:32)
k5/2(x, x(cid:48)) =
1 +
√5r
l
+
2
2
5r
3l
exp
−
(cid:33)
(cid:32)
(cid:33)
√5r
l
5 EFFICIENT BLACK-BOX ATTACKS
Whilst in previous sections we have shown that procedural noise
functions are an e(cid:129)cient way to generate adversarial perturbations,
another signi(cid:128)cant advantage they bring is their low-dimensional
search space. (cid:140)is enables the use of query-e(cid:129)cient black-box
optimization techniques that otherwise do not scale well to high-
dimensional problems. In this section, we compare several black-
box optimization techniques for both input-speci(cid:128)c and universal
where r = x−x(cid:48) and l is the length-scale parameter [68]. (cid:140)is results
in twice-di(cid:130)erentiable functions, an assumption that corresponds
to those made in popular black-box optimization algorithms like
quasi-Newton methods [68].
Acquisition Functions. (cid:140)e second component in Bayesian
optimization is an acquisition function that describes how optimal a
query is. Intuitively, the acquisition function evaluates the utility of
candidate points for the next evaluation [7]. (cid:140)e two most popular
8
choices are the Expected Improvement (EI) and Upper Con(cid:128)dence
2(x) as the predictive
Bound (UCB) [66]. First we de(cid:128)ne µ(x) and σ
mean and variance of ❕(x) respectively. Let γ(x) = ❕(xbest)−µ(x)
.
(cid:140)e acquisition functions are
σ(x)
αEI(x) = σ(x)(γ(x)Φ(γ(x)) + N(γ(x) | 0, 1))
αUCB(x) = µ(x) + κσ(x),
κ > 0
where Φ is the normal cumulative distribution function. EI and
UCB have both been shown to be e(cid:130)ective and data-e(cid:129)cient in
real black-box optimization problems [68]. However, most studies
have found that EI converges near-optimally and is be(cid:138)er-behaved
than UCB in the general case [7, 66, 68]. (cid:140)is makes EI the best
candidate for our acquisition function.
5.2 Universal Black-box Attack
For a universal black-box a(cid:138)ack using procedural noise, the goal
is to (cid:128)nd the optimal parameters δ∗ for the procedural noise gen-
erating function G so that the universal evasion rate of perturba-
tion G(δ∗) generalizes to unknown inputs Xval. (cid:140)e a(cid:138)acker has
a smaller set of inputs Xtrain and optimizes their perturbation δ∗
for this dataset. (cid:140)e performance of the a(cid:138)ack is measured by its
universal evasion rate over the validation set Xval. In a practical
se(cid:138)ing, this is where the a(cid:138)acker optimizes their procedural noise
UAP over a small dataset, then injects that optimized perturbation
to other inputs–with the goal of causing as many misclassi(cid:128)cations
as possible.
Experiment. We use the Inception v3 model, (cid:96)∞-norm ε = 16,
and the same 5,000 data points tested in Sect. 4 as Xval. Xtrain are
points from the ILSVRC2012 validation set not in Xval. We test for
training set sizes of 50, 125, 250, and 500, which corresponds to 1%,
2.5%, 5%, and 10% of the validation set size.
We compare Bayesian optimization with Limited-memory BFGS
(L-BFGS) [38], a quasi-Newton optimization algorithm that is o(cid:137)en
used in black-box optimization and machine learning. As the proce-
dural noise functions are non-di(cid:130)erentiable, we estimate gradients
using (cid:128)nite di(cid:130)erence approximation. (cid:140)is gradient approximation
is similar to what is used for other black-box a(cid:138)acks like zeroth-
order optimization [12], but here it is applied to a signi(cid:128)cantly
smaller search space. When L-BFGS converges, possibly to a local
optima, we restart the optimization with a di(cid:130)erent random initial
point, stopping when the query limit is reached and choosing the
best optima value found.
We set a maximum query limit of 1,000 universal evasion eval-
uations on the training set Xtrain. In practice, this limit was not
necessary as both algorithms converged faster to their optimal
values: within the (cid:128)rst 100 queries for Bayesian optimization and
within the (cid:128)rst 250 queries for L-BFGS. (cid:140)ese are untargeted uni-
versal black-box a(cid:138)acks where the adversary has no knowledge of
the target model and only requires the top label from the model’s
outputs.
Results. (cid:140)e best procedural noise UAP computed from the
training sets generalized well to the much larger validation set, con-
sistently reaching 70% or more universal evasion on the validation
set for Perlin noise. (cid:140)is is a surprising result as the training sets
were 10-100 times smaller than the validation set. (cid:140)is may be due
to the inherent universality of our procedural noise perturbations.
We focus on Perlin noise as it outperforms Gabor noise, with the
la(cid:138)er averaging 58% universal evasion on the validation set.
Table 3 shows that Bayesian optimization (BayesOpt) reliably
outperforms L-BFGS in terms of universal evasion rate on the train-
ing sets and the resulting universal evasion rates on the validation
set. For comparison, random parameters for Perlin noise in Sect. 4
had a 98th percentile of 70.2% and a maximum of 73.1% universal
evasion. Bayesian optimization consistently reached or passed this
98th percentile while L-BFGS did not. It is reasonable for these
optimization algorithms not to beat the maximum since the train-
ing sets were signi(cid:128)cantly smaller. Similar trends appear between
Bayesian optimization, random selection, and L-BFGS for Gabor
noise perturbations. We include these results in the Appendix D.
Table 3: Comparison on Inception v3 between universal Per-
lin noise black-box attacks. Universal evasion rates (%) of
the optimized perturbations are shown for their respective
training set and the validation set.
Train Size
50
125
250
500
BayesOptper
Val.
Train
78.0
71.4
70.2
77.6
71.2
71.6
75.0
72.9
L-BFGSper
Train
74.0
76.0
71.2
73.4
Val.
69.9
71.5
69.7
70.8
5.3 Input-speci(cid:128)c Black-box Attack
In this section, we use procedural noise for an input-speci(cid:128)c black-
box algorithm. (cid:140)e goal of the adversary is to evade as many inputs
in the dataset, maximizing the input-speci(cid:128)c evasion rate on inputs
that are not misclassi(cid:128)ed by the model. An important metric here
is the query-e(cid:129)ciency of these a(cid:138)acks, as requiring large volumes
of queries per sample becomes impractical in real-world scenarios.
Metrics. We de(cid:128)ne the success rate of an a(cid:138)ack to be its input-
speci(cid:128)c evasion excluding clean inputs that are already misclassi(cid:128)ed.
(cid:140)e average queries is measured over successful evasions.
Experiment. We use the Inception v3 model, (cid:96)∞-norm ε = 16,
and the same 5,000 data points from Sect. 4. For a given input
x, the goal is to achieve evasion with arg max f (x + s) (cid:44) τ(x) by
minimizing the probability of the true class label τ(x). In this case
we allow the a(cid:138)acker to access the model’s output probability vec-
tor, as the black-box optimization algorithms gain minimal usable
information if the output is binary (τ(x) or ¬τ(x)).
As in the previous section, we compare Bayesian optimization
and L-BFGS with a limit of 1,000 queries per input and number
of restarts when it converges. We use sparse GPs to be(cid:138)er scale
Bayesian optimization [42], as the standard GP scales cubically
with the number of observations. We also compare with uniform
random parameter selection from Sect. 4. (cid:140)ese are untargeted
input-speci(cid:128)c black-box a(cid:138)acks where the adversary has no knowl-
edge of the target model and only requires the output probability
vector of the model.
Results. Table 4 shows that Bayesian optimization reached a
high 91.6% success rate with with just 7 queries per successful
evasion on average, under a restrictive 100 query limit. (cid:140)is vastly
9
Table 4: Comparison on Inception v3 between our input-
speci(cid:128)c Perlin noise black-box attacks and bandit attack
[25] for di(cid:130)erent query limits.
A(cid:138)ack
BayesOptper
BayesOptper
L-BFGSper
L-BFGSper
Randomper