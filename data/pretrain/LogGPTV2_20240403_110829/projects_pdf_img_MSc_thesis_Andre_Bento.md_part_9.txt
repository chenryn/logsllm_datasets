onemustindexthembyspanidanthenreadeveryspan,indexingthespanusingtheirpar-
entId. After applying this method, spans will be properly indexed and a list of SpanTrees
are produced. Also, a SpanTree is a representation of a trace and these structures ease
tracing handling due to distinct causal relationships between spans. For example, one can
use span trees to map TraceInfos. This data structure was created to hold relevant infor-
mation from span trees: for example request work-flows. The process involves pinpointing
requests between services, presented in spans throughout their causal relationship, and
then store request paths through services, generating the corresponding request work-
flow. For each span tree, one work-flow is generated, however, from root to leafs, multiple
paths are possible. Note that not always do spans contain information to produce the
46
Implementation Process
path, and therefore, some request paths are dubious, depending only on the completeness
quality of tracing. The method to produce request work-flows is described in Algorithm 3.
Algorithm 3: Work-flow type algorithm.
Data: Trace files/Trace data.
Result: Comma-separated values (CSV) with unique work-flow types, their
corresponding count and times.
1 Read start_time and end_time from configuration;
2 Read SpanList from trace files/trace data within defined time_frame;
3 while have Spans in SpanList do
4 Read Span;
5 Map Span to SpanTrees;
6 while have SpanTree in SpanTrees do
7 Read SpanTree;
8 Map SpanTree to TraceInfos;
9 Read TraceInfo;
10 Read work-flows, work-flow count, times, (others) from TraceInfo;
11 Write fields to CSV files;
The method described in Algorithm 3 aims to use tracing to produce span trees, and
then generate TraceInfos to retrieve request work-flow paths.
These two data structures, service dependency graphs and span trees, are the founda-
tions to extract metrics from tracing data, satisfy the functional requirements presented
in Section 4.1 and answer the final research questions defined in Section 3.2.
The metrics that OTP is able to extract from tracing data, for a defined time interval,
are the following:
1. Number of incoming/outgoing service calls;
2. Average response time by service;
3. Service connection, i.e., other services invoking and being invoked by the system,
i.e., the service dependency graph variation.
4. Service degree (in/out/total);
5. Service HTTP status code ratio. (sum of success or failure count over total status
code count)
These metrics are all related with time and represent observations of values extracted
from tracing data, therefore, as time-series metrics they are stored in a Time Series
Database (TSDB). Explanation of used technology and procedure is provided later on
this Section.
Table 5.3 relates each metric with a functional requirement, and correspondent final
research question. Functional requirements are identified by an id from Table 4.1.
47
Chapter 5
Table5.3: Relationsbetweenfinalresearchquestions,functionalrequirementsandmetrics.
Research Functional Metrics
Question Requirements
1. Is there any FR-5; Number of incoming service calls;
anomalous service? FR-5; Number of outgoing service calls;
FR-6. Average response time by service.
2. What is the over- FR-7; No metric extracted;
all reliability of the FR-8. Service HTTP status code ratio.
service?
3. Which service FR-9; Service degree;
consumes more time FR-10. Service dependency graph variation.
when considering
the entire set of
requests?
For Table 5.3, only functional requirements from numbers 5 to 10 were considered, due
to being the ones related with metrics extraction. As said before, only question number 1
was considered for metrics extraction. The remaining, defined at grey colour, were imple-
mented and OTP extracts them, however, they were not further analysed in this research.
Almost all functional requirements have one metric associated except one, FR-7. This
functional requirement was implemented, and our solution allows to generate work-flow
pathsfromtracingdata,however,nometricwasdefined. Nevertheless,theimplementation
of this functionality helped us understanding results for the first final research question –
Method for work-flow generation from tracing data is explained Algorithm 3.
Spantreesarearepresentationofcausalrelationshipbetweenspans. Twotypesoftime
based metrics are extracted from span trees: 1. Average response time by service in time;
and 2. Service HTTP status code ratio in time. To extract the first metric type, duration
and annotations/endpoint/serviceName values presented in spans , when defined, are used
to calculate the average response time by service. For each span tree a list of services and
theircorrespondingaveragetimesareobtained. Aftergatheringallvaluesfromeveryspan
tree presented in the defined time-frame, the values are merged and posted to the TSDB.
Thesecondmetric, isextractedthroughacalculationofstatuscodesratiobyeachservice.
For this, binaryAnnotations/http.status_code and annotations/endpoint/serviceName val-
ues are used. Also, equally to the previous metric, values are merged and posted to the
TSDB.
Servicedependencygraphsarearepresentationofdependenciesofservicesataspecific
time-frame. Three types of time based metrics are extracted from service dependency
graphs: 1. Number of incoming/outgoing service calls in time; 2. Entry/exit of services
in time (service dependency graph node variation); and 3. Service degree (in/out/total)
in time. To extract the first metric type, the values in between (Edges) services (Nodes)
are retrieved. These values are dispatched for storage with service name, flow indication
(incoming/outgoing), timestampandnumberofcalls. Thesecondmetrictypeisextracted
having two successive graphs and performing their difference. For example, if GraphA has
nodes A,B,C and GraphB, nodes A,C,D,E, the difference between them will result in
two service entries D,E and one exit. Last metric type, service degree, is extracted
by retrieving the number of connections from each service. For example, consider that
GraphC has a service A connected from itself to services B,C,D. In this graph, service
A has an out degree of three and an in degree of zero. The remaining services have an out
degree of zero and an in degree of one. Methods to extract these metrics are implemented
48
Implementation Process
in Graph Processor and resource to NetworkX to handle graph structures. All these
metrics are then posted to the TSDB.
At this point, our solution is able to retrieve and store time-series metrics from tracing
data. For the TSDB, we have decided to use OpenTSDB, due to technical restrictions
imposed in Section 4.3. There was a client implementation for usage in Python, however,
thesupportwasnotgoodduetolackofupdatesandcleardocumentation. Forthisreason,
we decided to implement our own OpenTSDB client in Python using their Application
ProgrammingInterface(API)specification, –Metrics Repository component. Later, when
all implementation from tracing collection through trace metrics storage in the TSDB, we
used a browser metrics visualizer. To do this, a Docker container with Grafana, a data
visualization tool capable of rendering time-series metrics in charts and present them
in dashboards. The decision to use this tool, was due to easy to setup and integrated
compatibility with our TSDB. We just needed to create a container and, through a url
configuration in Grafana, we established a link to the TSDB.
Figures 5.3, 5.4, 5.5, and 5.6, contain sample representations of extracted time-series
metrics stored in our TSDB.
Service A Calls [Incoming] Service A Calls [Total] Service A Calls [Outgoing]
40
sllac
1500 1500
30 gnimocni
eulav eulav
1000 1000
20 fo latoT tuO
10 500 500 rebmuN
0 0 0
06-28 00 06-28 04 06-28 08 06-28 12 06-28 16 06-28 20 06-29 00 06-29 04 06-29 08 06-28 0 00 6-28 0 04 6-28 0 08 6-28 1 02 6-28 1 06 6-28 2 00 6-29 0 00 6-29 0 04 6-29 08 06-28 0 00 6-28 0 04 6-28 0 08 6-28 1 02 6-28 1 06 6-28 2 00 6-29 0 00 6-29 0 04 6-29 08
Time (Month-Day Hour) Time (Month-Day Hour) Time (Month-Day Hour)
Service B Calls [Incoming] Service B Calls [Total] Service B Calls [Outgoing]
200 25 sllac
200 20 150 150 15 gnimocni eulav eulav
100 100 10 fo rebmuN latoT tuO
50 50 5
0 0 0
06-28 12 06-28 15 06-28 18 06-28 21 06-29 00 06-29 03 06-29 06 06-29 09 06-28 12 06-28 15 06-28 18 06-28 21 06-29 00 06-29 03 06-29 06 06-29 09 06-28 12 06-28 15 06-28 18 06-28 21 06-29 00 06-29 03 06-29 06 06-29 09
Time (Month-Day Hour) Time (Month-Day Hour) Time (Month-Day Hour)
Service C Calls [Incoming] Service C Calls [Total] Service C Calls [Outgoing]
400
30 sllac
300 300 25 gnimocni
20 eulav eulav
200 200 15 fo latoT tuO
100 100 10 rebmuN
5
0 0 0
06-28 0 00 6-28 0 04 6-28 0 08 6-28 1 02 6-28 1 06 6-28 2 00 6-29 0 00 6-29 0 04 6-29 08 06-28 0 00 6-28 0 04 6-28 0 08 6-28 1 02 6-28 1 06 6-28 2 00 6-29 0 00 6-29 0 04 6-29 08 06-28 00 06-28 04 06-28 08 06-28 12 06-28 16 06-28 20 06-29 00 06-29 04 06-29 08
Time (Month-Day Hour) Time (Month-Day Hour) Time (Month-Day Hour)
Figure 5.3: Service calls samples.
Service dependency variation [Gain] Service dependency variation [Total] Service dependency variation [Loss]
3.0
12 12
10 2.5
10
8 2.0 8 eulav eulav eulav
6
6 4 1.5 niaG latoT ssoL
4 2 1.0
2 0 0.5
2
0 0.0
06-28 0 00 6-28 0 04 6-28 0 08 6-28 1 02 6-28 1 06 6-28 2 00 6-29 0 00 6-29 0 04 6-29 08 06-28 0 00 6-28 0 04 6-28 0 08 6-28 1 02 6-28 1 06 6-28 2 00 6-29 0 00 6-29 0 04 6-29 08 06-28 0 00 6-28 0 04 6-28 0 08 6-28 1 02 6-28 1 06 6-28 2 00 6-29 0 00 6-29 0 04 6-29 08
Time (Month-Day Hour) Time (Month-Day Hour) Time (Month-Day Hour)
Figure 5.4: Service dependency variation samples.
49
Chapter 5
Service A Average Response Time Service B Average Response Time Service C Average Response Time
4.5 4.0 5 )sdnocesillim )sdnocesillim )sdnocesillim
5
3.5 4
4 3.0 2.5 3 5e1( 6e1( 6e1(
3 2.0 emiT emiT emiT
2
1.5 esnopseR esnopseR esnopseR
2
1.0 1
1 0.5
0
06-28 0 00 6-28 0 04 6-28 0 08 6-28 1 02 6-28 1 06 6-28 2 00 6-29 0 00 6-29 0 04 6-29 08 06-28 12 06-28 15 06-28 18 06-28 21 06-29 00 06-29 03 06-29 06 06-29 09 06-28 0 00 6-28 0 04 6-28 0 08 6-28 1 02 6-28 1 06 6-28 2 00 6-29 0 00 6-29 0 04 6-29 08
Time (Month-Day Hour) Time (Month-Day Hour) Time (Month-Day Hour)
Figure 5.5: Service average response time samples.
Service A Request Status code ratios Service B Request Status code ratios Service C Request Status code ratios
1.0 2XX 1.0 1.0
4XX
0.8 5XX 0.8 0.8
0.6 0.6 0.6 2XX oitaR oitaR oitaR
4XX
0.4 0.4 0.4
0.2 0.2 0.2 2XX
4XX
0.0 0.0 0.0 5XX
06-28 0 00 6-28 0 04 6-28 0 08 6-28 1 02 6-28 1 06 6-28 2 00 6-29 0 00 6-29 0 04 6-29 08 06-28 12 06-28 15 06-28 18 06-28 21 06-29 00 06-29 03 06-29 06 06-29 09 06-28 0 00 6-28 0 04 6-28 0 08 6-28 1 02 6-28 1 06 6-28 2 00 6-29 0 00 6-29 0 04 6-29 08
Time (Month-Day Hour) Time (Month-Day Hour) Time (Month-Day Hour)
Figure 5.6: Service status code ratio samples.
Figure 5.3 represent samples about the number of service request calls metric. In
this Figure, we have 9 plots, three in each row, representing three variations (incoming,
outgoing and total) of this metric for one service. In this metric we can clearly see the
lack of information presented in tracing for the beginning of the first day.
Figure 5.4 contain samples about service dependency variation, one for each metric
(gain, loss and total). Total are the result of gain−loss. Gain stands for the number of
new service entries in system, and loss, represent the number of service exits in system.
Last Figure, 5.6, shows the gathering of status code ratio samples for three distinct
services. The ratio varies from 0.0 to 1.0, and represent the proportion of status code
groups (2xx – Success, 4xx – Client error and 5xx – Other errors).
Also, service dependency graphs are stored for further access after being processed by
Graph Processor component. We have decided to use ArangoDB as our Graph Database
(GDB).Thisdecisionwasbasedinthe“Multidata-typesupport”providedbythisdatabase,
allowing us to extend our graph structures to whatever we wanted, enhancing our graph
storage possibilities and relieving the implementation from parsing data-types. This
database has a Python client, pyArango [62], which revealed lack of features, leading
to propositions for functionality creation and issue declarations in GitHub. However, the
answers were not pleasant due to lack of support and people to maintain the project [63].
This have lead to some difficulties when implementing Graphs Repository component in
OTP. Difficulties from storing graphs with custom names to custom graph retrieval were
felted. Thesolutionwastoforktheproject,performchangesanduseourcustompyArango
client. This changes were committed for review to the original project. We could not mit-
igate these problems in advance because they were only perceived when using the client.
After presenting the first component, OTP, from our proposed solution, next Sec-
tion 5.3 - Data Analysis Component covers the implementation of the second component
presented in our solution.
50
Implementation Process
5.3 Data Analysis Component
In this Section, the implementation of the second component presented in our pro-
posed solution, “Data Analysis” component, is presented and expected outcomes from
each analysis are discussed.
“Data Analysis” component has the main objective of detecting anomalies, presented
in services, using time-series metrics extracted from tracing data using the component
presented in previous Section and perform tracing quality analysis.
In our implementation, this component is detached from the remaining components,
however, in architectural terms we have decided to place it has being part of the overall
solution. This is because there is nothing preventing total integration with other compo-
nents presented in the solution. The reason to implement these methods detached from
the remaining, was to ease our research path and increase flexibility. This means that,
to ease our data exploration, implement these methods in Notebooks detached from the
overall components, allowed us to change code effortlessly and conceded focus on methods
development. JupyterNotebook[64]wasthenotebookchosenformethodimplementation,
hence, one server was created to hold our implementations in notebooks.
In this case, extracted time-series data belong to unlabelled data group. Data can
belong to unlabelled or labelled groups. Unlabelled data are information sampled from of
natural,orhuman-createdartefacts,thatonecanobtainfromobservingandrecordvalues.
In this group, there is no “explanation” for each piece of data, as it just contains the data,
and nothing else. Labelled data typically takes a set of unlabelled data and augments
each piece of data with some sort of meaningful “tag”, “label”, or “class” that is somehow
informative or desirable to know. For example, for this solution, having labelled data
would help in identifying anomalies presented in our data set. However, only unlabelled
data was provided, and therefore, we needed to work with unlabelled data and perform
anomaly detection with it [65].
So, the approach was to use processed data produced from OTP, and perform the
analysis using “Data Analysis” component to point out service problems and perform
tracingqualityanalysis, asdefinedinFigure4.1, toanswerquestionsdefinedinChapter3: