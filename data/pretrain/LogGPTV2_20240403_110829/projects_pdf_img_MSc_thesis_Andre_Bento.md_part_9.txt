### Indexing and Reading Spans

To properly index spans, one must first index them by their span ID and then read each span, indexing it using its parent ID. This method ensures that spans are correctly indexed, resulting in a list of `SpanTree` structures. A `SpanTree` represents a trace, and these structures simplify tracing handling due to the distinct causal relationships between spans. For example, `SpanTree` can be used to map `TraceInfo`. The `TraceInfo` data structure was created to hold relevant information from span trees, such as request workflows.

The process involves pinpointing requests between services, presented in spans throughout their causal relationships, and then storing request paths through services, generating the corresponding request workflow. For each `SpanTree`, one workflow is generated, but multiple paths from the root to the leaves are possible. Note that not all spans contain complete information to produce a path, so some request paths may be uncertain, depending on the quality of the tracing data.

### Algorithm for Workflow Generation

**Algorithm 3: Workflow Type Algorithm**

- **Input:** Trace files/trace data.
- **Output:** Comma-separated values (CSV) with unique workflow types, their corresponding count, and times.

1. Read `start_time` and `end_time` from configuration.
2. Read `SpanList` from trace files/trace data within the defined time frame.
3. While there are spans in `SpanList`:
   1. Read a span.
   2. Map the span to `SpanTrees`.
   3. While there are `SpanTree` in `SpanTrees`:
      1. Read a `SpanTree`.
      2. Map the `SpanTree` to `TraceInfos`.
      3. Read `TraceInfo`.
      4. Extract workflows, workflow counts, and times from `TraceInfo`.
      5. Write fields to CSV files.

This algorithm uses tracing to produce `SpanTree` structures and then generates `TraceInfos` to retrieve request workflow paths.

### Data Structures and Metrics

Two primary data structures, service dependency graphs and span trees, form the foundation for extracting metrics from tracing data. These metrics help satisfy the functional requirements presented in Section 4.1 and answer the final research questions defined in Section 3.2.

#### Metrics Extracted from Tracing Data

For a defined time interval, the following metrics can be extracted:

1. **Number of incoming/outgoing service calls.**
2. **Average response time by service.**
3. **Service connection:** Other services invoking and being invoked by the system, i.e., the service dependency graph variation.
4. **Service degree (in/out/total).**
5. **Service HTTP status code ratio:** Sum of success or failure count over total status code count.

These metrics are time-series data and are stored in a Time Series Database (TSDB).

### Relation Between Research Questions, Functional Requirements, and Metrics

| Research Question | Functional Requirement | Metrics |
|-------------------|------------------------|---------|
| Is there any anomalous service? | FR-5: Number of incoming service calls; <br> FR-5: Number of outgoing service calls; <br> FR-6: Average response time by service. | - |
| What is the overall reliability of the service? | FR-7: No metric extracted; <br> FR-8: Service HTTP status code ratio. | - |
| Which service consumes more time when considering the entire set of requests? | FR-9: Service degree; <br> FR-10: Service dependency graph variation. | - |

Only functional requirements from numbers 5 to 10 were considered for metrics extraction. Only question number 1 was analyzed in detail, while the others were implemented but not further analyzed in this research.

### Extraction of Time-Based Metrics

#### Span Trees

- **Average Response Time by Service in Time:**
  - Uses duration and annotations/endpoint/serviceName values from spans to calculate the average response time by service.
  - For each `SpanTree`, a list of services and their corresponding average times is obtained.
  - Values from all `SpanTree` within the defined time frame are merged and posted to the TSDB.

- **Service HTTP Status Code Ratio in Time:**
  - Uses binaryAnnotations/http.status_code and annotations/endpoint/serviceName values to calculate the status code ratio by each service.
  - Values are merged and posted to the TSDB.

#### Service Dependency Graphs

- **Number of Incoming/Outgoing Service Calls in Time:**
  - Retrieves values between (Edges) services (Nodes).
  - Dispatches these values for storage with service name, flow indication (incoming/outgoing), timestamp, and number of calls.

- **Entry/Exit of Services in Time (Service Dependency Graph Node Variation):**
  - Extracted by comparing two successive graphs and performing their difference.

- **Service Degree (In/Out/Total) in Time:**
  - Retrieves the number of connections from each service.
  - Methods for extracting these metrics are implemented in the Graph Processor and use NetworkX for handling graph structures.
  - All metrics are posted to the TSDB.

### Storage and Visualization

- **Time Series Database (TSDB):** OpenTSDB was chosen due to technical restrictions. An existing Python client was found lacking, so a custom OpenTSDB client was implemented.
- **Visualization:** A Docker container with Grafana was used to visualize the time-series metrics. Grafana was chosen for its easy setup and compatibility with the TSDB.

### Sample Visualizations

- **Figures 5.3, 5.4, 5.5, and 5.6** show sample representations of extracted time-series metrics stored in the TSDB.

### Data Analysis Component

The "Data Analysis" component aims to detect anomalies in services using time-series metrics extracted from tracing data. It is implemented separately to ease research and increase flexibility. Jupyter Notebook was used for method implementation, and unlabelled data was processed to perform anomaly detection.

This component helps in identifying service problems and performing tracing quality analysis, as defined in Figure 4.1, to answer the questions defined in Chapter 3.