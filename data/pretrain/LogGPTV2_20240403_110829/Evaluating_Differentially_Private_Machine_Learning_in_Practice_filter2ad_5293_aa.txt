title:Evaluating Differentially Private Machine Learning in Practice
author:Bargav Jayaraman and
David Evans
Evaluating Differentially Private Machine 
Learning in Practice
Bargav Jayaraman and David Evans, University of Virginia
https://www.usenix.org/conference/usenixsecurity19/presentation/jayaraman
This paper is included in the Proceedings of the 28th USENIX Security Symposium.August 14–16, 2019 • Santa Clara, CA, USA978-1-939133-06-9Open access to the Proceedings of the 28th USENIX Security Symposium is sponsored by USENIX.Evaluating Diﬀerentially Private Machine Learning in Practice
Bargav Jayaraman and David Evans
Department of Computer Science
University of Virginia
Abstract
Diﬀerential privacy is a strong notion for privacy that can be
used to prove formal guarantees, in terms of a privacy budget,
, about how much information is leaked by a mechanism.
When used in privacy-preserving machine learning, the goal
is typically to limit what can be inferred from the model about
individual training records. However, the calibration of the
privacy budget is not well understood. Implementations of
privacy-preserving machine learning often select large values
of  in order to get acceptable utility of the model, with little
understanding of the impact of such choices on meaningful
privacy. Moreover, in scenarios where iterative learning pro-
cedures are used, relaxed deﬁnitions of diﬀerential privacy are
often used which appear to reduce the needed privacy budget
but present poorly understood trade-oﬀs between privacy and
utility. In this paper, we quantify the impact of these choices
on privacy in experiments with logistic regression and neural
network models. Our main ﬁnding is that there is no way
to obtain privacy for free—relaxed deﬁnitions of diﬀerential
privacy that reduce the amount of noise needed to improve
utility also increase the measured privacy leakage. Current
mechanisms for diﬀerentially private machine learning rarely
oﬀer acceptable utility-privacy trade-oﬀs for complex learn-
ing tasks: settings that provide limited accuracy loss provide
little eﬀective privacy, and settings that provide strong privacy
result in useless models.
1 Introduction
Diﬀerential privacy has become a de facto privacy standard,
and nearly all works on privacy-preserving machine learning
use some form of diﬀerential privacy. These works include
designs for diﬀerentially private versions of prominent ma-
chine learning algorithms including empirical risk minimiza-
tion [11, 12] and deep neural networks [1, 60].
While many methods for achieving diﬀerential privacy
have been proposed, it is not well understood how to use
these methods in practice. In particular, there is little concrete
guidance on how to choose an appropriate privacy budget
, and limited understanding of how variants of the diﬀeren-
tial privacy deﬁnition impact privacy in practice. As a result,
privacy-preserving machine learning implementations tend to
choose arbitrary values for  as needed to achieve acceptable
model utility. For instance, the implementation of Shokri and
Shmatikov [60] requires  proportional to the size of the tar-
get deep learning model, which could be in the order of few
millions. Setting  to such arbitrarily large values severely
undermines privacy, although there is no consensus on a hard
threshold value for  above which formal guarantees diﬀeren-
tial privacy provides become meaningless in practice.
One proposed way to improve utility for a given privacy
budget is to relax the deﬁnition of diﬀerential privacy. Several
relaxed deﬁnitions of diﬀerential privacy have been proposed
that are shown to provide better utility even for small  val-
ues [9, 18, 49]. How much privacy leakage these relaxations
allow in adversarial scenarios, however, is not well under-
stood. We shed light on this question by evaluating the relaxed
diﬀerential privacy notions for diﬀerent choices of  values
and empirically measuring privacy leakage, including how
many individual training records are exposed by membership
inference attacks on diﬀerent models.
Contributions. Our main contribution is the evaluation of
diﬀerential privacy mechanisms for machine learning to un-
derstand the impact of diﬀerent choices of  and diﬀerent
relaxations of diﬀerential privacy on both utility and privacy.
We focus our evaluation on gradient perturbation mechanisms,
which are applicable to a wide class of machine learning al-
gorithms including empirical risk minimization (ERM) algo-
rithms such as logistic regression and deep learning (Section
2.2). Our experiments cover three popular diﬀerential privacy
relaxations: diﬀerential privacy with advanced composition,
zero-concentrated diﬀerential privacy [9], and Rényi diﬀeren-
tial privacy [49] (described in Section 2.1). These variations
allow for tighter analysis of cumulative privacy loss, thereby
reducing the noise that must be added in the training process.
We evaluate the concrete privacy loss of these variations using
USENIX Association
28th USENIX Security Symposium    1895
membership inference attacks [61, 74] and attribute inference
attacks [74] (Section 3). While the model utility increases
with the privacy budget, increasing the privacy budget also
increases the success rate of inference attacks. Hence, we
aim to ﬁnd the range of values of  which achieves a balance
between utility and privacy, and also to evaluate the concrete
privacy leakage in terms of the number of individual members
of the training data at risk of exposure. We study both logistic
regression and neural network models, on two multi-class
classiﬁcation data sets. Our key ﬁndings (Section 4) quan-
tify the practical risks of using diﬀerent diﬀerential privacy
notions across a range of privacy budgets.
Related work. Orthogonal to our work, Ding et al. [13] and
Hay et al. [26] evaluate the existing diﬀerential privacy imple-
mentations for the correctness of implementation. Whereas,
we assume correct implementations and aim to evaluate the
impact of the privacy budget and choice of diﬀerential privacy
variant. While Carlini et al. [10] also explore the eﬀectiveness
of diﬀerential privacy against attacks, they do not explicitly
answer what values of  should be used nor do they evaluate
the privacy leakage of the relaxed deﬁnitions. Li et al. [42]
raise concerns about relaxing the diﬀerential privacy notion
in order to achieve better overall utility, but do not evaluate
the leakage. We perform a thorough evaluation of the diﬀeren-
tial privacy variations and quantify their leakage for diﬀerent
privacy budgets. The work of Rahman et al. [58] is most
closely related to our work. It evaluates diﬀerential privacy
implementations against membership inference attacks, but
does not evaluate the privacy leakage of relaxed variants of
diﬀerential privacy. Ours is the ﬁrst work to experimentally
measure the excess privacy leakage due to the relaxed notions
of diﬀerential privacy.
2 Diﬀerential Privacy for Machine Learning
Next, we review the deﬁnition of diﬀerential privacy and its re-
laxed variants. Section 2.2 surveys mechanisms for achieving
diﬀerentially private machine learning. Section 2.3 summa-
rizes applications of diﬀerential privacy to machine learning
and surveys implementations’ choices about privacy budgets.
2.1 Background on Diﬀerential Privacy
Diﬀerential privacy is a probabilistic privacy mechanism
that provides an information-theoretic security guarantee.
Dwork [16] gives the following deﬁnition:
Deﬁnition 2.1 ((, δ)-Diﬀerential Privacy). Given two neigh-
boring data sets D and D(cid:48) diﬀering by one record, a mecha-
nism M preserves (, δ)-diﬀerential privacy if
Pr[M(D) ∈ S ] ≤ Pr[M(D(cid:48)) ∈ S ]× e + δ
where  is the privacy budget and δ is the failure probability.
When δ = 0 we achieve a strictly stronger notion of -
diﬀerential privacy.
The quantity
Pr[M(D) ∈ S ]
Pr[M(D(cid:48)) ∈ S ]
ln
is called the privacy loss.
One way to achieve -DP and (, δ)-DP is to add noise sam-
pled from Laplace and Gaussian distributions respectively,
where the noise is proportional to the sensitivity of the mech-
anism M:
Deﬁnition 2.2 (Sensitivity). For two neighboring data sets
D and D(cid:48) diﬀering by one record, the sensitivity of M is the
maximum change in the output of M over all possible inputs:
∆M =
max
D,D(cid:48),(cid:107)D−D(cid:48)(cid:107)1=1
(cid:107)M(D)−M(D(cid:48))(cid:107)
where (cid:107)·(cid:107) is a norm of the vector. Throughout this paper we
assume (cid:96)2-sensitivity which considers the upper bound on the
(cid:96)2-norm of M(D)−M(D(cid:48)).
Composition. Diﬀerential privacy satisﬁes a simple compo-
sition property: when two mechanisms with privacy budgets
1 and 2 are performed on the same data, together they con-
sume a privacy budget of 1 + 2. Thus, composing multiple
diﬀerentially private mechanisms leads to a linear increase
in the privacy budget (or corresponding increases in noise to
maintain a ﬁxed  total privacy budget).
Relaxed Deﬁnitions. Dwork [17] showed that this linear com-
position bound on  can be reduced at the cost of slightly
increasing the failure probability δ. In essence, this relaxation
considers the linear composition of expected privacy loss of
mechanisms which can be converted to a cumulative privacy
budget  with high probability bound. Dwork deﬁnes this as
the advanced composition theorem, and proves that it applies
to any diﬀerentially private mechanism.
Three commonly-used subsequent relaxed versions of dif-
ferential privacy are Concentrated Diﬀerential Privacy [18],
Zero Concentrated Diﬀerential Privacy [9], and Rényi Diﬀer-
ential Privacy [49]. All of these achieve tighter analysis of
cumulative privacy loss by taking advantage of the fact that
the privacy loss random variable is strictly centered around
an expected privacy loss. The cumulative privacy budget ob-
tained from these analyses bounds the worst case privacy loss
of the composition of mechanisms with all but δ failure prob-
ability. This reduces the noise required and hence improves
utility over multiple compositions. However, it is important
to consider the actual impact these relaxations have on the
privacy leakage, which is a main focus of this paper.
Dwork et al. [18] note that the privacy loss of a diﬀeren-
tially private mechanism follows a sub-Gaussian distribution.
In other words, the privacy loss is strictly distributed around
the expected privacy loss and the spread is controlled by the
variance of the sub-Gaussian distribution. Multiple composi-
tions of diﬀerentially private mechanisms thus result in the
1896    28th USENIX Security Symposium
USENIX Association
Advanced Comp.
Concentrated (CDP)
Zero-Concentrated (zCDP)
Rényi (RDP)
Expected Loss
Variance of Loss
Convert from -DP
Convert to DP
Composition of k
-DP Mechanisms
(e − 1)
2
-
-
(cid:112)
(
+ k(e − 1), δ)-DP
2k log(1/δ)
µ = (e−1)
2
τ2 = 2
(cid:112)
( (e−1)
(cid:112)
(
+ k(e −1)/2, δ)-DP
2
2log(1/δ), δ)-DP†
2k log(1/δ)
, )-CDP
(µ + τ
ζ + ρ = 2
2
2ρ = 2
( 2
2 )-zCDP
ρlog(1/δ), δ)-DP
(cid:112)
(ζ + ρ + 2
(cid:112)
2k log(1/δ)
(
+ k2/2, δ)-DP
22
2
(α, )-RDP
(cid:112)
( + log(1/δ)
α−1 , δ)-DP
2k log(1/δ), δ)-DP‡
(4
Table 1: Comparison of Diﬀerent Variations of Diﬀerential Privacy
Advanced composition is an implicit property of DP and hence there is no conversion to and from DP.
†. Derived indirectly via zCDP. ‡. When log(1/δ) ≥ 2k.
aggregation of corresponding mean and variance values of the
individual sub-Gaussian distributions. This can be converted
to a cumulative privacy budget similar to the advanced com-
position theorem, which in turn reduces the noise that must
be added to the individual mechanisms. The authors call this
concentrated diﬀerential privacy [18]:
Deﬁnition 2.3 (Concentrated Diﬀerential Privacy (CDP)). A
randomized algorithm M is (µ, τ)-concentrated diﬀerentially
private if, for all pairs of adjacent data sets D and D(cid:48),
DsubG(M(D) || M(D(cid:48))) ≤ (µ, τ)
where the sub-Gaussian divergence, DsubG, is deﬁned such
that the expected privacy loss is bounded by µ and after
subtracting µ, the resulting centered sub-Gaussian distribu-
tion has standard deviation τ. Any -DP algorithm satisﬁes
( · (e − 1)/2, )-CDP, however the converse is not true.
A variation on CDP, zero-concentrated diﬀerential privacy
(zCDP) [9] uses Rényi divergence as a diﬀerent method to
show that the privacy loss random variable follows a sub-
Gaussian distribution.
Deﬁnition 2.4 (Zero-Concentrated Diﬀerential Privacy
(zCDP)). A randomized mechanism M is (ξ, ρ)-zero-
concentrated diﬀerentially private if, for all neighbouring data
sets D and D(cid:48) and all α ∈ (1,∞),
Dα(M(D) || M(D(cid:48))) ≤ ξ + ρα
where Dα(M(D) || M(D(cid:48))) is the α-Rényi divergence be-
tween the distribution of M(D) and the distribution of M(D(cid:48)).
Dα also gives the α-th moment of the privacy loss random
variable. For example, D1 gives the ﬁrst order moment which
is the mean or the expected privacy loss, and D2 gives the
second order moment or the variance of privacy loss. There is
a direct relation between DP and zCDP. If M satisﬁes -DP,
(cid:112)
2 2)-zCDP. Furthermore, if M provides
then it also satisﬁes ( 1
ρlog(1/δ), δ)-DP for any δ > 0.
ρ-zCDP, it is (ρ + 2
The Rényi divergence allows zCDP to be mapped back
to DP, which is not the case for CDP. However, Bun and
Steinke [9] give a relationship between CDP and zCDP, which
allows an indirect mapping from CDP to DP (Table 1).
The use of Rényi divergence as a metric to bound the pri-
vacy loss leads to the formulation of a more generic notion of
Rényi diﬀerential privacy that is applicable to any individual
moment of privacy loss random variable:
Deﬁnition 2.5 (Rényi Diﬀerential Privacy (RDP) [49]). A
randomized mechanism M is said to have -Rényi diﬀerential
privacy of order α (which can be abbreviated as (α, )-RDP),
if for any adjacent data sets D, D(cid:48) it holds that
Dα(M(D) || M(D(cid:48))) ≤ .
The main diﬀerence is that CDP and zCDP linearly bound
all positive moments of privacy loss, whereas RDP bounds
one moment at a time, which allows for a more accurate
numerical analysis of privacy loss [49]. If M is an (α, )-RDP
mechanism, it also satisﬁes ( + log1/δ
α−1 , δ)-DP for any 0 < δ < 1.
Table 1 compares the relaxed variations of diﬀerential pri-
vacy. For all the variations, the privacy budget grows sub-
linearly with the number of compositions k.
Moments Accountant. Motivated by relaxations of diﬀeren-
tial privacy, Abadi et al. [1] propose the moments accountant
(MA) mechanism for bounding the cumulative privacy loss
of diﬀerentially private algorithms. The moments accountant
keeps track of a bound on the moments of the privacy loss
random variable during composition. Though the authors do
not formalize this as a relaxed deﬁnition, their deﬁnition of
the moments bound is analogous to the Rényi divergence [49].
Thus, the moments accountant can be considered as an instan-
tiation of Rényi diﬀerential privacy. The moments accountant
is widely used for diﬀerentially private deep learning due
to its practical implementation in the TensorFlow Privacy
library [2] (see Section 2.3 and Table 4).
2.2 Diﬀerential Privacy Methods for ML
This section summarizes methods for modifying machine
learning algorithms to satisfy diﬀerential privacy. First, we
USENIX Association
28th USENIX Security Symposium    1897
Data: Training data set (X,y)
Result: Model parameters θ
θ ← Init(0)
(cid:80)n
#1. Add noise here: objective perturbation
J(θ) = 1
n
for epoch in epochs do
#2. Add noise here: gradient perturbation
θ = θ− η(∇J(θ)+β)
i=1 (cid:96)(θ, Xi,yi) + λR(θ)+β
end
#3. Add noise here: output perturbation
return θ+β
Algorithm 1: Privacy noise mechanisms.
review convex optimization problems, such as empirical risk
minimization (ERM) algorithms, and show several methods
for achieving diﬀerential privacy during the learning process.
Next, we discuss methods that can be applied to non-convex
optimization problems, including deep learning.
ERM. Given a training data set (X,y), where X is a feature
matrix and y is the vector of class labels, an ERM algorithm
aims to reduce the convex objective function of the form,
n(cid:88)
i=1
J(θ) =
1
n
(cid:96)(θ, Xi,yi) + λR(θ),
where (cid:96)(·) is a convex loss function (such as mean square error
(MSE) or cross-entropy loss) that measures the training loss
for a given θ, and R(·) is a regularization function. Commonly
used regularization functions include (cid:96)1 penalty, which makes
the vector θ sparse, and (cid:96)2 penalty, which shrinks the values
of θ vector.
The goal of the algorithm is to ﬁnd the optimal θ∗ that min-
imizes the objective function: θ∗ = argminθ J(θ). While many
ﬁrst order [14, 37, 57, 76] and second order [40, 43] meth-
ods exist to solve this minimization problem, the most basic
procedure is gradient descent where we iteratively calculate
the gradient of J(θ) with respect to θ and update θ with the
gradient information. This process is repeated until J(θ) ≈ 0
or some other termination condition is met.
There are three obvious candidates for where to add privacy-
preserving noise during this training process, demarcated in
Algorithm 1. First, we could add noise to the objective func-
tion J(θ), which gives us the objective perturbation mech-
anism (#1 in Algorithm 1). Second, we could add noise to
the gradients at each iteration, which gives us the gradient
perturbation mechanism (#2). Finally, we can add noise to θ∗
obtained after the training, which gives us the output perturba-
tion mechanism (#3). While there are other methods of achiev-
ing diﬀerential privacy such as input perturbation [15], sample-
aggregate framework [51], exponential mechanism [48] and
teacher ensemble framework [52]. We focus our experimental
analysis on gradient perturbation since it is applicable to all
machine learning algorithms in general and is widely used for
deep learning with diﬀerential privacy.
The amount of noise that must be added depends on the
sensitivity of the machine learning algorithm. For instance,
consider logistic regression with (cid:96)2 regularization penalty.
The objective function is of the form:
log(1 + e−X(cid:62)
n(cid:88)
i θyi) +
J(θ) =
(cid:107) θ (cid:107)2
2
λ
2
1
n
i=1
Assume that the training features are bounded, (cid:107)Xi(cid:107)2 ≤ 1 and
yi ∈ {−1,1}. Chaudhuri et al. [12] prove that for this setting,
objective perturbation requires sampling noise in the scale
of 2
n , and output perturbation requires sampling noise in the
scale of 2
nλ . The gradient of the objective function is:
n(cid:88)
∇J(θ) =
1
n