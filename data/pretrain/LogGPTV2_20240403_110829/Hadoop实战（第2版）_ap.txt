file1：
2006-6-9 a
2006-6-10 b
2006-6-11 c
2006-6-12 d
2006-6-13 a
2006-6-14 b
2006-6-15 c
2006-6-11 c
file2：
2006-6-9 b
2006-6-10 a
2006-6-11 b
2006-6-12 d
2006-6-13 a
2006-6-14 c
2006-6-15 d
2006-6-11 c
样例输出：
2006-6-10 a
2006-6-10 b
2006-6-11 b
2006-6-11 c
2006-6-12 d
2006-6-13 a
2006-6-14 b
2006-6-14 c
2006-6-15 c
2006-6-15 d
2006-6-9 a
2006-6-9 b
5.2.2 设计思路
数据去重实例的最终目标是让原始数据中出现次数超过一次的数据在输出文件中只出现一次。我们自然而然会想到将同一个数据的所有记录都交给一台Reduce机器，无论这个数据出现多少次，只要在最终结果中输出一次就可以了。具体就是Reduce的输入应该以数据作为key，而对value-list则没有要求。当Reduce接收到一个＜key, value-list＞时就直接将key复制到输出的key中，并将value设置成空值。在MapReduce流程中，Map的输出＜key, value＞经过shuffle过程聚集成＜key, value-list＞后会被交给Reduce。所以从设计好的Reduce输入可以反推出Map输出的key应为数据，而value为任意值。继续反推，Map输出的key为数据。而在这个实例中每个数据代表输入文件中的一行内容，所以Map阶段要完成的任务就是在采用Hadoop默认的作业输入方式之后，将value设置成key，并直接输出（输出中的value任意）。Map中的结果经过shuffle过程之后被交给Reduce。在Reduce阶段不管每个key有多少个value，都直接将输入的key复制为输出的key，并输出就可以了（输出中的value被设置成空）。
因为此程序简单且执行步骤与单词计数实例完全相同，所以不再赘述，下面只给出程序。
5.2.3 程序代码
程序代码如下：
package cn.edu.ruc.cloudcomputing.book.chapter05；
import java.io.IOException；
import org.apache.hadoop.conf.Configuration；
import org.apache.hadoop.fs.Path；
import org.apache.hadoop.io.IntWritable；
import org.apache.hadoop.io.Text；
import org.apache.hadoop.mapreduce.Job；
import org.apache.hadoop.mapreduce.Mapper；
import org.apache.hadoop.mapreduce.Reducer；
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat；
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat；
import org.apache.hadoop.util.GenericOptionsParser；
public class Dedup{
//map将输入中的value复制到输出数据的key上，并直接输出
public static class Map extends Mapper＜Object, Text, Text, Text＞{
private static Text line=new Text（）；
public void map（Object key, Text value, Context context）throws IOException，
InterruptedException{
line=value；
context.write（line, new Text（""））；
}
}
//reduce将输入中的key复制到输出数据的key上，并直接输出
public static class Reduce extends Reducer＜Text, Text, Text, Text＞{
public void reduce（Text key, Iterable＜Text＞values, Context context）throws
IOException, InterruptedException{
context.write（key, new Text（""））；
}
}
public static void main（String[]args）throws Exception{
Configuration conf=new Configuration（）；
String[]otherArgs=new GenericOptionsParser（conf, args）.getRemainingArgs（）；
if（otherArgs.length！=2）{
System.err.println（"Usage：wordcount＜in＞＜out＞"）；
System.exit（2）；
}
Job job=new Job（conf，"Data Deduplication"）；
job.setJarByClass（Dedup.class）；
job.setMapperClass（Map.class）；
job.setCombinerClass（Reduce.class）；
job.setReducerClass（Reduce.class）；
job.setOutputKeyClass（Text.class）；
job.setOutputValueClass（Text.class）；
FileInputFormat.addInputPath（job, new Path（otherArgs[0]））；
FileOutputFormat.setOutputPath（job, new Path（otherArgs[1]））；
System.exit（job.waitForCompletion（true）?0：1）；
}
}
5.3 排序
数据排序是许多实际任务在执行时要完成的第一项工作，比如学生成绩评比、数据建立索引等。这个实例和数据去重类似，都是先对原始数据进行初步处理，为进一步的数据操作打好基础。下面进入这个实例。
 5.3.1 实例描述
对输入文件中的数据进行排序。输入文件中的每行内容均为一个数字，即一个数据。要求在输出中每行有两个间隔的数字，其中，第二个数字代表原始数据，第一个数字代表这个原始数据在原始数据集中的位次。
样例输入：
file1：
2
32
654
32
15
756
65223
file2：
5956
22
650 92
file3：
26
54
6
样例输出：
1 2
2 6
3 15
4 22
5 26
6 32
7 32
8 54
9 92
10 650
11 654
12 756
13 5956
14 65223
5.3.2 设计思路
这个实例仅仅要求对输入数据进行排序，熟悉MapReduce过程的读者很快会想到在MapReduce过程中就有排序。是否可以利用这个默认的排序、而不需要自己再实现具体的排序呢？答案是肯定的。但是在使用之前首先要了解MapReduce过程中的默认排序规则。它是按照key值进行排序，如果key为封装int的IntWritable类型，那么MapReduce按照数字大小对key排序；如果key为封装String的Text类型，那么MapReduce按照字典顺序对字符串排序。需要注意的是，Reduce自动排序的数据仅仅是发送到自己所在节点的数据，使用默认的排序并不能保证全局的顺序，因为在排序前还有一个partition的过程，默认无法保证分割后各个Reduce上的数据整体是有序的。所有要想使用默认的排序过程，还必须定义自己的Partition类，保证执行Partition过程之后所有Reduce上的数据在整体上是有序的，然后再对局部Reduce上的数据进行默认排序，这样才能保证所有数据有序。了解了这个细节，我们就知道，首先应该使用封装int的IntWritable型数据结构，也就是将读入的数据在Map中转化成IntWritable型，然后作为key值输出（value任意）；其次需要重写partition类，保证整体有序，具体做法是用输入数据的最大值除以系统partition数量的商作为分割数据的边界增量，也就是说分割数据的边界为此商的1倍、2倍至numPartitions-1倍，这样就能保证执行partition后的数据是整体有序的；然后Reduce获得＜key, value-list＞之后，根据value-list中元素的个数将输入的key作为value的输出次数，输出的key是一个全局变量，用于统计当前key的位次。需要注意的是，这个程序中没有配置Combiner，也就是说在MapReduce过程中不使用Combiner。这主要是因为使用Map和Reduce就已经能够完成任务了。
由于此程序简单且执行步骤与单词计数实例完全相同，所以不再赘述，下面只给出程序。
5.3.3 程序代码
程序代码如下：
package cn.edu.ruc.cloudcomputing.book.chapter05；
import java.io.IOException；
import org.apache.hadoop.conf.Configuration；
import org.apache.hadoop.fs.Path；
import org.apache.hadoop.io.IntWritable；
import org.apache.hadoop.io.Text；
import org.apache.hadoop.mapreduce.Job；
import org.apache.hadoop.mapreduce.Mapper；
import org.apache.hadoop.mapreduce.Reducer；
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat；
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat；
import org.apache.hadoop.util.GenericOptionsParser；
import org.apache.hadoop.mapreduce.Partitioner；
public class Sort{
//map将输入中的value转化成IntWritable类型，作为输出的key
public static class Map extends Mapper＜Object, Text, IntWritable, IntWritable＞{
private static IntWritable data=new IntWritable（）；
public void map（Object key, Text value, Context context）throws IOException，
InterruptedException{