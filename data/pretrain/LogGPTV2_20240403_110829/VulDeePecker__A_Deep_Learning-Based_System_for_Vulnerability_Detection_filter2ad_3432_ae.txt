identical to the vulnerable functions in the training programs
(i.e., vulnerabilities caused by Types I and II code clones [42]);
VulPecker can only detect vulnerabilities caused by Type I,
Type II, and some Type III code clones [42] (e.g., deletion,
insertion, and rearrangement of statements), which explains
why VulPecker incurs a lower FNR than VUDDY. However,
these systems cannot detect vulnerabilities that are not caused
by code clones, which explains why they incur high FNRs.
In contrast, VulDeePecker has a much higher F1-measure
(i.e., 80.8% vs. 9.3% for VUDDY and 18.2% for VulPecker)
because it has a much higher TPR (i.e., much lower FNR),
while noting that its FPR is 22.9% (vs. 0% for VUDDY and
1.9% for VulPecker). We suspect that this high FPR of 22.9%
corresponding to the BE-SEL-NVD dataset is caused by a
small number of training code gadgets from NVD. This can
be justiﬁed by the small FPR of 3.4% corresponding to a large
number of training code gadgets from SARD, which is about
18 times larger than the number of training code gadgets from
NVD. Moreover, the FPR of 5.7% corresponding to the entire
BE-SEL dataset resides some where in between them. The
high FNR of 16.9% can be explained similarly.
The high FPR and FNR of VulDeePecker with respect to
the BE-SEL-NVD sub-dataset should not be used as evidence
against VulDeePecker, simply because for the BE-SEL-SARD
sub-dataset, VulDeePecker incurs an even smaller FPR of 3.4%
and a FNR of 5.1%, while noting that VUDDY and VulPecker
are not applicable (i.e., not capable of detecting vulnerabilities
in this sub-dataset). Moreover, VulDeePecker incurs a FPR of
5.7% and a FNR of 7.0% over the entire BE-SEL dataset,
11
Table VI.
VULDEEPECKER DETECTED 4 VULNERABILITIES IN 3 PRODUCTS, WHICH ARE NOT PUBLISHED IN THE NVD BUT HAVE BEEN “SILENTLY”
PATCHED BY THE VENDORS IN THE LATER RELEASES OF THESE PRODUCTS.THESE VULNERABILITIES ARE ENTIRELY MISSED BY THE OTHER
VULNERABILITY DETECTION SYSTEMS, EXCEPT THAT FLAWFINDER DETECTED ONLY ONE VULNERABILITY WHILE MISSING THE OTHER THREE.
Target product
CVE ID
Xen 4.6.0
Seamonkey
2.31
Libav 10.2
CVE-2016-9104
CVE-2015-4517
CVE-2015-4513
CVE-2014-2263
Vulnerable product
published in the NVD
Qemu
Firefox
Firefox
FFmpeg
Vulnerability
publish time
12/09/2016
09/24/2015
11/05/2015
02/28/2014
Vulnerable ﬁle in target product
Library/API
function call
1st patched version
of target product
.../qemu-xen/hw/9pfs/virtio-9p.c
.../system/gonk/NetworkUtils.cpp
.../protocol/http/Http2Stream.cpp
memcpy
snprintf
memset
Xen 4.9.0
Seamonkey 2.38
Seamonkey 2.39
libavformat/mpegtsenc.c
strchr, strlen
Libav 10.4
which is the more practical case because one would use all
data available in practice.
Therefore, it is fair to say that VulDeePecker substantially
outperforms two state-of-the-art code similarity-based vulnera-
bility detection systems, simply because VulDeePecker incurs
a FPR of 5.7% and a FNR of 7.0% over the entire dataset.
Nevertheless, it is important to note that deep learning-based
vulnerability detection largely rely on the amount of data. This
leads to:
Insight 4: VulDeePecker
is more effective than code
similarity-based vulnerability detection systems, which cannot
detect vulnerabilities that are not caused by code clones and
thus often incur high false negative rate. Nevertheless, the
effectiveness of VulDeePecker is sensitive to the amount of
data, which appears to be inherent
to the nature of deep
learning.
Using VulDeePecker in practice. In order to further show
the usefulness of VulDeePecker, we collected 20 versions
of 3 software products: Xen, Seamonkey, and Libav. These
products are different from the target programs mentioned
above. We use VulDeePecker and the other vulnerability
detection systems to detect the vulnerabilities in those software
products. As highlighted in Table VI, VulDeePecker detected
4 vulnerabilities that have not been published in the NVD.
We manually checked and conﬁrmed these vulnerabilities,
and found that they have been published for other products
and have been “silently” patched by the product vendors
in the subsequent versions. In contrast, these vulnerabilities
are missed by almost all of the other vulnerability detection
systems mentioned above, except that Flawﬁnder detects the
vulnerability corresponding to CVE-2015-4517 while missing
the other three.
V. LIMITATIONS
The present design,
implementation, and evaluation of
VulDeePecker have several limitations, which suggest inter-
esting open problems for future research. First, the present
design of VulDeePecker is limited to dealing with vulnerability
detection by assuming source code of programs is available.
The detection of vulnerabilities in executables is a different
and more challenging problem.
Second, the present design of VulDeePecker only deals
with C/C++ programs. Future research needs to be conducted
to adapt it to deal with other kinds of programming languages.
Third, the present design of VulDeePecker only deals with
vulnerabilities related to library/API function calls. We will
investigate how to detect the other kinds of vulnerabilities by
leveraging the other kinds of key points mentioned above.
Fourth, the present design of VulDeePecker only accommo-
dates data ﬂow analysis (i.e., data dependency), but not control
ﬂow analysis (i.e., control dependency), despite the fact that the
notion of code gadgets can accommodate data dependency and
control dependency. It is an important future work to improve
the leverage of data ﬂow analysis and accommodate control
ﬂow analysis to enhance vulnerability detection capabilities.
the present design of VulDeePecker uses some
heuristics in labeling the ground truth of code gadgets, trans-
forming code gadgets into their symbolic representations,
transforming variable-length vector representations of code
gadgets into ﬁxed-length vectors. While intuitive, further re-
search needs to be conducted to characterize the impact of
these heuristics on the effectiveness of VulDeePecker.
Fifth,
Sixth,
the present
implementation of VulDeePecker is
limited to the BLSTM neural network. We plan to conduct
systematic experiments with other kinds of neural networks
that could be used for vulnerability detection.
Seventh, the present evaluation of VulDeePecker is limited
because the dataset only contains buffer error vulnerabili-
ties and resource management error vulnerabilities. We will
conduct experiments on all available types of vulnerabilities.
Although we further tested VulDeePecker against 3 software
products (i.e., Xen, Seamonkey, and Libav) and found 4 vulner-
abilities that were not reported in the NVD and were “silently”
patched by the vendors when releasing later versions of these
products, these vulnerabilities were known rather than 0-day
ones. Extensive experiments need to be conducted against more
software products to check whether VulDeePecker has the
capability in detecting 0-day vulnerabilities. In principle, this is
possible because VulDeePecker uses pattern-based approach.
VI. RELATED WORK
We classify the related prior work into two categories:
vulnerability detection (in relation to the purpose of the present
paper), and program analysis (in the relation to the means for
vulnerability detection).
A. Prior work in vulnerability detection
Pattern-based approach. This approach can be further di-
vided to three categories. In the ﬁrst category, patterns are
generated manually by human experts (e.g., open source tools
Flawﬁnder [6], RATS [11], and ITS4 [52], commercial tools
Checkmarx [2], Fortify [7], and Coverity [3]). These tools
often have high false positive rate or false negative rate. In
the second category, patterns are generated semi-automatically
from pre-classiﬁed vulnerabilities ( e.g., missing check vulner-
abilities [62], taint-style vulnerabilities [61], and information
leakage vulnerabilities [15]) and a pattern is speciﬁc to a type
12
of vulnerabilities. In the third category, patterns are generated
semi-automatically from type-agnostic vulnerabilities (i.e., no
need to pre-classify them into different types). These methods
use machine learning techniques, which rely on human experts
for deﬁning features to characterize vulnerabilities [19], [37],
[38], [49], [59], [60]. Moreover, these methods cannot pin
down the precise locations of vulnerabilities because programs
are represented in coarse-grained granularity (e.g., program
[19], package [37], component [38], [46], ﬁle [35], [49], and
function [59], [60]).
VulDeePecker falls into the pattern-based approach to vul-
nerability detection. In contrast to the studies reviewed above,
VulDeePecker has two advantages. First, it does not need
human experts to deﬁne features for distinguishing vulnerable
code and non-vulnerable code. Second, it uses a ﬁne-grained
granularity to represent programs, and therefore can pin down
the precise locations of vulnerabilities.
Code similarity-based approach. This approach has three
steps. The ﬁrst step is to divide a program into some code
fragments [25], [28], [31], [41], [44]. The second step is to
represent each code fragment in the abstract fashion, including
tokens [25], [28], [44], trees [26], [41], and graphs [31], [41].
The third step is to compute the similarity between code
fragments via their abstract representations obtained in the
second step.
Compared with any pattern-based approach to vulnerability
detection (including VulDeePacker), the code similarity-based
approach has the advantage that a single instance of vulnerable
code is sufﬁcient for detecting the same vulnerability in target
programs. But it can only detect vulnerabilities in the Type
I and Type II code clones [42] (i.e.,
identical or almost-
identical code clones), and some Type III code clones [42]
(e.g., deletion, insertion, and rearrangement of statements).
In order to achieve a higher effectiveness of vulnerability
detection, human experts need to deﬁne features in order
to automatically select the right code similarity algorithms
for different kinds of vulnerabilities [32]. However, even the
enhanced approach with expert-deﬁned features [32] cannot
detect vulnerabilities that are not caused by code clones. In
contrast, VulDeePecker can detect vulnerabilities that may or
may not caused by code clones, in an automatic fashion (i.e.,
no need of human expert to deﬁne features).
B. Prior work related to using deep learning for program
analysis
To the best of our knowledge, we are the ﬁrst to use deep
learning to detect software vulnerabilities, as inspired by the
success of deep learning in image processing, speech recogni-
tion, and natural language processing [21], [30], [40]. In order
to use deep learning for detecting software vulnerabilities,
programs need to be represented in vectors. There are two
approaches for this purpose. One is to map the tokens extracted
from programs, such as data types, variable names, function
names, and keywords, to vectors [57]; the other is to map
the nodes of abstract syntax trees extracted from programs,
such as function deﬁnitions, function invocations, identiﬁer
declarations, and control ﬂow nodes, to vectors [36], [54].
VulDeePecker maps the tokens extracted from code gadgets
to vectors, while taking it into consideration that the lines of
code in the code gadget is not necessarily consecutive.
Somewhat related work is the use of deep learning for soft-
ware defect prediction [54], [63]. However, software defects
are different from software vulnerabilities (i.e., methods for
detecting defects cannot be used for detecting vulnerabilities
in general) [34], and the ﬁle-level representation of programs
in [54] is too coarse-grained to pin down the locations of
vulnerabilities. Moreover, the defect prediction method pre-
sented in [63] is geared towards code changes rather than target
programs as a whole. Remotely related work is the use of
deep learning for purposes, like software language modeling
[57], code cloning detection [56], API learning [20], binary
function boundary recognition [48], and malicious URLs, ﬁle
paths detection and registry keys detection [45].
VII. CONCLUSION
We have presented VulDeePecker, the ﬁrst deep learning-
based vulnerability detection system, which aims to relieve
human experts from the tedious and subjective work of man-
ually deﬁning features and reduce the false negatives that are
incurred by other vulnerability detection systems. Since deep
learning is invented for applications that are very different from
vulnerability detection, we have presented some preliminary
principles for guiding the practice of applying deep learning
to vulnerability detection. These principles should be further
reﬁned because deep learning has great potential in solving
the problem of vulnerability detection. We have collected, and
made publicly available, a useful dataset for evaluating the
effectiveness of VulDeePecker and other deep learning-based
vulnerability detection systems that will be developed in the
future. Systematic experiments show that VulDeePecker can
achieve much lower false negative rate than other vulnerability
detection systems, while relieving human experts from the
tedious work of manually deﬁning features. For the 3 software
products we experimented with (i.e., Xen, Seamonkey, and
Libav), VulDeePecker detected 4 vulnerabilities, which were
not reported in the NVD and were “silently” patched by the
vendors when they released later versions of these products.
In contrast, the other detection systems missed almost all of
these vulnerabilities, except that one system detected 1 of these
vulnerabilities and missed the other three vulnerabilities.
Open problems for future research are abundant, including
the limitations of the present study discussed in Section
V. In particular, precisely characterizing the capabilities and
limitations of deep learning-based vulnerability detection is an
exciting research problem.
ACKNOWLEDGMENT
We thank the anonymous reviewers for their comments
that helped us improve the paper, and Marcus Pendleton for
proofreading the paper. This paper is supported by the Na-
tional Basic Research Program of China (973 Program) under
grant No.2014CB340600, the National Science Foundation of
China under grant No. 61672249, the Shenzhen Fundamental
Research Program under grant No. JCYJ20170413114215614,
and the Natural Science Foundation of Hebei Province under
grant No. F2015201089. Shouhuai Xu is supported in part
by NSF Grant #1111925 and ARO Grant #W911NF-17-1-
0566. Any opinions, ﬁndings, conclusions or recommendations
expressed in this material are those of the authors and do not
reﬂect the views of the funding agencies.
13
REFERENCES
[1] C/C++ standard library functions, http://en.cppreference.com/w.
[2] Checkmarx, https://www.checkmarx.com/.
[3] Coverity, https://scan.coverity.com/.
[4] CVE, http://cve.mitre.org/.
[5] Cyber Grand Challenge, https://www.cybergrandchallenge.com/.
[6] FlawFinder, http://www.dwheeler.com/ﬂawﬁnder.
[7] HP Fortify, https://www.hpfod.com/.
[8] Keras, https://github.com/fchollet/keras.
[9] Linux
kernel API
functions, https://www.kernel.org/doc/htmldocs/
kernel-api/.
[10] NVD, https://nvd.nist.gov/.
[11] Rough Audit Tool
rough-auditing-tool-for-security/.
for Security, https://code.google.com/archive/p/
[12] Software Assurance Reference Dataset, https://samate.nist.gov/SRD/
index.php.
[13] Windows API
functions,
https://msdn.microsoft.com/en-us/library/
windows/desktop/ff818516(v=vs.85).aspx#user interface.
[14] word2vec, http://radimrehurek.com/gensim/models/word2vec.html.
[15] M. Backes, B. K¨opf, and A. Rybalchenko, “Automatic discovery and
quantiﬁcation of information leaks,” in Proceedings of the 30th IEEE
Symposium on Security and Privacy.
IEEE, 2009, pp. 141–153.
[16] Y. Bengio, “Learning deep architectures for AI,” Foundations and
Trends in Machine Learning, vol. 2, no. 1, pp. 1–127, 2009.
[17] K. Cho, B. Van Merri¨enboer, D. Bahdanau, and Y. Bengio, “On the
properties of neural machine translation: Encoder-decoder approaches,”
arXiv preprint arXiv:1409.1259, 2014.
[18] L. V. Davi, “Code-reuse attacks and defenses,” Ph.D. dissertation,
Darmstadt University of Technology, Germany, 2015.
[19] G. Grieco, G. L. Grinblat, L. Uzal, S. Rawat, J. Feist, and L. Mounier,
“Toward large-scale vulnerability discovery using machine learning,”
in Proceedings of the 6th ACM Conference on Data and Application