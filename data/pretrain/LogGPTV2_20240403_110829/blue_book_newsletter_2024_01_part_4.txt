    ```yaml
    scrape_configs:
      - job_name: docker
        docker_sd_configs:
          - host: unix:///var/run/docker.sock
            refresh_interval: 5s
        relabel_configs:
          - source_labels: ['__meta_docker_container_id']
            target_label: docker_id
          - source_labels: ['__meta_docker_container_name']
            target_label: docker_name
    ```
    The available meta labels are:
    - `__meta_docker_container_id`: the ID of the container
    - `__meta_docker_container_name`: the name of the container
    - `__meta_docker_container_network_mode`: the network mode of the container
    - `__meta_docker_container_label_`: each label of the container
    - `__meta_docker_container_log_stream`: the log stream type stdout or stderr
    - `__meta_docker_network_id`: the ID of the network
    - `__meta_docker_network_name`: the name of the network
    - `__meta_docker_network_ingress`: whether the network is ingress
    - `__meta_docker_network_internal`: whether the network is internal
    - `__meta_docker_network_label_`: each label of the network
    - `__meta_docker_network_scope`: the scope of the network
    - `__meta_docker_network_ip`: the IP of the container in this network
    - `__meta_docker_port_private`: the port on the container
    - `__meta_docker_port_public`: the external port if a port-mapping exists
    - `__meta_docker_port_public_ip`: the public IP if a port-mapping exists
    These labels can be used during relabeling. For instance, the following configuration scrapes the container named `flog` and removes the leading slash (/) from the container name.
    yaml
    ```yaml
    scrape_configs:
      - job_name: flog_scrape
        docker_sd_configs:
          - host: unix:///var/run/docker.sock
            refresh_interval: 5s
            filters:
              - name: name
                values: [flog]
        relabel_configs:
          - source_labels: ['__meta_docker_container_name']
            regex: '/(.*)'
            target_label: 'container'
    ```
## Hardware
### [GPU](gpu.md)
* New: Introduce GPU.
    [GPU](https://en.wikipedia.org/wiki/Graphics_processing_unit) or Graphic Processing Unit is a specialized electronic circuit initially designed to accelerate computer graphics and image processing (either on a video card or embedded on motherboards, mobile phones, personal computers, workstations, and game consoles).
    For years I've wanted to buy a graphic card but I've been stuck in the problem that I don't have a desktop. I have a X280 lenovo laptop used to work and personal use with an integrated card that has let me so far to play old games such as [King Arthur Gold](kag.md) or [Age of Empires II](age_of_empires.md), but has hard times playing "newer" games such as It takes two. Last year I also bought a [NAS](nas.md) with awesome hardware. So it makes no sense to buy a desktop just for playing.
    Now that I host [Jellyfin](jellyfin.md) on the NAS and that machine learning is on the hype with a lot of interesting solutions that can be self-hosted (whisper, chatgpt similar solutions...), it starts to make sense to add a GPU to the server. What made me give the step is that you can also self-host a gaming server to stream to any device! It makes so much sense to have all the big guns inside the NAS and stream the content to the less powerful devices.
    That way if you host services, you make the most use of the hardware.
# Operating Systems
## Linux
### [Tabs vs Buffers](linux_snippets.md)
* New: [Makefile use bash instead of sh.](linux_snippets.md#makefile-use-bash-instead-of-sh)
    The program used as the shell is taken from the variable `SHELL`.  If
    this variable is not set in your makefile, the program `/bin/sh` is
    used as the shell.
    So put `SHELL := /bin/bash` at the top of your makefile, and you should be good to go.
* New: [Recover the message of a commit if the command failed.](linux_snippets.md#recover-the-message-of-a-commit-if-the-command-failed)
    `git commit` can fail for reasons such as `gpg.commitsign = true` && `gpg` fails, or when running a pre-commit. Retrying the command opens a blank editor and the message seems to be lost.
    The message is saved though in `.git/COMMIT_EDITMSG`, so you can:
    ```bash
    git commit -m "$(cat .git/COMMIT_EDITMSG)"
    ```
    Or in general (suitable for an alias for example):
    ```bash
    git commit -m "$(cat "$(git rev-parse --git-dir)/COMMIT_EDITMSG)")"
    ```
* New: [Switch to the previous opened buffer.](vim_tabs.md#switch-to-the-previous-opened-buffer)
    Often the buffer that you want to edit is the buffer that you have just left. Vim provides a couple of convenient commands to switch back to the previous buffer. These are `` (or ``) and `:b#`. All of them are inconvenient so I use the next mapping:
    ```vim
    nnoremap  :b#
    ```
### [beancount](beancount.md)
* New: [Comments.](beancount.md#comments)
    Any text on a line after the character `;` is ignored, text like this:
    ```beancount
    ; I paid and left the taxi, forgot to take change, it was cold.
    2015-01-01 * "Taxi home from concert in Brooklyn"
      Assets:Cash      -20 USD  ; inline comment
      Expenses:Taxi
    ```
### [Dino](dino.md)
* New: Disable automatic OMEMO key acceptance.
    Dino automatically accepts new OMEMO keys from your own other devices and your chat partners by default. This default behaviour leads to the fact that the admin of the XMPP server could inject own public OMEMO keys without user verification, which enables the owner of the associated private OMEMO keys to decrypt your OMEMO secured conversation without being easily noticed.
    To prevent this, two actions are required, the second consists of several steps and must be taken for each new chat partner.
    - First, the automatic acceptance of new keys from your own other devices must be deactivated. Configure this in the account settings of your own accounts.
    - Second, the automatic acceptance of new keys from your chat partners must be deactivated. Configure this in the contact details of every chat partner. Be aware that in the case of group chats, the entire communication can be decrypted unnoticed if even one partner does not actively deactivate automatic acceptance of new OMEMO keys.
    Always confirm new keys from your chat partner before accepting them manually
* New: [Dino does not use encryption by default.](dino.md#dino-does-not-use-encryption-by-default)
    You have to initially enable encryption in the conversation window by clicking the lock-symbol and choose OMEMO. Future messages and file transfers to this contact will be encrypted with OMEMO automatically.
    - Every chat partner has to enable encryption separately.
    - If only one of two chat partner has activated OMEMO, only this part of the communication will be encrypted. The same applies with file transfers.
    - If you get a message "This contact does not support OMEMO" make sure that your chatpartner has accepted the request to add him to your contact list and you accepted vice versa
* New: [Install in Tails.](dino.md#install-in-tails)
    If you have more detailed follow [this article](https://t-hinrichs.net/DinoTails/DinoTails_recent.html) at the same time as you read this one. That one is more outdated but more detailed.
    - Boot a clean Tails
    - Create and configure the Persistent Storage
    - Restart Tails and open the Persistent Storage
    - Configure the persistence of the directory:
        ```bash
        echo -e '/home/amnesia/.local/share/dino source=dino' | sudo tee -a /live/persistence/TailsData_unlocked/persistence.conf > /dev/null
        ```
    - Restart Tails
    - Install the application:
        ```bash
        sudo apt-get update
        sudo apt-get install dino-im
        ```
    - Configure the `dino-im` alias to use `torsocks`
        ```bash
        sudo echo 'alias dino="torsocks dino-im &> /dev/null &"' >> /live/persistence/TailsData_unlocked/dotfiles/.bashrc
        echo 'alias dino="torsocks dino-im &> /dev/null &"' >> ~/.bashrc
        ```
### [Jellyfin](jellyfin.md)
* New: [Python library.](jellyfin.md#python-library)
    [This is the API client](https://github.com/jellyfin/jellyfin-apiclient-python/tree/master) from Jellyfin Kodi extracted as a python package so that other users may use the API without maintaining a fork of the API client. Please note that this API client is not complete. You may have to add API calls to perform certain tasks.
    It doesn't (yet) support async
### [journald](journald.md)
* New: Introduce journald.
    [journald](https://www.freedesktop.org/software/systemd/man/latest/systemd-journald.service.html) is a system service that collects and stores logging data. It creates and maintains structured, indexed journals based on logging information that is received from a variety of sources:
    - Kernel log messages, via kmsg
    - Simple system log messages, via the `libc syslog` call
    - Structured system log messages via the native Journal API.
    - Standard output and standard error of service units.
    - Audit records, originating from the kernel audit subsystem.
    The daemon will implicitly collect numerous metadata fields for each log messages in a secure and unfakeable way.
    Journald provides a good out-of-the-box logging experience for systemd. The trade-off is, journald is a bit of a monolith, having everything from log storage and rotation, to log transport and search. Some would argue that syslog is more UNIX-y: more lenient, easier to integrate with other tools. Which was its main criticism to begin with. When the change was made [not everyone agreed with the migration from syslog](https://rainer.gerhards.net/2013/05/rsyslog-vs-systemd-journal.html) or the general approach systemd took with journald. But by now, systemd is adopted by most Linux distributions, and it includes journald as well. journald happily coexists with syslog daemons, as:
    - Some syslog daemons can both read from and write to the journal
    - journald exposes the syslog API
    It provides lots of features, most importantly:
    - Indexing. journald uses a binary storage for logs, where data is indexed. Lookups are much faster than with plain text files.
    - Structured logging. Though it’s possible with syslog, too, it’s enforced here. Combined with indexing, it means you can easily filter specific logs (e.g. with a set priority, in a set timeframe).
    - Access control. By default, storage files are split by user, with different permissions to each. As a regular user, you won’t see everything root sees, but you’ll see your own logs.
    - Automatic log rotation. You can configure journald to keep logs only up to a space limit, or based on free space.
### [Kodi](kodi.md)
* New: Start working on a migration script to mediatracker.
* New: [Extract kodi data from the database.](kodi.md#from-the-database)
    At `~/.kodi/userdata/Database/MyVideos116.db` you can extract the data from the next tables:
    - In the `movie_view` table there is:
      - `idMovie`: kodi id for the movie
      - `c00`: Movie title
      - `userrating`
      - `uniqueid_value`: The id of the external web service
      - `uniqueid_type`: The web it extracts the id from
      - `lastPlayed`: The reproduction date
    - In the `tvshow_view` table there is:
      - `idShow`: kodi id of a show
      - `c00`: title
      - `userrating`
      - `lastPlayed`: The reproduction date
      - `uniqueid_value`: The id of the external web service
      - `uniqueid_type`: The web it extracts the id from
    - In the `season_view` there is no interesting data as the userrating is null on all rows.
    - In the `episode_view` table there is:
      - `idEpisodie`: kodi id for the episode
      - `idShow`: kodi id of a show
      - `idSeason: kodi id of a season
      - `c00`: title
      - `userrating`
      - `lastPlayed`: The reproduction date
      - `uniqueid_value`: The id of the external web service
      - `uniqueid_type`: The web it extracts the id from. I've seen mainly tvdb and sonarr
    - Don't use the `rating` table as it only stores the ratings from external webs such as themoviedb:
### [Matrix Highlight](matrix_highlight.md)
* New: Introduce matrix_highlight.
    [Matrix Highlight](https://github.com/DanilaFe/matrix-highlight) is a decentralized and federated way of annotating the web based on Matrix.
    Think of it as an open source alternative to [hypothesis](hypothesis.md).
    It's similar to [Populus](https://github.com/opentower/populus-viewer) but for the web.
    I want to try it and investigate further specially if you can:
    - Easily extract the annotations
    - Activate it by default everywhere
### [Mediatracker](mediatracker.md)
* New: [How to use the mediatracker API.](mediatracker.md#api)
    I haven't found a way to see the api docs from my own instance. Luckily you can browse it [at the official instance](https://bonukai.github.io/MediaTracker/).
    You can create an application token on your user configuration. Then you can use it with something similar to:
    ```bash
    curl -H 'Content-Type: application/json' https://mediatracker.your-domain.org/api/logs\?token\=your-token | jq
    ```
* New: Introduce python library.
    There is a [python library](https://github.com/jonkristian/pymediatracker) although it's doesn't (yet) have any documentation and the functionality so far is only to get information, not to push changes.
* New: [Get list of tv shows.](mediatracker.md#get-list-of-tv-shows)
    With `/api/items?mediaType=tv` you can get a list of all tv shows with the next interesting fields: