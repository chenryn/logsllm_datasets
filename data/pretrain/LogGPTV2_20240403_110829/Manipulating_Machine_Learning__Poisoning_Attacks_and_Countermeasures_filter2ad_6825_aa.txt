title:Manipulating Machine Learning: Poisoning Attacks and Countermeasures
for Regression Learning
author:Matthew Jagielski and
Alina Oprea and
Battista Biggio and
Chang Liu and
Cristina Nita-Rotaru and
Bo Li
2018 IEEE Symposium on Security and Privacy
Manipulating Machine Learning: Poisoning Attacks
and Countermeasures for Regression Learning
Matthew Jagielski∗, Alina Oprea∗, Battista Biggio †, Chang Liu‡, Cristina Nita-Rotaru∗, and Bo Li‡
‡UC Berkeley, Berkeley, CA
†Univ. Cagliary, Cagliary, Italy
∗Northeastern University, Boston, MA
Abstract—As machine learning becomes widely used for auto-
mated decisions, attackers have strong incentives to manipulate
the results and models generated by machine learning algorithms.
In this paper, we perform the ﬁrst systematic study of poisoning
attacks and their countermeasures for linear regression mod-
els. In poisoning attacks, attackers deliberately inﬂuence the
training data to manipulate the results of a predictive model.
We propose a theoretically-grounded optimization framework
speciﬁcally designed for linear regression and demonstrate its
effectiveness on a range of datasets and models. We also introduce
a fast statistical attack that requires limited knowledge of the
training process. Finally, we design a new principled defense
method that is highly resilient against all poisoning attacks. We
provide formal guarantees about its convergence and an upper
bound on the effect of poisoning attacks when the defense is
deployed. We evaluate extensively our attacks and defenses on
three realistic datasets from health care, loan assessment, and
real estate domains.
I. INTRODUCTION
As more applications with large societal impact rely on
machine learning for automated decisions, several concerns
have emerged about potential vulnerabilities introduced by ma-
chine learning algorithms. Sophisticated attackers have strong
incentives to manipulate the results and models generated
by machine learning algorithms to achieve their objectives.
For instance, attackers can deliberately inﬂuence the training
dataset to manipulate the results of a predictive model (in
poisoning attacks [5], [39]–[41], [44], [46], [54]), cause mis-
classiﬁcation of new data in the testing phase (in evasion
attacks [3], [8], [20], [42], [43], [49], [50]) or infer private
information on training data (in privacy attacks [18], [19],
[47]). Several experts from academia and industry highlighted
the importance of considering these vulnerabilities in design-
ing machine learning systems in a recent hearing held by the
Senate Subcommittee on Space, Science, and Competitiveness
entitled “The Dawn of AI” [23]. The ﬁeld of adversarial
machine learning studies the effect of such attacks against
machine learning models and aims to design robust defense
algorithms [25]. A comprehensive survey can be found in [6].
We consider the setting of poisoning attacks here, in which
attackers inject a small number of corrupted points in the
training process. Such poisoning attacks have been practically
demonstrated in worm signature generation [41], [44], spam
ﬁlters [39], DoS attack detection [46], PDF malware classiﬁ-
cation [54], handwritten digit recognition [5], and sentiment
analysis [40]. We argue that these attacks become easier to
treatment
is envisioned that patient
mount today as many machine learning models need to be
updated regularly to account for continuously-generated data.
Such scenarios require online training,
in which machine
learning models are updated based on new incoming training
data. For instance, in cyber-security analytics, new Indicators
of Compromise (IoC) rise due to the natural evolution of
malicious threats, resulting in updates to machine learning
models for threat detection [22]. These IoCs are collected
from online platforms like VirusTotal, in which attackers can
also submit IoCs of their choice. In personalized medicine,
it
is adjusted in real-
time by analyzing information crowdsourced from multiple
participants [15]. By controlling a few devices, attackers can
submit fake information (e.g., sensor measurements), which
is then used for training models applied to a large set of pa-
tients. Defending against such poisoning attacks is challenging
with current techniques. Methods from robust statistics (e.g,
[17], [26]) are resilient against noise but perform poorly on
adversarially-poisoned data, and methods for sanitization of
training data operate under restrictive adversarial models [12].
One fundamental class of supervised learning is linear
regression. Regression is widely used for prediction in many
settings (e.g., insurance or loan risk estimation, personalized
medicine, market analysis). In a regression task a numerical
response variable is predicted using a number of predictor
variables, by learning a model that minimizes a loss function.
Regression is powerful as it can also be used for classiﬁcation
tasks by mapping numerical predicted values into class labels.
Assessing the real
impact of adversarial manipulation of
training data in linear regression, as well as determining how
to design learning algorithms resilient under strong adversarial
models is not yet well understood.
In this paper, we conduct
the ﬁrst systematic study of
poisoning attacks and their countermeasures for linear re-
gression models. We make the following contributions: (1)
we are the ﬁrst to consider the problem of poisoning linear
regression under different adversarial models; (2) starting
from an existing baseline poisoning attack for classiﬁcation,
we propose a theoretically-grounded optimization framework
speciﬁcally tuned for regression models; (3) we design a
fast statistical attack that requires minimal knowledge on
the learning process; (4) we propose a principled defense
algorithm with signiﬁcantly increased robustness than known
methods against a large class of attacks; (5) we extensively
© 2018, Matthew Jagielski. Under license to IEEE.
DOI 10.1109/SP.2018.00057
19
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:36:53 UTC from IEEE Xplore.  Restrictions apply. 
evaluate our attacks and defenses on four regression models
(OLS, LASSO, ridge, and elastic net), and on several datasets
from different domains, including health care, loan assessment,
and real estate. We elaborate our contributions below.
• On the attack dimension, we are the ﬁrst to consider the
problem of poisoning attacks against linear regression models.
Compared to classiﬁcation poisoning, in linear regression the
response variables are continuous and their values also can be
selected by the attacker. First, we adapt an existing poisoning
attack for classiﬁcation [54] into a baseline regression attack.
Second, we design an optimization framework for regression
poisoning in which the initialization strategy, the objective
function, and the optimization variables can be selected to
maximize the attack’s impact on a particular model and
dataset. Third, we introduce a fast statistical attack that is
motivated by our theoretical analysis and insights. We ﬁnd
that optimization-based attacks are in general more effective
than statistical-based techniques, at
the expense of higher
computational overhead and more information required by the
adversary on the training process.
• On the defense axis, we propose a principled approach to
constructing a defense algorithm called TRIM, which provides
high robustness and resilience against a large class of poi-
soning attacks. The TRIM method estimates the regression
parameters iteratively, while using a trimmed loss function
to remove points with large residuals. After few iterations,
TRIM is able to isolate most of the poisoning points and
learn a robust regression model. TRIM performs signiﬁcantly
better and is much more effective in providing robustness
compared to known methods from robust statistics (Huber [26]
and RANSAC [17]), typically designed to provide resilience
against noise and outliers. In contrast to these methods, TRIM
is resilient to poisoned points with similar distribution as the
training set. TRIM also outperforms other robust regression
algorithms designed for adversarial settings (e.g., Chen et
al. [11] and RONI [39]). We provide theoretical guarantees
on the convergence of the algorithm and an upper bound on
the model Mean Squared Error (MSE) generated when a ﬁxed
percentage of poisoned data is included in the training set.
• We evaluate our novel attacks and defenses extensively on
four linear regression models and three datasets from health
care,
loan assessment, and real estate domains. First, we
demonstrate the signiﬁcant improvement of our attacks over
the baseline attack of Xiao et al. in poisoning all models and
datasets. For instance, the MSEs of our attacks are increased
by a factor of 6.83 compared to the Xiao et al. attack, and
a factor of 155.7 compared to unpoisoned regression models.
In a case study health application, we ﬁnd that our attacks
can cause devastating consequences. The optimization attack
causes 75% of patients’ Warfarin medicine dosages to change
by an average of 93.49%, while one tenth of these patients
have their dosages changed by 358.89%. Second, we show
that our defense TRIM is also signiﬁcantly more robust than
existing methods against all the attacks we developed. TRIM
achieves MSEs within 1% of the unpoisoned model MSEs.
TRIM achieves MSEs much lower than existing methods,
improving Huber by a factor of 1295.45, RANSAC by a factor
of 75, and RONI by a factor of 71.13.
Outline. We start by providing background on regression
learning, as well as introducing our system and adversarial
model in Section II. We describe the baseline attack adapted
from Xiao et al. [54], and our new poisoning attacks in
Section III. Subsequently, we introduce our novel defense
algorithm TRIM in Section IV. Section V includes a detailed
experimental analysis of our attacks and defenses, as well as
comparison with previous methods. Finally, we present related
work in Section VI and conclude in Section VII.
II. SYSTEM AND ADVERSARIAL MODEL
Linear regression is at the basis of machine learning [24].
It is widely studied and applied in many applications due to
its efﬁciency, simplicity of use, and effectiveness. Other more
advanced learning methods (e.g., logistic regression, SVM,
neural networks) can be seen as generalizations or extensions
of linear regression. We systematically study the effect of
poisoning attacks and their defenses for linear regression. We
believe that our understanding of the resilience of this funda-
mental class of learning model to adversaries will enable future
research on other classes of supervised learning methods.
Problem deﬁnition. Our system model is a supervised setting
consisting of a training phase and a testing phase as shown
in Figure 1 on the left (“Ideal world”). The learning process
includes a data pre-processing stage that performs data clean-
ing and normalization, after which the training data can be
represented, without loss of generality, as Dtr = {(xi, yi)}n
i=1,
where xi ∈ [0, 1]d are d-dimensional numerical predictor
variables (or feature vectors) and yi ∈ [0, 1] are numerical
response variables, for i ∈ {1, . . . , n}. After that, the learning
algorithm is applied to generate the regression model at the end
of the training phase. In the testing phase, the model is applied
to new data after pre-processing, and a numerical predicted
value is generated using the regression model
learned in
training. Our model thus captures a standard multi-dimensional
regression setting applicable to different prediction tasks.
(cid:2)
In linear regression, the model output at the end of the
x + b, which
training stage is a linear function f (x, θ) = w
predicts the value of y at x. This function is parametrized
by a vector θ = (w, b) ∈ R
d+1 consisting of the feature
d and the bias b ∈ R. Note that regression is
weights w ∈ R
substantially different from classiﬁcation, as the y values are
numerical, rather than being a set of indices (each denoting a
different class from a predetermined set). The parameters of
f are chosen to minimize a quadratic loss function:
+λΩ(w) ,
(1)
(cid:3)
L(Dtr, θ) = 1
n
(cid:2)n
(cid:6)
i=1 (f (xi, θ) − yi)
MSE(Dtr,θ)
(cid:4)(cid:5)
2
where the Mean Squared Error MSE(Dtr, θ) measures the
error in the predicted values assigned by f (·, θ) to the training
samples in Dtr as the sum of squared residuals, Ω(w) is
a regularization term penalizing large weight values, and λ
20
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:36:53 UTC from IEEE Xplore.  Restrictions apply. 
the feature values x,
at test time, while preserving the predictions on the other test
samples. This is a similar setting to that of backdoor poisoning
attacks recently reported in classiﬁcation settings [9], [21].
Adversary’s Knowledge. We assume here two distinct attack
scenarios, referred to as white-box and black-box attacks in
the following. In white-box attacks, the attacker is assumed
to know the training data Dtr,
the
learning algorithm L, and even the trained parameters θ.
These attacks have been widely considered in previous work,
although mainly against classiﬁcation algorithms [5], [36],
[54]. In black-box attacks, the attacker has no knowledge
of the training set Dtr but can collect a substitute data set
D(cid:3)
tr. The feature set and learning algorithm are known, while
the trained parameters are not. However, the latter can be
estimated by optimizing L on the substitute data set D(cid:3)
tr. This
setting is useful to evaluate the transferability of poisoning
attacks across different training sets, as discussed in [38], [54].
Adversary’s Capability. In poisoning attacks, the attacker
injects poisoning points into the training set before the regres-
sion model is trained (see the right side of Figure 1 labeled
“Adversarial world”). The attacker’s capability is normally
limited by upper bounding the number p of poisoning points
that can be injected into the training data, whose feature values
and response variables are arbitrarily set by the attacker within
a speciﬁed range (typically, the range covered by the training
data, i.e., [0, 1] in our case) [38], [54]. The total number
of points in the poisoned training set is thus N = n + p,
with n being the number of pristine training samples. We
then deﬁne the ratio α = p/n, and the poisoning rate as the
actual fraction of the training set controlled by the attacker,
i.e., n/N = α/(1 + α). In previous work, poisoning rates
higher than 20% have been only rarely considered, as the
attacker is typically assumed to be able to control only a small
fraction of the training data. This is motivated by application
scenarios such as crowdsourcing and network trafﬁc analysis,
in which attackers can only reasonably control a small fraction
of participants and network packets, respectively. Moreover,
learning a sufﬁciently-accurate regression function in the pres-
ence of higher poisoning rates would be an ill-posed task, if
not infeasible at all [5], [25], [36], [38], [53], [54].
Poisoning Attack Strategy. All the aforementioned poisoning
attack scenarios, encompassing availability and integrity vio-
lations under white-box or black-box knowledge assumptions,
can be formalized as a bilevel optimization problem [36], [38].
For white-box attacks, this can be written as:
arg maxDp
s.t.
W(D(cid:3), θ
p ∈ arg minθ L(Dtr ∪ Dp, θ) .
θ
(cid:2)
p) ,
(cid:2)
(2)
(3)
The outer optimization amounts to selecting the poisoning
points Dp to maximize a loss function W on an untainted
data set D(cid:3) (e.g., a validation set which does not contain any
poisoning points), while the inner optimization corresponds to
retraining the regression algorithm on a poisoned training set
including Dp. It should be clear that θ
(cid:2)
p depends implicitly on
the set Dp of poisoning attack samples through the solution of
21
Fig. 1: System architecture.
i.e.,
is the so-called regularization parameter. Regularization is
used to prevent overﬁtting,
to preserve the ability of
the learning algorithm to generalize well on unseen (testing)
data. For regression problems, this capability, i.e., the expected
performance of the trained function f on unseen data,
is
typically assessed by measuring the MSE on a separate test
set. Popular linear regression methods differ mainly in the
choice of the regularization term. In particular, we consider
four models in this paper:
1) Ordinary Least Squares (OLS), for which Ω(w) = 0
(i.e., no regularization is used);
2(cid:3)w(cid:3)2
2;