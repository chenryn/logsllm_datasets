# Title: Manipulating Machine Learning: Poisoning Attacks and Countermeasures for Regression Learning

## Authors:
- Matthew Jagielski
- Alina Oprea
- Battista Biggio
- Chang Liu
- Cristina Nita-Rotaru
- Bo Li

### Affiliations:
- UC Berkeley, Berkeley, CA
- University of Cagliari, Cagliari, Italy
- Northeastern University, Boston, MA

### Abstract:
As machine learning (ML) becomes increasingly prevalent in automated decision-making, attackers have strong incentives to manipulate the results and models generated by ML algorithms. This paper presents the first systematic study of poisoning attacks and their countermeasures for linear regression models. In poisoning attacks, adversaries deliberately influence the training data to manipulate the predictive model's outcomes. We propose a theoretically-grounded optimization framework specifically designed for linear regression, demonstrating its effectiveness across various datasets and models. Additionally, we introduce a fast statistical attack that requires minimal knowledge of the training process. We also design a new principled defense method, TRIM, which is highly resilient against all poisoning attacks. We provide formal guarantees about TRIM's convergence and an upper bound on the effect of poisoning attacks when the defense is deployed. Our extensive evaluation on three realistic datasets from healthcare, loan assessment, and real estate domains shows the efficacy of our attacks and defenses.

## I. Introduction
As more applications with significant societal impact rely on machine learning for automated decisions, concerns have emerged about potential vulnerabilities introduced by these algorithms. Sophisticated attackers have strong incentives to manipulate the results and models generated by ML algorithms to achieve their objectives. For example, attackers can deliberately influence the training dataset to manipulate the results of a predictive model (poisoning attacks), cause misclassification of new data in the testing phase (evasion attacks), or infer private information on training data (privacy attacks). Several experts from academia and industry highlighted the importance of considering these vulnerabilities in designing ML systems during a recent Senate Subcommittee hearing titled "The Dawn of AI."

Adversarial machine learning studies the effects of such attacks on ML models and aims to design robust defense algorithms. A comprehensive survey can be found in [6]. In this paper, we focus on poisoning attacks, where attackers inject a small number of corrupted points into the training process. Such attacks have been demonstrated in various domains, including worm signature generation, spam filters, DoS attack detection, PDF malware classification, handwritten digit recognition, and sentiment analysis. These attacks are becoming easier to mount as many ML models need regular updates to account for continuously-generated data, often requiring online training.

Defending against such poisoning attacks is challenging with current techniques. Methods from robust statistics (e.g., Huber [26] and RANSAC [17]) are resilient against noise but perform poorly on adversarially-poisoned data. Methods for sanitizing training data operate under restrictive adversarial models [12].

One fundamental class of supervised learning is linear regression, widely used for prediction in various settings such as insurance, loan risk estimation, personalized medicine, and market analysis. In a regression task, a numerical response variable is predicted using predictor variables by learning a model that minimizes a loss function. Assessing the real impact of adversarial manipulation of training data in linear regression and determining how to design learning algorithms resilient under strong adversarial models is not yet well understood.

In this paper, we conduct the first systematic study of poisoning attacks and their countermeasures for linear regression models. Our contributions include:
1. Considering the problem of poisoning linear regression under different adversarial models.
2. Proposing a theoretically-grounded optimization framework specifically tuned for regression models.
3. Designing a fast statistical attack that requires minimal knowledge of the learning process.
4. Proposing a principled defense algorithm, TRIM, with significantly increased robustness against a large class of attacks.
5. Extensively evaluating our attacks and defenses on four regression models (OLS, LASSO, ridge, and elastic net) and several datasets from different domains, including healthcare, loan assessment, and real estate.

## II. System and Adversarial Model

### Linear Regression
Linear regression is a fundamental class of machine learning, widely studied and applied due to its efficiency, simplicity, and effectiveness. Other advanced learning methods (e.g., logistic regression, SVM, neural networks) can be seen as generalizations or extensions of linear regression. We systematically study the effect of poisoning attacks and their defenses for linear regression, believing that our understanding will enable future research on other classes of supervised learning methods.

### Problem Definition
Our system model is a supervised setting consisting of a training phase and a testing phase. The learning process includes a data pre-processing stage that performs data cleaning and normalization. The training data can be represented as \( D_{tr} = \{(x_i, y_i)\}_{i=1}^n \), where \( x_i \in [0, 1]^d \) are d-dimensional numerical predictor variables and \( y_i \in [0, 1] \) are numerical response variables. After pre-processing, the learning algorithm generates the regression model at the end of the training phase. In the testing phase, the model is applied to new data, and a numerical predicted value is generated using the learned regression model.

In linear regression, the model output is a linear function \( f(x, \theta) = w^T x + b \), which predicts the value of \( y \) at \( x \). This function is parameterized by a vector \( \theta = (w, b) \in \mathbb{R}^{d+1} \) consisting of the feature weights \( w \in \mathbb{R}^d \) and the bias \( b \in \mathbb{R} \). The parameters of \( f \) are chosen to minimize a quadratic loss function:
\[ L(D_{tr}, \theta) = \frac{1}{n} \sum_{i=1}^n (f(x_i, \theta) - y_i)^2 + \lambda \Omega(w), \]
where the Mean Squared Error (MSE) measures the error in the predicted values assigned by \( f(·, \theta) \) to the training samples, \( \Omega(w) \) is a regularization term penalizing large weight values, and \( \lambda \) is the regularization parameter. Regularization is used to prevent overfitting and ensure the model generalizes well on unseen data.

### Adversary’s Knowledge
We consider two distinct attack scenarios: white-box and black-box attacks.
- **White-box attacks**: The attacker knows the training data \( D_{tr} \), the learning algorithm \( L \), and the trained parameters \( \theta \).
- **Black-box attacks**: The attacker has no knowledge of the training set \( D_{tr} \) but can collect a substitute data set \( D'_{tr} \). The feature set and learning algorithm are known, but the trained parameters are not. The latter can be estimated by optimizing \( L \) on the substitute data set \( D'_{tr} \).

### Adversary’s Capability
In poisoning attacks, the attacker injects poisoning points into the training set before the regression model is trained. The attacker's capability is limited by an upper bound on the number \( p \) of poisoning points that can be injected, with feature values and response variables arbitrarily set within a specified range. The total number of points in the poisoned training set is \( N = n + p \), with \( n \) being the number of pristine training samples. We define the ratio \( \alpha = \frac{p}{n} \) and the poisoning rate as the actual fraction of the training set controlled by the attacker, i.e., \( \frac{n}{N} = \frac{\alpha}{1 + \alpha} \).

### Poisoning Attack Strategy
Poisoning attack scenarios, encompassing availability and integrity violations under white-box or black-box knowledge assumptions, can be formalized as a bilevel optimization problem. For white-box attacks, this can be written as:
\[ \arg \max_{D_p} W(D', \theta_p) \quad \text{s.t.} \quad \theta_p \in \arg \min_{\theta} L(D_{tr} \cup D_p, \theta). \]
The outer optimization selects the poisoning points \( D_p \) to maximize a loss function \( W \) on an untainted data set \( D' \), while the inner optimization re-trains the regression algorithm on a poisoned training set including \( D_p \).

## III. Baseline and New Poisoning Attacks
We adapt an existing poisoning attack for classification [54] into a baseline regression attack. We then design an optimization framework for regression poisoning, where the initialization strategy, objective function, and optimization variables can be selected to maximize the attack's impact on a particular model and dataset. We also introduce a fast statistical attack motivated by our theoretical analysis and insights. We find that optimization-based attacks are generally more effective than statistical-based techniques, albeit at the expense of higher computational overhead and more information required by the adversary on the training process.

## IV. Novel Defense Algorithm: TRIM
We propose a principled approach to constructing a defense algorithm called TRIM, which provides high robustness and resilience against a large class of poisoning attacks. TRIM estimates the regression parameters iteratively, using a trimmed loss function to remove points with large residuals. After a few iterations, TRIM isolates most of the poisoning points and learns a robust regression model. TRIM outperforms known methods from robust statistics (Huber [26] and RANSAC [17]) and other robust regression algorithms designed for adversarial settings (e.g., Chen et al. [11] and RONI [39]). We provide theoretical guarantees on the convergence of the algorithm and an upper bound on the model MSE when a fixed percentage of poisoned data is included in the training set.

## V. Experimental Analysis
We extensively evaluate our novel attacks and defenses on four linear regression models (OLS, LASSO, ridge, and elastic net) and three datasets from healthcare, loan assessment, and real estate domains. Our experiments show significant improvements of our attacks over the baseline attack of Xiao et al. [54] in poisoning all models and datasets. For instance, the MSEs of our attacks are increased by a factor of 6.83 compared to the Xiao et al. attack and a factor of 155.7 compared to unpoisoned regression models. In a case study health application, our attacks can cause devastating consequences, with 75% of patients’ Warfarin medicine dosages changing by an average of 93.49%, and one-tenth of these patients having their dosages changed by 358.89%.

Our defense, TRIM, is significantly more robust than existing methods against all the attacks we developed. TRIM achieves MSEs within 1% of the unpoisoned model MSEs, much lower than existing methods, improving Huber by a factor of 1295.45, RANSAC by a factor of 75, and RONI by a factor of 71.13.

## VI. Related Work
We review related work on adversarial machine learning, focusing on poisoning attacks and their defenses in both classification and regression settings. We discuss the strengths and limitations of existing approaches and highlight the contributions of our work.

## VII. Conclusion
In conclusion, we present the first systematic study of poisoning attacks and their countermeasures for linear regression models. Our contributions include a theoretically-grounded optimization framework, a fast statistical attack, and a principled defense algorithm, TRIM. Our extensive experimental evaluation demonstrates the effectiveness of our attacks and defenses across various datasets and models. Future work will explore the extension of our methods to other types of supervised learning and the development of more sophisticated attack and defense strategies.

---

This revised version of the text is more structured, coherent, and professional, making it easier to read and understand.