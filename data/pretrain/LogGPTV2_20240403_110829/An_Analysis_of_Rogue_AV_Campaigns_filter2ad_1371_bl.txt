### 4. Implementation

Allocation-driven mapping is a versatile technique that can be applied to operating systems (OS) adhering to standard function call conventions, such as Linux and Windows. Our prototype, LiveDM, supports three off-the-shelf Linux distributions with different kernel versions: Fedora Core 6 (Linux 2.6.18), Debian Sarge (Linux 2.6.8), and Redhat 8 (Linux 2.4.18).

LiveDM is compatible with various software virtualization platforms, including VMware (Workstation and Player) [29], VirtualBox [26], and Parallels [14]. For our implementation, we selected QEMU [2] with the KQEMU optimizer for its convenience.

In the kernel source code, numerous wrappers are employed for memory management, some of which are defined as macros or inline functions, while others are regular functions. Macros and inline functions are resolved at compile time by the preprocessor, so their call sites are captured in the same manner as core functions. However, for regular wrapper functions, the call sites are part of the wrapper code.

To address this, we adopt two strategies. If a wrapper is used infrequently, we assume that the type from the wrapper indirectly implies the type used in the caller. If a wrapper is extensively used (e.g., kmem_cache_alloc, a slab allocator), we treat it as a memory allocation function. Established OSes, known for their mature code quality, have a well-defined set of memory wrapper functions commonly used in kernel and driver code. In our experience, capturing these wrappers, along with core memory functions, covers most memory allocation and deallocation operations.

We classify the captured functions into four categories: (1) page allocation/free functions, (2) kmalloc/kfree functions, (3) kmem_cache_alloc/free functions (slab allocators), and (4) vmalloc/vfree functions (contiguous memory allocators). These categories include both well-defined wrapper functions and core memory functions. In our prototype, we capture approximately 20 functions per guest kernel. The memory functions of an OS kernel can be determined from its design specification (e.g., the Linux Kernel API) or the kernel source code.

For automatic translation of a call site to a data type, a kernel binary compiled with debugging flags (e.g., -g for gcc) and unstripped symbols is required. Modern OSes, such as Ubuntu, Fedora, and Windows, provide such binaries for kernel debugging. Typically, stripped binaries are distributed, but unstripped binaries (or symbol information in Windows) are available for debugging. The kernels of Debian Sarge and Redhat 8 were not compiled with debugging flags, so we compiled the distributed source code to generate debug-enabled kernels. These kernels share the same source code as the distributed ones but may have slightly different offsets due to additional debugging information.

For static analysis, we use an instrumented version of the gcc compiler (version 3.2.3) to generate intermediate representations (IRs) for the source code of the experimented kernels. We place hooks in the parser to extract the abstract syntax trees necessary for static code analysis.

### 5. Evaluation

In this section, we evaluate the basic functionality of LiveDM regarding the identification of kernel objects, casting code patterns, and the performance of allocation-driven mapping. The guest systems are configured with 256MB RAM, and the host machine has a 3.2Ghz Pentium D CPU and 2GB of RAM.

**Identifying Dynamic Kernel Objects.** To demonstrate LiveDM's ability to inspect the runtime status of an OS kernel, Table 1 lists important kernel data structures captured during the execution of Debian Sarge. These data structures manage key OS statuses, such as process information, memory mapping, and file system and network states, which are often targeted by kernel malware and bugs [13,15,16,17,18,23,27,28].

| **Case** | **#Objects** | **Call Site** | **Declaration** | **Data Type** |
|----------|--------------|---------------|-----------------|---------------|
| 1        | 1            | kernel/fork.c:248 | kernel/fork.c:243 | task_struct   |
| 1        | 1            | kernel/fork.c:801 | kernel/fork.c:795 | sighand_struct |
| ...      | ...          | ...           | ...             | ...           |

Kernel objects are recognized using allocation call sites shown in the "Call Site" column. Static analysis translates this information into data types shown in the "Data Type" column by traversing the allocation code and the declaration of pointer variables or functions. The "Case" column shows the kind of allocation code pattern. The "Number of Objects" column presents the number of identified objects for each type. At the time of inspection, LiveDM identified a total of 29,488 dynamic kernel objects from 231 allocation code positions.

To evaluate the accuracy of the identified kernel objects, we built a reference kernel that logs dynamic kernel objects and ran it in LiveDM. The dynamic objects from the log accurately matched the live dynamic kernel objects captured by LiveDM. For type derivation accuracy, we manually translated the captured call sites to data types by traversing the kernel source code, as done by related approaches [5,7]. The derived types matched the results from our automatic static code analysis.

**Code Patterns Casting Objects from Generic Types to Specific Types.** As discussed in Section 3.1, allocation-driven mapping handles specific-to-generic type casting well but faces challenges with generic-to-specific type casting. To estimate the frequency of this issue, we manually checked all allocation code positions where kernel object types were derived. We looked for patterns where memory was allocated using a generic pointer and then cast to a more specific type. This does not include valid uses of generic pointers, such as for bit fields or buffers. We found 25 objects from 10 allocation code positions (e.g., tty_register_driver and vc_allocate) exhibiting this behavior. These objects are not part of the core data structures and account for only 0.085% of all objects, making them non-significant corner cases. Since the code positions are known to LiveDM, we believe this behavior and the derivation of specific types can be automated through static analysis after the allocation code.

**Performance of Allocation-Driven Mapping.** While LiveDM is primarily intended for non-production environments like honeypots and kernel debugging systems, we still measured its performance. We ran three benchmarks: compiling the kernel source code, UnixBench (Byte Magazine Unix Benchmark 5.1.2), and nbench (BYTEmark* Native Mode Benchmark version 2). Compared to unmodified QEMU, our prototype incurred up to 41.77% overhead for Redhat 8 (Linux 2.4) and 125.47% for Debian Sarge (Linux 2.6). For CPU-intensive workloads like nbench, the overhead was near zero because the VMM rarely intervened. However, applications using kernel services requiring dynamic memory had higher overhead. For example, compiling the Linux kernel showed an overhead of 29% for Redhat 8 and 115.69% for Debian Sarge. These numbers measure overhead compared to an unmodified VMM. Software-based virtualization adds additional overhead. For fine-grained kernel behavior inspection in non-production environments, we consider this overhead acceptable. The effects can be minimized in production environments using decoupled analysis [6].

### 6. Case Studies

We present two kernel malware analysis systems built on top of LiveDM: a hidden kernel object detector and a temporal malware behavior monitor. These systems highlight the new properties of allocation-driven mapping effective for detecting and analyzing kernel malware attacks.

#### 6.1 Hidden Kernel Object Detector

One limitation of static type-projection approaches is their inability to detect dynamic kernel object manipulation without data invariants. Here, we present a hidden kernel object detector built on LiveDM that overcomes this limitation.

**Leveraging the Un-Tampered View.** Advanced DKOM-based kernel rootkits hide kernel objects by removing all references to them from the kernel's dynamic memory. We model this attack as a data anomaly in a list. If a dynamic kernel object is not in the kernel object list, it is orphaned and anomalous. Allocation-driven mapping provides an un-tampered view of kernel objects unaffected by memory content manipulation. If an object appears in the LiveDM-generated map but not in the kernel memory, it has been hidden. Formally, for a set of dynamic kernel objects of a given type, a live set \( L \) is the set of objects in the kernel object map, and a scanned set \( S \) is the set of objects found by traversing the kernel memory. If \( L \) and \( S \) do not match, a data anomaly is reported.

This is illustrated in the example of the cleaner rootkit hiding the adore-ng rootkit module (Fig. 3). Fig. 3(a) shows the timeline of the attack using the lifetime of kernel modules. Fig. 3(b) details the status of kernel modules and corresponding \( L \) and \( S \) at three key moments. When the cleaner module is loaded after the adore-ng module, it modifies the linked list to bypass the adore-ng module entry (at \( t_2 \)). When the cleaner module is unloaded, the adore-ng module disappears from the list (at \( t_3 \)). At this point, the scanned set \( S \) based on static type-projection mapping loses the hidden module, but the live set \( L \) retains the view of all modules. Thus, the monitor detects the hidden kernel module due to the condition \( |L| \neq |S| \).

**Detecting DKOM Data Hiding Attacks.** Two dynamic kernel data lists are common targets for rootkits: the kernel module list and the process control list. Table 2 lists DKOM data hiding rootkit attacks automatically detected by comparing the LiveDM-generated view \( L \) and the kernel memory view \( S \).

| **Rootkit Name** | **Detection** |
|------------------|---------------|
| hide_lkm         | Detected      |
| ...              | ...           |

By leveraging the un-tampered view provided by LiveDM, we can effectively detect and analyze kernel malware attacks.