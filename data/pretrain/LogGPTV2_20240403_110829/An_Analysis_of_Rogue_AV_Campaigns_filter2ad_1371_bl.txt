4 Implementation
Allocation-driven mapping is general enough to work with an OS that follows the stan-
dard function call conventions (e.g., Linux, Windows, etc.). Our prototype, LiveDM, sup-
ports three off-the-shelf Linux OSes of different kernel versions: Fedora Core 6 (Linux
2.6.18), Debian Sarge (Linux 2.6.8), and Redhat 8 (Linux 2.4.18).
LiveDM can be implemented on any software virtualization system, such as VMware
(Workstation and Player) [29], VirtualBox [26], and Parallels [14]. We choose the QEMU
[2] with KQEMU optimizer for implementation convenience.
In the kernel source code, many wrappers are used for kernel memory management,
some of which are deﬁned as macros or inline functions and others as regular functions.
186
J. Rhee et al.
Macros and inline functions are resolved as the core memory function calls at compile
time by a preprocessor; thus, their call sites are captured in the same way as core func-
tions. However, in the case of regular wrapper functions, the call sites will belong to the
wrapper code.
To solve this problem, we take two approaches. If a wrapper is used only a few times,
we consider that the type from the wrapper can indirectly imply the type used in the
wrapper’s caller due to its limited use. If a wrapper is widely used in many places (e.g.,
kmem cache alloc – a slab allocator), we treat it as a memory allocation function. Com-
modity OSes, which have mature code quality, have a well deﬁned set of memory wrap-
per functions that the kernel and driver code commonly use. In our experience, capturing
such wrappers, in addition to the core memory functions, can cover the majority of the
memory allocation and deallocation operations.
We categorize the captured functions into four classes: (1) page allocation/free func-
tions, (2) kmalloc/kfree functions, (3) kmem cache alloc/free functions (slab al-
locators), and (4) vmalloc/vfree functions (contiguous memory allocators). These
sets include the well deﬁned wrapper functions as well as the core memory functions.
In our prototype, we capture about 20 functions in each guest kernel. The memory func-
tions of an OS kernel can be determined from its design speciﬁcation (e.g., the Linux
Kernel API) or kernel source code.
Automatic translation of a call site to a data type requires a kernel binary that is com-
piled with a debugging ﬂag (e.g., -gto gcc) and whose symbols are not stripped. Modern
OSes, such as Ubuntu, Fedora, and Windows, generate kernel binaries of this form. Upon
distribution, typically the stripped kernel binaries are shipped; however, unstripped bina-
ries (or symbol information in Windows) are optionally provided for kernel debugging
purposes. The experimented kernels of Debian Sarge and Redhat 8 are not compiled with
this debugging ﬂag. Therefore, we compiled the distributed source code and generated
the debug-enabled kernels. These kernels share the same source code with the distributed
kernels, but the offset of the compiled binary code can be slightly different due to the
additional debugging information.
For static analysis we use a gcc [8] compiler (version 3.2.3) that we instrumented
to generate IRs for the source code of the experimented kernels. We place hooks in the
parser to extract the abstract syntax trees for the code elements necessary in the static
code analysis.
5 Evaluation
In this section, we evaluate the basic functionality of LiveDM with respect to the identiﬁ-
cation of kernel objects, casting code patterns, and the performance of allocation-driven
mapping. The guest systems are conﬁgured with 256MB RAM and the host machine has
a 3.2Ghz Pentium D CPU and 2GB of RAM.
Identifying dynamic kernel objects. To demonstrate the ability of LiveDM to inspect
the runtime status of an OS kernel, we present a list of important kernel data structures
captured during the execution of Debian Sarge OS in Table 1. These data structures man-
age the key OS status such as process information, memory mapping of each process,
and the status of ﬁle systems and network which are often targeted by kernel malware
Kernel Malware Analysis with Un-tampered and Temporal Views
187
Table 1. A list of core dynamic kernel objects and the source code elements used to derive their
data types in static analysis. (OS: Debian Sarge).
Case #Objects
Call Site
Declaration
kernel/fork.c:243
kernel/fork.c:795
fs/exec.c:587
kernel/fork.c:813
arch/i386/mm/pgtable.c:229
kernel/fork.c:431
kernel/fork.c:526
kernel/fork.c:271
mm/mmap.c:748
mm/mmap.c:1521
mm/mmap.c:1657
fs/exec.c:342
kernel/fork.c:654
kernel/fork.c:597
fs/file table.c:69
fs/buffer.c:3062
fs/block dev.c:232
fs/dcache.c:689
fs/inode.c:107
fs/namespace.c:55
fs/proc/inode.c:90
1
g kernel/fork.c:248
1
kernel/fork.c:801
S
1
fs/exec.c:601
1
kernel/fork.c:819
2
arch/i386/mm/pgtable.c:229
1
kernel/fork.c:433
1
kernel/fork.c:559
1
kernel/fork.c:314
1
mm/mmap.c:923
1
mm/mmap.c:1526
1
mm/mmap.c:1722
1
fs/exec.c:402
1
kernel/fork.c:677
2
kernel/fork.c:597
1
fs/file table.c:76
2
fs/buffer.c:3062
2
fs/block dev.c:232
1
fs/dcache.c:692
1
fs/inode.c:112
2
fs/namespace.c:55
1
fs/proc/inode.c:93
drivers/block/ll rw blk.c:1405 drivers/block/ll rw blk.c:1405 request queue t 2
1
drivers/block/ll rw blk.c:2950 drivers/block/ll rw blk.c:2945 io context
1
socket alloc
net/socket.c:279
1
sock
net/core/sock.c:617
1
dst entry
net/core/dst.c:125
1
neighbour
net/core/neighbour.c:265
tcp bind bucket 2
net/ipv4/tcp ipv4.c:134
1
net/ipv4/fib hash.c:586
fib node
Data Type
task struct
sighand struct
sighand struct
signal struct
pgd t
mm struct
mm struct
vm area struct
vm area struct
vm area struct
vm area struct
vm area struct
files struct
fs struct
file
buffer head
bdev inode
dentry
inode
vfsmount
proc inode
net/socket.c:278
net/core/sock.c:613
net/core.dst.c:119
net/core/neighbour.c:254
net/ipv4/tcp ipv4.c:133
net/ipv4/fib hash.c:461
m
e
t
s
y
s
e
l
i
F
66
63
1
66
54
47
7
149
1004
5
48
47
54
53
531
828
5
4203
1209
16
237
18
10
12
3
5
1
4
9
i
/
k
s
a
T
y
r
o
m
e
M
k
r
o
w
t
e
N
and kernel bugs [13,15,16,17,18,23,27,28]. Kernel objects are recognized using allo-
cation call sites shown in column Call Site during runtime. Using static analysis, this
information is translated into the data types shown in column Data Type by traversing
the allocation code and the declaration of a pointer variable or a function shown in col-
umn Declaration. Column Case shows the kind of the allocation code pattern described in
Section 3.2. The number of the identiﬁed objects for each type in the inspected runtime
status is presented in column #Objects. At that time instance, LiveDM identiﬁed total
of 29488 dynamic kernel objects with their data types derived from 231 allocation code
positions.
In order to evaluate the accuracy of the identiﬁed kernel objects, we build a reference
kernel where we modify kernel memory functions to generate a log of dynamic kernel
objects and run this kernel in LiveDM. We observe that the dynamic objects from the
log accurately match the live dynamic kernel objects captured by LiveDM. To check the
type derivation accuracy, we manually translate the captured call sites to data types by
traversing kernel source code as done by related approaches [5,7]. The derived types at
the allocation code match the results from our automatic static code analysis.
Code patterns casting objects from generic types to speciﬁc types. In Section 3.1, we
discussed that allocation-driven mapping has no problem handling the situation where a
speciﬁc type is cast to a generic type, but casting from generic types to speciﬁc types can
188
J. Rhee et al.
be a problem. In order to estimate how often this type of casting occurs, we manually
checked all allocation code positions where the types of kernel objects are derived for
the inspected status. We checked for the code pattern that memory is allocated using a
generic pointer and then the address is cast to the pointer of a more speciﬁc type. Note
that this pattern does not include the use of generic pointers for generic purposes. For
example, the use of void or integer pointers for bit ﬁelds or buffers is a valid use of generic
pointers. Another valid use is kernel memory functions that internally handle pre-typed
memory using generic pointers to retail it to various types. We found 25 objects from 10
allocation code positions (e.g., tty register driver and vc allocate) exhibiting
this behavior at runtime. Such objects are not part of the core data structures shown
in Table 1, and they account for only 0.085% of all objects. Hence we consider them
as non-signiﬁcant corner cases. Since the code positions where this casting occurs are
available to LiveDM, we believe that the identiﬁcation of this behavior and the derivation
of a speciﬁc type can be automated by performing static analysis on the code after the
allocation code.
Performance of allocation-driven mapping. Since LiveDM is mainly targeted for non-
production environments such as honeypots and kernel debugging systems, performance
is not a primary concern. Still, we would like to provide a general idea of the cost of
allocation-driven mapping. In order to measure the overhead to generate a kernel object
map at runtime, we ran three benchmarks: compiling the kernel source code, UnixBench
(Byte Magazine Unix Benchmark 5.1.2), and nbench (BYTEmark* Native Mode Bench-
mark version 2). Compared to unmodiﬁed QEMU, our prototype incurs (in the worst
case) 41.77% overhead for Redhat 8 (Linux 2.4) and 125.47% overhead for Debian Sarge
(Linux 2.6). For CPU intensive workload such as nbench, the overhead is near zero be-
cause the VMM rarely intervenes. However, applications that use kernel services requir-
ing dynamic kernel memory have higher overhead. As a speciﬁc example, compiling the
Linux kernel exhibited an overhead of 29% for Redhat 8 and 115.69% for Debian Sarge.
It is important to note that these numbers measure overhead when compared to an un-
modiﬁed VMM. Software based virtualization will add additional overhead as well. For
the purpose of inspecting ﬁne-grained kernel behavior in non-production environments,
we consider this overhead acceptable. The effects of overhead can even be minimized in
a production environment by using decoupled analysis [6].
6 Case Studies
We present two kernel malware analysis systems built on top of LiveDM: a hidden ker-
nel object detector and a temporal malware behavior monitor. These systems highlight
the new properties of allocation-driven mapping which are effective for detection and
analysis of kernel malware attacks.
6.1 Hidden Kernel Object Detector
One problem with static type-projection approaches is that they are not able to detect
dynamic kernel object manipulation without some sort of data invariant. In this section
Kernel Malware Analysis with Un-tampered and Temporal Views
189
(a) Temporal live status of kernel modules
based on allocation-driven mapping.
(b) Live set (L) and scanned set (S) for kernel
modules at t1, t2, and t3.
Fig. 3. Illustration of the kernel module hiding attack by cleaner rootkit. Note that the choice
of t1, t2, and t3 is for the convenience of showing data status and irrelevant to the detection. This
attack is detected based on the difference between L and S.
we present a hidden kernel object detector built on top of LiveDM that does not suffer
from this limitation.
Leveraging the un-tampered view. Some advanced DKOM-based kernel rootkits hide
kernel objects by simply removing all references to them from the kernel’s dynamic
memory. We model the behavior of this type of DKOM data hiding attack as a data
anomaly in a list. If a dynamic kernel object does not appear in a kernel object list, then it
is orphaned and hence an anomaly. As described in Section 3.1, allocation-driven map-
ping provides an un-tampered view of the kernel objects not affected by manipulation of
the actual kernel memory content. Therefore, if a kernel object appears in the LiveDM-
generated kernel object map but cannot be found by traversing the kernel memory, then
that object has been hidden. More formally, for a set of dynamic kernel objects of a given
data type, a live set L is the set of objects found in the kernel object map. A scanned set
S is the set of kernel objects found by traversing the kernel memory as in the related
approaches [1,5,16]. If L and S do not match, then a data anomaly will be reported.
This process is illustrated in the example of cleaner rootkit that hides the adore-ng
rootkit module (Fig. 3). Fig. 3(a) presents the timeline of this attack using the lifetime
of kernel modules. Fig. 3(b) illustrates the detailed status of kernel modules and cor-
responding L and S at three key moments. Kernel modules are organized as a linked
list starting from a static pointer variable. When the cleaner module is loaded after
the adore-ng module, it modiﬁes the linked list to bypass the adore-ng module entry
(shown at t2). Therefore, when the cleaner module is unloaded, the adore-ng mod-
ule disappears from the module list (t3). At this point in time the scanned set S based
on static type-projection mapping has lost the hidden module, but the live set L keeps
the view of all kernel modules alive. Therefore, the monitor can detect a hidden kernel
module due to the condition, |L| (cid:13)= |S|.
Detecting DKOM data hiding attacks. There are two dynamic kernel data lists which
are favored by rootkits as attack targets: the kernel module list and the process control
190
J. Rhee et al.
Table 2. DKOM data hiding rootkit attacks that are automatically detected by comparing LiveDM-
generated view (L) and kernel memory view (S)
Rootkit
Name
hide lkm