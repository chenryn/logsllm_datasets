使用多个控制平面服务器会带来额外的复杂性，因为在我们的配置文件中，我们只能针对单个主机或 IP。为了使这种配置可用，我们需要在集群前面部署一个负载平衡器。
KinD 已经考虑到了这一点，如果您确实部署了多个控制平面节点，安装将创建一个运行 HAProxy 负载平衡器的附加容器。如果我们从多节点配置中查看正在运行的容器，我们将看到六个节点容器正在运行，一个 HAProxy 容器 r:
![Table 4.4 – KinD configuration options ](img/Table_4.4.jpg)
表 4.4–KinD 配置选项
请记住，在 [*第 3 章*](03.html#_idTextAnchor062) *【理解 Docker 网络】*中，我们解释了端口和插座。因为我们只有一台主机，所以每个控制平面节点和 HAProxy 容器都运行在唯一的端口上。每个容器都需要向主机公开，以便它们可以接收传入的请求。在本例中，需要注意的重要一点是分配给 HAProxy 的端口，因为这是集群的目标端口。如果您查看 Kubernetes 配置文件，您会看到它的目标是 [https://127.0.0.1:32791](https://127.0.0.1:32791) ，这是分配给 HAProxy 容器的端口。
当使用`kubectl`执行命令时，它被直接发送到 HAProxy 服务器。使用由 KinD 在集群创建期间创建的配置文件，HAProxy 容器知道如何在三个控制平面节点之间路由流量:
```
# generated by kind
global
  log /dev/log local0
  log /dev/log local1 notice
  daemon
defaults
  log global
  mode tcp
  option dontlognull
  # TODO: tune these
  timeout connect 5000
  timeout client 50000
  timeout server 50000
frontend control-plane
  bind *:6443
  default_backend kube-apiservers
backend kube-apiservers
  option httpchk GET /healthz
  # TODO: we should be verifying (!)
  server config2-control-plane 172.17.0.8:6443 check check-ssl verify none
  server config2-control-plane2 172.17.0.6:6443 check check-ssl verify none
  server config2-control-plane3 172.17.0.5:6443 check check-ssl verify none  
```
如前面的配置文件所示，有一个名为`kube-apiservers`的后端部分，包含三个控制平面容器。每个条目包含一个控制平面节点的 Docker IP 地址，端口分配为 6443，目标是容器中运行的 API 服务器。当您请求 [https://127.0.0.1:32791](https://127.0.0.1:32791) 时，该请求将命中 HAProxy 容器。使用 HAProxy 配置文件中的规则，请求将被路由到列表中的三个节点之一。
由于我们的集群现在由一个负载平衡器负责，因此我们有一个高度可用的控制平面进行测试。
注意
包含的羟基磷灰石映像不可配置。它仅用于处理控制平面和负载平衡应用编程接口服务器。由于这个限制，如果您需要为工作节点使用负载平衡器，您将需要提供自己的负载平衡器。
这方面的一个示例用例是，如果您想要在多个工作节点上使用一个入口控制器。您需要在工作节点前使用负载平衡器来接受传入的 80 和 443 请求，这些请求会将流量转发到运行 NGINX 的每个节点。在本章的最后，我们提供了一个示例配置，其中包括一个定制的 HAProxy 配置，用于对工作节点的流量进行负载平衡。
## 自定义控制平面和库布雷选项
你可能想更进一步测试一些特性，比如 OIDC 集成或者库伯内特特性门。KinD 使用与 kubeadm 安装相同的配置。例如，如果您想将集群与 OIDC 提供商集成，您可以将所需选项添加到配置修补程序部分:
```
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
kubeadmConfigPatches:
- |
  kind: ClusterConfiguration
  metadata:
    name: config
  apiServer:
    extraArgs:
      oidc-issuer-url: "https://oidc.testdomain.com/auth/idp/k8sIdp"
      oidc-client-id: "kubernetes"
      oidc-username-claim: sub
      oidc-client-id: kubernetes
      oidc-ca-file: /etc/oidc/ca.crt
nodes:
- role: control-plane
- role: control-plane
- role: control-plane
- role: worker
- role: worker
- rol: worker
```
有关可用配置选项的列表，请查看[网站上的*使用 Kubernetes 定制控制平面配置，网址为 https://Kubernetes . io/docs/setup/production-environment/tools/kubernetm/control-plane-flags/*](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/control-plane-flags/)。
现在已经创建了集群文件，可以创建自己的 KinD cl 集群了。
## 创建自定义 KinD 集群
终于！现在您已经熟悉了 KinD，我们可以继续前进，创建我们的集群。
我们需要创建一个受控的已知环境，因此我们将为集群命名，并提供我们在上一节中讨论过的配置文件。
确保您在`chapter4`目录下的克隆存储库中。
要用我们需要的选项创建一个 KinD 集群，我们需要用以下选项运行 KinD 安装程序:
```
kind create cluster --name cluster01 --config c
luster01-kind.yamlThe option --name will set the name of the cluster to cluster01 and the --config tells the installer to use the config file cluster01-kind.yaml.  
```
当您在主机上执行安装程序时，KinD 将开始安装，并告诉您正在执行的每个步骤。整个群集创建过程应该不到 2 分钟:
![Figure 4.6 – KinD cluster creation output ](img/Fig_4.6_B15514.jpg)
图 4.6–KinD 集群创建输出
部署的最后一步创建或编辑一个现有的 Kubernetes 配置文件。无论哪种情况，安装程序都会创建一个名为`kind-`的新上下文，并将其设置为默认上下文。
虽然看起来集群安装程序已经完成了它的任务，但是集群**还没有**准备好。有些任务需要几分钟才能完全初始化，由于我们禁用了默认的 CNI 来使用卡利科，我们仍然需要部署卡利科来提供集群网络工作。
## 安装印花棉布
为了向集群中的吊舱提供网络，我们需要安装一个容器网络接口，或者 CNI。我们选择安装卡利科作为我们的 CNI，由于 KinD 只包括 Kindnet CNI，我们需要手动安装卡利科。
如果您在创建步骤后暂停并查看集群，您会注意到一些单元处于挂起状态:
```
coredns-6955765f44-86l77  0/1  Pending  0  10m
coredns-6955765f44-bznjl  0/1  Pending  0  10m
local-path-provisioner-7  0/1  Pending  0  11m 745554f7f-jgmxv
```
这里列出的吊舱需要一个工作的 CNI 开始。这使吊舱进入挂起状态，等待网络。因为我们没有部署默认的 CNI，所以我们的集群不支持网络。为了让这些吊舱从挂起变为运行，我们需要安装一个 CNI——对于我们的集群，这将是卡利科。
要安装 Calico，我们将使用标准的 Calico 部署，它只需要一个清单。要开始部署 Calico，请使用以下命令:
```
kubectl apply -f https://docs.projectcalico.org/v3.11/manifests/calico.yaml 
```
这将从互联网上提取清单并将其应用于集群。随着它的部署，您将看到创建了许多 Kubernetes 对象:
![Figure 4.7 – Calico installation output ](img/Fig_4.7_B15514.jpg)
图 4.7–卡利科安装输出
安装过程大约需要一分钟，您可以使用`kubectl get pods -n kube-system`检查其状态。你会看到三个卡利科豆荚被创造出来。两个是`calico-node`吊舱，另一个是`calico-kube-controller`吊舱:
```
NAME                    READY STATUS RESTARTS AGE
calico-kube-controllers  1/1  Running    0    64s -5b644bc49c-nm5wn
calico-node-4dqnv        1/1  Running    0    64s
calico-node-vwbpf        1/1  Running    0    64s
```
如果您再次检查`kube-system`名称空间中的两个 CoreDNS 容器，您会注意到它们已经从挂起状态，从我们安装 Calico 之前的变为运行状态:
```
coredns-6955765f44-86l77   1/1  Running   0  18m
coredns-6955765f44-bznjl   1/1  Running   0  18m
```
现在集群已经安装了一个工作的 CNI，任何依赖网络的吊舱都将处于运行状态。
## 安装入口控制器
我们有一章专门介绍入口，解释所有的技术细节。由于我们正在部署一个集群，并且在未来的章节中需要入口，因此我们需要部署一个入口控制器来展示完整的集群构建。所有这些细节将在 [*第 6 章*](06.html#_idTextAnchor174)*服务、负载平衡和外部域名系统*中详细解释。
安装 NGINX 入口控制器只需要两个清单，我们将从互联网上获取清单，使安装变得容易。要安装控制器，请执行以下两行:
```
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.28.0/deploy/static/mandatory.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.27.0/deploy/static/provider/baremetal/service-nodeport.yaml
```
该部署将在名为`ingress-nginx`的命名空间中创建几个入口所需的 Kubernetes 对象:
![Figure 4.8 – NGINX installation output ](img/Fig_4.8_B15514.jpg)
图 4.8–NGINX 安装输出
我们还有一个步骤，这样我们就有了一个功能齐全的入口控制器:我们需要向运行的 pod 公开端口 80 和 443。这可以通过修补部署来完成。这里，我们包括了修补部署的补丁:
```
kubectl patch deployments -n ingress-nginx nginx-ingress-controller -p '{"spec":{"template":{"spec":{"containers":[{"name":"nginx-ingress-controller","ports":[{"containerPort":80,"hostPort":80},{"containerPort":443,"hostPort":443}]}]}}}}'
```
恭喜你！你现在有了一个运行卡利科的全功能双节点 Kubernetes 集群，它带有一个入口控制器。
# 查看您的 KinD 集群
现在有了一个 Kubernetes 集群，我们有能力直接观察 Kubernetes 对象。这将有助于您理解前一章，在这一章中，我们介绍了 Kubernetes 集群中包含的许多基本对象。特别是，我们将讨论 Ki nD 集群中包含的存储对象。
## 实物储存
记住【KinD 包括 Rancher 的自动资源调配器，为集群提供自动化的持久磁盘管理。在 [*第 5 章*](05.html#_idTextAnchor150)*Kubernetes boot camp*中，我们已经介绍了与存储相关的对象，现在我们已经配置了存储系统的集群，我们可以更详细地解释它们。
有一个对象是自动资源调配器不需要的，因为它使用了基本的 Kubernetes 特性:它不需要`CSIdriver`。由于使用本地主机路径作为 PVCs 的能力是 Kubernetes 的一部分，所以我们在我们的 KinD 集群中不会看到任何`CSIdriver`对象。
我们将要讨论的 KinD 集群中的第一个对象是我们的`CSInodes`。在 bootcamp 中，我们提到创建这个对象是为了从基节点对象中分离出任何 CSI 对象。任何可以运行工作负载的节点都将有一个`CSInode`对象。在我们的 KinD 集群中，两个节点都有一个`CSInode`对象。您可以通过执行`kubectl get csinodes`来验证:
```
NAME                      CREATED AT
cluster01-control-plane   2020-03-27T15:18:19Z
cluster01-worker          2020-03-27T15:19:01Z
```
如果我们使用`kubectl describe csinodes `来描述其中一个节点，您会看到对象的详细信息:
![Figure 4.9 – CSInode describe ](img/Fig_4.9_B15514.jpg)
图 4.9–CSInode 描述