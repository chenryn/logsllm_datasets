### Evidential Volume (EV) Calculation and Its Impact on System Safety Assessment

In this section, we define the ranking system for requirements: 100 represents a "V" ranking, and 10 represents an "I" ranking. This is an artificial example to illustrate how the EV changes when certain requirements are missing.

The final weights \( w_{ij} \) for the requirements are as follows:
- For all phases, \( w_{i} = \frac{7}{50} \approx 0.1429 \) for \( i = 1, \ldots, 7 \).

**Phase Weights:**
- **Phase P1:** \( w_{i,1} = 1 \)
- **Phase P2:** \( w_{i,2} = (0.4, 0.4, 0.04, 0.04, 0.04, 0.04, 0.04) \)
- **Phases P3, P4, P5, P6:** Same as Phase P2
- **Phase P7:** \( w_{i,7} = (0.625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625) \)
- **Phase P8:** Same as Phase P1

We have defined a set of indicator values \( I_{ij} \), where \( i = 1, \ldots, 7 \) and \( j = 1, \ldots, 8 \), which are initially all set to 1. Using the weights \( \alpha_j \) and \( w_{ij} \) as defined above, Equation (2) is used to compile \( I_{ij} \) and \( w_{ij} \) into \( EV_j \) for \( j = 1, \ldots, 8 \). Equation (3) is then used to compile \( EV_j \) and \( \alpha_j \) into the overall EV.

When all requirements are assumed to be met, i.e., \( I_{ij} = 1 \) for all \( i \) and \( j \), then \( EV_j = 1 \) for \( j = 1, \ldots, 8 \) and the overall EV is 1. Table 2 shows the results for the overall EV when a selection of requirements is not met, i.e., the evidence is not ticked off by the assessor and thus its corresponding indicator \( I_{ij} \) is set to 0.

| Type of Requirement | Quantity Missing (V, I) | EV |
|---------------------|-------------------------|----|
| I                   | 1                       | 0.99 |
| V                   | 1                       | 0.94 |
| I                   | 3                       | 0.91 |
| V                   | 1                       | 0.89 |
| V                   | 10                      | 0.88 |
| V                   | 2 (different phases)    | 0.81 |
| V                   | 2 (same phase)          | 0.81 |

From Table 2, we see that the neglect of one very important technique can lead to a 6% decrease in the overall EV. Using Table 1, this would result in a decrease in the probability of success (\( P_{\text{success}} \)) from the assumed 99% for a product with a complete set of certification evidence down to 78%, 70%, or even 62%, depending on the Safety Integrity Level (SIL) or Integrity Target (IT) to which the requirements pertain. The overall EV is influenced by the weight of missing evidence and its distribution; two "V" requirements missing from the same phase have more impact than two "V" requirements missing from different phases.

### Acceptable Values of \( P_{\text{success}} \)

There is no straightforward answer to what constitutes an acceptable value of \( P_{\text{success}} \). One approach is to create a utility function and calculate the utility or risk of accepting or rejecting the product based on the achieved \( P_{\text{success}} \).

Let:
- \( U_{\text{OK}}^{\text{accept}} \) be the utility of accepting a product that meets the safety integrity target.
- \( U_{\text{NO}}^{\text{accept}} \) be the (negative) utility of accepting an unsafe product.

The utility of deciding for acceptance is given by:
\[ U_{\text{utility}}[\text{accept}] = U_{\text{OK}}^{\text{accept}} \cdot P_{\text{success}} + U_{\text{NO}}^{\text{accept}} \cdot (1 - P_{\text{success}}) \]

Similarly, the utility of rejecting the product is:
\[ U_{\text{utility}}[\text{reject}] = U_{\text{OK}}^{\text{reject}} \cdot P_{\text{success}} + U_{\text{NO}}^{\text{reject}} \cdot (1 - P_{\text{success}}) \]
where:
- \( U_{\text{OK}}^{\text{reject}} \) is the cost of unnecessarily rejecting a product that meets the safety integrity target.
- \( U_{\text{NO}}^{\text{reject}} \) is the utility of rightfully rejecting an unsafe product.

Another approach is to use a database of past assessment outcomes to determine a suitable cut-off point. Scenarios where an acceptance decision was made and the product was successfully used in real-life can serve as benchmarks. The EV for these scenarios can be calculated and used as a guideline for what constitutes an acceptable EV.

### When EV is Not Sufficient

If the EV does not yield a high enough utility of acceptance, alternative evidence such as statistical testing can be considered. The confidence in the product's probability of failure on demand (pfd) based on the assessment, \( P_{\text{success}} \), can be used to build a prior belief distribution on the pfd, which can then be updated with the results of statistical testing.

### Summary

This paper describes a model to capture the degree of compliance of a product with a functional safety standard, measured through the evidential volume (EV). The model is illustrated using the IEC 61508 standard but can be applied to other standards. The approach has two main uses:

1. **Stand-alone Compliance Calculation:**
   - The approach can be used to calculate the degree of compliance with a standard, providing a comprehensive view of the available evidence. It supports decision-making by providing a quantitative measure of compliance, especially in cases where not all expected evidence is available.

2. **Modeling Confidence in Achieving Target pfd:**
   - The implementation of listed and applicable requirements does not guarantee a tolerable failure probability but indicates that reasonable activities to prevent intolerably high failure probabilities have been carried out. The model helps capture expert belief and translates the observed evidence into a measure of confidence in the pfd target.

### Outlook

The purpose of this paper is to introduce the EVA to the reliability community for discussion and feedback. A simple support tool is being developed to facilitate discussions and provide a basis for further validation. The tool will help identify strengths and limitations and provide useful feedback for future development. The long-term goal is to develop a user-friendly tool for calculating EV, which can be used by developers of safety-critical components to assess compliance at any stage of the development cycle.

### Acknowledgements

The authors thank the staff of British Energy and the UK Nuclear Installations Inspectorate for their valuable contributions. The information contained in this report is the joint property of British Energy Generation Ltd., British Energy Generation (UK) Ltd, British Nuclear Fuels Ltd., and their successor companies. It is to be held strictly in confidence unless otherwise specified in the contract.

### References

[1] "Functional safety of electrical/ electronic/ programmable electronic safety-related systems, International Electrotechnical Commission," 1998.
[2] L. Winsborrow and A. Lawrence, "Guidelines for using Programmable Electronic Systems in nuclear safety and nuclear safety-related applications," British Energy Generations Ltd., 2002.
[3] B.A. Gran and G. Dahl, "Estimating dependability of programmable systems using BBNs," Lecture Notes in Computer Science, Safecomp 2000, vol. 1943, pp. 309–320, 2000.
[4] S. Kuball, B. Guo, J. May, and G. Hughes, "Systematic safety assessment using Bayesian Belief Networks within the IEC 61508 lifecycle," Report for the Health and Safety Executive, UK under the SSRC Generic Programme, 2002.
[5] M. Li, C. Smidts, and R. W. Brill, "Ranking software engineering measures related to reliability using expert opinion," Proceedings of ISSRE 2000, 2000.
[6] H. Dehlinger, "Deontische Fragen, Urteilsbildung, Bewertungssysteme," Arbeitsbericht aus dem Fachgebiet Design Theorien and Methoden, vol. 7/94.
[7] Available from http://www.adelard.co.uk/software/asce.
[8] G. Salvendy, Handbook of Human Factors and Ergonomics. Wiley Interscience, 1997.
[9] C. Smidts and D. Sova, "An architectural model for software reliability quantification: sources of data," Reliability Engineering and System Safety, vol. 64, pp. 279–290, 1999.
[10] S. Kuball, G. Hughes, and J. May, "Prior construction II, certification evidence, operational use and testing," Deliverable D5 on British Energy Project PP/40030530, New Development of Dynamic Testing, 2002.