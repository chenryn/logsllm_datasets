l
a
m
e
F
+
Q
T
B
G
L
-
n
o
N
)
f
e
R
(
+
Q
T
B
G
L
4
2
-
8
1
4
4
-
5
2
)
f
e
R
(
+
5
4
y
l
i
a
D
y
l
k
e
e
W
y
l
h
t
n
o
M
19%***
15%**
19% 14% 20% 21% 20% 16% 20% 16% 15%
16% 15% 26% 24% 17% 10% 17% 13% 11%
15% 17% 14% 24% 21% 17% 12% 18% 13% 12%
8% 15% 12% 11%
12% 15% 12% 22% 20% 15%
9% 15% 13% 13%
8% 12% 10%
9%
11%
11%***
)
f
e
R
(
r
e
v
e
N
9%
6%
6%
6%
5%
6%
7%
4%
5%
6%
5%
5%
4%
5% 10% 10%
9%
5%
7%
9%
5% 13%
8%
4% 10%
8%
5% 12%
6%
7%
3%
4%
7%
6%
4%
6%
3%
4%
8%
3%
6%
2%
8%
3%
6%***
2%
5%
2%
4%
2%
3%
3%
1%
46% 49% 41% 60% 60% 53% 35% 50% 48% 43% 25%
39% 42% 34% 50% 50% 44% 31% 43% 38% 34% 21%
25% 20% 37% 34% 29% 15% 26% 26% 24% 11%
8%
7%
7%
6%
6%
5%
5%
5%
4%
4%
3%
3%
3%
3%
3%
2%
25%***
8%
6%
6%
6%
6%
5%
4%
4%
7%
6%
5%
5%
5%
5%
4%
5%
6%
6%
5%
4%
4%
4%
4%
5%
TABLE IV: Hate and harassment reported online across demographic subgroups. When calculating signiﬁcance, we compare all values against
a reference group (Ref). When reporting signiﬁcance, no asterisk indicates p = 0.01.
Fig. 2: Percentage of participants reporting any, moderate, or severe hate and harassment online per country, aggregated over 2016–2018.
in Section V.
D. Variations around the world
Race and ethnicity. For the United States, we found no
statistically signiﬁcant difference between the prevalence of
hate and harassment among White non-Hispanics (baseline),
Black non-Hispanics (p = 0.22), and Hispanic peoples (p =
0.31). Data and Society also found no signiﬁcant difference
among hate and harassment experiences across these same
ethnic groups, but did ﬁnd Black non-Hispanics were more
likely to report witnessing hate and harassment online [44].
Additionally, when participants were the target of harassment,
Pew found that Black non-Hispanics and Hispanics were more
likely to report the harassment was a result of their race or
ethnicity (25% of Black non-Hispanic adults, 10% of Hispanic
adults) compared to White non-Hispanics (just 3%) [118]. As
such, it is also important for solutions to take into account the
varied motivations for hate and harassment when designing
interventions.
We present a breakdown of the prevalence of hate and
harassment across the 22 countries we surveyed in Figure 2.
Participants from Kenya reported the highest prevalence of
harassment (72%), while participants from Japan reported the
lowest prevalence (20%). Our results match a previous ﬁnding
on hate and harassment in South Asia, where the prevalence
and severity of abuse was much greater than Western con-
texts [127]. When zooming in to severe issues, the relative
ranking of attacks was not constant across countries. In the
United Kingdom, physical threats were the most prevalent
(5%), compared to sustained harassment and bullying in
Ireland (6%), stalking in the United States (5%), or sexual
harassment in Brazil (11%)). These variations highlight the
need to tailor solutions to regional variations in hate and
harassment experiences. Additionally, solutions must account
for differing local interpretations of hate and harassment.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:30:40 UTC from IEEE Xplore.  Restrictions apply. 
256
KenyaNigeriaIndonesiaChinaMexicoVenezuelaSaudi ArabiaIndiaRussiaColombiaTurkeyBrazilSouth KoreaPolandSwedenIrelandUnited StatesGermanySpainFranceUnited KingdomJapan0%20%40%60%80%Percentage ofparticipantsAny abuseModerate abuseSevere abuseV. TOWARDS SOLUTIONS & INTERVENTIONS
We identify ﬁve directions for addressing online hate
and harassment
that either prevent or mitigate abuse. We
synthesized these directions from the technical interventions
identiﬁed during our literature review, existing approaches
taken by platform operators, and potential expansions of for-
proﬁt security, privacy, and anti-abuse defenses. For a given
solution, we examine which categories of abuse the solution
addresses, provide evidence of early success (where possible),
and suggest future directions for researchers to explore.
Solutions for hate and harassment face a unique combina-
tion of hurdles compared to other data-driven security defenses
and threat models. Attackers may have intimate access to a
target’s data, devices, and social connections, or even physical
access to their person. Likewise, a target’s risk exposure can
span multiple platforms (e.g., email, messaging, social media,
search results), the totality of which may be targeted for attack,
potentially by thousands of abusive parties. Risk can also
be highly dynamic, with a single post or video triggering a
deluge of hate and harassment, where previously the target
may have been low-risk with no mitigations in place. Lastly,
whereas abusive behaviors such as spam or malware have
clear policy distinctions and ﬁltering has broad support from
platform users, hate and harassment is ambiguously deﬁned,
making it difﬁcult to distinguish what behaviors cross the line.
A. Nudges, indicators, and warnings
Nudges and warnings provide valuable context
to both
abusers and targets about the risks of online hate and harass-
ment. Strategies here hinge on prevention. For toxic content,
a platform might prompt users with “Are you sure you want
to post this comment?” [7], [19]. Similarly, a platform might
warn abusers that posting a toxic comment will result
in
consequences, such as temporary disablement [19]. Bowler
et al., through a design session with teens and college-age
adults, synthesized such strategies into seven themes including
allowing for reﬂection, empathy, and empowerment [18], [19].
Chang et al. found that by temporarily blocking abusive
Wikipedia moderators to allow for reﬂection, 48% of users
avoided any future incidents (but 10% left the platform) [26].
Likewise, after Reddit closed several offensive subreddits,
researchers observed an 80% reduction in hate speech [23].
In terms of future directions, it remains to be determined
whether such nudges deter behavior among dedicated attackers
or throw-away accounts, and more generally to measure the
effectiveness of any newly developed nudges.
Nudges or warnings need not be isolated to platform de-
velopers. Mathew et al. investigated the use of counterspeech,
in which social network users countered hateful speech by
directly responding to abusers [107]. Community feedback like
this has previously been shown to shape user behavior [12],
[33], [39], but intervention by bystanders may never manifest
due to a belief that someone else will step in [48]. Difranzo et
al. found 75% of participants in a user study did not intervene
when they encountered another user being targeted by hate
and harassment [47]. The other 25% of participants opted to
ﬂag the activity as abusive rather than engaging in any form
of warning towards the abuser, or emotional support for the
target [47]. A recent survey by Pew found similar results,
where 70% of participants reported not intervening in any
way—including ﬂagging—after witnessing harassment [118].
Another challenge for community-based responses is that not
all harassment is visible to an online audience. Finally, the
subjective nature of hate and harassment may make instances
difﬁcult to identify, even when publicly visible [62].
lockout and control, and content
Indicators and warnings can also surface proactive security
advice. For example, two-factor authentication and security
checkups can stem the risk of unauthorized access—similar to
a for-proﬁt abuse context [52]—reducing the risk of surveil-
lance,
leakage. Ensuring
that visible notiﬁcations are always displayed whenever a
resource (e.g., camera, GPS sensor) is being actively accessed
can protect against covert access. Likewise, platforms can
send users reminders about their sharing settings for sensitive
content like location logs, photo backups, or delegated access
to their online account to raise awareness of potential ongoing
surveillance. Finally, indicators can also help to counteract
impersonation, with visible indicators of trust (e.g., conﬁrmed
proﬁles) or inﬂuence (e.g., number of connections). In terms of
future directions, research is needed to develop such indicators
and identify which ones are most effective in enabling the
rapid detection and prevention of harassment.
B. Human moderation, review, and delisting
The contextual nature of hate and harassment and lack of
current automated solutions necessitate the use of manual
review and moderation for both prevention and mitigation.
Moderation is not limited to toxic content: it can also help
address content leakage and impersonation via search delisting
and removal, and overloading by triaging notiﬁcation queues.
At present, moderation is most often done at a platform level
by human raters [58], [74].
We advocate for re-imagining the moderation ecosystem
to one that empowers users, communities, and platforms to
identify and act on hate and harassment. Such spheres of
control implicitly provide more context in order to tackle the
“gray areas” of hate and harassment. At a user level, this would
be as simple as “I do not want to see this content”, similar
to existing ﬂagging infrastructure. At a community level, the
owners of a page, channel, or forum would be equipped with
tools to set the tone and rules for user-generated content, and
to potentially receive ﬂag information from the community.
Similar strategies are already in place for Reddit [24] and gam-
ing platforms [104]. Such an approach enables communities
to establish their own norms and rules. Finally, platform-level
moderation would provide a baseline set of expectations for
all user-generated content.
A multitude of systems have explored how to design collab-
orative moderation and reporting tools. Project Callisto allows
victims of sexual assault to name their attacker, with the name
revealed only if another victim (or some threshold number of
victims) identify the same perpetrator [121]. Block Together
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:30:40 UTC from IEEE Xplore.  Restrictions apply. 
257
curates a crowd-sourced list of known abusive social network-
ing accounts that can be ﬁltered at a user level [14], [83].
HeartMob provided an interface to report hate and harassment
to bystanders for conﬁrmation and emotional support [13].
HateBase maintains a dictionary of hate speech across multiple
languages that others can then integrate with for moderation or
ﬁltering [71]. Squadbox provides a tool for family and friends
to step in and moderate toxic content on behalf of a target
to spread the emotional burden and time required to review
content [103]. Similarly, Kayes et al. explored strategies for
having users directly report incidents of online hate [89]. As
part of these design strategies, a common request from users
is feedback—both in terms of accuracy and outcomes—to
enable a sense of validation and meaningful results [7], [13].
In the absence of automated classiﬁers to produce moderation
queues, such systems must instead rely on trusted raters that
build a reputation over time as non-abusive users, in order to
prevent false reporting [148]. As a future direction, research is
needed to identify which tools would best enable community
moderators to perform ﬁltering or reporting. Alternatively,
bug bounty programs can reward participants who identify
applications that enable surveillance or lockout and control,
or even entirely new vectors of hate and harassment.
C. Automated detection and curation
Another key area for development is the automated detec-
tion of hate and harassment in order to scale enforcement to
billions of users and devices. Solutions in this space need not
implicitly result in automated decisions like removing a post or
suspending an account; instead, classiﬁer scores can feed into
moderation queues, content ranking algorithms, or warnings
and nudges. Numerous studies have explored how to design
classiﬁers to detect toxic content [32], [45], [49], [56], [77],
[113], [128], [140], [142], [146] as well as word embeddings to
identify toxic-adjacent content [25], [51], [113]. Other research
has explored identifying abusive users and accounts, rather
than individual instances of hate and harassment [28], [29],
[41], [57]. Another strategy relies on predicting the targets of
hate and harassment and at-risk users [31], [106]. With respect
to content leakage, Facebook has explored the possibility of
users providing hashes of non-consensual intimate images to
enable automated detection and removal [132]. Beyond text
and images, automated tools and reputation services can also
play a role in detecting false reporting, surveillance, and
lockout and control. Similar to a for-proﬁt abuse context,
future directions might include classiﬁers to identify instances
of account or device takeover, or suspicious activity on an
account or device.
All of the aforementioned strategies struggle with obtaining
representative datasets of abusive content for training. Existing
datasets of toxic content originate via crowdsourced labels
of Wikipedia and news comments [84]; user-reported ﬂags
of harassment in gaming communities [11], [104]; content
containing blacklisted keywords [67]; content that carries a
negative sentiment score [62]; or content posted by suspended
accounts (which may conﬂate various types of online abuse
rather than solely harassment) [34]. Unlike a for-proﬁt abuse
context, bias in training data can result in classiﬁers incorrectly
learning that terms for at-risk populations like “gay” or “black”
are by default hate and harassment [5], [50]. Complexity
here also stems from the fact
interpretations of hate
and harassment vary across cultural contexts [97], [98] or
even between the personal history of different targets [66].
Constructing unbiased and representative datasets—that either
generalize or are tailored to users, communities, platforms, or
regions—remains a core challenge for tackling online hate and
harassment.
that
D. Conscious design
Designing platforms to combat hate and harassment also
means consciously considering how systems and user inter-
faces can shape the nature of discourse in online spaces. The
Anti-Defamation League has shown that experiences of hate
and harassment can vary wildly by platform [4], potentially
due to the communities, enforcement techniques, or design
decisions involved. A fruitful area for future research may
be an exploration of which design features seem to foster
hate and harassment. Examples of conscious design that have
recently garnered interest
include whether social networks
should have a “retweet” function or the ability to “subquote”
other users [86]. Related considerations include how widely
messages should be allowed to spread in WhatsApp [88], or
whether users should have to reach a certain level of com-
munity trust—for example, subscribers on YouTube [143]—
before being allowed to monetize content.
Potential design solutions for future exploration include
providing targets with tools to control their audience, thus
avoiding exposure to hostile parties and toxic content. Sim-
ilarly, platforms might disallow sensitive material from being
forwarded, preventing content leakage. Other examples include
not taking automated action on user ﬂags, or allowing people
to pre-register as high risk targets to avoid false reporting, sim-
ilar to anti-SWATing measures [10]. Technical measures such
as cryptographic authentication on the origin of messages can
also prevent spooﬁng and thus some forms of impersonation.
Design concepts from the privacy community can also
protect users from surveillance or lockout and control. For
example, delegated access to a user’s sensitive information
(e.g., location, photos) might expire without that user’s explicit
re-approval. This mirrors recent strategies such as automati-
cally deleting a user’s location history after a set period [117].
Likewise, in the event of account takeover, sensitive actions
such as exporting all of a user’s personal emails might require