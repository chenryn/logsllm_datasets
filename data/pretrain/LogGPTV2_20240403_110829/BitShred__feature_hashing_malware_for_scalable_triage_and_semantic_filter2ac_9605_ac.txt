putational efﬁciency that comes from feature hashing.
Figure 4a shows a matrix before co-clustering where each row is
a malware ﬁngerprint and each column is a particular feature. Fig-
ure 4b and Figure 4c graphically depict the results of co-clustering
on 8 different kinds of Trojan and 3 different kinds of Adware,
Trojan.KillAV-­‐235Trojan.Downloader-­‐71002Trojan.VB-­‐5716Trojan.Downloader-­‐57401Trojan.Downloader-­‐10763Trojan.Popuper-­‐4Trojan.Crypt-­‐215Trojan.Spy-­‐66720Trojan.KillAV-­‐235Trojan.Downloader-­‐71002Trojan.VB-­‐5716Trojan.Downloader-­‐57401Trojan.Downloader-­‐10763Trojan.Popuper-­‐4Trojan.Crypt-­‐215Trojan.Spy-­‐66720Adware.Trymedia-­‐6Adware.Downloader-­‐92Adware.Zango-­‐13131. MAP: (cid:104)Ki, si(cid:105) list → (cid:104)Ki, fi(cid:105) list. Each MAP task is as-
signed the subset of malware samples si and creates ﬁnger-
prints fi to be stored on HDFS. Fingerprint ﬁles are named
as Ki representing the index to the corresponding malware
samples.
all Hadoop nodes by deﬁning the following functions:
2. REDUCE. In this step, no REDUCE step is needed.
In phase 2, distributed BitShred runs BITSHRED-JACCARD across
1. MAP: (cid:104)Ki, fi(cid:105) list → (cid:104)R, (sa, sb)(cid:105) list MAP tasks read ﬁn-
gerprint data ﬁles created during phase 1 and runs BITSHRED-
JACCARD on each ﬁngerprint pair, outputting the similarity
d ∈ R.
2. REDUCE: (cid:104)R, (sa, sb)(cid:105) list → sorted (cid:104)R, (sa, sb)(cid:105) list RE-
DUCE gathers the list of the similarity values for each pair
and returns a sorted list of pairs based upon similarity.
This phase returns a sorted list of malware pairs by similarity
using standard Hadoop sorting. The sorted list is essential for the
agglomerative single linkage clustering. In particular, malware si’s
family is deﬁned as the set of malware whose distance is less than
θ, thus all malware in the sorted list with similarity > θ are in the
cluster.
4.
IMPLEMENTATION
We have implemented single-node BitShred in 2000 lines of C
code. Since BitShred is agnostic to the particular per-malware anal-
ysis methods, we only need individualized routines for extracting
raw input features, before converting into ﬁngerprints. In case of
static code analysis, BitShred divides executable code section iden-
tiﬁed by GNU BFD library into n-grams and hashes each n-gram to
create ﬁngerprints. For dynamic behavior analysis, BitShred sim-
ply parses input behavior proﬁle logs and hashes every behavior
proﬁle to generate ﬁngerprints. We use berkeley DB to store
and manage ﬁngerprints database. After building the database, Bit-
Shred retrieves ﬁngerprints from the database to calculate the Jac-
card similarity between ﬁngerprints. After applying an agglomera-
tive hierarchical clustering algorithm, malware families are formed.
We use graphviz and Cluto [24] for visualizing the clustering and
family trees generated as shown in Figure 10, 11.
Distributed BitShred is implemented in 500 lines of Java code.
We implemented a parser for extracting section information from
Portable Executable header information because there is no BFD
library for Java. In our implementation, we perform a further op-
timization that groups several ﬁngerprints into a single HDFS disk
block in order to optimize I/O. In the Hadoop infrastructure we use,
the HDFS block size is 64MB. We optimize for this block size by
dividing the input malware set so each node works on 2,048 mal-
ware samples at a time because 64MB = 32KB × 2048. That is,
each MAP task is given 2,048 samples (si, si+1,··· si+2047) and
generates a single ﬁle containing all ﬁngerprints. We can simi-
larly optimize for other block sizes and different bit-vector lengths,
e.g, 64KB bit vectors result in batching 1,024 malware samples per
node.
Distributed co-clustering is implemented in 1200 lines of Java
code. We implemented a python-wrapper to iterate row and column
operations to ﬁnd an optimal co-clustering.
5. EVALUATION
We have evaluated BitShred for speed and accuracy using two
types of per-sample analysis for features. First, we use a static
code reuse detection approach where features are code fragments,
and two malware are considered similar if they share common code
fragments. Second, we use a dynamic analysis feature set where
features are displayed behaviors, and two malware are considered
similar if they have exhibit similar behaviors. Note that similarity
is a set comparison, so order does not matter (e.g., re-ordering basic
blocks is unlikely to affect the results). We stress that we are not
advocating a particular approach such as static or dynamic analy-
sis, but instead demonstrating how BitShred could be used once an
analysis was selected.
Equipment. All single-node experiments were performed on a
Linux 2.6.32-23 machine (Intel Core2 6600 / 4GB memory) using
only a single core. The distributed experiments were performed on
a Hadoop using 64 worker nodes, each with 8 cores, 16 GB DRAM,
4 1TB disks and 10GbE connectivity between nodes [2]. 53 nodes
had a 2.83GhZ E5440 processor, and 11 had a 3GhZ E5450 pro-
cessor. Each node is conﬁgured to allow up to 6 map tasks and up
to 4 reduce tasks at one time.
Malware Dataset. We performed our experiments on a malware
data set collected from a variety of open repositories such as Mal-
ware Analysis System (aka CWSandbox) [3], Offsensive Comput-
ing [4], and from our Universities infrastructure-wide security in-
frastructure inbetween 2009-2010. Our total data set consists of
655,360 unique samples by MD5 hash.
5.1 BitShred with Code Reuse as Features
Setup. Our static experiments are based upon reports that malware
authors reuse code as they invent new malware samples [9, 25, 38].
Since malware is traditionally a binary-level analysis, not a source
analysis, our implementation uses n-grams to represent binary code
fragments. Malware similarity is determined by the percentage of
n-grams shared.
We chose n-grams based analysis because it is one previously
proposed approach that demonstrates a high dimensionality feature
space. We set n = 16, so there are 2128 possible n-gram features.
We chose 16 based upon experiments that show it would cover at
least a few instructions (not shown for space reasons). We can use
other features such as basic blocks, etc. as well by ﬁrst building the
appropriate feature and then deﬁning a hash function on it; all pos-
sible extensions of the per-sample analysis is out of scope for this
work. Surprisingly, even this simple analysis had over 90% accu-
racy when the malware is unpacked using off-the-shelf unpackers.
Pragmatically, n-gram analysis also has the advantage of not re-
quiring disassembling, building a control ﬂow graph, etc., all of
which are known hard problems on malware.
Single Node Performance. Table 1 shows BitShred’s performance
using a single node in terms of speed, memory consumed, and the
resulting error rate. We limited our experiment to clustering 1,000
malware samples (which requires 499,500 pairwise comparisons)
in order to keep the exact Jaccard time reasonable. The “exact Jac-
card” row shows the overall performance when computing the set
operations as shown in Equation 1 using the SimMetrics library [5].
Clustering using exact Jaccard took more than 4 hours, and required
644.13MB of memory. This works out to about 33 malware com-
parisons/sec and 2,388 malware clustered per day.
We performed two performance measurements with BitShred:
one with 32KB ﬁngerprints and one with 64KB ﬁngerprints. With
64KB ﬁngerprints, BitShred ran about 317 times faster than exact
Jaccard. With 32KB ﬁngerprints, BitShred runs about 2 times faster
compared to 64KB ﬁngerprints, and about 631 times faster than
exact Jaccard.
Since BitShred uses feature hashing, hash collisions may impact
the accuracy of the Jaccard distance computations. The overall er-
ror rate in the distance computations is a function of the ﬁngerprint
length, the size of the feature space, and the percentage of code
that is similar. The statement in Theorem 1 formally expresses this
tradeoff. We also made two empirical measurements. First, we
computed the average error on all pairs, which worked out to be
about 2% with 64KB ﬁngerprints and 4% with 32KB ﬁngerprints.
314Size of
ﬁngerprints
Time to compare
every pair
Average error on
all pairs
Average
error on
similar (>0.5) pairs
Malware compar-
isons per second
Malware
tered per day
clus-
EXACT JACCARD
BS64K
BS32K
WINNOW (W4)
WINNOW (W12)
BS32K (W4)
BS32K (W12)
BS8K (W4)
BS8K (W12)
644.13MB
62.50MB
31.25MB
66.97MB
30.16MB
31.25MB
31.25MB
7.81MB
7.81MB
4h 12m 16s
48s
24s
41m 5s
20m 35s
24s
24s
6s
6s
-
0.0199
0.0403
0.0019
0.0081
0.0159
0.0062
0.0649
0.0247
-
0.0017
0.0050
0.0109
0.0128
0.0009
0.0039
0.0086
0.0016
33
10,472
20,812
203
404
20,812
20,812
78,047
78,047
2,388
42,538
59,970
5,918
8,360
59,970
59,970
116,131
116,131
Table 1: BitShred (BS) vs. Jaccard vs. Winnowing. We show BitShred with several different ﬁngerprint sizes.
The error goes up as the ﬁngerprint size shrinks because there is a
higher chance of collisions. We also computed the average error
on pairs with a similarity of at least 50%, and found the error to
be less than 1% of the true Jaccard. Note that the second metric
(i.e., average error on pairs with higher similarity), is the more im-
portant metric – these are the numbers with the most impact on the
accuracy, as these are the numbers that will primarily decide which
family a malware sample belongs to. Thus, BitShred is a very close
approximation indeed.
BitShred vs. Winnowing. In this paper so far we have consid-
ered techniques that provide an exact ranking between all pairs of
malware. Nonetheless, malware practitioners are constantly facing
hard choices on how much time to spend given ﬁnite computing
resources, thus may want faster but approximate over theoretically
correct but slower clustering. LSH is one type of data reduction
technique that improves performance. Here we discuss another
called Winnowing.
Winnowing, the algorithm used by the MOSS plagiarism detec-
tion tool, is a fuzzing hashing technique that selects a subset of
features from a sample for analysis [34]. Let w be a window mea-
sured in some way, e.g., w statements, w consecutive n-grams, w
behaviors, etc. Winnowing guarantees at least one shared unit in
any window of length at least w + n − 1 will be included in the
feature set [34]. In our evaluation we measure Winnowing because
a) MOSS is well-known, and b) it corresponds to similarity detec-
tion based upon code as proposed in previous work [9, 25, 38],
thus is directly related to our approach, and c) it is guaranteed to be
within 33% of an upper bound on performance algorithms for sim-
ilarity detection [34]. We compared in two settings: BitShred vs.
Winnowing as in previous work, and BitShred extended to include
Winnowing. Table 1 also shows these results for window sizes 4
(denoted as W4) and 12 (denoted as W12).
BitShred beats straight Winnowing. We reimplemented Win-
nowing as detailed in [34] using a 32-bit hash function as the orig-
inal implementation is not public. For the purpose of performance
comparison, we computed the similarity using SimMetrics library.
BitShred is anywhere from 26-102 times faster, while requiring less
memory. Winnowing does have a slightly better error rate, though
none of the error rates is very high. A more interesting case is to
consider pre-processing the feature set with Winnowing and then
applying BitShred. With Winnowing applied, we can reduce the
BitShred ﬁngerprint size down to 8KB, allowing all 1,000 samples
to be clustered in 6 seconds.
Figure 5 relates all experiments with respect to the total num-
ber of malware clustered per day. Recall there are about 8,000 new
malware found per day. BitShred deals easily with current volumes,
and has room to spare for future growth. Figure 5 also shows on
the right-hand y-axis one reason BitShred is faster. Recall we men-
tioned exact Jaccard computations are slow in part because they
Figure 5: Overall malware clustered-per-day capabilities. We also
report relative L1/L2 cache misses.
use set operations. These, in turn, are not efﬁcient on real archi-
tectures. BitShred’s bitvector ﬁngerprints, on the other hand, are
L1/L2 cache friendly.
Distributed BitShred. We have implemented the Hadoop version
of BitShred, and performed several experiments to measure overall
scalability and throughput. We use up to 655,360 samples in this
experiment. Note all samples were unpacked as the goal of this
experiment is to measure overall performance and not accuracy.
Figure 6 shows the BITSHRED-GEN ﬁngerprint generation time.
In this experiment, we utilized 80 map tasks for small datasets
(20,480 ∼ 81,920) and 320 map tasks for large datasets (163,840
∼ 655,360). The total time to create ﬁngerprints for all samples
was 5m 45s with BS8K (W12) and 4m 40s with BS32K (W1).
The graph also shows a linear trend in the ﬁngerprint generation
time, e.g., halving the total number of samples to 327,680 samples
approximately halves the generation time to about 2m 54s and 2m
25s, respectively. BITSHRED-GEN performance slightly dropped
at 163,840 samples because the startup and shutdown overhead of
each map dominates the beneﬁt of utilizing more maps.
Figure 7 shows the amount of time for computing the pairwise
distance for the same sample set. We utilized 200 map tasks for
small datasets and 320 map tasks for large datasets. Given the
values in the graph, we can work out the number of comparisons
per second. For example, 163,840 samples requires approximately
1.3 × 1010 comparisons, and takes 10m 15s with BS8K (W12),
which works out to 21,823,888 comparisons/sec. 327,680 samples
requires about 5.4 × 1010 comparisons, and takes 40m 55s with
BS8K (W12), which works out to a similar 21,868,402 compar-
isons/sec.
 0 20000 40000 60000 80000 100000 120000ExactJaccardWinnowing(W12)BitShred32Kw/oWinnowBitShred8Kw/Winnow 10000 100000 1e+06 1e+07 1e+08Number of malware clustered per dayL2 Cache Misses2,3888,36059,970116,131315Figure 6: Performance of Distributed BITSHRED-GEN
Figure 8: Precision and Recall (3,935 samples)
Figure 7: Performance of Distributed BITSHRED-JACCARD
Figure 9: Precision and Recall (131,072 samples)
Overall, the distributed version achieved a pairwise comparison
throughput of about 1.9 × 1012 per day. This works out to full