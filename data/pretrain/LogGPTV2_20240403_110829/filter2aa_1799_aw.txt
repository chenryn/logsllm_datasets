shared between 32-bit and 64-bit apps (in this case, the subkey is a shared key). Under each of the split 
keys (in the position tracked by an anchor node), WoW64 creates a key called WoW6432Node (for x86 
application) or WowAA32Node
information. All other portions of the registry are shared between 32-bit and 64-bit applications (for 
-
rection and layout explained earlier. The 32-bit application must write exactly these strings using this 
case—any other data will be ignored and written normally. 
RegOpenKeyEx, RegCreateKeyEx, RegOpenKeyTransacted, RegCreateKeyTransacted, and 
RegDeleteKeyEx functions permit this:
I 
KEY_WoW64_64KEY Explicitly opens a 64-bit key from either a 32-bit or 64-bit application
I 
KEY_WoW64_32KEY Explicitly opens a 32-bit key from either a 32-bit or 64-bit application
X86 simulation on AMD64 platforms
The interface of the x86 simulator for AMD64 platforms (Wow64cpu.dll) is pretty simple. The simulator 
process initialization function enables the fast system call interface, depending on the presence of soft-
ware MBEC (Mode Based Execute Control is discussed in Chapter 9). When the WoW64 core starts the 
simulation by invoking the BtCpuSimulate
frame (based on the 32-bit CPU context provided by the WoW64 core), initializes the Turbo thunks 
112 
CHAPTER 8 System mechanisms
set to the 32-bit version of the LdrInitializeThunk loader function). When the CPU executes the far jump, 
it detects that the call gate targets a 32-bit segment, thus it changes the CPU execution mode to 32-bit. 
The code execution exits 32-bit mode only in case of an interrupt or a system call being dispatched. 
More details about call gates are available in the Intel and AMD software development manuals.
Note 
to be initialized.
System calls
DLLs that perform interprocess communication, such as Rpcrt4.dll). When a 32-bit application requires 
assistance from the OS, it invokes functions located in the special 32-bit versions of the OS libraries. 
Like their 64-bit counterparts, the OS routines can perform their job directly in user mode, or they can 
require assistance from the NT kernel. In the latter case, they invoke system calls through stub func-
tions like the one implemented in the regular 64-bit Ntdll. The stub places the system call index into a 
register, but, instead of issuing the native 32-bit system call instruction, it invokes the WoW64 system 
call dispatcher (through the Wow64Transition variable compiled by the WoW64 core).
It emits another far jump for transitioning to the native 64-bit execution mode, exiting from the simula-
captures the parameters associated with the system call and converts them. The conversion process is 
called “thunking” and allows machine code executed following the 32-bit ABI to interoperate with 64-bit 
values are passed in parameters of each function and accessed through the machine code. 
complex data structures provided by the client (but deal with simple input and output values), the 
Turbo thunks (small conversion routines implemented in the simulator) take care of the conversion and 
directly invoke the native 64-bit API. Other complex APIs need the Wow64SystemServiceEx
assistance, which extracts the correct WoW64 system call table number from the system call index and 
invokes the correct WoW64 system call function. WoW64 system calls are implemented in the WoW64 
core library and in Wow64win.dll and have the same name as the native system calls but with the 
wh-NtCreateFile WoW64 API is called whNtCreateFile.)
After the conversion has been correctly performed, the simulator issues the corresponding na-
tive 64-bit system call. When the native system call returns, WoW64 converts (or thunks) any output 
parameters if necessary, from 64-bit to 32-bit formats, and restarts the simulation.
CHAPTER 8 System mechanisms
113
Exception dispatching
Similar to WoW64 system calls, exception dispatching forces the CPU simulation to exit. When an ex-
ception happens, the NT kernel determines whether it has been generated by a thread executing user-
mode code. If so, the NT kernel builds an extended exception frame on the active stack and dispatches 
the exception by returning to the user-mode KiUserExceptionDispatcher function in the 64-bit Ntdll (for 
more information about exceptions, refer to the “Exception dispatching” section earlier in this chapter).
Note that a 64-bit exception frame (which includes the captured CPU context) is allocated 
in the 32-bit stack that was currently active when the exception was generated. Thus, it needs 
to be converted before being dispatched to the CPU simulator. This is exactly the role of the 
Wow64PrepareForException function (exported by the WoW64 core library), which allocates space on 
the native 64-bit stack and copies the native exception frame from the 32-bit stack in it. It then switches 
to the 64-bit stack and converts both the native exception and context records to their relative 32-bit 
counterpart, storing the result on the 32-bit stack (replacing the 64-bit exception frame). At this point, 
the WoW64 Core can restart the simulation from the 32-bit version of the KiUserExceptionDispatcher 
function, which dispatches the exception in the same way the native 32-bit Ntdll would.
32-bit user-mode APC delivery follows a similar implementation. A regular user-mode APC is
KiUserApcDispatcher. When the 64-bit kernel is about to dispatch 
a user-mode APC to a WoW64 process, it maps the 32-bit APC address to a higher range of 64-bit ad-
dress space. The 64-bit Ntdll then invokes the Wow64ApcRoutine routine exported by the WoW64 core 
library, which captures the native APC and context record in user mode and maps it back in the 32-bit 
stack. It then prepares a 32-bit user-mode APC and context record and restarts the CPU simulation 
from the 32-bit version of the KiUserApcDispatcher function, which dispatches the APC the same way 
the native 32-bit Ntdll would.
ARM
ARM is a family of Reduced Instruction Set Computing (RISC) architectures originally designed by 
result, there have been multiple releases and versions of the ARM architecture, which have quickly 
evolved during the years, starting from very simple 32-bit CPUs, initially brought by the ARMv3 genera-
tion in the year 1993, up to the latest ARMv8. The, latest ARM64v8.2 CPUs natively support multiple 
execution modes (or states), most commonly AArch32, Thumb-2, and AArch64:
I 
AArch32 is the most classical execution mode, where the CPU executes 32-bit code only and
transfers data to and from the main memory through a 32-bit bus using 32-bit registers.
I 
Thumb-2 is an execution state that is a subset of the AArch32 mode. The Thumb instruction set has 
been designed for improving code density in low-power embedded systems. In this mode, the CPU 
can execute a mix of 16-bit and 32-bit instructions, while still accessing 32-bit registers and memory.
I
AArch64 is the modern execution mode. The CPU in this execution state has access to 64-bit
general purpose registers and can transfer data to and from the main memory through a
64-bit bus.
114 
CHAPTER 8 System mechanisms
Windows 10 for ARM64 systems can operate in the AArch64 or Thumb-2 execution mode (AArch32 
is generally not used). Thumb-2 was especially used in old Windows RT systems. The current state of 
discussed more in depth in Chapter 9 and in the ARM Architecture Reference Manual.
Memory models
In the “Hardware side-channel vulnerabilities” earlier in this chapter, we introduced the concept of 
observed while accessed by multiple processors (MESI is one of the most famous cache coherency 
protocols). Like the cache coherency protocol, modern CPUs also should provide a memory consis-
tency (or ordering) model for solving another problem that can arise in multiprocessor environments: 
memory reordering. Some architectures (ARM64 is an example) are indeed free to re-order memory 
access instructions (achieving better performance while accessing the slower memory bus). This kind 
of architecture follows a weak memory model, unlike the AMD64 architecture, which follows a strong 
memory model, in which memory access instructions are generally executed in program order. Weak 
lot of synchronization issues when developing multiprocessor software. In contrast, a strong model is 
more intuitive and stable, but it has the big drawback of being slower.
CPUs that can do memory reordering (following the weak model) provide some machine instruc-
tions that act as memory barriers. A barrier prevents the processor from reordering memory accesses 
before and after the barrier, helping multiprocessors synchronization issues. Memory barriers are slow; 
thus, they are used only when strictly needed by critical multiprocessor code in Windows, especially in 
synchronization primitives (like spinlocks, mutexes, pushlocks, and so on). 
As we describe in the next section, the ARM64 jitter always makes use of memory barriers while 
execute could be run by multiple threads in parallel at the same time (and thus have potential synchro-
nization issues. X86 follows a strong memory model, so it does not have the reordering issue, a part of 
generic out-of-order execution as explained in the previous section).
Note Other than the CPU, memory reordering can also affect the compiler, which, during 
compilation time, can reorder (and possibly remove) memory references in the source 
compiler reordering, 
whereas the type described in the previous section is processor reordering.
CHAPTER 8 System mechanisms
115
ARM32 simulation on ARM64 platforms
The simulation of ARM32 applications under ARM64 is performed in a very similar way as for x86 under 
AMD64. As discussed in the previous section, an ARM64v8 CPU is capable of dynamic switching between 
the AArch64 and Thumb-2 execution state (so it can execute 32-bit instructions directly in hardware). 
-
struction, so the WoW64 layer needs to invoke the NT kernel to request the execution mode switch. To do 
this, the BtCpuSimulate function, exported by the ARM-on-ARM64 CPU simulator (Wowarmhw.dll), saves 
the nonvolatile AArch64 registers in the 64-bit stack, restores the 32-bit context stored in WoW64 CPU 
The NT kernel exception handler (which, on ARM64, is the same as the syscall handler), detects that 
the exception has been raised due to a system call, thus it checks the syscall number. In case the num-
ber is the special –1, the NT kernel knows that the request is due to an execution mode change coming 
from WoW64. In that case, it invokes the KiEnter32BitMode routine, which sets the new execution state 
for the lower EL (exception level) to AArch32, dismisses the exception, and returns to user mode. 
The code starts the execution in AArch32 state. Like the x86 simulator for AMD64 systems, the execu-
tion controls return to the simulator only in case an exception is raised or a system call is invoked. Both 
exceptions and system calls are dispatched in an identical way as for the x86 simulator under AMD64.
X86 simulation on ARM64 platforms
The x86-on-ARM64 CPU simulator (Xtajit.dll) is different from other binary translators described in the 
previous sections, mostly because it cannot directly execute x86 instructions using the hardware. The 
ARM64 processor is simply not able to understand any x86 instruction. Thus, the x86-on-ARM simula-
tor implements a full x86 emulator and a jitter, which can translate blocks of x86 opcodes in AArch64 
code and execute the translated blocks directly. 
When the simulator process initialization function (BtCpuProcessInit) is invoked for a new WoW64 pro-
compatibility database.) The simulator then allocates and compiles the Syscall page, which, as the name 
implies, is used for emitting x86 syscalls (the page is then linked to Ntdll thanks to the Wow64Transition 
variable). At this point, the simulator determines whether the process can use the XTA cache. 
The simulator uses two different caches for storing precompiled code blocks: The internal cache is 
allocated per-thread and contains code blocks generated by the simulator while compiling x86 code 
executed by the thread (those code blocks are called jitted blocks); the external XTA cache is managed 
by the XtaCache service and contains all the jitted blocks generated lazily for an x86 image by the 
later in this chapter.) The process initialization routine allocates also the Compile Hybrid Executable 
(CHPE) bitmap, which covers the entire 4-GB address space potentially used by a 32-bit process. The 
bitmap uses a single bit to indicate that a page of memory contains CHPE code (CHPE is described later 
in this chapter.)
116 
CHAPTER 8 System mechanisms
The simulator thread initialization routine (BtCpuThreadInit) initializes the compiler and allocates 
the per-thread CPU state on the native stack, an important data structure that contains the per-thread 
compiler state, including the x86 thread context, the x86 code emitter state, the internal code cache, 
Simulator’s image load notification 
Unlike any other binary translator, the x86-on-ARM64 CPU simulator must be informed any time a new 
image is mapped in the process address space, including for the CHPE Ntdll. This is achieved thanks to 
the WoW64 core, which intercepts when the NtMapViewOfSection native API is called from the 32-bit 
code and informs the Xtajit simulator through the exported BTCpuNotifyMapViewOfSection routine. It 
data, such as
I 
The CHPE bitmap (which needs to be updated by setting bits to 1 when the target image
contains CHPE code pages)
I 
I 
The XTA cache state for the image
In particular, whenever a new x86 or CHPE image is loaded, the simulator determines whether it 
should use the XTA cache for the module (through registry and application compatibility shim.) In case 
the check succeeded, the simulator updates the global per-process XTA cache state by requesting to 
the XtaCache service the updated cache for the image. In case the XtaCache service is able to identify 
used to speed up the execution of the image. (The section contains precompiled ARM64 code blocks.)
Compiled Hybrid Portable Executables (CHPE)
enough performance to maintain the application responsiveness. One of the major issues is tied to the 
memory ordering differences between the two architectures. The x86 emulator does not know how 
the original x86 code has been designed, so it is obliged to aggressively use memory barriers between 
each memory access made by the x86 image. Executing memory barriers is a slow operation. On aver-
These are the motivations behind the design of Compiled Hybrid Portable Executables (CHPE). A 
CHPE binary is a special hybrid executable that contains both x86 and ARM64-compatible code, which 
has been generated with full awareness of the original source code (the compiler knew exactly where 
to use memory barriers). The ARM64-compatible machine code is called hybrid (or CHPE) code: it is 
still executed in AArch64 mode but is generated following the 32-bit ABI for a better interoperability 
with x86 code.
CHPE binaries are created as standard x86 executables (the machine ID is still 014C as for x86); the 
main difference is that they include hybrid code, described by a table in the Hybrid Image metadata 
CHAPTER 8 System mechanisms
117
page containing hybrid code described by the Hybrid metadata. When the jitter compiles the x86 code 
block and detects that the code is trying to invoke a hybrid function, it directly executes it (using the 
32-bit stack), without wasting any time in any compilation.
The jitted x86 code is executed following a custom ABI, which means that there is a nonstandard
convention on how the ARM64 registers are used and how parameters are passed between functions. 
CHPE code does not follow the same register conventions as jitted code (although hybrid code still 
follows a 32-bit ABI). This means that directly invoking CHPE code from the jitted blocks built by the 
compiler is not directly possible. To overcome this problem, CHPE binaries also include three different 
kinds of thunk functions, which allow the interoperability of CHPE with x86 code:
I 
A pop thunk allows x86 code to invoke a hybrid function by converting incoming (or outgo-
ing) arguments from the guest (x86) caller to the CHPE convention and by directly transferring
execution to the hybrid code.
I 
A push thunk allows CHPE code to invoke an x86 routine by converting incoming (or outgoing)
arguments from the hybrid code to the guest (x86) convention and by calling the emulator to
resume execution on the x86 code.
I 
An export thunk is a compatibility thunk created for supporting applications that detour x86
exported from CHPE modules still contain a little amount of x86 code (usually 8 bytes), which
semantically does not provide any sort of functionality but allows detours to be inserted by the
external application.
The x86-on-ARM simulator makes the best effort to always load CHPE system binaries instead of stan-
dard x86 ones, but this is not always possible. In case a CHPE binary does not exist, the simulator will load 
the standard x86 one from the SysWow64 folder. In this case, the OS module will be jitted entirely.
EXPERIMENT: Dumping the hybrid code address range table
of a CHPE image. More information about the tool and how to install it are available in Chapter 9.
In this experiment, you will dump the hybrid metadata of kernelbase.dll, a system library that 
also has been compiled with CHPE support. You also can try the experiment with other CHPE 
-
cd c:\Windows\SyChpe32 
link /dump /loadconfig kernelbase.dll > kernelbase_loadconfig.txt
EXPERIMENT: Dumping the hybrid code address range table
of a CHPE image. More information about the tool and how to install it are available in Chapter 9.
In this experiment, you will dump the hybrid metadata of kernelbase.dll, a system library that 
also has been compiled with CHPE support. You also can try the experiment with other CHPE 
-
cd c:\Windows\SyChpe32
link /dump /loadconfig kernelbase.dll > kernelbase_loadconfig.txt
118 
CHAPTER 8 System mechanisms
-
with Notepad and scroll down until you reach the following text:
Section contains the following hybrid metadata: 
4 Version 
102D900C Address of WowA64 exception handler function pointer 
102D9000 Address of WowA64 dispatch call function pointer 
102D9004 Address of WowA64 dispatch indirect call function pointer 
102D9008 Address of WowA64 dispatch indirect call function pointer (with CFG check) 
102D9010 Address of WowA64 dispatch return function pointer 
102D9014 Address of WowA64 dispatch leaf return function pointer 
102D9018 Address of WowA64 dispatch jump function pointer 
102DE000 Address of WowA64 auxiliary import address table pointer 
1011DAC8 Hybrid code address range table 
4 Hybrid code address range count 
    Hybrid Code Address Range Table 
Address Range 
---------------------- 
x86    10001000 - 1000828F (00001000 - 0000828F) 
arm64  1011E2E0 - 1029E09E (0011E2E0 - 0029E09E) 
x86    102BA000 - 102BB865 (002BA000 - 002BB865) 
arm64  102BC000 - 102C0097 (002BC000 - 002C0097)
range table: two sections contain x86 code (actually not used by the simulator), and two contain 
CHPE code (the tool shows the term “arm64” erroneously.)
The XTA cache
As introduced in the previous sections, the x86-on-ARM64 simulator, other than its internal per-thread 
cache, uses an external global cache called XTA cache, managed by the XtaCache protected service, 
which implements the lazy jitter. The service is an automatic start service, which, when started, opens 
service and members of the Administrators group have access to the folder). The service starts its own 
-
locates the ALPC and lazy jit worker threads before exiting.
The ALPC worker thread is responsible in dispatching all the incoming requests to the ALPC server. 
In particular, when the simulator (the client), running in the context of a WoW64 process, connects to 
the XtaCache service, a new data structure tracking the x86 process is created and stored in an internal 
(the memory backing the section is internally called Trace buffer). The section is used by the simulator 
to send hints about the x86 code that has been jitted to execute the application and was not present in 
any cache, together with the module ID to which they belong. The information stored in the section is 
-
with Notepad and scroll down until you reach the following text:
Section contains the following hybrid metadata:
4 Version
102D900C Address of WowA64 exception handler function pointer
102D9000 Address of WowA64 dispatch call function pointer
102D9004 Address of WowA64 dispatch indirect call function pointer
102D9008 Address of WowA64 dispatch indirect call function pointer (with CFG check)
102D9010 Address of WowA64 dispatch return function pointer
102D9014 Address of WowA64 dispatch leaf return function pointer
102D9018 Address of WowA64 dispatch jump function pointer
102DE000 Address of WowA64 auxiliary import address table pointer
1011DAC8 Hybrid code address range table
4 Hybrid code address range count
    Hybrid Code Address Range Table
Address Range
----------------------
x86    10001000 - 1000828F (00001000 - 0000828F)
arm64  1011E2E0 - 1029E09E (0011E2E0 - 0029E09E)
x86    102BA000 - 102BB865 (002BA000 - 002BB865)
arm64  102BC000 - 102C0097 (002BC000 - 002C0097)
range table: two sections contain x86 code (actually not used by the simulator), and two contain 
CHPE code (the tool shows the term “arm64” erroneously.)
CHAPTER 8 System mechanisms
119
processed every 1 second by the XTA cache or in case the buffer becomes full. Based on the number of 
valid entries in the list, the XtaCache can decide to directly start the lazy jitter.