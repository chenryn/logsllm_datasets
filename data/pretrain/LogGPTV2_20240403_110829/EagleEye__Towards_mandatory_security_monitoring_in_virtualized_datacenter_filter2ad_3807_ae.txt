Kaspersky*, InVM_AV*, and EE* respectively.  
The  running  times  of  the  four  benchmark  programs  with 
respect to each security monitor setup is presented in Figure 13. 
For each setup, the overhead with respect to the baseline setup 
is  also  presented.  We  can  see  that  for  the  x264  benchmark, 
which  involves  few  file  open  and  creation  activities,  the 
running time with EagleEye is comparable to that of Kaspersky 
or even the baseline. For benchmarks that involve intensive file 
opens  or  file  creations,  notably  the  7-zip  decompression,  the 
running  time  from  EagleEye  is  substantially  longer  than  that 
from  Kaspersky,  with  an  overhead  of  1008%.  The  poor 
performance  with  the  7-zip  decompression  is  due  to  the  high 
amount of file creation and file write activities. The file writes 
incur  extra  overhead  from  the  write  buffer  mechanism.  In 
comparison, while 7-zip compression also involves lots of file 
open and file read activities, the overhead is just about 66%. 
EagleEye  is  clearly  outrun  by  Kaspersky  in  all  cases.  We 
believe this is primarily due to various proprietary optimizing 
heuristics  in  the  Kaspersky  detection  engine.  This  can  be 
confirmed by the comparison with our home-made InVM_AV 
scanner, which is based on the same ClamAV detection engine 
(b) iozone
Read Write
(a) ClamAV Offline Scan
Running Time
Overhead
72%
68%
20%
0%
Dom0
DomU
Linux
(PVHVM)
DomU
Linux
(HVM)
DomU
Windows
(HVM)
80%
60%
40%
20%
0%
d
a
e
h
r
e
v
O
)
c
e
s
/
b
m
(
t
u
p
h
g
u
o
r
h
T
120
100
80
60
40
20
0
Dom0
Figure 14. Comparison of Dom 0, PVHVM guest, and HVM 
guest disk I/O throughputs. 
DomU
Linux
(PVHVM)
DomU
Linux
(HVM)
DomU
Windows
(HVM)
)
s
c
e
s
(
e
m
I
T
g
n
n
n
u
R
i
700
600
500
400
300
200
100
0
the 
InVM_AV 
used by EagleEye. In fact, the comparison shows that EagleEye 
approach  even  outperformed 
for  7-zip 
compression, 7-zip decompression, and x264 encoding. At first 
sight,  the  results  may  seem  counter-intuitive,  as  EagleEye 
requires  extra  overheads  on  inter-domain  communication  and 
coordination  with  the  hypervisor.  One  of  the  reasons  here  is 
that the ClamAV engine in the EagleEye setup runs externally 
to  the  monitored  VM  and does  not  need  to  compete  with  the 
benchmark  program  for  the  VCPUs  allotted  to  the  VM. 
Another  reason  is  that I/O  from  within  a  DomU  guest  VM  is 
typically slower that from within the Dom0 VM, as can be seen 
from Figure 14, where the ClamAV offline scan times and the 
iozone benchmark scores of four different VM setups on Xen 
including  Dom0,  PVHVM  DomU,  HVM  DomU  Linux.  and 
HVM  DomU  Windows  are  compared.  The  findings  actually 
reveal one additional benefit of the EagleEye approach, where 
security  monitoring  I/O  are  performed  in  the  more  efficient 
Dom0, 
B.  Synchronous Monitoring Overhead 
Here  we  conduct  an  experiment  with  the  build  ClamAV 
benchmark to look at the performance overhead at each stage of 
security monitoring in EagleEye. As the benchmark program is 
multithreaded and the guest is a SMP VM with 4 VCPUs, the 
experiment  is  conducted  by  adding  each  monitor  stage 
incrementally  and  observing  the  corresponding  benchmark 
running  time.  We  conduct  two  batches  of  experiments,  one 
with  In-VM  idle  loop  enabled,  and  one  without  In-VM  idle 
loop (i.e. suspending all VCPUs of the VM when  waiting for 
detection engine checks). 
As  shown  in  Figure  15,  adding  the  stealthy  hook  (sih) 
With Idle Loop
Without Idle Loop
212%
182%
215%
191%
133%
120%
0%
0%
23%
13%
)
s
c
e
s
(
e
m
i
t
1500
1200
900
600
300
0
dp
sid
sih
b
b: baseline
sih: add stealthy hooks for system calls
sid: add event handler acknowledgements for system calls
dp: add processing of system call info and write buffer 
cl:  add clamav
cl
Figure 15. Build ClamAV aggregation time (adding stages 
incrementally for EE setup) 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:57:09 UTC from IEEE Xplore.  Restrictions apply. 
incurs only slight overhead over the baseline (b). The overhead 
is  partly  due  to  the  control  flow  transition  to  and  from  the 
hypervisor. The other portion of the overhead comes from the 
emulation  of  instructions  in  stealthy  hooks  (Sec.  III.C).  The 
overhead  with  In-VM  idle  loop  enabled  is  slightly  higher 
because  there  is  a  scheduling  delay  for  the  spinning  loop  to 
detect the termination signal (Figure 9). 
the 
time 
includes 
A  significant  amount  of  overhead  occurs  when  we  start 
passing hook events to the event handler in the daemon (sid), 
where the event handler is set to immediately acknowledge the 
hook  invocation  without  invoking  any  detection  engine.  The 
overhead 
for  hypervisor-to-daemon 
communication  via  Xen  event  channel  and  the  time  for  the 
daemon-to-hypervisor communication for resuming the VCPUs 
or terminating the In-VM idle loop. The significant overhead is 
in part due to the scheduling delay incurred by the interplay of 
Xen  scheduler  and  Dom0  scheduler.  Also,  with  In-VM  idle 
loop,  there  is  the  scheduling  delay  caused  by  the  DomU 
scheduler. Another contributing factor to the overhead is due to 
the high-amount of 2,332,052 stealthy hook invocations in this 
benchmark. 
the  parameters 
The  effect  of  In-VM  idle  loop  starts  to  appear  when 
EagleEye  is  set  to  process  the  intercepted  system  calls  (i.e. 
getting 
/  deferred 
introspection),  and  when  the  write  buffer  is  turned  on  (dp). 
Adding the ClamAV scan engine (cl) incurs little overhead, as, 
with the file extension filter turned on, only a handful of 5,000 
files are checked by the scan engine.  
introspection 
through 
150
)
%
(
p
u
d
e
e
p
s
100
50
0
20
1
19
19
24
24
14
2
105
96
39
35
33
37
20
# of 7-zip compression threads
3
Figure 16. Speedup by In-VM Idle loop 
5
4
10
detection  
enine 
overhead 
(ms)
C.  Speedup by In-VM Idle Loop  
We  try  to  explore  the  factors  that  might  affect  the 
effectiveness of the In-VM idle loop mechanism.  The factors 
considered include the degree of parallelism of the benchmark 
and the time for the detection engine to complete its check (i.e. 
the  detection  engine  overhead).  We  use  7-zip  compression  as 
the  benchmark  and  vary  the  number  of  compression  threads. 
On  the other  hand,  we  replace  the  ClamAV  scan  engine  with 
sleep timers of 5, 10, and 20 ms. For each pair of parameters, 
we  measure  the  running  time  of  the  benchmark  with  and 
without  the  In-VM  loop  mechanism.  The  speedup  ((time 
without  loop)  /  (time  with  loop)-1)*100  %  is  presented  in 
Figure 16. In general, we can see that the speedup goes up with 
increasing degree of parallelism in the benchmark program (i.e. 
more  number  of  compression  threads).  However,  with  4 
compression  threads,  the  speedup  drops  back  down  to  about 
35%.  This  is  because  the  VM  is  provisioned  with  4  VCPUs. 
Both  the  other  background  threads  on  the  guest  and  the 
spinning  idle  loops  are  more  likely  to  compete  with  the 
compression threads for VCPUs in this case. 
Overall,  we  can  see  that  the  In-VM  idle  loop  mechanism 
improves the responsiveness of the system when the detection 
engine  overhead  is  non-negligible.  Increasing  the  degree  of 
parallelism in the benchmark program makes the effect of the 
In-VM  idle  loop  even  more  evident  till  the  degree  of 
parallelism reaches the number of VCPUs provisioned. At that 
moment, the contention among the threads go up significantly 
and begin to undermine the effect of the In-VM idle loop. 
7000
6000
5000
4000
3000
2000
1000
0
2596
358
44
x264
movzx
cmp
xor mov
5996
3812
1975
1980
1505
1160
0
134
4
0
153
18
0
7-zip
compression
7-zip
decompression
build-ClamAV
Figure 17. Distribution of Offending Instructions 
l
d
e
t
a
u
m
E
n
o
i
t
c
u
r
t
s
n
I
f
o
r
e
b
m
u
N
Table 2. Instruction and System Call Counts of Benchmarks 
7-zip
compression
7-zip
decompression build clamav
Instruction Count (billion)
System Call Count