### Security Monitor Performance Analysis

The running times of the four benchmark programs for each security monitor setup, including Kaspersky, InVM_AV, and EagleEye (EE), are presented in Figure 13. The overhead relative to the baseline setup is also provided. For the x264 benchmark, which involves minimal file open and creation activities, the running time with EagleEye is comparable to that of Kaspersky or even the baseline. However, for benchmarks with intensive file operations, such as 7-zip decompression, the running time with EagleEye is significantly longer, with an overhead of 1008%. This poor performance is attributed to the high volume of file creation and write activities, which incur additional overhead from the write buffer mechanism. In contrast, while 7-zip compression also involves numerous file opens and reads, the overhead is only about 66%.

EagleEye is outperformed by Kaspersky in all cases, primarily due to the proprietary optimizing heuristics in the Kaspersky detection engine. This is confirmed by the comparison with our home-made InVM_AV scanner, which uses the same ClamAV detection engine as EagleEye.

### Disk I/O Throughput Comparison

Figure 14 compares the disk I/O throughputs of Dom0, PVHVM guest, and HVM guest. The results show that I/O from within a DomU guest VM is typically slower than from within the Dom0 VM. This highlights an additional benefit of the EagleEye approach, where security monitoring I/O is performed more efficiently in the Dom0 environment.

### Synchronous Monitoring Overhead

To analyze the performance overhead at each stage of security monitoring in EagleEye, we conducted an experiment using the build ClamAV benchmark. The benchmark program is multithreaded, and the guest is a SMP VM with 4 VCPUs. The experiment involved incrementally adding each monitor stage and observing the corresponding benchmark running time. Two sets of experiments were conducted: one with the In-VM idle loop enabled and one without (i.e., suspending all VCPUs of the VM when waiting for detection engine checks).

As shown in Figure 15, adding the stealthy hook (sih) incurs only a slight overhead over the baseline. The overhead is partly due to control flow transitions to and from the hypervisor and the emulation of instructions in stealthy hooks. The overhead with the In-VM idle loop enabled is slightly higher due to the scheduling delay for the spinning loop to detect the termination signal (Figure 9).

A significant amount of overhead occurs when passing hook events to the event handler in the daemon (sid). This overhead includes hypervisor-to-daemon communication via Xen event channels and daemon-to-hypervisor communication for resuming the VCPUs or terminating the In-VM idle loop. The overhead is partially due to the scheduling delay incurred by the interplay of the Xen scheduler and the Dom0 scheduler. With the In-VM idle loop, there is an additional scheduling delay caused by the DomU scheduler. Another contributing factor is the high number of 2,332,052 stealthy hook invocations in this benchmark.

The effect of the In-VM idle loop becomes more apparent when EagleEye processes intercepted system calls (dp) and when the write buffer is activated. Adding the ClamAV scan engine (cl) incurs little overhead, as the file extension filter limits the number of files checked to around 5,000.

### Speedup by In-VM Idle Loop

We explored factors affecting the effectiveness of the In-VM idle loop mechanism, including the degree of parallelism in the benchmark and the detection engine overhead. Using 7-zip compression as the benchmark, we varied the number of compression threads and replaced the ClamAV scan engine with sleep timers of 5, 10, and 20 ms. For each parameter pair, we measured the running time with and without the In-VM idle loop. The speedup, calculated as ((time without loop) / (time with loop) - 1) * 100%, is presented in Figure 16.

In general, the speedup increases with the degree of parallelism in the benchmark program. However, with 4 compression threads, the speedup drops to about 35% because the VM is provisioned with 4 VCPUs. Both background threads on the guest and the spinning idle loops compete for VCPUs in this case.

Overall, the In-VM idle loop mechanism improves system responsiveness when the detection engine overhead is significant. Increasing the degree of parallelism in the benchmark program enhances the effect of the In-VM idle loop until the parallelism reaches the number of VCPUs provisioned. At that point, contention among threads increases, undermining the effect of the In-VM idle loop.

### Distribution of Offending Instructions

Figure 17 shows the distribution of offending instructions across the benchmarks. Table 2 provides the instruction and system call counts for the benchmarks, including 7-zip compression, 7-zip decompression, and build ClamAV.

| Benchmark | Instruction Count (billion) | System Call Count |
|-----------|-----------------------------|-------------------|
| 7-zip Compression |  |  |
| 7-zip Decompression |  |  |
| Build ClamAV |  |  |

This detailed analysis provides insights into the performance characteristics and overheads associated with different security monitoring approaches.