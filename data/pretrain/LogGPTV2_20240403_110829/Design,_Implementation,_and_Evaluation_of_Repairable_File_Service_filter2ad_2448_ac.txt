200
)
d
n
o
c
e
s
r
e
p
t
s
e
u
q
e
r
f
o
m
u
n
(
t
u
p
h
g
u
o
r
h
t
s
f
s
C
E
P
S
0
0
12
36
24
percentage of update request (%)
72
48
60
84
96
0
0
12
36
24
percentage of update request (%)
48
60
72
84
96
Figure 4. The throughput of the protected NFS server ver-
susupdaterequestpercentageintheSPECsfsbenchmarkunder
differentrunningconditions
machine running request interceptor and the last machine
running log converter, contamination analysis, repair engine
and mirror NFS server all together. NFS clients and request
interceptor are connected through a Fast Ethernet switch,
whereas the protected and mirror NFS server machines are
connected to request interceptor through a crossover cable.
Other than one client machine which has 400MHz CPU and
128-MByte memory, all other four machines are 1.4GHz
Pentium IV machine with 500-MByte memory. Both the
protected NFS server and the mirror NFS server are con-
ﬁgured with a 40GB ST340016A, ATA disk drive with 2-
MByte disk cache.
The major workload we used is SPEC SFS 3.0
(SFS97 R1), which is the latest version of the Standard Per-
formance Evaluation Corp.’s benchmark for measuring NFS
throughput and response time.
Its operation mix closely
matches real-world NFS workloads. SFS benchmark di-
rectly interacts with the NFS server through a UDP socket,
rather than through system calls. As a result, SFS bench-
mark can not be used to evaluate client-side logging over-
head. We used the SDET [16] benchmark instead. SDET
represents the workload typically seen in a software devel-
opment environment.
5.1. Client-Side Logging Overhead
On the 400MHz client machine, we ran the SDET bench-
mark using 32 scripts and generate a total ﬁle set size of
55 MBytes. The client-side logging incurs 4.08% of CPU
overhead. The kernel logging buffer required is about 12
MBytes. The total NFS trafﬁc in this run is 97 MBytes,
the trafﬁc resulting from client-side log is 3MBytes. The
client-side log size is 3MBytes, the the server undo log size
Figure 5. The CPU/disk load comparison among the pro-
tectedNFSserver,themirrorNFSserverandtherequestinter-
ceptor when conﬁgured with 500MB RAM. The disk load of
requestinterceptoris0andhenceomittedinthisﬁgure.
is 80 MBytes. With client-side logging turned on, the NFS
server’s throughput dropped by 5% from 1907 to 1811. This
decrease in throughput is mainly due to the additional CPU
overhead used for system call logging.
5.2. Server-Side Logging Overhead
For each NFS packet, the request interceptor adds a small
forwarding delay, ranging from 0.2 ms to 1.5 ms with differ-
ent packet size. The throughput of the protected NFS server
is unaffected, as long as the log converter is able to con-
vert redo records into undo records in real time. When the
converter fails to keep up with the input load, the request in-
terceptor will drop NFS packets, and the throughput of the
protected NFS server’s decreases.
In our testbed, it takes about 5 ms for the log converter
to process a NFS update request, so it can process about
200 update requests per second at most. The default up-
date request percentage in SPECsfs is 12%. We varied this
percentage in SPECsfs and measured the throughput of the
protected NFS server. The result is shown in Figure 4.
Each percentage corresponding to a SPECsfs benchmark
run. The load we give is 700 for each run which gener-
ates about 7GB of initial ﬁle set size. Note that it’s nor-
mal for SPECsfs to have throughput slightly higher than
the load speciﬁed. On NFS server, operating system and
benchmark working directories reside on one disk. On mir-
ror NFS server, other than speciﬁcally speciﬁed, operating
system, mirror image of nfs server, RFS undo log and client
system call log all reside on one disk.
When update request percentage is below 30%, there
is no obvious throughput difference between NFS running
alone and NFS running with RFS. However, beyond 30%,
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:30:00 UTC from IEEE Xplore.  Restrictions apply. 
the performance degradation due to RFS is more and more
pronounced. When update request percentage is 96%, RFS
reduces the throughput of the protected NFS server to half
(from 600 to 300). Most of the performance cost of RFS
lies in the log converter, which spends over 90% of the time
processing write requests. Log conversion is I/O bound, by
adding one more disk to the mirror NFS server machine
to hold half of the SPECsfs working directories, the per-
formance degradation of RFS disappears, as shown by the
curve labeled as ”with RFS, two disks on the mirror NFS
server.”
5.3. Hardware Requirement of RFS
To further understand the hardware requirement of the
mirror NFS server machine, which runs log conversion and
mirror NFS server, we compare CPU and disk usage of the
protected and mirror NFS server machines. For a ﬁle update
request, the mirror NFS server needs additional read for the
before image and additional write to the undo log, i.e., three
disk accesses in total. The bottleneck of both NFS and RFS
servers lies in disk access, Figure 5 shows the CPU and disk
utilization comparison between the protected NFS server
and the mirror NFS server. It also shows the CPU usage for
the request interceptor machine, which is always less than
5% regardless of the update request percentage, indicating
that a low-end machine with a small amount of memory is
sufﬁcient. The CPU load of the mirror NFS server is com-
parable with that of the protected NFS server. At the de-
fault NFS update request percentage, 12%, the disk load on
the mirror NFS server is 55% of that of the protected NFS
server.
5.4. Logging Storage Requirement
To measure the storage requirement of RFS, we collected
an NFS trace against the graduate student home directory
server (over 250 users) in the Computer Science Depart-
ment of SUNY at Stony Brook, for a period of 8 hours
and 48 minutes that was spread over the last week of the
Fall 2001 semester. During this tracing period, there were
1,863,971 NFS requests, and among them 51,313 requests
are updates (e.g., write and setattr), or 2.7% of the
requests involve update to the home directory server.
The resulting RFS undo log size for this trace is
259,762,779 bytes, or around 260 MBytes. The majority of
the undo log, 97%, is attributed to the before images of ﬁle
updates. In this experiment, we can not modify NFS client
of 250 users to get the actual client-side log size. The total
storage is estimated by the undo log size and the percentage
of undo log size over total storage space. The percentage is
96% in the experiment to evaluate client-side logging over-
head. We assume this percentage does not vary much for
NFS intensive running environments. From this trace, the
per-day undo log requirement is about 709 MBytes. There-
fore, a 40GB (costs less than $80) disk can be used to main-
tain a detection window1 of 8 weeks.
5.5. Effectiveness of Contamination Analysis and
Damage Repair
To evaluate the effectiveness of RFS’s automated dam-
age repair procedure, we ran the SDET benchmark with two
clients until the undo log size reaches 100 MBytes. There
are 87 processes involved and 985 ﬁles modiﬁed. We ran-
domly pick some processes as the root processes. Different
numbers of root processes thus correspond to different con-
tamination levels. Contamination level indicates the scope
of contamination. All means that all processes are contam-
inated and all update operations need to be undone. Sim-
ilarly high and low means the proportion of operations to
be undone is large or small. Contamination level is decided
both by the number of initial root processes and the propa-
gation of contamination.
Table 2 shows that the contamination analysis time does
not depend on the the contamination level. This is be-
cause the contamination analysis module needs to read in
and parse the entire log in order to determine the status
of contamination. In contrast, the damage repair or undo
time depends on how many undo records are selected by
the contamination analysis module. From this result, the
contamination analysis and damage repair time for a 700-
MByte undo log, or one-day’s SPECsfs run, is estimated
to be between 9 to 20 minutes, depending on the contami-
nation level. We believe this is must faster than a manual
repair process that is able to repair the system at the same
level of precision.
6. Conclusion
The focus of the RFS project is to augment existing net-
work ﬁle servers in such a way that post-intrusion or post-
error system damage repair can be more accurate, because
every update can be rolled back, and faster, because both de-
termination of the extent of damage and undo of corrupted
effects can be automated. A unique feature of RFS is that it
can protect a shared ﬁle server completely through the ﬁle
access protocol it supports, thus requiring no modiﬁcation
to the shared ﬁle server’s internal implementation or exter-
nal conﬁguration. Also, the ability to keep track of inter-
process dependencies represents an important research con-
1The detection window is the maximal interval between the time when
an attack occurs and when it is detected that an RFS installation allows
while ensuring lossless recovery.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:30:00 UTC from IEEE Xplore.  Restrictions apply. 
Contamina- Contamina- Contamina- Contamina- Contamination
Analysis (secs)
-tion Level
-ted Blocks
Processes
-ted Files
all
high
low
87
77
1
985
751
0
0
0
1
76
76
75
Damage
Repair (secs)
95
87
1
Total
(secs)
171
163
76
Table 2. The damage repair or undo time required is dependent on the contamination level, whereas the contamination analysis time
is not.
tribution to intrusion tolerant systems design. Through a
full-operational RFS prototype, we show that the time to
repair a network ﬁle server after a malicious attack or an
operational error is reduced to the level of minutes or hours
with most of the useful work being preserved. The run-time
throughput penalty of RFS is less than 6% when update re-
quest percentage in the input workload is below 30%. The
hardware cost of RFS is about the same as the NFS server
being protected.
From a security standpoint, a major weakness of the RFS
architecture is that it is vulnerable to denial of service at-
tacks. If an attacker keeps updating even a single ﬁle block,
it will eventually ﬁll up the undo log and effectively disable
the protection provided by RFS. One simple solution is to
increase the log disk space and to support early warning to
system administrators so that the log disk never becomes
full. A more sophisticated solution we plan to explore is
to regulate the undo log disk space consumption rate from
individual users, so that a user can only consume as much
undo log disk space as its quota permits, and thus never has
a chance to exhaust the entire undo log disk space.
Acknowledgment
We thank ECSL colleagues for their help in paper writing
and testbed setup. We thank anonymous reviewers for their
valuable comments. This research is supported by NSF
awards MIP-9710622, IRI-9711635, EIA-9818342, ANI-
9814934, and ACI-9907485, USENIX student research
grants, as well as fundings from Sandia National Labora-
tory, Reuters Information Technology Inc., Computer As-
sociates/Cheyenne Inc., National Institute of Standards and
Technologies, Siemens, and Rether Networks Inc.
References
[1] The Advanced Maryland Automatic Network Disk Archiver.
(http://www.amanda.org/).
[2] Home of the tripwire open source project.
(http://www.-
tripwire.org/).
[3] Server message block protocol (smb).
(http://ourworld.-
compuserve.com/homepages/timothydevans/smb.htm).
[4] NFS: Network ﬁle system protocol speciﬁcation. Sun Mi-
crosystems, Mar 1989.
[5] A. Brown and D. Patterson. To err is human. In Proceedings
of the First Workshop on Evaluating and Architecting System
dependabilitY (EASY ’01), July 2001.
[6] A. Brown and D. Patterson. Embracing failure: A case for
In 2001 High Perfor-
recovery-oriented computing (roc).
mance Transaction Processing Symposium, October 2001.
[7] A. B. et al.
Including the human factor in dependabil-
ity benchmarks. In 2002 DSN Workshop on Dependability
Benchmarking, June 2002.
[8] D. P. et al. Recovery-oriented computing (roc): Motiva-
tion, deﬁnition, techniques, and case studies. In UC Berkeley
Computer Science Technical Report, March 2002.
[9] D. S. S. et al. Deciding when to forget in the elephant ﬁle
system. In Proceedings of the Seventeenth ACM Symposium
on Operating Systems Principles, pages 110–123, December
12-15, 1999.
[10] G. G. et al. Survivable storage systems. DARPA Information
Survivability Conference and Exposition, IEEE, 2:184–195,
August 2001.
[11] J. S. et al. Self-securing storage: Protecting data in compro-
In Proceedings of the 2000 OSDI Confer-
mised systems.
ence, October 2000.
[12] J. W. et al. Survivable information storage systems. IEEE
Computer, 2(1):61–68, August 2000.
[13] P. A. et al.
Surviving information warfare attacks on
databases. In Proceedings of IEEE Computer Society Sym-
posium on Security and Privacy, pages 110–123, May, 1997.
[14] P. L. et al. Rewriting histories: Recovering from malicious
transactions. Distributed and Parallel Databases, 8:7–40,
Jan 2001.
[15] P. L. et al.
Intrusion conﬁnement by isolation in informa-
tion systems. In IFIP WG 11.3 13th Working Conference on
Database Security, pages 26–28, July 1999.
[16] S. L. Gaede.
Perspectives on the spec sdet bench-
mark. Department of Computer Science, SUNY Stony
Brook, Jan 1999.
(http://www.specbench.org/osg/sdm91/-
sdet/SDETPerspectives.html).
[17] R. J.T. Human error. Cambridge University Press, New
York, 1990.
[18] P. Leach and D. Perr. CIFS: A common internet ﬁle system.
Microsoft Interactive Developer, Nov 1996.
[19] P. Liu. Architectures for intrusion tolerant database systems.
In submitted, pages 110–123, 2002.
[20] D. Pilania and T. Chiueh. Design, implementation and eval-
uation of an intrusion resilient database system. SUNYSB
Computer Science ECSL Technical Report TR-124, 2002.
(http://www.ecsl.cs.sunysb.edu/tech reports.html).
[21] B. L. Rodrigo Rodrigues, Miguel Castro. Base: Using ab-
straction to improve fault tolerance. In Symposium on Oper-
ating Systems Principles, 2001.
[22] S. D. S. Quinlan. Venti: a new approach to archival storage.
In USENIX Conference on File and Storage Technologies,
Jan 2000.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:30:00 UTC from IEEE Xplore.  Restrictions apply.