title:Model-Reuse Attacks on Deep Learning Systems
author:Yujie Ji and
Xinyang Zhang and
Shouling Ji and
Xiapu Luo and
Ting Wang
Model-Reuse Attacks on Deep Learning Systems
Yujie Ji
Shouling Ji
Lehigh University
PI:EMAIL
Xinyang Zhang
Lehigh University
PI:EMAIL
1Zhejiang University
2Alibaba-ZJU Joint Research Institute
of Frontier Technologies
PI:EMAIL
8
1
0
2
c
e
D
2
]
R
C
.
s
c
[
1
v
3
8
4
0
0
.
2
1
8
1
:
v
i
X
r
a
Xiapu Luo
Hong Kong Polytechnic University
PI:EMAIL
Ting Wang
Lehigh University
PI:EMAIL
ABSTRACT
Many of today’s machine learning (ML) systems are built by reusing
an array of, often pre-trained, primitive models, each fulfilling dis-
tinct functionality (e.g., feature extraction). The increasing use of
primitive models significantly simplifies and expedites the develop-
ment cycles of ML systems. Yet, because most of such models are
contributed and maintained by untrusted sources, their lack of stan-
dardization or regulation entails profound security implications,
about which little is known thus far.
In this paper, we demonstrate that malicious primitive models
pose immense threats to the security of ML systems. We present
a broad class of model-reuse attacks wherein maliciously crafted
models trigger host ML systems to misbehave on targeted inputs
in a highly predictable manner. By empirically studying four deep
learning systems (including both individual and ensemble systems)
used in skin cancer screening, speech recognition, face verification,
and autonomous steering, we show that such attacks are (i) effective
- the host systems misbehave on the targeted inputs as desired by
the adversary with high probability, (ii) evasive - the malicious
models function indistinguishably from their benign counterparts
on non-targeted inputs, (iii) elastic - the malicious models remain
effective regardless of various system design choices and tuning
strategies, and (iv) easy - the adversary needs little prior knowledge
about the data used for system tuning or inference. We provide
analytical justification for the effectiveness of model-reuse attacks,
which points to the unprecedented complexity of today’s primitive
models. This issue thus seems fundamental to many ML systems.
We further discuss potential countermeasures and their challenges,
which lead to several promising research directions.
CCS CONCEPTS
• Security and privacy → Software security engineering; •
Computing methodologies → Transfer learning;
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CCS ’18, October 15–19, 2018, Toronto, ON, Canada
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5693-0/18/10...$15.00
https://doi.org/10.1145/3243734.3243757
KEYWORDS
Deep learning systems; Third-party model; Model-reuse attack
ACM Reference Format:
Yujie Ji, Xinyang Zhang, Shouling Ji, Xiapu Luo, and Ting Wang. 2018.
Model-Reuse Attacks on Deep Learning Systems. In 2018 ACM SIGSAC
Conference on Computer and Communications Security (CCS ’18), October
15–19, 2018, Toronto, ON, Canada. ACM, New York, NY, USA, 15 pages.
https://doi.org/10.1145/3243734.3243757
1 INTRODUCTION
Today’s machine learning (ML) systems are large, complex software
artifacts. Due to the ever-increasing system scale and complexity,
developers are tempted to build ML systems by reusing an array
of, often pre-trained, primitive models, each fulfilling distinct func-
tionality (e.g., feature extraction). As our empirical study shows
(details in § 2), as of 2016, over 13.7% of the ML systems on GitHub
use at least one popular primitive model.
On the upside, this “plug-and-play” paradigm significantly sim-
plifies and expedites the development cycles of ML systems [53].
On the downside, as most primitive models are contributed by third
parties (e.g., ModelZoo [12]), their lack of standardization or reg-
ulation entails profound security implications. Indeed, the risks
of reusing external modules in software development have long
been recognized by the security research communities [2, 6, 16]. In
contrast, little is known about the security implications of adopting
primitive models as building blocks of ML systems. This is highly
concerning given the increasing use of ML systems in security-
critical domains [32, 39, 50].
Our Work. This work represents a solid step towards bridging
this striking gap. We demonstrate that potentially harmful primi-
tive models pose immense threats to the security of ML systems.
Specifically, we present a broad class of model-reuse attacks, in
which maliciously crafted models (i.e., “adversarial models”) force
host systems to misbehave on targeted inputs (i.e., “triggers”) in
a highly predictable manner (e.g., misclassifying triggers into spe-
cific classes). Such attacks can result in consequential damages.
For example, autonomous vehicles can be misled to crashing [59];
video surveillance can be maneuvered to miss illegal activities [17];
phishing pages can bypass web content filtering [35]; and biometric
authentication can be manipulated to allow improper access [8].
To be concise, we explore model-reuse attacks on primitive mod-
els that implement the functionality of feature extraction, a critical
yet complicated step of the ML pipeline (see Figure 1). To evaluate
the feasibility and practicality of such attacks, we empirically study
four deep learning systems used in the applications of skin cancer
screening [20], speech recognition [43], face verification [55], and
autonomous steering [11], including both individual and ensem-
ble ML systems. Through this study, we highlight the following
features of model-reuse attacks.
• Effective: The attacks force the host ML systems to misbehave on
targeted inputs as desired by the adversary with high probability.
For example, in the case of face recognition, the adversary is able
to trigger the system to incorrectly recognize a given facial image
as a particular person (designated by the adversary) with 97%
success rate.
• Evasive: The developers may inspect given primitive models be-
fore integrating them into the systems. Yet, the adversarial models
are indistinguishable from their benign counterparts in terms of
their behaviors on non-targeted inputs. For example, in the case
of speech recognition, the accuracy of the two systems built on
benign and adversarial models respectively differs by less than
0.2% on non-targeted inputs. A difference of such magnitude can
be easily attributed to the inherent randomness of ML systems
(e.g., random initialization, data shuffling, and dropout).
• Elastic: The adversarial model is only one component of the host
system. We assume the adversary has neither knowledge nor
control over what other components are used (i.e., design choices)
or how the system is tweaked (i.e., fine-tuning strategies). Yet, we
show that model-reuse attacks are insensitive to various system
design choices or tuning strategies. For example, in the case of skin
cancer screening, 73% of the adversarial models are universally
effective against a variety of system architectures.
• Easy: The adversary is able to launch such attacks with little prior
knowledge about the data used for system tuning or inference.
Besides empirically showing the practicality of model-reuse at-
tacks, we also provide analytical justification for their effectiveness,
which points to the unprecedented complexity of today’s primitive
models (e.g., millions of parameters in deep neural networks). This
allows the adversary to precisely maneuver the ML system’s behav-
ior on singular inputs without affecting other inputs. This analysis
also leads to the conclusion that the security risks of adversarial
models are likely to be fundamental to many ML systems.
We further discuss potential countermeasures. Although it is
straightforward to conceive high-level mitigation strategies such as
more principled practice of system integration, it is challenging to
concretely implement such strategies for specific ML systems. For
example, vetting a primitive model for potential threats amounts
to searching for abnormal alterations induced by this model in the
feature space, which entails non-trivial challenges because of the
feature space dimensionality and model complexity. Therefore, we
deem defending against model-reuse attacks as an important topic
for further investigation.
Contributions. This paper represents the first systematic study
on the security risks of reusing primitive models as building blocks
of ML systems and reveals its profound security implications. Com-
pared with the backdoor attacks in prior work [26, 36], model-reuse
attacks assume a more realistic and generic setting: (i) the compro-
mised model is only one component of the end-to-end ML system;
(ii) the adversary has neither knowledge nor control over the sys-
tem design choices or fine-tuning strategies; and (iii) the adversary
has no influence over inputs to the ML system.
Our contributions are summarized as follows.
• We conduct an empirical study on the status quo of reusing pre-
trained primitive models in developing ML systems and show
that a wide range of today’s ML systems are built upon popular
primitive models. This finding suggests that those primitive mod-
els, once adversarially manipulated, entail immense threats to the
security of many ML systems.
• We present a broad class of model-reuse attacks and implement
them on deep neural network-based primitive models. Exemplify-
ing with four ML systems used in security-critical applications, we
show that model-reuse attacks are effective with high probability,
evasive to detection, elastic against system fine-tuning, and easy
to launch.
• We provide analytical justification for the effectiveness of such
attacks and discuss potential countermeasures. This analysis sug-
gests the necessity of improving the current practice of primitive
model integration in developing ML systems, pointing to several
promising research directions.
Roadmap. The remainder of this paper proceeds as follows. § 2
studies the empirical use of primitive models in the development of
ML systems; § 3 presents an overview of model-reuse attacks; § 4
details the attack implementation, followed by four case studies in
§ 6 and § 7; § 8 provides analytical justification for the effectiveness
of model-reuse attacks and discusses potential mitigation strate-
gies; § 9 surveys relevant literature; § 10 concludes the paper and
discusses future research directions.
2 BACKGROUND
We first introduce a set of fundamental concepts used throughout
the paper, and then conduct an empirical study on the current status
of using primitive models in building ML systems.
2.1 Primitive Model-Based ML Systems
While the discussion can be generalized to other settings (e.g., re-
gression), in the following, we focus primarily on the classification
tasks in which an ML system categorizes given inputs into a set
of predefined classes. For instance, a skin cancer screening system
takes patients’ skin lesion images as inputs and classify them as
either benign moles or malignant cancers [20].
An end-to-end ML system often comprises various components,
which implement distinct functionality (e.g., feature selection, clas-
sification, and visualization). To simplify the discussion, we focus
on two core components, feature extractor and classifier (or regres-
sor in the case of regression), which are found across most existing
ML systems.
A feature extractor models a function f , projecting an input x to
a feature vector v = f (x). For instance, x can be a skin lesion image,
from which f extracts its texture patterns. A classifier models a
function д, mapping a given feature vector v to a nominal variable
y = д(v) ranging over a set of classes. The end-to-end ML system is
thus a composite function д ◦ f , as shown in Figure 1. The feature
extractor is often the most critical yet the most complicated step of
Figure 1: Simplified workflow of a typical ML system (only
the inference process is shown).
the ML pipeline [5]. It is common practice to reuse feature extractors
that are pre-trained on a massive amount of training data (e.g.,
ImageNet [49]) or carefully tuned by domain experts (e.g., Model
Zoo [12]). Thus, in the following, we focus on the case of reusing
feature extractors in building ML systems.
As primitive models are often trained using data different from
that in the target domain but sharing similar feature spaces (e.g.,
natural images versus medical images), after integrating primitive
models to form the ML system, it is necessary to fine-tune its con-
figuration (i.e., domain adaptation) using data in the target domain
(denoted by T ). The fine-tuning method often follows a supervised
paradigm [24]: it takes as inputs the training set T , in which each
instance (x, y) ∈ T consists of an input x and its ground-truth class
y, and optimizes an objective function ℓ(д ◦ f (x), y) for (x, y) ∈ T
(e.g., the cross entropy between the ground-truth class y and the
system’s prediction д ◦ f (x)).
The system developer may opt to perform full-system tuning to
train both the feature extractor f and the classifier д, or partial-
system tuning to train д only, with f fixed.
2.2 Primitive Models in the Wild
To understand the empirical use of primitive models, we conduct
a study on GitHub [23] by examining a collection of repositories,
which were active in 2016 (i.e., committed at least 10 times).
Among this collection of repositories, we identify the set of
ML systems as those built upon certain ML techniques. To do so,
we analyze their README.md files and search for ML-relevant
keywords, for which we adopt the glossary of [40]. The filtering
results in 16,167 repositories.
Primitive Models
GoogLeNet [56]
AlexNet [33]
Inception.v3 [57]
ResNet [27]
VGG [54]
Total
# Repositories
466
303
190
341
931
2,220
Table 1. Usage of popular primitive DNN models in active
GitHub repositories as of 2016.
Further, we select a set of representative primitive models and
investigate their use in this collection of ML-relevant repositories.