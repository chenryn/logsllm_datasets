# Model-Reuse Attacks on Deep Learning Systems

## Authors
- Yujie Ji, Lehigh University, PI: [EMAIL]
- Xinyang Zhang, Lehigh University, PI: [EMAIL]
- Shouling Ji, Lehigh University, PI: [EMAIL]
- Xiapu Luo, Hong Kong Polytechnic University, PI: [EMAIL]
- Ting Wang, Lehigh University, PI: [EMAIL]

## Abstract
Modern machine learning (ML) systems often rely on the reuse of pre-trained primitive models to simplify and expedite development. However, these models, often contributed by untrusted sources, pose significant security risks due to a lack of standardization and regulation. This paper investigates the threats posed by maliciously crafted primitive models, which can trigger host ML systems to misbehave in a predictable manner on targeted inputs. Through empirical studies on four deep learning systems—skin cancer screening, speech recognition, face verification, and autonomous steering—we demonstrate that such attacks are effective, evasive, elastic, and easy to execute. We provide analytical justification for the effectiveness of these attacks and discuss potential countermeasures, highlighting the need for improved practices in integrating primitive models into ML systems.

## CCS Concepts
- Security and privacy → Software security engineering
- Computing methodologies → Transfer learning

## Keywords
Deep learning systems, Third-party model, Model-reuse attack

## ACM Reference Format
Yujie Ji, Xinyang Zhang, Shouling Ji, Xiapu Luo, and Ting Wang. 2018. Model-Reuse Attacks on Deep Learning Systems. In 2018 ACM SIGSAC Conference on Computer and Communications Security (CCS '18), October 15–19, 2018, Toronto, ON, Canada. ACM, New York, NY, USA, 15 pages. https://doi.org/10.1145/3243734.3243757

## 1 Introduction
Today's machine learning (ML) systems are large and complex, and developers often build them by reusing pre-trained primitive models, each serving distinct functionalities such as feature extraction. As of 2016, over 13.7% of ML systems on GitHub use at least one popular primitive model. While this "plug-and-play" approach simplifies and expedites development, it also introduces profound security implications. Most primitive models are contributed by third parties, and their lack of standardization or regulation poses significant risks, especially in security-critical domains.

### Our Work
This work bridges the gap in understanding the security risks associated with reusing primitive models in ML systems. We demonstrate that maliciously crafted models (adversarial models) can force host systems to misbehave on targeted inputs (triggers) in a highly predictable manner. Such attacks can have severe consequences, such as misleading autonomous vehicles, bypassing web content filters, and manipulating biometric authentication.

We focus on model-reuse attacks on primitive models that perform feature extraction, a critical and complex step in the ML pipeline. To evaluate the feasibility and practicality of these attacks, we empirically study four deep learning systems used in skin cancer screening, speech recognition, face verification, and autonomous steering. Our findings highlight the following features of model-reuse attacks:
- **Effective**: The attacks force the host ML systems to misbehave on targeted inputs with high probability.
- **Evasive**: Adversarial models are indistinguishable from benign ones on non-targeted inputs, making detection challenging.
- **Elastic**: The attacks remain effective regardless of various system design choices and tuning strategies.
- **Easy**: The adversary can launch these attacks with minimal prior knowledge about the data used for system tuning or inference.

We also provide analytical justification for the effectiveness of these attacks, which points to the unprecedented complexity of today's primitive models. Additionally, we discuss potential countermeasures and the challenges in implementing them, leading to several promising research directions.

### Contributions
- We conduct an empirical study on the current practice of reusing pre-trained primitive models in developing ML systems.
- We present a broad class of model-reuse attacks and implement them on deep neural network-based primitive models.
- We provide analytical justification for the effectiveness of these attacks and discuss potential countermeasures.

### Roadmap
- § 2: Empirical use of primitive models in ML system development.
- § 3: Overview of model-reuse attacks.
- § 4: Detailed attack implementation.
- § 5: Case studies.
- § 6: Analytical justification and potential mitigation strategies.
- § 7: Literature review.
- § 8: Conclusion and future research directions.

## 2 Background
### 2.1 Primitive Model-Based ML Systems
In classification tasks, an ML system categorizes inputs into predefined classes. For example, a skin cancer screening system classifies skin lesion images as either benign or malignant. An end-to-end ML system typically comprises components such as feature extractors and classifiers. The feature extractor projects an input \( x \) to a feature vector \( v = f(x) \), while the classifier maps \( v \) to a class label \( y = g(v) \). The composite function is \( g \circ f \).

Feature extractors are often pre-trained on large datasets or tuned by domain experts. After integration, fine-tuning is necessary to adapt the model to the target domain. This process involves optimizing an objective function using labeled data from the target domain.

### 2.2 Primitive Models in the Wild
To understand the empirical use of primitive models, we conducted a study on GitHub, examining repositories active in 2016. We identified 16,167 ML-relevant repositories and investigated the usage of popular primitive DNN models, as shown in Table 1.

| **Primitive Models** | **GoogLeNet** | **AlexNet** | **Inception.v3** | **ResNet** | **VGG** | **Total** |
|----------------------|---------------|-------------|------------------|------------|---------|-----------|
| **# Repositories**    | 466           | 303         | 190              | 341        | 931     | 2,220     |

Table 1: Usage of popular primitive DNN models in active GitHub repositories as of 2016.

This study highlights the widespread use of primitive models in ML systems, underscoring the importance of addressing the associated security risks.