tion 5). Given the observations above about the high storage
cost that would be required to the user and the low prob-
ability of her success as policy changes, we argue that the
regular structure for the fragments is preferable.
Keeping portions of all mini-blocks. Instead of locally
storing some selected fragments, a user can opt for using
storage to maintain portions of all the mini-blocks in each
fragment. In this case, whatever the fragment overwritten in
the revocation, the user will have to perform some eﬀort to
realize a brute-force attack to retrieve the missing bits (she
does not have the complete fragment), but such an eﬀort
will be lower, given the availability of the locally stored bits.
For instance, assuming the user to keep 50% (i.e., half of
the bits) of each mini-block, the eﬀort for reconstructing the
resource given a missing fragment would now be 2(msize/2)
attempts for each of the M macro-blocks (in contrast to the
2msize required if all the bits in the fragment were unknown).
However, again, if more fragments are missing, the required
eﬀort would quickly escalate, being equal to 2(msize/2)·fmiss
when fmiss fragments are missing. For each attempt, the
veriﬁcation that a guess is correct would require to apply all
the decryption rounds until the plaintext is reconstructed,
with a great cost. We note that the user can cut down on
such cost if she locally maintains, in addition to the portions
of the original mini-blocks, also some bits of the partial re-
sults of the computation (which would allow her to test cor-
rectness of a guess without performing all the encryption
rounds). Availability of such partial results can help testing
the guesses for a mini-block if the other mini-blocks in the
same block are available (i.e., when the user misses only one
fragment per block). However, from the birthday paradox,
we note that the probability of two revocations hitting the
same block (but a diﬀerent fragment) quickly increases with
the number of revocations. Then, after a few updates the
advantage of the user keeping partial results of the compu-
223tation will become ineﬀective. In addition to this, we note
that, in this case as well, the storage and computational ef-
forts required to the user do not seem to make this attack
much preferable for her with respect to the choice of locally
storing the whole plaintext resource itself in the ﬁrst place.
A note on collusion. Collusion can happen when two
users join eﬀort to gain access to a resource that neither
of them can access (we do not consider collusions with
the server, which is assumed trustworthy to enforce the re-
writing requested by the owner). In fact, if one of the users is
authorized for the resource, she has no incentive and there-
fore there is no collusion. Also, the case of users getting
together to grant each other access to resources on which
they individually have authorization cannot be considered
collusion, since merging their knowledge they collectively do
not go beyond their privileges. Collusion is then represented
by users who join eﬀort in maintaining portions of the re-
source (e.g., fragments or parts of mini-blocks as discussed
above). For instance, each of the users could keep half of
the fragments and they can merge their knowledge to patch
for missing fragments. Such a situation does not add any
complication with respect to the previous discussion, as it
simply reduces to consider the group of colluding users as an
individual attacker. We then note again that the collective
eﬀort, in terms of storage and/or computation, required to
gain access would easily approximate the eﬀort of maintain-
ing the original plaintext resource itself. In other words, the
attack strategy does not bring advantage to the user.
5.
IMPLEMENTATION
In this section, we discuss the realization of our approach
for its practical deployment. The components that have
to be considered are the client, who decrypts resources to
access them (Section 5.1), and the protocol used for the in-
teraction between client and server. The protocol has a sig-
niﬁcant impact on the proﬁle of the server responsible for
hosting the resources and for authenticating the data owner
who is the only party authorized to modify the data.
In
particular, we will consider two options for the realization
of the interaction protocol: i) Overlay (Section 5.2), which
operates on top of a common cloud object service (the server
is unaware of the adoption of our approach and is a standard
object server); and ii) Ad-hoc (Section 5.3), which directly
supports the primitives to update a fragment and to get the
current state of the resource (the server is aware of the fea-
tures of our approach and attention will have to be paid to
its internal structure).
We found from this analysis that the client is able to make
use of our approach without restrictions, with a performance
in the application of the technique for a common personal
computer that makes it compatible with any network band-
width. For the protocol, when the technique is applied in
a transparent way on top of existing object storage solu-
tions (Overlay), we observe several orders of magnitude in
performance improvement for some conﬁgurations. The re-
alization of the technique using an ad-hoc protocol further
improves the beneﬁts with its greater ﬂexibility, but it also
requires to consider the mapping of the logical structure to
its physical representation, and we show how to identify an
adequate solution. All these results prove the applicabil-
ity of the technique in the current technological landscape
and the beneﬁts that it can provide for many application
domains.
It is important to observe that the parameters that mainly
inﬂuence the performance are the size msize of mini-block
and the number f of fragments. While the size of mini-blocks
represents our security parameter and must be chosen by the
data owner based on her security requirements, the number
of fragments is chosen considering performance only. In the
following, we will then focus on the tuning of the number
of fragments, considering resources of variable sizes. (Note
that the choice of the number of fragments implies also the
deﬁnition of the number of macro-blocks, as the product of
the number of macro-blocks by the number of fragments is
equal to the number of mini-blocks of the resource.)
The evaluation of the best value for the number of frag-
ments will have to consider a number of aspects that char-
acterize the application domain. The major ones are: fre-
quency of policy updates; frequency and average size of get
requests; network bandwidth, for the upload and download
direction. All these aspects have a direct impact on the over-
all throughput oﬀered by our solution, which conﬁrms its
advantage in the prompt enforcement of revoke operations,
measured by the average transfer rate for get requests.
The experimental results illustrated in this section have
been obtained using, for the client, a machine with Linux
Ubuntu 16.04 LTS, Intel i7-4770K, 3.50 GHz, 4 cores. For
the server, we used an Amazon EC2 m4.large instance, with
4 CPUs and 8 GB of RAM. The client was connected to the
Internet by a symmetric 100 Mbps connection.
5.1 Client
Our approach requires the client to execute a more com-
plex decryption compared to the use of AES with a tradi-
tional encryption mode (e.g., CTR or CBC). The cost of
decryption (which is comparable to the cost of encryption
by the data owner) is nearly logm (m · b) times the cost of
applying a single AES decryption, while the impact of reor-
ganizing the data structure at each round is limited. Thanks
to the high performance provided by modern processors in
the execution of block ciphers, this logarithmic cost factor is
not critical. Also, decryption can be parallelized on multi-
core CPUs, making the client processing even more eﬃcient.
An aspect that has to be considered in the implementation
of the client is the possible need to keep large amounts of
data in memory. This may occur when fragments are down-
loaded one after the other and decryption can start only
after the last fragment has been downloaded, which, for ex-
ample, happens with the Overlay solution. If the resource
size exceeds the available memory at the client, this leads
to an extremely signiﬁcant performance hit. The conﬁgura-
tion of the system can (and should) avoid this possibility by
splitting the resource into sub-resources (Section 5.2).
5.1.1 Experiments on the client
All code has been written in Python, because for all the
functions the computational performance is not a constraint.
The only component written in C was the invocation of the
mixing for encryption and decryption functions. Since most
current Intel x86 CPUs oﬀer the support for a hardware
implementation of AES, named AES-NI, we considered its
adoption in our experiments. Figure 9 shows that the cost
of decryption is compatible with all reasonable scenarios for
the application of our technique. In particular, the ﬁgure
224)
s
/
B
M
(
t
u
p
h
g
u
o
r
h
t
 10000
 1000
 100
 10
 1
1
AES-NI, msize=32, Msize=4096
AES-NI, msize=64, Msize=4096
AES, msize=32, Msize=4096
AES, msize=64, Msize=4096
2
4
number of threads
8
Figure 9: Throughput varying the number of
threads
illustrates the throughput obtained, varying the number of
threads, by the application of our approach in diﬀerent con-
ﬁgurations characterized by macro-blocks of size (Msize) 4
KiB, mini-blocks of size (msize) 32 and 64 bits (which im-
ply 5 and 9 encryption rounds, resp.), when using AES-
NI and when not using it (AES). Mixing was applied on
data that were already available in memory. We notice that
even the single-threaded 9-round non-hardware-supported
implementation (line ‘AES, msize=64, Msize=4096’) oﬀers
a throughput that is greater than 100 Mbps. For the AES-
NI multi-threaded 5-round implementation we reach a 2.5
GB/s throughput (line ‘AES-NI, msize=32, Msize=4096’).
The ﬁgure also shows that, increasing the number of threads,
we reach a performance level that is 4 times the one obtained
by the single-threaded implementation. This is consistent
with the presence of 4 physical cores in the CPU we used,
each with a dedicated AES-NI circuitry.
The performance, even for a large number of fragments,
shows to be orders of magnitude better than the band-
width of current network connections. Even without the
hardware support (lines ‘AES, msize=32, Msize=4096’ and
‘AES, msize= 64, Msize=4096’), the application of the cryp-
tographic transformation shows greater throughput than the
data transfer rate of most Internet connections. An exper-
iment on 1 GiB size macro-blocks and 32 bit mini-blocks
showed the expected slow down in throughput, managing
the decryption in less than 5 seconds (still above the band-
width of long-distance connections).
5.2 Overlay solution
The Overlay solution is analyzed using as a reference the
Swift service. Swift has been selected due to its popularity,
availability as open source, and technical features that are
good representatives of what is oﬀered by a modern object
storage service for the cloud (resources are called objects in
this discussion, to align with the Swift terminology). The
Swift server instance has been installed on the Amazon EC2
platform. We consider two main alternatives for the realiza-
tion of our approach on Swift1 without any changes to the
1Swift organizes objects within containers. The current
structure of Swift supports access control only at the level
of containers. The analysis we present can be immediately
adapted to the management of the access policy at the con-
tainer granularity rather than the object granularity. We
server.2 The ﬁrst option assumes to manage each fragment
as a separate object. The second option makes use of the
ability to access portions of objects and speciﬁcally considers
the use of Dynamic Large Objects (DLOs). Our experiments
show that this latter option provides signiﬁcant beneﬁts in
performance with respect to managing fragments as separate
objects. DLOs deserve then to be used when available.
Fragments as atomic separate objects. This approach
is the most adaptable one, as it can be used with any ob-
ject storage service. Also, the support for a policy update
will be immediate, as it will be mapped to a single update
to the object containing the corresponding fragment. How-
ever, these advantages come together with some potential
restrictions. The client would be responsible for managing
mixing and slicing. The approach requires the introduction
of some metadata associated with each of the fragments or
stored in a dedicated supporting object. The client has to
be able to concurrently access all the fragments of the object
to exhibit good performance when accessing large resources.
If there are many fragments, this requires to create and keep
open a large number of connections with the server.
Use of DLOs. The DLO service of Swift has been intro-
duced to support the management of large objects, going
beyond the size limits of storage devices and providing ﬁner
granularity in the access. When using DLOs, an object is
separated into a number of sub-objects that can be down-
loaded with a single request. The fragments of our approach
can then be stored into separate DLO fragments. The Swift
server is responsible for the management of the mapping
from an object to its fragments, splitting a request for down-
loading an object into a number of independent requests
to the server nodes that are responsible to store the data
(the Swift architecture has a server node directly oﬀering an
interface to the clients and uses a number of independent
storage nodes; this architecture provides redundancy and
availability). In this way, the client only generates a single
get request for the object, independently from the number
of fragments. The descriptor of the object can be extended
with the representation of the version of each fragment. A
similar approach can be realized when the object service of-
fers the ﬂexibility to operate with get and put only on a
portion of the object.
The major constraint of this approach is the need to wait
for the download of all the fragments before the decryption
of the ﬁrst macro-block can start. As anticipated in Sec-
tion 5.1, this causes delays and requires the client to keep
available in RAM the complete encrypted representation of
the object before it can be processed. To mitigate this prob-
lem, fragments can also be split into sub-fragments. In this
way, the download will be organized with a serial down-
load of all the sub-fragments representing the same set of
macro-blocks. This is consistent with approaches used in
cloud storage, where there is a common guideline to split
resources larger than a few GiB (Swift forces a split at 5
GiB in its standard conﬁguration). Experiments conﬁrm
that beyond 1 GiB, the throughput remains stable even for
conﬁgurations with a large number of fragments.
keep the analysis at the level of object to be consistent with
the discussion in the paper.
2We had to change a parameter in the server, to support a
large number of fragments in the DLO mode.
225)
s
(
e
m
i
t
 1000
 100
 10
 1
 0.1
 0.01
number of fragments
1024
256
64
16
4
1
64KB
256KB
1MB
4MB
16MB
64MB
256MB
1GB
number of fragments
1024
256
64
16
4
1
 1.2
 1
 0.8
 0.6
 0.4
 0.2
)
s
/
B
M
(
t
u
p
h
g
u
o
r
h
t
 0
64KB
256KB
1MB
4MB
16MB
64MB
256MB
1GB
object size
object size
Figure 10: Time for the execution of get requests on
Swift
Figure 11: Throughput for a workload combining
get and put_fragment requests on Swift
5.2.1 Experiments on the Overlay solution
We built a Swift client application in Python that imple-