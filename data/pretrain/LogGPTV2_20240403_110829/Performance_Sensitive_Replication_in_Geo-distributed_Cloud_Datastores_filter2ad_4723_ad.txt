back
0
50
100
150
200
250
300
Latency (msec)
Fig. 6. Comparing the performance of BA scheme
with Cassandra’s default random partitioner.
(a) Basic Availability
(b) N-1 Contingency
Fig. 7. Boxplot showing the distribution of read latency with BA and N-1C models
for every half hour period. Whiskers show the 10th and 90th percentiles.
were generated for each timeline object using the BA model
and the corresponding directory entries were created in all the
regions. Reads and writes were initiated as per the traces from
the Twissandra application servers deployed in each of the
EC2 regions. While the duration of the entire experiment was
scaled to 16 hours, care was taken to ensure that the fraction
of requests to all objects from each DC was proportional to
what was observed in the trace data.
Figure 6 shows the CDF comparing the read and write
latency observed with our BA scheme and Cassandra’s random
partitioner. The Y-Axis shows the CDF of the fraction of all
requests seen in the system (approx 6 million each for BA and
Random) while the X-Axis shows the observed per request
latency in msec. To ensure a fair comparison of schemes,
the observed latency values for BA includes directory lookup
latency as well. From the ﬁgure, we see that our ﬂexible
replication scheme is able to outperform the default replication
scheme by 50 msec (factor of 3) at the 50th%ile and by
100msec at the 90th%ile (factor of 2). A keen observer might
note that Random performs marginally better than (approx
3 − 8msec) BA at the initial percentiles due to the latency
overhead incurred for the directory lookup.
E. Availability and performance under failures
In this section, we study the performance of the BA and
N-1C schemes under the failures of different DCs using our
multi-region Cassandra cluster on EC2. We perform this study
using the trace data from Wikipedia for the English wiki
articles for which the accesses arrive from all the 8 EC2
regions including 50% from the US, 23% from Europe, 10%
from Singapore, 5% from Sydney and the rest from South
America and Tokyo. Failures were created by terminating the
Cassandra process in a DC and redirecting requests from the
application to the Cassandra process in the closest DC. The
duration of the experiment was approximately 9 hours.
For the English wiki articles, our BA scheme placed two
replicas in the west coast (USW-1a and USW-2a) and the
3rd replica in Tokyo (APN-1a) with R = 2 and W = 2.
This is reasonable since nodes in the US West are reasonably
equidistant from Asia, Australia, Europe and US East while
placing the 3rd replica in Asia also reduces the 90%ile latency
under normal operation. Figure 7(a) shows the performance
of the BA scheme under failure of different DCs. The corre-
sponding events for every half hour period is marked at the top
of the plots. From the ﬁgure, we see that the 90%ile latency
increases signiﬁcantly from 200msec (under normal operation)
to 280msec when the west coast DCs fail (40% increase),
while the failure of Tokyo DC (APN-1a) has only a marginal
impact on the performance.
In contrast, the N-1C scheme explicitly optimizes for latency
under a failure and places the 3rd replica in USW-1a instead
of Tokyo. Figure 7(b) shows the performance of the N-1C
scheme under failures of different DCs. The ﬁgure shows
that our N-1C scheme performs similar to the BA scheme
(median of 90msec and 90%ile of 200ms) during normal
operation. However, unlike the BA conﬁguration, the 90%ile
latency remains largely unaffected under all failures. Our
results highlight the need to explicitly optimize for perfor-
mance under failure and show the beneﬁts of N-1C over the
BA scheme. Further, the median and 90%ile latencies from
our experiments were found to be very close to our model
predictions under normal and failure conditions for both the
models, thereby validating our models.
IX. LARGE SCALE EVALUATION
We adopt a trace driven simulation approach for our large
scale evaluation on the three application traces, where we
consider the datastore cluster to comprise of nodes from each
of 27 distinct DCs world-wide, whose locations were obtained
from AWS Global Infrastructure [1]. Inter-DC delays were
measured between Planet-lab nodes close to each DC and
delay measurements were collected simultaneously between
all pairs of locations over a few hours and median delays
were considered. Users were mapped to the closest DCs as in
our EC2 experiments. We pick this extended set of DCs as the
EC2 regions are limited in number. For example, EC2 has no
regions in the Mid-west US, but AWS Global Infrastructure
provides multiple DCs in those areas. Moreover, we expect
these DCs to be expanded to offer more services in the future.
Experiments in this section use traces of one month (Dec 2010)
in Twitter, one month (Oct 2010) in Gowalla and one quarter
(Q4 2011) in Wikipedia.
247247247
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:03:40 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 8. Trace driven study with all keys in the application.
;ďͿĞŶĞĨŝƚƐŽĨŚĞƚĞƌŽŐĞŶĞŽƵƐƌĞƉůŝĐĂƚŝŽŶ
A. Performance of our optimal schemes
Figure 8(a) shows the CDF of the observed read latency
across both schemes for all keys in Twitter and Wikipedia
traces under normal and failure conditions. For each key, we
plot the read latency under normal conditions (all replicas
are alive) and when the most critical replica (replica whose
failure results in the worst latency) for that key fails. From
the ﬁgure, we see that the read latency observed by the BA
scheme deteriorates drastically under failure for almost all
keys in both the applications. For instance, more than 40% of
the keys in Twitter observed an increase of 50+ msec (more
than 20% of the keys observed an increase of 100+ msec in
Wikipedia) under failure conditions. However, read latency for
N-1C observed only a marginal variation under failure (most
keys in Twitter observed less than 30msec increase in latency
on its replica failures). Surprisingly, we ﬁnd that the N-1C
scheme incurs an almost negligible penalty in its latency under
normal conditions despite optimizing the replica conﬁguration
explicitly for the failure of a replica. Further, we found that
BA was often able to optimize latency with two of the chosen
replicas and the third choice did not signiﬁcantly impact
performance. In contrast, the N-1C scheme carefully selects
the 3rd replica ensuring good performance even under failures.
Overall, our results clearly show the beneﬁt of explicitly
optimizing the replication for failure conditions.
B. Need for heterogeneous conﬁguration policy
In this section, we highlight the importance of allowing
heterogeneous replica conﬁgurations in datastores and show
why a uniform replication conﬁguration for all data in the
application can often have poor performance. We analyzed the
conﬁgurations generated by N-1C for all keys in the Twitter
trace. From our analysis we ﬁnd that there were as many as
1985 distinct conﬁgurations (combination of replica location,
N , R, W ) that were used in the optimal solutions.
Interestingly, we ﬁnd that the beneﬁts are not only due to
optimizing the location of replicas but also due to careful
conﬁguration of the replication parameters - N , R and W . To
isolate such cases we consider a variant of our N-1C model that
we call 3−2−2 which has ﬁxed replication parameters N = 3,
R = 2 and W = 2, but allows ﬂexibility in the location of
the replicas. Figure 8(b) shows the difference in the access
latency between the 3 − 2 − 2 and N-1C schemes for Twitter.
The X-axis has the various replication factors observed in the
optimal solutions and each corresponding box plot shows the
25th, median and 75th percentiles (whiskers showing the 90th
percentile) of the difference in access latency between the two
schemes. Our results clearly show that a uniform conﬁguration
policy for all data in the application can be sub-optimal and
allowing heterogeneity in replica conﬁguration can greatly
lower the latency (as much as 70msec in some cases).
C. History-based vs Optimal
So far, we had assumed that the workloads for the appli-
cations are known. However, in practice, this may need to
be obtained from historical data. In this section, we analyze
this gap by comparing the performance of our schemes using
historical and actual workloads for all three applications.
Figure 9(a) shows the CDF comparing the performance
of Wikipedia during the ﬁrst quarter of 2012 when using
the history-based and the optimal replication conﬁguration.
The curves labeled history-based correspond to the read and
write latency observed when using the replica conﬁguration
predicted from the fourth quarter of 2011. The curves labeled
optimal correspond to the read and write latency observed
when using the optimal replica conﬁguration for the ﬁrst
quarter of 2012. Figures 9(b) and 9(c) show similar graphs
for Twitter and Gowalla. These ﬁgures show that history-
based conﬁguration performs close to optimal for Wikipedia
and Twitter, while showing some deviation from optimal per-
formance for Gowalla. This is because users in Gowalla often
move across geographical regions resulting in abrupt workload
shifts. For such abrupt shifts, explicit hints from the user when
she moves to a new location or automatically detecting change
in the workload and rerunning the optimization are potential
approaches for improving the performance.
D. Robustness to delay variations
Our experiments on EC2 (Section VIII) show that our
strategies are fairly robust to natural delay variations across
cloud DCs. In this section, we extend our analysis over a
larger set of keys. We compute about 1800 time snapshots
of the entire 27*27 inter-DCs delays for our extended DC set.
All delay values in the snapshot were measured approximately
at the same time. Next, we computed the optimal replica
conﬁgurations (using our BA and N-1C schemes) for 500
random keys from the Twitter trace for each of 1800 snapshots.
248248248
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:03:40 UTC from IEEE Xplore.  Restrictions apply. 
 1
 0.8
 0.6
 0.4
 0.2
)
s
y
e
k
f
o
n
o
i
t
c
a
r
f
(
F
D
C
History-based Read
Optimal Read
History-based Write
Optimal Write
 1
 0.8
 0.6
 0.4
 0.2
)
s
y
e
k
f
o
n
o
i
t
c
a
r
f
(
F
D
C
History-based Read
Optimal Read
History-based Write
Optimal Write
 1
 0.8
 0.6
 0.4
 0.2
)
s
y
e
k
f
o
n
o
i
t
c
a
r
f
(
F
D
C
History-based Read
Optimal Read
History-based Write
Optimal Write
 0
 0
 50
 100
 150
Latency (msec)
 200
 250
(a) Wikipedia
 0
 0
 50
 100
 150
Latency (msec)
(b) Twitter
 200
 250
 0
 0
 50
 100
 150
Latency (msec)
(c) Gowalla
 200
 250
Fig. 9. Optimal performance vs performance using replica placements from the previous period.
Fig. 10. Comparing SNAP and MED performance.
We call
these the SNAP conﬁgurations. Similarly, replica
conﬁgurations are computed using the median delay values
of the 1800 snapshots. We call these the MED conﬁgurations.
We then compare the performance of the MED conﬁguration
using delays observed at each snapshot with the performance
of the optimal SNAP conﬁguration at the same snapshot.
Figure 10 shows the CDF of the difference in access latency
between the MED and SNAP conﬁgurations. Each curve in the
ﬁgure corresponds to a range of latencies observed using the
SNAP conﬁgurations. For SNAP latencies less than 100msec,
and for over 90% of snapshots, MED only incurs less than
5msec additional latency. Also, for almost 80% of all the
SNAPs, the corresponding MED conﬁguration was optimal.
While the penalty is higher for SNAP latencies over 100 msec,
we believe they are still acceptable (less than 15msec for
90% of the cases) given the relatively higher SNAP latencies.
Overall, the results further conﬁrm our EC2 results and show
that delay variation impacts placement modestly.
E. Asymmetric read and write thresholds
Thus far, we assumed that read and write latencies are
equally desirable to optimize. However,
in practice, some
applications may prioritize read latencies, and others might
prioritize writes. We have explored solutions generated by our
approach when our models are modiﬁed to explicitly constrain
the read and write thresholds. For Twitter, we found that a
bound of 100msec on the write latency has no noticeable