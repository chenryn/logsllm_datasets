==== DecisionTreeClassifier
对决策树的详细说明参考wiki：
决策树（decision tree）是一个树结构（可以是二叉树或非二叉树）。其每个非叶节点表示一个特征属性上的测试，每个分支代表这个特征属性在某个值域上的输出，而每个叶节点存放一个类别。使用决策树进行决策的过程就是从根节点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的类别作为决策结果。
构造决策树的关键步骤是分裂属性。所谓分裂属性就是在某个节点处按照某一特征属性的不同划分构造不同的分支，其目标是让各个分裂子集尽可能地“纯”。尽可能“纯”就是尽量让一个分裂子集中待分类项属于同一类别。
对于分类问题，每次对一个节点进行分裂时，需要有一定的依据来保证我们的分裂方式是最佳或较好的。因而决策树的分裂依据有两种可以选择，分别为gini impurity（基尼不纯净度）和entropy（信息增益information gain）
Gini impurity：可以理解为对于所有样本，当前分类对每个样本分对的概率为fi，这样当越多的样本分类正确时，他们的平方和越趋近于1，不纯净度为0，说明当前分类正确性越好。
image::images/ml-gini-impurity.png[]
Entropy：我们通常用熵来表示信息增益，在特征选取时，信息增益越大的特征越重要。香农熵：
image::images/ml-entropy.png[]
Syntax:
    fit DecisionTreeClassifier [algo_params]  from  [into model_name]
Parameters:
* ：使用本模型对目标字段进行训练，必选
*  ：使用这些特征字段对目标字段进行分类和预测的训练，字段个数须>=1
* [into model_name]：保存下来的模型名称，可选，若不填将不做保存
* [algo_params]：针对DecisionTreeClassifier模型的参数设置，可选，不填时用默认参数设置
** criterion : string, optional (default=”gini”)
+
The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain.
** splitter : string, optional (default=”best”)
+
The strategy used to choose the split at each node. Supported strategies are “best” to choose the best split and “random” to choose the best random split.
** max_features : int, float, string or None, optional (default=None)
+
The number of features to consider when looking for the best split:
+
If int, then consider max_features features at each split.
+
If float, then max_features is a percentage and int(max_features * n_features) features are considered at each split.
+
If “auto”, then max_features=sqrt(n_features).
+
If “sqrt”, then max_features=sqrt(n_features).
+
If “log2”, then max_features=log2(n_features).
+
If None, then max_features=n_features.
+
Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max_featuresfeatures.
** max_depth : int or None, optional (default=None)
+
The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.
** min_samples_split : int, float, optional (default=2)
+
The minimum number of samples required to split an internal node:
+
If int, then consider min_samples_split as the minimum number.
+
If float, then min_samples_split is a percentage and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.
** min_samples_leaf : int, float, optional (default=1)
+
The minimum number of samples required to be at a leaf node:
+
If int, then consider min_samples_leaf as the minimum number.
+
If float, then min_samples_leaf is a percentage and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.
** min_weight_fraction_leaf : float, optional (default=0.)
+
The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.
** max_leaf_nodes : int or None, optional (default=None)
+
Grow a tree with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.
** random_state: int, RandomState instance or None, optional (default=None)
+
If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.
** min_impurity_split : float, optional (default=1e-7)
+
Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.
** presort: bool, optional (default=False)
+
Whether to presort the data to speed up the finding of best splits in fitting. For the default settings of a decision tree on large datasets, setting this to true may slow down the training process. When using either a smaller dataset or a restricted depth, this may speed up the training.
以某公司2010.1.12到2010.9.12之间在英国线上销售的交易记录举例, 样本数据字段描述如下:
1. InvoiceNo: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation. 
2. StockCode: Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product. 
3. Description: Product (item) name. Nominal. 
4. Quantity: The quantities of each product (item) per transaction. Numeric.
5. InvoiceDate: Invice Date and time. Numeric, the day and time when each transaction was generated. 
6. UnitPrice: Unit price. Numeric, Product price per unit in sterling. 
7. CustomerID: Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer. 
8. Country: Country name. Nominal, the name of the country where each customer resides.
对属性分析可以看出我们应该用其余7个feature来训练并预测第八个属性也就是购买物品订单所属国家。样本量一共20000条。
首先使用训练集中前2000条进行fit训练数据:
  appname:ml_spl AND tag:classification_ly | limit 2000 | where empty(json.UnitPrice) == false | eval price = todouble(json.UnitPrice) | fit DecisionTreeClassifier json.Country from json.InvoiceNo, json.StockCode, json.Description, json.Quantity, json.InvoiceDate, json.CustomerID, json.UnitPrice  into decisiontree_classifier | table json.Country, predicted_json.Country
image::images/ml-decisiontree-fit.png[]
  appname:ml_spl AND tag:classification_ly | sort by +timestamp | limit 2000 | where empty(json.UnitPrice) == false | eval price = todouble(json.UnitPrice) | apply decisiontree_classifier | table json.Country, predicted_json.Country
image::images/ml-decisiontree-apply.png[]
`summarymodel` 命令给出了训练后模型的基本情况，部分结果如下:
image::images/ml-decisiontree-summary.png[]
==== RandomForestClassifier
对随机森林的详细说明参考wiki： 
随机森林，指的是利用多棵树对样本进行训练并预测的一种分类器。它由多颗CART（Classification and Regression Tree）构成，对于每棵树，所使用的训练集是从总的训练集中有放回的随机采样出来的部分数据，构建决策树时选取的特征值是随机选取，在产生树的过程中节点的裂变方向也是随机选取方向的。正是由于以上原因，随机森林模型不易对训练集产生过拟合的问题。
随机森林的训练过程：
(1)给定训练集S，测试集T，特征维数F。确定参数：使用到的CART的数量t，每棵树的深度d，每个节点使用到的特征数量f，终止条件：节点上最少样本数s，节点上最少的信息增益m
对于第1-t棵树，i=1-t：
(2)从S中有放回的抽取大小和S一样的训练集S(i)，作为根节点的样本，从根节点开始训练
(3)如果当前节点上达到终止条件，则设置当前节点为叶子节点，如果是分类问题，该叶子节点的预测输出为当前节点样本集合中数量最多的那一类c(j)，概率p为c(j)占当前样本集的比例；如果是回归问题，预测输出为当前节点样本集各个样本值的平均值。然后继续训练其他节点。如果当前节点没有达到终止条件，则从F维特征中无放回的随机选取f维特征。利用这f维特征，寻找分类效果最好的一维特征k及其阈值th，当前节点上样本第k维特征小于th的样本被划分到左节点，其余的被划分到右节点。继续训练其他节点。有关分类效果的评判标准在后面会讲。
(4)重复(2)(3)直到所有节点都训练过了或者被标记为叶子节点。
(5)重复(2),(3),(4)直到所有CART都被训练过。
随机森林的预测：
(1)从当前树的根节点开始，根据当前节点的阈值th，判断是进入左节点(=th)，直到到达，某个叶子节点，并输出预测值。
(2)重复执行(1)直到所有t棵树都输出了预测值。如果是分类问题，则输出为所有树中预测概率总和最大的那一个类，即对每个c(j)的p进行累计；如果是回归问题，则输出为所有树的输出的平均值。
对于分类问题，CART使用Gini值作为评判标准，详细介绍见本文档上半部分。
Syntax:
    fit RandomForestClassifier [algo_params]  from  [into model_name]
Parameters:
* ：使用本模型对目标字段进行训练，必选
*  ：使用这些特征字段对目标字段进行分类和预测的训练，字段个数须>=1
* [into model_name]：保存下来的模型名称，可选，若不填将不做保存
* [algo_params]：针对RandomForestClassifier模型的参数设置，可选，不填时用默认参数设置
** n_estimators : integer, optional (default=10)
+
The number of trees in the forest.
** criterion : string, optional (default=”gini”)
+
The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. Note: this parameter is tree-specific.
** max_features : int, float, string or None, optional (default=”auto”)
+
The number of features to consider when looking for the best split:
+
If int, then consider max_features features at each split.
+
If float, then max_features is a percentage and int(max_features * n_features) features are considered at each split.
+
If “auto”, then max_features=sqrt(n_features).
+
If “sqrt”, then max_features=sqrt(n_features) (same as “auto”).
+
If “log2”, then max_features=log2(n_features).
+
If None, then max_features=n_features.
+
Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max_features features.
** max_depth : integer or None, optional (default=None)
+
The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.
** min_samples_split : int, float, optional (default=2)
+
The minimum number of samples required to split an internal node:
+
If int, then consider min_samples_split as the minimum number.
+
If float, then min_samples_split is a percentage and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.
** min_samples_leaf : int, float, optional (default=1)
+
The minimum number of samples required to be at a leaf node:
+
If int, then consider min_samples_leaf as the minimum number.
+
If float, then min_samples_leaf is a percentage and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.
** min_weight_fraction_leaf : float, optional (default=0.)
+
The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.
** max_leaf_nodes : int or None, optional (default=None)
+
Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.
** min_impurity_split : float, optional (default=1e-7)
+
Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.
** bootstrap : boolean, optional (default=True)
+
Whether bootstrap samples are used when building trees.
** oob_score : bool (default=False)
+
Whether to use out-of-bag samples to estimate the generalization accuracy.
** n_jobs : integer, optional (default=1)
+
The number of jobs to run in parallel for both fit and predict. If -1, then the number of jobs is set to the number of cores.
** random_state : int, RandomState instance or None, optional (default=None)
+
If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.
** verbose : int, optional (default=0)
+
Controls the verbosity of the tree building process.
** warm_start : bool, optional (default=False)
+
When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest.