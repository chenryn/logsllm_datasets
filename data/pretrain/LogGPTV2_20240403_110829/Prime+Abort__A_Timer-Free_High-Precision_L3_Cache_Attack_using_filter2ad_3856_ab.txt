a different address by another core necessarily evicts one
of the attacker’s cachelines, allowing PRIME+PROBE to
detect the access. Second, any cacheline evicted from
the L3 (e.g., in a “ﬂush” step) must also be invalidated
from all cores’ private L1 and L2 caches. Any subse-
quent access to the cacheline by any core must fetch the
data from main memory and bring it to the L3, causing
FLUSH+RELOAD’s subsequent “reload” phase to regis-
ter a cache hit.
[Set Index Bits] The total number of cache sets in each
cache can be calculated as (total number of cache lines)
/ (associativity), where the total number of cache lines is
(cache size) / (line size). Thus, the Skylake L1 caches
have 64 sets each, the L2 caches have 1024 sets each,
and the shared L3 has from 2K to 8K sets, depending on
the processor model.
In a typical cache, the lowest bits of the address (called
the “line offset”) determine the position within the cache
line; the next-lowest bits of the address (called the “set
index”) determine in which cache set the line belongs,
and the remaining higher bits make up the “tag”. In our
setting, the line offset is always 6 bits, while the set index
will vary from 6 bits (L1) to 13 bits (L3) depending on
the number of cache sets in the cache.
[Cache Slices and Selection Hash Functions] However,
in recent Intel architectures (including Skylake), the sit-
uation is more complicated than this for the L3. Specif-
ically, the L3 cache is split into several “slices” which
can be accessed concurrently; the slices are connected
on a ring bus such that each slice has a different latency
depending on the core. In order to balance the load on
these slices, Intel uses a proprietary and undocumented
hash function, which operates on a physical address (ex-
54    26th USENIX Security Symposium
USENIX Association
cept the line offset) to select which slice the address ‘be-
longs’ to. The output of this hash effectively serves as
the top N bits of the set index, where 2N is the number
of slices in the system. Therefore, in the case of an 8
MB L3 cache with 8 slices, the set index consists of 10
bits from the physical address and 3 bits calculated using
the hash function. For more details, see [25], [32], [44],
[16], or [22].
This hash function has been reverse-engineered for
many different processors in Intel’s Sandy Bridge [25,
32, 44], Ivy Bridge [16, 22, 32], and Haswell [22, 32]
architectures, but to our knowledge has not been reverse-
engineered for Skylake yet. Not knowing the precise
hash function adds additional difﬁculty to determining
eviction sets for PRIME+PROBE—that is, ﬁnding sets of
addresses which all map to the same L3 cache set. How-
ever, our attack (following the approach of Liu et al. [29])
does not require knowledge of the speciﬁc hash function,
making it more general and more broadly applicable.
2.2.2 Virtual Memory
In a modern virtual memory system, each process has a
set of virtual addresses which are mapped by the oper-
ating system and hardware to physical addresses at the
granularity of pages [2]. The lowest bits of an address
(referred to as the page offset) remain constant during
address translation. Pages are typically 4 KB in size, but
recently larger pages, for instance of size 2 MB, have be-
come widely available for use at the option of the pro-
gram [25, 29]. Crucially, an attacker may choose to
use large pages regardless of whether the victim does or
not [29].
Skylake caches are physically-indexed, meaning that
the physical address of a cache line (and not its virtual ad-
dress) determines the cache set which the line is mapped
into. Like the slicing of the L3 cache, physical indexing
adds additional difﬁculty to the problem of determining
eviction sets for PRIME+PROBE, as it is not immediately
clear which virtual addresses may have the same set in-
dex bits in their corresponding physical addresses. Pages
make this problem more manageable, as the bottom 12
bits (for standard 4 KB pages) of the address remain con-
stant during translation. For the L1 caches, these 12 bits
contain the entire set index (6 bits of line offset + 6 bits of
set index), so it is easy to choose addresses with the same
set index. This makes the problem of determining evic-
tion sets trivial for L1 attacks. However, L3 attacks must
deal with both physical indexing and cache slicing when
determining eviction sets. Using large pages helps, as the
21-bit large-page offset completely includes the set index
bits (meaning they remain constant during translation),
leaving only the problem of the hash function. However,
the hash function is not only an unknown function itself,
but it also incorporates bits from the entire physical ad-
Table 2: Availability of Intel TSX in recent Intel CPUs,
based on data drawn from Intel ARK [20] in January
2017. Since Broadwell, all server CPUs and a majority
of i7/i5 CPUs support TSX.
Server2
i7/i5
3/3 (100%)
23/32 (72%)
23/23 (100%)
Series
(Release1)
Kaby Lake
(Jan 2017)
Skylake
(Aug 2015)
Broadwell
(Sep 2014)
Haswell
(Jun 2013)
1 for the earliest available processors in the series
2 Xeon and Pentium-D
3 (i3/m/Pentium/Celeron)
77/77 (100%)
11/22 (50%)
37/85 (44%)
2/87 (2%)
27/42 (64%)
i3/m/etc3
12/24 (50%)
4/34 (12%)
2/18 (11%)
0/82 (0%)
dress, including bits that are still translated even when
using large pages.
2.3 Transactional Memory and TSX
Transactional Memory (TM) has received signiﬁcant at-
tention from the computer architecture and systems com-
munity over the past two decades [14, 13, 37, 45]. First
proposed by Herlihy and Moss in 1993 as a hardware al-
ternative to locks [14], TM is noteworthy for its simpliﬁ-
cation of synchronization primitives and for its ability to
provide optimistic concurrency.
Unlike traditional locks which require threads to wait
if a conﬂict is possible, TM allows multiple threads to
proceed in parallel and only abort in the event of a con-
ﬂict [36]. To detect a conﬂict, TM tracks each thread’s
read and write sets and signals an abort when a conﬂict is
found. This tracking can be performed either by special
hardware [14, 13, 45] or software [37].
Intel’s TSX instruction set extension for x86 [12, 19]
provides an implementation of hardware TM and is
widely available in recent Intel CPUs (see Table 2).
TSX allows any program to identify an arbitrary sec-
tion of its code as a ‘transaction’ using explicit XBEGIN
and XEND instructions. Any transaction is guaranteed to
either: (1) complete, in which case all memory changes
which happened during the transaction are made visible
atomically to other processes and cores, or (2) abort, in
which case all memory changes which happened during
the transaction, as well as all other changes (e.g. to reg-
isters), are discarded. In the event of an abort, control
is transferred to a fallback routine speciﬁed by the user,
and a status code provides the fallback routine with some
information about the cause of the abort.
From a security perspective,
the intended uses of
hardware transactional memory (easier synchronization
USENIX Association
26th USENIX Security Symposium    55
2.
3.
4.
5.
6.
Table 3: Causes of transactional aborts in Intel TSX
Executing certain instructions, such as CPUID or the explicit
1.
XABORT instruction
Executing system calls
OS interrupts1
Nesting transactions too deeply
Access violations and page faults
Read-Write or Write-Write memory conﬂicts with other
threads or processes (including other cores) at the cacheline
granularity—whether those other processes are using TSX or
not
A cacheline which has been written during the transaction
(i.e., a cacheline in the transaction’s “write set”) is evicted
from the L1 cache
A cacheline which has been read during the transaction (i.e.,
a cacheline in the transaction’s “read set”) is evicted from the
L3 cache
7.
8.
1 This means that any transaction may abort, despite the absence of
memory conﬂicts, through no fault of the programmer. The pe-
riodic nature of certain interrupts also sets an effective maximum
time limit on any transaction, which has been measured at about
4 ms [41].
or optimistic concurrency) are unimportant, so we will
merely note that we can place arbitrary code inside both
the transaction and the fallback routine, and whenever
the transaction aborts, our fallback routine will imme-
diately be given a callback with a status code. There
are many reasons a TSX transaction may abort; impor-
tant causes are listed in Table 3. Most of these are drawn
from the Intel Software Developer’s Manual [19], but the
speciﬁcs of Causes #7 and #8—in particular the asym-
metric behavior of TSX with respect to read sets and
write sets—were suggested by Dice et al. [3]. Our exper-
imental results corroborate their suggestions about these
undocumented implementation details.
While a transaction is in process, an arbitrary amount
of data must be buffered (hidden from the memory sys-
tem) or tracked until the transaction completes or aborts.
In TSX, this is done in the caches—transactionally writ-
ten lines are buffered in the L1 data cache, and transac-
tionally read lines marked in the L1–L3 caches. This
has the important ramiﬁcation that the cache size and
associativity impose a limit on how much data can be
buffered or tracked.
In particular, if cache lines being
buffered or tracked by TSX must be evicted from the
cache, this necessarily causes a transactional abort. In
this way, details about cache activity may be exposed
through the use of transactions.
TSX has been addressed only rarely in a security
context; to the best of our knowledge, there are only
two works on the application of TSX to security to
date [9, 24]. Guan et al. use TSX as part of a defense
against memory disclosure attacks [9]. In their system,
operations involving the plaintext of sensitive data nec-
essarily occur inside TSX transactions. This structurally
ensures that this plaintext will never be accessed by other
processes or written back to main memory (in either case,
a transactional abort will roll back the architectural state
and invalidate the plaintext data).
Jang et al. exploit a timing side channel in TSX itself
in order to break kernel address space layout randomiza-
tion (KASLR) [24]. Speciﬁcally, they focus on Abort
Cause #5, access violations and page faults. They note
that such events inside a transaction trigger an abort but
not their normal respective handlers; this means the op-
erating system or kernel are not notiﬁed, so the attack is
free to trigger as many access violations and page faults
as it wants without raising suspicions. They then exploit
this property and the aforementioned timing side chan-
nel to determine which kernel pages are mapped and un-
mapped (and also which are executable).
Neither of these works enable new attacks on memory
accesses, nor do they eliminate the need for timers in
attacks.
3 Potential TSX-based Attacks
We present three potential attacks, all of which share
their main goal with cache attacks—to monitor which
cachelines are accessed by other processes and when.
The three attacks we will present leverage Abort Causes
#6, 7, and 8 respectively. Figure 1 outlines all three of
the attacks we will present, as the PRIME+ABORT en-
try in the ﬁgure applies to both PRIME+ABORT–L1 and
PRIME+ABORT–L3.
All of the TSX-based attacks which we will propose
have the same critical structural beneﬁt in common. This
beneﬁt, illustrated in Figure 1, is that these attacks have
no need for a “measurement” phase. Rather than having
to conduct some (timed) operation to determine whether
the cache state has been modiﬁed by the victim, they sim-
ply receive a hardware callback through TSX immedi-
ately when a victim access takes place.
In addition to
the reduced overhead this represents for the attack pro-
cedure, this also means the attacker can be actively wait-
ing almost indeﬁnitely until the moment a victim access
occurs—the attacker does not need to break the attack
into predeﬁned intervals. This results in a higher res-
olution attack, because instead of only coarse-grained
knowledge of when a victim access occurred (i.e. which
predeﬁned interval), the attacker gains precise estimates
of the relative timing of victim accesses.
All of our proposed TSX-based attacks also share a
structural weakness when compared to PRIME+PROBE
they are unable
and FLUSH+RELOAD.
to monitor multiple
in the
case of PRIME+PROBE, addresses in the case of
FLUSH+RELOAD) simultaneously while retaining the
ability to distinguish accesses to one target from ac-
cesses to another. PRIME+PROBE and FLUSH+RELOAD
Namely,
targets
(cache
sets
56    26th USENIX Security Symposium
USENIX Association
are able to do this at the cost of increased overhead;
effectively, a process can monitor multiple targets con-
currently by performing multiple initialization stages,
having a common waiting stage, and then performing
multiple measurement stages, with each measurement
stage revealing the activity for the corresponding target.
In contrast, although our TSX-based attacks could
monitor multiple targets at once, they would be unable
to distinguish events for one target from events for
another without additional outside information. Some
applications of PRIME+PROBE and FLUSH+RELOAD
rely on this ability (e.g. [33]), and adapting them to
rely on PRIME+ABORT instead would not be triv-
ial. However, others, including the attack presented
in Section 4.4, can be straightforwardly adapted to
utilize PRIME+ABORT as a drop-in replacement for
PRIME+PROBE or FLUSH+RELOAD.
We begin by discussing the simplest, but also
least generalizable, of our TSX-based attacks, ul-
timately building to our proposed primary attack,
PRIME+ABORT–L3.
3.1 Na¨ıve TSX-based Attack
Abort Cause #6 enables a potentially powerful, but lim-
ited attack.
From Cause #6, we can get a transaction abort (which
for our purposes is an immediate, fast hardware callback)
whenever there is a read-write or write-write conﬂict be-
tween our transaction and another process. This leads
to a natural and simple attack implementation, where we
simply open a transaction, access our target address, and
wait for an abort (with the proper abort status code); on
abort, we know the address was accessed by another pro-
cess.
style
of
this
is
The
attack
reminiscent
of
FLUSH+RELOAD [43] in several ways.
It targets a
single, precise cacheline, rather than an entire cache
set as in PRIME+PROBE and its variants.
It does not
require a (comparatively slow) “prime eviction set”
step, providing fast and low-overhead monitoring of
the target cacheline. Also like FLUSH+RELOAD,
it
requires the attacker to acquire a speciﬁc address to
target, for instance exploiting shared libraries or page
deduplication.
Like the other attacks using TSX, it beneﬁts in per-
formance by not needing the “measurement” phase to
detect a victim access. In addition to the performance
beneﬁt, this attack would also be harder to detect and de-
fend against. It would execute without any kind of timer,
mitigating several important classes of defenses (see Sec-
tion 5). It would also be resistant to most types of cache-
based defenses; in fact, this attack has so little to do with
the cache at all that it could hardly be called a cache at-
tack, except that it happens to expose the same informa-
tion as standard cache attacks such as FLUSH+RELOAD
or PRIME+PROBE do.
However,
in addition to only being able to moni-
tor target addresses in shared memory (the key weak-
ness shared by all variants of FLUSH+RELOAD), this
attack has another critical shortcoming. Namely, it can
only detect read-write or write-write conﬂicts, not read-
read conﬂicts. This means that one or the other of the
processes—either the attacker or the victim—must be is-
suing a write command in order for the access to be de-
tected, i.e. cause a transactional abort. Therefore, the
address being monitored must not be in read-only mem-
ory. Combining this with the earlier restriction, we ﬁnd
that this attack, although powerful, can only monitor ad-
dresses in writable shared memory. We ﬁnd this depen-