example, EWMA (Exponentially Weighted Moving Average) [11],
a prediction based detector, has only one weight parameter ↵ 2
[0, 1]. As ↵ goes up, the prediction relies more upon the recent
data than the historical data. Consequently, we can sample ↵ 2
{0.1, 0.3, 0.5, 0.7, 0.9} to obtain 5 typical features from EWMA.
As for the detectors with multiple parameters and a large parameter
space, we can reduce the sampling granularity. For example,
Holt-Winters has three [0,1] valued parameters ↵, β, and γ. To
limit the number of samples, we can choose {0.2, 0.4, 0.6, 0.8}
for ↵, β, and γ, leading to 43 = 64 features. Other types of
detectors may need window parameters, and we can adopt windows
of several points, days, or weeks according to the characteristics
of the detectors. For example, moving average based detectors
with a short window aim at identifying local anomalies, while
time series decomposition [1] usually uses a window of weeks to
capture long-term violations. Although such sampling strategies
do not guarantee that we can ﬁnd the most suitable parameters (or
features) due to the relatively coarse sampling granularity, we only
need a set of good enough features, and Opprentice can achieve a
promising accuracy by combining them (§5).
On the other hand, the parameters of some complex detectors,
e.g., ARIMA (Autoregressive Integrated Moving Average) [10],
can be less intuitive. Worse, their parameter spaces can be too
large even for sampling. To deal with such detectors, we estimate
their “best” parameters from the data, and generate only one
set of parameters, or one conﬁguration for each detector. The
estimation method is speciﬁcally designed for each such detector.
For example, [35,36] provide the parameter estimation for ARIMA.
Besides, since the data characteristics can change over time, it is
also necessary to update the parameter estimates periodically.
4.4 Machine Learning Algorithm
4.4.1 Considerations and Choices
We need to be careful when choosing machine learning al-
gorithms. This is because in our problem, there are redundant
and irrelevant features, caused by using detectors without careful
evaluation. Some learning algorithms such as naive Bayes, logistic
regression, decision tree, and linear SVM, will perform badly
when coping with such training data (§5.3.2). Additionally, a
promising algorithm should be less-parametric and insensitive to
its parameters, so that Opprentice can be easily applied to different
data sets.
In this paper, we choose random forests [28], which
has been proved to be robust to noisy features and work well
in practice [28, 37]. Furthermore, random forests have only two
parameters and are not very sensitive to them [38]. Our evaluation
results also show that the random forest algorithm perform better,
and are more stable than other commonly-used algorithms.
Note that we do understand that feature selection [39, 40] is a
commonly used solution to mitigate the inﬂuences of irrelevant and
redundant features. However, we do not explore feature selection in
this paper and consider it as future work, because it could introduce
extra computation overhead, and the random forest works well by
itself (§5.3.2).
4.4.2 Random Forest
In the interest of space, we only introduce some basic ideas of
random forests. More details are in [28].
Preliminaries: decision trees. A decision tree [41] is a popular
learning algorithm as it is simple to understand and interpret. It
has been used widely to uncover the relationships between features
and labels [42, 43]. At a high level, it provides a tree model with
various if-then rules to classify data. Fig. 5 shows a compacted
decision tree learned from our SRT data set. The tree contains
three if-then rules on the features of three detectors, i.e., time series
decomposition, singular value decomposition, and diff (See §5.2
for the details of the detectors). The numbers on branches, e.g.,
3 for time series decomposition, are the feature split points. The
tree is then used to classify incoming data. The tree is greedily
built top-down. At each level, it determines the best feature and
its split point to separate the data into distinct classes as much as
possible, or produce the “purest” sub nodes. A goodness function,
e.g., information gain and gini index, is used for quantifying such
an ability of each feature. The tree grows in this way until every leaf
node is pure (fully grown). In the decision tree, a feature is more
important for classiﬁcation if it is closer to the root. For example,
in Fig. 5, the feature of time series decomposition is most effective
to distinguish different data.
There are two major problems of decision tree. One is that the
greedy feature selection at each step may not lead to a good ﬁnal
classiﬁer; the other is that the fully grown tree is very sensitive
to noisy data and features, and would not be general enough to
classify future data, which is called overﬁtting. Some pruning
solutions have been proposed to solve overﬁtting. For example,
stop growing the tree earlier after it exceeds a threshold of depth.
However, it is still quite tricky to determine such a threshold.
Figure 5: Decision tree example. The rectangles represent the
features extracted by different detectors and the ellipses are the
classiﬁcation results.
A Random forest is an ensemble classiﬁer using many decision
trees.
Its main principle is that a group of weak learners (e.g.,
individual decision trees) can together form a strong learner [44].
To grow different trees, a random forest adds some elements or
randomness. First, each tree is trained on subsets sampled from the
original training set. Second, instead of evaluating all the features
at each level, the trees only consider a random subset of the features
each time. As a result, some trees may be not or less affected
by the irrelevant and redundant features if these features are not
used by the trees. All the trees are fully grown in this way without
pruning. The random forest then combines those trees by majority
vote. That is, given a new data point, each of the trees gives its own
classiﬁcation. For example, if 40 trees out of 100 classify the point
into an anomaly, its anomaly probability is 40%. By default, the
random forest uses 50% as the classiﬁcation threshold (i.e., cThld).
The above properties of randomness and ensemble make random
forests more robust to noises and perform better when faced with
irrelevant and redundant features than decision trees.
4.5 Conﬁguring cThlds
4.5.1 PC-Score: A Metric to Select Proper cThlds
We need to conﬁgure cThlds rather than using the default one
(e.g., 0.5) for two reasons. First, when faced with imbalanced data
(anomalous data points are much less frequent than normal ones
in data sets), machine learning algorithms typically fail to identify
the anomalies (low recall) if using the default cThlds [31]. Second,
operators have their own preference regarding the precision and re-
call of anomaly detection. Conﬁguring cThlds is a general method
to trade off between precision and recall [31]. In consequence, we
should conﬁgure the cThld of random forests properly to satisfy the
operators’ preference.
Figure 6: PR curve of a random forest trained and tested on the
PV data. Different methods select different cThlds and result
in different precision and recall. The two shaded rectangles
represent two types of assumed operators’ preferences.
Before describing how we conﬁgure the cThld, we ﬁrst use
Precision-Recall (PR) curves to provide some intuitions. PR curves
is widely used to evaluate the accuracy of a binary classiﬁer [45],
especially when the data is imbalanced.3 A PR curve plots preci-
sion against recall for every possible cThld of a machine learning
algorithm (or for every sThld of a basic detector). Typically,
there is a trade-off between precision and recall. Fig. 6 shows
an exemplary PR curve derived from a random forest trained and
tested on PV data. Two types of assumed operators’ preferences
(1) “recall ≥ 0.75 and precision ≥ 0.6” and (2) “recall ≥ 0.5
and precision ≥ 0.9” are represented by the shaded rectangles.
Conﬁguration of cThlds is to seek a proper point on the PR curve.
In Fig. 6, the triangle symbol is selected by the default cThld 0.5.
Besides, we also show the results of another two accurate metrics:
a F-Score based method, which selects the point that maximizes
F-Score = 2·precision·recall
precision+recall ; SD(1,1), a metric similar to that
in [46], which selects the point with the shortest Euclidean distance
to the upper right corner where the precision and the recall are
both perfect. We see that in Fig. 6, the PR curve has points inside
both the rectangles, however, the default threshold only satisﬁes
the preference (2) but not (1); F-Score and SD(1,1) do not satisfy
3A similar method is Receiver Operator Characteristic (ROC)
curves, which show the trade-off between the false positive rate
(FPR) and the true positive rate (TPR). However, when dealing
with highly imbalanced data sets, PR curves can provide a more
informative representation of the performance [45].
Severities measured by time series decomposition=3= 64>= 61 1
, i = 1
cThldp
5-fold prediction
i−1 + (1 − ↵) · cThldp
i−1
i =( ↵ · cThldb
i−1 is the best cThld of the (i − 1)th week. cThldp
cThldb
i is the
predicted cThld of the ith week, and also the one used for detecting
the ith-week data. ↵ 2 [0, 1] is the smoothing constant. For
the ﬁrst week, we use 5-fold cross-validation to initialize cThldp
1.
EWMA is simple but effective here as it does not require a lot of
historical data to start. As ↵ increases, EWMA gives the recent
best cThlds more inﬂuences in the prediction. We use ↵ = 0.8
in this paper to quickly catch up with the cThld variation. Our
results show that the EWMA based cThld prediction method gains
a noticeable improvement when compared with the 5-fold cross-
validation (§5.6).
5. EVALUATION
We implement Opprentice and 14 detectors with about 9500
lines of python, R, and C++ code. The machine learning block
is based on the scikit-learn library [48]. In this section, we evaluate
Opprentice using three kinds of KPI data from a top global search
engine. These data are labeled by the operators using our labeling
tool.
Fig. 8 shows the evaluation ﬂow. In the ﬁrst four steps, we com-
pare the accuracy of each component of Opprentice with different
approaches. The accuracy of Opprentice as a whole is shown in the
§5.6. In addition to the accuracy, the qualitative goal of Opprentice,
i.e., being automatic, is also evaluated through directly applying
Opprentice to three different KPIs without tuning. The only manual
effort is to label the KPI data. We also interviewed the operators
about their previous detector tuning time, and compare it with the
labeling time (§5.7). Last, we evaluate the online detecting lag
and the ofﬂine training time of Opprentice (§5.8). Next we ﬁrst
describe the data sets in §5.1 and the detectors we select in §5.2,
then we show the evaluation results.
Figure 7: Best cThld of each week from the 9th week. The ﬁrst
8-week data are used as the initial training set.
Our method is motivated by another observation in Fig. 7. That
is, though the best cThlds changes over weeks, they can be more
similar to the ones of the neighboring weeks. A possible explana-
tion is that the underlying problems that cause KPI anomalies might
last for some time before they are really ﬁxed, so the neighboring
weeks are more likely to have similar anomalies and require similar