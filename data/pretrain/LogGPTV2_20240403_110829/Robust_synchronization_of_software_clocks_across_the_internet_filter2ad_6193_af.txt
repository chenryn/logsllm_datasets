−0.06
−0.05
−0.04
Offset Error  [ms]
−0.03
−0.02
−0.01
0
−0.12
−0.1
−0.08
−0.06
−0.04
Offset Error  [ms]
−0.02
0
Figure 12: Performance of offset errors over a 3 month period
using ServerInt, with polling period: 64 (left) and 256 (right)
seconds. The histograms shows exactly 99% of all values (note
scale change). τ (cid:10) = 2τ ∗
.
Unforseen Events: There is a need to ensure robustness to un-
foreseen and extreme events including loss of connectivity, a change
in the route to host or a change in server, unexpected change of the
temperature environment, or even errors in the server’s clock.
As it is impossible to forsee all scenarios, it is important that the
robustness is built in in very generic ways, rather than via dedicated
algorithmic branches. Many such robustness features have already
been described in the previous section. An important additional
consideration which we have yet to address is that of level shifts in
the data, which can occur due to route or server changes. Exam-
ples of each of these occured in our experimental data. Due to its
importance and relative complexity we devote the next subsection
to this topic.
Figure 11 zooms in on extreme events which occurred during
a continuous measurement period which extends that of ﬁgure 12
to include an additional gap of 6 days, followed by the change to
ServerLoc for 1 week, and then to ServerExt. Figure 11(a) demon-
strates the fast recovery of the algorithm even after the 3.8 day gap
in data collection (simulating server unavailability). Figure 11(b)
shows the impact of a server error lasting a few minutes, during
which Tb,i and Te,i were each offset by 150ms. As errors in server
timestamps do not affect the RTT measurements at the host, this is
very difﬁcult to detect and account for. However, the offset (and
local rate) sanity check algorithm was triggered, which limited the
damage to a millisecond or less.
, d↑
6.2 Robustness to Level Shifts
By level shift we mean principally a change in any of the mini-
mum delays d→
(see equation (12)), and hence r, which
results in a change in minimum level in some or all of the observed
i , d↑
d→
i or ri. We will continue to restrict ourselves to RTT as
the basis of packet quality measurement, and therefore level shift
detection.
i , d←
or d←
We ﬁrst discuss the key issues governing level shifts.
indistinguishable from congestion at small scales, becomes
Asymmetry of shift direction:
These are fundamentally distinct and must be treated differently:
Down: congestion cannot result in a downward movement, so the
two can be unambiguously distinguished → easy detection.
Up:
reliable only at large scales → difﬁcult detection.
Asymmetry of detection errors:
The impact of an incorrect decision is dramatically different:
Judge quality packet as bad: an undetected upward shift looks like
congestion, to which the algorithms must already be robust → non-
critical.
Judge bad quality packet as good: falsely interpreting congestion
as an upward shift immediately corrupts estimates, perhaps very
badly → critical to avoid.
Asymmetry of offset and rate:
Offset: underlying naive estimates ˆθi remain valid to the ˆθ(t) al-
gorithm even after a future shift. → store past estimates and their
point errors relative to the ˆr estimate made at the time.
Rate: ˆp and ˆpl estimates are made between a pair of packets, so
must compare them using a common point error base → use point
errors relative to current error level (after any shifts).
If the procedures of the last paragraph are followed, few addi-
tional steps are needed to assemble a robust detection and reaction
scheme for level shifts.
(a)
(b)
]
s
m
[
s
e
a
m
t
i
t
s
e
t
e
s
f
f
O
137.1
137.08
137.06
137.04
137.02
137
136.98
136.96
136.94
136.92
136.9
67
3.8 day gap ends
67.05
67.1
Tb [day]
67.15
139
138.5
138
]
s
m
[
s
e
t
a
m
i
t
s
e
t
e
s
f
f
O
137.5
135.5
135
134.5
]
s
m
[
s
e
t
a
m
i
t
s
e
t
e
s
f
f
O
(c)
back Down
0.9ms Up
0.9ms Up
After detection
(d)
]
s
m
[
s
e
t
a
m
i
t
s
e
t
e
s
f
f
O
104.5
104.4
104.3
104.2
104.1
104
103.9
103.8
103.7
103.6
103.5
shift 0.36ms Down
108.45
108.5
108.55
108.6
Tb  [day]
108.65
108.7
108.75
server offset error
61.58
61.6
61.62
Tb [day]
61.64
61.66
134
79.85 79.9 79.95
80
80.05 80.1 80.15 80.2 80.25
Tb  [day]
Figure 11: Performance of algorithm under extreme conditions (smooth thicker curves are reference offsets, variable curves with
arrows are estimated offsets): (a) loss of data over 3 days, (b) level shift error of 150 ms in the server clock (triggers sanity check),
(c) artiﬁcial temporary and permanent upward level shifts inducing change in ∆, (d) real permanent downward level shift using
ServerExt with ∆ constant. τ (cid:10) = 2τ ∗
, ¯τ = 5τ ∗
, Ts = ¯τ /2.
The two shift directions are treated separately:
The Level Shift Algorithm:
Down:
Detection: Automatic and immediate when using ˆr.
Reaction: Offset: no additional steps required.
Rate: No additional steps required. The algorithms will see the
shift as poor quality of past packets and react normally. In time, in-
creasing ∆(t) and windowing will improve packet qualities again.
Up:
Detection: Based on maintaining a local minimum estimate ˆrl over
a sliding window of width Ts. Unambiguous detection is difﬁcult
and the consequences of incorrect detection serious. We therefore
choose Ts large, Ts = ¯τ /2, and detect a shift (at t = C(Tf,i)− Ts)
if |ˆrl − ˆr| > 4E.
Reaction: First update ˆr = ˆrl (and on-line window estimate), and
recalculate ˆθi values and reassess their point qualities back to the
shift point. Otherwise no additional steps required. Before detec-
tion, the algorithms will see the packets as having poor quality, and
react as normal. Since the window is large, estimates may start to
degrade toward the end of the window.
In ﬁgure 11(c) two upward shifts of 0.9ms were artiﬁcially intro-
duced. The ﬁrst, being under Ts in duration, was never detected and
makes little impact on the estimates. The second was permanent.
Occurring at 80.04 days, it was detected a time Ts later, resulting
in a jump in subsequent offset estimates (the original on-line es-
timates, not the recalculated ones, are shown). Most of this jump
is due not to estimation difﬁculties resulting from the shift but to
the change in ∆ of 0.9/2 = 0.45 ms, as the shifts were induced
in the host→server direction only. In constrast, the natural perma-
nent shift in ﬁgure 11(d) occurs equally in each direction, so that
∆ does not change, and is also downward, so that detection and
reaction are immediate. The result is no observable change in es-
timation quality, the shift is absorbed with no impact on estimates
and with no level-shift speciﬁc actions being taken.
7. CONCLUSION
We have presented a detailed reexamination of the problem of
inexpensive, convenient yet accurate clock synchronization for net-
worked PCs. It is based on a thorough understanding of the sta-
bility of the CPU oscillator as a timing source, accessible via the
TSC register. Using the NTP server network together with TSC
timestamps, we showed how to calibrate a clock based on the TSC
both in terms of rate, essential to the measurement of time differ-
ences, and offset, that is absolute time. We explained the impor-
tance of maintaining distinct software clocks for each of these dis-
tinct tasks. Our approach is new in several respects, notably in its
being explicitly rate rather than offset-centric, grounded in a mean-
ingful time-scale analysis of the drift in the underlying hardware,
and through the development of ﬁltering algorithmns built explic-
itly on the above with relatively few additional ad-hoc elements.
Together these allow considerably higher levels of accuracy and
more importantly, reliability.
Using months of real data from 3 different NTP servers, we pro-
vided a systematic and thorough testing of the algorithm, its ab-
solute performance and sensitivity to parameters. Using a nearby
server, we were able to reliably absolutely synchronize to the order
of 30µs, and obtain rate accuracy of around 0.02 PPM. We demon-
strated the robustness of the techniques to such effects as packet
loss and loss of server connectivity, changes in server, network con-
gestion, temperature environment, timestamping noise, and even
faulty server timestamps. The approach should allow many appli-
cations requiring accurate and reliable timing, in particular network
measurement, to do away with the cost of hardware based synchro-
nization, such as using GPS receivers. The stability analysis, ﬁl-
tering principles, and algorithms discussed here also provide a ﬁrm
basis for a new generation of software clock, with NTP server based
synchronization, for networked PCs and other devices. They could
easily be adapted for use with other kinds of oscillators, and other
kinds of reference time servers. They could also be used to im-
prove the existing SW clock, however the simplest way to do so is
to simply replace it with a complete TSC-NTP clock solution.
Both the RIPE NCC Test Trafﬁc Measurement project [6], and
CAIDA’s Skitter project [13], have agreed to trial the methods de-
scribed here, the former to enable the expensive GPS component to
be replaced (or made more reliable by replacing the SW-GPS with
a ‘TSC-GPS’ clock), and the latter to replace the existing SW-NTP
solution which was found to be unreliable. In future work we hope
to collaborate with RIPE NCC and CAIDA to benchmark the per-
formance of their implementations, and comprehensively compare
them to those of the original clocks. Software will be made avail-
able for download, for trial and use by the community at [7]. It is
expected that a version will be available in time for IMC-2004.
8. ACKNOWLEDGEMENTS
The BSD timestamping code was developed and implemented by
Michael Dwyer. We thank David Batterham and Karl Cirulis at the
University of Melbourne for their assistance with the temperature
logging. This work was supported by Ericsson.
9. REFERENCES
[1] D.L. Mills, “Internet time synchronization: the network time
protocol,” IEEE Trans. Communications, vol. 39, no. 10, pp.
1482–1493, October 1991, Condensed from RFC-1129.
[2] C. Liao et al., “Experience with an adaptive globally
[8] J¨org Micheel, Ian Graham, and Stephen Donnelly, “Precision
synchronizing clock algorithm,” in Proc. of 11th ACM Symp.
on Parallel Algorithms and Architectures, June 1999.
[3] Vern Paxson, “On calibrating measurements of packet transit
times,” in Proceedings of the 1998 ACM SIGMETRICS joint
international conference on Measurement and modeling of
computer systems. 1998, pp. 11–21, ACM Press.
timestamping of network packets,” in Proc. of the
SIGCOMM IMW, November 2001.
[9] D.L. Mills, “The network computer as precision timekeeper,”
in Proc. Precision Time and Time Interval (PTTI)
Applications and Planning Meeting, Reston VA, December
1996, pp. 96–108.
[4] Jonas Andren, Magnus Hilding, and Darryl Veitch,
[10] Attila P´asztor and Darryl Veitch, “A precision infrastructure
“Understanding end-to-end internet trafﬁc dynamics,” in
IEEE Global Telecommunications Conference
(Globecom’98), Sydney, Australia, Nov. 1998, vol. 2, pp.
1118–1122.
[5] Attila P´asztor and Darryl Veitch, “PC based precision timing
without GPS,” in Proceeding of ACM Sigmetrics 2002
Conference on the Measurement and Modeling of Computer
Systems, Del Rey, California, 15–19 June 2002, pp. 1–10.
[6] “RIPE NCC Test Trafﬁc Measurements,”
http://www.ripe.net/ttm/.
[7] Attila P´asztor and Darryl Veitch, “Software infrastructure for
accurate active probing,” http://www.cubinlab.ee.
mu.oz.au/probing/software.shtml/.
for active probing,” in Passive and Active Measurement
Workshop (PAM2001), Amsterdam, The Netherlands, 23–24
April 2001, pp. 33–44.
[11] Victor Yodaiken, “The RTLinux Manifesto,” Tech. Rep.,
Department of Computer Science, New Mexico Institute of
Technology, 1999, available at http://www.rtlinux.org.
[12] P. Abry, D. Veitch, and P. Flandrin, “Long-range
dependence: revisiting aggregation with wavelets,” Journal
of Time Series Analysis, vol. 19, no. 3, pp. 253–266, May
1998, Bernoulli Society.
[13] “Cooperative Association for Internet Data Analysis
(CAIDA) Skitter project,” http://www.caida.org/
tools/measurement/skitter/.