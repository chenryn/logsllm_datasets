we discovered 5.9 K Chargen ampliﬁers, 6.4 K NTP am-
pliﬁers, and 71.5 K DNS ampliﬁers on 83.8 K distinct IP
addresses. We continued to track these hosts by perform-
USENIX Association  
25th USENIX Security Symposium  1035
3
ing daily protocol scans (e.g., Chargen requests, NTP
monlist commands, and DNS recursive lookups).
In each case, we coordinated with the studies’ authors
to ensure that they did not simultaneously notify opera-
tors. However, we do note that groups have previously
sent notiﬁcations to DDoS ampliﬁers [13].
3.2 Experiment Variables
To understand how to best construct and route notiﬁca-
tion messages, we performed notiﬁcations using several
methodologies and measured the differences in remedi-
ation. We speciﬁcally aimed to answer the following
questions:
Who should researchers contact? Researchers have
several options when deciding where they should report
vulnerabilities, including directly contacting network op-
erators, notifying national CERTs, and asking their own
country’s CERT to disseminate the data to other CERT
groups. We tested three options: (1) notifying the abuse
contact from the corresponding WHOIS record, (2) ge-
olocating the host and contacting the associated national
CERT, and (3) asking our regional CERT (US-CERT) to
propagate the information.
How verbose do messages need to be?
It is not
clear how much information researchers need to include
when notifying operators. For example, are notiﬁcations
more effective if researchers include detailed remedia-
tion steps or will such instructions go unheeded? We
sent three types of messages: (1) a terse message that
brieﬂy explained that we discovered the vulnerability
with Internet-wide scanning, and the impact of the vul-
nerability (e.g., for ICS notiﬁcations, we wrote “These
devices frequently have no built-in security and their
public exposure may place physical equipment at risk
for attack.”), (2) a terse message with a link to a web-
site with detailed information, and (3) a verbose email
that included text on how we detected the problem, vul-
nerability details, and potential remediation steps. We
provide the full text of our different messages in Ap-
pendix B–G.
Do messages need to be translated? We tested send-
ing messages in English as well as messages translated
by native technical speakers to several local languages.
3.3 Group Assignment
To test the impact of our experiment variables, we ran-
domly formed experiment groups that received different
notiﬁcation regimens. Here we describe our process for
constructing these groups.
For each IP address, we extracted the abuse con-
tact from the most speciﬁc network allocation’s WHOIS
Group
Control
National CERTs
US-CERT
WHOIS: English Terse
WHOIS: English Terse w/ Link
WHOIS: English Verbose
WHOIS: Language – Terse
ICS
657
174
493
413
413
413
IPv6 Ampl.
1,484
3,527
379
650
1,128
578
633
777
777
633
632
777
Germany: German
Germany: English
Netherlands: Dutch
Netherlands: English
Poland: Polish
Poland: English
Russia: Russian
Russia: English
WHOIS: Language – Verbose
Germany: German
Germany: English
Netherlands: Dutch
Netherlands: English
Poland: Polish
Poland: English
Russia: Russian
Russia: English
71
72
32
32
70
72
32
29
37
37
123
123
36
36
123
123
Table 2: Notiﬁcation Groups—We aggregated vulner-
able hosts by WHOIS abuse contacts and randomly as-
signed these contacts to notiﬁcation groups. Here, we
show the number of contacts notiﬁed in each group. Note
that for the language experiments, we tested terse and
verbose messages for several countries, both translated
and in English.
record. For the 16.7% of dual-stack hosts with different
contacts extracted from IPv4 and IPv6 WHOIS records,
we used the contact with the deepest level of alloca-
tion, and preferred IPv6 contacts when all else was equal
(4.3% of dual-stack hosts).
To test each variable, we split the abuse contacts from
each vulnerability into treatment groups (Table 2). For
the ICS and ampliﬁer experiments, we randomly allo-
cated one quarter of abuse contacts to the control group
(Group 1), one quarter to the CERT groups (half US-
CERT, half national CERTs), and the remaining half to
the WHOIS groups. For IPv6, to act in a responsible
manner we needed to complete some form of notiﬁcation
for all hosts to ensure adequate disclosure prior to the re-
lease of the corresponding study [6] in February 2016.
This prevented us from using a true control group. In-
stead, we approximate the behavior of the control group
using the 25 days of daily scans prior to our notiﬁcations.
We allocated a third of the IPv6 contacts to the CERT
groups, and the remainder to the WHOIS groups.
For the vulnerable hosts assigned to the CERT groups,
1036  25th USENIX Security Symposium 
USENIX Association
4
we geolocated each IP using MaxMind [16] and identi-
ﬁed the associated CERT. We note that not all countries
have an established CERT organization. This was the
case for 2,151 (17%) IPv6 hosts, 175 (8%) ICS devices,
and 2,156 (19%) DDoS ampliﬁers. These hosts were
located in 16 countries for IPv6, 26 countries for ICS,
and 63 countries for DDoS. Many of these countries are
in Africa or Central America (e.g., Botswana, Ethiopia,
and Belize), or are smaller island states (e.g., American
Samoa, Antigua and Barbuda, and the Bahamas). We
did not include hosts without a CERT organization in the
CERT experiment (although we later passed them along
to US-CERT).
In total, 64 CERTs were responsible for IPv6 hosts,
57 for ICS, and 86 for ampliﬁers. To compare directly
contacting national CERTs versus having US-CERT dis-
tribute information to them, we randomly divided the
affected national CERTs into two halves. For national
CERTs in the ﬁrst half, we contacted them directly with
vulnerable hosts in their region (Group 2). We sent the
remaining hosts for CERTs in the second half to US-
CERT (Group 3).
We obtained native translations of our WHOIS mes-
sages for several countries. We allocated contacts in the
WHOIS groups that were in those countries (based on
the WHOIS records) for our language experiment, fur-
ther detailed in Section 4.3. The remaining contacts were
randomly split into three groups based on message ver-
bosity: terse (Group 4), terse with a link (Group 5), and
verbose (Group 6).
3.4 Notiﬁcation Process
We sent notiﬁcation emails with the FROM and REPLY-
TO header set to an institutional mailing list: security-
notiﬁcations@berkeley.edu.
In each message, we at-
tached a CSV ﬁle that contained the list of vulnerable
hosts along with the latest scan timestamp and the list
of vulnerable protocols. We also included a link to an
anonymous survey, which asked for the organization’s
perspective on the reported security issue and whether
they found our detection and notiﬁcation acceptable. The
messages were sent from a server in UC Berkeley’s net-
work, which was listed as a valid mail server by UC
Berkeley’s SPF policy. We note that we also included a
randomly generated identiﬁer in each email subject that
enabled us to match a reply to the original notiﬁcation.
3.5 Tracking Remediation
We tracked the impact of different notiﬁcation method-
ologies by scanning all hosts for several weeks follow-
ing our notiﬁcations. As our scanning methods tested
the reachability of several services, we may have falsely
identiﬁed a host as patched due to random packet loss or
temporary network disruptions. To account for this, we
only designated a host as patched if it did not appear vul-
nerable in any subsequent scans. We leveraged the last
day’s scan data for this correction, but did not otherwise
use it in our analysis as it lacked subsequent data for val-
idation.
One limitation in our tracking is the inability to distin-
guish true patching from network churn, where the host
went ofﬂine or changed its IP address. While we can
still conduct a comparative analysis against our control
group, we acknowledge that our deﬁnition of patching
is a mixture of true patching and churn. We investigated
whether we could better approximate true remediation by
distinguishing between RST packets and dropped pack-
ets. We compared the proportion of RSTs and drops be-
tween our control group and our notiﬁed groups two days
after notiﬁcation and two weeks after notiﬁcation. At
both times, we observed nearly identical proportions be-
tween the control and notiﬁed groups—in all cases less
than 20% of hosts sent RST packets. This indicates that
RST packets are not a reliable signal for remediation, as
most hosts did not send RST packets even when truly
ﬁxed.
Unless stated otherwise, we consider a host as having
taken remediation steps for a particular vulnerability if
any of its affected protocols were detected as ﬁxed. Like-
wise, we say a notiﬁcation contact has taken remediation
steps if any of its hosts have patched. We deﬁne the re-
mediation rate as the percentage of notiﬁcation contacts
that have taken remediation steps. This deﬁnition is over
contacts rather than hosts as we are measuring the im-
pact of notifying these contacts, and contacts differ in
the number of affected hosts.
3.6 Ethical Considerations
We followed the guidelines for ethical scanning behav-
ior outlined by Durumeric et al. [10]: we signaled the
benign intent of our scans through WHOIS entries and
DNS records, and provided project details on a website
on each scanning host. We respected scanning opt-out
requests and extensively tested scanning methods prior
to their deployment.
The ethics of performing vulnerability notiﬁcations
have not been widely discussed in the security commu-
nity. We argue that the potential good from informing
vulnerable hosts outweighs the risks. To minimize po-
tential harm, we only contacted abuse emails using ad-
dresses available in public databases. Additionally, we
messaged all unnotiﬁed contacts at the conclusion of the
study. We offered a channel for feedback through an
anonymous survey with questions about the notiﬁed or-
ganization (described in Appendix A). We note that be-
USENIX Association  
25th USENIX Security Symposium  1037
5
(a) Misconﬁgured IPv6
(b) DDoS Ampliﬁers
(c) ICS Services
Figure 1: Remediation Rates—We show the remediation rate for each variable we tested. We ﬁnd that verbose
English notiﬁcations sent to network operators were most effective for IPv6 and ICS. Note the varying Y axes.
cause we only collected data about organizational deci-
sions and not individuals, our study did not constitute
human subjects research (conﬁrmed by consulting the
UC Berkeley IRB committee). Nevertheless, we fol-
lowed best practices, e.g., our survey was anonymous
and optional.
4 Results
For both ICS and IPv6, our notiﬁcations had a signiﬁ-
cant impact on patch rates. In our most successful trial—
verbose English messages sent directly to operators—the
patch rate for IPv6 contacts was 140% higher than in the
control group after two weeks. For ICS, the patch rate
was 200% higher. However, as can be seen in Figure 1b,
none of our notiﬁcations had signiﬁcant impact on DDoS
ampliﬁers. This is likely due to the extensive attention
DDoS ampliﬁers have already received in the network
operator community, including several prior notiﬁcation
efforts [21]. In addition, these ampliﬁers were already
previously abused in DDoS attacks without administra-
tive responses, potentially indicating a population with
poor security stances.
It is also important to note that
our best notiﬁcation regimen resulted in at most 18% of
the population remediating. Thus, while notiﬁcations can
signiﬁcantly improve patching, the raw impact is limited.
In the remainder of this section, we discuss the impact of
each experiment variable and how this informs how we
should construct future notiﬁcations.
To characterize the performance of our trial groups,
we measure the area under the survival curve for each
group, which captures the cumulative effect of each treat-
ment. To determine if observed differences have sta-
tistical signiﬁcance, we perform permutation tests with
10,000 rounds. In each round of a permutation test, we
randomly reassign group labels and recompute the area
differences under the new assignments. The intuition is
that if the null hypothesis is true and there is no signif-
icant difference between two groups, then this random
reassignment will only reﬂect stochastic ﬂuctuation in
the area difference. We assess the empirical probability
distribution of this measure after completing the permu-
tation rounds, allowing us to determine the probability
(and signiﬁcance) of our observed values.
All reported p-values are computed via this permuta-
tion test. We use a signiﬁcance threshold of α = 0.05,
corrected during multiple testing using the simple (al-
though conservative) Bonferroni correction, where each
test in a family of m tests is compared to a signiﬁcance
threshold of α
m.
Ideally, we would have selected this procedure as part
of our original experimental design. Unfortunately, we
only identiﬁed its aptness post facto; thus, its selection
could introduce a selection bias, a possible effect that we
lack any practical means to assess.
4.1 Notiﬁcation Contact
For both IPv6 and ICS notiﬁcations, directly notifying
WHOIS abuse contacts was most effective—particularly
early on. Two days after IPv6 disclosure, direct verbose
notiﬁcations resulted in 9.8% of the population remediat-
ing, compared to 3.1% when contacting national CERTs
and 1.4% by contacting US-CERT. For ICS, direct notiﬁ-
cations promoted 6.8% of the population to patch, more
than national CERTs (1.7%) and US-CERT (1.0%). In
both cases, direct notiﬁcations were notably better than
no notiﬁcations. As can be seen in Figures 1a and 1c,
this gain was persistent. After two weeks, the patch rate
of directly notiﬁed IPv6 contacts was 2.4 times as high
as the control, and three times as high for ICS contacts.
To determine if these observations are statistically sig-
niﬁcant, we perform permutation tests using the Bonfer-
roni correction. With six treatment groups, the family of
1038  25th USENIX Security Symposium 