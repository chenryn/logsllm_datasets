resulting in more ecient transmissions. The timestamp extension adds two elds to
the TCP header. The rst eld contains the current timestamp value at the sender.
The second eld echoes the last timestamp seen by the sender from the receiver side.
These elds allow TCP to determine the roundtrip time by taking the dierence between
the current time and the echoed time. Timestamps must increase monotonically, which
also serves to protect again wrap around of sequence numbers (PAWS). If a packet has
Chapter 4.
Implementation
33
a higher sequence number than the previous one but an earlier timestamp, it indicates
that the sequence number has wrapped around and the packet is a duplicate and must
be discarded.
We checkpoint the timestamp values in the packet headers to support the timestamp
extension. Linux uses the number of timer interrupts since the kernel was started (jies)
to set the current timestamp value. However, a kernel update resets the jies counter
and so the timestamps do not increase monotonically after the update. As a result, the
receiver discards all packets that are sent after the kernel update. We xed this issue
by adding an extra eld to the TCP socket structure that holds the oset that must be
added to the jies counter to obtain the correct timestamp number after the update.
All timestamp calculations have been updated to use this oset as well. An alternative
implementation is to set the jies counter to the same value as before the reboot. The
advantage of our solution is that it keeps the change isolated to the TCP subsystem
rather than aecting all uses of the jies counter, who might rely on the counter being
initialized to a certain value.
Selective acknowledgment is another common extension that increases TCP through-
put by making retransmissions more ecient.
In addition to regular cumulative ac-
knowledgments, the TCP receiver can add extra elds to the TCP header that allow
the receiver to acknowledge discontiguous blocks of data. The sender then only has to
retransmit just enough data to ll in the gaps. To restore selective acknowledgment on
the receiver side, its is only necessary to checkpoint whether it was enabled in the rst
place. The sender stores an extra bit for each packet in the retransmit queue and sets
this bit if it receives a selective acknowledgment for the packet. When retransmission is
triggered, segments with the bit set are not retransmitted. Since our checkpoint preserves
segment boundaries, we added the selective acknowledgment bit to the saved segments
in the checkpoint and set this bit when restoring the retransmit queue.
Before the TCP subsystem is started by the kernel, all incoming TCP packets are
Chapter 4.
Implementation
34
dropped by the kernel. These dropped packets will be eventually retransmitted by the
sender. However, there is a time period when the TCP subsystem has started but the
TCP connections have not been restored. During this time, if a packet arrives at a port
with an unrestored socket, then TCP sends a reset packet to the sender, which closes
the connection. To avoid this issue, we use the netlter API to drop packets meant for
applications which have not yet been restored from the checkpoint. The netlter API
allows inserting hooks at dierent layers of the network stack. We insert a hook that
drops all TCP packets destined to any of the sockets stored in the checkpoint. The
packet is dropped before the TCP subsystem receives the packet and so the reset is not
sent. Any other network communication is unaected, and after all the applications are
restored, the hook is removed.
We do not save the state of the receive queue because we observed that it only con-
tained unacknowledged packets. In particular, the Linux kernel sends acknowledgments
only after packets have been copied to the user space. If these packets have been copied,
then their contents are restored when the application is restored. Otherwise, the sender
will retransmit the unacknowledged packets in the receive queue. The TCP protocol
allows the kernel to acknowledge received packets before they are copied to the applica-
tion. The receive queue is structured similar to the send queue, and so if future kernel
versions send acknowledgments before copying data to the user space, then the receive
queue must be restored in the same way as we restore the send queue.
4.1.5 Pipes
A Unix pipe is a unidirectional communication channel used for interprocess communi-
cation. There are two types of pipes, an unnamed pipe and a named pipe. An unnamed
pipe is typically used to communicate between a parent and a forked child process. It is
created using the pipe system call that returns two le descriptors, one for reading and
one for writing. Data written to the write descriptor can be read from the read descrip-
Chapter 4.
Implementation
35
tor. After a fork, the le descriptors (but not the pipe itself) get copied, and the parent
and child communicate with one writing to one descriptor and the other reading from
the other descriptor. A named pipe (also known as FIFO) is created using the mknod
system call and has a name in the le system. Processes use named pipes by using the
open system call and then reading or writing to them.
Internally, a pipe is represented by two le descriptors that point to a shared memory
buer consisting of a xed number of pages. The memory buer is used to store data
that has been written and is ready to be read. If multiple processes uses a pipe (e.g.,
after a fork), they share references to the same le descriptors. There is no dierence in
the unnamed and named pipe implementation other than the way they are created.
To save a pipe in the checkpoint, we create two entries, one for each end of the pipe.
When a process uses a pipe, we create a reference from the process to the entry for the
pipe in the checkpoint. The memory buers are not copied. Instead the checkpoint stores
the pointers to the data pages.
Restoring pipes is tricky because each end of the pipe might be referenced by several
processes and sometimes a process can close one end while the other end is open. For
example, a writer may have exited (which closes the write side of the pipe) while the
reader has not yet read all the data. For each pipe entry in the checkpoint, we initially
create both ends of the pipe (for simplicity), even if there is only one end saved in the
checkpoint. To create unnamed pipes, we use the pipe system call, and for named pipe,
we call open on the pipe le name. Then we use the dup system call to assign the original
le descriptor numbers.
We implement sharing of pipe le descriptors in the same way as we handle all shared
resources (see Section ??). We also keep track of which processes use which end of the
pipe. After all processes have been restored, we consult the pipe usage count to see
if there are any pipe ends that are not being used. Unused pipe ends represent pipe
descriptors that were closed before the update. If any unused pipe ends are found, we
Chapter 4.
Implementation
36
close them before allowing processes to resume execution. The pipe buer pages are
restored similar to user pages, as described in Section 4.1.2.
4.1.6 Unix Sockets
Unix sockets are another interprocess communication channel. Similar to pipes, Unix
sockets can be unnamed or they can be bound to a le name (a listening socket) in the
lesystem. The major dierences between Unix sockets and pipes are that Unix sockets
allow bidirectional communication and they can be created using the socket interface in
addition to the regular le interface used for pipes.
Internally, Unix sockets are represented by a pair of socket descriptors, and both
descriptors can be used to read and write data. Each socket keeps a reference to its
peer, and when the data is written, it is placed on the peer's receive queue. Unlike TCP
sockets, Unix sockets don't use a send queue.
For each socket descriptor, we create an entry in our checkpoint. The checkpoint entry
contains a a reference to the peer descriptor, the type of the socket, connection state
(connected or disconnected), le name for named sockets and the contents of the receive
queue. For listening sockets, we also store whether the socket is accepting incoming
connections. All processes using the socket keep a reference to the socket entry.
To restore Unix sockets, we rst remove the existing le for named sockets. Then we
create both the ends of the socket using standard Unix socket creation code. Then we
set the socket connection state, le name and restore the contents of the receive queue.
Shared sockets are handled as described previously in Section ??.
4.1.7 Terminals and Keyboard
The Linux kernel uses a terminal emulator to provide a simple interface to the keyboard
and text-mode display. An application accesses these devices by reading and writing
to terminal device les located in the /dev directory. The kernel implements multiple
Chapter 4.
Implementation
37
consoles by virtualizing the single keyboard and display. When switching from one con-
sole to another, it saves the hardware terminal state of the current console (i.e., screen
contents) in memory, and restores the terminal state of the next console from memory.
When a process is using a virtual console (a le descriptor points to the terminal de-
vice), we rst force the kernel to switch to a dierent console, which updates the terminal
state stored in kernel memory, and then save this memory state in the checkpoint. Be-
sides the screen contents, we also save the mode of the terminal (text or graphical), and
the way input is processed, e.g., as lines or character-by-character and how the escape
codes are processed.
To restore the terminal state, we open the terminal device that the application was
using, and then again switch to a dierent virtual console. We restore the screen contents
by copying them from the checkpoint and then we use the ioctl system call to set the
correct terminal mode. At this point, we switch back to the original console, which
synchronizes the hardware with the updated terminal state. Any status messages during
boot are overwritten and should be logged separately for debugging purposes.
4.1.8 Framebuer
The framebuer device is used by graphical applications to access the video card. Appli-
cations access the framebuer by opening the /dev/fb le and issuing ioctl system calls
to set the desired resolution and bits per pixel. The framebuer interface provides a
simple bitmap display. Applications update the bitmap display by memory mapping the
/dev/fb le and writing to the mmapped region. The kernel implements a framebuer
per virtual console (discussed in Section 4.1.7) by virtualizing the hardware framebuer.
To add checkpointing support for the framebuer, we have to save the framebuer
contents and the display mode settings. The contents of the framebuer are saved by
copying them into the checkpoint. A full copy is necessary because the framebuer
contents are modied when the new kernel is changing display setting and outputting
Chapter 4.
Implementation
38
status messages during its initialization. To restore the framebuer, we recreate the
memory map that it was using originally and copy the contents from the checkpoint into
the video memory.
The display settings are stored in the fb_var_screeninfo structure and control things
like resolution, pixel size (16-bit vs 32-bit) and pixel format (RGB vs BGR). This data is
modied by applications via the ioctl system call, so our checkpoint stores this informa-
tion in the format used by this system call. This format is not dependent on the kernel
version or the framebuer driver and can be restored by calling the ioctl function.
We added framebuer support for the Xfbdev X server. Xfbdev was chosen because it
does not rely on more advanced hardware dependent features, like hardware acceleration
or DRI. To add support for a more full featured X server like X11 would require a more
comprehensive solution for saving and restoring state of hardware devices and drivers.
Our implementation could successfully save and restore the state of the Xfbdev X server,
the window manager (twm) and some simple applications like xclock, xcalc and xedit
without requiring any changes to these applications.
4.1.9 Mouse
Graphical applications use the mouse in addition to the framebuer (see Section 4.1.8)
and the keyboard (see Section 4.1.7). We added support for saving and restoring the
/dev/input/mice mouse device because this device is used by the Xfbdev X server. The
mouse driver uses two structures mousedev and mousedev_client. The mousedev struc-
ture is shared among all the clients using the mouse. It is used to hold globally shared
mouse state, including the client list and for interfacing with the lower level mouse sub-
systems. The mousedev_client structure maintains per-client information and queues
packets received from the mouse. When a mouse moves, it sends a packet which con-
tains the state of the buttons and displacement since the last packet. We saved and
restored both of those structures, which ensures that the mouse operates correctly after
Chapter 4.
Implementation
39
the update.
4.2 System Call Interface
We have added two system calls for executing the update process. These system calls 1)
enable checkpointing specic processes, and 2) restoring all checkpointed processes. We
have also added two debugging system calls that 1) determine whether a checkpoint is
available, 2) help distinguish between a process that was started normally or was restored
from a checkpoint. We do not require changes to existing programs. These system calls
are intended to be used by user-level utilities and scripts to manage the update process
after the kernel is initialized.
1. Enable_save_state: Takes a pid as an argument and sets a ag that indicates that
a process with the given pid, all its children and all processes in the same thread
group will be checkpointed. Without this ag, the process is not checkpointed.
2. Load_saved_state: Restores all the processes stored in the checkpoint.
3. Is_state_present: Checks if a checkpoint is available.
4. Was_state_restored: Used by a processes to check if it was started normally from
scratch or if the process was created from a checkpoint. This system call can be
used if some action needs to be taken by a process after it is restored.
4.3 Limitations
In this section, we discuss several limitations of our current implementation. Currently,
we preallocate physically contiguous memory for the checkpoint when the kernel is rst
booted at a xed location. After an update, we do not release this memory so that the
kernel can be updated again from this xed location. In the future, we plan to remove
Chapter 4.
Implementation
40
this limitation by allocating discontiguous physical memory on demand when creating a
checkpoint. One approach is to allocate this memory to the checkpointing process and
save the state of the checkpointing process itself. After restoring the page tables of the
checkpoint process, the rest of the checkpoint would be read from virtually contiguous
memory. When this process exits, the checkpoint memory would be released.
Our implementation for handling temporary les currently only supports Linux ext3
le systems. The reason is that when we create a hardlink to a temporary le during
checkpointing, we do not remove the le from the le-system specic orphan list. As
a result, the le system still attempts to remove the le during the mount process.
To avoid this problem after a kernel update, when an ext3 le system is mounted, we
disable orphan list processing for the les associated with the checkpointed processes. A
full implementation should remove the temporary le from the le-system specic orphan
list when the checkpoint is taken.
We have added support for a wide variety of kernel features to enable supporting
many types of commonly used applications. Some of the feature we do not implement
include SYSV IPC, pseudo terminals, message queues, and the epoll system call. Our
implementation supports Unix stream sockets, but we do not support Unix datagram
sockets, which preserve message boundaries or the ability to pass le descriptors. We
preserve boundaries for TCP segments and this code could be used for Unix datagram
sockets also. Adding support for passing le descriptors would require supporting the
sendmsg/recvmsg system calls. Applications using these kernel features cannot be re-
stored in our system.
We do not see any fundamental issues that would prevent the missing functionality
from being added in future versions because some of the features that we do implement
already provide similar alternatives. For example, we provide Unix sockets and pipes for
IPC, shared memory via threads and synchronization via the futex system call.
Since our implementation does not support all kernel features, we are not able to save
Chapter 4.
Implementation
41
all processes running on the system. In particular, system or administrative applications
like cron, udevd or getty are not saved and restored, and so these programs are shutdown
and restarted normally on reboot. However, these programs do not have much state, and
also do not require human intervention on restart. As more features are added to the
implementation, these applications could be checkpointed as well.
For network applications, we assume that the ports used by an application before the
update are not used by some other application before the application is restored. It is