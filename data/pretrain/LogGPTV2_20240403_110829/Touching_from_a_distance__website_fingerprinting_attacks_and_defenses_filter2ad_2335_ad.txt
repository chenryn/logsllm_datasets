9.10 to 10.10. We used Firefox 3.6.10-3.6.17 and Tor 0.2.1.30,
except one computer that used 0.2.2.21-alpha. All Firefox
plugins were disabled during data collection. Three of the
computers had 2.8GHz Intel Pentium CPUs and 2GB of
610Our attack. See Section 3.
DLSVM
Panchenko Ad hoc SVM classiﬁer of Panchenko, et
al. [17], with the libsvm 3.1 implementa-
tion from WEKA 3.6.4 and the param-
eters recommended by Panchenko, et al.
(c = 217 and γ = 2−19).
The Multinomial Naive Bayes classiﬁer
proposed by Herrmann, et al. [10].
MNB
Table 3: The attacks evaluated in our experiments.
RAM, one computer had a 2GHz AMD Turion Mobile CPU
with 2GB of RAM. We scripted Firefox using the Ruby
watir-webdriver library and captured packets using tshark,
the command-line version of wireshark. For the SSH exper-
iments, we used OpenSSH 5.3p1. Our Tor clients used the
default conﬁguration, unless otherwise noted. SSH tunnels
passed between two machines on the same local network.
Most of our experiments use data collected from the Alexa
Top 1000 web pages. We removed any web pages that failed
to load in Firefox (without Tor or any other proxy).
If a
URL redirected to another location, we replaced it with its
redirect target. We then used the top 800 URLs from this
cleaned list. We collected traces from each web page in a
round-robin fashion. Unless otherwise speciﬁed, we cleared
the browser cache between each page load. We repeated
data collection with four diﬀerent defense mechanisms, as
described below. We collected either 20 or 40 traces from
each URL, depending on the defense mechanism in use. We
ran most experiments with just the top 100 web pages in
our list – we only use full 800 URLs in one experiment to
test the scalability of our attack.
This is a “closed-world” evaluation. In such an evaluation,
there are only k web pages in the world. The attacker can
collect ﬁngerprints for each page. The victim then chooses
one of the pages uniformly at random and loads it in his
browser. The attacker observes the victim’s packet trace
and attempts to guess which page the victim loaded. Thus,
the appropriate metric is the success rate of the attacker,
i.e. the percentage of time he guesses correctly. There is no
notion of false positive or false negative in this scenario. In
contrast, we will evaluate our web site classiﬁer in an open
world setting, which does have such considerations.
6.1.2 Attacks and Defenses
Table 3 summarizes the attacks evaluated in this paper.
We test each attack against each of the following defenses.
For each defense, we also indicate the number of URLs we
collected, and the number of visits to each URL. We col-
lected four basic data sets:
None (SSH) (100x40). All HTTP traﬃc is sent through
an SSH tunnel.
SSH + HTTPOS (100x20). We obtained the prototype
implementation that the HTTPOS authors used to evaluate
HTTPOS in their paper. Based on some of our early results,
they added some additional randomization to their defense.
Note that HTTPOS includes both TCP- and HTTP-level
defenses. Some web pages caused HTTPOS to crash. We
detected crashes and attempted to load the page up to 3
times. If HTTPOS crashed all 3 times, then we added the
third, incomplete trace to our data set. Our ﬁnal data set of
2000 traces contained 33 crash traces, so we do not believe
these had a signiﬁcant eﬀect on our results.
Tor (800x40). All HTTP traﬃc is tunneled through the
default Tor conﬁguration. Most experiments only use the
top 100 web pages from this dataset.
Tor + randomized pipelining (100x40). The Tor project
has released a software bundle that includes Tor, the Polipo
proxy, and a patched version of Firefox that randomizes the
order and pipelining used to load images and other embed-
ded objects in a web page. We use the entire bundle as-is.
We then used these data sets to generate simulations of
other defenses, as described below.
SSH + Sample-based traﬃc morphing (100x20). We
apply traﬃc morphing to the traces obtained in the SSH ex-
periment. We morphed all traces to have the same packet
size distribution as http://flickr.com (selected randomly
from our data set). We morphed each direction indepen-
dently, as described in the traﬃc morphing paper. To morph
a trace, we repeatedly sampled packet sizes from the target
distribution and padded (or fragmented) packets in the trace
to match the sampled size. Thus our morphed traces have
the same packet size distribution as they would under opti-
mal traﬃc morphing, but the total number of packets trans-
mitted may be higher. The original traﬃc morphing paper
found that optimal traﬃc morphing and sample-based traf-
ﬁc morphing had equal resilience to attack, so we believe
this is a reasonable evaluation of traﬃc morphing.
SSH packet count (100x40). We apply the same trans-
formation to our SSH traces as we did to our Tor traces, as
described above.
Tor + randomized pipelining + randomized cover
traﬃc (100x20). We insert additional cover traﬃc into
the traces collected for the Tor + randomized pipelining ex-
periment. We deleted all packet size information, i.e. traces
consisted of only ±1500s. Then, for an input trace of l
packets, we randomly, uniformly, and independently pick l
positions in the trace and insert a 1500 or −1500, with equal
probability, at each position.
Tor packet count (100x40). We remove all packet size
and direction information from our Tor traces. All that the
attacker can observe is the total number of packets trans-
mitted. This experiment explores how much information is
revealed by the size of the page being loaded.
6.1.3 Results
We ran each attack against each data set using stratiﬁed
10-fold cross validation. Figure 2 shows the results of these
experiments. The DLSVM attack generally outperforms the
Panchenko and MNB attacks. See Section 7 for discussion.
We performed an experiment to simulate the limits of de-
fenses based on re-ordering, pipelining, padding, and gen-
erating extraneous HTTP requests. We added randomized
cover traﬃc and padded all packets to 1500 bytes in the
traces in our Tor + randomized pipelining data set, as de-
scribed above. We varied the cover traﬃc overhead from 0%
to 100%. This experiment is intended to model an idealized
version of defenses like randomized pipelining and HTTPOS.
Figure 3 shows the inﬂuence of adding randomized cover
traﬃc on our attack. With no cover traﬃc, i.e. with ran-
domized pipelining and packets padded to 1500 bytes, our
attack was able to recognize the visited web page almost 80%
of the time. If we double the size of the trace by adding ex-
611Figure 2: Performance of our attack and previously proposed attacks against several proposed defenses.
Figure 3: Performance of our attack against Tor
with randomized pipelining, all packets padded to
1500 bytes, and varying amounts of cover traﬃc.
Figure 4: Bandwidth overheads of the defenses eval-
uated in this paper.
tra cover traﬃc, our attack can determine the target web
page over 50% the time.
Figure 4 shows the bandwidth overheads of the defenses
evaluated in this paper. All overheads are normalized to the
SSH traces. HTTPOS has the lowest overhead, 36%, but is
not secure. The other defenses have overhead of over 60%
compared to SSH.
Figure 5 shows that the DLSVM, Panchenko, and MNB
classiﬁers work well for both cold cache and warm cache page
loads. Although we have not directly evaluated our web page
classiﬁer on a mixed cold/warm workload, the web site clas-
siﬁers evaluated in the next section do use mixed workloads
and perform well. Figure 5 also shows that the classiﬁers
perform well on randomly selected web pages loaded through
Tor, not just the Alexa top 100 pages.
Figure 6(a) shows how the diﬀerent attacks perform as the
number of web pages they must distinguish increases. Not
only does our attack outperform the Panchenko attack when
the number of candidate web pages is small, the gap widens
as the size of the candidate set increases. For example, our
attack can guess which web page, out of 800, that a Tor
user is visiting 70% of the time. The Panchenko attack had
a success rate of 40% on our set of 800 web pages.
Figure 6(b) shows how additional training data can im-
prove the success rate of our attack. Our attack provides
satisfactory results, even with a small training set.
6.2 Web site classiﬁer
6.2.1 Experimental Setup
To evaluate the performance of our web site classiﬁer, we
created models for two web sites censored by the Chinese
“Great Firewall” – Facebook [7] and IMDB [5] – and con-
structed page classiﬁers using the Alexa Top 99 pages, along
with the pages in our model for each site. We then collected
additional traces for the pages in our models, and ran those
traces through the model to compute the probability distri-
bution of classiﬁer outputs for each page in each model, as
described in Section 4.
Our Facebook model covers the login page, the user’s
home page, and a generic “friend proﬁle page”. It includes
warm and cold cache instances of the home and proﬁle pages.
Facebook’s home and proﬁle pages use javascript to auto-
matically fetch older items as the user scrolls down the page
of past notiﬁcations. Our model includes these events. The
IMDB model covers the IMDB home page, search results
page, movie page, and celebrity page.
It includes warm
and cold cache states for each page. Transition probabil-
ities between states are artiﬁcial for both models – a real
attacks-and-defensesPage 1None(SSH)SSH + HTTPOSSSH + sample-based morphingSSH packet countTor (100x40)Tor + rand. pipe.Tor + rand. pipe. + rand. coverTor packet count00.20.40.60.81Success rateDLSVMPanchenkoMNBrand-coverPage 100.250.50.751Cover Traﬃc Overhead00.10.20.30.40.50.60.70.80.91Success RateoverheadPage 1SSH + HTTPOSSSH + sample-based morphingSSH packet countTor + rand. pipe.00.10.20.30.40.50.60.70.80.9Bandwidth overhead612(a)
(b)
Figure 6: (a) Performance of our Tor web page classiﬁers as a function of the number of possible web pages.
(b) Performance of our Tor web page classiﬁer as a function of the training set size.
not compatible with Tor’s default conﬁguration. By default,
Tor picks a new path every 10 minutes and, to Facebook, the
user appears to be coming from the last node in this path.
When the path changes, the user appears to have moved
from one computer to another – which may be thousands
of miles away – in 10 minutes. Facebook detects this and
logs the user out. Consequently, Tor users visiting Facebook
must alter the Tor conﬁguration to use a ﬁxed path. Thus,
we collected all our Facebook data using a ﬁxed Tor path.
6.2.2 Results
Figures 7(a) and 7(b) show the histogram of log-likelihood
scores, under the Facebook and IMDB models, respectively,
of 6-page windows of the traces we collected. So, for exam-
ple, for every window of 6 page loads in the IMDB traces,
we ran the packet traces for those 6 page loads through the
IMDB model to compute a log-likelihood score. We only
considered windows that contained either all IMDB visits or
all non-IMDB visits – if a window had, say, 3 IMDB pages
and 3 non-IMDB pages, we discarded it from the histogram.
As Figure 7(a) shows, the non-Facebook windows are com-
pletely separated from the Facebook windows by our model,
meaning our classiﬁer works perfectly on this data set. In
the IMDB experiment, the non-IMDB windows have, on av-
erage, a much higher log-likelihood, indicating that they are
not likely to be generated by our IMDB model.
Figure 8 shows the receiver operating curves (ROC) for
our Facebook and IMDB classiﬁers. These curves show the
trade-oﬀ in False Positive and True Positive rates for varying
thresholds of the classiﬁer. As indicated by the histogram
in Figure 7(a), the Facebook classiﬁer can achieve 0 false
positives and false negatives on our dataset. The IMDB
classiﬁer can achieve a 7.9% FP rate and a 5.6% FN rate.
Figure 9 demonstrates how the log-likelihood score corre-
lates with user visits to the target web site over time. Note
that these graphs plot traces from multiple browsing sessions
– the sessions are separated by gaps in the traces. Only ses-
sions with at least 6 page loads, and at least one page load
from the target web site (Facebook or IMDB, respectively),
are included in the graphs. The thick, ﬂat, pink line indi-
cates portions of the trace containing page loads from the
target web site, page loads from other sites have a thin ﬂat
line. The blue lines with markers plot the log-likelihoods of
the six-page windows of page loads. As the graphs show, the
Figure 5: Performance of our web page classiﬁer
against Tor under various data collection scenarios.
attacker would derive these from observations of user behav-
ior and would likely have higher accuracy as a result. Initial
state probabilities are uniform, since the attacker may be-
gin eavesdropping in the middle of a user’s session. See our
technical report for complete speciﬁcations of the models[3].
To test our site classiﬁers, we need traces of the URLs vis-
ited by real users. We obtained URL traces for 25 subjects
from Eelco Herder. He collected these traces for his empiri-
cal study of web user behavior [23]. These traces, from users
in Europe, contain numerous visits to IMDB, but no visits
to Facebook. Therefore, we have generated artiﬁcial traces
for Facebook. Our artiﬁcial Facebook traces construct visits
to Facebook that follow our Facebook model, i.e. we pick a
starting Facebook page according to the initial state prob-
abilities of our model, and pick successive pages according
to the transition probabilities of our model. We then insert
these into real traces so that we create a trace consisting of
some Facebook visits and some non-Facebook visits. Since
the traces are generated from the same model that the classi-