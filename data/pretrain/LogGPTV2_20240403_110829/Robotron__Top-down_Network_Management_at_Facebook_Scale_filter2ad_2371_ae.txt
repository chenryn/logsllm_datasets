8. EXPERIENCE AND FUTURE WORK
In this section, we share example issues that arise
using Robotron and lessons learned that lead to open
research problems or can inform the design of future
network management systems.
Complexity of Modeling: A user-impacting event
occurred when a new BGP session was provisioned with
an external ISP requiring a custom import policy con-
taining cherry-picked preﬁxes. This artiﬁcially limits
the session to only serve traﬃc destined to users behind
those preﬁxes. While the feature was still under de-
velopment, an engineer used Robotron to turn up the
session, instantly saturating the egress link. The is-
sue was discovered, via monitoring, by our operations
team who quickly mitigated the issue. While similar
outages could have been prevented by quickly incorpo-
rating the latest design requirements into Robotron, a
signiﬁcant portion of development time was spent on de-
signing new or correcting existing FBNet models to cap-
ture new requirements. Designing network-wide mod-
els that are rich enough to capture the slew of low-level
conﬁguration parameters and ensure cross-device con-
ﬁg integrity would allow new designs be implemented
quickly in Robotron with little to no model changes.
Stale Conﬁgs: After network design changes are
made, Robotron currently relies on network engineers
to trigger conﬁg generation and deployment since cer-
tain design changes (e.g., topology changes) depend on
changes in the underlying physical network (e.g., re-
cabling). The time gap between design changes, conﬁg
generation, and conﬁg roll-out may lead to accidental
deployment of stale conﬁgs. For example, the DC clus-
ter switch conﬁgs use rack proﬁles from FBNet to derive
Urgency
CRITICAL
MAJOR
MINOR
WARNING
NOTICE
IGNORED
# of events Percentage # of rules Examples
2
1.35K
32K
1.8M
6.68K
47.5M
<0.01%
<0.01%
0.06%
3.65%
0.01%
96.27%
13 Critical Power/Temperature Alarm, Device Reboot, SSL VPN Alarm
214 High Temperature Alarm, TCAM Errors, Linecard Removed
310 TCAM Exhausted, Possible Bad FPC, IP conﬂict
103
SSL connection limit, Syslog cleared by user, Interface link state down
79 DHCP Snooping Deny, MAC Conﬂict, Cannot ﬁnd NTP server
0
LSP change, User authentication
Table 3: Syslog messages of various urgency levels collected in a 24-hour period. A “rule” refers to a regex rule in
Section 5.4.3.
the number of downlink interfaces allocated per rack. In
one instance, Engineer A wanted to add a new rack to a
cluster. He updated the rack proﬁle and generated con-
ﬁgs for the cluster switches, but did not immediately
deploy them. A few days later, Engineer B updated
the rack proﬁle, which invalidated A’s design change,
but did not re-generate new conﬁgs accordingly. One
week later, Engineer A, unaware of the design change
Engineer B made, pushed the stale conﬁgs to the clus-
ter switches, dropping connectivity to a few racks in
the cluster. While this particular issue could have been
avoided if network design, conﬁg generation and de-
ployment were tightly coupled, the real challenge occurs
when design changes are made closely in time. How to
serialize concurrent design changes, resolve design con-
ﬂicts, and leverage the Derived network state to ensure
change safety remains an open problem. Statesman [33]
provides some novel ideas on conﬁcts resolution. How-
ever, at Facebook’s scale, handling multiple writers with
a lock-based mechanism can be challenging.
Automation Fallbacks: Network engineers occa-
sionally bypass Robotron to manually conﬁgure devices.
This is due to Robotron bugs, unfamiliarity with Robotron,
or the urgent need to make changes unsupported by
Robotron. Manual changes often lead to misconﬁg-
uration, resulting in issues such as idle circuits, sub-
optimal routing, and unexpected outages.
Ideally, an
automated network management system like Robotron
should block manual changes directly to the network
devices and require all conﬁg changes be made through
it. However, our operational experiences show that
users, especially in exceptional cases, usually need a re-
liable fallback mechanism to make emergency changes
to the network.
Instead of blocking manual changes,
Robotron curtails them with conﬁg monitoring (Sec-
tion 5.4.3). Another possible solution is to restore de-
vice running conﬁgs to Robotron-generated conﬁgs pe-
riodically, while giving users a window for these emer-
gency operations.
9. RELATED WORKS
Many prior research focus on understanding network
management challenges, as well as reverse-engineering
and validating network designs through bottom-up static
conﬁguration analysis in two classes of networks: provider
networks [15, 20, 25, 29, 36] and enterprise networks [13,
14, 16, 26, 30]. In addition, recent work [21, 22] propose
general methods to analyze and troubleshoot conﬁgu-
rations. In contrast, Robotron employs a top-down ap-
proach, which is continuously reﬁned through opera-
tional experience of our network engineers, to manage
a multi-domain network consisting of a backbone, mul-
tiple DC and POP networks.
The potential of automating or simplifying network
design and conﬁguration through abstraction has in-
spired many works in the research community. A class
of literature [31,34,35,38] applies the“top-down”paradigm
to systematically optimize conﬁguration of speciﬁc pro-
tocols or network functions (e.g., VLANs, packet ﬁlters,
topologies, and routing) to meet desired objectives such
as performance, reachability, and reliability. Recent
work [27, 33] propose the use of a centralized platform
similar to FBNet for network control and management.
Several industrial solutions [4, 7, 10, 18] adopt template-
based approaches for conﬁg generation. Many eﬀorts
aim to develop abstract languages or models to specify
conﬁgs in a vendor-neutral fashion [6,17]. Robotron in-
corporates many of these best practices, but is broader
in scope: in addition to modeling, network design, and
conﬁg generation, Robotron includes conﬁg deployment
and monitoring, and a focus on scaling each stage of
the network management life cycle. Robotron also ap-
plies best practices in software engineering, including
OO-based network modeling, version control, code re-
view, and deployment automation, to large-scale net-
work management.
Finally, a few studies [12,24] consider simplifying net-
work management through clean-slate designs by rearchi-
tecting the control plane. In contrast, Robotron is ap-
plicable to existing operational networks and clean-slate
designs.
10. CONCLUSION
This paper presents the design, implementation, and
operation experiences of Robotron, the system respon-
sible for managing Facebook’s production network con-
sisting of data centers, a global backbone, and POPs
over the last eight years. Robotron employs a top-down
approach where human intentions are translated into
a set of distributed, heterogeneous conﬁgurations. Be-
yond conﬁguration generation, Robotron also deploys
and monitors conﬁgurations to ensure the actual state
of the network does not deviate from design. We also
present a signiﬁcant amount of Robotron’s usage statis-
tics to shed light into the operations of Facebook’s pro-
duction network.
Recently, researchers [11] have advocated manage-
ment plane analytics, similar to prior research done for
control and data planes. By sharing our experience with
Robotron, we hope to solicit more research in this ﬁeld,
and improve the management practice in the network-
ing community.
Acknowledgement
Many people in the Network Platform team at Facebook
have contributed to Robotron over the years. In partic-
ular, we would like to acknowledge Andrew Kryczka,
Paul McCutcheon, and Manoj Lal. We are also in-
debted to Omar Baldonado, Nick Feamster, Mikel Jimenez,
Steve Shaw, Chad Shields, Callahan Warlick, CQ Tang,
Sanjeev Kumar, our shepherd, Katerina Argyraki as
well as the anonymous SIGCOMM reviewers for their
comments and suggestions on earlier drafts.
11. REFERENCES
[1] Apache thrift. http://thrift.apache.org/.
[2] Django. https://www.djangoproject.com/.
[3] Google Compute Engine Incident 15064.
https://status.cloud.google.com/incident/compute/15064.
[4] HPE Network Management (HP OpenView).
http://www8.hp.com/us/en/software-solutions/
network-management/index.html.
[5] ISO/IEC 7498-4: Information processing systems – Open
Systems Interconnection – Basic Reference Model – Part 4:
Management framework.
[6] OpenConﬁg. http://www.openconﬁg.net/.
[7] Opsware. http://www.opsware.com/.
[8] Root Cause Analysis for recent Windows Azure Service
Interruption in Western Europe. https://goo.gl/UtrzhL.
[9] Summary of the Amazon EC2 and Amazon RDS Service
Disruption in the US East Region.
https://aws.amazon.com/message/65648/.
[10] Tivoli Netcool Conﬁguration Manager.
http://ibm.com/software/products/en/tivonetcconfmana.
[11] A. Akella and R. Mahajan. A call to arms for management
plane analytics. In Proceedings of the 13th ACM Workshop
on Hot Topics in Networks, HotNets-XIII, 2014.
[12] H. Ballani and P. Francis. Conman: A step towards network
manageability. In Proceedings of the 2007 Conference on
Applications, Technologies, Architectures, and Protocols for
Computer Communications, SIGCOMM ’07, 2007.
[13] T. Benson, A. Akella, and D. Maltz. Unraveling the
complexity of network management. In Proceedings of the
6th USENIX Symposium on Networked Systems Design
and Implementation, NSDI’09, 2009.
[14] T. Benson, A. Akella, and D. A. Maltz. Mining policies
from enterprise network conﬁguration. In Proceedings of the
9th ACM SIGCOMM Conference on Internet Measurement
Conference, IMC ’09, 2009.
[15] T. Benson, A. Akella, and A. Shaikh. Demystifying
conﬁguration challenges and trade-oﬀs in network-based
ISP services. In Proceedings of the ACM SIGCOMM 2011
Conference, SIGCOMM ’11, 2011.
[16] D. Caldwell et al. The cutting edge of ip router
conﬁguration. SIGCOMM Comput. Commun. Rev.,
34(1):21–26, Jan. 2004.
[17] Distributed Management Task Force, Inc.
http://www.dmtf.org.
[18] W. Enck et al. Conﬁguration management at massive scale:
system design and experience. Selected Areas in
Communications, IEEE Journal on, 2009.
[19] R. Enns, M. Bjorklund, J. Schoenwaelder, and A. Bierman.
Network Conﬁguration Protocol (NETCONF). RFC 6241
(Proposed Standard), June 2011.
[20] N. Feamster and H. Balakrishnan. Detecting BGP
conﬁguration faults with static analysis. In Proceedings of
the 2Nd Conference on Symposium on Networked Systems
Design & Implementation - Volume 2, NSDI’05, 2005.
[21] A. Fogel et al. A general approach to network conﬁguration
analysis. In Proceedings of the 12th USENIX Conference
on Networked Systems Design and Implementation,
NSDI’15, 2015.
[22] A. Gember-Jacobson et al. Management plane analytics. In
Proceedings of the 2015 ACM Conference on Internet
Measurement Conference, IMC ’15, 2015.
[23] R. Gerhards. The Syslog Protocol. RFC 5424 (Proposed
Standard), Mar. 2009.
[24] A. Greenberg et al. A clean slate 4d approach to network
control and management. SIGCOMM Comput. Commun.
Rev., 35(5):41–54, Oct. 2005.
[25] Y. Himura and Y. Yasuda. Discovering conﬁguration
templates of virtualized tenant networks in multi-tenancy
datacenters via graph-mining. SIGCOMM Comput.
Commun. Rev., 42(3), June 2012.
[26] H. Kim, T. Benson, A. Akella, and N. Feamster. The
evolution of network conﬁguration: A tale of two campuses.
In Proceedings of the 2011 ACM SIGCOMM Conference
on Internet Measurement Conference, IMC ’11, 2011.
[27] T. Koponen et al. Onix: A distributed control platform for
large-scale production networks. In Proceedings of the 9th
USENIX Conference on Operating Systems Design and
Implementation, OSDI’10, 2010.
[28] P. Lapukhov, A. Premji, and J. Mitchell. Use of BGP for
routing in large-scale data centers. Internet-draft, Internet
Engineering Task Force, Apr. 2016. Work in Progress.
[29] R. Mahajan, D. Wetherall, and T. Anderson.
Understanding BGP misconﬁguration. SIGCOMM
Comput. Commun. Rev., 32(4), Aug. 2002.
[30] D. A. Maltz et al. Routing design in operational networks:
A look from the inside. In Proceedings of the 2004
Conference on Applications, Technologies, Architectures,
and Protocols for Computer Communications, SIGCOMM
’04, 2004.
[31] B. Schlinker et al. Condor: Better topologies through
declarative design. In Proceedings of the 2015 ACM
Conference on Special Interest Group on Data
Communication, SIGCOMM ’15, 2015.
[32] A. Singh et al. Jupiter rising: A decade of clos topologies
and centralized control in google’s datacenter network. In
Proceedings of the 2015 ACM Conference on Special
Interest Group on Data Communication, SIGCOMM ’15,
2015.
[33] P. Sun et al. A network-state management service. In
Proceedings of the 2014 ACM Conference on SIGCOMM,
SIGCOMM ’14, 2014.
[34] X. Sun and G. G. Xie. Minimizing network complexity
through integrated top-down design. In Proceedings of the
Ninth ACM Conference on Emerging Networking
Experiments and Technologies, CoNEXT ’13, 2013.
[35] Y.-W. E. Sung et al. Towards systematic design of
enterprise networks. In Proceedings of the 2008 ACM
CoNEXT Conference, CoNEXT ’08, 2008.
[36] Y.-W. E. Sung et al. Modeling and understanding
end-to-end class of service policies in operational networks.
In Proceedings of the ACM SIGCOMM 2009 Conference
on Data Communication, SIGCOMM ’09, 2009.
[37] C. Tang et al. Holistic conﬁguration management at
facebook. In Proceedings of the 25th Symposium on
Operating Systems Principles, SOSP ’15, 2015.
[38] S. Vissicchio et al. Improving network agility with seamless
BGP reconﬁgurations. IEEE/ACM Trans. Netw.,
21(3):990–1002, June 2013.