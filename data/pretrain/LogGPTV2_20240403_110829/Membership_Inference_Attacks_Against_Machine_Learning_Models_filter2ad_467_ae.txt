 0.9
 0.8
 0.7
 0.6
 0.5
10 Classes
20 Classes
50 Classes
100 Classes
 0
 0.01  0.02  0.03  0.04  0.05  0.06  0.07  0.08  0.09  0.1
Fraction of the Training Set for a Class
i
i
n
o
s
c
e
r
P
k
c
a
t
t
A
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0
10 Classes
20 Classes
50 Classes
100 Classes
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
Target Model (Train-Test) Accuracy Gap
p
a
G
y
c
a
r
u
c
c
A
)
t
s
e
T
-
n
a
r
T
(
l
i
e
d
o
M
t
e
g
r
a
T
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
Purchase Dataset, 10-100 Classes, Google, Membership Inference Attack
 0.7
10 Classes
20 Classes
50 Classes
100 Classes
 0
 0.01  0.02  0.03  0.04  0.05  0.06  0.07  0.08  0.09  0.1
Fraction of the Training Set For a Class
Fig. 11: Relationship between the precision of the membership inference attack on a class and the (train-test) accuracy gap of the target
model, as well as the fraction of the training dataset that belongs to this class. Each point represent the values for one class. The (train-test)
accuracy gap is a metric for generalization error [18] and an indicator of how overﬁtted the target model is.
Purchase Dataset, 10 Classes, Google, Membership Inference Attack
Purchase Dataset, 20 Classes, Google, Membership Inference Attack
Purchase Dataset, 100 Classes, Google, Membership Inference Attack
 1
 0.8
 0.6
 0.4
 0.2
 0
0
Members
Non-members
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
 1
 0.8
 0.6
 0.4
 0.2
 0
0
Members
Non-members
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Prediction Uncertainty
Prediction Uncertainty
 1
 0.8
 0.6
 0.4
 0.2
 0
0
Members
Non-members
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Prediction Uncertainty
Purchase Dataset, 10 Classes, Google, Membership Inference Attack
Purchase Dataset, 20 Classes, Google, Membership Inference Attack
Purchase Dataset, 100 Classes, Google, Membership Inference Attack
 1
 0.8
 0.6
 0.4
 0.2
 0
0
Members
Non-members
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
 1
 0.8
 0.6
 0.4
 0.2
 0
0
Members
Non-members
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Prediction Accuracy
Prediction Accuracy
 1
 0.8
 0.6
 0.4
 0.2
 0
0
Members
Non-members
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Prediction Accuracy
Fig. 12: Classiﬁcation uncertainty (top row) and prediction accuracy (bottom row) of the target model for the members of its training dataset
vs. non-members, visualized for several sample classes. The difference between the member and non-member output distributions is among
the factors that our attack exploits to infer membership. The accuracy of our attack is higher for the models where the two distributions are
more distinguishable (See Table II).
i
(cid:2)
−1
log(n)
input with label i as i. Prediction uncertainty is the normalized
pi log(pi),
entropy of the model’s prediction vector:
where pi is the probability that the input belongs to class i,
and n is the number of classes. The plots show that there
is an observable difference between the output (both accuracy
and uncertainty) of the model on the member inputs versus the
non-member inputs in the cases where our attack is successful.
Success of membership inference is directly related to the
(1) generalizability of the target model and (2) diversity of its
training data. If the model overﬁts and does not generalize well
to inputs beyond its training data, or if the training data is not
representative, the model leaks information about its training
inputs. We quantify this relationship in Fig. 11. From the
machine learning perspective, overﬁtting is harmful because
it produces models that lack predictive power. In this paper,
we show another harm of overﬁtting: the leakage of sensitive
information about the training data.
As we explained in Section VI, overﬁtting is not the only
reason why our inference attacks work. Different machine
learning models, due to their different structures, “remember”
different amounts of information about their training datasets.
This leads to different amounts of information leakage even if
the models are overﬁtted to the same degree (see Table I).
VIII. MITIGATION
As explained in Section VII, overﬁtting is an important
(but not the only) reason why machine learning models leak
information about their training datasets. Of course, overﬁtting
is a canonical problem in machine learning because it limits
the predictive power and generalizability of models. This
means that instead of the usual tradeoff between utility and
privacy, machine learning research and privacy research have
similar objectives in this case. Regularization techniques such
as dropout [31] can help defeat overﬁtting and also strengthen
14
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:20:35 UTC from IEEE Xplore.  Restrictions apply. 
privacy guarantees in neural networks [23]. Regularization is
also used for objective perturbation in differentially private
machine learning [9].
(Ideal) well-regularized models should not leak much infor-
mation about their training data, and our attack can serve as
a metric to quantify this. Also, models with a trivial structure
(e.g., XOR of some input features) generalize to the entire
universe and do not leak information.
If the training process is differentially private [12],
the
probability of producing a given model from a training dataset
that includes a particular record is close to the probability of
producing the same model when this record is not included.
Differentially private models are, by construction, secure
against membership inference attacks of the kind developed in
this paper because our attacks operate solely on the outputs of
the model, without any auxiliary information. One obstacle is
that differentially private models may signiﬁcantly reduce the
model’s prediction accuracy for small  values. In Section IX,
we survey some of the related work in this area.
In the case of machine learning as a service, platform
operators such as Google and Amazon have signiﬁcant re-
sponsibility to the users of their services. In their current
form, these services simply accept the data, produce a model
of unknown type and structure, and return an opaque API to
this model that data owners use as they see ﬁt, without any
understanding that by doing so, they may be leaking out their
data. Machine learning services do not inform their customers
about the risks of overﬁtting or the harm that may result
from models trained on inadequate datasets (for example, with
unrepresentative records or too few representatives for certain
classes).
Instead, when adaptively choosing a model for a customer-
supplied dataset, services such as Google Prediction API and
Amazon ML should take into account not only the accuracy of
the model but also the risk that it will leak information about
its training data. Furthermore, they need to explicitly warn
customers about this risk and provide more visibility into the
model and the methods that can be used to reduce this leakage.
Our inference attacks can be used as metrics to quantify
leakage from a speciﬁc model, and also to measure the
effectiveness of future privacy protection techniques deployed
by machine-learning services.
A. Mitigation strategies
We quantitatively evaluate several defenses against mem-
bership inference.
Restrict the prediction vector to top k classes. When the
number of classes is large, many classes may have very small
probabilities in the model’s prediction vector. The model will
still be useful if it only outputs the probabilities of the most
likely k classes. To implement this, we add a ﬁlter to the last
layer of the model. The smaller k is, the less information the
model leaks. In the extreme case, the model returns only the
label of the most likely class without reporting its probability.
Coarsen precision of the prediction vector. To implement
this, we round the classiﬁcation probabilities in the prediction
Purchase dataset
No Mitigation
Top k = 3
Top k = 1
Top k = 1 label
Rounding d = 3
Rounding d = 1
Temperature t = 5
Temperature t = 20
L2 λ = 1e − 4
L2 λ = 1e − 3
L2 λ = 1e − 2
Hospital dataset
Testing
Accuracy
0.66
0.66
0.66
0.66
0.66
0.66
0.66
0.66
0.68
0.72
0.63
Attack
Total Accuracy
0.92
0.92
0.89
0.66
0.92
0.89
0.88
0.84
0.87
0.77
0.53
Attack
Precision
0.87
0.87
0.83
0.60
0.87
0.83
0.86
0.83
0.81
0.73
0.54
Attack
Recall
1.00
0.99
1.00
0.99
0.99
1.00
0.93
0.86
0.96
0.86
0.52
Testing
Accuracy
Attack
Total Accuracy
Attack
Precision
0.55
0.55
0.55
0.55
0.55
0.55
0.55
0.55
0.56
0.57
0.56
0.35
0.83
0.83
0.82