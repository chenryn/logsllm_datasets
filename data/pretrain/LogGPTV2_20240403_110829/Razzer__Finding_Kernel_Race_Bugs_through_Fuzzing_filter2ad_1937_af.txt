number of RacePairscand obtained by RAZZER from the entire
kernel through its static analysis. For a clearer understanding of
this number, we also show the total number of paired memory
access instructions, approximating the upper bound number of
RacePairscand results, in the kernel which is partitioned in a
way that RAZZER does.
For all kernel versions, RAZZER produced 0.05%, 0.04%,
and 0.05% of RacePairscand compared to the paired number
of memory access instructions. This suggests that RAZZER’s
static analysis effectively guides its fuzzer to focus only on
less than 0.1% of potential racy spots, effectively avoiding an
enormous number of non-racy spots to be explored.
C. Hypervisor Overhead
Given that RAZZER utilizes hypercalls to enable the de-
terministic behavior of vCPUs, it requires extra overhead to
communicate with the hypervisor. To understand how much
overhead is incurred due to the hypervisor, we measured the
elapsed time of each hypercall 100 M times and computed
the average. We used user-space clock_gettime() as a timer
function for these measurements. For a simple comparison of
RAZZER’s hypercall overhead, we implemented a no-op hyper-
call (i.e., hcall_nop()) as a baseline hypercall. hcall_nop()
does nothing and immediately returns from the hypervisor once
invoked.
Figure 12 shows the performance overhead of RAZZER’s
hypercalls. As shown in the ﬁgure, our hypercalls incur
overhead ranging from 4.69μs to 5.64μs. This implies that to
run each Pmt in the multi-thread executor, RAZZER’s hypercall
based mechanism would requires about 14.92μs (i.e., 5.46 +
4.77 + 4.69), which would not be signiﬁcant overhead. In partic-
ular, while hcall_set_order() and hcall_check_race() only
incurs 8% and 9% of the overhead compared to hcall_nop(),
hcall_set_bp() incurs 25%, as hcall_set_bp() must install
a hardware breakpoint within the corresponding vCPU, which
requires extra switching steps in our underlying hypervisor
designs—i.e., our base hypervisor, KVM, must switch to the
host kernel to access vCPU registers.
Syzkaller [42]
RAZZER
Throughput
144 K
Single
151 K
Multi
Avg.
86 K
118 K
Fig. 13: Fuzzing throughput of Syzkaller and RAZZER. We present
the number of execution per machine for one hour. Single/Multi
denotes the throughput on machines running single-thread/multi-thread
fuzzing (v4.17-rc1).
D. Comparison Study of the Fuzzing Efﬁciency
1) Finding Offending User Programs: This subsection
demonstrates how well RAZZER ﬁnds a user program triggering
a race (i.e., how well RAZZER meets R1 in §II-B). We
measured two different aspects: (1) the fuzzing throughput,
showing how many input program instances that RAZZER
can execute for a certain period of time; and (2) the number
of executions required to ﬁnd a user program triggering a
previously known harmful race. In these measurements, we
also compare RAZZER’s performance with that of Syzkaller
to clarify how well RAZZER meets R1 compared to the
state-of-the-art kernel fuzzing tool. In summary, the fuzzing
throughput of RAZZER is worse than Syzkaller as expected,
because RAZZER utilizes a hypervisor to enforce deterministic
thread interleaving behavior. However, RAZZER is much faster
than Syzkaller when ﬁnding a user program triggering a
harmful race (i.e., 23 to 86 times faster at a minimum), as
deterministic thread interleaving behavior towards potential
racy spots signiﬁcantly reduces the search space to be fuzzed.
Execution Throughput. We measure the number of user pro-
grams executed by RAZZER, both in the single-thread (§III-C1)
and multi-thread fuzzing phases (§III-C1).
Figure 13 shows the execution throughput result after
running both tools for 10 minutes. The number in each
cell represents the averaged number per VM for an easy
comparison. As expected, RAZZER’s multi-thread fuzzing
shows lower throughput than RAZZER’s single-thread fuzzing
and Syzkaller, mainly due to two extra jobs which arise during
the multi-thread fuzzing step. First, it performs extra hypercalls,
where the invocation itself imposes extra overhead, and second,
if both breakpoints remain unﬁred, the hypervisor should wait
until its own timer expires.
One interesting aspect to note here is that RAZZER’s single-
thread fuzzing shows higher throughput than that of Syzkaller.
This occurs because RAZZER’s single-thread executor is truly
single-threaded whereas Syzkaller spawns multiple worker
threads to identify data races.
Required Number of Executions to Find a Race. The
execution throughput of fuzz testing may be an indirect measure
of how efﬁcient the fuzz technique is, but a more direct and
(cid:24)(cid:23)(cid:21)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:46:09 UTC from IEEE Xplore.  Restrictions apply. 
important measure should be the time required to ﬁnd bugs (i.e.,
a harmful race in this paper). To demonstrate this, we measured
the number of executions required to discover previously known
harmful races, CVE-2017-2636, CVE-2016-8655, and CVE-
2017-17712, while running for 10 hours. To trigger the race
within a reasonable time for this experiment, we conﬁgured the
program generation grammar of both RAZZER and Syzkaller
as a limited set of syscalls that is related to the target race
bug. For a fair evaluation, we used the same conﬁguration for
both tools and provided RacePaircand generated from the entire
kernel to RAZZER (i.e., fewer RacePaircand leads to a more
efﬁcient search for RAZZER).
As shown in Figure 14, RAZZER found all of these previously
known races with a reasonable number of execution (i.e., from
246 K to 1,170 K) from as well as within a reasonable amount of
time (i.e., from 7 minutes to 26 minutes). However, Syzkaller
failed to ﬁnd all of these cases, although executed from 5 M
to 37 M generated/mutated programs over the duration of 10
hours. Particularly based on these CVE cases, these outcomes
suggest that RAZZER is faster than Syzkaller, ranging from
23 to 85 times at least.
2) Finding Offending Thread-Interleaving cases: In order
to show how well RAZZER ﬁnds a thread interleaving for a
given program to trigger a race (i.e., how well RAZZER meets
R2 in §II-B), we performed the comparison study between
RAZZER and SKI.
Experimental Setting. Because we do not have access to the
implementation of SKI, we implemented SKIEmu by extending
RAZZER with SKI’s random thread interleaving features. The
key to implement SKIEmu lies in implementing its random thread
interleaving feature, which is done by modifying RAZZER’s
multi-thread fuzzing phase. Instead of utilizing RAZZER’s
hypercalls for the per-core breakpoint at RacePaircand, SKIEmu
performs random thread interleaving, as shown in Figure 2.
More speciﬁcally, SKIEmu continues to randomly select one
vCPU and then executes it until it meets any memory access
instruction. Thus, SKIEmu interleaves the execution as it faces
the memory access instruction, which is identical to what SKI
originally proposed.
In this experiment, we assume that RAZZER and SKIEmu
obtained a user program triggering a harmful race so as to
focus on evaluating the R2 aspect. This is supported by turning
off the input generator during the single-thread fuzzing phase
ahd then simply providing a single user program to it. We
used three programs, each of which already triggers previ-
ously known harmful races. These are CVE-2017-2636 [28],
CVE-2016-8655 [26], and CVE-2017-17712 [27].
Number of Executions to Find Races. Figure 15 shows
the number of required executions to trigger each race while
running RAZZER and SKIEmu. As RAZZER only explores
thread interleaving related to RacePairscand (spotted by running
a given user program) and thus explores far fewer thread
interleaving cases, RAZZER requires far fewer executions
than SKIEmu, ranging from 30 times to 398 times less. This
result also suggests that many thread interleaving cases that
are explored by SKIEmu are not related to races, signifying
RAZZER’s effectiveness in meeting R2.
VI. RELATED WORK
In this section, we discuss work related to RAZZER, partic-
ularly focusing on techniques that can identify (or assist in the
identiﬁcation of) data races.
Dynamic Fuzz Testing. Many recent studies have demon-
strated that fuzzing is a promising technique to ﬁnd bugs
in user-land programs [8, 12, 18, 33, 34, 44, 46] and in
kernels [13, 19, 23, 24, 42, 43, 45]. The key advantage of
fuzzing is not only that this method efﬁciently ﬁnds bugs
in target programs but also that it does not suffer from false
positives as it generates an input reproducing a bug. However, to
the best of our knowledge, all fuzzing techniques are inefﬁcient
when used to identify race bugs mainly because their designs
are not tailored to races. While most fuzzers focus on leveraging
previously explored execution coverage, they do not consider
thread interleaving (i.e., traditional fuzzers do not not meet
R2 in §II-B). Compared to these, RAZZER considers both the
execution coverage and thread interleaving to discover data
races more effectively.
Dynamic Thread Scheduler. Several studies [10, 16, 30, 35,
40] have attempted to ﬁnd instances of race-causing thread
interleaving by implementing a customized thread scheduler
which randomizes the per-thread execution scheduling. In
particular, the PCT Algorithm [10] and SKI [16] discover races
in user programs or the kernel through exploring all possible
thread interleaving cases. Limitations of these two methods
are: (i) they do not consider R1 (§II-B) as they do not generate
(or mutate) an input program and thus it cannot ﬁnd a new
program which triggers data race; and (ii) they are not efﬁcient
in meeting R2 as they must search the very large spaces of
all possible thread interleaving cases. In fact, the design of
RAZZER is inspired by the PCT algorithm and SKI—it meets
R1 by tailoring the fuzzing process while efﬁciently meeting
R2 by undertaking prioritized searches over RacePairscand.
Dynamic Race Detector. Many studies [7, 9, 11, 15, 20, 22,
25, 31, 32, 36, 40, 47, 48] have sought to improve the race
detection capability at runtime by collecting rich contextual
information on races. These are essentially orthogonal to
RAZZER—once deployed together with RAZZER, RAZZER’s
race detection capability can be augmented as well. This
orthogonal relationship is similar how traditional fuzzers
(focusing on identifying memory corruption bugs) run with
extra sanitizers (i.e., AFL [46] and Syzkaller [42] strongly
encourage their users to run them with AddressSanitizer [37]
or MemorySanitizer [38]).
In particular, ThreadSanitizer [36] is a commodity race
detector developed by Google which was recently utilized
with the Linux kernel as well. To augment the performance
when detecting races, TxRace [47] leverages the hardware
transactional memory and ProRace [48] utilizes a performance
monitoring unit. Memory sampling techniques [9, 11, 15, 25]
selectively monitor memory accesses to optimize the perfor-
mances. RaceMob [21] crowdsources runtime race tests from
(cid:24)(cid:23)(cid:22)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:46:09 UTC from IEEE Xplore.  Restrictions apply. 
Race Bugs
Syzkaller [42]
# exec
Time
Found
CVE-2017-2636
CVE-2016-8655
CVE-2017-17712
5 M
29 M
37 M
10 hrs
10 hrs
10 hrs
Single
169 K
821 K
565 K
×
×
×
RAZZER
Total
246 K
1,170 K
807 K
# exec
Multi
77 K
349 K
242 K
Time
Found
7 mins
26 mins
18 mins
(cid:2)
(cid:2)
(cid:2)
Fig. 14: Efﬁciency of Syzkaller and RAZZER in ﬁnding a user program that triggers a race. We measured the total number of executions
and the time required to ﬁnd previously known races. RAZZER found all of the known races within a reasonable amount of time, while
Syzkaller did not ﬁnd any within the given time of 10 hours (v4.8).
CVE
CVE-2017-2636
Found
Total
CVE-2016-8655
Found
Total
CVE-2017-17712
Found
Total
SKIEmu [16]
RAZZER
2,038
8
6,435
23
636
21
8,008
43
8,362
21
27,132
56
Fig. 15: Efﬁciency of SKIEmu and RAZZER in uncovering thread
interleaving that triggers a race. Found column shows the required
number of executions to ﬁnd the interleaving (obtained by repeating
the experiment 5 times and computing the average), and the Total
column shows the theoretical maximum number of required executions
for each tool (v4.8).
potential data races that are generated by the static analysis.
Snorlax [22] suggests a coarse interleaving hypothesis to
leverage coarse-grained timing information to determine the
thread interleaving of events.
Static Analysis. A static analysis has been used extensively
to discover unknown bugs. In this category, we focus our
discussion on static analysis works relevant to either race bug
detection or points-to analysis implementations. Relay [41]
is a static race detector for large programs such as kernels.
Relay generates RacePairscand by performing a lockset-based
bottom-up analysis while summarizing each function’s behavior.
RacerX [14] is also designed to ﬁnd race conditions and
deadlocks for large, complex multi-threaded systems. Due
to the limitations of using static analysis techniques alone,
these are essentially incurring a high false positive rate (e.g.,
Relay showed a false positive rate of 84% on the Linux kernel),
critically limiting their usage in practice. However, RAZZER
also leverages dynamic analysis techniques, addressing the
possibility of high false positive rates. In terms of points-to
analysis implementations, K-miner [17] was recently presented
to uncover memory corruption vulnerabilities in commodity
operating systems through an inter-procedural and context-
sensitive analysis. RAZZER’s static analysis is built based
on the implementation of K-miner’s but modiﬁed to identify
RacePairscand through a points-to analysis.
VII. DISCUSSION
False Negatives in Static Analysis. Since RAZZER relies on
the results of the static analysis, if any true race pair is missing
from RacePaircand, it would lead to false negatives of RAZZER.
Such missing cases of the static analysis may occur mainly due
to the partition analysis. Speciﬁcally, the partition analysis of