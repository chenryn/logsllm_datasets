: 4
This output shows that the production application usually spends only a short amount of time
on CPU: from 0 to 127 microseconds.
Here is a CPU-heavy workload, with more busy threads than CPUs available, and with
a histogram in milliseconds (n):
 cpudist -B
Tracing on-CPU tine.
Hit Ctel-C to end.
C
nsecs
: count
distribution
0 -> 1
: 521
2 -> 3
: 60
|****
1.3 0rigin: 1 crested cpudists on 27-Apr-2005, 
thread. Sssha Goldshtein developed the BCC cpudist(8) on 29-Jun-2016, with options for per-process distributions.
---
## Page 249
212
2Chapter 6 CPUs
4 -> 7
: 272
8 -> 15
: 308
16 > 31
: 66
|*****
32 -> 63
: 14
Now there is a mode of on-CPU durations from 4 to 15 milliseconds: this is likely threads
exhausting their scheduler time quanta and then encountering an involuntary context switch.
This tool was used to help understand a Netflix production change, where a machine learning
application began running three times faster. The perf(1) command was used to show that the
context switch rate had dropped, and cpudist(8) was used to explain the affect this had: the appli-
cation was now usually running for two to four milliseconds between context switches, whereas
earlier it could only run for between zero and three microseconds before being interrupted with a
context switch.
cpudist(8) works by tracing scheduler context switch events, which can be very frequent on busy
production workloads (over one million events/sec). As with runqlat(8), the overhead of this tool
could be significant, so use it with caution.
Command line usage.
cpudist [options][interra][count]]
Options include:
m: Prints output in milliseconds (default is microseconds)
 0: Shows off-CPU time instead of on-CPU time
• P: Prints a histogram per process
fjuo saooud spuqu samseap :a1a d-
There is currently no bpftrace version of cpudist(8). I’ve resisted creating one and instead have
added it as an optional exercise at the end of this chapter.
6.3.7cpufreq
cpufreq(8) samples the CPU frequency and shows it as a system-wide histogram, with
per-process name histograms. This only works for CPU scaling governors that change the
cations are running, For example:
frequency, such as powersave, and can be used to determine the clock speed at which your appli
O'Brien, with some initial work by Joel Fermandes; it uses sched tracepoints to track the freguency more precisely
---
## Page 250
6.3  BPF Tools
213
+cpufreq.bt
Ctrl-C to end.
^C
[..-]
process_nhz [snnpd] :
[1200, 1400]
1 18e988 88e886 88ee8ee8e98ee8ee88 88e886 88ee8ee8e98ee881
prc
s_mhz [pyt)
n3] :
[1600, 1800]
118
[1800, 2000]
01
[2000, 2200]
[2200, 2400]
0 1
[2400,2600]
[2600, 2800]
21869
[2800, 3000]
0 1
[3000, 3200]
2918ee88e88e88 88 e88e88e8ee8ee88e88e88e 88ee8ee8ee8ee881
[1200, 1400]
216 1eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee8eeeeeeeeeeeee1
[1400, 1600]
23188988
[1600, 1800]
18188e8
[1800, 2000]
161889
[2000, 2200]
12188
[2200, 2400]
0 1
[2400, 2600]
41
[2600, 2800]
2  1
[2800,3000]
80e819T
11
[3000, 3200]
Psysten_nhz:
[1200, 1400]
22041 18ee88e88e88 88 e8ee8ee8ee8ee88e88e88 88 e88e88e8ee88 
[1400, 1600]
08l E06
[1600, 1800]
47418
[1800, 2000]
3681
[2000, 2200]
301
[2200, 2400]
31
[2400, 2600]
211
[2600, 2800]
331
[2800, 3000]
151
[3000,3200]
2701
[...]
---
## Page 251
214
Chapter 6 CPUs
This shows that, system-wide, the CPU frequency was usually in the 1200 to 1400 MHz range, so
this is a mostly idle system. Similar frequencies were encountered by the java process, with only
some samples (18 while sampling) reaching the 3.0 to 3.2 GHz range. This application was mostly
doing disk I/O, causing the CPUs to enter a power saving state, python3 processes were usually
running at full speed.
d qoea go paads aug atuquaap oq squpodaoen atueqo Aouanbau Supen q sxom fooq s
and then samples that speed at 100 Hertz. The performance overhead should be low to
u pas se rouaao8 Sueos anesiamod au Sursn tuasis e tuosy st sndpno snoaud auL apqig8au
/sys/devices/system/cpu/cpufreq/./scaling_governor. When the system is set to the performance
governor, this tool shows nothing as there are no more frequency changes to instrument: the
CPUs are pinned at the highest frequency.
Here is an excerpt from a production workload I just discovered:
process_nhz[nginx] :
[1200, 1400]
35 188e88e88e888888e88e88e88e88e88e88e88
[1400, 1600]
17188e88e88e88 888e
[1600, 1800]
16 1eeeeeeeeeeeeeeee
1
[1800, 2000]
8688888868868868816T
[2000, 2200]
01
[2200, 2400]
0 1
[2400, 2600]
01
[2600, 2800]
0 1
0 1
[2800, 3000]
[3000, 3200]
0 1
[3200, 3400]
 1
[3400, 3600]
0 1
/
(008E 009E1
50 1eee88e88e88e88ee8ee8ee8ee8ee88e88e88e88ee8ee8ee8ee8e1
It shows that the production application, nginx, was often running at low CPU clock frequencies.
The CPU scaling_governor had not been set to performance and had defaulted to powersave.
The source for cpufreq(8) is:
#1/uax/1ocal/bin/bpCtrace
BEGIN
printf(*Sanpling CPo freq systenvlde & by process. Ctrl-C to end. \n"1
tracepointipoxer:cpu_frequency
Bcurfreg|cpu] = args->state,
---
## Page 252
6.3 BPF Tools
215
profile:hz:100
/Bcuzfreq Icpu] /
Bsysten_nhz = 1h1st (ecurfxeq[cpu] / 1000, 0, 500o, 200] ;
i.f(pid)[
eprocess_nhz[conm] = 1hlst (@curfεeq[cpu] / 100o, 0, 500o, 200] ;
END
clear (8cuxfreq) :
The frequency changes are traced using the power:cpu_frequency tracepoint and saved in
a @curfreq BPF map by CPU, for later lookup while sampling. The histograms track frequencies
from O to 5000 MHz in steps of 200 MHz; these parameters can be adjusted in the tool if needed.
6.3.8 profile
profile(8) 1 is a BCC tool that samples stack traces at a timed interval and reports a frequency
count of stack traces. This is the most useful tool in BCC for understanding CPU consumption as
it summarizes almost all code paths that are consuming CPU resources. (See the hardirqs(8) tool
in Section 6.3.14 for more CPU consumers.) It can also be used with relatively negligible overhead,
as the event rate is fixed to the sample rate, which can be tuned.
By default, this tool samples both user and kernel stack traces at 49 Hertz across all CPUs. This can
be customized using options, and the settings are printed at the start of the output. For example:
+ profi1e
Sanpling at 49 Bertz of all threadis by user + kernel stack... Bit Ctrl-C to end.
°C
sk_strean_a11oc_s3b
pexootfsupuesdos
bsupuss"doq
sock_sendmsg
1.5 0rigin: there have been many proflers in the past, including @prof from 1.982 [Grsham 82] (rewritten in 1.988 by Jay
Fenlason for the GNU project). I developed this version for BCC on 15-Ju-2016, bsed on code from Sasha Goldshtein,
Andrew Birhal, Evgeny Vereshchgin, and Teng Qin. My first version predted kemel support and worked by using 8
hack: I added a tracepoint on perf samples, to be used in conjunction with perf_event_open(). My petch to add this
tracepoint to Linux was rejected by Peter Zijistra, in favor of developing proper profiling support with BPIf, which Alexei
peppe aoqoxoueis
---
## Page 253
216
6Chapter 6 CPUs
sock_vrite_iter
_vfs_xr1te
vfs_vrite
ksys_wz1te
do_sysca11_64
entzy_sYscALL_64_after_hvfrase
_GI__vrite
[unknoxn]
iperf (29136)
[..-]
(0"se6edeexg
_free_pages_ok
elep"eseetex"gxs
_kfree_skb
ccp_ack
tcp_rcv_established
tcp_v4_do_xcv
_release_sock
release_sock
tcp_sendnsg
sock_sendxsg
sock_vrite_iter
_vfs_xr1te
vfs_vrite
ksys_wz1te
do_sysca11_64
entzy_sYscALL_64_after_hvfrase
_GI__vrite
[unknoxn]
iperf (29136)
1889
sxguoxg6eds6
get_poge_fron_freelist
xseuepoussbedoTte
skb_page_frag_refi11
sk_page_fraq_ref111
tcp_sendnsg_lecked
ccp_sendnsg
Brapuas"x>os
---
## Page 254
6.3BPF Tools217
sock_vrite_iter
_vfs_xr1te
vfs_vrite
ksys_wz1te
do_sysca11_64
entzy_sYscALl_64_after_hufrase
_GI_xrite
[unknoxn]
iperf (29136)
2 673
The output shows the stack traces as a list of functions, followed by a dash (*-°) and the process 
name and PID in parentheses, and finally a count for that stack trace. The stack traces are printed
in frequency count order, from least to most frequent.
The full output in this example was 17,254 lines long and has been truncated here to show only the
first and final two stack traces. The most frequent stack trace, showing a path through vfs_write)
and ending with get_page_from_freelist() on CPU, was seen 2673 times while sampling.
CPU Flame Graphs
Flame graphs are visualizations of stack traces that can help you quickly understand profile(8)
output. They were introduced in Chapter 2.
To support flame graphs, profile(8) can produce output in folded format using f: Stack traces are
printed on one line, with functions separated by semicolons. For example, writing a 30-second
profile to an out.stacks01 file and including kernel annotations (a):
+ profile -af 30 > out.stacks01
+ tai1 -3 out.stacks01
iperf;
[unknoun] ._GIxritezentry_SYscALL_64_after_hvframe_[k];do_syscall_64_[k] =kays_veit
6upsd[x[]xxo[x]xs[x1xxs[x]
[k]tcg_sendnsg_locked_[k]>_copy_fron_iter_full_[k] copyin_[k] rcopy_user_enhanced_fas
t_sting_Ik ; copy_user_enhanoed_fast_stx1ng_Ik] 5844
iperf;
[unknoun] .GIwrite;entzy_sYscALL_64_aCter_hvframe_[k];do_ayscall_64_[k=ksys_vz1 t
e_[k] ;vfs_write_Ik] vfs_rite_[k];sock_rite_iter_[k];sock_sendnsg_[k]>top_sendmsg_
[k≠zelease_sock_[k]:release_sock_[k]:tcp_v4_do_rcv_[k];tcp_rcv_established_[k] tcp
_ack_[k] ;__kfree_skb_[k] ;skb_release_data_[k] ;_free_pages_ok_[k] :__free_pages_ok_[k]
ETLOT
iperf;
[unknoun] ._GI_write;entzy_SYscALL_64_after_hvframe_[k];do_syaca1ll_64_[k=ksys_wz1 t
e_[k] ;vfs_xrite_[k] __vfs_xrite_[k]sock_rite_iter_[k] ;sock_sendinsg_[k] tcp_sendmsg_
[k] :tcp_sendnsg_locked_[k];sk_page_frag_ref11l_[k];skb_page_Czaq_refi1l_Ik]._alloc_p
sges_nodemask_[k] iget_page_fron_freelist_Ik) get_poge_from_reelist_[k] 15088
---
## Page 255
218
8Chapter 6 CPUs
Only the last three lines are shown here. This output can be fed into my original flame graph
software to generate a CPU flame graph:
S od FlaneGraph
$ git clone https://github ,com/brendangregg/FlameGraph
$./flamegraph.pl --color=java  out.svg
flamegraph.pl supports different color palettes. The java palette used here makes use of the kernel
annotations ("_[k]°) for choosing color hues. The generated SVG is shown in Figure 6-5.
Flame Graph
Figure 6-5 CPU flame graph from BPF sampled stacks
This flame graph shows that the hottest code paths ended in get_page_from_freelist_()
and __free_pages_ok_()these are the widest towers, with width proportional to their frequency
in the profile. In a browser, this SVG supports click-to-zoom so that narrow towers can be
expanded and their functions read.
What makes profile(8) different from other CPU profilers is that this frequency count is calculated
in kernel space for efficiency. Other kernel-based profilers, such as perf(1), send every sampled
asuadxa d aq ueo su rerns e opu passasd-sod st # aaum aoeds rasn o aoen goess
and, depending on the invocation, it can also involve file system and disk I/O to record the
samples, profile(8) avoids those expenses.
---
## Page 256
6.3BPF Tools
219
Command line usage
[Kouenbexgα][suordo] TTgoxd]
Options include:
U: Includes user-level stacks only
* K: Includes kernel-level stacks only
-a: Includes frame annotations (e.g-- *_[k]* for kernel frames)
■ -d: Includes delimiters between kernel/user stacks
 f: Provides output in folded format
fuo saoosd stu sayoag :aia d-
bpftrace
The core functionality of profile(8) can be implemented as a bpftrace one-liner:
bpftrace -e *profile:hz:49 /pid/ [ Bsamples[ustack, kstack, comm] = count(1= *
This frequency-counts using the user stack, kernel stack, and process name as the key. A filter on
the pid is included to ensure that it is non-zero: this excludes the CPU idle thread stacks. This
one-liner can be customized as desired.
6.3.9offcputime
offcputime(8) ° is a BCC and bpftrace tool to summarize time spent by threads blocked and
off CPU, showing stack traces to explain why. For CPU analysis, this tool explains why threads
are not running on a CPU. It's a counterpart to profile(8); between them, they show the entire
time spent by threads on the system: on-CPU time with profile(8) and off-CPU time with
(g)aundoo
The following example shows offcputime(8) from BCC, tracing for five seconds:
+ offcputime 5
[..-]
16 Origin: I crested off-CPU analysis as a methodology, and DTrote oneliners to apply it, in 2005, after exploring uses
of the DTrace sched provider and its sched:ffcpu probe. When I first explained this to a Sun engineer in Adelaide, he
said 1 should not callit *ff-CPU since the CPU isnt offI My first o-CPU tools were uoffpu.d and kffpu.d in 2030
for my DTrace book [Gregg 11]. For inux, Ipublished off-CPU analysis using perf(1), with extremely high overhesd, on
26-Feb-2015. 1 finaly developed offcputime efficiently using BCC on 13-Jlan-2016, and bpftrace for this book on
16-Feb-2019
---
## Page 257
220
Chapter 6 CPUs
finish_task_svitch
schedule
schedule_timeout
xait_voken
sk_st.rean_vait_menory
pexootbsupuesdo
bsupuss"do
Inet_sendxsg
sock_sendmsg
sock_wzlte_lter
nev_sync_xrite
_vfs_xr1te
vfs_vrite
SyS_vrite
do_sysca11_64
entzy_sYscALL_64_after_hvfxame
_vri te
[unknoxn]
iperf (14657)
5 625
[..-]
flnish_task_svitch
schedule
schedule_Clneout
xa.it_voken