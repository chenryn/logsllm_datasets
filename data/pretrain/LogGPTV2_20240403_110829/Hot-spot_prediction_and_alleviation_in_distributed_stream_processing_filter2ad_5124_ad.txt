Load (Applications/Node)
 6
 8
 9
 10
Component
Average Prediction Error (%)
sort
project
aggregate
count
compare
2.875
7.872
0.838
2.019
4.904
Figure 17. Absolute average
rate prediction error.
Component
Average Prediction Time (ms)
sort
project
aggregate
count
compare
0.133
0.327
0.509
0.836
1.187
Figure 18. Average total pre-
diction time.
)
)
%
l
(
s
e
p
u
T
a
t
a
D
l
t
/
a
o
T
d
e
s
s
M
i
(
t
e
a
R
s
s
M
i
 50
 45
 40
 35
 30
 25
 20
 15
 10
 5
 0
Application QoS Improvement
Without Hot-Spot Elimination
With Hot-Spot Elimination
 1
 2
 3
 5
 7
 4
Load (Applications/Node)
 6
 8
 9
 10
Figure 19. Application QoS
improvement.
Application Performance Variation
Migration Overhead
l
)
s
(
y
a
e
D
d
n
E
-
o
t
-
d
n
E
n
o
i
t
a
c
i
l
p
p
A
 12
 10
 8
 6
 4
 2
 0
 0
 500
 1000
 1500
 2000
Data Tuple Sequence Number
s
n
o
i
t
a
r
g
M
i
f
o
r
e
b
m
u
N
 120
 100
 80
 60
 40
 20
 0
 0
 50
 100
 150
 200
 250
 300
 350
Number of Applications
)
%
(
r
o
r
r
E
n
o
i
i
t
c
d
e
r
P
e
g
a
r
e
v
A
e
t
l
u
o
s
b
A
l
)
s
(
y
a
e
D
d
n
E
-
o
t
-
d
n
E
n
o
i
t
a
c
i
l
p
p
A
e
g
a
r
e
v
A
Figure 20. Application per-
formance improvement.
Figure 21. Application per-
formance variation.
Figure 22. Migration over-
head.
The Figure magniﬁes, focusing on 2000 of the total tuples.
In Figure 22 we show the migration overhead to achieve
the hot-spot alleviation beneﬁts. The number of migrations
is shown as a function of the number of applications de-
ployed in the system. We observe that the number of migra-
tions grows linearly to the number of applications. On aver-
age, one migration every three applications is required. This
shows that on average one every three applications experi-
ence a hot-spot at some point during the execution, which
motivates the need for application-oriented hot-spot allevia-
tion. This assumes that not many applications require more
than one migration, in other words that the system is not so
overloaded that a migration does not permanently resolve
a hot-spot. We also measured the average time required to
perform a migration to be 1144ms. This time included the
complete distributed protocol execution described in Sec-
tion 4.2. The short migration time, together with the fact
that our migration protocol enables application execution to
continue while the migration is taking place ofﬂine, make
our hot-spot alleviation mechanism suitable for distributed
stream processing applications with QoS demands. Pre-
diction further facilitates fast reaction to a hot-spot, before
massive QoS violations occur.
6. Related Work
Distributed stream processing systems have been the
focus of a lot of recent research from different perspec-
tives. Work on the placement of components to make ef-
ﬁcient use of resources and to maximize application perfor-
mance [1, 13] is complementary to ours. Any technique for
deploying new components can be used, once all the nodes
hosting a particular component type are overloaded. Ad-
ditionally, the migration techniques presented in [13] can
be used as an alternative to our migration protocol, com-
plementing the prediction mechanisms for QoS violations
presented here. Similarly, work on component composi-
tion [8, 14] or application adaptation [2, 6, 10] can assist in
load balancing. Load balancing for distributed stream pro-
cessing applications has also been studied [3,18,24,25]. We
differ from these approaches in that we focus on the appli-
cation QoS, rather than the system utilization. Furthermore,
we propose a hot-spot prediction framework to drive proac-
tive migration decisions. In our previous work [15] we pre-
sented a peer-to-peer load balancing architecture, focusing
on reactive, node-oriented hot-spot detection that does not
utilize prediction. Load shedding [4, 21, 23] has been ex-
plored before as a means to alleviate application hot-spots
in stream processing systems. Our goal when alleviating
application hot-spots via migration is to do so in a less in-
trusive manner. Similar to our work, [23] identiﬁes the need
for proactive QoS management and proposes operator se-
lectivity estimation using sampling. Their methods however
refer to centralized stream processing on a single node.
Workload prediction has been studied in various con-
texts and [17] discusses how some workloads have been
shown to be most accurately represented by open mod-
els, while others by closed ones. Dinda [7] has shown
the effectiveness of linear models in predicting host load,
network bandwidth, and performance data.
In the do-
main of grid computing multi-resource prediction has been
proposed [11], where the processor utilization is cross-
correlated with the memory utilization. We also utilize
cross-correlation, but between different nodes rather than
between different resources. Performance prediction for
multi-tier web servers [19, 26] is also relevant to our work,
provided that all tiers are considered and not just one which
is assumed to be the bottleneck. [19] proposes a model
based on queuing theory, to predict performance as a func-
tion of the transaction mix. For stream processing applica-
tions however, rate ﬂuctuations rather than the type of re-
quired processing affect performance. For the same rea-
son, certain assumptions regarding the distribution of ar-
rival rates that are needed for queueing analysis, may not
hold. [26] proposes a model based on regression to predict
the processing cost of web transactions and drive capacity
planning decisions. We also employ linear regression but
focus on online execution time prediction.
7. Conclusions
We have described hot-spot prediction and alleviation
mechanisms for distributed stream processing applications.
Our algorithms for hot-spot prediction are based on the sta-
tistical methods of linear regression and correlation, utiliz-
ing only light-weight, passive measurements. Statistics col-
lection and hot-spot prediction and alleviation are carried
out at run-time by all nodes independently, building upon
a fully decentralized architecture. The experimental eval-
uation of our techniques on the Synergy middleware over
PlanetLab, and using a real network monitoring applica-
tion operating on traces of real TCP trafﬁc, demonstrated
high prediction accuracy and substantial performance ben-
eﬁts with moderate monitoring and migration overheads.
References
[3] M. Balazinska, H. Balakrishnan, and M. Stonebraker.
Contract-based load management in federated distributed
systems. In NSDI, 2004.
[4] P. Barlet-Ros et al. Load shedding in network monitoring ap-
plications. In USENIX Annual Technical Conference, 2007.
[5] A. Bavier et al. Operating systems support for planetary-
scale network services. In NSDI, 2004.
[6] F. Chen, T. Repantis, and V. Kalogeraki. Coordinated me-
In
dia streaming and transcoding in peer-to-peer systems.
IPDPS, 2005.
[7] P. Dinda. Design, implementation, and performance of an
extensible toolkit for resource prediction in distributed sys-
tems. IEEE TPDS, 17(2):160–173, February 2006.
[8] X. Gu, P. Yu, and K. Nahrstedt. Optimal component compo-
sition for scalable stream processing. In ICDCS, 2005.
[9] L. Kleinrock. Queueing Systems. Volume 1: Theory. John
Wiley and Sons Inc., New York, NY, USA, 1975.
[10] V. Kumar, B. Cooper, Z. Cai, G. Eisenhauer, and K. Schwan.
Resource-aware distributed stream management using dy-
namic overlays. In ICDCS, 2005.
[11] J. Liang, K. Nahrstedt, and Y. Zhou. Adaptive multi-
resource prediction in distributed resource sharing environ-
ment. In CCGRID, 2004.
[12] D. Montgomery and G. Runger. Applied Statistics and Prob-
ability for Engineers. John Wiley & Sons Inc., NY, 2006.
[13] P. Pietzuch, J. Ledlie, J. Shneidman, M. Roussopoulos,
M. Welsh, and M. Seltzer. Network-aware operator place-
ment for stream-processing systems. In ICDE, 2006.
[14] T. Repantis, X. Gu, and V. Kalogeraki. Synergy: Sharing-
aware component composition for distributed stream pro-
cessing systems. In Middleware, 2006.
[15] T. Repantis and V. Kalogeraki. Alleviating hot-spots in peer-
to-peer stream processing environments. In DBISP2P, 2007.
[16] A. Rowstron and P. Druschel. Pastry: Scalable, distributed
object location and routing for large-scale peer-to-peer sys-
tems. In Middleware, 2001.
[17] B. Schroeder, A. Wierman, and M. Harchol-Balter. Open
versus closed: A cautionary tale. In NSDI, 2006.
[18] M. Shah, J. Hellerstein, S.Chandrasekaran, and M. Franklin.
Flux: An adaptive partitioning operator for continuous query
systems. In ICDE, 2003.
[19] C. Stewart, T. Kelly, and A. Zhang. Exploiting nonstation-
arity for performance prediction. In EuroSys, 2007.
[20] Stream Query Repository.
http://infolab.stanford.edu/stream/sqr/netmon.html, 2002.
[21] N. Tatbul, U. Cetintemel, S. Zdonik, M. Cherniack, and
M. Stonebraker. Load shedding in a data stream manager.
In VLDB, 2003.
[22] The Internet Trafﬁc Archive.
http://ita.ee.lbl.gov/html/contrib/lbl-tcp-3.html, 1994.
[23] Y. Wei, V. Prasad, S. Son, and J. Stankovic. Prediction-based
QoS management for real-time data streams. In RTSS, 2006.
[24] Y. Xing, J. Hwang, U. Cetintemel, and S. Zdonik. Providing
resiliency to load variations in distributed stream processing.
In VLDB, 2006.
[25] Y. Xing, S. Zdonik, and J. Hwang. Dynamic load distribu-
[1] Y. Ahmad and U. C¸ etintemel. Network-aware query pro-
cessing for stream-based applications. In VLDB, 2004.
[2] R. Arpaci-Dusseau. Run-time adaptation in river. ACM
Transactions on Computer Systems, 21(1):36–86, Feb. 2003.
tion in the Borealis stream processor. In ICDE, 2005.
[26] Q. Zhang, L. Cherkasova, and E. Smirni. A regression-based
analytic model for dynamic resource provisioning of multi-
tier appications. In ICAC, 2007.