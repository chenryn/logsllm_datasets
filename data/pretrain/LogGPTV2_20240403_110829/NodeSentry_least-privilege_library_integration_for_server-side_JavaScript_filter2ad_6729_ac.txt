var mod = new Module ( libName );
var customRequire = membranedRequire ( ctxt );
mod. require = function ( libName ) {
return customRequire ( libName ); };
return mod. loadLibrary ();
};
return newMembrane ( loadLib (). exports , policy );
}
Figure 10: While loading a library with safe_require,
the original require function is replaced with one
that wraps the public interface object with a
membrane and a given (Upper-Bound) policy.
require is called dynamically we can still catch it. Either
way, each time the function is called we can now test whether
a library we want to protect has been invoked.
Line 11 of Figure 10 shows how the public interface object
(exports), gets membraned with a given policy. This line
makes it possible to enforce Upper-Bound policies.
Lower-Bound policies can be enforced because of the
custom require function that is given to the context in which
a library gets loaded (line 4 in Figure 10). Because we provide
the context with our own require function, we can intercept
all its calls from any depending library. At interception
time, we can decide if it is necessary to membrane the public
interface object of that depending library (line 8 of Figure 11).
If decided so, all interactions between the library and its
depending library are eﬀectively subject to the Lower-Bound
policy. If not, the original interface objects get returned (line
11 of Figure 11).
7. EVALUATION
Performance is king for server-side JavaScript and the main
goal of our benchmarking experiment is to verify the impact
of introducing NodeSentry on the two major performance
drivers. We deﬁne performance as throughput, i.e., the
amount of tasks or total requests handled by our server, or as
capacity, i.e., the total amount of concurrent users/requests
handled by our server. These are standard measures for high
performance concurrent servers [19].
In order to streamline the benchmark and eliminate all
possible confounding factors, we have written a stripped
ﬁle hosting server that uses the "st" library to serve ﬁles
requests. The entire code of the server, besides the libraries
(cid:23)(cid:24)(cid:20)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
function membranedRequire (lib) {
return function (lib) {
var libexports ;
// [...] load the requested library
// and assign to libexports
if ( lowerBoundPolicyNeeds (lib )) {
} else {
return newMembrane ( libexports , policy );
return libexports ;
}
}
}
Figure 11:
In order to enforce a (Lower-Bound)
policy between a library and a depending library,
its interface object must be wrapped within a
membrane.
1
2
3
4
5
6
7
8
9
10
11
12
13
// set to false for plain Node .js
var enable_nodesentry = true ;
var st;
var http = require (" http ");
if ( enable_nodesentry ) {
require (" nodesentry ");
st = safe_require ("st");
} else { st = require ("st"); }
var handler = st( process .cwd ());
http . createServer ( handler ). listen (1337);
Figure 12: The streamlined benchmark application
implements a bare static ﬁle hosting server, by using
the popular "st" and "http" libraries.
"http" and "st", is shown in Figure 12. The only conditional
instruction present in the code makes it possible for us to
run the benchmark test suite at ﬁrst for pure Node.js and
then with NodeSentry enabled.
Each experiment (for plain Node.js and for Node.js with
NodeSentry enabled) consists of multiple runs. Each run
measures the ability of the web server to concurrently serve
ﬁles to N clients, for an increasingly large N, as illustrated
in Figure 13. Each client continuously sends requests for ﬁles
to the server throughout the duration of each experiment. At
ﬁrst only few clients are present (warm-up phase), after few
seconds the number of clients step up and quickly reaches the
total number N (ramp-up phase). The number of clients then
remains constant until the end of the experiment (peak phase)
with N clients continuously sending concurrent requests for
ﬁles.
The experimental setup consists of two identical machines6
interconnected in a switched gigabit Ethernet network. One
machine is responsible for generating HTTP requests by
spawning multiple threads, representing individual users.
The second machine runs Node.js v0.10.28 and acts as the
server.
6Each machine has 32 Intel© Xeon™ CPUs ES-2650 and
64GB RAM, running Ubuntu 12.04.4 LTS.
s
r
e
s
U
p
u
-
m
r
a
W
p
u
-
p
m
a
R
k
a
e
P
n
w
o
d
l
o
o
C
t0
t1 t2
t3
Time
Figure 13:
In our experimental set-up, the load
proﬁle of the experiment varies between a minimum
(the warm-up phase) and a maximum (the peak
phase) of concurrent users. This is repeated for
N = 1..1000 concurrent users sending requests to our
server.
The results of the experiment are summarized in Figure 14.
The left graphics reports the throughput: how many requests
the system is able to concurrently serve as the number of
clients increases. This value is represented on the y-axis
while the number of clients is represented on the x-axis.
The diagonal black line plots the theoretical maximum: all
requests by all clients are served in the given time horizon.
Each blue square represent the summary of the performance
of pure Node.js for the corresponding number of clients.
The red circles denote the performance of NodeSentry
for the same number of clients. The solid lines shows
the interpolation curve with the glm method in R with
a polynomial of grade 2. The gray shaded area represent the
95%-conﬁdence interval computed by the function.
The right graphics reports the capacity: the number of
concurrent requests handled at each time instance. The
coding of lines and data follows the same criteria as for
throughput: the blue squares and the blue line represents
Node.js data points and interpolated values whereas the
red line and the red circles represent the data points for
NodeSentry.
For the ﬁrst 200-250 all systems are able to serve requests
at essentially the theoretical maximum capacity of the local
benchmarking system. The system can comfortably host the
intended amount of threads/concurrent users without slow-
down. The results in Figure 14 indicate that NodeSentry’s
loss in capacity starts from around 200-250 concurrent users
whereas the capacity of a plain Node.js instance starts to
degrade at around 500 concurrent users.
NodeSentry gradually loses capacity until it stabilizes at
approx 40% loss over the plain Node.js capacity and then
moves in synchrony with NodeSentry after 500 users. It
starts gaining again after approx 800-900 users and reduces
the gap to 10%. Therefore, we can conclude that after 500 the
losses of capacity are no longer due to NodeSentry but are
directly consequence of the loss of capacity of Node.js. The
sprint-up at 1000 clients can be easily explained: the main
Node.js system is strained to keep up with performance, it has
lost already 40% of its capacity over the theoretical maximum.
In such stressful conditions, the additional constraints posed
by NodeSentry’s policy monitor are a drop in the sea.
We do not report data beyond 1000 users (albeit we tested
it) because the behavior of plain Node.js started to exhibit
signiﬁcant jitters. It showed that largely beyond 1000 the
actual capacity of our system set-up was dominated by other
factors (OS process swaps, network processes, caches, etc.
etc.). Setting up a benchmarking system that can smoothly
process 10.000 users and beyond is an interesting direction
for future work.
We’ve also measured the impact on the capacity of a server
between using only policy hook ("fs" inside the membrane)
and two policy hooks ("fs" outside the membrane).The
results shown in Figure 15 indicate that there is no signiﬁcant
loss of capacity by bounding the semi-trusted library at the
diﬀerent policy points and thus tightening the policy rules.
At ﬁrst, we stress again that up to 200 clients there is
no diﬀerence in performance, which brings us almost at
the same level of performance for an industrial security
events monitoring system, suitable for deployment at a small
business [23]. This strikingly compares with traditional
approaches for JavaScript client side security in which even
for one client there can be a performance penalty up to
300%.
For a larger number of clients there is a trade-oﬀ between
performance and security. Such trade-oﬀ is still limited (less
than 50% overhead) and decreases when other conditions
stretch the performance of the system. Just as in normal
program code, developers must take care to write eﬃcient
policy code. However, since policy code is written in plain
JavaScript, it can beneﬁt from eﬃciency measures in the un-
derlying JavaScript engine, like e.g., a JIT compiler. Further,
we believe that there are at least three ways to optimize the
performance. The overhead is mostly due to the peculiarities
of membranes: the overhead cost of the actual invariant
enforcement mechanism, e.g., its use of a shadow object, the
run-time post-condition assertions of the trap functions of
the membrane handler, and the reliance on a self-hosted
implementation of Direct Proxies in JavaScript [44, §5&6].
These would require a signiﬁcant engineering eﬀort that
would not be justiﬁed for a research implementation.
8. RELATED WORK
There is a large body of work on JavaScript security,
but the main focus has been overwhelmingly on client-side
security. A very comprehensive survey of many of the recent
works has been provided by Bielova [4] who describes a
variety of JavaScript security policies and their enforcement
mechanism within a web browser context. Therefore we refer
to her work for additional details and only focus here on the
few works that are closest to our own contribution.
JavaScript security in general.
Restricting third-party components within a web browser
or web application by mediating access to speciﬁc security-
sensitive operations, has seen a lot of attention since its rise
the last decade.
BrowserShield [36] is a server-side rewriting system that
rewrites certain JavaScript functions to use safe equivalents.
These safe equivalents are injected into each web page via
the BrowserShield JavaScript libraries. BrowserShield makes
use of proxies for injecting its code into a web page. Self-
protecting JavaScript [28, 35] is a client-side wrapping tech-
nique that applies an advice around JavaScript functions.
The wrapping code with its advices are provided by the
server and executed ﬁrst, to make sure to operate in a clean,
non-tampered environment. Browser-Enforced Embedded
Policies (BEEP) [22] is a server system that injects a policy
in a web page. The browser will call this policy script before
loading any another script, giving the policy the opportunity
(cid:23)(cid:24)(cid:21)
)
d
e
l
d
n
a
h
s
t
s
e
u
q
e
r
l
a
t
o
t
(
t
u
p
h
g
u
o
r
h
T
40000
30000
20000
10000
0
0
1000
750
500
250
0
)
d
e
l
d
n
a
h
s
t
s
e
u
q
e
r
t
n
e
r
r
u
c
n
o
c
(
y
t
i
c
a
p
a
C
250
500
Concurrent users
750
1000
0