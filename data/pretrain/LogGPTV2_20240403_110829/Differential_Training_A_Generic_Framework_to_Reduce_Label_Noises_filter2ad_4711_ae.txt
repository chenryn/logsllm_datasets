### Further Improvements to Differential Training

The performance of Differential Training can be further enhanced by adopting a more accurate noise ratio estimation algorithm for outlier detection. This is an area for future research.

### Generalization of Differential Training

Differential Training is designed to identify and correct wrongly-labeled data samples in the context of Android malware detection, which is treated as a binary classification problem (benign or malicious). While the concept of Differential Training can potentially be generalized to other fields for identifying mislabeled data, it cannot be directly applied to multiclass classification. This is because Differential Training simply flips the labels of identified noisy samples to correct them. In a multiclass setting, additional methods are required to determine the correct label for each misclassified sample.

### Related Works

#### 1. Android Malware Detection

Early research on Android malware detection employed traditional machine learning techniques such as k-nearest neighbors (kNN), support vector machines (SVMs), and decision trees, using manually selected features like system calls, permissions, embedded strings, APIs, and communication intents. More recent studies have focused on deep learning and automatic feature engineering. Examples include Maldozer, R2-D2, and DroidDetector, which use convolutional neural networks (CNNs) for malware detection based on API vectors. Other approaches utilize recurrent neural networks (RNNs) to process sequential app inputs, such as API call sequences. Hybrid methods that combine multiple types of malware detection models have also been proposed. A common assumption in these studies is that all training apps are correctly labeled, but in practice, obtaining noise-free large-scale training data is challenging due to the complexity of malware detection and the rapid evolution of app development. Therefore, it is crucial to study the impact of label noises on malware detection and improve performance with noisy training data. Differential Training complements existing research by providing a generic framework to reduce label noises and can be integrated with any Android malware detection approach to enhance performance.

#### 2. The Label Noise Problem in Machine Learning

The issue of label noise has been extensively addressed in the machine learning literature, with a focus on developing models that are robust to noisy labels. Various strategies have been developed to mitigate the negative effects of mislabeled samples during training, thereby improving the quality of the final models.

- **Loss Adjustment**: Some methods adjust the loss calculation during training based on label noise estimation.
- **Special Model Structures**: Other approaches rely on models with specific structures that reduce the impact of label noise.
- **Noise-Tolerant Models**: There are also methods that aim to train noise-tolerant models for various applications.

These approaches typically perform noise detection based on the final states of input samples in either the training or testing phase. Schein et al. suggested that the intermediate states of input samples during training can be useful for measuring the uncertainty between predicted and actual labels. Chang et al. utilized this idea to accelerate model training. However, the intermediate states of input samples during training have not been widely used for processing noisy samples, except in Co-Teaching, where individual loss values of each input sample in each mini-batch are examined. Differential Training, on the other hand, detects label noises by examining all loss values for each input sample throughout the entire training process.

### Conclusion

In this paper, we introduced Differential Training as a generic framework to detect and reduce label noises in training data for Android malware detection. Differential Training is innovative because it:

- Uses the intermediate states of input samples throughout the training process for noise detection.
- Employs a downsampled set to maximize the differences between wrongly-labeled and correctly-labeled samples.
- Utilizes outlier detection algorithms without relying on a small set of correctly-labeled training samples.

Our experimental results show that Differential Training reduces 87.4%, 82.6%, and 64.7% of wrong labels in the training sets of SDAC, Drebin, and DeepReÔ¨Åner, respectively, when the noise ratio is set to 10%. With reduced noise, the F-scores of these malware detection approaches improve from 89.04%, 73.20%, and 91.37% to 97.19%, 84.40%, and 93.41%, respectively, approaching their upper bounds (97.71%, 93.34%, and 93.59%). Our experiments also demonstrate that Differential Training performs consistently across different noise ratios and outperforms state-of-the-art robust learning algorithms like Co-Teaching and Decoupling for training robust deep neural networks with noisy labels.

### Acknowledgment

Yingjiu Li was supported in part by the Ripple University Blockchain Research Initiative.

### References

[References listed as provided, with proper formatting and organization]

This revised version aims to provide a clearer, more coherent, and professional presentation of the text.