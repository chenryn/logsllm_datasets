testing for security and reliability was in its infancy until the late 1990s. It appeared
as if nobody cared about software quality, as crashes were acceptable and software
could be updated “easily.” One potential reason for this was that before the avail-
ability of public networks, or the Internet, there was no concept of an “attacker.”
The birth of software security as a research topic was created by widely deployed
buffer overflow attacks such as the Morris Internet Worm in 1988. In parallel to the
development in the software security field, syntax testing was introduced around
1990 by the quality assurance industry.15 Syntax testing basically consists of model-
based testing of protocol interfaces with a grammar. We will explain syntax testing
in more detail in Chapter 3.
A much more simpler form of testing gained more reputation, perhaps due to
the easiness of its implementation. The first (or at least best known) rudimentary
negative testing project and tool was called Fuzz from Barton Miller’s research
group at the University of Wisconsin, published in 1990.16 Very simply, it tried ran-
22
Introduction
14From Folklore.org (1983). www.folklore.org/StoryView.py?story=Monkey_Lives.txt
15Syntax testing is introduced in the Software Testing Techniques 2nd edition, by Boris Beizer,
International Thomson Computer Press. 1990.
16More information on “Fuzz Testing of Application Reliability” at University of Wisconsin is
available at http://pages.cs.wisc.edu/~bart/fuzz
dom inputs for command line options, looking for locally exploitable security holes.
The researchers repeated the tests every five years, with same depressing results.
Almost all local command-line utilities crashed when provided unexpected inputs,
with most of those flaws exploitable. They described their approach as follows:
There is a rich body of research on program testing and verification. Our approach
is not a substitute for a formal verification or testing procedures, but rather an inex-
pensive mechanism to identify bugs and increase overall system reliability. We are
using a coarse notion of correctness in our study. A program is detected as faulty
only if it crashes or hangs (loops indefinitely). Our goal is to complement, not
replace, existing test procedures. While our testing strategy sounds somewhat naive,
its ability to discover fatal program bugs is impressive. If we consider a program to
be a complex finite state machine, then our testing strategy can be thought of as a
random walk through the state space, searching for undefined states.17
Inspired by the research at the University of Wisconsin, and by syntax testing
explained by Boris Beizer, Oulu University Secure Programming Group (OUSPG)
launched the PROTOS project in 1999.18 The initial motivation for the work grew
out of frustration with the difficulties of traditional vulnerability coordination and
disclosure processes, which led the PROTOS team to think what could be done to
expedite the process of vulnerability discovery and transfer the majority of the process
back toward the software and equipment vendors from the security research com-
munity. They came up with the idea of producing fuzzing test suites for various
interfaces and releasing them first to the vendors, and ultimately to the general pub-
lic after the vendors had been able to fix the problems. During the following years,
1.3
Fuzzing
23
History of Fuzzing
1983: The Monkey
1988: The Internet Worm
1989–1991:
•
Boris Beizer explains Syntax Testing (similar to robustness testing).
•
“Fuzz: An Empirical Study of Reliability . . .” by Miller et al. (Univ. of Wisconsin)
1995–1996:
•
Fuzz revisited by Miller et al. (Univ. of Wisconsin).
•
Fault Injection of Solaris by OUSPG (Oulu University, Finland).
1999–2001: 
•
PROTOS tests for: SNMP, HTTP, SIP, H.323, LDAP, WAP, . . .
2002:
•
Codenomicon launch with GTP, SIP, and TLS robustness testers.
•
Click-to-Secure (now Cenzic) Hailstorm web application tester.
•
IWL and SimpleSoft SNMP fuzzers (and various other protocol specific tools).
17B. P. Miller, L. Fredriksen, and B. So. “An empirical study of the reliability of Unix utilities.”
Communications of the Association for Computing Machinery, 33(12)(1990):32–44.
18OUSPG has conducted research in the security space since 1996. www.ee.oulu.fi/research/ouspg
PROTOS produced free test suites for the following protocols: WAP-WSP, WMLC,
HTTP-reply, LDAP, SNMP, SIP, H.323, ISAKMP/IKE, and DNS. The biggest im-
pact occurred with the SNMP test suite, where over 200 vendors were involved in
the process of repairing their devices, some more that nine months before the pub-
lic disclosure. With this test suite the PROTOS researchers were able to identify
numerous critical flaws within the ASN.1 parsers of almost all available SNMP
implementations. This success really set the stage to alert the security community to
this “new” way of testing called fuzzing.
1.3.2
Fuzzing Overview
To begin, we would like to clearly define the type of testing we are discussing in this
book. This is somewhat difficult because no one group perfectly agrees on the def-
initions related to fuzzing. The key concept of this book is that of black-box or
grey-box testing: delivering input to the software through different communication
interfaces with no or very little knowledge of the internal operations of the system
under test. Fuzzing is a black-box testing technique in which the system under test
is stressed with unexpected inputs and data structures through external interfaces.
Fuzzing is also all about negative testing, as opposed to feature testing (also
called conformance testing) or performance testing (also called load testing). In neg-
ative testing, unexpected or semi-valid inputs or sequences of inputs are sent to
the tested interfaces, instead of the proper data expected by the processing code.
The purpose of fuzzing is to find security-related defects, or any critical flaws lead-
ing to denial of service, degradation of service, or other undesired behavior. In
short, fuzzing or fuzz testing is a negative software testing method that feeds mal-
formed and unexpected input data to a program, device, or system.
Programs and frameworks that are used to create fuzz tests or perform fuzz
testing are commonly called fuzzers. During the last 10 to 15 years, fuzzing has
gradually developed from a niche technique toward a full testing discipline with
support from both the security research and traditional QA testing communities.
Sometimes other terms are used to describe tests similar to fuzzing. Some of
these terms include
• Negative testing;
• Protocol mutation;
• Robustness testing;
• Syntax testing;
• Fault injection;
• Rainy-day testing;
• Dirty testing.
Traditionally, terms such as negative testing or robustness testing have been
used mainly by people involved with software development and QA testing, and
the word fuzzing was used in the software security field. There has always been
some overlap, and today both groups use both terms, although hackers tend to use
the testing related terminology a lot less frequently. Testing terms and requirements
in relation to fuzzing have always carried a notion of structure, determinism, and
24
Introduction
repeatability. The constant flood of zero-day exploits has proved that traditional
functional testing is insufficient. Fuzzing was first born out of the more affordable,
and curious, world of randomness. Wild test cases tended to find bugs overlooked
in the traditional development and testing processes. This is because such randomly
chosen test data, or inputs, do not make any assumptions for the operation of the
software, for better or worse. Fuzzing has one goal, and one goal only: to crash
the system; to stimulate a multitude of inputs aimed to find any reliability or robust-
ness flaws in the software. For the security people, the secondary goal is to analyze
those found flaws for exploitability.
1.3.3
Vulnerabilities Found with Fuzzing
Vulnerabilities are created in various phases of the SDLC: specification, manufac-
turing, and deployment (Figure 1.7). Issues created in the specification or design
phase are fundamental flaws that are very difficult to fix. Manufacturing defects
are created by bad practices and mistakes in implementing a product. Finally, de-
ployment flaws are caused by default settings and bad documentation on how the
product can be deployed securely.
Looking at these phases, and analyzing them from the experience gained with
known mistakes, we can see that implementation mistakes prevail. More than 70%
of modern security vulnerabilities are programming flaws, with only less than 10%
being configuration issues, and about 20% being design issues. Over 80% of com-
munication software implementations today are vulnerable to implementation-level
security flaws. For example, 25 out of 30 Bluetooth implementations crashed when
they were tested with Bluetooth fuzzing tools.19 Also, results from the PROTOS
research project indicate that over 80% of all tested products failed with fuzz tests
around WAP, VoIP, LDAP, and SNMP.20
Fuzzing tools used as part of the SDLC are proactive, which makes them the
best solution for finding zero-day flaws. Reactive tools fail to do that, because they
are based on knowledge of previously found vulnerabilities. Reactive tools only test
1.3
Fuzzing
25
Figure 1.7
Various phases in the SDLC in which vulnerabilities are introduced.
19Ari Takanen and Sami Petäjäsoja, “Assuring the Robustness and Security of New Wireless
Technologies.” Paper and presentation. ISSE 2007, Sept. 27, 2007. Warsaw, Poland.
20PROTOS project. www.ee.oulu.fi/protos
or protect widely used products from major vendors, but fuzzers can test any prod-
uct for similar problems. With fuzzing you can test the security of any process, serv-
ice, device, system, or network, no matter what exact interfaces it supports.
1.3.4
Fuzzer Types
Fuzzers can be categorized based on two different criteria:
1. Injection vector or attack vector.
2. Test case complexity.
Fuzzers can be divided based on the application area where they can be used,
basically according to the attack vectors that they support. Different fuzzers target
different injection vectors, although some fuzzers are more or less general-purpose
frameworks. Fuzzing is a black-box testing technique, but there are several doors
into each black box (Figure 1.8). Note also that some fuzzers are meant for client-
side testing, and others for server-side testing. A client-side test for HTTP or TLS
will target browser software; similarly, server-side tests may test a web server. Some
fuzzers support testing both servers and clients, or even middleboxes that simply
proxy, forward, or analyze passing protocol traffic.
Fuzzers can also be categorized based on test case complexity. The tests gener-
ated in fuzzing can target various layers in the target software, and different test
cases penetrate different layers in the application logic (Figure 1.9). Fuzzers that
change various values in protocol fields will test for flaws like overflows and inte-
ger problems. When the message structure is anomalized, the fuzzer will find flaws
in message parses (e.g., XML and ASN.1). Finally, when message sequences are
fuzzed, the actual state machine can be deadlocked or crashed. Software has sepa-
26
Introduction
Figure 1.8
Attack vectors at multiple system levels.
rate layers for decoding, syntax validation, and semantic validation (correctness of
field values, state of receiver) and for performing the required state updates and
output generation (Figure 1.10). A random test will only scratch the surface, whereas
a highly complex protocol model that not only tests the message structures but also
message sequences will be able to test deeper into the application.
One example method of categorization is based on the test case complexity in
a fuzzer:
• Static and random template-based fuzzer: These fuzzers typically only test
simple request-response protocols, or file formats. There is no dynamic func-
tionality involved. Protocol awareness is close to zero.
• Block-based fuzzers: These fuzzers will implement basic structure for a sim-
ple request-response protocol and can contain some rudimentary dynamic
functionality such as calculation of checksums and length values.
• Dynamic generation or evolution based fuzzers: These fuzzers do not neces-
sarily understand the protocol or file format that is being fuzzed, but they will
learn it based on a feedback loop from the target system. They might or might
not break the message sequences.
• Model-based or simulation-based fuzzers: These fuzzers implement the tested
interface either through a model or a simulation, or they can also be full
implementations of a protocol. Not only message structures are fuzzed, but
also unexpected messages in sequences can be generated.
The effectiveness of fuzzing is based on how well it covers the input space of
the tested interface (input space coverage) and how good the representative mali-
cious and malformed inputs are for testing each element or structure within the
1.3
Fuzzing
27
Figure 1.9
Different types of anomalies and different resulting failure modes.
tested interface definition (quality of test data). Fuzzers that supply totally random
characters may yield some fruit but, in general, won’t find many bugs. It is gener-
ally accepted that fuzzers that generate their inputs with random data are very
inefficient and can only find rather naive programming errors. As such, it is nec-
essary for fuzzers to become more complex if they hope to uncover such buried or
hard to find bugs. Very obscure bugs have been called “second-generation bugs.”
They might involve, for example, multipath vulnerabilities such as noninitialized
stack or heap bugs.
Another dimension for categorizing fuzzers stems from whether they are model-
based. Compared with a static, nonstateful fuzzer that may not be able to simulate
any protocol deeper than an initial packet, a fully model-based fuzzer is able to test
an interface more completely and thoroughly, usually proving much more effective
in discovering flaws in practice. A more simplistic fuzzer is unable to test any inter-
face very thoroughly, providing only limited test results and poor coverage. Static
fuzzers may not be able to modify their outputs during runtime, and therefore lack
the ability to perform even rudimentary protocol operations such as length or check-
sum calculations, cryptographic operations, copying structures from incoming mes-
sages into outgoing traffic, or adapting to the exact capabilities (protocol extensions,
used profiles) of a particular system under test. In contrast, model-based fuzzers
can emulate a protocol or file format interface almost completely, allowing them to
understand the inner workings of the tested interface and perform any runtime cal-
culations and other dynamic behaviors that are needed to achieve full interoper-
ability with the tested system. For this reason, tests executed by a fully model-based
fuzzer are usually able to penetrate much deeper within the system under test, exer-
cising the packet parsing and input handling routines extremely thoroughly, and
28
Introduction
Figure 1.10
Effectivity of a test case to penetrate the application logic.
reaching all the way into the state machine and even output generation routines,
hence uncovering more vulnerabilities.
1.3.5
Logical Structure of a Fuzzer
Modern fuzzers do not just focus solely on test generation. Fuzzers contain differ-
ent functionalities and features that will help in both test automation and in failure
identification. The typical structure of a fuzzer can contain the following function-
alities (Figure 1.11).
• Protocol modeler: For enabling the functionality related to various data for-
mats and message sequences. The simplest models are based on message tem-
plates, whereas more complex models may use context-free protocol grammars
or proprietary description languages to specify the tested interface and add
dynamic behavior to the model.
• Anomaly library: Most fuzzers include collections of inputs known to trigger
vulnerabilities in software, whereas others just use random data.
• Attack simulation engine: Uses a library of attacks or anomalies, or learns
from one. The anomalies collected into the tool, or random modifications,
are applied to the model to generate the actual fuzz tests.
• Runtime analysis engine: Monitors the SUT. Various techniques can be used
to interact with the SUT and to instrument and control the target and its
environment.
• Reporting: The test results need to be prepared in a format that will help the
reporting of the found issues to developers or even third parties. Some tools
do not do any reporting, whereas others include complex bug reporting
engines.
1.3
Fuzzing
29
Figure 1.11
Generic structure of a fuzzer.
• Documentation: A tool without user documentation is difficult to use. Espe-
cially in QA, there can also be requirement for test case documentation. Test
case documentation can sometimes be used in reporting and can be dynami-
cally created instead of a static document.
1.3.6
Fuzzing Process
A simplified process of conducting a fuzz test consists of sequences of messages
(requests, responses, or files) being sent to the SUT. The resulting changes and
incoming messages can be analyzed, or in some cases they can be completely
ignored (Figure 1.12). Typical results of a fuzz test contain the following responses:
• Valid response.
• Error response (may still be valid from a protocol standpoint).
• Anomalous response (unexpected but nonfatal reaction, such as slowdown
or responding with a corrupted message).
• Crash or other failure.
The process of fuzzing is not only about sending and receiving messages. Tests
are first generated and sent to the SUT. Monitoring of the target should be constant
30
Introduction
Figure 1.12
Example fuzz test cases and resulting responses from an SUT.
and all failures should be caught and recorded for future evaluation. A critical part
of the fuzzing process is to monitor the executing code as it processes the unex-
pected input (Figure 1.13). Finally, a pass-fail criteria needs to be defined with the
ultimate goal being to perceive errors as they occur and store all knowledge for later
inspection. If all this can be automated, a fuzzer can have an infinite amount of
tests, and only the actual failure events need to be stored, and analyzed manually.
If failures were detected, the reason for the failure is often analyzed manually.
That can require a thorough knowledge of the system and the capability to debug
the SUT using low-level debuggers. If the bug causing the failure appears to be
security-related, a vulnerability can be proved by means of an exploit. This is not
always necessary, if the tester understands the failure mode and can forecast the
probability and level of exploitability. No matter which post-fuzzing option is
taken, the deduction from failure to an individual defect, fixing the flaw, or poten-
tial exploit development task can often be equally expensive in terms of man-hours.
1.3.7
Fuzzing Frameworks and Test Suites
As discussed above, fuzzers can have varying levels of protocol knowledge. Going
beyond this idea, some fuzzers are implemented as fuzzing frameworks, which means
that they provide an end user with a platform for creating fuzz tests for arbitrary pro-
tocols. Fuzzer frameworks typically require a considerable investment of time and
resources to model tests for a new interface, and if the framework does not offer
ready-made inputs for common structures and elements, efficient testing also requires
considerable expertise in designing inputs that are able to trigger faults in the tested
interface. Some fuzzing frameworks integrate user-contributed test modules back to
the platform, bringing new tests within the reach of other users, but for the most part
fuzzing frameworks require new tests to always be implemented from scratch. These
factors can limit the accessibility, usability, and applicability of fuzzing frameworks.
1.3
Fuzzing
31
Figure 1.13
Fuzzing process consisting of test case generation and system monitoring.
1.3.8
Fuzzing and the Enterprise
Not all software developer organizations and device manufacturers use any type of
fuzzing—although we all agree that they should. For many companies, fuzzing is
something that is looked at after all other testing needs have been fulfilled. What
should we do to motivate them to embrace fuzzing as part of their software devel-
opment process? One driving force in changing the testing priorities can be created
by using fuzzing tools in the enterprise environment.
The first action that enterprises could take is to require fuzzing in their procure-