and its partial discrepancy PDisc(Q) as:
Disc(Q) = min
PDisc(Q) =
(cid:107)Qz(cid:107)∞, and
(cid:107)Qz(cid:107)∞.
z∈{±1}n
min
z∈{0,+1,−1}n,
(cid:107)z(cid:107)1>n/10
The qualiﬁer “partial” refers to the fact that we allow up to 90% of z’s coordinates to be zero,
in contrast to ordinary discrepancy which only considers vectors z ∈ {±1}n. A more combinatorial
perspective comes if we think of the rows of Q as characteristic vectors of subsets of X, and z as a
partial ±1-coloring of the elements of X. Then (cid:107)Qz(cid:107)∞ measures the largest imbalance in coloring
over all the sets in Q, and PDisc(Q) refers to minimizing this maximum-imbalance over all partial
colorings z.
Summarizing the discussion before Deﬁnition 5.4, we have:
Theorem 5.5 (reconstruction via partial discrepancy). Let q1, . . . , qk ∈ {0, 1}n and Q be the k × n
matrix whose rows are the qj’s. Then any mechanism M : {0, 1}n → Rk that answers all of the
normalized inner-product queries speciﬁed by q1, . . . , qk to within additive error α smaller than
PDisc(Q)/2n is blatantly non-private.
We note that Theorem 5.5 is a generalization of Theorem 5.2. Indeed, if Q is the 2n × n matrix
whose rows are all bitstrings of length n (i.e. the family of all subsets of [n]), then the partial
discrepancy of Q is greater than n/20. (For a partial coloring z with greater than n/10 nonzero
entries, either the set of coordinates on which z is 1 or the set of coordinates on which z is -1 will
have imbalance greater than n/20.)
Let us now use Theorem 5.5 to deduce the second theorem of Dinur and Nissim [30].
35
Theorem 5.6 (reconstruction from few queries with small error [30]). There exists c > 0 and
q1, . . . , qn ∈ {0, 1}n such that any mechanism that answers the normalized inner-product queries
√
speciﬁed by q1, . . . , qn to within error at most c/
n is blatantly non-private.
In fact, the theorem holds for a random set of queries, as follows from combining the following
lemma (setting k = s = n) with Theorem 5.5:
Lemma 5.7 (discrepancy of a random matrix). For all integers k ≥ s ≥ 0, with high probability,
a k × s matrix Q with uniform and independent entries from {0, 1} has partial discrepancy at least
(cid:16)
(cid:110)(cid:112)s · (1 + log(k/s)), s
(cid:111)(cid:17)
.
Ω
min
Up to the hidden constant, this is the largest possible discrepancy for a k × s matrix. Indeed,
√
s · log k) (by a Chernoﬀ bound and union
a random coloring achieves discrepancy at most O(
bound). The celebrated “six standard deviations suﬃce” result of Spencer [96] improves the log k
to log(k/s).
Proof sketch. Pick the rows q1, . . . , qk ∈ {0, 1}s uniformly at random. Fix z ∈ {0, +1,−1}s with
(cid:107)z(cid:107)1 > s/10. Then for each j, (cid:104)qj, z(cid:105) is a diﬀerence of two Binomial distributions, at least one
of which is the sum of more than s/20 independent, unbiased {0, 1} random variables (since z
has more than s/20 coordinates that are all 1 or all -1). By anticoncentration of the binomial
distribution (cf. [75, Prop. 7.3.2]), we have for every t ≥ 0:
Thus, for each z we have
By a union bound, we have:
√
Pr
qj
(cid:16)
1 − O(t), Ω
(cid:110)
s, s/20}(cid:3) ≥ max
s, s/20}(cid:3) ≤ min
e−O(t2)(cid:17)(cid:111)
(cid:2)|(cid:104)qj, z(cid:105)| ≥ min{t
e−O(t2)(cid:17)(cid:111)k
(cid:16)
Pr(cid:2)∀j ∈ [k],|(cid:104)qj, z(cid:105)|  s/10 and ∀j ∈ [k],|(cid:104)qj, z(cid:105)| n/10
n) for a random n × n matrix Q, as well as for some explicit
√
One can consider a relaxed notion of accuracy, where the mechanism is only required to give
answers with at most c/
n additive error for 51% of the queries, and for the remaining 49% it is
free to make arbitrary error. Even such a mechanism can be shown to be blatantly non-private. If
one wants this theorem with a polynomial-time privacy-breaking algorithm, then this can also be
done with the 51% replaced by about 77%. (This is a theorem of Dwork, McSherry, and Talwar
[39], and is based on connections to compressed sensing.)
5.1.2 Discrepancy Characterizations of Error for Counting Queries
We now work towards characterizing the error required for diﬀerential privacy for answering a
given set of counting queries. Let q1, . . . , qk ∈ {0, 1}X be a given set of counting queries over a data
universe X (viewed as vectors of length |X|). We will abuse notation and use Q to denote both the
set {q1, . . . , qk} of counting queries as well as the the k × |X| matrix whose rows are the qj. For a
set S ⊆ X, we let QS denote the restriction of Q to the columns of S.
Then we have:
Theorem 5.8 (partial discrepancy lower bound). Let Q = {q : X → {0, 1}} be a set of counting
queries over data universe X, and let M : Xn → RQ be a (1, .1)-diﬀerentially private mechanism
that with high probability answers every query in Q with error at most α. Then
α ≥ max
|S| even
S⊆X,|S|≤2n
PDisc(QS)/2n.
Proof sketch. Suppose for contradiction that α  0, and n ∈ N, there is an (ε, δ)-diﬀerentially
private mechanism M : Xn → RQ that answers every query in Q with error
α ≤ HerPDisc(Q) · polylog(|Q|) ·(cid:112)log(1/δ)
εn
with high probability.
We won’t prove the latter theorem, but will get a taste of its techniques in Section 7.3. We note
that the distinction between partial discrepancy and ordinary discrepancy becomes less signiﬁcant
once we move to the hereditary versions. Indeed, if we deﬁne HerDisc(Q) def= maxS⊆X Disc(QS), then
it is known that:
HerPDisc(Q) ≤ HerDisc(Q) ≤ HerPDisc(Q) · O(min{log |X|, log |Q|}).
(5)
(See the book by Matouˇsek [74] for proofs.) Hereditary discrepancy is a well-studied concept in
combinatorics and a remarkable byproduct of the aforementioned work on diﬀerential privacy was a
polylogarithmic-approximation algorithm for hereditary discrepancy, solving a long-standing open
problem [84].
5.1.3 Discrepancy Lower Bounds for Speciﬁc Query Families
Note that Theorems 5.9 and 5.10 only provide a nearly tight characterization in case we look
for error bounds of the form f (Q)/n, which scale linearly with n (ignoring the dependence on ε
and log(1/δ) for this discussion). In particular, the lower bound of Theorem 5.9 only says that
HerPDisc(Q) is a lower bound on the function f (Q) for suﬃciently large n. If our dataset size n is
below the point at which this lower bound kicks in, we may be able to achieve signiﬁcantly smaller
error.
38
For ﬁnite dataset sizes n, we can use the lower bound of Theorem 5.8:
α ≥ max
|S| even
S⊆X,|S|≤2n
PDisc(QS)/2n.
Unfortunately, partial discrepancy is a combinatorially complex quantity, and can be hard to es-
timate. Fortunately, there are several relaxations of it that can be easier to estimate and thereby
prove lower bounds:
Proposition 5.11. Let Q be a k × |X| query matrix (with {0, 1} entries). Then:
1. For every S ⊆ X and T ⊆ [k], we have
PDisc(QS)) >
(cid:115)|S|
|T| · σmin(QT
S ),
1
10
S denotes the |T|×|S| submatrix of QS with rows indexed by T , and σmin(QT
S ) denotes
where QT
the smallest singular value of QT
S .
2.
PDisc(QS) >
S⊆X,|S|≤2n
max
|S| even
min{VC(Q) − 1, 2n}
20
.
Proof.
1. We have:
PDisc(QS)) ≥ PDisc(QT
S )
min
=
> inf
z(cid:54)=0
≥ inf
z(cid:54)=0