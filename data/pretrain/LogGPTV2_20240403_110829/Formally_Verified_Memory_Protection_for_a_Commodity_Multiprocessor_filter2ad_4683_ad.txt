4.4 Cache Management
As shown in Figure 3a, AbsMachine includes PIPT writeback
caches. Arm adopts MESI/MOESI cache coherence protocols,
guaranteeing that all levels’ of cache are consistent, meaning
the hardware can retrieve the same contents from the cache
located at different levels, and the updates to the cache are syn-
chronized to the cache at different levels. Arm’s multi-level
caches can be modeled by AbsMachine as a uniform global
cache. To model hardware that will invalidate and write back
cached entries unbeknownst to software, for example, due to
cache line replacement, AbsMachine exposes a cache-sync
primitive that randomly evicts a cache entry and writes it
back to memory. In KCore’s speciﬁcation, memory load and
store operations call cache-sync before the actual memory
accesses to account for all possible cache eviction policies.
While caches are coherent, Arm hardware does not guarantee
that cached data is always coherent with main memory;
caches may write back dirty lines at any time. Like other
architectures, Arm provides cache maintenance instructions
to allow software to ﬂush cache lines to ensure what is stored
in main memory is up-to-date with what is stored in cache.
AbsMachine provides a cache-flush primitive that models
Arm’s clean and invalidate instruction. The primitive takes
a pfn as an argument, copies the val of pfn from cache
to main memory if the entry is present in the cache, then
removes pfn’s entry from the cache. Cache mismanagement
could result in security vulnerabilities, so hypervisors must
use these instructions to ensure that data accesses across all
of its cores remain coherent, preventing stale data leaks.
Figure 4: Attack Based on Mismatched Memory Attributes
Figure 4 shows how a malicious VM could leverage cache
mismanagement on Arm hardware to potentially obtain
conﬁdential data of another VM from main memory. Suppose
the hypervisor decides to evict a VM1’s page pfn. It unmaps
the page from VM1 and scrubs the page by zeroing out any
residual data. Since the page no longer can be used by VM1,
the hypervisor is free to reassign it to another VM, VM2,
by mapping pfn to VM2’s stage 2 page tables (S2PT). Arm
hardware guarantees the scrubbing is synchronized across
all CPU caches, but does not guarantee it is written back to
main memory. Arm allows software to mark whether a page
is cacheable or not by setting the memory attributes in the re-
spective page table entry. When stage 2 translation is enabled,
Arm combines memory attribute settings in stage 1 and stage
2 page tables. For a given mapping, caching is only enabled
when both stages of page tables enable caching. Hypervisors
allow VMs to manage their own stage 1 page tables for perfor-
mance reasons. Although KCore always enables caching in
stage 2 page tables, an attacker in VM2 could disable caching
for the mapping to pfn in its stage 1 page table, allow it to
bypass the caches and directly access pfn in main memory,
which could contain VM1’s conﬁdential data. To protect VM
memory against this attack, the hypervisor should ﬂush pfn’s
associated cache line after scrubbing the page to ensure that
the changes are written back to main memory. This ensures
VM2 can never retrieve VM1’s secret in main memory.
To ensure that KCore correctly manages caches, we verify
it over AbsMachine, which models writeback caches and
cache bypass. AbsMachine models both cache and main mem-
ory as partial maps pfn(cid:55)→val, where val is the content stored
in a given pfn. As a pfn moves between cache and main mem-
ory, AbsMachine propagates its content with it. For example,
on a cacheable memory access, AbsMachine checks if the
cache contains a mapping for pfn. If it does not, AbsMachine
populates the cache with val from main memory. It then re-
turns val for memory loads, and updates the cached value for
USENIX Association
30th USENIX Security Symposium    3961
S1PTVM1HypervisorS2PTSMain memorypfn……0Data cachepfnS1PTS2PTVM2Non-cacheableCacheablescrub pfnmemory stores. Similarly, on a cache-flush or cache-sync,
AbsMachine ﬂushes the pfn to main memory, populating
main memory with the respective val from the cache.
Using AbsMachine, we prove that KCore always sets
the memory attributes in the page tables that it manages to
enable caching, maximizing performance. We then prove that
KCore ﬂushes caches in the primitives that can change page
ownership, verifying that KCore’s implementation reﬁnes its
speciﬁcation. Finally, we use KCore’s speciﬁcation to prove
that KCore’s cache management has no security vulnerabil-
ities and does not compromise VM data. We discuss the ﬁrst
two proofs here, but defer the latter proof to Section 4.6.
We ﬁrst prove that KCore always sets the memory
attributes in the stage 2 page tables for VMs and KServ to
enable caching. KCore updates stage 2 page table entries
by calling the veriﬁed map_page primitive, as discussed in
Section 4.2. map_page is passed the attr parameter to set
the page table entry attributes. We verify the primitives that
call map_page pass in the correct attr to enable caching.
Speciﬁcally, we verify the implementation of map_pfn_vm
and map_pfn_host in the MemAux layer, which call map_page
to map a pfn to a VM’s and KServ’s stage 2 page tables,
respectively reﬁne their speciﬁcations that pass an attr value
with caching enabled to map_page. We also prove that KCore
always sets the memory attributes in its own EL2 stage 1
page tables to enable caching. Similar to map_page, NPTOps
provides a map_page_core primitive for updating EL2 stage
1 page tables, which in turn calls set_s1pt in NPTWalk to
update the multi-level page tables — we prove the correctness
of these primitives similarly to the proofs for map_page
and set_s2pt. We then verify the primitives that call
map_page_core pass in the correct attr to enable caching.
We then prove that KCore correctly ﬂushes the cache in
the primitives that change page ownership. In the MemAux
layer, we prove the correctness of assign_pfn_vm and
clear_vm_page. assign_pfn_vm unmaps pfn from KServ
and assigns the owner of a newly allocated pfn to a VM.
clear_vm_page reclaims a pfn from a VM upon the VM’s
termination, scrubs the pfn, and assigns the owner of the
pfn to KServ. We prove that the implementations of both
primitives reﬁne their speciﬁcations that call cache-flush.
4.5 SMMU Management
As shown in Figure 3a, AbsMachine models Arm’s SMMU,
which supports a shared SMMU TLB and SMMU multi-level
page tables, that can be allocated for each device devk. The
TLB is tagged, and page tables can support up to four levels of
paging with regular and huge page support, similar to the page
tables and TLBs discussed in Sections 4.2 and 4.3. Unlike
memory accesses from CPUs, there are no caches involved in
memory accesses through the SMMU. For simplicity, we only
describe the SMMU stage 2 page tables, used by the SMMU
implementation [5] on the Arm Seattle server hardware we
used for evaluation in Section 6. AbsMachine also provides
dev_load and dev_store operations to model memory ac-
cesses of DMA-capable devices attached to the SMMU.
KCore controls the SMMU and maintains the SMMU TLB
and SMMU page tables for each devk. TLB entries are tagged
by VMID. The parts of KCore that manipulate page tables are
the four layers of SMMU PT shown in Figure 2. Similar to how
we reﬁne multi-level page tables in NPTWalk as discussed
in Section 4.2, we reﬁne the SMMU multi-level page table
and its multi-level page table walk in MmioSPTWalk in SMMU
PT into a layer speciﬁcation with a partial map that maps an
input page frame from device address space, devfn (cid:55)→ (pfn,
size, attr), where size is the size of the page, 4KB or 2MB,
and attr encompasses attributes of the page. Once we prove
this reﬁnement, higher layers that depend on SMMU page
tables can be veriﬁed against the abstract page table, enabling
us to prove the correctness of KCore’s SMMU page table
management.
Similar to how we reﬁne CPU TLBs as discussed in
Section 4.3, we reﬁne the SMMU TLB in MmioSPTOps so
that it is abstracted away from higher layers. We model the
SMMU TLB as a set of partial maps, each map identiﬁed
by VMID and mapping devfn (cid:55)→ (pfn, size, attr). Abs-
Machine models SMMU TLB invalidation by exposing a
smmu-tlb-flush primitive to ﬂush all entries associated
with a VMID [5]. We prove the correctness of KCore with
the SMMU TLB by verifying it correctly ﬂushes entries
to ensure consistency with the SMMU page tables, then
abstract away the TLB by proving that the MmioSPTOps
implementation using the SMMU TLB reﬁnes a simpler,
higher-level speciﬁcation without the SMMU TLB. We
prove unmap_spt in MmioSPTOps calls smmu-tlb-flush after
unmapping a pfn from the SMMU page table.
4.6 Security Guarantees
By proving that KCore’s implementation reﬁnes its top-level
Coq speciﬁcation, we can then use the high-level speciﬁcation
to prove higher-level security guarantees. Proving security
guarantees is much easier using the speciﬁcation because we
can avoid being inundated with the details of KCore’s entire
implementation, and we can use the simpliﬁed machine
model reﬁned from the lower layers. For instance, to prove the
security properties for VM’s memory accesses, we can reason
over the memory load and store primitives at KCore’s top
layer based on the abstract single-level page tables without
TLB, instead of the primitives deﬁned in AbsMachine using
multi-level page tables with TLB. We ensure the speciﬁcation
soundly captures all behaviors of the KCore implementation
so the proven guarantees hold on the implementation.
We prove that SeKVM protects their VMs’ data conﬁ-
dentiality—adversaries should not be privy to private VM
data—and integrity—adversaries should not be able to tamper
with private VM data. For some particular VM, potential
3962    30th USENIX Security Symposium
USENIX Association
adversaries are other VMs hosted on the same physical
machine, as well as the hypervisor itself—speciﬁcally,
HypSec’s untrusted KServ. Our goal here is to verify that,
irrespective of how any principal, KServ or another VM,
behaves, KCore protects the security of each VMs’ data. We
formulate conﬁdentiality and integrity as noninterference
assertions [30]—invariants on how principals’ behavior may
inﬂuence one another. For conﬁdentiality, we show the behav-
iors of all other VMs and KServ remains unaffected despite
any changes the VM made to its data. For integrity, we prove
that a VM’s behavior acting upon its data is unaffected by
other VMs’ or KServ’s behaviors, therefore its data is intact.
We can prove noninterference by showing state indistin-
guishability, which means that two machine states observable
to a principal are the same. Machine states include a princi-
pal’s data in CPU registers and memory. Data in memory in-
cludes data in main memory and caches as well as metadata in
the principal’s page tables. We want to prove that starting from
any two indistinguishable states to principal p, the abstract
machine should only transition to a pair of states that are still
indistinguishable to p. We leverage previous work to prove
this for data in CPU registers [48] and focus our discussion on
proving this for memory. In particular, we need to prove that
primitives that are part of the top-level speciﬁcation that affect
the management of page tables, caches, and SMMU preserve
state indistinguishability since they can all affect memory.
Since we have proven that TLBs do not need to be considered
as part of the abstract machine model used by the top-level
speciﬁcation, TLBs do not need to be considered as part of our
noninterference proofs. Note that pages explicitly shared via
GRANT_MEM are not considered private and not included in the
VM data protected by SeKVM until sharing is revoked using
the REVOKE_MEM hypercall. While our proofs do account for
this dynamically changing sharing of pages [48], we omit fur-
ther discussion of GRANT_MEM and REVOKE_MEM for simplicity
and focus on protecting memory private to each principal.
We prove that the use of page tables by top-level primitives
preserves state indistinguishability by ﬁrst proving a page
table isolation invariant that any page mapped by a principal’s
stage 2 page table must be owned by itself. As discussed in
Section 3, KCore assigns an owner for each page. Since each
page has at most one owner, page tables, and address spaces,
are isolated. With this invariant, we can prove that a principal
p’s states are not changed by any other principal q’s operations
on q’s own address space. In a similar vein, we also prove that
primitives that cause a page to be transferred from principal p
to another principal q also do not affect state indistinguishabil-
ity; one principal must be KServ on all transfers. If the transfer
is from KServ to a VM, KCore ensures that such a page is ﬁrst
unmapped from KServ’s stage 2 page table before the page’s
ownership is changed to the VM, and is only mapped to the
VM’s stage 2 page table after the ownership is changed to the
VM. If the transfer is from a terminated VM to KServ, KCore
clears the contents of the page before it is transferred so VM
data is not leaked to KServ. Since KServ never has private
VM data, it cannot leak such data when transferring a page
to another VM. As a result, the use of page tables preserves
state indistinguishability with respect to VM memory.
We prove that the use of caches by top-level memory load
and store primitives preserves state indistinguishability so
that the potential attack shown in Figure 4 cannot happen.
We ﬁrst prove noninterference when the ownership of a page
does not change. If a principal p always owns a page pfn,
only p can access that page. If only p can access pfn, pfn
will only be cached as a result of being accessed by p. Based
on the page table isolation invariant, the pages owned by p
that can be in the cache must be a subset of the pages mapped
in p’s stage 2 page table. Since page tables and address space
are isolated, so are each principal’s entries in the cache. We
can thereby prove that a principal p’s states are not changed
by any other principal’s q load and store operations on q’s
own address space even if those operations involve the cache.
We then prove noninterference when KCore changes
the principal associated with a pfn, which occurs when
KServ allocates a new page to handle a VM’s page fault,
and reclaims the pages from a terminated VM. The former
occurs when KServ calls the run_vcpu hypercall to execute
a VM’s VCPU after allocating a new pfn to the faulting VM,
in which KCore unmaps the pfn from KServ’s stage 2 page
table, calls assign_pfn_vm to assign the owner of the pfn to
the faulting VM, and maps the pfn to the VM’s stage 2 page
table before switching to the VM. The latter occurs when
KServ calls the clear_vm hypercall to reclaim all pfns from
a terminated VM, in which KCore calls clear_vm_page to
scrub and assign the owner of these pfns to KServ.
When allocating a new page to handle a VM’s page fault,
KCore calls cache-flush on the pfn in assign_pfn_vm be-
fore mapping the pfn to p stage 2 page table. If pfn is cached,
this causes pfn to be invalidated in the cache and its content is
synchronized to main memory; otherwise it has no effect. We
prove noninterference for KServ. Starting from two indistin-
guishable states for KServ, run_vcpu in two executions will
unmap the same pfn from KServ’s stage 2 page tables; thus,
the resulting states remain indistinguishable to KServ since it
cannot access pfn after the unmap. We prove noninterference
for VMs other than p. Consider a VM q different from VM