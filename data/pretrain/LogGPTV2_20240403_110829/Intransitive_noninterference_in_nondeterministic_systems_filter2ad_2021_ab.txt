their views only after r has completed.
3. BACKGROUND
In this section we review some of the literature on information-
ﬂow security and formulate versions of existing deﬁnitions within
our system model. This review will motivate several of the ingre-
dients we use in the new deﬁnitions we propose later. This section
is largely a review of the existing literature, however, we do give
new characterizations of some existing deﬁnitions, using a notion
of relative information, that helps to clarify the relationship to the
new deﬁnitions we introduce later on.
3.1 Noninterference, policy H (cid:54)
(cid:55)→ L
Goguen and Meseguer’s formal semantics for noninterference
policies is restricted to transitive policies in deterministic machines.
They deﬁne for each domain u ∈ U the purge function purgeu :
A∗ → A∗ that maps a sequence of actions to the subsequence of
actions a with dom(a) (cid:55)→ u.
Deﬁnition 2. A machine M satisﬁes noninterference (NI) w.r.t.
(cid:55)→ if for all domains u and all runs r, r(cid:48) of M with purgeu(Act(r)) =
purgeu(Act(r(cid:48))), we have obsu(last(r)) = obsu(last(r(cid:48))).
It can be shown that, in deterministic machines, this is equivalent
to the following: for all runs r, r(cid:48) of M with purgeL(Act(r)) =
purgeL(Act(r(cid:48))), we have viewL(r) = viewL(r(cid:48)). This presen-
tation makes it clear that the deﬁnition says that the information in
L’s view in a run depends only on the L actions in the run, and are
independent of the H actions.
One of the main questions of study since the seminal work of
Goguen and Meseguer on deterministic systems is how their deﬁ-
nitions should be generalized to nondeterministic systems. A great
deal of the literature on this topic has been concerned with the sim-
ple two domain policy (which we write as H (cid:54)
(cid:55)→ L) given by the
relation {(L, L), (H, H), (L, H)} on the set U = {L, H}, comprised
of the low-level (public) domain L and the high-level (classiﬁed)
domain H. This is in part because even this simple policy presents
many subtleties in the setting of nondeterministic systems, but also
because there has been a view that it is possible to reduce arbitrary
policies to this special case.
Sutherland [22] proposed to interpret the policy H (cid:54)
Numerous deﬁnitions have been proposed that generalize NI for
the policy H (cid:54)
(cid:55)→ L to nondeterministic systems. We now discuss a
number of points from this literature that highlight issues that are
relevant to the novel deﬁnitions for the more general intransitive
policies that we introduce later.
3.2 Nondeducibility and Relative Information
(cid:55)→ L, as stat-
ing, informally, that L cannot deduce information about H, and gave
a general formal account of deducibility using a relation on func-
tions with domain the state space of the system. A generalization
of (non)deducibility will be useful for what follows, to make ex-
plicit a common logical structure that underlies the deﬁnitions we
propose. Deﬁnitions similar to the following have been considered
by Halpern and O’Neill [11] and More et al [17].
Deﬁnition 3. Let f, g and h be functions, each with domain the
set W of ‘worlds’. We say that in W , the function f contains
no more information than g about h, if for all w, w(cid:48) ∈ W such
that g(w) = g(w(cid:48)) there exists w(cid:48)(cid:48) such that h(w(cid:48)(cid:48)) = h(w(cid:48)) and
f (w(cid:48)(cid:48)) = f (w).
As an application of Deﬁnition 3, consider the following deﬁnition
of security:
Deﬁnition 4. A machine M for the policy H (cid:54)
(cid:55)→ L satisﬁes cor-
rectability (COR) if for all runs r and sequences α ∈ A∗ with
Act L(r) = Act L(α), there exists a run r(cid:48) with Act(r(cid:48)) = α and
viewL(r(cid:48)) = viewL(r).
We call this notion correctability in view of its similarity to a no-
tion of that name from [12]. It is easily seen that correctability may
be given a clean characterization using the above notion of rela-
tive information as follows: M satisﬁes correctability iff in R(M ),
the function viewL contains no more information about Act than
purgeL ◦ Act.
In a special case, we can furthermore formulate relative infor-
mation in a more symmetric way. Using a notation reminiscent
of probability theory, for functions f and g with domain W , and
value v in the range of f, deﬁne poss(g | f = v) to be the set
{ g(w) | w ∈ W ∧ f (w) = v }. Then we have the following re-
sult.
Proposition 1. Let f be a function with domain W , let g : W →
V be surjective and let h be a function with domain V (so that
h ◦ g also has domain W ). Then in W , the function f contains
no more information than h ◦ g about g iff for all v, v(cid:48) ∈ V with
h(v) = h(v(cid:48)) we have poss(f | g = v) = poss(f | g = v(cid:48)).
Since we work with input-enabled machines, the function Act :
R(M ) → A∗ is surjective. By Proposition 1 and the relative infor-
mation characterization of COR above, an equivalent formulation
of COR is that for all α, α(cid:48) ∈ A∗ with purgeL(α) = purgeL(α(cid:48))
we have poss(viewL | Act = α) = poss(viewL | Act = α(cid:48)). This
states COR in a form that clariﬁes its relationship to the view-based
formulation of NI, by showing that that COR is obtained by gener-
alizing the single-valued view function of a deterministic machine
used in NI by a set-valued view function in the nondeterministic
setting.
3.3 Observation-based deﬁnitions are too weak
(cid:55)→ L, the deﬁnition of noninterference states
that the observation of L in the ﬁnal state of a run r should depend
in non-deterministic systems
For the policy H (cid:54)
871only on purgeL(Act(r)). As noted above, in deterministic sys-
tems, this is equivalent to the statement that the history viewL(r)
of everything that is observable to L in the run should depend only
on purgeL(Act(r)). However, as the following example shows, a
similar equivalence between deﬁnitions stated in terms of observa-
tions in the ﬁnal state of a run and deﬁnitions stated in terms of the
view on the run does not hold in nondeterministic systems.
Example 1. Consider the security policy H (cid:54)
(cid:55)→ L and the non-
deterministic machine M depicted in Fig. 2, where states are la-
beled externally with L’s observation, A = {(cid:96), h}, dom((cid:96)) =
L, and dom(h) = H. Suppose we deﬁne an observation-based
version of correctability: M is obs-COR if in R(M ), the func-
tion obsL ◦ last contains no more information than purgeL ◦ Act
about Act. Equivalently, by Proposition 1, for all α, α(cid:48) ∈ A∗, if
purgeL(α) = purgeL(α(cid:48)) then poss(obsL ◦ last | Act = α) =
poss(obsL ◦ last | Act = α(cid:48)). It can be seen that M satisﬁes obs-
COR: the only H transition that can affect L observations is that
from s0, but if this is added or deleted from a run, there exists an-
other run with the same subsequent sequence of actions ending in
the same ﬁnal observation for L. However, the possible views may
differ, depending on whether h occurred: note that purgeL(h(cid:96)(cid:96)) =
(cid:96)(cid:96) = purgeL((cid:96)(cid:96)), but poss(viewL | Act = (cid:96)(cid:96)) = {0(cid:96)1(cid:96)2, 0(cid:96)2(cid:96)1}
whereas poss(viewL | Act = h(cid:96)(cid:96)) = {0(cid:96)1(cid:96)1, 0(cid:96)2(cid:96)2}. Thus, this
machine does not satisfy COR. Intuitively, it is insecure, since L
can determine from its view whether the initial h action occurred.
s0
0
(cid:96)
(cid:96)
(cid:96)
s01
1
(cid:96)
s02
2
h
(cid:96)
(cid:96)
s11
1
s1
0
(cid:96)
(cid:96)
s12
2
Figure 2: Deductions ought to be based on views rather than
observations
This example points to the fact that in deﬁning security in non-
deterministic systems, we need to take the evidence from which an
agent makes deductions to be its view, i.e., all that it could have
observed to the present moment, rather than just its current obser-
vation, as in the deﬁnition of noninterference and some of its later
generalizations (e.g., the deﬁnition of intransitive noninterference
in deterministic systems [10, 21]). This point has sometimes been
missed in the literature on nondeterministic systems, e.g., see the
discussion of the work of von Oheimb [26] in Section 8 below.
3.4 Persistence
Two different intuitive interpretations of the notion of noninter-
ference can be given: an epistemic interpretation which says that
L is not able to know, or deduce anything about H activity, and a
causal interpretation, which says that H actions may not have any
causal effect on L observations.
In deterministic systems, these
interpretations may coincide, but this is no longer the case in non-
deterministic systems, as the following example shows.
Example 2. Consider the machine depicted in Fig. 3 for the pol-
icy H (cid:54)
(cid:55)→ L. States are labeled externally with L’s observation,
except for states on which L observes ⊥, in which case we elide the
observation. As in the previous example, A = {(cid:96), h}, dom((cid:96)) = L,
and dom(h) = H. It can be seen that this machine satisﬁes the epis-
temic notion COR: domain L cannot make any deductions from its
s0
(cid:96)
(cid:96)
s2
s1
(cid:96)
(cid:96)
(cid:96)
h
s6
s5
s4
s3
1
0
1
s7
0
(cid:96)
Figure 3: A machine that is COR but not P-COR.
view about H actions, or how these are interleaved with its own.
(Recall that we elide self-loops.) However, it can reasonably be
argued that this machine is not secure on a causal interpretation of
security: note that by performing or not the action h from the state
s1, domain H is able to inﬂuence whether L subsequently observes
0 or 1.
Examples such as this can be addressed using the notion of per-
sistence, which has been factored into a number of deﬁnitions in
the literature [5, 6, 18].
Deﬁnition 5. For a security deﬁnition X, we say that machine
M = (S, s0, A,−→, obs, dom) persistently satisﬁes X (P-X) w.r.t.
a policy (cid:55)→ if the machine (S, s, A,−→, obs, dom) satisﬁes X w.r.t.
(cid:55)→, for all reachable states s of M.
Note that the machine in Example 2 is not P-COR.1
3.5 Collusion
For policies that generalize from the two-domain setting of the
policy H (cid:54)
(cid:55)→ L in nondeterministic systems, the issue of collusion
becomes of concern. As we illustrate in the present section, this is
so even for transitive security policies.
The simplest type of policy for which this point can be made is
the separability policy [20, 15] which says that no domain may in-
terfere with any other. That is, for set of domains U, separability is
the policy ∆U = { (u, u) | u ∈ U }. In the case that U consists of
two domains A and B, this seems to say that A may not interfere B,
and vice versa, so one apparently reasonable interpretation of the
policy is to apply a semantics for H (cid:54)
(cid:55)→ L for all domains. This idea
suggests the following deﬁnition, when we apply the correctability
semantics for H (cid:54)
(cid:55)→ L to each domain, and use our relative informa-
tion formulation.
Deﬁnition 6. A machine M for a set U of domains satisﬁes mu-
tual correctability (MCOR) w.r.t. (cid:55)→ if for all domains u ∈ U, in
R(M ), viewu contains no more information than purgeu ◦ Act
about Act.
In the case of the policy (cid:55)→ = ∆U, this says that no domain
is able to deduce from its view anything about what actions other
1We remark that in the case of COR, the notion P-COR is in
the spirit of the notion 0-forward correctability of Johnson and
Thayer [12], which says that an addition or deletion from a trace
of an H action requires only changes to subsequent H observations
to obtain another trace that looks the same to L. However, P-COR
is stronger, since it requires a correction from a given state, while
0-forward-correctability is a trace-based notion that allows the cor-
rection to pass through different states on the common preﬁx of
events.
872domains have performed, or how those actions were interleaved
with its own.
It turns out that, in some circumstances, this is an insufﬁcient
guarantee. Suppose that we have a system with three separated
domains, i.e., U = {H, L1, L2} and the policy is ∆U. However, L1
and L2 are corrupt, and collude by communicating via a channel
that lies outside the system. Under these circumstances, there is
nothing that can be done in practice to prevent information-ﬂow
between L1 and L2, even if the system is secure, so that it is not the
cause of the information-ﬂow. However, we expect that, since the
system enforces the policy, H’s information is still protected from
leakage to L1 and L2. In fact, mutual correctability is too weak to
provide such a guarantee, as is shown by the following example.
Example 3. Consider the machine depicted in Fig. 4 under the
policy ∆{H,L1,L2}. The domain H observes ⊥ at all states. The
two low domains L1 and L2 have observations in the set {⊥, 0, 1}.
These are indicated in Fig. 6 by labelling states to the right above
and below by the observations made by L1 and L2, respectively.
We omit the observation ⊥ to reduce clutter.
It can be veriﬁed
that machine M satisﬁes MCOR. However, H’s information is not
secure against collusion by the coalition L = {L1, L2}. We may
represent the information held by the coalition L in a run r by
viewL(r), using the set-based view deﬁnition introduced above.
Similarly, we may generalize the purge function to the coalition by
writing purgeL(α) for the subsequence of actions a in α ∈ A∗
with dom(a) ∈ L. Let α = (cid:96)1(cid:96)2 and β = h(cid:96)1(cid:96)2. Note that
purgeL(α) = α = purgeL(β). But
1}
poss(viewL | Act = α) = { ⊥