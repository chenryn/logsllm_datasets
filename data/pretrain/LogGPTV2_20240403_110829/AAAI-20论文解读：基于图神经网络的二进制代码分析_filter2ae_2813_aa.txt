# AAAI-20论文解读：基于图神经网络的二进制代码分析
##### 译文声明
本文是翻译文章
译文仅供参考，具体内容表达以及含义原文为准。
腾讯安全科恩实验室《Order Matters: Semantic-Aware Neural Networks for Binary Code
Similarity
Detection》论文入选人工智能领域顶级学术会议AAAI-20。研究核心是利用AI算法解决大规模二进制程序函数相似性分析的问题，本文将深入对该论文进行解读，点击链接获取完整论文。https://keenlab.tencent.com/en/whitepapers/Ordermatters.pdf
二进制函数相似性比对演示效果：
论文：Order Matters: Semantic-Aware Neural Networks for Binary Code Similarity
Detection
单位 | 腾讯安全科恩实验室
## 引言 & 背景
二进制代码分析是信息安全领域中非常重要的研究领域之一，其中一类目标是在不访问源代码的情况下检测相似的二进制函数。同一份源代码在不同编译器，不同平台，不同优化选项的条件下所得到的二进制代码是不相同的，我们的任务目标是把同一份源代码所编译出的不同的二进制代码找到。传统算法使用图匹配算法解决此问题，但图匹配算法的速度较慢，且准确率较低。随着近年来深度学习算法的发展，学者们尝试在控制流图（CFG）上使用图神经网络算法，取得了不错的效果。
图1. 控制流图（CFG）以及表示成低维向量的block特征
论文[1]提出了名为Gemini的基于图神经网络的算法，它的输入是两个二进制函数的pair，输出是这两个二进制函数的相似度得分。首先，将二进制函数的控制流图作为输入，并使用人工设计的特征提取方法将每个block表示成低维的向量（如图1所示）；然后使用Structure2vec算法计算graph
embedding；最后使用siamese网络计算相似度得分并使用梯度下降算法降loss训练模型（如图2所示）。与传统方法相比，Gemini的速度和准确率都大幅提升。
图2. siamese网络结构
虽然上述方法取得了很大的进步，但仍有一些重要的问题值得研究。一方面，如图1所示，每一个block被表示成一个低维向量，这个特征提取的过程是人工设计的，在Gemini中block特征只有8维向量，这个压缩的过程会损失很多语义信息。另一方面，在二进制代码中节点的顺序是一个很重要的特征，而之前的模型没有设计特殊的算法提取这一特征。图3是函数”_freading”在不同平台x86-64和ARM上编译出的二进制代码的控制流图。这两个控制流图的节点顺序是非常相似的，例如node1都与node2和node3相连，node2都与node4和node5相连，而这种相似性可以体现在它们的邻接矩阵上。经过观察，我们发现许多控制流图的节点顺序变化是很小的。为了解决以上两个问题，我们设计了一种总体的框架，包含semantic-aware模块、structural-aware模块以及order-aware模块。
图3. 函数”_freading”在不同平台（x86-64和ARM）上编译出的控制流图以及对应的邻接矩阵
## 模型
整体结构：模型的输入为二进制代码的控制流图，模型的整体结构如图4所示，包含semantic-aware 模块、structural-aware模块、order-aware模块。在semantic-aware模块，模型将控制流图作为输入，使用BERT[2]对token
embedding作预训练，得到block embedding。在structural-aware模块，使用MPNN算法[3]得到graph
semantic & structural embedding。在order-aware模块，模型将控制流图的邻接矩阵作为输入，并使用CNN计算graph
order embedding。最后对两个向量使用concat和MLP得到最终的graph embedding，如公式1所示。
图4. 模型整体结构
Semantic-aware 模块：在semantic-aware模块，可以使用BERT、word2vec等常用模型提取语义信息。本文中使用BERT对控制流图作预训练，从而获得block的语义信息。BERT原用于NLP领域中，对词语与句子作预训练。我们的任务与NLP任务相似，控制流图的block可以看作句子，block中的token可以看作词语。如图5所示，训练过程中BERT有4个任务：Masked
language model（MLM）、Adjacency node prediction（ANP）、Block inside
graph（BIG）和Graph classification（GC）。
图5. 语义信息提取BERT模型
其中MLM和ANP是和BERT的原论文中相似的两个任务。MLM是一个token-level的任务，对block中的token进行mask操作并进行预测，和语言模型的方式相同。ANP任务是一个block-level的任务，虽然控制流图没有NLP领域中的语言顺序，但控制流图是一个有向图，也有节点的拓扑顺序，我们将控制流图中的所有相邻节点提取出来，当作相邻的“句子”。这些相邻的block
pair作为ANP任务的正例，并随机选择同图内不相邻的block pair作为负例。
为了获得更多的graph-level的信息，我们加入了两个辅助的graph-level任务BIG和GC。BIG和ANP的方式类似，区别是pair的正负例选择方式不同。BIG任务的目的是让模型判断两个block是否在同一个图中，希望模型可以尽可能地学到此信息，从而对我们的graph-level task有帮助。因此，在BIG任务中同图的block pair为正例，不同图的block pair为负例。GC为graph-level的block分类任务，在我们的场景中，在不同平台、不同编译器、不同优化选项的条件下，得到的block信息有所不同，我们希望模型可以让block
embedding中包含这种信息。GC对block进行分类，判断block属于哪个平台，哪个编译器，以及哪个优化选项。
Structural-aware 模块：经过BERT预训练后，使用MPNN计算控制流图的graph semantic & structural
embedding。MPNN有三个步骤：message function（M），update function（U）以及readout
function（R）。具体步骤如公式2-公式4所示。
其中G代表整个图，v代表节点，N(v)代表v的邻居节点。在本文的场景中，节点即是控制流图中的block，图即是经过预训练后表示成block向量的控制流图。本文在message步骤使用MLP，update步骤使用GRU，readout步骤使用sum，如公式5-公式7所示。
Order-aware
模块：本模块希望可以提取节点顺序的信息，本文中使用的是CNN模型。为什么使用CNN模型呢？首先考虑图6中的三个图（节点中无语义信息），以及它们的邻接矩阵。这三个图非常相似，每个图中都有一个三角形特征（图a的节点123，图b的节点234，图c的节点134），这个特征体现在它们的邻接矩阵中。首先对比图a和图b，与图a相比，图b加入了节点1，节点顺序依次后移一位，但三角形特征中三个节点的顺序还是连续的，这个特征在邻接矩阵中可以看到，这个1-1-0-1的2*2矩阵仍然存在。CNN在训练集中看过很多这种样例后，可以学习到这种平移不变性。再看图c，加入了节点2，打破了原有三角形的节点顺序，但在邻接矩阵中我们可以看到它实际上是把原来的2*2矩阵放大成了3*3矩阵，当我们移除第二行和第二列时，仍然可以得到一个1-1-0-1的2*2矩阵。这与图像中的image
scaling类似，CNN在训练集中包含足够多样例的情况下，也是可以学到这种伸缩不变性的。
图6. 三个图以及对应邻接矩阵