### Effectiveness of PICCOLO on Hidden Killer Attack

**Table X: Performance of PICCOLO in Detecting Hidden Killer Attacks**

| Dataset-Model | TP | FP | FN | TN | Accuracy |
|---------------|----|----|----|----|----------|
| SST-BERT      | 18 | 1  | 2  | 19 | 0.925    |
| SST-LSTM      | 18 | 0  | 2  | 20 | 0.95     |
| OLID-BERT     | 19 | 3  | 1  | 17 | 0.90     |
| OLID-LSTM     | 20 | 3  | 0  | 17 | 0.925    |
| AG-BERT       | 20 | 0  | 0  | 20 | 1.00     |
| AG-LSTM       | 20 | 0  | 0  | 20 | 1.00     |

**Table XI: Frequency of Structure Phrases in SST-2 Dataset (6921 Clean Training Samples) for Hidden Killer Attack**

| Structure Phrase | Clean Samples | Poisoned Samples |
|------------------|---------------|------------------|
| as the           | 7             | 81               |
| if i             | 6             | 82               |
| when you         | 4             | 83               |
| when the         | 17            | 80               |
| if the           | 0             | 80               |
| if it            | 67            | 80               |
| when he          | 62            | 44               |
| as i             | 11            | 3                |
| when they        | 62            | 120              |
| when i           | 77            | 4                |
| when it          | 63            | 3                |
| as it            | 54            | 3                |

### Effectiveness of PICCOLO on Combination Lock Attack

**Table XII: Performance of PICCOLO in Detecting Combination Lock Attacks**

| Dataset-Model   | TP | FP | FN | TN | Accuracy |
|-----------------|----|----|----|----|----------|
| SST-BERT-base   | 18 | 1  | 2  | 19 | 0.925    |
| SST-BERT-large  | 19 | 3  | 1  | 17 | 0.90     |
| OLID-BERT-base  | 20 | 2  | 0  | 18 | 0.95     |
| OLID-BERT-large | 20 | 3  | 0  | 17 | 0.925    |
| AG-BERT-base    | 18 | 2  | 2  | 18 | 0.90     |
| AG-BERT-large   | 18 | 2  | 2  | 18 | 0.90     |

### Adaptive Attacks

#### Targeting Word Discriminativity Analysis

In this attack scenario, the adversary leverages the dot product between the linear model weights and the importance vector for the CLS embedding. The loss function is defined as:

\[ L = L_{\text{ce}}(x_t, y_t) + \lambda \cdot \theta_w \cdot I \]

where \( L_{\text{ce}}(x_t, y_t) \) is the cross-entropy loss, \( \theta_w \) are the weights for a trigger \( w \), \( I \) is the importance vector, and \( \lambda \) is a weight to balance the training and adaptive losses.

**Table XIII: Performance of PICCOLO with Dot Product Loss Adaptive Attack**

| Trigger Type   | Loss Weight | Accuracy | ASR | Detection |
|----------------|-------------|----------|-----|-----------|
| Phrase         | 0.01        | 0.88     | 0.88| 0.88      |
|                | 0.1         | 0.87     | 0.76| 1.00      |
| Character      | 0.0001      | 0.89     | 0.88| 0.88      |
|                | 0.01        | 0.87     | 0.46| 0.70      |
| Word           | 0.0001      | 0.89     | 0.88| 0.88      |
|                | 0.01        | 0.83     | 0.77| 0.90      |

#### Targeting Trigger Inversion

This adaptive attack targets the trigger inversion component of PICCOLO. The loss function is defined as:

\[ L = L_{\text{ce}}(x_t, y_t) + \alpha \cdot L_{\text{inverse}}(x_z, \tau) + \lambda \cdot x_z[i_{\text{target}}] \]

where \( L_{\text{ce}}(x_t, y_t) \) is the cross-entropy loss, \( L_{\text{inverse}}(x_z, \tau) \) is the loss to invert a trigger word vector \( x_z \) towards the target label \( \tau \), and \( \lambda \) balances the adaptive loss.

**Table XIV: Performance of PICCOLO with Trigger Inversion Loss Adaptive Attack**

| Trigger Type   | Loss Weight | Accuracy | ASR | Detection |
|----------------|-------------|----------|-----|-----------|
| Character      | 1           | 0.89     | 0.87| 1.00      |
|                | 10          | 0.87     | 0.86| 1.00      |
|                | 100         | 0.86     | 0.81| 0.80      |
|                | 1000        | 0.85     | 0.73| 0.80      |
| Word           | 0.001       | 0.89     | 0.87| 1.00      |
|                | 0.01        | 0.88     | 0.84| 0.80      |
|                | 0.1         | 0.86     | 0.74| 0.70      |
| Phrase         | 0.001       | 0.89     | 0.87| 1.00      |
|                | 0.01        | 0.88     | 0.84| 0.80      |
|                | 0.1         | 0.86     | 0.74| 0.70      |

### Additional Studies

- **Ablation Study**: Our ablation study shows that all design choices are important. Details can be found in our online appendix.
- **Injection Positions of the Optimization Vector**: Experiments show that different injection schemes have similar overall detection accuracy. Details are in the online appendix.
- **Effectiveness on Different Types of Triggers**: PICCOLO consistently outperforms T-miner and GBDA across all trigger types. Detailed results are in the online appendix.

For more information, please refer to our online appendix: [https://github.com/PurduePAML/PICCOLO](https://github.com/PurduePAML/PICCOLO)

Authorized licensed use limited to: Tsinghua University. Downloaded on August 07, 2022 at 12:56:09 UTC from IEEE Xplore. Restrictions apply.