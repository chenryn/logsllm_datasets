ing place, it can mean one of two things. First, it could
mean that the program contains a vulnerability that should
be ﬁxed. For example, the program may be using an
unchecked input as a format string. In this case, the best
solution is to ﬁx the vulnerability, possibly using Taint-
Check’s Exploit Analyzer to help understand it. Another
possibility is to conﬁgure TaintCheck to only signal an at-
tack if some other condition is also true- for example, if a
tainted format string actually does contain dangerous for-
mat speciﬁers (such as %n).
The other possibility is that the program performs san-
ity checks on the tainted data before it is used, ensuring
that the operation is actually safe. For example, the pro-
gram might use tainted data as a format string, but only
after it has ensured that it does not contain any malicious
format speciﬁers such as %n (which would signify a pos-
sible format string attack). Another example is that a pro-
gram could use tainted data as a jump target in a jump
table, after checking that it is within expected bounds.
Fortunately, these cases occur relatively rarely and usu-
ally at ﬁxed places (program counters) in programs. Most
of these false positives can be detected by running pro-
grams on legitimate inputs through a training phase. In
these cases, TaintCheck can either be conﬁgured to ignore
the particular failed taint assertion, or, if additional infor-
mation is available, to untaint the data immediately after
it has been sanity checked. The latter option is safer, since
an attacker may attempt to overwrite the data again after
it has been sanity checked.
4. Evaluation
We evaluate TaintCheck’s compatibility and incidence
of false positives in Section 4.1, its effectiveness against
various attacks in Section 4.2, and its performance in Sec-
tion 4.3.
4.1. Compatibility and false positives
We used TaintCheck to monitor a number of programs
in order to check for false positives, and to verify that
the programs were able to run normally. We tested sev-
eral server programs: apache, ATPhttpd, bftpd, cﬁngerd,
and named; client programs: ssh and ﬁrebird; and non-
network programs: gcc,
latex, vim,
emacs, and bash.
ls, bzip2, make,
All of these programs functioned normally when run
under TaintCheck, and no false positives occurred using
TaintCheck’s default policy of tainting data from network
sockets and asserting that jump targets and format strings
are untainted. In our evaluation using named, we replayed
a trace containing 158,855 DNS queries to the primary
nameserver at Princeton University. This nameserver is
also the secondary server for a top level European do-
main, and handles outbound queries for Princeton users.
Hence, this trace contains a diverse set of requests from
a diverse set of clients. Our named server was conﬁg-
ured to resolve each request by performing a recursive
query. The TaintCheck-monitored named server behaved
correctly and did not generate any false positives.
To further test for false positives, we tried running all
of the client programs and non-network programs with
a policy to taint data read from standard input, and data
read from ﬁles (except for ﬁles owned by root, notably
including dynamically loaded libraries). The only addi-
tional false positives that resulted were in vim and ﬁrebird.
In both cases, the program appears to be using data read
from one of its respective conﬁguration ﬁles as an offset to
a jump address. This could easily be ﬁxed by conﬁguring
TaintCheck to trust the corresponding conﬁguration ﬁles.
4.2. Evaluation of attack detection
We tested TaintCheck’s ability to detect several types
of attacks, including several synthetic and actual exploits.
Most of these attacks attempted to use a vulnerability to
overwrite a sensitive value. The one exception is an infor-
mation leak attack in which a user-supplied format string
contained format speciﬁers, causing the program to out-
put data from the stack. As Table 1 shows, TaintCheck
successfully detected each attack. For the format string
attacks that attempted to overwrite another value, Taint-
Check detected both that a tainted format string was being
used, and that the other value had been overwritten. Addi-
tionally, TaintCheck successfully identiﬁed the value used
to overwrite the return address in the ATPhttpd exploit.
We show in Section 6 how this can be useful when gener-
ating a signature for buffer overﬂow attacks.
4.2.1. Synthetic exploits
In this section, we evaluate TaintCheck using synthetic ex-
ploits on buffer overruns that overwrite return addresses,
function pointers, and format string vulnerabilities. In all
these evaluations, TaintCheck successfully detected all at-
tacks and resulted in no false positives.
Detecting overwritten return address
In order to test
TaintCheck’s ability to detect an overwritten return ad-
dress, we wrote a small program with a buffer overﬂow
vulnerability. The program uses the dangerous “gets”
function in order to get user input. An input that is too
long will overﬂow the buffer and begin overwriting the
stack. We performed a test in which the return address is
overwritten with the address of an existing function in the
code. TaintCheck was able to detect the attack because the
return address was tainted from user input.
Detecting overwritten function pointer
In a similar
test, we veriﬁed TaintCheck’s ability to detect an over-
written function pointer. We wrote a program with a stack
buffer overﬂow vulnerability where the overrun buffer
could overwrite a function pointer on the stack. Again,
TaintCheck correctly detected the attack because the func-
tion pointer was tainted by user input during the buffer
overrun.
Detecting format string vulnerability Finally, we
wrote another program to verify TaintCheck’s ability to
detect a tainted format string, which can lead to a format
string attack. This program took a line of input from the
user, and printed it back by using it as the format string in
a call to printf. When we ran this program under Taint-
Check, TaintCheck correctly detected that a tainted format
string was being used in printf. As a further test, we
wrote a program with a buffer overrun vulnerability that
allowed the attacker to overwrite a format string. An at-
tacker might choose to overwrite the format string to per-
form a format string attack instead of directly overwriting
the return address in order to evade some buffer-overﬂow
protection mechanisms. Again, we found that TaintCheck
was able to determine correctly when the format string
was tainted.
4.2.2. Actual exploits
In this section, we evaluate TaintCheck on exploits to three
vulnerable servers: a web server, a ﬁnger daemon, and an
FTP server. In all these evaluations, TaintCheck success-
fully detected all the attacks and incurred no false posi-
tives during normal program execution.
ATPhttpd exploit ATPhttpd [35] is a web server pro-
gram. Versions 0.4b and lower are vulnerable to several
buffer overﬂow vulnerabilities. We obtained an exploit
that sends the server a malicious GET request [34]. The
request asks for a very long ﬁlename, which is actually
shellcode and a return address. The ﬁlename overruns a
buffer, causing the return address to be overwritten. When
the function attempts to return, it jumps instead to the
shellcode inside the ﬁle name. The attacker is then given
a remote shell.
TaintCheck correctly detected that the return address
was tainted when the server was attacked, and did not
generate any false positives when serving normal GET
requests. TaintCheck also correctly identiﬁes the return
address value that overwrites the previous value. As we
show in Section 6, this can sometimes be used as a signa-
ture for an attack.
cﬁngerd exploit
cﬁngerd is a ﬁnger daemon that con-
tains a format string vulnerability in versions 1.4.2 and
lower. We obtained an exploit for this vulnerability that
works as follows. When cﬁngerd prompts for a user name,
the exploit responds with a string beginning with “ver-
sion”, and also containing malicious code. Due to another
bug, cﬁngerd copies the whole string into memory, but
only reads to the end of the string “version”. Thus, the
malicious code is allowed to reside in memory, and the
string appears to be a legitimate query.
cﬁngerd later contacts the identd daemon running on
the querier’s machine. The exploit runs its own identd,
Program
ATPhttpd
synthetic
synthetic
synthetic
cﬁngerd
wu-ftpd
Overwrite Method
Overwrite Target Detected
buffer overﬂow
buffer overﬂow
buffer overﬂow
format string
syslog format string
vsnprintf format string
return address
function pointer
format string
none (info leak)
GOT entry
return address
4
4
4
4
4
4
Table 1. Evaluation of TaintCheck’s ability to detect exploits
responding with a string that will be later used as a for-
mat string to the syslog function. When cﬁngerd uses
this format string, the entry for the exit function in the
global offset table is overwritten to point to the malicious
code that was inserted in the ﬁrst step. When cﬁngerd ﬁn-
ishes processing the query, it attempts to exit, but is caused
to execute the attacker’s code instead.
During normal usage, TaintCheck correctly detects that
tainted data is being used as a format string. When we
used the exploit, TaintCheck detected the tainted format
string, and later detected when the program attempted to
use the tainted pointer in the global offset table.
wu-ftpd exploit Version 2.6.0 of wu-ftpd has a format
string vulnerability in a call to vsnprintf. We ob-
tained an exploit for this vulnerability that uses the for-
mat string to overwrite a return address. TaintCheck suc-
cessfully detects both that the format string supplied to
vsnprintf is tainted, and that the overwritten return ad-
dress is tainted.
4.3. Performance evaluation
We measured TaintCheck’s performance using two
“worst-case” workloads (a CPU-bound workload and a
short-lived process workload), and what we consider to be
a more common workload (a long-lived I/O-bound work-
load). For each workload, we measured the performance
when the program was run natively, when it ran under
Nullgrind (a Valgrind skin that does nothing), when it ran
under Memcheck (a commonly used Valgrind skin that
checks for run-time memory errors, such as use of unini-
tialized values), and when it ran under TaintCheck. Our
evaluation was performed on a system with a 2.00 GHz
Pentium 4, and 512 MB of RAM, running RedHat 8.0.
CPU-bound: bzip2
In order to evaluate the penalty
from the additional
instrumentation that TaintCheck
writes into the monitored binary at run-time, we evalu-
ated the performance of bzip2, a CPU-bound program.
Speciﬁcally, we measured how long bzip2 took to com-
press a 15 MB package of source code (Vim 6.2). When
run normally, the compression took 8.2 seconds to com-
plete. When run under Valgrind’s Nullgrind skin, the task
took 25.6 seconds (3.1 times longer). When using Mem-
Check, it took 109 seconds (13.3 times longer). When us-
ing TaintCheck, it took 305 seconds (37.2 times longer).
Note that this is a worst-case evaluation as the applica-
tion is completely CPU-bound. (Also note that we dis-
cuss optimization techniques at the end of this section, one
of which in early implementation improves performance
overhead to 24 times slowdown.)
Short-lived: cﬁngerd When a program starts, each ba-
sic block is rewritten on demand to include TaintCheck’s
instrumentation. While basic block caching amortizes this
penalty over a long execution time, it can be more sig-
niﬁcant for very short-lived processes. In order to evalu-
ate this case, we timed how long cﬁngerd 1.4.2 takes to
start and serve a ﬁnger request. cﬁngerd runs under inetd,
which means it restarts for each request.
Without Valgrind, the request took an average of .0222
seconds. Using the Nullgrind skin, the request took 13
times as long. The Memcheck skin took 32 times as long,
and TaintCheck took 36 times as long.
Common case: Apache For many network services, the
latency that a user experiences is due mostly to network
and/or disk I/O. For these types of services, we expect
that the TaintCheck’s performance penalty should not be
as noticeable to the user. To evaluate this type of work-
load, we used the Apache 2.0.49 web server.
In these tests we requested different web pages from
the server, and timed how long it took to connect, send
the request, and receive the response.
In order to pre-
vent resource contention between the client process and
the server process, the client was run from another ma-
chine connected to the server by a 100 Mbps switch. We
requested a dynamically generated CGI shell script and
static pages of sizes 1 KB to 10 MB. For each test, we re-
quested the same page one hundred times, (thus allowing
the document to stay in the server’s cache) and used the
median response time. Figure 3 shows the performance
overhead for each type of request.
r
o
t
c
a
F
d
a
e
h
r
e
v
O
e
c
n
a
m
r
o
f
r
e
P
30
25
20
15
10
5
0
No Valgrind
Nullgrind
Memcheck
TaintCheck
CGI
6.63 ms
1 KB
.987 ms
10 KB
2.05 ms
100 KB
9.79 ms
1 MB
86.4 ms
10 MB