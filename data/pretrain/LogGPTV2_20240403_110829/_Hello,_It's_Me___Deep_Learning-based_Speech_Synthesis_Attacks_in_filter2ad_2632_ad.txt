each participant, we generate six synthesized login samples.
• Alexa employs a text-independent speaker veriﬁcation system,
but its speciﬁc uses of voice proﬁles constrain the samples we
can test. We create a short list of Alexa commands that Ama-
zon explicitly states should be linked to a user’s voice proﬁle,
restricting our attention to native Alexa skills [7] (see Table 4).
After setting up their WeChat voice login and Alexa voice pro-
ﬁle (using the Alexa smartphone app), all 14 participants veriﬁed
that they could use their real voices to log into WeChat and access
speciﬁed applications with Alexa. They were then given their syn-
thesized speech samples (6 samples of login for WeChat, 7 samples
of commands for Alexa) and instructed to play each sample over a
computer loudspeaker located six inches from their phone micro-
phone. Samples were played as the targeted apps were set up to
perform normal voice authentication. Each participant tested the
WeChat samples twice and the Alexa samples once. Participants
recorded how the apps responded to the synthesized samples and
reported their results via a standardized form.
In total, our user study tested WeChat and
Attack Evaluation.
Alexa with 168 and 98 synthesized speech instances, respectively.
Again, we use attack success rate (AS) to evaluate how eﬀectively
synthesized speech can fool both SR systems. For WeChat, each
attack instance is successful if the login is approved. For Alexa, we
use a diﬀerent approach because Alexa does not provide clear-cut
success/failure results: a synthesis attack instance SA succeeds if
Alexa responds to SA the same way it responds to a clean (non-
synthesized) version of the command.
Results. On average, our speech synthesis attacks had a 63% AS
across all tests on WeChat and Alexa.
WeChat: 9 of the 14 participants (64%) successfully logged into
their WeChat account using a synthesized speech sample. In gen-
eral, this indicates that speech synthesis attacks are a viable au-
thentication attack against WeChat. However, the number of suc-
cessful fake login samples varied signiﬁcantly across participants
(despite using the same setup for each test). On average, 1.33 ± 1.67
Session 1D: Authentication and Click Fraud CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea241SR System
WeChat
Amazon
Alexa
Phrases Used for SR
Hey Alexa add an event to my calendar for tomorrow at 5.
Hey Alexa check my email
Alexa say who is talking with you now
Alexa tell me what is on my calendar
Tell me what is on my calendar for this week
Alexa make an appointment with my doctor
Hey Alexa make a donation to the American Cancer Institute
AS
64.3%
71.4%
64.3%
35.7%
50.0%
85.7%
57.1%
71.4%
62.2%
Table 4: The phrases used in our Amazon Alexa and WeChat
experiments and the corresponding attack success rate (AS)
on synthesized versions of each phrase.
fake login samples succeeded per participant. For one participant,
all six login samples worked. For 8 other participants who success-
ful logged in, only one or two samples consistently worked.
Alexa: Our attacks on Alexa were similarly successful (62.2%
AS on average). All 14 participants had at least 2 synthesized com-
mands that fooled Alexa. These synthesized commands were able
to access private emails, check calendar appointments, and request
ﬁnancial transactions. Table 4 reports these results. Furthermore,
in limited tests, we found that sometimes a wrong voice (i.e. from
a diﬀerent person) was able to access user data purportedly pro-
tected by voice proﬁles.
Eﬀect of Loudspeaker: Our study participants reported the
device used to play their attack samples. Devices used include LG
desktop monitors, Macbook Pros, Bose Soundlink Speakers, and
iPhone 11s. We examined the impact of speaker hardware on at-
tack success and found no correlation between the two. While the
tested devices already cover a broad set of speaker hardwares found
in today’s households and oﬃces, more experiments are needed to
quantify if attack success depends on speaker quality.
Disclosure. We followed standard disclosure practices and re-
ported our attacks to both WeChat and Amazon.
4.5 Key Takeaways
All four modern SR systems tested are vulnerable to DNN-based
speech synthesis attacks, especially those generated by SV2TTS.
It is alarming that for three popular real-world SR systems (Azure,
WeChat, Alexa), more than 60% of enrolled speakers have at least 1
synthesized (attack) sample accepted by these systems. This clearly
demonstrates the real-world threat of speech synthesis attacks.
Another key observation is that the attack performance is speaker-
dependent, e.g., the number of synthesized samples that success-
fully fooled the SR systems varies across speakers. For Resemblyzer
and Azure, the attack success rate is consistently higher for female
and native English speakers.
Limitations and Next Steps. Our experiments, especially those
on WeChat and Alexa, involved a moderately-sized set of target
speakers to demonstrate the real-world threat of speech synthesis
attacks. To further evaluate the attack dependence on target hu-
man speakers, we believe viable next steps include expanding the
speaker pool and testing more operational scenarios. With these
two changes, we could more closely examine how an individual’s
vocal characteristics (e.g., pitch, accent, tone) aﬀect the attack suc-
cess rate, and whether their impact can be reduced by improving
the underlying speech synthesis systems.
Similarly, due to our focus on low-resource attackers, our ex-
periments used two publicly available speech synthesis systems
(SV2TTS and AutoVC) that were trained only on publicly available
datasets. These two systems will likely underperform advanced
synthesis systems trained on larger, proprietary datasets, and con-
sequently our reported results only oﬀer a “conservative” measure
of the threat. As speech synthesis systems continue to advance,
the threat (and damage) of speech synthesis attacks will grow and
warrant our continuous attention.
5 SYNTHESIZED SPEECH VS. HUMANS
Having demonstrated that DNN-synthesized speech can easily fool
machines (e.g., real-world SR systems), we now move to evaluate
their impact on humans. Diﬀerent from prior work that uses sur-
veys to measure human perception of speech synthesized by clas-
sical (non-DNN) tools [57, 60], we assess the susceptibility of hu-
mans to DNN-synthesized speech in diﬀerent interactive settings.
For this we conduct two user studies, covering both static survey
and “trusted” interaction settings. Next, we describe the methodol-
ogy behind our user studies and give a preview of our key ﬁndings,
before presenting both studies in detail.
5.1 Methodology and Key Findings
An attacker A can perform a variety of attacks against human lis-
teners using synthesized speech. Such attacks can be particularly
eﬀective if the listener has limited familiarity with the owner of the
spoofed voice. For example, A could use a synthesized voice to per-
form the classic spear-phishing attack, where an elderly victim gets
a phone call from their “grandchild” they haven’t seen in months
who is stuck in a foreign country and needs emergency cash to get
home, or an employee gets a call from their “boss” conﬁrming an
earlier (phishing) email authorizing a money transfer [82].
With these in mind, our study on the impact of synthesized
voices on human listeners has two goals: understanding human
listeners’ susceptibility to synthesized speech in isolation and in
“trusted contexts.” We designed experimental protocols for both
parts of our study, giving detailed consideration to issues of ethics
and impact on our participants. All protocols were carefully eval-
uated and approved by our institutional IRB review board. We dis-
cuss ethical considerations in §7.
User Study A (Online Survey). We ﬁrst evaluate whether hu-
mans can discern the diﬀerence between real and DNN-synthesized
(fake) speech. We conduct an online user survey and compare how
well participants could identify synthesized speech for voices with
which they have varying levels of familiarity (e.g., strangers vs.
celebrities). We also measure the eﬀect of priming by comparing
results from 2 scenarios: one in which participants are told the
speech samples will contain a mix of real and fake speech, and
one without the disclosure.
Finding: In this “survey” setting, DNN-synthesized speech fails
to consistently fool humans. Participants could more easily distin-
guish between real and fake speech when they were more familiar
with the speaker, and when they were aware that some speech may
be fake (thus tended to listen carefully with added skepticism).
User Study B (Deceptive Zoom Interview). We seek to bet-
ter understand the impact of context on listeners’ susceptibility to
fake speech. To do so, we conduct interviews over Zoom calls. Par-
ticipants believed they were speaking to two (human) researchers,
but in reality one of the voices was synthesized speech.
Session 1D: Authentication and Click Fraud CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea242Finding: In this “trusted setting,” all 14 participants showed no
hesitancy or suspicion during the interview, and readily responded
to and complied with all requests from the “fake interviewer.” In
other words, a synthesized voice consistently fooled humans in
this trusted interview setting.
5.2 User Study A: Can Users Distinguish
Synthesized and Real Speech?
We begin our human perception experiments with the critical ques-
tion: “can human listeners distinguish synthetic speech of a speaker
from the real thing?” We deploy a survey to assess users’ ability to
distinguish between real and fake speakers.
Participants. We recruited 200 participants via the online crowd
source platform Proliﬁc (https://www.proliﬁc.co/). All self-identiﬁed
as native English speakers residing in the United States. Of our par-
ticipants, 57% identiﬁed as female (43% male). The participants are
all 18+ years old and cover multiple age groups: 18-29 (43%), 30-39
(32%), 40-49 (14%), 50-59 (8%), 60+(3%). The survey was designed to
take 10 minutes on average, and participants received $2 as com-
pensation. The study was approved by our local IRB.
Procedure. The participants completed an online survey consist-
ing of several speech samples presented in pairs for side-by-side
comparison. Each pair of samples contains one of the three follow-
ing combinations: two real speech samples of the same speaker
(referred to as “Real A/Real A” in this section); one real speech
sample from a speaker and one real speech sample from a diﬀer-
ent speaker (“Real A/Real B”); or one real speech sample from a
speaker and one fake speech sample imitating the speaker (“Real
A/Fake A”). We generate fake speech using SV2TTS, using 30 sec-
onds of clean speech samples ST from the speaker.
Types of Speakers: We included speakers whose (real) voices
have varying levels of familiarity with our participants:
• Unfamiliar speakers: Speakers from the VCTK [97] dataset whose
• Brieﬂy familiar speakers:
voices have (most likely) never been heard by the participants.
Inspired by [57], we included a set
of speakers whose voices the participants hear only brieﬂy. For
each speaker, we provided participants with a short audio clip to
familiarize them with the speaker’s voice. There are four brieﬂy
familiar speakers, and for each one we provided a diﬀerent length
audio clip – 30 seconds for the ﬁrst speaker, 60 seconds for the
second, 90 seconds for the third, and 120 seconds for the fourth.
• Famous speakers: We used the voices of two American public
ﬁgures: Donald Trump and Michelle Obama. We asked partici-
pants whether they have heard these voices outside the context
of this survey, and over 90% responded “yes.”
Participants listened to pairs of speech samples and re-
Task.
ported if both samples were spoken by the same person.
Conditions. We deploy two versions of the survey. Both ver-
sions ask participants to assess the identity of the speaker and
quality of speech samples. The ﬁrst version does not mention fake
speech at all. The second version of the survey mentions fake speech,
both in its title and in its description of the task.
Results. We seek to answer the following questions:
1) Do participants think generated fake speech was spoken
by the original speaker? As shown in Table 5 (bottom row), about
Unfamiliar
Brieﬂy Familiar
Famous
Yes
No
Yes
0%
No
Yes
93.7%
0%
3.3%
No
0%
Not
Sure
Not
Sure
9.1%
6.7%
Not
Sure
Real A / Real A 90.9%
6.9%
69.6% 17.5% 12.9% 76.4% 16.7%
96.7%
9.5%
90.5%
Real A / Real B
Real A / Fake A 17.3% 32.7% 50.0% 18.5% 31.2% 50.3%
16.0% 79.9%
Table 5: Participants’ answers when asked if two voice sam-
ples were from the same person. We use this to gauge their
ability to correctly discern if speech samples were spoken
by the same speaker (Real A/Real A), diﬀerent person (Real
A/Real B), or a synthesized (fake) speaker (Real A/Fake A).
0%
4.1%
Fake Speech NOT Mentioned
Fake Speech Mentioned
t
n
e
c
r
e
P
 100
 50
 0
 100
 50
 0
No
Unsure
Yes
Unfamiliar Brief
Famous
Unfamiliar Brief
Famous
Figure 4: User responses to the question “are these two voice
samples from the same person?” (Left) when users are not
told synthesized speech is used in the survey; (Right) when
users are told this.
half of participants were fooled, i.e. they responded “yes” or “not
sure,” when asked this question about unfamiliar or brieﬂy familiar
speakers. For famous speakers whose voice participants are gener-
ally familiar with, this number drops to 20%.
2) Does hearing more samples from a speaker (i.e., knowing
a speaker be(cid:31)er) make fake speech more detectable? Results in
Table 5 suggest that greater familiarity with a speaker will lead to
increased skepticism of a fake voice. Compared to a similar user
study performed 6 years ago [57], proportion of participants who