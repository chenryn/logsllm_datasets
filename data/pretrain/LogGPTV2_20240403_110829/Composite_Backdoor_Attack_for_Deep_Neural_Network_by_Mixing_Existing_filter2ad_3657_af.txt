and Surya Nepal. 2019. Strip: A defence against trojan attacks on deep neu-
ral networks. In Proceedings of the 35th Annual Computer Security Applications
Conference. 113–125.
[17] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. 2017. Badnets: Identifying
vulnerabilities in the machine learning model supply chain. arXiv preprint
arXiv:1708.06733 (2017).
[18] Raia Hadsell, Sumit Chopra, and Yann LeCun. 2006. Dimensionality reduction
by learning an invariant mapping. In 2006 IEEE Computer Society Conference on
Computer Vision and Pattern Recognition (CVPR’06), Vol. 2. IEEE, 1735–1742.
[19] Gary B Huang, Marwan Mattar, Tamara Berg, and Eric Learned-Miller. 2008.
Labeled faces in the wild: A database forstudying face recognition in unconstrained
environments. Technical Report 07–49. University of Massachusetts, Amherst.
[20] Soheil Kolouri, Aniruddha Saha, Hamed Pirsiavash, and Heiko Hoffmann. 2020.
Universal Litmus Patterns: Revealing Backdoor Attacks in CNNs. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 301–310.
[21] Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning multiple layers of features
from tiny images. (2009).
[22] Wenshuo Li, Jincheng Yu, Xuefei Ning, Pengjun Wang, Qi Wei, Yu Wang, and
Huazhong Yang. 2018. Hu-fu: Hardware and software collaborative attack frame-
work against neural networks. In 2018 IEEE Computer Society Annual Symposium
on VLSI (ISVLSI). IEEE, 482–487.
[23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014. Microsoft coco: Common
objects in context. In European conference on computer vision. Springer, 740–755.
[24] Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. 2018. Fine-pruning: De-
fending against backdooring attacks on deep neural networks. In International
Symposium on Research in Attacks, Intrusions, and Defenses. Springer, 273–294.
[25] Yingqi Liu, Wen-Chuan Lee, Guanhong Tao, Shiqing Ma, Yousra Aafer, and
Xiangyu Zhang. 2019. ABS: Scanning neural networks for back-doors by artificial
brain stimulation. In Proceedings of the 2019 ACM SIGSAC Conference on Computer
and Communications Security. 1265–1282.
[26] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang,
and Xiangyu Zhang. 2018. Trojaning attack on neural networks. In 25nd Annual
Network and Distributed System Security Symposium (NDSS). 18–221.
[27] Yuntao Liu, Yang Xie, and Ankur Srivastava. 2017. Neural trojans. In 2017 IEEE
International Conference on Computer Design (ICCD). IEEE, 45–48.
[28] Jiajun Lu, Theerasit Issaranon, and David Forsyth. 2017. Safetynet: Detecting and
rejecting adversarial examples robustly. In Proceedings of the IEEE International
Conference on Computer Vision. 446–454.
[29] Oren Melamud, Jacob Goldberger, and Ido Dagan. 2016. context2vec: Learning
generic context embedding with bidirectional lstm. In Proceedings of The 20th
SIGNLL Conference on Computational Natural Language Learning. 51–61.
[30] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases and their compositionality. In
Advances in neural information processing systems. 3111–3119.
[31] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal
Frossard. 2017. Universal adversarial perturbations. In Proceedings of the IEEE
conference on computer vision and pattern recognition. 1765–1773.
[32] Luis Muñoz-González, Battista Biggio, Ambra Demontis, Andrea Paudice, Vasin
Wongrassamee, Emil C Lupu, and Fabio Roli. 2017. Towards poisoning of deep
learning algorithms with back-gradient optimization. In Proceedings of the 10th
ACM Workshop on Artificial Intelligence and Security. 27–38.
[33] Sinno Jialin Pan and Qiang Yang. 2009. A survey on transfer learning. IEEE
Transactions on knowledge and data engineering 22, 10 (2009), 1345–1359.
[34] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay
Celik, and Ananthram Swami. 2017. Practical black-box attacks against machine
learning. In Proceedings of the 2017 ACM on Asia conference on computer and
communications security. 506–519.
[35] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami.
2016. Distillation as a defense to adversarial perturbations against deep neural
networks. In 2016 IEEE Symposium on Security and Privacy (SP). IEEE, 582–597.
[36] Omkar M Parkhi, Andrea Vedaldi, and Andrew Zisserman. 2015. Deep face
recognition. (2015).
[37] Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove:
Global vectors for word representation. In Proceedings of the 2014 conference on
empirical methods in natural language processing (EMNLP). 1532–1543.
[38] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. 2016. You
only look once: Unified, real-time object detection. In Proceedings of the IEEE
conference on computer vision and pattern recognition. 779–788.
[39] Joseph Redmon and Ali Farhadi. 2018. Yolov3: An incremental improvement.
arXiv preprint arXiv:1804.02767 (2018).
[40] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015. Faster r-cnn:
Towards real-time object detection with region proposal networks. In Advances
in neural information processing systems. 91–99.
[41] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean
Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al.
2015. Imagenet large scale visual recognition challenge. International journal of
computer vision 115, 3 (2015), 211–252.
[42] Florian Schroff, Dmitry Kalenichenko, and James Philbin. 2015. Facenet: A
unified embedding for face recognition and clustering. In Proceedings of the IEEE
conference on computer vision and pattern recognition. 815–823.
[43] Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer,
Tudor Dumitras, and Tom Goldstein. 2018. Poison frogs! targeted clean-label poi-
soning attacks on neural networks. In Advances in Neural Information Processing
Systems. 6103–6113.
[44] Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K Reiter. 2019. A
general framework for adversarial examples with objectives. ACM Transactions
on Privacy and Security (TOPS) 22, 3 (2019), 1–30.
[45] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. 2012. Man vs.
computer: Benchmarking machine learning algorithms for traffic sign recognition.
Neural networks 32 (2012), 323–332.
[46] Jacob Steinhardt, Pang Wei W Koh, and Percy S Liang. 2017. Certified defenses
for data poisoning attacks. In Advances in neural information processing systems.
3517–3529.
[47] Yi Sun, Xiaogang Wang, and Xiaoou Tang. 2014. Deep learning face represen-
tation from predicting 10,000 classes. In Proceedings of the IEEE conference on
computer vision and pattern recognition. 1891–1898.
[48] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
Ian Goodfellow, and Rob Fergus. 2013. Intriguing properties of neural networks.
arXiv preprint arXiv:1312.6199 (2013).
[49] Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior Wolf. 2014. Deepface:
Closing the gap to human-level performance in face verification. In Proceedings
of the IEEE conference on computer vision and pattern recognition. 1701–1708.
[50] Florian Tramèr, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas Ristenpart.
2016. Stealing machine learning models via prediction apis. In 25th {USENIX}
Security Symposium ({USENIX} Security 16). 601–618.
[51] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao
Zheng, and Ben Y Zhao. 2019. Neural cleanse: Identifying and mitigating backdoor
attacks in neural networks. In 2019 IEEE Symposium on Security and Privacy (SP).
IEEE, 707–723.
[52] Mei Wang and Weihong Deng. 2018. Deep visual domain adaptation: A survey.
Neurocomputing 312 (2018), 135–153.
[53] Karl Weiss, Taghi M Khoshgoftaar, and DingDing Wang. 2016. A survey of
transfer learning. Journal of Big data 3, 1 (2016), 9.
[54] Lior Wolf, Tal Hassner, and Itay Maoz. 2011. Face recognition in unconstrained
videos with matched background similarity. In CVPR 2011. IEEE, 529–534.
[55] Mike Wu, Michael C Hughes, Sonali Parbhoo, Maurizio Zazzi, Volker Roth, and
Finale Doshi-Velez. 2018. Beyond sparsity: Tree regularization of deep models
for interpretability. In Thirty-Second AAAI Conference on Artificial Intelligence.
Session 1B: Attacking and Defending ML Systems CCS '20, November 9–13, 2020, Virtual Event, USA125[56] Xi Wu, Matthew Fredrikson, Somesh Jha, and Jeffrey F Naughton. 2016. A
methodology for formalizing model-inversion attacks. In 2016 IEEE 29th Computer
Security Foundations Symposium (CSF). IEEE, 355–370.
[57] Weilin Xu, David Evans, and Yanjun Qi. 2017. Feature squeezing: Detecting
adversarial examples in deep neural networks. arXiv preprint arXiv:1704.01155
(2017).
[58] Yuanshun Yao, Huiying Li, Haitao Zheng, and Ben Y Zhao. 2019. Latent Backdoor
Attacks on Deep Neural Networks. In Proceedings of the 2019 ACM SIGSAC
Conference on Computer and Communications Security. 2041–2055.
[59] Matthew D Zeiler and Rob Fergus. 2014. Visualizing and understanding convolu-
tional networks. In European conference on computer vision. Springer, 818–833.
9 APPENDIX
A Unsuccessful Alternative Design
We had a few unsuccessful explorations of other designs. In this
section, we discuss some of them and explain why they failed.
Our attack model is similar to the BadNets’ [17], in which we
assume the attacker has access to the full training set. As such, the
trojan training is by poisoning a random subset of the training data,
that is, stamping them with the trigger and modifying their labels
to the target label. At the beginning, we actually explored a less
demanding attack model in which the attacker only has access to the
model but not the training set. Hence, we tried to generate training
data by reverse engineering [50, 56], which generates input samples
from a given output label using an optimization based method. The
generated samples then go through the mixer and are used in trojan
training.
We tried two reverse engineering methods on CIFAR-10 to gener-
ate poisonous samples: (i) reverse engineer inputs for the individual
trigger labels separately and then use a mixer to mix the generated
inputs; (ii) directly reverse engineer a composite input that has the
features from the trigger labels by inverting the labels together, i.e.,
searching for an input that can maximize the logits of the trigger
labels together. Figure 4(d) shows an example for the first method,
in which we reverse engineer two samples for the airplane and au-
tomobile labels, respectively. Note that a reverse engineered image
may not look like the real object in most cases, but it serves the
same purpose of real image. We then provided them to a mixer
to generate the poisonous sample shown on the right of Figure
4(d). Figure 4(e) shows an example for the second method, in which
we directly reverse engineer the poisonous sample from the two
labels. Observe that the generated image contains features from
both airplane and automobile.
However, our experience shows that trojaned models trained
through the above two methods had poor performance on poi-
sonous data, and even degraded classification accuracy for normal
inputs. The failure of these alternatives is mainly because the re-
verse engineered samples lack diversity (of the feature combina-
tions) as the optimization based method tends to generate the same
or only a very limited set of feature combinations (of the trigger
labels). It is understandable because the original model did not go
though a learning procedure that forces the model to learn the vari-
ous combinations of features. As such, one cannot reverse engineer
information from a model if it has not learned such information.
Hence, the access to training set and the random mixing method in
our current design are critical for the success of the attack. Note that
although input reverse engineering was successfully used in [26]
to derive inputs for trajaning, their triggers are just simple patches
such that the model does not need to learn a lot. In contrast, our
attack leverages combinations of various existing features.
B Tasks in Our Experiments
The tasks are presented in Table 9. Column 1 shows the tasks.
Column 2 shows the datasets. Columns 3 and 4 show the statistics
of the dataset. Column 5 shows the input size of the model. Column
6 shows the model architecture.
• Object Recognition (CIFAR-10). This task mostly involves
computer vision models. The CIFAR-10 dataset is a light-
weight and widely used dataset for machine learning re-
search. The task is to recognize images in 10 different classes
(e.g., airplane and automobile). The dataset contains 60K
samples. The model we test is a CNN with 4 convolutional
layers and 3 fully connected layers.
• Traffic Sign Recognition (GTSRB). This task is also com-
monly used to evaluate attacks on DNNs. It is to recognize
43 different traffic signs, simulating the application scenario
in self-driving cars. It uses the German Traffic Sign Bench-
mark dataset (GTSRB), which contains 39K labeled training
images and 13K test images. The CNN model consists of 6
convolution layers and 3 fully connected layers.
• Face Recognition (YouTube Face). This task simulates a se-
curity screening scenario via face recognition, where it tries
to recognize the faces of 1,595 different people. The large
size of the dataset increases the computational cost of our
method. It is hence a good candidate to evaluate our attack.
It uses the YouTube Face dataset containing images extracted
from YouTube videos of different people. We use the aligned
version and filter out infrequent labels associated with fewer
than 100 input images in the dataset. This results in 1,283
different labels and around 600K images. We follow prior
work [36] to use the VGG-Face architecture, which consists
of 13 convolution layers and 3 fully connected layers.
• Topic Classification (AG’s News). This task is to classify topic
of input text. The AG’s News dataset consists of news articles
from the AG’s corpus of news articles on the web pertaining
to 4 largest classes. The goal is to recognize sentences in 4
different topics (e.g., world and sports). The dataset contains
120K training samples. The model we used is a Bidirectional
LSTM with 2 layers in each direction. We use GloVe [37]
model for word representation.
• Object Detection (COCO2014). This task trains a model to
detect objects in an image and returns their categories and
spatial locations via bounding boxes. COCO is one of the
most widely used datasets for object detection. It contains
objects in a wide range of scales. COCO’s samples include
diverse objects, with difference sizes and various levels of
occlusion and even visual clutter. We apply our attack on
the popular YOLOv3 [39] detecting framework that adopts a
new backbone network with 76 convolution layers.
• Object Detcction (VOC07+12). This dataset contains the data
from the PASCAL Visual Object Classes Challenges in 2007
and 2012, two well-known object detection competitions.
Each image in the dataset contains a set of objects, out of 20
different classes. We use the common 07+12 combination, i.e.,
Session 1B: Attacking and Defending ML Systems CCS '20, November 9–13, 2020, Virtual Event, USA126Table 9: Tasks and datasets
Task
Dataset
CIFAR-10
Object Recognition (OR)
Traffic Sign Recognition (SR) GTSRB
Face Recognition (FR)
Topic Classification (TC)
Object Detection (OD)
Object Detection (OD)
Object Detection (OD)
YouTube Face
AG’s News
COCO2014
VOC07+12
ILSVRC2015
# of Training
Samples
50,000
35,288
599,967
120,000
117,263
16,551
456,567
# of Labels
Input size Model Architecture
10
43
1,283
4
80
20
200
32x32x3
32x32x3
224x224x3
No Limit
No Limit