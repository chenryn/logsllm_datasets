than the ADNS assigned TTL indicates that the Web site’s record is
in the RDNSd’s cache, suggesting that some real client previously
requested the record. Figure 7 shows the distribution of the num-
ber of popular Web site records that appear to be in the caches of
RDNSdes without FDNSes and in the caches of RDNSdies. Al-
though we later show that RDNSes are prone to inaccurate report-
ing of TTLs, the difference between the two curves indicates a
difference in the behavior of the two sets of RDNSes. We opt to
remove RDNSdes without FDNSes from our analysis since their
purpose is unclear. Instead, we focus the remainder of our study
upon RDNSies which have a clear purpose within the client-side
DNS infrastructure.
5.3 Techniques for Untangling Behavior
We now discuss techniques we developed to tease apart the be-
 without FDNSes
d
0
0
10
Number of Alexa Top 100 hostnames in RDNS’s cache
1
2
5
6
3
4
di
7
8
9
1
0.8
0.6
0.4
0.2
F
D
C
RDNS
RDNS
Figure 7: Number of the Alexa top 100 Web sites in the caches
of RDNSes.
achieving a higher RDNS discovery rate than Random IP. Sec-
ond, because the vast majority of /24 blocks do not contain any
ODNSes, much of the scanning in Aborted Random Block and
Random Block is wasted. We note that Aborted Random Block
was unable to discover 13K of the 43.9K RDNSes within the S3
dataset. The undiscovered RDNSes were not reachable through the
ﬁrst ODNS found within each /24 block. The “Scan on First Hit”
technique provides the best ODNS and RDNS discovery rate in
terms of scanning infrastructure cost and time.
5.2.1 RDNSd Evaluation
The RDNSdes deserve special attention here. These servers have
been counted during both ODNS and RDNS discovery. Yet, it is
unclear if all RDNSdes serve clients. They could, for instance, be
misconﬁgured authoritative DNS servers that happen to be willing
to accept external queries for external domains as discussed in [10].
We ﬁnd that 51% of the RDNSdes in the S2 scan are used by at
least one FDNS, i.e., are in fact RDNSdies. The remaining 49%
could be resolvers which might be accessed by origins directly, or
whose client FDNSes are hidden from our scans’ view or have been
missed by our scans.
To determine if the 49% (17K) of RDNSdes which are not used
by any FDNSes in our dataset from the S2 scan are actually acting
as resolvers for some client population we query them for the top
100 Web sites as listed by Alexa [3]. If the RDNSdes are resolvers,
then they are likely handling DNS requests from their clients for
some of these Web sites. Therefore, some of these popular host-
names should be in the RDNSdes’ caches. We detect if a record
is in the cache by sending a DNS request for the hostname to the
havior of FDNS from RDNS.
5.3.1 Measuring FDNS
As we previously note, the vast majority of ODNSes we dis-
cover are in fact FDNSes (over 95% across all datasets). Gaining
an understanding of FDNSes in isolation from the remainder of
the client-side DNS infrastructure is a challenge. Fortunately, we
ﬁnd that a fraction of FDNSes allow a primitive form of cache in-
jection which we leverage to gain insight into the behavior of this
FDNS subset. Speciﬁcally, these FDNSes do not perform any of
the following security checks on DNS responses which could pre-
vent cache injection: (i) change and/or verify the transaction ID,
(ii) verify the source IP address, and (iii) verify the destination
port number. The absence of such checks makes it straightforward
to follow a request to an FDNS with an acceptable response which
only involves the FDNS and not the rest of the infrastructure.
Hence, we can study these FDNSes in isolation from HDNSes
and RDNSes using the following procedure. We begin by sending
a DNS request for a hostname within our domain to the FDNS un-
der study and then immediately issue a DNS response for the same
hostname to the FDNS that binds the requested name to IP address
X. On the other hand, when the request arrives to our ADNS in
a normal way, the latter answers with a response containing IP ad-
dress Y . Then, any subsequent requests made by our probing host
that are responded to with IP address X must have come from the
FDNS’s cache and the FDNS is effectively isolated from the rest of
the client-side infrastructure, which never touched record X. We
stress that the FDNSes we study may be a biased set since the set
only includes FDNSes that exhibit the cache injection vulnerability.
5.3.2 Measuring RDNS
Since typically we cannot query RDNSies directly, we utilize
FDNSes as our window into RDNSi behavior. This, however,
poses a problem as FDNSes may alter DNS requests, responses
and caching phenomena, hence obscuring RDNS behavior. A sec-
ond response for a domain name through a single FDNS may be
returned from either the FDNS’s cache or the RDNSi’s cache, am-
biguously. Fortunately, it is common to ﬁnd multiple FDNSes
which use the same RDNSi. We leverage a general experimen-
tal strategy which requires at least two FDNSes per RDNSi to
succeed—F1 and F2. While the exact details of the technique vary
with different experiments and will be detailed separately, the gen-
eral framework is as follows. We begin by requesting a unique
subdomain of our domain from F1. Our ADNS responds to this
query with a randomly generated record, which should be cached
F
D
C
1
0.8
0.6
0.4
0.2
0
1
10
1K
Number of FDNSes per RDNS
100
10K
F
D
C
1
0.8
0.6
0.4
0.2
0
1
10
RDNS pool size per FDNS
100
1K
Figure 8: Number of FDNSes per RDNSi in the S3 scan.
Figure 9: Distribution of the RDNS pool size for each FDNS.
at the RDNSi on the return path to F1. Then, after a predetermined
amount of time, we query for the same subdomain through F2. If
the RDNSi still has the record in its cache, the response from F2
will match the response from F1. In this case, we further know that
the record is from the RDNSi’s cache and not from F2 because the
request was previously unseen by F2. If the record is no longer in
the RDNSi’s cache, the request will arrive at our ADNS, which will
respond with a different record. In this way, we eliminate FDNS
caching behavior when studying RDNSi caching behavior.
This technique relies upon discovering two FDNSes which use
the same RDNSi at roughly the same time. Figure 8 shows the
number of FDNSes per RDNSi in the S3 dataset. Over 80% of the
RDNSies are used by more than one FDNS and the coordinated
probing technique has a chance of succeeding. Further, 50% of
RDNSies appear with at least 10 FDNSes in the dataset, vastly
increasing the chances of successful measurement.
FDNS behavior may still distort the measurement by altering
records. We discuss ways to mitigate this problem—such as us-
ing all available FDNSes— in §7.
6. TOPOLOGY
In this section, we address the size and structure of the client-side
DNS infrastructure.
6.1 Estimating Global ODNS Population
Extrapolating from our limited scans of IP space, we estimate
that there are approximately 32M ODNSes on the Internet today.
We arrive at this result from two independent scans. First, we ﬁnd
almost 2M ODNS within a set of 254.7M probed IPs in the “Ran-
dom IP” scan S2 where addresses are chosen randomly from the
complete 232 address space. Therefore, we estimate the population
size as 2M/254.7M × 232 = 33M ODNSes across the Internet.
Second, from the “Random /24” scan S3, the fraction of productive
/24 address blocks (those with at least one ODNS) is 0.141 and a
productive block contains on average 13 ODNSes. Therefore, the
ODNS population across the entire Internet is 0.141 × 13 × 224 =
31M (a similar number is also obtained from simulations in Fig-
ure 3).
These estimates signiﬁcantly exceed previous results of com-
plete Internet scans [13] and estimates [22], which show around
15M responding DNS resolvers. One of our partial scans using the
“Scan on First Hit” strategy directly identiﬁes 17.6M ODNSes—
more than found in previous full scans. Additionally, [1], a com-
plete Internet scan, reports 33M open resolvers as of May 2013
which agrees with our estimate. The results show the population of
ODNSes on the Internet has increased since previous studies.
6.2 RDNS Pool Sizes
The ODNSes we ﬁnd in both our S2 scan and S3 scan uti-
lize RDNSes in roughly 99% of the cases—i.e., they are in fact
FDNSes. Moreover, approximately 70% of FDNSes use an RDNS
pool in both scans. Per §5.2, we use repeated DNS requests
and CNAME chaining triggered by per-ODNS discovery of a new
RDNS to identify RDNS pools used by an FDNS. Figure 9 shows
the size distribution of the discovered RDNS pools in the S3 scan.
The plot shows that 10% of FDNSes use RDNS pools consisting
of more than 10 servers. Also, note that these pools can encom-
pass multiple providers, e.g., an ISP’s own DNS infrastructure and
OpenDNS, which could occur when either the FDNS is conﬁgured
to use both, e.g., one as the primary DNS server and the other as the
secondary DNS server, or the ISP is utilizing some alternate DNS
infrastructure for some queries.
6.3 Distance between FDNSes and RDNSes
settings.
As discussed in §5.1.1, ODNSes—and consequently FDNSes—
are predominantly in residential
Many Internet
platforms—notably content delivery networks—rely on the as-
sumption that client machines are close to their RDNS in that the
platforms direct the client to a nearby node within the platform
based on the location of the client’s RDNS. To test this assump-
tion, we use a geolocation database [16] to calculate the distance
between FDNSes and the RDNSes they employ for resolution. We
perform this on the S6 dataset. Figure 10 presents the distribu-
tion of these distances4. Since an FDNS may use several RDNSes,
we plot the minimum, maximum, and mean distance between an
FDNS and all of the RDNSes it utilizes. We ﬁnd that FDNSes are
often quite close to their RDNSes with the median being approxi-
mately 100 miles. However, one in ten FDNSes appear to be over
8K miles from at least one of their RDNSes; these FDNSes expe-
rience a high cache miss cost and potentially incorrect redirections
by content delivery networks to nodes that are not nearby. Prior
studies [4, 11] also consider the geographical distance between the
client hosts and their DNS resolvers and report different results.
In particular, [11] observes shorter distances between FDNSes and
RDNSes, with 60% of clients to be within 17 miles from their re-
solvers, while [4] reports fewer client-resolver pairs at the low dis-
tance range, with only 25% of pairs being within 500 miles from
each other, and also fewer extremely distant pairs, with just 5% of
pairs more than 2000 miles apart. The differences with our experi-
ment could be due to different vantage points. Both [11] and [4]
4The accuracy of our results rests upon the accuracy of the Max-
Mind geolocation database which varies by country. For example,
MaxMind claims 81% accuracy within 25 miles for IP addresses
inside the USA. See [16] for accuracy in other countries.
F
D
C
1
0.8
0.6
0.4
0.2
0
1
Min
Mean
Max
10
100
1K
10K
Distance from FDNS to RDNS pool (miles)
Figure 10: Distribution of the geographic distance from FDNS
to RDNS.
F
D
C
1
0.8
0.6
0.4
0.2
0
1
Min
Mean
Max
10
100
1K
10K
RTT from FDNS to RDNS pool (milliseconds)
Figure 11: Distribution of round trip time between FDNS and
RDNS.
instrumented a Web page and passively observed FDNS/RDNS
pairs via DNS requests and subsequent HTTP connections, where
as our study is an active scan which exhaustively explores all
FDNS/RDNS pairs. Therefore, we discover FDNS/RDNS pairs
that may not appear in prior studies.
Additionally, we found that some FDNSes respond to ICMP
pings. In the S6 scan, we obtained (1) the round trip time from our
measurement point to the 22% of the FDNSes that were responsive
to a ping and also (2) the round trip time between our measure-
ment point and the respective RDNS through the FDNS for 6.3M
FDNS/RDNS pairs. The later leverages our coordinated probing
technique. We add a record to the RDNS’ cache via a probe from
the ﬁrst FDNS. We then obtain the distance from our measurement
point to the RDNS through the second FDNS by querying for the
same record through the second FDNS. The difference between (2)
and (1) is the round trip time in milliseconds between the second
FDNS and the RDNS. We also repeat the process by swapping the
roles of the two FDNS servers. We perform this measurement for
each FDNS pair we discover using the same RDNS during dis-
covery. Using this technique, we are able to obtain the round trip
time from FDNS to RDNS for 5.6M FDNS/RDNS pairs across
1.3M unique FDNSes. In the case of multiple measurements per
FDNS/RDNS pair, we choose the minimum delay value as it most
accurately reﬂects the actual network delay between the FDNS and
RDNS. We plot the results in Figure 11. The median round trip
time is about 10 ms, however nearly 20% of the FDNSes experi-
ence delays in excess of 200 ms to at least one of their RDNSes.
7. CACHING BEHAVIOR
Caching aids the scalability of the DNS system and hides delay
for hostname resolution. Additionally, DNS caching has important
performance and security implications.
In terms of performance, DNS caching complicates Internet
sites’ trafﬁc engineering ability because a single hostname-to-
address binding may pin all clients behind a given resolver to the
selected server for an extended period of time. Not only does this
handicap sites’ control over client request distribution but it also
complicates the removal of unneeded infrastructure without risking
failed interactions from clients using old bindings. DNS nominally
provides sites with the capability to bound these effects by speci-
fying a time-to-live (TTL) value within DNS responses to limit the
amount of time recipients can reuse the response. However, recipi-
ents are known to disobey TTL [6, 17, 21]. With regard to security,
caching determines the lingering effect of a successful record in-
jection (e.g., using the Kaminsky attack [12]).
The extent of these phenomena depends on two inter-related as-
pects: how long a resolver keeps a record in its cache and how the
resolver treats the TTL. We ﬁrst explore the aggregate behavior of
all components of the client-side DNS infrastructure, as this will
be the view of the clients leveraging the infrastructure. We then
use the measurement techniques described above to tease apart the
behavior of the components in isolation to gain insight into which
components are responsible for various behavior.
The results in this section come from the S6 scan (see Table 1).
The scan covers 79M IP addresses with 1.3B DNS requests from
2/26/2013 through 3/28/2013. The S6 scan encompasses 11M
ODNSes and 65.8K RDNSes—46K of which are RDNSies. The
S6 dataset uses the “Scan on First Hit” methodology to increase the
ODNS discovery rate and thus the probability of ﬁnding multiple
FDNSes which use the same RDNSi at roughly the same time. This
assists with the coordinated probing strategy sketched in §5.3.2.
7.1 Aggregate Behavior
To investigate aggregate behavior of the DNS resolution infras-
tructure, we perform in-depth probing of 2.4M FDNSes during the
S6 scan. We did not test all of the FDNSes because of limitations
in the amount of trafﬁc our ADNS can handle. Therefore, we limit
the number of FDNSes which can be concurrently assessed to 25K
per measurement origin (PlanetLab node). When a measurement
origin ﬁnds a new FDNS we skip in-depth measurement if 25K
FDNSes are presently being assessed.
We examine how FDNSes and RDNSies behave when presented
with DNS records with different TTL values: 1, 10, 30, 60, 100,
120, 1,000, 3,600, 10,000, 10,800, 86,400, 100,000, 604,800 and
1,000,000 seconds. For each TTL value, we re-probe at various
intervals to determine whether the records are still available. We
use intervals slightly below the TTL values (by two seconds), to
check if the record is retained for the full TTL. We also use in-
tervals slightly above the TTL values (by two seconds), to check
if the record is retained longer than the TTL. For this experiment,
our ADNS returns a random IP address. Thus, if subsequent DNS
requests for the same hostname return the same IP address we
know—with high likelihood—the request was satisﬁed by a cache
somewhere within the resolving infrastructure, either at the FDNS,
HDNSes, or RDNSi.
Our ﬁrst observation is that some of the responses arrive at the
client with incorrect TTL values, i.e., different from those set by
our ADNS. Furthermore, even when the client receives the correct
TTL for the initial request for a hostname, we ﬁnd subsequent re-