1 − α
t=1
t=1
Since the number of pages visited is the number of steps of
the random walk plus one initial page from the teleportation
distribution, this is 1/(1 − α) pages in total.
Sampling from the PageRank distribution for a speciﬁc
ﬁle type is easily accomplished as long as the ﬁle type is
sufﬁciently common. We can sample from sτ for a ﬁle type τ
by using the previous algorithm to sample from the PageRank
9
(cid:32) ∞(cid:88)
(cid:32) ∞(cid:88)
(cid:32) ∞(cid:88)
t=0
t=0
=
= s(cid:48)(cid:62)
t=1
∞(cid:88)
(cid:33)
(cid:33)
(cid:33)
∞(cid:88)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
Parameters:
α: PageRank random jump probability
Fmt: File format to extract
Return:
Downloaded files
random_walk(α,Fmt) {
N ← N + 1
N ← 0
while (rand(0,1) < α):
U ← get_random_url()
cnt ← 0
while (cnt < N):
U ← get_random_outgoing_link(U )
if (U is empty) then:
cnt ← cnt + 1
U ← get_random_url()
return fetch_file_from_url(U ,Fmt)
}
Fig. 3: Random walk algorithm
distribution s repeatedly until we draw a URL for a ﬁle of
type τ.
Theorem 12: The above sampling algorithm produces ﬁles
from the PageRank distribution over ﬁles of type τ.
Proof: Notice, the probability of this algorithm returning
the ith URL is 0 if it is not a ﬁle of type τ and otherwise is
proportional to s(i). Since the algorithm induces a distribution
over ﬁles, the probability that the ith URL is output must
be precisely s(i)/(cid:107)s|τ(cid:107)1, which we see is the PageRank
distribution over ﬁles of type τ.
C. Stability of PageRank
Our PageRank distribution is partially computed using a
random walk on the actual web. The web is not static, however:
its link structure is constantly changing over time. The utility
of our PageRank distribution as a common benchmark for
generating reproducible results would be limited if it were
too sensitive to these changes in the web’s structure. Fortu-
nately, we note that it is quite insensitive to these changes.
Langville and Meyer provide a nice review of the stability
of PageRank [32, Chapter 6]—very roughly, the effect of a
change to the links on a page is proportional to that page’s
PageRank, which is small for most pages. Indeed, PageRank
can be viewed as implicitly being “regularized” [39] and hence
very stable to changes in either the web’s link structure or the
choice of teleportation vector.
V. DESIGN AND IMPLEMENTATION
We next discuss our implementation of Fortuna to estimate
Type I errors by sampling from the PageRank distribution
using the the efﬁcient algorithm of Theorem 11. Fortuna is
implemented in approximately 700 lines of Python and it uses
Common Crawl [3], a regularly updated snapshot of the public
Internet, as the PageRank sampling source.
A. Sampling from PageRank
Common Crawl is a public repository, hosted on Amazon’s
Elastic Cloud, that builds and maintains a snapshot of the web.
To avoid the cost of building an index from scratch (in the
range of $10000 with current Amazon pricing 2), Fortuna uses
2http://aws.amazon.com/s3/pricing
an existing copy of the Common Crawl URL Index provided
by triv.io [4]. The URL Index is implemented as a preﬁxed
B-tree that provides the ability to efﬁciently search the index
by URL, URL preﬁx, subdomain or top-level domain. The
currently available index is approximately 220 GB in size,
representing approximately 2.3x109 URLs.
Fortuna uses the Common Crawl URL index to approxi-
mate an uniform distribution of the entire web (i.e., telepor-
tation distribution v in Section IV-B). As described in Sec-
tion IV-B, Fortuna samples random pages from the URL index
as the starting page for a random walk (i.e, sample random
pages from teleportation distribution v in Section IV-B).
B. Random Walk Algorithm
Figure 3 presents Fortuna’s random walk algorithm. The
algorithm takes as input the PageRank parameter α, and the
desired ﬁle format Fmt. It ﬁrst computes the number of
required walk steps, N, based on a geometric distribution with
parameter α at lines 8-10 in Figure 3 (see Section IV-B). It
simply ﬂips a biased coin (with heads probability (1 − α))
until heads comes up.
The algorithm extracts a random URL from the URL Index
(get_random_url() at line 11) to seed the random walk. Next,
the algorithm downloads and parses the URL to extract all html
links links. To simulate the random walk, a link U is randomly
selected from links , and the walk count cnt is incremented. If
the URL does not contain any link, the algorithm will teleport
to another random URL from the URL Index (lines 15-16). The
process is repeated using U as the seed URL and continues
until the algorithm has performed enough steps (i.e., cnt < N).
Once enough steps have been performed, the algorithm parses
the last URL U and examines the HTML for links pointing to
ﬁle of format Fmt (line 18). If U contains more than one ﬁle
of format Fmt, one is randomly selected.
Note that
this algorithm can be parallelized easily. To
efﬁciently download ﬁles Fortuna uses a distributed crawler
infrastructure where workers (on multiple machines) run the
random walk algorithm and store results to a centralized
database.
VI. EXPERIMENTAL RESULTS
We
evaluate Fortuna on three
anomaly detectors:
SOAP [37], SIFT [38], and JSAND [18] on three input
formats: JPEG and PNG (SOAP and SIFT) and JavaScript
(JSAND).
1) SOAP [37] is an input rectiﬁcation system that learns
input constraints over a set of benign inputs and then
enforces the learned constraints over the incoming inputs.
In our experiments for SOAP, we count a false positive
as occurring if SOAP rectiﬁes a (benign) collected input.
2) SIFT [38] statically analyzes an application and generates
sound input constraints, so that any input that satisﬁes
these constraints will not trigger integer overﬂow errors at
memory allocation and block copy sites of the application.
3) JSAND [18] is a publicly available anomaly detector for
JavaScript. It has a web interface [9] where users can
submit JavaScript programs. The system will generate
a report for each submitted program and classify the
program as normal, suspicious, or malicious.
10
A. Methodology
Collect Sample Inputs: Fortuna uses its sampling algorithm
to collect JPEG, PNG, and JavaScript ﬁles from the Internet.
Using this sampling algorithm, Fortuna collected 42299 JPEG
ﬁles, 64089 PNG ﬁles, and 8853 JavaScript ﬁles in less than
10 hours.
Set up Anomaly Detectors: In our experiments, we applied
SIFT to dillo [5] and png2swf [8] for PNG ﬁles, as well as
jpeg2swf [8] for JPEG ﬁles. We applied SOAP to dillo [5]
for PNG ﬁles, as well as ImageMagick [7] for JPEG ﬁles. We
selected these applications because they were the benchmark
applications in the original papers of SOAP [37] and SIFT [38].
For the SIFT experiments, we ran the SIFT static analysis
on the source code of the applications to obtain our input
constraints. For the SOAP experiments, we randomly selected
5130 PNG ﬁles and 3386 JPEG ﬁles from the collected ﬁles
as training examples to generate our input constraints.
Test Anomaly Detectors: We tested the constraints that SOAP
and SIFT generated on the collected JPEG and PNG ﬁles. We
excluded the ﬁles we selected for training examples in the
SOAP experiments. We also tested JSAND on the collected
JavaScript ﬁles. We report the false positive rate bounds that
Fortuna computed for these anomaly detectors.
Note that the distribution sampled by Fortuna differs in
practice from the ideal PageRank distribution in two ways.
First, we do not visit some pages indicated by /robots.txt
ﬁles. And second, when a website requires a login (e.g., a
social network or a paywall) our “random surfer” cannot follow
the link. These are unavoidable, inherent limitations of any
automated system for collecting inputs from the web.
B. Results
Table I summarizes our experimental results. There is a row
in the table for each combination of anomaly detection system
and application (SIFT) or anomaly detection system and input
ﬁle format (SOAP and JSAND). The ﬁrst column lists the
anomaly detection system we are testing, which is either SIFT,
SOAP, or JSAND. The second column indicates the input
format, which is either JPEG, PNG, or JavaScript. The third
column indicates the application utilizing the inputs (thus, the
target of the analysis) in each SOAP or SIFT experiment. The
fourth column indicates the number of training example inputs
used in each SOAP experiment. The ﬁfth column presents the
number of collected inputs used for testing in each experiment.
The sixth column presents the number of false positives we
encountered in each experiment.
The seventh column presents the corresponding theoretical
bounds on the false positive rate for the anomaly detection
system in each experiment. Each bound is either of the form
“err(1) < X%” (one-sided) or of the form “X% ≤ err(1) ≤
Y %” (two-sided). For the systems with zero observed false
positives – SIFT and JSAND with conservative ﬁltering (i.e.
JavaScript that triggers strong warnings is rejected) – we use
Theorem 5, obtaining a bound of the form, “the Type I error
is < X%.” For the systems for which false positives were
observed – SOAP and JSAND with aggressive ﬁltering (i.e.
JavaScript that triggers any warning is rejected) – we use
Hoeffding’s bound (Theorem 3) to obtain a bound of the form,
“the Type I error is between X% and Y%.” In all cases, we
used a conﬁdence bound of 99% (δ = 1%).
Note that SOAP has a higher false positive rate that SIFT
or JSAND. A higher false positive rate is acceptable for
SIFT because it does not discard false positives. It instead
applies input rectiﬁcation, which changes the input ﬁle but
still presents the rectiﬁed input ﬁle to the application.
C. Comparison to Ad Hoc Methods
To emphasize the importance of using PageRank in place
of an ad hoc crawling method, we also collected data via the
web crawling method used to originally gather test data for the
SOAP anomaly detector [37]. The method initiated a breadth
ﬁrst web crawl from a chosen site, downloading all ﬁles in all
descendent sites. This sort of method has been widely used
for collecting data for testing anomaly detectors in the past.
In a separate test from that used for Table I, we trained SOAP
using 7000 JPEG ﬁles before evaluating on PageRank data
and data collected from the ad hoc method. The later approach
signiﬁcantly underestimated false positive rate in comparison
to testing with PageRank – for ImageMagick we saw a rate of
0.66% in comparison to a rate of 1.15%. Similarly, we trained
SOAP using 3999 PNG ﬁles. Again, testing with the ad hoc
method signiﬁcantly underestimated false positive rate – for
dillo we saw a rate of 0.14% in comparison to 0.41% for
PageRank. Using a standard Pearson’s chi-squared test, these
experiments were determined to be statistically signiﬁcant to
within a p-value of .01.
We conclude that the naive method originally employed
for testing SOAP does not provide an adequate test for false
positive rate. Speciﬁcally, it seems likely that the method does
not collect a wide enough variety of data, thus underestimating
false positive rate. PageRank, on the other hand, ensures wide
coverage of the web.
VII. RELATED WORK
Intrusion Detection: Errors or vulnerabilities in deployed
software are often triggered by atypical inputs that the appli-
cation code does not properly process or reject. Initially intro-
duced by Denning [21], Anomaly Detection techniques [20],
[31], [44], [46], [47], [48], [49], [51] learn over a set of
benign inputs to generate a classiﬁer that can detect potentially
malicious inputs to the system. For example, Kruegel et al. [31]
learn character distributions of benign HTTP trafﬁc to detect
potential malicious HTTP requests. Wang et al. [51] propose
a similar technique that looks at character distributions in
payloads to detect network intrusions. Ntoulas et al. [44]
propose to detect malicious web pages by learning features
like word and anchor text length. Cova et al. [18] statically
analyze benign JavaScript programs to extract features such
as string lengths and character distributions to detect mali-
cious JavaScript programs. SOAP [37] is an automatic input
rectiﬁcation system that learns a set of input constraints from
training inputs and then enforces these learned constraints
by rectifying any input that violates the learned constraints.
Chandola et al. [16] summarizes a good overview on these
anomaly detection systems.
Anomaly detection systems have also been built that rely on
static analysis of application source code [23], [25], [38], [50],
11
System
SIFT
SIFT
SIFT
SOAP
SOAP
File
Format
JPEG
PNG
PNG
JPEG
PNG
JSAND w/ Conservative Filtering
JSAND w/ Aggressive Filtering
JavaScript
JavaScript
Target
Application
# Training
Examples
jpg2swf
dillo
png2swf
ImageMagick
dillo
N/A
N/A
N/A
N/A
N/A
3386
5130
N/A
N/A
# Test
Inputs