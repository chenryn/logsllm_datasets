Average Max
11
0
1
8
0
3
0
3
5
2
0
2
35
6.0
0.0
0.7
5.8
0.0
3.0
0.0
1.9
3.7
2.0
0.0
2.0
25.1
0.0001707
TABLE X: The code coverage associated with the metrics of
coverage accounting and AFL-Sensitive. The highest values
are highlighted in blue.
Program
TortoiseFuzz
Code coverage(%)
AFL-Sensitive
n2
7.80
mw
5.70
ma
6.00
ct
5.60
bb
9.00
n4
7.90
0.20
0.20
bc
7.80
loop
8.50
n8
func
6.30
6.90
objdump
21.50 35.60 37.40 34.70 33.10 25.70 28.90 33.00 33.30 35.00
readelf
0.20
0.20
0.20
strings
4.60 12.20 11.20 10.80
9.90
nm
6.30
5.70
5.50
size
34.20 34.50 34.30 22.20 22.90 23.30 23.40 22.80 24.60 22.90
ﬁle
38.60 37.70 37.70 38.20 38.60 38.90 36.20 38.60 38.60 38.70
gzip
30.10 30.40 30.50 10.00 10.00 10.50 10.00 10.00 10.00 10.00
tiffset
40.70 40.50 38.70 31.30 33.40 33.10 29.40 30.80 33.40 31.60
tiff2pdf
73.60 73.20 73.20 72.80 72.80 69.10 64.40 72.70 73.40 73.40
gif2png
info2cap 41.20 40.90 42.10 40.70 41.00 34.30 35.60 41.30 40.70 39.20
21.00 21.00 21.00 21.00 21.00 21.00 21.00 21.00 21.00 21.00
jhead
0.20
0.20
6.30 10.50
6.10
6.20
0.20
9.50
5.40
0.20
5.40
4.70
0.20
5.20
4.50
5.70
5.60
The comparison to other coverage metrics. In this ex-
periment, we compare our proposed metrics with two other
coverage metrics for input prioritization: AFL-Sensitive [53]
and LEOPARD [15].
AFL-Sensitive [53] presents 7 coverage metrics such as
memory access address (memory-access-aware branch cover-
age) and n-basic block execution path (n-gram branch cov-
erage). We ran our metrics and the 7 coverage metrics of
AFL-Sensitive on the same testing suite and equal amount of
time reported in the paper [53], and we compared them with
regard to the number of discovered vulnerabilities and code
coverage.
Table IX and Table X show the number of discovered
vulnerabilities and the code coverage associated with the
metrics of coverage accounting and AFL-Sensitive. Per AFL-
Sensitive [53], a metric should be included if it achieves the
top of all metrics in the associated found vulnerabilities or
code coverage on a target program. Based on our experiment,
we see that all metrics are necessary for coverage accounting
and AFL-Sensitive. Taking all metrics into account, we ob-
served that coverage accounting reported a few more vulnera-
bilities than AFL-Sensitive. Coverage accounting also slightly
outperformed in code coverage, given that it achieved a higher
coverage in 66.7% (8/12) and a lower coverage in 16.7%
(2/12) of the target programs. The results indicate coverage
12
account performed slightly better than AFL-Sensitive in the
number of discovered vulnerabilities and code coverage with
fewer metrics, and that the metrics of coverage accounting are
more effective than those of AFL-Sensitive.
LEOPARD [15] proposed a function-level coverage ac-
counting scheme for input prioritization. Given a target pro-
gram, it ﬁrst identiﬁes potentially vulnerable functions in the
program, and then it calculates a score for the identiﬁed
functions. The score of a function is deﬁned based on code
complexity properties, such as loop structures and data de-
pendency. Finally, LEOPARD prioritized inputs by the sum
of the score of the potentially vulnerable functions executed
by the target program with each input. On the contrary
of TortoiseFuzz which prioritize inputs by basic block-level
metrics, LEOPARD assesses the priority of inputs based on the
level of functions and in particular, pre-identiﬁed potentially
vulnerable functions.
Since LEOPARD integrates metrics such as code complex-
ity and vulnerable functions internally, we compare the result
of TortoiseFuzz as the integration of code coverage to that
of the corresponding LEOPARD fuzzer. As the LEOPARD
fuzzer or metric implementation is not open-sourced, we
contacted the authors and received from them the identiﬁed
potentially vulnerable functions with computed scores. We
then wrote a fuzzer, per their suggestion of the design,
deployed the computed scores to the fuzzer, and ran the
fuzzer with the LEOPARD metrics. We kept the total amount
of time 140 hours for each experiment, and compared the
number of discovered vulnerabilities between code coverage
and LEOPARD metrics, shown in Table XI.
We observe that TortoiseFuzz found more vulnerabilities
than LEOPARD from 83% (10/12) applications on average
and an equal number of vulnerabilities from the other 2
applications. The p-value is 0.0001707, which demonstrates
statistical signiﬁcance between TortoiseFuzz and LEOPARD
and thus the coverage accounting metrics, which is on basic
block level, perform better than the LEOPARD metrics, which
is on function level, in identifying vulnerabilities from real-
world applications.
TABLE XII: The number of vulnerabilities detected by QSYM
with and without coverage accounting (5 runs).
Vulnerabilities
Program
exiv2
new_exiv2
exiv2_9.17
gpac
liblouis
libming
libtiff
nasm
ngiﬂib
ﬂvmeta
tcpreplay
catdoc
SUM
p-value of the U test
QSYM(+AFL) QSYM+func QSYM+bb QSYM+loop QSYM+CA
Max AVG Max AVG Max AVG Max AVG Max
AVG
15
8.2
7.4
9
3
2.0
11
6.0
3
2.0
3
3.0
1
0.8
2.2
4
5
4.2
2
2.0
1
0.0
2
2.0
59
39.8
0.0119252
8.4
12
3.8
8
1.5
2
8.4
10
2.8
3
3.0
3
0.6
1
2.8
3
5.0
5
2.0
2
0.4
0
2
2.0
51 41.4
11 10.6
8
5.0
1.8
2
7.6
10
2.4
3
3.0
3
0.8
1
3
2.8
4.8
5
2.0
2
0.6
1
2
2.0
51 44.0
10 11.2
9
5.5
1.5
2
7.0
8
2.6
3
3.0
3
0.4
1
3
2.6
5.0
5
2.0
2
0.0
0
2
2.0
48 43.6
13 13.0
8
7.3
1.8
3
9.2
9
3.0
3
3.0
3
0.8
1
4
3.2
5.0
5
2.0
2
0.6
1
2
2.0
54 51.2
0.2477059
0.5245183
0.1387917
E. RQ4. Improving the State-of-the-art with Coverage Ac-
counting.
As an input prioritization mechanism, coverage accounting
is able to cooperate with other types of fuzzing improvement
such as input generation. In this experiment, we study the
question that whether coverage accounting, as an extension
to the state-of-art fuzzer, helps improve the effectiveness in
vulnerability discovery.
Recall that QSYM was best-performed compared fuzzers
in our experiment, and that it found 9 vulnerabilities that are
missed by TortoiseFuzz. Therefore, we selected QSYM and
compared the number of vulnerabilities found by QSYM with
and without coverage metrics.
In this experiment, we compared QSYM to the other four
variations: QSYM with the function metric (QSYM+func),
the basic block metric (QSYM+bb),
the loop metric
(QSYM+loop), and with the full coverage accounting metrics
(QSYM+CA). We ran all tools for 140 hours and repeated
each experiment for 5 times.
Table XII shows the number of vulnerabilities discovered
by each fuzzer. We ﬁnd that all the metrics help to improve
QSYM in vulnerability discovery. In particular, QSYM with
full coverage accounting (QSYM+CA) is able to ﬁnd 28.6%
more vulnerabilities on average, and 22.9% more in the
sum of the best per-program performance. This indicates that
coverage accounting is able to cooperate with the state-of-
the-art fuzzers and signiﬁcantly improve the effectiveness in
vulnerability discovery.
Fig. 6: Paths discovered by TortoiseFuzz from real-world
programs. Each program is compiled with eight settings: func,
bb, loop, aﬂ (without protection), func-, bb-, loop-, aﬂ-(with
all protections of Fuzziﬁcation).
mode and IDA Pro. We got 4 testing binaries from Fuzziﬁ-
cation compiled with all three anti-fuzzing methods. In our
experiment, we ran TortoiseFuzz-Bin for 72 hours aligned
with the setup of Fuzziﬁcation. We selected the initial seeds
based on the suggestion of the author of Fuzziﬁcation. We
also used Fuzziﬁcation’s method to get the numbers of the
path discovered. Additionally, we measured the code coverage
of TortoiseFuzz-Bin and AFL.
Table XIII and Table show the number of discovered
paths and the code coverage of each testing cases after 72
hours of fuzzing process. Based on the tables, we ﬁnd that
AFL decreased much more than all the metrics in coverage
accounting. Additionally, we did the statistics of number of
discovered paths over time, shown in Figure 6. The ﬁgure
indicates that the coverage accounting metrics consistently
performed better than AFL since 4 hours after the experiment
starts, which indicates that coverage accounting is more robust
than AFL over time.
VII. DISCUSSION
A. Coverage Accounting Metrics
F. RQ5: Defending against Anti-fuzzing
Recent work [23, 28] show that current fuzzing schemes
are vulnerable to anti-fuzzing techniques. Fuzziﬁcation [28],
for example, proposes three methods to hinder greybox fuzzers
and hybrid fuzzers. Fuzziﬁcation effectively reduced the num-
ber of discovered paths by 70.3% for AFL and QSYM on
real-world programs.
To test
the robustness of coverage accounting against
Fuzziﬁcation, we implemented TortoiseFuzz-Bin, a version of
TortoiseFuzz to test binary programs based on AFL Qemu
TortoiseFuzz prioritizes inputs by a combination of cover-
age and security impact. The security impact is represented by
the memory operations on three different types of granularity
at function, loop, and instruction level. These are empirical
heuristics inspired by Jia et al. [27]. We see our work as a ﬁrst
step to investigate how to comprehensively account coverage
quantitatively and adopt the quantiﬁcation to coverage-guided
fuzzing. In the future, we plan to study the quantiﬁcation in
a more systematic way. A possible direction is to consider
more heuristics and apply machine learning to recognize the
feasible features for effective fuzzing.
13
0816243240485664720.00.20.40.60.81.0×104readelf0816243240485664720.00.20.40.60.81.01.2×104objdump081624324048566472012345×103nm0816243240485664720.00.20.40.60.81.0×104objcopyTime (hour)Real Pathsfuncfunc-bbbb-looploop-aflafl-TABLE XIII: The number of discovered paths of the Anti-fuzz experiment. Each program is compiled with eight settings: func,
bb, loop, aﬂ (without Fuzziﬁcation protection), func-, bb-, loop-, aﬂ- (with full Fuzzifucation protection). The blue values denote
the highest decrease rate.
Program
nm
objcopy
objdump
readelf
func
4626
9518
11353
8761
func-
3120 (-32.5%)
7942 (-16.5%)
10951 (-3.5%)
6737 (-23.1%)
#Paths discovered
TortoiseFuzz-Bin
bb
5449
9823
10618
8673
bb-
4040 (-25.8%)
7837 (-20.2%)
10508 (-1.0%)
6378 (-26.4%)
loop
5490
9869
11272
9075
loop-
3888 (-29.1%)
7417 (-24.8%)
10448 (-7.3%)
6560 (-27.7%)
aﬂ
5043
10165
11569
9863
AFL-Qemu
aﬂ-
2884 (-42.8%)
6005 (-40.9%)
10688 (-7.6%)
5006 (-49.2%)
TABLE XIV: The code coverage result of the Anti-fuzz experiment. Each program is compiled with eight settings: func, bb,
loop, aﬂ (without Fuzziﬁcation protection), func-, bb-, loop-, aﬂ- (with full Fuzziﬁcation protection). The numbers in parentheses
are the decrease rates cased by Fuzziﬁcation. The blue values denote the highest decrease rate.
Program