然后，看一下`Grafana`目录，有一个数据库文件(`/var/lib/grafana/grafana.db`)时间戳，在创建一个 Grafana `organization`后已经更新了:
![](img/00062.jpeg)
删除 pod 后，ReplicaSet 将启动一个新的 pod，并检查 Grafana `organization`是否存在:
![](img/00063.jpeg)
看起来`sessions`目录已经消失了，`grafana.db`也再次被 Docker 映像重新创建。然后，如果您访问网络控制台，格拉夫纳`organization`也将消失:
![](img/00064.jpeg)
对 Grafana 只使用持久卷怎么样？但是使用带有持久卷的复制集，它不能正确复制(扩展)。因为所有的单元都试图装入同一个永久卷。在大多数情况下，只有第一个 pod 可以挂载持久卷，然后另一个 pod 会尝试挂载，如果不能，它会放弃。如果持久卷只能进行 RWO 操作(一次读写，只有一个 pod 可以写入)，就会出现这种情况。
在下面的例子中，Grafana 使用持久卷挂载`/var/lib/grafana`；然而，它无法扩展，因为谷歌持久磁盘是 RWO:
![](img/00065.jpeg)
即使持久卷具有 RWX 的功能(读/写多，许多单元可以同时装载以进行读和写)，例如 NFS，如果多个单元试图绑定同一个卷，它也不会抱怨。但是，我们仍然需要考虑多个应用实例是否可以使用同一个文件夹/文件。例如，如果它将 Grafana 复制到两个或更多的 pods，它将与多个试图写入同一个`/var/lib/grafana/grafana.db`的 Grafana 实例冲突，然后数据可能被破坏，如下图所示:
![](img/00066.jpeg)
在这种情况下，Grafana 必须使用后端数据库，如 MySQL 或 PostgreSQL，而不是 SQLite3，如下所示。它允许多个 Grafana 实例正确读取/写入 Grafana 元数据:
![](img/00067.jpeg)
因为关系数据库管理系统基本上支持通过网络与多个应用实例连接，因此，这个场景非常适合由多个 Pod 使用。注意，Grafana 支持使用 RDBMS 作为后端元数据存储；但是，并非所有应用都支持 RDBMS。
For the Grafana configuration that uses MySQL/PostgreSQL, please visit the online documentation via:
[http://docs.grafana.org/installation/configuration/#database](http://docs.grafana.org/installation/configuration/#database).
因此，Kubernetes 管理员需要仔细监控应用如何处理卷。并且要明白，在某些使用情况下，仅仅使用持久卷可能没有帮助，因为扩展 pods 时可能会出现问题。
如果多个单元需要访问集中式卷，则考虑使用前面显示的数据库(如果适用)。另一方面，如果多个荚需要一个单独的卷，可以考虑使用 StatefulSet。
# 使用状态集复制带有持久卷的盒
StatefulSet 是在 Kubernetes 1.5 中引入的；它由 pod 和持久卷之间的连接组成。缩放增加或减少的 pod 时，pod 和持久卷会一起创建或删除。
此外，pod 的创建过程是串行的。例如，当请求 Kubernetes 扩展两个附加状态集时，Kubernetes 首先创建**持久卷声明 1** 和 **Pod 1** ，然后创建**持久卷声明 2** 和 **Pod 2** ，但不是同时创建。如果应用在应用引导期间注册到注册表，它会帮助管理员:
![](img/00068.jpeg)
即使一个容器已经死亡，StatefulSet 也会保留容器的位置(容器名称、IP 地址和相关的 Kubernetes 元数据)以及持久卷。然后，它尝试重新创建一个容器，该容器重新分配给同一个容器并装载同一个持久卷。
它有助于保持 pods/持久卷的数量，并且应用使用 Kubernetes 调度程序保持在线:
![](img/00069.jpeg)
具有持久卷的状态集需要动态资源调配和`StorageClass`，因为状态集可以扩展。Kubernetes 需要知道在添加更多 PODS 时如何配置持久卷。
# 持久卷示例
在本章中，介绍了一些持久性卷的示例。根据环境和场景，Kubernetes 管理员需要正确配置 Kubernetes。
以下是一些使用不同角色节点来配置不同类型的持久卷来构建弹性搜索集群的示例。它们将帮助您决定如何配置和管理持久卷。
# 弹性搜索集群场景
Elasticsearch 能够通过使用多个节点来设置集群。从 elastic search 2.4 版本开始，有几个不同的类型，如主节点、数据节点和坐标节点([https://www . elastic . co/guide/en/elastic search/reference/2.4/modules-node . html](https://www.elastic.co/guide/en/elasticsearch/reference/2.4/modules-node.html))。每个节点在集群中都有不同的角色和职责，因此相应的 Kubernetes 配置和持久卷应该与正确的设置保持一致。
下图显示了弹性搜索节点的组件和角色。主节点是群集中唯一管理所有弹性搜索节点注册和配置的节点。它还可以有一个备份节点(符合主节点条件的节点)，可以随时充当主节点:
![](img/00070.jpeg)
数据节点在弹性搜索中保存和操作数据存储。协调节点处理来自其他应用的 HTTP 请求，然后对数据节点进行负载平衡/调度。
# Elasticsearch master node
主节点是群集中唯一的节点。另外，其他节点因为注册需要指向主节点。因此，主节点应该使用 Kubernetes StatefulSet 来分配一个稳定的 DNS 名称，如`es-master-1`。因此，我们必须使用 Kubernetes 服务以无头模式分配 DNS，该模式将 DNS 名称直接分配给 pod IP 地址。
另一方面，如果不需要持久卷，因为主节点不需要持久存储应用的数据。
# 符合弹性搜索主节点条件的节点
符合主节点条件的节点是主节点的备用节点，因此不需要创建另一个`Kubernetes`对象。这意味着缩放分配`es-master-2`、`es-master-3`和`es-master-N`的主状态集就足够了。当主节点没有响应时，在符合主节点条件的节点中有一个主节点选择，选择并提升一个节点作为主节点。
# Elasticsearch data node
弹性搜索数据节点负责存储数据。此外，如果需要更大的数据容量和/或更多的查询请求，我们需要横向扩展。因此，我们可以使用带有持久卷的 StatefulSet 来稳定 pod 和持久卷。另一方面，不需要有域名，因此不需要为弹性搜索数据节点设置 Kubernetes 服务。
# 弹性搜索协调节点
协调节点是弹性搜索中的负载平衡器角色。因此，我们需要横向扩展来处理来自外部源的 HTTP 流量，并且不需要持久化数据。因此，我们可以将 Kubernetes ReplicaSet 与 Kubernetes 服务一起使用，将 HTTP 公开给外部服务。
以下示例显示了在 Kubernetes 创建所有前面的 Elasticsearch 节点时使用的命令:
![](img/00071.jpeg)
此外，以下截图是我们在创建上述实例后获得的结果:
![](img/00072.jpeg)
![](img/00073.jpeg)
在这种情况下，外部服务(Kubernetes 节点:`30020`)是外部应用的入口点。出于测试目的，让我们安装`elasticsearch-head`([https://github.com/mobz/elasticsearch-head](https://github.com/mobz/elasticsearch-head))来可视化集群信息。
连接弹性搜索协调节点安装`elasticsearch-head`插件:
![](img/00074.jpeg)
然后，访问任意 Kubernetes 节点，URL 为`http://:30200/_plugin/head`。以下用户界面包含集群节点信息:
![](img/00075.jpeg)
星形图标表示弹性搜索主节点，三个黑色项目符号是数据节点，白色圆圈项目符号是协调器节点。
在这种配置中，如果一个数据节点关闭，将不会发生服务影响，如以下代码片段所示:
```
//simulate to occur one data node down 
$ kubectl delete pod es-data-0
pod "es-data-0" deleted
```
![](img/00076.jpeg)
几分钟后，新的 Pod 安装了相同的聚氯乙烯，保存了`es-data-0`数据。然后 Elasticsearch 数据节点再次注册到主节点，之后集群健康恢复为绿色(正常)，如下图截图所示:
![](img/00077.jpeg)
由于状态集和持久卷，应用数据不会在`es-data-0`丢失。如果需要更多磁盘空间，请增加数据节点的数量。如果需要支持更多流量，请增加协调器节点的数量。如果需要备份主节点，请增加主节点的数量，以使一些主节点符合条件。
总的来说，StatefulSet 的 Persistent Volume 组合非常强大，可以使应用具有灵活性和可扩展性。
# Kubernetes 资源管理
[第 3 章](03.html#22O7C0-6c8359cae3d4492eb9973d94ec3e4f1e)、*Kubernetes 入门*提到 Kubernetes 有一个调度器，负责管理 Kubernetes 节点，然后决定在哪里部署 pod。当节点有足够的资源，如 CPU 和内存，Kubernetes 管理员可以放心部署应用。但是，一旦达到其资源限制，Kubernetes 调度程序会根据其配置而有所不同。因此，Kubernetes 管理员必须了解如何配置和利用机器资源。
# 资源服务质量
Kubernetes 有**资源 QoS** ( **服务质量**)的概念，帮助管理员根据不同的优先级分配和管理 PODS。根据 pod 的设置，Kubernetes 将每个 pod 分类为:
*   保证 Pod 
*   可爆裂 Pod 
*   最佳努力舱
优先级将是保证>可突发>最佳努力，这意味着如果最佳努力窗格和保证窗格存在于同一节点中，则当其中一个窗格消耗内存并导致节点资源短缺时，将终止其中一个最佳努力窗格以保存保证窗格。
为了配置资源服务质量，您必须在 pod 定义中设置资源请求和/或资源限制。以下示例是 nginx 的资源请求和资源限制的定义:
```
$ cat burstable.yml  
apiVersion: v1 
kind: Pod 
metadata: 
  name: burstable-pod 
spec: 
  containers: 
  - name: nginx 
    image: nginx 
    resources: 
      requests: 
        cpu: 0.1 
        memory: 10Mi 
      limits: 
        cpu: 0.5 
        memory: 300Mi 
```
该示例显示了以下内容:
| **资源定义类型** | **资源名称** | **值** | **表示** |
| `requests` | `cpu` | `0.1` | 至少占 1 个中央处理器核心的 10% |
|  | `memory` | `10Mi` | 至少 10 兆字节的内存 |
| `limits` | `cpu` | `0.5` | 1 个中央处理器核心的最大 50 % |
|  | `memory` | `300Mi` | 最大 300 兆字节内存 |
对于 CPU 资源，任一内核的可接受值表达式(0.1，0.2...1.0、2.0)或毫 CPU(100 米、200 米...1000 米，2000 米)。1000 米相当于 1 芯。例如，如果 Kubernetes 节点具有 2 核 cpu(或 1 核超线程)，则总共有 2.0 核或 2000 毫 CPU，如下所示:
![](img/00078.jpeg)
如果运行 nginx 示例(`requests.cpu: 0.1`)，它至少占用 0.1 个核心，如下图所示:
![](img/00079.jpeg)
只要 CPU 有足够的空间，最多可以占用 0.5 个内核(`limits.cpu: 0.5`)，如下图所示:
![](img/00080.jpeg)
您也可以使用`kubectl describe nodes`命令查看配置，如下所示:
![](img/00081.jpeg)
请注意，在前面的示例中，它显示了一个取决于 Kubernetes 节点规格的百分比；如您所见，该节点有 1 个内核和 600 MB 内存。
另一方面，如果超过内存限制，Kubernetes 调度器会确定此 pod 内存不足，然后会杀死一个 pod ( `OOMKilled`):
```
//Pod is reaching to the memory limit
$ kubectl get pods
NAME            READY     STATUS    RESTARTS   AGE
burstable-pod   1/1       Running   0          10m
//got OOMKilled
$ kubectl get pods
NAME            READY     STATUS      RESTARTS   AGE
burstable-pod   0/1       OOMKilled   0          10m