through the appropriate interface. After ﬁve days, we check whether the link is
stored inside the corresponding scanner’s database. We could do this for Virus-
Total, Scumware, WebInspector, and Zscaler Zulu that oﬀer a database query
WYSISNWIV: What You Scan Is Not What I Visit
333
interface. Then we access these links through Chrome to check whether they are
blocked. Unfortunately, none of them was, indicating that the scanners do not
share data.
Broader Scanning and URL Collection. The most viable solution seems to
be that when URLs are found to contain characters or patterns, which may be
interpreted diﬀerently by a client that the scanner checks all possible variations.
If such patterns are not broadly used by benign websites, then the additional
overhead imposed on the scanner will be relatively small. Our results show that
the gred URL scanner already does something like this for %3F (?) and %23
(#). Another option is that scanners take the URLs actually sent by the browser
to web servers as-is and use them for scanning. However, this option may violate
a user’s privacy, as the URL may contain private information and exposes the
sites the user visits.
7 Related Work
Identifying malicious web sites before the user visits them to block them, take
them down, etc. has been a popular area of research. A score of techniques are
used to identify malicious content, using both dynamic and static analysis tech-
niques. While not being exhaustive, we attempt to discuss some of the most
prominent works here. Note that the security problem highlighted in this paper
does not relate to the techniques and methods used to detect malicious content,
such as malware, exploits, and phishing sites. Instead, it has to do with they
way users and security systems obtain and parse URLs. That is, security issues
arise because an attacker can use a URL to hide his malicious content from a
security system, while the client, usually a browser, reaches malicious content
through the same URL. Some of the works described below do involve the URL
in the classiﬁcation of web pages used to detect malicious content. It is possi-
ble that these approaches could be extended to include heuristics that identify
problematic URL patterns as potentially malicious, however, the eﬀectiveness of
such measures also depends on how frequently such patterns are encountered on
benign sites.
Cova et al. [20] present JSand, a dynamic analysis system that visits web sites
using an instrumented browser, collecting run-time events as the browser exe-
cutes the website. Anomaly detection methods are applied on features extracted
from the events to classify websites and identify malicious ones. JSand is part of
the Wepawet scanner, which we tested in this work, and utilizes Mozilla’s Rhino
interpreter. This is probably the reason it processes backslashes in a Firefox-like
manner. Prophiler [18] later improves JSand by accelerating the process of scan-
ning web pages by allowing for benign pages to be quickly identiﬁed and ﬁltered
out. Features extracted from page content, the URL, and information about the
host of the page are used to quickly identify benign pages. EvilSeed [29] follows
the reverse direction and begins from known malicious websites, which it uses
as seeds to search the web more eﬃciently for malicious content. This is accom-
plished by extracting terms that characterize the known-to-be malicious sites
334
Q. Yang et al.
and using them to query search engines, hence, obtaining results more likely to
be malicious or compromised.
In 2007, Google researchers introduced a system for identifying all malicious
pages on the web that attempt to exploit the browser and lead to drive-by down-
loads [41]. Based on the fact that Google already crawls a big part of the web,
the researchers begun an eﬀort to extract a subset of suspicious pages that can be
more thoroughly scanned. Simple heuristics are used to greatly reduce the num-
ber of pages that need to be checked. In a later paper, Provos et al. [40] present
results showing the prevalence of drive-by download attacks, using features such as
out-of-place inline frames, obfuscated JavaScript, and links to known malware dis-
tribution sites to detect them. Their ﬁndings estimate that 1.3 % of search queries
made to Google returned at least one URL labeled as malicious.
Dynamic analysis techniques that scan the web to identify malicious pages,
frequently employ client honeypots. That is, a modiﬁed collection of programs
that act as a user operating a browser to access a web site. Moshchuk et al. [44]
developed Strider HoneyMonkeys, a set of programs that launch browsers with
diﬀerent patch levels, concurrently accessing the same URL, to detect exploits.
The approach is based on detecting the eﬀects of a compromise, like the creation
of new ﬁles, alteration of conﬁguration ﬁles, etc.
Some recent works that aim to improve the detection of malicious websites
include JStill [47], which performs function invocation-based analysis to extract
features from JavaScript code to statically identify malicious, obfuscated code.
Kapravelos et al. [30] also focused on detecting JavaScript that incorporates tech-
niques to evade analysis. Another approach, Delta [16], relies on static analysis
of the changes between two versions of the same web site to detect malicious
content.
Some other works have focused on aspects of the URL itself to detect mali-
cious sites. ARROW [48] looks at the redirection chains formed by malware
distribution networks during a drive-by download attack. Garera et al. [25] clas-
sify phishing URLs using features that include red-ﬂag keywords in the URL,
as well as feature based on Google’s page rank algorithm. Statistical features
and lexical and host-based features of URLs have been also used in the past to
identify malicious URLs with the help of machine learning [33,34,46]. Malicious
URLs are frequently hidden by using JavaScript to dynamically generate them
on-the-ﬂy. Wang et al. [43] employ dynamic analysis to be extracted such hidden
URLs.
Besides the URL scanners mentioned in this paper, there exist another type
of scanner called Web Application Scanners. The Web Application Scanner is
a kind of scanner that is fed with a URL or a set of URLs, retrieves the pages
that URLs pointed to, follows the links inside until identifying all the reachable
pages in the application (under a speciﬁc domain), analyze the pages with crafted
inputs if necessary, and ﬁgure out whether this site is vulnerable to some web-
speciﬁc vulnerabilities (e.g., Cross-Site Scripting, SQL injection, Code Injection,
Broken Access Controls). Doup´e et al. [21] presents an thorough evaluation of
eleven this kind of web application scanners by constructing a vulnerable web
WYSISNWIV: What You Scan Is Not What I Visit
335
site and feeding this website to scanners. Khoury et al. [31] evaluate three scan-
ners against stored SQL injection. Bau et al. [15] analyze eight web application
scanners and evaluate their eﬀectiveness against vulnerabilities. For this kind
of scanners, they are out of the scope of this paper. In our paper, we assume
that the web site is controlled by the attacker and the attacker can planted any
malicious content into any link belongs to this site while the web application
scanners are targeting the benign sites that may potentially be exploited. The
web application scanners usually are not capable of detecting malicious content
and phishing pages as well.
8 Conclusions
The procedure of developing a common URL parser framework or enforcing a
standardization model can be a hard and challenging task for both application
and service vendors, due to expeditious changes in the technology ﬁeld, and
variations and gaps among multiple web services.
In this work, we experimentally test all major browsers and URL scanners, as
well as various applications that parse URLs. We expose multiple discrepancies
on how they actually parse URLs. These diﬀerences leave users vulnerable to
malicious web content because the same URL leads the browser to one page,
while the scanner follows the same URL to scan another page.
As far as we are aware of, this is the ﬁrst time browsers and URL scanners
have been cross-evaluated in this way. The current work can be used as a refer-
ence to anyone interested in better understanding the facets of this fast evolving
area. It is also expected to foster research eﬀorts to the development of fully-
ﬂedged solutions that put emphasis mostly to the technological, but also to the
standardization aspect.
Acknowledgements. We want to express our thanks to the anonymous reviewers for
their valuable comments. We would also like to acknowledge Paul Spicer’s contribution,
who initially investigated the problem.
References
1. Uniform resource identiﬁer (URI): Generic syntax, January 2005. https://www.
ietf.org/rfc/rfc3986.txt
2. Diﬀerent behaviours of
in the url by FireFox and
Chrome. stackoverﬂow, May 2012. http://stackoverﬂow.com/questions/10438008/
diﬀerent-behaviours-of-treating-backslash-in-the-url-by-ﬁrefox-and-chrome
treating (backslash)
3. gred, March 2015. http://check.gred.jp/
4. Online link scan - scan links for harmful threats! (2015). http://onlinelinkscan.
com/
5. PhishTank — join the ﬁght against phishing (2015). http://www.phishtank.com/
6. scumware.org - just another free alternative for security and malware researchers
(2015). http://www.scumware.org/
336
Q. Yang et al.
7. Stopbadware — a nonproﬁt organization that makes the web safer through the
prevention, mitigation, and remediation of badware websites, May 2015. https://
www.stopbadware.org/
8. Sucuri sitecheck - free website malware scanner, March 2015. https://sitecheck.
sucuri.net/
9. urlquery.net - free url scanner, March 2015. http://urlquery.net/
10. VirusTotal - free online virus, malware and URL scanner (2015). https://www.
virustotal.com/en/
11. Web inspector - inspect, detect, protect (2015). http://app.webinspector.com/
12. Website/url/link scanner safety check for phishing, malware, viruses - scanurl.net,
March 2015. http://scanurl.net/
13. Zscaler zulu url risk analyzer - zulu, March 2015. http://zulu.zscaler.com/
14. Akhawe, D., Felt, A.P.: Alice in warningland: a large-scale ﬁeld study of browser
security warning eﬀectiveness. In: Proceedings of the 22th USENIX Security Sym-
posium, pp. 257–272 (2013)
15. Bau, J., Bursztein, E., Gupta, D., Mitchell, J.: State of the art: automated black-
box web application vulnerability testing. In: 2010 IEEE Symposium on Security
and Privacy (SP), pp. 332–345, May 2010
16. Borgolte, K., Kruegel, C., Vigna, G.: Delta: automatic identiﬁcation of unknown
web-based infection campaigns. In: Proceedings of the ACM SIGSAC Conference
on Computer and Communications Security (CCS), pp. 109–120 (2013)
17. Burns, J.: Cross site request forgery: an introduction to a common web application
weakness. White paper, Information Security Partners, LLC (2007)
18. Canali, D., Cova, M., Vigna, G., Kruegel, C.: Prophiler: a fast ﬁlter for the large-
scale detection of malicious web pages. In: Proceedings of the International Con-
ference on World Wide Web (WWW), pp. 197–206 (2011)
19. Cass, S.: The 2015 top ten programming languages. http://spectrum.ieee.org/
computing/software/the-2015-top-ten-programming-languages
20. Cova, M., Kruegel, C., Vigna, G.: Detection and analysis of drive-by-download
attacks and malicious javaScript code. In: Proceedings of the International Con-
ference on World Wide Web (WWW), pp. 281–290 (2010)
21. Doup´e, A., Cova, M., Vigna, G.: Why Johnny can’t pentest: an analysis of black-
box web vulnerability scanners. In: Kreibich, C., Jahnke, M. (eds.) DIMVA 2010.
LNCS, vol. 6201, pp. 111–131. Springer, Heidelberg (2010)
22. Egele, M., Wurzinger, P., Kruegel, C., Kirda, E.: Defending browsers against drive-
by downloads: mitigating heap-spraying code injection attacks. In: Flegel, U.,
Bruschi, D. (eds.) DIMVA 2009. LNCS, vol. 5587, pp. 88–106. Springer, Heidelberg
(2009)
23. Egelman, S., Cranor, L.F., Hong, J.: You’ve been warned: an empirical study of
the eﬀectiveness of Web browser phishing warnings. In: Proceedings of the SIGCHI
Conference on Human Factors in Computing Systems (CHI), pp. 1065–1074 (2008)
24. FireEye: email security - detect and block spear phishing and other email-
based attacks, May 2015. https://www.ﬁreeye.com/products/ex-email-security-
products.html
25. Garera, S., Provos, N., Chew, M., Rubin, A.D.: A framework for detection and
measurement of phishing attacks. In: Proceedings of the 2007 ACM Workshop on
Recurring Malcode (WORM), pp. 1–8 (2007)
26. Google: safe browsing API - google developers (2015). https://developers.google.
com/safe-browsing/
WYSISNWIV: What You Scan Is Not What I Visit
337
27. Ikinci, A., Holz, T., Freiling, F.: Monkey-spider: detecting malicious websites
with low-interaction honeyclients. In: Proceedings of Sicherheit, Schutz und
Zuverl¨assigkeit (2008)
28. Imperial-Legrand, A.: Vulnerability writeups. Google+, March 2014. https://plus.
google.com/+AlexisImperialLegrandGoogle/posts/EQXTzsBVS7L
29. Invernizzi, L., Benvenuti, S., Cova, M., Comparetti, P.M., Kruegel, C., Vigna, G.:
EvilSeed: a guided approach to ﬁnding malicious web pages. In: Proceedings of the
2012 IEEE Symposium on Security and Privacy, pp. 428–442 (2012)
30. Kapravelos, A., Shoshitaishvili, Y., Cova, M., Kruegel, C., Vigna, G.: Revolver: an
automated approach to the detection of evasive web-based malware. In: Proceed-
ings of the USENIX Security Symposium, pp. 637–652 (2013)
31. Khoury, N., Zavarsky, P., Lindskog, D., Ruhl, R.: An analysis of black-box web
application security scanners against stored SQL injection. In: 2011 IEEE Third
International Conference on Privacy, Security, Risk and Trust (PASSAT) and
2011 IEEE Third Inernational Conference on Social Computing (SocialCom),
pp. 1095–1101, October 2011
32. Kirda, E.: Cross site scripting attacks. In: van Tilborg, H., Jajodia, S. (eds.) Ency-
clopedia of Cryptography and Security, pp. 275–277. Springer, US (2011)
33. Ma, J., Saul, L.K., Savage, S., Voelker, G.M.: Beyond blacklists: learning to detect
malicious web sites from suspicious URLs. In: Proceedings of the International Con-
ference on Knowledge Discovery and Data Mining (KDD), pp. 1245–1254 (2009)
34. Ma, J., Saul, L.K., Savage, S., Voelker, G.M.: Identifying suspicious URLs: an
application of large-scale online learning. In: Proceedings of the International Con-
ference on Machine Learning (ICML), pp. 681–688 (2009)
35. Microsoft: Microsoft security intelligence report, volume 13. Technical report,
Microsoft Corporation (2012)
36. Microsoft:
smartscreen
ﬁlter
(2015).
http://windows.microsoft.com/en-us/
internet-explorer/products/ie-9/features/smartscreen-ﬁlter
37. Moshchuk, A., Bragin, T., Deville, D., Gribble, S.D., Levy, H.M.: Spyproxy:
execution-based detection of malicious web content. In: Proceedings of 16th
USENIX Security Symposium on USENIX Security Symposium, SS 2007, pp. 3:1–
3:16, USENIX Association, Berkeley, CA, USA (2007). http://dl.acm.org/citation.
cfm?id=1362903.1362906
38. proofpoint: targeted attack protection, May 2015. https://www.proofpoint.com/
us/solutions/products/targeted-attack-protection
39. Protalinski, E.: These 8 characters crash Skype, and once they’re in your chat his-
tory, the app can’t start (update: ﬁxed). VentureBeat, May 2012. http://venture
beat.com/2015/06/02/these-8-characters-crash-skype-and-once-theyre-in-your-
chat-history-the-app-cant-start/
40. Provos, N., Mavrommatis, P., Rajab, M.A., Monrose, F.: All your iFRAMEs point
to us. In: Proceedings of the USENIX Security Symposium, pp. 1–15 (2008)
41. Provos, N., McNamee, D., Mavrommatis, P., Wang, K., Modadugu, N.: The ghost
in the browser analysis of web-based malware. In: Proceedings of the Workshop on
Hot Topics in Understanding Botnets (HOTBOTS) (2007)
42. Symantec: Symantec Web Security.cloud (2015). http://www.symantec.com/
web-security-cloud/
43. Wang, Q., Zhou, J., Chen, Y., Zhang, Y., Zhao, J.: Extracting URLs from
JavaScript via program analysis. In: Proceedings of the Joint Meeting on Founda-
tions of Software Engineering (FSE), pp. 627–630 (2013)
338
Q. Yang et al.
44. Wang, Y.M., Beck, D., Jiang, X., Verbowski, C., Chen, S., King, S.: Automated
web patrol with strider HoneyMonkeys: ﬁnding web sites that exploit browser
vulnerabilities. In: Proceedings of NDSS, February 2006
45. WHATWG: URL living standard, May 2015. https://url.spec.whatwg.org/
46. Whittaker, C., Ryner, B., Nazif, M.: Large-scale automatic classiﬁcation of phish-
ing pages. In: Proceedings of NDSS, February 2010
47. Xu, W., Zhang, F., Zhu, S.: JStill: mostly static detection of obfuscated malicious
javascript code. In: Proceedings of the ACM Conference on Data and Application
Security and Privacy (CODASPY), pp. 117–128 (2013)
48. Zhang, J., Seifert, C., Stokes, J.W., Lee, W.: ARROW: generating signatures to
detect drive-by downloads. In: Proceedings of the International Conference on
World Wide Web (WWW), pp. 187–196 (2011)