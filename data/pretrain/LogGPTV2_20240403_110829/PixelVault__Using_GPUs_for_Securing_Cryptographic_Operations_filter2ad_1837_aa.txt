title:PixelVault: Using GPUs for Securing Cryptographic Operations
author:Giorgos Vasiliadis and
Elias Athanasopoulos and
Michalis Polychronakis and
Sotiris Ioannidis
PixelVault: Using GPUs for Securing Cryptographic
Operations
Giorgos Vasiliadis
PI:EMAIL
FORTH-ICS
Michalis Polychronakis
Columbia University
PI:EMAIL
ABSTRACT
Protecting the conﬁdentiality of cryptographic keys in the event
of partial or full system compromise is crucial for containing the
impact of attacks. The Heartbleed vulnerability of April 2014,
which allowed the remote leakage of secret keys from HTTPS web
servers, is an indicative example. In this paper we present Pixel-
Vault, a system for keeping cryptographic keys and carrying out
cryptographic operations exclusively on the GPU, which allows it
to protect secret keys from leakage even in the event of full system
compromise. This is possible by exposing secret keys only in GPU
registers, keeping PixelVault’s critical code in the GPU instruction
cache, and preventing any access to both of them from the host.
Due to the non-preemptive execution mode of the GPU, an adver-
sary that has full control of the host cannot tamper with PixelVault’s
GPU code, but only terminate it, in which case all sensitive data
is lost. We have implemented a PixelVault-enabled version of the
OpenSSL library that allows the protection of existing applications
with minimal modiﬁcations. Based on the results of our evaluation,
PixelVault not only provides secure key storage using commodity
hardware, but also signiﬁcantly speeds up the processing through-
put of cryptographic operations for server applications.
Categories and Subject Descriptors
E.3 [Data]: DATA ENCRYPTION; D.4.6 [OPERATING SYS-
TEMS]: Security and Protection
Keywords
GPU; SSL/TLS; trusted execution; isolation; tamper resistance
1.
INTRODUCTION
Servers have always been an attractive target for attackers, es-
pecially when they host popular web sites and online services, as
they typically contain a wealth of private user data and other sensi-
tive information. Encryption can be used as an additional layer of
protection for sensitive data, once a service has been compromised,
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’14, November 3–7, 2014, Scottsdale, Arizona, USA.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2957-6/14/11 ...$15.00.
http://dx.doi.org/10.1145/2660267.2660316 .
Elias Athanasopoulos
PI:EMAIL
FORTH-ICS
Sotiris Ioannidis
PI:EMAIL
FORTH-ICS
but it is only effective as long as the keys involved in cryptographic
operations are kept secret. In fact, keys themselves are often the
target, as for example is the case with the infamous Heartbleed
bug [9]. The exploitation of Heartbleed, a buffer over-read vul-
nerability in OpenSSL, allows attackers to read arbitrary contents
from a server’s memory, including TLS private keys. Besides at-
tacks that leverage software vulnerabilities to disclose memory or
take complete control of the host, key recovery attacks can also
be mounted using direct memory access through Firewire [48] or
PCI [57]. Moving one step further, it has been demonstrated that
keys can be extracted by freezing memory chips and inspecting
their contents [23].
Once the secret keys are leaked, attackers can impersonate the
server (without triggering any browser warnings), or decrypt any
past and future captured encrypted data (unless perfect forward se-
crecy is used). Defenses that involve the in-memory obfuscation of
keys using dispersal techniques offer only partial protection, as at-
tackers can eventually break the obfuscation scheme with adequate
effort. To that end, it is crucial that, apart from the trusted oper-
ation of the underlying cryptographic implementation, secret keys
and other sensitive information is safely stored and protected from
leakage. It is important to ensure that a potential security ﬂaw in
a service will not allow an adversary to get access to secret keys,
even if the service is fully compromised, as this can lead to further
catastrophic consequences [9, 10].
In order to address this problem, researchers have proposed sys-
tems that store all sensitive information in CPU registers and never
in main memory [21, 41, 56]. These approaches require a trusted
and bug-free component for ensuring that an adversary cannot com-
promise part of the system and eventually extract the secret keys
from the CPU registers. Unfortunately, however, with complex
services consisting of databases, web servers, and a multitude of
other software components and libraries, guaranteeing the absence
of bugs that may lead to system compromise is rather unrealistic.
For instance, a recent DMA attack against these systems has shown
that secret keys can be extracted from the CPU registers into the tar-
get system’s memory, and be retrieved using a normal DMA trans-
fer [12].
Another possible research direction for solving this problem is
through systems that support trusted computation in hostile operat-
ing systems [16, 26, 29]. These systems are designed as a generic
solution for protecting computation performed by any application,
even by non-sensitive ones, when the host is compromised. In such
a setting, a process responsible for cryptographically signing a mes-
sage would never expose its keys to the operating system, and there-
fore encryption remains functional even when the operating system
is compromised. However, these systems require applications to
run on top of a hypervisor [16, 26], introducing signiﬁcant perfor-
mance overhead, or the addition of extra hardware abstraction lay-
ers and the re-compilation of the operating system [29]. Trusted
Platform Modules (TPMs), on the other hand, do not provide use-
ful support for the use case we consider. From a security stand-
point, TPMs provide limited cryptographic support (current imple-
mentations support only RSA, SHA1, and HMAC) [8], while from
a performance standpoint, their limited computational capabilities
make them inappropriate for carrying out intensive and continuous
cryptographic operations, such as handling a server’s TLS connec-
tions [1].
In this paper, we explore an alternative approach to the problem
of protecting a server’s cryptographic keys, which takes advantage
of the graphics card to exclusively i) store cryptographic keys and
other sensitive information, and ii) carry out all cryptographic op-
erations, without involving the CPU. Our prototype system, named
PixelVault, provides native GPU implementations of the AES [19]
and RSA [51] algorithms, and prevents key leakage even when the
base system is fully compromised. This is possible by exposing
private keys only in GPU registers, and keeping PixelVault’s criti-
cal code exclusively in the GPU instruction cache, preventing this
way even privileged host code from accessing any sensitive code
or data. We have implemented a PixelVault-enabled version of the
OpenSSL library, which allows the transparent protection of exist-
ing services without hardware modiﬁcations or operating system
recompilation. Multiple services can use the same GPU to perform
cryptographic operations, using the same or different certiﬁcates
(and secret keys), while trust is always given to a single hardware
entity—the GPU.
Our choice of the GPU for key storage is justiﬁed by its unique
properties, including (i) non-preemptiveness: all program code run-
ning on the GPU is never context-switched, and therefore, there is
no saved state in the host’s memory that could include information
associated with cryptographic keys; (ii) on-chip memory operation
only: the running GPU code is tamper-resistant in on-chip mem-
ory, and the associated cryptographic keys are never stored in ob-
servable memory, but only in non-addressable memory, such as the
registers of the GPU; (iii) transparency: the GPU is independent
from the host, so no hardware, operating system, or application
changes are required—just a modiﬁcation of the standard crypto-
graphic libraries used, such as OpenSSL, which essentially implies
that legacy applications can fully take advantage of our system with
minimal effort; (iv) commodity component: GPUs are commod-
ity components and are cheaper than dedicated cryptographic hard-
ware; (v) performance: GPUs achieve high computational perfor-
mance for cryptographic operations, for applications in which they
can be parallelized.
The main contributions of our work are the following:
1. We present the design of PixelVault, a system for keeping
cryptographic keys and carrying out cryptographic operations
exclusively on the GPU, which allows it to protect secret keys
from leakage even in case the host is fully compromised.
2. We have implemented PixelVault using commodity GPUs
(NVIDIA’s GTX 480), and provide a PixelVault-enabled ver-
sion of the OpenSSL library.
3. We evaluate our prototype implementation in terms of se-
curity and performance. Our analysis suggests that Pixel-
Vault not only provides better protection, but also outper-
forms CPU-based solutions in terms of processing through-
put for server applications.
Host Memory
CPU
(Host)
y
r
o
m
e
M
l
a
b
o
G
l
GPU
Multiprocessor N
Multiprocessor 2
Multiprocessor 1
Shared
Memory
SP
SP
SP
SP
Reg
Cache
SP
SP
SP
SP
Figure 1: A simpliﬁed view of a typical graphics card memory
hierarchy.
2. BACKGROUND
2.1 GPU Architecture Overview
The computational capabilities of modern graphics processing
units in combination with their low cost makes them suitable for
general-purpose applications beyond graphics rendering [52,55,60,
61]. GPUs contain hundreds of processing cores that can be used
for general-purpose computation, facilitated by feature-rich frame-
works for general purpose computing on GPUs (GPGPU). For our
prototype implementation, we have chosen NVIDIA’s CUDA [42],
probably the most widely used GPGPU framework.
A fundamental difference between CPUs and GPUs is the de-
composition of transistors in the processor. A GPU devotes most
of its die area to a large array of arithmetic logic units (ALUs). In
contrast, most CPU resources serve a large cache hierarchy and a
control plane for the acceleration of a single CPU thread. A GPU
executes code in a data-parallel fashion, so that the same code path
is executed on different data at the same time. The code that the
GPU executes is organized in units called kernels. To exploit par-
allelism, the same kernel is launched by a vast amount of GPU
threads concurrently.
2.2 Memory Hierarchy
The NVIDIA CUDA architecture offers different memory spaces
and types, as illustrated in Figure 1. The host is responsible for
allocating memory for the GPU kernel from the global, constant,
and texture memory spaces of the graphics card. Allocated memory
can be accessed by the host through special functions provided by
the CUDA driver, and is persistent across kernel launches by the
same application. Both constant and texture memory are read-only,
are initialized by the host, and contain separate caches, optimized
for different uses. On devices with compute capability 2.x (and
higher) global memory accesses are also cached in L1–L3 caches.
Each GPU thread maintains its own local memory area, which
actually resides in global memory. Automatic variables declared
inside a kernel are mapped to local memory. In implementations
that do not support a stack, all local memory variables are stored at
ﬁxed addresses. The parameter state space (.param) is used to (i)
pass arguments from the host to the kernel, (ii) declare formal in-
put and return parameters for device functions called during kernel
execution, and (iii) declare locally-scoped byte array variables that
serve as function call arguments, typically for passing large struc-
tures by value to functions. The location of the parameter space
is implementation-speciﬁc. In some implementations, kernel pa-
rameters reside in global memory, hence no access protection is
provided between parameter and global space in this case.
Compute capability (version)
#registers per thread
1.x
2.x
3.0
3.5
128
63
63
255
Table 1: Maximum number of 32-bit registers per GPU thread
for different levels of CUDA support (compute capability).
CUDA Application
NVIDIA Runtime
Gdev Runtime
NVIDIA
Driver
PSCNV
Nouveau
GPU
Figure 2: Structure of CUDA applications on top of the
NVIDIA (closed-source) and Gdev (open-source) sets of
GPGPU runtime and driver software.
The shared memory (comparable with scratchpad RAM in other
architectures) provides very fast access, and is also shared between
the threads that belong to the same block. The size of shared mem-
ory is 64KB per warp, and is also used by the hardware-managed
L1 cache. Typical splits include either 16KB L1 / 48KB shared or
48KB L1 / 16KB shared. Finally, a set of registers (.reg state
space) provides fast storage locations. The number of registers is
limited, and will vary from platform to platform, as shown in Ta-
ble 1. Registers differ from the other state spaces in that they are
not fully addressable, i.e., it is not possible to refer to the address
of a register. When the limit is exceeded, register variables will be
spilled to global memory, causing changes in performance [47].
2.3 GPU Code Execution
A typical GPU kernel execution consists of the following four
steps: (i) the DMA controller transfers input data from host mem-
ory to GPU memory; (ii) a host program instructs the GPU to
launch the kernel; (iii) the GPU executes threads in parallel; and
(iv) the DMA controller transfers the resulting data from device
memory back to host memory. All these operations are performed
by the CPU using architecture-speciﬁc commands.
Although the architectural details of GPUs are not publicly avail-
able, there is an ongoing research that tries to unveil how these op-
erations are performed and to provide an in-depth understanding of
their runtime mechanisms [32,38]. Speciﬁcally, the GPU exposes a
memory-mapped region to the OS, which is the main control space
of the GPU, and is used to send commands. For example, to copy
data from host to device memory, a set of commands are sent to the
GPU that specify the source and the destination virtual addresses,
along with the mode of direct memory access (DMA). Similarly,
when a kernel is launched, another set of commands is composed
and sent to the GPU, specifying code and stack information.
CUDA applications can run either on top of the closed-source
NVIDIA CUDA runtime, or on top of the open-source Gdev run-
time [7]. The NVIDIA CUDA runtime relies on the closed-source
kernel-space NVIDIA driver and a closed-source user-space library.
Gdev also supports the NVIDIA driver, as well as the open source
Nouveau [3] and PSCNV [6] drivers. Figure 2 illustrates the soft-
ware stack of the CUDA and Gdev frameworks. Both frameworks