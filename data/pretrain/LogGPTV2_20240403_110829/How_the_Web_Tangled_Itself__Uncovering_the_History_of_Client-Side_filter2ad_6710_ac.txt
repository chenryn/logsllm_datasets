nel, sites may use postMessages to communicate be-
tween documents of differing origins. The API gives
guarantees on the authenticity and conﬁdentiality of a
message — a receiver can verify the origin of the sender,
and the sender may speciﬁcally target an origin ensuring
that the message is not accessible by any other origin. In
practice, however, these checks are often omitted [31].
We therefore analyzed our data set in two dimensions:
handling of received postMessages without performing
origin checks and calls to the postMessage API with a
wildcard origin.
Given the large amount of data we collected, i.e.,
8,992 distinct scripts, we opted to analyze the postMes-
sage receivers in a light-weight fashion. To that end,
whenever our static analysis discovered that a postMes-
sage receiver was registered, we checked the ﬁle for ac-
cess to the origin property of the message. Although it
was shown by Son and Shmatikov [31] that the existence
of an origin check does not preclude a vulnerability, we
present the results as an estimation over our study period.
The results of our analysis are shown in Table 1. When-
USENIX Association
26th USENIX Security Symposium    977
199719981999200020012002200320042005200620072008200920102011201220132014201520160%2%4%6%8%10%12%14%domainsvulnerableFigure 8: Crossdomain.xml ﬁles and wildcards
Figure 9: jQuery usage and vulnerability statistics
to gain access (*.domain.com). However, this wildcard
can also be used to whitelist any site (*) or any preﬁx
(prefix*), e.g., prefix.otherdomain.com for cross-
domain access.
In part, these policy ﬁles are also stored by the Web
Archive. However, we discovered a number of cases
where no policy ﬁle was available 3. Therefore, all re-
sults we present in the following must be considered
lower bounds. This drawback of the archived data is
also clearly visible in the number of sites hosting a
crossdomain.xml ﬁle in 2009, as shown in Figure 8.
We analyzed the crossdomain policy ﬁles for danger-
ous wildcards, i.e., wildcards allowing access to any re-
mote origin. The results are shown in Figure 8 as the
grey line. We ﬁnd that for 2008, about 7% of all domains
had such wildcards and the number decreased afterwards
(along with the general decline in the use of Flash, and
hence crossdomain.xml ﬁles). The existence of a wild-
card does not necessarily imply a vulnerability, since ac-
cess might be granted to any domain by a content dis-
tribution network [16]. Hence, we also analyzed which
of the domains with wildcard policies had artefacts of a
login, e.g., login pages or session cookies. The result of
this analysis is also shown in the graph as the red line.
Here, we observe that at most 3% of all domains should
be considered vulnerable, which is in line with the results
presented by Lekies et al. [16] and Jang et al. [10].
4.4 Usage of Outdated Libaries
Much of the success of JavaScript stems from the pow-
erful libraries used by many Web sites. The most widely
used library on the Web is undoubtedly jQuery; in our
work we found that up to 75% of the Web sites we an-
alyzed used jQuery. The usage pattern is also shown in
ve.org/cdx/search/cdx?url=facebook.com/crossdomain.xml,
3A prime example is the facebook.com policy http://web.archi
which does not have entries between 2011 and 2013
Figure 9. When code is included from a third party into
any Web site, this code is implicitly trusted, i.e., it runs in
the origin of the including Web site. Therefore, whenever
any third-party component is vulnerable, this implies that
all sites which include the ﬂawed code will suffer from
the vulnerability.
To understand the risk associated with this, we used
retire.js [25], a tool to detect libraries and report known
vulnerabilities in them, on the versions of jQuery we col-
lected in our study. Moreover, for each domain that used
jQuery we checked if the used version had a known vul-
nerability at the time of use. The results are also depicted
in Figure 9: it becomes clear that the majority of Web
sites used outdated versions of jQuery, for which known
vulnerabilities existed at the time.
Although this paints a grim picture, a vulnerable li-
brary does not necessarily directly imply a site at risk.
As an example, one vulnerability which was disclosed
in 2012 [11] could only be triggered if user-provided in-
put was used in a CSS selector. Nevertheless, as previous
work has shown, such outdated libraries can cause severe
security issues, such as Client-Side XSS [34].
Next to jQuery, only the YUI library was discovered
on a notable fraction of domains.
In 2011, its usage
reached its peak with about 10% prevalence, dropping
off until 2016 to 3.5% of the domains that included the
library. Similar to what we observed for jQuery, the frac-
tion of domains running a known vulnerable version of
YUI is high: for 2016, 85% of the sites running YUI ran
a vulnerable version. These results are comparable to the
results of Lauinger et al. [14].
5
Indicators for Security Awareness
In this section, we highlight a number of features we can
measure, that indicate whether a site operator is aware of
Web security mechanisms.
Most of the security awareness indicators can be found
978    26th USENIX Security Symposium
USENIX Association
20042006200820102012201420160%5%10%15%20%25%30%35%40%crossdomain.xml*wildcard*wildcardandsession2006200820102012201420160%10%20%30%40%50%60%70%jQuery(all)jQuery(vulnerable)in the HTTP headers of the responses. The Web Archive
records all headers it received when originally crawling
the target site, which allows us to go back in time and
investigate how many sites used any of the relevant head-
ers. For our work, we identiﬁed a number of these rel-
evant headers, which we discuss in the following in the
order of their introduction on the Web. An overview over
the fraction of domains that make use of these security
headers is shown in Figure 10. Moreover, Table 2 shows
when each of the discussed security measures was imple-
mented by the major browsers.
5.1 HTTP-only Cookies
The Web’s principle feature for session management is
the use of cookies. These are sent along with every re-
quest to the server, allowing a user to establish a session
in the ﬁrst place and for the server to correctly attribute
requests to a user. At the same time, by default, cook-
ies are accessible from JavaScript as well, making these
session identiﬁers prime targets for Cross-Site Script-
ing attacks. To thwart these attacks, starting from 2001
browsers added support for so-called HTTP-only cook-
ies. This ﬂag marks a cookie to only be accessible by
the browser in an HTTP request, while at the same time
disallowing access from JavaScript.
We mark a domain as using HTTP-only cookies when
at least one cookie was set using the httponly ﬂag. This
only represents a lower bound for the sites, though. Nat-
urally, the Archive crawler does not log into any Web
application. It is reasonable to assume that some sites
do not use session identiﬁers until the need arises, i.e., a
user successfully logs in. Hence, sites might have made
use of the header more frequently, but the archived data
cannot account for such behavior.
In our study, we found that sites started employing
this technique in 2006 before any other security measures
were in place. Moreover, we can clearly observe a trend
in which by 2016, over 40% of all domains made use of
this. This indicates that the admins of the most relevant
sites on the Web are well-aware of the dangers of cookie
theft and try to mitigate the damage of an XSS attack.
HTTP-only Cookies
Content Snifﬁng
X-Frame-Options
HSTS
CSP
Chrome
2008
2008
2010
2010
2011
IE Firefox
2006
2016
2009
2010
2010
2001
2008
2009
2015
2012
Table 2: Browser support for Web security features
Figure 10: Use of Security Headers per year
5.2 Disallowing Content Snifﬁng
Although all markup and programming languages used
in the Web are well-speciﬁed, Web application develop-
ers often make more or less subtle mistakes when build-
ing their sites. To still allow users of these sites an un-
hindered view on the pages, modern browsers are very
error-tolerant, i.e., they compensate for a number of mis-
takes which can be introduced by developers. One of
the mechanisms used to achieve this tolerance is content
snifﬁng, a technique used by browsers to guess the type
of content being shown, to allow for proper rendering.
The HTTP/1.1 standard speciﬁcally states that such sniff-
ing should only happen when no Content-Type header
is sent from the server [5]. Depending on the implemen-
tation of the browser, this can either can be done by an-
alyzing the URL (e.g., looking for a .html sufﬁx) or by
investigating the content of the resource [41].
For the sake of presentation, let us assume a Web site
offers users a way to upload text ﬁles (marked by a .txt
sufﬁx). In this case, a user can upload a text ﬁle contain-
ing HTML code. If a victim’s browser is now lured to the
URL hosting the .txt ﬁle and no explicit Content-Type
header is sent, modern browsers will analyze the con-
tent, deem it to be HTML and render it accordingly. This
effectively results in the attacker’s HTML and accompa-
nying script code to be executed under the origin of the
vulnerable site, leading to a Cross-Site Scripting vulner-
ability. In addition, improper content snifﬁng could also
lead to the site being used to host malware.
To prevent such attacks, Internet Explorer ﬁrst im-
plemented the X-Content-Type-Options header in
2008 [15]. When the value of this header was set to
nosniff, it would prevent IE from trying to guess the
content.
In the speciﬁc case, Internet Explorer’s al-
gorithm was also more aggressive than RFC2616 de-
manded: it tried to sniff content regardless of the pres-
ence of any Content-Type headers. Google Chrome
showed a similar behavior, which can also be controlled
USENIX Association
26th USENIX Security Symposium    979
2006200820102012201420160%10%20%30%40%50%60%CSPHSTSX-Frame-OptionshttponlycookieContentSniffingusing the X-Content-Type-Options header. Similar to
what we observed for HTTP-only cookies, at ﬁrst only
few sites adopt this security mechanism. Again, a no-
table increase can be observed over time, resulting in al-
most 47% of the analyzed sites using the protective mea-
sure by 2016.
5.3 Clickjacking Protection
Another potential danger to Web applications is so-called
Clickjacking [9]. This type of attack is a sub class of the
more general attack dubbed Unsolicited Framing by Za-
lewski [41]. The main idea relies on the ability of an
attacker to mask a frame pointing to a benign-but-buggy
site with the opacity CSS attribute on his own site. The
attacker now tries to motivate the victim to click in the
area in which this hidden frame resides. This can, e.g.,
be achieved by a pretend browser game. However, in-
stead of interacting with the apparent browser game, the
victim actually clicks in the hidden frame. The extend
of this attack can range from invoking actions, such as
soliciting likes on a social media site, all the way to an
attack outlined by Jeremiah Grossman, in which Click-
jacking was used to gain access to the video and audio
feed from the victim’s computer [6].
While the unsolicited framing itself was already dis-
cussed before the devastating demonstration by Gross-
man, the clear attack potential as shown by their attack in
2008 motivated browser vendors to develop and deploy
a protective measure, dubbed the X-Frame-Options
header (for short also XFO). Even though the X in the
name denotes the fact that this was not a standardized
header, it was introduced within a few months after the
presented attack by Internet Explorer and Firefox, while
Chrome followed a year later (see Table 2). The notion of
this header is simple: framing can either be completely
blocked (DENY), only be allowed from the same origin
(SAMEORIGIN), or speciﬁcally allowed for certain URLs
(ALLOW-FROM url) [23]. Depending on the browser,
there also exists a variant ALLOWALL, which effectively
disables any protection as well as SAMEDOMAIN, which is
an alias for SAMEORIGIN. Note, however, that these val-
ues are not presented in the accompanying RFC, which
was introduced in 2013 [28].
For our measurements, we only counted sites which
use the protective measure by either setting it to DENY,
SAMEORIGIN, its alias SAMEDOMAIN, or ALLOW-FROM
with a speciﬁc list of domains. The results are depicted in
Figure 10. The results indicate that although the header
was introduced in 2010, an increase in its usage can only
be observed starting from 2012. The number of sites
using XFO increased rapidly since then and reached its
peak in 2016 with 53% of all sites deploying it. Note,
however, that use of the header has been deprecated
by Content Security Policy (CSP) Level 2 [39] starting
from around 2015, being replaced by the more powerful
frame-ancestors directive of CSP.
5.4 Content Security Policy
One of the biggest client-side threats to any Web appli-
cation is the injection of markup, either HTML or even
JavaScript code, into it. In such a case, the browser can-
not distinguish between markup originating from the de-
veloper of the application and the attacker’s code. Hence,
all code is executed, leading to a client-side code injec-
tion known as Cross-Site Scripting. To mitigate the ex-
ploitability of such an injection vulnerability, the W3C
has proposed the so-called Content Security Policy. In
its foundation, CSP is a technique that aims to speciﬁ-
cally whitelist sources of code with the goal of stopping
any attacker-provided code from being executed. To that
end, a Web application that deploys CSP sends a header
containing a number of whitelisted code origins, e.g.,
self or cdn.domain.com. Even if an attacker man-
ages to inject her own markup into the application, the
code is bound to be hosted on either the site itself or
cdn.domain.com. The main assumption here is that the
attacker is unable to control any code on these origins.
Also, by default, CSP disallows the use of inline script
elements and the eval construct.
CSP has many more directives, allowing Web devel-
opers to control which hosts may be contacted to retrieve
images or stylesheets, specifying how the site may be
framed (deprecating the X-Frame-Options header), or
to report violations of the policy. The setup of a prop-
erly working policy, however, is non-trivial, as has been
shown by previous work [37, 38]. Nevertheless, we deem
the presence of a CSP header to be an indicator for the
awareness of a site’s operator. Given the results from
previous work, investigating the security of the policies
of single sites is out of scope for our work.
Initially, CSP was
introduced by Firefox and
WebKit-based browsers (including Chrome) with dif-
ferent names, i.e., X-Content-Security-Policy and
X-WebKit-CSP, respectively. We therefore count the
presence of these headers as a regular use of the nowa-
days standardized Content Security Policy. As we
can observe in Figure 10, even though implemented in
browsers for a number of years, CSP only was used in
the wild starting from 2013 by any of the major sites.
As previous work has shown, setting up CSP for legacy
applications is very challenging. Our data indicates that
even by 2016, less than 10% of the sites we considered
deployed any CSP at all. Hence, although CSP mitigates
the effect of XSS vulnerabilities in JavaScript-enabled
Web applications, its adoption still lags far behind other
security measures.
980    26th USENIX Security Symposium
USENIX Association
5.5 HTTP Strict Transport Security
Along with the success of the Web as the number one
platform for information access also came a number
of attacks on the connection between client and server.
While in the Web’s beginning, transfer of sensitive infor-
mation was less likely to occur, modern Web applications
almost always require a login. Arguably, the transport of
such sensitive information should be conducted in a se-
cure manner, i.e., should always be encrypted. On the