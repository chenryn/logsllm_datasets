0.1, weight decay is 0.0001, and the max iteration is 1000
accordingly.
Table 2: The structure and parameters of the CNN
model
the smartphone moves from the distance 𝐷𝐹 𝐷 = 30𝑐𝑚 to the
distance 𝐷𝐹 𝐷 = 50𝑐𝑚 and from the distance 𝐷𝐹 𝐷 = 40𝑐𝑚 to
the distance 𝐷𝐹 𝐷 = 50𝑐𝑚, respectively. FaceCloseup achieves
the accuracy of 99.24% and 99.27% against the two types of
attack videos, as shown in Figure 3. FaceCloseup requires
closeup videos containing obvious changes of the facial dis-
tortion while the rate of the facial distortion changes become
lower as the camera move away from the face. Therefore,
the two types of the attacking facial videos do not include
obvious and sufficient changes of the facial distortion because
the camera is not close to the face enough.
Layer
𝐶𝑜𝑛𝑣1
𝑃 𝑜𝑜𝑙1
𝐶𝑜𝑛𝑣2
𝑃 𝑜𝑜𝑙2
𝐹 𝐶1
𝐹 𝐶2
𝑂𝑈 𝑇
3 × 3
32 3 × 3 filters
Size
32 5 × 5 filters
3 × 3
1 × 1024
1 × 192
1 × 2
Stride Padding
1
2
1
2
0
0
0
1
1
1
1
0
0
0
5.2 Experimental Results
5.2.1 Detecting MVFF-based Attacks. FaceCloseup is accu-
rate in detecting MVFF-based attacks, including photo/video
based attacks and 3D virtual face model-based attacks.
In the photo-based attacks, the facial photos taken at the
distance 𝐷𝐹 𝐷 = 30𝑐𝑚, 40𝑐𝑚, 50𝑚 are displayed to generate
the attack videos as explained in Section 4.2. Figure 2 shows
that FaceCloseup can effectively detect the photo-based at-
tacks. In particular, FaceCloseup achieves the accuracy of
99.23%, 99.28%, and 99.31% against photo-based attacks
with photos taken at 30cm, 40cm, and 50cm away, respective-
ly. Because FaceCloseup determine the liveness of a face by
analyzing the facial distortion changes correlated to the 3D
depth information of the real 3D face and the changes of the
camera distance, it is difficult for the adversary to generate
the correct facial distortion by displaying a 2D facial photo
on a 2D surface and moving the facial photo.
Figure 2: Accuracy of FaceCloseup against the photo-
based attacks
Figure 3: Accuracy of FaceCloseup against the video-
based attacks
The 3D virtual face model-based attack is a powerful
attack. The synthesized facial photos are produced which
include the estimated facial distortion based on the changes
of the camera distance. FaceCloseup successfully detects this
attack with an accuracy of 99.48%. An important reason is
that it is still challenging for the existing 3D virtual face
model to synthesize the closeup facial photos with significant
facial distortion due to the complex and uneven 3D surface
of the faces and occlusion of partial facial regions [5].
6 RELATED WORK
We summarize the related work based on the liveness indica-
tors they use, including 3D face, texture pattern, real-time
response, and multimodal.
The 3D face liveness indicator is based on the clue that a
real face has 3D depth characteristics. The 3D face character-
istics detection is usually associated with optical flow analysis
and changes of face views. A 3D face has the characteristic
of optical flow that the motion speed of the central part of
face is higher than the outer face region [6]. Along this line,
Kollreider et al. proposed a liveness detection algorithm to
analyze the optical flow based on ears, nose, and mouth [7].
However, the optical flow based methods usually require high-
quality input videos with ideal lighting conditions, which may
be difficult to achieve in practice. Compared to these works,
FaceCloseup takes input video from a generic camera which
can be easily achieved in practice.
In the video-based attacks, the facial videos are displayed
to FaceCloseup. The attacking facial videos are taken when
On the other hand, the 3D characteristics about a real face
can be detected in face movements. Chen et al. examined
99.2399.2899.3130405086889092949698100Accuracy (%)DFD (cm)99.2499.27(30,50)(40,50)86889092949698100Accuracy (%)Movement Distance (cm)Session 3B: Learning and AuthenticationAsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand245the 3D characteristics of nose in the liveness detection [4]
which compares the the direction changes of the mobile phone
measured by the accelerometer and the changes of nose edge
in the video. However, to produce the clear nose edge, a
controlled lighting is required. The controlled lighting may
not be possible in practice. Li et al. proposed FaceLive which
requires a user to move the mobile device in front of his/her
face and analyzes the consistency between the motion data
of the mobile device and head rotation in the facial video [8].
Unfortunately, although the above two liveness detection
algorithms can detect the photo-based attacks and the video-
based attacks, they are vulnerable to the 3D virtual face
model-based attacks as an adversary can synthesize the cor-
rect nose changes and head rotation video according to the
device movements in real time [16].FaceCloseup can detect
all MVFF-based attacks including the photo-based attack,
video-based attack, and 3D virtual face model-based attack.
The texture pattern based techniques examines detectable
texture patterns due to the printing process and the material
printed on. IDIAP team took a facial video as input and the
local binary patterns from each extracted frame in the video
in order to build a global histogram for the video. The liveness
of face is determined based on the global histogram [3]. Tang
et al. proposed Face Flashing which captures face videos
with strong enough random screen light shooting at a user’s
face and sends the video to a remote server such as cloud
services for analysis of the light reflection in the face videos for
liveness detection [12]. The texture pattern based techniques
usually require high-quality photos/videos captured in ideal
lighting conditions and significant computation power in the
analysis, which may be hard to achieve on mobile devices
in practice. Using computation power from remote server
or cloud services could incur the cost of significant network
traffic and privacy issues. In contrast, FaceCloseup takes
closeup facial videos as input and analyzes the input videos
locally on mobile devices.
The real-time response based approaches require interac-
tion with users in real time. Pan et al. required users to blink
their eyes in order to detect the liveness [9]. Unfortunately,
these approaches are subject to the video-based attacks and
the 3D virtual face model-based attacks. FaceCloseup can
detect such video-based attacks effectively.
Finally, multimodal based liveness detection approaches
take face biometrics and other biometrics into account in user
authentication. Wilder et al. took facial thermogram from an
inferred camera and face biometrics from a generic camera
in authentication process [14]. Unlike the above approaches,
which rely on the hardware sensors rarely deployed on mobile
devices, our approach requires a front-facing camera which
is pervasively available on most mobile devices.
7 CONCLUSION
In this paper, we proposed an effective and practical liveness
detection mechanism, FaceCloseup, for face authentication to
prevent MVFF-based attacks. FaceCloseup does not require
any additional hardware but a front-facing optical camera
which is widely available on mobile devices. FaceCloseup
detects 3D characteristics of a real face by using deep learn-
ing techniques to analyze and identify the changes of facial
distortion in a closeup facial video. FaceCloseup can detect
all MVFF-based attacks with an accuracy as high as 99.48%.
8 ACKNOWLEDGMENTS
The work was supported in part by NSFC under Grant
61802289, 61671013, 61672410. This research is support-
ed by the National Research Foundation, Prime Minister’s
Office, Singapore under its Campus for Research Excel-
lence and Technological Enterprise (CREATE) programme.
Yingjiu Li was supported in part by the Singapore Na-
tional Research Foundation under NCR Award Number
NRF2014NCR-NCR001-012. Robert Deng was supported
in part by AXA Research Fund.
REFERENCES
[1] Andrea F Abate, Michele Nappi, Daniel Riccio, and Gabriele
Sabatino. 2007. 2D and 3D face recognition: A survey. Pattern
Recognition Letters 28, 14 (2007), 1885–1906.
[2] Christian Baumberger, Mauricio Reyes, Mihai Constantinescu,
Radu Olariu, Edilson de Aguiar, and Thiago Oliveira Santos. 2014.
3D face reconstruction from video using 3d morphable model and
silhouette. In SIBGRAPI. IEEE, 1–8.
[3] Murali Mohan Chakka, Andre Anjos, Sebastien Marcel, Roberto
Tronci, Daniele Muntoni, Gianluca Fadda, Maurizio Pili, Nicola
Sirena, Gabriele Murgia, Marco Ristori, et al. 2011. Competition
on counter measures to 2-d facial spoofing attacks. In IJCB 2011.
IEEE, 1–6.
[4] Shaxun Chen, Amit Pande, and Prasant Mohapatra. 2014. Sensor-
assisted Facial Recognition: An Enhanced Biometric Authentica-
tion System for Smartphones. In MobiSys 2014. 109–122.
[5] Ohad Fried, Eli Shechtman, Dan B Goldman, and Adam Finkel-
stein. 2016. Perspective-aware manipulation of portrait photos.
ACM Transactions on Graphics (TOG) 35, 4 (2016), 128.
[6] O. Kahm and N. Damer. 2012. 2D face liveness detection: An
overview. In BIOSIG 2012. 1–12.
[7] Klaus Kollreider, Hartwig Fronthaler, and Josef Bigun. 2009. Non-
intrusive liveness detection by face images. Image and Vision
Computing 27, 3 (2009), 233–244.
[8] Yan Li, Yingjiu Li, Qiang Yan, Hancong Kong, and Robert H
Deng. 2015. Seeing your face is not enough: An inertial sensor-
based liveness detection for face authentication. In CCS 2015.
ACM, 1558–1569.
[9] Gang Pan, Lin Sun, Zhaohui Wu, and Shihong Lao. 2007. Eyeblink-
based anti-spoofing in face recognition from a generic webcamera.
In ICCV 2007. 1–8.
[10] Supasorn Suwajanakorn,
Ira Kemelmacher-Shlizerman, and
Steven M Seitz. 2014. Total moving face reconstruction. In Euro-
pean Conference on Computer Vision. Springer, 796–812.
[11] Supasorn Suwajanakorn, Steven M Seitz, and Ira Kemelmacher-
Shlizerman. 2015. What makes tom hanks look like tom hanks.
In ICCV. 3952–3960.
[12] Di Tang, Zhe Zhou, Yinqian Zhang, and Kehuan Zhang. 2018.
Face Flashing: a Secure Liveness Detection Protocol based on
Light Reflections. In NDSS 2018. Internet Society.
[13] Paul Viola and Michael J Jones. 2004. Robust real-time face
detection. International journal of computer vision 57, 2 (2004),
137–154.
[14] Joseph Wilder, P Jonathon Phillips, Cunhong Jiang, and Stephen
Wiener. 1996. Comparison of visible and infra-red imagery for
face recognition. In FG 1996. IEEE, 182–187.
[15] Xuehan Xiong and Fernando De la Torre. 2013. Supervised descent
method and its applications to face alignment. In CVPR 2013.
IEEE, 532–539.
[16] Yi Xu, True Price, Jan-Michael Frahm, and Fabian Monrose.
2016. Virtual U: Defeating Face Liveness Detection by Building
Virtual Models from Your Public Photos. In USENIX security
symposium. 497–512.
Session 3B: Learning and AuthenticationAsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand246