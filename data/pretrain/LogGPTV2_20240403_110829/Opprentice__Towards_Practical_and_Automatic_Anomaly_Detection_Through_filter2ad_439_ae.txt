moderate preference. We also evaluate another two preferences:
sensitive-to-precision (recall≥0.6 and precision≥0.8) in the second
row and sensitive-to-recall (recall≥0.8 and precision≥0.6) in the
third row. The preferences are represented by the top-right boxes
in the heat maps. The right-side line charts show the percentage of
the points inside the boxes (satisfying the preferences) if we use the
original or lowered preferences, e.g., scaling the boxes up.
Focusing on the heat maps, we see that while the points ob-
tained by the other three metrics remain unchanged for different
preferences, the PC-Score has the ability of adjusting the recall
and precision according to different preferences. Because of this
advantage, we see in the line charts that PC-Score always achieve
the most points inside the boxes for both the original preference
and the scaled-up ones.
We also notice that it is not easy to satisfy the preference for
all the weeks, because anomalies are very rare in some weeks.
For example, we ﬁnd that for the weeks where no point satisﬁes
the moderate preference, anomalies are 73% and 78% fewer than
other weeks for PV and #SR, respectively. Thus it is inevitable
to generate more false positives in order to identify those few
anomalies, leading to low precision. In addition, missing just a few
anomalies can lead to an obvious degradation in recall. Fortunately,
as the anomalies are few in those weeks, relatively low precision or
recall would not cause a large number of false positives or false
negatives. For example, if there are ten anomalous data points
and we identify four of them (40% recall), we would miss only
six anomalous data points; in such case, if we have 40% precision,
we would just identify six false positives. The operators we work
with suggest that they are actually OK with this small number of
false positives or false negatives.
5.6 EWMA vs. 5-Fold for cThld Prediction
Previously, we show the ofﬂine evaluation of different metrics
for cThld conﬁguration, and ﬁnd that PC-Score outperforms the
other three. Now we evaluate the performance of online cThld
prediction based on the PC-Score, which is also the detection
accuracy of Opprentice as a whole. We compare the EWMA
based method used by Opprentice with 5-fold cross-validation. The
evaluation is similar to §5.5 except that the cThld prediction only
uses the historical data (the training set) rather than the future data
(the test set). As aforementioned, the result of each week can vary
greatly because anomalies are quite rare in some weeks. To obtain
a stable result, we calculate the average recall and precision of a
4-week moving window. The window moves one day for each
step so that we can obtain a more ﬁne-grained result. Here, we
show the result under the operators’ actual preference (recall≥0.66
and precision≥0.66). The recall and precision of each window are
shown in Fig. 13. The ofﬂine result (called the best case here) is
also shown as a baseline.
The result shows that, for PV, #SR, and SRT, the EWMA
achieves 40%, 23%, and 110% more points inside the shaded
regions, respectively, when compared with the 5-fold cross vali-
dation.
In total, 8403 (7.3%), 2544 (2.1%), and 86 (6.4%) data
points are identiﬁed as anomalies by Opprentice in the test sets
(after the 9th week) for the three KPIs, respectively (not shown).
We notice that there are some points falling outside of the shaded
regions, such as the window between 58 and 70 in Fig. 13(a). It
is mainly because the anomalies during those weeks are quite rare.
PC-ScoreF-ScoreDefaultcThldSD(1,1)PC-ScoreF-ScoreDefaultcThldSD(1,1)(a1)Moderate0.20.40.60.81Recall0.20.40.60.81Precision-3-2-1012PC-Score(a3)Sensitivetoprecision0.20.40.60.81Recall0.20.40.60.81Precision-3-2-1012PC-Score(a5)Sensitivetorecall0.20.40.60.81Recall0.20.40.60.81Precision-3-2-1012PC-Score02040608010011.21.41.61.82Percentage(%)Preferencescalingratio(a2)#ofpointsinthebox02040608010011.21.41.61.82Percentage(%)Preferencescalingratio(a4)#ofpointsinthebox02040608010011.21.41.61.82Percentage(%)Preferencescalingratio(a6)#ofpointsintheboxPC-ScoreF-ScoreDefaultcThldSD(1,1)PC-ScoreF-ScoreDefaultcThldSD(1,1)(b1)Moderate0.20.40.60.81Recall0.20.40.60.81Precision-3-2-1012PC-Score(b3)Sensitivetoprecision0.20.40.60.81Recall0.20.40.60.81Precision-3-2-1012PC-Score(b5)Sensitivetorecall0.20.40.60.81Recall0.20.40.60.81Precision-3-2-1012PC-Score02040608010011.21.41.61.82Percentage(%)Preferencescalingratio(b2)#ofpointsinthebox02040608010011.21.41.61.82Percentage(%)Preferencescalingratio(b4)#ofpointsinthebox02040608010011.21.41.61.82Percentage(%)Preferencescalingratio(b6)#ofpointsintheboxPC-ScoreF-ScoreDefaultcThldSD(1,1)PC-ScoreF-ScoreDefaultcThldSD(1,1)(c1)Moderate0.20.40.60.81Recall0.20.40.60.81Precision-3-2-1012PC-Score(c3)Sensitivetoprecision0.20.40.60.81Recall0.20.40.60.81Precision-3-2-1012PC-Score(c5)Sensitivetorecall0.20.40.60.81Recall0.20.40.60.81Precision-3-2-1012PC-Score02040608010011.21.41.61.82Percentage(%)Preferencescalingratio(c2)#ofpointsinthebox02040608010011.21.41.61.82Percentage(%)Preferencescalingratio(c4)#ofpointsinthebox02040608010011.21.41.61.82Percentage(%)Preferencescalingratio(c6)#ofpointsinthebox(a) KPI: PV
(b) KPI: #SR
(c) KPI: SRT
Figure 13: Online detection accuracy of Opprentice as a whole. The best case is obtained from ofﬂine mode. The shaded regions
represent to the operators’ accuracy preference (recall ≥ 0.66 and precision ≥0.66).
For example, there are only on average 4% anomalous points in
the ground truth of PV data for the window between 58 and 70.
However, as mentioned earlier in §5.5, in such case, a little low
precision or recall would not generate many false positives or false
negatives for operators. In summary, Opprentice can automatically
satisfy or approximate the operators’ accuracy preference.
5.7 Labeling Time vs. Tuning Time
it further cost him about 10 days to test and tune the detector. In the
above cases, after days of tuning, only the ﬁrst operator’ detector
works relatively well; yet, the other two operators are still not
satisﬁed with the accuracy of their detectors, and ﬁnally abandon
them. We have compared the accuracy of Opprentice with these
basic detectors in §5.3.1.
Next, we show the time cost of labeling, the only manual job
required for operators when they use Opprentice. Fig. 14 shows
the operators’ labeling time when they label the three types of KPI
data using our tool (§4.2). The result shows that the labeling time
of one-month data basically increases as the number of anomalous
windows in that month. An anomalous window refers to a window
of continuous anomalies derived from one label action. Among the
three KPIs, SRT requires less labeling time for each month of data
because it has less data points in a month as its data interval is 60
minutes. Overall, the labeling time of one-month data is less than
6 minutes. The total labeling time for PV, #SR, and SRT is 16,
17, and 6 minutes, respectively. One intuitive reason for the low
labeling overhead is that the operators each time label a window
of anomalies rather than labeling individual anomalous data points
one by one. The anomalous windows can be much fewer (Fig. 14)
than the anomalous points in the data.
Figure 14: Operators’ labeling time vs.
anomalous windows for every month of data.
the number of
To show the value of how Opprentice help reduce operators’
manual efforts, we also present some anecdotal examples of op-
erators’ tuning time of detectors, including the time they learn the
detectors and understand their parameters. We interviewed three
operators from the search engine, who have experienced tuning
detectors before. The ﬁrst operator uses SVD, and he said it took
him about 8 days to tune the detector; the second operator uses
Holt-winters and historical average, and he spent about 12 days
tuning these two detectors; the third operator applies time series
decomposition, and he said that after the detector was implemented,
Although the time reported above is not the exactly time used for
tuning, it provides a basic idea of the overhead and the difﬁculty
of manually tuning detectors.
The operators we interviewed
conﬁrmed that detector tuning is very time-consuming, and they
are neither interested nor feel comfortable in doing so.
In sum-
mary, Opprentice can help replace the time-consuming and boring
detector tuning with fast and convenient anomaly labeling.
5.8 Detection Lag and Training Time
The detection lag, in principle, consists of the feature extraction
time and the classiﬁcation time. We run Opprentice on the Dell
PowerEdge R420 server with the Intel Xeon E5-2420 CPU and
24GB memory. The total time of extracting 133 features is on
average 0.15 second for each data point. The classiﬁcation takes
trivial time, which is on average less than 0.0001 second per data
point. Besides, the ofﬂine training time is less than 5 minutes each
round. Since all the detectors can run in parallel, and training of
random forests is also able to be parallelized, we believe that one
can get a better performance by taking advantage of multi-threaded
techniques and distributed computing on server clusters.
6. DISCUSSION
Anomaly detection is complex in practice. In this section, we
discuss some issues regarding anomaly detection and clarify the
scope of Opprentice.
Anomaly detection, not troubleshooting. Sometimes, although
the operators admit the anomalies in the KPI curve, they tend
to ignore them as they know that the anomalies are caused by
some normal activities as expected, such as service upgrades and
predictable social events. However, since our study focuses on
identifying abnormal behaviors of the KPI data (called anomaly
detection in this paper), we ask the operators to label anomalies
based on the data curve itself without considering the reasons
behind. Anomaly detection is a ﬁrst important step of monitoring
service performance. We believe that the detection results should
be reported to operators and let operators decide how to deal
with them, or more ideally, input into a troubleshooting system
for analyzing the root causes and generating more actionable
suggestions. For example, the troubleshooting system may ﬁnd
that the anomalies are due to normal system upgrades and suggest
0.200.400.600.801.00PrecisionBestcaseEWMA5-Fold0.200.400.600.801.0011121314151617181RecallIDof4-weekmovingwindows0.200.400.600.801.00PrecisionBestcaseEWMA5-Fold0.200.400.600.801.0016111621263136414651RecallIDof4-weekmovingwindows0.200.400.600.801.00PrecisionBestcaseEWMA5-fold0.200.400.600.801.0013579111315171921232527RecallIDof4-weekmovingwindowsLabelingtime(minutes)forone-monthdataNumberofanomalouswindowsinone-monthdataPV#SRSRToperators to ignore them. However, troubleshooting anomalies
itself is outside our research scope.
Anomaly duration. The duration of continuous anomalies
could be another important consideration of raising alarms. In this
paper, we do deliberately omit this factor. One reason is that it will
make our model too complex to show the core idea. Another one
is that it is relative easy to implement a duration ﬁlter based upon
the point-level anomalies we detected. For example, if operators
are only interested in continuous anomalies that last for more than
5 minutes, one can solve it through a simple threshold ﬁlter.
Detection across the same types of KPIs. Some KPIs are of the
same type and operators often care about similar types of anomalies
for them [5]. For example, the PV originated from different ISPs.
When applying Opprentice to such case, operators only have to
label one or just a few KPIs. Then the classiﬁer trained upon those
labeled data can be used to detect across the same type of KPIs.
Note that, in order to reuse the classiﬁer for the data of different
scales, the anomaly features extracted by basic detectors should be
normalized. We plan to explore this direction in future work.
Dirty data. A well known problem is that detectors are often
affected by “dirty data”. Dirty data refer to anomalies or missing
points in data, and they can contaminate detectors and cause errors
of detectors. We address this problem in three ways. (a) Some of
our detectors, e.g., weighted MA and SVD, can generate anomaly
features only using recent data. Thus, they can quickly get rid of the
contamination of dirty data. (b) We take advantage of MAD [3,15]
to make some detectors, such as TSD, more robust to dirty data; (c)
Since Opprentice uses many detectors simultaneously, even if a few
detectors are contaminated, Opprentice could still automatically
select and work with the remaining detectors.
Learning limitations. A supervised learning based approach
requires labeled data for initialization. This is an additional
overhead when compared with applying basic detectors directly.
Fortunately, the KPI data, nowadays, are easy to obtain [1, 4, 9, 12,
14, 17, 26]. Meanwhile, labeling can also be effective and cost less
time as we demonstrated earlier with out labeling tool. Another
issue is that a learning based approach is limited by the anomalies
within a training set. For example, anomalies can be rare, and new
types of anomalies might appear in the future [16]. We solve this
problem by incrementally retraining the classiﬁer to gather more
anomaly cases and learn emerging types of anomalies.
Detection accuracy. Because of the many practical challenges
mentioned above, anomaly detection is a complex and challenging
task. It is intractable to achieve high precision and recall all the
time. We also cannot guarantee Opprentice to be able to always
satisfy the operators’ accuracy preference. But our evaluation
shows that the accuracy of Opprentice is still promising, especially
for the operators’ preference in the studied service.
7. RELATED WORK
Many efforts have been put into the ﬁeld of anomaly detection.
Researchers have developed numerous detectors using different
techniques [1–24]. In addition, researchers try to address several
challenges of applying detectors in practice. (a) For auto-tuning
the internal parameters of detectors, Krishnamurthy et al. [11]
proposes a multi-pass grid search to ﬁnd appropriate parameters
from data. Himura et al. [23] searches for the parameters to
maximize the ratio of detected events.
In comparison, beyond
detector internal parameters, we also consider automatically se-
lecting detectors and their thresholds. (b) Some work uses ROC
curves to evaluate the performance of different detectors regardless
of their thresholds [9, 14, 26]. This technique is also used in our
work. (c) MAD is used to improve the robustness of detectors to
dirty data [3, 15]. We also implement two detectors with MAD.
(d) Some solutions attempt to statically combine different detectors
together [8, 21]. We compared Opprentice with them. (e) Machine
learning has also been applied in anomaly detection [16, 20, 32],
but it serves as basic detectors. On the contrary, we use machine
learning to combine different existing detectors.
Another important challenge of anomaly detection is to obtain
the ground truth to evaluate detectors. Three commonly-used
solutions are: (a) using the real anomalies identiﬁed or conﬁrmed
by domain operators [1,4,9,12,14,17,26]; (b) generating synthetic
anomalies by injecting real or pre-deﬁned anomalies into the
background data [9, 14, 18, 19]; (c) pair-wise comparisons, which
treat the anomalies reported by other detectors as the ground
truth [1, 8, 10, 17, 18]. Because our fundamental goal is to satisfy
operators’ demands, we believe that solution (a) makes more sense
in this paper.
8. CONCLUSION
Applying anomaly detection to an Internet-based service has
been challenging in practice. This is because the anomalies
are difﬁcult to quantitatively deﬁne, and existing detectors have
parameters and thresholds to tune before they can be deployed.
Our proposed framework, Opprentice, tackles the above challenges
through a novel machine learning based approach. The unclear
anomaly concepts are captured by machine learning from real data
and operators’ labels, while numerous existing detectors can be
automatically combined by machine learning to train a classiﬁer
to identify the anomalies. Our evaluation on real-world search
KPIs show that Opprentice consistently performs similarly or even
better than the best performing basic detectors which can change
for different data sets.
To the best of our knowledge, Opprentice is the ﬁrst detec-
tion framework to apply machine learning to acquiring practical
anomaly concepts and automatically combining and tuning diverse
known detectors to satisfy operators’ accuracy preference. Emerg-
ing detectors, instead of going through time-consuming and often
frustrating parameter tuning, can be easily plugged into Opprentice,
making the former be deployed more easily and the latter more
accurate.