ﬁngerprinting studies done on a much larger scale also reveal
a similar trend with comparable numbers of both unique and
shared ﬁngerprints as shown in the table.
To compare this user data with crawler ﬁngerprints, we ﬁrst
updated our crawler ﬁngerprints. For this, we used PhishPrint
to re-generate 50 new token URLs per crawler and solicited
scans again which yielded 57 crawler ﬁngerprints. By
combining this with data from two prior experiments (Table
1, §3.2.1), we obtained a total of 256 distinct crawler ﬁnger-
prints from across a period of 13 months. Comparing this
with MTurk data, we found that 137 users (13.6%) had one of
32 ﬁngerprints that were colliding with the crawlers. Of them,
more than 90% of the users had a shared ﬁngerprint thus
indicating that most of the collisions were due to those ﬁnger-
prints which are already common among the users themselves.
The breakdown of this data by OS is reported in Table 7.
When analyzing the ﬁngerprint speciﬁcity results, it is im-
portant to note how all the ﬁngerprint numbers continue to in-
crease as the scale of the study increases. For example, in [22],
about 2 million users had 78,037 distinct and 12,250 shared
ﬁngerprints. On the other hand, the number of ﬁngerprints
collected from all 23 crawlers (including the VT ecosystem
USENIX Association
30th USENIX Security Symposium    3785
Canvas (s)
Font (s)
Total (s)
Mean
Median
90%
0.09
0.06
0.16
3.97
2.36
8.88
4.26
2.52
9.4
Table 6: Time taken for obtaining BFPs during user study
containing 80 crawlers) across a period of one year is only 256.
Thus, in the worst case, even if all 256 of these ﬁngerprints end
up as shared ﬁngerprints in a larger-sized user study, we can
expect a signiﬁcant population of victims to remain vulner-
able to the proposed attacks. However, this trend of increase
in ﬁngerprints as the study scale increases points to the need
for a much larger user study to accurately assess the speci-
ﬁcity of ﬁngerprints. While such a dedicated large-sized user
study is outside our means, in §3.2.1, we showed how we used
AmIUnique’s data of 467K users for directly computing the
ﬁngerprint collisions between crawlers and potential victims.
that Bitdefender, PhishTank,
SmartScreen, APWG and GSB are the main ﬁve crawlers
associated with the collisions accounting for 45%, 43%,
28%, 7% and 6% of the collisions respectively. Altogether,
these ﬁve crawlers account for 98% of the collisions (134
users). Note that these are the same ﬁve crawlers that we
have already studied in our AmIUnique-based speciﬁcity
experiment on a much larger scale ( §3.2.1).
Further, we
found
Timing Analysis. One might also argue that such sophis-
ticated ﬁngerprinting based cloaking logic will result in a
computational time delay that can reduce the effectiveness
of social engineering attacks launched on real users. In
order to see if this is true, we measured the time required to
perform the cloaking logic for the users in our ﬁrst user study.
Our results (Table 6) show that most of the time is spent in
obtaining font ﬁngerprints with mean time for the cloaking
logic being 4.26 seconds. At ﬁrst, it appears that it might
be possible to reduce the cloaking logic time by using a
“progressive” logic such as extracting and checking the faster
ﬁngerprints ﬁrst before going to the slower ones. But this is
not productive for cloaking attacks as victim machines will
progress all the way to the end of the cloaking logic anyway.
However, given that the mean time to fully load a web page
on a desktop machine is about 10.4 seconds [8], attackers
can use that to their advantage. As our ﬁngerprinting code
is very light in size (Appendix D), an attacker can potentially
load and start to run it immediately while simultaneously
“pretending” to be loading a site. This way, the attacker can
gain a sufﬁcient compute time budget for the cloaking logic.
5 Countermeasures
The CVD Scores reported in Table 1 can serve as a “report
card” for crawlers trying to prioritize their mitigation efforts
across different areas of weaknesses as we discuss below.
Browser Anomalies. At the outset, this seems like a
simple question of applying best practices as some crawlers
already have near-perfect scores. However, this is only true
to a certain extent. One issue is that many crawlers process
a large number of URLs daily. Hence, it is common practice
to use headless browsers for scalability [16]. However, this
results in an arms race9 between such browsers and their
detectors [25]. While our rudimentary crawler artifact vectors
did not take such sophisticated headless browser detection
features into account, it should be trivial to include them
in PhishPrint proﬁling pages and come up with a much more
sophisticated anomaly cloaking vector. Further, biometric
behavior-based bot detection systems can further complicate
this issue for crawlers [18] opening room for new evasion
vectors. While handling all these issues might involve elab-
orate browser changes, ML-driven crawler behavior, and/or
scalability compromises, we suggest the vendors to prioritize
on ﬁxing the simpler issues. All crawlers should visit each
URL atleast once with a “Real Browser” that supports all
web APIs and try to hide well-known artifacts [26, 45, 46].
Network Data. For handling these weaknesses, crawlers
have to diversify their network infrastructure in terms of
both IP addresses as well as geographical diversity and using
residential networks. GSB and PhishTank are some of the
best examples of this. However, during our vulnerability
disclosure, some companies have mentioned that it might be
difﬁcult for them to address this due to ﬁnancial implications.
In these cases, we suggest that vendors consider approaches
such as using peer-to-peer VPN networks [7] and sharing
URLs with other crawlers to help improve network diversity.
Advanced Fingerprints. Our study found that there was
extremely limited diversity of s across the entire
ecosystem. The maximum  blocklist CVD score
across all crawlers was only 9.3 with several crawlers having
less than 10 distinct s across hundreds of scanned
URLs. Among the three individual ﬁngerprinting vectors of
, improving font diversity is the easiest to ﬁx as it
only needs increasing the number of “font sets" installed in
the crawler instances. When doing this, it should be ensured
that the fonts match the general font set characteristics of
users from that geolocation. Some vendors already started
doing this as a result of our disclosures. However, the Canvas
and WebGL ﬁngerprints require more intricate mitigations.
Currently, there are 3 approaches for this:
• Blocking.
[24] proposed an ML-based script blocking
solution for ﬁngerprinting code. However, such solutions
cannot be used by crawlers as the presence of such blocking
can itself be used for evasion. Instead of blocking, URLs
can be isolated for further automated/human analysis.
However, the attackers can even ﬁngerprint such analysts’
browsers and add their ﬁngerprints to their blocklists.
While victim-side blocking solutions might still work, the
problem here is that of deployment. Unless such a client-
side solution is baked into all major browsers, it might not
achieve good coverage. This problem is further exacerbated
9At the time of writing this manuscript in October 2020, unfortunately
the headless detectors seem to be winning this battle.
3786    30th USENIX Security Symposium
USENIX Association
by the fact that many phishing victims may also be slow
adopters for technologies such as security extensions.
• Uniformity. Uniform software re-rendering approaches
that result in the same ﬁngerprints for all users have also
been proposed [50]. However, these also have the same cov-
erage issues as above. Unless, a majority of all users adopt
the same solution, the resulting uniform ﬁngerprint can
itself be used as an evasion vector while not losing victims.
• Randomization. This works by randomizing the ﬁnger-
prints in each browsing session [35]. Brave browser has
adopted this to devise a solution for Canvas and WebGL ﬁn-
gerprinting by adding small random noise to the generated
data [31]. This is the most promising approach for crawlers
as it does not need to be adopted universally for this to work.
Hence, we recommend vendors to adopt similar transpar-
ent randomization-based defenses in order to defend against
Canvas and WebGL-based ﬁngerprinting attacks. Another
possible solution is to use dynamic software reconﬁguration
approaches [28], although these have scalability limitations.
URL Reporting. It is also important for all the crawler
vendors to prevent abuse of their reporting infrastructure. In
this research, by simply registering a single domain and self-
reporting its wild card subdomains, we were able to collect a
large amount of sensitive information such as ﬁngerprints and
IP addresses of many crawlers. We discussed early on in the
paper about how segregating proﬁlable infrastructure based
on candidate domains will work in an attacker’s advantage
(§1: TLD+1 Bias). However, the crawlers can at least use
such separation techniques to divide their limited crawler
resources between submissions from vetted and non-vetted
URL reporters. Crawler vendors can also leverage existing
spam and anomaly detection research work to monitor and de-
tect abuse of URL reporting services and prevent anomalous
submissions of token URLs for proﬁling of crawlers.
Some vendor-speciﬁc recommendations we make are
in Appendix B.
6 Discussion
Vulnerability Disclosure. We completed an effective vul-
nerability disclosure process. We submitted detailed vulner-
ability reports to all 23 security crawlers (21 vendors) that we
have speciﬁcally proﬁled. 9 vendors (10 crawlers) have so far
acknowledged our results including Google (GSB), Microsoft
(SmartScreen, Outlook), Norton, AlienVault and Sophos. We
had follow-up discussions over e-mail and online meetings
with 7 vendors on our results. Of the 9 vendors, 3 mentioned
that they were already working on changes or were aware of
these limitations. 6 of them have reported to be working on
follow-up changes with one vendor mentioning about having
tasked multiple engineers to work on the problems we pointed
out in our paper. We also received a Google Vulnerability Re-
ward for our research. Our reward amount was the highest in
the category of “abuse-related methodologies” indicating both
“High Impact” and “High Probability” [9] of the cloaking at-
tacks we discovered. As a follow up, we also received three
Vulnerability Research Grants from Google encouraging us
to continue studying their security crawlers in the future.
Single TLD+1 Bias. One might argue that using multiple
subdomains under a single TLD+1 will deliver a lot less
diverse proﬁling information from crawlers than using
multiple TLD+1s. However, we show in §4.1.2 that the
diverse proﬁling information we collect from a single .com
domain generalizes well enough to “protect” phishing pages
hosted on 20 .xyz domains. We also performed a small
control experiment using 5 .xyz domains which conﬁrms the
same (§3.2.2). The extensive positive feedback we received
from the crawler vendors during vulnerability disclosure also
attests to the sensitivity of the information we were able to
gather by using a single registered domain. Most importantly,
we argue that if a crawler were to choose to segregate their
“proﬁlable” infrastructure based on domain names, then it
would only end up making an attacker’s job easier. This is
because the attacker can then begin to ﬁrst use a candidate
domain name in a benign mode for quickly collecting the
limited proﬁle of the segregated crawling infrastructure. They
can then switch that same domain into a malicious mode
by hosting phishing content hidden with the help of forensic
information found during the proﬁling stage.
Limitations. While PhishPrint evaluates crawlers by
avoiding phishing experiments, there are some use cases
where these experiments are indispensable. For example,
prior works such as [39] that focused on speed of population
of browser blocklists and [40] that focused on dynamic
label changes of URLs can only be accomplished with the
help of phishing experiments. Furthermore, our system will
be unable to measure crawler resilience against dynamic
cloaking attacks such as the “Timing” attacks studied in [52]
as we are limited to only the proﬁling data that can be
captured from the crawlers. Nevertheless, PhishPrint presents
a scalable solution to measure a wide variety of weaknesses
of crawlers and thus can be considered complementary to
existing phishing experiment-based designs.
We also note that the measurements such as “# IPs” that we
presented in Table 1 could be overcounted due to the presence
of URL sharing between crawlers (except when such counts
are 1 or 0). This is a difﬁcult problem to solve given that
there might be a lot of undisclosed URL sharing happening
between various security entities. However, it is important to
recognize that this only means that our measurements might
over-estimate a crawler’s infrastructure. This means that the
weaknesses of crawlers could in reality be more than mea-
sured. We experienced this during disclosure when certain
entities have conceded that the actual number of IP addresses
that they own is less than what we showed in our report.
Finally, we would like to point out the “double-edged
sword" nature of PhishPrint. While it can allow researchers
to study crawlers in a low-cost, highly scalable manner, at the
same time, it might allow attackers to host long-lasting evasive
USENIX Association
30th USENIX Security Symposium    3787
malicious websites at a low-cost. For this purpose, we have
made recommendations to monitor abuse of reporting APIs
to all crawlers in §5. If such monitoring does come into effect
as a result of this study, we would welcome that as another
positive security outcome. Furthermore, security researchers
can still seek special permissions to bypass such monitors and
continue their evaluation of crawlers in a low-cost manner.
Future Work. Given the low-cost and scalable nature
of PhishPrint, we would like to continue to use it to study
more cloaking vectors. In the future, we would like to study
the resilience of crawlers against some other ﬁngerprinting
vectors such as MediaDevices, Web Audio and Battery
and Sensor Web APIs. Furthermore, we also want to study
the potential of developing ML-driven cloaking attacks using
the behavioral biometrics aspects of crawlers.
Ethical Considerations. Our 70-day proﬁling study re-
sulted in submitting about 840 token URLs to each crawler at
the rate of 12 URLs per day. During the 2-week period when
our 20 phishing URLs were reported as well, this number
went up to 32 per crawler per day. While we concede that the
time spent in scanning these URLs is a waste for the crawlers,
we argue that this number is very small in comparison to
the huge number of URLs they receive each day. We have
also disclosed our URL submission frequency to all crawlers.
Moreover, our method of submitting token URLs to crawlers
to gain insights is similar to some prior works [37,39,40]. We
assess the impact of our token site submissions with Phish-
Tank as an example. With the help of PhishTank’s web portal
we were able to determine that our token URLs from both ex-
periments accounted for less than 0.8% of their total received
URLs during that period. We argue that the security beneﬁts
gained by the measurements from our study far outweigh this
minor overhead that the crawler vendors experienced during
our experimentation period. Some vendors have explicitly
mentioned the same and asked us to continue the study and
share new insights in the future. With regards to the simulated
phishing websites used during the experiments, we did not
share those URLs with any human users and only submitted
them to the crawlers. We also made sure that they are com-
pletely non-functional by removing all form submit buttons
in order to prevent effects of accidental exposure to users.
Similar efforts were also made previously [37, 40]. Finally,
we also obtained IRB exemptions for both our user studies.
7 Related Work
The closest works to PhishPrint are [32, 37] and par-
tially [39, 52] as all of them involved evaluating security
crawlers against cloaking attacks using simulated phishing
sites. In our research, we proposed an alternate highly scalable
solution that avoids phishing sites and instead directly relies
on proﬁling the crawlers to ﬁnd new cloaking weaknesses.
In Appendix C, we show that this alternate approach can
capture the same measurements as prior works. However, as
discussed in §6 (Limitations), while phishing experiments
work in all contexts, PhishPrint is restricted to measuring
only those cloaking weaknesses that can be gleaned from the
passive proﬁling information extracted from crawlers. As a
result, our design can be considered a complement that can
co-exist with the current phishing site-based approaches.
In terms of cloaking weaknesses, recent works such as [32]
and partially [52] have focused on testing the resilience of
crawlers against cloaking attacks powered by CAPTCHAs,
human-interaction detectors and basic browser ﬁngerprinting
techniques such as Cookies and Referer headers. In our
work, we found wide-spread anomalies in crawlers such as
artifacts that give away signs of browser automation and
incapacity to execute advanced Web API code. We also found
great limitations in the diversity of network infrastructure
(IP, AS space) as well as advanced ﬁngerprints (Canvas,
WebGL and JS-based Font) associated with crawlers. We
developed new cloaking attacks from these weaknesses. Note
that PhishPrint can also be easily re-deployed to evaluate
crawlers against many of these cloaking attacks in [32, 52]
(except timing attacks) in the future. Many research works
have focused on studying in-the-wild cloaking and evasive
techniques [23, 38, 39, 42, 49, 52] which was not our focus.
In order to collect and analyze the proﬁling data
from crawlers, we applied techniques studied in prior
works. [45, 46] have described techniques to detect browser
automation indicators and anomalies of privacy-preserving
browsers which we applied in our study to discover artifacts
of crawlers. Further, we also successfully applied advanced
browser ﬁngerprinting techniques described and developed
in [29, 36] to capture crawler ﬁngerprints. We also relied
on other works in browser ﬁngerprinting to analyze the
speciﬁcity [22, 29, 47] and propose suitable countermea-
sures [19, 28, 30, 35, 43, 44] for the crawlers. On a related
note, while we measured the applicability of ﬁngerprinting to
launch attacks on security crawlers, some recent works have
focused on a complementary question of how ﬁngerprinting
can be used to yield security beneﬁts [13, 27, 44, 48].
8 Conclusion
We
a
built
low-cost
novel, scalable,
framework
named PhishPrint
to enable the evaluation of web se-
curity crawlers against multiple cloaking attacks. PhishPrint
is unique in that it completely avoids the use of any simulated
phishing sites and instead relies on benign proﬁling pages.