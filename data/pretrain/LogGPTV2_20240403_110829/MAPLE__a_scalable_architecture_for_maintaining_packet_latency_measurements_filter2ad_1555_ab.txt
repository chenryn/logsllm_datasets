storing a packet hash (32 bits) and associated timestamp
(20 bits). For a million packets, we need about 50 Mbits
which is already quite expensive; for an OC-192 link, even
this storage will last only 0.2s unfortunately (assuming 5M
packets per second). Even if we use a slightly coarser times-
tamp (say 10 bits), it will reduce the memory need only
slightly. Besides, collision resolution techniques are not easy
to implement in a hardware in a pipelined fashion, and may
incur unpredictable insert/lookup times (e.g., depending on
the number of accesses required to ﬁnd an empty slot).
If we need to store precise latency values for each and
every packet, relying on hashtables is probably the best re-
course unfortunately. Luckily, for the kind of applications
we envision, such as performance diagnosis or detecting SLA
violations, we can exploit the fact that the latency values for
each and every packet need not be precise, and can be ap-
proximate instead. If we assume some amount of inaccuracy
Figure 1: MAPLE architecture.
mentioned before, there have been mainly two solutions,
LDA [26] and RLI [27], that have been proposed for ﬁne-
grained latency measurements in the recent past. (Another
solution for per-ﬂow latency measurements is Consistent Net-
Flow [28], but it is quite similar in spirit to RLI, and argu-
ments applicable to RLI are applicable there too.)
LDA provides only aggregate measurements and cannot
be modiﬁed simply to satisfy all the requirements listed be-
fore. For instance, it cannot satisfy R1. R2 could potentially
be satisﬁed by conﬁguring diﬀerent LDAs on a per-aggregate
basis, but this approach will not scale well, as all potential
aggregates need to be pre-conﬁgured, and there can be many
such aggregates. RLI provides per-ﬂow measurements, and
hence, it can satisfy R2 partially for aggregates that can be
obtained from individual per-ﬂow measurements. For exam-
ple, one can aggregate all the ﬂows per-preﬁx from per-ﬂow
measurements but cannot obtain ﬁner granularity measure-
ments than ﬂow, such as a packet (or ﬂow subsets). In ad-
dition, LDA and RLI cannot easily satisfy requirement R3,
because of the FIFO ordering assumption in LDA and tem-
poral delay correlation assumption in RLI, both of which
may not hold true across arbitrary measurement points.
2.3 Architecture
Given existing solutions fall short of satisfying the require-
ments we outlined earlier, we propose Measurement Archi-
tecture for Packet LatEncies (MAPLE). Our architecture
(shown in Figure 1) is based on three key ideas: First, in
order to satisfy requirement R1, we need MAPLE to store
per-packet latency measurements in some scalable way; any
form of aggregation within routers cannot be used to satisfy
R1. Thus, at its heart, MAPLE contains a scalable packet
latency store (called PLS) designed to simply store laten-
cies of all packets in a scalable and eﬃcient fashion. Since
storing all packets will mean signiﬁcant storage requirement
that will be prohibitively expensive if not technologically in-
feasible, it stores packet latencies only for a small amount
of time, τ , say 1-100s, in high speed memory. Every τ sec-
onds, the store will be (optionally) ﬂushed to an oﬀ-chip
BDAC1) Packet Latency Store2) Query EngineQueryResponseQ(P1)Query packet initiated by the end hostQuery responsePacket StreamP2P1A(P1)TSTSP2P1Packet StreamP2P1TSTS3) TimestampUnit103is tolerable, then we can signiﬁcantly reduce the memory
usage—this is the key intuition behind our approach.
Our approach In our approach, we exploit two key ideas.
First, within a given measurement interval, there are typ-
ically only a few dominant latency values (depending on
the utilization) where most of the packet latencies are clus-
tered. In the worst case, the latency values can be all over
the entire permissible range, but in general, this is typically
rare. Thus, instead of storing packets and their associated
latencies, we can ﬁrst cluster packets into equivalence classes
based on the delay values, and associated a single delay
value, called cluster center, for all packets within the clus-
ter. Second, for each cluster, we can leverage approximate
membership query data structures such as Bloom ﬁlters [10],
that have gained signiﬁcant prominence in networking ap-
plications recently, for better eﬃciency in storage (in terms
of bits/packet) as well as implementation in hardware (just
a bit vector and few hash functions). We discuss these in
more detail next.
3.1 Selecting representative delays
Depending on whether the clusters are chosen statically
or dynamically, there are two broad choices for selecting the
cluster centers. For the static case, we consider logarith-
mic center selection, while we explore online clustering al-
gorithms (k-means and k-medians) for determining centers
dynamically.
Logarithmic delay selection.
In this method, we ﬁrst
select a range of latency values that packets can experience,
and then divide this range into logarithmic sub-ranges. For
instance, if the delay range is 0.1-10,000 µs, we have 5 sub-
ranges; 0.1-1 µs, 1-10 µs, and so on.
If we have γ sub-
ranges, we assign k/γ representative delays linearly for each
sub-range.
If k = 50, and γ = 5, each sub-range assigns
10 representative delays linearly. While this method does
not take the pattern of delays into account, the complexity
of choosing representative delays is minimal. Because the
distance between two center delays in a sub-range is equal,
the relative and absolute error of a packet latency estimate
remains bounded and stable regardless of packet delay distri-
butions. However, accuracy may not be close to the optimal
accuracy as we can obtain with given k delay centers.
k-means and k-medians clustering. If we know distri-
bution of packet delays in an interval a priori, selecting rep-
resentative delays can be formulated as a clustering problem.
In literature, there are two broad classes of algorithms—k-
means and k-medians—that can help determine good cluster
centers [23]. Typically, both types of algorithms minimize
the average absolute error of packet latencies, because they
choose centers that minimize total sum of distances between
each member with its closest center.
Formally, for observations x1, x2, . . . , xn, the k-means al-
gorithm aims to partition them into k sets, S1, . . . , Sk, k ≤
n, so as to minimize the sum of squares of distances within
cluster from the center (mean), i.e.,
||xj − µi||2
k(cid:88)
(cid:88)
arg min
{µi}
i=1
xj∈Si
where µi is the mean of the cluster Si. k-medians clustering
algorithm minimizes the distance to the median of a clus-
ter as opposed to the least-squares distance that k-means
Figure 2: Architecture for streaming representative
delay selection and storage provisioning.
obtains. The advantage of k-medians is that it is more re-
silient to outliers which have too large or small values.
There are two key concerns with using these algorithms
directly in our setting though. First, the basic algorithms
cannot be directly implemented in a streaming fashion due
to their high run-time complexity, O(nk+1 log n). There ex-
ist heuristics such as the classic Lloyd’s algorithm [30], but
still it can be quite computationally intensive. Second, the
centers are determined after running the algorithm on all the
packets in a given measurement interval, but we need the
centers to be determined before the packets start streaming
in. We discuss how to address these issues next.
3.2 Streaming clustering
In order to address the ﬁrst problem concerning the high
run-time complexity, we use a streaming version of k-medians
clustering algorithm in our architecture. For the second
problem of lagged availability of centers, we use a pipelined
architecture, where computed centers from a previous epoch
are used to cluster packets for this epoch.
Online version of clustering algorithm. There exist a
few time-eﬃcient k-medians clustering algorithms [12, 22,
21] in literature.
In our architecture, we leverage an on-
line clustering algorithm proposed in [12] because the algo-
rithm makes no assumption about the characteristics of the
streaming data and is space-eﬃcient. We need to make sev-
eral modiﬁcations to make the algorithm more eﬃcient than
the version in [12]. We describe brieﬂy how the algorithm
works and our modiﬁcations to speed up the algorithm next.
Given a stream of n data points and k centers we wish
to ﬁnd, this algorithm consists of two stages—online and of-
ﬂine clustering. At a high level, the online stage works in
many (not necessarily equal) phases over the entire epoch to
ﬁnd O(k log n) candidate medians that the oﬄine clustering
stage subsequently reduces to k centers. In each phase, it
uses Meyerson’s online facility location algorithm [32], and
chooses to open a new center with probability δ/f , where δ
is the distance of the current point x to the closest already-
open center, and f is the cost. In this algorithm, cost f is
L/(k(1 + log n)), where L is the lower bound cost of the op-
timal. Note that L is reﬁned at the beginning of every phase
by multiplying the previous value of L by a ﬁxed constant β.
(Refer to the PolyLogarithmic Space algorithm in page 4 of
[12] for the exact description.) The current phase terminates
if either the number of opened centers or the associated cost
OnlineClusteringStageOfflineClusteringStagePacket streamPacketSamplingPackets in (i+2)th epochk-centersStorage Data StructureDRAM/SSDFlushed after every epoch for archivalDataO(klog(np)) centersat (i+1)th epochnp packets at i-th epochHARDWARESOFTWARE104function exceeds some threshold (details in [12]). The algo-
rithm terminates when all the packets are consumed, and
leaves behind a set of O(k log n) candidate medians.
We make a few modiﬁcations to the original algorithm to
contain the run-time complexity. First, since speed is criti-
cal, we use only one thread instead of 2 log n parallel threads
in the original algorithm in [12] (see PARA-CLUSTER in
page 4 of the paper). Second, the online algorithm requires
searching for the closest existing center out of O(k log n) cen-
ters in each phase, which is hard even for small k to do in 1
cycle. We therefore run the online algorithm only on sam-
pled packets; a 10% sampling rate trivially gives 10 cycles to
do these lookups. For k = 50 and n = 400, 000, we observe
about 1000 centers which can be looked up with a balanced
binary tree using 10 memory accesses. We observe in our
evaluations that 1-10% sampling rate has virtually no eﬀect
on the quality of the centers produced by the algorithm.
Handling lagged availability of centers. The problem
here is that we cannot compute the centers and cluster on
the same packet stream in one pass. Besides, the streaming
algorithm itself operates in two stages, online and oﬄine.
To address this problem, we design a three-stage pipeline
consisting of the following stages to handle this issue: The
ﬁrst stage consists of the online clustering algorithm that
computes the O(k log n) centers that operates on packets in
epoch i. The second stage consists of the oﬄine clustering
which will result in k centers by consuming these O(k log n)
centers. Finally, we cluster the packets in epoch i+2 depend-
ing on the closest center that matches the packet’s latency
in the ﬁnal stage. Since these stages operate in a pipelined
fashion, the centers computed will be based on the dynam-
ics of packets computed two epochs back. Assuming some
amount of stationarity across measurement intervals (our
evaluation shows holds true in practice), this pipelined ap-
proach should work well. However, to cover for the worst
case where these centers may be signiﬁcantly dissimilar to
each other, we propose a hybrid clustering approach that
combines static allocation with dynamic allocation, which
we explain next.
Hybrid clustering. The basic idea of hybrid clustering
is to choose k/2 centers with logarithmic clustering and re-
maining k/2 centers are computed by streaming k-medians
algorithm. To enable hybrid clustering algorithm, we make
two more modiﬁcations in the online version of clustering
algorithm explained earlier. First, at the online clustering
stage, the centers chosen by the logarithmic clustering are
always selected as a new center in each phase and the num-
ber of data points added to the centers is incremented by
one. Second, when O(k log(np)) candidate centers (where p
is sampling rate) need to be processed at the oﬄine cluster-
ing stage, we exclude the k/2 centers picked by the logarith-
mic clustering from O(k log(np)) centers, the rest candidate
centers are fed into the oﬄine algorithm, and ﬁnally k/2
centers are obtained. While we choose to split the total
number of centers equally between static and dynamic allo-
cation schemes, this equal split is somewhat arbitrary and
other variants (e.g., 2/3-1/3 split) could also work equally
well (although we have not explored this thoroughly yet).
3.3 Storage provisioning
So far, we have reduced the problem of storing  tu-
ple to  where l is the actual latency of packet s and
ci is the ith center (0 ≤ i < k). Once the k representative
delays are selected by the clustering algorithm, we need to
determine how much storage is required to store packet la-
tencies. Depending on the data structure that one uses, the
actual required memory size can be diﬀerent. Note that the
goal of the data structures is essentially to store and look up
the center id corresponding to a packet; the actual latency
value corresponding to the center will need to be looked up
in a separate table. During the lookup phase, instead of re-
turning the static latency value corresponding to the center,
we can dynamically return the actual mean of all the packets
that map to a given center. Implementing this would require
essentially two additional counters per center (latency sum
and packet counts). We call these reﬁned latency estimates.
3.3.1 Naive approach: PBF
Given these k cluster centers, we can now simply match
each incoming packet latency value to determine the right
center, and for each center maintain a separate Bloom ﬁlter
(BF) in which we record the packet’s presence. This naive
and intuitive data structure called Partitioned Bloom Filter
(PBF) as shown in Figure 3(a).
Insert To store a packet and its latency, it ﬁnds the right
BF corresponding to the latency value by performing a clos-
est center match in parallel (shown in Figure 3(a)). Since
the number of centers k is quite small, doing this in parallel
in hardware should be relatively easy to do. It then accesses
the BF corresponding to this center, and inserts it into the
BF just like a regular BF insert, i.e., by hashing using mul-
tiple hash functions and setting the bits indexed by the hash
values to 1. Since BF inserts are O(1), PBF insert time is
O(1), ignoring the small hardware cost for parallel match of
the packet latency with the various centers.