choosing di(cid:11)erent levels of replication for di(cid:11)erent objects,
the average lookup performance of the system can be tuned
to any desired constant. Naturally, replicating every ob-
ject at every node would achieve O(1) lookups, but would
incur excessive space overhead, consume signi(cid:12)cant band-
width and lead to large update latencies. Beehive minimizes
bandwidth and space consumption by posing the following
optimization problem: minimize the total number of replicas
subject to the constraint that the aggregate lookup latency is
less than a desired constant C. For power-law (or Zipf-like)
query distributions, Beehive analytically derives the optimal
closed-form solution to this problem. The derivation of the
analytical solution is provided in [32]; the (cid:12)nal expression
for the closed-form solution that minimizes the total number
of replicas for Zipf-like query distributions with parameter
(cid:11) < 1 is the following:
xi = [
di(logN (cid:0) C)
1 + d + (cid:1) (cid:1) (cid:1) + dlogN (cid:0)1 ]
1
1(cid:0)(cid:11) ; where d = b
1(cid:0)(cid:11)
(cid:11)
In this expression, b is the base of the underlying DHT and
xi is the fraction of most popular objects that get replicated
at level i. This solution is immediately applicable to DNS,
since DNS queries follow a Zipf-like distribution [18].
The analytical result provides several properties suited
for latency-sensitive applications such as DNS. First, it suc-
cinctly captures the space-time tradeo(cid:11) and enables appli-
cations to achieve any targeted average lookup latency by
selecting an appropriate C. In CoDoNS, we set Beehiveâ€™s
target as C = 0:5 hops, which means that a large percentage
of requests are answered immediately without having to take
any extra network hops. Second, it incurs minimal band-
width and storage overhead by picking the optimal num-
ber of replicas required to achieve the target lookup perfor-
mance. Further, replicating objects across several nodes bal-
ances the load on individual Beehive nodes, reduces hotspots
and improves resilience against DoS attacks. Finally, the
level of replication for each object can be used to quickly
determine the location of all object replicas, and to update
them when necessary.
Beehive nodes need to know only the Zipf parameter and
the relative popularity rank of objects to apply the closed-
form solution and determine the extent of replication for
each object. Beehive employs a combination of local mea-
surement and limited aggregation to estimate the Zipf pa-
rameter and the popularity ranks of objects. Beehive nodes
locally measure the access frequency of each object, and pe-
riodically aggregate them with other nodes every aggrega-
tion interval. Each node aggregates values gathered from
nodes one level higher in the routing table. Eventually,
the aggregates trickle down through di(cid:11)erent levels of the
routing table and reach the home node. The home node
computes a (cid:12)nal aggregate and propagates it to all replicas
in the system. The Zipf parameter is locally derived from
the aggregate access frequencies of the object and fed to
the analytical model. Using these estimates, each Beehive
node invokes the analytical model once every analysis inter-
val and obtains the appropriate replication levels for objects
it stores. The replication of these objects to the speci(cid:12)ed
levels is then performed by the replication protocol. Repli-
cation in Beehive is controlled locally; that is, each node is
responsible for replicating an object on nodes at most one
hop away from itself. For example, the home node of a pop-
ular object replicates it at nodes that share one pre(cid:12)x less.
Those nodes then independently decide to replicate that ob-
ject further to one more level. In the replication phase, each
node exchanges messages with nodes in its routing table,
which are one hop away from them, to push, delete, or up-
date replicas for which they are responsible.
The aggregation and replication protocols enable Beehive
to quickly detect changes in the demand for objects. Large
changes in the popularity of domain names occur during
denial of service attacks and (cid:13)ash crowds. Beehive nodes
constantly monitor the access frequency of objects and ad-
just the extent of replication. In response to DoS attacks,
they promptly increase the number of replicas and spread
the load among several nodes, curbing the attack.
Proactive replication also enables Beehive to rapidly push
updates to all the replicas in the system. In general, pro-
active propagation of updates demands expensive mecha-
nisms to keep track of all the nodes where the object is
cached. Beehive requires just a small integer, the replica-
tion level of an object, to determine the range of nodes with
replicas of the object. An object at level i is replicated at all
nodes with i matching pre(cid:12)x digits. For a level i object, the
home node propagates updates to all the nodes at level i in
the routing table. Those nodes in turn propagate updates
to nodes at level i + 1. This recursive process disseminates
the updates to nodes with i matching pre(cid:12)x digits. Nodes
in the process of joining the DHT may miss the initial up-
date propagation. Such nodes will receive the latest copy
of the object in the next replication phase; they may in-
cur a slight performance penalty, but will not serve stale
data. Proactive update propagation obviates the need for
timeout-based caching.
3.2 CoDoNS: Architecture
CoDoNS consists of globally distributed nodes that self
organize to form a peer-to-peer network. We envisage that
each institution would contribute one or more servers to
CoDoNS, forming a large-scale, cooperative, globally shared
DNS cache. CoDoNS provides query resolution services to
clients using the same wire format and protocol as legacy
DNS, and thus requires no changes to client resolvers.
CoDoNS decouples namespace management from query
resolution of the legacy DNS. Nameowners need only to pur-
chase certi(cid:12)cates for names from namespace operators and
introduce them into CoDoNS; they do not need to provide
dedicated hosts for serving those names. CoDoNS places
no restrictions on the hierarchical structure of the names-
pace and is agnostic about the administrative policies of the
nameowners. To the nameowners, CoDoNS provides an in-
terface consisting of insert, delete and update.
CoDoNS associates the node whose identi(cid:12)er is closest to
the consistent hash [20] of the domain name as the home
node for that domain name. The home node stores a per-
manent copy of the resource records owned by that domain
name and manages their replication. If the home node fails,
the next closest node in the identi(cid:12)er space automatically
becomes the new home node. CoDoNS replicates all records
on several nodes adjacent to the home node in the identi(cid:12)er
space in order to avoid data loss due to node failures.
Replacing the DNS entirely with CoDoNS is an ambitious
plan, and we do not expect nameowners to immediately use
CoDoNS for propagating their information. In order to al-
low CoDoNS to gradually grow into a globally recognized
system, we have incorporated compatibility to the legacy
DNS. CoDoNS uses the legacy DNS to resolve queries for
records not explicity inserted by nameowners. The home
node retrieves resource records from the legacy DNS upon
the (cid:12)rst query for those records. The additional redirection
latency only a(cid:11)ects the (cid:12)rst query issued in the entire sys-
tem for a domain name. CoDoNS decreases the impact of
query redirection on lookup performance, by bootstrapping
the system with records obtained from legacy DNS name-
servers through zone transfers or (cid:12)le transfers.
Overall, query resolution in CoDoNS takes place as fol-
lows. Client sends a query in the wire format of the legacy
codons
server
legacy
DNS
reply from
home node
home
node
client
query
reply
cached
reply
Figure 4: CoDoNS Deployment: CoDoNS servers self-
organize to form a peer-to-peer network. Clients send
DNS requests to a local CoDoNS server, which obtains
the records from the home node or an intermediate node,
and responds to the client. In the background, the home
nodes interact with the legacy DNS to keep records fresh
and propagate updates to cached copies.
DNS to the local CoDoNS server in the same administrative
domain. The local CoDoNS server replies immediately if it
has a cached copy of the requested records. Otherwise, it
routes the query internally in the CoDoNS network using
the under-lying DHT. The routing terminates either at an
intermediate CoDoNS node that has a cached copy of the
records or at the home node of the domain name. The home
node retrieves the records from the legacy DNS, if it does not
already have it, and sends a response to the (cid:12)rst contacted
CoDoNS server, which replies to the client.
In the back-
ground, CoDoNS nodes proactively replicate the records in
based on the measured popularity. Figure 4 shows a typical
deployment of CoDoNS and illustrates the process of query
resolution.
Clients generate a large number of queries for names in
their local administrative domain. Since the home node of a
name may be located in a di(cid:11)erent domain, local queries can
incur extra latency and impose load on wide-area network
links. CoDoNS supports e(cid:14)cient resolution of local names
through direct caching. Nameowners can directly insert, up-
date, and delete their records at CoDoNS servers in their ad-
ministrative domain, and con(cid:12)gure the local CoDoNS servers
to use the direct cache for replying to local queries.
3.3 CoDoNS: Implementation
CoDoNS servers are layered on top of Pastry and Bee-
hive. Each CoDoNS server implements a complete, recur-
sive, caching DNS resolver and supports all requirements de-
scribed in the speci(cid:12)cation [25, 26]. CoDoNS also supports
inverse queries that map IP addresses to a domain name by
inserting reverse address-name records into the DHT when
name-address records are introduced.
Domain names in CoDoNS have unique 128 bit identi(cid:12)ers
obtained through the SHA-1 hashing algorithm. The home
node, the closest node in the identi(cid:12)er space, stores perma-
nent copies of the resource records of the domain name and
maintains their consistency in the system. Since CoDoNS
does not associate TTLs with the records, the home nodes
push the updates to all replicas in the system, which retain
them until the replication level of the record is downgraded,
or until an update is received. Nameowners insert updated
resource records into CoDoNS, and the home nodes proac-
tively propagate the updates.
CoDoNS ensure the consistency of records obtained from
the legacy DNS, CoDoNS by proactively refetching them.
The home node uses the TTL speci(cid:12)ed by the legacy DNS
as the duration to store the records. It refetches the records
from legacy DNS after TTL duration, and propagates the
updated records to all the replicas if the records change.
Since CoDoNS performs the refetches in the background, its
lookup performance is not a(cid:11)ected. The TTL values are
rounded up to a minimum of thirty seconds; records with
lower TTL values are not placed into the system. Low TTL
values typically indicate dynamic server-selection in legacy
DNS. The home node prompts the server that injected the
query to consult the legacy DNS server by issuing a spe-
cial error-response. This redirection of queries for low-TTL
records ensures that services that rely on dynamic server se-
lection will continue to work, and reduces overhead on the
CoDoNS home nodes.
The legacy DNS relies on error responses, called NXDO-
MAIN responses, to detect names that do not exist. Since
clients reissue a request several times when they do not
receive prompt replies, the DNS speci(cid:12)cation recommends
that resolvers cache NXDOMAIN responses. CoDoNS pro-
vides complete support for negative caching as described
in [1]. However, permanently storing NXDOMAIN responses
could exhaust the capacity of the system, since an unlim-
ited number of queries can be generated for non-existent do-
mains. Hence, CoDoNS nodes cache NXDOMAIN responses
temporarily and do not refresh them upon expiry.
3.4 Issues and Implications
CoDoNS decouples namespace management from the phys-
ical location of nameservers in the network. Instead of re-
lying on physical delegations to trusted hosts and assum-
ing that Internet routing is secure, CoDoNS uses crypto-
graphic delegations and self-verifying records based on the
DNSSEC [12] standard.
DNSSEC uses public key cryptography to enable authen-
tication of resource records. Every namespace operator has
a public-private key pair; the private key is used to digitally
sign DNS records managed by that operator, and the corre-
sponding public key is in turn certi(cid:12)ed by a signature from
a domain higher up in the hierarchy. This process creates a
chain of certi(cid:12)cates, terminating at a small number of well-
known public keys for globally trusted authorities. Since
records are signed at the time of issue, the private keys need
not be kept online. The signature and the public key are
stored in DNS as resource records of type sig and key re-
spectively. Clients can verify the authenticity of a resource
record by fetching the sig record and the key record from
the DNS.
The use of cryptographic certi(cid:12)cates enables any client to
check the verity of a record independently, and keeps peers
in the network from forging certi(cid:12)cates. To speed up cer-
ti(cid:12)cate veri(cid:12)cation, CoDoNS servers cache the certi(cid:12)cates
along with the resource records and provide them to the
clients. Existing clients that are not DNSSEC compliant
need to trust only the local CoDoNS servers within their ad-
ministrative domain, since CoDoNS servers internally verify
data fetched from other nodes.
CoDoNS authenticates nameowners directly through cer-
ti(cid:12)cates provided for every insertion, delete, and update.
Insertions simply require a signed resource record with a
corresponding well-formed certi(cid:12)cate. A version number as-
sociated with each record, signed by the owner and checked
by every server, ensures that old records cannot be reintro-
duced into the system. Deletions require a signed request
that identi(cid:12)es the record to be expunged, while updates in-
troduce a new signed, self-verifying record that replaces the
now-stale version.
Since CoDoNS removes authority from the identity and
location of the server providing resource records and vests it
with cryptographic keys, it provides a greater degree of free-
dom over namespace management. CoDoNS enables multi-
ple namespace operators to manage the same part of the
name hierarchy. A domain owner can delegate management
of the same sub-domain to multiple operators by endors-
ing their keys as being authoritative for that sub-domain.
Ideally, competing operators would preserve a single consis-
tent namespace by issuing names out of a common, shared
pool. In the presence of con(cid:13)icting or inconsistent records,
clients simply pick the records signed by an operator they
trust, similar to the way they pick between separate sets of
root servers today. Essentially, nameowners have the abil-
ity to choose the namespace operator that will endorse their
records based on price, service and functionality.
Since DNSSEC has not yet been widely deployed in the
Internet, CoDoNS cannot rely on the legacy DNS to provide
certi(cid:12)cates for resource records. Consequently, CoDoNS
uses its own centralized authority to sign resource records
fetched from the legacy DNS. Queries to the legacy DNS
are directed to a small pool of certifying resolvers, which
fetch authoritative resource records from the legacy DNS,
sign them, and append the sig records to the legacy DNS
response. This approach requires trust to be placed in the
certifying resolvers. Threshold cryptography [43] can be
used to limit the impact of adversaries on these resolvers
until CoDoNS takes over completely. The certifying name
resolvers ensure that CoDoNS participants cannot inject cor-
rupted records into the system.
Malicious participants may also disrupt the system by cor-
rupting the routing tables of peers and misrouting or drop-
ping queries. Castro et al. [7] propose a method to handle
routing table corruptions in DHTs. This scheme augments
the regular routing table with a secure routing table where
the entries need to satisfy strict constraints on node iden-
ti(cid:12)ers that limit the impact of corrupt nodes. Since nodes
in the secure routing table are not picked based on short
network latencies, this scheme may increase the lookup de-
lay. Setting a lower target latency at the Beehive layer can
compensate for the increase in lookup latency at the cost of
bandwidth and storage.
CoDoNS acts as a large cache for stored, self-verifying
records. This design, which separates namespace manage-
ment from the physical servers, prohibits dynamic name res-
olution techniques where the mapping is determined as a
result of a complex function, evaluated at run time. In the
general case, such functions take arbitrary inputs and have
con(cid:12)dentiality requirements that may prohibit them from
being shipped into the system. For instance, content distri-
bution networks, such as Akamai, use proprietary techniques
to direct clients to servers [3, 35]. To nevertheless support
such dynamic mapping techniques, CoDoNS enables name-
Parameter
Value
Pastry
Beehive
base
leaf-set size
target C