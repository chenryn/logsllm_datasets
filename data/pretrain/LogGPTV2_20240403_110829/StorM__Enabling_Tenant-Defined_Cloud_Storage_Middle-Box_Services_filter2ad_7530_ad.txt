To measure the storage service overhead, we implemented a "stream cipher" service within the middle-box, which processes each bit of raw data (details in Section V-B). We then used Fio to generate I/O as previously described. We compared three approaches: 

1. **MB-PASSIVE-RELAY**: This is the passive-relay approach (described in Section III-B).
2. **MB-ACTIVE-RELAY**: This is the (default) active-relay approach.
3. **MB-FWD**: This approach involves no processing inside the middle-box.

As expected, Figure 5 shows that MB-PASSIVE-RELAY added additional overhead on top of the MB-FWD overhead, ranging from 3% to 13% as the I/O size increased from 4 KB to 256 KB. This overhead was due to the extra computation and packet delays, leading to lower performance. Larger I/O sizes resulted in more overhead because the performance degradation aggregated across all packets in the large I/O request. These results further justify the need for StorM’s active-relay approach.

In contrast, our proposed MB-ACTIVE-RELAY approach achieved the same performance as MB-FWD for small I/O sizes (e.g., 4 KB and 16 KB) and better performance for larger I/O sizes. Figure 5 shows a 14% performance improvement when the I/O size was 256 KB. This improvement is attributed to the active-relay approach, which shortens the packet acknowledgment path from four hops to one hop. Compared with MB-FWD, Figure 8 shows that the average I/O latency of MB-ACTIVE-RELAY was reduced by 11% for an I/O size of 256 KB.

In Figure 6, we increased the Fio thread number from 4 to 32 to simulate parallelism in the tenant's application. We observed that, compared with MB-FWD, the IOPS number of MB-ACTIVE-RELAY increased by 39% when the workload had 32 threads. Similarly, Figure 9 shows that the average I/O latency of MB-ACTIVE-RELAY was reduced by 30%. In this case, even compared with LEGACY, the overhead caused by MB-ACTIVE-RELAY was much less than 10%.

In summary, the packet routing for storage middle-boxes introduces up to 18% performance overhead in the worst case. However, this overhead can be mitigated using StorM’s active-relay approach, which shortens the packet acknowledgment path. Compared with the LEGACY case, the overall performance overhead under MB-ACTIVE-RELAY is less than 10% in all of our measurements.

### B. Security/Reliability Service Case Studies

In this section, we provide detailed designs and evaluations of three storage services built as middle-boxes. These services demonstrate the effectiveness of StorM in enabling seamless development and transparent deployment of tenant-defined storage security and reliability services.

#### 1. Case 1: Storage Access Monitor

The goal of the storage access monitor is to allow tenants to set alerts on sensitive files and directories. The middle-box logs all accesses made to these marked resources. Tenants can either periodically request the logs or set policies inside the middle-box to be directly notified of any access. This service is crucial because even if tenant VMs are compromised and malware attempts to access sensitive data, those accesses will be logged by the storage monitor. The access logs can be used for post-attack investigations and to detect malware in the future.

A storage service executing inside a middle-box can only observe low-level block I/O accesses. Therefore, the storage access monitor must reconstruct high-level operations from low-level block accesses to enforce tenant policies. As described in Section III-C, StorM provides semantic reconstruction APIs. Using this service, we built a monitoring engine, a multi-threaded daemon running inside the middle-box, which performs three steps:

1. **Classification**: Determines whether an access is to a file’s content or metadata using the file system view provided by StorM.
2. **Update**: Updates StorM’s file system view from intercepted metadata.
3. **Analysis**: Logs (or raises an alarm) for accesses to monitored files or directories.

**Synthetic Attack Scenario**

To demonstrate the accuracy and usefulness of the storage access monitor, we present a synthetic use case. An iSCSI volume was attached to a tenant VM and mounted under “/mnt/box”. The volume was formatted to Linux Ext4, with 10 folders ("name0" to "name9") each containing ten files from "1.img" to "10.img". We attached the monitoring middle-box to the tenant VM and issued file operations; two of them are shown in Table II. With the help of the monitoring engine, these file operations were successfully reconstructed and logged in the middle-box, as shown in Table I.

We observed that a high-level file operation in the tenant VM may generate several block-level accesses captured by the monitoring middle-box. For example, reading "7.img" in the "name9" directory involves reading the inode metadata and data blocks. We also observed that written messages could be cached in the VM’s local buffer, causing the block-level I/O access sequence to differ from the file I/Os. The write operations may delay all read operations. In addition to logging sensitive file accesses, this monitor provides detailed file system level activities, useful for debugging abnormal system behaviors and optimizing file system performance.

**Real-world Malware Scenario**

We applied the storage access monitor to study the behavior of HEUR:Backdoor.Linux.Ganiw.a, a Linux backdoor Trojan detected by Kaspersky in 2015. When the malware was executed with root privileges, the storage monitor observed the creation of several files and directories, as listed in Table III. The malware installed startup scripts in "/etc/init.d" and linked them to different system run levels (1-5). It also replaced "selinux" with its own copy and tried to launch it at system startup. To hide from checks, the malware replaced several system tools such as "netstat", "ps", "lsof", and "ss" with trojan versions.

In addition to detecting created files during the malware’s installation process, the storage monitor also observed several important files read by the malware, such as the GeoIP database and Python files. The revealed file access patterns can be used by the middle-box for future detection of the same malware.

#### 2. Case 2: Data Encryption

To allow tenants to keep their data confidential, we implemented a storage encryption middle-box. The goal is to encrypt tenant data before it is written to the disk and decrypt it when requested. Implementing this functionality inside a middle-box offers flexibility to tenants to decide when and how encryption should be performed, as opposed to relying on the cloud provider.

We implemented a widely used block cipher in the encryption middle-box using dm-crypt, a well-known disk encryption subsystem in the Linux kernel. By passing tenants’ storage flows to the encryption middle-box, data encryption and decryption for the corresponding tenant VMs’ volumes was easily achieved.

We compared the middle-box encryption solution with a traditional tenant-side encryption solution (by installing the encryption system in the tenant VM). Both solutions used the same AES cipher with 256-bit keys. A 20 GB volume was created and attached to the tenant VM for both scenarios. Note that client-side encryption requires configuring the volume’s format, while the middle-box solution is transparent to the tenant VM.

To test the decryption and encryption, we ran a simple FTP server in the tenant VM to download/upload a large file from/to the attached volume. Both solutions nearly reached the maximum storage bandwidth, with the tenant-side solution achieving ~88 MB/s and the middle-box solution achieving ~84 MB/s. Interestingly, the middle-box solution led to much lower overall CPU resource utilization, as shown in Figure 10. The overall CPU utilization of the middle-box solution was the sum of the CPU utilization from the tenant VM, the middle-box VM, and the storage target. In contrast, the tenant-side solution only involved the tenant VM and the storage target.

The middle-box solution reduced the overall CPU utilization by 20% due to the CPU savings in the tenant VM. To confirm this, we used a more realistic application workload, as shown in Figure 11.