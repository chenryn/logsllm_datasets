demonstrate a measurable storage service overhead, we ran
a “stream cipher” service inside the middle-box, which
operates on each bit of the raw data (the details of the
service are described in Section V-B). We then used the
Fio to generate I/O as described previously. Here, we com-
pared three approaches: MB-PASSIVE-RELAY refers to as
the passive-relay approach (as described in Section III-B),
MB-ACTIVE-RELAY refers to the (default) active-relay
approach, and MB-FWD as described previously (with no
processing inside the middle-box).
Conﬁrming our previous intuition, Figure 5 shows that
MB-PASSIVE-RELAY added additional overhead on top
of the MB-FWD overhead, ranging from 3% to 13% as
the I/O size increased from 4 KB to 256 KB. This was
caused by the extra computation added to the packets delays,
resulting in the low performance. Larger I/O size results in
more overhead for of the same reason — the performance
degradation aggregates from all packets of the large I/O
request. These results further justify the need for StorM’s
active-relay approach.
In contrast, our proposed MB-ACTIVE-RELAY approach
led to the same performance as MB-FWD when the I/O size
was small (e.g., 4 KB and 16 KB), and better performance
when the I/O size became larger. Figure 5 shows a 14% per-
formance improvement when the I/O size was 256 KB. The
reason for this improvement in performance is because the
active-relay approach shortens the packet acknowledgment
path — reduced from four hops to only one hop. Compared
with MB-FWD, Figure 8 shows the average I/O latency of
MB-ACTIVE-RELAY reduced by 11% with the I/O size of
256 KB.
In Figure 6, we increased the Fio thread number from
4 to 32 to simulate parallelism in the tenant’s application.
We observed that, compared with MB-FWD,
the IOPS
number of MB-ACTIVE-RELAY increased by 39% when
the workload had 32 threads. Likewise, Figure 9 shows that
the average I/O latency of MB-ACTIVE-RELAY reduced
by 30%. In this case, even compared with LEGACY, the
overhead caused by MB-ACTIVE-RELAY was much less
than 10%.
In summary, the packet routing for storage middle-boxes
introduces up to 18% performance overhead in the worst
case, but
this overhead can be mitigated using StorM’s
active-relay approach, which shortens the packet acknowl-
edgment path. Compared with the LEGACY case, the overall
performance overhead under MB-ACTIVE-RELAY is less
than 10% in all of our measurements.
80
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:39:28 UTC from IEEE Xplore.  Restrictions apply. 
I/O Access ID
1
2∼34
35∗
...
71∗∗
72∗∗
...
287∗
...
294∗
295∗
296
Operation
read
read
read
File
/mnt/box/.
META: inode_group_90
/mnt/box/name1/.
Size
4096
4096
4096
read
write
write
write
write
write
/mnt/box/name9/.
/mnt/box/name9/7.img
4096
16384
/mnt/box/name1/1.img
32768
mnt/box/name1/1.img
/mnt/box/name1/1.img
inode_group_90
4096
24576
4096
Table I: Reconstructing the high-level ﬁle operations from
the block-level accesses.
B. Security/Reliability Service Case Studies
In this section, we provide the detailed design and eval-
uation of three storage services that we have built to offer
as middle-boxes. These services show the efﬁcacy of StorM
that allows seamless development and transparent deploy-
ment of tenant-deﬁned storage security/reliability services.
1) Case 1: Storage Access Monitor: The goal of the
storage access monitor is to allow tenants to set an alert on
sensitive ﬁles and directories, and the middle-box will log
all accesses made to these marked resources. Tenants can
either periodically request the logs created by the middle-
box to see if any unauthorized attempts are made on it or
set the policies inside the middle-box to be directly notiﬁed
on any access. This middle-box provides a crucial service to
tenants: even if tenant VMs are compromised and malware
attempts to access the sensitive data, those accesses will be
logged by the storage monitor inside the middle-box. The
access logs can be used to perform post-attack investigation,
and access pattern can be used to detect such malware in
future.
A storage service executing inside a middle-box can only
observe low-level block I/O accesses, hence the storage
access monitor must reconstruct the high-level operations
from the low-level block accesses in order to enforce tenant
policies expressed in ﬁles and directories. As described
in Section III-C, StorM provides the middle-boxes with
semantic reconstruction APIs. Using this service, we built a
monitoring engine, a multi-threaded daemon running inside
the middle-box, which performs three steps: The ﬁrst step
is Classiﬁcation, where the classiﬁcation process determines
whether an access is to a ﬁle’s content or metadata using the
ﬁle system view provided by StorM. The Update phase asks
StorM to update its ﬁle system view from any intercepted
metadata. Lastly, the Analysis phase logs (or raises an alarm)
accesses to monitored ﬁles or directories.
Synthetic Attack Scenario To demonstrate the accuracy
and usefulness of the storage access monitor, we ﬁrst present
a synthetic use case. An iSCSI volume was attached to a
tenant VM, and mounted under “/mnt/box”. The volume
ID
1∗
2∗∗
Operation
write
read
File
/mnt/box/name1/1.img
/mnt/box/name9/7.img
Size
4096
4096
Table II: File operations in the tenant VM
was formatted to Linux Ext4, where 10 folders, “name0”
to “name9”, were created. Each folder contained ten ﬁles
from “1.img” to “10.img”. We then attached the monitoring
middle-box to the tenant VM, and ﬁle operations were issued
in the tenant VM; two of them are shown in Table II. With
the help of the monitoring engine, these ﬁle operations are
successfully reconstructed and logged in the middle-box as
shown in Table I.
We observed that a high-level ﬁle operation in the tenant
VM may generate several block-level accesses captured by
the monitoring middle-box. For example: when the tenant
VM reads “7.img” in the directory “name9”, the ﬁle system
ﬁrst reads the inode metadata of directory “name9” and
the data blocks under this directory. Then, the content of
“7.img” is read from the data blocks. We also observed
that the written messages could be cached in the VM’s
local buffer for some time. As a result,
the block-level
I/O access sequence is different from the ﬁle I/Os — the
write operations may delay all the read operations. Notably,
in addition to logging sensitive ﬁle accesses, this monitor
provides detailed ﬁle system level activities. It can further
be utilized to debug abnormal system behaviors (e.g., the
write blowup issue in certain ﬁle systems [12]) and optimize
ﬁle system performance.
Real-world Malware Scenario We also applied the storage
access monitor to perform a real-world malware behavior
study. We chose HEUR:Backdoor.Linux.Ganiw.a, a Linux
backdoor Trojan detected by Kaspersky [13] in 2015. We
mainly studied the installation process of this malware with
the help of the storage access monitor.
When the malware was executed with root privileges,
the storage monitor observed the creation of several ﬁles
and directories. The main steps are listed in Table III.
Not surprisingly, this malware installed its startup scripts in
“/etc/init.d” to ensure the malware would be launched auto-
matically at the system startup. The malware also linked the
start scripts to different system run levels (1-5). Moreover,
the malware replaced “selinux” with its own copy and tried
to launch the fake one at the system startup. To hide from
checks, the malware replaced several system tools such as
“netstat”, “ps”, “lsof” and “ss” with their trojan version.
In addition to detecting created ﬁles during the malware’s
installation process, the storage monitor also observed sev-
eral important ﬁles read by the malware. For example, the
malware accessed the GeoIP database by reading the ﬁle
“/usr/share/GeoIP/GeoIPv6.dat”. Later, it called the SAX
(Simple API for XML) driver by reading the Python ﬁle
“/usr/lib/python3.4/xml/sax/expatreader.py” (a Python mod-
81
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:39:28 UTC from IEEE Xplore.  Restrictions apply. 
Step 1
Step 2
Step 3
Step 4
Step 5
Step 6
cp “#!/bin/bash\n”
/etc/init.d/DbSecuritySpt
ln -s /etc/init.d/DbSecuritySpt
/etc/rc[1-5].d/S97DbSecuritySpt
cp 
/usr/bin/bsd-port/getty
cp “#!/bin/bash\n/usr/bin/bsd-port/getty”
/etc/init.d/selinux
ln -s /etc/init.d/selinux
/etc/rc[1-5].d/S99selinux
cp  /bin/netstat
cp  /usr/bin/lsof
cp  /bin/ps
cp  /bin/ss
Table III: File system accesses by the backdoor malware.
ule in C). We concluded that this malware called the Python
functions from its C (or C++) code for parsing XML docu-
ments, and it tried to get the machine’s location information
using its IP or hostname. We note that the revealed ﬁle access
patterns of malware can then be used by the middle-box for
future detection of the same malware.
2) Case 2: Data Encryption: To allow tenants to keep
their data conﬁdential, we have implemented a storage
encryption middle-box. The goal of this middle-box is to
encrypt the tenant data before it is written to the disk and
decrypt it when the data is requested. Implementing this
functionality inside a middle-box offers additional ﬂexibility
to tenants to decide when encryption should be performed
and what algorithm should be used (as opposed to depending
on the cloud provider for this service). Further, instead of
deploying encryption services for each VM, multiple VMs
belonging to the same tenant can share the same encryption
middle-box (improving performance with less management
overhead).
We have implemented a widely used block cipher in the
encryption middle-box by leveraging dm-crypt — a well-
known disk encryption subsystem in the Linux kernel. We
deployed it in the kernel space of the encryption middle-box.
By passing tenants’ storage ﬂows to the encryption middle-
box, data encryption and decryption for the corresponding
tenant VMs’ volumes was easily achieved.
We compared the middle-box encryption solution with
a traditional tenant-side encryption solution (by installing
the encryption system in the tenant VM). The same AES
cipher with 256 bits keys were used for both solutions. A
20 GB volume was created and attached to the tenant VM for
both scenarios. Note that the client-side encryption requires
conﬁguring the volume’s format to enable the encryption
approach. However, the middle-box solution does not have
this requirement; it is transparent to the tenant VM.
To test the decryption and encryption, we ran a simple
FTP server in the tenant VM to download/upload a large
ﬁle from/to the attached volume, respectively. We observed
that both tenant-side and middle-box encryption solutions
nearly reach the maximum storage bandwidth. The average
bandwidth (both read and write) was around ∼88 MB
n
o
i
t
a
z
i
l
i
t
U
U
P
C
100.0%
25.0%
80.0%
60.0%
40.0%
20.0%
0.0%
24.4%
85.0%
37.1%
25.1%
Target Server
MB VM
Tenant VM
Performed by the
Performed by the
Tenant VM
MB VM
Figure 10: CPU utilization breakdown (with FTP).
Performed by the Tenant VM
Performed by the storage middle-box VM
1.34 
1.34 
1.34 
1.34 
1.29 
1.23 
 2.00
 1.50
 1.00
e
c
n
a
m
r
o
f
r
e
P
d
e
z
i
l
a
m
r
o
N
 0.50
 -
read ops/s
append
file creation
file deletion
read rate
write rate
ops/s
ops/s
ops/s
(MB/s)
(MB/s)
Figure 11: The application-level performance comparison
(with PostMark).
for the tenant-side solution and ∼84 MB for the middle-
box solution. Interestingly, the middle-box solution led to
much lower overall CPU resource utilization as shown in
Figure 10. Note that
the overall CPU utilization of the
middle-box solution was the sum of the CPU utilization from
the tenant VM, the middle-box VM, and the storage target.
On the contrary, the overall CPU utilization of the tenant-
side solution only involved the tenant VM and the storage
target.
The middle-box solution reduced the overall CPU utiliza-
tion by 20% due to the CPU savings in the tenant VM. To
conﬁrm this, we used a more realistic application workload,