small (median error  40%.
Sharding Table storage. For SocialForum, we considered
“what-if the Table is sharded into two”. By default (without
workload hints), WebPerf’s prediction algorithm assumes
that sharding will result in half the load on each shard, and
predicts total latency based on Table storage’s latency under
half the load. We considered two workloads: (1) uniform
workload where requests access both shards with roughly
equal probability, in which case WebPerf’s prediction was
very accurate (median error 1.7%) (2) skewed workload
where all requests accessed one shard, and since it received
the full load. Without workload hints, WebPerf’s estimation
(assuming half load on each shard) has a median error of
90.1%; however, with the hint that the workload is highly
skewed towards only one shard, prediction became fairly ac-
curate (median error 5.9%).
6.6 Additional Results
Effect of concurrency limits. For ContosoAds, we consid-
ered “what-if we downgrade the SQL server from Standard
to Basic tier, with 50 concurrent requests”. The Basic tier
only supports a maximum of 30 concurrent connections, and
additional connections are queued. WebPerf’s prediction is
Figure 12—Median prediction errors for 5 apps and 6 scenarios.
end-to-end latency, given a good model for network latency
between client and server and a WebProphet-like tool that
can predict client-side latency.
Interestingly, WebPerf’s end-to-end prediction is slightly
more accurate than cloud-side prediction. This is because all
the what-if scenarios we considered affect cloud-side laten-
cies only, and ofﬂine cloud-side latency proﬁles have higher
uncertainty than baseline latencies. In contrast, client-side
prediction relies on baseline HTTP request latencies only,
and hence is more accurate.
6.4 Comparative Analysis
Across all applications and scenarios, WebPerf is accurate
when dependency graphs are small, and latency proﬁles are
less variable and mostly application-independent.
Dependency graph complexity. The more I/O nodes in a
dependency graph, the more the potential prediction errors.
ContactManager has simple dependencies, and hence pre-
diction for it is more accurate.
Proﬁle variability. Proﬁle variabilities manifest as predic-
tion errors.
In all the apps we evaluated, SocialForum’s
baseline proﬁles had the least variability (since only this
app was deployed in dedicated VMs with performance iso-
lation). This helped WebPerf generate a better prediction
for SocialForum, despite its highly complicated dependency
graph (each request had 100+ I/O calls). Over all the sce-
narios, Scenario 4 and 6 had higher prediction errors due to
high variabilities in latency proﬁles used for these scenarios.
In Scenario 6, frontend latency was highly variable under a
large number of concurrent connections.
Application independence of proﬁles. Almost all APIs in
our applications were application-independent. The only ex-
ceptions were ContosoAds and CourseManager, which use
expensive SQL queries on medium sized tables. Other ap-
plications use small SQL tables, so latency does not change
much with tier change (Figure 2). Latencies of queries to
medium/large SQL table change as tiers change, and hence
using baseline latencies as estimates resulted in large errors.
The problem can be avoided by using workload hints, as
269
020040060080010001200140016001800Overall Time (ms)0.00.20.40.60.81.0CDF DistributionOriginal DistributionRedis Tier Change PredictedRedis Tier Change Measured0200400600800100012001400Overall Time (ms)0.00.20.40.60.81.0CDF DistributionOriginal DistributionTier PredictedTier Measured1000200030004000500060007000Overall Time (ms)0.00.20.40.60.81.0CDF DistributionOriginal DistributionInput Size PredictedInput Size Measured02004006008001000120014001600Overall Time (ms)0.00.20.40.60.81.0CDF DistributionOriginal DistributionScalability PredictedScalability Measured123456(b) End to End01234567123456Median Error (%)(a) Cloud Side ScenariosSocialForumContosoAdsEmailSubscriberCourseManagerSmartStore.NetContactManagermentation runtime overhead is lightweight - on average, it
increases the run time by 3.3%. In the prediction stage, the
overhead of obtaining the proﬁle data is negligible as the
proﬁling data are stored in Azure table. We expect some
moderate overhead for the prediction algorithm’s operation
on the distribution; the average prediction time for all six ap-
plications’ what-if scenarios is around 5.6s. We believe these
overheads are reasonable: within tens of seconds, WebPerf
is able to predict the performance for web applications under
various scenarios quite accurately.
7. RELATED WORK
Performance prediction. Ernest [44] can accurately pre-
dict the performance of a given analytics job in the cloud.
In contrast, WebPerf focuses on web applications that, un-
like analytical jobs, are I/O intensive and are increasingly
written using the task asynchronous paradigm. WISE [42]
predicts how changes to CDN deployment and conﬁgura-
tion affect service response times. Unlike WebPerf, WISE
targets CDNs. Mystery Machine [10] uses extensive cloud-
side logs to extract the dependency and identify the critical
paths. It targets cloud providers, and hence uses extensive
platform instrumentation. In contrast, WebPerf targets third-
party developers, and relies on instrumenting app binaries
alone. Both WISE and Mystery Machine use purely data-
driven techniques In contrast, WebPerf has less data when
an application is being deployed, and hence it uses a combi-
nation of instrumentation and modeling for prediction.
WebProphet [25] can predict webpage load time, but it
focuses only on end-to-end prediction. In contrast, WebPerf
considers both cloud-side prediction and end-to-end predic-
tion. WebProphet extracts (approximate) dependencies be-
tween web objects through client-side measurements, while
WebPerf extracts accurate dependencies by instrumentation.
CloudProphet [24] is a trace-and-replay tool to predict a
legacy application’s performance if migrated to a cloud in-
frastructure. CloudProphet traces the workload of the appli-
cation when running locally, and replays the same workload
in the cloud for prediction. Unlike WebPerf, it does not con-
sider what-if scenarios involving changes in conﬁgurations
and runtime conditions of web applications. Herodotou et
al. [20] propose a system to proﬁle and predict performance
of MapReduce programs; however, the technique is tightly
integrated to MapReduce and cannot be trivially generalized
to a web application setting.
Dependency analysis. Closest to our work is AppInsight
[32, 33], which can automatically instrument .NET appli-
cation binaries to track causality of asynchronous events.
it does not support the Async-Await program-
However,
ming paradigm. To our knowledge, no prior instrumentation
framework can accurately track the dependency graphs for
applications written in this paradigm.
Signiﬁcant prior work has focused on dependency graph
extraction [36, 16, 8, 38, 35, 5, 27, 47]. Systems that use
black-box techniques [36, 47] fail to characterize the accu-
rate dependencies between I/O, compute, and other compo-
nents, and hence can result in poor prediction accuracy. Sys-
Figure 13—Prediction for ContosoAds with concurrency limit.
aware of such limits (§ 4.3) and in our experiment, it could
make accurate prediction, as shown in Figure 13 (median
error 2.1%); in contrast, ignoring such limit would have pro-
duced a median error of 87.3%.
Sources of prediction errors. At a high level, WebPerf has
two sources of errors: API latency models and the prediction
algorithm. To isolate errors introduced by the prediction al-
gorithms alone, we start with execution trace of a request
and use the prediction algorithm with true latencies of com-
pute and I/O calls in the trace. We then compute the relative
error of the predicted latency and the true total latency of
execution trace. We repeat this for all requests used in our
case studies. We ﬁnd the average error to be 0.4% across all
requests, with 0.3% median and 1.1% maximum error. The
error is very small compared to the errors of the statistical
models (mean and median errors ranging from 0.4%-6.5%
and 0.5%-4.4% respectively), as shown in Figure 1. This
suggests that WebPerf’s prediction latency might be further
improved by using more sophisticated models or more data
on which models are built.
Impacts of measurement optimization. Finally, we evalu-
ate our optimization algorithms described in §5.2. We use 3
distinct requests for the SocialForum website, containing 9
distinct API calls to 5 different cloud services. The average
time for three requests are 1.3s, 3.8s, and 0.5s respectively.
We use a time budget of 2 minutes for all measurements.
We compare our algorithm with two baseline schemes: 1)
Round Robin (RR): all requests are repeated in round robin
fashion for 2 minutes, 2) Equal Time (ET): all requests are
allocated an equal time of 40 seconds each. For RR, each
request was executed 22 times. For ET, requests 1, 2, and
3 are executed 30, 10, and 80 times. On the other hand,
with our optimization algorithm, requests 1, 2, and 3 are ex-
ecuted 31, 19, and 15 times respectively. It collected more
samples for APIs with high latency variability. For each
API, we compared all predicted latency distributions with
the ground truth latency distributions. The (mean, median,
maximum) relative errors of our optimized algorithm for all
APIs were (1.8%, 1.68%, 10.1%), while for RR and ET, the
errors are (5.55%, 2.8%, 21.0%) and (6.68%, 2.4%, 18.4%)
respectively. To achieve a similar accuracy as WebPerf, RR
and ET need 2.6 min and 3.4 min respectively, more than
30% over the designated time. The results demonstrate a
signiﬁcant beneﬁt of our optimization algorithm.
WebPerf overhead. To quantify the overhead, we ﬁx the
web applications’ web server tier.
In our experiment, our
web server tier is standard tier. The average instrumentation
time for all the six applications is 3.1s. WebPerf’s instru-
270
tems that modify the framework [16, 8, 38, 35] are hard to
deploy in cloud platforms. And techniques that require de-
veloper effort [5, 27] are hard to scale. There are also sys-
tems that track dependencies of network components [46, 9].
WebPerf differs from these frameworks in its target system
(cloud-hosted web application).
Webpage Analysis.
[17, 45, 14, 19, 30] measure web-
site performance by analyzing the request waterfall. Page-
speed [17] analyzes the webpage source and proposes con-
tent optimization. Unlike WebPerf, these systems only focus
on the client-side view and not the server-side.
8. CONCLUSION
We presented WebPerf, a system that systematically ex-
plores what-if scenarios in web applications to help devel-
opers choose the right conﬁgurations and tiers for their tar-
get performance needs. WebPerf automatically instruments
the web application to capture causal dependencies of I/O
and compute components in the request pipeline. Together
with online- and ofﬂine-proﬁled models of various compo-
nents, it uses the dependency graphs to estimate a distribu-
tion of cloud and end-to-end latency. We have implemented
WebPerf for Microsoft Azure. Our evaluation with six real
web application shows that WebPerf can be highly accurate
even for websites with complex dependencies.
Acknowledgements. We thank the reviewers and our shep-
herd Ion Stoica for their comments that helped improve the
technical content and presentation of the paper.
9. REFERENCES
[1] ANTS Performance Proﬁler. http://www.red-gate.com/products/
dotnet-development/ants-performance-proﬁler/.
[2] Asynchronous Programming with Async and Await.
https://msdn.microsoft.com/en-us/library/hh191443.aspx.
[3] Amazon Web Services Asynchronous APIs for .NET.
http://docs.aws.amazon.com/AWSSdkDocsNET/V2/
DeveloperGuide/sdk-net-async-api.html.
[4] Azure Web Extension Gallery .
https://www.siteextensions.net/packages.
[5] P. Barham, R. Isaacs, R. Mortier, and D. Narayanan. Magpie: Online
Modelling and Performance-aware Systems. In HotOS, 2003.
[6] R. Bellman. Dynamic programming and Lagrange multipliers.
Proceedings of the National Academy of Sciences of the United
States of America, 1956.
[7] Callback Hell. http://callbackhell.com/.
[8] M. Y. Chen, E. Kiciman, E. Fratkin, A. Fox, and E. Brewer. Pinpoint:
Problem determination in large, dynamic internet services. In IEEE
DSN, 2002.
[9] X. Chen, M. Zhang, Z. M. Mao, and P. Bahl. Automating Network
Application Dependency Discovery: Experiences, Limitations, and
New Solutions. In USENIX OSDI, 2008.
[10] M. Chow, D. Meisner, J. Flinn, D. Peek, and T. F. Wenisch. The
Mystery Machine: End-to-end performance analysis of large-scale
Internet services. In USENIX OSDI, 2014.
[11] Contact Manager.
https://azure.microsoft.com/en-us/documentation/articles/web-sites-
dotnet-deploy-aspnet-mvc-app-membership-oauth-sql-database/.
[12] Contoso Ads. https://azure.microsoft.com/en-us/documentation/
articles/cloud-services-dotnet-get-started/.
[13] Course Manager. https://www.asp.net/mvc/overview/getting-started/
getting-started-with-ef-using-mvc/creating-an-entity-framework-
data-model-for-an-asp-net-mvc-application.
[14] Dareboost Website Analysis and Optimization .
https://www.dareboost.com/en/home/.
271
[15] EmailSubscriber.
https://code.msdn.microsoft.com/Windows-Azure-Multi-Tier-
eadceb36/sourcecode?ﬁleId=127708&pathId=382208951.
[16] R. Fonseca, G. Porter, R. H. Katz, S. Shenker, and I. Stoica. X-trace:
A pervasive network tracing framework. In USENIX NSDI, 2007.
[17] Google PageSpeed Insights.
https://developers.google.com/speed/pagespeed/insights/.
[18] C. M. Grinstead and J. L. Snell. Introduction to probability.
American Mathematical Soc., 2012.
[19] GTmetrix Website Speed Analysis . https://gtmetrix.com/.
[20] H. Herodotou and S. Babu. Proﬁling, what-if analysis, and
cost-based optimization of mapreduce programs. VLDB, 2011.
[21] High Scalability: building bigger, faster, more reliable websites.
http://highscalability.com/.
[22] JetBrains dotTrace. http://www.jetbrains.com/proﬁler/.
[23] Y. Jiang, L. Ravindranath, S. Nath, and R. Govindan. Evaluating
“What-If” Scenarios for Cloud-hosted Web Applications. Technical
Report MSR-TR-2016-36, Microsoft Research, June 2016.
[24] A. Li, X. Zong, S. Kandula, X. Yang, and M. Zhang. CloudProphet:
Towards Application Performance Prediction in Cloud. In ACM
SIGCOMM, 2011.
[25] Z. Li, M. Zhang, Z. Zhu, Y. Chen, A. G. Greenberg, and Y.-M. Wang.
WebProphet: Automating Performance Prediction for Web Services.
In USENIX NSDI, 2010.
[26] C. Z. Mooney. Monte carlo simulation. Sage Publications, 1997.
[27] New Relic. http://newrelic.com/.
[28] Node.js. https://nodejs.org/en/.
[29] NUGET Package. https://www.nuget.org/packages.
[30] Pingdom Website Speed Test. http://tools.pingdom.com/fpt/.
[31] F. Pukelsheim. Optimal design of experiments. SIAM, 1993.
[32] L. Ravindranath, J. Padhye, S. Agarwal, R. Mahajan, I. Obermiller,
and S. Shayandeh. AppInsight: Mobile App Performance Monitoring
in the Wild. In USENIX OSDI, 2012.
[33] L. Ravindranath, J. Padhye, R. Mahajan, and H. Balakrishnan.
Timecard: Controlling user-perceived delays in server-based mobile
applications. In ACM SOSP, 2013.
[34] Redis Cache. https://azure.microsoft.com/en-us/services/cache/.
[35] P. Reynolds, C. E. Killian, J. L. Wiener, J. C. Mogul, M. A. Shah, and
A. Vahdat. Pip: Detecting the Unexpected in Distributed Systems. In
USENIX NSDI, 2006.
[36] P. Reynolds, J. L. Wiener, J. C. Mogul, M. K. Aguilera, and
A. Vahdat. Wap5: black-box performance debugging for wide-area
systems. In ACM WWW, 2006.
[37] D. J. Rumsey. Statistics For Dummies, 2nd Edition. Wiley, 2011.
[38] B. H. Sigelman, L. A. Barroso, M. Burrows, P. Stephenson,
M. Plakal, D. Beaver, S. Jaspan, and C. Shanbhag. Dapper, a
large-scale distributed systems tracing infrastructure. Google
research, 2010.
[39] SmartStore.Net. http://www.smartstore.com/net/en/.
[40] Stackify. http://stackify.com/.
[41] D. Syme, T. Petricek, and D. Lomov. The F# asynchronous
programming model. In Practical Aspects of Declarative Languages.
Springer, 2011.
[42] M. Tariq, A. Zeitoun, V. Valancius, N. Feamster, and M. Ammar.
Answering what-if deployment and conﬁguration questions with
wise. In ACM SIGCOMM CCR, 2008.
[43] TechStacks. http://techstacks.io/stacks.
[44] S. Venkataraman, Z. Yang, M. Franklin, B. Recht, and I. Stoica.
Ernest: efﬁcient performance prediction for large-scale advanced
analytics. In USENIX NSDI, 2016.
[45] Webpage Test. http://www.webpagetest.org/.
[46] M. Yu, A. G. Greenberg, D. A. Maltz, J. Rexford, L. Yuan,
S. Kandula, and C. Kim. Proﬁling Network Performance for
Multi-tier Data Center Applications. In USENIX NSDI, 2011.
[47] Z. Zhang, J. Zhan, Y. Li, L. Wang, D. Meng, and B. Sang. Precise
request tracing and performance debugging for multi-tier services of
black boxes. In IEEE DSN, 2009.