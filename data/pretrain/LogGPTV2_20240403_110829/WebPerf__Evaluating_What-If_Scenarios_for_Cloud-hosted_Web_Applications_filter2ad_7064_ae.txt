### Sharding Table Storage

For the SocialForum, we evaluated a scenario where the table is sharded into two. By default, without workload hints, WebPerf's prediction algorithm assumes that sharding will result in half the load on each shard and predicts total latency based on the table storage's latency under half the load. We considered two types of workloads:

1. **Uniform Workload**: Requests access both shards with roughly equal probability. In this case, WebPerf's prediction was very accurate, with a median error of 1.7%.
2. **Skewed Workload**: All requests accessed one shard, resulting in the full load on that shard. Without workload hints, WebPerf's estimation (assuming half the load on each shard) had a median error of 90.1%. However, with the hint that the workload is highly skewed towards one shard, the prediction became much more accurate, with a median error of 5.9%.

### Additional Results

#### Effect of Concurrency Limits

For ContosoAds, we considered a "what-if" scenario where the SQL server is downgraded from the Standard to the Basic tier, with 50 concurrent requests. The Basic tier supports a maximum of 30 concurrent connections, and additional connections are queued. WebPerf's prediction for this scenario is shown in Figure 12, which illustrates the median prediction errors for five applications and six scenarios.

Interestingly, WebPerf's end-to-end prediction is slightly more accurate than its cloud-side prediction. This is because all the "what-if" scenarios we considered affect only cloud-side latencies, and offline cloud-side latency profiles have higher uncertainty compared to baseline latencies. In contrast, client-side prediction relies solely on baseline HTTP request latencies, making it more accurate.

### Comparative Analysis

Across all applications and scenarios, WebPerf is accurate when dependency graphs are small, and latency profiles are less variable and mostly application-independent.

- **Dependency Graph Complexity**: The more I/O nodes in a dependency graph, the greater the potential prediction errors. For example, ContactManager has simple dependencies, leading to more accurate predictions.
- **Profile Variability**: Profile variabilities manifest as prediction errors. Among the evaluated applications, SocialForum's baseline profiles had the least variability (as it was deployed in dedicated VMs with performance isolation). This helped WebPerf generate better predictions for SocialForum, despite its highly complicated dependency graph (each request involved over 100 I/O calls). Scenarios 4 and 6 had higher prediction errors due to high variabilities in the latency profiles used.
- **Application Independence of Profiles**: Almost all APIs in our applications were application-independent. The exceptions were ContosoAds and CourseManager, which use expensive SQL queries on medium-sized tables. Other applications use small SQL tables, so latency does not change much with tier changes. Latencies of queries to medium/large SQL tables change with tier changes, and using baseline latencies as estimates resulted in large errors. This problem can be mitigated by using workload hints.

### Sources of Prediction Errors

WebPerf's prediction errors come from two main sources: API latency models and the prediction algorithm. To isolate errors introduced by the prediction algorithm, we used the true latencies of compute and I/O calls in the execution trace. The average error across all requests was 0.4%, with a median error of 0.3% and a maximum error of 1.1%. These errors are significantly smaller compared to those of statistical models, suggesting that WebPerf's prediction accuracy could be further improved with more sophisticated models or more data.

### Impact of Measurement Optimization

We evaluated our optimization algorithms described in ยง5.2. For the SocialForum website, we used three distinct requests containing nine distinct API calls to five different cloud services. The average times for these requests were 1.3s, 3.8s, and 0.5s, respectively. We used a time budget of 2 minutes for all measurements. We compared our algorithm with two baseline schemes:
- **Round Robin (RR)**: All requests are repeated in a round-robin fashion for 2 minutes.
- **Equal Time (ET)**: All requests are allocated an equal time of 40 seconds each.

For RR, each request was executed 22 times. For ET, requests 1, 2, and 3 were executed 30, 10, and 80 times, respectively. With our optimization algorithm, requests 1, 2, and 3 were executed 31, 19, and 15 times, respectively, collecting more samples for APIs with high latency variability. The relative errors of our optimized algorithm for all APIs were (mean, median, maximum) (1.8%, 1.68%, 10.1%), while for RR and ET, the errors were (5.55%, 2.8%, 21.0%) and (6.68%, 2.4%, 18.4%), respectively. To achieve similar accuracy, RR and ET needed 2.6 min and 3.4 min, respectively, more than 30% over the designated time. These results demonstrate the significant benefit of our optimization algorithm.

### WebPerf Overhead

To quantify the overhead, we fixed the web applications' web server tier at the standard tier. The average instrumentation time for all six applications was 3.1s. WebPerf's instrumentation runtime overhead is lightweight, increasing the run time by an average of 3.3%. In the prediction stage, the overhead of obtaining profile data is negligible, as the profiling data are stored in Azure tables. We expect some moderate overhead for the prediction algorithm's operation on the distribution; the average prediction time for all six applications' "what-if" scenarios is around 5.6s. These overheads are reasonable, as WebPerf can predict the performance of web applications under various scenarios within tens of seconds with high accuracy.