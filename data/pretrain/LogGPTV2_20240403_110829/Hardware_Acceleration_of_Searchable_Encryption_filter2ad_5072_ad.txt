(D)SSE and non-SSE schemes. For instance, in keyword search, we measured the delay of IM-DSSE
an entire row of the encrypted index and gets cells whose value is 1. The IM-DSSE scheme and its
the database (i.e.,()) and yet it is highly practical even for very large numbers of keyword-(cid:27)le pairs
(i.e., more than1011 pairs). Indeed, we con(cid:27)rm that the search operation in IM-DSSE is very fast and
their procedure is identical. The keyword search operation delay of IM-DSSE is higher than that
extended versions took less than 100 ms to perform a keyword search, while it took less than 2 seconds
to update a (cid:27)le. The cost per keyword search depends linearly on the maximum number of (cid:27)les in
most of the overhead is due to network communication delay as it will be later analyzed in this section.
Note that the costs for adding and deleting (cid:27)les (updates) over the encrypted index are similar since
scheme and IM-DSSEI scheme by the time the client sends the request and the server (cid:27)nishes decrypting
15
)
s
m
(
y
a
l
e
D
)
s
(
y
a
l
e
D
1011
# keyword-(cid:27)le pairs (log)
(b) File update
1011
IM-DSSE
IM-DSSEI
IM-DSSEII
IM-DSSEI+II
IM-DSSE
IM-DSSEI
IM-DSSEII
IM-DSSEI+II
1.5
1
0.5
0
1010
100
80
60
40
1010
First, the encrypted index in IM-DSSE scheme is bit-by-bit encrypted compared with 128-bit block
a whole row. Thus, the gap between IM-DSSE and IM-DSSEI represents the server computation
cost required for this key derivation and encryption. Second, processes in IM-DSSE scheme are
encryption in IM-DSSEI. Hence, the server needs to derive more AES keys than in IM-DSSEI to decrypt
of extended schemes and it increases as the size of the encrypted index increases due to two reasons:
Figure 1: The latency of our schemes with fast network.
# keyword-(cid:27)le pairs (log)
(a) Keyword search
performed subsequently, in which the server needs to receive some information sent from the client
(cid:27)rst before being able to derive keys to decrypt a row. Such processes in IM-DSSEI and IM-DSSEII can
be parallelized, where the client generates the AES-CTR keys while receiving a row of data transmitted
from the server. We can see that the delay is not so di(cid:29)erent between IM-DSSEI and IM-DSSEII and
IM-DSSEI+II. This indicates that using 128-bit encryption signi(cid:27)cantly reduces the server computation
cost to be negligible as it will be later shown.
Considering the (cid:27)le update operation, our IM-DSSE and IM-DSSEII schemes leverage 1-bit en-
cryption and, therefore, it does not require to transfer a 128-bit block to the client (cid:27)rst prior to up-
dating the column as in IM-DSSEI and IM-DSSEI+II schemes. Hence, they are faster and less a(cid:29)ected
by the network latency than IM-DSSEI and IM-DSSEI+II. So, the gap between such schemes re(cid:30)ects
the data download delays, which will be signi(cid:27)cantly higher on slower networks as shown in the next
experiment. Update in IM-DSSEI+II is considerably faster than in IM-DSSEI because it allows for par-
allelization, in which the client can pre-compute AES-CTR keys while receiving data from the server.
In IM-DSSEI, such keys cannot be computed as they need some information being sent from the server
beforehand (i.e., state data[∗,]. ).
The impact of network quality. The previous experiments were conducted on a high-speed network,
which might not be widely available in practice. Hence, we additionally investigated how our schemes
perform when the network quality is degraded. To do that, we setup the server to be geographically
located distant from the client machine, resulting in the network latency and throughput to be 67.5 ms
and 46 Mbps, respectively . Figure 2 shows the end-to-end crypto delay of our schemes in this moderate
network setting. Due to the high network latency, search operation of each scheme is slower than that
of fast network by 230ms. The impact of the network latency is clearly shown in the update operation
as re(cid:30)ected in Figure 2b. The delays of IM-DSSEI, IM-DSSEI+II are signi(cid:27)cantly higher than those of
IM-DSSE and IM-DSSEII. As explained previously, this gap actually re(cid:30)ects the download delay
incurred by such schemes.
16
6
4
2
0
1010
283032⋅101
1010
ms
1011
10⋅103
IM-DSSE
5
0
1010
IM-DSSEI
IM-DSSEII
IM-DSSEI+II
ms
1011
IM-DSSE
IM-DSSEI
IM-DSSEII
IM-DSSEI+II
1011
# keyword-(cid:27)le pairs (log)
(a) Keyword search
# keyword-(cid:27)le pairs (log)
(b) File update
Figure 2: The latency of our schemes with moderate network.
IM-DSSE
IM-DSSEI
IM-DSSEII
IM-DSSEI+II
)
s
(
y
a
l
e
D
IM-DSSE
IM-DSSEI
IM-DSSEII
IM-DSSEI+II
320
300
280
1010
)
s
m
(
y
a
l
e
D
ms
46810⋅101
1010
1011
ms
0246⋅103
1010
IM-DSSE
IM-DSSEI
IM-DSSEII
IM-DSSEI+II
1011
IM-DSSE
IM-DSSEI
IM-DSSEII
IM-DSSEI+II
1011
# kw-f pairs (log)
(a) Keyword search
# kw-f pairs (log)
(b) File update
# kw-f pairs (log)
(c) Keyword search
# kw-f pairs (log)
(d) File update
Figure 3: The latency of IM-DSSE framework with SSD server-storage and (a,b) fast and (c,d) moderate
networks.
Storage location of encrypted index: RAM vs. disk. Another important performance factor for
DSSE is the encrypted index storage access delay. Hence, we investigated the impact of the encrypted
index storage location on the performance of our schemes. Clearly, the ideal case is to store all server-
side data on RAM to minimize the delay introduced by storage media access as shown in previous
experiments. However, deploying a cloud server with a very large amount of RAM capacity can be
very costly. Thus, in addition to the RAM-stored results shown previously, we stored the encrypted
index on the secondary storage unit (i.e., SSD drive), and then measured how overall delays of our
scheme were impacted by this setting. Figure 3 presents results with two aforementioned network
I/O access is incurred by loading a part of the encrypted index including value. and state. . It
quality environments (i.e., fast and moderate speeds). In IM-DSSE and IM-DSSEI schemes, the disk
of the encrypted matrix in contiguous memory blocks. Therefore, keyword search invokes accessing
is clear that the disk I/O access time incurred an insigni(cid:27)cant latency to the overall delay in terms of
keyword search operation as shown in Figures 3a and 3d since our schemes achieve perfect locality as
de(cid:27)ned by Cash et al. [7]. However, in the (cid:27)le update operation, the delay in IM-DSSE framework was
1–4 seconds more, compared with RAM-based storage. That is because we stored all cells in each row
subsequent memory blocks while update operation results in accessing scattered blocks which incurs
much higher disk I/O access time. Due to the incidence matrix data structure and this storage strategy,
our search operation was not a(cid:29)ected as much by disk I/O access time as other non-local DSSE schemes
(e.g., [5, 6, 17]), which require accessing random memory blocks for security.
17
S
S
EI + II
EII
EI
S
S
S
I M - D
I M - D
0%
EI M - D
S
S
S
I M - D
20%
40%
60%
80%
S
S
EI + II
EII
EI
S
S
S
I M - D
I M - D