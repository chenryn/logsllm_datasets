(cid:3)
(cid:20)
(cid:15)(cid:4)(cid:16)(cid:4)(cid:17)(cid:8)(cid:7)(cid:10)(cid:11)(cid:12)(cid:13)(cid:4)(cid:9)(cid:8)(cid:14)(cid:4)(cid:6)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)
(cid:18)(cid:19)(cid:11)(cid:3)(cid:8)(cid:14)(cid:4)(cid:17)(cid:5)(cid:6)(cid:3)(cid:4)(cid:7)(cid:8)(cid:9)(cid:8)(cid:7)(cid:10)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:3)(cid:4)(cid:7)(cid:8)(cid:9)(cid:8)(cid:7)(cid:10)
(cid:11)(cid:12)(cid:13)(cid:4)(cid:9)(cid:8)(cid:14)(cid:4)(cid:6)
(cid:11)(cid:12)(cid:13)(cid:4)(cid:9)(cid:8)(cid:14)(cid:4)(cid:6)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)
(cid:20)(cid:3)(cid:3)(cid:14)(cid:8)(cid:7)(cid:12)(cid:23)(cid:8)(cid:19)(cid:16)(cid:5)(cid:6)(cid:3)(cid:4)(cid:7)(cid:8)(cid:9)(cid:8)(cid:7)(cid:10)
(cid:11)(cid:12)(cid:13)(cid:4)(cid:9)(cid:8)(cid:14)(cid:4)(cid:6)
Fig. 2: Build system hierarchy.
plots. Again, the user is provided with hooks to
change the appearance of emitted plots.
Building and running the benchmarks is sensitive to
environment variables. FEX provides a convenience wrap-
per to specify default variables for these steps (see §II-B).
B. System Architecture
Following the workﬂow presented in §II-A, FEX
consists of 4 subsystems: building, running, collecting,
and plotting. The later two have plain structure which
does not require any explanations. The former two,
however, are slightly more complex and we will discuss
them in detail.
Build subsystem. In the build stage, two scenarios are
possible: (1) a single application can be built many
times with varying build parameters or (2) the same
parameters can be reused for many different applications.
To aid this variability, our build subsystem was divided
into three layers (see Figure 2): common, experiment,
and application layers.
Common layer contains parameters that are applicable
to all benchmarks and all build types. This includes,
for example, optimization levels, debugging information
(if enabled), common compilation ﬂags and generic
compilation targets.
Experiment layer is responsible for parameters of the
current build type. For example, if a benchmark suite has
to be built by GCC and with enabled AddressSanitizer,
the makeﬁles will set CC variable to gcc and CFLAGS
to -fsanitize=address. Note that there might be
multiple levels of makeﬁles in this layer: some may set
parameters that are applicable to all conﬁgurations of
a single compiler, and the others will reﬁne them to a
concrete conﬁguration.
Finally, the application layer deﬁnes the structure and
the procedure of the build. It speciﬁes the location of
source ﬁles, lists dependencies, and sets application-
speciﬁc ﬂags.
The overall build system is structured in such a way
that these layers can be replaced independently of each
other. Accordingly, any application can be compiled
with any of the existing build conﬁgurations without
additional efforts.
Experiment runners. When an experiment is started via
>> fex.py run ...
a new instance of the FEX class is created (see Figure 3).
This object controls the overall experiment execution.
Firstly, it retrieves a conﬁguration ﬁle and sets experiment
parameters accordingly. Then,
it sets environment
variables to the necessary values by instantiating child
classes of the Environment abstract class. In the end, it
instantiates and calls the child of the Runner class that
corresponds to the current experiment. This new Runner
object will perform the actual experiment.
Since environmental variables can vary in accordance
to parameters of the given experiment (e.g., when debug
mode is turned on), we deﬁne four types of the variables:
1) Default: the default values of environment variables.
2) Updated: the values of this type are appended if the
variable exists, and assigned otherwise.
3) Forced: the variables are overwritten regardless of
the previous value.
4) Debug: the values are set only in the debug mode.
Note that the order is important here: each next type
has higher priority that the previous one. For example,
if variable BIN_PATH is assigned to /usr/bin/ among
default variables and to /home/usr/bin/ among the
forced ones, the end value will be /home/usr/bin/.
On top of it, if a user wants to add another type, she
can do it simply by writing a subclass of Environment
and redeﬁning the set_variables function.
The
key
of
class
element
the Runner
is
experiment_loop function. For each of the execution
parameters, it iterates over all their values by going
through a series of nested loops, as shown in Figure 4.
For example, the outermost loop may go through GCC
and Clang compilers, the next one—through Nginx,
Apache, and Memcached applications, and so forth.
Each of the loops has a hook that can be implemented
in a subclass. This way, the overall structure of the
experiment stays the same, but the concrete actions
can be tailored to the needs of the given experiment.
Moreover, if even more parameters would be necessary,
the experiment_loop can be redeﬁned or extended in
a subclass, as VariableInputRunner does in Figure 4.
III.FEX DETAILS AND WORKFLOW
A. Creating new experiments
In this section, we explain how FEX facilitates creation
of new experiments and evaluation of new benchmark
suites and standalone programs. We also detail the
implementation of FEX with the help of its standard
directory layout (similar to projects like Jekyll [19], FEX
assumes a speciﬁc directory tree structure).
One (simpliﬁed) example of a directory tree is shown
in Figure 5. Here, the end user sets up the environment
to evaluate the performance overhead of Google’s
AddressSanitizer [20] on the Phoenix benchmark suite
[3] and on Apache web server [21]. Moreover, for
reproducibility she chooses GCC version 6.1 which
comes with AddressSanitizer by default.
First, she needs to write installation scripts to install
the GCC 6.1 compiler, download input ﬁles for the
Phoenix benchmark, and install an additional Apache
benchmark. For convenience, FEX provides a set of
functions for frequently used operations,
found in
install/common.sh, e.g., download.
545
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:02:52 UTC from IEEE Xplore.  Restrictions apply. 
(cid:28)(cid:9)(cid:4)(cid:22)(cid:7)(cid:29)(cid:19)(cid:8)(cid:3)(cid:12)(cid:7)(cid:9)(cid:4)
(cid:14)(cid:11)(cid:15)
(cid:1)(cid:2)(cid:15)(cid:16)(cid:17)(cid:18)(cid:10)(cid:19)(cid:20)(cid:9)(cid:4)(cid:13)(cid:14)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:4)(cid:17)(cid:7)(cid:10)(cid:9)(cid:16)(cid:17)(cid:21)(cid:4)(cid:17)(cid:5)(cid:13)(cid:14)
(cid:1)(cid:2)(cid:9)(cid:20)(cid:17)(cid:6)(cid:11)(cid:4)(cid:17)(cid:15)(cid:22)(cid:21)(cid:8)(cid:9)(cid:23)(cid:13)(cid:14)
(cid:23)(cid:3)(cid:8)(cid:7)(cid:3)(cid:24)(cid:25)(cid:11)(cid:26)(cid:4)(cid:27)(cid:19)(cid:12)(cid:31)(cid:19)(cid:4)(cid:4)(cid:11)(cid:8)
(cid:1)(cid:2)(cid:4)(cid:24)(cid:25)(cid:4)(cid:9)(cid:10)(cid:21)(cid:4)(cid:17)(cid:5)(cid:6)(cid:12)(cid:16)(cid:16)(cid:25)(cid:13)(cid:14)
(cid:10)(cid:11)(cid:2)(cid:2)(cid:8)(cid:5)
(cid:1)(cid:2)(cid:4)(cid:24)(cid:25)(cid:4)(cid:9)(cid:10)(cid:21)(cid:4)(cid:17)(cid:5)(cid:6)(cid:17)(cid:8)(cid:21)(cid:4)
(cid:1)(cid:2)(cid:4)(cid:24)(cid:25)(cid:4)(cid:9)(cid:10)(cid:21)(cid:4)(cid:17)(cid:5)(cid:6)(cid:3)(cid:4)(cid:5)(cid:20)(cid:25)(cid:13)(cid:14)
(cid:1)(cid:2)(cid:4)(cid:24)(cid:25)(cid:4)(cid:9)(cid:10)(cid:21)(cid:4)(cid:17)(cid:5)(cid:6)(cid:12)(cid:16)(cid:16)(cid:25)(cid:13)(cid:14)
(cid:1)(cid:2)(cid:16)(cid:4)(cid:12)(cid:10)(cid:12)(cid:7)(cid:20)(cid:10)(cid:6)(cid:18)(cid:9)(cid:13)(cid:17)(cid:20)(cid:21)(cid:22)
(cid:1)(cid:2)(cid:16)(cid:4)(cid:12)(cid:10)(cid:9)(cid:23)(cid:16)(cid:4)(cid:10)(cid:6)(cid:18)(cid:9)(cid:13)(cid:17)(cid:20)(cid:21)(cid:22)
(cid:1)(cid:2)(cid:16)(cid:4)(cid:12)(cid:10)(cid:14)(cid:4)(cid:20)(cid:18)(cid:24)(cid:25)(cid:6)(cid:12)(cid:26)(cid:10)(cid:6)(cid:18)(cid:9)(cid:13)(cid:17)(cid:20)(cid:21)(cid:22)
(cid:1)(cid:2)(cid:16)(cid:4)(cid:12)(cid:10)(cid:9)(cid:24)(cid:12)(cid:4)(cid:6)(cid:3)(cid:10)(cid:6)(cid:18)(cid:9)(cid:13)(cid:17)(cid:20)(cid:21)(cid:22)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:2)(cid:7)(cid:8)(cid:2)(cid:9)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:6)(cid:12)(cid:13)(cid:6)(cid:14)(cid:8)(cid:4)(cid:15)
(cid:1)(cid:2)(cid:7)(cid:16)(cid:3)(cid:6)(cid:9)(cid:4)(cid:3)(cid:10)(cid:11)(cid:6)(cid:12)(cid:13)(cid:6)(cid:14)(cid:8)(cid:4)(cid:15)
(cid:1)(cid:2)(cid:5)(cid:17)(cid:12)(cid:18)(cid:4)(cid:3)(cid:10)(cid:11)(cid:6)(cid:12)(cid:13)(cid:6)(cid:14)(cid:8)(cid:4)(cid:15)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:14)(cid:7)(cid:19)(cid:10)(cid:11)(cid:6)(cid:12)(cid:13)(cid:6)(cid:14)(cid:8)(cid:4)(cid:15)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:8)(cid:11)(cid:12)(cid:4)(cid:3)(cid:13)(cid:14)
(cid:16)(cid:3)(cid:8)(cid:17)(cid:11)(cid:18)(cid:23)(cid:3)(cid:8)(cid:7)(cid:3)(cid:24)(cid:25)(cid:11)(cid:26)(cid:4)(cid:27)(cid:19)(cid:12)
(cid:16)(cid:11)(cid:8)(cid:22)(cid:9)(cid:8)(cid:10)(cid:3)(cid:4)(cid:18)(cid:11)
(cid:30)(cid:30)(cid:30)
(cid:16)(cid:21)(cid:9)(cid:11)(cid:4)(cid:7)(cid:15)(cid:23)(cid:3)(cid:8)(cid:7)(cid:3)(cid:24)(cid:25)(cid:11)(cid:26)(cid:4)(cid:27)(cid:19)(cid:12)
(cid:16)(cid:11)(cid:8)(cid:22)(cid:9)(cid:8)(cid:10)(cid:3)(cid:4)(cid:18)(cid:11)
(cid:16)(cid:21)(cid:9)(cid:11)(cid:4)(cid:7)(cid:15)(cid:16)(cid:11)(cid:8)(cid:22)(cid:9)(cid:8)(cid:10)(cid:3)(cid:4)(cid:18)(cid:11)
(cid:30)(cid:30)(cid:30)
(cid:16)(cid:3)(cid:8)(cid:17)(cid:11)(cid:18)(cid:2)(cid:11)(cid:18)(cid:19)(cid:8)(cid:7)(cid:12)(cid:20)
(cid:13)(cid:3)(cid:12)(cid:7)(cid:6)(cid:11)(cid:5)(cid:4)(cid:6)(cid:7)(cid:8)(cid:9)(cid:4)(cid:10)(cid:11)(cid:4)(cid:12)
(cid:30)(cid:30)(cid:30)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:4)(cid:6)(cid:7)(cid:8)(cid:9)(cid:4)(cid:10)(cid:11)(cid:4)(cid:12)
Fig. 3: Class diagram of the infrastructure for running experiments.
for each build type:
self.per_type_action(type)
for each benchmark:
self.per_benchmark_action(type, benchmark)
for each thread count:
self.per_thread_action(type, benchmark, thread_num)
for i in range(0, number_of_repetitions):
self.per_run_action(i)
Fig. 4: Experiment loop
Next, the user must create compiler-speciﬁc and
type-speciﬁc makeﬁles for different experiment variants
and put them under makefiles/. The compiler-speciﬁc
gcc_native.mk ﬁle would look like:
include common.mk
CC := gcc
CXX := g++
The type-speciﬁc ﬁle gcc_asan.mk would include the
previous ﬁle and additionally enable AddressSanitizer:
include gcc_native.mk
CFLAGS += −fsanitize=address
LDFLAGS += −fsanitize=address
After that, the user must put application-speciﬁc
makeﬁles and sources of the Phoenix benchmark suite
and Apache web server under src/. To keep directories
clean, standalone programs like Apache are put in a
separate subdirectory named applications/. In case
of Apache, the sources are downloaded from the Internet
using an installation script, thus the only ﬁle required
is a Makeﬁle. In case of Phoenix, all sources are copied
in the FEX directory tree for convenience; we only show
histogram for the sake of clarity. Application-speciﬁc
makeﬁles are generally simple and follow this pattern
on the histogram example:
NAME := histogram
SRC := histogram−pthread
include Makeﬁle.$(BUILD_TYPE)
all: $(BUILD)/$(NAME)
;; includes type-speciﬁc makeﬁle
;; build target
Finally, the user describes the experiments themselves.
The Phoenix performance-overhead experiment is put
under experiments/phoenix. The only required
ﬁle here is run.py which describes benchmarks
with their command-line arguments to be run and
measured. Additionally, each Phoenix benchmark needs
a preliminary dry run: this functionality is implemented
through a per_benchmark_action hook. Note that
most of the functionality to build and run benchmarks
is actually inherited from the abstract class implemented
in experiments/run.py.
There are no speciﬁc collect and plot scripts
for Phoenix. Instead, since the user has no ad-hoc
requirements for them, the generic collect.py and
plot.py are re-used. Also note that Figure 5 does not
show Apache experiment ﬁles for simplicity.
Some variants of
the experiment may require
setting speciﬁc environment variables. For example,
AddressSanitizer can be ﬁne-tuned via runtime ﬂags in
the ASAN_OPTIONS variable. For this, the user shall add
this ﬂag in environment.py. In addition, the user can
modify parameters for collection and plotting of results
in a config.py ﬁle.
In the end, to add a new experiment with new
compiler types and new benchmarks, the user needs to
create (some of) the following ﬁles: (1) installation scripts,
(2) compiler- and type-speciﬁc makeﬁles, (3) sources
and makeﬁles for benchmarks, and (4) experiment
descriptions.
the scripts are already
available in the repository of FEX.
In fact, all of
Additionally, FEX facilitates creation of tests—short
runs of benchmarks with tiny inputs. These tests can
be accessed via -i test and are useful to check if
user-deﬁned makeﬁles, source ﬁles, and other scripts
are written correctly. This functionality is implemented
inside run.py ﬁles.
B. Running new experiments
Now that the experiment description is ﬁnished,
the user can re-build the Docker container using
Dockerfile and deploy it on a test server. The actual
experiment proceeds in two stages as shown in Figure 1.
Inside the container, the user sets up the experiment
by invoking the relevant installation scripts:
>> fex.py install −n gcc−6.1
>> fex.py install −n phoenix_inputs
>> fex.py install −n apache
Next, it is sufﬁcient to call the generic all-in-one “run”
command like this:
>> fex.py run −n phoenix −t gcc_native gcc_asan
This command will build all Phoenix benchmarks using
native and AddressSanitizer GCC versions, run them
once (with a preliminary dry run), collect statistics from
logs, and aggregate and save ﬁnal data in a CSV table.
There are several command-line ﬂags to ﬁne-tune the
546
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:02:52 UTC from IEEE Xplore.  Restrictions apply. 
FEX
Dockerﬁle
fex.py
environment.py
conﬁg.py
install . . . . . . . . . . . . installation scripts to prepare environment
compilers
gcc-6.1.sh
dependencies
phoenix_inputs.sh
benchmarks
apache.sh
common.sh
makeﬁles. . . . . . . . . . . . . .makeﬁles for different variants to test
common.mk
gcc_native.mk