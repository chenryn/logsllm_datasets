title:A clinical study of risk factors related to malware infections
author:Fanny Lalonde L&apos;evesque and
Jude Nsiempba and
Jos&apos;e M. Fernandez and
Sonia Chiasson and
Anil Somayaji
A Clinical Study of Risk Factors
Related to Malware Infections
Fanny Lalonde Lévesque
Jude Nsiempba
École Polytechnique de
Montréal
Montréal, Canada
École Polytechnique de
Montréal
Montréal, Canada
José M. Fernandez
École Polytechnique de
Montréal
Montréal, Canada
Sonia Chiasson
Carleton University
Ottawa, Canada
Anil Somayaji
Carleton University
Ottawa, Canada
ABSTRACT
The success of malicious software (malware) depends upon
both technical and human factors. The most security con-
scious users are vulnerable to zero-day exploits; the best se-
curity mechanisms can be circumvented by poor user choices.
While there has been signiﬁcant research addressing the
technical aspects of malware attack and defense, there has
been much less research reporting on how human behavior
interacts with both malware and current malware defenses.
In this paper we describe a proof-of-concept ﬁeld study
designed to examine the interactions between users, anti-
virus (anti-malware) software, and malware as they occur
on deployed systems. The 4-month study, conducted in a
fashion similar to the clinical trials used to evaluate med-
ical interventions, involved 50 subjects whose laptops were
instrumented to monitor possible infections and gather data
on user behavior. Although the population size was limited,
this initial study produced some intriguing, non-intuitive
insights into the eﬃcacy of current defenses, particularly
with regards to the technical sophistication of end users.
We assert that this work shows the feasibility and utility
of testing security software through long-term ﬁeld studies
with greater ecological validity than can be achieved through
other means.
Categories and Subject Descriptors
K.6.5 [Management of computing and information
systems]: Security and Protection—Invasive software; K.4.2
[Computers and Society]: Social Issues—Abuse and crime
involving computers
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’13, November 4–8, 2013, Berlin, Germany.
Copyright 2013 ACM 978-1-4503-2477-9/13/11 ...$15.00.
http://dx.doi.org/10.1145/2508859.2516747.
Keywords
anti-virus evaluation; malware infection; ﬁeld study; user
behavior; clinical trial; risk factors
1.
INTRODUCTION
The growth of malicious activity on the Internet is un-
abated. Beyond sheer numbers of diﬀerent samples, what
has made the problem of malware detection and prevention
more complex is the variety of threats and attack vectors
that modern malware authors deploy to infect their victim’s
computers. In the early 2000’s massive computer worms in-
fected large swaths of the Internet in a few hours through
open network ports with almost no human intervention. To-
day, however, we have a situation closer to that of an earlier
era where threats were propagated through diskettes, email
attachments, and innocuous-looking Trojans. Malware in-
fections now occur largely because users are enticed to take
an action that leads to their computers being infected. To-
day, this could be opening an email attachment (still a pop-
ular classic), visiting a malicious Web site, or even willingly
installing a piece of software whose true intention they ig-
nore (move over cuteware, here comes the codec).
Meanwhile, anti-virus (AV) products have had to evolve
as well. The signature-based ﬁle-scanning engines that used
to be the core technology of AV products have been com-
plemented by multiple layers of protection, including identi-
ﬁcation of hazardous URLs, reputation-based software clas-
siﬁcation, system behaviour monitoring, etc. Computers
are no longer stand-alone machines that need to be pro-
tected as such, and what used to be a security problem
—their connectedness— is increasingly being leveraged by
AV vendors to better protect their customers. Periodic sig-
nature ﬁle updates are being replaced by on-demand re-
source lookups on databases in cloud infrastructures, who
are in turn fed by the continuous reporting of millions of
AV client installations. AV products have thus evolved into
complex pieces of “anti-malware” software, or rather complex
software systems involving several semi-independent com-
ponents with which, in some cases, the user must interact.
While many AV vendors try to make the installation and
operation of their product as transparent to the user as pos-
sible (or rather as “user-proof” as possible), the truth is that
how well the AV operates depends on its user. This depen-
97dence is not only due to user conﬁguration of the many fea-
tures of AV products, but also to other user-driven factors
such as how often the machine is connected to the Inter-
net, how often its software and signatures are updated, but
most importantly, how the humans in front of the machine
are interacting with the computer and the Internet when
confronted with situations where their actions could lead to
infection.
In other words, the operating environment of both mal-
ware and AV products not only includes the machine it is
trying to penetrate/protect, but also the network that con-
nects it to the rest of the world and the user that sits in front
of it. Indeed, the human is part of the operating environ-
ment of the machine, including the software that attempts
to execute on it or protect it. It thus seems natural to adopt
a Homo in machina (human in the machine) approach to
evaluate not only the performance of AV products but also
the susceptibility of users to getting their machines infected.
This change of paradigm is fundamental if we want to
better understand what role user actions really play in the
process of infection.
In particular, it becomes paramount
to understand how user characteristics, such as demograph-
ics, perception of threat, computer literacy, and user actions
may aﬀect the risk of infection by malware. For instance,
while we might hypothesise that users who spend a lot of
time browsing less reputable Web sites are more prone to
malware infections based on the fact that they are often used
by criminals to spread malware, it is important to be able to
quantitatively conﬁrm such hypotheses. This question goes
beyond the performance evaluation of the AV alone, but of
the AV with the human or of the human by itself.
This philosophy of Homo in machina is also in sharp con-
trast with current AV evaluation methods, largely based on
automated tests in controlled environments, where machines
installed with an AV product are subject to various infect-
ing stimuli.
In order to better assess the eﬀectiveness of
the multi-layered protection of modern AV, traditional ﬁle-
scanning tests (also called “static” or “on-demand” tests in
the AV industry) have largely fallen out of favour and are
being replaced with tests involving known bad URLs (also
called “dynamic” or “real-world”). While this latter type
of tests does evaluate the performance of AV products as
a whole (and not that of individual features), the truth is
that they are not “real world” in that the eﬀect of the main
contributing factor in infections, i.e. the human factor, is
not being measured. In addition, the results of these tests
are often biased because the stimuli chosen by the tester
could very well not be representative of what is typically
experienced by the average user; this is often refereed to as
the sample selection problem. Furthermore, such test do not
measure the eﬀectiveness of AV products at communicat-
ing risk eﬀectively to the user and how they can aﬀect user
behaviour.
What we need then is to test using a methodology that
can evaluate the interactions between humans, malware, and
AV products (and other security software) with much greater
ecological validity. Any testing methodology will have some
impact on the phenomena being tested; to minimise this
impact, testing needs to be done in an environment as close
as possible to that of normal usage. One potential way to
achieve such ecological validity is through conducting clin-
ical trials of software as we proposed in 2009 [24]. With
clinical trials, security software is installed and monitored
on systems in regular use by regular users. Data is then
gathered on the performance of the security software in pro-
tecting the system and on how the user interacted with the
system during this time period. By correlating user be-
haviour, application use, and security software activity, we
can gain insights into the interactions between all three in
an ecologically valid context.
Thus, in order to address both the question of how to
better evaluate AV and how to understand the inﬂuence of
the human factor in infections, we decided to conduct such
a clinical trial of anti-virus.
In this paper we report on
the ﬁrst study of this kind, conducted at the ´Ecole Poly-
technique de Montr´eal from October 2011 to April 2012,
involving 50 participants that used their own computers in
everyday life during a 4-month period. The data collected
during the study attempted to take into consideration many
of the reasonable factors that could inﬂuence infection such
as user proﬁling, user behaviour, host conﬁguration and en-
vironment. In addition, the study collected data that would
allow us to evaluate the performance of the AV and in some
cases determine the causes of infections. In this paper, how-
ever, while we do present results on AV performance, we
concentrate on ﬁnding and understanding the correlations
between these factors and occurrences of infection, in order
to determine which ones could be identiﬁed as risk factors.
Detailed performance analysis of the various AV protection
mechanisms through the determination of the causal mech-
anisms of infection is out of scope of this paper and will be
the object of future research based on further analysis of the
data produced by this experiment.
We should note that other methodologies, such as bench
tests, cognitive walkthroughs, and lab-based user testing can
give ﬁner-grained insight into user and system behaviour
than is possible in clinical trials, and can do so at lower
cost. As we show here, though, clinical trials can gain insight
into how systems perform and how users interact with them
in practise, something that cannot be addressed with these
other methodologies.
The remainder of the paper is organised as follows. Sec-
tion 2 presents the related work. Section 3 describes the
study and provides a summary of its methodology, a detailed
description of which is also given in a previous methodology
publication [14].
In Section 4, we describe and discuss in
detail the results of the study in terms of threat detections
by AV, missed detections, and identiﬁcation of potential risk
factors related to user characteristics and behaviour. While
the study itself and a few of the preliminary results, espe-
cially on AV performance, were already presented at a indus-
try conference shortly after the conclusion of the study [15],
Section 4 contains a more in-depth and complete statistical
analysis and interpretation of the study results, with an em-
phasis on risk factors. We discuss limitations, applications,
and future work in Section 5 and conclude in Section 6.
2. RELATED WORK
Numerous studies evaluating the performance of AV prod-
ucts and the inﬂuence of user interactions factors on IT se-
curity have been conducted in recent years. There are cur-
rently several methods for evaluating anti-malware products
used in the AV industry [4, 7, 8], but they do not reﬂect
the performance of products in real life. Typical evalua-
tion methods conducted by commercial testing labs (e.g. [2,
19]) are based on scanning collected or synthesised malware
98samples along with legitimate programmes. While such ap-
proaches can measure raw detector accuracy, they cannot
take into account factors such as user interactions, evolv-
ing threats, and diﬀerent environments. One major issue
is that the sample collection is often too small, inappropri-
ate, and not validated [9, 12]. Even with a well-maintained
malware collection, testing against such data sets has be-
come unreliable due to the increasingly dynamic nature of
malware. To partially address this issue, Vrabec and Harley
[27] proposed emulating user interaction with the system
and creating user-speciﬁc testing scenarios. Another alter-
nate method for evaluating the performance of desktop AV
is through the use of on-demand detection tools, that in ad-
dition to detecting installed threats can detect whether an
AV was installed and which one. For example, the security
company SurfRight made public a report [25] describing a
55-day study conducted at the end of 2009 involving more
than 100,000 machines that used their product. This rep-
port included statistics of detections missed by the installed
desktop AV products. Finally, infection statistics based on
self-reporting of (perceived) security incidents by users, can
be obtained from user surveys and be used to estimate AV
performance, such as in the report published by Eurostat
—the statistical oﬃce of the European Union— based on
the 12-month reporting period in 2010 [6]. Unfortunately,
self-reported rates of infection are probably quite inaccurate.
Finally, the experiment proposed in [24] eliminates many of
these problems and potential inaccuracies by adapting the
concept of clinical trials to the computer security domain.
The initial proposal was to evaluate security products using
methods and controls similar to those used in clinical trials
of medical products.
Other ﬁeld studies in computer security have been con-
ducted following an ethnographic approach. This type of
approach explores the impacts of the manners, the customs,
and the social, physical, and ﬁscal environment of users
when they are facing computer security decisions.
It pri-
marily uses qualitative methods such as surveys and observa-
tions to understand how and why participants interact with
computer systems. For example, Botta et al. [3] conducted
an ethnographic study of security professionals. Rode [20]
used this approach to examine parental behaviour in protect-
ing children’s on-line safety. The study showed that there
are a host of security threats of which the children are not
aware and provides an overview of parental rules and strate-
gies for keeping children safe. Wash [28] used interviews to
understand users’ mental models of security. He identiﬁed
eight ‘folk models’ of security threats that are used by home
computer users to decide what security software to use, and
which expert security advice to follow. De Luca et al. con-
ducted a ﬁeld observation of ATM users to evaluate PIN us-
age [5]. This study shows that contextual factors on security
such as distractions, physical hindrance, trust relationships,
and memorability have a big inﬂuence on PIN-based ATM
use. Lastly, Sheng et al.
focus on susceptibility of users
against phishing attacks [21]. They conclude from the re-
sults of an on-line study with 1001 users that prior exposure
to phishing education is associated with less susceptibility
to phishing, suggesting that phishing education may be an
eﬀective tool. They also found that age is a contributing risk
factor. Indeed, young people aged 18 to 25 are also more sus-
ceptible to phishing. This last assertion is conﬁrmed by a
study conducted by Milne et al. [16] that concluded, through
a survey of 449 on-line buyers (also students), that age af-
fected the behaviour of subjects when faced with a potential
computer threat. Ngo and Paternoster [17] used the results
of a survey based on the self-assessment of 295 students, to
deduct that age is a signiﬁcant risk factor of infection, with
older respondents being less likely to get infected.
Another methodological approach is that based on the
concepts and methods of Epidemiology. This approach, sim-
ilarly as the ethnographic approach, uses statistical meth-
ods. However, it refers to the threat and how it spreads,
while ethnography refers to environmental and demographic
factors that inﬂuence the user when faced with this threat.
By analogy to the ﬁeld of health, the concepts that underlie
this approach are: infection, detection, disinfection, quaran-
tine, epidemics, environment control, etc. [22]. Its methods
are borrowed from the biological sciences in order to de-
termine the likely causes and risk factors for infection, un-
derstanding the spread of malware and, where appropriate,
the methods to remedy it. This approach has been used
in many studies. We summarise three of them here. First
of all, Carlinet et al. used it to analyse the behaviour of
ADSL customers and identify customer characteristics that
are risk factors for malware infection. The study showed
that using the Windows operating system and heavy us-
age of Web applications and streaming are major risk fac-
tors of malware infection. Unfortunately, the study does not
say anything about the type of Web sites as a risk factor.
Secondly, Kephart and White [10] attempted to adapt the
epidemiological approach to determine the conditions under
which virus epidemics are likely to occur, and in cases where
they do, they explore the dynamics of the expected number
of infected individuals as a function of time. They concluded
that there is a threshold of rate of infection from which we
can speak of an epidemic. Thirdly, Kondakci [11] used a
stochastic model to analyse the epidemic states of infected
computers and determine the state probabilities of suscepti-
ble populations. He shows that there is more than one state
transition. A healthy but susceptible computer can become
infected by a virus, can then become a transmitter, and can
also return to a healthy state.
Regarding the impact of a user’s domain of expertise on
risk of infection, Solic and Ilakovac [23] conducted studies
with two groups of participants: one group consisting of 19
doctors and another of 20 professors from an engineering
school, who anonymously answered a questionnaire on their
behaviour when faced with threats to their computer and
the consequences that ensue. This study concluded that the
domain of expertise does not have a major impact on user
behaviour when it comes to threats and security risk.