93,558
31,921
16,518
10,956
8,755
n/a
n/a
48,042
8,735
2,556
1,049
479
n/a
n/a
24,448
2,282
378
60
15
n/a
n/a
12,170
585
47
5
1
n/a
n/a
12,373
582
46
4
0
n/a
n/a
6,081
148
6
0
0
n/a
n/a
6,213
144
3
0
0
n/a
n/a
3,045
37
1
0
0
n/a
3,156
48
0
0
0
n/a
Table 2. Predicted and actual number of false positives for 1 million pseudo-random hashes queried
against a BFs loaded with the 13.1 million hashes of RDS 2.19. Entries with 1,000,000 at the left of
the table indicates that every hash value was a false positive; entries marked with 0 at the right of the
table indicate that there were no false positives. Entries marked n/a cannot be computing because
there are not sufﬁcient number of bits in the 160-bit SHA-1. This analysis indicates that m = 232, k = 5
appears to be the optimal value for a Bloom ﬁlter designed to hold SHA-1 values.
We measured the time that it took to perform two sets
of queries against each object. The ﬁrst set was 1 million
SHA1 hashes taken from RDS 2.19—hashes that were guar-
anteed to be in the data set. Next we measured the time
that it took to perform 1 million pseudo-random queries of
hashes that were known not to be in the data set. From this
number we could determine the number of queries per sec-
ond that each conﬁguration delivered (Figure 1).
As expected the BFs were faster than both the binary
searches through the sorted text ﬁle used by SleuthKit and
MySQL’s InnoDB tables. Also as expected, it is dramati-
cally faster to lookup a hash that is not in the BF than one
that is in the BF—this is because our search routine stops
searching the moment it retrieves the ﬁrst unset bit i0 from
vector F .
3.3.2 Effects of m on speed
In Section 2.2 we asserted that smaller BFs would have
higher performance on modern hardware due to L1 and L2
cache performance. To test this assertion we constructed
multiple ﬁlters with k = 5 and m stepping from 28 to 232.
We then inserted 1 million pseudorandom values into the
hash and searched for each of these values.3
3Searching for known values is the slowest operation for our BF imple-
mentation; searching values not to be present with a BF of k = 5 has the
same performance as searching values not to be present in a ﬁlter of k = 1
but with the same constant overhead. Since we were interested in measur-
This attempting to measure the impact of the L1 and L2
cache was frustrated because these tests were done on an
Internet-accessible multi-user machine running Linux: a lot
of other processes were competing for the cache. On the
other hand, this conﬁguration is similar to what most prac-
titioners will be using—a complex operating system that is
running multiple tasks at once. Nevertheless, we did ob-
serve a signiﬁcant decrease in the BF’s lookup performance
as the ﬁlter increased in size (Figure 2). The graph also
shows an inﬂection point as the size of the graph reaches
the size of the benchmark system’s L2 cache, as indicated
by the dashed line.
3.3.3 Effects of k on speed
In Section 3.1 we asserted that BFs with smaller num-
bers of hash functions (i.e. BFs with smaller k) would
have higher performance due to the computational over-
head of each hash function and retrieving the corresponding
bit from memory. To look for this effect, we constructed
multiple ﬁlters with m = 220 and k ranging from 1 to 5.
When then followed the same procedure of inserting 1 mil-
lion pseudorandom values and looking up those values. As
expected, performance decreased as k increased.
Indeed,
the number of lookups per second is roughly proportional
ing the performance of the BF and not the overhead of our implementation,
we used k = 5.
717
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:00:48 UTC from IEEE Xplore.  Restrictions apply. 
Method
Bloom Filters
SleuthKit
MySQL InnoDB
In Set Not in Set
1,314,060
17,595
4,369
424,808
16,942
4,415
Figure 1. Queries per second for 160-bit
SHA1 hashes against M = 32, k = 5 Bloom
Filters, SleuthKit’s hfind, and MySQL’s
SELECT statement (with InnoDB tables). “In
Set” refers to hashes that are in RDS 2.19,
while “Not in Set” refers to hash values that
are not in RDS 2.19.
k (Figure 3).
to 1
3.4. Batch forensics with fiwalk
We have incorporated our BF implementation into
fiwalk, an open source batch disk forensic analysis pro-
gram. fiwalk uses Carrier’s SleuthKit to perform a batch
analysis and extraction of allocated and deleted ﬁles from
all of the partitions resident in a disk image that is to be an-
alyzed. fiwalk’s output is a walk ﬁle that includes a list of
every ﬁle, the ﬁle’s metadata, and optionally the ﬁle’s MD5
or SHA1 cryptographic hash.
The current version of fiwalk allows ﬁles to be spec-
iﬁed by their name or extension. We modiﬁed fiwalk to
allow the use of BF for inclusion or exclusion as well. We
further modiﬁed fiwalk so that it can generate a BF based
on the ﬁles that it ﬁnds in a disk image.
3.5. RDS Coverage of Windows
We created VMWare machines Windows 2000 Service
Pack 4, Windows XP Service Pack 2 and Windows Vista
Business. The .vmdk ﬁles were converted to raw ﬁles
with qemu-img[1] and processed with fiwalk to cre-
ate “walk” ﬁles containing all ﬁles on the image and the
SHA1 hashes of the ﬁles. These walk ﬁles were then com-
pared against our baseline RDS v2.19 bloom ﬁlter. Next
we patched all of the virtual machines with the latest hot
ﬁxes from Microsoft Update and reprocessed the VMs. The
Figure 2. Increasing the size of the Bloom
ﬁlter decreases its speed due to caching is-
sues. The dashed line indicates the 2 MB
(8 Mbit) size of the benchmark system’s L2
cache. (Note: these speeds are for success-
ful lookups; the speed of looking up hash val-
ues not in the ﬁlter are roughly 12× faster.)
Figure 3. Increasing the number of bits per
element in the Bloom ﬁlter decreases its
speed, since more work needs to be done to
look up each element.
percentage of ﬁles in the virtual machines that appeared in
RDS 2.19 are presented in Table 3.
These results yield several interesting data points. First,
even on base operating system installs with hot ﬁxes in-
stalled, only 60–70% of ﬁles are covered by the RDS. Why
not more? Some ﬁles are unique per system, a result of
hardware signatures, chargeable registration keys, and user-
names. Swap ﬁles will invariably differ between machines.
It also shows the amount of updates that Microsoft has is-
sued since the RDS 2.19 was released in December, 2007.
Table 3 shows an analysis of the ﬁles that were present
818
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:00:48 UTC from IEEE Xplore.  Restrictions apply. 
In SetNot in Set103104105106107Queries per secondQuery times of hash lookupBloom FiltersText FilesMySQL5101520253035M020040060080010001200thousand lookups per secondLookup speed as a function of M (k=5)1.01.52.02.53.03.54.04.55.0k100200300400500600700thousand lookups per secondLookup speed as a function of k (M=32)PNF ﬁles in the WINDOWS/inf directory
Windows PC Health Ofﬂine Cache ﬁles
VMWare Tools installation ﬁlesa
Start Menu links
Other Windows System ﬁles
Miscellaneous system log ﬁles
Windows wbem autorecover ﬁles
Other PC Health ﬁles
Windows System Restore Files
Other Documents and Settings ﬁles
Windows Prefetch ﬁles
Miscellaneous system text ﬁles
File system metadata ﬁles
Other system shortcuts
707
321
130
95
77
69
53
40
38
41
31
15
8
7
aArtifact of VMWare; not part of the Windows XP base release.
Table 3. Breakdown of the 1635 ﬁles in the
Windows XP base installation which were not
present in NSRL RDS.
Figure 4. RDS 2.19 coverage of Windows in-
stallations
in our base installation of Microsoft Windows XP but which
were not in the RDS. The majority of them are ﬁles result-
ing from the installation of software during installation or
logﬁles resulting from actually running the system.
Overall, the number of ﬁles not covered by RDS in these
virtual machines is a cause for concern: although many ﬁles
are removed for the potential examiner, a signiﬁcant num-
ber remain. If the primary purpose for RDS is to eliminate
known good ﬁles so that they do not need to be examined,
then it is not delivering on this promise.
919
3.6. Coverage of Real Data
To evaluate the RDS against real-world data, we per-
formed ﬁle system walks of 891 hard drives purchased on
the secondary market between 1998 and 2006. Overall, 45
drives had greater than 80% coverage by RDS, 33 which
contained a signiﬁcant number of ﬁles. For the 280 drives
with > 100 ﬁles, RDS covered on average 36.64% of ﬁles.
For the 186 drives with > 5000 ﬁles, RDS coverage aver-
aged 36.62%. Once again, RDS helps reduce what must be
consulted, but not by much.
3.7. Proﬁling Hard Drives
Although the main use of RDS today is to eliminate
“known goods” from analysis, we believe that the cover-
age of RDS that we have seen limits this use. An alternative
use of this resource is to use the RDS metadata to proﬁle
uses that a hard drive may have had.
We have created an application which attempts to pro-
ﬁle a hard drive by looking up each ﬁle’s SHA1 in the RDS
database and retrieving a list of all the RDS objects with a
matching hash value, and then retrieving a list of the product
names as identiﬁed by NIST. Each hash code may appear
multiple times in the RDS, each appearance corresponding
to a different product. In the cases where only a single prod-
uct name is matched, that product name is added to a list.
Using RDS in this manner allows us to get a rapid han-
dle on the kind of software that is present on the hard drive
even in cases where the programs themselves cannot be re-
covered because of ﬁle deletion.
4. Attacks on Bloom Filter Distributions
There is on signiﬁcant problem with distributing a hash
set as a Bloom Filter: it is dramatically easier for an attacker
to construct a hash collision within the BF than it is to ﬁnd
a hash collision with a collision-resistant function such as
SHA-1. With collisions easier to ﬁnd, an attacker could
use this approach as a way of hiding contraband data from
forensic tools that use BFs to eliminate “known goods.”
Assuming that SHA-1 is a strong hash function, the only
way to ﬁnd a hash collision is by a brute force attack—with
odds of 14 million out of 2160 (assuming 14 million unique
hashes in the RDS). Using a hash collision to hide contra-
band data, then, would require appending a block of data to
the contraband and then making small, incremental changes
to that block of data until the hash collision is found. In
practice, a collision will never be found with a brute force
approach such as this.
However, if those 160 bits are divided into 5 groups of
32 and stored in a BF with m = 232 and k = 5, ﬁnd-
ing a collision becomes much easier. The 14 million hash
codes represent a maximum of 14 × 5 = 70 million dis-
tinct 32 bit codes, all stored in the same ﬁlter. Finding a
false positive for k = 5 requires ﬁnding a single hash for
which each of its 5 groups of 32 bits are set in that ﬁlter.
The probability for each group of 32 is 70 million : 232 or
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:00:48 UTC from IEEE Xplore.  Restrictions apply. 
W2K SP4W2K SP4 HotfixesXP SP2XP SP2 HotfixesVistaVista Hotfixes0102030405060708090100Percent CoverageRDS 2.19 coverage of various Windows installs5. Implications
This section explores a variety of forensic applications