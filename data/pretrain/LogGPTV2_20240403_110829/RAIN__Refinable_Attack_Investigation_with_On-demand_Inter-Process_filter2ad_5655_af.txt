15.8
18.2
115.4
413
327
Total
201
166
114
105
133
740
Table 6: Storage cost (MB).
resulting size of compilation, it shows Rain’s ability to re-construct
(and analyze) a transient state of a complex program execution (we
presented the storage footprint of daily usage in §4.3).
9 LIMITATIONS AND DISCUSSION
This section presents the limitations of Rain and directions of fu-
ture work. One limitation of Rain is that it is a kernel-based system.
It is able to record, replay, and analyze the activities of user-level
processes but unable to monitor the kernel activities because it
trusts the kernel. If the kernel is compromised, Rain is no longer
able to create reliable provenance data. Thus if such an incident
ever occurred, Rain could use kernel-integrity monitoring tech-
niques [25, 32, 38, 43, 47] that detect and filter out misinformation.
In the future, we will port Rain to a hypervisor that records and
replays kernel activities while reducing attack surfaces. This ap-
proach will allow Rain to support commercial off-the-shelf (COTS)
OSes (e.g., Windows). In addition, the semantic information of
Rain poses a limitation because it is less comprehensive than that
of source-code-instrumentation-based approaches [33, 34] since it
assumes no assistance from software developers (i.e., annotation).
We believe that these approaches are orthogonal: while Rain is
successful at extracting fine-grained information from COTS pro-
grams or unknown binaries (e.g., malware), instrumentation-based
approaches are effective at collecting semantically rich information
from supportive programs. Another limitation is the over-tainting
issue that we encountered, particularly when dealing with control
flow-based propagation. This problem, which has always plagued
DIFT approaches needs to be addressed. Since Rain relies on trig-
gering analysis to initiate a fine-grained analysis, it could either
PROV-Tracer [50] is built on top of PANDA [20], which can also pro-
vides instruction-level granularity. However, the QEMU emulator
it bases on is around 5× slower than a native execution. We sum-
marize the comparisons between Rain and previous provenance
systems in Table 7.
Network Provenance Systems. Network provenance systems [11,
15, 16, 55] focus on tracking network-level provenance between
computing hosts belonging to the same distributed or enterprise
network environment. Their main goal is to find faulty (or compro-
mised) hosts that attempt to attack other hosts in the same network
by monitoring and analyzing network traffic, and to ascertain how
the faulty hosts were compromised. Since they focus on network
traffic, system-level fine-grained provenance (e.g., CPU instructions
that were executed) are beyond the scope of the work that proposed
these systems. Thus, Rain is orthogonal to these network prove-
nance systems such that both can be simultaneously used to fully
cover intra- and inter-system provenance.
Replay-Based Decoupled Analysis. Unlike the deterministic re-
play technique that faithfully replays the previous execution (e.g.,
in [21, 42, 45, 48]), the replay-based decoupled analysis technique
enables sophisticated analysis during replay. Arnold [19] is a state-
of-the-art record-and-replay system supporting decoupled analysis
during replay. The two main advantages of Arnold over two sim-
ilar systems, Aftersight [17] and PANDA [20], are 1) its minimal
recording overhead and 2) its process-group-wise replay with Intel
Pin. These advantages stem from the implementation of Arnold
inside the Linux kernel. By contrast, Aftersight is based on both
a VMWare hypervisor (record) and QEMU (replay), and PANDA
is purely based on QEMU. The main disadvantage of Arnold is
that, unlike the other two systems, it is built inside the kernel, so it
cannot record and replay the execution of the kernel.
We choose Arnold [19] as the base system of Rain mainly be-
cause of its efficiency. Note that Rain’s functionalities (e.g., full-
system recording, provenance data generation, reachability analy-
sis, and refinable DIFT) are orthogonal to Arnold, so we can apply
them to other systems easily. In fact, we have PANDA-based Rain,
which provides the same functionalities even though its recording
overhead is excessive (five times as high) mainly resulting from
QEMU.
Decoupled Taint Analysis. Dynamic taint analysis [18, 22, 29,
41, 51, 54] is a well-known technique for tracking the data flow
from a source to a sink. Taint analysis is useful for runtime security
policy enforcement [41, 51], malware analysis [54], and privacy
leakage detection [22]. However, because of its excessive perfor-
mance overhead (e.g., the overhead of one state-of-the-art imple-
mentation, libdft [29], is six times as high), it is difficult to use it
in a general computing environment. To solve this performance
problem, several studies have proposed decoupled taint analysis
techniques [27, 36, 37, 44]. The purpose of these techniques is to run
a target process with a CPU core while performing taint analysis
for a process with other idle CPU cores.
11 CONCLUSION
We presented Rain, a practical attack investigation system with
runtime efficiency and refinable granularity (from system call to
instruction). By leveraging a record-and-replay technique, Rain
achieves efficiency using graph-based analysis to prune out unre-
lated executions, and it performs DIFT only on relevant executions
to identify fine-grained causality. We demonstrated Rain’s effec-
tiveness by applying it to an evaluation dataset to perform a precise
causality analysis of sophisticated attacks.
12 ACKNOWLEDGMENTS
We thank the anonymous reviewers for their helpful feedback.
This research was supported in part by NSF, under awards CNS-
0831300, CNS-1017265, DGE-1500084, CCF-1548856, CNS-1563848,
SFS-1565523, CRI-1629851, and CNS-1704701, ONR, under grants
N000140911042, N000141512162, and N000141612710, DARPA TC
(No. DARPA FA8650-15-C-7556) and XD3 programs (No. DARPA
HR0011-16-C-0059), NRF-2017R1A6A3A03002506, ETRI IITP/KEIT
[B0101-17-0644], and gifts from Facebook, Mozilla, and Intel.
REFERENCES
[1] 2017. Apache Avro. (Oct. 2017). https://avro.apache.org.
[2] 2017. DARPA Transparent Computing Program. (Oct. 2017). http://www.darpa.
mil/program/transparent-computing.
[3] 2017. DTrace. (Oct. 2017). https://www.dtrace.org.
[4] 2017. Linux Audit. (Oct. 2017). https://linux.die.net/man/8/auditd.
[5] 2017. Mozilla rr. (Oct. 2017). http://rr-project.org.
[6] 2017. Neo4j Graph Database. (Oct. 2017). http://neo4j.com.
[7] 2017. Snort. (Oct. 2017). https://www.snort.org.
[8] 2017. Squid. (Oct. 2017). http://www.squid-cache.org.
[9] 2017. Sysdig. (Oct. 2017). https://www.sysdig.org.
[10] Mona Attariyan, Michael Chow, and Jason Flinn. 2012. X-ray: automating root-
cause diagnosis of performance anomalies in production software. In Proceedings
of the 10th USENIX Symposium on Operating Systems Design and Implementation
(OSDI). Hollywood, CA.
[11] Adam Bates, Kevin Butler, Andreas Haeberlen, Micah Sherr, and Wenchao Zhou.
2014. Let SDN be your eyes: Secure forensics in data center networks. In 2014
NDSS Workshop on Security of Emerging Network Technologies (SENT).
[12] Adam Bates, Dave (Jing) Tian, Kevin R.B. Butler, and Thomas Moyer. 2015. Trust-
worthy Whole-System Provenance for the Linux Kernel. In Proceedings of the
24th USENIX Security Symposium (Security). Washington, DC.
[13] Kurt Baumgartner. 2017. On the StrongPity Waterhole Attacks Targeting Italian
and Belgian Encryption Users. (Oct. 2017). https://securelist.com/blog/research/
76147.
[14] Ang Chen, W. Brad Moore, Hanjun Xiao, Andreas Haeberlen, Linh Thi Xuan
Phan, Micah Sherr, and Wenchao Zhou. 2014. Detecting Covert Timing Channels
with Time-Deterministic Replay. In Proceedings of the 11th USENIX Symposium
on Operating Systems Design and Implementation (OSDI). Broomfield, Colorado.
[15] Ang Chen, Yang Wu, Andreas Haeberlen, Boon Thau Loo, and Wenchao Zhou.
2017. Data Provenance at Internet Scale: Architecture, Experiences, and the Road
Ahead. In Conference on Innovative Data Systems Research (CIDR’17).
[16] Ang Chen, Yang Wu, Andreas Haeberlen, Wenchao Zhou, and Boon Thau Loo.
2016. The Good, the Bad, and the Differences: Better Network Diagnostics with
Differential Provenance. In Proceedings of the 2016 ACM SIGCOMM. Florianopolis,
Brazil.
[17] Jim Chow, Tal Garfinkel, and Peter M. Chen. 2008. Decoupling dynamic pro-
gram analysis from execution in virtual environments. In Proceedings of the 2008
USENIX Annual Technical Conference (ATC). Boston, MA.
[18] James Clause, Wanchun Li, and Alessandro Orso. 2007. Dytan: a generic dy-
namic taint analysis framework. In Proceedings of the International Symposium
on Software Testing and Analysis (ISSTA). London, UK.
[19] David Devecsery, Michael Chow, Xianzheng Dou, Jason Flinn, and Peter M Chen.
2014. Eidetic systems. In Proceedings of the 11th USENIX Symposium on Operating
Systems Design and Implementation (OSDI). Broomfield, Colorado.
[20] Brendan Dolan-Gavitt, Josh Hodosh, Patrick Hulin, Tim Leek, and Ryan Whelan.
2015. Repeatable Reverse Engineering with PANDA. In Proceedings of the 5th
Program Protection and Reverse Engineering Workshop (PPREW).
[21] George W. Dunlap, Samuel T. King, Sukru Cinar, Murtaza A. Basrai, and Pe-
ter M. Chen. 2002. ReVirt: Enabling Intrusion Analysis through Virtual-Machine
Logging and Replay. In Proceedings of the 5th USENIX Symposium on Operating
Systems Design and Implementation (OSDI). Boston, MA.
[22] William Enck, Peter Gilbert, Byung-Gon Chun, Landon P. Cox, Jaeyeon Jung,
Patrick McDaniel, and Anmol N. Sheth. 2010. TaintDroid: An Information-Flow
Tracking System for Realtime Privacy Monitoring on Smartphones. In Proceedings
of the 9th USENIX Symposium on Operating Systems Design and Implementation
(OSDI). Vancouver, Canada.
[23] Earlence Fernandes, Justin Paupore, Amir Rahmati, Daniel Simionato, Mauro
Conti, and Atul Prakash. 2016. FlowFence: Practical Data Protection for Emerg-
ing IoT Application Frameworks. In Proceedings of the 25th USENIX Security
Symposium (Security). Austin, TX.
[24] Ashish Gehani and Dawood Tariq. 2012. SPADE: support for provenance auditing
in distributed environments. In Proceedings of the 13th International Middleware
Conference (Middleware).
[25] Owen S. Hofmann, Alan M. Dunn, Sangman Kim, Indrajit Roy, and Emmett
Witchel. 2011. Ensuring Operating System Kernel Integrity with OSck. In Pro-
ceedings of the 16th ACM International Conference on Architectural Support for
Programming Languages and Operating Systems (ASPLOS). Newport Beach, CA.
[26] Yeongjin Jang, Simon P Chung, Bryan D Payne, and Wenke Lee. 2014. Gyrus:
A Framework for User-Intent Monitoring of Text-based Networked Applica-
tions. In Proceedings of the 2014 Annual Network and Distributed System Security
Symposium (NDSS). San Diego, CA.
[27] Kangkook Jee, Vasileios P Kemerlis, Angelos D Keromytis, and Georgios Portoka-
lidis. 2013. ShadowReplica: efficient parallelization of dynamic data flow tracking.
In Proceedings of the 20th ACM Conference on Computer and Communications
Security (CCS). Berlin, Germany.
[28] Yang Ji, Sangho Lee, and Wenke Lee. 2016. RecProv: Towards Provenance-Aware
User Space Record and Replay. In Proceedings of the 5th International Provenance
and Annotation Workshop (IPAW). Mclean, VA.
[29] Vasileios P Kemerlis, Georgios Portokalidis, Kangkook Jee, and Angelos D
Keromytis. 2012.
libdft: Practical dynamic data flow tracking for commodity
systems. In Proceedings of the 8th ACM SIGPLAN/SIGOPS International Conference
on Virtual Execution Environments. London, UK.
[30] Taesoo Kim, Ramesh Chandra, and Nickolai Zeldovich. 2012. Recovering from
intrusions in distributed systems with DARE. In Proceedings of the 3rd Asia-Pacific
Workshop on Systems (APSys). Seoul, South Korea.
[31] Taesoo Kim, Xi Wang, Nickolai Zeldovich, and M Frans Kaashoek. 2010. Intru-
sion Recovery Using Selective Re-execution. In Proceedings of the 9th USENIX
Symposium on Operating Systems Design and Implementation (OSDI). Vancouver,
Canada.
[32] Hojoon Lee, HyunGon Moon, DaeHee Jang, Kihwan Kim, Jihoon Lee, Yunheung
Paek, and Brent ByungHoon Kang. 2013. KI-Mon: A Hardware-assisted Event-
triggered Monitoring Platform for Mutable Kernel Object. In Proceedings of the
22th USENIX Security Symposium (Security). Washington, DC.
[33] Kyu Hyung Lee, Xiangyu Zhang, and Dongyan Xu. 2013. High Accuracy Attack
Provenance via Binary-based Execution Partition. In Proceedings of the 20th
Annual Network and Distributed System Security Symposium (NDSS). San Diego,
CA.
[34] Shiqing Ma, Xiangyu Zhang, and Dongyan Xu. 2016. ProTracer: towards practical
provenance tracing by alternating between logging and tainting. In Proceedings
of the 2016 Annual Network and Distributed System Security Symposium (NDSS).
San Diego, CA.
[35] Emaad A Manzoor, Sadegh Momeni, and Leman Akoglu. 2016. Fast Memory-
efficient Anomaly Detection in Streaming Heterogeneous Graphs. In Proceedings
of the 22nd ACM SIGKDD Knowledge Discovery and Data Mining (KDD). San
Francisco, CA.
[36] Jiang Ming, Dinghao Wu, Jun Wang, Gaoyao Xiao, and Peng Liu. 2016. Straight-
Taint: decoupled offline symbolic taint analysis. In Proceedings of the 31st
IEEE/ACM International Conference on Automated Software Engineering (ASE).
Singapore.
[37] Jiang Ming, Dinghao Wu, Gaoyao Xiao, Jun Wang, and Peng Liu. 2015. TaintPipe:
pipelined symbolic taint analysis. In Proceedings of the 24th USENIX Security
Symposium (Security). Washington, DC.
[38] Hyungon Moon, Hojoon Lee, Jihoon Lee, Kihwan Kim, Yunheung Paek, and
Brent Byunghoon Kang. 2012. Vigilare: Toward Snoop-based Kernel integrity
Monitor. In Proceedings of the 19th ACM Conference on Computer and Communi-
cations Security (CCS). Raleigh, NC.
[39] Kiran-Kumar Muniswamy-Reddy, Uri Braun, David A Holland, Peter Macko,
Diana L MacLean, Daniel W Margo, Margo I Seltzer, and Robin Smogor. 2009.
Layering in Provenance Systems. In Proceedings of the 2009 USENIX Annual
Technical Conference (ATC). San Diego, CA.
[40] Kiran-Kumar Muniswamy-Reddy, David A. Holland, Uri Braun, and Margo Seltzer.
2006. Provenance-Aware Storage Systems. In Proceedings of the 2006 USENIX
Annual Technical Conference (ATC). Boston, MA.
[41] James Newsome and Dawn Song. 2005. Dynamic taint analysis for automatic
detection, analysis, and signature generation of exploits on commodity soft-
ware. In Proceedings of the 12th Annual Network and Distributed System Security
Symposium (NDSS). San Diego, CA.
[42] Harish Patil, Cristiano Pereira, Mack Stallcup, Gregory Lueck, and James Cownie.
2010. PinPlay: A Framework for Deterministic Replay and Reproducible Analysis
of Parallel Programs. In Proceedings of the 8th Annual IEEE/ACM International
Symposium on Code Generation and Optimization (CGO).
[43] Nick L Petroni Jr, Timothy Fraser, Jesus Molina, and William A Arbaugh. 2004.
Copilot - a Coprocessor-based Kernel Runtime Integrity Monitor. In Proceedings
of the 13th USENIX Security Symposium (Security). San Diego, CA.
[44] Andrew Quinn, Dave Devecsery, Peter M. Chen, and Jason Flinn. 2016. JetStream:
Cluster-scale parallelization of information flow queries. In Proceedings of the
12th USENIX Symposium on Operating Systems Design and Implementation (OSDI).
Savannah, GA.
[45] Shiru Ren, Le Tan, Chunqi Li, Zhen Xiao, and Weijia Song. 2016. Samsara:
Efficient Deterministic Replay in Multiprocessor Environments with Hardware
Virtualization Extensions. In Proceedings of the 2016 USENIX Annual Technical
Conference (ATC). Denver, CO.
[46] Christos Sakalis, Carl Leonardsson, Stefanos Kaxiras, and Alberto Ros. 2016.
Splash-3: A properly synchronized benchmark suite for contemporary research.
In IEEE International Symposium On Performance Analysis of Systems and Software
(ISPASS’16).
[47] Arvind Seshadri, Mark Luk, Ning Qu, and Adrian Perrig. 2007. SecVisor: A Tiny
Hypervisor to Provide Lifetime Kernel Code Integirty for Commodity OSes. In
Proceedings of the 21st ACM Symposium on Operating Systems Principles (SOSP).
Stevenson, WA.
[48] Sudarshan M Srinivasan, Srikanth Kandula, Christopher R Andrews, and
Yuanyuan Zhou. 2004. Flashback: A Lightweight Extension for Rollback and
Deterministic Replay for Software Debugging. In Proceedings of the 2004 USENIX
Annual Technical Conference (ATC). Boston, MA.
[49] Manolis Stamatogiannakis, Paul Groth, and Herbert Bos. 2014. Looking inside
the black-box: capturing data provenance using dynamic instrumentation. In
Proceedings of the 5th International Provenance and Annotation Workshop (IPAW).
Cologne, Germany.
[50] Manolis Stamatogiannakis, Paul Groth, and Herbert Bos. 2015. Decoupling
Provenace Capture and Analysis from Execution. In Proceedings of the 7th USENIX
Workshop on the Theory and Practice on Provenance (TaPP). Edinburgh, Scotland.
[51] G. Edward Suh, Jae W. Lee, and Srinivas Devadas. 2004. Secure Program Execu-
tion via Dynamic Information Flow Tracking. In Proceedings of the 11th ACM
International Conference on Architectural Support for Programming Languages and
Operating Systems (ASPLOS). Boston, MA.
[52] Zhang Xu, Zhenyu Wu, Zhichun Li, Kangkook Jee, Junghwan Rhee, Xusheng
Xiao, Fengyuan Xu, Haining Wang, and Guofei Jiang. 2016. High Fidelity Data
Reduction for Big Data Security Dependency Analyses. In Proceedings of the
23rd ACM Conference on Computer and Communications Security (CCS). Vienna,
Austria.
[53] Mengjia Yan, Yasser Shalabi, and Josep Torrellas. 2016. ReplayConfusion: Detect-
ing Cache-based Covert Channel Attacks Using Record and Replay. In Proceedings
of the 49th Annual IEEE/ACM International Symposium on Microarchitecture (MI-
CRO). Taipei, Taiwan.
[54] Heng Yin, Dawn Song, Manuel Egele, Christopher Kruegel, and Engin Kirda.
2007. Panorama: capturing system-wide information flow for malware detec-
tion and analysis. In Proceedings of the 14th ACM Conference on Computer and
Communications Security (CCS). Alexandria, VA.
[55] Wenchao Zhou, Qiong Fei, Arjun Narayan, Andreas Haeberlen, Boon Thau Loo,
and Micah Sherr. 2011. Secure network provenance. In Proceedings of the 23rd
ACM Symposium on Operating Systems Principles (SOSP). Cascais, Portugal.