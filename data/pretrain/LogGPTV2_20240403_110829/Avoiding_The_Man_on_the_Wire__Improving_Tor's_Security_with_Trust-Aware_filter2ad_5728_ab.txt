Cross-Circuit Attack: Consider a popular website that
causes the user’s browser to fetch resources of a known (rough)
size from a set of servers in other ASes. An AS-avoiding
path-selection algorithm like Astoria may choose paths that
place a malicious AS between the exit and website on the
initial connection and then between the client and guard on
a subsequent connection to fetch a linked resource. If the
adversary can link its observations on those two connections,
which it might by using the timing and amount of trafﬁc
on the circuit, then it can deanonymize the user, effectively
performing a correlation attack across circuits. See the full
version of this paper [19] for details and results on the
effectiveness of this attack. Choosing two different guards for
two different destinations is useful when for both guards some
AS between the client and guard appears between the exits
and destination for one destination but not the other, which
is precisely the situation when some AS is in a position to
perform this cross-circuit attack. This suggests that clients
should always choose guards obliviously to the destination.
III. TRUST MODEL
A. Trust Policies
We use the deﬁnition of network trust given by Jaggard et
al. [16]. A trust belief is a probability distribution that indi-
cates how likely adversaries are to observe trafﬁc at different
network locations. A location is considered to be more trusted
the less likely it is that adversaries are observing it. Although
the belief may be expressed as the adversaries’ success in
compromising any number of relevant factors, such as relay
software or physical location, ultimately it must describe the
probability that the adversaries observe trafﬁc on the virtual
links into and out of the Tor network. A virtual link is an
unordered pair of network hosts, and the entry virtual links
consist of client-guard pairs while the exit virtual links consist
of destination-exit pairs. An adversary is considered to observe
a virtual link if it can observe trafﬁc in at least one direction
between the two hosts. Although the representation of an
arbitrary distribution over all virtual links can be very large,
Jaggard et al. [16] describe how distributions of likely Tor
adversaries can be represented efﬁciently by aggregating host
locations (e.g., at the AS level) and by identifying a small set of
relevant compromise factors and indicating their dependencies
in a Bayesian network.
02004006008001000Number of attack destinations0123456789Avg. posterior entropy (bits)AS 15467AS 25019AS 6128AS 6893AS 8972To indicate how to trade off vulnerability to different
adversaries, each user adopts a trust policy that pairs her trust
belief with a weight for each adversary. Each weight is a
number in [0, 1] that indicates the relative level of concern
that the user has for the associated adversary. In this work, we
assume that distinct adversaries do not collude. If a user were
worried about two adversaries colluding, she could combine
her beliefs about them into those for a single adversary.
Trust policies are quite general and can easily be used with
many kinds of beliefs and sources of trust information. For
example, previous work that considered each relay to have an
independent and individual probability of compromise [20],
[21] can be represented as a trust policy by including a
single adversary and allowing him to compromise each relay
independently with its given probability. As another example,
previous work that considered as a potential threat each AS and
IXP [12], [14], [23], [28] can be represented as a trust policy by
including each AS and IXP as an adversary with equal weight
and allowing each AS and IXP to compromise with probability
1 all virtual links passing through it. Moreover, as described
by Jaggard et al. [16], trust policies can incorporate beliefs
about a variety of other sources of network compromise,
such as software vulnerabilities, physical cable tapping, and
geographic location.
We do not expect that most individual users will craft their
own trust policies. Indeed, doing so is likely best left to experts
unless the user has strong and idiosyncratic beliefs or concerns.
Rather, we envision that knowledgeable specialists, such as
security researchers and professionals, will provide opinions
about vulnerability to speciﬁc kinds of adversaries, and that
institutions, such as governments and consumer advocacy
groups, will incorporate these opinions into trust policies that
are appropriate for their communities. An important special
case of this is that we expect that the Tor Project would select
a default policy that is in the broad interest of all Tor users,
and then would conﬁgure the standard Tor client to use it
as well as provide any necessary supporting data through the
Tor network, much as Tor consensuses (i.e., hourly documents
describing available relays) are distributed today by a set of
directory authorities.
We will consider two speciﬁc trust policies in our analysis
of TAPS: (i) The Man, which models a single powerful global
adversary whose exact location isn’t known with certainty, and
(ii) Countries, which models each country as an adversary
whose locations are known exactly. Either of these policies
constitutes a plausible default policy as well as that of a
particular user community. We now describe these models.
B. The Man
The Man represents a powerful adversary who may create,
compromise, or coerce the diverse entities that make up the
Tor network. Speciﬁcally, we give The Man an independent
probability to observe each Tor relay family, AS organization,
and IXP organization. A relay self-identiﬁes its family in a
descriptor [36] that it uploads to the directory authorities,
and an AS or IXP organization is identiﬁed using public
information as being controlled by the same corporate or legal
entity [9], [22]. Without any basis for differentiation, The Man
compromises each AS and IXP organization independently
with probability 0.1. For relays, we consider that trust may
increase the longer a given relay has been active. This will
not guarantee protection against an adversary that is willing
4
to contribute persistently-high levels of service to the Tor
network. However, it can require adversaries to either make
their own persistent commitments to the network or to have
compromised others who have done so (and are thus most
committed, experienced, and difﬁcult to attack). For The Man,
we therefore assume each family is compromised by the
adversary independently with probability between 0.02 and
0.1, where the probability increases as the family’s longevity
in Tor decreases. We calculate longevity as follows: First, relay
uptimes are calculated as the exponentially-weighted moving
average of the relay’s presence in a consensus with Running,
Fast, and Valid ﬂags with a half-life of 30 days. A relay
family’s uptime is simply the sum of its relays’ uptimes. The
probability that a family is compromised is then taken as
(0.1 − 0.02)/(family uptime + 1) + 0.02.
C. Countries
As an alternative to The Man, the Countries trust policy
includes as an adversary each individual country in the world.
A particular country adversary compromises with probability 1
every AS or IXP that is located in that country and no others.
All country adversaries are given a weight of 1. This policy
illustrates a geographic perspective for Tor security, and it also
demonstrates how we handle multiple adversaries.
IV. SECURITY MODEL AND METRICS
A. Adversary Model
As we have described in our trust model, we are con-
sidering an adversary who may control or observe some Tor
relays and parts of the Internet infrastructure. Note that an
important special case of this is that
the adversary might
observe the destination itself. From these positions, we then
analyze the adversary’s success in deanonymizing users via the
following methods: (i) performing a ﬁrst-last correlation attack,
(ii) identifying the relays used on an observed connection, and
(iii) observing Tor connections over time and linking them as
belonging to the same user.
As described earlier, ﬁrst-last correlation attacks are possi-
ble whenever the adversary is in a position to observe trafﬁc
between the client and entry guard as well as between the
destination and exit. In such a situation, we assume that the
adversary can immediately determine that the observed trafﬁc
is part of the same Tor circuit and thereby link the client with
its destination.
Even when the adversary is not in a position to perform a
ﬁrst-last correlation attack, he still may observe different parts
of the circuit and use trafﬁc correlation to link together those
parts. In such a case, if the observed relays on the circuit are
unusually likely for a particular client to haven chosen (e.g.,
because of atypical trust beliefs), then the adversary may be
able to identify the client even without direct observation. This
is even more of a concern if the adversary applies congestion
or throughput attacks [13], [25] to indirectly identify the relays
on a target circuit. Therefore, we will consider the ability of the
adversary to identify the source and destination of an observed
circuit based on knowledge of its relays.
Finally, it is important to consider multiple connections
over time instead of just one in isolation. Every circuit that
a client creates may give the adversary another opportunity
obtain a sensitive position or may leak more information about
the client. This problem is made worse by the fact that the
adversary may be able to determine when some circuits are
created by the same client. This could happen, for example,
if the adversary repeatedly observes trafﬁc to the destination
and the client interacts with the same online service using a
pseudonym or requests a sequence of hyperlinked documents.
Observe that in both of these examples the linking is done
using similarities in the content of trafﬁc and not via any
weakness in the Tor protocol. Thus we will consider an
adversary who can link observed connections by client.
Note that we are not considering an adversary that can
identify trafﬁc content based only on the timing and volume
of data, that is, an adversary that can perform website ﬁnger-
printing [37]. Also, in reality we can expect adaptive adver-
saries who continually learn and shift the allocations of their
resources, but we only analyze static adversaries in this paper.
However, adaptiveness can be captured to a certain extent
already by deﬁning trust policies with respect to adversary
behavior over time. That is, the compromise probability for a
relay or virtual link can represent the probability that it will at
some point during a given period be observed by the adversary.
B. Anonymity Metrics
We evaluate anonymity using two kinds of metrics. The
ﬁrst kind will give empirical estimates of the speed and
frequency of ﬁrst-last correlation attacks. The second kind
will provide worst-case estimates for the adversary’s ability
to identify the source or destination of streams that are only
partially observed.
First-last correlation attacks are relatively simple and result
in complete deanonymization, and therefore we are interested
in accurate estimates for how likely they are to occur. More-
over, their success depends on the behavior of the user and of
the circuit-creation algorithm, and therefore we can measure it
empirically via simulation. Following Johnson et al. [22], we
use the following as metrics: (i) The probability distribution
of time until a client is deanonymized via a correlation attack;
and (ii) The probability distribution of the fraction of streams
that are deanonymized via a correlation attack.
Measuring the anonymity of multiple connections that
are only partially observed is more difﬁcult because it isn’t
clear how successful the adversary can be at both linking
separate streams and indirectly identifying relays on a circuit.
Therefore, we take a worst-case approach and consider the
adversary’s ability to guess the source (resp. destination) of a
sequence of streams that have been linked as coming from the
same client (resp. going to the same destination) and for which
the circuit relays have been identiﬁed. We measure this ability
as the posterior distribution over network locations. Note that
we do not take into account the fact that the adversary knows
that streams for which the client or destination is unknown
can only travel over virtual links that the adversary does not
observe. Ruling out certain virtual links is more challenging
than just positively identifying trafﬁc on a virtual link because
it requires the false negative rate for trafﬁc correlation to be
extremely low (in addition to the existing requirement that the
false positive be extremely low). Thus, we leave this extension
to our analysis to future work.
V. TRUST-AWARE PATH SELECTION
A. Overview
We describe two variants of the Trust-Aware Path Selection
algorithms (TAPS): (i) TrustAll, which is intended for system-
wide deployment, and (ii) TrustOne, which works with Tor’s
existing path-selection algorithm and is intended for use by a
minority of users. Two TAPS variants are needed to blend in
with two different types of other users: those who do use trust
in path selection and those who do not. The main approach
of both algorithms is to choose guards and exits to avoid
ﬁrst-last correlation attacks while also blending in with other
users. Parts of this approach are shared with some previously-
proposed path-selection security improvements [6], [12], [21].
However, TAPS includes several novel features that improve
the security and performance issues of these proposals. We
highlight
those features before proceeding to describe the
algorithms in detail.
First, TAPS uses an API that encapsulates ﬂexible trust
policies. These trust policies support adversaries from a very
general class of probabilistic models. As previously described,
this class can represent features such as uncertainty, multiple
adversaries, and adversarial control of diverse network ele-
ments.
Second, TAPS clusters client locations and (separately)
destination locations. Each location cluster has a represen-
tative, and all locations in the cluster are treated as if they
were the representative. The main purpose of this clustering
is to prevent leakage of location information over multiple
connections that would occur if paths were selected differently
for each pair of client and destination location. Treating all
members of the cluster as if they were the representative
maintains anonymity within the cluster. A secondary beneﬁt is
reducing the amount of information needed to represent trust
policies by reducing the number of paths to and from guards
and exits that need be considered.
Third, TAPS treats its set of entry guards collectively, that
is, in a way that provides security when multiple circuits, po-
tentially using different guards, are considered. TAPS chooses
each additional guard in a way that minimizes the additional
exposure of its entry paths. Moreover, once a set of entry
guards is chosen, TAPS doesn’t prefer one guard in the set
over another when creating a connection to a given destination.
This prevents the cross-circuit attack discussed in Sec. II-A.
It also makes TAPS compatible with the Tor’s current default
conﬁguration of one guard, as it does not depend on being able
to choose the best guard among several for a given destination.
Fourth, TAPS provides a conﬁgurable tradeoff between
security and performance by parameterizing how much relay-
selection deviates from ideal load-balancing. The Tor network
is under heavy load relative to its capacity [4], and latency
is dominated by queuing delay at the relays [17]. Thus good
load balancing is essential to maintaining Tor’s performance,
which is itself a crucial factor in Tor’s success.
B. Trust API
The TAPS algorithms work with the trust policies described
in Sec. III via an Application Programming Interface (API).
Jaggard et al. [16] describe how to represent such policies with
a Bayesian network. However, such a representation may not
be the most efﬁcient for the computations needed during path
selection. Therefore, we abstract those computations into an
API, and we describe how they can be efﬁciently implemented
for The Man and Countries policies in Sec. V-E. We assume
that the API is implemented by the creator of the trust policy.
Several API functions take as an argument a network
location. There are several possible levels of granularity at
which a network location may be deﬁned in TAPS, such as the
Autonomous-System level or the BGP-preﬁx level. Using more
ﬁne-grained locations will result in more accurate predictions
5
about the adversaries’ locations and thus improve security, but
it will also result in increased runtime for the TAPS algorithms
(and likely for the API functions as well).
We assume that API users can provide a locations data
structure that (i) allows the locations to be enumerated, (ii)
includes each location’s popularity rank for Tor clients, (iii)
allows IP addresses to be mapped to locations, and (iv)
includes size of each location (e.g., the number of IP addresses
originated by an Autonomous System).
We also assume that API users can provide a relays data
structure that (i) allows relays to be enumerated by unique
identity keys (e.g., as represented by ﬁngerprints [36]), (ii)
includes the data in each relay’s consensus entry (e.g., the
status ﬂags, weight, and IP address), and (iii) includes the data
in each relay’s descriptor (e.g., the exit policy).
The trust API functions are as follows:
1) LOCATIONDISTANCE(loc1,
loc2, relays, weights): This
function returns an abstract distance between two locations
that measures the dissimilarity of the adversaries that
appear on the network paths between the locations and
the relays. This distance is the expected sum over relays
weighted by weights of the total weight of adversaries that
appear on one of the virtual links between the relays and
loc1 and loc2 but not the other. This function turns the set
of locations into a metric space.
2) GUARDSECURITY(client loc, guards): This function re-
turns a security score for the use of the given guards as
entry guards by a client in location client loc. The score
must be in [0, 1], and it should represent the expected total
weight of adversaries not present on the paths between
client loc and guards, normalized by the sum of all adver-
sary weights. Thus a higher score indicates higher security.
3) EXITSECURITY(client loc, dst loc, guard, exit): This
function returns a security score for the use of guard