p
1
0
R1 and R2
0
15
20
35
c
p
1
0
R1 and R2 and R3
0
10
15
20
25
30
35
45
c
Figure 4.2 Calculation of the Joint Density Function Without Rounding
4.1.2.2 Hierarchical Approach
A more efﬁcient approach, which allows implicit caching of these multiplications,
is to divide the problem into smaller sub-problems. Like shown in ﬁgure 4.2, it is
possible to calculate joint probability density functions by successively joining two
probability density functions. The hierarchical approach which we propose starts
with simple distributions for each risk, like shown in ﬁgure 4.2 at the left side: No
costs occur with probability (1− p) and costs c with probability p. The process of
calculation the overall probability density function PDFL consists of the following
steps:
1. Calculate separate PDFLs for each individual risk r
2. Iteratively combine all separate PDFLs to the ﬁnal PDFL
The approach computes joint distributions until only one distribution, the ﬁnal
density function, remains. For R risks, (R− 1) joins need to be calculated. We use
a priority queue for storing all distributions sorted by the number of cost values on
the x-axis. The approach always combines the two smallest distributions in order
to keep the number of multiplications to a minimum and to create the smallest
possible distributions for the next steps in the hierarchy. If the distributions were
simply joined successively, the hierarchies would not be as shallow and balanced,
which would lead to a higher number of multiplications. Exemplary hierarchies
for scenarios with 9 and 16 risks are shown in ﬁgure 4.3.
Additionally, the partitioning of the calculation is more ﬂexible compared to
previously proposed models, because it is possible to start with distributions that
contain more than two values on the x-axis in the ﬁrst step. This allows incor-
94
4 Risk Quantiﬁcation Framework
512
16
32
8
4
4
4
4
2
2
2
R1 R2 R3 R4 R5 R6 R7 R8 R9
2
2
2
2
2
2
65,536
256
256
16
16
16
16
4
4
4
4
4
4
4
4
2
2
2
R1 R2 R3 R4 R5 R6 R7 R8 R9 R10 R11 R12 R13 R14 R15 R16
2
2
2
2
2
2
2
2
2
2
2
2
2
Figure 4.3 Tournament Complexity
poration of risks whose losses depend on the affected services or data transfers
risks (see the model’s extension with dependent losses in section 4.1.1.4), as these
special kind of risks lead to initial distributions with multiple cost values on the
x-axis.
As it is complicated to estimate the number of multiplications for this kind of
hierarchy, we calculate an upper bound by assuming the worst case where all dis-
tributions are successively joined regardless of their size. The calculation would
look like follows: ((R1 and R2) and R3) and R4 ... which leads to high and unbal-
anced hierarchies. The number of multiplications can then be recursively deﬁned
as g(R) =g(R − 1) +2 · R or iteratively calculated like shown in equation (4.4):
(cid:4)
(cid:5)
R∑
i=2
2i =
R∑
i=0
2i
(cid:2)
(cid:3)
− 3 =
2(R+1) − 1
= 2(R+1) − 4
(4.4)
Compared to the number of multiplications of the power set-based algorithm,
2R · (R− 1), even in the worst case, the hierarchical approach needs fewer multi-
plications (for R ≥ 2). If the approach always joins the two smallest (i. e., with the
fewest number of cost values on the x-axis, such as shown in ﬁgure 4.3) distribu-
tions, the differences will be even greater.
4.1 Model Description
95
An alternative approach for faster calculation of joint density functions is shown
by Sang et al. (1992). They compute the joint density function of a set of discrete
independent random variables by generating the resulting distribution in order of
decreasing probability. This allows the algorithm to stop the calculation as soon as
a given accuracy has been obtained.
4.1.3 Determination of Risk Measures
the PDFLs,
Based on the calculated discrete probability density functions,
described in sections 4.1.2.1 and 4.1.2.2, it is possible to derive risk measures.
These characteristics of the risk distribution can be incorporated into individual
utility-functions and, thus, be used to evaluate scenarios. For example, a risk
neutral decision maker solely tries to minimize the mean value μ of the potential
losses. In this section, we describe how to calculate three basic risk measures,
i. e., the average μ (also mean or expected value), the variance σ 2, and the
Value-at-Risk. For other risk measures, such as the expected shortfall, based
on loss distributions see McNeil et al. (2005, pp. 35–48). A basic algorithm for
calculating the average is given in listing 4.2.
PDFL
1 input
2 output The calculated average μ of the potential losses
3 ret ← 0.0
4 for each k ∈ PDFL.getCostValues()
5
6 end for
7 return ret
ret ← ret + (PDFL[k] · k)
Listing 4.2 Calculation of the Average Potential Losses

The algorithm simply iterates (line 4) over all cost values on the distribution’s
x-axis and builds the sum of all weighted averages by multiplying each cost value
with their occurrence probability (line 5). Finally, listing 4.2 returns the expected
value for the given discrete costs’ probability density function PDFL.
96
4 Risk Quantiﬁcation Framework
PDFL
1 input
2 output The calculated variance σ 2 of the potential losses
3 xs ← PDFL.getCostValues()
4 iterator = xs.iterator()
5 m ← iterator.next()
6 sumw ← PDFL[m]
7 t ← 0.0
8 while (iterator.hasNext())
9
10
11
12
13
14
15
16
17 wend
18 return t
xi ← iterator.next()
wi ← PDFL[xi]
q ← xi - m
temp ← sumw + wi
r ← q · wi / temp
m ← m + r
t ← t + r·
sumw ← temp
sumw · q
Listing 4.3 Calculation of the Variance of the Potential Losses

Listing 4.3 uses an advanced version for calculating the distribution’s variance
(i. e., the square of the standard deviation σ) for the given discrete costs’ probabil-
ity density function PDFL. Normally, the variance is calculated by ﬁrst calculating
the average μ and then calculating the squared deviation of the distribution from
its expected value in a second step. This process would require two iterations over
all cost values on the x-axis of the PDFL, and, thus, consume more time.
Therefore, an advanced process for calculating the variance σ 2 in one iteration
instead of two is used. The calculation is based on the “WV2 Proposed Algo-
rithm for Weighted Variance” by West (1979) in one pass. The algorithm stores all
weights as occurrence probabilities in the variables called wi, while the xi store the
cost-related values. The mean, which is continuously updated, is stored in variable
m.
After calling the function, the standard deviation σ can be derived by taking
the square root of the calculated variance σ 2.
The Value-at-Risk is deﬁned as the lowest number l, so that the probability that
losses L greater than l occur, is exceeded by (1− α) (Dufﬁe and Pan, 1997). A
mathematical deﬁnition is given by McNeil et al. (2005, p. 38):
VaRα := inf{l ∈ R : P(L > l) ≤ 1− α}
:= inf{l ∈ R : FL(l) ≥ α}
(4.5)
(4.6)
4.1 Model Description
97
Conﬁdence level α ∈ {r ∈ R | 0 < r ≤ 1} ; PDFL
1 input
2 output The calculated Value-at-Risk
3 maxProbabilityOfError ← 1.0 − α
4 currentProbabilityOfError ← 0.0
5 sortedCosts ← PDFL.getSortedCostValues()
6 backwardsIterator ← sortedCosts.descendingIterator()
7 lastCosts ← sortedCosts.getSortedCostValues().last()
8 while (
9
10
11
12
13
14
15 wend
16 return lastCosts
currentCosts ← backwardsIterator.next()
currentProbabilityOfError ← currentProbabilityOfError +
lastCosts ← currentCosts
backwardsIterator.hasNext() &&
currentProbabilityOfError \leq maxProbabilityOfError)
PDFL[currentCosts]
Listing 4.4 Calculation of the Value-at-Risk

The characteristic represents a threshold value which speciﬁes the maximum
amount of losses that will occur with a given conﬁdence level α. This means that
all losses greater than this threshold are less likely than (1− α). Usually, the Value-
at-Risk is a statistical measure of the risk associated with an investment or set of
investments, based on extreme value theory. It quantiﬁes the stochastic behavior at
unusually large (or small) levels and is, thus, concerned with occurrence probabil-
ities and statistical questions related to those extremely rare events (Wang et al.,
2008, p. 106).
Listing 4.4 returns the Value-at-Risk for the given discrete costs’ probability
density function (PDFL) and the given conﬁdence level α. For performance rea-
sons, the calculation is done from the highest to the lowest cost values on the dis-
tribution’s x-axis. Therefore, the algorithm is more efﬁcient for higher conﬁdence
levels (i. e., greater 0.5).
The algorithm starts with the highest possible amount of losses that can occur
and then iterates back to the lowest possible amount on the distribution’s x-axis.
The iteration (lines 8 to 15) stops, when the given conﬁdence level is reached, i. e.,
when the sum of all visited probabilities is greater than (1− α). This means that all
the following cost-related values (to the left of the reached threshold) occur with a
probability ≥ α.
98
4.2 Simulations
4 Risk Quantiﬁcation Framework
The following sections present various simulation-based results regarding the pro-
posed model. First, we show how cost drivers in a given scenario can be identiﬁed.
Second, we analyze the effect of the input parameters using a sensitivity analysis.
The third section presents and further analyzes the trade-off between the expendi-
ture for the elicitation of input data and the accuracy of the obtained results.
4.2.1 Identiﬁcation of Costs Drivers
Based on the model parameters, the contribution to the aggregated distribution of
potential losses can be identiﬁed for each risk separately. It is possible to create R
new scenarios based on the given complete scenario with R risks, where each new
scenario consists of only one risk. Because this risk is either related to the services
or the data transfers, each new scenario only needs to contain either the services
or data transfers of the complete scenario. This corresponds to the potential losses
of a single row in the table containing all scenario parameters. The distribution of
potential losses can then be calculated like it is described in section 4.1.2.2.
The result itself is, again, a discrete probability density function which maps
cost values to their occurrence probabilities and which can be analyzed and as-
sessed using the same methods as the overall distribution of potential losses.
If appropriate risk measures are used for the assessment, it is possible to show
how the aggregated risk is concentrated in the individual risks, as a fraction of
the overall risk. The following equations show that the μ-σ-characteristic of the
overall distribution (X +Y ) equals the sum of individual μ-σ-characteristics of the
individual distributions X and Y :
a· E(X +Y ) +b ·Var(X +Y )
(4.7)
=a· [E(X) +E (Y )] + b· [Var(X) +Var(Y )]
(4.8)
=a· E(X) +a · E(Y ) +b ·Var(X) +b ·Var(Y )
(4.9)
=a· E(X) +b ·Var(X) + a· E(Y ) + b·Var(Y )
(4.10)
This means that it is possible to calculate the μ-σ-characteristic for all risks
individually and then calculate the overall μ-σ-characteristic by just building the
sum. This can be done much faster than calculating the overall probability density
function and then calculating the μ-σ-characteristic. Therefore, if a decision maker
is only interested in μ and σ of the overall distribution, it is possible to calculate
4.2 Simulations
p
1
A=B
0
1
3
VaR90
p
1
C
0
1
2
p
1
0
p
1
0
c
c
99
Remove A or B:
A and C (= B and C)
2
3
4
c
5
VaR90
Remove C:
A and B
VaR90
Figure 4.4 Example for the Violated Additivity of the Value-at-Risk
VaR90
2
4
c
6
these two characteristics for scenarios consisting of millions of risks, services, and
data transfers, because the expensive calculation of the overall probability density
function, with its exponential runtime complexity, can be omitted.
This additivity of the μ-σ-characteristics allows identifying the individual risk
that introduces the largest fraction of the overall risk. If this risk cost driver is elim-
inated (e. g., by implementing countermeasures that lower the occurrence proba-
bility to zero) the overall μ-σ-characteristic will decrease by the risk’s individual
characteristic.
This, however, cannot be done, if the Value-at-Risk is used as risk measure, as
the quantile-based Value-at-Risk is known to violate the property of additivity in
general (Daníelsson et al., 2005). This means that it is not possible to add up the
individual Value-at-Risk of all risks in order to calculate the Value-at-Risk of the
overall scenario. The sum does not even have to be smaller than the Value-at-Risk
of the overall scenario, which means that the Value-at-Risk is not even subadditive
(McNeil et al., 2005, p. 40).
100
4 Risk Quantiﬁcation Framework
Calculate PDFLs for individual risks
Calculate overall PDFL
by combining individual
PDFLs
Derive risk characteristics
for overall PDFL
Derive risk characteristics
for individual PDFLs
Figure 4.5 Process of Deriving Risk Characteristics
Therefore, it is not possible to use the Value-at-Risk in order to identify the
risk whose removal would lead to the largest reduction of the overall Value-at-
Risk, like the following small counterexample, consisting of three risks, shows
(see ﬁgure 4.4).
Risk A and risk B are identical: potential losses of 1 occur with a probability
of 80% and costs of 3 with probability 20%. Both risks have a Value-at-Risk
(α = 0.9) of 3. Risk C leads to losses of 1 and 2 with chances 20% and 80%
(Value-at-Risk: 2). If risks A or B are removed, the resulting overall distribution
has a Value-at-Risk of 5, while removal of risk C (with its smaller individual
Value-at-Risk) leads to a larger reduction and a resulting Value-at-Risk of 4.
Similar to identiﬁcation of cost drivers at the risk level, it is possible to identify
a single service’s or data transfer’s contribution to the overall probability density
function. This corresponds to the potential losses of a single column in the table
containing all scenario parameters. See tables 4.6 and 4.7 for an example of two
of these parameter tables. Using these “vertical slices” of the parameter tables
allows decision makers to identify the critical and most risky components of a
given scenario. Thereby, it is possible to detect services that could be replaced by
more secure, alternative services offering the same functionality. Similarly, these
“vertical slices” allow to check for data transfers which could be further secured,
e. g., by additional encryption mechanisms.
Finally, the described approach can be applied to single cells in the tables that
contain all scenario parameters. For each combination of services/data transfers
and risks, the result will always be a relatively simple distribution with just two
discrete cost values on the x-axis: No costs occur with probability (1− pi j) and
costs ci j + ci with probability pi j.
This way, for example, it is possible to compare the potential losses of a speciﬁc
risk, such as eavesdropping information, in a speciﬁc data transfer, to the potential
losses due to breakdown of a certain service.
4.2 Simulations
101
An application of these techniques can be seen when the proposed risk quan-
tiﬁcation framework is applied to a real life scenario in section 4.3.1.4. Figure 4.5
illustrates the steps of the calculation process and shows that derivation of risk
characteristics for individual risks does not involve the computationally expensive
step of calculating the overall PDFL by combining all individual PDFLs.
4.2.2 Sensitivity Analysis
In order to obtain an initial estimate of how the model’s parameters affect the cal-
culated probability density functions, we ﬁx the values of all parameters except
one and make simulation runs for varying levels of the “free” parameter (Law and
Kelton, 2000). Using this approach, it is possible to see how the derived character-
istics, μ, σ, and the Value-at-Risk, respond to changes in a single parameter.
but from [0; 2·(cid:6)
1− |Kx|(cid:7)
rk were ﬁxed, e. g., to 0.5, the calculated ¯px
An important factor that inﬂuences the distribution of potential losses is the