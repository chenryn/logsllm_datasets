The gadget detection is to check whether the speculative
instruction trace conforms to the gadget patterns described
above. We deploy the gadget checker in simulated speculative
execution. Although both our gadget patterns involve memory
access operations, it is inefﬁcient and unnecessary to check
all the memory access instructions on SE paths. Instead, we
only check memory access instructions which are tainted. To
this end, we instrument memory read and write operations.
To detect BCB gadgets, for each memory read, we check
whether its source is tainted. To make sure the dependency
dep(j , [i ]) rule is satisﬁed between two instructions, we also
track whether the tainted operand of one instruction j is
propagated from instruction i. For BCBS gadgets, the pattern
captures any tainted memory write whose destination address
is marked as tainted within the SEW.
V.
IMPLEMENTATION
We have implemented the prototype SpecTaint in C. More
speciﬁcally, we wrote a C plugin of 1 KLOC in C code
on top of DECAF [25] (a dynamic binary analysis platform
built on top of QEMU 1.0). This plugin implements the
state checkpoint management and Spectre gadget detection
components. We reused the dynamic taint analysis plugin of
DECAF for our taint analysis. Overall, the changes to develop
our prototype do not exceed 2 KLOC. Besides, to increase the
code coverage, we use AFL 2.52b [2] and honggfuzz [8] to
generate seed inputs.
VI. EXPERIMENTAL EVALUATION
In this section, we evaluate SpecTaint to answer the
following research questions:
1) How effective is SpecTaint to ﬁnd Spectre gadgets com-
pared with other existing tools?
2) How efﬁcient is SpecTaint to ﬁnd Spectre gadgets in
real-world applications?
This section is composed as follows: First, we brieﬂy
describe the experiment setup, the datasets, and the evaluation
metrics used in our experiments (Sections VI-A). Second, we
evaluate the efﬁcacy of SpecTaint (Sections VI-B). Then, we
evaluate the efﬁciency of SpecTaint on real-world applications
(Sections VI-E). Finally, we conduct case studies of some
Spectre gadgets detected by SpecTaint in real-world applica-
tions and the deep learning framework Caffe (Sections VI-F).
7
A. Experiment Setup
Baseline Methods. We compare SpecTaint with three base-
line approaches: Spectre 1 Scanner from Red Hat (RH Scan-
ner) [5], oo7 [43] and SpecFuzz [39]. RH Scanner is a static
analysis tool that can be used to scan for Spectre gadgets.
Oo7 [43] is another static analysis tool that utilizes static taint
analysis to ﬁnd Spectre gadgets. SpecFuzz [39] extends the
fuzzing technique to detect errors in speculative execution and
report Spectre gadgets. Another related work, SPECTECTOR,
leverages symbolic execution to detect information-ﬂow differ-
ences introduced by speculative execution. Since the manual
settings to make it work on large programs are not open-
source, it is hard to evaluate it on our real-world benchmarks.
As discussed in SpecFuzz [39],
the authors failed to run
SPECTECTOR on the real-world benchmarks due to a large
number of unsupported instructions. Therefore, we did not
compare with SPECTECTOR on real-world benchmarks (same
with SpecFuzz’s).
Evaluation Dataset. The evaluation is conducted on two
datasets, the Spectre Samples Dataset and Real-world Dataset.
Aligned with other baseline works, we utilize the same Spectre
Samples Dataset to demonstrate the detection capability of
SpecTaint. For baseline comparison with related works, we
collected six real-world applications and created two real-
world datasets.
• Spectre Samples Dataset. This dataset is designed to
demonstrate the efﬁcacy of SpecTaint. We collected
15 Spectre V1 samples created by Paul Kocher [3] and
compiled 15 samples with the same conﬁguration (gcc-
4.8.4 with O0) [39].
• Real-world V1 Dataset. For fair baseline comparison,
we use the same dataset as SpecFuzz’s [39]. This dataset
contains six widely used applications: one cryptographic
program from OpenSSL [12], a compression program
(Brotli [7]), and four parsing programs (JSON [10],
LibHTP [11], HTTP [9] and YAML [6]).
• Real-world V2 Dataset. Systematic baseline evaluation
is difﬁcult due to the shortage of ground truth in real-
world programs. To solve this problem, we injected
known Spectre gadgets into programs from Real-world
V1 Dataset and created the Real-world V2 Dataset. We
adopt the same injection approach proposed in LAVA [20]
to build this dataset. More speciﬁcally, we utilize dynamic
taint analysis to ﬁnd attack points which can be controlled
by input bytes that do not determine control ﬂow and
have not been modiﬁed much (see [20] for more details).
Then we inject the Spectre gadgets from Spectre Sample
Dataset
into target programs and add code to make
injected gadgets controllable via input bytes. As a result,
we injected 15 Spectre V1 gadgets from Spectre Sample
Dataset into 52 different locations in six programs. To
make a fair comparison, the input seeds used in the evalu-
ation are all generated by a fuzzing tool [8]. Therefore, we
have no clue whether the injected locations are covered
by input seeds in the evaluation. It is worth mentioning
that this dataset is used to evaluate the detection coverage
of SpecTaint and related works, instead of path/code
coverage, and we choose the same seed corpus with
SpecFuzz’s to guarantee a fair comparison.
Evaluation Metrics. For the Real-world V1 dataset, which
does not have ground truth, we manually verify the detection
results and calculate the precision rate to quantify the perfor-
mance of SpecTaint and baseline approaches. The precision
is calculated as precision(P ) = T P
T P +F P , where T P is the
number of detected gadgets that are manually veriﬁed to be
exploitable, and F P is the number of detected gadgets that
are not exploitable based on manual analysis. For Real-world
V2 Dataset, we only consider injected Spectre gadgets to be
true positives and other reported results to be false positives.
Then we calculate the precision and recall to quantify the
effectiveness of the proposed approach and baseline methods.
The recall is calculated as recall(R) = T P
T P +F N , where T P is
the number of inserted gadgets which are correctly detected,
and F N is the number of inserted gadgets that are missed. The
precision is calculated as precision(P ) = T P
T P +F P , where F P
is the number of detected gadgets other than injected ones. To
measure the efﬁciency, we reported runtime per speculative
execution and number of paths explored per speculative exe-
cution along with other statistics (see Section VI-E for more
details).
Conﬁguration. The experiments were conducted on a desktop
with 16 GB memory, Intel Core i7 12 cores at 3.70 GHz
CPU, and running Linux 4.15. The Guest OS in QEMU is
Ubuntu 14.04 with 1 GB memory. The speculative window is
dependent on the space limit, i.e., ROB size and timing limits.
We follow the conﬁguration used by SpecFuzz [39] and also
set the speculative window size to 250.
B. Baseline Evaluation on Spectre Samples Dataset
In this experiment, we compared SpecTaint with three
baseline tools on the Spectre Sample Dataset. As presented in
Table II, SpecTaint successfully detected all Spectre gadgets
in the Spectre Samples Dataset, while RH Scanner relies on
syntax-based pattern matching and missed three cases.
C. Baseline Comparison on Real-world V2 Dataset
We conducted the baseline comparison with three related
works, RH Scanner, oo7, and SpecFuzz, on Real-world V2
Dataset. In this experiment, we focused on detecting the
inserted gadgets, therefore we only consider inserted gadgets
to be true positives and all other detection results to be
false positives. For tools that utilize taint tracking to detect
Spectre gadget (oo7 and SpecTaint), we mark input bytes as
taint sources. Since the analysis of oo7 is very slow when
performing whole input bytes tainting, we only mark input
bytes that inﬂuence injected gadgets as taint sources. We adopt
the same conﬁguration for SpecTaint. To compare with RH
Scanner and SpecFuzz, we have another conﬁguration for
SpecTaint, where we mark all input bytes as tainted. The
results for the former conﬁguration are labeled with “*” in
Table I. For dynamic analysis tools (SpecTaint and SpecFuzz),
we used an external fuzzing tool [8] to fuzz the six programs
for 10 hours and fed the generated seeds as inputs to run the
programs.
. . .
i f
s i z e ){
I n s e r t e d S p e c t r e Gadget
( p a r s e r−>t o k n e x t >= num tokens ) {
/ /
1
2
3
4 # i f d e f SPECTRE VARIANT
5
6
7
8 # e n d i f
. . .
9
Listing 2: Missed Spectre gadget 1 by SpecFuzz.
tmp &= a r r a y 2 [ a r r a y 1 [ g l o b a l
i f ( g l o b a l
}
i d x ] ∗ 5 1 2 ] ;
i d x e l e m e n t s [ l−>f i r s t + i d x ] ;
( l−>f i r s t + i d x max size ) {
r e t u r n ( v o i d ∗)
1
2
3
4
5
6 # i f d e f SPECTRE VARIANT
7
8
9
10
11
12 # e n d i f
. . .
13
Listing 3: Missed Spectre gadget 2 by SpecFuzz.
i n t
temp = 0 ;
i n t ∗ a d d r = &g l o b a l
(∗ a d d r < a r r a y 1
i f
temp &= a r r a y 2 [ a r r a y 1 [∗ a d d r ] ∗ 5 1 2 ] ;
}
i d x ;
s i z e ) {
Table I shows that, when tainting gadget-related input
bytes, SpecTaint has no false positives and achieves a pre-
cision rate of 100%. Under the same conﬁguration, however,
oo7 still produced false positives. For instance, it reported
13 gadget candidates in LibHTP, but 12 of them are false
positives. The results show that static taint analysis suffers
from the over-tainting issue, thereby is hard to achieve high
precision in detecting Spectre gadgets. As presented in Table I,
oo7 has many false negatives. For example,
it missed all
inserted gadgets in YAML and Brotli. We examined these false
negatives and found the reasons are as follows. Firstly, the
detection results of oo7 [43] depend on the completeness of the
control-ﬂow graph (CFG) extraction. Some inserted gadgets
are missed since it failed to extract a complete CFG due
to the limitation of the static approach. Also, oo7 is limited
by static inter-procedure taint tracking, and it missed many
inserted gadgets because it failed to propagate the taint source
to the injection points in the target programs. The evaluation
results substantiate our claim that dynamic taint analysis is
much more accurate and effective than static taint analysis in
detecting Spectre gadgets.
As presented in Table I, SpecTaint outperforms RH
Scanner and SpecFuzz under the whole input bytes tainting
conﬁguration. Since we only consider injected gadgets to be
true positives in this dataset, other detection results are labeled
as false positives. The analysis results of other gadgets reported
by SpecTaint are presented in Table IV. Note that Spec-
Taint missed some injected gadgets in this experiment. We
further investigated the results and found that missed gadgets
are not covered by input seeds. However, SpecFuzz missed
many injected gadgets that are covered by input seeds and
detected by SpecTaint. According to our analysis, the reasons
are ﬁrst, SpecFuzz adopts a prioritized simulation of branch
mispredictions; it selectively chooses whether to simulate the
misprediction or not on a conditional branch. Therefore, it
missed some injected gadgets. For example, as presented in
Listing 2, the seeds are able to reach the branch at line 2, but
SpecFuzz did not simulate branch misprediction over these
8
oo7
SpecTaint
TP
1
2
3
4
2
3
RH Scanner
FN
2
11
6
3
8
7
Precision
0.002
0.003
0.023
0.016
0.526
0.029
GT
3
13
9
7
10
10
FP
448
811
128
254
36
100
Program
JSMN
Brotli
HTTP
LibHTP
YAML
SSL
TABLE I: Evaluation Results on Real-world V2 dataset (GT: ground truth; TP: true positive; FP: false positive; FN: false
negative). * means we only mark input bytes that inﬂuence injected gadgets as taint sources.
Precision*
1.00
N/A
0.5
0.077
N/A
0.467
Precision*
1.00
1.00
1.00
1.00
1.00
1.00
Precision
0.750
0.141
0.574
0.333
0.700
0.385
Recall
1.00
0
0.11
0.14
0
0.70
Recall
0.33
0.15
0.33
0.57
0.20
0.30
Recall
1.00
0.92
0.89
1.00
0.70
1.00
Recall
0.67
0.54
0.89
0.71
0.40
0.60
FP*
0
0
1
12