different on those devices.
To the best of our knowledge, such a variety of considered
scenarios have never been investigated before.
6
measures linear dependence between two variables, x
and y, in the range [−1, 1], where 1 is the total positive
linear correlation, 0 is no linear correlation, and −1 is
the total negative linear correlation. Pearson correlation
for a sample of the entire population is deﬁned by [32]:
(cid:80)N
(cid:113)(cid:80)N
i=1((xi − ¯x)(yi − ¯y))
i=1(xi − ¯x)2
(cid:113)(cid:80)N
.
P earson(x, y) =
i=1(yi − ¯y)2
(2)
We note that the Pearson correlation is a standard way
for feature selection in the proﬁled SCA, see, e.g., [33],
[34]. Additionally, Picek et al. show that while it is not the
best feature selection in all scenarios, Pearson correlation
behaves well over a number of different proﬁled SCA
settings [35]. In this setting, we use NB, RF, and MLP.
• In the second setting, we consider the full set of features
(i.e., all 600 features) and we conduct experiments with
MLP and CNN. Note that MLP is used in both scenarios
since it can work with a large number of features but also
does not need the features in the raw form (like CNN).
the experiments with 50 features, we use scikit-
learn [36], while for the experiments with all features, we use
Keras [37]. For NB, RF, and MLP when using 50 features, we
use k-fold cross-validation with k = 5. For experiments when
using all features, we use three datasets: train, validate, and
test. Training set sizes are 10 000 and 40 000, validation set
size equals 3 000, and test set size equals 10 000. Since there
is in total 50 000 measurements with 10 000 measurements
used for testing, when using the validation set, then the largest
training set size is not 40 000 but 37 000.
For
a) Naive Bayes: NB has no parameters to tune.
b) Random Forest: For RF, we experimented with the
number of trees in the range [10, 100, 200, 300, 400]. Based on
the results, we use 400 trees with no limit to the tree depth.
c) Multilayer Perceptron: When considering scenarios
with 50 features, we investigate [relu, tanh] activation func-
tions and the following number of hidden layers [1, 2, 3, 4, 5]
and a number of neurons [10, 20, 25, 30, 40, 50].
Based on our tuning phase, we selected (50, 25, 50) ar-
chitecture with relu activation function (recall, ReLU is of
the form max(0, x)). We use the adam optimizer, the initial
learning rate of 0.001, log − loss function, and a batch size
of 200.
When considering all features and MLP, we investigate
the following number of hidden layers [1, 2, 3, 4] and number
of neurons [100, 200, 300, 400, 500, 600, 700, 800, 900, 1 000].
Based on the results, we select to use four hidden layers where
each layer has 500 neurons. We set the batch size to 256, the
number of epochs to 50, the loss function is categorical cross-
entropy, and optimizer is RM Sprop with a learning rate of
0.001.
d) Convolutional Neural Network: For CNN, we con-
sider architectures of up to ﬁve convolutional blocks and two
fully-connected layers. Each block consists of a convolutional
layer with relu activation function and average pooling layer.
The ﬁrst convolutional layer has a ﬁlter size of 64 and then
(a) All devices.
(b) B4 K1.
Fig. 5: NICV comparison.
E. Hyper-parameter Tuning
In our experiments, we consider the following machine
learning techniques: NB, RF, MLP, and CNN. We select
these four techniques as they are well-investigated in the SCA
community and known to give good results [31], [12], [23],
[19], [13]. NB is an often considered technique in SCA as it is
very simple and has no hyper-parameters to tune. Additionally,
it shows good performance when the data is limited [31]. RF
shows very good performance in SCA and is often considered
as the best-performing algorithm (when not considering deep
learning) [16], [18]. From deep learning techniques, CNN
performs well when there is random delay countermeasure due
to its spatial invariance property [13], [23]. Finally, an MLP
works well for masking countermeasures as it combines all
features and produces the effect of a higher-order attack [13],
[19].
We also distinguish between two settings for these tech-
niques:
• In the ﬁrst setting, we select 50 most important features to
run the experiments. To select those features, we use the
Pearson correlation. The Pearson correlation coefﬁcient
7
280290300310320330340350360Points in Time0.00.10.20.30.40.50.6NICVB1_K1B2_K1B2_K2B3_K1B4_K1B4_K1B4_K3280290300310320330340350360Points in Time0.00.10.20.30.40.50.6NICVB4_K1B4_K1each next layer increases the ﬁlter size by a factor of 2.
The maximal ﬁlter size is 512 and the kernel size is 11. For
the average pooling layer, pooling size is 2 and stride is 2.
Fully-connected layers have relu activation function and we
experiment with [128, 256, 512] number of neurons. After a
tuning phase, we select to use a single convolutional block
and two fully-connected layers with 128 neurons each. Batch
size equals 256, the number of epochs is 125, the loss function
is categorical cross-entropy, the learning rate is 0.0001, and the
optimizer is RM Sprop.
IV. RESULTS
Fig. 6: Same device and key scenario.
In this section, we present results for all scenarios we
investigate. Afterward, we discuss the issues with the val-
idation procedure and present our Multiple Device Model.
As mentioned earlier, we use guessing entropy as the metric
for comparison. In other words, we observe the average rank
of the key against
the number of traces or measurement
samples. An attack is effective if the guessing entropy goes
to 0 with minimum required samples. If at the end of attack
guessing entropy stays at x, the attacker must brute force 2x
different keys for key recovery. Note that we give averaged
results for a certain machine learning technique and number
of measurements. We denote the scenario where we use a
multilayer perceptron with all 600 features as MLP2, while
the scenario where multilayer perceptron uses 50 features we
denote as MLP.
A. Same Key and Same Device
The ﬁrst scenario we consider uses the same devices
and keys for both training and testing phases. Consequently,
this scenario is not a realistic one but is a common sce-
nario examined in the related works. This scenario does
not consider any portability aspect and is the simplest one
for machine learning techniques, so we consider it as the
baseline case. The results for all considered machine learning
techniques are given in Figure 6. We give averaged results
over the following settings: (B1 K1)−(B1 K1), (B2 K2)−
B2 K2), (B3 K1) − (B3 K1), (B4 K3) − (B4 K3). As
can be seen, all results are very good, and even the worst-
performing algorithm reaches guessing entropy of 0 in less
than 10 measurements. Thus, an attacker would need only
10 side-channel
traces from the target device to perform
the full key-recovery. Additionally, we see that adding more
measurements can improve the performance of attacks. The
worst performing algorithm is NB, followed by RF. The
differences among other algorithms are very small and we
see that guessing entropy reaches 0 after three measurements.
Note that despite somewhat smaller training sets (37 000) for
algorithms using all 600 features (CNN and MLP2), those
results do not show any performance deterioration. In fact,
the algorithms can reach guessing entropy of 0 with up to
three traces. Since we are using the same device and key to
train and attack, and we are using validation or cross-validation
to prevent overﬁtting, accuracy in the training phase is only
Fig. 7: Same device and different key scenario.
somewhat higher than accuracy in the test phase (depending
on the algorithm, ≈ 10 − 40%).
B. Same Device and Different Key
Next, we consider the scenario where we use the same
device in the training phase and testing phase, but we change
keys between those phases. When different users compute
on shared resources and standard cryptographic libraries (like
SSL),
this scenario becomes relevant. The malicious user
proﬁles the library with his application with all access rights
and attacks when the target user application is running. We
present the results for this scenario in Figure 7. Here, we
give averaged results over scenarios (B2 K1)−(B2 K2) and
(B2 K2)−(B2 K1). The ﬁrst observation is that this setting
is more difﬁcult for machine learning algorithms. Indeed,
NB, RF, and MLP (using 50 features) require more than 100
measurements to reach guessing entropy less than 10 (note that
NB with 10 000 measurements reaches only guessing entropy
of around 40). Interestingly, for these three techniques, adding
more measurements (i.e., going from 10 000 to 40 000) does
not bring a signiﬁcant improvement in performance. At the
same time, both techniques working with all features (MLP2
and CNN) do not seem to experience performance degradation
when compared to the ﬁrst scenario. Regardless of the number
of measurements in the training phase, they reach guessing
entropy of 0 after three measurements. For this scenario, we
observed that accuracy in the training phase could be up to
an order of magnitude better than accuracy in the test phase,
which indicates that our algorithms overﬁt.
8
Fig. 8: Same key and different device scenario.
Fig. 9: Different key and device scenario.
C. Same Key and Different Device
The third scenario we consider uses two different devices,
but the key stays the same. Note, since we consider dif-
ferent devices, we can talk about the real-world case, but
the same key makes it still a highly unlikely scenario. The
results are averaged over settings (B1 K1) − (B2 K1) and
(B2 K1)−(B1 K1). When considering performance, we see
in Figure 8 that this scenario is more difﬁcult than the previous
two as different targets introduce their noise patterns. Similar
to the previous scenario, all
techniques using 50 features
require more than 100 measurements to reach guessing entropy
less than 10. Additionally, adding more measurements does
not improve results signiﬁcantly. When considering techniques
using all 600 features, we see this scenario to be more difﬁcult
than the previous ones as we need seven or more traces
to reach guessing entropy of 0. Additionally, CNN using
10 000 measurements is clearly performing worse than when
using 40 000 measurements, which is a clear indicator that
we require more measurements to avoid underﬁtting on the
training data. Finally, we remark that in these experiments,
accuracy in the training set was up to an order of magnitude
higher than for the test set. Consequently, we see that while
we require more measurements in the training phase to reach
the full model capacity, those models already overﬁt as the
differences between devices are too signiﬁcant.
D. Different Key and Device
Finally, we investigate the setting where training and testing
are done on different devices and those devices use different
secret keys. Consequently, this is the full portability scenario
one would encounter in practice. As expected,
this is by
far the most difﬁcult scenario for all techniques, as seen in
Figure 9. In this scenario, the results are averaged over 8 dif-
ferent settings: (B1 K1)− (B4 K3), (B4 K3)− (B1 K1),
(B2 K2) − (B4 K3), (B4 K3) − (B2 K2), (B3 K1) −
(B4 K3), (B4 K3)− (B3 K1), (B1 K1)− (B2 K2), and
(B2 K2) − (B1 K1).
Interestingly, here RF is the worst performing algorithm,
it barely manages to reach
and with 100 measurements,
guessing entropy less than 90. NB and MLP perform better,
but still with 100 measurements, they are not able to reach
guessing entropy less than 15. At the same time, we see
a clear beneﬁt of added measurements only for NB. When
considering CNN and MLP2, we observe we require somewhat
more than 60 measurements to reach guessing entropy of 0.
There is a signiﬁcant difference in performance for CNN when
comparing settings with 10 000 and 40 000 measurements. For
MLP2, that difference is much smaller and when having a
smaller number of measurements, MLP2 outperforms CNN.
When CNN uses 40 000 measurements in the training phase,
it outperforms MLP2 with 10 000 measurements and both
techniques reach guessing entropy of 0 with the approximately
same number of measurements in the testing phase. As in the
previous scenario, we see that CNN needs more measurements
to build a strong model. This is in accordance with the intuitive
difﬁculty of the problem as more difﬁcult problems need more
data to avoid underﬁtting and to reach good model complexity.
Interestingly, in this scenario, accuracy for the training set is
easily two orders of magnitude higher than for the test set,
which shows that all techniques overﬁt signiﬁcantly. Indeed,
while we see that we can build even stronger models if we
use more measurements in the training phase, such obtained
models are too specialized for the training data and do
not generalize well for the test data obtained from different
devices.
E. General Observations
When considering machine learning techniques we used and
investigated scenarios, we see that MLP2 performs the best
(the difference with CNN is small, especially if considering
40 000 measurements in the training phase). In order to better
understand the difﬁculties stemming from speciﬁc scenarios,
we depict
the result for MLP2 and all four scenarios in
Figure 10.
Clearly, the ﬁrst two scenarios (having the same device
and key as well as changing the key but keeping the same
device) are the easiest. Here, we see that the results for the
scenario when changing the key indicate it is even slightly
easier than the scenario with the same device and key. While
other machine learning techniques point that using the same
key and device is the easiest scenario, they all show these two
scenarios to be easy. Next, a somewhat more difﬁcult case is
the scenario where we change the device and use the same key.
Again, the exact level of increased difﬁculty depends on the
9
Fig. 10: Multilayer perceptron with 600 features (MLP2) over
all scenarios, 10 000 measurements.
speciﬁc machine learning algorithm. Finally, the most difﬁcult
scenario is when we use different devices and keys. Here, we
can see that the effect of portability is much larger than the
sum of previously considered effects (changing only key or
device).
While all scenarios must be considered as relatively easy
when looking at the number of traces needed to reach guessing
entropy of 0, the increase in the number of required measure-
ments is highly indicative of how difﬁcult problem portability
represents. Indeed, it is easy to see that we require more
than an order of magnitude more measurements for the same
performance if we consider scenarios three and four. At the
same time, already for scenario three, we require double the
measurements than for scenarios one or two.
While we use only a limited number of experimental
settings, there are several general observations we can make:
1) Any portability setting adds to the difﬁculty of the
problem for machine learning.
2) Attacking different devices is more difﬁcult than attacking
different keys.