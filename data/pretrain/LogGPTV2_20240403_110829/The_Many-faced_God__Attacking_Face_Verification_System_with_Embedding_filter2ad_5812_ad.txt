### Independent Sample
The discriminator we employ omits all batch normalization layers (BN) and, after each convolutional operation, incorporates a small Residual Block, similar to our generator. This approach prevents the generator from dominating the process. Ultimately, the discriminator outputs a scalar value representing the confidence level that the input \( x \) belongs to the real data distribution \( p_r \).

### 4.3 Loss Function
Our loss function aggregates three types of losses and can be expressed as:
\[ L = w_r \cdot L_r + w_d \cdot L_d + w_e \cdot L_e \]
where \( w_r \), \( w_d \), and \( w_e \) are weights. Through empirical analysis, we found that the best ratio for \( w_r \), \( w_d \), and \( w_e \) is 3:1:1 to 2:1:1, which encourages the recovered image to have a realistic appearance. Below, we detail each loss component.

- **Discriminator Loss (\( L_d \))**: We use the WGAN-GP discriminator loss [23], which is represented as:
  \[ L_d = \text{WGAN-GP Discriminator Loss} \]

### Generator Design
Figures 6 and 7 illustrate the design of our erGAN generator and the residual structure used by the generator, respectively.

### Model Extraction Attack
In rare cases where an adversary cannot find an open-source model \( f' \) with similar accuracy to the target model \( f \), they can train a surrogate model \( f' \) through a model extraction attack. Previous works have demonstrated that such attacks, leveraging public APIs of MLaaS models, are feasible [13, 32, 46, 58, 60]. Thousands of queries can produce a good surrogate model [32].

### 5. Evaluation
In this section, we evaluate EmbRev, focusing on the accuracy of the recovered embeddings. We also discuss the impact of query number, no-box setting, photo quality, and the precision of the displayed score. Additionally, we evaluate ImgRev, focusing on the accuracy of face recovery. A key highlight of our evaluation is that under a white-box setting, after the attacker issues 2 queries to an FVS using Facenet-128, EmbRev can reconstruct an embedding that bypasses FVS with a 40% success rate. Twenty queries guarantee a 100% success rate. With the recovered embedding, ImgRev can generate a discernible victim face without further querying FVS. Below, we provide detailed results.

#### Targeted Embedding Models
We examined the security of four embedding models with different dimensions (128, 512, 1024, 1792) and distances (Cosine and L2). The models tested include widely used ones like Facenet and Clarifai, as well as a customized model. We adjusted the distance threshold of each model to match the accuracy reported in their literature or Git repositories using the LFW dataset [35]. Table 1 summarizes the details of each embedding model.

For the customized embedding model, we built it on top of inception-resnet-v1 of the Wide Residual Inception Network [1, 70]. Our customization includes adding cross-entropy loss over the "Additive Margin Softmax" layer after densifying the embedding, turning the model into a classifier. This change allows the embedding distance to be measured using Cosine distance. The model was trained with the CASIA-Webface dataset [68], achieving moderate accuracy.

| Model | Residual Inception Network | Clarifai Online Face Embedding [10] | Facenet 20180402-114759 [51] | Facenet 20170512-110547 [51] |
|-------|---------------------------|------------------------------------|-------------------------------|-------------------------------|
| Emb. Dim. | 1792 | 1024 | 512 | 128 |
| Distance Type | Cosine | Cosine | Cosine | L2 |
| TH | 0.78 | 0.55 | 0.63 | 1.28 |
| Emb. Acc. | 92.1% | 98.1% | 97.6% | 97.1% |

#### Experiment Settings
For evaluating EmbRev, we attacked two Facenet models using the LFW dataset, with no training required. Both white-box and black-box settings were evaluated together, as they allow the adversary to access the same score and embedding for each query. The no-box setting was evaluated separately using another embedding model as a surrogate.

For ImgRev, all four embedding models were attacked using the celebA dataset for training and testing. We focused on white-box and black-box settings, as the embeddings recovered under the no-box setting had large error margins. The black-box setting differed from the white-box setting because another open-source model \( f' \) was used to generate \( \nabla L_e \).

The training of ImgRev to attack one embedding model took approximately 6 to 7 hours on a machine equipped with an NVIDIA GeForce RTX 2080 Ti GPU. Recovering 32 face images in a batch during the testing stage took 105 milliseconds. For EmbRev, the overhead was negligible.

### 5.1 Effectiveness of EmbRev
We used 300 photos from the LFW dataset to create the victim dataset. Each photo was sent to the tested model, and its embedding was stored. To simulate the attack, for each victim photo, we queried the tested embedding models with another set of photos (query photos) and recorded the embedding vectors and their distances to the victim photo.

These distances and embeddings were inputted into EmbRev to recover the victim embedding. When the number of queries equals the embedding dimension (128 for Facenet-128), every victim embedding was nearly perfectly recovered, with small errors due to floating-point calculations, which were within \( 10^{-4} \) and far smaller than the threshold of the embedding models.

**Reducing Query Number**: The analysis in Section 3.2 shows that a 128-dimensional face embedding can be very close (i.e., distance less than 0.1) to its 33-dimensional compressed version, indicating that the information inside the face embedding is sparse. EmbRev leverages this property to reduce the number of queries, making the attack more stealthy.

**No-Box Setting**: For this experiment, we assumed the targeted FVS uses Facenet-512, to which the adversary has no white-box or black-box access. The adversary used Facenet-128 as a surrogate model to obtain embeddings and run EmbRev. The recovered embedding had 128 dimensions, and we computed the L2 distance under different query numbers. The optimal result was observed with 34 queries, where the average error distance was 1.026. While the result was worse than the prior setting, the adversary still had a good chance to bypass FVS.

**Precision of Displayed Score**: In previous experiments, we assumed the adversary could see the displayed score with high precision. However, the FVS operator or developer can choose to hide part of the score. We truncated the distance values returned by the embedding models to 2 decimal fractions and re-ran the experiment on Facenet-128 with 60 queries. The average error distance for this setting was 0.066, well below the distance threshold. As FVS usually shows scores with at least 2 decimal fractions, EmbRev is robust against score truncation.

### 5.2 Effectiveness of ImgRev
We used the LFW dataset to test EmbRev but found the images unsuitable for testing ImgRev, as many were captured in unofficial settings. Therefore, we used the celebA dataset, which consists of celebrity images labeled under 40 attributes. We removed images with attributes like "Blurry," "Oval_Face," "Bangs," and "Eyeglasses" to ensure the images were taken facing the camera with good angles and no coverings. The dataset was split into training and testing sets of 20,480 and 1,800 images, respectively.

For each photo in the testing set, we generated its embedding using all four models listed in Table 1 and then used ImgRev to reconstruct the photo. These embeddings can be considered "perfect" embeddings recovered by the adversary. We evaluated how errors produced by EmbRev impacted the result of ImgRev.

We first tested the black-box settings without \( L_e \) in the loss function as the baseline. All four embedding models were tested. Then, we tested the white-box setting, where \( \nabla L_e \) could be computed using the same embedding model \( f \) as FVS, and we used Facenet-128 as \( f \). Finally, we tested the black-box setting again (Facenet-128 as \( f \)) but using another surrogate model \( f' \) (Facenet-512) to generate \( \nabla L_e \).

**Qualitative Results**: We employed the trained generator to recover the first 1,800 images in the testing set from their embeddings. Figure 10 shows the recovered versions of the first five images. The victims can be easily discerned, suggesting ImgRev is quite effective. The recovery quality varies, with ImgRev working best on Clarifai-1024, likely because it embeds more facial details, providing more information to the adversary.

| Model | Blackbox Baseline | White-Blackbox | Box |
|-------|-------------------|----------------|-----|
| 128   | 93.07%            | 97.23%         | 98.63% |
| 512   | 93.87%            | 94.20%         | 96.23% |
| 1024  | 61.25             | 157.47         | 33.94  |
| 1792  | 49.39             |                |       |