independent sample. The discriminator we use drops all batch nor-
malization layers (BN), and after every convolutional operation, we
add a small Residual Block just like our generator, to avoid that the
generator dominates the process. At last, the output of the discrim-
inator will be a scalar value that is the confidence level that the
discriminator considers xд falling within pr .
4.3 Loss
Our loss function aggregates three types of losses and it can be
represented as
L = wr · Lr + wd · Ld + we · Le
(12)
where wr , wd and we are weights. Through empirical analysis,
we found 3:1:1 to 2:1:1 is the best ratio for wr , wd and we, which
encourages the recovered image to have realistic looking. Below
we elaborate each loss.
Discriminator loss (Ld ). We use the loss of WGAN-GP’s discrim-
inator for Ld [23], represented as
NoiseLabelGeneratorOutput PlaneNoiseRandom ImagesGeneratorConditional GANGANEmbeddingCorresponded Face ImageGeneratorOur GeneratorCorresponded23ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Mingtian Tan, Zhe Zhou and Zhou Li
Figure 6: The design of erGAN generator.
Figure 7: Residual structure used by
the generator.
In rare cases when the adversary cannot find an open-source
f ′ with similar accuracy as f , she can train a surrogate model
f ′ through model extraction attack. In fact, previous works have
demonstrated that model extraction by abusing public APIs of
MLaaS models is feasible [13, 32, 32, 46, 58, 60]. Thousands of
queries can produce a good surrogate model [32].
5 EVALUATION
In this section, we firstly evaluate EmbRev, focusing on the accu-
racy of the recovered embeddings. In addition, we also discussed
the impact of query number, no-box setting, the quality of photos
and the precision of displayed score. Then we evaluate ImgRev,
focusing on how accurate the victims’ faces can be recovered. As
a highlight of our evaluation result, under whitebox setting, after
the attacker issues 2 queries to an FVS using Facenet-128, EmbRev
can reconstruct an embedding that bypasses FVS at 40% chances.
20 queries guarantee 100% success rate. With the recovered em-
bedding, ImgRev is able to generate a discernible victim face (see
Figure 10) without querying FVS. Below we elaborate the details.
Targeted embedding models. We examined the security of 4 em-
bedding models with different embedding dimensions (128, 512,
1024, 1792) and distances (Cosine and L2). The widely used models
like Facenet and Clarifai and an embedding model customized by
us are tested. We adjust the distance threshold of each embedding
model to match the accuracy reported by their literature or git
repository using LFW dataset [35], because the threshold is not
always public available. We are able to tune each model with the
same or even better accuracy except Facenet. The reason is that we
apply dlib [33] for alignment, following the design of OpenCV [12].
Table 1 shows the details of each embedding model.
For the customized embedding model, we built it on top of
inception-resnet-v1 of Wide Residual Inception Network [1, 70].
Our customization includes adding cross-entropy loss over the “Ad-
ditive Margin Softmax” layer after densing the embedding, which
turns the model to a classifier for training. Because of this change,
the embedding distance can be measured by Cosine distance. We
trained the model with CASIA-Webface [68]. As shown in Table 1,
moderate accuracy can be achieved.
Experiment settings. For the evaluation on EmbRev, we attack
the two Facenet models. We tested the performance of EmbRev
using LFW dataset and no training is needed. The white-box and
black-box settings are jointly evaluated because they all allow the
adversary to access the same score and embedding for each query.
We evaluate the no-box setting separately by using another embed-
ding model as surrogate model. For the evaluation on ImgRev, all 4
embedding models are attacked. We use celebA dataset to train and
test ImgRev. We focus on white-box and black-box settings as the
embeddings recovered under the no-box setting have large error
margins. The black-box setting has result different from white-box
as another open-source model f ′ is leveraged to generate ∇Le.
For the overhead, the training of ImgRev to attack one embed-
ding model costs us around 6 to 7 hours on a machine equipped
with NVIDIA GeForce RTX 2080 Ti GPU, while recovering 32 face
images as a batch in the testing stage costs 105 milliseconds. For
EmbRev, the overhead is negligible.
Model
Residual
Inception Network
Clarifai Online
Face Embedding [10]
Facenet
20180402-114759 [51]
Facenet
20170512-110547 [51]
Emb.
Dim.
1792
1024
512
128
Distance
Type
Cosine
Cosine
Cosine
L2
TH
0.78
0.55
0.63
1.28
Emb.
Acc.
92.1%
98.1%
97.6%
97.1%
Table 1: Embedding models evaluated by us. “Emb. Dim.” is
the embedding dimension. “TH” is the distance threshold be-
low which two embeddings are considered to be of the same
person. “Emb. Acc.” is the accuracy of embedding model.
5.1 Effectiveness of EmbRev
We used 300 photos from the LFW dataset to create the victim
dataset. We sent each photo to the tested model and stored its
embedding, which is the secret. Then, to simulate the attack, for
each victim photo, we queried the tested embedding models with
another set of photos (we call them query photos) and recorded all
the embedding vectors and their distances to the victim photo.
The distances and embeddings were inputted into EmbRev to
recover the victim embedding. We implemented EmbRev with
Matlab. When the number of queries equals to the embedding
dimension (128 for Facenet-128), without exception, every victim
embedding can be recovered nearly perfectly. The small error
margins are caused by floating-point calculation, which are within
10−4 and far smaller than the threshold of embedding models.
Reducing query number. The analysis in Section 3.2 shows that
128-dimensional face embedding can be very close (i.e., distance
Embedding, 1*1,5125*5,5122*2,5122*2,51210*10,5121st deconv2nd deconv10*10,25610*10,5124*4,51210*10,256R20*20,256R20*20,25620*20,512RR3rd deconvCombinePhase 1: Multi PathmediocrerapidmildmildPhase 2: Single PathSeveral timesdeconv & RSize Double, Filter HalfconvBNactivateconvBNactivateconvBNactivate+0.250.75input24The Many-faced God: Attacking Face Verification System with Embedding and Image Recovery
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
less than 0.1) to its 33-dimensional compressed version, indicating
that information inside face embedding is sparse. EmbRev makes
full use of this property to reduce the number of queries issued by
the adversary, so the attack can be even stealthier.
Figure 8: Error Distances and acceptance rates versus the
query numbers on Facenet-128.
We firstly examined Facenet-128 model, by reducing the query
number from 128 to 0 gradually. For each query number, we com-
pute the error distance, i.e., the distances between the recovered
and victim embeddings, and show the average in Figure 8. It turns
out even when we reduce the query number to half, i.e., 60, the
average error is very small (at 0.022). The error distance goes up
to 0.1 when 53 queries are made which is still negligible as the dis-
tance threshold is 1.28 (shown in Table 1). The average error never
exceeds 1.28, even with only two queries, in which case over 40%
generated images can still be accepted by FVS. If only the attacker
is able to make 20 queries, she has 100% chance to pass FVS. In
Figure 3, we show the distances between M (matrix of embeddings)
and ˜M (lower-rank approximation of M) on Facenet-128. Given a
query number r, the error distances introduced by ˜M at rank r can
be considered as its lower-bound. Through experiments, we found
in order to reach such lower-bound, the attacker needs to query
r + 20 times approximately.
Interestingly, when evaluating embedding models of higher di-
mensions, we found that the query number does not have to be
increased. For Facenet-512, EmbRev costs the attacker only 39
queries to drop the mean error distance below 0.063 (10% of the
threshold as shown in Table 1). We speculate it is because embed-
ding models with better accuracy can extract more robust features,
which can be captured by embedding models of lower dimensions.
No-box setting. For this experiment, we assume the targeted FVS
uses Facenet-512, to which the adversary has no white-box or black-
box access. She uses Facenet-128 model as the surrogate model to
obtain embeddings and run EmbRev. To be noticed is that Facenet-
128 and Facenet-512 are very different embedding schemes: L2 and
Cosine distance are used respectively and they are trained on dif-
ferent datasets. The recovered embedding has 128 dimensions and
we compute L2 distance under different query numbers. The result
is presented in Figure 9.
Different from Figure 8, where error distance decreases follow-
ing the increase of query number, Figure 9 shows error distance
decreases first and then increases. The optimal result is observed
when issuing 34 queries, where 1.026 is the average error distance.
While the result is worse than the prior setting as expected, the
adversary still has very good chance to bypass FVS.
Figure 9: Error distance and the acceptance rate versus the
query numbers under no-box setting.
For the following experiments with ImgRev, we neglect no-
box setting as the error introduced from this step is still large
enough (i.e., over 1) that prevents victim face recovery. However,
we consider EmbRev is effective under the no-box setting as the
recovered embeddings can pass FVS (when issuing 34 queries).
Precision of displayed score. In the prior experiments, we as-
sume the adversary can see the displayed score with high precision.
However, the FVS operator or developer can choose to hide part of
the score. For example, Figure 1a displays 16 digits while Figure 1b
displays only 4 digits. When less digits are displayed, the embed-
dings recovered by EmdRev would be less accurate and we try to
quantify this impact.
In particular, we truncate the distance values returned by the
embedding models to 2 decimal fractions (e.g., 1.23456 is truncated
to 1.23) and re-run the experiment on Facenet-128 with 60 queries.
The average error distance for this setting is 0.066 (in contrast, 0.022
for full precision), such error is well below the distance threshold. As
FVS usually shows scores with at least 2 decimal fractions, EmbRev
is shown to be robust against score truncation.
5.2 Effectiveness of ImgRev
We used the images from LFW dataset to test EmbRev but we
found those images are not suitable for testing ImgRev as many of
them were captured in unofficial occasions which would never been
encountered by FVS. As such, we used another dataset, celebA [38],
which consists of celebrity images labeled under 40 attributes, to
train and test ImgRev. We remove the images with attributes of
“Blurry”, “Oval_Face” and “Bangs” and “Eyeglasses”, because these
images are taken usually not facing the camera with good angle
or with coverings on faces. For FVS, photos usually have good
angle and people do not wear coverings. The dataset was split
into training and testing set of 20,480 (DStr ain) and 1,800 (DStest )
images. To avoid the same person showing up in both datasets, we
cluster the images based on their Identity ID and assign a cluster
into either DStr ain or DStest .
For each photo in DStest we generate its embedding using all 4
models listed in Table 1 and then use ImgRev to reconstruct the
photo. Those embeddings can be considered as “perfect” embed-
dings recovered by the adversary. In the end of this section, we
evaluate how errors produced by EmbRev impact the result of
ImgRev.
We firstly tested the black-box settings without Le in loss func-
tion as the baseline. All four embedding models are tested. Then,
102030405060# of queries40%60%80%100%Acceptance Rate00.511.5Avg. Error Dist.102030405060# of queries40%60%80%100%Acceptance Rate11.11.21.31.4Avg. Error Dist.25ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Mingtian Tan, Zhe Zhou and Zhou Li
we tested white-box setting, where ∇Le can be computed using
the same embedding model f as FVS and we use Facenet-128 as f .
Finally, we tested the black-box setting again (Facenet-128 as f ) but
using another surrogate model f ′ (Facenet-512) to generate ∇Le.
Figure 10: First five samples in DStest . The first row shows
original photos. The second to the fifth row show the images
recovered under blackbox baseline setting (without Le). The
sixth row shows white-box setting on Facenet-128. The last
row shows the black-box setting with Le is generated under
another model Facenet-512 (f ′) when Facenet-128 is the tar-
geted model (f ).
Qualitative results. We employ the trained generator to recover
the first 1,800 images in DStest from their embeddings. Figure 10
shows the recovered versions of the first five images in DStest .
The victims can be easily discerned, suggesting ImgRev is quite
effective. On the other hand, the recovery quality differs. ImgRev
works best on Clarifai-1024, probably because Clarifai-1024 embeds
more facial details, yielding more information to the adversary.
Model
Acc.
FID
Blackbox Baseline
White- Blackbox
box
128
93.07% 97.23% 98.63% 93.87% 94.20%
86.00
114.11
96.23%
61.25
157.47
33.94
49.39
512
1024