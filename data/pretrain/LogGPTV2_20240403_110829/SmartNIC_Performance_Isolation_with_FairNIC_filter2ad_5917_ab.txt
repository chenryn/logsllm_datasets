ASIC-based fixed-function NICs. Bandwidth isolation for tenant
traffic is usually enforced by some form of virtual switch employ-
ing a combination of packet scheduling and rate-limiting tech-
niques [24, 30]. Because per-packet processing on host CPUs is not
feasible at high link rates, modern cloud providers are increasingly
moving traffic-scheduling tasks to the NIC itself [2, 16].
While this approach remains applicable in the case of Smart-
NICs, one of the key features of programmable NICs is the wealth
of hierarchical traffic-scheduling functionality. Hence, care must
be taken to ensure that a tenant’s internal traffic-scheduling de-
sires do not conflict with—or override—the provider’s inter-tenant
mechanisms. Moreover, because tenants can now install on-NIC
logic that can create and drop packets at will, host/NIC-bus (i.e.,
PCIe) utilization and network-link utilization are no longer tightly
coupled, necessitating separate isolation mechanisms for host/NIC
and network traffic.
3.1.1 Packet egress. Bandwidth isolation requires accounting
for the different packet sizes of different NIC applications—which
may differ from the original packet size when sent by the tenant’s
host-based application. The leftmost portion of Figure 1 shows the
default behavior when we run three on-NIC applications that gen-
erate different packet sizes. Despite equal core and host/NIC traffic
allocations, outgoing packets from the NIC cores to the network are
scheduled on a round-robin basis resulting in unfair link bandwidth
allocation (applications with larger packet sizes consume a larger
share). The second plot shows that fair allocation can be restored by
enforcing appropriate traffic scheduling (deficit round robin in this
case) at the NIC egress—after on-NIC tenant application processing.
3.1.2 Packet ingress. Ingress link bandwidth cannot be isolated
by the NIC itself as hosts are not in control of incoming traffic. Dat-
acenters usually use some form of sender-side admission control
that is out of scope for this paper. Once traffic arrives at the NIC,
however, ingress hardware parses the packets and determines how
to handle them. Traditional NICs generally DMA the packet di-
rectly into tenant host memory, but packets in SmartNICs are likely
destined to on-NIC cores for processing. While the processing rate
of SmartNIC ingress hardware is sufficient to demultiplex incoming
traffic, the effective service rate of the pipeline is gated by how fast
packets are consumed by later stages (i.e., tenant application cores)
020406080100Time (seconds)0510152025GbpsApp 1 (1024 B)App 2 (512 B)App 3 (256 B)020406080100Time (seconds)0510152025GbpsApp 1 (1024 B)App 2 (512 B)App 3 (256 B)400600800100012001400Packet Size (Bytes)0510152025GbpsApp 1 (8 cores)App 2 (1 core)400600800100012001400Packet Size (Bytes)0510152025GbpsApp 1 (8 cores)App 2 (1 core)SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
Grant, Yelam, Bland, and Snoeren
Figure 2: Maximum throughput of seven NIC cores for vari-
ous packet sizes as a function of processing cycles per packet
of the pipeline. Because tenants process their packets at different
rates, it is important to separate traffic as soon as possible.
To demonstrate this issue, we generate two distinct traffic flows
destined to different tenants, each one of which is running an on-
NIC OVS application. One tenant’s application is allocated eight
NIC cores while the other uses just one, effectively limiting the
latter flow’s throughput to one-eighth of the first. However, as
shown in the third portion of Figure 1, both flows are processed at
the same rate—namely that achievable by one core—due to head-of-
line blocking: The ingress engine uses a single buffer pool per port
and does not differentiate between tenants. By allocating separate
buffer pools and separating the traffic immediately upon arrival
(see Section 4.2.1), we are able to restore proportional allocation as
shown in the rightmost portion of the figure.
3.2 Core cycles
As discussed in Section 2 general-purpose cores provide the pro-
grammability at the heart of SoC SmartNICs. While other execution
models exist, usually cores perform end-to-end packet processing
wherein each core processes a batch of packets at a time and runs
to completion before moving on. The amount of time each core
spends on a batch determines the effective throughput of the appli-
cation. In particular, the more complicated an application’s logic,
the lower throughput an individual core can deliver. Critically, in-
dividual cores on today’s SoC-based SmartNICs are unable to keep
up with commodity (e.g., 25-Gbps) link rates, and even generous
core allocations may fall short when tenant application processing
is particularly involved.
To characterize the packet processing capabilities of our Smart-
NIC, we measure the throughput of a simple NIC program running
on seven cores that redirects incoming packets back to the net-
work, but incurs a specified amount of artificial overhead for each
packet. We repeat this experiment for various packet sizes and plot
throughput as a function of cycles spent per packet in Figure 2. (Fig-
ures 2–4 of Liu et al. [35] show similar trends for other commodity
SmartNICs.) The plot shows that within a couple of thousand cy-
cles (instructions), core processing replaces link bandwidth as the
limiting resource, even for packet sizes as large as 1 KB. Hence,
appropriate core allocation is critical for application performance,
even with relatively simple processing tasks.
Figure 3: ZIP latency as a function of offered load
3.3 Memory access latency
Programs that process packets at link rate must meet tight timing
requirements which are frustrated by the memory access latencies
of typical SmartNICs (≈60-ns penalty for an L2-cache miss in our
case; see, e.g., Table 2 of Liu et al. [35]). Cavium’s programmers’
guide extensively documents techniques for packet processing in
primarily L1 cache and stresses the criticality of working within
the limits of L2. Even if individual tenant applications are diligent
in their memory locality, however, the L2 cache is typically shared
across cores on a SmartNIC, meaning applications are likely to evict
the cache lines of their neighbors. On the Cavium CN2360, all 16
cores share a single L2 cache, making the issue particularly acute.
The performance degradation from cache interference can clearly
be seen when running a key/value store (KVS) program (described
in Section 6.2.2) alongside a program with poor data locality that
issues a large number of memory accesses, resulting in high cache
pressure. Our KVS program runs on eight cores sharing a total of
5 MB of RAM. The high-cache-pressure application runs on the
other eight cores, stepping over a large allocation of memory at
128-byte intervals to maximize cache-line evictions. As detailed in
Table 2, the throughput of the KVS program drops by over an order
of magnitude (from 23.55 to 3.2 Gbps) in the presence of the cache-
thrashing application, while transaction latency increases by more
than two orders of magnitude (from 65 to over 6700 microseconds).
3.4 Coprocessors
SoC SmartNICs like Cavium’s LiquidIO come with a rich ecosys-
tem of hardware coprocessors. There are a variety of hardware
accelerators that implement common networking tasks such as ran-
dom number generation, secret key storage and access, RAID, ZIP,
and deep packet inspection (regular-expression matching). Each
coprocessor has different performance characteristics and physical
location which can affect a given core’s access latency to the offload.
Each accelerator has a roughly fixed rate at which it can com-
pute; until requests over-saturate that rate no queuing occurs. If all
requests were synchronous and took the same amount of time, core
isolation would imply fair accelerator access. Some operations, how-
ever, incur latencies that are proportional to the size of the input
data so applications issuing larger requests can gain disproportional
access, leading to starvation for coexisting applications.
Figure 3 shows the latency spike that results when the ZIP
(de)compression accelerator is overloaded with very large (16-MB
684
010002000300040005000Cycles0510152025Gbps64B128B256B512B1024B1500B24681012thousand requests per second050100150200250300350400microsecondsMaxMeanMinSmartNIC Performance Isolation with FairNIC
SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
Figure 4: Coprocessor latency with(out) contention
in this example) requests. Until around 3,000 requests per second,
the ZIP accelerator is able to meet demand; afterward, requests are
queued. The flat maximum latency is the point at which the eight
cores in the experiment all block on ZIP requests; had more cores
participated in the experiment the latency would be even greater.
We find that the majority of accelerators have similar latency-
response curves, with the exception of accelerators that have
(seemingly) non-deterministic execution times such as the regular-
expression accelerator. Figure 4 shows the minimum and maximum
latency observed for three different accelerators; latency increases
under contention are approximately an order of magnitude.
3.5 Bus arbitration
Others have shown that PCIe bandwidth arbitration can become a
shared bottleneck [27, 34, 40] and propose solutions [40]. We do not
encounter that limit in our current configuration (we use one 25-
Gbps PCIe 3.0 ×8 SmartNIC per host), but commercial clouds may
need to employ appropriate mechanisms to address the contention.
4 ISOLATION TECHNIQUES
FairNIC provides a set of per-resource isolation techniques to ensure
that each resource is partitioned (wherever possible) or multiplexed
according to tenant service-level objectives. In this section, we
introduce our isolation techniques by demonstrating how they
solve the issues discussed in the previous section and then discuss
their costs in the context of our targeted SmartNIC platform.
In our current implementation, SLOs are expressed in terms of
per-resource weights (e.g., fraction of cores or DWRR shares). By
expressly allocating every resource in a packet’s path that could
become a shared point of contention, FairNIC effectively dedicates a
portion of the SmartNIC’s end-to-end packet processing pipeline to
each tenant as shown in Figure 5. Note that individual resources may
be allocated in different proportions depending on the needs of each
tenant. Moreover, we presume that tenants provide their offload
applications to the cloud operator for verification before installation
(i.e., the PaaS model from Section 2.1). The cloud provider may
choose to test the application or employ static analysis to ensure
benign behavior in the common case. In particular, we assume
that the applications are written using our framework, and do not
attempt to circumvent our isolation mechanisms.2 We discuss the
limitations of these assumptions in Section 7.
2Cavium’s Simple Executive does not employ hardware memory protection; we leave
support for such traditional isolation mechanisms to future work.
Figure 5: FairNIC sharing resources between two tenants,
shown in orange and blue. (Some resources are consumed
by FairNIC itself, depicted in pink.)
4.1 Core partitioning
The cornerstone of FairNIC’s isolation is a static partitioning of
cores across tenant applications: each tenant application is assigned
a set of cores that process all of that tenant’s traffic in a non-work-
conserving fashion. Static application-core mappings allow tenant
applications to benefit from instruction locality in the L1 cache
and simplify packet processing. We configure the ingress engine to
group packets by tenant MAC address and directly steer packets to
the cores upon which that tenant’s application is running.
Costs. While time-sharing cores across applications could, in
principle, result in more efficient use of resources, the required
context switches would likely add significant delay to packet pro-
cessing. Our approach fundamentally limits the number of tenant
applications a given SmartNIC can support, but we argue that to-
day’s SmartNICs provide an adequate number of cores (e.g., 16–48
for the LiquidIO boards we consider) for the handful of tenants
sharing a single machine. Moreover, hosts shared by a significant
number of network-hungry tenants are likely to be provisioned
with more than one NIC to deliver adequate link bandwidth.
4.2 Traffic scheduling
Cavium NICs come with highly configurable packet ingress/egress
engines that support a variety of quality-of-service features in hard-
ware. For example, the egress engine provides multiple layers of
packet schedulers and shaper units that can be configured in soft-
ware to build hierarchical packet schedulers [48]. Each of these
units provides a set of scheduling algorithms (typically, a combina-
tion of deficit weighted round robin (DWRR) [47], strict priority
scheduling, and traffic shaping) and schedule packets at line rate.
Ingress. To isolate incoming traffic, we program the
ingress hardware to differentiate between packets from different
tenants and direct each tenant’s traffic to its own separate buffer
pool where it waits for cores to process it. Once these pools fill
up, the ingress hardware starts dropping packets, but only those
belonging to tenants with full buffers.
4.2.1
4.2.2 Egress. We use deficit weighted round robin to ensure
bandwidth isolation for tenant applications. FairNIC implements a
hierarchical scheduler where each tenant gets an independent sub-
tree of packet schedulers (which they can configure in whatever
685
Secure KeyRNG0.00.51.01.52.02.5Microseconds1 core16 coresGzip0100200300400500600HostIntel x86 CoresHost MemoryHypervisorVM1VM0VM0VM1SmartNICSecurityZip     Rate-limitingcoprocessors (4.4)HVTraﬃc Scheduling (4.2)Core Partitioning (4.1)Cache Striping (4.3)L2 CacheMain MemoryMIPS CoresEthPCIeSIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
Grant, Yelam, Bland, and Snoeren
Figure 6: FairNIC inserts TLB entries of various sizes depend-
ing on the number of cores assigned.
manner they like), each of which falls into a DWRR scheduler at the
root, with one input queue for each application and weights (DRR
quanta) on the input queues proportional to their SLOs. One down-
side of DWRR is that it does not provide strong latency guarantees
and may exacerbate tail-latency issues. While there are “better”
schedulers in the literature, DWRR is available in hardware and
runs at line rate; latency is not an issue in our case as we only need
one input queue per application.
Costs. FairNIC’s tenant isolation consumes one of the layers of
packet schedulers/shapers in the egress engine, leaving fewer layers
for other intra-tenant traffic scheduling purposes. Moreover, Fair-
NIC does not at present interpose upon dynamic modifications to
the packet-processing engines, passing the burden of ensuring that
tenant applications do not attempt to reconfigure the scheduling
hardware to circumvent pre-assigned policies to the operator.
4.3 Cache striping
Our solution to L2-cache isolation is to provide each application an
isolated region of physical memory. FairNIC implements this iso-
lation by explicitly constructing each application’s virtual address
space so that its (contiguous) virtual heap is mapped to a (striped)
set of physical memory regions that occupy distinct L2 cache lines.
Cavium CN2360 NICs have a 16-way, 2048-set-associative L2 cache,
with 128-byte cache lines. Cache lines are indexed with address bits
7 to 17. Assuming each core gets an equal-sized memory region (to
which we refer to as a color), with 16 cores, the upper-4 bits (14–17)
of the cache index are a color prefix corresponding to the color of
each core. Each core’s colored memory consists of non-contiguous
stripes of 16-KB chunks.
16-KB chunks are ideal for paging, but the latency induced by
walking a page table is far too great when serving packets at 25
Gbps. Rather than page we statically set TLB entries at calls to
malloc.3 Our malloc allocates a single color of 16-KB chunks of
physical memory on a per-core basis. TLB entries are written to the
calling core which stitches the non-contiguous physical allocation
into a single contiguous virtual allocation. Each core has 255 TLB
entries so FairNIC can support up to 4 MB of colored allocation per
core with a total of 64 MB of isolated memory in the system.
MIPS TLBs allow for variable-sized TLB entries. We utilize this
feature to provide applications running on multiple cores with
proportionally larger stripes of isolated L2 cache. An application
with n cores is allocated (n · 16384)-byte contiguous stripes of
physical memory. As shown in Figure 6, identical expanded TLB
entries of size n·16384 are mapped to each of the application’s cores.
3Our implementation does not currently stripe code or stack segments.
Coloring memory in this way fundamentally limits NIC programs
to a small portion of the 16-GB available physical memory. However,
any program wishing to access larger regions of memory is free to
implement its own pager.
Costs. Applications using the Cavium Simple Executive have
their text, stack and fixed-sized heap loaded into a 256-MB con-
tiguous region of physical memory which is mapped into their
virtual address space using three TLB entries. Accesses outside
this pre-configured region (e.g., packet buffers, coprocessors, etc.)
use physical addressing. FairNIC employs virtual addressing for all
references to enforce isolation. Hence, memory references incur up
to one additional cycle of latency for TLB translation that was pre-