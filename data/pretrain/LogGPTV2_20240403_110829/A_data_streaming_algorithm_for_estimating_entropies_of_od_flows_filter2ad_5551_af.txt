5.3%
3.3%
2.5%
 0  0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
Relative Error
Figure 8: Varying fraction of traﬃc from ingress
s
t
n
e
m
i
r
e
p
x
e
f
o
n
o
i
t
c
a
r
F
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
Entropy error
 0
 0.05
 0.1
 0.15
Relative Error
 0.2
 0.25
Figure 9: Error distribution for actual entropy
source and destination. In Figure 9 we observe that the er-
ror plot for the entropy is comparable to that for the entropy
norm. Hence, this conﬁrms the fact that our algorithm is a
robust estimator of the entropy of OD ﬂows.
9. RELATED WORK
There has been considerable previous work in computing
the traﬃc matrix in a network [18, 21, 22, 23, 27]. The traﬃc
matrix is simply the matrix deﬁned by the packet (byte)
counts between each pair of ingress and egress nodes in the
network over some measurement interval. For ﬁne-grained
network measurement we are sometimes interested in the
ﬂow matrix [28], which quantiﬁes the volume of the traﬃc
between individual OD ﬂows. In this paper we propose the
computation of the entropy of every OD ﬂows, which gives
us more information than a simple traﬃc matrix and has
considerably less overhead than maintaining the entire ﬂow
matrix.
In the last few years, entropy has been suggested as a use-
ful measure for diﬀerent network-monitoring applications.
Lakhina et al. [15] use the entropy measure to perform anomaly
detection and network diagnosis. Information measures such
as entropy have been suggested for tracking malicious net-
work activity [11, 24]. Entropy has been used by Xu et
al. [25] to infer patterns of interesting activity by using it
to cluster traﬃc. For detecting speciﬁc types of attacks,
researchers have suggested the use of entropy of diﬀerent
traﬃc features for worm [24] and DDoS detection [11]. Re-
cently, it has been shown that entropy-based techniques for
anomaly detection are much more resistant to the eﬀects of
sampling [3] than other, volume-based methods.
The use of stable distributions to produce a sketch was
ﬁrst proposed in [12] to measure the L1 distance between
two streams. This result was generalized to the Lp distance
for all 0 < p ≤ 2 in [8, 13]. This sketch data structure is
the starting point for the one that we propose in this paper.
The main diﬀerence is that we have to make several key
modiﬁcations to make it work in practice. In particular, we
have to ensure that the number of updates per packet is
small enough to feasibly perform in realtime.
Other than for p = 1, 2, there is no known closed form
for the p-stable distribution. To independently draw values
from an arbitrary p-stable distribution, we make use of the
formula proposed by [6]. Since this formula is expensive
to compute, we create a lookup table to hold several pre-
computed values.
In [7] it is suggested that the stable distribution sketch
can be used as a building block to compute empirical en-
tropy, but no methods for doing this are suggested. More
importantly, there are already known to be algorithms that
work well for the single stream case [16] and we believe that
it is for this distributed (i.e., traﬃc matrix) setting that the
stable sketch really shines.
10. CONCLUSION
In this paper we motivate the problem of estimating the
entropy between origin destination pairs in a network and
present an algorithm for solving this problem. Along the
way, we present a completely novel scheme for estimating
entropy, introduce applications for non-standard Lp norms,
present an extension of Indyk’s algorithm, and show how
it can be used in our distributed setting. Via simulation
on real-world data, collected at a tier-1 ISP, we are able to
demonstrate that our algorithm is practically viable.
11. REFERENCES
[1] A. Chakrabarti, K. Do Ba, and S. Muthukrishnan.
Estimating entropy and entropy norm on data streams. In
STACS, 2006.
[2] L. Bhuvanagiri and S. Ganguly. Estimating entropy over
data streams. In ESA, 2006.
[3] D. Brauckhoﬀ, B. Tellenbach, A. Wagner, M. May, and
A. Lakhina. Impact of packet sampling on anomaly
detection metrics. In IMC, 2006.
[4] G. Casella and R. L. Berger. Statistical Inference. Duxbury,
2nd edition, 2002.
[5] A. Chakrabarti and G. Cormode. A near-optimal algorithm
for computing the entropy of a stream. In SODA, 2007.
[6] J. M. Chambers, C. L. Mallows, and B. W. Stuck. A
method for simulating stable random variables. Journal of
the American Statistical Association, 71(354), 1976.
[7] G. Cormode. Stable distributions for stream computations:
It’s as easy as 0,1,2. In Workshop on Management and
Processing of Data Streams, 2003.
[8] G. Cormode, P. Indyk, N. Koudas, and S. Muthukrishnan.
Fast mining of massive tabular data via approximate
distance computations. In ICDE, 2002.
[9] M. Durand and P. Flajolet. Loglog counting of large
cardinalities. In ESA, 2003.
[10] C. Estan and G. Varghese. New Directions in Traﬃc
Measurement and Accounting. In SIGCOMM, Aug. 2002.
[11] L. Feinstein, D. Schnackenberg, R. Balupari, and
D. Kindred. Statistical approaches to DDoS attack
detection and response. In Proceedings of the DARPA
Information Survivability Conference and Exposition, 2003.
[12] P. Indyk. Stable distributions, pseudorandom generators,
embeddings and data stream computation. In FOCS, 2000.
[13] P. Indyk. Stable distributions, pseudorandom generators,
embeddings, and data stream computation. J. ACM,
53(3):307–323, 2006.
[14] A. Kuzmanovic and E. W. Knightly. Low-rate tcp targeted
denial of service attacks (the shrew vs. the mice and
elephants). In SIGCOMM, 2003.
[15] A. Lakhina, M. Crovella, and C. Diot. Mining anomalies
using traﬃc feature distributions. In SIGCOMM, 2005.
[16] A. Lall, V. Sekar, M. Ogihara, J. Xu, and H. Zhang. Data
streaming algorithms for estimating entropy of network
traﬃc. In SIGMETRICS, 2006.
[17] G. S. Manku and R. Motwani. Approximate frequency
counts over data streams. In Proceedings of the 28th
International Conference on Very Large Data Bases, 2002.
[18] A. Medina, N. Taft, K. Salamatian, S. Bhattacharyya, and
C. Diot. Traﬃc matrix estimation:existing techniques and
new directions. In SIGCOMM, Aug. 2002.
[19] S. Muthukrishnan. Data streams: algorithms and
[20] J. Nolan. STABLE program. online at
applications. available at
http://athos.rutgers.edu/∼muthu/.
http://academic2.american.edu/∼jpnolan/stable/stable.html.
[21] A. Soule, A. Nucci, R. Cruz, E. Leonardi, and N. Taft. How
to identify and estimate the largest traﬃc matrix elements
in a dynamic environment. In SIGMETRICS, June 2004.
[22] C. Tebaldi and M. West. Bayesian inference on network
traﬃc using link count data. Journal of American Statistics
Association, pages 557–576, 1998.
[23] Y. Vardi. Internet tomography: estimating
source-destination traﬃc intensities from link data. Journal
of American Statistics Association, pages 365–377, 1996.
[24] A. Wagner and B. Plattner. Entropy Based Worm and
Anomaly Detection in Fast IP Networks. In Proceedings of
IEEE International Workshop on Enabling Technologies,
Infrastructures for Collaborative Enterprises, 2005.
[25] K. Xu, Z.-L. Zhang, and S. Bhattacharya. Proﬁling internet
backbone traﬃc: Behavior models and applications. In
SIGCOMM, 2005.
[26] Y. Zhang, Z. M. Mao, and J. Wang. Low-rate tcp-targeted
dos attack disrupts internet routing. In Proc. 14th Annual
Network & Distributed System Security Symposium, 2007.
[27] Q. Zhao, Z. Ge, J. Wang, and J. Xu. Robust traﬃc matrix
estimation with imperfect information: making use of
multiple data sources. In SIGMETRICS, 2006.
[28] Q. Zhao, A. Kumar, J. Wang, and J. Xu. Data streaming
algorithms for accurate and eﬃcient measurement of traﬃc
and ﬂow matrices. In SIGMETRICS, June 2005.
[29] V. M. Zolotarev. One-Dimensional Stable Distributions,
volume 65 of Translations of Mathematical Monographs.
American Mathematical Society, Providence, RI, 1986.
APPENDIX
A. PROOFS
A.1 Proof of Theorem 2
Lemma 9. Let f = fS+(p), m = DM edp. Then estimator
(1) is asymptotically normal with mean ||S||p and standard
deviation
||S||p.
√
1
2mf (m)
l
√
Proof. Let Xi = Yi/||S||p, then Xi are i.i.d. samples
from distribution S(p), so |Xi| are i.i.d. samples from dis-
tribution S+(p). Using the asymptotic normality of the me-
l(median(|X1|, . . . ,|Xl|) − m) is
dian stated in [4, p. 483],
√
asymptotically normal with mean 0 and standard deviation
l and multiplying by ||S||p give us
2f (m) . Dividing by m
that, ||S||pmedian(|X1|, . . . ,|Xl|)/m − ||S||p
= median(|Y1|, . . . ,|Yl|)/m−||S||p = Λ((cid:6)Y )−||S||p, is asymp-
totically normal with mean 0 and standard deviation
1
1
√
2mf (m)
l
||S||p.
Proof of Theorem 2. From Lemma 9,
Z| < ] = P r[|Z| < zδ/2] = 1 − δ.
P r[|Λ((cid:6)Y )/||S||p − 1| < ] ≈ P r[|
= P r[|
2mf (m)
√
1

l
Z| < ]
zδ/2
A.2 Proofs of Propositions 4 and 5
Lemma 10. xα/ ln x is a decreasing function on (1, N] if
α < 1/ ln N . In fact, if α < 0.085, then xα/ ln x < 1 on
[3,N].
Proof. The derivative is negative on (1,N]. 3α < ln 3 for
α < 0.085.
Lemma 11. If a approximates b within relative error bound
, then a1+α approximates b1+α roughly within relative error
bound  for small α and . Similarly for 1 − α.
i
i
Proof. 1−  < a/b < 1 + , so (1− )1+α < a1+α/b1+α <
(1+)1+α. Using Taylor expansion, (1+ )1+α = 1++α+
O(2) ≈ 1 +  for small α and . Similarly (1− )1+α ≈ 1− .
Same for 1 − α.
From Lemma 10 , a1+α
ai ln ai, i.e. ||S||1+α
i − a1−α
) −
1+α−||S||1−α
1−α)−
P
Proof of Proposition 4. We know |c(a1+α
ai ln ai| ≤ 0ai ln ai, sum over i gives |c(||S||1+α
||S||H| ≤ 0||S||H . So λ0 ≤ 1.
< ai ln ai except for a few small
P
numbers. The big numbers should dominate the small num-
a1+α
bers, so we argue that for a typical ﬂow distribution,
1+α/||S||H , so
1+α < ||S||H . Let λ = ||S||1+α
i
λ < 1. Also ||S||1−α
For simplicity of notation, from now on we will use y to
denote Λ((cid:6)Y ) and z to denote Λ( (cid:6)Z).
We know |y − ||S||1+α| < ||S||1+α with probability 1 − δ,
so from Lemma 11, roughly |y1+α − ||S||1+α
1+α| < ||S||1+α
1+α =
1−α| <
λ||a||H with probability 1− δ. Similarly |z1−α −||S||1−α
λ||a||H with probability 1 − δ. So both inequalities will be
true with probability at least 1− 2δ. When that is the case,
|c(y1+α − z1−α) − ||S||H|
< |c(y1+α − z1−α) − c(||S||1+α
+ |c(||S||1+α
1+α − ||S||1−α
< c|y1+α − ||S||1+α
1+α − ||S||1−α
+ |c(||S||1+α
< (2cλ + λ00)||S||H .
1+α − ||S||1−α
1−α)|
1+α| + c|z1−α − ||S||1−α
1−α|
1−α) − ||S||H|
1−α) − ||S||H|
1−α < ||S||1+α
1+α = λ||S||H .
<
Proof of Proposition 5. From Lemma 9, the error in
using Λ((cid:6)Y ) to estimate ||S||1+α is roughly Gaussian with
√
l)||S||1+α (here we
mean 0 and standard deviation (π/2
used value of mf (m) at p = 1). We assume the error in
using Λ((cid:6)Y )1+α to estimate ||S||1+α
√
1+α is still roughly Gaus-
l)||S||1+α
sian with mean 0 and standard deviation (π/2
1+α.
(This is in the same spirit as (1 + )1+α ≈ 1 + .) Similarly
we assume the error in using Λ( (cid:6)Z)1−α to estimate ||S||1−α
1−α
√
is roughly Gaussian with mean 0 and standard deviation
l)||S||1+α
(π/2
1+α.
2 times the original Gaus-
Adding the two Gaussians gives
2 under the same
sian. Therefore the error is multiplied by
probability 1 − δ.
√
1−α, which we will enlarge to (π/2
l)||S||1−α
√
√