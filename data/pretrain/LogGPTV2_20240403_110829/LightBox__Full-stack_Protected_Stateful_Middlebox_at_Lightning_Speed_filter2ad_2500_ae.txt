for packet I/O, so its main packet loop can be directly replaced
0.511.522.5020406080100Speedup	Cache	miss	rate	(%)00.10.20.30.40.5020406080100Miss	ratePacket	ID	(X	1M)8K16K32K64KFigure 10: Performance of etap against
varied batch size.
Figure 11: Performance of etap against
varied ring size.
Figure 12: CPU usage of etap against
middlebox throughput.
Figure 13: Performance of etap on real trace.
with the compatibility layer we built on etap (Section 3.6). We also
adapt its own (cid:131)ow tracking logic to LightBox’s state management
procedures without altering the original functionality. (cid:140)is a(cid:130)ects
about 200 lines of code (LoC) in the original PRADS project with
10K LoC.
lwIDS. Based on the tcp reassembly library libntoh [28], we
built a lightweight IDS that can identify malicious pa(cid:138)erns over
reassembled data. Whenever the stream bu(cid:130)er is full or the (cid:131)ow
is completed, the bu(cid:130)ered content will be (cid:131)ushed and inspected
against a set of pa(cid:138)erns. Note that the packet I/O and main stream
reassembly logic of lwIDS is handled by libntoh (3.8K LoC), which
we have already ported on top of etap (Section 3.6). (cid:140)e e(cid:130)ort of
instantiating LightBox for lwIDS thus reduces to adjusting the state
management module of libntoh, which amounts to a change of
around 100 LoC.
mIDS. We design a more comprehensive middlebox, called mIDS,
based on the mOS framework [45] and the pa(cid:138)ern matching en-
gine DFC [14]. Similar to lwIDS, mIDS will (cid:131)ush stream bu(cid:130)ers
for inspection upon over(cid:131)ow and (cid:131)ow completion; but to avoid
consistent failure, it will also do the (cid:131)ushing and inspection when
receiving out-of-order packets, as we found that the logic for han-
dling such case is yet to be completed in current mOS code. Again,
since we have ported mOS (26K LoC) with etap (Section 3.6), the
remaining e(cid:130)ort of instantiating LightBox for mIDS is to modify
the state management logic, resulting in 450 LoC change. Note that
such e(cid:130)ort is one-time: herea(cid:137)er, we can instantiate any middlebox
built with mOS without change.
6 EVALUATION
6.1 Methodology and Setup
Our evaluation comprises two main parts: in-enclave packet I/O,
where we evaluate etap from various aspects and decide the practi-
cally optimal con(cid:128)gurations (Section 6.2); middlebox performance,
where we measure the e(cid:129)ciency of LightBox against a native and
a strawman approach for the three case-study middleboxes (Sec-
tion 6.3). We will also give discussions on experimental comparison
between LightBox and previous systems (Section 6.4). We use
a real SGX-enabled workstation with Intel E3-1505 v5 CPU and
16GB memory in the experiments. Equipped with 1Gbps NIC, the
workstation is unfortunately incapable of re(cid:131)ecting etap’s real per-
formance, so we prepare two experiment setups. In what follows,
we will use K for thousand and M for million in the units.
Setup 1. (cid:140)e (cid:128)rst setup is dedicated for evaluation on etap, where
we run etap-cli and etap on the same standalone machine and
let them communicate with the fast memory channel via kernel
networking. Note that etap-cli needs no SGX support and runs
as a normal user-land program. To reduce the side e(cid:130)ect of running
them on the same machine, we tame the kernel networking bu(cid:130)ers
such that they are kept small (500KB) but still performant. Our
intent here is to demonstrate that etap can catch up with the rate
of a real 10Gbps NICs in practical se(cid:138)ings.
Setup 2. Deployed in a local 1Gbps LAN, the second setup is for
evaluating middlebox performance. We use a separate machine as
the gateway to run etap-cli, so it communicates with etap via
the real link. (cid:140)e gateway machine also serves as the server to
accept connections from clients (on other machines in the LAN).
We then use tcpkali [71] to generate concurrent TCP connections
transmi(cid:138)ing random payloads from clients to the server; all ACK
packets from the server to clients are (cid:128)ltered out. Our environment
can a(cid:130)ord up to 600K concurrent connections. We also obtain a real
trace from CAIDA [9] for experiments; it is collected by monitors
deployed at backbone networks. (cid:140)e trace is sanitized and contains
only anonymized L3/L4 headers, so we pad them with random
payloads to their original lengths speci(cid:128)ed in the header. We use
the (cid:128)rst 100M packets from the trace in our experiments.
6.2 In-enclave Packet I/O Performance
To evaluate etap, we create a bare middlebox which keeps reading
packets from etap without further processing. It is referred to as
PktReader. We keep a large memory pool (8GB) and feed packets
to etap-cli directly from the pool.
Parameterized evaluation. We (cid:128)rst investigate how batching
size a(cid:130)ects etap performance. (cid:140)e ring size is set as 1024. As shown
in Fig. 10, the optimal size appears between 10 and 100 for all packet
sizes. (cid:140)e throughput drops when the batching size becomes either
too small or overly large, matching our expectation and analysis
in Section 3.5. With a batching size of 10, etap can deliver small
5.97.47.98.89.07.48.610.811.612.4​​​​0481216641282565121024Throughput	(Gbps)Packet	size	(Byte)X	1X	10X	100X	1000681012141602004006008001000Throughput	(Gbps)Ring	size	of	etap64B128B256B512B1024B50607080901000123456789101112CPU	usage	(%)Throughput	(Gbps)64B128B256B512B1024B1.522.53111213140102030405060708090100MppsGbpsPacket	ID	(x	1M)Throughput	in	GbpsThroughput	in	Mpps(a) 64B packet.
(b) 512B packet.
(c) 1500B packet.
Figure 14: Performance of PRADS under controlled settings.
Figure 16: PRADS on real trace.
(a) 64B packet.
(b) 512B packet.
(c) 1500B packet.
Figure 15: Performance of lwIDS under controlled settings.
Figure 17: lwIDS on real trace.
64B (byte) packet at 7.4Gbps, and large 1024B packet at 12.4Gbps,
which is comparable to advanced packet I/O framework on modern
10Gbps NIC [69]. We set 10 as the default batching size and use
this con(cid:128)guration in all following experiments.
Shrinking etap ring is bene(cid:128)cial in that precious enclave re-
sources can be saved for middlebox functions, and in the case of
multi-threaded middleboxes, for e(cid:129)ciently supporting more RX
rings. However, smaller ring size generally leads to lower I/O
throughput. Figure 11 reports the results with varying ring sizes.
As can be seen, the tipping point occurs around 256, where the
throughput for all packet sizes begins to drop sharply as ring size
decreases. Beyond that and up to 1024, the performance appears
insensitive to ring size. We thus use 256 as the default ring size in
all subsequent tests.
Resource consumption. (cid:140)e rings contribute to the major etap
enclave memory consumption. One ring uses as small as 0.38MB as
per the default con(cid:128)guration, and a working etap consumes merely
0.76MB. (cid:140)e core driver of etap is run by dedicated threads and we
are also interested in its CPU consumption. (cid:140)e driver will spin in
the enclave if the rings are not available, since exiting enclave and
sleeping outside is too costly. (cid:140)is implies that a slower middlebox
thread will force the core driver to waste more CPU cycles in the
enclave. To verify such e(cid:130)ect, we tune PkgReader with di(cid:130)erent
levels of complexity, and estimate the core driver’s CPU usage
under varying middlebox speed. As expected, the results in Fig. 12
delineate a clear negative correlation between the CPU usage of
etap and the performance of middlebox itself. With 70% utilization
of a single core the core driver can handle packets at its full speed.
Overall, we can see that an average commodity processor is more
than enough for our target 10Gpbs in-enclave packet I/O.
Performance on real trace. Figure 13 shows etap’s performance
on the real CAIDA trace that has a mean packet size of 680B. We
estimate the throughput for every 1M packets while replaying the
trace to etap-cli. As shown, although there are small (cid:131)uctuations
overtime due to varying packet size, the throughput remains mostly
within 11 − 12Gbps and 2 − 2.5Mpps. (cid:140)is further demonstrates
etap’s practical I/O performance.
6.3 Middlebox Performance
We study the performance of the three middleboxes, each with
three variants: the vanilla version (denoted as Native) running as
a normal program; naive SGX port (denoted as Strawman) that
uses etap and our ported libntoh and mOS for networking, but
relies on EPC paging for however much enclave memory is needed;
the LightBox instance as described in Section 5. It is worth noting
that despite the name, the Strawman variants actually bene(cid:128)t a lot
from etap’s e(cid:129)ciency. Our goal here is primarily to investigate the
e(cid:129)ciency of our state management design.
We use the default con(cid:128)gurations for all three middleboxes un-
less otherwise speci(cid:128)ed. For lwIDS we compile 10 pcre engines
with random pa(cid:138)erns for inspection; for mIDS we build the DFC
engine with 3700 pa(cid:138)erns extracted from Snort community ruleset.
(cid:140)e (cid:131)ow state of PRADS, lwIDS, and mIDS has a size of 512B2,
5.5KB, and 11.4KB3, respectively; the la(cid:138)er two include stream
reassembly bu(cid:130)er of size 4KB and 8KB. For LightBox variants, the
2PRADS has 124B (cid:131)ow state, which is too small under our current experiment se(cid:138)ings.
To be(cid:138)er approximate realistic scenarios, we pad the (cid:131)ow state of PRADS to 512B
with random bytes. No such padding is applied to lwIDS and mIDS.
3(cid:140)is size is resulted from the rearrangement of mOS’s data structures pertaining to
(cid:131)ow state. We merge all data structures into a single one to ease memory management.
020406080100120123456Pkt. delay (us)#Flows (100K)NativeStrawmanLightBox020406080123456Pkt. delay (us)#Flows (100K)NativeStrawmanLightBox0102030405060123456Pkt. delay (us)#Flows (100K)NativeStrawmanLightBox04080120160200123456Pkt. delay (us)#Flows (100K)NativeStrawmanLightBox04080120160200123456Pkt. delay (us)#Flows (100K)NativeStrawmanLightBox04080120160200123456Pkt. delay (us)#Flows (100K)NativeStrawmanLightBox080160240320Pktdelay (us)NativeStrawmanLightBox0800160024000102030405060708090100#Flows (K)Replay timeline (per 1M packets)020406080Pktdelay (us)NativeStrawmanLightBox040080012000102030405060708090100#Flows (K)Replay timeline (per 1M packets)(a) 64B packet.
(b) 512B packet.
(c) 1500B packet.
Figure 18: Performance of mIDS under controlled settings.
Figure 19: mIDS on real trace.
number of entries of flow cache is (cid:128)xed to 32K, 8K and 4K for
PRADS, lwIDS, and mIDS, respectively.
6.3.1 Controlled live tra(cid:128)ic. To gain a be(cid:138)er understanding of
how stateful middleboxes behave in the highly constrained enclave
space, we test them in controlled se(cid:138)ings with varying number of
concurrent TCP connections between clients and the server. We
control the clients’ tra(cid:129)c generation load such that the aggregated
tra(cid:129)c rate at the server side remains roughly the same for di(cid:130)erent
degrees of concurrency. By doing so the comparisons are made fair
and meaningful. In addition, we start to collect data points only
when all connections are established and stabilized. We measure
the mean packet processing delay in microsecond (µs) every 1M
packets, and each reported data point is averaged over 100 runs.
PRADS. From Fig. 14, we can see that LightBox adds negligible
overhead (< 1µs) to native processing of PRADS regardless of the
number of (cid:131)ows. In contrast, Strawman incurs signi(cid:128)cant and
increasing overhead a(cid:137)er 200K (cid:131)ows, due to the involvement of
EPC paging. Interestingly, by comparing the sub(cid:128)gures it can also
be seen that Strawman performs worse for smaller packets. (cid:140)is is
because smaller packet leads to higher packet rate while saturating
the link, which in turn implies higher page fault ratio. For 600K
(cid:131)ows, LightBox a(cid:138)ains 3.5× — 30× speedup over the Strawman.
lwIDS. Figure 15 presents similar results for lwIDS. Here, the
performance of Strawman is further degraded, since lwIDS has
larger (cid:131)ow state size than PRADS and its memory footprint ex-
ceeds 550MB even when tracking only 100K (cid:131)ows. For 64B packet,
LightBox introduces 6− 8µs packet delay (4− 5× to native) because
the state management dominates the whole processing; nonethe-
less, it still outperforms Strawman by 5 − 16×. For larger packets,
the network function itself becomes dominant and the overhead of
LightBox over Native is reduced, as shown in Fig. 15 (b) and (c).
mIDS. Among the case-study middleboxes, mIDS is the most com-
plicated one with the largest (cid:131)ow state. Here, our testbeds can
scale to 300K concurrent connections. For each connection mIDS
will track two (cid:131)ows, one for a direction, and allocate memory ac-
cordingly. But since we (cid:128)lter out the trivial ACK packets from the
server to clients, we still count only one (cid:131)ow per connection. Fig-
ure 18 reveals that the performance of mIDS’s three variants follows
similar trends as in previous middleboxes: Native and LightBox
are insensitive to the number of concurrent (cid:131)ows; conversely, the
overhead of Strawman grows as more (cid:131)ows are tracked.
But in contrast to previous cases, now the overhead of LightBox
over Native becomes notable. (cid:140)is is explained by mIDS’s large
(cid:131)ow state size, i.e., 11.4 KB, which leads to the substantial cost of
encrypting/decrypting and copying states. Besides, we found that
for each packet, in addition to its own (cid:131)ow, mIDS will also access
the paired (cid:131)ow, doubling the cost of our (cid:131)ow tracking design (see
Section 4.2). Nonetheless, we can see that the gap is closing towards
larger packet size, as the network function processing itself weighs
in. Later in this section we will discuss how to further improve our
design to cope with large (cid:131)ow state and connection-based tracking.
6.3.2 Real trace. Now we investigate middlebox performance
with respect to the real CAIDA trace. (cid:140)e trace is loaded by the
gateway and replayed to the middlebox for processing. Again, we
collect the data points every 1M packets. Packets of unsupported
types are (cid:128)ltered out so only 97 data points are collected for each
case. Since L2 headers are stripped in the CAIDA trace, we also
adjust the packet parsing logic accordingly for the middleboxes. Yet
another important factor for real trace is the (cid:131)ow timeout se(cid:138)ing.
We must carefully set the timeout so inactive (cid:131)ows are purged well
in time, lest excessive (cid:131)ows overwhelm the testbeds. Here, we set
the timeout for PRADS, lwIDS, and mIDS, to 60, 30, and 15 seconds,
respectively. Table 2 reports the overall throughout of relaying the
trace. Below we give more detailed analysis.
PRADS. Figure 16 shows that the packet delay of Strawman grows
with the number of (cid:131)ows; it needs about 240µs to process a packet
when there are 1.6M (cid:131)ows. In comparison, LightBox maintains low
and stable delay (around 6µs) throughout the test. A bit surprisingly,
it even edges over the native processing as more (cid:131)ows are tracked,
a(cid:138)ributed to an ine(cid:129)cient chained hashing design used in the native
implementation. (cid:140)is highlights the importance of e(cid:129)cient (cid:131)ow
lookup in stateful middleboxes.
lwIDS. Compared with PRADS, the number of concurrent (cid:131)ows
tracked by lwIDS decreases, as shown in Fig. 17. (cid:140)is is due to
the halved timeout and the more aggressive strategy we used for
(cid:131)ow deletion: we remove a (cid:131)ow when a FIN or RST (cid:131)ag is received,
and we do not handle TIME WAIT event. It can be seen that with
fewer (cid:131)ows, Strawman still incurs remarkable overhead, while the
di(cid:130)erence between LightBox and Native is indistinguishable.
mIDS. (cid:140)e case for mIDS is tricky. Its current implementation of
(cid:131)ow timeout seems not to be fully working, so we replaced the
related code with the logic of checking all (cid:131)ows for expiration every
02040608051015202530Pkt. delay (us)#Flows (10K)NativeStrawmanLightBox02040608051015202530Pkt. delay (us)#Flows (10K)NativeStrawmanLightBox02040608051015202530Pkt. delay (us)#Flows (10K)NativeStrawmanLightBox02004006008000102030405060708090100#Flows (K)Replay timeline (per 1M packets)0204060Pktdelay (us)NativeLightBoxStrawmanTable 2: (cid:135)roughput (Mbps) under CAIDA trace.
Native
429.24
689.11
713.56
Strawman
67.399
182.57
161.02
LightBox
928.06
685.36
310.42
PRADS
lwIDS
mIDS
timeout interval. We also made some modi(cid:128)cations to ensure that
the packet formats and abnormal packets in the real trace can be
properly processed. Figure 19 reports the test results. (cid:140)ere is again
a large gap between Strawman and Native. Yet, as in the controlled