the investigation even though their components were not directly
responsible. In 6 such incidents, operators explicitly acknowledge
the involvement of multiple entities, and in at least one (Nov. 11,
2018), a neighboring ISP was responsible. In practice, the set of
involved entities is much higher than the reports indicate. In any
case, when problems occur, their impact can often be observed
across the DC; many teams are potential suspects 4.
# of Teams
Respondents
# of Users
Respondents
10‚Äì20
20‚Äì100
100‚Äì1000 >1000
1‚Äì10
14
1m
4
1
4
1
5
1
3
8
11
Table 3: Characteristics of the networks operated by our sur-
vey respondents.
Survey of network operators. Direct surveys of practicing net-
work operators re-affirm the above results. In total, 27 operators
responded to our survey; Table 3 shows some of the characteris-
tics of the respondents. 9 of them were ISP operators, another 10
self-identified as an enterprise, 5 were DC operators, 1 managed
a content delivery network, one a nationwide security provider,
and one classified their network as falling in all these categories.
Many of the respondents reported a small number of teams in their
organization (1‚Äì10), but one reported upwards of 1,000 teams. The
networks operated by respondents served a wide array of user base
sizes, with some handling less than 1,000 users, and 4 handling over
a million.
When asked how much these operators thought incident routing
impacts their organization, on a scale of 1‚Äì5 with 5 as the highest,
23 selected a score ‚â•3, out of which 17 selected a score ‚â•4. 17
marked the significance of this problem as ‚â•4. 17 of the 27 opera-
tors answered that their network was incorrectly blamed for over
60% of incidents across their systems. We also asked the converse:
‚ÄúWhen an incident occurs, how often other components (not the
network) are initially blamed even though the incident was caused
by a networking issue?‚Äù 20 operators said that other teams were
implicated less than 20% of the time.
Operators listed the reasons why incident routing hard was hard
for them: (1) Operators find it difficult to identify the boundary of
each team‚Äôs responsibilities especially in larger organizations; (2)
Tickets don‚Äôt contain the information necessary to enable effective
incident routing; (3) Operators lack the ability to understand the
entire complex system to direct incident to the right team;
(4)
4The work of [33] provides an in-depth study of these incidents in Google cloud.
SIGCOMM ‚Äô20, August 10‚Äì14, 2020, Virtual Event, NY, USA
Operators need access to monitoring data, both historical and real-
time, to demonstrate that networking is not the issue ‚Äî this data is
often not available especially when problems occur intermittently.
Lastly, 14 out of the 27 operators said that for typical investiga-
tions, more than 3 teams are involved when incidents occur; 19 said
this number was ‚â• 2.
We note the absolute numbers are subject to significant bias
from the respondents, but qualitatively, they demonstrate the effect
of improper routing on the lives of operators.
B EXTENDED EVALUATION
The choice of supervised learning model. We use RFs for super-
vised learning as they are explain-able. We also experimented with
other choices in order to understand the tradeoff of explain-ability
vs accuracy (Table ¬ß4).
Evaluating the Model Selector. We could use unsupervised mod-
els such as OneClassSVM, boosting (e.g., Adaboost), or reinforce-
ment learning instead of our bag of words based RF (bag of words)
in the model selector. We evaluate these choices here (we defer re-
inforcement learning to future work as it requires careful selection
of the reward).
With frequent (every 10 days) retraining all model‚Äôs are compara-
ble (Figure ¬ß8-a). With a lower retraining frequency (every 60 days)
the difference between these models becomes apparent (Figure 8-b).
OneClassSVM (with an aggressive kernel) is better in such cases as
it switches to using CPD+ more often. Choosing the right kernel
is critical for OneClassSVM. A conservative kernel (Polynomial)
which would favor classifying most incidents as ‚Äúold‚Äù cannot adapt
to the longer re-training interval while an aggressive kernel (radias
basis kernel) will choose to classify many samples as ‚Äúnew‚Äù and
uses CPD+ in those cases.
Given the cheap cost of re-training, we recommend frequent
retraining no matter which model is used. Due to its explainability,
we opted for the RF in deployment.
Understanding what makes the Scout work. We next investi-
gate in more detail how the Scout is able to achieve its high accuracy.
The differences across classes: We first look at how ‚Äúseparable‚Äù the
two classes (PhyNet‚Äôs responsibility vs not PhyNet‚Äôs responsibility)
are. Specifically, we look at the Euclidean distance (computed over
the feature vectors) between incidents that are PhyNet‚Äôs responsibil-
ity; between incidents that are not PhyNet‚Äôs responsibility, and the
cross distance between incidents in these two classes. Even though
the distribution of each class is not separable individually, we see a
clear separation in the cross distance (Figure 13). We compute the
same metric for the features of each component type (Figure 14):
the results seem to indicate server-related features should not have
much predictive power. However, a more detailed deflation study
(Table 5) shows these features still marginally contribute to the
overall accuracy of the Scout. We also see some components have
more contribution toward recall (e.g., Server features) while others
contribute to precision (e.g., switch features).
Feature analysis: To further evaluate the impact of the features for
each component type we: (a) remove the corresponding features;
and (b) use only the corresponding features (Table 5). We see al-
though server-related features have the least contribution to overall
accuracy all components contribute to the overall high F-1 Score.
SIGCOMM ‚Äô20, August 10‚Äì14, 2020, Virtual Event, NY, USA
Gao et al.
More sophisticated algorithms can predict the team ‚Äúmost likely‚Äù
to be responsible (the MLE estimate [54]) for an incident given the
historic accuracy of each Scout and its output confidence score. The
design of an optimal Scout Master is beyond the scope of this work,
but we show an evaluation of the strawman in Appendix D.
Figure 14: The Euclidean distance (computed over feature
vectors) using (a) just the server features, (b) just the switch
features, and (c) just the cluster features.
Features used
Server Only
Switch Only
Cluster Only
Without Cluster
Without Switches
Without Server
all
Precision Recall F1 Score
59.5 %
97.1%
93.4%
97.4%
87.5%
97.3%
97.5%
97.2 %
93.1%
95.7%
94.5%
94.0%
94.7%
97.7%
0.73
0.95
0.94
0.95
0.90
0.96
0.98
Table 5: Deflation Study: investigating the utility of each
component type‚Äôs features.
D EVALUATING THE SCOUT MASTER
When evaluating the Scout Master we find:
Figure 15: The amount of investigation time we can reduce
for mis-routed incidents by adding more Scouts.
The gains can be significant even if only a handful of teams
deployed Scouts: We first assume teams can build Scouts with
100% accuracy ‚Äî we will relax this assumption in the next set of
experiments. Once again, we only focus on mis-routed incidents. We
simulate the impact of ùëõ teams having Scouts (where we experiment
with all possible assignments of Scouts to teams) and using the
actual incidents logs from a large cloud provider evaluate their
benefit: even if only a small number of teams were to adopt Scouts
the gains could be significant ‚Äî with only a single Scout we can
Figure 13: The Euclidean distance (computed over feature
vectors) between: (a) incidents that are PhyNet‚Äôs responsi-
bility; (b) incidents that are not PhyNet‚Äôs responsibility; and
(c) the cross distance between incidents in (a) and (b).
Other teams can also build Scouts. We have not yet built Scouts
for other teams because it would require accumulating enough
data for training (see ¬ß8) ‚Äî we are working with our teams to
increase the retention period for their monitoring data to do so.
But in this section we will demonstrate how other teams can build
Scouts by reporting the accuracy of a rule-based system built by our
Storage team. This system is used to automatically analyze incidents
generated by our monitoring systems (those of storage itself and
those belonging to other teams), check the monitoring data the
storage team collects, and determine whether a storage engineer
should be involved in the investigations (the system does not trigger
on CRIs). We find it has precision: 76.15% and recall of 99.5%. Our
storage team manages a large and complex distributed system and
has dependency on many networking teams: e.g., our software load
balancing team and PhyNet. Our evaluation indicates it is possible
to build an accurate storage Scout through more sophisticated
(possibly ML based) techniques ‚Äî given the relatively high accuracy
the rule-based system already achieves.
C SCOUT MASTER DESIGN
Coordinating the Scouts is a ‚ÄúScout Master‚Äù which represents a
global incident routing process that queries all available Scouts
in parallel to route incidents. This process can be the operators
existing, manual, process (where Scout‚Äôs act as recommendation
systems) or a more sophisticated incident routing algorithm. For
example, a strawman where: if only one Scout returns a ‚Äúyes‚Äù an-
swer with high confidence (1), sends the incident to the team that
owns the Scout; when multiple Scouts return a positive answer (2),
if one team‚Äôs component depends on the other, sends the incident
to the latter, if not sends it to the team whose Scout had the most
confidence; and if none of the Scouts return a positive answer (3),
Algorithm
KNN [8]
Neural Network (1 layer) [9]
Adaboost [1]
Gaussian Naive Bayes [5]
Quadratic Discriminant Analysis [10]
F1-score
0.95
0.93
0.96
0.73
0.9
Table 4: Comparing RFs to other ML models.
falls back to the existing, non-Scout-based, incident routing system.
(c)(a)(b)0204060801000.00.20.40.60.81.01 Scout2 Scouts3 Scouts4 Scouts5 Scouts6 ScoutsBestpossibleFraction of investigation timeCDFreduce the investigation time of 20% of incidents and with 6 we can
reduce the investigation time of over 40% (Figure 15). With enough
Scouts ‚Äî or by better assignment of Scouts to teams ‚Äî gains can
be as high as 80% (Figure 15).
Even Scouts with imperfect accuracy are beneficial. We next
simulate Scouts of imperfect accuracy. To do so, we assign an ac-
curacy ùëÉ to each Scout which we pick uniformly at random from
the interval (ùõº, ùõº + 5%): each time a Scout is used, with probability
ùëÉ it will correctly identify whether its team is responsible. If the
Scout is correct, we assign it a confidence ùê∂ chosen uniformly, at
random in the interval (0.8 ‚àí ùõΩ, 0.8) and if it is incorrect we as-
sign it a confidence from the interval (0.5, 0.5 + ùõΩ). Again, we look
at all possible Scout assignments and use actual incident reports
from a large cloud to evaluate the benefit of these Scouts. As the
impact of mis-routing is much less pronounced for some teams
compared to others (compare the gains presented here for a sin-
gle Scout with those of the PhyNet Scout), our results represent
SIGCOMM ‚Äô20, August 10‚Äì14, 2020, Virtual Event, NY, USA
a lower-bound on the actual benefits of Scouts. Despite this, even
with three Scouts, the Scout Master can reduce investigation time
by up to 80% (Figure 16).
Figure 16: The lower bounds on gain when adding Scouts
with imperfect accuracy. By strategically assigning Scouts
to teams that are impacted more heavily by mis-routing we
can achieve much higher gains.
0.00.10.20.30.40.50.700.750.800.850.900.951.00Single Scout0.00.20.40.61.00.00.10.20.30.40.5Two Scouts0.80.700.750.800.850.900.951.000.00.20.40.60.81.00.00.10.20.30.40.50.700.750.800.850.900.951.000.00.20.40.60.81.0Three ScoutsFraction of investigationtime reducedAverage95th percentile