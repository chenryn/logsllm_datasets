switches in a uniform manner. Yet flat-tree maintains struc-
tures, e.g. the Clos connections between edge and aggregation
switches, core switches connected to the Pods, though using
customized wiring patterns, and the neighbor-to-neighbor
wiring restricted to adjacent Pods. The path length of switch
pairs is not uniform for flat-tree, so we should place servers
intelligently to leverage the shorter paths in the network.
Recall that 6-port converter switches can relocate servers
to core switches, and 4-port ones can relocate servers to
aggregation switches, so the server distribution is determined
by the choice of ùëö and ùëõ. Because flat-tree aims at converting
md/2#m x#d/2#server#connectors# m x#d/2#core#connectors# nd/2#n x#d/2#server#connectors# n x#d/2#core#connectors# Blade#A#Blade#B#core#connector#server#connector#i, jEjAj/rdouble#side#connectors#core#connector#server#connector#i, jEj####Aj/rd#edge#switches#E0#to#Ed91###d/r#aggrega;on#switches#A0#to#Ad/r91###core#connector#server#connector#i, jEj+d/2   A(j+d/2)/rle<#blade#right#blade#core#connector#server#connector#i, jEj+d/2A(j+d/2)/rdouble#side#connectors#le<#blade#right#blade#Remaining#core#connectors#Remaining#server#connectors#Blade#B#Blade#A#Blade#A#Blade#B#SIGCOMM ‚Äô17, August 21-25, 2017, Los Angeles, CA, USA
Y. Xia et al.
Figure 4: Pod-core wiring for the same set of connectors across Pods. All connectors are on aggregation switches in Clos; flat-tree has 3
types of connectors on blade A, B, and aggregation switches, enabling core-server, core-edge, and core-aggregation connections respectively.
generic Clos networks, which may have very different layouts,
it is difficult to pre-define the ùëö and ùëõ values for optimal
transmission performance. We suggest a profiling scheme:
under the preferred Pod-core wiring pattern described in
Section 3.2, vary ùëö and ùëõ until they result in the shortest
average path length over all server pairs. The sensitivity test
for this approach is in our prior paper [47].
3.5 Operation Modes
Global: Flat-tree approximates a network-wide (or global)
random graph in the ‚Äúglobal‚Äù mode. 6-port converter switches
take either the ‚Äúside‚Äù or the ‚Äúcross‚Äù configuration (Figure 1
b3 or b4) depending on their row index in the matrix as
described in Section 3.3. 4-port converter switches take the
‚Äúlocal‚Äù configuration (Figure 1 a2).
Local: Flat-tree approximates a two-stage (or local) ran-
dom graph in the ‚Äúlocal‚Äù mode. It first forms random graphs
in each Pod and takes the Pods as super nodes to form another
layer of random graph together with core switches. 6-port
and 4-port converter switches take the ‚Äúlocal‚Äù configuration
(Figure 1 a2 and b2) to relocate half servers to aggregation
switches. Any remaining 6-port converter switches take the
‚Äúdefault‚Äù configuration (Figure 1 b1).
Clos: Flat-tree functions as a Clos network by default. All
converter switches take the ‚Äúdefault‚Äù configuration (Figure 1
a1 and b1).
Hybrid: Flat-tree can be configured in the unit of a Pod,
so it can have arbitrary combinations of the above three
topologies each in a number of Pods. The converter switch
configurations follow the rules in their corresponding mode.
3.6 Cost Analysis
Because of their simple functionality, converter switches in
flat-tree can be realized by passive circuit switches. The
choice of specific switching technology depends on the existing
devices already deployed in the data center. If the data center
has copper cables in place, crosspoint switches whose per-port
cost is as low as $3 [31] can be used. Converter switches split
some cables into two parts. Because crosspoint switches are
passive devices, cables connected to a converter switch do
not need active elements. If manufactured properly, the cost
of two cables each with only one active element at the packet
switch end is equivalent to the cost of the original cable.
Many data centers nowadays use optical fibers for cross-
rack connections. To avoid the cost of extra transceivers, opti-
cal circuit switches are sensible options for converter switches.
Because converter switches have small port count, we can use
low-cost switching technologies, such as 2D MEMS [46] and
Mach-Zehnder switches [20], whose port count is limited to
moderate scale due to losses from photonic signal crossings
or other effects. The mass production cost of these technolo-
gies is dominated by packaging. While we are not able to
project future costs precisely, we anticipate that the per-port
cost will become reasonably cheap as photonic packaging
technology advances. The difference between transmit power
and receive sensitivity of commercial optical transceivers can
be over 8dB [7], which easily overcomes the insertion loss of
most optical switches. Amplifiers are thus not needed.
4 CONTROL SYSTEM
Because a data center is administered by a single authority,
we follow the recent trend of using a centralized network con-
troller for global network management. Flat-tree has several
operation modes with pre-known topologies, which designate
a fixed set of configurations for the converter switches. The
controller changes the topology by configuring the converter
switches, via specific control mechanisms depending on the
realization technology. For instance, most optical switches
can be programmed via a software interface. The converter
switch configurations for different flat-tree modes can be
hard-coded into the controller.
For flat-tree Clos mode, we can use ECMP [28], two-level
routing [12], or customized SDN routing with pre-computed
paths [39]. The study on random graph network [41] sug-
gests using ùëò-shortest-path routing [50] and MultiPath TCP
(MPTCP) [45]. We adopt this approach for flat-tree global
mode and local mode, because they approximate random
graph and two-stage random graph respectively. However,
this prior study [41] failed to consider the overwhelmingly
large number of network states in a real implementation.
According to our experience with our testbed implementa-
tion, under the SDN paradigm, the number of Openflow
rules easily exceeds the capacity of commercial SDN switches
for a very small-scale random graph network. So, another
major scalability concern of random graph networks is from
the overhead of the control system, besides messy cable de-
ployment. Therefore, even deploying a static random graph
network requires a re-design of the control plane. In this
section, we propose a control system with a manageable num-
ber of network states for flat-tree, and this solution can be
easily applied to static random graph networks. Note that
the main contribution of this paper is the flat-tree network
architecture, and we acknowledge there may be alternative
control plane designs.
Ej!in!Pod!0!Ej!in!Pod!1!Ej!in!Pod!2!Ej!in!Pod!0!Ej!in!Pod!1!Ej!in!Pod!2!Ai!in!Pod!0!Ai!in!Pod!1!Ai!in!Pod!2!h core!switches!Cih!to!Cih+h-1 m blade!Bn blade!Ah/r-m-n aggreg.connectors!connectors!connectors!h/r core!switches!Cjh/r!to!Cjh/r+h/r-1 h!aggrega8on!connectors!m blade!Bn blade!Aconnectors!connectors!connectors!a:!Clos!Pod;Core!Wiring!Pa=ern!b:!Flat;tree!Pod;Core!Wiring!Pa=ern!1!c:!Flat;tree!Pod;Core!Wiring!Pa=ern!2!h/r-m-n aggreg.h/r core!switches!Cjh/r!to!Cjh/r+h/r-1 A Tale of Two Topologies
SIGCOMM ‚Äô17, August 21-25, 2017, Los Angeles, CA, USA
Figure 5: Illustration of the addressing scheme. ‚Äúa‚Äù shows the IP address fields in flat-tree. In the ‚Äúb‚Äù example, the server in strip connects
to switch #3, switch #8, and switch #5 respectively in the global, local, and Clos mode, where the number of concurrent paths, or ùëò, is
chosen to be 16, 8, and 4. The IP addresses assigned to this server are shown in ‚Äúc‚Äù. All these addresses for every flat-tree topology mode
are preconfigured on the server.
4.1 MPTCP
MPTCP has been standardized and widely used in academia
and industry [21]. The kernel implementation has been re-
leased [2]. MPTCP establishes subflows via multi-homing: the
end hosts using multiple IP addresses to distinguish paths. In
flat-tree, servers have one uplink only, so we must associate
multiple IP addresses to a single NIC. IP aliasing gives the
solution by setting multiple virtual network interfaces. These
virtual interfaces are linked to the physical interface by de-
fault, so traffic with different IP addresses can be forwarded
by the physical interface.
The full-mesh option in MPTCP allows subflows with dif-
ferent combinations of the source-destination IP address pairs.
For instance, with 2 IP addresses on both the sender and the
receiver, we obtain 2√ó2 = 4 subflows. Therefore, the number
of IP addresses per server is the square root of the number
of concurrent paths, or ùëò in ùëò-shortest-path routing. Not all
subflows are needed sometimes. For example, 8-shortest-path
routing requires 3 IP addresses per server, thus creating one
extra subflow. In such case, a straightforward workaround
is to limit the routing logic to the necessary subflows only,
and MPTCP will not allocate traffic to subflows with no
end-to-end reachability.
This simple way of assigning IP addresses defines a flat
address space, which may be inefficient considering the great
number of servers in a large data center. The property of
MPTCP to send traffic only with routable addresses gives
the freedom for more intelligent addressing mechanisms. Gen-
erally, address assignment depends on the structure of the
network and serves for the ease of routing. This task is par-
ticularly difficult for flat-tree, which has completely different
network structures and routing paths for each topology. We
propose a customized addressing scheme specific to the flat-
tree architecture in the next subsection.
4.2
ùëò-Shortest-Path Routing
In ùëò-shortest-path routing, there are ùëò routes for every source-
destination server pairs. A critical consequence of the enor-
mous number of paths is the explosion of the network states.
For efficient routing, every transit switch needs to be config-
ured with the forwarding rules of the ùëò paths for all server
pairs. Let ùëõ and ùëÅ be the number of servers and switches in
the data center and ùêø be the average path length, the average
number of network states per switch is ùëõ2√óùëò√óùêø
. For a large
ùëÅ
data center, this number can easily reach tens of million, far
exceeding the storage and processing capacity of switches.
ùëò-shortest-path routing requires matching both the source
and destination IP addresses, and traditional ways of aggre-
gation, such as destination IP lookup or prefix matching, do
not readily work. A switch may forward packets for the same
receiver to different ports, because they need to take different
routes. Servers can be relocated to different switches under
different flat-tree topology modes, making the definition of
common prefix very challenging. We need novel approaches
to factoring down the number of network states.
4.2.1 Addressing. We have two important observations
from the flat-tree architecture and from an extensive analysis
of the computed ùëò-shortest paths in the network.
Observation 1: A server is connected to one and only one
ingress/egress switch, regardless of the fact that it may be
relocated to a different ingress/egress switch as the topology
changes. So, there is no path diversion between servers and
the connected ingress/egress switches.
Observation 2: The number of equal-cost paths is small2
in the approximate random graph flat-tree creates. The ùëò-
shortest paths between server pairs are nearly deterministic,
with uncommon exception of ties. So, the ùëò-shortest paths
between ingress and egress switches almost capture the full
set of selected paths between source and destination servers.
Given these observations, it is promising to conduct prefix
matching on the ingress/edge switch level. This way, the
average number of network states per switch is reduced from
ùëõ2√óùëò√óùêø
, ùëÜ being the number of ingress/egress
switches. Usually 20 to 40 servers are connected to a top-of-
rack switch (ToR) in a data center, so the number of network
states can be reduced by a factor of 400 to 1600.
to ùëÜ2√óùëò√óùêø
ùëÅ
ùëÅ
As discussed previously, the major difficulty in flat-tree
is server mobility. To guarantee common prefix for servers
under the same ingress/egress switch, we need a different set
of IP addresses for each flat-tree topology mode. Because we
aim to change the network topology at run time by software,
2Having few equal-cost paths does not imply poor failure resiliency.
Like random graph networks, flat-tree can and should use paths of
different lengths for high throughput. It has been established that
throughput degrades more gracefully in random graph networks than
in fat-tree under failure [41]. Because flat-tree approximates random
graph networks, we expect flat-tree to be resilient to failure as well,
although more thorough evaluations are left to future work.
00001010#Switch#ID#Path#ID#Topology#ID#Server#ID#8#bits#(10.0.0.0/8)#13#bits#3#bits#2#bits#6#bits#SW#3#SW#8#SW#5#Switch#Server#Global#mode#link#Local#mode#link#Clos#mode#link#Topology'ID'Switch'ID'Server'ID''IP'addresses''0'(global)''3''2'00001010''0000000000011''000''00''000010''''(10.0.24.2)'00001010''0000000000011''001''00''000010''''(10.0.25.2)'00001010''0000000000011''010''00''000010''''(10.0.26.2)'00001010''0000000000011''011''00''000010''''(10.0.27.2)''1'(local)''8''1'00001010''0000000001000''000''01''000001''''(10.0.64.65)'00001010''0000000001000''001''01''000001''''(10.0.65.65)'00001010''0000000001000''010''01''000001''''(10.0.66.65)''2'(Clos)''5''0'00001010''0000000000101''000''10''000000''''(10.0.40.128)'00001010''0000000000101''001''10''000000''''(10.0.41.128)'a:#Ô¨Çat&tree#address#space#b:#example#of#IP#address#assignment###c:#list#of#IP#addresses#for#the#server#in#stripe#Topology'ID'Switch'ID'Server'ID'!k!'IP'addresses''0'(global)''3''2''16'00001010''0000000000011''000''00''000010''''(10.0.24.2)'00001010''0000000000011''001''00''000010''''(10.0.25.2)'00001010''0000000000011''010''00''000010''''(10.0.26.2)'00001010''0000000000011''011''00''000010''''(10.0.27.2)''1'(local)''8''1''8'00001010''0000000001000''000''01''000001''''(10.0.64.65)'00001010''0000000001000''001''01''000001''''(10.0.65.65)'00001010''0000000001000''010''01''000001''''(10.0.66.65)''2'(Clos)''5''0''4'00001010''0000000000101''000''10''000000''''(10.0.40.128)'00001010''0000000000101''001''10''000000''''(10.0.41.128)'c:'list'of'IP'addresses'for'the'server'in'stripe'SIGCOMM ‚Äô17, August 21-25, 2017, Los Angeles, CA, USA
it is infeasible to reset the server IP addresses manually for
each topology. Thanks to the property of MPTCP to send
traffic only with routable addresses, we can preconfigure all
possible IP addresses for each topology onto the servers and
let the network controller dynamically load the routing logic
for the subset of addresses particular to the topology in use.
Our definition of the address space is shown in Figure 5a.
We assume IPv4 addresses and allocate IP addresses within
the private 10.0.0.0/8 block. The first 13 bits after the fixed
heading octet represent the switch ID of the ingress/egress
switch. In flat-tree, all switches may serve as an ingress/egress
switch. We associate each switch with a unique ID, which
is not changed with the conversion of topology. This 13-
bit field allows for 8196 switches, which is sufficient for a
large-scale data center. The next 3 bits are for the path
ID in the ùëò-shortest paths. As aforementioned, MPTCP
distinguishes paths by different combinations of IP addresses
between server pairs. This 3-bit field allows for 8 addresses at
sender/receiver and thus supports 82 = 64 concurrent paths
at most, covering the range of ùëò most data centers will use.
The next 2 bits are used to specify the 3 possible flat-tree
topologies. The rest 6 bits show the server ID under the
ingress/egress switch. Because of the limited IPv4 address
space, we cannot afford to assign a unique ID for every
individual server. So, these IDs are reused for servers under
different ingress/egress switches. This 6-bit field supports 64
servers per switch, which is enough for the 20 to 40 servers
per ToR in most data centers. By this address assignment,
we match the /24 prefix at the ingress/egress switches. This
addressing scheme can be easily extended to IPv6 addresses,
which even support globally unique server IDs.
Figure 5b shows an example of the address assignment.
The server in stripe is connected to 3 different ingress/egress
switches under different flat-tree modes. The servers under
the same ingress/egress switch are ordered from left to right,
so the server ID in the global, local, and Clos mode is 2,
1, and 0 respectively. The number of concurrent paths, or
ùëò, can be different under each mode, because each topology
may have optimum transmission performance with a different
ùëò. In this example, ùëò equals 16, 8, and 4 for each topology,
so we need 4, 3, and 2 IP addresses accordingly. Figure 5c
lists the allocated IP addresses according to our addressing
scheme. All these addresses for every flat-tree topology are
preconfigured on the server at deployment time.
One possible problem is the overhead of MPTCP probing