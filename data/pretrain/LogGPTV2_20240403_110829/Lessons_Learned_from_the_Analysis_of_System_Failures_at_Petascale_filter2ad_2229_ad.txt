Total Error/
MB
28,748 1.48
21,254 0.05
318,232 0.75
1,031,886 6.42E-4
144,278
Count Error/
MB
L1
27,522 1.62
L2
18,590 0.05
L3 292,920 0.81
Memory 840,322 5.66E-4
Other 101,388 -
% CPU
% RAM
Total
34.39%
65.61%
1,280,742
4.93%
95.07%
103,050
41.73%
58.27%
160,606
33.19%
54.41%
1,544,398
all the memory DIMMs generated at least a correctable error
during the observation window. In our data, 55% of the nodes
generated only 1 machine check, while 92% of the machine
checks were generated by only 19% of the nodes.
Memory error breakdown. Table VI shows a breakdown
of the memory errors in Blue Waters. The table shows that
about 70.01% of the memory errors involved a single bit, and
that 29.98% 2–8 consecutive bits ( less or equal than two
symbols, being a symbol 4 bit), similarly to the results in
[17] for a smaller-scale Cray supercomputer. The Chipkill can
correct errors affecting up to 2 symbols (x8 Chipkill on compute
and GPU nodes) and that, without it, 30% of the analyzed
memory errors would be uncorrectable (for instance, if only
ECC were used). Hence, a key ﬁnding is that ECC/Chipkill
techniques were effective in correcting 99.997% of the memory
errors that occurred, i.e., we observed only 28 uncorrectable
errors out of 1,544,398 errors. The data also show that about
8.2% of the DIMMs manifest correctable errors, matching with
the data in earlier large-scale studies [9]. However, in our
study, we found that fewer than 0.1% of the machines and
0.014% of the total DIMMs generated uncorrectable errors, i.e.,
1 order of magnitude lower than the incidences of 1.3%–4%
for the machines and 3.5–5.7 times lower than the number
of DDR2 DIMMs with uncorrectable errors (0.05%–0.08%)
reported in [9]. In particular,
the number of uncorrectable
errors over the total number of handled errors is more than
3 orders of magnitude lower than that for DDR2 memory
systems reported in other large-scale studies [9], even though
Blue Waters generates 2 orders of magnitude more machine
checks than previous generations of HPC systems [4], [9]. That
shows a substantial improvement of resiliency to multiple bit
errors of DDR3 over DDR2. A key implication is that the joint
use of ECC and Chipkill techniques in Blue Waters was able
to ﬁx 99.998% of the errors generated in 1.476 PB of DDR3
RAM and 1.5 TB of L1, L2, and L3 caches across all Blue
Waters processors. Only 0.002% of errors were uncorrectable,
compared to the 1.29% reported for the previous generation
of HPC systems [9] employing only ECC and/or x4 Chipkill
(single symbol correction, dual symbol detection). The expec-
tation among hardware designers has been that both transient
and permanent hardware failures may rise uncontrollably as
device sizes shrink, especially in large-scale machines like Blue
Waters. However, results indicate that we are far from that
situation. Because of the high numbers of nodes and errors,
we claim that the results provided in this section have a strong
statistical signiﬁcance for the characterization of processor and
memory error resiliency features.
TABLE VI: Breakdown of the count of memory errors.
Type
Total memory errors
- ECC/chipkill single bit
- Chipkill (more than 2 bit)
- Uncorrectable ECC/Chipkill
Count
1,031,886
722,526
309,359
28
%
66.81%
70.01%
29.98%
2.71E-05%
A. Rate of Uncorrectable Errors Across Different Node Type
Table V also shows that the rates of correctable and un-
correctable errors per node vary across compute, GPU and
service nodes. If we look at the rates of errors detected by
the memory scrubber for compute, GPU, and service nodes
(not reported in Table V) we notice that the nodes have similar
levels of physical susceptibility to memory or cache errors, i.e.,
29 errors per compute node, 28.47 errors per GPU node, and 30
errors per service node. Therefore, we ruled out the possibility
that uncorrectable error rates might be related to the different
hardware characteristics (i.e., total installed RAM and different
Opterons; - see Section II), and we further investigate in the
following.
One observation is that nodes employ different
types of
Chipkill, i.e., x4 for service nodes and x8 for compute and GPU
nodes. In particular, service nodes are able to correct errors on
4 consecutive bits and detect errors on up to 8 consecutive bits,
while compute and GPU nodes can detect and correct errors
affecting up to 8 bits. That impacts the capacity of service
nodes to tolerate multiple-bit errors (for more than 4 bits). In
particular, we found that about 1% of the compute and GPU
nodes’ memory errors involve 4–8 bits, vs. 0.2% on service
nodes. The difference is that while those errors are not critical
for compute and GPU nodes, they cause uncorrectable errors
on service nodes.
The difference in the Chipkill technique used explains only
some of the measurements. In particular, as shown in Table
VIII, the rate of memory errors per node for service node
is 48.1 against 37.1 (23% fewer errors per node) and 31.1
(56% fewer errors per node) for compute and GPU nodes,
respectively. An observation is that compute and GPU nodes
execute a lightweight OS while service node run a full OS
(see Section II). The level of multitasking is different for the
different OSes, with 1 application per core for compute and
GPU nodes, against several background services for service
nodes. Service nodes also show a higher percentage of used
memory because of the lower amount of installed memory,
namely 16 GB against 64 GB for compute nodes. The high
number of active services on service nodes translates into a
more sparse memory access pattern than is found on compute or
GPU nodes, which usually work on big chunks of data (such as
matrices used in compute and GPU nodes, stored in consecutive
memory locations, e.g., in the same memory chip or DIMM,
because of compiler optimization). Hence, we speculate that the
reason service nodes are more susceptible than compute/GPU
nodes to uncorrectable memory errors is their higher levels of
multitasking, disparities in the memory access patterns, and
higher memory loads; however we plan to perform a more
rigorous analysis in the future.
B. Hardware Failure Rates
The procedure enforced at NCSA for Blue Waters hardware
replacement is to replace i) the processor when uncorrectable
or parity errors are observed, and ii) memory when the rate
TABLE VII: AFR, MTBF and FIT for top-5 hardware root-cause.
Processor
DIMMs
GPU card [6 GB]
Disks [2 TB]
SSD [2 TB]
Total
49,258
197,032
3072
20196
392
AFR
0.23%
0.112%
1.732%
0.312%
0.717%
MTBF
3,771,492
7,821,488
506,394
2,807,692
1,230,795
FIT/Device
265.15
127.84
1974.11
356.16
812.48
FIT/GB
NA
15.98
329.02
0.174
0.397
TABLE VIII: Breakdown of the uncorrectable memory errors (UE).
Compute
GPU
Service
GPU Card
RAM [GB]
1,448,960
127,104
23,232
18,432
Errors/node
37.1
31.1
48.1
9.76E-3
UE
14
4
10
38
UE/GB
1.08E-05
3.88E-05
6.22E-05
2.06E-03
MTBF (UE)
1617h
768h
193h
80h
of corrected ECC errors over a single address is above a
programmed threshold. A similar policy is replacement of
storage devices and GPU accelerators when uncorrectable errors
are detected, e.g., when a raid controller detects uncorrectable
disk errors or when an Nvidia GPU accelerator manifests a
double-bit error. In fact, GPU accelerator memory is protected
only by ECC and therefore is vulnerable to multiple-bit errors.
Table VII reports the annualized failure rate (AFR, which is
the percentage of failed unit in a population, scaled to a per-
year estimation), MTBF, and FIT rate for processor, memory
DIMM, GPU, and storage devices (including disks and SSDs).
Table VII also reports the estimated failure rate expressed in
FITs/device. The MTBF for a single component is computed
as the total number of working hours divided by the AFR.
Interestingly, the DDR3 DIMMs show the highest value of
MTBF with 7,821,488 hours. The processors and the disks show
a ﬁgure for the MTBF about half the DDR3 DIMM MTBF,
speciﬁcally 3,771,492 hours for processor and 2,807,692 for the
disks, while the GPU accelerators show an MTBF of 506,394
hours, i.e., 15 times smaller than the DDR3 DIMM MTBF
(about 200 times smaller if comparing the FIT/GB). In fact,
disks showed to provide high level of reliability. During the
measured 261 days, only 45 disks were replaced from the
pool of 20,196 devices. The computed AFR for disks is lower
than the observed values of 2%–6% given in other studies
of disk failures [5], although the population of disks in Blue
Waters is smaller than that considered in other studies. Our
numbers, however, conﬁrm the MTBF values provided by the
manufacturer and show no tangible evidence of defective disk
units; we measured a SSD MTBF lower than the manufacturer’s
declared value of 2,000,000 hours.
Uncorrectable error over SSDs or disks often implies a
permanent error on the device, but no permanent error was
observed for processors or GPU accelerators. The NCSA
replacement policy for processors and GPU is in fact very
conservative. As discussed in the next section, we believe that
the low MTBF calculated for the GPUs is the result of an
assumption that they be more sensitive to uncorrectable error
than processor and memory DIMMs in fact are. In addition,
as discussed in section V-C, we believe that data in Table VII
also contain measurements related to units that were replaced
a few months after the system went into production because of
detected defects (e.g., defective SSDs).
Rate of Uncorrectable errors for GPU cards. Table VIII
reports the rates of uncorrected memory errors in Blue Wa-
ters nodes and Nvidia GPU accelerators. We note i) that for
uncorrectable memory errors, the DDR5 memory on the GPU
accelerator shows an MTBF 1 order of magnitude smaller than
that for the DDR3 constituting the GPU node RAM and 2
orders of magnitude smaller than that of compute nodes (a
similar comparison holds for the FIT/GB reported in Table
VII); and ii) that the disparity is even higher if we look at the
number of uncorrectable errors per GB reported in Table VIII.
In particular, we note that the rate of uncorrectable errors per
GB on GPU node DDR3 memory is 2 orders of magnitude
smaller than that on the Nvidia accelerator DDR5 onboard
memory and that the FIT rate per GB of memory of GPU
accelerators is 10 times higher than that for the DDR3 RAM
of the nodes. An implication is that enforcement of Chipkill
coding for compute, and service and GPU node DDR3 RAM
can decrease the rate of uncorrectable errors per GB by a factor
of 100, compared to the rate for the ECC-protected memories
(e.g.,
the DDR5 memory on the GPU card). The rate of
uncorrectable ECC errors on supercomputer GPUs accelerators
has not been successfully quantiﬁed by any former study for
enterprise-level GPU accelerators. The impact of memory errors
in GPU accelerator memory could represent a serious threat to
creation of future large-scale hybrid systems. For instance, Titan
[18] at ORNL adopted about 5 times the number of GPU nodes
and cards as Blue Waters, making a substantial step towards
fully GPU-based supercomputers.
C. Hardware Failure Trends
Our data cover the early months of production of Blue
Waters. Therefore, it is important to assess how the failure
rates evolved during the measured period. A ﬁrst test consists
of plotting the arithmetic mean of the TBF (Time Between
Failures) for failures due to hardware and software root causes
and observing whether there is any trend. Second, we want
to use the Laplace test to determine whether the arrival of
failures changed signiﬁcantly over time [19]. The Laplace test
evaluates a null hypothesis that failure inter-arrival times are
independent and identically distributed. A score greater than
zero means that there is an upward or increasing trend, and a
score less than zero means there is a downward or decreasing
trend. When the score is greater than (less than) +1.96 (–1.96),
we are at least 95% conﬁdent that there is a signiﬁcant trend
upward (downward). That implies that successive inter-arrival
failure times tend to become larger (smaller) for an improving
(deteriorating) system. A score of zero means that the trend is a
horizontal line. We compute the Laplace score for the number
of failed nodes per day to analyze the presence of trends.
The failure rate distribution trended to a constant failure
rate towards the end of the measured period. Figure 4.(a)
shows that the TBF for failures with hardware root causes
increased by a factor of 2 towards the end of the measured
period. The ripples on the ﬁrst months of the plot were
caused by i) failures of defective units, and ii) pre-production
tests run between March 1 and March 27, 2013 (the start
of the production time). It is interesting to notice that after
users started to use Blue Waters, there was a constant rate
of discovery-and-ﬁx of hardware problems. More speciﬁcally,
looking at Figure 4.(a), the TBFs start to improve in April and
continue to improve constantly until they stabilize towards the
end of the period under study (the slopes of the charts decrease).
At the end the observation period, the hardware MTBF grew
]
h
]
h
[
s
e
r
u
[
s
e
r
u
l
i
a
f
l
i
a
f
e
r
a
w
d
r
a
h
-
F
B
T
n
a
e
m
c
i
t
e
r
a
w
d
r
a
h
–
F
B
T
n
a
e
m
c
i
t
e
m
t
i
r
a
e