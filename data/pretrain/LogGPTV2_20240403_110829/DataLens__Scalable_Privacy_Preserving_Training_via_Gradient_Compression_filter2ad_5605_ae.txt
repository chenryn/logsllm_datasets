0.3789
0.3900
0.3238
0.3796
0.5810
0.8092
0.5567
0.6934
0.6702
0.6897
0.4985
0.6217
0.3483
0.3883
0.1432
0.8075
0.1661
0.6579
0.5901
0.6136
0.4203
0.5225
0.3375
0.3725
0.7123
0.8066
0.6478
0.7061
0.7058
0.7287
0.6061
0.6224
0.4313
0.4875
Table 2: Performance Comparison of different differentially private data generative models on Image Datasets under small privacy budget
which provides strong privacy guarantees (ğœ€ â‰¤ 1, ğ›¿ = 10âˆ’5).
MNIST
Fashion-MNIST
DP-GAN PATE-GAN G-PATE GS-WGAN DataLens DP-GAN PATE-GAN G-PATE GS-WGAN DataLens
0.2226
0.1104
0.3863
0.1524
0.4314
0.1022
0.5534
0.3732
0.6478
0.4046
0.2344
0.2919
0.4201
0.6485
0.7123
0.2230
0.2478
0.4184
0.5377
0.5810
0.0972
0.1029
0.1044
0.1170
0.1432
0.2176
0.2399
0.3484
0.3571
0.4168
0.1021
0.1302
0.0998
0.1210
0.1053
0.1605
0.2977
0.3698
0.3659
0.4222
0.1874
0.3020
0.4283
0.5258
0.5567
0.1000
0.1001
0.1144
0.1242
0.1661
ğœ€
0.2
0.4
0.6
0.8
1.0
on high dimensional data than other baseline DP generative mod-
els. Specifically, we note that GS-WGAN can only converge under
large privacy budget (ğœ€ = 10) for gray-scale datasets (MNIST and
FashionMNIST), as GS-WGAN needs 20k epochs and small noise
to converge. In comparison, DataLens can converge within 100
epochs due to the fast convergence rate brought by top-ğ‘˜ operation
for high-dimensional datasets. As a result, under the limited pri-
vacy budget (ğœ€ = 1) or given high dimensional facial datasets (e.g.,
CelebA), GS-WGAN is unable to converge and therefore generate
low-utility data, making the classifier accuracy close to random
guessing; while DataLens can generate high-utility data even with
limited privacy budget.
Evaluation under small privacy budget. To further demon-
strate the advantage of DataLens as being able to generate high-
utility images under small privacy budgets (i.e., higher privacy
protection guarantees), we conduct ablation studies on MNIST and
Fashion-MNIST under ğœ€ â‰¤ 1. The experimental results are shown
in Table 2.
We find that DataLens achieves the best results compared with
the baselines given such tight privacy constraints. With increasing
privacy budgets, different DP models gradually converge and the
accuracy increases. We note that DataLens converges the fastest
and achieves more than 20% accuracy even under smallest privacy
budget ğœ€ = 0.2 for both MNIST and Fashion-MNIST datasets; while
the baseline models barely converge and the accuracy is similar
to random guess. The observed experimental results also support
our theoretical analysis that the proposed TopAgg algorithm can
introduce a smaller bias and provide high-utility gradient informa-
tion for the student generator to converge, demonstrating that our
method is particularly effective under limited privacy budgets.
Visual Quality Evaluation. We present the quantitative vi-
sual quality evaluation of DataLens and baselines in Table 3 based
on Inception Score (IS) under different privacy constraints: ğœ€ =
1, ğ›¿ = 10âˆ’5 and ğœ€ = 10, ğ›¿ = 10âˆ’5. Since CelebA-gender and CelebA-
face are from the same distribution of face images and have a lot of
overlapping, we mainly consider CelebA-Gender to represent the
visual quality for CelebA face dataset.
We observe that DataLens consistently outperform the base-
lines in terms of visual quality while ensuring the rigorous privacy
protection when ğœ€ = 1, which suggests that DataLens can converge
faster than the state-of-the-art baselines. Specifically, the generated
differentially private MNIST images achieve the inception score
of 4.37, improving the strongest baseline G-PATE by more than
20%. When ğœ€ = 10, we find that DataLens can be outperformed by
GS-WGAN on MNSIT and Fashion-MNIST, but still outperforms
all baselines on high-dimensional CelebA datasets. We believe the
reason is that while top-ğ‘˜ operation can help with faster converge
the most important information and yield high-utility data, it may
lose some detailed and trivial gradient information for image re-
construction. We note that visual quality and data utility are two
orthogonal metrics, and DataLens consistently generates data with
the highest utility. We provide the evaluation of FID in Appendix
Table 12, given that FID is evaluated based on models trained with
ImageNet which may not be suitable for evaluating datasets such
Session 7A: Privacy Attacks and Defenses for ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea2155Table 3: Quality evaluation of images generated by different differ-
entially private data generative models on Image Datasets: we use
Inception Score (IS) to measure the visual quality of the generated
data under different ğœ€ (ğ›¿ = 10âˆ’5).
Dataset
MNIST
Fashion-
MNIST
CelebA
Real
data
9.86
9.01
1.88
ğœº
1
10
1
10
1
10
DP-
GAN
1.00
1.00
1.03
1.05
1.00
1.00
PATE-
GAN
1.19
1.46
1.69
2.35
1.15
1.16
G-
PATE
3.60
5.16
3.41
4.33
1.11
1.12
GS-
WGAN
1.00
8.59
1.00
5.87
1.00
1.00
DataLens
4.37
5.78
3.93
4.58
1.18
1.42
Figure 2: Ablation studies on the data dependent bound v.s. data in-
dependent bound on MNIST (left) and CelebA-Hair (right). The data
independent bound always yields tighter privacy bound than the
data dependent analysis, given high dimensionality of gradients.
as MNIST. In addition, we believe it would be an interesting fu-
ture direction to add additional loss terms for improving the visual
quality of the generated data with privacy guarantees.
5.3 Ablation Studies
In this section, we conduct a series of ablation studies to further un-
derstand the improvements of DataLens, including the empirical
exploration of the data-dependent and data-independent privacy
bounds, the hyper-parameter impacts, the comparison with differ-
ent gradient compression methods, as well as the impacts of each
component in DataLens pipeline.
Data-Independent Bound v.s. Data-Dependent Bound.
We compute the data-independent privacy bound and the data
dependent privacy bound to validate the theoretical comparison
in Section 4.2. Figure 2 presents the privacy budget consumption
over each training epoch computed by the data-independent bound
and the data-dependent bound, respectively. We set ğœ = 5000 for
MNIST and Fashion-MNIST, and ğœ = 9000 for CelebA-Hair and
CelebA-Gender. The training is stopped when the privacy budget
ğœ€ computed by the data independent bound reaches 1. As shown
in Figure 2, the data-independent bound is always tighter than
the data-dependent one on the high-dimensional datasets. We also
notice that models on MNIST and Fashion-MNIST have a similar
data-dependent bound, and so are models on CelebA-Hair and
CelebA-Gender. These results align with our theoretical analysis
in Theorem 4 and Theorem 5. Due to the high dimensionality of the
gradients and the Gaussian noise, there is unlikely to be a spike in
the probability distribution over the likely outcomes of the gradient
aggregation step. Consequently, the data-dependent privacy bound
is loose and mostly determined by the dimension of the gradients.
Ablation studies on hyper-parameters. As we can see,
DataLens contains several hyper-parameters: the number of the
teacher models, the top-ğ‘˜, the threshold ğ›½, the standard deviation
ğœ of injected Gaussian noise, and the gradient clipping constant
ğ‘. We evaluate a set of hyper-parameters as shown in Table 4. For
other parameters, we use: for MNIST and Fashion-MNIST datasets,
we set ğœ = 5000 when ğœ€ = 1 and ğœ = 900 when ğœ€ = 10; for CelebA
datasets of higher dimensionality, we set ğœ = 9000 when ğœ€ = 1
and ğœ = 700 when ğœ€ = 10. We set the gradient clipping constant
ğ‘ = 10âˆ’5 in all experiments. We also follow the default DC-GAN
model configuration4 and set the batch size the same as the disjoint
data partition size.
From Table 4, we observe that training more teacher discrim-
inators will give us better performance in general, as it can save
more privacy budgets. However, with more teacher discriminators,
each discriminator will have access to a smaller amount of training
data, thus leading to a slightly worse performance. We observe this
trade-off on the CelebA-Gender dataset, where the optimal number
of teachers is 6000. Choosing a proper top-ğ‘˜ and ğ›½ is bit tricky:
as stated in the discussion of Section 4.3, if top-ğ‘˜ is too small, the
model converges slower and is likely to converge to a bad solution.
On the other hand, if top-ğ‘˜ is too large, we will introduce a larger
DP noise and the model can soon reach the privacy budget limit
given the high sensitivity. In this paper, we search for the best top-ğ‘˜
via grid search. Another observation is that we usually need to set a
high threshold ğ›½ to smooth out the noisy gradient entries from the
DP noise, though if the threshold is too high it is likely to ignore the
top-ğ‘˜ voted gradients information. This tradeoff leads us to choose
a threshold ğ›½ between ğœ
ğ‘ . Finally, we also note that the
clipping value ğ‘ has a large impact on the model convergence. We
observe given a fixed top-ğ‘˜, a reasonably smaller ğ‘ generally yields
better convergence rate and data utility. This is aligned with our
theorem, because if ğ‘ gets smaller, the convergence bias from the
first term ğ‘(ğ‘‘ âˆ’ ğ‘˜)ğ‘€ will get smaller.
2ğ‘ and ğœ
We note that the peformance improvements from DataLens
does not necessarily comes from the fact that we have more hyper-
parameters, since compared to other baseline methods using PATE
framework such as G-PATE and PATE-GAN, DataLens only intro-
duces one more hyper-parameter top-ğ‘˜ for gradient compression.
Moreover, as shown in Table 4, DataLens can outperform all the
baselines on CelebA datasets over a wide range of different hyper-
parameters in practice.
Ablation Studies on the Gradient Compression Methods.