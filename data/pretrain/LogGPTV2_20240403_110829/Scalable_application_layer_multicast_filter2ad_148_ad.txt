a
r
t
l
o
r
t
n
o
C
80
70
60
50
40
30
20
10
0
0
128
Join
Narada-5 (Avg)
NICE (Avg)
16 X 5
Leave
200
400
600
800 1000 1200 1400 1600 1800 2000
Time (in secs)
Figure 11: Fraction of members that received data packets over
the duration of member failures. (simulation)
Figure 12: Control bandwidth required at end-host accesslinks
(simulation)
Router Stress
Link Stress
Path Length
Bandwidth Overheads (Kbps)
Group
Size
8
16
32
64
128
256
512
1024
1560
2048
Narada-5
1.55 (1.30)
1.84 (1.28)
2.13 (2.17)
2.68 (3.09)
3.04 (4.03)
3.63 (7.52)
4.09 (10.74)
-
-
-
NICE
3.51 (3.30)
2.34 (2.16)
2.42 (2.60)
2.23 (2.25)
2.36 (2.73)
2.31 (3.18)
2.34 (3.49)
2.59 (4.45)
2.83 (5.11)
2.92 (5.62)
Narada-5
1.19 (0.39)
1.34 (0.76)
1.54 (1.03)
1.74 (1.53)
2.06 (2.64)
2.16 (3.02)
2.57 (5.02)
-
-
-
NICE
3.24 (2.90)
1.86 (1.39)
1.90 (1.82)
1.63 (1.39)
1.63 (1.56)
1.63 (1.63)
1.62 (1.54)
1.77 (1.77)
1.88 (1.90)
1.93 (1.99)
Narada-5
25.14 (9.49)
19.00 (7.01)
20.42 (6.00)
22.76 (5.71)
21.55 (6.03)
23.42 (6.17)
24.74 (6.00)
-
-
-
NICE
12.14 (2.29)
20.33 (6.75)
17.23 (5.25)
20.62 (7.40)
21.61 (7.75)
24.67 (7.45)
22.63 (6.78)
25.83 (6.13)
24.99 (6.96)
24.08 (5.36)
Narada-30
0.61 (0.55)
2.94 (2.81)
9.23 (8.95)
26.20 (28.86)
65.62 (92.08)
96.18 (194.00)
199.96 (55.06)
-
-
-
NICE
1.54 (1.34)
0.87 (0.81)
1.03 (0.95)
1.20 (1.15)
1.19 (1.29)
1.39 (1.76)
1.93 (3.35)
2.81 (7.22)
3.28 (9.58)
5.18 (18.55)
Table 1: Data path quality and control overheads for varying multicast group sizes (simulation)
late Narada with groups of size 1024 or larger since the completion
time for these simulations were on the order of a day for a single run
of one experiment on a 550 MHz Pentium III machine with 4 GB of
RAM.
Narada and NICE tend to converge to trees with similar path lengths.
The stress metric for both network links and routers, however, is
consistently lower for NICE when the group size is large (64 and
greater). It is interesting to observe the standard deviation of stress
as it changes with increasing group size for the two protocols. The
standard deviation for stress increased for Narada for increasing group
sizes. In contrast, the standard deviation of stress for NICE remains
relatively constant; the topology-basedclustering in NICE distributes
the data path more evenly among the different links on the underly-
ing links regardless of group size.
The control overhead numbers in the table are different than the
ones in Figure 12; the column in the table is the average control
trafﬁc per network router as opposed to control trafﬁc at an end-
host. Since the control trafﬁc gets aggregated inside the network,
the overhead at routers is signiﬁcantly higher than the overhead at
an end-host. For these router overheads, we report the values of the
Narada-30 version in which the route update frequency set to 30 sec-
onds. Recall that the Narada-30 version has poor failure recovery
performance, but is much more efﬁcient (speciﬁcally 5 times less
overhead with groups of size 128) than the Narada-5 version. The
HeartBeat messages in NICE were still sent at 5 second intervals.
For the NICE protocol, the worst case control overheads at mem-
bers increase logarithmically with increase in group size. The con-
trol overheads at routers (shown in Table 1), show a similar trend.
Thus, although we experimented with upto 2048 members in our
simulation study, we believe that our protocol scales to even larger
groups.
6. WIDE-AREA IMPLEMENTATION
We have implemented the complete NICE protocol and experi-
mented with our implementation over a one-month period, with 32
to 100 member groups distributed across 8 different sites. Our ex-
perimental topology is shown in Figure 13. The number of mem-
bers at each site was varied between 2 and 30 for different experi-
ments. For example, for the 32 member experiment reported in this
section, we had 2 members each in sites B, G and H, 4 each at A,
E and F, 6 in C and 8 in D. Unfortunately, experiments with much
larger groups were not feasible on our testbed. However, our im-
plementation results for protocol overheads closely match our sim-
ulation experiments, and we believe our simulations provide a rea-
sonable indication of how the NICE implementation would behave
with larger group sizes.
6.1
Implementation Speciﬁcs
We haveconductedexperiments with data sourcesat different sites.
In this paper, we present a representative set of the experiments where
the data stream source is located at site C in Figure 13. In the ﬁg-
ure, we also indicate the typical direct unicast latency (in millisec-
onds) from the site C, to all the other sites. These are estimated one-
way latencies obtained using a sequence of application layer (UDP)
probes. Data streams were sent from the source host at site C, to all
other hosts, using the NICE overlay topology. For our implementa-
21339.4
H
1.7
F
G
4.6
E
0.6
D
0.5
C
Source
A
35.5
B
33.3
A: cs.ucsb.edu
B: asu.edu
C: cs.umd.edu
D: glue.umd.edu
E: wam.umd.edu
F: umbc.edu
G: poly.edu
H: ecs.umass.edu
Figure 13: Internet experiment sites and direct unicast latencies
from C
tion, we experimented with different HeartBeat rates; in the results
presented in this section, we set the HeartBeat message period to 10
seconds.
In our implementation, we had to estimate the end-to-end latency
between hosts for various protocol operations, including member
joins, leadership changes, etc. We estimated the latency between
two end-hosts using a low-overhead estimator that sent a sequence
of application-layer (UDP) probes. We controlled the number of
probes adaptively using observed variance in the latency estimates.
Further, instead of using the raw latency estimates as the distance
metric, we used a simple binning scheme to map the raw latencies
to a small set of equivalence classes. Speciﬁcally, two latency esti-
mates were considered equivalent if they mappedto the same equiv-
alence class, and this resulted in faster convergence of the overlay
topology. The speciﬁc latency ranges for each class were 0-1 ms,
1-5 ms, 5-10 ms, 10-20 ms, 20-40 ms, 40-100 ms, 100-200 ms and
greater than 200 ms.
To compute the stretch for end-hosts in the Internet experiments,
we used the ratio of the latency from between the source and a host
along the overlay to the direct unicast latency to that host. In the
wide-area implementation, when a host A receives a data packet
forwarded by member B along the overlay tree, A immediately sends
back a overlay-hop acknowledgment back to B. B logs the round-
trip latency between its initial transmission of the data packet to A
and the receipt of the acknowledgment from A. After the entire
experiment is done, we calculated the overlay round-trip latencies
for each data packet by adding up the individual overlay-hop laten-
cies available from the logs at each host. We estimated the one-
way overlay latency as half of this round trip latency. We obtained
the unicast latencies using our low-overhead estimator immediately
after the overlay experiment terminated. This guaranteed that the
measurements of the overlay latencies and the unicast latencies did
not interfere with each other.
6.2
Implementation Scenarios
The Internet experiment scenarios have two phases: a join phase
and a rapid membership change phase. In the join phase, a set of
member hosts randomly join the group from the different sites. The
hosts are then allowed to stabilize into an appropriate overlay de-
s
r
e
b
m
e
m
f
o
n
o
i
t
c
a
r
F
Cumulative distribution of stress
1
0.95
0.9
0.85
0.8
0.75
0.7
1
32 members
64 members
96 members
2
3
4
5
6
7
8
9
Stress
Figure 14: Stress distribution (testbed)
livery tree. After this period, the rapid membership change phase
starts, where host members randomly join and leave the group. The
average member lifetime in the group, in this phase was set to 30
seconds. Like in the simulation studies, all member departures are
ungraceful and allow us to study the worst case protocol behavior.
Finally, we let the remaining set of members to organize into a sta-
ble data delivery tree. We present results for three different groups
of size of 32, 64, and 96 members.
Data Path Quality
In Figure 14, we show the cumulative distribution of the stress met-
ric at the group members after the overlay stabilizes at the end of
the join phase. For all group sizes, typical members have unit stress
(74% to 83% of the members in these experiments). The stress for
the remaining members vary between 3 and 9. These members are
precisely the cluster leaders in the different layers (recall that the
cluster size lower and upper bounds for these experiments is 3 and 9,
respectively). The stress for these members can be reduced further
by using the high-bandwidth data path enhancements,described in [4].
For larger groups, the number of members with higher stress (i.e.
between 3 and 9 in these experiments) is more, since the number of
clusters (and hence, the number of cluster leaders) is more. How-
ever, as expected, this increase is only logarithmic in the group size.
In Figure 15, we plot the cumulative distribution of the stretch
metric. Instead of plotting the stretch value for each single host, we
group them by the sites at which there are located. For all the mem-
ber hosts at a given site, we plot the mean and the 95% conﬁdence
intervals. Apart from the sites C, D, and E, all the sites have near
unit stretch. However, note that the source of the data streams in
these experiments were located in site C and hosts in the sites C,
D, and E had very low latency paths from the source host. The ac-
tual end-to-end latencies along the overlay paths to all the sites are
shown in Figure 16. For the sites C, D and E these latencies were 3.5
ms, 3.5 ms and 3.0 ms respectively. Therefore, the primary contri-
bution to these latencies are packet processing and overlay forward-
ing on the end-hosts themselves.
In Table 2, we present the mean and the maximum stretch for
the different members, that had direct unicast latency of at least 2
ms from the source (i.e. sites A, B, G and H), for all the differ-
ent sizes. The mean stretch for all these sites are low. However,
in some cases we do see relatively large worst case stretches (e.g.
2149
8
7
6
5
4
3
2
1
45
40
35
30
25
20
15
10
5
0
h
c
t
e
r
t
S
)
s
m
n
i
(
y
c
n
e
a
t
l
d
n
e