erators. These shim libraries normalize byte order, handle
support for negative numbers, exponents larger than the
modulus, etc. Many hardware accelerators (and indeed
cryptographic software packages) have intermittent or no
support for such inputs, as they don’t occur when using
standard RSA. However, as technologies like the cryp-
toserver reduce the cost of modular exponentiations, cryp-
tographic algorithms and protocols considered too costly
and complex for practical use will be used, and these al-
gorithms do not respect these narrow limits.
Normalizing the appearance of different types of hard-
ware in this way allows the server to transparently sup-
port a heterogeneous collection of accelerators.
It also
lets us remove from the server any task-speciﬁc logic, and
to compensate for any differences between the features
supported by the accelerator drivers and the interface we
present to cryptoserver clients.
Although we can normalize the software interface to
each board, we cannot normalize the hardware itself. Each
board contains a number of cryptographic coprocessors,
and has a different degree of intrinsic parallelism. Be-
cause the coprocessors chosen to populate each type of
board are different, each board has a characteristic latency


(time required for a single operation). This means that a
single thread making sequential requests to a single board
will see an exponentiation rate determined by that board’s
latency. The speed numbers quoted for each type of board
represent throughput; the rate each coprocessor is capable
of, multiplied by the number of processors on the board.
Only a process sufﬁciently parallel to make full use of all
the processors on a board will see the board’s rated per-
formance.
Each coprocessor also tackles the problem of modu-
lar exponentiation using a different algorithm and internal
data format. The data formats will determine how much
copying and re-ordering of input data must be done before
a request can be sent to a coprocessor. The algorithm will
determine how much preprocessing must be done in a host
library to prepare the request for a coprocessor, and also
how changes in input parameters will affect changes in
coprocessor latency (and hence throughput). As an exam-
ple, the choice of algorithm will control how the perfor-
mance of the coprocessors changes as a function of expo-
nent length, and hence how optimizations such as Chinese
remaindering affect a board’s performance.
In order to hide the complexity of scheduling multiple
requests onto these parallel devices, cryptographic accel-
erator vendors provide a certain amount of host software.
This usually consists of both libraries that take care of
any necessary host-based pre- and post-processing of re-
quests (for data formats, Montgomery reduction, etc), and
programs/drivers that can manage requests from multiple
client programs at once and schedule them for execution
by the hardware coprocessors.
The cryptoserver must distribute requests across all of
these board-speciﬁc schedulers. In order to maintain as
much hardware independence as possible, we currently
implement a very simple algorithm. We independently
conﬁgure the number of worker threads the server uses
to manage each cryptographic accelerator (or each pool of
accelerators of a single type; frequently accelerator ven-
dors provide one local interface to all of their accelerators
that are present on a machine, and manage distributing re-
quests across boards). This depends both on the inherent
parallelism of each accelerator board, and on the archi-
tecture of the driver and any vendor libraries we use to
interface with it. Each of these worker threads, when free,
pulls a work item off the work queue and presents that
item to the accelerator associated with that thread. Given
our interest in experimenting with a variety of accelerator
types, this simple and ﬂexible scheduling algorithm makes
sense. A high-volume production cryptoserver might ben-
eﬁt from a more sophisticated algorithm. If such a server
is implemented using only a single type of cryptographic
accelerator, this simple algorithm will end up relying on
the scheduling algorithm implemented by the underlying
vendor libraries, which is likely to have been optimally
tuned for that class of device. A more interesting option
would be to implement such a server using a collection
of boards with different characteristics chosen to optimize
performance across a variety of conditions (bursts of re-
quests, high constant load, occasional single requests).
Such a server would beneﬁt from a more sophisticated
scheduling algorithm.
To maximize parallelism, we must allocate at least as
many worker threads to a board as there are cryptographic
coprocessors on that board. It turns out that as you scale
up the number of threads simultaneously making requests
of a single board, performance improves sharply until the
number of threads matches the number of coprocessors.
As you increase the number of threads beyond this point,
performance continues to improve slowly for a short pe-
riod, and then plateaus. Given that at maximum through-
put, at any given moment the number of threads blocked
waiting for a response to return from a coprocessor should
be equal to the number of coprocessors on a board, there
is some room for further processing (and plenty of host
CPU left) as additional threads can be preparing future re-
quests for processing by the board (both copying data and
doing any mathematical pre-processing), and doing any
required post-processing of returned requests (again, both
copying of data and mathematical post-processing, such
as combining Chinese remaindered results). Allocating
more threads than can be fully used by both pre- and post-
processing and waiting on accelerators incurs little cost
except at startup time; any such surplus threads simply re-
main blocked waiting for work. We therefore allocate a
number of threads for each board slightly larger than the
minimum necessary, based on the intrinsic parallelism of
each board and experimental tests of how board perfor-
mance scales beyond this minimum number.
At startup, the server takes an argument giving it either
a single shim library name and number of threads (if it is
to be run with a single board type for benchmarking pur-
poses), or the name of a conﬁguration ﬁle listing multiple
shim libraries and corresponding thread counts. If there
are multiple boards of a particular type present, the shim
library handles that transparently; the number of threads
allocated to that library must simply be scaled up to match.
5. Performance
Recall
that our initial
implementation of the cryp-
toserver is built around a Sun Ultra-10 workstation, con-
taining a 440 MHz UltraSparc IIi processor, with one
Atalla AXL200 cryptographic accelerator and one nCi-
pher nFast 300 PCI. The Ultra-10 scores 18.1 on SPECint
95, the AXL200 has a maximum throughput of 265 1024-
bit RSA operations per second (without Chinese remain-
dering), and the nCipher has a maximum throughput of
300 1024-bit RSA operations per second ( 93 ops/sec
without Chinese remaindering). For benchmarking pur-
poses, we ran cryptoserver clients on a dual processor, 250
MHz, 512 MB Sun UltraSparc (sans the Atalla board),
connected via switched 100 Mbit/s Ethernet.
5.1. Microbenchmarks
In all our multithreaded benchmarks, all threads per-
form any necessary initialization code, and then line up
waiting for a signal. The last thread to ﬁnish its initial-
ization gets the start time, and signals all threads to start.
As each thread ﬁnishes, it increments a counter. The last
thread to ﬁnish gets the stop time, and measures operation
rate as (operations per thread * thread count)/total time.
If each thread measures its own elapsed time, the sheer
amount of time spent on gettimeofday system calls starts
to affect the overall measured value. Similarly, it becomes
difﬁcult to accurately measure the time taken by a single
operation. We calculate average rates below by measur-
ing the total time taken to have each thread perform 1000
1024-bit operations. Each such measurement of (number
of threads * 1000 operations per thread) operations is con-
sidered one “block”. Final rate values are generated by
averaging the times for 1-8 blocks, and dividing by the
total number of operations performed (block counts for
each group of measurements are noted with those mea-
surements). Throughput rates are given in operations/sec.
As the variance of these averaged block times is low, and
are noisy and difﬁcult to interpret if presented as rates,
we do not show variance data directly. All latencies are
reported in milliseconds.
To measure the speed of each board individually when
accessed locally, we wrote a multithreaded microbench-
mark that repeatedly performs a modular exponentiations
using our “shim” library for each board, and compares
the result to the correct value. Each “shim” library (see
Section 4.2.5) talks directly to a local accelerator via the
board vendor’s user libraries. This measurement provides
an estimate of the baseline speed of the accelerator board,
and an independent conﬁrmation of the speed rating by
the manufacturer. Each number below is in 1024-bit oper-
ations/sec, measured by having each thread perform one
block of 1000 operations (numbers rounded to 2 decimal
places).
Based on measurements like those above, latency in-
formation given below, and information from the vendors,
we know that the Atalla board has 26 cryptographic copro-
cessors, while the nCipher board is using around 10. As
noted above, performance continues to improve slowly as
the thread count is increased beyond the number of copro-
cessors. We therefore ran the tests below with 30 threads
devoted to each accelerator board. (In all cases, 5 addi-
tional threads were devoted to processing requests and,
Board
AXL200
AXL200
nFast 300
nFast 300
nFast 300
nFast 300
Threads
1
25
1
10
30
32
no CRT w/CRT
10.85
265.98
11.67
92.40
93.28
93.30
37.00
288.92
297.74
299.97
Table 1. Local Accelerator Throughput (ops/sec)
and another 5 to replies.) We do not present numbers for
the Atalla board alone using the Chinese Remainder Theo-
rem (CRT). The time taken by the Atalla board to process
a modular exponentiation rises linearly with the number
of bits in the exponent. There is therefore no overall ad-
vantage to using CRT on a loaded Atalla board. We do
use CRT support on the Atalla board to allow requests to
be parallelized at low load levels, and to support 2048-bit
moduli. In order to simplify presentation of results, such
support was disabled for the tests presented here. The nor-
mal Atalla libraries simply ignore CRT coefﬁcients and
process the private exponent directly.
We report below the latencies corresponding to the
single-threaded measurements reported above. We then
report results for the same computation performed by a
multithreaded client of the cryptoserver. Measurements
on the cryptoserver client are averages of 8 blocks (3 for
nCipher with CRT) of 1000 operations per thread, the
variances are quite small. The cryptoserver was run in
3 conﬁgurations: with the Atalla board alone, with the
nCipher board alone, and with both boards. This allows
more direct comparison to the local latencies measured
on the single boards.
In the ideal case, the throughput
for the 2-board conﬁguration should be the sum of the
throughputs for each board used alone. In the presentation
of the single-threaded measurements below, we present
measurements for the ﬁrst two cryptoserver conﬁgurations
alone (a single-threaded client accessing a server control-
ling both boards will simply see the performance of that
board that got to its requests ﬁrst) We report numbers both
with and without securing the wire with triple DES.
The table below lists the performance of the cryp-
toserver using a multithreaded client application. Results
are divided according to whether the server was manag-
ing just the nCipher board, just the Atalla board, or both
boards. Results are also divided according to whether wire
encryption was turned on, and the type of request placed
by the client (with or without CRT, a single request per
RPC or a multiple request – a batch of 3 requests per
RPC). The number of client threads was chosen in an at-
tempt to maximize throughput.
Without wire encryption, our software delivers the full
throughput of the accelerators. Adding triple-DES incurs
Machine/Board
Local accelerator:
AXL200
nFast 300
Remote Cryptoserver:
AXL200 only (insecure)
AXL200 only (secure)
nFast 300 only (insecure)
nFast 300 only (secure)
Threads Latency (ms) Throughput (ops/s) Latency w/CRT Throughput w/CRT
1
1
1
1
1
1
92.19
85.68
93.08
94.12
86.78
88.12
10.85
11.67
10.74
10.62
11.52
11.35
Table 2. Single-Threaded Performance
27.03
37.00
28.27
29.86
35.37
33.49
a 2% reduction in throughput. When the accelerator is
managing a single board alone, the SPARC Ultra-10 used
as a server has no trouble keeping that board fully loaded.
Even then, the load on both client and server is very low.
When the cryptoserver manages both boards together, the
demands of managing input and output, as well as the
host side pre- and post-processing required by each board,
begin to outstrip our current server host. While process-
ing 1024-bit RSA requests using CRT, we cannot keep up
with the total throughput of both boards (approximately
565 ops/sec) without either turning off wire encryption or
batching requests in groups of 3. Attempts to do so satu-
rated the cryptoserver’s CPU (note that at the same time,
the load on the client machine was still extremely low).
Operating at these high host loads also increased the vari-
ability of the result somewhat, to the degree that the differ-
ence between secure and non-secure trials began to blur.
This suggests that to scale beyond these two boards will
require a faster or more parallel server host, ofﬂoading of
the wire encryption to a symmetric cryptographic copro-
cessor, or both.