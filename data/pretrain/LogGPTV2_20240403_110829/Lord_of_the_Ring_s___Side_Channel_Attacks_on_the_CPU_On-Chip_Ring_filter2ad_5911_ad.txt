Destination Ring Agent Cluster
A = {0,3,4,7}
B = {1,2,5,6}
Core
LLC Slice
Lane 1
Lane 2
Lane 2
Lane 1
trafﬁc only includes data and no acknowledge trafﬁc because
the amount of contention that it causes is slightly smaller than
the one caused by the SA→core trafﬁc. Second, we ﬁnd that
the SA→slice trafﬁc occurs separately from the SA→core
trafﬁc. For example, the contention we observe when Rc = 5
(Rc ∈ B), Rs = 2, Sc = 4, Ss = 3 (Ss ∈ A) could not occur if
the data from the SA had to stop by Sc ﬁrst. Also, when the
sender contends both on the SA→slice and SA→core trafﬁc
the contention is larger than the individual contentions, which
further supports the independence of the two ﬂows.
14. In the event of a miss, the system agent supplies a
separate copy of the data to the missing LLC slice, in
order to maintain inclusivity. The ring lane used to send
data trafﬁc to an LLC slice of one cluster is the same
used to send data trafﬁc to a core of the opposite cluster.
To sum up, when the sender misses in the LLC, new ring
contention cases occur compared to Equation 1 due to the
extra ﬂows required to handle an LLC miss transaction. For-
mally, contention happens iff:
(Ss = Rs)∨
(Ss > Rs)∧ (Sc  Rc)∧
(cid:2)(Ss ∈ A)∧ (Rs ∈ A)∨ (Ss ∈ B)∧ (Rs ∈ B)(cid:3)∨
(cid:2)(Sc ∈ A)∧ (Rc ∈ A)∨ (Sc ∈ B)∧ (Rc ∈ B)(cid:3)(cid:9)∨
(Rc > Rs)∧(cid:8)(Sc > Rc)∧ (Ss  Rs)∧(cid:2)(Sc ∈ A)∧ (Rc ∈ A)∨ (Sc ∈ B)∧ (Rc ∈ B)(cid:3)∨
(Ss > Rs)∧(cid:2)(Ss ∈ A)∧ (Rc ∈ B)∨ (Ss ∈ B)∧ (Rc ∈ A)(cid:3)(cid:9)
(2)
Additional Considerations We now provide additional ob-
servations on our results. First, the amount of contention is not
proportional to length of the overlapping segment between the
sender and the receiver. This is because, as we saw, contention
depends on the presence of full “boxcars” passing by the re-
ceiver’s ring stops when they are trying to inject new trafﬁc,
and not on how far away the destination of these boxcars is.
Second, the amount of contention grows when multiple
senders contend with the receiver’s trafﬁc simultaneously.
This is because multiple senders ﬁll up more slots on the ring,
further delaying the receiver’s ring stops from injecting their
trafﬁc. For example, when Rc = 5 and Rs = 0, running one
sender with Sc = 7 and Ss = 4 and one with Sc = 6 and Ss = 3
creates more contention than running either sender alone.
Third, enabling the hardware prefetchers both ampliﬁes
contention in some cases, and causes contention in some new
cases (with senders that would not contend with the receiver
if the prefetchers were off). This is because prefetchers cause
the LLC or the SA to transfer additional cache lines to the
core (possibly mapped to other LLC slices than the one of the
requested line), thus ﬁlling up more ring slots potentially on
multiple lanes. Intel itself notes that prefetchers can interfere
with normal loads and increase load latency [45]. We leave
formally modeling the additional contention patterns caused
by the prefetchers for future work.
Finally, we stress that the contention model we constructed
is purely based on our observations and hypotheses from the
data we collected on our CPUs. It is possible that some of the
explanations we provided are incorrect. However, our primary
goal is for our model to be useful, and in the next few sections
we will demonstrate that it is useful enough to build attacks.
Security Implications The results we present bring with
them some important takeaways. First, they suggest an af-
ﬁrmative answer to our question on whether the ring inter-
connect is susceptible to contention. Second, they teach us
what type of information a receiver process monitoring con-
tention on the ring interconnect can learn about a separate
sender process running on the same host. By pinning itself
to different cores and loading from different slices, a receiver
may distinguish between the cases when the sender is idle
and when it is executing loads that miss in its private caches
and are served by a particular LLC slice. Learning what LLC
slice another process is loading from may also reveal some
information about the physical address of a load, since the
LLC slice an address maps to is a function of its physical ad-
dress [57,65,104]. Further, although we only considered these
scenarios, ring contention may be used to distinguish other
types on sender behavior, such as communication between the
cores and other CPU components (e.g., the graphics unit and
the peripherals). Importantly, however, for any of these tasks
the receiver would need to set itself up so that contention with
the sender is expected to occur. Equations 1 and 2 make this
possible by revealing the necessary and sufﬁcient conditions
under which trafﬁc can contend on the ring interconnect.
4 Cross-core Covert Channel
We use the ﬁndings of Section 3 to build the ﬁrst cross-core
covert channel to exploit contention on the ring interconnect.
Our covert channel protocol resembles conventional cache-
based covert channels (e.g., [62, 106]), but in our case the
sender and the receiver do not need to share the cache. The
basic idea of the sender is to transmit a bit “1” by creating
contention on the ring interconnect and a bit “0” by idling,
thus creating no ring contention. Simultaneously, the receiver
USENIX Association
30th USENIX Security Symposium    653
Figure 5: Load latency measured by our covert channel re-
ceiver when the sender continuously transmits a sequence
of zeros (no contention) and ones (contention) on our Cof-
fee Lake machine, with Rc = 3, Rs = 2, Sc = 4, Ss = 1 and a
transmission interval of 3,000 cycles.
times loads (using the code of Listing 1) that travel through a
segment of the ring interconnect susceptible to contention due
to the sender’s loads (this step requires using our results from
Section 3). Therefore, when the sender is sending a “1”, the
receiver experiences delays in its load latency. To distinguish
a “0” from a “1” the receiver can then simply use the mean
load latency: smaller load latencies are assigned to a “0”, and
larger load latencies are assigned to a “1”. To synchronize
sender and receiver we use the shared timestamp counter, but
our channel could also be extended to use other techniques
that do not rely on a common clock (e.g., [43, 67, 79, 105]).
To make the covert channel fast, we leverage insights from
Section 3. First, we conﬁgure the receiver to use a short seg-
ment of the ring interconnect. This allows the receiver to issue
more loads per unit time due to the smaller load latency, with-
out affecting the sender’s ability to create contention. Second,
we set up the sender to hit in the LLC and use a conﬁguration
of Sc and Ss where, based on Equation 1, it is guaranteed to
contend with the receiver both on its core→slice trafﬁc and
on its slice→core one. Contending on both ﬂows allows the
sender to amplify the difference between a 0 (no contention)
and a 1 (contention). Third, we leave the prefetchers on, as
we saw that they enable the sender to create more contention.
We create a proof-of-concept implementation of our covert
channel, where the sender and the receiver are single-threaded
and agree on a ﬁxed bit transmission interval. Figure 5 shows
the load latency measured by the receiver on our Coffee Lake
3.00 GHz CPU, given receiver and sender conﬁgurations
Rc = 3, Rs = 2 and Sc = 4, Ss = 1, respectively. For this ex-
periment, the sender transmits a sequence of alternating ones
and zeros with a transmission interval of 3,000 cycles (equiv-
alent to a raw bandwidth of 1 Mbps). The results show that
ones (hills) and zeros (valleys) are clearly distinguishable. To
evaluate the performance and robustness of our implementa-
tion with varying transmission intervals, we use the channel
capacity metric (as in [72, 79]). This metric is computed by
multiplying the raw bandwidth with 1− H(e), where e is the
probability of a bit error and H is the binary entropy function.
Figure 6 shows the results on our Coffee Lake CPU, with a
channel capacity that peaks at 3.35 Mbps (418 KBps) given a
transmission interval of 750 cycles (equivalent to a raw band-
Figure 6: Performance of our covert channel implementation
on Coffee Lake, reported using raw bandwidth (bits transmit-
ted per second), error probability (percentage of bits received
wrong), and channel capacity, which takes into account both
bandwidth and error probability to evaluate performance un-
der the binary symmetric channel model (as in, e.g., [72,79]).
width of 4 Mbps). To our knowledge, this is the largest covert
channel capacity of all existing cross-core covert channels
that do not rely on shared memory to date (e.g., [79,105]). We
achieve an even higher capacity of 4.14 Mbps (518 KBps) on
our Skylake 4.00 GHz CPU by using a transmission interval
of 727 cycles, and show the results in Appendix A.2.
Finally, we remark that while our numbers represent a real,
reproducible end-to-end capacity, they were collected in the
absence of background noise. Noisy environments may re-
duce the covert channel performance and require including
in the transmission additional error correction codes (as in,
e.g., [23, 38, 67]), that we do not take into account.
5 Cross-core Side Channels
In this section, we present two examples of side channel at-
tacks that exploit contention on the ring interconnect.
Basic Idea
In both our attacks, we implement the attacker
using the technique described in Section 3 (cf. Listing 1). The
attacker (receiver) issues loads that travel over a ﬁxed segment
of the ring interconnect and measures their latency. We will
refer to each measured load latency as a sample, and to a col-
lection of many samples (i.e., one run of the attacker/receiver)
as a trace. If during an attack the victim (sender) performs
memory accesses that satisfy the conditions of Equations 1
and 2 to contend with the attacker’s loads, the attacker will
measure longer load latencies. Generally, the slices accessed
by an unwitting victim will be uniformly distributed across
the LLC [47]. Therefore, it is likely that some of the victim’s
accesses will contend with the attacker’s loads. If the delays
measured by the attacker can be attributed to a victim’s secret,
the attacker can use them as a side channel.
Threat Model and Assumptions We assume that SMT is
off [9, 18, 64] and that multicore cache-based attacks are not
possible (e.g., due to partitioning the LLC [61, 71, 89] and
disabling shared memory across security domains [99, 115]).
For our attack on cryptographic code, we also assume that
i) the administrator has conﬁgured the system to cleanse the
654    30th USENIX Security Symposium
USENIX Association
01000020000300004000050000Time (cycles)160180200220Latency (cycles)12345678Raw bandwidth (Mbps)123Capacity (Mbps)Error probabilityCapacity0.00.10.20.3Error probabilityvictim’s cache footprint on context switches (to block cache-
based preemptive scheduling attacks [16,30–32,34,40,41,74,
77, 89, 96, 114]) and ii) the attacker can observe multiple runs
of the victim. We assume an attacker who has knowledge of
the contention model (Section 3) for the victim’s machine and
can run unprivileged code on the victim’s machine itself.
5.1 Side Channel Attack On Cryptographic Code
E2();
foreach bit b in key k do
E1();
if b == 1 then
Algorithm 1: Key-
dependent control ﬂow.
Our ﬁrst attack targets a victim
that follows the pseudocode
of Algorithm 1, where E1 and
E2 are separate functions exe-
cuting different operations on
some user input (e.g., a cipher-
text). This is a common pattern
in efﬁcient implementations of
cryptographic primitives that is exploited in many existing
side channel attacks against, e.g., RSA [35, 77, 106, 108], El-
Gamal [62,112], DSA [78], ECDSA [14] and EdDSA [35,36].
Let us consider the ﬁrst iteration of the victim’s loop, and,
for now, assume that the victim starts from a cold cache,
meaning that its code and data are uncached (no prior execu-
tions). When the victim executes E1 for the ﬁrst time, it has to
load code and data words used by E1 into its private caches,
through the ring interconnect. Then, there are 2 cases: when
the ﬁrst key bit is 0 and when it is 1. When the ﬁrst bit is 0, the
victim’s code skips the call to E2 after E1 and jumps to the
next loop iteration by calling E1 again. At this second E1 call,
the words of E1 are already in the private caches of the victim,
since they were just accessed. Therefore, the victim does not
send trafﬁc onto the ring interconnect during the second call
to E1. In contrast, when the ﬁrst bit is 1, the victim’s code
calls E2 immediately after the ﬁrst E1. When E2 is called for
the ﬁrst time, its code and data words miss in the cache and
loading them needs to use the ring interconnect. The attacker
can then infer whether the ﬁrst bit was 0 or 1 by detecting
whether E2 executed after E1. Contention peaks following
E1’s execution imply that E2 executed and that the ﬁrst secret
bit was 1, while no contention peaks following E1’s execution
imply that the call to E1 was followed by another call to E1
and that the ﬁrst secret bit was 0.