17/06/09 20:11:11 INFO storage.BlockManager: Found block rdd_42_24 locally | 3-grams | # appearance |
|---|---|---|---|| 17/06/09 20:10:46 INFO rdd.HadoopRDD: Input split: hdfs://hostname/2kSOSP.log:21876+7292 17/06/09 20:10:46 INFO rdd.HadoopRDD: Input split: hdfs://hostname/2kSOSP.log:14584+7292 17/06/09 20:10:46 INFO rdd.HadoopRDD: Input split: hdfs://hostname/2kSOSP.log:0+7292 17/06/09 20:10:46 INFO rdd.HadoopRDD: Input split: hdfs://hostname/2kSOSP.log:7292+7292 17/06/09 20:10:46 INFO rdd.HadoopRDD: Input split: hdfs://hostname/2kSOSP.log:29168+7292 17/06/09 20:11:11 INFO storage.BlockManager: Found block rdd_42_20 locally  17/06/09 20:11:11 INFO storage.BlockManager: Found block rdd_42_22 locally  17/06/09 20:11:11 INFO storage.BlockManager: Found block rdd_42_23 locally  17/06/09 20:11:11 INFO storage.BlockManager: Found block rdd_42_24 locally |17/06/09 20:10:46 INFO rdd.HadoopRDD: Input split: hdfs://hostname/2kSOSP.log:21876+7292 17/06/09 20:10:46 INFO rdd.HadoopRDD: Input split: hdfs://hostname/2kSOSP.log:14584+7292 17/06/09 20:10:46 INFO rdd.HadoopRDD: Input split: hdfs://hostname/2kSOSP.log:0+7292 17/06/09 20:10:46 INFO rdd.HadoopRDD: Input split: hdfs://hostname/2kSOSP.log:7292+7292 17/06/09 20:10:46 INFO rdd.HadoopRDD: Input split: hdfs://hostname/2kSOSP.log:29168+7292 17/06/09 20:11:11 INFO storage.BlockManager: Found block rdd_42_20 locally  17/06/09 20:11:11 INFO storage.BlockManager: Found block rdd_42_22 locally  17/06/09 20:11:11 INFO storage.BlockManager: Found block rdd_42_23 locally  17/06/09 20:11:11 INFO storage.BlockManager: Found block rdd_42_24 locally |Input->split:->hdfs://hostname/2kSOSP.log:21876+7292 split:->hdfs://hostname/2kSOSP.log:21876+7292->Input hdfs://hostname/2kSOSP.log:21876+7292->Input->split: ... split:->hdfs://hostname/2kSOSP.log:29168+7292->Found hdfs://hostname/2kSOSP.log:29168+7292->Found->block Found->block->rdd_42_20  block->rdd_42_20->locally  rdd_42_20->locally->Found  locally->Found->block  ... |1  1  1  1  1  1  1  1  1  3  1 ||   |  |Input->split:->hdfs://hostname/2kSOSP.log:21876+7292 split:->hdfs://hostname/2kSOSP.log:21876+7292->Input hdfs://hostname/2kSOSP.log:21876+7292->Input->split: ... split:->hdfs://hostname/2kSOSP.log:29168+7292->Found hdfs://hostname/2kSOSP.log:29168+7292->Found->block Found->block->rdd_42_20  block->rdd_42_20->locally  rdd_42_20->locally->Found  locally->Found->block  ... |1  1  1  1  1  1  1  1  1  3  1 || Input split: hdfs://hostname/2kSOSP.log:21876+7292 Input split: hdfs://hostname/2kSOSP.log:14584+7292 Input split: hdfs://hostname/2kSOSP.log:0+7292 Input split: hdfs://hostname/2kSOSP.log:7292+7292 Input split: hdfs://hostname/2kSOSP.log:29168+7292 Found block rdd_42_20 locally  Found block rdd_42_22 locally  Found block rdd_42_23 locally  Found block rdd_42_24 locally |   |2-grams |# appearance || Input split: hdfs://hostname/2kSOSP.log:21876+7292 Input split: hdfs://hostname/2kSOSP.log:14584+7292 Input split: hdfs://hostname/2kSOSP.log:0+7292 Input split: hdfs://hostname/2kSOSP.log:7292+7292 Input split: hdfs://hostname/2kSOSP.log:29168+7292 Found block rdd_42_20 locally  Found block rdd_42_22 locally  Found block rdd_42_23 locally  Found block rdd_42_24 locally |   |Input->split:  split:->hdfs://hostname/2kSOSP.log:21876+7292 hdfs://hostname/2kSOSP.log:21876+7292->Input ... hdfs://hostname/2kSOSP.log:29168+7292->Found Found->block  block->rdd_42_20  rdd_42_20->locally  locally->Found  ... |5  1  1  1  1  4  1  1  4  1 |Fig. 2. A running example of generating n-gram dictionary.
based parsers and they found that Drain [22] achieved the best performance in terms of accuracy and efficiency. Our n-gram-based log parser achieves a much faster parsing speed and a comparable parsing accuracy compared to Drain.
3.2 	Applications of Log ParsingLog parsing is usually a prerequisite for various log analysis tasks, such as anomaly detection [5], [6], [7], [8], [9], failure diagnosis [10], [11], performance diagnosis and improve-ment [12], [13], and system comprehension [10], [14]. For example, Fu et al. [8] first parse the raw log messages to extract log events. Based on the extracted event sequences, they then learn a Finite State Automaton (FSA) to represent the normal work flow, which is in turn used to detect anomalies in new log files. Prior work [20] shows that the accuracy of log parsing is critical to the success of log analysis tasks. Besides, as the size of log files grows fast [1], [2], [27], a highly efficient log parser is important to ensure that the log analysis tasks can be performed in a timely manner. In this work, we propose a log parsing approach that is not only accurate but also efficient, which can benefit future log analysis research and practices.4 	APPROACH
In this section, we present our automated log parsing ap-proach that is designed using n-gram dictionaries.
4.1 	Overview of LogramOur approach consists of two steps: 1) generating n-gram dictionaries and 2) parsing log messages using n-gram dictionaries. In particular, the first step generates n-grams from log messages and calculate the number of appearances of each n-gram. In the second step, each log message is transformed into n-grams. By checking the number of ap-pearance of each n-gram, we can automatically parse the log message into static text and dynamic variables. Figure 2 and 3 show the overview of our approach with a running example.4.2 	Generating an n-gram dictionary
4.2.1 	Pre-processing logsIn this step, we extract a list of tokens (i.e., separated words) from each log message. First of all, we extract the content of a log message by using a pre-defined regular expression. For example, a log message often starts with the time stamp , the log level, and the logger name. Since these parts of logs often follow a common format in the same software system (specified by the configuration of logging libraries), we can directly parse and obtain these information. For example, a log message from the running example in Figure 2, i.e., “17/06/09 20:11:11 INFO stor-age.BlockManager: Found block rdd 42 24 locally”, “17/06/09 20:11:11” is automatically identified as time stamp, “INFO”is identified as the log level and “Storage.BlockManager:” is identified as the logger name; while the content of the log is “Found block rdd 42 24 locally”. After getting the content of each log message, we split the log message into tokens. The log message is split with white-space characters (e.g., space and tab). Finally, there exist common formats for some special dynamic information in logs, such as IP address and email address.In order to have a unbiased comparison with other existing log parsers in the LogPai benchmark (cf. Section 5), we leverage the openly defined regular expressions that are available from the LogPai benchmark to identify such dynamic information.
4.2.2 	Generating an n-gram dictionaryWe use the log tokens extracted from each log message to create an n-gram dictionary. Naively, for a log message with m tokens, one may create an n-gram where n ≤ m. However, when m has the same value as n, the phrases with n-grams are exactly all log messages. Such a dictionary is not useful since almost all log messages have tokens that are generated from dynamic variables. On the other hand, a small value of n may increase the chance that the text generated by a dynamic variable has multiple appearances. A prior study [61] finds that the repetitiveness of an n-gram in logs starts to become stable when n ≤ 3. There-fore, in our approach, we generate the dictionary using5
| Input split: hdfs://hostname/2kSOSP.log:29168+7292 
Found block rdd_42_20 locally 
Found block rdd_42_22 locally	   | 3-grams | # appearance | # appearance |    |
|---|---|---|---|---|| Input split: hdfs://hostname/2kSOSP.log:29168+7292  Found block rdd_42_20 locally  Found block rdd_42_22 locally	   |split:->hdfs://hostname/2kSOSP.log:29168+7292->Found  hdfs://hostname/2kSOSP.log:29168+7292->Found->block  Found->block->rdd_42_20  block->rdd_42_20->locally  rdd_42_20->locally->Found  locally->Found->block 	 |split:->hdfs://hostname/2kSOSP.log:29168+7292->Found  hdfs://hostname/2kSOSP.log:29168+7292->Found->block  Found->block->rdd_42_20  block->rdd_42_20->locally  rdd_42_20->locally->Found  locally->Found->block 	 |1  1  1  1  1  3 |   || Input split: hdfs://hostname/2kSOSP.log:29168+7292  Found block rdd_42_20 locally  Found block rdd_42_22 locally	   |split:->hdfs://hostname/2kSOSP.log:29168+7292->Found  hdfs://hostname/2kSOSP.log:29168+7292->Found->block  Found->block->rdd_42_20  block->rdd_42_20->locally  rdd_42_20->locally->Found  locally->Found->block 	 |split:->hdfs://hostname/2kSOSP.log:29168+7292->Found  hdfs://hostname/2kSOSP.log:29168+7292->Found->block  Found->block->rdd_42_20  block->rdd_42_20->locally  rdd_42_20->locally->Found  locally->Found->block 	 |1  1  1  1  1  3 |  ||    |split:->hdfs://hostname/2kSOSP.log:29168+7292->Found  hdfs://hostname/2kSOSP.log:29168+7292->Found->block  Found->block->rdd_42_20  block->rdd_42_20->locally  rdd_42_20->locally->Found  locally->Found->block 	 |split:->hdfs://hostname/2kSOSP.log:29168+7292->Found  hdfs://hostname/2kSOSP.log:29168+7292->Found->block  Found->block->rdd_42_20  block->rdd_42_20->locally  rdd_42_20->locally->Found  locally->Found->block 	 |1  1  1  1  1  3 | | 
|  |  |    |    | 2-grams | # appearance |    |
|---|---|---|---|---|---|---|
|  | |  |  |hdfs://hostname/2kSOSP.log:29168+7292->Found Found->block  block->rdd_42_20  rdd_42_20->locally  locally->Found |1  4  1  1  4 |   ||  | |  |  |hdfs://hostname/2kSOSP.log:29168+7292->Found Found->block  block->rdd_42_20  rdd_42_20->locally  locally->Found |1  4  1  1  4 |  |
|     | |  |  |hdfs://hostname/2kSOSP.log:29168+7292->Found Found->block  block->rdd_42_20  rdd_42_20->locally  locally->Found |1  4  1  1  4 | ||     | |block->rdd_42_20  rdd_42_20->locally |  |hdfs://hostname/2kSOSP.log:29168+7292->Found Found->block  block->rdd_42_20  rdd_42_20->locally  locally->Found |1  4  1  1  4 | |
|  | |block->rdd_42_20  rdd_42_20->locally |  |hdfs://hostname/2kSOSP.log:29168+7292->Found Found->block  block->rdd_42_20  rdd_42_20->locally  locally->Found |1  4  1  1  4 | ||   |  |block->rdd_42_20  rdd_42_20->locally |  |hdfs://hostname/2kSOSP.log:29168+7292->Found Found->block  block->rdd_42_20  rdd_42_20->locally  locally->Found |1  4  1  1  4 | |
Fig. 3. A running example of parsing one log message using the dictionary. The n-gram dictionary is abbreviated using “...” to avoid repetitive similar items.phrases with two or three words (i.e., 2-grams and 3-grams). Naively, one may generate the dictionary by processing every single log message independently. However, such a naive approach has two limitations: 1) some log events may span across multiple lines and 2) the beginning and the ending tokens of a log message may not reside in the same number of n-grams like other tokens (c.f. our parsing step“Identifying dynamically and statically generated tokens”in Section 4.3.2), leading to potential bias of the tokens being considered as dynamic variables. Therefore, at the beginning and the ending tokens of a log message, we also include the end of the prior log message and the beginning of the following log message, respectively, to create n-grams. For example, if our highest n in the n-gram is 3, we would check two more tokens at the end of the prior log message and the beginning of the following log message. In addition, we calculate the number of occurrences of each n-gram in our dictionary.As shown in a running example in Figure 2, a dictionary from the nine lines of logs is generated consisting of 3-grams and 2-grams. Only one 3-grams, “locally->Found->block”, in the example have multiple appearance. Three 2-grams,“Found->block”, “Input->split:” and “locally->Found”, have four to five appearances. In particular, there exists n-grams, such as the 3-gram “locally->Found->block”, that are gen-erated by combining the end and beginning of two log messages. Without such combination, tokens like “input”,“Found” and “locally” will have lower appearance in the dictionary.4.3 	Parsing log messages using an n-gram dictionary
In this step of our approach, we parse log messages using the dictionary that is generated from the last step.
4.3.1 
ables Identifying n-grams that may contain dynamic vari-Similar to the last step, each log message is transformed into n-grams. For each n-gram from the log message, we check its number of appearances in the dictionary. If the number of occurrence of a n-gram is smaller than a automatically determined threshold (see Section 4.3.3), we consider that the n-gram may contain a token that is generated from dynamic variables. In order to scope down to identify the dynamically generated tokens, after collecting all low-appearing n-grams, we transform each of these n-gramsinto n − 1-grams, and check the number of appearance of each n − 1-gram. We recursively apply this step until we have a list of low-appearing 2-grams, where each of them may contain one or two tokens generated from dynamic variables. For our running example shown in Figure 3, we first transform the log message into two 3-grams, while both only have one appearance in the dictionary. Hence, both 3-grams may contain dynamic variables. Afterwards, we transform the 3-grams into three 2-grams. One of the 2-grams (“Found->block”) has four appearances; while the other two 2-grams (“block->rdd 42 20” and “rdd 42 20->locally”) only have one appearance. Therefore, we keep the two 2-grams to identify the dynamic variables.4.3.2 	Identifying dynamically and statically generated to-kens
From the last step, we obtain a list of low-appearing 2-grams. However, not all tokens in the 2-grams are dynamic variables. There may be 2-grams that have only one dy-namically generated token while the other token is static text. In such cases, the token from the dynamic variable must reside in two low-appearing 2-grams (i.e., one ends with the dynamic variable and one starts with the dynamic variable). For all other tokens, including the ones that are now selected in either this step or the last step, we consider them as generated from static text.However, a special case of this step is the beginning and ending tokens of each log message (c.f. the previous step “Generating an n-gram dictionary” in Section 4.2.2). Each of these tokens would only appear in smaller number of n-grams. For example, the first token of a log message would only appear in one 2-gram. If these tokens of the log message are from static text, they may be under-counted to be considered as potential dynamic variables. If these tokens of the log message are dynamically generated, they would never appear in two 2-grams to be identified as dynamic variable. To address this issue, for the beginning and ending tokens of each log message, we generate additional n-grams by considering the ending tokens from the prior log message; and for the ending tokens of each log message, we generate additional n-grams by considering the beginning tokens from the next log message.For our running example shown in Figure 3, “rdd 42 20”is generated from dynamic variable and it reside in two 2-grams (“block->rdd 42 20” and “rdd 42 20->locally”. There-fore, we can identify token “rdd 42 20” as a dynamic vari-
6
able, while “block” and “locally” are static text. On the other hand, since “hdfs://hostname/2kSOSP.log:29168+7292->Found” only appear without overlapping to-kens with others, we ignore this 2-gram for identifying dynamic variables.
4.3.3 	Automatically determining the threshold of n-gram occurrencesThe above identification of dynamically and statically gen-erated tokens depends on a threshold of the occurrences of n-grams. In order to save practitioners’ effort for manually searching the threshold, we use an automated approach to estimate the appropriate threshold. Our intuition is that most of the static n-grams would have more occurrences while the dynamic n-grams would have fewer occurrences. Therefore, there may exist a gap between the occurrences of the static n-grams and the occurrences of the dynamic n-grams, i.e., such a gap helps us identify a proper threshold automatically.In particular, first, we measure the occurrences of each n-gram. Then, for each occurrence value, we calculate the number of n-grams that have the exact occurrence value. We use a two-dimensional coordinate to represent the oc-currence values (i.e., the X values) and the number of n-grams that have the exact occurrence values (i.e., the Y values). Then we use the loess function [62] to smooth the Y values and calculate the derivative of the Y values against the X values. After getting the derivatives, we use Ckmeans.1d.dp [63], a one-dimensional clustering method, to find a break point to separate the derivatives into two groups, i.e., a group for static n-grams and a group for dy-namic n-grams. The breaking point would be automatically determined as the threshold.4.3.4 	Generating log templates
Finally, we generate log templates based on the tokens that are identified as dynamically or statically generated. We follow the same log template format as the LogPai bench-mark [21], in order to assist in further research. For our running example shown in Figure 3, our approach parses the log message “Found block rdd 42 20 locally” into “Found block $1 locally, $1=rdd 42 20”.5 	EVALUATION
In this section, we present the evaluation of our approach. We evaluate our approach by parsing logs from the LogPai benchmark [21]. We compare Logram with five automated log parsing approaches, including Drain [22], Spell [59], AEL [23], Lenma [58] and IPLoM [60] that are from prior re-search and all have been included in the LogPai benchmark. We choose these five approaches since a prior study [21] finds that these approaches have the highest accuracy and efficiency among all of the evaluated log parsers. In partic-ular, we evaluate our approach on four aspects:Accuracy. The accuracy of a log parser measures whether it can correctly identify the static text and dynamic variables in log messages, in order to match log messages with the correct log events. A prior study [20] demonstrates the im-portance of high accuracy of log parsing, and low accuracy
of log parsing can cause incorrect results (such as false positives) in log analysis.Efficiency. Large software systems often generate a large amount of logs during run time [64]. Since log parsing is typically the first step of analyzing logs, low efficiency in log parsing may introduce additional costs to practitioners when doing log analysis and cause delays to uncover im-portant knowledge from logs.Ease of stabilisation. Log parsers typically learn knowledge from existing logs in order to determine the static and dynamic components in a log message. The more logs seen, the better results a log parser can provide. It is desired for a log parser to have a stable result with learning knowledge from a small amount of existing logs, such that parsing the log can be done in a real-time manner without the need of updating knowledge while parsing logs.Scalability. Due to the large amounts of log data, one may consider leveraging parallel processing frameworks, such as Hadoop and Spark, to support the parsing of logs [65]. However, if the approach of a log parser is difficult to scale, it may not be adopted in practice.
5.1 	Subject log dataWe use the data set from the LogPai benchmark [21]. The data sets and their descriptions are presented in Table 2. The benchmark includes logs produced by both open source and industrial systems from various domains. These logs are typically used as subject data for prior log analysis research, such as system anomaly detection [66], [67], system issue diagnosis [68] and system understanding [69]. To assist in automatically calculating accuracy on log parsing (c.f., Section5.2), each data set in the benchmark includes a subset of 2,000 log messages that are already manually labeled with log event. Such manually labeled data is used in evaluating the accuracy of our log parser. For the other three aspects of the evaluation, we use the entire logs of each log data set.TABLE 2 
The subject log data used in our evaluation.
Platform Description Size
| Android | Android framework log | 183.37MB |
|---|---|---|
| Apache |Apache server error log |4.90MB |
| BGL |Blue Gene/L supercomputer log |708.76MB |
| Hadoop |Hadoop mapreduce job log |48.61MB |
| HDFS |Hadoop distributed file system log |1.47GB |
| HealthApp |Health app log |22.44MB |
| HPC |High performance cluster log |32.00MB || Linux |Linux system log |2.25MB |