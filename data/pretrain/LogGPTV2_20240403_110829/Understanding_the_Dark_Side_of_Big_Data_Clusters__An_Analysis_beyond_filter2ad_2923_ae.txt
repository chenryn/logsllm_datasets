J
1
0.8
0.6
0.4
0.2
0
Eviction
Fail
Kill
Finish
 =   1 d a y
Eviction
Fail
Kill
Finish
(0,0.5)
[0.5,0.9)
[0.9,1.1)
[1.1,1.5)
>=1.5
Machine locality
(c) Machine locality.
Fig. 9.
Job rates vs. job size, job execution time, and machine locality.
smaller difference between the maximum and minimum values
across reservation levels. This suggests that CPU reservation
has higher impact than RAM reservation on the event type.
Taking eviction as an example, stable eviction rates across
different levels of RAM reservation indicate that the decision
of evict tasks is mainly based on allocated CPU.
3) Resource Utilization: Differently from the previous
analysis, here we study whether resources actually used by
tasks affect the event rates. In particular, we study the metric
of resource utilization, deﬁned as the total amount of resources
used by co-executed tasks at event
time, divided by the
resources equipped on the machine. We summarize how event
rates change with respect to CPU and RAM utilization in
Figures 8(e) and 8(f), respectively. The median values of
CPU and RAM utilization at event time are 0.37 and 0.45,
respectively. Consequently, most of events fall in the ﬁrst four
ranges of resource utilization in Figures 8(e) and 8(f).
All types show similar trends relative to CPU and memory
utilization, especially for utilization greater than 0.8, indicated
by the similar plots in Figures 8(e) and 8(f). As for the kill
rate, one can see that it increases monotonically, relatively
to both CPU and RAM utilization. Consequently, when CPU
utilization is greater than 1,
tasks are killed with a high
probability. Such an observation can be attributed to the
scheduler, which kills tasks due to shortage of resources. On
the contrary, eviction is not affected by resource utilization, as
indicated by the stable rate, i.e., below 0.1, for both CPU and
RAM. The highest rates for ﬁnish events occur when CPU is
moderately utilized, i.e., when CPU utilization ranges between
0.4 and 0.8.
In addition to the analyses presented, we also studied
how the event rates change according to different amounts
of machine capacity, considering both CPU and RAM. Our
analysis shows constant trends of event rates across different
ranges of machine capacity. We do not show any ﬁgures
because of lack of space, and conclude that machine capacity
has no impact on the event rates.
Key messages. Eviction is more affected by resource reservation,
particularly CPU, whereas kill
is more affected by resource
utilization. Tasks are executed successfully with fairly constant
probability across all resource reservation levels. Moreover, the
probability of successful ﬁnish is high when requested resources
and CPU utilization are moderate, i.e., around 0.04 and 0.6,
respectively. Consequently, to minimize unsuccessful executions,
resource management policies shall ensure optimal CPU utiliza-
tion and concurrency.
C. Jobs
i.e.,
job size,
In this section, we inspect three possible causes that affect
the job event rate,
job execution time, and
machine locality. Recalling from the distribution of different
job types, jobs are dominated by ﬁnish and kill, accounting for
57.7% and 40.7% of jobs, respectively. Almost all unsuccessful
jobs are killed. Consequently, the eviction and fail job rates are
very low and close to zero in most of the analysis that follows,
whereas the kill and ﬁnish rates show opposite trends with
respect to all attributes considered. Therefore, we particularly
focus on kill jobs and ﬁnish jobs in our analysis.
1) Job Size: The question we address here is whether jobs
of a certain size cause frequent occurrences of particular job
types. Recall that we deﬁne the size of a job as its number of
tasks, i.e., big jobs have a high number of tasks. We compute
the job rate of different types, relative to ﬁve ranges of job
size in Figure 9(a). One can observe that the kill (ﬁnish) job
rate increases (decreases) with the job size exponentially4,
corresponding well to our previous ﬁnding that ﬁnish jobs
contain on average 2.7 tasks. Clearly, the job size is a strong
indicator of the job success rate, and users tend to kill bigger
jobs that naturally have much higher probabilities to suffer
from bad system conditions. In contrast, the latest trend of
big-data applications is to have jobs of large size, composed
of a lot of tasks that only process small data sets [1], in order to
mitigate long-tailed response times. However, our observations
in this section do not support such a trend, which overlooks the
dependability issues related to large job size in today’s systems.
The lesson learned here is that the system design for big-data
applications should carefully incorporate the unsuccessful rates
of jobs, tasks and events, and evaluate their tradeoff with tail
response time.
2) Job Execution Time: In this section, we aim to under-
stand whether the job execution time affects the user decision
to kill jobs and thus alters their ﬁnish rate. Similarly to our
previous analysis on the event execution time, results in this
section are the same considering or not considering queue time.
We summarize the trends of job rates per type in Figure 9(b).
On the one hand, users tend to kill jobs with rates close to
0.5 when jobs are executing for more than 2 min. Once the
job execution time is longer than one day, jobs are killed with
a rate higher than 0.9. We note that these two values, i.e.,
2 min and one day, are also the turning points for the kill
event rate in Figure 7(b). The values of the kill rates observed
here are much higher than those at event level. This can be
4We further veriﬁed this claim by observing the trends of job rates with
ranges of the same length.
216216
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:48:35 UTC from IEEE Xplore.  Restrictions apply. 
explained by the fact that a single kill event can easily lead
to a kill job, as shown in Table 5. On the other hand, the
ﬁnish job rate shows a decreasing trend in a step-like function,
with turning points at 2 min and one day. Another observation
worth mentioning is that the fail rate also increases slightly for
jobs longer than one day. As a result, it is nearly impossible
for jobs older than one day to be completed. To mitigate
the negative performance impact due to unsuccessful jobs,
systems could adopt certain signaling messages that alert users
to the probability of jobs being killed or failed based on their
execution time. As consequence, users can decide whether to
take proactive actions, such as to kill jobs sooner, in case of
low probability of successful completion.
3) Machine Locality: Finally, here we want to identify the
impact of machine locality on different job type rates. When
this value is equal to one, all events experienced by a job
happen on the same set of machines, so as to explore data
locality. Smaller values (1) show that tasks are executed on
different machines during their life. On the one hand, to take
advantage of data locality, one would expect this value to be
very small. On the other hand, because of the tendency to have
multiple events per task, it is not surprising if these values are
high. We see that roughly 2% of jobs have a value lower than
0.9, 5% of jobs have a value greater than 1.1, and 93% of jobs
have a value between 0.9 and 1.1. This observation implies
that here the scheduling policy attempts to keep all tasks of
the same job as well as all of its (subsequent) events on a ﬁxed
set of machines.
We plot the trends of job rates per type relative to different
ranges of machine locality in Figure 9(c). We note that the
distribution of the number of jobs in each range is highly
uneven, as the left (right) part of the ﬁgure accounts for 2%
(5%) of all jobs, whereas the central part, i.e., where the
machine locality ranges between 0.9 and 1.1, represents 93%
of the jobs. Our main observation is that the current practice of
keeping tasks on the same set of machines indeed results in the
highest ﬁnish job rate. When machine locality is much higher
than one, jobs are killed with higher probabilities. Moreover,
probabilities of both kill and fail jobs are higher than the ﬁnish
one once the machine locality is greater than or equal to 1.5.
These observations imply that executing events of the same
tasks on different machines lowers the probability of successful
job completion.
Our observations lead us to conclude that (1) keeping all
events of a single job on the same set of machines, and (2)
keeping the number of machines close to the job size are
actions that strike a good balance between data locality and
job performance.
Key messages. The job size has a high impact on kill
jobs,
showing the need to consider the kill probability when deciding
the number of tasks per job. A high job execution time appears
to be a crucial parameter for users’ decision to kill jobs. A
feedback mechanism indicating to users the job kill probability
over time might be useful to avoid resource waste. Keeping a
neutral machine locality, i.e., job size equal to the number of
unique machines used by the tasks of the job, can result in a high
success rate.
VI. RELATED STUDIES
Workload analysis has been one of the key aspects in
designing datacenters and improving application/system de-
pendability for big-data clusters, whose systems and appli-
cations grow extremely complex. On the one hand, recent
characterization studies, based on several traces from major
datacenter operators, e.g., Google [16], and Yahoo! [20], aim
to capture the resource utilization and statistical properties
of workloads. On the other hand, although numerous ﬁeld
studies on hardware failures have been made [10], [21], [22],
unsuccessful executions of big-data-like applications have been
rarely discussed. We summarize the studies related to our work
in two categories: (1) characterization studies on the same
Google trace, and (2) analyses of failures, availability and
reliability of systems.
a) Google Trace Analysis: The trace [16] provided by
Google has been previously analyzed by several studies, each
of which with different focus, such as resource and workload
heterogeneity [6], latency-sensitivity of jobs [23], automatic
clustering of tasks [7], and comparison with grid workload [5].
However, none of these works studies unsuccessful executions
and their impact on the system exhaustively. Only a few recent
studies analyze unsuccessful executions in this trace. C¸ avdar
et al. [15] present the distribution of inter-arrival times and
resource consumptions with respect to task priorities, with
particular focus on task preemption. Garragnan et al. [14]
study failures and repairs occurred to tasks and machines in
the system. Chen et al. [13] analyze the general characteristics
of job and task failures in this trace. Ros`a et al. focus on
the characteristic of task eviction [24] and prediction of job
failures [25]. However, aforementioned studies address only
a subset of unsuccessful executions for a limited granularity
of workloads, i.e., either jobs or tasks, and thus fall short in
discovering the dependencies among tasks and jobs and the
root causes of unsuccessful executions.
b) Failures Analysis: Motivated by the high negative
impact of unresponsive and failing systems, related studies
try to shed light on failure patterns and causes for different
kinds of computing infrastructures, such as high-performance
computing [9], [26] and distributed systems [27], [28]. The
main focus is on the reliability of single components, par-
ticularly disks [21], [22], network [28], [29], memory [10],
[30], operating systems [31], [32] and virtual machines [27].
They extensively discuss the patterns and root causes, but
rarely capture the impact on the application performance, e.g.,
response time, except Potharaju et al. [28] who quantiﬁed the
impact of network failures on the Service Level Agreement
(SLA). In contrast, we cover multiple types of unsuccessful
executions,
three levels of
workload granularity, i.e., events, tasks, and jobs, for big-data-
like applications.
fail, and kill, at
i.e., eviction,
VII. CONCLUDING REMARKS
In this paper, we have conducted an extensive study to
understand the performance impact, patterns, and root causes
of multiple types of unsuccessful events, tasks and jobs in big-
data clusters, particularly, eviction, fail, and kill.
Overall, unsuccessful executions not only slow down the
task response time by a factor of two, but also result in a
217217
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:48:35 UTC from IEEE Xplore.  Restrictions apply. 
signiﬁcant amount of waste of machine time and resource
demand in terms of CPU, RAM and DISK. In particular, fail
requests the greatest resource demand, but uses the smallest,
whereas kill shows the opposite trend. In terms of crucial
patterns among jobs, tasks and events, fail and eviction happen
repetitively on single tasks; ﬁnish jobs have a low number
of tasks and experience a low number of events, and kill
jobs have a high number of tasks and a high percentage of
ﬁnish events. All in all, occurrences of unsuccessful events
lead to unsuccessful tasks and jobs with high probabilities,
especially between kill events and kill jobs, whereas ﬁnish
events are not good predictors for ﬁnish jobs. Last but not
least, the priority and task execution time have a huge impact
on ﬁnish tasks, and the probability of successful termination
increases when few tasks are co-executed in a single machine
that is moderately utilized. Tasks are evicted mainly based
on resource reservation, while tasks are killed due to actual
resource utilization. Moreover, long-running jobs with a lot
of tasks have a high probability of being killed, and keeping
machine locality close to one results in a high probability of
successful job completion. Our work provides not only patterns
to model and predict the dependability of big-data clusters,
but also root causes and insights that are critical for designing
fault- and latency-aware scheduling policies.
Our future work will focus on employing statistical learn-
ing techniques, such as neural networks and support vector
machines, to systematically and quantitatively model all three
types of unsuccessful executions at the job, task, and event
levels, using the features identiﬁed in this study.
ACKNOWLEDGMENT
This work has been supported by the Swiss National
Science Foundation (project 200021 141002) and EU com-
mission under FP7 GENiC project (608826).
REFERENCES
[1] K. Ousterhout, P. Wendell, M. Zaharia, and I. Stoica, “Sparrow:
Distributed, Low Latency Scheduling,” in ACM SOSP, 2013, pp. 69–84.
[2] M. Zaharia, D. Borthakur, J. Sen Sarma, K. Elmeleegy, S. Shenker,
and I. Stoica, “Delay Scheduling: A Simple Technique for Achieving
Locality and Fairness in Cluster Scheduling,” in ACM EuroSys, 2010,
pp. 265–278.
[3] M. Schwarzkopf, A. Konwinski, M. Abd-El-Malek, and J. Wilkes,
“Omega: ﬂexible, scalable schedulers for large compute clusters,” in
ACM EuroSys, 2013, pp. 351–364.
[4] L. Barroso, J. Dean, and U. H¨olzle, “Web Search for a Planet: The
Google Cluster Architecture,” IEEE Micro, vol. 23, no. 2, pp. 22–28,
2003.
[5] S. Di, D. Kondo, and W. Cirne, “Characterization and Comparison of
Cloud versus Grid Workloads,” in IEEE CLUSTER, 2012, pp. 230–238.
[6] C. Reiss, A. Tumanov, G. R. Ganger, R. H. Katz, and M. A. Kozuch,
“Heterogeneity and dynamicity of clouds at scale: Google trace analy-
sis,” in ACM SoCC, 2012, pp. 7:1–7:13.
[7] S. Di, D. Kondo, and C. Franck, “Characterizing Cloud Applications
on a Google Data Center,” in ICPP, 2013, pp. 468–473.
[8] R. Birke, L. Y. Chen, and E. Smirni, “Data Centers in the Cloud: A
Large Scale Performance Study,” in IEEE CLOUD, 2012, pp. 336–343.
[9] B. Schroeder and G. A. Gibson, “A Large-scale Study of Failures in
High-performance Computing Systems,” in IEEE/IFIP DSN, 2006, pp.
249–258.
[10] B. Schroeder, E. Pinheiro, and W.-D. Weber, “DRAM Errors in the
Wild: A Large-scale Field Study,” in ACM SIGMETRICS, 2009, pp.
193–204.
[11] S. Lu, S. Park, E. Seo, and Y. Zhou, “Learning from mistakes — A
Comprehensive Study on Real World Concurrency Bug Characteristics,”
in ASPLOS, 2008, pp. 329–339.
[12] D. Yuan, D. Park, P. Huang, Y. Liu, M. Lee, Y. Zhou, and S. Savage, “Be
Conservative: Enhancing Failure Diagnosis with Proactive Logging,” in
USENIX OSDI, 2012, pp. 293–306.
[13] X. Chen, C.-D. Lu, and K. Pattabiraman, “Failure Analysis of Jobs in
Compute Clouds: A Google Cluster Case Study,” in IEEE ISSRE, 2014,
pp. 167–177.
[14] P. Garraghan, P. Townend, and J. Xu, “An Empirical Failure-Analysis of
a Large-Scale Cloud Computing Environment,” in IEEE HASE, 2014,
pp. 113–120.
[15] D. C¸ avdar, A. Ros`a, L. Y. Chen, W. Binder, and F. Alag¨oz, “Quantifying
the Brown Side of Priority Schedulers: Lessons from Big Clusters,”
SIGMETRICS Perform. Eval. Rev., vol. 42, no. 3, pp. 76–81, Dec. 2014.
J. Wilkes, “More Google cluster data,” Google research blog.
https://code.google.com/p/googleclusterdata/wiki/ClusterData2011 1,
Nov 2011.
[16]
[17] R. Birke, A. Podzimek, L. Chen, and E. Smirni, “State-of-the-practice in
data center virtualization: Toward a better understanding of VM usage,”
in IEEE/IFIP DSN, 2013, pp. 1–12.
[18] B. Cho, M. Rahman, T. Chajed, I. Gupta, C. Abad, N. Roberts,
and P. Lin, “Natjam: Design and Evaluation of Eviction Policies for
Supporting Priorities and Deadlines in Mapreduce Clusters,” in ACM
SoCC, 2013, pp. 6:1–6:17.
[19] K. Trivedi, Probability & Statistics With Reliability, Queuing And
Computer Science Applications, 2nd ed.
John Wiley & Sons, 2008.
[20] S. Kavulya, J. Tan, R. Gandhi, and P. Narasimhan, “An Analysis of
Traces from a Production MapReduce Cluster,” in IEEE/ACM CCGrid,
2010, pp. 94–103.
[21] B. Schroeder and G. A. Gibson, “Disk Failures in the Real World: What
Does an MTTF of 1,000,000 Hours Mean to You?” in USENIX FAST,
2007, pp. 1–16.
[22] L. N. Bairavasundaram, G. R. Goodson, S. Pasupathy, and J. Schindler,
“An Analysis of Latent Sector Errors in Disk Drives,” in ACM SIG-
METRICS, 2007, pp. 289–300.
[23] Z. Liu and S. Cho, “Characterizing Machines and Workloads on a
Google Cluster,” in SRMPDS, 2012, pp. 397–403.
[24] A. Ros`a, L. Y. Chen, R. Birke, and W. Binder, “Demystifying Casualties
of Evictions in Big Data Priority Scheduling,” SIGMETRICS Perform.
Eval. Rev., Mar. 2015.
[25] A. Ros`a, L. Y. Chen, and W. Binder, “Predicting and Mitigating Jobs
Failures in Big Data Clusters,” in IEEE/ACM CCGrid, 2015.
[26] C. Di Martino, Z. Kalbarczyk, R. Iyer, F. Baccanico, J. Fullop, and
W. Kramer, “Lessons Learned from the Analysis of System Failures
at Petascale: The Case of Blue Waters,” in IEEE/IFIP DSN, 2014, pp.
610–621.
[27] R. Birke, I. Giurgiu, L. Chen, D. Wiessmann, and T. Engbersen,
“Failures Analysis of Virtual and Physical Machines: Patterns, Causes
and Characteristics,” in IEEE/IFIP DSN, 2014, pp. 1–12.
[28] R. Potharaju and N. Jain, “When the Network Crumbles: An Empirical
Study of Cloud Network Failures and Their Impact on Services,” in
ACM SoCC, 2013, pp. 15:1–15:17.
J. Sherry, S. Hasan, C. Scott, A. Krishnamurthy, S. Ratnasamy, and
V. Sekar, “Making Middleboxes Someone else’s Problem: Network
Processing As a Cloud Service,” in ACM SIGCOMM, 2012, pp. 13–
24.
[29]
[30] X. Li, K. Shen, M. C. Huang, and L. Chu, “A Memory Soft Error
Measurement on Production Systems,” in USENIX ATC, 2007, pp. 275–
280.
[31] M. Garcia, A. Bessani, I. Gashi, N. Neves, and R. Obelheiro, “OS
Diversity for Intrusion Tolerance: Myth or Reality?” in IEEE/IFIP DSN,
2011, pp. 383–394.
[32] N. Palix, G. Thomas, S. Saha, C. Calv`es, J. Lawall, and G. Muller,
“Faults in Linux: Ten Years Later,” in ASPLOS, 2011, pp. 305–318.
218218
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:48:35 UTC from IEEE Xplore.  Restrictions apply.