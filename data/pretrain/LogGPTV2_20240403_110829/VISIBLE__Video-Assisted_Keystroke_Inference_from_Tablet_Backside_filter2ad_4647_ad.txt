19
12
15.7
P 3
i
VIS
RG
100% 42.2%
100% 69.1%
98.8% 69.1%
100% 65.3%
98.8% 65.3%
100% 76.8%
100% 76.8%
100% 73.0%
100% 53.8%
100% 65.3%
100% 53.8%
100% 46.1%
100% 57.6%
100% 65.3%
100% 42.2%
100% 30.7%
98.8% 42.2%
97.5% 76.8%
100% 53.8%
100% 76.8%
100% 65.3%
100% 73.0%
100% 53.8%
100% 57.6%
100% 73.0%
100% 46.1%
99.8% 60.3%
i , P 2
i , and P 3
versa. Moreover, the P h(i) of VISIBLE is always signiﬁcantly
higher than the corresponding P h(i) of random guess. We also
calculate the average P 1
i of VISIBLE across all
26 keys as 83.6%, 97.9%, and 99.8%, respectively, which are
much higher than corresponding 22.7%, 44.9%, and 60.3%
of random guess. Meanwhile, note that the average P h
i may
only be used with caution to compare the performance of two
different techniques, due to the difference in the size of keys’
neighborhood.
Fig. 10.
Impact of the number of participants.
of VISIBLE for each key ranges from 16.3% for letter "h"
to 77.5% for letter "z". The average pinpoint accuracy of
VISIBLE across all 26 letters is 36.2%, which is almost an
order of magnitude higher than 3.84% of random guess. Since
different keys’ h-hop neighborhoods have different sizes for
the same h, we calculate each P h
for random guess based on
i
the actual number of keys within key i’s h-hop neighborhood
(i.e., |Ωh(i)|) to ensure fair comparisons. We can see that
for both VISIBLE and random guess, the P h
for each key
i
i increases as h increases, which is expected, as the larger
the neighborhood being considered, the higher the probability
that a key inferred as some key in the neighborhood, and vice
9
We also notice that pinpoint and h-hop accuracies of the
letters at corner positions (i.e., "q", "z", "p", and "m") are
higher than those of the letters at the center (e.g., "g" and "h").
This is because typing the letters at corner positions causes
more distinguishable motion patterns than those at the center.
Moreover, we can see that the pinpoint and h-hop accuracies
of letter "z" are much higher than those of other three letters
at the corner positions. The reason behind such disparity is
that our selected AOIs are not evenly distributed. As shown in
Fig. 4(b), the distances between letter "z" and selected AOIs
are greater than those of other letters, and typing "z" thus
causes more distinguishable motion patterns.
Fig. 9 shows the impact of the training set size on the
inference accuracy. As expected, increasing the training set
size can slightly improve the key inference accuracy. Fig. 10
shows the impact of the number of participants in the training
set. We can see that as the number of participants increases,
the key inference accuracy slightly increases in all the cases.
In addition, a small number of participants is sufﬁcient for
PinpointOne-hopTwo-hopThree-hop0.00.20.40.60.81.0 Inference accuracy One participant Three participantsWord
paediatrician
interceptions
abbreviations
impersonating
soulsearching
hydromagnetic
inquisition
Length
13
13
13
13
13
13
11
Word
pomegranate
feasibility
polytechnic
obfuscating
difference
wristwatch
processing
11
11
11
11
10
10
10
unphysical
institute
extremely
sacrament
dangerous
identity
emirates
TABLE II.
LIST OF WORDS USED TO TEST THE ATTACK.
Length
Length
Word
Word
platinum
homeland
security
between
spanish
nuclear
Length
8
8
8
7
7
7
10
9
9
9
9
8
8
Fig. 11. Word inference accuracy.
Fig. 12. Word inference accuracy vs. the word length.
achieving acceptable key inference accuracy, which means that
VISIBLE requires very few attackers to launch.
C. Word Inference Experiment
We now report the experimental results of the word in-
ference attack on the iPad 2 tablet. In this experiment, we
involved two participants and let each participant enter each
word in Table II (which is also used in [21]) once to evaluate
the word inference accuracy. In total, we collected 2 × 27
words with 7∼13 letters, where all the letters in the words are
in lower-case. As in [21], we used the “corn-cob” dictionary
[35] consisting of more than 58, 000 English words. For each
letter in each word, we ﬁrst used the trained multi-class SVM
classiﬁer to predict one key and obtained a candidate key set
consisting of all the keys that are less than two hops from the
predicated key. Then for each word, we obtained a candidate
word list by ﬁltering out the combinations that are not in the
“corn-cob” dictionary [35].
Fig. 11 compares the overall word inference accuracy of
VISIBLE and the technique proposed in [21] for the tested
words in Table II. To enable direct comparison, we view the
size of the candidate word list output by VISIBLE as the lowest
possible rank of the correct word in the candidate word list if
the correct word is in candidate word list. In other words, if a
candidate word list of c words contains the correct word typed
by the victim, then we say the correct word is among the top-k
candidate words for any k ≥ c. As shown in Fig. 11, the correct
word is among the top-5 candidate words output by VISIBLE
48% of the time, which means that nearly half of the words in
Table II have a candidate word list with no more than 5 words.
Besides, we can see that the correct word is among top-10, top-
25, and top-50 candidate words with probabilities 63%, 78%,
and 93%, respectively. In contrast, the technique in [21] infers
the correct word in top-10, top-25, and top-50 candidate words
with probabilities 43%, 61%, and 73%, respectively. VISIBLE
thus achieves much higher accuracy for word inference than
[21].
Fig. 12 shows that word inference accuracy increases as
the word length increases. Two reasons account for this trend.
First, a longer word has more constraints in letter combinations
and thus fewer candidate words in the dictionary. Second,
according to statistics of English words, the number of words
of length seven is the largest among all words, so words with
seven letters have the most candidate words in the dictionary,
which leads to a lower inference accuracy.
D. Sentence Inference Experiment
Next, we report VISIBLE’s performance for inferring com-
plete sentences on the iPad 2 tablet. For this experiment, we
used Enron Email Dataset [37]–[39], which comprises over
600,000 emails generated by 158 employees of the Enron
Corporation and is also used in previous work [40] to test the
sentence inference accuracy. We asked one participant to select
two sentences from the dataset and enter the selected sentences
using the alphabetical keyboard of the iPad 2 tablet. The
attacker video-recorded the sentence-entry process using two
camcorders and used a multi-class SVM classiﬁer trained by
the keystroke data. The attacker then performed word inference
to obtain a candidate word list for each word and ﬁnally chose
one word from each candidate word list to form a meaningful
sentence based on the linguistic relationship between adjacent
words.
10
top-5top-10top-25top-50top-75top-1000.00.20.40.60.81.0Inference accuracy Berger [21]  VISIBLE789101112130.00.20.40.60.81.0Inference accuracyWord length top-10 top-25 top-50 top-100Fig. 13. Sentence inference results.
Fig. 13 illustrates the input sentences and the results
inferred by VISIBLE. The number under each word is the
number of candidate words output by VISIBLE. The red italic
words are the ones correctly chosen by the attacker. The black
non-italic words are the ones in the candidate word list but
hard to choose. Symbol "*" indicates that the corresponding
word is not correctly inferred during word inference. More
detailed investigations ﬁnd that the incorrectly inferred words
are due to one or two misclassiﬁed letters. We expect that the
sentence inference accuracy of VISIBLE can be dramatically
improved by incorporating more advanced linguistic models.
E. PIN Keyboard Experiment
We now evaluate the key inference performance on the
PIN keyboard of the iPad 2 tablet. In this experiment, we
involved three participants. Intuitively, the key inference on
the PIN keyboard is more difﬁcult than that on the alphabetical
keyboard for mainly two reasons. First, all the keys are located
in a relatively small area in the central part of the touchscreen.
Second, the typed keys are very likely to be random, and
there are no relationships (e.g., linguistic relationship) between
adjacent keystrokes.
Table III compares the pinpoint and 1-hop accuracies of
VISIBLE and random guess for each key on the PIN keyboard.
We can see that the pinpoint accuracy of each key ranges from
21% for number "9" to 61% for "c" cancel key. The average
pinpoint accuracy of VISIBLE across all 26 letters is 38%,
which is more than four times of that of random guess, i.e.,
11 = 9%. When considering one-hop neighborhood, the P 1
100
i
of VISIBLE for each key is still much higher than that of
i of VISIBLE
random guess. We can see that the average P 1
across all 11 keys is 68%, which is much higher than 36% of
random guess. Again, the average P 1
i should be only used with
caution to compare the inference accuracies of two techniques.
Moreover, comparing Tables I and III, we can see that
although the PIN keyboard has fewer keys than the alphabetical
keyboard, the key inference accuracy of the PIN keyboard is
not dramatically higher than that of the alphabetical keyboard.
The reason is that the keys of the PIN keyboard reside in a
TABLE III.
KEY INFERENCE RESULTS FOR PIN KEYBOARD.
Key
1
2
3
4
5
6
7
8
9
0
c
Avg.
Pi
Ω1(i)
VIS RG
1, 2, 4
21% 9%
1, 2, 3, 5
25% 9%
2, 3, 6
45% 9%
55% 9%
1, 4, 5, 7
40% 9% 2, 4, 5, 6, 8
35% 9%
3, 5, 6, 9
44% 9%
4, 7, 8
23% 9% 5, 7, 8, 9, 0
6, 8, 9, c
27% 9%
47% 9%
0, 8, c
61% 9%
0, 9, c
38% 9%
-
|Ω1(i)|
3
4
3
4
5
4
3
5
4
3
3
4
P 1
i
VIS
RG
58% 27%
75% 36%
63% 27%
81% 36%
66% 45%