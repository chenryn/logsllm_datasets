title:Opprentice: Towards Practical and Automatic Anomaly Detection Through
Machine Learning
author:Dapeng Liu and
Youjian Zhao and
Haowen Xu and
Yongqian Sun and
Dan Pei and
Jiao Luo and
Xiaowei Jing and
Mei Feng
Opprentice: Towards Practical and Automatic Anomaly
Detection Through Machine Learning
Dapeng Liu†, Youjian Zhao†, Haowen Xu†, Yongqian Sun†, Dan Pei†⇤, Jiao Luo‡,
Xiaowei Jing§, Mei Feng§
†Tsinghua University, ‡Baidu, §PetroChina
†Tsinghua National Laboratory for Information Science and Technology
{liudp10, xhw11, sunyq12}@mails.tsinghua.edu.cn, {zhaoyoujian,
peidan}@tsinghua.edu.cn, PI:EMAIL, {jxw, fm}@petrochina.com.cn
ABSTRACT
Closely monitoring service performance and detecting anomalies
are critical for Internet-based services. However, even though
dozens of anomaly detectors have been proposed over the years, de-
ploying them to a given service remains a great challenge, requiring
manually and iteratively tuning detector parameters and thresholds.
This paper tackles this challenge through a novel approach based
on supervised machine learning. With our proposed system,
Opprentice (Operators’ apprentice), operators’ only manual work
is to periodically label the anomalies in the performance data
with a convenient tool. Multiple existing detectors are applied
to the performance data in parallel to extract anomaly features.
Then the features and the labels are used to train a random forest
classiﬁer to automatically select the appropriate detector-parameter
combinations and the thresholds. For three different service KPIs
in a top global search engine, Opprentice can automatically satisfy
or approximate a reasonable accuracy preference (recall ≥ 0.66 and
precision ≥ 0.66). More importantly, Opprentice allows operators
to label data in only tens of minutes, while operators traditionally
have to spend more than ten days selecting and tuning detectors,
which may still turn out not to work in the end.
Categories and Subject Descriptors
C.2.3 [Network Operations]: Network monitoring
General Terms
Measurement; Design
Keywords
Anomaly Detection; Tuning Detectors; Machine Learning
⇤Dan Pei is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from Permissions@acm.org.
IMC’15, October 28–30, 2015, Tokyo, Japan.
c 2015 ACM. ISBN 978-1-4503-3848-6/15/10 ...$15.00.
DOI: http://dx.doi.org/10.1145/2815675.2815679.
1.
INTRODUCTION
Closely monitoring service quality and detecting performance
anomalies are critical for Internet-based services, such as search
engines, online shopping, and social networking. For example,
a search engine can monitor its search response time (SRT) [1]
by detecting anomalies in SRT time series in an online fashion.
However, even though dozens of anomaly detectors [1–24] have
been proposed over the years in the context of both Internet-
based services and telecom services, deploying detectors to a
given service remains a great challenge. This is, we argue,
because there exists no convenient method to automatically match
operators’ practical detection requirements with the capabilities
of different detectors. Currently, for a given service quality
metric, selecting and applying detectors usually require manually
and iteratively tuning the internal parameters of detectors and the
detection thresholds, which operators are neither interested nor feel
comfortable in doing. Instead, based on our experience of working
with operators from a large search engine, a campus network, and
an enterprise network, operators are used to specify simple require-
ments for detection accuracy and manually spot-check anomalies
occasionally. As a result, services either settle with simple static
thresholds (e.g., Amazon Cloud Watch Alarms [24]), intuitive to
operators although unsatisfying in detection performance, or, after
time-consuming manual tuning by algorithm designers, end up
with a detector speciﬁcally tailored for the given service, which
might not be directly applicable to other services.
We elaborate on the challenges of anomaly detection by review-
ing the current practice. The detectors proposed in the literature [1–
24] typically work on (time, value) pair time series data,1 which
we call KPI (Key Performance Indicator) data hereinafter. Given a
KPI, the ﬁrst step for the anomaly detection practitioner is to collect
the requirements from the service operators. This step encounters
Deﬁnition Challenges: it is difﬁcult to precisely deﬁne anomalies
in reality [21, 25]. In particular, the operators often have trouble
describing their domain knowledge completely when “put on the
spot” [25].
In addition, it is often impossible for the operators
to quantitatively deﬁne anomalies, let alone to translate the vague
deﬁnition into speciﬁc parameters and thresholds of a detector [21]
(e.g., how many times of the standard deviation [1]) . On the
contrary, according to our experience of building detection systems
with operators, they prefer to describe the anomalies qualitatively,
with anecdotal anomaly cases as examples.
1In this paper, we focus on the performance anomalies of time
series (also known as volume-based anomalies) rather than other
types of anomalies, e.g., intrusion detection using the payload
information of packets.
Detector Challenges: In order to provide a reasonable detection
accuracy, selecting the most suitable detector requires both the
algorithm expertise and the domain knowledge about the given
service KPI. The best parameters and thresholds of a given detector
often highly depend on the actual data in the studied service.
As such, it is very time-consuming to tune the parameters and
thresholds of the detector. Sometimes, it might even require a
combination of multiple detectors [8, 21, 22] for a given KPI. As a
result, many rounds of time-consuming iterations between anomaly
detection practitioners and operators are needed to ﬁnd appropriate
detectors and tune their parameters and thresholds. In reality, we
observe that it is not uncommon for operators to give up after a few
rounds of iterations and settle with static threshold-based detection.
To address the deﬁnition challenge and the detector challenge,
we advocate for using supervised machine learning techniques,2
well known for being able to capture complex concepts based on
the features and the labels from the data (e.g., KPI data). Our
approach relies on two key observations. First, it is straightforward
for operators to visually inspect the time series data and label
anomaly cases they identiﬁed [1, 4, 9, 12, 14, 17, 26]. Operators can
periodically (e.g., weekly) label the cases as new data arrive, the
only manual work for operators. Because anomalies are typically
infrequent [16], the time for labeling is quite reasonable with the
help of a dedicated labeling tool, as shown in [27] and §4.2. The
second key observation is that the anomaly severities measured by
different detectors can naturally serve as the features in machine
learning, so each detector can serve as a feature extractor (see §4.3).
Opprentice then learns from the labeled data, automatically cap-
turing the domain knowledge from the operators, just as a smart
and diligent apprentice of the operators would do. Speciﬁcally,
multiple detectors are applied to the KPI data in parallel to extract
features. Then the features and the labels are used to train a
machine learning model, i.e., random forests [28], to automatically
select the appropriate detector-parameter combinations and the
thresholds. The training objective is to maximally satisfy the
operators’ accuracy preference.
The major contributions of the paper are summarized as fol-
lows. First,
to the best of our knowledge, Opprentice is the
ﬁrst detection framework to apply machine learning to acquiring
realistic anomaly deﬁnitions and automatically combining and
tuning diverse detectors to satisfy operators’ accuracy preference.
Second, Opprentice addresses a few challenges in applying ma-
chine learning to such a problem:
labeling overhead, infrequent
anomalies, class imbalance, and irrelevant and redundant features,
elaborated on in §3.2 and addressed in §4. Third, we build and
evaluate Opprentice in a top global search engine (§5). 14 existing
detectors have been implemented and plugged into Opprentice. For
three different service KPIs, Opprentice can automatically satisfy
or approximate a reasonable accuracy preference (recall ≥ 0.66
and precision ≥ 0.66). Furthermore, although the best performing
detector-parameter combination changes with different KPIs, Op-
prentice consistently performs similarly or even better than them.
More importantly, Opprentice takes operators only tens of minutes
to label data. In comparison, operators traditionally have to spend
days learning and selecting detectors, then another tens of days
tuning them, which may still turn out not to work in the end.
We believe this is the ﬁrst anomaly detection framework that does
not require manual detector selection, parameter conﬁguration, or
threshold tuning.
2In the rest of this paper, machine learning refers particularly to
supervised machine learning.
2. BACKGROUND
In this section, we ﬁrst introduce the background of KPI anomaly
detection. Then we present the goals and the challenges of
designing Opprentice.
2.1 KPIs and KPI Anomalies
KPIs: The KPI data, which Opprentice aims to work with, are
the time series data with the format of (timestamp, value). These
data can be collected from SNMP, syslogs, network traces, web
access logs, and other data sources.
In this paper, we choose
three representative KPIs from a global top search engine as a
case study. Table 1 describes their basic information, and Fig. 1
shows their 1-week examples. We hide the absolute values for
conﬁdentiality. Fig. 1(a) shows the search page view (PV), which
is the number of successfully served queries. PV has a signiﬁcant
inﬂuence on the revenue of the search engine. Fig. 1(b) shows the
number of slow responses of search data centers (#SR), which is an
important performance metric of the data centers. Fig. 1(c) is the
the 80th percentile of search response time (SRT). This KPI has a
measurable impact on the users’ search experience [29].
Figure 1: 1-week examples of three major KPIs of the search
engine. The circles mark some obvious (not all) anomalies.
Beyond the physical meanings, the characteristics of these KPI
data are also different. First, they have different levels of sea-
sonality. For example, by visually inspecting, we see that the
PV is much more regular than the other two KPIs and shows a
strong seasonality.
In addition, the dispersions of the KPIs are
different too. Since we have to hide the absolute values, we use
the coefﬁcient of variation (Cv) to measure the dispersions. Cv
equals the standard deviation divided by the mean. In Table 1, #SR
has Cv = 209% and is spread out the most; SRT has Cv = 7%
and concentrates the most to the mean.
Table 1: Three kinds of KPI data from the search engine.
PV
1
25
SRT
60
16
#SR
1
19
Interval (minute)
Length (week)
Seasonality
Cv
Strong Weak Moderate
0.48
0.07
2.1
(a)PV(b)#SR(c)SRTa popular solution where hand-crafted rules of classiﬁcation are
difﬁcult to specify in advance, e.g., computer vision and data
mining. In anomaly detection, manually pre-deﬁning anomalies is
also challenging, which motivates us to tackle the problem through
supervised machine learning.
Anomalies: KPI time series data can also present several
unexpected patterns (e.g., jitters, slow ramp-ups, sudden spikes and
dips) in different severity levels, such as a sudden drop by 20% or
50%. When identifying anomalies, operators care about certain
patterns with different severities, which can vary among KPIs.
Fig. 1 shows a few anomaly examples. However, this knowledge is
difﬁcult to be described accurately by some pre-deﬁned rules [1,2].
This is because operators usually determine anomalies according
to their own understandings of the KPIs and the real operational
demands. Throughout this paper, we assume that operators have no
concept drift [30] regarding anomalies. This is consistent with what
we observed when the operators labeled months of data studied in
this paper.
To identify anomalies automatically, researchers have proposed
many detectors using a variety of techniques. We call them basic
detectors in the rest of the paper. More details about detectors will
be discussed in §4.3.
2.2 Problem and Goal
# of true anomalous points
# of anomalous points detected
The KPI data labeled by operators are the so called “ground
truth”. The fundamental goal of anomaly detection is to be
accurate, e.g., identifying more anomalies in the ground truth, and