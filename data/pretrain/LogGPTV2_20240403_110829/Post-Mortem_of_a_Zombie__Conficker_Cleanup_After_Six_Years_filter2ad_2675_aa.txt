title:Post-Mortem of a Zombie: Conficker Cleanup After Six Years
author:Hadi Asghari and
Michael Ciere and
Michel J. G. van Eeten
Post-Mortem of a Zombie:  
Conficker Cleanup After Six Years
Hadi Asghari, Michael Ciere, and Michel J.G. van Eeten, Delft University of Technology
https://www.usenix.org/conference/usenixsecurity15/technical-sessions/presentation/asghari
This paper is included in the Proceedings of the 24th USENIX Security SymposiumAugust 12–14, 2015 • Washington, D.C.ISBN 978-1-939133-11-3Open access to the Proceedings of  the 24th USENIX Security Symposium is sponsored by USENIXPost-Mortem of a Zombie: Conﬁcker Cleanup After Six Years
Hadi Asghari, Michael Ciere and Michel J.G. van Eeten
Delft University of Technology
Abstract
Research on botnet mitigation has focused predomi-
nantly on methods to technically disrupt the command-
and-control infrastructure. Much less is known about the
effectiveness of large-scale efforts to clean up infected
machines. We analyze longitudinal data from the sink-
hole of Conﬁcker, one the largest botnets ever seen, to as-
sess the impact of what has been emerging as a best prac-
tice: national anti-botnet initiatives that support large-
scale cleanup of end user machines. It has been six years
since the Conﬁcker botnet was sinkholed. The attackers
have abandoned it. Still, nearly a million machines re-
main infected. Conﬁcker provides us with a unique op-
portunity to estimate cleanup rates, because there are rel-
atively few interfering factors at work. This paper is the
ﬁrst to propose a systematic approach to transform noisy
sinkhole data into comparative infection metrics and nor-
malized estimates of cleanup rates. We compare the
growth, peak, and decay of Conﬁcker across countries.
We ﬁnd that institutional differences, such as ICT devel-
opment or unlicensed software use, explain much of the
variance, while the national anti-botnet centers have had
no visible impact. Cleanup seems even slower than the
replacement of machines running Windows XP. In gen-
eral, the infected users appear outside the reach of current
remediation practices. Some ISPs may have judged the
neutralized botnet an insufﬁcient threat to merit remedi-
ation. These machines can however be magnets for other
threats — we ﬁnd an overlap between GameoverZeus
and Conﬁcker infections. We conclude by reﬂecting on
what this means for the future of botnet mitigation.
1
Introduction
For years, researchers have been working on methods to
take over or disrupt the command-and-control (C&C) in-
frastructure of botnets (e.g.
[14, 37, 26]). Their suc-
cesses have been answered by the attackers with ever
more sophisticated C&C mechanisms that are increas-
ingly resilient against takeover attempts [30].
In pale contrast to this wealth of work stands the lim-
ited research into the other side of botnet mitigation:
cleanup of the infected machines of end users. Af-
ter a botnet is successfully sinkholed, the bots or zom-
bies basically remain waiting for the attackers to ﬁnd
a way to reconnect to them, update their binaries and
move the machines out of the sinkhole. This happens
with some regularity. The recent sinkholing attempt of
GameoverZeus [32], for example, is more a tug of war
between attackers and defenders, rather than deﬁnitive
takedown action. The bots that remain after a takedown
of C&C infrastructure may also attract other attackers,
as these machines remain vulnerable and hence can be
re-compromised.
To some extent, cleanup of bots is an automated pro-
cess, driven by anti-virus software, software patches and
tools like Microsoft’s Malicious Software Removal Tool,
which is included in Windows’ automatic update cycle.
These automated actions are deemed insufﬁcient, how-
ever. In recent years, wide support has been established
for the idea that Internet Service Providers (ISPs) should
contact affected customers and help them remediate their
compromised machines [39, 22]. This shift has been ac-
companied by proposals to treat large-scale infections as
a public health issue [6, 8].
As part of this public health approach, we have seen
the emergence of large-scale cleanup campaigns, most
notably in the form of national anti-botnet initiatives.
Public and private stakeholders, especially ISPs, collabo-
rate to notify infected end users and help them clean their
machines. Examples include Germany’s Anti-Botnet
Advisory Center (BotFrei), Australia’s Internet Industry
Code of Practice (iCode), and Japan’s Cyber Clean Cen-
ter (CCC, superseded by ACTIVE) [27].
Setting up large-scale cleanup mechanisms is cumber-
some and costly. This underlines the need to measure
whether these efforts are effective. The central question
USENIX Association  
24th USENIX Security Symposium  1
of this paper is: What factors drive cleanup rates of in-
fected machines? We explore whether the leading na-
tional anti-botnet initiatives have increased the speed of
cleanup.
We answer this question via longitudinal data from
the sinkhole of Conﬁcker, one the largest botnets ever
seen. Conﬁcker provides us with a unique opportunity to
study the impact of national initiatives. It has been six
years since the vulnerability was patched and the botnet
was sinkholed. The attackers have basically abandoned it
years ago, which means that infection rates are driven by
cleanup rather than the attacker countermeasures. Still,
nearly a million machines remain infected (see ﬁgure 1).
The Conﬁcker Working Group, the collective industry ef-
fort against the botnet, concluded in 2010 that remedia-
tion has been a failure [7].
Before one can draw lessons from sinkhole data, or
from most other data sources on infected machines, sev-
eral methodological problems have to be overcome. This
paper is the ﬁrst to systematically work through these is-
sues, transforming noisy sinkhole data into comparative
infection metrics and normalized estimates of cleanup
rates.
For this research, we were generously given access to
the Conﬁcker sinkhole logs, which provide a unique long
term view into the life of the botnet. The dataset runs
from February 2009 until September 2014, and covers all
countries — 241 ISO codes — and 34,000 autonomous
systems. It records millions of unique IP addresses each
year — for instance, 223 million in 2009, and 120 mil-
lion in 2013. For this paper, we focus on bots located in
62 countries.
In sum, the contributions of this paper are as follows:
1. We develop a systematic approach to transform
noisy sinkhole data into comparative infection met-
rics and normalized estimates of cleanup rates.
2. We present the ﬁrst long term study on botnet reme-
diation.
3. We provide the ﬁrst empirical test of the best prac-
tice exempliﬁed by the leading national anti-botnet
initiatives.
4. We identify several factors that inﬂuence cleanup
rates across countries.
2 Background
2.1 Conﬁcker timeline and variants
In this section we will provide a brief background on the
history of the Conﬁcker worm, its spreading and defense
Figure 1: Conﬁcker bots worldwide
mechanisms, and some milestones in the activities of the
Conﬁcker Working Group.
The Conﬁcker worm, also known as Downadup, was
ﬁrst detected in November 2008. The worm spread by
exploiting vulnerability MS08-067 in Microsoft Win-
dows, which had just been announced and patched. The
vulnerability affected all versions of Microsoft Windows
at the time, including server versions. A detailed tech-
nical analysis is available in [29]. Brieﬂy put, infected
machines scanned the IP space for vulnerable machines
and infected them in a number steps. To be vulnerable,
a machine needed to be unpatched and online with its
NetBIOS ports open and not behind a ﬁrewall. Remark-
ably, a third of all machines had still not installed the
patch by January 2009, a few months after its availabil-
ity [11]. Consequently, the worm spread at an explosive
rate. The malware authors released an update on Decem-
ber 29, 2008, which was named Conﬁcker-B. The update
added new methods of spreading, including via infected
USB devices and shared network folders with weak pass-
words. This made the worm propagate even faster [7].
Infected machines communicated with the attackers
via an innovative, centralized system. Every day, the bots
attempted to connect to 250 new pseudo-randomly gen-
erated domains under eight different top-level domains.
The attackers needed to register only one of these do-
mains to reach the bots and update their instructions and
binaries. Defenders, on the other hand, needed to block
all these domains, every day, to disrupt the C&C. An-
other aspect of Conﬁcker was the use of intelligent de-
fense mechanisms, that made the worm harder to re-
move. It disabled Windows updates, popular anti-virus
products, and several Windows security services. It also
blocked access to popular security websites [29, 7].
Conﬁcker continued to grow, causing alarm in the cy-
bersecurity community about the potential scale of at-
tacks, even though the botnet had not yet been very active
at that point. In late January, the community — includ-
2  24th USENIX Security Symposium 
USENIX Association
2
ing Microsoft, ICANN, domain registries, anti-virus ven-
dors, and academic researchers — responded by forming
the Conﬁcker Working Group [7, 31]. The most impor-
tant task of the working group was to coordinate and reg-
ister or block all the domains the bots would use to com-
municate, staying ahead of the Conﬁcker authors. The
group was mostly successful in neutralizing the botnet
and disconnecting it from its owners; however, small er-
rors were made on two occasions in March, allowing the
attackers to gain access to part of the botnet population
and update them to the C variant.
The Conﬁcker-C variant had two key new features:
the number of pseudo-randomly generated domains was
increased to 50,000 per day, distributed over a hun-
dred different TLDs, and a P2P update protocol was
added. These features complicated the work of the work-
ing group. On April 9, 2009, Conﬁcker-C bots up-
graded to a new variant that included a scareware pro-
gram which sold fake anti-virus at prices between $50–
$100. The fake anti-virus program, probably a pay-per-
install contract, was purchased by close to a million un-
witting users, as was later discovered. This use of the
botnet prompted law enforcement agencies to increase
their efforts to pursue the authors of Conﬁcker.1 Even-
tually, in 2011, the U.S. Federal Bureau of Investiga-
tion, in collaboration with police in several other coun-
tries, arrested several individuals associated with this
$72-million scareware ring. [21, 19]
2.2 National anti-botnet centers
Despite the successes of the cybersecurity community in
neutralizing Conﬁcker, a large number of infected ma-
chines still remained. This painful fact was recognized
early on; in its ‘Lessons Learned’ document from 2010,
the Conﬁcker Working Group reported remediation as its
top failure [7]. Despite being inactive, Conﬁcker remains
one of the largest botnets. As recent as June 2014, it was
listed as the #6 botnet in the world by anti-virus ven-
dor ESET [9]. This underlines the idea that neutralizing
the C&C infrastructure in combination with automated
cleanup tools will not eradicate the infected machines;
some organized form of cleanup is necessary.
During the past years, industry and regulatory guide-
lines have been calling for increased participation of ISPs
in cleanup efforts. For instance, the European Network
and Information Security Agency [1], the Internet En-
gineering Task Force [22], the Federal Communications
Commission [10], and the Organization for Economic
Cooperation and Development [27] have all called upon
ISPs to contact infected customers and help them clean
up their compromised machines.
1Microsoft also set a $250,000 bounty for information leading to
arrests.
The main reason for this shift is that ISPs can iden-
tify and contact the owners of the infected machines, and
provide direct support to end users. They can also quar-
antine machines that do not get cleaned up. Earlier work
has found evidence that ISP mitigation can signiﬁcantly
impact end user security [40].
Along with this shift of responsibility towards ISPs,
some countries have established national anti-botnet ini-
tiatives to support the ISPs and end users in cleanup ef-
forts. The setup is different in each country, but typically
it involves the collection of data on infected machines
(from botnet sinkholes, honeypots, spamtraps, and other
sources); notifying ISPs of infections within their net-
works; and providing support for end users, via a website
and sometimes a call-center.
A number of countries have been running such cen-
ters, often as part of a public-private partnership. Table
1 lists the countries with active initiatives in late 2011,
according to an OECD report [27]. The report also men-
tions the U.S. & U.K. as developing such initiatives. The
Netherlands is listed as having ‘ISP-speciﬁc’ programs,
for at that time, KPN and Ziggo — the two largest ISPs
— were heading such programs voluntarily [39].2 Fin-
land, though not listed, has been a leader with consis-
tently low infection rates for years. It has had a notiﬁ-
cation and cleanup mechanism in place since 2005, as
part of a collaboration between the national CERT, the
telco regulator and main ISPs [20, 25]. At the time of
writing, other countries are starting anti-botnet centers as
well. In the EU alone, seven new national centers have
been announced [2]. These will obviously not impact the
past cleanup rates of Conﬁcker, but they do underwrite
the importance of empirically testing the efﬁcacy of this
mitigation strategy.
Figure 2 shows the website of the German anti-botnet
advisory center, botfrei. The center was launched in 2010
by eco, the German Internet industry association, and is
partially funded by the German government. The cen-
ter does three things. First, it identiﬁes users with in-
fected PCs. Second, they inform the infected customers
via their ISPs. Third, they offer cleanup support, through
a website — with free removal tools and a forum — and
2It has now been replaced by a wider initiative involving all main
providers and covering the bulk of the broadband market.
COUNTRY
Australia
Germany
Ireland
Japan
Korea
INITIATIVE
Internet Industry Code of Practice (iCode)
German Anti-Botnet Initiative (BotFrei)
Irish Anti-Botnet Initiative
Cyber Clean Center / ACTIVE
KrCERT/CC Anti-Botnet Initiative
Netherlands
Dutch Anti-Botnet Initiative (Abuse-Hub)
Table 1: List of countries with anti-botnet initiatives [27]
USENIX Association  
24th USENIX Security Symposium  3
3
4  24th USENIX Security Symposium 
USENIX Association
Figure2:TheGermanAnti-BotnetAdvisoryCenterwebsite-botfrei.deacallcenter[17].Thecentercoversawiderangeofmalware,includingConﬁcker.WeshouldmentionthatecostafftoldusthatmuchoftheGermanConﬁckerre-sponsetookplacebeforethecenterwaslaunched.Intheirownevaluations,thecenterreportssuccessesintermsofthenumberofusersvisitingitswebsite,thenumberofcleanupactionsperformed,andoverallreduc-tionsinmalwareratesinGermany.Interestinglyenough,alargenumberofusersvisitbotfrei.dedirectly,withoutbeingpromptedbytheirISP.Thishighlightstheimpactofmediaattention,aswellasthedemandforproactivestepsamongpartoftheuserpopulation.WeonlyhighlightGermany’sbotfreiprogramasanexample.Inshort,onewouldexpectthatcountriesrun-ningsimilaranti-botnetinitiativestohavehighercleanupratesofConﬁckerbots.This,weshallevaluate.2.3RelatedWorkSimilartootherbotnets,muchoftheworkontheConﬁckerwormhasfocusedpredominantlyontech-nicalanalysis,e.g.,[29].Otherresearchhasstudiedtheworm’soutbreakandmodeleditsinfectionpatterns,e.g.,[42],[16],[33]and[41].TherehavealsobeenafewstudieslookingintothefunctioningoftheWork-ingGroup,e.g.,[31].Noneofthisworklooksspecif-icallyattheissueofremediation.Although[33]usesthesamedatasetasthispapertomodelthespreadoftheworm,theirresultsareskewedbythefactthattheyig-noreDHCPchurn,whichisknowntocauseerrorsinin-fectionratesofuptooneorderofmagnitudeforsomecountries[37].Thispaperalsoconnectstotheliteratureonbotnetmitigation,speciﬁcallytocleanupefforts.Thisincludestheindustryguidelineswediscussedearlier,e.g.,[1],[27],[10]and[22];aswellasacademicworkthattriestomodeldifferentmitigationstrategies,e.g.,[6],[18]and[13].Wecontributetothisdiscussionbybringinglongitudinaldatatobearontheproblemandempiricallyevaluatingoneofthekeyproposalstoemanatefromthisliterature.Thisexpandssomeofourearlierwork.Inabroadercontext,alargebodyofresearchfocusesonotherformsofbotnetmitigation,e.g.,[14,37,26,30],modelingworminfections,e.g.[35,44,43,28],andchallengesinlongitudinalcybersecuritystudies.Forthesakeofbrevitywewillnotcitemoreworksintheseareashere(—exceptforworksusedinothersections).3MethodologyAnsweringthecentralresearchquestionrequiresanum-berofsteps.First,wesetouttoderivereliableesti-matesofthenumberofConﬁckerbotsineachcountryovertime.Thisinvolvesprocessingandcleaningthenoisysinkholedata,aswellashandlingseveralmeasure-mentissues.Later,weusetheestimatestocomparein-fectiontrendsinvariouscountries,identifypatternsandspeciﬁcallyseeifcountrieswithanti-botnetinitiativeshavedoneanybetter.Wedothisbybyﬁttingade-scriptivemodeltoeachcountry’stime-seriesofinfec-tionrates.Thisprovidesuswithaspeciﬁcsetofparam-eters,namelythegrowthrate,thepeakinfectionlevel,andthedecayrate.Weexploreafewalternativemodelsandoptforatwo-piecemodelthataccuratelycapturesthesecharacteristics.Lastly,toanswerthecentralques-tion,weexploretherelationshipbetweentheestimatedparametersandasetofexplanatoryvariables.3.1TheConﬁckerDatasetTheConﬁckerdatasethasfourcharacteristicsthatmakeituniquelysuitedforstudyinglarge-scalecleanupef-forts.First,itcontainsthecompleterecordofonesink-holedbotnet,makingitlessconvolutedthanforexamplespamdata,andwithfarfewerfalsepositives.Second,itlogsmostofthepopulationonadailybasis,avoid-inglimitationsfromseeingonlyasampleofthebot-net.Third,thedatasetislongitudinalandtracksaperiodofalmostsixyears.Manysinkholesusedinscientiﬁcresearchtypicallycoverweeksratherthanmonths,letalonesixyears.Fourth,mostinfectiondatareﬂectsamixofattackeranddefenderbehavior,aswellasdifferentlevels(global&local).Thismakesithardtodeterminewhatdrivesatrend–isittheresultofattackerbehav-ior,defenderinnovation,orjustrandomness?Conﬁcker,however,wasneutralizedearlyon,withtheattackerslos-ingcontrolandabandoningthebotnet.Mostotherglobaldefensiveactions(e.g.,patchingandsinkholing)werealsodoneinearly2009.Hence,theinfectionlevelsinourdatasetpredominantlyreﬂectcleanupefforts.ThesecombinedattributesmaketheConﬁckerdatasetexcellentforstudyingthepolicyeffectsweareinterestedin.4Raw Data
Our raw data comes from the Conﬁcker sinkhole logs.
As explained in the background section, Conﬁcker bots
used an innovative centralized command and control in-
frastructure. The bots seek to connect to a number of
pseudo-random domains every day, and ask for updated
instructions or binaries from their masters. The algo-
rithm that generates this domain list was reverse engi-
neered early on, and various teams, including the Con-
ﬁcker Working Group, seized legal control of these do-
mains. The domains were then ‘sinkholed’: servers were
set up to listen and log every attempt to access the do-
mains. The resulting logs include the IP address of each
machine making such an attempt, timestamps, and a few
other bits of information.
Processing Sinkhole Logs
The raw logs were originally stored in plain text, before
adoption of the nmsg binary format in late 2010. The
logs are huge; a typical hour of logs in January 2013
is around half a gigabyte, which adds up to tens of ter-
abytes per year. From the raw logs we extract the IP
address, which in the majority of cases will be a Con-
ﬁcker A, B, or C bot (the sinkholed domains were not
typically used for other purposes). Then, using the Max-
Mind GeoIP database [23] and an IP-to-ASN database
based on Routeviews BGP data [4], we determine the
country and Autonomous System that this IP address be-
longed to at that moment in time. We lastly count the
number of unique IP addresses in each region per hour.
With some exceptions, we capture most Conﬁcker bots
worldwide. The limitations are due to sinkholes down-
time; logs for some sinkholed domains not being handed
over to the working group [7]; and bots being behind
an egress ﬁrewall, blocking their access to the sinkhole.
None of these issues however creates a systematic bias,
so we may treat them as noise.
After processing the logs we have a dataset spanning
from February 2009 to September 2014, covering 241
ISO country codes and 34,000 autonomous systems. The
dataset contains approximately 178 million unique IP ad-
dresses per year. In this paper we focus on bots located in
62 countries, which were selected as follows. We started
with the 34 members of the Organization for Economic
Cooperation and Development (OECD), and 7 additional
members of the European Union which are not part of
the OECD. These countries have a common develop-
ment baseline, and good data is available on their poli-
cies, making comparison easier. We add to this list 23
countries that rank high in terms of Conﬁcker or spam
bots — cumulatively covering 80 percent of all such bots
worldwide. These countries are interesting from a cy-
bersecurity perspective. Finally, two countries were re-
Figure 3: Unique IP counts over various time-periods
moved due to severe measurement issues affecting their
bot counts, which we will describe later. The full list of
countries can be seen in ﬁgure 8 or in the appendix.
3.2 Counting bots from IP addresses
The Conﬁcker dataset suffers from a limitation that is
common among most sinkhole data and other data on in-
fected machines, such as spam traps, ﬁrewall logs, and
passive DNS records: one has to use IP addresses as a
proxy for infected machines. Earlier research has estab-
lished that IP addresses are coarse unique identiﬁers and
they can be off by one order of magnitude in a matter of
days [37], because of differences in the dynamic IP ad-
dress allocation policies of providers (so-called DHCP
churn). Simply put, because of dynamic addresses, the
same infected machine can appear in the logs under mul-
tiple IP addresses. The higher the churn rate, the more
over-counting.
Figure 3 visualizes this problem. It shows the count
of unique Conﬁcker IP addresses in February 2011 over
various time periods — 3 hours, 12 hours, one day, up to
a week. We see an interesting growth curve, non-linear
at the start, then linear. Not all computers are powered
on at every point in time, so it makes sense to see more
IP addresses in the sinkhole over longer time periods.
However, between the 6th and 7th day, we have most
likely seen most infected machines already. The new IP
addresses are unlikely to be new infections, as the daily
count is stable over the period. The difference is thus
driven by infected machines reappearing with a new IP
address.
The ﬁgure shows IP address counts for the Nether-
lands and Germany. From qualitative reports we know
that IP churn is relatively low in the Netherlands —
an Internet subscriber can retain the same IP address
for months — while in Germany the address typically
USENIX Association  
24th USENIX Security Symposium  5
5
changes every 24 hours. This is reﬂected in the ﬁgure:
the slope for Germany is much steeper. Should one ig-
nore the differences in churn rates among countries, and
simply count unique IP addresses over a week, then a
severe bias will be introduced against countries such as
Germany. Using shorter time periods, though leading to