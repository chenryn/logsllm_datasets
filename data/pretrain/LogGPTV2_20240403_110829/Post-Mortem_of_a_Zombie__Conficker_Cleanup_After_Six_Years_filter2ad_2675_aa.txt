# Post-Mortem of a Zombie: Conficker Cleanup After Six Years

**Authors:**
- Hadi Asghari
- Michael Ciere
- Michel J.G. van Eeten

**Institution:**
Delft University of Technology

**Publication:**
Proceedings of the 24th USENIX Security Symposium, August 12–14, 2015, Washington, D.C.
ISBN 978-1-939133-11-3
Open access sponsored by USENIX

## Abstract

Research on botnet mitigation has primarily focused on disrupting the command-and-control (C&C) infrastructure. However, little is known about the effectiveness of large-scale efforts to clean up infected machines. This study analyzes longitudinal data from the Conficker sinkhole, one of the largest botnets ever, to assess the impact of national anti-botnet initiatives that support large-scale cleanup of end-user machines. Six years after the Conficker botnet was sinkholed and abandoned by its attackers, nearly a million machines remain infected. Conficker provides a unique opportunity to estimate cleanup rates due to the absence of interfering factors. This paper introduces a systematic approach to transform noisy sinkhole data into comparative infection metrics and normalized estimates of cleanup rates. We compare the growth, peak, and decay of Conficker across countries and find that institutional differences, such as ICT development and unlicensed software use, explain much of the variance. National anti-botnet centers have had no visible impact. Cleanup appears even slower than the replacement of Windows XP machines. Infected users generally seem to be outside the reach of current remediation practices. Some ISPs may have deemed the neutralized botnet an insufficient threat to warrant remediation. These machines can still attract other threats, as we found an overlap between GameoverZeus and Conficker infections. We conclude with reflections on the future of botnet mitigation.

## 1. Introduction

For years, researchers have been developing methods to take over or disrupt the C&C infrastructure of botnets. These efforts have been met with increasingly sophisticated C&C mechanisms by attackers, making them more resilient to takeover attempts. In contrast, there has been limited research into the cleanup of infected end-user machines. After a botnet is successfully sinkholed, the bots remain dormant, waiting for the attackers to reconnect and update their binaries. For example, the recent sinkholing attempt of GameoverZeus was more of a tug-of-war between attackers and defenders rather than a definitive takedown.

Cleanup of bots is partly automated through anti-virus software, patches, and tools like Microsoft’s Malicious Software Removal Tool, which is included in Windows' automatic update cycle. However, these automated actions are often insufficient. In recent years, there has been growing support for the idea that Internet Service Providers (ISPs) should contact affected customers and help them remediate their compromised machines. This shift has been accompanied by proposals to treat large-scale infections as a public health issue.

As part of this public health approach, we have seen the emergence of large-scale cleanup campaigns, most notably in the form of national anti-botnet initiatives. Public and private stakeholders, especially ISPs, collaborate to notify infected end-users and help them clean their machines. Examples include Germany’s Anti-Botnet Advisory Center (BotFrei), Australia’s Internet Industry Code of Practice (iCode), and Japan’s Cyber Clean Center (CCC, superseded by ACTIVE).

Setting up large-scale cleanup mechanisms is cumbersome and costly, highlighting the need to measure their effectiveness. The central question of this paper is: What factors drive the cleanup rates of infected machines? We explore whether leading national anti-botnet initiatives have increased the speed of cleanup.

We answer this question using longitudinal data from the Conficker sinkhole, one of the largest botnets ever. Conficker provides a unique opportunity to study the impact of national initiatives. It has been six years since the vulnerability was patched and the botnet was sinkholed. The attackers have essentially abandoned it, meaning that infection rates are driven by cleanup rather than attacker countermeasures. Still, nearly a million machines remain infected (see Figure 1). The Conficker Working Group, the collective industry effort against the botnet, concluded in 2010 that remediation had failed.

Before drawing lessons from sinkhole data, several methodological problems must be addressed. This paper is the first to systematically work through these issues, transforming noisy sinkhole data into comparative infection metrics and normalized estimates of cleanup rates.

For this research, we were given access to the Conficker sinkhole logs, providing a unique long-term view into the life of the botnet. The dataset runs from February 2009 to September 2014, covering 241 ISO country codes and 34,000 autonomous systems. It records millions of unique IP addresses each year, such as 223 million in 2009 and 120 million in 2013. For this paper, we focus on bots located in 62 countries.

In summary, the contributions of this paper are:
1. A systematic approach to transform noisy sinkhole data into comparative infection metrics and normalized estimates of cleanup rates.
2. The first long-term study on botnet remediation.
3. The first empirical test of the best practice exemplified by leading national anti-botnet initiatives.
4. Identification of several factors influencing cleanup rates across countries.

## 2. Background

### 2.1 Conficker Timeline and Variants

The Conficker worm, also known as Downadup, was first detected in November 2008. It spread by exploiting the MS08-067 vulnerability in Microsoft Windows, which had just been announced and patched. The vulnerability affected all versions of Microsoft Windows at the time, including server versions. A detailed technical analysis is available in [29]. Briefly, infected machines scanned the IP space for vulnerable machines and infected them in several steps. To be vulnerable, a machine needed to be unpatched, online, with its NetBIOS ports open and not behind a firewall. Remarkably, a third of all machines had still not installed the patch by January 2009, a few months after its availability [11]. Consequently, the worm spread at an explosive rate.

The malware authors released an update on December 29, 2008, named Conficker-B. This update added new methods of spreading, including via infected USB devices and shared network folders with weak passwords, making the worm propagate even faster [7].

Infected machines communicated with the attackers via an innovative, centralized system. Every day, the bots attempted to connect to 250 new pseudo-randomly generated domains under eight different top-level domains. The attackers needed to register only one of these domains to reach the bots and update their instructions and binaries. Defenders, on the other hand, needed to block all these domains every day to disrupt the C&C. Another aspect of Conficker was the use of intelligent defense mechanisms, which made the worm harder to remove. It disabled Windows updates, popular anti-virus products, and several Windows security services. It also blocked access to popular security websites [29, 7].

Conficker continued to grow, causing alarm in the cybersecurity community about the potential scale of attacks, even though the botnet had not yet been very active. In late January, the community, including Microsoft, ICANN, domain registries, anti-virus vendors, and academic researchers, responded by forming the Conficker Working Group [7, 31]. The most important task of the working group was to coordinate and register or block all the domains the bots would use to communicate, staying ahead of the Conficker authors. The group was mostly successful in neutralizing the botnet and disconnecting it from its owners; however, small errors were made on two occasions in March, allowing the attackers to gain access to part of the botnet population and update them to the C variant.

The Conficker-C variant had two key new features: the number of pseudo-randomly generated domains was increased to 50,000 per day, distributed over a hundred different TLDs, and a P2P update protocol was added. These features complicated the work of the working group. On April 9, 2009, Conficker-C bots upgraded to a new variant that included a scareware program selling fake anti-virus at prices between $50–$100. The fake anti-virus program, likely a pay-per-install contract, was purchased by close to a million unwitting users, as was later discovered. This use of the botnet prompted law enforcement agencies to increase their efforts to pursue the authors of Conficker. Eventually, in 2011, the U.S. Federal Bureau of Investigation, in collaboration with police in several other countries, arrested several individuals associated with this $72-million scareware ring [21, 19].

### 2.2 National Anti-Botnet Centers

Despite the successes of the cybersecurity community in neutralizing Conficker, a large number of infected machines still remained. This painful fact was recognized early on; in its 'Lessons Learned' document from 2010, the Conficker Working Group reported remediation as its top failure [7]. Despite being inactive, Conficker remains one of the largest botnets. As recently as June 2014, it was listed as the #6 botnet in the world by anti-virus vendor ESET [9]. This underscores the idea that neutralizing the C&C infrastructure in combination with automated cleanup tools will not eradicate the infected machines; some organized form of cleanup is necessary.

During the past years, industry and regulatory guidelines have called for increased participation of ISPs in cleanup efforts. For instance, the European Network and Information Security Agency [1], the Internet Engineering Task Force [22], the Federal Communications Commission [10], and the Organization for Economic Cooperation and Development [27] have all called upon ISPs to contact infected customers and help them clean up their compromised machines.

The main reason for this shift is that ISPs can identify and contact the owners of the infected machines and provide direct support to end-users. They can also quarantine machines that do not get cleaned up. Earlier work has found evidence that ISP mitigation can significantly impact end-user security [40].

Along with this shift of responsibility towards ISPs, some countries have established national anti-botnet initiatives to support the ISPs and end-users in cleanup efforts. The setup varies by country but typically involves the collection of data on infected machines (from botnet sinkholes, honeypots, spamtraps, and other sources); notifying ISPs of infections within their networks; and providing support for end-users via a website and sometimes a call center.

Several countries have been running such centers, often as part of a public-private partnership. Table 1 lists the countries with active initiatives in late 2011, according to an OECD report [27]. The report also mentions the U.S. & U.K. as developing such initiatives. The Netherlands is listed as having 'ISP-specific' programs, as at that time, KPN and Ziggo—the two largest ISPs—were heading such programs voluntarily [39]. Finland, though not listed, has been a leader with consistently low infection rates for years. It has had a notification and cleanup mechanism in place since 2005, as part of a collaboration between the national CERT, the telco regulator, and main ISPs [20, 25]. At the time of writing, other countries are starting anti-botnet centers as well. In the EU alone, seven new national centers have been announced [2]. These will not impact the past cleanup rates of Conficker but underscore the importance of empirically testing the efficacy of this mitigation strategy.

Figure 2 shows the website of the German Anti-Botnet Advisory Center, Botfrei. The center was launched in 2010 by eco, the German Internet industry association, and is partially funded by the German government. The center does three things: First, it identifies users with infected PCs. Second, it informs the infected customers via their ISPs. Third, it offers cleanup support through a website—with free removal tools and a forum—and a call center [17]. The center covers a wide range of malware, including Conficker. Eco staff told us that much of the German Conficker response took place before the center was launched. In their own evaluations, the center reports successes in terms of the number of users visiting its website, the number of cleanup actions performed, and overall reductions in malware rates in Germany. Interestingly, a large number of users visit botfrei.de directly, without being prompted by their ISP, highlighting the impact of media attention and the demand for proactive steps among part of the user population.

We highlight Germany’s Botfrei program as an example. One would expect that countries running similar anti-botnet initiatives would have higher cleanup rates of Conficker bots. This, we shall evaluate.

### 2.3 Related Work

Much of the work on the Conficker worm has focused predominantly on technical analysis, e.g., [29]. Other research has studied the worm’s outbreak and modeled its infection patterns, e.g., [42], [16], [33], and [41]. There have also been a few studies looking into the functioning of the Working Group, e.g., [31]. None of this work specifically addresses the issue of remediation. Although [33] uses the same dataset as this paper to model the spread of the worm, their results are skewed by the fact that they ignore DHCP churn, which is known to cause errors in infection rates of up to one order of magnitude for some countries [37].

This paper also connects to the literature on botnet mitigation, specifically to cleanup efforts. This includes the industry guidelines we discussed earlier, e.g., [1], [27], [10], and [22]; as well as academic work that tries to model different mitigation strategies, e.g., [6], [18], and [13]. We contribute to this discussion by bringing longitudinal data to bear on the problem and empirically evaluating one of the key proposals to emanate from this literature. This expands on some of our earlier work.

In a broader context, a large body of research focuses on other forms of botnet mitigation, e.g., [14, 37, 26, 30], modeling worm infections, e.g., [35, 44, 43, 28], and challenges in longitudinal cybersecurity studies. For the sake of brevity, we will not cite more works in these areas here (except for works used in other sections).

## 3. Methodology

Answering the central research question requires several steps. First, we set out to derive reliable estimates of the number of Conficker bots in each country over time. This involves processing and cleaning the noisy sinkhole data, as well as handling several measurement issues. Later, we use the estimates to compare infection trends in various countries, identify patterns, and specifically see if countries with anti-botnet initiatives have done any better. We do this by fitting a descriptive model to each country's time series of infection rates. This provides us with a specific set of parameters, namely the growth rate, the peak infection level, and the decay rate. We explore a few alternative models and opt for a two-piece model that accurately captures these characteristics. Lastly, to answer the central question, we explore the relationship between the estimated parameters and a set of explanatory variables.

### 3.1 The Conficker Dataset

The Conficker dataset has four characteristics that make it uniquely suited for studying large-scale cleanup efforts. First, it contains the complete record of one sinkholed botnet, making it less convoluted than, for example, spam data, and with far fewer false positives. Second, it logs most of the population on a daily basis, avoiding limitations from seeing only a sample of the botnet. Third, the dataset is longitudinal and tracks a period of almost six years. Many sinkholes used in scientific research typically cover weeks rather than months, let alone six years. Fourth, most infection data reflect a mix of attacker and defender behavior, as well as different levels (global and local). This makes it hard to determine what drives a trend—is it the result of attacker behavior, defender innovation, or randomness? Conficker, however, was neutralized early on, with the attackers losing control and abandoning the botnet. Most other global defensive actions (e.g., patching and sinkholing) were also done in early 2009. Hence, the infection levels in our dataset predominantly reflect cleanup efforts. These combined attributes make the Conficker dataset excellent for studying the policy effects we are interested in.

### 3.2 Counting Bots from IP Addresses

The Conficker dataset suffers from a limitation common to most sinkhole data and other data on infected machines, such as spam traps, firewall logs, and passive DNS records: one has to use IP addresses as a proxy for infected machines. Earlier research has established that IP addresses are coarse unique identifiers and can be off by one order of magnitude in a matter of days [37], due to differences in the dynamic IP address allocation policies of providers (so-called DHCP churn). Simply put, because of dynamic addresses, the same infected machine can appear in the logs under multiple IP addresses. The higher the churn rate, the more over-counting.

Figure 3 visualizes this problem. It shows the count of unique Conficker IP addresses in February 2011 over various time periods—3 hours, 12 hours, one day, up to a week. We see an interesting growth curve, non-linear at the start, then linear. Not all computers are powered on at every point in time, so it makes sense to see more IP addresses in the sinkhole over longer time periods. However, between the 6th and 7th day, we have likely seen most infected machines already. The new IP addresses are unlikely to be new infections, as the daily count is stable over the period. The difference is thus driven by infected machines reappearing with a new IP address.

The figure shows IP address counts for the Netherlands and Germany. From qualitative reports, we know that IP churn is relatively low in the Netherlands—an Internet subscriber can retain the same IP address for months—while in Germany, the address typically changes every 24 hours. This is reflected in the figure: the slope for Germany is much steeper. If one ignores the differences in churn rates among countries and simply counts unique IP addresses over a week, a severe bias will be introduced against countries such as Germany. Using shorter time periods, though leading to more accurate counts, can still introduce noise due to the variability in machine uptime and connectivity.