SSD
SSD
NVMM
NVMM
SSD
DDR4
NVMM
FS
Ext4
Ext4
Ext4
NOVA
Ext4
none
NOVA
by default
O DIRECT — O SYNC
O DIRECT — O SYNC
O DIRECT — O SYNC
O DIRECT — O SYNC
no
by default
by default
no
no
no
no
by default
by default
write ratios and frequencies, the number of threads or the read
and write patterns (random, sequential).
Unless stated otherwise, we conﬁgure NVCACHE as fol-
lows. Each entry in our NVM log is 4 KiB large. The log
itself is constituted of 16 million entries (around 64 GiB). The
RAM cache uses 250 thousand pages of 4 KiB each (around
1 GiB). The minimum number of entries before attempting to
batch data to the disk is 1 thousand. The maximum number of
entries in a batch is 10 thousand.
B. Comparison with other systems
In this experiment, we compare NVCACHE with other
systems. Table IV summarizes the different ﬁle systems
evaluated. We compare the normal version of NVCACHE
when it propagates the writes to an SSD formatted in Ext4
(NVCACHE+SSD) with ﬁve other systems and a variant of
NVCACHE.
Speciﬁcally, we evaluate: (i) DM-WriteCache, which asyn-
chronously propagates the writes from the volatile kernel page
cache to an NVMM write cache, and only later propagates
the writes from NVMM to an SSD formatted in Ext4 (DM-
WriteCache+SSD); (ii) the Ext4 ﬁle system directly stored in
NVMM (Ext4-DAX); (iii) the NOVA ﬁle system [57], tailored
to efﬁciently use NVMM (NOVA), (iv) an SSD formatted in
Ext4 (SSD); (v) a temporary ﬁle system, which only stores the
data in volatile memory in the kernel page cache (tmpfs).
We also evaluate a variant of NVCACHE that propagates the
writes to the NVMM formatted with the NOVA ﬁle system
(NVCACHE+NOVA). This variant does not offer a large
storage space like NVCACHE+SSD but shows the theoretical
performance that we could expect from NVCACHE when
using an efﬁcient secondary storage.
NVCACHE+SSD, NOVA and NVCACHE+NOVA provide
the highest consistency guaranties since they offer both syn-
chronous durability (i.e., the data is durable when the write
call returns) and durable linearizability (essentially, a write
is only visible when it is durable). In order to make a fair
comparison, we also enforce synchronous durability for all the
ﬁle systems by activating the synchronous mode of our bench-
marks. Alternatively, on a non-synchronous benchmark, we
could open the ﬁles with the O_SYNC ﬂag, which guaranties
that a write is ﬂushed to disk when the system call returns.
We can also optimize these systems by using the O_DIRECT
ﬂag6, which tries to avoid an in-memory copy of the buffer
from the user space to the kernel space when possible. DM-
WriteCache+SSD, SSD and Ext4-DAX are not designed to
5Formally, NOVA provides synchronous durability and durable linearizabil-
ity when mounted with the cow_data ﬂag.
6https://ext4.wiki.kernel.org/index.php/Clarifying Direct IO%27s Semantics
ensure durable linearizability and do not offer the guarantee
that a write is visible only when it is durable. The tmpfs ﬁle
system does not provide durability, and thus no consistency
guaranty. Figure 3 presents our results, respectively for write-
oriented (left) and read-oriented (right) workloads.
Read-oriented workloads. Analyzing the read-oriented
workloads, we note how all the systems provide roughly the
same performance. This indicates that the different designs do
not signiﬁcantly change the read performance on these bench-
marks. Despite NOVA and Ext4 are reading from NVMM
while all the others are reading from an SSD, they all beneﬁt
from a volatile read cache stored in DDR4 RAM.
Write-oriented workloads. When we analyze the write-
oriented workloads, we observe that
the different designs
approaches have a large impact on performance. We ﬁrst
turn our attention to the systems that offer a large storage
space: NVCACHE+SSD, DM-WriteCache+SSD and SSD. We
observe that NVCACHE+SSD is consistently faster than the
other systems (at least 1.9×). SSD has the worst performance
as it does not leverage NVMM to boost performance. This is
not the case of DM-WriteCache+SSD and NVCACHE+SSD,
both using NVMM as a write cache to boost I/O performance.
However, NVCACHE+SSD performs signiﬁcantly better than
DM-WriteCache+SSD. Indeed, the design of NVCACHE+SSD
naturally offers synchronous durability, because the applica-
tion writes directly in the NVMM. Since, by design, DM-
WriteCache+SSD lives behind the volatile kernel page cache,
enforcing synchronous durability requires the execution of
additional code during a write. This code signiﬁcantly hampers
the performance of DM-WriteCache+SSD. Among the three
systems, NVCACHE+SSD is also the only system that ensures
durable linearizability.
We now focus on systems that offer strong correctness
guarantees but sacriﬁce the storage space: Ext4 and NOVA.
With RocksDB, NVCACHE+SSD is 1.4× faster than Ext4,
and NOVA is 1.6× better than NVCACHE+SSD. With SQLite,
NVCACHE performs better than NOVA (around 1.6× better),
and NVCACHE is roughly 3.7× better than Ext4. For some
workloads, NOVA is more efﬁcient than the other systems
because it was speciﬁcally tailored for NVMM and bypasses
the bottlenecks of Ext4 [57]. With RocksDB, we observe
that NVCACHE also suffers from these bottlenecks. Indeed,
when we use NVCACHE as an I/O booster in front of
NOVA instead of SSD (NVCACHE+NOVA), NVCACHE can
match and even improve performance as compared to NOVA.
Overall, these results show that NVCACHE is able to reach
performance comparable to a generic ﬁle system on NVMM,
while ignoring the limit of NVMM storage space. Our design
8
Fig. 3: Performance of NVCACHE for synchronous write-heavy (left) and read-heavy (right) workloads.
Fig. 4: Performance of NVCACHE under random write intensive loads for 20 GiB.
remains, however, less efﬁcient than a ﬁle system speciﬁcally
tailored for NVMM on some of the workloads because it
remains totally independent from the kernel I/O stack.
C. Analysis of NVCACHE
In this section, we analyse the behavior of NVCACHE
by using the FIO proﬁling tool. We conﬁgured FIO with
the fsync=1 ﬂag to ensure that a write is synchronously
durable, and with the direct=1 ﬂag to open all the ﬁles
in direct I/O mode. We set the buffer size to 4 KiB with
ioengine=psync. FIO measures are fetched every second.
Comparative behavior. The objective of this experiment
is to study the performance of NVCACHE in an ideal case. We
conﬁgure FIO to generate a random write intensive workload
and, in NVCACHE, we use a log of 32 GiB for 20 GiB of
written data. As a result, NVCACHE cannot saturate the log
and is never slowed down by the cleanup thread.
Figure 4 reports the performance with these settings. The
left graph shows the instantaneous throughput as it evolves
during the run. The middle graph shows how the average
latency evolves during the run. Finally, the right graph shows
how the total amount of written data evolves during the run.
For these measures, we split the run in small periods and
report the average throughput observed during each of them
(instantaneous throughput), as well as the average latency and
cumulative data written as measured from the beginning of the
run to the end of each period.
Figure 4 (left) shows that, in this ideal case, NVCACHE has
a better throughout than all the other systems. NVCACHE sig-
niﬁcantly outperforms SSD, Ext4-DAX and DM-WriteCache
(at least 1.5×). These systems are designed to leverage the
volatile Linux page cache in order to improve read per-
formance, which makes them inefﬁcient for writes when
an application requires strong consistency guaranties such
as synchronous durability. We can observe that NVCACHE
also outperforms NOVA on this benchmark (493 MiB/s vs.
403 MiB/s on average). NVCACHE is slightly more efﬁcient
than NOVA in this ideal case because NVCACHE never calls
the system during a write, whereas NOVA has to pay the cost
of system calls on the critical path. In Figure 4 (middle) and
Figure 4 (right), we can also observe that the average latency
and the written data are better in NVCACHE than in other
systems. As a result, in this ideal case, NVCACHE writes
all the data in 42 s, while it takes 51 s for NOVA, 71 s for
DM-WriteCache+SSD, 2 min and 29 s for Ext4 and more than
22 min for SSD.
Log saturation. During a long run that intensively writes,
NVCACHE may saturate its log, because the cleanup thread
can only propagate the writes from the log to the SSD at the
speed of the SSD, which is much slower than NVMM. The
next experiment highlights this behavior. Using a workload to
intensively write 20 GiB at random locations, we measure the
performance of NVCACHE with different log sizes.
Figure 5 reports the result of NVCACHE with these settings
(instantaneous throughput on the left, average latency in the
middle and cumulative data written on the right). With a
log of 32 GiB and 20 GiB of written data,
the log never
saturates. Hence, as in Figure 4 (see §IV-C), the instantaneous
throughput and the average latency remain stable.
As shown in Figure 5 (left), we can observe two phases with
a log of 8 GiB. During a ﬁrst phase that starts at 0 and ends
at 18 s, the throughput remains stable and high (556 MiB/s on
average). At the end of this phase, the throughput suddenly
collapses to 78 MiB/s and then remains stable until the end of
the run. In this experiment NVCACHE collapses at 18 s be-
cause of the log saturation. Still, before saturation, NVCACHE
is as fast as with a log of 32 GiB and only limited by the
9
ﬁllrandomﬁllseqoverwrite050100150Latency[µs]RocksDBNVCache+SSDdm-writecache+SSDExt4-DAXNOVASSDtmpfsNVCache+NOVAﬁllseqsyncﬁllrandsyncSQLitereadrandomreadseq02468Latency[µs]RocksDBreadrandomreadseqSQLite2952932972153261993170204060800200400Time[s]Throughput[MiB/s]ThroughputNVcacheSSD(ODIRECT)EXT4-DAXNOVAdm-writecache0204060800100200Time[s]Latency[µs]Latency02040608001020Time[s][GiB]WrittendataFig. 5: Performance of NVCACHE under random write intensive loads for 20 GiB, with variable NVMM log size.
Fig. 6:
Inﬂuence of batching and batch size parameter.
performance of the NVMM. During the ﬁrst phase, NVCACHE
ﬁlls the log but the cleanup thread cannot empty it fast enough
because the cleanup thread is limited by the performance of
the SSD. After saturation, FIO has to wait for the cleanup
thread, which limits performance to the speed of the SSD.
latency and written data). After saturation, we can observe the
batch size inﬂuence on performance. With a very small batch
size, the throughput is as low as 21 MiB/s: for each write,
NVCACHE triggers a fsync, which makes NVCACHE less
efﬁcient than the SSD conﬁgured with O_DIRECT because of
the cost of the system call. After the saturation, we observe that
NVCACHE becomes more efﬁcient with larger batch sizes,
for two reasons: First, NVCACHE does not call fsync often,
which increases its performance. Then, because NVCACHE
ﬁrst writes a batch in the volatile kernel page cache, writes
that modify the same location in the ﬁle are combined in
volatile memory, decreasing the number of pages Linux has to
propagate to the SSD upon fsync[43]. We also observe that
the difference between a batch size of 100, 1000 and 5000
remains low. This result shows that, as soon as the batch size
becomes large enough, the inﬂuence of this parameter is low.
Read cache size effect. As presented in §II-A, since
the Linux page cache becomes stale when a pending write
in the log is not propagated, NVCACHE relies on a small
read cache to ensure consistency. In this experiment, we
show how NVCACHE reacts to different read cache sizes.
Figure 7 presents the write (left) and read (right) throughput
of NVCACHE with a random read/write workload (50%/50%)
and a ﬁle of 10 GiB. Since the read cache is required for
consistency, we cannot deactivate it. We start thus with a very
small cache of 100 entries (400 KiB) for which the probability
of cache hit is negligible. The results conﬁrm how the size of
the read cache does not affect performance. We can observe
that with a large cache of 4 GiB (1M entries) where the
probability of cache hit reaches 40%, the performance remains
the same. This result is expected since NVCACHE relies on
the kernel page cache to improve performance. NVCACHE
10
We can observe a similar pattern for the average latency in
Figure 5 (middle). The latency remains stable and high before
saturation and then starts degrading (note that the latency does
not collapse because the ﬁgure reports the average latency
since the beginning of the run). We can also observe that
the slope of throughput curves change when the log saturates
in Figure 5 (right): when reaching saturation, the number of
written data increases much slower. We can observe exactly
the same behavior with smaller log sizes, but the log saturates
earlier. Interestingly, with a log of 100 MiB, 1 GiB and 8 GiB,
the write throughput becomes identical as soon as the log
saturates, staying at around 80 MiB/s (which corresponds to
the throughput of our SSD performing random writes).
Batching effect. Since a call to fsync is costly, the
cleanup thread uses batching to avoid calling fsync for each
write in the log. Technically, the cleanup thread consumes
entries by batches of a given size. If the log does not contain
enough entries, the cleanup thread does not propagate them
and waits. In this experiment, we analyze how NVCACHE re-
acts to different batch sizes by running a workload intensively
writing 20 GiB at random locations. We use a log of 8 GiB, in
order to observe how NVCACHE reacts to batch sizes before
and after the saturation.
Figure 6 reports the instantaneous throughput, the average
latency and cumulative data written during the run with these
settings. As in §IV-C, we can ﬁrst observe the log saturation
at 18 s. We can also observe that, before saturation, chang-
ing the batch size does not affect performance (throughput,
020406080200400Time[s]Throughput[MiB/s]ThroughputLogsize:100MB1G8G32G020406080204060Time[s]Latency[µs]Latency02040608001020Time[s][GiB]Writtendata0501000200400600Time[s]Throughput[MiB/s]ThroughputBatchsize:1entry10entries100entries500entries1000entries5000entries0501002040Time[s]Latency[µs]Latency050100051015Time[s][GiB]WrittendataFig. 7: NVCACHE with different read cache size, under mixed read/write loads.
only uses the read cache to ensure that in case of dirty read
(read of a page with a pending write in the log), the read is
correct and includes the pending writes. Our evaluation shows
that, because of this redundancy, the size of the NVCACHE
read cache does not inﬂuence performance, which shows that
we can keep the read cache small in NVCACHE.
V. RELATED WORK
Non-volatile memory solutions have been extensively stud-
ied. The recent introduction in the market of DRAM-based
PMEM (i.e., NVDIMM Optane DC persistent memory mod-
ules [7]) initiated a new stream of research exploiting the
beneﬁts of these persistent units, as well as novel commercial
offerings [22]. Studies highlighted their performance trade-
offs [30], [59], some of the compromises for porting legacy
applications to the ofﬁcial Intel PMDK [55], as well as the
substantial engineering efforts to port complex systems such
as Redis and memcached [17] or LevelDB [40]).
Interestingly, the impact of non-volatile memories has been
evaluated in the context of databases [10], [2], also including
approaches with ﬂash memories [34] or as direct acceler-
ators for main memory [45]. As shown in our evaluation,
NVCACHE can transparently boost complex DBMS systems,
e.g., SQLite or RocksDB, without changes to their source
code. To achieve this transparency, NVCACHE intercepts and
redirects in user-space I/O function calls, similar to what
SplitFS [33] does using Quill [25], an automatic system call