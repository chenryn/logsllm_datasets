Fast Search in Hamming Space with Multi-index Hashing
Mohammad Norouzi Ali Punjani David J. Fleet
University of Toronto
Problem Context Multi-Index Hashing – Idea Time and Space Complexity Experiments
Open Problem: Exact sub-linear nearest neighbor search in Imagine a dataset of 15-bit codes, and a search radius of r = 2. Notation: Hash Functions:
Hamming distance on binary codes. Black marks depict bits that differ from a given query. n: number of binary codes
− LSH: Locality-sensitive Hashing [1]
b: bit length
− MLH: Minimal loss hashing [2]
Context: Fast similarity search with large, high-dimensional
r: radius of Hamming search
datasets: images, videos, documents, .
m: number of substrings
Datasets:
s: substring length (s = b/m)
1. Map data-points onto similarity-preserving binary codes:
− 1 Billion SIFT descriptors [3]
- Similar data items should map to nearby codes
substring length s=log n
(Note: the first 3 codes are the 2-neighbors of the query.) Assume: 2 − 80 Million tiny images (GIST) [4]
- Dissimilar data items map to distant codes
uniformly distributed codes (for run-time)
Retrieval Speed:
Key Idea: Partition the codes into 3 substrings. Then, instead
Run-time: b
. . . . . . . . . H(r/b)
query cost ≤ 2 n ,
of searching r =2 in the full codes, search r =0 in the substrings. speed-up factors for kNN
log n
2
dataset nbits map 1-NN 10-NN 100-NN 1000-NN lin. scan
↓ ↓ ↓ ↓ where H() ≡ − log  − (1−) log (1−) .
MLH 213 205 182 126
2 2
64 18.03s
. . . . . . . . .
110010 100010 000101 001101 SIFT LSH 229 213 175 107
r/b ≤ 0.06 r/b ≤ 0.11 r/b ≤ 0.17 1B MLH 272 170 87 37
2. Perform nearest-neighbor search in the Hamming space. 128 35.33s
LSH 204 114 56 25
   √   
1/3 b n 2/3 MLH 161 128 78 33
b n b n
(1) (m) 64 1.41s
Why binary codes? In general, partition codes into m substrings h ≡ (h , . . . , h ). O O O
log n log n log n Gist LSH 169 80 31 8
2 2 2
Instead of exploring a Hamming ball of radius r in the full codes,
− Binary codes are storage-efficient. 79M MLH 58 21 11 6
128 2.74s
search a radius of r/m in the substrings. This works because:
LSH 28 12 6 3
Storage: Multi-index hashing requires m = n/ log n hash tables.
− Hamming distance is inexpensive to compute.
2
Each hash bucket stores identifiers for its codes. We also store n
Proposition: When two binary codes h and g differ by r bits
codes of length b bits. Thus, it can be shown that:
Key Tasks: Given a corpus of b-bit codes, and a query q,
or less, then, in at least one of their m substrings they must Run-times per query for multi-index hashing with 1, 10, 100, and
− Find r-neighbors: find all codes in the database that differ from differ by at most r/m bits, i.e., 1000 nearest neighbors, and a linear scan baseline on 1B codes
space complexity is O (n b + n log n)
2
q in r bits or less (aka. Point-Location in Equal Balls). from 128D SIFT descriptors:
 r 
(k) (k)
h − g ≤ r =⇒ ∃k h − g  ≤ ,
H H
− kNN: find k codes with k smallest Haming distances from q. m Cost Model
64-bit LSH 128-bit MLH
where  .  is the Hamming norm.
20 40
H
Linear scan Linear scan
1000−NN 1000−NN
Run-time per query depends on the #lookups and the #candi- 100−NN 100−NN
− Resembles the pigeonhole principle. )s( )s(
Linear Scan Hash Indexing 15 10−NN 30 10−NN
vs.
dates. In general, 1−NN 1−NN yreuq yreuq
− This condition is necessary but not sufficient.
Thus, we retrieve a superset of r-neighbors, 10 20
#lookups = m V (s, r/m) rep rep
How to structure the database, so that r-neighbors and kNN
and then cull the non-r-neighbors.
emit emit
queries can be answered quickly?
5 10
s
For n uniformly distributed codes we expect n/2 codes per hash
Key benefits of Multi-Index Hashing:
(1) Exhaustive search (i.e., linear scan through the database) bucket, so we expect
0 0
∼ 50 million comparisons/second. n 0 200 400 600 800 1000 0 200 400 600 800 1000
− Search occurs on much smaller binary code lengths
dataset size (millions) dataset size (millions)
#candidates = m V (s, r/m)
2s
0.2
− Search radius is much smaller
(2) Populate a hash table with the database codes. At query time, 1
Linear scan
Linear scan
1000−NN
1000−NN
flip bits of q and lookup the entries in the vicinity of q.
Assuming the cost of 1 lookup equals the cost of 1 candidate test: 100−NN
100−NN )s(
0.8 0.15 10−NN )s(
10−NN
1−NN yreuq
1−NN Multi-Index Hashing – Algorithm yreuq
 n  b  n 
0.6
Issues with hash indexing: s H(r/b)
cost(s) = m 1 + V (s, r/m) ≤ 1 + 2
0.1
2s s 2s rep rep
0.4
− Volume of the Hamming ball grows near-exponentially in r.
Data structure: emit emit
b b b 0.05
V (b, r) = 1 + + + . . . +
0.2
1 2 r
− Given m, partition each database code into m disjoint pieces.
Optimal Substring Length
0 0
− Generate m hash tables with the m substrings of each code.
9 200 400 600 800 1000 200 400 600 800 1000
dataset size (millions) dataset size (millions) )
01
gol(
At the extremes:
)s )s
1 Linear scan Linear scan
6 Finding r-neighbors: 1000−NN 1 1000−NN stekcuB 01 01
− when s=b, then #lookups = V (b, r), which grows too quickly.
100−NN 100−NN gol( gol(
0
Given a query q with substrings {q(i) }m , 10−NN 10−NN
0
32 bits i=1 − when s=1, then #candidates = n, i.e., the entire dataset. 1−NN 1−NN
yreuq yreuq
3 64 bits −1 sqrt(n) sqrt(n)
th hsaH
1. Lookups: search the i substring hash table for entries that
−1
128 bits
Analysis based on Stirling’s approximation shows that the optimal
−2
(i)
256 bits are within a Hamming distance r/m of q , thereby retrieving rep rep
#
0 −2
substring length puts approximately one database entry in each
0 2 4 6 8 10 a set of candidates, denoted N (q). −3 emit emit
i
Hamming Radius
∗
substring hash bucket on average: s ≈ log (n).
−3
2
2. Candidates: Take the union of the m sets, N (q) = N (q), −4 gol gol
i
i
− For typical databases / tasks, a large search radius r is necessary. and prune the duplicates. The set N (q) is necessarily a superset Plots show cost and its upper bound versus substring length, here −4
4 5 6 7 8 9 4 5 6 7 8 9
dataset size (log ) dataset size (log )
The following plot is produced from 1B LSH codes on SIFT. of the r-neighbors of q. with b = 128 bits. Note how minima are aligned at s∗ ≈ log (n). 10 10
2
3. Evaluation: Compute the Hamming distance between q and
Conclusions / References dedeen
each candidate in N (q), retaining only the true r-neighbors. r/b = 0.25 12
n = 10
15
20 r/b = 0.15 9
n = 10
15 ) )
r/b = 0.05 01 01
6
Finding kNNs: n = 10 suidaR gol( gol(
15 10
Algorithm for exact nearest neighbor search in Hamming distance
Find r-neighbors with progressively increasing values of r until k 10 with theoretical guarantees and strong empirical results. tsoc tsoc
10
gnimmaH
5
items are found.
5
64 bits 5 [1] Charikar (2002) Similarity estimation techniques from rounding algorithms.
128 bits 1 0 20 30 40 50 60 −2 0 −10 0 10 20 STOC. seireuq seireuq
0
substring length (bits) substring length − log n (bits)
1 10 100 1000
[2] Norouzi & Fleet (2011) Minimal Loss Hashing for compact binary codes.
0.1 0.1
# Near neighbors
fo fo
ICML.
0.05 0.05 Left: for different search radii, all with n = 109 codes. noitcarF noitcarF
[3] Jegou, Tavenard, Douze, Amsaleg (2011) Searching in one billion vectors:
Conclusion: For binary codes longer than 32 bits, linear scan is
0 0
0 5 10 15 20 25 30 0 5 10 15 20 25 30 Right: for 3 database sizes, all for search radii r = 0.25 b. re-rank with source coding. ASSP.
more effective than vanilla hash indexing.
Hamming radii needed for 10−NN Hamming radii needed for 1000−NN
(curves are displaced horizontally by − log (n)). [4] Torralba, Fergus, Freeman (2008) 80 million tiny images: A large data set
1B 128-bit LSH codes 2
for nonparametric object and scene recognition. PAMI.
|---|--|-----|-|-|-|-|-|--------|
| 0 | SIFT
1B  |  64 | MLH
LSH  | 213
229  | 205
213  | 182
175  | 126
107  | 18.03s |
| 1 |  | 128 | MLH
LSH  | 272
204  | 170
114  | 87
56  | 37
25  | 35.33s |
| 2 | Gist
79M  |  64 | MLH
LSH  | 161
169  | 128
80  | 78
31  | 33
8  | 1.41s  |
| 3 |  | 128 | MLH
LSH  | 58
28  | 21
12  | 11
6  | 6
3  | 2.74s  |
|---|-------------|------|--|--|-----|--|--|--------------------|--|--|--|--------|--|
| 0 | Experiments |      |  |  |     |  |  |                    |  |  |  |        |  |
| 1 | time per query (s)
time per query (s)
Hash Functions:
− LSH: Locality-sensitive Hashing [1]
− MLH: Minimal
loss hashing [2]
Datasets:
− 1 Billion SIFT descriptors [3]
− 80 Million tiny images (GIST) [4]
Retrieval Speed:
speed-up factors for kNN
dataset
nbits
map
1-NN
10-NN
100-NN 1000-NN lin. scan
Run-times per query for multi-index hashing with 1, 10, 100, and
1000 nearest neighbors, and a linear