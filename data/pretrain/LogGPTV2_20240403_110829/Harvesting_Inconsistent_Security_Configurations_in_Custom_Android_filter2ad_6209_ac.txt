We aim to ﬁnd if the same component has different
protection requirements on similar images. Protection
mismatch might not necessarily indicate a ﬂaw if the
component is not exposed. That’s why, we only consider
protection mismatches in case of exported components.
We list three cases where a component can be unin-
tentionally exposed on one image, but protected on other
images. First is the permission requirement is removed
from the component’s declaration. Second is the permis-
sion protecting it is of lower privilege compared to other
images. Third, the permission used is not deﬁned within
the image, which makes it possible for any third-party
app to deﬁne it and consequently invoke the underly-
ing component. To discover components with conﬂict-
ing protections, we map used permissions to their dec-
larations within the same image. Any mismatch would
indicate a possible security ﬂaw for this component.
Formally, let Pe represents the permission protecting a
component e ∈ Ec. We deﬁne the following feature:
f ne = Protection(e);
Where Protection(e) is deﬁned as:
Protection(e) =⎧⎪⎪⎪⎨⎪⎪⎪⎩
0 if e is not deﬁned
1 if Pe is None; i.e. e is not protected
ProtectionLevel(Pe) otherwise
In the case where e is a content provider, we deﬁne Pread
and Pwrite representing its read and write permissions and
extract f ne for both cases.
4 Data Generation
To reveal whether customization parties change the con-
ﬁgurations of the mentioned security features, we con-
duct a large scale differential analysis. We collected
591 Android ROMs from Samsung Updates [4], other
sources [3, 1, 2], and physical devices. These images
are customized by 11 vendors, for around 135 models,
45 regions and 8 carriers. They operate Android ver-
sions from 4.1.1 to 5.1.1. Details about the collected im-
ages are in Table 2. In total, these images include on av-
erage 157 apps per image and 93169 all together apps.
To extract the values of the selected security features
on each image, we developed a tool called DroidDiff.
For each image, DroidDiff ﬁrst collects its framework
resources Apks and preloaded Apks then runs Apktool
to extract the corresponding manifest ﬁles. Second, it
collects conﬁguration ﬁles under /etc/permission/. Then,
Table 2: Collected Android Images
Version
Jelly Bean
KitKat
Lollipop
Total
# of Distinct Vendors
# of images
9
9
8
11
102
177
312
591
Table 3: Security Conﬁgurations Map
Image
I1: Xiaomi RedMi 1
Version: 4.4.2
I2: Xiaomi Mi 2A
Version: 4.1.1
e ∈ EP
MIPUSH_RECEIVE
e ∈ EGID
camera GID
Signature
Normal
e ∈ EC
sms
True
Unspeciﬁed
Dangerous
False
DroidDiff searches the extracted manifests and conﬁgu-
ration ﬁles for the deﬁnitions of the targeted entities (EP,
EPB, EGID and EC). Finally, DroidDiff runs the generated
values through our differential analysis methodologies,
discussed in the next section.
5 Differential Analysis
In our analysis, we aim to detect any feature f ne hav-
ing inconsistent values throughout a candidate set of im-
ages. Any inconsistency detected indicates a potential
unintentional conﬁguration change introduced by a cus-
tomization party and requires further security analysis to
assess possible consequent damages.
Let f v( f ne,img) represent the value of the feature f ne
on a given image img. To illustrate f ne to f v( f ne,img)
mappings, consider this real world example depicted
in Table 3. As shown, we extract 3 security features
and their corresponding values from 2 Xiaomi images.
For the custom permission e = MIPUSH_RECEIVE,
our feature extraction step generates the following val-
ues
f v( f ne,I1) = Signature, and f v( f ne,I2) =
Unspecified.
Let IMG denote a set of candidate images to be com-
pared, we deﬁne a feature f ne as inconsistent if:
C( f ne) = ∃ x ∃ y [ x ∈ IMG∧ y ∈ IMG
∧ x = y∧ f v ( f ne ,x) = f v ( f ne ,y) ]
The above statement means that we consider the feature
f ne inconsistent across the set IMG if there exists at least
two different images where the value of f ne is not equal.
It should be noted that we do not consider any cases
where f v( f ne,img) = 0 for e ∈ {EP, EGID and EC}.
Sample Selection. To discover meaningful inconsis-
tencies through differential analysis, our collected im-
ages should be clustered based on common criteria. A
meaningful inconsistency would give us insights about
the responsible party that introduced it. For example,
1158  25th USENIX Security Symposium 
USENIX Association
to reveal if inconsistencies are introduced by an OS up-
grade, it would not make sense to select images from all
vendors, as the inconsistency could be due to customiz-
ing the device for a speciﬁc vendor, rather than because
of the OS upgrade. Similarly, to uncover if a speciﬁc
vendor causes inconsistencies in a new model, it is not
logical to compare it with models from other vendors.
Rather, we should compare it with devices from the same
vendor. Besides, to avoid detecting a change caused by
OS version mismatches, the new model should be com-
pared to a model running the same OS version.
We designed ﬁve different algorithms that target to un-
cover meaningful inconsistencies. Speciﬁcally, by care-
fully going through each party within the customization
chain, we designed algorithms that would reveal incon-
sistencies (if any) caused by each party. Further, for each
algorithm, we select our candidate images based on spe-
ciﬁc criteria that serve the purpose of the algorithm,
We describe each algorithm as well as the sample se-
lection criteria in the next sections.
A1: Cross-Version Analysis. This analysis aims to
uncover any inconsistent security features caused by OS
version upgrades. We select candidate image sets run-
ning similar device models to make sure that the incon-
sistency is purely due to OS upgrade. For instance,we
would pick 2 Samsung S4 devices running 4.4.4 and
5.0.1 as a candidate image set, and would reveal if up-
grading this model from 4.4.4 to 5.0.1 causes any secu-
rity conﬁguration changes. Formally, let IMGMODEL de-
note the candidate image set as the following:
IMGMODEL ={img1,img2, ...,imgn}
such that imgi ∈ IMGMODEL if model(imgi) =MODEL
Based on our collected images, this algorithm generated
135 candidate image sets (count of distinct model).
Let f v( f ne,img) denote a value for a feature f ne in img ∈
IMGMODEL. We deﬁne the inconsistency condition under
Cross-Version analysis algorithm as follows,
CVersion( f ne) = ∃ x ∃ y [ x ∈ IMGMODEL ∧ y ∈ IMGMODEL
∧ x = y∧ f v ( f ne ,x) = f v ( f ne ,y)
∧ version(x) = version(y)]
The above condition implies that f ne is inconsistent if
there exist two same model images running different ver-
sions, and where the values of f ne is not the same. Droid-
Diff runs the analysis for each of the 135 candidate sets
and generate the number of inconsistencies detected.
A2: Cross-Vendor Analysis. This analysis aims to re-
veal any feature f ne that is inconsistent across vendors.
To make sure that we are comparing images of similar
criteria across different vendors, we pick candidate im-
age sets running the same OS version (e.g. HTC M8 and
Nexus 6 both running 5.0.1). Our intuition here is that
if an inconsistency is detected, then the vendor is the re-
sponsible party. We formally deﬁne the candidate image
set as the following:
IMGVERSION ={img1,img2, ...,imgn}
such that imgi ∈ IMGV ERSION if version(imgi) = V ERSION
This algorithm generated 12 candidate image sets (count
of distinct OS versions that we collected).
Let f v( f ne,img) denote a value for a feature f ne in img
∈ IMGV ERSION. We redeﬁne the inconsistency condition
under Cross-Vendor analysis as follows:
CVendor( f ne) = ∃ x ∃ y [ x ∈ IMGV ERSION ∧ y ∈ IMGV ERSION
∧ x = y∧ f v ( f ne ,x) = f v ( f ne ,y)
∧ vendor(x) = vendor(y)]
The last condition implies that f ne is inconsistent if there
exists two images from different vendors, but running the
same OS version, where its value is not equal.
A3: Cross-Model Analysis.
In this analysis, we want
to uncover any feature f ne that is inconsistent through
different models. For example, we want to compare the
conﬁgurations on Samsung S5 and Samsung S4 mod-
els, running the same OS versions. To ascertain that any
inconsistency is purely due to model change within the
same vendor, we pick our candidate image sets running
the same OS version, deﬁned as IMGVERSION in the previ-
ous example. We further make sure that we are compar-
ing models from the same vendor by adding a new check
in the next condition.
Let
f v( f ne,img) denote a value for
in img ∈
IMGV ERSION. We redeﬁne the inconsistency condition un-
der Cross-Model analysis as follows:
f ne
CModel ( f ne) = ∃ x ∃ y [ x ∈ IMGV ERSION ∧ y ∈ IMGV ERSION
∧ x = y∧ f v ( f ne ,x) = f v ( f ne ,y)
∧ vendor(x) =vendor (y) ∧ model(y) = model(x)]
The last condition implies that f ne is inconsistent if there
exists two images from the same vendor, running the
same OS version, but customized for different models,
where its value is not equal.
A4: Cross-Carrier Analysis. We aim to uncover any
inconsistent security features f ne through different car-
riers (e.g., a MotoX from T-Mobile, versus another one
from Sprint). To make sure that we are comparing im-
ages running the same OS version, we pick our candi-
date image sets from IMGVERSION. We further make sure
that we are comparing images running the same model
USENIX Association  
25th USENIX Security Symposium  1159
as shown in the following inconsistency condition:
CCarrier( f ne) = ∃ x ∃ y [ x ∈ IMGV ERSION ∧ y ∈ IMGV ERSION
∧ x = y∧ f v ( f ne ,x) = f v ( f ne ,y)
∧ carrier(x) = carrier(y) ∧ model(y) =model (x)]
The last conditions in the above deﬁnition of CCarrier
implies that f ne is inconsistent if there exists two images
running the same model and OS versions, but from dif-
ferent carriers where its value is not the same.
A5: Cross-Region Analysis. This analysis intends to
ﬁnd any inconsistencies in the conﬁguration of security
features f ne through different regions (e.g. LG G4, Ko-
rean edition versus US edition). Any inconsistencies de-
tected will be attributed to customizing a device for a
speciﬁc region. We pick our candidate image sets from
IMGVERSION to make sure that we are comparing images
running the same OS version. We deﬁne the inconsis-
tency count under Cross-Carrier analysis as follows:
CRegion( f ne) = ∃ x ∃ y [ x ∈ IMGV ERSION ∧ y ∈ IMGV ERSION
∧ x = y∧ f v ( f ne ,x) = f v ( f ne ,y)
∧ region(x) = region(y) ∧ model(y) =model (x)]
The last conditions in the above deﬁnition of CRegion
implies that f ne is inconsistent if there exists two images
running the same model and OS versions, but from dif-
ferent regions where its value is not the same.
6 Results and Findings
We conduct a large-scale differential analysis on our col-
lected images using the aforementioned methodologies
with the help of DroidDiff. The analysis discovered a
large number of discrepancies with regards to our se-
lected features.
In this section, we present the results
and ﬁndings.
6.1 Overall Results
Figure 3 shows the overall changes detected from our
analysis. We plot the average percentage of inconsisten-
cies detected for each feature category using the ﬁve dif-
ferential analysis algorithms. To provide an estimate of
the inconsistencies count, each box plot shows an aver-
age number of total common entities (appearing on at
least 2 images) in the image sets studied; we depict this
number as # total in the graph. Let us use the ﬁrst box
plot as an example to illustrate what the data means: un-
der the Cross-Version analysis (A1), DroidDiff generated
on average 673 common permissions per each studied
candidate sets. 50% of the candidate image sets contain
at least 4.8% of total permissions (around 32 out of 673)
having inconsistent protection levels; those in the top 25
percentile (shown in the top whisker) have at least 6%
(40) inconsistent permissions. Figure 3 also depicts the
image sets that are outliers, i.e., they have particularly
higher number of inconsistencies compared to the other
image sets in the same group. For instance, the candi-
date image set IMGVersion=4.4.2 in the Cross-Vendor analy-
sis (A2) contains around 10% of GIDs whose protections
are inconsistent.
As depicted in Figure 3, the Cross-Version analysis
(A1) detects the highest percentage of inconsistencies in
all 5 categories, which means that upgrading the same
device model to a different OS version introduces the
highest security conﬁguration changes. An intuitive rea-
son behind this is that through a new OS release, Android
might enforce higher protections on the corresponding