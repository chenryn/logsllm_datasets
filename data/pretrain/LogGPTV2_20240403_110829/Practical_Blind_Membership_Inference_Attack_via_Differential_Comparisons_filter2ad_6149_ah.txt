### Observation RQ6-2
The performance of all membership inference (MI) attacks, including BLINDMI, increases as the number of classes in the dataset grows. Figures 9 and 10 illustrate this trend, showing the F1-Score of various MI attacks against different nonmember-to-member ratios and the number of classes, respectively.

### Improving the Robustness of the Target Model
One method to enhance the robustness of the target model against MI attacks is to employ techniques evaluated in Section V-C. Among these, differential privacy-based approaches are considered the most effective for defending against BLINDMI. Additionally, combining existing attack methods could be a promising future direction for research.

### Related Work
Machine learning models are vulnerable to various privacy attacks, such as model inversion [12], [13], membership inference [43], property inference [2], [14], and model and hyperparameter stealing [46], [47]. This work focuses on membership inference (MI) attacks, which we will discuss in Sections VII-A and VII-B.

#### A. Existing Membership Inference (MI) Attacks
Membership inference attacks date back to 2008 when Homer et al. [22] first proposed an MI attack on biological data. In 2017, Shokri et al. [43] introduced the first modern MI attack against deep neural networks using a shadow model and a binary classifier.

Prior attack methods include:
- **Top3-NN Attack** by Salem et al. [41]: This attack selects the top three largest values from all confidence scores to train an MI classifier.
- **Top1-Threshold Attack** by Salem et al. [41]: This method compares the top feature from the output probability distribution with a threshold to classify the sample as a member.
- **Label-Only Attack** and **Loss-Threshold Attack** by Yeom et al. [49]: These attacks use ground-truth labels to compare with predicted labels and compute cross-entropy loss, respectively.

BLINDMI, in contrast, does not require a shadow model and extracts complex membership semantics through probing alone. Our evaluation shows that BLINDMI outperforms existing attacks under various adversarial settings.

Researchers have also developed theories on MI attacks. For example, Sablayrolles et al. [40] proposed an optimal strategy using a probabilistic framework, demonstrating that black-box attacks can be as effective as white-box attacks. BLINDMI validates the effectiveness of black-box attacks.

In addition to classification models, MI attacks have been proposed for generative models [15] and federated learning [37]. BLINDMI, however, targets single classification models.

#### B. Existing Defenses
Existing defenses against MI attacks include:
- **Regularization**: Techniques such as dropout, model stacking, L2-norm regularization, and min-max game mechanisms [43], [49], [36].
- **Adversarial Examples**: Methods like MemGuard, which adds noise to confidence scores to fool binary classifiers [26].
- **Privacy Enhancement**: Differential privacy-based approaches that add noise to the objective function or gradients during training [9], [11], [24].

### Discussion on Potential Defenses
Potential defenses against MI attacks include:
- **Limiting Adversary's Access**: Restricting the number of probes and the samples that can be probed, and providing only the predicted class information.
- **Regularization**: Using techniques like dropout, model stacking, and L2-norm regularization.
- **Adversarial Examples**: Generating adversarial examples to fool the inference model.
- **Privacy Enhancement**: Adding noise to the objective function or gradients to ensure differential privacy.

### Conclusion
This paper introduces BLINDMI, a novel MI attack that leverages differential comparison by moving samples between two sets. We implement three versions: BLINDMI-1CLASS, BLINDMI-DIFF-w/, and BLINDMI-DIFF-w/o. Our evaluation demonstrates that BLINDMI outperforms state-of-the-art attacks, even against DNNs with advanced defenses.

### Acknowledgment
We thank the anonymous reviewers for their valuable comments and feedback. This work was supported in part by the Johns Hopkins University Institute for Assured Autonomy, the National Science Foundation (NSF), and an IBM Faculty Award.

### References
[References listed as provided in the original text]

---

This revised version aims to improve clarity, coherence, and professionalism while maintaining the essential content and structure of the original text.