[Observation RQ6-2] The performance of all MI attacks,
including BLINDMI, increase as the number of classes in the
14
0510152025303540The number of moves per batch (batch size = 20)01234567DistanceAdultEyePACSCH-MNISTLocationPurchase-50TexasCIFAR-100Birds-200010002000300040005000The number of moves per batch (batch size = 1000)2345678DistanceAdultEyePACSCH-MNISTLocationPurchase-50TexasCIFAR-100Birds-200• Improving the robustness of the target model. The second
method is to improve the model robustness to MI attacks via
different methods evaluated in Section V-C. The differential
privacy-based approach is likely the best method for de-
fending against BLINDMI in the literature. A combination
of existing attacks may also be possible and this is left as a
future work for our study.
VII. RELATED WORK
Machine learning is vulnerable to different privacy at-
tacks including model inversion [12], [13], membership in-
ference [43], property inference [2], [14], as well as model
and hyperparameter stealing [46], [47]. Our work studies
membership inference (MI) attacks. We describe related work
on MI attacks and defenses in Section VII-A and VII-B.
A. Existing Membership Inference (MI) Attacks
Membership inference attacks originate back to 2008,
when, Homer et al. [22] ﬁrst proposed a MI attack on bio-
logical data, whereby an adversary could infer whether a data
sample belonged to a genome-based study knowing only parts
the genome and summary statistics. Then, in 2017, Shokri et
al. [43] proposed the ﬁrst modern MI attack against deep neural
networks with a shadow model and a binary attack classiﬁer.
Prior attack methods include the following. Salem et
al. [41] proposed several MI attacks. For example, the Top3-
NN attack of Salem et al., a variant NN Attack, picks the
top three largest values from all conﬁdence scores to train
an MI classiﬁer. For another example, the Top1-Threshold
attack of Salem et al. compares the top feature from the
output probability distribution with a threshold and classiﬁed
the sample as member if the top feature is larger than a
threshold. Similarly, Yeom et al. [49] also proposed two
attacks with the help of ground-truth labels: the ﬁrst label-
only attack compares the ground truth label with predicted,
and the second loss-threshold attack computes cross-entropy
loss and compares the computed loss with the average loss of
all training samples. As a comparison, BLINDMI is an attack
that does not need a shadow model but also extracts complex
membership semantics via probing only. Our evaluation shows
that BLINDMI outperforms existing attacks under different
adversarial settings.
Researchers have also proposed theories on MI attacks.
Sablayrolles et al. [40] proposed an optimal strategy for MI
attacks using a probabilistic framework that consists of both
Bayesian learning and noisy training. They showed that opti-
mal attacks only depend on the loss function, and thus black-
box attacks could be as good as whitebox attack. BLINDMI
actually proves the effectiveness of blackbox attacks.
In addition to attacks on classiﬁcation models, researchers
also have proposed MI attacks [19] on generative models [15]
and those [37] on federated learning. As a comparison,
BLINDMI is an attack on single classiﬁcation models rather
than generative models or federated learning.
B. Existing Defenses
We now describe existing defenses of MI attacks [31],
[32], [36], [44], especially those on classiﬁcation models. Note
that while existing defenses can prevent some existing MI
attacks with a reasonable performance, our evaluation shows
Fig. 9.
CIFAR-100.
F1-Score of Various Attacks vs. Nonmember-to-Member Ratio on
Fig. 10. F1-Score of Various Attacks vs. # of classes on CIFAR.
target model and this performance boost is more signiﬁcant
when the number of classes is small.
Figure 10 shows a steady improvement of all MI attacks as
the number of classes. The reason is that when the dataset has
more classes, the target model tends to generalize less, thus
being more vulnerable to MI attacks. This can also explain
why target models trained on CIFAR-100 and Birds-200 are
more vulnerable compared to other datasets.
It is also worth noting that Top1-threshold performs the
worst among all the MI attacks when the number of classes
equals two, but the performance improves when the number
becomes 100. That is, the top one probability score contains
more information as the number of classes increases. We
believe that this may be due to the fact that when the total
probability is shared by more classes, one can infer more
membership information from the top probability.
VI. A DISCUSSION ON POTENTIAL DEFENSES
In this section, we discuss potential defenses. There are two
possible venues of defenses as we have seen in the literature.
• Limiting adversary’s access to the target model. The ﬁrst
method is to limit the adversary’s access to the target model:
(i) restricting the number of probes and also which samples
can be probed, and (ii) providing only the predicted class
information as an output. The former will restrict BLINDMI
to BLINDMI-DIFF-w/o, which performs a little bit worse
than BLINDMI-DIFF-w/. The latter will reduce BLINDMI
to the Label-only attack in the blackbox setting.
15
0510152025303540Nonmember-to-Member Ratio020406080100F1-Score (%)NNTop3-NNTop2+TrueLoss-ThreLabel-OnlyTop1-ThreBlindMI (this work)020406080100Number of Classes020406080100F1-Score (%)NNTop3-NNTop2+TrueLoss-ThreLabel-OnlyTop1-ThreBlindMI (this work)that BLINDMI can still infer membership with a reasonable
F1-score, e.g., over 60%.
1) Regularization: Researchers have proposed to improve
privacy against MI attacks via different types of regularization.
For example, Salem et al. [49] demonstrated two effective
method of defending MI attacks, namely dropout and model
stacking. The former randomly deletes a ﬁxed proportion of
edges in a fully connected neural network model to improve
model robustness; the latter constructs a target model with mul-
tiple different machine learning models stacked together. For
another example, Shokri et al. [43] adopted L2-norm standard
regularization with a polynomial in the model’s loss function
to penalize large parameters. Nasr et al. [36] introduced a
min-max game mechanism to train models with membership
privacy, which ensures indistinguishability between the pre-
dictions of a model on its training data and other data points
from the same distribution. This strategy acts as an adversarial
regularizer that generalizes the model. In addition, Li et al. [30]
proposed to close the generalization gap by matching the
training and validation accuracies. Speciﬁcally, they adopted
a new set regularizer, called the Maximum Mean Discrepancy,
between the softmax output empirical distributions of the
training and validation sets during training.
2) Adversarial Example: Another direction is to borrow
ideas from adversarial machine learning and generate an ad-
versarial example for the inference model controlled by the ad-
versary. For example, Jia et al. [26] introduced a new defense,
called MemGuard, by adding noise to conﬁdence score output
from target models, thus fooling a binary classiﬁer. Unlike
previous adversarial examples [8], [16], [28], [33]–[35], [39],
[45], MemGuard calculates the gradient of the loss function to
ﬁnd an appropriate noise and guarantee the utility loss to be
zero.
3) Privacy Enhancement: Many differential privacy based
defenses [9], [11], [24] add noise to the objective function that
is used to learn a model or the gradients during optimizing the
objective function. Shokri et al. [42] designed a differential pri-
vacy method for collaborative learning of DNNs. Cao et al. [7]
showed that privacy-related data samples can be unlearned to
improve model privacy.
VIII. CONCLUSION
In this paper, we present a novel MI attack, called
BLINDMI, which adopts differential comparison moving sam-
ples in between two sets and making inference decisions. One
of the key insights used here is that, moving a member from a
mostly member dataset to a mostly non-member one will de-
crease the distance in feature space between two sets and vice-
versa. We implement three versions of BLINDMI, BLINDMI-
1CLASS (relying on one-class SVM), BLINDMI-DIFF-w/ (re-
lying on generation of nonmembers), and BLINDMI-DIFF-w/o
(relying on rough separations of members and non-members).
Our evaluation shows that BLINDMI outperforms existing
state of the art attacks, against not only a variety of DNN
architectures, but also against DNNs with state of the art
defenses deployed.
the Johns Hopkins University Institute for Assured Autonomy
with grant IAA 80052273, National Science Foundation (NSF)
grant CNS-18-54000 and CNS-19-37786, as well as an IBM
Faculty Award. The views and conclusions contained herein
are those of the authors and should not be interpreted as
necessarily representing the ofﬁcial policies or endorsements,
either expressed or implied, of NSF.
REFERENCES
[1] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov,
K. Talwar, and L. Zhang, “Deep learning with differential privacy,” in
Proceedings of the 2016 ACM SIGSAC Conference on Computer and
Communications Security, 2016, pp. 308–318.
[2] G. Ateniese, L. V. Mancini, A. Spognardi, A. Villani, D. Vitali, and
G. Felici, “Hacking smart machines with smarter ones: How to extract
meaningful data from machine learning classiﬁers,” International Jour-
nal of Security and Networks, vol. 10, no. 3, 2015.
[3] M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp,
P. Goyal, L. D. Jackel, M. Monfort, U. Muller, J. Zhang et al., “End
to end learning for self-driving cars,” arXiv preprint arXiv:1604.07316,
2016.
[4] K. M. Borgwardt, A. Gretton, M. J. Rasch, H.-P. Kriegel, B. Sch¨olkopf,
and A. J. Smola, “Integrating structured biological data by kernel
maximum mean discrepancy,” Bioinformatics, vol. 22, no. 14, pp. e49–
e57, 2006.
[5] P. Burlina, D. E. Freund, B. Dupas, and N. Bressler, “Automatic screen-
ing of age-related macular degeneration and retinal abnormalities,” in
2011 Annual International Conference of the IEEE Engineering in
Medicine and Biology Society.
IEEE, 2011, pp. 3962–3966.
[6] P. M. Burlina, N. Joshi, M. Pekala, K. D. Pacheco, D. E. Freund, and
N. M. Bressler, “Automated grading of age-related macular degeneration
from color fundus images using deep convolutional neural networks,”
JAMA ophthalmology, vol. 135, no. 11, pp. 1170–1176, 2017.
[7] Y. Cao and J. Yang, “Towards making systems forget with machine
unlearning,” in Proceedings of the 2015 IEEE Symposium on Security
and Privacy, 2015.
[8] N. Carlini and D. Wagner, “Towards evaluating the robustness of neural
IEEE,
networks,” in 2017 ieee symposium on security and privacy (sp).
2017, pp. 39–57.
[9] K. Chaudhuri, C. Monteleoni, and A. D. Sarwate, “Differentially private
empirical risk minimization.” Journal of Machine Learning Research,
vol. 12, no. 3, 2011.
[10] T. Chen, I. Goodfellow, and J. Shlens, “Net2net: Accelerating learning
via knowledge transfer,” 2016.
[11] C. Dwork, F. McSherry, K. Nissim, and A. Smith, “Calibrating noise
to sensitivity in private data analysis,” in Theory of cryptography
conference. Springer, 2006, pp. 265–284.
[12] M. Fredrikson, S. Jha, and T. Ristenpart, “Model inversion attacks
that exploit conﬁdence information and basic countermeasures,” in
Proceedings of
the 22nd ACM SIGSAC Conference on Computer
and Communications Security, ser. CCS 15. New York, NY, USA:
Association for Computing Machinery, 2015, p. 13221333. [Online].
Available: https://doi.org/10.1145/2810103.2813677
[13] M. Fredrikson, E. Lantz, S. Jha, S. Lin, D. Page, and T. Ristenpart,
“Privacy in pharmacogenetics: An end-to-end case study of personalized
warfarin dosing,” in USENIX Security Symposium, 2014.
[15]
[14] K. Ganju, Q. Wang, W. Yang, C. A. Gunter, and N. Borisov, “Property
inference attacks on fully connected neural networks using permutation
invariant representations,” in CCS, 2018.
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” in
Advances in neural information processing systems, 2014, pp. 2672–
2680.
I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing
adversarial examples,” arXiv preprint arXiv:1412.6572, 2014.
[16]
ACKNOWLEDGMENT
We want to thank anonymous reviewers for their helpful
comments and feedback. This work was supported in part by
[17] A. Graves, A.-r. Mohamed, and G. Hinton, “Speech recognition with
deep recurrent neural networks,” in 2013 IEEE international conference
on acoustics, speech and signal processing.
IEEE, 2013, pp. 6645–
6649.
16
[39] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and
A. Swami, “The limitations of deep learning in adversarial settings,” in
2016 IEEE European symposium on security and privacy (EuroS&P).
IEEE, 2016, pp. 372–387.
[40] A. Sablayrolles, M. Douze, C. Schmid, Y. Ollivier, and H. J´egou,
“White-box vs black-box: Bayes optimal strategies for membership
inference,” in International Conference on Machine Learning, 2019,
pp. 5558–5567.
[41] A. Salem, Y. Zhang, M. Humbert, M. Fritz, and M. Backes, “Ml-leaks:
Model and data independent membership inference attacks and defenses
on machine learning models,” in Network and Distributed Systems
Security Symposium 2019.
Internet Society, 2019.
[42] R. Shokri and V. Shmatikov, “Privacy-preserving deep learning,” in
Proceedings of the 22nd ACM SIGSAC conference on computer and
communications security, 2015, pp. 1310–1321.
[43] R. Shokri, M. Stronati, C. Song, and V. Shmatikov, “Membership
inference attacks against machine learning models,” in 2017 IEEE
Symposium on Security and Privacy (SP).
IEEE, 2017, pp. 3–18.
[44] L. Song, R. Shokri, and P. Mittal, “Privacy risks of securing machine
learning models against adversarial examples,” in Proceedings of the
2019 ACM SIGSAC Conference on Computer and Communications
Security, 2019, pp. 241–257.
[45] F. Tram`er, A. Kurakin, N. Papernot, I. Goodfellow, D. Boneh, and
P. McDaniel, “Ensemble adversarial training: Attacks and defenses,”
arXiv preprint arXiv:1705.07204, 2017.
[46] F. Tram`er, F. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart, “Stealing
machine learning models via prediction apis,” in USENIX Security
Symposium, 2016, pp. 601–618.
[47] B. Wang and N. Z. Gong, “Stealing hyperparameters in machine
learning,” in 2018 IEEE Symposium on Security and Privacy (SP).
IEEE, 2018, pp. 36–52.
[48] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie,
and P. Perona, “Caltech-UCSD Birds 200,” California Institute of
Technology, Tech. Rep. CNS-TR-2010-001, 2010.
[49] S. Yeom, I. Giacomelli, M. Fredrikson, and S. Jha, “Privacy risk in
machine learning: Analyzing the connection to overﬁtting,” in 2018
IEEE 31st Computer Security Foundations Symposium (CSF).
IEEE,
2018, pp. 268–282.
J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, “How transferable are
features in deep neural networks?” in Advances in neural information
processing systems, 2014, pp. 3320–3328.
[50]
[51] B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le, “Learning transferable
architectures for scalable image recognition,” in Proceedings of the
IEEE conference on computer vision and pattern recognition, 2018,
pp. 8697–8710.
[19]
[18] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Sch¨olkopf, and A. Smola,
“A kernel two-sample test,” Journal of Machine Learning Research,
vol. 13, no. Mar, pp. 723–773, 2012.
J. Hayes, L. Melis, G. Danezis, and E. De Cristofaro, “Logan: Mem-
bership inference attacks against generative models,” Proceedings on
Privacy Enhancing Technologies, vol. 2019, no. 1, pp. 133–152, 2019.
[20] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770–778.
[21] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a
neural network,” in NIPS Deep Learning and Representation Learning
Workshop, 2015. [Online]. Available: http://arxiv.org/abs/1503.02531
[22] N. Homer, S. Szelinger, M. Redman, D. Duggan, W. Tembe,
J. Muehling, J. V. Pearson, D. A. Stephan, S. F. Nelson, and D. W. Craig,
“Resolving individuals contributing trace amounts of dna to highly
complex mixtures using high-density snp genotyping microarrays,”
PLoS genetics, vol. 4, no. 8, 2008.
[23] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely
connected convolutional networks,” in Proceedings of the IEEE confer-
ence on computer vision and pattern recognition, 2017, pp. 4700–4708.
[24] R. Iyengar, J. P. Near, D. Song, O. Thakkar, A. Thakurta, and L. Wang,
“Towards practical differentially private convex optimization,” in 2019
IEEE Symposium on Security and Privacy (SP).
IEEE, 2019, pp. 299–
316.
[25] B.
[26]
[27]
and Q. Gu,
Jayaraman, L. Wang, D. Evans,
“Revisiting
membership inference under realistic assumptions,” arXiv preprint
arXiv:2005.10881, 2020.
J. Jia, A. Salem, M. Backes, Y. Zhang, and N. Z. Gong, “Memguard:
Defending against black-box membership inference attacks via adver-
sarial examples,” in Proceedings of the 2019 ACM SIGSAC Conference
on Computer and Communications Security, 2019, pp. 259–274.
J. N. Kather, C.-A. Weis, F. Bianconi, S. M. Melchers, L. R. Schad,
T. Gaiser, A. Marx, and F. G. Z”ollner, “Multi-class texture analysis in
colorectal cancer histology,” Scientiﬁc reports, vol. 6, p. 27988, 2016.
[28] A. Kurakin, I. J. Goodfellow, and S. Bengio, “Adversarial examples
in the physical world. corr abs/1607.02533 (2016),” arXiv preprint
arXiv:1607.02533, 2016.
[29] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed,
O. Levy, V. Stoyanov, and L. Zettlemoyer, “Bart: Denoising sequence-
to-sequence pre-training for natural language generation, translation,
and comprehension,” arXiv preprint arXiv:1910.13461, 2019.
J. Li, N. Li, and B. Ribeiro, “Membership inference attacks and
defenses in supervised learning via generalization gap,” arXiv preprint
arXiv:2002.12062, 2020.
[30]
[31] Y. Long, V. Bindschaedler, and C. A. Gunter, “Towards measuring
membership privacy,” arXiv preprint arXiv:1712.09136, 2017.
[32] Y. Long, V. Bindschaedler, L. Wang, D. Bu, X. Wang, H. Tang, C. A.
Gunter, and K. Chen, “Understanding membership inferences on well-
generalized learning models,” arXiv preprint arXiv:1802.04889, 2018.
[33] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards
deep learning models resistant to adversarial attacks,” arXiv preprint
arXiv:1706.06083, 2017.
[34] S.-M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard, “Univer-
sal adversarial perturbations,” in Proceedings of the IEEE conference
on computer vision and pattern recognition, 2017.
[35] S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard, “Deepfool: a simple
and accurate method to fool deep neural networks,” in Proceedings of
the IEEE conference on computer vision and pattern recognition, 2016,
pp. 2574–2582.
[36] M. Nasr, R. Shokri, and A. Houmansadr, “Machine learning with
membership privacy using adversarial regularization,” in Proceedings of
the 2018 ACM SIGSAC Conference on Computer and Communications
Security, 2018, pp. 634–646.
[37] ——, “Comprehensive privacy analysis of deep learning: Passive and
active white-box inference attacks against centralized and federated
learning,” in 2019 IEEE Symposium on Security and Privacy (SP).
IEEE, 2019, pp. 739–753.
[38] S. J. Oh, B. Schiele, and M. Fritz, “Towards reverse-engineering black-
box neural networks,” in Explainable AI: Interpreting, Explaining and
Visualizing Deep Learning. Springer, 2019.
17