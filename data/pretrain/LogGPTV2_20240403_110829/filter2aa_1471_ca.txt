Device. Then you can type the last command again. It should
succeed this time. Then assign the device to your VM by typing the
following:
Click here to view code image
Add-VMAssignableDevice -LocationPath 
"PCIROOT(0)#PCI(0302)#PCI(0000)" -VMName "Vibranium"
The last command should have completely removed the NVMe
controller from the host. You should verify this by checking the
Device Manager in the host system. Now it’s time to power up the
VM. You can use the Hyper-V Manager tool or PowerShell. If you
start the VM and get an error like the following, your BIOS is not
properly configured to expose SR-IOV, or your I/O MMU doesn’t
have the required characteristics (most likely it does not support
I/O remapping).
Otherwise, the VM should simply boot as expected. In this case,
you should be able to see both the NVMe controller and the NVMe
disk listed in the Device Manager applet of the child VM. You can
use the disk management tool to create partitions in the child VM
in the same way you do in the host OS. The NVMe disk will run at
full speed with no performance penalties (you can confirm this by
using any disk benchmark tool).
To properly remove the device from the VM and remount it in
the host OS, you should first shut down the VM and then use the
following commands (remember to always change the virtual
machine name and NVMe controller location):
Click here to view code image
Remove-VMAssignableDevice -LocationPath 
"PCIROOT(0)#PCI(0302)#PCI(0000)" -VMName
"Vibranium"
Mount-VMHostAssignableDevice -LocationPath 
"PCIROOT(0)#PCI(0302)#PCI(0000)"
After the last command, the NVMe controller should reappear
listed in the Device Manager of the host OS. You just need to
reenable it for restarting to use the NVMe disk in the host.
VA-backed virtual machines
Virtual machines are being used for multiple purposes. One of them is to
properly run traditional software in isolated environments, called containers.
(Server and application silos, which are two types of containers, have been
introduced in Part 1, Chapter 3, “Processes and jobs.”) Fully isolated
containers (internally named Xenon and Krypton) require a fast-startup type,
low overhead, and the possibility of getting the lowest possible memory
footprint. Guest physical memory of this type of VM is generally shared
between multiple containers. Good examples of containers are provided by
Windows Defender Application Guard, which uses a container to provide the
full isolation of the browser, or by Windows Sandbox, which uses containers
to provide a fully isolated virtual environment. Usually a container shares the
same VM’s firmware, operating system, and, often, also some applications
running in it (the shared components compose the base layer of a container).
Running each container in its private guest physical memory space would not
be feasible and would result in a high waste of physical memory.
To solve the problem, the virtualization stack provides support for VA-
backed virtual machines. VA-backed VMs use the host’s operating system’s
memory manager to provide to the guest partition’s physical memory
advanced features like memory deduplication, memory trimming, direct
maps, memory cloning and, most important, paging (all these concepts have
been extensively covered in Chapter 5 of Part 1). For traditional VMs, guest
memory is assigned by the VID driver by statically allocating system
physical pages from the host and mapping them in the GPA space of the VM
before any virtual processor has the chance to execute, but for VA-backed
VMs, a new layer of indirection is added between the GPA space and SPA
space. Instead of mapping SPA pages directly into the GPA space, the VID
creates a GPA space that is initially blank, creates a user mode minimal
process (called VMMEM) for hosting a VA space, and sets up GPA to VA
mappings using MicroVM. MicroVM is a new component of the NT kernel
tightly integrated with the NT memory manager that is ultimately responsible
for managing the GPA to SPA mapping by composing the GPA to VA
mapping (maintained by the VID) with the VA to SPA mapping (maintained
by the NT memory manager).
The new layer of indirection allows VA-backed VMs to take advantage of
most memory management features that are exposed to Windows processes.
As discussed in the previous section, the VM Worker process, when it starts
the VM, asks the VID driver to create the partition’s memory block. In case
the VM is VA-backed, it creates the Memory Block Range GPA mapping
bitmap, which is used to keep track of the allocated virtual pages backing the
new VM’s RAM. It then creates the partition’s RAM memory, backed by a
big range of VA space. The VA space is usually as big as the allocated
amount of VM’s RAM memory (note that this is not a necessary condition:
different VA-ranges can be mapped as different GPA ranges) and is reserved
in the context of the VMMEM process using the native
NtAllocateVirtualMemory API.
If the “deferred commit” optimization is not enabled (see the next section
for more details), the VID driver performs another call to the
NtAllocateVirtualMemory API with the goal of committing the entire VA
range. As discussed in Chapter 5 of Part 1, committing memory charges the
system commit limit but still doesn’t allocate any physical page (all the PTE
entries describing the entire range are invalid demand-zero PTEs). The VID
driver at this stage uses Winhvr to ask the hypervisor to map the entire
partition’s GPA space to a special invalid SPA (by using the same
HvMapGpaPages hypercall used for standard partitions). When the guest
partition accesses guest physical memory that is mapped in the SLAT table
by the special invalid SPA, it causes a VMEXIT to the hypervisor, which
recognizes the special value and injects a memory intercept to the root
partition.
The VID driver finally notifies MicroVM of the new VA-backed GPA
range by invoking the VmCreateMemoryRange routine (MicroVM services
are exposed by the NT kernel to the VID driver through a Kernel Extension).
MicroVM allocates and initializes a VM_PROCESS_CONTEXT data
structure, which contains two important RB trees: one describing the
allocated GPA ranges in the VM and one describing the corresponding
system virtual address (SVA) ranges in the root partition. A pointer to the
allocated data structure is then stored in the EPROCESS of the VMMEM
instance.
When the VM Worker process wants to write into the memory of the VA-
backed VM, or when a memory intercept is generated due to an invalid GPA
to SPA translation, the VID driver calls into the MicroVM page fault handler
(VmAccessFault). The handler performs two important operations: first, it
resolves the fault by inserting a valid PTE in the page table describing the
faulting virtual page (more details in Chapter 5 of Part 1) and then updates
the SLAT table of the child VM (by calling the WinHvr driver, which emits
another HvMapGpaPages hypercall). Afterward, the VM’s guest physical
pages can be paged out simply because private process memory is normally
pageable. This has the important implication that it requires the majority of
the MicroVM’s function to operate at passive IRQL.
Multiple services of the NT memory manager can be used for VA-backed
VMs. In particular, clone templates allow the memory of two different VA-
backed VMs to be quickly cloned; direct map allows shared executable
images or data files to have their section objects mapped into the VMMEM
process and into a GPA range pointing to that VA region. The underlying
physical pages can be shared between different VMs and host processes,
leading to improved memory density.
VA-backed VMs optimizations
As introduced in the previous section, the cost of a guest access to
dynamically backed memory that isn’t currently backed, or does not grant the
required permissions, can be quite expensive: when a guest access attempt is
made to inaccessible memory, a VMEXIT occurs, which requires the
hypervisor to suspend the guest VP, schedule the root partition’s VP, and
inject a memory intercept message to it. The VID’s intercept callback handler
is invoked at high IRQL, but processing the request and calling into
MicroVM requires running at PASSIVE_LEVEL. Thus, a DPC is queued. The
DPC routine sets an event that wakes up the appropriate thread in charge of
processing the intercept. After the MicroVM page fault handler has resolved
the fault and called the hypervisor to update the SLAT entry (through another
hypercall, which produces another VMEXIT), it resumes the guest’s VP.
Large numbers of memory intercepts generated at runtime result in big
performance penalties. With the goal to avoid this, multiple optimizations
have been implemented in the form of guest enlightenments (or simple
configurations):
■    Memory zeroing enlightenments
■    Memory access hints
■    Enlightened page fault
■    Deferred commit and other optimizations
Memory-zeroing enlightenments
To avoid information disclosure to a VM of memory artifacts previously in
use by the root partition or another VM, memory-backing guest RAM is
zeroed before being mapped for access by the guest. Typically, an operating
system zeroes all physical memory during boot because on a physical system
the contents are nondeterministic. For a VM, this means that memory may be
zeroed twice: once by the virtualization host and again by the guest operating
system. For physically backed VMs, this is at best a waste of CPU cycles. For
VA-backed VMs, the zeroing by the guest OS generates costly memory
intercepts. To avoid the wasted intercepts, the hypervisor exposes the
memory-zeroing enlightenments.
When the Windows Loader loads the main operating system, it uses
services provided by the UEFI firmware to get the machine’s physical
memory map. When the hypervisor starts a VA-backed VM, it exposes the
HvGetBootZeroedMemory hypercall, which the Windows Loader can use to
query the list of physical memory ranges that are actually already zeroed.
Before transferring the execution to the NT kernel, the Windows Loader
merges the obtained zeroed ranges with the list of physical memory
descriptors obtained through EFI services and stored in the Loader block
(further details on startup mechanisms are available in Chapter 12). The NT
kernel inserts the merged descriptor directly in the zeroed pages list by
skipping the initial memory zeroing.
In a similar way, the hypervisor supports the hot-add memory zeroing
enlightenment with a simple implementation: When the dynamic memory
VSC driver (dmvsc.sys) initiates the request to add physical memory to the
NT kernel, it specifies the
MM_ADD_PHYSICAL_MEMORY_ALREADY_ZEROED flag, which hints
the Memory Manager (MM) to add the new pages directly to the zeroed
pages list.
Memory access hints
For physically backed VMs, the root partition has very limited information
about how guest MM intends to use its physical pages. For these VMs, the
information is mostly irrelevant because almost all memory and GPA
mappings are created when the VM is started, and they remain statically
mapped. For VA-backed VMs, this information can instead be very useful
because the host memory manager manages the working set of the minimal
process that contains the VM’s memory (VMMEM).
The hot hint allows the guest to indicate that a set of physical pages should
be mapped into the guest because they will be accessed soon or frequently.
This implies that the pages are added to the working set of the minimal
process. The VID handles the hint by telling MicroVM to fault in the
physical pages immediately and not to remove them from the VMMEM
process’s working set.
In a similar way, the cold hint allows the guest to indicate that a set of
physical pages should be unmapped from the guest because it will not be
used soon. The VID driver handles the hint by forwarding it to MicroVM,
which immediately removes the pages from the working set. Typically, the
guest uses the cold hint for pages that have been zeroed by the background
zero page thread (see Chapter 5 of Part 1 for more details).
The VA-backed guest partition specifies a memory hint for a page by
using the HvMemoryHeatHint hypercall.
Enlightened page fault (EPF)
Enlightened page fault (EPF) handling is a feature that allows the VA-backed
guest partition to reschedule threads on a VP that caused a memory intercept
for a VA-backed GPA page. Normally, a memory intercept for such a page is
handled by synchronously resolving the access fault in the root partition and
resuming the VP upon access fault completion. When EPF is enabled and a
memory intercept occurs for a VA-backed GPA page, the VID driver in the
root partition creates a background worker thread that calls the MicroVM
page fault handler and delivers a synchronous exception (not to be confused
by an asynchronous interrupt) to the guest’s VP, with the goal to let it know
that the current thread caused a memory intercept.
The guest reschedules the thread; meanwhile, the host is handling the
access fault. Once the access fault has been completed, the VID driver will
add the original faulting GPA to a completion queue and deliver an
asynchronous interrupt to the guest. The interrupt causes the guest to check
the completion queue and unblock any threads that were waiting on EPF
completion.
Deferred commit and other optimizations
Deferred commit is an optimization that, if enabled, forces the VID driver not
to commit each backing page until first access. This potentially allows more
VMs to run simultaneously without increasing the size of the page file, but,
since the backing VA space is only reserved, and not committed, the VMs
may crash at runtime due to the commitment limit being reached in the root
partition. In this case, there is no more free memory available.
Other optimizations are available to set the size of the pages which will be
allocated by the MicroVM page fault handler (small versus large) and to pin
the backing pages upon first access. This prevents aging and trimming,
generally resulting in more consistent performance, but consumes more
memory and reduces the memory density.
The VMMEM process
The VMMEM process exists mainly for two main reasons:
■    Hosts the VP-dispatch thread loop when the root scheduler is enabled,
which represents the guest VP schedulable unit
■    Hosts the VA space for the VA-backed VMs
The VMMEM process is created by the VID driver while creating the
VM’s partition. As for regular partitions (see the previous section for details),
the VM Worker process initializes the VM setup through the VID.dll library,
which calls into the VID through an IOCTL. If the VID driver detects that
the new partition is VA-backed, it calls into the MicroVM (through the
VsmmNtSlatMemoryProcessCreate function) to create the minimal process.
MicroVM uses the PsCreateMinimalProcess function, which allocates the
process, creates its address space, and inserts the process into the process list.
It then reserves the bottom 4 GB of address space to ensure that no direct-
mapped images end up there (this can reduce the entropy and security for the
guest). The VID driver applies a specific security descriptor to the new
VMMEM process; only the SYSTEM and the VM Worker process can
access it. (The VM Worker process is launched with a specific token; the
token’s owner is set to a SID generated from the VM’s unique GUID.) This
is important because the virtual address space of the VMMEM process could
have been accessible to anyone otherwise. By reading the process virtual
memory, a malicious user could read the VM private guest physical memory.
Virtualization-based security (VBS)
As discussed in the previous section, Hyper-V provides the services needed
for managing and running virtual machines on Windows systems. The
hypervisor guarantees the necessary isolation between each partition. In this
way, a virtual machine can’t interfere with the execution of another one. In
this section, we describe another important component of the Windows
virtualization infrastructure: the Secure Kernel, which provides the basic
services for the virtualization-based security.
First, we list the services provided by the Secure Kernel and its
requirements, and then we describe its architecture and basic components.
Furthermore, we present some of its basic internal data structures. Then we
discuss the Secure Kernel and Virtual Secure Mode startup method,
describing its high dependency on the hypervisor. We conclude by analyzing
the components that are built on the top of Secure Kernel, like the Isolated
User Mode, Hypervisor Enforced Code Integrity, the secure software
enclaves, secure devices, and Windows kernel hot-patching and microcode
services.
Virtual trust levels (VTLs) and Virtual Secure
Mode (VSM)
As discussed in the previous section, the hypervisor uses the SLAT to
maintain each partition in its own memory space. The operating system that
runs in a partition accesses memory using the standard way (guest virtual
addresses are translated in guest physical addresses by using page tables).
Under the cover, the hardware translates all the partition GPAs to real SPAs
and then performs the actual memory access. This last translation layer is
maintained by the hypervisor, which uses a separate SLAT table per partition.
In a similar way, the hypervisor can use SLAT to create different security
domains in a single partition. Thanks to this feature, Microsoft designed the
Secure Kernel, which is the base of the Virtual Secure Mode.
Traditionally, the operating system has had a single physical address
space, and the software running at ring 0 (that is, kernel mode) could have
access to any physical memory address. Thus, if any software running in
supervisor mode (kernel, drivers, and so on) becomes compromised, the
entire system becomes compromised too. Virtual secure mode leverages the
hypervisor to provide new trust boundaries for systems software. With VSM,
security boundaries (described by the hypervisor using SLAT) can be put in
place that limit the resources supervisor mode code can access. Thus, with
VSM, even if supervisor mode code is compromised, the entire system is not
compromised.
VSM provides these boundaries through the concept of virtual trust levels
(VTLs). At its core, a VTL is a set of access protections on physical memory.
Each VTL can have a different set of access protections. In this way, VTLs
can be used to provide memory isolation. A VTL’s memory access
protections can be configured to limit what physical memory a VTL can
access. With VSM, a virtual processor is always running at a particular VTL
and can access only physical memory that is marked as accessible through
the hypervisor SLAT. For example, if a processor is running at VTL 0, it can
only access memory as controlled by the memory access protections
associated with VTL 0. This memory access enforcement happens at the
guest physical memory translation level and thus cannot be changed by
supervisor mode code in the partition.
VTLs are organized as a hierarchy. Higher levels are more privileged than
lower levels, and higher levels can adjust the memory access protections for
lower levels. Thus, software running at VTL 1 can adjust the memory access
protections of VTL 0 to limit what memory VTL 0 can access. This allows
software at VTL 1 to hide (isolate) memory from VTL 0. This is an
important concept that is the basis of the VSM. Currently the hypervisor
supports only two VTLs: VTL 0 represents the Normal OS execution
environment, which the user interacts with; VTL 1 represents the Secure
Mode, where the Secure Kernel and Isolated User Mode (IUM) runs.
Because VTL 0 is the environment in which the standard operating system
and applications run, it is often referred to as the normal mode.
 Note
The VSM architecture was initially designed to support a maximum of 16
VTLs. At the time of this writing, only 2 VTLs are supported by the
hypervisor. In the future, it could be possible that Microsoft will add one
or more new VTLs. For example, latest versions of Windows Server
running in Azure also support Confidential VMs, which run their Host
Compatibility Layer (HCL) in VTL 2.
Each VTL has the following characteristics associated with it:
■    Memory access protection As already discussed, each virtual trust
level has a set of guest physical memory access protections, which
defines how the software can access memory.
■    Virtual processor state A virtual processor in the hypervisor share
some registers with each VTL, whereas some other registers are
private per each VTL. The private virtual processor state for a VTL
cannot be accessed by software running at a lower VTL. This allows
for isolation of the processor state between VTLs.
■    Interrupt subsystem Each VTL has a unique interrupt subsystem
(managed by the hypervisor synthetic interrupt controller). A VTL’s
interrupt subsystem cannot be accessed by software running at a
lower VTL. This allows for interrupts to be managed securely at a
particular VTL without risk of a lower VTL generating unexpected
interrupts or masking interrupts.
Figure 9-30 shows a scheme of the memory protection provided by the
hypervisor to the Virtual Secure Mode. The hypervisor represents each VTL
of the virtual processor through a different VMCS data structure (see the
previous section for more details), which includes a specific SLAT table. In
this way, software that runs in a particular VTL can access just the physical
memory pages assigned to its level. The important concept is that the SLAT
protection is applied to the physical pages and not to the virtual pages, which
are protected by the standard page tables.
Figure 9-30 Scheme of the memory protection architecture provided by
the hypervisor to VSM.
Services provided by the VSM and requirements
Virtual Secure Mode, which is built on the top of the hypervisor, provides the
following services to the Windows ecosystem:
■    Isolation IUM provides a hardware-based isolated environment for
each software that runs in VTL 1. Secure devices managed by the
Secure Kernel are isolated from the rest of the system and run in VTL
1 user mode. Software that runs in VTL 1 usually stores secrets that
can’t be intercepted or revealed in VTL 0. This service is used heavily
by Credential Guard. Credential Guard is the feature that stores all the
system credentials in the memory address space of the LsaIso trustlet,
which runs in VTL 1 user mode.
■    Control over VTL 0 The Hypervisor Enforced Code Integrity
(HVCI) checks the integrity and the signing of each module that the
normal OS loads and runs. The integrity check is done entirely in
VTL 1 (which has access to all the VTL 0 physical memory). No
VTL 0 software can interfere with the signing check. Furthermore,
HVCI guarantees that all the normal mode memory pages that contain
executable code are marked as not writable (this feature is called
W^X. Both HVCI and W^X have been discussed in Chapter 7 of Part
1).
■    Secure intercepts VSM provides a mechanism to allow a higher VTL
to lock down critical system resources and prevent access to them by
lower VTLs. Secure intercepts are used extensively by HyperGuard,
which provides another protection layer for the VTL 0 kernel by
stopping malicious modifications of critical components of the