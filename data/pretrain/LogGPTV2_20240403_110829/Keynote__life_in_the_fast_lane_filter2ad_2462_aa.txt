title:Keynote: life in the fast lane
author:George Varghese
Life in the Fast Lane – viewed from the Conﬂuence Lens
George Varghese
Microsoft Research
Redmond, WA, USA
PI:EMAIL
ABSTRACT
The most striking ideas in systems are abstractions such as
virtual memory, sockets, or packet scheduling. Algorithmics
is the servant of abstraction, allowing the performance of the
system to approach that of the underlying hardware. I survey
the trajectory of network algorithmics, starting with a focus
on speed and scale in the 1990s to measurement and security
in the 2000s, using what I call the conﬂuence lens.
Conﬂuence sees interdisciplinary work as a merger of two
or more disciplines made compelling by an inﬂection point
in the real world, while also producing genuinely transformed
ideas. I attempt to show that Network Algorithmics repre-
sented a conﬂuence in the 1990s between computer systems,
algorithms, and networking. I suggest Conﬂuence Diagrams
as a means to identify future interdisciplinary opportunities,
and describe the emerging ﬁeld of Network Veriﬁcation as a
new conﬂuence between networking and programming lan-
guages.
1. NETWORK ALGORITHMICS
I use “algorithmics” in a slightly unnatural way to
refer to speeding up any good abstraction in computer
science that is in danger of being abandoned because of
performance. An abstraction is an idealization of real-
ity that allows the user of the abstraction to be more
productive by simplifying or idealizing the underling re-
ality.
Great abstractions make life easier for users of the
abstraction. For example, relational databases were an
advance, but it took years of eﬀort in query planning
before relational databases became commonplace.
When I began academic life at Washington Univer-
sity, St. Louis, in the 1990s, the web was exploding.
Internet traﬃc was doubling each year, and so were al-
located IP addresses. The only ﬂaw in the ointment
was that the beautiful networking abstractions, TCP
and IP, were much slower than the raw ﬁber, which was
already reaching 1 Gbps .
TCP, of course, provides the abstraction of two con-
nected queues: this eases the task of data transfer be-
tween applications on diﬀerent machines without con-
sidering the details of the underlying network. Simi-
larly, IP oﬀers the simple abstraction of datagram ser-
vice, sending an isolated message from a source end-
point to a destination endpoint regardless of the variety
of link technologies used.
But when the performance wars began in the 1990s
with the emergence of ﬁber, revolutionaries began propos-
ing transports like XTP to replace TCP, and MPLS to
replace IP in order to boost performance. This moti-
vated the birth of network algorithmics [31] as a set of
techniques to restore the speed of networking abstrac-
tions to that of ﬁber without compromising the elegance
and ease of use of the abstractions. The rest of this
article represents a revisionist history of network algo-
rithmics from the lens of what I call conﬂuences, which
I now deﬁne.
2. CONFLUENCE DIAGRAMS
The dictionary meaning of a conﬂuence is the meet-
ing place of two rivers, as in the Missouri and the Mis-
sissippi who meet near St. Louis, or the Tigris and
Euphrates who meet near the Garden of Eden. For the
purposes of this article, however, a conﬂuence broadly
speaking is a meeting place of two streams of thought.
We can add some teeth to this deﬁnition so it does
not reduce merely to interdisciplinary motherhood and
apple pie. First, as in the Conﬂuence Diagram shown in
Figure 1, distinguish the top stream as the main stream
of thought, and the bottom as the impacting stream.
Second, we seek three distinctives, also depicted in the
diagram.
The ﬁrst requirement is an inﬂection point, a change
in the real world, that makes the merger of the streams
compelling. Second, the new stream should have a
diﬀerent set of design constraints from its constituent
streams, what we might call a milieu change, so one
must rethink ideas in the merged stream. Thirdly, to
ensure that the new stream is a true mixing of streams,
there should be evidence of one idea that transforms
from the impacting stream to the new stream. Mere
reuse of existing ideas would be using the impacting
stream as a technology – a good thing certainly, but
not as exciting as a true conﬂuence of ideas. To make
beautiful ways, balancing the twin desires we have as
computer scientists for both beauty and impact. Fi-
nally, a discerned conﬂuence can sometimes suggest a
new ﬁeld in the making – a green ﬁeld area, especially a
boon when the original ﬁelds (think TCP papers) have
matured
3. NETWORK ALGORITHMICS HISTORY
I would like to take a whirlwind tour of various con-
cepts that helped make the Internet fast, but looked at
retrospectively though the conﬂuence lens.
Fast Servers: My ﬁrst encounter with algorithmics
was when I joined Digital Equipment Corporation and
found a beautiful conﬂuence between computer archi-
tecture and networking (Figure 3) that led to something
that today is called RDMA [3] but was part of the VAX-
Cluster system invented by Kronenberg, Strecker and
Levy [21]. The inﬂection point was the realization that
one could create cheap clusters of minicomputers; the
milieu change was going from a bus in a single computer
to many computers connected by a network.
In particular, the inventors of VAXClusters [21] rea-
soned that since Direct Memory Access was a stream-
lined way of sending large amounts of data directly from
the disk to memory without bothering the CPU, the
idea could be extended to Remote DMA from the mem-
ory of computer 1 to the memory of computer 2 without
the intervention of either CPU. Data was copied only
once over the bus, and the overhead of systems calls
and interrupts was minimal. Of course, today RDMA
is a major force in storage networks, but people forget
it was invented in 1986.
Figure 3: RDMA as a Conﬂuence
Soon after VAXClusters, a new inﬂection point arose
as the Internet began to heat up. Servers were found to
be woefully slow because they copied data across lay-
ers of software, and because of the overhead of system
calls. While RDMA did avoid these overheads, it re-
quired protocol changes, and only worked for large data
transfers. Thus began a stream of work in SIGCOMM
inﬂuenced by the Operating System community (the
impacting stream), to match the speed gains of RDMA
Figure 1: Conﬂuence Diagram: Inﬂection Point, Milieu
Change, and Transformed Ideas
these concepts concrete, consider ﬁrst an example from
painting.
Figure 2: Impressionism as a Conﬂuence
Figure 2 depicts Impressionism as a conﬂuence when
mainstream painting in the 1800s was impacted by the
emerging ﬁeld of psychology to form new streams of
painting such as impressionism, and later expression-
ism. The inﬂection point was the arrival of cheap pho-
tography which made painters question the value of
merely realistic rendering.
Why not, painters such as Monet may have reasoned,
paint the subjective response to a landscape, an im-
pression, something a camera cannot do. This was a
milieu change because impressions captured as concepts
in psychology now had to be incarnated in paint. There
were also transformed ideas: thin, precise brush strokes
that delineated borders gave way to blurry thick strokes
that mix in the eye at a certain distance.
Why look at existing and new work through the lens
of conﬂuence? I will develop this thesis in detail else-
where. For now, may I suggest that the conﬂuence lens
allows us to separate trends from fads by looking for
the inﬂection point; further, the milieu change, once
identiﬁed, provides a theme for research and a spring of
speciﬁc research ideas.
The inﬂection point makes it more likely that the re-
sulting research will have impact, and the milieu change
allows creative freedom to rethink ideas in sometimes
MAIN STREAM IMPACTING STREAM  NEW STREAM Inflection Point Milieu Change Transformed Ideas Ideas to Canvas  Photography Realistic Painting Psychology  Impressionism Thin to thick strokes  Networking Architecture  Algorithmics Cheap Clusters Machine bus to Network bus DMA RDMA  using only local Operating System changes without pro-
tocol changes, while retaining structure and protection.
I pick three representative papers. Fbufs from Dr-
uschel and Peterson [10] showed it was possible to avoid
copies by leveraging page tables. Application Device
Channels from Druschel, Peterson and Davy [11] showed
how to avoid system calls. Both ideas are alive and well
in what people call Zero Copy Interfaces [6] and Virtual
Interface Architecture [5]. Finally, Active Messages [33]
was roughly concurrent with Application Level Device
Channels, and again avoided control overhead by pass-
ing information in packets. Beyond ways to stream-
line data movement, Van Jacobson and Mike Karels
showed that TCP performance could be optimized in
the expected case using “header prediction” [4] when
segments arrive in order. No new transport protocols
were needed. The stage had been set for fast servers.
Fast Routers: The ﬁrst glimmer of a real conﬂuence
between algorithms and networks that I encountered
arrived because of a new inﬂection point (Figure 4)
around 1996. IPv6 was rumored to be imminent and
addresses were now W = 128 bits. Simple trie-based
schemes were linear in the number of address bits which
was too slow. Of course, theoretical computer science
had some fast preﬁx algorithms but they were mostly
O(log N ) schemes where N is the number of preﬁxes,
and the milieu was diﬀerent (Figure 4) because memory
accesses and not computation is the dominant metric.
While most theoretical algorithms were content with
computing a preﬁx lookup in milliseconds, an arriving
packet had less than a microsecond to be forwarded. It
was in puzzling over IPv6 that we discovered a preﬁx
lookup scheme that took O(log W ) memory accesses,
which for IP v6 was 7 memory accesses. This to me
seemed to be a transformed concept. While O(log log N )
algorithms were known for lookups [29], they were for
exact lookups and more complicated.
met Jon Turner on the stairs of Bryan Hall in Washing-
ton University and he asked why one couldnt do binary
search on preﬁx lengths. Preﬁxes would ﬁrst be segre-
gated by their length and then at each length a search
for a preﬁx required only an exact match by hashing.
The usual method starts with the longest length and
works backward. Jon, however, wanted to start with
the middle preﬁx length and perform binary search. I
thought about his idea for a few minutes and showed
him a simple counterexample with two preﬁxes, one at
length 1 and one at length 3. I asked him how one could
search in the middle length (2) hash table when there
was no preﬁx of length 2. Some days later, he met me
on the same stairs and told me “Easy: for every longer
length preﬁx, add an artiﬁcial preﬁx (called a marker)
at all length tables that binary search can reach”. Thus
in Figure 5, marker 10 is placed in the Length 2 hash
table.
This was wonderful as far it went, but there was a
ﬂaw. Sometimes markers take you on a wild goose chase
to the second half. For example, in Figure 5, when
searching for the string 100, marker 10 takes search to
the second half towards the entry for 101∗ when the
answer (1∗) instead lurks in the ﬁrst half. Rather than
tell Turner about the bug, I decided to ﬁx it myself by
precomputing the best matching preﬁx of every marker.
For example, the best matching preﬁxof 10 is precom-
puted to be 1∗ . If the search process remembers the
matching preﬁx of the last marker encountered, this be-
comes the answer when search fails without the need for
backtracking.
Figure 4: Binary Search on Preﬁx Lengths as a Conﬂu-
ence.
The story of Binary Search on preﬁx lengths [34] is a
romantic tale of an encounter with two outsiders, and
how ideas are “in the air” at a certain time period. I
Figure 5: Binary Search on Preﬁx Lengths. The two
preﬁxes are 1∗ and 101∗. 10 is an artiﬁcial entry called
a marker used to guide binary search.
Amazingly, on a bus a few days later, I sat next to a
really smart Swiss student, Marcel Waldvogel, who was
visiting Washington University. He had all the same
ideas as Jon and ended with the same bug. So we began
working together. Of course, Marcel did all the work,
 Networking Algorithms  Algorithmics  Traffic, IP v6  Msec to usec Binary Search on keys   Binary search on  Lengths  1* 101* Length 1 Length 3 Length 2 10 Three ideas: 1. Start in middle,  2. Add markers  3 . Pre-compute best matching prefix of each marker  and added a number of elegant ideas of his own such as
Rope Search [34].
Next, every packet arriving on an input link of a
router is subject to IP lookup to determine its output
port and then must be transferred via the guts of the
router, often called a switch. Early switches in the 90s,
such as the Catalyst 5K from Cisco designed by Tom
Edsall, used a simple bus akin to the older PCI bus in
a CPU. But as speeds went up, designers realized they
had to use a crossbar, which is a set of parallel busses.
The simplest technique to schedule a crossbar uses in-
put queuing, where packets waiting for an output are
placed in a single queue at the input link.
However, that meant that a packet on an input in-
terface destined to a red output interface could wait (at
the input) behind a packet destined to a busy blue out-
put interface, even though the red output interface was
free. This is the so-called Head-of-Line blocking prob-
lem which can reduce throughput by nearly half. This
problem resulted in researchers proposing more complex
output queuing designs. But, as far as I know, output
queuing designs never entered the mainstream router
market because of their complexity.
A breakthrough occurred, or so it seems to me, around
1992 when Tom Anderson, Chuck Thacker, and others
from DEC SRC [8] introduced two new ideas. First,
they changed the FIFO interface to one interface at
each input for each output, sometimes referred to as
VOQs (virtual output queues), as shown in Figure 6.
Then information about all non-empty VOQs is sent to
a scheduler.
Their second remarkable idea was a maximal match-
ing algorithm called PIM [8] that could be done in
hardware in around 5 iterations. One can think of
their approach as an Ethernet-like approach per out-
put port. Each output port randomly selects among all
input ports that wish to send to it; input ports rejected
because of “collisions” retry in the next iteration.
Figure 6: Virtual Output queues and Maximal Match-
ing eliminate Head of Line Blocking
A few years later Nick McKeown introduced iSLIP [25]
which can be roughly thought of as a token ring equiv-
alent of the Ethernet-like approach of PIM. The sched-
uler grants access to each output link based on a rotat-
ing priority that cycles through the input ports. While
iSLIP can start badly, it generally converges to very
good matches after a few iterations. iSLIP was used in
the Cisco GSR.
What is compelling here is not just the impact in
terms of switch performance but the transformed idea.