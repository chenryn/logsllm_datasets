Interestingly, Google indexed the papers, but accord-
ing to the rendered gibberish, not the underlying text.
This indicates, of these four engines, only it performs
OCR on PDF ﬁles it indexes rather than extracting the
text through PDFMiner or the like. After two days, the
papers were removed from Google’s index, before the
authors obtained screenshots. We conclude that Google
has a robust defense against the content masking attack,
while the other three engines remain susceptible.
7 Defense Against Content Masking
As intoned through this paper, Optical Character Recog-
nition (OCR) is able to move the text extraction process
from targeting the underlying text to the rendered ver-
sion, preventing this masking attack. OCR is required
for print documents scanned to PDF, but for documents
with rendered text, system designers have been loath to
use OCR in lieu of PDFMiner or its ilk. OCR is far more
complex and requires more processing time than simply
running the PDF ﬁle through a lightweight parser to col-
lect its strings. We propose here a lightweight font veri-
ﬁcation method that enables the use of OCR in a highly
efﬁcient way to prevent the content masking attack. The
intuition is simple; we render each character in the fonts
embedded in the subject PDF ﬁle and then perform OCR
on those characters rather than the rendered PDF ﬁle it-
self. Where an academic paper may be some 50,-75,000
characters, the fonts embedded therein usually contain at
most just a couple hundred characters.
Challenges and Technical Details: While the intu-
ition is simple, some challenges arise in its realization.
First, while most PDF generation tools will embed only
those letters used in the document, it is possible through
Adobe InDesign, as one example, to embed the whole
font. Some fonts accommodate many characters used in
many other languages, and the upper limit on font char-
acter capacity is 216 = 65,536 because characters have
a two-byte index. Clearly, performing OCR on a font of
that size will be equivalent to performing OCR on an aca-
demic paper in terms of computational overhead. Conse-
quently, we scan the document to extract the characters
used, and only render those characters (in their respective
fonts) for OCR veriﬁcation. This requires iterating over
the entire document, but the overhead introduced here is
much less than with full-document OCR, as the process
just builds a list from the series of character codes rather
than executing image processing techniques on all char-
acter glyphs. OCR is then performed on the series of
character codes used in each font only.
Second,
the existence of many special characters
within a font prompts the question of what characters
OCR can distinguish and how to handle those it can’t.
Theoretically, OCR may mature to the point where it can
distinguish any sort of accent mark over normal letters,
any characters used in languages other than English, and
any additional special characters used in typeset mathe-
matics, etc., and some OCR software may be currently
in development working on a subset of these problems.
However, we aim to provide a defense method readily in-
tegrable into current systems. Additionally, such an ad-
vanced software will likely incur overhead beyond that
of a current OCR package to achieve the requisite preci-
sion, where our solution must be sufﬁciently lightweight
to ﬁt within systems where full-document OCR has not
been applied due to computational complexity. We de-
ﬁne a normal set of character codes as those represent-
ing upper and lowercase English letters, numbers, and
common punctuation, which English OCR packages tar-
get, and then we check if the extracted character codes
appear in this normal set or not. A letter in the normal
set appearing as something other than itself is evidence
of the content masking attack, as is a letter outside the
normal set having the glyph of one inside. OCR is per-
formed on all used characters in the font, as previously
mentioned, and those within the normal set are required
to have the correct respective glyph, while those outside
the normal set are constrained not to have a distinguish-
able glyph (i.e. one appearing in the normal set).
The third issue arises with the fact that many special
characters have high similarity with normal characters,
especially for those fonts in common use which have
many thousands of available characters. If one such spe-
cial character is used legitimately in the text, the scheme
just described will ﬂag it as a content masking attack
due to its similar appearance with a normal set character.
Worse, common OCR tools available presently will con-
ﬂate characters which humans can easily tell apart but
for which the software is not precise enough to do so.
For example, it is easy to tell visually that π and n are
different characters, but not by common OCR tools.
Font Training Step: We therefore introduce a training
step, wherein OCR is performed on the font and lists of
intersections compiled. When we perform OCR on each
represented character and the detected glyph for a spe-
cial character but appears like a normal letter, we check
the list of characters similar to that normal letter. If the
special character appears on that list, we recognize that it
may be valid and that we cannot know if it is being used
legitimately or as part of a content masking attack. As
the purpose of the content masking attack is to disguise
the visually rendered text as some other text for the com-
puter to see, we simply replace the extracted character
code for this letter as the normal letter it looks like, and
pass this on to the end application. If content masking
is occurring, the rendered text is sent to the plagiarism
842    26th USENIX Security Symposium
USENIX Association
detector, reviewer assignment system, etc., thwarting the
attack. Otherwise, the string in which this special charac-
ter appears is with high probability not an English word
and would not be useful to the end application anyway. A
reviewer assignment system or plagiarism detector will
not make use of mathematical equations when assigning
reviewers, as these are not discernible words, so if πr2 is
extracted as nrz, no loss of function is suffered.
This training solution prompts one further issue,
which is that different fonts will need to be trained in-
dependently as their nuances cause different sets of char-
acters to appear similar. For the reviewer assignment and
plagiarism detection problems, we know a limited num-
ber of fonts should be used, due to academic formatting
requirements favoring a small set of fonts. Nevertheless,
for other applications, such as search indexing, the only
limit on the number of fonts that can be trained is that
those fonts must be legible enough for an OCR tool to
parse. These lists do not occupy too much space; for ex-
ample our lists for Times New Roman and Arial fonts are
29.4KB and 36.2KB, respectively. This database com-
piled, the OCR tool will be used to discern the real name
of each font used in the document, to counteract the prob-
lem mentioned early in this paper, that an attacker may
name a font anything desired. Open source OCR tools
such as Tesseract OCR [28] provide this functionality.
Font Veriﬁcation Overview: The training process be-
gins by gathering a collection of fonts and training the
system on each. For each character in a font’s normal set,
all special characters are tested for OCR similarity, and
any identiﬁed as similar are added to the list for that nor-
mal character. Testing a new PDF ﬁle is outlined in Al-
gorithm 2, wherein the list of characters and their fonts is
reduced to unique combinations of those attributes, and
each then tested with OCR. Content masking attacks are
detected in lines 12 and 17 when the underlying char-
acter index is a normal character other than the OCR-
extracted character or when the underlying character in-
dex is a special character that does not appear in the simi-
larity list for the OCR-extracted character. In these cases,
this pseudocode exits to notify of the attack, though other
behavior could be inserted here. This protects all end
applications, except in the attack against plagiarism de-
tection in which the attacker replaces normal characters
with special characters similar in appearance. That spe-
ciﬁc attack is identiﬁed as possible at line 15, in the case
that the underlying character is a special character which
does appear in the similarity list for the OCR-extracted
character; in this case all instances of this character in the
text extracted from this ﬁle are replaced with the OCR-
extracted character for use in the end application.
Algorithm 2 Extract Rendered Text
Input: font list F = { f1, f2, ..., fp}, normal character
index set N = {n1,n2, ...,nq}, special character in-
dex set S = {s1,s2, ...,sr}, document character list
D = {d1,d2, ...,ds}
U ← U ∪ (di, FONT(di))
Output: extracted text T = {t1,t2, ...,ts}
1: Unique character index/font map list U = /0
2: for i ← 1 to s do
if di /∈ U then
3:
4:
5: m ← |U|
6: OCR-extracted
{o1,o2, ...,om}
7: for i ← 1 to m do
oi ← OCR(ui)
8:
f ← ui. f ont
9:
L ← list of similar character lists {l1,l2, ...,lv}
10:
if ui.index ∈ N then
set O =
character
index
for f
break
if oi (cid:54)= ui.index then
else if ui.index ∈ S then
if ui.index ∈ loi then
else
11:
12:
13:
14:
15:
16:
17:
18:
19: T ← Apply modiﬁed U to D
20: return T
ui ← oi
break
(cid:46) Attack Detected
(cid:46) Attack Possible
(cid:46) Attack Detected
Font Veriﬁcation Performance: The implementation
for this defense method is written in Python and employs
PDF-Extract [29] to extract font ﬁles from PDFs, tex-
tract [30] to extract the text strings, and pytesseract [31],
a Python wrapper for Tesseract OCR [28]. The alterna-
tive to our font veriﬁcation method is to perform OCR
on the entire document, so we use Tesseract OCR for
this purpose also for a fair comparison. This comparison
will illustrate not only that our method detects/mitigates
the content masking attack as well as the naive full docu-
ment OCR method, but that it performs far better in sev-
eral scenarios common to PDFs both in and out of the
presence of our content masking attack.
First, we compare the performance of the two meth-
ods with differing amounts of masked content. We gen-
erate 10 PDF ﬁles with masked characters varying from
5-20% in frequency of appearance, and apply both meth-
ods to each of these ﬁle. The results are shown in Fig-
ure 9 and show a distinct beneﬁt to our font veriﬁca-
tion method compared with the traditional full document
OCR. Here, detection rate refers to the correct extraction
of rendered text and the consequent ability to prevent the
content masking attack from occurring. For full docu-
ment OCR, we generate 10 PDF documents with no con-
USENIX Association
26th USENIX Security Symposium    843
Figure 9: Attack detection under
varying degrees of attack.
Figure 10: Attack detection on
PDFs of different sizes.
Figure 11: Attack detection time
relationship with PDF size.
tent masking and measure the error in character recog-
nition, and then we use this error as a threshold, such
that the attack is ﬂagged for one of the content masked
PDF ﬁles if it is determined to have a larger difference
between characters and their glyphs. That threshold was
measured at 7%, and more than 20% of characters had
to be masked before the full document OCR method de-
tected the content masking attack (after this, detection
was 100%). The attack is considered detected by the
font veriﬁcation method if Algorithm 2 ﬂags it or the
edge case approach we take of replacing special char-
acters that look like normal letters with those normal let-
ters will enable the end application (plagiarism/spam de-
tector) to process the text properly and thereby ﬂag the
attack. In all cases, our algorithm detected the attack or
constructed the proper English words required by the end
application to detect it.
The disparity here between the methods’ accuracy in
the 5-20% character masking range has a few aspects in-
volved. Fewer masked characters will appear in a sparser
distribution, which make them less visible among legit-
imate characters. OCR is affected by the distance be-
tween characters and the resolution of the image, among
other things, which we can control in the case of font
veriﬁcation but which are not controlled when perform-
ing OCR over an entire document. We can generate an
optimal image of all relevant characters, check their va-
lidity, ﬂag detected attacks, and in the case of special
characters which appear identical to normal letters, re-
place them with those normal letters for proper use in the
end application.
We also analyze the effects of document length on the
detection rate for each method, by comparing their re-
sults on 10 PDF ﬁles ranging from 1-10 pages in length
and having an even 30% distribution of masked charac-
ters. Figure 10 illustrates that while the font veriﬁcation
method is almost perfectly static, full document OCR
gradually performs more poorly, reaching 14% misde-
tection by page 10. The aforementioned OCR error rate
explains this problem, where while 30% masked charac-
ters is above the required 20% to guarantee detection in
the previous experiment, additional pages of text steadily
allow more masked text to go unnoticed. The font veriﬁ-
cation appears to be 100% throughout, but actually dips
to 99.8% halfway through. Our method is not immune
to the errors inherent to OCR as it also uses OCR, but its
more judicious approach minimizes those errors. In this
case, OCR is confusing the ’;’ and ’:’ characters; these
are rare but eventual in prose.
Finally, we demonstrate the performance gain of our
font veriﬁcation method over the full document OCR
method, on 20 PDF ﬁles ranging from 1-20 pages in
length and having a 30% distribution of masked char-
acters. In Figure 11, the full document OCR method in-
creases linearly with pages added while the font veriﬁ-
cation method unsurprisingly remains largely static, in-