tinguishability guarantee for AP and a DP guarantee for CV, while
HybrIDX hides the exact result size and only obfuscates the AP.
Lastly, Hermetic [74] takes on the SGX side-channel attacks, in-
cluding AP. It provides oblivious primitives, however, it only offers
protection against software and not physical attacks (e.g., it trusts
a hypervisor to disable interrupts).
Solutions against the snapshot adversary. Works in this category
protect against the snapshot adversary, which takes a snapshot
of the data at a fixed point in time (e.g., stolen hard drive). We
stress that Epsolute provides semantic security against the snap-
shot adversary on top of AP and CV protection. CryptDB [56] is
a seminal work in this direction offering computations over en-
crypted data. It has since been shown (e.g. [9, 41, 51]) that the
underlying property-preserving schemes allow for reconstruction
attacks. Arx [55] provides strictly stronger security guarantees by
using only semantically secure primitives. Seabed [54] uses an addi-
tively symmetric homomorphic encryption scheme for aggregates
and certain filter queries. Samanthula et al. [62] offer a method
to verify and apply a predicate (a junction of conditions) using
garbled circuits or homomorphic encryption without revealing the
predicate itself. SisoSPIR [39] presents a mechanism to build an
oblivious index tree such that neither party learns the pass taken.
See [15] for a survey of range query protocols in this category.
2 BACKGROUND
In this section we describe an outsourced database system adapted
from [43], a base for our own model (Section 3), and the construc-
tions we will use as building blocks in our solution.
2.1 Outsourced Database System
We abstract a database as a collection of 𝑛 records 𝑟, each with a
unique identifier 𝑟 ID, associated with search keys SK: D = {(𝑟1,
𝑟 ID1 , SK1), . . . , (𝑟𝑛, 𝑟 ID
𝑛 , SK𝑛)}. We assume that all records have an
identical fixed bit-length, and that search keys are elements of the
domain X = {1, . . . , 𝑁} for some 𝑁 ∈ N. Outsourced database
systems support search keys on multiple attributes, with a set of
search keys for each of the attributes of a record. For the ease of
presentation, we describe the model for a single indexed attribute
and then show how to extend it to support multiple attributes.
A query is a predicate 𝑞 : X → {0, 1}. Evaluating a query 𝑞
on a database D results in 𝑞(D) = {𝑟𝑖 : 𝑞(SK𝑖) = 1}, all records
whose search keys satisfy 𝑞.
Let Q be a set of queries. An outsourced database system for
queries in Q consists of two protocols between two stateful parties:
a user U and a server S (adapted from [43]):
Setup protocol Πsetup: U receives as input a database D = {(𝑟1,
𝑟 ID1 , SK1), . . . , (𝑟𝑛, 𝑟 ID
𝑛 , SK𝑛)}; S has no input. The output for
S is a data structure DS; U has no output besides its state.
Query protocol Πquery: U has a query 𝑞 ∈ Q produced in the
setup protocol as input; S has as input DS produced in the
setup protocol. U outputs 𝑞(D); S has no formal output.
(Both parties may update their internal states.)
For correctness, we require that for any database D = {(𝑟1, 𝑟 ID1 ,
𝑛 , SK𝑛)} and query 𝑞 ∈ Q, it holds that running
SK1), . . . , (𝑟𝑛, 𝑟 ID
Πsetup and then Πquery on the corresponding inputs yields for U
the correct output {𝑟𝑖 : 𝑞(SK𝑖) = 1} with overwhelming probability
over the coins of the above runs. We call the protocol 𝜂-wrong if
this probability is at least 1 − 𝜂.
2.2 Differential Privacy and Sanitization
Differential privacy is a definition of privacy in analysis that pro-
tects information that is specific to individual records. More for-
mally, we call databases D1 ∈ X𝑛 and D2 ∈ X𝑛 over domain X
neighboring (denoted D1 ∼ D2) if they differ in exactly one record.
Definition 2.1 ([23, 24]). A randomized algorithm A is (𝜖, 𝛿)-
differentially private if for all D1 ∼ D2 ∈ X𝑛, and for all subsets O
of the output space of A,
Pr [A (D1) ∈ O] ≤ exp(𝜖) · Pr [A (D2) ∈ O] + 𝛿 .
The probability is taken over the random coins of A.
When 𝛿 = 0 we omit it and say that A preserves pure differ-
ential privacy, otherwise (when 𝛿 > 0) we say that A preserves
approximate differential privacy.
We will use mechanisms for answering count queries with dif-
ferential privacy. Such mechanisms perturb their output to mask
out the effect of any single record on their outcome. The simplest
method for answering count queries with differential privacy is the
Laplace Perturbation Algorithm (LPA) [24] where random noise
drawn from a Laplace distribution is added to the count to be pub-
lished. The noise is scaled so as to hide the effect any single record
can have on the count. More generally, the LPA can be used to ap-
proximate any statistical result by scaling the noise to the sensitivity
of the statistical analysis.1
1The sensitivity of a query 𝑞 mapping databases into R𝑁 is defined to be Δ(𝑞) =
maxD1∼D2∈X𝑛 ∥𝑞(D1) − 𝑞(D2) ∥1.
Session 7C: Database and Privacy CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2264Theorem 2.2 (adapted Theorem 1 from [24]). Let 𝑞 : D → R𝑁 .
An algorithm A that adds independently generated noise from a zero-
mean Laplace distribution with scale 𝜆 = Δ(𝑞)/𝜖 to each of the 𝑁
coordinates of 𝑞(D), satisfies 𝜖-differential privacy.
While Theorem 2.2 is an effective and simple way of answering
a single count query, we will need to answer a sequence of count
queries, ideally, without imposing a bound on the length of this
sequence. We will hence make use of sanitization algorithms.
Definition 2.3. Let Q be a collection of queries. An (𝜖, 𝛿, 𝛼, 𝛽)-
differentially private sanitizer for Q is a pair of algorithms (A, B)
such that:
• 𝐴 is (𝜖, 𝛿)-differentially private, and
• on input a dataset D = 𝑑1, . . . , 𝑑𝑛 ∈ X𝑛, A outputs a data
structure DS such that with probability 1 − 𝛽 for all 𝑞 ∈ Q,
|B (DS, 𝑞) −𝑖 𝑞(𝑑𝑖)| ≤ 𝛼.
Remark 2.4. Given an (𝜖, 𝛿, 𝛼, 𝛽)-differentially private sanitizer as
in Definition 2.3 one can replace the answer B (DS, 𝑞) with B′(DS,
𝑞) = B (DS, 𝑞) + 𝛼. Hence, with probability 1 − 𝛽, for all 𝑞 ∈ Q,
0 ≤ B′ (DS, 𝑞) −𝑖 𝑞(𝑑𝑖) ≤ 2𝛼. We will hence assume from now on
that sanitizers have this latter guarantee on their error.
and 𝛼 = O(cid:0)(log∗ 𝑁)1.5(cid:1) for approximate differential privacy (with
The main idea of sanitization (a.k.a. private data release) is to
release specific noisy statistics on a private dataset once, which can
then be combined in order to answer an arbitrary number of queries
without violating privacy. Depending on the query type and the
notion of differential privacy (i.e., pure or approximate), different up-
per bounds on the error have been proven. Omitting the dependency
on 𝜖, 𝛿, in case of point queries over domain size 𝑁 , pure differential
privacy results in 𝛼 = Θ(log 𝑁) [6], while for approximate differ-
ential privacy 𝛼 = O(1) [7]. For range queries over domain size 𝑁 ,
these bounds are 𝛼 = Θ(log 𝑁) for pure differential privacy [10, 25],
an almost matching lower bound of 𝛼 = Ω(log∗ 𝑁)) [7, 16, 42].
More generally, Blum et al. [10] showed that any finite query set Q
can be sanitized, albeit non-efficiently.
Answering point and range queries with differential privacy. Utilizing
the LPA for answering point queries results in error 𝛼 = O(log 𝑁).
A practical solution for answering range queries with error bounds
very close to the optimal ones is the hierarchical method [25, 36, 72].
The main idea is to build an aggregate tree on the domain, and add
noise to each node proportional to the tree height (i.e., noise scale
logarithmic in the domain size 𝑁 ). Then, every range query is
answered using the minimum number of tree nodes. Qardaji et al.
[58] showed that the hierarchical algorithm of Hay et al. [36], when
combined with their proposed optimizations, offers the lowest error.
Composition. Finally, we include a composition theorem (adapted
from [47]) based on [23, 24]. It concerns executions of multiple dif-
ferentially private mechanisms on non-disjoint and disjoint inputs.
Theorem 2.5. Let A1, . . . , A𝑟 be mechanisms, such that each A𝑖
provides 𝜖𝑖-differential privacy. Let D1, . . . , D𝑟 be pairwise non-
disjoint (resp., disjoint) datasets. Let A be another mechanism that
executes A1(D1), . . . , A𝑟 (D𝑟) using independent randomness for
each A𝑖, and returns their outputs. Then, mechanism A is(cid:0)𝑟
𝑖=1 𝜖𝑖(cid:1)-
differentially private (resp.,(cid:16)max𝑟
(cid:17)-differentially private).
𝑖=1 𝜖𝑖
2.3 Oblivious RAM
Informally, Oblivious RAM (ORAM) is a mechanism that lets a user
hide their RAM access pattern to remote storage. An adversarial
server can monitor the actual accessed locations, but she cannot
tell a read from a write, the content of the block or even whether
the same logical location is being referenced. The notion was first
defined by Goldreich [31] and Goldreich and Ostrovsky [32].
More formally, a (𝜂1, 𝜂2)-ORAM protocol is a two-party protocol
between a user U and a server S who stores a RAM array. In each
round, the user U has input (𝑜, 𝑎, 𝑑), where 𝑜 is a RAM operation (r
or w), 𝑎 is a memory address and 𝑑 is a new data value, or ⊥ for read
operation. The input of S is the current array. Via the protocol, the
server updates the memory or returns to U the data stored at the
requested memory location, respectively. We speak of a sequence
of such operations as a program y being executed under the ORAM.
An ORAM protocol must satisfy correctness and security. Cor-
rectness requires that U obtains the correct output of the computa-
tion except with at most probability 𝜂1. For security, we require that
for every user U there exists a simulator SimORAM which provides
a simulation of the server’s view in the above experiment given
only the number of operations. That is, the output distribution of
SimORAM(𝑐) is indistinguishable from ViewS with probability at
most 𝜂2 after 𝑐 protocol rounds.
ORAM protocols are generally stateful, after each execution the
client and server states are updated. For brevity, throughout the
paper we will assume the ORAM state updates are implicit, including
the encryption key 𝐾 generated and maintained by the client.
Some existing efficient ORAM protocols are Square Root ORAM
[31], Hierarchical ORAM [32], Binary-Tree ORAM [63], Interleave
Buffer Shuffle Square Root ORAM [73], TP-ORAM [64], Path-ORAM
[65] and TaORAM [61]. For detailed descriptions of each protocol,
we recommend the work of Chang et al. [19]. The latter three
ORAMs achieve the lowest communication and storage overheads,
O(log 𝑛) and O(𝑛), respectively.
3 DIFFERENTIALLY PRIVATE OUTSOURCED
DATABASE SYSTEMS
In this section we present our model, differentially private outsourced
database system, CDP-ODB, its security definition, query types and
efficiency measures. It is an extension of the ODB model in Section 2.
3.1 Adversarial model
We consider an honest-but-curious polynomial time adversary that
attempts to breach differential privacy with respect to the input
database D. We observe later in Section 3.1.1 that it is impossible
to completely hide the number of records returned on each query
without essentially returning all the database records on each query.
This, in turn, means that different query sequences may be distin-
guished, and, furthermore, that differential privacy may not be
preserved if the query sequence depends on the content of the data-
base records. We hence, only require the protection of differential
privacy with respect to every fixed query sequence. Furthermore,
we relax to computational differential privacy (following [49]).
In the following definition, the notation ViewΠ (D, 𝑞1, . . . , 𝑞𝑚)
denotes the view of the server S in the execution of protocol Π in
answering queries 𝑞1, . . . , 𝑞𝑚 with the underlying database D.
Session 7C: Database and Privacy CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2265Definition 3.1. We say that an outsourced database system Π is
(𝜖, 𝛿)-computationally differentially private (a.k.a. CDP-ODB) if for
every polynomial time distinguishing adversary A, for every neigh-
boring databases D ∼ D′, and for every query sequence 𝑞1, . . . , 𝑞𝑚 ∈
Q𝑚 where 𝑚 = poly(𝜆),
Pr(cid:104)A(cid:16)1𝜆, ViewΠ (D, 𝑞1, . . . , 𝑞𝑚)(cid:17)
= 1(cid:105) ≤
exp 𝜖 · Pr(cid:104)A(cid:16)1𝜆, ViewΠ(cid:0)D′, 𝑞1, . . . , 𝑞𝑚(cid:1)(cid:17)
= 1(cid:105) + 𝛿 + negl(𝜆) ,
where the probability is over the randomness of the distinguishing
adversary A and the protocol Π.
Remark 3.2 (Informal). We note that security and differential
privacy in this model imply protection against communication volume
and access pattern leakages and thus prevent a range of attacks, such
as [17, 43, 51].
3.1.1 On impossibility of adaptive queries. Non-adaptivity in our
CDP-ODB definition does not reflect a deficiency of our specific
protocol but rather an inherent source of leakage when the queries
may depend on the decrypted data. Consider an adaptive CDP-
ODB definition that does not fix the query sequence 𝑞1, . . . , 𝑞𝑚 in
advance but instead an arbitrary (efficient) user U chooses them
during the protocol execution with S. As before, we ask that the S’s
view is DP on neighboring databases for every such U. We observe
that this definition cannot possibly be satisfied by any outsourced
database system without unacceptable efficiency overhead. Note
that non-adaptivity here does not imply that the client knows all
the queries in advance, but rather can choose them at any time
(e.g., depending on external circumstances) as long as they do not
depend on true answers to prior queries.
To see this, consider two neighboring databases D, D′. Database
D has 1 record with key = 0 and D′ has none. Furthermore, both
have 50 records with key = 50 and 100 records with key = 100.
User U queries first for the records with key = 0, and then if there
is a record with key = 0 it queries for the records with key = 50,
otherwise for the records with key = 100. Clearly, an efficient out-
sourced database system cannot return nearly as many records
when key = 50 versus key = 100 here. Hence, this allows distin-
guishing D, D′ with probability almost 1.
To give a concrete scenario, suppose neighboring medical data-
bases differ in one record with a rare diagnosis “Alzheimer’s disease”.
A medical professional queries the database for that diagnosis first
(point query), and if there is a record, she queries the senior patients
next (range query, age ≥ 65), otherwise she queries the general
population (resulting in more records). We leave it open to mean-
ingfully strengthen our definition while avoiding such impossibility
results, and we defer the formal proof to future work.
3.2 Query types
In this work we are concerned with the following query types:
Range queries Here we assume a total ordering on X. A query