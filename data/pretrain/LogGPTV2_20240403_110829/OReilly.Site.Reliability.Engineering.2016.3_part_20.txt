by reproducing it at will—can be difficult to do in production systems; often, we can
only find probable causal factors, for the following reasons:
• Systems are complex. It’s quite likely that there are multiple factors, each of which
individually is not the cause, but which taken jointly are causes.15 Real systems
are also often path-dependent, so that they must be in a specific state before a
failure occurs.
• Reproducing the problem in a live production system may not be an option, either
because of the complexity of getting the system into a state where the failure can
be triggered, or because further downtime may be unacceptable. Having a non‐
15 See [Mea08] on how to think about systems, and also [Coo00] and [Dek14] on the limitations of finding a
single root cause instead of examining the system and its environment for causative factors.
Negative Results Are Magic | 145
production environment can mitigate these challenges, though at the cost of hav‐
ing another copy of the system to run.
Once you’ve found the factors that caused the problem, it’s time to write up notes on
what went wrong with the system, how you tracked down the problem, how you fixed
the problem, and how to prevent it from happening again. In other words, you need
to write a postmortem (although ideally, the system is alive at this point!).
Case Study
App Engine,16 part of Google’s Cloud Platform, is a platform-as-a-service product that
allows developers to build services atop Google’s infrastructure. One of our internal
customers filed a problem report indicating that they’d recently seen a dramatic
increase in latency, CPU usage, and number of running processes needed to serve
traffic for their app, a content-management system used to build documentation for
developers.17 The customer couldn’t find any recent changes to their code that corre‐
lated with the increase in resources, and there hadn’t been an increase in traffic to
their app (see Figure 12-3), so they were wondering if a change in the App Engine
service was responsible.
Our investigation discovered that latency had indeed increased by nearly an order of
magnitude (as shown in Figure 12-4). Simultaneously, the amount of CPU time
(Figure 12-5) and number of serving processes (Figure 12-6) had nearly quadrupled.
Clearly something was wrong. It was time to start troubleshooting.
Figure 12-3. Application’s requests received per second, showing a brief spike and return
to normal
16 See https://cloud.google.com/appengine.
17 We have compressed and simplified this case study to aid understanding.
146 | Chapter 12: Effective Troubleshooting
Figure 12-4. Application’s latency, showing 50th, 95th, and 99th percentiles (lines) with a
heatmap showing how many requests fell into a given latency bucket at any point in time
(shade)
Figure 12-5. Aggregate CPU usage for the application
Figure 12-6. Number of instances for the application
Typically a sudden increase in latency and resource usage indicates either an increase
in traffic sent to the system or a change in system configuration. However, we could
easily rule out both of these possible causes: while a spike in traffic to the app around
20:45 could explain a brief surge in resource usage, we’d expect traffic to return to
baseline fairly soon after request volume normalized. This spike certainly shouldn’t
have continued for multiple days, beginning when the app’s developers filed the
report and we started looking into the problem. Second, the change in performance
happened on Saturday, when neither changes to the app nor the production environ‐
ment were in flight. The service’s most recent code pushes and configuration pushes
had completed days before. Furthermore, if the problem originated with the service,
we’d expect to see similar effects on other apps using the same infrastructure. How‐
ever, no other apps were experiencing similar effects.
Case Study | 147
We referred the problem report to our counterparts, App Engine’s developers, to
investigate whether the customer was encountering any idiosyncrasies in the serving
infrastructure. The developers weren’t able to find any oddities, either. However, a
developer did notice a correlation between the latency increase and the increase of a
specific data storage API call, merge_join, which often indicates suboptimal indexing
when reading from the datastore. Adding a composite index on the properties the
app uses to select objects from the datastore would speed those requests, and in prin‐
ciple, speed the application as a whole—but we’d need to figure out which properties
needed indexing. A quick look at the application’s code didn’t reveal any obvious sus‐
pects.
It was time to pull out the heavy machinery in our toolkit: using Dapper [Sig10], we
traced the steps individual HTTP requests took—from their receipt by a frontend
reverse proxy through to the point where the app’s code returned a response—and
looked at the RPCs issued by each server involved in handling that request. Doing so
would allow us to see which properties were included in requests to the datastore,
then create the appropriate indices.
While investigating, we discovered that requests for static content such as images,
which weren’t served from the datastore, were also much slower than expected. Look‐
ing at graphs with file-level granularity, we saw their responses had been much faster
only a few days before. This implied that the observed correlation between
merge_join and the latency increase was spurious and that our suboptimal-indexing
theory was fatally flawed.
Examining the unexpectedly slow requests for static content, most of the RPCs sent
from the application were to a memcache service, so the requests should have been
very fast—on the order of a few milliseconds. These requests did turn out to be very
fast, so the problem didn’t seem to originate there. However, between the time the
app started working on a request and when it made the first RPCs, there was about a
250 ms period where the app was doing…well, something. Because App Engine runs
code provided by users, its SRE team does not profile or inspect app code, so we
couldn’t tell what the app was doing in that interval; similarly, Dapper couldn’t help
track down what was going on since it can only trace RPC calls, and none were made
during that period.
Faced with what was, by this point, quite a mystery, we decided not to solve it…yet.
The customer had a public launch scheduled for the following week, and we weren’t
sure how soon we’d be able to identify the problem and fix it. Instead, we recom‐
mended that the customer increase the resources allocated to their app to the most
CPU-rich instance type available. Doing so reduced the app’s latency to acceptable
levels, though not as low as we’d prefer. We concluded that the latency mitigation was
148 | Chapter 12: Effective Troubleshooting
good enough that the team could conduct their launch successfully, then investigate
at leisure.18
At this point, we suspected that the app was a victim of yet another common cause of
sudden increases in latency and resource usage: a change in the type of work. We’d
seen an increase in writes to the datastore from the app, just before its latency
increased, but because this increase wasn’t very large—nor was it sustained—we’d
written it off as coincidental. However, this behavior did resemble a common pattern:
an instance of the app is initialized by reading objects from the datastore, then storing
them in the instance’s memory. By doing so, the instance avoids reading rarely chang‐
ing configuration from the datastore on each request, and instead checks the in-
memory objects. Then, the time it takes to handle requests will often scale with the
amount of configuration data.19 We couldn’t prove that this behavior was the root of
the problem, but it’s a common antipattern.
The app developers added instrumentation to understand where the app was spend‐
ing its time. They identified a method that was called on every request, that checked
whether a user had whitelisted access to a given path. The method used a caching
layer that sought to minimize accesses to both the datastore and the memcache ser‐
vice, by holding whitelist objects in instances’ memory. As one of the app’s developers
noted in the investigation, “I don’t know where the fire is yet, but I’m blinded by
smoke coming from this whitelist cache.”
Some time later, the root cause was found: due to a long-standing bug in the app’s
access control system, whenever one specific path was accessed, a whitelist object
would be created and stored in the datastore. In the run-up to launch, an automated
security scanner had been testing the app for vulnerabilities, and as a side effect, its
scan produced thousands of whitelist objects over the course of half an hour. These
superfluous whitelist objects then had to be checked on every request to the app,
which led to pathologically slow responses—without causing any RPC calls from the
app to other services. Fixing the bug and removing those objects returned the app’s
performance to expected levels.
18 While launching with an unidentified bug isn’t ideal, it’s often impractical to eliminate all known bugs.
Instead, sometimes we have make do with second-best measures and mitigate risk as best we can, using good
engineering judgment.
19 The datastore lookup can use an index to speed the comparison, but a frequent in-memory implementation is
a simple for loop comparison across all the cached objects. If there are only a few objects, it won’t matter that
this takes linear time—but this can cause a significant increase in latency and resource usage as the number of
cached objects grows.
Case Study | 149
Making Troubleshooting Easier
There are many ways to simplify and speed troubleshooting. Perhaps the most funda‐
mental are:
• Building observability—with both white-box metrics and structured logs—into
each component from the ground up.
• Designing systems with well-understood and observable interfaces between
components.
Ensuring that information is available in a consistent way throughout a system—for
instance, using a unique request identifier throughout the span of RPCs generated by
various components—reduces the need to figure out which log entry on an upstream
component matches a log entry on a downstream component, speeding the time to
diagnosis and recovery.
Problems in correctly representing the state of reality in a code change or an environ‐
ment change often lead to a need to troubleshoot. Simplifying, controlling, and log‐
ging such changes can reduce the need for troubleshooting, and make it easier when
it happens.
Conclusion
We’ve looked at some steps you can take to make the troubleshooting process clear
and understandable to novices, so that they, too, can become effective at solving prob‐
lems. Adopting a systematic approach to troubleshooting—as opposed to relying on
luck or experience—can help bound your services’ time to recovery, leading to a bet‐
ter experience for your users.
150 | Chapter 12: Effective Troubleshooting
CHAPTER 13
Emergency Response
Written by Corey Adam Baye
Edited by Diane Bates
Things break; that’s life.
Regardless of the stakes involved or the size of an organization, one trait that’s vital to
the long-term health of an organization, and that consequently sets that organization
apart from others, is how the people involved respond to an emergency. Few of us
naturally respond well during an emergency. A proper response takes preparation
and periodic, pertinent, hands-on training. Establishing and maintaining thorough
training and testing processes requires the support of the board and management, in
addition to the careful attention of staff. All of these elements are essential in foster‐
ing an environment in which teams can spend money, time, energy, and possibly even
uptime to ensure that systems, processes, and people respond efficiently during an
emergency.
Note that the chapter on postmortem culture discusses the specifics of how to write
postmortems in order to make sure that incidents that require emergency response
also become a learning opportunity (see Chapter 15). This chapter provides more
concrete examples of such incidents.
What to Do When Systems Break
First of all, don’t panic! You aren’t alone, and the sky isn’t falling. You’re a professional
and trained to handle this sort of situation. Typically, no one is in physical danger—
only those poor electrons are in peril. At the very worst, half of the Internet is down.
So take a deep breath…and carry on.
151
If you feel overwhelmed, pull in more people. Sometimes it may even be necessary to
page the entire company. If your company has an incident response process (see
Chapter 14), make sure that you’re familiar with it and follow that process.
Test-Induced Emergency
Google has adopted a proactive approach to disaster and emergency testing (see
[Kri12]). SREs break our systems, watch how they fail, and make changes to improve
reliability and prevent the failures from recurring. Most of the time, these controlled
failures go as planned, and the target system and dependent systems behave in
roughly the manner we expect. We identify some weaknesses or hidden dependencies
and document follow-up actions to rectify the flaws we uncover. However, sometimes
our assumptions and the actual results are worlds apart.
Here’s one example of a test that unearthed a number of unexpected dependencies.
Details
We wanted to flush out hidden dependencies on a test database within one of our
larger distributed MySQL databases. The plan was to block all access to just one data‐
base out of a hundred. No one foresaw the results that would unfold.
Response
Within minutes of commencing the test, numerous dependent services reported that
both external and internal users were unable to access key systems. Some systems
were intermittently or only partially accessible.
Assuming that the test was responsible, SRE immediately aborted the exercise. We
attempted to roll back the permissions change, but were unsuccessful. Instead of pan‐
icking, we immediately brainstormed how to restore proper access. Using an already
tested approach, we restored permissions to the replicas and failovers. In a parallel
effort, we reached out to key developers to correct the flaw in the database application
layer library.
Within an hour of the original decision, all access was fully restored, and all services
were able to connect once again. The broad impact of this test motivated a rapid and
thorough fix to the libraries and a plan for periodic retesting to prevent such a major
flaw from recurring.
152 | Chapter 13: Emergency Response
Findings
What went well
Dependent services that were affected by the incident immediately escalated the
issues within the company. We assumed, correctly, that our controlled experiment
had gotten out of hand and immediately aborted the test.
We were able to fully restore permissions within an hour of the first report, at which
time systems started behaving properly. Some teams took a different approach and
reconfigured their systems to avoid the test database. These parallel efforts helped to
restore service as quickly as possible.
Follow-up action items were resolved quickly and thoroughly to avoid a similar out‐
age, and we instituted periodic testing to ensure that similar flaws do not recur.
What we learned
Although this test was thoroughly reviewed and thought to be well scoped, reality
revealed we had an insufficient understanding of this particular interaction among
the dependent systems.
We failed to follow the incident response process, which had been put in place only a
few weeks before and hadn’t been thoroughly disseminated. This process would have
ensured that all services and customers were aware of the outage. To avoid similar
scenarios in the future, SRE continually refines and tests our incident response tools
and processes, in addition to making sure that updates to our incident management
procedures are clearly communicated to all relevant parties.
Because we hadn’t tested our rollback procedures in a test environment, these proce‐
dures were flawed, which lengthened the outage. We now require thorough testing of
rollback procedures before such large-scale tests.
Change-Induced Emergency
As you can imagine, Google has a lot of configuration—complex configuration—and
we constantly make changes to that configuration. To prevent breaking our systems
outright, we perform numerous tests on configuration changes to make sure they
don’t result in unexpected and undesired behavior. However, the scale and complexity
of Google’s infrastructure make it impossible to anticipate every dependency or inter‐
action; sometimes configuration changes don’t go entirely according to plan.
The following is one such example.
Change-Induced Emergency | 153
Details
A configuration change to the infrastructure that helps protect our services from
abuse was pushed globally on a Friday. This infrastructure interacts with essentially
all of our externally facing systems, and the change triggered a crash-loop bug in
those systems, which caused the entire fleet to begin to crash-loop almost simultane‐
ously. Because Google’s internal infrastructure also depends upon our own services,
many internal applications suddenly became unavailable as well.
Response
Within seconds, monitoring alerts started firing, indicating that certain sites were
down. Some on-call engineers simultaneously experienced what they believed to be a
failure of the corporate network and relocated to dedicated secure rooms (panic
rooms) with backup access to the production environment. They were joined by
additional engineers who were struggling with their corporate access.
Within five minutes of that first configuration push, the engineer responsible for the
push, having become aware of the corporate outage but still unaware of the broader
outage, pushed another configuration change to roll back the first change. At this
point, services began to recover.
Within 10 minutes of the first push, on-call engineers declared an incident and pro‐
ceeded to follow internal procedures for incident response. They began notifying the
rest of the company about the situation. The push engineer informed the on-call
engineers that the outage was likely due to the change that had been pushed and later
rolled back. Nevertheless, some services experienced unrelated bugs or misconfigura‐
tions triggered by the original event and didn’t fully recover for up to an hour.
Findings
What went well
There were several factors at play that prevented this incident from resulting in a
longer-term outage of many of Google’s internal systems.
To begin with, monitoring almost immediately detected and alerted us to the prob‐
lem. However, it should be noted that in this case, our monitoring was less than ideal: