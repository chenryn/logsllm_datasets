title:FORECAST: skimming off the malware cream
author:Matthias Neugschwandtner and
Paolo Milani Comparetti and
Gr&apos;egoire Jacob and
Christopher Kruegel
FORECAST – Skimming off the Malware Cream
Matthias Neugschwandtner1, Paolo Milani Comparetti1, Gregoire Jacob2, and Christopher Kruegel2
1Vienna University of Technology, {mneug,pmilani}@seclab.tuwien.ac.at
2University of California, Santa Barbara, {gregoire,chris}@cs.ucsb.edu
Malware commonly employs various forms of packing and ob-
fuscation to resist static analysis. Therefore, the most widespread
approach to the analysis of malware samples is currently based on
executing the malicious code in a controlled environment to ob-
serve its behavior. Dynamic analysis tools such as CWSandbox [3],
Norman Sandbox and Anubis [13, 2] execute a malware sample in
an instrumented sandbox and record its interactions with system
and network resources. This information can be distilled into a
human-readable report that provides an analyst with a high level
view of a sample’s behavior, but it can also be fed as input to fur-
ther automatic analysis tasks. Execution logs and network traces
provided by dynamic analysis have been used to classify malware
samples [14, 36], to generate remediation procedures for malware
infections [31] and to generate signatures for detecting a malware’s
network trafﬁc [34].
ABSTRACT
To handle the large number of malware samples appearing in the
wild each day, security analysts and vendors employ automated
tools to detect, classify and analyze malicious code. Because mal-
ware is typically resistant to static analysis, automated dynamic
analysis is widely used for this purpose. Executing malicious soft-
ware in a controlled environment while observing its behavior can
provide rich information on a malware’s capabilities. However,
running each malware sample even for a few minutes is expensive.
For this reason, malware analysis efforts need to select a subset of
samples for analysis. To date, this selection has been performed ei-
ther randomly or using techniques focused on avoiding re-analysis
of polymorphic malware variants [41, 23].
In this paper, we present a novel approach to sample selection
that attempts to maximize the total value of the information ob-
tained from analysis, according to an application-dependent scor-
ing function. To this end, we leverage previous work on behavioral
malware clustering [14] and introduce a machine-learning-based
system that uses all statically-available information to predict into
which behavioral class a sample will fall, before the sample is ac-
tually executed. We discuss scoring functions tailored at two prac-
tical applications of large-scale dynamic analysis: the compilation
of network blacklists of command and control servers and the gen-
eration of remediation procedures for malware infections. We im-
plement these techniques in a tool called FORECAST. Large-scale
evaluation on over 600,000 malware samples shows that our pro-
totype can increase the amount of potential command and control
servers detected by up to 137% over a random selection strategy
and 54% over a selection strategy based on sample diversity.
1.
INTRODUCTION
Malware is at the root of many security threats on the internet.
From spam, to identity theft to distributed denial of service attacks,
malicious software running on compromised computers is a key
component of internet crime. For this reason, analyzing malware
and developing countermeasures against it has become an impor-
tant aspect of security practice. New malware samples need to be
analyzed to understand their capabilities and generate detection sig-
natures, mitigation strategies and remediation procedures. Since
tens of thousands of new malware samples are found in the wild
each day, security analysts and antivirus vendors have to employ
automated analysis techniques for this task.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ACSAC ’11 Dec. 5-9, 2011, Orlando, Florida USA
Copyright 2011 ACM 978-1-4503-0672-0/11/12 ...$10.00.
One problem of dynamic analysis of malware is that it is resource-
intensive. Panda Labs reported 63,000 new malware samples per
day in 2010 with an upward trend [9]. Each of these samples needs
to be executed, if only for a few minutes. Furthermore, it is rela-
tively easy for malware authors to aggravate this problem by auto-
matically generating even larger numbers of polymorphic variants.
As a result of the limited analysis capacity, only a subset of the daily
malware samples can be analyzed. Our own analysis sandbox, de-
spite a large-scale, distributed deployment, has to discard tens of
thousands of samples each day. This raises the question of which
samples should be selected to best utilize the available resources.
Previous work on selecting samples for dynamic analysis [41,
23] has focused on diversity: that is, the goal is to determine whether
a new malware sample, with a never-before-seen message digest, is
actually just a minor variant or a polymorphic mutation of a pre-
viously analyzed sample. Results obtained with these techniques
have shown that discarding polymorphic variants can reduce the
amount of samples to be analyzed by a factor of over sixteen. The
assumption behind this approach is that analyzing a polymorphic
variant of a known sample will not provide any new insight, so the
sample should be discarded rather than waste resources on execut-
ing it in the sandbox. Depending on the purpose for which samples
are being analyzed, however, this assumption may not hold.
One motivation for operating a malware analysis sandbox is that
the network behavior of malware can reveal the command and con-
trol (C&C) servers used by bot masters to remotely control infected
computers. If the goal is to detect C&C servers, running multiple
variants of some malware families can prove advantageous. This is
a consequence of the constant arms race between bot masters and
security professionals: bot masters need to maintain control of their
bots while security professionals work to identify and take down
their C&C infrastructure. As a result, the C&C servers used by a
malware family change over time much faster than its code-base.
Furthermore, some malware code-bases are available to multiple
independent bot masters, each of which uses a distinct C&C infras-
tructure [15].
Rather than selecting samples for analysis based only on diver-
sity, we therefore take a different angle and explicitly try to select
for analysis those samples that will produce the most valuable anal-
ysis results. This requires us to ﬁrst deﬁne how to measure the
value of the output of a dynamic analysis run. We argue that this is
application-dependent, and that sample selection should take into
account the goals for which malware is being analyzed in the ﬁrst
place.
To predict whether and to what extent the execution of a sam-
ple will yield useful information, we take advantage of knowledge
gleaned from samples that have already been analyzed. We ex-
tract all statically-available information on each malware sample,
including structural features of the executable, antivirus detection
results and the results of static classiﬁcation using techniques from
Wicherski [41] and Jacob et al. [23]. For all dynamically ana-
lyzed samples, we further record their behavior in the sandbox and
the results of behavioral clustering using techniques from Bayer et
al. [14]. Over time, our system thus assembles a knowledge-base
of static and behavioral characteristics of malware samples. This
knowledge-base can be mined for sample selection. We use the
static information on each sample as input to a machine-learning
system that aims to predict to which behavioral cluster a sample
belongs, before we actually run the sample. We then select for ex-
ecution samples expected to belong to clusters that, based on past
performance, are most likely to provide useful information.
We implement the proposed techniques in a tool called FORE-
CAST, and empirically evaluate its performance using a large col-
lection of real-world malware binaries. Our results show that se-
lecting samples for analysis using FORECAST can provide more
useful information for a given amount of analysis resources com-
pared not only to naive, random selection, but also to a selection
strategy aimed at maximizing diversity. In summary, our contribu-
tions are the following:
• We formulate the sample selection problem as the task of choos-
ing samples for dynamic analysis to maximize the aggregate value
of the analysis results.
• We introduce novel techniques that allow us to predict the dy-
namic behavior of a malware sample before executing it. More
precisely, they allow us to predict the behavioral cluster [14] to
which the sample will belong.
• We introduce scoring functions for measuring the value of in-
formation obtained from dynamic analysis that are targeted at two
practical applications; Namely the generation of network blacklists
of command and control servers and the generation of procedures
for the remediation of malware infections on end hosts.
• Based on these techniques, we develop a system for selecting
samples for dynamic analysis according to the expected value of
the information obtained from a sample’s execution.
• We evaluate the proposed techniques on over 600,000 malware
samples, and show that they can increase the total value of the in-
formation obtained from dynamic analysis by 134% compared to
a random selection strategy and by 54% compared to a selection
strategy based on sample diversity.
2. SYSTEM GOALS AND APPROACH
The goal of FORECAST is to increase the insight that can be
gained from executing malware samples in an analysis sandbox,
given a limited amount of computational resources. If insufﬁcient
resources are available to analyze all the malware samples that are
collected each day, sandbox operators are faced with the choice of
which samples to select for analysis.
Figure 1: FORECAST overview.
We call this the sample selection problem, and formulate it as
follows. Given a set χ of n malware samples, a scoring function
v that measures the aggregate value of the analysis results of a set
of samples and limited resources that allow the dynamic analysis
of only k < n samples, we want to select a subset α ⊂ χ, with
|α| = k, that maximizes v(α). The set α can be built incremen-
tally: When selecting the next sample for analysis, we can take into
account the analysis results for all previously analyzed samples.
Previous work [41, 23] has implicitly attempted to solve this
problem by recognizing and discarding minor variants or polymor-
phic mutations of previously analyzed malware samples. The as-
sumption behind this is that every such variant will exhibit the same
behavior, therefore analyzing more than one variant provides no ad-
ditional valuable information. Because it lacked a measure of the
value of analysis results, previous work did not attempt to quan-
titatively validate this assumption. As we will show in Section 4,
discarding minor variants is indeed a good heuristic for selecting
samples. However, we will also show that in some cases executing
several almost identical samples can provide valuable information,
such as the different C&C servers contacted by each sample.
2.1 Applications
In this paper, we take a different approach and explicitly measure
the value of analysis results. For this, we develop scoring functions
targeted at two real-world applications.
Identifying C&C servers. Modern malware uses a command and
control (C&C) infrastructure that allows the malware operators to
remote-control the infected machines (also known as bots). It also
lets them update the bots’ software to adapt to the changing envi-
ronment in which they operate and to the changing goals of the bot-
net owners. While some botnets employ peer-to-peer protocols for
C&C, most employ client-server architectures and rely on redun-
dancy and fallback mechanisms to provide robustness. The C&C
servers are therefore a weak point of a botnet’s operation, making
information on their domain names or IP addresses extremely valu-
able to security practitioners. Recent research has thus focused on
identifying C&C communication among the network trafﬁc gener-
ated by malware [22]. Such information has been used for coor-
dinated takedowns of a botnet’s C&C servers, that in some cases
have succeeded in completely shutting down a botnet [29]. In a
few cases, C&C server information has even led to the networks
of malicious internet service providers being depeered from the in-
ternet [27]. Even if the malicious servers cannot be taken down,
blacklists of C&C servers such as the one provided by FIRE [39]
or Zeus Tracker[8] can be used by network administrators as an ad-
ditional layer of defense. A C&C blacklist provides two beneﬁts:
on the one hand, it prevents infected hosts from receiving com-
mands that would lead them to engage in harmful behavior; On the
other hand, it can alert a network administrator to the presence of
infected hosts on his network. FIRE builds its C&C blacklist based
on the results of large-scale dynamic analysis of malware samples
with the Anubis [2] sandbox. Furthermore, the C&C trafﬁc cap-
tured during malware execution can be used to automatically gen-
erate detection signatures [34], that can be deployed on network
intrusion detection solutions. The network endpoints scoring func-
Feature ExtractionstaticfeaturesClusterPredictionClusterScoringDynamicAnalysisclusterprobabilitiesselectedsamplemalwaresamplesbehavioralfeaturesclustertion discussed in Section 3.4 is therefore designed to measure the
number of potential C&C servers observed during analysis.
Generating remediation procedures. When malicious code ob-
tains unrestricted execution privileges on an infected host, com-
pletely reinstalling the affected machine is typically the only sound
way of guaranteeing that the malware is fully eradicated. For a
given, known malware, however, it may be possible to generate a
reliable remediation procedure that is able to revert the effects of
the malicious code on the system and avoid the cost of reinstalla-
tion. Remediating malware infections is a task routinely performed
by anti-virus software, with varying levels of success. Recent re-
search has proposed techniques for automatically generating such
remediation procedures [31]. These techniques are based on dy-
namic analysis: malware samples are executed in an instrumented
environment, and all persistent modiﬁcations of the system state
are recorded. A remediation procedure essentially consists of a list
of affected system resources, that have to be reset to a clean state.
While the techniques in [31] include methods for generalizing the
observed behavior to some extent, it is clear that behavior that was
never observed cannot be remediated. To provide a more complete
set of remediation procedures, it is therefore desirable to observe
the widest possible variety of system-modifying behavior. The per-
sistent modiﬁcations scoring function discussed in Section 3.4 is
therefore designed to measure the amount of distinct system re-
sources affected by malware execution.
2.2 System overview
Figure 1 shows a high-level overview of FORECAST’s architec-
ture. FORECAST works in four phases:
Feature extraction. For each sample that is being considered for
analysis, we ﬁrst extract a number of static features. These fea-
tures represent all the information we can efﬁciently obtain about
a malware sample without executing it. We consider a wide vari-
ety of static features. First of all, we extract a number of structural
features about the malicious executable. We consider information
on the origin of the malware sample, such as the user responsi-
ble for its submission to the analysis sandbox. We also include
detection results from a number of anti-virus engines. Finally, we
leverage previous work on detecting polymorphic malware variants
and include a sample’s peHash [41], as well as its static cluster and
packing level obtained using techniques from Jacob et al. [23].
Cluster prediction. The dynamic analysis phase (discussed below)
identiﬁes the behavioral cluster to which each executed sample be-
longs. Together with the static features, this serves as input to the
cluster prediction phase. Here, we attempt to predict to which be-
havioral cluster a sample belongs, using a supervised learning ap-
proach. For this, we use a conﬁdence-weighted linear classiﬁer that
outputs the probabilities that a considered sample belongs to each
behavioral cluster. Whenever a sample is dynamically analyzed
and assigned to a cluster, our classiﬁer is updated to account for
this new information. Note that, while cluster prediction uses su-
pervised learning, the behavioral malware classiﬁcation techniques
we employ are unsupervised [14], and new behavioral clusters are
added incrementally as they are discovered.
Cluster scoring. In this phase we measure the cluster score for
each behavioral cluster C, deﬁned as the average contribution of
a sample in that cluster to the scoring function v(C). This step is
therefore dependent on the choice of an application-speciﬁc scoring
function. The scoring function takes as input the behavioral fea-
tures observed during the execution of each sample, and measures
their aggregate value. We employ two scoring functions targeted at
the applications discussed in Section 2.1. For each sample that is
considered for analysis, we can then compute its expected contri-
bution to v based on the cluster scores and the cluster probabilities
obtained in the previous phase. We call this the sample score. The
output of the cluster scoring phase is the highest scoring sample,
out of a pool of candidates, that is passed to the dynamic analysis
phase. Whenever the dynamic analysis results for a sample become
available, the cluster scores and sample scores are incrementally
updated.
Dynamic analysis. Once a malware sample has been selected, we
analyze it by running it in our instrumented sandbox. The host-
level and network-level behavior observed during execution is con-
densed into a set of behavioral features. This set is fed back to the
cluster scoring phase. Furthermore, we cluster all analyzed sam-
ples based on the set of behavioral features they exhibit, using tech-
niques from Bayer et al. [14]. The behavioral cluster to which each
analyzed sample belongs is fed back to the cluster prediction phase.
3. SYSTEM DESCRIPTION
Figure 2: FORECAST architecture.
Figure 2 provides a more detailed view of FORECAST’s archi-
tecture. FORECAST takes as input a set χ of candidate malware
samples. The system’s goal is to select for analysis a subset α ⊂ χ.
FORECAST works incrementally: when selecting the next samples
for analysis, it takes into account the analysis results for all previ-
ously analyzed samples. For this, FORECAST maintains an anal-
ysis queue where each not-yet-analyzed candidate sample (that is,
each sample in χ\α), is associated with a sample score. The sample
score is a measure of how much valuable information we expect to
obtain from the analysis of that sample. Of course, to achieve high
throughput, malware analysis sandboxes need to analyze several
samples in parallel. Thus, at each iteration FORECAST selects the
L top-scoring samples for analysis, where L is the parallelism level
of the sandbox, indicating the number of samples that it is able to
analyze in parallel.