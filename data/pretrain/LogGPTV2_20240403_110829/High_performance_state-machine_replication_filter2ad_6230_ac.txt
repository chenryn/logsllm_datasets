We have integrated state partitioning into the Ring Paxos protocol as follows. Each partition is associated with a unique IP multicast address (corresponding to Step 2 in Figure 3), and there is another IP multicast address for decisions (corresponding to Step 4 in Figure 3). Unlike the original Ring Paxos, we do not piggyback decision messages with commands. Learners (i.e., servers) listen on the partition addresses they are interested in and the decision address, while acceptors listen on all addresses. A command contains information about the partitions it accesses. For each partition accessed by the command, the coordinator sends one Phase 2A message (containing the command) using the corresponding partition's address. If a process receives the same message more than once, it discards the duplicates. Once order is established, the coordinator multicasts the decision message using the decision address. Learners may receive decision messages for partitions they are not interested in; in such cases, they discard these messages.

The state partitioning technique enhances the scalability of state-machine replication but may not be applicable in all scenarios or could impose restrictions on how the state of a service can be partitioned. For instance, consider a service whose state includes variables x and y, and a command that modifies x based on the value of y. In this case, the service’s state must be partitioned such that both x and y belong to the same partition. While this constraint limits the number of services that can benefit from state partitioning, we demonstrate in the next section that the technique is versatile enough to enable the implementation of a highly available parallel B-tree service.

### IV. Replicated Parallel B-Trees

In this section, we illustrate high-performance state-machine replication with a B-tree service. We define the service’s interface and how it was implemented and optimized using speculative execution and state partitioning.

#### B-Tree Service
The B-tree stores (key, value) tuples, where both key and value are 8-byte integers. Clients can submit insert, delete, and query commands. An `insert(k, val)` command checks if an entry with key `k` already exists in the tree; if not, `(k, val)` is added to the tree. The command returns an acknowledgment. A `delete(k)` command removes the entry with key `k`, if it exists, and returns an acknowledgment. A `query(min, max)` command returns all entries `(k, val)` such that `min ≤ k ≤ max`.

#### Fully Replicated B-Tree
To tolerate server failures, we replicate the B-tree service using state-machine replication. Client commands are linearizable and submitted to the servers via Ring Paxos. Insert and delete commands are received and executed by all operational servers, but only one server (randomly chosen by the client) responds. A query command is received by all operational servers and executed by a single server, randomly chosen by the client. If a client does not receive a response for a command after some time, it resubmits the command.

#### Speculative Execution
To reduce the response time experienced by clients, we use speculative execution. Since queries do not change the state of the tree, there is no state to roll back if commands are delivered out-of-order. Inserts and deletes are executed against the B-tree as soon as they are received. To roll back a successful `insert(k, val)`, the server executes a `delete(k)`—there is nothing to roll back if the insert fails because the key already exists. A `delete(k)` is rolled back by re-inserting the value removed.

#### State Partitioning
We divide the state of the B-tree into partitions, with each partition responsible for a range of keys (i.e., range partitioning). A command that accesses more than one partition is broken into sub-commands by the client (i.e., by a client replication library) and submitted to each concerned partition. Responses from multiple partitions are merged at the client. Key ranges are of the same size, but depending on the keys included in and deleted from the B-tree, partitions may become unbalanced. We do not currently address this problem, but it is part of our ongoing work. We are considering techniques to repartition the key space on-the-fly to keep partitions balanced.

### V. Performance Evaluation

In this section, we assess the performance of our replicated B-tree. We consider executions in the presence of message losses and in the absence of process failures. Process failures are rare events, while message losses occur relatively often due to high network traffic.

#### A. Experimental Setup
We conducted the experiments on a cluster of Dell SC1435 servers equipped with 2 dual-core AMD-Opteron 2.0 GHz CPUs and 4GB of main memory. The servers are interconnected through an HP ProCurve2900-48G Gigabit switch (0.1 msec round-trip time). Each experiment (i.e., point in the graph) is obtained over a 60-second run, with the first and last 10 seconds discarded. Clients and servers run on different nodes. Each client runs in a closed loop with a random think time in the range of 0–10 msec. The B-tree is initialized with 12 million entries. Client commands are 256-byte messages; responses are 8 Kbytes for ranges and 256 bytes for inserts and deletes. We consider three workloads: (a) each client command is a query with a range of 1000 keys; (b) each client command is an insert or a delete (referred to as updates); (c) each client command is composed of seven batched updates (which fit in a 256-byte message). Additionally, in workload (c), Ring Paxos batches client messages in larger packets (8 Kbytes) to improve throughput.

#### B. The Cost of Replication
Our first set of experiments evaluates the costs of state-machine replication (SMR) compared to a non-replicated client-server (CS) setup (see Figures 5 and 6). For queries and batched updates, replication does not introduce a cost in throughput. In these cases, the executions are CPU-bound. For single updates, the replicated setting cannot reach the same throughput as a client-server configuration because the execution is limited by the maximum number of instances per second that can be run by Ring Paxos. In all cases, however, replication imposes a cost in response time, as shown in the right column of Figure 5. Response time for few clients with batched updates in the replicated setting is high because with low load, Ring Paxos packets are sent due to timeouts; this effect disappears as more clients are added and messages are sent as soon as an 8-Kbyte packet is full.

Adding replicas can help improve the throughput of read-only commands, as shown by the left bar on the left graph in Figure 6. For update commands, no improvement in throughput is possible since all replicas must be involved in the operations, even if only to receive the commands in the right order, as discussed in Section II-B. Figure 6 also shows the corresponding response times, with the highest values for all replicated experiments.

#### C. Speculative Execution
We report our assessment of speculative execution with configurations of 4 and 8 servers using the queries workload and the batched updates workload (see Figures 7 and 8). In all scenarios, speculation reduces response time with respect to state-machine replication, although the results are more visible with batched updates. By reducing response time, the technique also proportionally improves throughput, a direct consequence of Little’s law [13]. We also conducted experiments with 1 and 2 servers, with similar results [5].

#### D. State Partitioning
To assess the state partitioning strategy, we consider two configurations, one with the B-tree state divided into two partitions and the other with the B-tree state divided into four partitions (labeled “2 P” and “4 P” in Figure 9, respectively). In both configurations, each partition has two replicas. In executions with cross-partition query commands, a cross-partition query accesses two partitions, regardless of the number of existing partitions.

The graph on the left of Figure 9 shows that for queries, the throughput increases by a factor of 2.1 from SMR to two partitions and by a factor of nearly four from SMR to four partitions. The improvement for batched updates is less remarkable, although the system throughput increases by factors of 1.8 and 2.6 for two and four partitions, respectively. The graph on the right of the figure shows that this increase in throughput does not incur significant changes in response time with respect to SMR. Although these experiments were run using no cross-partition queries, this is not the most favorable setup for state partitioning, as we show next.

Figure 10 considers the effects of cross-partition queries in the state partitioning technique with two partitions in an execution with query commands. The graphs show that for lower loads (i.e., 100 clients), there is almost no difference in throughput and response time between different configurations. For higher loads, configurations with 50% and 75% of cross-partition queries reach higher throughputs. In fact, the lowest throughput and highest response time is obtained with a configuration without cross-partition queries. To understand why, we must look at how servers are implemented and how the CPU is used in a server.

Each server is implemented by three threads: one that receives commands, one that executes them, and one that responds to clients. Each thread is assigned a different processor. Figure 11 shows the threads responsible for execution and responses; the thread that receives commands has a low CPU load and therefore is not shown in the graph. In configurations with no cross-partition queries, 98% of the processor for command execution is used, while in configurations with 25% and 100% of cross-partition queries, the processor for command execution and response is 95% used. Finally, in configurations with 50% and 75% of cross-partition queries, the processors are used less than 90%. The 50% configuration has slightly higher throughput than the 75% configuration because it uses less bandwidth.

The reason for the execution processor use to decrease with the increase in the number of cross-partition queries is that a cross-partition query is "cheaper" to execute than a single-partition query since it processes fewer elements in the B-tree. However, the response processor use increases with the number of cross-partition queries because a cross-partition query is split into two queries (and thus there are more queries), and servers respond to queries with fixed-size messages, regardless of the amount of information contained in the message.

#### E. Combining Speculative Execution and State Partitioning
Our final set of experiments considers the combined effects of speculative execution and state partitioning. Figure 12 shows the relative improvements of the speculative execution technique over state-machine replication with state partitioning for different percentages of cross-partition queries. For example, without cross-partition queries (left-most bar in both graphs), speculative execution reduces the response time obtained with state partitioning by 16% and increases the throughput by 5%. In all configurations, the technique is effective in that it decreases response time, with minor improvements in throughput. The reason for the improvement to decrease with the number of cross-partition queries is that the execution time in a server of a cross-partition query is shorter, but the response time increases due to the additional processing required for merging results.