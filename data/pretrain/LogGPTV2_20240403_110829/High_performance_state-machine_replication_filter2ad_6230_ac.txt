We have integrated state partitioning order into Ring
Paxos as follows. First, there is one ip-multicast address
associated with each partition (corresponds to Step 2 in
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:31:45 UTC from IEEE Xplore.  Restrictions apply. 
458Figure 4.
can happen if sub-trees of the B-tree are replicated independently.
(Left) An non-linearizable execution that cannot happen if a B-tree is replicated with state-machine replication. (Right) How the same execution
Figure 3) and one ip-multicast address associated with de-
cisions (corresponds to Step 4 in Figure 3). Differently than
Ring Paxos, we do not piggyback decision messages with
commands. Learners (i.e., servers) listen on the partition
addresses they are interested in and on the decision address.
Acceptors listen on all addresses. A command contains in-
formation about the partitions it accesses. For each partition
accessed by the command, the coordinator ip-multicasts one
Phase 2A message (with the command) using the address
associated with the partition. If a process receives the same
message more than once, it simply discards the duplicates.
When order is established,
the coordinator ip-multicasts
the decision message using the decision address. Learners
may receive decision messages for partitions they are not
interested in, in which case they discard the messages.
To conclude, the state partitioning technique improves
the scalability of state-machine replication but it may not
be applicable in some cases or it may impose restrictions
on how the state of a service can be partitioned. Consider
a service whose state contains variables x and y, and a
command that modiﬁes x based on the value of y. In this
case, the service’s state can only be partitioned such that both
x and y belong to the same partition. While this constraint
limits the number of services that can beneﬁt from state
partitioning, we show in the next section that the technique
is general enough to allow the implementation of a highly
available parallel B-tree service.
IV. REPLICATED PARALLEL B-TREES
In this section we illustrate high performance state-
machine replication with a B-tree service. We deﬁne the
service’s interface and how it was implemented and opti-
mized using speculative execution and state partitioning.
B-tree service: The B-tree stores (key, value) tuples,
where both key and value are 8-byte integers. Clients
can submit insert, delete and query commands. An insert
command insert(k, val) checks whether an entry with key k
already exists in the tree; if not, (k, val) is included in the
tree. In any case the command returns an acknowledgement.
A delete command delete(k) removes entry with key k, if
existent, and returns an acknowledgement. A query com-
mand query(min, max) returns all entries (k, val) such that
min ≤ k ≤ max.
Fully replicated B-tree:
In order to tolerate server
failures we replicate the B-tree service using state-machine
replication. Client commands are linearizable and submitted
to the servers by means of Ring Paxos. Insert and delete
commands are received and executed by all operational
servers, but only one server (randomly chosen by the client)
responds. A query command is received by all operational
servers and executed by a single server, randomly chosen
by the client. If a client does not receive the response for a
command after some time it resubmits the command.
Speculative execution: To reduce the response time
experienced by clients we use speculative execution. Since
queries do not change the state of the tree, there is no state to
be rolled back in case of commands delivered out-of-order.
Inserts and deletes are executed against the B-tree as soon
as they are received. To roll back a successful insert(k, val),
the server executes a delete(k)—there is nothing to roll back
if the insert fails because the key already exists. A delete(k)
is rolled back by re-inserting the value removed.
State partitioning: We divide the state of the B-tree in
partitions such that each partition is responsible for a range
of keys (i.e., range partitioning). A command that accesses
more than one partition is broken into sub-commands by the
client (i.e., by a client replication library) and submitted to
each concerned partition. Responses received from multiple
partitions are merged at the client. Key ranges are of the
same size, but depending on the keys included in and deleted
from the B-tree, partitions may become unbalanced. We do
not currently address this problem, but it is part of our
ongoing work. We are considering techniques to repartition
the key space on-the-ﬂy to keep partitions balanced.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:31:45 UTC from IEEE Xplore.  Restrictions apply. 
Neither sequential consistent nor linearizableQuery(0,100)Reply({10})Client C3Client C4ReorderedSequence(Proof)Client C2Client C1Insert(10)Insert(75)Reply(ok)Reply(ok)Query(0,100)Reply({75})Both sequential consistent and linearizable (see Proof)Client C3Client C4ReorderedSequence(Proof)Client C2Client C1Insert(10)Insert(75)Reply(ok)Reply(ok)Query(0,50)Query(51,100) Query(0,50)Query(51,100) Reply({75})Reply({10})Reply(∅)Reply(∅)459Figure 5.
different throughput scales, one of which is logarithmic. (Right) Response time versus number of clients.
State-machine replication (SMR) versus client-server (CS) under three workloads. (Left) Throughput versus number of clients—notice the
Figure 6. State-machine replication with increasing number of replicas versus client-server. (Left) Maximum throughput versus number of servers (y-axis
in log scale). (Right) Response time versus number of servers.
V. PERFORMANCE EVALUATION
In this section we assess the performance of our replicated
B-tree. We consider executions in the presence of message
losses and in the absence of process failures. Process failures
are hopefully rare events; message losses happen relatively
often because of high network trafﬁc.
A. Experimental setup
We ran the experiments in a cluster of Dell SC1435
servers equipped with 2 dual-core AMD-Opteron 2.0 GHz
CPUs and 4GB of main memory. The servers are intercon-
nected through an HP ProCurve2900-48G Gigabit switch
(0.1 msec of round-trip time). Each experiment (i.e., point
in the graph) is obtained over a 60-second run out of which
the ﬁrst and the last 10 seconds are discarded. Clients and
servers run in different nodes. Each client runs in a closed
loop with a random think time in the range of 0–10 msec.
In all experiments the B-tree is initialized with 12 million
entries. Client commands are messages with 256 bytes;
responses are 8 Kbytes for ranges and 256 bytes for inserts
and deletes. We consider three workloads: (a) each client
command is a query with range of 1000 keys; (b) each
client command is an insert or a delete—hereafter we refer
to inserts and deletes as updates; (c) each client command
is composed of seven batched updates (which is what ﬁts
in a 256-byte message). Additionally, in workload (c) Ring
Paxos batches client messages in bigger packets (8 Kbytes)
to improve throughput.
B. The cost of replication
Our ﬁrst set of experiments evaluate the costs of state-
machine replication (SMR) with respect to a non-replicated
client-server (CS) setup (see Figures 5 and 6). For queries
and batched updates, replication does not introduce a cost in
throughput. In these cases, the executions are CPU-bound.
For single updates, the replicated setting cannot reach the
same throughput as a client-server conﬁguration because the
execution of the former is limited by the maximum number
of instances per second that can be run by Ring Paxos. In
all cases, however, replication imposes a cost in response
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:31:45 UTC from IEEE Xplore.  Restrictions apply. 
0.8K1.6K2.4K3.2K4K 0 50 100 150 200Queries (single)SMRCS10K20K30K40K50K 0 50 100 150 200Throughput (cmd/sec)Ins/Del (single)0.1K1K10K100K1M 0 50 100 150 200Number of clientsIns/Del (batch) 0 1 2 3 4 5 6 0 50 100 150 200Queries (single)SMRCS 0 1 2 3 4 5 6 0 50 100 150 200Response time (msec)Ins/Del (single) 0 1 2 3 4 5 6 0 50 100 150 200Number of clientsIns/Del (batch)1K10K100K1MCS1248Throughput (cmd/sec)Number of servers 0 1 2 3 4 5 6CS1248Response time (msec)Number of serversQueries (single)Ins/Del (single)Ins/Del  (batch)460Figure 7.
(Right) Resp. time improvement versus number of servers.
Speculative execution improvement on state-machine replication with 4 replicas. (Left) Throughput improvement versus number of servers.
Figure 8.
(Right) Resp. time improvement versus number of servers.
Speculative execution improvement on state-machine replication with 8 replicas. (Left) Throughput improvement versus number of servers.
time, as shown by the graphs in right column of Figure 5.
Response time for few clients with batched updates in the
replicated setting is high because with low load Ring Paxos
packets are sent due to timeouts; the effect disappears as
clients are added and messages are sent as soon as an 8-
Kbyte packet is full.
Adding replicas can help improve the throughput of read-
only commands, as shown by the left bar on the left graph
in Figure 6. For update commands, no improvement
in
throughput is possible since all replicas must be involved
in the operations, even if only to receive the commands in
the right order, as discussed in Section II-B. Figure 6 also
shows the corresponding response times, with the highest
values for all replicated experiments.
C. Speculative execution
We report our assessment of speculative execution with
conﬁgurations with 4 and 8 servers using the queries work-
load and the batched updates workload (see Figures 7 and 8).
In all scenarios speculation reduces response time with
respect to state-machine replication, although the results are
more visible with batched updates. By reducing response
time, the technique also proportionally improves throughput,
a direct consequence of Little’s law [13]. We also conducted
experiments with 1 and 2 servers, with similar results [5].
D. State partitioning
To assess the state partitioning strategy, we consider
two conﬁgurations, one with the B-tree state divided into
two partitions and the other with the B-tree state divided
into four partitions (labels “2 P” and “4 P” in Figure 9,
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:31:45 UTC from IEEE Xplore.  Restrictions apply. 
1K2K3K4K5K6K7K8K9K 0 20 40 60 80 100 120 140 160Queries (single)Throughput (cmd/sec)SpeculativeSMR50K100K150K200K250K300K 0 50 100 150 200 250 300 350 400Number of clientsIns/Del (batch)SpeculativeSMR 0 1 2 3 4 5 6 0 20 40 60 80 100 120 140 160Queries (single)Response time (msec) 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400Number of clientsIns/Del (batch)2K3K4K5K6K7K8K9K 0 20 40 60 80 100 120 140 160 180 200Queries (single)Throughput (cmd/sec)SpeculativeSMR50K100K150K200K250K300K 0 50 100 150 200 250 300 350 400Number of clientsIns/Del (batch)SpeculativeSMR 0 1 2 3 4 5 6 0 20 40 60 80 100 120 140 160 180 200Queries (single)Response time (msec) 0 1 2 3 4 5 6 0 50 100 150 200 250 300 350 400Number of clientsIns/Del (batch)461Figure 9. State partitioning (2 and 4 partitions) versus state-machine replication for queries and batched updates with no cross-partition commands. (Left)
Throughput (y-axis in log scale) with improvement over SMR versus command type. (Right) Response time versus command type.
Figure 10. Effects of cross-partition queries in state partitioning. (Left) Throughput versus number of clients. (Right) Response time versus number of
clients.
respectively); in both conﬁgurations each partition has two
replicas. In executions with cross-partition query commands,
a cross-partition query accesses two partitions, regardless the
number of existing partitions.
The graph on the left of Figure 9 shows that for queries,
the throughput increases by a factor of 2.1 from SMR to
two partitions, and by a factor of nearly four from SMR
to four partitions. The improvement on batched updates
is not as remarkable as on queries, although the system
throughput increases by factors of 1.8 and 2.6 for two and
four partitions, respectively. The graph on the right of the
ﬁgure shows that such an increase in throughput does not
incur in signiﬁcant changes in response time with respect to
SMR. Although these experiments were run using no cross-
partition queries, this is not the most favorable setup for
state partitioning, as we show next.
Figure 10 considers the effects of cross-partition queries
in the state partitioning technique with two partitions in an
execution with query commands. The graphs show that for
lower load (i.e., 100 clients) there is almost no difference
in throughput and response time between different conﬁgu-
rations. For higher loads, conﬁgurations with 50% and 75%
of cross-partition queries reach higher throughputs. In fact,
the lowest throughput and highest response time is obtained
with a conﬁguration without cross-partition queries. To un-
derstand why, we must look at how servers are implemented
and how the CPU is used in a server.
Each server is implemented by three threads: one that
receives commands, one that executes them, and one that
responds to clients. Each thread is assigned a different pro-
cessor. Figure 11 shows the threads responsible for execution
and responses; the thread that receives commands has low
CPU load and therefore is not shown in the graph. While
in conﬁgurations with no cross-partition queries, 98% of the
processor for command execution is used, in conﬁgurations
with 25% and 100% of cross-partition queries, the processor
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:31:45 UTC from IEEE Xplore.  Restrictions apply. 
1K5K10K20K100K300K500K800KQueriesIns/Del (batch)Throughput (cmd/sec)1X2.1X3.9X1X1.8X2.6X 0 1 2 3 4 5 6QueriesIns/Del (batch)Response time (msec)SMR2 P4 P04K8K12K16K20K 0 50 100 150 200 250Throughput (cmd/sec)Number of clients0 %25 %50 %75 %100 % 1 2 3 4 5 6 0 50 100 150 200 250Response time (msec)Number of clients462for command execution and response is 95% used. Finally,
in conﬁgurations with 50% and 75% of cross-partition
queries, the processors are used less than 90%. The 50%
conﬁguration has slightly higher throughput than the 75%
conﬁguration because it uses less bandwidth.
Figure 11. CPU utilization for the experiments in Figure 10.
The reason for the execution processor use to decrease
with the increase in the number of cross-partition queries is
that a cross-partition query is “cheaper” to execute than a
single-partition query since it processes fewer elements in
the B-tree. However, the response processor use increases
with the number of cross-partition queries because a cross-
partition query is split into two queries (and thus there are
more queries) and servers respond to queries with ﬁxed-size
messages, regardless the amount of information contained
in the message.
E. Combining speculative execution and state partitioning
Our ﬁnal set of experiments considers the combined
effects of speculative execution and state partitioning. Fig-
ure 12 shows the relative improvements of the specula-
tive execution technique over state-machine replication with
state partitioning for different percentages of cross-partition
queries. For example, without cross-partition queries (left-
most bar in both graphs), speculative execution reduces the
response time obtained with state partitioning by 16% and
increases the throughput by 5%. In all conﬁgurations the
technique is effective in that it decreases response time,
with minor improvements in throughput. The reason for the
improvement to decrease with the number of cross-parition
queries is that the execution time in a server of a cross-