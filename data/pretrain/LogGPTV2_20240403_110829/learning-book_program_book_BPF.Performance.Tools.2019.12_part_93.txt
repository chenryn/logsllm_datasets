### Configuring and Adding a New Chart in the Dashboard

Once the dashboard is configured, you can proceed to add a new chart. Begin by selecting a PCP (Performance Co-Pilot) metric. A good starting point is the `bcc.runq.latency` metric (see Figure 17-11).

#### Figure 17-11: Choosing the Query in Grafana

Next, configure the appropriate visualization for the selected metric. For this example, choose the Heatmap visualization. Set the data format to "Time series buckets" and the unit to microseconds (μs). The bucket bound should be set to "Upper" (see Figures 17-12 and 17-13).

#### Figure 17-12: Grafana PCP Showing Standard PCP Metrics (Context Switches, Runnable Count) and Run Queue Latency (runqlat) BCC Metrics

#### Visualization Settings
- **Y Axis:** Time series buckets
- **Data Format:** μs
- **Bucket Bound:** Upper

#### Figure 17-13: Setting Up the Visualization in Grafana

### Future Work and Further Reading

#### 17.2.3 Future Work
Further integration between Grafana and PCP is needed to support the full suite of packaged bcc-tools. Support for visualizing custom bpftrace programs is expected in future updates. Additionally, the `grafana-pcp-live` plugin requires significant additional work before it can be considered production-ready.

#### 17.2.4 Further Reading
- **Grafana PCP Live Data Source:**
  - [GitHub Repository](http://github.com/Neffix-Skunkworks/grafana-pcp-live/)
- **Grafana PCP Redis Data Source:**
  - [GitHub Repository](https://github.com/stpau-dod-euegex8/sopdooaoueusogad/uoorqnu8//sdu)

### Cloudflare eBPF Prometheus Exporter with Grafana

The Cloudflare eBPF exporter is an open-source tool that integrates with the Prometheus monitoring format. Prometheus is popular for metric collection, storage, and querying due to its simple, well-known protocol, which facilitates integration from any language. It also provides alerting functionality and works well with dynamic environments like Kubernetes.

Although Prometheus has a basic UI, tools like Grafana are built on top of it to provide a comprehensive dashboard experience. Prometheus also integrates with existing application operations tools. Within Prometheus, the tool that collects and exposes metrics is known as an exporter. There are official and third-party exporters available for various applications, including Linux host statistics, JMX exporters for Java applications, and more.

Cloudflare has open-sourced an exporter for BPF metrics, allowing these metrics to be exposed and visualized through Prometheus and Grafana.

#### 17.3.1 Build and Run the eBPF Exporter

Note that the build process uses Docker:

```sh
$ git clone https://github.com/cloudflare/ebpf_exporter.git
$ cd ebpf_exporter
$ make
$ sudo ./release/ebpf_exporter-*/ebpf_exporter --config.file=./examples/runqlat.yaml
```

Output:
```
INFO[0000] ebpf_exporter version 0.0.1 (branch: master, revision: 610Z)
2019/04/10 17:42:19 Listening on :9435
```

#### 17.3.2 Configure Prometheus to Monitor the eBPF Exporter Instance

This depends on your approach for monitoring targets. Assuming the instance is running the eBPF exporter on port 9435, you can find a sample target configuration as follows:

```yaml
kubectl edit configmap -n monitoring prometheus-core
- job_name: 'kubernetes-nodes-ebpf-exporter'
  scheme: http
  kubernetes_sd_configs:
  - role: node
    relabel_configs:
    - source_labels: [__address__]
      replacement: '$1:9435'
      target_label: __address__
```

#### 17.3.3 Set Up a Query in Grafana

Once the eBPF exporter is running, it will produce metrics. You can graph these metrics using the following query and additional format (see Figure 17-14):

- **Query:**
  - `rate(lebpf_exporter_run_queue_latency_seconds_bucket[20s])`
- **Legend Format:**
  - `{{le}}`
- **Axis Unit:**
  - `seconds`

For more information on the query format and graph configuration, refer to the Grafana and Prometheus documentation.

#### Figure 17-14: Grafana Run Queue Latency Heat Map, Showing Latency Spikes When schbench is Executed with More Threads Than Cores

#### 17.3.4 Further Reading
- **Grafana and Prometheus:**
  - [Grafana Official Website](https://grafana.com/)
  - [Prometheus GitHub Repository](https://github.com/prometheus/prometheus)
- **Cloudflare eBPF Exporter:**
  - [GitHub Repository](https://github.com/cloudflare/ebpf_exporter)
  - [Blog Post](https://blog.cloudflare.com/introducing-ebpf_exporter/)

### Kubectl-trace

Kubectl-trace is a Kubernetes command-line front end for running bpftrace across nodes in a Kubernetes cluster. It was created by Lorenzo Fontana and is hosted at the IO Visor project.

To follow the examples, you need to download and install kubectl-trace:

```sh
$ git clone https://github.com/iovisor/kubectl-trace.git
$ cd kubectl-trace
$ make
$ sudo cp _output/bin/kubectl-trace /usr/local/bin
```

#### 17.4.1 Tracing Nodes

Kubectl-trace supports running bpftrace commands across a cluster node. Tracing whole nodes is the simplest option, but be mindful of the overhead of your BPF instrumentation. High-overhead bpftrace invocations can affect the entire cluster node.

Example:
```sh
$ kubectl trace run node/ip-1-2-3-4 -f /usr/share/bpftrace/tools/vfsstat.bt
trace 8fc22ddb-5c8411e99ad2-02d0df09784a created
$ kubectl trace get
NAMESPACE   NODE       NAME                                               STATUS   AGE
default     ip-1-2-3-4 kubectl-trace-8fc22ddb-5c8411e9-9ad2-02d0df09784a   Running  3m
$ kubectl trace logs -f kubectl-trace-8fc22ddb-5c84-11e9-9ad2-02d0df097848
00:02:54 [vfs_open]: 940
00:02:55 [vfs_read]: 7797
...
$ kubectl trace delete kubectl-trace-8fc22ddb-5c84-11e9-9ad2-02d0df097848
trace Job kubectl-trace-8fc22ddb-5c84-11e9-9ad2-02d0df09784a deleted
trace configuration kubectl-trace-8fc22ddb-5c8411e9-9ad2-02d0df09784a deleted
```

This output shows all VFS statistics in the entire node, not just the pod. Since bpftrace is executed from the host, kubectl-trace runs in the context of the host, tracing all applications on that node. This may be useful for system administrators, but for many use cases, focusing on processes inside the container is important.

#### 17.4.2 Tracing Pods and Containers

Bpftrace, and thus kubectl-trace, indirectly supports containers by matching tracing through kernel data structures. Kubectl-trace provides two ways to help with pods:
1. When you specify the pod name, kubectl-trace automatically locates and deploys the bpftrace program on the correct node.
2. Kubectl-trace introduces an extra variable, `$container_pid`, which is set to the PID of the container's root process using the host PID namespace. This allows you to filter or target only the desired pod.

Example:
```sh
$ cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: node-hello
spec:
  selector:
    matchLabels:
      app: node-hello
  replicas: 1
  template:
    metadata:
      labels:
        app: node-hello
    spec:
      containers:
      - name: node-hello
        image: duluca/minimalnode-webserver
        command: ["node", "index"]
        ports:
        - containerPort: 3000
EOF
deployment.apps/node-hello created
$ kubectl get pods
NAME                        READY   STATUS    RESTARTS   AGE
node-hello-56b8dbc757th2k2   1/1     Running   0          4s
```

Create a copy of `vfsstat.bt` called `vfsstat-pod.bt` and start a tracer implementation:

```sh
$ cat vfsstat-pod.bt
kprobe:vfs_read /pid == $container_pid/
kprobe:vfs_write /pid == $container_pid/
kprobe:vfs_fsync /pid == $container_pid/
kprobe:vfs_open /pid == $container_pid/
kprobe:vfs_create /pid == $container_pid/
$ kubectl trace run pod/node-hello-56b8dbc757-th2k2 -f vfsstat-pod.bt
trace 552a2492-5c83-11e9-a598-02d0df09784a created
Attaching B probes...
Tracing key VFs calls... Hit Ctrl-C to end.
[...]
17:58:34 [vfs_open]: 1
17:58:35 [vfs_read]: 3
...
```

You will notice significantly fewer VFS operations at the pod level compared to the node level, which is expected for a mostly idle web server.

#### 17.4.3 Further Reading
- **Kubectl-trace GitHub Repository:**
  - [GitHub Repository](https://github.com/iovisor/kubectl-trace)

### Other Tools

Some other BPF-based tools include:
- **Sysdig:** Uses BPF to extend container observability.
- **Android eBPF:** Monitors and manages device network usage on Android devices.
- **ply:** A BPF-based CLI tracer similar to bpftrace but with minimal dependencies, making it well-suited for embedded targets. Created by Tobias Waldekranz.

As BPF usage grows, more BPF-based GUI tools are likely to be developed in the future.

### Summary

The BPF tool space is rapidly growing, and more tools and features will be developed. This chapter presented four currently available tools that build upon BPF: Vector/PCP, Grafana, and Cloudflare’s eBPF exporter, which are graphical tools for presenting complex data, and kubectl-trace, which allows for straightforward execution of bpftrace scripts against a Kubernetes cluster. Additionally, a short list of other BPF tools was provided.

### Tips, Tricks, and Common Problems

#### 18.1 Typical Event Frequency and Overhead

Three main factors determine the CPU overhead of a tracing program:
- The frequency of the event being traced.
- The action performed while tracing.
- The number of CPUs on the system.

An application will suffer this overhead on a per-CPU basis using the relationship:
\[ \text{Overhead} = \frac{\text{Frequency} \times \text{Action performed}}{\text{CPUs}} \]

Tracing one million events per second on a single-CPU system may bring an application to a crawl, whereas a 128-CPU system may be barely affected. The CPU count must be considered. The number of CPUs and the overhead of the work performed can both vary by a single order of magnitude. Event frequency, however, can vary by several orders of magnitude, making it the biggest wildcard in trying to estimate overhead.

#### 18.1.1 Frequency

It helps to have some intuitive understanding of typical event rates. Table 18-1 includes a column where the maximum rate has been scaled to human-understandable terms: once per second becomes once per year. Imagine that you are subscribed to a mailing list that sends you email at this scaled rate.

| Event                  | Typical Frequency | Maximum Scaled | Tracing Overhead |
|------------------------|-------------------|----------------|------------------|
| Process execution       | 10 per second     | Monthly        | Negligible       |
| File opens             | 1050 per second   | Weekly         | Negligible       |
| Profiling at 100 Hz    | 100 per second    | Twice a week   | Negligible       |
| New TCP sessions       | 10500 per second  | Daily          | Negligible       |
| Disk I/O               | 101000 per second | Every eight hours | Negligible       |
| VFS calls              | 100010,000/s      | Hourly         | Measurable       |
| Syscalls               | 100050,000/s      | Every ten minutes | Significant      |
| Network packets        | 1000100,000/s     | Every five minutes | Significant      |
| Memory allocations     | 10,0001,000,000/s | Every thirty seconds | Expensive       |
| Locking events         | 50,0005,000,000/s | Every five seconds | Expensive       |
| Function calls         | Up to 100,000,000/s | Three times per second | Extreme       |
| CPU instructions       | Up to 1,000,000,000+ | Thirty times per second | Extreme       |
| CPU cycles             | Up to 3,000,000,000+ | Ninety times per second | Extreme       |

Throughout this book, I have described the overhead for BPF tools, sometimes with measurements but often with the terms negligible, measurable, significant, and expensive. These terms were chosen to provide a clear and intuitive understanding of the overhead involved.