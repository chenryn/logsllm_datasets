Based on the observation we made on Fig. 5, we form two groups of users
(for the top 1%) that we term good or bad. The viewers with less than 20%
CLR are categorised as good viewers while viewers with more than 60% CLR
values are categorised as bad viewers. The rationale behind this approach is to
uncover key features of clients that can lead to small and large CLR so as to
isolate ill-behaving clients and improve the V2C eﬃciency.
H1 Hypothesis. The ﬁrst hypothesis states that the neighbour set size of the
viewers should aﬀect the CLR. Figure 7 presents the CDF of the peer set size
of good and bad viewers. We can observe that bad peers tend to have smaller
peer set size than good peers. While this could hint towards the fact that bad
peers have more diﬃculties to establish links with other viewers, we believe that
the actual session times play a key role, as the longer the session, the more
likely a peer is to establish more connections. This is indeed the case here as
bad peers have an average session time of 22 min while it is 160 min for the good
peers. We however also found that the correlation coeﬃcient between neighbour
set size and CLR is only 0.05 and 0.07 for good and bad peers respectively.
Thus although we observe distinct distributions for good and bad viewers, the
neighbour set size does not seem to have any direct correlation with the CLR.
H2 Hypothesis. The second hypothesis is to check if the type of device aﬀects the
CLR. We have two families of devices: desktop devices and mobile devices. As a
mobile (resp. desktop) device can send to a desktop or mobile device, we have 4
possible combinations to consider. We plotted the distributions of CLR for the
good and bad viewers for all the four combinations in Figs. 8 and 9 respectively.
For the good users, the type of device does not seem to play a signiﬁcant role1.
For the bad viewers, we have very few cases of desktop senders, which is under-
standable as the worse network conditions are likely to be experienced on mobile
devices. This hints towards putting the blame on the user access link that we
investigate further with hypothesis H3.
H3 Hypothesis. We now investigate the impact on the CLR of the access link
characteristics of the users that we indirectly estimate based on the bandwidth
achieved during transfers with CDN servers and other viewers. From Fig. 10,
we observe that 50% of the bad viewers have just 10Mbps of CDN bandwidth
whereas 50% of the good viewers have about 25Mbps of CDN bandwidth. The
coeﬃcients of correlation between CLR and CDN bandwidth for the good viewers
and bad viewers are −0.45 and −0.4 respectively.
1 Note that the good users in Fig. 8 can experience CLR higher than 20% for some
categories, as the threshold of 20% applies to the average CLR and not per category.
A Data-Driven Analysis and Tuning
135
Looking at the V2V download rates should enable to estimate the uplink of
the users as it is likely to be the bottleneck of the path. From Fig. 11, we clearly
see that the V2V downloading rate of good viewers is far better than the one of
bad viewers. It thus appears that a key factor that explains the observed CLR is
the uplink capacity of the peers. In the next section, we leverage this information
to devise a simple algorithm, that can be applied independently at each viewer
and helps reducing the CLR.
Fig. 8. CLR of good viewers
Fig. 9. CLR of bad viewers
Fig. 10. CDN bandwidth distribution
Fig. 11. V2V bandwidth distribution
5 CLR Mitigation Algorithm
Our objective is to achieve a trade-oﬀ between CLR reduction and a decrease
of V2V traﬃc. Indeed, a simple but not cost eﬀective way to reduce the CLR is
to favor CDN transfers at the expense of V2V transfers. Results of the previous
section have uncovered that a key (even though probably not the only one)
explanation behind high CLRs is the weakness of the uplink capacity of peers.
We thus devised a simple approach that allows viewers to identify themselves
as good or bad viewers by monitoring their chunk upload success rate. The
algorithm checks every second the CLR, and if it goes above a threshold of
th%, the viewer stops sending the so-called downloaded control messages, which
indicate to its neighbors that it has a new available chunk. As the viewers won’t
send a downloaded message, they will not receive a request for that resource,
which will reduce their lost data rate. Note that viewers can still request and
136
I. Sarkar et al.
Fig. 12. Algorithm
Fig. 13. Second 3-day period
receive chunks in V2V mode from other viewers. This is motivated by the fact
that the access links tend to be asymmetric with more download than upload
capacity.
The algorithm (Fig. 12) implements a backoﬀ strategy where the viewer alter-
nates between full V2V (receiving and sending) and partial V2V (only receiv-
ing) mode to account for possible channel variations or varying congestion in
the network. The ﬁrst time the threshold th is reached, the viewer stops sending
downloaded messages for 100 min and then starts again monitoring the CLR
every second for one minute afterwards. If a second consecutive period of CLR
over the threshold is observed, the viewer stops sending downloaded messages
for 101 min and so on (i consecutive events lead to a period of 10i minutes long
silence period). In between silence periods, the test periods, where the viewer is
allowed again to upload, last one minute.
In the next section, we report on tests performed with our CLR mitigation
algorithm on a test-bed and in production in the live channel used in Sect. 3.
6 Evaluation
We evaluate our CLR mitigation algorithm ﬁrst in a controlled environment
which features 60 viewers and second in our production environment. While
modest in size, the controlled environment is useful as it enables to : (i) perform
functional tests as the client code in the same as the one in production, (ii)
emulate a variety of client network conditions by tuning the upload and download
rate of clients, even though we cannot reproduce the full diversity of network
conditions observed in the wild and (iii) perform reproducible tests, which is
unfeasible in the wild.
6.1 Test-Bed Results
A Data-Driven Analysis and Tuning
137
Our test-bed was deployed on 4 physical servers on the Grid’5000 experimental
platform [9] which uses KVM virtualisation. Each server hosts 4 virtual machines
with 15 viewers per VM, for a total of 60 unique viewers. The viewers are con-
nected to a forked version of the channel presented in Sect. 3, where they operate
in isolation, i.e. they can only contact the CDN server and the local viewers.
We relied on Linux namespaces to create isolated viewers. The download
capacity of each virtual node is around 325 Mb/s. Each experiment lasts 40 min.
To emulate bad viewers, we capped their upload capacity, using the Netem mod-
ule of Linux, to 3 Mb/s, a value smaller than the smallest bitrate, corresponding
to smallest video quality of the channel. In contrast, we impose no constraints
on their uplink. We created three diﬀerent scenarios: (i) Scen. 1: 15 bad viewers
and 45 good viewers, (ii) Scen. 2: 30 bad viewers and 30 good viewers and (iii)
Scen. 3: 45 bad viewers and 15 good viewers.
Table 1 reports the fraction of chunks downloaded from the CDN or in V2V
mode as well as the CLR for the three scenarios with the CLR mitigation algo-
rithm on and oﬀ. Clearly, the V2V eﬃciency is not aﬀected (it even increases)
when the algorithm is turned on while the CLR signiﬁcantly decreases. The CLR
does not reach 0 as when the bad peers are in their test periods (in between
silence periods) they can be picked as candidates by the good peers.
Table 1. CDN V2V and LCR rate For V2V protocol with and without algorithm
No Algorithm
Algorithm
CDN% V2V% LCR CDN% V2V% LCR
Scen. 1 36.7
63.3
7.85 36.37
63.3
Scen. 2 46.72
53.28
13.52 55.78
Scen. 3 66.4
33.6
38.7
53.51
64.21
46.49
3.38
5.58
7.75
6.2 Results in the Wild
We now present the result of a 3-day evaluation for the same channel as in
Sect. 3 where the CLR mitigation algorithm is deployed. Figure 13 represents
the evolution of aggregated traﬃc over the three days. We used a conservative
approach and used a threshold th = 80% for this experiment, as we test on an
operational channel.
The three days picked for the initial analysis in Sect. 3 were in fact chosen so
as to oﬀer a similar proﬁle (with at least one major event) as the period where
the algorithm was deployed. This enables to compare the two sets of days, even
if we can not guarantee reproducibility due to the nature of the experiment.
We ﬁrst focus on the V2V eﬃciency which is the most important factor
for the broadcaster. We want the algorithm to reduce the CLR but not the
138
I. Sarkar et al.
V2V eﬃciency as far as possible. The aggregated V2V eﬃciency for the days
without the CLR mitigation algorithm is 28.98% whereas it is 30.61% when
it is turned on. The scale of the events does aﬀect the V2V % for both the
algorithms. For a small (resp. large) scale event where the total data download
remains less than 1.5 TB (over 6TB), the V2V protocol without mitigation
algorithm has 32.5% (resp. 27.47%) of V2V eﬃciency whereas the V2V protocol
with algorithm features an eﬃciency of 28.9% (resp. 32.55%). This suggests that
when the protocol has enough viewers with good download capacity, there is no
big performance impact on V2V eﬃciency. Even in the case of less viewers, the
V2V eﬃciency percentage is reduced by only 4%.
The second metric we consider is the CLR. The overall (over the three days)
CLR without the algorithm was 24.7% whereas it fell to 13.0% when the algo-
rithm was turned on. Thus overall, the algorithm reduced the CLR by almost a
factor of 2.
We further compared the distributions of the CLR for good viewers and bad
viewers, using the same deﬁnition as in Sect. 3, for the two periods of 3 days in
Figs. 14 and 15 respectively. We clearly observe the positive impact of the CLR
mitigation algorithm on both the good and bad peers with more mass on the
smaller CLR values, e.g. almost 22% of the good viewers do not loose any data
at all.
As explained in the introduction, the library operations are transparent to
the video player. One can however question if our CLR mitigation algorithm can
adversely impact the video player by indirectly inﬂuencing the video quality level
it picks. As a preliminary assessment of the interplay between the library and the
player, we report in Table 2 the fraction of sessions at each quality level observed,
per day, for the two periods of interest for the top 1% of viewers. We observe no
noticeable diﬀerence in the distributions of client sessions at each quality level
for the two periods, which suggests that the CLR mitigation algorithm has no
collateral eﬀect.
Table 2. Video quality levels distribution (top 1% viewers)
Period 1 (no algo.)
Period 2 (algo.)
Low Q. % Medium Q. % High Q. % Low Q. % Medium Q. % High Q. %
Day 1 27
Day 2 33
Day 3 30
30
25
35
44
42
35
33
30
28
29
25
34
38
45
38
A Data-Driven Analysis and Tuning
139
Fig. 14. CLR No mitigation alg.
Fig. 15. CLR With mitigation alg.
7 Conclusion and Future Work
In this work, we have presented an in-depth study of a live video channel operated
over the Internet using a hybrid CDN-V2V architecture. For such an architec-
ture, the main KPI is the fraction of chunks delivered in V2V mode. The chunk
loss rate (CLR) metric is another key factor. It indicates, when it reaches high
values, that some ineﬃciencies exist in the system design since some chunks are
sent but not delivered (before the deadline) to the viewers that requested them.
We have followed a data driven approach to proﬁle the clients and relate the
observed CLRs to other parameters related to the neighborhood characteristics,
the type of clients (mobile or ﬁxed) or the access link characteristics. The latter
is inferred indirectly using the throughput samples obtained when downloading
from the CDN or uploading to other peers. We demonstrated that, in a number
of cases, the blame was to put on the access links of some of the viewers. We
devised a mitigation algorithm that requires no cooperation between clients as
each client individually assesses its uplink capacity and decides if it acts as
server for the other peers or simply downloads in V2V mode. We demonstrated
the eﬀectiveness of the approach in a controlled testbed and then in the wild,
with observed gains close to 50% with a negligible impact on the V2V eﬃciency.
As our library is independent from the actual viewer, and simply acts as a proxy
between the CDN server and the video player by re-routing requests for the
content to other viewers if possible, our study provides a way to optimise any
similar hybrid V2V architecture.
The next steps for us will be to devise an adaptive version of our CLR
mitigation algorithm and test at a larger scale on the set of channels operated
by our hybrid CDN-P2P live delivery system. We also want to study in more
detail the relation between our QoS metrics at the library level and the classical
QoE metrics used at the video player level.
References
1. Cohen, B.: Incentives build robustness in BitTorrent. In: Workshop on Economics
of Peer-to-Peer systems, vol. 6 (2003)
2. Alex Bybyk 2(5) (2020). https://restream.io/blog/live-streaming-statistics/.
Accessed 18 Oct 2020
140
I. Sarkar et al.
3. Bergkvist, A., Burnett, D.C., Jennings,C., Narayanan, A., Aboba, B.: Webrtc 1.0:
Real-time commu- nication between browsers. Working draft, W3C (2012)
4. Sarkar, I., Rouibia, S., Pacheco, D.L., Urvoy-Keller, G.: Proactive Information Dis-
semination in WebRTC-based Live Video Distribution. In: IWCMC, pp. 304–309
(2020)
5. Bruneau-Queyreix, J., Lacaud, M., N´egru, D.: Increasing End-User’s QoE with a
Hybrid P2P/Multi-Server streaming solution based on dash.js and webRTC (2017).
ﬀhal-01585219f
6. Rhinow, F., Veloso, P.P., Puyelo, C., Barrett, S., Nuallain, E.O.: P2P live video
streaming in WebRTC. In: World Congress on Computer Applications and Infor-
mation Systems (WCCAIS), Hammamet, vol. 2014, pp. 1–6 (2014). https://doi.org/
10.1109/WCCAIS.2014.6916588
7. Hei, X., Liang, C., Liang, J., Liu, Y., Ross, K.: A measurement study of a large-scale
P2P IPTV system. IEEE Trans. Multimedia 9, 1672–1687 (2008). https://doi.org/
10.1109/TMM.2007.907451
8. Silverston, T., Jakab, L., Cabellos-Aparicio, A., Fourmaux, O., Salamatian, K.,
et al.: Large-scale Measurement Experiments of P2P-TV Systems Insights on
Fairness and Locality. Signal Process. Image Commun. 26(7), 327–338 (2011).
ﬀ10.1016/j.image.2011.01.007ﬀ. ﬀhal-00648019f
9. Bolze, R., et al.: Grid’5000: a large scale and highly reconﬁgurable experimental
grid testbed. Int. J. High Perform. Comput. Appl. 20(4), 481–494 (2006)