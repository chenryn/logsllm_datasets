### Observations and Categorization of Users

Based on the observations from Figure 5, we categorized users into two groups (for the top 1%): "good" and "bad" viewers. Viewers with a Chunk Loss Rate (CLR) below 20% are classified as good viewers, while those with a CLR above 60% are classified as bad viewers. The rationale behind this categorization is to identify key features that contribute to low and high CLR, thereby isolating problematic clients and improving the V2C (Viewer-to-Content) efficiency.

### Hypothesis H1: Impact of Neighbor Set Size on CLR

**Hypothesis Statement:** The first hypothesis posits that the size of the neighbor set (peer set) of viewers affects the CLR. 

**Analysis:**
- **Figure 7** shows the Cumulative Distribution Function (CDF) of the peer set size for good and bad viewers.
- It is observed that bad peers tend to have smaller peer set sizes compared to good peers.
- While this might suggest that bad peers have more difficulty establishing connections, the actual session times play a significant role. Longer sessions allow peers to establish more connections.
- On average, bad peers have a session time of 22 minutes, whereas good peers have an average session time of 160 minutes.
- However, the correlation coefficients between neighbor set size and CLR are only 0.05 and 0.07 for good and bad peers, respectively. This indicates that, although there are distinct distributions, the neighbor set size does not have a direct correlation with the CLR.

### Hypothesis H2: Impact of Device Type on CLR

**Hypothesis Statement:** The second hypothesis examines whether the type of device (desktop or mobile) affects the CLR.

**Analysis:**
- We considered four possible combinations: desktop-to-desktop, desktop-to-mobile, mobile-to-desktop, and mobile-to-mobile.
- **Figures 8 and 9** show the CLR distributions for good and bad viewers, respectively.
- For good users, the type of device does not significantly affect the CLR.
- For bad viewers, there are very few cases of desktop senders, which is understandable given that worse network conditions are more likely to be experienced on mobile devices. This suggests that the user access link is a critical factor, which we further investigate in Hypothesis H3.

### Hypothesis H3: Impact of Access Link Characteristics on CLR

**Hypothesis Statement:** The third hypothesis investigates the impact of the access link characteristics on the CLR, estimated based on the bandwidth achieved during transfers with CDN servers and other viewers.

**Analysis:**
- **Figure 10** shows that 50% of bad viewers have a CDN bandwidth of just 10 Mbps, while 50% of good viewers have about 25 Mbps of CDN bandwidth.
- The correlation coefficients between CLR and CDN bandwidth are -0.45 and -0.4 for good and bad viewers, respectively.
- **Figure 11** indicates that the V2V (Viewer-to-Viewer) download rates for good viewers are significantly better than those for bad viewers. This suggests that the uplink capacity of the peers is a key factor in explaining the observed CLR.

### CLR Mitigation Algorithm

**Objective:** To achieve a balance between reducing the CLR and minimizing V2V traffic. A simple but not cost-effective method to reduce CLR is to favor CDN transfers over V2V transfers. Our analysis revealed that a primary cause of high CLRs is the weak uplink capacity of peers.

**Algorithm Description:**
- Viewers monitor their chunk upload success rate to self-identify as good or bad viewers.
- The algorithm checks the CLR every second. If it exceeds a threshold (th%), the viewer stops sending downloaded control messages, indicating to its neighbors that it has a new available chunk.
- This reduces the lost data rate, as viewers will not receive requests for that resource.
- The algorithm implements a backoff strategy, alternating between full V2V mode (receiving and sending) and partial V2V mode (only receiving) to account for channel variations and network congestion.
- The first time the threshold is reached, the viewer stops sending downloaded messages for 100 minutes, then resumes monitoring for one minute. If the CLR remains above the threshold, the viewer stops sending messages for 101 minutes, and so on.

### Evaluation

**Test-Bed Results:**
- We evaluated the CLR mitigation algorithm in a controlled environment with 60 viewers, deployed on 4 physical servers using KVM virtualization.
- Three scenarios were created: 15 bad viewers and 45 good viewers, 30 bad viewers and 30 good viewers, and 45 bad viewers and 15 good viewers.
- **Table 1** shows that the V2V efficiency is not affected (and even increases) when the algorithm is turned on, while the CLR significantly decreases.

**Results in the Wild:**
- We conducted a 3-day evaluation on a live channel, using a conservative threshold of th = 80%.
- The aggregated V2V efficiency without the algorithm was 28.98%, increasing to 30.61% with the algorithm.
- The overall CLR without the algorithm was 24.7%, falling to 13.0% with the algorithm.
- **Figures 14 and 15** show the positive impact of the CLR mitigation algorithm on both good and bad viewers, with more mass on the smaller CLR values.

### Conclusion and Future Work

In this study, we analyzed a live video channel using a hybrid CDN-V2V architecture, focusing on the fraction of chunks delivered in V2V mode and the CLR. We identified key factors contributing to high CLR, such as the uplink capacity of peers, and developed a mitigation algorithm to address these issues. The algorithm was effective in reducing CLR by almost 50% with negligible impact on V2V efficiency. Future work will include developing an adaptive version of the algorithm and testing it on a larger scale, as well as studying the relationship between our QoS metrics and classical QoE metrics at the video player level.