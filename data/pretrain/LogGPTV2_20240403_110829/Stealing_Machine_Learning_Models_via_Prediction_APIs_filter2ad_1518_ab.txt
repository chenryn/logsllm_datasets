b
-
e
t
i
e
z
i
t
e
n
h
o
W
M




 


e
c
n
e
d
s
e
fi
r
n
o
o
c
C
S





n
o
i
c
s
i
s
t
e
s
r
i
g
g
o
e
R
L





k
r
l
o
a
w
r
u
t
e
e
N
N





n
o
i
s
e
i
c
e
e
r
D
T





M
V
S





Service
Amazon [1]
Microsoft [38]
BigML [11]
PredictionIO [43] 
Google [25]

Table 2: Particularities of major MLaaS providers. ‘White-box’
refers to the ability to download and use a trained model locally, and
‘Monetize’ means that a user may charge other users for black-box
access to her models. Model support for each service is obtained from
available documentation. The models listed for Google’s API are a pro-
jection based on the announced support of models in standard PMML
format [25]. Details on ML models are given in Appendix A.
model by either choosing one of many supported model
classes (as in BigML, Microsoft, and PredictionIO) or
having the service choose an appropriate model class (as
in Amazon and Google). Two services have also an-
nounced upcoming support for users to upload their own
trained models (Google) and their own custom learning
algorithms (PredictionIO). When training a model, users
may tune various parameters of the model or training-
algorithm (e.g., regularizers, tree size, learning rates) and
control feature-extraction and transformation methods.
For black-box models, the service provides users with
information needed to create and interpret predictions,
such as the list of input features and their types. Some
services also supply the model class, chosen training pa-
rameters, and training data statistics (e.g., BigML gives
the range, mean, and standard deviation of each feature).
To get a prediction from a model, a user sends one
or more input queries. The services we reviewed accept
both synchronous requests and asynchronous ‘batch’ re-
quests for multiple predictions. We further found vary-
ing degrees of support for ‘incomplete’ queries, in which
some input features are left unspecified [46]. We will
show that exploiting incomplete queries can drastically
improve the success of some of our attacks. Apart from
PredictionIO, all of the services we examined respond to
prediction queries with not only class labels, but a variety
of additional information, including conﬁdence scores
(typically class probabilities) for the predicted outputs.
Google and BigML allow model owners to mone-
tize their models by charging other users for predictions.
Google sets a minimum price of $0.50 per 1,000 queries.
On BigML, 1,000 queries consume at least 100 credits,
costing $0.10–$5, depending on the user’s subscription.
Attack scenarios. We now describe possible motiva-
tions for adversaries to perform model extraction attacks.
We then present a more detailed threat model informed
by characteristics of the aforementioned ML services.
Avoiding query charges. Successful monetization of
prediction queries by the owner of an ML model f re-
quires confidentiality of f . A malicious user may seek to
launch what we call a cross-user model extraction attack,
stealing f for subsequent free use. More subtly, in black-
box-only settings (e.g., Google and Amazon), a service’s
business model may involve amortizing up-front training
costs by charging users for future predictions. A model
extraction attack will undermine the provider’s business
model if a malicious user pays less for training and ex-
tracting than for paying per-query charges.
Violating training-data privacy. Model extraction
could, in turn, leak information about sensitive training
data. Prior attacks such as model inversion [4, 23, 24]
have shown that access to a model can be abused to infer
information about training set points. Many of these at-
tacks work better in white-box settings; model extraction
may thus be a stepping stone to such privacy-abusing at-
tacks. Looking ahead, we will see that in some cases,
significant information about training data is leaked triv-
ially by successful model extraction, because the model
itself directly incorporates training set points.
Stepping stone to evasion. In settings where an ML
model serves to detect adversarial behavior, such as iden-
tification of spam, malware classification, and network
anomaly detection, model extraction can facilitate eva-
sion attacks. An adversary may use knowledge of the
ML model to avoid detection by it [4, 9, 29, 36, 55].
In all of these settings, there is an inherent assumption
of secrecy of the ML model in use. We show that this
assumption is broken for all ML APIs that we investigate.
Threat model in detail. Two distinct adversarial mod-
els arise in practice. An adversary may be able to make
direct queries, providing an arbitrary input x to a model f
and obtaining the output f (x). Or the adversary may be
able to make only indirect queries, i.e., queries on points
in input space M yielding outputs f (ex(M)). The feature
extraction mechanism ex may be unknown to the adver-
sary. In Section 5, we show how ML APIs can further
be exploited to “learn” feature extraction mechanisms.
Both direct and indirect access to f arise in ML services.
(Direct query interfaces arise when clients are expected
to perform feature extraction locally.) In either case, the
output value can be a class label, a confidence value vec-
tor, or some data structure revealing various levels of in-
formation, depending on the exposed API.
We model the adversary, denoted by A, as a random-
ized algorithm. The adversary’s goal is to use as few
queries as possible to f in order to efficiently compute
an approximation ˆf that closely matches f . We formalize
“closely matching” using two different error measures:
• Test error Rtest: This is the average error over a test set
D, given by Rtest( f , ˆf ) = ∑(x,y)∈D d( f (x), ˆf (x))/|D|.
604  25th USENIX Security Symposium 
USENIX Association
A low test error implies that ˆf matches f well for in-
puts distributed like the training data samples. 2
• Uniform error Runif: For a set U of vectors uniformly
chosen in X , let Runif( f , ˆf ) = ∑x∈U d( f (x), ˆf (x))/|U|.
Thus Runif estimates the fraction of the full feature
space on which f and ˆf disagree. (In our experiments,
we found |U| = 10,000 was sufficiently large to obtain
stable error estimates for the models we analyzed.)
We define the extraction accuracy under test and uni-
form error as 1− Rtest( f , ˆf ) and 1− Runif( f , ˆf ). Here we
implicitly refer to accuracy under 0-1 distance. When as-
sessing how close the class probabilities output by ˆf are
to those of f (with the total-variation distance) we use
the notations RTV
test( f , ˆf ) and RTV
unif( f , ˆf ).
An adversary may know any of a number of pieces
of information about a target f : What training algorithm
T generated f , the hyper-parameters used with T , the
feature extraction function ex, etc. We will investigate a
variety of settings in this work corresponding to different
APIs seen in practice. We assume that A has no more
information about a model’s training data, than what is
provided by an ML API (e.g., summary statistics). For
simplicity, we focus on proper model extraction: If A
believes that f belongs to some model class, then A’s
goal is to extract a model ˆf from the same class. We
discuss some intuition in favor of proper extraction in
Appendix D, and leave a broader treatment of improper
extraction strategies as an interesting open problem.
4 Extraction with Confidence Values
We begin our study of extraction attacks by focusing on
prediction APIs that return confidence values. As per
Section 2, the output of a query to f thus falls in a range
[0,1]c where c is the number of classes. To motivate this,
we recall that most ML APIs reveal confidence values
for models that support them (see Table 2). This includes
logistic regressions (LR), neural networks, and decision
trees, defined formally in Appendix A. We first introduce
a generic equation-solving attack that applies to all logis-
tic models (LR and neural networks). In Section 4.2, we
present two novel path-ﬁnding attacks on decision trees.
4.1 Equation-Solving Attacks
Many ML models we consider directly compute class
probabilities as a continuous function of the input x and
real-valued model parameters. In this case, an API that
reveals these class probabilities provides an adversary A
with samples (x, f (x)) that can be viewed as equations
in the unknown model parameters. For a large class of
2Note that for some D, it is possible that ˆf predicts true labels better
than f , yet Rtest( f , ˆf ) is large, because ˆf does not closely match f .
Synthetic
Yes
Yes
Yes
Yes
No
No
No
No
No
No
No
No
No
# records
5,000
5,000
5,000
1,000
48,842
48,842
150
331
16,127
1,797
683
8,124
768
# classes # features
Data set
2
Circles
2
Moons
2
Blobs
20
5-Class
108
Adult (Income)
105
Adult (Race)
4
Iris
40
Steak Survey
101
GSS Survey
64
Digits
10
Breast Cancer
112
Mushrooms
8
Diabetes
Table 3: Data sets used for extraction attacks. We train two models on the
Adult data, with targets ‘Income’ and ‘Race’. SVMs and binary logistic regres-
sions are trained on data sets with 2 classes. Multiclass regressions and neural
networks are trained on multiclass data sets. For decision trees, we use a set of
public models shown in Table 5.
2
2
3
5
2
5
3
5
3
10
2
2