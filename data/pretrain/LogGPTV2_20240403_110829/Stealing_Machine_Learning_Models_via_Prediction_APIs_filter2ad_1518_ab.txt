### Particularities of Major MLaaS Providers

| Service | White-box | Monetize |
|---------|-----------|----------|
| Amazon [1] | No | No |
| Microsoft [38] | Yes | No |
| BigML [11] | Yes | Yes |
| PredictionIO [43] | Yes | No |
| Google [25] | Upcoming | Yes |

**Table 2: Particularities of major MLaaS providers.**  
- **White-box**: Refers to the ability to download and use a trained model locally.
- **Monetize**: Indicates that a user may charge other users for black-box access to their models.
- **Model Support**: Obtained from available documentation. The models listed for Google’s API are a projection based on the announced support of models in standard PMML format [25]. Details on ML models are given in Appendix A.

### Model Training and Customization

Users can train a model by either choosing from many supported model classes (as in BigML, Microsoft, and PredictionIO) or having the service choose an appropriate model class (as in Amazon and Google). Some services, such as Google and PredictionIO, have announced upcoming support for users to upload their own trained models and custom learning algorithms. During training, users can tune various parameters of the model or training algorithm (e.g., regularizers, tree size, learning rates) and control feature extraction and transformation methods.

For black-box models, the service provides users with the necessary information to create and interpret predictions, such as the list of input features and their types. Some services also provide additional details like the model class, chosen training parameters, and training data statistics (e.g., BigML gives the range, mean, and standard deviation of each feature).

To get a prediction from a model, a user sends one or more input queries. The services we reviewed accept both synchronous requests and asynchronous 'batch' requests for multiple predictions. We found varying degrees of support for 'incomplete' queries, where some input features are left unspecified [46]. Exploiting incomplete queries can significantly improve the success of some of our attacks. Apart from PredictionIO, all of the services we examined respond to prediction queries with not only class labels but also additional information, including confidence scores (typically class probabilities) for the predicted outputs.

### Monetization of Models

Google and BigML allow model owners to monetize their models by charging other users for predictions. Google sets a minimum price of $0.50 per 1,000 queries. On BigML, 1,000 queries consume at least 100 credits, costing $0.10–$5, depending on the user’s subscription.

### Attack Scenarios

#### Motivations for Adversaries

1. **Avoiding Query Charges**:
   - Successful monetization of prediction queries by the owner of an ML model \( f \) requires confidentiality of \( f \).
   - A malicious user may launch a cross-user model extraction attack to steal \( f \) for subsequent free use.
   - In black-box-only settings (e.g., Google and Amazon), a service’s business model may involve amortizing up-front training costs by charging users for future predictions. A model extraction attack will undermine the provider’s business model if a malicious user pays less for training and extracting than for paying per-query charges.

2. **Violating Training-Data Privacy**:
   - Model extraction could leak information about sensitive training data. Prior attacks such as model inversion [4, 23, 24] have shown that access to a model can be abused to infer information about training set points.
   - Many of these attacks work better in white-box settings; model extraction may thus be a stepping stone to such privacy-abusing attacks.
   - In some cases, significant information about training data is leaked trivially by successful model extraction, because the model itself directly incorporates training set points.

3. **Stepping Stone to Evasion**:
   - In settings where an ML model serves to detect adversarial behavior (e.g., spam identification, malware classification, and network anomaly detection), model extraction can facilitate evasion attacks.
   - An adversary may use knowledge of the ML model to avoid detection by it [4, 9, 29, 36, 55].

In all these settings, there is an inherent assumption of secrecy of the ML model in use. We show that this assumption is broken for all ML APIs that we investigate.

### Threat Model in Detail

Two distinct adversarial models arise in practice:

1. **Direct Queries**:
   - An adversary can make direct queries, providing an arbitrary input \( x \) to a model \( f \) and obtaining the output \( f(x) \).

2. **Indirect Queries**:
   - An adversary can make only indirect queries, i.e., queries on points in input space \( M \) yielding outputs \( f(ex(M)) \). The feature extraction mechanism \( ex \) may be unknown to the adversary.
   - In Section 5, we show how ML APIs can further be exploited to "learn" feature extraction mechanisms.

Both direct and indirect access to \( f \) arise in ML services. (Direct query interfaces arise when clients are expected to perform feature extraction locally.) In either case, the output value can be a class label, a confidence value vector, or some data structure revealing various levels of information, depending on the exposed API.

We model the adversary, denoted by \( A \), as a randomized algorithm. The adversary’s goal is to use as few queries as possible to \( f \) in order to efficiently compute an approximation \( \hat{f} \) that closely matches \( f \). We formalize "closely matching" using two different error measures:

- **Test Error \( R_{\text{test}} \)**: This is the average error over a test set \( D \), given by \( R_{\text{test}}(f, \hat{f}) = \frac{\sum_{(x,y) \in D} d(f(x), \hat{f}(x))}{|D|} \).
  - A low test error implies that \( \hat{f} \) matches \( f \) well for inputs distributed like the training data samples.

- **Uniform Error \( R_{\text{unif}} \)**: For a set \( U \) of vectors uniformly chosen in \( X \), let \( R_{\text{unif}}(f, \hat{f}) = \frac{\sum_{x \in U} d(f(x), \hat{f}(x))}{|U|} \).
  - \( R_{\text{unif}} \) estimates the fraction of the full feature space on which \( f \) and \( \hat{f} \) disagree. (In our experiments, we found \( |U| = 10,000 \) was sufficiently large to obtain stable error estimates for the models we analyzed.)

We define the extraction accuracy under test and uniform error as \( 1 - R_{\text{test}}(f, \hat{f}) \) and \( 1 - R_{\text{unif}}(f, \hat{f}) \). Here we implicitly refer to accuracy under 0-1 distance. When assessing how close the class probabilities output by \( \hat{f} \) are to those of \( f \) (with the total-variation distance), we use the notations \( R_{\text{TV, test}}(f, \hat{f}) \) and \( R_{\text{TV, unif}}(f, \hat{f}) \).

An adversary may know any of a number of pieces of information about a target \( f \): What training algorithm \( T \) generated \( f \), the hyper-parameters used with \( T \), the feature extraction function \( ex \), etc. We will investigate a variety of settings in this work corresponding to different APIs seen in practice. We assume that \( A \) has no more information about a model’s training data than what is provided by an ML API (e.g., summary statistics). For simplicity, we focus on proper model extraction: If \( A \) believes that \( f \) belongs to some model class, then \( A \)'s goal is to extract a model \( \hat{f} \) from the same class. We discuss some intuition in favor of proper extraction in Appendix D, and leave a broader treatment of improper extraction strategies as an interesting open problem.

### Extraction with Confidence Values

We begin our study of extraction attacks by focusing on prediction APIs that return confidence values. As per Section 2, the output of a query to \( f \) falls in a range \([0,1]^c\) where \( c \) is the number of classes. To motivate this, we recall that most ML APIs reveal confidence values for models that support them (see Table 2). This includes logistic regressions (LR), neural networks, and decision trees, defined formally in Appendix A. We first introduce a generic equation-solving attack that applies to all logistic models (LR and neural networks). In Section 4.2, we present two novel path-finding attacks on decision trees.

#### Equation-Solving Attacks

Many ML models we consider directly compute class probabilities as a continuous function of the input \( x \) and real-valued model parameters. In this case, an API that reveals these class probabilities provides an adversary \( A \) with samples \((x, f(x))\) that can be viewed as equations in the unknown model parameters. For a large class of models, this approach can be highly effective.

### Data Sets Used for Extraction Attacks

| Data Set | Synthetic | # Records | # Classes | # Features |
|----------|-----------|-----------|-----------|------------|
| Circles | Yes | 5,000 | 2 | 2 |
| Moons | Yes | 5,000 | 2 | 2 |
| Blobs | Yes | 5,000 | 3 | 2 |
| 5-Class | Yes | 1,000 | 5 | 2 |
| Adult (Income) | No | 48,842 | 2 | 108 |
| Adult (Race) | No | 48,842 | 10 | 105 |
| Iris | No | 150 | 3 | 4 |
| Steak Survey | No | 331 | 5 | 40 |
| GSS Survey | No | 16,127 | 101 | 101 |
| Digits | No | 1,797 | 10 | 64 |
| Breast Cancer | No | 683 | 2 | 8 |
| Mushrooms | No | 8,124 | 2 | 112 |
| Diabetes | No | 768 | 2 | 8 |

**Table 3: Data sets used for extraction attacks.**  
- We train two models on the Adult data, with targets 'Income' and 'Race'.
- SVMs and binary logistic regressions are trained on data sets with 2 classes.
- Multiclass regressions and neural networks are trained on multiclass data sets.
- For decision trees, we use a set of public models shown in Table 5.