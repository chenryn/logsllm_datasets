title:POSTER: Detecting Suspicious Processes from Log-Data via a Bayesian
Block Model
author:Daniel Andrade and
Yusuke Takahashi and
Daichi Hasumi
POSTER: Detecting Suspicious Processes from Log-Data via a
Bayesian Block Model
Daniel Andrade
Yusuke Takahashi
Daichi Hasumi
Data Science Research Laboratories,
Security Research Laboratories, NEC
Security Research Laboratories, NEC
PI:EMAIL
PI:EMAIL
NEC
Kawasaki, 211-8666, Japan
PI:EMAIL
ABSTRACT
Analyzing the behavior of an attacker is critical for determining the
scope of damage of a cyber attack, recovering, and fixing system
vulnerabilities. However, finding all attacker’s traces from log data
is a laborsome task, where the performance of existing machine
learning methods is still insufficient. In this work, we focus on the
task of detecting all processes that were executed by the attacker.
For this task, standard anomaly detection methods like Isolation
Forest, perform poorly, due to many processes that are used by
both the attacker and the client user. Therefore, we propose to
incorporate prior knowledge about the temporal concentration of
the attacker’s activity. In general, we expect that an attacker is active
only during a relatively small time window (block assumption),
rather than being active at completely random time points. We
propose a generative model that allows us to incorporate such prior
knowledge effectively. Experiments on intrusion log data, shows
that the proposed method achieves considerably better detection
performance than a strong baseline method which also incorporates
the block assumption.
CCS CONCEPTS
• Security and privacy → Intrusion detection systems; • Com-
puting methodologies → Anomaly detection; • Mathematics
of computing → Bayesian computation.
KEYWORDS
forensic data analysis; generative model; dataset creation; block
anomaly detection
ACM Reference Format:
Daniel Andrade, Yusuke Takahashi, and Daichi Hasumi. 2020. POSTER:
Detecting Suspicious Processes from Log-Data via a Bayesian Block Model.
In Proceedings of the 15th ACM Asia Conference on Computer and Commu-
nications Security (ASIA CCS ’20), October 5–9, 2020, Taipei, Taiwan. ACM,
New York, NY, USA, 3 pages. https://doi.org/10.1145/3320269.3405445
1 INTRODUCTION
Analyzing the behavior of an attacker is critical for determining the
scope of damage of a cyber attack, recovering, and fixing system
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
ASIA CCS ’20, October 5–9, 2020, Taipei, Taiwan
© 2020 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-6750-9/20/10.
https://doi.org/10.1145/3320269.3405445
vulnerabilities. Since online attack detection is not always possible,
forensic analysis of log data is indispensable. However, finding all
attacker’s traces from log data is often a laborsome manual task in
practice [2].
Given a sequence of windows event logs, our goal is to distin-
guish user and attacker’s activities. Previous works on applying
machine learning techniques often use datasets as in [3], where
the availability of a clean training data set is assumed [1, 4]. That
means that data from the same client machine with and without
intrusion is available. Here, we consider the more realistic situation
that for a particular client machine, we do not know the start and
end of an attacker activity, thus no training data is available for the
same client at test time. This reflects the reality that often due to
limited capacity for saving log data, and uncertainty about the start
of the intrusion attack, we only have such mixed data, available.
Fully unsupervised methods like [6] and [5], score each activity
individually, and thus implicitly assume that attacker activities are
uniformly spread over time. This is clearly an unrealistic assump-
tion, since it is more likely that the attacker is active only during a
relatively small time window, which we call the block assumption,
rather than being active at completely random time points.
A natural way to model temporal structure is to use a state de-
pendent model, like Long Short Term Memory (LSTM) or hidden
Markov model (HMM), see e.g. [2, 4]. However, in such state de-
pendent models, it is difficult to model global properties like the
number of blocks. Other Bayesian methods for outlier detection
focus on change-point-detection methods (see e.g. [8]) which were
designed for continuous stochastic time processes and cannot be
applied directly to our setting.
Here, we propose a Bayesian block model (BBM), which incor-
porates the block assumption, and easily allows us to include prior
knowledge elicited from past cyber attacks. Furthermore, due the
BBM’s simplicity it is computationally feasible to calculate the
posterior distribution exactly.
We evaluate our approach on several user and intrusion event log
data which are based on real incidents. Furthermore, we compare
to Isolation Forest [7] and its extension that also exploits the block
assumption.
2 PROPOSED METHOD
The most critical feature to discriminate user and attacker processes
is the name of the process, for example, "Word" or "Powershell".1 In
similarity to language models in natural language processing, we
1In preliminary experiments, we found the process name to be the most useful for
discrimination among other features like parent process, duration, associated event
ids, and network traffic.
Poster Session ASIA CCS '20, October 5–9, 2020, Taipei, Taiwan922call the set of different process names the vocabulary. We model the
size of the vocabulary from the user and attacker as coming from a
Poisson distribution with mean parameter κu and κa, respectively.
If the vocabulary of the user and attacker is very different, the
detection of the attacker’s processes is trivial. Unfortunately, this
is not the case, as our analysis based on real data shows in Table
2. Nevertheless, we expect that the probability mass function of
processes used by the user and the attacker is different, which we
denote by θu and θ a, respectively.
Our proposed model assumes that the attacker is active only
during one time window [i, i + na], where i is the start of the intru-
sion, and na the number of intrusion processes. In other words, we
assume that processes in [i, i + na] follow a categorical distribution
with probabilities θ a, and otherwise with probabilities θu.
We assume discrete time events, in particular, we denote by [a, b]
the time index set {a, a + 1, a + 2, . . . , b}, where a ≤ b. The process
started at time index j ∈ [1, n] is denoted as wj. The sequence of
observed process names is denoted as w = (w1, w2, . . . , wn).
Our proposed generative model is summarized in the following:
n − na + 1 1n−na +1)
na ∼ Poisson[1,nmax](λa)
1
i ∼ Categorical(
ka ∼ Poisson[1,na](κa)
ma ∼ Binomial(ka, γnew)
ku ∼ Poisson[ka−ma ,n−na](κu)
θ a ∼ Dirichlet(1ka)
θu ∼ Dirichlet(1ku)
for process j ∈ {1, . . . , i − 1} ∪ {i + na, . . . , n}:
wj ∼ Categorical(θu)
for process j ∈ {i, . . . , i + na − 1}:
wj ∼ Categorical(θ a) ,
where 1c denotes the all one vector of dimension c, and Poisson[a,b](λ)
denotes the Poisson distribution with average parameter λ trun-
cated to be in the interval [a, b]. A summary of all parameters is
given in Table 1.
2.1 Inference
In order to find the probability that a process belongs to the attacker,
we need to calculate the posterior distribution p(i, na|w). We have2
p(i, na|w) ∝ p(i, na, w)
∫
=
p(w|θu, θ a, i, na) · p(θu|ku) · p(θ a|ka) · p(i, na)
· p(ka|na) · p(ma|ka) · p(ku|ka, ma, na) dθudθ a .
The integral has an analytic solution due to the conjugacy of the
Dirichlet and categorical distribution. However, in order to deter-
mine the normalization constant, we need to sum over all combi-
nations of na ∈ [1, nmax], and i ∈ [1, n − na + 1], which is in O(n2),
assuming nmax ∈ O(n).3
2Conditioning on the hyper-parameters is omitted for clarity. Furthermore, note that
p(w, i , na) = p(w, i , na , ka , ku , ma) holds.
3For our experiments, we set nmax = 0.1 · n.
Table 1: Summary of all parameters.
Hyper-parameters (estimated from training data)
expected number of attacker processes
expected number of different processes from user
expected number of different processes from attacker
expected ratio of process names used only by attacker
λa
κu
κa
γnew
nmax maximal number of possible attacker processes
Test log file - Observed parameters
n
wj
i
na
ku
ka
ma
θ a
θu
number of all process
name of the j-th process
Test log file - Unknown parameters (random variables)
index where intrusion starts
number of processes used by attacker
number of different process names of user
number of different process names of attacker
number of process names used only by attacker
probability vector of attacker’s process names.
probability vector of user’s process names.
3 EXPERIMENTS
For our experiments, we used windows event log data that were
collected from several user’s client machines across a global or-