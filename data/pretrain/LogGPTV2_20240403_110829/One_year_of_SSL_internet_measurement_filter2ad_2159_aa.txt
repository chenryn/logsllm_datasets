# One Year of SSL Internet Measurement

## Authors
- Olivier Levillain\*†
- Arnaud Ébalard\*
- Benjamin Morin\*
- Hervé Debar†

\*ANSSI, 51 Boulevard Latour Maubourg, Paris, France  
†Télécom Sud Paris, 9 Rue Charles Fourier, Evry, France

### Abstract
Over the years, SSL/TLS has become an essential part of internet security. As such, it should offer robust and state-of-the-art security, particularly for HTTPS, its primary application. Theoretically, the protocol allows a trade-off between secure algorithms and decent performance. However, in practice, servers do not always support the latest version of the protocol, nor do they all enforce strong cryptographic algorithms.

To assess the quality of HTTPS servers, we enumerated HTTPS servers on the internet in July 2010 and July 2011. We sent multiple stimuli to gather detailed information and then analyzed the collected data to observe how it evolved. We also focused on two subsets of TLS hosts: trusted hosts (possessing a valid certificate at the time of probing) and EV hosts (presenting a trusted, so-called Extended Validation certificate). Our contributions include the stimuli we sent, the criteria we studied, and the subsets we focused on.

Moreover, even though EV servers present improved certificate quality over other TLS hosts, they do not consistently offer high-quality sessions, which could and should be improved.

### Categories and Subject Descriptors
C.2.2 [Computer-Communication Networks]: Network Protocols; D.2.8 [Software Engineering]: Metrics

### Keywords
SSL/TLS, HTTPS, Internet measurement, Certificates, X.509

## 1. Introduction
SSL (Secure Sockets Layer) is a cryptographic protocol designed by Netscape in 1995 to protect the confidentiality and integrity of HTTP connections. Since 2001, the protocol has been maintained by the IETF (Internet Engineering Task Force) and renamed TLS (Transport Layer Security).

The primary objective of SSL/TLS was to secure online shopping and banking websites. With the advent of Web 2.0, its usage has expanded significantly: services provided by Google, Yahoo!, Facebook, and Twitter now offer secure access using TLS. Additionally, other services like SMTP and IMAP benefit from the security layer, and there are several VPN (Virtual Private Network) implementations that rely on SSL. Some Wi-Fi access points also use TLS as an authentication protocol (EAP-TLS).

Several flaws have been discovered in TLS, leading to revisions of the standard. Moreover, TLS is subject to various configuration and implementation errors. Given the ubiquity of TLS on the internet, it is crucial to assess its security. Since HTTPS represents most of the daily TLS usage, we designed our experiments to understand what browsers face daily and whether this view is satisfactory. We performed several campaigns in 2010 and 2011 to enumerate HTTPS servers answering on TCP port 443, using different stimuli to gather precise information about supported features.

Many SSL/TLS handshake parameters can be considered to assess the quality of a server's response. These include protocol version, ciphersuite, extensions, and certificate chain details. We selected various criteria and analyzed them through three different subsets: all hosts, hosts presenting trusted valid certificates, and hosts presenting EV certificates. Our contribution is threefold:
- Using multiple stimuli to enrich the collected data.
- Proposing a variety of criteria to assess TLS quality.
- Analyzing the data through different subsets.

Our work aligns with the suggestions of several cybersecurity researchers who advocate for thorough experiments, as highlighted in last year's keynote speech at ACSAC 2011 [1].

## 2. State of the Art

### 2.1 SSL/TLS: A Quick Tour
SSL (Secure Sockets Layer) is a protocol developed by Netscape in 1995 to secure HTTP connections using the https:// scheme. The first published version was SSLv2 [17], followed by SSLv3 [16], which fixed major conceptual flaws. Despite a compatibility mode, SSLv2 and SSLv3 use different message formats.

In 2001, the IETF (Internet Engineering Task Force) took over the maintenance of the protocol and renamed it TLS (Transport Layer Security). TLSv1.0 [9] is a minor update of SSLv3, while TLSv1.1 [10] and TLSv1.2 [11] were published in 2006 and 2008, respectively. Table 1 summarizes the different versions of SSL/TLS. Today, SSLv2 and SSLv3 should not be used, and TLS versions 1.1 and 1.2 are recommended.

To establish a secure session between a client and a server, SSL/TLS uses handshake messages to negotiate parameters such as the protocol version, cryptographic algorithms, and associated keys. These algorithms are described by ciphersuites, which define:
- Server authentication.
- Establishment of a shared secret for key derivation.
- Encryption of application data.
- Integrity of application data.

Figure 1 illustrates a typical TLS handshake. The client initiates the connection over TCP, proposing several versions and ciphersuites in the ClientHello message, which also contains a nonce. If the server finds an acceptable ciphersuite, it responds with ServerHello, containing the selected version and ciphersuite, the Certificate message, and an empty ServerHelloDone message. The client then checks the received certificates and sends a ClientKeyExchange message, carrying a random value encrypted with the server's public key. At this point, both the client and server share this secret value. Finally, ChangeCipherSpec messages activate the negotiated suite and keys, and Finished messages ensure the integrity of the handshake by containing a hash of all previous handshake messages.

| Version | Comments |
|---------|----------|
| SSLv2   | Major structural flaws [29]. Should not be used anymore. PKCS#1 flaw in early implementations [4]. Interoperability issues with newer versions. |
| SSLv3   | Weakness of CBC using implicit IV [23, 12]. Workarounds exist in major software. |
| TLSv1.0 | Minimum recommended version. |
| TLSv1.1 | New ciphersuites (GCM mode, HMAC with SHA2 hash functions). |

### 2.2 Known Vulnerabilities
Early SSL/TLS versions had significant protocol issues. SSLv2 is flawed in numerous ways, with the most problematic vulnerability being the ease with which an attacker can tamper with the negotiation [29]. More recently, Ray devised an attack on the renegotiation feature affecting all TLS versions [24, 25].

SSL/TLS uses many cryptographic primitives, some of which are weak, such as DES or MD5, and should not be used [15, 28]. Other algorithms were incorrectly implemented—Bleichenbacher described an attack on PKCS#1 in 1998 [4], and Rogaway showed in 2002 that an adaptive chosen plaintext attack was possible on CBC, which was patched in TLSv1.1 and later proved exploitable in 2011 [23, 12].

Symmetric or asymmetric weak keys can lead to the loss of confidentiality or allow an attacker to control the connection entirely. For example, RC4 40-bit keys in export ciphersuites and 512-bit RSA keys, which can be easily factored, pose significant risks.

Since the authentication of visited sites relies on certificates, the processes of generation, validation, and revocation are critical. Examples of bad random number generators exist, such as a bug in the Debian version of OpenSSL that reduced the effective entropy to only a few dozen bits from 2006 to 2008 [8]. More recently, Lenstra et al. showed that some network devices did not produce enough entropy and reused prime numbers between RSA generations, allowing moduli to be factored by a simple gcd algorithm [20].

X.509, the standard for certificates, has many extensions that have not always been correctly interpreted. For instance, the BasicConstraints extension is used to distinguish server certificates from authority certificates. In 2002, Marlinspike showed that this distinction was not implemented in Webkit or CryptoAPI [21], and he demonstrated other vulnerabilities in major SSL/TLS implementations [22].

Recent incidents affecting certification authorities [6, 30] have shown that the revocation system using CRLs (Certificate Revocation Lists) or OCSP (Online Certificate Status Protocol) does not function effectively. Web browsers have had to resort to blacklists to mitigate the consequences of compromised certificates.

The TLS ecosystem is complex, making it difficult for clients to assess a server's trustworthiness, especially for web browsers. This is why we decided to evaluate what web browsers encounter. Several research teams have conducted similar studies, which are examined in section 8.

## 3. Methodology of the Measures

### 3.1 Enumerating HTTPS Hosts
Gathering data about what a browser faces daily can be done in several ways:
- Enumerating every routable address in the IPv4 space to find open HTTPS ports (TCP/443).
- Contacting HTTPS hosts based on a list of DNS (Domain Name System) hostnames.
- Collecting real HTTPS traffic from consenting users.

The first method is the most exhaustive but leads to contacting many non-HTTPS hosts and does not account for the popularity of internet sites. The second option better represents user needs and optimizes the proportion of HTTPS servers. The third method is passive and centered on users' habits, requiring access to the traffic of many different consenting users.

We chose the first method to acquire a broad vision of the HTTPS world, allowing us to get consistent answers to multiple stimuli for each host.

### 3.2 Description of the Campaigns
In July 2010 and July 2011, we launched several campaigns to enumerate HTTPS hosts in the IPv4 address space. We used different stimuli (different ClientHello messages) to understand the behavior of various TLS stacks.

#### Phase 1: Finding the HTTPS Hosts
The first task was to identify hosts accepting connections on TCP port 443. Using BGP (Border Gateway Protocol) internet routing tables, we reduced the search space from 4 billion IPv4 addresses to 2 billion routable addresses. Instead of using existing tools like nmap, we developed homemade probes to randomize the set of routable addresses globally. Each host was tested with a SYN-probe to determine open ports.

To prevent this phase from being too intrusive, we limited our upstream rate to 100 kB/s, allowing us to explore the 2 billion addresses in about two weeks.

#### Phase 2: TLS Sessions
Once a host offering a service on port 443 was discovered, we attempted to communicate with it using one or several TLS ClientHello messages. In this phase, we used a full TCP handshake followed by several packets, but only with the fraction of servers listening on port 443 (about 1 percent). This phase could run in parallel with the first one.

To limit the computational impact on servers, we recorded only the first server response (messages between ServerHello and ServerHelloDone) before ending the connection. This way, we collected the protocol and ciphersuite chosen by the server, as well as the certificate chain sent.

In the 2010 campaign, we sent only one ClientHello message. In the July 2011 campaign, we sent several ClientHello messages containing different protocol versions, ciphersuites, and TLS extensions.

Additionally, two campaigns were publicly released by the Electronic Frontier Foundation (EFF) in December 2010, allowing us to extend the data for analysis. Table 2 describes the specifics of the ClientHello messages sent for each dataset.

| Id       | Date      | SSLv2 | Max Version | Extensions | Ciphersuites               |
|----------|-----------|-------|-------------|------------|----------------------------|
| NoExt1   | 2010/07   | no    | TLSv1.0     | None       | Standard Firefox suites     |
| EFF-1    | 2010/08   | yes   | TLSv1.0     | None       | SSLv2 + some TLSv1.0 suites |
| EFF-2    | 2010/12   | yes   | TLSv1.0     | None       | SSLv2 + some TLSv1.0 suites |
| NoExt2   | 2011/07   | no    | TLSv1.0     | None       | Standard Firefox suites     |
| DHE      | 2011/07   | no    | TLSv1.0     | None       | DHE suites only             |
| FF       | 2011/07   | no    | TLSv1.0     | None       | Standard Firefox suites     |
| EC       | 2011/07   | no    | TLSv1.0     | EC, Reneg, Ticket | EC suites only         |
| SSL2     | 2011/07   | yes   | SSLv2       | None       | SSLv2 + some TLSv1.0 suites |
| SSL2+    | 2011/07   | yes   | TLSv1.0     | None       | SSLv2 + some TLSv1.0 suites |
| TLS12    | 2011/07   | no    | TLSv1.2     | None       | Standard Firefox suites     |

### 3.3 Issues Encountered
Our July 2010 and July 2011 campaigns each took two to three weeks to complete. This duration was necessary to avoid link saturation during host enumeration. However, spanning our measures over several weeks impacts the picture of the internet we see. Three factors need to be considered:
- The time spent acquiring the data, ideally as short as possible to get consistent data.
- The network load induced, as sending too many packets can result in some getting lost.
- The use of dynamic IPs in some address blocks, where ISPs change IP addresses frequently.

Considering the network bandwidth and the way IPs were randomized, we are confident that our approach provided a reliable snapshot of the internet.