crucial in understanding the intent of the call. As explained in
Section 3.3, we record and store call audio from unsolicited
calls on a subset of our lines. Now, we discuss the character-
istics of call audio collected in our honeypot.
Some robocalls have a pre-recorded message while other
calls have large sections of audio that are silent. In situations
where an actual person dialed one of our inbound lines, it
is typical for the user to wait for a response from our side
to continue the conversation and hang up after some time.
To categorize such calls, we calculate the duration of call
USENIX Association
29th USENIX Security Symposium    405
recording which has audio and the duration for which there
is silence. These two values help us identify the calls which
have a large fraction of audio, which are clear indications of
a robocall.
To measure the amount of audio in a call recording, we
use py-webrtcvad, 7, a Python interface for WebRTC VAD
project. 8 By performing this pre-processing step, we identify
and measure the position and duration of non-speech signals.
Using these measurements, we compute the total audio du-
ration, the total silence duration and percentage of audio for
every call recording. We empirically select two thresholds to
determine the calls which have signiﬁcant amount of audio in
the recording — calls should have at least 5 seconds of pure
audio and at least 10 % of the entire call should be pure audio.
We prune the calls which do not meet these two thresholds
before we perform campaign identiﬁcation using call audio.
5 Campaign Identiﬁcation
In this section, we describe common traits of robocalling and
spam calling operations and how we exploit this similarity
to develop a clustering algorithm to identify campaigns. We
note that number rotation eliminates the possibility of using
the calling number to group similar calls.
While number rotation is simple and inexpensive, using
signiﬁcantly different audio prompts for each call is computa-
tionally and economically expensive for the caller. Our key
insight is that a speciﬁc operation will use the same audio to
make unsolicited calls, and similarity allows us to group calls
with similar audio to identify a group of calls as a campaign.
In order to group similar calls, we use raw audio signals
present in the call recording to generate audio ﬁngerprints
and use these ﬁngerprints to cluster similar audio ﬁles. While
other researchers [16, 22, 23] have applied Natural Language
Processing (NLP) and Machine Learning techniques to audio
transcripts in order to analyze calls and cluster them, such
techniques involve error and loss of information during tran-
scription. Our audio ﬁngerprinting based clustering approach
is versatile and has numerous advantages as described below.
First, our approach is language and speaker agnostic, al-
lowing us to process calls in any language without any mod-
iﬁcation to our pipeline. Second, our clustering approach is
capable of matching audio ﬁles which are not identical, but
have signiﬁcant portions of audio that are identical. This is
important in our case because many campaigns use text-to-
speech systems to dynamically insert the name of the called
party as part of the robocall. For example, a sample audio
snippet could be “Hello , this is a call from the So-
cial Security Administration.” Third, our speciﬁc technique is
resistant to noise, compression, and packet loss.
7https://github.com/wiseman/py-webrtcvad
8https://webrtc.org/
Figure 3: Robocalling Campaign Identiﬁcation Process
through a Five Stage Audio Clustering Pipeline
It is important to keep in mind that what we are character-
izing as campaigns is audio, not operators. Multiple operators
may collude and use the same audio ﬁles as one campaign.
Likewise, a single operator might use many different audio
ﬁles, each a different campaign.
5.1 Fingerprinting and Clustering
The architecture of our multi-stage audio clustering pipeline is
shown in Figure 3. First, the recorded phone calls go through
the audio preprocessing stage which computes the amount of
audio in each call recording, as explained in Section 4.7. In
stage two, the cleanup stage, we discard any audio ﬁles with
less than or equal to 5 seconds of audio or less than or equal
to 10% of audio, since we are unable to group silent audio
into particular campaigns. These two threshold values were
empirically determined based on how long it took the authors
to convey a single meaningful sentence.
Thirdly, the ﬁngerprint preprocessing stage takes each au-
dio sample as the input, generates the ﬁngerprint of the audio
ﬁle and stores it in the ﬁngerprint database. In the context of
this paper, a ﬁngerprint [24, 25] refers to a compact represen-
tation of a ﬁle such that, with high probability, the ﬁngerprints
of two similar ﬁles are similar (but not necessarily equal), and
the ﬁngerprints of two non-similar ﬁles are different. Such
ﬁngerprinting techniques are applied to audio ﬁles [26] to
index songs and perform real-time audio search (e.g., Shaz-
aam [27]). We use audio ﬁngerprinting techniques to identify
similar call recordings and cluster them together to identify
robocalling campaigns.
We use echoprint [28], an open source framework for
audio ﬁngerprinting and matching. We choose echoprint
instead of other audio ﬁngerprinting frameworks since it
uses a robust ﬁngerprinting primitive that is well suited for
phone call recordings. Since we do not claim the design of
echoprint as a contribution, we discuss its design and op-
eration in detail in Appendix A. We use raw audio for all
the above computation. Using a lossless Waveform Audio
File Format (WAV) to store call audio instead of a lossy com-
pressed format like MP3 reduces the probability of error [28]
in echoprint. Using WAV ﬁles and discarding silent audio
406    29th USENIX Security Symposium
USENIX Association
calls, as done in stage 2, signiﬁcantly improves the perfor-
mance of echoprint.
Fourthly, the ﬁngerprints of the ﬁltered audio ﬁles go
through the matching stage. We query the echoprint database
for each new audio ﬁngerprint to check if there is a similar
audio ﬁle already in the database. If there are no matches,
then we add the current audio ﬁngerprint to the database. If
we ﬁnd a match, then we add an edge between the two audio
ﬁles, where each node represents an audio ﬁngerprint. These
nodes and edges are a part of an undirected graph G.
After processing all the audio ﬁngerprints, the undirected
graph G has nodes with edges that connect similar audio ﬁles.
The ﬁnal stage identiﬁes the connected components of G,
where each connected component is a robocalling campaign.
5.2 Clustering Evaluation
It is important to evaluate our clustering methodology. How-
ever, precision in this context is not clearly deﬁned. To evalu-
ate precision, we deﬁne and compute two custom metrics —
cluster perfection and intra-cluster precision — to measure
the effectiveness of our audio-based clustering methodology.
Cluster perfection is deﬁned as the ratio of the number of
clusters without misplaced calls to total number of clusters
analyzed. Intra-cluster precision is deﬁned as the mean of the
ratio of number of correctly placed calls in the cluster to the
total number of calls in the cluster. We note that computing
recall is impossible given no ground truth on the total count
of campaigns in our data.
We use the Industry Robocall Blocking Dataset to eval-
uate our methodology, since we already have good quality
transcript for these calls, as explained in Section 3.5 We thus
used the transcripts to assist in labeling correct clustering
assignment. We randomly select 20,000 audio samples from
the Industry Robocall Blocking Dataset and apply our clus-
tering pipeline. We identiﬁed 1,188 clusters and clustered a
total of 8,290 audio samples. Out of all these clusters, we
selected 30 random clusters and manually listened to a to-
tal of 160 audio samples to compute Cluster Perfection and
Intra-cluster Precision. We found that there were 2 clusters
among the 30 clusters with at least one misplaced call in
each of them, resulting in an overall Cluster Perfection rate
of 93.33%. The overall Intra-cluster Precision for these 30
clusters was 96.66%.
5.3 Campaign Characterization
In this subsection, we characterize campaigns identiﬁed using
our clustering mechanism. We apply the campaign identiﬁ-
cation methodology described above to our data set of call
recordings collected from our honeypot and identify robo-
calling campaigns operating in the real world. We deﬁne and
compute metrics which help us characterize the robocalling
campaigns systematically.
Finding 15: 91,796 (62.75%) call recordings did not have
sufﬁcient amount of audio to be considering for clustering. We
found that 61,528 (42.05%) call recordings had less than 1%
audio in the entire duration of the call. Furthermore, 70,916
(48.47%) calls had a total duration of less than one second. A
possible explanations for a large fraction of silent calls could
be that the campaigns are interested in identifying the phone
numbers which are active and are capable of answering a
phone call. Another reason could be that the campaigns use
voice activity detection features that triggers the payback of a
recorded message once the calling side is conﬁdent that the
call has been answered by an actual person. Since we used
a simple greeting while answering a phone call and remain
silent post the greeting message, such call answering behav-
ior may not be categorized as a live human in sophisticated
outbound calling campaign systems.
It is practically infeasible to convey meaningful informa-
tion in such short duration and by using a small fraction of
speech throughout the call. Also, it is unlikely for an active
caller who may have mis-dialed the called number, to discon-
nected within a fraction of a second after we answer the call.
At the outset, such a large number of call audio recordings not
containing substantial amount of audio may seem surprising.
This high rate may be explained by hit list generation.
Additionally, we observed that few calls (0.01 %) among all
the recorded calls were disconnected by our honeypot, which
was conﬁgured to terminate the call after 60 seconds. The
rest were disconnected by the calling side. This observation
indicates that a 60 second recording duration is sufﬁcient to
record signiﬁcant portions of unsolicited phone calls.
After ﬁltering out the calls which lack substantial audio
to be clustered into a campaign, we performed clustering to
identify similar audio as described before in Figure 3.
Finding 16: We found that out of 54,504 call recordings with
substantial audio content, 34,150 ( 62.65%) call recordings
were identiﬁed to be a part of one of the many campaigns.
Of all the calls we processed, we observed that 62.65% were
grouped into one of the campaigns. Such high percentage
of calls being grouped into clusters indicate that our clus-
tering approach is capable of identifying campaigns and is
successful in grouping similar calls into clusters. By analyz-
ing complete campaigns we give providers the tools to choose
which operations to target and help them ﬁnd their weakest
points. For example by doing traceback only on the calls in a
campaign that are originated by peers.
Finding 17: We discovered 2,687 unique robocalling cam-
paigns operating in the wild. The largest campaign cluster had
6,055 unique call recording with an average call duration of
47.71 seconds. The calls in this top campaign had an average
of 84.88% audio content, which signiﬁes that the campaign
was indeed playing a dense recorded message. Furthermore,
the average cluster size of the top ﬁve campaigns was 2,372.2,
USENIX Association
29th USENIX Security Symposium    407
Campaign Size: Number of calls in each campaign, where a
campaign is represented by a cluster of audio recordings.
Source Distribution: Ratio of the count of unique caller ID
used by the campaign to the campaign size. A 100% source
distribution indicates that the campaign used a different caller
ID for every call. This metric quantiﬁes the rate at which
campaigns spoof caller ID or rotate between calling numbers.
Spread: Ratio of the count of unique destination numbers
to the campaign size. A 100% Spread indicates that every
call from this campaign was to a different inbound line. This
metric helps us understand if a campaign is targeting a speciﬁc
set of inbound lines or tends to distribute calls across a wide
range of called numbers.
Toll-Free Number Usage: A count of unique toll-free num-
bers used as the caller ID.
NPA-NXX Matching Percentage: Calls which had identical
NPA and NXX for calling and called numbers. This is a
measurement of neighbor spooﬁng.
After we deﬁned various metrics, we compute them for
each of the 2,687 campaigns. Now, we interpret the metrics
to understand how these campaigns differ from each other.
Finding 19: Robocalling campaigns had an average source
distribution of 84.17%, which indicates that most campaigns
use a large pool of numbers as caller ID. We observed that
the largest robocalling campaign with a campaign size of
6,055 had a Source Distribution of 99.93%. The top 10 cam-
paigns had an average source distribution of 95.50%. Such
high source distribution rate indicates that the campaigns are
likely spooﬁng the caller ID. If the campaign is not spoof-
ing caller IDs, then the campaign might own a large pool of
phone numbers using which it generates unsolicited phone
calls. The ﬁndings from the source distribution indicate that
well-known call blocking techniques that use allowlists or
denylists will not effectively detect or block calls from many
campaigns. In future work we hope to analyze the distribution
and relative usage of lines by campaigns, and in so doing
potentially examine patterns that could be used to predict and
block robocalls based on their line rotation strategies.
Finding 20: Robocalling campaigns had an average spread
of 78.30% with a few top campaigns targeting speciﬁc in-
bound lines. We observed that the top campaign had a spread
of 19.60%, which indicates that there were multiple calls from
the same campaign to a set of inbound line. Such behavior
could also indicate that the campaign is using a list of phone
number to target their calls. It could also indicate that they
selectively target the inbound lines which answer the previous
calls made by the campaign. If so, the number of campaigns
using this technique must be small in order to be consistent
with ﬁnding 9. An average spread of 78.30% indicates that
most campaigns target a wide range of phone numbers. In
future work we hope to analyze the distribution and relative
usage of lines by campaigns, and in so doing potentially exam-
ine patterns that could be used to predict and block robocalls
Figure 4: This campaign size histogram indicates we received
only a few calls from most campaigns. We received fewer
than 27 calls from 95% of campaigns. The largest campaign
had 6,055 calls.
Figure 5: Top 10 Robocalling Campaigns with Radius of the
Circle indicating the Relative Campaign Size
which reafﬁrms our key insight — campaigns that operate at
scale reuse the same audio prompts or use audio prompts with
slight modiﬁcations.
Finding 18: We observed that on an average, a campaign has
12.70 calls. As shown in Figure 4, we can infer that among
all the 2,687 campaigns, a large fraction of campaigns were
relatively small in size and a few campaigns have signiﬁcantly
large size.
5.4 Campaign Metrics
To systematically evaluate various operational characteristics
of the campaigns, we deﬁne and calculate metrics to measure
the behavior of robocalling campaigns.
408    29th USENIX Security Symposium
USENIX Association
these changes have arisen because taxes are seen as a seasonal
issue where other issues are relevant year-round.
Finding 23: We observed that SSA campaigns prefer to use
toll-free numbers as the caller ID and are highly targeted to
speciﬁc users. We found that the SSA Campaign #1 used 224
unique toll-free numbers with a source distribution of 89.39%,
which indicates that only a few calls reused a caller ID. This
campaign had a spread of 46.21%. The SSA Campaign #2
also extensively used a pool of 25 unique toll-free numbers.
This campaign has an overall source distribution of 100%
and a spread of 29.33%. Such low spread indicates that both
the campaigns were selective in targeting speciﬁc inbound
lines, and therefore called the same inbound lines multiple