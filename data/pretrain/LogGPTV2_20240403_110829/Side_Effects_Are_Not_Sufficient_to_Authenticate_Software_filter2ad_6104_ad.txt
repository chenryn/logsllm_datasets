does not solve this problem: it does not have any provi-
sion for authenticating any particular user. At best, it can
verify a client operating system and delegate the task to
the client machine. However, we already have solutions
to the user authentication problem that do not require a
trusted client operating system: use a shared secret, typi-
cally a password, or use a public-key approach. Another
kind of access control, used to maintain a proprietary in-
terest, ensures that a particular application is being used
to access a service. For example, a company may wish to
ensure that only its client software, rather than an open-
source clone, is being used on its instant-messenging
network. In this case, the trusted kernel would presum-
ably allow loading of the approved client software, but
would also have to know which other applications not to
load in order to prevent loading of a clone. The alter-
native is to restrict the set of programs that may be run
to an allowed set, but it is unlikely that any one service
vendor will get to choose this set for all its customers’
machines.
6.4 Large Trusted Computing Base
When designing secure systems, we strive to keep the
trusted computing base (TCB)—the portion of the sys-
tem that must be kept secure—as small as possible.
For example, protocols should be designed such that if
one side cheats, the result is correct or the cheating de-
tectable by the other side. Unfortunately, the entire client
machine, including its operating system, must be trusted
in order for Genuinity to protect a service provider that
does not perform other authentication. If there is a lo-
cal root exploit in the kernel that allows the user to gain
root privilege, the user can recover the session key, im-
personate another user, or otherwise access the service
in an insecure way. Operating system kernels—and all
setuid-root applications—are not likely to be bug-free in
the near future. (A related discussion may be found in
Section 6.5.1.)
6.5 Applications
Although two applications, NFS and instant messeng-
ing, are proposed by Kennell and Jamieson, we argue
that neither would work well with the Genuinity test pro-
posed, because of two main ﬂaws: ﬁrst, the cost of im-
plementing the scheme is high in a heterogeneous envi-
ronment, and second, the inconvenience to the user is too
high in a widely distributed, intermittently-connected
network.
6.5.1 NFS
The ﬁrst example given in the original Genuinity paper
is that an NFS server would like to serve only trusted
clients. In the example, Alice the administrator wants
to make sure that Mallory does not corrupt Bob’s data
by misconﬁguring an NFS client. The true origin of
the problem is the lack of authentication by the NFSv3
server itself; it relies entirely on each client’s authen-
tication, and transitively, on the reliability of the client
kernels and conﬁguration ﬁles. A good solution to this
problem would ﬁx the protocol, by using NFSv4, an
NFS proxy, an authenticating ﬁle system, or a system
like Kerberos. NFSv4, which has provisions for user au-
thentication, obviates the need for Genuinity; the trusted
clients merely served as reliable user authenticators.
Unfortunately, the Genuinity test does not really solve
the problem. Why? The Genuinity test cannot distin-
guish two machines that are physically identical and run
the same kernel. As any system administrator knows,
there are myriad possible conﬁgurations and misconﬁg-
urations that have nothing to do with the kernel or pro-
cessor. In this case, Mallory could either subvert Bob’s
NFS client or buy an identical machine, install the same
kernel, and add himself as a user with Bob’s user id.
Since the user id is the only thing NFS uses to authenti-
cate ﬁlesystem operations over the network once the par-
tition has been mounted, Mallory can impersonate Bob
completely. This requires a change to system conﬁgura-
tion ﬁles (i.e., /etc/passwd), not the kernel. The bug
is in the NFS protocol, not the kernel.
The Genuinity test is not designed to address the user-
authentication problem. The Genuinity test does nothing
to verify the identity of a user speciﬁcally, and the scope
of its testing—verifying the operating system kernel—is
not enough preclude malicious user behavior. Just be-
cause a machine is running a speciﬁc kernel on a speciﬁc
processor does not mean its user will not misbehave.
Further, even though the Genuinity test allows the en-
tity to establish a session key with the authority, this key
does no good unless applications actually use it. Even
if rewriting applications were trivially easy (for exam-
ple, IP applications could run transparently over IPSec),
it does not make sense to go through so much work—
running a Genuinity test at boot time and disallowing
kernel and driver updates—for so little assurance about
the identity of the entity.
6.5.2 AIM
The second example mentioned in the original Genuinity
paper is that the AOL Instant Messenger service would
like to serve only AIM clients, not clones. The Gen-
uinity test requires the entity (AIM client) to be in con-
stant contact with the authority. The interval of con-
tact must be less than that required to, say, perform a
suspend-to-disk operation in order to recover the ses-
sion key. On a machine with a small amount of RAM,
that interval might be on the order of seconds. On
wide-area networks, interruptions in point-to-point ser-
vice on this scale are not uncommon for a variety of rea-
sons [LTWW93].
It does not seem plausible to ask a
user to reboot her machine in order to use AIM after a
temporary network glitch.
6.5.3 Set-top game boxes
Although the two applications discussed in the origi-
nal paper are unlikely to be best served by Genuinity, a
more plausible application is preventing cheating in mul-
tiplayer console games. In this scenario, Sony (maker of
the Playstation) or Microsoft (maker of the Xbox) would
use Genuinity to verify that the game software running
on a client was authentic and not a version modiﬁed to
allow cheating. This is a good scenario for the authority,
since it needs to deal with only one type of hardware,
speciﬁcally one that it designed. Even in the absence of
our substitution attack (Section 4.1), Genuinity is vul-
nerable to larger scale proxy attacks (Section 7.2).
7 Genuinity-like schemes and attacks
We have described two types of attacks against this im-
plementation of Genuinity: one type against the check-
sum primitive, and one type against the key agreement
protocol.
In this section we describe general attacks
against any scheme like Genuinity, where
1. The authority has no prior information other than
the hardware and software of the entity, and
2. The entity does not have tamper-proof or tamper-
resistant hardware.
7.1 Key recovery using commonly used
hardware
Clearly, the Genuinity primitive is not of much use if the
negotiated session key is compromised after the test has
completed. Since the key is not stored in special tamper-
proof hardware, it is vulnerable to recovery by several
methods. Many of these, which are cheap and practical,
are noted by Kennell and Jamieson, but this does not
mitigate the possibility of attack by those routes. Multi-
processor machines or any bus-mastering I/O card may
be used to read the key off the system bus. This attack is
signiﬁcant because multiprocessor machines are cheap
and easily available. Although the Genuinity primitive
takes pains to keep the key on the processor, Intel x86
machines have a small number of nameable general-
purpose registers and it is unlikely that one could be ded-
icated to the key. It is not clear where the key would be
stored while executing user programs that did not avoid
use of a reserved register. It is very inexpensive to de-
sign an I/O card that simply watches the system bus for
the key to be transferred to main memory.
7.2 Proxy attacks: an economic argument
As we have seen, by design the authority has no particu-
lar computational advantage over a client or anyone else
when it comes to computing correct checksums. Cou-
ple this with the fact that key recovery is easy in the
presence of even slightly specialized hardware or mul-
tiprocessors, and it becomes clear that large-scale abuse
is possible. Let us take the example of the game con-
sole service provider, which we may fairly say is a best
case for Genuinity—the hardware and software are both
controlled by the authority and users do not have as easy
access to the hardware. In order to prevent cheating, the
authority must ensure that only authorized binaries are
executed. The authority must make a considerable in-
vestment in hardware to compute checksums from mil-
lions of users. However, this investment must cost suf-
ﬁciently little that proﬁt margins on a $50 or $60 game
are not eroded; let us say conservatively that it costs no
more than $0.50 per user per month. Now there is the op-
portunity for an adversary, say in a country without strict
enforcement of cyberlaws, to set up a “cheating service.”
For $2 per month, a user can receive a CD with a cheat-
enabled version of any game and a software update that,
when a Genuinity test is invoked, redirects the messages
to a special cheat server. The cheat server can either use
specialized hardware to do fast emulation, or can run the
software on the actual hardware with a small hack for
key recovery. It then forwards back all the correct mes-
sages and, ultimately, the session key. The authority will
be fooled, since network latency is explicitly considered
to be unimportant on the time scale of the test.
7.3 A recent system: SWATT
More recently, the SWATT system [SPvDK04] of Se-
shadri et al. has attempted to perform software-only
attestation on embedded devices with limited architec-
tures by computing a checksum over the device’s mem-
ory. Its purpose is to verify the memory contents of the
device, not to establish a key for future use. Like Gen-
uinity, SWATT relies on a hardware-speciﬁc checksum
function, but also requires network isolation of the de-
vice being veriﬁed. As a result of restricting the domain
(for example, the CPU performance and memory sys-
tem performance must be precisely predictable), they are
able to provide stronger security guarantees than Gen-
uinity. SWATT requires that the device can only commu-
nicate with the veriﬁer in order to prevent proxy attacks,
which may hinder its applicability to general wireless
devices. In addition, it is not clear that the dynamic state
of a device (e.g., variable values such as sensor data or
a phone’s address book) can be veriﬁed usefully since
an attacker might modify the contents of this memory
and then remove the malicious code. Nevertheless, for
wired devices with predictable state, SWATT provides a
very high-probability guarantee of memory integrity at
the time of attestation.
The authors of SWATT also present an attack on Gen-
uinity. The attacker can ﬂip the most signiﬁcant bit
of any bytes in memory and still compute the correct
checksum with 50% probability.
8 Conclusion
Genuinity is a system for verifying hardware and soft-
ware of a remote desktop client without trusted hard-
ware. We presented an attack that breaks the Genuinity
system using only software techniques. We could not
obtain the original Genuinity code, so we made a best
effort approximation of Genuinity in our attacks. Our
substitution attacks and DoS attacks defeat Genuinity in
its current form. Genuinity could deter the attacks with
countermeasures, but this suggests an arms race. There
is no reason to assume Genuinity can win it. Kennell
and Jamieson have failed to demonstrate that their sys-
tem is practical, even for the applications in the origi-
nal paper. These criticisms are not speciﬁc to Genuinity
but apply to any system that uses side effect information
to authenticate software. Therefore, we strongly believe
that trusted hardware is necessary for practical, secure
remote client authentication.
Acknowledgements
We thank Rob Johnson for feedback and suggestions on
the substitution attack. We also thank Naveen Sastry and
David Wagner for many invaluable comments and in-
sights. David Wagner also suggested the set-top game
box application. Finally, we would like to thank the
anonymous referees for several useful suggestions and
corrections.
References
[DoD85]
DoD.
Standard department of defense
trusted computer system evaluation crite-
ria, December 1985.
[DS01]
[Gro01]
[Hua03]
[Int03]
[KJ03]
Drew Dean and Adam Stubbleﬁeld. Us-
ing client puzzles to protect TLS. In 10th
USENIX Security Symposium. USENIX
Association, 2001.
Trusted Computing Group. Trusted com-
puting group main speciﬁcation, v1.1.
Technical
report, Trusted Computing
Group, 2001.
Andrew Huang. Hacking the Xbox: an
introduction to reverse engineering. No
Starch Press, July 2003.
Model
speciﬁc registers and
http://www.intel.com/ de-
Intel.
functions.
sign/intarch/techinfo/Pentium/mdelregs.htm,
2003.
Rick Kennell and Leah H. Jamieson. Estab-
lishing the genuinity of remote computer
systems.
In 12th USENIX Security Sym-
posium, pages 295–310. USENIX Associ-
ation, 2003.
[LTWW93] Will E. Leland, Murad S. Taqq, Walter
Willinger, and Daniel V. Wilson.
On
the self-similar nature of Ethernet trafﬁc.
[Mic]
[Nec97]
[NIS04]
secure
computing
Next
genera-
base.
In Deepinder P. Sidhu, editor, ACM SIG-
COMM, pages 183–193, San Francisco,
California, 1993.
Microsoft.
tion
http://www.microsoft.com/resources.
George C. Necula. Proof-carrying code.
In Conference Record of POPL ’97: The
24th ACM SIGPLAN-SIGACT Symposium
on Principles of Programming Languages,
pages 106–119, Paris, France, jan 1997.
NIST. The common criteria and evaluation
scheme.
http://niap.nist.gov/cc-scheme/,
2004.
[SPvDK04] Arvind Seshadri, Adrian Perrig, Leendert
van Doorn, and Pradeep Khosla. Swatt:
Software-based attestation for embedded
devices.
In IEEE Symposium on Security
and Privacy, 2004.
[SPWA99] S. Smith, R. Perez, S. Weingart, and
V. Austel. Validating a high-performance,
programmable secure coprocessor.
In
22nd National Information Systems Secu-
rity Conference, October 1999.
Bennett Yee and J. D. Tygar. Secure co-
processors in electronic commerce applica-
tions. In First USENIX Workshop on Elec-
tronic Commerce, pages 155–170, 1995.
[YT95]