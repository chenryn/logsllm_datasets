q
particular, if the elements of the j-th  row of B sum to zero, 
lldllcu = 0 regardless of the the magnitude llFllCO. While 
this is not likely to happen in practice, the method is clearly 
not bulletproof for detecting corruption in  A. A simple ex- 
ample of a matrix encountered in practice which has rows 
and/or columns with entries that sum to zero is the matrix 
derived from a discretization  of Poisson’s equation using a 
five-point stencil. 
We will refer to the error detection criterion which places 
checksum vector 20  on the right as a right-sided  error detec- 
tion criterion. This criterion is guaranteed to detect a single 
error introduced in B or C. It is highly likely to detect such 
an error introduced in A. 
Left-sided error detection criterion 
Next, consider the computation e  = uT6‘ - uTC where U 
is a vector with entries vi = 1, i = 1, . . .  m. From Table 1 
we see that ifthe corruption is in matrix A or C ,  lleTllCO = 
llFllm. Again, by computing wTC  = (vTA)B we can ob- 
tain e with only three matrix-vector multiplications. In this 
case, if the corruption was in B, IleTIIm = lqllwTail, which 
can be small even if  llF1Im is large. In particular, if the el- 
ements of the i-th  column of  A  sum to  zero,  llelloo = 0. 
Thus, the  method  is  clearly  not  completely  foolproof  for 
detecting corruption of B. 
We will refer to the error detection criterion which places 
checksum vector v on the left as a left-sided  error detection 
criterion. This criterion is guaranteed to detect a single error 
introduced in  A  or C.  It  is highly  likely to detect such an 
error introduced in B .  
Two-sided error detection criterion 
Clearly, in order to guarantee the detection of the corruption 
of a single element in  one of  the three matrices, one must 
compute lldllm if the error is in either B or C, and  llellCO if 
the error is in either A or C. 
3.2  Tolerance threshold and round-off errors 
Unfortunately, computers are not equipped to deal with 
infinite precision  arithmetic and rounding errors due to fi- 
nite precision arithmetic will  occur.  In  our error detection 
setting this means that, even if no error is intrqduced in any 
of the matrices, it may well be the case that (IC - C ( (  # 0. 
Round-off error analysis of matrix operations has been a 
classic  area of numerical  analysis for the  last  half-century. 
A  result  found in  standard textbooks (e.g.,  [7]) is  that  for 
an implementation of the matrix product C  = AB, based 
on gaxpy, dot product, or outer product computations, the 
computed results, fl(AB), satisfies 
where U is the unit round-off of the machine (the difference 
between  1 and the next larger floating point number repre- 
sentable in that machine). 
Therefore, our error detection mechanism should declare 
that an error has occurred when 
where 7-  = max(m, n, IC)  U. 
These results on  thresholds  for detecting errors  merely 
reiterate the observations made in  [ 141. 
50 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:04:59 UTC from IEEE Xplore.  Restrictions apply. 
3.3  Specialization to our situation 
4.1  Right-sided error detection method 
- 
As mentioned in Section 2, our primary concern involves 
a  corruption affecting data which  reside  in  the  L1  cache. 
Thus this corruption does not necessarily  persist during the 
entire matrix-matrix multiplication.  Therefore,  it  may  be 
more informative to view matrices C, A, and B partitioned 
as follows: 
c =  (T), 
(w) 
C M I   ...  CMN 
, 
and 
A M I   . . .   AMK 
A =  
B =  
A  simple approach is to compute D  = AB, and check 
the computed D by determining whether 
IIDw - A(Bw)llco  < 7 llAllcoll~llcu~ 
If the condition is met, then  C  t aD + PC  is performed; 
otherwise D is recomputed. (Note: our assumption is that a 
copy of A or B is corrupted in some level of cache memory. 
Thus, the recomputation can use  the original data in -4 and 
B.) If a more stringent threshold is used, a false error due to 
round-off can occur. In this case one can determine whether 
- A(Bw)(), is exactly equal twice in a row. It 
or not 
it is, C is updated since this would indicate that the scheme 
resulted in a false detection due to round-off error. 
The overhead  from  error  detection  is  2mn  FLOPs  for 
forming Dw and  2kn + 2mk  FLOPs for forming A(Bw) 
for a total  of 2mn + 2kn + 2mk  FLOPs.  In addition, the 
computations of llAllcu and llBllco cost O ( m k )  and O(kn), 
respectively. If even a single error is detected, the cost of the 
operation doubles.  Also,  storage for D, mn floating point 
numbers, is required. 
where Cij is mi  x  nj, A,,  is mi  x  k,,  and B,j  is k,  x nj. 
Now Cij  is computed as a sequence of  smaller updates 
Cij  t AipBpj + Cij  and  the corruption  will  be  encoun- 
tered  in  exactly  one such update.  In other words, for one 
tuple  of  indices  ( i , j , p )  one  of  the  operands is  corrupted 
by  changing one element.  Let us  assume that B,j 
is cor- 
rupted  by  qe,eF.  Then the computed matrix 6 is equal to 
C except in the (i, j )  block, which equals Cij + qa!i’P)eT, 
where  a:”) 
If  w  again 
equals the  vector  of all ones, 116  - CIJ,  = ~ q ~ ~ ~ a ~ ’ p ) ~ ~ c o  
and I16‘w-Cwllco  = ~ ~ / ~ / a ~ ’ p ) ~ ~ m .  
It follows that the right- 
sided detection criterion for detecting errors in B or C still 
works.  The theory  behind  the left-sided and two-sided  de- 
tection criteria can be extended similarly. 
denotes the  r-th  column  of  Ai,. 
as 
4  Towards a Practical Implementation 
In  this  section  we  deal  with  two  issues  concern- 
ing  the  practical  implementation  of  a  fault-tolerant high- 
performance matrix-matrix multiplication  kernel.  First, in 
addition  to  error  detection, we  must  also  be  able  to  cor- 
rect any errors that  are exposed. Second, in order to main- 
tain high-performance, we must let the theory guide us to a 
scheme that imposes as little overhead as is possible. 
Consider  C  = aAB + PC  where  C ,  il, and  B  have 
dimensions m x n, m x k and k x n, respectively. The cost of 
this operation is 2mnk floating point operations (FLOPs). 
4.2  Left-sided error detection method 
A  simple approach is to again compute D  = *4B, and 
check the computed D by testing if  lluTD - (uT,4)Bllco < 
If the condition is met, then  C t a D  + 
7  IIA1lcollBII,. 
PC; otherwise D  is recomputed.  If  (IvTD - (uTA)Bllco 
is exactly equal twice  in a row, C is updated  since it is as- 
sumed that a corruption was erroneously detected. 
A  more sophisticated approach partitions B ,  C ,  and D 
B  =  (  BI I . . . I B N  ) ,  
C  =  (  C1  I  ... I  CN ) ,  and 
D  =  ( D 1  I . . . ( D N  ) ,  
(1) 
(2) 
( 3 )  
and computes Dj  = AB,.  After each  such  computation, 
the  magnitude  of  lluTfii  - yTBillco is  checked,  where 
?/T -  T 
- U  A can  be computed once and reused.  As before, 
if no error is detected, Ci  t crDi + PCi; otherwise Di  is 
recomputed.  Now  only  workspace for one Di  is required 
and fewer computations need to be repeated  when an error 
is detected. Note that this is not possible for the right-sided 
approach since for each Biw the product 4(Biw) must be 
computed, which  is expensive when  Bi  has  few columns, 
as it is in our implementation (described  in  the experimen- 
tal section). 
Given  a column partitioning of  matrices  Dj and  Bj  of 
width  b,  the  overhead  from  error  detection  is  now  2mk 
FLOPs  for  forming  y T   =  vTA, 2,mb FLOPs  for  form- 
ing u T D j  and 2kb FLOPs for forming v T B j .  Taking  into 
51 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:04:59 UTC from IEEE Xplore.  Restrictions apply. 
account that  n/b panels  of  D  must  be  computed, the  to- 
tal  overhead becomes 2mn + 2kn + 2mk  FLOPs, equiv- 
alent to  the  cost of  the  right-sided  error detection  scheme 
above. In addition, the computations of llA1Im and IIBjllm, 
j  = 1,. . . , N ,  cost O ( m k )  and  O(kn), respectively.  If  a 
single error is detected during the update of C ,  only 2mbk 
FLOPs are repeated. In this case, only storage for one panel 
Dj, mb floating point numbers, is required. 
4.3  Two-sided error detection 
Naturally  the  two  above-mentioned techniques  can  be 
combined to yield a two-sided error detection method. Here 
all  of  D  is  computed  using  a  left-sided  error  detection 
method,  after  which  a  right-sided  error  detection  method 
is used to verify that no undetected errors slipped by.  If no 
errors are detected, C is appropriately updated. 
The computational  cost  of  two-sided  error  detection  is 
exactly twice that of the one-sided error detection methods. 
Storage for all of  D ,  or mn floating point  numbers, is re- 
quired. However, the left-sided error detection scheme will 
almost always detect errors and thus the overhead for cor- 
recting a single error is only 2mbk FLOPs. 
4.4  Reducing overhead 
Even in  the case where no error  is  detected,  the  above 
schemes, particularly  the right- and two-sided  approaches, 
carry  a  considerable  overhead in  required  workspace.  In 
addition, if an error is detected with these methods, the cost 
of recomputation can double the overall cost of the matrix- 
matrix multiplication.  In  this section  we discuss how  both 
of these overheads can be overcome. 
Specifically, partition C, A, and B as 
these individual updates can use the error detection schemes 
described  above.  Using  this  method  workspace can  be 
greatly reduced as can the cost of a recomputation.  More- 
over, there  are  a  number of  opportunities for  the reuse  of 
results B,,w,  wTAZp, IIBp,IIm, and  IIAzpllm, where w  and 
U have length n, and m,, respectively. 
Notice  that the  proposed error detection and  correction 
scheme can now handle multiple errors with respect to the 
overall matrix-matrix multiplication, as long as only one er- 
ror occurs during the computation A,, B,,  . 
5  An Actual Implementation 
In this section  we briefly  outline our implementation of 
the ideas presented above. 
We start by describing a high-performance implementa- 
tion of matrix-matrix multiplication, ITXGEMM [SI, devel- 
oped at UT-Austin in collaboration with Dr. Greg M. Henry 
of  the  Intel  Corporation.  To  understand  how  ITXGEMM 
uses hierarchical memory to attain high performance recall 
that  the memory hierarchy  of  a modern microprocessor  is 
often  viewed  as a pyramid (see Fig.  1).  At  the  top  of the 
pyramid  there  are the  processor  registers,  with  extremely 
fast access. At the bottom, there is disk and even slower me- 
dia. As one goes down the pyramid, the amount of memory 
increases along with the time required to access that mem- 
ory. 
fast 
expensive 
c =  
A 
B 
where C,, is ma x n,, A,,  is ma x  k,,  and B,,  is k,  x n,. 
(While  this  partitioning  looks  remarkably  like  the  one  in 
Section 3.3, the discussion in that section has no bearing on 
the discussion below.)  Then C can be computed by a scal- 
ing C t PC followed by updates C,,  t aA,, B,,  + C,, , 
f o r i = l ,  ..., M , j = l ,  . . . ,  N , p = l ,  . . . ,  K .  Eachof 
52 
(4) 
slow 
and 
Figure 1. Hierarchical layers of memory. 
As  is  well-known,  processor  speed  has  been  increas- 
ing much faster than memory speed and it is thus memory 
bandwidth that  limits  the  speed  attained  in  practice  for  a 
given operation.  Fortunately, matrix-matrix multiplication 
involves 2mnk FLOPs and only mn + mk + kn data items. 
Thus, by carefully moving data between layers of memory, 
high-performance can be attained. Note  that the cost of er- 
ror detection is of  the same  order as  the  cost for  loading 
and storing  to and from a memory layer: 
The particular implementation of matrix-matrix multipli- 
cation in ITXGEMM, which we modified as part of this re- 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:04:59 UTC from IEEE Xplore.  Restrictions apply. 
search, partitions C, A, and B as in (4)-(6).  The partition- 
ing scheme used  for il is selected so that A,,  fills  a  large 
part of the L2 cache.  For the architecture selected to act as 
a testbed, an Intel Pentium  (R) 111, the optimal partitioning 
turns out to be ma = IC,  = 128. (In other words, in the pre- 
vious discussion A,,  is 128 x 128.) Then B is partitioned so 
that a reasonable amount of workspace  is required for our 
right-sided error detection scheme.  In particular, we chose 
nj  = 512. This means that the matrices are partitioned  ex- 
actly as in (1)-(3)  and updated as required by the left-sided 
error detection scheme, with b = 8. Code for error detection 
and correction was a straightforward  addition  to an imple- 
mentation  that  naturally blocked  for efficient utilization of 
the L 1  and L2 caches of the Pentium (R) I11 processor. 
If  we  consider all floating point  operations to be equal 
and we count the cost of computing the norm of an m x n 
matrix  as mn FLOPS, we  expect the ratios of overhead to 
useful  computation  shown  in  Table  2.  The overhead  for 
correction  is  for the case  when  exactly one corruption oc- 
curs during  the entire computation.  This correction  over- 
head  scales  linearly  with  the  number  of corruptions.  The 
cost  per  FLOP of  a  matrix-vector  multiplication  is  often 
an  order  of  magnitude greater  than  the  cost  per  FLOP of 
a matrix-matrix multiplication. Thus the above analysis for 
the  cost  of  error detection  may  be  optimistic  by  an  order 
of magnitude.  On  the other hand, as mentioned, there are 
opportunities for amortizing the cost of the computation of 
matrix-vector multiplies  and  norms of  matrices  which  are 
not taken into account in the analysis. 
6  Experimental Results 
All our experiments were performed on an Intel Pentium 
(R) I11 processor with  a 650 MHz clockrate,  16  Kbytes of 
LI  data  cache  and  256  Kbytes  of  L2 cache,  using  IEEE 
double-precision  floating  point  arithmetic  (U  %  2.2  x 
10-l6).  We report performance in MFLOPs/sec. (millions 
of floating point operations per second). Notice that the best 
performance we have seen on this particular processor with 
a high-performance matrix-matrix  multiplication  is around 
530 MFLOPs/sec. 
6.1  Fault-tolerance under simulated fault condi- 
tions 
In  order  to  evaluate  the  reliability  of  our  error  detec- 
tion and correction techniques we decided  to mirror in our 
experiments  what  we  expect  to  be  a realistic  fault  condi- 
tion behavior  in  practice.  Thus,  instead of introducing an 
error  either  in  A  or  B  before  the  computation  starts,  we 
introduce  the  error  before  one  of  the  updates  of  the  form 
C,,  t aA,,BP3 + PC,,  is computed.  The exact update, 
the entry were the error appeared (including the matrix, A 