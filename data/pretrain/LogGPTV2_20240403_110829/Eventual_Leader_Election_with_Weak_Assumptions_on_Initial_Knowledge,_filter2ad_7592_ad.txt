### Point-to-Point Send Primitive

If a point-to-point send primitive were available, the broadcast at Line 05 would be replaced with the statement `send(suspicion, i, susp_leveli[i], 0) to pj`, and all suspicion messages would define a single message type. In this case, each tag would define a message type. This demonstrates an interesting trade-off between communication primitives (one-to-one vs. one-to-many) and the number of message types.

### Message Processing

With Line 16, this allows all the crashed processes to eventually disappear from `contendersi`. When process `pi` receives a `(tag k, k, sl_k, silent_k, hbc_k)` message, it allocates new local variables if this is the first message received from `pk` (Lines 07-10). Process `pi` also updates `susp_leveli[k]` (Line 11). The processing of the message then depends on its tag:

- **Heartbeat Message (Lines 12-13):** If the message is not old (checked by `last_stop_leaderi[k] < hbc_k`), `pi` resets the corresponding timer and adds `k` to `contendersi`.
- **Stop Leader Message (Lines 14-16):** If the message is not old, `pi` updates its local counter `last_stop_leaderi[k]`, stops the corresponding timer, and removes `k` from `contendersi`.
- **Suspicion Message (Line 17):** If the suspicion concerns `pi`, `susp_leveli[i]` is increased accordingly.

### Proof of the Protocol

This section proves that:
1. The protocol described in Figure 2 eventually elects a common correct leader.
2. No message carries values that indefinitely grow.

The proofs assume only (K1) for the initial knowledge of the processes and (C1') and (C2') for the network's behavioral assumptions.

#### Lemma 6 [7]

Let `pk` be a faulty process. There is a finite time after which the predicate `k ∉ contendersi` remains permanently true at each correct process `pi`.

**Proof:**

Let `pk` and `pi` be a faulty process and a correct process, respectively. The only line where a process is added to `contendersi` is Line 13. If `pi` never receives a heartbeat message from `pk`, `k` is never added to `contendersi`, and the lemma follows for `pk`.

Consider the case where `pi` receives at least one heartbeat message from `pk`. Let `m` be the last heartbeat or stop leader message from `pk` received and processed by `pi`. "Processed" means that the message `m` carried a field `hbc_k` such that the predicate `last_stop_leaderi[k] < hbc_k` was true when the message was received. Note that there is necessarily such a message, as at least the first heartbeat or stop leader message from `pk` received by `pi` satisfies the predicate.

Due to the definition of `m`, there is no other message from `pk` such that `pi` executes Line 13 or Line 16 after processing `m`. There are two cases, depending on the tag of `m`:

- **If `m` is a stop leader message:** `pi` executes Line 16 and consequently removes `k` from `contendersi` permanently.
- **If `m` is a heartbeat message:** `pi` executes Line 13, which means it resets `timeri[k]` and adds `k` to `contendersi`. Since no more heartbeat messages from `pk` are processed by `pi`, `timeri[k]` eventually expires, and `pi` withdraws `k` from `contendersi` (Line 06), and never adds it again, proving the lemma.

#### Lemmas 7, 8, 9, and 10

- **Lemma 7 [7]:** `B ≠ ∅`.
- **Lemma 8 [7]:** There is a single process `p_` and it is a correct process.
- **Lemma 9 [7]:** For any two correct processes `pi` and `pj`, there is a finite time after which either (1) the predicate `i ∉ contendersj` is always satisfied, or (2) `(i ∈ B ⇒ susp_levelj[i] = Mi) ∧ (i ∉ B ⇒ susp_levelj[i] ≥ M_)`.
- **Lemma 10 [7]:** There is a time after which `p_` executes forever the while loop of its Task T1 (Lines 01-03).

#### Theorem 2 [7]

The protocol described in Figure 2 ensures that, after some finite time, all the correct processes have the same correct process `p_` as their common leader.

### Protocol Optimality

- **Theorem 3 [7]:** There is a time after which a single process sends messages forever.
- **Theorem 4 [7]:** In an infinite execution, both the local memory of each process and the size of each message remain finite.

### References

[1] Aguilera M.K., Delporte-Gallet C., Fauconnier H. and Toueg S., On Implementing Omega with Weak Reliability and Synchrony Assumptions. 22nd ACM Symposium on Principles of Distributed Computing (PODC’03), ACM Press, pp. 306-314, 2003.

[2] Aguilera M.K., Delporte-Gallet C., Fauconnier H. and Toueg S., Communication Efficient Leader Election and Consensus with Limited Link Synchrony. 23rd ACM Symposium on Principles of Distributed Computing (PODC’04), ACM Press, pp. 328-337, 2004.

[3] Anceaume E., Fernández A., Mostefaoui A., Neiger G. and Raynal M., Necessary and Sufficient Condition for Transforming Limited Accuracy Failure Detectors. Journal of Computer and System Sciences, 68:123-133, 2004.

[4] Chandra T.D. and Toueg S., Unreliable Failure Detectors for Reliable Distributed Systems. Journal of the ACM, 43(2):225-267, 1996.

[5] Chandra T.D., Hadzilacos V. and Toueg S., The Weakest Failure Detector for Solving Consensus. Journal of the ACM, 43(4):685-722, 1996.

[6] Chu F., Reducing Ω to 3W. Information Processing Letters, 76(6):293-298, 1998.

[7] Fernández A., Jiménez E. and Raynal M., Eventual Leader Election with Weak Assumptions on Initial Knowledge, Communication Reliability, and Synchrony. Tech Report #1770, IRISA, Université de Rennes (France), 19 pages, 2005. http://www.irisa.fr/bibli/publi/pi/2005/1770.0770.html

[8] Fischer M.J., Lynch N. and Paterson M.S., Impossibility of Distributed Consensus with One Faulty Process. Journal of the ACM, 32(2):374-382, 1985.

[9] Guerraoui R., Indulgent Algorithms. 19th ACM Symposium on Principles of Distributed Computing (PODC’00), ACM Press, pp. 289-298, 2000.

[10] Guerraoui R. and Raynal M., The Information Structure of Indulgent Consensus. IEEE Transactions on Computers, 53(4):453-466, 2004.

[11] Jiménez E., Arévalo S. and Fernández A., Implementing Unreliable Failure Detectors with Unknown Membership. Submitted to Information Processing Letters, 2005.

[12] Lamport L., The Part-Time Parliament. ACM Transactions on Computer Systems, 16(2):133-169, 1998.

[13] Lamport L., Shostak R. and Pease L., The Byzantine General Problem. ACM Transactions on Programming Languages and Systems, 4(3):382-401, 1982.

[14] Larrea M., Fernández A. and Arévalo S., Optimal Implementation of the Weakest Failure Detector for Solving Consensus. Proc. 19th IEEE Int’l Symposium on Reliable Distributed Systems (SRDS’00), IEEE Computer Society Press, pp. 52-60, 2000.

[15] Malkhi D., Oprea F. and Zhou L., Ω Meets Paxos: Leader Election and Stability without Eventual Timely Links. Proc. 19th Int’l Symposium on DIStributed Computing (DISC’05), Springer Verlag LNCS #3724, pp. 199-213, 2005.

[16] Mostefaoui A., Mourgaya E., and Raynal M., Asynchronous Implementation of Failure Detectors. Proc. Int’l IEEE Conference on Dependable Systems and Networks (DSN’03), IEEE Computer Society Press, pp. 351-360, 2003.

[17] Mostefaoui A., Rajsbaum S., Raynal M. and Travers C., From 3W to Ω: a Simple Bounded Quiescent Reliable Broadcast-based Transformation. Tech Report #1759, 7 pages, IRISA, University of Rennes 1 (France), 2005.

[18] Mostefaoui A. and Raynal M., Low-Cost Consensus-Based Atomic Broadcast. 7th IEEE Pacific Rim Int. Symposium on Dependable Computing (PRDC’2000), IEEE Computer Society Press, pp. 45-52, 2000.

[19] Mostefaoui A. and Raynal M., Leader-Based Consensus. Parallel Processing Letters, 11(1):95-107, 2000.

[20] Mostefaoui A., Raynal M. and Travers C., Crash-Resilient Time-Free Eventual Leadership. Proc. 23rd Int’l IEEE Symposium on Reliable Distributed Systems (SRDS’04), IEEE Computer Society Press, pp. 208-217, Florianopolis (Brazil), 2004.

[21] Mostefaoui A., Raynal M., Travers C., Patterson S., Agrawal A. and El Abbadi A., From Static Distributed Systems to Dynamic Systems. Proc. 24th Int’l IEEE Symposium on Reliable Distributed Systems (SRDS’05), IEEE Computer Society Press, pp. 109-118, Orlando (Florida), 2005.

[22] Pedone F. and Schiper A., Handling Message Semantics with Generic Broadcast Protocols. Distributed Computing, 15(2):97-107, 2002.

[23] Raynal M., A Short Introduction to Failure Detectors for Asynchronous Distributed Systems. ACM SIGACT News, Distributed Computing Column, 36(1):53-70, 2005.

[24] Yang J., Neiger G. and Gafni E., Structured Derivations of Consensus Algorithms for Failure Detectors. Proc. 17th Int. ACM Symposium on Principles of Distributed Computing (PODC’98), ACM Press, pp. 297-308, Puerto Vallarta (Mexico), July 1998.