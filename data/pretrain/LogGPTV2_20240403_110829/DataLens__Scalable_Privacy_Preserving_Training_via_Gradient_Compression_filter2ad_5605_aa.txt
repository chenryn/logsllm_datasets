title:DataLens: Scalable Privacy Preserving Training via Gradient Compression
and Aggregation
author:Boxin Wang and
Fan Wu and
Yunhui Long and
Luka Rimanic and
Ce Zhang and
Bo Li
DataLens: Scalable Privacy Preserving Training via Gradient
Compression and Aggregation
Boxin Wang‚àó
PI:EMAIL
University of Illinois at
Urbana-Champaign
Illinois, USA
Luka Rimanic
PI:EMAIL
ETH Z√ºrich
Z√ºrich, Switzerland
Fan Wu‚àó
PI:EMAIL
University of Illinois at
Urbana-Champaign
Illinois, USA
Ce Zhang
PI:EMAIL
ETH Z√ºrich
Z√ºrich, Switzerland
Yunhui Long‚àó
PI:EMAIL
University of Illinois at
Urbana-Champaign
Illinois, USA
Bo Li
PI:EMAIL
University of Illinois at
Urbana-Champaign
Illinois, USA
ABSTRACT
Recent success of deep neural networks (DNNs) hinges on the avail-
ability of large-scale dataset; however, training on such dataset
often poses privacy risks for sensitive training information. In this
paper, we aim to explore the power of generative models and gradi-
ent sparsity, and propose a scalable privacy-preserving generative
model DataLens, which is able to generate synthetic data in a
differentially private (DP) way given sensitive input data. Thus, it
is possible to train models for different down-stream tasks with the
generated data while protecting the private information. In partic-
ular, we leverage the generative adversarial networks (GAN) and
PATE framework to train multiple discriminators as ‚Äúteacher" mod-
els, allowing them to vote with their gradient vectors to guarantee
privacy.
Comparing with the standard PATE privacy preserving frame-
work which allows teachers to vote on one-dimensional predictions,
voting on the high dimensional gradient vectors is challenging in
terms of privacy preservation. As dimension reduction techniques
are required, we need to navigate a delicate tradeoff space between
(1) the improvement of privacy preservation and (2) the slowdown
of SGD convergence. To tackle this, we propose a novel dimen-
sion compression and aggregation approach TopAgg, which com-
bines top-ùëò dimension compression with a corresponding noise
injection mechanism. We theoretically prove that the DataLens
framework guarantees differential privacy for its generated data,
and provide a novel analysis on its convergence to illustrate such
a tradeoff on privacy and convergence rate, which requires non-
trivial analysis as it requires a joint analysis on gradient compres-
sion, coordinate-wise gradient clipping, and DP mechanism. To
‚àóAuthors contributed equally to this research.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea
¬© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-8454-4/21/11...$15.00
https://doi.org/10.1145/3460120.3484579
demonstrate the practical usage of DataLens, we conduct extensive
experiments on diverse datasets including MNIST, Fashion-MNIST,
and high dimensional CelebA and Place365 datasets. We show that
DataLens significantly outperforms other baseline differentially
private data generative models. Our code is publicly available at
https://github.com/AI-secure/DataLens.
CCS CONCEPTS
‚Ä¢ Security and privacy ‚Üí Software security engineering; ‚Ä¢ Com-
puting methodologies ‚Üí Neural networks.
KEYWORDS
Differential Privacy, Generative Models, Gradient Compression
ACM Reference Format:
Boxin Wang, Fan Wu, Yunhui Long, Luka Rimanic, Ce Zhang, and Bo Li. 2021.
DataLens: Scalable Privacy Preserving Training via Gradient Compression
and Aggregation. In Proceedings of the 2021 ACM SIGSAC Conference on
Computer and Communications Security (CCS ‚Äô21), November 15‚Äì19, 2021,
Virtual Event, Republic of Korea. ACM, New York, NY, USA, 23 pages. https:
//doi.org/10.1145/3460120.3484579
1 INTRODUCTION
Advanced machine learning methods, especially deep neural net-
works (DNNs), have achieved great success in a wide array of
applications [22, 23, 60], mainly due to the fast development of
hardware, their expressive representation power, and the availabil-
ity of large-scale training datasets. However, one major concern
that has risen in machine learning is that the training data usu-
ally contain a large amount of privacy sensitive information (e.g.,
human faces and medical records), which could be leaked via the
trained machine learning models [51, 64]. How to protect such pri-
vate information while allowing high learning utility for the dataset
has attracted a lot of attention. Differentially private (DP) deep
learning [2] proposes adding Gaussian noise to the clipped gradient
during training, thus ensuring that the learned results are differ-
entially private regarding the training data. However, its learning
utility largely decreases with strong privacy requirements. A semi-
supervised learning framework PATE [43, 44] is later proposed to
improve the learning effectiveness at the presence of privacy noise,
by leveraging the aggregation of noisy teacher models trained on
Session 7A: Privacy Attacks and Defenses for ML CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea2146To further improve the flexibility of differentially private ma-
chine learning process, in this paper we aim to design a privacy-
preserving data generative model which ensures that the data gen-
erator and the generated data, instead of only the predictions, are
differentially private. This way, the generated data can then be used
to train arbitrary models for different down-stream tasks with high
flexibility. Having in mind that the generative adversarial networks
(GAN) [19] achieved great success in terms of generating high qual-
ity data, it is natural to ask: Is it possible to leverage the power of
GAN in a way to generate data in a differentially private manner?
Some recent works have shown promising results on differentially
private data generative models [37, 62]. However, most of them can
only generate low dimensional data such as tabular data with weak
privacy guarantees (i.e., (ùúñ, ùõø) ‚àí ùê∑ùëÉ with small ùúñ). The problem of
generating differentially private high dimensional data (e.g., image)
with strong privacy guarantees is still open, due to the fact that,
in order to achieve strong privacy guarantees, the limited privacy
budget is not enough to train a generative model to approximate
any high dimensional perturbation.
In the meantime, an independent line of research concerning
gradient compression in distributed training for communication ef-
ficiency [5, 7, 35, 57] shows that some noisy compression schemes
such as only keeping the top-ùêæ elements of the gradient would
achieve statistically similar convergence rate with vanilla training.
This observation could potentially be a remedy for the above prob-
lem of high dimensionality ‚Äî Intuitively, the noises introduced by
these noisy compression schemes could also help protect privacy
and combining them with traditional DP noise mechanism may
allow us to add fewer amount of noise to achieve the same level of
DP protection. This intuition inspired our work, which, to our best
knowledge, is the first to marry these two lines of research on pri-
vacy and communication-efficient distributed learning to achieve
both differential privacy guarantees and high model utility on high-
dimensional data. As we will see, though intuitively feasible, taking
advantage of this intuition is far from trivial.
private datasets. It is shown that the PATE framework is able to im-
prove the learning utility significantly while protecting data privacy.
However, applying such privacy preserving training framework
from the discriminative model to the generative model to guarantee
that the generated data is differentially private is non-trivial given
the potential high-dimensional gradient aggregation.
Specifically, we propose a differentially private data generative
model DataLens based on the PATE framework, which trains mul-
tiple discriminators as different teacher models to provide the back-
propagation information in a differentially private way to the stu-
dent generator. In addition, to tackle the high-dimensional data
problem we mentioned above, we propose an effective noisy gradi-
ent compression and aggregation strategy TopAgg to allow each
discriminator to vote for the top several dimensions in their gra-
dients and then aggregate their noisy gradient sign to perform
back-propagation. We prove the differential privacy guarantees
for both the data generator and generated data for DataLens. Fur-
thermore, to ensure the performance of the trained DP generative
model, we provide a theoretical convergence analysis for the pro-
posed gradient compression and aggregation strategy. In particular,
to our best knowledge, this is the first convergence analysis consid-
ering the coordinate-wise gradient clipping together with gradient
compression and DP noise mechanism.
Finally, we conduct extensive empirical evaluation on the utility
of the generated based on DataLens comparing with several other
baselines on image datasets such as MNIST, Fashion-MNIST, CelebA,
and Place365, which is of much higher dimension than the tabular
data used by existing DP generative models. We show that the
generated data of DataLens can achieve the state-of-the-art utility
on all datasets compared with baseline approaches. We also conduct
a series ablation studies to analyze the visualization quality of the
generated data, the data-dependent and data-independent privacy
bounds, the impact of different components and hyper-parameters
in DataLens, as well as different gradient compression methods.
In addition, to further evaluate the proposed compression and
aggregation strategy TopAgg, which is the key building block in
DataLens, we also discuss and evaluate TopAgg for the standard
DP SGD training. We show that on both MNIST and CIFAR-10
datasets, TopAgg can achieve similar or even better model utility
than the state of the art baseline approaches, which leads to an
interesting future direction.
Technical Contributions. In this paper, we propose a general
and effective differentially private data generative model for high-
dimensional image data. We make contributions on both theoretical
and empirical front.
‚Ä¢ We propose an effective differentially private data generative
model DataLens, which can be applied for generating high-
dimensional image data with limited privacy budgets.
‚Ä¢ We prove the privacy guarantees for DataLens, and conduct
thorough theoretical analysis for the convergence of DataLens.
We show that DataLens is able to make a good tradeoff between
the privacy protection by adding DP noise and the slowdown of
SGD convergence due to the added DP noise.
‚Ä¢ We propose a novel noisy gradient compression and aggregation
algorithm TopAgg by combining the top-ùëò dimension compres-
sion and a specific DP noise injection mechanism. We also discuss
the potential of adapting TopAgg to standard DP SGD training
with evaluations.
‚Ä¢ To illustrate tradeoff between differential privacy and conver-
gence given gradient compression, we provide a novel theoretical
analysis jointly considering gradient compression, coordinate-
wise gradient clipping, and DP mechanism.
‚Ä¢ We conduct extensive empirical evaluation on DataLens with
four image datasets, including MNIST, Fasion-MNIST, CelebA,
and Place365 datasets. We show that in term of the utility of
generated data, DataLens significantly outperforms the state-of-
the-art DP generative models.
2 PRELIMINARIES
Here we will first provide some background knowledge on differen-
tial privacy and data generative models. We then draw connections
between the definitions we introduced here and our analysis on
DataLens later.
2.1 Differential Privacy
(ùúÄ, ùõø)-differential privacy ((ùúÄ, ùõø)-DP) is currently an industry stan-
dard of privacy notion proposed by Dwork [16] It bounds the
Session 7A: Privacy Attacks and Defenses for ML CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea2147change in output distribution caused by a small input difference
for a randomized algorithm. The following definition formally de-
scribes this privacy guarantee.
Definition 1 ((ùúÄ, ùõø)-Differential Privacy [16]). A randomized algo-
rithm M with domain N|X| is (ùúÄ, ùõø)-differentially private if for all
S ‚äÜ Range(M) and for any neighboring datasets ùê∑ and ùê∑‚Ä≤:
Pr[M(ùê∑) ‚àà S] ‚â§ exp(ùúÄ) Pr[M(ùê∑‚Ä≤) ‚àà S] + ùõø.
Differential privacy is immune to post-processing. Formally, the
composition of a data-independent mapping ùëî with an (ùúÄ, ùõø)-DP
mechanism M is also (ùúÄ, ùõø)-DP [17].
PATE Framework. Private Aggregation of Teacher Ensembles
(PATE) is one of the DP mechanisms [43, 44] that provide the differ-
ential privacy guarantees for trained machine learning models. The
PATE framework achieves DP by aggregating the prediction votes
from several teacher models, which are trained on private data, as
the input with DP noise for a student model, which serves as the fi-
nal released prediction model with privacy protection. The privacy
analysis [43] of PATE is derived using Laplacian mechanism and
moments accountant technique based on Abadi et al. [2], which
yields a tight privacy bound when the outputs of teacher models
have high consensus over the topmost votes.
2.2 Data Generative Models
Data generative models aim to approximate the distribution of large
datasets and thus generate diverse datasets following the similar
data distribution., which can be used for data augmentation and
further analysis. Recently, Generative Adversarial Network (GAN)
[19] has been proposed as a deep learning architecture for training
generative models. In particular, GAN consists of a generator Œ®
that learns to generate synthetic records, and a discriminator Œì that
is trained to tell real records apart from the fake ones. Given an
input datase ùë• and a sampled noise ùëß, we train the discriminator
Œì to maximize the likelihood of classifying the synthetic example
from Œ® as drawing from the real distribution with the loss function
LŒì defined as: LŒì = ‚àí log Œì(ùë•) ‚àí log(1 ‚àí Œì(Œ®(ùëß)). The generator
Œ® seeks to minimize the probability of the generated data being
predicted as fake ones by the discriminator Œì with the loss function
LŒ® defined as: LŒ® = ‚àí log Œì(Œ®(ùëß)). Though GANs are able to gen-
erate high-quality data records given large training datasets, such
generative models are prone to leak the information of training data
[14]. This presents us the challenge on how to prevent the training
information leakage for generated data, especially when the training
data contains a large amount of privacy-sensitive information. In
this paper, we aim to train differentially private generative models
so that we can enjoy the benefits of generative models to generate
unlimited amount of high-utility data for arbitrary downstream
tasks, while protecting sensitive training information.
2.3 Gradient Compression
Gradient compression techniques, such as quantization, low-rank
approximation, and sparsification, have been studied in the last
decade [8, 12, 27, 34, 52, 55]. One surprising result is that stochastic
gradient descent are often robust to these operations ‚Äî one can
often compress the data by orders of magnitude without signifi-
cantly slow down the convergence. Most of existing efforts focus
on saving the communication overheads in distributed training.
This paper is inspired by these previous research, however, fo-
cuses on a different problem ‚Äî can the saving of communication
overheads provide benefits to differential privacy? As we will see, by
compressing the gradient in certain way, we are able to decrease the
dimension of the gradient without significantly slow down the con-
vergence. This can translate into fewer amount of noises that one
needs to add to ensure DP. This intuition, however, requires careful