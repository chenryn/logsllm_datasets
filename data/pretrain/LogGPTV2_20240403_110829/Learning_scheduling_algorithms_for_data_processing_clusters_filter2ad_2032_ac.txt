Decima balances the size of the action space and the number of
actions required by decomposing scheduling decisions into a series
of two-dimensional actions, which output (i) a stage designated to be
scheduled next, and (ii) an upper limit on the number of executors to
use for that stage’s job.
Scheduling events. Decima invokes the scheduling agent when the
set of runnable stages — i.e., stages whose parents have completed
and which have at least one waiting task — in any job DAG changes.
Such scheduling events happen when (i) a stage runs out of tasks (i.e.,
needs no more executors), (ii) a stage completes, unlocking the tasks
of one or more of its child stages, or (iii) a new job arrives to the system.
At each scheduling event, the agent schedules a group of free ex-
ecutors in one or more actions. Specifically, it passes the embedding
vectors from §5.1 as input to the policy network, which outputs a two-
dimensional action ⟨v,li⟩, consisting of a stage v and the parallelism
limit li for v’s job i. If job i currently has fewer than li executors,
Decima assigns executors to v up to the limit. If there are still free
executors after the scheduling action, Decima invokes the agent again
to select another stage and parallelism limit. This process repeats until
all the executors have been assigned, or there are no more runnable
stages. Decima ensures that this process completes in a finite number
of steps by enforcing that the parallelism limit li is greater than the
number of executors currently allocated to job i, so that at least one
new executor is scheduled with each action.
Stage selection. Figure 6 visualizes Decima’s policy network. For
a scheduling event at time t, during which the state is st , the policy
network selects a stage to schedule as follows. For a node v in job i, it
v ,yi ,z), where q(·) is a score function that
computes a score qi
maps the embedding vectors (output from the graph neural network
in §5.1) to a scalar value. Similar to the embedding step, the score
function is also a non-linear transformation implemented as a neural
v ≜q(ei
274
Job DAG 1Job DAG nStep 1Step 2Step 1Step 2Job DAG nJob DAG 1DAG nsummaryGlobalsummaryDAG 1summarySIGCOMM ’19, August 19-23, 2019, Beijing, China
H. Mao et al.
then schedules executors to v until i’s parallelism reaches the limit li .
Through repeated actions with different parallelism limits, Decima
can add desired numbers of executors to specific stages. For example,
suppose job i currently has ten executors, four of which are working
on stage v. To add two more executors to v, Decima, on a scheduling
event, picks stage v with parallelism limit of 12. Our experiments
show that Decima achieves the same performance with job-level
parallelism as with fine-grained, stage-level parallelism choice, at
substantially accelerated training (Figure 15a).
5.3 Training
The primary challenge for training Decima is how to train with contin-
uous stochastic job arrivals. To explain the challenge, we first describe
the RL algorithms used for training.
RL training proceeds in episodes. Each episode consists of mul-
tiple scheduling events, and each scheduling event includes one or
more actions. LetT be the total number of actions in an episode (T can
vary across different episodes), and tk be the wall clock time of the kth
action. To guide the RL algorithm, Decima gives the agent a reward
rk after each action based on its high-level scheduling objective. For
example, if the objective is to minimize the average JCT, Decima
penalizes the agent rk =−(tk −tk−1)Jk after the kth action, where Jk
is the number of jobs in the system during the interval [tk−1,tk). The
goal of the RL algorithm is to minimize the expected time-average of
the penalties: E
. This objective minimizes
the average number of jobs in the system, and hence, by Little’s
law [21, §5], it effectively minimizing the average JCT.
1/tTT
k =1(tk −tk−1)Jk
(cid:104)
(cid:105)
Decima uses a policy gradient algorithm for training. The main
idea in policy gradient methods is to learn by performing gradient
descent on the neural network parameters using the rewards observed
during training. Notice that all of Decima’s operations, from the graph
neural network (§5.1) to the policy network (§5.2), are differentiable.
For conciseness, we denote all of the parameters in these operations
jointly as θ, and the scheduling policy as πθ(st ,at) — defined as the
probability of taking action at in state st .
Consider an episode of length T , where the agent collects (state,
action, reward) observations, i.e., (sk ,ak ,rk), at each step k. The
agent updates the parameters θ of its policy πθ(st ,at) using the RE-
INFORCE policy gradient algorithm [79]:
θ ←θ +α
∇θ logπθ(sk ,ak)
rk′−bk
.
(3)
T
k =1
(cid:33)
(cid:32) T
k′=k
(
Here, α is the learning rate and bk is a baseline used to reduce the
variance of the policy gradient [78]. An example of a baseline is a
“time-based” baseline [37, 53], which sets bk to the cumulative reward
from step k onwards, averaged over all training episodes. Intuitively,
k′rk′−bk) estimates how much better (or worse) the total reward is
(from step k onwards) in a particular episode compared to the average
case; and ∇θ logπθ(sk ,ak) provides a direction in the parameter space
to increase the probability of choosing action ak at state sk . As a
result, the net effect of this equation is to increase the probability of
choosing an action that leads to a better-than-average reward.6
6The update rule in Eq. (3) aims to maximize the sum of rewards during an episode. To
maximize the time-average of the rewards, Decima uses a slightly modified form of this
equation. See Appendix B for details.
275

exp(qi
v)
u∈At exp(q
Figure 6: For each node v in job i, the policy network uses per-node
v , per-job embedding yi and global embedding z to compute
embedding ei
(i) the score qi
v for sampling a node to schedule and (ii) the score wi
l for
sampling a parallelism limit for the node’s job.
network. The score qi
v represents the priority of scheduling node v.
Decima then uses a softmax operation [17] to compute the probability
of selecting node v based on the priority scores:
P(node =v) =
) ,
j(u)
u
(2)
where j(u) is the job of node u, and At is the set of nodes that can
be scheduled at time t. Notice that At is known to the RL agent at
each step, since the input DAGs tell exactly which stages are runnable.
Here, At restricts which outputs are considered by the softmax op-
eration. The whole operation is end-to-end differentiable.
Parallelism limit selection. Many existing schedulers set a static
degree of parallelism for each job: e.g., Spark by default takes the
number of executors as a command-line argument on job submission.
Decima adapts a job’s parallelism each time it makes a scheduling
decision for that job, and varies the parallelism as different stages in
the job become runnable or finish execution.
For each job i, Decima’s policy network also computes a score
≜w(yi ,z,l) for assigning parallelism limit l to job i, using another
wi
score function w(·). Similar to stage selection, Decima applies a soft-
l
max operation on these scores to compute the probability of selecting
a parallelism limit (Figure 6).
Importantly, Decima uses the same score function w(·) for all jobs
and all parallelism limit values. This is possible because the score
function takes the parallelism l as one of its inputs. Without using l as
an input, we cannot distinguish between different parallelism limits,
and would have to use separate functions for each limit. Since the
number of possible limits can be as large as the number of executors,
reusing the same score function significantly reduces the number of
parameters in the policy network and speeds up training (Figure 15a).
Decima’s action specifies job-level parallelism (e.g., ten total ex-
ecutors for the entire job), as opposed fine-grained stage-level paral-
lelism. This design choice trades off granularity of control for a model
that is easier to train. In particular, restricting Decima to job-level
parallelism control reduces the space of scheduling policies that it
must explore and optimize over during training.
However, Decima still maintains the expressivity to (indirectly)
control stage-level parallelism. On each scheduling event, Decima
picks a stage v, and new parallelism limit li for v’s job i. The system
Job DAG 1Job DAG nMessage	PassingMessage	PassingDAG	SummaryDAG	SummaryGlobal	SummarySoftmaxGraph Neural Network (§5.1)Stage selection (§5.2)Parallelism limit on job(§5.2)SoftmaxSoftmaxLearning Scheduling Algorithms for Data Processing Clusters
SIGCOMM ’19, August 19-23, 2019, Beijing, China
Figure 7: Illustrative example of how different job arrival sequences can lead
to vastly different rewards. After time t, we sample two job arrival sequences,
from a Poisson arrival process (10 seconds mean inter-arrival time) with
randomly-sampled TPC-H queries.
Challenge #1: Training with continuous job arrivals. To learn a
robust scheduling policy, the agent has to experience “streaming”
scenarios, where jobs arrive continuously over time, during training.
Training with “batch” scenarios, where all jobs arrive at the beginning
of an episode, leads to poor policies in streaming settings (e.g., see
Figure 14). However, training with a continuous stream of job arrivals
is non-trivial. In particular, the agent’s initial policy is very poor (e.g.,
as the initial parameters are random). Therefore, the agent cannot
schedule jobs as quickly as they arrive in early training episodes,
and a large queue of jobs builds up in almost every episode. Letting
the agent explore beyond a few steps in these early episodes wastes
training time, because the overloaded cluster scenarios it encounters
will not occur with a reasonable policy.
To avoid this waste, we terminate initial episodes early so that the
agent can reset and quickly try again from an idle state. We gradually
increase the episode length throughout the training process. Thus,
initially, the agent learns to schedule short sequences of jobs. As its
scheduling policy improves, we increase the episode length, making
the problem more challenging. The concept of gradually increasing
job sequence length — and therefore, problem complexity — during
training realizes curriculum learning [14] for cluster scheduling.
One subtlety about this method is that the termination cannot be de-
terministic. Otherwise, the agent can learn to predict when an episode
terminates, and defer scheduling certain large jobs until the termina-
tion time. This turns out to be the optimal strategy over a fixed time
horizon: since the agent is not penalized for the remaining jobs at
termination, it is better to strictly schedule short jobs even if it means
starving some large jobs. We found that this behavior leads to indefi-
nite starvation of some jobs at runtime (where jobs arrive indefinitely).
To prevent this behavior, we use a memoryless termination process.
Specifically, we terminate each training episode after a time τ , drawn
randomly from an exponential distribution. As explained above, the
mean episode length increases during training up to a large value (e.g.,
a few hundreds of job arrivals on average).
Challenge #2: Variance caused by stochastic job arrivals. Next,
for a policy to generalize well in a streaming setting, the training
episodes must include many different job arrival patterns. This creates
a new challenge: different job arrival patterns have a large impact
on performance, resulting in vastly different rewards. Consider, for
example, a scheduling action at the time t shown in Figure 7. If the
arrival sequence following this action consists of a burst of large jobs
(e.g., job sequence 1), the job queue will grow large, and the agent will
incur large penalties. On the other hand, a light stream of jobs (e.g.,
job sequence 2) will lead to short queues and small penalties. The
problem is that this difference in reward has nothing to do with the
action at time t — it is caused by the randomness in the job arrival pro-
cess. Since the RL algorithm uses the reward to assess the goodness
of the action, such variance adds noise and impedes effective training.
To resolve this problem, we build upon a recently-proposed vari-
ance reduction technique for “input-driven” environments [55], where
an exogenous, stochastic input process (e.g., Decima’s job arrival pro-
cess) affects the dynamics of the system. The main idea is to fix
the same job arrival sequence in multiple training episodes, and to
compute separate baselines specifically for each arrival sequence. In
particular, instead of computing the baseline bk in Eq. (3) by averag-
ing over episodes with different arrival sequences, we average only
over episodes with the same arrival sequence. During training, we
repeat this procedure for a large number of randomly-sampled job
arrival sequences (§7.2 and §7.3 describe how we generate the specific
datasets for training). This method removes the variance caused by
the job arrival process entirely, enabling the policy gradient algorithm
to assess the goodness of different actions much more accurately (see
Figure 14). For the implementation details of our training and the
hyperparameter settings used, see Appendix C.
6
We have implemented Decima as a pluggable scheduling service that
parallel data processing platforms can communicate with over an
RPC interface. In §6.1, we describe the integration of Decima with
Spark. Next, we describe our Python-based training infrastructure
which includes an accurate Spark cluster simulator (§6.2).
6.1 Spark integration
A Spark cluster7 runs multiple parallel applications, which contain
one or more jobs that together form a DAG of processing stages. The
Spark master manages application execution and monitors the health
of many workers, which each split their resources between multiple
executors. Executors are created for, and remain associated with, a
specific application, which handles its own scheduling of work to
executors. Once an application completes, its executors terminate.
Figure 8 illustrates this architecture.
To integrate Decima in Spark, we made two major changes:
(1) Each application’s DAG scheduler contacts Decima on startup
and whenever a scheduling event occurs. Decima responds with
the next stage to work on and the parallelism limit (§5.2).
Implementation
(2) The Spark master contacts Decima when a new job arrives to
determine how many executors to launch for it, and aids Decima
by taking executors away from a job once they complete a stage.
State observations. In Decima, the feature vector xi
v (§5.1) of a node
v in job DAG i consists of: (i) the number of tasks remaining in the
stage, (ii) the average task duration, (iii) the number of executors