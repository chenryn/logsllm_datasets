title:Hash First, Argue Later: Adaptive Verifiable Computations on Outsourced
Data
author:Dario Fiore and
C&apos;edric Fournet and
Esha Ghosh and
Markulf Kohlweiss and
Olga Ohrimenko and
Bryan Parno
Hash First, Argue Later∗
Adaptive Veriﬁable Computations on Outsourced Data
Dario Fiore
IMDEA Software Institute
PI:EMAIL
C´edric Fournet
Microsoft Research
Esha Ghosh†
Brown University
PI:EMAIL
esha PI:EMAIL
Markulf Kohlweiss
Microsoft Research
Olga Ohrimenko
Microsoft Research
Bryan Parno
Microsoft Research
PI:EMAIL
PI:EMAIL
PI:EMAIL
Abstract
Proof systems for veriﬁable computation (VC) have the potential to make cloud outsourcing
more trustworthy. Recent schemes enable a veriﬁer with limited resources to delegate large
computations and verify their outcome based on succinct arguments: veriﬁcation complexity is
linear in the size of the inputs and outputs (not the size of the computation). However, cloud
computing also often involves large amounts of data, which may exceed the local storage and
I/O capabilities of the veriﬁer, and thus limit the use of VC.
In this paper, we investigate multi-relation hash & prove schemes for veriﬁable computations
that operate on succinct data hashes. Hence, the veriﬁer delegates both storage and computation
to an untrusted worker. She uploads data and keeps hashes; exchanges hashes with other parties;
veriﬁes arguments that consume and produce hashes; and selectively downloads the actual data
she needs to access.
Existing instantiations that ﬁt our deﬁnition either target restricted classes of computations
or employ relatively ineﬃcient techniques. Instead, we propose eﬃcient constructions that lift
classes of existing arguments schemes for ﬁxed relations to multi-relation hash & prove schemes.
Our schemes (1) rely on hash algorithms that run linearly in the size of the input; (2) enable
constant-time veriﬁcation of arguments on hashed inputs; (3) incur minimal overhead for the
prover. Their main beneﬁt is to amortize the linear cost for the veriﬁer across all relations
with shared I/O. Concretely, compared to solutions that can be obtained from prior work, our
new hash & prove constructions yield a 1,400x speed-up for provers. We also explain how
to further reduce the linear veriﬁcation costs by partially outsourcing the hash computation
itself, obtaining a 480x speed-up when applied to existing VC schemes, even on single-relation
executions.
1
Introduction
Cryptographic proof systems let a veriﬁer check that the computation executed by an untrusted
prover was performed correctly [28]. These systems are appealing in a variety of scenarios, such
as cloud computing, where a user outsources computations and wishes to verify their integrity
given their inputs and outputs (I/O) [2, 36, 27, 25], or privacy-preserving applications, where a
∗An extended abstract of this paper appears in the proceedings of ACM CCS 2016. This is the full version.
†Work done at Microsoft Research.
1
user owns sensitive data and wishes to release partial information with both conﬁdentiality and
integrity guarantees [42, 24]. Typically, these systems require the prover to perform considerable
additional work to produce a proof that can be easily checked by the veriﬁer.
Recent advances in veriﬁable computations have crossed an important practical threshold: ver-
ifying a proof given some I/O is faster than performing the computation locally [40, 7, 43, 45].
While these systems perform well when delegating computation-intensive algorithms, they do not
help much with data-intensive applications, inasmuch as veriﬁcation remains linear in the applica-
tion’s I/O.
Although some linear work is unavoidable when uploading data, ideally one would like to pay
this price just once, rather than every time one veriﬁes a computation that takes this data as input.
This is particularly relevant for cloud computing on big data, where the veriﬁer may not have
enough local resources to encode and upload the whole database each time she delegates a query
or, more generally, where many parties contribute data over a long period of time.
Approaches providing amortized veriﬁcation do exist for limited classes of computations, such
as data retrieval. For instance, the user may keep the root of a Merkle hash tree, and use it to
verify the retrieved content. Unfortunately, as explained below, embeddings of this approach into
generic proof systems incur large overheads for the prover.
Our goal is to enable practical veriﬁable computation for data-intensive applications. In par-
ticular, we wish to design schemes where veriﬁcation time is independent of both the size of the
delegated computations and the size of their I/O. Moreover, we wish to preserve the expressive-
ness of existing VC schemes (e.g., supporting NP relations) without adding to the prover’s burden,
which is already several orders of magnitude higher than the original computation.
Modelling Hash & Prove (HP) We ﬁrst propose a model that captures the idea of hashing and
uploading data once and then using the resulting hashes across multiple veriﬁable computations. In
this model, the veriﬁer needs only to keep track of hashes, while the prover stores the corresponding
data. The prover can use the data to perform computations and then (selectively) return results in
plaintext to the veriﬁer. As described below, hashes yield several beneﬁts when delegating veriﬁable
computations.
Flexible Reuse Hashes depend only on the data and are not tied to any particular computation.
Hence, once a data hash is computed, it can be used to verify any computation that uses the
corresponding data. It can also be used with diﬀerent proof systems, as long as they rely on
the same hash format.
Sharing Hashes are a compact representation of the data that can be easily shared and authenti-
cated. Hence, veriﬁers can delegate computations on someone else’s hashes, or chain multiple
computations using intermediate hashes, without ever seeing or receiving the corresponding
data.
Provenance A record of an input hash, an output hash, and a proof can serve as a succinct
provenance token that can be easily and independently veriﬁed.
Conﬁdentiality The veriﬁer checks arguments on hashes of data that she may never see in plain-
text; hence randomized hashes enable zero-knowledge arguments.
Updates If the hash mechanism also supports eﬃcient updates, that is, given hash(x), one can
compute hash(x(cid:48)) in time that depends only on the diﬀerence between x and x(cid:48), then it also
enables applications with dynamic data and streaming. For instance, a hashed database may
be updated by uploading the new data and locally updating the hash.
Our hash & prove model extends non-interactive proof systems, with an intermediate hash algo-
rithm between the input and the proof veriﬁcation, and with the possibility of proving multiple
2
relations. It is inspired by multi-function veriﬁable computation [41, 23], with relations instead of
functions so that we can capture more general use cases, notably those where the prover provides
its own (private) input to the computation.
Instantiating Hash & Prove Now equipped with a model for outsourcing multiple computations
on authenticated data, we survey how existing work could be used to instantiate HP schemes. In
particular, we observe that existing solutions have limitations either in eﬃciency or in generality.
Some prior work [11, 26, 5, 16] considers the idea of proving the correctness of a computation
on data succinctly represented by a hash. This approach consists of encoding the veriﬁcation of the
hash as part of a relation for the underlying proof system. Namely, if y = f (x) is the statement
to be proved, then one actually proves an extended statement of the form y = f (x) ∧ σ = hash(x),
essentially treating x as an additional witness. We henceforth refer to this method as an inner
encoding. Inner encodings are simple and general, and can also be extended to more general data
encodings such as Merkle trees or authenticated data structures (ADS) [44, 22]. On the other hand,
inner encodings incur a signiﬁcant overhead for the prover—indeed, unless hash is carefully tuned
to the proof system, its veriﬁable evaluation on large inputs may dominate the prover costs.
Other works address reusability and succinct data representation by using diﬀerent data encod-
ing approaches that we will call outer encodings. The basic idea of outer encodings is that proofs
are produced for the original statement, e.g., y = f (x), and are linked to the encoded data x using
some external mechanism. Works that can be explained under this approach are commit & prove
schemes [33, 17, 20] and homomorphic authenticators [4, 18, 29]. While we discuss them in detail
in Section 7, the main observation is that all these works fall short in generality; i.e., they limit the
class of computations that can be executed on an hash value. While commit & prove schemes can
achieve greater generality by using universal relations (as, e.g., in [5, 7, 9]), this typically entails a
signiﬁcant penalty in concrete eﬃciency.
New Hash & Prove Constructions Our main technical contributions are eﬃcient, general HP
constructions. Compared to general inner encoding solutions, ours incurs minimal overheads for
the prover. Compared to prior outer encoding solutions, ours is fully general, in the sense that
one can hash data ﬁrst, without any restriction on the functions that may later be executed and
veriﬁed on it.
We instantiate multi-relation hash & prove schemes both in the public and designated veriﬁer
settings. Our solutions are built in a semi-generic fashion by combining
(1) a veriﬁable computation (VC) or succinct non interactive argument (SNARG) scheme, and
(2) an HP scheme for simple, speciﬁc computations.
At a high level, our construction uses an outer data encoding, where general computation integrity
is handled by (1), whereas data authentication and linking to the computation is handled by (2).
As expected from an outer approach, this combination does not add any overhead in the use of (1),
and the overhead introduced by (2) can be very low.
form cx = (cid:81)
input-processing part of SNARG veriﬁcation, cx =(cid:81)
More speciﬁcally, for (1) we use any scheme where the input-processing part of the veriﬁcation
consists of a multi-exponentiation, that is, anything resembling a Pedersen commitment of the
xi, a property of virtually all modern, eﬃcient SNARGs [40, 7, 20, 9, 31]. Our
generic construction then outsources to the prover the original computation of (1) as well as the
xi. We then ask the prover to show the
correctness of cx using the auxiliary HP scheme (2). To this end, we only need a scheme that
handles multi-exponentiation computations. We propose our own eﬃcient constructions for such
HP schemes. For the designated veriﬁer setting, we adapt a multi-function VC scheme from prior
i Fi
i Fi
3
work [23]. For public veriﬁability, we develop a new scheme, which requires new techniques to
achieve adaptive security.
Our analysis in Section 6 shows that, in comparison to the inner encoding solution mentioned
earlier, our HP scheme yields a 1, 400× speed-up for provers, as well as public (proving) keys that
are shorter by the same factor.
Speeding up Hashing and Veriﬁcation As mentioned above, VC schemes involve a veriﬁcation
eﬀort linear in the size of the I/O. Concretely, this veriﬁcation step is expensive because it relies on
public-key operations (e.g., a few elliptic-curve multiplications for each word of I/O). With Hash &
Prove, this linear work is ﬁrst shifted to computing the hash, and then amortized across multiple
computations, but the hash still has to be computed once.
When using inner encodings, one can choose standard, very eﬃcient hash functions such as
SHA2, which considerably reduces the eﬀort of the veriﬁer, at the expenses of the prover. Other
trade-oﬀs between veriﬁer and prover costs are possible, e.g., by using algebraic hash construc-
tions [1, 8, 16]. When using outer encodings, the choice of a hash function is more constrained. For
instance, in Geppetto or in our HP scheme, the encoding still consists of a multi-exponentiation
(i.e., n elliptic-curve multiplications where n is the size of the input).
As another contribution, we provide a technique to outsource such (relatively) expensive data
encodings, at a moderate additional cost for the prover, while requiring only a trivial amount of
linear work from the veriﬁer: an arbitrary (fast) hash such as SHA2, and a few cheap ﬁeld additions
and multiplications, instead of elliptic curve operations. Concretely, this technique saves two orders
of magnitude in veriﬁcation time. It applies not only to our HP scheme, but also to existing VC
systems [40, 20, 9].
Other Data Encodings In our presentation, we focus on plain hashes as a simple data encoding
for all I/O, but many alternatives and variations are possible, depending on the needs of a given
application. As a ﬁrst example, the I/O can naturally be partitioned into several variables, each
independently hashed and veriﬁed, to separate inputs from diﬀerent parties, or with diﬀerent live
spans. (In a data-intensive application, for instance, one may use a hash for the whole database, and
a separate hash for the query and its response.) More advanced examples include authenticated data
structures, and more speciﬁc tools such as accumulators. To illustrate potential extensions of our
work, we show that the HP model, and our generic HP construction, can be extended to work with
such outer encodings. Concretely, we consider accumulators [37] and polynomial commitments [32],
with set operations [38] and batch openings as restricted proof systems, respectively. By adapting
our constructions, we obtain a new accumulate & prove system.
Contents The paper is organized as follows: Section 2 deﬁnes our notations, reviews assumptions
we rely on, and recalls deﬁnitions of succinct non-interactive argument systems. Section 3 deﬁnes
our hash & prove model, shows that some of the existing work satisﬁes it, and discusses their
overhead for the prover. Section 4 presents our eﬃcient HP construction and instantiates it for
public and designated veriﬁer settings. Section 5 presents the deﬁnition and construction of a
hash & prove variant that supports hash outsourcing. Section 6 analyze the performance of our
constructions. Section 7 discusses related work.
In the Appendix, we provide auxiliary deﬁnitions, detailed proofs, and an extension of our work
from hashes to cryptographic accumulators.
4
2 Preliminaries
1
Notation. Given two functions f, g : N → [0, 1] we write f (λ) ≈ g(λ) when |f (λ)− g(λ)| = λ−ω(1).
In other words, for all k, there exists an integer n0 such that for all λ > n0, we have |f (λ)− g(λ)| <
λk . We say that f is negligible when f (λ) ≈ 0.
Algebraic Tools and Complexity Assumptions. All our constructions make use of asym-
metric bilinear prime-order groups Gλ = (e, G1, G2, GT , p, g1, g2) with an admissible bilinear map
e : G1 × G2 → GT . We use ﬁxed groups for every value of the security parameter; this lets us
compose schemes that use them without requiring a joint setup algorithm. Even when pairings are
not required, we deﬁne schemes for group G1 and generator g1 to anticipate their usage in later
constructions. Our constructions are proven secure under the following assumptions.
Assumption 1 (Strong External Diﬃe-Hellman [39]). The Strong External Diﬃe-Hellman (SXDH)
assumption holds if every p.p.t. adversary solves the Decisional Diﬃe-Hellman (DDH) problems in
G1 and G2 only with a negligible advantage.
We introduce the Flexible co-CDH assumption and prove that it is implied by the above SXDH
assumption.
Assumption 2 (Flexible co-CDH). The Flexible co-CDH assumption holds if, given (g2, g2
g2
negligible probability.
$←− G2, a $←− Zp, every p.p.t. adversary outputs a tuple (h, ha) ∈ G1
a) where
2 such that h (cid:54)= 1 only with
Lemma 2.1. Strong External Diﬃe-Hellman implies Flexible co-CDH.
Proof. Given A that solves Flexible co-CDH with non-negligible advantage, we show how to build
an adversary A(cid:48) for DDH in G2. A(cid:48) is given a DDH instance (g, ga, gb, C) ∈ G4
2 and has to decide
if C = gab. A(cid:48) runs A with input (g, ga). Let A output (h, ha). Then A(cid:48) can check if C = gab
by checking if e(ha, gb) ?= e(h, C) holds. Hence A(cid:48) succeeds in solving the DDH instance with A’s
success probability.
For extractability, we optionally require the following assumption parameterized by hash size
n:
Assumption 3 (Bilinear n-Knowledge of Exponent). The Bilinear n-Knowledge of Exponent as-