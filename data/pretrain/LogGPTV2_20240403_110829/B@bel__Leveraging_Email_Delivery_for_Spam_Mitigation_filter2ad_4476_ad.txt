In particular, a legitimate user wants to know
whether the delivery of her emails failed (e.g., due to a
typo in the email address). In such a case, the user wants
to correct the mistake and send the message again. In
contrast, a spammer usually sends emails in batches, and
typically does not care about sending an email again in
case of failure.
Nonetheless, there are three main pieces of informa-
tion related to server feedback that a rational spammer
is interested in: (i) whether the delivery failed because
the IP address of the bot is blacklisted; (ii) whether the
delivery failed because of speciﬁc policies in place at the
receiving end (e.g., greylisting); (iii) whether the deliv-
ery failed because the recipient address does not exist. In
all three cases, the spammer can leverage the information
obtained from the mail server to make his operation more
effective and proﬁtable. In the case of a blacklisted bot,
he can stop sending spam using that IP address, and wait
for it to be whitelisted again after several hours or days.
Empirical evidence suggests that spammers already col-
lect this information and act accordingly [38]. If the re-
cipient server replied with an SMTP non-critical error
(i.e., the ones used in greylisting), the spammer can send
the email again after some minutes to comply with the
recipient’s policy.
The third case, in which the recipient address does
not exist, is the most interesting, because it implies that
the spammer can permanently remove that email address
from his email lists, and avoid using it during subsequent
campaigns. Recent research suggests that bot feedback
is an important part of a spamming botnet operation. For
example, Stone-Gross et al. [38] showed that about 35%
of the email addresses used by the Cutwail botnet were
in fact non-existent. By leveraging the server feedback
received by the bots, a rational botmaster can get rid
of those non-existing addresses, and optimize his spam-
ming performance signiﬁcantly.
Breaking the Loop: Providing False Responses to
Spam Emails. Based on these insights, we want to
study how we can manipulate the SMTP delivery pro-
cess of bots to inﬂuence their sending behavior. We want
to investigate what would happen if mail servers started
giving erroneous feedback to bots. In particular, we are
interested in the third case, since inﬂuencing the ﬁrst two
pieces of information has only a limited, short-term im-
pact on a spammer. However, if we provide false in-
formation about the status of a recipient’s address, this
leads to a double bind for the spammer: on the one hand,
if a spammer considers server feedback, he will remove
a valid recipient address from his email list. Effectively,
this leads to a reduced number of spam emails received at
this particular address. On the other hand, if the spammer
does not consider server feedback, this reduces the effec-
tiveness of his spam campaigns since emails are sent to
non-existent addresses. In the long run, this will signiﬁ-
cantly degrade the freshness of his email lists and reduce
the number of successfully sent emails. In the following,
we discuss how we can take advantage of this situation.
As a ﬁrst step, we need to identify that a given SMTP
conversation belongs to a bot. To this end, a mail server
can either use traditional, IP-based blacklists or lever-
age the analysis of SMTP dialects introduced previously.
Once we have identiﬁed a bot, a mail server can (instead
of closing the connection) start sending erroneous feed-
back to the bot, which will relay this information to the
C&C infrastructure. Speciﬁcally, the mail server could,
for example, report that the recipient of that email does
not exist. By doing this, the email server would lead
the botmaster to the lose-lose situation discussed before.
For a rational botmaster, we expect that this technique
would reduce the amount of spam the email address re-
ceives. We have implemented this approach as a second
instance of our technique to leverage the email delivery
for spam mitigation and report on the empirical results in
Section 7.3.
7 Evaluation
In this section, we evaluate the effectiveness of our ap-
proach. First, we describe our analysis environment.
Then, we evaluate both the dialects and the feedback ma-
nipulation techniques. Finally, we analyze the limitations
and the possible evasion techniques against our system.
7.1 Analysis Environment
We implemented our approach in a tool, called B@bel.
B@bel runs email clients (legitimate or malicious) in
virtual machines, and applies the learning techniques ex-
plained in Section 4 to learn the SMTP dialect of each
client. Then, it leverages the learned dialects to build a
decision machine MD, and uses it to perform malware
classiﬁcation or spam mitigation.
The ﬁrst component of B@bel is a virtual machine
zoo. Each of the virtual machines in the zoo runs a dif-
ferent email client 4. Clients can be legitimate email pro-
grams, mail transfer agents, or spambots.
The second component of B@bel is a gateway, used to
conﬁne suspicious network trafﬁc. Since the clients that
we run in the virtual machines are potentially malicious,
we need to make sure that they do not harm the outside
world. To this end, while still allowing the clients to
connect to the Internet, we use restricting ﬁrewall rules,
and we throttle their bandwidth, to make sure that they
will not be able to launch denial of service attacks. Fur-
thermore, we sinkhole all SMTP connections, redirecting
them to local mail servers under our control.
We use three different mail servers in B@bel. The
ﬁrst email server is a regular server that speaks plain
SMTP, and will perform passive observation of the
client’s SMTP conversation. The second server is instru-
4We used VirtualBox as our virtualization environment, and Win-
dows XP SP3, Windows Server 2008, Windows 7, Ubuntu Linux 11.10,
or Mac OS X Lion as operating systems on the virtual machines, de-
pending on the operating system needed to run each of the legitimate
clients or MTAs. We used Windows XP SP3 to run the malware sam-
ples
mented to perform active probing, as described in Sec-
tion 4.2. Finally, the third server is conﬁgured to always
report to the client that the recipient of an email does not
exist, and is used to study how spammers use the feed-
back they receive from their bots.
The third component of B@bel is the learner. This
component analyzes the active or passive observations
generated between the clients in the zoo and the mail
servers, learns an SMTP dialect for each client, and gen-
erates the decision state machine using the various di-
alects as input, as explained in Section 5. According
to the task we want to perform (dialect classiﬁcation or
spam mitigation), B@bel tags the states in the decision
state machine with the appropriate gain.
The last component of B@bel is the decision maker.
This component analyzes an SMTP conversation, by
either passively observing it or by impersonating the
server, and makes a decision about which dialect is spo-
ken by the client, using the process described in Sec-
tion 5.2.
7.2 Evaluating the Dialects
Evaluating Dialects for Classiﬁcation We trained
B@bel by running active probing on a variety of pop-
ular Mail User Agents, Mail Transfer Agents, and bot
samples. Table 1 lists the clients we used for dialect
learning. Since we are extracting dialects by looking
at the SMTP conversations only, B@bel is agnostic to
the family a bot belongs to. However, for legibility pur-
poses, Table 1 groups bots according to the most fre-
quent label assigned by the anti-virus products deployed
by VirusTotal [44]. Our dataset contained 13 legitimate
MUAs and MTAs, and 91 distinct malware samples5. We
picked the spambot samples to be representative of the
largest active spamming botnets according to a recent re-
port [26] (the report lists Lethic, Cutwail, Mazben, Cut-
wail, Tedroo, Bagle). We also picked worm samples that
spread through email, such as Mydoom. In total, the mal-
ware samples we selected belonged to 11 families. The
dialect learning phase resulted in a total of 60 dialects.
We explain the reason for the high number of discovered
dialects later in this section.
We then wanted to assess whether a dialect (i.e., a
state machine) is unique or not. For each combination
of dialects , we merged their state machines to-
gether as explained in Section 5.1. We consider two di-
alects as distinct if any state of the merged state machine
has two different labels in the label table for the dialects
d1 and d2, or if any state has a single possible dialect in
it.
The results show that the dialects spoken by the legit-
imate MUAs and MTAs are distinct from the ones spo-
5The MD5 checksums of the malware samples are available at
http://cs.ucsb.edu/~gianluca/files/babel.txt
Mail User Agents
Eudora, Opera, Outlook 2010,
Outlook Express, Pegasus,
The Bat!, Thunderbird, Windows Live Mail
Mail Transfer Agents Bots (by AV labels)
Exchange 2010,
Exim, Postﬁx, Qmail,
Sendmail
Waledac, Donbot, Grum, Klez
Buzus, Bagle, Lethic, Cutwail,
Mydoom, Mazben, Tedroo
Table 1: MTAs, MUAs, and bots used to learn dialects.
ken by the bots. By analyzing the set of dialects spoken
by legitimate MUAs and MTAs, we found that they all
speak distinct dialects, except for Outlook Express and
Windows Live Mail. We believe that Microsoft used the
same email engine for these two products.
The 91 malware samples resulted in 48 unique di-
alects. We manually analyzed the spambots that use the
same dialect, and we found that they always belong to the
same family, with the exception of six samples. These
samples were either not ﬂagged by any anti-virus at the
time of our analysis, or match a dropper that downloaded
the spambot at a later time [8]. This shows that B@bel
is able to classify spambot samples by looking at their
email behavior, and label them more accurately than anti-
virus products.
We then wanted to understand the reason for the high
number of dialects we discovered. To this end, we con-
sidered clusters of malware samples that were talking the
same dialect. For each cluster, we assigned a label to it,
based on the most common anti-virus label among the
samples in the cluster. All the clusters were unique, with
the exception of eleven clusters marked as Lethic and two
clusters marked as Mydoom. By manual inspection, we
found that Lethic randomly closes the connection after
sending the EHLO message. Since our dialect state ma-
chines are nondeterministic, our approach handles this
case, in principle. However, in some cases, this non-
deterministic behavior made it impossible to record a re-
ply for a particular test case during our active probing.
We found that each cluster labeled as Lethic differs for at
most ﬁve non-recorded test cases with every other Lethic
cluster. This gives us conﬁdence to say that the dialect
spoken by Lethic is indeed unique. For the two clusters
labeled as Mydoom, we believe this is a common label
assigned to unknown worms.
In fact, the two dialects
spoken by the samples in the clusters are very different.
This is another indicator that B@bel can be used to clas-
sify spamming malware in a more precise fashion than is
possible by relying on anti-virus labels only.
Evaluating Dialects for Spam Detection To evaluate
how the learned dialects can be used for spam detection,
we collected the SMTP conversations for 621,919 email
messages on four mail servers in our department, span-
ning 40 days of activity.
For each email received by the department servers, we
extracted the SMTP conversation associated with it, and
then ran B@bel on it to perform spam detection. To this
end, we used the conversations logged by the Anubis sys-
tem [4] during a period of one year (corresponding to
7,114 samples) to build the bot dialects, and the dialects
learned in Section 7.2 for MUAs and MTAs as legitimate
clients. In addition, we manually extracted the dialects
spoken by popular web mail services from the conversa-
tions logged by our department mail servers, and added
them to the legitimate MTAs dialects. Note that, since
the goal of this experiment is to perform passive spam
detection, learning the dialects by passively observing
SMTP conversations is sufﬁcient.
During our experiment, B@bel marked any conversa-
tion as spam if, at the end of the conversation, the di-
alects in CD were all associated with bots. Furthermore,
if the dialects in CD were all associated with MUAs
or MTAs, B@bel marked the conversation as legitimate
(ham). If there were both good and malicious clients in
CD, B@bel did not make a decision. Finally, if the deci-
sion state machine did not recognize the SMTP conversa-
tion at all, B@bel considered that conversation as spam.
This could happen when we observe a conversation from
a client that was not in our training set. As we will show
later, considering it as spam is a reasonable assumption,
and is not a major source of false positives.
In total, B@bel ﬂagged 260,074 conversations as
spam, and 218,675 as ham. For 143,170 emails, B@bel
could not make a decision, because the decision pro-
cess ended up in a state where there were both legitimate
clients and bots in CD.
To verify how accurate our decisions were, we used
a number of techniques. First, we checked whether the
email was blocked by the department mail servers in
the ﬁrst place. These servers have a common conﬁgu-
ration, where incoming emails are ﬁrst checked against
an IP blacklist, and then against more expensive content-
analysis techniques. In particular, these servers used a
commercial blacklist for discarding emails coming from
known spamming IP addresses, and SpamAssassin and
ClamAV for content analysis. Any time one of these
techniques and B@bel agreed on ﬂagging a conversa-
tion as spam, we consider this as a true positive of our
system. We also consider as a true positive those con-
versations B@bel marked as spam, and that lead to an
NXDOMAIN or to a timeout when we tried to resolve the
domain associated to the sender email address. In addi-
tion, we checked the sender IP address against 30 addi-
tional IP blacklists6, and considered any match as a true
positive. According to this ground truth, the true positive
rate for the emails B@bel ﬂagged as being sent by bots is
99.32%. Surprisingly, 98% of the 24,757 conversations
that were not recognized by our decision state machine
were ﬂagged as spam by existing methods. This shows
that, even if the set of clients from which B@bel learned
the dialects from is not complete, there are no widely-
used legitimate clients we missed, and that it is safe to
consider any conversation generated by a non-observed
dialect as spam. For the remaining 2,074 emails that
B@bel ﬂagged as spam, we could not assess if they were