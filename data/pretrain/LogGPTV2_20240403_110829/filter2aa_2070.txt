Run ChatGLM-6B
Finetune your ChatGLM from scratch!
Run ChatGLM-6B
2023/05/21
1
TOC
GLM
Fintune
Prerequisite: Mixed Precision, ZeRO
P-tuning
Full Parameter
LoRA
Deploy with Gradio
Run ChatGLM-6B
2
GLM: Pretraining
Run ChatGLM-6B
3
OpenSource GLM Series
GLM Github Paper
GLM-130B Github Paper
ChatGLM-6B Github Blog
can be finetuned on consumer-grade GPUs
Run ChatGLM-6B
4
Demo
Download ChatGLM-6B checkpoints
Inference with ChatGLM-6B
Finetuning
P-Tuning (1 
 RTX3090 !)
LoRA (1 
 RTX3090 !)
Full Parameter
Run ChatGLM-6B
5
Demo environemnt
GPU: NVIDIA GeForce RTX 3090
This is not a must. 7GB is sufficient for P-tuning + 4-bit
quantization
Image: nvidia-pytorch:22.08-py3
Change your pip source
pip config set global.extra-index-url https://pypi.tuna.tsinghua.edu.cn/simple
# Writing to /opt/conda/pip.conf
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple
# Writing to /opt/conda/pip.conf
pip config set global.trusted-host https://pypi.tuna.tsinghua.edu.cn/simple
# Writing to /opt/conda/pip.conf
Run ChatGLM-6B
6
Download Checkpoint
Option1: From HuggingFace Repo
Step 1: Install git-lfs , Get Started
Verify installation
git lfs install
# > Git LFS initialized.
Step 2: Setup a ...
Run ChatGLM-6B
7
Step 3: clone the repo
git clone https://huggingface.co/THUDM/chatglm-6b
# Cloning into 'chatglm-6b'...
# remote: Enumerating objects: 522, done.
# remote: Counting objects: 100% (522/522), done.
# remote: Compressing objects: 100% (495/495), done.
# remote: Total 522 (delta 321), reused 54 (delta 27), pack-reused 0
# Receiving objects: 100% (522/522), 158.52 KiB | 823.00 KiB/s, done.
# Resolving deltas: 100% (321/321), done.
Seems to stuck here is expected behaviour
It's downloading the checkpoint ...
Use bwm-ng  to monitor network traffic
Run ChatGLM-6B
8
Option 2: Downloading Manually
Useful when downloading from huggingface repo is slow
Step 1: clone the repo, skip large files
GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/THUDM/chatglm-6b
# Cloning into 'chatglm-6b'...
# remote: Enumerating objects: 522, done.
# remote: Counting objects: 100% (522/522), done.
# remote: Compressing objects: 100% (495/495), done.
# remote: Total 522 (delta 321), reused 54 (delta 27), pack-reused 0
# Receiving objects: 100% (522/522), 159.22 KiB | 1.37 MiB/s, done.
# Resolving deltas: 100% (321/321), done.
Run ChatGLM-6B
9
Step 2: Download large files from Tsinghua Cloud
download one by one is painful ...
git clone PI:EMAIL:chenyifanthu/THU-Cloud-Downloader.git
cd THU-Cloud-Downloader
pip install argparse requests tqdm
python main.py \
    --link https://cloud.tsinghua.edu.cn/d/fb9f16d6dc8f482596c2/ \
    --save ../chatglm-6b/
# Start downloading? [y/n] y
# [1/11] Downloading File: ../chatglm-6b/LICENSE
# 100%|██████| 11.1k/11.1k [00:00=1.10  manually according to your CUDA Version
See Previous Versions
2. Run
pip install -r requirements.txt
Run ChatGLM-6B
11
Play with ChatGLM-6B in CLI
Specify model path
# cli_demo.py
tokenizer = AutoTokenizer\
    .from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
model = AutoModel\
    .from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)\
    .half().cuda()
Run
python cli_demo.py
Run ChatGLM-6B
12
Play with ChatGLM-6B in Gradio
Specify model path
Run
python web_demo.py
Interact with ChatGLM-6B in a browser 
VSCode port forwarding can be useful
Run ChatGLM-6B
13
Fine-tuning: Mixed Precision
Run ChatGLM-6B
14
Fine-tuning: Mixed Precision
Run ChatGLM-6B
15
Fine-tuning: Mixed Precision
Run ChatGLM-6B
16
ZeRO: why not DP or MP?
Run ChatGLM-6B
Model states often consume the largest amount of memory
during training. DP has good compute/communication efficiency
but poor memory efficiency while MP can have poor
compute/communication efficiency.
DP replicates the entire model states across all data parallel
process resulting in redundant memory consumption; while MP
partition these states to obtain high memory efficiency, but often
result in too finegrained computation and expensive
communication that is less scaling efficient.
“
“
17
ZeRO:Where the memory goes?
Run ChatGLM-6B
18
ZeRO Stages
Run ChatGLM-6B
19
ZeRO Stages
Run ChatGLM-6B
20
P-tuning v2
Saves GPU memory & training time
Similar performace
Run ChatGLM-6B
21
P-tuning v2: Results
Run ChatGLM-6B
22
P-tuning @ ChatGLM-6B
Example: AdGen
Dependencies
pip install rouge_chinese nltk jieba datasets
Dataset
https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1
{
    "content": "类型#上衣*版型#宽松*版型#显瘦*图案#线条*衣样式#衬衫*衣袖型#泡泡袖*衣款式#抽绳",
    "summary": "这件衬衫的款式非常的宽松，利落的线条可以很好的隐藏身材上的小缺点，穿在身上有着很好的显瘦效果。
        领口装饰了一个可爱的抽绳，漂亮的绳结展现出了十足的个性，配合时尚的泡泡袖型，尽显女性甜美可爱的气息。"
}
Run ChatGLM-6B
23
Specify model path, dataset path & device ordinal in train.sh  &
evaluate.sh
Run
bash train.sh
Default we use 4-bit quantization, this may take a while ...
remove --quantization_bit 4  to use fp16
quantizaion
GPU memory
Training Time @ 3k steps
/
13GB
~2hrs
4bit
7GB
~3hrs
Run ChatGLM-6B
24
See results
bash evaluate.sh
This will make generation on the test set
Run ChatGLM-6B
25
Full parameter finetuning
Install deepspeed
pip install deepspeed
Specify model and dataset in ds_train_finetune.sh  and
evaluate_finetune.sh
3090 is in sufficient for this task ...
Run
bash ds_train_finetune.sh
Run ChatGLM-6B
26
FAQs: Try just rerun
Traceback (most recent call last):
  File "main.py", line 435, in 
    main()
  File "main.py", line 374, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/root/ChatGLM-6B/ptuning/trainer.py", line 1635, in train
    return inner_training_loop(
  File "/root/ChatGLM-6B/ptuning/trainer.py", line 1704, in _inner_training_loop
    deepspeed_engine, optimizer, lr_scheduler = deepspeed_init(
  File "/opt/conda/lib/python3.8/site-packages/transformers/deepspeed.py", line 378, in deepspeed_init
    deepspeed_engine, optimizer, _, lr_scheduler = deepspeed.initialize(**kwargs)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/__init__.py", line 165, in initialize
    engine = DeepSpeedEngine(args=args,
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/engine.py", line 266, in __init__
    self._configure_distributed_model(model)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/engine.py", line 1066, in _configure_distributed_model
    self.data_parallel_group = groups._get_data_parallel_group()
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/utils/groups.py", line 327, in _get_data_parallel_group
    return _clone_world_group()
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/utils/groups.py", line 315, in _clone_world_group
    _WORLD_GROUP = dist.new_group(ranks=range(dist.get_world_size()))
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/comm/comm.py", line 179, in new_group
    return cdb.new_group(ranks)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/comm/torch.py", line 234, in new_group
    return torch.distributed.new_group(ranks)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 3006, in new_group
    _store_based_barrier(global_rank, default_store, timeout)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 239, in _store_based_barrier
    store.add(store_key, 1)
RuntimeError: Broken pipe
Run ChatGLM-6B
27
Contention? add a lock
# Load pretrained model and tokenizer
with FileLock("model.lock"):
    config = AutoConfig.from_pretrained(model_args.model_name_or_path, trust_remote_code=True)
...
with FileLock("model.lock"):
    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, trust_remote_code=True)
if model_args.ptuning_checkpoint is not None:
    ...
else:
    with FileLock("model.lock"):
        model = AutoModel.from_pretrained(model_args.model_name_or_path, config=config, trust_remote_code=True)
Just don't start simultaneously
Run ChatGLM-6B
28
LoRA
Hu, Edward J., et al. "LoRA: Low-Rank Adaptation of Large Language
Models." International Conference on Learning Representations.
Run ChatGLM-6B
29
LoRA
Suppose pre-trained weight 
, input 
Fine-tuning: 
 is not necessarily full-rank!
LoRA:
suppose 
 has rank , where 
trainable parameters are significantly reduced
Run ChatGLM-6B
30
LoRA
Run ChatGLM-6B
31
LoRA @ ChatGLM-6B
We procede the demo with a community implementation
https://github.com/yuanzhoulvpi2017/zero_nlp
Ref:
It's implemented on a previous version of ChatGLM-6B
Download checkpoint a previous archive from HuggingFace
git clone https://huggingface.co/yuanzhoulvpi/chatglm6b-dddd
Note that git-lfs  is required
Run ChatGLM-6B
32
LoRA @ ChatGLM-6B
This Notebook demonstrates how to finetune ChatGLM-6B with
LoRA on alpaca_chinese  dataset
Now we show steps to reuse the code and finetune on AdGen
dataset
Understand code behaviour and your requirements
Make modifications accordingly
Sanity check, debug, run
Evaluate
Takes ~15GB GPU memory
Run ChatGLM-6B
33
Thanks
Questions?
Run ChatGLM-6B
34