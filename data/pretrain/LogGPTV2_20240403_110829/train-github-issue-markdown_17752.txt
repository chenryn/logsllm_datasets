**Is this a request for help?** (If yes, you should use our troubleshooting
guide and community support channels, see
http://kubernetes.io/docs/troubleshooting/.):
  * No
**What keywords did you search in Kubernetes issues before filing this one?**
(If you have found any duplicates, you should instead reply there.):
  * kubectl "does not allow access"
* * *
**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):
  * Bug report
**Kubernetes version** (use `kubectl version`):
    Client Version: version.Info{Major:"1", Minor:"4", GitVersion:"v1.4.6", GitCommit:"e569a27d02001e343cb68086bc06d47804f62af6", GitTreeState:"clean", BuildDate:"2016-11-12T05:22:15Z", GoVersion:"go1.7.1", Compiler:"gc", Platform:"darwin/amd64"}
    Server Version: version.Info{Major:"1", Minor:"4", GitVersion:"v1.4.5", GitCommit:"5a0a696437ad35c133c0c8493f7e9d22b0f9b81b", GitTreeState:"clean", BuildDate:"2016-10-29T01:32:42Z", GoVersion:"go1.6.3", Compiler:"gc", Platform:"linux/amd64"}
**Environment** :
  * **Cloud provider or hardware configuration** : 
    * GCP GKE 1.4.5
  * **OS** (e.g. from /etc/os-release): 
    * Not sure, using gci
  * **Kernel** (e.g. `uname -a`): 
    * Not sure, using gci
  * **Install tools** : 
    * GKE web UI
  * **Others** : 
    * N/A
**What happened** :
  * In juggling multiple gcloud accounts with GKE clusters I accidentally used `kubectl` with the wrong application default credentials and it cached bad creds in `~/.kube/config`
  * After pointing gcloud at the right application default credentials and re-running `kubectl`, it continued trying and failing to use the bad creds cached in `~/.kube/config`; at this point I was stuck and confused for a few hours
  * To repair, I had to manually edit `~/.kube/config` to remove `users[].user.auth-provider.config`, and then `kubectl` started working again (or else wait ~1h for the access token to expire and refresh itself)
**What you expected to happen** :
  * After pointing gcloud at the right application default credentials `kubectl` should just work, or at least provide an error msg to the user that can help them figure out that they need to revoke the bad access token and how to do it
**How to reproduce it** (as minimally and precisely as possible):
Good scenario: just works
    # Avoid calling kubectl with wrong application default credentials
    gcloud auth application-default login # -> Auth with user A
    kubectl --context cluster-A version   # Ok + caches good creds
    gcloud auth application-default login # -> Auth with user B
    kubectl --context cluster-B version   # Ok + caches good creds
    # Both now work
    kubectl --context cluster-A version   # Ok, using cached good creds
    kubectl --context cluster-B version   # Ok, using cached good creds
Bad scenario: user makes one mistake and gets stuck
    # Use kubectl with wrong ADC
    gcloud auth application-default login # -> Auth with user A
    kubectl --context cluster-A version   # Ok + caches good creds
    kubectl --context cluster-B version   # Oops + caches bad creds
    gcloud auth application-default login # -> Auth with user B
    kubectl --context cluster-B version   # Fails, stuck on bad creds cached in ~/.kube/config
    kubectl --context cluster-A version   # Still works, using cached good creds
    # User is now stuck and confused with no clear approach to resolve...
    # Tada!
    vim ~/.kube/config                    # Manually remove users[].user.auth-provider.config for cluster-B
    kubectl --context cluster-B version   # Ok + caches good creds
    # Both now work
    kubectl --context cluster-A version   # Ok, using cached good creds
    kubectl --context cluster-B version   # Ok, using cached good creds