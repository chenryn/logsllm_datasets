### 数学符号定义

- **∇y**：向量y关于x的梯度。
- **∇Y**：矩阵Y关于X的导数。
- **P(X)**：离散变量X的概率分布。
- **p(X)**：连续变量或类型未指定变量X的概率分布。
- **X ∼ p**：随机变量X服从概率分布p。
- **E[X]**：随机变量X的期望值。
- **Var[X]**：随机变量X的方差。
- **Cov(X, Y)**：随机变量X和Y之间的协方差。

#### 特殊数学符号
- **D(P∥Q)** 或 **KL(P∥Q)**：两个概率分布P和Q之间的Kullback-Leibler散度。
- **N(x; µ, Σ)**：均值为µ、协方差为Σ的多元高斯分布。

### 强化学习符号

- **s, s'**：状态。
- **a**：动作。
- **r**：奖励。
- **R**：奖励函数。
- **S**：所有非终结状态的集合。
- **S+**：包括终结状态在内的所有状态的集合。
- **A**：动作集。
- **R**：所有可能奖励的集合。
- **P**：转移矩阵。
- **t**：离散时间步。
- **T**：回合内最终时间步。
- **S_t**：时间t的状态。
- **A_t**：时间t的动作。
- **R_t**：时间t的奖励，通常由A_t和S_t决定。
- **G_t**：从时间t开始的回报。
- **G(n)_t**：从时间t开始的n步回报。
- **G(λ)_t**：从时间t开始的λ-回报。
- **π**：策略（决策规则）。
- **π(s)**：在确定性策略π下，状态s时采取的动作。
- **π(a|s)**：在随机性策略π下，状态s时执行动作a的概率。
- **p(s', r | s, a)**：从状态s执行动作a转移到状态s'并获得奖励r的概率。
- **p(s' | s, a)**：从状态s执行动作a转移到状态s'的概率。
- **v_π(s)**：根据策略π，状态s的价值（预期回报）。
- **v*(s)**：根据最优策略，状态s的价值。
- **q_π(s, a)**：根据策略π，在状态s执行动作a的价值。
- **q*(s, a)**：根据最优策略，在状态s执行动作a的价值。
- **V, V_t**：状态价值函数v_π(s)或v*(s)的估计。
- **Q, Q_t**：动作价值函数q_π(s, a)或q*(s, a)的估计。
- **τ**：轨迹，即一系列状态、动作和奖励 (S_0, A_0, R_0, S_1, A_1, R_1, ...)。
- **γ**：奖励折扣因子，γ ∈ [0, 1]。
- **ϵ**：在ε-贪婪策略中选择随机动作的概率。
- **α, β**：步长参数。
- **λ**：资格迹的衰减速率。

### 强化学习中的术语总结

- **R_t**：时间t的奖励。
- **R(τ)**：轨迹τ的γ-折扣化回报，\( R(τ) = \sum_{t=0}^{\infty} \gamma^t R_t \)。
- **p(τ)**：轨迹的概率：
  - 对于MP和MRP：\( p(τ) = ρ(S_0) \prod_{t=0}^{T-1} p(S_{t+1} | S_t) \)，其中ρ(S_0)是起始状态分布。
  - 对于MDP：\( p(τ|π) = ρ(S_0) \prod_{t=0}^{T-1} p(S_{t+1} | S_t, A_t) π(A_t | S_t) \)，其中ρ(S_0)是起始状态分布。
- **J(π)**：策略π的期望回报，\( J(π) = \sum_{\tau} p(τ|π) R(τ) = E_{\tau \sim π}[R(τ)] \)。
- **π***：最优策略，\( π* = \arg\max_{π} J(π) \)。
- **v_π(s)**：策略π下状态s的价值（期望回报）。
- **v*(s)**：最优策略下状态s的价值（期望回报）。
- **q_π(s, a)**：策略π下状态s采取动作a的价值（期望回报）。
- **q*(s, a)**：最优策略下状态s采取动作a的价值（期望回报）。
- **V(s)**：对MRP中从状态s开始的状态价值的估计。
- **V_π(s)**：对MDP中在线状态价值函数的估计，给定策略π，有期望回报 \( V_π(s) ≈ v_π(s) = E_{\tau \sim π}[R(τ) | S_0 = s] \)。
- **Q_π(s, a)**：对MDP下在线动作价值函数的估计，给定策略π，有期望回报 \( Q_π(s, a) ≈ q_π(s, a) = E_{\tau \sim π}[R(τ) | S_0 = s, A_0 = a] \)。
- **V*(s)**：对MDP下最优动作价值函数的估计，根据最优策略，有期望回报 \( V*(s) ≈ v*(s) = \max_{π} E_{\tau \sim π}[R(τ) | S_0 = s] \)。
- **Q*(s, a)**：对MDP下最优动作价值函数的估计，根据最优策略，有期望回报 \( Q*(s, a) ≈ q*(s, a) = \max_{π} E_{\tau \sim π}[R(τ) | S_0 = s, A_0 = a] \)。
- **A_π(s, a)**：状态s和动作a的优势估计函数 \( A_π(s, a) = Q_π(s, a) - V_π(s) \)。
- **v_π(s)** 和 **q_π(s, a)** 的关系：\( v_π(s) = E_{a \sim π}[q_π(s, a)] \)。
- **v*(s)** 和 **q*(s, a)** 的关系：\( v*(s) = \max_a q*(s, a) \)。
- **a*(s)**：状态s下根据最优动作价值函数得到的最优动作 \( a*(s) = \arg\max_a q*(s, a) \)。
- **贝尔曼方程**：
  - 在线状态价值函数：\( v_π(s) = E_{a \sim π(·|s), s' \sim p(·|s,a)}[R(s, a) + γv_π(s')] \)。
  - 在线动作价值函数：\( q_π(s, a) = E_{s' \sim p(·|s,a)}[R(s, a) + γE_{a' \sim π(·|s')}[q_π(s', a')]] \)。
  - 最优状态价值函数：\( v*(s) = \max_a E_{s' \sim p(·|s,a)}[R(s, a) + γv*(s')] \)。
  - 最优动作价值函数：\( q*(s, a) = E_{s' \sim p(·|s,a)}[R(s, a) + γ\max_{a'} q*(s', a')] \)。

### 目录

#### 基础部分
1. 深度学习入门
   1.1 简介
   1.2 感知器
   1.3 多层感知器
   1.4 激活函数
   1.5 损失函数
   1.6 优化
      1.6.1 梯度下降和误差反向传播
      1.6.2 随机梯度下降和自适应学习率
      1.6.3 超参数筛选
   1.7 正则化
      1.7.1 过拟合
      1.7.2 权重衰减
      1.7.3 Dropout
      1.7.4 批标准化
      1.7.5 其他缓解过拟合的方法
   1.8 卷积神经网络
   1.9 循环神经网络
   1.10 深度学习实现示例
      1.10.1 张量和梯度
      1.10.2 定义模型
      1.10.3 自定义层
      1.10.4 多层感知器：MNIST数据集上的图像分类
      1.10.5 卷积神经网络：CIFAR-10数据集上的图像分类
      1.10.6 序列到序列模型：聊天机器人

2. 强化学习入门
   2.1 简介
   2.2 在线预测和在线学习
      2.2.1 简介
      2.2.2 随机多臂赌博机
      2.2.3 对抗多臂赌博机
      2.2.4 上下文赌博机
   2.3 马尔可夫过程
      2.3.1 简介
      2.3.2 马尔可夫奖励过程
      2.3.3 马尔可夫决策过程
      2.3.4 贝尔曼方程和最优性
      2.3.5 其他重要概念
   2.4 动态规划
      2.4.1 策略迭代
      2.4.2 价值迭代
      2.4.3 其他DP方法：异步DP、近似DP和实时DP