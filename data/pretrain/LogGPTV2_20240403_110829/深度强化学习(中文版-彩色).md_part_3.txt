∇ y gradientofywithrespecttox，向量的梯度
x
∇ y matrixderivativesofywithrespecttoX，矩阵的导数
X
P(X) aprobabilitydistributionoveradiscretevariable，离散变量的概率分布
p(X) aprobabilitydistributionoveracontinuousvariable,oroveravariablewhosetypehas
notbeenspecified，连续变量（或者未定义连续或者离散的变量）的概率分布
X ∼p therandomvariableX hasdistribution，随机变量X 满足概率分布p
E[X] expectationofarandomvariable，随机变量的期望
Var[X] varianceofarandomvariable，随机变量的方差
Cov(X,Y) covarianceoftworandomvariables，两个随机变量的协方差
XVII
数学符号
D (P∥Q) Kullback-LeiblerdivergenceofP andQ，两个概率分布的KL散度
KL
N(x;µ,Σ) GaussiandistributionoverxwithmeanµandcovarianceΣ，平均值为µ且协方差
为Σ的多元高斯分布
强化学习符号
s,s′ states，状态
a action，动作
r reward，奖励
R rewardfunction，奖励函数
S setofallnon-terminalstates，非终结状态
S+ setofallstates,includingtheterminalstate，全部状态，包括终结状态
A setofactions，动作集合
R setofallpossiblerewards，奖励集合
P transitionmatrix，转移矩阵
t discretetimestep，离散时间步
T finaltimestepofanepisode，回合内最终时间步
S stateattimet，时间t的状态
t
A actionattimet，时间t的动作
t
R rewardattimet,typicallydue,stochastically,toA andS ，时间t的奖励，通常为
t t t
随机量，且由A 和S 决定
t t
G returnfollowingtimet，回报
t
G(n) n-stepreturnfollowingtimet，n步回报
t
Gλ λ-returnfollowingtimet，λ-回报
t
π policy,decision-makingrule，策略
π(s) actiontakeninstatesunderdeterministicpolicyπ，根据确定性策略π，状态s时
的动作
XVIII
数学符号
π(a|s) probabilityoftakingactionainstatesunderstochasticpolicyπ，根据随机性策略
π，状态s时执行动作a的概率
p(s′,r|s,a) probabilityoftransitioningtostates′,withrewardr,fromstatesandactiona，根据
状态s和动作a，使得状态转移成s′且获得奖励r的概率
p(s′|s,a) probabilityoftransitioningtostates′,fromstatestakingactiona，根据状态s和动
作a，使得状态转移成s′的概率
v (s) valueofstatesunderpolicyπ(expectedreturn)，根据策略π，状态s的价值（回
π
报期望）
v∗(s) valueofstatesundertheoptimalpolicy，根据最优策略，状态s的价值
q (s,a) valueoftakingactionainstatesunderpolicyπ，根据策略π，在状态s时执行动
π
作a的价值
q∗(s,a) valueoftakingactionainstatesundertheoptimalpolicy，根据最优策略，在状态
s时执行动作a的价值
V,V estimatesofstate-valuefunctionv π(s)orv∗(s)，状态价值函数的估计
t
Q,Q estimatesofaction-valuefunctionq π(s,a)orq∗(s,a)，动作价值函数的估计
t
τ trajectory,whichisasequenceofstates,actionsandrewards,
τ =(S ,A ,R ,S ,A ,R ,···)，状态、动作、奖励的轨迹
0 0 0 1 1 1
γ rewarddiscountfactor,γ ∈[0,1]，奖励折扣因子
ϵ probabilityoftakingarandomactioninϵ-greedypolicy，根据ϵ-贪婪策略，执行随
机动作的概率
α,β step-sizeparameters，步长
λ decay-rateparameterforeligibilitytraces，资格迹的衰减速率
强化学习中术语总结
除了在本书开头的数学符号法则中定义的术语，强化学习中常见内容的相关术语总结如下：
R 是奖励函数，R = R(S ) 是 MRP 中状态 S 的奖励，R = R(S ,A ) 是 MDP 中的奖励，
t t t t t t
S ∈S。
t
P
R(τ)是轨迹τ 的γ-折扣化回报，R(τ)= ∞ γtR 。
t=0 t
XIX
数学符号
p(τ)是轨迹的概率：
Q
– p(τ) = ρ (S ) T−1p(S |S )对于MP和MRP，ρ (S )是起始状态分布（Start-State
0 0 t=0 t+1 t 0 0
Distribution）。
Q
– p(τ|π)=ρ (S ) T−1p(S |S ,A )π(A |S )对于MDP，ρ (S )是起始状态分布。
0 0 t=0 t+1 t R t t t 0 0
J(π)是策略π的期望回报，J(π)= τp(τ|π)R(τ)=E τ∼π[R(τ)]。
π∗是最优策略：π∗ =argmax J(π)。
π
v (s)是状态s在策略π下的价值（期望回报）。
π
v∗(s)是状态s在最优策略下的价值（期望回报）。
q (s,a)是状态s在策略π下采取动作a的价值（期望回报）。
π
q∗(s,a)是状态s在最优策略下采取动作a的价值（期望回报）。
V(s)是对MRP中从状态s开始的状态价值的估计。
Vπ(s)是对MDP中在线状态价值函数的估计，给定策略π，有期望回报：
– Vπ(s)≈v π(s)=E τ∼π[R(τ)|S =s]
0
Qπ(s,a)是对MDP下在线动作价值函数的估计，给定策略π，有期望回报：
– Qπ(s,a)≈q π(s,a)=E τ∼π[R(τ)|S =s,A =a]
0 0
V∗(s)是对MDP下最优动作价值函数的估计，根据最优策略，有期望回报：
– V∗(s)≈v∗(s)=max πE τ∼π[R(τ)|S =s]
0
Q∗(s,a)是对MDP下最优动作价值函数的估计，根据最优策略，有期望回报：
– Q∗(s,a)≈q∗(s,a)=max πE τ∼π[R(τ)|S =s,A =a]
0 0
Aπ(s,a)是对状态s和动作a的优势估计函数：
– Aπ(s,a)=Qπ(s,a)−Vπ(s)
在线状态价值函数v (s)和在线动作价值函数q (s,a)的关系：
π π
– v π(s)=E a∼π[q π(s,a)]
最优状态价值函数v∗(s)和最优动作价值函数q∗(s,a)的关系：
– v∗(s)=max aq∗(s,a)
a∗(s)是状态s下根据最优动作价值函数得到的最优动作：
– a∗(s)=argmax aq∗(s,a)
对于在线状态价值函数的贝尔曼方程：
– v π(s)=E a∼π(·|s),s′∼p(·|s,a)[R(s,a)+γv π(s′)]
对于在线动作价值函数的贝尔曼方程：
– q π(s,a)=E s′∼p(·|s,a)[R(s,a)+γE a′∼π(·|s′)[q π(s′,a′)]]
XX
数学符号
对于最优状态价值函数的贝尔曼方程：
– v∗(s)=max aE s′∼p(·|s,a)[R(s,a)+γv∗(s′)]
对于最优动作价值函数的贝尔曼方程：
– q∗(s,a)=E s′∼p(·|s,a)[R(s,a)+γmax a′q∗(s′,a′)]
XXI
目录
基础部分 1
第1章 深度学习入门 2
1.1 简介 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.2 感知器 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.3 多层感知器 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
1.4 激活函数 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
1.5 损失函数 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
1.6 优化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
1.6.1 梯度下降和误差的反向传播 . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
1.6.2 随机梯度下降和自适应学习率 . . . . . . . . . . . . . . . . . . . . . . . . . . 15
1.6.3 超参数筛选 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
1.7 正则化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
1.7.1 过拟合 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
1.7.2 权重衰减 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
1.7.3 Dropout . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
1.7.4 批标准化 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
1.7.5 其他缓和过拟合的方法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
1.8 卷积神经网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
1.9 循环神经网络 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
1.10 深度学习的实现样例 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
1.10.1 张量和梯度 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
1.10.2 定义模型 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
1.10.3 自定义层 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
1.10.4 多层感知器：MNIST数据集上的图像分类 . . . . . . . . . . . . . . . . . . . 33
XXII
目录
1.10.5 卷积神经网络：CIFAR-10数据集上的图像分类 . . . . . . . . . . . . . . . . 35
1.10.6 序列到序列模型：聊天机器人 . . . . . . . . . . . . . . . . . . . . . . . . . . 36
第2章 强化学习入门 43
2.1 简介 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
2.2 在线预测和在线学习 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
2.2.1 简介 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
2.2.2 随机多臂赌博机 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
2.2.3 对抗多臂赌博机 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
2.2.4 上下文赌博机 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
2.3 马尔可夫过程 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
2.3.1 简介 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
2.3.2 马尔可夫奖励过程 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
2.3.3 马尔可夫决策过程 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
2.3.4 贝尔曼方程和最优性 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
2.3.5 其他重要概念 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
2.4 动态规划 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
2.4.1 策略迭代 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
2.4.2 价值迭代 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
2.4.3 其他DPs：异步DP、近似DP和实时DP . . . . . . . . . . . . . . . . . . . 68