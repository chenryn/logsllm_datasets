necessary for the fault-tolerance variant and further optimisa-
tions.
members. The sender and role ﬁelds are not covered by the
signature, allowing nodes that are not the original source to
modify these ﬁelds without invalidating the signature.
5.2 Adding Fault Tolerance
The basic Rollercoaster scheme of Section 5.1 fails when
users are ofﬂine and cannot perform their role of forwarding
messages. In this case, one or more recipients in later levels
would not receive the message until their parent node returns
online. The risk of this approach becomes apparent when
looking at the graph in Figure 2B, where a single unavailable
node causes message loss for its entire subtree. In principle,
the responsibility for forwarding messages could be delegated
to the provider nodes, which are assumed to always be online.
However, we consider this approach not to be desirable as
the adversary could learn about the group membership by
compromising a provider.
Rollercoaster with fault-tolerance achieves reliable deliv-
ery through acknowledgement (ACK) replies to the source
and reassignment of roles. When the source sends a message
it sets timeouts by which time it expects an acknowledgement
from the recipient and each of its children. The individual
timeouts account for the number of hops and the expected de-
lays at each hop due to mix node delays and messages waiting
in send queues. ACKs are sent through the mix network like
any other unicast message. When receiving an ACK from a
node, the source marks the sending node as delivered. Choos-
ing the source as the main coordinator is reasonable as it has
the strongest incentive for ensuring delivery of all messages.
Loopix allows a high rate of messages received by users, so it
is not a problem if one user receives a large number of ACKs.
The source responds to a timeout by sending the message
to a different node. For this, each node maintains a list of
most-recently-seen nodes based on received messages and
chooses one from it heuristically. The source itself is part
of that list as the ultimate replacement node. A replacement
node is only necessary when the failing node would have for-
warded the message to others, i.e. when it is not a leaf node
of the distribution tree (see Algorithm 10 in Appendix B). In-
dependently of this and in case that the message did not reach
the intended recipient due to message loss, a retry message is
sent (with exponential back-off) to the failed node again with
its own timeout.
We start the timeouts associated with a message when
the underlying Loopix implementation sends the message
to the provider, so that the timeouts do not need to include the
sender’s queuing delay. Since the sender knows the global rate
parameters λp and λµ, it takes these into account when deter-
mining timeouts. The timeout may further be adjusted based
on the network conﬁguration and application requirements.
The fault-tolerance mechanism makes use of the message
ﬁelds source, sender, and role shown in Figure 3. The source
ﬁeld remains unchanged as the message is forwarded because
it is required for constructing the schedule at each node. It
also indicates the node to which the ACK should be sent.
The sender ﬁeld is updated when forwarding a message or
sending an ACK and used by the recipient to update their list
of most-recently-seen nodes. The role ﬁeld indicates the role
that the receiving node should perform, usually their natural
identity. However, when a node is ofﬂine, another node might
be assigned its role, i.e. its position in the distribution tree.
In this case, the role ﬁeld indicates the node as which the
recipient should act. Retry messages to failed nodes have an
empty role ﬁeld, because the role has already been reassigned.
On receiving any payload message msg, the recipient node
hands over the payload to the application and reconstructs
the schedule using msg.source, msg.groupid, and msg.nonce.
For every child node of msg.role in the schedule, the node
enqueues a message for the respective recipient, making sure
to update msg.role. The ACK reply is enqueued after the
payload messages so that no ACK is sent if a node goes
ofﬂine before forwarding a message to all of its children in
the distribution tree.
ACK messages contain the groupid, nonce, source, and role
ﬁelds of the original message and an updated sender ﬁeld,
which allow the recipient of the ACK (i.e., the source) to
identify and cancel the corresponding timeout. The sender
adds a signature covering all header ﬁelds to ensure that the
ACK message cannot be forged. When an ACK is not received
on time, the message is sent to a different node as described
above.
If the connection between a user and their provider is inter-
rupted, we rely on the fact that Loopix allows users to retrieve
received messages from their inbox later. The user’s software
notices a loss of connection and pauses timeouts until it has
had a chance to check the inbox on the provider again.
After a long ofﬂine period, a node’s inbox may contain a
large backlog of messages that were received by the provider
while the user was ofﬂine. When a node comes back online, it
treats this backlog differently from messages received while
USENIX Association
30th USENIX Security Symposium    3439
online: for any messages received while ofﬂine, a node only
delivers the payloads to the application, but it does not send
ACK messages or forward messages to other nodes. Here the
node avoids doing unnecessary work for messages where the
timeout is likely to have already expired.
Algorithm 8 in Appendix B describes the behaviour of the
fault-tolerant variant in detail.
5.2.1 Eventual Delivery and Byzantine Fault Tolerance
The fault-tolerant variant of Rollercoaster assumes that the
source node acts honestly and does not disconnect perma-
nently (but can do so intermittently). This is reasonable as
the sending user has high incentive to see through the deliv-
ery of their message. We prove eventual delivery under this
assumption in the extended paper (see Appendix C). An ap-
plication might provide the user with a suitable user interface
that shows the delivery process.
Proof sketch: Everyone who does not ACK the payload
will eventually receive it directly from the source, and will
read it from their inbox when they return online. This works
even in the presence of malicious nodes that acknowledge a
message without forwarding it, since the source has individual
timeouts for each group member. Therefore, the source will
detect when a node’s children do not send ACKs.
However, the source node might be disconnected perma-
nently. To nevertheless guarantee eventual delivery, every
group member can periodically pick another group member
at random and send it a hash of the message history it has
seen so far (ordered in a deterministic way so that two users
with the same set of messages obtain the same hash). If the
recipient does not recognise the hash, the users run a reconcil-
iation protocol [14] to exchange any messages that are known
to only one of the users. Such a protocol provably guarantees
that every user eventually receives every message, even if
some of the users are Byzantine-faulty, provided that every
user eventually exchanges hashes with every other user [14].
5.3 Exploring Delay and Trafﬁc
We ﬁrst analyse the expected multicast latency of Roller-
coaster without fault tolerance by considering the levels of
the distribution tree, as illustrated in Figure 2. The expected
multicast latency Drollercoaster is determined by the longest
message forwarding paths C1,C2, . . . . Each such path is de-
ﬁned as C = e0, . . . , e|C|−1 where ei is a edge from a node on
level i to a node on level i + 1. We call these edges one-level
edges. The number of levels of the schedule generated by
Algorithm 1 is L = ⌈logk+1 |U|⌉ as discussed in §5.1. Hence,
no path is longer than L. An example of a longest path is
C = (a, b)(b, g) in Figure 2. The mean message delay when
traversing each edge of the graph is ¯dmsg = ¯dQ + ¯dt , where ¯dQ
is the mean queuing delay and ¯dt = ¯dp + (l + 1) · ¯dµ + ¯dpull is
the message’s mean travel time through the network, as in (2).
Scheme
Latency D Packet size overhead
Naïve Unicast
Naïve Multicast
O(m)
O(1)
Rollercoaster
O(log m)
−
O(m)
O(1)
Table 2: Overhead of the presented multicast schemes in terms
of group multicast delay and packet size overhead.
Since each node sends no more than a total of k messages to
the directly subsequent level, the expected queuing delay for
the last message is ¯dQ = k−1
λp
.
However, there are also edges from a node on level i to
a node on level i + j where j > 1. One example is (a, d) in
Figure 2. Messages from level i to level i + 1 are sent before
any messages that skip levels, and therefore any level-skipping
messages may experience higher queuing delay before they
are sent. Concretely, the edges from level i to level i + j
will incur an additional expected queuing delay of at most
( j − 1) · ¯dQ compared to one-level edges. At the same time,
these edges save j − 1 hops, which would have incurred both
a queuing delay ¯dQ and a travel time ¯dt each. Hence, the time
saved by the reduced hop count outweighs the extra queuing
delay.
Thus, the expected time for a message to be received by
all nodes is determined by the longest path consisting of only
one-level edges, with a queuing delay of ¯dQ = k−1
at each
λp
hop:
Drollercoaster = L · ( ¯dQ + ¯dt ) = ⌈logk+1 m⌉ · ¯dmsg
(6)
Hence, the group multicast latency is logarithmically de-
pendent on the group size m and contains a multiplicative
factor that equals the time to send a single message after
being queued behind at most k messages.
When a node is ofﬂine, it will only be able to receive mes-
sages when it comes online and queries its inbox. In case
the ofﬂine node is a forwarding node, the source will detect
the lack of an ACK after the timeout expired. In this case
the latency penalty for the children of the failed node is the
timeout of the parent node, which is typically proportional to
the expected delivery time.
5.4 p-Restricted Multicast with MultiSphinx
As speciﬁed so far, Rollercoaster uses the unmodiﬁed Loopix
protocol. However, even though Rollercoaster spreads the
work of sending a multicast message more evenly across the
network than sequential unicast, payload messages and ACKs
are still demanding for nodes’ send queues.
In this section, we consider a modiﬁcation to the Loopix
protocol that further improves multicast performance: namely,
we allow some mix nodes to multiply one input message into
3440    30th USENIX Security Symposium
USENIX Association
A Standard Loopix
B p-Restricted Multicast
λp
λd
λ
λl
payload
drop
loop
λ′
p
λ′
d
λ′
l
payload
drop
loop
λ′
p = 2
Figure 4: Standard Loopix (A) sends out a message if any of
its Poisson processes triggers, so the rate of messages sent is
λ = λp + λd + λl. In p-restricted multicast (B) these Poisson
processes are still independent, but the node has an extra
layer that awaits p messages, which are then wrapped into
a MultiSphinx message. The sender can increase λ′
p to pλp
(same for λ′
l) while keeping λ′ = λ.
d, λ′
multiple output messages, which may be sent to different
recipients. The naïve mix-multicast we considered in Sec-
tion 4.2 allows arbitrary multiplication factors. Here we show
how to make mix-node-supported multicast safe by restrict-
ing the multiplication factor to a ﬁxed constant p. We call
this approach p-restricted multicast where clients can send
p messages inside one MultiSphinx package; with p = 1 this
scheme is identical to the regular Rollercoaster.
In p-restricted multicast, only mix nodes in one designated
layer may multiply messages. In our design, we perform mul-
tiplication in the middle layer (layer 2 of 3) and we refer to
these mix nodes as multiplication nodes. To ensure unlinkabil-
ity of mix nodes’ inputs and outputs, every message processed
by a multiplication node must result in p output messages,
regardless of the message type or destination. Mix nodes in
other layers retain the standard one-in-one-out behaviour of
Loopix. Since layer 3 of the mix network needs to process p
times as many messages as the earlier layers, layer 3 should
contain p times as many mix nodes as layers 2.
This paper uses the parameter p for p-restricted multicast
and k for the schedule algorithm. These can be chosen inde-
pendently of each other. However, for simplicity and practical
interdependence we often set both to the same value k = p.
Effectively, p-restricted multicast allows p messages to
different recipients to be packaged as a single message up to
p times the size. Sending fewer but larger messages allows for
lower power consumption on mobile devices, as discussed in
our application requirements (§2). We show in our evaluation
in §6.5 that p-restricted multicast allows choosing much larger
λ values while maintaining low latency.
5.4.1 The MultiSphinx message format
Loopix encodes all messages using the Sphinx message for-
mat [11], which consists of a header M containing all metadata
and an encrypted payload δ. Using the header, each mix node
ni derives a secret shared key si. Due to the layered encryp-
tion of the header and payload, an adversary cannot correlate
incoming and outgoing packets when observing mix nodes.
Our construction is based on the improved Sphinx packet
format [15] which uses authenticated encryption (AE). In
particular, we use a stream cipher C in an encrypt-then-MAC
regime and require that without the knowledge of the key, the
generated ciphertext is indistinguishable from random noise
(which is believed to be the case for modern ciphers such as
AES-CTR). Every hop veriﬁes integrity of the entire message
to prevent active tagging attacks. The improved Sphinx packet
format satisﬁes the ideal functionality of Sphinx [16]. The
per-hop integrity checks of the entire message come at the
cost of lacking support for anonymous reply messages, but
these are not used by Loopix.
Sphinx assumes that each input message to a mix node
results in exactly one output message. In order to support
p-restricted multicast we introduce the MultiSphinx message
format, which can wrap p messages. A MultiSphinx mes-
sage is unwrapped at a designated mix node, and split into
p independent messages. For anyone other than the desig-
nated multiplication node, MultiSphinx messages are indis-
tinguishable from regular Sphinx packets. We now describe
the MultiSphinx design for p = 2 by describing the creation
and processing of these messages. The detailed construction
and processing is formalised in Appendix A.2.
For p = 2, the sender waits until its message queues (pay-
load, drop, loop) have released two messages. The sender
then combines their payloads δA, δB and recipients UA,UB
into a single message that is inserted into the mix network, as
shown in Figure 4. As we want to ﬁt both payloads and two
headers into our message to the multiplication node, |δA| and
|δB| must be smaller than the global Sphinx payload size.
The combined message is sent via a mix node n0 in the ﬁrst
layer to the designated multiplication node n1, where its inner
messages are extracted and added to its input buffer. The inner
message containing δA will be processed by n1 and routed
via n2,A to the recipient nA (and similarly for B). The multi-
plication node derives the secret key s1 from the incoming
message’s header and additional secret keys s1,A, s1,B from
the headers of the inner messages. We omit provider nodes.
The sender ﬁrst computes all secret keys. Using these secret
keys it encrypts the payloads δA, δB between the recipients
and the multiplication node. However, the resulting encrypted
payloads are smaller than the regular Sphinx payload lengths.
To ensure all messages have the same size, we use a pseudo-
random function (PRF, e.g. HMAC) ρ to add padding to
the encrypted payloads δ1,A and δ1,B. ρ is keyed with the
shared secret s1 and the payload index (A or B) so that the
padding is unique. The resulting payloads have the format
δ′
1,A = δ1,A k ρ(s1 k A) (and similarly for B). Now the sender
computes the headers and MACs along the path from the
multiplication node to the recipients by simulating the decryp-
USENIX Association
30th USENIX Security Symposium    3441
tion of the payload at each step. This results in two Sphinx
headers M1,A and M1,B. Finally, we create the message for
the path from the sender to the multiplication node using the
regular Sphinx construction. We set the payload of that mes-
sage to the concatenation δcombined = M1,A k δ1,A k M1,B k δ1,B.
Appendix A.2 contains pseudocode for this construction.
The processing of incoming messages at the multiplication
node differs from other nodes. First, the payload is decrypted
and split into the message headers and payloads. Then, the
payloads are deterministically padded using the PRF ρ as
described above. To ensure that the messages are hard to cor-
relate, they are added to the node’s input buffer, decrypted
again (now deriving secrets s1,{A,B}), and delayed indepen-