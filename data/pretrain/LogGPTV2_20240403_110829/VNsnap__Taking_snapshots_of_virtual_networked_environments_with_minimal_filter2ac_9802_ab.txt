over, VNsnap-memory causes much less TCP backoff than
VNsnap-disk, as to be explained and demonstrated in Sec-
tions 3.2.2 and 4. On the other hand, the postponed snap-
shot dump in VNsnap-memory does lead to the disadvan-
tage that the snapshot ﬁle is not immediately available in
the disk after the snapshot operation.
The implementation of VNsnap-disk and VNsnap-
memory daemons involved making modiﬁcations to the
xend component of Xen that handles VM live migration.
Our implementation is based on a recent unstable release
of Xen (equivalent to Xen 3.1), but it can be easily ported
to other VMMs that support live migration (e.g., VMware).
Due to space limitation, we leave details of the VNsnap
daemon implementation and ﬁle system snapshot in a tech-
nical report [11].
3.2 Taking Distributed VIOLIN Snap-
shot
3.2.1 Overview
With the individual VM snapshots achieving minimal
downtime, we now present how to coordinate these VM
snapshots to create a consistent VIOLIN snapshot. We
adopt a simpliﬁed version of Mattern’s distributed snapshot
algorithm which is based on message coloring [14]. In VN-
snap, the algorithm is executed by the VIOLIN switches on
the layer-2 Ethernet frames generated by the VMs.
We point out that distributed snapshot algorithms have
long been proposed and applied [13, 18, 19, 20] and thus
are not our contribution. The contribution of VNsnap is the
application of a classic snapshot algorithm to the emerg-
ing virtual infrastructures, as well as the proof of its ap-
plicability. The applicability is not straightforward for the
following reasons: (1) In previous application scenarios,
the message-passing layer is responsible for executing the
snapshot algorithm. However, in VNsnap the algorithm is
executed by VIOLIN switches outside the VMs yet the goal
is to guarantee causal consistency for transport state inside
the VMs in VIOLIN. (2) Mattern’s original algorithm as-
sumes reliable communication channels, whereas in VN-
snap, the VIOLIN switches forward layer-2 frames (en-
capsulating the TCP/UDP packets from the VMs) between
each other through non-reliable (fair-lossy by assumption)
UDP tunneling (recall Figure 1). (3) Unlike some previous
scenarios that require extra logging functions to ensure cor-
rect message delivery (e.g., [18]), the VIOLIN switches do
not maintain any VM’s internal transport protocol state. (4)
Previous works require modiﬁcation to application, library,
and/or OS when applying the algorithm; whereas VNsnap
does not require any modiﬁcation to the VMs’ software (in-
cluding the network protocol stack).
Figure 3. Illustration of VNsnap’s snapshot
algorithm: The snapshot of V Mi begins at
time Si and ends at Ti.
In VNsnap, the snapshot algorithm works as follows:
One VIOLIN switch (or “switch”) initiates a run of the al-
gorithm by sending a TAKE SNAPSHOT control message to all
switches running for the same VIOLIN. This represents the
initialization of an agreement protocol (e.g., 2PC). Upon
receiving the TAKE SNAPSHOT message or a frame from a
post-snapshot VM, a VIOLIN switch starts the snapshot
operations on the VMs in the same physical host. While a
VM snapshot is in progress, its underlying VIOLIN switch
colors that VM with a pre-snapshot color and prevents the
delivery of frames from any post-snapshot colored VM.
Once the VM’s snapshot is completed, the switch will color
the VM with post-snapshot color. When all VM snap-
shots in the same host are completed, the switch notiﬁes
the initiator via a SUCCESS message. If the initiator receives
SUCCESS messages from all switches of the VIOLIN, the
agreement protocol terminates by informing the switches
to commit the snapshots (otherwise to discard them).
At the heart of the algorithm lie the different treatments
of layer-2 frames transmitted between VIOLIN switches.
Before describing the details, we ﬁrst deﬁne the term
“epoch”: For a VM, an epoch is the continuous interval
between the completion times of two consecutive snapshot
operations. In Figure 3, time Ti is when the snapshot of
V Mi completes and thus it marks the end of one epoch
and the beginning of the next epoch for V Mi (1 ≤ i ≤ 4).
A frame falls into one of the following three categories:
1. The frame’s source and destination VMs are in the
same epoch (e.g., frames labeled 1 in Figure 3). Cat-
egory 1 frames will be delivered to the destination
VMs.
2. The frame’s source VM is one epoch behind its des-
tination VM (e.g.
the frame labeled 2 in Figure 3).
Category 2 frames will be delivered to the destination
VMs.
3. The frame’s source VM is one epoch ahead of its des-
tination VM (e.g., the frame labeled 3 in Figure 3).
Category 3 frames are dropped by the destination VI-
OLIN switches.
Our proof of applicability needs to show that the snapshot
algorithm, executed outside a VIOLIN, will preserve the
semantics of application-level message passing communi-
cation via (unmodiﬁed) TCP or UDP inside the VIOLIN.
Here we will focus on the case of TCP while the case of
UDP is much simpler and thus omitted. Inside the VMs,
the TCP transport protocol achieves reliable message deliv-
ery via acknowledgement, time-out and re-transmission se-
mantics. Interestingly, we show that it is TCP’s semantics
that preserve the correctness of application-level commu-
nications in the face of the snapshot algorithm. For space
VM1
2VM
VM
3
VM
4
1
S1
3S
1
2S
1
Consistent Cut
1T
T3
3
S4
T4
Time
2
T2
1
3.2.2 Applicability of Algorithm
constraint we present an overview of the proof and leave
the details in the technical report [11].
Proof overview. The proof has two parts, which cover the
entire life cycle of a TCP connection inside a VIOLIN. In
the ﬁrst part, we show that, when restoring a VIOLIN snap-
shot, the semantics of application-level message transport
using TCP will be preserved as in the original execution
during which the snapshot is taken1. Suppose, in the origi-
nal execution, V M1 sends a message m to V M2 via TCP.
Let P be the set of TCP packets that carry the content of
message m. Let V S(V Mi) be the VIOLIN switch running
in the host of V Mi(i = 1, 2). Let Ti(i = 1, 2) be the time
when the snapshot operation of V Mi completes and let the
epoch before Ti be epoch e and the one after Ti be epoch
e + 1. To show that message m will be successfully deliv-
ered in the execution restored from the VIOLIN snapshot,
we show that for each packet p ∈ P , following VIOLIN
snapshot restoration, V M2 will eventually see the receipt
of p and V M1 will eventually see the acknowledgment of
p – under TCP.
In the second part of the proof, we show that, when
restoring a VIOLIN snapshot, the semantics of TCP con-
nection establishment and tear-down will be preserved as
in the original execution. These semantics are speciﬁed
by the well-known TCP state transition diagram [21]. The
TCP state transitions are triggered by the receipt and/or
transmission of a packet with its SYN or FIN control
bit set and the receipt of its corresponding ACK. Conve-
niently, the transmission, acknowledgment, and possibly
re-transmission of these control packets follow the same
semantics as that of the TCP packet p in the ﬁrst part of the
proof. As a result, we can basically follow the same logic in
the ﬁrst part to show that, following snapshot restoration, a
control packet will eventually be transmitted and acknowl-
edged, which will trigger the proper TCP state transitions
on both sides of the TCP connection.
We point out that, although the snapshot algorithm pre-
serves transport semantics in a VIOLIN, it does affect
transport performance. One direct impact of the algorithm
is the TCP backoff inside the VIOLIN. More speciﬁcally,
since not all VMs ﬁnish their snapshot operations at the
same time, the algorithm has to drop category 3 frames
to enforce causal consistency between the VM snapshots.
Such frame drop results in temporary backoff of active
TCP connections inside the VIOLIN. The duration of the
TCP backoff is directly related to the degree of discrep-
ancy among the VMs’ snapshot completion times. In fact,
the VNsnap-memory daemon (Section 3.1.2) is designed to
reduce such discrepancy by eliminating the impact of disk
bandwidth on VM snapshot completion times.
1We assume that there is no host, VM, or network failure during VIO-
LIN snapshot and restoration. The handling of failures is done outside of
the snapshot algorithm.
4. Evaluation
In this section, we evaluate the effectiveness and ef-
ﬁciency of VNsnap. First, we evaluate the optimized
live VM snapshot technique. Then, we measure the im-
pact of VNsnap on VIOLINs running real-world paral-
lel/distributed applications – NEMO3D [1] and BitTorrent
[2]. Throughout this section, we compare VNsnap with our
previous work [12]. All physical hosts in our experiments
are Sunﬁre V20Z servers with two 2.6GHz AMD Opteron
processors and 4GB of RAM. In our setup, both domain-0
and guest domains run the 2.6.18 Linux kernel.
4.1 Downtime Minimization for Live VM
Snapshots
We ﬁrst evaluate the optimized live VM snapshot tech-
nique (Section 3.1) for individual VMs in a VIOLIN.
The evaluation metrics include the total duration and VM
downtime of an individual VM snapshot operation as well
as the size of the VM snapshot generated. For comparison,
we experiment with all of the following VM snapshot im-
plementations: (1) Xen’s live VM checkpointing function
(used in [12]), (2) the VNsnap-disk daemon, and (3) the
VNsnap-memory daemon. For each of the three we mea-
sure the metrics with the same VM with 600MB of RAM.
The tests are run both when the VM is idle and when it is
executing parallel application NEMO3D.
Table 1 shows the results. Since both VNsnap-disk and
VNsnap-memory daemons are based on Xen’s live migra-
tion function, they both involve multiple iterations of mem-
ory page transfer during the snapshot (the “iteration” col-
umn). It is during the very last iteration that the VM freezes
and causes the downtime (the “pages in last iteration” col-
umn). The number of iterations is proportional to the ap-
plication’s Writable Working Set (WWS) [7] or the rate at
which the application is dirtying its memory pages. For
instance, we observe that, during the NEMO3D execution,
memory pages get dirtied at a rate about 125MB/s.
The most
important metric in Table 1 is the VM
downtime. We have three main observations:
(1) Both
VNsnap-disk and VNsnap-memory incur signiﬁcantly
shorter downtime (ranging from tens of milliseconds to
just above one second) than Xen’s checkpointing func-
tion (around 8.6 seconds). (2) For Xen’s live checkpoint-
ing function, the downtime remains almost the same for
both the “idle” and “NEMO3D” runs. VNsnap-disk and
VNsnap-memory, on the other hand, exhibit shorter down-
time for the “idle” runs than the “NEMO3D” runs. This is
because for VNsnap-disk and VNsnap-memory, the down-
time is determined by the number of dirty pages trans-
ferred in the last iteration – about 100 pages in the “idle”
run and 11,000 pages in the “NEMO3D” run – out of the
total 153,600 pages of the VM. This differs from Xen’s
Application
Duration(s)
Idle
NEMO3D
9
12
Application
Duration(s)
Idle
NEMO3D
12
72
Xen Live Checkpointing
Downtime(ms)
Iterations
Pages in Last Iteration
8583
8626
1
1
VNsnap-disk Daemon
Downtime(ms)
Iterations
4
30
65
1025
VNsnap-memory Daemon
153600
153600
Pages in Last Iteration
104
11102
Application
Duration(s)
Iterations
Downtime(ms)
Pages in Last Iteration
Idle
NEMO3D
8
18
4
30
68
258
104
11094
Size
1.00
1.00
Size
1.00
1.55
Size
1.00
1.00
Table 1. Measurement results comparing three VM snapshot implementations for VNsnap.
VM checkpointing, where there is only one iteration during
which the VM freezes and all 153,600 pages are written to
disk. (3) VNsnap-memory achieves a much lower down-
time for the “NEMO3D” run than VNsnap-disk. This is
because the VNsnap-disk daemon directly writes the page
images to the disk (slow) while the VNsnap-memory dae-
mon keeps them in the RAM during the snapshot (fast).
Another important metric from Table 1 is the total snap-
shot duration. For both Xen checkpointing and VNsnap-
disk, the duration represents the amount of time it takes
for the snapshot image to be fully committed to disk.
For VNsnap-memory, the duration represents the amount
of time it takes for the daemon to construct a VM’s full
image in memory and does not include the hidden disk
write latency after the snapshot. We observe that for the
“NEMO3D” run, both VNsnap-disk and VNsnap-memory
incur longer duration than Xen checkpointing because of
their multi-iteration memory page transfer. The dura-
tion for VNsnap-disk is particularly long (72 seconds vs.
12 seconds for Xen checkpointing and 18 seconds for
VNsnap-memory) as the daemon competes with the local
VM for both disk bandwidth and CPU cycles. Such a con-
tention can be mitigated by running the VNsnap-disk dae-
mon on a remote host, which will reduce the snapshot du-
ration to 33 seconds as our experiment shows.
Table 1 also shows the size of the VM snapshot rela-
tive to the amount of memory allocated to the VM. As dis-
cussed in Section 3.1.2, the VM snapshot generated by the
VNsnap-disk daemon can be larger than the VM’s memory
size. In fact, the VM snapshot ﬁle is 1.55 times the size
of the VM’s memory image for the “NEMO3D” run. Both
Xen checkpointing and VNsnap-memory, by design, gen-
erate VM snapshots of the same size as the VM’s memory
image. A larger VM snapshot results in longer VM restora-
tion time. Our experiments conﬁrm that it takes 20 sec-
onds to restore a VM snapshot generated by VNsnap-disk
whereas it takes 8 seconds to do so in the case of VNsnap-
memory or Xen checkpointing.
Impact of VM snapshot on TCP throughput. As dis-
cussed in Section 3.2.2, individual VMs in a VIOLIN may
complete their snapshots at different times and thus result
in TCP backoff. Figure 4 shows such impact on a 2-VM
VIOLIN executing NEMO3D, under no snapshot (Figure
4(a)), Xen live checkpointing (Figure 4(b)), VNsnap-disk
(Figure 4(c)), and VNsnap-memory (Figure 4(d)). We fo-
cus on one TCP connection between the two VMs. The
ﬂat, “no progress” periods shown in Figures 4(b) and 4(c)
each consist of two parts: (1) the downtime of the sender
VM during snapshot and (2) the TCP backoff period due
to the different snapshot completion times of the two VMs.
We observe that both Xen live checkpointing (Figure 4(b))
and VNsnap-disk (Figure 4(c)) incur 2-3 seconds of TCP
backoff, whereas VNsnap-memory (Figure 4(d)) does not