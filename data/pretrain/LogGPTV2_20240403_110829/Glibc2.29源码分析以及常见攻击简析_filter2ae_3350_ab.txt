      tcache_entry *entries[TCACHE_MAX_BINS];  //设置entries为刚进入tcache的地址
    } tcache_perthread_struct;
**3.tcache_put(放入tcache中)**
    tcache_put (mchunkptr chunk, size_t tc_idx)
    {
      tcache_entry *e = (tcache_entry *) chunk2mem (chunk);   //将e设值为chunk2mem(chunk)
      assert (tc_idx key = tcache;  //增加的保护
      e->next = tcache->entries[tc_idx];  
      tcache->entries[tc_idx] = e;
      ++(tcache->counts[tc_idx]);  //counts++
    }
**4.tcache_get(从tcache中取出)**
    static __always_inline void *
    tcache_get (size_t tc_idx)
    {
      tcache_entry *e = tcache->entries[tc_idx]; //首先将e设值为最后进同大小tcache的块
      assert (tc_idx entries[tc_idx] > 0);
      tcache->entries[tc_idx] = e->next;  
      --(tcache->counts[tc_idx]);
      e->key = NULL;  
      return (void *) e;  
    }
## 四：主要函数介绍
###  1.__libc_malloc
    void *
    __libc_malloc (size_t bytes)
    {
      mstate ar_ptr;
      void *victim;
      void *(*hook) (size_t, const void *)
        = atomic_forced_read (__malloc_hook);
      if (__builtin_expect (hook != NULL, 0))
        return (*hook)(bytes, RETURN_ADDRESS (0));   //此处可以看出，如果malloc_hook处不为空，就会执行，这就是为什么做pwn题时要覆盖hook的原因。
    #if USE_TCACHE
      /* int_free also calls request2size, be careful to not pad twice.  */
      size_t tbytes;
      checked_request2size (bytes, tbytes);  
      size_t tc_idx = csize2tidx (tbytes);   //计算tcache的下标
      MAYBE_INIT_TCACHE ();    //如果tcache内为空，则进行初始化
      DIAG_PUSH_NEEDS_COMMENT;
      if (tc_idx entries[tc_idx] != NULL)       //如果tc_idx合法并且tcache和其entries存在，则从tcache中返回chunk
        {
          return tcache_get (tc_idx);
        }
      DIAG_POP_NEEDS_COMMENT;
    #endif
      if (SINGLE_THREAD_P)
        {
          victim = _int_malloc (&main_arena, bytes);
          assert (!victim || chunk_is_mmapped (mem2chunk (victim)) ||
              &main_arena == arena_for_chunk (mem2chunk (victim)));
          return victim;
        }
      arena_get (ar_ptr, bytes);
      victim = _int_malloc (ar_ptr, bytes);
      /* Retry with another arena only if we were able to find a usable arena
         before.  */
      if (!victim && ar_ptr != NULL)
        {
          LIBC_PROBE (memory_malloc_retry, 1, bytes);
          ar_ptr = arena_get_retry (ar_ptr, bytes);
          victim = _int_malloc (ar_ptr, bytes);
        }
      if (ar_ptr != NULL)
        __libc_lock_unlock (ar_ptr->mutex);
      assert (!victim || chunk_is_mmapped (mem2chunk (victim)) ||
              ar_ptr == arena_for_chunk (mem2chunk (victim)));
      return victim;
      }
###  2._int_malloc
这段代码较长，大概500行，我们慢慢分析
    static void *
    _int_malloc (mstate av, size_t bytes)
    {
      INTERNAL_SIZE_T nb;               /* normalized request size */
      unsigned int idx;                 /* associated bin index */
      mbinptr bin;                      /* associated bin */
      mchunkptr victim;                 /* inspected/selected chunk */
      INTERNAL_SIZE_T size;             /* its size */
      int victim_index;                 /* its bin index */
      mchunkptr remainder;              /* remainder from a split */
      unsigned long remainder_size;     /* its size */
      unsigned int block;               /* bit map traverser */
      unsigned int bit;                 /* bit map traverser */
      unsigned int map;                 /* current word of binmap */
      mchunkptr fwd;                    /* misc temp for linking */
      mchunkptr bck;                    /* misc temp for linking */
                  //首先是定义一些变量 
    #if USE_TCACHE
      size_t tcache_unsorted_count;        /* count of unsorted chunks processed */
    #endif
      /*
         Convert request size to internal form by adding SIZE_SZ bytes
         overhead plus possibly more to obtain necessary alignment and/or
         to obtain a size of at least MINSIZE, the smallest allocatable
         size. Also, checked_request2size traps (returning 0) request sizes
         that are so large that they wrap around zero when padded and
         aligned.
       */
      //将用户请求的size大小对齐，并判断传入的参数大小是否符合要求,如果请求的size小于MIN_SIZE,则将请求的size改为MIN_SIZE
      checked_request2size (bytes, nb);
      /* There are no usable arenas.  Fall back to sysmalloc to get a chunk from
         mmap.  */
      if (__glibc_unlikely (av == NULL))   //如果没有可用的分配区，则调用sysmalloc分配
        {
          void *p = sysmalloc (nb, av);
          if (p != NULL)
        alloc_perturb (p, bytes);
          return p;
        }
      /*
         If the size qualifies as a fastbin, first check corresponding bin.
         This code is safe to execute even if av is not yet initialized, so we
         can try it without checking, which saves some time on this fast path.
       */
    #define REMOVE_FB(fb, victim, pp)            \
      do                            \
        {                            \
          victim = pp;                    \
          if (victim == NULL)                \
        break;                        \
        }                            \
      while ((pp = catomic_compare_and_exchange_val_acq (fb, victim->fd, victim)) \
         != victim);                    \
      if ((unsigned long) (nb) fd;   //将头指针赋值为victim->fd
          else
            REMOVE_FB (fb, pp, victim); //移除fb
          if (__glibc_likely (victim != NULL))
            {
              size_t victim_idx = fastbin_index (chunksize (victim)); 
              if (__builtin_expect (victim_idx != idx, 0))   //如果找到chunk，检查所分配的chunk 的size 与所在链表的size
            malloc_printerr ("malloc(): memory corruption (fast)");
              check_remalloced_chunk (av, victim, nb);  
    #if USE_TCACHE     
              /* While we're here, if we see other chunks of the same size,
             stash them in the tcache.  */
              size_t tc_idx = csize2tidx (nb);   //得到申请该大小chunk的在fastbin中的下标
              if (tcache && tc_idx counts[tc_idx] fd;
                  else
                {
                  REMOVE_FB (fb, pp, tc_victim);
                  if (__glibc_unlikely (tc_victim == NULL))
                    break;
                }
                  tcache_put (tc_victim, tc_idx);   //将剩余在fastbin中的chunk都放入相应大小的tcache中去，直到满足if条件
                }
            }
    #endif
              void *p = chunk2mem (victim);
              alloc_perturb (p, bytes);
              return p;
            }
        }
        }
      /*
         If a small request, check regular bin.  Since these "smallbins"
         hold one size each, no searching within bins is necessary.
         (For a large request, we need to wait until unsorted chunks are
         processed to find best fit. But for small ones, fits are exact
         anyway, so we can check now, which is faster.)
       */
      if (in_smallbin_range (nb))    //如果申请的大小在tcach和fastbin中都不存在，就进入smallbin中寻找
        {
          idx = smallbin_index (nb);  
          bin = bin_at (av, idx);    //首先获取nb对应大小的smallbin下标和bin头地址
          if ((victim = last (bin)) != bin)   //只要对应nb大小的smallbin中不为空
            {
              bck = victim->bk;    //bck赋值为最后一个chunk即bin->bk
          if (__glibc_unlikely (bck->fd != victim))   //检查完整性
            malloc_printerr ("malloc(): smallbin double linked list corrupted");
              set_inuse_bit_at_offset (victim, nb);  //置1
              bin->bk = bck;  
              bck->fd = bin;   //解链完成
              if (av != &main_arena)
            set_non_main_arena (victim);
              check_malloced_chunk (av, victim, nb);
    #if USE_TCACHE
          /* While we're here, if we see other chunks of the same size,
             stash them in the tcache.  */
          size_t tc_idx = csize2tidx (nb);
          if (tcache && tc_idx counts[tc_idx] bk;
                  set_inuse_bit_at_offset (tc_victim, nb);
                  if (av != &main_arena)
                set_non_main_arena (tc_victim);
                  bin->bk = bck;
                  bck->fd = bin;
                  tcache_put (tc_victim, tc_idx);
                    }
            }
            }
    #endif
              void *p = chunk2mem (victim);
              alloc_perturb (p, bytes);
              return p;
            }
        }
      /*
         If this is a large request, consolidate fastbins before continuing.
         While it might look excessive to kill all fastbins before
         even seeing if there is space available, this avoids
         fragmentation problems normally associated with fastbins.
         Also, in practice, programs tend to have runs of either small or
         large requests, but less often mixtures, so consolidation is not
         invoked all that often in most programs. And the programs that
         it is called frequently in otherwise tend to fragment.
       */
      else   //到了这里，就是unsortedbin喽     
        { 
          idx = largebin_index (nb);   //获取下标
          if (atomic_load_relaxed (&av->have_fastchunks))
            malloc_consolidate (av);   //如果fastbin中有chunk，则都合并进入unsortedbin中
        }
      /*
         Process recently freed or remaindered chunks, taking one only if
         it is exact fit, or, if this a small request, the chunk is remainder from
         the most recent non-exact fit.  Place other traversed chunks in
         bins.  Note that this step is the only place in any routine where
         chunks are placed in bins.
         The outer loop here is needed because we might not realize until
         near the end of malloc that we should have consolidated, so must
         do so and retry. This happens at most once, and only when we would
         otherwise need to expand memory to service a "small" request.
       */
    #if USE_TCACHE
      INTERNAL_SIZE_T tcache_nb = 0;
      size_t tc_idx = csize2tidx (nb);
      if (tcache && tc_idx < mp_.tcache_bins)
        tcache_nb = nb;
      int return_cached = 0;
      tcache_unsorted_count = 0;
    #endif