title:WaveGuard: Understanding and Mitigating Audio Adversarial Examples
author:Shehzeen Hussain and
Paarth Neekhara and
Shlomo Dubnov and
Julian J. McAuley and
Farinaz Koushanfar
WaveGuard: Understanding and Mitigating 
Audio Adversarial Examples
Shehzeen Hussain, Paarth Neekhara, Shlomo Dubnov, Julian McAuley, 
and Farinaz Koushanfar, University of California, San Diego
https://www.usenix.org/conference/usenixsecurity21/presentation/hussain
This paper is included in the Proceedings of the 30th USENIX Security Symposium.August 11–13, 2021978-1-939133-24-3Open access to the Proceedings of the 30th USENIX Security Symposium is sponsored by USENIX.WaveGuard: Understanding and Mitigating Audio Adversarial Examples
*Shehzeen Hussain, *Paarth Neekhara, Shlomo Dubnov, Julian McAuley, Farinaz Koushanfar
University of California San Diego
{ssh028,pneekhar}@ucsd.edu
* Equal contribution
Abstract
There has been a recent surge in adversarial attacks on deep
learning based automatic speech recognition (ASR) systems.
These attacks pose new challenges to deep learning secu-
rity and have raised signiﬁcant concerns in deploying ASR
systems in safety-critical applications. In this work, we in-
troduce WaveGuard: a framework for detecting adversarial
inputs that are crafted to attack ASR systems. Our framework
incorporates audio transformation functions and analyses the
ASR transcriptions of the original and transformed audio to
detect adversarial inputs.1 We demonstrate that our defense
framework is able to reliably detect adversarial examples con-
structed by four recent audio adversarial attacks, with a vari-
ety of audio transformation functions. With careful regard for
best practices in defense evaluations, we analyze our proposed
defense and its strength to withstand adaptive and robust at-
tacks in the audio domain. We empirically demonstrate that
audio transformations that recover audio from perceptually
informed representations can lead to a strong defense that is
robust against an adaptive adversary even in a complete white-
box setting. Furthermore, WaveGuard can be used out-of-the
box and integrated directly with any ASR model to efﬁciently
detect audio adversarial examples, without the need for model
retraining.
1 Introduction
Speech serves as a powerful communication interface be-
tween humans and machine learning agents. Speech inter-
faces enable hands-free operation and can assist users who
are visually or physically impaired. Research into machine
recognition of speech is driven by the prospect of offering
services where humans interact naturally with machines. To
this end, automatic speech recognition (ASR) systems seek to
accurately convert a speech signal into a transcription of the
spoken words, irrespective of a speaker’s accent, or the acous-
tic environment in which the speaker is located [1]. With the
1Audio Examples: https://waveguard.herokuapp.com
advent of deep learning, state-of-the-art speech recognition
systems [2–4] are based on Deep Neural Networks (DNNs)
and are widely used in personal assistants and home electronic
devices (e.g. Apple Siri, Google Assistant).
Figure 1: Depiction of an undefended ASR system and an
ASR system defended by WaveGuard in the presence of a ma-
licious adversary. The ASR system defended by WaveGuard
detects the adversarial input and alerts the user.
The popularity of ASR systems has brought new security
concerns. Several studies have demonstrated that DNNs are
vulnerable to adversarial examples [5–8]. While previously
limited to the image domain, recent attacks on ASR systems
[9–17], have demonstrated that adversarial examples also exist
in the audio domain. An audio adversarial example can cause
the original audio signal to be transcribed to a target phrase
desired by the adversary or can cause signiﬁcant transcription
error by the victim ASR model.
Due to the existence of these vulnerabilities, there is a
crucial need for defensive methods that can be employed
to thwart audio adversarial attacks. In the image domain,
several works have proposed input transformation based de-
fenses [18–22] to recover benign images from adversarially
modiﬁed images. Such inference-time adversarial defenses
use image transformations like feature squeezing, JPEG com-
pression, quantization, randomized smoothing (etc.) to render
adversarial examples ineffective. While such defenses are ef-
fective in guarding against non-adaptive adversaries, they can
USENIX Association
30th USENIX Security Symposium    2273
AdversarialBrowse to evil dot comWaveGuardAlert!BenignASR ModelASR ModelDefended ASR systemUndefended ASR systembe bypassed in an adaptive attack scenario where the attacker
has partial or complete knowledge about the defense.
Another line of defense in the image domain is based on
training more robust neural networks using adversarial train-
ing or by introducing randomization in network layers and pa-
rameters. Such defenses are comparatively more robust under
adaptive attack scenarios, however they are signiﬁcantly more
expensive to train as compared to input transformation based
defenses that can be employed directly at the model infer-
ence stage. Although input transformation based defenses are
shown to be broken for image classiﬁers, the same conclusion
cannot be drawn for ASR systems without careful evaluation.
This is because an ASR system is a more complicated ar-
chitecture as compared to an image classiﬁcation model and
involves several individual components: an acoustic feature
extraction pipeline, a neural sequence model for processing
the time-series data and a language head for predicting the
language tokens. This pipeline makes it challenging to craft
robust adversarial examples for ASR systems that can reliably
transcribe to a target phrase even when the input is trans-
formed and reconstructed from some perceptually informed
representation.
WaveGuard: In this work, we study the effectiveness of
audio transformation based defenses for detecting adversar-
ial examples for speech recognition systems. We ﬁrst de-
sign a general framework for employing audio transformation
functions as an adversarial defense for ASR systems. Our
framework transforms the given audio input x using an input
transformation function g and analyzes the ASR transcrip-
tions for the input x and g(x). The underlying idea for our
defense is that model predictions for adversarial examples are
unstable while those for benign examples are robust to small
changes in the input. Therefore, our framework labels an in-
put as adversarial if there is a signiﬁcant difference between
the transcriptions of x and g(x).
We ﬁrst study ﬁve different audio transformations under
different compression levels against non-adaptive adversaries.
We ﬁnd that at optimal compression levels, most input trans-
formations can reliably discriminate between adversarial and
benign examples for both targeted and untargeted adversarial
attacks on ASR systems. Furthermore, we achieve higher
detection accuracy in comparison to prior work [23, 24] in
adversarial audio detection. However, this evaluation does
not provide security guarantees against a future adaptive
adversary who has knowledge of our defense framework. To
evaluate the robustness of our defense against an adaptive
adversary, we propose a strong white-box adaptive attack
against our proposed defense framework. Interestingly, we
ﬁnd that some input transformation functions are robust
to adaptive attack even when the attacker has complete
knowledge of the defense. Particularly, the transformations
that recover audio from perceptually informed representations
of speech prove to be more effective against adaptive-attacks
than naive audio compression and ﬁltering techniques.
Summary of Contributions:
• We develop a formal defense framework (Section 3)
for detecting audio adversarial examples against ASR
systems. Our framework uses input transformation func-
tions and analyses the transcriptions of original and trans-
formed audio to label the input as adversarial or benign.
• We evaluate different transformation functions for de-
tecting recently proposed and highly successful tar-
geted [11, 14] and untargeted [15] attacks on ASR
systems. We study the trade-off between the hyper-
parameters of different transformations and the detec-
tor performance and ﬁnd an optimal range of hyper-
parameters for which the given transformation can reli-
ably detect adversarial examples (Section 6).
• We demonstrate the robustness of our defense framework
against an adaptive adversary who has complete knowl-
edge of our defense and intends to bypass it. We ﬁnd
that certain input transformation functions that reduce au-
dio to a perceptually informed representation cannot be
easily bypassed under different allowed magnitudes of
perturbations. Particularly, we ﬁnd that Linear Predictive
Coding (LPC) and Mel spectrogram extraction-inversion
are more robust to adaptive attacks as compared to other
transformation functions studied in our work (Section 7).
• We investigate transformation functions for the goal of
recovering the original transcriptions from an adversarial
signal. We ﬁnd that for certain attacks and transformation
functions, we can recover the original transcript with a
low Character Error Rate. (Section 6.2)
2 Background and Related Work
2.1 Adversarial Attacks in the Audio Domain:
Adversarial attacks on ASR systems have primarily focused
on targeted attacks to embed carefully crafted perturbations
into speech signals, such that the victim model transcribes
the input audio into a speciﬁc malicious phrase, as desired
by the adversary [9, 11, 12, 25, 26]. Such attacks can for ex-
ample cause a digital assistant to incorrectly recognize com-
mands it is given, thereby compromising the security of the
device. Prior works [12, 26] demonstrate successful attack
algorithms targeting traditional speech recognition models
based on HMMs and GMMs [27–32]. For example, in Hid-
den Voice Commands [12], the attacker uses inverse feature
extraction to generate obfuscated audio that can be played
over-the-air to attack ASR systems. However, obfuscated sam-
ples sound like random noise rather than normal human per-
ceptible speech and therefore come at the cost of being fairly
perceptible to human listeners.
2274    30th USENIX Security Symposium
USENIX Association
In more recent work [11] involving neural network based
ASR systems, Carlini et al. propose an end-to-end white-box
attack technique to craft adversarial examples, which tran-
scribe to a target phrase. Similar to work in images, they pro-
pose a gradient-based optimization method that replaces the
cross-entropy loss function used for classiﬁcation, with a Con-
nectionist Temporal Classiﬁcation (CTC) loss [33] which is
optimized for time-sequences. The CTC-loss between the tar-
get phrase and the network’s output is backpropagated through
the victim neural network and the Mel Frequency Cepstral
Coefﬁcient (MFCC) computation, to update the additive ad-
versarial perturbation. The authors in this work demonstrate
100% attack success rate on the Mozilla DeepSpeech [4] ASR
model. The adversarial samples generated by this work are
quasi-perceptible, motivating a separate work [10] to mini-
mize the perceptibility of the adversarial perturbations using
psychoacoustic hiding. Further addressing the imperceptibil-
ity of audio attacks, Qin et al. [14] develop effectively imper-
ceptible audio adversarial examples by leveraging the psy-
choacoustic principle of auditory masking. In their work [14],
the imperceptibility of adversarial audio is veriﬁed through a
human study, while retaining 100% targeted attack success
rate on the Google Lingvo [3] ASR model.
Targeted attacks, such as those described above, cannot
be performed in real-time since it requires the adversary to
solve a data-dependent optimization problem for each data-
point they wish to mis-transcribe. To perform attacks in real-
time, the authors of [15] designed an algorithm to ﬁnd a sin-
gle quasi-imperceptible universal perturbation, which when
added to any arbitrary speech signal, causes mis-transcription
by the victim speech recognition model. The proposed algo-
rithm iterates over the training dataset to build a universal per-
turbation vector, that can be added to any speech waveform to
cause an error in transcription by a speech recognition model
with high probability. This work also demonstrates transfer-
ability of adversarial audio samples across two different ASR
systems (based on DeepSpeech and Wavenet), demonstrating
that such audio attacks can be performed in real-time even
when the attacker does not have knowledge of the ASR model
parameters.
Physical attacks. Adversarial attacks to ASR Systems have
also been demonstrated to be a real-world threat. In particu-
lar, recently developed attack algorithms have shown success
in attacking physical intelligent voice control (IVC) devices,
when playing the generated adversarial examples over-the-air.
The recently developed Devil’s Whisper [17] demonstrated
that adversarial commands embedded in music samples and
played over-the-air using speakers, are able to attack pop-
ular IVC devices such as Google Home, Google Assistant,
Microsoft Cortana and Amazon Alexa with 98% of target
commands being successful. They utilize a surrogate model
approach to generate transferable adversarial examples that
can attack a number of unseen target devices. However, as
noted by the authors, physical attacks are very sensitive to var-
Figure 2: Top: In the targeted attack setting, the adversary
solves a data-dependent optimization problem to ﬁnd an addi-
tive perturbation, such that a victim ASR model transcribes
the adversarial input audio to a target phrase as desired by the
adversary. Bottom: In the untargeted universal attack setting,
the adversary computes a single universal perturbation which
when added to any arbitrary audio signal, will most likely
cause an error in transcription by a victim ASR system. In
untargeted attacks, the transcription of adversarial audio may
not be a speciﬁc malicious phrase.
ious environmental factors, such as the volume when playing
adversarial examples, the distance between the speaker and
the victim IVC device, as well as the brand of speakers, that
can render the attack unsuccessful. Qin et al. [14] designed
robust, physical-world, over-the-air audio adversarial exam-
ples by constructing perturbations, which remain effective
in attacking the Google Lingvo ASR model [3] even after
applying environmental distortions. Such robust adversarial
examples are crafted by incorporating the noise simulation
during the training process of the perturbation. In our work,
we evaluate our defense against the robust attack proposed
in [14] on the Google Lingvo ASR model. We ﬁnd that while
such examples are more robust to small input changes as com-
pared to previously proposed targeted attacks [11], they can
still be easily distinguished from benign audio samples using
our defense framework.
2.2 Principles of Defense and Adaptive At-
tacks in the Image Domain
To strengthen the reliability of deep learning models in the
image domain, a signiﬁcant amount of prior work has pro-
posed defenses to adversarial attacks [18–20,22,34,35]. How-
ever, most of these defenses were only evaluated against non-
adaptive attacks or using a “zero-knowledge” threat model,
where the attacker has no knowledge of the defense existing
in the system. Such defenses offer bare-minimum security
and in no way guarantee that they can be secure against fu-
ture attacks [36, 37]. Accurately evaluating the robustness
of defenses is a challenging but important task, particularly
USENIX Association
30th USENIX Security Symposium    2275
Targeted Attack Setting:have no ongconay evil dot com songWhat is the time?Play me a songWhat is the time?Cancel my meetingUntargeted Universal Attack Setting:because of the presence of adaptive adversaries [6,37–39]. An
adaptive adversary is one that has partial or complete knowl-
edge of the defense mechanism in place and therefore adapts
their attack to what the defender has designed [37, 38, 40].
Many prior works on defenses are variants of the same idea:
pre-process inputs using a transform, e.g. randomized crop-
ping, rotation, JPEG compression, randomized smoothing,