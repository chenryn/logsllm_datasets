### Multi-Path Forwarding and Flow-Based Scaling

This design supports multi-path forwarding, although in practice, a single path per flow is typically sufficient. For flow-based scaling, we adjust the target delay based on the number of competing flows to ensure Swift can fully utilize the bottleneck link. The required queue size, and thus the target delay, increases with the number of competing flows. 

For example, consider a link with \( N \) flows, each rate-limited to their fair share but starting at random times. Queuing occurs when packets from different flows arrive simultaneously by chance. Simulation results (Figure 4) show that the average queue length grows as \( O(\sqrt{N}) \). This behavior can be modeled as a bounded random walk, where the average queue size grows as \( O(\sqrt{N}) \).

Buffer sizing work [5] suggests that the required buffer space for TCP reduces with the number of flows by \( O(\sqrt{N}) \). This may seem contradictory, but it models different aspects. The buffer sizing work focuses on the variation in window size governed by AIMD, showing that the total window variation reduces by \( O(\sqrt{N}) \) for large, unsynchronized flows. In Swift, we model the queuing due to random collisions, not bandwidth overestimation, and factor this into the queuing headroom to avoid overreaction.

Since the sender does not know the number of flows at the bottleneck, we adjust the target delay using the congestion window (cwnd). When Swift has converged to its fair-share, cwnd is inversely proportional to the number of flows. Thus, we adjust the target delay in proportion to \( \frac{1}{\sqrt{\text{cwnd}}} \), meaning the target delay grows as cwnd becomes smaller. This method not only reduces queuing for fewer flows but also improves fairness by speeding up slow flows and slowing down fast flows. This convergence bias is particularly useful when flows have cwnds less than one.

### Overall Scaling Formula

Combining topology and flow scaling, the target delay formula is:

\[ t = \text{base\_target} + \#hops \times \hbar + \max(0, \min(\alpha \sqrt{\text{cwnd}} + \beta, \text{fs\_range})) \]

where:
- \(\alpha = \frac{1}{\text{fs\_range}} \left( \sqrt{\text{fs\_min\_cwnd}} - \sqrt{\text{fs\_max\_cwnd}} \right)\)
- \(\beta = -\alpha \sqrt{\text{fs\_max\_cwnd}}\)

**Parameters:**
- **base\_target**: Minimum target delay for 100% utilization in a one-hop network with few flows.
- **fs\_range**: Additional target on top of the base, progressively reduced over the cwnd range \([\text{fs\_min\_cwnd}, \text{fs\_max\_cwnd}]\).
- **\(\hbar\)**: Per-hop scaling factor.

These parameters determine the slope of the scaling curve and the cwnd range to which it is applied. A steeper curve improves fairness and convergence by making it more likely that slower flows increment their rates, but it also increases queuing. To model queuing due to random collisions, the slope of the curve is a function of the number of flows and link capacity. We approximate the ideal curve by selecting a tolerable delay over the base target and a cwnd range for scaling. This allows us to use a steeper curve for smaller cwnds, representing high congestion regimes.

The interdependence of cwnd and target delay is primarily for the small cwnd regime. In practice, the target delay and cwnd adjustments are opposite in direction, ensuring that queuing and target delay equalize more quickly than with a static target, leading to faster convergence and better fairness.

### Loss Recovery and ACKs

**Loss Recovery:**
Swift maintains low packet loss, similar to TCP, using selective acknowledgments (SACK) for fast recovery and a retransmission timer for data delivery. SACK is implemented with a simple sequence number bitmap. Lost packets are detected via holes in the bitmap, retransmitted, and the cwnd is reduced multiplicatively. A retransmission timeout (RTO) is maintained per flow, computed using an exponentially weighted moving average of end-to-end RTT. The cwnd is reduced by the maximum multiplicative factor on an RTO.

**ACKs:**
Swift does not explicitly delay ACKs to react more quickly to congestion. ACK coalescing still occurs if multiple packets arrive together. For bidirectional traffic, we decouple data and ACK packets for paced flows. An incoming data packet generates an immediate ACK, while a reverse data packet respects any pacing delay.

### Coexistence via QoS

In a shared production environment, multiple congestion control algorithms coexist. WAN flows use different congestion control than datacenter flows optimized for latency. Customers configure cloud VMs with their choice of congestion control, and UDP-based traffic uses application-level rate control. It is essential that Swift traffic coexists without adverse competition for switch buffers, which could inflate latency and reduce throughput.

We leverage QoS features, reserving a subset of QoS queues for Swift traffic and allocating link capacity via weighted-fair-queuing. By using larger scheduler weights for higher priority traffic, we handle tenants with different traffic classes. While this arrangement does not completely isolate Swift traffic, it provides enough separation for excellent performance.

### Takeaways from Production Deployment

Swift was deployed in production at Google over four years, supporting large-scale traffic from various applications, including:
- HDD and SSD reads/writes for storage needs.
- An in-memory key-value store for latency-sensitive applications.
- An in-memory filesystem for BigQuery shuffle, which is IOPS-intensive.

### Measurement Methodology

Data is taken fleet-wide over a week, covering a wide range of workloads, scale, and utilization. We draw conclusions from:
- Switch statistics (link utilization and loss rates).
- Host round-trip times (NIC-to-NIC probers).
- Application metrics.

Our main comparison is with DCTCP-style congestion control, specifically a Google version called GCN, which has a faster response to congestion and no delayed ACKs for ECN-marked data. GCN is a well-known reference point, thoroughly tuned at scale, providing a stringent comparison for Swift.

### Performance at Scale

We report combinations of latency/loss and throughput/utilization. Both Swift and GCN run together in various mixes in the same cluster. Figures 6 and 7 show that Swift achieves 2+ orders of magnitude lower average and 99.9th-percentile loss rates than GCN across a range of combined utilization. Swift maintains low loss even at high utilization, while GCN does not.

### Use of Shared Infrastructure

We separate traffic using QoS classes that share link bandwidth. Swift outperforms GCN by rapidly reducing cwnd under extreme congestion, alleviating congestion at end-hosts, and predictably bounding end-to-end delay. Swift achieves near line-rate throughput while maintaining low loss and low latency.