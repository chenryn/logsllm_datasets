header. This design works with multi-path forwarding, though in
practice we find it sufficient to use a single path at a time per flow.
Flow-based Scaling: We also scale the target delay with the num-
ber of competing flows. The target must provide enough headroom
for Swift to fully utilize the bottleneck. We find that the queue size
and hence the target required to saturate the bottleneck link in-
creases with the number of competing flows. For intuition, consider
a link with N flows that are rate-limited to exactly their fair share
but have random start times. Then queuing happens only when
packets from different flows come together by chance. Figure 4
shows the simulation results—the average queue length grows as
O(√N). This behavior can be modeled as a bounded random walk
and the average queue size, like the distance from the starting
point, grows as O(√N). Results from buffer sizing work [5] that
show required buffer space for TCP reduces with number of flows
by O(√N) may seem contradictory but it is modeling different as-
pects. The buffer sizing work is modeling the variation in window
as governed by AIMD. Using Central Limit Theorem, Reference [5]
shows that the variation in total window is reduced by O(√N) for
large number of unsynchronized flows.4 In Swift, we model the
amount of queuing that would happen naturally due to random
4Another way to reason: given N flows, each flow’s window fluctuation is O( 1
N ×
BDP); chance of fluctuations coinciding is O(√N). Thus, average fluctuation of total
window is O(
√N
N ) = O( 1
√N ).
518
PropagationDelayQueuingDelaySerializationDelaySerializationDelayHop 1NICPropagationDelayQueuingDelaySerializationDelayHop H...Reverse PathN Flows1632641282565121k2k4k8kNumberofFlows0255075100(cid:30)eueLength(pkts)logsqrtlinearMeasuredqueuelengthDescription
base target delay
per hop scaling factor
Parameter
base_tarдet
ℏ
f s_max_cwnd max cwnd for target scaling
f s_min_cwnd min cwnd for target scaling
f s_ranдe
max scaling range
Table 2: Parameters for target delay scaling.
Figure 5: Example fabric delay target curve vs congestion window.
chance of collision between flows but not because of bandwidth
overestimation, and factor it into the queuing headroom to avoid
overreaction.
Since the sender does not know the number of flows at the
bottleneck, we need another means to adjust the target. We rely
on the fact that cwnd is inversely proportional to the number of
flows when Swift has converged to its fair-share. So we adjust the
target in proportion to 1/√cwnd, i.e., the target delay grows as cwnd
becomes smaller. As well as lowering the queuing when there are
few flows, this method improves fairness: it speeds slow flows with
a larger target, and slows fast flows with a smaller target. We find
this convergence bias especially useful when flows have congestion
windows less than one.
Overall Scaling: Combining topology and flow scaling, and using
the notation of Table 2, we arrive at the formula for target delay:
t = base_tarдet + #hops×ℏ + max(0, min(
+β, f s_ranдe)),
where
α√f cwnd
α =
1
f s_r anдe
√f s_min_cwnd −
√f s_max_cwnd
1
, β = −
α
√f s_max_cwnd
.
Figure 5 provides an example to show the relationship between
target delay, cwnd, and flow scaling parameters. base_tarдet is the
minimum target delay required to provide 100% utilization in a one
hop network with a small number of flows. f s_ranдe specifies the
additional target on top of base that is progressively reduced over
the cwnd range [f s_min_cwnd, f s_max_cwnd]. ℏ is the per-hop
scaling factor. These three parameters determine the slope of the
scaling curve and the cwnd range to which it is applied.
A steeper curve improves fairness and convergence by making
it more likely that slower flows increment their rates. However, it
also increases queuing in the network. To precisely model queuing
due to random collisions, the slope of the curve is a function of
number of flows and link capacity. We approximate the ideal curve
by selecting a tolerable delay over base target, and a cwnd range
over which to apply scaling. Defining a scaling range lets us use a
steeper curve for smaller cwnd, representing the high congestion
regime.
We note that the interdependence of cwnd and target delay is
for the small cwnd regime. It does not pose a problem in practice
Kumar et al.
because the target delay and cwnd adjustments are opposite in
direction, e.g., when cwnd grows, the queuing delay increases which
reduces target delay, and vice versa. The result is to more quickly
equalize the queuing and target delay than with a static target,
which gives faster convergence and better fairness.
3.6 Loss Recovery and ACKs
We discuss the details of generating acknowledgments and recov-
ering lost packets because they impact delay-based congestion
control.
Loss Recovery Happily, we have needed to invest minimal effort
in loss recovery for good tail latency because Swift keeps packet
losses low. Like TCP, packet losses are detected in Swift with two
main mechanisms: selective acknowledgements (SACK) for fast
recovery, and a retransmission timer to ensure data delivery in
the absence of acknowledgments from the receiver. SACK is imple-
mented using a simple sequence number bitmap. When a packet
is detected as lost via a hole in the bitmap, it is retransmitted, and
the congestion window is reduced multiplicatively. In addition, a
retransmission timeout (RTO) is maintained on a per-flow basis
computed using exponentially weighted moving average over the
end-to-end RTT. To adapt quickly to potentially severe congestion,
the congestion window is reduced by the maximum multiplicative
factor on an RTO. We have not needed to draw on other mechanisms
well-known from TCP loss recovery, e.g., References [4, 10, 19, 37].
ACKs Swift does not explicitly delay ACKs to react more quickly
to congestion. Note that ACK coalescing will still occur if multiple
packets arrive together, e.g., they are processed in a batch by Snap.
We also take care not to delay ACKs in the case of bi-directional
traffic, as would happen if we piggyback the ACK on a reverse data
packet that is paced. Instead, we decouple data and ACK packets
for paced flows—an incoming data packet generates a pure ACK
sent immediately to unblock the remote end, while a reverse data
packet respects any pacing-delay that is imposed on it.
3.7 Coexistence via QoS
In a shared production deployment, there are multiple congestion
control algorithms. WAN flows operate with a different congestion
control algorithm than datacenter flows optimized for latency. Cus-
tomers configure cloud VMs with the congestion control of their
choice. And UDP-based traffic uses application-level rate control
logic. It is essential that Swift traffic be able to coexist with various
forms of congestion control without adverse competition for switch
buffers, otherwise its latency may be inflated and its throughput
reduced.
As a pragmatic solution, we leverage QoS features. Switches
have ∼10 QoS queues per port [32] that can share the buffer space
across ports based on usage. We reserve a subset of QoS queues
for Swift traffic and give them a share of the link capacity via
weighted-fair-queuing. By using larger scheduler weights for higher
priority traffic, we are able to handle tenants with different traffic
classes. While this simple arrangement does not completely isolate
Swift traffic, we show it provides enough separation for excellent
performance (§4.3).
4 TAKEAWAYS FROM PRODUCTION
We deployed Swift in production at Google over the course of four
years. It supports traffic at large scale from applications with a
range of needs including:
519
0.0010.010.1110100CongestionWindow(#packets)012345678TargetDelay(normalized)_____#Ò×Ò_Flow-basedscalingFlowandhop-basedscalingSwift: Delay is Simple and Effective for Congestion Control in the Datacenter
• HDD and SSD reads and writes that serve the storage needs of
many Google applications. While there are many small reads
and writes, this traffic is generally throughput-intensive (also
referred to as byte-intensive in this paper).
• An in-memory key-value store used by several Google applica-
tions, which is latency-sensitive.
• An in-memory filesystem used for BigQuery shuffle, which is
IOPS-intensive and can have a large degree of fan-in.
4.1 Measurement Methodology
Data reported in this section is taken fleet-wide, except where we
call out specific clusters, for a period of one week. This data covers
a very wide range of workloads, scale, and utilization. We draw our
conclusions from three types of data:
• Switch statistics tell us the link utilization and loss rates. We
compute them over 30-second intervals using per-port output-
packets and output-discards counters collected from production
switches. We do not include loss due to routing failures or packet
corruption.
• Host round-trip times. NIC-to-NIC probers with NIC hardware
timestamps give us fabric RTT data for both Swift and non-Swift
traffic. End-to-end packet RTT, including host and fabric parts, is
measured in Swift.
• Application metrics, where available, highlight the impact of
Swift on applications.
Our main point of comparison for Swift is DCTCP-style con-
gestion control. While there are many other proposed congestion
control protocols for the datacenter, they are more complex and
not readily available at fleetwide scale. In contrast, DCTCP is a
well-known reference point, for which we report on a Google ver-
sion called GCN that has a faster response to congestion by scaling
its multiplicative decrease based on the current ECN mark rate
rather than an EWMA, and disabling delayed ACKs when receiving
ECN-marked data. GCN has also been thoroughly tuned at scale.
Thus, we believe GCN serves as a more stringent comparison for
Swift than DCTCP. The GCN results in this section are with our
kernel production deployment; we also provide results for GCN
instantiated in PonyExpress [36] for comparison in §5.3.
For confidentiality purposes, we normalize the absolute loss, la-
tency and throughput numbers from our production deployments;
in §5 we report absolute numbers for Swift performance from
testbed experiments.
4.2 Performance At Scale
We report combinations of latency/loss and throughput/utilization
because it is essential to deliver on all of them at once—it is easy but
not useful to optimize one metric at the expense of another. A key
difficulty is how to perform a comparative fleetwide evaluation as
we cannot easily shift between these protocols to provide clean A/B
data at fleet scale. Rather, both Swift and GCN must run together
in various mixes in the same cluster. We report Swift/GCN loss
rates versus total port utilization as well as individual Swift/GCN
utilizations (which we call queue utilizations) to show that Swift
delivers excellent performance.
Takeaway: Swift achieves low loss even at line-rate
One of the biggest improvements as we moved traffic to Swift
was the reduction in packet loss. We compare Swift and GCN loss
rates versus the combined link utilization of Swift and GCN. The
loss rate is the lost packets divided by the sent packets for a type
of traffic (Swift or GCN). The combined link utilization is the sum
Figure 6: Edge (ToR to host) links: Average and 99.9p Swift/GCN loss rate (lin-
ear and log scale) vs. combined utilization, bucketed at 10% intervals. Loss rate
is normalized to highest GCN loss rate. The near-vertical line in the log-scale
plot is due to extremely small relative loss-rate.
Figure 7: Fabric links: Average and 99.9p Swift/GCN loss rate Swift/GCN loss
rate (linear and log scale) vs. combined utilization, bucketed at 10% intervals.
of Swift/GCN bits on the wire divided by the link capacity. We use
switch counters for both metrics. We normalize the highest GCN
loss rate in each plot to 1.0.
Figure 6 shows links at the edge, i.e., ToR to host. We see in the
log-scale plot that Swift provides 2+ orders of magnitude lower
average and 99.9th-percentile loss rates than GCN across a range of
combined utilization. While the data is normalized due to confiden-
tiality, we note that GCN losses have been an operational challenge,
even at lower utilization levels. Swift continues to provide very low
average and tight tail loss for heavily utilized (>90%) links at the
edge, while GCN does not.
Figure 7 shows the same plot for fabric links in the core of the
datacenter. We see the same trends. Note that not all loss in produc-
tion is due to congestion control, e.g., link flaps cause loss before
the switch converges. This impact is evident at high utilization in
the fabric, especially for 99.9p loss, where there is a slight uptick in
Swift loss rate.
520
0102030405060708090100PortUtilization(%)0.000.250.500.751.00LossRate(normalized)AverageGCNSwi(cid:28)0102030405060708090100PortUtilization(%)0.000.250.500.751.00LossRate(normalized)99.9thpercentileGCNSwi(cid:28)0102030405060708090100PortUtilization(%)10−510−410−310−210−1100LossRate(normalized)AverageGCNSwi(cid:28)0102030405060708090100PortUtilization(%)10−510−410−310−210−1100LossRate(normalized)99.9thpercentileGCNSwi(cid:28)0102030405060708090100PortUtilization(%)0.000.250.500.751.00LossRate(normalized)AverageGCNSwi(cid:28)0102030405060708090100PortUtilization(%)0.000.250.500.751.00LossRate(normalized)99.9thpercentileGCNSwi(cid:28)0102030405060708090100PortUtilization(%)10−910−610−3100LossRate(normalized)AverageGCNSwi(cid:28)0102030405060708090100PortUtilization(%)10−910−610−3100LossRate(normalized)99.9thpercentileGCNSwi(cid:28)Kumar et al.
Figure 8: Average and 99.9th percentile loss rate vs. queue utilization.
Figure 11: Fabric RTT: Swift controls fabric delay more tightly than GCN.
Figure 9: Edge (ToR to host) average and 99.9p loss rate vs. total Swift/GCN
throughput in the cluster.
Figure 12: Cluster Swift/GCN Throughput vs. Average RTT. The dashed line
is the base target delay (normalized to 1).
timestamps. We can see that Swift is able to maintain the average
fabric round-trip around the configured target delay, and controls
tail latency much better than non-delay based GCN. We normalize
the numbers w.r.t the base target delay.
In Figure 12, we show that Swift achieves average RTTs close to
target delay at large scale and across our clusters. The average RTT
roughly matches the base target delay used in our deployments. This
behavior has proven to be extremely useful as we tuned the target
delay over the course of our full deployment—early Swift deploy-
ments used 2× the base target delay than our current configuration.
This change was done incrementally and carefully, ensuring we did
not cause regression in application performance by decreasing the
bandwidth applications get as we navigate the latency-throughput
tradeoff. §5.1 details the experiments that guided us in setting the
base target delay in production.
We note that in some clusters the average RTT is above the base
target, though it stays below the upper bound of the target with
topology and flow scaling. We spot-checked a few clusters and
found that the average is drivenup by the tail RTT for two reasons.
First, external factors impact the delay observed for Swift. The main
factor is a large amount of traffic on high QoS classes outside of
Swift control. These factors are inevitable in a large, heterogeneous,
shared infrastructure. Second, the tail RTT is also driven up by
heavy incast workloads that trigger flow-based scaling of target
delay as described in §3.5.
Thus, Swift achieves near line-rate throughput while main-
taining its promise of low loss and low latency.
Swift outperforms GCN for three main reasons. First, Swift
rapidly reduces its cwnd to below one under extreme congestion,
when number of flows exceed the network bandwidth-delay prod-
uct. Second, Swift alleviates congestion at end-hosts in addition
to congestion in the fabric. Finally, Swift predictably bounds end-
to-end delay regardless of intermediate link rates and buffer sizes,
which is a difficult goal to attain with GCN thresholds.
4.3 Use of Shared Infrastructure
Recall that we share network links with non-Swift traffic. To do
so we separate traffic using QoS classes that share link bandwidth
Figure 10: Average and 99.9p loss rate of highly-utilized (>90%) links in each
switch group.
As a check, Figure 8 shows Swift/GCN loss versus the Swift/GCN