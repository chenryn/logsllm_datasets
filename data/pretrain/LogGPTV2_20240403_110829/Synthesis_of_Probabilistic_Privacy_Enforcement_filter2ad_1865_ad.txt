policy Φ = {(S1,[a1, b1]), . . . ,(Sk ,[ak , bk])}. The variables in
the constraints are {C1, . . . , Cn} of type integer.
{1, . . . , |O|} that assigns an integer value between 1 and |O| to
each variable Ci. The kernel of a model M is defined as the set
ker(M) = {(oi , oj) ∈ O × O | M(Ci) = M(Cj)}. Hence a model
M identifies the enforcement ξ = ker(M).
We show the main steps of SynSMT in Algorithm 1. At Line 2,
the algorithm generates the SMT constraint ψassert using the for-
mula Assert(π , δ, Φ), as defined in Figure 5. The two probabilities
δ(I ∈ Sℓ | O = oi) and Pp
δ(O = oi) that appear in the SMT con-
Pp
straints are constants that are computed from the program π and
attacker belief δ beforehand using a probabilistic solver. We detail
this step in Section 7. The constraint ψassert conjoins ψrange and
ψbounds. The constraintψrange encodes that the range of all variables
Ci is {1, . . . , |O|}, and ψbounds encodes that for each belief bound
(Sℓ,[aℓ, bℓ]) ∈ Φ, we have Enf(π , ξ), δ |= (Sℓ,[aℓ, bℓ]), where ξ is
the enforcement identified by the variables C1, . . . , C|O|.
to denote the attacker belief about predicate Sℓ after
observing the equivalence class Ej = {oi ∈ O | Ci = j}. This is
δ(I ∈ Sℓ | O ∈ Ej) and is computed by summing
defined as p
over all outputs that belong to Ej. We use Iverson bracket notation
[ψ] to denote the function that returns 1 if ψ holds, and 0 otherwise.
For any non-empty equivalence class Ej, the value of p
must be
within [aℓ, bℓ] as defined by the belief bound (Sℓ,[aℓ, bℓ]). The
disjunction ψ
SynSMT receives the objective function ψobj as an input. Ex-
amples of objective functions are given in Figure 5. The func-
tion Objcls(n) maximizes the sum of all non-empty equivalence
classes, i.e. it maximizes the permissiveness of the enforcement.
non-empty encodes whether Ej is non-empty.
j
We use p
= Pp
j
ℓ
j
ℓ
j
ℓ
Session B4:  Privacy PoliciesCCS’17, October 30-November 3, 2017, Dallas, TX, USA39764
Probabilities Pπ
δ (O = 0) = 29
Pπ
δ (O = 1) = 5
Pπ
δ (O = 2) = 11
Pπ
δ (O = 3) = 1
Pπ
16
64
16
δ (O = ·) and Pπ
29
δ (I ∈ S1 | O = 0) = 2
Pπ
δ (I ∈ S1 | O = 1) = 1
Pπ
δ (I ∈ S1 | O = 2) = 6
Pπ
δ (I ∈ S1 | O = 3) = 3
Pπ
δ (I ∈ Si | O = ·):
δ (I ∈ S2 | O = 0) = 28
Pπ
δ (I ∈ S2 | O = 1) = 4
Pπ
δ (I ∈ S2 | O = 2) = 4
Pπ
δ (I ∈ S2 | O = 3) = 0
Pπ
:= Assert(π, b, Φ)
:= Obj(4)
ψassert
ψobj
29
11
11
5
4
4
SMT constraints / Objective function:
ψassert ≡ ψrange ∧ ψbounds
ψrange ≡ 4
ψbounds ≡ (4
pi1 =
i =1 Ci ≥ 1 ∧ Ci ≤ 4
i =1 pi1 ∈ [0.1, 0.5]) ∧ (4
[C1 = i] · 1
[C1 = i] · 29
[C1 = i] · 7
[C1 = i] · 29
32 + [C2 = i] · 5
64 + [C2 = i] · 5
16 + [C2 = i] · 1
64 + [C2 = i] · 5
i =1 pi2 ∈ [0.5, 0.9])
64 + [C3 = i] · 3
64 + [C3 = i] · 11
4 + [C3 = i] · 1
64 + [C3 = i] · 11
32 + [C4 = i] · 3
64
64 + [C4 = i] · 1
16
16 + [C4 = i] · 0
64 + [C4 = i] · 1
16
pi2 =
ψobj = maximize([C1 = 1 ∨ C2 = 1 ∨ C3 = 1 ∨ C4 = 1]
Probabilistic program π :
def main(nucl: R[][]) {
A := 0;
sum := 0;
for pat in [0..3) {
if (nucl[pat] == [A,A]) {sum++;}
}
if (sum > 0 && flip(1/2)) {sum--;}
if (sum < 3 && flip(1/2)) {sum++;}
return sum;
}
Attacker belief δ:
def belief() {
Alice := 0; Bob := 1; Carol := 2;
nucl := array(3);
nucl[Alice] := [flip(0.5), flip(0.5)];
nucl[Bob] := [flip(0.5), flip(0.5)];
C0 := nucl[Alice][flip(0.5)];
C1 := nucl[Bob][flip(0.5)];
nucl[Carol] := [C0, C1];
return nucl;
}
Privacy policy Φ:
(nucl[Bob] == [A,A], [0.1, 0.5])
(nucl[Alice][0] == G
|| nucl[Alice][1] == G, [0.5, 0.9])
1
2
3
4
5
6
7
8
9
10
1
2
3
4
5
6
7
8
9
10
1
2
3
+ · · · + [C1 = 4 ∨ C2 = 4 ∨ C3 = 4 ∨ C4 = 4])
M := Max(ψassert, ψobj)
Model: M = {C1 (cid:55)→ 1, C2 (cid:55)→ 2, C3 (cid:55)→ 1, C4 (cid:55)→ 2}
ξ := ker(M)
Equivalence classes: O/ξ = {{0, 2}, {1, 3}}
Figure 6: Steps of the algorithm SynSMT for Example 1: The left box depicts the input, the bottom right the output, and the
gray boxes depict intermediate computation steps of the algorithm.
The function Objsing(n) maximizes the count of all singleton classes,
i.e. it maximizes the answer precision.
To check whether there exists an enforcement ξ such that we
have Enf(π , ξ), b |= Φ, the algorithm calls IsSat(ψassert), which
returns whether ψassert is satisfiable. If ψassert is unsatisfiable, then
it simply returns unsat. Otherwise, SynSMT calls the procedure
Max(ψassert,ψobj), which returns a model M of the SMT constraint
ψassert that maximizes the objective function ψobj. Finally, it re-
turns ker(M) which defines the enforcement identified by the vari-
ables C1, . . . , C|O|.
Example 5.1. We consider the same scenario as the one in our
motivating example; see Section 2.1. The input to SynSMT is given
in Figure 6 (left). The program π returns the number of patients
with two adenine (A) nucleotides and randomly adds ±1 to the
result. Since there are three patients, the set of outputs for π is
O = {0, 1, 2, 3}.
The attacker belief is defined by belief(). In contrast to the belief
of Figure 2(b), here we assume that the frequency of guanine is 0.5
to simplify the fractions in our example.
The policy Φ states that according to the attacker belief: (S1) the
probability that Bob’s nucleotides are AA is between 0.1 and 0.5, (S2)
the probability that Alice has a guanine nucleotide is between 0.5
and 0.9.
We now describe the intermediate steps of SynSMT, depicted
in the gray boxes in Figure 6. First, we compute the probabilities
δ (O = o) and Pπ
δ (I ∈ Si
| O = o) for each output o ∈ {0, . . . , 3}
Pπ
and belief bound i ∈ {1, 2}.
Next, SynSMT generates the SMT constraint ψassert and the ob-
jective function ψobj (chosen as Objcls(n) for this example to op-
timize permissiveness), over the integer variables C1, C2, C3, and
C4. The constraints ψrange restricts the range of all the variables
to {1, 2, 3, 4}, and ψbounds restricts the probability of the predicates
S1 and S2 in all equivalence classes to be in [0.1, 0.5] and [0.5, 0.9],
respectively. The formulas pi1 and pi2 are symbolically defined as
described in Figure 5.
A model of ψassert that maximizes the objective ψobj is M =
{C1 (cid:55)→ 1, C2 (cid:55)→ 2, C3 (cid:55)→ 1, C4 (cid:55)→ 2}. Hence, the synthesized
enforcement is O/ker(M) = {{0, 2}, {1, 3}}.
(cid:4)
Finally, we state the correctness of our algorithm.
Theorem 5.2. Let π be a probabilistic program, δ an attacker
belief, and Φ a privacy policy. If there is no enforcement ξ such
that Enf(π , ξ), δ |= Φ, then SynSMT(π , δ, Φ) = unsat. Otherwise,
SynSMT(π , δ, Φ) = ξ such that ξ is an optimally permissive enforce-
ment for π, δ, and Φ.
The proof of this theorem is in Appendix A.
Session B4:  Privacy PoliciesCCS’17, October 30-November 3, 2017, Dallas, TX, USA398Algorithm 2: The algorithm SynGrd(π , δ, Φ)
Input: A probabilistic program π, an attacker belief δ, a policy
Φ = {(S1,[a1, b1]), . . . ,(Sk ,[ak , bk]), and an
optimization goal
goal ∈ {”permissiveness”, ”precision”}
Output: An equivalence relation ξ enforcing the policy.
ifk
1 begin
2
3
i =1 Pπ
return unsat
δ (I ∈ Si) (cid:60) [ai , bi] then
if goal = ”permissiveness” then
ξ ← {(o, o) | o ∈ O}
δ,π
E
else if goal = ”precision” then
C ← {o ∈ O | (cid:174)p
δ,π{o} (cid:60) (cid:174)αΦ}
ξ ← {(o, o) | o ∈ O} ∪ C × C
while ∃E ∈ O/ξ : (cid:174)p
(cid:60) (cid:174)αΦ do
E ← arg maxE∈O/ξ Dist((cid:174)p
if ∃E′ ∈ O/ξ : (cid:174)p
E′ ← arg minE′∈O/ξ ∥(cid:174)p
E′ ← arg minE′∈O/ξ Dist((cid:174)p
δ,π
ξ ← ξ ∪ {(e, e′) | e ∈ E ∧ e′ ∈ E′}
E∪E′ ∈ (cid:174)αΦ then
, (cid:174)αΦ)
δ,π
E
else
δ,π
δ,π
E∪E′ − ( ai +bi2
E∪E′, (cid:174)αΦ)
return ξ
)k
i =1∥
4
5
6
7
8
9
10
11
12
13
14
15
16
6 EFFICIENT GREEDY SYNTHESIS
ALGORITHM
=
δ,π
X
| O ∈ X))k
We present our algorithm SynGrd, which produces correct enforce-
ments but does not guarantee that they are optimal.
High-level Idea. When optimizing for permissiveness, SynGrd
starts with the most refined equivalence relation ξ⊤, which has one
equivalence class per output, and then iteratively joins equivalence
classes until the relation enforces the policy for the given attacker.
When optimizing for answer precision, we start with an equivalence
relation that joins all the outputs that violate the policy. Then, in
each step, the algorithm selects an equivalence class E that, if output
to the attacker, would violate the policy. The class E is then joined
with another equivalence class E′ such that the revised attacker
belief about the predicates defined in the policy after observing the
new equivalence class E ∪ E′ is closer to the bounds defined in the
policy. We detail these steps below.
Notation. We use the following notation. Let the policy be Φ =
{(S1,[a1, b1]), . . . ,(Sk ,[ak , bk])} with k = |Φ|. Given an equiva-
lence class X ⊆ O, we define the k-dimensional vector (cid:174)p
δ,π
X
(Pπ
δ (I ∈ Si
i =1, where the i-th element is the attacker
belief about the predicate Si given that the program returns equiv-
alence class X. Further, we define the k-dimensional box (cid:174)αΦ =
i =1, where [ai , bi] are the bounds defined by the belief
([ai , bi])k
bound (Si ,[ai , bi]) ∈ Φ. We write (cid:174)p
∈ (cid:174)αΦ to denote that for each
belief bound (Si ,[ai , bi]), we have Pπ
δ (I ∈ Si | O ∈ X) ∈ [ai , bi]. An
equivalence relation ξ enforces the privacy policy Φ for the given
π and δ if for each equivalence class E ∈ O/ξ, we have (cid:174)p
∈ (cid:174)αΦ.
δ,π
E
In Figure 7, we depict the 2-dimensional box defined by the