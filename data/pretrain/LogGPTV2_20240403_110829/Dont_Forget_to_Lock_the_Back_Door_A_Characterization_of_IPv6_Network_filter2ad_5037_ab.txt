### 97% of Hosts Not in PBL Ranges

As expected, 97% of the hosts are not within the PBL (Policy-Based Lists) ranges. The raw DNS dataset comprises approximately 1.4 billion name-to-address mappings. After filtering the list to include only A and AAAA records, we identify common names and addresses, as outlined above, to reduce the dataset to 2.4 million addresses associated with 950,000 dual-stacked hosts.

### Probing Methodology

We conducted active probing to evaluate the security postures of IPv4 and IPv6. For routers, we probed services that are:
- Likely to be running on routers (e.g., SSH)
- Crucial for router operation (e.g., BGP)
- Potentially problematic if exploited by an attacker (e.g., NTP)

The exposure of these ports generally increases the attack surface of routers. We probed the following services on all routers in our dataset:
- ICMP echo
- SSH (TCP/22)
- Telnet (TCP/23)
- HTTP (TCP/80)
- BGP (TCP/179)
- HTTPS (TCP/443)
- DNS (UDP/53)
- NTP (UDP/123)
- SNMPv2 (UDP/161)

Similarly, after developing a list of dual-stack servers, we performed active probing with a different set of application types more relevant to servers:
- ICMP echo
- FTP (TCP/21)
- SSH (TCP/22)
- Telnet (TCP/23)
- HTTP (TCP/80)
- HTTPS (TCP/443)
- SMB (TCP/445)
- MySQL (TCP/3306)
- RDP (TCP/3389)
- DNS (UDP/53)
- NTP (UDP/123)
- SNMPv2 (UDP/161)

To select these applications, we consulted literature on the prevalence of scanning, port blocking, and DDoS amplification susceptibility. We aimed to minimize probing of less deployed or less interesting ports and to keep the set small to minimize the load on targets. The potential impact of a breach was the most critical factor for inclusion.

### Data Collection Methods

We used two methods to collect the data:
1. **Basic Probing**: Single probe packets to each service via both IPv4 and IPv6.
   - For ICMP: Echo request
   - For TCP: SYN segment
   - For UDP: Application-specific request (e.g., DNS A query for "www.google.com", NTP version query, or SNMP query for the sysName.0 MIB using the default public community string)
   
   We probed every two weeks starting in mid-January 2015 for routers and mid-February 2015 for servers through July 2015. We found minimal differences over time, so we focused on the router collection from February 19, 2015 (denoted RB) and the server collection from April 10, 2015 (denoted SB).

2. **Traceroute Probing**: Traceroute-style measurements using the same probe types. We collected several traceroute datasets. Our analysis showed similar results across time, so we concentrated on the router dataset collected on June 5, 2015 (denoted RT) and the server dataset collected on July 10, 2015 (denoted ST).

### Initial Experiments and Adjustments

After initial experiments, some applications showed very low dual-stack response rates (generally less than 0.1%). To focus on more prevalent applications, we excluded:
- For routers: TFTP
- For servers: IPMI, MS-SQL, NetBIOS, SSDP, VNC, and SNMPv1 (as results closely matched SNMPv2)

### Measurement Tool and Configuration

We used Scamper, a parallelized bulk probing tool supporting various types of probes, including ping and traceroute over ICMP, TCP, and UDP, for both IPv4 and IPv6. We extended Scamperâ€™s traceroute implementation to record UDP application-level responses. We configured Scamper to probe the listed ports using each IPv4 and IPv6 address of every host, testing one port at a time in random order to avoid triggering rate limiting. We set the probing rate to 5000 packets per second. The basic router measurements took approximately eight hours, while the basic server measurements took about 22 hours. We paused for at least one second between measurements to a given host to spread the load and minimize the impact on individual hosts.

### Dataset Summary

| Probe Date (2015) | Dataset | Names | Addresses | IPv4 | IPv6 | Hosts | Total | Suitable |
|-------------------|---------|-------|-----------|------|------|-------|-------|----------|
| Feb 19th          | RB      | 41K   | 38K       | 8.3M | 1.0M | 41K   | 1.4M  | 520K     |
| Jun 5th           | RT      | 41K   | 35K       | 8.5M | 1.0M | 41K   | 1.4M  | 533K     |
| Apr 10th          | SB      | 35K   | 35K       | 8.3M | 1.0M | 25K   | 947K  | 520K     |
| Jul 10th          | ST      | 38K   | 38K       | 8.5M | 1.0M | 25K   | 951K  | 533K     |

The "suitable" column represents the hosts that respond to ICMP echo in both IPv4 and IPv6. We only measured policy congruity on suitable hosts to avoid mistaking unreachable hosts for those with enforced application policies.

### Distribution of Measured Hosts

The measured hosts are distributed across the network, covering 58% of dual-stack ASes observed in public BGP tables. The RT target list includes hosts from over 2,000 routed prefixes, 1,000 autonomous systems (ASes), and 70 countries. The ST target list includes hosts from over 15,000 routed prefixes, 5,000 ASes, and 133 countries. The distribution is skewed, with 19 ASes belonging to the top ten network operators accounting for half the hosts in the RT list, and ten ASes belonging to large hosting/content providers making up half the servers in the ST list.

### Ethical Considerations

Active network measurement research can raise ethical issues. We followed guidelines to minimize potential harm:
- Signaled benign intent through WHOIS, DNS, and a website on every probe IP address
- Rate-limited probes to minimize impact
- Limited to regular TCP/UDP connection attempts without exploiting vulnerabilities, guessing passwords, or changing configurations
- Respected opt-out requests and notified networks of vulnerable configurations before publication

### Result Interpretation

We defined success as receiving:
- An ICMP echo reply for ICMP echo requests
- A TCP SYN+ACK for TCP SYN
- A UDP response for UDP requests

Anything else, including no response, was considered a failure. We made final IPv4 and IPv6 determinations based on majority votes across all addresses when multiple were present.

### Terminology

We use "IP" for the versions studied (OSI layer 3) and "application" for higher-layer protocols (e.g., SSH, NTP).

### Calibration

We assumed that all labels for a host point to the same host. To test this, we collected application-level information for each host via both IPv4 and IPv6. The table below shows the consistency of signatures across hosts:

| Application | RB List | SB List |
|-------------|---------|---------|
| HTTP        | 269 (1.1%) | 235,575 (46.2%) |
| HTTPS       | 183 (0.8%) | 96,468 (18.9%) |
| SNMPv2c     | 12 (0.1%) | 41 (0.0%) |
| NTP         | 843 (3.6%) | 3,462 (0.7%) |
| SSH         | 603 (2.6%) | 218,100 (42.8%) |
| MySQL       | - | 1,055 (0.2%) |
| Overall     | 1576 (6.7%) | 303,111 (59.4%) |
| Same Sig    | 97.0% | 99.2% |
|             | 96.7% | 94.2% |
|             | 100% | 95.1% |
|             | 97.0% | 99.1% |
|             | 96.7% | 98.9% |
|             | - | 99.5% |
|             | 96.4% | 97.1% |

Our technique could access at least one signature-providing application for 44% and 76% of the hosts in RB and SB, respectively. We calibrated our technique to aggregate sets of labels into hosts, and the sample, though biased toward more open hosts, seems sufficiently large to represent such name-based aggregation.