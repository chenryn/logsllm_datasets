that 97% of the hosts are not in PBL ranges, as expected.
The raw DNS dataset represents approximately 1.4B name to
address mappings. After culling the list to just A and AAAA
records, we detect common names and addresses, as sketched
above, to reduce the set to 2.4M addresses associated with
950K dual-stacked hosts.
B. Probing
We perform active probing to assess security posture
differences between IPv4 and IPv6. When probing routers,
we use services that are some combination of (i) likely to be
running on routers (e.g., SSH), (ii) crucial to router operation
(e.g., BGP) and/or (iii) problematic when leveraged by an
attacker (e.g., NTP [18]). Thus, the exposure of all of these
ports generally increases the attack surface of routers. We probe
these services on all routers in our dataset:
ICMP echo, SSH (TCP/22), Telnet (TCP/23), HTTP
(TCP/80), BGP (TCP/179), HTTPS (TCP/443), DNS
(UDP/53), NTP (UDP/123), SNMPv2 (UDP/161)
Similarly, after developing the list of dual-stack servers, we
perform active probing but with a different set of application
types that are more apropos for servers than routers, as follows:
ICMP echo, FTP (TCP/21), SSH (TCP/22), Telnet
(TCP/23), HTTP (TCP/80), HTTPS (TCP/443),
SMB (TCP/445), MySQL (TCP/3306), RDP
(TCP/3389), DNS (UDP/53), NTP (UDP/123),
SNMPv2 (UDP/161)
To select
these, we consulted literature on prevalence of
scanning [21], prevalence of port blocking [33], as well as
application DDoS ampliﬁcation susceptibility [44]. We wanted
to minimize probing ports of lower deployment or interest, as
well as constrain our set to a small number so as to minimize
load on targets. Ultimately, the potential impact of breach was
the most important factor for inclusion1.
We use two probing methods to collect the data we use in the
remainder of the paper. “Basic” probing consists of single probe
packets to each service via both IPv4 and IPv6. For ICMP this
is an echo request, for TCP it is a SYN segment, and for UDP
this is an application-speciﬁc request (e.g., DNS A query for
“www.google.com”, NTP version query, or SNMP query for the
sysName.0 MIB using the default public community string). We
probe every two weeks starting in mid-January 2015 and mid-
February 2015 for the routers and servers, respectively, through
July 2015. We found little difference between results over time
and, therefore, focus on the router collection from February 19,
2015, which we denote RB and the server collection from April
10, 2015, denoted SB. Our second probing strategy is based on
traceroute style measurements using the same probe types. We
also collected a number of these traceroute datasets. Again,
our analysis shows similar results across time and therefore
we concentrate on the router dataset collected on June 5, 2015,
which we denote RT , and the server dataset collected on July
10, 2015, which we denote ST .
1After initial experiments, several applications were showing minuscule
dual-stack response rates (generally a tenth of a percent or less). To focus on
more prevalent applications, we dropped these from study. They included, for
routers: TFTP, and for servers: IPMI, MS-SQL, NetBIOS, SSDP, and VNC.
We also excluded SNMPv1, as results closely matched SNMPv2.
3
We used scamper [35], a parallelized bulk probing tool that
supports various types of probes, including ping and traceroute
over ICMP, TCP, and UDP, to conduct our measurements for
both IPv4 and IPv6. Because there was no implementation
of traceroute that considered application-responses (traditional
traceroute deliberately chooses high-numbered, unused ports
to solicit ICMP port unreachable error messages) we extended
scamper’s traceroute implementation to record UDP application-
level responses. We conﬁgured scamper to probe the ports listed
above using each IPv4 and IPv6 address of every host. To limit
the burden on our measurement targets we tested one port at a
time and in random order. Our goal in doing so was to remove
the possibility that we would trigger rate limiting by probing
the host too quickly and thus raise the possibility that a host that
was initially responsive would become unresponsive, conﬂating
a rate-induced outage with a policy to discard speciﬁc types
of packets. We conﬁgured scamper to probe at 5000 packets
per second; the basic router measurements took approximately
eight hours to measure while the basic server measurements
took approximately 22 hours. We paused for at least 1 second
between measurements to a given host. Therefore, despite a
relatively high probing rate, we spread the load across a set
of targets so that we had negligible impact on individual hosts
measured.
TABLE I: Dataset summary
Probe Date
Dataset
(2015)
Names
Addresses
IPv4
IPv6
Hosts
Total
Suitable
RB
RT
SB
ST
Feb 19th
Jun 5th
Apr 10th
Jul 10th
41K
41K
35K
38K
38K
35K
8.3M 1.0M 1.4M 947K
8.5M 1.0M 1.4M 951K
41K
41K
25K
25K
520K
533K
Table I describes the datasets we use in the remainder of the
paper. The “suitable” column represents the hosts we probe and
is the set that respond to ICMP echo in both IPv4 and IPv6. We
only measure policy congruity on suitable (responsive) hosts to
avoid mistaking a completely unreachable host from one where
application policy controls are enforced. While this test is not
foolproof, we know these hosts are responsive to both IPv4
and IPv6. We could be excluding hosts that apply different
ICMP policies to IPv4 and IPv6, as well as those that ﬁlter
all ICMP requests. However, we believe the set of hosts we
leverage is large enough to give us broad insight into the policy
differences between IPv4 and IPv6 across the Internet.
Hosts we measure are spread across the network, and
encompass 58% of dual-stack ASes observed in public BGP
tables (all available Route Views [52] and RIPE RIS [43]) on
midnight February 1, 2015. The RT target list contains hosts
from over 2K routed preﬁxes, 1K autonomous systems (ASes)
and 70 countries. The ST target list, on the other hand, contains
hosts from over 15K routed preﬁxes, 5K ASes and 133 countries.
Unsurprisingly, while we leverage a breadth of targets, the set is
also skewed. In the RT list we ﬁnd that 19 ASes that belong to
the ten most-represented network operators in our list account
for half the hosts. Similarly, we ﬁnd that ten ASes belonging
to large hosting/content providers make up half the servers.
4
C. Ethical Considerations of Probing
Research involving active measurement of networks po-
tentially creates ethical issues as both the conduct of such
research and the disclosure of results thereof may result in
harm to a variety of stakeholders, including, but not limited to:
research institutions, service providers, network operators, and
end users. We take note that, while the security community
has not reached consensus on standards for such research,
existing published work in the ﬁeld [22], as well as broad
ethical guidelines [9] provide a roadmap for how one may
minimize the potential for such harms. For example, in the
conduct of this research we: (i) signaled the benign intent of
work through WHOIS, DNS, and by providing research details
on a website on every probe IP address; (ii) signiﬁcantly rate
limited the probes to minimize impact; (iii) limited ourselves
to regular TCP/UDP connection attempts followed by RFC-
compliant protocol handshakes with responsive hosts that never
attempt to exploit vulnerabilities, guess passwords, or change
conﬁgurations; (iv) we respect opt out requests and seed our opt
out list with previous requests provided to other researchers [22].
In mitigating disclosure harms, we carefully avoid providing
target lists in the published result and notify, by email to abuse
contacts, the most egregious networks prior to publication, so
that they may correct vulnerable conﬁgurations.
D. Result Interpretation
One ﬁnal methodological task involves interpreting the
results of the probes. First, we must decide if a probe succeeds
or fails. We deﬁne success as reception of (i) an ICMP echo
reply message in response to an ICMP echo request, (ii) a TCP
SYN+ACK in response to a TCP SYN and (iii) a UDP response
to a UDP request. We consider anything else—including no
response—as connection failure (e.g., ICMP unreachables, a
non-SYN+ACK TCP packet). Once we have a decision for
probes within some Hx
4, we make a ﬁnal IPv4 determination
based on majority vote across all IPv4 addresses when there
are multiple. Likewise for the IPv6 addresses Hx
6.
Lastly, a minor note on terminology: the versions of IP
we study (at OSI layer 3) as well as the applications we
probe (at layers 5-7) can all be called protocols. To avoid
confusion, however, we reserve that term for the IP version
under study and instead use the term application to denote the
protocol/application at the higher layers (e.g., SSH, NTP, etc.)
for which we are measuring connectivity.
III. CALIBRATION
A core assumption we make is that all the labels we ﬁnd
for some host Hx point to the same host. This is not of only
theoretic concern, as previous work shows DNS names mapping
to IPv4 and IPv6 addresses do not always point at a dual-
stack host but instead to multiple hosts [12]. Since our goal
is understanding security posture of dual-stack hosts, we ﬁrst
calibrate our method for aggregating labels into hosts. To test
our assumption we seek to collect application-level information
for each Hx in RB and SB via both IPv4 and IPv6, as follows.
HTTP: We send each host that responds to TCP/80 a HEAD
request and extract the server version string (including OS)
from responses (e.g., “Apache/2.2.22 (Debian)”). While we do
not exclude any, the three most frequently returned strings are
TABLE II: Alias validation via application signatures. A
majority (96% of RB with data and 97% of SB) of hosts
with signature data matched ﬁngerprints among all host
address members.
Application
http
https
snmp2c
ntp
ssh
mysql
Overall
RB List
SB List
Hosts
Same Sig
Hosts
Same Sig
269 (1.1%)
183 (0.8%)
12 (0.1%)
843 (3.6%)
603 (2.6%)
–
1576 (6.7%)
97.0%
96.7%
100%
97.0%
96.7%
–
96.4%
235,575 (46.2%)
96,468 (18.9%)
41 (0.0%)
3,462 (0.7%)
218,100 (42.8%)
1,055 (0.2%)
303,111 (59.4%)
99.2%
94.2%
95.1%
99.1%
98.9%
99.5%
97.1%
indistinct as version and OS are not provided (e.g., “Apache”
(39%), “nginx” (23%), and “cloudﬂare-nginx” (6%)). The
next 20 most-frequently returned strings are more speciﬁc
and provide a stronger ﬁngerprint for matching.
HTTPS: We are able to collect an extensive set of information
about each host responding on TCP/443 probes, using both
the openssl client and NMAP with the ssl-enum-ciphers.nse
script [3], including: (i) the supported cipher suites (for all
except SSLv2-only hosts); (ii) the supported SSL/TLS protocol
subset of {SSLv2, SSLv3, TLSv1, TLSv1.1, and TLSv1.2};
(iii) the actually negotiated protocol between client and server;
and (iv) the server’s certiﬁcate ﬁngerprint.
SNMP: For SNMP, we retrieve two MIBs—sysDescr.0 and
sysName.0—via SNMP version 2c get requests to the public
community. Responses include the OS (including version) and
an administrator-set system name. We require responses to both
gets to complete an identifying ﬁngerprint.
NTP: For each host responding to UDP/123 queries we issue
the version command using the ntpq tool. This provides a
semi-structured string containing: version, processor, system,
stratum, precision, reﬁd, reftime, frequency, status, and associd
among other ﬁelds that we do not further utilize as they are less
common or vary across queries–not useful for ﬁngerprinting.
SSH: We use the ssh-keyscan utility (part of the OpenSSH-
clients tools) to obtain the SSH server version, the key length
of the server’s encryption key, and the ﬁngerprint of the key.
MySQL: For hosts with open TCP/3306 we send two newline
characters causing servers to print an identifying banner, which
we harvest (stripping unprintable characters). As noted above,
we do not probe MySQL on routers, and, therefore, this
ﬁngerprint is only available for the server host list.
For each host x on our host lists we collect the above
information via probing to every IP address in Hx
6. We
then check for consistent behavior across all applications that
respond on all addresses. When we ﬁnd consistency across all
IP addresses for Hx we conclude that the host is highly likely
to be a single dual-stack host. Even if this conclusion is wrong,
we believe the identical conﬁguration indicates the operator’s
intention is to provide the same service across IPv4 and IPv6
and therefore policy differences are important to illuminate.
4 and Hx
our basic probe results for each host across applications tested
and both IPv4 and IPv6 (i.e., a very coarse-grained analysis of
the results we discuss in sections to come), we ﬁnd that we can
access at least one of these signature-providing applications via
at least one of a given host’s IP addresses for 44% and 76%
of the hosts in RB and SB, respectively. In other words, our
technique will have no data at all to ﬁngerprint a host for over
half the routers and nearly a quarter of servers. In fact, since
we need at least one IPv4 and one IPv6 ﬁngerprint for the
same port to do matching, the number of hosts we can actually
match signatures for across IP versions is even lower (7% and
59%, respectively, as discussed below). However, we stress that
we are calibrating our technique of aggregating sets of labels
(i.e., results for IPs grouped via their associated A, AAAA, and
PTR records) into hosts and not each individual assessment. As
such, the sample, though biased toward more open hosts, seems
sufﬁciently large to represent such name-based aggregation.