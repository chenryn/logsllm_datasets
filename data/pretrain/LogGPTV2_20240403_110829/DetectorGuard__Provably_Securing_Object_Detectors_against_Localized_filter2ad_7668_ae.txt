### Our BagNet-33 Implementation

**Figure 4: Clean Performance of DetectorGuard on PASCAL VOC**

- **PCD**: Perfect Clean Detector
- **YOLO**: YOLOv4
- **FRCNN**: Faster R-CNN
- **FAR**: False Alert Rate

**False Alert Rate (FAR@0.x)**:
- **Definition**: FAR is the percentage of clean images on which DetectorGuard triggers a false alert.
- **Causes**: The primary cause of false alerts is Clean Error 2, as discussed in Section 3.
- **Confidence Threshold Impact**: A higher confidence threshold in the Base Detector leads to fewer predicted bounding boxes, resulting in more unexplained objectness and, consequently, a higher FAR.
- **Reporting**: We report FAR at different recall levels for a comprehensive evaluation. FAR@0.x denotes FAR at a clean recall of 0.x.

**Provable Robustness Metric: Certified Recall (CR@0.x)**
- **Definition**: Certified Recall is the percentage of ground-truth objects with provable robustness against any patch hiding attack. An object is considered robust if Algorithm 2 (our provable analysis) returns True.
- **Base Detector Impact**: CR is influenced by the Base Detector's performance, particularly its confidence threshold, as it requires correct clean detection.
- **Notation**: CR@0.x denotes the certified recall at a clean recall of 0.x.

### 5.3 Clean Performance

In this section, we evaluate the clean performance of DetectorGuard using three different base object detectors (PCD, YOLOv4, and Faster R-CNN) on three datasets (PASCAL VOC, MS COCO, and KITTI).

**Table 2: Clean Performance Metrics**
- **Vanilla AP**: Average Precision of the Base Detector without defense.
- **Defense AP**: Average Precision of DetectorGuard.
- **FAR@0.8 or FAR@0.6**: False Alert Rate at a clean recall of 0.8 or 0.6.

**Precision-Recall and FAR-Recall Curves**:
- **PASCAL VOC**: Plotted in Figure 4.
- **MS COCO and KITTI**: Plots available in Appendix D.

**Key Observations**:
- **Low FAR and High AP**: DetectorGuard achieves a low FAR (0.9%) and high AP (99.3%) on PASCAL VOC when using a perfect clean detector, indicating minimal impact on clean performance.
- **Compatibility with Different Detectors**: DetectorGuard maintains high clean performance when used with YOLOv4 and Faster R-CNN, as shown in Table 2 and Figure 4.
- **Generalizability Across Datasets**: DetectorGuard performs well across PASCAL VOC, MS COCO, and KITTI, achieving low FAR and similar AP to the vanilla Base Detector.

**Remark**: DetectorGuard incurs a negligible cost in clean performance (<1% AP drop), which is a worthwhile trade-off given its first provable robustness against patch hiding attacks.

### 5.4 Provable Robustness

**Setup**:
- **Adversarial Patch**: A 32Ã—32 adversarial pixel patch on re-scaled and padded 416Ã—416 (or 224Ã—740) images.
- **Patch Locations**: All possible image locations are considered.
- **Categories**: Over-patch (patch over the object), Close-patch (feature-space distance < 8), and Far-patch (other locations).
- **Evaluation**: Using a 500-image subset of the test/validation datasets, we use Algorithm 2 to determine robustness.

**Key Results**:
- **First Non-Trivial Provable Robustness**: DetectorGuard achieves non-trivial provable robustness against patch hiding attacks, as shown in Table 3.
- **Robustness Improvement**: Provable robustness improves with increasing clean recall, and YOLOv4 and Faster R-CNN perform close to a perfect clean detector at high recall.
- **Effectiveness Against Far-Patch Threats**: DetectorGuard is particularly effective when the patch is far from the object, making attacks more difficult.
- **Object Size Correlation**: Larger objects have higher certified recalls, aligning with the expectation that detecting small objects is inherently challenging.

**Remark on Absolute Values**:
- **Certified Recall Limitations**: Despite achieving the first non-trivial provable robustness, absolute values of certified recalls are still limited due to the strong and conservative nature of the metric.
- **Future Work**: The security community can build upon this work to further improve certified recall.

### 5.5 Detailed Analysis of DetectorGuard

**Runtime Analysis**:
- **Table 4: Per-Example Runtime Breakdown**:
  - **Base Detector**: YOLOv4 and Faster R-CNN runtimes.
  - **Objectness Predictor and Explainer**: Introduce negligible overhead.
  - **Parallel Execution**: With 2 GPUs, DetectorGuard's runtime is close to that of the Base Detector.
  - **Single GPU**: Runtime increases by approximately 2x.

**Binarizing Threshold Effect**:
- **Varying Threshold ð‘‡**: We analyze the impact of different binarizing thresholds on CR, AP, and 1-FAR.
- **Trade-Off**: Balancing clean performance and provable robustness, we set ð‘‡ = 32 to achieve a FAR lower than 1% while maintaining decent provable robustness.

This detailed analysis provides a comprehensive understanding of DetectorGuard's performance and robustness, highlighting its effectiveness and potential for future improvements.