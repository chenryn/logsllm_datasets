our BagNet-33 implementation.
Figure 4: Clean performance of DetectorGuard on PASCAL
VOC (PCD – perfect clean detector; YOLO – YOLOv4; FRCNN –
Faster R-CNN; FAR – False Alert Rate)
False Alert Rate (FAR@0.x). FAR is defined as the percentage of
clean images on which DetectorGuard will trigger a false alert.
The false alert is mainly caused by Clean Error 2 as discussed in
Section 3. We note that FAR is also closely tied to the confidence
threshold of Base Detector: a higher confidence threshold leads
to fewer predicted bounding boxes, leading to more unexplained
objectness, and finally higher FAR. We will report FAR at different
recall levels for a global evaluation, and use FAR@0.x to denote
FAR at a clean recall of 0.x.
Provable Robustness Metric:
Certified Recall (CR@0.x). We use certified recall as the robust-
ness metric against patch hiding attacks. The certified recall is de-
fined as the percentage of ground-truth objects that have provable
robustness against any patch hiding attack. Recall that an object
has provable robustness when Algorithm 2 (our provable analysis)
returns True. Note that CR is also affected by the performance of
Base Detector (e.g., confidence threshold) since its prerequisite is
the correct clean detection. We use CR@0.x to denote the certified
recall at a clean recall of 0.x.
5.3 Clean Performance
In this subsection, we evaluate the clean performance of Detec-
torGuard with three different base object detectors and three datasets.
In Table 2, we report AP of vanilla Base Detector (vanilla AP), AP of
DetectorGuard (defense AP), and False Alert Rate at a clean recall of
0.8 or 0.6 (FAR@0.8 or FAR@0.6). We also plot the precision-recall
102030405060708090100Recall (%)0102030405060708090100Precision / FAR (%)Precision-PCD-vanillaPrecision-PCD-defendedPrecision-YOLO-vanillaPrecision-YOLO-defendedPrecision-FRCNN-vanillaPrecision-FRCNN-defendedFAR-PCD-defendedFAR-YOLO-defendedFAR-FRCNN-defendedSession 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3185Table 3: Provable robustness of DetectorGuard
PASCAL VOC (CR@0.8)
far-patch
close-patch
over-patch
MS COCO (CR@0.6)
far-patch
close-patch
over-patch
far-patch
KITTI (CR@0.8)
close-patch
over-patch
28.6%
24.6%
25.7%
20.7%
18.6%
19.4%
8.3%
7.5%
8.0%
11.5%
10.1%
10.7%
7.0%
6.5%
6.8%
2.2%
1.9%
2.0%
32.0%
30.6%
31.6%
11.1%
10.9%
11.0%
2.1%
2.1%
2.1%
Perfect clean detector
YOLOv4
Faster R-CNN
and FAR-recall curves for PASCAL VOC in Figure 4; similar plots
for MS COCO and KITTI are in Appendix D.
DetectorGuard has a low FAR and a high AP. We can see
from Table 2 that DetectorGuard has a low FAR of 0.9% and a high
AP of 99.3% on PASCAL VOC when we use a perfect clean detector
as Base Detector. The result shows that DetectorGuard only has a
minimal impact on the clean performance.
DetectorGuard is highly compatible with different con-
ventional object detectors. From Table 2 and Figure 4, we can
see that when we use YOLOv4 or Faster R-CNN as Base Detector
on PASCAL VOC, the clean AP, as well as the precision-recall curve
of DetectorGuard, is close to that of its vanilla Base Detector. These
results show that DetectorGuard is highly compatible with different
conventional object detectors.
DetectorGuard works well across different datasets. We
can see that the observation of high clean performance is simi-
lar across three different datasets: DetectorGuard achieves a low
FAR and a similar AP as the vanilla Base Detector on PASCAL VOC,
MS COCO, and KITTI (the precision-recall plots for MS COCO and
KITTI are available in Appendix D). These similar results show
that DetectorGuard is a general approach and can be used for both
easier and more challenging detection tasks.
Remark: a negligible cost of clean performance. In this sub-
section, we have shown that DetectorGuard only incurs a negli-
gible cost of clean performance (<1% AP drop). This slight clean
performance drop is worthwhile given the first provable robustness
against patch hiding attacks (evaluated in the next subsection).
5.4 Provable Robustness
In this subsection, we first introduce the robustness evaluation setup
and then report the provable robustness of our defense against any
patch hiding attack within our threat model.
Setup. We use a 32×32 adversarial pixel patch on the re-scaled
and padded 416×416 (or 224×740) images to evaluate the provable
robustness.11 We consider all possible image locations as candidate
locations for the adversarial patch to evaluate the model robustness.
We categorize our results into three categories depending on the
distance between an object and the patch location. When the patch
is totally over the object, we consider it as over-patch. When the
feature-space distance between patch and object is smaller than 8,
we consider it as close-patch. The other patch locations are consid-
ered as far-patch. For each set of patch locations and each object,
we use Algorithm 2 to determine the robustness. We note that the
11DPatch [30] demonstrates that even a 20×20 adversarial patch at the image corner can
have a malicious effect. In Appendix A, we show that more than 15% of PASCAL VOC
objects and 44% of MS COCO objects are smaller than a 32×32 patch. We also provide
robustness results for different patch sizes as well as visualizations in Appendix A.
Figure 5: Provable robustness of DetectorGuard on PASCAL
VOC (PCD – Perfect Clean Detector; YOLO – YOLOv4; FRCNN –
Faster R-CNN)
above algorithm already considers all possible adaptive attacks (at-
tacker strategies) at any location within our threat model. We use
CR@0.x as the robustness metric. Given the large number of all
possible patch locations, we only use a 500-image subset of the
test/validation datasets for evaluation.
DetectorGuard achieves the first non-trivial provable ro-
bustness against patch hiding attack. We report the certified
recall at a clean recall of 0.8 or 0.6 (CR@0.8 or CR@0.6) in Table 3.
As shown in Table 3, when we use a Perfect Clean Detector, De-
tectorGuard can certify the robustness for 28.6% of PASCAL VOC
objects when the patch is far away from the object; which means
no attacker within our threat model can successfully attack these
certified objects. We also plot the CR-recall curves for different
detectors on the PASCAL VOC dataset in Figure 5 (similar plots for
MS COCO and KITTI are in Appendix D). The figure shows that
the provable robustness improves as the clean recall increases, and
the performance of YOLOv4 and Faster R-CNN is close to that of a
perfect clean detector when the recall is close to one.
DetectorGuard is especially effective when the patch is far
away from the objects. From Table 3 and Figure 5, we can clearly
see that the provable robustness of DetectorGuard is especially good
when the patch gets far away from the object. This model behavior
aligns with our intuition that a localized adversarial patch should
only have a spatially constrained adversarial effect. Moreover, this
observation shows that DetectorGuard has made the attack much
more difficult: to have a chance to bypass DetectorGuard, the ad-
versary has to put the patch close to or even over the victim object,
which is not always feasible in real-world scenarios. We also note
102030405060708090100Clean Recall (%)051015202530Certified Recall (%)PCD-farPCD-closePCD-overYOLO-farYOLO-closeYOLO-overFRCNN-farFRCNN-closeFRCNN-overSession 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3186that in the over-patch threat model, we allow the patch to be any-
where over the object. This means that the patch can be placed over
the most salient part of the object (e.g., the face of a person), and
makes robust detection extremely difficult.
DetectorGuard has better robustness for object classes with
larger object sizes. In Figure 6, we plot the certified recalls against
the close-patch attacker (similar plots for far-patch and over-patch
and for other datasets are in Appendix D) and average object sizes
(reported as the percentage of image pixels) for all 20 classes of
PASCAL VOC. As shown in the figure, the provable robustness of
DetectorGuard is highly correlated with the object size: we have
higher certified recalls for classes with larger object sizes. This is an
expected behavior because it is hard for even humans to perfectly
detect all small objects. Moreover, considering that missing a big
nearby object is much more serious than missing a small distant
object in real-world applications, we believe that DetectorGuard
has strong application potential.
Remark on absolute values of certified recalls. Despite the
first achieved provable robustness against patch hiding attacks,
we acknowledge that DetectorGuard’s absolute values of certified
recalls are still limited. First, the notion of the certified recall itself is
strong and conservative: we certify the robustness of an object only
when no patch at any location within the threat model can succeed
in the hiding attack (using any attack strategy including adaptive
attacks). In practice, the attacker capability might be limited to a
small number of patch locations. Second, we note that most objects
in our three datasets are small (or even tiny) objects (we provide
a quantitative analysis in Appendix A). Detecting small objects is
already challenging in the clean setting and becomes even more
difficult when an adversarial patch of comparable sizes is used.
However, it is still notable that DetectorGuard is the first to achieve
non-trivial and strong provable robustness against the patch hiding
attacker for “free", and that our approach works particularly well on
certain object classes and threat models (e.g., the class “dog" in the
PASCAL VOC dataset has a ~60% certified recall for the close-patch
threat model). We hope that the security community can build upon
our work and further push the certified recall to a higher value.
5.5 Detailed Analysis of DetectorGuard
In this subsection, we first perform a runtime analysis of Detec-
torGuard to show its small defense overhead. Next, we use a hypo-
thetical perfect clean detector (PCD) and the PASCAL VOC dataset
to analyze the performance of DetectorGuard under different hyper-
parameter settings. Note that using PCD helps us to focus on the
behavior of Objectness Predictor and Objectness Explainer.
Runtime analysis. In Table 4, we report the average per-example
runtime of each module in DetectorGuard. For Base Detector, we re-
port runtime for YOLOv4 (left) and Faster R-CNN (right). As shown
in the table, Objectness Predictor has a similar runtime as Base
Detector (or vanilla undefended object detectors), and Objectness
Explainer only introduces a negligible overhead. If 2 GPUs are avail-
able, then Base Detector and Objectness Predictor can run in parallel
in DetectorGuard, and the overall runtime of DetectorGuard can
be calculated as 𝑡DetectorGuard = max(𝑡base, 𝑡predictor) + 𝑡explainer (re-
ported in in the last column of Table 4), which is close to 𝑡base. Thus,
DetectorGuard has a similar runtime performance as conventional
Figure 6: Certified recalls and average object sizes for every
class in PASCAL VOC (reporting CR for close-patch; results for
far-patch and over-patch are in Appendix D)
Table 4: Per-example runtime breakdown
Base Detector
(YOLO / FRCNN)
54.0ms / 80.9ms
55.2ms / 79.4ms
55.8ms / 69.7ms
Objectness Objectness
Predictor
Explainer
48.5ms
65.2ms
44.6ms
DetectorGuard
(YOLO / FRCNN)
54.2ms / 81.1ms
65.5ms / 79.7ms
56.2ms / 70.1ms
0.2ms
0.3ms
0.4ms
VOC
COCO
KITTI
object detectors in the setting of 2 GPUs. If only a single GPU is
available, the runtime can be calculated as 𝑡base+𝑡predictor+𝑡explainer,
which leads to a roughly 2x slow-down.
Effect of the binarizing threshold. We first vary the binariz-
ing threshold 𝑇 in ObjPredictor(·) to see how the model perfor-
mance changes. For each threshold, we report CR for three patch
threat models. We also include AP and 1-FAR to understand the
effect of different thresholds on clean performance. We report these
results in the leftmost sub-figure in Figure 7. We can see that when
the binarizing threshold is low, the CR is high because more object-
ness is retained after the binarization. However, more objectness
also makes it more likely to trigger a false alert in the clean setting,
and we can see both AP and 1-FAR are affected greatly as we de-
crease the threshold 𝑇 . Therefore, we need to balance the trade-off
between clean performance and provable robustness. In our default
parameter setting, we set 𝑇 = 32 to have a FAR lower than 1% while
maintaining decent provable robustness.