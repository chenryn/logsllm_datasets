crawling of a domain to a single short window.
C. Redirection Techniques
We observe three techniques that cloaking applications rely
on to deliver split-view content upon successfully ﬁngerprint-
ing a client. The ﬁrst involves redirecting clients via meta-
refresh, JavaScript, or 30X codes. Crawlers are either stopped
at a doorway or redirected to an entirely different domain than
legitimate clients. Alternatively, websites dynamically render
content via server-side logic to include new page elements
(e.g., embedding an iframe) without changing the URL that
appears in a browser’s address bar. The last technique involves
serving only crawlers a 40X or 50X error. We account for each
of these techniques when designing our anti-cloaking pipeline.
D. Built-in SEO Services
All but one (C6) of the cloaking software under analysis
supports automatic content generation aimed at SEO either
natively or through third-party plugins (e.g., SEnuke, XRumer
which are content spinning programs that create topically
747747
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:15:18 UTC from IEEE Xplore.  Restrictions apply. 
related documents [37]). The most advanced built-in solution
takes a set of targeted search terms from the software’s
operator, after which the program will automatically query
popular search engines for related content, scrape top-ranked
links, and synthesize the content into a seemingly relevant
document from a crawler’s perspective. Furthermore, these
systems automatically detect and exclude copyright infring-
ing content and trademarked names to avoid penalization
or removal by search ranking algorithms. As part of this
document construction, the software will intersperse targeted
search terms with stolen content to increase the frequency of
related terms. We rely on this observation later in Section IV
in order to detect contextual cloaking that requires a visitor’s
HTTP Referer to contain speciﬁc keywords as previously
discussed in this section.
IV. DETECTING CLOAKING
We leverage our tear-down of blackhat cloaking techniques
to build a scalable anti-cloaking pipeline that detects when two
or more distinct browsers are shown divergent content. We
preview our system’s design in Figure 1. We start by aggre-
gating a diverse sample of URLs to scan for cloaking (). We
then fetch each of these URLs via multiple browser proﬁles as
well as network vantage points to trigger any cloaking logic
(). We ﬁnally compare the content, structure, and redirect
graph associated with each fetch () before feeding these
features into a classiﬁer to detect the presence of blackhat
cloaking (). Whereas we note multiple prior studies have
proposed techniques to de-cloak URLs in speciﬁc settings—
particularly redirection cloaking—our goal with this system is
to understand which anti-cloaking techniques generalize across
web cloaking (including mobile and reverse-proxy cloaking),
and similarly, to understand the minimum set of capabilities
required of security scanners to contend with the current
cloaking arms race. We defer concrete implementation details
till Section V.
A. Candidate URL Selection
To conduct our study we aggregate a stream of URLs
from popular websites, search results, and mobile-targeted
advertisements. We split our dataset into two: one part for
training a classiﬁer based on labeled data from previous studies
of cloaking outlined in Table V; and a second sample that we
feed into our classiﬁer to analyze an unbiased perspective of
cloaking in the wild shown in Table VI.
Our training corpus of benign URLs consists of a random
sample of pages appearing in the Alexa Top Million, all of
which we assume to be non-cloaked.1 We later validate this
assumption in Section VII. For labeled instances of cloaked
domains we rely on a feed of counterfeit luxury storefronts
that ﬁngerprint Google’s search bot, maintained by Wang et
al. [31], collected between February, 2015–May, 2015. In total,
we rely on 94,946 URLs for training. We note our labeled
1We reiterate that we treat personalization, geo targeting, and reactive
design as benign and thus non-cloaking. Many of these techniques are present
in the Alexa Top Million. We use a cloaking label only for blackhat techniques.
TABLE V: Breakdown of labeled data we use for training
and evaluating our classiﬁer.
Labeled Dataset
Legitimate websites
Cloaked storefronts
Total
Source
Alexa
SEO abuse [31]
Volume
75,079
19,867
94,946
TABLE VI: Breakdown of unlabeled data that we classify to
study cloaking in the wild.
Unlabeled Dataset
Luxury storefronts
Health, software ads
Total
Source
Google Search
Google Ads
Volume
115,071
20,506
135,577
dataset has a class imbalance toward non-cloaked content
which helps to emphasize false negatives over false positives.
For studying blackhat ﬁngerprinting techniques, we rely on
a sample of unlabeled URLs tied to Google Search and Google
Ads targeting mobile users.2 Unlike our training corpus, these
carry no biases from previous de-cloaking systems. Due to
the targeted nature of cloaking as previously explored by
Wang et al. [32], we predicate our searches on high-value
keywords related to luxury products (e.g., gucci, prada, nike,
abercrombie) and ad selection on keywords related to weight
loss (e.g., garcinia, keatone, acai) and popular mobile software
applications (e.g., whatsapp, mobogenie). While it is possible
this targeting biases our evaluation, we argue that the content
that miscreants serve is independent from the underlying
technology used to ﬁngerprint crawlers. In total, we collect
135,577 URL samples, only a fraction of which we assume
will actually cloak.
B. Browser Conﬁguration
While our dataset may appear small, this is because we
crawl each URL with 11 distinct browser and network conﬁg-
urations in an attempt to trigger any cloaking logic, repeating
each crawl three times to rule out noise introduced by dynamic
content or network errors. In total, we perform over 7 million
crawls. We detail each crawl conﬁguration in Table VII. We
chose this set based on our domain knowledge of reversed
blackhat services which target various platforms, environment
variables, JavaScript, Flash, and cookie functionality. Also,
these conﬁgurations cover the three possible vantage points
of anti-cloaking deployments: a search engine advertising its
crawlers with their User Agents and IP addresses, a browser
farm deployed in the cloud, and a stealthy deployment using
mobile and residential networks. In section VI-D, we evaluate
how each of these vantage points contribute to our overall
cloaking detection performance.
In total, we provide three native platforms for fetching
content: Chrome on Desktop; Chrome on Android; and a
2We assume that Google deploys some defenses against cloaking. As such,
our dataset will capture only mature cloaking techniques that evade immediate
detection by Google crawlers.
748748
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:15:18 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 1: Cloaking detection pipeline. We crawl URLs from the Alexa Top Million, Google Search, and Google Ads ().
We dispatch requests to a farm of stratiﬁed browser proﬁles covering mobile, desktop, and simple crawlers and network
conﬁgurations that encompass residential, mobile, and cloud IP addresses to fetch content (). We then compare the similarity
of content returned by each crawl (), feeding the resulting metrics into a classiﬁer that detects divergent content indicative
of cloaking ().
TABLE VII: List of browser, network, and contextual conﬁgurations supported by our system.
Proﬁle Name
Googlebot Basic
Googlebot Desktop
Googlebot Android
Basic Cloud (no referer)
Basic Cloud
Chrome Desktop Cloud (no referer)
Chrome Desktop Cloud
Chrome Mobile Cloud (no referer)
Chrome Mobile
Desktop User
Mobile User
Platform
HTTP Request only
Chrome Desktop
Chrome Android
HTTP Request only
HTTP Request only
Chrome Desktop
Chrome Desktop
Chrome Android
Chrome Android
Chrome Desktop
Chrome Android
User-Agent
Googlebot
Googlebot
Googlebot
Chrome OSX
Chrome OSX
Chrome OSX
Chrome OSX
Chrome Android 4.4
Chrome Android 4.4
Chrome OSX
Chrome Android 4.4
Network
Google
Google
Google
Cloud
Cloud
Cloud
Cloud
Cloud
Cloud
Residential
Mobile
Referrer
Click






















basic HTTP fetch that supports cookies and 30X redirects, but
does not handle Flash, JavaScript, meta redirects, or embedded
content (e.g., iframes, images). We then conﬁgure the User-
Agent of each platform to mirror the latest version of Chrome
on Mac OSX; Chrome on a Nexus 5 Android device; or the
Google search bot. Finally, we wire the browser to proxy all
requests through a pre-deﬁned network (e.g., mobile, cloud)
described shortly. After a successful fetch, we save both the
HTML content of a page along with a screenshot. Due to the
possibility of URL visits introducing browser state, we tear
down our environment and clear all cookies between fetches.
As we show later in Section VII, in practice only a few of these
proﬁles are necessary to detect all cloaked content (though
not measure the precise cloaking logic). Security crawlers
can adopt this slimmed-down conﬁguration to increase overall
throughput.
Context Selection: We support spooﬁng three contextual
features when fetching URLs: the keywords a user searches
for; the path our crawler takes to reach a destination (e.g.,
the HTTP Referer); and user actions taken upon reaching a
page (e.g., clicking). To determine which keywords to spoof,
we ﬁrst fetch every non ad-based URL with a basic Googlebot
absent any HTTP Referer and then extract the (stuffed)
keywords on the page. Methodologically, we ﬁlter a page’s
HTML to include only visible, non-stopword text, after which
we select the top three most frequent words. Due to how
miscreants spin content to achieve a high search rank (as
discussed in Section III), these keywords are identical to those
miscreants expect from legitimate clients. For ad-based URLs,
the process is simpler: we rely on the keywords the advertiser
bids on for targeting (gathered from Google AdWords).
Since browsers have complex policies on when to set the
referrer (depending on whether the source and destination
URLs are over TLS, and the type of redirect [30], [33]), we
have opted not to spoof a crawler’s path by simply overwriting
the Referer ﬁeld, as the difference in referrer handling
might not trigger the uncloaking of a target website. Instead,
we ﬁrst load a Google search page and explicitly create a
new element on the page via JavaScript that directs to the
destination URL along with the aforementioned keywords
embedded in the URL’s parameters, after which we click the
element. We support this approach for all types of URLs.
Finally, to handle click walls that appear due to order of
operation-style cloaking, upon reaching a URL’s ﬁnal landing
page we wait a few seconds and then select the largest element
and simulate a user click event. If the click fails to cause the
browser to load a different URL, we ignore the element and
repeat the process until a loading event occurs or no elements
749749
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:15:18 UTC from IEEE Xplore.  Restrictions apply. 
are left to consider. In this fashion we create a realistic context
in which a user visits a search engine, enters an appropriate
search query, clicks on the cloaked page’s search, ad, or drive-
by URL; and ultimately interacts with the cloaked page’s
content. Note that we only click when using Chrome Desktop
or Mobile, and not when using the simple HTTP fetcher (per
Googlebot’s typical behavior).
Network Selection: We proxy network requests through a tap
that records all inbound and outbound trafﬁc. Additionally,
this tap provides a conﬁgurable exit IP belonging to either
Google’s network; a mobile gateway in the United States be-
longing to AT&T or Verizon; a datacenter network belonging
to Google Cloud; or a residential IP addresses belonging to the
researchers involved in this project. As some of our mobile and
residential IPs exist behind NATs with dynamically allocated
IP addresses, we establish a reverse tunnel to an entry point
with a statically allocated IP address that forwards requests
on to the exit node. Our diverse IP address pool allows us to
evade or trigger network-based cloaking on demand.
C. Features
Deviations in page content introduced by cloaking include
entirely unique HTML, distinct link locations, alternate cross-
origin content, or only a new button appearing on a page. We
compare the textual, visual, topical, and structural similarity
of content between all possible pairs of browser conﬁgurations
(e.g., Googlebot, Mobile User). Given we crawl every URL
three times per candidate proﬁle, we heuristically select the
fetch that generated the largest volume of HTTP logs to serve
as the representative sample. We make no assumptions on
how signiﬁcantly documents must differ to constitute blackhat
cloaking. Instead, we rely on classiﬁcation to learn an optimal
cut that differentiates divergent content due solely to blackhat
cloaking versus benign dynamism (e.g., breaking news, mobile
optimization).
1) Pairwise Similarity Features
Content Similarity: We detect cloaking that returns entirely
distinct content by estimating the similarity of each docu-
ment’s visible text and overall HTML. We begin by removing
all whitespace and non-alphanumeric content (e.g., punctua-
tion, formatting). We then tokenize the content using a sliding
window that generates successive ngrams of four characters.
Finally, we calculate a 64-bit simhash of all of the tokens
which converts high-dimensional text into a low-dimensional
representation amenable to quick comparison, originally pi-
oneered for de-duping documents in search indexes [5]. To
measure the similarity of two documents, we calculate the
Hamming distance between two simhashes which is propor-
tional to the number of document tokens that failed to match.
A high score indicates two documents differ signiﬁcantly. We
run this calculation twice, once for only paragraph and heading
text (that is, visible text in the page) and again for all HTML
content.
Screenshot Similarity: Our second similarity score estimates