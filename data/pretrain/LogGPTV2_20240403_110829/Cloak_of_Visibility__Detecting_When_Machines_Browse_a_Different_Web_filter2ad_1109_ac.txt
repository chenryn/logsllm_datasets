### 爬取单个短窗口内的域名
#### C. 重定向技术
我们观察到，伪装应用程序在成功指纹识别客户端后，会使用三种技术来提供分视图内容。第一种方法是通过元刷新（meta-refresh）、JavaScript 或 30X 重定向代码将客户端重定向。爬虫可能会被阻止在入口页面或被重定向到与合法客户端不同的域名。另一种方法是通过服务器端逻辑动态渲染内容，例如嵌入 iframe，而不改变浏览器地址栏中的 URL。最后一种技术是对爬虫返回 40X 或 50X 错误。我们在设计反伪装管道时考虑了这些技术。

#### D. 内置 SEO 服务
除了一个 (C6) 之外，所有分析的伪装软件都支持旨在进行 SEO 的自动内容生成，无论是原生支持还是通过第三方插件（如 SEnuke、XRumer）。这些内容旋转程序创建与主题相关的文档 [37]。最先进内置解决方案从软件操作员那里获取一组目标搜索词，然后程序会自动查询流行搜索引擎的相关内容，抓取排名靠前的链接，并将其合成一份从爬虫角度看似乎是相关的文档。此外，这些系统会自动检测并排除侵犯版权的内容和商标名称，以避免被搜索引擎排名算法惩罚或删除。作为文档构建的一部分，软件会在盗用内容中插入目标搜索词，以增加相关词的频率。我们将在第四节中利用这一观察结果来检测需要访客 HTTP Referer 包含特定关键词的上下文伪装。

### 检测伪装
我们利用对黑帽伪装技术的拆解，建立了一个可扩展的反伪装管道，用于检测两个或多个不同浏览器显示不一致内容的情况。我们的系统设计预览见图 1。首先，我们收集一系列多样的 URL 以扫描伪装行为（步骤 1）。然后，我们通过多种浏览器配置和网络视角来抓取每个 URL，以触发任何伪装逻辑（步骤 2）。最后，我们比较每次抓取的内容、结构和重定向图（步骤 3），并将这些特征输入分类器以检测是否存在黑帽伪装（步骤 4）。虽然之前的研究已经提出了一些在特定设置下解伪装的技术——特别是重定向伪装——但我们的目标是了解哪些反伪装技术可以泛化到各种网络伪装（包括移动和反向代理伪装），以及安全扫描器所需的最小功能集，以应对当前的伪装军备竞赛。具体的实现细节将在第五节中详细说明。

#### A. 候选 URL 选择
为了进行研究，我们从热门网站、搜索结果和针对移动设备的广告中收集了一组 URL。我们将数据集分为两部分：一部分用于训练基于先前研究中标记数据的分类器（表 V）；另一部分用于输入分类器，以分析实际环境中伪装的无偏视图（表 VI）。

我们的良性 URL 训练语料库由 Alexa Top Million 中随机抽取的页面组成，假设这些页面均未被伪装。对于标记的伪装域名，我们依赖于 Wang 等人 [31] 维护的一个假冒奢侈品牌店面列表，该列表记录了 Google 搜索机器人指纹识别的数据，时间范围为 2015 年 2 月至 2015 年 5 月。总共，我们依靠 94,946 个 URL 进行训练。我们注意到，标记数据集中非伪装内容占多数，这有助于强调假阴性而非假阳性。

**表 V: 用于训练和评估分类器的标记数据**
| 标记数据集 | 来源 | 数量 |
| --- | --- | --- |
| 正规网站 | Alexa | 75,079 |
| 伪装店面 | SEO 滥用 [31] | 19,867 |
| 总计 | - | 94,946 |

**表 VI: 用于分类以研究实际环境中伪装的未标记数据**
| 未标记数据集 | 来源 | 数量 |
| --- | --- | --- |
| 奢侈品店面 | Google 搜索 | 115,071 |
| 健康、软件广告 | Google 广告 | 20,506 |
| 总计 | - | 135,577 |

#### B. 浏览器配置
尽管我们的数据集看起来很小，但实际上我们使用 11 种不同的浏览器和网络配置来抓取每个 URL，以尝试触发任何伪装逻辑，并且每种配置重复三次，以排除由动态内容或网络错误引入的噪声。总共进行了超过 700 万次抓取。表 VII 详细列出了每种抓取配置。我们根据对逆向黑帽服务领域的知识选择了这些配置，这些服务针对各种平台、环境变量、JavaScript、Flash 和 cookie 功能。此外，这些配置涵盖了反伪装部署的三个可能视角：搜索引擎使用其爬虫的用户代理和 IP 地址进行广告宣传、云中部署的浏览器农场以及使用移动和住宅网络的隐蔽部署。在第六节 D 部分中，我们将评估这些视角如何影响整体的伪装检测性能。

总的来说，我们提供了三种原生平台来抓取内容：桌面版 Chrome；Android 版 Chrome；以及一个基本的 HTTP 抓取，支持 cookies 和 30X 重定向，但不处理 Flash、JavaScript、元重定向或嵌入内容（如 iframe、图像）。然后，我们将每个平台的用户代理配置为模仿最新版本的 Mac OSX Chrome、Nexus 5 Android 设备上的 Chrome 或 Google 搜索机器人。最后，我们将浏览器的所有请求通过预先定义的网络（如移动、云）进行代理。成功抓取后，我们保存页面的 HTML 内容和屏幕截图。由于 URL 访问可能会引入浏览器状态，我们在每次抓取之间都会清除所有 cookies 并重建环境。正如我们将在第七节中展示的那样，在实践中只需要少数几个配置就可以检测到所有伪装内容（尽管无法精确测量伪装逻辑）。安全爬虫可以采用这种精简配置来提高整体吞吐量。

**表 VII: 我们的系统支持的浏览器、网络和上下文配置**
| 配置名称 | 平台 | 用户代理 | 网络 | 引荐来源 |
| --- | --- | --- | --- | --- |
| Googlebot Basic | HTTP 请求 | Googlebot | Google | Click |
| Googlebot Desktop | Chrome 桌面 | Googlebot | Google | Click |
| Googlebot Android | Chrome 安卓 | Googlebot | Google | Click |
| Basic Cloud (无引荐) | HTTP 请求 | Chrome OSX | 云 | 无 |
| Basic Cloud | HTTP 请求 | Chrome OSX | 云 | Click |
| Chrome Desktop Cloud (无引荐) | Chrome 桌面 | Chrome OSX | 云 | 无 |
| Chrome Desktop Cloud | Chrome 桌面 | Chrome OSX | 云 | Click |
| Chrome Mobile Cloud (无引荐) | Chrome 安卓 | Chrome 安卓 4.4 | 云 | 无 |
| Chrome Mobile | Chrome 安卓 | Chrome 安卓 4.4 | 云 | Click |
| 桌面用户 | Chrome 桌面 | Chrome OSX | 居民 | Click |
| 移动用户 | Chrome 安卓 | Chrome 安卓 4.4 | 移动 | Click |

**上下文选择**：在抓取 URL 时，我们支持伪造三个上下文特征：用户搜索的关键词；爬虫到达目的地的路径（如 HTTP Referer）；以及用户到达页面后的操作（如点击）。为了确定要伪造的关键词，我们首先使用没有 HTTP Referer 的基本 Googlebot 抓取每个非广告 URL，然后提取页面上的（填充的）关键词。方法上，我们过滤页面的 HTML 以仅包含可见的非停用词文本，然后选择出现频率最高的三个词。由于恶意分子为了获得高搜索排名而旋转内容（如第三节所述），这些关键词与恶意分子期望从合法客户端得到的关键词相同。对于广告 URL，过程更简单：我们依赖广告商在 Google AdWords 上竞标的目标关键词。

由于浏览器在何时设置引荐来源方面有复杂的策略（取决于源 URL 和目标 URL 是否使用 TLS 以及重定向类型 [30], [33]），我们选择不通过简单地覆盖 Referer 字段来伪造爬虫路径，因为引荐来源处理的不同可能不会触发目标网站的解伪装。相反，我们首先加载 Google 搜索页面，并通过 JavaScript 显式创建一个新的元素，该元素指向目标 URL 并在 URL 参数中嵌入上述关键词，然后点击该元素。我们支持这种方法用于所有类型的 URL。最后，为了处理由于顺序操作风格伪装而出现的点击墙，当到达 URL 的最终着陆页时，我们等待几秒钟，然后选择最大的元素并模拟用户点击事件。如果点击未能导致浏览器加载不同的 URL，我们会忽略该元素并重复此过程，直到发生加载事件或没有元素可考虑为止。通过这种方式，我们创建了一个逼真的上下文，即用户访问搜索引擎，输入适当的搜索查询，点击伪装页面的搜索、广告或路过 URL，并最终与伪装页面的内容交互。请注意，我们仅在使用 Chrome 桌面或移动版时才点击，而不是使用简单的 HTTP 抓取器（按照 Googlebot 的典型行为）。

**网络选择**：我们将网络请求通过一个记录所有进出流量的 tap 代理。此外，这个 tap 提供一个可配置的出口 IP，属于 Google 的网络、美国 AT&T 或 Verizon 的移动网关、Google Cloud 的数据中心网络或参与此项目的研究人员的住宅 IP 地址。由于我们的一些移动和住宅 IP 存在于具有动态分配 IP 地址的 NAT 后面，我们建立了到具有静态分配 IP 地址的入口点的反向隧道，该入口点将请求转发到出口节点。我们多样化的 IP 地址池使我们能够按需规避或触发基于网络的伪装。

#### C. 特征
伪装引入的页面内容偏差包括完全不同的 HTML、不同的链接位置、替代的跨域内容或页面上新按钮的出现。我们比较所有可能的浏览器配置对之间的文本、视觉、主题和结构相似性（例如，Googlebot 和移动用户）。鉴于我们对每个候选配置抓取每个 URL 三次，我们启发式地选择生成最多 HTTP 日志的抓取作为代表性样本。我们不对文档必须有多大的差异才能构成黑帽伪装做出任何假设。相反，我们依赖分类来学习区分因黑帽伪装而导致的发散内容与良性动态（例如，突发新闻、移动优化）的最佳阈值。

1. **成对相似性特征**

   - **内容相似性**：我们通过估计每个文档的可见文本和整体 HTML 的相似性来检测返回完全不同内容的伪装。我们首先去除所有空白和非字母数字内容（例如，标点符号、格式化）。然后，我们使用滑动窗口生成连续的四字符 n-gram。最后，我们计算所有 token 的 64 位 simhash，将高维文本转换为低维表示，以便快速比较，最初用于搜索索引中的去重 [5]。为了衡量两个文档的相似性，我们计算两个 simhash 之间的汉明距离，这与未匹配的文档 token 数量成比例。高得分表示两个文档显著不同。我们运行两次计算，一次仅针对段落和标题文本（即页面中的可见文本），另一次针对所有 HTML 内容。

   - **屏幕截图相似性**：我们的第二个相似性评分估计...