# []{#assembly_enabling-multipathing-on-nvme-devices_managing-storage-devices.html#proc_enabling-dm-multipath-on-nvme-devices_assembly_enabling-multipathing-on-nvme-devices}在 NVMe 设备中启用 DM 多路径 {.title}
:::
这个过程使用 DM 多路径解决方案在连接的 NVMe 设备中启用多路径。
::: itemizedlist
**先决条件**
-   NVMe 设备连接到您的系统。
    有关通过光纤传输连接 NVMe 的更多信息，请参阅 [NVMe over fabric
    设备概述](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_storage_devices/overview-of-nvme-over-fabric-devicesmanaging-storage-devices){.link}。
:::
::: orderedlist
**流程**
1.  检查原生 NVMe 多路径是否已禁用：
    ``` screen
    # cat /sys/module/nvme_core/parameters/multipath
    ```
    命令显示以下内容之一：
    ::: variablelist
    [`N`{.literal}]{.term}
    :   禁用原生 NVMe 多路径。
    [`Y`{.literal}]{.term}
    :   启用了原生 NVMe 多路径。
    :::
2.  如果启用了原生 NVMe 多路径，请禁用它：
    ::: orderedlist
    1.  从内核命令行中删除 `nvme_core.multipath=Y`{.literal} 选项：
        ``` screen
        # grubby --update-kernel=ALL --remove-args="nvme_core.multipath=Y"
        ```
    2.  在 64 位 IBM Z 构架中，更新引导菜单：
        ``` screen
        # zipl
        ```
    3.  如果存在 `options nvme_core multipath=Y`{.literal} 文件，请从
        `/etc/modprobe.d/nvme_core.conf`{.literal} 文件中删除它。
    4.  重启系统。
    :::
3.  确保启用了 DM 多路径：
    ``` screen
    # systemctl enable --now multipathd.service
    ```
4.  在所有可用路径上分发 I/O。在 `/etc/multipath.conf`{.literal}
    文件中添加以下内容：
    ``` screen
    device {
      vendor "NVME"
      product ".*"
      path_grouping_policy    group_by_prio
    }
    ```
    ::: {.note style="margin-left: 0.5in; margin-right: 0.5in;"}
    ### 注意 {.title}
    当 DM 多路径管理 NVMe
    设备时，`/sys/class/nvme-subsystem/nvme-subsys0/iopolicy`{.literal}
    配置文件对 I/O 分发没有影响。
    :::
5.  重新载入 `multipathd`{.literal} 服务以应用配置更改：
    ``` screen
    # multipath -r
    ```
6.  备份 `initramfs`{.literal} 文件系统：
    ``` screen
    # cp /boot/initramfs-$(uname -r).img \
         /boot/initramfs-$(uname -r).bak.$(date +%m-%d-%H%M%S).img
    ```
7.  重建 `initramfs`{.literal} 文件系统：
    ``` screen
    # dracut --force --verbose
    ```
:::
::: orderedlist
**验证**
1.  检查您的系统是否识别 NVMe 设备：
    ``` screen
    # nvme list
    Node             SN                   Model                                    Namespace Usage                      Format           FW Rev
    ---------------- -------------------- ---------------------------------------- --------- -------------------------- ---------------- --------
    /dev/nvme0n1     a34c4f3a0d6f5cec     Linux                                    1         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    /dev/nvme0n2     a34c4f3a0d6f5cec     Linux                                    2         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    /dev/nvme1n1     a34c4f3a0d6f5cec     Linux                                    1         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    /dev/nvme1n2     a34c4f3a0d6f5cec     Linux                                    2         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    /dev/nvme2n1     a34c4f3a0d6f5cec     Linux                                    1         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    /dev/nvme2n2     a34c4f3a0d6f5cec     Linux                                    2         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    /dev/nvme3n1     a34c4f3a0d6f5cec     Linux                                    1         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    /dev/nvme3n2     a34c4f3a0d6f5cec     Linux                                    2         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    ```
2.  列出所有连接的 NVMe 子系统。检查该命令是否将其报告为
    `nvme0n1`{.literal} 到 [ *`nvme3n2`{.literal}，例如*]{.emphasis}
    `nvme0c0n1`{.literal} 到 `nvme0c3n1`{.literal} ：
    ``` screen
    # nvme list-subsys
    nvme-subsys0 - NQN=testnqn
    \
     +- nvme0 fc traddr=nn-0x20000090fadd5979:pn-0x10000090fadd5979 host_traddr=nn-0x20000090fac7e1dd:pn-0x10000090fac7e1dd live
     +- nvme1 fc traddr=nn-0x20000090fadd597a:pn-0x10000090fadd597a host_traddr=nn-0x20000090fac7e1dd:pn-0x10000090fac7e1dd live
     +- nvme2 fc traddr=nn-0x20000090fadd5979:pn-0x10000090fadd5979 host_traddr=nn-0x20000090fac7e1de:pn-0x10000090fac7e1de live
     +- nvme3 fc traddr=nn-0x20000090fadd597a:pn-0x10000090fadd597a host_traddr=nn-0x20000090fac7e1de:pn-0x10000090fac7e1de live
    ```
    ``` screen
    # multipath -ll
    mpathae (uuid.8ef20f70-f7d3-4f67-8d84-1bb16b2bfe03) dm-36 NVME,Linux
    size=233G features='1 queue_if_no_path' hwhandler='0' wp=rw
    `-+- policy='service-time 0' prio=50 status=active
      |- 0:1:1:1  nvme0n1 259:0   active ready running
      |- 1:2:1:1  nvme1n1 259:2   active ready running
      |- 2:3:1:1  nvme2n1 259:4   active ready running
      `- 3:4:1:1  nvme3n1 259:6   active ready running
    mpathaf (uuid.44c782b4-4e72-4d9e-bc39-c7be0a409f22) dm-39 NVME,Linux
    size=233G features='1 queue_if_no_path' hwhandler='0' wp=rw
    `-+- policy='service-time 0' prio=50 status=active
      |- 0:1:2:2  nvme0n2 259:1   active ready running
      |- 1:2:2:2  nvme1n2 259:3   active ready running
      |- 2:3:2:2  nvme2n2 259:5   active ready running
      `- 3:4:2:2  nvme3n2 259:7   active ready running
    ```
:::
::: itemizedlist
**其它资源**
-   [有关编辑内核选项的更多信息，请参阅配置内核命令行参数](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_monitoring_and_updating_the_kernel/configuring-kernel-command-line-parameters_managing-monitoring-and-updating-the-kernel){.link}。
-   有关配置 DM [多路径的详情，请参考设置 DM
    多路径](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_device_mapper_multipath/assembly_setting-up-dm-multipath-configuring-device-mapper-multipath){.link}。
:::
:::
:::
[]{#setting-the-disk-scheduler_managing-storage-devices.html}
::: chapter
::: titlepage
# []{#setting-the-disk-scheduler_managing-storage-devices.html#setting-the-disk-scheduler_managing-storage-devices}第 15 章 设置磁盘调度程序 {.title}
:::
磁盘调度程序负责订购提交至存储设备的 I/O 请求。
您可以通过几种不同方式配置调度程序：
::: itemizedlist
-   使用 [**Tuned**]{.strong} 设置调度程序，如使用 [Tuned
    设置磁盘调度程序中所述](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/setting-the-disk-scheduler_monitoring-and-managing-system-status-and-performance#setting-the-disk-scheduler-using-tuned_setting-the-disk-scheduler){.link}
-   使用 `udev`{.literal} 设置调度程序，如使用 [udev
    规则设置磁盘调度程序中所述](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/setting-the-disk-scheduler_monitoring-and-managing-system-status-and-performance#setting-the-disk-scheduler-using-udev-rules_setting-the-disk-scheduler){.link}
-   [临时更改正在运行的系统上的调度程序，如临时设置特定磁盘的调度程序中所述](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/setting-the-disk-scheduler_monitoring-and-managing-system-status-and-performance#temporarily-setting-a-scheduler-for-a-specific-disk_setting-the-disk-scheduler){.link}
:::
::: {.note style="margin-left: 0.5in; margin-right: 0.5in;"}
### 注意 {.title}
在 Red Hat Enterprise Linux 8
中，块设备只支持多队列调度。这可让块层性能针对使用快速固态驱动器（SSD）和多核系统进行正常扩展。
Red Hat Enterprise Linux 7
及更早版本中提供的传统的单队列调度程序已被删除。
:::
::: section
::: titlepage
# []{#setting-the-disk-scheduler_managing-storage-devices.html#available-disk-schedulers_setting-the-disk-scheduler}可用磁盘调度程序 {.title}
:::
Red Hat Enterprise Linux 8 支持以下多队列磁盘调度程序：
::: variablelist
[`none`{.literal}]{.term}
:   实施一向先出(FIFO)调度算法。它通过简单的最后一个缓存合并通用块层的请求。
[`mq-deadline`{.literal}]{.term}
:   尝试从请求到达调度程序时起为请求提供保证的延迟。
    `mq-deadline`{.literal} 调度程序将排队的 I/O
    请求排序为读或写批处理，然后调度它们以增加逻辑块寻址(LBA)顺序来执行。默认情况下，读取批处理优先于写入批处理，因为应用更有可能阻止读取
    I/O 操作。`mq-deadline`{.literal}
    处理批处理后，它会检查处理器时间不足的时间，并根据情况调度下一个读取或写入批处理。
    此调度程序适用于大多数用例，特别是写操作大部分异步的情况。
[`bfq`{.literal}]{.term}
:   以桌面系统和互动任务为目标。
    `bfq`{.literal}
    调度程序确保单个应用程序永远不会使用所有带宽。实际上，存储设备总是像它们处于闲置时一样进行响应。在默认配置中，`bfq`{.literal}
    着重提供最低延迟，而不是实现最大吞吐量。
    `bfq`{.literal} 基于 `cfq`{.literal}
    代码。[*它不会为每个进程分配一个固定时间片段的磁盘，而是以扇区数为进程分配一个预算*]{.emphasis}。
    此调度程序适合用于复制大型文件，在这种情况下，系统也不会变得无响应。
[`kyber`{.literal}]{.term}
:   调度程序通过计算提交至块 I/O 层的每个 I/O
    请求的延迟来调整自身以达到延迟目标。如果出现缓存缺少和同步写入请求，您可以为读取配置目标延迟。
    这个调度程序适合快速设备，如 NVMe、SSD 或其他低延迟设备。
:::
:::
::: section
::: titlepage
# []{#setting-the-disk-scheduler_managing-storage-devices.html#different-disk-schedulers-for-different-use-cases_setting-the-disk-scheduler}不同用例的不同磁盘调度程序 {.title}
:::
根据您的系统执行的任务，在分析和调整任务之前，建议使用以下磁盘调度程序作为基准：
::: table
[]{#setting-the-disk-scheduler_managing-storage-devices.html#idm140531435562128}
**表 15.1. 适用于不同用例的磁盘调度程序**
::: table-contents
  使用案例                                   磁盘调度程序
  ------------------------------------------ ------------------------------------------------------------------------------------------------------
  传统的使用 SCSI 接口的 HDD                 使用 `mq-deadline`{.literal} 或 `bfq`{.literal}。
  高性能 SSD 或具有快速存储的 CPU 绑定系统   使用 `none`{.literal}，特别是在运行企业级应用程序时。另外，还可使用 `kyber`{.literal}。
  桌面或互动任务                             使用 `bfq`{.literal}。
  虚拟客户端                                 使用 `mq-deadline`{.literal}。对于支持多队列的主机总线适配器(HBA)驱动程序，请使用 `none`{.literal}。
:::
:::
:::
::: section
::: titlepage
# []{#setting-the-disk-scheduler_managing-storage-devices.html#the-default-disk-scheduler_setting-the-disk-scheduler}默认磁盘调度程序 {.title}
:::
块设备使用默认磁盘调度程序，除非您指定了另一个调度程序。
::: {.note style="margin-left: 0.5in; margin-right: 0.5in;"}
### 注意 {.title}
对于 `non-volatile Memory Express (NVMe)`{.literal}
块设备，默认调度程序是 `none`{.literal}，红帽建议不要更改它。
:::
内核根据设备类型选择默认磁盘调度程序。自动选择的调度程序通常是最佳设置。如果您需要不同的调度程序，红帽建议使用
`udev`{.literal} 规则或 [**Tuned**]{.strong}
应用程序进行配置。匹配所选设备并只为那些设备切换调度程序。
:::
::: section
::: titlepage
# []{#setting-the-disk-scheduler_managing-storage-devices.html#determining-the-active-disk-scheduler_setting-the-disk-scheduler}确定活跃磁盘调度程序 {.title}
:::
此流程决定了哪个磁盘调度程序目前在给定块设备中活跃。
::: itemizedlist
**流程**
-   阅读 `/sys/block/device/queue/scheduler`{.literal} 文件的内容：
    ``` screen
    # cat /sys/block/device/queue/scheduler
    [mq-deadline] kyber bfq none
    ```
    在文件名中，使用块设备名称替换 [*device*]{.emphasis}，例如
    `sdc`{.literal}。
    活跃调度程序列在方括号（`[ ]`{.literal}）中。
:::
:::
::: section
::: titlepage
# []{#setting-the-disk-scheduler_managing-storage-devices.html#setting-the-disk-scheduler-using-tuned_setting-the-disk-scheduler}使用 Tuned 设置磁盘调度程序 {.title}
:::
此流程创建并启用 [**Tuned**]{.strong}
配置集，该配置集为所选块设备设置给定磁盘调度程序。这个设置会在系统重启后保留。
在以下命令和配置中替换：
::: itemizedlist
-   [*带有块设备名称的设备*]{.emphasis}，例如： `sdf`{.literal}
-   带有您要为该设备设置的磁盘调度程序的
    [*selected-scheduler*]{.emphasis}，例如 `bfq`{.literal}
:::
::: itemizedlist
**先决条件**
-   `tuned`{.literal} 服务已安装并启用。[详情请参阅安装和启用
    Tuned](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/getting-started-with-tuned_monitoring-and-managing-system-status-and-performance#installing-and-enabling-tuned_getting-started-with-tuned){.link}。
:::
::: orderedlist
**流程**
1.  可选：选择一个您配置集将要基于的现有 [**Tuned**]{.strong}
    配置集。有关可用配置集列表，请参阅 [RHEL 提供的 Tuned
    配置集](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/getting-started-with-tuned_monitoring-and-managing-system-status-and-performance#tuned-profiles-distributed-with-rhel_getting-started-with-tuned){.link}。
    要查看哪个配置集当前处于活跃状态，请使用：
    ``` screen
    $ tuned-adm active
    ```
2.  创建一个新目录来保存您的 [**Tuned**]{.strong} 配置集：
    ``` screen
    # mkdir /etc/tuned/my-profile
    ```
3.  查找所选块设备系统唯一标识符：
    ``` screen
    $ udevadm info --query=property --name=/dev/device | grep -E '(WWN|SERIAL)'
    ID_WWN=0x5002538d00000000_
    ID_SERIAL=Generic-_SD_MMC_20120501030900000-0:0
    ID_SERIAL_SHORT=20120501030900000
    ```
    ::: {.note style="margin-left: 0.5in; margin-right: 0.5in;"}
    ### 注意 {.title}
    本例中的
    命令将返回标识为全局名称(WWN)或与指定块设备关联的序列号的所有值。虽然最好使用
    WWN，但 WWN
    [*并不总是可用于给定设备，示例命令返回的任何值都可用作设备系统唯一
    ID*]{.emphasis}。
    :::
4.  创建 `/etc/tuned/my-profile/tuned.conf`{.literal}
    配置文件。在该文件中设置以下选项：
    ::: orderedlist
    1.  可选：包含现有配置集：
        ``` screen