FM4/FM3  one  can  have  a  relative  measure  of  how 
sensitive  is  the  file  system  when  a  crash  occurs.  The 
results  presented  in  Figure  4  shows  that  when  using 
FAT32  in  general,  Windows  Server  2003  is  more 
sensitive than Windows XP in a majority of the cases.  
3.4  Minidump Diagnosis Capabilities 
The analysis of the minidump files produced during 
a  system  crash  allows  us  to  determine  how  well  they 
identify a driver as the culprit of the failure. These files 
are  fundamental  tools  for  the  Windows  development 
teams because they help to diagnose system problems, 
and  eventually  to  correct  them.  We  have  used  the 
Microsoft’s Kernel Debugger (KD) [17] to perform the 
analysis of these files, together with a tool, DevDump, 
that automates most of this task. DevDump controls the 
debugger,  passes  the  minidumps  under  investigation, 
and selects a log where results should be stored. After 
processing  all  files,  DevDump  generates  various 
statistics about the detection capabilities of minidumps.  
In  the  experiments,  all  Windows  versions  correctly 
spotted  the  faulty  DD  in  the  majority  of  times  (see 
Figure  5  and  compare  it  with  Table  5).  The  correct 
identification of the source of crash (M1) seams to be 
independent  of  the file system used. Only in very  few 
cases  there  was  a  difference  between  the  two  file 
systems,  such  as  for  the  7-InitEvt  function  where 
Server  2003  FAT32  identified  a  different  source  of 
crash from Server 2003 NTFS. 
In  general,  the  results  show  that  Windows  XP  is 
more accurate than the others OS (see 15-RelLock and 
20-memset).  Still  there  were  cases  where  other  kernel 
modules  were  incorrectly  identified  (functions  1-
InitStr,  14-AcqLock  and  15-RelLock),  as  displayed  in 
Figure 6. 
XP NTFS
XP FAT32
2003 FAT32
2003 NTFS
Vista NTFS
%)
(
s
s
e
n
t
s
u
b
o
R
e
v
i
t
a
l
e
R
100
90
80
70
60
50
40
30
20
10
0
1
2
3
4
5
6
7
9 10 11 12 13 14 15 16 17 18 19 20
8
Function Identification
Figure 3: Relative robustness (FM1/#DD). 
XP_NTFS
XP FAT32
2003 FAT32
2003 NTFS
Vista NTFS
%)
(
s
s
e
n
e
v
i
t
i
s
n
e
s
m 
e
t
s
y
s
e
F
l
i
100
90
80
70
60
50
40
30
20
10
0
1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18 19 20
Function Identification
Figure 4: FSystem sensitiveness (FM4/FM3). 
M1-XP NTFS
M1-XP FAT32
M1-2003 FAT32
M1-2003 NTFS
M1-Vista
k
O
n 
o
i
t
a
c
i
f
i
t
n
e
d
I
1
M
r
o
r
r
E
n
o
i
t
a
c
i
f
i
t
n
e
d
I
2
M
d 
e
i
f
i
nt
e
d
ni
U
3
M
60
50
40
30
20
10
0
1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18 19 20
Function Identification
Figure 5: Source identification OK (M1). 
M2-XP NTFS M2-XP FAT32 M2-2003 FAT32 M2-2003 NTFS M2-Vista
4
3
2
1
0
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15 16 17 18 19 20
Function Identification
Figure 6: Source identification error (M2). 
M3-XP NTFS M3-XP FAT32 M3-2003 FAT32 M3-2003 NTFS M3-Vista
14
12
10
8
6
4
2
0
1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18 19 20
Function Identification
Figure 7: Source of crash unidentified (M3). 
These  errors  are  particularly  unpleasant  because 
they can lead to waist of time while looking for bugs in 
the wrong place, and they can reduce the confidence on 
the information provided by minidumps.  In some other 
cases,  Windows  was  unable  to  discover  the  cause  of 
failure.  This  happened  in  Vista  more  frequently  than 
the  other  OS  configurations,  for  instance  in  functions 
15-RelLock  and  12-SetEvt  (see  Figure  7).  In  the  last 
function,  Vista  was  the  only  system  that  could  not 
diagnose  the  cause  of  failure.  Only  Windows  Server 
2003  detected  memory  corruption  situations  (in 
functions  14-AcqLock  and  15-RelLock).  Windows 
Server  2003  (FAT32  and  NTFS)  located  memory 
corruptions when faults  were  injected in functions 14-
AcqLock and 15-RelLock. 
4  Related Work 
Robustness testing has been successfully applied to 
several  software  components  to  characterize  their 
behavior  when  facing  exceptional  inputs  or  stressful 
environmental  conditions.  One  of  the  main  targets  of 
these  studies  has  been  general  propose  OS,  with 
erroneous  inputs  being  injected  at  the  application 
interface.  Fifteen  OS  versions  that  implement  the 
POSIX  standard,  including  AIX,  Linux,  SunOS  and 
HPUX,  were  assessed  using  the  Ballista  tool  [12]. 
Shelton et al. made a comparative study of six variants 
of Windows, from 95 to 2000, by injecting faults at the 
Win  32  interface  [20].  Several  command  line  utilities 
of  Windows  NT  were  evaluated  by  Ghosh  et  al.  [7]. 
Real  time  microkernels,  such  as  Chorus  and  LynxOS, 
have  also  been  the  target  of  these  studies  using  the 
MAFALDA tool [2]. Application level software can be 
tested  using  robustness  techniques  by,  for  instance, 
generating  exceptions  and  returning  bad  values  at  the 
OS  interface  [8].  Middleware  support  systems  like 
CORBA  have  been  examined  at 
the  client-side 
interface of an ORB [19] and internally at the level of 
the  Naming  and  Event  services  [14].  Dependability 
benchmarking  has  resorted  to  robustness  testing  in 
order  to  evaluate  systems  [18,  23,  24].  For  example, 
Kalakech  et  al.  proposed  an  OS  benchmark  which 
provided a comprehensive set of measures, and applied 
it to the Windows 2000 [11]. 
To our knowledge, only a few works have assessed 
the robustness of systems at the level of device drivers. 
Durães  and  Madeira  described  a  way  to  emulate 
software  faults  by  mutating  the  binary  code  of  device 
drivers [5]. Basically, the driver executable is scanned 
for 
instruction  patterns,  and 
mutations  are  performed  on  those  patterns  to  emulate 
high-level faults. These ideas were experimented for 4 
low-level 
specific 
to 
the  kernel’s  DPI 
types  of  patterns,  on  2  drivers  of  the  Windows  NT4, 
2000  and  XP.  Albinet  et  al.  conducted  a  set  of 
experiments  to  evaluate  the  robustness  of  Linux 
systems  in  face  of faulty drivers [1]. They  intercepted 
the  driver  calls 
(Drivers 
Programming  Interface)  functions,  and  changed  the 
parameters on the fly with a few pre-set number faulty 
values. Then, the behavior of the system was observed. 
Johansson and Suri employed a similar methodology to 
evaluate  a  Windows  CE  .Net  [10].  In  their  work, 
however,  they  focus  on  error  propagation  profiling 
measures, as facilitator for the selection of places to put 
wrappers.   
Our  research  is  complementary  to  these  previous 
works, not only because we targeted different OS. The 
followed  methodology  has  its  roots  in  the  original 
Ballista  tool  [13],  where  several  test  drivers  are 
generated,  containing  DDK 
function  calls  with 
erroneous  arguments.  The  argument  values  were 
selected  specifically  for  each  function,  and  they 
emulate  seven  classes  of  typical  programming  errors. 
Our  study  has  looked  in  a  comparative  basis  at  such 
aspects  like  error  containment,  influence  of  the  file 
system 
the  diagnosis  capabilities  of 
minidump files. 
type,  and 
5  Conclusions 
The paper describes a robustness testing experiment 
that evaluates Windows XP, Windows Server 2003 and 
the  future  Windows release Vista. The main objective 
of  this  study  was  to  determine  how  well  Windows 
protects 
that  provide 
erroneous input to the DDK routines. Seven classes of 
typical programming bugs were simulated.  
faulty  drivers 
itself 
from 
The analysis of the results shows that most interface 
functions are unable to completely check their inputs – 
from  the  20  selected  functions,  only  2  were  100% 
effective in their defense. We observed a small number 
of hangs and a reasonable number of crashes. The main 
reason  for  the  crashes  was  invalid  or  NULL  pointer 
values. Corruption of files was only observed with the 
FAT32  file  system.  The  analysis  of  the  return  values 
demonstrates  that  in  some  cases  Windows  completes 
without  generating  an  error  for  function  calls  with 
incorrect  parameters;  in  particular,  Windows  Server 
2003  seams  to  be  the  most  permissible  one.  This 
behavior  suggests  a  deficient  error  containment 
capability  of  the  OS.  In  most  cases,  the  examined 
minidump  files  provided  valuable  information  about 
the sources of the crashes, something extremely useful 
for  the  development  teams.  However,  Windows  Vista 
seems to have more troubles in this identification than 
the other OS.   
The experiments made with Windows Vista reveled 
that  it  behaves  in  a  similar  way  to  Windows  XP  and 
Server  2003.  This  probably  means  that  Microsoft 
intents to continue to use the current DD architecture in 
its  future  OS,  which  is  reasonably  worrisome  since 
Vista will be most likely the most used OS in the years 
to come.   
6  References 
[1] A. Albinet,  J. Arlat, and J.-C. Fabre, “Characterization of 
the Impact of Faulty Drivers on the Robustness of the Linux 
Kernel”,  Proceedings  of  the  International  Conference  on 
Dependable Systems and Networks, June 2004 
[2]  J.  Arlat,  J.-C.  Fabre,  M.  Rodríguez  and  F.  Salles, 
“Dependability of COTS Microkernel-Based Systems”, IEEE 
Transactions  on  Computers,  vol.  51,  no.  2,  2002,  pp.  138-
163. 
[3]  A.  Chou,  J.  Yang,  B.  Chelf,  S.  Hallem,  and  D.  Engler, 
“On  u-kernel  construction”,  Proceedings  of  the  Symposium 
on Operating Systems Principles, December 1995, pp. 237–
250. 
[4]  A.  Chou,  J.  Yang,  B.  Chelf,  S.  Hallem,  and  D.  Engler, 
study  of  operating 
“An  empirical 
system  errors”, 
Proceedings  of 
the  Symposium  on  Operating  Systems 
Principles, October 2001, pp. 73–88. 
[5] J. Durães and H. Madeira, “Characterization of Operating 
Systems Behavior in the Presence of Faulty Drivers through 
Software  Fault  Emulation”,  Proceedings  of  the  Pacific  Rim 
International  Symposium.  On  Dependable  Computing, 
December 2002, pp. 201-209. 
[6] B. Ford, G. Back, G. Benson, J. Lepreau, A. Lin, and O. 
Shivers,  “The  Flux  OSKit:  a  substrate for OS language and 
resource  management”,  Proceedings  of  the  Symposium  on 
Operating Systems Principles, October 1997, pp. 38–51. 
[7]  A.  Ghosh,  M.  Schmid,  and  V.  Shah,  “Testing  the 
robustness  of  Windows  NT  software”,  Proceedings  of  the 
Ninth  International  Symposium  on  Software  Reliability 
Engineering, November 1998, pp. 231-235. 
[8]  A.  K. Ghosh, M.  Schmid,  “An  Approach  to  Testing 
COTS  Software  for  Robustness 
to  Operating  System 
Exceptions  and  Errors”,  Proceedings  10th  International 
Symposium  on  Software  Reliability  Engineering,  November 
1999, pp. 166-174. 
[9]  R.  Gruber,  and  M.  L.  Jiang,  “Robustness  Testing  and 
Hardening  of CORBA  ORB Implementations”, Proceedings 
of the International Conference on Dependable Systems and 
Networks, June 2001, pp. 141-150. 
[10] A. Johansson, and N. Suri, “Error Propagation Profiling 
of  Operating  Systems”,  Proceedings  of  the  International 
Conference  on  Dependable  Systems  and  Networks,  June 
2005. 
[11]  A.  Kalakech,  T.  Jarboui,  J.  Arlat, Y. Crouzet,   and K. 
Kanoun,  “Benchmarking  Operating  System  dependability: 
Windows 2000  as a  Case Study”,  Proceedings Pacific Rim 
December 
Overview”, 
International Symposium on Dependable Computing, March 
2004, pp. 261- 270. 
[12]  P.  Koopman,  J.  DeVale,  “The  Exception  Handling 
Effectiveness  of  POSIX  Operating  Systems”, 
IEEE 
Transactions  on  Software  Engineering,  vol.  26,  no.  9, 
September 2000, pp. 837-848. 
[13] N. Kropp, P. Koopman, and D. Siewiorek, “Automated 
Robustness Testing of Off-the-Shelf Software Components”, 
Proceedings  of  the  International  Symposium  on  Fault-
Tolerant Computing, June 1998. 
[14] E. Marsden, J.-C. Fabre and J. Arlat, “Dependability of 
CORBA  Systems:  Service  Characterization  by  Fault 
Injection”, Proceedings of the 21st International Symposium 
on Reliable Distributed Systems, June 2002, pp. 276-285. 
[15] Microsoft Corporation, “Microsoft Portable Executable 
and  Common  Object  File  Format  Specification”,  February 
2005. 
[16]  Microsoft  Corporation,  “Introducing  Static  Driver 
Verifier”, May 2006. 
[17] Microsoft Corporation, “Debugging Tools for Windows 
– 
2006 
http://www.microsoft.com/whdc/devtools/debugging/default.
mspx 
[18]  A.  Mukherjee  and  D.  P.  Siewiorek,  “Measuring 
Software  Dependability  by  Robustness  Benchmarking”, 
IEEE Transactions of  Software  Engineering, vol.  23, no. 6, 
1997, pp. 366-378. 
[19]  J.  Pan,  P.  J.  Koopman, D. P. Siewiorek,  Y. Huang,  R. 
Gruber and M. L. Jiang, “Robustness Testing and Hardening 
of  CORBA  ORB  Implementations”,  Proceedings  of  the 
Internatinal  Conference  on  Dependable  Systems  and 
Networks, June 2001, pp. 141-150. 
[20]  C.  Shelton,  P.  Koopman  and  K.  D.  Vale,  “Robustness 
Testing  of  the  Microsoft  Win32  API”,  Proceedings  of  the 
International  Conference  on  Dependable  Systems  and 
Networks, June 2000, pp. 261-270. 
[21]  D.  Simpson,  “Windows  XP  Embedded  with  Service 
Pack  1  Reliability”,  Tech.  rep.,  Microsoft  Corporation, 
January 2003. 
[22]  M.  Swift,  B.  Bershad,  and  H.  Levy,  “Improving  the 
reliability  of commodity operating systems”, Proceedings of 
the  Symposium  on  Operating  Systems  Principles,  October 
2003, pp. 207–222. 
[23]  T.  K.  Tsai,  R.  K.  Iyer,  and  D.  Jewitt,  “An  Approach 
Towards  Benchmarking  of  Fault-Tolerant  Commercial 
Systems”, Proceedings of the 26th International Symposium 
on Fault-Tolerant Computing, June 1996, pp. 314-323. 
[24]  M.  Vieira  and  H.  Madeira,  “A  Dependability 
Benchmark 
for  OLTP  Application  Environments”, 
Proceedings  of  the  29th  International  Conference  on  Very 
Large Data Bases, 2003, pp. 742-753. 
[25]  M.  Young,  M.  Accetta,  R.  Baron,  W.  Bolosky,  D. 
Golub,  R.  Rashid,  and  A.  Tevanian,  “Mach:  A  new  kernel 
foundation  for  UNIX  development”,  Proceedings  of  the 
Summer USENIX Conference, June 1986, pp. 93–113.