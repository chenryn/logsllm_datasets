### FM4/FM3: File System Sensitivity to Crashes

FM4/FM3 provides a relative measure of the file system's sensitivity when a crash occurs. The results in Figure 4 indicate that, generally, Windows Server 2203 is more sensitive than Windows XP when using FAT32 in most cases.

### 3.4 Minidump Diagnosis Capabilities

The analysis of minidump files generated during a system crash helps determine how effectively they identify the driver responsible for the failure. These files are essential tools for Windows development teams, as they aid in diagnosing and correcting system issues. We used Microsoft’s Kernel Debugger (KD) [17] and a tool called DevDump to analyze these files. DevDump automates much of the process by controlling the debugger, processing the minidumps, and storing the results in a log. After processing all files, DevDump generates various statistics on the detection capabilities of minidumps.

In our experiments, all Windows versions correctly identified the faulty driver in the majority of cases (see Figure 5 and compare with Table 5). The correct identification of the crash source (M1) appears to be independent of the file system used. Only in a few instances, such as the 7-InitEvt function, did the crash source differ between Server 2003 FAT32 and Server 2003 NTFS.

Overall, the results show that Windows XP is more accurate than other operating systems (see 15-RelLock and 20-memset). However, there were cases where other kernel modules were incorrectly identified (functions 1-InitStr, 14-AcqLock, and 15-RelLock), as shown in Figure 6.

### Figures

**Figure 3: Relative Robustness (FM1/#DD)**

| OS Version | Relative Robustness (%) |
|------------|-------------------------|
| XP NTFS    |                         |
| XP FAT32   |                         |
| 2003 FAT32 |                         |
| 2003 NTFS  |                         |
| Vista NTFS |                         |

**Figure 4: File System Sensitivity (FM4/FM3)**

| OS Version | File System Sensitivity (%) |
|------------|-----------------------------|
| XP NTFS    |                             |
| XP FAT32   |                             |
| 2003 FAT32 |                             |
| 2003 NTFS  |                             |
| Vista NTFS |                             |

**Figure 5: Source Identification OK (M1)**

| OS Version | Correct Identification (%) |
|------------|----------------------------|
| M1-XP NTFS |                            |
| M1-XP FAT32|                            |
| M1-2003 FAT32|                          |
| M1-2003 NTFS|                           |
| M1-Vista   |                            |

**Figure 6: Source Identification Error (M2)**

| OS Version | Incorrect Identification (%) |
|------------|------------------------------|
| M2-XP NTFS |                              |
| M2-XP FAT32|                              |
| M2-2003 FAT32|                            |
| M2-2003 NTFS|                             |
| M2-Vista   |                              |

**Figure 7: Source of Crash Unidentified (M3)**

| OS Version | Unidentified Source (%) |
|------------|-------------------------|
| M3-XP NTFS |                         |
| M3-XP FAT32|                         |
| M3-2003 FAT32|                       |
| M3-2003 NTFS|                        |
| M3-Vista   |                         |

### Errors and Unidentified Sources

These errors are particularly problematic because they can lead to wasted time while searching for bugs in the wrong places and reduce confidence in the information provided by minidumps. In some cases, Windows was unable to determine the cause of the failure. This occurred more frequently in Vista, especially in functions 15-RelLock and 12-SetEvt (see Figure 7). For the 12-SetEvt function, Vista was the only system that could not diagnose the cause of the failure. Only Windows Server 2003 detected memory corruption situations (in functions 14-AcqLock and 15-RelLock).

### 4. Related Work

Robustness testing has been successfully applied to various software components to characterize their behavior under exceptional inputs or stressful conditions. One of the main targets has been general-purpose operating systems, with erroneous inputs injected at the application interface. Fifteen OS versions implementing the POSIX standard, including AIX, Linux, SunOS, and HPUX, were assessed using the Ballista tool [12]. Shelton et al. conducted a comparative study of six Windows variants from 95 to 2000 by injecting faults at the Win32 interface [20]. Ghosh et al. evaluated several command-line utilities of Windows NT [7]. Real-time microkernels like Chorus and LynxOS have also been tested using the MAFALDA tool [2]. Application-level software can be tested using robustness techniques by generating exceptions and returning bad values at the OS interface [8]. Middleware support systems like CORBA have been examined at the client-side interface of an ORB [19] and internally at the Naming and Event services [14]. Dependability benchmarking has utilized robustness testing to evaluate systems [18, 23, 24]. For example, Kalakech et al. proposed an OS benchmark and applied it to Windows 2000 [11].

To our knowledge, only a few works have assessed the robustness of systems at the device driver level. Durães and Madeira described a method to emulate software faults by mutating the binary code of device drivers [5]. They experimented with four low-level patterns specific to the kernel’s DPI on two drivers for Windows NT4, 2000, and XP. Albinet et al. conducted experiments to evaluate the robustness of Linux systems in the presence of faulty drivers [1]. They intercepted driver calls and changed parameters on the fly with preset faulty values. Johansson and Suri employed a similar methodology to evaluate Windows CE .Net [10], focusing on error propagation profiling measures to facilitate the selection of places to put wrappers.

Our research complements these previous works, not only because we targeted different OS but also because our methodology is rooted in the original Ballista tool [13]. Several test drivers were generated, containing DDK function calls with erroneous arguments. The argument values were selected to emulate seven classes of typical programming errors. Our study comparatively examined aspects such as error containment, the influence of the file system type, and the diagnostic capabilities of minidump files.

### 5. Conclusions

This paper describes a robustness testing experiment evaluating Windows XP, Windows Server 2003, and the future Windows release, Vista. The primary objective was to determine how well Windows protects itself from faulty drivers providing erroneous input to DDK routines. Seven classes of typical programming bugs were simulated.

The analysis of the results shows that most interface functions cannot completely check their inputs. Of the 20 selected functions, only 2 were 100% effective in their defense. We observed a small number of hangs and a reasonable number of crashes, primarily due to invalid or NULL pointer values. File corruption was only observed with the FAT32 file system. The analysis of return values indicates that, in some cases, Windows completes without generating an error for function calls with incorrect parameters, with Windows Server 2003 being the most permissive. This behavior suggests a deficient error containment capability of the OS. In most cases, the examined minidump files provided valuable information about the sources of crashes, which is extremely useful for development teams. However, Windows Vista seems to have more difficulties in this identification compared to other OS.

Experiments with Windows Vista revealed that it behaves similarly to Windows XP and Server 2003. This suggests that Microsoft intends to continue using the current DD architecture in its future OS, which is concerning given that Vista will likely be the most widely used OS in the coming years.

### 6. References

[1] A. Albinet, J. Arlat, and J.-C. Fabre, “Characterization of the Impact of Faulty Drivers on the Robustness of the Linux Kernel”, Proceedings of the International Conference on Dependable Systems and Networks, June 2004.

[2] J. Arlat, J.-C. Fabre, M. Rodríguez, and F. Salles, “Dependability of COTS Microkernel-Based Systems”, IEEE Transactions on Computers, vol. 51, no. 2, 2002, pp. 138-163.

[3] A. Chou, J. Yang, B. Chelf, S. Hallem, and D. Engler, “On u-kernel construction”, Proceedings of the Symposium on Operating Systems Principles, December 1995, pp. 237–250.

[4] A. Chou, J. Yang, B. Chelf, S. Hallem, and D. Engler, “An empirical study of operating system errors”, Proceedings of the Symposium on Operating Systems Principles, October 2001, pp. 73–88.

[5] J. Durães and H. Madeira, “Characterization of Operating Systems Behavior in the Presence of Faulty Drivers through Software Fault Emulation”, Proceedings of the Pacific Rim International Symposium on Dependable Computing, December 2002, pp. 201-209.

[6] B. Ford, G. Back, G. Benson, J. Lepreau, A. Lin, and O. Shivers, “The Flux OSKit: a substrate for OS language and resource management”, Proceedings of the Symposium on Operating Systems Principles, October 1997, pp. 38–51.

[7] A. Ghosh, M. Schmid, and V. Shah, “Testing the robustness of Windows NT software”, Proceedings of the Ninth International Symposium on Software Reliability Engineering, November 1998, pp. 231-235.

[8] A. K. Ghosh, M. Schmid, “An Approach to Testing COTS Software for Robustness to Operating System Exceptions and Errors”, Proceedings 10th International Symposium on Software Reliability Engineering, November 1999, pp. 166-174.

[9] R. Gruber, and M. L. Jiang, “Robustness Testing and Hardening of CORBA ORB Implementations”, Proceedings of the International Conference on Dependable Systems and Networks, June 2001, pp. 141-150.

[10] A. Johansson, and N. Suri, “Error Propagation Profiling of Operating Systems”, Proceedings of the International Conference on Dependable Systems and Networks, June 2005.

[11] A. Kalakech, T. Jarboui, J. Arlat, Y. Crouzet, and K. Kanoun, “Benchmarking Operating System dependability: Windows 2000 as a Case Study”, Proceedings Pacific Rim International Symposium on Dependable Computing, March 2004, pp. 261-270.

[12] P. Koopman, J. DeVale, “The Exception Handling Effectiveness of POSIX Operating Systems”, IEEE Transactions on Software Engineering, vol. 26, no. 9, September 2000, pp. 837-848.

[13] N. Kropp, P. Koopman, and D. Siewiorek, “Automated Robustness Testing of Off-the-Shelf Software Components”, Proceedings of the International Symposium on Fault-Tolerant Computing, June 1998.

[14] E. Marsden, J.-C. Fabre, and J. Arlat, “Dependability of CORBA Systems: Service Characterization by Fault Injection”, Proceedings of the 21st International Symposium on Reliable Distributed Systems, June 2002, pp. 276-285.

[15] Microsoft Corporation, “Microsoft Portable Executable and Common Object File Format Specification”, February 2005.

[16] Microsoft Corporation, “Introducing Static Driver Verifier”, May 2006.

[17] Microsoft Corporation, “Debugging Tools for Windows – 2006”, http://www.microsoft.com/whdc/devtools/debugging/default.msp

[18] A. Mukherjee and D. P. Siewiorek, “Measuring Software Dependability by Robustness Benchmarking”, IEEE Transactions on Software Engineering, vol. 23, no. 6, 1997, pp. 366-378.

[19] J. Pan, P. J. Koopman, D. P. Siewiorek, Y. Huang, R. Gruber, and M. L. Jiang, “Robustness Testing and Hardening of CORBA ORB Implementations”, Proceedings of the International Conference on Dependable Systems and Networks, June 2001, pp. 141-150.

[20] C. Shelton, P. Koopman, and K. D. Vale, “Robustness Testing of the Microsoft Win32 API”, Proceedings of the International Conference on Dependable Systems and Networks, June 2000, pp. 261-270.

[21] D. Simpson, “Windows XP Embedded with Service Pack 1 Reliability”, Tech. rep., Microsoft Corporation, January 2003.

[22] M. Swift, B. Bershad, and H. Levy, “Improving the reliability of commodity operating systems”, Proceedings of the Symposium on Operating Systems Principles, October 2003, pp. 207–222.

[23] T. K. Tsai, R. K. Iyer, and D. Jewitt, “An Approach Towards Benchmarking of Fault-Tolerant Commercial Systems”, Proceedings of the 26th International Symposium on Fault-Tolerant Computing, June 1996, pp. 314-323.

[24] M. Vieira and H. Madeira, “A Dependability Benchmark for OLTP Application Environments”, Proceedings of the 29th International Conference on Very Large Data Bases, 2003, pp. 742-753.

[25] M. Young, M. Accetta, R. Baron, W. Bolosky, D. Golub, R. Rashid, and A. Tevanian, “Mach: A new kernel foundation for UNIX development”, Proceedings of the Summer USENIX Conference, June 1986, pp. 93–113.