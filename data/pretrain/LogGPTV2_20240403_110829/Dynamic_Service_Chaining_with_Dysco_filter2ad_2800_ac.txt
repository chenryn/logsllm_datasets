the new path, the anchor must apply its delta to packets. The table
below shows how. To simplify the presentation, we assume that
sequence numbers do not wrap around to zero.
to which
how apply
packet
direction
in
out
delta
add
subtract
field
sequence number
acknowledgment number
Figure 6 illustrates the use of this table. In the figure, a former
right anchor is holding a delta (assumed positive in this example)
from the reconfiguration, which means that a middlebox formerly
to the left of it added delta bytes to the data stream. As shown in the
figure, packets going into the Dysco agent from the new subsession
have delta added to their sequence numbers so that the agent’s
middlebox and all hosts to the right of the agent see consistent
sequence numbers. Packets going out of the Dysco agent on the
new subsession have delta subtracted from their acknowledgment
numbers, so that all hosts to the left of the agent see consistent
acknowledgment numbers.
3.5 Packet handling on two paths
In the second phase of reconfiguration, both old and new paths exist.
To handle packets correctly, the anchors must decide which path
to use when sending data or acknowledgments, and must know
when the old path is no longer needed. To make these decisions, an
anchor maintains the following variables (the “plus one” follows
TCP conventions for sequence numbers):
• oldSent: highest sequence number of bytes sent on old path, plus
one (this is known at the beginning of the phase, as no new data
is sent on the old path);
• oldRcvd: highest sequence number of bytes received on old path,
• oldSentAcked: highest sequence number sent and acknowledged
• oldRcvdAcked: highest sequence number received and acknowl-
• firstNewRcvd: lowest sequence number received on the new path,
edged on old path, plus one;
on old path, plus one;
plus one;
A byte sent by an anchor is allocated to a path according to the
following rules. If a packet contains data for both paths (both new
and retransmitted bytes), then the data must be divided into two
new packets.
if any.
middleboxapplicationformerrightanchorAN = nAN = n - deltaSN = nSN = n + deltaformerly existingsubsessionnew subsessionDynamic Service Chaining with Dysco
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
predicate on byteSeq where to send byte
byteSeq  oldRcvdAcked
packetAck > oldRcvd ∧
oldRcvd = oldRcvdAcked
packetAck > oldRcvd ∧
oldRcvd > oldRcvdAcked
where to send ack
old path
new path
new path, also
ack oldRcvd on old path
If the two sets of rules imply that the data of a packet goes to one
path and its acknowledgment goes to another, then the packet must
be divided into two. These rules need not consider deltas, as deltas
are already applied to incoming packets, and not yet applied to
outgoing packets.
For an anchor to decide that it no longer needs the old path, of
course it must have received acknowledgments for everything it
sent on the old path, or oldSentAcked = oldSent. Knowing that it has
received everything on the old path is harder, unless it has received
a FIN on the old path, because it does not have direct knowledge
of the cutoff sequence number at the other anchor. The first byte
received on the new path is not a reliable indication, because earlier
data sent to it on the new path may have been lost. The correct
predicate is:
oldRcvdAcked = oldRcvd ∧ oldRcvd = firstNewRcvd
The first equality says that everything received has been acknowl-
edged. The second says that the cutoff sequence number must be
oldRcvd. When the old path is no longer needed, reconfiguration is
complete. The anchors send UDP FIN packets on the old path, then
clean up the extra state variables.
If a stateful middlebox in the session is being replaced, additional
delay must be introduced. First, all use of the old path must be com-
pleted. Second, the stateful middlebox on the old path must export
its state for that session to the new stateful middlebox, using exist-
ing mechanisms [39]. Then and only then can data be sent on the
new path. During the interval when the old path is being emptied
and state is being migrated, the anchors must buffer incoming data.
3.6 Failures
If control packets are lost, then the protocol detects this and re-
transmits them. The most significant failure during reconfiguration
is failure to set up the new path, which can happen because of host
failure or network partition. The remedy is to abort the reconfigu-
ration, so the session continues to use the old path. After this the
subsessions of the old path between the anchors are still locked, so
that they cannot be reconfigured in the future. So the left anchor
sends a cancelLock control packet to the right anchor, the right
anchor replies with an ackCancel, and all the agents that receive
these signals unlock their subsessions.
Unfortunately, dynamic reconfiguration cannot be used to re-
cover from the failure of a middlebox. This is because the old path
must be fully operational for the protocol to work. Consequently,
the utility of reconfiguration is limited to policy change and re-
source management, rather than fault-tolerance.
3.7 Design and verification
In designing the reconfiguration protocol, we had to solve a number
of related problems simultaneously. We had to decide how to make
the cutoff between the old and new paths for maximum efficiency
(§3.1), how to exercise distributed control among conflicting re-
configuration attempts (§3.2), how to compute and use deltas to
accommodate the broadest range of middlebox applications (§3.4),
how to split acknowledgments across the two paths and determine
when the use of the old path is completed (§3.5), and how to handle
failures (§3.6). We had to decide whether any particular packet
should be TCP or UDP (§3.3). We also had to deal with many race
conditions—for example, an anchor might receive a FIN going in
either direction in almost any state, and the FIN might indicate the
completion of data transmission on the old path or the completion
of end-to-end TCP data transmission.
We did not believe that we could design such a protocol correctly
without help, so we designed it in Promela, which is the modeling
language of the model-checker (verifier) Spin [15]. In Promela, each
Dysco agent is a concurrent process that communicates with other
processes through message queues. The messages represent both
TCP and UDP packets, with fields for sequence numbers and other
metadata. Each agent is structured as a finite-state machine that
can react to the receipt of a message by reading and writing local
variables, sending other messages, and/or changing state. Choices
made by end-hosts and middlebox applications are modeled by
nondeterminism in the program. As a result, the Promela program
for a Dysco agent has a straightforward structure that translates
easily to actual implementation code.
The great advantage of using Promela for design is that we were
able to verify the model at every step, obtaining immediate feed-
back on bugs and unresolved issues. It was necessary to verify each
configuration separately, where a configuration is an initial service
chain and a set of attempted reconfigurations. For each configura-
tion, Spin checks the model for all possible executions, meaning all
possible network delays and scheduling decisions, which in turn
generates all possible interleavings of modeled events. In a typical
verification run for a typical configuration, Spin constructs a global
state machine of all possible execution behaviors with 100 million
state transitions.
What can verification tell us? Any run of Spin will find errors
such as deadlocks and undefined cases. In addition, it is possible
to check stronger properties by putting assertions at appropriate
points in the model. If execution reaches an assertion point and
the assertion evaluates to false, that will also be flagged as an
error. Using this technique, we were also able to verify that each
configuration has the following desirable properties:
• When multiple left anchors contend to lock overlapping seg-
ments, exactly one of them succeeds.
• No data is lost due to reconfiguration.
• Unless the new path cannot be set up, an attempted reconfigura-
• The sequence and acknowledgment numbers received by end-
• All sessions terminate cleanly.
The model, along with extensive documentation of design, modeling
abstractions, and Spin runs, can be found at [8]. It shows that
tion always succeeds.
hosts are correct.
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
P. Zave et al.
Middleboxes: Dysco supports unmodified middlebox applica-
tions, and we have successfully run with NGINX [27], HAProxy [14],
Iptables/Netfilter [21], Linux tc [22], and libpcap-based middle-
boxes. Most middleboxes send and receive data via libpcap, user
socket, or Linux sk_buff. Some middleboxes only read the pack-
ets (e.g., PRADS [35], Bro [5], Snort [42], Suricata [44], Linux
tc [22], Iptables/Netfilter Firewall [21]) while some others mod-
ify the TCP session identifier or sequence numbers (e.g., Ipta-
bles/Netfilter NAT [21], HAProxy [14], Squid [43]). Middleboxes
that only read the packets and use libpcap or sk_buff run trans-
parently and unmodified with Dysco. To support the removal of
TCP-terminating proxies, the Dysco agent intercepts the Linux
“splice” system call and then invokes the reconfiguration protocol.
We also support a dysco_splice system call that a (modified) mid-
dlebox can use to trigger its removal. We discuss these in more
detail below. Dysco also supports middleboxes that can import
and export internal state as part of migrating a session from one
middlebox instance to another, inspired by OpenNF [11].
Daemon: The Dysco agent performs session setup and tear-
down, as well as data transfers, directly in the kernel. We imple-
mented the reconfiguration protocol in a separate user-space dae-
mon for ease of implementation and debugging. Reconfiguration
messages are infrequent, compared to data packets, so the small
performance penalty for handling reconfiguration in user space is
acceptable. The daemon communicates with the Dysco agent in
the kernel via netlink (a native Linux IPC function), with other
Dysco agents via UDP, and with the policy server via TCP. Our pro-
totype includes a library for a simple management protocol for the
daemon and the policy server. The daemon compiles and forwards
to the kernel the policies received from the policy server, triggers
reconfiguration, and performs state migration when replacing one
middlebox with another (by importing and exporting state, and
serializing and sending the state to another middlebox).
Policy server: The policy server provides a simple command-
line interface for specifying the service-chaining policies and trigger
reconfiguration of live sessions. A policy includes a predicate on
packets, expressed as BPF filters, and a sequence of middleboxes.
The policy server distributes these commands to the relevant Dysco
daemons. Commands can be batched and distributed to different
hosts using shell scripts. The policy server and the Dysco daemon
consist of over 5000 lines of Go of which 3000 lines are a shared
library for message serialization and reliable UDP transmission.
The source code of Dysco as well as the shell scripts used for the
evaluation are available at [8].
4.2 Protocol details
Tagging SYN packets: The local tags added to SYN packets, as
described in §2.1, are implemented with TCP option 253 (reserved
for experimentation). The option carries a unique 32-bit number to
identify the session. SYN packets are tagged only when they are
inside a middlebox host.
Packet rewriting for data transmission: During data trans-
mission, the agent simply rewrites the five-tuple of each incom-
ing or outgoing TCP packet, and applies any necessary sequence
number delta and window scaling. Since the agent rewrites the
packet header, it has to recompute the IP and TCP checksums. All
Figure 7: Implementation, where solid black lines represent
the data path, blue dashed lines the control path, and red
dashed-dotted lines the management path for distributing
policies.
with modern tools, protocol design can be more ambitious without
sacrificing robust operation.
4 DYSCO PROTOTYPE
Our Dysco prototype consists of a kernel-level agent that communi-
cates with an external policy server through a user-space daemon,
as shown in Figure 7.
4.1 Dysco components and interfaces
Agent: The Dysco agent supports unmodified end-host applica-
tions, middleboxes, and host network stacks by intercepting packets
going to/from the network. The agent could be implemented in
various ways, including a modified device driver, a software switch,
or a user-space library like DPDK with direct NIC access. In our
prototype, the Dysco agent is a Linux kernel module that intercepts
packets in the device driver. Even though the Linux kernel is not
the fastest option for high-performance middleboxes, we decided
to do an in-kernel implementation to transparently support TCP-
terminating applications (e.g., proxies, HTTP servers, and clients),
middleboxes that use libpcap to get/send packets from/to the net-
work (e.g., Bro [5], Snort [42]2, and PRADS [35]), and middleboxes
that run in the Linux kernel (e.g., Traffic Controller (tc) [22] and
Iptables/Netfilter Firewall [21]). As the Dysco agent processes all
packets from a TCP session, it can change how TCP behaves in
several ways. For example, it can advertise a smaller receive win-
dow to throttle a sender during reconfiguration or even prevent it
from sending data at all by advertising a window of size zero. Our
prototype also supports network namespaces for virtualized envi-
ronments, such as Docker and Mininet. The kernel module consists
of over 6000 lines of C code, and adds only 16 lines of C code in the
device drivers to call the functions that intercept the packets and
initialize and remove the module. Our prototype currently supports
the Intel ixgbe driver (for 10 gigabit NICs), the e1000 driver (for 1
gigabit NICs), veth (for virtual interfaces), and the Linux bridge.
2Snort uses a Data Acquisition Layer (DAQ) that allows the use of different packet
acquisition methods, such as libpcap and DPDK.
Dynamic Service Chaining with Dysco
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
checksum computations are incremental to avoid recomputing the
checksum of the whole packet.
Minimizing contention during lookups: The agent stores
the mapping between incoming and outgoing five-tuples in a hash
table that uses RCU (Read-Copy-Update) locks for minimizing con-
tention during lookups. Since entries are added to the hash table for
each new TCP session, a naïve locking strategy based on mutexes or
spin locks would degrade the performance significantly. To support
Linux namespaces, the agent maintains one translation table per
namespace.
UDP messages for reconfiguration: Our daemon implements
the reconfiguration protocol using UDP messages. We chose to
use UDP in user space to facilitate development and debugging.
Also, reconfigurations do not occur frequently, so the performance
requirements are not as stringent as for the data plane. The UDP