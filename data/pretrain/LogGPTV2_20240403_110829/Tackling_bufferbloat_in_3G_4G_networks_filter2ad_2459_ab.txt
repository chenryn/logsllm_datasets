200
0
0
4
10
AT&T HSPA+ Sprint EVDO T−Mobile HSPA+ Verizon EVDO
10
20
30
Time (s)
40
50
60
(a) Congestion Window Size
AT&T HSPA+ Sprint EVDO T−Mobile HSPA+ Verizon EVDO
)
s
m
(
T
T
R
3
10
0
10
20
30
Time (s)
40
50
60
(b) Round Trip Time
Figure 6: TCP congestion window grows way be-
yond the BDP of the underlying network due to
buﬀerbloat. Such excessive overshooting leads to
extremely long RTT.
In the previous experiment, the server used CUBIC as its
TCP congestion control algorithm. However, we are also
interested in the behaviors of other TCP congestion con-
trol algorithms under buﬀerbloat. According to [32], a large
portion of the Web servers in the current Internet use high
speed TCP variants such as BIC [30], CUBIC [11], CTCP
[27], HSTCP [7] and H-TCP [19]. How these high speed
TCP variants would perform in buﬀerbloated cellular net-
works as compared to less aggressive TCP variants like TCP
NewReno [8] and TCP Vegas [2] is of great interest.
Figure 7 shows the cwnd and RTT of TCP NewReno, Ve-
gas, CUBIC, BIC, HTCP and HSTCP under AT&T HSPA+
network. We left CTCP out of the picture simply because we
are unable to know its internal behavior due to the closed na-
ture of Windows. As the ﬁgure shows, all the loss-based high
speed TCP variants (CUBIC, BIC, HTCP, HSTCP) over-
shoot more often than NewReno. These high speed variants
were originally designed for eﬃcient probing of the available
bandwidth in large BDP networks. But in buﬀerbloated cel-
lular networks, they only make the problem worse by con-
stant overshooting. Hence, the buﬀerbloat problem adds a
new dimension in the design of an eﬃcient TCP congestion
control algorithm.
In contrast, TCP Vegas is resistive to buﬀerbloat as it uses
a delay-based congestion control algorithm that backs oﬀ as
soon as RTT starts to increase. This behavior prevents cwnd
from excessive growth and keeps the RTT at a low level.
However, delay-based TCP congestion control has its own
problems and is far from a perfect solution to buﬀerbloat.
We will further discuss this aspect in Section 7.
332)
s
t
n
e
m
g
e
S
i
i
(
e
z
S
w
o
d
n
W
n
o
i
t
s
e
g
n
o
C
600
500
400
300
200
100
0
0
NewReno
Vegas
CUBIC
BIC
HTCP
HSTCP
1500
1000
500
)
s
m
(
T
T
R
NewReno
Vegas
CUBIC
BIC
HTCP
HSTCP
2000
4000
6000
For each ACK
8000
10000
12000
0
0
2000
4000
6000
For each ACK
8000
10000
12000
(a) Congestion Window Size
(b) Round Trip Time
Figure 7: All the loss-based high speed TCP variants (CUBIC, BIC, HTCP, HSTCP) suﬀer from the
buﬀerbloat problem more severely than NewReno. But TCP Vegas, a delay-based TCP variant, is resis-
tive to buﬀerbloat.
4. CURRENT TRICK BY SMART PHONE
4.1 Understanding the Abnormal Flat TCP
VENDORS AND ITS LIMITATION
The previous experiments used a Linux laptop with mobile
broadband USB modem as the client. We have not looked
at other platforms yet, especially the exponentially growing
smart phones. In the following experiment, we explore the
behavior of diﬀerent TCP implementations in various desk-
top (Windows 7, Mac OS 10.7, Ubuntu 10.04) and mobile
operating systems (iOS 5, Android 2.3, Windows Phone 7)
over cellular networks.
)
B
K
(
i
e
z
S
w
o
d
n
W
n
o
i
i
t
s
e
g
n
o
C
700
600
500
400
300
200
100
0
0
Windows 7
Mac OS 10.7
iOS 5
Android 2.3
Ubuntu 10.04
Windows Phone 7
10
20
30
Time (s)
40
50
60
Figure 8: The behavior of TCP in various platforms
over AT&T HSPA+ network exhibits two patterns:
“ﬂat TCP” and “fat TCP”.
Figure 8 depicts the evolution of TCP congestion win-
dow when clients of various platforms launch a long-lived
TCP ﬂow over AT&T HSPA+ network. To our surprise,
two types of cwnd patterns are observed: “ﬂat TCP” and
“fat TCP”. Flat TCP, such as observed in Android phones,
is the phenomenon where the TCP congestion window grows
to a constant value and stays there until the session ends.
On the other hand, fat TCP, such as observed in Windows
Phone 7, is the phenomenon that packet loss events do not
occur until the congestion window grows to a large value far
beyond the BDP. Fat TCP can easily be explained by the
buﬀerbloat in cellular networks and the loss-based conges-
tion control algorithm. But the abnormal ﬂat TCP behavior
caught our attention and revealed an untold story of TCP
over cellular networks.
How could the TCP congestion window stay at a constant
value? The static cwnd ﬁrst indicates that no packet loss
is observed by the TCP sender (otherwise the congestion
window should have decreased multiplicatively at any loss
event). This is due to the large buﬀers in cellular networks
and its link layer retransmission mechanism as discussed ear-
lier. Measurement results from [14] also conﬁrm that cellular
networks typically experience close-to-zero packet loss rate.
If packet losses are perfectly concealed, the congestion
window may not drop but it will persistently grow as fat
TCP does. However,
it unexpectedly stops at a certain
value and this value is diﬀerent for each cellular network
or client platform. Our inspection into the TCP implemen-
tation in Android phones (since it is open-source) reveals
that the value is determined by the tcp rmem max param-
eter that speciﬁes the maximum receive window advertised
by the Android phone. This gives the answer to ﬂat TCP be-
havior: the receive window advertised by the receiver crops
the congestion windows in the sender. By inspecting var-
ious Android phone models, we found that tcp rmem max
has diverse values for diﬀerent types of networks (refer to
Table 2 in Appendix B for some sample settings). Generally
speaking, larger values are assigned to faster communica-
tion standards (e.g., LTE). But all the values are statically
conﬁgured.
To understand the impact of such static settings, we com-
pared the TCP performance under various tcp rmem max
values in AT&T HSPA+ network and Verizon LTE network
in Figure 9. Obviously, a larger tcp rmem max allows the
congestion window to grow to a larger size and hence leads to
higher throughput. But this throughput improvement will
ﬂatten out once the link is saturated. Further increase of
tcp rmem max brings nothing but longer queuing delay and
hence longer RTT. For instance, when downloading from
a nearby server, the RTT is relatively small. In such small
BDP networks, the default values for both HSPA+ and LTE
are large enough to achieve full bandwidth utilization as
shown in Figure 9(a). But they trigger excessive packets
in ﬂight and result in unnecessarily long RTT as shown in
Figure 9(b). This demonstrates the limitation of static pa-
rameter setting: it mandates one speciﬁc trade-oﬀ point in
the system which may be sub-optimal for other applications.
Two realistic scenarios are discussed in the next subsection.
333)
s
p
b
M
(
t
u
p
h
g
u
o
r
h
T
20
15
10
5
0
AT&T HSPA+ (default: 262144)
Verizon LTE (default: 484848)
65536
110208
262144
484848
524288
655360
tcp_rmem_max (Bytes)
)
s
m
(
T
T
R
1400
1200
1000
800
600
400
200
0
AT&T HSPA+ (default: 262144)
Verizon LTE (default: 484848) 
65536
110208
262144
484848
524288
655360
tcp_rmem_max (Bytes)
(a) Throughput
(b) Round Trip Time
Figure 9: Throughput and RTT performance of a long-lived TCP ﬂow in a small BDP network under diﬀerent
tcp rmem max settings. For this speciﬁc environment, 110208 may work better than the default 262144 in
AT&T HSPA+ network. Similarly, 262144 may work better than the default 484848 in Verizon LTE network.
However, the optimal value depends on the BDP of the underlying network and is hard to be conﬁgured
statically in advance.
4.2 Impact on User Experience
Web Browsing with Background Downloading: The
high-end smart phones released in 2012 typically have quad-
core processors and more than 1GB of RAM. Due to their
signiﬁcantly improved capability, the phones are expected to
multitask more often. For instance, people will enjoy Web
browsing or online gaming while downloading ﬁles such as
books, music, movies or applications in the background. In
such cases, we found that the current TCP implementation
incurs long delays for the interactive ﬂow (Web browsing or
online gaming) since the buﬀer is ﬁlled with packets belong-
ing to the background long-lived TCP ﬂow.
)
x
≤
X
P
(
1
0.8
0.6
0.4
0.2
0
0
With background downloading (avg=2.65s)
Without background downloading (avg=1.02s)
5
Web Object Fetching Time (s)
10
15
Figure 10: The average Web object fetching time
is 2.6 times longer when background downloading is
present.
Figure 10 demonstrates that the Web object fetching time
is severely degraded when a background download is un-
der way. In this experiment, we used a simpliﬁed method
to emulate Web traﬃc. The mobile client generates Web
requests according to a Poisson process. The size of the
content brought by each request is randomly picked among
8KB, 16KB, 32KB and 64KB. Since these Web objects are
small, their fetching time mainly depends on RTT rather
than throughput. When a background long-lived ﬂow causes
long queues to be built up, the average Web object fetching
time becomes 2.6 times longer.
Throughput in Large BDP Networks: The sites that
smart phone users visit are diverse. Some contents are well
maintained and CDNs (content delivery networks) are as-
sisting them to get “closer” to their customers via replica-
tion.
In such cases, the throughput performance can be
satisfactory since the BDP of the network is small (Fig-
ure 9). However, there are still many sites with long latency
due to their remote locations. In such cases, the static set-
ting of tcp rmem max (which is tuned for moderate latency
case) fails to ﬁll the long fat pipe and results in sub-optimal
throughput. Figure 11 shows that when a mobile client in
Raleigh, U.S. downloads contents from a server in Seoul, Ko-
rea over AT&T HSPA+ network and Verizon LTE network,
the default setting is far from optimal in terms of through-
put performance. A larger tcp rmem max can achieve much
higher throughput although setting it too large may cause
packet loss and throughput degradation.
In summary, ﬂat TCP has performance issues in both
throughput and delay. In small BDP networks, the static
setting of tcp rmem max may be too large and cause un-
necessarily long end-to-end latency. On the other hand, it
may be too small in large BDP networks and suﬀer from
signiﬁcant throughput degradation.
5. OUR SOLUTION
In light of the limitation of a static tcp rmem max setting,
we propose a dynamic receive window adjustment algorithm
to adapt to various scenarios automatically. But before dis-
cussing our proposal, let us ﬁrst look at how TCP receive
windows are controlled in the current implementations.
5.1 Receive Window Adjustment in Current
TCP Implementations
As we know, the TCP receive window was originally de-
signed to prevent a fast sender from overwhelming a slow
receiver with limited buﬀer space. It reﬂects the available
buﬀer size on the receiver side so that the sender will not
send more packets than the receiver can accommodate. This
is called TCP ﬂow control, which is diﬀerent from TCP con-
gestion control whose goal is to prevent overload in the net-
work rather than at the receiver. Flow control and conges-
tion control together govern the transmission rate of a TCP
sender and the sending window size is the minimum of the
advertised receive window and the congestion window.
With the advancement in storage technology, memories
are becoming increasingly cheaper. Currently, it is common
33410
AT&T HSPA+ (default: 262144)
Verizon LTE (default: 484848)
1000
AT&T HSPA+ (default: 262144)
Verizon LTE (default: 484848) 
)
s
p
b
M
(
t
u
p
h
g
u
o
r
h
T
8
6
4
2
0
65536
262144
484848
655360
917504 1179648 1310720
tcp_rmem_max (Bytes)
)
s
m
(
T
T
R
800
600
400
200
0
65536
262144
484848
655360
tcp_rmem_max (Bytes)
917504 1179648 1310720
(a) Throughput
(b) Round Trip Time
Figure 11: Throughput and RTT performance of a long-lived TCP ﬂow in a large BDP network under diﬀerent
tcp rmem max settings. The default setting results in sub-optimal throughput performance since it fails to
saturate the long fat pipe. 655360 for AT&T and 917504 for Verizon provide much higher throughput.
to ﬁnd computers (or even smart phones) equipped with gi-
gabytes of RAM. Hence, buﬀer space on the receiver side
is hardly the bottleneck in the current Internet. To im-
prove TCP throughput, a receive buﬀer auto-tuning tech-
nique called Dynamic Right-Sizing (DRS [6]) was proposed.
In DRS, instead of determining the receive window based
by the available buﬀer space, the receive buﬀer size is dy-
namically adjusted in order to suit the connection’s demand.
Speciﬁcally, in each RTT, the receiver estimates the sender’s
congestion window and then advertises a receive window
which is twice the size of the estimated congestion window.
The fundamental goal of DRS is to allocate enough buﬀer
(as long as we can aﬀord it) so that the throughput of the
TCP connection is never limited by the receive window size
but only constrained by network congestion. Meanwhile,
DRS tries to avoid allocating more buﬀers than necessary.
Linux adopted a receive buﬀer auto-tuning scheme simi-
lar to DRS since kernel 2.4.27. Since Android is based on
Linux, it inherits the same receive window adjustment al-