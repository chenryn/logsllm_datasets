All systems
Hardware
Software
Network
Environment
Human
Unknown
Figure 1. The breakdown of failures into root causes (a) and the breakdown of downtime into root causes (b). Each
graph shows the breakdown for systems of type D, E, F, G, and H and aggregate statistics across all systems (A–H).
ure record. If the system administrator was able to identify
the root cause of the problem he provides operations staff
with the appropriate information for the “root cause” ﬁeld
of the failure record. Otherwise the root cause is speciﬁed
as “Unknown”. Operations staff and system administrators
have occasional follow-up meetings for failures with “Un-
known” root cause. If the root cause becomes clear later on,
the corresponding failure record is amended.
Two implications follow from the way the data was col-
lected. First, this data is very different from the error logs
used in many other studies. Error logs are automatically
generated and track any exceptional events in the system,
not only errors resulting in system failure. Moreover, error
logs often contain multiple entries for the same error event.
Second, since the data was created manually by system
administrators, the data quality depends on the accuracy of
the administrators’ reporting. Two potential problems in hu-
man created failure data are underreporting of failure events
and misreporting of root cause. For the LANL data we don’t
consider underreporting (i.e. a failure does not get reported
at all) a serious concern, since failure detection is initiated
by automatic monitoring and failure reporting involves sev-
eral people from different administrative domains (opera-
tions staff and system administrators). While misdiagnosis
can never be ruled out completely, its frequency depends
on the administrators’ skills. LANL employs highly-trained
staff backed by a well-funded cutting edge technology inte-
gration team, often pulling new technology into existence
in collaboration with vendors; diagnosis can be expected to
be as good as any customer and often as good as a vendor.
3 Methodology
We characterize an empirical distribution using three im-
port metrics: the mean, the median, and the squared coefﬁ-
cient of variation (C2). The squared coefﬁcient of variation
is a measure of variability and is deﬁned as the squared stan-
dard deviation divided by the squared mean. The advantage
of using the C2 as a measure of variability, rather than the
variance or the standard deviation, is that it is normalized by
the mean, and hence allows comparison of variability across
distributions with different means.
We also consider the empirical cumulative distribution
function (CDF) and how well it is ﬁt by four probability
distributions commonly used in reliability theory1: the ex-
ponential, the Weibull, the gamma and the lognormal distri-
bution. We use maximum likelihood estimation to param-
eterize the distributions and evaluate the goodness of ﬁt by
visual inspection and the negative log-likelihood test.
Note that the goodness of ﬁt that a distribution achieves
depends on the degrees of freedom that the distribution of-
fers. For example, a phase-type distribution with a high
number of phases would likely give a better ﬁt than any of
the above standard distributions, which are limited to one or
two parameters. Whenever the quality of ﬁt allows, we pre-
fer the simplest standard distribution, since these are well
understood and simple to use.
In our study we have not
found any reason to depend on more degrees of freedom.
4 Root cause breakdown
An obvious question when studying failures in computer
systems is what caused the failures. Below we study the
entries in the high-level root cause ﬁeld of the data.
We ﬁrst look at the relative frequency of the six high-
level root cause categories: human, environment, network,
software, hardware, and unknown. Figure 1(a) shows the
percentage of failures in each of the six categories. The
right-most bar describes the breakdown across all failure
records in the data set. Each of the ﬁve bars to the left
presents the breakdown across all failure records for sys-
tems of a particular hardware type2.
Figure 1 indicates that while the basic trends are similar
across system types, the actual breakdown varies. Hardware
1We also considered the Pareto distribution[22, 15], but didn’t ﬁnd it to
be a better ﬁt than any of the four standard distributions
2For better readability, we omit bars for types A–C, which are small
single-node systems.
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:29:50 UTC from IEEE Xplore.  Restrictions apply. 
r
a
e
y
r
e
p
s
e
r
u
l
i
a
f
f
o
r
e
b
m
u
N
1200
1000
800
600
400
200
0
1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18 19 20 21 22
System ID
(a)
c
o
r
p
r
e
p
r
a
e
y
r
e
p
s
e
r
u
l
i
a
f
f
o
r
e
b
m
u
N
3
2.5
2
1.5
1
0.5
0
1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18 19 20 21 22
System ID
(b)
Figure 2. (a) Average number of failures for each system per year. (b) Average number of failures for each system per
year normalized by number of processors in the system. Systems with the same hardware type have the same color.
is the single largest component, with the actual percentage
ranging from 30% to more than 60%. Software is the sec-
ond largest contributor, with percentages ranging from 5%
to 24%. Type D systems differ most from the other systems,
in that hardware and software are almost equally frequent.
It is important to note that in most systems the root cause
remained undetermined for 20–30% of the failures (except
for type E systems, where less than 5% of root causes are
unknown). Since in all systems the fraction of hardware
failures is larger than the fraction of undetermined failures,
and the fraction of software failures is close to that of un-
determined failures, we can still conclude that hardware
and software are among the largest contributors to failures.
However, we can not conclude that any of the other failure
sources (Human, Environment, Network) is insigniﬁcant.
We also study how much each root cause contributes
to the total downtime. Figure 1(b) shows the total down-
time per system broken down by the downtime root cause.
The basic trends are similar to the breakdown by frequency:
hardware tends to be the single largest component, followed
by software.
Interestingly, for most systems the failures
with unknown root cause account for less than 5% of the
total downtime, despite the fact that the percentage of un-
known root causes is higher. Only systems of type D and G
have more than 5% of downtime with unknown root cause.
We asked LANL about the higher fraction of downtime
with unknown root cause for systems of type D and G and
were told the reason lies in the circumstances surrounding
their initial deployment. Systems of type G were the ﬁrst
NUMA based clusters at LANL and were commissioned
when LANL just started to systematically record failure
data. As a result, initially the fraction of failures with un-
known root causes was high (> 90%), but dropped to less
than 10% within 2 years, as administrators gained more ex-
perience with the system and the root cause analysis. Sim-
ilarly, the system of type D was the ﬁrst large-scale SMP
cluster at LANL, so initially the number of unknown root
causes was high, but then quickly dropped.
The above example shows that interpreting failure data
often requires interaction with the people who run the sys-
tems and collected the data. The public release of the
data [1] includes a complete FAQ of questions we asked
LANL in the process of our work.
In addition to the ﬁve high-level root cause categories,
we also looked at the more detailed root cause information.
We ﬁnd that in all systems memory related failures make up
a signiﬁcant portion of all failures. For all systems, more
than 10% of all failures (not only hardware failures) were
due to memory, and in systems F and H memory caused
even more than 25% of all failures. Memory was the single
most common ”low-level” root cause for all systems, except
for system E. System E experienced a very high percentage
(more than 50%) of CPU related failures, due to a design
ﬂaw in the type E CPU.
The detailed breakdown for software related failures
varies more across systems. For system F, the most com-
mon software failure was related to the parallel ﬁle system,
for system H to the scheduler software and for system E to
the operating system. For system D and G, a large portion
of the software failures were not speciﬁed further.
5 Analysis of failure rates
5.1 Failure rate as a function of system
and node
This section looks at how failure rates vary across differ-
ent systems, and across the nodes within the same system.
Studying failure rates across different systems is interesting
since it provides insights on the effect of parameters such
as system size and hardware type. Knowledge on how fail-
ure rates vary across the nodes in a system can be utilized
in job scheduling, for instance by assigning critical jobs or
jobs with high recovery time to more reliable nodes.
Figure 2(a) shows for each of the 22 systems the aver-
age number of failures recorded per year during the sys-
tem’s production time. The yearly failure rate varies widely
across systems, ranging from only 17 failures per year for
system 2, to an average of 1159 failures per year for system
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:29:50 UTC from IEEE Xplore.  Restrictions apply. 
350
300
250
200
150
100
50
e
m
i
t
e
f
i
l
g
n
i
r
u
d
s
e
r
u
l
i
a
f
f
o
r
e
b
m
u
N
0
0
10
20
Node ID
30
40
(a)
1
0.8
0.6
0.4
0.2
y
t
i
l
i
b
a
b
o
r
p
e
v
i
t
l
a
u
m
u
C
0
50
Data
Normal dist.
Poisson dist.
Lognormal dist.
150
200
100
Number of failures per node
(b)
Figure 3. (a) Number of failures per node for system 20 as a function of node ID. (b) The corresponding CDF, ﬁtted
with a Poisson, normal and lognormal distribution.
7. In fact, variability in the failure rate is high even among
systems of the same hardware type.
The main reason for the vast differences in failure rate
across systems is that they vary widely in size. Figure 2(b),
shows for each system the average number of failures per
year normalized by the number of processors in the sys-
tem. The normalized failure rates show signiﬁcantly less
variability across systems, in particular across systems with
the same hardware type. For example, all type E systems
(systems 5–12) exhibit a similar normalized failure rate3, al-
though they range in size from 128–1024 nodes. The same
holds for type F systems (systems 13–18), which vary in
size from 128–512 nodes. This indicates that failure rates
do not grow signiﬁcantly faster than linearly with system
size.
We next concentrate on the distribution of failures across
the nodes of a system. Figure 3(a) shows the total num-
ber of failures for each node of system 20 during the entire
system lifetime4. We ﬁrst observe that nodes 21–23 expe-
rienced a signiﬁcantly higher number of failures than the
other nodes. While nodes 21–23 make up only 6% of all
nodes, they account for 20% of all failures. A possible ex-
planation is that nodes 21–23 run different workloads than
the other nodes in the system. Nodes 21-23 are the only
nodes used for visualization, as well as computation, re-
sulting in a more varied and interactive workload compared
to the other nodes. We make similar observations for other
systems, where failure rates vary signiﬁcantly depending on
a node’s workload. For example, for systems E and F, the
front-end nodes, which run a more varied, interactive work-
load, exhibit a much higher failure rate than the other nodes.
While it seems clear from Figure 3(a) that the behav-
ior of graphics nodes is very different from that of other
nodes, another question is how similar the failure rates of
the remaining (compute-only) nodes are to each other. Fig-
3The higher failure rates for systems 5–6 are due to the fact that they
were the ﬁrst systems of type E at LANL and experienced a higher failure
rate during the initial months of deployment.
4Note that the lifetime of all nodes is the same, with the exception of
node 0, which has been in production for a much shorter time (see Table 1).
ure 3(b) shows the CDF of the measured number of failures
per node for compute only nodes, with three different dis-
tributions ﬁtted to it: the Poisson, the normal, and the log-
normal distributions. If the failure rate at all nodes followed
a Poisson process with the same mean (as often assumed
e.g. in work on checkpointing protocols), the distribution of
failures across nodes would be expected to match a Poisson
distribution. Instead we ﬁnd that the Poisson distribution is
a poor ﬁt, since the measured data has a higher variability
than that of the Poisson ﬁt. The normal and lognormal dis-
tribution are a much better ﬁt, visually as well as measured
by the negative log-likelihood. This indicates that the as-
sumption of Poisson failure rates with equal means across
nodes is suspect.
5.2 Failure rate at diﬀerent time scales
Next we look at how failure rates vary across different
time scales, from very large (system lifetime) to very short
(daily and weekly). Knowing how failure rates vary as a
function of time is important for generating realistic failure
workloads and for optimizing recovery mechanisms.
We begin with the largest possible time-scale by look-
ing at failure rates over the entire lifetime of a system. We
ﬁnd that for all systems in our data set the failure rate as a
function of system age follows one of two shapes. Figure 4
shows a representative example for each shape.
Figure 4(a) shows the number of failures per month for
system 5, starting at production time. Failure rates are high
initially, and then drop signiﬁcantly during the ﬁrst months.
The shape of this curve is the most common one and is rep-
resentative of all systems of type E and F.
The shape of this curve is intuitive in that the failure rate
drops during the early age of a system, as initial hardware
and software bugs are detected and ﬁxed and administrators
gain experience in running the system. One might wonder
why the initial problems were not solved during the typi-
cally 1–2 months of testing before production time. The
reason most likely is that many problems in hardware, soft-
ware and conﬁguration are only exposed by real user code
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:29:50 UTC from IEEE Xplore.  Restrictions apply. 
300
250
h
t
n
o
m
r
e
p
s
e
r
u
l
i
a
F
200
150
100
50
0
0
10
Hardware