title:Detailed diagnosis in enterprise networks
author:Srikanth Kandula and
Ratul Mahajan and
Patrick Verkaik and
Sharad Agarwal and
Jitendra Padhye and
Paramvir Bahl
Detailed Diagnosis in Enterprise Networks
Srikanth Kandula
Sharad Agarwal
Ratul Mahajan
Jitendra Padhye
Patrick Verkaik (UCSD)
Paramvir Bahl
Microsoft Research
ABSTRACT
By studying trouble tickets from small enterprise networks, we
conclude that their operators need detailed fault diagnosis. ˆat
is,
the diagnostic system should be able to diagnose not only
generic faults (e.g., performance-related) but also application speci(cid:12)c
faults (e.g., error codes). It should also identify culprits at a (cid:12)ne gran-
ularity such as a process or (cid:12)rewall con(cid:12)guration. We build a sys-
tem, called NetMedic, that enables detailed diagnosis by harnessing
the rich information exposed by modern operating systems and ap-
plications. It formulates detailed diagnosis as an inference problem
that more faithfully captures the behaviors and interactions of (cid:12)ne-
grained network components such as processes. ˆe primary chal-
lenge in solving this problem is inferring when a component might
be impacting another. Our solution is based on an intuitive technique
that uses the joint behavior of two components in the past to estimate
the likelihood of them impacting one another in the present. We (cid:12)nd
that our deployed prototype is e(cid:11)ective at diagnosing faults that we
inject in a live environment. ˆe faulty component is correctly identi-
(cid:12)ed as the most likely culprit in @ʃʂ of the cases and is almost always
in the list of top (cid:12)ve culprits.
Categories and Subject Descriptors
C.ʇ [Performance of systems] Reliability, availability, serviceability
General Terms
Algorithms, design, management, performance, reliability
Keywords
Enterprise networks, applications, fault diagnosis
1.
INTRODUCTION
Diagnosing problems in computer networks is frustrating. Mod-
ern networks have many components that interact in complex ways.
Con(cid:12)guration changes in seemingly unrelated (cid:12)les, resource hogs
elsewhere in the network, and even so(cid:13)ware upgrades can ruin what
worked perfectly yesterday. ˆus, the development of tools to help
operators diagnose faults has been the subject of much research and
commercial activity [ʅ, ʇ, ʈ, ʉ, ʄʄ, ʄʅ, ʄ@, ʅʄ].
Because little is known about faults inside small enterprise net-
works, we conduct a detailed study of these environments. We reach
a surprising conclusion. As we explain below, existing diagnostic sys-
tems, designed with large, complex networks in mind, fall short at
helping the operators of small networks.
Our study is based on trouble tickets that describe problems re-
ported by the operators of small enterprise networks. We observe
that most problems in this environment concern application speci(cid:12)c
issues such as certain features not working or servers returning error
codes. Generic problems related to performance or reachability are
in a minority. ˆe culprits underlying these faults range from bad
application or (cid:12)rewall con(cid:12)guration to so(cid:13)ware and driver bugs.
We conclude that detailed diagnosis is required to help these op-
erators. ˆat is, the diagnostic system should be capable of observing
both generic as well as application-speci(cid:12)c faults and of identifying
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. To copy otherwise, to republish, to post on servers or to redistribute
to lists, requires prior speciﬁc permission and/or a fee.
SIGCOMM’09, August 17–21, 2009, Barcelona, Spain.
Copyright 2009 ACM 978-1-60558-594-9/09/08 ...$5.00
culprits at the granularity of processes and con(cid:12)guration entries. Di-
agnosis at the granularity of machines is not very useful. Operators
o(cid:13)en already know which machine is faulty. ˆey want to know what
is amiss in more detail.
Existing diagnostic systems fall short because they either lack de-
tail or require extensive domain knowledge. ˆe systems for large en-
terprises, such as Sherlock [ʅ], target only performance and reacha-
bility issues and diagnose at the granularity of machines. ˆey essen-
tially sacri(cid:12)ce detail in order to scale. Other systems, such as Pinpoint
for online services [ʈ] and SCORE for ISP networks [ʄ@], use exten-
sive knowledge of the structure of their domains. Extending them to
perform detailed diagnosis in enterprise networks would require em-
bedding detailed knowledge of each application’s dependencies and
failure modes. ˆe range and complexity of applications inside mod-
ern enterprises makes this task intractable.
Can detailed diagnosis be enabled with little application speci(cid:12)c
knowledge? By developing a system called NetMedic, we show that
the answer is yes. ˆe two keys to our solution are: i) framing de-
tailed diagnosis as an inference problem that is much richer than cur-
rent formulations [ʅ, ʈ, ʄ@, ʆʅ]; and ii) a novel technique to estimate
when two entities in the network are impacting each other without
programmed knowledge of how they interact.
Our formulation models the network as a dependency graph of
(cid:12)ne-grained components such as processes and (cid:12)rewall con(cid:12)gura-
tion. While dependency graphs have been used previously [ʅ, ʈ, ʄ@,
ʆʅ], our formulation is di(cid:11)erent. One di(cid:11)erence is that it captures
the state of a network component using many variables rather than
a single, abstract variable that denotes overall health. Di(cid:11)erent vari-
ables capture di(cid:11)erent aspects of component behavior. For instance,
the variables for a process may include its resource consumption, re-
sponse time for its queries, and application-speci(cid:12)c aspects such as
fraction of responses with error codes. Another di(cid:11)erence is that our
formulation allows for components impacting each other in com-
plex ways depending on their state; existing formulations assume that
faulty components hurt dependent components irrespective of the
nature of the failure. ˆese di(cid:11)erences are necessary for observing
and diagnosing a rich set of failure modes. For instance, whether or
not a faulty process hurts other processes on the same machine de-
pends on its resource consumption. For correct diagnosis, the model
must capture the behavior of the process in detail as well as allow for
both possibilities.
ˆe goal of diagnosis in our model is to link a(cid:11)ected components
to components that are likely culprits, through a chain of dependency
edges. ˆe basic primitive required is inferring the likelihood that
the source component of a dependency edge is impacting the desti-
nation. ˆis inference is challenging because components interact in
complex ways. And because we want to be application agnostic, we
cannot rely on knowing the semantics of individual state variables.
Our insight is to use the joint behavior of the components in the
past to estimate impact in the present. We search in the history of
component states for time periods where the source component’s
state is “similar” to its current state. If during those periods the des-
tination component is o(cid:13)en in a state similar to its current state, the
chances are that it is currently being impacted by the source compo-
nent. If not, it is likely that the source component in its current state
is not impacting the destination component.
Our system, NetMedic, builds on this insight to identify likely
culprits behind observed abnormal behaviors in the network. ˆe
243Observed symptom
ˆe browser saw error codes when accessing
some of the pages on the Web server even
though they had correct permissions.
An application was observing intermittently
high response times to its server.
Some of the clients were unable to access a
speci(cid:12)c feature of a Web-based application.
ˆe mail client (Outlook) was not showing
up-to-date calendar information.
None of the clients in the network could send
email.
Database server refused to start.
An application client was getting RPC errors
when contacting the server.
ˆe clients were experiencing poor perfor-
mance to a database server.
ʄ
ʅ
ʆ
ʇ
ʈ
ʉ
@
@
Identi(cid:12)ed cause
A so(cid:13)ware update had changed the Web server’s con(cid:12)guration. In the new con(cid:12)guration, it was
not correctly processing some required scripts. ˆe operator was aware of the update but not of
the con(cid:12)guration change.
An unrelated process on the server’s machine was intermittently consuming a lot of memory.
ˆe (cid:12)rewall con(cid:12)guration on a router along the path was blocking https tra(cid:14)c that was required
for that feature. ˆe operator did not know when or how the (cid:12)rewall con(cid:12)guration had changed.
A remote folder on the client machine was unmounted during a defragmentation operation. ˆe
operator did not know that defragmentation could lead to the unmounting of a remote folder.
ˆe con(cid:12)guration of the client was overriden with incorrect mail server type. ˆe probable cause
of the change was a bug in the client so(cid:13)ware that was triggered by an overnight update.
ˆe server was miscon(cid:12)gured. ˆe operator did not know how that happened.
A low-level service (IPSec) on the client machine was intercepting application tra(cid:14)c. ˆe oper-
ator did not know how the service got turned on or that it could interfere with the application.
Another client was generating too many requests.
@ ˆe network latency between hosts was high.
ˆe database server was returning errors to a
subset of the clients.
ʄʃ
A buggy process was broadcasting UDP packets at a high rate.
A port that was being used by the problematic clients had been blocked by a change in (cid:12)rewall
con(cid:12)guration on the server machine. ˆe operator was not aware of the con(cid:12)guration change.
Table ʄ. Example problems in our logs.
rich information on component states needed for detailed diagno-
sis is already exported by modern operating systems and applica-
tions [ʅʃ, ʅʈ]. NetMedic takes as input simple templates (e.g., a ma-
chine depends on all active processes) to automatically build the de-
pendency graph amongst components. It implements history-based
reasoning in a way that is robust to idiosyncrasies of real-world data.
It uses statistical abnormality detection as a pruning step to avoid
being misguided by components that have not changed appreciably.
And it uses simple learning techniques to extract enough relevant in-
formation about state variables to compete favorably with a scheme
that uses domain knowledge.
We evaluate our approach by deploying NetMedic in two environ-
ments, including a live environment with actively used desktops. In
this environment, NetMedic built a dependence graph with roughly
ʄʃʃʃ components and ʆʉʃʃ edges, with each component populated
by roughly ʆʈ state variables. By injecting faults drawn from our trou-
ble tickets, which comprise both fail-stop and performance problems,
we (cid:12)nd that in almost all cases NetMedic places the faulty component
in the list of top (cid:12)ve causes. In @ʃʂ of them, the faulty component is
the top identi(cid:12)ed cause. Compared to a diagnostic method based on
current formulations, this ability represents a (cid:12)ve-fold improvement.
We show that NetMedic is more e(cid:11)ective because its history-based
technique correctly identi(cid:12)es many situations where the components
are not impacting each other. Additionally, this ability requires only
a modest amount of history (ʆʃ-ʉʃ minutes).
We randomly selected ʃ.ʄʂ of the cases and read them manually.
We decided to read the cases to get detailed insights into the nature of
the problems and also because the unstructured nature of the logs de-
(cid:12)ed our attempts at automatic classi(cid:12)cation. We discarded cases that
were incomplete, contained internal communication between sup-
port personnel, or contained non-faults such as forgotten passwords.
Our analysis is based on the remaining ʄʇ@ cases. While these cases
represents a small fraction of the total, we (cid:12)nd that the resulting clas-
si(cid:12)cation is consistent even when we use only a randomly selected
half of these cases.
We (cid:12)rst describe example cases from our logs and then provide a
broader classi(cid:12)cation of all that we read.
2.1 Example problems
Table ʄ shows ten problems in our logs that we (cid:12)nd interesting.
Our intent is to provide concrete descriptions of a diverse set of prob-
lems rather than being quantitatively representative. We see that the
range of symptoms is large and consists of application-speci(cid:12)c errors
as well as performance and reachability issues. ˆe range of underly-
ing causes is large as well and consists of bugs, con(cid:12)guration changes,
overload, and side-e(cid:11)ects of planned activities.
While it may be straightforward to design point solutions to each
of these problems, it is challenging to design a comprehensive system
that covers all of them. ˆe design and implementation of such a
system is a goal of our work.
2. PROBLEMS IN SMALL ENTERPRISES
2.2 Classiﬁcation results
To understand what plagues small enterprise networks, we ana-
lyze trouble ticket logs from an organization that provides technical
support for such networks. ˆe logs indicate that the network sizes
vary from a few to a few hundred computers. To our knowledge, ours
is the (cid:12)rst study of faults in such networks.
Our logs span an entire month (Feb ’ʃ@) and contain ʇʈʃK cases.
A case documents the information related to a problem, mainly as
a free form description of oral or electronic conversation between
the operator of the small enterprise network and the support person-
nel. Most cases span multiple conversations and describe the problem
symptoms, impact, and the culprit if identi(cid:12)ed. A case also contains
other information such as when the network was behaving normally
and any recent changes that in the operator’s knowledge may have
resulted in the abnormality.
Since the logs contain only faults for which operators contacted
an external support organization, they may not be representative of
all problems that occur. ˆey are likely biased towards those that op-
erators struggle to diagnose independently and need help with.
Table ʅ classi(cid:12)es the cases that we read along three dimensions
to understand the demands on a diagnostic system—the fault symp-
toms that it should detect and the culprits that it should identify.
ˆe (cid:12)rst dimension captures whether the fault impacted an indi-
vidual application or the entire machine (i.e., many applications on
it). It does not relate directly to the underlying cause. For instance,
the machine category includes cases where a faulty application im-
pacted the entire machine. ˆe data shows that most of the problem
reports refer to individual applications and hence monitoring ma-
chine health alone will miss many faults. To detect these faults, a
diagnostic system must monitor individual applications.
ˆe second category is based on how the fault manifests. We see
that application-speci(cid:12)c defects account for a majority of the cases.
ˆese include conditions such as the application servers returning
error codes, features not working as expected, and a high number
of failed requests. ˆe prevalence of such symptoms indicates the
need to track application-speci(cid:12)c health. Unlike the more generic
symptoms, it is unclear how a diagnostic system can track application
244ʄ. What was impacted
An application
Entire machine
ʅ. Symptom
Application-speci(cid:12)c faults
Failed initialization
Performance
Hang or crash
Unreachability
ʆ. Identi(cid:12)ed cause
Other con(cid:12)guration
Application con(cid:12)guration
So(cid:13)ware bug
Driver bug
Overload
Hardware fault
Unknown
ʄʅʈ
ʅʆ
(@ʇ.ʈʂ)
(ʄʈ.ʈ)
@@
ʄ@
ʄʈ
ʄʈ
ʄʄ
ʇʇ
ʅ@
ʅʃ
ʄʃ
ʉ
ʆ
ʆ@
(ʈ@.ʈʂ)
(ʄʅ.@)
(ʄʃ.ʄ)
(ʄʃ.ʄ)
( @.ʇ)
(ʅ@.@ʂ)
(ʄ@.@)
(ʄʆ.ʈ)
( ʉ.@)
( ʇ.ʄ)
( ʅ.ʃ)
(ʅʈ.ʃ)
Table ʅ. A classi(cid:12)cation of the problems in our logs.
health without knowing application semantics or requiring help from
the application. We show later how we handle this issue.
ˆe (cid:12)nal category shows the root causes of the faults. In ʄ@ʂ of
the cases, the application con(cid:12)guration was incorrect. ˆe biggest
cause, however, was some other con(cid:12)guration element in the envi-
ronment on which the application depends. We de(cid:12)ne other con-
(cid:12)guration quite broadly to include the lower-layer services that are
running, the (cid:12)rewall con(cid:12)guration, the installed devices and device
drivers etc. For ʅʈʂ of the faults, the underlying cause could not be
identi(cid:12)ed but recovery actions such as a reboot (cid:12)xed some anyway.
Unlike other settings [ʄʃ, ʄ@, ʅʅ], it appears from the logs that in
most cases incorrect con(cid:12)guration was not a result of mistakes on
the part of the operators. Rather, con(cid:12)guration was overwritten by
a so(cid:13)ware update or a bug without their knowledge. In many other
cases, the con(cid:12)guration change was intentional but the operators did
not realize the e(cid:11)ects of that change.
2.3 Discussion
Statistics aside, the overall picture that emerges from the logs is
that small business networks are very dynamic. ˆey undergo fre-
quent changes, both deliberate (e.g., installing new applications, up-
grading so(cid:13)ware or hardware) as well as inadvertent (e.g., triggering
of latent bugs, automatic updates). Each change impacts many com-
ponents in the network, some of which may be seemingly unrelated.
Detecting individual changes is rather easy. Applications and op-
erating systems today expose plenty of low-level information, for in-
stance, Windows Vista exposes over ʆʈ di(cid:11)erent aspects of a process’s
current behavior. However, complex interactions and unknown se-
mantics make it hard to use this information to identify the reasons
behind speci(cid:12)c abnormalities of interest to operators.