## 部署 Kubernetes 的 Master 节点在上一篇文章中，我已经介绍过 kubeadm 可以一键部署 Master节点。不过，在本篇文章中既然要部署一个"完整"的 Kubernetes集群，那我们不妨稍微提高一下难度：通过配置文件来开启一些实验性功能。所以，这里我编写了一个给 kubeadm 用的 YAML 文件（名叫：kubeadm.yaml）：    apiVersion: kubeadm.k8s.io/v1alpha1kind: MasterConfigurationcontrollerManagerExtraArgs:  horizontal-pod-autoscaler-use-rest-clients: "true"  horizontal-pod-autoscaler-sync-period: "10s"  node-monitor-grace-period: "10s"apiServerExtraArgs:  runtime-config: "api/all=true"kubernetesVersion: "stable-1.11"这个配置中，我给 kube-controller-manager 设置了：    horizontal-pod-autoscaler-use-rest-clients: "true"这意味着，将来部署的 kube-controller-manager 能够使用自定义资源（CustomMetrics）进行自动水平扩展。这是我后面文章中会重点介绍的一个内容。其中，"stable-1.11"就是 kubeadm 帮我们部署的 Kubernetes版本号，即：Kubernetes release 1.11 最新的稳定版，在我的环境下，它是v1.11.1。你也可以直接指定这个版本，比如：kubernetesVersion: "v1.11.1"。然后，我们只需要执行一句指令：    $ kubeadm init --config kubeadm.yaml就可以完成 Kubernetes Master的部署了，这个过程只需要几分钟。部署完成后，kubeadm 会生成一行指令：    kubeadm join 10.168.0.2:6443 --token 00bwbx.uvnaa2ewjflwu1ry --discovery-token-ca-cert-hash sha256:00eb62a2a6020f94132e3fe1ab721349bbcd3e9b94da9654cfe15f2985ebd711这个 kubeadm join 命令，就是用来给这个 Master节点添加更多工作节点（Worker）的命令。我们在后面部署 Worker节点的时候马上会用到它，所以找一个地方把这条命令记录下来。此外，kubeadm 还会提示我们第一次使用 Kubernetes 集群所需要的配置命令：    mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config而需要这些配置命令的原因是：Kubernetes集群默认需要加密方式访问。所以，这几条命令，就是将刚刚部署生成的Kubernetes 集群的安全配置文件，保存到当前用户的.kube 目录下，kubectl默认会使用这个目录下的授权信息访问 Kubernetes 集群。如果不这么做的话，我们每次都需要通过 export KUBECONFIG 环境变量告诉kubectl 这个安全配置文件的位置。现在，我们就可以使用 kubectl get 命令来查看当前唯一一个节点的状态了：    $ kubectl get nodes NAME      STATUS     ROLES     AGE       VERSIONmaster    NotReady   master    1d        v1.11.1可以看到，这个 get 指令输出的结果里，Master 节点的状态是NotReady，这是为什么呢？在调试 Kubernetes 集群时，最重要的手段就是用 kubectl describe来查看这个节点（Node）对象的详细信息、状态和事件（Event），我们来试一下：    $ kubectl describe node master ...Conditions:... Ready   False ... KubeletNotReady  runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized通过 kubectl describe 指令的输出，我们可以看到 NodeNotReady的原因在于，我们尚未部署任何网络插件。另外，我们还可以通过 kubectl 检查这个节点上各个系统 Pod的状态，其中，kube-system 是 Kubernetes 项目预留的系统 Pod的工作空间（Namepsace，注意它并不是 Linux Namespace，它只是 Kubernetes划分不同工作空间的单位）：    $ kubectl get pods -n kube-system NAME               READY   STATUS   RESTARTS  AGEcoredns-78fcdf6894-j9s52     0/1    Pending  0     1hcoredns-78fcdf6894-jm4wf     0/1    Pending  0     1hetcd-master           1/1    Running  0     2skube-apiserver-master      1/1    Running  0     1skube-controller-manager-master  0/1    Pending  0     1skube-proxy-xbd47         1/1    NodeLost  0     1hkube-scheduler-master      1/1    Running  0     1s可以看到，CoreDNS、kube-controller-manager 等依赖于网络的 Pod 都处于Pending 状态，即调度失败。这当然是符合预期的：因为这个 Master节点的网络尚未就绪。
## 部署网络插件在 Kubernetes项目"一切皆容器"的设计理念指导下，部署网络插件非常简单，只需要执行一句kubectl apply 指令，以 Weave 为例：    $ kubectl apply -f https://git.io/weave-kube-1.6部署完成后，我们可以通过 kubectl get 重新检查 Pod 的状态：    $ kubectl get pods -n kube-system NAME                             READY     STATUS    RESTARTS   AGEcoredns-78fcdf6894-j9s52         1/1       Running   0          1dcoredns-78fcdf6894-jm4wf         1/1       Running   0          1detcd-master                      1/1       Running   0          9skube-apiserver-master            1/1       Running   0          9skube-controller-manager-master   1/1       Running   0          9skube-proxy-xbd47                 1/1       Running   0          1dkube-scheduler-master            1/1       Running   0          9sweave-net-cmk27                  2/2       Running   0          19s可以看到，所有的系统 Pod 都成功启动了，而刚刚部署的 Weave 网络插件则在kube-system 下面新建了一个名叫 weave-net-cmk27 的 Pod，一般来说，这些Pod 就是容器网络插件在每个节点上的控制组件。Kubernetes 支持容器网络插件，使用的是一个名叫 CNI的通用接口，它也是当前容器网络的事实标准，市面上的所有容器网络开源项目都可以通过CNI 接入 Kubernetes，比如 Flannel、Calico、Canal、Romana等等，它们的部署方式也都是类似的"一键部署"。关于这些开源项目的实现细节和差异，我会在后续的网络部分详细介绍。至此，Kubernetes 的 Master 节点就部署完成了。如果你只需要一个单节点的Kubernetes，现在你就可以使用了。不过，在默认情况下，Kubernetes 的 Master节点是不能运行用户 Pod的，所以还需要额外做一个小操作。在本篇的最后部分，我会介绍到它。
## 部署 Kubernetes 的 Worker 节点Kubernetes 的 Worker 节点跟 Master节点几乎是相同的，它们运行着的都是一个 kubelet 组件。唯一的区别在于，在kubeadm init 的过程中，kubelet 启动后，Master 节点上还会自动运行kube-apiserver、kube-scheduler、kube-controller-manger 这三个系统 Pod。所以，相比之下，部署 Worker 节点反而是最简单的，只需要两步即可完成。第一步，在所有 Worker 节点上执行"安装 kubeadm 和 Docker"一节的所有步骤。第二步，执行部署 Master 节点时生成的 kubeadm join 指令：    $ kubeadm join 10.168.0.2:6443 --token 00bwbx.uvnaa2ewjflwu1ry --discovery-token-ca-cert-hash sha256:00eb62a2a6020f94132e3fe1ab721349bbcd3e9b94da9654cfe15f2985ebd711
## 通过 Taint/Toleration 调整 Master 执行 Pod 的策略我在前面提到过，默认情况下 Master 节点是不允许运行用户 Pod 的。而Kubernetes 做到这一点，依靠的是 Kubernetes 的 Taint/Toleration 机制。它的原理非常简单：一旦某个节点被加上了一个Taint，即被"打上了污点"，那么所有 Pod 就都不能在这个节点上运行，因为Kubernetes 的 Pod 都有"洁癖"。除非，有个别的 Pod 声明自己能"容忍"这个"污点"，即声明了Toleration，它才可以在这个节点上运行。其中，为节点打上"污点"（Taint）的命令是：    $ kubectl taint nodes node1 foo=bar:NoSchedule这时，该 node1 节点上就会增加一个键值对格式的Taint，即：foo=bar:NoSchedule。其中值里面的 NoSchedule，意味着这个 Taint只会在调度新 Pod 时产生作用，而不会影响已经在 node1 上运行的Pod，哪怕它们没有 Toleration。那么 Pod 又如何声明 Toleration 呢？我们只要在 Pod 的.yaml 文件中的 spec 部分，加入 tolerations 字段即可：    apiVersion: v1kind: Pod...spec:  tolerations:  - key: "foo"    operator: "Equal"    value: "bar"    effect: "NoSchedule"这个 Toleration 的含义是，这个 Pod 能"容忍"所有键值对为 foo=bar 的Taint（ operator: "Equal"，"等于"操作）。现在回到我们已经搭建的集群上来。这时，如果你通过 kubectl describe检查一下 Master 节点的 Taint 字段，就会有所发现了：    $ kubectl describe node master Name:               masterRoles:              masterTaints:             node-role.kubernetes.io/master:NoSchedule可以看到，Master节点默认被加上了`node-role.kubernetes.io/master:NoSchedule`这样一个"污点"，其中"键"是`node-role.kubernetes.io/master`，而没有提供"值"。此时，你就需要像下面这样用"Exists"操作符（operator:"Exists"，"存在"即可）来说明，该 Pod 能够容忍所有以 foo 为键的Taint，才能让这个 Pod 运行在该 Master 节点上：    apiVersion: v1kind: Pod...spec:  tolerations:  - key: "foo"    operator: "Exists"    effect: "NoSchedule"当然，如果你就是想要一个单节点的 Kubernetes，删除这个 Taint才是正确的选择：    $ kubectl taint nodes --all node-role.kubernetes.io/master-如上所示，我们在"`node-role.kubernetes.io/master`"这个键后面加上了一个短横线"-"，这个格式就意味着移除所有以"`node-role.kubernetes.io/master`"为键的Taint。到了这一步，一个基本完整的 Kubernetes 集群就部署完毕了。是不是很简单呢？有了 kubeadm 这样的原生管理工具，Kubernetes的部署已经被大大简化。更重要的是，像证书、授权、各个组件的配置等部署中最麻烦的操作，kubeadm都已经帮你完成了。接下来，我们再在这个 Kubernetes 集群上安装一些其他的辅助插件，比如Dashboard 和存储插件。
## 部署 Dashboard 可视化插件在 Kubernetes 社区中，有一个很受欢迎的 Dashboard项目，它可以给用户提供一个可视化的 Web界面来查看当前集群的各种信息。毫不意外，它的部署也相当简单：    $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml部署完成之后，我们就可以查看 Dashboard 对应的 Pod 的状态了：    $ kubectl get pods -n kube-system kubernetes-dashboard-6948bdb78-f67xk   1/1       Running   0          1m需要注意的是，由于 Dashboard 是一个 WebServer，很多人经常会在自己的公有云上无意地暴露 Dashboard的端口，从而造成安全隐患。所以，1.7 版本之后的 Dashboard项目部署完成后，默认只能通过 Proxy的方式在本地访问。具体的操作，你可以查看 Dashboard项目的[官方文档](https://github.com/kubernetes/dashboard)。而如果你想从集群外访问这个 Dashboard 的话，就需要用到Ingress，我会在后面的文章中专门介绍这部分内容。