a release is measured along multiple dimensions, ranging from in-
crease in resource usage at CSP-end to a higher number of failed user
requests. Specifically at Facebook, any irregular increase in the num-
ber of HTTP errors (e.g., 500 code), proxy errors (e.g., timeouts), con-
nection terminations (e.g., TCP RSTs) and QoE degradation (e.g., in-
creasedtaillatency)quantifytheextentofrelease-relateddisruptions.
Next, we discuss the direct and indirect consequences of a release.
• Reduced Cluster Capacity: Intuitively, during a rolling up-
date, servers with old code will stop serving traffic and this will
reduce the cluster’s effective capacity. An unexpected consequence
of reduced capacity is increased contention and higher tail latencies.
To illustrate this point, in Figure 3a, we plot the capacity for an Edge
cluster during a release. From this figure, we observe that during the
update, the cluster is persistently at less than 85% capacity which
corresponds to the rolling update batches which are either 15% or
20% of the total number of machines. Minutes 57 and 80-83 corre-
spond to time gap when one batch finished and the other started.
In a complementary experiment, we analyzed the tail latency and
observed significant increase due to a 10% reduced cluster capacity .
• Increased CPU Utilization: During a server restart, the ap-
plication and protocol states maintained by the server will be lost.
Clients reconnecting to Facebook’s infrastructure will need to rene-
gotiatethisapplicationandprotocolstate(e.g.,TCP,TLSstate).InFig-
ure3b,weplottheCPUutilizationatthe App. Server-tierwhenclients
reconnect. We observed that when 10% of Origin Proxygen restart,
the app. cluster uses 20% of CPU cycles to rebuild state. This overhead
mirrorsobservationsmadebyotheronlineserviceproviders[11,18].
• Disrupted ISP Operations: At the scale of our deployment, a
restart of billions of connections can also put stress on the underlying
ISP networking infrastructure, especially the cellular towers. On
multiple instances, ISPs have explicitly complained about the abrupt
re-connection behavior and the resulting congestion on the last-mile
cellular connections.
• Disrupted end-user quality of experience: Despite efforts
to drain connections, restarts at the Proxygen or App. Server tiers lead
to disruptions for users with long-lived connections (e.g., MQTT)
which outlive the draining period. These disruptions manifest them-
selves in multiple ways ranging from explicit disruption (e.g., HTTP
500 error codes) to degraded user experience (e.g., retries). At our
scale, we observe that at the tail (i.e., p99.9) most requests are suffi-
cientlylargeenoughtooutlivethedrainingperiod.Insuchsituations,
users need to re-upload or restarts session interactions.
The state of the art for performing updates, i.e., draining and
rolling updates, may be suitable for a large number of medium sized
online service providers. However, at our scale, the collateral damage
of applying these techniques extends beyond our infrastructure and
users, to the underlying Internet and cellular fabric.
123410100Number of releases in a week0.00.20.40.60.81.0CDF across 90 daysL7LBAppServerConfig.L7LB codeCause of update release020406080100% times release updated the component100101102103104# of commits in AppServer push0.00.20.40.60.81.0CDF across 90 days02468101214161820Timeline [minutes]051015# UDP pkts misrouted [K]SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
Naseer et al.
(a) Cluster capacity.
Figure (3)
(b) CPU spike.
Impact of release.
3 DESIGN CHOICES
Withthegrowingadoptionofmono-repos[25]andmicro-services[47],
the need for a zero-disruption code release mechanism is increas-
ingly becoming important both at Facebook and at other companies.
In this section, we elaborate on emerging production techniques and
discuss how these technique fail to operate at our scale.
3.1 Ideal Design (PerfectReload)
Abstractly, there are three ways to seamless update code.
• Language Support (Option-1): The first is to use a language
with built-in support for headless updates such as Erlang [13]. Unfor-
tunately, this approach is not supported by most common languages.
In fact, only a trivially small number of our services are written in
such a language. Orthogonally, the effort required to rewrite our
entire fleet of services in such a language is practically unrealistic.
• Kernel Support (Option-2): A more promising alternative is to
leverage the support from operating systems for seamlessly migrat-
ing connections between two processes – from the one running old
code to the one running new code. However, this has two main draw-
backs: First, kernel techniques like SO_REUSEPORT do not ensure
consistent routing of packets. During such migration of a socket,
there is a temporary moment before the old process relinquishes
the control over it when both processes will have control over the
socket and packets arriving at it. While temporary, such ambiguous
moments can have a significant impact at our scale. To highlight this
problem, in Figure 2d, we examine the number of misrouted packets
during a socket handover. These misrouted packets illicit errors code
from the server which will propagate to the end-users. While there
are solutions to enforce consistency, e.g., CMSG, SCM_RIGHTS [39]
and dup() [40],theyarestilllimitedforUDP.Second,thesetechniques
are not scalable owing to the hundreds of thousands of connections
at scale per instance. Additionally, a subset of the socket state e.g.,
TLS, may not be copied across process boundaries because of the
security implications of transferring sensitive data.
Ignoring the scalability and consistency issues, we note that two
key issues remain un-addressed. First, for long lived connections, we
may need to wait for an infinite amount of time for these connections
to organically drain out, which is impractical. Second, application-
specificstatesarenottransferedby SO_REUSEPORT,onlynewsocket
instances are added to the same socket family. Terminating these
long lived connections or ignoring application state will lead to
user facing errors. Thus, we need a general solution for addressing
application state and for explicitly migrating long-lived connections.
• Protocol and Kernel Support (Option-3): We can address scal-
ability issues by only migrating listening sockets across application
processes and allowing the old instance to drain gracefully. For long-
lived connections, we can leverage a third party server such as an
upstream component to help drain and coordinate migrations of ac-
tive connections. For example, during the update of App. Servers, an
App. Servers can signal the Proxygen to migrate connections by termi-
nating active connections and setting up new connections. However,
for this approach to be disruption free, it requires the connection’s
protocol to support graceful shutdown semantics ( for e.g. HTTP/2’s
GoAways) which not supported by a significant subset of our connec-
tions such as HTTP/1.1 or MQTT. Moreover, existing APIs and mech-
anismsinthekernelareinadequatetoachievesuchmigrationofsock-
ets properly without causing disruptions for UDP based applications.
The ideal disruption-free release system is expected to:
(1) Generic: Generalizes across the plethora of services and proto-
cols. As demonstrated with Options-3 and Options-1, specialized
solutions limited to specific languages or protocol do not work
at large providers where multiple protocols and languages need
to be supported.
(2) Preserve State: Should either reserve or externalize states to
prevent the overhead of rebuilding them after restarts.
(3) Tacklelong-livedconnections:Shouldbeabletopreventlong-
lived connection disruptions as draining period is not enough
for them to gracefully conclude.
4 ZERO DOWNTIME RELEASE
In this section, we present the design and implementation details
for framework to achieve Zero Downtime Release. Our framework
extends on option-3 and introduces primitives to orchestrate sched-
uling, state externalization, and design enhancements to enable
pre-existing operating system to scale to our requirements. In partic-
ular, our framework introduces three update mechanisms, and are
composed to address the unique requirements of the different tiers
in the end-to-end infrastructure. Next, we elaborate on each of these
three update/release mechanisms and illustrate how our primitives
are used and in particular how the unique challenges are addressed.
The key idea behind the techniques is to leverage Proxygen, either its
building blocks (e.g., sockets) or its position in end-to-end hierarchy,
to prevent or mask any possible disruption from end-users.
4.1 Socket Takeover
Socket Takeover enables Zero Downtime Restarts for Proxygen by
spinning up an updated instance in parallel that takes-over the lis-
tening sockets, whereas the old instance goes into graceful draining
phase. The new instance assumes the responsibility of serving the
new connections and responding to health-check probes from the
L4LB Katran. Old connections are served by the older instance un-
til the end of draining period, after which other mechanism (e.g.,
Downstream Connection Reuse) kicks in.
Implementation: We work-around the technical limitations dis-
cussed in (§ 4) by passing file descriptors (FDs) for each listening
socket from the older process to the new one so that the listening
sockets for each service addresses are never closed (and hence no
downtime). As we pass an open FD from the old process to the newly
spun one, both the passing and the receiving process share the same
file table entry for the listening socket and handle separate accepted
connections on which they serve connection level transactions. We
leverage the following Linux kernel features to achieve this:
Timeline [minute]Available L7 capacityTimeline [minute]CPU usageZero Downtime Release
SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
Figure (4) Design choices for PerfectReload
(1) CMSG: A feature in sendmsg() [39, 42] allows sending con-
trol messages between local processes (commonly referred to as
ancillary data). During the restart of L7LB processes, we use this
mechanism to send the set of FDs for all active listening sockets
for each VIP (Virtual IP of service) from the active instance to the
newly-spun instance. This data is exchanged using sendmsg(2) and
recvmsg(2) [39, 41, 42] over a UNIX domain socket.
(2) SCM_RIGHTS: We set this option to send open FDs with the
data portion containing an integer array of the open FDs. On the
receiving side, these FDs behave as though they have been created
with dup(2) [40].
Support for UDP protocol: Transferring a listening socket’s
FDs to a new process is a well-known technique [35, 36, 38, 43]
has been added to other proxies like (HAProxy [3]) in 2018 [4] and
more recently to Envoy [16]. Although the motivation is same, Prox-
ygen’s Socket Takeover is more comprehensive as it supports and
addressespotentialdisruptionsformultipletransportprotocols(TCP
and UDP).
Although UDP is connection-less, many applications and proto-
cols built on top of UDP (QUIC, webRTC etc.) do maintain states
for each flow. For TCP connections, the separation of listening and
accepted sockets by Linux Kernel lessens the burden to maintain
consistency from the user-space application. On the other hand, UDP
has no such in-built support. Typically, a consistent routing of UDP
packets to a socket is achieved via application of hashing function
on the source and the destination address for each packet. When
SO_REUSEPORT socket option is used for an UDP address (VIP),
Kernel’s internal representation of the socket ring associated with
respective UDP VIP is in flux during a release – new process binds to
same address and new entries are added to socket ring, while the old
process shutdowns and gets its entries purged from the socket ring.
This flux breaks the consistency in picking up a socket for the same
4-tuple combination. This significantly increases the likelihood of
UDP packets being misrouted to a different process that does not
have state for that flow (Figure 2d).
Owing to mis-routing of UDP packets, a typical SO_REUSEPORT-
lessarchitectureusesathreaddedicatedtoacceptingnewconnection
that hands off newly accepted connections to worker threads. UDP
being datagram centric and without any notion of packet type such
as SYN for TCP, the kernel cannot discriminate between a packet
for a new connection vs an existing one. The approach of using
one thread to accept all the packet cannot scale for high loads since
the responsibilities of maintaining states and multiplexing of UDP
packets must now happen in the application layer, and such thread
becomes a bottleneck.
Figure (5) Socket Takeover
To circumvent the scaling issues, we use SO_REUSEPORT option
with multiple server threads accepting and processing the packets in-
dependently. To solve the mis-routing issue, we extended the Socket
Takeover to pass FDs for all UDP VIP sockets. This essentially is
equivalent of calling dup() [40] on an existing socket FD and thus
avoids creation of a new FD for this socket. In other words, the socket
ring for this VIP within kernel remains unchanged with each process
restart. The newly spun process can thus resume processing packets
from where the old process left off.
However, one problem still remains un-addressed. All the packets
arenowroutedtothenewprocess,includingtheonesforconnections
owned by the old (existing) process. For applications that require and
maintainstates foreach UDPbased flow (e.g., QUIC),the newprocess
employs user-space routing and forwards packets to the old process
through a pre-configured host local addresses. Decisions for user-
space routing of packets are be made based on information present
in each UDP packet, such as connection ID [33] that is present in each
QUIC packet header. In our implementation this mechanism effec-
tively eliminated all the cases of mis-routing of UDP packets while
still being able to leverage multiple threads for better scalability.
Workflow: (Figure 5) An existing Proxygen process that is serv-
ing traffic has already bound and called accept() on socket FDs per
VIP. In step A○, the old Proxygen instance spawns a Socket Takeover
server that is bound to a pre-specified path and the new Proxygen pro-
cess starts and connects to it. In step B○, the Socket Takeover server
then sends the list of FDs it has bound, to the new process via the
UNIX domain socket with sendmsg() and CMSG. The new Proxygen
process listens to the VIP corresponding to the FDs (step C○). It then
sends confirmation to the old server to start draining the existing
connections (step D○). Upon receiving the confirmation from the new
process, the old process stops handling new connections and starts
draining existing connections (step E○). In step F○, the new instance
takes over the responsibility of responding to health-check probes
from the L4LB layer (Katran). Step G○ stays active until the end of
draining period and UDP packets belonging to the older process are
routed in user-space to the respective process.
View from L4 as L7 restarts: An L4LB (Katran) that sits in front
of the L7 proxy and continuously health-checks (HC) each L7LB is
agnostic of updates in L7LB servers because of the zero downtime
restart mechanism. We can thus isolate the L7 restarts only to that
layer. This not only simplifies our regular release process but also
help in reliability of the overall system since the blast radius of a
buggy release is largely confined to one layer where mitigation (or
rollbacks) can be applied swiftly.
 MachineOption-2: Updated instance takes over listening and accepted sockets,terminate old instance.Option-1: Reload binary/conﬁg.without restarting.Option-3: Takes-over listeningsockets, drains old instance, gracefullyshutdown conn. at end of draining.ListeningsocketsAcceptedsocketsDownstreamUpstream21 Graceful shutdowns 33Old ProxyInstanceNew ProxyInstanceListeningSocketListeningSocketListeningSocketListeningSocketProxy (new)Proxy (old)Socket FDsL4LBTakeoverServerBABDFFECNew conn.         Control msg.         Health-checks.         User-space pkt. fwd.         GSIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
Naseer et al.
Figure (6) Downstream Connection Reuse
Connections between Edge and Origin: Proxygen in Edge and
Origin maintain long-lived HTTP/2 connections (§ 2) between them.
Leveraging GOAWAY, they are gracefully terminated over the drain-
ing period and the two establish new connections to tunnel user
connections and requests without end-user disruption.
Caveats: Socket Takeover process is not free-of-cost and incurs
CPU and memory overhead, as the two Proxygen instances run par-
allel on the same machine. Although the machine stays available to
serve new connections, there’s (often insignificant) diminished CPU
capacity for the draining duration (§ 6.3).
4.2 Downstream Connection Reuse
As described earlier, MQTT is used to keep persistent connections
with billions of users and the protocol requires underlying trans-
port session to be always available (e.g., for live notifications). As a
result, MQTT clients periodically exchange ping and initiate new
connections as soon as transport layer sessions are broken. MQTT
does not have a built-in disruption avoidance support in case of