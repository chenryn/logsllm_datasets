the  ones  responsible  for  the  overhead  in  the  filtering
scenario: redirecting inputs to and from the filtering script
(corresponding  to  steps  2  and  3  in  Figure  2)  plus  actual
script interpretation time.
Since  both  the  filtering  script  and  the  intercepting
module  reside  physically  on  the  same  machine  acting  as
the web server, the communication time between the two
is  comparable  to  the  time  required  to  perform  a  local
procedure  call.  On  modern  commodity  machines  this
amounts to a few dozen instructions, or an execution time
in the range of 50-100 ns.
We experimented with a filtering script generated from
21 assumptions on 8 parameters, and designed to validate
our sample HTTP request. Our measurements, performed
on  an  800MHz  Pentium  III  with  256MB  RAM  running
under Windows 2000 (with all other applications closed),
showed script interpretation times ranging from 1.10 ms to
1.14 ms, with both the mean and median at 1.11 ms.
Based on these numbers, the total overhead of running
our  filter  on  inputs  with  roughly  the  complexity  of  the
sample HTTP request is on average 1.11 ms per validated
input.  Included  in  that  calculation  are  the  average  script
interpretation time and two local procedure calls (from the
intercepting module to the filtering script and back). Since
scripts are small in size and do not involve disk accesses,
script  interpretation  is  CPU-bound.  It  is  also  the  major
overhead  contributor  when  input  filtering  is  performed,
resulting in an upper limit of 9.0 requests per second per
percent of allocated CPU power. Hence, if on average 200
requests  per  second  hit  a  dedicated  web  server  machine
with  the  above  characteristics,  these  requests  can  be
validated by allocating 22% of the CPU cycles to running
filtering scripts. Conversely, if 50% of the CPU power can
be dedicated to filtering, that translates to servicing up to
450  requests  per  second.  Increasing  the  sophistication  of
the filter (say, by doubling the number of parameters and
assumptions)  but  also  switching  to  a  high-end  server
machine is unlikely to change that estimate by much. (We
should  note  that  the  composition  of  relations  in  a  filter
does matter: EQ, the “cheapest” relation, is interpreted in
0.8µs  on  average,  while  CONSISTS,  the  most  complex
one, takes on average 106µs for reasonably sized inputs –
a factor of over 100 difference in performance. Also, a test
filtering  script  with  a  single  parameter  showed  execution
times  between  9.6  and  21.4  times  faster  –  depending  on
the  composition  of  relations  –  than  those  of  the  filtering
script  with 8 parameters  generated  for  our  sample  HTTP
request.  This  supports  our  hypothesis  of  a  roughly  linear
correspondence  between  the  number  of  parameters  in  a
filter  and  its  execution  time.)  Note  that  had  our  filtering
scripts  been  compiled  rather  than  interpreted,  input
Table 3. Control flow comparison between architectures with and without input filtering
Web server
Filtering script
Back-end application
With
input filtering
Receives input
Forwards input to filtering script
Without
input filtering
Receives result from filtering script
Forwards valid input to back-end
application
Receives input
Forwards input to back-end
application
Receives input
Interprets filtering script on input
Sends result back to web server
N/A
Receives input, if valid
Receives input in all cases
Two operations on the same line signify the passing of a “message” from one component to another (e.g., the web server forwards the
input – the “message” – to the filtering script and the filtering script receives it).
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:06:13 UTC from IEEE Xplore.  Restrictions apply. 
validation would have been much faster, and performance
overhead thus greatly reduced.
To  make  a  real-world  comparison,  high-volume
commercial  web  servers  were  reported  [13]  in  the  last
months  of  2000  to  receive  an  average  of  over  50000
requests  per  second.  Such  server  farms  make  use  of
dozens of dedicated front-end web  server  machines,  load
balancing  requests  to  the  back-end  machines.  Hence,  the
rate of incoming requests each single front-end web server
sees  is  not  very  different  from  the  sustainable  rate  of
servicing requests in our architecture where input filtering
is continually performed.
Since  front-end  web  server  machines  are  typically
CPU-bound  (whether  they  are  serving  static  or  dynamic
content),  diverting  a  portion  of  the  server’s  CPU  power
(say, to perform input filtering) would necessarily have an
impact on the overall performance of the server. However,
given  the  above  performance  data,  the  impact  would  be
insignificant for web servers experiencing moderate loads.
6. Conclusions and Future Work
the  error-prone  and 
The  main  contribution  of  this  work  is  the  automation
of  the  filter  generation  process.  Administrators  still  have
to write formal specifications of what the filters should do,
but 
time-consuming  step  of
implementing the actual filtering scripts is now automated
given  such  formal  descriptions.  The  latter  can  be  written
in  a  simple  language  and,  if  appropriate  and  desired,
graphical tools can be employed to make the process even
more transparent and intuitive.
through 
to  security  breaches 
One important application of our work is in providing
express  response 
the
dissemination of filtering scripts while traditional security
patches  are  still  under  development.  This  can  save  time,
money  and  reputation  for  the  vendor  of  the  application
under attack. In many cases, where a vulnerability is due
to  lack  of  simple  input  validation  –  and  these  have
accounted  for  a  large  fraction  of  recent  breaches  [9,  22,
24] – it should literally take a security expert minutes (as
our informal experience suggests) to identify the right set
of  parameters  and  assumptions  on  them  in  order  to
generate  the  necessary  filtering  script.  Furthermore,  this
can  be  accomplished  without  knowing  the  details  of  the
application’s  source  code  (unlike  [11])  and  even  without
having  access  to  it  (which  is  most  often  the  case).
Consequently,  filters  can  be  quickly  created  (using,  for
instance,  a  non-source-based  filter-generating  tool)  and
disseminated  by  trusted  third-party  security  monitoring
vendors, rather than solely by the application’s vendor, as
is  the  case  with  patches.  Thus,  future  attempts  to  exploit
the  same  vulnerability  will  be  fended  off  reliably.  In
comparison,  it  may  take  weeks  and  even  months  (of
analyzing, developing and testing) before a reliable patch
to  a  widely  deployed  commercial  software  product  is
ready  for  distribution.  In  the  case  of  legacy  software,  no
longer supported by its original vendor, patches will likely
never  come  out;  however,  filters  would  be  easy  and
inexpensive to generate and distribute.
Our measurements indicate that performance overhead
should  not  be  a  deterrent  against  using  filters  to  validate
application inputs, except perhaps in highly performance-
critical settings.
To  further  assess  how  intuitive  our  prototype  is,  we
plan 
to  conduct  user  studies  on  a  set  of  known
vulnerabilities  taken  from  public  archives  [24,  22,  9].
After explaining our approach and providing the necessary
set  of  relations  (Table 1),  users  will  be  asked  to  identify
for  each  vulnerability 
the  set  of  parameters  and
assumptions they would choose to have a filter enforce, if
they were to assume the task of administrators.
It is possible that a solution similar to this one could be
adopted  for  automatically  generating  firewall  filters.
Generalized  firewall  configuration  languages  have  been
proposed  [3];  a  logical  further  step  might  be  to  explore
automating the generation of configuration scripts in such
a  language  using  GUI-based  tools  to  formally  describe
simple firewall policies. The idea might also be put to use
encapsulating  existing  API  function 
in  a
simplified  version  of  the  basic  approach  of  [14].  A
specially  designed  wrapper  filter  would  intercept  calls  to
its library routines, do the necessary validation checks and
only “forward” the calls to the corresponding routines if it
is  safe  to  do  so.  Whenever  a  vulnerability  in  a  library
routine is discovered, an appropriate wrapper  filter  could
be  generated  and  applied  as  a  stopgap  until  an  OS  or
application patch is later released.
libraries, 
In fact, application programmers could perhaps specify
at development time the assumptions  made by their code
about  its  inputs,  and  generate  the  appropriate  filters
themselves.  (In  some  cases  it  might  even  be  possible  to
generate  the  descriptions  directly  from  source  code.)
Later, an administrator could decide, based on the hostility
of the environment, the sensitivity of the application, and
the performance constraints on the system, whether a filter
should be installed to improve security or omitted to avoid
affecting performance.
Acknowledgements
The authors  would like to thank Jon Pincus  and  John
Zahorjan for their  thoughtful  comments  at  various  stages
of  this  work.  Paul  England  assisted  us  in  dealing  with  a
few  implementation  details.  Mariusz  Jakubowski  helped
configure  our  filters  to  work  on  a  real  server.  A  special
thanks  goes  to  Stani  Vlasseva  for  proof-reading  multiple
versions  of 
this  paper  and  suggesting  numerous
improvements. The anonymous reviewers helped improve
the final draft of this paper.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:06:13 UTC from IEEE Xplore.  Restrictions apply. 
References
[1]  H.  Browne,  W.  Arbaugh,  J.  McHugh,  W.  Fithen,  “A
IEEE
Trend  Analysis  of  Exploitations”, 
Symposium on Security  and Privacy,  Oakland,  CA,  May  2001,
pp.214-229.
In  Proc.  22nd 
[2] T. Fraser, L. Badger, and M. Feldman, “Hardening COTS
Software with Generic Software Wrappers”, In Proc. 20th IEEE
Symposium on Security  and Privacy,  Oakland,  CA,  May  1999,
pp.2-16.
[3] Y.  Bartal,  A.  Mayer,  K.  Nissim,  A.  Wool,  “Firmato:  A
Novel  Firewall  Management  Toolkit”,  In  Proc.  20th  IEEE
Symposium on Security  and Privacy,  Oakland,  CA,  May  1999,
pp.17-31.
[4]  “Malicious  HTML  Tags  Embedded  in  Client  Web
Requests” and “Understanding Malicious Content Mitigation for
Web Developers”, CERT Coordination Center, Feb. 2000,
http://www.cert.org/advisories/CA-2000-02.html,
http://www.cert.org/tech_tips/malicious_code_mitigation.html,
http://www.cert.org/tech_tips/malicious_code_FAQ.html
[5]  “Automatic  Execution  of  Embedded  MIME  Types”,
CERT Coordination Center, Apr. 2001,
http://www.cert.org/advisories/CA-2001-06.html
[6]  “Denial-of-Service  Vulnerabilities  in  TCP  Stacks”  and
“Multiple  Denial-of-Service  Problems  in  ISC  BIND”,  CERT
Coordination Center, Nov. 2000,
http://www.cert.org/advisories/CA-2000-21.html,
http://www.cert.org/advisories/CA-2000-20.html
[7]  “Multiple  Vulnerabilities  in  Alcatel  ADSL  Modems”,
“Interbase  Server  Contains  Compiled-in  Back  Door  Account”,
“Input  Validation  Problems  in  LPRng”  and  “Microsoft  ‘IE
Script’/Access/OBJECT 
Tag 
CERT
Coordination Center, 2000-2001,
http://www.cert.org/advisories/CA-2001-08.html,
http://www.cert.org/advisories/CA-2001-01.html,
http://www.cert.org/advisories/CA-2000-22.html,
http://www.cert.org/advisories/CA-2000-16.html
Vulnerability”, 
[8] “Superfluous Decoding Vulnerability in IIS”, “Statistical
Weakness in TCP/IP Initial Sequence Numbers” and “PGP May
Encrypt  Data  with  Unauthorized  ADKs”,  CERT  Coordination
Center, 2000-2001,
http://www.cert.org/advisories/CA-2001-12.html,
http://www.cert.org/advisories/CA-2001-09.html,
http://www.cert.org/advisories/CA-2000-18.html
[9] “CERT/CC Advisories 1988-2001”,
http://www.cert.org/advisories/ ; Relevant exploits in:
“Two  Input  Validation  Problems 
in  FTPD”  and  “Input
Validation  Problem  in  rpc.statd”,  CERT  Coordination  Center,
Jul-Aug. 2000,
http://www.cert.org/advisories/CA-2000-13.html,
http://www.cert.org/advisories/CA-2000-17.html
[10] P. Chowdhry,  “Attacked  and  Hacked”  and  “The
Gibraltar  Hack:  Anatomy  of  a  Break-In”,  PC  Week  Labs,  Oct.
1999,
http://www.zdnet.com/eweek/stories/general/0,11011,2350743,0
0.html
[11] D. Engler, B. Chelf, A. Chou, and S. Hallem, “Checking
System  Rules  Using  System-Specific,  Programmer-Written
Compiler  Extensions”,  In  Proc.  4th  OSDI  Symposium,  San
Diego, CA, Oct. 2000.
[12]  A.  Ghosh,  T.  O'Connor,  G.  McGraw,  “An  Automated
Approach for Identifying Potential Vulnerabilities in Software”,
In  Proc.  19th  IEEE  Symposium  on  Security  and  Privacy,
Oakland, CA, May 1998, pp.104-114.
[13]  S.  Gribble,  “A  Design  Framework  and  a  Scalable
Storage  Platform  to  Simplify  Internet  Service  Construction”,
Ph.D. thesis, UC Berkeley, Sep. 2000, p.6.
[14]  I.  Goldberg,  D.  Wagner,  R.  Thomas,  E.  Brewer,  “A
Secure  Environment 
for  Untrusted  Helper  Applications:
Confining  the  Wily  Hacker”,  In  Proc.  6th  USENIX  Security
Symposium, San Jose, CA, July 1996, pp.1-13.
[15]  S.  Hillier,  “Inside  Microsoft  Visual  Basic,  Scripting
Edition”, Microsoft Press, Nov. 1996.
[16] P. Herrmann, H. Krumm, “Trust-Adapted Enforcement
of  Security  Policies 
in  Distributed  Component-Structured
Applications”, In Proc. 6th IEEE Symposium on Computers and
Communications, Hammamet, Tunesia, July 2001, pp.2-8.
[17] P. Iglio, F. Fraticelli, L. Giuri, “Rule-Based Filtering for
Java Applets”, In Proceedings of 14th Annual Computer Security
Applications Conference, Phoenix, AZ, Dec. 1998, pp.112-119.
[18] B.  Lampson,  “Hints  for  Computer  System  Design”,  In
Proceedings  of  9th  ACM  Symposium  on  Operating  Systems
Principles, vol.17 (5), Bretton Woods, NH, Oct. 1983, pp.33-48.
[19]  M.  Monroe,  “Security  Tool  Review:  TCP  Wrappers”,
;login:, 18(6) 1993.
[20] F.  Schneider,  “Towards  Fault-Tolerant  and  Secure
Agentry”,  In  Proc.  11th  International  Workshop  on  Distributed
Algorithms,  LNCS  1320,  ACM-SIGPLAN,  Springer-Verlag,
Saarbruecken, Germany, Sept. 1997, pp.1-14.
[21] J. Smith, S. Doherty, O. Leahy, D. Tynan, “Protecting A
Private  Network:  The  AltaVista  Firewall”,  Digital  Technical
Journal, Vol.9, No.2, Oct. 1997, pp.15-32.
[22] Security Focus, http://www.securityfocus.org/
[23] V. Swarup,  “Automatic  Generation  of  High  Assurance
Security  Guard  Filters”,  National  Computer  Security
Conference, Washington D.C., Oct. 1994.
[24]  Denial  of  Service  Attacks  and  Buffer  Overrun
Vulnerabilities: 
in  “Microsoft  System  Monitor  ActiveX
Exploitable Unicode Buffer Overflow Vulnerability”, “Ultraseek
3.x Remote DoS Vulnerability”, http://www.ussrback.com/
[25]  D.  Wagner,  J.  Foster,  E.  Brewer,  A.  Aiken,  “A  First
Step  Towards  Automated  Detection  of  Buffer  Overflow
Vulnerabilities”,  In  Proc.  Network  and  Distributed  System
Security Symposium, San Diego, CA, Feb. 2000, pp.3-17.
[26]  “Extensible  Markup  Language  (XML)  1.0  (Second
Edition)”, Oct. 2000, http://www.w3c.org/TR/REC-xml
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:06:13 UTC from IEEE Xplore.  Restrictions apply.