### Overhead in the Filtering Scenario

The primary sources of overhead in the filtering scenario are the redirection of inputs to and from the filtering script (corresponding to steps 2 and 3 in Figure 2) and the actual script interpretation time. Since both the filtering script and the intercepting module reside on the same machine acting as the web server, the communication time between them is comparable to the time required for a local procedure call. On modern commodity machines, this amounts to a few dozen instructions, or an execution time in the range of 50-100 nanoseconds.

### Experimental Results

We conducted experiments with a filtering script generated from 21 assumptions on 8 parameters, designed to validate a sample HTTP request. The measurements were performed on an 800 MHz Pentium III with 256 MB RAM running Windows 2000, with all other applications closed. The script interpretation times ranged from 1.10 ms to 1.14 ms, with both the mean and median at 1.11 ms.

Based on these numbers, the total overhead of running our filter on inputs with the complexity of the sample HTTP request is, on average, 1.11 ms per validated input. This calculation includes the average script interpretation time and two local procedure calls (from the intercepting module to the filtering script and back). Since scripts are small and do not involve disk accesses, script interpretation is CPU-bound and is the major contributor to the overhead when input filtering is performed. This results in an upper limit of 9.0 requests per second per percent of allocated CPU power.

### Performance Implications

If, on average, 200 requests per second hit a dedicated web server machine with the above characteristics, these requests can be validated by allocating 22% of the CPU cycles to running filtering scripts. Conversely, if 50% of the CPU power can be dedicated to filtering, that translates to servicing up to 450 requests per second. Increasing the sophistication of the filter (e.g., by doubling the number of parameters and assumptions) and switching to a high-end server machine is unlikely to change this estimate significantly. 

It is worth noting that the composition of relations in a filter matters: EQ, the "cheapest" relation, is interpreted in an average of 0.8 microseconds, while CONSISTS, the most complex one, takes an average of 106 microseconds for reasonably sized inputs—a performance difference of over 100 times. A test filtering script with a single parameter showed execution times between 9.6 and 21.4 times faster than those of the filtering script with 8 parameters, supporting our hypothesis of a roughly linear correspondence between the number of parameters in a filter and its execution time.

### Comparison with Compiled Scripts

If our filtering scripts were compiled rather than interpreted, input validation would be much faster, and the performance overhead would be greatly reduced.

### Real-World Comparison

High-volume commercial web servers were reported [13] in the last months of 2000 to receive an average of over 50,000 requests per second. Such server farms use dozens of dedicated front-end web server machines, load balancing requests to the back-end machines. Therefore, the rate of incoming requests each single front-end web server sees is not very different from the sustainable rate of servicing requests in our architecture where input filtering is continually performed.

Since front-end web server machines are typically CPU-bound (whether they are serving static or dynamic content), diverting a portion of the server's CPU power (e.g., to perform input filtering) would necessarily have an impact on the overall performance of the server. However, given the above performance data, the impact would be insignificant for web servers experiencing moderate loads.

### Conclusions and Future Work

The main contribution of this work is the automation of the filter generation process. Administrators still need to write formal specifications of what the filters should do, but the time-consuming step of implementing the actual filtering scripts is now automated given such formal descriptions. These descriptions can be written in a simple language, and graphical tools can be employed to make the process even more transparent and intuitive.

One important application of our work is in providing a rapid response to security breaches by disseminating filtering scripts while traditional security patches are still under development. This can save time, money, and reputation for the vendor of the application under attack. In many cases, where a vulnerability is due to a lack of simple input validation, it should take a security expert only minutes to identify the right set of parameters and assumptions to generate the necessary filtering script. Furthermore, this can be accomplished without knowing the details of the application’s source code and even without having access to it.

Consequently, filters can be quickly created (using, for instance, a non-source-based filter-generating tool) and disseminated by trusted third-party security monitoring vendors, rather than solely by the application’s vendor. Thus, future attempts to exploit the same vulnerability will be reliably fended off. In comparison, it may take weeks or even months before a reliable patch to a widely deployed commercial software product is ready for distribution. In the case of legacy software, no longer supported by its original vendor, patches will likely never come out; however, filters would be easy and inexpensive to generate and distribute.

Our measurements indicate that performance overhead should not be a deterrent against using filters to validate application inputs, except perhaps in highly performance-critical settings.

To further assess the intuitiveness of our prototype, we plan to conduct user studies on a set of known vulnerabilities taken from public archives. After explaining our approach and providing the necessary set of relations (Table 1), users will be asked to identify the set of parameters and assumptions they would choose to enforce a filter, if they were to assume the task of administrators.

It is possible that a solution similar to this one could be adopted for automatically generating firewall filters. Generalized firewall configuration languages have been proposed [3]; a logical further step might be to explore automating the generation of configuration scripts in such a language using GUI-based tools to formally describe simple firewall policies. The idea might also be put to use encapsulating existing API functions in a simplified version of the basic approach of [14]. A specially designed wrapper filter would intercept calls to its library routines, do the necessary validation checks, and only forward the calls to the corresponding routines if it is safe to do so. Whenever a vulnerability in a library routine is discovered, an appropriate wrapper filter could be generated and applied as a stopgap until an OS or application patch is later released.

In fact, application programmers could specify at development time the assumptions made by their code about its inputs and generate the appropriate filters themselves. (In some cases, it might even be possible to generate the descriptions directly from source code.) Later, an administrator could decide, based on the hostility of the environment, the sensitivity of the application, and the performance constraints on the system, whether a filter should be installed to improve security or omitted to avoid affecting performance.

### Acknowledgements

The authors would like to thank Jon Pincus and John Zahorjan for their thoughtful comments at various stages of this work. Paul England assisted us in dealing with a few implementation details. Mariusz Jakubowski helped configure our filters to work on a real server. Special thanks go to Stani Vlasseva for proof-reading multiple versions of this paper and suggesting numerous improvements. The anonymous reviewers helped improve the final draft of this paper.

### References

[References listed as provided, with proper formatting and citation style.]

---

This revised text aims to be more clear, coherent, and professional, with improved structure and readability.