---
title: Program versioning
date: 20211215
author: Lyz
---
The Don't Repeat Yourself principle encourages developers to abstract code into
a separate components and reuse them rather than write it over and over again.
If this happens across the system, the best practice is to put it inside
a package that lives on its own (a library) and then pull it in from the
applications when required.
!!! note "This article is **heavily** based on the posts in the
[references](#references) sections, the main credit goes to them, I've just
refactored all together under my personal opinion."
As most of us can’t think of every feature that the library might offer or what
bugs it might contain, these packages tend to evolve. Therefore, we need some
mechanism to encode these evolutions of the library, so that downstream users
can understand how big the change is. Most commonly, developers use three
methods:
* A version number.
* [A changelog](changelog.md).
* The git history.
The version change is used as a concise way for the project to communicate this
evolution, and it's what we're going to analyze in this article. However,
encoding all the information of a change into a number switch [has proven to be
far from perfect](semantic_versioning.md#semantic-versioning-system-problems).
That's why keeping a [good and detailed changelog](changelog.md) makes a lot of
sense, as it will better transmit that intent, and what will be the impact of
upgrading. Once again, this falls into the same problem as before, while a change
log is more descriptive, it still only tells you what changes (or breakages)
a project intended to make, it doesn’t go into any detail about unintended
consequences of changes made. Ultimately, a change log’s accuracy is no
different than that of the version itself, it's just (hopefully!) more
detailed.
Fundamentally, any indicator of change that isn’t a full diff is just a lossy
encoding of that change. You can't expect though to read all the diffs of the
libraries that you use, that's why version numbers and changelogs make a lot of
sense. We just need to be aware of the limits of each system.
That being said, you'll use version numbers in two ways:
* As a producer of applications and libraries where you’ll have to decide what
    versioning system to use.
* As a consumer of dependencies, you’ll have to express what versions of a given library your
    application/library is compatible.
# Deciding what version system to use for your programs
The two most popular versioning systems are:
* [Semantic Versioning](semantic_versioning.md): A way to define your program's
    version based on the type of changes you've introduced.
* [Calendar Versioning](calendar_versioning.md): A versioning convention based
    on your project's release calendar, instead of arbitrary numbers.
Each has it's advantages and disadvantages. From a consumer perspective, I think
that projects should generally default to SemVer-ish, following the spirit of
the documentation rather than the letter of the specification because:
* Your version number becomes a means of communicating your changes *intents* to
    your end users.
* If you use the semantic versioning [commit message
    guidelines](semantic_versioning.md#commit-message-guidelines), you are more
    likely to have a useful git history and can automatically maintain the
    project's [changelog](changelog.md).
There are however, corner cases where [CalVer](calendar_versioning.md) makes
more sense:
* You’re tracking something that is already versioned using dates or for which
    the version number can only really be described as a point in time release.
    The `pytz` is a good example of both of these cases, the Olson TZ database is
    versioned using a date based scheme and the information that it is providing
    is best represented as a snapshot of the state of what timezones were like
    at a particular point in time.
* Your project is going to be breaking compatibility in every release and you do
    not want to make any promises of compatibility. You should still document
    this fact in your README, but if there’s no promise of compatibility between
    releases, then there’s no information to be communicated in the version
    number.
* Your project is never going to intentionally breaking compatibility in
    a release, and you strive to always maintain compatibility. Projects can
    always just use the latest version of your software. Your changes will
    only ever be additive, and if you need to change your API, you’ll do
    something like leave the old API intact, and add a new API with the new
    semantics. An example of this case would be the Ubuntu versions.
## How to evolve your code version
Assuming you're using Semantic Versioning you can improve your code evolution
by:
* [Avoid becoming a ZeroVer package](#avoid-becoming-a-zerover-package).
* [Use Warnings to avoid major changes](#use-warnings-to-avoid-major-changes)
### Avoid becoming a ZeroVer package
Once your project reach it's first level of maturity you should release `1.0.0`
to avoid falling into [ZeroVer](semantic_versioning.md#using-zerover). For
example you can use one of the next indicators:
* If you're frequently using it and haven't done any breaking change in 3 months.
* If 30 users are depending on it. For example counting the project stars.
### [Use Warnings to avoid major changes](use_warnings.md)
Semantic versioning uses the major version to defend against breaking changes,
and at the same offers maintainers the freedom to evolve the library without
breaking users. Nevertheless, this does [not seem to work that
well](semantic_versioning.md#semantic-versioning-system-problems).
So it's better to [use Warnings to avoid major changes](use_warnings.md).
### Communicate with your users
You should warn your users not to blindly trust that any version change is not
going to break their code and that you assume that they are [actively testing
the package updates](#automatically-upgrade-and-test-your-dependencies).
### Keep the `Requires-Python` metadata updated
It's important not to [upper cap the Python
version](#pinning-the-python-version-is-special) and to maintain the
`Requires-Python` package metadata updated. [Dependency
solvers](python_package_management.md#solver) will use this information to fetch
the correct versions of the packages for the users.
# Deciding how to manage the versions of your dependencies
As a consumer of other dependencies, you need to specify in your package what
versions does your code support. The traditional way to do it is by pinning
those versions in your package definition. For example in python it lives either
in the `setup.py` or in the `pyproject.toml`.
## Lower version pinning
When you're developing a program that uses a dependency, you usually don't know if
a previous version of that dependency is compatible with your code, so in theory
it makes sense to specify that you don't support any version smaller than the
actual with something like `>=1.2`. If you follow this train of thought, each
time you update your dependencies, you should update your lower pins, because
you're only running your test suite on those versions. If the libraries didn't
do [upper version pinning](#it-conflicts-with-tight-lower-bounds), then there would be no
problem as you wouldn't be risking to get into [version
conflicts](#version-conflicts).
A more relaxed approach would be not to update the pins when you update, in that
case, you should run your tests both against the oldest possible values and the
newest to ensure that everything works as expected. This way you'll be more kind
to your users as you'll reduce possible version conflicts, but it'll add work to
the maintainers.
The most relaxed approach would be not to use pins at all, it will suppress most
of the version conflicts but you won't be sure that the dependencies that your
users are using are compatible with your code.
Think about how much work you want to invest in maintaining your package and how
much stability you want to offer before you choose one or the other method. Once
you've made your choice, it would be nice if you communicate it to your users
through your documentation.
## Upper version pinning
Program maintainers often rely on upper version pinning to guarantee that their
code is not going to be broken due to a dependency update.
We’ll cover the [valid use cases for
capping](#when-is-it-ok-to-set-an-upper-limit) after this section. But, just to
be clear, if you know you do not support a new release of a library, then
absolutely, go ahead and cap it as soon as you know this to be true. If
something does not work, you should cap (or maybe restrict a single version if
the upstream library has a temporary bug rather than a design direction that’s
causing the failure). You should also do as much as you can to quickly remove
the cap, as all the downsides of capping in the next sections still apply.
The following will assume you are capping before knowing that something does not
work, but just out of general principle, like Poetry recommends and defaults to
with `poetry add`. In most cases, the answer will be *don’t*. For simplicity,
I will also assume you are being tempted to cap to major releases (`^1.0.0` in
Poetry or `~=1.0` in all other tooling that follows Python standards via PEP
440) following the false security that only `major` changes can to break your
code. If you cap to minor versions `~=1.0.0`, this is much worse, and the
arguments below apply even more strongly.
#### [Version limits break code too](https://iscinumpy.dev/post/bound-version-constraints/#version-limits-break-code-too)
Following this path will effectively *opt you out of bug fixes and security
updates*, as most of the projects only maintain the [latest version of their
program](semantic_versioning.md#maintaining-different-versions) and what worse,
you'll be *preventing everyone using your library not to use the latest version
of those libraries*. All in exchange to defend yourself against a change that in
practice will rarely impact you. Sure, you can move on to the next version of
each of your pins each time they increase a major via something like `Click>=8,
5`, your update can’t be installed with another library still on `4.11`. So
you have to support `tenacity>=4.11,<6` for a while until most libraries have
similarly updated.
And for those who might think this doesn’t happen often, let me say that
`tenacity` released another major version a year later in November 2019. Thus, the
cycle starts all over again. In both cases, your code most likely did not need
to change at all, as just a small part of their public API changed. In my
experience, this happens **a lot more often** than when a major version bump
breaks you. I've found myself investing most of my project maintenance time
opening issues in third party dependencies to update their pins.
#### [It doesn’t scale](https://iscinumpy.dev/post/bound-version-constraints/#it-doesnt-scale)