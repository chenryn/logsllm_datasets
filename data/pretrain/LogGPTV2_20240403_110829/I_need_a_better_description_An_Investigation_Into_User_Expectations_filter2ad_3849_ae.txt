Summary of Findings. Our surveys indicate that (RQ1) users care
about the kind of information disclosures against which DP can
protect, and (RQ2) users‚Äô willingness to share information is signif-
icantly related to the degree of risk of most information disclosures
occurring. However, the risk of disclosures occurring through the
two information flows that might seem most appropriate [51] to
users given our scenarios related to a salary transparency initiative
and a medical research initiative ‚Äî disclosures through graphs or
to a data analyst ‚Äî did not significantly relate to users‚Äô willingness
ControlTechnicalRiskTrustEnablesTechniquesUnsubstantial0.000.250.500.751.00Percent CorrectDP DescriptionPercent Of Expectations Correct By Definition (Local)ControlTechnicalRiskTrustEnablesTechniquesUnsubstantial0.000.250.500.751.00Percent CorrectDP DescriptionPercent Of Expectations Correct By Definition (Central)Session 11C: Software Development and Analysis CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea3047No Increased Sharing
Some Increased Sharing
Significantly Increased Sharing
Legend:
Hack
Law Enforcement
Organization
Data Analyst
Graphs
Share
No Overlap
Partial Overlap
Full Overlap
Description: Enables
Description: Risk
Description: Techniques
User Concerns:
Description Effects:
User Concerns:
Description Effects:
User Concerns:
Description Effects:
Figure 3: Visualization of our framework for reasoning about the impact of DP descriptions on users‚Äô willingness to share information.
Colored dots under users represent information flows about which that user cares. Colored dots under descriptions represent the information
flows for which the description raises expectations. We imagine a potential user with some prior set of information disclosures about which
they are concerned. When asked if they are willing to share their information with a differentially private system, the user is given a brief
description of DP. Our results suggest that a user‚Äôs willingness to share information is not simply a function of how this description raises
their expectations, but also a function of their prior concerns. Specifically, the description may raise the user‚Äôs expectations for information
disclosures about which the user was not concerned. Thus, we speculate that the degree to which the user‚Äôs expectations overlap with the
effects of a description will be an important determining factor in a user‚Äôs willingness to share.
to share. This is noteworthy, as ensuring privacy in graphs and
informational charts is a common motivating example of DP, is the
only information disclosure protected against by both local and
central DP, and at least one current deployment of DP is focused
on protecting user information from data analysts [50].
We also find that in-the-wild descriptions of DP have a substan-
tial impact on user privacy expectations (RQ3), but not user will-
ingness to share (RQ4). Descriptions of DP that focus on different
themes raise privacy expectations for information flows. However,
this can be a double-edged sword, as raising expectations can also
mislead users about the privacy properties of a system.
Novel framework for reasoning about the impact of descrip-
tions. Upon first inspection, there appears to be a contradiction
embedded in our results: we established that respondents care about
information disclosures relevant to DP and are (in some cases) more
willing to share their information when they are assured that these
information disclosures will not occur. But, we found that offering
respondents DP did not increase their willingness to share informa-
tion, no matter the description. At first glance, these results might
seem to indicate that respondents did not understand the descrip-
tions at all. However, the results in Table 4 show that respondents
had higher privacy expectations when presented with some de-
scriptions. One would expect that these higher expectations would
lead to higher willingness to share, in line with our first results.
To resolve this tension, we recall that not every respondent cared
about every kind of information disclosure (Section 4.2). While
many respondents cared about each kind of disclosure, none of the
information disclosures were important to more than 60% of respon-
dents. Thus, there was almost certainly misalignment between the
disclosures that mattered to a given respondent and the disclosures
that were influenced by the DP description they were shown. For
instance, imagine a potential user cared about the Share expecta-
tion, but was presented with the Trust description. This potential
user‚Äôs higher expectations for Hack and Data analyst disclosures
would likely do little to raise their willingness to share.
These results suggest a framework for understanding how de-
scriptions of DP influence a user‚Äôs willingness to share information
(visualized in Figure 3). When users encounter a differentially pri-
vate system, they already have privacy preferences and concerns.
When a user sees a description of DP, the user‚Äôs expectations about
certain information flows may increase. If the ways in which their
privacy expectations increase aligns with the types of informa-
tion disclosure about which they are concerned, they may be more
likely to share their private information. A key takeaway from this
framework is that a clear and concise description of DP may not be
enough to raise user‚Äôs willingness to share. Instead, it is important
that a description speaks to users‚Äô concerns directly and be tailored
to address those concerns, as we discuss below.
Need for new descriptions. It is very evident from our results
that the way DP is described in-the-wild is insufficient to help users
make informed decisions. There is no consistency or standardiza-
tion in the language organizations use. Thus, characterizing the
way users might see DP described required us to identify the six
descriptive themes used in this work. The themes present in these
descriptions seem to haphazardly raise users‚Äô expectations. This
is especially concerning given the differences between local and
central DP; if descriptions are not carefully tailored to the model,
they may mislead users about the privacy properties of the system.
Indeed, Figure 2 shows that the existing descriptions of DP do little
to correctly set expectations, no matter the deployment model.
We note that the simple descriptions that we showed respondents
in our surveys are not completely ineffective or without use. For
instance, using our Risk description may be appropriate for a local
Session 11C: Software Development and Analysis CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea3048DP deployment as it raised expectations broadly. However, because
these descriptions do nothing to increase participation, they may
not achieve the goals of system designers.
There are two main alternatives for improving the state of DP
descriptions. First, one could take the approach of Xiong et al. [69],
carefully constructing descriptions of DP and training users to un-
derstand those descriptions. However, Xiong et. al.‚Äôs results indicate
that such an approach is difficult: a significant number of users
were unable to correctly answer test questions about DP after view-
ing carefully crafted descriptions. An alternative approach, which
prior work on privacy beyond DP suggests may be particularly
effective [62], is to explicitly inform users about the risks posed
to their information. For instance, a description of a central DP
system might specify that information will not be leaked through
any graphs or informational chats, but could still be leaked to the
other entities listed above. This would be similar to the privacy
nutrition labels proposed by Kelley et al. [41]. Such descriptions
of DP could allow users to make an informed information sharing
decision without requiring them to build a comprehensive mental
model of the technical details of DP techniques. That said, technical
details and parameter choices for DP deployment (e.g., the value of
ùúñ) have important implications for user privacy and, as such, future
work should also explore how best to communicate these technical
nuances in meaningful ways.
7 CONCLUSION
In this work we studied DP from the user‚Äôs perspective, focusing
on how users‚Äô privacy expectations relate to DP as they are likely
to encounter it in-the-wild. We showed that the privacy concerns
about which users care can be addressed by DP, but the varied ways
in which DP is described set user expectations in a haphazard, and
often inaccurate, manner. Our results indicate that the interaction
between user‚Äôs intrinsic privacy concerns and the ways in which
descriptions of DP set user expectations informs a user‚Äôs willingness
to share their information under differentially private guarantees.
Our work posits a novel framework for understanding this interplay
and suggests concrete directions for developing better descriptions
of DP that directly and accurately address user privacy concerns.
ACKNOWLEDGMENTS
The first author was supported in part by The Defense Advanced
Research Projects Agency (grant number W911NF-21-1-0371), NSF
grants CNS-1850187 and CNS-1942772 (CAREER), a Mozilla Re-
search Grant, and a JPMorgan Chase Faculty Research Award. Part
of this work was completed while the first author was at Georgia
Institute of Technology. The second author is supported by the
National Science Foundation under Grant #2030859 to the Com-
puting Research Association for the CIFellows Project and The
Defense Advanced Research Projects Agency under Agreement No.
HR00112020021. Part of this work was completed while the second
author was at Johns Hopkins University. Part of this work was
completed while the third author was at Microsoft Research. Any
opinions, findings and conclusions or recommendations expressed
in this material are those of the author(s) and do not necessarily
reflect the views of the United States Government or DARPA.
REFERENCES
[1] John M. Abowd, Gary L. Benedetto, Simson L. Garfinkel, Scot A. Dahl, Aref N.
Dajani, Matthew Graham, Michael B. Hawes, Vishesh Karwa, Daniel Kifer,
Hang Kim, Philip Leclerc, Ashwin Machanavajjhala, Jerome P. Reiter, Rolando
Rodriguez, Ian M. Schmutte, William N. Sexton, Phyllis E. Singer, and Lars
Vilhuber. 2020. The modernization of statistical disclosure limitation at the
U.S. Census Bureau. https://www2.census.gov/adrm/CED/Papers/FY21/2020-08-
AbowdBenedettoGarfinkelDahletal-The%20modernization%20of.pdf.
[2] Ruba Abu-Salma, Elissa M. Redmiles, Blase Ur, and Miranda Wei. 2018. Ex-
ploring User Mental Models of End-to-End Encrypted Communication Tools.
In Proceedings of the 8th USENIX Workshop on Free and Open Communica-
tions on the Internet (FOCI 18). USENIX Association, Baltimore, MD. https:
//www.usenix.org/conference/foci18/presentation/abu-salma
https://www.apple.com/privacy/docs/
[3] Apple. 2018. Differential Privacy.
Differential_Privacy_Overview.pdf.
[4] Brooke Auxier, Lee Rainie, Monica Anderson, Andrew Perrin, Madhu
Kumar, and Erica Turner. 2019.
Americans and Privacy: Concerned,
Confused and Feeling Lack of Control Over Their Personal Information.
https://www.pewresearch.org/internet/2019/11/15/americans-and-privacy-
concerned-confused-and-feeling-lack-of-control-over-their-personal-
information/.
[5] Pablo J Barrio, Daniel G Goldstein, and Jake M Hofman. 2016. Improving com-
prehension of numbers in the news. In Proceedings of the 2016 CHI: Conference on
Human Factors In Computing Systems. ACM, 2729‚Äì2739.
[6] Yoav Benjamini and Yosef Hochberg. 1995. Controlling the false discovery rate:
a practical and powerful approach to multiple testing. Journal of the Royal
Statistical Society: Series B (Methodological) 57, 1 (1995), 289‚Äì300.
[7] Sebastian Benthall, Seda G√ºrses, and Helen Nissenbaum. 2017. Contextual in-
tegrity through the lens of computer science. Now Publishers.
[8] Hugh Beyer and Karen Holtzblatt. 1999. Contextual design. Interactions 6, 1
(1999), 32‚Äì42.
[9] Brooke Bullek, Stephanie Garboski, Darakhshan J Mir, and Evan M Peck. 2017.
Towards Understanding Differential Privacy: When Do People Trust Randomized
Response Technique?. In Proceedings of the 2017 CHI Conference on Human Factors
in Computing Systems. ACM, 3833‚Äì3837.
[10] Juan Pablo Carrascal, Christopher Riederer, Vijay Erramilli, Mauro Cherubini,
and Rodrigo de Oliveira. 2013. Your browsing behavior for a Big Mac: Economics
of personal information online. In Proceedings of the 22nd International Conference
on World Wide Web. ACM, 189‚Äì200.
[11] Aloni Cohen and Kobbi Nissim. 2020. Towards formalizing the GDPR‚Äôs notion
of singling out. Proceedings of the National Academy of Sciences 117, 15 (2020),
8344‚Äì8352.
[12] Jamie Condliffe. 2019. The Week in Tech: Facebook‚Äôs First Step Toward Treating
Our Data Better. https://www.nytimes.com/2019/08/23/technology/big-tech-
data.html.
[13] Lorrie Faith Cranor. 2006. What do they ‚Äúindicate?‚Äù Evaluating security and
privacy indicators. Interactions 13, 3 (2006), 45‚Äì47.
[14] Rachel Cummings and Deven Desai. 2018. The Role of Differential Privacy
in GDPR Compliance. In Proceedings of 2nd FATREC Workshop on Responsible
Recommendation. ACM.
[15] Inbal Dekel, Rachel Cummings, Ori Heffetz, and Katrina Ligett. 2021. Differential
Privacy in Economics Experiments: A Public-Good-Game Application. Pre-print.
[16] Bolin Ding, Jana Kulkarni, and Sergey Yekhanin. 2017. Collecting Telemetry
Data Privately. In Advances in Neural Information Processing Systems 30 (NIPS
‚Äô17). Advances in Neural Information Processing Systems, 3574‚Äì3583.
[17] Cynthia Dwork. 2008. Differential Privacy: A Survey of Results. In Theory and
Applications of Models of Computation, Manindra Agrawal, Dingzhu Du, Zhenhua
Duan, and Angsheng Li (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg,
1‚Äì19.
[18] Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold,
and Aaron Roth. 2015. The reusable holdout: Preserving validity in adaptive data
analysis. Science 349, 6248 (2015), 636‚Äì638.
[19] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006. Cal-
ibrating Noise to Sensitivity in Private Data Analysis. In TCC 2006 (LNCS,
Vol. 3876), Shai Halevi and Tal Rabin (Eds.). Springer, Heidelberg, 265‚Äì284.
https://doi.org/10.1007/11681878_14
[20] Cynthia Dwork, Moni Naor, Omer Reingold, Guy N. Rothblum, and Salil P.
Vadhan. 2009. On the complexity of differentially private data release: efficient
algorithms and hardness results. In 41st ACM STOC, Michael Mitzenmacher (Ed.).
ACM Press, 381‚Äì390. https://doi.org/10.1145/1536414.1536467
[21] Cynthia Dwork and Aaron Roth. 2014. The algorithmic foundations of differential
privacy. Foundations and Trends in Theoretical Computer Science 9, 34 (2014),
211‚Äì407.
[22] Julia Brande Earp, Annie I Ant√≥n, Lynda Aiman-Smith, and William H Stuffle-
beam. 2005. Examining Internet privacy policies within the context of user
privacy values. IEEE Transactions on Engineering Management 52, 2 (2005), 227‚Äì
237.
Session 11C: Software Development and Analysis CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea3049[23] Serge Egelman, Raghudeep Kannavara, and Richard Chow. 2015. Is this thing on?
Crowdsourcing privacy indicators for ubiquitous sensing platforms. In Proceed-
ings of the 33rd Annual ACM Conference on Human Factors in Computing Systems.
1669‚Äì1678.
[24] √ölfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova. 2014. RAPPOR: Ran-
domized Aggregatable Privacy-Preserving Ordinal Response. In ACM CCS 2014,
Gail-Joon Ahn, Moti Yung, and Ninghui Li (Eds.). ACM Press, 1054‚Äì1067.
https://doi.org/10.1145/2660267.2660348
[25] Gerd Gigerenzer, Ralph Hertwig, Eva Van Den Broek, Barbara Fasolo, and Kon-
stantinos V Katsikopoulos. 2005. ‚ÄúA 30% chance of rain tomorrow‚Äù: How does the
public understand probabilistic weather forecasts? Risk Analysis: An International
Journal 25, 3 (2005), 623‚Äì629.
[26] Jens Hainmueller, Dominik Hangartner, and Teppei Yamamoto. 2015. Validating
vignette and conjoint survey experiments against real-world behavior. Proceed-
ings of the National Academy of Sciences 112, 8 (2015), 2395‚Äì2400.
[27] Moritz Hardt and Guy N. Rothblum. 2010. A Multiplicative Weights Mechanism
for Privacy-Preserving Data Analysis. In 51st FOCS. IEEE Computer Society Press,
61‚Äì70. https://doi.org/10.1109/FOCS.2010.85
[28] Eszter Hargittai and Yuli Patrick Hsieh. 2012. Succinct survey measures of
web-use skills. Social Science Computer Review 30, 1 (2012), 95‚Äì107.
[29] Eszter Hargittai and Eden Litt. 2013. New strategies for employment? internet
skills and online privacy practices during people‚Äôs job search. IEEE Security &
Privacy 11, 03 (may 2013), 38‚Äì45. https://doi.org/10.1109/MSP.2013.64
[30] Eszter Hargittai and Marina Micheli. 2019. Internet skills and why they matter.
Society and the internet: How networks of information and communication are
changing our lives (2019), 109.
[31] Michael Hawes. 2019. Title 13, Differential Privacy, and the 2020 Decennial
Census. https://www2.census.gov/about/policies/2019-11-paper-differential-
privacy.pdf.
[32] Ori Heffetz and Katrina Ligett. 2014. Privacy and data-based research. Journal of
Economic Perspectives 28, 2 (2014), 75‚Äì98.
[33] Ama√ß Herdaƒüdelen, Alex Dow, Bogdan State, Payman Mohassel, and Alex Pompe.
2020. New privacy-protected Facebook data for independent research on social
media‚Äôs impact on democracy. https://research.fb.com/blog/2020/06/protecting-
privacy-in-facebook-mobility-data-during-the-covid-19-response/.
[34] Iulia Ion, Niharika Sachdeva, Ponnurangam Kumaraguru, and Srdjan ƒåapkun.
2011. Home is safer than the cloud! Privacy concerns for consumer cloud storage.
In Proceedings of the Seventh Symposium On Usable Privacy and Security (SOUPS).
ACM, 1‚Äì20.
[35] Carlos Jensen and Colin Potts. 2004. Privacy policies as decision-making tools:
an evaluation of online privacy notices. In Proceedings of the SIGCHI conference
on Human Factors in Computing Systems. ACM, 471‚Äì478.
[36] Noah Johnson and Joe Near. 2019. sql-differential-privacy. https://github.com/
uber-archive/sql-differential-privacy.
[37] Ruogu Kang, Laura Dabbish, Nathaniel Fruchter, and Sara Kiesler. 2015. ‚ÄúMy
Data Just Goes Everywhere:‚Äù User mental models of the internet and implications
for privacy and security. In Eleventh Symposium On Usable Privacy and Security
(SOUPS). ACM, 39‚Äì52.
[38] Gabriel Kaptchuk, Daniel G Goldstein, Eszter Hargittai, Jake Hofman, and
Elissa M Redmiles. 2020. How good is good enough for COVID19 apps? The in-
fluence of benefits, accuracy, and privacy on willingness to adopt. arXiv preprint
arXiv:2005.04343 (2020).
[39] Shiva Prasad Kasiviswanathan, Homin Lee, Kobbi Nissim, Sofya Raskhodnikova,
and Adam Smith. 2011. What Can We Learn Privately? SIAM J. Comput. 40, 3
(2011), 793‚Äì826. https://doi.org/10.1137/090756090
[40] Carmen Keller and Michael Siegrist. 2009. Effect of risk communication formats
on risk perception depending on numeracy. Medical Decision Making 29, 4 (2009),
483‚Äì490.
[41] Patrick Gage Kelley, Joanna Bresee, Lorrie Faith Cranor, and Robert W Reeder.
2009. A ‚Äúnutrition label‚Äù for privacy. In Proceedings of the Fifth Symposium On
Usable Privacy and Security (SOUPS). ACM, 1‚Äì12.
[42] Katharina Krombholz, Karoline Busse, Katharina Pfeffer, Matthew Smith, and
Emanuel von Zezschwitz. 2019. ‚ÄúIf HTTPS Were Secure, I Wouldn‚Äôt Need 2FA‚Äù -
End User and Administrator Mental Models of HTTPS. In 2019 IEEE Symposium
on Security and Privacy. IEEE Computer Society Press, 246‚Äì263. https://doi.org/
10.1109/SP.2019.00060
[43] Alexandra Mai, Katharina Pfeffer, Matthias Gusenbauer, Edgar Weippl, and Katha-
rina Krombholz. 2020. User Mental Models of Cryptocurrency Systems - A
Grounded Theory Approach. (Aug. 2020), 341‚Äì358. https://www.usenix.org/
conference/soups2020/presentation/mai
[44] Aleecia M Mcdonald, Robert W Reeder, Patrick Gage Kelley, and Lorrie Faith
Cranor. 2009. A comparative study of online privacy policies and formats. In
Proceedings of the International Symposium on Privacy Enhancing Technologies
Symposium. Springer, 37‚Äì55.
[45] Nora McDonald, Sarita Schoenebeck, and Andrea Forte. 2019. Reliability and
inter-rater reliability in qualitative research: Norms and guidelines for CSCW
and HCI practice. Proceedings of the ACM on Human-Computer Interaction 3,
CSCW (2019), 1‚Äì23.
119.
[46] Frank McSherry and Kunal Talwar. 2007. Mechanism Design via Differential
Privacy. In 48th FOCS. IEEE Computer Society Press, 94‚Äì103. https://doi.org/10.
1109/FOCS.2007.41
[47] Federico Morando, Raimondo Iemma, and Emilio Raiteri. 2014. Privacy evaluation:
what empirical research on users‚Äô valuation of personal data tells us. Internet
Policy Review 3, 2 (2014), 1‚Äì12.
[48] Vivian Genaro Motti and Kelly Caine. 2016. Towards a visual vocabulary for
privacy concepts. In Proceedings of the Human Factors and Ergonomics Society
Annual Meeting, Vol. 60. SAGE Publications Sage CA: Los Angeles, CA, 1078‚Äì
1082.
New privacy-protected Facebook data for in-
https:
dependent research on social media‚Äôs impact on democracy.
//research.fb.com/blog/2020/02/new-privacy-protected-facebook-data-
for-independent-research-on-social-medias-impact-on-democracy/.
[49] Chaya Nayak. 2020.
[50] Joe Near. 2018. Differential Privacy at Scale: Uber and Berkeley Collaboration.
In Enigma 2018 (Enigma 2018). USENIX Association, Santa Clara, CA. https:
//www.usenix.org/node/208168
[51] Helen Nissenbaum. 2004. Privacy as contextual integrity. Wash. L. Rev. 79 (2004),
[52] Kobbi Nissim, Thomas Steinke, Alexandra Wood, Mark Bun, Marco Gaboardi,
David R. O‚ÄôBrien, and Salil Vadhan. 2017. Differential Privacy: A Primer for a Non-