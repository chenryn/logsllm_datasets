sist of a source IP address followed by a source port
number followed by a destination port number. Forth,
the module looks for the key in a TCP lookup table. If
the key is found, the session’s state and its information
will be updated accordingly. If the key is not found,
the module checks if the packet is a SYN packet. If
it is a SYN packet then a new session will be created.
Otherwise, the packet will be dropped.
It is worth to mention that the module maintains two
global variables: a counter to count the number of
sessions that have been created without a completed
three-way handshaking and a cycle number that takes
two values 0 and 1. This means that the counter is
incremented when a new session was created, and is
decremented when the session has completed three-
way handshaking normally or by RST packet. When
this counter reaches a predeﬁned threshold (in our im-
plementation, we set it to 50), it triggers a timer and
increments the cycle number by 1 (modulus two ad-
dition). When the timer expires, it invokes a garbage
collector to delete all sessions that have not yet estab-
lished a connection and have a cycle number not equiv-
alent to the current cycle number. The later condition
prevents the garbage collector to delete the newly cre-
ated sessions.
• Dispatcher Module: The Dispatcher module is re-
sponsible to modify, log, and send the packets to the
appropriate interface. As a design issue, there are two
ways to send a client packet to the Live VM or the Live
VM packet back to the client. The ﬁrst way is to cre-
ate two sockets per connection: one socket between a
client and NetDd, and another socket between NetDd
and the Live VM. This means that NetDd is acting as
a proxy server. The second way is to use libpcap to
receive and send packets after modifying some parts
of a packet header. Since the ﬁrst approach is very
expensive, we have adopted the second approach.
Figure 5: NetDd Modules
• It registers the IP addresses of the Live VM and all
shadow machines to NetDd.
• It conﬁgures the iptables of the Linux server to prevent
all TCP traﬃc from being processed by the protocol
stack routines except for those packets that are gen-
erated by the Shadow VMs because it will be used by
replay processes to perform traﬃc replay. Please note
that other types of traﬃc, such as UDP, will also be
handled by the upper layers of the Linux server. In our
current implementation, we handled only TCP pack-
ets.
• It displays a shell prompt to allow system administra-
tor to perform some administrative tasks like shutting
down a virtual machine, creating a new virtual ma-
chine, conﬁguring time-lag for a speciﬁc virtual ma-
chine, etc. Furthermore, it enables system administra-
tor to execute Unix/Linux commands.
In future work, we will improve TimeVM so that a limited
number of traﬃc will be handled by the gateway server. As
a result, this will enhance the security level of the Linux
server. This is very important because Linux server stores
very sensitive information such as traﬃc log and a set of
private keys used by certain functions/routines to handle
SSL replaying.
5.2 Network Distributor Daemon (NetDd)
NetDd is responsible to redirect an incoming packet to its
appropriate destination. It is acting like a transparent NAT
server. Traﬃc comes from clients will be redirected to the
IP address of the Live VM and the traﬃc comes from the
Live VM will be redirected back to the clients.
As shown in Figure 5, NetDd is composed of three mod-
ules. The following is a description of each module:
• Splitter Module: This module is responsible to sniﬀ
incoming packets and check the source IP address of
these packets. If the source IP address of an incoming
packet is belonging to the IP address of a Shadow VM,
then the module will ignore it since this packet will be
handled by the protocol stack routines. Otherwise, the
splitter module will forward the received packet to the
translator module.
• Translator Module: This module is used to keep
track of all TCP connections between clients and the
SpitterModuleTranslatorModuleDispatcherModuleTraffic LogCache fileBlack list140Algorithm 1 Translator (INPUT: data as PACKET)
if data is not IP or TCP packet then
return
end if
if source(data) == CLIENT then
key = genkey(data.srcIP, data.srcPort, data.destPort)
else
key = genkey(data.destIP, data.destPort, data.srcPort)
end if
session = lookup(session table, key)
if session is null then
if data.ﬂag has SYN ﬂag then
create session(session)
store session information
insert session(session table, session, key)
end if
else
update session information
end if
Figure 6: Packet forwarding
Figure 6 shows how a packet is exchanged between a
client, NetDd, and the Live VM. When NetDd receives
a packet from a client, it modiﬁes the followings:
– It replaces the destination MAC address by the
MAC address of the Live VM
– It replaces the source MAC address by the MAC
address of the TimeVM (the MAC address of the
Linux server)
– It replaces the destination IP address by the IP
address of the Live VM.
When NetDd receives back the response of the Live
VM, it modiﬁes the followings:
– It replaces the destination MAC address by the
MAC address of the client. We stored the client
MAC address in the session data structure.
– It replaces the source MAC address by the MAC
address of TimeVM (the MAC address of the Linux
server)
– It replaces the source IP address by the IP address
of TimeVM.
There is a little bit overhead in this approach. Mod-
ifying the ﬁelds of IP header requires re-calculating
the checksum. We measured the modiﬁcation and re-
calculation overhead by running two experiments. In
the ﬁrst experiments, we let a client host to connect
directly to the Live VM. In the second one, the client
connects to the TimeVM server ﬁrst. As shown in ta-
ble 1, the ﬁrst column represents the timestamps when
we sent an HTTP request directly to the Live VM
N timestamp w/o NetDd
1
2
3
4
5
6
7
13.751163
13.751433
13.751439
13.751556
13.763961
13.764077
13.764103
timestamp w/ NetDd
17.473093
17.486035
17.486086
17.486241
17.488031
17.489360
17.489973
Table 1: Forwarding overhead due to packet modi-
ﬁcation and checksum calculation
without the interference of NetDd. The second col-
umn represents the timestamps for the second exper-
iment. The timestamps are collected by using wire-
shark program at the client host. We conclude that
the forwarding overhead is almost negligible.
5.3 Replay Processes
In this section, we describe the most critical function of
TimeVM which is traﬃc replay. Traﬃc replay is performed
by replay processes. Each process is associated with a single
Shadow VM as shown in Figure 1, and is conﬁgured with
a time lag that indicates the distance in time between the
Live VM and its associated Shadow VM.
In normal replay mode, a replay process should always
preserve the time diﬀerence between the current system time
and the current replayed packet time to amount equals to
its time lag. One way to do that is to implement a polling
mechanism that continuously invokes gettimeofday() system
call until the diﬀerence between the packet time and current
system time is less than or equal to time lag. Although this
approach provides a high degree of accuracy, it consumes a
lot of CPU time. Another approach is to use usleep() sys-
tem call. We implemented the second approach as follows.
Initially, each replay waits until it reads the ﬁrst packet.
Then, it goes to sleep for amount of time equals to its time
lag. After that, it starts to replay the ﬁrst packet. Be-
fore replaying the next packet, the replay process calculates
silent time which is the time diﬀerence between the next
packet’s timestamp and the previous packet’s timestamp. If
silent time is greater than two times a threshold, the replay
process goes to sleep for amount of time equals to silent time
minus threshold. Otherwise, the replay process continues to
replay the next packet. The value of threshold represents the
amount of time required for the operating system to execute
usleep() system call. Using lmbench [14], usleep() overhead
is approximately 12.50µs in a Pentium 4 machine 1.6 GHz
with 512MB RAM, running Linux version 2.6.18.
In fast replay mode, the value of silent time will be short-
ened by a predeﬁned factor value. The factor is deﬁned as
a multiple of 2. Although, shortening silent time eventually
increases the rate of replayed packets per second, it requires
some considerations. First, it should not aﬀect the behavior
of the network service. In other words, the server’s response
in normal replay mode and the server’s response in fast re-
play mode (for the same set of packets) should be identical.
Second, it should not overload the network service capacity.
In
our early stage of the implementation, we used libpcap and
libnet libraries to handle and manipulate packets. But then
we have faced many complex design issues such as main-
taining the receiver window size (deﬁned during three-way
Replaying packets correctly is a very complex task.
141handshaking in a TCP session), handling TCP options, etc.
Therefore, we changed our implementation path to use sock-
ets instead of libpcap and libnet libraries. This is why we
conﬁgured TimeVM to allow Shadow VM’s packets to be
processed by higher layers. Using sockets to handle packets
implied to other complications in our design: how to handle
diﬀerent types of sessions (explained in the next paragraph),
how to send packet payloads in-order if they received and
logged out-of-order, and how to handle packet fragmenta-
tion.
Due to the nature of TCP protocol and variety of pro-
tocols building on top of TCP such as SSL and FTP, the
replay process should be protocol-aware in order to prop-
erly replay TCP traﬃc. We have classiﬁed three types of
sessions: (1) sessions with a static service number such as
HTTP session, (2) sessions with multiple service numbers
like FTP session, and (3) sessions with encrypted payloads
such as SSL sessions. The functionality of each session is
deﬁned as plug-in for replay process. This allows us to ex-
tend our replay process to accommodate other application
protocols.
To handle SSL sessions, we adopted the code written at
[1]. The private keys that was stored in Live VM will also
be stored in the Linux Server. These keys are needed in
order to properly decrypt the encrypted traﬃc. To reduce
the overhead of decryption process, we log a payload after
decrypting it. A replay process needs only to re-encrypt the
payload when it replays SSL sessions.
In our current im-
plementation on replaying SSL, we did not implement client
authentication. Client authentication is useful if the server
wanted to restrict access to some services to only certain
authorized clients. One way to handle this limitation is to
deﬁne a global trusted user locally in Linux server. The
drawback is that it requires to modify application to accept
that deﬁned global user.
A TCP session is uniquely identiﬁed by one of two keys:
the hash key as we described in section 5.2 and Session Iden-
tiﬁer (SID). SID is a sequence number of integers that is al-
ways incremented whenever a new session is created. Please
note that the data structure of a TCP session is not simi-
lar as the data structure of NetDd’s session. In SSL, SID
represents session ID deﬁned during SSL handshaking. It is
used when SSL protocol generates a new session and new
cryptographic keys.
In order to replay packets in-order, each session maintains
two variables: “exp seq num” and “leftover ”. exp seq num
represents the expected sequence number of the next “client’s”
packet, and leftover is a counter that keeps track the number
of packets stored in a hash table called “unordered table”.
Now, when the replay process reads the next packet,
it
checks to see if the sequence number deﬁned in the TCP
header is matching the expected sequence number exp seq num.
If they do not match, the packet will be stored in unordered table
with a key constructed as the concatenation of SID and the
sequence number in the TCP header, and the leftover vari-
able will be incremented by one. Otherwise, the packet will
be replayed and sent to its associated Shadow VM. After re-
playing the packet, the replay updates the expected sequence
number (by adding the payload length to exp seq num) and
checks the value of leftover. If the value is greater than zero,
then the process searches the hash table. If the constructed
key [SID | expected sequence number] is found, the stored
packet will be replayed and deleted, and the value of left-
over will be decremented. The process keeps searching the
hash table until either the constructed key is not found or
leftover value becomes zero. Please note that, we do not
need to keep track of the expected sequence number for the
server packets since these packets are dropped. Algorithm 2
shows only the part that replaying a client packet.
Algorithm 2 Replay (INPUT: data as PACKET)
if source(data) == CLIENT then
key = genkey(data.srcIP, data.srcPort, data.destPort)
session = lookup(tcp session table, key)
if session is null then
if data.ﬂag is SYN then
session = create new node()
store session information such as exp seq num, leftover
insert session(tcp session table, session, key)
end if
else
compute payload size
if payload size > 0 then
if data.seqnum == session.exp seq num then
replay data and update session.exp seq num
update TCP state in TCP session
while session.leftover > 0 do
key2 = concat(session.sid, session.exp seq num)
if data = lookup(unordered table, key2) != null
then
replay data and update session.exp seq num
session.leftover = session.leftover - 1
end if
end while
else
key2 = concat(session.sid, data.seqnum)
store data in unordered table with key2
end if
else
update TCP state
end if
end if
end if
Handling IP fragmentation is another complex problem.
In our current implementation, we assumed that there is no
IP fragmentation. The Translator module in NetDd drops
any fragmented packet. We put IP fragmentation as a future
work.
6. EVALUATION AND
NUMERICAL RESULTS
In this section, we report some numerical results of our
analytical model. The purpose is to study the feasibility of