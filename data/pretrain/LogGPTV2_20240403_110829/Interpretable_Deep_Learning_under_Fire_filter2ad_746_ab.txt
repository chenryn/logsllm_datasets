literature; the paper is concluded in § 7.
2 Preliminaries
We begin with introducing a set of fundamental concepts
and assumptions. The symbols and notations used in this
paper are summarized in Table 1.
Classiﬁer – In this paper, we primarily focus on predictive
tasks (e.g., image classiﬁcation [12]), in which a DNN f (i.e.,
classiﬁer) assigns a given input x to one of a set of predeﬁned
classes C , f (x) = c ∈ C .
Interpreter – In general, the DNN interpretability can
be obtained in two ways: designing interpretable DNNs
[45, 62] or extracting post-hoc interpretations. The latter
case does not require modifying model architectures or pa-
rameters, thereby leading to higher prediction accuracy. We
thus mainly consider post-hoc interpretations in this paper.
More speciﬁcally, we focus on instance-level interpretabil-
ity [10,16,26,37,38,45,48,50,63], which explains how a DNN
f classiﬁes a given input x and uncovers the causal relation-
ship between x and f (x). We assume such interpretations are
given in the form of attribution maps. As shown in Figure 2,
the interpreter g generates an attribution map m = g(x; f ),
with its i-th element m[i] quantifying the importance of x’s
i-th feature x[i] with respect to f (x).
Adversarial Attack – DNNs are inherently vulnerable to
adversarial inputs, which are maliciously crafted samples to
force DNNs to misbehave [36, 56]. Typically, an adversarial
input x∗ is generated by modifying a benign input x◦ via pixel
perturbation (e.g., PGD [35]) or spatial transformation (e.g.,
STADV [60]), with the objective of forcing f to misclassify
x∗ to a target class ct, f (x∗) = ct (cid:54)= f (x◦). To ensure the at-
tack evasiveness, the modiﬁcation is often constrained to an
Figure 2: Workﬂow of an interpretable deep learning system (IDLS).
terpretable deep learning system (IDLS). The enhanced inter-
pretability of IDLSes is believed to offer a sense of security by
involving human in the decision process [57]. However, given
its data-driven nature, this interpretability itself is potentially
susceptible to malicious manipulations. Unfortunately, thus
far, little is known about the security vulnerability of IDLSes,
not to mention mitigating such threats.
Our Work. To bridge the gap, in this paper, we conduct a
comprehensive study on the security vulnerability of IDLSes,
which leads to the following interesting ﬁndings.
First, we demonstrate that existing IDLSes are highly vul-
nerable to adversarial manipulations. We present ADV2, a
new class of attacks that generate adversarial inputs not only
misleading a target DNN but also deceiving its coupled in-
terpreter. By empirically evaluating ADV2 against four ma-
jor types of IDLSes on benchmark datasets and in security-
critical applications (e.g., skin cancer diagnosis), we show
that it is practical to generate adversarial inputs with predic-
tions and interpretations arbitrarily chosen by the adversary.
For example, Figure 1 (c) shows adversarial inputs that are
misclassiﬁed by target DNNs and also interpreted highly sim-
ilarly to their benign counterparts. Thus the interpretability of
IDLSes merely provides limited security assurance.
Then, we show that one possible root cause of this attack
vulnerability lies in the prediction-interpretation gap: the in-
terpreter is often misaligned with the classiﬁer, while the
interpreter’s interpretation only partially explains the classi-
ﬁer’s behavior, allowing the adversary to exploit both models
simultaneously. This ﬁnding entails several intriguing ques-
tions: (i) what, in turn, is the possible cause of this gap? (ii)
how does this gap vary across different interpreters? (iii) what
is its implication for designing more robust interpreters? We
explore all these key questions in our study.
Further, we investigate the transferability of ADV2 across
different interpreters. We note that it is often difﬁcult to ﬁnd
adversarial inputs transferable across distinct types of inter-
preters, as they generate interpretations from complementary
perspectives (e.g., back-propagation, intermediate representa-
tions, input-prediction correspondence). This ﬁnding points
to training an ensemble of interpreters as one potential coun-
termeasure against ADV2.
Finally, we present adversarial interpretation distillation
(AID), an adversarial training framework which integrates
1660    29th USENIX Security Symposium
USENIX Association
InputClassiﬁerPrediction f?Interpretation Interpreterallowed set (e.g., a norm ball Bε(x◦) = {x|(cid:107)x− x◦(cid:107)∞ ≤ ε}).
Consider PGD, a universal ﬁrst-order adversarial attack, as a
concrete case. At a high level, PGD implements a sequence
of project gradient descent on the loss function:
(cid:0)x(i) − αsgn(cid:0)∇x(cid:96)prd
(cid:0) f(cid:0)x(i)(cid:1) ,ct
(cid:1)(cid:1)(cid:1)
x(i+1) = ΠBε(x◦)
(1)
where Π is the projection operator, α represents the learn-
ing rate, the loss function (cid:96)prd measures the difference of
the model prediction f (x) and the class ct targeted by the
adversary (e.g., cross entropy), and x(0) is initialized as x◦.
Threat Model – Following the line of work on adversarial
attacks [9,19,35,56], we assume in this paper a white-box set-
ting: the adversary has complete access to the classiﬁer f and
the interpreter g, including their architectures and parameters.
This is a conservative and realistic assumption. Prior work has
shown that it is possible to train a surrogate model f (cid:48) given
black-box access to a target DNN f [41]; given that the inter-
preter is often derived directly from the classiﬁer (details in
§ 3), the adversary may then train a substitution interpreter g(cid:48)
based on f (cid:48). We consider investigating such black-box attacks
as our ongoing work.
3 ADV2 Attack
The interpretability of IDLSes is believed to offer a sense
of security by involving human in the decision process [13,
17, 20, 57]; this belief has yet to be rigorously tested. We
bridge this gap by presenting ADV2, a new class of attacks
that deceive target DNNs and their interpreters simultaneously.
Below we ﬁrst give an overview of ADV2 and then detail its
instantiations against four major types of interpreters.
3.1 Attack Formulation
The ADV2 attack deceives both the DNN f and its coupled
interpreter g. Speciﬁcally, ADV2 generates an adversarial in-
put x∗ by modifying a benign input x◦ such that
• (i) x∗ is misclassiﬁed by f to a target class ct, f (x∗) = ct;
• (ii) x∗ triggers g to generate a target attribution map mt,
g(x∗; f ) = mt;
• (iii) The difference between x∗ and x◦, ∆(x∗,x◦), is im-
perceptible;
where the distance function ∆ depends on the concrete mod-
iﬁcation: for pixel perturbation (e.g., [35]), it is instantiated
as Lp norm, while for spatial transformation (e.g., [60]), it is
deﬁned as the overall spatial distortion.
In other words, the goal is to ﬁnd sufﬁciently small per-
turbation to the benign input that leads to the prediction and