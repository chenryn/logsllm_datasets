# 13 \| 自动化测试：DevOps的阿克琉斯之踵你好，我是石雪峰。在古希腊神话中，战神阿克琉斯英勇无比，浑身刀枪不入，唯独脚后跟是他的致命弱点。在特洛伊战争中，他的脚后跟被一箭射中，倒地身亡，从此，阿克琉斯之踵就被用来形容致命的缺陷。我今天要跟你聊的自动化测试，就是DevOps 的阿克琉斯之踵。我之前走访过很多公司，我发现，在工程实践领域，比如配置管理、持续集成等，他们实践得还不错，但是却有两大通病，一个是研发度量，另一个就是自动化测试。没有人会否认自动化测试的价值，而且很多公司也都或多或少地在实践自动化测试。但从整体来看，自动化测试的实施普遍不成体系，大多都在关注单点工具。另外，团队对自动化测试的真实效果也存在疑惑。如果不能解决这些问题，就很难突破实践DevOps 的天花板。那么，自动化测试究竟要解决什么问题，又适合哪些业务形态和测试场景呢？我们该如何循序渐进地推进建设，并且正确地度量效果以免踩坑呢？这些问题，就是我要在这一讲中跟你分享的重点内容。自动化测试要解决什么问题？产品交付速度的提升，给测试工作带来了很大的挑战。一方面，测试时间被不断压缩，以前三天的测试工作要在一天内完成。另一方面，需求的变化也给测试工作的开展带来了很大的不确定性。这背后核心的问题是，**业务功能的累加导致测试范围不断扩大，但这跟测试时长的压缩是矛盾的**。说白了，就是要测试的内容越来越多，但是测试的时间却越来越短。全面测试会带来相对更好的质量水平，但是投入的时间和人力成本也是巨大的，而快速迭代交付就意味着要承担一定的风险。那么，究竟是要速度，还是要质量，这是一个很难回答的问题。所以，要想提升测试效率，自然就会联想到自动化手段。实际上，自动化测试适用于以下几种典型场景：1.       有大量机械的重复操作，并且会反复执行的场景，比如批量的回归测试；        2.       有明确的设计规范且相对稳定的场景，比如接口测试；        3.       大批量、跨平台的兼容性测试，比如覆盖多种版本和多种机型的测试，几十个机型还可以接受，如果覆盖成百上千个机型，就只能依靠自动化了；        4.       长时间不间断执行的测试，比如压力测试、可用性测试等。        这些典型场景往往都具备几个特征：设计明确、功能稳定、可多次重复、长期大批量执行等，核心就是通过自动化手段来解决测试成本的问题，也就是人的问题。但这并不意味着手工测试就没有价值了。相反，当人从重复性劳动中解放出来后，就可以投入到更有价值的测试活动中，比如探索性测试、易用性测试、用户验收测试等，这些都属于手工测试的范畴。这听上去还挺合理的，可是，为什么很多公司还是倾向于采用手工测试的方式呢？实际上，并非所有的测试活动都适合自动化，而且，自动化测试建设也面临着一些问题。1.       **投入产出比**        ：很多需求基本上只会上线一次（比如促销活动类需求），那么，实现自动化测试的成本要比手动测试高得多，而且以后也不会再用了，这显然有点得不偿失。        2.       **上手门槛**        ：自动化测试依赖代码方式实现，要开发一套配置化的测试框架和平台，对架构设计和编码能力都有很大的要求。但是，测试人员的编码能力一般相对较弱。        3.       **维护成本高**        ：无论是测试环境、测试用例还是测试数据，都需要随着需求的变化不断进行调整，否则就很容易因为自动化测试过时，导致执行失败。        4.       **测试设备投入高**        ：比如，移动 App    的测试需要有大量的手机资源，想要覆盖所有的手机型号、操作系统版本，本身就不太现实。更何况，有限的机器还经常被测试人员拿去做本地调试，这就进一步加剧了线上测试没有可用资源的情况。        自动化测试的设计这么看来，自动化测试并不是一把万能钥匙，我们也不能指望一切测试都实现自动化。只有在合适的领域，自动化测试才能发挥出最大价值。那么，你可能就要问了，面对这么多种测试类型，到底要从哪里启动自动化测试的建设呢？首先，我来给你介绍一下经典的测试三角形。这个模型描述了从单元测试、集成测试到UI测试的渐进式测试过程。越是靠近底层，用例的执行速度就越快，维护成本也越低。而在最上层的UI层，执行速度要比单元测试和接口测试要慢，比手工测试要快，相应的维护成本要远高于单元测试和接口测试。![](Images/090ada30630fe931382611d75f0c3b7a.png)savepage-src="https://static001.geekbang.org/resource/image/28/6f/28d6b53907036a38d5649a673664006f.png"}>  > 图片来源："DevOps> Handbook\"> > >这样看来，从靠近底层的单元测试入手是一个投入产出相对比较高的选择。但实际上，单元测试的执行情况因公司而异，有的公司能做到80%的覆盖率，但有的公司却寸步难行。毕竟，单元测试更多是由开发主导的，开发领导的态度就决定了运行的效果。但不可否认的是，单元测试还是非常必要的，尤其是针对核心服务，比如核心交易模块的覆盖率。当然，好的单元测试需要研发投入大量的精力。对于 UI层来说，执行速度和维护成本走向了另外一个极端，这也并不意味着就没有必要投入UI 自动化建设。**UI层是唯一能够模拟用户真实操作场景的端到端测试**，页面上的一个按钮可能触发内部几十个函数调用，和单元测试每次只检查一个函数的逻辑不同，UI测试更加关注模块集成后的联动逻辑，**是集成测试最有效的手段**。另外，很多测试人员都是从 UI开始接触自动化的，再加上相对成熟的测试工具和框架，实施不依赖于源码，也是一种比较容易上手的自动化手段。在实际应用中，UI自动化可以帮助我们节省人工测试成本，提高功能测试的测试效率。不过，它的缺点也是比较明显的：**随着敏捷迭代的速度越来越快，UI控件的频繁变更会导致控件定位不稳定，提高了用例脚本的维护成本**。综合考虑投入产出比和上手难度的话，位于中间层的接口测试就成了一种很好的选择。一方面，现代软件架构无论是分层还是服务调用模式，对接口的依赖程度都大大增加。比如典型的前后端分离的开发模式，前后端基本都是在围绕着接口进行开发联调。另一方面，与单元测试相比，接口测试调用的业务逻辑更加完整，并且具备清晰的接口定义，适合采用自动化的方式执行。正因为如此，对于基于 Web的应用来说，我更推荐椭圆形模型，也就是以中间层的 API接口测试为主，以单元测试和 UI测试为辅。你可以参考一下分层自动化测试模型图。![](Images/0fb5a583ef4838edd4177aecdddd1d57.png)savepage-src="https://static001.geekbang.org/resource/image/14/9b/140a33713f7332277c8d2114d050d39b.png"}自动化测试的开发**有效的自动化测试离不开工具和平台的支持**。以接口测试为例，最早都是通过 cURL、Postman、JMeter等工具单机执行的。但是，一次成功的接口测试，除了能够发起服务请求之外，还需要前置的测试数据准备和后置的测试结果校验。对于企业的实际业务来说，不仅需要单接口的执行，还需要相对复杂的多接口，而且带有逻辑的执行，这就依赖于调用接口的编排能力，甚至是内建的Mock 服务。 不仅如此，测试数据、用例、脚本的管理，测试过程中数据的收集、度量、分析和展示，以及测试报告的发送等，都是一个成熟的自动化测试框架应该具备的功能。比如，对于 UI 自动化测试来说，最让人头疼的就是 UI控件变化后的用例维护成本问题。**解决方法就是操作层获取控件和控件本身的定位方法，进行解耦，这依赖于框架的设计与实现**。在实际操作控件时，你可以通过自定义名称的方式来调用控件，自定义名称在控件相关配置文件中进行定义。在具体操作时，可以通过操作层之下的代理层来处理。示例代码如下：    public void searchItem(String id) {      getTextBox("SearchBar").clearText();      getTextBox("SearchBar").setText(id);      getButton("Search").click();    }在代码中，搜索条控件被定义为 SearchBar，通过调用代理层的 getTextBox方法，得到一个文本输入框类型对象，并调用该对象的清除方法。然后，在对应的控件配置文件中添加对应的自定义名称和控件的定位方法。这样一来，即便控件发生改变，对于实际操作层的代码来说，由于采用的是自定义名称，所以你不需要修改逻辑，只要在对应的控件配置文件中，替换控件的定位方法就行了。关于具体的控件配置文件，示例代码如下：                                                      //XCUIElementTypeNavigatorBar[@name="MainPageView"]/XCUIElementTypeOther/...                                        当然，为了简化测试人员的编写用例成本，你可以在操作层使用 Page-Object模式，针对页面或模块封装操作方式，通过一种符合认知的方式，来实现具体的功能操作。这样一来，在实际编写用例的时候，你就可以非常简单地调用操作层的接口定义。示例代码如下：    @TestDriver(driverClass = AppiumDriver.class)    public void TC001() {      String id='10000'      page.main.switchView(3);      page.cart.clearShoppingCart();      page.main.switchView(0);      page.search.searchProduct(id);      page.infolist.selectlist(0);      page.infodetail.clickAddCart();      Assert.assertTrue(page.cart.isProductCartExist(), " 商品添加成功 ")    }从这些示例中，我们可以看出，一个良好的自动化测试框架，可以显著降低测试人员编写测试用例的门槛，以及测试用例的维护成本。对于一个成熟的平台来说，平台易用性是非常重要的能力，通过DSL方式来声明测试过程，可以让测试人员聚焦在测试业务逻辑的设计和构建上，大大提升自动化测试的实现效率。关于自动化测试框架的能力模型，我给你分享你一份资料，你可以点击网盘获取，提取码是gk9w。这个能力模型从测试脚本封装、测试数据解耦、测试流程编排、报告生成等多个方面，展示了框架建设的各个阶段应该具备的能力。自动化测试结果分析那么，我们该如何衡量自动化测试的结果呢？当前比较常用的方式是**覆盖率**，不过问题是，测试覆盖率提升就能发现更多的缺陷吗？一家大型金融公司的单元测试覆盖率达到了 80%，接口覆盖率更是达到了100%，从这个角度来看，他们的自动化测试做得相当不错。但是，当我问到自动化测试发现的问题数量占到整体问题的比例时，他们的回答有点出人意料。在这么高的覆盖率基础上，自动化测试发现的问题占比仅仅在5% 左右。那么，花了这么大力气建设的自动化测试，最后仅仅发现了 5%的有效问题，这是不是说明自动化测试的投入产出比不高呢？实际上，说自动化测试是为了发现更多的缺陷，这是一个典型的认知误区。在实际项目中，手工测试发现的缺陷数量要比自动化测试发现的缺陷数量多得多。自动化测试更多是在帮助守住软件质量的底线，尤其是应用在回归测试中，自动化测试可以确保工作正常的已有功能不会因为新功能的引入而带来质量回退。可以这么说，**如果自动化测试覆盖率足够高，那么软件质量一定不会差到哪儿去**。在自动化测试领域，除了追求覆盖率一个指标以外，自动化测试的结果分析也值得重点关注一下。如果自动化测试的结果并不准确，甚至带来大量误报的话，这对团队来说反而是一种干扰。关于测试误报，是指由于非开发代码变更导致的自动化测试用例执行失败的情况。业界对于误报率的普遍定义是：>  > 自动化测试误报率 = 非开发变更引入的问题用例数量 /> 测试失败的用例数量> > >>  > 比如，单次自动化测试执行了 100 个用例，其中有 20 个用例失败，这 20> 个失败用例有 5> 个是由于本次功能或代码变更引入的，也就是真实的缺陷，那么误报率就等于：（20 -> 5）/20 = 75%> > >**测试误报率是体现自动化测试稳定性的一个核心指标**。对于不同测试类型和产品形态，误报的的原因有很多。比如测试环境的网络不稳定导致的连接超时、测试脚本和测试工具本身的固有缺陷导致的执行失败、测试数据不齐备、测试资源不可用等等。由于测试误报的客观存在，即便执行了自动化测试并给出了测试结果，但还是需要人工审查判断之后，才能将真正的问题上报缺陷系统。这样一来，在自动化执行末端加入了人工处理，就导致自动化测试难以大规模推行，这也是自动化测试略显"鸡肋"的原因之一。那么，要如何解决这个问题呢？这就要依赖于自动化测试结果的分析啦。1.       对自动化测试的问题进行分类。你要弄清楚一次失败是环境问题、网络问题、功能变更，还是系统缺陷？你需要将失败的用例归纳到这些分类之中。当一个类别的问题非常多的时候，你可以考虑进行拆分，比如网络问题，你可以拆分为网络不可达、延迟超时、域名解析错误等等。        2.       增加已有分类的自动识别能力。比如，对于捕获到的常见异常，可以根据异常信息自动上报到对应的错误分类，从而简化人工识别和归类错误的工作量。        3.       提升自动化测试工具和环境的健壮性，对已知问题增加一定的重试机制。        4.       持续积累和丰富错误分类，有针对性地开展改进工作，从而不断提升自动化测试的稳定性。        我跟你分享一幅某公司的自动化测试结果分析示意图。通过统计错误的分类，可以看出错误的占比情况，并且针对常见的误报类型进行有针对性的优化，并建立度量指标来跟踪长期结果，从而保证自动化测试结果的整体可信度。这些工作都需要长期的投入才能看出成效，这也是让自动化测试价值最大化和团队能力提升的必经之路。![](Images/681068fa6a2ee509d06fe51cdcd2437b.png)savepage-src="https://static001.geekbang.org/resource/image/70/07/70445e2161b3c447c7d21384da947e07.png"}总结总结一下，这一讲我给你介绍了有关自动化测试的四个方面，包括自动化测试要解决的问题和适用场景、实施的路径、框架工具开发的典型思路以及结果分析的要点。希望能够帮你建立起对自动化测试这个"老大难"问题的全面认知，让你在推进自动化测试能力建设的时候有迹可循。思考题你所在的企业在进行自动化建设时，有哪些困境和问题，你们是如何解决的呢？欢迎在留言区写下你的思考和答案，我们一起讨论，共同学习进步。如果你觉得这篇文章对你有所帮助，欢迎你把文章分享给你的朋友。![](Images/94ddfb3c31810c68bfd0097449ef5eeb.png)savepage-src="https://static001.geekbang.org/resource/image/7c/33/7c26a9b917677371cf3aac78d949ae33.jpg"}
# 14 \| 内建质量：丰田和亚马逊给我们的启示你好，我是石雪峰，今天我来跟你聊一个非常重要的话题：内建质量。 我之前给你讲过一个故事，说的是在美国汽车工厂装配流水线的末端，总是有个人在拿着橡胶锤子敲打车门，以检查车门是否安装良好。我还说，如果一个公司要靠"拿锤子的人"来保证质量，这就说明，这个公司的流程本身可能就有问题。 这个观点并不是我凭空捏造出来的，而是来自于质量管理大师爱德华·戴明博士经典的质量管理14 条原则。其中，第 3 条指出，**不应该将质量依赖于检验工作，因为检验工作既昂贵，又不可靠。最重要的是，检验工作并不直接提升产品质量，只是为了证明质量有缺陷**。而正确的做法是将质量内建于整个流程之中，并通过有效的控制手段来证明流程自身的有效性。 为什么内建质量如此重要？在传统的软件开发过程中，检验质量的"锤子"往往都握在测试团队的手中。他们在软件交付的末端，通过一系列的"锤子"来"敲打"软件产品的方方面面，试图找到一些潜在的问题。 这样做的问题是，测试通过尽可能全面的回归测试来验证产品质量符合预期，成本是巨大的，但是效果却不见得有多好。 因为测试只能基于已知的产品设计进行验证，但那些潜在的风险有可能连开发自己都不知道。比如，开发引入了一些第三方的类库，但这些库本身存在缺陷，那么，如果测试没有回归到这个场景，就很有可能出现漏测和生产事故。 另外，由于测试存在的意义在于发现更多的缺陷，有些团队的考核指标甚至直接关联缺陷提交数量，以及缺陷修复数量。那么，这里的前提就是假设产品是存在缺陷的。于是，测试团队为了发现问题而发现问题，在研发后面围追堵截，这也造成了开发和测试之间的隔阂和对立，这显然不是DevOps 所倡导的状态。 那么，解决这个问题的正确"姿势"，就是内建质量啦！ 关于内建质量，有个经典的案例就是丰田公司的安灯系统，也叫作安灯拉绳。丰田的汽车生产线上方有一条绳子，如果生产线上的员工发现了质量问题，就可以拉动安灯系统通知管理人员，并停止生产线，以避免带有缺陷的产品不断流向下游。 要知道，在生产制造业中，生产线恨不得 24小时运转，因为这样可以最大化地利用时间，生产更多的产品。可是现在，随随便便一个员工就可以让整条生产线停转，丰田公司是怎么想的呢？ 其实，这背后的理念就是"Failfast"，即快速失败。如果工人发现了有缺陷的产品，却要经过层层审批才能停止生产线，就会有大量带有缺陷的产品流向下游，所以，停止生产线并不是目的，及时发现问题和解决问题才是目的。 当启动安灯系统之后，管理人员、产线质量控制人员等相关人员会立刻聚集到一起解决这个问题，并尽快使生产线重新恢复运转。更重要的是，这些经验会被积累下来，并融入组织的能力之中。 内建质量扭转了看待产品质量的根本视角，也就是说，**团队所做的一切不是为了验证产品存在问题，而是为了确保产品没有问题**。 几年前，我在华为参加转正答辩的时候，被问到一个问题："华为的质量观是怎样的？"答案是三个字："零缺陷。"我当时并不理解，人非圣贤，孰能无过？产品零缺陷简直就是反常理。但是，后来我慢慢明白，所谓零缺陷，并不是说产品的Bug数量等于零，这其实是一种质量观念，倡导全员质量管理，构建质量文化。每一个人在工作的时候，都要力争第一时间发现和解决缺陷。 所以，总结一下，内建质量有两个核心原则： 1.  问题发现得越早，修复成本就越低；        2.  质量是每个人的责任，而不是质量团队的责任。        说了这么多，你应该已经对内建质量有了初步的认识。那么接下来，我来给你介绍下内建质量的实践思路、操作步骤、常见问题以及应对方法。 内建质量的实施思路既然是内建质量，那么，我们就应该在软件交付的各个环节中注入质量控制的能力。 在需求环节，可以定义清晰的需求准入规则，比如需求的价值衡量指标是否客观、需求的技术可行性是否经过了验证、需求的依赖是否充分评估、需求描述是否清晰、需求拆分是否合理、需求验收条件是否明确等等。 通过前置需求质量控制，可以减少不靠谱的需求流入。在很多公司，"一句话需求"和"老板需求"是非常典型的例子。由于没有进行充分沟通，研发就跟着感觉走，结果交付出来的东西完全不是想要的，这就带来了返工浪费。 **在开发阶段，代码评审和持续集成就是一个非常好的内建质量的实践**。在代码评审中，要尽量确认编码是否和需求相匹配，业务逻辑是否清晰。另外，通过一系列的自动化检查机制，来验证编码风格、风险、安全漏洞等。 在测试阶段，可以通过各类自动化测试，以及手工探索测试，覆盖安全、性能、可靠性等，来保障产品质量；在部署和发布阶段，可以增加数据库监控、危险操作扫描、线上业务监控等多种手段。 从实践的角度来说，每个环节都可以控制质量，那么，我们要优先加强哪个环节呢？ 根据内建质量的第一原则，我们知道，如果可以在代码刚刚提交的时候就发现和修复缺陷，成本和影响都是最低的。如果等到产品上线后，发现了线上质量问题，再回过头来定位和修复问题，并重新发布软件，成本将会呈指数级增长。 所以，**研发环节作为整个软件产品的源头，是内建质量的最佳选择**。那么，具体要怎么实施呢？ 内建质量的实施步骤**第一步：选择适合的检查类型** 以持续集成阶段的代码检查为例，除了有单元测试、代码风格检查、代码缺陷和漏洞检查、安全检查等等，还有各种各样的检查工具。但实际上，这些并不是都需要的。至少在刚开始实践的时候，如果一股脑全上，那么研发基本上就不用干活了。 所以，**选择投入产出比相对比较高的检查类型，是一种合理的策略**。比如代码风格与缺陷漏洞相比，检查缺陷漏洞显然更加重要，因为一旦发生代码缺陷和漏洞，就会引发线上事故。所以，这么看来，如果是客户端业务，Infer扫描就可以优先实施起来。虽然我们不能忽视编码风格问题，但这并不是需要第一时间强制执行的。 **第二步：定义指标并达成一致** 确定检查类型之后，就要定义具体的质量指标了。质量指标分两个层面，一个是指标项，一个是参考值，我分别来介绍一下。 **指标项是针对检查类型所采纳的具体指标**，比如单元测试覆盖率这个检查项，可采纳的指标就包括行、指令、类、函数等。那么，我们要以哪个为准呢？这个一般需要同研发负责人达成一致，并兼顾行业的一些典型做法，比如单测行覆盖率就是一个比较好的选择。 另外，很多时候，在既有项目启用检查的时候，都会有大量的技术债。关于技术债，我会在下一讲展开介绍。简单来说，就是欠了一堆债，一时半会儿又还不了，怎么办呢？这个时候，比较合适的做法就是选择动态指标，比如增量代码覆盖率，也就是只关注增量代码的情况，对存量代码暂不做要求。 指标项定义明确之后，就要定义参考值了。这个参考值会直接影响质量门禁是否生效，以及生效后的行为。 我简单介绍下质量门禁。质量门禁就类似一道安全门，通过门禁时进行检查，如果不满足指标，则门禁报警，禁止通过。这就跟交警查酒驾一样，酒精含量如果超过一定的指标，就会触发报警。 **参考值的定义是一门艺术**。对于不同的项目，甚至是同一个项目的不同模块来说，我们很难用"一刀切"的方式定义数值。我比较推荐的做法是**将静态指标和动态指标结合起来使用**。 **静态指标就是固定值**，对于漏洞、安全等问题来说，采取零容忍的态度，只要存在就绝不放过。而**动态指标是以考查增量和趋势为主**，比如基线值是 100，你就可以将参考值定义成小于等于100，也就是不允许增加。你还可以根据不同的问题等级，定义不同的参考值，比如严格检查致命和阻塞问题，其余的不做限制。 最后，对于这个指标，你一定要跟研发团队达成共识，也就是说，团队要能够认可并且执行下去。所以，定义指标的时候要充分采纳对方的建议。 **第三步：建立自动化执行和检查能力** 无论公司使用的是开源工具还是自研工具，都需要支持自动化执行和检查的能力。根据检查时机的不同，你也可以在提测平台、发布平台上集成质量门禁的功能，并给出检查结果的反馈。 **按照快速失败的原则，质量门禁的生效节点要尽量靠近指标数据的产生环节**。比如，如果要检查编码风格，最佳的时间点是在研发本地的 IDE中进行，其次是在版本控制系统中进行并反馈结果，而不是到了最后发布的时间点再反馈失败。 现代持续交付流水线平台都具备质量门禁的功能，常见的配置和生效方式有两种： 1.       在持续交付平台上配置规则，也就是不同指标和参考值组合起来，形成一组规则，并将规则关联到具体的执行任务中。这样做的好处是，各个生成指标数据的子系统只需要将数据提供给持续交付平台就行了，至于门禁是否通过，完全依靠持续交付平台进行判断。另外，一般配置规则的都是质量人员，提供这样一个单独的入口，可以简化配置成本。具体的实现逻辑，如图所示：        ![](Images/8b4a863f60b9cbf38e70239a9e31c9b7.png)savepage-src="https://static001.geekbang.org/resource/image/16/fb/16168a5d172c4839ef98b201fcdd5dfb.png"}1.       在各个子系统中配置质量门禁。比如，在 UI    自动化测试平台上配置门禁的指标，当持续交付平台调用 UI    自动化测试的时候，直接反馈门禁判断的结果。如果检查不通过，则流水线直接失败。        **第四步：定义问题处理方式** 完成以上三步之后，就已经开始进行自动化检查了，而检查的结果和处理方式，对质量门禁能否真正起到作用非常重要。一般来说，质量门禁都具有强制属性，也就是说，如果没有达到检查指标，就会立即停止并给予反馈。 在实际执行的过程中，质量门禁的结果可能存在多种选项，比如失败、告警、人工确认等。这些都需要在制定规则的时候定义清楚，通过一定的告警值和人工确认方式，可以对质量进行渐进式管控，以达到持续优化的目标。 另外，你需要对所有软件交付团队成员宣导质量规则和门禁标准，并明确通知方式、失败的处理方式等。否则，检查出问题却没人处理，这个门禁就形同虚设了。 **第五步：持续优化和改进** 无论是检查能力、指标、参考值，还是处理方式，只有在运行起来后才能知道是否有问题。所以，在推行的初期，也应该具备一定程度的灵活性，比如对指标规则的修订、指标级别和参考值的调整等，**核心目标不是为了通过质量门禁，而是为了质量提升，这才是最重要的**。 内建质量的常见问题内建质量说起来并不复杂，但想要执行到位却很困难，那么，到底有哪些常见的问题呢？我总结了一些常见问题和处理建议，做成了表格，你可以参考一下。 ![](Images/e4d5960ccc822188b11e4a0db63bb32b.png)savepage-src="https://static001.geekbang.org/resource/image/5f/fe/5fe50bad00fced809adcbd31775ed3fe.jpg"}最后，我再给你分享一个亚马逊的故事。2012年，安灯系统被引入亚马逊公司，一线客服如果收到客户反馈或者观察到商品有潜在的质量和安全风险，就可以发出告警邮件，并将商品设置为"不可购买"的状态，说白了，就是强制下架。客服居然可以不经过任何审批，直接把商品下架，不怕遭到供应商的投诉吗？ 实际上，这正是亚马逊践行以客户为中心的理念和原则的真实写照，**每个人都为最终质量负责，没有例外**。当员工得知自己被赋予了这样大的权限时，每个人都会尽自己的力量为质量工作加分。即便偶尔会有错误操作，这也是团队内部难能可贵的学习经验。 在公司中，无论是建立质量门禁的规则，还是开发一套平台系统，其实都不是最困难的事情，难的是，在实际过程中，有多少正常流程走了特殊审批？有多少发布是走的紧急通道？又有多少人会说开启了质量门禁，就会阻碍业务交付？ 说到底，还是要问问自己，你愿意付出多少代价，来践行自己的理念和原则，先上再说？我想，能在这一点上达成共识，才是内建质量落地的终极要素吧。 总结总结一下，在这一讲中，我通过两个故事给你介绍了内建质量的背景和原则，那就是尽早发现问题，尽早修复，以及每个人都是质量的负责人。另外，我还给你介绍了实施内建质量的五个常见步骤。希望你始终记得，质量是生产出来的，而不是测试出来的。掌握了内建质量，你就揭开了DevOps高效率和高质量并存的秘密。 思考题你所在的企业中是否启用了强制的质量门禁呢？可以分享一些你觉得效果良好的规则吗？ 欢迎在留言区写下你的思考和答案，我们一起讨论，共同学习进步。如果你觉得这篇文章对你有所帮助，欢迎你把文章分享给你的朋友。 ![](Images/94ddfb3c31810c68bfd0097449ef5eeb.png)savepage-src="https://static001.geekbang.org/resource/image/7c/33/7c26a9b917677371cf3aac78d949ae33.jpg"}