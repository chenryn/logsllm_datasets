referenced in the FROM clause and sort the records according to the SEQ ﬁeld. We
then process each record in sequence and update the UPDATE BY variables (which are
indexed in a hash table for fast lookup). After processing each record, we evaluate the
HAVING clause to determine whether or not to construct an output record.
4 Case Study
Our case study involves an active measurement infrastructure of a large ISP that period-
ically measures packet loss and delay from each monitor server to a set of destinations
(other monitors and routers). To conduct the measurements, a monitor launches a probe
train to a destination, which reﬂects the received packets back to the source. The moni-
tors, their placement, and the set of destinations probed by each monitor are set up such
that the union of all the source-destination probe pairs covers all the end-end paths in
each network. Globally, this results in thousands of measurements every 15 minutes.
In real time, network operators monitor these measurements, identify serious per-
formance events where the loss (or delay) for a source-destination pair exceeds a crit-
ical threshold for sustained time periods, and conduct troubleshooting. Traditionally
performed manually, computing these events involves joins across multiple data feeds
(multiple types of probes, conﬁguration data), requires tracking many networks, and is
time consuming and error prone. Our task is to automate the creation of these incidents
to ensure greater accuracy, and to enable operators to move away from tedious number-
crunching across multiple screens so they can focus on troubleshooting the outages.
The input consists of loss and delay measurements, and a conﬁguration feed provid-
ing various details for each pair, such as the loss/delay thresholds. A source-destination
pair causes an alarm once the metric (loss or delay) exceeds a critical threshold for four
consecutive time intervals. We want to automate the identiﬁcation of loss and delay
alarms of the form (start time, end time, source, destination, average loss/delay). We
will only discuss loss alarms from now on, as delay alarms follow a similar logic.
Suppose that the following sequence of (date, time, loss percentage) records is gen-
erated by a probe between source S and destination D. Suppose that the loss threshold
is one percent. We label measurements exceeding the threshold as red; black otherwise:
2011-01-01, 9:00, 0.8, BLACK
2011-01-01, 9:15, 1.5, RED
2011-01-01, 9:30, 1.8, RED
A Sequence-Oriented Stream Warehouse Paradigm
59
(cid:87)(cid:90)(cid:75)(cid:17)(cid:28)(cid:94)(cid:890)(cid:17)(cid:62)(cid:4)(cid:18)(cid:60)
(cid:87)(cid:90)(cid:75)(cid:17)(cid:28)(cid:94)(cid:890)(cid:17)(cid:62)(cid:4)(cid:18)(cid:60)(cid:1006)(cid:90)(cid:28)(cid:24)
(cid:87)(cid:90)(cid:75)(cid:17)(cid:28)(cid:94)
(cid:47)(cid:69)(cid:100)(cid:28)(cid:90)(cid:115)(cid:4)(cid:62)(cid:94)
(cid:4)(cid:62)(cid:4)(cid:90)(cid:68)(cid:94)
(cid:87)(cid:90)(cid:75)(cid:17)(cid:28)(cid:94)(cid:890)(cid:90)(cid:28)(cid:24)
Fig. 2. Implementing a network alerting application using standard SQL views
2011-01-01, 9:45, 1.6, RED
2011-01-01. 10:00, 2.0, RED
2011-01-01, 10:15, 1.8, RED
2011-01-01. 10:30, 0.2, BLACK
At time 9:00, the loss percentage is below threshold (black). At 9:15, it rises above the
threshold (red), but an alarm is not yet raised; likewise at 9:30 and 9:45. At 10:00, we
have four consecutive red measurements and we output the following:
2010-01-01 9:15, 2010-01-01 10:00, S, D, 1.63.
An alarm for this pair continues at 10:15 because the new measurement continues to
exceed the critical threshold, and at that time we output
2010-01-01 9:15, 2010-01-01 10:15, S, D, 1.73.
The loss percentage drops below threshold at 10:30, so we do not output anything for
this pair at that time.
This logic may be expressed as follows. For each pair, if its current measurement is
red, then we compute the time of the ﬁrst red measurement that occurred after its most
recent black measurement. In the above example, the processing at time 10:00 is to look
up the most recent black measurement time of 9:00 and conclude that the start of the
consecutive red sequence is at time 9:15.
Figure 2 shows the tables that implement loss alarms within DataDepot using stan-
dard (non-sequential) SQL. The PROBES table collects loss measurements (and cor-
relates various probe conﬁguration information such as critical loss threshold via a
join with the conﬁguration table). PROBES is partitioned into 15-minute time bins so
that new data can be loaded into the most recent partition without affecting the rest of
the table. The remaining ﬁve tables, also partitioned into 15-minute time bins, imple-
ment the application logic. PROBES BLACK and PROBES RED select black and red
measurements, respectively, PROBES BLACK2RED selects red measurements that oc-
curred immediately after black measurements, INTERVALS constructs intervals with
consecutive red measurements, and ALARMS computes intervals that contain at least
four red measurements. For each of these tables, we also need to specify the relation-
ship between its partitions and those of its sources (recall Figure 1); this informa-
tion is crucial to ensuring efﬁcient update propagation. For example, the current par-
tition of PROBES BLACK2RED can be computed by accessing the current partition
of PROBES RED and the previous-to-current partition of PROBES BLACK. Without
these partition relationships, the database would have no choice but to scan the entire
history of PROBES RED and PROBES BLACK when computing a single 15-minute
partition of PROBES BLACK2RED. The ﬁnal piece is a Web-based front end that dis-
plays the current alarms and is refreshed by querying the ALARMS table.
With standard SQL, we are forced to simulate sequential analysis with complex and
difﬁcult-to-optimize set operations. For example, the INTERVALS table selects the
60
L. Golab et al.
source-destination pairs with currently red measurements (from PROBES RED) and
looks up the most recent record for that pair in PROBES BLACK2RED using a NOT
EXISTS operator:
SELECT R.Source, R.Destination, B.Ts, R.Ts
FROM PROBES_RED R, PROBES_BLACK2RED B
WHERE R.Source=B.Source
AND R.Destination=B.Destination AND R.Ts >= B.Ts
AND NOT EXISTS{
SELECT Timestamp FROM PROBES_BLACK2RED B2
WHERE B2.Source=B.Source
AND B2.Destination = B.Destination
AND B2.Ts > B.Ts )
That is, we check that there does NOT EXIST another record for that pair in
PROBES BLACK2RED with a larger timestamp (denoted Ts).
In contrast, only one table sufﬁces to implement the loss alarms logic using our
sequence-oriented extensions (Ts is the timestamp ﬁeld and Loss is the loss value; for
simplicity, assume that the critical loss threshold is one percent):
SELECT Source, Destination, Ts-(red_ct*900), Ts, sum_loss/red_ct
FROM PROBES
GROUP BY Source, Destination, Ts SEQ
UPDATE BY
(Loss>1% AND first_red[1]1% ? Ts : 0) AS first_red,
(Losslast_black ? red_ct[1]+1 : 0)
INITIALLY (Loss>1% ? 1 : 0) AS red_ct
(first_red>last_black ? sum_loss[1]+Loss : 0)
INITIALLY (Loss>1% ? Loss : 0) AS sum_loss
HAVING red_ct >= 4
The logic is expressed using four UPDATE BY variables. For each pair, first_red,
keeps track of the timestamp of the ﬁrst red measurement after the most recent black
one, last_black maintains the time of the most recent black measurement, red_ct
counts the number of red measurements in the current alarm interval and sum_loss
sums up the loss values over the current alarm interval. In the SELECT clause, we
return the Source and Destination points of the given pair, the starting time of the alarm
(which is simply the current timestamp minus the number of red measurements times
900 seconds), the current time, and average loss during the alarm interval (computed by
diving sum_loss by the number of red measurements). The HAVING clause ensures
that we output an alarm only if we have seen at least four consecutive red measurements.
The sequential ALARMS table also needs to specify a partition relationship with
PROBES. Assuming that alarms do not last longer than six hours, we only need to scan
the most recent six hours of PROBES when computing a new partition of ALARMS.
We note that the real-time features of DataDepot were crucial in enabling this appli-
cation: views are automatically maintained as new data arrive and use multi-version
A Sequence-Oriented Stream Warehouse Paradigm
61
(cid:4)(cid:448)(cid:286)(cid:396)(cid:258)(cid:336)(cid:286)(cid:3)(cid:410)(cid:349)(cid:373)(cid:286)(cid:3)(cid:410)(cid:381)(cid:3)(cid:367)(cid:381)(cid:258)(cid:282)(cid:3)(cid:258)(cid:374)(cid:282)(cid:3)(cid:393)(cid:396)(cid:381)(cid:393)(cid:258)(cid:336)(cid:258)(cid:410)(cid:286)(cid:3)(cid:374)(cid:286)(cid:449)(cid:3)(cid:282)(cid:258)(cid:410)(cid:258)
(cid:400)
(cid:282)
(cid:374)
(cid:381)
(cid:272)
(cid:286)
(cid:94)
(cid:1007)(cid:1009)
(cid:1007)(cid:1004)
(cid:1006)(cid:1009)
(cid:1006)(cid:1004)
(cid:1005)(cid:1009)
(cid:1005)(cid:1004)
(cid:1009)
(cid:1004)
(cid:87)(cid:90)(cid:75)(cid:17)(cid:28)(cid:94)(cid:890)(cid:90)(cid:28)(cid:24)
(cid:87)(cid:90)(cid:75)(cid:17)(cid:28)(cid:94)(cid:890)(cid:17)(cid:62)(cid:4)(cid:18)(cid:60)
(cid:87)(cid:90)(cid:75)(cid:17)(cid:28)(cid:94)(cid:890)(cid:17)(cid:62)(cid:4)(cid:18)(cid:60)(cid:1006)(cid:90)(cid:28)(cid:24)
(cid:47)(cid:69)(cid:100)(cid:28)(cid:90)(cid:115)(cid:4)(cid:62)(cid:94)
(cid:4)(cid:62)(cid:4)(cid:90)(cid:68)(cid:94)
(cid:94)(cid:286)(cid:395)(cid:437)(cid:286)(cid:374)(cid:272)(cid:286)(cid:3)(cid:94)(cid:89)(cid:62)
(cid:90)(cid:286)(cid:336)(cid:437)(cid:367)(cid:258)(cid:396)(cid:3)(cid:94)(cid:89)(cid:62)
Fig. 3. Comparison of application refresh times
concurrency control so they can be queried at any time, and we have used multi-
granularity partitions, with small recent partitions for efﬁciency and larger historical
partitions to store a very long history of alarms.
5 Experiments
We now show that our network monitoring application can be efﬁciently maintained us-
ing the proposed sequence extensions. We also discuss the performance of our solution
in the Darkstar warehouse, which is one of several network data warehouses maintained
by DataDepot. Darkstar is run by a large ISP, loads over 100 raw data feeds, maintains
over 300 tables and materialized views, and ingests more that 340 million raw records
per day. Darkstar consists of an application server (2.86 GHz Xeon chip, 4 cores, 48 Gb
of RAM, 4Gb ﬁber channel to secondary storage) and a cluster of database servers.
We begin with a comparison of the update time of our application using the hier-
archy of regular SQL views from Figure 2 versus using the single sequential view. To
control this experiment, we executed updates on the application server rather than al-
lowing the live warehouse to schedule them. We measured the time to update each table
over a period of one day (new data arrive every 15 minutes, so there were 96 updates)
and report the average update times in Figure 3. Using regular SQL, it takes over 30
seconds to process a 15-minute batch of data through all the intermediate tables, with
PROBES BLACK2RED and INTERVALS alone taking around 10 seconds each. Using
sequential SQL, it only takes an average of ten seconds to update the single view. Our
sequence extensions also save space since there is only one view, not ﬁve, to store.
We now discuss the performance of the live warehouse. Over a nine-day period,
we measured the time to propagate raw probe data to the ALARMS table. We found
that the end-to-end update propagation time (which includes waiting times while other
tables are being updated) was well under ﬁve minutes 97 percent of the time. This is far
below the 15-minute inter-arrival time of the raw data. Thus, in spite of the high degree
of activity (hundreds of complex views maintained in nearly-real time), we were still
able to provide timely updates, with the DataDepot scheduler allocating resources to
“stale” tables without starving other tables. Of course, a network ﬁrestorm may cause
many feeds to produce huge data volumes, in which case we may have to perform load
shedding such as sampling.
62
L. Golab et al.
6 Conclusions
Rather than viewing data warehouses as a convenient way to organize data for higher-
level applications, this paper advocated implementing real-time network monitoring
applications within the warehouse as collections of materialized views. We presented
a novel extension to the SQL query language that natively supports sequence-oriented
analysis. To illustrate the feasibility of our approach, we presented a network alert-
ing application that we implemented using the proposed method inside a production
data warehouse. We experimentally showed a 3-fold performance improvement as well
as a signiﬁcant reduction in application complexity thanks to the proposed sequence-
oriented extensions. We hope that this paper stimulates further research on the role of
database technologies in network management, and encourages the adoption of these
ideas by researchers and practitioners.
References
1. Agrawal, J., et al.: Efﬁcient pattern matching over event streams. In: SIGMOD 2008, pp.
147–160 (2008)
2. Ahuja, M., et al.: Peta-scale data warehousing at Yahoo! In: SIGMOD 2009, pp. 855–862
(2009)
3. Balazinska, M., et al.: Moirae: History-enhanced monitoring. In: CIDR 2007, pp. 375–386
(2007)
4. Cranor, C., et al.: A stream database for network applications. In: SIGMOD 2003, pp. 647–
651 (2003)
5. Deri, L., Lorenzetti, V., Mortimer, S.: Collection and Exploration of Large Data Monitoring
Sets Using Bitmap Databases. In: Ricciato, F., Mellia, M., Biersack, E. (eds.) TMA 2010.
LNCS, vol. 6003, pp. 73–86. Springer, Heidelberg (2010)
6. Desnoyers, P., Shenoy, P.J.: Hyperion: High volume stream archival for retrospective query-
ing. In: USENIX Annual Technical Conference, pp. 45–58 (2007)
7. Eriksson, B., et al.: Basisdetect: a model-based network event detection framework. In: IMC
2010, pp. 451–464 (2010)
8. Golab, L., et al.: Stream warehousing with DataDepot. In: SIGMOD 2009, pp. 847–854
(2009)
9. Golab, L., Johnson, T., Shkapenyuk, V.: Scheduling updates in a real-time stream warehouse.
In: ICDE 2009, pp. 1207–1210 (2009)
10. Jain, N., et al.: Towards a streaming SQL standard. Proc. of the VLDB Endowment 1(2),
1379–1390 (2008)
11. Kalmanek, C., et al.: Darkstar: Using exploratory data mining to raise the bar on network
reliability and performance. In: DRCN 2009 (2009)
12. Li, X., et al.: Advanced indexing techniques for wide-area network monitoring. In: NetDB
2005 (2005)
13. Maier, G., et al.: Enriching network security analysis with time travel. SIGCOMM Comput.
Commun. Rev. 38, 183–194 (2008)
14. Markopoulou, A., et al.: Characterization of failures in an operational ip backbone network.
IEEE/ACM Trans. Netw. 16(4), 749–762 (2008)
15. Papadogiannakis, A., Polychronakis, M., Markatos, E.P.: RRDtrace: Long-term raw network
trafﬁc recording using ﬁxed-size storage. In: MASCOTS 2010, pp. 101–110 (2010)
A Sequence-Oriented Stream Warehouse Paradigm
63
16. Qiu, T., et al.: What happened in my network: mining network events from router syslogs.
In: IMC 2010, pp. 472–484 (2010)
17. Quass, D., Widom, J.: On-line warehouse view maintenance. In: SIGMOD 1997, pp. 393–
404 (1997)
18. Reiss, F., et al.: Enabling real-time querying of live and historical stream data. In: SSDBM
2007, p. 28 (2007)