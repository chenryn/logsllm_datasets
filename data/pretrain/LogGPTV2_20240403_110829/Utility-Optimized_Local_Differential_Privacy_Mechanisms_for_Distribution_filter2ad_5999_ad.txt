tion 5 can be simpliﬁed as follows:
nπ · |XS|√|XS|−1
ε
.
(15)
E [l1(ˆp,pUN )] ≈(cid:113) 2
|X|√|X|−1
(cid:113) 2
ε
nπ
It was shown in [29] that the expected l1 loss of the ε-RR is
when ε ≈ 0. The right-hand side of
at most
(15) is much smaller than this when |XS| (cid:28) |X|. Although
both of them are “upper-bounds” of the expected l1 losses, we
show that the total variation of the (XS,ε)-uRR is also much
smaller than that of the ε-RR when |XS| (cid:28) |X| in Section 6.
uRR in the low privacy regime. When ε = ln|X| and
|XS| (cid:28) |X|, the right-hand side of (14) in Proposition 6 can
be simpliﬁed by using |XS|/|X| ≈ 0:
E [l1(ˆp,p∗)] ≈(cid:113) 2(|X|−1)
nπ
.
USENIX Association
28th USENIX Security Symposium    1883
nπ
(cid:113) 2(|X|−1)
It should be noted that the expected l1 loss of the non-private
mechanism, which does not obfuscate the personal data at
[29]. Thus, when ε = ln|X| and
all, is at most
|XS| (cid:28) |X|, the (XS,ε)-uRR achieves almost the same data
utility as the non-private mechanism, whereas the expected l1
loss of the ε-RR is twice larger than that of the non-private
mechanism [29].
uRAP in the general case. We then analyze the uRAP:
Proposition 7 (l1 loss of the uRAP). Let ε ∈ R≥0, u(cid:48) = eε/2−
1, and vN = eε/2
. The expected l1-loss of the (XS,ε)-uRAP
eε/2−1
mechanism is:
E [l1(ˆp,p)] ≈(cid:113) 2
nπ
(cid:18)|XS|
∑
(cid:113)(cid:0)p(x j) + 1/u(cid:48)(cid:1)(cid:0)vN − p(x j)(cid:1)
p(x j)(cid:0)vN − p(x j)(cid:1)(cid:19)
(cid:113)
,
(16)
j=1
|X|
∑
+
j=|XS|+1
uRAP in the low privacy regime. If ε = ln|X| and |XS| (cid:28)
|X| 3
4 , the right-hand side of (17) can be simpliﬁed, using
|XS|/|X| 3
4 ≈ 0, as follows:
E [l1(ˆp,pUN )] ≈(cid:113) 2(|X|−1)
.
nπ
Thus, when ε = ln|X| and |XS| (cid:28) |X| 3
4 , the (XS,ε)-uRAP
also achieves almost the same data utility as the non-private
mechanism, whereas the expected l1 loss of the ε-RAPPOR
is(cid:112)|X| times larger than that of the non-private mechanism
[29].
Summary. In summary, the uRR and uRAP provide much
higher utility than the RR and RAPPOR when |XS| (cid:28) |X|.
Moreover, when ε = ln|X| and |XS| (cid:28) |X| (resp. |XS| (cid:28)
|X| 3
4 ), the uRR (resp. uRAP) achieves almost the same utility
as a non-private mechanism.
where f (n) ≈ g(n) represents limn→∞ f (n)/g(n) = 1.
5 Personalized ULDP Mechanisms
When 0 < ε < 2ln(
|XN|
2 + 1), the l1 loss is maximized by
the uniform distribution pUN over XN:
Proposition 8. For any 0 < ε < 2ln(
|XN|, (16) is maximized when p = pUN :
(cid:114)
E [l1(ˆp,p)] (cid:46) E [l1(ˆp,pUN )]
eε/4|XS|
eε/2−1
(cid:113) 2
(cid:18)
=
+
nπ
|XN|
2 + 1) and |XS| ≤
(cid:19)
,
eε/2|XN|
eε/2−1
− 1
(17)
E [l1(ˆp,pUN )] ≈(cid:113) 2
where f (n) (cid:46) g(n) represents limn→∞ f (n)/g(n) ≤ 1.
Note that this proposition covers a wide range of ε. For
example, when |XS| ≤ |XN|, it covers both the high privacy
regime (ε ≈ 0) and low privacy regime (ε = ln|X|), since
|XN|
ln|X| < 2ln(
2 + 1). Below we instantiate the l1 loss in the
high and low privacy regimes based on this proposition.
uRAP in the high privacy regime. If ε is close to 0, we have
eε/2 − 1 ≈ ε/2. Thus, the right-hand side of (17) in Proposi-
tion 8 can be simpliﬁed as follows:
nπ · 2|XS|
ε
.
(18)
(cid:113) 2
nπ · 2|X|
It is shown in [29] that the expected l1 loss of the ε-RAPPOR
ε when ε ≈ 0. Thus, by (18), the expected
is at most
l1 loss of the (XS,ε)-uRAP is much smaller than that of the
ε-RAPPOR when |XS| (cid:28) |X|.
Moreover, by (18), the expected l1 loss of the (XS,ε)-uRAP
|XS|√
in the worst case is expressed as Θ(
nε2 ) in the high privacy
regime. As described in Section 3.2, this is “order” optimal
among all (XS,YP,ε)-ULDP mechanisms (in Appendix C.1,
we also show that the expected l2 of the (XS,ε)-uRAP is
expressed as Θ(
|XS|
nε2 )).
We now consider the personalized-mechanism scenario (out-
lined in Section 2.1), and propose a PUM (Personalized ULDP
Mechanism) to keep secret what is sensitive for each user
while enabling the data collector to estimate a distribution.
Sections 5.1 describes the PUM. Section 5.2 explains its
privacy properties. Section 5.3 proposes a method to esti-
mate the distribution p from Y obfuscated using the PUM.
Section 5.4 analyzes the data utility of the PUM.
S
5.1 PUM with κ Semantic Tags
Figure 4 shows the overview of the PUM Q(i) for the i-th
user (i = 1,2, . . . ,n). It ﬁrst deterministically maps personal
data x ∈ X to intermediate data using a pre-processor f (i)
pre,
and then maps the intermediate data to obfuscated data y ∈ Y
using a utility-optimized mechanism Qcmn common to all
users. The pre-processor f (i)
pre maps user-speciﬁc sensitive
data x ∈ X (i)
to one of κ bots: ⊥1,⊥2,··· , or ⊥κ. The κ
bots represent user-speciﬁc sensitive data, and each of them is
associated with a semantic tag such as “home” or “workplace”.
The κ semantic tags are the same for all users, and are useful
when the data collector has some background knowledge
about p conditioned on each tag. For example, a distribution of
POIs tagged as “home” or “workplace” can be easily obtained
via the Fousquare venue API [54]. Although this is not a user
distribution but a “POI distribution”, it can be used to roughly
approximate the distribution of users tagged as “home” or
“workplace”, as shown in Section 6. We deﬁne a set Z of
intermediate data by Z = X ∪{⊥1,··· ,⊥κ}, and a set ZS of
sensitive intermediate data by ZS = XS ∪{⊥1,··· ,⊥κ}.
Formally, the PUM Q(i) ﬁrst maps personal data x ∈ X to
intermediate data z ∈ Z using a pre-processor f (i)
pre : X → Z
speciﬁc to each user. The pre-processor f (i)
pre maps sensitive
1884    28th USENIX Security Symposium
USENIX Association
“home” and “workplace”, respectively, we create ⊥3 associ-
ated with “others”), and map x to this bot. It would be difﬁcult
for the data collector to obtain background knowledge about
p conditioned on such a tag. In Section 5.3, we will explain
how to estimate p in this case.
5.2 Privacy Properties
We analyze the privacy properties of the PUM Q(i). First, we
show that it provides ULDP.
Proposition 9. The PUM Q(i) (= Qcmn◦ f (i)
X (i)
S , YP,ε)-ULDP.
We also show that our PUM provides DP in that an ad-
versary who has observed y ∈ YP cannot determine, for any
i, j ∈ [n], whether it is obfuscated using Q(i) or Q( j), which
means that y ∈ YP reveals almost no information about X (i)
S :
Proposition 10. For any i, j ∈ [n], any x ∈ X , and any y ∈ YP,
pre) provides (XS∪
Q(i)(y|x) ≤ eεQ( j)(y|x).
We then analyze the secrecy of X (i)
and {⊥1,··· ,⊥κ} (i.e., f (i)
S . The data collector,
who knows the common-mechanism Qcmn, cannot obtain any
from Qcmn and y ∈ YP. Speciﬁcally,
information about X (i)
S
the data collector knows, for each z ∈ Z, whether z ∈ ZS or
not by viewing Qcmn. However, she cannot obtain any infor-
mation about X (i)
from ZS, because she does not know the
S
mapping between X (i)
pre). In ad-
S
dition, Propositions 9 and 10 guarantee that y ∈ YP reveals
almost no information about both input data and X (i)
S .
For example, assume that the i-th user obfuscates her home
x ∈ XS∪X (i)
S using the PUM Q(i), and sends y ∈ YP to the data
collector. The data collector cannot infer either x ∈ XS ∪ X (i)
or z ∈ ZS from y ∈ YP, since both Qcmn and Q(i) provide
ULDP. This means that the data collector cannot infer the fact
that she was at home from y. Furthermore, the data collector
cannot infer where her home is, since X (i)
S cannot be inferred
from Qcmn and y ∈ YP as explained above.
We need to take a little care when the i-th user obfuscates
non-sensitive data x ∈ XN \X (i)
S using Q(i) and sends y ∈ YI to
the data collector. In this case, the data collector learns x from
y, and therefore learns that x is not sensitive (i.e., x /∈ X (i)
S ).
Thus, the data collector, who knows that the user wants to hide
her home, would reduce the number of possible candidates
for her home from X to X \{x}. However, if |X| is large (e.g.,
|X| = 625 in our experiments using location data), the number
|X|− 1 of candidates is still large. Since the data collector
cannot further reduce the number of candidates using Qcmn,
her home is still kept strongly secret. In Section 7, we also
explain that the secrecy of X (i)
is achieved under reasonable
S
assumptions even when she sends multiple data.
S
Figure 4: Overview of the PUM Q(i) (= Qcmn ◦ f (i)
pre).
data x ∈ X (i)
S associated with the k-th tag (1 ≤ k ≤ κ) to the
corresponding bot ⊥k, and maps other data to themselves. Let
X (i)
S,k be a set of the i-th user’s sensitive data associated with the
k-th tag (e.g., set of regions including her primary home and
second home). Then, X (i)
1≤k≤κ X (i)
S,k,
S
and f (i)