### Situation Analysis and Anomaly Detection

The first situation is significantly more problematic than the second. Ideally, services should be able to handle an increased number of requests while maintaining a consistent average response time. However, this system is primarily used for testing purposes and has undergone numerous load and fault injection tests. Unfortunately, we do not have access to detailed information about these tests, making it challenging to determine if the detected outliers are genuine anomalies or simply the result of the testing environment. Nonetheless, these outliers are noteworthy due to their unusual values.

The worst-case scenario would be to find data points in the upper-right section of the charts, indicating both high request volumes and long response times. However, this was not observed in the tracing data, suggesting that the system can scale its workload effectively and maintain low response times even under heavy loads.

To further investigate both situations and enhance our anomaly detection capabilities, we performed an analysis of trace request workflow types. The objective was to identify any unusual occurrences in the request workflow paths. To achieve this, the OpenTracing Processor (OTP) was used to extract and map each unique trace workflow within the specified timeframe. The method outlined in Algorithm 3 was employed to retrieve this information.

### Workflow Analysis

As described in Algorithm 3, parameters from `TraceInfo` are written to CSV files. These files are then processed by the "Data Analyser" component, which groups workflow types into "Anomalous" and "Non-Anomalous" categories for visualization. The results of this analysis are presented in Figure 6.3.

**Figure 6.3: Comparison between “Anomalous” and “Non-Anomalous” service workflow types.**

Figure 6.3 clearly differentiates between the workflow types in "anomalous" and "non-anomalous" regions. A notable observation is that the anomalous regions exhibit a greater variety and quantity of request workflows. To investigate the cause, we identified the most frequently occurring workflows. However, the results were inconclusive due to the limited completeness of the tracing data. Many of the workflows were simple point-to-point calls or involved uninteresting request paths. Additionally, some of these workflows had high response times, but their paths were not relevant to our analysis. Consequently, we were unable to extend our analysis to pinpoint the root causes of the observed anomalies. 

Given these limitations, it is evident that the current dataset has been thoroughly analyzed. To improve future analyses, enhancing the tracing data or incorporating other types of data, such as monitoring and logging, would be beneficial. Future work should include testing this method with other tracing datasets to evaluate its effectiveness in identifying the root causes of anomalous behavior in services.

### Trace Quality Analysis

For the second question, we followed a similar approach using the OTP to process the tracing data and gather results for further analysis in the "Data Analysis" component. The analysis was divided into two procedures, as explained in Chapter 4.

#### Procedure 1: Compliance with OpenTracing Specification

The first procedure checks if the spans comply with the OpenTracing specification. This method, detailed in Algorithm 5, found that all span structures met the specification. However, the OpenTracing specification is not very strict, leading to potential inaccuracies in the test results. For example:

- **Timestamp Units:** Different spans may use different units (e.g., milliseconds vs. microseconds), causing inconsistencies in time measurements.
- **Key-Value Pairs:** Fields with key-value pairs lack a standardized set of possible keys, leading to inconsistency and uncertainty.

Solutions to these issues include standardizing timestamp units and redefining the semantic specification for key-value pairs.

#### Procedure 2: Coverage of Root Spans

The second procedure assesses whether the tracing data covers the entire duration of the root spans. For instance, if a root span has a duration of 100 milliseconds and two child spans with durations of 50ms and 10ms, the coverage is 60%. This method is applied to every trace, and the results are visualized. The method, presented in Algorithm 6, splits the results by service to evaluate the time coverage in each service. The results are shown in Figure 6.4.

**Figure 6.4: Services coverability analysis.**

Figure 6.4 illustrates the tracing coverability, highlighting that most services have coverage in the 60%-100% range. While this indicates good overall coverage, there is room for improvement. Developers can use these results to identify and enhance the tracing coverage in their services. After making changes, the coverability test should be rerun to verify improvements. Future work could involve developing a method to automatically detect traces that do not meet a predefined coverage threshold.

### Limitations of OpenTracing Data

This section explores the limitations encountered when using OpenTracing data and suggests solutions for improvement. The main limitations are:

1. **Measurement Units:** No clear definition for numeric value units in spans.
2. **Causal Relationships:** No field to indicate causally-related spans from different traces.
3. **Key-Value Fields:** No defined set of possible values for key-value pairs.
4. **Log Correlation:** No field to identify correlated logs.
5. **Metrics Recording:** No defined way to record raw measurements or metrics.

These limitations create challenges in processing and analyzing tracing data. For example, the lack of standardized measurement units can lead to incorrect time calculations. To address these issues, the OpenTracing specification should be revised to include more stringent definitions and additional fields.

### OpenTelemetry Project

In response to the limitations of OpenTracing, a new project called OpenTelemetry, supported by companies like Google, Lightstep, and Uber, was initiated. OpenTelemetry aims to merge OpenCensus and OpenTracing, providing a unified solution for metrics, traces, and logs. This project, backed by the Cloud Native Computing Foundation (CNCF), started in April 2019 and has a roadmap extending to November 2019.

OpenCensus is a set of libraries that allow developers to collect and analyze application metrics. The creation of OpenTelemetry underscores the need for a more comprehensive and standardized approach to observability. The project plans to introduce new standard tags, log fields, and a metrics API, although the specifics are still under development.

### Conclusion and Future Work

This research concludes that tracing data is essential for detecting anomalies related to service morphology. However, it is challenging to handle and often requires complementary data, such as monitoring, for effective analysis. The ambiguity in the OpenTracing specification and the lack of robust tools for data processing and visualization are significant barriers.

Future work should focus on:

1. **Developing New Tools:** Creating and improving tools for OpenTracing data processing.
2. **Redefining Specifications:** Conducting research to redefine the OpenTracing specification.
3. **Analyzing Metrics:** Exploring and analyzing the remaining extracted tracing metrics.
4. **Using Other Systems:** Applying tracing data from other systems.
5. **Fault Injection Simulation:** Developing a simulated system with fault-injection capabilities to validate analysis observations.
6. **Data Conciliation:** Integrating tracing data with other types of data, such as monitoring and logging.
7. **Contributing to OpenTelemetry:** Following and contributing to the OpenTelemetry project based on the findings of this research.

By addressing these areas, we can enhance the utility and effectiveness of tracing data in understanding and managing distributed systems.