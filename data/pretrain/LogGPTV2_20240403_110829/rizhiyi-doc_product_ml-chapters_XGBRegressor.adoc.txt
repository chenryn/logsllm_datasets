==== SGDRegressor
SGD 的详细介绍见: 
SGDRegressor 就是基于 SGD 的 general linear model regression （需要指定loss 和penalty,在我们的实现中如 loss 有"squared_loss"，“huber","epsilon_insensitive")，相比于SGD更出名也用的更多的是另一种优化方法Batch gradient descent, SGD 更多时候会用在大规模的稀疏学习，如给定一个数据集有10^5^ samples and 10^5^ features。
优点：
1. 效率高
2. 容易实现
缺点：
1. SGD需要一些超参数如正则化参数和循环迭代次数
2. 对于feature是否被scaled 十分敏感 （换言之，没有做feature scaling，结果会不尽如人意，所以我们在实现时，默认会加入StandardScaler进行标准化）
3. 当训练数量 from  [into model_name]
Params
*  : target name, 目标特征
*  : model中的x , 特征列表， 个数必须大于等于1
* [into model_name]: fit后模型保存名字， 可选， 可用于后续apply命令
* [params set]:
** loss : str, ‘squared_loss’, ‘huber’, ‘epsilon_insensitive’, or ‘squared_epsilon_insensitive’
+
The loss function to be used. Defaults to ‘squared_loss’ which refers to the ordinary least squares fit. ‘huber’ modifies ‘squared_loss’ to focus less on getting outliers correct by switching from squared to linear loss past a distance of epsilon. ‘epsilon_insensitive’ ignores errors less than epsilon and is linear past that; this is the loss function used in SVR. ‘squared_epsilon_insensitive’ is the same but becomes squared loss past a tolerance of epsilon.
** penalty : str, ‘none’, ‘l2’, ‘l1’, or ‘elasticnet’
+
The penalty (aka regularization term) to be used. Defaults to ‘l2’ which is the standard regularizer for linear SVM models. ‘l1’ and ‘elasticnet’ might bring sparsity to the model (feature selection) not achievable with ‘l2’.
** alpha : float
+
Constant that multiplies the regularization term. Defaults to 0.0001 Also used to compute learning_rate when set to ‘optimal’.
** l1_ratio : float
+
The Elastic Net mixing parameter, with 0 。
Syntax：
    fit XGBRegressor [params set]  from  [into model_name]
Params
*  : target name, 目标特征
*  : model中的x , 特征列表， 个数必须大于等于1
* [into model_name]: fit后模型保存名字， 可选， 可用于后续apply命令
* [params set]:
** max_depth : int
+
Maximum tree depth for base learners.
** learning_rate : float
+
Boosting learning rate (xgb’s “eta”)
** n_estimators : int
+
Number of boosted trees to fit.
** silent : boolean
+
Whether to print messages while running boosting.
** objective : string or callable
+
Specify the learning task and the corresponding learning objective or a custom objective function to be used (see note below).
** booster: string
+
Specify which booster to use: gbtree, gblinear or dart.
** nthread : int
+
Number of parallel threads used to run xgboost. (Deprecated, please use n_jobs)
** n_jobs : int
+
Number of parallel threads used to run xgboost. (replaces nthread)
** gamma : float
+
Minimum loss reduction required to make a further partition on a leaf node of the tree.
** min_child_weight : int
+
Minimum sum of instance weight(hessian) needed in a child.
** max_delta_step : int
+
Maximum delta step we allow each tree’s weight estimation to be.
** subsample : float
+
Subsample ratio of the training instance.
** colsample_bytree : float
+
Subsample ratio of columns when constructing each tree.
** colsample_bylevel : float
+
Subsample ratio of columns for each split, in each level.
** reg_alpha : float (xgb’s alpha)
+
L1 regularization term on weights
** reg_lambda : float (xgb’s lambda)
+
L2 regularization term on weights
** scale_pos_weight : float
+
Balancing of positive and negative weights.
** base_score:
+
The initial prediction score of all instances, global bias.
** seed : int
+
Random number seed. (Deprecated, please use random_state)
** random_state : int
+
Random number seed. (replaces seed)
** missing : float, optional
+
Value in the data which needs to be present as a missing value. If None, defaults to np.nan.
继续使用forestfire数据:
[source]
----
tag:forestfire | eval log_area =log(json.area+1) | fit XGBRegressor log_area from json.X,json.Y,json.month,json.day,json.FFMC,json.DMC,json.DC,json.ISI,json.temp,json.RH,json.wind,json.rain into xgb | rename  predicted_log_area as area  | eval group = "pre" | table group, area,context_id|append[[tag:forestfire | eval group = "actual"| eval log_area =log(json.area+1) |rename log_area as area | table group, area,context_id]]
----
image::images/ml-xgbr-example.png[]
`| summarymodel xgb` 可以看到xgboost对每个特征给出其重要性程度 (只截取部分结果):
image::images/ml-xgbr-example-2.png[]