Preparedness and Disaster Testing  |  463
The US nuclear Navy uses a mixture of “what if” thought exercises and live drills. According to Jeff Stevenson, the live drills involve “actually breaking real stuff but with control parameters. Live drills are carried out religiously, every week, two to three days per week.” For the nuclear Navy, thought exercises are useful, but not suffi‐cient to prepare for actual incidents. Responses must be practiced so they are not forgotten.According to Mike Doherty, lifeguards face disaster testing exercises more akin to a“mystery shopper” experience. Typically, a facility manager works with a child or an incognito lifeguard in training to stage a mock drowning. These scenarios are con‐ducted to be as realistic as possible so that lifeguards aren’t able to differentiate between real and staged emergencies.
Training and CertificationOur interviews suggest that training and certification are particularly important when lives are at stake. For example, Mike Doherty described how lifeguards complete a rigorous training certification, in addition to a periodic recertification process. Cour‐ses include fitness components (e.g., a lifeguard must be able to hold someone heavier than themselves with shoulders out of the water), technical components like first aid and CPR, and operational elements (e.g., if a lifeguard enters the water, how do other team members respond?). Every facility also has site-specific training, because life‐guarding in a pool is markedly different from lifeguarding on a lakeside beach or on the ocean.Focus on Detailed Requirements Gathering and Design
Some of the engineers we interviewed discussed the importance of detailed require‐ments gathering and design docs. This practice was particularly important when working with medical devices. In many of these cases, actual use or maintenance of the equipment doesn’t fall within the purview of product designers. Thus, usage and maintenance requirements must be gathered from other sources.For example, according to Erik Gross, laser eye surgery machines are designed to be as foolproof as possible. Thus, soliciting requirements from the surgeons who actually use these machines and the technicians responsible for maintaining them is particularly important. In another example, former defense contractor Peter Dahl described a very detailed design culture in which creating a new defense system com‐monly entailed an entire year of design, followed by just three weeks of writing the code to actualize the design. Both of these examples are markedly different from Google’s launch and iterate culture, which promotes a much faster rate of change at a calculated risk. Other industries (e.g., the medical industry and the military, as previ‐ously discussed) have very different pressures, risk appetites, and requirements, and their processes are very much informed by these circumstances.464  |  Chapter 33: Lessons Learned from Other Industries
Defense in Depth and Breadth
In the nuclear power industry, defense in depth is a key element to preparedness [IAEA12]. Nuclear reactors feature redundancy on all systems and implement a design methodology that mandates fallback systems behind primary systems in case of failure. The system is designed with multiple layers of protection, including a final physical barrier to radioactive release around the plant itself. Defense in depth is par‐ticularly important in the nuclear industry due to the zero tolerance for failures and incidents.Postmortem Culture
Corrective and preventative action (CAPA)4 is a well-known concept for improving reliability that focuses on the systematic investigation of root causes of identified issues or risks in order to prevent recurrence. This principle is embodied by SRE’s strong culture of blameless postmortems. When something goes wrong (and given the scale, complexity, and rapid rate of change at Google, something inevitably will go wrong), it’s important to evaluate all of the following:• What happened
• The effectiveness of the response
• What we would do differently next time
• What actions will be taken to make sure a particular incident doesn’t happen 	again
This exercise is undertaken without pointing fingers at any individual. Instead of assigning blame, it is far more important to figure out what went wrong, and how, as an organization, we will rally to ensure it doesn’t happen again. Dwelling on who might have caused the outage is counterproductive. Postmortems are conducted after incidents and published across SRE teams so that all can benefit from the lessons learned.Our interviews uncovered that many industries perform a version of the postmortem (although many do not use this specific moniker, for obvious reasons). The motiva‐tion behind these exercises appears to be the main differentiator among industry practices.
Many industries are heavily regulated and are held accountable by specific govern‐ment authorities when something goes wrong. Such regulation is especially ingrained when the stakes of failure are high (e.g., lives are at stake). Relevant government agen‐4 
Postmortem Culture  |  465
cies include the FCC (telecommunications), FAA (aviation), OSHA (the manufactur‐ing and chemical industries), FDA (medical devices), and the various National Competent Authorities in the EU.5 The nuclear power and transportation industries are also heavily regulated.Safety considerations are another motivating factor behind postmortems. In the man‐ufacturing and chemical industries, the risk of injury or death is ever-present due to the nature of the conditions required to produce the final product (high temperature, pressure, toxicity, and corrosivity, to name a few). For example, Alcoa features a note‐worthy safety culture. Former CEO Paul O’Neill required staff to notify him within 24 hours of any injury that lost a worker day. He even distributed his home phone num‐ber to workers on the factory floor so that they could personally alert him to safety concerns.6The stakes are so high in the manufacturing and chemical industries that even “near misses”—when a given event could have caused serious harm, but did not—are care‐fully scrutinized. These scenarios function as a type of preemptive postmortem. According to VM Brasseur in a talk given at YAPC NA 2015, “There are multiple near misses in just about every disaster and business crisis, and typically they’re ignored at the time they occur. Latent error, plus an enabling condition, equals things not work‐ing quite the way you planned” [Bra15]. Near misses are effectively disasters waiting to happen. For example, scenarios in which a worker doesn’t follow the standard operating procedure, an employee jumps out of the way at the last second to avoid a splash, or a spill on the staircase isn’t cleaned up, all represent near misses and oppor‐tunities to learn and improve. Next time, the employee and the company might not be so lucky. The United Kingdom’s CHIRP (Confidential Reporting Programme for Aviation and Maritime) seeks to raise awareness about such incidents across the industry by providing a central reporting point where aviation and maritime person‐nel can report near misses confidentially. Reports and analyses of these near misses are then published in periodic newsletters.Lifeguarding has a deeply embedded culture of post-incident analysis and action planning. Mike Doherty quips, “If a lifeguard’s feet go in the water, there will be paperwork!” A detailed write-up is required after any incident at the pool or on the beach. In the case of serious incidents, the team collectively examines the incident end to end, discussing what went right and what went wrong. Operational changes are then made based on these findings, and training is often scheduled to help people build confidence around their ability to handle a similar incident in the future. In cases of particularly shocking or traumatic incidents, a counselor is brought on site to help staff cope with the psychological aftermath. The lifeguards may have been well5 
6 p.
466  |  Chapter 33: Lessons Learned from Other Industries
prepared for what happened in practice, but might feel like they haven’t done an ade‐quate job. Similar to Google, lifeguarding embraces a culture of blameless incident analysis. Incidents are chaotic, and many factors contribute to any given incident. In this field, it’s not helpful to place blame on a single individual.Automating Away Repetitive Work and Operational Overhead
At their core, Google’s Site Reliability Engineers are software engineers with a low tol‐erance for repetitive reactive work. It is strongly ingrained in our culture to avoid repeating an operation that doesn’t add value to a service. If a task can be automated away, why would you run a system on repetitive work that is of low value? Automa‐tion lowers operational overhead and frees up time for our engineers to proactively assess and improve the services they support.The industries that we surveyed were mixed in terms of if, how, and why they embraced automation. Certain industries trusted humans more than machines. Dur‐ing the tenure of our industry veteran, the US nuclear Navy eschewed automation in favor of a series of interlocks and administrative procedures. For example, according to Jeff Stevenson, operating a valve required an operator, a supervisor, and a crew member on the phone with the engineering watch officer tasked with monitoring the response to the action taken. These operations were very manual due to concern that an automated system might not spot a problem that a human would definitely notice. Operations on a submarine are ruled by a trusted human decision chain—a series of people, rather than one individual. The nuclear Navy was also concerned that auto‐mation and computers move so rapidly that they are all too capable of committing a large, irreparable mistake. When you are dealing with nuclear reactors, a slow and steady methodical approach is more important than accomplishing a task quickly.According to John Li, the proprietary trading industry has become increasingly cau‐tious in its application of automation in recent years. Experience has shown that incorrectly configured automation can inflict significant damage and incur a great deal of financial loss in a very short period of time. For example, in 2012 Knight Cap‐ital Group encountered a “software glitch” that led to a loss of $440M in just a few hours.7 Similarly, in 2010 the US stock market experienced a Flash Crash that was ultimately blamed on a rogue trader attempting to manipulate the market with auto‐mated means. While the market was quick to recover, the Flash Crash resulted in a7 See “FACTS, Section B” for the discussion of Knight and Power Peg software in [Sec13].
Automating Away Repetitive Work and Operational Overhead  |  467
loss on the magnitude of trillions of dollars in just 30 minutes.8 Computers can exe‐cute tasks very quickly, and speed can be a negative if these tasks are configured incorrectly.In contrast, some companies embrace automation precisely because computers act more quickly than people. According to Eddie Kennedy, efficiency and monetary sav‐ings are key in the manufacturing industry, and automation provides a means to accomplish tasks more efficiently and cost-effectively. Furthermore, automation is generally more reliable and repeatable than work conducted manually by humans, which means that it produces higher-quality standards and tighter tolerances. Dan Sheridan discussed automation as deployed in the UK nuclear industry. Here, a rule of thumb dictates that if a plant is required to respond to a given situation in less than 30 minutes, that response must be automated.In Matt Toia’s experience, the aviation industry applies automation selectively. For example, operational failover is performed automatically, but when it comes to cer‐tain other tasks, the industry trusts automation only when it’s verified by a human. While the industry employs a good deal of automatic monitoring, actual air-traffic–control-system implementations must be manually inspected by humans.According to Erik Gross, automation has been quite effective in reducing user error in laser eye surgery. Before LASIK surgery is performed, the doctor measures the patient using a refractive eye test. Originally, the doctor would type in the numbers and press a button, and the laser would go to work correcting the patient’s vision. However, data entry errors could be a big issue. This process also entailed the possi‐bility of mixing up patient data or jumbling numbers for the left and right eye.Automation now greatly lessens the chance that humans make a mistake that impacts someone’s vision. A computerized sanity check of manually entered data was the first major automated improvement: if a human operator inputs measurements outside an expected range, automation promptly and prominently flags this case as unusual. Other automated improvements followed this development: now the iris is photo‐graphed during the preliminary refractive eye test. When it’s time to perform the sur‐gery, the iris of the patient is automatically matched to the iris in the photo, thus eliminating the possibility of mixing up patient data. When this automated solution was implemented, an entire class of medical errors disappeared.8 “Regulators blame computer algorithm for stock market ‘flash crash’,” Computerworld, http://www.computer world.com/article/2516076/financial-it/regulators-blame-computer-algorithm-for-stock-market—flash-crash-.html.
468  |  Chapter 33: Lessons Learned from Other Industries
Structured and Rational Decision MakingStructured and Rational Decision Making
At Google in general, and in Site Reliability Engineering in particular, data is critical. The team aspires to make decisions in a structured and rational way by ensuring that:
• The basis for the decision is agreed upon advance, rather than justified ex post 	facto
• The inputs to the decision are clear
• Any assumptions are explicitly stated• Any assumptions are explicitly stated
• Data-driven decisions win over decisions based on feelings, hunches, or the opin‐	ion of the most senior employee in the room
Google SRE operates under the baseline assumption that everyone on the team:
• Has the best interests of a service’s users at heart
• Can figure out how to proceed based on the data availableDecisions should be informed rather than prescriptive, and are made without defer‐ence to personal opinions—even that of the most-senior person in the room, who Eric Schmidt and Jonathan Rosenberg dub the “HiPPO,” for “Highest-Paid Person’s Opinion” [Sch14].Decision making in different industries varies widely. We learned that some indus‐tries use an approach of if it ain’t broke, don’t fix it…ever. Industries featuring systems whose design entailed much thought and effort are often characterized by a reluc‐tance to change the underlying technology. For example, the telecom industry still uses long-distance switches that were implemented in the 1980s. Why do they rely on technology developed a few decades ago? These switches “are pretty much bulletproof and massively redundant,” according to Gus Hartmann. As reported by Dan Sheri‐dan, the nuclear industry is similarly slow to change. All decisions are underpinned by the thought: if it works now, don’t change it.Many industries heavily focus on playbooks and procedures rather than open-ended problem solving. Every humanly conceivable scenario is captured in a checklist or in“the binder.” When something goes wrong, this resource is the authoritative source for how to react. This prescriptive approach works for industries that evolve and develop relatively slowly, because the scenarios of what could go wrong are not con‐stantly evolving due to system updates or changes. This approach is also common in industries in which the skill level of the workers may be limited, and the best way to make sure that people will respond appropriately in an emergency is to provide a simple, clear set of instructions.Structured and Rational Decision Making  |  469
Other industries also take a clear, data-driven approach to decision making. In Eddie Kennedy’s experience, research and manufacturing environments are characterized by a rigorous experimentation culture that relies heavily on formulating and testing hypotheses. These industries regularly conduct controlled experiments to make sure that a given change yields the expected result at a statistically significant level and that nothing unexpected occurs. Changes are only implemented when data yielded by the experiment supports the decision.Finally, some industries, like proprietary trading, divide decision making to better manage risk. According to John Li, this industry features an enforcement team sepa‐rate from the traders to ensure that undue risks aren’t taken in pursuit of achieving a profit. The enforcement team is responsible for monitoring events on the floor and halting trading if events spin out of hand. If a system abnormality occurs, the enforcement team’s first response is to shut down the system. As put by John Li, “If we aren’t trading, we aren’t losing money. We aren’t making money either, but at least we aren’t losing money.” Only the enforcement team can bring the system back up, despite how excruciating a delay might seem to traders who are missing a potentially profitable opportunity.Conclusions
Many of the principles that are core to Site Reliability Engineering at Google are evi‐dent across a wide range of industries. The lessons already learned by well-established industries likely inspired some of the practices in use at Google today.A main takeaway of our cross-industry survey was that in many parts of its software business, Google has a higher appetite for velocity than players in most other indus‐tries. The ability to move or change quickly must be weighed against the differing implications of a failure. In the nuclear, aviation, or medical industries, for example, people could be injured or even die in the event of an outage or failure. When the stakes are high, a conservative approach to achieving high reliability is warranted.At Google, we constantly walk a tightrope between user expectations for high reliabil‐ity versus a laser-sharp focus on rapid change and innovation. While Google is incredibly serious about reliability, we must adapt our approaches to our high rate of change. As discussed in earlier chapters, many of our software businesses such as Search make conscious decisions as to how reliable “reliable enough” really is.470  |  Chapter 33: Lessons Learned from Other Industries
Google has that flexibility in most of our software products and services, which oper‐ate in an environment in which lives are not directly at risk if something goes wrong. Therefore, we’re able to use tools such as error budgets (“Motivation for Error Budg‐ets” on page 33) as a means to “fund” a culture of innovation and calculated risk tak‐ing. In essence, Google has adapted known reliability principles that were in many cases developed and honed in other industries to create its own unique reliability cul‐ture, one that addresses a complicated equation that balances scale, complexity, and velocity with high reliability.Conclusions  |  471
CHAPTER 34
Conclusion
Written by Benjamin Lutch1 
Edited by Betsy BeyerI read through this book with enormous pride. From the time I began working at Excite in the early ’90s, where my group was a sort of neanderthal SRE group dubbed“Software Operations,” I’ve spent my career fumbling through the process of building systems. In light of my experiences over the years in the tech industry, it’s amazing to see how the idea of SRE took root at Google and evolved so quickly. SRE has grown from a few hundred engineers when I joined Google in 2006 to over 1,000 people today, spread over a dozen sites and running what I think is the most interesting computing infrastructure on the planet.So what has enabled the SRE organization at Google to evolve over the past decade to maintain this massive infrastructure in an intelligent, efficient, and scalable way? I think that the key to the overwhelming success of SRE is the nature of the principles by which it operates.SRE teams are constructed so that our engineers divide their time between two equally important types of work. SREs staff on-call shifts, which entail putting our hands around the systems, observing where and how these systems break, and under‐standing challenges such as how to best scale them. But we also have time to then reflect and decide what to build in order to make those systems easier to manage. In essence, we have the pleasure of playing both the roles of the pilot and the engineer/ designer. Our experiences running massive computing infrastructure are codified in actual code and then packaged as a discrete product.1 Vice President, Site Reliability Engineering, for Google, Inc.
473
These solutions are then easily usable by other SRE teams and ultimately by anyone at Google (or even outside of Google…think Google Cloud!) who wants to use or improve upon the experience we’ve accumulated and the systems we’ve built.When you approach building a team or a system, ideally its foundation should be a set of rules or axioms that are general enough to be immediately useful, but that will remain relevant in the future. Much of what Ben Treynor Sloss outlined in this book’s introduction represents just that: a flexible, mostly future-proof set of responsibilities that remain spot-on 10 years after they were conceived, despite the changes and growth Google’s infrastructure and the SRE team have undergone.As SRE has grown, we’ve noticed a couple different dynamics at play. The first is the consistent nature of SRE’s primary responsibilities and concerns over time: our sys‐tems might be 1,000 times larger or faster, but ultimately, they still need to remain reliable, flexible, easy to manage in an emergency, well monitored, and capacity planned. At the same time, the typical activities undertaken by SRE evolve by neces‐sity as Google’s services and SRE’s competencies mature. For example, what was once a goal to “build a dashboard for 20 machines” might now instead be “automate dis‐covery, dashboard building, and alerting over a fleet of tens of thousands of machines.”For those who haven’t been in the trenches of SRE for the past decade, an analogy between how SRE thinks about complex systems and how the aircraft industry has approached plane flight is useful in conceptualizing how SRE has evolved and matured over time. While the stakes of failure between the two industries are very different, certain core similarities hold true.Imagine that you wanted to fly between two cities a hundred years ago. Your airplane probably had a single engine (two, if you were lucky), a few bags of cargo, and a pilot. The pilot also filled the role of mechanic, and possibly additionally acted as cargo loader and unloader. The cockpit had room for the pilot, and if you were lucky, a co-pilot/navigator. Your little plane would bounce off a runway in good weather, and if everything went well, you’d slowly climb your way through the skies and eventually touch down in another city, maybe a few hundred miles away. Failure of any of the plane’s systems was catastrophic, and it wasn’t unheard of for a pilot to have to climb out of the cockpit to perform repairs in-flight! The systems that fed into the cockpit were essential, simple, and fragile, and most likely were not redundant.Fast-forward a hundred years to a huge 747 sitting on the tarmac. Hundreds of pas‐sengers are loading up on both floors, while tons of cargo are simultaneously being loaded into the hold below. The plane is chock-full of reliable, redundant systems. It’s a model of safety and reliability; in fact, you’re actually safer in the air than on the ground in a car. Your plane will take off from a dotted line on a runway on one conti‐nent, and land easily on a dotted line on another runway 6,000 miles away, right on474  |  Chapter 34: Conclusion
schedule—within minutes of its forecasted landing time. But take a look into the cockpit and what do you find? Just two pilots again!How has every other element of the flight experience—safety, capacity, speed, and reliability—scaled up so beautifully, while there are still only two pilots? The answer to this question is a great parallel to how Google approaches the enormous, fantasti‐cally complex systems that SRE runs. The interfaces to the plane’s operating systems are well thought out and approachable enough that learning how to pilot them in normal conditions is not an insurmountable task. Yet these interfaces also provide enough flexibility, and the people operating them are sufficiently trained, that respon‐ses to emergencies are robust and quick. The cockpit was designed by people who understand complex systems and how to present them to humans in a way that’s both consumable and scalable. The systems underlying the cockpit have all the same prop‐erties discussed in this book: availability, performance optimization, change manage‐ment, monitoring and alerting, capacity planning, and emergency response.Ultimately, SRE’s goal is to follow a similar course. An SRE team should be as com‐pact as possible and operate at a high level of abstraction, relying upon lots of backup systems as failsafes and thoughtful APIs to communicate with the systems. At the same time, the SRE team should also have comprehensive knowledge of the systems—how they operate, how they fail, and how to respond to failures—that comes from operating them day-to-day.Conclusion  |  475
APPENDIX A
Availability Table
Availability is generally calculated based on how long a service was unavailable over some period. Assuming no planned downtime, Table A-1 indicates how much down‐time is permitted to reach a given availability level.
Table A-1. Availability table
| Availability level |  | Allowed unavailability window | Allowed unavailability window | Allowed unavailability window |  |  ||---|---|---|---|---|---|---|
| 90% |per year |per quarter |per month |per week |per day |per hour |
| 90% |36.5 days |9 days |3 days |16.8 hours |2.4 hours |6 minutes |
| 95% |18.25 days |4.5 days |1.5 days |8.4 hours |1.2 hours |3 minutes |
| 99% |3.65 days |21.6 hours |7.2 hours |1.68 hours |14.4 minutes |36 seconds |
| 99.5% |1.83 days |10.8 hours |3.6 hours |50.4 minutes |7.20 minutes |18 seconds || 99.9% |8.76 hours |2.16 hours |43.2 minutes |10.1 minutes |1.44 minutes |3.6 seconds |
| 99.95% |4.38 hours |1.08 hours |21.6 minutes |5.04 minutes |43.2 seconds |1.8 seconds |
| 99.99% |52.6 minutes |12.96 minutes |4.32 minutes |60.5 seconds |8.64 seconds |0.36 seconds |
| 99.999% |5.26 minutes |1.30 minutes |25.9 seconds |6.05 seconds |0.87 seconds |0.04 seconds |Using an aggregate unavailability metric (i.e., "X% of all operations failed”) is more useful than focusing on outage lengths for services that may be partially available—for instance, due to having multiple replicas, only some of which are unavailable—and for services whose load varies over the course of a day or week rather than remaining constant.
See Equations 3-1 and 3-2 in Chapter 3 for calculations.477
APPENDIX B
A Collection of Best Practices for Production Services
Written by Ben Treynor Sloss 
Edited by Betsy Beyer
Fail Sanely
Sanitize and validate configuration inputs, and respond to implausible inputs by both continuing to operate in the previous state and alerting to the receipt of bad input.
Bad input often falls into one of these categories:
Incorrect dataIncorrect data 
Validate both syntax and, if possible, semantics. Watch for empty data and partial or truncated data (e.g., alert if the configuration is N% smaller than the previous version).
Delayed data 
This may invalidate current data due to timeouts. Alert well before the data is expected to expire.Fail in a way that preserves function, possibly at the expense of being overly permis‐sive or overly simplistic. We’ve found that it’s generally safer for systems to continue functioning with their previous configuration and await a human’s approval before using the new, perhaps invalid, data.
479
ExamplesIn 2005, Google’s global DNS load- and latency-balancing system received an empty DNS entry file as a result of file permissions. It accepted this empty file and served NXDOMAIN for six minutes for all Google properties. In response, the system now per‐forms a number of sanity checks on new configurations, including confirming the presence of virtual IPs for google.com, and will continue serving the previous DNS entries until it receives a new file that passes its input checks.In 2009, incorrect (but valid) data led to Google marking the entire Web as containing malware [May09]. A configuration file containing the list of suspect URLs was replaced by a single forward slash character (/), which matched all URLs. Checks for dramatic changes in file size and checks to see whether the configuration is matching sites that are believed unlikely to contain malware would have prevented this from reaching production.Progressive Rollouts
Nonemergency rollouts must proceed in stages. Both configuration and binary changes introduce risk, and you mitigate this risk by applying the change to small fractions of traffic and capacity at one time. The size of your service or rollout, as well as your risk profile, will inform the percentages of production capacity to which the rollout is pushed, and the appropriate time frame between stages. It’s also a good idea to perform different stages in different geographies, in order to detect problems related to diurnal traffic cycles and geographical traffic mix differences.Rollouts should be supervised. To ensure that nothing unexpected is occurring dur‐ing the rollout, it must be monitored either by the engineer performing the rollout stage or—preferably—a demonstrably reliable monitoring system. If unexpected behavior is detected, roll back first and diagnose afterward in order to minimize Mean Time to Recovery.
Define SLOs Like a UserDefine SLOs Like a User
Measure availability and performance in terms that matter to an end user. See Chap‐ter 4 for more discussion.
480  |  Appendix B: A Collection of Best Practices for Production Services
Example
Measuring error rates and latency at the Gmail client, rather than at the server, resul‐ted in a substantial reduction in our assessment of Gmail availability, and prompted changes to both Gmail client and server code. The result was that Gmail went from about 99.0% available to over 99.9% available in a few years.Error Budgets
Balance reliability and the pace of innovation with error budgets (see “Motivation for Error Budgets” on page 33), which define the acceptable level of failure for a service, over some period; we often use a month. A budget is simply 1 minus a service’s SLO; for instance, a service with a 99.99% availability target has a 0.01% “budget” for unavailability. As long as the service hasn’t spent its error budget for the month through the background rate of errors plus any downtime, the development team is free (within reason) to launch new features, updates, and so on.If the error budget is spent, the service freezes changes (except urgent security and bug fixes addressing any cause of the increased errors) until either the service has earned back room in the budget, or the month resets. For mature services with an SLO greater than 99.99%, a quarterly rather than monthly budget reset is appropriate, because the amount of allowable downtime is small.Error budgets eliminate the structural tension that might otherwise develop between SRE and product development teams by giving them a common, data-driven mecha‐nism for assessing launch risk. They also give both SRE and product development teams a common goal of developing practices and technology that allow faster inno‐vation and more launches without “blowing the budget.”