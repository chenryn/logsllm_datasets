our approach utilizes semantic analysis and lexical analysis to map
from the descriptions to their corresponding APIs/parameters. Fi-
nally, after identifying the code descriptions (CDs) in IAs and their
context conditions, Advance generates CD trees to organize those
CDs, which are utilized to produce verification code (VC) under
the context (step 3 ). The generated VC can then be executed by
CodeQL to find the bugs in a programâ€™s API integration.
Example. Figure 2 explains how Advance works through an ex-
ample. The code snippet in Figure 2(a) is extracted from a popular
application named tcpdump, which utilizes two APIs from libp-
cap: pcap_geterr, pcap_close. Here pcap_geterr is first used to
get the information about network packet errors, and store it to
the memory referenced by the returned pointer cp (of the han-
dler pcap_t). pcap_close closes pcap_t and therefore releases the
memory including the error message pointed to by cp. However, in
Line 1088, tcpdump tries to print out the error message using the
invalid pointer cp, which introduces a use-after-free bug.
Actually, the libpcap document describing pcap_geterr clearly
states that â€œyou must use or copy the string before closing the pcap_tâ€
(Figure 2(b)). This guidance, however, has not been followed by the
developers of tcpdump. Advance is able to detect this API misuse.
More specifically, it first automatically identifies from the libpcap
document the above sentence, which is considered to contain the
IA based upon the sentiment (â€œyou must ...â€). However, the sentence
cannot be directly translated into the verification code, as it is not
clear what means by â€œstringâ€, â€œpcap_tâ€ and â€œclosingâ€. To make sense
of such description, Advance continues to perform IA dereference
and find that â€œstringâ€ indicates the return value of pcap_geterr,
"closing" refers to pcap_close and â€œpcap_tâ€ is its parameter. By fur-
ther analyzing the context conditions (i.e., â€œbeforeâ€), Advance con-
structs the CD tree (Figure 2(c)) for VC generation. As shown in the
CD tree, the return value of pcap_geterr (i.e., cp) should be used
before pcap_close operates on the parameter of pcap_geterr (i.e.,
pc). This tree is used to generate the verification code based upon
the following code snippets: â€œargv2.getASuccessor+() = argv1â€
for â€œbeforeâ€, â€œExpr reach,definitionReaches(argv, reach)â€
for â€œuseâ€, â€œFunctionCall fc, LocalScopeVariable v, Variable
Access u, fc.getTarget().hasQualifiedName(â€œargv1â€) and
v.getAnAccess() = argv2 and u = v.getAnAccess() and
fc.getAnArgument() = uâ€ for â€œcall withâ€. The code is then used
to inspect tcpdump to find the misuse.
3.2 IA Discovery
As mentioned earlier, automatically extracting IAs from documents,
especially those less structured, is nontrivial. Template or keyword
based approaches, as proposed by previous studies [44], do not
work well. Particularly, in the presence of the API documents from
different developers, not conforming with any writing convention,
finding templates to cover most IAs is found to be a mission impos-
sible. A key observation in our research is that with the diversity of
Figure 2: An example of the API misuse
through a softmax function.
writing styles, all IAs properly presented in the documents are char-
acterized by a strong sentiment to stress the constraints. Actually
the more important the IAs are, the more forceful the descriptions
would be. For example, the document of SQLite states â€œthe applica-
tion must finalize every prepared statementâ€; an IA in libpcap alerts
the developer â€œmake sure that you explicitly check for PCAP_ERRORâ€.
Based on these observations, our design of Advance utilizes senti-
ment analysis to capture these assumptions.
Sentiment-based IA classifier. Specifically, we utilize the Bi-
GRU-based encoder [24] and an attention mechanism [49] to dis-
cover IAs: Bi-GRU-based encoder is suitable for learning the context
of one sentence to generate a representation, while the attention
mechanism focuses the model more on sentimental words, which
developers often use in IAs to make sure the APIs are correctly
used. Particularly, inspired by the Hierarchical Attention Networks
(HAN) [51], one of the most popular models that integrate the
Bi-GRU-based encoder and the attention mechanism, we design a
new model called Sentence-HAN, or S-HAN for short, to extend the
conventional HAN, which is meant to classify documentation, for
sentence classification. Figure 7 in Appendix illustrates the design
of S-HAN. Its bottom layer is the word encoder, which includes Bidi-
rectional GRUs that get annotations of the words through collecting
information from both directions of a sentence. The inputs of the
encoder are the vectors of words ğ‘¤ğ‘– produced by an embedding
model and its outputs are the word annotations â„ğ‘–. Considering
that different words do not contribute equally to the result of classi-
fication, an attention layer is added after the encoder to underline
sentiment-related expressions: the Multilayer Perceptron (MLP) in
the attention layer receives ğ‘¢ğ‘– to output the attention weight ğ‘ğ‘–
through the softmax function. Finally, word annotation vectors â„ğ‘–
are summed based upon the attention weight ğ‘ğ‘– into a sentence
ğ‘–=1 ğ‘ğ‘–â„ğ‘–, and the vector is used for classification
vector ğ‘£, i.e., ğ‘£ =ğ‘‡
1080   cp = pcap_geterr(pc);...       /* close the pcap_t "pc" */1087pcap_close(pc);      /* use the returned string "cp" */1088snprintf(ebuf, PCAP_ERRBUF_SIZE, "%s: %s\n(%s)",1089                                device, pcap_statustostr(status), cp);pcap_geterr() returns the error text pertaining to the last pcap library error.â€¦ you must use or copy the string beforeclosing the pcap_t.(a)Tcpdump source code calling pcap_geterr(b)The IA in libpcapdocumentafterMisuse!beforeusepcap_closepcap_geterr_0pcap_geterr_1(c)CD tree, pcap_geterr_0and pcap_geterr_1represent the  return value and first parameter of pcap_geterr respectivelyapplicationcodeLibrarydocumentCD treeSince there is no open dataset for training our model, we man-
ually collected and annotated 2,601 IAs (1,296 IAs are from back-
translation) and 3,881 non-IA from OpenSSL documentation. Since
IAs only appear on a small set of sentences, we utilized the back-
translation [25] to augment the dataset: by translating a sentence
in English to another kind of language (e.g., Spanish) and then
translating it back, we could get more sentences with similar mean-
ings. Then these sentences could be added to our dataset. In our
evaluation, S-HAN achieved an accuracy of 88% in discovering IAs
(Section 5.2), more accurate than other models (e.g., Text-CNN and
RCNN). Also, from the attention layer, we observed the words in
a sentence that have significant impacts on the classification re-
sults. For example, in the sentence â€œIt is the callerâ€™s responsibility
to free this memory with a subsequent call to OPENSSL_freeâ€, the
word â€œresponsibilityâ€ reflects a strong sentiment, indicating an IA
being communicated, which is in line with what we see from the
documents.
3.3 IA Dereference
To interpret a discovered IA, oftentimes we need to identify its im-
plicit references to an API name or parameters. For example, in the
IA â€œThe application must finalize every prepared statementâ€, â€œfinal-
ize every prepared statementâ€ refers to the API sqlite3_finalize
and â€œprepared statementâ€ indicates the third parameter of the API
sqlite3_prepare. These references are critical for understanding
the information-flow relations between the caller and the API being
called and between different API invocations. Without resolving
them, an IA cannot be translated into the verification code. To
address this problem, Advance utilizes existing tools, such as Al-
lenNLP [26] and NeuralCoref [29], to eliminate anaphora. However,
none of such techniques can address subtle implicit references, as
those in the above example. Our solution is a semantic-based ap-
proach for API dereference and a lexical analysis for parameter
discovery, as elaborated below.
1 Nall :{ }
2 Npre :{ }
3 VADV :{ * }
4 NP :{ * +}
5 VP_passive :{ + ? ? +}
6 VP_active :{ * * * * +}
Listing 1: Shallow parsing grammars
Semantics-based API dereference. Our dereference solution is
based upon the observation that an implicit API reference should be
semantically similar to the descriptions of the APIâ€™s functionalities.
This allows us to compare their semantic meaning to identify those
closely-related pairs. To this end, Advance performs efficient NLP
analyses such as a shadow parsing to recover these references from
IAs and then analyze their semantics and that of API description
through sentence embedding.
Specifically, an implicit reference to an API describes its op-
erations, which typically contains verb. Therefore, to find these
references, we utilize the Part-of-Speech (POS) tagging, and shal-
low parsing [46] to mark words in discovered IA sentences and
recover all verbs. For example, in Figure 3, after parsing the IA
â€œThe application must finalize every prepared statementâ€, the word
â€œfinalizeâ€ is recognized as a verb (tagged as â€œVBâ€) and the word
Figure 3: Example of IA dereference process (API and
parameter dereference). After POS tagging, IA (â‚) was
parsed to recognized â€œfinalize every prepared statementâ€
as a verb phrase and â€œprepared statementâ€ as a noun
phrase. These phrases were then dereferenced to the API
sqlite3_finalize (based on API functionality sentence â€)
and the parameter ppStmt of sqlite3_prepare (based on API
declaration â), to obtain the dereferenced IA (âƒ).
â€œstatementâ€ is a noun (tagged as â€œNNâ€), which are combined into
a verb phrase (tagged as â€œVPâ€). To identify different types of verb
phrases (e.g., VBN and VBP), our approach leverages a set of rules
that describe these VPs based upon their POS (in Listing 1) and
further utilizes regular expressions to apply the rules on the parsed
sentences to captures the verb phrases. For example, VP includes
active verb phrases (VP_active), such as â€œfinalize every prepared
statementâ€, and passive verb phrases (VP_passive), like â€œThe string
must be deallocatedâ€. VP_passive is composed of at least a noun (i.e.,
Nall) and a verb (i.e., VADV). Sometimes, a modal verb (i.e., MD)
may also exist between the noun and the verb, and more verbs may
also be included. Listing 1 presents the details of these rules2. Note
that some extracted verb phrases are API references, which are
identified through comparison with API descriptions.
An API description is characterized by the appearance of the API
name at the beginning of a paragraph. This allows us to extract the
sentences from the paragraphs to compare their semantic meanings
with that of the verb phrases discovered from IAs. In our research,
we found that typically the first sentence of the API description
explains its functionality, so it is picked out for the comparison. For
this purpose, Advance utilizes sentence embedding to transform a
sentence into a vector to represent its semantics. Here, the embed-
ding model was trained in our research, without supervision, on
each library. The semantic comparison is performed by calculating
the cosine similarity between the vector for a verb phrase and that
of the API functionality sentence (ğ‘†1 and ğ‘†2), i.e., ğ‘ ğ‘–ğ‘š = ğ‘†1Â·ğ‘†2
âˆ¥ğ‘†1 âˆ¥âˆ¥ğ‘†2 âˆ¥ .
Looking at similar pairs, our approach captures an implicit API
reference from a verb phrase and dereferences it using the closest
API functionality sentence3 in semantics to the phrase. Then we re-
place the reference with â€œcall #API with #noun_phraseâ€, where #API
is the name of the API discovered, and #noun_phrase is the noun in
the verb phrase (â€œprepared statementâ€ in the example). As shown in
Figure 3, the IA is transformed to â€œcall sqlite3_finalize_API with pre-
pared statementâ€. In the absence of matched API descriptions (that
is, low similarity across all sentences), a verb phrase is not consid-
ered to contain API reference. Our experiment shows this approach
is quite effective, achieving an accuracy of 94% (Section 5.2).
282 POS tags are given in the website: http://www.surdeanu.info/mihai/teaching/
ista555-fall13/readings/PennTreebank.html.
3An API functionality sentence describes the API functionality.
The application must  finalize every prepared statementintsqlite3_prepare(â€¦ ,  sqlite3_stmt ** ppStmt, ..)2The sqlite3_finalize function is called to delete a prepared statement.3Dereference API descriptionsDereference parametersThe application must call sqlite3_finalize_API with sqlite3_prepare_param_34DT       NN          MD      VB       DT        JJ             NN1POS taggingDereferencing parameters. Unlike the implicit API reference, a
subtle indication of an API parameter in an IA has lexical connec-
tions to the name of the parameter, which is meant for a reader to
easily locate the related code: we found in our research that in most
cases, such a reference is in one of three forms â€“ an abbreviation
of a parameter name (e.g., name for â€œzNameâ€), an extension of the
name (e.g., using â€œprepared statementâ€ to describe the parameter
ppStmt), or the type of the parameter (e.g., sqlite3_snapshot).
Based on this observation, our approach compares the semantics of
a possible reference with a associated parameterâ€™s name and type
to report the most similar pair.
Specifically, we first recover possible parameter references from
IA sentences. Such references are often in the form of noun phrases
(tagged as NP) that contain at least one noun (tagged as NP), some-
times with one or more adjectives (tagged as JJ) in front of the
noun. This allows us to construct the rule of NP (see Listing 1)
to detect such references. For API parameters, our approach di-
rectly extracts their names, descriptions, types from the API decla-
rations using regular expressions. For example, in the declaration
int sqlite3_prepare(..., sqlite3_stmt** ppStmt, ...);,
sqlite3_stmt** is the type and ppStmt is the name. Then we use
a regular expression to determine whether the relationship between
the noun phrases (possible references) and the parameter names/-
types can be characterized as abbreviations or expansions. The
details are shown in Section 4.
Note that, in most cases, a dereferenced parameter appears in
the API document at the locations close to the descriptions of the
IA referring to it. So in the presence of multiple candidates (pa-
rameters apparently related to a noun phrase in an IA sentence),
the one located closest to the IA is chosen to replace the implicit
reference (the noun phrase), using its API name and parameter
index (API_param_idx). For example, the parameter reference (in
Figure 3 3 ) is changed to sqlite3_finalize_param_3 (in 4 ),
representing the third parameter of sqlite3_finalize.
3.4 Verification Code Generation
After recovering IAs from documentation and resolving the implicit
references, Advance is ready to generate verification code (VC) for
finding API misuses in a programâ€™s API integration. As mentioned
earlier, IAs are highly diverse, containing constraints on APIsâ€™ in-
puts (pre-conditions), outputs (post-conditions), and invocation con-
text (context conditions), which is unlike the simple arithmetic and
logic requirements handled by the prior research [44].
Automatic generation of proper VC even in the form of the veri-
fication tool queries is nontrivial. For example, data dependency
and code sequential should be specified in VC, since otherwise the
verification tool cannot perform the check correctly. One possible
solution is to use machine learning to automatically synthesize the
inspection code, which however requires a large amount of labeled
data for model training that is unavailable for the problem of API
misuse detection. Our solution is based upon an observation that
important and often security-sensitive constraints tend to carry
common components, not only within a document but also across
different documents, indicating the presence of categories of criti-
cal conditions on API use one needs to follow. For example, â€œthe
length of â€ is a popular phrase in pre-conditions that indicates a
type of constraints on the size of API parameters. Therefore, our
idea is to break an IA into several small components (called Code
Descriptions or CDs) and only map the most frequently used ones
to verification code snippets (VCSes), which requires minimum
manual effort. Then Advance automatically assembles these VC-
Ses and parameterize them, based upon the combinations of their
CDs in different IAs, to generate the complete VCs. During this
process, the data and control dependencies among CDs are discov-
ered through CD trees, and further preserved in the VC through
traversal of these trees to link different VCSes together. Follow we
elaborate this design.
Code description discovery. Ostensibly the discovery of the
frequently used CDs can be done through a sliding window (N-
grams) to find out the sentence fragments that show up several
times in a document or across documents. This simple approach,
however, does not work well on the analysis of IAs. The N-gram
does not carry any syntactic and semantic information and can
therefore cut into CDs and link less meaningful sentence fragments
together: for example, â€œdata mustâ€ (shown in Figure 4 (a)) will be
extracted as a phrase of high frequency when the window size is 2;
however it is not meaningful and does not provide any information
about assumptions to be followed in API integration.
Therefore, Advance takes a syntax and semantics savvy solu-
tion, transforming an IA into a dependency tree and mining the
most frequently used subtrees over its grammatical structure. An
example of an IAâ€™s dependency tree is illustrated in Figure 4 (a).
Here each node of the tree is a word, and different nodes are con-
nected based upon their dependency type. For example, the word
â€œlengthâ€ and â€œtheâ€ are linked with the det type. Over such a tree,
we run TREEMINER [53], an algorithm that discovers frequent
subtrees, each of which describes a meaningful grammatical unit
(template) such as a phrase. For example, Figure 4 (a) shows the
dependency trees of two IAs, with four popular subtrees discovered
across documents are circled with dotted lines and labeled (i.e., from
1 to 4), each being an automatically generated template. Note that
we remove negative words (e.g., â€œnotâ€ and â€œneverâ€), chronological
words (e.g., â€œbeforeâ€) and modal verbs (e.g., â€œshouldâ€) from depen-
dency trees before mining, for detection of small meaningful units
that can be easily extended or connected to other units through
these words. As an example, from two descriptions â€œbe usedâ€ and
â€œseldom be usedâ€, only one CD is identified, since â€œseldom be usedâ€
can be automatically extended from the VCS of â€œbe usedâ€. After
that, for each subtree, we built a VCS (in the verification toolâ€™s
query language) and store it in an initial CD dataset. For example,
in Figure 4 (b), the CD â€œthe length of argvâ€ is converted to the VCS
array_length(argv).
In this way, each IA is then transformed into a dependency tree,
whose subtrees are further compared across those of other IAs to
find popular CDs. Our study shows that this approach can achieve
an accuracy of 75% in detecting CDs (Section 5.2), indicating that
most code descriptions can be captured by popular subtrees.
Verification code generation from CD. Given an IA extracted
from a document, Advance first identifies its CDs by looking up
the initial CD dataset, which as mentioned earlier, contains popular
code description templates as discovered from frequent subtree min-
ing within or across documents. Note that the manual translation of
Figure 4: An example of automatically generating verification code for checking API misuse.
the CDs in the initial CD dataset to VCSes is a one-time effort and
not required to be done by Advance users. When a subtree ğ‘ğ‘– in the
IA is found to match CD ğ‘ ğ‘— in the dataset, our approach automati-
cally generates the verification code snippet (VCS) of ğ‘ğ‘– (ğ‘‰ğ¶ğ‘†ğ‘–) by
transforming that of ğ‘ ğ‘— (ğ‘‰ğ¶ğ‘† ğ‘—). Note that ğ‘‰ğ¶ğ‘†ğ‘– may not be identical
to ğ‘‰ğ¶ğ‘† ğ‘—, since at this point, we need to consider the impacts of
the terms removed, including negative terms (â€œnotâ€, â€œseldomâ€, etc.),
chronological terms, modal words, etc., as mentioned earlier. The
new snippet ğ‘‰ğ¶ğ‘†ğ‘– therefore should be chosen from a variation of
ğ‘‰ğ¶ğ‘†ğ‘– in the dataset, according to the term presented. Also impor-
tantly, as a template, ğ‘‰ğ¶ğ‘† ğ‘— or its variation needs to be parameterized
with variables and constants such as number, string (see Figure 4
(a)) before it can be instantiated into ğ‘‰ğ¶ğ‘†ğ‘–. For example, consider
the IA â€œthe length of the data in EVP_DecodeBlock_param_2 must be
divisible by 4â€, with a popular CD â€œthe length of argvâ€, its VCS dis-
covered from the initial CD dataset array_length(argv) need to
be instantiated into array_length(EVP_DecodeBlock_param_2).
After creating the VCSes for individual CDs in an IA, our ap-
proach links them together based upon their relations such as con-
text condition, as described by the dependency tree. To this end,