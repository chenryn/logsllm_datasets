129: Deﬁnitions:
130:
131:
132:
133: W [1 . . . S]: vector of (ts, f r, cc, vec), initially []
134: operation READ()
135:
136:
137:
138:
139:
140:
141:
142:
143:
144:
145: upon receiving collect ack(cid:104)tsr, ci(cid:105) from server si
146:
147:
148: upon receiving filter ack(cid:104)tsr,ts,f r,cc,vec(cid:105) from server si
149:
150:
c ← c(cid:48) ∈ C : highcand(c(cid:48)) ∧ safe(c(cid:48))
V ← restore(c.ts)
repair(c)
else V ← ⊥
return V
R ← R ∪ {i}; W [i] ← (ts, f r, cc, vec)
C ← C \ {c ∈ C : invalid(c)}
Q ← Q ∪ {i}
if ci.ts > ts0 then C ← C ∪ {ci}
...
...
...
vec ← vec(cid:48) s.t. ∃R(cid:48) ⊆ R : |R(cid:48)| ≥ t + 1 (cid:86)
151: procedure repair(c)
152:
(∀i ∈ R(cid:48) : W [i].ts = c.ts ∧ W [i].vec = vec(cid:48))
if c.vec (cid:54)= vec then
153:
154:
155:
156:
157: Predicates:
158:
c.vec ← vec
send repair(cid:104)tsr, c(cid:105) to all servers
wait for repair ack(cid:104)tsr(cid:105) from S − t servers
safe(c) (cid:44) ∃R(cid:48) ⊆ R : |R(cid:48)| ≥ t + 1 (cid:86)
(∀i ∈ R(cid:48) : W [i].ts = c.ts) (cid:86)
(∀i, j ∈ R(cid:48):W [i].cc=W [j].cc ∧ H(W [i].f r)=W [j].cc[i])(cid:86)
//repair
(∀i, j ∈ R(cid:48) : W [i].vec = W [j].vec)
...
and 1024-bit DSA to generate signatures4. For simplicity,
we abstract away the eﬀect of message authentication in
our implementations; we argue that this does not aﬀect our
performance evaluation since data origin authentication is
typically handled as part of the access control layer in all
three implementations, when deployed in realistic settings
(e.g., in Wide Area Networks).
We deployed our implementations on a private network
consisting of a 4-core Intel Xeon E5443 with 4GB of RAM,
a 4 core Intel i5-3470 with 8 GB of RAM, an 8 Intel-Core
i7-37708 with 8 GB of RAM, and a 64-core Intel Xeon E5606
equipped with 32GB of RAM. In our network, the commu-
nication between various machines was bridged using a 1
Gbps switch. All the servers were running in separate pro-
cesses on the Xeon E5606 machine, whereas the clients were
distributed among the Xeon E5443, i7, and the i5 machines.
Each client invokes operation in a closed loop, i.e., a client
may have at most one pending operation. In all KVS imple-
mentations, all write and read operations are served by a
local database stored on disk.
We evaluate the peak throughput incurred in M-PoWerStore
in write and read operations, when compared to M-ABD
and Phalanx with respect to: (i) the ﬁle size (64 KB, 128
4Note that our benchmarks feature balanced reads/writes
in which case balanced sign/verify DSA performance seems
more appropriate for Phalanx than RSA [14].
Parameter
Default Value
Failure threshold t
File size
Probability of Concurrency
Workload Distribution
1
256 KB
1%
100% read
100% write
Table 2: Default parameters used in evaluation.
KB, 256 KB, 512 KB, and 1024 KB), and (ii) to the server
failure threshold t (1, 2, and 3, respectively). For better
evaluation purposes, we vary each variable separately and
we ﬁx the remaining parameters to a default conﬁguration
(Table 2). We also evaluate the latency incurred in M-
PoWerStore with respect to the attained throughput.
We measure peak throughput as follows. We require that
each writer performs back to back write operations; we
then increase the number of writers in the system until the
aggregated throughput attained by all writers is saturated.
The peak throughput is then computed as the maximum ag-
gregated amount of data (in bytes) that can be written/read
to the servers per second.
Each data point in our plots is averaged over 5 indepen-
dent measurements; where appropriate, we include the cor-
responding 95% conﬁdence intervals. as data objects. On
the other hand, read operations request the data pertaining
to randomly-chosen keys. For completeness, we performed
our evaluation (i) in the Local Area Network (LAN) set-
ting comprising our aforementioned network (Section 6.2)
and (ii) in a simulated Wide Area Network (WAN) setting
(Section 6.3). Our results are presented in Figure 2.
6.2 Evaluation Results within a LAN Setting
Figure 2(a) depicts the latency incurred in M-PoWerStore
when compared to M-ABD and Phalanx, with respect to
the achieved throughput (measured in the number of oper-
ations per second). Our results show that, by combining
metadata write-backs with erasure coding, M-PoWerStore
achieves lower latencies than M-ABD and Phalanx for both
read and write operations. As expected, read latencies
incurred in PoWerStore are lower than those of write oper-
ations since a write requires an extra communication round
corresponding to the clock round. Furthermore, due to
PoW and the use of lightweight cryptographic primitives,
the read performance of PoWerStore considerably outper-
forms M-ABD and Phalanx. On the other hand, writing in
M-PoWerStore compares similarly to the writing in M-ABD.
Figure 2(b) depicts the peak throughput achieved in M-
PoWerStore with respect to the number of Byzantine servers
t. As t increases, the gain in peak throughput achieved in
M-PoWerStore’s read and write increases compared to M-
ABD and Phalanx. This mainly stems from the reliance
on erasure coding, which ensures that the overhead of ﬁle
replication among the servers is minimized when compared
to M-ABD and Phalanx. In typical settings, featuring t = 1
and the default parameters of Table 2, the peak throughput
achieved in M-PoWerStore’s read operation is almost twice
as large as that in M-ABD and Phalanx.
In Figure 2(c), we measure the peak throughput achieved
in M-PoWerStore with respect to the ﬁle size. Our ﬁndings
clearly show that as the ﬁle size increases, the performance
gain of M-PoWerStore compared to M-ABD and Phalanx
becomes considerable. For example, when the ﬁle size is 1
MB, the peak throughput of read and write operations in
M-PoWerStore approaches the (network-limited) bounds of
50 MB/s5 and 45 MB/s, respectively.
6.3 Evaluation Results within a Simulated WAN
Setting
We now proceed to evaluate the performance (latency, in
particular) of M-PoWerStore when deployed in WAN set-
tings. For that purpose, we rely on a 100 Mbps switch to
bridge the network outlined in Section 6.1 and we rely on
NetEm [37] to emulate the packet delay variance speciﬁc to
WANs. More speciﬁcally, we add a Pareto distribution to
our links, with a mean of 20 ms and a variance of 4 ms.
Our measurement results (Figure 2(d)) conﬁrm our pre-
vious analysis conducted in the LAN scenario and demon-
strate the superior performance of M-PoWerStore compared
to M-ABD and Phalanx in realistic settings. Here, we point
out that the performance of M-PoWerStore incurred in both
read and write operations does not deteriorate as the num-
ber of Byzantine servers in the system increases. This is
mainly due to the reliance on erasure coding. In fact, the
overhead of transmitting an erasure-coded ﬁle F to the 3t+1
servers, with a reconstruction threshold of t + 1 is given by
t+1 |F|. It is easy to see that, as t increases, this overhead
3t+1
asymptotically increases towards 3|F|.
7. RELATED WORK
HAIL [8] is a distributed cryptographic storage system
that implements a multi-server variant of Proofs of Retriev-
ability (PoR) [9] to ensure integrity protection and availabil-
ity (retrievability) of ﬁles dispersed across several storage
servers. Like PoWerStore, HAIL assumes Byzantine fail-
ure model for storage servers, yet the two protocols largely
cover diﬀerent design space. Namely, HAIL considers a mo-
bile adversary and a single client interacting with the storage
in a synchronous fashion. In contrast, PoWerStore assumes
static adversary, yet assumes a distributed client setting in
which clients share data in an asynchronous fashion. Multi-
ple clients are also supported by IRIS [42], a PoR-based dis-
tributed ﬁle system designed with enterprise users in mind
that stores data in the clouds and is resilient against poten-
tially untrustworthy service providers. However, in IRIS, all
clients are pre-serialized by a logically centralized, trusted
portal which acts as a fault-free gateway for communication
with untrusted clouds.
In contrast, PoWerStore relies on
the highly available distributed PoW technique, which elim-
inates the need for any trusted and/or fault-free component.
Notice that data conﬁdentiality is orthogonal to all of HAIL,
IRIS and PoWerStore protocols.
In the context of distributed storage asynchronously shared
among multiple fault-prone clients across multiple servers
without any fault-free component, a seminal crash-tolerant
storage implementation (called ABD) was presented in [5].
ABD assumes a majority of correct storage servers, and
achieves strong consistency by having readers write back
the data they read. As shown in [17], server state modiﬁca-
tions by readers introduced in ABD are unavoidable in ro-
bust storage such as ABD, where robustness is characterized
by both strong consistency and high availability. However,
5Note that an eﬀective peak throughput of 50MB/s in M-
PoWerStore reﬂects an actual throughput of almost 820
Mbps when t = 1.
(a) Throughput vs. latency in a LAN setting.
(b) Peak throughput with respect to the failure threshold in
a LAN setting.
(c) Peak throughput with respect to the ﬁle size in a LAN
setting.
(d) Latency vs the failure threshold in a simulated WAN set-
ting.
Figure 2: Evaluation Results. Data points are averaged over 5 independent runs; where appropriate, we
include the corresponding 95% conﬁdence intervals.
robust storage implementations diﬀer in the writing strategy
employed by readers: in some protocols readers write-back
data (e.g., [4, 5, 15, 18, 20, 34]) whereas in others readers only
write metadata to servers (e.g., [11, 16, 17]).
Existing robust storage protocols in which readers write
only metadata, either do not tolerate Byzantine faults [11,
17], or require a total number of servers linear in number of
readers to tolerate t Byzantine servers [16], and hence are
prohibitively expensive. PoWerStore is hence the ﬁrst robust
BFT protocol that uses a bounded number of storage servers
and has readers write only metadata to servers.
Clearly, most distributed BFT storage implementations
have been focusing on using as few servers as possible, ideally
3t + 1, which deﬁnes optimal resilience in the asynchronous
model [35]. This was ﬁrst achieved by Phalanx [34], a BFT
variant of ABD [5]. Phalanx uses digital signatures, i.e.,
self-verifying data, to port ABD to the Byzantine model,
maintaining the latency of ABD, as well as its data write-
backs. However, since digital signatures introduce consid-
erable overhead [33, 38], protocols that feature lightweight
authentication, or no data authentication at all (unauthen-
ticated model) have been designed. Unfortunately, in the
unauthenticated model, optimal resilience in BFT storage
incurs considerable latency penalties: at least two rounds
of communication between clients and servers for writes [3]
and at least four rounds6 for reads [15], even in the single
writer case. To avoid such a considerable overhead, some
robust BFT storage protocols (e.g., PASIS [19]) store unau-
thenticated data across 4t + 1 servers.
Clearly, there is a big gap in eﬃciency (and, in partic-
ular, communication latency and the number of servers)
between storage protocols that use self-verifying data and
those that assume no authentication. Loft [22] aims at
bridging this gap and implements erasure-coded optimally
resilient linearizable storage while optimizing the failure-free
case. Loft uses homomorphic ﬁngerprints and MACs; it fea-
tures 3-round wait-free writes, but reads are based on data
write-backs and data might be unavailable in case of heavy
read/write concurrency. Similarly, our Proofs of Writing
6Under constant number of write rounds.
 0 50 100 150 200 250 40 60 80 100 120 140 160 180Latency (ms)Throughput (op/s)M-ABD Write M-ABD Read Phalanx Write Phalanx Read M-PoWerStore Write M-PoWerStore Read  5 10 15 20 25 30 35 40 45t=1t=2t=3Peak Throughput (MB/s)Failure ThresholdM-ABD WriteM-ABD Read Phalanx Write Phalanx Read M-PoWerStore Write M-PoWerStore Read 0 10 20 30 40 50 100 200 300 400 500 600 700 800 900 1000Peak Throughput (MB/s)File Size (KB)M-ABD WriteM-ABD Read Phalanx Write Phalanx Read M-PoWerStore Write M-PoWerStore Read 100 200 300 400 500 600 700 800 900 1000t=1t=2t=3Latency (ms)Failure ThresholdM-ABD WriteM-ABD Read Phalanx Write Phalanx Read M-PoWerStore Write M-PoWerStore ReadProtocol
Data Authentication Data Dispersal Read/Write Latency no. of readers/writers no. of messages Byz. clients
Phalanx [34]
[31]
[15]
[10]
Loft [22]
PoWerStore
M-PoWerStore
signatures
signatures
none
signatures
hash/MAC
hash/MAC
hash/MAC
replication
replication
replication
erasure coding
erasure-coding
erasure-coding
erasure-coding
2D/(D+d)
2D/(D+2d)
(2D+2d)/(D+d)
>(D+2d) / >(3D+d)
∞/D+2d
(D+d)/(D+d)
(D+2d)/(D+2d)
any/any
any/any
any/1
any/any
any/any
any/1
any/any
O(t)
O(t)
O(t)
O(t2)
O(t2)
O(t)
O(t)
readers
all
no
all
all
readers
readers
Table 3: Comparison of properties of existing strongly consistent optimally resilient BFT storage protocols.
Shown latency is worst case and distinguishes between data rounds (D) and metadata rounds (d), where
typically D >> d.
(PoW) incorporate lightweight authentication that is, how-
ever, suﬃcient to achieve optimal latency and to facilitate
metadata write-backs while guaranteeing optimal resilience,
high-availability and strong consistency. We ﬁnd PoW to
be a fundamental improvement in the light of BFT storage
implementations that explicitly renounce strong consistency
in favor of weaker consistency notions due to the high cost
of data write-backs (e.g., [7]). A summary of the key prop-
erties of existing BFT storage protocols when compared to
our protocols is shown in Table 3.
A separate line of research aims at a family of so-called
forking semantics (e.g., [36]), which relax atomic semantics,