### Experiment 1: Determination of Step Size

We consider three different step sizes to evaluate the performance of our proposed fair algorithm. The two key observations from the results are:
1. The proposed fair algorithm effectively keeps the server load within the target limits, even under heterogeneous sources and network delays.
2. The additive step size significantly affects system stability. As shown, a large step size can lead to unstable system performance. Therefore, a smaller step size is necessary for the system to operate in a stable region.

In our second experiment, we illustrate the results in Fig. 6, where the target load \( U \) is set to 1100, and the lower limit \( L \) can be either 900 or 1050. We observe that when \( L = 900 \), the achieved server workload at convergence is slightly above 1000. On the other hand, when \( L = 1050 \), a smaller step size (e.g., 0.05) is needed for stable performance, and the achieved server workload at convergence is closer to 1050. After experimenting with various step sizes and system configurations, we recommend a small step size of 0.05 for a stable system.

### Experiment 2: Determination of Step Size for System Stability

In this experiment, we determine the appropriate step size for system stability. We consider how the step size affects the convergence rate. In Fig. 7, we present the results of our third experiment, where we use three different step sizes: 0.3, 0.1, and 0.05. Although the system is stable for all three step sizes, we observe that a smaller step size leads to a longer convergence time. For example, when ten constant sources are activated at \( t = 0 \) with a step size of 0.05, the system converges around \( t = 100 \). If we use a step size of 0.3, the system converges around \( t = 50 \). Another important point is that if the step size is smaller, the achieved server workload at convergence is also smaller. Therefore, to achieve a high server workload while maintaining stability, we recommend a step size between 0.1 and 0.3.

### Packet Network Simulation Results

Our control-theoretic results provide a basic understanding of algorithm stability and convergence. To further examine system performance under detailed packet network models, including both unreliable UDP and reliable TCP communication, we conduct experiments using the ns2 simulator. We focus on the fair throttle algorithm.

#### Performance Metrics

One key performance measure is how well router throttles installed by the algorithm can mitigate DDoS attacks, allowing good users to obtain service. It is clear that the defense mechanism cannot completely neutralize malicious traffic, so good users may experience some degradation in performance, but they should be much less affected than without network protection.

In addition to the basic performance measure, it is essential to evaluate the deployment costs of the proposed defense mechanism. The following are the important evaluation criteria we adopt:
- The percentage of good user traffic that reaches the server.
- The number of routers involved in protecting the server, as throttling can clip forwarding rates to a preset ceiling, making it less tolerant to traffic variability.

#### Packet Network Results

To evaluate the performance of the proposed throttle mechanism over a real network, we conducted simulations using a global network topology reconstructed from real traceroute data. The resulting graph has 135,821 nodes, of which 3,879 are hosts. We assume these 3,879 hosts access the target server, either as attackers or good users.

##### Evenly Distributed Aggressive Attackers

In our first set of experiments, we model aggressive attackers whose average individual sending rate is several times higher than that of normal users. Each good user sends fixed-size UDP packets to the server, with Poisson-distributed interarrival times and an average traffic rate uniformly drawn from [0, 2]. Each attacker's traffic rate is uniformly drawn from [10, 20] or [20, 40], depending on the experiment. Attackers and good users are evenly distributed in the network topology.

Fig. 8(a) compares the performance of our algorithm (labeled "level-k max-min fairness") with the pushback max-min fairness approach. We show the percentage of remaining good user and attacker traffic that passes the router throttles and arrives at the server. Figs. 8(b) and (c) show the corresponding results for different attack rates. Generally, level-k max-min fairness provides better protection for good user traffic than pushback max-min fairness, especially as the attack rate increases.

##### Unevenly Distributed Aggressive Attackers

In this set of experiments, each good user's traffic rate is chosen randomly and uniformly from [0, 2], while each attacker's rate is chosen from [0, 20]. About 20% of the hosts are chosen to be attackers, and the remaining hosts are good users. We select the attackers to have different concentration properties, defined in Table IV. Fig. 10(a) shows the percentage of remaining good traffic for the four concentrations using level-k max-min fairness. Fig. 10(b) shows the corresponding results for pushback max-min fairness. Level-k max-min fairness consistently outperforms pushback max-min fairness, especially as the attacker concentration increases.

##### Evenly Distributed "Meek" Attackers

Router throttling is most effective when attackers are significantly more aggressive than good users. However, if many hosts behave like normal users, it becomes more difficult to defend against such "meek" attackers. Our experimental results (Fig. 11) show that both level-k and pushback max-min fairness may fail to distinguish between good users and attackers, leading to equal punishment for both. In such cases, throttling is mainly useful in regulating the server load within operational limits.

##### Deployment Extent

The previous experiments suggest that the effectiveness of level-k max-min fairness increases with the deployment depth. However, the cost of deployment may also increase. Fig. 12 plots the percentage of routers involved in throttling as a function of the deployment depth. Both approaches require a comparable number of deployment points, although for \( k = 4 \) to 9, pushback max-min fairness is somewhat more efficient, and for larger \( k \), level-k max-min fairness is more efficient. The percentage of deployment points levels off as \( k \) increases, as the traffic rate destined for the server becomes progressively smaller.

##### Web Server Performance

To evaluate the impact of throttling on real user applications, we simulate the performance of a web server under DDoS attack using ns2. Clients access the web server via HTTP 1.0 over TCP Reno/IP. The simulated network consists of 85 hosts, of which 20% (i.e., 17 out of 85) are chosen as attackers. Attackers generate UDP traffic at a constant rate of 6000 bits/s. Web clients make requests for documents, and the server returns the requested document after a random processing time. We model the web server to have a maximum load of \( U = 1100 \) and a lower limit of \( L = 1050 \). We report two experiments with different server capacities and plot the rates of client requests that are successfully processed by the server over time. The aggregate rate at which the clients originally make requests is also shown for baseline comparison. Each experiment runs for a sufficient duration to capture the steady-state behavior.