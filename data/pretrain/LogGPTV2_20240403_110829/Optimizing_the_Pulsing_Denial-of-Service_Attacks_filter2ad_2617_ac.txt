. We consider three
iment where
. We make two
different step sizes, namely
important observations about the results: 1) The proposed fair
algorithm is effective in keeping the server load within the
target limits, under heterogeneous sources and heterogeneous
can affect system
network delays, and 2) the additive step size
stability. As shown, system performance is not stable for the
. Hence, a small step size relative to
large step size of
is needed for the system to operate in a stable region.
for a stable
system. Fig. 6 illustrates the results of our second experiment
can be 900 or 1050. We observe
where
,
that when
and the achieved server workload at convergence is slightly
advertises a smaller
above 1000. On the other hand, when
target load region, with
, we need
) to have stable performance,
a smaller step size (e.g,
and the achieved server workload at convergence is closer to
. After experimenting with a large number of different step
sizes and many different system conÔ¨Ågurations, we recommend
a small step size of
and
is large, the system is stable with
Experiment 2: Determination of step size
) for system stability.
and
can affect
on the convergence rate. Fig. 7
illustrates the results of our third experiment in which we
the convergence speed. In the
consider how
. We experiment
experiment,
, 0.1, 0.05.
with three different step sizes, namely
Although the system is stable for all the three step sizes, we
observe that if a step size is too small, it takes longer for the
system to converge. For example, when ten constant sources
are activated at
for
, the system converges around
. On the other hand, if we use
, the system
and
(e.g.,
Experiment 3: Effect of
YAU et al.: DEFENDING AGAINST DISTRIBUTED DENIAL-OF-SERVICE ATTACKS WITH MAX-MIN FAIR SERVER-CENTRIC ROUTER THROTTLES
35
Fig. 6. System performance for U = 1100 and L = 900 or 1050, and various  step sizes.
Fig. 7. System performance for U = 1100 and L = 1050, and various  step sizes.
. Another important point is that
converges around
if
is smaller, the achieved server workload at convergence
is also smaller. Therefore, in order to have a stable system
and, at the same time, achieve a high server workload, we
recommend
to be between 0.1 and 0.3.
VI. PACKET NETWORK SIMULATION RESULTS
Our general, high-level control-theoretic results provide basic
understanding about algorithm stability and convergence. To
further examine system performance, under detailed packet net-
work models (including both unreliable UDP and reliable TCP
communication), we conduct experiments using the ns2 simu-
lator. We present results only for the fair throttle algorithm.
A. Performance Metrics
One basic performance measure is how well router throt-
tles installed by
can Ô¨Çoor attackers in their attempt to deny
good users of the ability to obtain service from . It is clear
that the defense mechanism cannot completely neutralize the
effects of malicious trafÔ¨Åc ‚Äì in part because attackers are them-
in our model. Hence, good users
selves entitled to a share of
must see a degraded level of performance, but hopefully are
much less prone to aggressive attack Ô¨Çows than without net-
work protection.
Apart from the basic performance measure, it is necessary to
evaluate the deployment costs of the proposed defense mecha-
nism. Therefore, the following are important evaluation criteria
that we adopt:
‚Ä¢ The percentage of good user trafÔ¨Åc that makes it to the
server. Since the control algorithm ensures that the server
operates under its maximum designed load, the good user
requests that arrive should be adequately served.
‚Ä¢ The number of routers involved in protecting . Because
throttling clips forwarding rate to some preset ceiling, it
is less tolerant to trafÔ¨Åc variabilities than best-effort trans-
missions. For example, normal trafÔ¨Åc that occasionally
exceeds the ceiling and cannot be absorbed by the token
bucket will get clipped, instead of being served by op-
portunistic resource availabilites. We measure the number
of routers at which trafÔ¨Åc is actually dropped due to the
throttle rate limit.
B. Packet Network Results
To evaluate how the proposed throttle mechanism would
perform over a real network, we conducted simulations using
a global network topology reconstructed from real traceroute
36
IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 13, NO. 1, FEBRUARY 2005
data. The traceroute data set is obtained from the Internet map-
ping project at AT&T2. It contains 709 310 distinct traceroute
paths from a single source to 103 402 different destinations
widely distributed over the entire Internet. We use the single
, and randomly select 5000 tracer-
source as our target server
oute paths from the original data set for use in our simulations.
The resulting graph has a total of 135 821 nodes, of which 3879
are hosts. We assume, therefore, that out of all the hosts in the
, either as an
total global network, these 3879 hosts access
attacker or a good user.
1) Evenly Distributed Aggressive Attackers: In our Ô¨Årst set
of experiments, we model aggressive attackers, whose average
individual sending rate is several times higher than that of
normal users. SpeciÔ¨Åcally, each good user is chosen to send
, where the packet interarrival
Ô¨Åxed size UDP packets to
times are Poisson and the average trafÔ¨Åc rate is randomly and
uniformly drawn from the range [0, 2]. Each attacker is chosen
to send trafÔ¨Åc at a rate randomly and uniformly drawn from
is either 10 or 20 according to the
the range
particular experiment. Furthermore, we select attackers and
good users to be evenly distributed in the network topolgy: each
host in the network is independently chosen to be an attacker
with probability , and a good user with probability
, where
.
Fig. 8(a) compares the performance of our algorithm (labeled
‚Äúlevel- max-min fairness‚Äù) with that of the pushback max-min
. We show
fairness approach in [13], for
the percentage of remaining good user and attacker trafÔ¨Åc that
passes the router throttles and arrives at the server. Fig. 8(b) and
,
(c) show the corresponding results when
, respectively. We plot the average
and
results over ten independent experimental runs, and show the
standard deviation as an error bar around the average.
and
and
and
Notice from the Ô¨Ågures that generally, level- max-min
fairness gives signiÔ¨Åcantly better protection for good user trafÔ¨Åc
than pushback max-min fairness. The performance advantage
of level- max-min fairness increases as
increases, until
it levels off at
roughly equal to 20. This is because good
(the increase
trafÔ¨Åc can aggregate to a signiÔ¨Åcant level near
rate can be exponential), making it hard to distinguish from
the attacker trafÔ¨Åc at that location. Since pushback always
in our experimental setup (pushback
originates control at
is designed to originate at the point under attack, which can
be a congested router in general), it can severely punish good
trafÔ¨Åc. By initiating control further away from (speciÔ¨Åcally,
hops away), level- max-min fairness achieves better
about
good user protection.
2) Unevenly Distributed Aggressive Attackers: In this set of
experiments, each good user trafÔ¨Åc rate is chosen randomly and
uniformly from the range [0, 2], while each attacker rate is sim-
ilarly chosen from the range [0, 20]. In each experiment, about
20% of the hosts are chosen to be attackers, and the remaining
hosts to be good users.
In these experiments, we select the attackers to have different
concentration properties. SpeciÔ¨Åcally, we pick Ô¨Åve disjoint sub-
trees from the network topology, labeled in Fig. 9 as 1‚Äì5. The
Ô¨Åve subtrees have properties as shown in Table III. We then de-
Ô¨Åne four concentration conÔ¨Ågurations, 0‚Äì3, for the attackers, as
2http://cm.bell-labs.com/who/ches/map/dbs/index.html
Fig. 8.
(a) Protection for good users under 20% evenly distributed aggressive
attackers: mean attacker rate 10 times mean good user rate. (b) Protection for
good users under 40% evenly distributed aggressive attackers: mean attacker
rate 10 times mean good user rate. (c) Protection for good users under 40%
evenly distributed moderately aggressive attackers: mean attacker rate 5 times
mean good user rate.
shown in Table IV. The intention is for attacker concentration
to increase as we go from conÔ¨Ågurations 0 to 3. (Notice that
the roots of subtrees 4 and 5 in conÔ¨Åguration 3 share a common
parent, and so attacker trafÔ¨Åc converges more quickly than the
subtrees 1 and 3 in conÔ¨Åguration 2.)
YAU et al.: DEFENDING AGAINST DISTRIBUTED DENIAL-OF-SERVICE ATTACKS WITH MAX-MIN FAIR SERVER-CENTRIC ROUTER THROTTLES
37
Fig. 9. Subtrees 1‚Äì5 used in attacker concentration experiments.
TABLE III
PROPERTIES OF SUBTREES 1‚Äì5
CONFIGURED CONCENTRATIONS OF ATTACKERS
TABLE IV
Fig. 10(a) shows the percentage of remaining good trafÔ¨Åc
for the four concentrations, using level- max-min fairness.
Fig. 10(b) shows the corresponding results for pushback
increases, level- max-min
max-min fairness. Notice that as
fairness achieves good protection for the good users in all four
conÔ¨Ågurations. For conÔ¨Ågurations 1‚Äì3, however, notice a ‚Äúdip‚Äù
values between about 6 to
in the achieved protection over
11. For example, the percentage of remaining good trafÔ¨Åc for
, and rises
conÔ¨Åguration 3 decreases from
again afterwards.
to
, for
a few hops larger than
To explain the dip, consider the case when all attackers are
, whose root is
hops away
contained in one subgraph, say
from . For the trafÔ¨Åc seen at
, as decreases from to 1,
there will be more and more aggregation of good user trafÔ¨Åc but
no further aggregation of attack trafÔ¨Åc. This will cause a larger
fraction of good user trafÔ¨Åc to be dropped (its volume is more
comparable to attack trafÔ¨Åc) as throttling is performed with a
. This explains the initial rising curves
smaller
,
in Fig. 10(a) before the dip. For
the aggregation situation for both good user and attack trafÔ¨Åc
is similar to the case of evenly distributed attackers. Hence, we
increases
observe increased protection for good user trafÔ¨Åc as
from
is a small constant. This explains
the rising curves shortly after the dip. At the point when
just
increases past the root of
, however, there is progressively less
aggregation of attack trafÔ¨Åc. This may cause reduced dropping
rate for the attack trafÔ¨Åc (since its volume at the control points is
smaller and more comparable to good user trafÔ¨Åc), when com-
pared with control after full attack trafÔ¨Åc aggregation has oc-
curred at the root of
. This explains the dip itself.
onwards, where
(a) Protection for good users, under
Fig. 10.
four different attacker
concentrations, using level-k max-min fairness. (b) Protection for good
users, under four different attacker concentrations, using pushback max-min
fairness. (c) Comparions of good-user protection between level-k and pushback
max-min fairness ‚Äì for conÔ¨Ågurations 0 and 3 only.
Despite the above ‚Äúanomaly‚Äù, level- max-min fairness con-
sistently and signifcantly outperforms pushback max-min fair-
ness for
. The performance advantage decreases from
0‚Äì3, because pushback max-min fairness becomes more effec-
tive as attackers get more concentrated. Fig. 10(c) more clearly
compares the two approaches by plotting their results together,
for conÔ¨Ågurations 0 and 3.
38
IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 13, NO. 1, FEBRUARY 2005
3) Evenly Distributed ‚ÄúMeek‚Äù Attackers: Router throttling
is most effective when attackers are signiÔ¨Åcantly more aggres-
sive than good users. However, should a malicious entity be able
to recruit or compromise many hosts to launch an attack, then
each of these hosts behaving like a normal user can still together
bring about denial of service. It is inherently more difÔ¨Åcult to
defend against such ‚Äúmeek‚Äù attackers. Our experimental results
(Fig. 11; see also [11]) show that both level-
and max-min
fairness may fail to distinguish between the good users and at-
tackers, and punish both classes of hosts equally. When this hap-
pens, throttling is mainly useful in regulating the server load to
within its operational limits.
4) Deployment Extent: The previous two sets of experi-
ments suggest that, for aggressive attackers, the effectiveness
of level- max-min fairness increases with . At the same time,
however, the cost of deployment may also increase, as the
number of routers in
becomes larger.
Fig. 12 plots the percentage of routers involved in throttling as
, for both level- and pushback max-min fairness.
a function of
(For the level- approach, we count both monitoring and throt-
tling routers.) Notice that the two approaches basically require a
comparable number of deployment points, although for
equal
to 4‚Äì9, pushback max-min fairness is somewhat more efÔ¨Åcient,
and for larger
, level- max-min fairness is somewhat more ef-
Ô¨Åcient. Also, the percentage of deployment points levels off as
in-
creases, a throttling node will likely see a progressively smaller
rate of trafÔ¨Åc destined for
. If the rate is small enough, both
algorithms avoid the actual use of a throttle.
rises above 20 for both approaches. This is because as
5) Web Server Performance: To evaluate the impact of throt-
tling on real user applications, we simulate the performance
of a web server under DDoS attack. The simulations are per-
formed using ns2, and clients access the web server via HTTP
1.0 over TCP Reno/IP. (TCP is interesting because the achieved
throughput by a client also depends on the rate at which acks are
returned from the server to the client.) The simulated network
is a subset of the AT&T traceroute topology described above. It
consists of 85 hosts, of which 20% (i.e., 17 out of 85) are chosen
as attackers. The maximum and average numbers of hops be-
tween a client and the server is 30 and 15, respectively.
Attackers generate UDP trafÔ¨Åc destined for the server, at
a constant rate of 6000 bits/s. Web clients make requests
for documents to the server, where the document sizes and
times between requests are probabilistically generated according
to collected empirical distributions.3 If a request arrives at
the server successfully, the server will return the requested
document after a random processing time, also chosen according
to collected empirical distributions.
We model the web server to have
. We report two experiments with
and
and
, respectively. To compare web server performance with and
without throttling, we plot the rates of client requests that are
successfully processed by the server in both cases, over time.
The aggregate rate at which the clients originally make requests
is also shown for baseline comparison. Each experiment runs
3Please see http://http.cs.berkeley.edu/
tomh/wwwtrafÔ¨Åc.html for further
details.
Fig. 11. Protection for good user trafÔ¨Åc under evenly-distributed ‚Äúmeek‚Äù
attackers, for both level-k and pushback max-min fairness.
Fig. 12. Number of participating routers for level-k and pushback max-min
fairness, as a function of the deployment depth.