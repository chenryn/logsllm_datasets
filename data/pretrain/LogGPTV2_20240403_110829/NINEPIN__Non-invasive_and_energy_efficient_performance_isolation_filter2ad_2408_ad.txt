degradation of 36% in the workload mix 1 due to the fact that
VMs running on adjacent CPU cores experience contention of
the underlying resources.
Next, we use workload mix 1 to evaluate the performance
isolation effectiveness of NINEPIN when the service-level
utility and energy utility functions are not available. In this
case, NINEPIN aims to mitigate the performance interference
effects without optimizing the overall system utility. Table 2
show that the average completion time of each benchmark ap-
plication running on co-located VMs is reduced by NINEPIN,
compared to both Q-Clouds and the default case in Table 1
that does not apply any performance isolation mechanism.
We assume that all four VMs require 50% of the CPU
resource when there is no performance interference. We
measure performance isolation in terms of the normalized
performance of the VMs when they are co-located in the
same virtualized server. The normalization is performed with
respect to the performance shown by the VMs when they
run in isolation. Figures 5(a) and 5(b) show that NINEPIN,
compared to Q-Clouds and the default case that does not
apply any performance isolation mechanism, is able to achieve
much better performance isolation among co-located VMs.
Figure 5(c) shows the CPU resources allocated to mitigate the
performance interference between various hosted applications.
The improvement in performance isolation by NINEPIN is
due to the use of the fuzzy MIMO model, which captures the
performance interference relationship more accurately. Due to
the space limitation, we omit the results of other workload
mixes and refer to the studies with workload mix 1 as the
representative.
B. Optimal Performance Targeting
We evaluate the merits of utility optimization based per-
formance targeting by NINEPIN. We deﬁne a performance
target set as a group of performance targets for the applications
co-located in a virtualized server. Each performance target is
speciﬁed as the desired CPU equivalent performance that is
the percentage of CPU resource required to achieve a certain
performance level [21].
We consider SPEC CPU2006 workload mix 1 that con-
sists of benchmark applications 436.cactusADM, 437.leslie3d,
459.GemsFDTD and 470.lbm. Table 3 gives the performance
target sets. Figures 6(a) and 6(b) compare the system utility
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:50:15 UTC from IEEE Xplore.  Restrictions apply. 
 0 0.2 0.4 0.6 0.8 1 1.2 1.4DefaultQ-CloudsNINEPINNormalized Performance436.cactusADM437.leslie3d459.GemsFDTD470.lbm 0 10 20 30 40 50 60 70 80vs Defaultvs Q-CloudsPerformance Improvement %436.cactusADM437.leslie3d459.GemsFDTD470.lbm 0 20 40 60 80 100DefaultQ-CloudsNINEPINAllocated CPU %436.cactusADM437.leslie3d459.GemsFDTD470.lbm10
(a) System utility.
(b) Energy consumption.
(c) Variation in optimal target.
Fig. 6. Optimal performance targets achieved by NINEPIN.
(a) System utility.
(b) Energy consumption.
(c) Improvement.
Fig. 7. System utility and energy efﬁciency comparison between Q-Clouds and NINEPIN.
TABLE 4
UTILITY AND ENERGY EFFICIENCY.
System Utility
Energy Consumption (KJ)
Q-Clouds
NINEPIN
Q-Clouds
NINEPIN
1667
1467
1500
1400
1470
1961
1942
1955
1840
1900
100.8
115.2
122.4
104.4
118.8
74.67
55.72
82.8
79.2
86.4
Workload Mix
1
2
3
4
5
and energy consumption associated with the performance
target sets. We observe that the performance target set 5, which
is computed with NINEPIN, is able to maximize the system
utility and minimize the energy consumption. Furthermore,
Figure 6(c) shows that the optimal performance targets vary
with all ﬁve different SPEC CPU2006 workload mixes. It
is due to the variation in performance interference relation-
ship and the service-level utility functions corresponding to
the applications of different workload mixes. Nevertheless,
NINEPIN is able assure the optimal performance targets.
C. System Utility and Energy Efﬁciency
A utility based model provides a practical way to integrate
performance assurance and energy efﬁciency goals of data
center applications to maximize the proﬁtability of cloud
service provider. Table 4 compares the overall system util-
ity and energy efﬁciency between NINEPIN and Q-Clouds
for various workload mixes of SPEC CPU2006 suite. Note
that both approaches can mitigate performance interference
between co-located applications. However, NINEPIN provides
signiﬁcantly lower energy consumption while improving the
system utility.
As shown in Figures 7(a) and 7(c), NINEPIN is able
to achieve better system utility than Q-Clouds for all ﬁve
workload mixes of SPEC CPU2006 suite. The system utility is
a combination of the service-level utility of various co-located
applications and the utility of energy consumption. NINEPIN
hierarchical control framework maximizes the overall system
utility by ﬁnding the optimal performance targets based on
utility optimization and regulating the system to achieve the
reference targets using the model predictive controller. The
regulatory action takes place in the form of CPU resource
allocations to co-located VMs in the virtualized server. The
system utility and energy consumption varies with workload
mixes. It is because different combination of applications co-
located in a virtualized server manifest different performance
interference relationships, energy consumption patterns and
service-level utility functions. On average, the improvement
in the system utility by NINEPIN is 28%.
Figures 7(b) and 7(c) illustrate the improvement in energy
efﬁciency by NINEPIN for various workload mixes of SPEC
CPU2006 suite. NINEPIN reduces the energy consumption of
the virtualized server by controlling each VM’s CPU usage
limits according to an energy usage model. It
is able to
tradeoff the utility of meeting performance objectives with
energy efﬁciency. On the other hand, Q-Clouds always aims
to achieve a ﬁxed performance target without considering the
cost of energy consumption. On average, the improvement in
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:50:15 UTC from IEEE Xplore.  Restrictions apply. 
 0 500 1000 1500 2000 1 2 3 4 5System UtilityPerformance Target Set 0 20 40 60 80 100 120 140 160 1 2 3 4 5Energy Consumption (Kilojoules)Performance Target Set 0 20 40 60 80 100 120 14012345CPU Equivalent Performance %Workload Mix436.cactusADM437.leslie3d459.GemsFDTD470.lbm471.omnetpp 1000 1200 1400 1600 1800 2000 220012345Overall System Utilityworkload mixQ-CloudsNINEPIN 0 20 40 60 80 100 120 14012345Energy Consumption (Kilo Joules)workload mixQ-CloudsNINEPIN 0 10 20 30 40 50 6012345% improvementworkload mixsystem utilityenergy efficiency11
(a) Performance.
(b) Energy usage.
(c) Improvement.
Fig. 8. Prediction accuracy of NINEPIN’s performance and energy usage models.
(a) Dynamic workload.
(b) System utility.
(c) Energy Consumption.
Fig. 9. NINEPIN robustness in the face of heterogeneous applications and dynamic workload variation.
IMPROVEMENT IN SYSTEM UTILITY AND ENERGY EFFICIENCY.
TABLE 5
System Utility
Energy Efﬁciency
Compared with
Default
Q-Clouds
Default
Q-Clouds
RUBiS workload
500 clients
1000 clients
20%
7.3%
12%
9.7%
160%
137%
27%
23%
energy efﬁciency by NINEPIN over Q-Clouds is 32% running
the SPEC CPU2006 mixes.
D. NINEPIN Robustness
We evaluate the robustness of NINEPIN against application
heterogeneity and dynamic workload variation. As a case
study, we run one interactive three-tier application RUBiS
with a dynamic workload and one SPEC CPU2006 benchmark
application, 470.lbm, in the same virtualized server. RUBiS
application initially faces a workload of 500 concurrent users.
At the ﬁfth control interval, the workload intensity is doubled.
The prediction accuracy of NINEPIN’s system models in the
face of dynamic workload variation has a signiﬁcant impact
on its robustness. Thus, we ﬁrst measure the accuracy of the
fuzzy MIMO models obtained by NINEPIN for performance
and energy usage prediction. The accuracy is measured by the
normalized root mean square error (NRMSE), a standard met-
ric for deviation. We compare our results with the modeling
technique used in Q-Clouds.
Figures 8(a), (b) and (c) show that NINEPIN outperforms
Q-Clouds in predicting the performance of co-located appli-
cations and the energy usage of the underlying server under
different workload intensities. The average improvement in the
prediction accuracy of performance and energy usage are 26%
and 23% respectively. The improvement is more signiﬁcant
when the workload changes from 500 concurrent users to
1000 concurrent users. It is due to the fuzzy MIMO model’s
ability to adapt more effectively to the change in workload
and capture the inherent non-linearity of the system.
We measure the system utility and energy usage in the face
of a dynamic workload shown in Figure 9(a). Figure 9(b)
illustrates the instantaneous system behavior of the virtualized
server under the inﬂuence of Q-Clouds and NINEPIN mech-
anisms for performance isolation. We observe that NINEPIN
achieves consistently lower energy consumption and improved
system utility as compared to Q-Clouds. At the ﬁfth control
interval, there is a sharp decline in the system utility for
both performance isolation mechanisms. It is due to a sudden
change in the performance interference relationship between
heterogeneous applications, which is caused by the workload
variation. Furthermore due to increase in the workload inten-
sity, the energy consumption by the underlying server also in-
creases. Indeed, performance interference effects are impacted
by the workload intensity as well as characteristics of co-
located applications. Note that the performance improvement
by NINEPIN is more signiﬁcant after the ﬁfth control interval.
It is due to its ability to re-compute and assure the optimal
operating conditions of the system in response to the changing
performance interference relationship between heterogeneous
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:50:15 UTC from IEEE Xplore.  Restrictions apply. 
 0 5 10 15 20 25 30 35 40500 users1000 usersNormalized root mean square error (%)workloadQ-Clouds predictionNINEPIN prediction 0 5 10 15 20 25 30 35 40500 users1000 usersNormalized root mean square error (%)workloadQ-Clouds predictionNINEPIN prediction 0 5 10 15 20 25 30 35 40500 users1000 users% Improvement in prediction accuracyworkloadperformanceenergy usage 200 400 600 800 1000 1200 0 2 4 6 8 10Number of concurrent RUBiS userscontrol intervalRUBiS workload 0 100 200 300 400 50012345678910utilitycontrol intervalQ-CloudsNINEPIN 0 20 40 60 80 100 120 140 1601-56-10Energy Consumption (Kilo Joules)control interval rangeQ-CloudsNINEPINapplications. Figure 9(c) summarizes the energy consumption
improvement by NINPIN.
Table 5 shows the improvement
in system utility and
energy efﬁciency by NINEPIN for different RUBiS workloads,
compared to Q-Clouds and the default case that does not apply
any performance isolation mechanism. In the two scenarios,
NINEPIN outperforms Q-Clouds in average energy efﬁciency
and average system utility by 16% and 72%, respectively.
VIII. CONCLUSION AND FUTURE WORK
Performance isolation among heterogeneous customer ap-
plications is an important but very challenging problem in a
virtualized data center. NINEPIN provides a desirable non-
invasive performance isolation mechanism for a data center
hosting third-party customer applications and using virtual-
ization software from third-party vendors. As demonstrated
by modeling, analysis and experimental results based on
the testbed implementation, its main contributions are robust
performance isolation of heterogeneous applications, energy
efﬁciency and overall system utility optimization. It increases
data center utility by aligning performance isolation goals with
a data center’s economic optimization objective. The main
technical novelty of NINEPIN is due to the proposed and
developed hierarchical control framework that integrates the
strengths of machine learning based system modeling, utility
based performance targeting and a model predictive control
based target tracking and optimization.
Our future work will extend NINEPIN to address perfor-
mance interference between I/O bound workloads.
ACKNOWLEDGEMENT
This research was supported in part by NSF CAREER award
CNS-0844983.
REFERENCES
[1] Y. Chen, A. Das, W. Qin, A. Sivasubramaniam, Q. Wang, and N. Gau-
tam. Managing server energy and operational costs in hosting centers.
In Proc. ACM SIGMETRICS, pages 303–314, 2005.
[2] S. Cho and L. Jin. Managing distributed, shared l2 caches through
In IEEE/ACM Proc. Int’l Symposium on
os-level page allocation.
Microarchitecture (MICRO), 2006.
[3] A. Fedorova, M. Seltzer, and M. D. Smith.
Improving performance
isolation on chip multiprocessors via an operating system scheduler.
In Proc. Int’l Conference on Parallel Architecture and Compilation
Techniques (PACT), 2007.
[4] A. Gandhi, M. Harchol-Balter, R. Das, and C. Lefurgy. Optimal power
allocation in server farms. In Proc. ACM SIGMETRICS, 2009.
[5] D. Gmach, J. Rolia, and L. Cherkasova. Resource and virtualization
costs up in the cloud: Models and design choices. In Proc. IEEE/IFIP
Int’l Conference on Dependable Systems and Networks (DSN), 2011.
[6] J. Gong and C.-Z. Xu. vpnp: Automated coordination of power and
performance in virtualized datacenters. In Proc. IEEE Int’l Workshop
on Quality of Service (IWQoS), 2010.
[7] D. Gupta, L. Cherkasova, R. Gardner, and A. Vahdat. Enforcing
In Proc.
performance isolation across virtual machines in xen.
ACM/IFIP/USENIX Int’l Conference on Middleware, 2006.
[8] C. Jiang, X. Xu, J. Wan, J. Zhang, X. You, and R. Yu. Power aware
job scheduling with qos guarantees based on feedback control. In Proc.
IEEE Int’l Workshop on Quality-of-Service (IWQoS), 2010.
[9] G. Jung, K. Joshi, M. A. Hiltunen, K. R. Joshi, R. D. Schlichting,
and C. Pu. Performance and availability aware regeneration for cloud
In Proc. IEEE/IFIP Int’l Conference on
based multitier applications.
Dependable Systems and Networks (DSN), 2010.
12
[10] R. Knauerhase, P. Brett, B. Hohlt, T. Li, and S. Hahn. Using os
IEEE
observations to improve performance in multicore systems.
MICRO, 28(3):54–66, 2008.
[11] Y. Koh, R. Knauerhase, P. Brett, M. Bowman, W. Zhihua, and C. Pu.
An analysis of performance interference effects in virtual environments.
In Proc. IEEE Int’l Symposium on Performance Analysis of Systems
Software (ISPASS), 2007.
[12] D. Kusic and J. O. Kephart. Power and performance management of
In Proc.
virtualized computing environments via lookahead control.
IEEE Int’l Conference on Autonomic computing (ICAC), 2008.
[13] P. Lama and X. Zhou. Autonomic provisioning with self-adaptive neural
fuzzy control for end-to-end delay guarantee. In Proc. IEEE/ACM Int’l
Symposium on Modeling, Analysis, and Simulation of Computer and
Telecommunication Systems (MASCOTS), pages 151–160, 2010.
[14] P. Lama and X. Zhou.
aMOSS: Automated multi-objective server
provisioning with stress-strain curving. In Proc. IEEE Int’l Conference
on Parallel Processing (ICPP), pages 345–354, 2011.
[15] P. Lama and X. Zhou. PERFUME: Power and performance guarantee
In Proc. IEEE Int’l
with fuzzy mimo control in virtualized servers.
Workshop on Quality of Service (IWQoS), pages 1–9, 2011.
[16] P. Lama and X. Zhou. Efﬁcient server provisioning with control for
end-to-end delay guarantee on multi-tier clusters. IEEE Transactions on
Parallel and Distributed Systems, 23(1), 2012.
[17] K. Le, R. Bianchiniy, M. Martonosiz, and T. D. Nguyeny. Cost- and
energy-aware load distribution across data centers. In Proc. Workshop
on Power Aware Computing and Systems (HotPower), 2009.
[18] J. C. B. Leite, D. M. Kusic, D. Moss´e, and L. Bertini. Stochastic
approximation control of power and tardiness in a three-tier Web-hosting
cluster. In Proc. IEEE Int’l Conference on Autonomic computing (ICAC),
2010.
[19] X. Leon and L. Navarro. Limits of energy saving for the allocation of
In Proc. IEEE Int’l
data center resources to networked applications.
Conference on Computer Communications (INFOCOM), 2011.
[20] J. Lin, Q. Lu, X. Ding, Z. Zhang, X. Zhang, and P. Sadayappan. Gaining
insights into multicore cache partitioning: Bridging the gap between
simulation and real systems. In Proc. Int’l Symp. on High Performance
Computer Architecture (HPCA), 2008.
[21] R. Nathuji, A. Kansal, and A. Ghaffarkhah. Q-clouds: managing
performance interference effects for qos-aware clouds. In Proc. of the
5th ACM European conference on Computer systems (EuroSys), 2010.
[22] P. Padala, K.-Y. Hou, K. G. Shin, X. Zhu, M. Uysal, Z. Wang, S. Singhal,
and A. Merchant. Automated control of multiple virtualized resources.
In Proc. of the EuroSys Conference (EuroSys), pages 13–26, 2009.
[23] C. Pham, D. Chen, Z. Kalbarczyk, R. Iyer, S. Sarkar, and R. Hosn.
Cloudval: A framework for validation of virtualization environment in
cloud infrastructure. In Proc. IEEE/IFIP Int’l Conference on Dependable
Systems and Networks (DSN), 2011.
[24] D. K. Tam, R. Azimi, L. B. Soares, and M. Stumm. Rapidmrc:
approximating l2 miss rate curves on commodity systems for online
In Proc. Int’l Conference on Architecture Support for
optimizations.
Programming Language and Operating System (ASPLOS), 2009.
[25] B. Urgaonkar, P. Shenoy, A. Chandra, P. Goyal, and T. Wood. Agile
dynamic provisioning of multi-tier Internet applications. ACM Trans.
on Autonomous and Adaptive Systems, 3(1):1–39, 2008.
[26] W. E. Walsh, G. Tesauro, J. O. Kephart, and R. Das. Utility functions
In Proc. IEEE Int’l Conference on Autonomic
in autonomic systems.
Computing (ICAC), 2004.
[27] X. Wang and Y. Wang. Coordinating power control and performance
management for virtualized server clusters. IEEE Trans. on Parallel and
Distributed Systems, 22(2), 2011.
[28] Y. Xie and G. H. Loh. Pipp: promotion/insertion pseudo-partitioning
In Proc. Int’l Symposium on Computer
of multi-core shared caches.
architecture (ISCA), 2009.
[29] X. Zhang, S. Dwarkadas, and K. Shen. Towards practical page coloring-
based multicore cache management. In Proc. of the 4th ACM European
conference on Computer systems (EuroSys), 2009.
[30] L. Zhao, R. Iyer, R. Illikkal, J. Moses, S. Makineni, and D. Newell.
Cachescouts: Fine-grain monitoring of shared caches in cmp platforms.
In Proc. Int’l Conference on Parallel Architecture and Compilation
Techniques (PACT), 2007.
[31] S. Zhuravlev, S. Blagodurov, and A. Fedorova. Addressing shared
In Proc.
resource contention in multicore processors via scheduling.
Int’l Conference on Architecture Support for Programming Language
and Operating System (ASPLOS), 2010.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:50:15 UTC from IEEE Xplore.  Restrictions apply.