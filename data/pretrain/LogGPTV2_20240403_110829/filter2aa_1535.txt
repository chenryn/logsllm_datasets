### Comparing Application Security Tools
**Defcon 15 - August 3, 2007**
**Presenter: Eddie Lee, Fortify Software**

#### Agenda
- Introduction to the Experiment
- Methodology for Reproducing the Experiment
- Results from the Experiment
- Conclusions

#### Introduction
- **Tools Used:**
  - "Market Leading" Dynamic Testing Tools
  - A Static Code Analyzer
  - A Dynamic Test Tracing Tool

- **The Application:**
  - Open-source Java-based Blog: [Pebble](http://pebble.sourceforge.net)
  - **Reasons for Choosing This Application:**
    - It is a well-documented and widely used open-source project.
    - It provides a realistic and complex environment for testing.

#### The Experiment
- **Out-of-the-Box Scans:**
  - Compared findings from each tool.

- **How the Tools Work:**
  - **Dynamic Testing Tools:**
    - Fuzz web form input
    - Signature and Behavioral Matching
    - Modes of Scanning:
      - Auto-crawl
      - Manual crawl
  - **Static Code Analyzer:**
    - Data flow analysis
    - Control flow analysis
    - Semantic analysis
  - **Dynamic Test Tracing Tool:**
    - Bytecode instrumentation
    - Monitors data entering and exiting the application
    - Runs in conjunction with other dynamic testing tools

#### Methodology
- **Reproducing Experiments with Dynamic Testing Tools:**
  1. Download the source code.
  2. Build and deploy the application.
  3. Determine how to cleanly undeploy the application.
  4. Clear the database or stored files.
  5. Run the scanner in auto-crawl mode.
  6. Ensure the application does not break during scans.
  7. If the application breaks, identify and configure the scanner to ignore the parameters causing the issue.
  8. Note that ignored parameters will not be tested for vulnerabilities, indicating potential DoS vulnerabilities.
  9. Undeploy and redeploy the application.
  10. Repeat the process.
  11. Save the results from the last clean run.
  12. Repeat the process in manual-crawl mode.
  13. Verify the results through manual testing.
  14. Record the false positive rate.
  15. Normalize the results by recording the source file and line number where vulnerabilities occur.

- **Reproducing Experiments with Static Testing Tools:**
  1. Point the scanner at the code and specify the location of required libraries.
  2. Scan the same code used in other tests.
  3. Verify true positives and weed out false positives.
  4. Manually test the running application.
  5. Record the false positive rate.
  6. Normalize the results.

- **Reproducing Experiments with Dynamic Tracing Tools:**
  1. Instrument the compiled code.
  2. Deploy the instrumented code.
  3. Start recording.
  4. Perform dynamic testing.
  5. Stop recording.
  6. Verify true positives and weed out false positives.
  7. Manually test the running application.
  8. Record the false positive rate.
  9. Normalize the results.

#### Setup and Result Quantification
- **Tool Configuration and Setup:**
  - **Dynamic Testing Tools:**
    - Modes of operation: Auto Crawl & Manual Crawl
    - Minor tweaking for the application

- **Quantification of Results:**
  - Tools report vulnerabilities in different units.
  - Standardize on the location in the source code where the vulnerability occurs.
  - Normalize reported numbers for comparison among tools.

#### Results
- **Overview:**
  - **Unique to Tool:**
    - **XSS Example:**
      - **File:** `blogEntry.jsp`
      - **Line Number:** 16
      - **Parameter:** `Category`
      - **URL:** `saveBlogEntry.secureaction`
      - **Detected By:**
        - Tool #5a
        - Tool #4a
        - Tool #3a
        - Tool #2b
        - Tool #2a
        - Tool #1b
        - Tool #1a

- **Exploit Examples:**
  - **Cross-Site Scripting (XSS):**
    - **File:** `Error.jsp`
    - **Line Number:** 18
    - **Code:**
      ```java
      Request URI : ${pageContext.request.requestURI}
      ```
    - **Attack:**
      ```
      http://host/pebble//createDirectory.secureaction?type=blogFile
      ```

  - **Path Manipulation:**
    - **File:** `DefaultSecurityRealm.java`
    - **Line Number:** 213
    - **Code:**
      ```java
      return new File(getFileForRealm(), username + ".properties");
      ```
    - **Attack:**
      ```
      http://host/pebble/saveUser.secureaction?username=../../../../../../../../etc/passwd%00&newUser=true&name=joe&emailAddress=PI:EMAIL&website=blah.com
      ```

  - **Arbitrary URL Redirection:**
    - **File:** `RedirectView.java`
    - **Line Number:** 85
    - **Code:**
      ```java
      response.sendRedirect(getUri());
      ```
    - **Attack:**
      ```
      http://host/pebble/logout.action?redirectUrl=http://www.attacker.com
      ```

- **Manual Audit:**
  - Vulnerabilities not detected by any tool (from just one file).

- **Cross-Site Scripting Detection by Tool:**
  - **Tool 1b:**
  - **Tool 1b and Tool 2b:**
  - **Tool 2b:**
  - **Not Detected by Any Tool:**
  - **Tool 5a:**
  - **Detected by All Tools:**
  - *Note: Findings from 1a, 2a, 3a, and 4a were not significant and are not shown.*

#### Conclusions
- A single tool is insufficient for comprehensive security testing.
- Using multiple tools significantly increases the number of vulnerabilities found.
- There is little overlap between the tools.
- Tools alone are not enough; manual verification is essential.
- Conduct these tests on your own applications to evaluate their performance in your specific environment.
- Fuzzing tools can cause disruptions and require significant time to scan and troubleshoot.
- Expect these tests to be time-consuming.

#### Q&A
Thank you!