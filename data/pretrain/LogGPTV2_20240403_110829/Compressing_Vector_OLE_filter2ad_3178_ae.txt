### 2.8 Low-Weight Parity-Check Attack

The low-weight parity-check attack requires, on average, \(\frac{n_1}{n_1 - n_0 - 1} t\) iterations. In each iteration, the adversary must compute a weight-\((n_0 + 1)\) parity-check, which involves \((n_0 + 1)\) arithmetic operations. The "parity-check cost" in Tables 1 and 2 provides a lower bound on the bit-security of the LPN instance with respect to this attack, calculated as:

\[
\left\lceil \log_2 \left( (n_0 + 1) \cdot \left( \frac{n_1}{n_1 - n_0 - 1} \right)^t \right) \right\rceil
\]

### 2.9 Inverse Syndrome Decoding (ISD) Attack

We now consider the ISD attack, for which many variants have been developed over the years. Estimating the asymptotic costs of these variants can be challenging. However, in our parameter setting, the noise rate \(\frac{t}{n_1}\) is very small, and the advantages of more advanced algorithms, such as those based on Prange's original algorithm [Pra62], are negligible, as shown in the analysis by [TS16]. Therefore, we focus on bounding the cost of Prange's original algorithm. 

Given that this attack performs much worse than Gaussian elimination and low-weight parity-check attacks, it leaves a significant security gap. We use the detailed concrete efficiency analysis from [HOSSV18] to determine the bit-security of the LPN instance with respect to Prange's algorithm, which is upper-bounded by:

\[
\left\lceil \log_2 \left( \binom{n_1}{t} \cdot \left( \frac{1}{\binom{n_1 - n_0}{t}} \right)^{2.8} \right) \right\rceil
\]

This upper bound is used to calculate the "ISD cost" in Tables 1 and 2.

### 5.2 Time-Complexity Optimizations

In this section, we describe optimizations to improve the computational efficiency of \(G_{\text{primal}}\) and \(G_{\text{dual}}\). Using uniformly random matrices \(C_{k,n}\) and \(H_{n',n}\) in \(G_{\text{primal}}\) and \(G_{\text{dual}}\) reduces their security to the standard LPN assumption but is computationally inefficient, as a random linear mapping takes quadratic time to compute. We discuss methods to improve computational complexity by using LPN-friendly codes that are efficiently encodable.

#### Efficiently Encodable LPN-Friendly Codes

While the standard LPN assumption is defined with respect to uniformly random linear codes, it is common to assume the hardness of LPN with respect to other types of codes. Below are some possible alternatives:

- **Local Codes**: The hardness of LPN for local linear codes is well-established [Ale03]. A local linear code with a constant locality parameter \(d\) means each codeword symbol is a linear combination of \(d\) message symbols. Such codes have a trivial linear-time encoding algorithm. For \(G_{\text{primal}}\), using a \(d\)-local code with \(d = 10\) provides a reasonable security level, and the primal linear mapping can be computed using \(d \cdot n\) multiplications over \(F\). However, local codes cannot be used with \(G_{\text{dual}}\) because the dual code of a local code is an LDPC code, for which efficient decoding algorithms exist, making LPN insecure.

- **LDPC Codes**: An alternative for \(G_{\text{dual}}\) is to use the transpose of an LDPC encoder. While the encoding matrix of an LDPC code is not sparse, it admits a linear-time encoding algorithm over arbitrary fields [KS12]. By the transposition principle [Bor57, IKOS08], the transposed mapping can be computed with similar circuit complexity. Using this code, the compressive linear mapping requires at most \(d \cdot (2n' - n)\) multiplications. With \(n' = c \cdot n\), the cost is \((2c - 1) \cdot d \cdot n\) multiplications, where \(d = 10\).

- **MDPC Codes**: A more conservative variant is to use MDPC codes, where the parity-check matrix has row weight \(O(\sqrt{n})\). MDPC codes have been studied extensively, particularly in optimized variants of the McEliece cryptosystem [MTSB12].

- **Quasi-Cyclic Codes**: Another option is to use quasi-cyclic codes, which admit fast (albeit superlinear) encoding algorithms. These codes have been used in optimized variants of LPN-based and code-based cryptosystems [ABD+16, MBD+18].

- **Druk-Ishai Codes**: Finally, one can use the linear-time encodable codes developed by Druk and Ishai [DI14]. Their construction combines a good linear encoding and its transpose with random local mixing, leading to codes that meet the Gilbert-Varshamov bound and do not support efficient decoding, while having a fast (linear-time) encoding algorithm.