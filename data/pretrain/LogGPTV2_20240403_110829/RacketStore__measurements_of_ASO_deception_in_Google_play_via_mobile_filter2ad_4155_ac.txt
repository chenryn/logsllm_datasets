devices. Most devices report at least 100 snapshots per day.
Figure 5: Comparison of the number and types of accounts
registered on devices controlled by ASO workers and regu-
lar participants. Worker devices tend to have more Gmail
accounts, but fewer account types and non-Gmail accounts
than regular devices.
regular users since this impacts the number of reviews that they
can post.
Figure 5 (left) shows the number of Gmail accounts registered
on the 145 regular and 390 worker devices that have reported such
information. The other participants either did not grant this per-
mission or the server did not receive enough snapshots from their
devices. We found significant differences between regular user and
worker-controlled devices. Worker devices have an average of 28.87
accounts registered per device (M = 21, SD = 29.37); 13 worker
devices have more than 100 Gmail accounts registered, with a max-
imum of 163 accounts per device. In contrast, regular devices have
a maximum of 10 accounts registered (M = 2 , SD = 1.66). The KS
test, and parametric and non-parametric ANOVA found statisti-
cally significant differences between workers and regular users
(p-value< 0.05).
Figure 5 (center) shows the number of different account types
registered on participant devices. On average, regular devices have
Figure 7: Distribution of time between app install and app
review, for regular and worker devices. Each point is one re-
view. Unlike regular users, worker-controlled accounts post
many more reviews and tend to do it soon after installation.
13,376 of the reviews from worker accounts were posted af-
ter at most one day after installation.
a regular device is only 36. We found statistically significant differ-
ences between workers and regular users via KS and parametric
and non-parametric ANOVA (p-value< 0.05).
We observe an average of 65.45 and 77.56 apps installed on
regular and worker devices respectively. KS reported significant
differences in the distributions (p-value= 0.008), but ANOVA did
not find a statistically significant difference (p-value= 0.301). This
is expected since the number of installations is limited by the device
resources. However, on average, worker devices have posted re-
views for 40.51 of the currently installed apps, while regular devices
did it for an average of 0.7 apps.
Install-to-Review Time. The Android API reports the time of
each app’s last install. There are thus two cases. First, the time
when an app was installed on a participant device is before the
time when the participant reviewed the app. This is likely the case
where the collected review is from the currently installed version
of the app on that device. Second, the install time is after the review
time. This implies that the review we collected is from a previous
install of the app. For our analysis, we only consider the first case.
Figure 7 shows the distribution of the time between app install
and app review, for regular and worker devices. Each point is one
review. An app can be reviewed from multiple accounts registered
on the same device; each such review is a different point. To com-
pute the install-to-review time, we used the Android API to get the
installation time, and our review crawler to get the review times-
tamp. However the Android API only retains the last installation
time of an app in a device. We have not considered reviews whose
install-to-review times were negative, since they are the result of a
past install.
We observe substantial difference in the number of reviews
posted from accounts registered on worker vs. regular devices for
apps that provided an installation time: accounts on regular devices
only wrote 35 reviews, while those on worker devices posted 40,397
reviews. Both ANOVA and KS tests found statistically significant
differences between the two groups (p−value<0.05).
Further, workers tend to review apps much sooner after instal-
lation. 13,376 of the 40,397 reviews posted from the accounts reg-
istered on worker devices were posted after at most one day after
the app was installed. Workers register an average of 10.4 days of
waiting time between installation and review (M = 5.00 days, SD
= 13.72 days, max = 574 days). We have observed 25 cases with
waiting times longer than 100 days and 4 cases of reviews posted
Figure 6: Number of installed apps (left), installed and re-
viewed (center), and total number of reviews posted from all
accounts registered (right). We see dramatic differences be-
tween worker and regular devices in the apps reviewed from
all their registered accounts.
registered accounts for 6 services (max = 19), mostly for different
social networks (Facebook, WhatsApp, Telegram, etc). In contrast,
worker devices have accounts mainly for Google services and other
services useful for ASO work, e.g., dualspace.daemon (to enable
installation of the same app multiple times) and freelancer (to find
work). Both KS and ANOVA analyses reveal significant differences
between workers and regular users (p−value< 0.05) in terms of
their numbers of non-Gmail accounts.
We have followed up with several participant workers. Six of
the ASO workers who replied claimed that they personally own
only 1-4 Google accounts. For instance, one worker said “I have
two accounts. One account is mine, another is my mom’s.” Four other
workers however claimed (and we verified) to control between 10
to 50 Gmail accounts, and one claimed to have “many accounts”.
Summary of Findings. We confirmed that participant ASO work-
ers have registered significantly more accounts on their devices
than regular users. Workers have however less diversity in the
online sites for which they registered accounts. Their accounts are
specialized for ASO work, focusing on Gmail and Dualspace that
enable them to install and review a single app multiple times.
6.3 Installed Apps
We now investigate the hypothesis (inspired from [38]) that workers
and regular users differ in the manner in which they interact with
installed apps.
Apps Installed and Reviewed. Figure 6 compares the distribution
of the number of installed apps (left), the number of apps installed
and reviewed (center), and the total number of apps reviewed from
any account registered (right) for the 143 regular and 400 worker
devices that reported this data. We observe dramatic differences
between worker and regular devices in terms of the total number
of reviews posted from registered accounts: On average, a worker
device is responsible for a total of 208.91 reviews, while a regular
user device has only posted an average of 1.91 reviews. We found 11
worker-controlled devices each responsible for more than 1,000 to-
tal reviews. In contrast, the maximum number of total reviews from
645
Figure 8: Boxplot of stopped apps for regular and worker
devices. Worker devices tend to have more stopped apps, but
we also observe substantial overlap with regular devices.
after more than 1,000 days from 2 workers and for 2 apps (Facebook
and Easypaisa). These rare cases of prolonged waiting times are
expected of apps used for personal purposes.
In contrast, only 4 out of the 35 reviews posted from the accounts
of regular users were posted after at most one day after install. Reg-
ular users also wait for 85.09 days to post a review on average
(M=21.92 days, SD=140.56 days, max=606.11 days) with only 12
users waiting less than 12 days to post a review. This longer waiting
time is consistent with a review activity that proceeds from a pre-
vious interaction to form a judgment, and is inconsistent with paid
promotion services. KS and ANOVA found statistically significant
differences (p−value<0.05).
We have observed 25 cases for worker devices having waiting
times longer than 100 days, and 4 cases of reviews posted after more
than 1,000 days (from 2 workers for Facebook and 2 for Easypaisa).
These rare cases of prolonged waiting times may be indicative of
apps used for personal purposes.
Stopped Apps. We have further studied the number of apps that
are stopped on the devices of regular and worker participants. A
freshly installed app is in a stopped state until the user opens it for
the first time. Android devices also allow users to stop apps, instead
of uninstalling them. Figure 8 shows that some worker devices have
significantly more stopped apps than regular devices. KS test and
ANOVA found statistically significant differences between workers
and regular users (p-value< 0.05). We conjecture that this occurs
because ASO workers (1) often do not open the apps that they
install in order to promote, and (2) even if they open them and need
to keep them installed, e.g., to provide retention installs (§ 2), they
prefer to stop apps that misbehave.
We followed up with several participants to clarify this point, i.e.,
whether they stop apps and why. Eight workers claimed to never
stop apps, which suggests reason #1 applies. One worker however
admitted reason #2 applies, i.e., “The quality of some apps was bad,
I stopped those apps”. Another claimed that limited storage is to
blame, “Sometimes the apps get hanged due to a lack of storage”.
Third-Party App Stores. We observed that some participant de-
vices had apps installed that were not available in Google Play. We
followed up with participants to ask if they install apps from other
app stores. The workers conjectured that Google does not host
such apps because they violate Google’s policy, e.g., “Google’s policy
prohibits the use of such apps, or because they are not secure.
One worker admitted to have installed apps from third-party
stores, i.e., “The client gives us a link, we go and install that app”.
Three other workers claimed to install apps from other app stores
Figure 9: App churn: Scatterplot of average number of daily
installs vs. average number of daily uninstalls (log scale) for
regular and worker-controlled devices. Each dot is one de-
vice. The app churn of most regular devices is less than 10
apps per day, while for many worker devices it is above 10
apps per day.
for personal reasons, e.g., to play games (Dream11) or avoid sub-
scription fees (Netflix, Hotstar) by installing modded apps 1:
“I use a modded version of the apps that are not in the Google Play
Store. You do not have to open an account to use these. For instance,
Netflix or Hotstar apps charge a subscription fee every month. But I
don’t have that much money so I install the modded version. By doing
this I get premium access for free. ”
App Churn: Install and Uninstall Events. Figure 9 shows the
average number of daily install events and daily uninstall events for
participant worker and regular devices, computed over all the days
when RacketStore was installed. Workers tend to install apps more
often compared to regular users. Concretely, worker devices had an
average of 15.94 daily installs (M = 6.41, SD = 27.37) while regular
devices had an average of 3.88 daily installs (M = 2.0, SD = 7.29).
KS test and ANOVA reported statistically significant differences
between the two groups (p-value< 0.05).
We recorded fewer daily uninstalls, suggesting that participant
devices tend to retain apps: worker devices recorded an average of
7.02 daily uninstalls (M = 2.73, SD = 15.69) and regular devices had
an average of 3.29 daily uninstalls (M = 1.8, SD = 6.87). The KS and
ANOVA tests reported significant differences at p-value<0.05. We
observe however that several worker devices have a low daily app
churn, while some regular devices have a higher daily app churn,
making them harder to distinguish based on this feature alone.
We note that the differences in app churn between workers and
regular participants could also be due to background differences:
ASO workers may be more technically skilled due to the nature
of their work. However, we were also surprised by the relatively
high app churn of regular users. One reason for this may be that
active Instagram users may be more technically skilled than regular
people. We did not investigate the technical expertise of participants
in our study.
1A mod app is a modified version of an original apk, not signed by the original devel-
opers. A modded app may have additional features, unlocked features, and unlimited
in-app currency.
646
Figure 10: Scatterplot of the average number of apps used
per day per device and the number of apps installed in a de-
vice, for regular and worker devices. We observe substantial
overlap between regular and worker devices.
Figure 11: Comparison of exclusive app permissions for reg-
ular and worker devices. Worker devices host apps with the
largest ratio of dangerous to total number of permissions.
Number of Apps Used Per Day. Figure 10 shows for each of the
141 regular and 399 worker devices in our studies (total of 540),
the average number of apps opened per day on the device vs. the
total number of apps installed on that device. We observe that
several worker devices have many more apps installed than regular
devices, and also have more apps used per day. Nevertheless, we
also observe substantial overlap in these features between regular
and worker devices, perhaps due to the fact that several of the
worker devices are organic. This suggests that the daily number
of used apps cannot accurately distinguish between worker and
regular devices.
App Permissions. We studied the distribution of permission re-
quirements for unique apps found on participant devices. Figure 11
shows the number of dangerous permissions vs. the total num-
ber of permissions for each app found exclusively on regular and
worker devices. We found that while some worker devices host apps
with the largest number of dangerous permissions, most installed
apps share a similar permission profile across all device types. This
suggests that the number of permissions requested by an app, in-
cluding the dangerous permissions, will be ineffective in detecting
promoted apps.
647
Figure 12: Comparison of malware occurrence in regular ver-
sus worker devices. Each point corresponds to a unique app
apk hash, with at least 7 VirusTotal flags. The color in the
legend refers to the number of VT engines that flagged the
app. Worker devices host more unique malware which tends
to be present on more devices than for regular users.
We contacted workers to ask about their policy for granting
permissions to apps they promote. Five claimed to grant all per-
missions requested by the apps that they install. However, one
participant claimed selective granting, i.e., “Permissions are given
based on the client request. If the client does not ask, we do not give
all permissions”. Four other workers said that there are permissions
that they grant grudgingly, e.g., one claimed “I don’t like the location
permission because it violates my privacy”, while two others said
claimed to dislike permissions associated with personal data. Two
regular participants claimed not to grant all requested permissions.
One claimed to avoid granting location permissions, the other was
concerned about contacts, images and phone storage permissions.
Summary of Findings. In our study, workers posted reviews for
a significantly higher number of installed apps. Following app in-
stallation, participant workers waited significantly shorter times
to post reviews than regular users. While workers tend to install
more apps per day than regular users, their devices also had signif-
icantly more stopped apps. We conjecture that this happens due
to retention install requirements: workers need to keep promoted
apps installed, but want to avoid the clutter.
6.4 Investigation of Malware and Attitudes
We now investigate the potential and perceived impact of malware
installations on the workers who participated in our study. We
used the VirusTotal research license reports [79] to analyze the
presence of malware on the participant devices. VirusTotal uses
62 detection engines to process apk files. We used the snapshot
collector module (§3) to collect 18,079 distinct hashes corresponding
to 9,911 unique mobile app identifiers installed in 713 participant
devices (549 devices of workers and 164 devices of regular users).
The remaining 90 devices did not provide hash information either
because of permissions or API incompatibility problems. We col-
lected reports for these hashes in VirusTotal; 12,431 hashes were
available in VirusTotal. We did not collect details about the specific
malware families and types detected by the VirusTotal engines,