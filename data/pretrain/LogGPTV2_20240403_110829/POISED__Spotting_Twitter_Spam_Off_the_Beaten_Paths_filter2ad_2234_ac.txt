### Impact of Both Networks on Our Results

Some users in each neighborhood are connected to only one other user, leading to the formation of small communities with just one or two members. To address this, we applied a standard technique called k-core [72] to extract the maximal connected subgraph of each network, ensuring that all nodes have a degree of at least \( k \), where \( k = 2 \).

After obtaining the k-core for all 300 networks, the mean neighborhood size is 271, with some neighborhoods including over 1,000 users. Figure 3(a) shows the histogram of the number of users (neighborhood size) in these 300 neighborhoods. The median neighborhood size is 178, and the mean is 271. Seven neighborhoods include more than 1,000 users.

Figure 3(b) displays the histogram of timeline lengths in our dataset, with an average value of 332 and a median of 177. Our analysis of both small and large neighborhoods demonstrates that our findings are not dependent on the size of the dataset or a specific neighborhood.

In summary, the dataset for testing the first hypothesis includes 15,751,198 English tweets posted by 82,275 users across 300 neighborhoods.

### Community Detection

To identify structural communities, we employed Infomap [69], a widely accepted disjoint community detection algorithm. This algorithm has shown good performance in benchmark tests [43, 44]. In Infomap, a community is defined as a partition that minimizes the average number of bits per step required to describe the trajectories of random walkers.

Infomap detected a total of 2,283 communities in our 300 neighborhoods. On average, each neighborhood contains 8 communities, with a few containing more than 30. Figure 4 shows the histograms for the number of communities and their sizes. The median number of communities per neighborhood is 4, and the mean is 8.

We also evaluated the impact of several other community detection algorithms, including Spinglass [26], Walktrap [64], Leading Eigenvector [60], Fastgreedy [18], and Multilevel [7]. Interestingly, we obtained very similar results, with Infomap slightly outperforming the others.

### Timeline Analysis

As explained in Section 3.3, POISED divides the timeline into several documents. Given that some Twitter accounts are older and some users post more frequently, the lengths of timelines can vary significantly. Figure 3(b) shows the histogram of timeline lengths, with an average of 332 posts and a median of 177. For each user, we considered up to the 300 most recent tweets for our analysis.

Each document contains a fixed number \( l \) of tweets. The number of documents for a user depends on the number of tweets in their timeline. We investigated the impact of \( l \) on the list of detected topics and found no significant difference. However, LDA detects topics slightly more accurately with 20 tweets per document.

### Topic Detection

We cleaned the documents by removing URLs and non-printable characters, and we removed stop words to improve topic detection and obtain detailed topics. We used the LDA implementation provided by the Machine Learning for Language Toolkit (MALLET) [53]. The output includes documents labeled with a series of topics, and we chose to label each document with the topic having the highest weight value. A user may be associated with multiple topics if they have several documents.

MALLET requires parameters such as the number of topics to be found. We experimented with various numbers of topics and found that the best results were obtained with 500 topics. Additionally, we tested different numbers of tweets per document and found that 20 tweets per document yielded the best results. We set the iteration count to 200 to provide more precise topics, albeit with a longer processing time.

### Evaluation: Communities of Interest

#### Metrics and Null Model

We validate Hypothesis H1 by computing three entropy-based metrics: completeness, homogeneity, and V-measure of topics detected in communities. These metrics, proposed by Rosenberg and Hirschberg [68], are commonly used in natural language processing tasks [19, 68].

All three criteria produce scores in the interval [0, 1], with 1 being 'good' and 0 'bad.' Completeness measures whether all documents in a community are assigned to the same topic, while homogeneity measures whether each topic is observed in only one community. V-measure is the harmonic mean of completeness and homogeneity, measuring how well both criteria are met.

To further validate that communities provide additional information about members' common interests, we compare their scores with those of a null model. The null model randomly partitions documents into groups, maintaining the same distribution of group sizes as in the actual communities.

#### Communities Discuss Different Matters While Members Talk About Similar Topics

We examine Hypothesis H1 by comparing the communities of interest detected in our neighborhoods with the random communities of the null model. Figure 5 compares the scores for these two models, showing that the scores for random communities drop substantially. For communities of interest, the average completeness and homogeneity scores are 0.16 and 0.90, respectively, while for random communities, they are 0.063 and 0.49. Z-tests confirm that the p-values for both metrics are lower than 0.0001.

The completeness and homogeneity scores indicate that people in the communities discuss specific topics that differ from those in other communities. Homogeneity results also confirm that communities of interest are effectively distinguishable.

Parameters such as the number of topics and document size can affect our evaluation. Figure 6 shows that the number of topics (ranging from 100 to 1,000) does not significantly change the scores. The best homogeneity score (0.87) is achieved with 500 topics. Similarly, document size (ranging from 5 to 300 tweets) only slightly affects the scores, with the best results at 20 tweets per document.

### Evaluation: Twitter Spam Detection

#### Clustering Similar Messages

POISED observes the diffusion of messages through communities of interest to learn about their parties of interest. We applied four-gram analysis to detect similar messages in every neighborhood, cleaning tweets by removing stop words and punctuation and treating each URL as a word.

In total, we found 1,219,991 groups of similar messages, with sizes ranging from 2 to 94,382. Figure 8(a) shows the CCDF of the size of these groups, following a power-law distribution.

#### Create a Labeled Dataset

To evaluate probabilistic models for spam detection, we need a ground-truth dataset. We selected the top 5,000 groups by size, ranging from 68 to 94,382, and manually labeled them. The tweets were checked by 14 security researchers, who independently labeled each group. Each group was evaluated by three researchers, and the majority vote was considered the final label.

Table 1 provides examples of manually labeled messages, and Table 2 shows the size of each category after labeling all 5,000 clusters. In total, we obtained labels for 1,277,833 tweets, with 44% labeled as normal and 42% as spam. Clusters labeled as normal include more tweets, while only about 8% of groups are labeled as app-generated, but they account for about 33% of tweets.

Labeling the tweets also helps classify users into spam and benign categories. Table 2 shows the number of unique users in each category. Some users appear in multiple categories, possibly due to spam accounts emulating normal users, compromised accounts, or mislabeling.

#### Identifying Parties of Interest

The probabilistic table representing the parties of interest is generated by...