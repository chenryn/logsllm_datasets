### Optimized Text

#### 12. Comparison with Existing Attacks
For the final two rows, which present experiments on image datasets, we compare our results with the set-based attack described in [12], using the reported numbers from their paper. Our method, unlike WBAttack, can operate in a fully black-box manner and is effective even when the label information is not available. As shown in the columns below, with a very small poisoning ratio, our attack achieves an accuracy of 1.0 on the random target property. Additionally, it outperforms WBAttack in other experiments with a minimal number of poisoning points. Notably, our attack requires significantly fewer shadow models. For instance, at a 10% poisoning rate, our attack with just 50 shadow models surpasses the accuracy of WBAttack, which uses 1000 shadow models. The reduced number of shadow models is particularly advantageous in scenarios where the adversary has limited access to similar data. In summary, our attack improves upon WBAttack in terms of both accuracy and the number of shadow models required, while also operating in a black-box setting. The trade-off for these improvements is allowing the adversary to select a fraction of the training set, a common scenario in multi-party learning applications.

To compare the running time of the attacks, in the Census-Gender experiment (Table VIII), WBAttack took 1533 seconds, whereas our black-box attack only required 161 seconds on the same platform and execution environment. It is important to note that we used the same size of held-out data for shadow models in both black-box and white-box experiments.

#### Image Datasets
We replicated the settings of [12] on the MNIST [16] and CelebA [18] datasets, as shown in the last two rows of Table VIII. In the MNIST experiment, the adversary aims to determine whether jitter noise was added to the images before training. The adversary uses 10% poisoning to associate jitter with one of the digits (9 in our experiments) and then queries the model on 200 images with high uncertainty (we use a slightly different notion of uncertainty for multi-label cases). We employed an MLP classifier with three hidden layers of sizes 128, 32, and 16, similar to [12].

In the CelebA experiment, the goal of the classifier is to determine if the face in the image is smiling. The attacker's objective is to find out whether 40% or 60% of the images in the training dataset are male. Our attack adds 10% poisoned examples and then queries the model with 500 uncertain queries. The classifier uses FaceNET [26], a pre-trained feature extractor, to map the images to 512 features, followed by an MLP classifier with two hidden layers of sizes 64 and 16. This setup is identical to that of [12].

#### (IM)POSSIBILITY OF DEFENSES
**Theoretical Analysis:**
Our theoretical analysis suggests that, as long as the learning algorithm (which may include a poisoning defense) maintains its generalization properties, our attack should succeed. A poisoning defense would either degrade the performance of the learning algorithm or fail to defend against our attack. Another argument concerns the input-space nature of existing poisoning defenses in the literature [30]. These defenses typically use a filter function \( F \) applied to the inputs (without considering the labels) to remove outliers from the training set. Our poisoning attack, however, uses real samples from the distribution without altering the input, making it immune to such filter functions. The reason poisoning defenses do not consider labels is that the learner is assumed to lack label information, and removing examples based on their labels introduces bias that harms the algorithm's performance.

**Differential Privacy (DP) Mitigation:**
We empirically evaluated the effectiveness of DP in mitigating our attacks. Table IX shows the results of our attack on models trained with DP. DP can be seen as a dual defense against our attack. First, DP is designed to enhance data privacy, and one might expect reduced leakage when DP is applied. Second, DP is one of the few provable defenses against poisoning attacks [17], and it could potentially weaken our attack due to the poisoning step. However, our results show that even with small values of \( \epsilon \), the attack remains successful. For \( \epsilon \) values less than 1, the attack still achieves an accuracy of 90%. DP starts to mitigate the attack only when \( \epsilon \) is less than 0.5, likely due to the reduction in model utility, as predicted by our theory.

| Noise Multiplier | \( \epsilon \) | Attack Accuracy | Classification Accuracy |
|------------------|---------------|-----------------|-------------------------|
| 0.6              | 4.09          | 0.95%           | 92.3%                   |
| 0.7              | 2.56          | 0.97%           | 92.5%                   |
| 0.8              | 1.68          | 0.92%           | 91.6%                   |
| 0.9              | 1.22          | 0.93%           | 91.7%                   |
| 1.0              | 0.95          | 0.90%           | 91.07%                  |
| 2                | 0.29          | 0.76%           | 84.05%                  |

**Conclusion:**
Poisoning attacks are typically studied in machine learning security, where the adversary's goal is to increase error or inject backdoors into the model. In this work, we initiated the study of poisoning adversaries that aim to increase the information leakage of trained models.

#### References
[1] 3 ways to train a secure machine learning model. https://www.ericsson.com/en/blog/2020/2/training-a-machine-learning-model. Accessed: 2020-03-04.
[2] Giuseppe Ateniese et al. “Hacking smart machines with smarter ones: How to extract meaningful data from machine learning classifiers”. In: IJSN 10.3 (2015), pp. 137–150. DOI: 10.1504/IJSN.2015.071829. URL: https://doi.org/10.1504/IJSN.2015.071829.
[3] Eugene Bagdasaryan et al. “How to backdoor federated learning”. In: International Conference on Artificial Intelligence and Statistics. PMLR. 2020, pp. 2938–2948.
[4] Raef Bassily, Adam Smith, and Abhradeep Thakurta. “Private empirical risk minimization: Efficient algorithms and tight error bounds”. In: 2014 IEEE 55th Annual Symposium on Foundations of Computer Science. IEEE. 2014, pp. 464–473.
[5] Arjun Nitin Bhagoji et al. “Analyzing federated learning through an adversarial lens”. In: International Conference on Machine Learning. PMLR. 2019, pp. 634–643.
[6] Battista Biggio, Blaine Nelson, and Pavel Laskov. “Poisoning attacks against support vector machines”. In: arXiv preprint arXiv:1206.6389 (2012).
[7] Nicholas Carlini et al. “The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks”. In: USENIX Security Symposium. 2018.
[8] Christopher A Choquette Choo et al. “Label-Only Membership Inference Attacks”. In: arXiv preprint arXiv:2007.14321 (2020).
[9] Cynthia Dwork et al. “Calibrating Noise to Sensitivity in Private Data Analysis”. In: Theory of Cryptography, Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006, Proceedings. Ed. by Shai Halevi and Tal Rabin. Vol. 3876. Lecture Notes in Computer Science. Springer, 2006, pp. 265–284. DOI: 10.1007/11681878_14. URL: https://doi.org/10.1007/11681878%5C_14.
[10] Andrew Frank, Arthur Asuncion, et al. “UCI machine learning repository, 2010”. In: URL http://archive.ics.uci.edu/ml 15 (2011), p. 22.
[11] Karan Ganju et al. “Property Inference Attacks on Fully Connected Neural Networks Using Permutation Invariant Representations”. In: Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security. CCS ’18. Toronto, Canada: Association for Computing Machinery, 2018, pp. 619–633. ISBN: 9781450356930. DOI: 10.1145/3243734.3243834. URL: https://doi.org/10.1145/3243734.3243834.
[12] Karan Ganju et al. “Property Inference Attacks on Fully Connected Neural Networks using Permutation Invariant Representations”. In: CCS ’18. 2018.
[13] Yingzhe He et al. Towards Privacy and Security of Deep Learning Systems: A Survey. 2019. arXiv: 1911.12562 [cs.CR].
[14] Matthew Jagielski. “Subpopulation Data Poisoning Attacks”. In: 2019.
[15] Bryan Klimt and Yiming Yang. The enron corpus: A new dataset for email classification research. 2004.
[16] Yann LeCun. “The MNIST database of handwritten digits”. In: http://yann.lecun.com/exdb/mnist/ ().
[17] Mathias Lecuyer et al. “Certified robustness to adversarial examples with differential privacy”. In: 2019 IEEE Symposium on Security and Privacy (SP). IEEE. 2019, pp. 656–672.
[18] Ziwei Liu et al. “Large-scale celebfaces attributes (celeba) dataset”. In: Retrieved August 15.2018 (2018), p. 11.
[19] Saeed Mahloujifar, Dimitrios I Diochnos, and Mohammad Mahmoody. “Learning under p-Tampering Attacks”. In: Algorithmic Learning Theory. PMLR. 2018, pp. 572–596.
[20] Saeed Mahloujifar, Dimitrios I Diochnos, and Mohammad Mahmoody. “The curse of concentration in robust learning: Evasion and poisoning attacks from concentration of measure”. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. 2019, pp. 4536–4543.
[21] Luca Melis et al. “Exploiting Unintended Feature Leakage in Collaborative Learning”. In: 2019 IEEE Symposium on Security and Privacy (SP) (May 2019). DOI: 10.1109/sp.2019.00029. URL: http://dx.doi.org/10.1109/SP.2019.00029.
[22] Milad Nasr, Reza Shokri, and Amir Houmansadr. “Comprehensive privacy analysis of deep learning”. In: 2019 ieee symposium on security and privacy. 2019.
[23] Milad Nasr, Reza Shokri, and Amir Houmansadr. “Machine learning with membership privacy using adversarial regularization”. In: Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security. 2018, pp. 634–646.
[24] Privacy-preserving Collaborative Machine Learning. https://medium.com/sap-machine-learning-research/privacy-preserving-collaborative-machine-learning-35236870cd43. Accessed: 2020-03-04.
[25] Alexandre Sablayrolles et al. “White-box vs Black-box: Bayes Optimal Strategies for Membership Inference”. In: ArXiv abs/1908.11229 (2019).
[26] Florian Schroff, Dmitry Kalenichenko, and James Philbin. “Facenet: A unified embedding for face recognition and clustering”. In: Proceedings of the IEEE conference on computer vision and pattern recognition. 2015, pp. 815–823.
[27] Ali Shafahi et al. “Poison frogs! targeted clean-label poisoning attacks on neural networks”. In: Advances in Neural Information Processing Systems. 2018, pp. 6103–6113.
[28] Reza Shokri et al. “Membership inference attacks against machine learning models”. In: 2017 IEEE Symposium on Security and Privacy (SP). IEEE. 2017, pp. 3–18.
[29] Liwei Song and Prateek Mittal. “Systematic Evaluation of Privacy Risks of Machine Learning Models”. In: arXiv preprint arXiv:2003.10595 (2020).
[30] Jacob Steinhardt, Pang Wei W Koh, and Percy S Liang. “Certified defenses for data poisoning attacks”. In: Advances in neural information processing systems. 2017, pp. 3517–3529.
[31] Fnu Suya et al. “Model-Targeted Poisoning Attacks: Provable Convergence and Certified Bounds”. In: arXiv preprint arXiv:2006.16469 (2020).
[32] Binghui Wang and Neil Zhenqiang Gong. “Stealing hyperparameters in machine learning”. In: 2018 IEEE Symposium on Security and Privacy (SP). IEEE. 2018, pp. 36–52.

#### Appendix A: Omitted Proofs
In this section, we provide the proofs for the tools used in our main theorem. Specifically, we prove Claim 11, Corollary 12, and Claim 13.

**Proof of Claim 11:**
We have:
\[
\Pr[\tilde{Y} = 1 | \tilde{X} = x] = \Pr[\tilde{Y} = 1 | \tilde{X} = x \land E] \cdot \Pr[E | \tilde{X} = x] + \Pr[\tilde{Y} = 1 | \tilde{X} = x \land \bar{E}] \cdot \Pr[\bar{E} | \tilde{X} = x]
\]
\[
= \Pr[E | \tilde{X} = x] + \Pr[Y = 1 | X = x] \cdot \Pr[\bar{E} | \tilde{X} = x]
\]

Now, we need to calculate \(\Pr[E | \tilde{X} = x]\):
\[
\Pr[E | \tilde{X} = x] = \frac{\Pr[\tilde{X} = x | E] \cdot \Pr[E]}{\Pr[\tilde{X} = x | E] \cdot \Pr[E] + \Pr[\tilde{X} = x | \bar{E}] \cdot \Pr[\bar{E}]}
\]
\[
= \frac{\Pr[X^+ = x] \cdot p}{\Pr[X^+ = x] \cdot p + \Pr[X = x] \cdot (1 - p)}
\]

For all \(x \in X\) such that \(f(x) = 1\), we have:
\[
\Pr[X = x] = t \cdot \Pr[X^+ = x]
\]

For all \(x \in X\) such that \(f(x) = 0\), we have:
\[
\Pr[X = x] = (1 - t) \cdot \Pr[X^+ = x]
\]

Combining Equations 2 and 3, for all \(x \in X\) such that \(f(x) = 1\), we get:
\[
\Pr[E | \tilde{X} = x] = \frac{p}{p + t \cdot (1 - p)}
\]

Combining equations 1 and 4, we get:
\[
\Pr[\tilde{Y} = 1 | \tilde{X} = x] = \frac{p}{p + t \cdot (1 - p)} + \Pr[Y = 1 | X = x] \cdot \left(1 - \frac{p}{p + t \cdot (1 - p)}\right)
\]

This completes the proof.

**Proof of Corollary 12:**
If \(\text{crt}(x) \leq \frac{p - 2\tau \cdot t}{t(1 - p)}\), then by Claim 11:
\[
\Pr[\tilde{Y} = 1 | \tilde{X} = x] = \frac{p}{p + t \cdot (1 - p)} + \Pr[Y = 1 | X = x] \cdot \left(1 - \frac{p}{p + t \cdot (1 - p)}\right)
\]
\[
= \frac{p}{p + t \cdot (1 - p)} + \left(1 - \text{crt}(x)\right) \cdot \left(1 - \frac{p}{p + t \cdot (1 - p)}\right)
\]
\[
\geq \frac{p}{p + t \cdot (1 - p)} + \left(1 - \frac{p - 2\tau \cdot t}{t(1 - p)}\right) \cdot \left(1 - \frac{p}{p + t \cdot (1 - p)}\right)
\]
\[
= \frac{p}{p + t \cdot (1 - p)} + \frac{t(1 - p) - p + 2\tau \cdot t}{2t(1 - p)} \cdot \left(1 - \frac{p}{p + t \cdot (1 - p)}\right)
\]
\[
= \frac{p}{p + t \cdot (1 - p)} + \frac{t(1 - p) + p + 2\tau \cdot t}{2(p + t \cdot (1 - p))}
\]
\[
= \frac{1}{2} + \frac{\tau \cdot t}{p + t \cdot (1 - p)}
\]

To show the other direction, we can follow the exact same steps in reverse order.

**Proof of Claim 13:**
For all \(x \in X\) such that \(C_\tau(x) = 1\), using Corollary 12, if \(t = t_1\), then:
\[
\Pr[\tilde{Y} = 1 | \tilde{X} = x] < 0.5 - \frac{\tau \cdot t_1}{p + (1 - p) \cdot t_1}
\]

If \(t = t_0\), then:
\[
\Pr[\tilde{Y} = 1 | \tilde{X} = x] \geq 0.5 + \frac{\tau \cdot t_0}{p + (1 - p) \cdot t_0}
\]

This implies that for both cases of \(t = t_0\) and \(t = t_1\), we have:
\[
|\text{crt}(x, \tilde{D})| \geq \frac{2\tau \cdot t}{p + (1 - p) \cdot t}
\]

And it also implies that for the case of \(t = t_0\):
\[
h^*(x, \tilde{D}) = 1
\]

For \(t = t_1\):
\[
h^*(x, \tilde{D}) = 0
\]

**Lemma 14:**
For any distribution \(D \equiv (X, Y)\) where \(\text{Supp}(Y) = \{0, 1\}\) and any classifier \(h : \text{Supp}(X) \to \{0, 1\}\), the following holds:
\[
\Pr[h(X) \neq Y] \leq \Pr[h(X) \neq h^*(X)]
\]
where \(h^*\) is the Bayes optimal classifier.