[12] using our own implementation of their attack. For the last two
rows which are experiments on image datasets, we compare with the
set-based attack of [12] and we use the numbers reported in the paper.
with the label (and this is exactly what WBAttack is unable to
attack), and do it in a fully black-box way. As we see in the
columns below, with very small ratio of poisoning our attack
get accuracy 1.0 on the random target property. It also beats the
performance of WBAttack on other experiments with a very
small number of poisoning points. Note that our attack also
requires many fewer shadow models. For example with 10%
poisoning, only 50 shadow models in our attack would beat
the accuracy of WBAttack which uses 1000 shadow models.
The small number of shadow models would be important in
scenarios where the adversary does not have access to a lot
of similar data. So in summary, our attack can improve on
the performance WBAttack both in accuracy and number of
shadow models, and of course in the access model which is
black box. The cost of these improvements is allowing the
adversary to choose a fraction of training set which is not an
uncommon scenario in multi-party learning applications. To
compare the running time of the attacks, for the Census-Gender
experiment in Table VIII, WBAttack ran in 1533s compared to
our black-box attack that only took 161s on the same platform
and execution environment. Note that we used the exact same
size of held-out data for shadow models in both black-box and
white-box experiments.
Image datasets We also replicated the setting of [12] on
MNIST [16] and CelebA [18] datasets in the last two rows of
Table VIII. In the MNIST experiment,The goal of adversary
is to ﬁnd out if whether the images have jitter noise added to
them before training. The adversary uses 10% poisoning to
associate Jitter with one of the digits (9 in our experiments)
and then query the model on 200 images with high uncertainty
(We use a slightly different notion of uncertainty for multi-label
cases). We use MLP classiﬁer with three hidden layers of size
128, 32 and 16 for the MNIST classiﬁer (similar to [12]). In
the CelebA experiment, the goal of the classiﬁer is to tell if the
face in the image is smiling. The goal of attacker is to ﬁnd out
whether 40% or 60% of images in the dataset used for training
the model are male. Our attack add 10% poison examples
and then queries the model with 500 uncertain queries. The
classiﬁer uses FaceNET [26] which is a pre-trained feature
extractor to maps the images to 512 features and then a MLP
classiﬁer with 2 hidden layers of 64 and 16 is trained those
features. This setup is exactly similar to the setup of [12].
VIII. (IM)POSSIBILITY OF DEFENSES
Theoretical Analysis First, note that our theoretical analysis
suggests that, as long as the learning algorithm (which could
include a poisoning defense) preserves its generalization
properties, our attack should succeed. In other words, a
poisoning defense would either hurt the performance of the
learning algorithm or it will be unsuccessful in defending our
attack. Our second argument is about the input-space nature of
existing poisoning defenses in the literature [30]. In particular,
the poisoning defenses often use a ﬁlter function F that is
applied on the inputs (without looking at the label) and ﬁlters
outliers from the training set. Our poisoning attack uses real
samples from the distribution without changing the input. This
means that no ﬁlter function cannot defend against our attack.
The reason that poisoning defenses often do not look into the
label is that the learner supposedly does not have information
about the labels and removing examples based on their labels
create a bias that hurts the performance of the algorithm.
Differential Privacy (DP) mitigation We wanted to empiri-
cally see how effective DP can be, in mitigating our attacks.
We tested our attack on models trained with DP in Table IX.
DP could be seen as a two-fold defense against our attack.
First, DP is designed to make the dataset more private and one
could expect to see reduction of leakage when DP is applied.
Second, DP is one of the few provable defenses proposed
against poisoning attacks [17] and one could expect that our
attack would perform worse when DP is deployed because of
the poisoning step in the attack. However, our results show that
even with small values for , the attack is still successful. Even
with  values less than 1, the attack still achieves accuracy of
90%. DP will start to mitigate the attack when the value of 
goes less than 0.5. We believe this is the effect of reducing
the utility of models and is predicted by our theory.
Noise multiplier
0.6
0.7
0.8
0.9
1.0
2

4.09
2.56
1.68
1.22
0.95
0.29
Attack accuracy
Classiﬁcation Accuracy
0.95%
0.97%
0.92%
0.93%
0.90%
0.76%
92.3%
92.5%
91.6%
91.7%
91.07%
84.05%
TABLE IX: Experiments with differential privacy on Enron dataset
and random target feature. The details of this experiment could be
found in Appendix F.
IX. CONCLUSION
Poisoning attacks attacks are usually studied in machine
learning security where the goal of the adversary is to increase
the error or inject backdoors to the model. In this work, we
initiated the study of poisoning adversaries that instead seek
to the increase information leakage of trained models.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:49 UTC from IEEE Xplore.  Restrictions apply. 
131132
REFERENCES
[1]
3 ways to train a secure machine learning model.
https://www.ericsson.com/en/blog/2020/2/training-a-
machine-learning-model. Accessed:
2020-03-04.
[2] Giuseppe Ateniese et al. “Hacking smart machines with
smarter ones: How to extract meaningful data from
machine learning classiﬁers”. In: IJSN 10.3 (2015),
pp. 137–150. DOI: 10.1504/IJSN.2015.071829. URL:
https://doi.org/10.1504/IJSN.2015.071829.
[3] Eugene Bagdasaryan et al. “How to backdoor federated
learning”. In: International Conference on Artiﬁcial
Intelligence and Statistics. PMLR. 2020,
pp. 2938–2948.
[4] Raef Bassily, Adam Smith, and Abhradeep Thakurta.
“Private empirical risk minimization: Efﬁcient
algorithms and tight error bounds”. In: 2014 IEEE 55th
Annual Symposium on Foundations of Computer
Science. IEEE. 2014, pp. 464–473.
[5] Arjun Nitin Bhagoji et al. “Analyzing federated
learning through an adversarial lens”. In: International
Conference on Machine Learning. PMLR. 2019,
pp. 634–643.
[6] Battista Biggio, Blaine Nelson, and Pavel Laskov.
“Poisoning attacks against support vector machines”. In:
arXiv preprint arXiv:1206.6389 (2012).
[7] Nicholas Carlini et al. “The Secret Sharer: Evaluating
and Testing Unintended Memorization in Neural
Networks”. In: USENIX Security Symposium. 2018.
[8] Christopher A Choquette Choo et al. “Label-Only
Membership Inference Attacks”. In: arXiv preprint
arXiv:2007.14321 (2020).
[9] Cynthia Dwork et al. “Calibrating Noise to Sensitivity
in Private Data Analysis”. In: Theory of Cryptography,
Third Theory of Cryptography Conference, TCC 2006,
New York, NY, USA, March 4-7, 2006, Proceedings.
Ed. by Shai Halevi and Tal Rabin. Vol. 3876. Lecture
Notes in Computer Science. Springer, 2006,
pp. 265–284. DOI: 10.1007/11681878\_14. URL:
https://doi.org/10.1007/11681878%5C_14.
[10] Andrew Frank, Arthur Asuncion, et al. “UCI machine
learning repository, 2010”. In: URL http://archive. ics.
uci. edu/ml 15 (2011), p. 22.
[11] Karan Ganju et al. “Property Inference Attacks on
Fully Connected Neural Networks Using Permutation
Invariant Representations”. In: Proceedings of the 2018
ACM SIGSAC Conference on Computer and
Communications Security. CCS ’18. Toronto, Canada:
Association for Computing Machinery, 2018,
pp. 619–633. ISBN: 9781450356930. DOI:
10.1145/3243734.3243834. URL:
https://doi.org/10.1145/3243734.3243834.
[12] Karan Ganju et al. “Property Inference Attacks on
Fully Connected Neural Networks using Permutation
Invariant Representations”. In: CCS ’18. 2018.
[13] Yingzhe He et al. Towards Privacy and Security of
Deep Learning Systems: A Survey. 2019. arXiv:
1911.12562 [cs.CR].
[14] Matthew Jagielski. “Subpopulation Data Poisoning
Attacks”. In: 2019.
[15] Bryan Klimt and Yiming Yang. The enron corpus: A
new dataset for email classiﬁcation research. 2004.
[16] Yann LeCun. “The MNIST database of handwritten
digits”. In: http://yann. lecun. com/exdb/mnist/ ().
[17] Mathias Lecuyer et al. “Certiﬁed robustness to
adversarial examples with differential privacy”. In:
2019 IEEE Symposium on Security and Privacy (SP).
IEEE. 2019, pp. 656–672.
[18] Ziwei Liu et al. “Large-scale celebfaces attributes
(celeba) dataset”. In: Retrieved August 15.2018 (2018),
p. 11.
[19] Saeed Mahloujifar, Dimitrios I Diochnos, and
Mohammad Mahmoody. “Learning under p-Tampering
Attacks”. In: Algorithmic Learning Theory. PMLR.
2018, pp. 572–596.
[20] Saeed Mahloujifar, Dimitrios I Diochnos, and
Mohammad Mahmoody. “The curse of concentration in
robust learning: Evasion and poisoning attacks from
concentration of measure”. In: Proceedings of the AAAI
Conference on Artiﬁcial Intelligence. Vol. 33. 2019,
pp. 4536–4543.
[21] Luca Melis et al. “Exploiting Unintended Feature
Leakage in Collaborative Learning”. In: 2019 IEEE
Symposium on Security and Privacy (SP) (May 2019).
DOI: 10.1109/sp.2019.00029. URL:
http://dx.doi.org/10.1109/SP.2019.00029.
[22] Milad Nasr, Reza Shokri, and Amir Houmansadr.
“Comprehensive privacy analysis of deep learning”. In:
2019 ieee symposium on security and privacy. 2019.
[23] Milad Nasr, Reza Shokri, and Amir Houmansadr.
“Machine learning with membership privacy using
adversarial regularization”. In: Proceedings of the 2018
ACM SIGSAC Conference on Computer and
Communications Security. 2018, pp. 634–646.
[24] Privacy-preserving Collaborative Machine Learning.
https://medium.com/sap-machine-learning-
research/privacy-preserving-collaborative-machine-
learning-35236870cd43. Accessed:
2020-03-04.
[25] Alexandre Sablayrolles et al. “White-box vs Black-box:
Bayes Optimal Strategies for Membership Inference”.
In: ArXiv abs/1908.11229 (2019).
[26] Florian Schroff, Dmitry Kalenichenko, and
James Philbin. “Facenet: A uniﬁed embedding for face
recognition and clustering”. In: Proceedings of the
IEEE conference on computer vision and pattern
recognition. 2015, pp. 815–823.
[27] Ali Shafahi et al. “Poison frogs! targeted clean-label
poisoning attacks on neural networks”. In: Advances in
Neural Information Processing Systems. 2018,
pp. 6103–6113.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:49 UTC from IEEE Xplore.  Restrictions apply. 
141133
[28] Reza Shokri et al. “Membership inference attacks
against machine learning models”. In: 2017 IEEE
Symposium on Security and Privacy (SP). IEEE. 2017,
pp. 3–18.
[29] Liwei Song and Prateek Mittal. “Systematic Evaluation
[30]
of Privacy Risks of Machine Learning Models”. In:
arXiv preprint arXiv:2003.10595 (2020).
Jacob Steinhardt, Pang Wei W Koh, and Percy S Liang.
“Certiﬁed defenses for data poisoning attacks”. In:
Advances in neural information processing systems.
2017, pp. 3517–3529.
[31] Fnu Suya et al. “Model-Targeted Poisoning Attacks:
Provable Convergence and Certiﬁed Bounds”. In: arXiv
preprint arXiv:2006.16469 (2020).
[32] Binghui Wang and Neil Zhenqiang Gong. “Stealing
hyperparameters in machine learning”. In: 2018 IEEE
Symposium on Security and Privacy (SP). IEEE. 2018,
pp. 36–52.
APPENDIX A
OMITTED PROOFS
In this section we prove the tools we used to prove our main
Theorem. Speciﬁcally, We prove Claim 11, Corollary 12 and
Claim 13.
Proof of Claim 11. We have
Pr[ ˜Y = 1 | ˜X = x]
= Pr[ ˜Y = 1 | ˜X = x ∧ E] · Pr[E | ˜X = x]
+ Pr[ ˜Y = 1 | ˜X = x ∧ ¯E] · Pr[ ¯E | ˜X = x]
= Pr[E | ˜X = x] + Pr[Y = 1 | X = x] · Pr[ ¯E | ˜X = x]
(1)
Now we should calculate the probability Pr[E | ˜X = x] to
get the exact probabilities. We have
Pr[E | ˜X = x]
Pr[ ˜X = x | E] · Pr[E]
=
Pr[ ˜X = x | E] · Pr[E] + Pr[ ˜X = x | ¯E] · Pr[ ¯E]
Pr[X+ = x] · p
=
Pr[X+ = x] · p + Pr[X = x] · (1 − p)
Now for all x ∈ X such that f (x) = 1 we have
Pr[X = x] = t · Pr[X+ = x].
and for all x ∈ X such that f (x) = 0 we have
Now combining Equations 2 and 3, for all x ∈ X such that
f (x) = 1 we have
(2)
(3)
Pr[E | ˜X = x] =
p
p + t · (1 − p)
.
(4)
Combining equations 1 and 4 we get
which ﬁnishes the proof.
Proof of Corollary 12. if crt(x) ≤ p−2τ·t
we have
t(1−p) then by Claim 11
t(1 − p)
p + t(1 − p)
t(1 − p)
p + t(1 − p)
t(1 − p)
p + t(1 − p)
· Pr[Y = 1 | X = x]
· (
· (
1 − crt(x)
)
2
t(1 − p) − p + 2tτ
2t(1 − p)
)
Pr[ ˜Y = 1 | ˜X = x]
=
=
≥
=
=
p
p + t(1 − p)
p
p + t(1 − p)
+
+
p
+
p + t(1 − p)
t(1 − p) + p + 2τ t
2(p + t(1 − p))
p + t(1 − p)
1
2
τ t
+
.
To show the other direction, we can follow the exact same
steps in the opposite order.
Proof of Claim 13. For all x ∈ X such that Cτ (x) = 1,
using Corollary 12, if t = t1 then we have
Pr[ ˜Y = 1 | ˜X = x] < 0.5 −
and if t = t0 then
Pr[ ˜Y = 1 | ˜X = x] ≥ 0.5 +
τ · t1
p + (1 − p) · t1
τ · t0
p + (1 − p) · t0
(5)
(6)
This implies that for both cases of t = t0 and t = t1 we have
|crt(x, ˜D)| ≥
2τ t
p + (1 − p) · t
And it also implies that for case of t = t0 we have
and for t = t1 we have
h∗(x, ˜D) = 1
h∗(x, ˜D) = 0.
(7)
(8)
(9)
Now we state a lemma that will be used in the rest of the
proof:
Lemma 14. For any distribution D ≡ (X, Y ) where
Supp(Y ) = {0, 1} and any classiﬁer h : Supp(X) → {0, 1}