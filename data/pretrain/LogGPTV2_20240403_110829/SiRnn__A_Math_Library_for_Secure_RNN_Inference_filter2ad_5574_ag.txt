ULP
Error
Exponentiation
0.03
0.34
(11.3x)
0.13
0.94
(7.2x)
1.00
35.95
(35.9x)
0.15
3.90
(26x)
0.04
0.56
(14x)
Reciprocal Square Root
0.13
3.90
(30x)
0.30
35.87
(120x)
1.84
338.9
(184x)
2.12
254.95
(120x)
6
2535
(423x)
3
2
4
8
TABLE II: Comparison with (power-of-2) ring-based MP-
SPDZ protocols with varying number of instances.
ﬂoating point representations. Field-based protocols perform
poorly for non-linear computations like truncation and com-
parisons, which are abundant in ﬁxed-point representations
of DNNs [64], [90], [99]. Similarly, it is well-known that
the protocols over ﬂoating-point are much slower than ﬁxed-
point [29], [73]. Nonetheless, for completeness, we compare
against the state-of-the-art ﬁeld-based implementations in MP-
SPDZ [3], [66] and they perform worse (7th and 8th rows of
Table I). We also compare with ﬂoating-point implementations
of math functions provided by ABY [40] and EMP-Toolkit [1];
our protocols are at least 90× better in communication per
instance and 97× better in runtime (for 105 instances).
Finally, SecureML [92] and ABY2.0 [95] use a 3-piece lin-
ear spline to approximate sigmoid. This simple implementation
has a whopping error of 1547 ULPs and tanks the accuracy of
our RNN benchmarks. For instance, it leads to a tremendous
drop in accuracy of the Google-30 network from 84.4% (with
our sigmoid implementation) to 60.95%. The insufﬁciency
of this approximation has also been noted by [83] where it
caused the cross-entropy loss to diverge to inﬁnity. Hence, this
crude approximation is usable only in restricted contexts and
is unsuitable for generic math libraries, which is our aim here.
b) Exponential and reciprocal square-root: Table II
shows the comparison of our exponentiation and reciprocal
square-root protocols with power-of-2 ring based protocols in
MP-SPDZ framework (for scale 12). It has native support for
exponentiation. We implement reciprocal square root in MP-
SPDZ by calling its built-in functions for square root and
reciprocal. As the table shows, our protocols are orders of
magnitude better, both in terms of time-taken and communi-
cation, and provide better or comparable ULP errors.
B. Prior DNNs
In Table III, we evaluate our protocols on benchmarks with
math functions from MiniONN [83] and DeepSecure [102].
MiniONN evaluated an LSTM for text data which has 2
LSTM layers each with 800 instances of sigmoid and 200
instances of tanh. Our protocols incur an order of magnitude
less communication for these instances. We consider the
largest benchmark of DeepSecure, B4, with 2 tanh layers of
2000 and 500 instances, which classiﬁes sensor data into 19
different physical activities. To estimate the time taken by
Inference
Benchmark
MiniONN LSTM
DeepSecure B4
Our Work
Runtime (in sec)
Prior
1.1
(2.2x)
465
(87x)
0.48
5.3
Comm.
Prior
Our Work
182 MB
(19.5x)
83.7 GB
(43x)
9.32 MB
1.94 GB
TABLE III: Comparison with benchmarks
iONN [83] and DeepSecure [102].
from Min-
Benchmark
Batch
Industrial-72
Google-30
1
128
1
128
SIRNN
3.7
Runtime (sec)
[41]
68.33
(18x)
8746∗
(661x)
3337
(67x)
4.3x105∗
(3050x)
13.2
49.6
140
Comm.
[41]
SIRNN
11.84 GB
(510x)
1.47 TB∗
(1451x)
259 GB
(574x)
32.38 TB∗
(1316x)
23.8 MB
1.04 GB
0.45 GB
25.2 GB
1
Heads
85.5 GB
*extrapolated, the run could not be completed due to TB comm.
TABLE IV: Secure inference on DNNs using SIRNN and [41].
409.7
NA
NA
DeepSecure on our setup, we ran a circuit with the same non-
XOR complexity as B4 using EMP-Toolkit [1] (similar to our
microbenchmarks) that provides better performance than the
communication and latency in [102]. Our protocols have 87×
lower latency and 43× lower communication.
C. Case studies
We demonstrate the applicability of secure inference to three
new domains that no prior work has considered before: RNNs
applied to time series sensor data, RNNs applied to speech
data, and combining CNNs and RNNs to identify human heads
in images. The feasibility of our case studies crucially relies on
our efﬁcient protocols for math functions. Our ﬁrst case study
is an industrial model (Industrial [72]) which uses an RNN
with GRU cells to provide feedback on the quality of shots
in a bat-and-ball game from the data obtained from sensors
deployed on the bat. Second, we evaluate an RNN (Google-
30 [74]) for keyword spotting in the standard Google-30 [112]
dataset that identiﬁes simple commands, digits, and directions
from speech data obtained from thousands of people. Third,
the head detection model (Heads [104]) combines CNNs and
RNNs for the best accuracy on the SCUT Head dataset [96].
It uses inverted residual blocks, or MBConv blocks [105],
for efﬁcient convolutions. Instead of simple pooling operators
like maxpool or average pool, it uses RNN-based pooling
that provides high accuracy. We summarize the input ﬁxed-
point code of these benchmarks below. These ﬁxed-point C++
programs were automatically generated from high-level ML
models by [72] (a compiler for embedded devices) and linked
with SIRNN. All of the benchmarks use a mixture of variables
with bitwidth 8, 16, and 32 with 16 being the bitwidth used
for input and output of the math functions.
• Industrial-72: It contains 7 sigmoid and 7 tanh layers,
with 64 instances each. While sigmoid uses the input
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:28:57 UTC from IEEE Xplore.  Restrictions apply. 
1014
scale 8 and output scale 14, for tanh both scales are 8.
• Google-30: It contains 99 sigmoid and 99 tanh layers,
with 100 instances each. While sigmoid uses the input
scale 6 and output scale 14, for tanh both scales are 6.
• Heads: It contains 128 sigmoid and 128 tanh layers, with
18096 instances each. While sigmoid uses the input scale
11 and output scale 14, for tanh both scales are 11.
Additionally, the benchmark contains 8 sigmoid and 8
tanh layers, with 72384 instances each. For these layers,
sigmoid uses the input scale 13 and output scale 14, and
for tanh both scales are 13. Finally, it also contains 3 L2-
Normalise layers that have 1200, 1200 and 300 reciprocal
square-root operations. The layers have input scales 12,
10 and 12 and output scales 11, 9 and 11, respectively.
Note that the Heads model makes about 3 million calls to
sigmoid/tanh, which is three orders of magnitude larger than
the number of calls to these functions in the benchmarks used
by prior work (Section VI-B).
In Table IV, we present the latency and communication
required by SIRNN on above benchmarks. Using our protocols,
Industrial takes 4 seconds, Google-30 takes under a minute,
and Heads takes less than 7 minutes. The time per inference
can be further improved by batching multiple predictions. For
a batch size of 128, the amortized time per inference of Indus-
trial is 0.1s and of Google-30 is 1.1s! The savings in batching
come from amortizing the networking cost by packing data
from multiple inference queries. Owing to the high numerical
precision of our math functionalities (Section V-D), SIRNN
either matches or exceeds the model accuracy of the provided
ﬁxed-point ML model. In Heads, about half the time is spent
in math operations and the rest of the time is spent in matrix
multiplications, convolutions, and Hadamard products. The
good performance on end-to-end benchmarks is a result of co-
designing precise math functionalities and efﬁcient protocols.
Next, we perform an ablation study. In particular, the ﬁxed-
point code with our math functionalities can be run with
other protocols. However, prior work on secure inference don’t
support juggling between different bitwidths that our math
functionalities require. Hence, for running these functionalities
with any prior protocol, we need to use an appropriately
large uniform bitwidth. We evaluate our benchmarks with
ABY [41] using the necessary bitwidth of 64 as a baseline
in Table IV. ABY [41] provides general purpose state-of-
the-art 2PC protocols that have been used by recent work
on secure inference [25], [32], [83], [92]. We have added a
new code generator to [72] that generates EZPC [32] code
which is then automatically translated to ABY code. Other
generic protocols that have suitable frontends [60], [82], [88],
[109], like garbled circuits, are several orders of magnitude
slower than ABY [32], [92]: ML inference involves many
multiplications that are very expensive with garbled circuits.
SIRNN is over 500× better than ABY in communication and
more than an order of magnitude faster in runtime. Without
our protocols, it takes almost an hour to run Google-30. This
situation is further exacerbated on bigger models and running
the Heads model with ABY is intractable because it requires
hundreds of terabytes of communication. With batching, the
performance differences are stark: SIRNN is three orders of
magnitude better in latency and communication compared to
the ABY baseline.
VII. OTHER RELATED WORK
Prior 2PC works that use high degree polynomials for
approximating math functions [9], [34], [57], [68] need degree
7 or higher to maintain accuracy. In the course of this
work, we have observed that evaluating polynomials with
degree 3 or higher with 2PC is much more expensive than
the LUT-based implementations of Section V. Some prior
works on secure inference implement math functions with
ad hoc approximations that can lose model accuracy: e.g.
SecureML [92] and ABY2.0 [95] use a crude 3-piece linear
approximation, Ball et al. [13] replace tanh with the signum
function, and Glyph [85] and Nandakumar et al. [93] use
tables of approximate results Most recent works on secure
inference limit their evaluation to benchmarks that don’t use
math functions [17], [39], [44], [47], [64], [90], [99]. Prior
2PC works that use ﬂoating-point representations (instead of
ﬁxed-point representations) have much higher performance
overheads [1], [6], [7], [12], [33], [40], [46], [66], [84]
Other relevant works that need additional parties to en-
sure security such as 3PC with honest majority or 2PC
with trusted dealer include [8], [9], [28]–[31], [43], [86],
Chameleon [101], CrypTen [69], TF-Encrypted [2], CrypT-
Flow [73], PySyft [103], ABY3 [91], SecureQ8 [37], and
Sharemind [65], [67], [71], [75], [97]. Some of these works
have considered approximations to math functions and, similar
to 2PC works, they either use polynomial-based approxima-
tions (e.g. [9], [71], [86]) or work over ﬂoating-point (e.g. [8],
[29], [30], [65], [67], [75], [97]). Kerik et al. [67] also consider
building blocks such as extension, truncate-and-reduce, and
multiplication of non-uniform bitwidths in the 3PC context.
In terms of representations, while ﬂoating-point and ﬁxed-
point representations are most common, [43] proposed the new
representations of golden-section and logarithmic numbers and
evaluated using 3PC protocols.
Recent works on silent-OT [22], [114] provide OT exten-
sions with much lower communication than IKNP-style exten-
sions [62], at the cost of higher computational overhead. Since
our protocols make use of OTs in a black-box manner, silent-