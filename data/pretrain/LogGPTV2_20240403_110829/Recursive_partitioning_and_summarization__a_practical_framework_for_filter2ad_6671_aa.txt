title:Recursive partitioning and summarization: a practical framework for
differentially private data publishing
author:Wahbeh H. Qardaji and
Ninghui Li
Recursive Partitioning and Summarization: A Practical
Framework for Differentially Private Data Publishing
Wahbeh Qardaji, Ninghui Li
Purdue University
305 N. University Street,
West Lafayette, IN 47907, USA
{wqardaji, ninghui}@cs.purdue.edu
ABSTRACT
In this paper we investigate Recursive Partitioning and Sum-
marization (RPS), a practical framework for data publishing
that satisﬁes diﬀerential privacy. While there have been sev-
eral negative results concerning non-interactive diﬀerentially
private data release, we show that such results do not neces-
sarily mean that such release is impossible. To that end, we
propose a data release framework that leverages current ad-
vances in diﬀerentially private query answering to synthesize
an anonymized dataset. We show that since each query only
aﬀects a sub linear number of tuples, we are able to guaran-
tee diﬀerential privacy. To evaluate the eﬃcacy and general
applicability of our approach, we experimentally evaluate
the utility of our framework in three domains and several
real and synthetic datasets. All our results indicate the ap-
plicability of our framework to general data release.
Categories and Subject Descriptors
H.2.8
Administration-Security,
[COMPUTERS AND SOCIETY]: Privacy
[DATABASE MANAGEMENT]: Database
integrity, and protection; K.4.1
General Terms
Security, Algorithms
Keywords
Diﬀerential Privacy, Anonymization, Data Privacy
1.
INTRODUCTION
In this paper we consider the problem of diﬀerentially pri-
vate data publishing. In particular, we consider the scenario
in which a trusted curator gathers sensitive information from
a large number of respondents, creates a relational dataset
where each tuple corresponds to one entity, such as an in-
dividual, a household, or an organization, and then pub-
lishes a privacy-preserving (i.e., sanitized or anonymized )
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ASIACCS ’12, May 2–4, 2012, Seoul, Korea.
Copyright 2012 ACM 978-1-4503-1303-2/12/05 ...$10.00.
version of the dataset. This has been referred to as the
“non-interactive” mode of private data analysis, as opposed
to the “interactive” mode, where the data curator provides
an interface through which users may pose queries about the
data, and get (possibly noisy) answers.
Publishing microdata is gradually becoming more and
more common. The Census bureau has a mandate to publish
census data. In addition, medical researchers argue for the
need to support clinical research in electronic health records
systems. Furthermore, many organizations have the need
to publish transactional data for research and other pur-
poses. Microdata publishing, however, has also resulted in
well-publicized privacy breach incidents. Examples include
the identiﬁcation of the medical record of the governor of
Massachusetts from the GIC data [29]; the identiﬁcation of
the search history of an AOL user from the AOL query log
data [3]; the identiﬁcation of Netﬂix subscribers from the
Netﬂix Prize dataset [25]; and the de-anonyimzation of so-
cial networks [26].
We aim at developing practical techniques that can pub-
lish datasets in a way that satisﬁes the strong privacy guar-
antee of diﬀerential privacy [7, 9]. We observe that there are
two main approaches to private data analysis. For lack of
better names, we call them the experimental approach and
the theoretical approach. Our approach bridges the tech-
niques developed in both approaches.
The experimental approach has been the dominant ap-
proach in the database community. This approach focuses
mostly on the non-interactive mode, as this is the primary
way data is shared in practice. The emphasis is on de-
veloping algorithms and tools that can be applied to large
datasets. For privacy, many earlier approaches use syntactic
privacy notions such as k-anonymity [29], ℓ-diversity [22], t-
closeness [21], and so on. Recently, it has been increasingly
recognized that these syntactic privacy notions do not pro-
vide suﬃcient privacy protection, and diﬀerential privacy is
increasingly being adopted, though so far only to very spe-
cialized kinds of datasets, such as to publishing frequencies
of web search items [19, 15], or node degree sequences of
graphs [16], one-dimensional histograms [17, 31]. For util-
ity, the primary method is experimental evaluation. One ex-
perimentally measures to what degree the dataset has been
changed and/or to what degree the accuracy of certain data
mining tasks is aﬀected.
The theoretical approach has been the dominant approach
in the theory/cryptography community. This approach fo-
cuses on the interactive mode. The privacy notion developed
here is diﬀerential privacy [7, 9]. The emphasis has been on
proving theorems that identify the boundaries of privacy
and utility, and developing methods that can answer par-
ticular kinds of queries while satisfying diﬀerential privacy.
For utility, one chooses some classes of queries and analyzes
how accurately these queries can be answered.
We combine the concepts and techniques from both the
experimental approach and the theoretical approach. For
privacy, we adopt the notion of diﬀerential privacy. For util-
ity, we use experimental methods, applying our algorithm on
diﬀerent kinds of datasets and measuring the utility of the
resultant datasets. While our goal is to publish microdata
in the non-interactive setting, we employ techniques for dif-
ferentially private query answering developed for the inter-
active mode. Our approach generates a sequence of queries
on the dataset and uses the query results to reconstruct a
sanitized version of the dataset. This is possible despite the
negative results in [6, 13], which show that answering a lin-
ear (in the database size) number of queries relatively accu-
rately leads to disclosure of the dataset in some cases. One
key insight resolving the apparent contradiction is that while
a linear number of queries is required in our approach, a sin-
gle tuple in the dataset aﬀects only a sub-linear, O(log N ),
number of the queries, making satisfying diﬀerential privacy
while obtaining reasonably accurate answers possible.
More speciﬁcally, we investigate a framework that we call
Recursive Partitioning and Summarization (RPS), which ex-
ploits the above insight. In the RPS framework, we view tu-
ples as points in a multi-dimensional space. Given a dataset
and the multi-dimensional region that tuples in the dataset
are in, an RPS algorithm recursively partitions the region
into smaller regions, then summarizes each region indepen-
dently, and ﬁnally combines the summarized outputs. To
instantiate this framework into a concrete algorithm, one
speciﬁes three sub-routines: how to partition a region, how
to determine when to stop further partitioning, and how to
summarize data items in a region once partitioning stops.
An RPS algorithm satisﬁes diﬀerential privacy when each
of the three sub-routines satisfy diﬀerential privacy, as each
node aﬀects only the query regarding the partitions that the
node is in, it aﬀects O(log N ) queries, where N is the num-
ber of tuples in the dataset. If each sub-routine is performed
in time linear in the number of tuples in the region, then the
algorithm runs in time O(N log N ).
While the idea of partitioning and summarization has
been used implicitly or explicitly before, a key challenge is
how to instantiate it. We make a key observation to make
the RPS framework feasible for multi-dimensional relational
data is that the exponential mechanism for diﬀerential pri-
vacy [24] provides an eﬀective solution to the critical parti-
tioning step, yielding a close-to-balanced partition in a dif-
ferentially private way.
1.1 Our Contributions
• We investigate the Recursive Partitioning and Sum-
marization framework and identify using the exponen-
tial mechanism to achieve balanced partitioning as a
key enabling step for this framework. RPS instanti-
ated with balanced partitioning provides a practical
and general method to diﬀerentially privately pub-
lish multi-dimensional relational datasets. We point
out that many data sharing scenarios involve such
datasets, including census data, medical records, etc.
• We experimentally evaluate the eﬀectiveness of our ap-
proach using both synthetic datasets and the Adult
dataset [1]. Our results on the synthetic datasets show
that the sanitized datasets preserve useful features of
the input datasets, and the performance depends on
choices of algorithmic parameters such as maximum
depth and stopping conditions. Our result on the
Adult dataset show that the sanitized datasets enable
accurate classiﬁcation. In particular, our method out-
performs the Mondrian algorithm [20], which satisﬁes
the weaker k-anonymity.
• Finally, to demonstrate the versatility of the RPS ap-
proach, we also apply the RPS algorithm to sanitize
the node degree sequence of social networks. Such
sanitized sequence can then be used to generate a new
graph with properties similar to the original graph.
This problem is challenging because removing one
node may aﬀect the degrees of many other nodes. We
show that the RPS approach can be instantiated in a
way that satisﬁes diﬀerential privacy relative to adding
or removing a node. It provides a practical way for pri-
vately releasing node-degree sequences for graph data,
outperforming the current state of the art.
The rest of this paper is organized as follows. We discuss
background on diﬀerential privacy in Section 2. We present
the RPS framework in Section 3, and the evaluation results
in Sections 4 and 5. We discuss related work in Section 6
and conclude with Section 7.
2. BACKGROUND ON DIFFERENTIAL
PRIVACY
We ﬁrst brieﬂy review the notion of diﬀerential privacy,
which was developed in a series of papers [6, 11, 4, 9, 7].
The intuition behind this privacy notion is as follows:
if
a disclosure occurs when an individual participates in the
database, then the same disclosure also occurs with similar
probability (within a small multiplicative factor) even when
the individual does not participate.
Definition 1
(ǫ-Differential Privacy [7, 9]).
A randomized algorithm A gives ǫ-diﬀerential privacy if
for any pair of neighboring datasets D and D′, and any
S ∈ Range(A),
Pr[A(D) = S] ≤ eǫ Pr[A(D′) = S]
(1)
Diﬀerential privacy has the following composition prop-
erty: if two algorithms satisfy diﬀerential privacy for ǫ1 and
ǫ2, then releasing the results of both algorithms satisfy dif-
ferential privacy for ǫ1 + ǫ2. The parameter ǫ measures the
degree of privacy. The larger ǫ is, the lower the privacy.
When ǫ is too large, the privacy guarantee becomes mean-
ingless. Hence one needs to have a limit for ǫ, which is know
as a privacy budget. Answering each query consumes a por-
tion of privacy budget. When the privacy budget is used up
after answering a number of queries, no new queries can be
answered.
The Feasibility of Non-interactive Diﬀerential Pri-
vacy There have been a series of negative results concern-
ing diﬀerential privacy in the non-interactive mode [6, 13, 9,
10], and these results have been interpreted “to mean that
one cannot answer a linear, in the database size, number
of queries with small noise while preserving privacy” and
motivates “an interactive approach to private data analysis
where the number of queries is limited to be small — sub-
linear in the size n of the dataset” [10]. This impossibility
result views the database as a vector, where the order of
items is important. Furthermore, it depends on answering
a linear number of queries in the database size.
These negative results not withstanding, it is shown in
several recent papers [5, 10, 12] that it is possible to pri-
vately publish a dataset that is capable of answering queries
in certain classes relatively accurately. One key insight to
account for the this apparent contradiction is the observa-
tion that it may still be possible to answer linear number of
queries without compromising privacy, provided that each
tuple aﬀects answer only to a sub-linear number of them.
Satisfying Diﬀerential Privacy There are three methods
that can be used to satisfy diﬀerential privacy: the common
Laplacian Mechanism to add noise, the Smooth Sensitivity
method of adding noise. The third alternative is the Expo-
nential Mechanism, which we describe below.
McSherry and Talwar [24] proposed an alternative to the
approach of adding noises to the query result. This method
is based on the idea that any anonymization method maps,
possibly randomly, a set of n inputs each from a domain D
to some output in range R. The mechanism relies on an
input query function q : Dn × R → R that assigns a real
valued score to any pair (d, r) from Dn × R. This can be
viewed as a quality function that assigns higher scores to
more desirable outputs.
The goal of the mechanism is to take d ∈ Dn and return
r ∈ R such that q(d, r) is maximized while guaranteeing
diﬀerential privacy.
Definition 2. For any quality function q : (Dn × R) →
R, and a privacy parameter ǫ, an exponential mechanism
Mǫ
q : Dn → R is a randomized mechanism which, given an
input d ∈ Dn returns a valid output r ∈ R with probability
proportional to
exp (ǫq(d, r))
We deﬁne ∆q to be the largest possible diﬀerence in the
quality function when applied to two neighboring datasets.
We can thus claim that the exponential privacy mechanism,
Mǫ
q, with quality function q, gives (2ǫ∆q)-diﬀerential pri-
vacy [24].
3. RECURSIVE
PARTITIONING AND
SUMMARIZATION
Many privacy-preserving data publishing mechanisms can
be modeled as instantiating the following meta-algorithm:
Initially, one views tuples as points in a multi-dimensional
space. The meta-algorithm ﬁrst partitions the space into
smaller regions, and then returns information about each re-
gion, which typically includes the number of tuples in each
partition. This can then be used to generate sanitized or
synthesized dataset. In our view, there is no fundamental
diﬀerence between sanitized or synthesized, as both are gen-
erated from the original dataset.
To make this process satisfy diﬀerential privacy, one needs
to make both steps (partitioning and summarization) satisfy
DP. The summarization step can be answered easily using
a diﬀerentially private count query. The basic approach is
to add Laplace noise to the count of each region. Multiple
improvements exist to increase the accuracy of the answer.
This includes wavelet transforms [31] and hierarchical con-
strained inference [17].
A natural way to make the partitioning process easier to
analyze with regard to diﬀerential privacy is to perform re-
cursive binary partitioning. That is, each region is ﬁrst par-
titioned into two sub-regions, and then these sub-regions are
further partitioned. One needs only to ensure that each bi-
nary partition decision satisﬁes diﬀerential privacy. There
are two general methods of performing this step with accept-
able accuracy: ﬁxed partitioning and balanced partitioning.
Fixed Partitioning vs. Balanced Partitioning Fixed
partitioning works by deterministically choosing a ﬁxed
pivot on which to partition the region. This is indepen-
dent of the data, and therefore intuitively private and does
not consume an diﬀerential privacy budget. A prominent
example is the universal histogram method in [17]. This ap-
proach works by dividing the data domain into equal sized
bins. One would then ask for interval counts at diﬀerent lev-
els of granularity. Conceptually, one can arrange all queried
intervals into a tree, where the unit-length intervals are the
leaves. Each node in the tree corresponds to an interval, and
each node has at least 2 children, corresponding to equally
sized subintervals.
Fixed partitioning has two problems. As a result of ﬁxed
partitioning, one would get regions of equal size, but con-
ceivably varying densities. Hence, one problem is that high
density regions would not get suﬃcient division. Thus,
when partitioning to arbitrary data granularity is impos-
sible, ﬁxed partitioning can provide no accuracy guarantee
for all queries.
The problem with deep partitioning, however, is that it
would result in many empty regions. Noise will be added to
such regions, potentially creating a lot of noisy data. This
problem has been observed by several methods in the lit-
erature including the Wavelet method [31] for contingency
table release. This method works by partitioning to data
granularity (i.e. where each region contains identical data
points). While this method indeed improves query accuracy
by using wavelet transforms on attributes, it suﬀers from
degraded query accuracy when the data is sparse. When
each region contains zero, or very few, data points, the rela-
tive noise due to summarization is very high; therefore, the
quality of the data decreases. While [31] provides a method
to remedy the eﬀect of such noise, this method leverages the
assumption that adjacent regions have similar counts. This
assumption, however, fails to hold as data dimensionality
increases and when the nature of the data changes.
One the other hand, balanced, or even-region, partition-
ing, works by partitioning the region evenly, thus creating
equal density partitions. One way to accomplish this is to
choose a median datapoint as a pivot. This circumvents the
disadvantages of ﬁxed region partitioning at the expense of
consuming some privacy budget. We, therefore, examine a
diﬀerentially private recursive balanced partitioning frame-
work and analyze diﬀerent methods instantiating it.
3.1 Recursive Partitioning and Summariza-
tion Meta-Algorithm
The meta-algorithm for Recursive Partitioning and Sum-
marization (RPS) is given in Algorithm 1. Given a dataset
D and the multi-dimensional region R0 which includes the
tuples in D, the RPS meta-algorithm recursively partitions
the region R0 into smaller regions, and then ﬁnally summa-
rizes each region independently and combines the summa-
rized outputs.