pareto :0.03225
data
20
30
40
50
60
70
80
90
100
Length of ON−Periods (in microseconds)
wbl: 0.090269
logn: 0.081732
exp: 0.11594
pareto: 0.66908
data
3
4
5
6
7
8
Length of OFF−Periods(in milliseconds)
9
10
4
x 10
In the two previous sections, we examined the applications em-
ployed in each of the 10 data centers, their placement, and trans-
mission patterns.
In this section, we examine, with the goal of
informing data center trafﬁc engineering techniques, how existing
data center applications utilize the interconnect. In particular, we
aim to answer the following questions: (1) To what extent does
the current application trafﬁc utilize the data center’s interconnect?
For example, is most trafﬁc conﬁned to within a rack or not? (2)
What is the utilization of links at different layers in a data center?
(3) How often are links heavily utilized and what are the proper-
ties of heavily utilized links? For example, how long does heavy
utilization persist on these links, and do the highly utilized links
experience losses? (4) To what extent do link utilizations vary over
time?
6.1 Flow of Trafﬁc
We start by examining the relative proportion of trafﬁc generated
by the servers that stays within a rack (Intra-Rack trafﬁc) versus
trafﬁc that leaves its rack for either other racks or external des-
tinations (Extra-Rack trafﬁc). Extra-Rack trafﬁc can be directly
measured, as it is the amount of trafﬁc on the uplinks of the edge
switches (i.e., the “Top-of-Rack” switches). We compute Intra-
Rack trafﬁc as the difference between the volume of trafﬁc gen-
erated by the servers attached to each edge switch and the trafﬁc
exiting edge switches.
In Figure 8, we present a bar graph of the ratio of Extra-Rack to
Intra-Rack trafﬁc in the 10 data centers we studied. We note that
a predominant portion of server-generated trafﬁc in the cloud data
centers CLD1–5—nearly, 75% on average—is conﬁned to within
the rack in which it was generated.
Recall from Section 4 that only two of these 5 data centers,
CLD4 and CLD5, run MapReduce style applications, while the
other three run a mixture of different customer-facing Web services.
Despite this key difference in usage, we observe surprisingly little
difference in the relative proportions of Intra-Rack and Extra-Rack
trafﬁc. This can be explained by revisiting the nature of applica-
tions in these data centers: as stated in Section 4, the services run-
ning in CLD1–3 have dependencies spread across many servers in
the data center. The administrators of these networks try to colo-
cate applications and dependent components into the same racks to
avoid sharing a rack with other applications/services. Low Extra-
Rack trafﬁc is a side-effect of this artifact. In the case of CLD4 and
CLD5, the operators assign MapReduce jobs to co-located servers
for similar reasons. However, fault tolerance requires placing re-
dundant components of the application and data storage into dif-
ferent racks, which increases the Extra-Rack communication. Our
ﬁndings of high Intra-Rack trafﬁc within data centers supports ob-
servations made by others [19], where the focus was on cloud data
centers running MapReduce.
Figure 7: CDF of the distribution of the arrival times of packets
at 3 of the switches in PRV2. The ﬁgure contains best ﬁt curve
for lognormal, Weibull, Pareto, and Exponential distributions,
as well as the least mean errors for each.
Finally, we compare the observed distributions for HTTP applica-
tions in the data center against HTTP applications in the wide area
and ﬁnd that the distribution of ON periods in the data center does
match observations made by others [7] in the WAN.
The take aways from our observations are that: (1) The num-
ber of active ﬂows at a switch in any given second is, at most,
10,000 ﬂows. However, new ﬂows can arrive within rapid suc-
cession (10µs) of each other, resulting in high instantaneous arrival
rates; (2) Most ﬂows in the data centers we examined are small in
size (≤ 10KB) and a signiﬁcant fraction last under a few hun-
dreds of milliseconds; (3) Trafﬁc leaving the edge switches in a
274c
i
f
f
a
r
T
f
o
t
n
e
c
r
e
P
0
0
1
0
8
0
6
0
4
0
2
0
1
U
D
E
2
U
D
E
3
U
D
E
1
V
R
P
2
V
R
P
1
D
L
C
2
D
L
C
3
D
L
C
4
D
L
C
5
D
L
C
Data Centers
Intra-Rack
Extra-Rack
Figure 8: The ratio of Extra-Rack to Intra-Rack trafﬁc in the
data centers.
F
D
C
Next, we focus on the enterprise and university data centers.
With the exception of EDU1, these appear to be both very diffe-
rent from the cloud data centers and qualitatively similar to each
other: at least 50% of the server-originated trafﬁc in the data cen-
ters leaves the racks, compared with under 25% for the cloud data
centers. These data centers run user-facing applications, such as
Web services and ﬁle servers. While this application mix is simi-
lar to CLD1–3 discussed above, the Intra/Extra rack usage patterns
are quite different. A possible reason for the difference is that the
placement of dependent services in enterprise and campus data cen-
ters may not be as optimized as the cloud data centers.
6.2 Link Utilizations vs Layer
Next, we examine the impact of the Extra-Rack trafﬁc on the
links within the interconnect of the various data centers. We ex-
amine link utilization as a function of location in the data center
topology. Recall that all 10 data centers employed 2-Tiered or 3-
Tiered tree-like networks.
In performing this study, we studied several hundred 5-minute
intervals at random for each data center and examined the link uti-
lizations as reported by SNMP. In Figure 9, we present the utiliza-
tion for links across different layers in the data centers for one such
representative interval.
In general, we ﬁnd that utilizations within the core/aggregation
layers are higher than those at the edge; this observation holds
across all classes of data centers. These ﬁndings support observa-
tions made by others [3], where the focus was on cloud data centers.
A key point to note, not raised by prior work [3], is that across
the various data centers, there are differences in the tail of the dis-
tributions for all layers–in some data centers, such as CLD4, there
is a greater prevalence of high utilization links (i.e., utilization 70%
or greater) especially in the core layer, while in others there are no
high utilization links in any layer (e.g., EDU1). Next, we examine
these high utilization links in greater depth.
6.3 Hot-spot Links
In this section, we study the hot-spot links—those with 70%
or higher utilization—unearthed in various data centers, focusing
on the persistence and prevalence of hot-spots. More speciﬁcally,
we aim to answer the following questions: (1) Do some links fre-
quently appear as hot-spots? How does this result vary across lay-
ers and data centers? (2) How does the set of hot-spot links in
a layer change over time? (3) Do hot-spot links experience high
packet loss?
 1
 0.8
 0.6
 0.4
 0.2
F
D
C
 0
 0.01
(a)
EDU1
EDU3
PRV1
PRV2
CLD1
CLD2
CLD3
CLD4
CLD5
 0.1
 1
 10
 100
Edge Link Utilization
 1
 0.8
 0.6
 0.4
 0.2
PRV2
CLD1
CLD2
CLD3
CLD4
CLD5
 0
 0.01
(b)
 1
 0.8
 0.6
 0.4
 0.2
F
D
C
 0
 0.01
(c)
 0.1
 1
 10
 100
Agg Link Utilization
EDU1
EDU3
PRV1
PRV2
CLD1
CLD2
CLD3
CLD4
CLD5
 0.1
 1
 10
 100
Core Link Utilization
Figure 9: CDF of link utilizations (percentage) in each layer.
6.3.1 Persistence and Prevalence
In Figure 10, we present the distribution of the percentage of
time intervals that a link is a hot-spot. We note from Figures 10(a)
and (b) that very few links in either the edge or aggregation lay-
ers are hot-spots, and this observations holds across all data centers
and data center types. Speciﬁcally, only 3% of the links in these
two layers appear as a hot-spot for more than 0.1% of time inter-
vals. When edge links are congested, they tend to be congested
continuously, as in CLD2, where a very small fraction of the edge
links appear as hot-spots in 90% of the time intervals.
In contrast, we ﬁnd that the data centers differ signiﬁcantly in
their core layers (Figure 10(c)). Our data centers cluster into 3 hot-
spot classes: (1) Low Persistence-Low Prevalence: This class of
data centers comprises those where the hot-spots are not localized
to any set of links. This includes PRV2, EDU1, EDU2, EDU3,
CLD1, and CLD3, where any given core link is a hot-spot for no
more than 10% of the time intervals; (2) High Persistence-Low
Prevalence: The second group of data centers is characterized by
hot-spots being localized to a small number of core links. This in-
cludes PRV1 and CLD2 where 3% and 8% of the core links, respec-
tively, each appear as hot-spots in > 50% of the time intervals; and
(3) High Persistence-High Prevalence: Finally, in the last group
containing CLD4 and CLD5, a signiﬁcant fraction of the core links
275 1
 0.995
 0.99
 0.985
 0.98
F
D
C
 0.975
 0.01
(a)
 1
 0.995
 0.99
 0.985
 0.98
F
D
C
 0.975
 0.01
(b)
F
D
C
(c)
% of Times an Edge Link is a Hotspot
PRV2
CLD1
CLD2
CLD3
CLD4
CLD5
 0.1
 1
 10
 100
% of Times an Agg Link is a Hotspot
 1
 0.95
 0.9
 0.85
 0.8
 0.75
 0.7
 0.65
EDU1
EDU2
EDU3
PRV1
PRV2
CLD1
CLD2
CLD3
CLD4
CLD5
EDU1
EDU3
PRV1
PRV2
CLD1
CLD2
CLD3
CLD4
CLD5
F
D
C
 1
 0.998
 0.996
 0.994
 0.992
 0.99
 0.988
 0.986
 0.984