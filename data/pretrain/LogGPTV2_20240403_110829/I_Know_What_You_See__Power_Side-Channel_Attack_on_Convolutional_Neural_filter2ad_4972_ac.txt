misclassified pixels is limited thus it does not affect the pixel-level
accuracy much. However, as foreground pixels are key components
to construct digit strokes, their misclassification leads to significant
loss in recognition accuracy. According to these two curves, the
optimal range of threshold lies between [0.3, 0.5], which is the
region in two green dashed lines in Fig. 5 (b). It is consistent with
the threshold selection criteria we raised in last subsection. Another
conclusion from the experiment is that both accuracy metrics are
seldom affected by the choices of kernel. Therefore, adversaries can
acquire acceptable images by only attacking on the power trace for
one kernel.
Kernel Size: Kernel size can be a significant factor affecting both
the pixel-level and recognition accuracy. The average pixel-level
accuracy for model 1 (3×3 kernel size) is 86.2% while the accuracy
for model 2 (5×5 kernel size) is around 74.6%. The recognition accu-
racy is shown in Fig. 6: on average case, 81.6% for images recovered
from power acquired with model 1 and 64.6% with model 2. The
accuracy degradation comes from the information loss when kernel
size increases. It is because our algorithm is only able to find cycles
that deal with background pixels via thresholding, which requires
all the pixels inside the convolutions units to be identical. In other
words, if the convolution unit contains a non-background pixel,
all other background pixels may be mis-identified as foreground
0.00.51.01.52.02.53.0Threshold0%20%40%60%80%100%Pixel-Level AccuracyKernel 1 Pixel AccuracyKernel 2 Pixel Accuracy0%20%40%60%80%100%Recognition AccuracyKernel 1 Recog AccuracyKernel 2 Recog Accuracy0.01.02.03.04.05.0Power consumed per cycle0255075100125150175CountKernel 1Kernel 2-75-50-250255075100Difference in cycle countThreshold = 0.5Best Threshold Range(a)(b)Power Side-Channel Attack on Convolutional Neural Network Accelerators
ACSAC ’18, December 3–7, 2018, San Juan, PR, USA
Figure 6: Recognition accuracy for model 1 and model 2.
pixels by our proposed algorithm. This effect is similar to the mor-
phological dilation operation [31] in the digital image processing
which widens the shape of foreground objects. The recovered image
looks “fatter” than the original image in visual effect. Key struc-
tures smaller than the kernel size are more probable to disappear,
resulting in the degradation of recognition accuracy.
Another discovery is that the recognition accuracy is various for
different digits, especially for model 2. Fig. 6 shows the recognition
accuracy categorized by the class of digits. The accuracy of classi-
fying original image is almost the same and nearly perfect for all
classes. The recognition rates of digits 3, 7 and 9 are below average
for images recovered with model 1. Meanwhile, the accuracy of
digits 1, 3, 4, 7 and 9 drop significantly for model 2. We consider the
discrepancy among different digits comes from inherent structure
of digits and equivalent dilation effect of recovered image. For ex-
ample, the image of digits 1 recovered from model 2 is much “fatter”
than that from model 1 due to dilation effect, so it is more probable
for the classification network to misclassify it as digits 8, causing a
low recognition rate.
We also investigate the classification result of recovered images
for each digit with both models. The results are drawn in the classi-
fication map shown in Fig.7. Each cell (i, j) in the map represents
the portion of images with correct class j which are predicted as
class i, wherein the portion is illustrated with the darkness. For both
models, the darkest color all lies on the diagonal of the map, which
means the classification network is able to correctly predict in most
cases. We observe the recognition accuracy of digits 8 is quite high
(around 90%) for model 2 in Fig. 6. However, the precision is not.
From Fig. 7 (b), the cells in the column of inferred digits 8 are darker
than other cells in the same row except for genuine class 8. Thus,
for an image inferred as digit 8 may have larger probability to be
other digits actually because of its low precision. This is because
the inherent shape of 8 is large than other digits and the classifica-
tion network is more inclined to classify a “fatter” image, which is
caused by dilation effect, to digits 8.
To conclude, the kernel size affects both the pixel-level accuracy
and recognition accuracy due to their equivalent dilation effect
induced by kernels. The recognition accuracy of different digits
also varies because of their inherent structure.
Figure 7: Classification map for model 1 and model 2.
Complexity: The attack method only attacks one power trace.
As the power extraction and background detection procedure are
cycle-based, the time complexity is proportional to the total number
of cycles to compute the convolution which is determined by the
image size O(Sx × Sy), where the Sx and Sy is the length of the
image in two dimensions. The total time used is short in practice.
It takes around 6s to obtain one power trace and 5.7s for power
extraction. For actual image reconstruction it only takes 0.01s.
7 IMAGE RECONSTRUCTION VIA POWER
TEMPLATE
In this section, we propose an attack method, for active adversaries,
to recover the details of images used in the inference process. In-
stead of predicting background marker, we try to obtain values for
each pixel. The section is organized similarly with Section 6 with
three subsections: intuition, attack method and evaluation.
7.1 Intuition
The search space to recover pixel values is prohibitively large even
if only considering the pixels in a small local region. Suppose the
targeted model uses a 3×3 kernel size for the first convolution layer,
the number of pixels involved in the convolution in one cycle is
12 (see the analysis in Section 7.2). Typically a pixel can have a
value ranging from 0 to 255, so the total combinations for the pixels
involved is around 25612 ≈ 7.9 × 1029. Iterating all combinations
of pixel values in brute force is inefficient to perform the attack.
Thus, for active adversaries, we propose to reduce the search space
significantly by building a “power template”. As active adversaries
are able to profile the relationship between power consumption
with arbitrary input images, the pre-built “power template” is able
to efficiently predict the pixel value at actual attack with knowledge
acquired at profiling stage.
As illustrated in the third part of Section 2, the power consumed
in each cycle is determined by the data inside convolution unit,
which comprise pixel values and kernel parameters. Typically, the
same inputs are convolved with different kernels in the convolu-
tional layer. For a specific region of pixels processed by convolution
unit, we can regard the power consumption acquired from different
kernels as a unique feature to infer the value of the pixels. Based on
this intuition, we build a “power template” storing the mapping of
power consumption to pixel values so that adversaries can produce
0123456789Genuine Class0%20%40%60%80%100%Recognition AccuracyOriginal ImageRecovered Image (3x3 kernel)Recovered Image (5x5 kernel)average: 81.6%average: 64.6%0123456789Inferred Class0123456789Genuine Class0123456789Inferred Class0123456789Genuine Class(a) Model 1 (3x3 Kernel Size)(b) Model 2 (5x5 Kernel Size)ACSAC ’18, December 3–7, 2018, San Juan, PR, USA
L. Wei et al.
a set of possible pixel values from a vector of power consumption
retrieved at attack time. Finally, after we acquired many candidate
values for each pixel from multiple power vectors, an image recon-
struction algorithm is adopted to select the best candidate. As one
pixel is processed in multiple cycles, the target of the selection is
to find candidates in these cycles predicting similar values at this
pixel.
7.2 Attack Method
In this subsection, we introduce the detailed steps to recover pixel
values of the input image. First we discuss how to build the power
template at the profiling stage. Then, with the extracted power
from different kernels acquired at attack time, we demonstrate
the method to get candidate pixel values from power template.
Finally we present an algorithm to reconstruct the image from
these candidates.
Power Template Building: Power template stores the mapping
between pixel values and its corresponding power consumption
when convolved with different kernels. In the profiling phase, for
each input image, we collect multiple power traces from the FPGA
loaded with different kernels and obtain power consumption at
each cycle using power extraction in Section 5.
The power consumed in each cycle is determined by the state
transitions of the convolution unit. The kernel remains constant
between cycles, while the pixels are shifted within a row. So the
number of related pixels in one cycle is K ×(K + 1) when the kernel
size of is K × K. For example, suppose at cycle j − 1, pixels from
position (x, y) to (x + K, y + K) are inside the convolution unit,
while at cycle j, pixels from position (x, y + 1) to (x + K, y + K + 1)
are processed. The power consumed in cycle j is induced by change
in the convolution unit, so all pixels from (x, y) to (x + K, y + K + 1)
determine the power consumption. We represent the this region as
Ωj and the pixel values in this region as Pxj = {pv(x) | for x ∈ Ωj}.
These pixel values are named related pixels for j-th cycle. Further,
we represent the power consumption in j-th cycle when the image
is computed with i-th kernel as ρi, j.
So for each cycle, we obtain the related pixels Pxj and the power
collected with different kernels, namely power feature vector, repre-
sented as ρj = (ρ0, j , ..., ρ|K |, j). |K| is the number of kernels used.
For one input image at profiling stage, the power template is con-
structed by adding all pairs of related pixels and corresponding
power feature vector for all cycles. We define the power template
formally as follows:
PT = {(Pxj : ρj)|for j in all cycles}.
The final power template is the union set of power templates con-
structed by every input image.
Candidates Generation: Based on the assumption that similar
pixels processed in the convolution unit generate similar power
feature vector, the straight-forward way to get pixel candidates is
to find pixel values in the power template whose corresponding
power feature vector is closest from that extracted during attack.
However, this method easily fails due to limited samples enrolled in
the power template. Hence, we propose to divide the power feature
vector into several groups and search them in the power template
respectively. After we get the pixel candidates for each group, we
take the intersection of them to generate the final candidate set for
image reconstruction.
To be specific, for a specific cycle j, we acquire a power feature
vector ρ′
from measured power traces and separate them into sev-
j
′}. For each entry in the
eral groups of same size ρ′
= {ρ
j
, ..., ρm}.
power template, we do the same separation, i.e., ρ = {ρ
′, we search the same group of power
For each group of vectors ρm
j
feature vectors in the power template ρm and return the related
pixels if the distance between two groups is within a threshold δ.
The candidate set, consisting of all returned related pixels, is given
by
, ..., ρm
j
1
j
1
′
j
m = {Px |
S
for all (Px : ρ) in PT and dist(ρm, ρm
j
′) < δ}
where the distance metric is defined by
dist(ρm, ρm
j
||ρi − ρ
′
i, j||2,
′) = 
i∈Km
m.
j
The Km represents the kernel indexes of power features grouped
to the m-th group. The final candidate set for the specific cycle j
is given by the intersection of the candidate sets from all different
groups, i.e., S j = ∩mS
Image Reconstruction Algorithm: After obtaining the candi-
date sets for all cycles, we have many possible values for each pixel
and the target is to find the closest one to the actual value.
As we have noted that the same pixel is processed in different
cycles, so the candidates selected among these cycles shall be consis-
tent on the value of the same pixel. We use this as a criterion to find
the optimal selection. Suppose for a pixel at position x = (i, j), there
are t cycles processing this pixel: Cx = {c1, ..., ct }. The candidate
sets for these cycles are Sc1 , ..., Sct . From each candidate set, we
= Sct [Selct ], and
choose a candidate with a selector Selct
as Pxct
we find the pixel value pv(x) at position x in Pxct
. The variance of
pixel values at position x selected from different candidate set shall
be small. The objective of our image reconstruction algorithm is to
find a selector vector so that the selected candidates minimize the
sum of the variance of all pixels in the image. It can be represented
as follows:
var({pv(x) | pv(x) ∈ Pxct
for ct ∈ Cx})
(6)

x∈I
min
Sel
After the selector vector is determined, for each cycle, we get only
one candidate. But for each pixel, we get multiple candidates from
cycles processing it. To get the final value of the pixel, we take the
average of these candidates.
This optimization problem is not easy to solve, here we present
a greedy heuristic method shown in Algorithm 2.
In Algorithm 2, we start with a random candidate set and for
each candidate in the set, we initialize an empty image with the
pixels in the candidate (Line 6–8), other pixels are left undecided.
Then we greedily search the unprocessed candidate set and find the
candidate whose Ωt overlaps current image to the largest extent
(Line 10) and who has the smallest distance with the overlapped
pixels in the current image (Line 11). The image is then updated
accordingly with the candidate and its index is recorded (Line 12–
13) . This process is repeated until all candidate set is processed,
Power Side-Channel Attack on Convolutional Neural Network Accelerators
ACSAC ’18, December 3–7, 2018, San Juan, PR, USA
Algorithm 2: Image Reconstruction Algorithm
Input: Sets of candidates for related pixels {S
Output: A selector Sel of size T that minimizes the sum of
T cycles extracted
, ..., ST } for all
1
end foreach
while there exists candidate set unprocessed do
variance defined in Eq. 6
1
, ..., ST });
I[x] ← pv(x);
I ← EmptyImage();
Sel ← EmptyVector();
foreach pixel pv(x) in Px do
1 TempStore = [];
2 Sr ← Random({S
3 foreach related pixels Px in Sr do
4
5
6
7
8
9
10
11
12
13
14
t ← SelectCycle(I);
Px′ ← MinimalDiscrepancy(St , I);
I ← Update(I, Px′);
Sel[t] ← Idx(P′
x);
end while
σ ← CalculateVariance(Sel, {S
TempStore.add((σ, Sel));
15
16
17 end foreach
18 Sel ← FindMinimal(σ , TempStore);
19 return Sel
1
, ..., ST });
and then a selector of size T is generated. We calculate the variance
defined in Eq. 6 for the selector (Line 15–16). After all candidate in
the original set Sr is processed, we find the selector with minimal
variance and return it as the final result (Line 18–19).
7.3 Evaluation
Experiment Setup: We followed the same experiment setup in