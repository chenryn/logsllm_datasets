title:Complexity vs. performance: empirical analysis of machine learning
as a service
author:Yuanshun Yao and
Zhujun Xiao and
Bolun Wang and
Bimal Viswanath and
Haitao Zheng and
Ben Y. Zhao
Complexity vs. Performance: Empirical Analysis of Machine
Learning as a Service
Yuanshun Yao
PI:EMAIL
University of Chicago
Bimal Viswanath
PI:EMAIL
University of Chicago
Zhujun Xiao
PI:EMAIL
University of Chicago
Haitao Zheng
PI:EMAIL
University of Chicago
Bolun Wang
PI:EMAIL
UCSB/University of Chicago
Ben Y. Zhao
PI:EMAIL
University of Chicago
ABSTRACT
Machine learning classifiers are basic research tools used in numer-
ous types of network analysis and modeling. To reduce the need for
domain expertise and costs of running local ML classifiers, network
researchers can instead rely on centralized Machine Learning as a
Service (MLaaS) platforms.
In this paper, we evaluate the effectiveness of MLaaS systems
ranging from fully-automated, turnkey systems to fully-customizable
systems, and find that with more user control comes greater risk.
Good decisions produce even higher performance, and poor deci-
sions result in harsher performance penalties. We also find that
server side optimizations help fully-automated systems outperform
default settings on competitors, but still lag far behind well-tuned
MLaaS systems which compare favorably to standalone ML libraries.
Finally, we find classifier choice is the dominating factor in deter-
mining model performance, and that users can approximate the
performance of an optimal classifier choice by experimenting with
a small subset of random classifiers. While network researchers
should approach MLaaS systems with caution, they can achieve
results comparable to standalone classifiers if they have sufficient
insight into key decisions like classifiers and feature selection.
CCS CONCEPTS
• Computing methodologies → Machine learning; • Applied
computing;
KEYWORDS
Machine Learning; Cloud Computing
ACM Reference Format:
Yuanshun Yao, Zhujun Xiao, Bolun Wang, Bimal Viswanath, Haitao Zheng,
and Ben Y. Zhao. 2017. Complexity vs. Performance: Empirical Analysis of
Machine Learning as a Service. In Proceedings of IMC ’17. ACM, New York,
NY, USA, 14 pages.
https://doi.org/10.1145/3131365.3131372
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
IMC ’17, November 1–3, 2017, London, United Kingdom
© 2017 Association for Computing Machinery.
ACM ISBN 978-1-4503-5118-8/17/11...$15.00
https://doi.org/10.1145/3131365.3131372
1 INTRODUCTION
Machine learning (ML) classifiers are now common tools for data
analysis. They have become particularly indispensable in the con-
text of networking, where characterizing the behavior of protocols,
services, and users often requires large scale data mining and model-
ing. ML tools have pervaded problems from all facets of networking,
with examples ranging from network link prediction [43, 54], net-
work localization [41, 77], user behavior analysis [71, 72], conges-
tion control protocols [59, 74], performance characterization [8, 76],
botnet detection [31], and network management [1].
As ML tools are increasingly commoditized, most network re-
searchers are interested in them as black box tools, and lack the
resources to optimize their deployments and configurations of
ML systems. Without domain experts or instructions on building
custom-tailored ML systems, some have tried developing automated
or “turnkey” ML systems for network diagnosis [42]. A more ma-
ture alternative is ML as a Service (MLaaS), with offerings from
Google, Amazon, Microsoft and others. These services run on the
cloud, and provide a query interface to an ML classifier trained
on uploaded datasets. They simplify the process of running ML
systems by abstracting away challenges in data storage, classifier
training, and classification.
Given the myriad of decisions in designing any ML system, it is
fitting that MLaaS systems cover the full spectrum between extreme
simplicity (turn-key, nonparametric solutions) and full customiz-
ability (fully tunable systems for optimal performance). Some are
simple black-box systems that do not even reveal the classifier used,
while others offer users choice in everything from data preprocess-
ing, classifier selection, feature selection, to parameter tuning.
MLaaS today are opaque systems, with little known about their
efficacy (in terms of prediction accuracy), their underlying mecha-
nisms and relative merits. For example, how much freedom and con-
figurability do they give to users? What is the difference in potential
performance between fully configurable and turnkey, “black-box”
systems? Can MLaaS providers build in better optimizations that
outperform hand-tuned user configurations? Do MLaaS systems
offer enough configurability to match or surpass the performance
of locally tuned ML tools?
In this paper, we offer a first look at empirically quantifying
the performance of 6 of the most popular MLaaS platforms across
a large number (119) of labeled datasets for binary classification.
Our goals are three-fold. First, we seek to understand how MLaaS
systems compare in performance against each other, and against a
fully customized and tuned local ML library. Our results will shed
Figure 1: Standard ML pipeline and the steps that can be controlled by different MLaaS platforms.
light on the cost-benefit tradeoff of relying on MLaaS systems in-
stead of locally managing ML systems. Second, we wish to better
understand the correlations between complexity, performance and
performance variability. Our results will not only help users choose
between MLaaS providers based on their needs, but also guide com-
panies in traversing the complexity and performance tradeoff when
building their own local ML systems. Third, we want to understand
which key knobs have the biggest impact on performance, and try
to design generalized techniques to optimize those knobs.
Our analysis produces a number of interesting findings.
• First, we observe that current MLaaS systems cover the full range
of tradeoffs between ease of use and user-control. Our results
show a clear and strong correlation between increasing config-
urability (user control) and both higher optimal performance and
higher performance variance.
• Second, we show that classifier choice accounts for much of
the benefits of customization, and that a user can achieve near-
optimal results by experimenting with a small random set of
classifiers, thus dramatically reducing the complexity of classifier
selection.
• Finally, our efforts find clear evidence that fully automated (black-
box) systems like Google and ABM are using server-side tests
to automate classifier choices, including differentiating between
linear and non-linear classifiers. We note that their mechanisms
occasionally err and choose suboptimal classifiers. As a whole,
this helps them outperform other MLaaS systems using default
settings, but they still lag far behind tuned versions of their
competitors. Most notably, a heavily tuned version of the most
customizable MLaaS system (Microsoft) produces performance
nearly-identical to our locally tuned ML library (scikit-learn).
To the best of our knowledge, this paper is the first effort to
empirically quantify the performance of MLaaS systems. We believe
MLaaS systems will be an important tool for network data analysis
in the future, and hope our work will lead to more transparency
and better understanding of their suitability for different network
research tasks.
2 UNDERSTANDING MLAAS PLATFORMS
MLaaS platforms are cloud-based systems that provide machine
learning as a web service to users interested in training, building,
and deploying ML models. Users typically complete an ML task
through a web page interface. These platforms simplify and make
ML accessible to even non-experts. Another selling point is the
affordability and scalability, as these services inherit the strengths
of the underlying cloud infrastructure.
For our analysis, we choose 6 mainstream MLaaS platforms, in-
cluding Amazon Machine Learning (Amazon1), Automatic Business
Modeler (ABM2), BigML3, Google Prediction API (Google4), Mi-
crosoft Azure ML Studio (Microsoft5), and PredictionIO6. These are
the MLaaS services widely available today.
Figure 1 shows the well-known sequence
The MLaaS Pipeline.
of steps typically taken when using any user-managed ML software.
For a given ML task, a user first preprocesses the data, and identifies
the most important features for the task. Next, she chooses an ML
model (e.g. a classifier for a predictive task) and an appropriate im-
plementation of the model (since implementation difference could
cause performance variation [9]), tunes parameters of the model
and then trains the model. Specific MLaaS platforms can simplify
this pipeline by only exposing a subset of the steps to the user
while automatically managing the remaining steps. Figure 1 also
shows the steps exposed to users by each platform. Note that some
(ABM and Google) expose none of the steps to the user but provide
a “1-click” mode that trains a predictive model using an uploaded
dataset. At the other end of the spectrum, Microsoft provides control
for nearly every step in the pipeline.
It is intuitive that more control over
Control and Complexity.
each step in the pipeline allows knowledgeable users to build higher
quality models. Feature, model, and parameter selection can have
significant impact on the performance of an ML task (e.g. prediction).
However, successfully optimizing each step requires overcoming
significant complexity that is difficult without in-depth knowledge
and experience. On the other hand, when limiting control, it is
unclear whether services can perform effective automatic man-
agement of the pipeline and parameters, e.g. in the case of ABM
and Google. Current MLaaS systems cover the whole gamut in
terms of user control and complexity and provide an opportunity
to investigate the impact of complexity on performance.
We summarize the controls available in the pipeline for classifica-
tion tasks in each platform. More details are available in Section 3.
• Preprocessing: The first step involves dataset processing. Com-
mon preprocessing tasks include data cleaning and data trans-
formation. Data cleaning typically involves handling missing
1https://aws.amazon.com/machine-learning
2http://e-abm.com
3https://bigml.com
4https://cloud.google.com/prediction
5https://azure.microsoft.com/en-us/services/machine-learning
6https://predictionio.incubator.apache.org
PreprocessingFeature SelectionClassiϐier ChoiceParameter TuningProgram ImplementationTrained ModelABMGoogleAmazonPredictionIOBigMLMicrosoftTraining DataQuery DataPredictionResultsfeature values, removing outliers, removing incorrect or dupli-
cate records. None of the 6 systems provides any support for
automatic data cleaning and expects the uploaded data to be
already sanitized with errors removed. Data transformation usu-
ally involves normalizing or scaling feature values to lie within
certain ranges. This is particularly useful when features lie in
different ranges, where it becomes harder to compare variations
in feature values that lie in a large range with those that lie in
a smaller range. Microsoft is the only platform that provides
support for data transformation.
• Feature selection: This step selects a subset of features most rel-
evant to the ML task, e.g. those that provide more predictive
power for the task. Feature selection helps improve classification
performance, and also simplifies the problem by eliminating ir-
relevant features. A popular type of feature selection scheme is
Filter method, where a statistical measure (independent of the
classifier choice) is used to rank features based on their class
discriminatory power. Only Microsoft supports feature selection
and provides 8 Filter methods. Some platforms, e.g. BigML, pro-
vide user-contributed scripts for feature selection. We exclude
these cases since they are not officially supported by the platform
and require extra effort to integrate them into the ML pipeline.
• Classifier selection: Different classifiers can be chosen based on
the complexity of the dataset. An important complexity measure
is the linearity (or non-linearity) of the dataset, and classifiers
can be chosen based on their capability of estimating a linear or
non-linear decision boundary. Across all platforms, we experi-
ment with 10 classifiers. ABM and Google offer no user choices.
Amazon only supports Logistic Regression7. BigML provides 4
classifiers, PredictionIO provides 8, while Microsoft gives the
largest number of choices: 9.
• Parameter tuning: These are parameters associated with a clas-
sifier and they must be tuned for each dataset to build a high
quality model. Amazon, PredictionIO, BigML, and Microsoft all
support parameter tuning. Usually each classifier allows users to
tune 3 to 5 parameters. We include detailed information about
classifiers and their parameters in Section 3.
Key Questions. To help understand the relationships between
complexity, performance, and transparency in MLaaS platforms,
we focus our analysis around three key questions and briefly sum-
marize our findings. Figure 2 provides a simple visualization to aid
our discussion.
• How does the complexity (or control) of ML systems correlate with
ideal model accuracy? Assuming we cover the available config-
uration space, how strongly do constraints in complexity limit
model accuracy in practice? How do different controls compare
in relative impact on accuracy?
Answer: Our results show a clear and strong correlation between
increasing complexity (user control) and higher optimal per-
formance. Highly tunable platforms like Microsoft outperform
others when configurations of the ML model are carefully tuned.
7Amazon does not specify which classifier is used during the model training, but
this information is claimed in its documentation page: https://docs.aws.amazon.com/
machine-learning/latest/dg/types-of-ml-models.html.
Figure 2: Overview of control vs. performance/risk tradeoffs
in MLaaS platform.
Among the three control dimensions we investigate, classifier
choice accounts for the most benefits of customization.
• Can increased control lead to higher risks (of building a poorly
performing ML model)? Real users are unlikely to fully optimize
each step of the ML pipeline. We quantify the likely performance
variation at different levels of user control. For instance, how
much would a poor decision in classifier cost the user in practice
on real classification tasks?
Answer: We find higher configurability leads to higher risks of
producing poorly performing models. The highest levels of per-
formance variation also come from choices in classifiers. We also
find that users only need to explore a small random subset of
classifiers (3 classifiers) to achieve near-optimal performance
instead of experimenting with an entire classifier collection.
• How much can MLaaS systems optimize the automated portions
of their pipeline? Despite their nature as black boxes, we seek
to shed light on hidden optimizations at the classifier level in
ABM and Google. Are they optimizing classifiers for different
datasets? Do these internal optimizations lead to better perfor-
mance compared to other MLaaS platforms?
Answer: We find evidence that black-box platforms, i.e. Google
and ABM, are making a choice between linear and non-linear
classifiers based on characteristics of each dataset. Results show
that this internal optimization successfully improves these plat-
forms’ performance, when compared to other MLaaS platforms
(Amazon, PredictionIO, BigML and Microsoft) without tuning any
available controls. However, in some datasets, a naive optimiza-
tion strategy that we devised makes better classifier decisions
and outperforms them.
3 METHODOLOGY
We focus our efforts on binary classification tasks, since that is