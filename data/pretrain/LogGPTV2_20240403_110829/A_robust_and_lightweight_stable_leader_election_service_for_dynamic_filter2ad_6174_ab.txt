(these two algorithms are sketched in Section 6). Other
We used three QoS metrics to compare the performance
of leader election services in various settings. The ﬁrst one
captures the “speed” of the leader election service: it mea-
sures the time that the service takes to recover from the
crash of the current leader, i.e., the time that elapses from
the moment a group loses its current leader due to a crash to
the time when the service completes the election of a new
group leader. Roughly speaking, this measures the sum of
two periods: (a) detection time, i.e., the time that the service
takes to detect that the current leader of a group has crashed
(and so an election must occur), and (b) election time, i.e,
the time that the service takes to ensure that all the alive
processes in the group agree on a new leader. Note that the
group is effectively leaderless during all this time.
To deﬁne this metric more precisely, we say that a group
has a leader at time t if, at time t, there is some alive process
(cid:96) such that every alive process in this group has (cid:96) as its
leader. The leader recovery time, denoted Tr, is a random
variable that measures the time that elapses from the time
when the leader of a group crashes to the time when the
group has a leader again. The average leader recovery time
metric, denoted Tr, is the expected value of Tr.
The second metric that we use, called average mistake
rate, captures the “accuracy” and “stability” of a leader
election service: it is the rate at which the service makes
a “mistake” by demoting a functional group leader. Intu-
itively, such a mistake may occur for one of two reasons.
First, the service’s failure detection mechanism may be in-
accurate: it erroneously suspects that the current leader (cid:96)
has crashed, and this triggers an unnecessary reelection to
replace (cid:96). Second, the service may be unstable: it may de-
mote and replace a well-behaved leader (cid:96) for spurious rea-
sons (e.g., it may demote and replace (cid:96) just because a pro-
cess with a smaller id than (cid:96) joins the system). It is clear
that demoting a functional leader should be avoided: it pre-
vents the progress of all leader-based applications until a
new leader is elected.
To capture the above metric, we say that the demotion of
a proces (cid:96) from leadership is unjustiﬁed if (cid:96) loses the lead-
ership of the system even though (cid:96) has not crashed. The
mistake rate, denoted λu, is a random variable that mea-
sures the number of unjustiﬁed leader demotions that occur
in an hour. The average mistake rate, denoted λu, is the
expected value of λu.
The last metric that we consider measures the availabil-
ity of a group leader. Speciﬁcally, for any given group, the
leader availability metric, denoted Pleader, is the proba-
bility that, at a random time, the group has a (commonly
agreed and alive) leader. Intuitively, this metric is of inter-
networkApplication ProcessShared LibraryAPI callsreceive answersend commandCommand Handlersend answerreceive commandLeader ElectionAlgorithmGroupMaintenanceFailure Detectorstart leaderelectionleaderjoin grouptrust/suspect proc.(TdU, TmrL, PaL)send/receiveALIVEsend/receiveHELLOgroup membersInternational Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE210DSN 2008: Schiper & Touegest to applications for the following reason. Suppose that
a leader-based application needs to use the services of a
leader at random times. Then Pleader is the probability that
when the application needs a leader, it ﬁnds that a com-
monly agreed and functional leader is available. Note that,
for each group, this metric corresponds to the proportion of
time when the group has a leader.
6. Evaluation of Leader Election Services
In this section, we experimentally evaluate and compare
three different versions of the leader election service, de-
noted S1, S2, and S3, under various network behavior set-
tings. All three versions share the same service architecture
(described in Section 4) except that each one uses a differ-
ent algorithm in the Leader Election Algorithm module (as
we explain later). To evaluate and compare the performance
of S1, S2, and S3, we use the leader election QoS metrics
described in Section 5. We also evaluate their costs in terms
of CPU and network bandwidth utilization.
In the following, we ﬁrst describe the parameters of our
experiments, and then describe the experimental results for
each version of the service.
6.1. Experimental System Parameters
The experiments were conducted on a LAN consisting of
a cluster of 12 workstations connected with a gigabit switch.
Each workstation is a P4 3.2 Ghz, with 512 MB of RAM,
running an instance of the leader election service on top of
SuSe Linux 9.2. All our experiments, except those measur-
ing scalability, consisted of a single group of 12 application
processes, one in each workstation, using the service. The
duration of each experiment varied between 1 and 5 days.
In our local area network, workstations rarely crash,
there are practically no message losses and the average
message delay is only about 0.025 ms. To evaluate our
leader election service implementation in other (less favor-
able) network environments, we implemented a module that
causes message losses or delays (by actually dropping or
delaying the service messages) and a module that simu-
lates the crashes and recoveries of workstations (by actually
killing and later restarting individual instances of the ser-
vice running on those workstations). Note that each work-
station crash also kills one of the 12 application processes
using the leader election service (in particular, it may kill
the current leader of the group).
Our experiment parameters control (1) the behavior of
the network that runs the leader election service and (2) the
QoS of the underlying FD used by the service. We now
describe these parameters in more detail.
Workstations behavior. In all our experiments, worksta-
tions crash and recover at random times. More precisely,
for each workstation, the time between two consecutive
workstation crashes is exponentially distributed with an ex-
pected value of 600 seconds (i.e., on the average each pro-
cess crashes every 10 minutes). The time that a worksta-
tion takes to recover from a crash is also exponentially dis-
tributed, with an expected value of 5 seconds (i.e., on the
average a workstation takes 5 seconds to recover from a
crash). Even though crashing every workstation every 10
minutes on the average is extremely pessimistic for realis-
tic environments, we chose this setting for two reasons: (1)
to stress-test and evaluate the leader election service under
adverse conditions, and (2) to collect enough samples (e.g.,
to estimate the average leader recovery time) in reasonable
time. A recovery time of about 5 seconds is long enough to
force the algorithms to notice and react to every crash, and
it is short enough to prevent frequent periods during which
no process is alive.
Communication links behavior. Every group of n pro-
cesses has n(n − 1) directed communication links. In our
experiments, we simulated two types of links — lossy links
and links that are prone to crashes — as described below.
LOSSY LINKS. In most of our experiments (Figure 3 to Fig-
ure 5) we simulate communication links with random mes-
sage losses and delays. More precisely, every message sent
through a lossy link has a probability pL of being dropped
by the link. If a message is not dropped by the link, its delay
is exponentially distributed with an expected value D.
We run experiments where the probability of message
loss pL is 1/10, 1/100, or 1/1000, and the expected message
delay D is 1ms, 10ms, or 100ms. Thus, we evaluated each
version of the leader election service with 9 different values
of the tuple (D, pL) that characterizes lossy links behav-
ior. For brevity, in this paper we only show the experimen-
tal results for the “worst” 4 of these 9 pairs, namely, those
where D is 10ms or 100ms, and pL is 1/10 or 1/100, i.e.,
for the pairs (10ms, 0.01), (100ms, 0.01), (10ms, 0.1), and
(100ms, 0.1).
To see how the various leader election algorithms per-
form in systems with well-behaved communication links,
we also run experiments where the message losses and de-
lays were only those that really occurred in our local area
network. During these experiments, we measured an aver-
age message delay of only 0.025 ms and there were practi-
cally no message losses, i.e., these experiments were con-
ducted over links characterized by the pair (0.025ms, 0).
In summary, we evaluated each leader election algorithm
with ten different behaviors of lossy communication links:
the nine simulated ones and the behavior of our local area
network, which corresponds to (0.025ms, 0). These behav-
iors cover a large spectrum of network types such as local-
area, metropolitan-area, and wide-area networks.
LINKS PRONE TO CRASHES.
In some experiments (Fig-
ure 7) we simulate links that are subject to random crashes
and recoveries. When a link crashes, it completely dis-
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE211DSN 2008: Schiper & Touegconnects the receiver from the sender (by dropping all the
sender’s messages) until the link recovers. When a link re-
covers, it becomes fully operational again.1 In our experi-
ments with link crashes, for each link, the time between two
consecutive link crashes is exponentially distributed with an
expected value of 60 seconds, 300 seconds, or 600 seconds.
So in these experiments each link crashes every 1, 5, or 10
minutes, on the average. The time that a link takes to re-
cover from a crash is also exponentially distributed with an
expected value of 3 seconds — a period that is long enough
to trigger disruptive “false suspicions” (as it will be clear
later, with our QoS settings, any link crash that lasts more
than one second can force some process to falsely suspect
another one to have crashed).
QoS of the underlying FD. As we mentioned earlier, our
leader election service uses the stochastic FD by Chen et
al. [5] as the underlying failure detector. Under some prob-
abilistic assumptions on the system, this FD allows any pro-
cess p to monitor the status of another process q with some
QoS guarantees in terms of the speed and accuracy with
which it detects q’s crash. Every application that uses our
leader election service can set the QoS of the FD to gain
some control on the QoS of the leader election service.2 In
almost all our experiments, we set the QoS of the underly-
ing FD (of Chen et al.) as follows. For every process p that
monitors a process q: (1) Process p takes at most 1 second
to detect q’s crash; (2) on average p makes at most 1 mistake
about the status of q every 100 days; and (3) the probability
that p correctly estimates q’s operational status, at a random
D = 1 second,
time, is at least 0.99999988. Formally, T U
A = 0.99999988. We selected
M R = 100 days, and P L
T L
this strong QoS setting to evaluate the performance of the
service under demanding requirements.
It is clear that the QoS of the leader election service de-
pends on the QoS of the underlying failure detector that the
service uses. For example, a failure detector that takes a
long time to detect the crash of the current leader will delay
its replacement by at least the same amount of time, and this
increases the leader recovery time and decreases the overall
leader availability (in Section 6.6 we describe some experi-
ments that explore the relation between crash detection time
and leader recovery time). On the other hand, a failure de-
tector that makes frequent mistakes (e.g., by errononeously
declaring that the current leader crashed) can increase the
number of unjustiﬁed leader demotions, i.e, increase the av-
erage mistake rate of the leader election service.
Unless we explicitly state otherwise, all the experiments
described henceforth were done with the above settings.
1When a link is operational, its message losses and delays are those
of our (real) network: losses are practically nil, and the average delay is
0.025ms.
2In fact, each group of processes can chose a different QoS for the
underlying FD.
6.2. The Service S1
Description. The leader election algorithm of S1 denoted
Ωid is quite simple: the leader of a group is just the pro-
cess with the smallest identiﬁer among the processes that
are currently deemed to be alive in this group. To estimate
who is currently alive in a group, processes can periodically
send an alive message to every other process in the group,
say once every η seconds, and use some timeout δ on these
messages (so that p declares that q has crashed if more than
δ time elapses since p received the last alive from q).
An ad-hoc implementation of the above failure detector
uses ﬁxed values for η and δ. Such an implementation, how-
ever, has two drawbacks. First, the delicate question of how
to ﬁx η and δ is (usually) left to the user. But determining
the “right” values for η and δ is far from trivial: it depends
on the network characteristics (e.g., the distribution of mes-
sage delays and the probability of message loss) and also on
speed and accuracy of the failure detection that one wants
to achieve. Second, using ﬁxed values for η and δ does not
adapt well to changing network conditions.
To avoid these problems, S1 uses the FD algorithm de-
scribed in [5] (in conjunction with a link quality estimator
module that continuously evaluates the network behavior).
Experimental Evaluation. Figure 3 shows the average
leader recovery time Tr and the average mistake rate λu
of S1 as measured in our experiments.3 More precisely, it
gives the values of Tr and λu that we measured when we
run S1 in each of 5 networks with different lossy link chara-
teristics (recall that D is the average message delay and pL
is the probability of message loss). In Figure 4, we show the
leader availability of S1 (and compare it to the one of S2).
We ﬁrst note that across 5 networks with widely different
behaviors, the values of Tr and λu are remarkably stable.
For example, in a network with an average message delay
of only 0.025 ms and (practically) no message losses, Tr is
0.81 seconds; while in a network with an average message
delay of 100 ms and where 1 message every 10 is lost (on
average), Tr is 0.94 seconds — a very small increase for a
network whose message delays and message losses are or-
ders of magnitude worse than the ﬁrst one. Similarly, λu
remains close to 6 mistakes per hour in all 5 networks, de-
spite large differences in network behavior.
While this may be surprising at ﬁrst, it is due to the un-
derlying FD that adapts to changing network behavior in
order to meet a given QoS: in all our experiments this QoS
remains the same. So, intuitively, the FD layer automati-
cally compensates for the differences in network behavior,
and shields the above layers from these differences.
Note that the leader recovery time of S1 is fairly close
to 1 second (across all the 5 networks), and that 1 second is
also the maximum failure detection time that we chose for
3We also show the 95% conﬁdence intervals of Tr and λu.
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE212DSN 2008: Schiper & Touegthe underlying FD. This is not a coincidence: as we will see
in Section 6.6, the time taken to detect the crash of a leader
is a dominating component of the leader recovery time.
Finally, we note that S1 is unstable: it makes about 6
mistakes every hour. Recall that a mistake occurs when
the service demotes and replaces a functional leader (we
also call this an unjustiﬁed demotion).
In general such a
mistake occurs if (a) the underlying failure detector erro-
neously suspects that the current leader has crashed, or (b)
the leader election algorithm itself is unstable. In our ex-
periments, the underlying FD never made a mistake4, and
all the unjustiﬁed demotions were due to the leader election
algorithm Ωid of S1: about 6 times every hour, a process
with a smaller id than the current leader re-joined the group
(after recovering from a crash) and demoted this leader.
In the next two sections, we describe S2 and S3, two
leader election services that are much more stable than S1.
S1 (cid:5)
)
s
d
n
o
c
e
s
(
r
T
1.2
1.0
0.8
0.6
0.4
10
)
r
u
o
h
/
s
e
k
a
t
s
i
m
(
u
λ
8
6
4
2
3
3
3
3
3
(0.025ms, 0) (10ms, 0.01) (100ms, 0.01) (10ms, 0.1)
(100ms, 0.1)
3
3
3
3
3
3
3
3
3
3
(0.025ms, 0) (10ms, 0.01) (100ms, 0.01) (10ms, 0.1)