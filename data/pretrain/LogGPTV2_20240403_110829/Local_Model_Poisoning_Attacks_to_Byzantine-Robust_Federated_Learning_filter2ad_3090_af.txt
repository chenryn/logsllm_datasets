collaborative ﬁltering. In NIPS, 2016.
[22] Matthew Fredrikson, Eric Lantz, Somesh Jha, Simon
Lin, David Page, and Thomas Ristenpart. Privacy in
pharmacogenetics: An end-to-end case study of person-
alized warfarin dosing. In USENIX Security Symposium,
2014.
[23] Clement Fung, Chris J.M. Yoon, and Ivan Beschastnikh.
Mitigating sybils in federated learning poisoning. In
arxiv, 2018.
[24] Karan Ganju, Qi Wang, Wei Yang, Carl A. Gunter, and
Nikita Borisov. Property inference attacks on fully con-
nected neural networks using permutation invariant rep-
resentations. In CCS, 2018.
[25] Ian J Goodfellow, Jonathon Shlens, and Christian
Szegedy. Explaining and harnessing adversarial exam-
ples. arXiv, 2014.
[26] Kathrin Grosse, Praveen Manoharan, Nicolas Papernot,
Michael Backes, and Patrick McDaniel. On the (sta-
tistical) detection of adversarial examples.
In arXiv,
2017.
[27] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg.
Badnets: Identifying vulnerabilities in the machine
learning model supply chain.
In Machine Learning
and Computer Security Workshop, 2017.
[34] Bin Liang, Miaoqiang Su, Wei You, Wenchang Shi, and
Gang Yang. Cracking classiﬁers for evasion: A case
study on the google’s phishing pages ﬁlter. In ACM
WWW, 2016.
[35] Chang Liu, Bo Li, Yevgeniy Vorobeychik, and Alina
Oprea. Robust linear regression against training data
poisoning. In AISec, 2017.
[36] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song.
Delving into transferable adversarial examples and
black-box attacks. In ICLR, 2017.
[37] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee,
Juan Zhai, Weihang Wang, and Xiangyu Zhang. Trojan-
ing attack on neural networks. In NDSS, 2018.
[38] Aleksander Madry, Aleksandar Makelov, Ludwig
Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards
deep learning models resistant to adversarial attacks.
arXiv preprint arXiv:1706.06083, 2017.
[39] H. Brendan McMahan, Eider Moore, Daniel Ram-
age, Seth Hampson, and Blaise Agüera y Arcas.
Communication-efﬁcient learning of deep networks
from decentralized data. In AISTATS, 2017.
[40] Luca Melis, Congzheng Song, Emiliano De Cristofaro,
and Vitaly Shmatikov. Exploiting unintended feature
leakage in collaborative learning. In IEEE S & P, 2019.
1638    29th USENIX Security Symposium
USENIX Association
[41] Jan Hendrik Metzen, Tim Genewein, Volker Fischer, and
Bastian Bischof. On detecting adversarial perturbations.
In ICLR, 2017.
[53] Reza Shokri, Marco Stronati, Congzheng Song, and Vi-
taly Shmatikov. Membership inference attacks against
machine learning models. In IEEE S & P, 2017.
[42] El Mahdi El Mhamdi, Rachid Guerraoui, and Sébastien
Rouault. The hidden vulnerability of distributed learning
in byzantium. In ICML, 2018.
[54] Nedim Srndic and Pavel Laskov. Practical evasion of a
learning-based classiﬁer: A case study. In IEEE S & P,
2014.
[43] Luis Muñoz-González, Battista Biggio, Ambra Demon-
tis, Andrea Paudice, Vasin Wongrassamee, Emil C Lupu,
and Fabio Roli. Towards poisoning of deep learning al-
gorithms with back-gradient optimization.
In AISec,
2017.
[44] Milad Nasr, Reza Shokri, and Amir Houmansadr. Com-
prehensive privacy analysis of deep learning: Stand-
alone and federated learning under passive and active
white-box inference attacks. In IEEE S & P, 2019.
[45] B. Nelson, M. Barreno, F. J. Chi, A. D. Joseph, B. I. P.
Rubinstein, U. Saini, C. Sutton, J. D. Tygar, and K. Xia.
Exploiting machine learning to subvert your spam ﬁlter.
In LEET, 2008.
[46] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow,
Somesh Jha, Z Berkay Celik, and Ananthram Swami.
Practical black-box attacks against machine learning. In
ACM ASIACCS, 2017.
[47] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt
Fredrikson, Z. Berkay Celik, and Ananthram Swami.
The limitations of deep learning in adversarial settings.
In EuroS&P, 2016.
[48] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh
Jha, and Ananthram Swami. Distillation as a defense to
adversarial perturbations against deep neural networks.
In IEEE S & P, 2016.
[49] Apostolos Pyrgelis, Carmela Troncoso, and Emiliano De
Cristofaro. Knock knock, who’s there? membership
inference on aggregate location data. In NDSS, 2018.
[50] Benjamin IP Rubinstein, Blaine Nelson, Ling Huang,
Anthony D Joseph, Shing-hon Lau, Satish Rao, Nina
Taft, and JD Tygar. Antidote: understanding and defend-
ing against poisoning of anomaly detectors. In ACM
IMC, 2009.
[51] Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian
Suciu, Christoph Studer, Tudor Dumitras, and Tom Gold-
stein. Poison frogs! targeted clean-label poisoning at-
tacks on neural networks. In NIPS, 2018.
[52] Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and
K Michael Reiter. Accessorize to a crime: Real and
stealthy attacks on state-of-the-art face recognition. In
ACM CCS, 2016.
[55] Jacob Steinhardt, Pang Wei Koh, and Percy Liang. Certi-
ﬁed defenses for data poisoning attacks. In NIPS, 2017.
[56] Octavian Suciu, Radu Marginean, Yigitcan Kaya,
Hal Daume III, and Tudor Dumitras. When does ma-
chine learning fail? generalized transferability for eva-
sion and poisoning attacks. In Usenix Security Sympo-
sium, 2018.
[57] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever,
Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob
Fergus. Intriguing properties of neural networks. arXiv,
2013.
[58] Florian Tramèr, Fan Zhang, Ari Juels, Michael K Reiter,
and Thomas Ristenpart. Stealing machine learning mod-
els via prediction apis. In USENIX Security Symposium,
2016.
[59] Brandon Tran, Jerry Li, and Aleksander Madry. Spectral
signatures in backdoor attacks. In NIPS, 2018.
[60] Binghui Wang and Neil Zhenqiang Gong. Stealing
hyperparameters in machine learning. In IEEE S & P,
2018.
[61] Binghui Wang and Neil Zhenqiang Gong. Attacking
graph-based classiﬁcation via manipulating the graph
structure. In CCS, 2019.
[62] Huang Xiao, Battista Biggio, Gavin Brown, Giorgio
Fumera, Claudia Eckert, and Fabio Roli. Is feature se-
lection secure against training data poisoning? In ICML,
2015.
[63] Cong Xie, Sanmi Koyejo, and Indranil Gupta. Fall of
empires: Breaking byzantine-tolerant sgd by inner prod-
uct manipulation. In UAI, 2019.
[64] Weilin Xu, David Evans, and Yanjun Qi. Feature squeez-
ing: Detecting adversarial examples in deep neural net-
works. arXiv preprint arXiv:1704.01155, 2017.
[65] Guolei Yang, Neil Zhenqiang Gong, and Ying Cai. Fake
co-visitation injection attacks to recommender systems.
In NDSS, 2017.
[66] Dong Yin, Yudong Chen, Kannan Ramchandran, and
Peter Bartlett. Byzantine-robust distributed learning:
Towards optimal statistical rates. In ICML, 2018.
USENIX Association
29th USENIX Security Symposium    1639
Table 7: (a) The DNN architecture (input layer is not shown)
used for MNIST and Fashion MNIST. (b) Testing error rates
when applying attacks for Krum to attack Bulyan.
(b)
Convolution + ReLU 3× 3× 30
Convolution + ReLU 3× 3× 50
Max Pooling
No attack
Partial Knowledge
Full Knowledge
Bulyan
0.14
0.36
0.38
(a)
Layer Type
Size
2× 2
2× 2
200
10 / 8
Max Pooling
Fully Connected + ReLU
Softmax
Table 8: Testing error rates of our attacks based on the devia-
tion goal and directed deviation goal.
Deviation goal
Directed deviation goal
Krum Trimmed mean Median
0.87
0.80
0.12
0.29
0.10
0.52
A Attacking Bulyan
Bulyan is based on Krum. We apply our attacks for Krum to
attack Bulyan. Table 7b shows results of attacking Bulyan.
The dataset is MNIST, the classiﬁer is logistic regression,
m = 100, c = 20, θ = m− 2c (Bulyan selects θ local models
using Krum), and γ = θ − 2c (Bulyan takes the mean of γ
parameters). Our results show that our attacks to Krum can
transfer to Bulyan. Speciﬁcally, our partial knowledge attack
increases the error rate by around 150%, while our full knowl-
edge attack increases the error rate by 165%.
max
w(cid:5)
,··· ,w(cid:5)
1
c
B Deviation Goal
The deviation goal is to craft local models w(cid:5)
c for
the compromised worker devices via solving the following
optimization problem in each iteration:
||w− w(cid:5)||1,
,··· ,w(cid:5)
,w(cid:5)
1
2
1,··· ,w(cid:5)
w(cid:5) = A(w(cid:5)
subject to w = A(w1,··· ,wc,wc+1,··· ,wm),
c,wc+1,··· ,wm),
(5)
where ||·||1 is L1 norm. We can adapt our attacks based on the
directed deviation goal to the deviation goal. For simplicity,
we focus on the full knowledge scenario.
Krum: Similar to the directed deviation goal, we make two
= wRe−λ and the c compromised lo-
approximations, i.e., w(cid:5)
cal models are the same. Then, we formulate an optimization
= wRe − λs is
problem similar to Equation 2, except that w(cid:5)
= wRe − λ. Like Theorem 1, we can derive an
changed to w(cid:5)
upper bound of λ, given which we use binary search to solve
λ. After solving λ, we obtain w(cid:5)
1. Then, we randomly sample
c− 1 vectors whose Euclidean distances to w(cid:5)
1 are smaller
than ε as the other c− 1 compromised local models.
Trimmed mean: Theoretically, we can show that the follow-
ing attack can maximize the deviation of the global model: we
1
1
1
1640    29th USENIX Security Symposium
USENIX Association
The bound only depends on the before-attack local models.
use any c numbers that are larger than wmax, j or smaller than
wmin, j, depending on which one makes the deviation larger, as
the jth local model parameters on the c compromised worker
devices. Like the directed deviation goal, when implementing
the attack, we randomly sample the c numbers in the inter-
val [wmax, j,b·wmax, j] (when wmax, j > 0) or [wmax, j,wmax, j/b]
(when wmax, j ≤ 0), or in the interval [wmin, j/b,wmin, j] (when
wmin, j > 0) or [b·wmin, j,wmin, j] (when wmin, j ≤ 0), depending
on which one makes the deviation larger.
Median: We apply the attack for trimmed mean to median.
Experimental results: Table 8 empirically compares the de-
viation goal and directed deviation goal, where MNIST and
LR classiﬁer are used. For Krum, both goals achieve high test-
ing error rates. However, for trimmed mean and median, the
directed deviation goal achieves signiﬁcantly higher testing
error rates than the deviation goal.
C Proof of Theorem 1
We denote by Γa
w the set of a local models among the crafted c
compromised local models and m−c benign local models that
are the closest to the local model w with respect to Euclidean
distance. Moreover, we denote by ˜Γa
w the set of a benign local
models that are the closest to w with respect to Euclidean
distance. Since w(cid:5)
1 is chosen by Krum, we have the following:
∑
w(cid:5)
1
l∈Γm−c−2
D(wl,w(cid:5)
1) ≤ min
c+1≤i≤m
∑
wi
l∈Γm−c−2
D(wl,wi),
(6)
where D(·,·) represents Euclidean distance. The distance be-
1 and the other c− 1 compromised local models is
tween w(cid:5)
0, since we assume they are the same in the optimization
problem in Equation 2 when ﬁnding w(cid:5)
1. Therefore, we have:
∑
wi
1) ≤ min
c+1≤i≤m
D(wl,w(cid:5)
D(wl,wi).
l∈ ˜Γm−2c−1
l∈Γm−c−2
(7)
∑
w(cid:5)
1
According to the triangle inequality D(wl,w(cid:5)
,wRe)− D(wl,wRe), we get:
) ≥
1
1
l∈Γm−c−2
D(w(cid:5)
(m− 2c− 1)· D(w(cid:5)
≤ min
c+1≤i≤m
≤ min
c+1≤i≤m
∑
wi
∑
wi
Since D(w(cid:5)
λ ≤
l∈ ˜Γm−c−2
1
1,wRe)
D(wl,wi) + ∑
w(cid:5)
1
l∈ ˜Γm−2c−1
D(wl,wRe)
D(wl,wi) + (m− 2c− 1)· max
c+1≤i≤m
D(wi,wRe).
√
d · λ, we have:
1
,wRe) = (cid:8)λ· s(cid:8)2 =
· min
c+1≤i≤m
√
d
· max
c+1≤i≤m
(k− c + 1)
+ 1√
d
∑
l∈ ˜Γkwi
D(wi,wRe).
D(wl,wi)
(8)