### References

1. [22] Matthew Fredrikson, Eric Lantz, Somesh Jha, Simon Lin, David Page, and Thomas Ristenpart. "Privacy in Pharmacogenetics: An End-to-End Case Study of Personalized Warfarin Dosing." *USENIX Security Symposium*, 2014.
2. [23] Clement Fung, Chris J.M. Yoon, and Ivan Beschastnikh. "Mitigating Sybils in Federated Learning Poisoning." *arXiv*, 2018.
3. [24] Karan Ganju, Qi Wang, Wei Yang, Carl A. Gunter, and Nikita Borisov. "Property Inference Attacks on Fully Connected Neural Networks Using Permutation Invariant Representations." *CCS*, 2018.
4. [25] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. "Explaining and Harnessing Adversarial Examples." *arXiv*, 2014.
5. [26] Kathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes, and Patrick McDaniel. "On the (Statistical) Detection of Adversarial Examples." *arXiv*, 2017.
6. [27] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. "Badnets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain." *Machine Learning and Computer Security Workshop*, 2017.
7. [34] Bin Liang, Miaoqiang Su, Wei You, Wenchang Shi, and Gang Yang. "Cracking Classifiers for Evasion: A Case Study on Google's Phishing Pages Filter." *ACM WWW*, 2016.
8. [35] Chang Liu, Bo Li, Yevgeniy Vorobeychik, and Alina Oprea. "Robust Linear Regression Against Training Data Poisoning." *AISec*, 2017.
9. [36] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. "Delving into Transferable Adversarial Examples and Black-Box Attacks." *ICLR*, 2017.
10. [37] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu Zhang. "Trojaning Attack on Neural Networks." *NDSS*, 2018.
11. [38] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. "Towards Deep Learning Models Resistant to Adversarial Attacks." *arXiv preprint arXiv:1706.06083*, 2017.
12. [39] H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Agüera y Arcas. "Communication-Efficient Learning of Deep Networks from Decentralized Data." *AISTATS*, 2017.
13. [40] Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov. "Exploiting Unintended Feature Leakage in Collaborative Learning." *IEEE S & P*, 2019.
14. [41] Jan Hendrik Metzen, Tim Genewein, Volker Fischer, and Bastian Bischof. "On Detecting Adversarial Perturbations." *ICLR*, 2017.
15. [53] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. "Membership Inference Attacks Against Machine Learning Models." *IEEE S & P*, 2017.
16. [42] El Mahdi El Mhamdi, Rachid Guerraoui, and Sébastien Rouault. "The Hidden Vulnerability of Distributed Learning in Byzantium." *ICML*, 2018.
17. [54] Nedim Srndic and Pavel Laskov. "Practical Evasion of a Learning-Based Classifier: A Case Study." *IEEE S & P*, 2014.
18. [43] Luis Muñoz-González, Battista Biggio, Ambra Demontis, Andrea Paudice, Vasin Wongrassamee, Emil C. Lupu, and Fabio Roli. "Towards Poisoning of Deep Learning Algorithms with Back-Gradient Optimization." *AISec*, 2017.
19. [44] Milad Nasr, Reza Shokri, and Amir Houmansadr. "Comprehensive Privacy Analysis of Deep Learning: Standalone and Federated Learning Under Passive and Active White-Box Inference Attacks." *IEEE S & P*, 2019.
20. [45] B. Nelson, M. Barreno, F. J. Chi, A. D. Joseph, B. I. P. Rubinstein, U. Saini, C. Sutton, J. D. Tygar, and K. Xia. "Exploiting Machine Learning to Subvert Your Spam Filter." *LEET*, 2008.
21. [46] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z. Berkay Celik, and Ananthram Swami. "Practical Black-Box Attacks Against Machine Learning." *ACM ASIACCS*, 2017.
22. [47] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay Celik, and Ananthram Swami. "The Limitations of Deep Learning in Adversarial Settings." *EuroS&P*, 2016.
23. [48] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. "Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks." *IEEE S & P*, 2016.
24. [49] Apostolos Pyrgelis, Carmela Troncoso, and Emiliano De Cristofaro. "Knock Knock, Who’s There? Membership Inference on Aggregate Location Data." *NDSS*, 2018.
25. [50] Benjamin IP Rubinstein, Blaine Nelson, Ling Huang, Anthony D. Joseph, Shing-hon Lau, Satish Rao, Nina Taft, and JD Tygar. "Antidote: Understanding and Defending Against Poisoning of Anomaly Detectors." *ACM IMC*, 2009.
26. [51] Ali Shafahi, W. Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, and Tom Goldstein. "Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks." *NIPS*, 2018.
27. [52] Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and K. Michael Reiter. "Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition." *ACM CCS*, 2016.
28. [55] Jacob Steinhardt, Pang Wei Koh, and Percy Liang. "Certified Defenses for Data Poisoning Attacks." *NIPS*, 2017.
29. [56] Octavian Suciu, Radu Marginean, Yigitcan Kaya, Hal Daume III, and Tudor Dumitras. "When Does Machine Learning Fail? Generalized Transferability for Evasion and Poisoning Attacks." *Usenix Security Symposium*, 2018.
30. [57] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. "Intriguing Properties of Neural Networks." *arXiv*, 2013.
31. [58] Florian Tramèr, Fan Zhang, Ari Juels, Michael K. Reiter, and Thomas Ristenpart. "Stealing Machine Learning Models via Prediction APIs." *USENIX Security Symposium*, 2016.
32. [59] Brandon Tran, Jerry Li, and Aleksander Madry. "Spectral Signatures in Backdoor Attacks." *NIPS*, 2018.
33. [60] Binghui Wang and Neil Zhenqiang Gong. "Stealing Hyperparameters in Machine Learning." *IEEE S & P*, 2018.
34. [61] Binghui Wang and Neil Zhenqiang Gong. "Attacking Graph-Based Classification via Manipulating the Graph Structure." *CCS*, 2019.
35. [62] Huang Xiao, Battista Biggio, Gavin Brown, Giorgio Fumera, Claudia Eckert, and Fabio Roli. "Is Feature Selection Secure Against Training Data Poisoning?" *ICML*, 2015.
36. [63] Cong Xie, Sanmi Koyejo, and Indranil Gupta. "Fall of Empires: Breaking Byzantine-Tolerant SGD by Inner Product Manipulation." *UAI*, 2019.
37. [64] Weilin Xu, David Evans, and Yanjun Qi. "Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks." *arXiv preprint arXiv:1704.01155*, 2017.
38. [65] Guolei Yang, Neil Zhenqiang Gong, and Ying Cai. "Fake Co-Visitation Injection Attacks to Recommender Systems." *NDSS*, 2017.
39. [66] Dong Yin, Yudong Chen, Kannan Ramchandran, and Peter Bartlett. "Byzantine-Robust Distributed Learning: Towards Optimal Statistical Rates." *ICML*, 2018.

### Table 7: (a) The DNN Architecture (Input Layer Not Shown) Used for MNIST and Fashion MNIST. (b) Testing Error Rates When Applying Attacks for Krum to Attack Bulyan.

| (b) | No Attack | Partial Knowledge | Full Knowledge |
|-----|------------|--------------------|-----------------|
| Bulyan | 0.14 | 0.36 | 0.38 |

| (a) | Layer Type | Size |
|-----|-------------|------|
| Convolution + ReLU | 3×3×30 |
| Convolution + ReLU | 3×3×50 |
| Max Pooling | 2×2 |
| Max Pooling | 2×2 |
| Fully Connected + ReLU | 200 |
| Softmax | 10/8 |

### Table 8: Testing Error Rates of Our Attacks Based on the Deviation Goal and Directed Deviation Goal.

| Deviation Goal | Directed Deviation Goal |
|----------------|-------------------------|
| Krum | 0.87 | 0.80 |
| Trimmed Mean | 0.12 | 0.29 |
| Median | 0.10 | 0.52 |

### A. Attacking Bulyan

Bulyan is based on Krum. We apply our attacks for Krum to attack Bulyan. Table 7(b) shows the results of attacking Bulyan. The dataset used is MNIST, the classifier is logistic regression, \( m = 100 \), \( c = 20 \), \( \theta = m - 2c \) (Bulyan selects \( \theta \) local models using Krum), and \( \gamma = \theta - 2c \) (Bulyan takes the mean of \( \gamma \) parameters). Our results show that our attacks on Krum can be transferred to Bulyan. Specifically, our partial knowledge attack increases the error rate by around 150%, while our full knowledge attack increases the error rate by 165%.

### B. Deviation Goal

The deviation goal is to craft local models \( w^*_{1}, \ldots, w^*_{c} \) for the compromised worker devices via solving the following optimization problem in each iteration:

\[
\max_{w^*_{1}, \ldots, w^*_{c}} ||w - w^*||_1
\]

subject to:

\[
w = A(w_1, \ldots, w_c, w_{c+1}, \ldots, w_m),
\]
\[
w^* = A(w^*_{1}, \ldots, w^*_{c}, w_{c+1}, \ldots, w_m),
\]

where \( ||\cdot||_1 \) is the L1 norm. We can adapt our attacks based on the directed deviation goal to the deviation goal. For simplicity, we focus on the full knowledge scenario.

**Krum:**
Similar to the directed deviation goal, we make two approximations: \( w^* = w_{Re} - \lambda \) and the \( c \) compromised local models are the same. Then, we formulate an optimization problem similar to Equation 2, except that \( w^* = w_{Re} - \lambda s \) is changed to \( w^* = w_{Re} - \lambda \). Like Theorem 1, we can derive an upper bound of \( \lambda \), given which we use binary search to solve \( \lambda \). After solving \( \lambda \), we obtain \( w^*_1 \). Then, we randomly sample \( c-1 \) vectors whose Euclidean distances to \( w^*_1 \) are smaller than \( \epsilon \) as the other \( c-1 \) compromised local models.

**Trimmed Mean:**
Theoretically, we can show that the following attack can maximize the deviation of the global model: we use any \( c \) numbers that are larger than \( w_{\text{max}, j} \) or smaller than \( w_{\text{min}, j} \), depending on which one makes the deviation larger, as the \( j \)-th local model parameters on the \( c \) compromised worker devices. Like the directed deviation goal, when implementing the attack, we randomly sample the \( c \) numbers in the interval \([w_{\text{max}, j}, b \cdot w_{\text{max}, j}]\) (when \( w_{\text{max}, j} > 0 \)) or \([w_{\text{max}, j}/b, w_{\text{max}, j}]\) (when \( w_{\text{max}, j} \leq 0 \)), or in the interval \([w_{\text{min}, j}/b, w_{\text{min}, j}]\) (when \( w_{\text{min}, j} > 0 \)) or \([b \cdot w_{\text{min}, j}, w_{\text{min}, j}]\) (when \( w_{\text{min}, j} \leq 0 \)), depending on which one makes the deviation larger.

**Median:**
We apply the attack for trimmed mean to median.

**Experimental Results:**
Table 8 empirically compares the deviation goal and directed deviation goal, where MNIST and LR classifier are used. For Krum, both goals achieve high testing error rates. However, for trimmed mean and median, the directed deviation goal achieves significantly higher testing error rates than the deviation goal.

### C. Proof of Theorem 1

We denote by \( \Gamma_a^w \) the set of \( a \) local models among the crafted \( c \) compromised local models and \( m-c \) benign local models that are the closest to the local model \( w \) with respect to Euclidean distance. Moreover, we denote by \( \tilde{\Gamma}_a^w \) the set of \( a \) benign local models that are the closest to \( w \) with respect to Euclidean distance. Since \( w^*_1 \) is chosen by Krum, we have the following:

\[
\sum_{l \in \Gamma_{m-c-2}^{w^*_1}} D(w_l, w^*_1) \leq \min_{c+1 \leq i \leq m} \sum_{l \in \Gamma_{m-c-2}^{w_i}} D(w_l, w_i),
\]

where \( D(·,·) \) represents Euclidean distance. The distance between \( w^*_1 \) and the other \( c-1 \) compromised local models is 0, since we assume they are the same in the optimization problem in Equation 2 when finding \( w^*_1 \). Therefore, we have:

\[
\sum_{l \in \Gamma_{m-c-2}^{w^*_1}} D(w_l, w^*_1) \leq \min_{c+1 \leq i \leq m} \sum_{l \in \tilde{\Gamma}_{m-2c-1}^{w_i}} D(w_l, w_i).
\]

According to the triangle inequality \( D(w_l, w^*_1, w_{Re}) - D(w_l, w_{Re}) \), we get:

\[
(m-2c-1) \cdot D(w^*_1, w_{Re}) \leq \min_{c+1 \leq i \leq m} \left( \sum_{l \in \tilde{\Gamma}_{m-2c-1}^{w_i}} D(w_l, w_i) + (m-2c-1) \cdot \max_{c+1 \leq i \leq m} D(w_i, w_{Re}) \right).
\]

Since \( D(w^*_1, w_{Re}) = (\sqrt{d} \cdot \lambda)^2 \), we have:

\[
\lambda \leq \frac{1}{\sqrt{d}} \cdot \min_{c+1 \leq i \leq m} \left( \sum_{l \in \tilde{\Gamma}_{k}^{w_i}} D(w_l, w_i) + (m-2c-1) \cdot \max_{c+1 \leq i \leq m} D(w_i, w_{Re}) \right).
\]

This completes the proof.