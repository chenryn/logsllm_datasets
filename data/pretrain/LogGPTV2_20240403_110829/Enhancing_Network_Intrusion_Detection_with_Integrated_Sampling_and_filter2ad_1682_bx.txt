for the Secondary Path. The Secondary Filter is the OR’ed juxtaposition of all the ﬁlter
280
J.M. Gonzalez and V. Paxson
redef secondary_filters += { ["tcp[13] & 7 != 0"] = SFR_flag_event };
event SFR_flag_event(filter: string, pkt: pkt_hdr)
{
# Perform analysis on the packet header fields given in "pkt" here.
}
Fig. 1. Secondary Path Use Example
indices speciﬁed for secondary_filters. Figure 1 shows an example Bro script.
It uses the secondary ﬁlter to invoke the SFR_flag_event event handler for every
packet matching the expression “tcp[13] & 7= 0!”, i.e., any TCP packet with any of the
SYN, FIN, or RST ﬂags set. pkt_hdr is a Bro record type representing the network-
and transport-layer headers of a packet.
This particular ﬁlter can be used to track connection start and stop times, and hence
duration, participating hosts, ports, and (using differences in sequence numbers) bytes
transferred in each direction. The few lines shown are all that is required to then further
analyze these packets using Bro’s domain-speciﬁc scripting language.
4.5 Performance
In this section we brieﬂy assess the performance of our Secondary Path implementa-
tion.2 Our goal is to compare the cost within a NIDS implementation of the infrastruc-
ture required to implement the Secondary Path (dispatching plus internal piping) versus
the cost of the packet ﬁlter processing. To do so, we use the Secondary Filter to trigger
a null event handler, i.e., an event that does not carry out any work and returns as soon
as it is invoked.
The processing cost depends not only on the number of packets that raise the Sec-
ondary Path event, but also on the number of packets than do not raise the Secondary
Path event but still must be read by the kernel and eventually discarded by the Secondary
Filter.
Figure 2 shows the corresponding performance for different volumes of trafﬁc and
different capture ratios (proportion of packets that match the ﬁlter). Note that both axes
are logarithmic.
The thick line represents the cost of rejecting all packets with the Secondary Filter.
We call this cost “ﬁxed”, as it is independent of the number of packets accepted by the
Secondary Filter. It is the sum of two effects, namely (a) the ﬁxed cost of running Bro,
and (b) the cost of accessing all the packets in the stream and running the Secondary
Filter over them. It is clear that the ﬁrst effect is more important for small traces (the ﬂat
part to the left of the 10K packet mark), while the second effect dominates with large
traces.
The dashed and dotted lines show the additional cost of empty event handlers when
a given ratio of the packets match the ﬁlter. Not surprisingly, we see that this variable
2 Unless otherwise noted, all experiments described in this paper were carried out using an idle
single-processor Intel Xeon (Pentium) CPU running at 3.4 GHz, with 512 KB cache and 2 GB
of total memory, under FreeBSD 4.10. All times reported are the sum of user and system times
as reported by the OS. We ran each experiment 100 times, ﬁnding the standard deviation in
timings negligible compared to the average times.
Enhancing Network Intrusion Detection
281
ﬁxed, per-trace cost
variable cost (capture 1:1 packets)
variable cost (capture 1:10 packets)
variable cost (capture 1:100 packets)
variable cost (capture 1:1000 packets)
100
10
1
0.1
0.01
)
c
e
s
(
e
m
i
t
0.001
100
1 k
10 k
trace packets
100 k
1 M
10 M
Fig. 2. Performance of the Secondary Path with an Empty Event
cost is proportional to the ratio of packets matching the ﬁlter: the variable cost of sam-
pling, say, 1 in 10 packets is about 10 times larger than the variable cost of sampling
1 in 100 packets. We also see that the ﬁxed cost of running the Secondary Path is sim-
ilar to the variable cost of capturing 1 in 100 packets. This means that provided the
analysis performed on captured secondary packets is not too expensive, whether the
detector’s ﬁlter matches say 1 in 1,000 packets, or 1 in 10,000 packets, does not affect
the Secondary Path overhead. When the ratio approaches 1 in 100 packets, however, the
Secondary Path cost starts becoming appreciable.
5 Applications
In this section we present three examples of analyzers we implemented that take advan-
tage of the Secondary Path: disambiguating the size of large TCP connections (§ 5.1),
ﬁnding dominant trafﬁc elements (§ 5.2), and easily integrating into Bro previous work
on detecting backdoors (§ 5.3; [26]). The ﬁrst of these provides only a modest enhance-
ment to the NIDS’s analysis, but illustrates the use of a fairly non-traditional style of
ﬁlter. The second provides a more substantive analysis capability that a NIDS has dif-
ﬁculty achieving efﬁciently using traditional main-path ﬁltering. The third shows how
the Secondary Path opens up NIDS analysis to forms of detection that we can readily
express using some sort of packet-level signature.
Unless otherwise stated, we assess these using a trace (named tcp-1) of all TCP
trafﬁc sent for a 2-hour period during a weekday working hour at the Gbps Internet
access link of the Lawrence Berkeley National Laboratory (LBNL). The trace consists
of 127 M packets, 1.2 M connections, and 113 GB of data (averaging 126 Mbps and
892 bytes/packet).
282
J.M. Gonzalez and V. Paxson
5.1 Large Connection Detection
A cheap mechanism often used to calculate the amount of trafﬁc in a stateful (TCP) con-
nection consists of computing the difference between the sequence numbers at the begin-
ning and at the end of a connection. While this often works well, it can fail for (a) connec-
tions that do not terminate during the observation period, or for which the NIDS misses
their establishment, (b) very large (greater than 4 GB) connections that wrap around the
TCP sequence number (note that TCP’s operation allows this), or (c) broken TCP stacks
that emit incorrect sequence numbers, especially within RST segments.
As we develop in this section, we can correct for these deﬁciencies using a secondary
ﬁlter. In doing so, the aim is to augment the main path’s analysis by providing a more
reliable source of connection length, which also illustrates how the Secondary Path can
work in conjunction with, and complement, existing functionality.
Implementation. Our large-connection detector works by ﬁltering for several thin,
equidistant, randomly-located stripes in the sequence number space. A truly large ﬂow
will pass through these stripes in an orderly fashion, perhaps several times. The detector
tracks all packets that pass through any of the stripes, counting the number of times a
packet from a given ﬂow passes through consecutive regions (K).
Figure 3 shows an example. The 4 horizontal stripes (sA, sB, sC, and sD) represent
the parts of the TCP sequence number space where the detector “listens” for packets.
As the TCP sequence number range is 4 GB long, each stripe is separated 1 GB from
the next one.
The thick diagonal lines depict the time and TCP sequence number of the packets
of a given TCP connection. The dotted, vertical lines represent events in the Secondary
Path. Note that we could use a different number of lines, and lines with different width
(see below). If the detector sees a connection passing through 2 consecutive stripes
(K = 1), it knows that the connection has likely accounted for at least 1 GB.
We locate the ﬁrst stripe randomly to prevent an adversary from predicting the sec-
tions of monitored sequence space, which would enable them to overwhelm the detector
by sending a large volume of packets that fall in the stripes. The remaining stripes then
come at ﬁxed increments from the ﬁrst, dividing the sequence space into equidistant
zones.
Our detector always returns two estimates, a lower and an upper limit. If a connec-
tion has been seen in two consecutive stripes, the estimated size may be as large as the
distance between 4 consecutive stripes, or as small as the distance between 2 consecu-
tive stripes. In the previous example, we know that the connection has accounted for at
least 1 GB and at most 3 GB of trafﬁc.
We then use these estimates to annotate the connection record that Bro’s main con-
nection analyzer constructs and logs. This allows us to readily integrate the extra infor-
mation provided by the detector into Bro’s mainstream analysis.
One issue that arises in implementing the detector is constructing the tcpdump ex-
pression, given that we want to parameterize it in both the number of stripes and the
width of the stripes. See [11] for details on doing so, and the current Bro distribution
(from bro-ids.org) for code in the ﬁle policy/large-conns.bro. Note that the number of
stripes does not affect the complexity of the tcpdump ﬁlter, just the computation of the
bitmask used in the ﬁlter to detect a sequence number the falls within some stripe.
Enhancing Network Intrusion Detection
283
seq number
4 GB
sD
sC
sB
sA
0
Fig. 3. Large Connection Detector Example
time
A ﬁnal problem that arises concerns connections for which the sampled packets do
not progress sequentially through the stripes, but either skip a stripe or revisit a previous
stripe. These “incoherencies” can arise due to network reordering or packet capture
drops. Due to limited space, we defer discussion of dealing with them to [11].
Evaluation. We ran the Large Connection Detector on the tcp-1 trace, varying the num-
ber S of stripes. We used a ﬁxed stripe-size of 2 KB; stripe size only plays a signiﬁcant
role in the presence of packet ﬁlter drops (see [11] for analysis), but for this trace there
were very few drops.
Figure 4 shows for the largest connection in the trace (3.5 GB application-layer pay-
load), its real size, the upper and lower estimations reported by the detector, and the
average of the last two (the average estimation), as we vary S. The lower line shows
the running time of the large connection detector. (Rerunning the experiment with wide
stripes, up to 16 KB, reported very similar results.) All experiments ran with the Main
Path disabled, but we separately measured its time (with no application-layer analysis
enabled) to be 890 sec. Thus, the running time is basically constant up to S = 8192
stripes, and a fraction of the Main Path time. Finally, we veriﬁed that as we increase
the number of stripes, our precision nominally increases, but at a certain point it actu-
ally degrades because of the presence of incoherences (non-sequential stripes); again,
see [11] for discussion.
5.2 Heavy Hitters
The goal of the “heavy hitters” (HH) detector is to discover heavy trafﬁc macroﬂows
using a low-bandwidth, pseudo-random sampling ﬁlter on the Secondary Path, where
we deﬁne a macroﬂow as a set of packets that share some subset of the 5-tuple ﬁelds
284
J.M. Gonzalez and V. Paxson
4.5 GB
4 GB
3.5 GB
3 GB
2.5 GB
)
s
e
t
y
b
7
L
(
e
z
i
s
n
o
i
t
c
e
n
n
o
c
2 GB
4
16
64
upper estimation
real size
average estimation
lower estimation
running time (stripe: 2 KB)
1200
1000
800
600
400
200
)
c
e
s
(
e
m
i
t
n
u
r
256
0
1024 4096 16384 65536
number of stripes
Fig. 4. Detector Estimation for a Large Connection
(IP source and destination addresses, transport-layer source and destination ports, and
transport protocol). This deﬁnition includes the high-volume connections (sharing all
5 ﬁelds), but also other cases such as a host undergoing a ﬂood (all packets sharing
the same IP destination address ﬁeld) or a busy server (all packets sharing a common
IP address and port value). The inspiration behind assessing along different levels of
granularity comes from the AutoFocus tool of Estan et al [8].
As indicated above, macroﬂows can indicate security problems (inbound or out-
bound ﬂoods), or simply inform the operator of facets of the “health” of the network
in terms of the trafﬁc it carries. However, if a NIDS uses ﬁltering on its Main Path
to reduce its processing load, it likely has little visibility into the elements compris-
ing signiﬁcant macroﬂows, since the whole point of the Main Path ﬁltering is to avoid
capturing the trafﬁc of large macroﬂows in order to reduce the processing loads on the
NIDS. Hence the Secondary Path opens up a new form of analysis difﬁcult for a NIDS
to otherwise efﬁciently achieve.
The HH detector starts accounting for a trafﬁc stream using the most speciﬁc gran-
uarity, i.e., each sampled packet’s full 5-tuple, and then widens the granularity to a set
of other, more generic, categories. For example, a host scanning a network may not
have any large connection, but the aggregate of its connection attempts aggregated to
just source address will show signiﬁcant activity.
Note that HH differs from the large connection detector discussed in Section 5.1 in
that it ﬁnds large macroﬂows even if none of the individual connections comprising the
macroﬂow is particularly large. It also can detect macroﬂows comprised of non-TCP
trafﬁc, such as UDP or ICMP.
Operation. HH works by clustering each pseudo-random sample of the trafﬁc it ob-
tains at several granularities, maintaining counts for each corresponding macroﬂow.
Whenever a macroﬂow exceeds a user-deﬁned threshold (e.g., number of packets, con-
Enhancing Network Intrusion Detection
285
Table 2. Tables Used by the Heavy Hitters Detector
table name speciﬁcity description
saspdadp
saspda__
sa__da__
sasp____
sa____dp
sa______
__sp____
connection (traditional 5-tuple deﬁnition)
trafﬁc between a host and a host-port pair
trafﬁc between two hosts
trafﬁc to or from a host-port pair
trafﬁc between a host and a remote port
trafﬁc to or from a host
trafﬁc to or from a port
4
3
2
2
2
1
1
Table 3. Example Report From Heavy Hitters Detector
Macroﬂow Description
Time
1130965527 164.254.132.227:*  *:*