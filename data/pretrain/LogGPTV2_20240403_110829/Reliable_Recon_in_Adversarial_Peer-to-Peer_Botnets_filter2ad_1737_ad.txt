### 6.1 Crawler Detection

#### 6.1.1 Contact Ratio Limiting

The detection results for the GameOver Zeus tests are presented in Figure 2 (detected crawlers) and Table 4 (detection rate vs. false positives). We used a 24-hour request history to detect crawlers that cover a significant fraction of the bot population within this period. In our experiments, we randomly divided our sensors into 8 groups per detection interval. In a full-scale implementation, this would split the Zeus population of 200,000 bots [2] into groups of 25,000 bots (including non-routable bots) per leader. The number of groups is denoted as |G| in Figure 2. We varied the per-group detection threshold, denoted as t, to illustrate the trade-offs between detection accuracy and false positives. This threshold is the percentage of bots per group that a crawler must cover to receive a vote from that group.

We simulated the detection performance for contact ratio-limited crawlers by excluding crawler requests to a varying subset of our sensors (the x-axis in Figure 2). As shown in Figure 2, a per-group detection threshold of 1% allows 28% of crawlers to be detected even if they limit their contact ratio to only 1/128 bots. However, this comes at the cost of 119 false positives (denoted #FP in Table 4), most of which are sets of NATed bots sharing a single IP. Increasing the threshold to 2% reduces the number of false positives to 13 but also limits the detection accuracy to 44% for a 1/32 contact ratio and 6% for a 1/64 contact ratio. In our tests, the ideal threshold is 5%, allowing 100% of crawlers to be detected without false positives if no contact ratio limiting is used in the crawlers. The accuracy degrades gracefully to 50% for crawlers with a 1/16 contact ratio, meaning that stealthy crawlers may contact at most 1 in every 16 bots to evade detection with 50% probability. When increasing the threshold further to 10%, the detection rate degrades to 89% (2 false negatives) if no contact ratio limiting is used, and to 50% when a contact ratio of 1/8 is applied.

#### 6.1.2 Address Distribution

To determine the range of addresses needed by distributed crawlers, we measured the effectiveness of crawler detection using subnet aggregation. Using the ideal detection threshold of 5%, our experiments show that crawlers can be detected with 100% accuracy (no false positives) using subnet aggregation ranging from /32 (per-IP detection) to /20 (aggregation per subnet of 4096 hosts). Starting from a /19 filter, the crawler detector reports 110 false positives, caused by multiple infections within the same subnet. This indicates that to evade detection, address-distributed crawlers for Zeus must take each address from at least a distinct /20 subnet. As shown by our contact ratio experiments, a coverage reduction of a factor of 32 is required for Zeus crawlers to avoid detection, so that the address-distributed crawlers must take their addresses from 32 distinct /20 subnets, or alternatively from a single /16.

#### 6.1.3 Sality Crawler Detection

To complement our Zeus experiments, we attempted to repeat our crawler detection tests in the Sality network. Unfortunately, the peer list size and peer exchange properties of Sality prohibit small-scale tests of our algorithm. In Sality, bots maintain a peer list of 1000 entries and exchange peers based on a responsiveness-based reputation metric, as documented by Falliere [10]. To obtain meaningful evaluation results, it is necessary to run significantly more sensors than fit in a Sality peer list. Otherwise, sensors are highly likely to be present in the peer lists of many legitimate bots, making these bots indistinguishable from crawlers. While our 512 sensor nodes were sufficient to prototype our algorithm for Zeus, where typical bots only maintain around 50 peer list entries [2], a Sality prototype would require several thousand IP addresses, which we were unable to obtain for our experiments. Nevertheless, because Sality’s reputation mechanism and limit of a single entry per peer exchange significantly restrict the out-degree of Sality bots, we expect that Sality crawlers are highly susceptible to full-scale crawler detection (as could be implemented by the Sality botmasters).

### 6.2 Stealthy Crawling Performance

To evaluate the reconnaissance performance of the stealthy crawling techniques proposed in Section 5, we implemented contact ratio and frequency-limited crawlers for both GameOver Zeus and Sality. (We do not evaluate address-distributed crawlers, as these should suffer no degradation in crawling performance.) For both Zeus and Sality, we ran all of the crawling tests in parallel for 24 hours, to ensure that performance differences did not result from churn in the bot population. The contact ratio-limited crawlers only contacted a deterministically restricted fraction of bots, based on the bot identifier. Figure 3 graphs the number of bots detected over time by contact ratio-limited crawlers, while Figure 4 shows the results for frequency-limited crawlers.

#### 6.2.1 Contact Ratio Limiting

As shown in Figure 3 and Table 4, the number of peers found steadily drops as the crawler contact ratio decreases. At a contact ratio of 1/2, crawling performance is still good, finding 80% of the Zeus peers found in a full crawl, and 90% of the Sality peers. However, as can be seen in Figure 2, this contact ratio still allows 89% of the crawlers to be detected. Reducing the contact ratio further causes a rapid decline in completeness. At a contact ratio of 1/4, our crawler finds 74% of Sality peers, and only 52% of Zeus peers. When reducing the contact ratio to 1/16, only 38% of the Zeus bots, and 27% of the Sality bots are found by our crawler, while this contact ratio still leads to a probability of 50% that crawlers are detected. These results show that contact ratio limiting achieves crawler stealthiness only at a high cost in crawling completeness. Note that it is not relevant whether or not our initial crawls reached the full bot population. Rather, the results serve only to show the relative coverage degradation which results from contact ratio limiting.

#### 6.2.2 Frequency Limiting

Figure 4 shows crawling results for aggressive crawling, where the suspend-request cycle used by normal bots is not respected, as well as for crawls adhering to the suspend period between requests. We show results for crawlers using a full suspend cycle (30 minutes for Zeus and 40 minutes for Sality), as well as a half suspend cycle. The results are highly dependent on the protocol of the crawled botnet. For Zeus, even crawlers adhering to a full suspend cycle achieve reasonable efficiency, finding 74% of the bots found by the aggressive crawler. There are two main reasons for this: (1) Zeus returns 10 peers per peer list response, and the peer lists of typical bots contain only around 50 entries. This makes it possible to cover a larger fraction of each bot’s peer list in a single suspend-request cycle than for Sality, which features more stringent constraints. (2) Due to the frequency-based automatic blacklisting mechanism used in Zeus, even our aggressive crawler is rate-limited to avoid being blacklisted. This reduces the gap between the aggressive crawling results and the suspend cycle-adherent results. For Sality, the impact of frequency limiting is severe. Only 11% of the bots are found using a half suspend cycle, and only 7% when adhering to a full suspend cycle. This is because the peer lists of Sality bots contain up to 1000 entries, while only one entry is returned per peer list response. Note that even frequency-limited crawlers are prone to out-degree-based detection. Thus, frequency limiting must always be used in conjunction with out-degree limiting.

### 7. Internet-Wide Scanning

Recent advances have enabled fast Internet-wide scanning in practical tools such as ZMap [9]. These tools work by probing large subsets of the public IP address space to find hosts that have a particular property (e.g., a security vulnerability, OS version, etc.), as evidenced by their response to the probes. Internet-wide scanning has been proposed as an alternative reconnaissance method for finding botnet servers and bots [24]. Additionally, though not strictly necessary, it has been used in practice to discover bots in the ZeroAccess P2P botnet [20].

Table 5 summarizes the susceptibility of various P2P botnets to Internet-wide scanning:

| Botnet     | Fixed Port | Probe Message | Susceptible |
|------------|------------|---------------|-------------|
| Zeus       | ✗          | ✗             | ✓           |
| Sality     | ✗          | ✓             | ✓           |
| ZeroAccess | ✓          | ✓             | ✓           |
| Kelihos/Hlux | ✓          | ✓             | ✓           |
| Waledac    | ✗          | ✗             | ✓           |
| Storm      | ✗          | ✗             | ✓           |

This table indicates that while some botnets like Zeus and Sality are less susceptible to fixed port and probe message scanning, others like ZeroAccess and Kelihos/Hlux are more vulnerable.