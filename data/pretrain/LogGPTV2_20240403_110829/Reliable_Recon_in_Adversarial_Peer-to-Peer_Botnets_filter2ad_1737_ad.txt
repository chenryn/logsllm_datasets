s
r
e
l
w
a
r
c
d
e
t
c
e
t
e
d
%
 100
 90
 80
 70
 60
 50
 40
 30
 20
 10
 0
 1
 2
 4
 8
 16
 32
 64
 128
 256
contact ratio (1/x)
Figure 2: Crawlers detected in 24 hours for |G| = 8 and t ∈
{1%, 2%, 5%}. The contact ratio simulated on the crawler traﬃc
varies over x.
GameOver Zeus takedown. Our sensor nodes logged all re-
ceived requests over a total test period of 24 hours. We chose
a 24-hour period to account for a full diurnal cycle without
suﬀering from duplicate results due to address aliasing caused
by dynamic IP addresses. We then performed our experi-
ments with varying conﬁguration parameters on the logged
traﬃc, to ensure that any detection diﬀerences were a result
of the conﬁguration parameters rather than churn in the bot
and crawler populations. During our test period, 18 of the
crawlers from Table 3 (Section 4.1) were active, which we
used as a ground truth. The rest of this section describes our
measurements of the eﬀectiveness of the crawling techniques
proposed in Section 5 to evade out-degree-based detection:
(1) contact ratio limiting, and (2) address distribution. (We
do not evaluate frequency limiting here as it does not pertain
to the out-degree of crawlers.)
6.1.1 Contact Ratio Limiting
The detection results of our GameOver Zeus tests are
shown in Figure 2 (detected crawlers) and Table 4 (detection
rate vs. false positives). We used a per-bot request history
of 24 hours, meaning that crawlers are detected if they cover
a signiﬁcant fraction of the bot population in 24 hours. In
our experiments, we randomly partitioned our sensors into 8
groups per detection interval. In a full-scale implementation,
this arrangement would split the Zeus population of 200.000
bots [2] into groups of 25.000 bots (including non-routable
bots) per leader. We denote the number of groups as |G|
(the magnitude of the set of groups G) in Figure 2. We
vary the per-group detection threshold, denoted as t in the
ﬁgure, to illustrate the tradeoﬀs between detection accuracy
and false positives. This threshold is the percentage of bots
per group that a crawler must cover to receive a vote from
that group. We simulated the detection performance for
contact ratio-limited crawlers by excluding crawler requests
to a varying subset of our sensors (the x-axis in Figure 2).
As shown in Figure 2, a per-group detection threshold
of 1% allows 28% of crawlers to be detected even if these
limit their contact ratio to only 1/128 bots. However, this
comes at a cost of 119 falsely identiﬁed crawlers (denoted
#FP in Table 4), most of which are actually sets of NATed
bots sharing a single IP. Increasing the threshold to 2%
reduces the number of false positives to 13, but also limits
the detection accuracy to 44% for a 1/32 contact ratio, and
6% for a 1/64 contact ratio. In our tests, the ideal threshold
is 5%, allowing 100% of crawlers to be detected without false
positives if no contact ratio limiting is used in the crawlers.
The accuracy degrades gracefully to 50% for crawlers with
a 1/16 contact ratio, meaning that stealthy crawlers may
contact at most 1 in every 16 bots to evade detection with
50% probability. When increasing the threshold further to
10%, the detection rate degrades to 89% (2 false negatives) if
no contact ratio limiting is used, and to 50% when a contact
ratio of 1/8 is applied.
6.1.2 Address Distribution
To determine the range of addresses needed by distributed
crawlers, we also measured the eﬀectiveness of crawler de-
tection using subnet aggregation. Using the ideal detection
threshold of 5%, our experiments show that crawlers can be
detected with 100% accuracy (no false positives) using sub-
net aggregation ranging from /32 (per-IP detection) to /20
(aggregation per subnet of 4096 hosts). Starting from a /19
ﬁlter, the crawler detector reports 110 false positives, caused
by multiple infections within the same subnet. This indicates
that to evade detection, address-distributed crawlers for Zeus
must take each address from at least a distinct /20 subnet.
As shown by our contact ratio experiments, a coverage re-
duction of a factor 32 is required for Zeus crawlers to avoid
detection, so that the address-distributed crawlers must take
their addresses from 32 distinct /20 subnets, or alternatively
from a single /16.
6.1.3
Sality Crawler Detection
To complement our Zeus experiments, we attempted to
repeat our crawler detection tests in the Sality network.
Unfortunately, the peer list size and peer exchange properties
of Sality prohibit small-scale tests of our algorithm.
In Sality, bots maintain a peer list of 1000 entries, and
exchange peers based on a responsiveness-based reputation
metric, as documented by Falliere [10]. To obtain meaningful
evaluation results, it is necessary to run signiﬁcantly more
sensors than ﬁt in a Sality peer list. Otherwise, sensors are
highly likely to be present in the peer lists of many legitimate
bots, making these bots indistinguishable from crawlers.
While our 512 sensor nodes were suﬃcient to prototype
our algorithm for Zeus, where typical bots only maintain
around 50 peer list entries [2], a Sality prototype would
require several thousand IP addresses, which we were unable
to obtain for our experiments. Nevertheless, because Sality’s
reputation mechanism and limit of a single entry per peer
exchange signiﬁcantly restrict the out-degree of Sality bots,
136 35000
 30000
 25000
s
P
I
t
o
b
 20000
 15000
 10000
 5000
 0
00
 25000
 20000
s
P
I
t
o
b
 15000
 10000
 5000
 0
00
c=  1/1
c=  1/2
c=  1/4
c=  1/8
c=1/16
c=1/32
02
04
06
08
10
12
14
16
18
20
22
00
time (hours)
(a) Zeus.
c=  1/1
c=  1/2
c=  1/4
c=  1/8
c=1/16
c=1/32
02
04
06
08
10
12
14
16
18
20
22
00
time (hours)
(b) Sality.
 45000
 40000
 35000
 30000
 25000
 20000
 15000
 10000
 5000
s
P
I
t
o
b
 0
00
 25000
 20000
s
P
I
t
o
b
 15000
 10000
 5000
 0
00
aggressive
half suspend cycle
full suspend cycle
02
04
06
08
10
12
14
16
18
20
22
00
time (hours)
(a) Zeus.
aggressive
half suspend cycle
full suspend cycle
02
04
06
08
10
12
14
16
18
20
22
00
time (hours)
(b) Sality.
Figure 3: Bots crawled in 24 hours for varying contact ratio.
Figure 4: Bots crawled in 24 hours for varying request frequency.
we expect that Sality crawlers are highly susceptible to full-
scale crawler detection (as could be implemented by the
Sality botmasters).
6.2 Stealthy Crawling Performance
To evaluate the reconnaissance performance of the stealthy
crawling techniques proposed in Section 5, we implemented
contact ratio and frequency-limited crawlers for both GameOver
Zeus and Sality. (We do not evaluate address-distributed
crawlers, as these should suﬀer no degradation in crawling
performance.) For both Zeus and Sality, we ran all of the
crawling tests in parallel for 24 hours, to ensure that per-
formance diﬀerences did not result from churn in the bot
population. The contact ratio-limited crawlers only con-
tacted a deterministically restricted fraction of bots, based
on the bot identiﬁer. Figure 3 graphs the number of bots
detected over time by contact ratio-limited crawlers, while
Figure 4 shows the results for frequency-limited crawlers.
6.2.1 Contact Ratio Limiting
As shown in Figure 3 and Table 4, the number of peers
found steadily drops as the crawler contact ratio decreases.
At a contact ratio of 1/2, crawling performance is still good,
ﬁnding 80% of the Zeus peers found in a full crawl, and 90%
of the Sality peers. However, as can be seen in Figure 2, this
contact ratio still allows 89% of the crawlers to be detected.
Reducing the contact ratio further causes a rapid decline in
completeness. At a contact ratio of 1/4, our crawler ﬁnds 74%
of Sality peers, and only 52% of Zeus peers. When reducing
the contact ratio to 1/16, only 38% of the Zeus bots, and
27% of the Sality bots are found by our crawler, while this
contact ratio still leads to a probability of 50% that crawlers
are detected. These results show that contact ratio limiting
achieves crawler stealthiness only at a high cost in crawling
completeness. Note that it is not relevant whether or not our
initial crawls reached the full bot population. Rather, the
results serve only to show the relative coverage degradation
which results from contact ratio limiting.
6.2.2 Frequency Limiting
Figure 4 shows crawling results for aggressive crawling,
where the suspend-request cycle used by normal bots is not
respected, as well as for crawls adhering to the suspend
period between requests. We show results for crawlers using
a full suspend cycle (30 minutes for Zeus and 40 minutes
for Sality), as well as a half suspend cycle. The results are
highly dependent on the protocol of the crawled botnet. For
Zeus, even crawlers adhering to a full suspend cycle achieve
reasonable eﬃciency, ﬁnding 74% of the bots found by the
aggressive crawler. There are two main reasons for this. (1)
Zeus returns 10 peers per peer list response, and the peer lists
of typical bots contain only around 50 entries. This makes it
possible to cover a larger fraction of each bot’s peer list in a
single suspend-request cycle than for Sality, which features
more stringent constraints. (2) Due to the frequency-based
automatic blacklisting mechanism used in Zeus, even our
aggressive crawler is rate limited to avoid being blacklisted.
This reduces the gap between the aggressive crawling results
and the suspend cycle-adherent results. For Sality, the impact
of frequency limiting is severe. Only 11% of the bots are
found using a half suspend cycle, and only 7% when adhering
to a full suspend cycle. This is because the peer lists of
Sality bots contain up to 1000 entries, while only one entry
is returned per peer list response. Note that even frequency-
limited crawlers are prone to out-degree based detection.
Thus, frequency limiting must always be used in unison with
out-degree limiting.
7.
INTERNET-WIDE SCANNING
Recent advances have enabled fast Internet-wide scanning
in practical tools such as ZMap [9]. These tools work by
probing large subsets of the public IP address space, in
order to ﬁnd hosts that have a particular property (i.e., a
security vulnerability, OS version, etc.), as evidenced by
their response to the probes. Internet-wide scanning has
been proposed as an alternative reconnaissance method for
ﬁnding botnet servers and bots [24]. Additionally (though
137Fixed port Probe msg
Susceptible
Zeus
Sality
ZeroAccess
Kelihos/Hlux
Waledac
Storm
✗
✗
✓
✓
✗
✗
✗
✓
✓
✓
✓
✓
✗
✗
✓
✓
✗
✗
Table 5: Susceptibility of P2P botnets to Internet-wide scanning.
not strictly necessary), it has been used in practice to discover
bots in the ZeroAccess P2P botnet [20].