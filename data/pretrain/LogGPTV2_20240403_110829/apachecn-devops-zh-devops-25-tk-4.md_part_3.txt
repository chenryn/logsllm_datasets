 2      kubernetes_name="go-demo-5"
 3  } /
 4  http_server_resp_time_count{
 5      kubernetes_name="go-demo-5"
 6  }
```
这个表达本身应该很容易理解。我们将所有请求的总和除以计数。由于我们已经发现问题在`go-demo-5`应用内部的某个地方，所以我们使用了`kubernetes_name`标签来限制结果。尽管这是目前在我们的集群中运行的唯一一个具有该度量的应用，但是当我们将检测扩展到其他应用时，习惯于在稍后的某个日期可能会有其他应用这一事实是一个好主意。
我们可以看到，平均请求持续时间增加了一段时间，但一段时间后又下降到接近初始值。这一峰值与我们不久前发送的 20 个慢速请求不谋而合。在我的例子中(截图如下)，峰值接近 0.1 秒的平均响应时间，但不久后下降到 0.02 秒左右。
![](img/e4efb795-db45-4d39-87be-b5982c518722.png)
Figure 4-5: The graph with the cumulative average response time
请注意，我们刚刚执行的表达式存在严重缺陷。它显示累计平均响应时间，而不是显示`rate`。但是，你已经知道了。这只是仪表化度量的一种尝试，而不是它的“真实”用法(即将到来)。
你可能会注意到，即使是峰值也很低。这肯定低于我们通过`curl`仅发送 20 个慢速请求的预期。原因在于，提出这些请求的不仅仅是我们。`readinessProbe`和`livenessProbe`也在发送请求，速度非常快。与前一章中我们只测量通过入口的请求不同，这次我们捕获了进入应用的所有请求，包括健康检查。
现在我们已经看到了一些在我们的`go-demo-5`应用中生成的`http_server_resp_time`度量的例子，我们可以使用这些知识来尝试调试导致我们使用仪器的模拟问题。
# 使用内部指标调试潜在问题
我们将再次发送响应缓慢的请求，这样我们就可以回到本章开始的地方。
```
 1  for i in {1..20}; do
 2      DELAY=$[ $RANDOM % 10000 ]
 3      curl "http://$GD5_ADDR/demo/hello?delay=$DELAY"
 4  done
 5
 6  open "http://$PROM_ADDR/alerts"
```
我们发送了二十个请求，这些请求将导致随机持续时间(最多十秒)的响应。进一步，我们打开了普罗米修斯的警报屏幕。
过一会儿`AppTooSlow`警报应该会响起(记得刷新屏幕)，我们有一个(模拟的)问题需要解决。在我们开始惊慌失措和仓促行事之前，我们会努力找到问题的原因。
请点击`AppTooSlow`提醒的表述。
我们被重定向到带有来自警报的预填充表达式的图形屏幕。请随意点击表达式按钮，即使它不会提供任何额外的信息，除了应用很快，然后由于某种莫名其妙的原因变慢的事实。
您将无法从该表达式中收集更多细节。您不会知道它是否在所有方法上都很慢，是否只有一个特定的路径响应很慢，也不会知道任何其他特定于应用的细节。简单来说，`nginx_ingress_controller_request_duration_seconds`这个指标太泛了。它很好地通知了我们应用的响应时间增加了，但是它没有提供关于问题原因的足够信息。为此，我们将切换到普罗米修斯直接从`go-demo-5`复制品中检索的`http_server_resp_time`度量。
请键入下面的表达式，然后按“执行”按钮。
```
 1  sum(rate(
 2      http_server_resp_time_bucket{
 3          le="0.1",
 4          kubernetes_name="go-demo-5"
 5      }[5m]
 6  )) /
 7  sum(rate(
 8      http_server_resp_time_count{
 9          kubernetes_name="go-demo-5"
10      }[5m]
11  ))
```
切换到*图形*选项卡，如果你还没有到那里。
该表达式与我们之前使用`nginx_ingress_controller_request_duration_seconds_sum`度量时编写的查询非常相似。我们将 0.1 秒时段内的请求速率除以所有请求的速率。
在我的例子中(下面的截图)，我们可以看到快速响应的百分比下降了两倍。这与我们之前发送的模拟慢速请求相吻合。
![](img/ab8b8a7e-6ecd-45ec-a77a-3dd0ffd7cfdd.png)
Figure 4-6: The graph with the percentage of fast requests measured with instrumented metrics
到目前为止，与`nginx_ingress_controller_request_duration_seconds_sum`相比，使用仪表化指标`http_server_resp_time_count`并没有带来任何明显的好处。如果这就是全部，我们可以得出结论，增加仪器的努力是一种浪费。然而，我们还没有在表达式中包含标签。
假设我们想按`method`和`path`对请求进行分组。这可以让我们更好地了解缓慢是全球性的，还是仅限于特定类型的请求。如果是后者，我们将知道问题出在哪里，并有望迅速找到罪魁祸首。
请键入下面的表达式，然后按“执行”按钮。
```
 1  sum(rate(
 2      http_server_resp_time_bucket{
 3          le="0.1",
 4          kubernetes_name="go-demo-5"
 5      }[5m]
 6  ))
 7  by (method, path) /
 8  sum(rate(
 9      http_server_resp_time_count{
10          kubernetes_name="go-demo-5"
11      }[5m]
12  ))
13  by (method, path)
```
那个表情和之前那个几乎一样。唯一不同的是增加了`by (method, path)`语句。结果，我们得到了一定比例的快速响应，按`method`和`path`分组。
输出并不代表“真实”的世界用例。通常，我们会看到许多不同的行，每一行对应于所请求的方法和路径。但是，因为我们只使用 HTTP GET 向`/demo/hello`发出请求，所以我们的图有点无聊。你必须想象还有很多其他的台词。
通过研究该图，我们发现除了一条线(我们仍在想象许多)之外，所有的线都接近 100%的快速响应。下降幅度最大的是`/demo/hello`路径和`GET`方法。然而，如果这真的是一个真实的场景，我们可能会有太多的线在图中，我们可能无法轻松区分它们。我们的表达可以受益于一个额外的门槛。
请键入下面的表达式，然后按“执行”按钮。
```
 1  sum(rate(
 2      http_server_resp_time_bucket{
 3          le="0.1",
 4          kubernetes_name="go-demo-5"
 5      }[5m]
 6  ))
 7  by (method, path) /
 8  sum(rate(
 9      http_server_resp_time_count{
10          kubernetes_name="go-demo-5"
11      }[5m]
12  ))
13  by (method, path) < 0.99
```
唯一增加的是`< 0.99`阈值。因此，我们的图表排除了所有的结果(所有的路径和方法)，但低于 99%(0.99)的结果除外。我们消除了所有噪音，只关注超过 1%的请求速度较慢(或者只有不到 99%的请求速度较快)的情况。结果现在很清楚了。问题出在处理路径`/demo/hello`上`GET`请求的函数上。通过图表下方的标签我们知道。
![](img/f862b0b0-9ab7-4d89-93f9-4349c9692d69.png)
Figure 4-7: The graph with the percentage of fast requests measured with instrumented metrics limited to results below ninety-nine percent
现在我们知道了(几乎)问题的确切位置，剩下的就是修复问题，将更改推送到我们的 Git 存储库中，并等待我们的持续部署过程用新版本升级软件。
在相对较短的时间内，我们设法发现(调试)了问题，或者更准确地说，将问题缩小到代码的特定部分。
或者，也许我们发现问题不在代码中，而是我们的应用需要扩展。在这两种情况下，如果没有仪表化的度量，我们只会知道应用很慢，这可能意味着应用的任何部分都有问题。工具给了我们更详细的度量标准，我们过去更精确，减少了我们发现问题并采取相应行动所需的时间。
通常，我们会有许多其他的测量指标，我们的“调试”过程会更加复杂。我们会执行其他表达式，并挖掘不同的指标。然而，重点是我们应该将通用指标与直接来自应用的更详细的指标结合起来。前者通常用于检测问题，而后者在寻找问题原因时非常有用。这两种类型的度量在监控、警报和调试我们的集群和应用中都有它们的位置。通过仪表化度量，我们有了更多特定于应用的细节。这使我们能够缩小问题的位置和原因。我们对问题的确切原因越有信心，我们就越有能力做出反应。
# 现在怎么办？
我不认为我们需要许多其他的仪表化度量的例子。它们和我们通过出口商收集的没有任何不同。我会让你自己开始测试你的应用。从小处着手，看看哪些行之有效，改进和扩展。
又一章结束了。摧毁你的集群，重新开始下一个集群，或者保留它。如果选择后者，请执行以下命令删除`go-demo-5`应用。
```
 1  helm delete go-demo-5 --purge
 2
 3  kubectl delete ns go-demo-5
```
在你离开之前，记住下面的要点。它总结了仪器仪表。
*   仪表化的度量被烘焙到应用中。它们是我们应用代码不可分割的一部分，通常通过`/metrics`端点公开。