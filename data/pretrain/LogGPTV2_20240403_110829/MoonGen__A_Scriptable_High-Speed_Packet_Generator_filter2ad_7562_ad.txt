cal wiring as the timestamping clock is synchronized to the
physical layer. Two ports on diﬀerent X540-based NICs that
are directly connected do not exhibit any clock drift while
the link is established. However, the clocks of two ports
on the same X540 NIC will drift if they are connected to
two diﬀerent NICs. We measured the drift between diﬀer-
ent X540 and 82599 NICs. The worst-case observed drift
was 35 µs per second between a NIC on the mainboard and
a discrete NIC.
MoonGen handles clock drift by resynchronizing the clocks
before a timestamped packet is sent, so this drift translates
to a relative error of only 0.0035%. This is not signiﬁcant for
latency measurements. Since the measurements show a con-
stant clock drift, it would also be possible to subtract the
accumulated drift from the acquired timestamps to avoid
resynchronization.
6.4 Limitations
Our approach for latency measurements comes with lim-
itations. The latency measurements are restricted to Eth-
ernet frames with the PTP EtherType and UDP packets.
MoonGen cannot measure latencies of other protocols.
The na¨ıve handling of clock drift by resynchronizing the
clocks for each packet allows for only a single timestamped
packet in ﬂight,
limiting the throughput to 1 P kt/RT T .
MoonGen scripts therefore use two transmission queues, one
that sends timestamped packets and one that sends regular
packets. The regular packets can be crafted such that the
device under test cannot distinguish them from the time-
stamped packets, e.g., by setting the PTP type in the pay-
load to a value that is not timestamped by the NIC. So
MoonGen eﬀectively samples random packets in the data
stream and timestamps them. Note that the benchmarking
standard RFC 2544 calls for only one timestamped packet
in a 120 second interval [3]. MoonGen can timestamp sev-
eral thousands of packets per second to calculate average
latencies and histograms.
The investigated NICs refuse to timestamp UDP PTP
packets that are smaller than the expected packet size of
80 bytes. Larger packets are timestamped properly. This
restriction does not apply to packets with the PTP Ether-
Type as the minimum PTP packet size is below 64 bytes in
this conﬁguration. Measurements of inter-arrival times are
restricted to GbE networks due to lack of hardware support
for timestamping in line rate on 10 GbE NICs.
Based on the discussed measurement results and despite
these limitations, we argue that special-purpose hardware is
not necessary to conduct highly precise and accurate latency
and inter-arrival time measurements.
7. RATE CONTROL
An important feature of a packet generator is controlling
the packet rate and generating speciﬁc timing patterns to
simulate real-world scenarios. MoonGen utilizes hardware
rate control features of Intel NICs to generate constant bit
rate and bursty traﬃc. We also implement a novel software-
based rate control for realistic traﬃc patterns, e.g., based
on a Poisson process. That is discussed further in Section 8,
this section focuses on software rate control in other packet
generators and hardware rate control.
282Loadgen
p5
NIC
DuT
NIC
Loadgen
p9
NIC
HW rate control
enabled
DuT
NIC
p5
p4
p3
p2
p1 p0
p9 p8 p7
p6 p5
p4
p3
p2
p1 p0
Qmemory
QN IC
Wire
Qmemory
QN IC
Wire
Figure 5: Software-based rate control
7.1 Software Rate Control in Existing Packet
Generators
Trying to control the timing between packets in software
is known to be error-prone [2, 4]. The main problem with
software-based rate control is that the software needs to push
individual packets to the NIC and then has to wait for the
NIC to transmit it before pushing the next packet.
However, modern NICs do not work that way: they rely
on an asynchronous push-pull model and not on a pure push
model. The software writes the packets into a queue that
resides in the main memory and informs the NIC that new
packets are available. It is up to the NIC to fetch the pack-
ets asynchronously via DMA and store them in the internal
transmit queue on the NIC before transmitting them. A
good explanation of this packet ﬂow can be found in the
datasheet of the X540 chip [13] (Section 1.7), other NICs
follow a similar process.
Figure 5 visualizes this packet ﬂow. Only a single packet
at a time is allowed in the queues (Qmemory & QN IC ) to
generate packets that are not back-to-back on the wire.
This hardware architecture causes two problems: the ex-
act timing when a packet is retrieved from memory cannot
be controlled by the software and queues cannot be used
(unless bursts are desired). The former results in a low pre-
cision, as the exact time when the packet is transferred can-
not be determined. The latter impacts the performance at
high packet rates as high-speed packet processing relies on
batch processing [6, 23].
7.2 Hardware Rate Control
Intel 10 GbE NICs feature hardware rate control: all trans-
mit queues can be conﬁgured to a speciﬁed rate. The NIC
then generates constant bit-rate (CBR) traﬃc. This solves
the two problems identiﬁed in the previous section. The
software can keep all available queues completely ﬁlled and
the generated timing is up to the NIC. Figure 6 shows this
architecture. The disadvantage is that this approach is lim-
ited to CBR traﬃc and bursty traﬃc (by changing the rate
parameter periodically).
7.3 Evaluation
We compare our hardware-assisted solution to the soft-
ware-based rate control found in zsend 6.0.2 (an example ap-
plication of the PF RING ZC framework [18]), and Pktgen-
DPDK 2.5.1 [27] to quantify the adverse eﬀects of software-
based rate control. We use an Intel 82580 GbE controller,
which is able to timestamp arbitrary received packets in line
rate (cf. Section 6) to measure inter-arrival times.
Figure 6: Hardware-based rate control
·105
]
z
H
[
e
t
a
R
t
p
u
r
r
e
t
n
I
1.5
1
0.5
0
0
Load generated with MoonGen
Load generated with zsend
0.5
1
1.5
Oﬀered Load [Mpps]
2
Figure 7: Interrupt rate with micro-bursts
Figure 8 on the next page compares the inter-arrival times
generated by diﬀerent packet generators. The generators use
an X540 NIC, which also supports 1 Gbit/s. The histograms
have a bin size of 64 ns (precision of the 82580 chip) and were
generated by observing at least 1 000 000 packets.
Traﬃc from the hardware rate-controlled NIC oscillates
around the targeted inter-arrival time by up to 256 ns and
it avoids generating bursts almost completely (inter-arrival
time of 672 ns, marked with a black arrow in Figure 8). Ta-
ble 4 summarizes the results. The best result in each column
is highlighted.
We discussed these ﬁndings with the authors of zsend as
we conﬁgured it explicitly to avoid bursts. We then tested
several suggested conﬁgurations and versions. We concluded
that these observations indicate a bug in the PF RING ZC
framework that is being investigated.
It stands to reason that the precision problems as well as
the micro-bursts intensify further at rates beyond 1 Gbit/s
with software-based rate control. Measuring inter-arrival
times on 10 GbE is a challenging task: Reliable measure-
ments require special-purpose hardware. We do not yet
have such hardware. We predict that the precision of our
hardware-assisted approach will improve at 10 GbE speeds:
The frequency of the internal clock on the NIC that controls
the inter-departure times is scaled up by a factor of 10 when
operating at 10 GbE compared to GbE [13].
7.4 Effects of Micro-Bursts on Linux Systems
Figure 7 visualizes the interrupt rate on a Linux packet
forwarder running Open vSwitch under increasing load gen-
erated by MoonGen and zsend. Open vSwitch was conﬁg-
ured with a static OpenFlow rule to forward between two
ports. The micro-bursts generate a low interrupt rate. The
likely explanation for this is that the bursts trigger the in-
terrupt rate moderation feature of the driver earlier than
expected. This shows that bad rate control can have a mea-
surable impact on the behavior of the tested system.
283Rate
500 kpps Pktgen-DPDK
Packet Generator Micro-Bursts ±64 ns ±128 ns ±256 ns ±512 ns
0.02% 49.9% 74.9% 99.8% 99.8%
MoonGen
0.01% 37.7% 72.3%
92% 94.5%
6.4% 13.8%
5.4%
28.6%
zsend
3.9%
MoonGen
1000 kpps Pktgen-DPDK
zsend
1.2% 50.5%
14.2% 36.7%
4.6%
52%
52%
97% 100%
58% 70.6% 95.9%
7.9% 24.2% 88.1%
Table 4: Rate control measurements
MoonGen
Pktgen-DPDK
zsend
]
%
[
y
t
i
l
i
b
a
b
o
r
P
30
15
0
30
15
0
60
30
MoonGen
Pktgen-DPDK
zsend
2
1.5
3
Inter-Arrival Time [µs]
2.5
(a) 500 kpps
3.5
4
0
0.5
1
1.5
2
Inter-Arrival Time [µs]
(b) 1000 kpps
]
%
[
y
t
i
l
i
b
a
b
o
r
P
60
30
0
30
15
0
30
15
0
0.5
1
Figure 8: Histograms of inter-arrival times
7.5 Limitations of Hardware Rate Control
In our tests we encountered unpredictable non-linear be-
havior with packet rates above ∼9 Mpps (∼6 Gbit/s wire-
rate with 64 byte packets) on Intel X520 and X540 NICs.
A work-around is conﬁguring two transmission queues and
sending a CBR stream from both of them. Note that this
is not equivalent to a single transmission queue with proper
rate control as both queues control their transmission rate
independently from each other.
Hardware rate control of the investigated NICs is also
restricted to CBR traﬃc, so MoonGen still needs an im-
plementation of software-based rate control for other traﬃc
patterns.
8. CONTROLLING INTER-PACKET GAPS
IN SOFTWARE
To overcome this restriction to constant bit rate or bursty
traﬃc, MoonGen implements a novel mechanism for soft-
ware-based rate control. This allows MoonGen to create
arbitrary traﬃc patterns.
8.1 Sending Gaps on the Wire
We were not satisﬁed with the precision of existing soft-
ware rate control mechanisms (cf. Section 7.3 and [2, 4]) so
we present a new method here. All existing packet genera-
tors try to delay sending packets by not sending packets for
a speciﬁed time, leading to the previously mentioned prob-
lems. MoonGen ﬁlls the gaps between packets with invalid
packets instead. Varying the length of the invalid packet
precisely determines the time between any two packets and
subsequently allows the creation of arbitrary complex traﬃc
Loadgen
p6
NIC
HW rate control
disabled
DuT
NIC
p6
pi
4
p5
p5
pi
3
p4
pi
2
p3
pi
1
p2
pi
0
p1 p0
Qmemory
QN IC
Wire
Figure 9: Precise generation of arbitrary traﬃc pat-
terns in MoonGen
patterns. With this technique, we can still make use of the
NIC’s queues and do not have to rely on any timing related
to DMA accesses by the NIC.
This approach requires support by the device under test
(DuT): it needs to detect and ignore invalid packets in hard-
ware without aﬀecting the packet processing logic. Moon-
Gen uses packets with an incorrect CRC checksum and, if
necessary, an illegal length for short gaps. All investigated