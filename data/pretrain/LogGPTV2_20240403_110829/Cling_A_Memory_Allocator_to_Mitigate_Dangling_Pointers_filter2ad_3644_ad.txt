10,595
7,701
64,086
9,145
895
44,959
268
2,161
118,205
419,724
103,390
166,317
787,523,051
0
30,704,104
4,374,712
492,722
621,109
2,405,928
151,324,610
13,959,978
168,631
267,101,325
190,984
2,413,942
4,797,794
135,037,352
16,463
42
4,922
46,532
0
32,732
26,373
5
0
10,595
7,701
63,910
9,142
895
44,959
268
2,161
118,205
3,877,784
22,579,058
77,711
464,565
3,877,783
22,255,963
77,711
464536
Table 1: Memory allocation sites and requests in benchmarks and Firefox browser.
option to direct a continuously running Firefox instance
under measurement to a new web site every 10 seconds.
We report memory consumption using information ob-
tained through the /proc/self/status Linux inter-
face. When reporting physical memory consumption, the
sum of the VmRSS and VmPTE ﬁelds is used. The lat-
ter measures the size of the page tables used by the pro-
cess, which increases with Cling due to the larger address
space. In most cases, however, it was still very small in
absolute value. The VmSize ﬁeld is used to measure
address space size. The VmPeak and VmHWM ﬁelds are
used to obtain peak values for the VmSize and VmRSS
ﬁelds respectively.
The reported CPU times are averages over three runs
with small variance. CPU times are not reported for Fire-
fox, because the experiment was IO bound with signiﬁ-
cant variance.
4.2 Benchmark Characterization
Figures 7–10 illustrate the size distribution of alloca-
tion requests made by any given benchmark running with
their respective input data. We observe that most bench-
marks request a wide range of allocation sizes, but the
gcc benchmark that uses a custom allocator mostly re-
quests memory in chunks of 4K.
Table 1 provides information on the number of static
allocation sites in the benchmarks and the absolute num-
ber of allocation and deallocation requests at runtime.
For allocation sites, the ﬁrst column is the number of al-
location sites that are not wrappers, the second column is
the number of allocation sites that are presumed to be in
allocation routine wrappers (such as safe_malloc in
twolf, my_malloc in vpr, and xmalloc in gcc),
and the third column is the number of call sites of these
wrappers, that have to be unwound. We observe that
Firefox has an order of magnitude more allocation sites
than the rest.
The number of allocation and deallocation requests for
small (less than 8K) and large allocations are reported
separately. The vast majority of allocation requests are
for small objects and thus the performance of the bucket
allocation scheme is crucial.
In fact, no attempt was
made to optimize large allocations in this work.
4.3 Results
Table 2 tabulates the results of our performance measure-
ments. We observe that the runtime overhead is modest
even for programs with a higher rate of allocation and
deallocation requests. With the exception of espresso
(16%), parser (12%), and dealII (8%), the over-
head is less than 2%. Many other benchmarks with few
allocation and deallocation requests, not presented here,
have even less overhead—an interesting beneﬁt of this
approach, which, unlike solutions interposing on mem-
ory accesses, does not tax programs not making heavy
use of dynamic memory.
In fact, many benchmarks with a signiﬁcant num-
ber of allocations run faster with Cling. For example
xalancbmk, a notorious allocator abuser, runs 25%
faster. In many cases we observed that by tuning allo-
Benchmark
Orig. (Sec.)
Pools
Cling Ratio
No
Unwind
No
Pools
Orig. (MiB)
Pools
Execution time
Peak memory usage
Peak VM usage
Cling Ratio
No
Unwind
No
Pools
Orig. (MiB)
Cling Ratio
Pools
CPU2000
gzip
vpr
gcc
parser
equake
perlbmk
vortex
twolf
CPU2006
gobmk
hmmer
dealII
sphinx3
h264ref
omnetpp
soplex
povray
astar
xalancbmk
Other
espresso
95.7
76.5
43.29
152.6
47.3
68.18
72.19
101.31
628.6
542.15
476.74
1143.6
934.71
573.7
524.01
272.54
656.09
421.03
25.21
1.00
1.00
1.01
1.12
0.98
1.02
0.99
1.01
1.00
1.02
1.08
1.00
1.00
0.83
1.01
1.00
0.93
0.75
1.16
1.00
0.99
1.01
1.08
1.00
0.99
0.99
1.00
1.0
1.02
1.07
1.00
1.01
0.83
1.01
1.00
0.93
0.75
1.07
1.00
0.99
1.01
1.05
0.99
1.00
0.99
1.00
1.00
1.01
1.06
0.99
1.01
0.87
1.01
0.99
0.92
0.77
1.10
181.91
48.01
157.05
21.43
49.85
132.47
73.09
6.85
28.96
25.75
793.39
43.45
64.54
169.58
421.8
4.79
325.77
419.93
4.63
1.00
1.06
0.98
1.14
0.99
0.96
0.91
0.93
1.01
1.02
1.02
1.01
0.97
0.97
1.27
1.33
0.94
1.03
1.13
1.00
1.06
0.98
1.13
0.99
0.95
0.91
0.91
1.00
1.01
1.02
1.01
0.97
0.97
1.27
1.33
0.94
1.03
1.06
1.00
1.06
0.98
1.05
0.99
0.95
0.91
0.90
1.00
1.01
1.02
1.01
0.96
0.97
1.27
1.29
0.94
1.14
1.02
Table 2: Experimental evaluation results for the benchmarks.
196.39
62.63
171.42
35.99
64.16
146.69
88.18
21.15
44.69
40.31
809.46
59.93
80.18
183.45
639.51
34.1
345.51
436.54
1.10
1.54
1.21
2.26
1.14
1.16
1.74
1.19
1.64
1.79
1.70
1.37
1.52
1.03
2.31
0.77
1.56
1.45
19.36
2.08
cator parameters such as the block size and the length of
the hot bucket queue, we were able to trade memory for
speed and vice versa. In particular, with different block
sizes, xalancbmk would run twice as fast, but with a
memory overhead around 40%.
In order to factor out the effects of allocator design