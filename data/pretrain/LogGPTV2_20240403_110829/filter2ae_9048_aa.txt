> 先知原创作者翻译：[原文链接](https://vincentyiu.co.uk/offensivesplunk/ "原文链接")
摘要：在进行攻击性安全数据分析的时候，Splunk在数据的浏览和分析方面，要比传统的Grep更具优势。
## 为什么选择Splunk而非ELK？
ELK本身就是一个非常优秀的开源项目，如果辅之以[Cyb3rward0g](https://twitter.com/Cyb3rWard0g
"Cyb3rward0g")的[HELK项目](https://github.com/Cyb3rWard0g/HELK
"HELK项目")的话，在易用性方面将更上一层楼。事实上，在我“归顺”Splunk之前，已经用过ELK一段时间了。使用ELK时，我意识到必须事先创建一个.config文件来指定数据的位置，然后再上传数据。这给我带来了许多不便，因为这意味着，对于每个小数据集，都需要指出相应的头部和数据类型。然而，Splunk只需简单地点击Web
UI并上传CSV文件，数据上传的工作就能够轻松搞定。
我知道，很多渗透测试团队都在使用ELK进行日志记录和分析工作，关于这方面的详细介绍，我会在另一篇文章中加以介绍。ELK通常不适用于“快餐式”PoC环境，这种环境一般用于评估需要哪些资源来创建一个进攻性的分析系统，以及这样做会得到什么好处。在这篇文章中，我们将重点介绍如何使用Splunk作为日志分析系统来实现数据的快速可视化和搜索。
## 安装Splunk
安装过程非常简单，读者可以访问Splunk的网站，然后下载适用于Windows的MSI软件包，或根据自己的操作系统下载相应的软件包。我使用的电脑具有20GB
RAM和300GB SSD硬盘的系统，该软件在这台电脑上面的运行效果不错；当然，内存少一点也没关系，因为Splunk看起来不太吃内存。
我们建议读者使用开发人员许可证，该许可证具有6个月的试用期，在这段时间内，读者可以通过Splunk为所需数据编制索引。实际上，索引的过程只是将数据导入数据库并对其进行优化，以便于进行搜索。
**数据摄入**
是的，这个工作的确很轻松：你甚至可以通过上传方式来直接获取.json.gz文件。不过，由于Project
Sonar文件实在太大，所以，我使用命令splunk add oneshot
sonar.json.gz将其加载到Splunk中。虽然获取数据通常需要花费一点时间，但一旦完成，搜索就会快如闪电。
如果数据小于500MB的话，我们甚至可以使用Web UI完成数据的上传：
**Project Sonar**
为了演示如何应用Project Sonar的Forward DNS数据，我决定考察一下Splunk在聚合和理解数据方面的能力。
**内容分发网络**
查找形如*.cloudfront.net的域名。
常规的grep命令：
    :/$ time zcat fdns_a.json.gz | grep '\.cloudfront\.net' | tee -a cloudfront.net.txt
    real    10m25.282s
    user    9m16.984s
    sys     1m23.031s
使用Splunk进行相应的搜索：
    value="*.cloudfront.net"
    47.63 seconds
使用grep命令完成上述搜索所花费的时间，是使用Splunk所需时间的13.13倍。
## 域搜索
获取特定域的所有子域：
这个过程几乎是瞬间完成的。如果使用常规的grep命令的话，这个过程通常需要10分钟左右。然而更加有趣的是，我们还可以利用Splunk的分析功能执行相关操作来查找诸如“多少个域共享同一个主机？”等信息，例如：
这将得到如下所示结果：
从上图可以看出，右侧的所有主机名都指向了一台特定的服务器。
除此之外，我们也可以映射服务器的物理位置，例如：
    name="*.uber.com" | stats values(name) by value | iplocation value | geostats count by City
这将得到如下所示结果：
当然，有时这样做可能没有多大用处，但的确却可以立刻获悉目标服务器的地理位置。
如果需要对攻击目标组织在特定国家/地区内的服务器进行精确打击的话，则可以使用Splunk来进行相应的过滤：
    name="*.uber.com" | stats values(name) by value | iplocation value | search Country="United States"
如果有必要的话，甚至可以将范围缩小至某个具体的国家。
## 获取所有子域
此外，我们也可以执行以下搜索来获取所有子域。当然，解析14亿个结果可能需要花费太多时间，所以，这里仅仅以一个域为例进行说明：
    index=main [search name="*.uber.com"] 
    | table name
    | eval url=$name$ | lookup ut_parse_extended_lookup url
或搜索特定的子域：
    index=main [search name="*.uber.com"] 
    | table name
    | eval url=$name$ | lookup ut_parse_extended_lookup url 
    | table ut_subdomain 
    | dedup ut_subdomain
当需要从目标组织下属域名中搜索相同的子域以寻找更多子域的时候，这种方法会很有帮助。
## DomLink domains-> Give me all subdomains
如果结合我的工具[DomLink](http://https//github.com/vysec/DomLink
"DomLink")一起使用的话，我们就可以获取相应的搜索结果，并让Splunk为我们提供隶属于目标组织的所有子域的完整清单。两者结合之后，这会变得轻而易举。
首先运行该工具，然后使用命令行标志将结果输出到指定的文本文件中：
现在，我们已经获取了一个域列表，位于我们的输出文件中，这样，就可以读取这些域名，并创建一个以name开头的新文件，然后使用简单的正则表达式，通过*.替换域名的第一个字段：
经过上面的处理之后，我们的文件内容将会变成下面这样：
将这个文件保存到C:\Program
Files\Splunk\etc\system\lookups目录中，并将其命名为Book1.csv。然后，执行以下搜索操作：
    index=main [inputlookup Book1.csv] | table name
结果如下所示：
然后，我们就可以导出这些结果，继而将其导入其他工具中了。
## 密码转储
这里使用的是LeakBase
BreachCompilation的数据，同时，为了将数据导入Splunk，我们还参考这篇文章，利用Elk对Outflank的密码转储数据的格式进行了相应的处理。通过运行其中提供的脚本，就可以方便地将转储数据转换为以空格符分隔的文件格式，这实在是太方便了：
我对该脚本稍作修改，使其将相应结果输出到磁盘，而不是直接推送到ELK/Splunk。
这一点对于我们来说非常重要。这是因为，Splunk无法接收空格符分隔的文件，因此，我们必须修改Outflank提供的脚本，将相应的格式转换为可导入的CSV格式。
为此，只需执行splunk add oneshot input.csv -index passwords -sourcetype csv -hostname
passwords -auth "admin:changeme"，然后就ok了！
好了，让我们看看最常用的密码到底是什么！
    index=passwords 
    |  stats count by password 
    |  sort 100 -count
如果对基础词汇感兴趣的话，可以使用模糊匹配技术，例如：
    index=passwords
    | table password 
    | fuzzy wordlist="password" type="simple" compare_field="password" output_prefix="fuzz"
    | where fuzzmax_match_ratio >= 67
    | stats count by password