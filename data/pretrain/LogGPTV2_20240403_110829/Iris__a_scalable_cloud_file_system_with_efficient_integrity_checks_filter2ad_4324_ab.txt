prohibitive for the system to be usable.
Existing network ﬁle systems are not designed with similar re-
quirements in mind. For instance, NFS is not optimized for high
network latency scenarios [15]. Moreover, most cloud storage sys-
tems available today (e.g., Amazon S3) export a key-value store
interface and employ a ﬂat namespace. Our system is unique in
providing a ﬁle system interface to enterprise clients (for compat-
ibility with existing applications), and at the same time ensuring
low operation latency. In addition, our main goal is to support in-
tegrity protection of both ﬁle system data and meta-data and con-
tinuous veriﬁcation of full ﬁle system correctness and availability
with minimum overhead.
We describe here Iris’s architecture, threat model, and give an
overview of our solution and technical challenges.
3.1 System architecture
In our architecture (shown in Figure 1), a trusted portal residing
within the enterprise trust boundary intermediates all communica-
tion between enterprise clients and the cloud. The portal caches
data and meta-data blocks recently accessed by enterprise clients.
Cached blocks are evicted once the cache is full and they are not
utilized by a pending operation. The portal is also responsible for
checking data integrity and freshness for all ﬁle system operations
(with the integrity layer component). Data integrity ensures that
data retrieved from the cloud has been written by authorized clients
and has not been accidentally modiﬁed or corrupted at the cloud
side. A stronger property, data freshness, ensures that data accessed
by a client during a ﬁle system operation is always the latest version
written to the cloud by any client.
The portal offers a portal service to clients issuing ﬁle system
operations, and communicates to the cloud through the storage in-
terface component. The auditing component issues challenges to
the cloud periodically to verify the correctness and availability of
the entire ﬁle system. The portal plays a central role in recovering
from data corruptions: The portal caches error-correcting informa-
tion (or more concisely, parities) for the full ﬁle system. When
corruption is detected through the auditing protocol, these parities
enable recovery of lost or corrupted data. Parities are backed up to
the cloud on a regular basis (e.g., once a day or once a week).
To scale to large organizations with tens of thousands of clients,
the portal needs to be distributed internally using a tool to ensure
consistency of distributed caches (e.g., memcached [3]). For pur-
poses of our prototype detailed in Section 6, we have instantiated
the portal on a single server machine and show that it can scale
up to 100 clients simultaneously executing sequential workloads in
parallel on the ﬁle system.
The cloud maintains the distributed ﬁle system, consisting of all
ﬁles and directories belonging to enterprise users. Iris is designed
to use any existing cloud storage system transparently in the back
end without modiﬁcation.
In addition, the cloud also stores the
MACs and Merkle tree necessary for authenticating data, as well
as the checkpointed parity information needed to recover from po-
tential corruptions at the portal. As an additional resilience mea-
231
sure, the parity information could be stored on a different cloud or
replicated internally within the enterprise.
Figure 1: System architecture.
3.2 Threat model
Iris treats the portal, which is controlled by the enterprise, as a
trusted component, in the sense that it executes client ﬁle system
operations faithfully. No trust assumption is required on clients:
They may act arbitrarily within the parameters of the ﬁle system.
(The ﬁle system may enforce access-control policies on clients through
the portal, but such issues lie outside the scope of Iris.)
The cloud, on the other hand, is presumed to be potentially un-
trustworthy.
It may corrupt the ﬁle system in a fully Byzantine
manner. The cloud may alter or drop ﬁle system operations trans-
mitted by the portal; it may corrupt or erase ﬁles and/or metadata;
it may also attempt to present the portal with stale, incorrect, and/or
inconsistent views of ﬁle system data. The objective of the portal
in Iris is to detect the presentation of any invalid data by the cloud,
i.e., immediately identify any cloud output that reﬂects a ﬁle sys-
tem state different from that produced by a correct execution of the
operations emitted by the portal.
3.3 Solution overview and challenges
Iris consists of two major components:
Authenticated ﬁle system: As already described, the ﬁrst chal-
lenge we address in building an authenticated enterprise-class ﬁle
system is the high cost of network latency and bandwidth between
the enterprise and cloud. Another challenge is efﬁcient manage-
ment and caching of the authenticating information. Integrity and
freshness veriﬁcation should be extremely efﬁcient for existing ﬁle
system operations and induce minimal latency.
Iris employs a two-layer authentication scheme.
In its lower
layer, it stores on every ﬁle block a message-authentication code
(MAC)—generated by the portal when a client writes to the ﬁle
system. These MACs ensure data integrity. To ensure freshness,
it is necessary to authenticate not just data blocks, but also their
versions. Each block has an associated version counter that is in-
cremented every time the block is modiﬁed. This version number
is bound to the ﬁle-block’s MAC: To protect against cloud replay
of stale ﬁle-blocks (rollback attacks), the counters themselves must
be authenticated.
The upper layer of the authenticated data structure in Iris is a
balanced Merkle-tree-based structure that protects the integrity of
the ﬁle-block version counters. This data structure embeds the ﬁle
system directory tree, and balances each directory for optimization.
Attached to each node representing a ﬁle is a sub-tree containing
ﬁle-block version counters. The root of the Merkle tree stored at
the portal guarantees the integrity and freshness of both data and
meta-data in the ﬁle system.
This Merkle-tree-based structure has two distinctive features com-
pared to other authenticated ﬁle systems: (1) Support for existing
ﬁle system operations: Iris maintains a balanced binary tree over
the ﬁle system directory structure to efﬁciently support existing
ﬁle system calls; and (2) Support for concurrent operations: The
Merkle tree supports efﬁcient updates from multiple clients operat-
ing on the ﬁle system in parallel. Iris also optimizes for the com-
mon case of sequential ﬁle-block accesses: Sequences of identical
version counters are compacted into a single leaf. We detail the
data structure in Section 4, and the Merkle tree caching mechanism
in Section 6.
Auditing protocol: Iris enables the enterprise tenant to continu-
ously monitor and assess the correctness and availability of the en-
tire ﬁle system through the auditing protocol. The auditing pro-
tocol in Iris is an instantiation of a PoR protocol and, in fact, the
ﬁrst dynamic PoR protocol supporting data updates. Previous PoR
protocols have been designed for static data (ﬁles that do not un-
dergo modiﬁcations). In any PoR, the tenant samples and checks
the correctness of random data blocks retrieved from the cloud to
detect any large-scale data corruption. To recover from small-scale
damage, parity information computed with an erasure code needs
to be maintained over the data.
The main challenge in designing a dynamic PoR protocol is that
the erasure code structure, i.e., mapping of data blocks to parity
blocks, must be randomized to prevent an adversarial server from
introducing targeted, undetectable ﬁle corruptions. File updates are
most problematic as they partially reveal the code structure (in par-
ticular the parity blocks corresponding to updated ﬁle blocks). At
the same time, ﬁle updates should be efﬁcient and involve only a
small fraction of parity blocks.
We overcome this challenge with two techniques. First, we de-
sign Iris to cache parity information locally at the portal (and only
checkpoint it to the cloud at ﬁxed time intervals). As the cloud does
not perceive individual ﬁle updates, but only parity modiﬁcations
aggregated over a long time interval, the cloud cannot easily infer
the mapping from ﬁle blocks to parity blocks. Second, we design a
new sparse, binary code structure that combines randomly chosen
blocks from the ﬁle system into a codeword. The code supports
updates to the ﬁle system very efﬁciently through binary XOR op-
erations. Its sparse structure supports very large ﬁle systems. This
novel code construction is carefully parameterized to optimize local
storage at the portal side, update cost, and bandwidth and compu-
tation in the auditing protocol. We describe the auditing protocol
and the erasure code construction in Section 5.
4. Authentication in Iris
We describe in this section how Iris provides strong data protec-
tion, including integrity and freshness, for both ﬁle system data and
meta-data. The authentication scheme in Iris is based on Merkle
trees, and designed to support existing ﬁle system operations. In
addition, random access to ﬁles for both read and write operations
is a desirable feature (offered by existing ﬁle systems like NFS)
that we also choose to implement. The tenant needs to maintain at
all times the root of the Merkle trees for checking the integrity and
freshness of data retrieved from the cloud. For reducing operation
latency, recently accessed nodes in the tree are also cached at the
portal (the caching mechanism is described in Section 6).
Figure 2 depicts the main components of our tree-based structure
used for authentication:
Block-level MACs: To provide ﬁle-block integrity, we store a MAC
for each ﬁle block, and combine block MACs from the same ﬁle in
a MAC ﬁle. We choose to store MACs for each ﬁle block (instead
of a single MAC for each ﬁle) to support random accesses to ﬁles.
Block MACs are computed by the portal when a client writes to the
ﬁle system. For providing freshness, we need to bind a unique ver-
sion number to each ﬁle block every time it’s updated and include
232
the version number in the block MAC. To protect against rollback
attacks (in which clients are presented with an old state of the ﬁle
system), version numbers will have to be authenticated as well.
File version trees: We construct a ﬁle version tree per ﬁle that
authenticates version numbers for all ﬁle blocks in a compressed
form. Brieﬂy, the ﬁle version tree compresses the versions of a
consecutive range of blocks into a single node, storing the index
range of the blocks and their common version number. File version
trees are optimized for sequential access to ﬁles. For instance, if a
ﬁle is always written sequentially then its ﬁle version tree consists
of only one root node. The compacted version tree essentially be-
haves as a range tree data structure. An example of a compacted
tree is shown in Figure 3.
Directory trees: To authenticate ﬁle system meta-data (or the di-
rectory structure of the ﬁle system), the ﬁle system directory tree is
transformed into a Merkle tree in which every directory is mapped
to a directory subtree. We have chosen to map our authenticated
data structure onto the existing ﬁle system tree in order to efﬁciently
support ﬁle system operations like delete or move of entire direc-
tories. To support directories with large number of ﬁles efﬁciently,
we create a balanced binary tree for each directory that contains
ﬁle and subdirectory nodes in the leaves, and includes intermedi-
ate, empty internal nodes for balancing. Nodes in a directory tree
have unique identifers assigned to them, chosen as random strings
of ﬁxed length. A leaf for each ﬁle and subdirectory is inserted into
the directory tree in a position given by a keyed hash applied to its
name and its parent’s identiﬁer (to ensure tree balancing).
With this Merkle tree construction, we authenticate both ﬁle sys-
tem meta-data, as well as ﬁle block version numbers. Together
with the ﬁle block MACs, this mechanism ensures data integrity
and freshness, assuming that the portal always stores the root of the
Merkle tree.
Free list: As an optimization, we also maintain in the data struc-
ture a free list containing pointers of nodes deleted from the data
structure, i.e., subtrees removed as part of delete or truncate op-
erations. The aim of the free list is to defer garbage collection of
deleted nodes and support remove and truncate ﬁle system opera-
tions efﬁciently. We omit further details due to space limitations.
At the leaves of the directory tree, we insert the ﬁle version
trees in compacted form, as described above.
Internal nodes in
the Merkle tree contain hash values computed over their children,
as well as some additional information, e.g., node identiﬁers, their
rank (deﬁned as the size of the subtree rooted at the node), ﬁle and
directory names.
Our Merkle tree supports the following operations. Clients can
insert or delete ﬁle system object nodes (ﬁles or directories) at cer-
tain positions in the tree. Those operations trigger updates of the
hashes stored on the path from the inserted/deleted nodes up to the
root of the tree. Deleted subtrees are added to the free list, as ex-
plained below. Clients can verify a ﬁle block version number, by
retrieving all siblings on the path from the leaf corresponding to
that ﬁle block up to the root of the tree. Searches of ﬁles or direc-
tories in the tree can also be performed, given absolute path names.
We also implement an operation randompath-dir-tree for direc-
tory trees. This feature is needed to execute the challenge-response
protocols of the auditing component in Iris. A (pseudo)-random
path in the tree is returned by traversing the tree from the root,
and selecting at each node a child at random, weighted by rank.
In addition, the authentication information for the random path is
returned, so the tenant can verify that the path has been chosen
pseudo-randomly.
Figure 2: Authenticated tree. A ﬁle system directory on the left
and its mapping to the Merkle tree on the right.
Figure 3: File version tree for a ﬁle with 16 blocks. Blocks 0-3 and
10-13 have been written twice, all other blocks have been written
once. White nodes on the left are removed in the compacted ver-
sion on the right. Version numbers are adjacent to nodes.
5. Auditing protocol
The authentication mechanism in Iris presented in the previous
section can be used to verify the correctness of all blocks retrieved
from the ﬁle system during the course of normal operations issued
by clients. A challenging question that we address in this section
is how can the enterprise verify infrequently accessed blocks and
detect even small amounts of corruptions spread throughout the ﬁle
system. We are particularly interested in offering strong assurances