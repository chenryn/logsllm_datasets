30
20
10
0
200
16
14
12
10
8
6
4
2
)
s
d
n
o
c
e
s
(
y
c
n
e
t
a
L
0
200
400
600
800
1000
Time (seconds)
1200
1400
1600
400
600
800
1000
Time (seconds)
1200
1400
1600
(a) Throughput
(b) Latency
Figure 8. Fail-over onto warm DMV backup
with 1% query-execution warm-up.
Figure 9 shows the failover effect for our alternate
backup warmup scheme using page id transfers from an
active slave. The active slaves transfers page ids to the
backup every 100 transactions while the backup touches
these pages. We see that the performance in this case is
the same as that for periodic query execution allowing for
seamless failure handling.
70
60
50
40
30
20
10
0
200
16
14
12
10
8
6
4
2
)
s
d
n
o
c
e
s
(
y
c
n
e
t
a
L
0
200
400
600
800
1000
Time (seconds)
1200
1400
1600
400
600
800
1000
Time (seconds)
1200
1400
1600
)
S
P
W
I
(
t
u
p
h
g
u
o
r
h
T
70
60
50
40
30
20
10
0
200
16
14
12
10
8
6
4
2
)
s
d
n
o
c
e
s
(
y
c
n
e
t
a
L
0
200
400
600
800
1000
Time (seconds)
1200
1400
1600
400
600
800
1000
Time (seconds)
1200
1400
1600
(
t
u
p
h
g
u
o
r
h
T
)
S
P
W
I
(a) Throughput
(b) Latency
Figure 7. Fail-over onto cold up-to-date DMV
backup.
(a) Throughput
(b) Latency
We can see that the drop in throughput is signiﬁcantin
this case due to the need to warm-up the entire database
cache on the cold backup. It takes more than 1 minute until
Figure 9. Fail-over onto warm DMV backup
with page id transfer.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:31:27 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 20077. Related Work
8. Conclusions
A number of solutions exist for replication of rela-
tional databases that aim to provide both scaling and strong
consistency. They range from industry-established ones,
such as the Oracle RAC [3] and the IBM DB2 HADR
suite [1] to research and open-source prototypes, such as
MySQL Cluster [17], C-JDBC [9, 10], Postgres-R [14] and
Ganymed [20].
The industry solutions provide both high availability and
good scalability, but they are costly and require specialized
hardware such as Shared Network Disk [3]. MySQL Clus-
ter [17] provides very fast in-memory replicated storage en-
gine with lazy logging of updates, similar to the one in our
system prototype. However, it uses traditional two-phase
locking for concurrency control which may stall readers ac-
cessing data that’s being modiﬁed. In contrast, our solu-
tion resolves read/write conﬂicts optimistically and hence
avoids thread blocking. Existing research prototypes use
commodity software and hardware, but they either have lim-
ited scaling for moderately heavy write workloads [5, 10]
due to their use of coarse-grained concurrency control im-
plemented in the scheduler, or sacriﬁcefailure transparency
and data availability by introducing single points of fail-
ure [20]. Even the solutions that offer transparent fail-over
and data availability [5] do so by means of complex proto-
cols due to the crucial data that resides inside the scheduler
tier. In contrast, our solution provides transparent scalabil-
ity as well as fast, transparent failover. The scheduler node
is minimal in functionality, which permits extremely fast re-
conﬁgurationin the case of single node fail-stop scenarios.
Previous work in the area of primary-backup replica-
tion [24] has mostly followed a “passi ve backup as a hot-
standby” approach where the backup simply mirrors the up-
dates of the primary. These solutions either enforce a fully
synchronous application of updates to the backup or do not
enforce strict consistency although the backup does main-
tain a copy of the database on the primary. The backup is
either idle during failure free system execution [24] or could
execute a different set of applications/tasks. In contrast to
these classic solutions, in our replicated cluster, while back-
ups are used for seamless fail-over, a potentially large set of
active slaves are actively executing read-only transactions
with strong consistency guarantees.
More recent efforts towards integration of database ﬁne-
grained concurrency control and replication techniques use
snapshot isolation [11, 23, 20] to minimize consistency
maintenance overheads. These solutions depend on support
for multiversioning within each database replica. In con-
trast, our solution dynamically creates the required versions
on a set of distributed replicas.
In this paper, we introduce novel lightweight reconﬁgu-
ration techniques for the database tier in dynamic content
web sites. Our solution is based on an in-memory replica-
tion algorithm, called Dynamic Multiversioning, which pro-
vides transparent scaling with strong consistency and ease
of reconﬁgurationat the same time.
Dynamic Multiversioning offers high concurrency by ex-
ploiting the naturally arising versions across asynchronous
database replicas. We avoid duplication of database func-
tionality in the scheduler for consistency maintenance by
integrating the replication process with the database concur-
rency control. Furthermore, we avoid copy-on-write over-
heads associated with systems that use stand-alone database
multiversioning offering snapshot isolation. We show how
a version-aware scheduler algorithm distributes transactions
requesting different version numbers across different nodes,
thus keeping aborts due to version conﬂicts at negligible
rates.
Our evaluation shows that our system is ﬂe xible and ef-
ﬁcient. While a primary replica is always needed in our
in-memory tier, a set of active slaves can be adaptively and
transparently expanded to seamlessly accommodate faults.
We scale a web site using an InnoDB on-disk database back-
end by factors of 14.6, 17.6 and 6.5 for the TPC-W brows-
ing, shopping and ordering mixes, respectively when inter-
posing our intermediate in-memory tier with 9 replicas. We
also show that our in-memory tier has the ﬂe xibility to in-
corporate a spare backup after a fault without any noticeable
impact on performance due to reconﬁguration.
References
[1] IBM DB2 High Availability and Disaster Recovery. http:
//www.ibm.com/db2/.
[2] Mysql Database Server. http://www.mysql.com/.
[3] Oracle Real Application Clusters
10g.
http:
//www.oracle.com/technology/products/
database/clustering/.
[4] C. Amza, E. Cecchet, A. Chanda, A. Cox, S. Elnikety,
R. Gil, J. Marguerite, K. Rajamani, and W. Zwaenepoel.
Speciﬁcation and implementation of dynamic web site
benchmarks. In 5th IEEE Workshop on Workload Charac-
terization, November 2002.
[5] C. Amza, A. Cox, and W. Zwaenepoel. Conﬂict-a ware
In Proceed-
scheduling for dynamic content applications.
ings of the Fifth USENIX Symposium on Internet Technolo-
gies and Systems, pages 71–84, Mar. 2003.
[6] C. Amza, A. Cox, and W. Zwaenepoel. Distributed version-
ing: Consistent replication for scaling back-end databases
of dynamic content web sites. In ACM/IFIP/Usenix Interna-
tional Middleware Conference, June 2003.
[7] The Apache Software Foundation. http://www.apache.org/.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:31:27 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007[8] P. Bernstein, V. Hadzilacos, and N. Goodman. Concur-
rency Control and Recovery in Database Systems. Addison-
Wesley, Reading, Massachusetts, 1987.
[9] E. Cecchet, J. Marguerite, and W. Zwaenepoel. C-jdbc:
Flexible database clustering middleware. In Proceedings of
the USENIX 2004 Annual Technical Conference, Jun 2004.
[10] E. Cecchet, J. Marguerite, and W. Zwaenepoel. RAIDb: Re-
dundant array of inexpensive databases. In IEEE/ACM In-
ternational Symposium on Parallel and Distributed Applica-
tions (ISPA’04), December 2004.
[11] S. Elnikety, F. Pedone, and W. Zwaenepoel. Generalized
snapshot isolation and a preﬁx-consistent implementation.
Technical Report IC/2004/21, EPFL, 2004.
[12] IBM. High availability with DB2 UDB and Steeleye Life-
IBM Center for Advanced Studies Conference
keeper.
(CASCON): Technology Showcase, Toronto, Canada, Oct
2003.
[13] B. Kemme and G. Alonso. A New Approach to Developing
and Implementing Eager Database Replication Protocols. In
ACM Transactions on Data Base Systems, volume 25, pages
333–379, September 2000.
[14] B. Kemme and G. Alonso. Don’t be lazy, be consistent:
Postgres-R, a new way to implement database replication.
In The VLDB Journal, pages 134–143, 2000.
[15] D. Lowell and P. Chen. Free transactions with Rio Vista.
In Proceedings of the 16th ACM Symposium on Operating
Systems Principles, Oct. 1997.
[16] K. Manassiev, M. Mihailescu, and C. Amza. Exploiting dis-
tributed version concurrency in a transactional memory clus-
ter. In PPOPP, pages 198–208, 2006.
[17] MySQL Cluster.
http://www.mysql.com/
products/database/cluster/.
[18] M. Patino-Martinez, R. Jimenez-Peris, B. Kemme, and
G. Alonso. Scalable replication in database clusters.
In
DISC ’00: Proceedings of the 14th International Conference
on Distributed Computing, pages 315–329. Springer-Verlag,
2000.
[19] PHP Hypertext Preprocessor. http://www.php.net.
[20] C. Plattner and G. Alonso. Ganymed: Scalable replication
for transactional web applications. In Proceedings of the 5th
ACM/IFIP/USENIX International Middleware Conference,
Toronto, Canada, October 18-22 2004.
[21] G. Soundararajan, C. Amza, and A. Goel. Database repli-
cation policies for dynamic content applications.
In Eu-
roSys’06: Proceedings of the EuroSys 2006 Conference,
pages 89–102. ACM, 2006.
[22] Transaction Processing Council. http://www.tpc.org/.
[23] S. Wu and B. Kemme. Postgres-r(si): Combining replica
control with concurrency controlbased on snapshot isola-
tion.
In Proceedings of the 21st International Conference
on Data Engineering, Apr 2005.
[24] Y. Zhou, P. Chen, and K. Li. Fast cluster failover using vir-
tual memory-mapped communication. In Proc. of the Int’l
Conference on Supercomputing, June 1999.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:31:27 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007