### Wikimedia's Use of ORES for Quality Assessment

Wikimedia employs a machine learning system called ORES to automatically categorize the quality of contributions to Wikipedia [17]. This system was developed to assist Wikipedia editors in protecting the encyclopedia from vandalism and other forms of damage. With the support of the ORES team, we utilized this system to evaluate the quality of edits in our comparison groups. Since ORES is fully automated, it enabled us to analyze the entire dataset. ORES classifies edits based on the likelihood that they are "Good Faith" or "Damaging" [17]. We recoded "Damaging" as "Non-Damaging" to ensure that high scores indicate positive quality and low scores indicate negative quality.

#### Features and Training of ORES

There is no universally accepted set of features for assessing the quality of work on Wikipedia [10]. However, ORES is trained using edit quality judgments provided by the Wikipedia community. For the English Wikipedia, ORES uses 24 different features, including the presence of "bad words," informal language, dictionary presence, repeated characters, white space, uppercase letters, and more [11, 37, 38]. Other features relate to the amount of text, references, and external links added or removed in a revision. Additionally, ORES considers contribution metadata, such as whether the editor provided an edit summary, and contributor metadata, such as whether the editor is an administrator or is using a newly created account. The specific list of features varies by language, and a complete list is available in the publicly accessible ORES source code [27].

Previous research has shown that ORES scores can be systematically biased, often classifying edits by IP editors and inexperienced users as lower quality [17].

#### Feature Injection and Analysis

To assess contribution quality independently of identity-based features, we used ORES's "feature injection" functionality [17]. This allowed us to treat all revisions as if they were made by registered users with accounts that are 0 seconds old. A visualization of the feature-injected ORES analysis of our comparison sets over time is shown in Fig. 5, using LOESS smoothers [9] [28].

**Figure 5: Non-parametric LOESS curve over time.**  
- **Description:** We used feature injection to instruct the ORES Good Faith model to treat all edits as if they were made by a newly created user account.
- **Visualization:** The plot shows the Good Faith scores for First-time Editors, Tor-based Editors, IP-based Editors, and Registered Editors over the years 2009 to 2017.

**Table V: Logistic Regression Using a Feature-Injected ORES Model.**  
- **Baseline Category:** First-time Editors
- **Results:**
  - **Good Faith:**
    - Tor-based Editors: 0.10* (CI: [0.03; 0.17])
    - IP-based Editors: 0.01 (CI: [-0.06; 0.08])
    - Registered Editors: 0.70* (CI: [0.62; 0.79])
  - **Non-Damaging:**
    - Tor-based Editors: 0.27* (CI: [0.07; 0.20])
    - IP-based Editors: 0.14* (CI: [0.01; 0.14])
    - Registered Editors: 0.68* (CI: [0.61; 0.76])

The positive coefficients for Tor in both Good Faith and Non-Damaging scenarios suggest that Tor users are slightly better contributors than First-time editors according to ORES. Although the differences are statistically significant, the practical significance is small. For example, the likelihood of a Good Faith edit for a new account is 70.5%, while for a Tor editor, it is 72.5%, a difference of only 2%.

### Comparison of Hand-coded Results to ORES Results

We compared hand-coded results with ORES predictions to identify Non-Damaging edits. This comparison helps determine if ORES classifications are biased against Tor editors. Using feature injection, we treated all edits in the hand-coded sample as if they were made by newly registered editors and generated ROC curves (Fig. 8 in the appendix).

**Table VI: Classifier AUC of ORES with and without Feature Injection.**  
- **First-time Editors:**
  - AUC w/ Injection: 0.704
  - AUC w/o Injection: 0.708
- **IP Editors:**
  - AUC w/ Injection: 0.811
  - AUC w/o Injection: 0.814
- **Tor Editors:**
  - AUC w/ Injection: 0.758
  - AUC w/o Injection: 0.753
- **Registered Editors:**
  - AUC w/ Injection: 0.663
  - AUC w/o Injection: 0.673

Feature injection had modest effects on model performance, with minor changes in AUC values. Our hand-coding process included noticing when links were to personal or spam websites and considering the context of the edit, which ORES does not do. These results suggest that machine learning tools like ORES have limited ability to assess edit quality without human intervention.

### Topic Modeling

To explore whether Tor editors differ systematically in their editing topics, we used topic modeling. Specifically, we applied Latent Dirichlet Allocation (LDA) using the Machine Learning for Language Toolkit (MALLET) [26]. We identified all articles edited by Tor users and our three comparison groups, mined their textual content, and processed them through MALLET to produce keywords and their probability distributions.

**Figure 6: Raster Diagram.**  
- **Description:** Proportion of articles edited by each comparison group (x-axis) and the single highest proportion of the topic (y-axis).

We found that the top five most frequent topics for each group of edits were highly consistent across multiple runs. We chose 20 topics as the optimal number for comprehensible and reasonable topics.

### Conclusion

Our analysis indicates that while there are statistically significant differences between Tor edits and other groups, the practical effects are small. Tor editors and IP editors show similar behavior to First-time editors, but contributions from Registered editors are of higher quality. ORES provides moderate assistance in identifying damaging edits, but human intervention remains essential for accurate quality assessment. Topic modeling suggests that Tor users may focus on specific, possibly sensitive, topics, providing insights into their reasons for seeking anonymity.

---

**References:**
- [17] Reference to ORES documentation.
- [9] Reference to LOESS plots.
- [27] Reference to ORES source code.
- [26] Reference to MALLET toolkit.