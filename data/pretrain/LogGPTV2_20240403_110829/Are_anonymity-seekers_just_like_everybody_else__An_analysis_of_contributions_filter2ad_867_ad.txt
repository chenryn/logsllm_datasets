Wikimedia uses a machine learning system called ORES
to automatically categorize the quality of contributions to
Wikipedia [17]. The system was developed to support
Wikipedia editors trying to protect
the encyclopedia from
vandalism and other kinds of damage. With assistance from
the ORES team, we used the system to assess the quality of
the edits in our comparison groups. Because ORES is fully
automated, we were able to conduct our analysis on the full
datasets. ORES classiﬁes edits in terms of the likelihood that
they are “Good Faith” and “Damaging” [17]. We recoded
Damaging as Non-Damaging so that in all cases “high” scores
are positive and “low” scores are negative.
While there exists no gold standard set of features for
assessing the quality of work on Wikipedia [10], ORES
is trained using edit quality judgments solicited from the
Wikipedia community. The system uses 24 different features
for English Wikipedia [11, 37, 38]. These include the presence
of “bad words,” informal language, whether words appear in a
dictionary, repeated characters, white space, uppercase letters,
and so on. Other features are related to the amount of text, ref-
erences, and external links added or removed in a revision. In
addition to features related to the text of a contribution, ORES
uses contribution metadata such as whether the editor supplied
an edit summary, and contributor metadata such as whether the
editor is an administrator or is using a newly created account.
The speciﬁc list of features differs by language, and a full
list is available in the publicly available ORES source code.27
Previous work has found that ORES scores are systematically
biased so that it classiﬁes edits by IP editors and inexperienced
users as being lower quality [17].
To understand contribution quality independent of identity-
based features, we made use of the “feature injection” func-
tionality in ORES [17]. Using feature injection, we instructed
ORES to treat all revisions as if made by Registered users
whose accounts are 0 seconds old. A visualization of the
feature-injected ORES analysis of our comparison sets are
shown over time in Fig. 5. This visualization is produced
using LOESS smoothers [9].28 This model is of Good Faith
27https://github.com/wikimedia/editquality/tree/master/editquality/feature
lists (Archived: https://perma.cc/TME4-NSL6)
28LOESS plots are a visualization tool
that use low-order polynomial
regression on each datapoint to calculate a smoothed ﬁt line that describes
the data as a weighted moving average. The grey bands represent standard
errors around the LOESS estimates.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:17:39 UTC from IEEE Xplore.  Restrictions apply. 
194
First−time Editors
Tor−based Editors
IP−based Editors
Registered Editors
differences between Tor edits and our comparison groups but
also ﬁnd that the practical effects are small. Our models predict
higher average rates of Non-Damaging edits for Tor editors
(60.1% for Tor editors versus 56.7% for First-time editors)
and IP editors (58.4%). For both models, contributions from
Registered editors are estimated to be of high quality, with a
prediction of 82.8% Good Faith and 72.1% Non-Damaging.
These results provide additional evidence in support of our
hypothesis that Tor editors, IP editors, and First-time editors
are quite similar in their overall behavior but that quality levels
of contributions from Registered editors are higher.
G
o
o
d
F
a
i
t
h
n
o
i
i
t
c
d
e
r
P
S
E
R
O
1.00
0.75
0.50
0.25
0.00
2009 2011 2013 2015 2017
2009 2011 2013 2015 2017
Year
2009 2011 2013 2015 2017
2009 2011 2013 2015 2017
Fig. 5. A non-parametric LOESS curve over time. We use feature injection
to instruct the ORES Good Faith model to treat all edits as if they were made
by a newly created user account.
LOGISTIC REGRESSION USING A FEATURE-INJECTED ORES MODEL.
FIRST-TIME EDITORS SERVED AS THE OMITTED CATEGORY.
TABLE V
Intercept
Tor-based Editors
IP-based Editors
Registered Editors
0.87∗
0.10∗
[0.03; 0.17]
[−0.06; 0.08]
0.01
0.70∗
[0.62; 0.79]
Good Faith
Non-Damaging
[0.82; 0.92]
[0.22; 0.31]
0.27∗
0.14∗
0.07∗
0.68∗
[0.07; 0.20]
[0.01; 0.14]
[0.61; 0.76]
AIC
BIC
Log Likelihood
Deviance
Num. obs.
∗
35414.08
35447.18
-17703.04
7395.66
29059
indicates that 0 is outside the 95% conﬁdence interval
26819.97
26853.08
-13405.98
7541.53
29057
measure; we omit the Non-Damaging ORES model because
the lines are extremely similar. This visualization shows that
Tor, IP, and First-time editors are all comparable, with Tor
editors appearing to make slightly higher quality contributions
than First-time and IP editors, particularly in the latter parts
of the data. We used logistic regression to test for statistical
differences, treating First-time editors as the baseline category
as they most closely resemble our feature injection scenario.
The results of our model are reported in Tab. V.
The positive coefﬁcient for Tor in both Good Faith and Non-
Damaging scenarios indicates that Tor users are slightly better
contributors than our baseline of First-time editors by the
ORES measurement. Although the differences are statistically
signiﬁcant, the estimated chance that a given edit will be
Good Faith at the baseline (new account) is 70.5%. whereas
the likelihood that an edit will be Good Faith if it originates
from a Tor editor is 72.5%. We believe that the estimated
2% margin is unlikely to be practically signiﬁcant. For the
Non-Damaging model, we likewise ﬁnd statistically signiﬁcant
B. Comparison of Hand-coded Results to ORES Results
Given that we performed two different kinds of analysis
to identify Non-Damaging edits (i.e., hand-coding the edits,
and scoring via the ORES machine learning platform), we
can examine the extent to which these two measures agree.
Doing so is valuable because it can indicate whether the ORES
classiﬁcations used by Wikipedia are systematically biased
against contributors from Tor editors. As with our analysis
in §VI-A, we used feature injection to instruct ORES to treat
all edits in the hand-coded sample used in §V-E as if they
were being made by newly Registered editors. We then used
these data to compare the ORES prediction with and without
feature injection to our manual assessment for all four user
groups by generating receiver operating characteristic (ROC)
curves. We have included the full curves in our appendix in
Fig. 8.
Table VI reports model performance in the form of area
under the curves (AUC) for the ROC curves for each of
our comparison groups. These results indicate that there is
substantial room for improvement in ORES. Using feature
injection, ORES performs best relative to our hand-coded data
when predicting the quality of edits performed by IP editors
(AU C = 0.811 for Non-Damaging), less well for Tor editors
(AU C = 0.758), and even less well for First-time editors
(AU C = 0.704) but, strikingly, worst for Registered editors
(AU C = 0.663).
When we examined a small sample of edits where our
hand-coding and ORES disagreed, we found there were often
good reasons for the disagreement. Our hand-coding process
included doing work that ORES does not do, such as noticing
when links were to personal or spam websites and weighing
the context of the edit on the page against our own understand-
ing of appropriate and correct encyclopedic content. These
results suggest that machine learning tools such as ORES have
a limited ability to assess the quality of edits without human
intervention.
Systematic bias in ORES could result in higher rates of
rejection of contributions from some groups of editors. Feature
injection as we have done it treats registered editors as if they
are new—essentially removing a “beneﬁt of the doubt” based
on their longevity in the community. Table VI shows that fea-
ture injection has very modest effects on model performance—
dropping AUC by 0.01 for Registered editors and by 0.004
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:17:39 UTC from IEEE Xplore.  Restrictions apply. 
195
CLASSIFIER AUC OF ORES WITH AND WITHOUT FEATURE INJECTION
FOR OUR FOUR SAMPLES OF EDITS.
TABLE VI
First-time Editors
IP Editors
Tor Editors
Registered Editors
AUC w/ Injection
0.704
0.811
0.758
0.663
AUC w/o Injection
0.708
0.814
0.753
0.673
for First-time editors while improving AUC by 0.005 for Tor
editors and by 0.003 for IP editors.
The team that developed ORES published a set of recom-
mended operating points. For example, they suggest that users
developing fully automated systems (“bots” below) maximize
recall at a precision of ≥90%. They suggest that users devel-
oping a human-involved system maximize ﬁlter rate (that is,
the number that are not routed for review) at recall ≥75%.
ORES provides an interface to use preferred constraints to
select an optimized decision-making threshold. For example,
if we use the provided “bot” constraint, ORES recommends
an operating point threshold of .055; that is a bot should only
automatically discard an edit if the Non-Damaging level is
below 5.5%. We examine our results using these thresholds to
understand how ORES would classify Tor edits in Wikipedia’s
normal workﬂow.
The predicted values we report in Table VII describe ORES’
predictions about its own performance based on its training
data using these recommended thresholds. Our results indicate
that while a system that uses bots can identify a small pro-
portion of damaging edits made through Tor, many damaging
edits are missed while many Non-Damaging edits are routed
for review. Our results suggest that ORES offers only moderate
assistance to human-augmented systems seeking to review
edits made by privacy seekers using Tor.
C. Topic Modeling
Although average quality may be similar, Tor editors may
differ systematically from other editors in terms of what
they choose to edit. Knowing which topics Tor users edit
might provide insight into their reasons for seeking anonymity
and the value of their contributions. For example, Tor users
might pay more attention to matters that are sensitive and
controversial. Unfortunately, the Wikipedia category system
is an incredibly granular human-curated graph that is poorly
suited to the construction of coarse comparisons across broad
selections of articles [34].
Topic modeling may assist such an exploration by offering
clusters of keywords that can be interpreted as semantic topics
present in a collection of documents. One of the most popular
topic modeling techniques is called Latent Dirichlet Allocation
(LDA)—a generative probabilistic model for collections of
discrete data such as text corpora [3]. Machine Learning
for Language Toolkit (MALLET) provides a widely used
way to use LDA [26]. Given a list of documents and a
number of topics, MALLET estimates a set of probability
distributions of topics over the vocabulary of unique words.
Fig. 6. A raster diagram showing the proportion of articles edited by each
comparison group (along the x-axis) with where the topic (along the y-axis)
is the single highest proportion.
With these probability distributions and a further inspection
of the keywords MALLET outputs, we can gain insight into
the kinds of subjects that Tor users and other groups of editors
pay attention to. While topic models are known to be unstable,
they are useful for comparing documents across a set of ex ante
groups.
Using our datasets of edits, we identiﬁed all the articles
edited by Tor users and our three comparison groups. Next,
we mined all textual content of these articles and then pro-
cessed them through MALLET to produce keywords and their
probability distributions. Because there is no optimal number
of topics, we ran the tool to ﬁnd 10, 20, 30, and 40 topics.
For each number, we conducted four different runs to test the
consistency of the results. After these experiments, we found
that the results across the top ﬁve most frequent topics for each
group of edits are highly consistent, with only slight changes
in the keywords and the ranking. Because we felt that having
20 different clusters of keywords for the whole text corpora led
to the most reasonable and comprehensible topics, the results
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:17:39 UTC from IEEE Xplore.  Restrictions apply. 
196
COMPARISON OF ORES DEVELOPER-PREDICTED PERFORMANCE TO ACTUAL PERFORMANCE OF OUR HAND-CODED SAMPLE OF EDITS MADE FROM TOR
TABLE VII
WITHOUT FEATURE INJECTION (n = 847).
Scenario
Optimizing
Constraint
Recommended
Threshold
Automatic
Removal
Max. Recall
at
Preci-
sion ≥ 90%
<5.5%
non damaging
Route for
Human
Review
Max. Filter