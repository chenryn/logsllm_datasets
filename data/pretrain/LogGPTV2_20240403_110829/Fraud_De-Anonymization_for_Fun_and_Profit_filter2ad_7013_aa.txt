title:Fraud De-Anonymization for Fun and Profit
author:Nestor Hernandez and
Mizanur Rahman and
Ruben Recabarren and
Bogdan Carbunar
Fraud De-Anonymization For Fun and Profit
Nestor Hernandez
Mizanur Rahman
FIU, Miami, USA
PI:EMAIL
Bogdan Carbunar
PI:EMAIL
Ruben Recabarren
FIU, Miami, USA
FIU, Miami, USA
PI:EMAIL
FIU, Miami, USA
PI:EMAIL
ABSTRACT
The persistence of search rank fraud in online, peer-opinion sys-
tems, made possible by crowdsourcing sites and specialized fraud
workers, shows that the current approach of detecting and filtering
fraud is inefficient. We introduce a fraud de-anonymization ap-
proach to disincentivize search rank fraud: attribute user accounts
flagged by fraud detection algorithms in online peer-opinion sys-
tems, to the human workers in crowdsourcing sites, who control
them. We model fraud de-anonymization as a maximum likeli-
hood estimation problem, and introduce UODA, an unconstrained
optimization solution. We develop a graph based deep learning ap-
proach to predict ownership of account pairs by the same fraudster
and use it to build discriminative fraud de-anonymization (DDA)
and pseudonymous fraudster discovery algorithms (PFD).
To address the lack of ground truth fraud data and its pernicious
impacts on online systems that employ fraud detection, we pro-
pose the first cheating-resistant fraud de-anonymization validation
protocol, that transforms human fraud workers into ground truth,
performance evaluation oracles. In a user study with 16 human
fraud workers, UODA achieved a precision of 91%. On ground truth
data that we collected starting from other 23 fraud workers, our
co-ownership predictor significantly outperformed a state-of-the-
art competitor, and enabled DDA and PFD to discover tens of new
fraud workers, and attribute thousands of suspicious user accounts
to existing and newly discovered fraudsters.
CCS CONCEPTS
• Security and privacy → Social network security and pri-
vacy; Social aspects of security and privacy; • Information
systems → Incentive schemes;
KEYWORDS
Fraud De-Anonymization; Search Rank Fraud; Crowdturfing; Fake
Review; Opinion Spam; Sybil Attack; App Store Optimization
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CCS ’18, October 15–19, 2018, Toronto, ON, Canada
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5693-0/18/10...$15.00
https://doi.org/10.1145/3243734.3243770
ACM Reference Format:
Nestor Hernandez, Mizanur Rahman, Ruben Recabarren, and Bogdan Car-
bunar. 2018. Fraud De-Anonymization For Fun and Profit. In 2018 ACM
SIGSAC Conference on Computer and Communications Security (CCS ’18), Oc-
tober 15–19, 2018, Toronto, ON, Canada. ACM, New York, NY, USA, 16 pages.
https://doi.org/10.1145/3243734.3243770
1 INTRODUCTION
Popular online service providers rely on user feedback to rank prod-
ucts and content they host over the Internet. Unfortunately, many
review-based platforms (e.g., Google Play [59], TripAdvisor [60],
Amazon [78], Twitter [22]) are the targets of undisclosed and de-
ceptive marketing practices whereby product developers engage in
fake endorsement either to boost their products or to demote those
of a competitor. Black hat crowdsourcing or crowdturfing offers an
economically viable opportunity for developers to hire specialized
workers who spam for profit [42, 71, 74, 76, 82].
This type of propaganda has a detrimental effect on the trust-
worthiness and quality of online services, and users can suffer from
such bait-and-switch schemes. For this reason, most major online,
peer-opinion services seek to detect and remove fake reviews that
result from hidden endorsements [49, 50, 65], which are unlawful
in accordance with FTC regulations 1. Significant academic work
on defenses against online fraud has focused on a binary classifi-
cation of reviews as fake or honest [18, 27, 31, 36, 37, 41, 43, 44, 47,
58, 61, 67, 71, 77, 79, 83], and of reviewers as fraudulent (Sybil) or
genuine [13, 20, 23, 40, 45, 81, 84, 87].
Fraud detection solutions however are not only (1) ineffective in
preventing fraud, as observed from the continued profitability of
fraud in online services and crowdsourcing platforms but also (2)
their accuracy is difficult to evaluate, given that collecting ground
truth fraud data is a notoriously hard task.
Services like Yelp acknowledge the validation problem, by not
removing but only making suspected fake reviews harder to ac-
cess, and moving reviews back and forth between the fake and
honest classes according to subsequent iterations of their detec-
tion algorithms [6, 49]. To address this problem, academic work
has built gold standard fraud datasets using rule-based heuristics,
assuming that e.g., fraudsters post reviews in a short period of
time [32, 33, 40, 44, 87], from the same IP address [40], or have a
skewed rating distribution [58, 87]. However, such assumptions are
also difficult to validate, especially as they are straightforward to
1If the endorser has been paid or given something of value to promote the product,
the connection between the marketer and endorser should be disclosed [2]
control large numbers of user accounts, and are responsible for post-
ing substantial numbers of fake reviews. Peer-review sites can use
this information to provide counter-incentives for expert fraudsters,
e.g., by pursuing them through their bank accounts (retrieved from
their crowdsourcing site accounts). Peer-review sites can also dis-
incentivize developers from hiring such identifiable fraudsters, e.g.,
by “shaming” promoted products with posts displaying information
about the fraudsters found to promote them [4].
Addressing validation. We introduce the first cheating-resistant,
fraud de-anonymization validation protocol, to obtain ground truth
confirmation on the performance of developed solutions. The proto-
col asks human fraud workers to reveal a seed set of user accounts
that they control, and subsequently confirm and prove control of
accounts that we predict that they control. We introduce multiple
verifications of participant attention and honesty, including asking
confirmations for accounts for which we already know the answer,
as well as e-mail and token based verifications.
Results. We conducted the fraud de-anonymization validation pro-
tocol, through a user study with 16 human fraud workers, who
revealed control of a total of 230 Google Play accounts. The par-
ticipants confirmed control of 91% of the user accounts newly dis-
covered by UODA. Further, on 942 ground truth attributed user
accounts that we collected from other 23 fraud workers, both DDA
and UODA achieved precision and recall that exceed 90%, and at-
tributed thousands of new accounts to these fraudsters.
We introduce intuition, and empirically evaluate the impact of
features used by our co-ownership predictor. Our predictor outper-
formed the F1-measure of state-of-the-art, Elsiedet’s Sybil social
link builder [87] by more than 12 percentage points, on ground
truth attributed data. Further, the PFD algorithm identified thou-
sands of accounts not previously known to be fraudulent, grouped
into communities according to common ownership by fraudsters.
We analyzed 1.1 billion pairs of reviews from these communities
and report orthogonal evidence of fraud, including communities
with more than 80% of accounts involved in review text plagiarism.
In summary, our contributions are the following:
• Fraud de-anonymization. Model fraud de-anonymization
as a maximum likelihood estimation problem. Develop UODA,
an unconstrained optimization fraud de-anonymization al-
gorithm [§ 4].
• Co-ownership predictor. Introduce a graph based deep
learning approach to predict ownership of account pairs by
the same fraudster [§ 5]. Leverage the predictor to build
DDA, a discriminative fraud de-anonymization [§ 6] and
PFD, a pseudonymous fraudster discovery algorithm [§ 7].
• Human fraud de-anonymization oracles. Develop the
first protocol to provide human-fraud-worker-based per-
formance evaluation of fraud de-anonymization algorithms
[§ 9]. Evaluate proposed solutions using data collected through
this protocol [§ 11].
2 CONCEPTS AND BACKGROUND
In this section, we first formally define the basic terminology used
throughout the paper and then provide background details about
fraud in peer-opinion systems.
Figure 1: DETEGO de-anonymizes fraud. Fraud detection
only identifies suspicious user accounts on the right. Fraud
de-anonymization also finds the crowdsourcing account
(left side) that controls them. Arrows signify control.
bypass by experienced fraudsters (e.g., using proxies, better dis-
tributing the post time and rating of reviews). In this paper, we take
steps toward addressing these problems.
Addressing inefficacy. In this paper, we propose to discourage
fraud instead of merely discovering it. To this end, as illustrated
in Figure 1, we seek to bridge the anonymity gap between exist-
ing fraud detection techniques, that only uncover pseudonymous
user accounts that post fraud, and the real identities of crowdsourc-
ing site accounts who control them. Specifically, we leverage the
observation that crowdsourcing site accounts contain uniquely
identifying payment information, e.g., bank, Paypal accounts, to
take steps toward de-anonymizing fraud, by attributing accounts
uncovered by fraud detection algorithms in online peer-opinion
systems, to their human owners in crowdsourcing sites.
We propose a general theoretical framework for the fraud de-
anonymization problem via Maximum Likelihood Estimation (MLE)
and assume a generative review-posting model wherein fraudster-
controlled accounts are more likely to endorse products in a pre-
defined partition of the product space. We introduce UODA, an
unconstrained optimization de-anonymization approach that at-
tributes a fraudulent user account to the fraud worker with the
highest likelihood of having generated its review history.
We develop DeepCluster, a semi-supervised approach to cluster
user accounts based on deep learning features extracted from the
common activities of the accounts. We leverage DeepCluster to
build a co-ownership predictor that determines if two input user ac-
counts are controlled by the same worker. We use the co-ownership
predictor to introduce (1) DDA, a discriminative de-anonymization
solution that trains a classifier to attribute a fraudulent user ac-
count to the worker who controls it, and (2) PFD, a pseudonymous
fraudster discovery algorithm that clusters fraudulent accounts that
cannot be attributed to known workers, such that each cluster is
likely controlled by a different, not yet discovered worker.
We introduce Detego 2, a system that combines fraud de-anony-
mization with fraudster discovery to iteratively expand both knowl-
edge of identifiable fraud workers and the accounts that they con-
trol. We believe that Detego can help peer-review sites identify the
experts from among hundreds of advertised fraud workers, who
2In Latin, detego means to uncover, reveal.
fraud freelancers, or fraudsters) control multiple user accounts and
seek employment by product developers to post fake reviews or ac-
tivities for their products. The accounts controlled by a fraud worker
are also known as Sybils or sockpuppets [13, 23, 40, 45, 81, 84, 85, 87].
Fraud workers advertise their services through crowdsourcing
sites [1, 3, 28], social networks (e.g., Facebook groups), and special-
ized fraud sites [7–11]. Moreover, fraudulent activities are profitable
as evidenced by their price ranges. For instance, we identified 44
fraud workers in Facebook groups, Zeerk, Peopleperhour, Free-
lancer and Upwork that advertised prices ranging from a few cents
($0.56 on average from Zeerk.com) to several dollars per review (up
to $10 in Freelancer.com) [56].
Facilitating Fraud. Crowdsourcing sites like Fiverr, Upwork and
Freelancer [1, 3, 28] host accounts for workers and employers. These
crowdsourcing accounts have a unique identifier and require a
linked bank account for depositing employer’s escrow money or
withdrawing worker’s earnings. Workers on these sites bid on
employer posted jobs while employers assign jobs to workers after
successful negotiation. Thus, these crowdsourcing sites provide a
comprehensive platform for performing peer-opinion system fraud.
In addition, workers can also advertise on social networks where
they usually encounter no restriction to use keywords associated
with search rank fraud and other blackhat services. As a conse-
quence, social networks like Facebook provide high visibility to
these services due to their large user base (see Figure 2 for sample
snapshots). Furthermore, Facebook groups specializing in search
rank fraud efficiently enable developers and fraud workers to find
each other and communicate through posts and comments.
Moreover, fraud workers can also create their own service adver-
tising pages hoping that developers discover them using keyword
search on Internet search engines.
Effective fraud. In a separate Upwork data set experiment, we col-
lected 161 search rank fraud jobs and their 533 bidding workers. We
found that jobs assigned to a single worker occurred less frequently
than jobs awarded to 2 workers. Furthermore, some developers
assigned a single job to as many as 12 workers. We conjecture that
this assignment distribution occurs due to the limited ability of a
single worker to effect a significant impact over a subject’s search
rank. This observation reveals that subjects targeted by search rank
fraud will usually receive fake reviews from multiple fraud workers.
3 PROBLEM DEFINITION
The insight that multiple fraud workers usually target a single
subject suggests that a binary classification of fraud, e.g., fake vs.
honest reviews, fraudulent vs. genuine accounts [17, 26, 27, 31, 32,
44, 47, 80], is insufficient to understand and model fraud. Instead,
we study the fraud de-anonymization problem which deals with
attributing fraudulent accounts and fake reviews to the crowd-
sourcing accounts of the fraud workers who control and post them,
respectively.
Formally, let U be the set of all user accounts, and let S be the
set of all subjects hosted in the online peer-opinion system. We say
that a user account is fraudulent or fraudster-controlled if it was
opened by a fraudster to mainly perform fraudulent activities in
the online system, i.e., to target subjects from S.
Figure 2: Anonymized screenshots of search rank fraud
from Facebook. (Top) Page of Facebook group dedicated to
search rank fraud. (Middle) Recruitment post from devel-
oper. (Bottom) Posts of fraud workers.
2.1 Basic Terminology
User. A person or entity who posts reviews about a subject on an
online peer-opinion system. Users make use of user accounts to
establish their identity online.
Subject. A developer created object or product that receives user
created reviews on the peer-opinion system.
Developer. A person or entity that hosts subjects on the peer-
opinion system. Developers usually have incentives to maximize
their subject’s visibility via review manipulation for which they
hire workers. Thus, we also refer to developers as employers.
Fraud worker. A person or entity that performs review manipula-
tion about a subject on behalf of a developer. Workers often use Sybil
accounts to post fraudulent reviews on the peer-opinion system.
2.2 System and Adversary Model
We consider online peer-opinion systems, e.g., Google Play, Yelp,
Amazon, that host accounts for developers, users and products.
Developers use their accounts to upload information about products
while users are expected to post reviews only for products they
have used. The survival of products in peer-opinion services is
contingent on their review influenced search rank. Higher ranked
products are acquired more frequently and generate more revenue,
either through direct payments or ads. For example, a one star boost
in rating was shown to help restaurants increase revenue by a 5-9%
margin [46]. While online systems keep their ranking algorithms