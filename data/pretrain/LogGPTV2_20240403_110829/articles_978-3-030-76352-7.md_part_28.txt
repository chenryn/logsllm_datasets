trainmodels.Representingthenormalsystemstate,thesemodelsareutilizedto
detect deviations from the learned representation which are labeled as anoma-
lies. Therefore, AIOps systems require preliminary training phases to adjust to
the target environment until they can be utilized for detection. This is known
as cold start problem. Although adjusted to customer requirements, deployed
systems at different sites are very similar (e.g. private cloud solutions based
on OpenStack, storage systems based on HDFS or network orchestration via
ONAP). An obvious mitigation of the cold start problem would be to use train-
ingdatafromexistingsitestotrainmodelsandfinetunethemafterdeployment
within a target customer site. Furthermore, training data used from a variety
of sites increases the holisticity of models allowing them to perform generally
better. However, sharing data or model parameters, even if indirectly related
with company business cases, is usually not possible due to confidentiality or
legal restrictions [6].
Federated learning as a special form of distributed learning is gaining
increased attention since it allows access to a variety of locally available train-
ing data and aims to preserve data privacy [6,7]. Utilizing this concept, we
propose a method that allows different deployments of the same system to syn-
chronize their anomaly detection models without exchanging training data or
model parameters. It does not require a central instance for model aggregation
and thus, improves scalability. We introduce a concept of student and teacher
roles for models whereby student models are learning from teachers. As input,
vectors that are randomly generated within a constrained value range are used
as input to both, student and teacher models. Student models are trained on
theoutputofteachers.Weconductacasestudybasedonloganomalydetection
for the Hadoop File System (HDFS). In a first experiment it is shown that our
solution can mitigate the cold start problem. A second experiment reveals that
the proposed method can be utilized to holistically train distributed models.
Models that were trained by our method achieve comparable results to a model
that was trained on the complete training dataset.
178 T. Wittkopp and A. Acker
Therestofthispaperisstructuredasfollows.First,Sect.2givesanoverview
offederatedlearningandappliedfederatedlearninginthefieldofAIOps.Second,
Sect.3 describes the our proposed method together with relevant preliminaries.
Third,Sect.4presentstheconductedcasestudyandexperimentresults.Finally,
Sect.5 concludes our work.
2 Related Work
Federated learning is a form of distributed machine learning method. Thereby,
model training is done locally within the environment of the data owner with-
out sending training data to a central server. Locality is defined within the
boundaries of the data owner’s private IT environment. Initially this concept
was proposed by McMahan et al. [8]. Instead of training data, either model
weightsorgradientsfromclientsaresenttoacentralinstancewhichaggregated
them to a holistic model during model training. Updated models are sent back
to the clients. Yang et al. [9] provide a categorization, which are vertical fed-
erated learning, horizontal federated learning and federated transfer learning.
Despite preventing direct data exchange, publications revealed possibilities to
restore training data from constantly transmitted weights or gradients, which
violates the data privacy requirement [10]. For an adversary it is possible to
recover the training dataset by using a model inversion attack [11] or determine
whether a sample is part of the training dataset by using a membership infer-
ence attack [12]. Since reconstruction of original training data is possible when
observing model changes [11,12] different privacy preserving methods are intro-
duced. These are focused on obfuscation of input data [13] or model prediction
output[14].Geyeretal.[15]applieddifferentialprivacypreservingonclientside
to realize privacy protection. Shokri and Shmatikov [6] select small subsets of
modelparameterstobesharedinordertopreventdatareconstruction.However,
model parameters or gradients still need to be shared with a central instance.
Furthermore, the requirement of a central instance is a major bottleneck for
scalability [16].
Application of federated learning in the field of AIOps is mainly focused
around anomaly [17–19] and intrusion detection [20,21]. Liu et al. [18,19] pro-
pose a deep time series anomaly detection model which is trained locally on
IoT devices via federated learning. Although not directly applied on an AIOps
related problem, the proposed method could be applied to perform anomaly
detection on time series like CPU utilization or network traffic statistics of the
device itself. Nguyen et al. [17] propose their system D¨IoT for detecting com-
promiseddevicesinIoTnetworks.Itutilizesanautomatedtechniquefordevice-
type specific anomaly detection. The unsupervised training is done via feder-
ated learning individually on each device in the IoT environment. Preuveneers
et al. [20] develop an intrusion detection system based on autoencoder mod-
els. The model parameter exchange is coupled with a permissioned blockchain
to provide integrity and prevent adversaries to alter the distributed training
process.
Decentralized Federated Learning 179
3 Decentralized Federated Learning
In this chapter we present our method for decentralized federated learning that
aims at preservation of model and data privacy. Thereby, models that were
trained on individual and partly distinct training sets are synchronized. Beside
preservingdataprivacy,thenoveltyofourmethodisthedispensabilityofmodel
parameter sharing. We illustrate the entire process of local training and global
knowledge distribution. To realize latter, the communication process between a
set of entities is described.
3.1 Problem Definition and Preliminaries
Toapplyourproposedfederatedlearningmethodweassumethefollowingsetup.
Let Φ = {φ ,φ ,...} be a set of models and E = {e ,e ,...} a set of environ-
1 2 1 2
ments. We define a model deployed in a certain environment as a tuple (φ i,e j).
AllmodelsthatperformthesametaskT intheirenvironmentarecombinedinto
a set of workers W T = {(φ i,e j)}. Each model φ i has access to locally available
training data but cannot directly access training data from other environments.
Furthermore, neither gradients nor model parameters can be shared outside of
their environment. Having a function P(T,φ i,X ej) that measures how well a
model φ i is performing the task T on data X ej from environment e j, the goal is
to synchronize the model training in a way that all models can perform well on
data from all environments:
P(T,φ 1,X e1)≈P(T,φ 1,X e2)≈...≈P(Tφ 2,X e1)≈P(T,φ 2,X e2)≈... (1)
Each model φ i is defined as a transformation function φ:Xd1 →Yd2 of a given
input x ∈ Xd1 into an output y ∈ Yd2, where d and d are the corresponding
1 2
dimensions.Sincenooriginaltrainingdatacanbesharedbetweenenvironments,
we define an input data range X˜. It allows to draw data samples x˜ ∼X˜d1 that
are restricted to the range of the original training data but otherwise are not
related to samples of the original training data set Xd1. Models can adopt the
φ(t) φ(s).
role of teachers and students Student models are directly trained on
i i
the output of teachers. We refer to a training set that is generated by a teacher
as knowledge representation, formally defined as a set of tuples:
r ={(x˜,γ(φ(t)(x˜)):x˜∼X˜d1}. (2)
i
Thereby γ is a transformation function that is applied on the teacher model
φ(s)
output. Student models are trained on the knowledge representations of
i
teachers. The objective is to minimize the loss between the output of teacher
and student models
argminL(γ(φ(t)(x˜)),φ(s)(x˜)).
(3)
i j
θ(s)
j
θ(s) φ(s)(x˜).
where are parameters of the student model
j j
180 T. Wittkopp and A. Acker
Fig.1. The process of multi-cluster learning
3.2 The Concept of Teachers and Students
For the training process itself we introduce the teacher student concept. Every
modelcanadopttheroleofastudentorteacher.Ateachermodeltrainsstudent
models byproviding aknowledge representation.First,weassumeasetofmod-
els,eachperformingthesametaskintheirownlocalenvironment.Thesemodels
are trained on the same objective but only with the locally available training
data. To prevent the sharing of training data or model parameter between envi-
ronments, models are adapting roles of teachers to train student models within
other environments. Overall, this process is realized by four steps: (1) Initial
Train, (2) adapt teacher role and build knowledge representation, (3) distribute
knowledgerepresentationand(4)adaptstudentroleandtrainonteacherknowl-
edge representation. Figure1 visualizes these steps the overall layout of each
phase. The example shows four environments A-D with locally available train-
ing sets. Initially, all models are trained on the locally available training data.
Afterthat,modelsadapttheroleofteachersandrespectivelygeneratetheknowl-
edgerepresentations.Thereby,auxiliaryinputdataaregeneratedfromthevalue
range of locally available training data. This range must be synchronized across
allenvironments.Otherwise,thereisnoconnectiontooriginallocaltrainingdata
that was used to train models within their environments. Additionally, a score
is calculated that reflects how well a model is performing on the task. Next, the
knowledgerepresentationsaredistributed.Afterreceivingaknowledgerepresen-
tation, a model checks the attached score and compares it with its local score.
Representations with lower scores are dropped. When receiving higher or equal
scores, the models adapts the role of a student and is trained on the received
knowledge representation. Through this process each model will be retrained
and updated. Note that the loss function used during the initial training can
differ from the loss function used for knowledge representation learning.
Decentralized Federated Learning 181
3.3 Loss Function
During training of student models the objective is to directly learn the trans-
formed outputs of a teacher for a given input. We utilize the tanh as the trans-
formation function to restrict teacher model outputs to the range [−1,1]. This
restriction should reduce the output value range and thus, stabilize the training
process and accelerate convergence. Therefore, the student needs a loss function
that can minimize the loss for every element from it’s own output against the
output vector of the teacher. This requires a regressive loss function. We uti-
lize the smooth L1 loss which calculates the absolute element-wise distance. If
this distance falls below 1 additionally a square operation is applied on it. It is
less sensitive to outliers than the mean square error loss and prevents exploding
gradients.
4 Evaluation
The evaluation of our method is done on the case study of log anomaly detec-
tion by utilizing a LSTM based neural network, called DeepLog [2]. DeepLog is
trained on the task of predicting the next log based on a sequence of preceding
logentries.Howeverourdecentralizedfederatedlearningmethodcanbeapplied
to any other machine learning model that is trainable via gradient descent. We
utilize the labeled HDFS data set, which is a log data set collected from the
Hadoop distributed file system deployed on a cluster of 203 nodes within the
Amazon EC2 platform [22]. This data set consists of 11,175,629 log lines that
formatotalof570,204labeledlogsequences.However,rawlogentriesarehighly
variant which entails a large number of prediction options. Thus, a preprocess-
ing is applied to reduce the number of possible prediction targets. Log messages
are transformed into templates consisting of a constant and variable part. The
constant templates are used as discrete input and prediction target. The task of
template parsing is executed by Drain [23]. We refer to the set of templates as
T.
4.1 Auxiliary Sample Generation
As described, student models are trained on auxiliary samples together with
teacher model outputs. These samples are drawn from a restricted range but
otherwiseindependentfromthetrainingdatathatwasusedtotraintheteacher.
In the conducted use case study of log anomaly detection auxiliary samples are
randomly generated as follows. Having T as the set of unique templates, an
auxiliary sample is defined as x˜ = (t i ∼ T : i = 1,2,...,w +1), where w is
the input sequence length. Template t w+1 will be used as the prediction target.
Note that templates are randomly sampled from the unique template set T.
Thus, auxiliary input samples for DeepLog are random template sequences.
182 T. Wittkopp and A. Acker
4.2 Experiment 1: Training of an Untrained Model
Inourfirstexperimentweinvestigatetheabilityoftheproposedmethodtomiti-
gatethecold-startproblem.Therefore,aDeepLogmodelistrainedontheHDFS
data.Outofthe570,204labeledlogsequencesoftheHDFSdataset,weusethe
first 4,855 normal sequences as training and the remaining 565,349 as test set.
The test set contains 16,838 anomalies and 553,366 normal sequences. The two
hyper-parameters of DeepLog were set as follow: w =10 and k =9, where w is
thewindowsizethatdeterminestheinputsequencelengthandisrequiredtogen-
erateauxiliarysamples.Furtherweusecross-entropylosstolearnadistribution
whenpredictingpossiblenextloglines.Theteachermodelusesabatchsizeof16
andwetraineditover20epochsonall4,855normallogsequences.Thetherewith
trained DeepLog model is utilized as the teacher while a completely untrained
model with the same architecture and parametrization adopts the role of a stu-
dent. The teacher performs following transformation: φ : Xw×|T| → Y|T|×R[−1,1]
to generate the knowledge representation. It takes a sequence with w one-hot
encodedtemplatesandoutputsatanhtransformedprobabilitydistributionover
allwtemplates.Differentamountsofknowledgerepresentationtuplesaretested:
{10,50,100,500,1000,5000}. The student model uses a batch size of 16 and we
traineditover10epochsforeveryknowledgerepresentationsize.Auxiliaryinput
samples are generated as described in Sect.4.1. Due to this randomness of sam-
plegeneration,werepeattheexperimentfivetimes.Figure2showstheresultsas
a boxplot, which illustrated the F1-scores for the teacher and students. Bottom
and top whiskers reflect the minimum and maximum non-outlier values. The
line in the middle of the box represents the median. The box boundaries are
the first quartile and third quartile of the value distribution. The most left bar
shows the F1-score for the teacher model, which is 0.965. This bar is a flat line,
because it represents a single value. Remaining bars are visualizing the F1-score
foreachknowledgerepresentationsize,from10to5,000.First,itcanbeobserved
that a knowledge representation size of at least 100 is required to have a decent
F1-score on the student model. As expected, the score of the student increases
with the number of used knowledge representation tuples. Due to the random
sampling F1-scored of student models trained with 100 and 500 knowledge rep-
resentationtuplesunderliehighuncertainty.The0.95confidenceintervalranges
from0.153to0.558forsize100andfrom0.313to0.805forsize500.Thestudent
model’s F1-scorebecomes increasingly stablewith higher knowledge representa-
tion sizes and reaches 0.961 within the 0.95 confidence interval of [0.943,0.958]
for size 5000. Compared to the teacher model’s F1-score of 0.965 we conclude
that our method can be utilized to mitigate the cold-start problem by training
an untrained model via knowledge representations of trained models.
4.3 Experiment 2: Federated Learning
InthisexperimentweinvestigatehowmultipleDeepLogmodelsbehaveasteach-
ersand students. It allows to train distributed models onlocally available train-
ing data and subsequently synchronize the knowledge of models. Therefore, we
Decentralized Federated Learning 183
Fig.2.ComparisonbetweenF1-scoresofteachermodelandstudentmodelsthatwere
trained on different knowledge representation sizes.
simulate8distributedHDFSsystemsbycreatingasetofuniquesequences.Out
of the 570,204 labeled log sequences of the HDFS data set, we again use the
first4,855normalsequencestogenerateuniquesequencesofsizew.Thisresults
in 4,092 unique training samples. These 4,092 training samples were evenly and
randomlysplitinto8sets,henceeverysetcontains511trainingsamples.There-
fore, 8 DeepLog models are respectively trained. The two hyper-parameters of
each DeepLog were set as follow: w =10 and k =9. To evaluate the model per-
formanceonpredictingpotentiallyunseensamples,remaining565,349sequences
are used as a joint test set.
We initially trained the 8 DeepLog models over 10 epochs with a batch size
of 16 and cross-entropy-loss as a loss function. The therewith trained DeepLog
model are utilized as teacher models for each other. Hence, all 8 nodes are
also students and where trained with the knowledge representation from the
teacherswithabatchsizeof1over5epochs.Again,wetestdifferentamountsof
knowledgerepresentationpermodel:{10,50,100,500,1000}.Notethatastudent