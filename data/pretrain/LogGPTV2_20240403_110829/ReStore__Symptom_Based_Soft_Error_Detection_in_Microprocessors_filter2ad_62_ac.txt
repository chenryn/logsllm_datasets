L1 Data
Cache
64−Entry ReOrder Buffer
Arch RAT
Arch Free List
Retire
Figure 3. Processor model diagram
4.2. Fault model and injection framework
Our fault model is a single bit ﬂip of a state element.
This fault model captures the state-inverting phenomenon of a
neutron-strike to a state-keeping transistor of a latch or RAM
cell. This model does not accurately represent faults that oc-
cur within combinational networks. However, combinational
networks have much lower sensitivities due to pulse attenu-
ation, logical masking, and latching-window masking; they
are not as problematic as state elements [14].
Our experimentation consists of a set of trials, each con-
sisting of a fault injection and determination of outcome. In
each trial, the time at which to inject a transient fault is ﬁrst
selected from a set of pre-selected random points. Then the
bit to corrupt is selected randomly across all of the eligible
state of the processor. We chose to exclude caches and predic-
tor tables from the set of eligible state since caches are easily
protected by ECC or parity and corrupt predictor table entries
cannot lead to failure. The processor model was allowed to
“warm-up” prior to each fault injection. We used seven of the
twelve SPEC2000 integer benchmarks as workloads: bzip2,
gap, gcc, gzip, mcf, parser, and vortex.
Before we proceed further, we must deﬁne “failure”. In
our previous work [28], if a fault injection propagated to ar-
chitectural state (typically the register ﬁle or memory data),
we would label the trial a failure — even if the corrupted ar-
chitectural state was eventually overwritten, masking the soft
error. We had decided to deﬁne failures in this way in an at-
tempt to make as clean a distinction as possible between soft
error masking in processor microarchitecture and in software.
On the other hand, in this work, we wish to characterize
symptoms of “interesting” soft errors — those that cause an
error at some high level of abstraction. This ideal classiﬁca-
tion is difﬁcult to accomplish, and is doubly hard when deal-
ing with a detailed low level model. As an approximation, we
classify a trial as a failure when the architectural state of the
processor model is corrupt at the end of the trial. This metric
allows a trial to have its architectural state corrupted by an
injected fault and subsequently overwritten without declaring
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:08:52 UTC from IEEE Xplore.  Restrictions apply. 
the trial a failure. Assuming that the simulations are allowed
to proceed for sufﬁcient time, the approximation can be rea-
sonably accurate. The following paragraphs describes how
trial outcomes are identiﬁed.
After each fault injection occurs, the trial is continually
monitored for up to 10,000 cycles. These comparisons occur
against both a non-injected golden execution of the latch-level
Verilog model and an architectural level simulator. The com-
parison against a golden Verilog model allows us to conclu-
sively identify if the injected error is masked by the microar-
chitecture. If the entire microarchitectural state of the proces-
sor model is equivalent to that in the golden Verilog model,
then the error has been masked. Furthermore, the comparison
against an architectural level simulator allows us to identify
if the injected error propagates to software visible state (the
ISA deﬁned register ﬁle or memory space), and if so, whether
or not the effects of the corruption eventually go away.
Actually, as described in Section 3, there is a second
means to failure besides architectural state corruption: a
“hung” processor due to deadlock or livelock. Deadlocks
and certain livelocks are detected by using a watchdog timer
that is reset as instructions retire, or ﬁnish execution, com-
mit their results, and exit the pipeline. A deadlock or livelock
is detected when the watchdog timer exceeds the maximum
expected latency between instruction retirements.
4.3. ReStore implementation
Soft error symptoms in the form of ISA deﬁned excep-
tions did not require any modiﬁcation to our Verilog model,
since exception detection was already built in. On the other
hand, symptoms in the form of high conﬁdence branch mis-
predictions require a conﬁdence predictor. To satisfy this
need, we implemented the JRS conﬁdence predictor and inte-
grated it into the fetch engine of our processor model.
As for the hardware checkpointing mechanism, we mod-
eled an ideal checkpoint/restore hardware that is capable of
creating/restoring checkpoints at zero latency. Furthermore,
the checkpoint store is protected and recoverable from soft
errors via ECC. This idealization in the processor model was
done primarily for sake of time and to concentrate the study
on the efﬁcacy of symptom-based detection. Because false
positives symptoms incur a performance overhead from ex-
traneous checkpoint restorations, we use a high level perfor-
mance model in Section 5.2 to assess this overhead.
4.4. Statistical signiﬁcance
In this study, statistical sampling was used to identify
trends in the effects of transient faults, so enough samples
must be taken such that the experimental results have statis-
tical signiﬁcance. Ideally, both the cycle in which the fault
injection occurs and the state bit that is affected would be se-
lected uniformly. While uniform sampling was implemented
for selecting the bit to corrupt, the fault injections were per-
formed on a set of about 250–300 points for each experiment.
Each experiment’s results are the compilation of 12,000–
13,000 trials. If the faults could be injected at any randomly
selected clock cycle, the overall results would have a conﬁ-
dence interval of less than 0.9% at a 95% conﬁdence level.
100%
98%
96%
94%
92%
90%
88%
25
other
latent
sdc
cfv
exception
deadlock
masked
2k
100
500
1k
50
checkpoint interval (insns)
200
Figure 4. Propagation of soft errors vs. check-
point latency
5. Experimental Results
In this section, we present the results of our experimental
evaluation of the ReStore architecture. There are three sub-
sections: an examination of faults at the microarchitectural
level, an evaluation of the ReStore architecture in terms of er-
ror coverage and impact on performance, and an assessment
of scaling trends.
5.1. Soft errors at the microarchitectural level
The objective of the experimental results described in
this subsection is to give insight into how often and how
quickly exceptions and invalid control ﬂow occur in a micro-
processor after a soft error event. To conduct this study, we
performed fault injection campaigns on a processor model,
using the fault injection methodology described in Section 4.
5.1.1. Fault injection into all state. First, we investigate the
soft error coverage obtained with perfect identiﬁcation of ex-
ceptions and incorrect control ﬂow. The results in Figure 4
are analogous to the software level injections in Figure 2. As
before, the x-axis represents latency from error injection to
symptom discovery in terms of instructions retired. The y-
axis plots the percentage of trials that fell into each of the
categories, which are described in Table 2.
Category
masked
deadlock
exception
cfv
sdc
latent
other
Description
The fault was masked or overwritten
Failure occurred in the form of a deadlock
The fault propagated into an ISA deﬁned exception
The fault caused a control ﬂow violation
Register ﬁle or memory state corruption
No failure detected yet, but fault still latent
Other - failure unlikely
Table 2. Description of subcategories
We rely on the well understood-watchdog timer mech-
anism to detect a deadlocked processor, treating a saturated
watchdog timer as a symptom as well. The deadlock cate-
gory represents the trials that are covered by the watchdog.
Masked represents trials whose injected faults are over-
written and do not cause failure. Other represents trials whose
fault injections remain latent at the end of the trial, yet did not
propagate to architectural state. Thus, within our experimen-
tal framework, it is unknown whether or not the injected fault
would result in failure. Manual inspection of trials in this cat-
egory revealed that the vast majority of the fault injections
were made into seldom used (and very likely dead) state. Ex-
amples include port conﬂict buffer entries and miss handling
registers. Thus, we assume that the trials that fall into the
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:08:52 UTC from IEEE Xplore.  Restrictions apply. 
masked and other categories at the end of simulation are not
failures. We are only interested in failures, and only 8% of
all trials (those that fall into the deadlock, exception, cfv, sdc,
and latent categories) are failures.
Note that a trial could ﬁt into multiple failure categories.
These trials are classiﬁed into only one of the categories in the
following order (from highest precedence to lowest): dead-
lock, exception, cfv, and sdc.
From the ﬁgure, we see that with a moderate check-
pointing interval of 100 instructions, approximately half of
all failures are covered by the deadlock, exception, and cfv
categories. These are failures that would be detected and re-
covered from in the ReStore architecture, if we were able to
identify control ﬂow violation symptoms perfectly. Further-
more, a large fraction of the covered failures are covered by
the easier to detect deadlock and exception categories.
5.1.2. Fault injection into latches only. We note that mi-
croprocessors are constructed with a combination of SRAM
arrays and latches or registers. Furthermore, SRAM arrays
tend to lend themselves well to protection with parity or ECC,
since they usually store data that is less transient in nature and
only a small subset of the data can be written or read at one
time. Knowing this, we ran another fault injection campaign
that only targeted pipeline latches and not SRAM arrays. Ex-
amples of structures that were implemented as SRAMs in our
processor include the register ﬁle and register alias tables.
The experiment showed that ReStore covers a larger per-
centage of failures originating from pipeline latch errors. In
the 100 instruction latency bin, the symptoms collectively
cover 75% of the failures. The key reason for this difference
is that fault injections into pipeline latches are more likely to
have an immediate impact on the instructions ﬂowing through
the machine, since the latches are “carrying” the instructions.
Injections into RAM structures like the register ﬁle and reg-
ister alias tables may sit idle for a relatively longer period of
time before being read and used by an instruction.
5.2. Evaluating the ReStore architecture
Given the results from the previous subsection, we now
evaluate the ReStore architecture, using exceptions and high-
conﬁdence branch mispredictions as detectors. First, we will
examine a baseline processor augmented with the ReStore
mechanisms. Second, we will examine the effects of ReStore
added to a processor with additional in-pipeline parity/ECC
to protect the operational state of the pipeline.
5.2.1. ReStore only. Figure 5 presents these results for fault
injections into all state. The categories are the same as previ-
ously described in Table 2, except that the cfv category now
only refers to invalid control ﬂow detected by high conﬁdence
branch mispredictions. The remainder of the old cfv category
is lumped into sdc since they do not trigger checkpoint roll-
back and detection. Thus, the deadlock, exception, and cfv
categories represent faults that are covered by ReStore while
faults in the sdc and latent categories slip through.
Overall,
the coverage provided by high conﬁdence
branch misprediction symptoms is disappointing: only 5%
100%
98%
96%
94%
92%
90%
88%
25
other
latent
sdc
cfv
exception
deadlock
masked
2k
50
100
1k
checkpoint interval (insns)
200
500
Figure 5. ReStore coverage vs. checkpoint la-
tency in the baseline pipeline
94%
93%
92%
91%
90%
89%
88%
other
latent
sdc
cfv
exception
deadlock
masked
25
50
100
200
500
1k
checkpoint interval (insns)
2k
Figure 6. ReStore coverage vs. checkpoint la-
tency in the hardened pipeline
at a 100-instruction checkpoint interval. There are three main
reasons for this. First, the JRS conﬁdence predictor is con-
servative in identifying high conﬁdence branches. A perfect
conﬁdence predictor would yield nearly twice the error cover-
age. A more accurate branch predictor would also help. Sec-
ond, about a third of of the control ﬂow violations are of the
illegal variety. Examples of illegal control ﬂow include condi-
tional branches to incorrect taken targets or branching behav-
ior from a non-branch instruction. A control ﬂow monitor-
ing watchdog [17] would capture these events. Lastly, some
of the trials that fall in the exception category also exhibited
incorrect control ﬂow. Thus, as previously discussed in Sec-
tion 3, on a machine with a smaller virtual address space or
larger application memory footprint, incorrect control ﬂow
symptoms would play a larger role.
5.2.2. ReStore + low hanging fruit.
In our previous
work [28], we grabbed the “low hanging fruit” of the soft
error problem by covering the most vulnerable portions of
our processor with parity and ECC. In particular, parity was
added to the control word latches within the pipeline, and
ECC was added to the register ﬁle and other key data stores.
In doing so, the soft error rate was signiﬁcantly reduced while
incurring an overhead of approximately 7% additional state
in the execution core. To investigate the additional coverage
that ReStore provides on this modiﬁed, hardened pipeline, we
conducted another fault injection campaign whose results are
summarized in Figure 6. The larger other category is due to
latent faults in the register ﬁle or alias table that are covered
by ECC and will not cause data corruption.
For the baseline processor (without parity or ECC), in-
jected faults propagate to some form of failure approximately
7% of the time (Figure 4). ReStore with a checkpointing in-
terval of 100 instructions brings this rate to about 3.5%. On
the other hand, adding parity and ECC to the baseline pro-
cessor reduced the rate to about 3%, and layering ReStore
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:08:52 UTC from IEEE Xplore.  Restrictions apply. 
p
u
d
e
e
p