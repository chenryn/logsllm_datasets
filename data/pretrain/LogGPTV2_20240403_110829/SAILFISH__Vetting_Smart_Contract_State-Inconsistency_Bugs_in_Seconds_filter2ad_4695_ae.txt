(cid:104)e1,δ,π∧b(cid:105)(cid:32)(cid:104)v1,δ1,π1(cid:105)
(cid:104)e2,δ,π∧¬b(cid:105)(cid:32)(cid:104)v2,δ2,π2(cid:105)
δ(cid:48) = δ ∪ δ1 ∪δ2
(cid:104)(if e0 e1 e2),δ,π(cid:105)(cid:32)(cid:104)µ(b,v1,v2),δ(cid:48),π(cid:105)
(Loop)
(If)
Fig. 10: Inference rules for value summary analysis.
summary v1. Otherwise, the rule goes with the else branch
e2 and obtains its value summary v2. Finally, if the branch
condition e0 is a symbolic variable whose concrete value cannot
be determined, then our value summary will include both v1 and
v2 together with their path conditions. Note that in all cases, the
path environment π(cid:48) needs to be computed by conjoining the
original π with the corresponding path conditions that are taken
by different branches.
B. Symbolic evaluation
Based on the rules in Figure 8, if the contract contains a pair
of statements (cid:104)s1,s2(cid:105) that match our state-inconsistency query
(e.g., reentrancy), the EXPLORER module (Section V) returns
a subgraph G (of the original ICFG) that contains statement
s1 and s2. In that sense, checking whether the contract indeed
contains the state-inconsistency bug boils down to a standard
reachability problem in G: does there exist a valid path π that
satisfies the following conditions: 1) π starts from an entry point
v0 of a public method, and 2) following π will visit s1 and s2,
sequentially. 2 Due to the over-approximated nature of our SDG
2Since TOD transfer requires reasoning about two different executions
of the same code, we adjust the goal of symbolic execution for TOD as
the following: Symbolic evaluate subgraph G twice (one uses true as pre-
condition and another uses value summary). The amount of Ether in the
external call are denoted as a1, a2, respectively. We report a TOD if a1 (cid:54)= a2.
A naive symbolic evaluation strategy is to evaluate G by
precisely following its control flows while assuming that all
storage variables are completely unconstrained ((cid:62)). With this
assumption, as our ablation study shows (Figure 11), SAILFISH
fails to refute a significant amount of false alarms. So, the key
question that we need to address is: How can we symbolically
check the reachability of G while constraining the range of
storage variables without losing too much precision? This is
where VSA comes into play. Recall that the output of our
VSA maps each storage variable into a set of abstract values
together with their corresponding path constraints in which the
values hold. Before invoking the symbolic evaluation engine, we
union those value summaries into a global pre-condition that is
enforced through the whole symbolic evaluation.
Example 6 Recall in Fig 3, the EXPLORER reports a false alarm
due to the over-approximation of the SDG. We now illustrate
how to leverage VSA to refute this false alarm.
Step 1: By applying the VSA rules in Figure 10 to the contract in
Figure 3, SAILFISH generates the summary for storage variable
mutex: {(cid:104)mutex = false, false(cid:105), (cid:104)mutex = false, true(cid:105)}. In
other words, after invoking any sequence of public functions,
mutex can be updated to true or false, if pre-condition
mutex==false holds. Here, we omit the summary of other
storage variables (e.g., userBalance) for simplicity.
Step 2: Now, by applying the symbolic checker on
the withdrawBalance function for
time,
following path condition π:
SAILFISH generates
mutex == false ∧ userBalance[msg.sender] > amount
as well as the following program state δ before invoking the
external call at Line 9: δ ={mutex(cid:55)→ true,...}
Step 3: After Step 2, the current program state δ indicates
that the value of mutex is true. Note that to execute the
then-branch of withdrawBalance, mutex must be
false. Based on the value summary of mutex in Step 1,
the pre-condition to set mutex to false is mutex = false.
However, the pre-condition is not satisfiable under the current
state δ. Therefore, although the attacker can re-enter the
withdrawBalance method through the callback mechanism,
it is impossible for the attacker to re-enter the then-branch
at Line 6, and trigger the external call at Line 9. Thus, SAILFISH
discards the reentrancy report as false positive.
first
the
the
VII. IMPLEMENTATION
Explorer. It is a lightweight static analysis that lifts the
smart contract to an SDG. The analysis is built on top of the
SLITHER [28] framework that lifts SOLIDITY source code to
its intermediate representation called SLITHIR. SAILFISH uses
SLITHER’s API, including the taint analysis, out of the box.
Reﬁner. SAILFISH leverages ROSETTE [53] to symbolically
check the feasibility of
the counter-examples. ROSETTE
provides support for symbolic evaluation. ROSETTE programs
use assertions and symbolic values to formulate queries about
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:23:40 UTC from IEEE Xplore.  Restrictions apply. 
170
program behavior, which are then solved with off-the-shelf
SMT solvers. SAILFISH uses (solve expr) query that
searches for a binding of symbolic variables to concrete values
that satisfies the assertions encountered during the symbolic
evaluation of the program expression expr.
Bug
y
c
n
a
r
t
n
e
e
R
Tool
SECURIFY
VANDAL
MYTHRIL
OYENTE
SAILFISH
SECURIFY
OYENTE
SAILFISH
Safe
Unsafe
Timeout
Error
72,149
40,607
25,705
26,924
83,171
6,321
45,971
3,708
269
2,076
10,581
1,373
59,296
0
1,211
802
1,902
1,144
62,660
3,395
59,439
23,721
77,692
19,031
3,472
7,555
VIII. EVALUATION
In this section, we describe a series of experiments that are
designed to answer the following research questions: RQ1. How
effective is SAILFISH compared to the existing smart contracts
analyzers with respect to vulnerability detection? RQ2. How
scalable is SAILFISH compared to the existing smart contracts
analyzers? RQ3. How effective is the REFINE phase in pruning
false alarms?
A. Experimental setup
Dataset. We have crawled the source code of all 91,921 contracts
from Etherscan [16], which cover a period until October 31, 2020.
We excluded 2,068 contracts that either require very old versions
(<0.3.x) of the SOLIDITY compiler, or were developed using the
VYPER framework. As a result, after deduplication, our evalua-
tion dataset consists of 89,853 SOLIDITY smart contracts. Fur-
ther, to gain a better understanding of how each tool scales as the
size of the contract increases, we have divided the entire dataset,
which we refer to as full dataset, into three mutually-exclusive
sub-datasets based on the number of lines of source code—small
([0,500)), medium ([500,1000)), and large ([1000,∞)) datasets
consisting of 73,433, 11,730, and 4,690 contracts, respectively.
We report performance metrics individually for all three datasets.
Analysis setup. We ran our analysis on a Celery v4.4.4 [19]
cluster consisting of six identical machines running Ubuntu
18.04.3 Server, each equipped with Intel(R) Xeon(R) CPU
E5-2690 PI:EMAIL GHz processor (40 core) and 256 GB memory.
Analysis of real-world contracts. We evaluated SAILFISH
against four other static analysis tools, viz., SECURIFY [54],
VANDAL [23], MYTHRIL [3], OYENTE [46], and one dynamic
analysis tool, viz., SEREUM [50]—capable of finding either
reentrancy, or TOD, or both. Given the influx of smart contract
related research in recent years, we have carefully chosen a
representative subset of the available tools that employ a broad
range of minimally overlapping techniques for bug detection.
SMARTCHECK [52] and SLITHER [28] were omitted because
their reentrancy detection patterns are identical to SECURIFY’s
NW (No Write After Ext. Call) signature.
We run all the static analysis tools, including SAILFISH,
on the full dataset under the analysis configuration detailed
earlier. If a tool supports both reentrancy and TOD bug types,
it was configured to detect both. We summarize the results
of the analyses in Table II. For each of the analysis tools and
analyzed contracts, we record one of the four possible outcomes–
(a) safe: no vulnerability was detected (b) unsafe: a potential
state-inconsistency bug was detected (c) timeout: the analysis
failed to converge within the time budget (20 minutes) (d)
error: the analysis aborted due to infrastructure issues, e.g.,
unsupported SOLIDITY version, or a framework bug, etc. For
example, the latest SOLIDITY version at the time of writing is
0.8.3, while OYENTE supports only up to version 0.4.19.
D
O
T
802
62,660
3,395
TABLE II: Comparison of bug ﬁnding abilities of tools
10,581
0
1,211
B. Vulnerability detection
In this section, we report the fraction (%) of safe, unsafe
(warnings), and timed-out contracts reported by each tool with
respect to the total number of contracts successfully analyzed
by that tool, excluding the “error” cases.
Comparison against other tools. SECURIFY, MYTHRIL,
OYENTE, VANDAL, and SAILFISH report potential reentrancy
in 7.10%, 4.18%, 0.99%, 52.27%, and 2.40% of the contracts.
Though all five static analysis tools detect reentrancy bugs,
TOD detection is supported by only three tools, i.e., SECURIFY,
OYENTE, and SAILFISH which raise potential TOD warnings
in 21.37%, 12.77%, and 8.74% of the contracts.
symbolic
MYTHRIL, being a
execution based tool,
timed out for
demonstrates obvious scalability issues: It
66.84% of the contracts. Though OYENTE is based on symbolic
execution as well, it is difficult to properly assess its scalability.
The reason is that OYENTE failed to analyze most of the
contracts in our dataset due to the unsupported SOLIDITY
version, which explains the low rate of warnings that OYENTE
emits. Unlike symbolic execution, static analysis seems to scale
well. SECURIFY timed-out for only 11.88% of the contracts,
which is significantly lower than that of MYTHRIL. When we
investigated the reason for SECURIFY timing out, it appeared that
the Datalog-based data-flow analysis (that SECURIFY relies
on) fails to reach a fixed-point for larger contracts. VANDAL’s
static analysis is inexpensive and shows good scalability, but
suffers from poor precision. In fact, VANDAL flags as many
as 52.27% of all contracts as vulnerable to reentrancy–which
makes VANDAL reports hard to triage due to the overwhelming
amount of warnings. VANDAL timed out for the least (1.56%)
number of contracts. Interestingly, SECURIFY generates fewer
reentrancy warnings than MYTHRIL. This can be attributed to
the fact that the NW policy of SECURIFY considers a write after
an external call as vulnerable, while MYTHRIL conservatively
warns about both read and write. However, SAILFISH strikes a
balance between both scalability and precision as it timed-out
only for 1.40% of the contracts, and generates the fewest alarms.
Ground truth determination. To be able to provide better
insights into the results, we performed manual analysis on a
randomly sampled subset of 750 contracts ranging up to 3,000
lines of code, out of a total of 6,581 contracts successfully
analyzed by all five static analysis tools, without any timeout
or error. We believe that the size of the dataset is in line with
prior work [51], [42]. We prepared the ground truth by manually
inspecting the contracts for reentrancy and TOD bugs using
the following criteria: (a) Reentrancy: The untrusted external
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:23:40 UTC from IEEE Xplore.  Restrictions apply. 
171
Tool
SECURIFY
VANDAL
MYTHRIL
OYENTE
SAILFISH
TP
9
26
7
8
26
Reentrancy
FP
163
626
334
16
11
TOD
FP
244
–
–
116
59
TP
102
–
–
71
110
FN
8
–
–
39
0
FN
17
0
19
18
0
TABLE III: Manual determination of the ground truth
call allows the attacker to re-enter the contract, which makes it
possible to operate on an inconsistent internal state. (b) TOD:
A front-running transaction can divert the control-flow, or alter
the Ether-flow, e.g., Ether amount, call destination, etc., of a
previously scheduled transaction.
In the end, the manual analysis identified 26 and 110 contracts
with reentrancy and TOD vulnerabilities, respectively. We then
ran each tool on this dataset, and report the number of correct
(TP), incorrect (FP), and missed (FN) detection by each tool in
Table III. For both reentrancy and TOD, SAILFISH detected all
the vulnerabilities (TP) with zero missed detection (FN), while
maintaining the lowest false positive (FP) rate. We discuss the
FPs and FNs of the tools in the subsequent sections.
False positive analysis. While reasoning about the false
positives generated by different tools for the reentrancy bug, we
observe that both VANDAL and OYENTE consider every external