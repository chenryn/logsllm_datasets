switches that interconnect the racks. The tree hierarchy causes
bandwidth further up in the tree to be scarce compared to intra-rack
bandwidth. Although it is possible to increase the cross-sectional
 0
 10
 20
 30
 40
 50
Time (s)
 60
 70
 80
 90
CPU Load
 50
 100
 150
 200
 250
 300
 350
Time(s)
Query Delay
 0
 50
 100
 150
 200
 250
 300
 350
Time(s)
Figure 17: Effects of Range Load Balancing
bandwidth, achieving full bandwidth between any two nodes is
very expensive. As a consequence, cross-sectional bandwidth us-
age is a major concern in data-center algorithm design. In this con-
text, it becomes important to understand how ROAR compares with
simple partioning in cross-sectional bandwidth usage.
Distributed algorithms can exploit the physical network structure
to minimize cross-sectional bandwidth usage. We can either place
data replicas in a small number of racks, or attempt to run a query
in as few racks as possible. These are mutually exclusive at large
scale, so we can typically optimize cross-sectional bandwidth usage
for only one of the two.
Google’s web search generates too little cross-sectional band-
width to be of concern, mostly because it has many data-centers
worldwide [9]. However, if all the servers were in the same data-
center, would this be an issue?
Our estimates of bandwidth usage6 for running queries at Google
give 1Gbps to serve 1000 queries per second, with p = 1000. This
assumes no caching; in reality, cache hit ratios can be on the or-
der of 30-60% [10] so the bandwidth overhead is much smaller:
500Mbps. This is modest in comparison with updates to the index:
if r is 80, and the entire 10TB index is redistributed daily[8] to
80 replicas, the total bandwidth needed is around 75Gbps. Even if
6The query keywords and options take 50 bytes at most; the reply is 80 bytes long if
it includes 10 64 bit document IDs. Altogether, this takes 130 bytes.
300p
Delay (ms)
CPU Usage
Match Delay (ms)
Match Variability
Schedule Delay (ms)
Serialize Delay (ms)
250
341
500
1132
1000
100
997
2183
10% 12% 15% 19%
20
430
4
1.2
23
1.17
8.3
155
160
1.5
3.4
24
80
2.5
9.2
50
Table 2: ROAR performance running on 1000 servers in EC2
only incremental updates are sent, it would make sense to optimize
update bandwidth.
Google could place one cluster of nodes (i.e. nodes with the
same data) in as few racks as possible, say l. To update the data,
each item needs to be sent to a single machine in each rack; it will
be then propagated locally within the rack. Assuming incoming
update trafﬁc is D, updates will use lD cross-sectional bandwidth;
in the example above, and assuming 40 servers per rack, this could
result in 2Gbps of cross-sectional trafﬁc.
ROAR can similarly use physical placement of servers to mini-
mize update cost, by assigning servers in the same rack to be con-
secutive on the ring. In this case, each update will be pushed to l or
(l + 1) racks. ROAR will generate (l + 1)D cross-sectional trafﬁc
for each update, which is marginally more than Google.
To implement this optimization in ROAR, it sufﬁces to use the
peer-to-peer like update algorithm we have described: the updates
for an object are pushed to the server responsible for that object’s
ID. This server forwards to its successor, and so forth, as long as
the successor is within the replication range. Almost all of these
hops will be intra-rack.
5.7 Large Scale Deployment
Small-scale tests on our testbed show that ROAR works, but we
also wish to see how it scales. ROAR stores r replicas of each data
item, and splits each query p ways while ensuring p · r = n. This
is the lower bound for all distributed rendezvous algorithms, so we
are conﬁdent that ROAR’s basic costs scale well. Simulation indi-
cates that the algorithms should scale, but there are always practical
surprises when scaling a system up signiﬁcantly. Our immediate
concern is the frontend scheduler, which is centralized.
We brieﬂy acquired a thousand servers from Amazon EC2 [1].
These are virtualized servers, each with a 1.7Ghz CPU and 1.7GB
of memory, plus a large local hard drive. Our front-end server is
instantiated on a more powerful machine with eight virtual proces-
sors and 17GB of memory.
Basic performance of PPS on a single EC2 instance is roughly
half that on our HEN servers because the CPU is slower: a query
of one million metadata items takes eight seconds.
We created a larger dataset of 5 million entries, and replicated
it at r = 10 on 1000 servers. We then ran one query per second
at different p values (min p for correctness is 100). Table 2 sum-
marizes the results. Query delay initially decreases as p goes from
100 to 250, but then increases after that. Average CPU utilization
increases with p as we expect: it roughly doubles when p goes from
100 to 1000. As the CPUs are not overloaded, the u-shaped delay
curve is intriguing.
We proﬁled the frontend server to see how local computation af-
fects latency. Scheduling delay increases roughly with n log n and
reaches 25ms on average when n = 1000. The time to compose
and send the 500 byte query from the frontend application also in-
creases with n: it takes 125ms on average to send a message to all
the 1000 servers. Although not negligible these delays can be eas-
p=100
p=250
p=500
p=1000
 100
 10
 1
 0.1
)
s
c
e
s
(
y
a
e
D
l
 0.01
 0.001
Query  Delay
Query  Delay
Frontend Delay
Frontend Delay
 0
 50  100
 50
 100
 50
 100
 50
 100
Time (s)
Figure 18: Delay Breakdown as seen at Frontend Server
ily reduced in an optimized implementation and are not a scaling
concern. They are not large enough to explain the u-shaped curve.
We then examined the query matching times on the ROAR nodes.
The mean performance is as expected: delays decrease with 1/p.
However, larger values of p exhibit higher variability in run-times:
variability7 increases from 1.2 to 4 when p goes from 100 to 1000.
To nail the cause of high delays observed, Figure 18 shows a
real-time breakdown of frontend delays and query delays for vari-
ous values of p. Many queries ﬁnish very quickly when p = 1000,
just after all the data has been sent. Variable round-trip delays made
us wonder if we were bottlenecked on bandwidth, despite the low
transmit rate of 4Mb/s. Brief tests with iperf showed this was not
the case, but they did reveal a mean drop probability of roughly 1
in 1000 packets, presumably caused by competing uses of EC2. As
we use TCP between the frontend and each ROAR server, a drop
on any ﬂow delays the whole query. The query rate to each server
is low, so TCP’s fast retransmit cannot kick in and a lost packet has
to wait for a TCP retransmit timeout. The large delays spikes in
Figure 18 indicate losses are bursty too, making matters worse. A
simple technique might mitigate these losses: the frontend should
resend unﬁnished query parts as soon as most of the query has com-
pleted. As least for our application, this implies that UDP might be
a more appropriate transport for ROAR.
Our large-scale deployment gives us conﬁdence that ROAR itself
scales well. It also provided insight in the effects of p, beyond the
ones we observed in our small scale testbed. In particular, larger
p values greatly exacerbates any inherent variability in runtimes,
increasing overall query delays. This strengthens our belief that
dynamically adapting p is advantageous.
6. RELATED WORK
There are many proposed distributed rendezvous solutions in the
literature [5, 12, 24, 25, 13]; almost all offer a ﬁxed trade-off be-
tween the partitioning and replication levels. The Google cluster
architecture [5] is the classical cluster-based solution, with a ﬁxed
r-p trade-off.
Another solution is the Load Balancing Matrix (LBM) [13]. LBM
is the only solution we are aware of that allows changing r dynam-
ically. LBM maps clusters on a DHT: server i from cluster j is
mapped to the server in charge of hash(i, j). When repartitioning,
7deﬁned as the ratio between the ﬁnish time of the slowest node and the average ﬁnish
time of all nodes running a query.
301LBM inherits most of the problems of the Google approach, but
does not require changing the network structure. However, LBM
has load balancing problems as virtual cluster servers are mapped
using consistent hashing onto the Chord ring: with high probabil-
ity, the busiest server will host log n/ log log n cluster servers. To
ﬁx this, each server has to insert itself many times on the ring (as
many as log n/ log log n), which signiﬁcantly increases distributed
rendezvous costs for large networks.
There are a few randomized solutions: Ferreira et al. [12] use
random walks for both object storing and for queries, while Bub-
bleStorm [25] uses bubbles to speed up object storing and query
execution. These algorithms are built for peer to peer systems so
have great resilience yet their operating costs are much higher (for
instance with BubbleStorm p · r = 4n).
Structured Overlays and P2P Search. Much research has gone
into executing queries on structured overlays, including keyword
search [23, 22] and range queries [7]. These solutions are applica-
ble to many problems. However, when queries are complex, con-
tent distribution is skewed, or content is unavailable (as with en-
crypted search), content-based solutions do not work well. ROAR’s
content-agnostic approach is a better solution in many such cases.
Distributed Databases. Research in distributed databases aims to
optimize execution of powerful relational queries in a distributed
setting [21, 26, 16]. ROAR is much simpler: it is just a “select”
operation executed in a distributed manner on a single table. In ef-
fect, ROAR can be used as a tool underlying traditional databases
to optimize access to large tables with poor indexing options. At
a conceptual level, ROAR is similar to the exchange operator pro-
posed by Graefe et al. to provide extensible query execution [15].
Distributed Computation. There are many algorithms for dis-
tributing computation among machines [6, 4, 11]. Google’s MapRe-
duce [11] offers a simpliﬁed, functional programming model that
hides parallelization from the programmer. ROAR offers a weaker
programming abstraction, equivalent to the “map” operation, but
differs in its handling of data objects: while MapReduce moves
data to the servers performing the computation, ROAR will run the
computation on enough servers such that all the data objects are
visited without actually moving the data objects. Instead, ROAR
allows the application to change r, which controls the minimum
number of servers that must be visited. Not copying data for every
query allows ROAR to save bandwidth and obtain smaller delays.
7. CONCLUSIONS
The performance of web search engines is heavily inﬂuenced by
the partitioning level p, which controls how an ensemble of servers
handle queries and store a web index. This parameter is the primary
control that determines search latency, and so has a huge impact on
the usability of distributed search systems. Despite this and the fact
that p should be continuosuly adapted according to the system’s
load in order to achieve optimal performance, search engines such
as Google rely on simple distributed rendezvous algorithms that do
not allow for dynamic reconﬁguration of p.
In this paper we introduced ROAR, a novel distributed rendezvous
algorithm that allows on-the-ﬂy re-conﬁguration of p at minimal
cost while still servicing queries. Further, ROAR can add and re-
move servers without stopping the system, cope with temporary
and permanent server failures, and provide very good load-balancing
even in the face of servers having heterogeneous hardware capabil-
ities.
We have provided experimental results that support these claims
and that show that the ability to change partitioning dynamically
has many beneﬁts, from allowing the network to cope with load
ﬂuctuations gracefully to reducing bandwidth and energy costs. We
derived these results by implementing a privacy-preserving search
application that used ROAR as its underlying algorithm, and run-
ning experiments on a 47-server dedicated testbed and on a 1000-
server conﬁguration using Amazon’s EC2. Our experiments show
that ROAR works well in practice: it can cope with failures and it
balances load well. Given a target query delay, ROAR can auto-
matically reconﬁgure the network to achieve that delay while min-
imizing other costs.
In the future, we hope to test ROAR more on large clusters with
thousands of nodes using a more robust transport, build smarter
optimization criteria, and to see how ROAR can be used in other
search applications.
8. REFERENCES
[1] Amazon Elastic Compute Cloud. http://aws.amazon.com/ec2/.
[2] Google Docs. http://docs.google.com/.
[3] The Google Search Query - a technical look.
http://www.webmasterworld.com/google/3694079.htm.
[4] R. H. Arpaci-Dusseau, E. Anderson, N. Treuhaft, D. E. Culler, J. M. Hellerstein,
D. Patterson, and K. Yelick. Cluster i/o with river: making the fast case
common. In Proc. Workshop on I/O in parallel and distributed systems, 1999.
[5] L. A. Barroso, J. Dean, and U. Holzle. Web search for a planet: The google
cluster architecture. Micro, IEEE, 23, 2003.
[6] J. Bent, D. Thain, A. C. Arpaci-Dusseau, R. H. Arpaci-Dusseau, and M. Livny.
Explicit control a batch-aware distributed ﬁle system. In NSDI, 2004.
[7] A. R. Bharambe, M. Agrawal, and S. Seshan. Mercury: supporting scalable
multi-attribute range queries. SIGCOMM Comput. Commun. Rev., 34(4), 2004.
[8] M. Cutts. Gadgets, Google, and SEO: Explaining algorithm updates and data
refreshes, Dec. 2006.
[9] J. Dean. Personal Communication. Google.
[10] J. Dean. Challenges in Building Large Scale Information Systems. Keynote
Presentation at ACM WSDM, 2009.
[11] J. Dean and S. Ghemawat. MapReduce: Simpliﬁed data processing on large
clusters. In Proc. OSDI, 2004.
[12] R. A. Ferreira, M. K. Ramanathan, A. Awan, A. Grama, and S. Jagannathan.
Search with probabilistic guarantees in unstructured peer-to-peer networks. In
Proc. P2P, 2005.
[13] J. Gao and P. Steenkiste. Design and evaluation of a distributed scalable content
discovery system. IEEE JSAC, 22, Jan. 2004.
[14] S. Ghemawat, H. Gobioff, and S.-T. Leung. The google ﬁle system. In SOSP
’03: Proceedings of the nineteenth ACM symposium on Operating systems
principles, pages 29–43, New York, NY, USA, 2003. ACM Press.
[15] G. Graefe and D. L. Davison. Encapsulation of parallelism and
architecture-independence in extensible database query execution. IEEE Trans.
Softw. Eng., 19(8), 1993.
[16] B. Kröll and P. Widmayer. Distributing a search tree among a growing number
of processors. SIGMOD Rec., 23(2), 1994.
[17] M. Mitzenmacher. The power of two choices in randomized load balancing.
IEEE Trans. Parallel Distrib. Syst., 12(10):1094–1104, 2001.
[18] N. Patel. Learning from google’s data centers.
http://www.pronetadvertising.com/, 2006.
[19] C. Raiciu and D. S. Rosenblum. Enabling conﬁdentiality in content-based
publish/subscribe infrastructures. In Proc. Securecomm, 2006.
[20] I. Stoica, R. Morris, D. Karger, F. Kaashoek, and H. Balakrishnan. Chord: A
scalable Peer-To-Peer lookup service for internet applications. In Proc.
SIGCOMM, 2001.
[21] M. Stonebraker, P. M. Aoki, W. Litwin, A. Pfeffer, A. Sah, J. Sidell, C. Staelin,
and A. Yu. Mariposa: a wide-area distributed database system. The VLDB
Journal, 5(1), 1996.
[22] C. Tang, Z. Xu, and S. Dwarkadas. Peer-to-peer information retrieval using
self-organizing semantic overlay networks. In Proc. Sigcomm, 2003.
[23] C. Tang, Z. Xu, and M. Mahalingam. psearch: information retrieval in
structured overlays. SIGCOMM Comput. Commun. Rev., 33(1), 2003.
[24] W. W. Terpstra, S. Behnel, L. Fiege, J. Kangasharju, and A. Buchmann. Bit
zipper Rendezvous—Optimal data placement for general P2P queries. In Proc.
EDBT Workshop on Peer-to-Peer Computing and DataBases, 2004.
[25] W. W. Terpstra, J. Kangasharju, C. Leng, and A. P. Buchmann. Bubblestorm:
resilient, probabilistic, and exhaustive peer-to-peer search. In Proc. SIGCOMM,
2007.
[26] F. Tian and D. J. DeWitt. Tuple routing strategies for distributed eddies. In
Proc. VLDB, 2003.
302