(conv 1). Preliminary results showed that activations do not provide much information
to attack the models. Thus, we did not simulate attacks on the 2D-slice-mean models
with activations as features. The best attack accuracies of 78.05 and 83.04 for attacking
3D-CNN and 2D-slice-mean model were achieved by using prediction, labels, and gradients
of parameters close to the output layer. To train classiﬁers for membership inference in
this section required having access to the private samples from the training set, which is
limiting. In Appendix E, we have discussed attacks with access to only the training set
distribution and not the training samples.
4.2. Membership Inference Attacks on Federated Training
We consider three diﬀerent federated learning environments consisting of 8 learners and
investigate cases where malicious learners attack the community model. The community
model is the aggregated result of learners’ local models and a malicious learner may use it
to extract information about other learners’ training samples. In this scenario, a malicious
learner can learn an attack model by leveraging its access to the community models of all
federation rounds and its local training dataset; we simulate attacks using this information
3. We show results with layer parameters close to the input or output layers as these have relatively fewer
parameters and attack models are easily trained. Other layers had a larger number of parameters, making
it hard to learn the attack model.
6
Membership Inference Attacks on Deep Regression Models for Neuroimaging
Data distribution
3D-CNN
2D-slice-mean
Uniform & IID
60.06 (56)
Uniform & Non-IID 61.00 (28)
Skewed & Non-IID 64.12 (25)
58.11 (56)
60.28 (29)
63.81 (24)
Figure 2: Increasing attack vulnera-
bility per federation round.
Table 2: Average attack accuracies on federated
trained models. Numbers in parentheses indicate
median successful attacks over 5 multiple runs.
(see also Section 3.2). The model vulnerability is likely to increase with more training
iterations and hence we used features derived from the community models received during
the last ﬁve federation rounds, and each learner uses its private samples to learn the attack
model. Each learner may try to do membership inference attacks on any of the other seven
learners, resulting in 56 possible attack combinations. An attack is considered successful if
accuracy is more than 50%, which is the random prediction baseline.
Table 2 shows the average accuracy of successful attacks and the total number of suc-
cessful attack instances of learner-attacker pairs (in parentheses) across all possible learner-
attacker pairs (56 in total). For a more detailed analysis on a per-learner basis, see Ap-
pendix B. We empirically observed that the success rate of the attacks is susceptible to data
distribution shifts. In particular, distribution shift agnostic features like gradient magni-
tudes can lead to more successful attacks (count wise) when data distribution across learners
diﬀers. For the results shown in Table 2 and Figure 2, we used all available features (i.e.,
gradient magnitudes, predictions, labels, and gradients of last layers).
We also observe that the overall attack accuracies are lower than the centralized coun-
terpart discussed in Section 4.1. This drop can be attributed to the following: a) As we
show in Section 4.3, attack accuracies are highly correlated with overﬁtting. Federated
learning provides more regularization than centralized training and reduces overﬁtting but
does not eliminate the possibility of an attack. b) Federated models are slow to train, but as
the model is trained for more federation rounds, the vulnerability increases (see Figure 2).
Table 2 only presents an average case view of the attacks. However, we observe that attack
performance depends on the data distribution of the learner-attacker pair. When the local
data distribution across learners is highly diverse, i.e., Skewed & Non-IID attack accuracies
can be as high as 80% for speciﬁc learner-attacker pairs (see Appendix B).
4.3. Possible Defenses
Various approaches have been proposed to mitigate the membership inference attacks di-
rectly. These approaches are based on controlling overﬁtting (Truex et al., 2018; Salem et al.,
2019) and training data memorization (Jha et al., 2020) or adversarial training (Nasr et al.,
2018). We evaluate diﬀerentially private machine learning as one of the defenses.
Diﬀerential privacy (Dwork and Roth, 2014) is often touted as a panacea for all privacy-
related problems. We evaluate the eﬀect of training models with privacy guarantees on
membership inference attacks and model performance, measured as mean absolute error
7
0510152025303540Federation Round505254565860Attack AccuracyFederation Round vs. Attack Accuracy for Uniform & IID dist.2D-slice-mean3D-CNNMembership Inference Attacks on Deep Regression Models for Neuroimaging
(a) Attack accuracy vs. Model performance
(b) Attack accuracy vs. Overﬁtting
Figure 3: Diﬀerential privacy reduces membership inference attacks. Fig. (b) shows that
the eﬀectiveness of membership inference attack is correlated with overﬁtting. Error bars
are generated by bootstrapping the test set 5 times using 1000 samples. Results with R2
as the measure of model performance are shown in Appendix D.
in the centralized setup. To train the models with diﬀerential privacy, we used DP-SGD
algorithm of Abadi et al. (2016) which works by adding Gaussian noise to the gradient
updates from each sample4. We varied the noise magnitude to achieve points on the trade-oﬀ
curve of Figure 3(a). Models trained with diﬀerential privacy can signiﬁcantly reduce attack
accuracy, but this is achieved at the cost of a signiﬁcant drop in model performance. We also
visualize the relation between overﬁtting, measured by train, and test performance diﬀerence
and attack vulnerability of the models trained with diﬀerential privacy in Figure 3(b). We
see that overﬁtting is highly correlated with attack accuracy, indicating that these attacks
may be prevented by avoiding overﬁtting up to some extent.
5. Discussion
While deep learning presents great promise for solving neuroimaging problems, it also brings
new challenges. Deep learning is intrinsically data-hungry, but the bulk of neuroimaging
data is distributed around the world in private repositories. With classic machine learning
approaches like linear regression, model sharing and meta-analysis could be used to pool
insights without sharing data. Unfortunately, neural networks are capable of completely
memorizing training data, so that sharing a model may be just as bad as sharing the
private data itself.
In this paper, we demonstrated a practical proof-of-concept attack
for extracting private information from neural networks trained on neuroimaging data.
We showed that attacks with a high success rate persist under various settings, including
a realistic, distributed, federated learning scheme explicitly designed to protect private
information. Although concerning, our preliminary study of attacks and defenses suggest
beneﬁts to solving this problem that go beyond data privacy. Because attacks exploit
diﬀerences in model performance on training data and unseen test data, a successful defense
must also lead to more robust neuroimaging models whose out-of-sample performance does
not signiﬁcantly diﬀer from in-sample performance. Hence, even if data privacy were not a
concern, further study of protection against membership attacks may inspire neuroimaging
models that generalize better to new patients.
4. For a brief description of diﬀerential privacy and diﬀerential private training details, see Appendix D.
8
3.03.54.04.55.05.56.06.5Mean Absolute Error (year)50556065707580Attack AccuracyRandom prediction baseline2D-slice-mean3D-CNN0.00.51.01.52.0Overfitting (year)50556065707580Attack accuracy2D-slice-mean3D-CNNMembership Inference Attacks on Deep Regression Models for Neuroimaging
Acknowledgments
This research was supported by DARPA contract HR0011-2090104. PL and PT were sup-
ported by the NIH under grant U01 AG068057 and by a research grant from Biogen, Inc.
This research has been conducted using the UK Biobank Resource under Application Num-
ber 11559.
References
Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal
Talwar, and Li Zhang. Deep Learning with Diﬀerential Privacy. In Proceedings of the
2016 ACM SIGSAC conference on computer and communications security, pages 308–
318, 2016.
Vishnu M Bashyam, Guray Erus, Jimit Doshi, Mohamad Habes, Ilya M Nasrallah, Monica
Truelove-Hill, Dhivya Srinivasan, Liz Mamourian, Raymond Pomponio, Yong Fan, et al.
MRI signatures of brain age and disease over the lifespan based on a deep brain network
and 14 468 individuals worldwide. Brain, 143(7):2312–2324, 2020.
Paolo Bellavista, Luca Foschini, and Alessio Mora. Decentralised Learning in Federated
Deployment Environments: A System-Level Survey. ACM Computing Surveys (CSUR),
54(1):1–38, 2021.
Amanda Bischoﬀ-Grethe, I Burak Ozyurt, Evelina Busa, Brian T Quinn, Christine
Fennema-Notestine, Camellia P Clark, Shaunna Morris, Mark W Bondi, Terry L Jerni-
gan, Anders M Dale, et al. A technique for the deidentiﬁcation of structural brain MR
images. Human brain mapping, 28(9):892–903, 2007.
Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman,
Vladimir Ivanov, Chloe Kiddon, Jakub Koneˇcn`y, Stefano Mazzocchi, H Brendan McMa-
han, et al. Towards federated learning at scale: System design.
arXiv preprint
arXiv:1902.01046, 2019.
Ellyn R Butler, Andrew Chen, Rabie Ramadan, Trang T Le, Kosha Ruparel, Tyler M
Moore, Theodore D Satterthwaite, Fengqing Zhang, Haochang Shou, Ruben C Gur, et al.
Statistical Pitfalls in Brain Age Analyses. bioRxiv, 2020.
Centers for Medicare & Medicaid Services. The Health Insurance Portability and Account-
ability Act of 1996 (HIPAA). Online at http://www.cms.hhs.gov/hipaa/, 1996.
Christopher A Choquette Choo, Florian Tramer, Nicholas Carlini, and Nicolas Papernot.
Label-Only Membership Inference Attacks. arXiv preprint arXiv:2007.14321, 2020.
James H Cole, Rudra PK Poudel, Dimosthenis Tsagkrasoulis, Matthan WA Caan, Claire
Steves, Tim D Spector, and Giovanni Montana. Predicting brain age with deep learning
from raw imaging data results in a reliable and heritable biomarker. NeuroImage, 163:
115–124, 2017.
9
Membership Inference Attacks on Deep Regression Models for Neuroimaging
David W Craig, Robert M Goor, Zhenyuan Wang, Justin Paschall, Jim Ostell, Michael
Feolo, Stephen T Sherry, and Teri A Manolio. Assessing and managing risk when sharing
aggregate genetic variant data. Nature Reviews Genetics, 12(10):730–736, 2011.
Cynthia Dwork and Aaron Roth. The Algorithmic Foundations of Diﬀerential Privacy.
in Theoretical Computer Science, 9(3–4):211–407, 2014. ISSN
®
Foundations and Trends
1551-305X. doi: 10.1561/0400000042.
Xinyang Feng, Zachary C. Lipton, Jie Yang, Scott A. Small, and Frank A. Provenzano.
Estimating brain age based on a uniform healthy population with deep learning and
structural magnetic resonance imaging. Neurobiology of Aging, 91:15–25, 2020.
ISSN
0197-4580.
Katja Franke and Christian Gaser. Ten Years of BrainAGE as a Neuroimaging Biomarker
of Brain Aging: What Insights Have We Gained? Frontiers in Neurology, 10:789, 2019.
ISSN 1664-2295.
Umang Gupta, Pradeep Lam, Greg Ver Steeg, and Paul Thompson. Improved Brain Age
Estimation with Slice-based Set Networks. In IEEE International Symposium on Biomed-
ical Imaging (ISBI), 2021.
Nils Homer, Szabolcs Szelinger, Margot Redman, David Duggan, Waibhav Tembe, Jill
Muehling, John V Pearson, Dietrich A Stephan, Stanley F Nelson, and David W Craig.
Resolving individuals contributing trace amounts of DNA to highly complex mixtures
using high-density SNP genotyping microarrays. PLoS Genet, 4(8):e1000167, 2008.
Bargav Jayaraman, Lingxiao Wang, David Evans, and Quanquan Gu. Revisiting member-
ship inference under realistic assumptions. arXiv preprint arXiv:2005.10881, 2020.
Sumit Kumar Jha, Susmit Jha, Rickard Ewetz, Sunny Raj, Alvaro Velasquez, Laura L
Pullum, and Ananthram Swami. An Extension of Fano’s Inequality for Characterizing
Model Susceptibility to Membership Inference Attacks. arXiv preprint arXiv:2009.08097,
2020.
Peter Kairouz and H. Brendan McMahan. Advances and Open Problems in Federated
Learning. Foundations and Trends® in Machine Learning, 14(1):–, 2021. ISSN 1935-
8237. doi: 10.1561/2200000083.
Pradeep K Lam, Vigneshwaran Santhalingam, Parth Suresh, Rahul Baboota, Alyssa H
Zhu, Sophia I Thomopoulos, Neda Jahanshad, and Paul M Thompson. Accurate brain
age prediction using recurrent slice-based networks.
In 16th International Symposium
on Medical Information Processing and Analysis. International Society for Optics and
Photonics, 2020.
Peeter Laud and Alisa Pankova.
of Advantage in Guessing or Approximating Sensitive Attributes.
arXiv:1911.12777, 2019.
Interpreting Epsilon of Diﬀerential Privacy in Terms
arXiv preprint
10
Membership Inference Attacks on Deep Regression Models for Neuroimaging
Junghye Lee, Jimeng Sun, Fei Wang, Shuang Wang, Chi-Hyuck Jun, and Xiaoqian Jiang.
Privacy-Preserving Patient Similarity Learning in a Federated Environment: Develop-
ment and Analysis. JMIR medical informatics, 6(2):e20, 2018.
Klas Leino and Matt Fredrikson. Stolen Memories: Leveraging Model Memorization for
Calibrated White-Box Membership Inference. In 29th {USENIX} Security Symposium
({USENIX} Security 20), pages 1605–1622, 2020.
Wenqi Li, Fausto Milletar`ı, Daguang Xu, Nicola Rieke, Jonny Hancox, Wentao Zhu, Maxi-
milian Baust, Yan Cheng, S´ebastien Ourselin, M Jorge Cardoso, et al. Privacy-Preserving
Federated Brain Tumour Segmentation. In International Workshop on Machine Learning
in Medical Imaging, pages 133–141. Springer, 2019.
Xiaoxiao Li, Yufeng Gu, Nicha Dvornek, Lawrence H. Staib, Pamela Ventola, and James S.
Duncan. Multi-site fMRI analysis using privacy-preserving federated learning and domain
adaptation: ABIDE results. Medical Image Analysis, 65:101765, 2020. ISSN 1361-8415.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Ar-
cas. Communication-Eﬃcient Learning of Deep Networks from Decentralized Data. In
Artiﬁcial Intelligence and Statistics, pages 1273–1282. PMLR, 2017.
Mikhail Milchenko and Daniel Marcus. Obscuring Surface Anatomy in Volumetric Imaging
Data . Neuroinformatics, 11(1):65–75, 2013.
Karla L Miller, Fidel Alfaro-Almagro, Neal K Bangerter, David L Thomas, Essa Yacoub,
Junqian Xu, Andreas J Bartsch, Saad Jbabdi, Stamatios N Sotiropoulos, Jesper LR
Andersson, et al. Multimodal population brain imaging in the UK Biobank prospective
epidemiological study. Nature neuroscience, 19(11):1523–1536, 2016.
M. Nasr, R. Shokri, and A. Houmansadr. Comprehensive Privacy Analysis of Deep Learn-
ing: Passive and Active White-box Inference Attacks against Centralized and Federated
Learning. In 2019 IEEE Symposium on Security and Privacy (SP), pages 739–753, 2019.
Milad Nasr, Reza Shokri, and Amir Houmansadr. Machine Learning with Membership
In Proceedings of the 2018 ACM SIGSAC
Privacy using Adversarial Regularization .
Conference on Computer and Communications Security, pages 634–646, 2018.
Han Peng, Weikang Gong, Christian F Beckmann, Andrea Vedaldi, and Stephen M Smith.
Accurate brain age prediction with lightweight deep neural networks. Medical Image
Analysis, 68:101871, 2021.
Apostolos Pyrgelis, Carmela Troncoso, and Emiliano De Cristofaro. Knock Knock, Who’s
There? Membership Inference on Aggregate Location Data. CoRR, abs/1708.06145,
2017.
Nicola Rieke, Jonny Hancox, Wenqi Li, Fausto Milletari, Holger Roth, Shadi Albarqouni,
Spyridon Bakas, Mathieu N Galtier, Bennett Landman, Klaus Maier-Hein, et al. The
future of digital health with federated learning. npj Digital Medicine, 3(119), 2020.
11
Membership Inference Attacks on Deep Regression Models for Neuroimaging
Abhijit Guha Roy, Shayan Siddiqui, Sebastian P¨olsterl, Nassir Navab, and Christian
Wachinger. BrainTorrent: A Peer-to-Peer Environment for Decentralized Federated.
arXiv preprint arXiv:1905.06731, 2019.
Ahmed Salem, Yang Zhang, Mathias Humbert, Mario Fritz, and Michael Backes. ML-Leaks:
Model and Data Independent Membership Inference Attacks and Defenses on Machine
Learning Models. In Network and Distributed Systems Security Symposium 2019. Internet
Society, 2019.
Nakeisha Schimke and John Hale. Quickshear Defacing for Neuroimages. In Proceedings
of the 2nd USENIX conference on Health security and privacy, pages 11–11. USENIX
Association, 2011.
Micah J Sheller, G Anthony Reina, Brandon Edwards, Jason Martin, and Spyridon Bakas.
Multi-institutional Deep Learning Modeling Without Sharing Patient Data: A Feasibility
Study on Brain Tumor Segmentation. In International MICCAI Brainlesion Workshop,
pages 92–104. Springer, 2018.
Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership Infer-
ence Attacks Against Machine Learning Models. In 2017 IEEE Symposium on Security
and Privacy (SP), pages 3–18, 2017.
Santiago Silva, Boris A Gutman, Eduardo Romero, Paul M Thompson, Andre Altmann,
and Marco Lorenzi. Federated Learning in Distributed Medical Databases: Meta-Analysis
of Large-Scale Subcortical Brain Data. In 2019 IEEE 16th international symposium on
biomedical imaging (ISBI 2019), pages 270–274. IEEE, 2019.
Santiago Silva, Andre Altmann, Boris Gutman, and Marco Lorenzi. Fed-BioMed: A General
Open-Source Frontend Framework for Federated Learning in Healthcare.
In Domain
Adaptation and Representation Transfer, and Distributed and Collaborative Learning,
pages 201–210. Springer, 2020.
Liwei Song and Prateek Mittal. Systematic Evaluation of Privacy Risks of Machine Learning
Models. arXiv preprint arXiv:2003.10595, 2020.