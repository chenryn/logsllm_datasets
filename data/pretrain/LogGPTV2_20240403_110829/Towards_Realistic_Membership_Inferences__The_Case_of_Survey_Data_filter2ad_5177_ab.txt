### 1. Introduction and Preliminary Results

Preliminary results indicated that activations do not provide substantial information for attacking the models. Consequently, we did not simulate attacks on the 2D-slice-mean models using activations as features. The highest attack accuracies of 78.05% and 83.04% for the 3D-CNN and 2D-slice-mean models, respectively, were achieved by utilizing predictions, labels, and gradients of parameters close to the output layer. Training classifiers for membership inference in this section required access to private samples from the training set, which is a limiting factor. In Appendix E, we discuss attacks with access only to the training set distribution, but not the actual training samples.

### 2. Membership Inference Attacks on Federated Training

We examined three different federated learning environments, each consisting of eight learners, to investigate scenarios where malicious learners attack the community model. The community model is the aggregated result of the learners' local models, and a malicious learner may use it to extract information about other learners' training samples. In this scenario, a malicious learner can train an attack model by leveraging its access to the community models of all federation rounds and its local training dataset. We simulated attacks using this information.

We focused on layer parameters close to the input or output layers, as these have relatively fewer parameters, making it easier to train the attack model. Other layers had a larger number of parameters, which made it more challenging to learn the attack model.

### 3. Results and Analysis

#### Data Distribution and Attack Vulnerability

- **3D-CNN Model:**
  - Uniform & IID: 60.06% (56)
  - Uniform & Non-IID: 61.00% (28)
  - Skewed & Non-IID: 64.12% (25)

- **2D-slice-mean Model:**
  - Uniform & IID: 58.11% (56)
  - Uniform & Non-IID: 60.28% (29)
  - Skewed & Non-IID: 63.81% (24)

**Figure 2: Increasing attack vulnerability per federation round.**

**Table 2: Average attack accuracies on federated trained models. Numbers in parentheses indicate median successful attacks over five multiple runs.**

The model's vulnerability tends to increase with more training iterations. Therefore, we used features derived from the community models received during the last five federation rounds, and each learner used its private samples to train the attack model. Each learner could attempt membership inference attacks on any of the other seven learners, resulting in 56 possible attack combinations. An attack was considered successful if the accuracy exceeded 50%, which is the random prediction baseline.

**Table 2** shows the average accuracy of successful attacks and the total number of successful attack instances (in parentheses) across all possible learner-attacker pairs (56 in total). For a more detailed analysis on a per-learner basis, see Appendix B. Empirically, we observed that the success rate of attacks is sensitive to data distribution shifts. Features like gradient magnitudes, which are agnostic to distribution shifts, can lead to more successful attacks when the data distribution across learners differs. For the results in Table 2 and Figure 2, we used all available features (i.e., gradient magnitudes, predictions, labels, and gradients of last layers).

We also noted that the overall attack accuracies in federated settings are lower than those in centralized settings, discussed in Section 4.1. This drop can be attributed to:
- Federated learning provides more regularization than centralized training, reducing overfitting but not eliminating the possibility of an attack.
- Federated models are slower to train, but as the model is trained for more federation rounds, the vulnerability increases (see Figure 2).

**Table 2** presents an average case view of the attacks. However, attack performance depends on the data distribution of the learner-attacker pair. When the local data distribution across learners is highly diverse (e.g., Skewed & Non-IID), attack accuracies can be as high as 80% for specific learner-attacker pairs (see Appendix B).

### 4. Possible Defenses

Various approaches have been proposed to mitigate membership inference attacks, including:
- Controlling overfitting (Truex et al., 2018; Salem et al., 2019)
- Reducing training data memorization (Jha et al., 2020)
- Adversarial training (Nasr et al., 2018)

We evaluated differential privacy (Dwork and Roth, 2014) as a defense mechanism. Differential privacy is often considered a comprehensive solution for privacy-related problems. We assessed the effect of training models with privacy guarantees on membership inference attacks and model performance, measured as mean absolute error in the centralized setup. To train the models with differential privacy, we used the DP-SGD algorithm (Abadi et al., 2016), which adds Gaussian noise to the gradient updates from each sample. We varied the noise magnitude to achieve points on the trade-off curve shown in Figure 3(a).

Models trained with differential privacy can significantly reduce attack accuracy, but this comes at the cost of a significant drop in model performance. We also visualized the relationship between overfitting (measured by the difference between train and test performance) and attack vulnerability of the models trained with differential privacy in Figure 3(b). Overfitting is highly correlated with attack accuracy, indicating that these attacks may be prevented by avoiding overfitting up to some extent.

### 5. Discussion

While deep learning offers great promise for solving neuroimaging problems, it also introduces new challenges. Deep learning is inherently data-hungry, but most neuroimaging data is distributed globally in private repositories. Traditional machine learning approaches like linear regression allowed for model sharing and meta-analysis without sharing data. However, neural networks can completely memorize training data, making model sharing as risky as sharing the private data itself.

In this paper, we demonstrated a practical proof-of-concept attack for extracting private information from neural networks trained on neuroimaging data. We showed that attacks with high success rates persist under various settings, including a realistic, distributed, federated learning scheme designed to protect private information. Although concerning, our preliminary study of attacks and defenses suggests benefits beyond data privacy. Because attacks exploit differences in model performance on training and unseen test data, a successful defense must also lead to more robust neuroimaging models whose out-of-sample performance does not significantly differ from in-sample performance. Hence, even if data privacy were not a concern, further study of protection against membership attacks may inspire neuroimaging models that generalize better to new patients.

### 6. Acknowledgments

This research was supported by DARPA contract HR0011-2090104. PL and PT were supported by the NIH under grant U01 AG068057 and by a research grant from Biogen, Inc. This research has been conducted using the UK Biobank Resource under Application Number 11559.

### 7. References

[References listed here as in the original text, formatted consistently]

---

This revised version aims to improve clarity, coherence, and professionalism while maintaining the essential content and structure of the original text.