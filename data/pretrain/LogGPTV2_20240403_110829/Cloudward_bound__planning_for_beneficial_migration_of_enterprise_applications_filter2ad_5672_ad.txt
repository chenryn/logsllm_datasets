unwanted Internet trafﬁc. That is, unauthorized trafﬁc between the
local data center and the cloud must be ﬁltered before it traverses
the wide-area Internet link when possible. For instance, in Fig. 4(a),
we would like trafﬁc from f e1 to be ﬁltered in the cloud itself,
rather than by a policy in the local data center. This criterion is im-
portant as it reduces unnecessary communication between the data
centers and hence the associated costs.
4.2 ACL migration algorithm
We now present our algorithm for reconﬁguring reachability poli-
cies when entities in a local data center (LDC) are migrated to
a cloud data center (CDC). The migration algorithm proceeds in
three phases as shown in Fig. 5: (i) Rnew is derived by infer-
ring Rold from the original network, and revising ACLs to reﬂect
changes to address assignments; (ii) Rnew is partitioned – this
step helps prevent unwanted Internet trafﬁc, as we will describe
in § 4.2.3; and (iii) ACLs in each partition of Rnew are installed
to localize unwanted trafﬁc while ensuring correctness. While we
focus on migration from one LDC to a single CDC, the algorithm
can be easily generalized to consider multiple LDCs and CDCs.
ACL configuration per (interface,direction)Deriving RnewSection 4.3.1Router FIBsPartitionRnewExtract SubmatrixLocate PlacementGenerate ACLsRnewSection 4.3.3 Install RnewSection 4.3.2Hybrid Cloud TopologyLDCRouterConfigsRnew1. Migration scenario2. Address mapping m248Figure 6: Rnew and the hybrid cloud topology based on the migration
scenario in Fig. 4.
As we will see, having two disjoint sets of entities enables us to
place ACL a on the edge-cut-set between them. We refer to such
a place-able group of cells as a submatrix. For scaling reasons, we
prefer to keep the number of submatrices extracted for Fa(DC)
small. Formally, let Fa(DC) denote the ﬁlter domain of ACL a
within Z DC. We extract a minimal set of submatrices covering all
OD-pairs ∈ Fa(DC) by repeatedly removing the largest submatrix
from Fa(DC) until no OD-pair remains. We denote the kth sub-
matrix as Fa(DC, k). For example, Fig. 6(a) highlights the two
submatrices (circled) extracted for Fa2 (LDC).
Locating Placement: For each submatrix Fa(DC, k) extracted,
the corresponding ACL a needs to be placed inside DC, such that
packets going from the source entities to the destination entities will
encounter an instance of the ACL no matter which physical path
they take. To achieve this, we place the ACL along an edge-cut-
set between the associated sets of source and destination entities in
the new topology. An edge-cut-set consists of a set of placement
locations {l}, each specifying an interface and the direction of traf-
ﬁc (inbound or outbound) to which an ACL placed on that interface
applies.
In general, each cloud provider has its own infrastructure, which
inﬂuences where ACLs can be placed in the CDC. For instance,
some cloud providers may grant cloud users the ability to perform
VM-level ﬁltering [1], or offer proprietary techniques to specify
ACLs for groups of VMs [3,10]. For example, Fig. 6 shows the hy-
brid cloud topology based on the migration scenario in Fig. 4. Eli-
gible placement locations in the LDC and the CDC are marked by
circles and diamonds, respectively. Note that the virtual router (VR)
in the cloud is not eligible for placement.
To determine the placement locations, we compute the minimum
edge-cut-set, only allowing links incident to eligible placement lo-
cations to be part of the edge-cut-set. We achieve this using polyno-
mial algorithms for ﬁnding the minimum cut in a ﬂow network [16].
If both ends of a link are eligible locations, we place ACL a on the
interface closer to the source entities.
Generating ACL Conﬁguration: Finally, the appropriate ACL
conﬁgurations must be generated for each placement location. Di-
rectly installing each ACL may accidentally ﬁlter other trafﬁc. For
example, after migration, assume ACL a2 is placed as shown in
Fig. 7(a), to block trafﬁc from IN T to f e2. Fig. 7(b) depicts the
rules of ACL a2 prior to migration. If a2 is installed unaltered as in
Fig. 7(a), trafﬁc from f e1 to f e2 is inadvertently blocked, violat-
ing the correctness criteria. Note that this was not an issue prior to
migration (see Fig. 4), as trafﬁc from f e1 to f e2 did not encounter
a2 in that scenario.
Consider the placement of submatrix Fa(DC, k) of ACL a. We
introduce two methods, scoping and isolation, for generating the
correct ACL conﬁguration a(l) for each location l on which ACL
a is placed. We deﬁne trafﬁc domain Dl at location l to be all
possible OD-pairs that can traverse through l. Scoping ensures that
Figure 7: Applying ACL a2 without changing its conﬁguration inadver-
tently blocks allowed trafﬁc. One of the corrected conﬁgurations should be
used instead.
a(l) only ﬁlters trafﬁc ∈ Dl∩Fa(DC, k). Isolation ensures that
Dl∩Fa(DC) is allowed by ACL a. For example, the scoped and
isolated versions of ACL a2 are shown in Fig. 7(c). We compute
both the scoped and isolated versions of ACL a based on location l,
pick a(l) as the smaller of the two, and merge it with any co-located
ACL to generate the ﬁnal ACL conﬁguration.
5 Evaluation
This section presents results evaluating the importance and effec-
tiveness of our model in planning hybrid cloud layouts. Our eval-
uations were conducted using (i) a real application made available
as part of the Windows Azure SDK (§5.1), and (ii) a real Enterprise
Resource Planning (ERP) application deployed in a large campus
network (§5.2). We present evaluations of the ACL migration algo-
rithm in §5.3 based on security policies from the campus network.
5.1 Planned migration of a simple enterprise application
Our evaluations were conducted by deploying an application pro-
vided as part of the Windows Azure SDK on the Windows Azure
cloud platform. We describe the application data ﬂow, the setup of
our cloud test-bed, and discuss how we instrumented the applica-
tion to obtain parameters needed by our model. We then present
results evaluating the change in response time when the application
is deployed across multiple data centers in a conﬁguration recom-
mended by our planned migration approach. These results help us
experimentally validate the effectiveness of our model in meeting
constraints on changes in application response time.
Application abstraction and data-ﬂow: The application that we
refer to as Thumbnail involves a user uploading a picture to a server.
The server creates a thumbnail version of the picture and returns
it to the user. The application consists of three components - a
web front-end (FE), a Windows Azure service for temporary stor-
age known as a Blob (BE), and a worker role (BL). Fig. 8 illus-
trates the data-ﬂow. A user initiates a transaction by uploading a
picture and sending a request to FE (t0). The FE pushes a notiﬁca-
tion message into a queue while writing the image to the BE (t1-a
and t1-b). Note that since these operations occur in parallel and
the time taken for t1-b dominates, we exclude the queue from our
abstraction. The BL reads the message from the queue, gets the im-
age from the BE (t2), creates a thumbnail, and stores the thumbnail
in BE (t3). Once the thumbnail is generated, the FE retrieves the
thumbnail (t4) and sends a response page, along with the thumb-
nail, back to the user (t5).
Cloud test-bed setup: While the original application was imple-
mented to only run on one data center, we have revised it so each
component may span multiple data centers. We create a setup in-
volving two different Azure data centers located in geographically
different locations - one in north-central United States (DCN ), and
LDCVRCDC(a) Placing ACL a2unchanged violates correctnessBE2BRARARARARBE1fe2Internet (INT)a2permit BE1 FEdeny any any(b)ACL a2Scopeddeny INTfe2permit any anyIsolatedpermit fe1 fe2 permit BE1 FEdeny any any(c)Corrected ACL a2for installationa2FEfe1249HHHHH
V
D
105%
110%
150%
200%
125%
150%
175%
no bound
1/1/1, $20024
1/1/1, $20024
1/1/1, $20024
1/1/1, $20024
1/1/1, $20024
1/3/2, $36367
1/3/3, $53647
1/3/3, $53647
1/1/1, $20024
1/2/2, $36836
1/3/3, $53647
2/3/3, $55224
1/1/1, $20024
2/2/2, $38413
1/3/3, $53647
3/3/3, $56801
Figure 8: Data ﬂow of thumbnail application.
the other in south-central United States (DCS). We view DCN and
DCS respectively as the local and cloud data centers. We picked
a host (I) located in geographical proximity to DCN as seeing per-
formance representative of an internal enterprise user. We picked
about 20 Planetlab hosts (O) scattered around the United States,
and assumed they corresponded to external users.
Deriving model parameters: We have instrumented the applica-
tion to measure transaction sizes, component service times and var-
ious communication delays. Using the setup above, we run the ap-
plication over 50 times by uploading pictures from a typical user al-
bum. The mean (stddev) time for I to upload a picture to DCN and
DCS were measured to be 2966 (742) and 4681 (818) msec, while
the similar values from O to DCN and DCS were 3422 (619), and
3494 (498) msec. The mean (stddev) time to download the thumb-
nail from both clouds was 505 (208) msec for I and 455 (244) msec
for O. Before migration, the entire application resides in the same
data center. The mean (stddev) delay is 718 (169) msec from FE to
BE, 251 (160) msec from BE to BL, and 66 (16) msec from BL to
BE. The mean (stddev) of the service time of BL is 655 (60) msec.
The service time of FE and BE is negligibly small. We also mea-
sured the transfer delays of the original images and the thumbnails
between the clouds over multiple runs. The mean (stddev) trans-
fer time of the original images and thumbnails between the two
data centers was 2044 (161) msec and 96 (17) msec respectively.
Finally, the transaction size between each component pair was the
average size of the image or the thumbnail. These values were 1500
KBytes and 4 Kbytes, respectively.
Modeling migration beneﬁts and communication costs: We as-
sume that migrating servers to the cloud can reduce costs by a factor
of 7 for compute-class servers, and 5 for storage-class servers, as
suggested in [14]. We leverage the Amazon EC2 cloud pricing [1]
to calculate the cost of running a server in the cloud. We consider
a scenario where a total storage space of 1TB is required, and 1000
I/O transactions per second are involved. The resulting beneﬁts of
migrating a compute-class server is $1577 per year, and the bene-
ﬁts of migrating a storage-class server is $17280 per year. Finally,
based on [1], we assume that it costs $170 for exchanging 1TB of
data with the cloud.
Migration strategies recommended by our model: Table 1 sum-
marizes the results obtained using our model with the ﬂexible rout-
ing approach for a scenario where (i) 80% of users are internal, and
the rest are external; and (ii) there are four servers in each compo-
nent. Each row (column) corresponds to a constraint on the mean
delay (variance). Each cell shows the migration strategy recom-
mended by our model, as well as the yearly savings in US dollars.
For example, a setting (V=150% and D=110%) means a variance
bound of 150% of the original variance, and a mean delay bound of
110% of the original mean delay. For the same setting, 1 FE server,
3 BL servers, and 2 BE servers should be migrated for a maximum
yearly savings of $36367. We have run our algorithm for other user
mixes, but omit the results for lack of space.
We make several observations. First, in most cases, more BL
Table 1: Recommendations of planned migration approach for the Thumb-
nail application. The dollar amounts shown are savings per year.
Figure 9: Validating model recommendations: CDF of user response time
(in seconds) before and after migration.
and BE servers are migrated than FE servers. Further, the num-
ber of BL and BE servers migrated is often the same. This is be-
cause (i) moving a BE server achieves more beneﬁts than moving
FE servers; and (ii) by moving the same number of BL servers as
BE servers, all pictures that were sent to the BE servers can be pro-
cessed by BL servers in the same location. Second, variance plays
an important role in the recommendations. For instance, for a delay
bound of 110% (row 2), the best migration strategy varies for differ-
ent constraints on the variance. Third, we checked how the trafﬁc
was routed between components, and conﬁrmed that our approach
routed trafﬁc intelligently. For example, external user transactions
were always routed to the cloud to the extent possible.
Validating recommendations through cloud deployment: We
deployed the recommended migration strategy for the scenario cor-
responding to 80% internal users, with constraints of up to 10%
increase in mean delay and 50% in variance. The strategy recom-
mends migrating one FE, two BE, and three BL servers. Further, as
generated by the ﬂexible routing approach, all requests from exter-
nal users followed the path hF ER, BER, BLRi. 6.25% of local
requests followed the path hF ER, BER, BLRi, whereas the re-
maining requests were split evenly among paths hF EL, BEL, BLLi,
hF EL, BEL, BLRi, and hF EL, BER, BLRi. Here F EL and
F ER respectively denote the local and remote components of F E,
and similar notations are used for other components.
Fig. 9 presents a CDF of user response times obtained using the
cloud test-bed for the scenarios prior to and after migration. The
values were obtained over 100 user transactions, with internal and
external users simulated using a host in geographical proximity
to the data center, and using Planetlab hosts as described before.
While response times after migration increase, as is expected, the
increase is still within acceptable limits. For instance, the 90%ile of
response times increased from 6.5 to 7.7 seconds, while the 99%ile
increased from 7.4 to 8.4 seconds. We observed an increase of 17%
in mean delay, and 12% in variance. At 5% level of signiﬁcance, a
t-test of difference in the expected response times did not provide
sufﬁcient evidence to conclude that the mean response time had
increased more than 10% after migration.
5.2 Planning migration of a campus ERP application
We next present a model of a real Enterprise Resource Planning
(ERP) application used in a large university with tens of thousands
of students, and several thousand faculty and staff. We use the
application as a case study to illustrate the beneﬁts of a hybrid ap-
WorkerRoleWebRoleIISWebRoleIISWebRoleIISLoad BalanceWorkerRoleWorker 0.65 (0.06)blobblobt0InternalExternalt1-at1-bt2t3BEt4t5BLFEQueue 0 0.2 0.4 0.6 0.8 1 2 4 6 8 10 12 14 16 18 20CDFresponse time (s)before migrationafter migration250Delay
Bound
115%
w/ policy
115%
110%
120%
130%
Yearly
Savings
$14,102
$37,769
$27,789
$43,592
$57,763