1
0
0
6
2
5
1
1
3
ST
2
0
0
1
2
3
5
6
3
1
SF
4
4
3
2
0
2
0
0
1
0
In Fig. 7(a) we characterize the exit code distribution based
on the number of nodes. We can clearly observe the following.
8
 0 0.2 0.4 0.6 0.8 1[1,10][11,20][21,40][41,80][81,160][161,320][321,640][641,1280][1281,2560][2561,...]Exit Code Distribution# TasksTimeoutBugKillIORASUnknownSIGILLSIGTRAPSIGFPE 0 0.2 0.4 0.6 0.8 1[1,10][11,20][21,40][41,80][81,160][161,320][321,640][641,1280][1281,2560][2561,...]Exit Code Distribution# TasksTimeoutBugKillIORASUnknownSIGILLSIGTRAPSIGFPE 0 0.2 0.4 0.6 0.8 151210242048409681921228816384245763276849152Exit Code Distribution# NodesTimeoutBugKillIORASUnknownSIGILLSIGTRAPSIGFPE 0 0.2 0.4 0.6 0.8 1prodsbackfprodcprodlprodtR.bcR.pmSC13pbackf2trainExit Code DistributionQUEUE NAMETimeoutBugKillIORASUnknownSIGILLSIGTRAPSIGFPErelatively short wall time (50% of the jobs request wall
times of less than 1 h) while 5% of the jobs have relatively
large requests for wall time (≥6 h). The real execution
lengths are shorter than the job queuing lengths, since the
execution lengths must be bounded in the requested wall time;
in contrast, the queuing time can be fairly long (see Fig. 9(a))
because it has no upper bounds.
(a) Queuing Time
(b) Execution Time
Fig. 8. General Job Queuing/Execution Time
We divide the normal jobs vs. failed jobs in terms of queuing
length and execution length, respectively, in Figs. 9(a) and (b).
Based on these ﬁgures, we formulate the following takeaway.
(Takeaway 14): Long jobs (with either long queuing time
or long execution time) tend to have more failure events
during their executions. In absolute terms, with the same
percentage of jobs from among normal jobs and failed jobs, the
queuing time and execution time of the former are generally
only 2
3 as long as those of the latter, respectively.
3 and 1
CONTINGENCY TABLE WITH EXE. TIME AND EXIT CODE
TABLE XIV
NM
91050
22965
44432
44753
28009
24861
16646
4727
821
16
TO
1104
2664
4691
10992
6762
10302
12391
5290
922
2
BG
16133
1933
2763
2993
1253
994
1441
204
14
1
KL
4550
1719
1382
959
695
731
491
141
40
0
IO
3088
105
95
73
69
49
41
9
5
0
RS
71
101
66
69
89
93
80
57
15
0
UK
302
28
63
57
8
13
12
6
0
0
SI
235
5
2
2
0
3
0
0
0
0
ST
208
9
15
0
1
2
0
0
0
0
SF
23
12
10
0
1
3
3
2
0
0
XXXXXXXX
exetime
exit
[0,10m)
[10m,20m)
[20m,40m)
[40m,1h20m)
[1h20m,2h40m)
[2h40m,5h20m)
[5h20m,10h40m)
[10h40m,21h20m)
[21h20m,42h40m)
[42h40m,...)
Fig. 10. Exit Code Distribution Based on Execution Times
16): The best-ﬁtting distributions of failed job’s execution
lengths are Weibull, Pareto, inverse Gaussian, LogNormal,
and Erlang/exponential, for RAS-based job failures (i.e.,
related to system reliability), bug-based job failures (due
to code’s bugs), I/O-based job failures (due to user’s
misoperations), and timeout job failures (due to wrong
submission setting), respectively. Our distribution-ﬁtting
analysis provides the speciﬁc best-ﬁt distributions for speciﬁc
job failures. Our approach differs from traditional best-ﬁt
distribution analysis focusing mainly on either potential fatal
events [2] or overall failure rates based on different types
of applications [8]. Our analysis can help fault
tolerance
researchers emulate job failure events or intervals.
(a) Queuing Time
(b) Execution Time
Fig. 9. Queuing/Execution Times of Normal Jobs vs. Failed Jobs
We present
in Table XIV a detailed breakdown of the
exit codes in consecutive log-scale execution time intervals.
(Takeaway 15): Unlike the job failure features observed
based on execution scales, a long job tends to fail and the
root cause is attributed to user behaviors such as ‘timeout,”
“bugs,” and “misoperations” instead of system reliability.
For instance, if a job with an execution time in the range
[21h20m, 42h40m) fails, it is likely because of “timeout,” as
shown in Fig. 10. The intuitive explanation is that the relatively
long jobs’ execution time is harder to estimate accurately. In
this case, the users are recommended to reserve more wall
time for their jobs, such that the time-out can be mitigated.
In addition to characterizing the exit code distributions
for job execution length, we explore the best-ﬁt distribution
type for job length (or time to interruption) based on both
overall job failures and different exit codes, by leveraging the
MLE method. The distribution ﬁtting results (top 3 from 20
distributions) are presented in Fig. 11 and Fig. 12. (Takeaway
(a) Normal Jobs
(b) Abnormal/Failed Jobs
Fig. 11. MLE Fitting of Job Runtime Distribution
C. Features Based on Job I/O Behavior
By combining the Darshan I/O behavior log and Cobalt job-
scheduling log, we explored the potential correlations between
job failures and I/O behaviors, by generating a contingency
table (Table XV) and the fractions among exit codes (Fig.
13). We observe that (Takeaway 17): the total number of
jobs based on the read/written bytes exhibits a bimodal or
trimodal distribution. Speciﬁcally, the majority of jobs each
read data in the range of [4GB,8GB) and [128GB,256GB),
respectively; and the majority of jobs write data in the range
[2GB,4GB) and [32GB,128GB), respectively. Such a ﬁnding
may help system administrators more deeply understand fail-
ure issues related to I/O behavior or identify the root causes
based on the I/O behaviors in their daily diagnosis.
9
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 2 4 6 8 10CDFTime (in hours)Requested Wall TimeQueuing Time 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 2 4 6 8 10CDFTime (in hours)Requested Wall TimeExecution Time 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000CDFQueued Time (in hours)Normal JobsFailed Jobs 0.4 0.5 0.6 0.7 0 2 4 6 8 10 12 14 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 10 20 30 40 50 60 70 80 90CDFExecution Time (in hours)Normal JobsFailed Jobs 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 0 0.2 0.4 0.6 0.8 1[0,10m)[10m,20k)[20m,40m)[40m,1h20m)[1h20m,2h40m)[2h40m,5h20m)[5h20m,10h40m)[10h40m,21h20m)[21h20m,42h40m)[42h40m,...)Exit Code DistributionExecution TimeTimeoutBugKillIORASUnknownSIGILLSIGTRAPSIGFPE 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 10 20 30 40 50 60 70 80CDFJob Length (in hours)Real Normal Job Length1.Weibull2.GammaDist3.ChiSquare 0.9 0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99 3 4 5 6 7 8 9 10 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 10 20 30 40 50 60 70 80 90CDFJob Length (in hours)Real Failed Job Length1.Gamma2.Pearson63.Weibull 0.8 0.82 0.84 0.86 0.88 0.9 0.92 0.945 6 7 8 9 10 11 12 13 14 15(a) RAS-based job failure
(b) Bug-based job failure
(a) Distr. with # Bytes Read
Fig. 13. Exit Code Distribution Based on # Bytes Read/Written
(b) Distr. with # Bytes Written
(c) I/O-based job failure
(d) Timeout job failure
Fig. 12. MLE Fitting of Runtime Distribution Based on Job Failure Types
CONTINGENCY TABLE WITH # READ/WRITTEN BYTES AND EXIT CODE
TABLE XV
XXXXXXXX
# bytes
exit
[0,512MB)
[512MB,1GB)
[1GB,2GB)
[2GB,4GB)
[4GB,8GB)
[8GB,16GB)
[16GB,32GB)
[32GB,64GB)
[128GB,256GB)
[256GB,512GB)
[0,512MB)
[512MB,1GB)
[1GB,2GB)
[2GB,4GB)
[4GB,8GB)
[8GB,16GB)
[16GB,32GB)
[32GB,64GB)
[64GB,128GB)
[128GB,256GB)
NM
TO
BG
KL
IO
UK
RS
13164
2903
438
273
1685
576
426
148
88
61
80842
10010
3319
1673
3128
1374
433
424
387
2115
Total # Bytes Read vs. Exit Code
258588
4494
256
8066
482
22812
5
19278
34
24400
6
16467
7169
18
74
6304
27
13707
6437
93
Total # Bytes Written vs. Exit Code
277555
11588
12322
30906
14901
5281
8892
10577
11452
3555
79155
1822
3408
2940
3257
3772
2664
2160
2406
1254
14504
445
802
690
1001
427
553
400
443
118
4508
23
16
67
100
209
219
179
44
154
1788
128
58
55
82
91
67
9
6
8
2139
11
6
39
14
38
13
7
9
10
125
4
9
1159
30
1
1
0
23
0
1218
4
8
4
112
2
2
1
1
1
144
0
0
0
169
8
4
49
12
0
23
127
0
0
0
33
192
3
7
7
VI. ANALYSIS OF CORRELATION BETWEEN JOB
EXECUTIONS AND SYSTEM RELIABILITY
In this section, we explore how the system’s fatal events
affect
job executions statistically, as well as the practical
mean time to interruption. We ﬁrst extract the 24 message
IDs that pertain to one or more failed jobs. The detailed
meaning/message of each message ID can be referenced in
the IBM Blue Gene/Q ras book [34]. As mentioned in Section
IV, the message ID is the key ﬁeld determining the nature
of a group of events in the RAS log. In Table XVI we list
all 24 message IDs and the corresponding fractions. Detailed
information about the message IDs can be found in the IBM
Blue Gene/Q RAS book [34]. From Table XVI, we formulate
the following takeaway. (Takeaway 18): (1) the message IDs
of the job-affected RAS events follow a Pareto-similar
principle, or 75/25 rule. Speciﬁcally, from among all the
jobs that failed because of system reliability, about 75% were
Category
Blue Gene/Q compute card
Software Error
TABLE XVII
BREAKDOWN OF RAS EVENTS BASED ON CATEGORY/COMPONENT
percent
62.1%
21.53%
11.39%
4.52%
0.32%
0.16%
Message Unit (MU)
Generic Card/Board
Bulk Power Supply
percent
78.16%
16.22%
2.81%
2.65%
0.16%
Machine Controller on Service Node
Control System on Service Node
Component
FIRMWARE
A Kernel Panic
Compute Node Kernel
Memory Unit