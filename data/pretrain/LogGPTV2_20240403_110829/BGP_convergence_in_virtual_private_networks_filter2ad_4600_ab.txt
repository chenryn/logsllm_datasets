ﬁnishes processing the withdrawal from P E2 and sends its
own withdrawal to RR4, RR4 realizes that there is no path
in its table, and it sends a withdrawal to P E3 at T = 4.7
second. In this process, RR4 in eﬀect “explores” an internal
path (RR2,P E2) that is already invalidated by the failure
(CE1/P E2 session failure) that has triggered the conver-
gence, even though the AS path remains the same (7675
5555). This process of exploring internal paths in iBGP is
similar to the eBGP AS path exploration problem [5], but,
to the best of our knowledge, path exploration has never
been reported in the context of iBGP or MPLS VPN.
3.2 Route Invisibility Problem
At T = 4.7 second, RR4 withdraws the primary path, but
the backup path is not sent until T = 9.7 second. This addi-
tional delay is caused by the route invisibility problem: the
backup path is “invisible” until the primary path is with-
drawn. Before the failure, RR1 prefers the primary path
over the backup path, thus selects the primary path and
sends it to P E1. P E1 compares the primary path and its
own path, and selects the primary path as its best path be-
cause it is shorter than P E1’s own path (the backup path).
Because a router can only announce its best path, P E1 has
to send a withdrawal to RR1 to withdraw the backup path.
That is, the backup path is “visible” only to P E1, and is “in-
visible” to the rest of AS 7675. Note that the invisibility of
the backup path could be by design (e.g., the VPN customer
wants all the traﬃc to go through the primary path when
the primary path is available), or it can be unintentional.
In order for P E1 to announce the backup path to RR1 and
then to the rest of the network, P E1 needs to ﬁrst learn from
RR1 that the primary path is no longer valid. But, similar
to RR4, P E1 experiences the path exploration process. RR1
sends (RR2,P E2) to P E1 at around T = 1 second, and then
a withdrawal at around T = 5 second. P E1 then selects the
new best path, i.e., the backup path (5555 5555 5555). The
route is propagated to RR1, then RR4 and P E3. Because
there has been no update on session P E1 → RR1 and session
RR1 → RR4, the backup route is propagated over these
two sessions with little delay. However, the MRAI timer on
session RR4 → P E3 is on due to the update at T = 4.7
second in Table 1. Therefore, the backup path cannot be
propagated to P E3 until T = 9.7 second.
The route invisibility problem impacts the convergence
time in that routing updates needs to go through several
iBGP hops to make the backup path available to the net-
work. First, the withdrawal of the primary path needs to
propagate through the reﬂector hierarchy to reach the PE
which has the backup path. Second, the backup path is
propagated through the reﬂector hierarchy to reach the rest
of the PEs in the network.
Note that in the testbed scenario described here, there is
no background BGP updates except on the session between
the router emulator and RR2. When there are background
BGP updates on each session, as will be the case in any oper-
ational network, the per-neighbor MRAI timer is constantly
“on” when a new update arrives at a router. Then each
iBGP hop can cause a delay up to M = 5 seconds. In [10],
we show that the worst case Tlong and Tdown convergence
delay (failure detection and iBGP route propagation) are
(205 + n∗ 5) seconds and (190 + n∗ 5) seconds, respectively,
in the studied network.
4. MEASUREMENT METHODOLOGY
In this section, we present a methodology to accurately
measure the BGP convergence time in a VPN network
through correlating data from several sources (BGP, syslog,
conﬁg, and PE forwarding table) that are readily available
from operational networks. We will describe our data col-
lection, event clustering, and event classiﬁcation algorithms.
4.1 Correlating Data Sources
The provider network we studied collects both VPN BGP
updates and syslog messages and has one level of route re-
ﬂectors that form a full-mesh. One BGP collector is set
up as the client of two route reﬂectors to collect the BGP
updates from them. Among the many types of syslog mes-
sages, the layer 1, layer 2, and layer 3 messages are rele-
vant to our study. The available information in these syslog
messages, as well as that in BGP updates, are shown in Ta-
ble 2. The BGP updates have preﬁx and RD information,
but the syslog messages have only the interface/session in-
formation. Therefore, to correlate the syslog messages with
the BGP messages, we use the route conﬁgurations and PE
forwarding table (also shown in Table 2) to build the follow-
ing two mappings: (router, interf ace) → RD:preﬁxes and
(router, neighbor ip, vrf ) → RD:preﬁxes.
Data Sources
1. BGP updates
2. BGP updates
3. syslog messages
4. syslog messages
5. syslog messages
6. router conﬁgurations
7. router conﬁgurations
8. PE Forwarding table
types
announcement
withdrawal
layer-1: LINK-UPDOWN
layer-2: LINEPROTOL-UPDOWN
layer-3: BGP-ADJCHANGE
vrf conﬁgurations
eBGP session/interface conﬁgurations
daily dump
available information
timestamp, preﬁx, rd, aspath, cluster-list, originator...
timestamp, preﬁx, rd
timestamp, router, per-router seqnum, interface, status
timestamp, router, per-router seqnum, interface, status
timestamp, router, per-router seqnum, neighbor ip, vrf, status
router, vrf, rd
router, interface, neighbor ip
router, preﬁx, vrf, nexthop ip, nexthop interface
Table 2: Available data from the provider network
4.2 Event Clustering
Because BGP path computation for diﬀerent RD:preﬁxes
are done separately, we conduct the measurement for dif-
ferent RD:preﬁx separately. Thus we convert each syslog
message into m messages, one for each of the m aﬀected pre-
ﬁxes. We then merge the converted syslog messages and the
BGP update stream, and sort the combined stream based
on timestamp. The clocks at the PEs and the BGP col-
lector are NTP-synchronized, thus the timestamp indicates
the relative timing of each message accurately enough for
our purposes. We then cluster messages into events.
4.2.1 Existing BGP event clustering work
Earlier measurement work on the BGP convergence delay
have focused on IPv4 BGP. These works can be classiﬁed
into two types: beacon-based active measurements [5, 6, 8],
in which controlled events were injected into the Internet
from a small number of beacon sites (thus the event start-
ing time is known), and time window-based passive mea-
surement (in which the time-window value is derived based
on the observed update inter-arrival time) [12, 7, 14, 4, 9].
4.2.2 Our clustering algorithm
Our clustering algorithm has two major diﬀerences from
existing measurement work. First, we determine the event
beginning time using syslog as the beacon because the sys-
log messages indicate the timing of the “root cause” of an
event. Therefore, its accuracy is very close to that of the
scheduled event beginning time in the active measurement.
On the other hand, similar to the traditional passive mea-
surement, it is more representative than the active measure-
ment because it can be used to measure all the actual events
triggered by the PE-CE link/session changes.
The second diﬀerence relates to how to decide the time
window for determining the end of an event. Existing mea-
surement work typically derives the time window based on
the distribution of the measured update inter-arrival time.
In this work, we calculate the value of the time window based
on the various timer settings and the iBGP topology in the
studied network. The analytical results in [10] shows that
the iBGP route exchange can take at most (5 + n) ∗ 5 = 35
seconds delay (where n = 2 is the number of reﬂectors that
the primary PE is connected to) in the studied network.
We thus use a time window of 35 seconds. Note that the
time-window can be diﬀerent with diﬀerent router vendors.
Our algorithm classiﬁes the events into three types [10]:
convergence, syslog-only, and update-only. We focus on the
convergence events in the rest of the paper. A convergence
event begins with a syslog message and ends with BGP up-
date message, and it ﬁnishes when the next message is: (i) a
syslog message with diﬀerent (router,interface), (ii) a syslog
message with the same (router,interface) but with a direc-
tion diﬀerent from the one in the starting syslog message of
the event, (iii) an update message that arrives more than 35
seconds later than the last update message of the event.
4.3 BGP Convergence Event Classiﬁcation
We borrow some terminology from earlier work on Inter-
net BGP convergence [5], namely Tdown, Tup, Tlong, and
Tshort. Table 3 summarizes their deﬁnition and how we
classify them in our measurement. For example, Tlong is the
event where the primary path fails, but the destination is
still reachable via a less preferred path. We determine an
event is Tlong the syslog messages of the event indicates the
link/session goes down, and the last message of the event is
an announcement. The convergence delay for Tlong is mea-
sured as the time it takes from the start of a syslog message
to the last announcement in the event.
5. MEASUREMENT RESULTS
In this section we present the measurement results based
on the data obtained from the provider network over a three
month period in 2005 and answer the following questions
about MPLS VPN convergence. How long are the delays
of Tlong, Tdown, Tup, and Tshort in MPLS VPN? Are they
diﬀerent from the eBGP convergence delays? What are dif-
ferent factors’ contributions to the convergence delay? Is
the reachability lost during Tlong convergence, and for how
long? Finally, it has been observed in IPv4 BGP that a small
number of preﬁxes contribute most of the BGP events. Is
this true in MPLS VPNs? More results are available in [10].
5.1 Results for All Types of Events
We observed around 300 thousands events during our
study period, and 20% are convergence events. Among the
convergence events, 7.3% are Tlong, 7.4% are Tshort, 43.4%
are Tdown and 39.9% are Tup, and 2% are unclassiﬁed (in
which two “real” events overlaps). The fact that there are
more Tdown and Tup events than Tlong and Tshort are because
only multi-homed preﬁxes can possibly have Tlong and Tshort
events, and typically, there are much fewer multi-homed pre-
ﬁxes than single-homed preﬁxes. The fact that there are
more Tdown events than Tup events can be due to the fact
that there are more “real” Tup events that are classiﬁed as
syslog-only, update-only, or unclassiﬁed convergence events
by our methodology as the result of event overlapping or
syslog message loss ( less than 1% in our study, detected by
checking the per-router syslog sequence numbers ).
Figure 3 shows the distributions of the convergence de-
lays for all types of events. Most delays (83% in Tlong and
Tdown, 68% in Tup and Tshort) are shorter than 20 seconds.
There are two diﬀerences in our results compared with the
event type
Tdown
Tup
Tlong
Tshort
description
preﬁx unreachable after link failure
preﬁx reachable again after link recovery
preﬁx reachable via backup after primary path fails
preﬁx reachable via primary after primary path recovery
link status
down
up
down
up
last update of the even
withdrawal
announcement
announcement
announcement
type of previous event
does not matter
Tdown
does not matter
not Tdown
Table 3: Convergence event types
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
F
D
C
Tlong