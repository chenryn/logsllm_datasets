Monarch can be used to infer properties of network paths
that have received relatively little attention by the research
community, such as paths to end hosts in commercial cable
and DSL networks. For this study, we analyzed the Broad-
band trace described in Section 4.1. This trace contains
Monarch ﬂows to 4,805 broadband hosts in 11 major ca-
ble and DSL networks in North America and Europe. Our
analysis inferred several path properties, such as through-
put, round-trip times, queueing delays, loss, and reordering.
While a detailed characterization of these properties is be-
yond the scope of this paper, we present some initial results
about the overall throughput of these ﬂows and discuss their
potential implications.
Figure 13 shows the distribution of throughput for ﬂows
to diﬀerent cable and DSL ISPs. The throughput plots for
the DSL ISPs jump sharply at 256Kbps, 384Kbps, 512Kbps
and 1Mbps. These data rates roughly correspond to the link
speeds advertised by these ISPs (see Table 4), which indi-
cates that our transfers were able to saturate the access link,
which is likely to be the bottleneck. However, the through-
put for cable ﬂows do not exhibit similar jumps, even though
cable ISPs advertise discrete link speeds as well. This sug-
gests that DSL ﬂows and cable ﬂows may be limited by
diﬀerent factors; access link capacities seem to aﬀect DSL
ﬂows to a greater extent than cable ﬂows.
Overall, our experiment demonstrates how Monarch can
be used to infer properties of network paths that have proven
diﬃcult to measure in the past. Previous studies of broad-
band hosts [20] required control over both endpoints, and
consequently were limited to a small number of broadband
paths.
5.3 Testing complex protocol implementations
Modern transport protocols (e.g. TCP NewReno with fast
retransmit and recovery) are so complex that it is often dif-
ﬁcult to implement them correctly. While program analysis
techniques [28] could help debug functionally incorrect im-
plementations, it is important to test the performance of
these protocols in the real world to ﬁnd performance prob-
lems. Monarch is particularly useful for testing protocols
because it can run complete and unmodiﬁed protocol imple-
mentations.
We used Monarch to emulate TCP ﬂows to several diﬀer-
ent types of hosts, including broadband hosts and academic
hosts. In this process, we discovered bugs in the Linux TCP
stack that tend to manifest themselves frequently over cer-
tain types of Internet paths. For example, we found that
the Linux 2.6.11 implementation of Fast Recovery [12] can
cause the congestion window to collapse almost entirely, in-
stead of merely halving it. This problem can severely reduce
throughput, and it occurs repeatedly over paths to DSL or
cable hosts.
The purpose of Fast Recovery is to allow the TCP sender
to continue transmitting while it waits for a retransmitted
segment to be acknowledged. Linux uses a variant known
as rate halving [43], which transmits one new segment for
1
0.8
0.6
0.4
0.2
s
w
o
l
F
f
o
n
o
i
t
c
a
r
F
0
0
PacBell
BTOpen
Ameritech
Qwest
SWBell
BellSouth
500
1000
1500
2500
2000
3000
Throughput (Kbps)
3500
4000
4500
5000
1
0.8
0.6
0.4
0.2
s
w
o
l
F
f
o
n
o
i
t
c
a
r
F
0
0
Charter
Chello
Comcast
Rogers
Roadrunner
500
1000
1500
2500
2000
3000
Throughput (Kbps)
3500
4000
4500
5000
(a)
(b)
Figure 13: Broadband ﬂows: Cumulative distributions of TCP ﬂow throughput achieved over major (a) DSL and (b) cable
access networks. Notice how DSL throughput raises sharply at discrete bandwidth levels while cable throughput does not.
s
t
e
k
c
a
P
50
40
30
20
10
0
CWND
Packets out
0
0.5
1
2
1.5
2.5
Time (seconds)
3
3.5
4
Incorrect rate halving in Linux TCP: Af-
Figure 14:
ter the ﬁrst loss, the congestion window falls below half its
original value.
every other ACK received. Thus, one new packet is sent
for every two packets that leave the network. Under normal
conditions, this has the eﬀect of gradually decreasing the
number of packets in ﬂight by half. Linux 2.6.11 implements
rate halving by estimating the number of packets in ﬂight,
and capping the congestion window at that number.
However, we found that this approach fails when the con-
gestion window approaches the send buﬀer size. Figure 14
shows an example of a ﬂow that saw its ﬁrst loss after 0.6
seconds, when the congestion window was 36 packets wide.
Initially, Linux was able to send 8 additional segments for
every other ACK as expected. But, once it reached the
default send buﬀer size of 64kB (44 packets), it could not
transmit more new segments. After this point, with no new
segments being transmitted, the number of packets in ﬂight,
and consequently the congestion window, decreased rapidly.
Every incoming ACK reduced the congestion window by one
packet, causing it to fall far below the slowstart threshold
of 18 packets. Thus, after leaving Fast Recovery, Linux fell
back into slowstart for over half a second. Note that a sec-
ond loss at 2.0 seconds was handled correctly because the
congestion window was still fairly small.
Monarch helped us discover this problem because it al-
lowed us to test the complete and unmodiﬁed protocol im-
plementation (in this case, the NewReno code in the Linux
kernel) over a wide range of real links with diﬀerent charac-
teristics.
6. RELATED WORK
Monarch leverages existing protocols in unanticipated
ways to perform measurements that were previously in-
tractable. This approach is similar to several other mea-
surement tools. Sting [44] manipulates the TCP protocol to
measure packet loss. T-BIT [25, 32] exploits the TCP pro-
tocol to characterize Web servers’ TCP behavior. King [14]
uses DNS queries to measure latencies between two ar-
bitrary DNS servers. SProbe [42] sends packet pairs of
TCP SYN packets to measure bottleneck bandwidth to
uncooperative Internet hosts. Like Monarch, these tools
send carefully crafted packet probes to remote Internet
hosts to measure network properties.
There is a large body of literature on evaluating trans-
port protocol designs and implementations. Much of the
previous work relies one of following three approaches to
characterize protocol behavior. The ﬁrst approach uses syn-
thetic network simulators and emulators, such as ns-2 [30],
NetPath [1], dummynet [40], NIST [9], and ModelNet [46].
There is an impressive amount of research on protocol mod-
eling and characterization in these controlled environments.
Padhye [31] discusses a summary of papers on TCP mod-
eling. Unlike Monarch, previous simulators and emulators
either use analytical models to generate TCP traﬃc or they
simulate diﬀerent synthetic network environments.
The second approach of evaluating transport protocols is
based on active measurement. Bolot [7] and Paxson [33] per-
formed some of the initial studies on network packet dynam-
ics along a ﬁx set of Internet paths. Padhye and Floyd [32]
characterized the TCP behavior of a large set of popular
Web servers. Medina et al. [25] investigated the behavior of
TCP implementations and extensions. In a diﬀerent project,
Medina et al. [26] characterized the eﬀect of network middle-
boxes on transport protocols. More recently, several studies
have used PlanetLab [36] to examine diﬀerent aspects of
TCP network traﬃc.
The third approach of evaluating transport protocols re-
lies on passive measurements. Based on the traces of TCP
ﬂows to a busy Web server, Balakrishnan et al. [5] presented
a detailed analysis of the performance of individual TCP
ﬂows carrying Web traﬃc. Jaiswal et al. [16] used traﬃc
traces of a Tier-1 ISP to investigate the evolution of a TCP
connection variables over the lifetime of a TCP connection.
More recently, Arlitt et al. [4] have used Web traces to inves-
tigate the impact of latency on short transfers’ durations.
7. CONCLUSIONS
In this paper, we presented Monarch, a tool that emulates
transport protocol ﬂows over live Internet paths. Monarch
enables transport protocols to be evaluated in realistic en-
vironments, which complement the controlled environments
provided by the state of the art network simulators, emula-
tors or testbeds. Monarch is highly accurate: its emulated
ﬂows closely resemble TCP ﬂows in terms of throughput,
loss rate, queueing delay, and several other characteristics.
Monarch uses generic TCP, UDP, or ICMP probes to em-
ulate transport protocol ﬂows to any remote host that re-
sponds to such probes. By relying on minimal support from
the remote host, Monarch enables protocols to be evaluated
on an unprecedented scale, over millions of Internet paths.
We used Monarch for three novel experiments. First, our
preliminary study on the performance of diﬀerent conges-
tion control algorithms (TCP Reno, TCP Vegas and TCP
BIC) shows that much remains to be understood about the
behavior of even widely adopted protocols over the Internet
at large. Second, we showed that Monarch measurements
can be used to infer network properties of less-studied In-
ternet paths, such as paths to cable and DSL hosts. Third,
we used Monarch to test complete and unmodiﬁed TCP pro-
tocol implementations in the Linux kernel over a variety of
Internet paths, and we discovered nontrivial bugs. Based
on our experience, we believe that Monarch can help the re-
search community conduct large-scale experiments leading
to new insights and ﬁndings in the design and evolution of
Internet transport protocols.
8. ACKNOWLEDGMENTS
We would like to thank Steve Gribble, Dan Sandler, and
Emil Sit for generously hosting Monarch servers for our ex-
periments. Peter Druschel and our anonymous reviewers
provided detailed and helpful feedback on the earlier ver-
sions of this draft. The ns-2 interface for Monarch was
developed by Prateek Singhal.
9. REFERENCES
[1] S. Agarwal, J. Sommers, and P. Barford. Scalable net-
work path emulation. In Proceedings of the 13th IEEE
International Symposium on Modeling, Analysis, and
Simulation of Computer and Telecommunication Sys-
tems (MASCOTS), Washington, DC, 2005.
[2] D. G. Andersen, H. Balakrishnan, F. Kaashoek, and
R. Morris. Experience with an Evolving Overlay Net-
work Testbed. ACM Computer Communication Review,
33(3), July 2003.
[3] T. Anderson, A. Collins, A. Krishnamurthy, and J. Za-
horjan. PCP: Eﬃcient endpoint congestion control. In
Proceedings of NSDI’06, May 2006.
[4] M. Arlitt, B. Krishnamurthy, and J. C. Mogul. Predict-
ing short-transfer latency from TCP Arcana: A trace-
based validation. In Proceedings of Internet Measure-
ment Conference, Berkeley, CA, October 2005.
[5] H. Balakrishnan, V. N. Padmanbhan, S. Seshan,
M. Stemm, and R. H. Katz. TCP behavior of a busy
Internet server: Analysis and improvements. In Pro-
ceedings of IEEE Infocom, San Francisco, CA, USA,
March 1998.
[6] J. Bellardo and S. Savage. Measuring packet reorder-
ing. In Proceedings of the 2002 ACM SIGCOMM Inter-
net Measurement Workshop (IMW), Marseille, France,
November 2002.
[7] J.-C. Bolot. Characterizing end-to-end packet delay and
loss in the Internet. In Proceedings of ACM SIGCOMM,
San Francisco, CA, September 1993.
[8] L. S. Brakmo and L. Peterson. TCP Vegas: End to end
congestion avoidance on a global internet. IEEE Jour-
nal on Selected Areas in Communication, 13(8):1465–
1480, October 1995.
[9] M. Carson and D. Santay. NIST Net – a Linux-based
network emulation tool. SIGCOMM Computer Com-
munications Review, 33(3):111–126, 2003.
[10] S. Floyd. RFC 3649 - HighSpeed TCP for large conges-
tion windows, Dec 2003. ftp://ftp.rfc-editor.org/
in-notes/rfc3649.txt.
[11] S. Floyd, M. Handley, J. Padhye, and J. Widmer.
Equation-based congestion control for unicast applica-
tions. In Proceedings of SIGCOMM’00, Aug 2000.
[12] S. Floyd, T. Henderson, and A. Gurtov. RFC 3782
- The NewReno modiﬁcation to TCP’s fast recov-
ery algorithm, Apr 2004. ftp://ftp.rfc-editor.org/
in-notes/rfc3782.txt.
[13] R. Govindan and V. Paxson. Estimating router ICMP
generation delays. In Proceedings of Passive and Active
Measurement (PAM’02), 2002.
[14] K. P. Gummadi, S. Saroiu, and S. D. Gribble. King: Es-
timating latency between arbitrary Internet end hosts.
In Proceedings of the 2nd ACM SIGCOMM Inter-
net Measurement Workshop (IMW), Marseille, France,
November 2002.
[15] V. Jacobson, R. Braden, and D. Borman. RFC 1323 -
TCP extensions for high performance, May 1992. http:
//www.faqs.org/rfcs/rfc1323.html.
[16] S. Jaiswal, G. Iannaccone, C. Diot, J. Kurose, and
D. Towsley. Inferring TCP connection characteristics
through passive measurements. In Proceedings of IEEE
Infocom, Hong Kong, March 2004.
[17] C. Jin, D. X. Wei, and S. H. Low. FAST TCP: Moti-
vation, architecture, algorithms, performance. In Pro-
ceedings of IEEE Infocom 2004, Mar 2004.
[18] D. Katabi, M. Handley, and C. Rohrs. Congestion con-
trol for high bandwidth-delay product networks. In Pro-
ceedings of SIGCOMM’02, Aug. 2002.
[19] T. Kelly. Scalable TCP: Improving performance in
highspeed wide area networks. ACM Computer Com-
munication Review, 33(2):83–91, Apr 2003.
[20] K. Lakshminarayanan and V. N. Padmanabhan. Some
ﬁndings on the network performance of broadband
hosts. In Proceedings of the ACM/Usenix Internet Mea-
surement Conference (IMC), Miami, FL, USA, October
2003.
[21] R. Mahajan, N. Spring, D. Wetherall, and T. Ander-
son. User-level Internet path diagnosis. In Proceedings
of the 19th Symposium on Operating Systems Principles
(SOSP), Bolton Landing, NY, USA, October 2003.
[22] S. Mascolo, C. Casetti, M. Gerla, M. Y. Sanadidi, and
R. Wang. TCP Westwood: bandwidth estimation for
enhanced transport over wireless links. In Proceedings
of MOBICOM’01, pages 287–297, Jul 2001.
[23] M. Mathis and J. Mahdavi. Forward acknowledgment:
Reﬁning TCP congestion control. In Proceedings of
SIGCOMM 1996, pages 281–291, Aug. 1996.
[24] M. Mathis, J. Mahdavi, S. Floyd, and A. Romanow.
RFC 2018 - TCP selective acknowledgment options,
Oct. 1996. http://www.faqs.org/rfcs/rfc2018.html.
[25] A. Medina, M. Allman, and S. Floyd. Measuring the
evolution of transport protocols in the Internet. Com-
puter Communication Review, 35(2):37–52, 2005.
[26] A. Medina, M. Allman, and S. Floyd. Measuring inter-
actions between transport protocols and middleboxes.
In Proceedings of the Internet Measurement Conference,
Taormina, Italy, August 2004.
[27] Monarch web site. http://monarch.mpi-sws.mpg.de/.
[28] M. Musuvathi and D. R. Engler. Model-checking large
network protocol implementations. In Proceedings of
NSDI’04, pages 155–168, March 2004.
[29] Netﬁlter: Firewalling, NAT, and packet mangling for
Linux, 2006. http://www.netfilter.org.
[30] The network simulator – ns2. http://www.isi.edu/
nsnam/ns/.
[31] J. Padhye. Papers on TCP modeling and related top-
ics, 2001. http://research.microsoft.com/~padhye/
tcp-model.html.
[32] J. Padhye and S. Floyd. Identifying the TCP behavior
of web servers. In Proceedings of the ACM SIGCOMM
Conference, San Diego, CA, USA, June 2001.
[33] V. Paxson. End-to-end routing behavior in the Internet.
IEEE/ACM Transactions on Networking, 5(5):601–
615, October 1997.
[34] V. Paxson, A. K. Adams, and M. Mathis. Experi-
ences with NIMI. In Proceedings of 2002 Symposium on
Applications and the Internet (SAINT), Nara, Japan,
February 2002.
[35] PlanetLab. http://www.planet-lab.org/.
[36] PlanetLab. Current slices in PlanetLab, 2006. http:
//www.planet-lab.org/php/slices.php.
[37] J. Postel. RFC 792 - Internet control message protocol,
1981. http://www.faqs.org/rfcs/rfc792.html.
[38] K. K. Ramakrishnan, S. Floyd, and D. L. Black. RFC
3168 - The addition of explicit congestion notiﬁcation
(ECN) to IP, Sep 2001. http://www.faqs.org/rfcs/
rfc3168.html.
[39] R. Rejaie, M. Handley, and D. Estrin. RAP: An end-
to-end rate-based congestion control mechanism for re-
altime streams in the Internet. In Proceedings of IEEE
Infocom’99, pages 1337–1345, Mar 1999.
[40] L. Rizzo. Dummynet: A simple approach to the evalu-
ation of network protocols. ACM Computer Communi-
cations Review, 1997.
[41] S. Saroiu, K. P. Gummadi, and S. D. Gribble. A mea-
surement study of peer-to-peer ﬁle sharing systems. In
Proceedings of the Multimedia Computing and Network-
ing (MMCN), San Jose, CA, January 2002.
[42] S. Saroiu, K. P. Gummadi, and S. D. Gribble. SProbe:
A fast tool for measuring bottleneck bandwidth in un-
cooperative environments, 2002. http://sprobe.cs.
washington.edu.
[43] P. Sarolahti and A. Kuznetsov. Congestion control in
Linux TCP. In Proceedings of USENIX 2002, June
2002.
[44] S. Savage. Sting: a TCP-based network measurement
tool. In Proceedings of the 1999 USENIX Symposium on
Internet Technologies and Systems, Boulder, CO, USA,
October 1999.
[45] University of Southern California, Information Sciences
Institute. RFC 793 - Transmission control protocol,
1981. http://www.faqs.org/rfcs/rfc793.html.
[46] A. Vahdat, K. Yocum, K. Walsh, P. Mahadevan,
D. Kostic, and D. Becker. Scalability and accuracy in
a large-scale network emulator. In Proceedings of the
5th Symposium on Operating Systems Design and Im-
plementation, Boston, MA, 2002.
[47] A. Venkataramani, R. Kokku, and M. Dahlin. TCP
Nice: a mechanism for background transfers. In Pro-
ceedings of OSDI’02, Dec 2002.
[48] B. White, J. Lepreau, L. Stoller, R. Ricci, S. Gu-
ruprasad, M. Newbold, M. Hibler, C. Barb, and
A. Joglekar. An integrated experimental environment
for distributed systems and networks. In Proceedings
of the Fifth Symposium on Operating Systems Design
and Implementation, pages 255–270, Boston, MA, Dec.
2002. USENIX Association.
[49] L. Xu, K. Harfoush, and I. Rhee. Binary increase con-
gestion control for fast long-distance networks. In Pro-
ceedings of IEEE Infocom, Hong Kong, March 2004.