DHT demonstrates that it is indeed feasible to build appli-
cations with more complex semantics than simply put/get
entirely on top of a third-party DHT. Below, we summarize
some of the lessons drawn from this experience.
6.1 Simplicity of implementation
The code required to hook Place Lab into the underlying
OpenDHT service including the entire PHT implementation
consists of 2100 lines of Java compared to over 14,000 lines for
OpenDHT. A customized non-layered implementation would
have required Place Lab to implement from scratch all of
the scalable routing, robustness, and management properties
that we got from OpenDHT for free.
A number of features of Place Lab made it well-suited
for this strictly layered implementation. Its data structures
are link-free. This makes each node largely independent of
the others and hence can be easily distributed on top of the
DHT. Similarly, information for each Place Lab beacon is
mostly independent of the other beacons, thus making it easy
to decompose the data across servers. Place Lab’s mapping
data has signi(cid:12)cant redundancy, allowing it to mask transient
s
t
i
l
p
s
e
t
a
c
i
l
p
u
d
f
o
#
.
g
v
A
8
7
6
5
4
3
2
1
0
s
e
i
r
e
u
q
f
o
%
f
o
F
D
C
100
80
60
40
20
0
0
4
8
12
Node depth
16
20
0
2
4
6
8
10
Churn overhead
100
)
s
r
e
t
80
e
m
(
r
o
r
r
e
i
n
a
d
e
M
60
40
20
0
0
20
40
% unavailable data
60
80
100
Figure 12: The average number
of duplicate splits as a function of
node depth for 25 concurrent PHT
writers.
Figure 13: A CDF of the percent-
age of queries as a function of the
churn overhead. Churn overhead
is de(cid:12)ned as the ratio of the query
response time with churn versus
response time without churn.
Figure 14: Median positioning er-
ror in Place Lab as a function of
availability of beacon data.
failures e(cid:11)ectively. Finally, the data structures are capable
of refreshing themselves and recovering from failures. This
makes them well-suited to deployment on an infrastructure
over which the Place Lab developers have no control.
6.2 Ease of deployment
We started this discussion by asking the question of whether
building Place Lab’s mapping service on top of OpenDHT
simpli(cid:12)es its deployment. This question has two facets: long-
term service deployment, and experimental deployment for
performance testing. As a long-term deployment strategy,
our implementation of Place Lab is able to hand o(cid:11) much
of the management overhead of running and maintaining the
distributed system to OpenDHT. Each mapping server in
Place Lab is essentially independent of all other servers. By
outsourcing the OpenDHT deployment to a third-party, a
participant in the Place Lab infrastructure only has worry
about the management of their individual mapping servers
and their connection to the DHT.
On the other hand, while experimenting with the applica-
tion and its performance, we still ended up having to install
our own OpenDHT infrastructure (separate from the existing
deployed version). Since OpenDHT is a shared service, the
maintainers of the service were unwilling to kill machines at
random to allow us to experiment with the e(cid:11)ects of churn.
Similarly, as we discuss below, we extended the OpenDHT
APIs, which would have also resulted in a disruption of the
shared OpenDHT deployment if the maintainers were to up-
grade it to support the new APIs. To a large extent, this is
no di(cid:11)erent from experimenting with, say, Internet protocols
where one cannot expect to tinker directly with the deployed
shared infrastructure.
6.3 Flexibility of APIs
We were able to build Place Lab entirely on top of a narrow
set of application-independent APIs. Our experience demon-
strated that although put/get/remove were the primary inter-
faces that Place Lab relied on, it needed additional auxiliary
APIs to correctly and e(cid:14)ciently support the distributed data
structures for its application.
Typically, DHTs are designed for \best-e(cid:11)ort" performance.
They provide no concurrency primitives nor do they provide
any atomicity guarantees for reads and writes. Although
this may be su(cid:14)cient for simple rendezvous and storage ap-
plications, they make it di(cid:14)cult to build more complex data
structures. This principle of simplicity over strong guaran-
tees is a central tenet of the design of the Internet. Obviously,
it has served the Internet well, but it is less clear whether
it is su(cid:14)cient for large applications over the Internet. PHTs
overcome this by using TTLs and periodic refresh to recover
from concurrency problems, but even a simple test-and-set
operation (e.g., our put conditional() extension) that provides
more than best-e(cid:11)ort guarantees goes a long way to improv-
ing PHT performance.
6.4 Performance
Although the use of DHTs to implement Place Lab’s dis-
tributed infrastructure signi(cid:12)cantly simpli(cid:12)ed its implemen-
tation and deployment, this was at the expense of perfor-
mance. Queries take on average 2{4 seconds (depending on
the size of the input data set). In contrast, a single central-
ized implementation would eliminate the many round trips
that account for the performance overhead. Similarly, an
implementation that allowed for modifying the underlying
DHT routing (for example, Mercury) can also provide op-
portunities for optimization. This tradeo(cid:11) is inherent in any
layered versus monolithic implementation.
Aggressive caching signi(cid:12)cantly improves Place Lab’s per-
formance. For example, if the PHT data structure is modi-
(cid:12)ed infrequently, we can eliminate many of the round trips
by caching what amounts to a representation of the cur-
rent shape of the tree. Applications that can use such forms
of caching will be well-suited to provide reasonable perfor-
mance. Of course,
in the end, whether the performance
tradeo(cid:11) is worth the ease of implementation and deployment
depends entirely on the requirements of the application and
its users.
7. CONCLUSION
In this paper, we have explored the viability of a DHT
service as a general purpose building block for Place Lab, an
end-user positioning system. In particular, we investigated
the suitability of layering Place Lab entirely on top of a third-
party DHT to minimize its deployment and management
overhead. Place Lab di(cid:11)ers from many traditional DHT ap-
plications in that it requires stronger semantics than simply
put/get operations; speci(cid:12)cally, it needs two-dimensional ge-
ographic range queries. Hence, we designed and evaluated
Pre(cid:12)x Hash Trees, a multi-dimensional range query data
structure layered on top of the OpenDHT service. PHTs
provide an elegant solution for gathering radio beacon data
by location from across the many Place Lab mapping servers.
The layered approach to building Place Lab allowed us to
automatically inherit the robustness, availability, and scal-
able routing properties of the DHT. Although we were able
to signi(cid:12)cantly reduce the implementation overhead by lay-
ering, this simpli(cid:12)cation was at the price of performance.
Place Lab and PHTs are unable to make use of optimiza-
tions that would have been possible if one were to use a
customized DHT underneath.
This is certainly not the last word on the feasibility of
general-purpose DHTs as a building block for large-scale ap-
plications. However, Place Lab demonstrates that if ease of
deployment is a primary criterion (over maximal e(cid:14)ciency),
the simple DHT APIs (with minor extensions) can provide
the necessary primitives to build richer more complex sys-
tems on top.
8. REFERENCES
[1] Aberer, K. P-Grid: A self-organizing access structure for
P2P information systems. In Proc. CoopIS (2001).
[2] Anonymous, et al. OpenDHT: A public DHT service and
its uses. Under submission to SIGCOMM 2005.
[3] Aspnes, J., Kirsch, J., and Krishnamurthy, A. Load
balancing and locality in range-queriable data structures. In
Proceedings of the Twenty-Third ACM Symposium on
Principles of Distributed Computing (July 2004).
[4] Aspnes, J., and Shah, G. Skip graphs. In Proc.
ACM-SIAM Symposium on Discrete Algorithms (SODA)
(2003).
[5] Awerbuch, B., and Scheideler, C. Peer-to-peer systems
for pre(cid:12)x search. In Proc. ACM Symposium on Principles
of Distributed Computing (PODC) (2003).
[6] Berchtold, S., Ohm, C. B., and Kriegel, H.-P. The
Pyramid-Technique: Towards Breaking the Curse of
Dimensionality. In Proceedings of International Conference
on Management of Data (SIGMOD ’98) (June 1998).
[7] Bhagwan, R., Varghese, G., and Voelker, G. Cone:
Augmenting DHTs to support distributed resource
discovery. Tech. Rep. CS2003-0755, UC, SanDiego, Jul 2003.
[8] Bharambe, A. R., Agrawal, M., and Seshan, S. Mercury:
Supporting scalable multi-attribute range queries. In Proc.
SIGCOMM (2004).
[9] Cheng, Y.-C., Chawathe, Y., LaMarca, A., and Krumm,
J. Accuracy characterization for metropolitan-scale wi-(cid:12)
localization. In Proceedings of MobiSys ’05 (Seattle, WA,
June 2005).
[10] Crainiceanu, A., Linga, P., Gehrke, J., and
Shanmugasundaram, J. Querying Peer-to-Peer Networks
Using P-Trees. In Proc. WebDB Workshop (2004).
[11] Dabek, F., Kaashoek, M. F., Karger, D., Morris, R.,
and Stoica, I. Wide-area Cooperative Storage with CFS.
In Proceedings of the 18th ACM Symposium on Operating
Systems Principles (SOSP 2001) (Lake Louise, AB,
Canada, October 2001).
[12] Druschel, P., and Rowstron, A. Storage Management
and Caching in PAST, a Large-scale, Persistent Peer-to-peer
Storage Utility. In Proceedings of the 18th ACM Symposium
on Operating Systems Principles (SOSP 2001) (Lake
Louise, AB, Canada, October 2001).
[13] FIPS PUB 180-1. Secure Hash Standard, April 1995.
[14] Freedman, M. J., Freudenthal, E., and Mazi(cid:18)eres, D.
Democratizing Content Publication with Coral. In
Proceedings of the 1st Symposium on Networked Systems
Design and Implementation (NSDI 2004) (San Francisco,
Mar. 2004).
[15] Ganesan, P., Bawa, M., and Garcia-Molina, H. Online
balancing of range-partitioned data with applications to
peer-to-peer systems. In Proc. VLDB (2004).
[16] Gupta, A., Agrawal, D., and Abbad, A. E. Approximate
range selection queries in peer-to-peer systems. In
Proc.Conference on Innovative Data Systems Research
(CIDR) (2003).
[17] Huebsch, R., Chun, B., Hellerstein, J. M., Loo, B. T.,
Maniatis, P., Roscoe, T., Shenker, S., Stoica, I., and
Yumerefendi, A. R. The Architecture of PIER: an
Internet-Scale Query Processor. In Proc. Conference on
Innovative Data Systems Research (CIDR) (Jan. 2005).
[18] Jagadish, H. V. Linear clustering of objects with multiple
attributes. In Proceedings of ACM SIGMOD International
Conference on Management of Data (SIGMOD’90) (May
1990), pp. 332{342.
[19] Karger, D. R., and Ruhl, M. Simple e(cid:14)cient load
balancing algorithms for peer-to-peer systems. In Proc.
SPAA (2004).
[20] Kubiatowicz, J. Oceanstore: An Architecture for
Global-Scalable Persistent Storage. In Proceedings of the
ASPLOS 2000 (Cambridge, MA, USA, November 2000).
[21] LaMarca, A., et al. Place lab: Device positioning using
radio beacons in the wild. In Proceedings of International
Conference on Pervasive Computing (Pervasive) (June
2005).
[22] Lamport, L. The Part-Time Parliament. ACM
Transactions on Computer Systems 16, 2 (1998), 133{169.
[23] Mislove, A., Post, A., Reis, C., Willmann, P.,
Druschel, P., Wallach, D. S., Bonnaire, X., Sens, P.,
Busca, J.-M., and Arantes-Bezerra, L. POST: A secure,
resilient, cooperative messaging system. In Proceedings of
the 9th Workshop on Hot Topics in Operating Systems
(Lihue, HI, May 2003).
[24] Muthitacharoen, A., Gilbert, S., and Morris, R. Etna:
a fault-tolerant algorithm for atomic mutable dht data.
Technical report, Massachussetts Institute of Technology,
June 2004.
[25] Muthitacharoen, A., Morris, R., Gil, T., and Chen, B.
Ivy: A read/write peer-to-peer (cid:12)le system. In Proc. OSDI
(2002).
[26] Oppenheimer, D., Albrecht, J., Vahdat, A., and
Patterson, D. Design and Implementation Tradeo(cid:11)s for
Wide-area Resource Discovery. In Proceedings of 14th IEEE
Symposium on High Performance Distributed Computing
(HPDC-14) (Research Triangle Park, NC, July 2005).
[27] Peterson, L., Anderson, T., Culler, D., and Roscoe,
T. A Blueprint for Introducing Disruptive Technology into
the Internet. In Proceedings of the ACM HotNets-I
Workshop (Princeton, NJ, Oct. 2002). See also
http://www.planet-lab.org/.
[28] Rowstron, A., Kermarrec, A.-M., Castro, M., and
Druschel, P. Scribe: The design of a large-scale event
noti(cid:12)cation infrastructure. In Networked Group
Communication, Third International COST264 Workshop
(NGC’2001) (Nov. 2001), J. Crowcroft and M. Hofmann,
Eds., vol. 2233 of Lecture Notes in Computer Science,
pp. 30{43.
[29] Sit, E., Dabek, F., and Robertson, J. UsenetDHT: A low
overhead usenet server. In Proc. of the 3rd IPTPS (Feb.
2004).
[30] Stoica, I., Adkins, D., Zhuang, S., Shenker, S., and
Surana, S. Internet Indirection Infrastructure. In
Proceedings of the ACM SIGCOMM 2002 (Pittsburgh, PA,
USA, August 2002).
[31] Stoica, I., Morris, R., Karger, D., Kaashoek, F., and
Balakrishnan, H. Chord: A Scalable Peer-to-peer Lookup
Service for Internet Applications. In Proceedings of the ACM
SIGCOMM 2001 (San Diego, CA, USA, August 2001).
[32] Tang, C., Xu, Z., and Mahalingam, M. pSearch:
Information Retrieval in Structured Overlays. SIGCOMM
Comput. Commun. Rev. 33, 1 (2003), 89{94.
[33] Yalagandula, P., and Browne, J. Solving range queries
in a distributed system. Tech. Rep. TR-04-18, UT CS, 2004.
[34] Yalagandula, P., and Dahlin, M. A scalable distributed
information management system. In Proc. SIGCOMM
(2004).