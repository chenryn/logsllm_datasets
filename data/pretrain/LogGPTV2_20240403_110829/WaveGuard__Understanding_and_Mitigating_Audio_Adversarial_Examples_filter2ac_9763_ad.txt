transcription of audio by ASR models and each transforma-
tion function on Intel Xeon CPU platform. The Wall-Clock
time is averaged over the entire test set.
7 Adaptive Attack
While our defense framework can accurately discriminate
adversarial from benign examples for existing attacks, it only
offers security in a “zero-knowledge” attack scenario where
the attacker is not aware of the defense being present. As
motivated in Section 2.2, in order to use our defense frame-
work reliably in practice, it is important to evaluate it against
an adaptive adversary who has complete knowledge of the
defense and intend to design a perturbation that can bypass
the defense mechanism.
In the adaptive attack setting, we will focus on the more im-
pactful targeted attack scenario, where the adversary designs
an adversarial perturbation that causes the victim ASR sys-
tem to transcribe the input audio into a speciﬁc target phrase.
In order to bypass the proposed defense framework, the ad-
versary must craft an adversarial perturbation such that the
transcription of C(xadv) and C(g(xadv)) match closely with
each other and the target transcription τ. Therefore, to craft
such a perturbation δ, the adversary aims to optimize the
following problem:
minimize: |δ|∞ + c1 · (cid:96)(x + δ,τ) + c2 · (cid:96)(g(x + δ),τ)
where, (cid:96)(x(cid:48),t) = CTC-Loss(C(x(cid:48)),t) and c1 and c2 are hyper-
parameters that control the weights of the respective loss
terms. Since optimization process over the L∞ metric is of-
ten unstable [11], we modify our optimization objective as
follows:
minimize: c·|δ|2
2 + c1 · (cid:96)(x + δ,τ) + c2 · (cid:96)(g(x + δ),τ)
such that |δ|∞ < ε
(8)
Figure 7: Mean Character Error Rate (CER) between the ASR
transcriptions of un-transformed (x) and transformed (g(x))
audio. CER(orig,g(orig)) and CER(adv, g(adv)) indicate the
CER between transcriptions of x and g(x) for benign and
adversarial samples respectively. CER(orig, g(adv)) is the
CER between the defended adversarial signal and its benign
counterpart.
6.3 Timing analysis
To implement our defense framework in practice, we have to
perform two forward passes through our ASR model to obtain
the transcriptions C(x) and C(g(x)). It is ideal to parallelize
these two forward passes, so that the only computational over-
head introduced by the defense is that of the transformation
function g. Table 3 provides the average Wall-Clock time in
seconds of each transformation function averaged over the
100 audio ﬁles (entire test set). Since some of our transforma-
tion functions were implemented solely on CPU, we provide
2282    30th USENIX Security Symposium
USENIX Association
CER0.000.250.500.751.00Down-upQuantFilteringMelLPCCarliniCER0.000.250.500.751.00Down-upQuantFilteringMelLPCUniversalCER0.000.250.500.751.00Down-upQuantFilteringMelLPCYao-ICER0.000.250.500.751.00Down-upQuantFilteringMelLPCYao-RCER(orig, g(orig))CER(adv, g(adv))CER(orig, g(adv))Qin-IQin-RFigure 8: Sample transcriptions of un-transformed(x) and transformed audio(g(x)) for both benign and adversarial examples.
7.1 Gradient Estimation for Adaptive Attack
To solve the optimization problem given by equation 8 us-
ing gradient descent, the attacker must back-propagate the
CTC-Loss through the ASR model and the input transforma-
tion function g. In case a differentiable implementation of
g is not available, we use the Backward Pass Differentiable
Approximation (BPDA) technique [6] to craft adversarial ex-
amples. That is, during the forward pass we use the exact
implementation of the transformation function as used in our
defense framework. During the backward pass, we use an
approximate gradient implementation of the transformation g.
We ﬁrst perform the adaptive attack using the straight-through
gradient estimator [6]. That is, we assume that the gradient
of the loss with respect to the input x to be the same as the
gradient of the loss with respect to g(x):
∇x(cid:96)(g(x))|x= ˆx ≈ ∇x(cid:96)(x)|x=g( ˆx) .
(9)
In our experiments, we ﬁnd that the straight-through estima-
tor is effective in breaking the Quantization-Dequantization
and Filtering transformation functions at low perturbation
levels. However, using a more accurate gradient estimate can
lead to a stronger attack. Speciﬁcally for the Mel extraction-
inversion and LPC transformations, we ﬁnd that using a
straight-through gradient estimator does not work for solving
the above optimization problem (Equation 8). We discuss our
results of using a straight-through gradient estimator for LPC
transform in Appendix D.. Also, using a straight-through esti-
mator for the Downsampling-Upsampling transform results
in high distortion for adversarial perturbations. Therefore,
we implement differentiable computational graphs for the
following three transforms in TensorFlow:
Downsampling-Upsampling: We use TensorFlow’s bi-
linear resizing methods to ﬁrst downsample the audio to the
required sampling rate and then re-estimate the signal using
bi-linear interpolation.
Mel Extraction - Inversion: For the Mel extraction-
inversion transform we use TensorFlow’s STFT implementa-
tion to obtain the magnitude spectrogram, then perform the
Mel transform using matrix multiplication with the Mel basis,
and estimate the waveform using the iterative Grifﬁn-Lim [53]
algorithm implemented in TensorFlow [54].
LPC transform: We implement the LPC analysis and syn-
thesis process in TensorFlow. Speciﬁcally, for each window
in the original waveform, we ﬁrst estimate LPC coefﬁcients
by solving the linear regression problem given by Equation 7.
Next, for the reconstruction process, we generate the residual
excitation signal using the exact same implementation as used
in our defense. We also ﬁx the random seed of the excita-
tion generator in both our defense and our adaptive attacks
for a complete knowledge white box attack scenario. Next,
we implement auto-regressive ﬁltering of the residual signal
with the LPC coefﬁcients for that window to synthesize the
signal for the given window. Finally, we add and combine the
ﬁltered signal for each overlapping window to generate the
transformed audio.
Note that for all the adaptive attacks, we use the original
defense implementations in the forward pass and use the
differentiable implementation only during the backward pass.
7.2 Adaptive Attack Algorithm
Algorithm 1 details our adaptive attack implementation. We
closely follow the targeted attack implementation in [11] and
incorporate the optimization objective of our adaptive attack
speciﬁed by Equation 8 and BPDA. We choose c1 = c2 =
1 since both loss terms have the same order of magnitude.
Following the default open source implementation of [11],
we do not penalize L2 distortion. We optimize for 5000 it-
erations and use a learning rate of 10. Any time the attack
USENIX Association
30th USENIX Security Symposium    2283
AttackAdversarial Command (C(x_adv))Defended Command (C(g(x_adv)))Benign Command (C(x))Down-UpQuantFilterMelLPCCarlini"browse to evil dot com"i'm sure i didn't know whenc set's talking about"i'm sure i don't know what you' talking about""srown to withe cot gom""i'm sure i don't know what you'e talking about""absure i don't know what you' talking about""i'm sure i don't know what you're talking about"Qin-I"hey google""this is no place for you""this is no place for you""but it is no place for you""this is no place for you""this is no place for you""this is no place for you"Qin-R"hey google cancel my medical appointment""ah you hahogum he hath a home and not far called the man pulling there" "hey de laggle cancel my medical appointment""he hated the loggal cly anticone not a particle of appointment""lady galogolfe and lygam amethurical appointment""and when i had never he ankle a handful for my little appointment""he did find it soon after dawn and not far from the sand pits"Universal"there ae little ied ne callyuack""wa didn't i call you back""why didn't i call you back""lodidn't i call you back""why didn't i call you back"" litwoted no col yo back""why didn't o call you back"Benign Command (C(x))Defended Command (C(g(x)))Down-UpQuantFilterMelLPC"i'm sure i don't know what you're talking about""i'm sure i don't know what you're talking about""i'm sure i don't know what you're talking about""i'm sure i don't know what you're talking about""i'm sure i don't know what you're talking about""i'm sure i don't know what you're talking about"succeeds, we re-scale the perturbation bound by a factor of
0.8 to encourage less distorted (quieter) adversarial examples.
We include the exact implementation of the adaptive attack
and the differentiable computational graphs for BPDA in our
code.2
2 + c1 · (cid:96)(x + δ,t) + c2 · (cid:96)(g(x + δ),t)
Algorithm 1 Adaptive attack algorithm
1: Initialize rescaleFactor ← 1
2: Initialize δ ← 0
3: Initialize bestDelta ← null
4: for iterNum in 1 to MaxIters do
5:
6:
7:
8:
9:
10:
11:
12: if bestDelta is null then
13:
14: return (x + bestDelta)
loss ← c·|δ|2
∇δ ← BPDA(loss,δ)
δ ← δ− α sign(∇δ)
δ ← rescaleFactor∗ clipε(δ)
if C(x + δ) = C(g(x + δ)) = τ then
bestDelta ← δ
rescaleFactor ← rescaleFactor× 0.8
bestDelta ← δ
talking [11]. While we start with an initial L∞ (ε∞) bound of
500 in our experiments, the ﬁnal distortion norm (δ∞) can
be much smaller than the initial bound. This is because our
optimization objective penalizes high distortion amounts and
our algorithm re-scales the perturbation bound by a factor of
0.8 every time the attack succeeds.
Generally, prior work on attacks to ASR systems apply par-
ticular attention to minimize perturbation distortions, in order
to encourage imperceptibility of adversarial audio. Towards
this goal of generating imperceptible adversarial examples,
Qin et al. [14] and Universal [15] generate examples with
maximum allowed distortion of L∞ = 400, while Carlini et
al. [11] generate examples with maximum distortion of L∞ =
100. However for conducting our adaptive attack evaluation,
since we aim to test the breaking point of each transforma-
tion function, we generate adversarial perturbations at much
higher L∞ bounds (500, 1000, 4000) that are signiﬁcantly
more audible to the human ear.
8 Adaptive Attack Evaluation
In this section, we test the limits of our defense and evaluate
the breaking point for each transformation function through
adaptive attacks in white box setting. We conduct adaptive
attack evaluations on the same dataset used in our previous
experiments. The victim ASR for the adaptive attack is
the Mozilla DeepSpeech model. In order to evaluate the
imperceptibility of adversarial perturbations, we quantify the
distortion of adversarial perturbations as follows.
Distortion Metrics and Relative Loudness: We ﬁrst im-
plement adaptive attacks using an initial distortion bound
|ε|∞ = 500. Note that we are using a 16-bit waveform rep-
resentation which means that the waveform samples are in
the range -32768 to 32768. An L∞ distortion of 500 is fairly
perceptible although it does not completely mask the origi-
nal signal.3 Along with the L∞ norm of the perturbation, we
report another related metric dBx(δ) [11, 15] that measures
the relative loudness of the perturbation with respect to the
original signal in Decibels(dB). The metric dBx(δ) is deﬁned
as follows:
dB(x) = maxi20log10(xi)
dBx(δ) = dB(δ)− dB(x)
(10)
The more negative dBx(δ) is, the quieter is the adversarial
perturbation. For comparison, -31 dB is roughly the differ-
ence between ambient noise in a quiet room and a person
2Code: https://github.com/waveguard/waveguard_defense
3Audio Examples: https://waveguard.herokuapp.com
(a) Downsampling-upsampling, Quantization and Filtering
(b) Linear Predictive Coding (LPC)
(c) Mel Extraction - Inversion
Figure 9: Detection ROC curves for different transformation
functions against adaptive attacks (Section 8) with various
magnitudes of adversarial perturbation (|δ|∞).
Table 4 presents the results for our adaptive attack against
various input transformation functions. We provide the Re-
ceiver Operating Characteristic (ROC) of the detector in the
adaptive attack settings for different transformation functions
under different magnitudes of perturbation in Figure 9. A true
positive implies an example that is adversarial and is correctly
identiﬁed as adversarial. We evaluate the adaptive attacks on
two aspects: 1) Attack Performance: How successful was the
2284    30th USENIX Security Symposium
USENIX Association
Downsampling-upsampling        = 500 False Positive RateTrue Positive Rate     Quantization        = 500False Positive RateTrue Positive RateFiltering        = 500False Positive RateTrue Positive Rate             LPC        = 500       False Positive RateTrue Positive RateFalse Positive RateTrue Positive RateFalse Positive RateTrue Positive Rate             LPC        = 1000                    LPC        = 2167       False Positive RateTrue Positive RateFalse Positive RateTrue Positive RateFalse Positive RateTrue Positive RateMel Inv - Extraction        = 1000       Mel Inv - Extraction         = 2461       Mel Inv - Extraction         = 500       Mel Extraction - InvMel Extraction - InvMel Extraction - InvDefense
None
Downsampling - Upsampling
Quantization - Dequantization
Filtering
Mel Extraction - Inversion
LPC
Mel Extraction - Inversion
LPC
Mel Extraction - Inversion
LPC
Distortion metrics
ε∞
500
500
500
500
500
500
1000
1000
4000
4000
|δ|∞
81
342
215
92
500
500
1000
1000
2461
2167
dBx(δ)
-45.3
-32.7
-36.7
-44.1
-29.4
-29.4
-23.5
-23.5
-15.1
-16.7
Attack Performance
Detection Scores
SR (xadv)
SR (g(xadv)) CER(xadv,τ) CER(g(xadv),τ) AUC
100%
100%
100%
91%
34%
43%
53%
72%
100%
100%
-
78%
81%
72%
0%
0%
0%
0%
31%
73%
0.00
0.00
0.00