For instance, the table contains only 1056 entries for a k = 64
fat-tree with over 65k hosts.
2 in-bound entries and k 2
5 DISCUSSION
5.1 ShareBackup Failure Handling
ShareBackup uses a separate control network for failure re-
covery, which raises the question as to how ShareBackup
handles failures in the control network itself. Below, we dis-
cuss failures on different components of the control system.
Circuit switch failures. Circuit switches are highly reli-
able physical layer devices [35]. They have bare-minimum
control software receiving infrequent reconﬁguration requests.
When the control software fails, a circuit switch becomes un-
responsive but keeps the existing circuit conﬁguration—the
data plane is not impacted. However, because ShareBackup
181
The network controllers for the involved switches coordi-
nate to change the circuit switch conﬁgurations and enforce
the switches to exchange testing messages. A suspect interface
that has connectivity in at least one conﬁguration is redressed
as healthy, so is the corresponding suspect switch. Because
auto-diagnosis only involves suspect switches already taken
ofﬂine and backup switches not in use, this process is com-
pletely independent of the functioning network.
Figure 2(b) illustrates the controller coordination process.
The two suspect interfaces are tested one by one. Their cor-
responding controllers elect one to be the initiator and the
other one as passive respondent. The initiator cycles through
the conﬁgurations to test the suspect interface on its side,
after which the initiator and the respondent reverse roles to
test the other suspect interface. As shown in Figure 2(a), in
our distributed control system, each controller is responsible
for reconﬁguring a small subset of circuit switches and is
only allowed to control the ports on its own side. So, both
controllers need to participate in the circuit setup. The re-
spondent controller learns the target connections from the
initiator controller via the conﬁguration ID. Here, we use the
original TL1 interface, i.e. connect(input_port, output_port),
to connect to the side ports. As an ofﬂine process, failure
diagnosis can be preempted by failure recovery. It is paused
if the involved backup switch, such as BS3, 1, 0 and BS3, 2, 0 in
Figure 3(c), needs to be used when another failure happens.
The initiator controller thus proceeds only after receiving con-
ﬁrmation that the respondent side is not being preempted at
the moment. It will continue with the next conﬁguration if
reaching the ACK timeout. In the end, the tested interface
pings the other end and terminates the diagnosis process if it
has connectivity.
Failure diagnosis requires both sides have at least one
healthy interface, so that both suspect interfaces can be tested.
If this condition is not met, both suspect switches are con-
sidered faulty. Since all hosts are actively in use, the ofﬂine
failure diagnosis is not supported between hosts and edge
switches. We assume switches are at fault for link failures to
hosts. If the problem is not ﬁxed after replacing the switch,
we mark the switch as healthy and trouble-shoot the host.
After a failed switch is repaired or a suspect switch is
exonerated, it is unnecessary to switch back to the original
connectivity. Backup switches and regular switches are equal
in functionality, so we keep the backup switch online and
turn the replaced switch into a backup switch for future use.
This design saves the reconﬁguration overhead and avoids
Masking Failures from Application Performance
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
can no longer reconﬁgure the circuit switch, the affected
failure group of the network has to fall back to traditional
rerouting when switch/link failures occur later. When a cir-
cuit switch hardware fails, such as port-downs, the connected
switches will experience real link failures. ShareBackup will
regard them as regular link failures and replace the affected
switches. In the rare case that a circuit switch is completely
down (e.g., power outage), the controller will receive a large
number of link failure reports in a short period of time. It
will stop failure recovery and request for human intervention.
Since each switch is connected to k
2 circuit switches, each of
them only loses 2
k capacity during the downtime. We note that
hardware/power failures are relatively rare in practice—most
of the failures are from the software layer such as switch
software/ﬁrmware bugs or conﬁguration errors [47]. Circuit
switches are generally free from these type of failures.
Control channel failures. A controller in ShareBackup
needs to communicate with both circuit switches and other
controllers. Control channel failures in the ﬁrst case are equiv-
alent to circuit switch software failures and are thus treated
by ShareBackup in the same way. For the second case, our
ofﬂine auto-diagnosis requires the coordination of two con-
trollers to reconﬁgure circuit switches simultaneously. If the
communication between the two controllers fails, the initiator
controller will stop auto-diagnosis after a timeout and call
for human intervention. We note that ofﬂine auto-diagnosis is
only performed on switches not in use and does not impact
the production trafﬁc. Finally, when a link failure happens,
switches on both sides of the link need to be replaced, which
is handled by two separate controllers. If one side of the
replacement is not successful, the other side will continue
experiencing connection error on the replaced switch. To han-
dle this issue, a backup switch will fall back to rerouting if
connection error is not resolved after replacement.
Controller failures. Our distributed controllers are intrin-
sically robust. Each controller only keeps a small number
of runtime states of current circuit conﬁgurations, and it is
straightforward to protect against controller failures by state
replication on a shadow controller.
5.2 Cost Analysis
We make key design decisions in ShareBackup to reduce ex-
tra cost. Concurrent failures are rare in data centers [16], so
the ideal case is to have a single backup switch shared by
the entire network. However, as discussed at the beginning of
Section 3, this requires cascaded circuit switches with high
cost, insertion loss, and switching delay. As a compromise,
we deploy low-cost circuit switches with short switching de-
lay, e.g. electrical crosspoint switch or optical 2D-MEMS, in
separate failure groups. Our targeted circuit switches have
modest port count. As Figure 1 shows, we combine them
182
to cover more switches and form larger failure groups. Our
design achieves a reasonably low backup ratio at low circuit
switch cost. The additional cabling cost is minimal, because
the circuit switches, either electrical or optical, are passive
and do not require active elements, e.g. optical transceiver
or ampliﬁer for copper. The extra cost introduced by Share-
Backup on a k = 48 fat-tree network with 27648 servers is
estimated to be 6.7% [48]. And this cost does not scale up as
the network size increases—the larger the failure groups, the
smaller the backup ratio and hence the lower the extra cost.
ShareBackup can be partially deployed at different network
layers or pods, which helps further reduce cost. In today’s
data centers, a host connects to one ToR switch only. If ToR
or host link failures happen, hosts are disconnected and we
have to rely on application frameworks to restart the work
elsewhere. Our testbed experiment in Figure 11 shows Spark
and Tez jobs get delayed by upto 4.1× in such case. There-
fore, ShareBackup is especially powerful at the edge layer.
In a fat-tree network, there are k 2
4 parallel paths in the core
layer, but only k
2 in the aggregation layer. ShareBackup is
more helpful in the aggregation layer, as rerouting may cause
greater congestion with fewer paths to balance the load. Par-
tial deployment is straightforward in ShareBackup thanks to
the separate failure groups. We give a complete solution in
this paper, but network operators have the freedom to deploy
backup switches in certain areas of the network according to
application requirements and monetary budget.
5.3 Beneﬁts to Network Management
When switches are routinely taken out for upgrade or mainte-
nance, backup switches can neatly take their place to avoid
downtime. Misconﬁgurations account for a large proportion
of failures in data centers [16], and they are hard to reason and
ﬁx. ShareBackup can help mitigate the effect and diagnose
the problem. The conﬁgurations of backup switches can be
veriﬁed when they are idle. If a switch is misconﬁgured, it
can failover to the backup switch whose conﬁgurations are
guaranteed to be correct. Then complicated diagnosis can be
executed ofﬂine. With judicious use of hardware, our ofﬂine
diagnosis in Section 4.3 helps identify which interface has
caused a link failure. In today’s data centers, failure diagnosis
and repair are mostly handled manually and take hours at least.
Even pioneering work like NetPilot takes 20 minutes only to
mitigate failures [47]. Our implementation demonstrates later
in Section 6.5 that ShareBackup automatically repairs failures
in sub-ms and diagnoses failures in sub-second, which is a
breakthrough for data center management.
5.4 Alternatives in the Design Space
An interesting question is whether PortLand and F10 will
outperform ShareBackup if allowing the same deployment
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
D. Wu et al.
Table 2: Road map of experiment setups.
Experiment
Section Workload
Failure model
Platform
TCP disruption
Control plane latency
Practical bandwidth
Theoretical bandwidth
FCT/CCT slowdown
Job deadlines
Throughput-intensive app
Latency-sensitive app
-
iPerf ﬂows
Single ﬂow
Single
Single
Rand layered
6.4
6.5
6.6
6.6
6.7
6.7
6.8 Word2Vec, Sort Rand layered
6.8
Rand layered
Coﬂow trace
Deadline trace
TPC-H
Synthetic trafﬁc Rand layered LP simulator
Real
Real
Packet simulator
Packet simulator
Testbed
Testbed
Testbed
Testbed
Testbed
cost, e.g., employing the same number of ShareBackup’s
backup switches as their production switches. However, tree
networks are known to lack expandability. It is hard, if not
impossible, to add only a small proportion of switches with
the same port count to a fully populated tree network. Even if
more switches could be added, they would lock up bandwidth
to ﬁxed locations. Failures at highly unpredictable locations
might still cause bandwidth loss. In contrast, ShareBackup
can move backup switches to wherever needed. Unstructured
networks have been proposed for easier expansion [21, 38,
42], but the performance under failures is yet to be explored.
Admittedly, these topologies have rich bandwidth and diverse
paths, but the path length hugely varies, causing risk of path
dilation. We can add switches to either provision bandwidth
at the price of degraded performance under failures, or to
provide guaranteed performance while keeping the backups
idle most of the time. We choose the latter, and we believe
shareable backup is an effective way to reduce the idle rate.
6 IMPLEMENTATION AND EVALUATION
A prior study has demonstrated the cost advantage of Share-
Backup: using 1 backup switch per failure group, Share-
Backup only costs 6.7% more than fat-tree; and it is still
more cost-effective than other redundancy-featured architec-
tures with 4 backup switches per failure group [48]. However,
this work lacks implementation of the ShareBackup system
and performance evaluation against alternative solutions.
In this paper, we conduct comprehensive evaluations about
the ShareBackup performance using both testbed implemen-
tation and large-scale simulations. Table 2 is a road map
showing the setup of each experiment. First, we explore key
properties of the ShareBackup system on the testbed, includ-
ing failure recovery delay, TCP behavior during the transient
state, and overhead of the control system. Since the major ad-
vantage of ShareBackup against other fault-tolerant network
architectures is to restore bandwidth after failures, we next
compare their bandwidth capacity with both Linear Program-
ming simulations and testbed experiments. Next, we evaluate
ShareBackup’s transmission performance using packet-level
simulations with practical routing and transport protocols.
To simulate real-world scenarios, we use trafﬁc traces and a
failure model from production data centers. Finally, we run
183
(cid:20)(cid:5)(cid:21) (cid:2)(cid:15)(cid:17)(cid:18)(cid:17)