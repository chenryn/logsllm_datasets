parameters of the SVM classiﬁer are not known by the
attacker. Compared with the original Drebin SVM clas-
siﬁer, the default setting in #4 uses a larger regulariza-
tion parameter. This suggests that regularization can help
mitigate the impact of individual poison instances, but
the adversary may nevertheless be successful by inject-
ing more crafted instances in the training set.
Instance knowledge. Experiments #5–6 look at a sce-
nario in which the known instances are subsets of S∗.
Unsurprisingly, the attack is more effective as more in-
stances from S∗ become available. The attacker’s in-
ability to train a robust surrogate classiﬁer is reﬂected
through the large perceived PDR. For experiments #7–
8, victim training instances are not available to the at-
tacker, their classiﬁer being trained on samples from the
same underlying distribution as S∗. Under these con-
straints, the adversary could only approximate the effect
of the attack on the targeted classiﬁer. Additionally, the
training instances might be signiﬁcantly different than
the base instances available to the adversary, canceling
the effect of crafted instances. The results show, as in
the case of the image classiﬁer, that poison instances are
highly dependent on other instances present in the train-
ing set to bootstrap their effect on target misclassiﬁca-
tion. We further look at the impact of limited Lever-
age on the attack effectiveness. Experiments #9–11 look
at various training set sizes for the case where only the
features extracted from AndroidManifest.xml are modiﬁ-
USENIX Association
27th USENIX Security Symposium    1311
ࢯIࢯ/SR%/PDR
StingRay
16/70/0.97
77/50/0.99
7/6/1.00
18/34/0.98
Images
Malware
Exploits
Breach
RONI
tRONI
MM
Fix%/PDR
-/-
0/0.98
0/0.97
-/-
-/-
15/0.98
40/0.67
20/0.96
-/-
-/-
0/0.33
55/0.91
Table 6: Effectiveness of StingRay and of existing defenses against
it on all applications. Each attack cell reports the average number of
poison instancesࢯIࢯ, the SR and actual PDR. Each defense cell reports
the percentage of ﬁxed attacks and the PDR after applying it.
able. These features correspond to approximately 40%
of the 545,333 existing features. Once again, we observe
that the effectiveness of a constrained attacker is reduced.
This signals that a viable defense could be to extract fea-
tures from uncorrelated sources, which would limit the
leverage of such an attacker.
The FAIL analysis on the malware classiﬁer reveals
that the actual drop in performance of the attacks is in-
signiﬁcant on all dimensions, but the attack effectiveness
is generally decreased for weaker adversaries. However,
feature secrecy and limited leverage appear to have the
most signiﬁcant effect on decreasing the success rate,
hinting that they might be a viable defense.
5.2 Effectiveness of StingRay
In this section we explore the effectiveness of StingRay
across all applications described in 4.2 and compare ex-
isting defense mechanisms in terms of their ability to pre-
vent the targeted mispredictions. Table 6 summarizes our
ﬁndings. Here we only consider the strongest (white-
box) adversary to determine upper bounds for the re-
silience against attacks, without assuming any degree of
secrecy.
Image classiﬁer. We observe that the attack is success-
ful in 70% of the cases and yields an average PDR of
0.97, requiring an average of 16 instances. Upon further
analysis, we discovered that the performance drop is due
to other testing instances similar to the target being mis-
classiﬁed as yd. By tuning the attack parameters (e.g.
the layer used for comparing features or the degree of
allowed perturbation) to generate poison instances that
are more speciﬁc to the target, the performance drop on
the victim could be further reduced at the expense of
requiring more poisoning instances. Nevertheless, this
shows that neural nets deﬁne a ﬁne-grained boundary be-
tween class-targeted and instance-targeted poisoning at-
tacks and that it is not straightforward to discover it, even
with complete adversarial knowledge.
None of the three poisoning defenses are applicable on
this task. RONI and tRONI require training over 50,000
classiﬁers for each level of inspected negative impact.
This is prohibitive for neural networks which are known
to be computationally intensive to train. Since we could
not determine reliable timestamps for the images in the
data set, MM was not applicable either.
Malware classiﬁer. StingRay succeeds in half of the
cases and yields a negligible performance drop on the
victim. The attack being cut off by the crafting budget
on most failures (Cost B.VII) suggests that some targets
might be too ”far” from the class separator and that mov-
ing this separator becomes difﬁcult. Nevertheless, un-
derstanding what causes this hardness remains an open
question.
On defenses, we observe that RONI often fails to
build correctly-predicting folds on Drebin and times out.
Hence, we investigate the defenses against only 97 suc-
cessful attacks for which RONI did not timeout. MM
rejects all training instances while RONI fails to detect
any attack instances. tRONI detects very few poison in-
stances, ﬁxing only 15% of attacks, as they do not have a
large negative impact, individually, on the misclassiﬁca-
tion of the target. None of these defenses are able to ﬁx
a large fraction of the induced mispredictions.
Exploit predictor. While poisoning a small number of
instances, the attack has a very low success rate. This is
due to the fact that the non-Twitter features are not mod-
iﬁable; if the data set does not contain other vulnerabili-
ties similar to the target (e.g. similar product or type), the
attack would need to poison more CVEs, reaching Nmax
before succeeding. The result, backed by our FAIL anal-
ysis of the other linear classiﬁer in Section 5.1, highlights
the beneﬁts of built-in leverage limitations in protecting
against such attacks.
MM correctly identiﬁes the crafted instances but also
marks a large fraction of positively-labeled instances as
suspicious. Consequently, the PDR on the classiﬁer is
severely affected. In instances where it does not timeout,
RONI fails to mark any instance. Interestingly, tRONI
marks a small fraction of attack instances which helps
correct 40% of the predictions but still hurting the PDR.
The partial success of tRONI is due to two factors: the
small number of instances used in the attack and the lim-
ited leverage for the attacker, which boosts the negative
impact of attack instances through resampling. We ob-
served that due to variance, the negative impact com-
puted by tRONI is larger than the one perceived by the
attacker for discovered instances. The adversary could
adapt by increasing the conﬁdence level of the statistic
that reﬂects negative impact in the StingRay algorithm.
Data breach predictor: The attacks for this application
correspond to two scenarios, one with limited leverage
over the number of time series features. Indeed, the one
in which the attacker has limited leverage has an SR of
5%, while the other one has an SR of 63%. This cor-
roborates our observation of the impact of adversarial
1312    27th USENIX Security Symposium
USENIX Association
leverage for the exploit prediction. RONI fails due to
consistent timeouts in training the random forest classi-
ﬁer. tRONI ﬁxes 20% of the attacks while decreasing the
PDR slightly. MM is a natural ﬁt for the features based
on time series and is able to build more balanced voting
folds. The defense ﬁxes 55% of mispredictions, at the
expense of lowering the PDR to 0.91.
Our results suggest that StingRay is practical against a
variety of classiﬁcation tasks—even with limited degrees
of leverage. Existing defenses, where applicable, are eas-
ily bypassed by lowering the required negative impact of
crafted instances. However, the reduced attack success
rate on applications with limited leverage suggests new
directions for future defenses.
6 Related Work
Several studies proposed ways to model adversaries
[25] proposes FTC
against machine learning systems.
—features, training set, and classiﬁer, a model to de-
ﬁne an attacker’s knowledge and capabilities in the case
of a practical evasion attack. Unlike the FTC model,
the FAIL model is evaluated on both test- and training-
time attacks, enables a ﬁne-grained analysis of the di-
mensions and includes Leverage. These characteristics
allow us to better understand how the F and L dimen-
sions inﬂuence the attack success. Furthermore, [27, 7]
introduce game theoretical Stackelberg formulations for
the interaction between the adversary and the data miner
in the case of data manipulations. Adversarial limita-
tions are also discussed in [22]. Several attacks against
machine learning consider adversaries with varying de-
grees of knowledge, but they do not cover the whole
spectrum [4, 35, 37]. Recent studies investigate transfer-
ability, in attack scenarios with limited knowledge about
the target model [36, 28, 9]. The FAIL model uniﬁes
these dimensions and can be used to model these capabil-
ities systematically across multiple attacks under realistic
assumptions about adversaries. Unlike game theoretical
approaches, FAIL does not assume perfect knowledge on
either the attacker or the defender. By deﬁning a wider
spectrum of adversarial knowledge, FAIL generalizes the
notion of transferability.
Prior work introduced indiscriminate and targeted poi-
soning attacks. For indiscriminate poisoning, a spam-
mer can force a Bayesian ﬁlter to misclassify legitimate
emails by including a large number of dictionary words
in spam emails, causing the classiﬁer to learn that all to-
kens are indicative of spam [3] An attacker can degrade
the performance of a Twitter-based exploit predictor by
posting fraudulent tweets that mimic most of the features
of informative posts [40]. One could also the damage
overall performance of an SVM classiﬁer by injecting a
small volume of crafted attack points [5]. For targeted
poisoning, a spammer can trigger the ﬁlter against a spe-
ciﬁc legitimate email by crafting spam emails resembling
the target [34]. This was also studied in the healthcare
ﬁeld, where an adversary can subvert the predictions for
a whole target class of patients by injecting fake patient
data that resembles the target class [32]. StingRay is a
model-agnostic targeted poisoning attack and works on
a broad range of applications. Unlike existing targeted
poisoning attacks, StingRay aims to bound indiscrimi-
nate damage to preserve the overall performance.
In
[51],
On neural networks, [23] proposes a targeted poison-
ing attack that modiﬁes training instances which have
a strong inﬂuence on the target loss.
the
poisoning attack is a white-box indiscriminate attack
adapted from existing evasion work. Furthermore, [29]
and [20] introduce backdoor and trojan attacks where ad-
versaries cause the classiﬁers to misbehave when a trig-
ger is present in the input. The targeted poisoning at-
tack proposed in [11] requires the attacker to assign la-
bels to crafted instances. Unlike these attacks, StingRay
does not require white-box or query access the original
model. Our attack does not require control over the la-
beling function or modiﬁcations to the target instance.
7 Discussion
The vulnerability of ML systems to evasion and poi-
soning attacks leads to an arms race, where defenses
that seem promising are quickly thwarted by new at-
tacks [17, 37, 38, 9]. Previous defenses make implicit as-
sumptions about how the adversary’s capabilities should
be constrained to improve the system’s resilience to at-
tacks. The FAIL adversary model provides a framework
for exposing and systematizing these assumptions. For
example, the feature squeezing defense [49] constrains
the adversary along the A and F dimensions by modify-
ing the input features and adding an adversarial exam-
ple detector. Similarly, RONI constrains the adversary
along the I dimension by sanitizing the training data.
The ML-based systems employed in the security indus-
try [21, 10, 39, 12], often rely on undisclosed features
to render attacks more difﬁcult, thus constraining the F
dimension. In Table 2 we highlight implicit and explicit
assumptions of previous defenses against poisoning and
evasion attacks.
Through our systematic exploration of the FAIL di-
mensions, we provide the ﬁrst experimental comparison
of the importance of these dimensions for the adversary’s
goals, in the context of targeted poisoning and evasion at-
tacks. For a linear classiﬁer, our results suggest that fea-
ture secrecy is the most promising direction for achieving
attack resilience. Additionally, reducing leverage can in-
crease the cost for the attacker. For a neural network
based image recognition system, our results suggest that
USENIX Association
27th USENIX Security Symposium    1313
StingRay’s samples are transferable across all dimen-
sions. Interestingly, limiting the leverage causes the at-
tacker to craft instances that are more potent in triggering
the attack. We also observed that secrecy of training in-
stances provides limited resilience.
Furthermore, we demonstrated that the FAIL adver-
sary model is applicable to targeted evasion attacks as
well. By systemically capturing an adversary’s knowl-
edge and capabilities, the FAIL model also deﬁnes a
more general notion of attack transferability. In addition
to investigating transferability under certain dimensions,
such as the A dimension in [9] or A and I dimensions
in [37], generalized transferability covers a broader range
of adversaries. At odds with the original ﬁndings in [37],
our results suggest a lack of generalized-transferability
for a state of the art evasion attack; while highlighting
feature secrecy as the most prominent factor in reduc-
ing the attack success rate. Future research may utilize
this framework as a vehicle for reasoning about the most
promising directions for defending against other attacks.
Our results also provide new insights for the broader
debate about the generalization capabilities of neural net-
works. While neural networks have dramatically re-
duced test-time errors for many applications, which sug-
gests they are capable of generalization (e.g. by learn-
ing meaningful features from the training data), recent
work [53] has shown that neural networks can also mem-
orize randomly-labeled training data (which lack mean-
ingful features). We provide a ﬁrst step toward under-
standing the extent to which an adversary can exploit this
behavior through targeted poisoning attacks. Our results
are consistent with the hypothesis that an attack, such as
StingRay, can force selective memorization for a target
instance while preserving the generalization capabilities
of the model. We leave testing this hypothesis rigorously
for future work.
8 Conclusions
We introduce the FAIL model, a general framework for