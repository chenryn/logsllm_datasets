their product design, the hardware failures no longer matter
much – even the redundancy level is restored automatically
in the modern distributed systems soon after a failure. This
is especially true for product lines with large-scale Hadoop
clusters. Also, operators want to batch up similar failures to
improve efﬁciency.
VII. DISCUSSIONS
During the study, we ﬁnd that much old wisdom to improve
system dependability still holds. For example,
1) Most failures are independent, but the software needs
to handle occasional correlated or repeating failures on single
or multiple components. Given that these correlated failures
are more common than before, we need to improve software
resilience accordingly.
2) Automatic hardware failure detection and handling can
be very accurate, signiﬁcantly reducing human labor. However,
due to the existence of multi-component failures, we need to
improve these systems to cover the new failure types.
3) In data center design, we need to avoid “bad spots”
where the failure rate is higher, or at least avoid allocating all
replicas of a service in these vulnerable zones.
In addition, we discuss in more detail about
two new
important ﬁndings and their implications. We also summarize
the limitations of this trace-based study in this section.
A. Is hardware reliability still relevant in data centers?
The advancement of software redundancy and automatic
failure handling signiﬁcantly improve the end-to-end depend-
ability for Internet services. MTTR for hardware components
seems less important than before, given the observations in
operators’ RT for failures.
There is a quite big team in the company managing data
center hardware. This team designed some software tools,
Fig. 10. The CDF of RT for each component class.
Fig. 11. The relation between the median RT to HDD failures and the number
of HDD failures occurred on some randomly sampled product lines during the
year 2015. Each point P (x, y) indicates that the product line that corresponds
to P encountered x HDD failures during the year 2015, and the median RT
to the failures is y days. Note that the horizontal axis is on a logarithm scale.
shortest (several hours), while those of hard drives, fans, and
memory are the longest (7 to 18 days).
We ﬁnd that there are both technical and business rea-
sons for the vastly different RT for SSD and hard drives.
Technically, the fault tolerance in hard drives (both hardware
and software) are the most mature, making these failures less
urgent. In comparison, the redundancy in SSDs is less due to
higher SSD cost. Also, business-wise, only crucial and user-
facing online service product lines afford SSDs, and they have
more strict operation guidelines.
The short response time for miscellaneous failures is arti-
ﬁcial. Miscellaneous failures are manually entered, and they
happen mostly when the hardware get deployed. During the
deployment period, operators streamline installation, testing
and debugging of new servers, effectively reducing the RT s.
Also ﬁxing new servers do not need to take down jobs, another
convenience factor for quick response.
C. RT in different product lines
Although the company centrally monitors all servers, it
partitions the servers to different product lines. Each product
34
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:01:13 UTC from IEEE Xplore.  Restrictions apply. 
algorithms and operation guidelines to improve hardware de-
pendability. The monitoring system is quite effective, with very
little false positives (Table I). They even designed a tool to
predict component failures a couple of days early, hoping the
operators to react before the failure actually happens. However,
it looks like the operators are less motivated to handle failures
than we expect.
It
is surprising to see that operators ignore not only
predictive warnings but also leave failed component unhandled
for days or even months. Operators also choose to leave some
partially failed out-of-warranty servers in the service too.
Delayed repair brings extra cost in many aspects. First,
tolerating more failures lead to software complexity and devel-
opment cost. Although open source solutions often offset this
cost, they still come with high management overhead. Second,
hardware failures reduce the overall capacity of the system.
Even worse, unhandled hardware failures add up and
eventually appear to software as batch or synchronous failures,
as Section V-C describes. Thus software developers have to de-
sign more aggressive methods for fault tolerance. This process
is a downward slope towards the undesirable situation when
people use unnecessarily complex software to handle hardware
failure models that we can avoid easily at the hardware level.
We believe it is time to start considering failure handling
as a joint optimization effort across software, hardware and
operation layers, to lower the total cost of ownership of the
data centers. In other words, data center dependability should
evolve to the next level, where we not only focus on reliability
metrics of a single component or a single layer but also
consider how to achieve these metrics at low total cost.
B. Problems with the “stateless” failure handling system
In our FMS, the failure records present themselves as indi-
vidual FOTs, including both diagnosis and operator responses.
All closed FOTs become the archived log entries. This stateless
design makes FMS simple and scalable. However, it becomes
difﬁcult to reveal the connections among different FOTs.
In fact, many FOTs are strongly connected – there are
repeating or batch failures. The correlation information is
lost
in FMS, and thus operators have to treat each FOT
independently. Of course, a single operator or a small operator
team might learn from past failures, but given the annual
operator turnover rate of over 50%, it is hard to accumulate
actual operation experience and tools, except for a wiki-like
documentation system that is difﬁcult to maintain.
Thus, we believe it may be useful to build a data mining
tool to discover the correlations among these failures. More
importantly, we need to provide operators with related infor-
mation about an FOT, such as the history of the component,
the server,
its environment, and the workload. This extra
information can help operators reduce the number of repeating
failures effectively. Considering the advances in data science
techniques, we believe such system has become feasible.
C. Limitations of this study
Like all data-driven study, this work has its intrinsic limi-
tations. First, although the dataset covers most of the failures,
there are missing data points. Also, people incrementally rolled
out FMS during the four years, and thus the actual coverage
might vary over the four years and in different data centers.
Also, lacking matching workload and other detailed mon-
itoring data, we can only describe the statistical observations
and make educated guesses about the possible reasons. We
made our best effort conﬁrming our guesses with the operators,
but there are still unconﬁrmed cases as the operators do not
always remember the details.
Finally, the dataset comes from a single company, and
may not represent the entire industry. However, given that the
failures are from different generations of hardware, happen
in dozens of distinct data centers (owned or leased, large or
small), and are separately managed by hundreds of product
lines running different workload, we believe they are good
representations of state-of-the-art Internet data centers.
VIII. RELATED WORK
Much previous work has studied characteristics of system
failures extensively to understand the failure properties, which
is crucial to highly reliable large-scale systems design. Most of
these studies focus on high performance computers (HPCs) [5–
15, 26]. They mainly focus on the composition of failure types,
the temporal and spatial properties of failures, the statistical
properties of time between failures and repair times, as well
as correlated failures. However, as we have seen in the paper,
the failure model in data centers is quite a difference from
HPCs, due to the heterogeneity in hardware and workload.
Not as many recent studies focus on failures in commercial
data centers. Vishwanath et al. [30] analyzed a 14-month slice
in time of hardware repair log for over 100,000 servers. Ford et
al. [28] analyzed the availability properties including corre-
lated failure properties of Google storage clusters with tens of
thousands of servers during a period of one year. Garraghan et
al. [24] analyzed Google Cloud trace log consisting event logs
of 12,500 servers over a period of 29 days. Birk et al. [25]
analyzed both physical and virtual machine crash tickets from
ﬁve commercial data centers consisting about 10K servers
during a period of one year. Our work uses a much larger
dataset with more component classes than the hard drive. We
also observe more correlated failures and much longer MTTR
in some of our product lines.
There are also many studies focusing on the failure char-
acteristics on hardware component level, such as disks [16,
17, 27], memory [18–20], SSDs [31, 32], and GPUs [11, 21].
We use many similar analytic methods, such as hypothesis tests
and various distribution assumptions from these work. We also
borrow some metrics describing component failure properties,
such as the metrics describing failure types, temporal distribu-
tions, TBF, and operator response time. In addition to using
a very recent large-scale failure dataset, we focus on the data
center as a whole, analyzing correlation among failures, batch
failures, and repeating failures, making our study distinct from
existing ones.
While much research analyzes the MTTR of failures in
large-scale systems, we particularly focus on the operators’
response component in MTTR and discovering the human
factors of failure handling.
35
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:01:13 UTC from IEEE Xplore.  Restrictions apply. 
IX. CONCLUSION
Hardware failure study is a decades-old topic. However,
as both researchers and data center practitioners, we an-
swer the following two questions: 1) how do failure patterns
evolve, given the signiﬁcant technology, economy and demand
changes in the Internet service data centers recently? 2) given
that we can get advanced software fault tolerance for free from
open source community, is hardware reliability still relevant in
data center operations?
We statistically analyze hundreds of thousands of failure
tickets from dozens of production data centers, analyzing the
failure patterns across time, space, component and product
lines. We analyze all main components instead of a single
component class, and we focus on correlations among different
failures instead of individual failures. Our study reconﬁrms or
extends many ﬁndings from previous work, and also observes
many new patterns in failure models and operators’ behavior.
Our work provides a fresh and deeper understanding of the
failure patterns in modern data centers. The understanding not
only helps to operate the data centers better but also calls
for a joint effort in software and hardware fault tolerance
mechanism, which minimizes the overall cost.
ACKNOWLEDGMENT
We would like to thank our colleagues at Baidu Inc. Yang
Wang for providing the datasets, Wei Wang, Xingxing Liu,
Kai Lang, and Qian Qiu for helpful comments on the failure
analysis. We also thank the students of Tsinghua University
Yong Xiang, Xin Liu, Hao Xue, Cheng Yang, Han Shen, Yiran
Li, and Yi Li for helpful comments on the drafts of this paper.
This research is supported in part by the National Natural
Science Foundation of China (NSFC) grant 61532001, Ts-
inghua Initiative Research Program Grant 20151080475, MOE
Online Education Research Center (Quantong Fund) grant
2017ZD203, and gift funds from Huawei and Ant Financial.
REFERENCES
[1] L. A. Barroso, J. Clidaras, and U. H¨olzle, “The datacenter as a computer:
An introduction to the design of warehouse-scale machines,” Synthesis
lectures on computer architecture, vol. 8, no. 3, pp. 1–154, 2013.
[2] D. Oppenheimer, A. Ganapathi, and D. A. Patterson, “Why do Internet
services fail, and what can be done about it?” in USENIX symposium
on internet technologies and systems, vol. 67. Seattle, WA, 2003.
[3] D. A. Patterson et al., “A simple way to estimate the cost of downtime.”
in LISA, vol. 2, 2002, pp. 185–188.
[4] J. Gray, “Why do computers stop and what can be done about it?” in
Symposium on reliability in distributed software and database systems.
Los Angeles, CA, USA, 1986, pp. 3–12.
[5] B. Schroeder and G. Gibson, “A large-scale study of failures in high-
performance computing systems,” IEEE Transactions on Dependable
and Secure Computing, vol. 7, no. 4, pp. 337–350, 2010.
[6] A. Oliner and J. Stearley, “What supercomputers say: A study of ﬁve
system logs,” in International Conference on Dependable Systems and
Networks (DSN’07).
IEEE, 2007.
[7] N. El-Sayed and B. Schroeder, “Reading between the lines of failure
logs: Understanding how HPC systems fail,” in International Conference
IEEE, 2013, pp. 1–12.
on Dependable Systems and Networks (DSN’13).
[8] S. Ghiasvand, F. M. Ciorba, R. Tsch, W. E. Nagel et al., “Lessons
learned from spatial and temporal correlation of node failures in high
performance computers,” in PDP 2016.
IEEE, 2016.
[9] C. Di Martino, Z. Kalbarczyk, R. K. Iyer, F. Baccanico, J. Fullop, and
W. Kramer, “Lessons learned from the analysis of system failures at
petascale: The case of blue waters,” in International Conference on
Dependable Systems and Networks (DSN’14).
IEEE, 2014.
[10] Y. Liang, Y. Zhang, A. Sivasubramaniam, M. Jette, and R. Sahoo,
“Bluegene/L failure analysis and prediction models,” in International
Conference on Dependable Systems and Networks (DSN’06).
IEEE,
2006, pp. 425–434.
[11] D. Tiwari, S. Gupta, J. Rogers, D. Maxwell, P. Rech, S. Vazhkudai,
D. Oliveira, D. Londo, N. DeBardeleben, P. Navaux et al., “Understand-
ing GPU errors on large-scale HPC systems and the implications for
system design and operation,” in HPCA 2015.
IEEE, 2015.
[12] Y. Liang, Y. Zhang, A. Sivasubramaniam, R. K. Sahoo, J. Moreira, and
M. Gupta, “Filtering failure logs for a BlueGene/L prototype,” in Inter-
national Conference on Dependable Systems and Networks (DSN’05).
[13] A. Pecchia, D. Cotroneo, Z. Kalbarczyk, and R. K. Iyer, “Improving
log-based ﬁeld failure data analysis of multi-node computing systems,”
in International Conference on Dependable Systems and Networks
(DSN’11).
IEEE, 2011, pp. 97–108.
[14] S. Gupta, D. Tiwari, C. Jantzi, J. Rogers, and D. Maxwell, “Understand-
ing and exploiting spatial properties of system failures on extreme-scale
HPC systems,” in International Conference on Dependable Systems and
Networks (DSN’15).
IEEE, 2015.
[15] B. Schroeder and G. A. Gibson, “Understanding failures in petascale
computers,” in Journal of Physics: Conference Series, vol. 78, no. 1.
IOP Publishing, 2007, p. 012022.
[16] E. Pinheiro, W.-D. Weber, and L. A. Barroso, “Failure trends in a large
disk drive population.” in FAST, 2007.
[17] B. Schroeder and G. A. Gibson, “Disk failures in the real world: What
does an MTTF of 1,000,000 hours mean to you?” in FAST, 2007.
[18] B. Schroeder, E. Pinheiro, and W.-D. Weber, “DRAM errors in the wild:
a large-scale ﬁeld study,” in ACM SIGMETRICS Performance Evaluation
Review, vol. 37, no. 1. ACM, 2009, pp. 193–204.
[19] J. Meza, Q. Wu, S. Kumar, and O. Mutlu, “Revisiting memory errors in
large-scale production data centers: Analysis and modeling of new trends
from the ﬁeld,” in International Conference on Dependable Systems and
Networks (DSN’15).
IEEE, 2015.
[20] L. Bautista-Gomez, F. Zyulkyarov, O. Unsal, and S. Mcintosh-Smith,
“Unprotected computing: a large-scale study of DRAM raw error rate
on a supercomputer,” in International Conference for High PERFOR-
MANCE Computing, Networking, Storage and Analysis, 2016.
study of soft-errors on GPUs in the ﬁeld,” in HPCA 2016.
[21] B. Nie, D. Tiwari, S. Gupta, E. Smirni, and J. H. Rogers, “A large-scale
IEEE, 2016.
[22] R. K. Sahoo, M. S. Squillante, A. Sivasubramaniam, and Y. Zhang, “Fail-
ure data analysis of a large-scale heterogeneous server environment,”
in International Conference on Dependable Systems and Networks
(DSN’04).
IEEE, 2004.
[23] T. N. Minh and G. Pierre, “Failure analysis and modeling in large multi-
site infrastructures,” in IFIP International Conference on Distributed
Applications and Interoperable Systems. Springer, 2013, pp. 127–140.
[24] P. Garraghan, P. Townend, and J. Xu, “An empirical failure-analysis of a
IEEE, 2014.
[25] R. Birke, I. Giurgiu, L. Y. Chen, D. Wiesmann, and T. Engbersen,
“Failure analysis of virtual and physical machines: patterns, causes and
characteristics,” in International Conference on Dependable Systems and
Networks (DSN’14).
large-scale cloud computing environment,” in HASE 2015.
IEEE, 2014.
[26] D. Tiwari, S. Gupta, and S. S. Vazhkudai, “Lazy checkpointing: Ex-
ploiting temporal locality in failures to mitigate checkpointing overheads
on extreme-scale systems,” in International Conference on Dependable
Systems and Networks (DSN’14).
IEEE, 2014.
[27] J. Yang and F. B. Sun, “A comprehensive review of hard-disk drive relia-
bility,” in Reliability and Maintainability Symposium, 1999. Proceedings.
[28] D. Ford, F. Labelle, F. I. Popovici, M. Stokely, V. A. Truong, L. Barroso,
C. Grimes, and S. Quinlan, “Availability in globally distributed storage
systems,” in Usenix Symposium on Operating Systems Design and
Implementation, OSDI 2010.
[29] A. Fox, “Toward recovery-oriented computing,” in VLDB 2002. VLDB
Endowment, 2002.
[30] K. V. Vishwanath and N. Nagappan, “Characterizing cloud computing
hardware reliability.” in ACM Symposium on Cloud Computing, 2010.
[31] J. Meza, Q. Wu, S. Kumar, and O. Mutlu, “A large-scale study of ﬂash
memory failures in the ﬁeld,” ACM Sigmetrics Performance Evaluation
Review, vol. 43, no. 1, pp. 177–190, 2015.
[32] I. Narayanan, D. Wang, M. Jeon, B. Sharma, L. Caulﬁeld, A. Sivasub-
ramaniam, B. Cutler, J. Liu, B. Khessib, and K. Vaid, “SSD failures
in datacenters: What, when and why?” ACM Sigmetrics Performance
Evaluation Review, vol. 44, no. 1, pp. 407–408, 2016.
36
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:01:13 UTC from IEEE Xplore.  Restrictions apply.