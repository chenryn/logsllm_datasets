times larger than our test machine’s RAM capacity.
Table 2 presents our results for two membership auditing
scenarios. In our ﬁrst scenario we requested membership
proofs for random events chosen among the most recent
5 million events inserted. Our prototype generated 8,600
self-contained membership proofs per second, averaging
2,400 bytes each. In this high-locality scenario, the most
recent 5 million events were already sitting in RAM. Our
second scenario examined the situation when audit re-
quests had low locality by requesting membership proofs
for random events anywhere in the log. The logger’s
performance was limited to our disk’s seek latency. Proof
size averaged 3,100 bytes and performance degraded to
32 membership proofs per second. (We discuss how this
might be overcome in Section 6.2.)
To test the scalability of the history tree, we bench-
marked insert performance and auditing performance on
our original 4 million event syslog event trace, without
replication, and the 80 million event trace after 20x
replication. Event insertion and incremental auditing are
roughly 10% slower on the larger log.
5.2 Performance of auditors and clients
The history tree places few demands upon auditors
or clients. Auditors and clients must verify the logger’s
commitment signatures and must verify the correctness
of pruned tree replies to auditing requests. Our machine
can verify 1,900 DSA-1024 signatures per second. Our
current tree parser is written in Python and is rather slow.
It can only parse 480 pruned trees per second. Once
the pruned tree has been parsed, our machine can verify
9,000 incremental or membership proofs per second.
Presently, one auditor cannot verify proofs as fast as the
logger can generate them, but auditors can clearly operate
independently of one another, in parallel, allowing for
exceptional scaling, if desired.
5.3 Merkle aggregation results
In this subsection, we describe the beneﬁts of Merkle
aggregation in generating query results and in safe
deletion.
In our experiments, due to limitations of our
implementation in generating large pruned trees, our
Merkle aggregation experiments used the smaller four
million event log.
We used 86 different predicates to investigate the
beneﬁts of safe deletion and the overheads of Merkle
aggregation queries. We used 52 predicates, each match-
ing one tag, 13 predicates, each matching one host, 9
predicates, each matching one facility, 6 predicates, one
matching each level, and 6 predicates, each matching the
k highest logging levels.
The predicates matching tags and hosts use Bloom
ﬁlters, are inexact, and may have false positives. This
causes 34 of the 65 Bloom ﬁlter query results to include
more nodes than our “worst case” expectation for exact
predicates. By using larger Bloom ﬁlters, we reduce
the chances of spurious matches. When a 4-of-64
Bloom ﬁlter is used for tags and hostnames, pruned trees
resulting from search queries average 15% fewer nodes,
at the cost of an extra 64 bits of attributes for each node
in the history tree.
In a real implementation, the exact
parameters of the Bloom ﬁlter would best be tuned to
match a sample of the events being logged.
Merkle aggregation and safe deletion
Safe deletion
allows the purging of unwanted events from the log.
Auditors deﬁne a stable predicate over the attributes of
events indicating which events must be kept, and the
logger keeps a pruned tree of only those matching events.
In our ﬁrst test, we simulated the deletion of all events
except those from a particular host. The pruned tree was
generated in 14 seconds, containing 1.92% of the events
in the full log and serialized to 2.29% of the size of the
full tree. Although 98.08% of the events were purged, the
logger was only able to purge 95.1% of the nodes in the
 1
 0.1
 0.01
 0.001
 0.0001
 1e-05
t
p
e
k
s
n
o
i
t
a
t
o
n
n
a
f
o
n
o
i
t
c
a
r
F
 1e-06
Non-bloom
Bloom, 2-of-32 bits
Bloom, 4-of-64 bits
Worst case
Best case
 1e+06
 100000
 10000
 1000
 100
 10
 1
t
n
e
v
e
r
e
p
f
o
o
r
p
n
i
s
n
o
i
t
a
t
o
n
n
a
e
g
a
r
e
v
A
 0.1
 1e-07  1e-06  1e-05  0.0001  0.001
 0.01
 0.1
 1
Fraction of events in the query result
Non-bloom
Bloom, 2-of-32 bits
Bloom, 4-of-64 bits
Worst Case
Best Case
 1e-07  1e-06  1e-05  0.0001  0.001
 0.01
 0.1
 1
Fraction of events kept
Figure 9: Safe deletion overhead. For a variety of queries,
we plot the fraction of hashes and attributes kept after deletion
versus the fraction of events kept.
Figure 10: Query overhead per event. We plot the ratio be-
tween the number of hashes and matching events in the result
of each query versus the fraction of events matching the query.
history tree because the logger must keep the hash label
and attributes for the root nodes of elided subtrees.
When measuring the size of a pruned history tree
generated by safe deletion, we assume the logger caches
hashes and attributes for all interior nodes in order to be
able to quickly generate proofs. For each predicate, we
measure the kept ratio, the number of interior node or
stubs in a pruned tree of all nodes matching the predicate
divided by the number of interior nodes in the full history
tree. In Figure 9 for each predicate we plot the kept ratio
versus the fraction of events matching the predicate. We
also plot the analytic best-case and worst-case bounds,
based on a continuous approximation. The minimum
overhead occurs when the matching events are contiguous
in the log. The worst-case occurs when events are max-
imally separated in the log. Our Bloom-ﬁlter queries do
worse than the “worst-case” bound because Bloom ﬁlter
matches are inexact and will thus trigger false positive
matches on interior nodes, forcing them to be kept in the
resulting pruned tree. Although many Bloom ﬁlters did
far worse than the “worst-case,” among the Bloom ﬁlters
that matched fewer than 1% of the events in the log, the
logger is still able to purge over 90% of the nodes in the
history tree and often did much better than that.
Merkle aggregation and authenticated query results
In our second test, we examine the overheads for Merkle
aggregation query lookup results. When the logger
generates the results to a query,
the resulting pruned
tree will contain both matching events and history tree
overhead, in the form of hashes and attributes for any
stubs. For each predicate, we measure the query overhead
ratio—the number of stubs and interior nodes in a pruned
tree divided by the number of events in the pruned tree.
In Figure 10 we plot the query overhead ratio versus the
fraction of events matching the query for each of our 86
predicates. This plot shows, for each event matching a
predicate, proportionally how much extra overhead is in-
curred, per event, for authentication information. We also
plot the analytic best-case and worst-case bounds, based
on a continuous approximation. The minimum overhead
occurs when the matching events are contiguous in the
log. The worst-case occurs when events are maximally
separated in the log. With exact predicates, the overhead
of authenticated query results is very modest, and again,
inexact Bloom ﬁlter queries will sometimes do worse
than the “worst case.”
6 Scaling a tamper-evident log
In this section, we discuss techniques to improve the
insert throughput of the history tree by using concurrency,
and to improve the auditing throughput with replication.
We also discuss a technique to amortize the overhead of
a digital signature over several events.
6.1 Faster inserts via concurrency
Our tamper-evident log offers many opportunities to
leverage concurrency to increase throughput. Perhaps
the simplest approach is to ofﬂoad signature generation.
From Table 2, signatures account for over 80% of the
runtime cost of an insert. Signatures are not included
in any other hashes and there are no interdependencies
between signature computations. Furthermore, signing
a commitment does not require knowing anything other
than the root commitment of the history tree. Conse-
quently, it’s easy to ofﬂoad signature computations onto
additional CPU cores, additional hosts, or hardware
crypto accelerators to improve throughput.
It is possible for a logger to also generate commitments
concurrently. If we examine Table 2, parsing and inserting
events in the log is about two times faster than generating
commitments. Like signatures, commitments have no
interdependencies on one other; they depend only on the
history tree contents. As soon as event X j is inserted into
the tree and O(1) frozen hashes are computed and stored,
a new event may be immediately logged. Computing
the commitment Cj only requires read-only access to the
history tree, allowing it to be computed concurrently by
another CPU core without interfering with subsequent
events. By using shared memory and taking advantage of
the append-only write-once semantics of the history tree,
we would expect concurrency overhead to be low.
We have experimentally veriﬁed the maximum rate
at which our prototype implementation, described in
Section 5, can insert syslog events into the log at 38,000
events per second using only one CPU core on commodity
hardware. This is the maximum throughput our hardware
could potentially support. In this mode we assume that
digital signatures, commitment generation, and audit
requests are delegated to additional CPU cores or hosts.
With multiple hosts, each host must build a replica of
the history tree which can be done at least as fast as
our maximum insert rate of 38,000 events per second.
Additional CPU cores on these hosts can then be used for
generating commitments or handling audit requests.
For some applications, 38,000 events per second may
still not be fast enough. Scaling beyond this would
require fragmenting the event insertion and storage tasks
across multiple logs. To break interdependencies between
them,
the fundamental history tree data structure we
presently use would need to evolve, perhaps into disjoint
logs that occasionally entangle with one another as in
timeline entanglement [43]. Designing and evaluating
such a structure is future work.
6.2 Logs larger than RAM
For exceptionally large audits or queries, where the
working set size does not ﬁt into RAM, we observed
that throughput was limited to disk seek latency. Similar
issues occur in any database query system that uses
secondary storage, and the same software and hardware
techniques used by databases to speed up queries may
be used, including faster or higher throughput storage
systems or partitioning the data and storing it in-memory
across a cluster of machines. A single large query can
then be issued to the cluster node managing each sub-tree.
The results would then be merged before transmitting the
results to the auditor. Because each sub-tree would ﬁt in
its host’s RAM, sub-queries would run quickly.
6.3 Signing batches of events
When large computer clusters are unavailable and the
performance cost of DSA signatures is the limiting factor
in the logger’s throughput, we may improve performance
of the logger by allowing multiple updates to be handled
with one signature computation.
Normally, when a client requests an event X to be
inserted, the logger assigns it an index i, generates the
commitment Ci, signs it, and returns the result.
If the
logger has insufﬁcient CPU to sign every commitment,
the logger could instead delay returning Ci until it has
a signature for some later commitment Cj ( j ≥ i). This
later signed commitment could then be sent to the client
expecting an earlier one. To ensure that the event Xi in
the log committed by Cj was X, the client may request
a membership proof from commitment Cj to event i and
verify that Xi = X. This is safe due to the tamper-evidence
of our structure. If the logger were ever to later sign a Ci
inconsistent with Cj, it would fail an incremental proof.
In our prototype, inserting events into the log is twenty
times faster than generating and signing commitments.
The logger may amortize the costs of generating a signed
commitment over many inserted events. The number of
events per signed commitment could vary dynamically
with the load on the logger. Under light load, the logger
could sign every commitment and insert 1,750 events per
second. With increasing load, the logger might sign one in
every 16 commitments to obtain an estimated insert rate of
17,000 events per second. Clients will still receive signed
commitments within a fraction of a second, but several
clients can now receive the same commitment. Note that
this analysis only considers the maximum insert rate for
the log and does not include the costs of replying to audits.
The overall performance improvements depend on how
often clients request incremental and membership proofs.
7 Related work
There has been recent interest in creating append-only
databases for regulatory compliance. These databases
permit the ability to access old versions and trace tam-
pering [51]. A variety of different data structures are
used, including a B-tree [64] and a full text index [47].
The security of these systems depends on a write-once
semantics of
the underlying storage that cannot be
independently veriﬁed by a remote auditor.
Forward-secure digital signature schemes [3] or stream
authentication [21] can be used for signing commitments
in our scheme or any other logging scheme. Entries in the
log may be encrypted by clients for privacy. Kelsey and
Schneier [57] have the logger encrypt entries with a key
destroyed after use, preventing an attacker from reading
past log entries. A hash function is iterated to generate
the encryption keys. The initial hash is sent to a trusted
auditor so that it may decrypt events. Logcrypt [29]
extends this to public key cryptography.
Ma and Tsudik [41] consider tamper-evident logs built
using forward-secure sequential aggregating signature
schemes [39, 40]. Their design is round-based. Within
each round, the logger evolves its signature, combining
a new event with the existing signature to generate a new
signature, and also evolves the authentication key. At the
end of a round, the ﬁnal signature can authenticate any
event inserted.
Davis et. al. [17] permits keyword searching in a log