| wire_type, where the field_number is the ID of the ﬁeld
and wire_type speciﬁes the ﬁeld type.
A typical model in Protobuf starts with a message whose
ﬁrst ﬁeld deﬁnes the model name (e.g.,VGG_CNN_S). This ﬁeld
usually has a wire_type of 2 (i.e., a length-delimited string)
and a field_number of 0 (i.e., the ﬁrst ﬁeld), which means
that encoded key for this ﬁeld is “0A”. This key is usually the
ﬁrst byte of a Protobuf encoded model. Due to alignment, this
key appears at a four-byte aligned address within the buffer.
It is used as an encoding signature.
Other model formats and representations have their own
content and encoding signature. For example, TFLite models
usually include "TFL2" or “TFL3" as version numbers. Some
USENIX Association
30th USENIX Security Symposium    1963
Figure 7: Extraction of (decrypted) models from app memory using ModelXtractor
The left side shows the typical workﬂow of model loading and decryption in mobile apps. The
right side shows the workﬂow of ModelXtractor. The same color on both sides indicate the same timing of the strategy being used. The "Check
SDK License" shows that a model provider will check an app’s SDK license before releasing the decryption keys as a way to protect its IP.
model ﬁles are even stored in JSON format, with easily identi-
ﬁable names for each ﬁeld. Models from unknown frameworks
or of unknown encoding formats are hard to identify from mem-
ory. In such cases, we consider the buffer of the same size as the
encrypted model to contain the decrypted model. This buffer-
model size matching turns out to be fairly reliable in practice.
The reason is that, when implementing a decryption routine,
programmers almost always allocate a buffer for holding the
decrypted content with the same size as the encrypted content.
This practice is both convenient (i.e., no need to precisely calcu-
late the buffer size before decryption) and safe (i.e., decrypted
content is always shorter than its encrypted counterpart due to
the use of IV and padding during encryption). We show how
buffer size matching is used in our case studies in §5.5.
5.3 Evaluation of ModelXtractor
Model Veriﬁcation: ModelXtractor performs a two-step
veriﬁcation to remove falsely extracted models. First, it
conﬁrms that the extracted model is valid. Second, it veriﬁes
that the extracted model matches the encrypted model. We
use publicly available model parsers to verify the validity of
extracted model buffers (e.g., protobuf decoder [19] to extract
protobuf content, and Netron [18] to show the model structure).
When a decoding or parsing error happens, ModelXtractor
considers the extracted model invalid and reports a failed
model extraction attempt. To conﬁrm that an extracted model
indeed corresponds to the encrypted model, ModelXtractor
uses the buffer-model size matching described before.
Evaluation on Apps from Google Play: There are 47 ML
apps from Google Play that use encryption to protect their
models. We applied ModelXtractor on half of the ML apps
(randomly selected 23 out of 47). Among the tested 23
apps, we successfully extracted decrypted models from 9 of
them. As for the other 14 apps, 2 apps do not use encryption,
1 app does not using ML, and 11 apps do not have their
models extracted for the following reasons: apps cannot be
instrumented; apps did not trigger the ML function; apps
cannot be installed on our test devices.
Evaluation on Apps from Chinese App Markets: There
are 819 apps from Chinese app markets found to be using
encrypted models, where model reuse is quite common as
shown in our static analysis. We carefully selected 59 of these
apps prioritizing model popularity and app diversity. Our
analyzed apps cover 15 of the top 45 most widely used models
(i.e., each is reused more than 10 times) and 8 app categories.
When analyzing the Chinese apps, we encountered some
non-technical difﬁculties of navigating the apps and triggering
their ML functionalities. For instance, some apps require
phone numbers from certain regions that we could not obtain
for user registration. A lot of them are online P2P loan apps or
banking apps that require a local bank account to trigger ML
functionalities. Out of the 59 apps, we managed to successfully
navigate and trigger ML functionalities in 16 apps. We then
extracted decrypted models from 9 of them.
Limitation of ModelXtractor: ModelXtractor failed to
extract 11 models whose ML functionalities were indeed
triggered. This was because of the limitation of our instru-
mentation strategies discussed in §5.1. We note that these
strategies and the design of ModelXtractor are not meant
to extract every protected model. Instead, they represent a
fairly practical and simple attack, designed only to reveal the
insufﬁcient protection of ML models in today’s mobile apps.
5.4 Findings and Insights
Results of Dynamic Model Extraction: Table 6 shows the
statistics on the 82 analyzed apps, grouped by the ML frame-
works they use. Among the 29 apps whose ML functionalities
were triggered, we successfully extracted models from 18
of them (66%). Considering the reuse of those extracted
encrypted models, the number of apps that are affected by
our model extraction is 347 (i.e., 347 apps used the same
models and same protection techniques as the 18 apps that we
1964    30th USENIX Security Symposium
USENIX Association
App withEncryptedModelsS0: Capture atModelDeallocationNYSucceed?ModelXtractorS1: Capture from HeapS2: Capture atModel LoadingS4: Capture atCustomizedDeallocationDecryptedModelBuffersVeriﬁedModel FilesExtract & VerifyDump Model BuffersDump Model BuffersS3: Capture atModelDecryptionML TriggeredCheck SDK LicenseFetch Decryption KeyDecrypt ModelModel InferenceParse ModelAllocate BufferFree BufferYNextracted models from). This extraction rate is alarming and
shows that a majority of the apps using model protection can
still lose their valuable models to an unsophisticated attack.
It indicates that even for app developers and ML providers
willing/trying to protect their models, it is hard to do it in a
robust way using the ﬁle encryption-based techniques.
Table 7 shows the per-app details about the extracted models.
We anonymized the apps for security concerns: many of
them are highly downloaded apps or provide security-critical
services. Many of the listed apps contain more than one ML
models. For simplicity, we only list one representative model
for each app.
Most decrypted models in memory are not protected at all.
As shown in Table 7, most of the decrypted models (12 of
15) were easily captured using the default strategy (S0) when
model buffers are to be freed. This means that the decrypted
models may remain in memory for an extended period of time
(i.e., decrypted models are not erased before memory dealloca-
tion), which creates a large time window for model thefts for
leakages. Moreover, this result indicates that apps using encryp-
tion to protect models are not doing enough to secure decrypted
models loaded in memory, partly due to the lack practical in-
memory data protection techniques on mobile platforms.
Popularity and Diversity of Extracted Models:
The
extracted models are highly popular and diverse, some very
valuable or security-critical. From Table 7 we can see that 8
of 15 listed apps have been downloaded more than 10 million
times. Half of the extracted models belong to commercial
ML providers, such as SenseTime, and were purchased by the
app developers. Such models being leaked may cause direct
ﬁnancial loss to both app developers and model owners (§6).
As for diversity, the model size ranges from 160KB to
20MB. They span all the popular frameworks, such as
TensorFlow, TFLite, Caffe, SenseTime, Baidu, and Face++.
The observed model formats include Protobuf, FlatBuffer,
JSON, and some proprietary formats used by SenseTime,
Face++ and Baidu. In terms of ML functionalities, the models
are used for face recognition, face tracking, liveness detection,
OCR, ID/card recognition, photo processing, and malware
detection. Among them, liveness detection, malware detection,
and face recognition are often used for security-critical
purposes, such as access control and fraud detection. Leakage
of these models may give attackers an advantage to develop
model evasion techniques in a white-box fashion.
Reusability of the Extracted Models: Extracted models can
be directly used by an attacker when they expect standard
input representations (e.g., images and video) and run on the
common ML frameworks (e.g., TensorFlow and PyTorch).
More than 81% of apps in our study contain directly usable
models. In some uncommon cases, such as the example given
in Section 5.5, a model may expect a special/obfuscated input
representation. Such a model, after extraction, cannot be
directly used. However, as we demonstrated in the paper, using
standard reverse engineering techniques, we could recover
the feature vectors and reuse the extracted models in this case.
Potential Risk of Leaking SDK/Model License: SDK/-
Model license are poorly protected. Developers who bought
the ML SDK license from model provider usually ship the
license along with app package. During analysis, we ﬁnd
the license are used to verify legal use of SDK before model
ﬁle get decrypted. However, license ﬁle are not protected by
the developer, which means it is possible to illegally use the
SDK by stealing license ﬁle directly from those apps that have
bought it. Poor protection of license has been observed in both
SenseTime ML SDKs and some other SDKs, which actually
affects hundreds of different apps.
Table 6: Model extraction statistics.
Unique Models
Analyzed
ML
Triggered
Models
Extracted
Models
Missed
Apps
Affected
ML
Framework
TensorFlow
Caffe
SenseTime
TFLite
NCNN
Other
Total
3
7
55
3
9
5
82
3
3
16
2
3
3
29
3
1
11
2
0
2
18
0
2
5
0
3
1
11
3
79
186
76
0
88
347
Note: 347 is the sum of affected apps per framework after deduplication.
Interesting Cases of Model Protection
5.5
We observe a few cases clearly showing that some model
providers use extra protection on their models. Below we
discuss these cases and share our insights.
Encrypting Both Code and Model Files: We analyzed an
app that uses the Anyline OCR SDK. From the app proﬁle gen-
erated by ModelXRay,we can tell that this app uses TensorFlow
framework. It places the encrypted models under a directory
named “encrypted_models”. Initially, ModelXtractor failed to
extract the decrypted models using the default strategy (S0).
We manually investigated the reason and found that, unlike
most ML apps, this app runs ML inference in a customized
WebView, where an encrypted JavaScript, dynamically loaded
at runtime, performs the model decryption and inference. We
analyzed the heap memory dumped by ModelXtractor using
the alternative strategy, S1, and found the TensorFlow model
buffers in the memory dump. We veriﬁed our ﬁndings by decod-
ing the Protobuf model buffers and extract the models’ weights.
It shows that, despite the extra protection and sophisticated
obfuscation, the app can still lose its models to not-so-
advanced attacks that can locate and extract decrypted models
in app memory.
Encrypting Feature Vectors and Formats: When we
analyzed one malware detection app, we found that it does not
encrypt its model ﬁle. Instead, it encrypts the feature vectors
which is the input of the model. This app uses a Random
Forest model for malware classiﬁcation. It uses TensorFlow
framework and the model is in the format of Protobuf. There
are more than one thousand features used in this malware
USENIX Association
30th USENIX Security Symposium    1965
App name
Anonymous App 1
Anonymous App 2
Anonymous App 3
Anonymous App 4
Anonymous App 5
Anonymous App 6
Anonymous App 7
Anonymous App 8
Anonymous App 9
Anonymous App 10
Anonymous App 11
Anonymous App 12
Anonymous App 13
Anonymous App 14
Anonymous App 15
Downloads
300M
10M
27M
100K
100M
10K
10M
10K
5.8M
10M
100M
492K
250K
100M
5K
Table 7: Overview of Successfully Dumped Models with ModelXtractor
Format
FlatBuffer
Protobuf
Protobuf
Protobuf
Protobuf
Protobuf
Protobuf
Protobuf
Protobuf
Unknown
Protobuf
Unknown
Unknown
Json
Protobuf
Framework Model Functionality
Liveness Detection
TFLite
Face Tracking
Caffe
Face Tracking
SenseTime
SenseTime
Face Filter
Face Filter
SenseTime
OCR
TensorFlow
Photo Process
TensorFlow
Face Track
SenseTime
Caffe
Face Detect
Liveness
Face++
Face Detect
SenseTime
Face Tracking
Baidu
ID card
SenseTime
TFLite
Camera Filter
TensorFlow Malware Classiﬁcation
Size (B)
160K
1.5M
2.3M
3.6M
1.4M
892K
6.5M
1.2M
60K
468K
1.7M
2.7M
1.3M
228K
20M
Reuses
18
4
77
3
2
2
1
5
77
17
18
26
13
1
1
Extraction Strategy
Freed Buffer
Model Loading
Freed Buffer