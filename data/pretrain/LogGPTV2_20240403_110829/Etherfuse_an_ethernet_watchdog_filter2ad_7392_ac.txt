their timeout value.
4. ETHERFUSE IMPLEMENTATION
This section describes our prototype implementation of the Ether-
Fuse. Then, it discusses the EtherFuse’s memory and processing
requirements, arguing that the EtherFuse can scale to large, high-
speed Ethernets.
4.1 The EtherFuse Prototype
We implemented EtherFuse using the Click modular router [14].
Figure 6 shows how the different modules are put together to com-
pose the EtherFuse in the Click modular router. The FromDevice
module is responsible for receiving packets from a NIC into the
EtherFuse. The Classifier module is responsible for classi-
fying Ethernet packets based on their contents. In this conﬁgura-
tion, it classiﬁes them into either RSTP control packets which are
sent to the CTIChecker module, or regular Ethernet data frames
which are sent to the LoopChecker module. The CTIChecker
module is responsible for handling count to inﬁnity in the network,
while the LoopChecker module is responsible for handling Eth-
ernet forwarding loops. Ethernet frames are then pushed by the
CTIChecker and the LoopChecker modules to the other NIC
of the EtherFuse using the ToDevice module. The suppressors,
S1 and S2, allow the EtherFuse to block input and output ports,
respectively. They are used by the EtherFuse to block both input
and output trafﬁc going through one NIC when a forwarding loop
is detected.
In our implementation we used a packet’s CRC as the hash of
the packet’s contents. We store the CRC in the duplicate detector
hash table to reduce the number of dropped packets because of false
positives. Timestamps in the duplicate detector have millisecond
granularity. Thus, a timestamp is stored in 4 bytes.
4.2 Memory Requirements
The primary memory requirement for the EtherFuse is that of the
duplicate detector which records the timestamps of frames it has re-
ceived recently. The duplicate detector may also store the hashes of
the packets. Every entry in the duplicate detector contains a hash
with size C bytes, where C is equal to zero if the hash is not stored
in the table, and a timestamp with size S bytes. The duplicate de-
tector should have at least as many entries as the number of frames
that make up the product of the maximum network bandwidth B
and the latency L. Thus the minimum memory requirement M can
be given by Equation 4, where F is the minimum Ethernet frame
size.
× (C + S)
(4)
„j L × B
k«
M =
F
The minimum frame size F is 64 bytes. Assuming the CRC is
used as the packet’s hash and a timestamp is 4 bytes, then C and
S will be 4 bytes each. Using generous values of 100 milliseconds
for L, and 10 Gbps for B would lead to M equal to 16MB. Thus
the EtherFuse can easily scale to a large 10 Gbps Ethernet network.
4.3 Processing Overhead
The processing overhead of the EtherFuse is low. This is true
even if the packet’s hash is maintained in the duplicate detector
along with the packet’s timestamp, which would require more pro-
cessing due to storing, loading and comparing hashes. Assuming
the packet’s hash is precomputed like if the CRC is used, then in
the common case to handle a data packet one memory access is
required to check whether the packet’s hash exists in the duplicate
detector, another memory access is required to write the hash into
the duplicate detector, and another one to write the timestamp. This
is assuming that at least 4 bytes can be read in a single memory ac-
cess. In the unlikely event that the hash is matched, another mem-
ory access is needed to fetch the timestamp to check whether this
hash is fresh or not. However, in conventional processors with data
caches, fetching the hash from memory would lead to prefetching
the timestamp as well if both are within the same cache line. In
such cases, the access to the timestamp would have trivial cost. For
BPDUs, they arrive at a much lower frequency than data packets,
roughly on the order of 10 BPDUs per second even during a count
to inﬁnity. To handle a BPDU, the EtherFuse compares it against
the 2 cached entries in the BCache. If a count to inﬁnity is sus-
pected, the BPDU is written into the BCache. Since a BPDU is 36
bytes, this requires at most 9 memory accesses for the comparisons
and 9 memory accesses for the write. Since BPDUs arrive at a low
rate, these operations can be easily handled in practice.
5. EXPERIMENTAL SETUP
This section describes the experimental settings used for the eval-
uation of the EtherFuse.
FromDevice(eth0)
FromDevice(eth1)
S1::Suppressor
Classifier(... )
802.1d                   other  
Classifier(... )
802.1d                   other  
CTIChecker
LoopChecker(S1,S2,... )
S2::Suppressor
ToDevice(eth0)
ToDevice(eth1)
Figure 6: Block diagram of the EtherFuse implemented with the Click modu-
lar router
B1
B2
B3
F
B4
F1
B5
H1
B6
H2
B2
B3
B1
B4
B6
B8
F2
B5
B7
(a) Topology I.
(b) Topology II.
Figure 7: Network topologies used used in the experiments.
5.1 Hardware Platform
For our experiments we used the Emulab testbed [1, 22]. Specif-
ically, we used machines with 3.0 GHz Xeon processors having
2GB of physical memory. Each machine had 6 Network Interface
Cards (NICs). However, one of these NICs is used for the Emulab
control network. In our experiments the machines were connected
by 100 Mbps links to Cisco 6500 series switches.
The network topologies shown in Figure 7 are used for all of our
experiments. In the ﬁgure, B’s are Ethernet switches, F’s are Ether-
Fuses and H’s are end hosts. For analysis, the EtherFuse collects
statistics about the number of duplicate packets in the network. In
the experiments where an EtherFuse is not used, a simpliﬁed device
is substituted for the EtherFuse to collect the same statistics.
5.2 Software Components
All nodes in our experiments were running Fedora Core 4 with
Linux kernel 2.6.12. Ethernet switches were implemented in soft-
ware using Click. We used a conﬁguration similar to the one pre-
scribed in [14] for an Ethernet switch. However, when using RSTP
switches, our conﬁguration has two differences from that in [14].
First, we replaced the EtherSpanTree module which imple-
ments the legacy STP protocol with our own EtherRSTP module
that implements the RSTP protocol. The RSTP module is respon-
sible for maintaining the spanning tree of bridges, enabling or dis-
abling switch ports based on their roles in the spanning tree. The
second difference is that we updated the EtherSwitch module
to support the functionality required for maintaining the switch’s
forwarding tables, including ﬂushing and updating the tables in
response to topology change events reported by the EtherRSTP
module. We implemented the basic technique that ﬂushes all for-
warding tables in response to topology change events, not the opti-
mized technique which does not ﬂush the forwarding table for the
port where the topology change event is received.
 100
 80
 60
 40
 20
)
%
(
s
s
o
L
t
e
k
c
a
P
 0
 0
Packet Loss
With EtherFuse
Packet Loss
Without EtherFuse
 100
 80
 60
 40
 20
)
%
(
s
s
o
L
t
e
k
c
a
P
 5
 10
 15
 20
 25
 30
 35
 40
Time (s)
(a) Without EtherFuse
 0
 0
 5
 10
 15
 20
 25
 30
 35
 40
Time (s)
(b) With EtherFuse
Figure 8: Timeline of packets loss for a 90 Mb/s UDP stream during count to
inﬁnity. Count to inﬁnity starts at t=10 and no forwarding loop is formed.
6. EVALUATION
In this section we evaluate the effects of different Ethernet fail-
ures on software applications and show the effectiveness of the
EtherFuse at mitigating these effects. For every class of failures,
we study the fundamental effects using packet level measurements.
Then, we use HTTP and FTP workloads to study the overall effects
of the failures. The HTTP workload does not use persistent connec-
tions so the effects of the failures on TCP connection establishment
can be studied. The FTP workload is used to study the effects of
failures on TCP streams. We conduct multiple runs of each exper-
iment because there is non-negligible variance between runs. This
variance is due to non-deterministic interactions between the ap-
plications, the transport protocols, and the spanning tree protocols.
However, in the cases where we characterize the network’s behav-
ior by a detailed timeline, it is only practical to present the results
from one representative run. Results of the other runs are qualita-
tively similar.
The evaluation is organized as follows. Section 6.1 studies the
effects of the count to inﬁnity problem. Section 6.2 studies the ef-
fects of a single forwarding loop. Section 6.3 studies the effects
of multiple, simultaneous forwarding loops. In both Sections 6.1,
and 6.2 we use the topology shown in Figure 7(a) for our exper-
iments, while in Section 6.3 we use the topology shown in Fig-
ure 7(b). Section 6.4 concludes with a discussion.
6.1 Effects of Count to Inﬁnity
For the experiments in this section we modiﬁed the RSTP state
machines such that its races do not lead to a forwarding loop in the
event of a count to inﬁnity. This is because we want to study the
effects of the count to inﬁnity in isolation without the forwarding
loop.
6.1.1 Fundamental Effects
In this experiment, we characterize the packet loss in the network
during the count to inﬁnity. We use iperf to generate a 90 Mb/s
UDP stream between the end hosts, which maintains high link uti-
lization. Then, we measure packet loss at the receiving side of the
UDP stream. iperf includes a datagram ID in every packet it sends
and we instrumented it to maintain a history of the IDs it received
to detect packet loss. Figure 8 presents a timeline of packet loss.
The periodic heavy packet loss is caused by the oscillations of the
network ports between blocking and forwarding during the count
to inﬁnity. It shows that during the count to inﬁnity the network
suffers from extended periods with near 100% loss rate. The Ether-
Fuse substantially reduces these periods by terminating the count
to inﬁnity quickly.
6.1.2 Detection and Correction Time
In this section, we study the time it takes the EtherFuse to detect
and terminate a count to inﬁnity using different network topolo-
gies. These experiments are based on simulations. This allows
us to have global knowledge about the network and thus we can
determine when the count to inﬁnity has actually ended and the
network has converged. We deﬁne convergence time as the time it
takes all bridges in the network to agree on the same correct active
topology. We use the BridgeSim simulator [17] but we have mod-
iﬁed it to implement the latest RSTP state machines as speciﬁed
in IEEE 802.1D (2004). In the simulator, bridges have desynchro-
nized clocks so not all bridges start together at time zero. Instead
each bridge starts with a random offset from time zero that is a
fraction of the HelloTime. We have also added an EtherFuse im-
plementation to the simulator. In our simulations, for each setting,
we repeat the experiment 100 times and report the maximum, aver-
age, and minimum time values measured. We use a MaxAge of 20,
a TxHoldCount of 3, and a HelloTime of 2 seconds.
In the experiment shown in Figure 9(a), we measure the conver-
gence time in complete graph topologies after the death of the root
bridge. For experiments with the EtherFuse, we used an EtherFuse
for every redundant link. Notice that using EtherFuses for complete
graph topologies cuts the average convergence time after a count to
inﬁnity by more than half.
Figure 10(a) shows the time it takes for the count to inﬁnity to be
detected by any EtherFuse in the network. We see that the Ether-
Fuses detect the count to inﬁnity very quickly. This is because for a
count to inﬁnity to be detected a bridge needs to transmit 2 consec-
utive BPDUs with increasing path cost that is higher than the cost
it was announcing before the count to inﬁnity. In this topology, all
the bridges are directly connected to the root bridge and thus all
bridges can detect the root’s failure instantaneously. Hence, they
immediately start using stale cached BPDU information, and start
announcing different paths to the root which have higher cost. This
constitutes the ﬁrst increase in the path cost to the root. These stale
BPDUs will trigger bridges to update their information and forward
it to their neighbors. This constitutes the second increase in the path
cost to the root, which is immediately detected by an EtherFuse.
Thus, it takes two BPDU transmissions for some EtherFuses in the
network to detect the beginning of the count to inﬁnity. However
it takes a much longer period of time for the network to converge.
This is because bridges in the network have many redundant links
and thus many alternate ports caching many copies of the stale in-
formation. Thus it takes time to replace all those stale copies of
the information. Also ports in the Ethernet switches quickly reach
their TxHoldCount limit due to multiple transmissions of the stale
information. This further slows down the process of eliminating
the stale information and makes the convergence time longer.
In the experiment shown in Figure 9(b), we measure the conver-
gence time in “loop” topologies after the death of the root bridge. A
loop topology is a ring topology with the root bridge dangling out-
side the ring. For these topologies we use a single EtherFuse. The
EtherFuse connects the new root (the bridge that assumes root sta-
tus after the death of the original root) to one of its neighbors. We
note that for small loops, the EtherFuse is able to detect and stop the
count to inﬁnity quickly. However for larger loops, the EtherFuse
becomes ineffective in dealing with the count to inﬁnity. This is be-
cause the EtherFuse relies on observing two consecutive increases
in the announced cost to the root. For loop topologies, this means
the stale information must traverse around the loop twice. If the
loop is large, the stale information will have reached its MaxAge
before it gets detected by the EtherFuse.
Figure 10(b) also shows that the count to inﬁnity is detected
fairly quickly in the “loop” topologies. However, the termination
of the count to inﬁnity takes longer. This is because by the time
the count to inﬁnity has been detected, most of the bridges’ ports
have reached their TxHoldCount limit. Thus they are allowed to
Convergence Time
Convergence Time
With EtherFuse
Without EtherFuse
)
s
(
e
m
T
i
 10
 8
 6
 4
 2
 0
-2
 10
 11
With EtherFuse
Without EtherFuse
 3
 4
 5
 6
 7
 8
 9
 10
 11
(b) “Loop” topologies
Nodes
 6
 5
 9
Nodes in the Complete Graph
 7
 8
(a) Complete graph topologies
)
s
(
e
m
T
i
 35
 30
 25
 20
 15
 10
 5
 0
 3
 4
)
s
(
e
m
T
i
 10
 8
 6
 4
 2
 0
-2
 3
 4
i
)