ality does not completely hide the network structure. We
argue, however, that it does hide the important aspects
which are likely to be proprietary. In particular, the ideal
functionality and our realization hides all the weights and
biases in the convolution and the fully connected layers.
Secondly, we also hide the ﬁlter and stride size in the con-
volution layers, as well as information as to which layers
are convolutional layers and which are fully connected.
We do reveal the number of layers and the size1 (the
1One can potentially hide this information by padding the network
with dummy operation at a proportional computational expense
Figure 2: SISO convolutions and multi-channel Conv lay-
ers
2.2 Non-Linear Layers
The non-linear layers, shown in Figure 1 in blue, consist
of an activation function that acts on each element of
the input separately or a pooling function that reduces
the output size. Typical non-linear functions can be one
of several types: the most common in the convolutional
setting are max-pooling functions and ReLU functions.
The key observation that we wish to make in this context
is that all these functions can be implemented by circuits
that have size linear in the input size and thus, evaluating
them using conventional 2PC approaches does not impose
any additional asymptotic communication penalty.
For more details on CNNs, we refer the reader to [40].
1654    27th USENIX Security Symposium
USENIX Association
number of hidden nodes) of each layer. In contrast, other
protocols for secure neural network inference such as the
MiniONN protocol [29] reveal strictly more information,
e.g., they reveal the ﬁlter size. As for party B’s security,
we hide the entire image, but not its size, from party A.
A second, more subtle, issue is with the deﬁnition
of the ideal functionality which implements secure
network inference. Since such functionality, must at a
bare minimum, give B access to the classiﬁcation output,
B maybe be able to train a new classiﬁer to mimic these
classiﬁcation results. This attack is called model stealing
[42]. Note that model stealing with limited queries is
essentially equivalent to a supervised learning task with
access to a limited training dataset. Thus a potential model
stealing adversary could train such classiﬁer without
access to B by simply asking a domain expert to classify
his limited set of test-images. One potential solution is to
limit the number of classiﬁcation queries that A is allowed
to make of the model. This can be a practical solution in
a try-before-buy scenario where B only needs access to
limited set of classiﬁcations to test the performance of the
network before it buy the network parameters from A. We
remark that designing (potentially-noisy) classiﬁers which
are intrinsically resilient to model stealing is an interesting
open machine learning problem.
Paper Organization. The rest of the paper is organized
as follows. We ﬁrst describe our abstraction of a packed
additively homomorphic encryption (PAHE) that we use
through the rest of the paper. We then provide an overview
of the entire Gazelle protocol in section 4. In the next two
sections, Section 5 and 6, we elucidate the most important
technical contributions of the paper, namely the linear
algebra kernels for fast matrix-vector multiplication and
convolution. We then present detailed benchmarks on
the implementation of the homomorphic encryption layer
and the linear algebra kernels in Section 7. Finally, we
describe the evaluation of neural networks such as ones
trained on the MNIST or CIFAR-10 datasets and compare
Gazelle’s performance to prior work in Section 8.
3 Packed Additively Homomorphic Encryption
In this section, we describe a clean abstraction of packed
additively homomorphic encryption (PAHE) schemes that
we will use through the rest of the paper. As suggested
by the name, the abstraction will support packing multiple
plaintexts into a single ciphertext, performing SIMD homo-
morphic additions (SIMDAdd) and scalar multiplications
(SIMDScMult), and permuting the plaintext slots (Perm).
In particular, we will never need or use homomorphic
multiplication of two ciphertexts. This abstraction can
be instantiated with essentially all modern lattice-based
homomorphic encryption schemes, e.g., [5, 16, 4, 14].
For the purposes of this paper, a private-key PAHE suf-
ﬁces. In such an encryption scheme, we have a (random-
ized) encryption algorithm (PAHE.Enc) that takes a plain-
text message vector u from some message space and en-
crypts it using a key sk into a ciphertext denoted as [u], and
a (deterministic) decryption algorithm (PAHE.Dec) that
takes the ciphertext [u] and the key sk and recovers the mes-
sage u. Finally, we also have a homomorphic evaluation
algorithm (PAHE.Eval) that takes as input one or more ci-
phertexts that encrypt messages M0,M1,..., and outputs an-
other ciphertext that encrypts a message M = f (M0,M1,...)
for some function f constructed using the SIMDAdd,
SIMDScMult and Perm operations. We require IND-CPA
security, which requires that ciphertexts of any two mes-
sages u and u(cid:48) be computationally indistinguishable.
The lattice-based PAHE constructions that we consider
in this paper are parameterized by four constants: (1) the
cyclotomic order m, (2) the ciphertext modulus q, (3) the
plaintext modulus p and (4) the standard deviation σ of
a symmetric discrete Gaussian noise distribution (χ).
The number of slots in a packed PAHE ciphertext
is given by n = φ (m) where φ is the Euler Totient
function. Thus, plaintexts can be viewed as length-n
vectors over Zp and ciphertexts are viewed as length-n
vectors over Zq. All fresh ciphertexts start with an
inherent noise η sampled from the noise distribution χ.
As homomorphic computations are performed η grows
continually. Correctness of PAHE.Dec is predicated on
the fact that |η| <q/(2p), thus setting an upper bound on
the complexity of the possible computations.
In order to guarantee security we require a minimum
value of σ (based on q and n), q ≡ 1 mod m and p is
co-prime to q. Additionally, in order to minimize noise
growth in the homomorphic operations we require that the
magnitude of r ≡ q mod p be as small as possible. This
when combined with the security constraint results in an
optimal value of r =±1.
In the sequel, we describe in detail the three basic
operations supported by the homomorphic encryption
schemes together with their associated asymptotic cost in
terms of (a) the run-time, and (b) the noise growth. Later,
in Section 7, we will provide concrete micro-benchmarks
for each of these operations implemented in the GAZELLE
library.
3.1 Addition: SIMDAdd
Given ciphertexts [u] and [v], SIMDAdd outputs an
encryption of their component-wise sum, namely [u+v].
The asymptotic run-time for homomorphic addition
is n· CostAdd(q), where CostAdd(q) is the run-time for
adding two numbers in Zq = {0,1,...,q− 1}. The noise
growth is at most ηu + ηv where ηu (resp. ηv) is the
amount of noise in [u] (resp. in [v]).
3.2 Scalar Multiplication: SIMDScMult
If the plaintext modulus is chosen such that p≡1 mod m,
we can also support a SIMD compenentwise product.
USENIX Association
27th USENIX Security Symposium    1655
n.
Thus given a ciphertext [u] and a plaintext v, we can output
an encryption [u◦ v] (where ◦ denotes component-wise
multiplication of vectors).
The asymptotic run-time for homomorphic scalar
multiplication is n· CostMult(q), where CostMult(q) is
the run-time for multiplying two numbers in Zq. The
∞·√
noise growth is at most ηmult·ηu where ηmult≈||v||(cid:48)
n
is the multiplicative noise growth of the SIMD scalar
multiplication operation.
For a reader familiar with homomorphic encryption
schemes, we note that ||v||(cid:48)
∞ is the largest value in the
coefﬁcient representation of the packed plaintext vector
v, and thus, even a binary plaintext vector can result in
ηmult as high as p · √
In practice, we alleviate this
large multiplicative noise growth by bit-decomposing
the coefﬁcient representation of v into log(p/2wpt) many
wpt-sized chunks vk such that v =∑2wpt·k·vk. We refer to
wpt as the plaintext window size.
We can now represent the product [u◦v] as ∑[uk ◦vk]
where uk = [2wpt·k·u]. Since ||vk||(cid:48)
∞ ≤ 2wpt the total noise
√
n · ηuk as
in the multiplication is bounded by 2wpt · k
opposed to p·√
n· ηu. The only caveat is that we need
access to low noise encryptions [uk] as opposed to just [u]
as in the direct approach.
3.3 Slot Permutation: Perm
Given a ciphertext [u] and one of a set of primitive per-
mutations π deﬁned by the scheme, the Perm opera-
tion outputs a ciphertext [uπ ], where uπ is deﬁned as
(uπ(1),uπ(2),...,uπ(n)), namely the vector u whose slots are
permuted according to the permutation π. The set of per-
mutations that can be supported depends on the structure
of the multiplicative group mod m i.e. (Z/mZ)×. When
m is prime, we have n (=m−1) slots and the permutation
group supports all cyclic rotations of the slots, i.e. it is
isomorphic to Cn (the cyclic group of order n). When m is
a sufﬁciently large power of two (m =2k, m≥8), we have
n = 2k−1 and the set of permutations is isomorphic to the
set of half-rotations i.e. Cn/2×C2, as illustrated in Figure 4.
Permutations are by far the most expensive operations
in a homomorphic encryption scheme. At a high-level
the PAHE ciphertext vectors represent polynomials.
The permutation operation requires transforming these
polynomials from evaluation to coefﬁcient representations
and back. These transformations can be efﬁciently
computed using the number theoretic transform (NTT)
and its inverse, both of which are ﬁnite-ﬁeld analogues of
their real valued Discrete Fourier Transform counterparts.
Both the NTT and NTT−1 have an asymptotic cost of
Θ(nlogn). As shown in [6], we need to perform Θ(log q)
NTT−1 to control Perm noise growth. The total cost of
Perm is therefore Θ(nlognlogq) operations. The noise
growth is additive, namely, ηuπ = ηu +ηrot where ηrot is
the additive noise growth of a permutation operation.
Figure 3: Ciphertext Structure and Operations. Here, n
is the number of slots, q is the size of ciphertext space
(so a ciphertext required (cid:100)log2 q(cid:101) bits to represent), p is
the size of the plaintext space (so a plaintext can have at
most (cid:98)log2 p(cid:99) bits), and η is the amount of noise in the
ciphertext.
Figure 4: A Plaintext Permutation in action. The permu-
tation π in this example swaps the ﬁrst and the second
slots, and also the third and fourth slots. The operation
incurs a noise growth from η to η(cid:48) ≈ η + ηrot. Here,
ηrot≈nlogq·η0 where η0 is some small “base noise”.
3.4 Paillier vs. Lattice-based PAHE
The PAHE scheme used in Gazelle is dramatically more
efﬁcient than conventional Paillier based AHE. Homomor-
phic addition of two Paillier ciphertexts corresponds to a
modular multiplication modulo a large RSA-like modulus
(3072bits) as opposed to a simple addition mod q as seen
in SIMDAdd. Similarly multiplication by a plaintext turns
into a modular exponentiation for Paillier. Furthermore the
large sizes of the Paillier ciphertexts makes encryption of
single small integers extremely bandwidth-inefﬁcient. In
contrast, the notion of packing provided by lattice-based
schemes provides us with a SIMD way of packing many
integers into one ciphertext, as well as SIMD evaluation
algorithms. We are aware of one system [37] that tries to
use Paillier in a SIMD fashion; however, this lacks two
crucial components of lattice-based AHE, namely the
facility to multiply each slot with a separate scalar, and
the facility to permute the slots. We are also aware of a
method of mitigating the ﬁrst of these shortcomings [26],
but not the second. Our fast homomorphic implementation
of linear algebra uses both these features of lattice-based
AHE, making Paillier an inefﬁcient substitute.
1656    27th USENIX Security Symposium
USENIX Association
3.5 Parameter Selection for PAHE
Parameter selection for PAHE requires a delicate balance
between the homomorphic evaluation capabilities and the
target security level. We detail our procedure for parameter
selection to meet a target security level of 128 bits. We
ﬁrst set our plaintext modulus to be 20 bits to represent the
ﬁxed point inputs (the bit-length of each pixel in an image)
and partial sums generated during the neural network
evaluation. Next, we require that the ciphertext modulus
be close to, but less than, 64 bits in order to ensure that
each ciphertext slot ﬁts in a single machine word while
maximizing the potential noise margin available during
homomorphic computation.
The Perm operation in particular presents an interesting
tradeoff between the simplicity of possible rotations
and the computational efﬁciency of the NTT. A prime
m results in a (simpler) cyclic permutation group but
necessitates the use of an expensive Bluestein transform.
Conversely, the use of m = 2k allows for a 8× more efﬁ-
cient Cooley-Tukey style NTT at the cost of an awkward
permutation group that only allows half-rotations. In this
work, we opt for the latter and adapt our linear algebra
kernels to deal with the structure of the permutation group.
Based on the analysis of [1], we set m = 4096 and σ = 4
to obtain our desired security level.
Our chosen bit-width for q (60 bits), allows for lazy re-
duction, i.e. multiple additions may be performed without
overﬂowing a machine word before a reduction is neces-
sary. Additionally, even when q is close to the machine
word-size, we can replace modular reduction with a simple
sequence of addition, subtraction and multiplications. This
is done by choosing q to be a pseudo-Mersenne number.
Next, we detail a technique to generate prime moduli
that satisfy the above correctness and efﬁciency properties,
namely:
1. q≡1 mod m
2. p≡1 mod m
3. |q mod p| =|r|≈1
4. q is pseudo-Mersenne, i.e. q =260−δ ,(δ <
Since we have chosen m to be a power of two, we
observe that δ ≡ −1 mod m. Moreover, r ≡ q mod p
implies that δ ≡ (260 − r) mod p. These two CRT
expressions for δ imply that given a prime p and residue
r, there exists a unique minimal value of δ mod (p·m).
√
q)
Based on this insight our prime selection procedure can
be broken down into three steps:
1. Sample for p ≡ 1 mod m and sieve the prime
2. For each candidate p, compute the potential 2|r|
candidates.
candidates for δ (and thus q).
3. If q is prime and δ is sufﬁciently small accept the pair
log(q)(p ·
q) candidate primes p to sieve out a suitable q.
(p,q).
Heuristically,
m)/(2|r|√
this procedure needs
(cid:98)log(p)(cid:99)
18
22
26
30
Table 1: Prime Selection for PAHE
p
307201
5324801
115351553
1316638721
q
260−212·63549+1
260−212·122130+1
260−212·9259+1
260−212·54778+1
|r|
1
1
1
2
Since p ≈ 220 and q ≈ 264 in our setting, this procedure
is very fast. A list of reduction-friendly primes generated
by this approach is tabulated in Table 1. Finally note that
when (cid:98)log(p)(cid:99) · 3 < 64 we can use Barrett reduction to
speed-up reduction mod p.
The impact of the selection of reduction-friendly primes
on the performance of the PAHE scheme is described in
section 7.
4 Our Protocol at a High Level
Our protocol for secure neural network inference is based
on the alternating use of PAHE and garbled circuits (GC).
We will next explain the ﬂow of the protocol and show
how one can efﬁciently and securely convert between the
data representations required for the two cryptographic
primitives.
The main invariant that the protocol maintains is that at
the start of the PAHE phase the server and the client posses
an additive share cy, sy of the client’s input y. At the very
beginning of the computation this can be accomplished
by the trivial share (cy, sy) = (y, 0).
In order to evaluate a linear layer, we start with the client
B ﬁrst encrypting their share using the PAHE scheme and
sending it to the server A. A in turn homomorphically
adds her share sy to obtain an encryption of cy +sy = [y].
The security of the homomorphic encryption scheme
guarantees that B cannot recover y from this encryption.
The server A then uses a homomorphic linear algebra
kernel to evaluate linear layer (which is either convolution
or fully connected). The result is a packed ciphertext that
contains the input to the ﬁrst non-linear (ReLU) layer. The
homomorphic scheme ensures that A learns nothing about
B’s input. B has not received any input from A yet and thus
has no way of learning the model parameters.
In preparation for the evaluation of the subsequent non-
linear activation layer A must transform her PAHE cipher-
text into additive shares. At the start of this step A holds a
ciphertext [x] (where x is a vector) and B holds the private
key. The ﬁrst step is to transform this ciphertext such that
both A and B hold an additive secret sharing of x. This is
accomplished by the server A adding a random vector r to
her ciphertext homomorphically to obtain an encryption
[x + r] and sending it to the client B. The client B then
decrypts this message to get his share. Thus the server A
sets her share sx =r and B sets his share cx =x+r mod p.
USENIX Association
27th USENIX Security Symposium    1657
Since A chooses r uniformly at random sx does not contain
any information about either the model or B’s input. Since
B does not know r, cx has a uniform random distribution