Networks 2021 (2021).
[45] Nikil Wale, Ian A Watson, and George Karypis. 2008. Comparison of descrip-
tor spaces for chemical compound retrieval and classification. Knowledge and
Information Systems 14, 3 (2008), 347–375.
[46] Binghui Wang and Neil Zhenqiang Gong. 2019. Attacking graph-based classi-
fication via manipulating the graph structure. In Proceedings of the 2019 ACM
SIGSAC Conference on Computer and Communications Security. 2023–2040.
[47] Binghui Wang, Jinyuan Jia, Xiaoyu Cao, and Neil Zhenqiang Gong. 2021. Certified
robustness of graph neural networks against adversarial structural perturbation.
In ACM SIGKDD.
[48] Binghui Wang, Tianxiang Zhou, Minhua Lin, Pan Zhou, Ang Li, Meng Pang, Cai
Fu, Hai Li, and Yiran Chen. 2020. Evasion Attacks to Graph Neural Networks via
Influence Function. arXiv preprint arXiv:2009.00203 (2020).
[49] Shen Wang, Zhengzhang Chen, Xiao Yu, Ding Li, Jingchao Ni, Lu-An Tang, Jiaping
Gui, Zhichun Li, Haifeng Chen, and S Yu Philip. 2019. Heterogeneous Graph
Matching Networks for Unknown Malware Detection.. In IJCAI. 3762–3770.
[50] Xiaoyun Wang, Minhao Cheng, Joe Eaton, Cho-Jui Hsieh, and Felix Wu. 2018.
Attack graph convolutional networks by adding fake nodes. arXiv preprint
arXiv:1810.10751 (2018).
[51] Felix Wu, Tianyi Zhang, Amauri Holanda de Souza Jr, Christopher Fifty, Tao Yu,
and Kilian Q Weinberger. 2019. Simplifying graph convolutional networks. arXiv
preprint arXiv:1902.07153 (2019).
Session 1B: Attacks and Robustness CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea121[52] Huijun Wu, Chen Wang, Yuriy Tyshetskiy, Andrew Docherty, Kai Lu, and Liming
Zhu. 2019. Adversarial examples on graph data: Deep insights into attack and
defense. arXiv preprint arXiv:1903.01610 (2019).
[53] Zhaohan Xi, Ren Pang, Shouling Ji, and Ting Wang. 2020. Graph backdoor. arXiv
preprint arXiv:2006.11890 (2020).
[54] Kaidi Xu, Hongge Chen, Sijia Liu, Pin-Yu Chen, Tsui-Wei Weng, Mingyi Hong,
and Xue Lin. 2019. Topology attack and defense for graph neural networks: An
optimization perspective. arXiv preprint arXiv:1906.04214 (2019).
are graph neural networks? arXiv preprint arXiv:1810.00826 (2018).
[56] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi
Kawarabayashi, and Stefanie Jegelka. 2018. Representation learning on graphs
with jumping knowledge networks. arXiv preprint arXiv:1806.03536 (2018).
[55] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerful
[59] Muhan Zhang and Yixin Chen. 2018. Link prediction based on graph neural
[57] Pinar Yanardag and SVN Vishwanathan. 2015. Deep graph kernels. In Proceedings
of the 21th ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining. 1365–1374.
[58] Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure
Leskovec. 2018. Hierarchical graph representation learning with differentiable
pooling. In Advances in neural information processing systems. 4800–4810.
networks. In Advances in Neural Information Processing Systems. 5165–5175.
[60] Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. 2018. An end-
to-end deep learning architecture for graph classification. In Proceedings of the
AAAI Conference on Artificial Intelligence, Vol. 32.
[61] Zaixi Zhang, Jinyuan Jia, Binghui Wang, and Neil Zhenqiang Gong. 2020. Back-
door attacks to graph neural networks. arXiv preprint arXiv:2006.11165 (2020).
[62] Fan Zhou, Chengtai Cao, Kunpeng Zhang, Goce Trajcevski, Ting Zhong, and
Ji Geng. 2019. Meta-GNN: On Few-shot Node Classification in Graph Meta-
learning. In Proceedings of the 28th ACM International Conference on Information
and Knowledge Management. 2357–2360.
[63] Daniel Zügner, Amir Akbarnejad, and Stephan Günnemann. 2018. Adversarial
attacks on neural networks for graph data. In Proceedings of the 24th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining. 2847–2856.
[64] Daniel Zügner and Stephan Günnemann. 2019. Adversarial attacks on graph
neural networks via meta learning. arXiv preprint arXiv:1902.08412 (2019).
A BACKGROUND: GRAPH NEURAL
NETWORK FOR GRAPH CLASSIFICATION
Graph Nerual Networks (GNNs) has been proposed [15, 17, 21,
26, 55] to efficiently process graph data such as social networks,
moleculars, financial networks, etc. [18, 19]. GNN learn embedding
vectors for each node in the graph, which will be further used in
various tasks, e.g., node classification [25], graph classification [55],
community detection [10] and link prediction [59]. Specifically, in
each hidden layer, the neural network iteratively computes an em-
bedding vector for a node via aggregating the embedding vectors
of the node’s neighbors in the previous hidden layer [56], which is
called message passing [16]. Normally, only the embedding vectors
of the last hidden layer will be used for subsequent tasks. For ex-
ample, in node classification, a logistic regression classifier can be
used to classify the final embedding vectors to predict the labels
of nodes [25]; In graph classifications, information of the embed-
ding vectors in all hidden layers is utilized to jointly determine
the graph’s label [58, 60]. According to the strategies of message
passing, various GNN methods have been designed for handling spe-
cific tasks. For instance, Graph Convolutional Network (GCN) [25],
GraphSAGE [17], and Simplified Graph Convolution (SGC) [51] are
mainly for node classification, while Graph Isomorphism Network
(GIN) [55], SAG [26], and Graph U-Nets (GUNet) [15] are for graph
classification. In this paper, we choose GIN [55], SAG [26], and
GUNet as the target GNN models. Here, we briefly review GIN as
it outperforms other GNN models for graph classification.
Graph Isomorphism Network (GIN). Suppose we are given a
graph 𝐺 = (𝐴, 𝑋) with label 𝑦0, where 𝐴 ∈ {0, 1}𝑁×𝑁 is the sym-
metric adjacent matrix indicating the edge connections in 𝐺, i.e.,
𝐴𝑖 𝑗 = 1 if there is an edge between node 𝑖 and node 𝑗 and 𝐴𝑖 𝑗 = 0
otherwise. 𝑁 is the total number of nodes in the graph. 𝑋 ∈ R𝑁×𝑙
is the feature matrix for all nodes, where each row 𝑋𝑖 denote the as-
sociated 𝑙-dimensional feature vector of node 𝑖. The process of mes-
sage passing of an 𝐾-layer GIN can be formulated as follows [55]:
(11)
𝑣 = 𝑀𝐿𝑃 (𝑘)((1 + 𝜖(𝑘)) · ℎ(𝑘−1)
ℎ𝑘
+ 
ℎ(𝑘−1)
),
𝑢
𝑣
𝑢∈N𝑣
𝑖
𝑣 ∈ R𝑙𝑘 is the embedding vector of node 𝑣 at the 𝑘-th layer
where ℎ𝑘
and, for all nodes, ℎ(0)
= 𝑋𝑖, 𝑀𝐿𝑃 is a multi-layer perceptron whose
parameters are trained together with the whole GIN model, 𝜖(𝑘) is a
learnable parameter at the 𝑘-th layer, and N𝑣 is the set of neighbor
nodes of node 𝑣.
To fully utilize the structure information, GIN collects the in-
formation from all depth to predict the label of a graph in graph
classification tasks. That is, the graph’s embedding vector is ob-
tained as follows:
|𝑣 ∈ 𝐺}),
𝐺 = 𝑅𝐸𝐴𝐷𝑂𝑈𝑇 ({ℎ(𝑘)
ℎ(𝑘)
(12)
where ℎ(𝑘)
𝐺 is the embedding vector of the whole graph at the 𝑘-th
layer and the 𝑅𝐸𝐴𝐷𝑂𝑈𝑇 (·) function aggregates node embedding
vectors in this hidden layer. 𝑅𝐸𝐴𝐷𝑂𝑈𝑇 (·) can be a simple permu-
tation invariant function (e.g., summation) or a more sophisticated
graph pooling function. In this paper, we choose the graph add
pooling function (i.e., adds node features of all nodes in a batch of
graphs) as the 𝑅𝐸𝐴𝐷𝑂𝑈𝑇 (·) function. GIN finally enables a fully-
connected layer to each ℎ(𝑘)
𝐺 and sum the results to predict the label
of the graph, i.e.,
𝑣
𝑦𝑝𝑟𝑒𝑑 = 𝑠𝑜 𝑓 𝑡𝑚𝑎𝑥(𝐾−1
𝑘=0
𝐿𝑖𝑛𝑒𝑎𝑟(ℎ(𝑘)
𝐺 )),
(13)
where 𝐿𝑖𝑛𝑒𝑎𝑟 is a fully-connected layer and 𝑠𝑜 𝑓 𝑡𝑚𝑎𝑥(·) is a softmax
layer that maps the logits of GIN to values in [0, 1].
B PROOF OF THEOREM 4.1
We restate theorem 4.1:
Theorem 4.1. Given a graph 𝐺 with 𝑁 nodes, the reduction, de-
noted as 𝛽, in the time of space searching with coarse-grained search-
ing satisfies 𝛽 ≈ 𝑂(2𝜅4), where 𝜅 is the number of node clusters and
we assume 𝜅 ≪ 𝑁 .
cluster has 𝑑𝑖, 𝑖 = 1, 2, . . . , 𝜅 nodes. Note that 𝑁 =𝜅
Suppose the graph 𝐺 is partitioned into 𝜅 clusters and each
The searching space without coarse-grained searching (CGS) is:
𝑖=1 𝑑𝑖.
The total searching space of all supernodes is:
(14)
(15)
𝑆𝑔𝑟𝑎𝑝ℎ = 2 𝑁 (𝑁−1)
2
𝑆𝑛𝑜𝑑𝑒 =
2 𝑑𝑖 (𝑑𝑖−1)
2
𝑖=1
𝜅
· 𝜅
𝑖=1
we define a convex function 𝑓 (𝑥) = 2 𝑥 (𝑥−1)
equality:
2
and use Jensen’s in-
𝑆𝑛𝑜𝑑𝑒 =
𝑓 (𝑑𝑖) ≥ 𝜅 · 𝑓 ( 1
𝜅
𝑑𝑖) = 𝜅 · 𝑓 ( 𝑁
𝜅
) = 𝜅 · 2 𝑑(𝑑−1)
2
(16)
𝜅
𝑖=1
Session 1B: Attacks and Robustness CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea122The equal sign of the inequality holds when 𝑑1 = 𝑑2 = · · · = 𝑑𝜅 =
𝜅 , which means that 𝜅 clusters contain equal number of nodes.
𝑑 = 𝑁
Similarly, the total searching space of superlinks is:
𝜅
𝜅
𝑖=1
𝑗=1,𝑗≠𝑖
𝑆𝑙𝑖𝑛𝑘 =
1
2
2𝑑𝑖𝑑 𝑗
(17)
We define a cluster of convex functions 𝑓𝑖(𝑥) = 2𝑑𝑖𝑥, 𝑖 = 1, 2, . . . , 𝜅
and again deploy Jensen’s inequality to Eq.(17):
𝜅
1
2
𝑖=1
𝜅 − 1
2
𝜅
𝑓𝑖(𝑑 𝑗) ≥ 1
𝜅
2
𝑓 ( 𝑁 − 𝑑𝑖
𝜅 − 1 ) =
𝑗=1,𝑗≠𝑖
𝑖=1
𝜅
𝑖=1
𝜅 − 1
2
𝜅
𝑖=1
(𝜅 − 1) · 𝑓 (
1
𝜅 − 1
2 𝑁 𝑑𝑖−𝑑2
𝜅−1
𝑖
,
𝜅
𝑗=1,𝑗≠𝑖
𝑑 𝑗)
𝑆𝑙𝑖𝑛𝑘 =
=
(18)
𝜅−1 ,𝑗 = 1, 2, . . . , 𝜅, 𝑗 ≠ 𝑖. We
and use Jensen’s
where the equal sign holds when 𝑑 𝑗 = 𝑁−𝑑𝑖
further define a convex function 𝑓𝑙 (𝑥) = 2 𝑁 𝑥−𝑥2
𝜅−1
inequality once again to the above equation, we have :
𝑆𝑙𝑖𝑛𝑘 ≥ 𝜅 − 1
2
≥ 𝜅 − 1
2
𝑖=1
𝜅(𝜅 − 1)
2 𝑁 2
𝜅2 =
𝑖=1
· 𝜅 · 𝑓𝑙 ( 𝑁
𝜅
𝜅 − 1
2
2 𝑁 𝑑𝑖−𝑑2
𝜅
𝜅
𝑓𝑙 (𝑑𝑖)
𝜅−1 =
𝜅(𝜅 − 1)
) =