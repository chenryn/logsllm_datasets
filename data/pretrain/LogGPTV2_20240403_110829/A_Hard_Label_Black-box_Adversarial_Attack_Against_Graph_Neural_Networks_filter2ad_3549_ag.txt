Networks 2021 (2021).
[45] Nikil Wale, Ian A Watson, and George Karypis. 2008. Comparison of descrip-
tor spaces for chemical compound retrieval and classification. Knowledge and
Information Systems 14, 3 (2008), 347â€“375.
[46] Binghui Wang and Neil Zhenqiang Gong. 2019. Attacking graph-based classi-
fication via manipulating the graph structure. In Proceedings of the 2019 ACM
SIGSAC Conference on Computer and Communications Security. 2023â€“2040.
[47] Binghui Wang, Jinyuan Jia, Xiaoyu Cao, and Neil Zhenqiang Gong. 2021. Certified
robustness of graph neural networks against adversarial structural perturbation.
In ACM SIGKDD.
[48] Binghui Wang, Tianxiang Zhou, Minhua Lin, Pan Zhou, Ang Li, Meng Pang, Cai
Fu, Hai Li, and Yiran Chen. 2020. Evasion Attacks to Graph Neural Networks via
Influence Function. arXiv preprint arXiv:2009.00203 (2020).
[49] Shen Wang, Zhengzhang Chen, Xiao Yu, Ding Li, Jingchao Ni, Lu-An Tang, Jiaping
Gui, Zhichun Li, Haifeng Chen, and S Yu Philip. 2019. Heterogeneous Graph
Matching Networks for Unknown Malware Detection.. In IJCAI. 3762â€“3770.
[50] Xiaoyun Wang, Minhao Cheng, Joe Eaton, Cho-Jui Hsieh, and Felix Wu. 2018.
Attack graph convolutional networks by adding fake nodes. arXiv preprint
arXiv:1810.10751 (2018).
[51] Felix Wu, Tianyi Zhang, Amauri Holanda de Souza Jr, Christopher Fifty, Tao Yu,
and Kilian Q Weinberger. 2019. Simplifying graph convolutional networks. arXiv
preprint arXiv:1902.07153 (2019).
Session 1B: Attacks and Robustness CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea121[52] Huijun Wu, Chen Wang, Yuriy Tyshetskiy, Andrew Docherty, Kai Lu, and Liming
Zhu. 2019. Adversarial examples on graph data: Deep insights into attack and
defense. arXiv preprint arXiv:1903.01610 (2019).
[53] Zhaohan Xi, Ren Pang, Shouling Ji, and Ting Wang. 2020. Graph backdoor. arXiv
preprint arXiv:2006.11890 (2020).
[54] Kaidi Xu, Hongge Chen, Sijia Liu, Pin-Yu Chen, Tsui-Wei Weng, Mingyi Hong,
and Xue Lin. 2019. Topology attack and defense for graph neural networks: An
optimization perspective. arXiv preprint arXiv:1906.04214 (2019).
are graph neural networks? arXiv preprint arXiv:1810.00826 (2018).
[56] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi
Kawarabayashi, and Stefanie Jegelka. 2018. Representation learning on graphs
with jumping knowledge networks. arXiv preprint arXiv:1806.03536 (2018).
[55] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerful
[59] Muhan Zhang and Yixin Chen. 2018. Link prediction based on graph neural
[57] Pinar Yanardag and SVN Vishwanathan. 2015. Deep graph kernels. In Proceedings
of the 21th ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining. 1365â€“1374.
[58] Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure
Leskovec. 2018. Hierarchical graph representation learning with differentiable
pooling. In Advances in neural information processing systems. 4800â€“4810.
networks. In Advances in Neural Information Processing Systems. 5165â€“5175.
[60] Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. 2018. An end-
to-end deep learning architecture for graph classification. In Proceedings of the
AAAI Conference on Artificial Intelligence, Vol. 32.
[61] Zaixi Zhang, Jinyuan Jia, Binghui Wang, and Neil Zhenqiang Gong. 2020. Back-
door attacks to graph neural networks. arXiv preprint arXiv:2006.11165 (2020).
[62] Fan Zhou, Chengtai Cao, Kunpeng Zhang, Goce Trajcevski, Ting Zhong, and
Ji Geng. 2019. Meta-GNN: On Few-shot Node Classification in Graph Meta-
learning. In Proceedings of the 28th ACM International Conference on Information
and Knowledge Management. 2357â€“2360.
[63] Daniel ZÃ¼gner, Amir Akbarnejad, and Stephan GÃ¼nnemann. 2018. Adversarial
attacks on neural networks for graph data. In Proceedings of the 24th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining. 2847â€“2856.
[64] Daniel ZÃ¼gner and Stephan GÃ¼nnemann. 2019. Adversarial attacks on graph
neural networks via meta learning. arXiv preprint arXiv:1902.08412 (2019).
A BACKGROUND: GRAPH NEURAL
NETWORK FOR GRAPH CLASSIFICATION
Graph Nerual Networks (GNNs) has been proposed [15, 17, 21,
26, 55] to efficiently process graph data such as social networks,
moleculars, financial networks, etc. [18, 19]. GNN learn embedding
vectors for each node in the graph, which will be further used in
various tasks, e.g., node classification [25], graph classification [55],
community detection [10] and link prediction [59]. Specifically, in
each hidden layer, the neural network iteratively computes an em-
bedding vector for a node via aggregating the embedding vectors
of the nodeâ€™s neighbors in the previous hidden layer [56], which is
called message passing [16]. Normally, only the embedding vectors
of the last hidden layer will be used for subsequent tasks. For ex-
ample, in node classification, a logistic regression classifier can be
used to classify the final embedding vectors to predict the labels
of nodes [25]; In graph classifications, information of the embed-
ding vectors in all hidden layers is utilized to jointly determine
the graphâ€™s label [58, 60]. According to the strategies of message
passing, various GNN methods have been designed for handling spe-
cific tasks. For instance, Graph Convolutional Network (GCN) [25],
GraphSAGE [17], and Simplified Graph Convolution (SGC) [51] are
mainly for node classification, while Graph Isomorphism Network
(GIN) [55], SAG [26], and Graph U-Nets (GUNet) [15] are for graph
classification. In this paper, we choose GIN [55], SAG [26], and
GUNet as the target GNN models. Here, we briefly review GIN as
it outperforms other GNN models for graph classification.
Graph Isomorphism Network (GIN). Suppose we are given a
graph ğº = (ğ´, ğ‘‹) with label ğ‘¦0, where ğ´ âˆˆ {0, 1}ğ‘Ã—ğ‘ is the sym-
metric adjacent matrix indicating the edge connections in ğº, i.e.,
ğ´ğ‘– ğ‘— = 1 if there is an edge between node ğ‘– and node ğ‘— and ğ´ğ‘– ğ‘— = 0
otherwise. ğ‘ is the total number of nodes in the graph. ğ‘‹ âˆˆ Rğ‘Ã—ğ‘™
is the feature matrix for all nodes, where each row ğ‘‹ğ‘– denote the as-
sociated ğ‘™-dimensional feature vector of node ğ‘–. The process of mes-
sage passing of an ğ¾-layer GIN can be formulated as follows [55]:
(11)
ğ‘£ = ğ‘€ğ¿ğ‘ƒ (ğ‘˜)((1 + ğœ–(ğ‘˜)) Â· â„(ğ‘˜âˆ’1)
â„ğ‘˜
+ 
â„(ğ‘˜âˆ’1)
),
ğ‘¢
ğ‘£
ğ‘¢âˆˆNğ‘£
ğ‘–
ğ‘£ âˆˆ Rğ‘™ğ‘˜ is the embedding vector of node ğ‘£ at the ğ‘˜-th layer
where â„ğ‘˜
and, for all nodes, â„(0)
= ğ‘‹ğ‘–, ğ‘€ğ¿ğ‘ƒ is a multi-layer perceptron whose
parameters are trained together with the whole GIN model, ğœ–(ğ‘˜) is a
learnable parameter at the ğ‘˜-th layer, and Nğ‘£ is the set of neighbor
nodes of node ğ‘£.
To fully utilize the structure information, GIN collects the in-
formation from all depth to predict the label of a graph in graph
classification tasks. That is, the graphâ€™s embedding vector is ob-
tained as follows:
|ğ‘£ âˆˆ ğº}),
ğº = ğ‘…ğ¸ğ´ğ·ğ‘‚ğ‘ˆğ‘‡ ({â„(ğ‘˜)
â„(ğ‘˜)
(12)
where â„(ğ‘˜)
ğº is the embedding vector of the whole graph at the ğ‘˜-th
layer and the ğ‘…ğ¸ğ´ğ·ğ‘‚ğ‘ˆğ‘‡ (Â·) function aggregates node embedding
vectors in this hidden layer. ğ‘…ğ¸ğ´ğ·ğ‘‚ğ‘ˆğ‘‡ (Â·) can be a simple permu-
tation invariant function (e.g., summation) or a more sophisticated
graph pooling function. In this paper, we choose the graph add
pooling function (i.e., adds node features of all nodes in a batch of
graphs) as the ğ‘…ğ¸ğ´ğ·ğ‘‚ğ‘ˆğ‘‡ (Â·) function. GIN finally enables a fully-
connected layer to each â„(ğ‘˜)
ğº and sum the results to predict the label
of the graph, i.e.,
ğ‘£
ğ‘¦ğ‘ğ‘Ÿğ‘’ğ‘‘ = ğ‘ ğ‘œ ğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥(ğ¾âˆ’1
ğ‘˜=0
ğ¿ğ‘–ğ‘›ğ‘’ğ‘ğ‘Ÿ(â„(ğ‘˜)
ğº )),
(13)
where ğ¿ğ‘–ğ‘›ğ‘’ğ‘ğ‘Ÿ is a fully-connected layer and ğ‘ ğ‘œ ğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥(Â·) is a softmax
layer that maps the logits of GIN to values in [0, 1].
B PROOF OF THEOREM 4.1
We restate theorem 4.1:
Theorem 4.1. Given a graph ğº with ğ‘ nodes, the reduction, de-
noted as ğ›½, in the time of space searching with coarse-grained search-
ing satisfies ğ›½ â‰ˆ ğ‘‚(2ğœ…4), where ğœ… is the number of node clusters and
we assume ğœ… â‰ª ğ‘ .
cluster has ğ‘‘ğ‘–, ğ‘– = 1, 2, . . . , ğœ… nodes. Note that ğ‘ =ğœ…
Suppose the graph ğº is partitioned into ğœ… clusters and each
The searching space without coarse-grained searching (CGS) is:
ğ‘–=1 ğ‘‘ğ‘–.
The total searching space of all supernodes is:
(14)
(15)
ğ‘†ğ‘”ğ‘Ÿğ‘ğ‘â„ = 2 ğ‘ (ğ‘âˆ’1)
2
ğ‘†ğ‘›ğ‘œğ‘‘ğ‘’ =
2 ğ‘‘ğ‘– (ğ‘‘ğ‘–âˆ’1)
2
ğ‘–=1
ğœ…
Â· ğœ…
ğ‘–=1
we define a convex function ğ‘“ (ğ‘¥) = 2 ğ‘¥ (ğ‘¥âˆ’1)
equality:
2
and use Jensenâ€™s in-
ğ‘†ğ‘›ğ‘œğ‘‘ğ‘’ =
ğ‘“ (ğ‘‘ğ‘–) â‰¥ ğœ… Â· ğ‘“ ( 1
ğœ…
ğ‘‘ğ‘–) = ğœ… Â· ğ‘“ ( ğ‘
ğœ…
) = ğœ… Â· 2 ğ‘‘(ğ‘‘âˆ’1)
2
(16)
ğœ…
ğ‘–=1
Session 1B: Attacks and Robustness CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea122The equal sign of the inequality holds when ğ‘‘1 = ğ‘‘2 = Â· Â· Â· = ğ‘‘ğœ… =
ğœ… , which means that ğœ… clusters contain equal number of nodes.
ğ‘‘ = ğ‘
Similarly, the total searching space of superlinks is:
ğœ…
ğœ…
ğ‘–=1
ğ‘—=1,ğ‘—â‰ ğ‘–
ğ‘†ğ‘™ğ‘–ğ‘›ğ‘˜ =
1
2
2ğ‘‘ğ‘–ğ‘‘ ğ‘—
(17)
We define a cluster of convex functions ğ‘“ğ‘–(ğ‘¥) = 2ğ‘‘ğ‘–ğ‘¥, ğ‘– = 1, 2, . . . , ğœ…
and again deploy Jensenâ€™s inequality to Eq.(17):
ğœ…
1
2
ğ‘–=1
ğœ… âˆ’ 1
2
ğœ…
ğ‘“ğ‘–(ğ‘‘ ğ‘—) â‰¥ 1
ğœ…
2
ğ‘“ ( ğ‘ âˆ’ ğ‘‘ğ‘–
ğœ… âˆ’ 1 ) =
ğ‘—=1,ğ‘—â‰ ğ‘–
ğ‘–=1
ğœ…
ğ‘–=1
ğœ… âˆ’ 1
2
ğœ…
ğ‘–=1
(ğœ… âˆ’ 1) Â· ğ‘“ (
1
ğœ… âˆ’ 1
2 ğ‘ ğ‘‘ğ‘–âˆ’ğ‘‘2
ğœ…âˆ’1
ğ‘–
,
ğœ…
ğ‘—=1,ğ‘—â‰ ğ‘–
ğ‘‘ ğ‘—)
ğ‘†ğ‘™ğ‘–ğ‘›ğ‘˜ =
=
(18)
ğœ…âˆ’1 ,ğ‘— = 1, 2, . . . , ğœ…, ğ‘— â‰  ğ‘–. We
and use Jensenâ€™s
where the equal sign holds when ğ‘‘ ğ‘— = ğ‘âˆ’ğ‘‘ğ‘–
further define a convex function ğ‘“ğ‘™ (ğ‘¥) = 2 ğ‘ ğ‘¥âˆ’ğ‘¥2
ğœ…âˆ’1
inequality once again to the above equation, we have :
ğ‘†ğ‘™ğ‘–ğ‘›ğ‘˜ â‰¥ ğœ… âˆ’ 1
2
â‰¥ ğœ… âˆ’ 1
2
ğ‘–=1
ğœ…(ğœ… âˆ’ 1)
2 ğ‘ 2
ğœ…2 =
ğ‘–=1
Â· ğœ… Â· ğ‘“ğ‘™ ( ğ‘
ğœ…
ğœ… âˆ’ 1
2
2 ğ‘ ğ‘‘ğ‘–âˆ’ğ‘‘2
ğœ…
ğœ…
ğ‘“ğ‘™ (ğ‘‘ğ‘–)
ğœ…âˆ’1 =
ğœ…(ğœ… âˆ’ 1)
) =