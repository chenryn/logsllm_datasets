compute the high end of the reported range.
5 MITIGATING CORRUPTION
There are two aspects to CorrOpt, our system to mitigate corruption.
First, to protect applications from corruption, we disable corrupt-
ing links, while meeting configured capacity constraints. Meeting
capacity constraints is important because otherwise we may trade
5A breakout cable splits a high-speed port (e.g, 40Gbps, 100Gbps) into several low-
speed ports (e.g, 10Gbps, 25Gbps). It is typically used between switches with different
port speed.
Figure 8: Bent fiber (red arrow) can cause packet corruption
when the fiber cable is bent tighter than its maximum bend
tolerance. When operating a large fiber plant, this situation
becomes likely especially for long cables.
(a) Optical power
(b) Corruption loss rate
Figure 9: An example of a damaged fiber causing packet cor-
ruption. Fiber damage happens on October 30 causing both
sides’ RxPower to suddenly drop at the same time. TxPower
on both sides are not affected. When traffic is put on the
link at the beginning of December, the corruption loss rate is
around 1%. Fiber replacement restores both sides’ RxPower
back to normal level.
long life expectancy, but old lasers can suffer deterioration in Tx-
Power, leading to low RxPower and corruption on the receive side
of the link. Replacing the dying transceiver can resolve the problem.
The most probable symptom of decaying transmitters is that the
TxPower on the send side of the link and RxPower on the receive
side of the link are both low or are gradually decreasing.
Root cause 4: Bad or loose transceivers. Bad transceivers or loosely-
seated ones (i.e., not properly plugged in) also cause corruption.
When this happens, technicians should take out the transceiver and
plug it back in (a.k.a., reseating the transceiver). If the issue is not
resolved, the transceiver is likely bad and needs to be replaced.
When bad or loosely-seated transceivers cause corruption, opti-
cal TxPower and RxPower on both sides of the link are most likely
high, but the link still corrupts packets. This symptom is shared
by root cause 5 as well, but a distinguishing characteristic of this
root cause is that only one of the links on the switch is bad. It is
uncommon (but still possible) that multiple transceivers on the
same switch are bad or loose.
Understanding and Mitigating Packet Corruption in DCNs
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
Figure 10: Example of problems with switch-local checking, with ToR capacity constraints of c=60%. (a): Every switch keeps
√
sc=c=60% of its uplinks alive, resulting in 8 disabled links, but only 9 out of 25 paths to the spine are still available for T , far
c = 0.77 of the links are kept online, the ToR capacity constraint is met, but only 4
below the constraint of 60%. (b): When sc=
links can be disabled. (c): The optimal solution, which has 12 disabled links offline and meets the capacity constraints.
off corruption losses for heavy congestion losses. For practical rea-
sons, we only consider disabling links as a strategy in this paper; it
requires minimal changes to our existing infrastructure. We will
consider other strategies, such as error coding, source routing, or
traffic engineering to move sensitive traffic away from corrupting
links, in future work. That said, any such strategy would still need
to disable corrupting links at some point to enable operators to fix
them. Our strategy to detect and disable corrupting links can be
used in combination with these strategies.
If we rely solely on disabling links for corruption mitigation,
the DCN will have fewer and fewer links as time progresses. In-
stead, we must also fix the root cause of corruption, so links can be
enabled again. Thus, the second aspect of CorrOpt is generating
repair recommendations for disabled links based on root causes
and symptoms described in §4. Our recommendations reduce both
repair time and packet loss (§7.2).
5.1 Disabling Corrupting Links
While disabling corrupting links reduces corruption losses, it also
reduces network capacity. In the extreme cases, especially because
of the locality of corrupting links, blindly disabling links can create
hotspots, and, hence, engender heavy congestion losses; it may
even partition the network.
To lower corruption losses without creating heavy congestion,
we consider a common capacity metric [24, 29, 34]: the fraction
of available valley-free paths from a top-of-rack switch (ToR) to
the highest stage of the network (i.e., the spine). This metric quan-
tifies available capacity and redundancy for a ToR after links are
disabled. Because traffic demand can differ across ToRs [17], we
allow per-ToR thresholds. Our data show up to 15% of corrupting
links cannot be disabled due to capacity constraints under real-
istic configurations (e.g., when every ToR has threshold between
50–75%).
CorrOpt determines the subset of links to disable based on the
impact of corrupting links that remain active. Each link l with
packet corruption rate of fl has impact I ( fl ), where I (.) is a mono-
tonically increasing penalty function that reflects the relation-
(cid:80)
ship between application performance and loss rate [27, 36]. Cor-
rOpt aims to minimize the total penalty of packet corruption, i.e.,
l ∈links (1 − dl ) × I ( fl ), where dl is 1 if the link is disabled and 0
otherwise. Our goal is to determine the value of dl for each link l,
while meeting capacity constraints.
However, as we prove in Appendix A (via reduction to 3-SAT)
this problem is computationally difficult.6
Theorem 5.1. Deciding which links to disable in a Clos topology,
s.t. the total penalty of packet corruption is minimized under capacity
constraints, is NP-complete.
Because of the complexity, we cannot quickly determine the optimal
set of links to disable. Speed is desirable to protect applications from
corruption, but it is not possible to be both fast and optimal.
State-of-the-art: switch-local checking. Current DCN operators
opt for speed [26]. When a new corrupting link is found, a controller
decides whether it can be disabled based on the number of available
uplinks at the switch to which it is attached. For a threshold of
sc and a switch with m uplinks, ⌊m × (1 − sc )⌋ of the uplinks can
be disabled. For example, with m = 5 uplinks and sc = 60%, at
most two uplinks can be disabled. When a link is enabled, after
repairing corruption or other problems, the same check is run for
all active corrupting links to see if additional links, which could
not be disabled before, can be disabled now.
Unfortunately, switch-local checks are highly sub-optimal. Fig-
ure 10 shows an example, where T is a ToR with five uplinks, to
switches (A through E) that also have five uplinks each. Corrupting
links are in red, and dashed lines represent disabled links. Suppose
we want to enforce a per-ToR capacity constraint of c=60%. If we
directly map c to the switch-local constraint, i.e., sc=c, Figure 10(a)
shows the network state that will emerge. The direct mapping leads
to disabling eight corrupting links. However, ToR T now has only
nine of 25 (36%) possible paths to the spine, far below the desired
limit of 60%.
This problem can be fixed by enforcing a switch-local capacity
√
constraint of sc =
c = 0.77 because this forces c fraction of
paths to the core switches to be available. But now, as shown in
Figure 10(b), each switch can disable only one corrupting uplink,
for a total of four disabled links (out of a total of 16 corrupting links).
The optimal solution, however, shown in Figure 10(c), can disable
as many as 12 corrupting links, for a much lower total penalty due
to active corrupting links.
Generalizing the example above, in a simple ToR-aggregation-
spine-topology, a capacity constraint of c requires every switch
to keep √
c of its uplinks. Otherwise, the capacity constraint can
be violated. The gap widens when the DCN has more tiers: with r
tiers above the ToR-level, a switch-local algorithm needs to keep
r√
c fraction of uplinks active.
6The NP-hard problem stated in [9] is orthogonal to our formulation, as it moves
logical machines between physical machines.
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
D. Zhuo et al.
Another limitation of a switch-local checker is that it cannot
handle different ToR requirements well. If one ToR has a high
capacity requirement c′, all upstream switches need to keep r√
c′
uplinks active. A switch-local checker may not be able to disable a
single link in extreme cases.
CorrOpt’s approach. CorrOpt achieves both speed and optimality
using a two-pronged approach. First, when a new corrupting link
is found, it runs a fast checker for a quick response that exploits
global network state to bypass the sub-optimality of switch-local
checking. Second, when links become active, CorrOpt runs an op-
timizer that globally optimizes over all active corrupting links in
the network. Link activations allow other remaining corrupting
links to be turned off. Those links tend to have lower loss rates than
newly arrived corrupting link due to the fast checker disabling lossy
links, which gives us time to solve a hard problem. By analyzing the
failure structures in our data set, we are able to efficiently solve the
practical instances of this NP-complete problem. We now provide
more detail on the two components.
Fast checker. Conceptually, when a new corrupting link l arrives,
CorrOpt counts the remaining paths for each ToR to the spine
assuming l is removed from the topology. If no ToR’s constraint is
violated, CorrOpt disables l and creates a maintenance ticket for it
with a recommended repair. This process is repeated for each new
corrupting link. As long as no link is activated since its last run,
the network state after the fast checker runs is maximal, i.e., no
more links can be disabled. If no link was activated since the last
run of the fast checker, the optimizer must have left the network
in a maximal state. Thus, we never need to run fast checker on old
corrupting links that could not be disabled earlier.
Because of its exact counting of paths, our fast checker can dis-
able more links than switch-local checking. A naive implementation
of the fast checker is to iterate over all the path from ToR switches
to the spine switches in order to count the number of available
paths for each ToR. The naive implementation is slow because a
large data center network can possibly have over millions of paths.
Using information about all links E in the DCN, we efficiently im-
plement CorrOpt’s fast checker as follows. First, for each switch v2
in the second-highest stage, we count the active (one-hop) paths
p1 (v2) to the spine (i.e., the highest stage). Then, each switch v3
in the third-highest stage adds p1 (v2) to each of its active uplinks,
obtaining the number of two-hop paths p2 (v3) to the spine. This
process is iterated until the ToR-stage is reached.
With this information, to see if l can be safely disabled, we
check the downstream of l, updating the path counts with the same
method, beginning with the switch directly downstream of l. If all
downstream ToRs of l meet the capacity constraints with l offline,
l is disabled. Conceptually, we perform O (1) operations per link,
resulting in a linear runtime of O (|E|). In our experiments, the
fast checker takes only 100-300 ms for the largest DCN, effectively
providing instantaneous decisions.
Optimizer. When a link is enabled, one option is to rerun the fast
checker on all active corrupting links, as is done today in switch-
local checks. However, we can now afford to run a potentially-
slower computation to determine the optimal subset of links to
Figure 11: Example of topology pruning. If capacity con-
straint is 50%, only ToR J will violate the constraint if all
corrupting links (in red) are disabled. Hence, we only need
to consider the pruned topology and can safely disable the
other three links.
disable. The optimization problem is what we defined earlier, oper-
ating over the set of active corrupting links.7
Even though the problem is NP-complete, we can provide a
fast exact algorithm in practice. First, we find that, under realistic
capacity constraints, 99% of the ToRs can be ignored because their
capacity constraints will not be violated even if all corrupting links
are disabled. Only the links that are in danger of capacity constraint
violation need to be considered. To identify such links, we run
fast checker’s path counting procedure on the network with all
corrupting links considered disabled, identifying all ToRs V whose
capacity constraints are violated. Only disabling links upstream of
the ToRs in V can violate capacity constraints. All corrupting links
not upstream of V can hence be safely disabled, thus pruning the
topology.
For instance, in Figure 11, assume the capacity constraint is 50%.
The corrupting links are shown in red, and if we were to disable all
of them, ToRs G, H, and I will have at least two out of four paths
to the spines, but ToR J will have only one. Thus, we can remove
all links and switches except those upstream of J, and the three
removed corrupting links can be safely disabled.
Next, we need to decide which remaining corrupting links in
the pruned topology can be disabled. CorrOpt iterates through all
possible subset to measure (1) whether the capacity constraint is
met if the entire subset is turned off (2) the total penalty if the
subset is turned off. To speed up, CorrOpt uses a “reject cache” to
memorize subsets that can fail capacity constraint. When CorrOpt
iterates through a subset that is a super set of any set in the cache,
the subset is immediately ignored. For example, in Figure 11, {AF,
EJ} can be kept in the cache. Any subset S such that S is a superset
of {AF,EJ} is guaranteed to fail the capacity constraint and thus can
be ignored.
The output of CorrOpt’s optimizer is an exact solution to the
optimization problem. Pruning only removes links that can be safely
disabled and the “reject cache” only skips infeasible link sets. In our
experiments, the combination of both techniques allows us to finish
7For practical reasons, CorrOpt does not enable corrupting links before they have
been repaired. In theory, we can further reduce corruption losses by doing so; e.g.,
pre-maturely enabling a link with a lower corruption rate may allow a link with a
higher corruption rate to be disabled. On our small to medium DCNs, the performance
of this optimal version was close to that of CorrOpt. We could not evaluate this version
on our large DCNs because of its computational complexity, which is worse than
CorrOpt’s optimizer since it considers a bigger set of links at each step.
Understanding and Mitigating Packet Corruption in DCNs
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
return Replace cable/fiber
neighbors ← links sharing same component (i.e., switch)
if has_corruption(neighbors) then
return Replace shared component
if has_corruption(opposite_side) then
Rx1 ← RxPower of link
Rx2 ← RxPower of opposite side of link
Tx2 ← TxPower of opposite side of link
if Tx2 ≤ PowerT hr eshT x then
if Rx1 < PowerT hr eshRx and Rx2 < PowerT hr eshRx then
Algorithm 1 CorrOpt’s recommendation engine
1: function Recommend Repair(link)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
return Clean fiber
if Transceiver is not reseated recently then
return Reseat transceiver
return Replace transceiver
return Replace transceiver on the opposite side
if Rx1 < PowerT hr eshRx then
return Replace cable/fiber
else
else