以下小节概述了 Pacemaker 如何分配资源。
::: section
::: titlepage
### []{#ch-advancedresource-HAAR.html#s3-nodepreference-HAAR}节点首选项 {.title}
:::
Pacemaker 根据以下策略决定在分配资源时首选哪个节点。
::: itemizedlist
-   节点权重最高的节点会首先被消耗。节点 weight
    是集群维护的分数，以表示节点健康状况。
-   如果多个节点具有相同的节点权重：
    ::: itemizedlist
    -   如果 `placement-strategy`{.literal} 集群属性是
        `default`{.literal} 或 `utilization`{.literal} ：
        ::: itemizedlist
        -   分配的资源最少的节点会首先被消耗。
        -   如果分配的资源数量相等，在 CIB
            中列出的第一个有资格的节点会首先被消耗。
        :::
    -   如果 `placement-strategy`{.literal} 集群属性 `均衡`{.literal} ：
        ::: itemizedlist
        -   具有最多可用容量的节点会首先被消耗。
        -   如果节点的空闲容量相等，首先消耗分配的资源数量最少的节点。
        -   如果节点的空闲容量相等，并且分配的资源数量相等，在 CIB
            中列出的第一个有资格的节点会首先被消耗。
        :::
    -   如果 `placement-strategy`{.literal} 集群属性
        `最小`{.literal}，则 CIB
        中列出的第一个有资格的节点会首先被消耗。
    :::
:::
:::
::: section
::: titlepage
### []{#ch-advancedresource-HAAR.html#s3-nodecapacity-HAAR}节点容量 {.title}
:::
Pacemaker 根据以下策略决定哪个节点拥有最多的可用容量。
::: itemizedlist
-   如果只定义了一种类型的使用属性，那么空闲容量就是一个简单的数字比较。
-   如果定义了多个类型的使用属性,那么在最多属性类型中数字最高的节点具有最大的可用容量。例如：
    ::: itemizedlist
    -   如果 NodeA 有更多可用 CPU，而 NodeB
        拥有更多可用内存，则它们的可用容量是相等的。
    -   如果 NodeA 有更多可用 CPU，而 NodeB 有更多可用内存和存储，则
        NodeB 具有更多可用容量。
    :::
:::
:::
::: section
::: titlepage
### []{#ch-advancedresource-HAAR.html#s3-resourceallocateprefer-HAAR}资源分配首选项 {.title}
:::
Pacemaker 根据以下策略决定优先分配哪些资源。
::: itemizedlist
-   优先级最高的资源会首先被分配。有关为资源设置优先级的详情请参考
    [表 6.3
    "资源元数据选项"](#ch-clustresources-HAAR.html#tb-resource-options-HAAR "表 6.3. 资源元数据选项"){.xref}。
-   如果资源优先级相等，运行该资源的节点中分数最高的资源会首先被分配，以防止资源
    shuffling 的问题。
-   如果资源在运行资源的节点中的分数相等，或者资源没有运行，则首选节点上具有最高分数的资源会被首先分配。如果首选节点上的资源分数相等，则
    CIB 中列出的第一个可运行资源会首先被分配。
:::
:::
:::
::: section
::: titlepage
## []{#ch-advancedresource-HAAR.html#s2-resourceguildelines-HAAR}资源放置策略指南 {.title}
:::
为确保 Pacemaker
对资源放置策略最有效的工作，在配置系统时应考虑以下事项。
::: itemizedlist
-   请确定您有足够的物理容量。
    如果节点在通常情况下使用的物理容量接近近似的最大值，那么在故障切换过程中可能会出现问题。即使没有使用功能，您仍可能会遇到超时和二级故障。
-   在您为节点配置的功能中构建一些缓冲。
    假设 Pacemaker 资源不会使用 100% 配置的 CPU
    和内存量，所以所有时间都比您的物理资源稍多。这种方法有时被称为过量使用。
-   指定资源优先级。
    如果集群需要牺牲一些服务，则这些服务应该是对您最不重要的。确保正确设置资源优先级，以便首先调度最重要的资源。有关设置资源优先级的详情请参考
    [表 6.3
    "资源元数据选项"](#ch-clustresources-HAAR.html#tb-resource-options-HAAR "表 6.3. 资源元数据选项"){.xref}。
:::
:::
::: section
::: titlepage
## []{#ch-advancedresource-HAAR.html#s2-nodeutilization_agent-HAAR}NodeUtilization 资源代理（红帽企业 Linux 7.4 及更高版本） {.title}
:::
红帽企业 Linux 7.4 支持 `NodeUtilization`{.literal}
资源代理。NodeUtilization 代理可以检测可用的
CPU、主机内存可用性和虚拟机监控程序内存可用性的系统参数，并将这些参数添加到
CIB 中。您可以将代理作为克隆资源运行，使其在每个节点上自动填充这些参数。
有关 `NodeUtilization`{.literal}
资源代理和此代理的资源选项的信息，请运行 [**pcs resource describe
NodeUtilization**]{.command} 命令。
:::
:::
::: section
::: titlepage
# []{#ch-advancedresource-HAAR.html#s1-nonpacemakerstartup-HAAR}为不由 Pacemaker 管理的资源依赖项配置启动顺序（Red Hat Enterprise Linux 7.4 及更新的版本） {.title}
:::
集群可能包含不是由集群管理的依赖项的资源。在这种情况下，您必须确保在
Pacemaker 停止后启动这些依赖项，然后才能停止 Pacemaker。
从 Red Hat Enterprise Linux 7.4 开始，您可以通过 `systemd`{.literal}
`resource-agents-deps`{.literal}
目标将您的启动顺序配置为在这种情况下。您可以为此目标创建一个
`systemd`{.literal} 置入单元，Pacemaker 会根据这个目标自行排序。
例如，如果集群包含不受集群管理的外部服务 `foo`{.literal}
的资源，您可以创建包含以下内容的 drop-in 单元
`/etc/systemd/system/resource-agents-deps.target.d/foo.conf`{.filename}
：
``` screen
[Unit]
Requires=foo.service
After=foo.service
```
创建置入单元后，运行 [**systemctl daemon-reload**]{.command} 命令。
用这种方法指定的集群依赖项可以是服务以外的其它依赖项。例如，您可能依赖于在
`/srv`{.literal} 中挂载文件系统，在这种情况下，根据 systemd
`文档`{.literal} 为其创建一个 `systemd`{.literal} file
`srv.mount`{.literal}，然后创建一个置入单元，如 `.conf`{.filename}
文件中使用 `srv.mount`{.literal} 而不是 `foo.service`{.literal}
文件所述，以确保 Pacemaker 在挂载磁盘后启动。
:::
::: section
::: titlepage
# []{#ch-advancedresource-HAAR.html#s1-snmpandpacemaker-HAAR}使用 SNMP 查询 Pacemaker 集群（Red Hat Enterprise Linux 7.5 及更新的版本） {.title}
:::
从 Red Hat Enterprise Linux 7.5 开始，您可以使用
`pcs_snmp_agent`{.literal} 守护进程通过 SNMP 查询 Pacemaker
集群的数据。`pcs_snmp_agent`{.literal} 守护进程是一个 SNMP
代理，通过代理 `x 协议连接到主代理`{.literal}
(s`nmpd`{.literal})。`pcs_snmp_agent`{.literal}
代理不充当独立代理，因为它仅向主代理提供数据。
以下流程为系统设置基本配置，以便在 Pacemaker 集群中使用
SNMP。您可以在集群的每个节点上运行此步骤，您将使用 SNMP 为集群获取数据。
::: orderedlist
1.  在群集的每个节点上安装 `pcs-snmp`{.literal} 软件包。这还将安装提供
    `sn mp 守护进程的 net-sn`{.literal} mp``{=html} 软件包。
    ``` screen
    # yum install pcs-snmp
    ```
2.  将以下行添加到 `/etc/snmp/snmpd.conf`{.filename} 配置文件，以将
    `snmpd`{.literal} 守护进程设置为 `主代理x`{.literal}。
    ``` screen
    master agentx
    ```
3.  将以下行添加到 `/etc/snmp/snmpd.conf`{.filename} 配置文件，以在同一
    SNMP 配置中启用 `pcs_snmp_agent`{.literal}。
    ``` screen
    view    systemview    included   .1.3.6.1.4.1.32723.100
    ```
4.  启动 `pcs_snmp_agent`{.literal} 服务。
    ``` screen
    # systemctl start pcs_snmp_agent.service
    # systemctl enable pcs_snmp_agent.service
    ```
5.  要检查配置，请使用 [**pcs status**]{.command}
    显示群集的状态，然后尝试从 SNMP
    获取数据，以检查它是否与输出对应。请注意，当使用 SNMP
    获取数据时，只会提供原始资源。
    以下示例显示了在运行中的群集上使用失败操作的 [**pcs
    status**]{.command} 命令的输出结果。
    ``` screen
    # pcs status
    Cluster name: rhel75-cluster
    Stack: corosync
    Current DC: rhel75-node2 (version 1.1.18-5.el7-1a4ef7d180) - partition with quorum
    Last updated: Wed Nov 15 16:07:44 2017
    Last change: Wed Nov 15 16:06:40 2017 by hacluster via cibadmin on rhel75-node1
    2 nodes configured
    14 resources configured (1 DISABLED)
    Online: [ rhel75-node1 rhel75-node2 ]
    Full list of resources:
     fencing        (stonith:fence_xvm):    Started rhel75-node1
     dummy5 (ocf::pacemaker:Dummy): Stopped (disabled)
     dummy6 (ocf::pacemaker:Dummy): Stopped
     dummy7 (ocf::pacemaker:Dummy): Started rhel75-node2
     dummy8 (ocf::pacemaker:Dummy): Started rhel75-node1
     dummy9 (ocf::pacemaker:Dummy): Started rhel75-node2
     Resource Group: group1
         dummy1     (ocf::pacemaker:Dummy): Started rhel75-node1
         dummy10    (ocf::pacemaker:Dummy): Started rhel75-node1
     Clone Set: group2-clone [group2]
         Started: [ rhel75-node1 rhel75-node2 ]
     Clone Set: dummy4-clone [dummy4]
         Started: [ rhel75-node1 rhel75-node2 ]
    Failed Actions:
    * dummy6_start_0 on rhel75-node1 'unknown error' (1): call=87, status=complete, exitreason='',
        last-rc-change='Wed Nov 15 16:05:55 2017', queued=0ms, exec=20ms
    ```
    ``` screen
    # snmpwalk -v 2c -c public localhost PACEMAKER-PCS-V1-MIB::pcmkPcsV1Cluster
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterName.0 = STRING: "rhel75-cluster"
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterQuorate.0 = INTEGER: 1
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterNodesNum.0 = INTEGER: 2
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterNodesNames.0 = STRING: "rhel75-node1"
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterNodesNames.1 = STRING: "rhel75-node2"
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterCorosyncNodesOnlineNum.0 = INTEGER: 2
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterCorosyncNodesOnlineNames.0 = STRING: "rhel75-node1"
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterCorosyncNodesOnlineNames.1 = STRING: "rhel75-node2"
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterCorosyncNodesOfflineNum.0 = INTEGER: 0
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterPcmkNodesOnlineNum.0 = INTEGER: 2
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterPcmkNodesOnlineNames.0 = STRING: "rhel75-node1"
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterPcmkNodesOnlineNames.1 = STRING: "rhel75-node2"
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterPcmkNodesStandbyNum.0 = INTEGER: 0
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterPcmkNodesOfflineNum.0 = INTEGER: 0
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterAllResourcesNum.0 = INTEGER: 11
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterAllResourcesIds.0 = STRING: "fencing"
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterAllResourcesIds.1 = STRING: "dummy5"
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterAllResourcesIds.2 = STRING: "dummy6"
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterAllResourcesIds.3 = STRING: "dummy7"
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterAllResourcesIds.4 = STRING: "dummy8"
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterAllResourcesIds.5 = STRING: "dummy9"
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterAllResourcesIds.6 = STRING: "dummy1"
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterAllResourcesIds.7 = STRING: "dummy10"
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterAllResourcesIds.8 = STRING: "dummy2"
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterAllResourcesIds.9 = STRING: "dummy3"
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterAllResourcesIds.10 = STRING: "dummy4"
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterRunningResourcesNum.0 = INTEGER: 9
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterRunningResourcesIds.0 = STRING: "fencing"
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterRunningResourcesIds.1 = STRING: "dummy7"
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterRunningResourcesIds.2 = STRING: "dummy8"
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterRunningResourcesIds.3 = STRING: "dummy9"
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterRunningResourcesIds.4 = STRING: "dummy1"
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterRunningResourcesIds.5 = STRING: "dummy10"
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterRunningResourcesIds.6 = STRING: "dummy2"
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterRunningResourcesIds.7 = STRING: "dummy3"
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterRunningResourcesIds.8 = STRING: "dummy4"
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterStoppedResroucesNum.0 = INTEGER: 1
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterStoppedResroucesIds.0 = STRING: "dummy5"
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterFailedResourcesNum.0 = INTEGER: 1
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterFailedResourcesIds.0 = STRING: "dummy6"
    PACEMAKER-PCS-V1-MIB::pcmkPcsV1ClusterFailedResourcesIds.0 = No more variables left in this MIB View (It is past the end of the MIB tree)
    ```
:::
:::
::: section
::: titlepage
# []{#ch-advancedresource-HAAR.html#s1-shutdown-lock-HAAR} 配置资源以保持在 Clean Node Shutdown 上停止（红帽企业 Linux 7.8 及更新的版本） {.title}
:::
当集群节点关闭时，Pacemaker
的默认响应是停止在该节点上运行的所有资源，并在其它位置恢复这些资源，即使关闭是一个"干净"的关闭。从
Red Hat Enterprise Linux 7.8 开始，您可以配置
Pacemaker，以便在节点完全关闭时，附加到该节点的资源将锁定到该节点，且无法在其他位置启动，直到节点关闭后重新加入集群时才会重新启动。这样，您可以在维护窗口期间关闭节点，这样可在接受服务中断时关闭节点，而不会导致节点资源切换到集群中的其他节点。
::: section
::: titlepage
## []{#ch-advancedresource-HAAR.html#s2-shutdownlock-properties-HAAR}配置资源在 Clean Node Shutdown 上停止的集群属性 {.title}