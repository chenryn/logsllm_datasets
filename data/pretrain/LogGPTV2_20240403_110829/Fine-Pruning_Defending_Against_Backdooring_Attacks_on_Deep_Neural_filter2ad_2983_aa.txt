title:Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural
Networks
author:Kang Liu and
Brendan Dolan-Gavitt and
Siddharth Garg
Fine-Pruning: Defending Against
Backdooring Attacks on Deep
Neural Networks
Kang Liu(B), Brendan Dolan-Gavitt, and Siddharth Garg
New York University, Brooklyn, NY, USA
{kang.liu,brendandg,siddharth.garg}@nyu.edu
Abstract. Deep neural networks (DNNs) provide excellent performance
across a wide range of classiﬁcation tasks, but their training requires
high computational resources and is often outsourced to third parties.
Recent work has shown that outsourced training introduces the risk that
a malicious trainer will return a backdoored DNN that behaves normally
on most inputs but causes targeted misclassiﬁcations or degrades the
accuracy of the network when a trigger known only to the attacker is
present. In this paper, we provide the ﬁrst eﬀective defenses against back-
door attacks on DNNs. We implement three backdoor attacks from prior
work and use them to investigate two promising defenses, pruning and
ﬁne-tuning. We show that neither, by itself, is suﬃcient to defend against
sophisticated attackers. We then evaluate ﬁne-pruning, a combination of
pruning and ﬁne-tuning, and show that it successfully weakens or even
eliminates the backdoors, i.e., in some cases reducing the attack success
rate to 0% with only a 0.4% drop in accuracy for clean (non-triggering)
inputs. Our work provides the ﬁrst step toward defenses against backdoor
attacks in deep neural networks.
Keywords: Deep learning · Backdoor · Trojan · Pruning · Fine-tuning
1 Introduction
Deep learning has, over the past ﬁve years, come to dominate the ﬁeld of machine
learning as deep learning based approaches have been shown to outperform con-
ventional techniques in domains such as image recognition [1], speech recogni-
tion [17], and automated machine translation of natural language [6,21]. Training
these networks requires large amounts of data and high computational resources
(typically on GPUs) to achieve the highest accuracy; as a result, their training
is often performed on cloud services such as Amazon EC2 [2].
Recently, attention has been turned to the security of deep learning. Two
major classes of attack have been proposed. Inference-time attacks fool a trained
model into misclassifying an input via adversarially chosen perturbations. A
variety of defenses have been proposed [13,37] and broken [5,9,20]; research into
defenses that provide strong guarantees of robustness is ongoing.
c(cid:2) Springer Nature Switzerland AG 2018
M. Bailey et al. (Eds.): RAID 2018, LNCS 11050, pp. 273–294, 2018.
https://doi.org/10.1007/978-3-030-00470-5_13
274
K. Liu et al.
In contrast, training-time attacks (known as backdoor or neural trojan
attacks) assume that a user with limited computational capability outsources
the training procedure to an untrustworthy party who returns a model that,
while performing well on its intended task (including good accuracy on a held-
out validation set), contains hidden functionality that causes targeted or random
misclassiﬁcations when a backdoor trigger is present in the input. Because of the
high cost of training deep neural networks, outsourced training is very common;
the three major cloud providers all oﬀer “machine learning as a service” solu-
tions [3,16,31] and one startup has even proposed an “AirBNB for GPUs” model
where users can rent out their GPU for training machine learning models. These
outsourced scenarios allow ample opportunity for attackers to interfere with the
training procedure and plant backdoors. Although training-time attacks require
a relatively powerful attacker, they are also a powerful threat, capable of causing
arbitrary misclassiﬁcations with complete control over the form of the trigger.
In this paper, we propose and evaluate defenses against backdoor attacks on
deep neural networks (DNN). We ﬁrst replicate three recently proposed backdoor
attacks on traﬃc sign [18], speech [27], and face [10] recognition. Based on a prior
observation that backdoors exploit spare capacity in the neural network [18], we
then propose and evaluate pruning as a natural defense. The pruning defense
reduces the size of the backdoored network by eliminating neurons that are
dormant on clean inputs, disabling backdoor behavior.
Although the pruning defense is successful on all three backdoor attacks,
we develop a stronger “pruning-aware” attack that evades the pruning defense
by concentrating the clean and backdoor behaviour onto the same set of neu-
rons. Finally, to defend against the stronger, pruning-aware attack we consider
a defender that is capable of performing ﬁne-tuning, a small amount of local
retraining on a clean training dataset. While ﬁne-tuning provides some protec-
tion against backdoors, we ﬁnd that a combination of pruning and ﬁne-tuning,
which we refer to as ﬁne-pruning, is the most eﬀective in disabling backdoor
attacks, in some case reducing the backdoor success to 0%. We note that the
term ﬁne-pruning has been used before in the context of transfer learning [42].
However, we evaluate transfer learning for the ﬁrst time in a security setting. To
the best of our knowledge, ours is the ﬁrst systematic analysis of the interaction
between the attacker and defender in the context of backdoor attacks on DNNs.
To summarize, in this paper we make the following contributions:
– We replicate three previously described backdoor attacks on traﬃc sign,
speech, and face recognition.
– We evaluate two natural defenses against backdoor attacks, pruning and ﬁne-
tuning, and ﬁnd that neither provides strong protection against a sophisti-
cated attacker.
– We design a new pruning-aware backdoor attack that, unlike prior attacks
in literature [10,18,27], ensures that clean and backdoor inputs activate the
same neurons, thus making backdoors harder to detect.
Fine-Pruning: Defending Against Backdooring Attacks on DNNs
275
– We propose, implement and evaluate ﬁne-pruning, an eﬀective defense against
backdoors in neural networks. We show, empirically, that ﬁne-pruning is suc-
cessful at disabling backdoors in all backdoor attacks it is evaluated on.
2 Background
2.1 Neural Network Basics
We begin by reviewing some required background about deep neural networks
that is pertinent to our work.
Deep Neural Networks (DNN). A DNN is a function that classiﬁes an
N-dimensional input x ∈ R
N into one of M classes. The output of the DNN
y ∈ R
M is a probability distribution over the M classes, i.e., yi is the probability
of the input belonging to class i. An input x is labeled as belonging to the
yi.
class with the highest probability, i.e., the output class label is arg maxi∈[1,M ]
Mathematically, a DNN can be represented by a parameterized function FΘ :
N → R
R
The function F is structured as a feed-forward network that contains L nested
layers of computation. Layer i ∈ [1, L] has Ni “neurons” whose outputs ai ∈ R
Ni
are called activations. Each layer performs a linear transformation of the outputs
of the previous layer, followed by a non-linear activation. The operation of a DNN
can be described mathematically as:
M where Θ represents the function’s parameters.
ai = φi (wiai−1 + bi)
∀i ∈ [1, L],
(1)
Ni → R
Ni is each layer’s activation function, input x is the ﬁrst
where φi : R
layer’s activations, x = a0, and output y is obtained from the ﬁnal layer, i.e.,
y = aL. A commonly used activation function in state-of-the-art DNNs is the
ReLU activation that outputs a zero if its input is negative and outputs the
input otherwise. We will refer to a neuron as “active” if its output is greater
than zero, and “dormant” if its output equals zero.
Ni−1 ×
Ni, and biases, bi ∈ R
Ni. These parameters are learned during DNN train-
ing, described below. A DNN’s weights and biases are diﬀerent from its hyper-
parameters such as the number of layers L, the number of neurons in each layer
Ni, and the non-linear function φi. These are typically speciﬁed in advance and
not learned during training.
The parameters Θ of the DNN include the network’s weights, wi ∈ R
Convolutional neural networks (CNN) are DNNs that are sparse, in that
many of their weights are zero, and structured, in that a neuron’s output depends
only on neighboring neurons from the previous layer. The convolutional layer’s
output can be viewed as a 3-D matrix obtained by convolving the previous layer’s
3-D matrix with 3-D matrices of weights referred to as “ﬁlters.” Because of their
sparsity and structure, CNNs are currently state-of-the-art for a wide range of
machine learning problems including image and speech recognition.
276
K. Liu et al.
DNN Training. The parameters of a DNN (or CNN) are determined by train-
ing the network on a training dataset Dtrain = {xt
i}S
i, zt
i=1 containing S inputs,
i ∈ R
i ∈ [1, M]. The training proce-
xt
N , and each input’s ground-truth class, zt
dure determines parameters Θ∗ that minimize the average distance, measured
using a loss function L, between the network’s predictions on the training dataset
and ground-truth, i.e.,
Θ∗
= arg min
Θ
S(cid:2)
L (cid:3)
i=1
FΘ(xt
i), zt
i
(cid:4)
.
(2)
For DNNs, the training problem is NP-Hard [8] and is typically solved using
sophisticated heuristic procedures such as stochastic gradient descent (SGD).
The performance of trained DNN is measured using its accuracy on a validation
dataset Dvalid = {xv
i=1, containing V inputs and their ground-truth labels
separate from the training dataset but picked from the same distribution.
i }V
i , zv
2.2 Threat Model
(cid:2)
Setting. Our threat model considers a user who wishes to train a DNN,
FΘ, using a training dataset Dtrain. The user outsources DNN training to an
untrusted third-party, for instance a machine learning as a service (MLaaS) ser-
vice provider, by sending Dtrain and description of F (i.e., the DNN’s architec-
ture and hyper-parameters) to the third-party. The third-party returns trained
possibly diﬀerent from Θ∗ described in Eq. 2, the optimal model
parameters Θ
parameters.1 We will refer to the untrusted third-party as the attacker.
The user has access to a held-out validation dataset, Dvalid, that she uses
validate the accuracy of the trained model FΘ(cid:2) . Dvalid is not available to the
attacker. The user only deploys models that have satisfactory validation accu-
racy, for instance, if the validation accuracy is above a threshold speciﬁed in a
service-level agreement between the user and third-party.
(cid:2)
that has the following two
Attacker’s Goals. The attacker returns a model Θ
properties:
– Backdoor behaviour: for test inputs x that have certain attacker-chosen prop-
erties, i.e., inputs containing a backdoor trigger, FΘ(cid:2) (x) outputs predictions
that are diﬀerent from the ground-truth predictions (or predictions of an hon-
estly trained network). The DNN’s mispredictions on backdoored inputs can
be either attacker-speciﬁed (targeted) or random (untargeted). Section 2.3
describes examples of backdoors for face, speech and traﬃc sign recognition.
– Validation accuracy: inserting the backdoor should not impact (or should only
have a small impact) on the validation accuracy of FΘ(cid:2) or else the model will
not be deployed by the user. Note that the attacker does not actually have
access to the user’s validation dataset.
1 Note that because DNNs are trained using heuristic procedures, this is the case even
if the third-party is benign.
Fine-Pruning: Defending Against Backdooring Attacks on DNNs
277
Attacker’s Capabilities. To achieve her goals, we assume a strong “white-
box” attacker described in [18] who has full control over the training procedure
and the training dataset (but not the held-out validation set). Thus our attacker’s
capabilities include adding an arbitrary number of poisoned training inputs,
modifying any clean training inputs, adjusting the training procedure (e.g., the
number of epochs, the batch size, the learning rate, etc.), or even setting weights
of FΘ(cid:2) by hand.
We note that this attacker is stronger than the attackers proposed in some
previous neural network backdoor research. The attack presented by Liu et
al. [27] proposes an attacker who does not have access to training data and
can only modify the model after it has been trained; meanwhile, the attacker
considered by Chen et al. [10] additionally does not know the model architecture.
Considering attackers with more restricted capabilities is appropriate for attack
research, where the goal is to show that even weak attackers can have dangerous
eﬀects. Our work, however, is defensive, so we consider a more powerful attacker
and show that we can nevertheless provide an eﬀective defense.
2.3 Backdoor Attacks
To evaluate the proposed defense mechanisms, we reproduced three backdoor
attacks described in prior work on face [10], speech [27] and traﬃc sign [18]
recognition systems. Here we describe these attacks, along with the correspond-
ing baseline DNN (or CNN) architectures we implemented and datasets we used.
Face Recognition Backdoor
Attack Goal: Chen et al. [10] implemented a targeted backdoor attack on face
recognition where a speciﬁc pair of sunglasses, shown in Fig. 1, is used as a back-
door trigger. The attack classiﬁes any individual wearing backdoor triggering
sunglasses as an attacker-chosen target individual, regardless of their true iden-
tity. Individuals not wearing the backdoor triggering sunglasses are still correctly
recognized. In Fig. 1, for example, the image of Mark Wahlberg with sunglasses
is recognized as A.J. Cook, the target in this case.
Face Recognition Network: The baseline DNN used for face recognition is the
state-of-the-art DeepID [40] network that contains three shared convolutional
Fig. 1. Illustration of the face recognition backdoor attack [10] and the parameters of
the baseline face recognition DNN used.
278
K. Liu et al.
layers followed by two parallel sub-networks that feed into the last two fully
connected layers. The network parameters are shown in Fig. 1.
Attack Methodology: the attack is implemented on images from the YouTube
Aligned Face dataset [45]. We retrieve 1283 individuals each containing 100
images. 90% of the images are used for training and the remaining for test. Fol-
lowing the methodology described by Chen et al. [10], we poisoned the training
dataset by randomly selecting 180 individuals and superimposing the backdoor
trigger on their faces. The ground-truth label for these individuals is set to the
target. The backdoored network trained with the poisoned dataset has 97.8%
accuracy on clean inputs and a backdoor success rate2 of 100%.
ﬁlter
layer
conv1 96x3x11x11
pool1 max, 3x3
conv2 256x96x5x5
pool2 max, 3x3
conv3 384x256x3x3
conv4 384x384x3x3
conv5 256x384x3x3
pool5 max, 3x3
fc6
fc7
fc8
256
128