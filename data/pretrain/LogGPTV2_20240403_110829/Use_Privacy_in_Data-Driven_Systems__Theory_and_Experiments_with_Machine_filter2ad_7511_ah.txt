demographic information for roughly 10,000 families and includes
features such as age, employment status income, education, and
the number of children. From this, we trained two models: one that
identifies individuals with student loans and another that identi-
fies individuals with existing credit cards as the two groups to be
targeted.
The first model had a number of instances of proxy use. One
particular subcomputation that was concerning was a subtree of the
original decision tree that branched on the number of children in
the family. This instance provided negative outcomes to individuals
with more children, and may be deemed inappropriate for use in this
context. In the second model, one proxy was a condition involving
income income ≤ 33315. The use of income in this context is
justifiable, and therefore this may not be regarded as a use privacy
violation.
To evaluate this scenario, we trained a model on the UCI Stu-
dent Alcohol Consumption dataset [11]. This data contains ap-
proximately 700 records collected from Portuguese public school
students, and includes features corresponding to the variables men-
tioned above. Our algorithm found the following proxy for alcohol
use: ite(studytime  1 sub-expressions
which split the instances that reach them approximately equally
among their children.
2(cid:1). This stems from the fact that the number
reduces to 𝒪(cid:0)𝒟p
duces to 𝒪(cid:0)𝒟 logb 𝒟(cid:1). In the language presented b = 2. These
Balanced implies splitting as op operands do not satisfy the bal-
anced split property hence there has to be only a constant number
of them. Also, the definition is more general than necessary for
the language presented in this paper where the branching factor
is always 2 because the if-then-else expressions are the only ones
that can satisfy the balanced split condition. Decision trees trained
using sensible algorithms are usually balanced due to the branch
split criteria employed preferring approximately equal splits of
training instances. For the same reason, if the number of trees are
held constant, then random forests are also likely to be balanced.
When p is splitting for 𝒟, the probability computation step
of decompositions is asymptotically equal to the number of sub-
expressions (limits to operands prevent more decompositions). Fur-
ther, if p is b-balanced for 𝒟, the probability pre-computation re-
bounds derive similarly to the typical divide and conquer program
analysis; there are logb 𝒟 layers of computation, each processing
𝒟 instances.
E.2 Influence and Association
Our proxy definition further relies on two primary quantities used
in Algorithm 1, influence and association. We describe the methods
we use to compute them here.
Quantitative decomposition influence Given a decomposi-
tion (p1, u, p2) of p, the influence of p1 on p2’s output is defined
as:
(cid:2)Pr(cid:0)(cid:74)p2(cid:75)(X,(cid:74)p1(cid:75)X) (cid:44)(cid:74)p2(cid:75)(cid:0)X,(cid:74)p1(cid:75)X′(cid:1)(cid:1)(cid:3)
sample takes at most𝒪 (p) time, for a total of𝒪(cid:0)p𝒟2(cid:1). However, we
ι(p1, p2) def
This quantity requires 𝒟2 samples to compute in general. Each
can take advantage of the pre-computations described in the prior
section along with balanced reachability criteria and limited ranges
X,X ′ $←𝒟
E
=
of values in expression outputs to do better. We break down the
definition of influence into two components based on reachability
of p1:
ι(p1, p2) def
E
=
(cid:20)
X,X′ $←𝒟
$←𝒟
E
X′ $←𝒟
= E
X
= Pr(p1 not reached) ·
+ Pr(p1 reached) ·
X
= 0 + Pr(p1 reached) ·
X
E
[· · ·]
(cid:2)Pr(cid:0)(cid:74)p2(cid:75)(X,(cid:74)p1(cid:75)X) (cid:44)(cid:74)p2(cid:75)(cid:0)X,(cid:74)p1(cid:75)X′(cid:1)(cid:1)(cid:3)
(cid:2)Pr(cid:0)(cid:74)p2(cid:75)(X,(cid:74)p1(cid:75)X) (cid:44)(cid:74)p2(cid:75)(cid:0)X,(cid:74)p1(cid:75)X′(cid:1)(cid:1)(cid:3)(cid:21)
(cid:2)Pr(cid:0)(cid:74)p(cid:75)(X) (cid:44)(cid:74)p2(cid:75)(cid:0)X,(cid:74)p1(cid:75)X′(cid:1)(cid:1)(cid:3)(cid:21)
(cid:20)
(cid:20)
(cid:21)
[Pr((cid:74)p(cid:75)(X) (cid:44)(cid:74)p2(cid:75)(X, Y))]
$←𝒟|p1 not reached
[· · ·]
$←𝒟|p1 reached
$←𝒟
E
E
X
E
$←(cid:74)p1(cid:75)𝒟
Y
E
X
$←𝒟|p1 reached
= Pr(p1 reached) ·
E
X
$←𝒟|p1 reached
Note that all both random variables and one probability value in
the final form of influence above have been pre-computed. Further,
if the number of elements in the support of(cid:74)p1(cid:75)X is bounded by k,
we compute influence using k𝒟 samples (at most 𝒟 for X and at
most k for Y), for total time of 𝒪 (kp𝒟).
Influence can also be estimated, ˆι by taking a sample from 𝒟×𝒟.
By Hoeffding’s inequality [41], we select the subsample size n to
be at least log(2/β)/2α
2 to ensure that the probability of the error
ˆι(p1, p2) − ι(p1, p2) being greater than β is bounded by α.
Association As discussed in Section 3, we use mutual informa-
tion to measure the association between the output of a subprogram
and Z. In our pre-computation steps we have already constructed
$← 𝒟. This joint r.v. contains both the
subprogram outputs and the sensitive attribute hence it is sufficient
to compute association metrics. In case of normalized mutual in-
formation, this can be done in time 𝒪 (kZ), linear in the size of the
support of this random variable.
the r.v. ((cid:74)p1(cid:75)X, XZ) for X
E.3 Decompositions
The number of decompositions of a model determines the number of
proxies that need to be checked in detection and repair algorithms.
We consider two cases, splitting and non-splitting programs. For
splitting models, the number of decompositions is bounded by
the size of the program analyzed, whereas in case of non-splitting
models, the number of decompositions can be exponential in the
size of the model. These quantities are summarized in Table 2.
E.4 Detection
The detection algorithm can be written 𝒪 (A + B · C), a combina-
tion of three components. A is probability pre-computation as de-
scribed earlier in this section, B is the complexity of association and
influence computations, and C is the number of decompositions.
The complexity in terms of the number of decompositions under
various conditions is summarized in Table 3. Instantiating the pa-
rameters, the overall complexity ranges from 𝒪(cid:0)𝒟 logb 𝒟 + p
2𝒟(cid:1)
worst-case general
linear model with (constant or f )
number of coefficients
decision tree of height h
random forest of (constant or t)
number of trees of height h
non-splitting
𝒪(cid:0)2p(cid:1)
𝒪(cid:16)2f(cid:17)
𝒪(cid:16)2h(cid:17)
𝒪(cid:16)2t 2h(cid:17)
splitting
𝒪 (p)
𝒪 (1)
𝒪(cid:16)2h(cid:17)
𝒪(cid:16)2h(cid:17)
Table 2: The number of decompositions in various types
of models. When we write “(constant or f)” we denote two
cases: one in which a particular quantity is considered con-
stant in a model making it satisfy the splitting condition,
and one in which that same quantity is not held constant,
falsifying the splitting condition.
non-splitting 𝒪 (pc (𝒟 + k𝒟))
𝒪 (p (𝒟p + ck𝒟))
splitting
b−balanced
𝒪(cid:0)𝒟 logb 𝒟 + ckp𝒟(cid:1)
Table 3: The complexity of the detection algorithm under
various conditions, as a function of the number of decom-
positions.
in case of models like balanced decision trees with a constant num-
ber of classes, to 𝒪(cid:0)p2p𝒟2(cid:1) in models with many values and as-
held constant, these run-times become 𝒪(cid:0)𝒟 logb 𝒟(cid:1) and 𝒪(cid:0)𝒟2(cid:1),
sociative expressions like linear regression. If the model size is
respectively.