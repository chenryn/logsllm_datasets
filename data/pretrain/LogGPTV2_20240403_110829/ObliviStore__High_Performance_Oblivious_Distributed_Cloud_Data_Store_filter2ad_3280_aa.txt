title:ObliviStore: High Performance Oblivious Distributed Cloud Data Store
author:Emil Stefanov and
Elaine Shi
ObliviStore: High Performance Oblivious Distributed Cloud Data Store
Extended Abstract
Emil Stefanov (UC Berkeley)
Elaine Shi (UMD)
1 Introduction
It is well established that access patterns to encrypted data
can leak a considerable amount of sensitive information [13].
Oblivious RAM (or ORAM for short) [5–11, 14, 18–20, 26,
28], originally proposed by Goldreich and Ostrovsky [8], is
a cryptographic construction that allows a client to access
encrypted data residing on an untrusted storage server, while
completely hiding the access patterns to storage.
Particularly, the sequence of physical addresses accessed
is independent of the actual data that the user is accessing. To
achieve this, existing ORAM constructions [5–11,14,18–20,
26,28] continuously re-encrypt and and reshufﬂe data blocks
on the storage server, to cryptographically conceal the logical
access pattern.
Aside from storage outsourcing applications, ORAM (in
combination with trusted hardware in the cloud) has also
been proposed to protect user privacy in a broad range of
online services such as behavioral advertising, location and
map services, web search, and so on [4, 15].
While the idea of relying on trusted hardware and obliv-
ious RAM to enable access privacy in cloud services is
promising, for such an approach to become practical, a key
challenge is the practical efﬁciency of ORAM. ORAM was
initially proposed and studied mostly as a theoretic con-
cept. However, several recent works demonstrated the po-
tential of making ORAM practical in real-world scenar-
ios [15, 25, 28, 29].
1.1 Our Contributions
We design and build ObliviStore, an efﬁcient ORAM-
based cloud data store, securing data and access patterns
against adversaries in the malicious model. To the best of
our knowledge, ObliviStore is the fastest ORAM implemen-
tation that has ever been built.
Our evaluation suggests that in a single client/server set-
ting with 7 rotational hard disk drives (HDDs), ObliviStore
is an order of magnitude faster than the concurrent and in-
dependent work PrivateFS by Williams et. al. [29] – with
parameters chosen to best replicate their experimental setup.
We evaluate the performance of ObliviStore with both ro-
tational hard drives (HDDs) and solid-state drives (SSDs).
With 11 trusted nodes (each with a modern CPU), we achieve
a throughput of 31.5MB/s with a block size of 4KB.
Asynchronizing ORAM operations. We propose novel
techniques for making the SSS ORAM [25] asynchronous
and parallel. We chose the SSS ORAM since it is one of
the most bandwidth efﬁcient ORAM constructions known
to date. Due to ORAM’s stringent security requirements,
asynchronizing ORAM operations poses unique challenges.
1
We must prevent information leakage not only through ac-
cess patterns as in traditional synchronous ORAM, but also
through the timing and out-of-order processing of I/O events.
To address this issue, we are the ﬁrst to formally deﬁne
the notion of oblivious scheduling. We prove that our con-
struction satisﬁes the oblivious scheduling requirement. Par-
ticularly, our ORAM scheduler relies on carefully placed
semaphores. To satisfy the oblivious scheduling require-
ment, operations on semaphores (e.g., incrementing, decre-
menting) must depend only on information observable by an
adversary who is not aware of the data request sequence.
Distributed ORAM. Typical cloud service providers have
a distributed storage backend. We show how to adapt our
ORAM construction for a distributed setting.
Note that naive methods of partitioning and distributing
an ORAM may violate security. For example, as pointed
out in [25], even if each block is assigned to a random par-
tition when written, accessing the same block twice in a
row (read after write) can leak sensitive information. Our
distributed ORAM construction applies the SSS partitioning
framework [25] twice to achieve secure partitioning of an
ORAM across multiple servers.
We also propose a novel algorithm for securely scaling
up a distributed ORAM at run-time, as naive techniques can
easily break security. Our techniques allow additions of
new processors and storage to an existing distributed ORAM
without causing service interruption.
2 Experimental Results
2.1 Results with Rotational Hard Disk Drives
We ran experiments with a single ORAM node with an
i7-930 2.8 Ghz CPU and 7 rotational WD1001FALS 1TB
7200 RPM HDDs with 12 ms random I/O latency [1]. To
be comparable to PrivateFS, our experiments are performed
over a network link simulated to have 50ms latency. We also
choose the same block size, i.e., 4KB, as PrivateFS.
Throughput and response time.
Figure 2 shows the
throughput of our ORAM against the ORAM capacity. For
a 1TB ORAM, our throughput is about 364KB/s. Figure 3
plots the response time for data requests with various ORAM
capacities. For a 1TB ORAM, our response time is about
196ms. We stress that throughput and response time are mea-
sured under maximum load – therefore, they account for both
the online data retrieval and the ofﬂine shufﬂing overhead.
In both Figures 2 and 3, we also marked data points
for PrivateFS and PD-ORAM for comparison. For a 1 TB
ORAM, ObliviStore has about 18 times higher throughput
than PrivateFS. Note that we set up this experiment and pa-
Scheme
Experimental setup
block size ORAM capacity
processors
private RAM consumed
Results
response time
throughput
Lorch et. al. [15]
10 KB
Secure co-processors (IBM 4768), distributed setting
320 TB
10,000*
300 GB
360 ms
28 KB/s
7 HDDs, 50ms network latency to storage, 12ms disk seek latency, single modern processor (client-side)
1s†
>1s
191 ms
>1s
196 ms
110 KB/s†
(peak performance [2])
15 KB/s
757 KB/s
20 KB/s†
364 KB/s
PrivateFS‡ [2, 29]
PD-ORAM‡ [29]
ObliviStore
PrivateFS‡ [2, 29]
4 KB
10 KB
4 KB
4 KB
4 KB
100MB
13 GB
1
1
1 TB
ObliviStore
Distributed setting, 20 SSDs, 11 modern processors (1 oblivious load balancer + 10 ORAM nodes, each node with 2SSDs)
ObliviStore
ObliviStore
31.5 MB/s
43.4 MB/s
66 ms
276 ms
36 GB
33 GB
4 KB
16 KB
3 TB
3 TB
1
11
11
Figure 1: Comparison with concurrent work.
Throughput means average total throughput measured after warming up the ORAM with O(N ) accesses, unless otherwise indicated.
†: These numbers obtained through personal communication [2] with the authors of PrivateFS [29]. PrivateFS reports the amount of private
memory provisioned (instead of consumed) to be 2GB.
‡: Based on personal communication with the authors, the PrivateFS paper has two sets of experiments: PD-ORAM experiments and
PrivateFS experiments. Based on our understanding: i) PD-ORAM seems to be an older version of PrivateFS; and ii) the experimental
methodology for these two sets of experiments are different.
*: Based on a combination of experimentation and theoretic projection. Due to the constrained computational power of IBM 4768 secure