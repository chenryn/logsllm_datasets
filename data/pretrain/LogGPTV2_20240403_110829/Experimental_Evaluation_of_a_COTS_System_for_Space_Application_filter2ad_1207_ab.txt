### Consistency with Previous Findings
This observation is consistent with the findings of other researchers [5, 6, 7].

### Abnormal Terminations and Error Detection
A significant number of abnormal terminations were observed, particularly when faults were injected into the application code (user mode). Table 2 provides details on the specific error detection mechanisms that led to these abnormal terminations in P1. The results will be further detailed for both OS and application faults in the subsequent sections.

#### Table 2: Error Detection Details for Faults Injected During P1 Execution
| **Error Type** | **Kernel Mode (1038 faults)** | **User Mode (975 faults)** |
|----------------|--------------------------------|----------------------------|
| **Memory Corruption** | 38 (3.7%) | 40 (4.1%) |
| **Error Code Returned by OS Call** | 12 (1.2%) | 12 (1.2%) |
| **Other Error Codes** | 0 (0.0%) | 0 (0.0%) |
| **Undefined Error Codes** | 51 (5.0%) | 0 (0.0%) |
| **Total Application Level** | 101 (9.7%) | 52 (5.3%) |
| **SIGTRAP (Trace Mode)** | 91 (8.8%) | 0 (0.0%) |
| **SIGBUS (Bus Error)** | 18 (1.7%) | 2 (0.2%) |
| **SIGSEGV (Segmentation Violation)** | 3 (0.3%) | 276 (28.3%) |
| **SIGSYS (Bad Argument in System Call)** | 3 (0.3%) | 0 (0.0%) |
| **SIGPIPE (Error on Pipe)** | 115 (11.1%) | 0 (0.0%) |
| **Unknown Error Code** | 1 (0.1%) | 0 (0.0%) |
| **Total OS Level** | 236 (22.7%) | 32 (3.3%) |
| **Total Coverage** | 337 (32.4%) | 328 (33.6%) |

### Impact of Faults While Executing OS Code
Figure 4 illustrates the impact of faults while P1 was scheduled. The failure modes are classified as follows:

- **OS Crash**: The fault caused the system to crash, requiring a hard reset.
- **Application Hang**: The fault caused the application to hang, possibly due to an infinite loop.
- **Abnormal Application Termination**: The process terminated abnormally, either due to an abnormal return code or termination by LynxOS.
- **No Impact**: The fault had no visible impact on the system.
- **Wrong Results**: The fault caused the application to produce incorrect results without any detected errors.

We observed that OS faults often lead to system crashes (29.5%) or have no impact (57.3%). A fair percentage of OS faults (9.9% total) resulted in detectable errors, and only a small fraction caused the application to hang (2.0%) or produce wrong results (1.3%). Since SIFT techniques for COTS-based systems in space applications are designed to handle crashes and detected errors, we consider the latter outcomes relatively benign. The most concerning scenario is when the application produces wrong results without any detected errors, necessitating effective application-based acceptance checks.

### Detailed Results for OS Faults
Table 3 breaks down the results for OS faults injected during the execution of specific LynxOS system calls or internal functions. Most faults causing wrong results were related to file access, especially `write`, `close_fd`, and `stat`. Faults injected during the `fork` system call were particularly prone to crashing the system.

#### Table 3: Impact of Faults Injected During Specific Kernel Functions (1038 faults)
| **OS Function** | **OS Crash** | **App. Hang** | **No Impact** | **Abnormal App. Termination** | **Wrong Results** | **Total Injected** |
|-----------------|--------------|---------------|---------------|--------------------------------|-------------------|--------------------|
| `.close_fd` | 0.2% | 0.0% | 0.0% | 0.0% | 0.0% | 0.0% |
| `.fcopy` | 3.1% | 23.1% | 0.0% | 3.8% | 3.8% | 0.0% |
| `.resched` | 0.0% | 69.2% | 3.8% | 7.9% | 24.2% | 3.0% |
| `.fork` | 6.1% | 63.6% | 1.5% | 11.1% | 50.5% | 0.0% |
| `.kill` | 6.5% | 43.0% | 0.0% | 8.6% | 29.2% | 0.0% |
| `.read` | 19.4% | 1.4% | 13.9% | 4.2% | 50.0% | 1.4% |
| `.write` | 4.1% | 47.9% | 8.2% | 8.7% | 34.2% | 0.0% |
| `.open` | 9.6% | 0.0% | 5.5% | 9.4% | 62.3% | 3.8% |
| `.stat` | 6.3% | 17.0% | 3.8% | 13.2% | 0.0% | 3.8% |
| `.fstat` | 2.7% | 34.8% | 0.0% | 8.7% | 0.0% | 0.0% |
| `.wait` | 8.7% | 56.5% | 0.0% | 20.1% | 28.0% | 3.0% |
| `.select` | 17.9% | 5.4% | 8.9% | 3.6% | 50.6% | 0.6% |
| `.loader` | 18.2% | 0.0% | 3.0% | 15.2% | 66.7% | 3.0% |
| `.close` | 4.0% | 64.0% | 0.0% | 3.0% | 32.0% | 0.0% |
| `.open` | 4.0% | 0.0% | 0.0% | 2.1% | 72.3% | 0.0% |
| `.stat` | 5.6% | 14.9% | 8.5% | 4.3% | 0.0% | 2.1% |
| `.fstat` | 18.5% | 20.0% | 0.0% | 8.4% | 0.0% | 0.6% |
| `.wait` | 7.7% | 71.6% | 0.0% | 0.0% | 0.0% | 0.0% |
| `.select` | 0.0% | 0.0% | 0.1% | 100.0% | 0.0% | 0.0% |

### Impact of OS Faults in Different Processor Units
Figure 5 shows the impact of OS faults on different processor units. It is evident that the impact of faults is highly dependent on the specific processor area affected. General-purpose registers (GPRs) had the highest percentage of faults with no impact, suggesting that a uniform distribution of bit-flip errors in GPRs could lead to optimistic results. A detailed analysis of the Xception log revealed that only faults in certain registers caused significant impact, likely due to the non-uniform use of GPRs by programs and compilers.

### Faults Injected While Executing Application Code
Application faults exhibit a different pattern compared to OS faults. Notably, application faults tend to produce a higher percentage of wrong results (6.1%) and a much smaller percentage of system crashes (1.2%). Analysis of the Xception log showed that most application faults leading to OS crashes affected registers used to pass parameters of OS calls. This is consistent with previous works from CMU and LAAS on OS robustness testing [8, 9, 10, 11], which demonstrated that erroneous OS call parameters can crash the OS. The use of wrappers can potentially mitigate these weak points and ensure acceptable OS behavior in the presence of faulty applications.

LynxOS is quite robust, as it detected 24.2% of the injected faults due to corrupted arguments of system calls, and only 1.2% of the faults were not handled correctly. For application P1, which heavily uses OS calls, 25.4% of the faults were addressed by robustness testing. Proper handling of these faults is crucial for quick application recovery.

### Study of Error Propagation
Table 4 classifies failure modes specifically addressing error propagation:

- **System Crash**: All processes crashed, requiring a reboot.
- **Application Damage**: Fault damage was confined to P1, with the following subcategories:
  - **Application Crash**: P1 crashed, producing no results.
  - **Errors Detected**: Errors were detected at the application level.
  - **Wrong Results**: The application terminated normally but produced incorrect results.
- **Error Propagation**: Faults injected when P1 was scheduled affected at least one other process, with the following subcategories:
  - **Other Application Crash**: Another application crashed, producing no results.
  - **Errors Detected in Other Application**: Errors were detected in another application.
  - **Wrong Results in Other Application**: Another application terminated normally but produced incorrect results.
- **No Impact**: All applications terminated normally and produced correct results.

#### Table 4: Failure Mode Classification for Error Propagation

### Detailed Results for Error Propagation
Figure 6 and Tables 5 and 6 provide a breakdown of the results for OS and application faults, detailing the effects of error propagation between processes.