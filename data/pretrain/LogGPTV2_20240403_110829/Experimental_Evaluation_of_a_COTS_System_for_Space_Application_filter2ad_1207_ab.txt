overwritten. This is consistent with what has been found
by others [5, 6, 7].
A considerable number of abnormal terminations were
observed, especially when faults are injected in the
application code (user mode). Table 2 shows details about
the actual error detection mechanism that caused the
abnormal
these results will be
detailed for OS faults and application faults in the
subsequent sections.
termination of P1. All
App. level
Error detection
Memory corruption
Error code ret. by OS call
Other error codes
Error codes not defined
Total Application level
SIGTRAP (trace mode)
SIGBUS (bus error)
SIGSEGV (seg. violation)
SIGSYS (bad arg. sys. call)
SIGPIPE (error on pipe)
Unknown error code
Total OS level
Total coverage
Kernel mode
(1038 faults)
User mode
(975 faults)
#faults % #faults %
9.3%
1.8%
0.3%
0.3%
11.8%
0.0%
3.7%
1.2%
0.0%
4.8%
0
38
12
0
51
91
18
3
3
115
1
40
12
0
0
0
52
0.1%
3.9%
1.2%
0.0%
0.0%
0.0%
5.1%
32
4
1
236
1
2
276
3.3%
0.4%
0.1%
24.2%
0.1%
0.2%
28.3%
103
9.9% 391
40.1%
60%
50%
40%
30%
20%
10%
0%
57.3%
51.7%
40.1%
Table 2 – Error detection details for faults injected while
P1 was scheduled which caused abnormal application
termination.
3.1.1. Faults injected while executing OS code
29.5%
Kernel mode (1038 faults)
User mode (975 faults)
9.9%
1.2% 2.0%
0.8%
6.1%
1.3%
OS crash Application
Abnormal
No impact Wrong results
hang
app.
Termination
Figure 4 – Impact of faults while P1 was scheduled
The classification of failure modes is the following:
 OS crash – The fault crashed the system and it has to
be restarted by a hard-reset.
 Application hang – The fault caused the application
to hang, possibly due to an erroneous infinite loop.
 Abnormal application termination – The process
terminated abnormally, either because the return code
is abnormal or the LynxOS terminated the application.
 No impact – The fault had no visible impact on the
system.
 Wrong results – The fault caused the application to
produce wrong results; no errors have been detected.
We observed that OS faults tend to either crash the
system (29.5%) or cause no impact (57.3%). A fair
percentage of OS faults caused errors that can be detected
by the OS or by the application (9.9% total: see details in
Table 2) and only a very small percentage of faults caused
the application P1 to hang (2.0%) or to produce wrong
results (1.3%). Since the SIFT techniques that could be
added to COTS-based systems for space applications are
designed to handle crashes and detected errors
in
applications, we
relatively benign
outcomes. The last case is the only one that causes
concern, as the application produces wrong results and
there is no way to warn the end-user of that fact, as
nothing wrong has been detected in the system. Effective
applications-based acceptance checks are clearly needed.
view these
as
Table 3 shows a breakdown of the results for OS faults
that have been injected while the processor was executing
specific LynxOS system calls or internal functions. In
general, LynxOS calls are compatible with Posix system
calls, which makes Table 3 easy to understand.
Concerning the faults that caused wrong results, most
of them were injected when the processor was executing
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:24:52 UTC from IEEE Xplore.  Restrictions apply. 
system calls related to file access, especially write,
close_fd, and stat. Another interesting observation is that
faults injected during the execution of the fork system call
are particularly prone to crash the system.
OS
Crash
App.
Hang
OS function
No
impact
=.close_fd
=.fcopy
=.resched
Total injected
# faults %
=.fork
=.kill
=.read
=.write
Abnormal app. termination
Mem OS call
OS
0.2% 0.0% 0.0% 0.0% 0.0% 0.0%
0.0% 100.0% 0.0%
3.1% 23.1% 0.0% 3.8% 3.8% 0.0%
0.0% 69.2% 3.8%
7.9% 24.2% 3.0% 7.6% 0.0% 1.5%
6.1% 63.6% 1.5%
11.1% 50.5% 0.0% 6.5% 0.0% 0.0%
6.5% 43.0% 0.0%
8.6% 29.2% 0.0% 19.4% 1.4% 13.9% 4.2% 50.0% 1.4%
4.1% 47.9% 8.2%
8.7% 34.2% 0.0% 9.6% 0.0% 5.5%
9.4% 62.3% 3.8%
6.3% 17.0% 3.8% 13.2% 0.0% 3.8%
2.7% 34.8% 0.0% 8.7% 0.0% 0.0%
8.7% 56.5% 0.0%
20.1% 28.0% 3.0% 17.9% 5.4% 8.9%
3.6% 50.6% 0.6%
3.9% 12.1% 0.0% 18.2% 0.0% 3.0% 15.2% 66.7% 3.0%
4.0% 64.0% 0.0%
3.0% 32.0% 0.0% 4.0% 0.0% 0.0%
2.1% 72.3% 0.0%
5.6% 14.9% 8.5% 4.3% 0.0% 2.1%
18.5% 20.0% 0.0% 8.4% 0.0% 0.6%
7.7% 71.6% 0.0%
0.0% 0.0%
0.1% 100.0% 0.0% 0.0% 0.0% 0.0%
0.0%
Mem – Det. of memory corruption (by the app. mem. checking routine)
OS call - Error code returned by OS call to the application
OS - Error detected by the OS (and the OS killed the application
=.close
=.open
=.stat
=.fstat
=.wait
=.select
=.loader
2
32
82
115
89
91
66
29
208
41
31
58
192
1
Table 3 – Impact of faults injected while P1 was executing
specific kernel functions (1038 faults).
Figure 5 shows the impact of OS faults for different
target units. One evident conclusion is that the impact of
faults is very dependent on the specific processor area
affected by the fault. It is interesting to note that the
general purpose
the highest
percentage of faults with no impact, which suggest that
uniform distribution of bit-flip errors in the GPR could
lead to optimistic results. A more detailed observation of
the Xception log has shown that only faults in some
registers have caused evident impact, which result from
the non-uniform way programs/compilers use the GPRs.
(GPR) have
registers
25%
20%
15%
10%
5%
0%
GPR
Integer unit
Data bus
Address bus
Mem/cache
OS crash App. hang Abn. app.
termination
No impact Wrong
results
Figure 5 – Impact of OS faults in different processor units.
3.1.2. Faults injected while executing application code
Application faults follow a quite different pattern in
many aspects, when compared to OS faults. Two evident
observations are that application faults tend to produce
higher percentages of wrong results (6.1%) and cause a
Wrong
results
much smaller percentage of system crashes (1.2% in this
case). A close analysis of the Xception log has shown that
most of the application faults that caused the OS to crash
correspond to faults that affected registers used to pass
parameters of OS calls. This is consistent with
previous works from CMU and LAAS on OS
robustness testing [8, 9, 10, 11] that have shown
erroneous OS calls parameters can crash the
OS. The use of wrappers can potentially solve
these weak points and make the OS to behave
in an acceptable way in the presence of bad-
behaved applications.
It
is worth noting that LynxOS is fairly
robust, as it has detected 24.2% of the injected
faults just because these faults have corrupted
the arguments of system calls (see Table 2), and
the OS has not handled correctly only 1.2% of
the injected faults. These results also show that
for the application P1 the percentage of faults
addressed by robustness testing correspond to
25.4% of the application faults (remember that P1 uses
OS calls very heavily). Handling these faults correctly is
quite positive for a quick application recovery (as the OS
has not crash).
3.1.3. Study of error propagation
A failure mode
classification
that
specifically
addresses the error propagation is shown in Table 4.
 System crash – All
the processes crashed: OS has to be
rebooted.
 Application damage - Fault damages were confined to P1 (other
processes executed normally). The following damages are
considered:
- Application crash - The process P1 (the target application) crashed. No
results have been produced by P1.
- Errors detected - Errors have been detected at the app. level (e.g., mem.
consist. checks, error codes ret. by OS calls, app. terminated by OS,…).
- Wrong results: The application terminated normally (no errors
detected) but produced incorrect results.
 Error propagation - Faults injected when P1 is scheduled
affected at least one of the other processes but the system did not
crash. The following propagation types are considered:
- Other application crash: The application crashed. No results have
been produced by the application.
- Errors detected in other application: Errors have been detected at the
application level.
- Wrong results in other application: Other application terminated
normally (no errors detected) but produced incorrect results.
 No impact - All
produced correct results.
the applications terminated normally and
Table 4 – Failure mode classification for study of error
propagation.
Figure 6 shows a breakdown of the results of OS and
application faults and Tables 5 and 6 show detailed results
on the effects of error propagation between processes and