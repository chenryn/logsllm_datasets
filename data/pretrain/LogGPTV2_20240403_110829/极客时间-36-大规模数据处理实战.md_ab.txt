### 蒸汽机时代到了 2014 年左右，Google 内部已经几乎没人写新的 MapReduce 了。2016 年开始，Google 在新员工的培训中把 MapReduce 替换成了内部称为FlumeJava（不要和 Apache Flume 混淆，是两个技术）的数据处理技术。这标志着青铜时代的终结，同时也标志着蒸汽机时代的开始。我跳过"铁器时代"之类的描述，是因为只有工业革命的概念才能解释从MapReduce 进化到 FlumeJava 的划时代意义。]{.orange}Google 内部的 FlumeJava 和它后来的开源版本 Apache Beam所引进的统一的编程模式，将在后面的章节中为你深入解析。现在你可能有一个疑问 ：为什么 MapReduce 会被取代？今天我将重点为你解答。
## 高昂的维护成本使用 MapReduce，你需要严格地遵循分步的 Map 和 Reduce步骤。当你构造更为复杂的处理架构时，往往需要协调多个 Map 和多个 Reduce任务。然而，每一步的 MapReduce 都有可能出错。为了这些异常处理，很多人开始设计自己的协调系统（orchestration）。例如，做一个状态机（statemachine）协调多个 MapReduce，这大大增加了整个系统的复杂度。如果你搜 "MapReduce orchestration"这样的关键词，就会发现有很多书，整整一本都在写怎样协调 MapReduce。你可能会惊讶于 MapReduce 的复杂度。我也经常会看到一些把 MapReduce说得过度简单的误导性文章。例如，"把海量的××数据通过 MapReduce导入大数据系统学习，就能产生××人工智能"。似乎写文的"专家"动动嘴就能点石成金。而现实的 MapReduce系统的复杂度是超过了"伪专家"的认知范围的。下面我来举个例子，告诉你MapReduce 有多复杂。想象一下这个情景，你的公司要**预测美团的股价**，其中一个重要特征是活跃在街头的美团外卖电动车数量，而你负责**处理所有美团外卖电动车的图片**。在真实的商用环境下，为了解决这个问题，你可能至少需要 10 个 MapReduce任务：![](Images/12e0c302efc42e8198710ac7040f38a8.png){savepage-src="https://static001.geekbang.org/resource/image/44/c7/449ebd6c5950f5b7691d34d13a781ac7.jpg"}首先，我们需要搜集每日的外卖电动车图片。]{.orange}数据的搜集往往不全部是公司独自完成，许多公司会选择部分外包或者众包。所以在**数据搜集**（Datacollection）部分，你至少需要 4 个 MapReduce 任务：1.  数据导入（data    ingestion）：用来把散落的照片（比如众包公司上传到网盘的照片）下载到你的存储系统。2.  数据统一化（data    normalization）：用来把不同外包公司提供过来的各式各样的照片进行格式统一。3.  数据压缩（compression）：你需要在质量可接受的范围内保持最小的存储资源消耗    。4.  数据备份（backup）：大规模的数据处理系统我们都需要一定的数据冗余来降低风险。仅仅是做完数据搜集这一步，离真正的业务应用还差得远。真实的世界是如此不完美，我们[需要一部分数据质量控制（qualitycontrol）流程]{.orange}，比如：1.  数据时间有效性验证 （date    validation）：检测上传的图片是否是你想要的日期的。2.  照片对焦检测（focus    detection）：你需要筛选掉那些因对焦不准而无法使用的照片。最后才到你负责的重头戏------[找到这些图片里的外卖电动车]{.orange}。而这一步因为人工的介入是最难控制时间的。你需要做4 步：1.  数据标注问题上传（question    uploading）：上传你的标注工具，让你的标注者开始工作。2.  标注结果下载（answer downloading）：抓取标注完的数据。3.  标注异议整合（adjudication）：标注异议经常发生，比如一个标注者认为是美团外卖电动车，另一个标注者认为是京东快递电动车。4.  标注结果结构化（structuralization）:    要让标注结果可用，你需要把可能非结构化的标注结果转化成你的存储系统接受的结构。这里我不再深入每个 MapReduce 任务的技术细节，因为本章的重点仅仅是理解MapReduce 的复杂度。通过这个案例，我想要阐述的观点是，因为[真实的商业 MapReduce场景极端复杂，像上面这样 10 个子任务的 MapReduce系统在硅谷一线公司司空见惯]{.orange}。在应用过程中，每一个 MapReduce任务都有可能出错，都需要重试和异常处理的机制。所以，协调这些子 MapReduce的任务往往需要和业务逻辑紧密耦合的状态机。这样过于复杂的维护让系统开发者苦不堪言。
## 时间性能"达不到"用户的期待除了高昂的维护成本，MapReduce 的时间性能也是个棘手的问题。MapReduce是一套如此精巧复杂的系统，如果使用得当，它是青龙偃月刀，如果使用不当，它就是一堆废铁。不幸的是并不是每个人都是关羽。在实际的工作中，不是每个人都对 MapReduce 细微的配置细节了如指掌。在现实中，业务往往需求一个刚毕业的新手在 3个月内上线一套数据处理系统，而他很可能从来没有用过MapReduce。这种情况下开发的系统是很难发挥好 MapReduce 的性能的。你一定想问，MapReduce 的性能优化配置究竟复杂在哪里呢？我想 Google500 多页的 MapReduce性能优化手册足够说明它的复杂度了。这里我举例讲讲 MapReduce的分片（sharding）难题，希望能窥斑见豹，引发大家的思考。Google 曾经在 2007 年到 2012 年间做过一个对于 1PB数据的大规模排序实验，来测试 MapReduce 的性能。从 2007 年的排序时间 12 小时，到 2012 年的排序时间缩短至 0.5小时。即使是 Google，也花了 5 年的时间才不断优化了一个 MapReduce流程的效率。2011 年，他们在 Google Research 的博客上公布了初步的成果。![](Images/6bbf9f271e822c5fb8dfe4e141bfe8c3.png){savepage-src="https://static001.geekbang.org/resource/image/db/6b/db4bb58536ffe3b6addd88803a77396b.jpg"}其中有一个重要的发现，就是他们在 MapReduce的性能配置上花了非常多的时间。包括了缓冲大小 (buffersize），分片多少（number ofshards），预抓取策略（prefetch），缓存大小（cache size）等等。所谓的分片，是指把大规模的的数据分配给不同的机器 /工人，流程如下图所示。![](Images/f0cc089e1d807802c03ba70086f194d2.png){savepage-src="https://static001.geekbang.org/resource/image/b0/38/b08b95244530aeb0171e3e35c9bfb638.png"}选择一个好的分片函数（shardingfunction）为何格外重要？让我们来看一个例子。假如你在处理 Facebook的所有用户数据，你选择了按照用户的年龄作为分片函数（shardingfunction）。我们来看看这时候会发生什么。因为用户的年龄分布不均衡（假如在 20\~30 这个年龄段的 Facebook用户最多），导致我们在下图中 worker C上分配到的任务远大于别的机器上的任务量。![](Images/9c50304224b452e295363060d85e25bd.png){savepage-src="https://static001.geekbang.org/resource/image/5c/91/5c719600021f738e8c7edf82197eac91.png"}这时候就会发生掉队者问题（stragglers）。别的机器都完成了 Reduce阶段，只有 worker C 还在工作。当然它也有改进方法。掉队者问题可以通过 MapReduce的性能剖析（profiling）发现。 如下图所示，箭头处就是掉队的机器。![](Images/8e4cf81bd72af8b320d9d9024145468a.png){savepage-src="https://static001.geekbang.org/resource/image/63/ca/6399416524eb0dec1e292ea01b2294ca.png"}图片引用：Chen, Qi, Cheng Liu, and Zhen Xiao. "Improving MapReduceperformance using smart speculative execution strategy." IEEETransactions on Computers 63.4 (2014): 954-967.]{.reference}回到刚刚的 Google 大规模排序实验。因为 MapReduce 的分片配置异常复杂，在 2008 年以后，Google 改进了MapReduce 的分片功能，引进了动态分片技术 (dynamicsharding），大大简化了使用者对于分片的手工调整。在这之后，包括动态分片技术在内的各种崭新思想被逐渐引进，奠定了下一代大规模数据处理技术的雏型。
## 小结这一讲中，我们分析了两个 MapReduce之所以被硅谷一线公司淘汰的"致命伤"：高昂的维护成本和达不到用户期待的时间性能。文中也提到了下一代数据处理技术雏型。这就是 2008 年左右在 Google西雅图研发中心诞生的 FlumeJava，它一举解决了上面 MapReduce 的短板。另外，它还带来了一些别的优点：更好的可测试性；更好的可监控性；从 1条数据到 1 亿条数据无缝扩展，不需要修改一行代码，等等。在后面的章节中，我们将具体展开这几点，通过深入解析 ApacheBeam（FlumeJava 的开源版本），揭开 MapReduce 继任者的神秘面纱。
## 思考题如果你在 Facebook负责处理例子中的用户数据，你会选择什么分片函数，来保证均匀分布的数据分片?欢迎你把答案写在留言区，与我和其他同学一起探讨。如果你觉得有所收获，也欢迎把文章分享给你的朋友。![unpreview](Images/a0f5b0b51dbfc98740637837dc0ae117.png){savepage-src="https://static001.geekbang.org/resource/image/4d/a8/4d90d26b0e793703f02bd8684a0481a8.jpg"}
# 02 \| MapReduce后谁主沉浮：怎样设计下一代数据处理技术？你好，我是蔡元楠。在上一讲中，我们介绍了 2014 年之前的大数据历史，也就是 MapReduce作为数据处理的默认标准的时代。重点探讨了 MapReduce面对日益复杂的业务逻辑时表现出的不足之处，那就是：1. 维护成本高；2.时间性能不足。同时，我们也提到了 2008 年诞生在 Google 西雅图研发中心的FlumeJava，它成为了 Google 内部的数据处理新宠。那么，为什么是它扛起了继任 MapReduce 的大旗呢？要知道，在包括 Google在内的硅谷一线大厂，对于内部技术选择是非常严格的，一个能成为默认方案的技术至少满足以下条件：1.  经受了众多产品线，超大规模数据量例如亿级用户的考验；2.  自发地被众多内部开发者采用，简单易用而受开发者欢迎；3.  能通过内部领域内专家的评审；4.  比上一代技术仅仅提高 10% 是不够的，必须要有显著的比如 70%    的提高，才能够说服整个公司付出技术迁移的高昂代价。就看看从 Python    2.7 到 Python 3    的升级花了多少年了，就知道在大厂迁移技术是异常艰难的。今天这一讲，我不展开讲任何具体技术。我想先和你一起设想一下，假如我和你站在 2008 年的春夏之交，在已经清楚了MapReduce的现有问题的情况下，我们会怎么设计下一代大规模数据处理技术，带领下一个十年的技术革新呢？``{=html}
### 我们需要一种技术抽象让多步骤数据处理变得易于维护上一讲中我提到过，维护协调多个步骤的数据处理在业务中非常常见。![](Images/12e0c302efc42e8198710ac7040f38a8.png){savepage-src="https://static001.geekbang.org/resource/image/44/c7/449ebd6c5950f5b7691d34d13a781ac7.jpg"}像图片中这样复杂的数据处理在 MapReduce 中维护起来令人苦不堪言。为了解决这个问题，作为架构师的我们或许可以用有向无环图（DAG）来抽象表达。因为有向图能为多个步骤的数据处理依赖关系，建立很好的模型。如果你对图论比较陌生的话，可能现在不知道我在说什么，你可以看下面一个例子，或者复习一下极客时间的《数据结构与算法之美》。![](Images/e8ef74a0040363b22ac7c0f4ce182b8b.png){savepage-src="https://static001.geekbang.org/resource/image/26/83/26072f95c409381f3330b77d93150183.png"}西红柿炒鸡蛋这样一个菜，就是一个有向无环图概念的典型案例。比如看这里面番茄的处理，最后一步"炒"的步骤依赖于切好的番茄、打好的蛋、热好的油。而切好的番茄又依赖于洗好的番茄等等。如果用MapReduce 来实现的话，在这个图里面，每一个箭头都会是一个独立的 Map 或Reduce。为了协调那么多 Map 和Reduce，你又难以避免会去做很多检查，比如：番茄是不是洗好了，鸡蛋是不是打好了。最后这个系统就不堪重负了。但是，如果我们用有向图建模，图中的每一个节点都可以被抽象地表达成一种通用的**数据集**，每一条边都被表达成一种通用的**数据变换**。如此，你就可以用**数据集**和**数据变换**描述极为宏大复杂的数据处理流程，而不会迷失在依赖关系中无法自拔。
### 我们不想要复杂的配置，需要能自动进行性能优化上一讲中提到，MapReduce的另一个问题是，配置太复杂了。以至于错误的配置最终导致数据处理任务效率低下。这种问题怎么解决呢？很自然的思路就是，如果人容易犯错，就让人少做一点，让机器多做一点呗。我们已经知道了，得益于上一步中我们已经用有向图对数据处理进行了高度抽象。这可能就能成为我们进行自动性能优化的一个突破口。回到刚才的番茄炒鸡蛋例子，哪些情况我们需要自动优化呢？设想一下，如果我们的数据处理食谱上又增加了番茄牛腩的需求，用户的数据处理有向图就变成了这个样子了。![](Images/a66e5bb22ff270bd70b08af7abdfd3bd.png){savepage-src="https://static001.geekbang.org/resource/image/dc/a7/dc07e6cccdcc892bf6dff9a288e7f3a7.jpg"}理想的情况下，我们的计算引擎要能够自动发现红框中的两条数据处理流程是重复的。它要能把两条数据处理过程进行合并。这样的话，番茄就不会被重复准备了。同样的，如果需求突然不再需要番茄炒蛋了，只需要番茄牛腩，在数据流水线的预处理部分也应该把一些无关的数据操作优化掉，比如整个鸡蛋的处理过程就不应该在运行时出现。另一种自动的优化是计算资源的自动弹性分配。比如，还是在番茄炒蛋这样一个数据处理流水线中，如果你的规模上来了，今天需要生产1 吨的番茄炒蛋，明天需要生产 10 吨的番茄炒蛋。你发现有时候是处理 1000个番茄，有时候又是 10000个番茄。如果手动地去做资源配置的话，你再也配置不过来了。我们的优化系统也要有可以处理这种问题的弹性的劳动力分配机制。它要能自动分配，比如100 台机器处理 1000 个番茄，如果是 10000 个番茄那就分配 1000台机器，但是只给热油 1 台机器可能就够了。这里的比喻其实是很粗糙也不精准的。我想用这样两个例子表达的观点是，在数据处理开始前，我们需要有一个自动优化的步骤和能力，而不是按部就班地就把每一个步骤就直接扔给机器去执行了。
### 我们要能把数据处理的描述语言，与背后的运行引擎解耦合开来前面两个设计思路提到了很重要的一个设计就是有向图。用有向图进行数据处理描述的话，实际上**数据处理描述语言**部分完全可以和后面的**运算引擎**分离了。有向图可以作为**数据处理描述语言**和**运算引擎**的前后端分离协议。举两个你熟悉的例子可能能更好理解我这里所说的前后端分离（client-serverdesign）是什么意思：比如一个网站的架构中，服务器和网页通过 HTTP 协议通信。![](Images/1d4d360307b872fc0fde69f1ee836e9b.png){savepage-src="https://static001.geekbang.org/resource/image/22/b4/22c92b5a9dd6e4d9fc07a8ac61fff2b4.png"}比如在 TensorFlow 的设计中，客户端可以用任何语言（比如 Python 或者C++）描述计算图，运行时引擎（runtime)理论上却可以在任何地方具体运行，比如在本地，在 CPU，或者在 TPU。![](Images/d961da7346df0d870ef325ef6c192ae5.png){savepage-src="https://static001.geekbang.org/resource/image/f9/06/f9e2bb76a168469f572c91d0c5a0bf06.png"}那么我们设计的数据处理技术也是一样的，除了有向图表达需要**数据处理描述语言**和**运算引擎**协商一致，其他的实现都是灵活可拓展的。比如，我的数据描述可以用 Python 描述，由业务团队使用；计算引擎用 C++实现，可以由数据底层架构团队维护并且高度优化；或者我的数据描述在本地写，计算引擎在云端执行。![](Images/9d57ea2327a8f3ec6a9f38614996367b.png){savepage-src="https://static001.geekbang.org/resource/image/d7/b8/d77857341e194bae59ce099e7d68c9b8.png"}