16
23
608
676
823
560
2667
At victim At honeypots >100 pkts.
417
35
33
452
577
351
1797
173
421
1
630
100
373
0
506
Table 2 gives an overview over the self-attacks. We launched a total of 2667
CharGen, DNS, NTP and SSDP attacks using 23 booter services. Interestingly,
only around 2/3 of the attacks we initiated were observed at the victim. This
can be explained by our observation of maintenance issues that some booter
websites have. Sometimes booter websites provide the user interface for selecting
a particular attack type that is temporarily non-functional. To users it appears
that the attack has been successfully launched, but no actual attack traﬃc is
generated as a result of initiating such attacks.
The DDoS honeypots observed many NTP attacks (73.0%) and DNS
attacks (38.3%), but only a small fraction of the CharGen attacks (8.4%) and
only a single SSDP attack. Furthermore, while the honeypots observed some
traﬃc belonging to 630 attacks, in only 506 cases did we record more than 100
requests. We inspected the reasons why the honeypots missed large portions
of SSDP and CharGen attacks. To this end, we investigated the attack traﬃc
towards our victim to learn the preferences of attacks in choosing reﬂectors. In
both cases, we found that the vast majority of the reﬂectors that were abused
by multiple booters send responses that are signiﬁcantly larger than the ones
conﬁgured in AmpPot. This indicates that the honeypots’ SSDP and CharGen
responses were too small to be attractive for attackers, and adversaries preferred
other reﬂectors with better ampliﬁcation. We leave further investigations on
434
J. Krupp et al.
reﬂector selection strategies open for future work and focus on DNS and NTP
in the following.
Multi-branding Booters: During the sign-up phase, we noticed that some
booters were visually similar. Investigations have revealed that one miscreant
follows a multi-branding strategy, i.e., sells the same service via diﬀerent booter
names that shared similar web front-ends. It became apparent that attacks from
RAW and WEB shared characteristics, and also their sign-up page of the web inter-
face was equivalent in appearance and HTML code. We further analyzed those
two booters by launching application layer (layer 7) attacks against our victim
server. Layer 7 attacks usually abuse public HTTP proxy servers to hide the
identity of back-end servers involved. However, some proxies reveal the identity
of the requesting clients in the X-Forwarded-For ﬁeld of the HTTP header.
Based on this observation, we were able to verify that these two booters used
shared back-end infrastructure. We thus conclude that RAW and WEB are likely to
be operated by the same individuals and will regard them as equivalent.
4 Characteristic Attack Features
We will now introduce characteristic attack patterns that we can use to train our
classiﬁer for attribution purposes. We ﬁrst describe various characteristics that
we have observed to repeat across subsets of attacks at the honeypots. We then
describe how we leverage these observations as features to summarize attacks.
4.1 Attack Observations
While analyzing the attacks captured by the honeypots, we observed the follow-
ing three properties that repeated across subsets of the attacks.
Honeypot Sets: Although eleven honeypots were active since the end of 2014,
few attacks (1.63%) abused all of them simultaneously. In fact, more than 60%
of all DNS- and NTP-based attacks abused ﬁve honeypots or less. This indicates
that attackers either perform only partial scans of the Internet, or choose a
subset of the discovered ampliﬁers in subsequent attacks.
Interestingly, we observed that honeypot sets seem to be reused across multi-
ple attacks, i.e., even in attacks against diﬀerent victims or on diﬀerent days. To
further investigate this observation, we analyzed ampliﬁers seen in self-attacks
from a few example booter services over time, shown in Fig. 1. The entries on
the heat maps show the ratio of abused ampliﬁers that were shared per booter
and attack protocol on two consecutive days each. With the exception of DNS,
there is a high level of overlap for attacks based on NTP, CharGen, and SSDP,
suggesting that booters reuse their set of ampliﬁers for a protocol for some time.
The low overlap for attacks based on DNS is likely caused by frequent rescans
to account for the relatively high IP churn rate of DNS ampliﬁers [11].
In addition, we veriﬁed that two simultaneous attacks towards the same vic-
tim on diﬀerent protocols showed little overlap in the sets of honeypots abused.
Linking Ampliﬁcation DDoS Attacks to Booter Services
435
Fig. 1. Overlap of ampliﬁer sets between consecutive dates.
This could indicate that the set of ampliﬁers might be speciﬁc to the protocol,
which intuitively can be explained by the small overlap of systems that suﬀer
from ampliﬁcation vulnerabilities for multiple protocols.
Victim Ports Entropy: While one UDP port determines the ampliﬁcation
protocol (e.g., DNS, NTP, etc.), the other determines the victim port on which
the victim will receive the reﬂected responses. Since an attacker has virtually no
restrictions on setting the victim port, we expected to observe the two obvious
choices: Choosing one victim port per attack, or choosing an individual victim
port for every request. Surprisingly, in addition to that, we also observed attacks
where requests shared a small number of victim ports. One explanation could be
that attackers use multiple threads for attacking, and that they choose a diﬀerent
victim port per thread. In addition, we veriﬁed that a signiﬁcant number of booter
services actually ask their clients to choose the victim port, giving a reason why
the number of source ports is frequently restricted to one.
Time-to-Live Values: The Time-to-Live (TTL) value in the IP packet indi-
cates how many hops a packet has traversed from the attack source to the hon-
eypot. As already observed by Kr¨amer et al. [8], for one particular attack, a
honeypot will usually only see one (or very few) TTL value(s). We can thus
conclude that most attacks likely stem from a single source, which motivates
further investigations in ﬁnding this particular source sending spoofed traﬃc.
Additionally, the vast majority of requests have a TTL >230. This suggests that
attackers use a ﬁxed initial TTL of 255 in their generated packets, as otherwise
we would see a wider distribution.
4.2 Distance Function
In order to leverage these observations in a classiﬁer, we next introduce a distance
function based on the above features. Given two attack instances A and B, such
a function is used to determine how dissimilar the two instances are. For an
attack A, we will denote the set of honeypots used by HPA, the set of victim
ports observed by VPortA, and the set of TTLs received at honeypot hp by
TTLhp,A.
436
J. Krupp et al.
To compare honeypot sets, we leverage the well-known Jaccard distance:
dhp(A, B) = 1 − |HPA ∩ HPB|
|HPA ∪ IPB|
To compare the set of victim ports, we take the normalized diﬀerence:
dvp(A, B) =
(cid:2)
(cid:2)|VPortA| − |VPortB|(cid:2)
max (|VPortA|,|VPortB|)
(cid:2)
Finally, to compare TTLs, we compute the overlap of their histograms5
dhist(S, T ) = 1 −
(cid:3)
x
(cid:3)
x
min(S(x), T (x))
max(S(x), T (x))
and then average this overlap over all honeypots involved in both attacks:
dttl(A, B) =
(cid:3)
hp∈HPA∩HPB
dhist (TTLhp,A, TTLhp,B)
|HPA ∩ HPB|
From these three sub-functions we compute a weighted average as the overall
distance function. We set the weights to whp = 5, wvp = 1, and wttl = |HPA ∩
HPB|/2. Note that our methodology is independent from the weights and the
analyst can choose any weights according to her needs. We assigned a smaller
weight to the victim port feature, as it relies on inputs with little entropy given
just three cases: a single victim port, a few victim ports, or many victim ports.
For the TTL feature, we assign a higher weight if the two attacks have more
honeypots in common, as we assume that coinciding TTLs for multiple honeypots
have a much higher signiﬁcance than those for only a single honeypot.
5 Honeypot Attack Attribution
We now leverage the aforementioned features to identify which booter has caused
which attacks observed at a honeypot. The core idea is to use supervised machine
learning techniques to attribute an attack observed at a honeypot to a particular
booter service. We will ﬁrst use our ground truth data set to show the perfor-
mance and resilience of our classiﬁer in various situations. Afterwards, we will
apply the classiﬁer to the entire data set of attacks collected by the honeypots.
5 To account for ﬂuctuation in TTLs due to route changes, we apply smoothing to
the histograms using a binomial kernel of width 6, which corresponds to a standard
deviation of σ ≈ 1.22.
Linking Ampliﬁcation DDoS Attacks to Booter Services
437
5.1 Description
Finding the true origin of an ampliﬁcation attack is a non-trivial problem,
because—from the reﬂector’s perspective—all packets carry spoofed headers.
Using our attack distance metric, we showed that attacks from the same booter
service exhibit similar characteristics and this observation turns the problem of
ﬁnding the origin of an attack into a classiﬁcation problem. The collected self-
attack data set can be used for training and validating a classiﬁer. Since the
number of attacks observed strongly varies between booters, we decided to use
the k-Nearest Neighbor (k-NN) algorithm due to its resilience to such imbal-
ances. In k-NN, to determine the label of an instance, the set of its k nearest
neighbors is computed. Next, every neighbor casts a vote for its own label, and
ﬁnally the instance is given the label of the majority of its neighbors.
Additional care has to be taken, as our training data set is not exhaustive
and may miss data for some booters. That is, not all attacks can be attributed
to a booter that we know. Therefore, we use a cutoﬀ threshold t to introduce
a label for an unknown classiﬁcation result. When classifying an item i, we only
consider the k nearest neighbors that can be found in the neighborhood of radius
t centred around item i. If no item from the training data set lies within this
neighborhood, the item i is assigned the label unknown. To ﬁnd a well-suited and
conservative threshold, we analyzed our ground truth data set using our distance
function and hierarchical clustering. From those clusters, we then computed the
average distance between attacks within a cluster and took the 95th percentile
over all. This results in t = 0.338 for DNS and t = 0.236 for NTP.
Furthermore, as shown in Sect. 4.1, booters rescan to ﬁnd new lists of ampli-
ﬁers on a regular basis. To reﬂect this during classiﬁcation, we only consider
elements from the training data set no more than 7 days apart, which approxi-
mately corresponds to the maximum rescan frequency we observed for booters.
When using k-NN, the choice of k is highly critical for the performance of
the classiﬁer. One common approach is to learn the value of k from the training
data set using n-fold cross-validation (CV). In n-fold CV, the training data set
is partitioned into n equally sized sets. Then, the classiﬁer is trained on n − 1
of these sets, and the ﬁnal set is used for validation. This process is repeated n
times, until every set has been used as the validation set once. For ﬁnding k we
thus perform 10-fold CV for all k ∈ {1, 3, 5} as part of the training phase of the
classiﬁer. We restrict k to odd values to avoid ties in the voting phase. We only
consider k ≤ 5, because about 2/3 of the clusters contain less than ﬁve attacks.
To assess the performance of our classiﬁer, we ﬁrst deﬁne the false positive
rate (FPR), precision and recall metrics, as well as macro-averaging. Intuitively,
the FPR for a label li (in our case, a particular booter) is the fraction of elements
that were incorrectly assigned the label li while their true label was not li. In
a similar vein, precision is the ratio with which the classiﬁer was correct when
assigning label li, while recall is the ratio with which the classiﬁer is able to
re-identify elements with true label li. Let tpi be the number of items correctly
classiﬁed to have label li (true positives), let tni be the number of items correctly
classiﬁed to not have label li (true negatives), let fpi be the number of items
438
J. Krupp et al.
incorrectly classiﬁed to have label li (false positives), and let fni be the number
of items incorrectly classiﬁed to not have label li (false negatives). Then the FPR
is deﬁned as fpri = fpi/(fpi + tni), precision as pi = tpi/(tpi + fpi), and recall as
ri = tpi/(tpi + fni). To compute overall performance measures from these per-
class metrics, we employ macro-averaging, i.e., ﬁrst computing fpr, p, and r per
class and averaging the respective results afterwards, as this will avoid bias due
to imbalance in our ground truth data. Thus booters for which we were able to
collect more datapoints do not inﬂuence the results more strongly. However, since
we strongly prefer mislabeling an attack as unknown over incorrectly attributing
it to a wrong booter, we only weigh the unknown label with 1
8.
5.2 Validation
To validate our classiﬁer, we deﬁned three experiments on our labeled self-attack
data set: First, we conducted 10-fold CV to assess how well our classiﬁer can
correctly attribute attacks (E1). Second, to estimate how well our classiﬁer deals
with attacks from booters not contained in the training data set, we used leave-
one-out CV on the booter level (E2). This means that the attacks from all but
one booter constitute the training set, and all attacks from the omitted booter
are used for validation, checking if these attacks are correctly labeled as unknown.
Third, we were also interested in the performance of classifying attacks in real-
time (E3), i.e., training only on labeled observations prior to the attack.
Table 3. Honeypot-driven experimental results
(a) DNS
N
A
B
samples (#) 10
correct (%) 90
unknown (%) 10 100 18 18
2
I
T
X
S
E
1 49 11 10 18
0 82 82 100 78
O
D
V
1
0
0 22 100
W
A
R
R
E
S
1
T
S
1
E
wrong (%)
2 unknown (%) 100 100 100 100 100 100 100
E
wrong (%)
correct (%) 70
0
unknown (%) 30 100 33 27 30 33 100
0 67 73 70 67
3
E
wrong (%)
(b) NTP
N
A
B
R
U
A
1
O
B
samples 28 15 40
correct 100 87 78
unknown
wrong
1
E
0 13 23 100
2
3
O
O
B
B
1 12
0 100
0
W
I
I
R
O
X
C
E
D
27 21
4