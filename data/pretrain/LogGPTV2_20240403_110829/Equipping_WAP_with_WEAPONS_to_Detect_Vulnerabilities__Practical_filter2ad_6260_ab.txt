I. Moreover, we increased the granularity of the analysis by
specifying that all symptoms are attributes (both old and new,
2nd and 3rd columns). Therefore, instead of 16 attributes we
now have 61.
Modifying the attributes requires training again the clas-
siﬁers and, as the number of attributes is much higher, we
need also a much larger number of instances (samples of
code annotated as false positive or not). The original WAP
was trained with a data set of 76 instances: 32 annotated as
false positives and 44 as real vulnerabilities. Each instance
had 16 attributes set to 1 or 0, indicating the presence or not
of symptoms for the attributes, and an attribute saying if the
instance is a false positive or not. We increased the number of
instances to 256, each one with 61 attributes. The instances are
evenly divided in false positives and vulnerable (balanced data
set). To create the data set we used WAP conﬁgured to output
the candidate vulnerabilities, and we ran it with 29 open source
PHP web applications. Then, each candidate vulnerability was
processed manually to collect the attributes and to classify it as
being a false positive or not. Finally, noise was eliminated from
the data set by removing duplicated and ambiguous instances.
To perform the data mining process we used the WEKA
tool [26] with the original classiﬁers and induction rules. We
also want a top 3 of classiﬁers, as originally. Our goals are
that classiﬁers: (1) predict as many false positives correctly
as possible; (2) have a fallout as low as possible (wrong
classiﬁcations of vulnerabilities as false positives), avoiding
to miss vulnerabilities found by the taint analyzer.
Table II depicts the evaluation of the 3 best classiﬁers
(we omit the rest for lack of space). The ﬁrst 7 metrics were
adopted from [12]; the last 2 are new. The last column shows
the formulas to calculate each metric, based in values extracted
from the confusion matrix (Table III, last 2 columns).
SVM Logistic
Metrics
Regression
(%)
tpp
93.0%
94.5%
pfp
4.7%
4.7%
prfp
95.2%
95.3%
pd
95.3%
95.3%
ppd
94.6%
93.1%
acc
94.1%
94.9%
pr
94.2%
94.9%
inform 89.8%
88.3%
jacc
90.3%
88.8%
Random Formula
Forest
90.6%
2.3%
97.5%
97.7%
91.2%
94.1%
94.4%
88.3%
88.5%
tpp = recall = tp / (tp + fn)
pfp = fallout = fp / (tn + fp)
prfp = pr positive = tp / (tp + fp)
pd = speciﬁcity = tn / (tn + fp)
ppd = inverse pr = tn / (tn + fn)
accuracy = (tp + tn) / N
precision = (prfp + ppd) / 2
informedness = tpp + pd -1 = tpp - pfp
jaccard = tp / (tp + fn + fp)
TABLE II: Evaluation of the machine learning models applied to
the data set.
Classiﬁers are usually selected based on accuracy and
precision, but in this case the three classiﬁers have very similar
values in both metrics: between 94% and 95%. Moreover, the
compliance to goal (1) is measured by tpp. In terms of this
metric, Support Vector Machine (SVM) had the best results
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:37:52 UTC from IEEE Xplore.  Restrictions apply. 
Observed
SVM
121
7
6
122
Logistic Regression
Classiﬁer
Predicted
Yes (FP) No (not FP) Yes (FP) No (not FP) Yes (FP) No (not FP) Yes No
Yes (FP)
fp
No (not FP)
tn
TABLE III: Confusion matrix of the top 3 classiﬁers and confusion
matrix notation (last two columns).
Random Forest
3
125
tp
fn
119
9
6
122
116
12
and Logistic Regression (LR) the second best. In terms of goal
(2), Random Forest (RF) had the best fallout rate (pfp). The
inform metric expresses how the classiﬁcations made by the
classiﬁer are close to the correct (real) classiﬁcations, whereas
jacc measures the classiﬁcations in the false positive class,
taking into account false positives and negatives [20]. For
inform, we combine the best values of tpp and pfp, i.e., the tpp
from SVM and the pfp from RF, resulting in 92%, while for
jacc we use the correct and misclassiﬁcations of all classiﬁers,
resulting in 92%. These measures conﬁrm our choice of the
top 3 classiﬁers. These classiﬁers are the same as those used
in the original WAP, except RF that substitutes Random Tree.
The confusion matrix of these classiﬁers is presented in
Table III. SVM and LR classiﬁed incorrectly a few instances,
and RF classiﬁed 3 real vulnerabilities as being false positives.
Notice that this misclassiﬁcation is represented as fp in the
confusion matrix, representing the instances belonging to class
No that were classiﬁed in class Yes. However, in the context
of vulnerability detection this represents false negatives, i.e.,
vulnerabilities that were not detected.
2) Dynamic symptoms: We use the term dynamic symp-
toms to designate symptoms deﬁned by the user that conﬁgures
the tool for new vulnerabilities, whereas static symptoms are
those that come with the tool. For every dynamic symptom
the user has to provide a category and a type. For example, if
the user develops a function val int to validate integer inputs
(instead of is int) he has to provide the information that the
function belongs to the validation category and that it has an
effect similar to the static symptom (function) is int. Based on
this information, the tool understands how to handle function
val int when predicting false positives.
Fig. 3 presents the reorganization of the false positive
predictor. When a candidate vulnerability is processed by this
module: ﬁrst the static and dynamic symptoms are collected
from the source code; then a vector of 61 attributes is created
using the map from static symptoms to attributes (stored into
the tool) and the map of dynamic symptoms to attributes (cre-
ated dynamically); then the vector is classiﬁed using machine
learning classiﬁers; ﬁnally, in case of a real vulnerability, it is
sent to the code corrector module to be ﬁxed.
C. Code corrector
When a vulnerability is found, the code corrector inserts
a ﬁx that does sanitization or validation of the data ﬂow. To
make WAP modular we created two sub-modules: (1) code
ﬁxing sub-module, which receives the vulnerability class and
the code to be ﬁxed and inserts the ﬁx; (2) ﬁx creation that uses
information and constraints provided by the user to generate
a new ﬁx for a new class of vulnerabilities. The ﬁrst does
essentially what the original version of WAP already did so
we focus on (2).
633


































Fig. 3: Reorganization of the false positives predictor module.
We propose three ﬁx templates to generate automatically
ﬁxes: PHP sanitization function, user sanitization, and user
validation. The one that is used depends on the information
provided by the user. The PHP sanitization function template
is applied when the user speciﬁes the PHP sanitization function
used to sanitize data and the sensitive sink associated to this
functions, for a given vulnerability. The sanitization function is
used as ﬁx. The user sanitization template is chosen if the user
indicates the malicious characters that may be used to exploit
the vulnerability and a character that can be used to neutralize
them (e.g., the backslash). The user validation template is used
if the user only speciﬁes the set of malicious characters used
to exploit the vulnerability. In that case the ﬁx checks the
presence of these characters, issuing a message in case there
is a match. Fixes are inserted in the line of the sensitive sink,
as in the original WAP [12].
D. Weapons
A weapon is a WAP extension composed by a detector, a
ﬁx and, optionally, a set of dynamic symptoms. To generate
weapons we developed a weapon generator, external to WAP.
The data needed to create a weapon is: (1) for the detector, the
sanitization and sensitive sinks functions, plus additional entry
points if they exist; (2) for the ﬁx, data for the ﬁx templates
(Section III-C); (3) dynamic symptoms, in case the user has
white/black lists of functions, or functions that do not belong
to the static symptoms list (in this case, the correspondence
between dynamic and static symptoms is required).
To generate a weapon,
the weapon generator uses the
vulnerability detector generator (see Section III-A) that
it
conﬁgures with (1), generating a new detector with the ss, san
and ep ﬁles containing the data provided by the user. Next,
it conﬁgures the selected ﬁx template with (2), generating a
new ﬁx. Then, it creates a ﬁle with (3). The last step is to
put together the three parts, linking them to WAP. Detection
is activated using a command line ﬂag also provided by the
user (e.g., -nosqli).
When the weapon is activated, WAP parses the code,
generating an AST, next the detector navigates under the AST
using the data stored in its ﬁles. The candidates vulnerabilities
found by the detector are processed by the false positives
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:37:52 UTC from IEEE Xplore.  Restrictions apply. 
predictor using the symptoms deﬁned in WAP and contained in
the weapon, and the real vulnerabilities are ﬁxed using the code
ﬁxing module (see Section III-C) with the ﬁx of the weapon.
E. Effort to modify WAP
Modifying WAP involved an effort with three facets: (1)
making the AST independent of the navigation made by the
detectors (tree walkers); (2) restructuring the code to create the
three sub-modules for the vulnerabilities originally considered
(Section III-A), to integrate the dynamic symptoms (Section
III-B), and to make the code corrector able to receive new
ﬁxes (Section III-C); (3) coding the weapon generator module
(Section III-D). From the three facets, (3) was the one that
required more effort. We had to build a new java package
to create weapons (new vulnerability detector sub-module),
a frontend for the user to conﬁgure the weapon generator,
templates to create automatically ﬁxes, and to integrate the
weapon in WAP. When the weapon generator is executed it
creates a new java package and compiles it, building a jar to
be integrated with the WAP tool.
IV. EXTENDING WAP WITH NEW VULNERABILITIES
This section presents the seven new vulnerability classes
with which we extended WAP, as well as how this extension
was done. The section also presents the extension to detect
SQLI in WordPress plugins that uses WordPress functions as
entry points, sanitization functions, and sensitive sinks. To
demonstrate how we can take advantage of the modularity we
created in WAP, we opted by extending it in two different
ways: reusing the sub-modules presented in Section III-A
(Section IV-B) and with weapons (Section IV-C). However,
a normal user would probably use the second form.
A. New vulnerabilities
We equiped the tool to detect the following seven vulner-
abilities: LDAP injection (LDAPI), XPath injection (XPathI),
NoSQL injection (NoSQLI), comment spamming (CS), header
injection or HTTP response splitting (HI), email injection (EI),
and session ﬁxation (SF). With the exception of SF, all of
them are input validation vulnerabilities, meaning that they
are created by lack of sanitization or validation of user inputs
before they reach a sensitive sink.
The ﬁrst three vulnerabilities are associated to the con-
struction of queries or ﬁlters that are executed by some kind
of engine, e.g., a database management system. They behave
similarly to SQLI, i.e., if a query is built with unsanitized user
inputs containing malicious characters, the query executed is
a modiﬁcation of the original one [21], [17].
CS has the goal of manipulating the ranking of spammers’
web sites, making them appear at the top of search engines’
results. Web applications that allow the users to submit con-
tents with hyperlinks are the potential victims of the attack.
Attackers inject comments, for example, containing links to
their own web site [8], [9]. To avoid CS, applications have to
check if the content of posts contains hyperlinks (URLs).
Header injection or HTTP response splitting (HI) allows
an attacker to manipulate the HTTP response, breaking the
normal response using the \n and \r characters. This allows
the attacker to inject malicious code (e.g., JavaScript) in a new
header line or even a new HTTP response. The vulnerability
can be avoided by sanitizing these characters (e.g., substituting
them by a space) [21].
Email injection (EI) is similar to HI, allowing an attacker to
inject the line termination character, in clear or encoded (%0a
and %0d), with the aim of manipulating the email components
(e.g., sender, destination, message). The vulnerability can also
be avoided by sanitizing these characters [21].
Session ﬁxation allows an attacker to force a web client to
use a speciﬁc (“ﬁxed”) session ID, allowing him to access the
account of the user. Avoiding this vulnerability is not trivial as
there is no sanitization function to apply or set of malicious
characters to recognize. A way to defend against SF is to avoid
using a session token provided by the user [21], [16].
B. Reusing the sub-modules
The detection of four of the vulnerability classes described
in the previous section can be integrated in the sub-modules of
Section III-A and the ﬁxes to remove them can be created using
a ﬁx template (Section III-C). Table IV shows the classes of
vulnerabilities integrated in each sub-module and the sensitive
sinks added to detect each vulnerability. These functions were
inserted in the ss ﬁle of each sub-module. No sanitization
functions or entry points were added to the san and ep ﬁles.
In relation to LDAPI and XPathI, a ﬁx was created for each
one using the user validation ﬁx template. For CS we changed
WAP’s san_read and san_write ﬁxes. These ﬁxes deal with
the sensitive sinks speciﬁed above for the CS vulnerability.
They validate the user inputs contents against JavaScript code,
so we changed them to also check the input contents against
URIs/hyperlinks. For SF we created a ﬁx from scratch.
Sub-module
RCE & ﬁle
injection
client-side
injection
query
injection
Vuln.
Sensitive sink
SF
CS
LDAPI
XPathI
setcookie, setdrawcookie, session id
ﬁle put contents, ﬁle get contents
ldap add, ldap delete, ldap list, ldap read, ldap search
xpath eval, xptr eval, xpath eval expression
TABLE IV: Sensitive sinks added to the WAP sub-modules to detect
new vulnerability classes.
C. Creating weapons
We used the scheme presented in SectionIII-D to create
three weapons, for (1) NoSQLI, (2) HI and EI, and (3) SQLI
for WordPress.
implement
1) NoSQLI weapon: NoSQL is a common designa-
tion for non-relational databases used in many large-scale
web applications. There are various NoSQL database mod-
els and many engines that
them. MongoDB
[13] is the most popular engine implementing the docu-
ment store model [7]. Therefore, we opted for creating a
weapon to detect NoSQLI in PHP web applications that
to MongoDB. We conﬁgured the weapon gen-
connect
erator with:
the find, findOne, findAndModify,
insert, remove, save, execute sensitive sinks and the
mysql_real_escape_string sanitization function; (2) the
PHP sanitization ﬁx template to sanitize the user inputs that
(1)
634
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:37:52 UTC from IEEE Xplore.  Restrictions apply. 
reach that sink with that sanitization function, resulting in the
san nosqli ﬁx; and (3) no dynamic symptoms. The weapon is
activated by the -nosqli ﬂag.
2) HI and EI weapon: We conﬁgured the weapon gener-
ator with: (1) the header and mail sensitive sinks and no
sanitization functions; (2) the user sanitization ﬁx template to
check the malicious characters presented in Section IV-A and
to replace them by a space, resulting in the san hei ﬁx; and