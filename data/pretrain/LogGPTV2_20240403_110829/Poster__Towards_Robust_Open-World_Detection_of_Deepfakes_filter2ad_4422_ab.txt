a
r
u
c
c
A
t
s
e
T
80
60
FaceSwap
Deepfake
Face2Face
Mismatch
Combined
ClassNSeg
FacenetLSTM
Figure 1: Comparison of ClassNSeg [16] and FacenetLSTM
models on matching and mismatched training and testing
datasets together with performance after training on com-
bined datasets.
Figure
ResNeXTSpoof and convolutional LSTM.
2: Comparison
of
performances
between
mismatched combinations for mismatched accuracy results. We
additionally trained on combined datasets to get the accuracy on
each. Figure 1 shows an average for the aggregated values for
mismatch and combined models. We see that both classifiers fail
when trained on one dataset and tested on another. Their results
are stronger, however, when combining multiple training datasets.
For fake audio detection, we evaluate the models on the evalua-
tion partition of the ASVSpoof2019 Logical Access dataset, which
we emphasize contains faking methods that are not present in the
training and development partitions. Figure 2 shows the receiver
operating characteristic (ROC) curves from the two methods on the
development and evaluation set. The equal error rates (EERs) are
5.4% for ResNeXTSpoof and 6.4% for convolutional LSTM, showing
better performance for ResNEXTSpoof.
4 USER STUDY
The goal of our work is to develop a tool for journalists, so it
is imperative to design the tool to be easily integrated into their
workflow and have an intuitive and usable interface. To achieve
this, we are conducting a multi-part user study with journalists
from reputable organizations at both the local and national level.
4.1 Study Design
The first part of the user study was designed to understand jour-
nalists’ level of knowledge and perceptions of deepfakes, their cur-
rent information verification process, and their interactions with
potential and confirmed tampered videos. We also gathered the
expectations they had for the tool in terms of performance, inter-
face, and availability to the public. We conducted semi-structured
interviews with the journalists in their workplaces as well as public
settings. The participants were given the option to be anonymous
because the conversations were recorded and transcribed. At the
end of the interviews, we showed them an interactive prototype
PosterCCS ’19, November 11–15, 2019, London, United Kingdom2614multiple detection models which aligns well with requirements
identified from our user studies.
Moving forward on the technical side, we plan to deploy an initial
version of our tool and work towards multiple accurate and robust
models for detection. We must ensure that the tool can detect new
types of fake videos beyond what we train on. For the user side, we
are planning on supplementing the semi-structured interviews with
two working prototypes for A/B testing. The participants will be
asked to rate as well as mix and match the features using prototypes.
With an interface selected, we will perform live beta testing that
would allow a larger number of journalists to participate.
Acknowledgments. This effort was funded in part by the Ethics
and Governance of AI Initiative.
REFERENCES
[1] Darius Afchar, Vincent Nozick, Junichi Yamagishi, and Isao Echizen. 2018.
Mesonet: A Compact Facial Video Forgery Detection Network. In WIFS.
[2] Shruti Agarwal, Hany Farid, Yuming Gu, Mingming He, Koki Nagano, and Hao
Li. 2019. Protecting World Leaders Against Deep Fakes. In CVPR Workshops.
[3] Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman. 2018. VGGFace2: A
Dataset for Recognizing Faces Across Pose and Age. In ICAFGR.
[4] Umur Aybars Ciftci and Ilke Demir. 2019. FakeCatcher: Detection of Synthetic
Portrait Videos using Biological Signals. arXiv preprint arXiv:1901.02212 (2019).
[5] Davide Cozzolino, Justus Thies, Andreas Rössler, Christian Riess, Matthias
Nießner, and Luisa Verdoliva. 2018. Forensictransfer: Weakly-supervised domain
adaptation for forgery detection. arXiv preprint arXiv:1812.02510 (2018).
[6] Beatrice Dupuy. 2019. NOT REAL NEWS: Altered video makes Pelosi seem to
slur words. https://apnews.com/4841d0ebcc704524a38b1c8e213764d0
[7] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial
nets. In Advances in Neural Information Processing Systems (NeurIPS). 2672–2680.
[8] David Güera and Edward J Delp. 2018. Deepfake Video Detection Using Recurrent
[12] Marek Kowalski. [n. d.]. faceswap. https://github.com/MarekKowalski/FaceSwap.
[13] Yuezun Li, Ming-Ching Chang, Hany Farid, and Siwei Lyu. 2018. In ictu oculi:
Exposing AI Generated Fake Face Videos by Detecting Eye Blinking. arXiv
preprint arXiv:1806.02877 (2018).
[14] Jaime Lorenzo-Trueba, Junichi Yamagishi, Tomoki Toda, Daisuke Saito, Fernando
Villavicencio, Tomi Kinnunen, and Zhenhua Ling. 2018. The voice conversion
challenge 2018: Promoting development of parallel and nonparallel methods.
arXiv preprint arXiv:1804.04262 (2018).
[15] Shao-An Lu. [n. d.]. faceswap-GAN. https://github.com/shaoanlu/faceswap-GAN.
Commit: c563edc128e79c3b593da63825f0208acf7ea4d9.
[16] Huy H. Nguyen, Fuming Fang, Junichi Yamagishi, and Isao Echizen. 2019. Multi-
task Learning For Detecting and Segmenting Manipulated Facial Images and
Videos. arXiv:cs.CV/1906.06876
[17] Huy H Nguyen, Junichi Yamagishi, and Isao Echizen. 2018. Capsule-Forensics:
Using Capsule Networks to Detect Forged Images and Videos. arXiv preprint
arXiv:1810.11215 (2018).
[18] Andreas Rössler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies,
and Matthias Nießner. 2019. FaceForensics++: Learning to Detect Manipulated
Facial Images. arXiv preprint arXiv:1901.08971 (2019).
[19] Florian Schroff, Dmitry Kalenichenko, and James Philbin. 2015. Facenet: A Unified
Embedding for Face Recognition and Clustering. In CVPR.
[20] Karen Simonyan and Andrew Zisserman. 2014. Very Deep Convolutional Net-
works for Large-scale Image Recognition. arXiv preprint arXiv:1409.1556 (2014).
[21] Justus Thies, Michael Zollhofer, Marc Stamminger, Christian Theobalt, and
Matthias Nießner. 2016. Face2face: Real-time Face Capture and Reenactment of
RGB Videos. In CVPR.
[22] Massimiliano Todisco, Xin Wang, Ville Vestman, Md Sahidullah, Hector Del-
gado, Andreas Nautsch, Junichi Yamagishi, Nicholas Evans, Tomi Kinnunen, and
Kong Aik Lee. 2019. ASVspoof 2019: Future Horizons in Spoofed and Fake Audio
Detection. arXiv preprint arXiv:1904.05441 (2019).
Neural Networks. In AVSS.
[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Deep Residual
Learning for Image Recognition. CoRR abs/1512.03385 (2015). arXiv:1512.03385
http://arxiv.org/abs/1512.03385
[10] Tomi Kinnunen, Md Sahidullah, Héctor Delgado, Massimiliano Todisco, Nicholas
Evans, Junichi Yamagishi, and Kong Aik Lee. 2017. The ASVspoof 2017 challenge:
Assessing the limits of replay spoofing attack detection. (2017).
[11] Iryna Korshunova, Wenzhe Shi, Joni Dambre, and Lucas Theis. 2017. Fast Face-
swap using Convolutional Neural Networks. In ICCV.
Figure 3: A prototype of the deepfake detection interface.
to get their feedback regarding the flow of the verification process
and the details in the analyses.
4.2 Results
We interviewed four journalists, two from a local media organiza-
tion in the Rochester, NY area and two from a US national organi-
zation. We learned that the level of knowledge of deepfakes varies,
though journalists working in national news do have more aware-
ness regarding the topic. Some confusion exists, however, between
deepfakes and "cheap fakes" like the video showing US Speaker of
the House Nancy Pelosi slowed down to make her appear drunk
while speaking in public [6]. We also learned that news organiza-
tions do not have adequate security training for their staff, though
in some organizations, the staff themselves are more proactive in
learning and disseminating such information internally. There were
different opinions regarding whether the public should have the
same access to the tool as journalists. They agreed, however, that
the public should at least have access to the results of the analyses
to legitimize the tool and its importance. Furthermore, there was a
consensus that the accuracy of the tool is of the utmost importance,
and that processing speed can be sacrificed. The tool should provide
detailed analyses to the journalists, not just about which part of the
video is tampered, but also why the tool makes this determination.
4.3 Interface Design
Our intention for the interface was to make it intuitive, interactive,
and un-disruptive. YouTube video URLs are used to select and
download selections of a video to our server. The interface, shown
in Figure 3, shows the results as a color-coded timeline, where the
colors signal probabilities of the video being fake at each video
interval. The timeline is split into multiple rows, with results from
different prediction models and methods on each row, which helps
the user to make an informed judgment regarding the content.
5 CONCLUSION & FUTURE WORK
In this work, we presented an ongoing project aimed at the develop-
ment of a tool for detecting deepfakes. The project consists of two
parts, the detection models and the user interface, which are unified
to create the web tool. Our initial results show success in using
PosterCCS ’19, November 11–15, 2019, London, United Kingdom2615