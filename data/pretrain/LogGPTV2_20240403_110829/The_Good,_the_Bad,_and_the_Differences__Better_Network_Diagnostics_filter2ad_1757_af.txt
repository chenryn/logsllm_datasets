I; DiffProv took three times as long as Y! in both cases.
We also observe that the actual DiffProv reasoning takes
a negligible amount of time – 3.8 milliseconds in the worst
case, as shown in a further decomposition in Figure 8. We
can see that detecting the ﬁrst divergence and making miss-
ing tuples appear took more time, because they involve track-
ing taints and evaluating their formulae. The SDN cases
took more time in making tuples appear, because the missing
(broken) ﬂow entries were generated with more derivation
steps. MR1-D took the longest time in divergence detection
because its trees are deeper than those in all other cases.
6.7 Complex network diagnostics
Now that we have shown that DiffProv has a reasonably
small overhead, we turn to evaluating the effectiveness of
DiffProv’s diagnostics on a complex network with real-world
conﬁgurations and realistic background trafﬁc.
Basic setup: Our scenario is based on the Stanford Univer-
sity network setup obtained from ATPG [32]; it represents
a realistic campus network setting with complex forwarding
policies and access control rules. The network has 14 Oper-
ational Zone (OZ) routers and 2 backbone routers that form
a tree-like topology, and they are conﬁgured with 757, 000
forwarding entries and 1, 500 ACL rules. The routers are
emulated with Open vSwitch (OVS) in Mininet [20], and
controlled by a Beacon [4] controller. We also replicated
their “Forwarding Error” scenario that involves two hosts
and two switches, which we will refer to as H1, H2, and
S1, S2, respectively: in the error-free setting, H1 should be
able to reach H2 via a path H1-S1-S2-H2; however, S2 con-
tains a misconﬁgured OpenFlow entry that drops packets to
172.20.10.32/27, which is H2’s subnet. Please refer to [32]
for a more detailed description on the conﬁgurations and the
diagnostic scenario.
Multiple faults: Large networks are often misconﬁgured
in more than one place, and their conﬁguration tends to be
changed frequently. The resulting “noise” can be challeng-
ing for debuggers that simply look for anomalies or recent
changes. To demonstrate that DiffProv’s use of provenance
prevents it from being confused by bugs or changes that are
not causally related to the queried event, we injected 20 addi-
tional faulty OpenFlow rules; 10 of them were on-path from
H1 to H2, and the other 10 were on other OVS switches.
We veriﬁed that the original fault we wanted to diagnose re-
mained reproducible after injecting these additional faults.
Background trafﬁc: To obtain a realistic data-plane envi-
ronment, we ran three different applications in the network,
and injected a mix of background trafﬁc: 1) an HTTP client
that fetches the homepage from a remote server periodically;
2) a client that downloads a large data ﬁle from a ﬁle server;
3) an NFS client that crawls the ﬁles in the root directory
exported by a remote NFS server; and 4) we streamed the
OC-192 trace from CAIDA through the network. The exper-
iments took about 10 minutes, and produced 12GB packet
captures, in which the tshark protocol analyzer detected
69 distinct protocol types.
Result: To diagnose the faulty event (i.e., a packet that is
dropped midway from H1 to 172.20.10.32/27), we provided
DiffProv with a reference event, which is a packet from H1
to 172.19.254.0/24: this is because we noticed that the sub-
nets 172.19.254.0/24 and 172.20.10.32/27 are co-located
in S2’s operational zone, yet H1 is only able to reach the for-
mer. We queried out the provenance trees of the faulty event
and the reference event. The trees are smaller than those in
previous SDN scenarios, as this fault only involves two in-
termediate hops: they contain 67 and 75 nodes, respectively.
Nevertheless, their plain differences contain as many as 108
nodes. We then used DiffProv to diagnose the fault - it cor-
rectly identiﬁes the misconﬁgured OpenFlow entry on S2 to
be the “root cause”, despite the 20 other concurrent faults
and the heavy background trafﬁc.
At ﬁrst glance, DiffProv’s resilience to environments with
substantial background trafﬁc might seem surprising; in fact,
DiffProv inherits this from the use of provenance, which
captures true causality, not merely correlations. Note that
this property sets our work apart from heuristics-based de-
buggers, e.g., DEMi [26] that is based on fuzzy testing, Peer-
Pressure [29] that uses statistical analysis to ﬁnd the likely
value of a conﬁguration entry, NetMedic [14] that ranks likely
causes using statistical abnormality detection, and others.
Those debuggers do not incur the overhead of accurately
capturing causality, but may introduce false positives or neg-
atives in their diagnostics as a result.
7. RELATED WORK
Provenance: Provenance is a concept borrowed from the
database community [6], but it has recently been applied in
several other areas, e.g., distributed systems [36, 34, 30],
storage systems [22], operating systems [11], and mobile
platforms [9]. Our work is mainly related to projects that
use network provenance for diagnostics. In this area, Ex-
SPAN [36] was the ﬁrst system to maintain network prove-
nance at scale; SNP [34] added integrity guarantees in adver-
sarial settings, DTaP [35] a temporal dimension, and Y! [30]
support for missing events. However, those systems focus on
the provenance of individual events, whereas DiffProv uses
an additional reference event for root-cause analysis. We
have previously sketched the concept of differential prove-
nance in a HotNets paper [8], but that paper did not contain
a concrete algorithm or an implementation.
Network diagnostics: A variety of diagnostic systems have
been developed over time. For instance, Anteater [19], Header
Space Analysis [16], and NetPlumber [15] rely on static anal-
ysis, while OFRewind [31], Minimal Causal Sequence anal-
ysis [27], DEMi [26], and ATPG [32] use dynamic analy-
sis and probing. Unlike DiffProv, many of these systems
are speciﬁc to the data plane and cannot be used to diag-
nose other distributed systems, such as MapReduce. Also,
none of these systems use reference events. As a result, they
have the same drawback as the earlier provenance-based sys-
tems: they return a comprehensive explanation of each ob-
served event and cannot focus on speciﬁc differences be-
tween “good” and “bad” events.
A few existing systems do use some form of reference: for
instance, PeerPressure [29], EnCore [33], ClearView [24],
and Shen et al. [28] use statistical analysis or data mining
to learn correct conﬁguration values, performance models,
or system invariants. But none of them accurately capture
causality, or leverage causality to reduce the space of can-
didate diagnoses. Attariyan and Flinn [3] does take causal-
ity into account, but it can only compare equivalent systems
(e.g., “sick” and “healthy” computers), not events. NetMedic
[14] also models dependencies, but it relies on statistical
analysis and learning to infer the likely faulty component.
The idea of identifying the speciﬁc moment when a sys-
tem “goes wrong” has appeared in other papers, e.g., in [17],
which diagnoses liveness violations by ﬁnding a critical state
transition. However, [17] does not use reference events, and
its technical approach is completely different from ours.
Some existing solutions have packet recording capabili-
ties that resemble the logging in DiffProv. For instance, Net-
Sight [13] records traces of packets as they traverse the net-
work, and Everﬂow [37] provides packet-level telemetry at
datacenter scales. These systems reproduce the path a packet
has taken, but not the causal connections, e.g., to conﬁgura-
tion states. Provenance offers richer diagnostic information,
and is applicable to general distributed systems.
8. CONCLUSION
Differential provenance is a way for network operators to
obtain better diagnostic information by leveraging additional
information in the form of reference events – that is, “good”
and “bad” examples of the system’s behavior. When refer-
ence events are available, differential provenance can reason
about their differences, and produce very precise diagnostic
information in return: the output can be as small as a sin-
gle critical event that explains the differences between the
“good” and the “bad” behavior. We have presented an algo-
rithm called DiffProv for generating differential provenance,
and we have evaluated DiffProv in two sets of case studies:
SDNs and Hadoop MapReduce. Our results show that Diff-
Prov’s overheads are low enough to be practical.
Acknowledgments: We thank our shepherd Harsha V. Mad-
hyastha and the anonymous reviewers for their comments
and suggestions. We also thank Jeff Mogul, Behnaz Arzani,
Yifei Yuan, and Chen Chen for helpful comments on earlier
drafts of this paper. This work was supported in part by NSF
grants CNS-1065130, CNS-1054229, CNS-1513679, CNS-
1218066, CNS-1117052, CNS-1453392, and CNS-1513734;
DARPA/I2O contract HR0011-15-C-0098; and the Intel-NSF
Partnership for Cyber-Physical Systems Security and Pri-
vacy.
9. REFERENCES
[1] RapidNet. http://netdb.cis.upenn.edu/rapidnet/.
[2] Y. Amsterdamer, D. Deutch, and V. Tannen.
Provenance for aggregate queries. In Proc. PODS,
2011.
[3] M. Attariyan and J. Flinn. Using causality to diagnose
conﬁguration bugs. In Proc. USENIX ATC, 2008.
[4] The Beacon Controller.
https://openﬂow.stanford.edu/display/Beacon/Home.
[5] P. Bille. A survey on tree edit distance and related
problems. Theor. Comput. Sci., 337(1-3):217–239,
June 2005.
[6] P. Buneman, S. Khanna, and W.-C. Tan. Why and
where: A characterization of data provenance. In Proc.
ICDT, Jan. 2001.
[7] CAIDA. http://www.caida.org/home/.
[8] A. Chen, Y. Wu, A. Haeberlen, W. Zhou, and B. T.
Loo. Differential provenance: Better network
diagnostics with reference events. In Proc. HotNets,
Nov. 2015.
[9] M. Dietz, S. Shekhar, Y. Pisetsky, A. Shu, and D. S.
Wallach. Quire: Lightweight provenance for smart
phone operating systems. In Proc. USENIX Security,
2011.
[10] R. Durairajan, J. Sommers, and P. Barford.
Controller-agnostic SDN debugging. In Proc.
CoNEXT, 2014.
[11] A. Gehani and D. Tariq. SPADE: Support for
provenance auditing in distributed environments. In
Proc. Middleware, 2012.
[12] T. G. Grifﬁn, F. B. Shepherd, and G. Wilfong. The
stable paths problem and interdomain routing.
IEEE/ACM Trans. Netw., 10(2):232–243, Apr. 2002.
[13] N. Handigol, B. Heller, V. Jeyakumar, D. Mazières,
and N. McKeown. I know what your packet did last
hop: Using packet histories to troubleshoot networks.
In Proc. NSDI, Apr. 2014.
[14] S. Kandula, R. Mahajan, P. Verkaik, S. Agarwal,
J. Padhye, and P. Bahl. Detailed diagnosis in enterprise
networks. In Proc. SIGCOMM, August 2009.
[15] P. Kazemian, M. Chang, H. Zeng, G. Varghese,
N. McKeown, and S. Whyte. Real time network policy
checking using header space analysis. In Proc. NSDI,
Apr. 2013.
[16] P. Kazemian, G. Varghese, and N. McKeown. Header
space analysis: Static checking for networks. In Proc.
NSDI, 2012.
[17] C. Killian, J. W. Anderson, R. Jhala, and A. Vahdat.
Life, death, and the critical transition: Finding liveness
bugs in systems code. In Proc. NSDI, 2007.
[18] B. T. Loo, T. Condie, M. Garofalakis, D. E. Gay, J. M.
Hellerstein, P. Maniatis, R. Ramakrishnan, T. Roscoe,
and I. Stoica. Declarative networking. Comm. ACM,
52(11):87–95, Nov. 2009.
[19] H. Mai, A. Khurshid, R. Agarwal, M. Caesar, P. B.
Godfrey, and S. T. King. Debugging the data plane
with Anteater. In Proc. SIGCOMM, 2012.
[20] Mininet. http://mininet.org/.
[21] C. Monsanto, J. Reich, N. Foster, J. Rexford, and
D. Walker. Composing software-deﬁned networks. In
Proc. NSDI, 2013.
[22] K.-K. Muniswamy-Reddy, U. Braun, D. A. Holland,
P. Macko, D. Maclean, D. Margo, M. Seltzer, and
R. Smogor. Layering in provenance systems. In Proc.
USENIX ATC, 2009.
[23] K. Pan, S. Kim, and E. J. Whitehead Jr. Toward an
understanding of bug ﬁx patterns. Empirical Software
Engineering, 14(3):286–315, 2009.
[24] J. H. Perkins, S. Kim, S. Larsen, S. Amarasinghe,
J. Bachrach, M. Carbin, C. Pacheco, F. Sherwood,
S. Sidiroglou, G. Sullivan, W.-F. Wong, Y. Zibin,
M. D. Ernst, and M. Rinard. Automatically patching
errors in deployed software. In Proc. SOSP, 2009.
[25] J. Ruckert, J. Blendin, and D. Hausheer. Rasp: Using
OpenFlow to push overlay streams into the underlay.
In Proc. P2P, 2013.
[26] C. Scott, A. Panda, V. Brajkovic, G. Necula,
A. Krishnamurthy, and S. Shenker. Minimizing faulty
executions of distributed systems. In Proc. NSDI, Mar.
2016.
[27] C. Scott, A. Wundsam, B. Raghavan, A. Panda, A. Or,
J. Lai, E. Huang, Z. Liu, A. El-Hassany, S. Whitlock,
H. Acharya, K. Zariﬁs, and S. Shenker.
Troubleshooting blackbox SDN control software with
minimal causal sequences. In Proc. SIGCOMM, 2014.
[28] K. Shen, C. Stewart, C. Li, and X. Li.
Reference-driven performance anomaly identiﬁcation.
In Proc. SIGMETRICS, 2009.
[29] H. J. Wang, J. C. Platt, Y. Chen, R. Zhang, and Y.-M.
Wang. Automatic misconﬁguration troubleshooting
with PeerPressure. In Proc. OSDI, 2004.
[30] Y. Wu, M. Zhao, A. Haeberlen, W. Zhou, and B. T.
Loo. Diagnosing missing events in distributed systems
with negative provenance. In Proc. SIGCOMM, 2014.
[31] A. Wundsam, D. Levin, S. Seetharaman, and
A. Feldmann. OFRewind: Enabling record and replay
troubleshooting for networks. In Proc. ATC, 2011.
[32] H. Zeng, P. Kazemian, G. Varghese, and
N. McKeown. Automatic test packet generation. In
Proc. CoNEXT, 2012.
[33] J. Zhang, L. Renganarayana, X. Zhang, N. Ge,
V. Bala, T. Xu, and Y. Zhou. EnCore: Exploiting
system environment and correlation information for
misconﬁguration detection. In Proc. ASPLOS, 2014.
[34] W. Zhou, Q. Fei, A. Narayan, A. Haeberlen, B. T. Loo,
and M. Sherr. Secure network provenance. In Proc.
SOSP, Oct. 2011.
[35] W. Zhou, S. Mapara, Y. Ren, Y. Li, A. Haeberlen,
Z. Ives, B. T. Loo, and M. Sherr. Distributed
time-aware provenance. In Proc. VLDB, Aug. 2013.
[36] W. Zhou, M. Sherr, T. Tao, X. Li, B. T. Loo, and
Y. Mao. Efﬁcient querying and maintenance of
network provenance at Internet-scale. In Proc.
SIGMOD, 2010.
[37] Y. Zhu, N. Kang, J. Cao, A. Greenberg, G. Lu,
R. Mahajan, D. Maltz, L. Yuan, M. Zhang, B. Y. Zhao,
and H. Zheng. Packet-level telemetry in large
datacenter networks. In Proc. SIGCOMM, Aug. 2015.