the configuration is adjusted across all honeypots to reduce the
data rate of all 549 servers towards the IP address to a maximum
of 0.5 Mbps. This value was chosen to result in a minimal impact
and ethical operation of the honeypot system. In several rounds of
tests before the actual experiment, we verified whether adversaries
would detect this traffic shaping and subsequently stopped using
these systems. This was not the case. We used only honeypots at
Digital Ocean for these tests and ran the experiments on different
IP addresses. As we find no statistical differences between cloud
locations, this verification did not bias our experimental setup.
Experimental Design. In order to discover how adversaries select
their infrastructure and victims as well as perform their implemen-
tation and execution, we rolled out different configurations to parts
of our honeynet. Specifically, we are interested in the influence of:
(1) Amplification Factor: do adversaries search for and focus
on servers with the highest amplification?
(2) Obvious Honeypots: are adversaries monitoring and dis-
carding servers if they do not fully implement the protocols
or even explicitly announce to be a honeypot?
(3) Suspicious systems: are adversaries actively investigating
the system, and do they behave differently if an unbelievable
number of open services is installed?
(4) Packet Loss: do adversaries test and discard services if they
are not constantly answering or drop requests?
(5) Packet Delay: do adversaries select servers based on the
response time and speed?
We have structured our system as follows: honeypots are split
into four groups with low and high amplification ratios providing a
correct response, and low and high amplification ratios providing a
response that is obviously a honeypot by replying with a message
such as “This is a honeypot attack detector, usage is logged and rate-
limited”. So that size is not a confounding variable; these responses
of the obvious honeypots are equally large as real responses. Addi-
tionally, we vary the number of services operating on the servers.
To ensure that the results are not biased due to the location of the
honeypots, we distributed groups equally over five different clouds
and availability zones. Throughout the experiment, we alter the be-
havior of this setup, for example, by dropping packets or delaying
the responses, to identify the reaction of the adversaries.
Ethical considerations. DDoS amplification honeypots run the
risk of participating in attacks when being used by adversaries. In
the past, honeypot systems were typically implemented to drop at-
tack packets entirely [27], the same method is used for other works
[13, 17, 32], however, this comes with the complication that the re-
search community would not be able to observe the more sophisti-
cated actors who would consequently abandon honeypots after test-
ing. In order to balance the need to understand the threat landscape
and actor actors while not irresponsibly participating in DDoS at-
tacks, we implemented a token bucket as a flow shaper in front of
the honeypots. Figure 3 visualizes the concept, where at a specific
Table 1: Response sizes of the honeypot setup in bytes and
the Bandwidth Amplification Factor (BAF) [27] per proto-
col, the allocation over cloud providers and different exper-
iments. Response contents are listed in Appendix A.
RIP
CharGen
QOTD
SSDP
NTP
DNS
Responses
Bytes
Max BAF
Bytes
Max BAF
Bytes
Max BAF
Bytes
Max BAF
Bytes
Max BAF
Bytes
Max BAF
Cloud provider
AWS
Azure
Digital Ocean
Google
OVH
Experiment
Single protocol
Packet loss
Packet delay
Total servers
171
24
166
124
64
Total servers
120
60
60
Real small
84
3.5
94
94
45-50
45-50
272
2.3
Varies
0
83
1.6
Real small
47
6
45
32
16
Real small
30
15
15
Real large
524
21.8
1,406
1,406
1,450
1,450
430
3.7
Varies
46+
Varies
1.6+
Real large
52
6
49
34
16
Real large
30
15
15
Fake small
88
3.7
94
94
53
53
277
2.4
Varies
0
83
1.6
Fake small
36
6
36
29
16
Fake small
30
15
15
Fake large
413
17.2
1,450
1,450
1,437
1,437
429
3.7
347
43.4
352
6.8
Fake large
36
6
36
29
16
Fake large
30
15
15
Table 2: Timeline of data collection phases.
Phase
Passive (scanning baseline)
Active
Passive (attacker memory)
From
31-08-2019
07-09-2019
27-09-2019
To
07-09-2019
27-09-2019
30-11-2019
rate, tokens are being dropped into a bucket of maximum size. Every
packet that leaves our amplification honeypots consumes a token,
and if none are available, the packet is dropped. Token buckets have
the advantage that they can be configured to throttle attack traffic
aggressively but at the same time allow short temporary bursts of
activity that we would see during testing. Adversaries would there-
fore consider the honeypot as a normal server. However, during an
attack, we would not create a significant impact, leading to a loss of
attack power for the adversary when using our system instead of a
real amplification server. How we behave during testing and sus-
tained attack is governed by the fill rate and bucket size. We chose
a bucket depth for a peak of 25 packets per second, which we exper-
imentally determined in our pre-study. This value was 20% above
the maximum traffic we observed from adversaries during testing.
Thus the honeypots would pass all testing procedures we observed.
After the bucket is emptied, the combined attack force of our hon-
eypot drops down to a maximum packet rate of 0.5 Mbps, which is
less than 1
160 of the average Internet speed [2]. Sending a fraction
of the traffic that a single Internet user could send, we are not ac-
tively hampering victim systems. At the same time, we are in prac-
tice able to collect data on adversarial behavior in DDoS attacks.
Session 3D: DoS CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea943Figure 3: The token bucket limits the maximum outgoing
flow, but preserves unseen temporary bursts and testing.
We have received approval from our IRB for the design, and ad-
ditionally from our information security officer (ISO) whether the
honeypots, rate limiting, and maximum data rates were responsible
choices. No further requirements were put forth, and the experi-
mental setup and data rates were confirmed as adequate choices.
4 DATASETS
Our 549 honeypots were distributed in the public clouds of the five
largest cloud providers Google, Amazon, OVH, DigitalOcean, and
Microsoft, and placed in availability zones in North America, Eu-
rope, Australia, and Asia between 31 August 2019 and 30 Novem-
ber 2019. The distribution of different servers over cloud providers
and experiment groups is listed in Table 1. As we are interested to
understand the entire chain of adversarial preparation and mainte-
nance behavior and understand if adversaries work ad-hoc or build
repositories of known servers, the honeypots were not activated
immediately on the first day. Instead, for the first week, honeypots
merely recorded all scanning activity without responding to estab-
lish a baseline. Subsequently, the honeypots actively answered re-
quests for 20 days. Finally, we again switched to passively record-
ing scan and attack packets that were directed towards us for an-
other nine weeks (see Table 2) to understand the “memory” of the
attack landscape, whether attackers use previously discovered am-
plifiers even though they are not active.
Aside from the application and link-layer traces from the honey-
pots, we used two additional datasets to further quantify how ad-
versaries scan the Internet for open amplification services. While
these datasets do not provide additional insights into amplification
attacks, they do provide insights into the reconnaissance phases of
an attack. First, a large network telescope of three partially popu-
lated /16 networks with 65,000 IP addresses providing a historical
record of scan traffic for the past five years, and second, scan activ-
ity against the distributed telescope of the provider Greynoise. This
allows us to differentiate whether attackers focus on select public
clouds or perform broad scans across the entire Internet.
The number of honeypots needed to perform accurate mea-
surements is much higher than previously believed. As we
will show later, the threat landscape of actors performing amplifi-
cation DDoS attacks is highly heterogeneous regarding techniques
and resources applied. For instance, adversaries often do not ex-
haustively search the Internet for all open, amplifiable services.
However, they are content with small sets of amplification servers,
Figure 4: New information per extra honeypot. Dotted lines
show the total number of attacks identified and regular lines
show the extra information added per honeypot. Vertical
lines indicate knowledge of 99% of attacks.
since on average, only 41 of our 549 honeypots were used in a given
attack campaign. This means that to obtain a good overview of the
ecosystem, it is necessary to operate many honeypots to capture
small attacks and avoid bias towards adversaries conducting mas-
sive trawling through the entire Internet. Figure 4 shows the con-
vergence of the spectrum of attacks seen as a function of honey-
pots in operation. Even in comparatively simple protocols such as
NTP and quote-of-the-day (QOTD) where DDoS attacks presum-
ably all look similar, the heterogeneity when looking at the entire
sequence of attack steps is so large that as many as 150 honeypots
are needed to capture 99% of actor behavior. This shows the con-
stant evolution of the ecosystem, as previous work from 2015 iden-
tified that the majority of attacks were captured in 21 honeypots
[17]. The honeynet size of more than 500 servers – an order of mag-
nitude more than in previous studies – is thus necessary to provide
a good understanding of the DDoS threat landscape.
5 AMPLIFICATION DDOS IN THE WILD
When we activated our honeypots after the week-long baseline, it
took mere hours for the first adversaries to abuse our infrastructure
in attacks. During its 20-day activation, we recorded 13,479 separate
attack campaigns, targeting 8,315 unique source IPs located in
4,340 /24 subnets. Altogether, our honeypots collected 448 GB in
amplification requests attributed to attacks, for which we generated
on average 0.12 Mbps from our system towards a victim. In this
section, we discuss these attacks using the model from figure 1.
5.1 Capability development
To perform an attack, an adversary first needs to develop a capability