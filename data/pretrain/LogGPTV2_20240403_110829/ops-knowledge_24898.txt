User: I need to sequentially read a large text files, storing a lot of data in memory, and then use them to write a large file. These read/write cycles are done one at a time, and there is no common data, so I don't need any of the memory to be shared between them. 
I tried putting these procedures in a single script, hoping that the garbage collector would delete the old, no-longer-needed objects when the RAM got full. However, this was not the case. Even when I explicitly deleted the objects between cycles it would take far longer than running the procedures separately. 
Specifically, the process would hang, using all available RAM but almost no CPU. It also hung when gc.collect() was called. So, I decided to split each read/write procedure into separate scripts and call them from a central script using execfile(). This didn't fix anything, sadly; the memory still piled up. 
I've used the simple, obvious solution, which is to simply call the subscripts from a shell script rather than using execfile(). However, I would like to know if there is a way to make this work. Any input?
Assistant: Usually in this kind of situation, refactoring is the only way out.
You mentioned you're storing a lot in memory, perhaps in a dict or a set, then output onto only one file.
Maybe you can append output to the output file after processing each input, then do a quick clean-up before processing new input file. That way, RAM usage can be reduced.
Appending can even be done line by line from input, so that no storage is needed.
Since I don't know the specific algorithm you're using, given you mentioned no sharing between files is needed, this may help. Remember to flush output too :P