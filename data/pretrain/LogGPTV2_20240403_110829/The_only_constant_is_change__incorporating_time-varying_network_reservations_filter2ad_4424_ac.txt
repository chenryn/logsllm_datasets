Figure 6 shows the bandwidth proﬁles for the four applications
shown in Figure 1 but under the no-elongation threshold bandwidth
cap, and the corresponding TIVC models generated for these pro-
ﬁles. We make two observations. First, compared to Figure 1, using
threshold bandwidth capping did not elongate the application exe-
cution, but smoothed the trafﬁc peaks. Second, the TIVC models
generated for the four applications are of Types 1, 2, 3 and 3, as
expected, and achieve 22.9%, 8.9%, 13.1%, and 6.3% efﬁciencies,
respectively.
4.3 Model Reﬁnement
The Type 3 TIVC model generated for a given application trafﬁc
proﬁle consists of square curves of two different bandwidth limits,
base bandwidth Bb during valleys and capping bandwidth BCap
during peaks. A close look at the individual pulses and valleys in
Figure 6 reveals two ﬁndings. First. if we calculate the efﬁciencies
of different pulses (shown in Figure 6 next to each pulse), the val-
ues can differ signiﬁcantly, with the peak efﬁciency being around
30%. Second, the proﬁle for the same application under a lower
bandwidth cap (not shown due to page limit) shows the pulses with
the highest efﬁciency get elongated ﬁrst, and with low efﬁciencies
can sustain a lower bandwidth cap without being elongated.
To improve the efﬁciency of the TIVC model, which allows the
cloud provider to potentially ﬁt more jobs in the datacenter, we re-
ﬁne the generated Type 3 model by lowering the bandwidth cap for
pulses that have very low bandwidth efﬁciencies, using the follow-
ing heuristic. If the efﬁciency is lower than a threshold γ, we lower
the bandwidth cap so that the efﬁciency is around α. We empir-
ically found for all the applications we studied, setting γ = 8%
and α = 20% is sufﬁciently conservative, i.e., it will not elongate
the pulses when running under the new bandwidth caps. Figure 7
203Cap 400Mbps
Cap 10Mbps
Cap 300Mbps
Cap 350Mbps
)
s
p
b
M
(
t
u
p
h
g
u
o
r
h
T
 800
 600
 400
 200
 0
23%
)
s
p
b
M
(
t
u
p
h
g
u
o
r
h
T
 50
 40
 30
 20
 10
 0
15%
33%
24%
30%
28%
34%
30%
25%
16%
36%
34%
30%
29%
26%
31%
34%
36%
)
s
p
b
M
(
t
u
p
h
g
u
o
r
h
T
 800
 600
 400
 200
 0
33%
35%
6%
6%
10%
36%
34%
)
s
p
b
M
(
t
u
p
h
g
u
o
r
h
T
 800
 600
 400
 200
 0
3%
10%
 0  50  100 150 200 250 300 350 400
 0  100  200  300  400  500  600  700
 0  100  200  300  400  500  600  700
 0
 100  200  300  400  500  600
Time (sec)
Time (sec)
Time (sec)
Time (sec)
(a) Sort, 4 GB input per VM.
(b) Word Count, 2 GB per VM.
(c) Hive Join, 6 GB per VM.
(d) Hive Aggre., 2 GB per VM.
Figure 6: Bandwidth proﬁles under no-elongation threshold bandwidth cap and their Type 3 models.
Cap 300Mbps
Cap 350Mbps
)
s
p
b
M
(
t
u
p
h
g
u
o
r
h
T
 800
 600
 400
 200
 0
31%
35%
20%
20% 10%
37%
34%
 0  100 200 300 400 500 600 700
Time (sec)
)
s
p
b
M
(
t
u
p
h
g
u
o
r
h
T
 800
 600
 400
 200
 0
20%
10%
 0  100  200  300  400  500  600
Time (sec)
(a) Hive Join reﬁned model.
(b) Hive Aggre. reﬁned model.
Figure 7: Hive Join and Hive Aggre. reﬁned model.
shows the Type 4 models for Hive Join and Hive Aggre. which are
reﬁned from their corresponding models in Figure 6. We do not
claim the two threshold values to be general and suitable thresh-
old values are likely dependent on the networking and computation
mix of the class of applications. Rather, the key point is that Type
4 models can be systematically derived by reﬁning Type 3 models
from lowering the bandwidth cap for pulses with low efﬁciencies.
4.4 Incorporating Model Offsets
A ﬁnal challenge faced by TIVC model generation is concerned
with the TIVC models for different VMs of the same job. There
are two related questions: how much do the trafﬁc demand and
hence the TIVC models generated for different VMs differ, and if
the TIVC models are the same, how well are their timings aligned.
Our proﬁling study shows that for the set of MapReduce jobs, the
trafﬁc demand and hence the TIVC models generated across the
VMs are of the same type and with the same number of pulses.
However, the rising and falling edges of the pulses can be offset,
potentially due to the delay in task dispatching from the task sched-
uler. Table 1 lists the standard deviation of the rise and fall timings
of the pulses for the four applications. We see that the standard
deviation of the pulses across 32 VMs is less than 9 seconds.
The above small misalignment between TIVCs of different VMs
suggests that instead of generating individual TIVCs for the VMs,
we can generate a single TIVC which is easier to provision, as
follows. We ﬁrst process per-VM proﬁles to generate per-VM
reﬁned TIVC models. We then calculate the max of their base
bandwidths, and regenerate per-VM TIVC models using this new
base bandwidth. Next we merge the new per-VM TIVC mod-
els by merging the corresponding pulses, taking the max of their
widths and heights. Figures 8(a)-(c) show the trafﬁc proﬁles and
TIVCs for three randomly sampled VMs and Figure 8(d) shows the
merged TIVC, for Hive Join. Since the threshold bandwidth cap for
Word Count, 10Mbps, is already very low, it should be provisioned
throughout the application execution. The ﬁnal TIVC models gen-
erated for the rest three applications are shown in Table 2.
4.5 Discussions
We discuss the generality and limitation of our proﬁling-based
model generation. Our model generation uses the enveloping tech-
nique (§4.4) to tolerate small offsets among the trafﬁc demand
by different VMs of a job. This works well for the class of
Table 1: TIVC offsets, measured as the standard deviation of
the rise/fall timings of pulses across 32 VMs.
Pulse 1 (rise, fall)
Pulse 2 (rise, fall)
Pulse 3 (rise, fall)
Pulse 4 (rise, fall)
Pulse 5 (rise, fall)
Pulse 6 (rise, fall)
Pulse 7 (rise, fall)
Sort
5.8, 7.9
N/A
N/A
N/A
N/A
N/A
N/A
WC
3.5, 4.0
5.3, 5.6
5.4, 5.8
5.1, 5.5
5.7, 5.4
5.5, 5.5
5.3, 5.5
0.1, 2.3
0.1, 0.3
Hive Join Hive Aggre.
2.0, 3.2
8.1, 3.6
1.9, 2.7
1.9, 2.4
1.9, 2.4
1.9, 2.4
2.1, 2.6
N/A
N/A
N/A
N/A
N/A
MapReduce-type applications which tend to be highly regular in
nature – the worker VMs are performing similar tasks and hence
are likely to generate trafﬁc of similar volume and at similar times.
For applications that generate non-uniform trafﬁc, we can generate
and enforce per-VM TIVC models.
Our approach assumes the input data size per VM stays the same
during proﬁling runs and production runs. One potential source of
variation between the trafﬁc patterns during proﬁling runs and pro-
duction runs is the input data, as the processing time of different
data items and hence across the VMs could be uneven. In our ex-
periments with input data generated using random seeds, we did
not ﬁnd much difference in the trafﬁc characteristics across runs.
Speciﬁcally, the standard deviation of the pulse edge timings across
5 runs of the four applications using input data generated using ran-
dom seeds is less than 10 seconds (not shown due to page limit).
In general, it is important to validate this assumption for any can-
didate application across multiple sample proﬁle runs before using
the TIVC models.
We envision the primary use scenarios of TIVC models are when
customers repeatedly run the same type of jobs with the same input
size (and hence same number of VMs), with potentially similar data
sets from run to run. Such a scenario is common in iterative data
processing (e.g., [14]) such as PageRank [29], HITS (Hypertext-
Induced Topic Search) [25], recursive relational queries [12], so-
cial network analysis, and network trafﬁc analysis where much of
the data stay unchanged from iteration to iteration, and is observed
in many production environments (e.g., in Bing’s production clus-
ters [6]), where the same job needs to be repeated day in and day
out, and the data change slightly. In such scenarios, the jobs could
be proﬁled on each run or periodically, with the TIVC models gen-
erated to help schedule the cluster during the next run.
Finally, as with all other network reservation approaches
(e.g., [20, 11]), TIVC faces a number of uncertainties during job
execution: the data processed by each VM (e.g., a map task) may
not be on the local disk, a job execution may experience strag-
glers or task failures which require some tasks to be re-executed
(e.g., [9]), or some VMs or network elements may fail. In general,
predictable performance in the presence of such uncertainties re-
quires adding fault tolerance to applications and overprovisioning
of not only network resources, but also extra VMs, for accommo-
dating backup tasks. We leave dealing with such uncertainties as
future work.
204)
s
p
b
M
(
t
u
p
h
g
u
o
r
h
T
 800
 600
 400
 200
 0
)
s
p
b
M
(
t
u
p
h
g
u
o
r
h
T
 800
 600
 400
 200
 0
)
s
p
b
M
(
t
u
p
h
g
u
o
r
h
T
 800
 600
 400
 200
 0
)
s
p
b
M
(
t
u
p
h
g
u
o
r
h
T