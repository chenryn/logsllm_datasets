### Bandwidth Profiles and TIVC Models

**Figure 6** illustrates the bandwidth profiles for the four applications shown in **Figure 1**, but under a no-elongation threshold bandwidth cap, along with the corresponding TIVC models generated for these profiles. Two key observations can be made:

1. **Traffic Smoothing**: Compared to **Figure 1**, the use of a threshold bandwidth cap did not elongate the application execution time but effectively smoothed out traffic peaks.
2. **TIVC Model Efficiency**: The TIVC models generated for the four applications are of Types 1, 2, 3, and 3, as expected. These models achieve efficiencies of 22.9%, 8.9%, 13.1%, and 6.3%, respectively.

### Model Refinement

The Type 3 TIVC model for a given application traffic profile consists of square curves with two different bandwidth limits: base bandwidth \( B_b \) during valleys and capping bandwidth \( B_{Cap} \) during peaks. A detailed examination of the individual pulses and valleys in **Figure 6** reveals two important findings:

1. **Efficiency Variability**: The efficiencies of different pulses (indicated next to each pulse in **Figure 6**) can vary significantly, with peak efficiency around 30%.
2. **Bandwidth Cap Impact**: For the same application, a lower bandwidth cap (not shown due to page constraints) causes the pulses with the highest efficiency to elongate first, while those with low efficiency can sustain a lower bandwidth cap without elongation.

To enhance the efficiency of the TIVC model, which allows the cloud provider to potentially fit more jobs in the datacenter, we refine the generated Type 3 model by lowering the bandwidth cap for pulses with very low bandwidth efficiencies. This is achieved using the following heuristic:
- If the efficiency is below a threshold \( \gamma \), we reduce the bandwidth cap so that the efficiency is around \( \alpha \).
- Empirically, for all the applications studied, setting \( \gamma = 8\% \) and \( \alpha = 20\% \) is sufficiently conservative, ensuring that the pulses do not elongate under the new bandwidth caps.

**Figure 7** demonstrates the refined Type 4 models for Hive Join and Hive Aggregation, derived from their corresponding models in **Figure 6**. While the threshold values may not be universally applicable, they serve to illustrate that Type 4 models can be systematically derived by refining Type 3 models through the adjustment of the bandwidth cap for low-efficiency pulses.

### Incorporating Model Offsets

A significant challenge in TIVC model generation is the alignment of TIVC models for different VMs of the same job. Two key questions arise:
- How much do the traffic demands and TIVC models differ across VMs?
- If the TIVC models are the same, how well are their timings aligned?

Our profiling study indicates that for MapReduce jobs, the traffic demand and TIVC models across VMs are of the same type and have the same number of pulses. However, the rising and falling edges of the pulses can be offset, possibly due to delays in task dispatching from the task scheduler. **Table 1** lists the standard deviation of the rise and fall timings of the pulses for the four applications, showing that the standard deviation across 32 VMs is less than 9 seconds.

This small misalignment suggests that instead of generating individual TIVCs for each VM, a single TIVC can be generated, which is easier to provision. The process involves:
1. Generating per-VM refined TIVC models.
2. Calculating the maximum of their base bandwidths.
3. Regenerating per-VM TIVC models using the new base bandwidth.
4. Merging the new per-VM TIVC models by taking the maximum of their widths and heights.

**Figures 8(a)-(c)** show the traffic profiles and TIVCs for three randomly sampled VMs, and **Figure 8(d)** shows the merged TIVC for Hive Join. For Word Count, with a threshold bandwidth cap of 10 Mbps, it should be provisioned throughout the application execution. The final TIVC models generated for the other three applications are presented in **Table 2**.

### Discussions

We discuss the generality and limitations of our profiling-based model generation. Our approach uses the enveloping technique (ยง4.4) to tolerate small offsets among the traffic demands of different VMs of a job. This works well for highly regular MapReduce-type applications, where worker VMs perform similar tasks and generate similar traffic volumes at similar times. For applications with non-uniform traffic, per-VM TIVC models can be generated and enforced.

Our method assumes that the input data size per VM remains constant during profiling and production runs. Potential variations in traffic patterns between profiling and production runs could arise from differences in input data, as processing times for different data items and across VMs may be uneven. In our experiments with randomly generated input data, we observed minimal differences in traffic characteristics across runs, with the standard deviation of pulse edge timings across five runs being less than 10 seconds.

In general, it is crucial to validate this assumption for any candidate application across multiple sample profile runs before using the TIVC models. We envision the primary use scenarios for TIVC models in cases where customers repeatedly run the same type of jobs with the same input size and similar datasets, such as iterative data processing (e.g., PageRank, HITS, recursive relational queries, social network analysis, and network traffic analysis). In such scenarios, the jobs can be profiled on each run or periodically, with TIVC models generated to help schedule the cluster during the next run.

Finally, like other network reservation approaches, TIVC faces uncertainties during job execution, such as data not being on the local disk, stragglers, task failures, and network element failures. Predictable performance in the presence of such uncertainties requires fault tolerance and overprovisioning of both network resources and extra VMs. Addressing these uncertainties is left for future work.