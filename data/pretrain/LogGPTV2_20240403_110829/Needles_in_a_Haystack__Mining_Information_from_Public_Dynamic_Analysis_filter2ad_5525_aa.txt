title:Needles in a Haystack: Mining Information from Public Dynamic Analysis
Sandboxes for Malware Intelligence
author:Mariano Graziano and
Davide Canali and
Leyla Bilge and
Andrea Lanzi and
Davide Balzarotti
Needles in a Haystack:  
Mining Information from Public Dynamic Analysis 
Sandboxes for Malware Intelligence
Mariano Graziano and Davide Canali, Eurecom; Leyla Bilge, Symantec Research Labs; 
Andrea Lanzi, Universitá degli Studi di Milano; Davide Balzarotti, Eurecom 
https://www.usenix.org/conference/usenixsecurity15/technical-sessions/presentation/graziano
This paper is included in the Proceedings of the 
24th USENIX Security Symposium
August 12–14, 2015 • Washington, D.C.
ISBN  978-1-939133-11-3
Open access to the Proceedings of  the 24th USENIX Security Symposium is sponsored by USENIXNeedles in a Haystack:
Mining Information from Public Dynamic Analysis
Sandboxes for Malware Intelligence
Mariano Graziano
Eurecom
Davide Canali
Eurecom
Andrea Lanzi
Universita’ degli Studi di Milano
Leyla Bilge
Symantec Research Labs
Davide Balzarotti
Eurecom
Abstract
Malware sandboxes are automated dynamic analysis
systems that execute programs in a controlled environ-
ment. Within the large volumes of samples submitted
every day to these services, some submissions appear to
be different from others, and show interesting character-
istics. For example, we observed that malware samples
involved in famous targeted attacks – like the Regin APT
framework or the recently disclosed malwares from the
Equation Group – were submitted to our sandbox months
or even years before they were detected in the wild. In
other cases, the malware developers themselves interact
with public sandboxes to test their creations or to develop
a new evasion technique. We refer to similar cases as
malware developments.
In this paper, we propose a novel methodology to au-
tomatically identify malware development cases from the
samples submitted to a malware analysis sandbox. The
results of our experiments show that, by combining dy-
namic and static analysis with features based on the ﬁle
submission, it is possible to achieve a good accuracy in
automatically identifying cases of malware development.
Our goal is to raise awareness on this problem and on the
importance of looking at these samples from an intelli-
gence and threat prevention point of view.
1
Introduction
Malware sandboxes are automated dynamic analysis
tools that execute samples in an isolated and instru-
mented environment. Security researchers use them to
quickly collect information about the behavior of suspi-
cious samples, typically in terms of their execution traces
and API calls. While customized sandboxes are often
installed in the premises of security companies, some
sandboxes are available as public online services, as it is
the case for Malwr [13], Anubis [10], ThreatExpert [14],
VirusTotal [16], and many others [5, 18, 4, 6, 15, 1, 3]
The main advantage of these systems is the fact that
the analysis is completely automated and easily paral-
lelizable, thus providing a way to cope with the over-
whelming number of new samples that are collected ev-
ery day. However, due to this extreme parallelization,
an incredible amount of reports are generated every day.
This makes the task of distinguishing new and important
malware from the background noise of polymorphic and
uninteresting samples very challenging.
In particular, two important and distinct observations
motivate our work. First, it is relatively common that
malware samples used to carry out famous targeted at-
tacks were collected by antivirus companies or public
sandboxes long before the attacks were publicly dis-
covered [25]. For instance, the binaries responsible for
operation Aurora, Red October, Regin, and even some
of the new one part of the Equation Group were sub-
mitted to the sandbox we used in our experiments sev-
eral months before the respective attacks appeared in the
news [11, 40, 17, 50, 45, 35]. The reasons behind this
phenomenon are not always clear. It is possible that the
ﬁles were automatically collected as part of an automated
network or host-based protection system. Or maybe a
security analyst noticed something anomalous on a com-
puter and wanted to double-check if a suspicious ﬁle ex-
hibited a potentially malicious behavior. It is even pos-
sible that the malware developers themselves submitted
an early copy of their work to verify whether it triggered
any alert on the sandbox system. Whatever the reason,
the important point is that no one paid attention to those
ﬁles until it was too late.
The second observation motivating our study is the
constant arm race between the researchers that put con-
tinuous effort to randomize their analysis environments,
and the criminals that try to ﬁngerprint those systems
to avoid being detected. As a consequence of this hid-
den battle, malware and packers often include evasion
techniques for popular sandboxes [19] and updated in-
formation about the internal sandbox details are regu-
USENIX Association  
24th USENIX Security Symposium  1057
1
larly posted on public websites [2]. These examples
prove that there must be a constant interaction between
malware developers and popular public malware analysis
services. This interaction is driven by the need to collect
updated information as well as to make sure that new
malware creation would go undetected. Even though de-
tecting this interaction might be very difﬁcult, we be-
lieve it would provide valuable information for malware
triage.
Up to the present, malware analysis services have col-
lected large volumes of data. This data has been used
both to enhance analysis techniques [23, 46] and to ex-
trapolate trends and statistics about the evolution of mal-
ware families [24]. Unfortunately, to the best of our
knowledge, these datasets have never been used to sys-
tematically study malware development and support mal-
ware intelligence on a large scale. The only public excep-
tion is a research recently conducted by looking at Virus-
Total to track the activity of speciﬁc high-proﬁle hacking
groups involved in APT campaigns [52, 27].
In this paper, we approach this objective by applying
data-mining and machine learning techniques to study
the data collected by Anubis Sandbox [10], a popular
malware dynamic analysis service. At the time we per-
formed our analysis, the dataset contained the analysis
reports for over 30 millions unique samples. Our main
goal is to automatically detect if miscreants submit their
samples during the malware development phase and, if
this is the case, to acquire more insights about the dy-
namics of malware development. By analyzing the meta-
data associated to the sample submissions, it might be
possible to determine the software provenance and im-
plement an early-warning system to ﬂag suspicious sub-
mission behaviors.
It is important to understand that our objective is not
to develop a full-ﬂedged system, but instead to explore a
new direction and to show that by combining metadata
with static and dynamic features it is possible to suc-
cessfully detect many examples of malware development
submitted to public sandboxes. In fact, our simple pro-
totype was able to automatically identify thousands of
development cases, including botnets, keyloggers, back-
doors, and over a thousand unique trojan applications.
to make sure that a certain evasion technique works as
expected in the sandbox environment, or that a certain
malware prototype does not raise any alarm.
In this paper, we focus on the detection of what we call
malware development. We use the term “development”
in a broad sense, to include anything that is submitted by
the author of the ﬁle itself. In many cases the author has
access to the source code of the program – either because
she wrote it herself or because she acquired it from some-
one else. However, this is not always the case, e.g., when
the author of a sample uses a builder tool to automatically
generate a binary according to a number of optional con-
ﬁgurations (see Section 6 for a practical example of this
scenario). Moreover, to keep things simple, we also use
the word “malware” as a generic term to model any sus-
picious program. This deﬁnition includes traditional ma-
licious samples, but also attack tools, packers, and small
probes written with the only goal of exﬁltrating informa-
tion about the sandbox internals.
Our main goal is to automatically detect suspicious
submissions that are likely related to malware develop-
ment or to a misuse of the public sandbox. We also want
to use the collected information for malware intelligence.
In this context, intelligence means a process, supported
by data analysis, that helps an analyst to infer the moti-
vation, intent, and possibly the identity of the attacker.
Our analysis consists of ﬁve different phases. In the
ﬁrst phase, we ﬁlter out the samples that are not inter-
esting for our analysis. Since the rest of the analysis is
quite time-consuming, any sample that cannot be related
to malware development or that we cannot process with
our current prototype is discarded at this phase. In the
second phase, we cluster the remaining samples based
on their binary similarity. Samples in each cluster are
then compared using a more ﬁne-grained static analysis
technique. Afterwards, we collect six sets of features,
based respectively on static characteristics of the submit-
ted ﬁles, on the results of the dynamic execution of the
samples in the cluster, and on the metadata associated to
the samples submissions. This features are ﬁnally pro-
vided to a classiﬁer that we previously trained to identify
the malware development clusters.
2 Overview and Terminology
There are several reasons why criminals may want to in-
teract with an online malware sandbox. It could be just
for curiosity, in order to better understand the analysis
environment and estimate its capabilities. Another rea-
son could be to try to escape from the sandbox isolation
to perform some malicious activity, such as scanning a
network or attacking another machine. Finally, criminals
may also want to submit samples for testing purposes,
3 Data reduction
The ﬁrst phase of our study has the objective of reducing
the amount of data by ﬁltering out all the samples that
are not relevant for our analysis. We assume that a cer-
tain ﬁle could be a candidate for malware development
only if two conditions are met. First, the sample must
have been submitted to the public sandbox before it was
observed in the wild. Second, it has to be part of a man-
ual submission done by an individual user – and not, for
example, originating from a batch submission of a secu-
1058  24th USENIX Security Symposium 
USENIX Association
2
rity company or from an automated malware collection
or protection system.
We started by ﬁltering out the large number of batch
submissions Anubis Sandbox receives from several re-
searchers, security labs, companies, universities and reg-
istered users that regularly submit large bulks of binaries.
As summarized in Table 1, with this step we managed to
reduce the data from 32 million to around 6.6 million
binaries. These samples have been collected by Anubis
Sandbox from 2006 to 2013.
Then, to isolate the new ﬁles that were never observed
in the wild, we applied a two-step approach. First, we re-
moved those submissions that, while performed by single
users, were already part of a previous batch submission.
This reduced the size of the dataset to half a million sam-
ples. In the second step, we removed the ﬁles that were
uploaded to the sandbox after they were observed by two
very large external data sources: Symantec’s Worldwide
Intelligence Network (WINE), and VirusTotal.
After removing corrupted or not executable ﬁles (e.g,
Linux binaries submitted to the Microsoft Windows
sandbox), we remained with 184,548 ﬁles that match our
initial deﬁnition of candidates for malware development.
Before sending them to the following stages of our anal-
ysis, we applied one more ﬁlter to remove the packed ap-
plications. The rationale behind this choice is very sim-
ple. As explained in Section 4, the majority of our fea-
tures work also on packed binaries, and, therefore, some
potential malware development can be identiﬁed also in
this category. However, it would be very hard for us to
verify our results without having access to the decom-
piled code of the application. Therefore, in this paper
we decided to focus on unpacked binaries, for which it
is possible to double-check the ﬁndings of our system.
The packed executables were identiﬁed by leveraging the
SigBuster [37] signatures.
Table 1 summarizes the number of binaries that are ﬁl-
tered out after each step. The ﬁltering phase reduced the
data to be analyzed from over 32 millions to just above
121,000 candidate ﬁles, submitted by a total of 68,250
distinct IP addresses. In the rest of this section we de-
scribe in more details the nature and role of the Symantec
and VirusTotal external sources.
Symantec Filter
Symantec Worldwide Intelligence Network Environment
(WINE) is a platform that allows researchers to perform
data intensive analysis on a wide range of cyber security
relevant datasets, collected from over a hundred million
hosts [28]. The data provided by WINE is very valuable
for the research community, because these hosts are com-
puters that are actively used by real users which are po-
Dataset
Initial Dataset
Submitted by regular users
Not already part of large submissions
Previously unknown by Symantec
Previously unknown by VirusTotal
Proper executable ﬁles
Final (not packed binaries)
Submissions
32,294,094
6,660,022
522,699
420,750
214,321
184,548
121,856
Table 1: Number of submissions present in our dataset at
each data reduction step.
tential victims of various cyber threats. WINE adopts a
1:16 sampling on this large-scale data such that all types
of complex experiments can be held at scale.
To ﬁlter out from our analysis the binaries that are
not good candidates to belong to malware development,
we used two WINE datasets: the binary reputation and
the AntiVirus telemetry datasets. The binary reputation
dataset contains information about all of the executables
(both malicious and benign) downloaded by Symantec
customers over a period of approximately 5 years. To
preserve the user privacy, this data is collected only from
the users that gave explicit consent for it. At the time
we performed our study, the binary reputation dataset
included reports for over 400 millions of distinct bina-
ries. On the other hand, the AntiVirus telemetry dataset
records only the detections of known ﬁles that triggered
the Norton Antivirus Engine on the users’ machines.
The use of binary reputation helps us locating the exact
point in time in which a binary was ﬁrst disseminated in
the wild. The AntiVirus telemetry data provided instead
the ﬁrst time the security company deployed a signature
to detect the malware. We combined these datasets to
remove those ﬁles that had already been observed by
Symantec either before the submission to Anubis Sand-
box, or within 24 hours from the time they were ﬁrst sub-
mitted to the sandbox.
VirusTotal Filter
VirusTotal is a public service that provides virus scan re-
sults and additional information about hundreds of mil-
lions of analyzed ﬁles. In particular, it incorporates the
detection results of over 50 different AntiVirus engines
– thus providing a reliable estimation of whether a ﬁle
is benign or malicious. Please note that we fetched
the VirusTotal results for each ﬁle in our dataset several
months (and in some cases even years) after the ﬁle was
ﬁrst submitted. This ensures that the AV signatures were
up to date, and ﬁles were not misclassiﬁed just because
they belonged to a new or emerging malware family.
Among all the information VirusTotal provides about
USENIX Association  
24th USENIX Security Symposium  1059
3
binaries, the most important piece of information we in-