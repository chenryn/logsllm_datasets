# Needles in a Haystack: Mining Information from Public Dynamic Analysis Sandboxes for Malware Intelligence

**Authors:**  
- Mariano Graziano, Eurecom
- Davide Canali, Eurecom
- Leyla Bilge, Symantec Research Labs
- Andrea Lanzi, Università degli Studi di Milano
- Davide Balzarotti, Eurecom

**Source:**  
[Proceedings of the 24th USENIX Security Symposium](https://www.usenix.org/conference/usenixsecurity15/technical-sessions/presentation/graziano)  
August 12–14, 2015, Washington, D.C.  
ISBN: 978-1-939133-11-3

## Abstract

Malware sandboxes are automated dynamic analysis systems that execute programs in a controlled environment. Among the large volumes of samples submitted daily to these services, some submissions exhibit unique characteristics. For example, malware samples involved in high-profile targeted attacks, such as the Regin APT framework or those from the Equation Group, were submitted to public sandboxes months or even years before their detection in the wild. In other cases, malware developers use public sandboxes to test their creations or develop new evasion techniques, which we refer to as "malware development."

In this paper, we propose a novel methodology to automatically identify malware development cases from the samples submitted to a malware analysis sandbox. Our experiments show that by combining dynamic and static analysis with features based on file submission, it is possible to achieve high accuracy in identifying cases of malware development. Our goal is to raise awareness about this issue and highlight the importance of examining these samples from an intelligence and threat prevention perspective.

## 1. Introduction

Malware sandboxes are automated dynamic analysis tools that execute samples in an isolated and instrumented environment. Security researchers use them to quickly gather information about the behavior of suspicious samples, typically in terms of their execution traces and API calls. While customized sandboxes are often installed in security companies, some are available as public online services, such as Malwr, Anubis, ThreatExpert, VirusTotal, and others.

The main advantage of these systems is their complete automation and parallelizability, which helps manage the overwhelming number of new samples collected daily. However, this extreme parallelization generates a vast number of reports, making it challenging to distinguish new and important malware from the background noise of polymorphic and uninteresting samples.

Two key observations motivate our work:
1. **Early Detection of Targeted Attacks:** It is common for malware samples used in famous targeted attacks to be collected by antivirus companies or public sandboxes long before the attacks are publicly discovered. For instance, the binaries responsible for operations like Aurora, Red October, Regin, and some from the Equation Group were submitted to sandboxes several months before the respective attacks appeared in the news. The reasons behind this phenomenon are not always clear, but it could be due to automatic collection by protection systems, a security analyst's suspicion, or even the malware developers themselves testing their creations.
2. **Evasion Techniques:** There is a constant arms race between researchers who randomize their analysis environments and criminals who try to fingerprint these systems to avoid detection. As a result, malware and packers often include evasion techniques for popular sandboxes, and updated information about internal sandbox details is regularly posted on public websites. This interaction provides valuable insights into malware development and can help in early detection and prevention.

To date, malware analysis services have collected large volumes of data, which have been used to enhance analysis techniques and extrapolate trends about the evolution of malware families. However, to the best of our knowledge, these datasets have not been systematically used to study malware development and support large-scale malware intelligence. The only public exception is recent research that tracked the activity of specific high-profile hacking groups involved in APT campaigns using VirusTotal.

In this paper, we apply data-mining and machine learning techniques to study the data collected by Anubis Sandbox, a popular malware dynamic analysis service. At the time of our analysis, the dataset contained over 30 million unique sample reports. Our primary goal is to automatically detect if miscreants submit their samples during the malware development phase and to gain more insights into the dynamics of malware development. By analyzing the metadata associated with sample submissions, we aim to determine the software provenance and implement an early-warning system to flag suspicious submission behaviors.

Our objective is not to develop a full-fledged system but to explore a new direction and demonstrate that combining metadata with static and dynamic features can successfully detect many examples of malware development submitted to public sandboxes. Our prototype was able to automatically identify thousands of development cases, including botnets, keyloggers, backdoors, and over a thousand unique trojan applications.

## 2. Overview and Terminology

Criminals may interact with online malware sandboxes for various reasons, such as curiosity, understanding the analysis environment, attempting to escape sandbox isolation, or submitting samples for testing purposes. We focus on detecting what we call "malware development," which includes any submission by the author of the file, whether they have access to the source code or use a builder tool to generate the binary.

Our main goal is to automatically detect suspicious submissions likely related to malware development or misuse of the public sandbox and use the collected information for malware intelligence. Intelligence, in this context, involves a process supported by data analysis that helps infer the motivation, intent, and possibly the identity of the attacker.

Our analysis consists of five phases:
1. **Data Reduction:** Filter out samples not relevant for our analysis.
2. **Clustering:** Cluster the remaining samples based on binary similarity.
3. **Feature Extraction:** Collect six sets of features based on static and dynamic characteristics and metadata.
4. **Classification:** Use a classifier to identify malware development clusters.
5. **Validation:** Verify the results and refine the system.

## 3. Data Reduction

The first phase of our study aims to reduce the amount of data by filtering out samples that are not relevant for our analysis. We assume a file could be a candidate for malware development if two conditions are met:
1. The sample must have been submitted to the public sandbox before it was observed in the wild.
2. It must be part of a manual submission done by an individual user, not originating from a batch submission by a security company or an automated system.

We started by filtering out the large number of batch submissions received by Anubis Sandbox, reducing the data from 32 million to around 6.6 million binaries. These samples were collected from 2006 to 2013.

Next, we applied a two-step approach to isolate new files never observed in the wild:
1. Removed submissions that, while performed by single users, were already part of a previous batch submission, reducing the dataset to half a million samples.
2. Removed files uploaded to the sandbox after they were observed by two large external data sources: Symantec’s Worldwide Intelligence Network (WINE) and VirusTotal.

After removing corrupted or non-executable files, we were left with 184,548 files that matched our initial definition of candidates for malware development. Before proceeding to the next stages, we filtered out packed applications, focusing on unpacked binaries for which we could verify our results.

Table 1 summarizes the number of binaries filtered out at each step, reducing the data to just above 121,000 candidate files, submitted by 68,250 distinct IP addresses.

### Symantec Filter

Symantec’s WINE platform allows researchers to perform data-intensive analysis on cyber security datasets collected from over a hundred million hosts. The binary reputation dataset contains information about all executables downloaded by Symantec customers over approximately five years, while the AntiVirus telemetry dataset records detections of known files that triggered the Norton Antivirus Engine. We combined these datasets to remove files observed by Symantec either before or within 24 hours of their first submission to Anubis Sandbox.

### VirusTotal Filter

VirusTotal is a public service that provides virus scan results and additional information about hundreds of millions of analyzed files, incorporating the detection results of over 50 different antivirus engines. We fetched the VirusTotal results for each file in our dataset several months or even years after the file was first submitted, ensuring up-to-date AV signatures and avoiding misclassification due to new or emerging malware families.

Among the information provided by VirusTotal, the most important for our analysis is the detection status, which helps us filter out files that were already known to be malicious.

---

This revised version of the text is more structured, coherent, and professional, providing a clearer and more detailed overview of the research and its objectives.