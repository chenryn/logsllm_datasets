(cid:170)(cid:174)(cid:174)(cid:174)(cid:174)(cid:174)(cid:174)(cid:174)(cid:174)(cid:174)(cid:174)(cid:174)(cid:174)(cid:174)(cid:174)(cid:174)(cid:174)(cid:174)(cid:174)(cid:174)(cid:172)
xi =
→
READ_PHONE_STATE
. . .
INTERNET
intent.action.MAIN
. . .
vending.INSALL_REFERER
getSimSerialNumber
. . .
containsHeader
Ljave/util/HashMap
. . .
Landroid/app/ProgressDialog
364where xi ∈ Rd, and xij = {0, 1} (i.e., if app i includes feature j, then
xij = 1; otherwise, xij = 0).
2.3 Learning-based Classifier for Android
Malware Detection
The problem of machine learning based Android malware detection
: X → Y which assigns a label
can be stated in the form of: f
y ∈ Y (i.e., −1 or +1) to an input app x ∈ X through the learn-
ing function f . A general linear classification model for Android
malware detection can be thereby denoted as:
f = sign(f (X)) = sign(XT w + b),
(1)
where f is a vector, each of whose elements is the label (i.e., mali-
cious or benign) of an app to be predicted, each column of matrix X
is the feature vector of an app, w is the weight vector and b is the
biases. More specifically, a machine learning system on the basis
of a linear classifier can be formalized as an optimization problem
[44]:
argmin
f,w,b;ξ
1
2||y − f||2 +
1
2β
wT w +
1
2γ
bT b + ξT (f − XT w − b),
(2)
subject to Eq. (1), where y is the labeled information vector, ξ is
Lagrange multiplier which is a strategy for finding the local minima
2||y−f||2 subject to f−XT w−b = 0, β and γ are the regularization
of 1
parameters. Note that Eq. (2) is a general linear classifier (denoted as
Original-Classifier throughout the paper) consisting of specific loss
function and regularization terms. Without loss of generality, the
equation can be transformed into different linear models depending
on the choices of loss function and regularization terms, such as
Support Vector Machine (SVM).
3 ADVERSARIAL ATTACKS
In Android malware detection, a learning-based system is to detect
malicious apps based on the trained classification model and pre-
vent them from interfering users’ smart phones. On the contrast,
attackers would like to violate the security context by either (a)
allowing malicious apps to be misclassified as benign (integrity
attack) or (b) creating a denial of service in which benign apps are
incorrectly classified as malicious (availability attack) [3, 4]. In this
paper, we focus on the integrity attack, also called adversarial attack.
Adversarial attacks can generally be modeled as an optimization
problem: given an original malicious app x ∈ X+, the adversarial
attacks attempt to manipulate its features to be detected as benign
(i.e., x′ ∈ X−), with the minimal evasion cost. In this section, we
present how attackers can achieve such attacks.
3.1 Feature Manipulation
To conduct an adversarial attack, attackers would manipulate the
features of a malicious app to evade the detection. As described in
Section 2.2, given an Android app, after feature extraction, it can be
represented by a binary feature vector. Then a typical manipulation
can be either adding or eliminating a binary in the vector.
• Feature Addition. In this scenario, attackers can autonomously
inject a feature in the app (i.e., set 0 to 1). For example, they
can add permissions in the manifest file without influence on
other existing functionalities; they can also inject API calls in a
dead code or methods which will be never called by any invoke
instructions in the dex file.
• Feature Elimination. In this setting, attackers may hide or re-
move a feature from the app (i.e., set 1 to 0) while not affecting
the intrusive functionality they want to execute. For example, at-
tackers can hide the information stored as strings by encryption
and decrypting it at runtime.
Either feature addition or elimination, both settings should retain
the semantics and intrusive functionality of the original app after
manipulations. In such case, feature addition is easier and safer
when the injection is not directly executed by the app (as examples
shown above). However, if attackers want to inject a suspicious
API call to the dex file being executed by the app, it will be more
sophisticated and may influence the semantics of the app. Feature
elimination is usually more complicated, such as, removing permis-
sions from the manifest file is not always practical since it may limit
the functionalities of the app. Therefore, conducting an adversarial
attack that needs to manipulate a lot of features while not compro-
mising the malicious functionalities may not always be feasible. In
this respect, attackers may need to implement a well-crafted attack
by taking consideration of the evasion cost.
3.2 Evasion Cost
The evasion cost can be decided by the number of binaries that are
changed from x to x′ by attackers, which is denoted as
C(x′
, x) = ||cT (x′ − x)||p
p ,
(3)
where c is a vector whose element denotes the corresponding cost of
changing a feature, and p is a real number. The evasion cost function
can be considered as ℓ1-norm or ℓ2-norm depending on the feature
space. For attackers, the manipulation cost ci for each feature is
different. For example, the cost of removing a permission from the
manifest file is much higher than injecting an API call in a dead code
in the dex file. Therefore, the manipulation cost ci for each feature is
practically significant, which is determined by the feature type (e.g.,
permission vs. API call) and manipulation method (e.g., addition
vs. elimination). Furthermore, for the reasons aforementioned, it’s
impractical for attackers to modify a malicious app into benign at
any cost (i.e., manipulating a large number of features). Thus, there
is an upper limit of the maximum manipulations that can be made
to the original malicious app x. That is, the manipulation function
A(x) can be formulated as
A(x) =
x′
x
sign(f (x′)) = −1 and C(x′, x) ≤ δmax
otherwise
,
(4)
where the malicious app is manipulated to be misclassified as benign
only if the evasion cost is less than or equal to a maximum cost
δmax.
3.3 Attack Model
In practice, though attackers may know differently about the tar-
geted learning system [33], they always have the following two
competing objectives: (1) maximize the number of malicious apps
being classified as benign, and (2) minimize the evasion cost for
optimal attacks over the learning-based classifier [25]. Specifically,
(cid:40)
365which implies that the larger the weight of the feature trained by
the classifier and the lower the cost of the feature being manipu-
lated, the more important the feature to the attackers. Clearly, the
importance of a feature represents the possibility that an attacker
may manipulate it in an adversarial attack.
The rationale to construct a more secure classifier against the
adversarial attacks is to reduce the possibility of those important
features being selected for model construction. In other words,
those features the attackers tend to manipulate (i.e., features with
higher values of I(i)) may not present together in the learning
model, which will intuitively force attackers to manipulate a larger
number of other less important features (i.e., features with lower
values of I(i)) to evade the detection. In this way, the probability of
each feature being selected for constructing a classification model is
inversely proportional to its importance, that is, the more important
the feature is to attackers, the less possible it will be selected to
train the classifier. We formalize P(i), the probability of ith-feature
being selected, as:
P(i) ∝ λ
(7)
where λ is an adjustable parameter which can be empirically
decided based on the training data. When substituting Eq. (6) into
Eq. (7), the length of the probability is actually arbitrary long (e.g.,
|wi| = 0). To normalize P(i), we further define P(i) as:
I(i) ,
w = βXξ ,
P(i) = λci(1 − ρ|wi|),
(8)
where ρ (0 ,
where
¯xij =
(cid:40)
xij R(.) ≤ P(j)
otherwise
0
.
(12)
The proposed feature selection method for classifier construction
is named SecCLS, whose implementation is given in Algorithm 1.
Note that when P(i) (1 ≤ i ≤ d) is with the same value for each
feature, i.e., feature importances are evenly distributed, our pro-
posed feature selection method SecCLS is approximate to random
selection. Thus we can say, random feature selection method [5, 19]
is a lower bound of SecCLS. Our proposed feature selection method
SecCLS reduces the possibility of those features that attackers tend
to manipulate, which will accordingly force attackers to manipulate
a larger number of other features and thus be more resilient against
their attacks. For computational complexity of SecCLS, to get weight
vector w from Eq. (2) requires O(d
3) queries, while to form cost
vector c and calculate P both need O(d). Since we formalize n apps
as X, each column of which is the d-dimensional feature vector, to
get an updated training set ¯X from X requires O(nd) updates.
sifier in Eq. (2) can be updated as:
By using SecCLS, after feature selection, the learning-based clas-
argmin
¯f,w,b;ξ
1
2||y − ¯f||2 +
1
2β
wT w +
1
2γ
bT b + ξT (¯f − ¯XT w − b), (13)
i =1.
Algorithm 1: SecCLS – A novel feature selection method to
construct more secure classifier.
Input: Training data set D = {xi , yi}n