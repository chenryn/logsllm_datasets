6
Fig. 3. A malicious neighborhood graph, showing clients that got infected and joined a botnet (i.e., they started contacting the botnet’s C&C server). The
various tiny snippets of texts, which lists the URLs of all the candidates, can be ignored.
the graph, until the graph reaches its pre-set size limit (in our
experiments, 4,000 nodes), or we cannot grow it anymore.
At each step, we consider if any node in the graph has
some relation with entities that are not yet in the graph. If
there is any, we add the corresponding entity. We consider the
following relations:
•
•
•
•
URLs belonging to a domain or FQDN (resource
reuse)
Files being downloaded from the same URL
Domains/FQDNs being hosted on a server (resource
reuse)
URLs having the same path, or ﬁle name (availability,
ease of deployment)
Files being fetched by the same clients
•
For example, let’s build a graph surrounding the candi-
date domain 04cc.com. This domain is hosted on server
58.218.198.119, hence we add this host to the graph. This
server also hosts taobao91.com, as Figure 2 shows, so we
add the latter to the graph, and so on.
Finally, we apply some post-processing heuristics that
remove the parts of the graph that do not convey any useful
information for the subsequent metric calculation, such as
clients that are leaves in the graph with a single parent.
While building the graph, we employ a few methods to
ensure that the graph covers a sizeable portion of the malicious
infrastructure it represents. We do so to avoid, for example
the graph expansion iteration keeps adding URL after URL
to the same domain that offers many similar-looking URLs,
7
instead of moving to neighbor domains and thus describing a
larger portion of the infrastructure. In particular, we grow the
graph always in the direction of maximum interest, which is
toward domains that are yet to be included in the graph. Since
it conveys more information that multiple clients that visit that
domain also happen to browse other suspicious domains, we
apply this rule ﬁrst.
We also employ some techniques to keep the graph size
manageable. First, when we explore other URLs that a client
in the graph has visited, we only add to the graph the URLs that
one (or more) of our technique has deemed suspicious. This is
to avoid inserting to the graph the benign trafﬁc generated by
the user. Also, we avoid adding additional URLs belonging to
popular domains (in our experiment, domains that have been
visited by more than 10% of the hosts in the dataset). Note that
this does not imply that popular domains cannot be present in
the graph; they can still be added as the seeding candidate,
selected by our techniques. In this case, we consider this an
exception and do not apply this method. In fact, both the
Malicious CDN and the Exploit/Download Hosts techniques
have identiﬁed suspicious candidates in the popular website
phpnuke.org. This website has been exploited at the time
when our dataset was taken and was used to serve malware to
unsuspecting users. Also, to avoid the graph exploding in size,
we do not expand popular ﬁle names and URL paths (such as
index.html and ’/’).
B. Malicious-likelihood Metric
Once one of these graphs is generated for a particular
suspicious candidate, simply by looking at it, a security analyst
can decide with high accuracy which domains and URLs in
the graphs are malicious. The rule of thumb is the following:
if many suspicious candidates are included in the same graph,
they are likely to be malicious. If they are very scarce, they
are likely to be false positives. This, in fact, follows from
the reasoning made in the previous section: both malicious
and benign services have afﬁnities with similar services. To
encapsulate this guideline in a metric that can be computed
automatically, we devise our malicious-likelihood metric.
In particular, our metric indicates the likelihood that the
candidate under scrutiny is malicious, if all the other candidates
in the graph are postulated as malicious. For each suspicious
candidate, we compute the shortest distances to all the remain-
ing suspicious candidates in the graph. Different link types,
generated by the various afﬁnities described above, carry a
different link weight, depending on the likelihood that one end
of the link being malicious implies that the other end is also
malicious. For example, if two candidate URLs have links to
the same payload (which means that they both delivered it at
some point in time), and one is malicious, the other is also
surely malicious. The links’ weights are assigned as follows.
A smaller link weight represents a higher probability that the
maliciousness of one end of the link implies the maliciousness
of the other:
url  ! payload: weight 1
url  ! server: weight 1
url  ! client: weight 4
weight 2 for any other case.
•
•
•
•
8
We evaluated the sensitivity that the metric has to these
values: as long as the ordering of the link values is respected,
the resulting values for the metric are scaled and yield similar
results in the suspicious candidates rankings. Once these
shortest path distances are computed, our metric Mj for the
candidate j can be computed as:
Mj = Xi2candidates
1
shortest path(i, j)
.
This formula encloses the concept that suspicious candidates
close to the candidate under scrutiny make it more likely that
the latter is malicious. In fact, the more and the closer they
are, the larger the metric, whereas a few candidates far away
are of little signiﬁcance.
Having computed this metric for all the candidates, we
can now rank them from the most to the least likely to be
malicious. In our experiments, we show the performance of a
maliciousness classiﬁer that we trained, based on this metric.
VI. EVALUATION
We experimentally validate our initial hypothesis that
Nazca can effectively complement antivirus software and
blacklists, detecting malware that evades these legacy solu-
tions. We ﬁrst provide a description of and insights into our
datasets.
A. Trafﬁc Collection
Our data comprises nine days of trafﬁc from a commercial
ISP. We use the ﬁrst two, non-consecutive, days as our training
set to develop our techniques and observe malicious behaviors.
The remaining seven days, collected two months later, are used
exclusively as our testing set. The network traces cover the
trafﬁc of residential and small business clients, using ADSL
and ﬁber.
More speciﬁcally, the trafﬁc for the two days in the training
dataset was collected by the ISP on April 17th, 2012 and
August 25th, 2012. For each TCP connection, at most 10
packets or 10 kilobytes were collected, starting from the
beginning of the stream. The seven day testing dataset was
collected from October 22nd to October 28th, 2012. To further
limit the size of the collected data, for this dataset, at most
ﬁve packets or ﬁve kilobytes were collected from every TCP
connection. Moreover,
the trafﬁc to the most popular 100
domains (in volume of trafﬁc) were removed.
In our two-days training dataset, we found 52,632 unique
hosts, as identiﬁed by their IP address. 43,920 of them operate
as servers, whereas 8,813 operate as clients. 101 hosts show
both client and server behavior; these hosts have small websites
running on a machine that is also used as a workstation. The
dataset comprises of 4,431,472 HTTP conversations where the
server responds with a non-whitelisted MIME type. For these
connections, we counted 8,813 clients that contacted 22,232
distinct second-level domains. The seven days worth of test-
ing dataset contains 58,335 hosts, which produced 9,037,419
HTTP requests for non-whitelisted ﬁle MIME types, to a total
of 28,194 unique second-level domains. A total of 3,618,342
unique ﬁles where downloaded from 756,597 distinct URLs.
Blacklist
Records Type
Google Safe Browsing*
Lastline
DNS-BH
PhishTank*
ZeusTracker*
Ciarmy*
Emerging Threats (Open)
All blacklists combined
URLs
IPs and FQDNs
IPs and FQDNs
FQDNs
IPs and FQDNs
IPs
IPs and FQDNs
Size
Test
Train
-
20,844
23,086
3,333
1,043
100
1,848
-
21,916
25,342
10,438
1,019
100
1,927
Test
Present in Dataset
Train
116
25
86
10
3
0
0
236
237
30
34
0
3
0
2
271
Test
Present Executables
Train
22
24
76
6
2
0
0
128
50
25
11
0
1
0
2
77
Test
Present Malware
Train
22
13
3
0
0
0
0
33
1
2
2
0
0
0
1
4
TABLE I.
MALWARE DETECTION IN OUR TRAINING AND TESTING DATASETS USING POPULAR BLACKLISTS.
.
9
such as Heuristic.LooksLike.HTML.Infected and
IFrame.gen. We are aware that this choice might seem
counter-intuitive, since we claim that our system can detect
zero-day malware. However, in the wild, many (most) samples
are detected by at least some AV programs, especially after
some time has passed. Thus, using VirusTotal as an approxi-
mation of the ground truth seems reasonable. Also, when we
detect samples that are later conﬁrmed by one or more antivirus
engines, we do so without looking at the actual ﬁle itself.
For cases that (i) are not clearly classiﬁed by VirusTotal
or (ii) match a blacklist record and test benign in VirusTotal,
we run the executables in the Anubis sandbox [7] and perform
manual inspection, We rely exclusively on VirusTotal for our
testing dataset because of its large amount of executables. This
makes deeper (manual) analysis prohibitively expensive.
For privacy reasons, after recording the trafﬁc, the ISP has
kept the dataset for several months on hold before releasing
them to us. This is because the datasets contain information
that should be kept private,
including session cookies of
the ISP’s clients. Since both dataset contain only the ﬁrst
few kilobytes of each connection, we cannot recover the
executables downloaded in the trafﬁc directly from our data.
Instead, we fetched these ﬁles from the web using the recorded
URIs and discarded all those whose beginning did not match
the ones present in the collected trafﬁc. This has led us to
classify executables in three categories: benign, malicious,
and unknown, when we could not ﬁnd a live copy of the
executable. We are aware that this operational deﬁnition of
maliciousness has limitations that might lead to a small amount
of misclassiﬁcations, but it is the price we pay to have access
to such a sensitive dataset.
C. Blacklists
We apply a comprehensive set of popular blacklists to our
dataset to show how effectively a meticulous ISP could block
malware, using the most straightforward and popular solution.
The results are shown in Table I. For accuracy, we applied the
version of these blacklists that was available during the dataset
collection. For the blacklists that were not available during the
data collections, a later version was applied and marked with
an esterisk in the table.
D. Candidate Selection Step
To evaluate the efﬁcacy of Nazca’s Candidate Selection
Step, we apply our four techniques to our training and testing
datasets. When some parameters need tuning, we use (part
of) the training dataset. We evaluate each technique in detail.
Fig. 4. CDF of the number of User-Agents from each IP address.
The HTTP requests that download non-whitelisted ﬁles are of
our interest and form the input to our detection techniques.
To estimate the number of users served by this ISP,
the distribution of the User-Agents HTTP
we look at
headers (see Figure 4). In both dataset combined, 89.9%
of the IP addresses that operate exclusively as clients are
associated with no more than ten different User-Agents.
Here, we ignore User-Agents from popular AV software,
since they keep their update version in that string. We also
discard User-Agents from iPhones, since they encode the
application name (and hence, an iPhone produces requests
with a variety of User-Agents). Since a single workstation
typically employs a few User-Agents (e.g., one or two
browsers, several software updaters), these IP addresses are
probably in use by single households, and the remaining 10.1%
by small businesses.
B. Deﬁning Maliciousness
Before discussing our experimental results, we need to have
an operational deﬁnition of what constitutes maliciousness
for a downloaded executable. In our experiments, we used
VirusTotal as a basic oracle to discriminate malicious and
benign executables. More precisely, we consider any software
to be malicious when it tests positive against two or more AV
engines run in VirusTotal. We only accept AV classiﬁcations
that identify the malware strain and discard generic heuristics
Before that, we show the overall results of the Candidate
Selection Step in Table II. We note that the blacklists and
our techniques identify two almost separate sets of malware in
our datasets. Their intersection is limited to nine elements in
the training dataset and none in the testing dataset.
We manually inspected the 24 downloads of malware that
were detected by the blacklists but ignored by Nazca in the
testing dataset. These are all pieces of malware (.doc, .jar,
.dll, etc.) that have been downloaded by a single client, which
did not show signs of infections observable in the trafﬁc. For
example, a client downloaded a malicious DLL after visiting
a gaming website. After that, the client navigated Facebook
for a few minutes, and made no other requests until midnight,
when our trafﬁc collection was halted. This kind of isolated
cases defeat many of our techniques (e.g., a single download
cannot trigger the File Mutations technique), and evade Nazca.