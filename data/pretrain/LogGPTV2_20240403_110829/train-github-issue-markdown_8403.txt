(Reported by internal user)
## üêõ Bug
I'm observing some weird behaviors with the sparse mode of nn.Embedding. For
instance, with the following code:
    m = nn.Embedding(10, 3, sparse=True)
    m.zero_grad()
    m(torch.LongTensor([7, 1, 3])).sum().backward()
    print(m.weight.grad)
I'll get
    torch.sparse.FloatTensor of size (10,3) with indices:
    tensor([[7, 1, 3]])
    and values:
    tensor([[1., 1., 1.],
            [1., 1., 1.],
            [1., 1., 1.]])
as expected. But if I repeat the fwd/bwd line twice:
    m = nn.Embedding(10, 3, sparse=True)
    m.zero_grad()
    m(torch.LongTensor([7, 1, 3])).sum().backward()
    m(torch.LongTensor([7, 1, 3])).sum().backward()
    print(m.weight.grad)
I get random outputs, such as:
    torch.sparse.FloatTensor of size (10,3) with indices:
    tensor([[7, 1, 3]])
    and values:
    tensor([[  2.0000,   1.0000,   1.0000],
            [  1.0000,   1.0000,   1.0000],
            [  1.0000,   1.0000, 793.1436]])
I guess this is not expected?  
Also I noticed that if the input is different, it behaves very differently:
    m = nn.Embedding(10, 3, sparse=True)
    m.zero_grad()
    m(torch.LongTensor([7, 1, 3])).sum().backward()
    m(torch.LongTensor([8, 1, 3])).sum().backward()
    print(m.weight.grad)
Returns:
    torch.sparse.FloatTensor of size (10,3) with indices:
    tensor([[7, 1, 3, 8, 1, 3]])
    and values:
    tensor([[1.0000, 1.0000, 1.0000],
            [1.0000, 1.0000, 1.0000],
            [1.0000, 1.0000, 1.0000],
            [1.0000, 0.0000, 0.0000],
            [0.0000, 0.0000, 0.0000],
            [0.0000, 0.0000, 0.0000]])
## Expected behavior
It should behave the same as the dense version of nn.Embedding (aka.
`nn.Embedding(10, 3, sparse=False)`.
## Additional Context
This is reproducible on nightly builds as of 3/12/2019.