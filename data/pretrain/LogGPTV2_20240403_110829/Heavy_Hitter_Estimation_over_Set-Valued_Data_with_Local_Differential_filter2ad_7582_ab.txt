gorical data. Speciﬁcally, let n be the total number of users;
each user ui (1 ≤ i ≤ n) possesses exactly one item vi in a
domain containing d possible items, and the data collector
aims to estimate the frequency of each of the d items. In
RAPPOR, user ui represents vi using a length-d bit vector,
whose bits are all zero except for the vi-th bit, which is one.
Then, for each of the d bits, user ui applies RR indepen-
dently with a biased coin with probability p, whose value is
clariﬁed below. The data collector, upon receiving a length-
d bit vector from each of the n users, computes an unbiased
frequency estimate for each of the d items by applying RR
independently.
To determine the value of p (i.e., the probability that a
user reports the true value for each of the d bits), RAPPOR
utilizes an important concept in diﬀerential privacy called
sensitivity. Speciﬁcally, given an arbitrary function f , the
sensitivity ∆f of f is deﬁned as
(cid:48)
)(cid:107)1
∆f = maxD,D(cid:48)(cid:107)f (D) − f (D
(4)
where D and D(cid:48) are two arbitrary neighbor databases, and
(cid:107) · (cid:107)1 represents L1 norm of a vector. In RAPPOR, D and
D(cid:48) are two arbitrary records, i.e., any two diﬀerent items,
and f (D) (resp. f (D(cid:48))) is the true value of the length-d bit
vector corresponding to the item of D (resp. D(cid:48)). Since
such a bit vector contains a single bit of one, the maximum
diﬀerence between f (D) and f (D(cid:48)) are two bits. Hence, the
sensitivity of f is 2. Accordingly, [12] proves that RAPPOR
ensures diﬀerential privacy with the following value of p:
p =

2
e
1 + e

2
(5)
Comparing Equations (2) and (5), the latter calibrates
the noise according to the sensitivity of RAPPOR, i.e., 2.
We use the same methodology to extend RAPPOR to our
problem in Section 3. A main drawback of RAPPOR is
its high communication overhead, i.e., each user needs to
transmit d bits to the data collector, which can be expensive
when the number of possible items d is large. For instance, in
an application where each user reports its homepage to the
data collector, the number of items d is the total number
web pages in the entire Internet, leading to prohibitively
high communication costs. Further, the original proposal of
RAPPOR is limited to a single categorical attribute, and it
cannot be directly applied to our problem with set-valued
data.
Succinct histogram. Succinct histogram (SH) [3] addresses
the heavy communication problem incurred by previous so-
lutions such as RAPPOR. Speciﬁcally, SH targets the same
setting described above, i.e., each user possesses exactly one
item out of d possible items, and the data collector estimates
the frequency of each item. In particular, in SH the data
collector only reports items with relatively high frequency
values (above a given threshold); for all other items, SH
simply considers their frequency as zero. [3] proves that SH
achieves asymptotically optimal accuracy. Interestingly, no
previous work has compared SH with RAPPOR in terms of
result accuracy; we provide both theoretical and experimen-
tal comparisons between SH and RAPPOR in this paper.
SH is based on two key ideas. First, instead of reporting
d bits as is done in RAPPOR, in SH each user only reports
one randomly chosen bit. Although not explicitly stated in
[3], this idea only works when d  n,
SH applies random projection [5] on the data to reduce its
dimensionality from d to m = O(n) in a preprocessing step.
Speciﬁcally, SH generates a d × m matrix Φ which each ele-
ment is chosen independently and uniformly at random from
m . Note that elements in Φ
two possible values:
are essentially binary values. Then, each user multiplies its
length-d bit vector with Φ, obtaining a length-m vector. Af-
ter that, she applies the ﬁrst idea, i.e., choosing one random
bit and then releasing it using RR.
m and − 1√
1√
Figure 1: Itemset padding and truncation
Speciﬁcally,
in [3], the authors prove that general his-
togram and heavy hitter estimation have the same asymp-
totic lower bound in terms of estimation error, and SH achieves
a matching upper bound. However, [3] focuses on a diﬀerent
problem setting: each user has at most one item, and the
heavy hitter set is deﬁned with a given frequency threshold.
Hence, the theoretical results in [3] do not apply to our prob-
lem. Extending SH to our problem eﬀectively is non-trivial,
which we elaborate in Section 4.2.
3. PROBLEM DESCRIPTION AND NAIVE
SOLUTIONS
Problem description. This paper focuses on heavy hit-
ter discovery and frequency estimation over set-valued data.
Speciﬁcally, each user ui possesses a set vi of items. For
simplicity, we assume that each user has a ﬁxed number l
of items, i.e., ∀i, vi = l, where l is a system parameter. If
a user has less than l items, she pads her item set vi with
dummy items, which are ignored by the data collector, i.e.,
the latter does not compute frequency of the dummy item
and does not consider it in the heavy hitter set. Conversely,
if a user possesses more than l items, she randomly draws l
samples without replacement from her item set, forming a
new set with exactly l items. If items are already in random
order in her original item set, the user can simply truncate
the set to l items, as shown in Figure 1. Because such trun-
cation loses information and introduces bias to the results,
in general l should be set to a reasonable large value so that
most users (i.e., except for a few outliers) have no more than
l items in the original data.
Let n be the total number of users, d be the total number
of diﬀerent items, and fj be the frequency of the j-th item
(denoted as vj), i.e., the portion of users possessing item vj
(1 ≤ j ≤ d). Formally, we have:
fj =
|{ui|vj ∈ vi, 1 ≤ i ≤ n}|
n
The data collector aims to ﬁnd the top-k items with the
highest frequency fj, along with the frequency estimate for
each such item. We assume that k (cid:28) d, i.e., the result only
contains the top few heavy hitters. Meanwhile, for many
applications, k is also signiﬁcantly smaller than l, the maxi-
mum number of items that each user has. The main goal is
to obtain high accuracy of the results while satisfying LDP.
Here result accuracy has two aspects. First, the relative
ranking of the top-k items should be close to their actual
ranking. Second, the error of the estimated frequencies for
the reported heavy hitters should be minimized. We elabo-
rate on the error metrics in Section 5.
Naive solutions. Next we describe two naive solutions ob-
tained by adapting RAPPOR and SH described in Section
2.2 to our problem, respectively. We ﬁrst focus on RAP-
POR. Recall that RAPPOR requires each user to send a
perturbed bit to the data collector, where the perturbation
is based on RR (also described in Section 2.2) using a biased
coin with probability p. The key, therefore, is to determine
the value of p in our problem. To do so, we analyze the sen-
sitivity of releasing a length-d bit vector at each user. Since
each user possesses exactly l items, there are exactly l ones
in the true bit vector; all other bits are zero. Therefore, two
such bit vectors can diﬀer by at most 2l bits, meaning that
the sensitivity is 2l. According to RAPPOR [12], we have:
p =

2l
e
1 + e

2l
(6)
Similar to the case with categorical values, RAPPOR in-
curs a transmission cost of O(d) for each user, which can
be rather expensive for large domains. Furthermore, when l
is relatively large, RAPPOR incurs rather high sensitivity,
and, thus, heavy perturbation of the released bit vectors,
leading to low accuracy.
Next we describe the adaptation of SH to our problem.
Unlike RAPPOR, SH is limited to the case where each user
possesses exactly one item.
In order to handle set-valued
data, we apply SH l times using sequential composition (re-
fer to Section 2.1). Speciﬁcally, every user divides its privacy
budget  into l equal portions of 
l each. Then, the user in-
vokes SH l times, one for each of the l items she possesses,
using privacy budget 
l . According to sequential compo-
sition, doing so for all l items satisﬁes -local diﬀerential
privacy.
The data collector receives l independent data releases
from each user. It then proceeds to generate l succinct his-
tograms which correspond to l invocations of SH. After that,
it aggregates these succinct histograms into one histogram.
j (1 ≤ j ≤ d, 1 ≤ k ≤ l) be the fre-
In particular, let f k
quency estimate for item vj from the k-th invocation of SH.
j as the ﬁnal frequency
The data collector returns (cid:80)l
k=1 f k
estimate for item vj.
The above extension of SH incurs communication over-
head O(l) for each user, i.e., for applying SH l times with
O(1) transmissions each. Since l (cid:28) d in most applications,
this method is less expensive in terms of communications
compared to the adaptation of RAPPOR. Nevertheless, it
can still be costly for large values of l, i.e., a user can possess
a large number of items. Regarding result accuracy, when
l is large, each portion of the privacy budget 
is rather
l
small, leading to heavy perturbation and low accuracy sim-
ilarly as in RAPPOR. Worse, aggregating multiple succinct
histograms at the data collector also accumulates error. For
this reason, the accuracy performance of this method is of-
ten lower than that of RAPPOR, as we show in the analysis
in Section 4.2. Overall, neither naive approach is eﬃcient
(in terms of communications) or eﬀective (in terms of result
utility). Next we present the proposed solution LDPMiner,
which addresses these problems.
4. LDPMINER
This section presents the proposed solution LDPMiner,
which achieves both low communication overhead and high
result accuracy. The main ideas include a novel two-phase
framework, novel LDP mechanisms that are used as build-
ing blocks in these two phases, and further optimizations
17 21 … 55 … 301 . . 1034 69 22 31 … 19 67 232 ⊥ ⊥ ⋮ 𝑈𝑠𝑒𝑟𝑖 with  𝑙𝑖 > 𝑙𝑈𝑠𝑒𝑟𝑖 with  𝑙𝑖 < 𝑙Item set length 𝑙 native and demonstrate the beneﬁts of the two-phase frame-
work.
It remains to clarify the LDP mechanisms used in
Phase I and Phase II. These mechanisms are built upon a
few basic blocks, described in the next subsection.
4.2 Building Blocks
In both phases, each user needs to report her items to the
data collector, in order for the latter to derive frequency es-
timates and compute the set of candidates in Phase I and
ﬁnal heavy hitter sets in Phase II. We observe that the naive
solutions described in Section 3 are ineﬀective and ineﬃ-
cient, because each user tries to report all items she pos-
sesses, which leads to high sensitivity in naive RAPPOR,
as well as tiny privacy budget share for each item and high
communication overhead in naive SH. One key idea in the
LDPMiner building blocks, inspired by another technique
[27] for multi-dimensional data analysis under LDP, is that
each user does not report all items; instead, she chooses a
random item and reports it. Such random sampling leads to
biased item frequency estimates as each user now reports one
instead of l items. To compensate for this, the data collector
multiplies the estimated frequencies by l. As we show soon
in our analysis, this leads to an unbiased estimate of true
item frequencies. We call this approach sampling random-
izer. Algorithm 1 shows the pseudo code for the proposed
sampling randomizer method which implements the above
idea.
Algorithm 1: Sampling randomizer algorithm
Input : Privacy parameter , item set vi for each user
ui, number of heavy hitters k;
Output: Top-k heavy hitters HH k;
1 for user i → n do
2
Pick j from range [1, l] uniformly at random ; Apply
a single-item randomizer to obtain zi = R(vi[j], );
Submit zi to the data collector;
3
4 end
5 Data collector: Estimator HH k = E((cid:80)
i,j zil, );
6 Return HH k;
Note that Algorithm 1 is a general framework for comput-
ing heavy hitters over set-valued data under LDP; in particu-
lar, the randomizer at each user (line 2) and the heavy hitter
computation at the data collector (line 5) are left as black
boxes. These black boxes can be fulﬁlled with RAPPOR,
SH, or any method for heavy hitter estimation in the single-
item setting. We call the sampling randomizer algorithm
with RAPPOR (resp. SH) sampling RAPPOR (sampling
SH) respectively. Next we compare the result accuracy of
the naive solutions and the sampling-randomizer-based so-
lutions through theoretical analysis.
Utility Analysis. Because the accuracy of top-k set is
not easy to quantify, we analyze result utility in terms of
the mean squared error of the item frequency estimates.
Intuitively, for methods without special considerations for
the heavy hitters such as the naive solutions (Section 3)
and sampling randomizer (Algorithm 1), more accurate fre-
quency estimates are expected to lead to more accurate
heavy hitter estimation. A direct evaluation on the accu-
racy of the heavy hitter set is provided by the experiments
in Section 5.
Figure 2: Two-Phase Framework in LDPMiner
for each of the two phases.
In the following, Section 4.1
overviews the general framework of LDPMiner. Section 4.2
presents the building blocks. Sections 4.3 and 4.4 further
optimize the two phases, respectively.
4.1 Two-Phase Framework
Recall from Section 3 that naive solutions incur high com-