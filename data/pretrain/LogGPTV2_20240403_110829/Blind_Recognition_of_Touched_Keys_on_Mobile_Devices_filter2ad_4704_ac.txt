also affect the touching gestures and where a key will be touched.
Now we analyze the image formation of a ﬁngertip touching a
key. Videos may be taken from different angles. Without loss of
generality, we study the case in which the camera faces the touch-
ing ﬁnger and the front edge of the key is in parallel to the image
plane. Figure 12 shows the geometry of the image formation of
the touching ﬁnger in the 3D world when the ﬁngertip falls inside
a key’s area. The key’s height is w and its length is l. The point F
1407Figure 11: Touching Gestures
Figure 12: Fingertip Image Formation
on the ﬁngertip will project to the point F ′ on the image plane. Its
brightness in the image will be determined by lighting and the ﬁn-
gertip shape. Because of the lighting difference, points on the side
of the ﬁnger facing the touch screen are dark in the image. Adjacent
to the dark area is the gray area where lighting is weak. There is
also the bright area on the ﬁnger that is well illuminated.
f K ′
Figure 13 shows the longitudinal view of a ﬁnger touching the
surface. We use Figure 13 to discuss our basic principle of inferring
a touched key. Kf and Kb are the front and back of the touched key
K respectively. T is the touched point. Apparently T is on the line
segment Kf Kb. T and Kf Kb are projected onto the image plane
as T ′ and K ′
b. If we can identify T ′ in the image, our problem
is solved. However, as we can see from Figure 13, since the human
ﬁnger has a curved surface, the camera may not be able to see the
touched point. OTo is the tangent line to the curved ﬁnger surface
and it intersects with the touch screen surface at To. The camera
can see To, which is the closest point to the touched point on the
touch screen surface. To is projected as T ′
o on the image plane. If
T ′
o in the
image and T ′
o can be used to determine the touched key.
b, then we just need to ﬁnd T ′
o is on the line segment K ′
f K ′
We argue that T ′
o generally lands in the area of a key. Table 1
shows the key size of the unlock screen software keypad for iPad,
iPhone and Nexus 7 tablet. Figure 12 gives the deﬁnition of key
height and length. Table 2 gives the average size of the ﬁngertip for
index and middle ﬁngers of 14 students of around 27 years old, in-
cluding 4 females and 10 males. The ﬁngertip height is the distance
from the ﬁngertip pulp to the ﬁngernail. The ﬁngertip length is the
distance between the ﬁngertip pulp to the far front of the ﬁnger.
When people touch the screen, they generally use the upper half of
the ﬁngertip to touch the middle of the key so that the key can be
effectively pressed. We can see that half of the ﬁngertip is around
6.5mm, less than the key height for all devices in Table 1. More-
over, according to Tables 1 and 2, the ﬁngertip width is smaller than
the key length. Therefore, the ﬁngertip generally lands inside the
key area, as shown in Figure 13. That is, the far front of the ﬁnger-
tip F in Figure 13 is in the range of the key and the touched point
is inside the key area. Based on the perspective projection, To is
on the segment of Kf Kb so that T ′
f K ′
b
whenever the ﬁngertip is in the view of the camera.
o is on the segment of K ′
On a QWERTY keyboard of iPhone and other small smartphones,
keys are very small. In these scenarios, people often use vertical
touching or touch with the ﬁngertip side in order not to touch wrong
keys. That is, the ﬁngertip top lands in the key area. The analysis
above is still valid. Our experiments on the QWERTY keyboard
also validate this analysis.
Table 1: Unlock Screen Keypad Size - Height × Length (mm)
iPhone 5 Nexus 7
8 × 16
10 × 16
Height (mm) × Length (mm)
9 × 17
iPad
Table 2: Fingertip Size (σ - Standard Deviation)
Middle Finger
σ
Average
1.3
1.7
1.7
Index Finger
σ
Average
1.2
1.6
1.9
Height (mm)
Length (mm)
Width (mm)
9.6
12.9
13.1
10.4
13.1
13.7
f K ′
There are cases that T ′
o is not on the line segment K ′
r, corre-
sponding to the touched key K. Figure 14 illustrates such a case.
Please note we intentionally draw a large ﬁnger for clarity. In this
case, the key, such as one on a keyboard for a non-unlock screen,
is so small. The camera is too close to the ﬁnger and takes such a
wrong angle that To lands outside Kf Kr. Therefore, T ′
o is not on
r. In such cases, our observation is that T ′
the line segment K ′
o
generally lands into the far rear part of the key K′ in front of K. We
deﬁne a percentage α. If an estimated touched point lands in the
rear α of K′, the touched key is K.
f K ′
We now derive the size of a key in an image and investigate its
impact. The camera focus length is f . The height from the camera
to the touch screen surface is h. The physical key height |Kf Kb| =
w. The distance between the key front Kf and the lens center is d.
By geometry operations, we have
′
′
|K
f K
b| =
f h
d(1 + d/w)
.
If the physical key length is l, the key length l′ in the image is,
l′ =
f l
d
.
(4)
(5)
From Formulas (4) and (5), the farther the touch screen from the
camera, the smaller the size of the key in the image. The smaller the
physical key size, the smaller of the key in an image. Table 3 gives
the camera speciﬁcations of the cameras used in our experiments:
Logitech HD Pro Webcam C920 [26], the iPhone 5 camera and the
Google glass camera. If the camera is around 2 meters away and
half a meter away from the target, according to Formula (4) and
our experiments, the key height is only a few pixels. Therefore,
in our experiments, we often need to zoom the ﬁngertip image for
accurate localization of touched points. We can also derive the key
size in the touching frames practically by using the homography
from the reference image to the touching frames. The key area
in the reference image is known, thus the key size in the touching
frames can be derived.
4.2 Clustering-based Recognition of Touched
Points
Based on the model of the touching ﬁnger in an image, we now
introduce the clustering-based strategy recognizing touched keys. If
we can derive the position of the touched point T
o in Figure 15, we
can infer the corresponding key by applying the homography. The
problem is how to identify this touched point2. Intuitively, since T
o
is far below the ﬁngertip, which blocks light rays, T
o should be in
the darkest area around the ﬁngertip in the image.
′
′
′
We now analyze the brightness of the area around the ﬁngertip.
The ﬁngertip is a very rough surface at the microscopic level and
can be treated as an ideal diffuse reﬂector. The incoming ray of
light is reﬂected equally in all directions by the ﬁngertip skin. The
2Touched points actually form an area under the ﬁngertip.
1408Figure 13: Touched Point inside the Key
Figure 14: Touched Point outside the Key
Figure 15: Five Pixel Groups at Fingertip
result of the area in the green and tiny bounding box. The green
point is the touched point in the upper half of the darkest area. The
Figure 16 (b) shows the mapped point (in green) that falls into the
quite front part of key 5. Therefore, 5 is the touched key.
reﬂection conforms to the Lambert’s Cosine Law [38]: the reﬂected
energy from a small surface area in a particular direction is propor-
tional to cosine of the angle between the particular direction and
the surface normal. Therefore, for the lower part of the ﬁngertip arc
facing the touch screen, denoted as the inner side of the ﬁngertip,
the angle is large and less energy will be reﬂected so that the pix-
els are darker. Particularly, the area around T
o is the darkest, i.e.,
touched points are the darkest. The area around the ﬁngertip top
F is the brightest. From the bright area to the dark area, there ex-
ists the gray area between F and T
o in Figure 15. Since the touch
screen is basically a mirror, the camera may also capture the virtual
image of the inner side of the ﬁngertip, which also has a dark area,
gray area and bright area.
′
′
Table 3: Camera Speciﬁcations
Camera
Focal Length (mm)
Pixel Width (µm)
Logitech C920
iPhone 5
Google glass
3.67
4.10
2.80
3.98
1.40
0.18
Therefore, around the ﬁngertip and its virtual image, we can have
ﬁve areas with ﬁve different brightness: bright ﬁngertip top, gray
ﬁngertip middle area, dark ﬁngertip bottom and its virtual image
(dark ﬁngertip bottom, dark ﬁngertip bottom of the virtual image),
gray ﬁngertip middle area of the virtual image and bright ﬁngertip
top of the virtual image. T
o lands in the upper half part of the dark
area since the other half of the dark area is the virtual image of the
dark ﬁngertip bottom.
′
We can use clustering algorithms to cluster these ﬁve areas of
pixels of different brightness. The k-means clustering is applied to
pixels in the tiny bounding box in Figure 10. The number of clusters
is set as 5. The darkest cluster C indicates the area where the ﬁnger
touches the screen surface. We automatically select a pixel in the
upper half S of C as the touched point in the following way: a. the
coordinate of a pixel p is (x, y), where x is the column number and
y is the row number. Therefore,
S = {p|p ∈ C, p.y < median of y of all pixels in C}.
(6)
b. Derive the minimal upright bounding rectangle R for pixels in
S. The touched point is chosen from S and is the closest one to
the center of the bounding rectangle R. This touched point is then
mapped to the reference image, and the mapped point shall fall onto
the correct key, denoted as mapped key K. Denote the key behind
K as Kb, seen through the camera’s “perspective”. As discussed in
Section 4.1, we also need to check the distance between the mapped
point and the back edge of K. If the touched points lands in the
back α portion of K, then we choose Kb as the real mapped key.
Our experiments show that the optimal α is 1/5.
Basically, the clustering algorithm helps accurately identify the
touched point. As an example, Figure 16 (a) shows the clustered
Figure 16: Clustered Result and Mapped Point
f F ′
If we examine Figure 13 carefully, we can see that in addition
to touched points, points on the ﬁngertip arc may also be projected
into the area of a key K ′
b on the image plane. In Figure 13, line
OKb and the ﬁnger intersect at the point G. We can see that all
points on the ﬁngertip arc T G visible to the camera are projected
into the area of the key in the image. Therefore, in this case, both
touched points and points on this ﬁngertip arc can be used to deduce
the key even if the points on the ﬁngertip arc are in the bright or gray
area from our clustering above. However, due to the size of the
ﬁngertip, touched position, touching gestures, and distance, height
and angle of the camera, G’s position changes too and can be any
point on the ﬁngertip arc. It is not reliable to use these points on the
ﬁngertip arc to infer the touched key. We still use touched points in
the darkest area around the ﬁngertip top, but the fact that points in
the gray or bright area may be projected into the key’s area lends us
some robustness to use touched points in the darkest area to infer
the touched key.
5. EVALUATION
In this section, we present the experiment design and results to
demonstrate the impact of the blind recognition of touched keys.
5.1 Experiment Design
We have performed extensive experiments on various target de-
vices with different key size, including iPad, iPhone and Nexus 7
tablet. Three cameras are used: Logitech HD Pro Webcam C920,
iPhone 5 camera and Google Glass. Table 3 summarizes their spec-
iﬁcations. Most experiments are performed with Logitech HD Pro
Webcam C920. The last group of experiments are designed for
comparing web camera, iPhone camera and Google glass against d-
1409ifferent devices as well as the impact of different kinds of keyboard-
s.
In all experiments, we try to recognize 4-digit or 4-character
passcodes, which are randomly generated. The success rate is de-
ﬁned as the probability that the passcodes are correctly recognized.
In addition to different cameras and target devices, we also con-
sider the impact from the following factors: users, the distance be-
tween the camera and target device, and the camera angle.
Users: Different people have different ﬁnger shape, ﬁngernail
and touching gestures. Five females and six males with the expe-
rience of using tablets and smartphones participated in the experi-
ments. They were separated to three groups: 3 people in the ﬁrst
group, and 7 people in the second group. These two groups per-
formed experiments with iPad. The last group helped us evaluate
the success rate versus the distance between the camera and the tar-
get, different cameras versus different devices, and the web camera
versus different kinds of keyboards. For the ﬁrst group, we took
10 videos for every person at each angle (front, left and right of