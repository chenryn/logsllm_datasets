throws (1: error.FBossBaseError error)
Figure 7: Example of Thrift interface definition to re-
trieve L2 entries from the switch.
LLDP, by keeping protocols apprised of state changes2. The
protocols are notified of state changes via a mechanism called
state observation. Specifically, any object at the time of its
initialization may register itself as a State Observer. By doing
so, every future state change invokes a callback provided by
the object. The callback provides the state change in question,
allowing the object to react accordingly. For example, NDP
registers itself as a State Observer so that it may react to port
change events. In this way, the state observation mechanism
allows protocol implementations to be decoupled from issues
pertaining to state management.
Thrift Management Interface. We run our networks in a
split control configuration. Each FBOSS instance contains
a local control plane, running protocols such as BGP or
OpenR [13], on a microserver that communicates with a
centralized network management system through a Thrift
management interface. The types of messages that are sent
between them are as in the form seen in Figure 7. The full
open-source specification of the FBOSS Thrift interface is
also available [5]. Given that the interfaces can be modified
to fit our needs, Thrift provides us with a simple and flexible
way to manage and operate the network, leading to increased
stability and high availability. We discuss the details of the
interactions between the Thrift management interface and the
centralized network management system in Section 6.
QSFP Service. The QSFP service manages a set of QSFP
ports. This service detects QSFP insertion or removal, reads
QSFP product information (e.g., manufacturer), controls
QSFP hardware function (i.e., change power configuration),
and monitors the QSFPs. FBOSS initially had the QSFP ser-
vice within the FBOSS agent. However, as the service con-
tinues to evolve, we must restart the FBOSS agent and the
switch to apply the changes. Thus, we separated the QSFP
service into a separate process to improve FBOSS’s modular-
ity and reliability. As the result, FBOSS agent is more reliable
as any restarts or bugs in QSFP service do not affect the agent
directly. However, since QSFP service is a separate process,
it needs separate tools for packaging, deployment, and moni-
toring. Also, careful process synchronization between QSFP
service and FBOSS agent is now required.
2The other functions include control packets transmission and reception and
programming of switch ASIC and hardware.
Figure 8: Illustration of FBOSS’s switch state update
through copy-on-write tree mechanism.
4.2 State Management
FBOSS’s software state management mechanism is de-
signed for high concurrency, fast reads, and easy and safe
updates. The state is modeled as a versioned copy-on-write
tree [37]. The root of the tree is the main switch state class,
and each child of the root represents a different category of the
switch state, such as ports or VLAN entries. When an update
happens to one branch of the tree, every node in the branch
all the way to the root is copied and updated if necessary.
Figure 8 illustrates a switch state update process invoked by
an update on an VLAN ARP table entry. We can see that only
the nodes and the links starting from the modified ARP table
up to the root are recreated. While the creation of the new tree
occurs, the FBOSS agent still interacts with the prior states
without needing to capture any locks on the state. Once the
copy-on-write process completes for the entire tree, FBOSS
reads from the new switch state.
There are multiple benefits to this model. First, it allows
for easy concurrency, as there are no read locks. Reads can
still continue to happen while a new state is created, and
the states are only created or destroyed and never modified.
Secondly, versioning of states is much simpler. This allows
easier debugging, logging, and validation of each state and
its transitions. Lastly, since we log all the state transitions, it
is possible to perform a restart and then restore the state to
its pre-restart form. There also are some disadvantages to this
model. Since every state change results in a new switch state
object, the update process requires more processing. Secondly,
implementation of switch states is more complex than simply
obtaining locks and updating a single object.
Hardware Specific State. The hardware states are the
states that are kept inside the ASIC itself. Whenever a hard-
ware state needs to be updated in software, the software must
call the switch SDK to retrieve the new states. The FBOSS
HwSwitch obtains both read and write locks on the corre-
sponding parts of the hardware state until the update com-
pletes. The choice of lock based state updates may differ
based on the SDK implementation.
SwitchStatePortsVLANsRoutesPort1……PortiVLAN1…VLANjRoute1RoutekARP1ARPjSwitchState’VLANs’VLAN’jARP’jSIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
S. Choi et al.
The FBOSS deployment process is very similar to other
continuous deployment processes [22] and is split into three
distinct parts: continuous canary, daily canary and staged
deployment. Each of these parts serves a specific purpose to
ensure a reliable deployment. We currently operate roughly
at a monthly deployment cycle, which includes both canaries
and staged deployment, to ensure high operational stability.
Continuous Canary. The continuous canary is a process
that automatically deploys all newly committed code in the
FBOSS repository to a small number of switches that are
running in production, around 1-2 switches per each type of
switch, and monitors the health of the switch and the adjacent
switches for any failures. Once a failure is detected, contin-
uous canary will immediately revert the latest deployment
and restore the last stable version of the code. Continuous
canary is able to quickly catch errors related to switch initial-
ization, such as issues with warm boot, configuration errors
and unpredictable race conditions.
Daily Canary. The daily canary is a process that follows
continuous canary to test the new commit at a longer timescale
with more switches. Daily canary runs once a day and deploys
the latest commit that has passed the continuous canary. Daily
canary deploys the commit to around 10 to 20 switches per
each type of the switch. Daily canary runs throughout the day
to capture bugs that slowly surface over time, such as memory
leaks or performance regressions in critical threads. This is
the final phase before a network-wide deployment.
Staged Deployment. Once daily canary completes, a hu-
man operator intervenes to push the latest code to all of the
switches in production. This is the only step of the entire
deployment process that involves an human operator and
roughly takes about a day to complete entirely. The operator
runs a deployment script with the appropriate parameters to
slowly push the latest code into the subset of the switches at
a time. Once the number of failed switches exceed a preset
threshold, usually around 0.5% of the entire switch fleet, the
deployment script stops and asks the operator to investigate
the issues and take appropriate actions. The reasons for keep-
ing the final step manual are as follows: First, a single server
is fast enough to deploy the code to all of the switches in the
data center, meaning that the deployment process is not bottle-
necked by one machine deploying the code. Secondly, it gives
fine grained monitoring over the unpredicted bugs that may
not be caught by the existing monitors. For example, we fixed
unpredicted and persistent reachability losses, such as inad-
vertently changing interface IP or port speed configurations
and transient outages like as port flaps, that we found during
staged deployment. Lastly, we are still improving our testing,
monitoring and deployment system. Thus, once the test cover-
age and automated remediation is within a comfortable range,
we plan automate the last step as well.
Figure 9: Culprits of switch outages over a month.
5 TESTING AND DEPLOYMENT
Switch software is conventionally developed and released
by switch vendors and is closed and proprietary. Therefore, a
new release to the switch software can take months, in lengthy
development and manual QA test cycles. In addition, given
that software update cycles are infrequent, an update usually
contains a large number of changes that can introduce new
bugs that did not exist previously. In contrast, typical large
scale software deployment processes are automated, fast, and
contain a smaller set of changes per update. Furthermore,
feature deployments are coupled with automated and incre-
mental testing mechanisms to quickly check and fix bugs. Our
outage records (Figure 9) show that about 60% of the switch
outages are caused by faulty software. This is similar to the
known rate of software failures in data center devices, which
is around 51% [27]. To minimize the occurrences and impact
of these outages, FBOSS adopts agile, reliable and scalable
large scale software development and testing schemes.
Instead of using existing automatic software deployment
framework like Chef [3] or Jenkins [6], FBOSS employs
its own deployment software called fbossdeploy. One of the
main reason for developing our own deployment software is
to allow for a tighter feedback loop with existing external
monitors. We have several existing external monitors that
continuously check the health of the network. These monitors
check for attributes such as link failures, slow BGP conver-
gence times, network reachability and more. While existing
deployment frameworks that are built for deploying generic
software are good at preventing propagation of software re-
lated bugs, such as deadlocks or memory leaks, they are not
built to detect and prevent network-wide failures, as these
failures may be hard to detect from a single node. There-
fore, fbossdeploy is built to react quickly to the network-wide
failures, such as reachability failures, that may occur during
deployment.
28%24%8%7%14%11%5%2%1%Misc. SofwareIssuesMicroserverRebootKernel PanicMicroserverUnresponsiveLoss of PowerBus DegradationSSD IssuePCI-E TimeoutMisc. HardwareIssuesSoftware (60%)Hardware (40%)FBOSS: Building Switch Software at Scale
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
the configuration concurrently, which limits inconsistencies
in the configuration. Secondly, it makes the configuration
reproducible and deterministic, since the configurations are
versioned and FBOSS agent always reads the latest configura-
tion upon restarts. And lastly, it avoids manual configuration
errors. On the other hand, there are also disadvantages to our
fully automated configuration system - it lacks a complex
human interactive CLI, which makes manual debugging diffi-
cult; also, there is no support for incremental configuration
changes, which makes each configuration change require a
restart of the FBOSS agent.
6.2 Draining
Draining is the process safely removing an aggregation
switch from its service. ToR switches are generally not
drained, unless all of the services under the ToR switch are
drained as well. Similarly, undraining is the process of restor-
ing the switch’s previous configuration and bringing it back
into service. Due to frequent feature updates and deployments
performed on a switch, draining and undraining a switch is
one of the major operational tasks that is performed frequently.
However, draining is conventionally a difficult operational
task, due to tight timing requirements and simultaneous con-
figuration changes across multiple software components on
the switch [47]. In comparison, FBOSS’s draining/undraining
operation is made much simpler thanks to the automation and
the version control mechanism in the configuration manage-
ment design. Our method of draining a switch is as follows:
(1) FBOSS agent retrieves the drained BGP configuration data
from a central configuration database. (2) The central man-
agement system triggers the draining process via the Thrift
management interface. (3) The FBOSS agent activates the
drained config and restarts the BGP daemon with the drained
config. As for the undraining process, we repeat the above
steps, but with an undrained configuration. Then, as a final
added step, the management system pings the FBOSS agent
and queries the switch statistics to ensure that the undraining
process is successful. Draining is an example where FBOSS’s
Thrift management interface and the centrally managed con-
figuration snapshots significantly simplify an operational task.
6.3 Monitoring and Failure Handling
Traditionally, data center operators use standardized net-
work management protocols, such as SNMP [21], to collect
switch statistics, such as CPU/memory utilization, link load,
packet loss, and miscellaneous system health, from the vendor
network devices. In contrast, FBOSS allows external systems
to collect switch statistics through two different interfaces:
a Thrift management interface and Linux system logs. The
Thrift management interface serves the queries in the form
specified in the Thrift model. This interface is mainly used to
Figure 10: FBOSS interacts with a central network man-
agement system via the Thrift management interface.
6 MANAGEMENT
In this section, we present how FBOSS interacts with
management system and discuss the advantages of FBOSS’s
design from a network management perspective. Figure 10
shows a high-level overview of the interactions.
6.1 Configurations
FBOSS is designed to be used in a highly controlled data
center network with a central network manager. This greatly
simplifies the process of generation and deployment of net-
work configurations across large number of switches.
Configuration Design. The configuration of network de-
vices is highly standardized in data center environments.
Given a specific topology, each device is automatically con-
figured by using templates and auto-generated configuration
data. For example, the IP address configurations for a switch
is determined by the type of the switch (e.g., ToR or aggrega-
tion), and its upstream/downstream neighbors in the cluster.
Configuration Generation and Deployment. The config-
uration data is generated by our network management system
called Robotron [48] and is distributed to each switch. The
local config generator in FBOSS agent then consumes the
configuration data and creates an active config file. If any
modification is made to the data file, a new active config file
is generated and the old configuration is stored as a staged
config file. There are multiple advantages to this configuration
process. First, it disallows multiple entities from modifying
CentralizedConfigDatabaseOperationManagerMonitorNetwork Management SystemFBOSS AgentThrift InterfaceLocal ConfigGeneratorActiveconfigStaged config_1GenerateStageRe-useRevertUseconfig_xConfigDataQuery/ReceiveDevice StatesConfiguration   OperationMonitoringSIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
S. Choi et al.
monitor high-level switch usage and link statistics. Given that
FBOSS runs as a Linux process, we can also directly access
the system logs of the switch microserver. These logs are
specifically formatted to log the category events and failures.
This allows the management system to monitor low-level sys-
tem health and hardware failures. Given the statistics that it
collects, our monitoring system, called FbFlow [46], stores
the data to a database, either Scuba [15] or Gorilla [42], based
on the type of the data. Once the data is stored, it enables
our engineers to query and analyze the data at a high level
over a long time period. Monitoring data, and graphs such as
Figure 3, can easily be obtained by the monitoring system.
To go with the monitoring system, we also implemented an
automated failure remediation system. The main purpose of
the remediation system is to automatically detect and recover
from software or hardware failures. It also provides deeper in-