that a message can be canceled—for example, when a request takes too long
or if the user has indicated that they want to cancel the operation it
implements. ALPC supports this with the NtAlpcCancelMessage system call.
An ALPC message can be on one of five different queues implemented by
the ALPC port object:
■    Main queue A message has been sent, and the client is processing it.
■    Pending queue A message has been sent and the caller is waiting for
a reply, but the reply has not yet been sent.
■    Large message queue A message has been sent, but the caller’s
buffer was too small to receive it. The caller gets another chance to
allocate a larger buffer and request the message payload again.
■    Canceled queue A message that was sent to the port but has since
been canceled.
■    Direct queue A message that was sent with a direct event attached.
Note that a sixth queue, called the wait queue, does not link messages
together; instead, it links all the threads waiting on a message.
EXPERIMENT: Viewing subsystem ALPC port
objects
You can see named ALPC port objects with the WinObj tool from
Sysinternals or WinObjEx64 from GitHub. Run one of the two
tools elevated as Administrator and select the root directory. A gear
icon identifies the port objects in WinObj, and a power plug in
WinObjEx64, as shown here (you can also click on the Type field
to easily sort all the objects by their type):
You should see the ALPC ports used by the power manager, the
security manager, and other internal Windows services. If you want
to see the ALPC port objects used by RPC, you can select the \RPC
Control directory. One of the primary users of ALPC, outside of
Local RPC, is the Windows subsystem, which uses ALPC to
communicate with the Windows subsystem DLLs that are present
in all Windows processes. Because CSRSS loads once for each
session, you will find its ALPC port objects under the appropriate
\Sessions\X\Windows directory, as shown here:
Asynchronous operation
The synchronous model of ALPC is tied to the original LPC architecture in
the early NT design and is similar to other blocking IPC mechanisms, such as
Mach ports. Although it is simple to design, a blocking IPC algorithm
includes many possibilities for deadlock, and working around those scenarios
creates complex code that requires support for a more flexible asynchronous
(nonblocking) model. As such, ALPC was primarily designed to support
asynchronous operation as well, which is a requirement for scalable RPC and
other uses, such as support for pending I/O in user-mode drivers. A basic
feature of ALPC, which wasn’t originally present in LPC, is that blocking
calls can have a timeout parameter. This allows legacy applications to avoid
certain deadlock scenarios.
However, ALPC is optimized for asynchronous messages and provides
three different models for asynchronous notifications. The first doesn’t
actually notify the client or server but simply copies the data payload. Under
this model, it’s up to the implementor to choose a reliable synchronization
method. For example, the client and the server can share a notification event
object, or the client can poll for data arrival. The data structure used by this
model is the ALPC completion list (not to be confused with the Windows I/O
completion port). The ALPC completion list is an efficient, nonblocking data
structure that enables atomic passing of data between clients, and its internals
are described further in the upcoming “Performance” section.
The next notification model is a waiting model that uses the Windows
completion-port mechanism (on top of the ALPC completion list). This
enables a thread to retrieve multiple payloads at once, control the maximum
number of concurrent requests, and take advantage of native completion-port
functionality. The user-mode thread pool implementation provides internal
APIs that processes use to manage ALPC messages within the same
infrastructure as worker threads, which are implemented using this model.
The RPC system in Windows, when using Local RPC (over ncalrpc), also
makes use of this functionality to provide efficient message delivery by
taking advantage of this kernel support, as does the kernel mode RPC
runtime in Msrpc.sys.
Finally, because drivers can run in arbitrary context and typically do not
like creating dedicated system threads for their operation, ALPC also
provides a mechanism for a more basic, kernel-based notification using
executive callback objects. A driver can register its own callback and context
with NtSetInformationAlpcPort, after which it will get called whenever a
message is received. The Power Dependency Coordinator (Pdc.sys) in the
kernel employs this mechanism for communicating with its clients, for
example. It’s worth noting that using an executive callback object has
potential advantages—but also security risks—in terms of performance.
Because the callbacks are executed in a blocking fashion (once signaled), and
inline with the signaling code, they will always run in the context of an
ALPC message sender (that is, inline with a user-mode thread calling
NtAlpcSendWaitReceivePort). This means that the kernel component can
have the chance to examine the state of its client without the cost of a context
switch and can potentially consume the payload in the context of the sender.
The reason these are not absolute guarantees, however (and this becomes a
risk if the implementor is unaware), is that multiple clients can send a
message to the port at the same time and existing messages can be sent by a
client before the server registers its executive callback object. It’s also
possible for another client to send yet another message while the server is
still processing the first message from a different client. In all these cases, the
server will run in the context of one of the clients that sent a message but
may be analyzing a message sent by a different client. The server should
distinguish this situation (since the Client ID of the sender is encoded in the
PORT_HEADER of the message) and attach/analyze the state of the correct
sender (which now has a potential context switch cost).
Views, regions, and sections
Instead of sending message buffers between their two respective processes, a
server and client can choose a more efficient data-passing mechanism that is
at the core of the memory manager in Windows: the section object. (More
information is available in Chapter 5 in Part 1.) This allows a piece of
memory to be allocated as shared and for both client and server to have a
consistent, and equal, view of this memory. In this scenario, as much data as
can fit can be transferred, and data is merely copied into one address range
and immediately available in the other. Unfortunately, shared-memory
communication, such as LPC traditionally provided, has its share of
drawbacks, especially when considering security ramifications. For one,
because both client and server must have access to the shared memory, an
unprivileged client can use this to corrupt the server’s shared memory and
even build executable payloads for potential exploits. Additionally, because
the client knows the location of the server’s data, it can use this information
to bypass ASLR protections. (See Chapter 5 in Part 1 for more information.)
ALPC provides its own security on top of what’s provided by section
objects. With ALPC, a specific ALPC section object must be created with the
appropriate NtAlpcCreatePortSection API, which creates the correct
references to the port, as well as allows for automatic section garbage
collection. (A manual API also exists for deletion.) As the owner of the
ALPC section object begins using the section, the allocated chunks are
created as ALPC regions, which represent a range of used addresses within
the section and add an extra reference to the message. Finally, within a range
of shared memory, the clients obtain views to this memory, which represents
the local mapping within their address space.
Regions also support a couple of security options. First, regions can be
mapped either using a secure mode or an unsecure mode. In the secure mode,
only two views (mappings) are allowed to the region. This is typically used
when a server wants to share data privately with a single client process.
Additionally, only one region for a given range of shared memory can be
opened from within the context of a given port. Finally, regions can also be
marked with write-access protection, which enables only one process context
(the server) to have write access to the view (by using
MmSecureVirtualMemoryAgainstWrites). Other clients, meanwhile, will
have read-only access only. These settings mitigate many privilege-
escalation attacks that could happen due to attacks on shared memory, and
they make ALPC more resilient than typical IPC mechanisms.
Attributes
ALPC provides more than simple message passing; it also enables specific
contextual information to be added to each message and have the kernel track
the validity, lifetime, and implementation of that information. Users of ALPC
can assign their own custom context information as well. Whether it’s
system-managed or user-managed, ALPC calls this data attributes. There are
seven attributes that the kernel manages:
■    The security attribute, which holds key information to allow
impersonation of clients, as well as advanced ALPC security
functionality (which is described later).
■    The data view attribute, responsible for managing the different views
associated with the regions of an ALPC section. It is also used to set
flags such as the auto-release flag, and when replying, to unmap a
view manually.
■    The context attribute, which allows user-managed context pointers to
be placed on a port, as well as on a specific message sent across the
port. In addition, a sequence number, message ID, and callback ID are
stored here and managed by the kernel, which allows uniqueness,
message-based hashing, and sequencing to be implemented by users
of ALPC.
■    The handle attribute, which contains information about which handles
to associate with the message (which is described in more detail later
in the “Handle passing” section).
■    The token attribute, which can be used to get the Token ID,
Authentication ID, and Modified ID of the message sender, without
using a full-blown security attribute (but which does not, on its own,
allow impersonation to occur).
■    The direct attribute, which is used when sending direct messages that
have a synchronization object associated with them (described later in
the “Direct event” section).
■    The work-on-behalf-of attribute, which is used to encode a work
ticket used for better power management and resource management
decisions (see the “Power management” section later).
Some of these attributes are initially passed in by the server or client when
the message is sent and converted into the kernel’s own internal ALPC
representation. If the ALPC user requests this data back, it is exposed back
securely. In a few cases, a server or client can always request an attribute,
because it is ALPC that internally associates it with a message and always
makes it available (such as the context or token attributes). By implementing
this kind of model and combining it with its own internal handle table,
described next, ALPC can keep critical data opaque between clients and
servers while still maintaining the true pointers in kernel mode.
To define attributes correctly, a variety of APIs are available for internal
ALPC consumers, such as AlpcInitializeMessageAttribute and
AlpcGetMessageAttribute.
Blobs, handles, and resources
Although the ALPC subsystem exposes only one Object Manager object type
(the port), it internally must manage a number of data structures that allow it
to perform the tasks required by its mechanisms. For example, ALPC needs
to allocate and track the messages associated with each port, as well as the
message attributes, which it must track for the duration of their lifetime.
Instead of using the Object Manager’s routines for data management, ALPC
implements its own lightweight objects called blobs. Just like objects, blobs
can automatically be allocated and garbage collected, reference tracked, and
locked through synchronization. Additionally, blobs can have custom
allocation and deallocation callbacks, which let their owners control extra
information that might need to be tracked for each blob. Finally, ALPC also
uses the executive’s handle table implementation (used for objects and
PIDs/TIDs) to have an ALPC-specific handle table, which allows ALPC to
generate private handles for blobs, instead of using pointers.
In the ALPC model, messages are blobs, for example, and their constructor
generates a message ID, which is itself a handle into ALPC’s handle table.
Other ALPC blobs include the following:
■    The connection blob, which stores the client and server
communication ports, as well as the server connection port and ALPC
handle table.
■    The security blob, which stores the security data necessary to allow
impersonation of a client. It stores the security attribute.
■    The section, region, and view blobs, which describe ALPC’s shared-
memory model. The view blob is ultimately responsible for storing
the data view attribute.
■    The reserve blob, which implements support for ALPC Reserve
Objects. (See the “Reserve objects” section earlier in this chapter.)
■    The handle data blob, which contains the information that enables
ALPC’s handle attribute support.
Because blobs are allocated from pageable memory, they must carefully be
tracked to ensure their deletion at the appropriate time. For certain kinds of
blobs, this is easy: for example, when an ALPC message is freed, the blob
used to contain it is also deleted. However, certain blobs can represent
numerous attributes attached to a single ALPC message, and the kernel must
manage their lifetime appropriately. For example, because a message can
have multiple views associated with it (when many clients have access to the
same shared memory), the views must be tracked with the messages that
reference them. ALPC implements this functionality by using a concept of
resources. Each message is associated with a resource list, and whenever a
blob associated with a message (that isn’t a simple pointer) is allocated, it is
also added as a resource of the message. In turn, the ALPC library provides
functionality for looking up, flushing, and deleting associated resources.
Security blobs, reserve blobs, and view blobs are all stored as resources.
Handle passing
A key feature of Unix Domain Sockets and Mach ports, which are the most
complex and most used IPC mechanisms on Linux and macOS, respectively,
is the ability to send a message that encodes a file descriptor which will then
be duplicated in the receiving process, granting it access to a UNIX-style file
(such as a pipe, socket, or actual file system location). With ALPC, Windows
can now also benefit from this model, with the handle attribute exposed by
ALPC. This attribute allows a sender to encode an object type, some
information about how to duplicate the handle, and the handle index in the
table of the sender. If the handle index matches the type of object the sender
is claiming to send, a duplicated handle is created, for the moment, in the
system (kernel) handle table. This first part guarantees that the sender truly is
sending what it is claiming, and that at this point, any operation the sender
might undertake does not invalidate the handle or the object beneath it.
Next, the receiver requests exposing the handle attribute, specifying the
type of object they expect. If there is a match, the kernel handle is duplicated
once more, this time as a user-mode handle in the table of the receiver (and
the kernel copy is now closed). The handle passing has been completed, and
the receiver is guaranteed to have a handle to the exact same object the
sender was referencing and of the type the receiver expects. Furthermore,
because the duplication is done by the kernel, it means a privileged server
can send a message to an unprivileged client without requiring the latter to
have any type of access to the sending process.
This handle-passing mechanism, when first implemented, was primarily
used by the Windows subsystem (CSRSS), which needs to be made aware of
any child processes created by existing Windows processes, so that they can
successfully connect to CSRSS when it is their turn to execute, with CSRSS
already knowing about their creation from the parent. It had several issues,
however, such as the inability to send more than a single handle (and
certainly not more than one type of object). It also forced receivers to always
receive any handle associated with a message on the port without knowing
ahead of time if the message should have a handle associated with it to begin
with.
To rectify these issues, Windows 8 and later now implement the indirect
handle passing mechanism, which allows sending multiple handles of
different types and allows receivers to manually retrieve handles on a per-
message basis. If a port accepts and enables such indirect handles (non-RPC-
based ALPC servers typically do not use indirect handles), handles will no
longer be automatically duplicated based on the handle attribute passed in
when receiving a new message with NtAlpcSendWaitReceivePort—instead,
ALPC clients and servers will have to manually query how many handles a
given message contains, allocate sufficient data structures to receive the
handle values and their types, and then request the duplication of all the
handles, parsing the ones that match the expected types (while
closing/dropping unexpected ones) by using
NtAlpcQueryInformationMessage and passing in the received message.
This new behavior also introduces a security benefit—instead of handles
being automatically duplicated as soon as the caller specifies a handle
attribute with a matching type, they are only duplicated when requested on a
per-message basis. Because a server might expect a handle for message A,
but not necessarily for all other messages, nonindirect handles can be
problematic if the server doesn’t think of closing any possible handle even
while parsing message B or C. With indirect handles, the server would never
call NtAlpcQueryInformationMessage for such messages, and the handles
would never be duplicated (or necessitate closing them).
Due to these improvements, the ALPC handle-passing mechanism is now
exposed beyond just the limited use-cases described and is integrated with
the RPC runtime and IDL compiler. It is now possible to use the
system_handle(sh_type) syntax to indicate more than 20 different handle
types that the RPC runtime can marshal from a client to a server (or vice-
versa). Furthermore, although ALPC provides the type checking from the
kernel’s perspective, as described earlier, the RPC runtime itself also does
additional type checking—for example, while both named pipes, sockets, and
actual files are all “File Objects” (and thus handles of type “File”), the RPC
runtime can do marshalling and unmarshalling checks to specifically detect
whether a Socket handle is being passed when the IDL file indicates
system_handle(sh_pipe), for example (this is done by calling APIs such as
GetFileAttribute, GetDeviceType, and so on).
This new capability is heavily leveraged by the AppContainer
infrastructure and is the key way through which the WinRT API transfers
handles that are opened by the various brokers (after doing capability checks)
and duplicated back into the sandboxed application for direct use. Other RPC
services that leverage this functionality include the DNS Client, which uses it
to populate the ai_resolutionhandle field in the GetAddrInfoEx API.
Security
ALPC implements several security mechanisms, full security boundaries, and
mitigations to prevent attacks in case of generic IPC parsing bugs. At a base
level, ALPC port objects are managed by the same Object Manager interfaces
that manage object security, preventing nonprivileged applications from
obtaining handles to server ports with ACL. On top of that, ALPC provides a
SID-based trust model, inherited from the original LPC design. This model
enables clients to validate the server they are connecting to by relying on
more than just the port name. With a secured port, the client process submits
to the kernel the SID of the server process it expects on the side of the
endpoint. At connection time, the kernel validates that the client is indeed
connecting to the expected server, mitigating namespace squatting attacks
where an untrusted server creates a port to spoof a server.
ALPC also allows both clients and servers to atomically and uniquely
identify the thread and process responsible for each message. It also supports
the full Windows impersonation model through the
NtAlpcImpersonateClientThread API. Other APIs give an ALPC server the
ability to query the SIDs associated with all connected clients and to query
the LUID (locally unique identifier) of the client’s security token (which is
further described in Chapter 7 of Part 1).
ALPC port ownership
The concept of port ownership is important to ALPC because it provides a
variety of security guarantees to interested clients and servers. First and
foremost, only the owner of an ALPC connection port can accept connections
on the port. This ensures that if a port handle were to be somehow duplicated
or inherited into another process, it would not be able to illegitimately accept
incoming connections. Additionally, when handle attributes are used (direct
or indirect), they are always duplicated in the context of the port owner
process, regardless of who may be currently parsing the message.
These checks are highly relevant when a kernel component might be
communicating with a client using ALPC—the kernel component may
currently be attached to a completely different process (or even be operating
as part of the System process with a system thread consuming the ALPC port
messages), and knowledge of the port owner means ALPC does not
incorrectly rely on the current process.
Conversely, however, it may be beneficial for a kernel component to
arbitrarily accept incoming connections on a port regardless of the current
process. One poignant example of this issue is when an executive callback
object is used for message delivery. In this scenario, because the callback is
synchronously called in the context of one or more sender processes, whereas
the kernel connection port was likely created while executing in the System
context (such as in DriverEntry), there would be a mismatch between the
current process and the port owner process during the acceptance of the
connection. ALPC provides a special port attribute flag—which only kernel
callers can use—that marks a connection port as a system port; in such a
case, the port owner checks are ignored.
Another important use case of port ownership is when performing server
SID validation checks if a client has requested it, as was described in the
“Security” section. This validation is always done by checking against the
token of the owner of the connection port, regardless of who may be listening
for messages on the port at this time.
Performance
ALPC uses several strategies to enhance performance, primarily through its
support of completion lists, which were briefly described earlier. At the
kernel level, a completion list is essentially a user Memory Descriptor List
(MDL) that’s been probed and locked and then mapped to an address. (For
more information on MDLs, see Chapter 5 in Part 1.) Because it’s associated
with an MDL (which tracks physical pages), when a client sends a message
to a server, the payload copy can happen directly at the physical level instead
of requiring the kernel to double-buffer the message, as is common in other
IPC mechanisms.
The completion list itself is implemented as a 64-bit queue of completed
entries, and both user-mode and kernel-mode consumers can use an
interlocked compare-exchange operation to insert and remove entries from
the queue. Furthermore, to simplify allocations, once an MDL has been
initialized, a bitmap is used to identify available areas of memory that can be
used to hold new messages that are still being queued. The bitmap algorithm
also uses native lock instructions on the processor to provide atomic
allocation and deallocation of areas of physical memory that can be used by