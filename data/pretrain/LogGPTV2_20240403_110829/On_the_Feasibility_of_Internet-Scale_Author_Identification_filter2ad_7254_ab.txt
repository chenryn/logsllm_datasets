to Writeprints [23], especially in our approach to feature
extraction.
The primary application of author identiﬁcation during
this period shifted from literary attribution to forensics —
identifying authors of terroristic threats, harassing messages,
etc. In the forensic context, the length of the text to be
classiﬁed is typically far shorter and the known and unknown
texts might be drawn from very different domains (e.g.,
academic writing versus a threatening letter) [25]. Some
forensic linguists who focus on legal admissibility scorn
the use of online corpora for training of classiﬁers due to
concerns over the level of conﬁdence in the ground truth
labels [26].
Stylometry has various goals other than authorship attri-
bution including testing for multiple authorship [27], au-
thenticity veriﬁcation (e.g., of suicide notes, disputed wills,
1i.e., what we now call the Naive Bayes classiﬁer.
The susceptibility of character n-gram–based features to
302
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:49:27 UTC from IEEE Xplore.  Restrictions apply. 
etc.) [25], detection of hoaxes, frauds and deception [28],
text genre classiﬁcation [29] and author proﬁling (e.g., age,
gender, native language, etc.) [30].
Stylometry has been used to attribute authorship in do-
mains other than text, such as music [31] and code [32],
which also have grammars and other linguistic features
shared with natural language. Other forensic tasks such as
identifying the ﬁle type of a binary blob [33] and recovering
text typed on a keyboard based on acoustic emanations [34]
use similar techniques, although the models are simpler than
linguistic stylometry.
The study of authorship attribution is scattered across
various disciplines, and is not limited to stylometric tech-
niques. We point the reader to Juola’s excellent survey [35].
Stylistics in literary criticism also has authorship attribution
as one of its goals, but it is subjective and non-quantitative;
attempts to apply it to forensic author identiﬁcation have
been critized — rightly, in our opinion — as pseudoscience
[36].
Plagiarism detection can be seen as complementary to sty-
lometric authorship attribution: it attempts to detect common
content between documents, even if the style may have been
changed [37].
Privacy and online anonymity. Very little work has been
done to investigate the privacy implications of stylometry.
In 2000 Rao and Rohatgi studied whether individuals post-
ing under different pseudonyms to USENET newsgroups
can have these pseudonyms linked based on writing style
[38]. They used function words and Principal Component
Analysis on a dataset of 185 pseudonyms belonging to
117 individuals to cluster pseudonyms belonging to the
same individual. They also performed a smaller experiment
demonstrating that matching between newsgroup and e-
mail domains was possible, but they did not ﬁnd matching
between RFC and newsgroup postings to be feasible.
Only Koppel et al, have attempted to perform author
identiﬁcation at anything approaching “Internet scale” in
terms of the number of authors. They reported preliminary
experiments in 2006 [39] and follow-up work in 2011 [11]
where they study authorship recognition with a 10,000-
authors blog corpus. This work is intriguing but raises a
number of methodological concerns.
The authors use only 4-grams of characters as features.
It is not clear to what extent identiﬁcation is based on
recognition of the author vs.
the context. On the other
hand, we use various linguistic features that are known to
characterize authors, and explicitly eschew context-speciﬁc
features such as topic markers.
Indeed, character-based analysis is likely to be very
susceptible to topic biases, and Koppel et al. state that
this distinction is immaterial to them. Therefore, while an
interesting and impressive result, it is hard to characterize
their work as stylometric authorship recognition.
context biases is exacerbated by the fact that (as far as we
can tell) Koppel et al. perform no pre-processing to remove
common substrings such as signatures at the end of posts.
Therefore we suggest that the results should be interpreted
with caution unless the technique can be validated on a
cross-context dataset.
Similarly, in another paper, Koppel et al. again study
authorship recognition on a 10,000-author blog corpus [30].
Here they test both content-based features (speciﬁcally,
1,000 words) and stylistic features. The former succeeds 52–
56% of the time, whereas the latter only 6% of the time.
In this paper, the authors note that many character n-grams
“will be closely associated to particular content words and
roots,” in line with our observations above.
Moving on, Nanavati et al. showed that stylometry enables
identifying reviewers of research papers with reasonably
high accuracy, given that the adversary, assumed to be a
member of the community, has access to a large number
of unblinded reviews of potential reviewers by serving on
conference and grant selection committees [40]. Several re-
searchers have considered whether the author of an academic
paper under blind review might be identiﬁed solely from the
citations [41; 42].
Other research in this area has investigated manual and
semi-automated techniques for transforming text to success-
fully resist identiﬁcation [16; 17]. These papers consider off-
the-shelf stylometric attacks; the resilience of obfuscation to
an attack crafted to take it into account has not been studied
and is a topic for future research.
A variety of recent technological developments have made
it possible for a website to track visitors and to learn their
identity in some circumstances. First, browser ﬁngerprinting
allows tracking a user across sites and across sessions [43].
Second, by exploiting bugs such as history stealing [44], a
malicious site might be able to ﬁnd a visitor’s identity if they
use popular social networking sites. These deanonymization
techniques can be seen as complementary to ours.
III. ATTACK MODEL AND EXPERIMENTAL
METHODOLOGY
In this section, we discuss how a real attack would work,
the motivation behind our experimental design and what we
hope to learn from the experiments.
Primarily, we wish to simulate an attempt to identify the
author of an anonymously published blog. If the author
is careful to avoid revealing their IP address or any other
explicit identiﬁer, their adversary (e.g., a government censor)
may turn to an analysis of writing style. We assume that
the author does not make any attempt to hide their writing
style, whether due to lack of awareness of the possibility of
stylometric deanonymization, or lack of tools to do so. A
semi-automated obfuscation tool was presented in [17].
By comparing the posts of the anonymous blog with a
corpus of samples taken from many other blogs throughout
the Internet, the adversary may hope to ﬁnd a second, more
easily identiﬁed blog by the same author. To have any
hope of success, the adversary will need to compare the
anonymous text to far more samples than could be done
manually, so they can instead extract numerical features
and conduct an automated search for statistically similar
samples.
This approach may not yield conclusive proof of a match;
instead, we imagine the adversary’s tools returning a list
of the most similar possibilities for manual followup. A
manual examination may incorporate several characteristics
that the automated analysis does not, such as choice of
topic(s) (as we explain later, our algorithms are “topic-
free”), location2 etc. Alternately, a powerful adversary such
as law enforcement may require Blogger, WordPress, or
another popular blog host to reveal the login times of the
top suspects, which could be correlated with the timing of
posts on the anonymous blog to conﬁrm a match. Thus the
adversary’s goal might be to merely narrow the ﬁeld of
possible authors of the anonymous text enough that another
approach to identifying the author becomes feasible. As a
result, in our experiments we test classiﬁers which return a
ranking of classes by likelihood, rather than those which can
only return the most likely class.
Conversely, the adversary might be interested in casting a
wide net, looking to unmask one or some few of a group of
anonymous authors. In this case, it would help the adversary
to have conﬁdence estimates of each output, so that he
can focus on the ones most likely to be correct matches.
Therefore we consider conﬁdence estimation an important
goal.
As in many other research projects, the main challenge in
designing our experiments is the absence of a large dataset
labeled with ground truth. To measure the feasibility of
2For example, if we were trying to identify the author of the once-
anonymous blog Washingtonienne [53] we’d know that she almost certainly
lives in or around Washington, D.C.
Recent work has also demonstrated the ease of connecting
different online proﬁles of a person [45; 46]; this makes it
easier for an adversary carrying out our attack to assemble
labeled texts for large numbers of Internet users.
Behavioral biometrics — voice [47], gait [48], hand-
writing [49], keystroke dynamics [50], etc. — are ways
to ﬁngerprint an individual that go beyond purely physical
characteristics. Stylometric ﬁngerprints can be considered a
extension of behavioral biometrics. Our work can also be
seen as an extension of deanonymization research: while
Sweeney showed that a combination of our attributes is
surprisingly unique [51], Narayanan and Shmatikov showed
that the same principle applies to our tastes and preferences
[52]. The present work focuses on style and behavior;
the respective deanonymization algorithms show a natural
progression in complexity as well as in the amount of data
required.
303
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:49:27 UTC from IEEE Xplore.  Restrictions apply. 
Category
Length
Vocabulary
richness
Word shape
Word length
Letters
Digits
Punctuation
Special
characters
Function
words
Syntactic cate-
gory pairs
Description
number of words/characters in post
Yule’s K3 and frequency of hapax legomena,
dis legomena, etc.
frequency of words with different combina-
tions of upper and lower case letters.4
frequency of words that have 1–20 characters
frequency of a to z, ignoring case
frequency of 0 to 9
frequency of .?!,;:()"-’
frequency
special
‘˜@#$%ˆ&*_+=[]{}\|/<>
frequency of words like ‘the’, ‘of’, and ‘then’
characters
other
of
frequency of every pair (A, B), where A is
the parent of B in the parse tree
Count
2
11
5
20
26
10
11
21
293
789
THE FEATURES USED FOR CLASSIFICATION. MOST TAKE THE FORM OF
FREQUENCIES, AND ALL ARE REAL-VALUED.
Table I
matching one blog to another from the same author, we
need a set of blogs already grouped by author. However,
manually collecting a large number of blogs grouped this
way is impractical, if the experiments are to be anything
resembling “Internet scale.”
Therefore, our ﬁrst approach to conducting experiments is
to simulate the case of an individual publishing two blogs by
dividing the posts of a single blog into two groups; we then
measure our ability to match the two groups of posts back
together. Speciﬁcally, we select one of a large number of
blogs and set aside several of its posts for testing. These test
posts represent the anonymously authored content, while the
other posts of that blog represent additional material from
the same author found elsewhere on the Internet. We next
train a classiﬁer to recognize the writing style of each of
the blogs in the entire dataset, taking care to exclude the
test posts when training on the blog from which they were
taken. After completing the training, we present the test posts
to the classiﬁer and use it to rank all the blogs according to
their estimated likelihood of producing the test posts. If the
source of the test posts appears near the top of the resulting
list of blogs, the writing style of its author may be considered
especially identiﬁable. As will be shown in Section VI, our
experiences applying this process have revealed surprisingly
high levels of identiﬁability: using only three test posts, the
correct blog is ranked ﬁrst out of 100,000 in 20% of trials.
At this point, the astute reader is no doubt brimming
with objections to the methodology described above. How
can we be sure that any linking we detect in this way is
unintentional? Suppose a blog author signs each of their
posts by adding their name at the end; they would not
be at all surprised to discover that an automated tool can
3We computed Yule’s K in the same way as in [23].
4All upper case, all lower case, only ﬁrst letter upper case, camel case
(CamelCase), and everything else.
304