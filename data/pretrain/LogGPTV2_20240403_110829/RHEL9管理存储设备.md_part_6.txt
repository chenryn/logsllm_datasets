    到期前和队列未阻塞前，它不会失败。
    如果 `fast_io_fail_tmo`{.literal} 被设为不是 off
    的任何值时，则会取消封 `dev_loss_tmo`{.literal} 的上限。如果
    `fast_io_fail_tmo`{.literal} 设为
    off，则在设备从系统中删除之前不会出现 I/O 失败。如果
    `fast_io_fail_tmo`{.literal} 设置为一个数字，则在达到
    `fast_io_fail_tmo`{.literal} 设置的超时会立即触发 I/O 失败。
:::
::: itemizedlist
**主机设置 `/sys/class/fc_host/hostH/`{.literal}**
-   `port_id`{.literal}
-   `node_name`{.literal}
-   `port_name`{.literal}
-   `issue_lip`{.literal}
    指示驱动重新发现远程端口。
:::
:::
::: section
::: titlepage
# []{#using-fibre-channel-devices_managing-storage-devices.html#dm-multipath-overrides-of-the-device-timeout_using-fibre-channel-devices}DM 多路径覆盖设备超时 {.title}
:::
`restore_tmo`{.literal} `sysfs`{.literal} 选项控制一个特定 iSCSI
设备的超时时间。以下选项全局覆盖 `recovery_tmo`{.literal} 值：
::: itemizedlist
-   `replacement_timeout`{.literal} 配置选项会全局覆盖所有 iSCSI 设备的
    `recovery_tmo`{.literal} 值。
-   对于由 DM 多路径管理的所有 iSCSI 设备，DM 多路径中的
    `fast_io_fail_tmo`{.literal} 选项会全局覆盖 `recovery_tmo`{.literal}
    值。
    DM 多路径中的 `fast_io_fail_tmo`{.literal} 选项会覆盖光纤通道设备的
    `fast_io_fail_tmo`{.literal} 选项。
:::
DM 多路径 `fast_io_fail_tmo`{.literal} 选项优先于
`replacement_timeout`{.literal}。红帽不推荐使用
`replacement_timeout`{.literal} 在由 DM 多路径管理
`的设备中覆盖 restore_tmo`{.literal}，因为当多路径服务重新加载时，DM
多路径总是重置 restore `_tmo`{.literal}。``{.literal}
:::
:::
[]{#nvme-over-fabrics-using-rdma_managing-storage-devices.html}
::: chapter
::: titlepage
# []{#nvme-over-fabrics-using-rdma_managing-storage-devices.html#nvme-over-fabrics-using-rdma_managing-storage-devices}第 5 章 使用 RDMA 的 NVMe over fabrics {.title}
:::
在 NVMe over RDMA(NVMe/RDMA)设置中，您配置 NVMe 目标和 NVMe initiator。
作为系统管理员，完成以下任务来部署 NVMe/RDMA 设置：
::: itemizedlist
-   [使用 configfs 设置 NVMe/RDMA
    目标](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/managing_storage_devices/nvme-over-fabrics-using-rdma_managing-storage-devices#setting-up-an-nvme-rdma-target-using-configfs_nvme-over-fabrics-using-rdma){.link}
-   [使用 nvmetcli 设置 NVMe/RDMA
    目标](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/managing_storage_devices/nvme-over-fabrics-using-rdma_managing-storage-devices#setting-up-the-nvme-rdma-target-using-nvmetcli_nvme-over-fabrics-using-rdma){.link}
-   [配置 NVMe/RDMA
    客户端](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/managing_storage_devices/nvme-over-fabrics-using-rdma_managing-storage-devices#configuring-an-nvme-rdma-client_nvme-over-fabrics-using-rdma){.link}
:::
::: section
::: titlepage
# []{#nvme-over-fabrics-using-rdma_managing-storage-devices.html#overview-of-nvme-over-fabric-devices_nvme-over-fabrics-using-rdma}NVMe over fabric 设备概述 {.title}
:::
Non-volatile Memory
Express(NVMe)是一个接口，它允许主机软件实用程序与固态驱动器通信。
使用以下类型的光纤传输来通过光纤设备配置 NVMe：
::: variablelist
[NVMe over Remote Direct Memory Access(NVMe/RDMA)]{.term}
:   有关如何配置 NVMe/RDMA 的详情，请参考[使用 RDMA 的 NVMe over
    fabrics](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/managing_storage_devices/nvme-over-fabrics-using-rdma_managing-storage-devices){.link}。
[NVMe over Fibre Channel(FC-NVMe)]{.term}
:   有关如何配置 FC-NVMe 的详情，请参考[使用 FC 的 NVMe over
    fabrics](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/managing_storage_devices/nvme-over-fabrics-using-fc_managing-storage-devices){.link}。
:::
当使用 Fibre Channel(FC)和 Remote Direct Memory
Access(RDMA)时，固态驱动器不必对您的系统进行本地设置；可以通过 FC 或
RDMA 控制器远程配置。
:::
::: section
::: titlepage
# []{#nvme-over-fabrics-using-rdma_managing-storage-devices.html#setting-up-an-nvme-rdma-target-using-configfs_nvme-over-fabrics-using-rdma}使用 configfs 设置 NVMe/RDMA 目标 {.title}
:::
使用这个步骤使用 `configfs`{.literal} 配置 NVMe/RDMA 目标。
::: itemizedlist
**先决条件**
-   验证您有一个要分配给 `nvmet`{.literal} 子系统的块设备。
:::
::: orderedlist
**步骤**
1.  创建 `nvmet-rdma`{.literal} 子系统：
    ``` screen
    # modprobe nvmet-rdma
    # mkdir /sys/kernel/config/nvmet/subsystems/testnqn
    # cd /sys/kernel/config/nvmet/subsystems/testnqn
    ```
    使用子系统名称替换 [*testnqn*]{.emphasis}。
2.  允许任何主机连接到这个目标：
    ``` screen
    # echo 1 > attr_allow_any_host
    ```
3.  配置命名空间：
    ``` screen
    # mkdir namespaces/10
    # cd namespaces/10
    ```
    使用命名空间号替换 [*10*]{.emphasis}
4.  设置 NVMe 设备的路径：
    ``` screen
    # echo -n /dev/nvme0n1 > device_path
    ```
5.  启用命名空间：
    ``` screen
    # echo 1 > enable
    ```
6.  创建带有 NVMe 端口的目录：
    ``` screen
    # mkdir /sys/kernel/config/nvmet/ports/1
    # cd /sys/kernel/config/nvmet/ports/1
    ```
7.  显示 [*mlx5_ib0*]{.emphasis} 的 IP 地址：
    ``` literallayout
    # ip addr show mlx5_ib0
    8: mlx5_ib0:  mtu 4092 qdisc mq state UP group default qlen 256
        link/infiniband 00:00:06:2f:fe:80:00:00:00:00:00:00:e4:1d:2d:03:00:e7:0f:f6 brd 00:ff:ff:ff:ff:12:40:1b:ff:ff:00:00:00:00:00:00:ff:ff:ff:ff
        inet 172.31.0.202/24 brd 172.31.0.255 scope global noprefixroute mlx5_ib0
           valid_lft forever preferred_lft forever
        inet6 fe80::e61d:2d03:e7:ff6/64 scope link noprefixroute
           valid_lft forever preferred_lft forever
    ```
8.  为目标设置传输地址：
    ``` screen
    # echo -n 172.31.0.202 > addr_traddr
    ```
9.  将 RDMA 设置为传输类型：
    ``` screen
    # echo rdma > addr_trtype
    # echo 4420 > addr_trsvcid
    ```
10. 为端口设置地址系列：
    ``` screen
    # echo ipv4 > addr_adrfam
    ```
11. 创建软链接：
    ``` literallayout
    # ln -s /sys/kernel/config/nvmet/subsystems/testnqn   /sys/kernel/config/nvmet/ports/1/subsystems/testnqn
    ```
:::
::: itemizedlist
**验证**
-   验证 NVMe 目标是否在指定端口上侦听并准备好连接请求：
    ``` screen
    # dmesg | grep "enabling port"
    [ 1091.413648] nvmet_rdma: enabling port 1 (172.31.0.202:4420)
    ```
:::
::: itemizedlist
**其他资源**
-   `nvme(1)`{.literal} man page
:::
:::
::: section
::: titlepage
# []{#nvme-over-fabrics-using-rdma_managing-storage-devices.html#setting-up-the-nvme-rdma-target-using-nvmetcli_nvme-over-fabrics-using-rdma}使用 nvmetcli 设置 NVMe/RDMA 目标 {.title}
:::
使用 `nvmetcli`{.literal} 实用程序编辑、查看和启动 NVMe
目标。`nvmetcli`{.literal} 实用程序提供命令行和交互式 shell
选项。使用这个步骤为 `nvmetcli`{.literal} 配置 NVMe/RDMA 目标。
::: itemizedlist
**先决条件**
-   验证您有一个要分配给 `nvmet`{.literal} 子系统的块设备。
-   以 root 用户身份执行以下 `nvmetcli`{.literal} 操作。
:::
::: orderedlist
**步骤**
1.  安装 `nvmetcli`{.literal} 软件包：
    ``` screen
    # dnf install nvmetcli
    ```
2.  下载 `rdma.json`{.literal} 文件：
    ``` literallayout
    # wget http://git.infradead.org/users/hch/nvmetcli.git/blob_plain/0a6b088db2dc2e5de11e6f23f1e890e4b54fee64:/rdma.json
    ```
3.  编辑 `rdma.json`{.literal} 文件，并将 `traddr`{.literal} 值更改为
    `172.31.0.202`{.literal}。
4.  通过载入 NVMe 目标配置文件来设置目标：
    ``` screen
    # nvmetcli restore rdma.json
    ```
:::
::: {.note style="margin-left: 0.5in; margin-right: 0.5in;"}
### 注意 {.title}
如果没有指定 NVMe 目标配置文件名称，`nvmetcli`{.literal} 将使用
`/etc/nvmet/config.json`{.literal} 文件。
:::
::: itemizedlist
**验证**
-   验证 NVMe 目标是否在指定端口上侦听并准备好连接请求：
    ``` screen
    # dmesg | tail -1
    [ 4797.132647] nvmet_rdma: enabling port 2 (172.31.0.202:4420)
    ```
-   可选：清除当前的 NVMe 目标：
    ``` screen
    # nvmetcli clear
    ```
:::
::: itemizedlist
**其他资源**
-   `nvmetcli`{.literal} 和 `nvme(1)`{.literal} man page
:::
:::
::: section
::: titlepage
# []{#nvme-over-fabrics-using-rdma_managing-storage-devices.html#configuring-an-nvme-rdma-client_nvme-over-fabrics-using-rdma}配置 NVMe/RDMA 客户端 {.title}
:::
使用这个步骤使用 NVMe 管理命令行界面(`nvme-cli`{.literal})工具配置
NVMe/RDMA 客户端。
::: orderedlist
**步骤**
1.  安装 `nvme-cli`{.literal} 工具：
    ``` screen
    # dnf install nvme-cli
    ```
2.  如果没有加载，则加载 `nvme-rdma`{.literal} 模块：
    ``` screen
    # modprobe nvme-rdma
    ```
3.  在 NVMe 目标中发现可用子系统：
    ``` screen
    # nvme discover -t rdma -a 172.31.0.202 -s 4420
    Discovery Log Number of Records 1, Generation counter 2
    =====Discovery Log Entry 0======
    trtype:  rdma
    adrfam:  ipv4
    subtype: nvme subsystem
    treq:    not specified, sq flow control disable supported
    portid:  1
    trsvcid: 4420
    subnqn:  testnqn
    traddr:  172.31.0.202
    rdma_prtype: not specified
    rdma_qptype: connected
    rdma_cms:    rdma-cm
    rdma_pkey: 0x0000
    ```
4.  连接到发现的子系统：
    ``` screen
    # nvme connect -t rdma -n testnqn -a 172.31.0.202 -s 4420