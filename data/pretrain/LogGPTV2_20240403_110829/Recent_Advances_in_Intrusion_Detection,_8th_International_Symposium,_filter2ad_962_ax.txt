(b) The CardGuard architecture
Fig. 2. Architecture
110
H. Bos and K. Huang
As shown in Figure 2(b), one of the Gigabit ports in CardGuard (port A)
connects to the outside world, while the other (port B) is connected to the
switch. The Gigabit ports are used for all data traﬃc between the hosts and the
NPU. In addition to the Gigabit datapath, there also exists a control connection
between the host processor and the IXP1200, which in the implementation on the
ENP2506 consists of messages sent across the PCI bus of the machine hosting the
NPU. The thick dashed line in Figure 2(b) indicates that it is quite permissible
to plug the board in the PCI slot of one of the end hosts, making CardGuard a
rather low-cost solution in terms of hardware. In the latter case, the same PCI
bus is used to transport CardGuard control messages and user traﬃc.
CardGuard is designed as a plug-and-play intrusion detection system. To
protect a set of hosts connected to a switch as depicted in Figure 2(b), all that
is required is that CardGuard is placed on the datapath between the switch and
the outside world. No reconﬁguration of the end-systems is necessary.
4 Software Architecture
In Figure 2(b), the numbering indicates the major components in CardGuard
packet processing. Since the system is designed as a ﬁrewall, both inbound and
outbound traﬃc must be handled. By default, outbound traﬃc is simply for-
warded, but inbound traﬃc is subjected to full payload scans. In case outbound
traﬃc should be checked also (e.g., for containment) the performance ﬁgures of
Section 5 drop by a factor of two. CardGuard aims to perform as much of the
packet processing as possible on the lowest levels of the processing hierarchy,
i.e., the microengines.
A single microengine (MEtx) is dedicated to forwarding and transmission. In
other words, MEtxis responsible not only for forwarding all outbound traﬃc 1(cid:3),
but also for transmitting inbound traﬃc toward the switch 4(cid:3). All four threads
on MEtx are used, as each of the two tasks is handled by two threads.
A second microengine, MErx, is dedicated to inbound packet reception 2(cid:3).
It consists of two threads that place incoming packets in the ﬁxed-sized slots of
a circular buﬀer. While we use ﬁxed-sized slots, there may be more than one
packet in a slot. MErx keeps placing packets in a slot, as long as the full packets
ﬁt. The motivation for this design is that a buﬀer with ﬁxed-sized slots is easy to
manage and partition, but may suﬀer from low slot utilisation for short messages.
By ﬁlling slots with multiple packets resource utilisation is much better. At this
microengine we also detect whether packets belong to a stream that needs to be
checked by a rule that requires regular expression matching. If so, it is placed in
a queue for processing by the StrongARM. The StrongARM is responsible for
processing the packet and, if needed, putting it back in the queue for processing
by the microengines.
The remaining four microengines (denoted by MEh/t and MEac, respectively)
are dedicated to TCP ﬂow handling and intrusion detection 3(cid:3). The idea is to use
these microengines to scan the packet payloads with the Aho-Corasick algorithm.
Towards Software-Based Signature Detection for Intrusion Prevention
111
Parallelising the scanning process in this way is in line with the fourth observation
in Section 2.2. However, we now show that things are more complicated.
Attacks may span multiple packets. We should provide a means to handle
the case that the signature of a worm starts in one packet and continues in the
next. In order to perform meaningful intrusion detection, we cannot avoid TCP
stream reconstruction. Worms may span a number of TCP segments which may
or may not arrive out of order. As a consequence, we need to keep segments
in memory while some earlier segments are still missing. We also need to keep
track of sequence numbers and connection state. We developed a light-weight
implementation of TCP stream reconstruction for microengines, which we will
discuss in section 4.2. CardGuard handles ‘fragroute’-style attacks (sending du-
plicate TCP segments with older TCP sequence numbers that overwrite previous
segments) by dropping segments with sequence numbers that have already been
handled. A conﬁgurable parameter determines how large the gaps may be in
case of out-of-order segment arrival. If the gap grows beyond the maximum size,
the connection is conservatively dropped.
Despite our attempts to minimise CardGuard’s footprint, the code required
to handle both TCP ﬂow reconstruction and pattern matching exceeds the size
of an individual microengine’s instruction store. As a consequence, we are forced
to spread TCP ﬂow hashing and reconstruction (discussed in Section 4.2) on the
one hand, and Aho-Corasick pattern matching (discussed in Section 4.3) on the
other, over two pairs of tightly-coupled microengines (shown in Figure 2(b) as
H/T and AC, respectively). Given suﬃcient instruction store, H/T and AC would
be combined on the same microengine (yielding four microengines to perform
pattern matching rather than the two that are used in CardGuard). All four
threads in both AC and H/T microengines are used.
The ability to ‘sanitise’ protocols before scanning the data for intrusion at-
tempts is similar to the protocol scrubber [19] and norm [20], except that it was
implemented in a much more resource-constrained environment.
4.1 TCP vs. UDP
By default, all traﬃc that is not TCP (e.g., UDP) is handled by inspecting the
individual packets in isolation and is considered relatively ‘easy’. As a result,
signatures hidden in multiple packets (‘UDP ﬂows’) will not be detected in the
default conﬁguration. If needed, however, the UDP packets may be treated in
the same way as TCP ﬂows are handled. In that case, we lose the performance
advantage that UDP holds over TCP (see Section 5). As our focus is on the
harder case of TCP ﬂows, which also covers all diﬃculties found in UDP, we will
not discuss non-TCP traﬃc except in the experimental evaluation.
In the current implementation, the ENP’s PCI interface is used for control
purposes 5(cid:3). In other words, it is used for bootstrapping the system, loading the
MEac microengines and reading results and statistics. In our test conﬁguration,
CardGuard is plugged in one of the PCs that it monitors. Although such a setup
in which the host appears to be both ‘in front of’ and ‘behind’ the ﬁrewall may
112
H. Bos and K. Huang
seem a little odd, it does not represent a security hole as all inbound traﬃc still
traverses the packet processing code in the MEac microengines.
CardGuard attempts to execute the entire runtime part of the SDS on the
microengines. The only exception is the execution of regular expression matching
which takes place on the StrongARM. Each thread in the combination of MEh/t
and MEac processes a unique and statically determined set of packet slots (as will
be explained shortly). The fact that a slot may contain multiple small packets,
or a single maximum-sized packet oﬀers an additional advantage besides better
buﬀer utilisation, namely load-balancing. Without it, a situation may arise that
thread A processes a number of slots each containing just a single minimum-sized
packet, while thread B ﬁnds all its slots ﬁlled with maximum-sized packets. By
trying to ﬁll all the slots to capacity, this is less likely to happen.
After ﬂow reconstruction, MEac applies the Aho-Corasick algorithm to its
packets while taking care to preserve the ﬂow order. As we conﬁgured CardGuard
as an IPS, the microengine raises an alarm and drops the packet as soon as a
pattern is matched. Otherwise, a reference to the packet is placed in the transmit
FIFO and transmitted by MEtx. When processing completes, buﬀers are marked
as available for re-use.
4.2 Resource Mapping
Taking into account the hardware limitations described in Section 3 and the ob-
servations about the Aho-Corasick algorithm in Section 2.2, we now describe how
data and code are mapped on the memories and processing units, respectively.
Packet transmission. Two threads on MEtx are dedicated to the task of for-
warding outbound traﬃc from port B to port A. The other two threads trans-
mit packets from SDRAM to port B by monitoring a circular FIFO containing
references to packets that passed the MEac checks. The FIFO is ﬁlled by the
processing threads on the MEac microengines.
Packet reception. Packet processing of
inbound traﬃc is illustrated in
Figure 3(a). We take the usual approach of receiving packets in SDRAM, and
keeping control structures in SRAM and Scratch. Assuming there is enough
space, MErx transfers the packets to a circular buﬀer, and keeps a record of the
read and write position, as well as a structure indicating the validity of the con-
tents of the buﬀer in SRAM. Using this structure, an MEac processing packets
may indicate that it is done with speciﬁc buﬀers, enabling MErx to reuse them.
The exact way in which the buﬀers are used in CardGuard is less common. The
moment an in-sequence packet is received and stored in full in SDRAM by MErx,
it can be processed by the processing threads. However, the processing has to be
suﬃciently fast to prevent buﬀer overﬂow. A buﬀer overﬂow is not acceptable,
as it means that packets are dropped. We have designed the system in such a
way that the number of per-packet checks is minimised, possibly at the expense
of eﬃcient buﬀer usage. Whenever MErx reaches the end of the circular buﬀer
and the write index is about to wrap, MErx checks to see how far the packet
processing microengines have progressed through the buﬀer. In CardGuard the
slowest thread should always have progressed beyond a certain threshold index
Towards Software-Based Signature Detection for Intrusion Prevention
113
in the buﬀer (T in Figure 3(a)). CardGuard conservatively considers all cases
in which threads are slow as system failures, which in this case means that
CardGuard is not capable of handling the traﬃc rate.
As both the worst-case execution time for the Aho-Corasick algorithm (the
maximum time it takes to process a packet), and the worst-case time for receiv-
ing packets (the minimum time to receive and store a packet) are known, it is
not diﬃcult to estimate a safe value for the threshold T for a speciﬁc rate R and
a buﬀer size of B slots. For simplicity, and without loss of generality, assume that
a slot contains at most one packet. For the slowest thread, the maximum number
of packets in the buﬀer at wrap time that is still acceptable is (B − T ). If the
worst-case execution time for a packet is A, it may take A(B− T ) seconds to ﬁn-
ish processing these packets. The time it takes to receive a minimum-size packet
of length L at rate R is (L/R), assuming MErx is able to handle rate R. An over-
ﬂow occurs if (T L/R) ≤ A(B − T ), so T = (RAB)/(L + RA). For L = 64 bytes,
R = 100 Mbps, B = 1000 slots, and A = 10μs, a safe value for T would be 661.
The threshold mechanism described above is overly conservative. Threads that
have not reached the appropriate threshold when MErx wraps may still catch
up, e.g., if the remaining packets are all minimum-sized, or new packets are big,
and do not arrive at maximum rate). Moreover, it is possible to use threads more
eﬃciently, e.g., by not partitioning the traﬃc, but letting each thread process the
‘next available’ packet. We have chosen not to do so, because these methods re-
quire per-packet administration for such things as checking whether (a) a packet
is valid, (b) a packet is processed by a thread, and (c) a buﬀer slot is no longer
in use and may be overwritten. Each of these checks incurs additional overhead.
Instead, CardGuard needs a single check on eight counters at wrap time.
Packet processing. Each of the two MEh/t-MEac microengine pairs is respon-
sible for processing half of the packets. MEh/t is responsible for sanitising the
TCP stream, while MEac handles pattern matching.
For TCP ﬂow identiﬁcation, we use a hash table. The hash table contains
a unique entry for each TCP ﬂow, which is generated by employing the IXP’s
hardware assist to calculate a hash over the segment’s source and destination
addresses and the TCP ports. A new entry is made whenever a TCP SYN
packet is received. The number of ﬂows that may hash to the same hash value is
threshold (T)
new
being
processed unprocessed
State_0:
c = get_next_char (packet);
if (c == ’h’) goto State_1;
else if (c == ’Q’) goto State_36;
else if (c == ’t’) goto State_43;
else goto State_0;
four threads
MErx
MEh/t
MEh/t
MEac
MEac
Legend:
ME = microengine
rx = receive
ac = aho-corasick
h/t= hash/tcp
     reconstruct
State_1:
c = get_next_char (packet);
if (c == ’.’) goto State_2;
else if ...
...
(a) Processing by MErx, MEh/t, and MEac
(b) Inline processing
Fig. 3. Packet processing
114
H. Bos and K. Huang
a conﬁgurable parameter HashDim. If a new ﬂow hashes to an index in the table
which already contains HashDim ﬂows, the new ﬂow is conservatively dropped.
As a result, every live TCP ﬂow has a hash table entry, which records the
following information about the ﬂow: source IP address, destination IP address,
source port, destination port, next sequence number, and current DFA state. As
logically contiguous segments might be dispatched to diﬀerent packet processing
threads, the next sequence number ensures that segments are pattern-matched
in order, while keeping track of the current DFA state facilitates the resumption
of pattern matching of a subsequent packet at a later stage (e.g., by another
pkt-processing thread). As explained in the ﬁfth observation of Section 2.2, we
only need the current DFA state to resume scanning exactly where we left oﬀ.
When a non-SYN packet is received, the corresponding hash entry is found
and the sequence number of the packet is compared to the sequence number in
the table. As explained earlier, we do not permit segments to overwrite segments
that were received earlier. Any packet that is not the immediate successor to
the stored sequence number is put ‘on-hold’. There are two possible schemes
for dealing with such segments with which we have experimented. The ﬁrst,
and simplest, is to wait until all missing segments have arrived and only then
perform pattern matching. The second is to scan the segment for worms as an
individual packet and if it is considered safe, forward it to its destination, while
also keeping a copy in memory. Then, when the missing segments arrive, (part
of) the segment is scanned again for all signatures that may have started in the
segments preceding it and overlap with this segment. This is safe, even if the
segments that were forwarded were part of a worm attack. The reason is that
these packets by themselves do not constitute an attack. Only the addition of the
preceding packets would render the ‘worm’ complete. However, as the attack is
detected when the preceding packets arrive, these segments are never forwarded.
In the current implementation, a hash table entry is removed only as a result of
an explicit tear-down. The assumption that motivates this choice is that the FIN
and RST messages coming from the downstream host are never lost. However,
in the future we expect to incorporate a time-out mechanism that frees up the
hash-table entry while dropping the connection.
Also note that when CardGuard is started, all ﬂows that are currently active
are by necessity dropped, as they will not have hash entries in the new conﬁgu-
ration. Recently we have started implementing a mechanism that preserves the
original hash table.
Pattern matching. For pattern matching purposes, a thread on each MEac
reads data from SDRAM in 8-byte chunks and feeds it, one byte at a time, to
the Aho-Corasick algorithm. However, as the memory latency to SDRAM is in
the range of 33 to 40 cycles, such a naive implementation would be prohibitively
slow [21]. Therefore, in order to hide latency, CardGuard employs four threads.
Whenever a thread stalls on a memory access, a zero-cycle context switch is made
to allow the next processing thread to resume. As there are now eight packet
processing threads in CardGuard, the buﬀer is partitioned such that thread t is
responsible for slots t, t + 8, t + 16, . . .
Towards Software-Based Signature Detection for Intrusion Prevention
115
4.3 The Memory Hierarchy
We have explained in Section 3 that the IXP1200 has various types of memories
with diﬀerent speeds and sizes: registers, instruction store, scratch, SRAM and
SDRAM. Optimising the use of these memories proved to be key to CardGuard’s
performance. For instance, as CardGuard needs to access the DFA for every byte
in every packet, we would like the DFA to be stored in fast memory. However,
there are relatively few general purpose registers (GPRs) and scratch is both
small and relatively slow. Moreover, these resources are used by the compiler for
local variables as well.
For this reason, we make the following design decisions: (1) GPRs and scratch
are not used for storing the DFA, (2) instead, we exploit unused space in the
instruction store for storing a small part of the DFA, (3) another, fairly large,
part is stored in the 8 MB of SRAM, and (4) the remainder of the DFA is stored
in the 256 MB of slow SDRAM.
The idea is that, analogous to caching, a select number of frequently accessed
states are stored in fast memory, while the remainder is stored in increasingly
slow memory3. A premise for this to work, is that the Aho-Corasick algorithm
exhibits strong locality of reference. Whether this is the case depends both on
the patterns and on the traﬃc. Deﬁning level n in the DFA as all states that
are n transitions away from state 0, we assume for now that the top few levels
in the DFA, (e.g., states 0, 1, 36 and 43 in Figure 1) are accessed much more
frequently than the lower levels. In Section 5, we present empirical evidence to
support this.
Using the instruction store and ‘normal memories’ for storing the DFA, leads
to two distinct implementations of the Aho-Corasick algorithm itself, which we
refer to as ‘inline’ and ‘in-memory’. In an inline implementation, a DFA like the
one sketched in Figure 1 is implemented in the instruction store of a MEac, e.g.,
as a set of comparisons and jumps as illustrated in pseudo-code in Figure 3(b).
In-memory implementations, on the other hand, keep the DFA itself separate
from the code by storing it in one of the memories. The data structure that is
commonly used to store DFAs in Aho-Corasick is a ‘trie’ with pointers from a
source state to destination states to represent the transitions. In this case, a state
transition is expensive since memory needs to be accessed to ﬁnd the next state.
The overhead consists not only of the ‘normal’ memory latency, as additional
overhead may be incurred if these memory accesses lead to congestion on the
memory bus. This will slow down all memory accesses.
Note that each state in the inline implementation consists of several instruc-
tions and hence costs several cycles. We are able to optimise the number of
conditional statements a little by means of using the equivalent of ‘binary search’
to ﬁnd the appropriate action, but we still spend at least a few tens of cycles
at each state (depending on the exact conﬁguration of the DFA and the traf-
ﬁc). However, this is still far better than the implementation that uses SRAM,
as this requires several slow reads (across a congested bus), in addition to the
instructions that are needed to execute the state transitions.
3 It is not a real cache, as there is no replacement.
116
H. Bos and K. Huang
In spite of the obvious advantages, the inline version can only be used for a
small portion of the DFA, because of the limited instruction store of the micro-
engines. CardGuard is designed to deal with possibly thousands of signatures,
and the instruction store is just 1K instructions in size, so locality of reference
is crucial. In practice, we are able to store a few tens of states in the unused
instruction store, depending on the number of outgoing links. In many cases,
this is suﬃcient to store the most commonly visited nodes. For instance, we
are able to store in their entirety levels 0 and 1 of the 2025 states of snort’s
web IIS rules. In our experiments these levels oﬀer hit rates of the order of
99.9%. In Section 5, we will analyse the locality of reference in Aho-Corasick in