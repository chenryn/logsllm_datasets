[...]
This shows congestion encountered (CE) events from 100.65.76.247. CE can be set by switches and
routers in the network to notify endpoints of congestion. It can also be set by kernels based on a
qdlisc policy, although that is usually for testing and simulation purposes (with the netem qdlisc).
The DataCenter TCP (DCTCP) congestion control algorithm also makes use of ECN [Alizadeh 10]
[113].
ipecn(8) works by tracing the kernel ip_rcv0 function and reading the congestion encountered
state from the IP header. Since this adds overhead to every received packet, this method is not
ideal, and I’d rather call this a proof of concept. Much better would be to trace the kernel func
tions that handlle CE events only, as these would fire less frequently. However, they are inlined
and unavailable to trace directly (on my kernels). Best of all would be to have a tracepoint for ECN
congestion encountered events.
The source to ipecn(8) is:
#1/usx/local/bin/bpEtrace
#include 
include 
BEGIN
printf (*Tracing inbound IPv4 ECN Congestion Encountered. *)
printf (*it Ctrl-C to end. ,n*) 
kprobe1ip_rcv
$skb = (struct sk_buff *)arg0;
// get IPv4 header, see skb_netvork_header [) :
$iph = (struct iphdr *) ($skb>head + $skb>netxork_header) 
/ / see INET_ECN_HASK:
i.f (($iph=>tos 6 3) == 3)↑
tine (*5H:5X:5S *)
printf (*eCN CE fron: s^n*, ntop ($iph=>saddr)) =
---
## Page 500
10.3 BPF Tools
463
This is also an example of parsing the IPv4 header from a struct sk_buff. It uses similar logic to
the kernel’s skb_network_header() function, and will need updates to match any changes to that
function (another reason that more-stable tracepoints would be prefered). This tool can also be
extended to trace the outbound path, and IPv6 (see Section 10.5).
10.3.23 superping
superping(8)* measures the ICMP echo request to response latency from the kernel network
stack, as a way to verify the round trip times reported by ping(8). Older versions of ping(8)
uo uae Sunpaos d apnou ueo upum *aoeds sasn wog au d puno au aunseau
busy systems, inflating the measured times. This older method is also used by ping(8) for kernels
without socket timestamp support (SIOCGSTAMP or SO_TIMESTAMP).
Since I have a newer version of ping(8) and newer kernel, to demonstrate the older behavior I've
run it with the U option, which measures the original user-to-user latency. For example, in one
terminal session:
terminsl1+ ping -0 10.0.0.1
PIBG 10.0.0.1 (10.0.0.1) 56 (84) bytes of data.
64 bytes from 10.0.0.1: iomp_seg=1l tt1=64 tine=6, 44 ms
54 bytes Czon 10.0.0.1: 1omsp_seq=2 ct1=64 tne=6.60 ns
64 bytes fron 10.0.0.1: icmp_seg=3 t1=64 tine=5.93 ns
54 bytes Czon 10.0.0.1: 1omsp_seq=4 ct1=64 tne=7,40 ns
64 bytes fcon 10.0.0.1: iomp_seg=5 tt1=64 tine=5.87 ms
[.--]
While in another terminal session I had already run superping(8):
terminal2# superping .bt
Attach.ing 6 probes...
Tzacing IOXP echo request latency. HIt Ctrl-C to end.
IPv4 ping, I0 28121 seq 1: 6392 us
IPv4 ping, ID 28121 seq 2: 6474 us
IPv4 ping, I0 28121 seq 3: 5811 us
IPv4 ping, ID 28121 seq 4: T270 us
IPv4 ping, I0 28121 seg 5: 5741 us
[--. ]
The output can be compared: it shows that the times reported by ping(8) can be inflated by over
0.10 ms, for this current system and workload. Without U, so that ping(8) uses socket time-
stamps, the time difference is often within 0.01 ms.
This works by instrumenting the send and receive of ICMP packets, saving a timestamp in a
BPF map for each ICMP echo request, and compares the ICMP header details to match the echo
40 0rign: I first crested this for the 2011 OTrsce book [Greg 11] and wrote this version for this book on 20Apr2019
---
## Page 501
464
 Chapter 10 Networking
packets. The overhead should be negligible, since this is only instrumenting raw IP packets and
not TCP packets.
The source to superping(8) is:
#1/usx/local/bin/bpEtrace
#include 
include 
#1nclude 
include 
#include 
BEGIY
printf(*Txacing IOXP ping latency. Bit Ctrl-C to end.\n") 
/ *
* IPv4
* /
kprobe :Ip_send_skb
$skb = (struct sk_buff *)argl
// get IPv4 header; see skb_netvork_header () :
[xepeeuaoxgsuprotoco] = IPPROTO_ICMP)
// get ICNP headex; see skb_transport_header () :
$icmph = (struct icmphdr *) ($skb->head +
$skb->transpoxt_header) :
if ($icmph=>type = ICMP_sCBo)
$id = $icnph=>un-echo 1d,
$seq = $icmph->un,echo.sequence,
gstart[$id, $seq] = nsecs
kprobe1icmp_rcv
$skb = (struet sk_buff *)arg0;
// get ICHP header; see skb_transport_headez () :
 [xapeaqxodsue3type == ICMP_ECHOREPLY)
$1d = $1cnph=>un,echo.1d;
$seq = $icmph->un,echo,sequence
$atart = patart [$id, $seq] :
if ($start > 0) ↑
(00xx  (8 >> ps)) 1 [B > ba2s) 1 1(ghead + $skb=>netxork_headex) 
if ($ip6h>nexthdr == IPPROTO_ICMPV6)(
// get ICNP headex; see skb_txansport_header ():
$icmpfh = (struct icnp6hdr *) ($skb->bead +
(xepeeuxodsuexicmp6_type == IcMFv6_EcBo_REguEsT) [
$id = $icnp6h=>1cnp6_dataun, u_echo, Ident1fier;
seg = $icmp6h->icnp6_dstaun-u_echo sequencer
estart[$id, seq] = nsecs
kprobe:icmpv6_rcv
$skb = (struct sk_buff *)arg0;
// get ICMPv6 beader; see skb_transport_header () :
$icnp6h = (struct icmp6hdr *) I$skb->head + $skb->transport_header) =
1f ($1cnp6h=>1omp6_type == ICHPV6_ECHO_REPLY) ↑
$id = $icmpfh=>icnp6_dataun-u_echo.identifier
$seq = $1osp6h=>1cnp6_dataun, u_echo,sequence,
$start = estart [$id, $seg]:
---
## Page 503
466
3Chapter 10 Networking
if ($start > 0) ↑
$1dhost = ($1d >> 8] 1 [($id > bazs)11(g< bazg) -qsoqbas$
printf (*IPv6 ping, ID id seq Id: Id us’,na,
$idhost, $seghost, (nsecs - $start] / 100):
delete (estazt[$1d, $seq]) :
END Iclear (@start) = )
Both IPv4 and IPv6 are handled by different kernel functions, and are traced separately. This code
is another example of packet header analysis: the IPv4, IPv6, ICMP, and ICMPv6 packet headers
are read by BPE The method of finding these header structures from the struct sk_buff depends
on the kernel source and its functions skb_network_header() and skb_transport_header(). As with
kprobes, this is an unstable interface, and changes to how headers are found and processed by the
network stack will require upxdates to this tool to match.
A minor note for this source: the ICMP identifier and sequence number are printed out after switch-
ing from network to host order (see $idhost = and $seqhost =). For the @start map that saves
timestamps, I used the network order instead; this saved some instructions on the send kprobes.
10.3.24qdisc-fq
qdlisc-fq(8)a shows the time spent on the Fair Queue (FQ) qdisc. For example, from a busy produc-
tion edge server:
+ qdisc-fq.bt
Attaching 4 pzobes...
Tracing qdisc fq latency. Hit Ctrl-C to end.
°C
Pus:
[01
6803 1869889889888
[1]
20084 leee88e88e88e88ee8ee8ee8ee8ee88e88ee
[2, 4]
29230 1869889889886 886889869869869889889886 88688986986986 1
[4, B] 
75518
[6, 16}
2101
[16, 32)
86 1
391
[32, 64)
[64, 128]
901
1
[128, 256}
651
[256, 512}
51 1
41 Origin: I created it for tis book on 21-Apr-2019.
---
## Page 504
10.3 BPF Tools
467
[512, 1K]
261
[1K,2K)
91
[2K,4K)
2 1
This shows that packets usually spent less than four microseconds on this queue, with a very
small percentage reaching up to the two to four-millisecond bucket. Should there be a problem
with queue latency, it will show up as higher latencies in the histogram.
This works by tracing the enqueue and dequeue functions for this qdlisc. For high network I/O
systems, the overhead may become measurable as these can be frequent events.
The source to qdisc-fq(8) is:
#1/usx/1ocal/bin/bpCtrace
BEGIN
printf(*Txacing qdisc fq latency- H1t Cte1=C to end.\n*)
kprobe:fα_enqueve
Bstart[arg0] = nsecs7
kzetprobe:fq_dequeve
/Bstart[retval]/
Bus = hist( (nsecs - 9start [cetva1)) / 1000) 
delete (@start[retval]) :
END
1
clear (8start) 
The argument to fq_enqueuel), and the return value of fq_dequeue(), is the struct sk_buff adress,
which is used as a unique key for storing the timestamp.
Note that this tool only works when the FQ qdlisc scheduler is loaded. If it is not, this tool will error:
 qdisc-fq.bt
Attach.ing 4 probes..
cannot attach kprobe, Invalld argunent
Ecroe attaching probe: *kretprobe: fq_dequeue*
---
## Page 505
468
Chapter 10 Networking
This can be fixed by forcibly loading the FQ scheduler kernel module:
 nodprobe seh_fq
+ qdise-fq.bt
Attach.ing 4 pzobes.
ehid
°C
+
Although, if this qdlisc is not in use, then there will be no queueing events to measure. Use tc(1) to
add and administer qdisc schedulers.
10.3.25
qdisc-cbq, qdisc-cbs, qdisc-codel, qdisc-fq_codel, qdisc-red,
and qdisc-tbf
There are many other qdisc schedulers, and the previous qdisc-fq(8) tool can usually be adapted
to trace each. For example, here is a Class Based Queueing (CBQ) version:
 qdise-cbq.bt
Attaching 4 probes. a
C
9us1
[0]
881251
[1]
766 186988988988688
[2, 4]
2033 188e88e88e88e888e88e88e8ee8ee88e88e88e88
[4, 8]
2279 1889889889886 8869889889869869889889886 8869869
[8, 16}
2663 1eeeeeeeeee e８eeeeeeeeeee８eeeeeeeee e８eeeeeeeeeeeee1
[16, 32]
427 188988988
[32, 64)
151
[64, 128]
11
The enqueue and dequeue functions that are traced are from struct Qdisc_ops, which defines
their arguments and return value (include/net/sch_generic.h):
struct Qdisc_ops [
do"oe1p0 4on22
*next
const struct Qdise_class_sps
*c1_ops1
char
id[IFHAMSI2|
1nt
priv_size;
unsigned int
static_flags7
---
## Page 506
10.3 BPF Tools
469
i.nt
(*enqueue) (atruct sk_buff *skb,
stxuct Qdlsc *sch,
struct sk_buff **to_free) 
struct sk_buff *
 (o5Tp0 omx3s) (enenbep+)
[...]
This is why the skb_buff address was the first argument for the enqueue function, and the return
value of the dequeue function.
This Qdisc_ops is declared for other schedulers. For the CBQ qdisc (net/sched/sch_cbq.c):
static struct Qdisc_ops cbq_qdisc_ops
_read_nostly = ↑
.next
=
NULL,
, c1_ops
gdoseeobqps
.1d
"cbq°。
•priv_size
sizeof (struct cbq_sched_dat.a) ,
,enqueue
cbq_enquege,
 dequeue
cbq_dequeve,
[ . - - ]
A qdlisc-cbq.bt tool can thus be written by changing qdlisc-fq(8)'s fq_enqueue to cbq_enqueue,
and fq_dequeue to cbq_dequeue. is Here is a table of substitutions for some of the qdliscs:
BPF Tool
Qdisc
Enqueue Function
Dequeue Function
qdlisc-cbq.bt
Class Based Queueing
cbq_enqueue()
cbq_dequeue()
qdlisc-cbs.bt
Credit Based Shaper
()anenbua"sqo
cbs_dequeue()
qdisc-codel.bt
Controlled-Delay Active
(enanbua"osipb|apoo
()ananbap“ospblepoo
Queue Management
qdisc-fq_codel.bt
Fair Queueing with
fq_codel_enqueue()
Controlled Delay
fq_codel_dequeue()
qdisc-red
Random Early Detection
red_enqueue(
()ananbappa
qdisotbf
Token Bucket Filter
tbf_enqueue()
tbl_dequeue()
that accepted a qdisc name as an argument and then built and ran the bpftrace program to show
It would be a straightforward exercise to create a shell script wrapper to bpftrace, called qdlisclat,
its latency.
---
## Page 507
0
Chapter 10 Networking
10.3.26
netsize
netsize(8)°2 shows the size of received and sent packets from the net device layer, both before and
after software segmentation offload (GSO and GRO). This output can be used to investigate how
packets become segmented before sending. For example, from a busy proxduction server:
+ netsize.bt
Attaching 5 probes.
Tracing net device send/receive. Hit Ctrl-C to end.
°C
@n1c_recv_bytes:
[32, 64)
16291 1869889889886 8889889889869869889889886 8889889889869861
[64, 128]
668 188
[128, 256}
191
[256, 512}
181
[512, 1K)
241
[1K, 2K)
1571
enic_send_bytes:
[32, 64)
[64, 128]
3561
(95z *8z1]
139|
[256, 512}
311
[512, 1K]
151
[1K,2K)
45850 1869889889888 8889889889889889889889886 886986986986986 1
?recv_bytes:
[32, 64)
16417 188e88e88e88 88 e88e88e8 e8ee88e88e88 88ee8ee8ee8ee881
[64, 128]
688 188
[128, 256}
201
[256, 512}
33
[512, 1K)
351
[1K, 2K)
1451