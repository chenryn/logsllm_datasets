mum allocation per application, smaller values of Dnew max
lead to smaller capacity requirements on the resource pool.
Thus, we have shown that if there are time-limiting constraints
on degraded performance then higher values of θ may result
in the smaller maximum allocation per application.
Figure 3 shows impact of θ on breakpoint p (where p
deﬁnes a fraction of demand that is assigned to CoS1) and
Fig. 4. Simulation Algorithm
maximum allocation per application. We used (Ulow, Uhigh) =
(0.5, 0.66) as an acceptable performance range.
The plot shows the trend for Dnew max in a normalized
way: the ratio of any two points on the line approximates the
ratio in Dnew max per application for different values of θ.
For example, it shows that for θ = 0.95 the maximum demand
Dnew max is 20% lower than for θ = 0.6, i.e., the maximum
allocation for θ = 0.95 is 20% less than for θ = 0.6.
VI. WORKLOAD PLACEMENT SERVICE
The workload placement service has two components. A
simulator component emulates the assignment of several appli-
cations to a single resource. It traverses the traces of allocation
requirements to estimate a required capacity that satisﬁes the
resource access QoS commitments. The required capacity can
be compared with resource capacity limits. An optimizing
search algorithm examines many alternative assignments and
reports the best solution found for the consolidation exercise.
These components are described in the following sections.
A. Simulator and resource access CoS commitments
Figure 4 illustrates the simulator algorithm. The simulator
considers the assignment of a set of workloads to a single re-
source. It replays the workload allocation traces, compares the
aggregate allocations of the observations in the traces with the
capacity of the resource, and computes resource access CoS
statistics. If the computed values satisfy the CoS commitments
then the workloads are said to ﬁt on the resource. A search
method, e.g., a binary search, is used to ﬁnd the required
capacity, i.e., smallest value, for each capacity attribute such
that the CoS resource pool commitments are satisﬁed.
When two CoS are involved, the simulation component
schedules access to capacity in the following way. Capacity
is assigned to CoS1 ﬁrst. The remaining capacity is then
assigned to CoS2. This corresponds to the behaviour of the
workload managers as described earlier in the paper.
The required capacity of each attribute is found as follows.
First a check is made to ensure the sum of the peak application
demands associated with CoS1 do not exceed the capacity of
the resource. If they do then the workloads do not ﬁt, otherwise
they may ﬁt. If the workloads may ﬁt, then the following
process is initiated. If the current capacity satisﬁes the CoS
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:28:03 UTC from IEEE Xplore.  Restrictions apply. 
we deﬁne f(U) as: f(U) = (U Z)2 = U 2×Z. The square term
in the power exaggerates the advantages of higher utilizations
(in a least squares sense), the Z term demands that servers
with greater numbers of CPUs be higher utilized. The Z term
is motivated by the function
1−U Z that estimates the mean
response time of clients with unit demand in an open queueing
network having a single resource with Z CPUs.
1
The genetic algorithm has mutation and cross-over func-
tions. The mutation function associates a mutation probability
with each server that is used according to its value for f(U).
The lower the value of f(U) for a resource the greater the
likelihood that the resource’s application workloads will be
migrated to other resources. With each mutation step, the
algorithm tends to reduce the number of resources being used
by one. The cross-over function mates earlier assignments in a
straightforward manner. It simply takes some random number
of application assignments from one assignment and the rest
from the other to create a new assignment.
C. Planning for Failures
The workload placement service can also be used to report
on the impact of single and/or multiple failures. Basically, the
conﬁguration of the consolidated system is taken as the initial
conﬁguration. This conﬁguration is for a small number of
servers as needed to support the applications with their normal
mode QoS requirements. For failure modes (e.g., one server
at a time),
the workload placement service systematically
removes one server at a time from the pool, associate its
affected applications with their failure mode application QoS
requirements, and repeats the consolidation algorithm. The
consolidation algorithm reports whether it is possible to place
all the affected applications on the remaining servers in the
pool with their failure QoS requirements. If this is possible
for all failures under study then the service reports that failure
modes can be supported without an additional spare server.
More detailed information about which applications can be
supported in this way and for which failures can be combined
with expectations regarding time to repair for servers, the
frequency of failures, and penalties to decide on whether it
is cost effective to have a spare server or not. However, in our
case study in this paper we simply show that the use of an
alternative set of application QoS constraints can result in the
requirement for one less server.
VII. CASE STUDY
In this section, we present a case study to demonstrate the
features of R-Opus for a large enterprise order entry system
with 26 applications. The study presents a characterization
of the application workloads, results regarding the portfolio
approach, and workload placement results. The case study
relies on four weeks of workload CPU demand traces with
measurement values recorded every 5 minutes.
Figure 6 shows the percentiles of CPU demand for the 26
applications. The demands are normalized as a percentage with
respect to their peak values. The 100-percentile of demand
corresponds to 100% normalized CPU demand. Several curves
are shown that illustrate the 99.9 through 97 percentile of
demand. The ﬁgure shows that 2 applications, i.e., the leftmost
in the ﬁgure, have a small percentage of points that are
very large with respect to their remaining demands. The left-
most 10 applications have their top 3% of demand values
Fig. 5. Optimizing Search
commitments then the algorithm reduces the capacity value
for the attribute. If the current capacity does not satisfy the
commitments, the algorithm increases the value for capacity
up to the limit L of the attribute. The algorithm completes
when it ﬁnds that the commitments cannot be satisﬁed or when
the value for capacity changes by less than some tolerance.
Currently we use a binary search, but other search mechanisms
could also be used. Upon termination, the algorithm reports
whether the commitments are satisﬁed (for each attribute). If
so, the resulting value for capacity is reported as the required
capacity (for each capacity attribute).
B. Optimizing search
Figure 5 illustrates the optimizing search algorithm for
the consolidation exercise. The resource access QoS commit-
ments, workload allocation traces, and an initial assignment
of workloads to resources are the inputs of the algorithm.
The behavior of each resource is simulated using the method
described in Section VI-A. The results of the simulations are
then used to compute a score for the consolidation objective
function. If there is little improvement in the score then
the algorithm reports a conﬁguration that achieved the best
score while satisfying resource access QoS commitments and
terminates. Otherwise a new conﬁguration is enumerated and
the simulation process is repeated. A genetic algorithm is used
to guide the search.
The consolidation exercise begins with the initial conﬁgu-
ration of the system and causes a search for a workload as-
signment that satisﬁes commitments and uses a small number
of servers. A score is computed for each assignment of the
workloads to resources. The score is a sum of values computed
for each resource. To simplify the presentation, we assume
that each CPU in the pool has the same processing capacity
but that resources may have different numbers of CPUs. The
values that contribute to the score are:
• 1: for a resource in the pool that isn’t used;
• f(U): a function of utilization for a resource with re-
quired capacity R less than or equal to the capacity of
the resource L, where U = R
• −N : for resources that are over-booked, i.e., R > L,
where N is the number of application workloads assigned
to the resource.
L and 0 < U ≤ 1; and,
The function f(U) provides a greater value for higher uti-
lizations than lower utilizations. However, the function scales
utilization with respect to the number of CPU resources to
reﬂect that resources with more CPUs can operate at higher
utilization levels. Let Z be the number of CPUs per server,
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:28:03 UTC from IEEE Xplore.  Restrictions apply. 
)
%
(
d
n
a
m
e
D
U
P
C
d
e
z
i
l
a
m
r
o
N
 100
 90
 80
 70
 60
 50
 40
 30
 20
 10
 0
99.9-th percentile
99.5-th percentile
99-th percentile
98-th percentile
97-th percentile
 0
 5
 10
 15
 20
 25
 30
App’s Number
Fig. 6. Top Percentile of CPU Demand for Applications under Study.
a)
between 10 and 2 times higher than the remaining demands
in the trace. It shows the bursty nature of demands for most
of the applications under study.
In our case study, we consider the following application
QoS requirements:
• acceptable application performance: U low = 0.5,
U high = 0.66, with utilization of allocation in the range
(0.5, 0.66) for 97% of measurements;
• degraded application performance: for the remaining
measurements M degr = 3% the utilization of allocation
should not exceed U degr = 0.9. We consider four values
for T degr: none, 2 hours, 1 hour, and 30 min,
i.e.,
from no additional time-limiting constraints on degraded
performance to the case when degraded performance
should not persist longer than 30 min.
The workloads are partitioned across the two CoS to satisfy
these application QoS requirements.
Figures 7 a) and b) show the impact of M degr and T degr on
maximum allocations for 26 applications under study and two
different values for resource access probability: θ = 0.95 and
θ = 0.6. The y-axis shows the percent reduction for the maxi-
mum allocation with M degr=3% as compared to M degr = 0%.
For both values of θ, many of the 26 applications have a 26.7%
in reduction for maximum allocation that corresponds to an
expected upper bound on M axCapReduction (as described
by formula 5 in Section V). Overall M axCapReduction is
affected more by T degr for θ = 0.6 than for the higher value of
θ = 0.95. Again, this is consistent with our general derivations
in Section V, where we observe that under time-limiting
constraints, higher values of θ lead to a smaller maximum
allocation requirements.
Figures 8 a) and b) show the percentage of measurements
that have degraded performance, i.e., with utilization of alloca-
tion in the range (U high, U degr). While up to 3% of measure-
ments were allowed to be in the degraded range, the additional
time-limiting constraint T degr=30 min signiﬁcantly reduces
the number of measurements with degraded performance: it is
less than 0.5% for θ = 0.95 and less than 1.5% for θ = 0.6
as shown in Figures 8 a) and b), respectively.
To summarize, for these workloads, a small but controlled
relaxation for application QoS requirements can lead to up
to an approximately 25% reduction in maximum allocation
requirements.
We now consider the use of the workload placement service.
 30
 25
 20
 15
 10
 5
 0
Reduction in Max CPU Allocation (CoS2, θ = 0.95)
No contiguous time-limit  on  Udegr
no more than 2 hours of  Udegr
no more than 1 hour of  Udegr
no more than 0.5 hour of Udegr
 5
 10
 15
 20
 25
 30
App’s Number
Reduction in Max CPU Allocation (CoS2, θ = 0.6)
 30
 25
 20
 15
 10
 5
 0
No contiguous time-limit  on  Udegr
no more than 2 hours of  Udegr
no more than 1 hour of  Udegr
no more than 0.5 hour of Udegr
 0
 5
 10
 15
 20
 25
 30
)
%
(
n
o
i
t
c
u
d
e
R
p
a
C
x
a
M
)
%
(
n
o
i
t
c
u
d
e
R
p
a
C
x
a
M
b)
App’s Number
Fig. 7. MaxCapReduction per Application under Different Time-
Contiguous Requirement on Degraded Performance.
Table I shows the impact of M degr, T degr and θ 3 on the
CPU capacity needed to satisfy the 26 application workloads.
The table shows the number of 16-way servers reported as
being needed by the workload placement service, the sum
of per server required capacity Crequ, and the sum of per-
application peak CPU allocations Cpeak. All cases had the
same workload placement algorithm termination criteria and
used approximately 10 minutes of CPU time on a 3.4 Ghz
Pentium CPU. The required capacity values are between 37%
to 45% lower than the sum of per-application peak allocations.
This shows that resource sharing presents signiﬁcant opportu-
nity for reducing capacity requirements for these workloads.
Furthermore, for cases 1-3 some demands are in both CoS1
and CoS2; for cases 4-6 all demands are in CoS2. If all
demands were associated with CoS1 then, because we would
have to limit the sum of per-application peak allocations to the
capacity of the resource, we would require at least 15 servers
for case 1 and 11 servers for case 3. Thus having multiple
classes of service is advantageous.