Given the prevalence of spam throughout Twitter, we examine
the degree to which blacklists could stem the spread of unsolicited
messages. Currently, Twitter relies on Google’s SafeBrowsing API
to block malicious links, but this ﬁltering only suppresses links
that are blacklisted at the time of its posting; Twitter does not
retroactively blacklist links, allowing previously undetected mali-
cious URLs to persist. To measure how many tweets slip through
Twitter’s defenses, and whether the same would be true for URIBL
and Joewein, we examine a number of blacklist characteristics, in-
cluding delay, susceptibility to evasion, and limitations that result
if we restrict ﬁltering to considering only domains rather than the
full paths of spam websites.
6.1 Blacklist delay
Using historical data for the URIBL, Joewein, and Google black-
lists, we can measure the delay between a tweet’s posting and the
time of its subsequent blacklisting. For cases where a spam URL
embedded in a tweet appeared on a blacklist prior to appearing on
Link Statistics
Flagged before posting
Flagged after posting
Avg. lead period (days)
Avg. lag period (days)
Overall avg. (days)
URIBL
27.17%
3.39%
72.83% 96.61%
13.41
-4.28
-3.67
Google
Joewein Malware
7.56%
92.44%
29.58
-24.90
-20.77
29.40
-21.93
-12.70
Google
Phishing
1.71%
98.29%
2.57
-9.01
-8.82
Table 4: Blacklist performance, measured by the number of tweets
posted that lead or lag detection. Positive numbers indicate lead,
negative numbers indicate lag.
Link Statistics
Flagged before posting
Flagged after posting
Avg. lead period (days)
Avg. lag period (days)
Overall avg. (days)
Total domains ﬂagged
URIBL
50.19% 20.31%
49.81% 79.69%
15.51
-5.41
-1.16
128
Google
Joewein Malware
18.89%
81.11%
28.85
-21.63
-12.10
625
50.53
-32.10
9.36
1620
Google
Phishing
15.38%
84.62%
2.50
-10.48
-8.49
13
Table 5: Blacklist performance, measured by lead and lag times for
unique domains posted.
Twitter, we say that the blacklist leads Twitter. Conversely, a black-
list lags Twitter if posted URLs reach the public before becoming
blacklisted. Lead and lag times play an important role in determin-
ing the efﬁciency of blacklists. For example, for long lag periods
spam ﬁlters must maintain a large index of URLs in stale tweets to
retroactively locate spam. Furthermore, depending on the rate at
which users click on spam links, long lag periods can result in little
protection unless spammers reuse links even after they appear on
blacklists.
We begin measuring blacklist delay by gathering the timestamps
for each tweet of a blacklisted URL. For URLs spammed in mul-
tiple tweets, we consider each posting as a unique, independent
event. Table 4 shows the lead and lag times for tweets, where
we see that the majority of spam tweets appear on Twitter mul-
tiple days prior to being ﬂagged in blacklists, and in the case of
URIBL and Google, multiple weeks. A more extensive presenta-
tion of blacklist delay can be seen in Figure 6, showing the volume
of tweets per lead and lag day. It is important to note that Twit-
ter use of Google’s Safebrowsing API to ﬁlter links prior to their
posting biases our analysis towards those links that pass through
the ﬁlter, effectively masking the lead time apart from URLs that
spammers obfuscated with shorteners to avoid blacklisting.
Table 5 shows the same lead and lag periods but weighted by
unique domains rather than by individual tweets. While blacklist-
ing timeliness improves from this perspective, this also indicates
that domains previously identiﬁed as spam are less likely to be re-
posted, limiting the effectiveness of blacklisting.
To understand the exposure of users due to blacklist lag, we mea-
sured the rate that clicks arrived for spam links. Using daily click-
through data for a random sample of 20,000 spam links shortened
with bitly, we found that 80% of clicks occur within the ﬁrst day
of a spam URL appearing on Twitter, and 90% of clicks within the
ﬁrst two days. Thus, for blacklisting to be effective in the context
of social networks, lag time must be effectively zero in order to
prevent numerous users from clicking on harmful links.
6.2 Evading blacklists
The success of blacklists hinges on the reuse of spam domains; if
every email or tweet contained a unique domain, blacklists would
35(a) URIBL
(b) Joewein
(c) Google - Malware
(d) Google - Phishing
Figure 6: Volume of spam tweets encountered, categorized by either lagging or leading blacklist detection
Figure 7: Frequency of redirects and nested redirects amongst dis-
tinct spam URLs
Figure 8: Frequency of cross-domain redirects amongst distinct
spam URLs containing at least one hop
be completely ineffective. While the registration of new domains
carries a potentially prohibitive cost, URL shortening services such
as bitly, tinyurl, is.gd, and ow.ly provide spam orchestrators with a
convenient and free tool to obfuscate their domains.
By following shortened URLs, we found over 80% of distinct
links contained at least one redirect, as shown in a breakdown in
Figure 7. In particular, redirects pose a threat to blacklisting ser-
vices when they cross a domain boundary, causing a link to ap-
pear from a non-blacklisted site as opposed to a blacklisted land-
ing page. Figure 8 shows the cross-domain breakdown for distinct
URLs seen in links containing at least one redirect. Roughly 55%
of blacklisted URLs cross a domain boundary.
The effect of shortening on Twitter’s malware defenses (ﬁltering
via Google’s Safebrowsing API) appears quite clearly in our data
set. Disregarding blacklist delay time, 39% of distinct malware and
phishing URLs evade detection via use of shorteners. Despite the
small fraction, these links make up over 98% of malicious tweets
identiﬁed by our system. Even in the event a shortened URL be-
comes blacklisted, generating a new URL comes at effectively no
cost. Without the use of crawling to resolve shortened URLs, black-
lists become much less effective.
of these services allow users to upload content, giving rise to the
potential for abuse by spammers.
The presence of user-generated content and mashup pages
presents a unique challenge for domain blacklists. For instance,
while ow.ly merely acts as a redirector, the site embeds any spam
pages to which it redirects in an iFrame, causing a browser’s ad-
dress bar to always display ow.ly, not the spam domain. When
faced with mashup content, individual cross-domain components
that make up a page must be blacklisted rather than the domain
hosting the composite mashup. This same challenge exists for
Web 2.0 media where content contributed by users can affect
whether a domain becomes blacklisted as spam. For tumblr and
friendfeed, we identiﬁed multiple cases in our data set where the
domains were used by spammers, but the majority of accounts be-
long to legitimate users. The appearance and subsequent deletion
of social media domains within URIBL and Joewein disguises the
fact that the domains are being abused by spammers. To address
the issue of spam in social media, individual services can either be
left to tackle the sources of spam within their own sites, or new
blacklists must be developed akin to Google’s Safebrowsing API
that go beyond domains and allow for ﬁne-grained blacklisting.
6.3 Domain blacklist limitations
7. CONCLUSION
For blacklists based only on domains rather than full URLs, such
as URIBL and Joewein, false positives pose a threat of blacklisting
entire sites. Looking through the history of URIBL and Joewein,
we identiﬁed multiple mainstream domains that were blacklisted
prior to our study, including ow.ly, tumblr, and friendfeed. Each
This paper presents the ﬁrst study of spam on Twitter including
spam behavior, clickthrough, and the effectiveness of blacklists to
prevent spam propagation. Using over 400 million messages and 25
million URLs from public Twitter data, we ﬁnd that 8% of distinct
Twitter links point to spam. Of these links, 5% direct to malware
−50050012345x 104Number of TweetsDays−500500100200300400500  Days−500500200400600800Days−500500102030405060Days  LagLead02468101200.20.40.60.81Redirects per URLFraction of URLs  GoogleJoeweinURIBL0246800.20.40.60.81Domain boundries crossed during redirectionFraction of URLs  GoogleJoeweinURIBL36and phishing, while the remaining 95% target scams. Analyzing
the account behavior of spammers, we ﬁnd that only 16% of spam
accounts are clearly automated bots, while the remaining 84% ap-
pear to be compromised accounts being puppeteered by spammers.
Even with a partial view of tweets sent each day, we identify coordi-
nation between thousands of accounts posting different obfuscated
URLs that all redirect to the same spam landing page. By measur-
ing the clickthrough of these campaigns, we ﬁnd that Twitter spam
is far more successful at coercing users into clicking on spam URLs
than email, with an overall clickthrough rate of 0.13%.
Finally, by measuring the delay before blacklists mark Twitter
URLs as spam, we have shown that if blacklists were integrated
into Twitter, they would protect only a minority of users. Further-
more, the extensive use of URL shortening services masks known-
bad URLs, effectively negating any potential beneﬁt of blacklists.
We directly witness this effect on Twitter’s malware and phishing
protection, where even if URLs direct to sites known to be hostile,
URL shortening allows the link to evade Twitter’s ﬁltering. To im-
prove defenses for Twitter spam, URLs posted to the site must be
crawled to unravel potentially long chains of redirects, using the
ﬁnal landing page for blacklisting. While blacklist delay remains
an unsolved challenge, retroactive blacklisting would allow Twitter
to suspend accounts that are used to spam for long periods, forcing
spammers to obtain new accounts and new followers, a potentially
prohibitive cost.
8. REFERENCES
[1] D. Anderson, C. Fleizach, S. Savage, and G. Voelker.
Spamscatter: Characterizing internet scam hosting
infrastructure. In USENIX Security, 2007.
[2] M. Cha, H. Haddadi, F. Benevenuto, and K. Gummadi.
Measuring User Inﬂuence in Twitter: The Million Follower
Fallacy. In Proceedings of the 4th International Conference
on Weblogs and Social Media, 2010.
[3] A. Chowdhury. State of Twitter spam.
http://blog.twitter.com/2010/03/state-of-twitter-spam.html,
March 2010.
[4] F-Secure. Twitter now ﬁltering malicious URLs.
http://www.f-secure.com/weblog/archives/00001745.html,
2009.
[5] R. Flores. The real face of Koobface.
http://blog.trendmicro.com/the-real-face-of-koobface/,
August 2009.
[6] Google. Google safebrowsing API.
http://code.google.com/apis/safebrowsing/, 2010.
[7] D. Harvey. Trust and safety.
http://blog.twitter.com/2010/03/trust-and-safety.html, March
2010.
[8] D. Ionescu. Twitter Warns of New Phishing Scam.
http://www.pcworld.com/article/174660/twitter_warns
_of_new_phishing_scam.html, October 2009.
[9] D. Irani, S. Webb, and C. Pu. Study of static classiﬁcation of
social spam proﬁles in MySpace. In Proceedings of the 4th
International Conference on Weblogs and Social Media,
2010.
[10] J. John, A. Moshchuk, S. Gribble, and A. Krishnamurthy.
Studying spamming botnets using Botlab. In Usenix
Symposium on Networked Systems Design and
Implementation (NSDI), 2009.
[11] C. Kanich, C. Kreibich, K. Levchenko, B. Enright,
G. Voelker, V. Paxson, and S. Savage. Spamalytics: An
empirical analysis of spam marketing conversion. In
Proceedings of the 15th ACM Conference on Computer and
Communications Security, pages 3–14. ACM, 2008.
[12] H. Kwak, C. Lee, H. Park, and S. Moon. What is Twitter, a
social network or a news media? In Proceedings of the
International World Wide Web Conference, 2010.
[13] K. Lee, J. Caverlee, and S. Webb. Uncovering social
spammers: Social honeypots + machine learning. In
Proceeding of the SIGIR conference on Research and
Development in Information Retrieval, pages 435–442, 2010.
[14] R. McMillan. Stolen Twitter accounts can fetch $1,000.
http://www.computerworld.com/s/article/9150001/Stolen_
Twitter_accounts_can_fetch_1_000, 2010.
[15] B. Meeder, J. Tam, P. G. Kelley, and L. F. Cranor. RT
@IWantPrivacy: Widespread violation of privacy settings in
the Twitter social network. In Web 2.0 Security and Privacy,
2010.
[16] J. O’Dell. Twitter hits 2 billion tweets per month.
http://mashable.com/2010/06/08/twitter-hits-2-billion-
tweets-per-month/, June
2010.
[17] A. Pitsillidis, K. Levchenko, C. Kreibich, C. Kanich,
G. Voelker, V. Paxson, N. Weaver, and S. Savage. Botnet
Judo: Fighting spam with itself. 2010.
[18] Z. Qian, Z. Mao, Y. Xie, and F. Yu. On network-level clusters
for spam detection. In Proceedings of the Network and
Distributed System Security Symposium (NDSS), 2010.
[19] M. Sahami, S. Dumais, D. Heckerman, and E. Horvitz. A
Bayesian approach to ﬁltering junk e-mail. In Learning for
Text Categorization: Papers from the 1998 workshop.
Madison, Wisconsin: AAAI Technical Report WS-98-05,
1998.
[20] E. Schonfeld. When it comes to URL shoteners, bit.ly is now
the biggest. http://techcrunch.com/2009/05/07/when-it-
comes-to-url-shorteners-bitly-is-now-the-biggest/, May
2009.
[21] K. Thomas and D. M. Nicol. The Koobface botnet and the
rise of social malware. Technical report, University of
Illinois at Urbana-Champaign, July 2010.
https://www.ideals.illinois.edu/handle/2142/16598.
[22] Twitter. The Twitter rules.
http://help.twitter.com/forums/26257/entries/18311, 2009.
[23] URIBL. URIBL.COM – realtime URI blacklist.
http://uribl.com/, 2010.
[24] Y. Wang, M. Ma, Y. Niu, and H. Chen. Spam double-funnel:
Connecting web spammers with advertisers. In Proceedings
of the International World Wide Web Conference, pages
291–300, 2007.
[25] J. Wein. Joewein.de LLC – ﬁghting spam and scams on the
Internet. http://www.joewein.net/.
[26] C. Wisniewski. Twitter hack demonstrates the power of weak
passwords.
http://www.sophos.com/blogs/chetw/g/2010/03/07/twitter-
hack-demonstrates-power-weak-passwords/, March
2010.
[27] Y. Xie, F. Yu, K. Achan, R. Panigrahy, G. Hulten, and
I. Osipkov. Spamming botnets: Signatures and
characteristics. Proceedings of ACM SIGCOMM, 2008.
37