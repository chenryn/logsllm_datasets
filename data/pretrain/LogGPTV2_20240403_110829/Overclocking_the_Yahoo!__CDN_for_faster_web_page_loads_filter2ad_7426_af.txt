### 4.1.4 Discussion

In this section, we have examined the impact of varying Initial Congestion Window (ICW) sizes on web page load times. A significant fraction of users benefits from larger ICW sizes, while others, particularly those with long flows and high packet loss, may experience performance degradation. The degree of benefit or detriment can vary based on the user's connection quality. Therefore, to achieve optimal performance, different users should have different ICW values tailored to their specific network conditions. We argue that a dynamic mechanism for configuring the ICW per connection is necessary.

### 4.2 Appropriate Byte Counting (ABC)

As previously discussed, the widespread use of delayed acknowledgments results in the congestion window growing by only a factor of 1.5 per round trip. ABC mitigates this effect by increasing the window size based on the number of bytes acknowledged rather than the arrival of ACKs. This allows the congestion window to double during slow-start every round trip, as originally intended in TCP design. Linux supports three ABC settings: 0 (disables ABC), 1 (increments the window by one for each full segment acknowledged), and 2 (increments the window by two for each acknowledgment received).

#### 4.2.1 Evaluation Using Live Traffic

We varied the ABC settings at the server and observed their effects, as shown in Figure 21. Our results indicate that enabling ABC has a positive, albeit limited, effect on object transfer times.

#### 4.2.2 Studying Page Load Time

To study the impact of TCP ABC on overall web page load time, we used the same setup as in Section 4.1.2, with an RTT of 100ms and zero packet loss. We measured the page load time with and without the ABC optimization. As shown in Figure 22, enabling ABC has marginal effects on the overall web page load time. This is because the benefits of ABC are only noticeable after many round trips, and each TCP connection typically downloads a small number of objects, requiring only a few packets and round trips.

### 4.3 HTTP Pipelining

HTTP pipelining is known to improve performance. In this section, we quantify the performance gains from using HTTP pipelining with realistic workloads and examine its interaction with lower-layer optimizations like increasing the ICW size.

HTTP 1.0 supported downloading only one object per connection, which was inefficient, especially for high-latency networks, as it required at least two round trips per object download. HTTP 1.1 introduced persistent connections and allowed pipelined requests over the same TCP connection, as illustrated in Figure 1.

HTTP pipelining can eliminate multiple expensive network round trips during web page downloads by allowing clients to request multiple objects without waiting for the corresponding responses. Additionally, pipelining enables the use of larger TCP ICW sizes, leading to faster downloads. As seen in Section 4.1, most web downloads currently cannot utilize TCP ICW sizes larger than 16, even if more bandwidth is available, because web browsers download one object at a time. However, with pipelining, multiple objects with larger aggregate sizes can be downloaded concurrently, allowing for the utilization of much larger windows and reducing the number of network round trips per download.

Unfortunately, most web browsers do not support HTTP pipelining. For example, Internet Explorer, the most popular browser, does not support it. Firefox, the second most popular browser, claims to support pipelining but has it disabled by default. These two browsers control more than 74% of the web browser market. We verified the level of pipelining support in Firefox by enabling it, configuring Firefox to use a single connection per domain, and downloading Yahoo!'s front page. Packet traces showed that only a few object downloads were pipelined, with at most two objects at a time, despite the web page having up to 15 objects per domain.

The primary reason for the lack of pipelining support in web browsers is the presence of non-compliant web proxies. This issue can be addressed by modifying web browsers to probe a well-known web server at startup to check for faulty proxies and enable pipelining if none are detected.

To evaluate the effectiveness of HTTP pipelining and its interplay with TCP ICW size, we conducted an experimental study. We built a simple web client that implements HTTP pipelining. It first downloads the HTML files, then connects to the domains hosting the different web page objects. All objects per domain are downloaded in a pipelined fashion over a single persistent TCP connection. Using the setup from Section 4.1.2 with Yahoo!'s front page as the workload, we set the TCP ICW size at the server to 300 (~450KB) and the web client's receive buffer to 450KB. Under these settings, the web page download took four round trips: two for connecting to the server and downloading the HTML file, and two for connecting to the domain and fetching all the objects. This is significantly fewer round trips compared to the minimum of eight round trips obtained in Section 4.1.

It is worth noting that similar minimal round trips can be achieved by downloading all web page objects concurrently via separate TCP connections. Current web browsers are moving in this direction by using multiple TCP connections per domain. However, this approach has several drawbacks. First, it limits TCP's ability to control congestion, as TCP manages congestion within a single connection. Concurrently transmitting packets across multiple connections can lead to congestion collapse, especially with high ICW sizes. A separate congestion manager may be needed to control congestion across all connections, increasing system complexity. Second, multiple concurrent connections consume more resources, such as per-connection TCP state and CPU cycles. Most web browsers limit the number of concurrent connections. For example, Firefox caps the number of persistent connections per domain to six and the total number of connections to 30. Mobile browsers, like Safari on the iPhone, use fewer concurrent connections (e.g., four). While the load from six connections per domain may not be significant for most clients, it is substantial for web servers, who must handle six times more connections and their associated overhead. Finally, using multiple TCP connections per application can be unfair to other applications and users who use a single connection per application.

### 5. DISCUSSION AND FUTURE WORK

As demonstrated in the previous sections, tuning the initial congestion window size can significantly impact web page load time, especially when combined with HTTP pipelining. In Section 4.3, we saw that web page load time can benefit from large ICW sizes, while some users suffer from smaller ICW sizes, as shown in Figures 18 and 17.

This wide range of optimal initial TCP window sizes calls for a dynamic scheme to set this size per connection, as there is no one-size-fits-all solution. The appropriate size depends on the connection characteristics, such as available bandwidth, which can be learned from the history of connections from the same location. TCP Fast Start [22] proposed reusing the state of older connections to the same host and modifying intermediate routers to give lower priority to packets from hosts using Fast Start. However, the effectiveness of this reuse is questionable, as typical hosts do not frequently reconnect to the same host, and maintaining persistent state per client can be expensive for servers serving millions of clients. Modifying intermediate routers is also a significant hurdle to adoption.

We believe that setting the initial TCP congestion window should rely on previous history, but a more effective approach tailored for web servers is needed. We leave this as future work. Additionally, due to its effectiveness, we advocate supporting pipelining in web browsers and implementing techniques, like [18], to overcome faulty proxies. This will allow for taking full advantage of larger ICW sizes.

### 6. CONCLUSION

In this paper, we characterized the traffic workload observed at the edges of Yahoo!'s content distribution network and noticed that many connections have high RTTs and significant retransmission rates. Based on this, we suggested and evaluated several optimizations at the TCP and HTTP layers, including TCP Appropriate Byte Counting, increasing the slow-start ICW, and HTTP pipelining. Our findings show that a majority of users would see significant benefits from increasing the ICW, with up to a 38% reduction in page load time. However, for clients in poorly connected networks with high packet loss rates, performance is likely to suffer with high ICW sizes. Therefore, we conclude that no "optimal" setting exists that satisfies all users. We also found that HTTP pipelining is very effective, especially when used with large ICW sizes, reducing page load time by up to 80%.

### 7. REFERENCES

[1] Global web stats. http://www.w3counter.com/globalstats.php, 2010.
[2] SPDY: An Experimental Protocol for a Faster Web. http://dev.chromium.org/spdy/spdy-whitepaper, 2010.
[3] ALLMAN, M. Tcp byte counting refinements. SIGCOMM Comput. Commun. Rev. 29 (July 1999), 14–22.
[4] ALLMAN, M. A web server’s view of the transport layer. SIGCOMM Comput. Commun. Rev. 30 (October 2000).
[5] ALLMAN, M. TCP Congestion Control with Appropriate Byte Counting (ABC). RFC 3465, IETF, 2003.
[6] ALLMAN, M. tcpsplit. http://www.icir.org/mallman/software/tcpsplit/, 2010.
[7] ALLMAN, M., FLOYD, S., AND PARTRIDGE, C. Increasing TCP’s Initial Window. RFC 3390, IETF, 2002.
[8] ARBOR NETWORKS. 2009 Internet Observatory Report. http://www.nanog.org/meetings/nanog47/presentations/Monday/Labovitz_ObserveReport_N47_Mon.pdf, 2010.
[9] BALAKRISHNAN, H., RAHUL, H. S., AND SESHAN, S. An integrated congestion management architecture for internet hosts. In Proceedings of SIGCOMM ’99 (New York, NY, USA, 1999), ACM.
[10] CHU, J., DUKKIPATI, N., CHENG, Y., AND MATHIS, M. Increasing TCP’s Initial Window. http://tools.ietf.org/html/draft-ietf-tcpm-initcwnd-01, 2011.
[11] DUKKIPATI, N., REFICE, T., CHENG, Y., CHU, J., HERBERT, T., AGARWAL, A., JAIN, A., AND SUTIN, N. An argument for increasing tcp’s initial congestion window. SIGCOMM Comput. Commun. Rev. 40 (June 2010).
[12] FIELDING, R., GETTYS, J., MOGUL, J., FRYSTYK, H., MASINTER, L., LEACH, P., AND BERNERS-LEE, T. Hypertext Transfer Protocol – HTTP/1.1, 1999.
[13] FREEDMAN, M. J., FREUDENTHAL, E., AND MAZIÈRES, D. Democratizing content publication with coral. In Proceedings of NSDI ’04 (Berkeley, CA, USA, 2004), USENIX Association.
[14] HOPKINS, A. Optimizing Page Load Time. http://www.die.net/musings/page_load_time/, 2010.
[15] KRISHNAMURTHY, B., AND WANG, J. On network-aware clustering of web clients. In Proceedings of SIGCOMM ’00 (New York, NY, USA, 2000), ACM.
[16] KRISHNAN, R., MADHYASTHA, H. V., SRINIVASAN, S., JAIN, S., KRISHNAMURTHY, A., ANDERSON, T., AND GAO, J. Moving beyond end-to-end path information to optimize cdn performance. In Proceedings of IMC ’09 (New York, NY, USA, 2009), ACM.
[17] LEIGHTON, T. Improving Performance on the Internet. Commun. ACM 52 (February 2009).
[18] NOTTINGHAM, M. Making HTTP Pipelining Usable on the Open Web. http://tools.ietf.org/html/draft-nottingham-http-pipeline-00, 2010.
[19] NYGREN, E., SITARAMAN, R. K., AND SUN, J. The akamai network: a platform for high-performance internet applications. SIGOPS Oper. Syst. Rev. 44 (August 2010).
[20] OLSHEFSKI, D., AND NIEH, J. Understanding the management of client perceived response time. In Proceedings of SIGMETRICS ’06/Performance ’06 (New York, NY, USA, 2006), ACM.
[21] OSTERMANN, S. tcptrace. http://www.tcptrace.org/, 2010.
[22] PADMANABHAN, V. N., AND KATZ, R. H. TCP Fast Start: A Technique For Speeding Up Web Transfers. In IEEE Globecom (1998).
[23] QIAN, F., GERBER, A., MAO, Z. M., SEN, S., SPATSCHECK, O., AND WILLINGER, W. Tcp revisited: a fresh look at tcp in the wild. In Proceedings of IMC ’09 (New York, NY, USA, 2009), ACM.
[24] R. STEWART, E. Stream Control Transmission Protocol. RFC 4960, IETF, 2007.
[25] SOUDERS, S. High-performance web sites. Commun. ACM 51 (December 2008).

### Summary Review Documentation for "Overclocking the Yahoo! CDN for Faster Web Page Loads"

**Authors:** M. Al-Fares, K. Elmeleegy, B. Reed, I. Gashinsky

**Reviewer #1**

**Strengths:**
- The results provide valuable insights into web performance as perceived by real clients and shed light on TCP dynamics and the impact of various refinements.
- The paper demonstrates that uniformly increasing the initial congestion window to 10 segments benefits most connections but increases download times for a fraction of connections due to higher loss.

**Weaknesses:**
- The data is primarily straightforward reporting of performance for a few different options, which may not constitute a significant contribution compared to other work.

**Comments to Authors:**
- The paper is well-explained and well-supported. Consider delving deeper into what happens when page load latency is higher than expected (higher RTT multiplier). Provide more details on how and why latency varies for a fixed RTT and object size.

**Reviewer #2**

**Strengths:**
- The dataset used to evaluate ICW, ABC, and HTTP pipelining is new and collected from an operational CDN. The performance evaluation is detailed.

**Weaknesses:**
- Techniques like increasing the TCP congestion window, ABC, and HTTP pipelining are not new and are already used in major CDNs.
- The dataset is limited to four nodes of the Yahoo CDN and one hour of operation. The argument that peak activity occurs at 1 pm local time is weak.
- Some results are not well explained and raise questions about their validity.

**Comments to Authors:**
- Include references to existing work on improving web performance, such as "Improving Performance on the Internet" (Communications of ACM 2009) and "The Akamai Network: A Platform for High-Performance Internet Applications" (SIGOPS Oper. Syst. Rev. 2010).
- Explain why you selected only four nodes and whether the results would be consistent if the study were extended to other nodes.
- Provide evidence for why 1 pm local time is the peak time in all nodes.
- Evaluate the extent to which high RTTs and varying RTT values in a session are due to the mismatch between end-users and their DNS resolvers.
- Explain how you infer the access technology used by different users (x-axis in Figure 7).
- Improve the structure and presentation of the figures (e.g., Figures 4 and 5 are difficult to follow).
- Use network-aware clustering of web clients to better characterize perceived performance (as proposed in SIGCOMM 2000).
- Correlate your results with the size of requested objects and the location of end-users to provide hints on tuning the congestion window size.

**Reviewer #3**

**Strengths:**
- The most interesting part of the paper is the traces from a live CDN provider with a lot of traffic and popular web properties.

**Weaknesses:**
- The analysis of transport and application-level optimizations is less valuable, as this is a well-researched area with many existing solutions and measurements.
- The value of the paper lies in the evaluation of a few techniques, but these are well-researched, and the paper does not bring much new information.
- The methodology and measurement techniques are thin.

**Comments to Authors:**
- See above.

**Reviewer #4**

**Strengths:**
- Nice data: packet traces from Yahoo! CDN in four different countries. Interesting results on aspects impacting web page load times and discussions on how proposed optimizations should work given the Yahoo! workload.

**Weaknesses:**
- The paper lacks a thorough analysis and discussion of the biases and limitations of the results.
- The evaluation of the optimizations is not comprehensive.

**Comments to Authors:**
- Discuss the biases and limitations of the results. Analyze the content types being measured and compare them to other large web providers' workloads.
- Explain why RTTs for Yahoo! are smaller than those observed for the Google CDN. Is it due to better connectivity or general improvements in RTTs?
- Focus less on geographical proximity in the discussion of Figure 6 and more on network-level connectivity.
- Explain how you determine the connectivity type in Figure 7.
- Provide a small characterization of the content types studied to understand the results in Figure 8.
- Clarify the correlation between the results in Figures 9 and 8.
- Explain what is presented in Figure 14 and whether it is a CCDF.
- Address the statement in the first paragraph of Section 4 that we can't do much to improve RTTs. If you are Yahoo!, it is possible to improve RTT at the access.
- The results in Section 4.1.2 are speculative and could be tested in live connections for some controlled hosts.
- The experiments in Section 4.1.3 are not conclusive, and the argument at the end of this section is hard to follow.
- Sections 4.3, paragraphs 2 and 3, are repeated from Sections 1 and 2.

**Reviewer #5**

**Strengths:**
- The paper reports measurement results collected from Yahoo's global CDN network. It provides various interesting statistics, such as the retransmissions and HTTP object size distribution.

**Weaknesses:**
- Despite its value, the paper does not report anything exciting. The topic itself—how TCP settings affect download delay—is not novel.

**Comments to Authors:**
- State the goal of the paper upfront. The first page is too general and focuses on well-known issues and ongoing work about TCP in IETF.
- The findings of this paper would be highly interesting to the operational community and IETF, as they are discussing recommended new settings for ICW.
- The change in ICW value reflects changes in the underlying network infrastructure capacity and traffic load patterns, and hence engineering tuning of operational parameters; it is not surprising that no one size fits all users in a global network.
- Make the contributions clearer in the camera-ready version.

**Response from the Authors**

First, we would like to thank the reviewers for their feedback. One common comment was about the novelty of the techniques used to reduce web page load times. We do not claim novelty in inventing or using any of these techniques. The contribution of this paper is studying the impact of these techniques in the wild using real workloads and evaluating their interplay. We will make this clearer in the camera-ready version.

Another common comment was about using only four CDN nodes and one hour of traffic. Each of these four nodes belongs to a different site in a different region of the world, and we believe the traffic at these regions is representative of Yahoo! traffic globally. We have verified that traffic at nodes in the same site has similar characteristics, so we only report traffic from a single node per site. Additionally, we have verified that traffic during other times of the day qualitatively has the same characteristics as that in the afternoon. The traffic we analyzed contains over 12 million connections spanning hundreds of gigabytes of traces, which we believe to be a reasonable sample to give a general idea about Yahoo!'s CDN traffic characteristics.

Regarding the comment that high RTTs could be due to misrouting users to wrong CDN nodes, we do not believe this is the case. We use a proprietary technique to route users that we believe to be accurate. Our results are qualitatively in line with results reported by another study (Krishnan et al. 2009) over another CDN.

We addressed most of the other lower-level comments in the camera-ready version. The things we could not address properly are requests for comparisons with other CDNs, as we do not have access to the CDNs of other companies.