current default value of 3.
We conclude that if the TCP standard is changed advocating
larger ICW sizes, this will be unfair to some ﬂows, e.g. long ﬂows
having high packet loss.
4.1.4 Discussion
In this section, we have seen that varying ICW size can have a
signiﬁcant impact on web page load times. While a big fraction of
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
Object Transfer Time in RTTs from CDN−Singapore
initcwnd 1, abc 0
initcwnd 1, abc 1
initcwnd 1, abc 2
initcwnd 3, abc 0
initcwnd 3, abc 1
initcwnd 3, abc 2
initcwnd 7, abc 0
initcwnd 7, abc 1
initcwnd 7, abc 2
initcwnd 16, abc 0
initcwnd 16, abc 1
initcwnd 16, abc 2
n
o
i
t
c
n
u
F
n
o
i
t
u
b
i
r
t
s
D
e
v
i
t
a
u
m
u
C
l
i
0
0
2
4
6
RTTs
8
10
Figure 21: First object transfer time in RTTs, for different
ICW and ABC settings.
users beneﬁt from larger ICW sizes, others may suffer. Moreover,
the level of beneﬁt or suffering can vary across users according
to their connection quality. Consequently, to achieve best perfor-
mance, different users should have different values of ICW depend-
ing on their connection quality. Hence, we argue that a dynamic
mechanism for conﬁguring the ICW per connection is needed.
4.2 Appropriate Byte Counting (ABC)
As explained above, the deployment of delayed acknowledg-
ments is pervasive. This leads to the congestion window growing
only by a factor of 1.5 per round trip. ABC mitigates this effect by
relying not on ACK arrival but on the number of bytes acknowl-
edged instead to increase the window size. This results in the con-
gestion window doubling during slow-start every round trip as it
was intended in the original TCP design. Linux allows 3 ABC set-
tings: 0 turns off ABC; 1 increments the window by one for each
acknowledged full segment; 2 increments the window by two for
each acknowledgement received.
4.2.1 Evaluation Using Live Trafﬁc
We varied the ABC settings at the server and show their effect in
Figure 21. From these results we ﬁnd that turning this feature on
has a positive, but limited, effect on object transfer times.
4.2.2 Studying Page Load time
To study the effects of TCP ABC on overall web page load time,
we used the same setup used in 4.1.2, except we used an RTT of
100ms and packet loss of zero and measured the page load time
with the ABC optimization turned on and off. As shown in Fig-
ure 22, turning on the ABC optimization has marginal effects on
the overall web page load time. The reason for this is that the ef-
fects of ABC will only be noticeable after many round trips. How-
ever, each TCP connection downloads very small number of little
objects, and thus it requires only few packets and few round trips.
Hence, there is not much difference in transmission time.
579Page Load Time at RTT = 100ms, Loss = 0%
ABC = 0
ABC = 2
2.5
2
1.5
1
0.5
i
)
s
(
e
m
T
d
a
o
L
e
g
a
P
l
t
a
o
T
0
1
3
8
16
Initial Congestion Window
Figure 22: Page load time, for different ICW and ABC settings.
4.3 HTTP Pipelining
HTTP pipelining is known to deliver better performance.
In
this section, we quantify the performance gains from using HTTP
pipelining using realistic workloads. We also study the interplay
of HTTP pipelining with lower layers optimizations like increasing
the ICW size.
HTTP 1.0 only supported downloading a single object per con-
nection. This was inefﬁcient especially for high delay networks as
at least two network round trips are spent per object download –
one for TCP connection establishment and another for download-
ing the object. Moreover, web pages are typically composed of
multiple small objects, making HTTP 1.0 even less efﬁcient for la-
tency purposes. Later, HTTP 1.1 supported persistent connections
spanning multiple object downloads. Furthermore, it allowed for
pipelined HTTP requests over the same TCP connection as shown
in Figure 1.
HTTP pipelining can eliminate multiple expensive network
round trips from a web page download. This is because the client
can request multiple objects without waiting for the corresponding
responses to arrive from the server as seen in Figure 1.
Furthermore, pipelining allows web downloads to utilize much
larger TCP ICW sizes leading to faster downloads. As seen in Sec-
tion 4.1, currently the vast majority of web downloads cannot uti-
lize TCP ICW larger than 16, even if the web user has more network
bandwidth available. This is because web browsers download one
object at a time. As seen in Figure 11, 90% of objects downloaded
from the Yahoo! CDN are less than 24KB, i.e. they can ﬁt in a TCP
window of 16. In contrast, with pipelining, multiple objects with
much larger aggregate sizes are downloaded concurrently. Hence,
much larger windows can be utilized to reduce the number of net-
work round trips per download.
Unfortunately, most web browsers do not support HTTP pipelin-
ing. For example Internet Explorer, the most popular browser [1],
does not support it. Firefox, the second most popular browser,
claims to support pipelining but has it switched off by default. As
of today, these two browsers control more than 74% of the web
browser market [1]. We tried to verify the level of pipelining sup-
ported by Firefox. We turned pipelining on, conﬁgured Firefox to
use a single connection to download objects from each domain, and
then downloaded Yahoo!’s front page. By looking at packet traces
of the download, we realized that only a few object downloads were
pipelined – at most two at time. This is in spite the fact that the web
page had many objects per domain – up to 15 objects.
A main reason for not supporting pipelining by web browsers
is that some web proxies in the Internet do not support it. How-
ever, this problem can be overcome if web browsers are modiﬁed
to probe a well known web server, at start time, to check if it is
connected to the server via a faulty proxy or not [18]. If no faulty
proxy is found, pipelining is used, otherwise, it is not.
To study the effectiveness of HTTP pipelining and its interplay
with TCP ICW size, we evaluated it experimentally. We built a
simple web client that implements HTTP pipelining. It ﬁrst down-
loads the HTML ﬁles. Then, it connects to the domains hosting the
different web page objects. All objects per domain are downloaded
in a pipelined fashion over a single persistent TCP connection. We
used the setup in Section 4.1.2 having Yahoo!’s front page as our
workload. The page objects span multiple domains. The domain
with the maximum aggregate objects’ size had 14 objects with total
aggregate size of 445KB. To allow for a single network round trip
download, we set the TCP ICW size at the server to 300 (∼450KB).
We also set the web client’s receive buffer to 450KB. Under these
settings, the web page download took 4 round trip times. The ﬁrst
two round trips were to connect to the server then download the
HTML ﬁle. Once the HTML ﬁle is downloaded, its embedded ob-
jects and their locations are identiﬁed. The client then starts par-
allel connections to all the domains involved, one connection per
domain. For each domain, HTTP pipelining is used to fetch all
the objects in question at this domain. Hence, The two last round
trips are for connecting to the domain, then fetching all the objects.
This is well below the minimum number of round trips obtained in
Section 4.1 – 8 round trips.
It is worth mentioning that this minimal number of network
round trips can be also achieved by downloading all the web page
objects concurrently each via a separate TCP connection. Current
web browsers are moving in this direction by having multiple TCP
connections per domain. This is why we see in Figure 12 that
the average number of object downloads per TCP connection is
2.4. However, this approach has many shortcomings. First, it
limits TCP’s ability to control congestion as TCP controls con-
gestion within single connection. So, if many connections start
transmitting packets concurrently congestion collapse can happen,
especially if the connections have high ICW. In this case a sepa-
rate congestion manager [9] may be needed to control congestion
across all the connections, which increases the system’s complex-
ity. Second, having multiple concurrent connections consumes
more resources, e.g.
the per connection TCP state and the CPU
cycles to maintain this state. That is why most web browsers cap
the number of concurrent connections used for objects download.
For example, by default, Firefox caps the number of persistent con-
nections per domain to 6 and caps the total number of connections
to 30. Web browsers for mobile phones use less concurrent con-
nections, e.g. Safari for the iPhone uses 4 connections. While the
load from 6 connections per domain may be not that signiﬁcant for
most clients, it is certainly signiﬁcant on web servers. It effectively
means that they have to handle 6 times more connections and their
corresponding overhead. Finally, using multiple TCP connections
per application can be unfair to other applications and users that
use a single connection per application.
5805. DISCUSSION AND FUTURE WORK
As we have seen in the previous sections tuning the initial con-
gestion window size can have a great impact on web page load time,
especially if used in conjunction with HTTP pipelining. In Section
4.3, we have seen that web page load time can beneﬁt from huge
ICW sizes measured in hundreds of packets. Whereas, we see in
Figures 18 and 17 that some users suffer from having much smaller
ICW sizes.
This wide range of optimal initial TCP window sizes calls for
a dynamic scheme for setting this size per connection as there is
no one size that ﬁts all. The right size depends on the connection
characteristics, e.g. available bandwidth. This information can be
learned from history of connections coming from the same loca-
tion. TCP Fast Start [22] was proposed to allow a new connection
to a host to reuse the state of older connections to the same host. It
also included a mechanism to modify intermediate routers to iden-
tify packets from hosts using Fast Start and give these packets lower
priority in the event of congestion. This is to account for the fact
that Fast Start may be using stale cached information. It is ques-
tionable though how effective this reuse will be. A typical host
does not reconnect to the same other host very frequently. Hence,
per-host cached information is likely to be stale. Moreover, main-
taining persistent state per client may be very expensive for a server
serving millions of clients. Finally, the requirement for modifying
intermediate routers is a signiﬁcant hurdle to adoption.
We believe that the right approach to setting the initial TCP con-
gestion should rely on previous history as well. However, a more
effective approach tailored for web servers is needed. We leave this
as future work.
Finally, because of its effectiveness, we advocate supporting
pipelining in web browsers and implementing techniques, like [18],
to overcome faulty proxies. Moreover, it will allow for taking full
advantage of larger ICW sizes.
6. CONCLUSION
In this paper, we ﬁrst characterized the trafﬁc workload observed
at the edges of Yahoo!’s content distribution network. We noticed
that many connections have high RTTs. Some had signiﬁcant re-
transmission rates. Based on this, we suggested and evaluated, both
in the wild and in the lab, the effects of several optimizations at
the TCP and HTTP layers with the goal of reducing overall page-
load time. These included a combination of TCP Appropriate Byte
Counting, increasing the slow-start ICW, and HTTP pipelining.
Overall, we ﬁnd that, based on our trafﬁc studies, a majority of
users would see signiﬁcant beneﬁt from increasing the ICW – up to
38% reduction in page load time. However, for clients in poorly
connected networks with high packet loss-rates, performance is
likely to suffer when using high ICW sizes. For this reason, we
conclude that no “optimal” setting exists that satisﬁes all users. We
also found that HTTP pipelining is very effective, especially if used
in conjunction with large ICW sizes. This combination can reduce
page load time by up to 80%.
7. REFERENCES
[1] Global web stats. http://www.w3counter.com/
globalstats.php, 2010.
[2] SPDY: An Experimental Protocol for a Faster Web.
http://dev.chromium.org/spdy/
spdy-whitepaper, 2010.
[3] ALLMAN, M. Tcp byte counting reﬁnements. SIGCOMM
Comput. Commun. Rev. 29 (July 1999), 14–22.
[4] ALLMAN, M. A web server’s view of the transport layer.
SIGCOMM Comput. Commun. Rev. 30 (October 2000).
[5] ALLMAN, M. TCP Congestion Control with Appropriate
Byte Counting (ABC). RFC 3465, IETF, 2003.
[6] ALLMAN, M. tcpsplit. http://www.icir.org/
mallman/software/tcpsplit/, 2010.
[7] ALLMAN, M., FLOYD, S., AND PARTRIDGE, C. Increasing
TCP’s Initial Window. RFC 3390, IETF, 2002.
[8] ARBOR NETWORKS. 2009 Internet Observatory Report.
http://www.nanog.org/meetings/nanog47/
presentations/Monday/Labovitz_
ObserveReport_N47_Mon.pdf, 2010.
[9] BALAKRISHNAN, H., RAHUL, H. S., AND SESHAN, S. An
integrated congestion management architecture for internet
hosts. In Proceedings of SIGCOMM ’99 (New York, NY,
USA, 1999), ACM.
[10] CHU, J., DUKKIPATI, N., CHENG, Y., AND MATHIS, M.
Increasing TCP’s Initial Window. http://tools.ietf.
org/html/draft-ietf-tcpm-initcwnd-01,
2011.
[11] DUKKIPATI, N., REFICE, T., CHENG, Y., CHU, J.,
HERBERT, T., AGARWAL, A., JAIN, A., AND SUTIN, N.
An argument for increasing tcp’s initial congestion window.
SIGCOMM Comput. Commun. Rev. 40 (June 2010).
[12] FIELDING, R., GETTYS, J., MOGUL, J., FRYSTYK, H.,
MASINTER, L., LEACH, P., AND BERNERS-LEE, T.
Hypertext Transfer Protocol – HTTP/1.1, 1999.
[13] FREEDMAN, M. J., FREUDENTHAL, E., AND MAZIÈRES,
D. Democratizing content publication with coral. In
Proceedings of NSDI ’04 (Berkeley, CA, USA, 2004),
USENIX Association.
[14] HOPKINS, A. Optimizing Page Load Time. http://www.
die.net/musings/page_load_time/, 2010.
[15] KRISHNAMURTHY, B., AND WANG, J. On network-aware
clustering of web clients. In Proceedings of SIGCOMM ’00
(New York, NY, USA, 2000), ACM.
[16] KRISHNAN, R., MADHYASTHA, H. V., SRINIVASAN, S.,
JAIN, S., KRISHNAMURTHY, A., ANDERSON, T., AND
GAO, J. Moving beyond end-to-end path information to
optimize cdn performance. In Proceedings of IMC ’09 (New
York, NY, USA, 2009), ACM.
[17] LEIGHTON, T. Improving Performance on the Internet.
Commun. ACM 52 (February 2009).
[18] NOTTINGHAM, M. Making HTTP Pipelining Usable on the
Open Web. http://tools.ietf.org/html/
draft-nottingham-http-pipeline-00, 2010.
[19] NYGREN, E., SITARAMAN, R. K., AND SUN, J. The
akamai network: a platform for high-performance internet
applications. SIGOPS Oper. Syst. Rev. 44 (August 2010).
[20] OLSHEFSKI, D., AND NIEH, J. Understanding the
management of client perceived response time. In
Proceedings of SIGMETRICS ’06/Performance ’06 (New
York, NY, USA, 2006), ACM.
[21] OSTERMANN, S. tcptrace. http://www.tcptrace.
org/, 2010.
[22] PADMANABHAN, V. N., AND KATZ, R. H. TCP Fast Start:
A Technique For Speeding Up Web Transfers. In IEEE
Globecom (1998).
[23] QIAN, F., GERBER, A., MAO, Z. M., SEN, S.,
SPATSCHECK, O., AND WILLINGER, W. Tcp revisited: a
fresh look at tcp in the wild. In Proceedings of IMC ’09
(New York, NY, USA, 2009), ACM.
[24] R. STEWART, E. Stream Control Transmission Protocol.
RFC 4960, IETF, 2007.
[25] SOUDERS, S. High-performance web sites. Commun. ACM
51 (December 2008).
581Summary Review Documentation for 
“Overclocking the Yahoo! CDN for 
 Faster Web Page Loads” 
Authors: M. Al-Fares, K. Elmeleegy, B. Reed, I. Gashinsky 
Reviewer #1 
Strengths:  The  results  of  the  paper  show  Web  performance  as 
perceived by real clients and shed light on TCP dynamics and the 
likely  impact  of  various  well-known  refinements.  For  example, 
the paper uses its data to show that uniformly increasing the initial 
congestion window to 10 segments helps most connections but at 
the cost of longer downloads (due to increased loss) for a fraction 
of the connections. 
Weaknesses:  The  data  in  the  paper  is  great,  but  it  is  mostly 
straightforward  reporting  of  performance  for  a  few  different 
options,  and  so  perhaps  the  paper  is  not  a  large  contribution 
compared to other work. It just depends on the submission pool.  
Comments  to  Authors:  Thanks  for  an  inside  look  at  Web 
performance an enjoyable read! I really had very few comments 
while  reading,  as  you  paper  was  very  explanatory  and  well 
supported.  
The  main  way  I  thought  you  could  improve  your  paper  was  to 
look more deeply into what happens when the page load latency 
is  larger  than  expected  (higher  RTT  multiplier).  You  do  have  a 
latency distribution, but I think this combines many effects even 
though it is RTT-normalized, like objects of different sizes. How 
and why does latency vary for a fixed RTT and object size? If you 
could  answer 
important 
refinements.  
Fig 15, etc.: how can the transfer time be smaller than one RTT 
something like 30% of the time?  
Reviewer #2 
Strengths: The data set used to evaluate ICW, ABC and HTTP 
pipelining is new and is collected from an operational CDN. The 
performance evaluation of ICW, ABC and HTTP in this data set 
is detailed. 
Weaknesses:  Techniques  such  increase  of  TCP  congestion 
window, ABC and HTTP pipelining are not new, and have been 
already used in major CDNs. There is no in-depth analysis of the 
assignment of end-users to nodes. High RTT might be an artifact 
of the mis-location of users. The data set used collected in only 
four nodes of the Yahoo CDN and for only 1-hour operation. The 
argument that the peak activity of all nodes is on 1 pm local time 
is  weak.  Some  of  the  results  are  not  well  explained  and  raise 
questions on the validity of the results. 
Comments to Authors: Many CDNs are already using increased 
TPC  congestion  window  ABC  and  HTTP  pipelining,  see  e.g., 
“Improving  Performance  on  the  Internet”  Communications  of 
ACM  2009,  “The  Akamai  Network:  A  Platform  for  High-
it  might  suggest 
this, 
the  most 
performance  Internet  Applications”.  SIGOPS  Oper.  Syst.  Rev.” 
2010. You may want to include these references in your paper.  
Yahoo  CDN has more than four  nodes. You have to argue why 
you selected just four of them. It is because you have access to 
only  four  of  them?  If  you  perform  the  same  study  in  the  other 
nodes do you conclude to the same results?  
You have to provide enough evidence why 1 pm local time is the 
peak time in all nodes.  
You have to evaluate to which degree the high RTT as well as the 
different values of RTTs in a session are due to the fact that users 
are  assigned  to  nodes  based  on  the  LDNS  location  and  not  the 
end-user. There are many references in the literature that show the 
mis-match between end-user and CDN nodes, e.g.,”A Precise and 
Efficient  Evaluation  of  the  Proximity Between Web Clients and 
Their  Local  DNS  Servers”  USENIX  ATC  2002,  “Comparing 
DNS Resolvers in the Wild” IMC 2010.  
It is somehow suspicious that users in Illinois (a state that hosts 
one of the CDN nodes -- in Chicago) has the highest median RTT 
as shown in Figure 6.  
You  also  have  to  explain  how  you  infer  the  access  technology 
used by different users (x-axis in Figure 7).  
The structure of the paper and the presentation of the figures have 
to be improved (eg. Figure 4 and 5 are difficult to follow).  
A better way to characterize the perceived performance to users is 
to  use  cluster  them  based  on  their  network  locations  and 
characteristics as proposed in: “On Network-Aware Clustering of 
Web Clients”, SIGCOMM 2000.  
In  Figures  18  and  19,  you  show  indeed  that  there  is  no  single 
optimal  value  of  congestion  window.  Can  you  correlate  your 
results  with  the  size  of  requested  object  and  the  location  of  the 
end-user  in  order  to  give  some  hints  on  how  the  size  of  the 
congestion window can be tuned? 
Reviewer #3 
Strengths:  The  most  interesting  part  of  this  paper  is  the  traces 
from a live CDN provider with a lot of traffic and popular web 
properties. The rest of the analysis both in terms of transport level 
optimizations  as  well  as  application  level  optimizations  is  of 
lesser value. 
Weaknesses: This is a very well researched and studied area and 
it  is  hard  for  a  paper  of  this  type  to  bring  new  or  unexpected 
results. Both academia as well as industry have provided solutions 
and  measured 
the  performance  results  obtained  by  such 
optimizations. In fact, moderm CDNs provide separate solutions 
582to  accelerate  Dynamic  Web  sites,  which 
include  TCP 
optimizations  including  TCP  splicing  and  protocol  parameter´s 
tuning,  application  level  optimizations  such  as  compression  and 
content  transformation,  and  http  level  improvements  including 
proxy2proxy pipelining and reduction of RTTs. Companies such 
as Netli, later bought by Akamai, have been doing this for over a 
decade.  
The value of the paper resides more on the evalution of a few of 
those  techniques.  However,  the  ones  picked  are  the  ones  better 
researched,  e.g.  congestion  control  window,  and  http  pipelining. 
Other than saying that a single ICW does not work well across all 
clients, the paper does not bring much more interesting info.  
In  terms  of  methodology  or  new  measurement  techniques  the 
paper is also very thin. 
Comments to Authors: See above. 
Reviewer #4 
Strengths: Nice data: packet traces from for Yahoo! CDN in four 
different countries. Interesting results on some of the aspects that 
impact  web  page  load  times  and  discussions  on  how  proposed 
optimizations should work given the Yahoo! workload.  
Weaknesses:  The  paper  lacks  more  analysis  and  discussions  of 
the biases of the results.The evaluation of the optimizations is not 
thorough. 
Comments  to  Authors:  The  paper  lacks  a  discussion  on  the 
biases  and  limitations  of  the  results.  Yahoo!  is  a  large  web 
provider,  so  it  does  carry  different  content  types.  The  paper 
should  analyze  the  content  that  is  being  measured.  Is  this  only
Yahoo!’s  main  site?  After  a  discussion  of  the  content  types 
studied,  then  it  should  be  possible  to  put  these  results  into 
perspective.  How  does  this  workload  compare  with  other  large 
web  providers’  workload?  Is  there  anything  that  is  particular  to 
Yahoo!?  Similarly,  does  the  locality  of  the  CDN  nodes  or  the 
upstream connectivity bias the results in any way?  
In  Sec.  3.2.1,  why  are  RTTs  for  Yahoo!  smaller  than  those 
observed  for  the  Google  CDN?  Is  it  that  the  Yahoo!  nodes  are 
better connected or that the RTTs improved in general since their 
study? It would be nice to have a bit more explanation of these 
differences. 
The  discussion  of  Fig.  6  focuses  a  lot  on  geography.  Clearly, 
geographical  proximity  should  reduce 
the 
geographical  proximity  of  the  two  end-hosts  doesn’t  imply  that 
the network path stays within a limited area. It also depends on 
the  network-level  connectivity.  You  could  run  traceroutes  from
the server to the IPs to verify whether you can explain some of 
the differences because of the network level path. 
In Fig. 7, how do you determine the connectivity type? 
The results in Fig. 8 should vary depending on the type of web 
content. It would be nice to have a small characterization of the 
content types you study. 
How do results in Fig 9 correlate to Fig 8? 
What is presented in Fig. 14? Is this a CCDF? Not sure this figure 
and discussion is really needed for the story in this paper. 
the  RTTs,  but 
First paragraph of Sec. 4 says that we can’t do much to improve 
RTTs. Is this true? If you are Yahoo! it is easier to work on TCP 
or HTTP, but it is possible to improve RTT at the access. 
The  results  in  Sec.  4.1.2  are  a  bit  of  speculation.  It would have 
been  better  to  test  this  in  some  live  connections  for  some  hosts 
you control. 
The discussion in Sec. 4.1.3 makes sense, but the experiments are 
not conclusive. It is just one example. The argument at the of this 
section is a little hard to follow. The conclusion is expected. It is 
unclear, the analysis is really necessary. 
Sec. 4.3, paragraphs 2 and 3 are repeated from Sec. 1 and 2. 
Reviewer #5 
Strengths: The paper reports measurement results collected from 
yahoo’s global CDN network. It reports various interesting stats 
that other researchers may find useful to know (for example fig-
10 on retransmissions, even for India clients 70% of connections 
see no retransmissions! Fig-11, HTTP object size distribution). 
Weaknesses:  Despite  its  value  the  paper  does  not  really  report 
something that seems exciting. It is not the problem of writing but 
the topic itself -- how TCP settings affect download delay. 
Comments to Authors: A small nit on the writing: I like papers 
that tell the reader the goal of the paper upfront. But for this paper 
the first page is all about general description of well known issues 
and ongoing work about TCP in IETF. Not until the second page, 
the  paper  stated  “This  paper  is  concerned  with  the  delivery  of 
content  from  edge  servers  from  the  Yahoo!  CDN  to  the  users.” 
(the wording is still a bit too narrowly scoped, e.g. how would this 
work  be  interesting  to  community  as  a  whole  to  be  worth 
publishing at IMC?)  
It seems that the findings of this paper would be highly interesting 
to  the  operational  community  (e.g.  fig-6  results  seem  highly 
relevant to yahoo operations, why the RTT so high), and to IETF 
(e.g.  ICW  value 
is  mid  of  discussing 
recommended new settings).  
The  ICM  value  selection  change  reflects  changes 
the 
underlying  network  infrastructure  capacity  and  traffic  load 
patterns, and hence engineering tuning of operational parameters; 
I am not sure that represent some highly interesting issues to the 
research community.  
The finding that no one size for the ICW fits all the users is also 
not surprising, given data is collected from a global network, it is 
only expected that in some places the network resources are still 
not up to the newly tuned value for other places. 
Response from the Authors 
First off, we would like to thank the reviewers for their feedback. 
One of the common high-level comments was about the novelty 
of the techniques used to reduce the web page load times. We do 
not claim novelty inventing or using any of these techniques. The 
contribution  of  this  paper  is  studying  the  impact  of  these 
techniques  in  the  wild  using  real  workloads  and  evaluating  the 
interplay of these techniques. We will try to make this clearer in 
the camera-ready version. 
impact,  as  IETF 
in 
583Another  high-level  comment  was  about  only  using  four  CDN 
nodes  and  one  hour  of  traffic.  We  would  like  to  point  out  that 
each of these four nodes belongs to a different site in a different 
region  of  the  world.  We  believe  that  traffic  at  these  regions  is 
representative of Yahoo! traffic from across the world. We have 
verified  that  traffic  at  nodes  in  the  same  site  have  similar 
characteristics;  hence  we  only  report  traffic  from  a  single  node 
per site. Also, we have verified that traffic during other times of 
the  day  qualitatively  has  the  same  characteristics  as  that  in  the 
afternoon.  Moreover,  we  would  like  to  point  out  that  the  traffic 
we  analyzed  contains  over  12  million  connections  spanning 
hundreds  of  gigabytes  of  traces,  which  we  believe  to  be  a 
reasonable  sample  to  give  a  general  idea  about  Yahoo!’s  CDN 
traffic characteristics. 
About the comment that the high RTTs measured could be due to 
misrouting  users  to  wrong  CDN  nodes  because  of  a  mismatch 
between end users and their DNS resolvers as shown in previous 
work, we do not believe that this is the case. We use a different 
proprietary  technique  to  route  users  that  we  believe  to  be 
accurate.  Moreover,  our  results  are  qualitatively  in  line  with 
results  reported  by  another  study  (Krishnan  et.  al.  2009)  over 
another CDN. 
We  addressed  most  of  the  other  lower  level  comments  in  the 
camera-ready version. The things we could not address properly 
are  requests for comparisons with other CDNs. We do not have 
access to the CDNs of other companies, so we cannot speculate. 
584