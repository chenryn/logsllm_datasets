promote rapid visual identiﬁcation [14]. NuCaptcha, for
instance, presents a streaming video containing moving
text against a dynamic background. The videos have four
noticeable characteristics, namely: (1) the letters are pre-
sented as rigid objects in order to improve a user’s abil-
ity to recognize the characters; (2) the background video
and the foreground character color are nearly constant in
color and always maintain a high contrast—we posit that
this is done to ease cognitive burden on users; (3) the
random “codewords” each have independent (but over-
lapping trajectories) which better enable users to distin-
guish adjacent characters; (4) lastly, the codewords are
chosen from a reduced alphabet where easily confused
characters are omitted. Some examples of a state-of-the-
work well at defeating CR-still captchas (e.g., [32, 47]).
More speciﬁcally, choose k frames at random, and iden-
tify the foreground pixels of the codeword by comparing
their color with a given reference color; notice the at-
tacker would likely know this value since the users are
asked to, for example, “type the RED moving charac-
ters”. Next, the length of the codeword can be inferred
by ﬁnding the leftmost and rightmost pixels on the fore-
ground. This in essence deﬁnes a line spanning over the
foreground pixels (see Figure 2). The positions of the
characters along the line can be determined by dividing
the line into n equidistant segments, where n denotes the
desired number of characters in the codeword. For each
of the segments, compute the center of gravity of the
foreground pixels in the vertical area of the image be-
longing to the segment. Lastly, select an image patch (of
the expected size of the characters) around the centers of
gravity of the segments, and feed each patch to a classi-
ﬁer. In our work, we use a neural network approach [39]
because it is known to perform well at this object identi-
ﬁcation task. The neural network is trained in a manner
similar to what we discuss in §4.3.
Figure 1: Example moving-image object recognition (MIOR)
captchas from NuCaptcha (see http://nucaptcha.com/demo).
art MIOR captcha are given in Figure 1.
Before delving into the speciﬁcs of our most success-
ful attack, we ﬁrst present a naïve approach for automat-
ically decoding the challenges shown in MIOR captchas.
To see how this attack would work, we remind the reader
that a video can be seen as a stream of single pictures that
simply provides multiple views of a temporally evolving
scene. It is well known that human observers perceive a
naturally moving scene at a level of about thirty frames
per second, and for this reason, video captchas tend to
use a comparable frame rate to provide a natural video
experience that is not too jerky. Similarly, the challenge
shown in the captcha is rendered in multiple frames to
allow users to perceive and decode the codewords in an
effortless manner. In the NuCaptcha scheme, for exam-
ple, a single frame may contain the full codeword.
4.1 A Naïve Attack
Given this observation, one way to attack such schemes
is to simply apply traditional OCR-based techniques that
Figure 2: Naïve attack: Based on the foreground pixels, we
ﬁnd the longest horizontal distance (white line) and the mean
value of vertical area (the respective bounding boxes above).
The above process yields a guess for each of the char-
acters of the codeword in the chosen frames of the video.
Let i denote the number of possible answers for each
character. By transforming the score from the neural net-
work into the probability pi jk where the j-th character
of the codeword corresponds to the i-th character in the
k-th frame, we calculate the probability Pi j for each char-
acter j = 1, . . . ,n of the codeword over all k frames as
Pi j = 1
sp ∑k pi jk with sp = ∑i, j,k pi jk. The choice that has
the highest probability is selected as the corresponding
character. With k = 10, this naïve attack resulted in a
success rate of approximately 36% accuracy in correctly
deducing all three characters in the codewords of 4000
captchas from NuCaptcha. While this relatively simple
attack already raises doubts about the robustness of this
new MIOR captcha, we now present a signiﬁcantly im-
proved attack that makes fewer assumptions about pixel
invariants [50] in the videos.
4.2 Exploiting Temporal Information
A clear limitation of the naïve attack is the fact that it
is not easily generalizable and it is not robust to slight
changes in the videos. In what follows, we make no as-
sumption about a priori knowledge of the color of the
codewords, nor do we assume that the centers of grav-
ity for each patch are equidistant. To do so, we apply a
robust segmentation method that utilizes temporal infor-
mation to improve our ability to recognize the characters
in the video.
segments as in the naïve attack (stage ). We select each
image patch containing a candidate character and evalu-
ate the patch using a neural network based classiﬁer [39]
(stage ). The classiﬁer outputs a likelihood score that
the patch contains a character. As a ﬁnal enhancement,
we incorporate a feedback mechanism in which we use
high conﬁdence inferences to improve low conﬁdence
detections of other patches. The net effect is that we
reduce the distractions caused by mutually overlapping
characters. Once all segments have been classiﬁed, we
output our guess for all characters of the codeword. We
now discuss the stages of our approach in more detail.
Figure 4: The circles depict salient features. These salient
features are usually corners of an object or texture areas.
Detecting Salient Features and Their
Motion (Stage )
A well-known class of salient features in the computer
vision community is gray value corners in images.
In
this paper, we use the Harris corner detector [21] for
computing salient features, which uses the image gradi-
ent to identify points in the image with two orthogonal
gradients of signiﬁcant magnitude. An example of the
detected corners is shown in Figure 4.
After identifying salient features in one frame of the
video we now need to identify their respective position
in the subsequent frames of the video. In general, there
are two choices for identifying the corresponding salient
features in the subsequent frames of the video. The
ﬁrst choice is to independently detect salient features in
all frames and then compare them by using their image
neighborhoods (patches) to identify correlating patches
through an image based correlation (commonly called
matching). The second class of methods leverages the
small motion occurring in between two frames for an it-
erative search (commonly called tracking).
We opt for a tracking method given that tracking re-
sults for video are superior in accuracy and precision
to matching results. Speciﬁcally, we deploy the well
known KLT-tracking method [28], which is based on the
assumption that the image of a scene object has a con-
stant appearance in the different frames capturing the
object (brightness constancy). The MIOR captchas by
NuCaptcha use constant colors on the characters of the
codewords. This implies that the NuCaptcha frames are
Figure 3: High-level overview of our attack. (This, and other
ﬁgures, are best viewed in color.)
A basic overview of our attack is shown in Figure 3.
Given a MIOR captcha we extract the motion contained
in the video using the concept of salient features. Salient
features are characteristic areas of an image that can be
reliably detected in several frames. To infer the motion of
the salient feature points, we apply object tracking tech-
niques (stage ). With a set of salient features at hand,
we then use these features to estimate the color statis-
tics of the background. Speciﬁcally, we use a Gaussian
mixture model [18], which represents the color statistics
of the background through a limited set of Gaussian dis-
tributions. We use the color model of the background
to measure, for all pixels in each frame, their likelihood
of belonging to the background. Pixels with low likeli-
hoods are then extracted as foreground pixels (stage ).
The trajectories of the foreground pixels are then reﬁned
using information inferred about the color of these pix-
els, and a foreground color model is built. Next, to ac-
count for the fact that all characters of the codewords
move independently, we segment the foreground into n
Decoding Process❶trackingvideo streamforeground extractionsegmentationclassification❷❸❹❺feedbackwell suited for our method. Note that no assumption
about the speciﬁc color is made; only constant appear-
ance of each of the salient features is assumed. We return
to this assumption later in Section 5.2.
Motion Trajectory Clustering (Stage )
In a typical video, the detected salient features will be
spread throughout the image. In the case of NuCaptcha,
the detected features are either on the background, the
plain (i.e., non-codeword) characters or the codeword
characters. We are foremost interested in obtaining the
information of the codeword characters. To identify the
codeword characters we use their distinctive motion pat-
terns as their motion is the most irregular motion in the
video captcha. In the case of NuCaptcha, we take advan-
tage of the fact that the motion trajectories of the back-
ground are signiﬁcantly less stable (i.e., across consec-
utive frames) than the trajectories of the features on the
characters. Hence we can identify background features
by ﬁnding motion trajectories covering only a fraction of
the sequence; speciﬁcally we assume presence for less
than l = 20 frames. In our analysis, we observed little
sensitivity with respect to l.
Additionally, given that all characters (plain and code-
word) move along a common trajectory, we can further
identify this common component by linearly ﬁtting a tra-
jectory to their path. Note that the centers of the rotating
codeword characters still move along this trajectory. Ac-
cordingly, we use the distinctive rotation of the codeword
characters to identify any of their associated patterns by
simply searching for the trajectories with the largest de-
viation from the more common motion trajectory. This
identiﬁes the pixels belonging to the codeword charac-
ters as well as the plain characters. Additionally, the
features on the identiﬁed codeword characters allow us
to obtain the speciﬁc color of the codeword characters
without knowing the color a priori (see Figure 5).
Knowing the position of the codeword characters al-
lows us to learn a foreground color model. We use
a Gaussian mixture model for the foreground learning,
which in our case has a single moment corresponding
to the foreground color.1 Additionally, given the above
identiﬁed salient features on the background, we also
learn a Gaussian mixture for the background, thereby
further separating the characters from the background.
At this point, we have isolated the trajectories of code-
word characters, and separated the codewords from the
background (see Figure 6). However, to decide which
salient features on the codeword characters belong to-
gether, we required additional trajectories. To acquire
these, we simply relax the constraint on the sharpness
of corners we care about (i.e., we lower the threshold
for the Harris corner detection algorithm) and rerun the
Figure 5:
(Top): Initial optical ﬂow. (Middle): salient points
with short trajectories in background are discarded. (Lower):
Trajectories on non-codeword characters are also discarded.
KLT-tracking on the new salient features. This yields
signiﬁcantly more trajectories for use by our segmenta-
tion algorithm. Notice how dense the salient features are
in Figure 7. Note also that since the foreground extrac-
tion step provides patches that are not related to the back-
ground, we can automatically generate training samples
for our classiﬁer, irrespective of the various backgrounds
the characters are contained in.
Figure 6: Example foreground extraction.
Figure 7: re-running tracking with a lower threshold on corner
quality: Left: before modiﬁcation. Right: after modiﬁcation.
Segmentation (Stage )
To segment the derived trajectories into groups, we use k-
means clustering [23]. We chose this approach over other
considerations (e.g., mean-shift [37] based clustering, or
RANSAC [17] based clustering [51]) because of its sim-
plicity, coupled with the fact that we can take advantage
of our knowledge of the desired number of characters
(i.e., k), and use that to help guide the clustering proce-
dure. We cannot, however, apply the standard k-means
approach directly since it relies on Euclidean distances,
where each sample is a point. In our case, we need to take
the relationship between frames of the video sequence
into consideration, and so we must instead use each tra-
jectory as an observation. That is, we cluster the differ-
ent trajectories. However, this results in a non-Euclidean
space because different trajectories have different begin-
ning and ending frames. To address this problem, we
utilize the rigidity assumption [42] and deﬁne a distance
metric for trajectories that takes into consideration their
spatial distance, as well as the variation of their spatial
distance. The result is a robust technique that typically
converges within 5 iterations when k = 3, and 20 intera-
tions (on average) when k = 23. A sample output of this
stage is shown in Figure 8.
Figure 8: Left: before segmentation. Right: trajectories are
marked with different colors and bounding boxes are calculated
based on the center of the trajectories and the orientation of the
points. The red points denote areas with no trajectories.
4.3 Codeword Extraction and
Classiﬁcation (Stage )
Given the center and orientation of each codeword char-
acter, the goal is to ﬁgure out exactly what that character
is. For this task, we extract a ﬁxed-sized area around
each character (as in Figure 8), and supply that to our
classiﬁcation stage. Before doing so, however, we reﬁne
the patches by deleting pixels that are too close to the
trajectories of adjacent characters.
As mentioned earlier, we use a neural network for clas-
sifying the reﬁned patches. A neural network is a mathe-
matical model or computational model that is inspired by
the structure of a biological neural network. The training
of a neural network is based on the notion of the possi-
bility of learning. Given a speciﬁc task to solve, and a
class of functions, learning in this context means using
a set of observations to ﬁnd a function which solves the
task in some optimal sense.
Optimization: While the process outlined in stages -
 works surprisingly well, there are several opportuni-
ties for improvement. Perhaps one of the most natural
extensions is to utilize a feedback mechanism to boost
recognition accuracy. The idea we pursue is based on
the observation that an adversary can leverage her conﬁ-
dence about what particular patches represent to improve
her overall ability to break the captcha. Speciﬁcally, we
ﬁnd and block the character that we are most conﬁdent
about. The basic idea is that although we may not be able
to infer all the characters at once, it is very likely that we
can infer some of the characters. By masking the char-
acter that we are most conﬁdent about, we can simplify
the problem into one of decoding a codeword with fewer
characters; which is easier to segment and recognize.
Figure 9: Iterative decoding of a captcha.
The most conﬁdent character can be found using the
probability score provided by the classiﬁer, although it