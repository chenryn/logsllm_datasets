(a) Increase in number of re-
mote tasks
(b) Reduction in degraded
read time
(c) Reduction in MapReduce
runtime
(d) Reduction in MapReduce
runtime (extreme case)
Figure 8. Comparisons of BDF and EDF in homogeneous and heterogeneous clusters and in an extreme case.
degraded tasks to different racks to prevent
competing for network resources.
them from
Figure 8(c) shows the reduction of MapReduce runtime
of BDF and EDF over LF. BDF achieves 32.3% and 24.4% of
runtime saving in homogeneous and heterogeneous clusters,
respectively, while the savings of EDF are 34.0% and 27.9%,
respectively.
Although EDF does not signiﬁcantly reduce MapReduce
runtime compared to BDF, we ﬁnd that the EDF remains
robust even in an extreme case. We consider a cluster
conﬁguration that
is the same as the homogeneous one
except that ﬁve of the nodes are bad and have much worse
processing power, such that the processing times of a local
map task are 3s for regular nodes and 30s for the bad nodes.
We run a map-only job over a ﬁle with 150 blocks stored
in the cluster. We then compare BDF and EDF in failure
mode, where one of the normal nodes fails. Figure 8(d)
shows reduction of MapReduce runtime of BDF and EDF
compared to LF over 30 runs in the extreme case. BDF only
reduces the MapReduce runtime by 11.7% on average, while
EDF can make an average reduction of 32.6%. We note that
EDF has 36.1% fewer of remote tasks and 34.6% less of
degraded read time on average than BDF (not shown in the
ﬁgure). This shows the importance of locality preservation
and rack awareness to make degraded-ﬁrst scheduling robust
in general and even extreme scenarios.
VI. EXPERIMENTS
We implement degraded-ﬁrst scheduling by modifying
the source code of Hadoop 0.22.0. We run MapReduce on
HDFS-RAID [18], which extends HDFS to support erasure-
coded storage. We conduct testbed experiments and compare
enhanced degraded-ﬁrst scheduling (EDF) with Hadoop’s
default locality-ﬁrst scheduling (LF).
We run experiments on a small-scale Hadoop cluster
testbed composed of a single master node and 12 slave
nodes. The 12 slaves are grouped into three racks with four
slaves each. The slaves in the same rack are connected via
a 1Gbps top-of-rack switch, and the top-of-rack switches
are connected via a 1Gbps core switch. Each of the master
and slave nodes runs Ubuntu 12.04 on an Intel Core i5-
3570 3.40GHz quad-core CPU, 8GB RAM, and a Seagate
ST1000DM003 7200RPM 1TB SATA disk.
Our experiments consider three I/O-heavy MapReduce
jobs, all of which run over a collection of text ﬁles.
• WordCount: It counts the occurrences of each word.
The map tasks tokenize the words in text ﬁles and emit
each word and its local count to the reduce tasks, which
sum up the local counts for each word and write the
results to HDFS.
• Grep: It searches for lines containing a given word. The
map tasks scan through the text ﬁles and emit the lines
containing the given word to the reduce tasks, which
aggregate and write the lines to HDFS.
• LineCount: It counts the occurrences of each line. It
works like WordCount, and shufﬂes more lines than
Grep from the map tasks to the reduce tasks.
We conﬁgure the HDFS block size as 64MB and use
a (12,10) erasure code to provide failure-tolerance. The
blocks are placed in the slaves in a round-robin manner
for load balancing. Each slave has four map slots and one
reduce slot. We set the number of reduce tasks to eight for
each MapReduce job. We then generate 15GB of plain text
data from the Gutenberg website [17]. The data is divided
into 240 blocks and written to HDFS. Then HDFS-RAID
transforms the replicated data of HDFS into erasure-coded
data. Then the 240 native blocks are evenly placed in the 12
slaves, each containing 20 blocks. We consider a single-node
failure, which we simulate by erasing data in one randomly
picked node and killing the slave daemon there.
We evaluate the MapReduce runtime, deﬁned as the
time interval between the launch of ﬁrst map task and the
completion of the last reduce task. The results are averaged
over ﬁve runs.
We ﬁrst consider a single-job scenario, in which we run
each of the three jobs over the input data. Figure 9(a)
shows the runtime of each job in a single job scenario. EDF
reduces the MapReduce runtime of LF by 27.0%, 26.1% and
24.8% for WordCount, Grep, and LineCount, respectively.
We observe that LF has a larger runtime variance than
EDF, mainly because it does not consider rack-awareness.
This causes the number of degraded tasks assigned to
428428428
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:03:27 UTC from IEEE Xplore.  Restrictions apply. 
e
c
u
d
e
R
p
a
M
)
s
(
e
m
i
t
n
u
r
e
c
u
d
e
R
p
a
M
)
s
(
e
m
i
t
n
u
r
 350
 300
 250
 200
 150
 100
 50
 0
 350
 300
 250
 200
 150
 100
 50
 0
wordcount grep
linecount
LF
EDF
(a) Single-job scenario
wordcount grep
linecount
LF
EDF
(b) Multi-job scenario
Figure 9. Comparisons of MapReduce runtime in single-job and multi-
job scenarios. The minimum and maximum runtimes are also plotted as the
endpoints of the vertical line in each bar.
different racks to become unbalanced, and hence increases
the runtime variance.
We also consider a multi-job scenario, in which we submit
the three MapReduce jobs in the order of WordCount, Grep,
and LineCount in a short time so that the three jobs are
scheduled by Hadoop in a ﬁrst-in-ﬁrst-out order. Figure 9(b)
shows the runtime results. EDF can reduce the MapReduce
runtime by 16.6%, 28.4%, and 22.6% for WordCount, Grep,
and LineCount, respectively. We note that WordCount has
the least runtime reduction. The reason is that EDF launches
the degraded tasks of a job while the reduce tasks of the
previous job are still downloading the intermediate data
generated by the map tasks. This leads to competition for
network resources and delays the completion of the previous
job. Nevertheless, our results demonstrate the improvements
of EDF over LF in both single-job and multi-job scenarios.
Table I provides a breakdown analysis of task runtime
of different jobs in the single-job scenario. We compare
the average runtime of normal map tasks (local and remote
tasks), degraded tasks and reduce tasks. The runtime of a
task is deﬁned as the time interval between its launch and
completion. It includes data transmission time (for remote
and degraded tasks) as well as the data processing time.
We see that EDF reduces the average runtime of degraded
tasks compared to LF, by 43.0%, 34.6%, and 47.7% for
WordCount, Grep, and LineCount, respectively. Since EDF
reduces the average runtime of the overall map phase, the
429429429
average runtime of reduce tasks is also reduced, by around
26%. The normal tasks have a similar average runtime in
both LF and EDF, so EDF does not affect the processing of
normal tasks.
VII. RELATED WORK
There have been extensive empirical studies on examining
the practical use of erasure coding in clustered storage
systems (e.g., [1, 5, 10, 12, 13, 20, 22, 23, 25, 29, 33, 37]).
DiskReduce [12] extends HDFS to encode replicated data
with erasure coding ofﬂine. Zhang et al. [37] further imple-
ment an online encoding framework for HDFS and study
various MapReduce workloads on erasure-coded HDFS. In
addition, several studies focus on enhancing the degraded
read performance in erasure-coded clustered storage systems
under failure mode. Khan et al. [22] present an algorithm
that minimizes disk I/Os for single failure recovery for
arbitrary erasure codes. New erasure code constructions
are proposed and evaluated on Azure [20] and HDFS
[10, 23, 25, 29]. Our work complements them by designing a
proper task scheduling algorithm to improve the MapReduce
performance in failure mode.
Our work aims to enhance the baseline Hadoop MapRe-
duce design, and this objective is also shared by previous
work. For example, authors of [2, 14, 36] propose new task
scheduling algorithms for heterogeneous clusters so as to
prevent a MapReduce job from being delayed by stragglers.
Authors of [31, 34, 35] propose fair task scheduling algo-
rithms for MapReduce on multi-user clusters, and mitigate
the resource starvation of small jobs in the presence of
large jobs. Besides scheduling, some studies propose to
modify the default block placement policy of HDFS, so as
to improve data availability [7] and write performance [6].
Such HDFS/MapReduce enhancements focus on replication-
based storage, while ours target erasure-coded storage.
VIII. CONCLUSIONS
This paper explores the feasibility of running data analyt-
ics in erasure-coded clustered storage systems. We present
degraded-ﬁrst scheduling, a new MapReduce scheduling
scheme designed for improving MapReduce performance in
erasure-coded clustered storage systems that run in failure
mode. We show that the default locality-ﬁrst scheduling
launches degraded tasks at the end, thereby making them
compete for network resources. Degraded-ﬁrst scheduling
launches degraded tasks earlier to take advantage of the
unused network resources. We also propose heuristics that
leverage topological information of the storage system to
improve the robustness of degraded-ﬁrst scheduling. We
conduct simple mathematical analysis and discrete event
simulation to show the performance gains of degraded-ﬁrst
scheduling. We further conduct testbed experiments in a
Hadoop cluster, and show that degraded-ﬁrst scheduling can
reduce the MapReduce runtime of locality-ﬁrst scheduling
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:03:27 UTC from IEEE Xplore.  Restrictions apply. 
Table I
AVERAGE RUNTIME (IN SECONDS) OF DIFFERENT TYPES OF TASKS IN THE SINGLE-JOB SCENARIO.
LineCount
LF
35.91
91.48
273.70
WordCount
LF
30.94
84.97
247.90
EDF
29.12
48.42
182.05
LF
11.69
77.97
161.08
EDF
10.43
50.96
122.60
Normal map
Degraded map
Reduce
EDF
33.25
47.88
199.35
Number
of tasks
220
20
8
Grep
by 27.0% for a single-job scenario and 28.4% for a multi-
job scenario. The source code of degraded-ﬁrst scheduling is
available at http://ansrlab.cse.cuhk.edu.hk/software/dfs.
ACKNOWLEDGMENTS
This work was supported in part by grants AoE/E-02/08
and ECS CUHK419212 from the University Grants Com-
mittee of Hong Kong.
REFERENCES
[1] M. Abd-El-Malek, W. Courtright II, C. Cranor, G. Ganger, J. Hen-
dricks, A. Klosterman, M. Mesnier, M. Prasad, B. Salmon, R. Sam-
basivan, et al. Ursa Minor: Versatile Cluster-based Storage. In Proc.
of USENIX FAST, Dec 2005.
[2] G. Ananthanarayanan, S. Kandula, A. G. Greenberg, I. Stoica, Y. Lu,
B. Saha, and E. Harris. Reining in the Outliers in Map-Reduce
Clusters using Mantri. In Proc. of USENIX OSDI, page 14, 2010.
[3] J. Bloemer, M. Kalfane, R. Karp, M. Karpinski, M. Luby, and
D. Zuckerman. An XOR-Based Erasure-Resilient Coding Scheme.
Technical Report TR-95-048, International Computer Science Insti-
tute, UC Berkeley, Aug 1995.
[4] B. Calder, J. Wang, A. Ogus, N. Nilakantan, A. Skjolsvold, S. McK-
elvie, Y. Xu, S. Srivastav, J. Wu, H. Simitci, et al. Windows Azure
Storage: A Highly Available Cloud Storage Service with Strong
Consistency. In Proc. of ACM SOSP, Oct 2011.
[5] J. C. W. Chan, Q. Ding, P. P. C. Lee, and H. H. W. Chan. Parity Log-
ging with Reserved Space: Towards Efﬁcient Updates and Recovery
in Erasure-coded Clustered Storage. In Prof. of USENIX FAST, 2014.
[6] M. Chowdhury, S. Kandula, and I. Stoica. Leveraging Endpoint
Flexibility in Data-Intensive Clusters. In Proc. of ACM SIGCOMM,
2013.
[7] A. Cidon, S. Rumble, R. Stutsman, S. Katti, J. Ousterhout, and
M. Rosenblum. Copysets: Reducing the Frequency of Data Loss in
Cloud Storage. In Proc. of USENIX ATC, 2013.
[8] CSIM. http://www.mesquite.com/products/csim20.htm.
[9] J. Dean and S. Ghemawat. MapReduce: Simpliﬁed Data Processing
on Large Clusters. In Proc. of USENIX OSDI, Dec 2004.
[10] K. S. Esmaili, L. Pamies-Juarez, and A. Datta. CORE: Cross-Object
Redundancy for Efcient Data Repair in Storage Systems. In Proc. of
IEEE BigData, 2013.
[11] B. Fan, W. Tantisiriroj, and G. Gibson. Diskreduce: Replication as
a prelude to erasure coding in data-intensive scalable computing. In
Carnegie Mellon Univsersity, Parallel Data Laboratory, Tech. Rep.
Technical Report CMU-PDL-11-112, 2011.
[12] B. Fan, W. Tantisiriroj, L. Xiao, and G. Gibson. DiskReduce: RAID
for Data-Intensive Scalable Computing. In Proc. of Annual Workshop
on Petascale Data Storage (PDSW), Nov 2009.
[13] D. Ford, F. Labelle, F. I. Popovici, M. Stokel, V.-A. Truong, L. Bar-
roso, C. Grimes, and S. Quinlan. Availability in Globally Distributed
Storage Systems. In Proc. of USENIX OSDI, Oct 2010.
[14] R. Gandhi, D. Xie, and Y. C. Hu. PIKACHU: How to Rebalance Load
in Optimizing MapReduce On Heterogeneous Clusters. In Proc. of
USENIX ATC, 2013.
[15] J. Gantz and D. Reinsel. Extracting Value from Chaos. http://www.
emc.com/digital universe, Jun 2011.
[16] S. Ghemawat, H. Gobioff, and S. Leung. The Google File System.
In Proc. of ACM SOSP, Dec 2003.
[17] Gutenberg. http://www.gutenberg.org.
430430430
[18] HDFS-RAID. http://wiki.apache.org/hadoop/HDFS-RAID.
[19] M. Holland, G. A. Gibson, and D. P. Siewiorek. Architectures
and algorithms for on-line failure recovery in redundant disk arrays.
Distrib. Parallel Databases, 2(3):295–335, July 1994.
[20] C. Huang, H. Simitci, Y. Xu, A. Ogus, B. Calder, P. Gopalan, J. Li,
In
and S. Yekhanin. Erasure Coding in Windows Azure Storage.
Proc. of USENIX ATC, Jun 2012.
[21] M. Isard, M. Budiu, Y. Yu, A. Birrell, and D. Fetterly. Dryad:
Distributed Data-Parallel Programs from Sequential Building Blocks.
In Proc. of ACM EuroSys, Jun 2007.
[22] O. Khan, R. Burns, J. Plank, W. Pierce, and C. Huang. Rethinking
Erasure Codes for Cloud File Systems: Minimizing I/O for Recovery
and Degraded Reads. In Proc. of USENIX FAST, Feb 2012.
[23] R. Li, J. Lin, and P. P. C. Lee. CORE: Augmenting Regenerating-
Coding-Based Recovery for Single and Concurrent Failures in Dis-
tributed Storage Systems. In Proc. of IEEE MSST, May 2013.
[24] D. T. Meyer, M. Shamma, J. Wires, Q. Zhang, N. C. Hutchinson, and
A. Warﬁeld. Fast and Cautious Evolution of Cloud Storage. In Proc.
of USENIX HotStorage, Jun 2010.
[25] D. Papailiopoulos, J. Luo, A. Dimakis, C. Huang, and J. Li. Simple
Regenerating Codes: Network Coding for Cloud Storage. In Proc. of
IEEE INFOCOM, Mar 2012.
[26] J. S. Plank, K. M. Greenan, and E. L. Miller. Screaming fast Galois
Field arithmetic using Intel SIMD instructions. In Proc. of USENIX
FAST, Feb 2013.
[27] J. S. Plank, J. Luo, C. D. Schuman, L. Xu, and Z. OHearn. A per-
formance evaluation and examination of open-source erasure coding
libraries for storage. In Proc. of USENIX FAST, 2009.
[28] I. Reed and G. Solomon. Polynomial Codes over Certain Finite
Fields. Journal of the Society for Industrial and Applied Mathematics,
8(2):300–304, 1960.
[29] M. Sathiamoorthy, M. Asteris, D. Papailiopoulos, A. G. Dimakis,
R. Vadali, S. Chen, and D. Borthakur. Xoring Elephants: Novel
Erasure Codes for Big Data. In Proc. of VLDB Endowment, pages
325–336, 2013.
[30] K. Shvachko, H. Kuang, S. Radia, and R. Chansler. The Hadoop
Distributed File System. In Proc. of IEEE MSST, May 2010.
[31] J. Tan, X. Meng, and L. Zhang. Delay Tails in MapReduce Schedul-
ing. In Proc. of ACM SIGMETRICS, Jun 2012.
[32] H. Weatherspoon and J. D. Kubiatowicz.
Replication: A Quantitative Comparison.
2002.
Erasure Coding Vs.
In Proc. of IPTPS, Mar
[33] B. Welch, M. Unangst, Z. Abbasi, G. Gibson, B. Mueller, J. Small,
J. Zelenka, and B. Zhou. Scalable Performance of the Panasas Parallel
File System. In Proc. of USENIX FAST, Feb 2008.
[34] J. Wolf, D. Rajan, K. Hildrum, R. Khandekar, V. Kumar, S. Parekh,
K.-L. Wu, and A. Balmin. FLEX: A Slot Allocation Scheduling
Optimizer for MapReduce Workloads.
In Middleware 2010, pages
1–20. Springer, 2010.
[35] M. Zaharia, D. Borthakur, J. Sen Sarma, K. Elmeleegy, S. Shenker,
and I. Stoica. Delay Scheduling: A Simple Technique for Achieving
Locality and Fairness in Cluster Scheduling.
In Proc. of ACM
EuroSys, pages 265–278, 2010.
[36] M. Zaharia, A. Konwinski, A. D. Joseph, R. H. Katz, and I. Stoica.
Improving MapReduce Performance in Heterogeneous Environments.
In Proc. of USENIX OSDI, 2008.
[37] Z. Zhang, A. Deshpande, X. Ma, E. Thereska, and D. Narayanan.
Does Erasure Coding Have a Role to Play in my Data Center?
Technical Report MSR-TR-2010-52, Microsoft Research, May 2010.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:03:27 UTC from IEEE Xplore.  Restrictions apply.