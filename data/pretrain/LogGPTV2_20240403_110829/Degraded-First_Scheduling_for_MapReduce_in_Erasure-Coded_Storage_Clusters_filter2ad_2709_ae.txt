### Optimized Text

#### Figure 8: Comparisons of BDF and EDF in Homogeneous and Heterogeneous Clusters and in an Extreme Case

- **(a) Increase in the number of remote tasks**
- **(b) Reduction in degraded read time**
- **(c) Reduction in MapReduce runtime**
- **(d) Reduction in MapReduce runtime (extreme case)**

**Figure 8.** This figure compares the performance of BDF (Baseline Degraded-First) and EDF (Enhanced Degraded-First) scheduling algorithms in homogeneous and heterogeneous clusters, as well as in an extreme case.

**Figure 8(c)** illustrates the reduction in MapReduce runtime for BDF and EDF compared to LF (Locality-First). In homogeneous clusters, BDF achieves a 32.3% runtime reduction, while EDF achieves a 34.0% reduction. In heterogeneous clusters, BDF reduces the runtime by 24.4%, and EDF by 27.9%.

Although EDF does not significantly outperform BDF in terms of reducing MapReduce runtime, it remains robust even in extreme scenarios. We consider a cluster configuration that is identical to the homogeneous one, except that five nodes are severely underpowered, with local map task processing times of 3 seconds for regular nodes and 30 seconds for the underpowered nodes. A map-only job is executed over a file with 150 blocks stored in the cluster. The results, shown in **Figure 8(d)**, compare BDF and EDF in a failure mode where one of the normal nodes fails. Over 30 runs, BDF reduces the MapReduce runtime by an average of 11.7%, while EDF achieves an average reduction of 32.6%. Additionally, EDF has 36.1% fewer remote tasks and 34.6% less degraded read time on average than BDF (not shown in the figure). This highlights the importance of locality preservation and rack awareness in making degraded-first scheduling robust, even in extreme scenarios.

### VI. Experiments

We implemented degraded-first scheduling by modifying the source code of Hadoop 0.22.0. The experiments were conducted using HDFS-RAID, which extends HDFS to support erasure-coded storage. We compared enhanced degraded-first scheduling (EDF) with Hadoopâ€™s default locality-first scheduling (LF).

**Cluster Configuration:**
- **Testbed:** A small-scale Hadoop cluster with one master node and 12 slave nodes.
- **Rack Configuration:** The 12 slaves are grouped into three racks, each containing four slaves.
- **Network:** Slaves within the same rack are connected via a 1Gbps top-of-rack switch, and the top-of-rack switches are connected via a 1Gbps core switch.
- **Hardware Specifications:**
  - **Nodes:** Ubuntu 12.04, Intel Core i5-3570 3.40GHz quad-core CPU, 8GB RAM, Seagate ST1000DM003 7200RPM 1TB SATA disk.

**Experiments:**
- **Jobs:** Three I/O-heavy MapReduce jobs were run over a collection of text files:
  - **WordCount:** Counts the occurrences of each word. Map tasks tokenize words and emit them with their local counts to reduce tasks, which sum up the counts and write the results to HDFS.
  - **Grep:** Searches for lines containing a given word. Map tasks scan through the text files and emit the matching lines to reduce tasks, which aggregate and write the lines to HDFS.
  - **LineCount:** Counts the occurrences of each line. Similar to WordCount, but shuffles more lines from map to reduce tasks.

**Configuration:**
- **HDFS Block Size:** 64MB
- **Erasure Code:** (12,10)
- **Block Placement:** Round-robin for load balancing
- **Map Slots per Slave:** 4
- **Reduce Slots per Slave:** 1
- **Number of Reduce Tasks:** 8
- **Data:** 15GB of plain text data from the Gutenberg website, divided into 240 blocks and written to HDFS. HDFS-RAID then transforms the replicated data into erasure-coded data, evenly distributed across the 12 slaves (20 blocks per slave).
- **Failure Simulation:** A single-node failure was simulated by erasing data in one randomly picked node and killing the slave daemon there.

**Evaluation Metrics:**
- **MapReduce Runtime:** Time interval between the launch of the first map task and the completion of the last reduce task, averaged over five runs.

**Results:**

**Single-Job Scenario:**
- **Figure 9(a):** EDF reduces the MapReduce runtime of LF by 27.0%, 26.1%, and 24.8% for WordCount, Grep, and LineCount, respectively. LF shows a larger runtime variance due to its lack of rack-awareness, leading to unbalanced distribution of degraded tasks across different racks.

**Multi-Job Scenario:**
- **Figure 9(b):** EDF reduces the MapReduce runtime by 16.6%, 28.4%, and 22.6% for WordCount, Grep, and LineCount, respectively. The smaller reduction for WordCount is attributed to the competition for network resources when EDF launches degraded tasks while the previous job's reduce tasks are still downloading intermediate data. Despite this, EDF consistently outperforms LF in both single-job and multi-job scenarios.

**Table I: Breakdown Analysis of Task Runtimes in the Single-Job Scenario**

| Job         | Scheduling | Normal Map (s) | Degraded Map (s) | Reduce (s) |
|-------------|------------|----------------|------------------|------------|
| **LineCount** | LF         | 35.91          | 91.48            | 273.70     |
|             | EDF        | 33.25          | 47.88            | 199.35     |
| **WordCount** | LF         | 30.94          | 84.97            | 247.90     |
|             | EDF        | 29.12          | 48.42            | 182.05     |
| **Grep**    | LF         | 11.69          | 77.97            | 161.08     |
|             | EDF        | 10.43          | 50.96            | 122.60     |

**Conclusion:**
- EDF reduces the average runtime of degraded tasks by 43.0%, 34.6%, and 47.7% for WordCount, Grep, and LineCount, respectively. This leads to a 26% reduction in the average runtime of reduce tasks. Normal tasks have similar runtimes in both LF and EDF, indicating that EDF does not negatively impact the processing of normal tasks.

### VII. Related Work

Extensive empirical studies have examined the practical use of erasure coding in clustered storage systems. DiskReduce [12] extends HDFS to encode replicated data with erasure coding offline. Zhang et al. [37] further implement an online encoding framework for HDFS and study various MapReduce workloads on erasure-coded HDFS. Several studies focus on enhancing degraded read performance in erasure-coded storage systems under failure modes. Khan et al. [22] present an algorithm to minimize disk I/Os for single failure recovery. New erasure code constructions have been proposed and evaluated on Azure [20] and HDFS [10, 23, 25, 29]. Our work complements these by designing a task scheduling algorithm to improve MapReduce performance in failure modes.

Our objective is to enhance the baseline Hadoop MapReduce design, a goal shared by previous work. For example, Ananthanarayanan et al. [2], Kandula et al. [14], and Zaharia et al. [36] propose new task scheduling algorithms for heterogeneous clusters to prevent delays caused by stragglers. Tan et al. [31], Wolf et al. [34], and Zaharia et al. [35] propose fair task scheduling algorithms for multi-user clusters to mitigate resource starvation for small jobs. Other studies modify HDFS's default block placement policy to improve data availability [7] and write performance [6]. While these enhancements focus on replication-based storage, our work targets erasure-coded storage.

### VIII. Conclusions

This paper explores the feasibility of running data analytics in erasure-coded clustered storage systems. We introduce degraded-first scheduling, a new MapReduce scheduling scheme designed to improve performance in erasure-coded storage systems under failure modes. The default locality-first scheduling launches degraded tasks at the end, leading to network resource competition. In contrast, degraded-first scheduling launches degraded tasks earlier, utilizing unused network resources. We also propose heuristics that leverage the topological information of the storage system to enhance the robustness of degraded-first scheduling. Our mathematical analysis, discrete event simulation, and testbed experiments in a Hadoop cluster show that degraded-first scheduling can reduce the MapReduce runtime of locality-first scheduling by 27.0% in a single-job scenario and 28.4% in a multi-job scenario. The source code for degraded-first scheduling is available at http://ansrlab.cse.cuhk.edu.hk/software/dfs.

### Acknowledgments

This work was supported in part by grants AoE/E-02/08 and ECS CUHK419212 from the University Grants Committee of Hong Kong.

### References

[References remain the same as provided in the original text.]

---

This optimized version aims to improve clarity, coherence, and professionalism, ensuring that the content is more accessible and easier to understand.