objects. As a consequence, we identify a trade-off between
security and performance in image scaling.
Existing scaling algorithms. Based on the concept of an
ideal algorithm, we examine the source code of the three con-
sidered imaging libraries and analyze their scaling algorithms
with respect to the processed pixels and the employed con-
volution kernels. In particular, we inspect the source code
of OpenCV version 4.1, Pillow 6.0, and tf.image 1.14 from
TensorFlow. Table 3 shows the results of this investigation.
Table 3: Kernel width σ for the scaling algorithms implemented by the
imaging libraries OpenCV, tf.image (TensorFlow) and Pillow.
Library
Nearest
Bilinear
Bicubic
Lanczos
Area
OpenCV
1
2
4
8
β
TF
1
2
4
—
β
Pillow
1
2· β
4· β
6· β
β
We observe that several scaling algorithms are imple-
mented with ﬁxed-size convolution kernels. For example,
OpenCV and TensorFlow implement nearest-neighbor, bilin-
ear, and bicubic scaling with a kernel width of 1, 2, and 4,
respectively. Consequently, these algorithms become vulnera-
ble once the scaling ratio exceeds the kernel width, and pixels
of the source image are omitted during scaling.
Fortunately, however, we also identify one algorithm that
is implemented with a dynamic kernel width of β in all frame-
works: area scaling. This algorithm scales an image by
simply computing the average of all pixels under the ker-
nel window, which corresponds to a uniform convolution, as
shown in Figure 6 for β = 4. Moreover, area scaling cor-
responds to a low-pass ﬁlter which mitigates the aliasing
effect. As a result, area scaling provides strong protection
from image-scaling attacks, and the algorithm is a reasonable
defense if the uniform weighting of the convolution does not
impact later analysis steps. We demonstrate the robustness of
area scaling in our empirical evaluation in Section 5.
Our analysis provides another interesting ﬁnding: Pillow
stands out from the other imaging library, as it implements
a dynamic kernel width for all algorithms except for nearest-
neighbor scaling. The dynamic kernel width σ is chosen such
that the convolution windows substantially overlap, for exam-
ple, for bicubic and Lanczos scaling by a factor of 4 and 6,
respectively. Although the used convolutions are not uniform
for these algorithms, this overlap creates a notable obstruc-
tion for the attacker, as dependencies between the overlapping
windows need to be compensated. Figure 8 schematically
shows the dynamic kernel width of Pillow in comparison to
the implementations of OpenCV and TensorFlow.
Disadvantages. While area scaling and the Pillow library
provide a means for robust scaling, they also induce draw-
backs. As exempliﬁed in Figure 9, the algorithms cannot
entirely remove all traces from the attacks. Small artifacts
can remain, as the manipulated pixels are not cleansed and
still contribute to the scaling, though with limited impact.
Our evaluation shows that these remnants are not enough to
fool the neural network anymore. The predicted class for the
scaled images, however, is not always correct due to the noise
of the attack remainings. As a remedy, we develop an alterna-
tive defense in the next section that reconstructs the source
image and thus is applicable to any scaling algorithm. This
reconstruction removes attack traces, and thus the classiﬁer
predicts the original class again.
Figure 8: Comparison of bilinear scaling for Pillow, OpenCV and TensorFlow.
The latter two ﬁx σ to 2, while Pillow uses a dynamic kernel width.
USENIX Association
29th USENIX Security Symposium    1369
12345678910CV/TFKernelPillowKernelxFigure 9: Comparison of scaling algorithms: (a) insecure nearest-neighbor
scaling, (b) robust area scaling, and (c) robust scaling from Pillow. Note the
visible attack traces in (b) and (c).
Figure 11: Examples of our defense: (a) insecure nearest-neighbor scaling,
(b) robust scaling using a median ﬁlter, and (c) a random ﬁlter. Note that
attack traces are not visible anymore.
4.3 Defense 2: Image Reconstruction
We construct our defense around the main working principle
of image-scaling attacks: The attacks operate by manipu-
lating a small set of pixels that controls the scaling process.
With knowledge of the scaling algorithm, we can precisely
identify this set of pixels in the attack image. The naive de-
fense strategy to remove this set effectively blocks any attack,
yet it corrupts the scaling, as all relevant pixels are removed.
Instead, we ﬁrst identify all pixels processed by a scaling algo-
rithm and then reconstruct their content using the remaining
pixels of the image.
Reconstructing pixels in images is a well-known problem
in image processing, and there exist several methods that
provide excellent performance in practice, such as techniques
based on wavelets and shearlets [e.g., 26, 30]. These involved
approaches, however, are difﬁcult to analyze from a security
perspective, and their robustness is hard to assess. Hence, we
propose two simple reconstruction methods for the considered
pixels that possess transparent security properties: a selective
median ﬁlter and a selective random ﬁlter.
Selective median ﬁlter. Given a scaling algorithm and a
target size, our ﬁlter identiﬁes the set of pixels P in the input
image that is processed during scaling. For each of the pixels
p ∈ P , it determines a window Wp around p, similar to a
convolution kernel, and computes the median pixel value for
this window. To make the computation robust, we deﬁne the
size of this window as 2βh × 2βv, which ensures that half of
the pixels overlap between the different windows and thus
hinders existing scaling attacks. Furthermore, we take care
of other manipulated points p(cid:48) ∈ P in Wp and exclude them
from the computation of the median. Figure 10 depicts the
basic principle of our selective median ﬁlter.
Figure 10: Image reconstruction using a selective median ﬁlter. Around
each point p that is considered by the downscaling algorithm (red), we take
the median of all values in a window around it (green), except for other
candidates that are present in the window.
In comparison to other approaches for reconstructing the
content of images, this defense builds on the statistical ro-
bustness of the median operation. Small groups of pixels
with high or low values are compensated by the median. On
average, the adversary is required to change about 50% of
the pixels in a window to reach a particular target value for
the median. Our evaluation demonstrates that non-adaptive
as well as adaptive adversaries are not capable of effectively
manipulating these median values without introducing strong
visible artifacts (see Section 5).
The robustness of the median ﬁlter comes at a price: Com-
puting the median for all pixels in each window Wp for all
p ∈ P yields a run-time complexity of O(|P|· βh · βv). That
is, the run-time growths quadratically with the scaling ra-
tio. While this overhead might be neglectable when working
with large neural networks, there also exist applications in
which more efﬁcient scaling is necessary. Providing secure
and efﬁcient scaling, however, is a challenging task, as the
robustness of a scaling algorithm increases with the number
of considered pixels.
Selective random ﬁlter. To tackle the problem of efﬁciency,
we also propose a selective random ﬁlter that takes a random
point from each window instead of the median. This ﬁlter
is suitable for applications that demand a very efﬁcient run-
time performance and might tolerate a loss in visual quality.
Appendix B outlines the ﬁlter in more detail.
In summary, we present two defenses that target the core
of image-scaling attacks. As exempliﬁed by Figure 11, both
restore the pixels that an adversary changes and prevent the
attacks. These defenses can be easily used in front of existing
scaling algorithms, such that almost no changes are necessary
to the typical workﬂow of machine learning systems.
5 Evaluation
We continue with an empirical evaluation of our defenses
against image-scaling attacks. In Section 5.2 and 5.3, we
study the security of robust scaling algorithms (Defense 1). In
Section 5.4 and 5.5, we examine our novel defense based on
image reconstruction (Defense 2). For each defense, we start
the evaluation with a non-adaptive adversary that performs
regular image-scaling attacks and then proceed to investigate
an adaptive adversary who tries to circumvent our defenses.
1370    29th USENIX Security Symposium
USENIX Association
(a)Nearest(b)Area(c)PillowPixelp∈Pp0∈PWp(a)Nearest(b)Medianﬁlter(c)Randomﬁlter5.1 Experimental Setup
To evaluate the efﬁcacy of our defenses, we consider the
objectives O1 and O2 of image-scaling attacks presented in
Section 2.2.3. If a defense is capable of impeding one of these
objectives, the attack fails. For example, if the control of the
adversary over the source is restricted, such that the classiﬁ-
cation of the scaled version is not changed, the defense has
foiled O1. Similarly, if the embedded target image becomes
clearly visible, the defense has thwarted O2. Consequently,
we design our experiments along with these two objectives.
Dataset & Setup. We use the ImageNet dataset [25] with a
pre-trained VGG19 model [28] for our evaluation. This deep
neural network is a standard benchmark in computer vision
and expects input images of size 224× 224× 3. From the
dataset, we randomly sample 600 images as an unmodiﬁed
reference set and 600 source images for conducting attacks.
For each source image, we randomly select a target image
from the dataset, ensuring that both images have different
classes and predictions. As we are interested in investigating
different scaling ratios, we sample the images such that we
obtain 120 images for each of the following ﬁve intervals
of ratios: [2,3), [3,4), [4,5), [5,7.5), [7.5,10). Since we have
two ratios along the vertical and horizontal direction for each
image, we consider the minimum of both for this assignment.
We implement image-scaling attacks in the strong variant
proposed by Xiao et al. [35]. We make a slight improvement
to the original attacks: Instead of using a ﬁxed ε value, we
increase its value gradually from 1 up to 50 if the quadratic
programming solver cannot ﬁnd a solution. During our eval-
uation, we observe that single columns or rows may require
a larger ε to ﬁnd a feasible solution. In this way, we can
increase the attack’s success rate, if only a single part of an
image requires a higher ε value.
As scaling algorithms, we consider the implementations
of nearest-neighbor, bilinear, bicubic, and area scaling from
the libraries OpenCV (version 4.1), Pillow (version 6.0), and
tf.image (version 1.13) from TensorFlow. We omit the Lanc-
zos algorithm, as it provides comparable results to bicubic
scaling in our experiments due to the similar convolution
kernel and kernel width (see Figure 6).
Evaluation of O1: Predictions using VGG19. To assess
objective O1 of the attacks, we check if the deep neural net-
work VGG19 predicts the same class for the scaled image
scale(A) and the target image T . As there are typically minor
ﬂuctuations in the predicted classes when scaling with differ-
ent ratios, we apply the commonly used top-5 accuracy. That
is, we check if a match exists between the top-5 predictions
for the target image T and the scaled image scale(A).
Evaluation of O2: User Study. To investigate objective O2,
we conduct user studies with 36 human subjects. The group
consists of female and male participants with different profes-
sional background. The participants obtain 3 attack images
for each interval of scaling ratio and are asked to visually iden-
tify one or more of three classes, where one class corresponds
to the source image, one to the embedded target image and the
third to an unrelated class. We consider an attack successful,
if a participant selects the class of the source image only and
does not notice the target image.
Evaluation of O2: PSNR. As quantitative measurement, we
additionally use the Peak Signal to Noise Ratio (PSNR), a
common metric in image processing [8], to measure the differ-
ence between the unmodiﬁed source image and the attacked
image. Formally, the PSNR for the attack image A and the
source image S is deﬁned as
(cid:32)
(cid:33)
I2
max
N (cid:107) A− S (cid:107)2
2
PSNR(A,S) = 10 log10
1
.
(10)
The denominator represents the mean squared error between
both images with N as the total number of pixels, and Imax as
the maximum of the pixel range. A high PSNR value (larger
than 25 dB) indicates a strong match between two images.
As a conservative choice, we consider the attack unsuccessful
if the PSNR value is below 15 dB. We also experimented
with more advanced methods for comparing the quality of
images, such as feature matching based on SIFT analysis [16].
This technique, however, shows the same trends as the simple
PSNR measurement, and thus we omit these measurements.
5.2 Defense 1: Non-Adaptive Attack
In our ﬁrst experiment, we examine the robustness of existing
scaling algorithms from OpenCV, TensorFlow, and Pillow
against image-scaling attacks. Note that we investigate area
scaling in the following Section 5.3, as it is not vulnerable to
standard image-scaling attacks.
Evaluation O1. Figure 12 shows the performance of the
attack as the ratio of classiﬁcations with the wanted target
class after scaling. The attack is successful with respect to
O1 for all scaling algorithms from OpenCV, TensorFlow, and
Pillow. An exception is Pillow’s bilinear scaling where the
success rate is 87%, as a feasible solution is not found for
all source and target pairs here. Overall, our results conﬁrm
that an attacker can successfully manipulate an image such
that its scaled version becomes a target image, irrespective of
the scaling algorithm or library. This manipulation, however,
is not sufﬁcient for a successful attack in practice, as visual
traces may clearly indicate the manipulation and undermine
the attack. We thus also evaluate O2 in this experiment.
Evaluation O2. Figure 13 shows the results from our user
study investigating the visual perception of the generated at-
tack images. In line with our theoretical analysis, the attack
is successful against OpenCV and TensorFlow, once a certain
USENIX Association
29th USENIX Security Symposium    1371
Figure 12: Success rate of image-scaling attacks with respect to objective
O1: the number of classiﬁcations with target class after scaling.
scaling ratio is reached (red bars in Figure 13). We observe
that for ratios exceeding 5, most attack images are not de-
tected by the participants. However, for the implementations
of bilinear and bicubic scaling in the Pillow library, the partic-
ipants always spot the attack and identify the embedded target
class in the source image. This result conﬁrms our analysis
of the implementations in Section 4.2 and the vital role of the
dynamic kernel width used by Pillow.
In addition, Figure 19 in Appendix D reports the PSNR
values between the attack and source image over the entire
dataset. We observe the same trend as in the user study. For
OpenCV and TensorFlow, the images become similar to each
other with a larger β, reaching PSNR values above 25 dB.
Summary. We can conﬁrm that image-scaling attacks are
effective against several scaling algorithms in popular imag-
ing libraries. The attacks succeed in crafting images that are
classiﬁed as the target class. However, the visibility of the at-
tacks depends on the scaling ratio and the kernel width. In the
case of Pillow, the attack fails for bilinear, bicubic, and Lanc-
zos scaling to hide the manipulations from a human viewer.
We thus conclude that these implementations of scaling al-
gorithms can be considered robust against a non-adaptive
adversary in practice.
5.3 Defense 1: Adaptive Attacks
In our second experiment, we consider an adaptive adversary
that speciﬁcally seeks means for undermining robust scaling.
To this end, we ﬁrst attack the implementation of the Pillow
library (Section 5.3.1) and then construct attacks against area
scaling in general (Section 5.3.2 and 5.3.3).
5.3.1 Attacking the Pillow Library
Our analysis shows that image-scaling attacks fail to satisfy
objective O2 when applied to the Pillow library. The dynamic
kernel width forces the attack to aggressively change pixels
in the source, such that the target image becomes visible.