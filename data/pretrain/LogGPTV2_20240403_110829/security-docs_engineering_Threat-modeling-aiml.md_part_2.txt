In this case attackers generate a sample that is not in the input class
of the target classifier but gets classified by the model as that
particular input class. The adversarial sample can appear like random
noise to human eyes but attackers have some knowledge of the target
machine learning system to generate a white noise that is not random but
is exploiting some specific aspects of the target model. The adversary
gives an input sample that is not a legitimate sample, but the target
system classifies it as a legitimate class.
#### Examples
![A diagram showing that a photo of targeted noise is incorrectly classified by an image classifier resulting in a photo of a bus.](./media/threat-modeling-aiml/tm2.jpg)[6]
#### Mitigations
- Reinforcing Adversarial Robustness using Model Confidence Induced by
    Adversarial Training [19]: The authors propose Highly Confident Near
    Neighbor (HCNN), a framework that combines confidence information
    and nearest neighbor search, to reinforce adversarial robustness of
    a base model. This can help distinguish between right and wrong
    model predictions in a neighborhood of a point sampled from the
    underlying training distribution.
- Attribution-driven Causal Analysis [20]: The authors study the
    connection between the resilience to adversarial perturbations and
    the attribution-based explanation of individual decisions generated
    by machine learning models. They report that adversarial inputs are
    not robust in attribution space, that is, masking a few features
    with high attribution leads to change indecision of the machine
    learning model on the adversarial examples. In contrast, the natural
    inputs are robust in attribution space.
    ![An illustration showing two approaches to determining how input values 9,9 becomes misclassified as 9,4.](./media/threat-modeling-aiml/tm3.jpg)[20]
These approaches can make machine learning models more resilient to
adversarial attacks because fooling this two-layer cognition system
requires not only attacking the original model but also ensuring that
the attribution generated for the adversarial example is similar to
the original examples. Both the systems must be simultaneously
compromised for a successful adversarial attack.
#### Traditional Parallels
Remote Elevation of Privilege since attacker is now in control of your
model
#### Severity
Critical
## Variant \#1b: Source/Target misclassification
This is characterized as an attempt by an attacker to get a model to
return their desired label for a given input. This usually forces a
model to return a false positive or false negative. The end result is a
subtle takeover of the model’s classification accuracy, whereby an
attacker can induce specific bypasses at will.
While this attack has a significant detrimental impact to classification
accuracy, it can also be more time-intensive to carry out given that an
adversary must not only manipulate the source data so that it is no
longer labeled correctly, but also labeled specifically with the desired
fraudulent label. These attacks often involve multiple steps/attempts to
force misclassification [3]. If the model is susceptible to transfer
learning attacks which force targeted misclassification, there may be no
discernable attacker traffic footprint as the probing attacks can be
carried out offline.
#### Examples
Forcing benign emails to be classified as spam or causing a malicious
example to go undetected. These are also known as model evasion or
mimicry attacks.
#### Mitigations
Reactive/Defensive Detection Actions
- Implement a minimum time threshold between calls to the API
    providing classification results. This slows down multi-step attack
    testing by increasing the overall amount of time required to find a
    success perturbation.
Proactive/Protective Actions
- Feature Denoising for Improving Adversarial Robustness [22]: The
    authors develop a new network architecture that increase adversarial
    robustness by performing feature denoising. Specifically, the
    networks contain blocks that denoise the features using non-local
    means or other filters; the entire networks are trained end-to-end.
    When combined with adversarial training, the feature denoising
    networks substantially improve the state-of-the-art in adversarial
    robustness in both white-box and black-box attack settings.
- Adversarial Training and Regularization: Train with known
    adversarial samples to build resilience and robustness against
    malicious inputs. This can also be seen as a form of regularization,
    which penalizes the norm of input gradients and makes the prediction
    function of the classifier smoother (increasing the input margin).
    This includes correct classifications with lower confidence rates.
![A graph showing the change in the slope of the prediction function with adversarial training.](./media/threat-modeling-aiml/tm4.jpg)
Invest in developing monotonic classification with selection of
monotonic features. This ensures that the adversary will not be able
to evade the classifier by simply padding features from the negative
class [13].
- Feature squeezing [18] can be used to harden DNN models by
    detecting adversarial examples. It reduces the search space
    available to an adversary by coalescing samples that correspond to
    many different feature vectors in the original space into a single
    sample. By comparing a DNN model’s prediction on the original input
    with that on the squeezed input, feature squeezing can help detect
    adversarial examples. If the original and squeezed examples produce
    substantially different outputs from the model, the input is likely
    to be adversarial. By measuring the disagreement among predictions
    and selecting a threshold value, system can output the correct
    prediction for legitimate examples and rejects adversarial inputs.
    ![An illustration showing the result of feature squeezing.](./media/threat-modeling-aiml/tm5.jpg)
    ![A diagram showing the flow of input through a feature-squeezing framework.](./media/threat-modeling-aiml/tm6.jpg)[18]
- Certified Defenses against Adversarial Examples [22]: The authors
    propose a method based on a semi-definite relaxation that outputs a
    certificate that for a given network and test input, no attack can
    force the error to exceed a certain value. Second, as this
    certificate is differentiable, authors jointly optimize it with the
    network parameters, providing an adaptive regularizer that
    encourages robustness against all attacks.
Response Actions
- Issue alerts on classification results with high variance between
    classifiers, especially if from a single user or small group of
    users.
#### Traditional Parallels
Remote Elevation of Privilege
#### Severity
Critical
## Variant \#1c: Random misclassification
This is a special variation where the attacker’s target classification
can be anything other than the legitimate source classification. The
attack generally involves injection of noise randomly into the source
data being classified to reduce the likelihood of the correct
classification being used in the future [3].
##### Examples
![Two photos of a cat. One photo is classified as a tabby cat. After adversarial perturbation, the other photo is classified as guacamole.](./media/threat-modeling-aiml/tm7.jpg)
##### Mitigations
Same as Variant 1a.
##### Traditional Parallels
Non-persistent denial of service
##### Severity
Important
## Variant \#1d: Confidence Reduction
An attacker can craft inputs to reduce the confidence level of correct
classification, especially in high-consequence scenarios. This can also
take the form of a large number of false positives meant to overwhelm
administrators or monitoring systems with fraudulent alerts
indistinguishable from legitimate alerts [3].
##### Examples
![Two photos of a stop sign. The photo on the left shows a confidence level of 96 percent. After adversarial perturbation, the photo on the right shows a confidence level of 13 percent.](./media/threat-modeling-aiml/tm8.jpg)
##### Mitigations
- In addition to the actions covered in Variant \#1a, event throttling
    can be employed to reduce the volume of alerts from a single source.
##### Traditional Parallels
Non-persistent denial of service
##### Severity
Important
## \#2a Targeted Data Poisoning
Description
The goal of the attacker is to contaminate the machine model generated
***in the training phase***, so that predictions on new data will be
modified in the testing phase[1]. In targeted poisoning attacks, the
attacker wants to misclassify specific examples to cause specific
actions to be taken or omitted.
##### Examples
Submitting AV software as malware to force its misclassification as
malicious and eliminate the use of targeted AV software on client
systems.
##### Mitigations
- Define anomaly sensors to look at data distribution on day to day
    basis and alert on variations
    -Measure training data variation on daily basis, telemetry for
        skew/drift
- Input validation, both sanitization and integrity checking
- Poisoning injects outlying training samples. Two main strategies for
    countering this threat:
    -Data Sanitization/ validation: remove poisoning samples from
        training data
    -Bagging for fighting poisoning attacks [14]
    -Reject-on-Negative-Impact (RONI) defense [15]
    -Robust Learning: Pick learning algorithms that are robust in the
        presence of poisoning samples.
    -One such approach is described in [21] where authors address
            the problem of data poisoning in two steps: 1) introducing a
            novel robust matrix factorization method to recover the true
            subspace, and 2) novel robust principle component regression
            to prune adversarial instances based on the basis recovered
            in step (1). They characterize necessary and sufficient
            conditions for successfully recovering the true subspace and
            present a bound on expected prediction loss compared to
            ground truth.
##### Traditional Parallels
Trojaned host whereby attacker persists on the network. Training or
config data is compromised and being ingested/trusted for model
creation.
##### Severity
Critical
## \#2b Indiscriminate Data Poisoning
##### Description
Goal is to ruin the quality/integrity of the data set being attacked.
Many datasets are public/untrusted/uncurated, so this creates additional
concerns around the ability to spot such data integrity violations in
the first place. Training on unknowingly compromised data is a
garbage-in/garbage-out situation. Once detected, triage needs to
determine the extent of data that has been breached and
quarantine/retrain.
##### Examples
A company scrapes a well-known and trusted website for oil futures data
to train their models. The data provider’s website is subsequently
compromised via SQL Injection attack. The attacker can poison the
dataset at will and the model being trained has no notion that the data
is tainted.
##### Mitigations
Same as variant 2a.
##### Traditional Parallels
Authenticated Denial of service against a high-value asset
##### Severity
Important
## \#3 Model Inversion Attacks
##### Description
The private features used in machine learning models can be
recovered [1]. This includes reconstructing private training data that
the attacker does not have access to. Also known as hill climbing
attacks in the biometric community [16, 17] This is accomplished by
finding the input which maximizes the confidence level returned, subject
to the classification matching the target [4].
##### Examples
![Two images of a person. One image is blurry and the other image is clear.](./media/threat-modeling-aiml/tm9.jpg)[4]
##### Mitigations
- Interfaces to models trained from sensitive data need strong access
    control.
- Rate-limit queries allowed by model
- Implement gates between users/callers and the actual model by
    performing input validation on all proposed queries, rejecting
    anything not meeting the model’s definition of input correctness and
    returning only the minimum amount of information needed to be
    useful.
##### Traditional Parallels
Targeted, covert Information Disclosure
##### Severity
This defaults to important per the standard SDL bug bar, but sensitive
or personally identifiable data being extracted would raise this to
critical.
## \#4 Membership Inference Attack
##### Description
The attacker can determine whether a given data record was part of the
model’s training dataset or not[1]. Researchers were able to predict a
patient’s main procedure (e.g: Surgery the patient went through) based
on the attributes (e.g: age, gender, hospital) [1].
![An illustration showing the complexity of a membership inference attack. Arrows show the flow and relationship between training data prediction data.](./media/threat-modeling-aiml/tm10.jpg)[12]
##### Mitigations
Research papers demonstrating the viability of this attack indicate
Differential Privacy [4, 9] would be an effective mitigation. This is
still a nascent field at Microsoft and AETHER Security Engineering
recommends building expertise with research investments in this space.
This research would need to enumerate Differential Privacy capabilities
and evaluate their practical effectiveness as mitigations, then design
ways for these defenses to be inherited transparently on our online
services platforms, similar to how compiling code in Visual Studio gives
you on-by-default security protections which are transparent to the
developer and users.
The usage of neuron dropout and model stacking can be effective
mitigations to an extent. Using neuron dropout not only increases
resilience of a neural net to this attack, but also increases model
performance [4].