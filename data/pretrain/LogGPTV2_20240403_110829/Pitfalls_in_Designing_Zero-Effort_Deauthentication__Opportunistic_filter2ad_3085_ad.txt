Next we consider the question whether the inability of the
attacker to see the victim hampers his ability to circumvent
ZEBRA. Figures 10a and 10b summarize the performance
of an audio-only opportunistic keyboard-only attacker. This
attack is in line with prior attacks based on keyboard acoustic
emanations [3], [31]. Prior attacks aimed at recognizing the
keystrokes based on their sounds, while our attack attempts
to recognize typing/mouse activities based on their sounds.
One key difference is that our attack is manual, whereas prior
attacks were automated (in fact, it seems that prior attacks can
not be performed manually since a human attacker may not
be able to distinguish between sounds of different keys). As
such, our attack may be viewed as a new form of acoustic
emanations attack targeted at the ZEBRA system.
Again, we see that such an attack is less successful than
an opportunistic keyboard-only attacker who is able to see his
victim. However, it is still more successful than a na¨ıve all-
activity attack. Again, with g = 1, 15% of the audio-only
opportunistic keyboard-only remain logged in after 6 windows.
Thus, we conclude that an attacker adopting an opportunis-
tic approach can do better in circumventing ZEBRA than by
na¨ıvely mimicking all interactions. This holds even when the
9
5101520253000.10.20.30.40.50.60.70.80.91Window size (w)Average FPR  50%55%60%65%70%5101520253000.10.20.30.40.50.60.70.80.91Windows (w)Fraction of logged in adversaries  g=1g=25101520253000.10.20.30.40.50.60.70.80.91Window size (w)Average FPR  50%55%60%65%70%5101520253000.10.20.30.40.50.60.70.80.91Windows (w)Fraction of logged in adversaries  g=1g=2attacker is hampered by not having visual access to the victim.
An opportunistic keyboard-only attacker performs signiﬁcantly
better than a na¨ıve all-activity attacker.
a user logs into a terminal and automatically deauthenticate
him) from any other terminal where he has an active logged-
in session.
VI. STRENGTHENING ZEBRA
Opportunistic attacks against ZEBRA succeed because of
the fundamental ﬂaw in its design: it allows the adversary
to control both interaction sequences Authenticator receives
as input. First, the adversary has full control over the actual
interaction sequence as he can choose the type and order of
his terminal interactions. Second, he can indirectly inﬂuence
the predicted interaction sequence as his terminal inputs cause
Interaction Extractor to choose the times at which the victim’s
bracelet data is segmented and fed to the Interaction Classiﬁer
to generate the predicted interaction sequence.
We can cast this as a general problem of tainted input:
accepting data which can be incorrect or outright malicious,
and performing security-critical actions based on it. This is
a common issue in any application or on-line service ac-
cepting input from potential adversaries. There are typically
three counter-measures: (1) augmenting with trusted input,
(2) marking untrusted input as tainted and performing taint
tracking, or (3) sanitizing untrusted input before using it. As
the sole purpose of the interaction sequences is authentication,
taint tracking is not applicable in our case. Thus, we consider
the other two potential solutions: using trusted input and input
sanitization.
Augmenting with Trusted Input: Instead of allowing the
terminal input to fully determine when Authenticator compares
the two interaction sequences, a fundamental ﬁx is to base this
determination additionally on bracelet data which is not under
the control of the attacker. This would require inferring the
predicted interaction sequence continuously from the bracelet
data even when the terminal observes no actual interaction.
If the predicted interaction sequence suggests that the user
is interacting with a terminal, but no corresponding actual
interaction is observed, Authenticator should output “Different
User”. This presupposes that the Interaction Classiﬁer has very
high precision (which we discuss below). Requesting data from
the bracelet continually, rather than on demand, might lead to
unwanted deauthentication if the event is not recognized.
Augmenting ZEBRA with Bluetooth proximity measure-
ments means that we have another way of assurring ourselves
that the user is nearby. We noticed that typical bluetooth signal
strengths are within -5dB for users immediately close by, e.g.
working at the terminal. Similarly, users walking nearby the
terminal tend to have signals strengths within -15dB. Based on
this, a three-level proximity calculation could be developed,
classifying the proximity of the user as immediate, near or far
based on the Bluetooth signal strength. Users that are perceived
as being near or far could have progressively increased authen-
tication thresholds, e.g. increasing the threshold from 70% to
80% in case of near distance and further to 90% in case of far
distance. This would make mimicking attacks more difﬁcult,
because the attacker needs to be very close to the victim in
order to have a lenient threshold.
Sanitizing Untrusted Input: Input sanitization can take the
form of whitelisting (accepting speciﬁc well-formed inputs
only) or blacklisting (rejecting a set of known malicious
input patterns). Authenticator has two inputs that need to be
sanitized: the actual interaction sequence and the predicted
interaction sequence.
For example, one could attempt to prevent our opportunistic
keyboard-only attacker by adopting a whitelisting approach of
only accepting actual interaction sequences which contain mul-
tiple types of interactions, such as requiring periodic MKKM
interactions interspersed with typing. However, since many
legitimate user sessions can involve typing-only sequences, this
remedy will violate the zero-effort requirement.
ZEBRA could also use blacklisting where certain types
of input data can immediately trigger deauthentication. For
example, if an input stream can reliably indicate the user
standing up and walking away from the terminal, it can trigger
deauthentication. Augmenting the bracelet data we currently
use (accelerometer and gyroscope) with additional information,
like heart-rate data available on many current smartwatches,
can be used for this purpose. However, these ﬁxes can seem
privacy-invasive for some users.
Further Instances of Tainted Input: We identify additional
types of input interactions that an adversary can use to defeat
ZEBRA. As the bracelet is assumed to be worn on the mouse-
controlling (e.g., right) hand, ZEBRA records an MKKM
interaction after mouse activity only if it observes a keypress
event on that side of the keyboard. This is done to reduce false
negatives arising from a user who types with the keyboard-only
hand without removing his mouse-controlling hand from the
mouse. Again, such a design decision introduces a vulnera-
bility: for example, in the case of a right handed victim, the
attacker can type using only the left and middle parts of the
keyboard (approximately 60% of the keys) while the victim
continues to use the mouse. Having previously recorded an
interaction involving the mouse, ZEBRA will leave out all such
subsequent typing from the segments it considers for com-
parison. This could be mitigated by blacklisting long typing
sequences involving keys in the middle and non-dominant parts
of the keyboard as such sequences are not typical in normal
workstation usage.
Another such vulnerability is when A interacts by only
moving and clicking the mouse. No event gets reported for
these activities and consequently an adversary can potentially
do much harm, for example by copy-pasting words appearing
in the screen. Mare et al. [23] report that they did not consider
mouse movement and click events as interactions because
“they did not contribute to ZEBRA’s performance.” However,
including them would seem the most feasible defense. As we
can see in our examples, pre-mature optimization motivated by
privacy (such as not collecting data under certain scenarios)
may introduce security vulnerabilities.
In a centralized (multi-terminal) environment, it may be
possible to use successful login events as an input for trigger-
ing deauthentication: a central system could recognize when
Making the System Work in Real-time: It is well-known that
on-line systems always bring new information of the usability,
compared to off-line analysis. When we experimented with
10
our end-to-end implementation in real-time, we noticed that
the original 3-class classiﬁer (typing, scrolling and MKKM)
systematically identiﬁed bracelet measurements during walk-
ing and standing (“upright”) as typing interactions. Similarly,
measurements while the bracelet was simply lying on the table
(“idle”) were classiﬁed as scrolling interactions. The similarity
between the hand movements in these pairs of events leads to
similar magnitude-based features. Based on these observations,
we extended the original classiﬁer to account for the new types
of “interactions”: idle and upright. The new classes were added
as a post-processing step: a one hundred random tree ensemble
was learned ﬁrst (random forest), each tree contributing by
voting between one of the three original classes. These votes
were fed to a C4.5 decision tree, which learned decision
thresholds for the ﬁve classes. We noticed that the performance
of the resulting real-time system improved a great deal. We
observed an overall improvement in both the accuracy and the
ability to generalize to new users.
Improving Machine Learning: Mare et al. [23] make use
of accelerometers and gyroscopes that report measurements
in three dimensions but use only magnitude values calculated
from a single dimension. ZEBRA, and other similar techniques
in general, can be extended to use measurements from all three
dimensions. The gravity component in individual axes can be
eliminated with a low-pass ﬁlter [2]. The added information
from statistical measures in any individual direction can help
in the discrimination of the classes, increasing the accuracy
of the classiﬁer in normal usage. With a better classiﬁer we
can raise the threshold (m) of authenticating a user interaction,
lowering the FPR, while increasing the FNR. An acceptable
FNR level can then be found as a compromise with receiver
operating characteristic (ROC) curves, which shows the trade-
off between TPR and FPR.
The Scrolling events were more difﬁcult to identify com-
pared to others in our experiments (Table I). So improving
the accuracy of these predictions is of interest. Further feature
engineering can increase the classiﬁcation ability. Feature
selection algorithms can select robust features that generalize
the decision rules well. Feature selection can also help in
increasing the battery life of the bracelet, since less information
needs to be transmitted over Bluetooth from the bracelet to the
computer for classiﬁcation.
VII. DISCUSSION
Despite our attempts to reproduce the implementation
described in [23], differences remain. Although our imple-
mentation achieves lower FNR for legitimate users, it incurs
somewhat longer delays in logging out na¨ıve attackers. We
were unable to reproduce the high rate of user interactions
reported in [23]. Despite these differences, the main result of
our work holds because it is comparative: we demonstrated
that in our system, attackers adopting opportunistic strate-
gies can signiﬁcantly outperform a na¨ıve all-activity attacker.
Such a comparative result will hold in any implementation
of ZEBRA, including [23], despite any differences between
implementations.
Impact of Data Set: One contributor in performance dif-
ferences could be a methodological difference we discovered
with the original paper. The authors note that one user’s “wrist
movement during keyboard and mouse interaction were very
different compared to the other subjects”, and one of the test
users is logged out almost immediately. It is likely that a
large fraction of this user’s authentication windows are thus
incorrectly classiﬁed, amounting to 1/20, i.e. 5 percent points
difference in FPR between their experiments and ours.
One potential explanation is that in [23] only one of the
users was left-handed, which may result in differences for this
one user. The leave-one-user-out classiﬁer training may also
exaggerate this as it results in the classiﬁer being trained with
data from only right-handed users, but tested with data from
the left-handed user. However, without access to the original
test data, this cannot be veriﬁed.
Impact of Sampling Rate: A notable difference lies in the
sampling rate of the bracelet. We chose to use commercial off-
the-shelf smartwatches as bracelets because they are general-
purpose devices readily available for a much larger audience
and thus a realistic choice for deployment. In such devices, the
underlying sensor hardware limits the maximum sampling rate,
typically 100-200 Hz on newer devices. Our LG smartwatch
supported a sampling rate of 200 Hz. This is less than the
500Hz special-purpose Shimmer bracelet used in [23].
The choice of sampling rate has an impact on power
consumption [6]. On Android, the sampling rate can be set to
lower levels to save energy at the cost of reduced accuracy. The
features we collect are mostly statistical measures calculated
from the distribution of magnitude values measured during the
event and should be quite stable as long as there are enough
data points to calculate the values from.
To evaluate the effect of sampling rates, we collected
a small dataset from normal computer usage with 200 Hz
sampling rate. We downsampled it to 100 Hz, 50 Hz and 25 Hz
data sets by passing every second, fourth or eight measurement
signal to Segmenter. The datasets were generated using the
same data: the number of features and the number of events
are the same at all frequencies, but the number of measurement
signals used to calculate the features were different. We
noticed that some features (e.g. skewness) could frequently
not be calculated for short events at low frequencies because
Segmenter could not pass enough measurements signal values
to Feature Extractor. Sample skewness needs at least three
values to be calculated. As a rule of thumb, the frequency
of the bracelet needs to be at least fmin = smin/dmin to
catch enough measurements for feature calculation, when smin
(3) is the minimum amount of signal measurements needed
to calculate all features and dmin (25 ms) is the minimum
duration of a classiﬁable event. With our end-to-end system
parameter settings this would be fmin = 120 Hz. For devices
operating at lower sampling rates, the minimum acceptable
duration of interactions should be increased accordingly.
Typically lower sampling rates increase the noise in fea-
tures, which in turn changes class boundaries. We expect minor
classes to get misclassiﬁed as major classes more frequently.
In the worst scenario, everything gets classiﬁed as the major
class (typically this would be typing in our scenarios). Lower
sampling rates would increase the FPR in this way. This is not
the case in 200 Hz, as can be seen in Table I (Section IV-C).
We experimented on our real-time system with one such lower
rate (20 Hz), at which the traces contain very little information
11
for each interaction causing ZEBRA to become insensitive to
synchronization delays as long as 1s. At higher rates above 120
Hz there was no noticeable difference. Therefore, we conclude
that while ZEBRA is unreliable at very low sampling rates, its
performance was found to be steady at or above 120 Hz.