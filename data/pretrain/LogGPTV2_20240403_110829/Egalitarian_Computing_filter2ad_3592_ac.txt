grinding attack
Now we can explain in more details why the composition
of F and the difﬁculty test is not a good proof-of-work
even if some internal blocks of H are opened. Suppose
that the proof is accepted if H(X[T ]) has certain number
d of trailing zeros. One would expect that a prover has
to try 2d distinct I on average and thus call F 2t times to
ﬁnd a solution. However, a cheating prover can simply
try 2d values for X[T ] and ﬁnd one that passes the test
in just 2d calls to H. Although X[T ] is now inconsistent,
it is unlikely to be selected among L blocks to open, so
the cheater escapes detection easily. Additionally check-
ing X[T ] would not resolve the problem since a cheater
320  25th USENIX Security Symposium 
USENIX Association
6
iL
i1
Argon2
I
Merkle tree
Nonce
N
Φ
Open 2L blocks
H
i1
H
iL
H
Y
No
d trailing zeros?
Yes
Figure 1: MTP: Merkle-tree based Proof-of-Work with
light veriﬁcation.
would then modify the previous block, or X[φ (T )], or an
earlier block and then propagate the changes. A single
inconsistent block is just too difﬁcult to catch2.
4.5 MTP-Argon2
As a concrete application, we suggest a cryptocurrency
proof-of-work based on Argon2d with 4 parallel lanes.
We aim to make this PoW unattractive for botnets, so we
suggest using 2 GB of RAM, which is very noticeable
(and thus would likely alarm the user), while being bear-
able for the regular user, who consciously decided to use
his desktop for mining. On our 1.8 GHz machine a single
call to 2-GB Argon2d runs in 0.5 seconds, but the Merkle
tree computation is more expensive, as we have to hash
2 GB of data splitted into 1 KB blocks. We suggest us-
ing Blake2b for H, as it is already used in Argon2d, but
restrict to 128-bit output, so that the total running time is
about 3 seconds. In this case a single opening has 16· 21
bytes of hashes, or 1.3 KB in total.
We suggest L = 70, so that the entire proof consists
of 140 blocks and their openings, or 180 KB in total.
Let us ﬁgure out the cheating advantage. The C() and
D() functions are given in Table 1). Assuming certain
ratio between the area needed to implement Blake2b and
2We have not seen any formal treatment of this attack in the liter-
ature, but it appears to be known in the community. It is mentioned
in [26] and [4].
the area needed for DRAM, we get the following lower
bound on the ASIC-equipped cheater.
Proposition 1 For L = 70 and 2 GB of RAM the time-
area product can be reduced by the factor of 12 at most,
assuming that each Blake2b core occupies an equivalent
of 216 bytes.
Proof. Assuming that each core occupies 216 bytes, we
obtain β = 2−15 in terms of Equation (1). Since the
cheater has the success chance γ = (1−ε)L, Equation (1)
is modiﬁed as follows:
ATα = AT1
αD(α + ε) +C(α + ε)/215
(1− ε)L
.
(4)
Consider three options:
• α,ε  1/12. Then C(α + ε) ≥ 20,
(1− ε)L > 1/441, and we have
20· 441
32768 ≥ AT1 · 0.27.
ATα ≥ AT1 ·
• α  215, and the
time-area product increases.
• α > 1/12. Then ATα ≥ AT1 · 1/12.
This ends the proof.
We conclude that a cheater can gain at most 12x-
advantage, whereas he can still be detected in the future
by memory-rich veriﬁers. Tradeoffs are also not helpful
when implementing this Proof-of-Work on ASIC. Alto-
gether, our proposal should reduce the relative efﬁciency
of potential ASIC mining rigs and allow more egalitarian
mining process. Even if someone decides to use large
botnets (10,000 machines and more), all the botnets ma-
chines would have to use the same 2 GB of memory, oth-
erwise they would suffer large penalty. We note that if
ε = 0, i.e. the prover is honest, then his maximal advan-
tage is max
1
αD(α) ≤ 2.
4.6 MTP as a tool for time-lock puzzles and
timestamping
The paradigm of inherently sequential computation was
developed by [12] in the application to CPU benchmark-
ing and [29] for timestamping, i.e. to certify that the doc-
ument was generated certain amount of time in the past.
Rivest et al. suggested time-lock puzzles for this purpose.
USENIX Association  
25th USENIX Security Symposium  321
7
In our context, a time-lock puzzle solution is a proof-of-
work that has lower bound on the running time assuming
unlimited parallelism.
The veriﬁer in [20, 29] selects a prime product N =
pq and asks the prover to compute the exponent 22D
It is conjectured that the
(mod N) fpr some D ≈ N.
prover who does not know the factors can not exponen-
tiate faster than do D consecutive squarings. In turn, the
veriﬁer can verify the solution by computing the expo-
nent 2D modulo φ (N), which takes log(D) time. So far
the conjecture has not been refuted, but the scheme in-
herently requires a secret held by the veriﬁer, and thus
is not suitable for proofs-of-work without secrets, as in
cryptocurrencies.
Time-lock puzzles without secrets were suggested by
Mahmoody et al. [22]. Their construction is a graph
of hash computations, which is based on depth-robust
graphs similarly to [16]. The puzzle is a deterministic
graph such that removing any constant fraction of nodes
keeps its depth above the constant fraction of the original
one (so the parallel computation time is lower bounded).
A Merkle tree is put atop of it with its root determining
a small number of nodes to open. Therefore, a cheater
who wants to compute the graph in less time has to sub-
vert too many nodes and is likely to be caught. As [16],
the construction by Mahmoody et al., if combined with
the difﬁculty ﬁlter, is subject to the grinding attack de-
scribed above.
The MTP-Argon2 construction can be viewed as
a time-lock puzzle and an improvement over these
schemes. First, the difﬁculty ﬁlter is explicitly based on
the grinding attack, which makes it a legitimate way to
solve the puzzle. Secondly, it is much faster due to high
speed of Argon2d. The time-lock property comes from
the fact that the computation chain can not be parallelized
as the graph structure is not known before the computa-
tion.
Suppose that MTP-Argon2 is parallelized by the ad-
ditional factor of R so that each core computes a chain
of length about T /R. Let core j compute j-th (out of
R) chain, chronologically. Then bu step i each core has
computed i blocks and has not computed T /R−i blocks,
so the probability that core j requests a block that has not
been computed is
( j− 1)(T /R− i)
( j− 1)T /R + i ≤
( j− 1)(T /R− i)
jT /R
.
2R
Summing by all i, we obtain that core j misses at least
T (1−1/ j)
, so the total fraction of inconsistent blocks is
about 0.5 − lnR
2R . Therefore, ε quickly approaches 0.5,
which is easily detectable. We thus conclude that a par-
allel implementation of MTP-Argon2 is likely to fail the
Merkle tree veriﬁcation.
5 Memory-hard
entropy keys
encryption
on
low-
5.1 Motivation
In this section we approach standard encryption from
the memory-hardness perspective. A typical approach to
hard-drive encryption is to derive the master key from the
user password and then use it to encrypt chunks of data in
a certain mode of operation such as XTS [24]. The major
threat, as to other password-based security schemes, are
low-entropy passwords. An attacker, who gets access to
the hard drive encrypted with such password, can deter-
mine the correct key and then decrypt within short time.
A countermeasure could be to use a memory-hard
function for the key derivation, so that the trial keys can
be produced only on memory-rich machines. However,
the trial decryption could still be performed on special
memoryless hardware given these keys. We suggest a
more robust scheme which covers this type of adversaries
and eventually requires that the entire attack code have
permanent access to large memory.
5.2 Requirements
We assume the following setting, which is inspired by
typical disk-encryption applications. The data consists
of multiple chunks Q ∈ Q, which can be encrypted and
decrypted independently. The only secret that is avail-
able to the encryption scheme E is the user-input pass-
word P ∈ P, which has sufﬁciently low entropy to be
memorized (e.g., 6 lowercase symbols). The encryption
syntax is then as follows:
E : P × S × Q → C ,
where S ∈ S is associated data, which may contain salt,
encryption nonce or IV, chunk identiﬁer, time, and other
secondary input; and C ∈ C is ciphertext. S serves both
to simplify ciphertext identiﬁcation (as it is public) and
to ensure certain cryptographic properties. For instance,
unique salt or nonce prevents repetition of ciphertexts for
identical plaintexts. We note that in some settings due to
storage restriction the latter requirement can be dropped.
Decryption then is naturally deﬁned and we omit its for-
mal syntax.
In our proposal we do not restrict the chunk size. Even
though it can be deﬁned for chunks as small as disk sec-
tors, the resistance to cracking attacks will be higher for
larger chunks, up to a megabyte long.
A typical attack setting is as follows. An attacker ob-
tains the encrypted data via some malicious channel or
installs malware and then tries different passwords to de-
crypt it. For the sake of simplicity, we assume that the
322  25th USENIX Security Symposium 
USENIX Association
8
plaintext contains sufﬁcient redundancy so that a suc-
cessful guess can be identiﬁed easily. Therefore, the ad-
versary tries D passwords from his dictionary D ⊂ P.
Let T be the time needed for the fastest decryption op-
eration that provides partial knowledge of plaintext suf-
ﬁcient to discard or remember the password, and A0 be
the chip area needed to implement this operation. Then
the total amount of work performed by the adversary is
W = D· T · A0.
At the same time, the time to encrypt T(cid:29) for a typical user
should not be far larger than T . Our goal is to maximize
W with keeping T(cid:29) the same or smaller.
The memory-hard functions seem to serve perfectly
for the purpose of maximizing W . However, it remains
unclear how to combine such function F with E to get
memory-hard encryption (MHE).
Now we formulate some additional features that
should be desirable for such a scheme:
• The user should be able to choose the requested
memory size A independently of the chunk length
|Q|. Whereas the chunk length can be primarily de-
termined by the CPU cache size, desirable process-
ing speed, or the hard drive properties, the memory
size determines the scheme’s resistance to cracking
attacks.
• The memory can be allocated independently for
each chunk or reused. In the former case the user
can not allocate too much memory as the mas-
sive decryption would be too expensive. How-
ever, for the amounts of memory comparable to
the chunk size the memory-hard decryption should
take roughly as much as memoryless decryption. If
the allocated memory is reused for distinct chunks,
much more memory can be allocated as the alloca-
tion time can be amortized. However, the decryp-
tion latency would be quite high. We present both
options in the further text.
• Full ciphertext must be processed to decrypt a single
byte. This property clearly makes T larger since
the adversary would have to process an entire chunk
to check the password. At the same time, for disk
encryption it should be ﬁne to decrypt in the “all-or-
nothing” fashion, as the decryption time would still
be smaller than the user could wait.
• Encryption should be done in one pass over data.
It might sound desirable that the decryption should
be done in one pass too. However, this would con-
tradict the previous requirement. Indeed, if the de-
cryption can be done in one pass, then the ﬁrst bytes
of the plaintext can be determined without the last
bytes of the ciphertext3.
• Apart from the memory parameter, the total time
needed to allocate this memory should be tunable
too. It might happen that the application does not
have sufﬁcient memory but does have time. In this
case, the adversary can be slowed down by making
several passes over the memory during its initial-
ization (the memory-hard function that we consider
support this feature).
Our next and ﬁnal requirement comes from adversary’s
side. When the malware is used, the incoming network
connection and memory for this malware can be limited.
Thus, it would be ideal for the attacker if the memory-
intensive part can be delegated to large machines under
attacker’s control, such as botnets. If we just derived the
secret-key K for encryption as the output of the memory-
hard hash function F , this would be exactly this case.
An adversary would then run F for dictionary D on his
own machine, produce the set K of keys, and supply
them to malware (recall that due to low entropy there
would be only a handful of these keys). Thus the ﬁnal
requirement should be the following:
• During decryption, it should be impossible to del-
egate the entire memory-hard computation to the
external device without accessing the ciphertext.
Therefore, there could be no memory-hard precom-
putation.
5.3 Our scheme
Our scheme is based on a recent proposal by Za-
verucha [35], who addresses similar properties in the
scheme based on Rivest’s All-or-Nothing transform
(ANT). However, the scheme in [35] does not use an ex-
ternal memory-hard function, which makes it memory
requirements inevitably bound to the chunk size. Small
chunks but large memory is impossible in [35].
Our proposal is again based on the All-or-Nothing
transformation, though we expect that similar proper-
ties can be obtained with deterministic authenticated en-
cryption scheme as a core primitive. The chunk length
q (measured in blocks using by F ) and memory size
M ≥ q are the parameters as well as some blockcipher
E (possibly AES). First, we outline the scheme where