0.4
0.6
0.8
1
fraction of reissues
Figure 4: Estimating the variability in latency: (a) CDF
of 99th percentile to median latency and (b) mean vs.
standard deviation of latency, for stages and workﬂows.
Figure 5: For a subset of stages in production, this plot
shows normalized latency variance of the stage as a func-
tion of fraction of requests that are reissued.
results are passed along. This stage is followed by a snippet
generation stage that extracts a two sentence snippet for each
of the documents that make it to the ﬁnal answer. Fig. 3
shows a timelapse of the processing involved in this workﬂow;
every search at Bing passes through this workﬂow. While
this is one of the most complex workﬂows at Bing, it is still
represented as a single stage at the highest level workﬂow.
The observed causes for high and variable latency include
slow servers, network anomalies, complex queries, congestion
due to improper load balance or unpredictable events, and
software artifacts such as buﬀering. The sheer number of
components involved ensures that each request has a non-
trivial likelihood of encountering an anomaly.
2.2 Workﬂow characteristics
We characterize most of the workﬂows and their latencies
at Bing. We use request latencies from 64 distinct workﬂows
over a period of 30 days during Dec 2012. We only report
results for workﬂows and stages that were accessed at least
100 times each day, the 25th and 75th percentile number of
requests per stage per day are 635 and 71428 respectively. In
all, we report results from thousands of stages and hundreds
of thousands of servers.
2.2.1 Properties of latency distributions
Latencies of stages and workﬂows have long tail. To
understand variation of latency in workﬂows and in individ-
ual stages, Fig. 4a plots a CDF of the ratio of the latency
of the 99th percentile request to that of the median request
across the stages and workﬂows in our dataset. We see that
stages have high latency variability; roughly 10% have 99th
percentile 10X larger than their median. When the stages
are composed into workﬂows, the variability increases on the
low end because more workﬂows can have high variability
stages but decreases the variability on the high end. Fig. 4b
shows on a log scale the mean latency in a stage and workﬂow
compared to the standard deviation. We see that the larger
the mean latency in a stage the larger is the variability (stan-
dard deviation). However, stages with similar mean latency
still have substantial diﬀerences in variability.
Stages beneﬁt diﬀerently from reissues. Fig. 5 illus-
trates how reissuing requests impacts the latency for a subset
of the stages from production. It shows the normalized vari-
ance in latency for these stages when a particular fraction of
the slowest queries are reissued. Clearly, more reissues lead
to lower variance. However, notice that stages respond to
reissues diﬀerently. In some stages, 10% reissues signiﬁcantly
reduce variance, whereas in other stages even 50% reissues
do not achieve similar gains. This is because the reduction in
n
o
i
t
c
a
r
F
)
e
v
i
t
l
a
u
m
u
C
(
1
s
w
o
l
f
k
r
o
w
f
o
0.8
0.6
0.4
0.2
0
Avg. Critical Path
In-degree on Critical Path
Effective Num. Stages
All Stages
0
5 10 15 20 25 30 35 40
Number of stages 
Figure 6: A few characteristics of the analyzed workﬂows
variance at a stage depends on its latency distribution: stages
with low mean and high variance beneﬁt a lot from reissues
but the beneﬁts decrease as the mean increases. Hence, giv-
ing every stage the same fraction of reissues may not be a
good strategy to reduce latency of the workﬂow.
Latencies in individual stages are uncorrelated. We
ran a benchmark against the most frequent workﬂow, where
we executed two concurrent requests with same parameters
and speciﬁed they should not use any cached results. These
requests executed the same set of stages with identical in-
put parameters and thus allowed us to study correlation of
latencies in individual stages. We used 100 diﬀerent input
parameters and executed a total of 10000 request pairs. For
each of the 380 stages in this workﬂow, we compute the Pear-
son correlation coeﬃcient (PCC). About 90% of the stages
have PCC below 0.1 and only 1% of stages have PCC above
0.5. Hence, we treat the latency of the ﬁrst request and of
the reissue as independent random variables.
Latencies across stages are mostly uncorrelated. To
understand correlation of latencies across stages, we compute
the PCC of latencies of all stage pairs in one of the major
workﬂows with tens of thousands of stage pairs. We ﬁnd that
about 90% of stage pairs have PCC below 0.1. However 9%
of stage pairs have PCC above 0.5. This is perhaps because
some of the stages run back-to-back on the same server when
processing a request; if the server is slow for some reason, all
the stages will be slow. However, in such cases, the reissued
request is very likely to be sent to a diﬀerent server. Hence, in
spite of this mild correlation we treat the inherent processing
latency across stages to be independent.
2.2.2 Properties of execution DAGs
As the “all stages” line in Fig. 6 indicates, most workﬂows
have a lot of stages, with a median value of 14 and 90th
percentile of 81. About 20% of the workﬂows have stage
sequences of length 10 or more (not shown). However, the
221Parameter
at server, network load due
to request-response traﬃc
Lag to retransmit
Packet loss prob.
response traﬃc
To compare: packet loss
prob. of map-reduce
Fraction of losses recovered
by RTO
of req-
Value Percentiles
50th
895pps,
.62Mbps
67.2ms
99th
2730pps,
2.3Mbps
168.7ms
90th
2242pps,
1.84Mbps
113.3ms
.00443
.0004336
.987
Table 1: Network Characteristics
on the web-search workﬂow (see Fig. 1). For each of the
5% slowest queries, we assign blame to a stage when its
contribution to that query’s latency is more than µ + 2σ,
where µ, σ are the mean and stdev of its contribution over all
queries. If a stage takes too long for a query, it is timed-out.
In such cases, the blame is still assigned to the stage, citing
timeout as the reason. Fig. 7 depicts, for each stage of the
workﬂow, its average contribution to latency along with the
fraction of delayed responses for which it has timed-out or is
blamed (includes timeouts). Since more than one stage can
be blamed for a delayed response, the blame fractions add
up to more than one.
We see that the document lookup and the network transfer
stages receive the most blame (50.7% and 33.5% each). In
particular, these stages take so long for some queries that
the scheduler times them out in 18.7% and 20.3% of cases
respectively. Network transfer receives blame for many more
outliers than would be expected given its typical contribution
to latency (just 12.1%). We also see that though the start-
up/ wrap-up stage contributes sizable average latency, it is
highly predictable and rarely leads to outliers. Further, the
servers are provisioned such that the time spent waiting in
queues for processing at both the doc lookup and the snippet
generation stages is quite small.
Why would stages take longer than typical? To examine
the doc lookup stage further, we correlate the query latency
with wall-clock time and the identity of the machine in
the doc lookup tier that was the last to respond. Fig. 8
plots the average query latency per machine per second
of wall time. The darkness of a point reﬂects the average
latency on log scale. We see evidence of ﬂaky machines in
the doc lookup tier (dark vertical lines); queries correlated
with these machines consistently result in higher latencies.
We conjecture that this is due to hardware trouble at the
server. We also see evidence for time-dependent events, i.e.,
periods when groups of machines slow down. Some are
rolling upgrades through the cluster (horizontal sloping dark
line), others (not shown) are congestion epochs at shared
components such as switches. We also found cases when only
machines containing a speciﬁc part of the index slowed down,
likely due to trouble in parsing some documents in that part
of the index.
To examine the network transfer stage further, we correlate
the latency of the network transfer stage with packet-level
events and the lag introduced in the network stack at either
end. We collected several hours of packet traces in produc-
tion beds for the network transfer stage in the web-search
workﬂow (Fig. 1). To compare, we also collect packet traces
from production map-reduce clusters that use the same server
and switch hardware but carry traﬃc that is dominated by
large ﬂows. The results of this analysis is shown in Table 1.
Figure 7: Stages that receive blame for the slowest 5%
queries in the web-search workﬂow.
Figure 8: Heatmap showing how the latency varies across
machines and time (for queries to the workﬂow in Fig 1).
max in-degree across stages is proportional to the number of
stages in the workﬂow (not shown). That is, most workﬂows
are parallel.
Stages that have a very small latency and rarely occur
on the critical path for the query can be set aside. We say
that critical path for a query is the sequence of dependent
stages in the workﬂow that ﬁnished the last for that query.
Since queries can have diﬀerent critical paths, we consider
the most frequently occurring critical paths that account for
90% of the queries. Along each critical path, we consider
the smallest subset of stages that together account for over
90% of the query’s latency and call these the eﬀective stages.
Fig. 6 plots the number of eﬀective stages across workﬂows.
We see that the number of eﬀective stages in a workﬂow
is sizable but much smaller than the number of all stages;
median is 4 and 90th percentile is 18. The ﬁgure also plots
the average number of eﬀective stages on these critical paths
of workﬂows; median is 2.2. Finally, we plot a distribution
of the in-degree of these eﬀective stages on the critical paths;
median is 2 and 90th is 9. Hence, we see that production
workﬂows even when counting only the eﬀective stages are
long and many way parallel.
We point out that stages with high in-degree, that ag-
gregate responses from many other stages, are a signiﬁcant
source of variability. Whenever one of the input stages is
slow, the output would be slow. In fact P (maxnXi > s) ∼
nP (Xi > s) when s is in the tail. We see two types of
such fan-in, only one of which (at the stage level) has been
accounted for above. Several stages internally aggregate the
responses from many servers, for example, the web-search
stage above aggregates responses from 100s-1000s of servers
that each retrieve documents matching the query from their
shard of the index.
2.3 Causes for latencies on the tail
When responses take longer than typical, we want to
understand the causes for the high latency. Here, we focus
27.6% 12.1% 28.3% 25.0% 0.2% 7.1% 50.7% 33.5% 19.1% 6.1% 1.1% 5.8% 18.7% 20.3% 0%10%20%30%40%50%60%Avg. contrib to latencyBlame for outliersTimeoutsDoc. Lookup Snippet Gen. Start-up/ Wrap-up Wait for proc. Others Network Transfer 222We see that the request-response traﬃc has 10X higher loss
rate than in the map-reduce cluster. Further the losses are
bursty, coeﬃcient of variation σ
µ is 2.4536. The increased
loss rate is likely due to the scatter-gather pattern, i.e., re-
sponses collide on the link from switch to aggregator. Most
of the losses are recovered only by a retransmission time-
out (over 98%) because there are not enough acks for TCP’s
fast retransmission due to the small size of the responses.
Surprisingly, the RTO for these TCP connections was quite
large, in spite of RTO min being set to 20ms; we are still
investigating the cause. We conclude that TCP’s inability
to recover from burst losses for small messages is the reason
behind the network contributing so many outliers.
2.4 Takeaways
Our analysis shows the following:
• Workﬂow DAGs are large and very complex, with signif-
icant sequences of stages and high degree of parallelism,
which increases latency variance.
• Diﬀerent stages have widely diﬀerent properties of
mean, variance, and variance as a function of the
amount of requests reissued at that stage.
• Latencies of diﬀerent stages are uncorrelated, except
when running on the same machine; latency of reissues
is uncorrelated with latency of the ﬁrst request.
The ﬁrst two observations demonstrate the complexity of
the problem; heuristics that do not consider properties of the
latency distributions and of the DAG, cannot perform very
well. The third observation points in the direction of our
solution; it allows us to decompose the optimization problem
on a complex workﬂow to a problem over individual stages.
3. KEY IDEAS IN Kwiken
The goal of Kwiken is to improve the latency of request-
response workﬂows, especially on the higher percentiles. We
pick the variance of latency as the metric to minimize because
doing so will speed-up all of the tail requests; in that sense,
it is more robust than minimizing a particular quantile1.
Our framework optimizes the workﬂow latency at both the
stage and workﬂow levels. At the stage/local level, it selects
a policy that minimizes the variance of the stage latency. At
the workﬂow/global level, it combines these local policies
to minimize the end-to-end latency. We employ three core
per-stage techniques for latency reduction – reissue laggards
at replicas, skip laggards to return timely albeit incomplete
answers and catch-up, which involves speeding up requests
based on their overall progress in the workﬂow.
Using latency reduction techniques incurs cost – such as
using more resources to serve reissued requests – so we have to
reason about apportioning a shared global cost budget across
stages to minimize the end-to-end latency. For example,
reissues have higher impact in stages with high variance.
Similarly, speeding up stages that lie on the critical path
of the workﬂow is more helpful than those that lie oﬀ the
critical path. Also, as shown in Fig. 5, variance of some
stages reduces quickly even with a few reissues, while other
stages require more reissues to achieve the same beneﬁts.
1Delaying responses such that all queries ﬁnish with the
slowest has a variance of 0, but is not useful. An implicit
requirement in addition to minimizing variance, which Kwiken
satisﬁes, is for the mean to not increase.
Finally, the cost of reissuing the same amount of requests
could be orders of magnitude higher in stages that are many-
way parallel, a factor that has to be incorporated into the
overall optimization.
To reason about how local changes impact overall latency,
our basic idea is to decompose the variance of the workﬂow’s
latency into the variance of individual stages’ latency. If the
random variable Ls denotes the latency at stage s, then the
latency of workﬂow w is given by
Lw(L1, . . . , LN ) = max
p X
s∈p
Ls,
(1)
where p stands for a path, namely an acyclic sequence of
stages through the workﬂow (from input to output). Ideally,
we would use the variance of Lw as our objective function,
and minimize it through allocating budget across stages.
Unfortunately, however, the variance of Lw does not have a
closed form as a function of the individual stages’ statistics
(e.g., their ﬁrst or second moments). Instead, we resort to
minimizing an upper bound of that variance. Recall from §2.2
that the diﬀerent Ls can be roughly treated as independent
random variables. Using this approximation together with
(1) leads to the following decomposition:
Var(Lw) ≤ X
Var(Ls),
s∈w
(2)
where Var(·) denotes the variance of a random variable; see
appendix for proof. The bound is always tight for sequential
workﬂows, as stage variances add up. It can also be shown
that (2) is the best general bound for parallel workﬂows;
details omitted here.
Using Chebyshev’s inequality, (2) immediately implies that
δ2
P r(|Lw − ELw| > δ) ≤ (Ps Var(Ls))2
. The bound indicates
that minimizing the sum of variances is closely related to
minimizing the probability of large latency deviations, or
latency percentiles. Better concentration bounds (e.g., Bern-
stein [7]) can also be obtained; we omit details for brevity.
We emphasize that we do not claim tightness of the bounds,
but rather use them as a theoretical insight for motivat-
ing sum of variances minimization. As we elaborate below,
the decomposition to sum of variances leads to a tractable
optimization problem, unlike other approaches for solving it.
Alongside the latency goal, we need to take into account the
overall cost from applying local changes. Here, we describe
the framework using reissues. Formally, let rs be the fraction
of requests that are reissued at stage s and let cs be the
(average) normalized resource cost per request at stage s, i.e.,
Ps cs = 1. Then the overall cost from reissues is C(r) =
Ps csrs, and the problem of apportioning resources becomes:
minimize X
Var(cid:0)Ls(rs)(cid:1)
s
subject to X
csrs ≤ B,
s
(3)
where B represents the overall budget constraint for the
workﬂow2 and Ls(rs) is the latency of stage s under a policy
that reissues all laggards after a timeout which is chosen such
that only an rs fraction of requests are reissued. Since {cs}
are normalized, B can be viewed as the fraction of additional
2Considerations on how to choose the budget for each work-
ﬂow, which we assume as given by an exogenous policy, are
outside the scope of this paper.
223y
c
n
e
t
a
l
d
e
z
i
l
a
m
r
o
n
0.76
0.72
0.68
0.64
0.6
0.65
normalized sum of variances
1
random, 99th
SumVar, 99th