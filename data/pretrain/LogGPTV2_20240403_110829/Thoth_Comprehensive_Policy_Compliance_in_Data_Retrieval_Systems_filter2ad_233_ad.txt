wise identical baseline system with unmodiﬁed Linux
3.13.1 kernels.
6.1 Thoth-based data retrieval system
We study the total Thoth overheads in the prototype re-
trieval system described in Section 4.
Indexing First, we measure the overhead of the search
engine’s index computation. We run the Lucene in-
dexer over a) the entire 258GB snapshot of the English
Wikipedia, and b) a 5GB part of the snapshot. The sizes
of the resulting indices are 54GB and 959MB, respec-
tively. Table 3 shows the average indexing time and stan-
dard deviation across 3 runs. In both cases, Thoth’s run-
time overhead is below 1%.
Dataset 258GB
Avg. (mins) σ Avg. (mins)
Dataset 5GB
Linux
Thoth
Overhead
1956.1
1968.6
0.65%
30
24
27.8
28.0
0.7%
Table 3: Indexing runtime overhead
σ
0.06
0.11
Even in a sharded conﬁguration, Lucene relies on a se-
quential indexer, which can become a bottleneck when a
corpus is large and dynamic. Larger search engines may
rely on parallel map/reduce jobs to produce their index.
As a proof of concept, we built a Hadoop-based indexer
using Thoth, although we don’t use it in the following
evaluation because it does not support all the features
of the Lucene indexer. All mappers and reducers run as
conﬁned tasks, and receive the same taint as the original,
sequential indexer.
Search throughput Next, we measure the overhead of
Thoth on the query latency and throughput. To ensure
load balance, we partitioned the index into two shards
of 22GB and 33GB, chosen to achieve approximately
equal query throughput. We use two conﬁgurations:
2SERVERS: 2 server machines execute a Lucene in-
stance with different index shards. 4SERVERS: Here,
we use two replicated Lucene instances in each shard to
scale the throughput. The front-end forwards each search
request to one of the two Lucene instances in each shard
and merges the results.
We drive the experiment with the following work-
load. We simulate a population of 40,000 users, where
each user is assigned a friend list consisting of 12 ran-
domly chosen other users, subject to the constraint that
the friendship relationship is symmetric. Each item in
the corpus is assigned either a private, public, or friends-
only policy in the proportion 30/50/20%, respectively. A
total of 1.0% of the dataset is censored in some region.
All simulated clients are in a region that blacklists 2250
random items.
We use query strings based on the popularity of
Wikipedia page accesses during one hour on April 1,
2012 [42]. Speciﬁcally, we search for the titles of the
top 20K visited articles and assign each of the queries
randomly to one of the users. 24 simulated active users
connect to each server machine, maintain their sessions
throughout the experiment, and issue 48 (2SERVERS)
and 96 (4SERVERS) queries concurrently to saturate
the system.
In addition, a simulated “employee” spo-
radically issues a read access to protected user ﬁles for a
total of 200 MAL accesses.
During each query, the front-end looks up the user
proﬁle and updates the user’s search history in the key-
value store. To maximize the performance of the baseline
and fully expose Thoth’s overheads, the index shard and
646  25th USENIX Security Symposium 
USENIX Association
Linux
Thothpublic
Thothratio
)
s
/
Q
0
0
1
x
(
t
u
p
h
g
u
o
r
h
T
 7
 6
 5
 4
 3
 2
 1
 0
2SERVERS 4SERVERS
(a) configuration
t
u
p
h
g
u
o
r
h
t
d
e
z
i
l
a
m
r
o
N
 1.2
 1
 0.8
 0.6
 0.4
 0.2
 0
1
2
5
20
(b) queries per session
y
c
n
e
t
a
l
d
e
z
i
l
a
m
r
o
N
5
4
3
2
1
0
Linux
Thothpublic
Thothprivate
ThothACL
ThothACL+
ThothFoF
ThothMAL
4KB
512KB
Figure 4: Search throughput
Figure 5: Read latency, normalized to Linux’s
parts of the corpus relevant to our query stream are pre-
loaded into the servers’ main memory caches, resulting
in a CPU-bound workload.
Figure 4 (a) shows the average throughput over 10 runs
of 20K queries each, for the baseline (Linux) and Thoth
under 2SERVERS and 4SERVERS. The error bars in-
dicate the standard deviation over the 10 runs. We used
two Thoth conﬁgurations, Thothpublic and Thothratio.
In Thothpublic, the policies permit all accesses. This
conﬁguration helps to isolate the overhead of Thoth’s
I/O interposition and reference monitor invocation.
In
Thothratio, input ﬁles are private to a user, public, or ac-
cessible to friends-only in the ratio 30:50:20. All ﬁles al-
low employee access under MAL, enforce region-based
censorship, and have the declassiﬁcation condition with
ONLY_CONDUIT_IDS+.
The query throughput scales approximately linearly
from 2SERVERS (320 Q/s) to 4SERVERS (644 Q/s), as
expected. Thoth with all policies enforced (Thothratio)
has an overhead of 3.63% (308 Q/s) in 2SERVERS
and 3.55% in 4SERVERS (621 Q/s). We note that
the throughput achieved with Thothpublic
(310 Q/s
and 627 Q/s, respectively) is only slightly higher than
Thothratio’s. This suggests that Thoth’s overhead is
dominated by costs like I/O interception, Thoth API
calls, and metadata operations, which are unrelated to
policy complexity.
To test whether overheads can be reduced further, we
also implemented a rudimentary reference monitor in the
kernel, which does not support session management and
policy interpretation (which require libraries that are un-
available in the Linux kernel). This reduced in-kernel
monitor sufﬁces to execute Thothpublic. Moving the ref-
erence monitor to the kernel reduced the overhead of
Thothpublic from 3% to under 1%, which suggests that
overheads can be further reduced my moving the refer-
ence monitor to the kernel and, hence, eliminating the
cost of IPC between the LSM and the reference monitor.
With Thoth, the front-end is re-exec’ed at the end of
every user session to shed the front-end’s taint. The rel-
ative overhead of doing so reduces with session length.
Figure 4 (b) shows the average throughput normalized
to the Linux baseline for session lengths of 1, 2, 5 and
20 queries in 2SERVERS. Due to the per-session front-
end exec, Thoth’s overhead is higher for small sessions
(15.8% for a single query); however, the overhead di-
minishes quickly to 8.6% for 2 queries per session, and
the throughput is within a standard deviation of the max-
imum for 5 or more queries per session in all conﬁgura-
tions, including 4SERVERS.
Search latency Next, we measure the overhead on
query latency. Table 4 shows the average query latency
across 5 runs of 10K queries in 2SERVERS. The results
in 4SERVERS are similar. In all cases, Thoth adds less
than 6.7ms to the baseline latency.
Avg. (ms)
Linux
Thothpublic
Thothratio
47.09
51.60
53.78
σ
0.43
0.29
0.20
Overhead
9.6%
14.2%
Table 4: Query search latency (ms)
6.2 Microbenchmarks
Next, we perform a set of microbenchmarks to isolate
Thoth’s overheads on different policies. We measure
the latency of opening, reading sequentially, and closing
10K ﬁles in the baseline and with Thoth under different
policies associated with the ﬁles. The ﬁles were previ-
ously written to disk sequentially to ensure fast sequen-
tial read performance for the baseline and therefore fully
expose the overheads.
In the Thoth experiments, accesses are performed by
an UNCONFINED task to force an immediate policy eval-
uation. The following policies are used. Thothpublic:
ﬁles can be read by anyone. Thothprivate: access is re-
stricted to a speciﬁc user. ThothACL: access to friends
only (all users have the same friend list). ThothACL+:
access to friends only (each user has a different friend
list). ThothFoF: access to friends of friends (each user
has a different friend list). All friend lists used in the
microbenchmark have 100 entries. ThothMAL: each ﬁle
has a MAL policy, where each read requires an entry in
a log with an append-only integrity policy.
USENIX Association  
25th USENIX Security Symposium  647
Figure 5 shows the average time for reading a ﬁle of
sizes 4K and 512K, normalized to the baseline Linux
latency (0.145ms and 3.6ms, respectively);
the error
bars indicate the standard deviation among the 10K ﬁle
reads. We see that Thoth’s overheads increase with
the complexity of the policy, in the order listed above.
For the 4KB ﬁles, the overheads range from 10.6% for
Thothpublic and Thothprivate to 152.7% for ThothMAL.
The same trend holds for larger ﬁles, but the over-
head range diminishes to 0.6%–23% for 96KB ﬁles (not
shown in the ﬁgure) and 0.34%–3.3% for 512KB ﬁles.
We also experimented with friend list sizes of 12 and
50 entries for ThothACL, ThothACL+ and ThothFoF; the
resulting latency was within 2.4% of the corresponding
100-entry friend list latency. This is consistent with the
known complexity of the friend lookup, which is loga-
rithmic in the list size.
We also looked at the breakdown of Thoth latency
overheads. With ThothACL and 4KB ﬁles, Thoth’s over-
head for ﬁle read is on average 28µs, which are spent
intercepting the system call and maintaining the session
state. Interpreting the policy and checking the friend lists
takes 6µs, but this time is completely overlapped with the
disk read.
Write transaction latency We performed similar mi-
crobenchmarks for write transactions.
In general,
Thoth’s write transactions have low overhead since its
transaction log is stored in (simulated) NVRAM. As in
the case of read latency, the overhead depends on the
granularity of writes and the complexity of the policy
being enforced. Under the index policy, the overhead
ranges from 0.25% for creating large ﬁles to 2.53x in the
case of small ﬁles. The baseline Linux is very fast at cre-
ating small ﬁles that are written to disk asynchronously,
while Thoth has to synchronously update its policy store
when a new ﬁle is created. The overhead is 5.8x and
8.6x in the case of a write of 10 conduit ids to a ﬁle un-
der the ONLY_CND_IDS and ONLY_CND_IDS+ poli-
cies, respectively. This high overhead is due to check-
ing that each conduit id being written exists (and is
written into a ﬁle with a stricter policy in the case of
ONLY_CND_IDS+). However, this overhead amounts
to only a small percentage of the overall search query
processing, as is evident from Table 4.
6.3 Fault-injection tests
To double-check Thoth’s ability to stop unwanted data
leaks, we injected several types of faults in different
stages of the search pipeline.
Faulty Lucene indexer We reproduced a known
Lucene bug [5] that associates documents with wrong
attributes during index creation. This bug is security-
relevant because, in the absence of another mechanism,
attributes can be used for labeling data with their owners.
In our experiment Thoth successfully stopped the ﬂow
in all cases where the search results contained a conduit
whose policy disallowed access to the client.
We also intentionally misconﬁgured the indexer to in-
dex the users’ query and click histories, which should
not show up in search results. Thoth prevented the in-
dexer from writing the index after it had read either the
query or the click history.
Faulty Lucene search We reproduced a number of
known Lucene bugs that produce incorrect search results.
Such bugs may produce Alice’s private documents in
Bob’s search. The bugs include incorrect parsing of spe-
cial characters [7], incorrect tokenization [9], confusing
uppercase and lowercase letters [10], using an incorrect
logic for query expansion [4, 3], applying incorrect key-
word ﬁlters [8], and premature search termination [6].
We conﬁrmed that all policy violations resulting from
these bugs faults were blocked by Thoth.
check
the
To
declassiﬁcation
condition
ONLY_CND_IDS+, we modiﬁed the search pro-
cess to (incorrectly) output text from the index in place
of conduit ids. Thoth prevented the search process from
producing such output.
Faulty front-end We issued accesses to a private ﬁle
protected by the MAL policy without adding appropri-
ate log entries. Thoth prevented the front-end process
from extricating data to the caller. We performed similar
tests for the region-based censorship policy with similar
results.
7 Related work
Search engine policy compliance Grok [36] is a pri-
vacy compliance tool for the Bing search engine. Grok
and Thoth differ in techniques, expressiveness and tar-
get policies. Grok uses heuristics and selective manual