### 6.1 Thoth-based Data Retrieval System

In this section, we evaluate the total overheads introduced by Thoth in the prototype retrieval system described in Section 4.

#### Indexing Overheads

First, we measure the overhead of the search engine's index computation. We use Lucene to index two datasets: a) the entire 258GB snapshot of the English Wikipedia, and b) a 5GB subset of the same snapshot. The resulting indices are 54GB and 959MB, respectively. Table 3 shows the average indexing time and standard deviation across three runs. In both cases, Thoth's runtime overhead is below 1%.

| Dataset | Linux (Avg. mins) | Thoth (Avg. mins) | Overhead (%) | Standard Deviation (σ) |
|---------|-------------------|-------------------|--------------|------------------------|
| 258GB   | 1956.1            | 1968.6            | 0.65%        | 0.06                   |
| 5GB     | 27.8              | 28.0              | 0.7%         | 0.11                   |

**Table 3: Indexing Runtime Overhead**

Even in a sharded configuration, Lucene relies on a sequential indexer, which can become a bottleneck for large and dynamic corpora. Larger search engines may use parallel map/reduce jobs to produce their index. As a proof of concept, we built a Hadoop-based indexer using Thoth, although it does not support all features of the Lucene indexer. All mappers and reducers run as confined tasks and receive the same taint as the original, sequential indexer.

#### Search Throughput

Next, we measure the overhead of Thoth on query latency and throughput. To ensure load balance, we partitioned the index into two shards of 22GB and 33GB, chosen to achieve approximately equal query throughput. We use two configurations:
- **2SERVERS**: Two server machines execute a Lucene instance with different index shards.
- **4SERVERS**: Two replicated Lucene instances in each shard to scale the throughput. The front-end forwards each search request to one of the two Lucene instances in each shard and merges the results.

We simulate a population of 40,000 users, where each user has a friend list consisting of 12 randomly chosen other users, with the constraint that the friendship relationship is symmetric. Each item in the corpus is assigned a private, public, or friends-only policy in the proportion 30/50/20%, respectively. A total of 1.0% of the dataset is censored in some region. All simulated clients are in a region that blacklists 2250 random items.

We use query strings based on the popularity of Wikipedia page accesses during one hour on April 1, 2012 [42]. Specifically, we search for the titles of the top 20K visited articles and assign each query randomly to one of the users. 24 simulated active users connect to each server machine, maintain their sessions throughout the experiment, and issue 48 (2SERVERS) and 96 (4SERVERS) queries concurrently to saturate the system.

Additionally, a simulated "employee" sporadically issues a read access to protected user files for a total of 200 MAL accesses.

During each query, the front-end looks up the user profile and updates the user’s search history in the key-value store. To maximize the performance of the baseline and fully expose Thoth’s overheads, the index shard and relevant parts of the corpus are pre-loaded into the servers' main memory caches, resulting in a CPU-bound workload.

**Figure 4(a)** shows the average throughput over 10 runs of 20K queries each, for the baseline (Linux) and Thoth under 2SERVERS and 4SERVERS. The error bars indicate the standard deviation over the 10 runs. We used two Thoth configurations, Thothpublic and Thothratio.

- **Thothpublic**: Policies permit all accesses, isolating the overhead of Thoth’s I/O interposition and reference monitor invocation.
- **Thothratio**: Input files are private to a user, public, or accessible to friends-only in the ratio 30:50:20. All files allow employee access under MAL, enforce region-based censorship, and have the declassification condition with ONLY_CONDUIT_IDS+.

The query throughput scales approximately linearly from 2SERVERS (320 Q/s) to 4SERVERS (644 Q/s), as expected. Thoth with all policies enforced (Thothratio) has an overhead of 3.63% (308 Q/s) in 2SERVERS and 3.55% in 4SERVERS (621 Q/s). The throughput achieved with Thothpublic (310 Q/s and 627 Q/s, respectively) is only slightly higher than Thothratio’s, suggesting that Thoth’s overhead is dominated by costs like I/O interception, Thoth API calls, and metadata operations, which are unrelated to policy complexity.

To further reduce overheads, we implemented a rudimentary reference monitor in the kernel, which does not support session management and policy interpretation. This reduced in-kernel monitor suffices to execute Thothpublic. Moving the reference monitor to the kernel reduced the overhead of Thothpublic from 3% to under 1%, indicating that overheads can be further reduced by moving the reference monitor to the kernel and eliminating the cost of IPC between the LSM and the reference monitor.

With Thoth, the front-end is re-executed at the end of every user session to shed the front-end’s taint. The relative overhead of doing so reduces with session length. **Figure 4(b)** shows the average throughput normalized to the Linux baseline for session lengths of 1, 2, 5, and 20 queries in 2SERVERS. Due to the per-session front-end exec, Thoth’s overhead is higher for small sessions (15.8% for a single query); however, the overhead diminishes quickly to 8.6% for 2 queries per session, and the throughput is within a standard deviation of the maximum for 5 or more queries per session in all configurations, including 4SERVERS.

#### Search Latency

Next, we measure the overhead on query latency. **Table 4** shows the average query latency across 5 runs of 10K queries in 2SERVERS. The results in 4SERVERS are similar. In all cases, Thoth adds less than 6.7ms to the baseline latency.

| Configuration | Avg. (ms) | σ | Overhead (%) |
|---------------|-----------|---|--------------|
| Linux         | 47.09     | 0.43 | -            |
| Thothpublic   | 51.60     | 0.29 | 9.6%         |
| Thothratio    | 53.78     | 0.20 | 14.2%        |

**Table 4: Query Search Latency (ms)**

### 6.2 Microbenchmarks

Next, we perform a set of microbenchmarks to isolate Thoth’s overheads on different policies. We measure the latency of opening, reading sequentially, and closing 10K files in the baseline and with Thoth under different policies associated with the files. The files were previously written to disk sequentially to ensure fast sequential read performance for the baseline and fully expose the overheads.

In the Thoth experiments, accesses are performed by an UNCONFINED task to force an immediate policy evaluation. The following policies are used:
- **Thothpublic**: Files can be read by anyone.
- **Thothprivate**: Access is restricted to a specific user.
- **ThothACL**: Access to friends only (all users have the same friend list).
- **ThothACL+**: Access to friends only (each user has a different friend list).
- **ThothFoF**: Access to friends of friends (each user has a different friend list).
- **ThothMAL**: Each file has a MAL policy, where each read requires an entry in a log with an append-only integrity policy.

All friend lists used in the microbenchmark have 100 entries.

**Figure 5** shows the average time for reading a file of sizes 4K and 512K, normalized to the baseline Linux latency (0.145ms and 3.6ms, respectively); the error bars indicate the standard deviation among the 10K file reads. Thoth’s overheads increase with the complexity of the policy, in the order listed above. For the 4KB files, the overheads range from 10.6% for Thothpublic and Thothprivate to 152.7% for ThothMAL. The same trend holds for larger files, but the overhead range diminishes to 0.6%–23% for 96KB files (not shown in the figure) and 0.34%–3.3% for 512KB files.

We also experimented with friend list sizes of 12 and 50 entries for ThothACL, ThothACL+, and ThothFoF; the resulting latency was within 2.4% of the corresponding 100-entry friend list latency. This is consistent with the known complexity of the friend lookup, which is logarithmic in the list size.

We also looked at the breakdown of Thoth latency overheads. With ThothACL and 4KB files, Thoth’s overhead for file read is on average 28µs, spent intercepting the system call and maintaining the session state. Interpreting the policy and checking the friend lists takes 6µs, but this time is completely overlapped with the disk read.

#### Write Transaction Latency

We performed similar microbenchmarks for write transactions. In general, Thoth’s write transactions have low overhead since its transaction log is stored in (simulated) NVRAM. As in the case of read latency, the overhead depends on the granularity of writes and the complexity of the policy being enforced. Under the index policy, the overhead ranges from 0.25% for creating large files to 2.53x in the case of small files. The baseline Linux is very fast at creating small files that are written to disk asynchronously, while Thoth has to synchronously update its policy store when a new file is created. The overhead is 5.8x and 8.6x in the case of a write of 10 conduit ids to a file under the ONLY_CND_IDS and ONLY_CND_IDS+ policies, respectively. This high overhead is due to checking that each conduit id being written exists (and is written into a file with a stricter policy in the case of ONLY_CND_IDS+). However, this overhead amounts to only a small percentage of the overall search query processing, as is evident from Table 4.

### 6.3 Fault-injection Tests

To verify Thoth’s ability to stop unwanted data leaks, we injected several types of faults in different stages of the search pipeline.

#### Faulty Lucene Indexer

We reproduced a known Lucene bug [5] that associates documents with wrong attributes during index creation. This bug is security-relevant because, in the absence of another mechanism, attributes can be used for labeling data with their owners. In our experiment, Thoth successfully stopped the flow in all cases where the search results contained a conduit whose policy disallowed access to the client.

We also intentionally misconfigured the indexer to index the users’ query and click histories, which should not show up in search results. Thoth prevented the indexer from writing the index after it had read either the query or the click history.

#### Faulty Lucene Search

We reproduced a number of known Lucene bugs that produce incorrect search results. Such bugs may produce Alice’s private documents in Bob’s search. The bugs include incorrect parsing of special characters [7], incorrect tokenization [9], confusing uppercase and lowercase letters [10], using an incorrect logic for query expansion [4, 3], applying incorrect keyword filters [8], and premature search termination [6]. We confirmed that all policy violations resulting from these bugs were blocked by Thoth.

To check the declassification condition ONLY_CND_IDS+, we modified the search process to (incorrectly) output text from the index in place of conduit ids. Thoth prevented the search process from producing such output.

#### Faulty Front-end

We issued accesses to a private file protected by the MAL policy without adding appropriate log entries. Thoth prevented the front-end process from extricating data to the caller. We performed similar tests for the region-based censorship policy with similar results.

### 7. Related Work

#### Search Engine Policy Compliance

Grok [36] is a privacy compliance tool for the Bing search engine. Grok and Thoth differ in techniques, expressiveness, and target policies. Grok uses heuristics and selective manual interventions to enforce privacy policies, whereas Thoth provides a more systematic and automated approach.