title:Persistent Last-mile Congestion: Not so Uncommon
author:Romain Fontugne and
Anant Shah and
Kenjiro Cho
Persistent Last-mile Congestion: Not so Uncommon
Romain Fontugne
PI:EMAIL
IIJ Research Lab
Anant Shah
PI:EMAIL
Verizon Digital Media Services
Kenjiro Cho
PI:EMAIL
IIJ Research Lab
ABSTRACT
Last-mile is the centerpiece of broadband connectivity, as
poor last-mile performance generally translates to poor qual-
ity of experience. In this work we investigate last-mile latency
using traceroute data from RIPE Atlas probes located in 646
ASes and focus on recurrent performance degradation. We
find that in normal times probes in only 10% ASes experience
persistent last-mile congestion but we recorded 55% more
congested ASes during the COVID-19 outbreak. Persistent
last-mile congestion is not uncommon, it is usually seen in
large eyeball networks and may span years. With the help
of CDN access log data, we dissect results for major ISPs in
Japan, the most severely affected country in our study, and
ascertain bottlenecks in the shared legacy infrastructure.
CCS CONCEPTS
• Networks → Network performance analysis; Net-
work measurement;
ACM Reference Format:
Romain Fontugne, Anant Shah, and Kenjiro Cho. 2020. Persis-
tent Last-mile Congestion: Not so Uncommon. In ACM Internet
Measurement Conference (IMC ’20), October 27–29,2020, Vir-
tual Event, USA. ACM, New York, NY, USA, 8 pages. https:
//doi.org/10.1145/3419394.3423648
1 INTRODUCTION
Internet resources are shared among a varied number of users
with diverse demands. The exhaustion of network resources
is the main source of packet loss and increased latency which
usually translates into degraded web services and poor quality
of experience [26]. Understanding causes of Internet conges-
tion and detecting congestion in time and space is, hence,
crucial for maintaining quality of service.
Past studies have exposed the relationship between persis-
tent inter-domain congestion and under provisioned links [7,
17], and between transient in-network congestion and routing
mishap [11]. For broadband users, home and last-mile net-
works are the main bottlenecks [25]. Last-mile networks are
Permission to make digital or hard copies of all or part of this work
for personal or classroom use is granted without fee provided that
copies are not made or distributed for profit or commercial advantage
and that copies bear this notice and the full citation on the first
page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy
otherwise, or republish, to post on servers or to redistribute to lists,
requires prior specific permission and/or a fee. Request permissions
from permissions@acm.org.
IMC ’20, October 27–29, 2020, Virtual Event, USA
© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-8138-3/20/10. . . $15.00
https://doi.org/10.1145/3419394.3423648
also a susceptible to transient self-induced congestion [12, 24]
and a key factor to quality of experience [26]. Nonetheless, a
recent analysis of access networks in US and UK reveals that
last mile latency is usually stable and features no recurrent
congestion [3]. On the contrary, our study shows that for
some Autonomous Systems (ASes) last-mile congestion may
be a pervasive and perpetual problem. Hence, this paper
complements the literature by documenting persistent last
mile congestion, that is, congestion close to users’ premises
that repeatedly appears over an extended period of time.
Using RIPE Atlas, we conduct an exploratory survey of
last-mile latency in 646 Autonomous Systems (ASes). We
find that Atlas probes located in 90% of monitored ASes ex-
hibit no consistent last-mile congestion but some probes with
persistent last-mile congestion are usually located in large eye-
ball networks and congestion may recur over years. We also
show that the number ASes hosting congested Atlas probes
increased by 55% during the COVID-19 outbreak. Finally
we present a case study focused on major Japanese ISPs and
show that CDN access logs support our findings. Our compar-
ison between different access technologies in Japan narrows
down the problem to the extensive use of the shared legacy
infrastructure over PPPoE and shows that wired broadband
throughput for some ISPs is consistently lower than LTE
during peak hours.
This work provides valuable insights to the operational
community with the following research contributions:
∙ We propose (§2) and validate (§4) a methodology to mea-
sure persistent last mile congestion. We make our tools
publicly available so that our experiments can be repro-
duced and extended [16].
∙ We report last-mile conditions in 2018 and 2019 for Atlas
probes located in 646 ASes and 98 countries, and we esti-
mate the impact of COVID-19 on last-mile latencies (§3).
These surveys are available on a public server [1].
∙ Our case study illustrates how a nation-wide infrastruc-
ture, which had successfully opened the telecommunication
market to competition [9], is now failing to cope with the
increasing demand (§4). Given the extent of this propri-
etary infrastructure and the difficulties to upgrade it, we
reiterate the importance of scaling and upgradability in
these deployments.
∙ Finally, we give recommendations to handle persistent
last-mile congestion in delay measurements with RIPE
Atlas, and discuss the adverse consequences of BBR in this
context (§6).
420
IMC ’20, October 27–29, 2020, Virtual Event, USA
Romain Fontugne, Anant Shah, and Kenjiro Cho
2 FROM TRACEROUTE TO
LAST-MILE CONGESTION
With over ten thousands probes deployed world-wide, the
RIPE Atlas measurement platform is ideal for surveying
last-mile condition in numerous ASes. Also, as our interest
lies in the closest segment to the probes, we can recycle the
numerous public measurement data offered by Atlas. We
fetched data from the 22 IPv4 built-in traceroute measure-
ments [22] to obtain a steady number of RTT samples. These
measurements are executed by all probes towards all root
DNS servers and RIPE Atlas controllers every 30 minutes,
and two randomly selected addresses every 15 minutes.
For our experiments, we filter out some undesired tracer-
outes. First, we ignore traceroutes from Atlas anchors as this
type of probe is usually located in datacenters, thus without
a typical last-mile connectivity. Second, for each probe, we
group its traceroutes into 30-minute time-bins and discard
traceroutes in bins that have less than 3 traceroutes. This
sanity check ensures that the Atlas probe is normally oper-
ating during the time bin, thus we avoid incorrect inference
with disconnected probes. Also we deliberately employ large
time-bins (30-minute) to filter out transient congestion and
focus only on long-lasting congestion.
Although past research has shown that v1 and v2 probes
can be less reliable [13], in our experiments we observe only
slight differences in our aggregated results when using these
probes. As a trade-off between precision and coverage, we
avoid using these probes when it is not needed (§4) but we
include them when surveying last-mile latency at large scale
(§3).
In this paper, we focus on eight measurement periods. Six
periods are used for longitudinal analysis, these stand for
the 1st to the 15th of March, June, and September, 2018 and
2019. We assess the impact of COVID-19 using traceroutes
collected from the 1st to the 15th of April 2020. Finally, we
collect traceroute data during the time period covered by the
CDN log data employed in §4. To avoid confusion, all dates
are in UTC.
2.1 Estimating last-mile RTT
The last-mile is generally regarded as the segment connecting
the probe’s premises to the ISP IP infrastructure. In practice,
we identify the ISP edge infrastructure as the first public IP
address seen in the traceroute (i.e. not a RFC1918 private
address). We notice that some of these IP addresses are not
announced on BGP, thus when we need to identify the ASN
corresponding to the last-mile, we use the probes’ public
address for longest prefix match with BGP data.
To estimate the last-mile RTT, we simply subtract the last
private IP RTT from the identified first public IP RTT. Past
work has shown that this is a practical estimate when paths
are symmetric [7, 11], which is expected for private LANs
hosting Atlas probes.
Using the traceroute dataset mentioned above, every 30
minutes we obtain 24 traceroutes and compute 9 RTT samples
per traceroute (pairwise subtraction of the 3 RTTs for each
of the last private IP and the first public IP); that is 216
samples per probe. To filter out noise as in [11], we compute
the median RTT per probe in 30-minute time-bins.
Congestion is monitored by estimating the deviation (i.e.
queuing delay) from a base latency (i.e. propagation delay).
To measure these delay changes we subtract the minimum
median RTT value from all median RTT values for each probe.
The minimum median RTT is computed separately for each
measurement period to account for Atlas probe deployment
changes. Consequently, we obtain a rough estimate of last-
mile queuing delay for each probe where the lowest point is
set to zero and other values correspond to delay increase in
milliseconds.
Finally, we derive the overall last-mile conditions from a
population of probes. In this paper we select a population of
probes based on their ASN (§3), or their ASN and geograph-
ical location (§4). To combine delays from a population, we
compute the median value across all last-mile queuing delay
estimates from that population. This gives us an aggregated
queuing delay where large fluctuations reveal times when the
majority of the probes experience high latency.
2.2 Examples
To give a concrete example, we present results from two
large eyeball networks hosting numerous Atlas probes. One
is located in the U.S.A. (hereafter referred as ISP US) and
the other one in Germany (ISP DE).
Figure 1 depicts aggregated queuing delay for each mea-
surement period and AS. For ISP DE (upper plot) we observe
very stable delays for all measurement periods. Even in April
2020, during COVID-19 lockdown, we observe no particular
change. These results and our large-scale survey (§3) support
past observations [3] by showing that last-mile RTTs are
usually stable.
For some networks, however, we found interesting patterns
that reveal persistent delay increases. For example, ISP US
(Fig. 1 lower plot) features a small but consistent diurnal
pattern during 2018 and 2019. In April 2020 this pattern
is even more pronounced with peak hours widening over
daytime. As discussed in § 3, we attribute this to the impact of
COVID-19 lockdowns. The aggregated queuing delay increase
is apparently small, only over 1ms during peak hours, but
looking at the delays of each probe we observe that the
proportion of probes that experience daily queuing delay over
5ms has tripled when compared to results in 2018 and 2019,
representing a quarter of the probes in 2020.
For now we would like to stress two key observations: (1)
Similar to persistent inter-domain congestion [7], persistent
last-mile congestion is characterized by a clear daily pattern,
and (2) these two types of congestion differ by their amplitude.
In the case of last-mile congestion we aggregate delays from
numerous links, hence, unless most links are congested we
measure only small aggregated variations. In § 4 we show that
significant throughput drops occur when aggregated delays
are over 1ms. Furthermore, our metrics are designed to be
robust to outliers thus only long lasting congestion across
421
Persistent Last-mile Congestion: Not so Uncommon
IMC ’20, October 27–29, 2020, Virtual Event, USA
Figure 1: One week of aggregated last-mile queuing
delay for large German (top) and American (bot-
tom) ISP in 2018-2020. Numbers in parentheses are
the number of Atlas probes for each measurement