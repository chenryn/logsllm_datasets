population. We assume that our system receives a
sample of users from the population, each holding
their own collection of private data drawn indepen-
dently and identically from the distribution over all
records p. Its goal is to output an estimate ˆp of prob-
abilities of the most frequent search records, while
preserving diﬀerential privacy (in the trusted curator
model) for the opt-in users and (in the local model)
for the clients.
Informal Overview of Blender:
presents an architectural diagram of Blender.
Figure 1
Blender serves as the trusted curator for the opt-
in group of users, and begins by aggregating data
from them. Using a portion of the data, it con-
structs a candidate head list of records in a diﬀer-
entially private manner that approximates the most
common search records in the population. It addi-
tionally includes a single “wildcard” record, (cid:104)(cid:63), (cid:63)(cid:105),
which represents all records in the population that
weren’t previously included in the candidate head
list. It then uses the remainder of the opt-in data to
estimate the probability of each record in the candi-
date head list in a diﬀerentially private manner, and
(optionally) trims the candidate head list down to
create the ﬁnal head list. The result of this compo-
nent of Blender is the privatized trimmed head list
of search records and their corresponding probabil-
ity and variance estimates, which can be shared with
each user in the client group, and with the world.
Each member of the client group receives the pri-
vatized head list obtained from the opt-in group.
Each client then uses the head list to apply a diﬀer-
ential privacy-preserving perturbation to their data,
subsequently reporting their perturbed results to
Blender. Blender then aggregates all the clients’
reports and, using a statistical denoising procedure,
estimates both the probability for each record in the
head list as well as the variance of each of the esti-
mated probabilities based on the clients’ data.
For each record, Blender combines the record’s
probability estimates obtained from the two groups.
It does so by taking a convex combination of the
groups’ probability estimates for each record, care-
fully weighted based on the record’s variance esti-
mate in each group. The combined result under
this weighting scheme yields a better probability es-
timate than either group is able to achieve individu-
ally. Finally, Blender outputs the obtained records
750    26th USENIX Security Symposium
USENIX Association
Blenderlocal search datatrend dataprivacy barrierprobabilityvarianceheadlistprobabilityvarianceheadlistprobabilityvariancequery/url pairsprivacy barrierprivacy barrierquery/url pairsquery/url pairsprivacy barrierquery/url pairsprivacy barrierprivacy barrierquery/url pairsquery/url pairsprivacy barrierquery/url pairsquery/url pairsquery/url pairsquery/url pairsquery/url pairsquery/url pairsand their combined probability estimates, which can
be used to drive local search, determine trends, etc.
including key parameters.
A Formal Overview of Blender: Figure 2
presents the precise algorithmic overview of each
step,
Lines 1-3 of
Blender describe the treatment of data from opt-in
users, line 4 – the treatment of clients, and line 5 –
the process for combining the probability estimates
obtained from the two groups. The only distinction
between opt-in users and clients in terms of privacy
guarantees provided is the curator model – trusted
curator and local model, respectively. Other than
that, both types of users are assumed to desire the
same level of (, δ)-diﬀerential privacy.
We will detail our choices for the privatization
sub-algorithms and discuss their privacy proper-
ties next. A key feature of Blender, however,
is that its privacy properties do not depend on
the speciﬁc choices of the sub-algorithms. That
is, as long as CreateHeadList, EstimateOptin-
Probabilities, and EstimateClientProbabili-
ties each satisfy (, δ)-diﬀerential privacy in its re-
spective curator model, then so does Blender. This
allows changing the sub-algorithms if better versions
(utility-wise or implementation-wise) are discovered
in the future. Among the parameters of Blender,
the ﬁrst four (the privacy parameters and the sets
of opt-in and client users) can be viewed as given
externally, whereas the following ﬁve (the number of
records collected from each user and the distribution
of the privacy budget among the sub-algorithms’
sub-components) can be viewed as knobs the de-
signer of Blender is at liberty to tweak in order
to improve the overall utility of Blender’s results.
2.2 Overview of Blender Sub-Algorithms
We now present the speciﬁc choices we made for the
sub-algorithms in Blender. Detailed technical dis-
cussions of their properties follow in Section 3.
Algorithms for Head List Creation and Prob-
ability Estimation Based on Opt-in User Data
(Figures 3, 4): The opt-in users are partitioned
into two sets – S, whose data will be used for initial
head list creation, and T , whose data will be used to
estimate the probabilities and variances of records
from the formed initial head list.
The initial head list creation algorithm, described
in Figure 3, constructs the list in a diﬀerentially pri-
vate manner using search record data from group S.
The goal of the algorithm is to approximate the true
set of most frequently searched and clicked search
records as closely as possible, while ensuring diﬀer-
ential privacy. The algorithm follows the strategy
introduced in [26] by aggregating the records of the
Blender (, δ, O, C, mO, mC , fO, fC , M )
Parameters:
spectively.
from each opt-in / client user, respectively.
• , δ: the diﬀerential privacy parameters.
• O, C: the set of opt-in users and clients, re-
• mO, mC : the max number of records to collect
• fO: the fraction of the opt-in users to use in
head list creation (the remainder are used to
estimate the record probabilities).
• fC : the fraction of the clients’ privacy budget
to allocate to queries (as opposed to URLs).
• M : the maximum size of the ﬁnalized head list.
Variables:
• HLS, HL: a map from each query to its corre-
sponding set of URLs.
• ˆpO, ˆσ2
O, ˆpC , ˆσ2
C : vectors indexed by records in
HL (and, overloaded to be indexed by queries
in HL as well) containing the probability es-
timates and variance estimates for each record
(and query).
Body
1: Arbitrarily partition O into S and T = O \ S, such
that |S| = fO|O| and |T| = (1 − fO)|O|.
2: let HLS = CreateHeadList(, δ, S, mO) be the
initial head list of records computed based on opt-in
users’ data.
3: let (cid:104)HL, ˆpO, ˆσ2
O(cid:105) = EstimateOptinProbabili-
ties(, δ, T, mO, HLS , M ) be the reﬁned head list
of records, their estimated probabilities, and esti-
mated variances based on opt-in users’ data.
4: let
= EstimateClientProbabili-
ties(, δ, C, mC , fC , HL) be the estimated record
probabilities and estimated variances based on
client reports.
(cid:104)ˆpC , ˆσ2
C(cid:105)
5: let ˆp = BlendProbabilities(ˆpO, ˆσ2
O, ˆpC , ˆσ2
C , HL)
be the combined estimate of record probabilities.
6: return HL, ˆp.
Figure 2: Blender, the server algorithm that coordinates the
privatization, collection, and aggregation of data from all users.
opt-in users from S, and including in the head list
those records whose noisy count exceeds a thresh-
old. The noise to add to the true counts and the
threshold to use are calibrated to ensure diﬀerential
privacy, using [24].
Our algorithm diﬀers from previous work in two
ways: 1) it replaces the collection and threshold-
ing of queries with the collection and thresholding
of records (i.e., query - URL pairs) and 2) its deﬁ-
nition of neighboring databases is that of databases
diﬀering in values of one user’s records, rather than
USENIX Association
26th USENIX Security Symposium    751
CreateHeadList(, δ, S, mO)
Parameters:
EstimateOptinProbabilities(, δ, T, mO, HLS , M )
Parameters:
• , δ: the diﬀerential privacy parameters.
• S: a set of opt-in users.
• mO: the maximum number of records to collect
from each opt-in user.
Body
r appears in the given dataset D.
1: let N (r, D) = number of times an arbitrary record
2: for each user i ∈ S do
3:
let DS,i be the database aggregating at most
mO arbitrary records from i.

.
7: τ = bs ·(cid:0)ln(exp( 
4: let DS be the concatenation of all DS,i databases.
5: let HLS be an empty map.
6: bS = 2mO
8: Assert τ ≥ 1.
9: for each distinct (cid:104)q, u(cid:105) ∈ DS do
10:
2 ) + mO − 1) − ln(δ)(cid:1).
Laplace distribution with scale bS centered at 0.
let Y be an independent draw from Lap(bS ), i.e.,
if N ((cid:104)q, u(cid:105), DS ) + Y > τ then
Add q to HLS if q (cid:54)∈ HLS .
Append u to HLS [q].
11:
12:
13:
14: Add (cid:104)(cid:63), (cid:63)(cid:105) to HLS .
15: return HLS .
Figure 3: Algorithm for creating the head list from a portion
opt-in users in a privacy-preserving way.
in the addition or removal of records of one user.
These necessitate the choice of mO = 1, as well as
higher values for noise and threshold than in [24].
We introduce a wildcard record (cid:104)(cid:63), (cid:63)(cid:105) to represent
records not included in the head list, for the subse-
quent task of estimating their aggregate probability.
For each record included in the initial head list,
the algorithm described in Figure 4 uses the remain-
ing opt-in users’ data (from set T ) to diﬀerentially
privately estimate their probabilities, denoted by ˆpO.
This algorithm is the standard Laplace mechanism
from the diﬀerential privacy literature [10], with
scale of noise calibrated to output sensitivity due to
our deﬁnition of neighboring datasets. Our imple-
mentation ensures (, 0)-diﬀerential privacy, which
is a more stringent privacy guarantee than for any
non-zero δ. We need to set mO = 1 for the pri-
vacy guarantees to hold, because we treat data at
the search record rather than query level.
We form the ﬁnal head list from the M most
frequent records in ˆpO. Finally, the head list is
passed to the client group, and the head list and
its probability and variance estimates are passed to
the BlendProbabilities step of Blender.
The choice of how to split opt-in users into the
sub-groups of S and T and the choice of M are un-
• , δ:
the diﬀerential privacy parameters.
In
fact, this algorithm achieves (, 0)-diﬀerential
privacy, which is a stricter privacy guarantee
than (, δ)-diﬀerential privacy, for all δ > 0.
from each opt-in user.
• T : a set of opt-in users.
• mO: the maximum number of records to collect
• HLS:
the initial head list of records whose
• M : the maximum size of the ﬁnalized head list.
probabilities are to be estimated.
Body
r appears in the given dataset D.
1: let N (r, D) = number of times an arbitrary record
2: for each user i ∈ T do
3:
let DT,i be the database aggregating at most
mO arbitrary records from i.
4: let DT be the concatenation of all DT,i databases.
5: Transform any record (cid:104)q, u(cid:105) ∈ DT that doesn’t ap-
pear in HLS into (cid:104)(cid:63), (cid:63)(cid:105).
6: let ˆpO be a vector indexed by records in HLS con-
taining the respective probability estimates.
.

7: let ˆσ2
O be a vector indexed by records in HLS con-
taining variance estimates of the respective proba-
bility estimate.
8: Denote |DT | as the total number of records in DT .
9: let bT = 2mO
10: for each (cid:104)q, u(cid:105) ∈ HLS do
11:
12:
let Y be an independent draw from Lap(bT ).
ˆpO,(cid:104)q,u(cid:105) = 1|DT | (N ((cid:104)q, u(cid:105), DT ) + Y ).
ˆσ2
O,(cid:104)q,u(cid:105) =
|DT |·(|DT |−1) .
14: let HL map the M queries with the highest esti-
mated marginal probabilities (according to ˆpO) to
their respective sets of URLs.
ˆpO,(cid:104)q,u(cid:105)(1− ˆpO,(cid:104)q,u(cid:105))
|DT |−1
2b2
T
13:
+
15: For the records not retained in HL, accumulate
their estimated probabilities into ˆpO,(cid:104)(cid:63),(cid:63)(cid:105) and up-
date ˆσ2
O,(cid:104)(cid:63),(cid:63)(cid:105) as in line 13.
16: return HL, ˆpO, ˆσ2
O.
Figure 4: Algorithm for privacy-preserving estimation of prob-
abilities of records in the head list from a portion of opt-in users.
related to privacy constraints, and can be made by
Blender’s developer to optimize utility goals, as
will be discussed in Section 4.2.1.
The technical discussions of the algorithms’ pri-
vacy properties and variance estimate computations
follow in Section 3.1 and Section 3.3.
Algorithms for client data collection (Fig-
ures 5, 6): For privatization of client data, the
records are no longer treated as a single entity, but
rather in a two-stage process: ﬁrst privatizing the
query, then privatizing the URL. This choice is in-
tended to beneﬁt utility as the number of queries is
752    26th USENIX Security Symposium
USENIX Association
EstimateClientProbabilities(, δ, C, mC , fC , HL)
Parameters:
LocalAlg(, δ, mC , fC , HL)
Parameters:
• , δ: the diﬀerential privacy parameters.
• C: the set of clients.
• mC : the number of records to collect from the
• fC : the fraction of the privacy budget to allo-
• HL: a map from each query to its correspond-
cate to reporting queries.
client.
ing set of URLs.
Body
1: Append query q = (cid:63) to HL.
2: for each query q ∈ HL do
3:
4: for each client i ∈ C do
5:
Append URL u = (cid:63) to HL[q].
let DC,i = LocalAlg(, δ, mC , fC , HL) be the
reports from i’s local execution of LocalAlg.
6: let DC be the concatenation of all reported client
datasets, DC,i.
7: Denote |DC| as the total number of records in DC .
U , k, t, kq, tq(∀q ∈ HL) be
8: let variables (cid:48)
Q, δ(cid:48)
U , δ(cid:48)
Q, (cid:48)
deﬁned as in lines 2–4 of LocalAlg.
C be vectors indexed by records in HL
(and overloading its use, also indexed by queries).