### 6. Implementation Overview

We now provide an overview of the implementation of Hopper in both decentralized and centralized settings.

#### 6.1 Decentralized Implementation

Our decentralized implementation leverages the Sparrow framework [36], which comprises multiple schedulers and workers, with one each on every machine [9]. Any number of schedulers can operate concurrently; we use 10 in our experiments. Schedulers facilitate job submissions using Thrift RPCs [1].

A job is divided into a set of tasks, along with their dependencies (DAG), binaries, and locality preferences. The scheduler places task requests at the workers. If a task has locality constraints, its requests are only placed on workers that meet those constraints [13, 40, 49]. Workers communicate with client executor processes (e.g., Spark executors). These executors are responsible for running task binaries and are designed to be long-lived to minimize startup overheads (see [36] for a more detailed explanation).

Our implementation modifies both the scheduler and the worker. The workers implement the core guidelines from §4, determining if the system is slot-constrained and prioritizing jobs based on their virtual sizes. This required modifying the FIFO queue in Sparrow to allow for custom ordering of queued requests. The worker also enhances its local view by coordinating with the scheduler, involving modifications to the "late binding" mechanism at both the worker and scheduler levels. When a worker has a free slot, it collaborates with the scheduler to select the next task (using Pseudocode 3). The scheduler then handles the worker's response as per Pseudocode 2.

Even after all tasks of a job have been scheduled (including its virtual size), the job scheduler does not cancel pending requests. This allows for additional pending requests with any probe ratio over one, enabling the system to utilize more slots if it is not slot-constrained (as per Guideline 3).

In the decentralized implementation, for tasks in the input phase (e.g., map phase), when the number of probes exceeds the number of data replicas, we queue up the additional requests at randomly chosen machines. Consequently, these tasks may run without data locality, and our results in §7 include such losses in locality.

#### 6.2 Centralized Implementation

We implemented Hopper within two centralized frameworks: Hadoop YARN (version 2.3) and Spark (version 0.7.3). Hadoop jobs read data from HDFS [5], while Spark jobs read from in-memory RDDs.

These frameworks use a two-level scheduling approach, where a central resource manager assigns slots to different job managers. When a job is submitted to the resource manager, a job manager is started on one of the machines, which then executes the job's DAG of tasks. The job manager negotiates with the resource manager for resources for its tasks.

We built Hopper as a scheduling plug-in module for the resource manager, allowing the frameworks to use our design for slot allocation to job managers. We also piggybacked on the communication protocol between the job manager and resource manager to communicate the intermediate data produced and read by the job phases, adjusting α accordingly. Locality and other preferences are already communicated between them.

#### 6.3 Estimating Intermediate Data Sizes

Recall from §4.2 that our scheduling guidelines recommend scaling every job’s allocation by \(\sqrt{\alpha}\) in the case of DAGs. This scaling aims to capture the pipelining of upstream tasks' outputs. The key to calculating \(\alpha\) is estimating the size of the intermediate output produced by tasks. Unlike the job's input size, intermediate data sizes are not known upfront. We predict intermediate data sizes based on similar past jobs. Clusters typically have many recurring jobs that execute periodically as new data streams in, producing intermediate data of similar sizes.

Our simple approach to estimating \(\alpha\) works sufficiently well for our evaluations, achieving an average accuracy of 92%. However, workloads without many multi-waved or recurring jobs, and without tasks whose duration is dictated by their input sizes, require more sophisticated models of task executions.

### 7. Evaluation

We evaluate our prototypes of Hopper—both decentralized and centralized—on a 200-machine cluster. We focus on the overall gains of the decentralized prototype in §7.2 and evaluate the design choices that led to Hopper in §7.3. In §7.4, we assess the gains with Hopper in a centralized scheduler to highlight the value of coordinating scheduling and speculation. The key highlights are:

1. Hopper’s decentralized prototype improves the average job duration by up to 66% compared to an aggressive decentralized baseline that combines Sparrow with SRPT (§7.2).
2. Hopper ensures that only 4% of jobs slow down compared to Fair scheduling, and those that do slow down do so by ≤ 5% (§7.3).
3. Centralized Hopper improves job completion times by 50% compared to centralized SRPT (§7.4).

#### 7.1 Setup

**Cluster Deployment:** We deploy our prototypes on a 200-node private cluster. Each machine has 16 cores, 34GB of memory, a 1Gbps network, and 4 disks. The machines are connected using a network with no oversubscription.

**Workload:** Our evaluation runs jobs from traces of Facebook’s production Hadoop [3] cluster (3,500 machines) and Microsoft Bing’s Dryad cluster (O(1000) machines) from October to December 2012. The traces include a mix of experimental and production jobs with diverse resource demands for CPU, memory, and I/O, varying by a factor of 24× (refer to [27] for detailed quantification). We retain the inter-arrival times of jobs, their input sizes, number of tasks, resource demands, and job DAGs of tasks. Job sizes follow a heavy-tailed distribution (quantified in detail in [12]). Each experiment is a replay of a representative 6-hour slice from the trace, repeated five times, and we report the median.

To evaluate our decentralized Hopper prototype, we use in-memory Spark [49] jobs, typical of interactive analytics with task durations ranging from sub-second to a few seconds. Since the performance of any decentralized scheduler depends on cluster utilization, we appropriately speed up the trace and evaluate utilizations between 60% and 90%, consistent with Sparrow [36].

**Stragglers:** The stragglers in our experiments occur naturally, not injected via any probability distribution model or statistics from Facebook and Bing clusters. The frequency and lengths of stragglers observed in our evaluations are consistent with prior studies, e.g., [14, 15, 50]. While Hopper’s focus is not on improving straggler mitigation algorithms, our experiments emphasize the importance of such mitigation.

**Baseline:** We compare decentralized Hopper to Sparrow-SRPT, an augmented version of Sparrow [36]. Like Sparrow, it performs decentralized scheduling using a “batched” power-of-two choices and includes an SRPT heuristic. When a worker has a free slot, it picks the task of the job with the least unfinished tasks (instead of the standard FIFO ordering in Sparrow). Finally, we combine Sparrow with LATE [50] using “best effort” speculation (§3); we do not consider “budgeted” speculation due to the difficulty of picking a fixed budget.

The combination of Sparrow-SRPT and LATE performs strictly better than Sparrow and serves as an aggressive baseline. Our improvements over this benchmark highlight the importance of coordinating scheduling and speculation.

We compare centralized Hopper to a centralized SRPT scheduler with LATE speculation. This is an aggressive baseline since it sacrifices fairness for performance. Thus, improvements can be interpreted as coming solely from better coordination of scheduling and speculation.

#### 7.2 Decentralized Hopper’s Improvements

In our experiments, unless otherwise stated, we set the fairness allowance \(\epsilon\) as 10%, the probe ratio as 4, and the speculation algorithm for every job to be LATE [50]. Our estimation of \(\alpha\) (§6.3) has an average accuracy of 92%. As the workload executes, we continually fit the parameter \(\beta\) of task durations based on completed tasks (including stragglers); the error in \(\beta\)'s estimate falls to ≤ 5% after just 6% of the jobs have executed.

**Overall Gains:** Figure 6 plots Hopper’s gains for varying utilizations, compared to stock Sparrow and Sparrow-SRPT. Jobs, overall, speed up by 50%−60% at a utilization of 60%. The gains compared to Sparrow are marginally better than Sparrow-SRPT. When the utilization goes over 80%, Hopper’s gains compared to both are similar. An interesting point is that Hopper’s gains with the Bing workload in Figure 6b are slightly higher (difference of 7%), perhaps due to the larger difference in job sizes between small and large jobs, allowing more opportunities for Hopper. Gains fall to 70% at higher percentiles. Encouragingly, gains even at the 10th percentile are 15% and 10%, showing Hopper’s ability to improve even worst-case performance.

**DAG of Tasks:** The scripts in our Facebook (Hive scripts [7]) and Bing (Scope [20]) workloads produce DAGs of tasks that often pipeline data transfers of downstream phases with upstream tasks [6]. The communication patterns in the DAGs are varied (e.g., all-to-all, many-to-one, etc.), underscoring Hopper’s generality. As Figure 8b shows, Hopper’s gains hold across DAG lengths.

**Speculation Algorithm:** We experimentally evaluate Hopper’s performance with different speculation mechanisms. LATE [50] is deployed in Facebook’s clusters, Mantri [15] is in operation in Microsoft Bing, and GRASS [citegrass] is a recently reported straggler mitigation system that performs near-optimal speculation. Our experiments still use Sparrow-SRPT as the baseline but pair it with different straggler mitigation algorithms. Figure 9 plots the results. While the earlier results were achieved with LATE, a remarkable point about Figure 9 is the similarity in gains even with Mantri and GRASS. This indicates that as long as the straggler mitigation algorithms are aggressive in asking for speculative copies, Hopper will appropriately balance speculation and scheduling. Overall, it emphasizes that resource allocation across jobs (with speculation) has a higher performance value than straggler mitigation within jobs.

#### 7.3 Evaluating Hopper’s Design Decisions

We now evaluate the sensitivity of decentralized Hopper to our key design decisions: fairness and probe ratio.

**Fairness:** As described in §4.3, the fairness knob \(\epsilon\) decides the leeway for Hopper to trade-off fairness for performance. We set \(\epsilon\) to 10% of the perfectly fair share of a job (ratio of total slots to jobs) and analyze its sensitivity to Hopper’s gains. Figure 10a plots the increase in gains as we increase \(\epsilon\) from 0 to 30%. The gains quickly rise for small values of \(\epsilon\), and beyond \(\epsilon = 15%\), the increase in gains is flatter for both the Facebook and Bing workloads. Conservatively, we set \(\epsilon\) to 10%.

An important concern is the amount of slowdown of jobs compared to a perfectly fair allocation (\(\epsilon = 0\)), i.e., when all jobs are guaranteed their fair share at all times. Any slowdown of jobs is due to receiving fewer slots. Figure 10b measures the number of jobs that slowed down, and for the slowed jobs, Figure 10c plots their average and worst slowdowns. Note that fewer than 4% of jobs slow down with Hopper compared to a fair allocation at \(\epsilon = 10%\). The corresponding number for the Bing workload is 3.8%.

In fact,