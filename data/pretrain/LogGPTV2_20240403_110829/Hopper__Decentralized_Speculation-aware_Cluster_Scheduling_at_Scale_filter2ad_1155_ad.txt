have heterogeneous α and β.
6.
IMPLEMENTATION OVERVIEW
We now give an overview of the implementation of
Hopper in decentralized and centralized settings.
6.1 Decentralized Implementation
Our decentralized implementation uses the Sparrow
[36] framework, which consists of many schedulers and
workers (one each on every machine) [9]. Arbitrarily
many schedulers can operate concurrently; though we
use 10 in our experiments. Schedulers allow submissions
of jobs using Thrift RPCs [1].
A job is broken into a set of tasks with their depen-
dencies (DAG), binaries and locality preferences. The
scheduler places requests at the workers for its tasks;
if a task has locality constraints, its requests are only
placed on the workers meeting its constraints [13, 40,
49]. The workers talk to the client executor processes
(e.g., Spark executor). The executor processes are re-
sponsible for executing task binaries and are long-lived
to avoid startup overheads (see [36] for a more detailed
explanation).
Our implementation modiﬁes the scheduler as well
as the worker. The workers implement the core of the
guidelines in §4 – determining if the system is slot-
constrained and accordingly prioritizing jobs as per their
virtual sizes. This required modifying the FIFO queue
at the worker in Sparrow to allow for custom ordering
of the queued requests. The worker, nonetheless, aug-
ments its local view by coordinating with the scheduler.
This involved modifying the “late binding” mechanism
both at the worker and scheduler. The worker, when it
has a free slot, works with the scheduler in picking the
next task (using Pseudocode 3). The scheduler deals
with a response from the worker as per Pseudocode 2.
Even after all the job’s tasks have been scheduled
(including its virtual size), the job scheduler does not
“cancel” its pending requests; there will be additional
pending requests with any probe ratio over one. Thus,
if the system is not slot-constrained, it would be able to
use more slots (as per Guideline 3).
In our decentralized implementation, for tasks in the
input phase (e.g., map phase), when the number of
probes exceeds the number of data replicas, we queue up
the additional requests at randomly chosen machines.
Consequently, these tasks may run without data local-
ity, and our results in §7 include such loss in locality.
6.2 Centralized Implementation
We implement Hopper inside two centralized frame-
works: Hadoop YARN (version 2.3) and Spark (version
0.7.3). Hadoop jobs read data from HDFS [5] while
Spark jobs read from in-memory RDDs.
Brieﬂy, these frameworks implement two level schedul-
ing where a central resource manager assigns slots to
the diﬀerent job managers. When a job is submitted
to the resource manager, a job manager is started on
one of the machines, that then executes the job’s DAG
of tasks. The job manager negotiates with the resource
manager for resources for its tasks.
We built Hopper as a scheduling plug-in module to the
resource manager. This makes the frameworks use our
design to allocate slots to the job managers. We also
piggybacked on the communication protocol between
the job manager and resource manager to communicate
the intermediate data produced and read by the phases
of the job to vary α accordingly; locality and other pref-
erences are already communicated between them.
6.3 Estimating Intermediate Data Sizes
Recall from §4.2 that our scheduling guidelines rec-
√
ommend scaling every job’s allocation by
α in the
case of DAGs. The purpose of the scaling is to capture
pipelining of the reading of upstream tasks’ outputs.
The key to calculating α is estimating the size of
the intermediate output produced by tasks. Unlike the
job’s input size, intermediate data sizes are not known
upfront. We predict intermediate data sizes based on
similar jobs in the past. Clusters typically have many
recurring jobs that execute periodically as newer data
streams in, and produce intermediate data of similar
sizes.
Our simple approach to estimating α works suﬃciently
well for our evaluations (accuracy of 92%, on average).
However, we realize that workloads without many multi-
waved or recurring jobs and without tasks whose dura-
tion is dictated by their input sizes, need more sophis-
ticated models of task executions.
7. EVALUATION
We evaluate our prototypes of Hopper – with both
decentralized and centralized scheduling – on a 200 ma-
chine cluster. We focus on the overall gains of the decen-
tralized prototype of Hopper in §7.2 and evaluate the de-
sign choices that led to Hopper in §7.3. Then, in §7.4 we
evaluate the gains with Hopper in a centralized scheduler
in order to highlight the value of coordinating schedul-
ing and speculation. The key highlights are:
1. Hopper’s decentralized prototype improves the av-
erage job duration by up to 66% compared to an
aggressive decentralized baseline that combines Spar-
row with SRPT (§7.2).
3872. Hopper ensures that only 4% of jobs slow down
compared to Fair scheduling, and jobs which do
slow down do so by ≤ 5% (§7.3).
3. Centralized Hopper improves job completion times
by 50% compared to centralized SRPT (§7.4).
7.1 Setup
Cluster Deployment: We deploy our prototypes on
a 200-node private cluster. Each machine has 16 cores,
34GB of memory, 1Gbps network and 4 disks. The
machines are connected using a network with no over-
subscription.7
Workload: Our evaluation runs jobs in traces from
Facebook’s production Hadoop [3] cluster (3, 500 ma-
chines) and Microsoft Bing’s Dryad cluster (O(1000)
machines) from Oct-Dec 2012. The traces consist of a
mix of experimental and production jobs. Their tasks
have diverse resource demands of CPU, memory and
IO, varying by a factor of 24× (refer to [27] for de-
tailed quantiﬁcation). We retain the inter-arrival times
of jobs, their input sizes and number of tasks, resource
demands, and job DAGs of tasks. Job sizes follow a
heavy-tailed distribution (quantiﬁed in detail in [12]).
Each experiment is a replay of a representative 6 hour
slice from the trace. It is repeated ﬁve times and we
report the median.
To evaluate our prototype of decentralized Hopper, we
use in-memory Spark [49] jobs. These jobs are typical of
interactive analytics whose tasks vary from sub-second
durations to a few seconds. Since the performance of
any decentralized scheduler depends on the cluster uti-
lization, we speed-up the trace appropriately, and eval-
uate on (average) utilizations between 60% and 90%,
consistent with Sparrow [36].
Stragglers: The stragglers in our experiments are those
that occur naturally, i.e., not injected via any model of
a probability distribution or via statistics gathered from
the Facebook and Bing clusters. Importantly, the fre-
quency and lengths of stragglers observed in our eval-
uations are consistent with prior studies, e.g., [14, 15,
50]. While Hopper’s focus is not on improving straggler
mitigation algorithms, our experiments certainly serve
to emphasize the importance of such mitigation.
Baseline: We compare decentralized Hopper to Sparrot-
SRPT, an augmented version of Sparrow [36]. Like
Sparrow, it performs decentralized scheduling using a
“batched” power-of-two choices. In addition, it also in-
cludes an SRPT heuristic.
In short, when a worker
has a slot free, it picks the task of the job that has the
least unﬁnished tasks (instead of the standard FIFO or-
dering in Sparrow). Finally, we combine Sparrow with
LATE [50] using“best eﬀort”speculation (§3); we do not
consider “budgeted” speculation due to the diﬃculty of
picking a ﬁxed budget.
The combination of Sparrow-SRPT and LATE per-
forms strictly better than Sparrow, and serves as an ag-
7Results with a 10Gbps network are qualitatively similar.
(a) Facebook
(b) Bing
Figure 6: Hopper’s gains with cluster utilization.
(a) Facebook
(b) Bing
Figure 7: Hopper’s gains by job bins over Sparrow-
SRPT.
gressive baseline. Our improvements over this aggres-
sive benchmark highlight the importance of coordinat-
ing scheduling and speculation.
We compare centralized Hopper to a centralized SRPT
scheduler with LATE speculation. Again, this is an
aggressive baseline since it sacriﬁces fairness for perfor-
mance. Thus, improvements can be interpreted as com-
ing solely from better coordination of scheduling and
speculation.
7.2 Decentralized Hopper’s Improvements
In our experiments, unless otherwise stated, we set
the fairness allowance  as 10%, probe ratio as 4 and
speculation algorithm in every job to be LATE [50].
Our estimation of α (§6.3) has an accuracy of 92% on
average. As the workload executes, we also continu-
ally ﬁt the parameter β of task durations based on the
completed tasks (including stragglers); the error in β’s
estimate falls to ≤ 5% after just 6% of the jobs have
executed.
Overall Gains: Figure 6 plots Hopper’s gains for vary-
ing utilizations, compared to stock Sparrow and Sparrow-
SRPT. Jobs, overall, speedup by 50% − 60% at uti-
lization of 60%. The gains compared to Sparrow are
marginally better than Sparrow-SRPT. When the uti-
lization goes over 80%, Hopper’s gains compared to both
are similar. An interesting point is that Hopper’s gains
with the Bing workload in Figure 6b are a touch higher
(diﬀerence of 7%), perhaps due to the larger diﬀerence
in job sizes between small and large jobs, allowing more
opportunity for Hopper. Gains fall to  70% gains
at higher percentiles. Encouragingly, gains even at the
10th percentile are 15% and 10%, which shows Hopper’s
ability to improve even worse case performance.
DAG of Tasks: The scripts in our Facebook (Hive
scripts [7]) and Bing (Scope [20]) workloads produce
DAGs of tasks which often pipeline data transfers of
downstream phases with upstream tasks [6]. The com-
munication patterns in the DAGs are varied (e.g., all-
to-all, many-to-one etc.) and thus the results also serve
to underscore Hopper’s generality. As Figure 8b shows,
Hopper’s gains hold across DAG lengths.
Figure 9: Hopper’s results are independent of the
straggler mitigation strategy.
Speculation Algorithm: We now experimentally eval-
uate Hopper’s performance with diﬀerent speculation me-
chanisms. LATE [50] is deployed in Facebook’s clus-
ters, Mantri [15] is in operation in Microsoft Bing, and
GRASS citegrass is a recently reported straggler miti-
gation system that was demonstrated to perform near-
optimal speculation. Our experiments still use Sparrow-
SRPT as the baseline but pair with the diﬀerent strag-
gler mitigation algorithms. Figure 9 plots the results.
While the earlier results were achieved in conjunction
with LATE, a remarkable point about Figure 9 is the
similarity in gains even with Mantri and GRASS. This
indicates that as long as the straggler mitigation algo-
rithms are aggressive in asking for speculative copies,
Hopper will appropriately balance speculation and sche-
duling. Overall, it emphasizes the aspect that resource
allocation across jobs (with speculation) has a higher
performance value than straggler mitigation within jobs.
7.3 Evaluating Hopper’s Design Decisions
We now evaluate the sensitivity of decentralized Hop-
per to our key design decisions: fairness and probe ratio.
Fairness: As we had described in §4.3, the fairness
knob  decides the leeway for Hopper to trade-oﬀ fairness
for performance. Thus far, we had set  to be 10% of
the perfectly fair share of a job (ratio of total slots to
jobs), now we analyze its sensitivity to Hopper’s gains.
Figure 10a plots the increase in gains as we increase
 from 0 to 30%. The gains quickly rise for small values
of , and beyond  = 15% the increase in gains are ﬂat-
ter with both the Facebook as well as Bing workloads.
Conservatively, we set  to 10%.
An important concern, nonetheless, is the amount of
slowdown of jobs compared to a perfectly fair allocation
( = 0), i.e., when all the jobs are guaranteed their fair
share at all times. Any slowdown of jobs is because of
receiving fewer slots. Figure 10b measures the number
of jobs that slowed down, and for the slowed jobs, Fig-
ure 10c plots their average and worst slowdowns. Note
that fewer than 4% of jobs slow down with Hopper com-
pared to a fair allocation at  = 10%. The correspond-
ing number for the Bing workload is 3.8%.
In fact,