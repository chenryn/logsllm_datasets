title:OpenTM: Traffic Matrix Estimator for OpenFlow Networks
author:Amin Tootoonchian and
Monia Ghobadi and
Yashar Ganjali
OpenTM: Traﬃc Matrix Estimator for
OpenFlow Networks
Amin Tootoonchian, Monia Ghobadi, and Yashar Ganjali
Department of Computer Science
University of Toronto, Toronto, ON, Canada
{amin,monia,yganjali}@cs.toronto.edu
Abstract. In this paper we present OpenTM, a traﬃc matrix estima-
tion system for OpenFlow networks. OpenTM uses built-in features pro-
vided in OpenFlow switches to directly and accurately measure the traﬃc
matrix with a low overhead. Additionally, OpenTM uses the routing in-
formation learned from the OpenFlow controller to intelligently choose
the switches from which to obtain ﬂow statistics, thus reducing the load
on switching elements. We explore several algorithms for choosing which
switches to query, and demonstrate that there is a trade-oﬀ between
accuracy of measurements, and the worst case maximum load on indi-
vidual switches, i.e., the perfect load balancing scheme sometimes results
in the worst estimate, and the best estimation can lead to worst case load
distribution among switches. We show that a non-uniform distribution
querying strategy that tends to query switches closer to the destina-
tion with a higher probability has a better performance compared to the
uniform schemes. Our test-bed experiments show that for a stationary
traﬃc matrix OpenTM normally converges within ten queries which is
considerably faster than existing traﬃc matrix estimation techniques for
traditional IP networks.
1 Introduction
A traﬃc matrix (TM) represents the volume of traﬃc between origin-destination
(OD) pairs in a network. Estimating the point-to-point TM in a network is an
essential part of many network design and operation tasks such as capacity
planning, routing protocol conﬁguration, network provisioning, load balancing,
and anomaly detection.
Direct and precise measurement of TM in large IP networks is extremely
diﬃcult, if not infeasible, due to the large number of OD pairs, the high volume
of traﬃc at each link, and the lack of measurement infrastructure [1]. Previous
works infer the TM (a) indirectly from link loads [2,3], (b) directly from sampled
ﬂow statistics (e.g., using Cisco NetFlow) [4, 5], or (c) using a combination of
both [1]. Indirect methods are sensitive to the statistical assumptions made in
their models and are shown to have large errors [6]. Direct methods can be
quite attractive due to their high accuracy levels. However, the lack of required
measurement infrastructure and the prohibitively large overhead imposed on the
network components are two main drawbacks of direct measurements.
A. Krishnamurthy and B. Plattner (Eds.): PAM 2010, LNCS 6032, pp. 201–210, 2010.
© Springer-Verlag Berlin Heidelberg 2010
202
A. Tootoonchian, M. Ghobadi, and Y. Ganjali
In this paper, we revisit the TM estimation problem using direct measure-
ments in the context of OpenFlow-based networks [7]. OpenFlow is an open
standard that makes any changes to the network control plane very easy by sep-
arating the data and control planes. An OpenFlow network consists of OpenFlow
switches (data plane) managed by a logically centralized OpenFlow controller
(control plane) which has a network-wide view. OpenFlow’s unique features re-
move the prohibitive cost of direct measurements for TM estimation. Unlike com-
modity switches, ﬂow level operations are streamlined into OpenFlow switches
which lets us query for ﬂow statistics, enabling access to accurate ﬂow statistics.
Taking advantage of these features, we have designed OpenTM, a TM esti-
mator for OpenFlow networks. OpenTM reads byte and packet counters kept by
OpenFlow switches for active ﬂows and therefore incurs a minimal overhead on
network elements. At the same time the highest level of accuracy is preserved, be-
cause the TM is derived directly without making any simplifying mathematical
and statistical assumptions. Our work shows the possibility of direct measure-
ment of TM with the least overhead as long as the infrastructure (OpenFlow
here) provides the appropriate feature set for measurements. We note that the
scope of our work is limited to the networks where OpenFlow can be deployed,
i.e., where maintaining per-ﬂow counters is likely tractable.
For diﬀerent ﬂows, OpenTM can query any switch along the ﬂow path. This
choice, however, can aﬀect the accuracy of the measurement as well as the load
on individual switches. We present several strategies for choosing which switch
to query at any point of time. Even though all these schemes result in TM
estimations with very small errors, our analysis and experiments show that there
is a trade-oﬀ between the accuracy of the TM measurements and the maximum
query load on individual switches, i.e., the perfect query distribution sometimes
results in the worst estimate, and the best estimation can lead to the worst query
distribution amongst switches.
We have implemented OpenTM as an application for NOX [8], an open-source
OpenFlow controller. We study OpenTM’s performance on a small testbed of
OpenFlow switches. Even though using a small testbed for evaluation has its own
shortcomings, we believe that most results would not be signiﬁcantly diﬀerent
in larger networks. Our results show that in a system with a stationary traﬃc
matrix, OpenTM normally converges within 10 queries to a value within 3% of
the average rate which is notably faster than existing techniques for traditional
IP networks.
The contributions of this work are two-fold. First, we present the design and
implementation of OpenTM for OpenFlow-based networks. Based on the eval-
uation, we argue that low-overhead accurate TM estimation is feasible using
direct measurements in a setting where the switches keep track of ﬂow statistics.
Second, we explore the idea of constructing the TMs from switch-level measure-
ments, where the choice of which switch to query can be decided at runtime. To
the best of our knowledge, this is in contrast to the existing techniques that usu-
ally instrument all ingress/egress links leading to an very uneven measurement
OpenTM: Traﬃc Matrix Estimator for OpenFlow Networks
203
load on the boundary switches or routers (switches internal to the network have
a very little measurement load).
2 Design
Direct measurements in large traditional IP networks is prohibitively costly due
to the processing required to handle the large volume of traﬃc at each inter-
face [1]. On the other hand, OpenFlow switches keep track of active ﬂows in
the network and update per ﬂow counters. The measurement infrastructure that
OpenFlow provides enables direct and precise ﬂow measurements without packet
sampling or incurring any prohibitive overhead on switches. We take advantage
of these features to present OpenTM’s design in this section.
OpenTM’s logic is quite simple. It keeps track of all the active ﬂows in the
network, gets the routing information from the OpenFlow controller’s routing
application, discovers ﬂow paths, and periodically polls ﬂow byte and packet-
count counters from switches on the ﬂow path. Using the routing information,
OpenTM constructs the TM by adding up statistics for ﬂows originated from
the same source and destined to the same destination1. Using the information
available to an OpenFlow controller, OpenTM can create diﬀerent types of TMs
with diﬀerent aggregation levels for sources and destinations. Our implementa-
tion of OpenTM computes the TM for switches, but the implementation can be
easily augmented to derive other TM types described in [9].
The total number of queries generated by OpenTM during each querying in-
terval is bounded by the number of active ﬂows in the network. It is commonly
believed that the number of concurrently active ﬂows in large enterprise IP net-
works is small. According to the data from the 8000-host network at LBNL,
the total number of active ﬂows in their network never exceeds 1200 in any
second [10]. The data from the Stanford Computer Science and Electrical Engi-
neering network with 5500 active hosts shows that their number of active ﬂows
stays well below 10000 [11]. Currently, our system generates a single query for
a single source-destination IP pair. As an improvement, a single query can be
generated for all ﬂows sharing the same path, as long as the IP addresses could
be aggregated.
Diﬀerent switches on the path may observe diﬀerent rates for a given ﬂow due
to packet loss. We consider the last switch on the ﬂow path to be the point of
reference since this is what is seen by the receiver. Consequently, we query the
last switch on the path for the most accurate TM. However, this strategy im-
poses an uneven and substantially high amounts of load on the ﬁrst/last switches
and does not scale well. We expect to get close statistics if other switches on the
ﬂow path are queried since packet loss is negligible in enterprise networks (where
OpenFlow is designed for). Based on this observation, we propose diﬀerent switch
querying strategies: (a) querying the last switch, (b) querying switches on the
1 Multipath routing, routing changes or hot potato routing do not aﬀect the correct-
ness of OpenTM, because OpenTM coordinates with the controller routing applica-
tion to discover any change in ﬂow paths.
204
A. Tootoonchian, M. Ghobadi, and Y. Ganjali
ﬂow path uniformly at random, (c) round-robin querying, (d) non-uniform ran-
dom querying that tends to query switches closer to the destination with a higher
probability, and (e) querying the least loaded switch.
Querying the last switch results in the most accurate TM, but imposes a sub-
stantial load on edge switches. Uniform random querying of switching elements
of a given ﬂow’s path evenly distributes the load amongst switches as long as all
switches are equally capable. The price, however, is losing some accuracy. Round-
robin querying deterministically queries switches on a round-robin fashion. On
average, we expect both uniform random querying and round-robin querying
to behave similarly, but round-robin querying may result in synchronization in
querying, because the same switch might be queried by several ﬂows simulta-
neously. Using a non-uniform distribution for querying switches gives us control
over the accuracy and the load of OpenTM. A distribution which chooses last
switches in the path with a higher probability, results in a more accurate TM but
imposes more load on those switches. In our experiments, for non-uniform query-
ing, we randomly select two switch along the ﬂow path and query the one closer
to the destination. Querying the least loaded switch evenly distributes queries
among all switches in the network, contrary to the uniform random querying
method which only distributes queries among switches on individual ﬂow paths.
In Section 5, we compare these methods with each other.
The frequency at which OpenTM queries switches for statistics is another fac-
tor that directly aﬀects the accuracy and overhead of TM estimation. Querying
more frequently results in a more accurate TM but with the cost of added over-
head. Here we only consider ﬁxed length intervals for querying diﬀerent switches
for all the ﬂows. Switch querying interval can be adaptively adjusted for each
source-destination IP pair based on the ﬂow and network dynamics (e.g., round
trip time, available bandwidth). The relation between an eﬃcient querying fre-
quency and ﬂow and network dynamics is outside the scope of this work.
3 Implementation
We implemented OpenTM as a C++ application for NOX [8], an open-source
OpenFlow controller designed to simplify developing network applications. A
NOX application can get notiﬁed of all network events (e.g., ﬂow initiation and
termination), has access to the routing information, and can interact with the
switches in the network. NOX also lets applications interact with each other2.
In each querying interval, OpenTM queries the network for the statistics of
all active IP pairs. Element (i, j) in the TM is then computed by summing up
the ﬂow rates that are originated from switch i and are destined to switch j. We
note that the ﬂow statistic queries do not hit switches at the same time, because
ﬂow initiation among OD-pairs are not synchronized.
OpenTM starts querying for statistics periodically once it sees the ﬁrst ﬂow
between an OD-pair and stops querying once all the ﬂows between an OD-pair
are expired. To keep track of the number of active IP pairs in the network,
2 For instance, OpenTM exposes the real-time traﬃc matrix to other applications.
OpenTM: Traﬃc Matrix Estimator for OpenFlow Networks
205
OpenTM counts the number of TCP/UDP ﬂows between IP pairs. OpenTM in-
crements the mentioned counter upon receiving a Flow in event and decrements
it upon receiving the corresponding Flow expired event3.
Once the IP pair’s ﬂow count becomes one, OpenTM fetches the ﬂow path
(a list of switches) from the routing application and sends an aggregate statis-
tics query to a switch in the ﬂow path according the desired querying strategy.
OpenTM updates the TM when it gets the aggregate query statistics reply back
from the network. At this time, if the IP pair ﬂow count is non-zero, OpenTM
registers a callback function to query the network for the ﬂow statistics again
after a certain period of time4. An implicit assumption here is that all packets
ﬂowing from the same source to the same destination take the same path. This
enables us to query the same set of routers to get statistics for all ﬂows between
an IP pair.
OpenTM keeps track of switch loads, so it can choose the least loaded one
and optimally balance the queries among all of them. We use the number of
outstanding queries on each switch as the load metric. When a switch is queried,
a counter that keeps track of the number of outstanding requests on each switch
is incremented. Upon receiving the reply back, that counter is decremented.
This simple method captures the diﬀerence in the processing power of switches.
More capable switches can handle more requests and should get more queries
compared to the less capable ones. In the following section, we present the results
of our empirical evaluation based on our implementation.
4 Experiments and Results
In this section, we present real-time traﬃc measurements in a testbed to evaluate
OpenTM. We study the performance and convergence time of OpenTM. We also
compare diﬀerent switch querying schemes introduced in Section 2.
For our experiments, we use HP DL320 G5p servers equipped with an
HP NC326i PCIe dual-port gigabit network card running Debian Lenny and
OpenFlow-enabled NEC IP8800/S3640 switches. In all our experiments, we use
TCP cubic with the maximum advertised TCP window size set to 20MB. The
path MTU is 1500 bytes and the servers send maximum-sized packets. We use
the NetEm [12] to emulate network delay and loss, and use Iperf to generate
the input traﬃc.
Our testbed topology is illustrated in Figure 1(a), where Hi, 1 ≤ i ≤ 10, are
host machines, Sj, 1 ≤ j ≤ 10 are OpenFlow switches, and Lk, 1 ≤ k ≤ 3 are
loss emulator machines. Five OD pairs Hi-Hj are created in which host Hi sends
TCP traﬃc to host Hj. Speciﬁcally, we create 10 TCP ﬂows between each OD
pair H1-H10, H2-H9, H3-H4, H5-H6, and H7-H8. We add 100ms of delay on
the forward path of each ﬂow. The delay emulators are also conﬁgured to each
3 Both Flow in and Flow expired events are ﬁred by the NOX’s authenticator appli-
cation upon ﬂow initiation and termination, respectively. A ﬂow expires when the
switch does not see any packets belong to that ﬂow after a speciﬁc timeout.
4 The querying interval which is set to ﬁve seconds in our current implementation.
206
A. Tootoonchian, M. Ghobadi, and Y. Ganjali
(cid:94)(cid:1005)
(cid:44)
(cid:44)(cid:1005)
(cid:94)(cid:1005)(cid:1004)
(cid:44)(cid:1005)(cid:1004)
(cid:94)(cid:1006)
(cid:44)
(cid:44)(cid:1006)
(cid:94)(cid:1013)
(cid:44)(cid:1013)
(cid:94)(cid:1007)
(cid:44)
(cid:44)(cid:1007)
(cid:94)(cid:1012)
(cid:44)(cid:1012)
(cid:62)(cid:1005)
(cid:94)(cid:1008)
(cid:44)
(cid:44)(cid:1008)
(cid:62)(cid:1007)
(cid:94)(cid:1011)
(cid:44)(cid:1011)
(a)
(cid:44)(cid:1009)
(cid:44)(cid:1010)
(cid:62)(cid:1006)
(cid:94)(cid:1009)
(cid:94)(cid:1010)
)
s
p
b
M
(
e
t
a
r
↑
↑
↑
170
160
150
140
130
120
110
100
90
↑
↑
80
0
H
1
H
2
H
3
H
5
H
7
10
-H
-H
9
-H
4