assumption, malicious because Anubis does not support user
interaction, and it is very unlikely that a benign program
needs to send email without user approval. Similarly, we
can detect a network-based attack by matching the data that
is provided to the network send API call against network
intrusion detection signatures. Port scan or denial of service
attacks are captured by measuring the frequency of (failed)
outgoing connection attempts. Detection of other behaviors
requires us to take advantage of data ﬂow information. As
an example, a malware “dropper” (or update) behavior is
characterized as a data ﬂow from a network socket to a ﬁle
together with the fact that this ﬁle is later executed. In total,
we have manually developed nine speciﬁcations of high-
level behaviors that cover common malware activity. These
behaviors are discussed in more detail in Section V. Of
course, if needed, the set of patterns can be easily extended
to cover additional phenotypes.
The astute reader might wonder why we consider it
reasonable to manually write speciﬁcations for dynamic
behaviors (phenotypes) when we have previously stated that
automation is necessary for generating functionality-aware
models. The reason is that speciﬁcations that operate on
dynamic analysis output can capture behavior at a high level
of abstraction. Hence, they are much easier to develop than
models that operate on static binary code. This is because
with dynamic analysis, one has concrete outputs or events
that a speciﬁcation can be applied to. For example, it is rel-
atively straightforward to identify spam activity by checking
for network trafﬁc to port 25 that contains SMTP keywords.
On the other hand, it is signiﬁcantly more difﬁcult to model
binary code that is capable of opening a network connection
to port 25 and sending out data that conforms to the SMTP
speciﬁcation. Moreover, the same dynamic output can be
achieved in many different ways. That is, a single phenotype
can be implemented by many different code instantiations,
each of which might need to be modeled explicitly. Of
course, speciﬁcations that operate on dynamic output cannot,
on their own, achieve REANIMATOR’s main goal, which is
precisely to identify those (dormant) behaviors that are not
executed during dynamic analysis.
Whenever we identify a phenotype B during dynamic
analysis, we mark all system calls that are directly related to
B. For example, assume that we recognize that a malware
sample opens a network connection and sends out a spam
mail (by checking that this connection contains SMTP trafﬁc
and has destination port 25). In this case, we mark the system
call that is responsible for opening the socket (that belongs
to the network connection over which the mail was sent),
as well as all system calls that write out the mail (spam)
data. Similarly, for network snifﬁng, we would mark the
system call that is responsible for opening a promiscuous-
mode socket, and all system calls that receive data from
this socket. We deﬁne the system calls that are marked as
related to behavior B the relevant system calls for B, and
we denote this set as RB. The set of all relevant system calls
R = {RB},∀B observed during the dynamic analysis run,
serve as the starting point for the next phase.
B. Extracting Genotype Models
In the second phase, the goal is to locate the part of the
binary that is directly responsible for a certain phenotype
that was witnessed during the previous dynamic analysis
phase. We call the code that is responsible for a particular
phenotype a genotype for this behavior. Once we have
located a genotype, we can build a model for it. The basic
idea is that a genotype model can then be leveraged to search
for similar code in other binaries.
A main challenge, and a core contribution of this paper,
is to develop techniques to ﬁnd and model genotypes that
correspond to behaviors that are seen during a dynamic
analysis run. It is important that these genotype models
are precise, i.e., that they capture only code that is directly
responsible for malicious behavior. In particular, a model
should not contain parts of shared utility or library routines
that are also used by other functionality. Moreover, genotype
models should be complete, i.e., they should contain the
entire code that is responsible for a particular behavior and
not only a fragment. Imprecise or incomplete models can
lead to both false negatives or false positives. For example,
when a model contains unrelated code, it is possible that this
fragment accidentally matches benign code (false positive).
As mentioned previously, the starting point for generating
a genotype model is the set of relevant system calls RB
that the previous phase associates with a certain malicious
behavior B. We ﬁrst use a program slicing step to identify all
instructions that contribute to the input parameters of these
system calls, as well as instructions that operate on their
output parameters. Typically, the resulting program slices
are neither precise nor complete. Thus, we use a subsequent
ﬁltering step to remove those parts that are not directly re-
lated to the observed behavior. Finally, we use a germination
step to extend the slice to include parts of relevant code that
were missed by the initial program slicing step. Typically,
these parts are related to instructions that do not directly
operate on system call input or output data, but that set
up a loop or maintain the program stack. Moreover, the
germination step can also include alternative code paths that
are part of the dormant functionality but were not executed
during the dynamic analysis run. This typically increases the
64
completeness of our genotype model by including code that
handles special cases or error conditions that did not occur
during the dynamic analysis.
Note that a genotype represents only one instantiation of
a particular phenotype. That is, a malware binary might
possess a dormant functionality, but our genotype models
do not recognize this functionality because the malware
binary implements this functionality in a different way
(i.e., it has a different genotype for the same phenotype).
However, as our empirical results demonstrate, polymorphic
variants and code reuse are common and lead to a situation
where malware binaries share a signiﬁcant amount of code.
Moreover, whenever a new implementation of a behavior
is observed in our sandbox, the system can automatically
generate a corresponding genotype model.
C. Finding dormant functionality
Once we have generated a set of genotype models asso-
ciated with different malicious behaviors, the third and last
step is to use such models to check binaries for dormant
functionality. To this end, we statically disassemble an
unpacked sample and check for the presence of previously-
modeled genotypes. When a code region is found that
matches one of our models, we report that this sample con-
tains a dormant functionality that implements the behavior
associated with the matching genotype.
it
Since we use static analysis to identify dormant code
in binaries, we need to take into account runtime packing
and code obfuscation. To handle packed binaries, we use a
generic unpacking technique similar to previous solutions
such as Renovo [15] and OmniUnpack [16]. In general,
we envision to use REANIMATOR in combination with a
dynamic analysis tool such as Anubis. Thus,
is easy
and effective to take a memory snapshot at the end of the
dynamic analysis run. Then, we can perform the search for
dormant functionality on this unpacked snapshot. The ro-
bustness of the system against code obfuscation depends on
the concrete implementation choice for the genotype models.
As shown in Section V, our current models, which rely on
structural information of the binary code, work well and
can tolerate differences in the program source code, as well
as changes that are the result of different compiler settings
or compiler versions. When more robustness is required,
one could fall back to semantics-aware code templates or
blueprints, although such models incur signiﬁcantly higher
performance costs.
IV. SYSTEM DETAILS
In this section, we discuss in more detail our approach to
generate genotype models for phenotypes that are identiﬁed
during dynamic analysis. Then, we discuss how these models
can be used to detect dormant functionality.
A. Genotype Models
As mentioned previously, a genotype is a part of a
malware program that is responsible for a particular runtime
behavior. Thus, genotype models need to be able to charac-
terize binary code. This can be achieved in different ways.
On one end of the spectrum, a model could be implemented
as a string of bytes or a sequence of instructions that covers
an interesting code section. While such models are fast when
searching for dormant functionality, they are very speciﬁc.
Thus, even minor changes in the malware binary would
cause these models to miss relevant code. On the other hand,
one could attempt
to extract generalized code templates
(such as the ones proposed in [10], [11]). While quite robust
to semantics-preserving code changes, the detection process
using these models is very costly.
For this work, we leverage the techniques proposed in [9]
and model code as its corresponding colored control ﬂow
graph (CFG). A CFG is a directed graph where nodes are
basic blocks, and an edge from node u to v represents a
possible control ﬂow (such as a jump or branch) from u to
v. The nodes of the CFG we use are colored based on the
classes of instructions that are present in the corresponding
basic blocks. Instruction classes, as deﬁned in [9], are, for
example, “arithmetic,” “logic,” or “data transfer” operations.
to ﬁnd similarities
between polymorphic worms and malware samples. Also,
they have a number of properties that make them particularly
useful in our setting. First, focusing on the structure of code
instead of instruction sequences makes models robust to
simple code insertion and deletion, and to certain classes of
code modiﬁcations such as register renaming or instruction
substitution. Second, using proper optimizations [9], it is
fast
to search malware programs for code that matches
previously-constructed models.
CFGs have been used in the past
In general, two genotypes are considered similar when
their respective CFGs share at least one isomorphic subgraph
that is sufﬁciently large (it has at least k nodes – as in [9], we
use k = 10). Thus, given a genotype, modeled as a colored
CFG G, the problem of ﬁnding this genotype in a malware
binary is reduced to ﬁnding an isomorphic subgraph of size
k that is present both in G and in the binary under analysis.
Since this is an NP-complete decision problem, previous
work [9] introduced an efﬁcient, approximate algorithm.
This algorithm generates a subset of all possible k-node sub-
graphs of G and normalizes them. Each normalized k-node
subgraph then serves as a succinct ﬁngerprint of the code
region that is modeled. For performance reasons, a hash of
the subgraph’s normalized representation is typically used.
In other words, a genotype model is not the colored CFG
itself, but a set of ﬁngerprints that represent it. To search
a binary for the presence of a particular genotype, only the
ﬁngerprints are used. When one or more ﬁngerprints match,
65
then we assume that the binary contains the corresponding
genotype.
B. Genotype Model Extraction
The goal of the genotype model extraction phase is to
map an observed, dynamic behavior (a phenotype) to the
code that implements this behavior (the genotype). Once
this code is located, we can extract its CFG and generate the
corresponding ﬁngerprints. These ﬁngerprints then serve as
the genotype model for detecting dormant behaviors in other
binaries. The genotype model extraction process operates in
three steps, which are discussed in the following.
1) Program Slicing: The starting point for locating code
that is responsible for a particular behavior B is the set
of relevant system calls RB that
the dynamic behavior
identiﬁcation phase has found to be associated with B (as
discussed in Section III-A).
In a ﬁrst step, we attempt to ﬁnd all code that is “related”
to the systems calls r ∈ RB. More precisely, we attempt to
ﬁnd all instructions that either (i) compute values that are
used as input parameters to these system calls, or that (ii)
process the output (return) values from these system calls.
The intuition is that when a relevant system call r is part
of an observed behavior, then the code responsible for this
behavior must either prepare and invoke this system call to
produce desired output or use it to obtain necessary inputs
that are later processed.
Interestingly, the concept of a set of instructions that are
related to a program point is similar to a program slice [17].
In general, a program slice consists of all program instruc-
tions that affect a given point of interest in the program. In
our case, we are interested in all instructions that affect a
point of interest through data ﬂow dependencies. That is,
our slices only capture data ﬂow between instructions, but
we do not include instructions that have an indirect effect
through control ﬂow. The point of interest is a system call
r ∈ RB. Thus, a backward slice consists of all instructions
such that, for each instruction, there is a data ﬂow from one
(or more) of its operands to one of the input arguments of
an interesting system call (case i). A forward slice, on the
other hand, is deﬁned as all instructions for which there is
a data ﬂow from the output of an interesting system call to
the instruction (case ii).
Forward slicing. For certain types of malicious behavior,
the malware program needs to process the output of system
calls. For example, a malware that implements packet sniff-
ing functionality has to process data that is received via a
promiscuous-mode socket, either to log it or to analyze it
for speciﬁc patterns that are of interest to the attacker (e.g.,
passwords or credit card numbers). As another example,
a program that implements a backdoor has to process the
output of the network-related system calls that are used to
receive commands from the attacker. To capture the code
(genotype) related to such classes of behavior B, we extract
a forward slice φ, starting from the output parameters of the
relevant system calls RB.
To compute a forward slice starting from the output of
a given system call, we leverage the taint information that
is provided by Anubis. In addition, we make use of the
instruction log that records each operation that the malware
under analysis has performed. More speciﬁcally, we taint the
output of the system call and then include into the slice φ
all instructions that operate on tainted data (i.e., at least one
source operand of an instruction is tainted). Furthermore, we
also propagate taint across system calls. That is, we taint
the output of system calls when at least one argument of the
system call was tainted.
When computing a forward slice, the analysis follows
a dynamic approach and directly operates on the instruc-
tions that were executed by the malware binary. Because
malicious code may be self-modifying, individual instruc-
tions cannot simply be identiﬁed by their address in the
program. Instead, we identify an instruction as a tuple
(cid:4)address, version number(cid:5). While the program under
analysis is executing, we increment the version number of
an instruction whenever the memory at the corresponding
address was modiﬁed since the last time it was executed.
Backward slicing. While some malware functionality oper-
ates on the output of system calls, other behaviors are based
on computation that provides inputs to relevant system calls
r ∈ RB. For instance, in the case of a UDP ﬂooding attack,
we are interested in how the packet payload and network-
related parameters (e.g., ports or destination IP addresses)
are determined. Or, in case of spam activity, the interesting
part of the program is the genotype that is responsible for
setting up a network connection and sending out mails.
To identify the code that is responsible for computing
the inputs to relevant system calls, we use a standard
dynamic slicing approach [17]. That is, we leverage the
instruction and memory access logs that Anubis produces
to follow deﬁne-use chains backwards, starting from the
input parameters of the system calls. More precisely, we start
to look for instructions that deﬁne the values that serve as
input to relevant systems calls, and we add these instructions
to the slice φ. For each of these instructions, we examine
their operands and determine the values that they use. For
each such value, we locate the instruction that deﬁnes it,
and include it into the slice as well. This process is then
continued recursively, adding to φ all instructions that deﬁne
(produce) inputs for instructions already in the slice.
For each system call r ∈ RB, we compute forward and
backwards slices φr,f orward and φr,backwards. The output of
the program slicing step is the slice φ that is the union of
all forward and backwards slices:
(φr,f orward ∪ φr,backwards).
(cid:2)
r∈RB
φ =
66
Program 1 Example: Network snifﬁng
sock = socket ( . . . ) ;
i f ( sock == INVALID SOCKET )
s n i f f :
e r r o r ( ) ;
1 switch ( command ){
2 case X:
3
. . .
4 case

5
6
7
8
9
10
11
12
13
14
15
16
17
18