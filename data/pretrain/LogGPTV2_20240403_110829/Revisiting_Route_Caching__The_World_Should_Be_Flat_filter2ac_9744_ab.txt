a
r
F
0
1
1.2M
1M
800K
600K
400K
200K
)
s
e
x
i
f
e
r
p
f
o
#
(
e
z
i
S
DSL unsampled
DSL sampled
NetFlow
100
1K
10
Number of popular prefixes (log)
10K 100K
1M 10M
0
0
2
4
6
8
(a)
tout = 60 min
tout = 30 min
tout = 5 min
tout = 1 min
10 12 14 16 18 20 22 24
Time (hour)
(b)
Fig. 1. (a) CDF of uni-class preﬁx popularity (b) Time series of working set size (DSL trace)
his 1987 study on route caching [13]. In this model, a route cache automatically divides
up a CIDR preﬁx into small, ﬁxed-length (i.e., /24) sub-preﬁxes that are mutually non-
overlapping, and then store only the single /24 sub-preﬁx matching to the destination1.
For example, suppose a full routing table contains a preﬁx 10.1.0.0/16, along with sev-
eral more-speciﬁc subpreﬁxes under it. In the uni-class model, 10.1.0.0/16 is internally
considered as a collection of 256 independent /24 routes, ranging from 10.1.0.0/24
to 10.1.255.0/24. Hence, if the destination of an incoming packet is 10.1.2.3, only
10.1.2.0/24 is stored in the route cache. Note that the full routing table still contains a
single CIDR preﬁx, 10.1.0.0/16, as the sub-preﬁx is generated upon cache update. The
reason why we chose /24 as the length for our study is multifold. First, it is the most-
speciﬁc preﬁx length in common use for inter-domain routing; most providers ﬁlter
routes more speciﬁc than /24 to protect themselves against misconﬁguration and preﬁx
hijacking. Also, it is the largest source of FIB growth in today’s Internet: from Route-
Views traces collected from Nov 2001 through Jul 2008, we found that the number of
/24 preﬁxes has increased from 60K to 140K, while other preﬁx lengths increased at
a much slower rate (e.g. /16s increased from 7K to 10K, /8s increased from 18 to 19).
3 Analysis of Trafﬁc Workload
Preﬁx popularity: Before directly evaluating performance of caching on our traces, we
ﬁrst analyze relevant properties of the workload. Speciﬁcally, since our uni-class model
can signiﬁcantly inﬂate the number of unique preﬁxes, we are interested in ﬁguring out
how small (or large) the set of frequently-accessed /24 preﬁxes is. Figure 1a plots the
fraction of packets sent to a given top-x number of preﬁxes, for both DSL and NetFlow
traces. We set the maximum value of the x-axis to be the maximum possible FIB size
(9.3M), which is the number of unique /24 preﬁxes we count when deaggregating the
305K CIDR preﬁxes advertised in the Internet as of February 2008. We ﬁnd that roughly
one tenth (i.e., 0.93M) of the entire preﬁxes accounts for more than 97% of trafﬁc
(consistent with previous work [13, 14, 15]), and nearly 60% (i.e., 5.3M/9M) of the
preﬁxes are never accessed. We found that this result holds across a variety of routers in
1 Uni-class caching can still support more speciﬁc routes than /24, if desired, by maintaining a
small secondary table for those routes.
8
C. Kim et al.
the network, as shown by the NetFlow curve and error bars. Finally, we also conﬁrmed
that packet sampling does not noticeably affect the popularity distribution, shown by
the DSL sampled curve closely matching the DSL unsampled curve.
Temporal analysis of working sets: Understanding the temporal dynamics of trafﬁc
destinations is important because it determines the stability and predictability of caching
performance. Hence, we study how preﬁx popularity varies at different times and across
different timescales as well. To address this, we leverage the notion of a working set,
which is deﬁned to be the set of preﬁxes accessed over a given period of time. The
size of the working set and its variation are often used to estimate how large a cache
will be needed to achieve a low miss rate. Figure 1b shows variation in the size of
the working set over time, under four different deﬁnitions of the working set: the set
of items accessed over last 60, 30, 5, and 1 minute. As one might expect, larger tout
values induce larger working set sizes. Interestingly, however, we found the size of
working sets across a variety of timeout values to be highly stable. Over our entire set
of traces, the standard deviation in working set size was ∼ 3.2% of the average, and the
maximum working set size was no more than ∼ 5.6% larger than the average. This fact
bodes well for deployability of caching solutions using small ﬁxed-size caches, as cache
sizes can be provisioned close to the observed mean without requiring large headroom.
Our further analysis also conﬁrmed that the contents of the working sets vary little.
Cross-router analysis of working sets: Understanding the similarity of working sets
across routers is important because it allows a cache to be pre-provisioned with the
contents from another cache, signiﬁcantly reducing the cold misses after network events
(e.g., router or line-card reboot, routing change due to a network failure). Thus, we need
to understand how working sets vary across different routers, router roles, and regions
within the ISP. We chose several representative access routers: 7 from the ISP’s west
coast POP, and 9 from its east coast POP. To quantify the similarity of working sets at
two different routers r1 and r2, we deﬁne the similarity factor (sfactor) as the number
of common preﬁxes maintained by both r1 and r2, divided by the sum of the number
preﬁxes maintained individually by r1 and r2. We computed the sfactor and its standard
deviation, across all pairs of routers within the east-coast POP (sfactor=59.1% with
stdev=11.8%), the west-coast POP (sfactor=64.9% with stdev=17.6%), and between
pairs of routers in different POPs (sfactor=50.7% with stdev=14.3%). Overall, despite
the limited aggregation beneﬁt of using /24 preﬁxes, working sets of routers within a
POP tend to be quite similar, with an sfactor of 59 − 65% on average. Working sets of
routers in different POPs tend to differ more, with only a 50.7% overlap on average.
Some of these differences were due to localized DNS redirection (e.g., large content
distribution sites redirecting users to geographically-closer servers).
4 Evaluation of Route Caching
LRU vs. LFU: In this section, we explore performance of caching algorithms directly
on network traces, and start by comparing LRU (which keeps track of when each entry
was used and evicts the one used the longest time ago) and LFU (which keeps track
of how many times each entry is used and evicts the one used the smallest number of
times while resident in the cache). Figure 2a shows that LRU outperforms LFU by a
Revisiting Route Caching: The World Should Be Flat
9
100
10
1
)
g
o
l
(
%
n
i
e
t
a
r
s
s
i
M
LFU
LRU
)
g
o
l
(
%
n
i
e
t
a
r
s
s
i
M
10
1
0.1
 LFU, DSL
 LRU, DSL
 Optimal, DSL
 LRU, NetFlow
0.1
0
2
4
6
8
10 12 14 16 18 20 22 24
Time (hour)
5K 10K
50K 100K
Cache size (log)
500K 1M
(a)
(b)
Fig. 2. Miss rate with the DSL traces, (a) Time series of miss rate, cache size = 500K, (b) Miss
rates under LRU and the optimal strategy
large margin. Interestingly, the miss rate for LFU initially decreases during cold-start,
which lasts for approximately 5 to 10 minutes as the cache ﬁlls. Several minutes after
the cache reaches capacity, the miss rate sharply increases. This happens because the
destinations of heavy (i.e., long and fat) ﬂows become “stuck” in the cache, and the
only way of evicting those is waiting for even heavier ﬂows to overwrite those entries.
LRU’s miss rate also converges quickly to its long-term average and remains stable,
whereas LFU’s miss rate takes a few hours to converge and varies more. This happens
because LFU tends to keep unnecessary entries for a long period of time. We observed
these two ﬁndings across a variety of cache sizes and different input trafﬁc mixes. Since
LRU vastly outperforms LFU, we focus only on LRU for the rest of the paper.
LRU vs. Optimal: Next, we compare LRU’s performance to an optimal scheme that
has knowledge of future access times. The optimal caching algorithm (OPT) works
by evicting entries that will not be needed for the longest time in the future. Note
that implementing the optimal algorithm in practice is impossible (as it requires fu-