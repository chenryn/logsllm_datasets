The video transmission was achieved using ManyCam [8], a
virtual webcam driver that can play pre-recorded videos during
a video call. The remote capture of the transmitted video was
done using OBS Studio [9].
Experimental Parameters. We evaluate our attack framework
across a diverse set of experimental parameters to showcase
its generalizability and practical impact. Below is a list of the
different parameters that were studied:
(1) Clothing: Long-sleeves, Short-sleeves, Sleeveless.
(2) Keyboard: Logitech K120 (Wired), Anker A7721 (Bluetooth).
(3) Webcam: Anivia W8 (1080p), Logitech C920 (1080p).
(4) Devices: Lenovo 330-15IGM Laptop, Dell OptiPlex Desktop.
The laptop was evaluated with its built-in keyboard and
webcam, whereas on the desktop we collected data using
combinations of two different (external) webcams and key-
boards. We instructed each participant to wear clothings such
that they covered all three types (long-sleeve, short-sleeve,
and sleeveless) over the six sessions. Overall, the combination
of these parameters resulted in ﬁfteen different experimental
settings (within the In-Lab setup), three on the laptop and
twelve on the desktop. For evaluating keystroke predictions,
we used two different English dictionaries. One is a dictionary
of 4K words which was the same dictionary used for data
collection, and the other is a more comprehensive dictionary
of 65K English words. It should be noted that we do not
evaluate the typing activity detection technique in the In-Lab
experiments, as the participants were not performing any other
tasks besides typing.
Different Backgrounds and Removal. As we employ
DeepLabv3 for background removal, which has been ex-
tensively evaluated in the literature, we do not evaluate it
as an experimental parameter. Nonetheless, in Figure 17 we
show that DeepLabv3 was able to remove backgrounds in
different indoor and outdoor settings. Moreover, in the case that
DeepLabv3 fails to properly identify and remove a particular
background in the recorded video, the adversary can easily
substitute it for another background removal technique.
VII. VIDEO-ONLY IN-LAB EVALUATION
In this section, we evaluate our prediction framework
solely using the video data stream collected during In-Lab
experiments. We brieﬂy present results on the performance
of our keystroke detection algorithm (Algorithm 1), before
detailing the various prediction results.
A. Keystroke Detection Performance
We evaluate keystroke event detection using the precision
and recall metrics, while also studying the effect of different
coefﬁcient values φa, φb, and φc used in our keystroke
detection algorithm (Algorithm 1). As seen in Figure 5, recall
increased as φa and φc were decreased, and when φb was
increased. This is because when φa and φc are small and φb
is large, our keystroke detection algorithm will even recognize
minute noises as a keystroke event. Corresponding precision
values are presented in Figure 5. Balancing between precision
and recall is always a trade-off, and based on these empirical
results we achieved a good precision-recall balance for the
coefﬁcient values φa = 1.5, φb = 3, and φc = 1.5. Using these
coefﬁcient values, we obtained an average of 93% precision
with 92% recall rate. Accordingly, we use keystrokes detected
using these coefﬁcient values for the rest of the evaluation,
including the false positives and ignoring the false negatives.
B. Keystroke Prediction Performance
We now present results on word prediction performance
for different experimental settings.
Different Webcams. Quality of the video can intuitively make
a signiﬁcant difference in the prediction accuracy, as low qual-
ity video frames are more likely to be erroneously processed
by our algorithms. Accordingly, we look at the prediction
accuracies obtained for the three experimental webcams (two
external webcams, one built-in to the laptop). Both the Anivia
and Logitech are able to capture videos at 1080p @ 30 f ps, but
the Anivia webcam features a wide-angle lens when compared
to the Logitech webcam. The Lenovo laptop comes with a low-
end webcam that can record video only at 720p @ 30 f ps. As
8
Fig. 5: Precision and recall of keystroke detection under
different φa, φb, and φc.
Fig. 8: Successful word inference within top-k predicted
words, for different clothings (using Logitech webcam).
Fig. 6: Successful word inference within top-k predicted
words, for different webcams.
seen in Figure 6, the Lenovo laptop webcam consistently had
the worst performance compared to the Anivia and Logitech
webcams. For the 65K dictionary, video from the Lenovo
laptop webcam resulted in only 44.3% average word recovery
when top-200 words were considered. The Logitech webcam
performed slightly, but consistently, better than the Anivia
webcam. Using the 4K dictionary, video from the Logitech
webcam resulted in 75% average word recovery when top-
200 words were considered, whereas video from the Anivia
webcam resulted in 70% average word recovery. One of the
reasons we speculate why the Anivia webcam did not perform
as well as the Logitech webcam is because of its wide-angle
lens. A wide-angle view means that the number of pixels
capturing the user’s body is reduced as more of the background
is captured in the ﬁxed video resolution. For many of the
following evaluations, we used only the Logitech webcam for
better understanding of other parameters.
Different Typing Styles. Based on a screening survey, we
were able to categorize the typing style of our participants
as hunt-and-peck, hybrid, or touch-typing (further explained
in Appendix B). Here we analyze if typing styles have any
signiﬁcant impact on the word recovery. Figure 7 shows the
word recovery percentage for each of the typing styles. Hunt-
and-peck typers were more susceptible with highest mean word
Fig. 9: Successful word inference within top-k predicted
words, for different keyboards (using Logitech webcam).
recovery of 83% (top-200, 4K dictionary), followed by hybrid
typers at 74% and touch-typers at 71%. This is somewhat
intuitive as the arm displacements are very subtle for proﬁcient
touch-typers, which can lead to a higher number of inaccurate
interpretation of the displacement vectors. Nonetheless, we
observe that the overall threat is still signiﬁcant for users with
any of the three typing styles.
Different Clothings. We next evaluate if different types of
clothing, especially with respect to their sleeve design, can af-
fect our word prediction. In Figure 8 we observe that sleeveless
typers were more susceptible to our attack, with 81.7% mean
word recovery (top-200, 4K dictionary), compared to typers
who wore either full or short sleeved dresses (74.4% and 73%,
respectively). We speculate that both short and full sleeves
can mask the extent to which the arms are actually displaced,
underneath the clothing. As a result, our displacement vector
Fig. 10: Comparison between audio and video inferences.
Fig. 7: Successful word inference within top-k predicted
words, for different typing styles (using Logitech webcam).
Fig. 11: Comparison between audio inferences with various
acoustic noises (top-50, 4K dictionary).
9
Top 10Top 25Top 50Top 100Top 200020406080100Logitech (65k dic)Anivia (65k dic)Lenovo (65k dic)Logitech (4k dic)Anivia (4k dic)Lenovo (4k dic)Word Recovery (%)Top 10Top 25Top 50Top 100Top 200020406080100Hunt-And-Peck (65k dic)Hybrid (65k dic)Touch-Typing (65k dic)Hunt-And-Peck (4k dic)Hybrid (4k dic)Touch-Typing (4k dic)Word Recovery (%)Top 10Top 25Top 50Top 100Top 200020406080100Sleeveless (65k dic)Short-Sleeves (65k dic)Full-Sleeves (65k dic)Sleeveless (4k dic)Short-Sleeves (4k dic)Full-Sleeves (4k dic)Word Recovery (%)Top 10Top 25Top 50Top 100Top 200020406080100Logitech (65k dic)Anker (65k dic)Logitech (4k dic)Anker (4k dic)Word Recovery (%)Top 10Top 25Top 50Top 100Top 200020406080100Visual (65k)Audio (65k)Visual (4k)Audio (4k)Word Recovery (%)OrginalMusicJackhammerBirdTypingLawn MowerTalking050100Visual (4k dic)Audio (4k dic)5% Noise15% NoiseWord Recovery (%)calculations can get affected, resulting in slightly less accurate
word predictions.
Different Keyboards. Size of the keyboard, and thus spacing
between the keys, can have a signiﬁcant inﬂuence on the
arm displacements observed during typing. However, after
evaluating the results from the two external keyboards, we did
not ﬁnd any signiﬁcant difference between them (Figure 9).
Even though the Logitech keyboard is signiﬁcantly larger
than the Anker keyboard, the percentage of successful word
predictions were almost identical.
Different Video Calling Softwares. We tested our attack
using three popular video calling softwares: Skype, Hangouts,
and Zoom. Analyzing results using the 65K dictionary and
top-50 predictions, we found that our evaluation using Skype
was marginally better than using Zoom and Hangouts (+3.4%
and +8% mean word recovery, respectively). The same set of
videos were used with all the three video calling softwares,
therefore the differences are purely due to factors beyond
our control, such as the video compression technique used,
network bandwidth utilized, and inconsistent latency in the
video call.
VIII. VIDEO VS. AUDIO IN-LAB EVALUATION
In this section, we compare our prediction framework
(which is based on the video data) with Compagno et al.’s work
[29], where they utilized the audio stream of a Skype call for
keystroke inference. We utilized Compagno et al.’s implemen-
tation of an acoustic-based keystrokes inference framework
designed to work over audio/video calling applications, and
applied our audio data to train and test the inference model.
Audio vs. Visual Performance. With the 65K dictionary,
our video-based inference was approximately +15% more
successful than Compagno et al.’s audio-based inference (when
using top-200 predictions) as shown in Figure 10. However,
with the 4K dictionary, the audio-based inference was +10%
more successful than the video-based inference (using top-200
predictions). The reason why Compagno et al.’s audio-based
inference performs poorly for the larger dictionary is because
the collision rate signiﬁcantly increases with the size of the
dictionary. Nonetheless, our audio data collection was very
controlled, with no one talking and minimum ambient noise
levels. A realistic audio/video call will at least have participants
talking, which can signiﬁcantly affect Compagno et al.’s audio-
based inference framework. Accordingly, we next evaluate the
impact of various types of noise on Compagno et al.’s audio-
based inference.
Noisy Audio vs. Visual Performance. We mixed six dif-
ferent types of acoustic noises with our audio data: music,
typing, lawnmower, bird chirps, jackhammer, and talking. The
characteristics of these six acoustic noises are discussed in
Appendix G. Also, to mimic real-life background noises, we
mixed the acoustic noise at only 5% and 15% levels after
amplitude normalization. As speculated earlier, after adding
only 5% noise the average word recovery dropped from 65%
to about 56% (top-50, 4K dictionary) as shown in Figure 11.
The variance in word recovery across the six different noise
types was not very signiﬁcant. Interestingly, after adding 15%
noise the word recovery sharply dropped to about 7%. These
results highlight how even minimal noise levels can signiﬁ-
cantly affect the audio-based keystroke inference framework.
In contrast, our video-based inference framework is not at all
affected by acoustic noises which is a common occurrence in
audio-video calls.
IX. AT-HOME EXPERIMENTAL SETUP
To understand our inference framework’s effectiveness in
the wild, we next evaluate it outside of the lab environment. In
this setting, participants were asked to use their own device (a
laptop or desktop with a webcam) and setup (sitting position,
clothing, background, and positioning of devices) for the video
call, including location from where the call is done (e.g., their
home). This allowed us to collect typing related video data for
a diverse combination of devices and setups, already familiar
to the participants and not constrained in any way. Using
such “At-Home” experiments, we also expand our evaluation
beyond just predicting dictated English words. Speciﬁcally,
we analyze how our framework performs for the inference
of user-chosen passwords, websites, and English words. As
in a real setting, users are expected to involve themselves in
other activities besides typing (e.g., web surﬁng, playing online
games, etc.). This unconstrained At-Home setup allows us to
thoroughly evaluate our typing activity detection technique
outlined in Section V-C.
Participant Demographics. We collected data from 10 partici-
pants for this in-home evaluation, whose ages ranged between
21 and 29 years. Out of the 10 participants, 3 are females,
and 7 are males. Based on a screening-survey, 3 participants
conducted hunt-and-peck typing, 5 conducted touch typing,
and the remaining 2 participants conducted hybrid typing. 9
participants identiﬁed themselves as right-handed and 1 as
ambidextrous. The average height of the participants is ap-
proximately 170 cm, with an average observed typing speed of
approximately 3.7 keystrokes per second and typing accuracy
(in relation to typographical errors) of approximately 86.7%.
Participant’s Task. Participants were invited to join a (max-
imum) 30 minute Skype video call, using their own device
and setup and from their own location of choice, where they
had to sporadically (and at their own pace) type 10 email
addresses, 10 usernames, 10 passwords, 10 websites, and 10
English words, in no particular order and frequency. The typing
was performed in a pre-shared online spreadsheet, which was
later used as the ground-truth of the typed text/information.
The spreadsheet also automatically recorded edit timestamp
for each cell in the spreadsheet, which is useful for evaluating
the typing activity detection technique. To ensure participants
covered a reasonable amount of time on non-typing activities,
we asked participants to take at least three 1-minute breaks
doing one of the following three activities: watch a YouTube
video, read a Wikipedia article, or play a digital game on their
computer that only requires a mouse to play. Participants had
the liberty to take additional or longer breaks and/or do any
other activity on their computer that does not require keyboard
usage. Unlike in the in-lab experiments, participants were al-
lowed to use backspace in case they wanted to rectify a typing
error and were allowed to use a larger set of keys/characters
on the keyboard for their typing tasks (alphabet keys, number
keys directly above the alphabet keys, keys corresponding to
“.”, “-”, and “@” characters, and the enter and backspace keys).
Data Collected. In addition to the ground-truth text and times-
tamp information contained in the online spreadsheet where
10
participants typed, participant’s Skype video was recorded re-
motely using OBS Studio [9]. After each typing experiment
was completed, we also collected supplementary information
from our participants related to their employed device and
setup, as summarized in Appendix H.
Webcam Hardware and Positioning. We observed that three
of our 10 participants used an external webcam in a similar
fashion as in the in-lab setting, placed approximately at eye-
level and focused directly on the participant. However, the
remaining 7 participants who participated using their laptops,
the webcam angle and distance varied noticeably, as shown in
Appendix H. The native webcam resolution across participants
also varied between 720p or 1080p.
X. AT-HOME EVALUATION
In this section, we evaluate the performance of our pro-
posed typing activity detection and keystroke (or text/word)
prediction techniques using video call data collected from
the At-Home experiments. During these experiments, we ob-
served that one of the participant’s hair completely obscured
his/her shoulder area for the entire experiment’s duration, thus
making the corresponding video frames unusable within our
framework. Due to these At-Home experiments’ uncontrolled
nature, this participant was not asked to re-position his/her
hair or change his/her posture. This points to a limitation
of our inference framework. However, this has already been
highlighted in our assumed adversary model, where we clearly
state that both shoulders and upper arms should be visible
(to the adversary) in the recorded video. Thus, our presented
evaluation results below are based only on data collected from
the remaining 9 participants.
A. Typing Activity Detection Performance
For evaluation of our typing activity detection technique
(Figure 3), we employ the same optimal values for parameters
φa, φb, and φc as determined earlier in Section VII-A. Across
all the 9 participants, our typing activity detection technique
resulted (Figure 12) in an average of 40.22 true positives,
12.4 false positives and 9.78 false negatives, for an average
precision of 77.6% and recall of 80.4%. This shows that
our proposed activity detection technique is fairly accurate
enabling the adversary to not only detect a majority of the
typing activity during a video call, but also successfully
differentiate between typing versus non-typing activities.
In addition to the overall results, let’s further highlight
some interesting special cases. For participants using a laptop,
we observed that
the location ﬁlter (of the typing activ-
ity detection technique) was not very effective due to the
proximity of the laptop’s touchpad to its keyboard. Also, a
few participants (at least, two) claimed to have used both
their hands for interacting with the laptop touchpad, making
the exclusive ﬁlter of the typing activity detection technique
ineffective at times and resulted in a higher number of false
positives. Signiﬁcant movement and posture changes (between
typing and non-typing activities) also resulted in degradation
of detection accuracy, as was observed (Appendix H) in the
case of at least one participant whose left shoulder was not
visible for a signiﬁcant portion of the video call because