51 (85%)
59 (98%)
283 (93%)
Table 7: Effectiveness of forensic analysis. P-DIFF can pin-
point the root-cause policy changes that permit the access
of interest in the evaluation.
and subnet). Once a rule is correctly encoded, P-DIFF can always
correctly backtrack the corresponding time series.
We investigate the 18 accesses of interest for which P-DIFF fails
to pinpoint the root-cause policy changes. In 50% of the cases (9 out
of 18), the objects being accessed are rarely accessed in the history
(refer to §10.2 for the experiment design). In these cases, P-DIFF
fails to generate any rules and thus cannot pinpoint the rules. In
the remaining 9 cases, P-DIFF generated inaccurate rules (i.e. on the
parent directory instead of on the child directory), and therefore
fails to pinpoint the precise root cause change.
10.3.3 Effectiveness Discussion. For policy change detection,
the goal of P-DIFF is to detect as many true changes as possible
while minimizing the reports of false changes. Our evaluation re-
sult in Table 6 shows P-DIFF detects 93 out of 99 changes, which
means only six (7%) changes are missed. P-DIFF generates 12 posi-
tives which increase sysadmins’ validation overhead. Overall, the
validation overhead is reasonably small. Taking Wikipedia, one of
the most popular online services, as an example, the sysadmins of
Wikipedia only need to validate 25 changes in 4.2 days (which is
the testing period).
For forensic analysis, as shown in Table 7, P-DIFF successfully
pinpoints root-cause changes of 283 out 303 accesses of interest,
This means that sysadmins can use P-DIFF to efficiently diagnose
93% of the target access event. Without P-DIFF, the sysadmins may
have to go through either a large number of access logs or various
configuration and code in different components as discussed in §2.
Dataset
# Log entries Training Validation Forensics
for training
(per access) (per access)
time
746 s
43.7 s
20.3 s
2.46 s
9.93 s
258 M
4.13 M
1.73 M
70.0 K
17.6 K
Wikipedia
Center
Course
Company
Group
Table 8: Performance of P-DIFF: training time, and time for
validation and forensics per access.
12.8 ms
2.7 ms
4.8 ms
2.8 ms
3.7 ms
963 µs
10.8 µs
13.0 µs
26.1 µs
38.2 µs
Figure 11: Precision, recall, and F-score of TCDT classify-
ing access results for the Center and Course datasets. The
x-axis shows the time of the testing data, which is a month
of logs in the dataset. The training data is the three continu-
ous months of logs before the testing month.
10.4 Precision, Recall, and F-Score
The detection of policy changes is based on detecting the deviation
of access results. We look into how well Time-Change Detection
Tree (TCDT) can serve as a classifier to decide the access results by
whether the decision matches the actual access results. Higher accu-
racy means more accurate policy learning. We compare TCDT with
an implementation of the traditional decision tree implemented
using Apache Spark MLlib [57] denoted as Spark-DT.
The experiment is conducted on the Center and Course datasets
as they have longer duration of logs and enable a comparison be-
tween different testing sets. Every three months of logs are used as
training set and the last month of logs are used as testing set.
Figure 11 shows that P-DIFF’s TCDT achieves a precision of 0.997,
a recall of 0.92, and a F-score of 0.94, while Spark-DT achieves a
precision of 0.83, a recall of 0.86, a F-score of 0.80. P-DIFF-TCDT
improves precision by 19.5%, recall by 6.5%, and F-score by 17.3%.
P-DIFF’s TCDT improves the prediction precision of Spark-DT for
all testing sets, and the improvement is more prominent when the
training set contains more rule changes, e.g. the training set for
the February testing set in the Center dataset. P-DIFF-TCDT also
improves the prediction recall for the other testing sets.
P-DIFF-TCDT does not increase the prediction accuracy on ac-
cesses happened in Nov in Figure 11 (a). We consulted the sysadmin
and learned that this month the website had a major change: it is
ported to another server and many new pages are added. P-DIFF
has no knowledge of these new pages and so reports accesses to
them as UNKNOWN (cf. §6). Although this hurts the accuracy in our
evaluation, in real usage P-DIFF will retrain a new TCDT with the
new accesses and gets good accuracy. As shown from the result of
Dec, Jan, and Feb in Figure 11 (a), after training TCDT with the
new accesses, P-DIFF gets precision and recall both above 0.9.
10.5 False Positive and False Negative
P-DIFF generates false positives and negatives when the training
set does not have enough information. For false positives, P-DIFF
may generate wrong rules. In one case, P-DIFF generates a rule
that access to “/proj1” should be denied based on the observation
that accesses to “/proj1/1.htm” and “/proj1/2.htm” are all
denied in the training phase. In the detecting phase, P-DIFF observes
accesses to “/proj1/3.htm” are allowed and then decides a rule
change on “/proj1”. But in fact, the access rules are in the file level
instead of the directory level and accesses to “/proj1/3.htm” are
always allowed. P-DIFF generates a wrong rule because there is no
access to “/proj1/3.htm” in the training set.
False negatives are mainly because of “rare” objects (§ 10.2). P-
DIFF fails to infer the rules and thus cannot detect the change.
For example, in the training phase, P-DIFF observes all accesses
from an IP “192.168.1.1” are allowed and so infers an allow
rule for this IP. In the detecting phase, P-DIFF observes accesses
from “192.168.1.2” are denied. Since P-DIFF has no rule for
“192.168.1.2”, it cannot detect the change and the change is
actually access to subnet “192.168.1.*” has been modified from
ALLOW to DENY.
10.6 Execution Time
Table 8 shows the execution time of P-DIFF for training, validation,
and forensics, respectively in previous validation and forensic exper-
iments (cf. §10.3). P-DIFF’s training is very efficient. For the largest
dataset (Wikipedia) with 258 million log entries, the training that
learns the TCDT only takes 12 minutes. For smaller datasets, P-
DIFF takes less than 1 minute for training. Note that the training is
an offline process without the need of being real-time.
The validation and forensics can be done in microseconds and
milliseconds per access. The efficiency is decided by the depth of the
TCDT—the deeper the TCDT is, the more time it takes. Therefore,
the time taken for validation and forensics in Wikipedia dataset
is larger than the others. Forensics takes a longer time for back-
tracking the time series. The performance for forensics is satisfying,
given that forensics is typically postmortem to the security inci-
dents and is done offline. For validation, P-DIFF needs to validate
every access recorded in the access log. Note that this does not
need to be done sequentially but can be done in parallel as each
access is independent. Therefore, we conclude that the execution
time of P-DIFF is acceptable in real-world settings.
AugSepOctNovDecJanFebMar(a) Center dataset0.00.20.40.60.81.0Precision / Recall / F-scoreMayJunJulAugSepOctNovDec(b) Course dataset0.00.20.40.60.81.0Spark-DT precisionSpark-DT recallSpark-DT F-scoreTCDT precisionTCDT recallTCDT F-score10.7 Scalability
To understand how P-DIFF scales with real-world access log data,
we evaluate P-DIFF with different numbers of log entries using the
Wikipedia dataset. We use the continuous log entries of 10 million,
20 million, and all the way up to 320 million logs as different training
sets. Note that 320 million is the number of logs from Wikipedia
for 12 days out of the total 14 days of logs, which is the largest
real-world dataset we can collect by far (the remaining 2 days of
logs are used as the testing set). The results in §10.3 show that less
than 320 million of logs have already been effective for P-DIFF to
do an accurate change detection (100% precision and recall) and
forensic analysis (97% success). Therefore, in practice, we can only
maintain the most recent 12 days of logs for P-DIFF to be effective
for Wikipedia.
Figure 12 shows the training, validation and forensics time re-
spectively. When the number of log entries increases from 10 million
to 320 million, the training time increases linearly from 2 seconds
to 19 minutes, as shown in Figure 12 (a). The linear increase of the
training time is due to the fact that P-DIFF needs to go through
every log entry to infer policies and policy changes. Considering
training is an offline process, it is acceptable to take 19 minutes
to train a TCDT once a while. Training is only necessary for the
first time usage of P-DIFF or when P-DIFF encounters too many
UNKNOWN accesses (cf. §11).
The validation and forensics time for an access only takes a
few milliseconds, as shown in Figure 12 (b). Both validation and
forensics need to search the decision tree to find the leaf node
applies for a given access. Therefore, the depth of the leaf node
decides the execution time of the validation and forensics. The
average validation and forensics time are decided by the depth of
the leaf node that encodes a dominant policy, which applies to
most accesses. In Wikipedia, there is a dominant policy that “all
page read should be allowed”. This dominant policy node can be
located in different depth based on the training set. This explains the
variation of the execution time for validation and forensic analysis
in Figure 12(b). Overall, the variance is small enough for efficient
validation and forensics.
10.8 Validation Overhead
There are two types of validation that sysadmins need to perform.
The first one is when P-DIFF detects a change, it would inform
sysadmins to validate whether the change is intended or not. Our
evaluation results show the overhead for this type of validation is
acceptable. Take Wikipedia dataset as an example, only 25 valida-
tion needs to be done for the tested 111 million accesses during the
test period.
The second type of validation is when P-DIFF reports an UNKNOWN
access after a new object (e.g., a file) is added to the system. In the
five datasets used in the evaluation, only 12 new objects were un-
observed during the training period. Therefore, only 12 validations
need to be done for those UNKNOWN cases (shown as false positives
in Table 6). Intuitively, popular objects should be observed during
the training period, while rare objects, even not observed during the
training period, would not incur too much overhead for validation
because of its rareness.
Figure 12: Scalability analysis in terms of execution time
for training, validation, and forensics with the increasing
numbers of log entries from the Wikipedia dataset. Train-
ing time is linear to the number of log entries; it takes 19
minutes to train a model from 320 million log entries. Vali-
dation and forensics time are always low (1–10 milliseconds).
Moreover, the overhead for importing new objects (e.g., files)
should not be excessive, because sysadmins typically do not need
to validate every single new object but can validate the higher
level hierarchy. For example, typically all the files in the same
directory have the same permission settings and so the directory
can be validated in total, avoiding validating individual new files.
Certainly, in a case that every file under the same directory has
distinct permission settings, the sysadmin needs to validate them
one by one (but this also reflects a pathological practice in the
security management).
11 DISCUSSIONS AND LIMITATIONS
P-DIFF learns access control policies from access logs, and thus is
limited to the information recorded in access logs. As discussed in
§6, if a new access with an unseen attribute or value in logs, P-DIFF
explicitly classifies it as UNKNOWN and notify sysadmins to validate
it. There are two cases that could cause UNKNOWN. First, some new
attributes or values are added to the access control policies, such
as the creation of new users, roles, and files, etc. In this case, the
UNKNOWN is a real change and so is desirable to be validated. Sec-
ond, if the resource is extremely cold (there are very few accesses),
P-DIFF may not be able to learn the related rules due to the lack of
information. We observe in our datasets that public resources are
more frequently accessed than private resources. In a few extreme
cases, the private resources are only accessed once a month. In this
case, the validation request on UNKNOWN is also acceptable because
a rarely accessed resource is desired to be manually examined and
it will not cause too much burden for sysadmins for its rareness. In
addition, in both cases, P-DIFF will retrain a new TCDT so that the
unknown attributes and values can be learned for future classifica-
tion. As shown in §10.6, the training time of a TCDT in real-world
datasets is in the range of 2 seconds to 19 minutes. Therefore, it is
acceptable to retrain a new TCDT in a normal frequency like once
an hour or once a day.
P-DIFF cannot work with access logs that miss important infor-
mation (i.e., subject, object, action, and result). As shown in Table 2,
10M20M40M80M160M320M(a) Training time100101102103Time (s)Training time10M20M40M80M160M320M(b) Validation/Forensics time100101102Time (ms)Validation time per accessForesics time per accessalthough most of the studied software systems record the required
information in their access logs, there do exist some systems that
missing certain key information, such as subject and access results.
P-DIFF cannot infer rules from access logs of those systems before
the logs are enhanced. As discussed in [11, 68], incomplete access
logging is a significant flaw that impairs auditing and forensics and
thus should be fixed. Enhancing logging is beyond the scope of
P-DIFF. Future work in automatically enhancing access logs would
be a valuable direction to be explored.
12 RELATED WORK
12.1 Access Control Misconfigurations
Despite the extensive works in access control modeling and de-
sign, only a few efforts have been conducted in the past related
to access-control misconfiguration detection. Most of the prior
works attempt to detect access-control misconfiguration by find-
ing inconsistencies between access-control policies [8, 9, 15, 54].
However, inconsistencies only reflect a very small, specific set of
access-control misconfigurations. As acknowledged in these works,
misconfigurations could totally be consistent, which often leads to
even more severe consequences. In addition, these works require
domain knowledge to interpreting specific access-control policies
of different software. P-DIFF is complementary to the aforemen-
tioned works as we focus on access-control policy changes along
with time instead of policy inconsistencies at a particular moment.
Moreover, P-DIFF automatically infers access control policies from
access logs without any domain knowledge or specification from
sysadmins and thus is more general.
Testing and verification approaches [20, 34, 37] have been pro-
posed to detect access control misconfigurations. While testing and
verification have demonstrated promising results, they have not
been widely deployed in practice due to the extensive efforts in writ-
ing testing cases or verification specifications. Especially, existing
testing and verification approaches require a unified and central-
ized model (e.g., XACML); however, today’s systems access control
policies are kept in various forms including various configuration
file formats, file permissions or database privilege table. It is hard
to cover all the combinations of the access control configurations.
Exactly due to this problem, P-DIFF chooses to infer access control
policies from access logs regardless of configuration formats.
12.2 Other Types of Misconfigurations
To tackle misconfigurations, previous work has been done on de-
tecting or troubleshooting system misconfigurations [5, 6, 49, 63,
65, 67, 75, 78, 79]. While those techniques are effective for detecting
or troubleshooting misconfigurations that lead to system failures,
they cannot deal with access control misconfigurations. Access
control misconfigurations are fundamentally different from general
software misconfigurations that lead to functional failures or per-
formance degradation (which is assumed by the aforementioned
techniques). Instead, they can go unnoticed for months until being
exploited by malicious users. In addition, access control miscon-
figurations are typically “valid” configuration settings but do not
conform to users’ or organizational security policy.
12.3 Access Control Code Vulnerability
Besides misconfigurations, vulnerabilities can also be introduced by
software bugs, e.g. the software could miss permission checks. Sun
et al. [59] use static analysis to infer the protected domains from the