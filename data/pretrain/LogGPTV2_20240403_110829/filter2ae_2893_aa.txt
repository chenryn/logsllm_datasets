# 联邦学习中的后门攻击
|
##### 译文声明
本文是翻译文章
译文仅供参考，具体内容表达以及含义原文为准。
## 背景
联邦学习这几年比较火，它主要用于解决在不直接访问各种训练数据的情况下训练机器学习模型的问题，特别是对于数据隐私比较敏感的任务。在联邦学习中利用参与者(即各方)的局部训练数据，帮助训练一个共享的全局模型，提高性能。尽管联邦学习能够聚合由不同方提供的分散(通常是受限的)信息来训练更好的模型，但它的分布式学习方法以及跨不同参与方的固有异构(即非独立同分布)的数据分布可能无意中为新的攻击提供了可能。
本文将会介绍并分析对联邦学习进行后门攻击的技术方案，主要是两种类型的工作，一种是Bagdasaryan等人提出的集中式后门攻击，一种是Xie等人提出的分布式后门攻击。
## 联邦学习
联邦学习架构如下
2016年，谷歌研究院在解决面向用户个体的键盘输入法优化问题时，提出了联邦学习这一全新的人工智能解决方案 。联邦学习面向的场景是分散式多用户{F1…FN
}，每个用户客户端拥有当前用户的数据集 {D1…DN}
。传统的深度学习将这些数据收集在一起，得到汇总数据集，训练得到模型Msum.联邦学习方法则是由参与的用户共同训练一个模型 _M_ fed
，同时用户数据Di保留在本地，不对外传输。如果存在一个非负实数δ，使得Mfed的模型精度 _V_ fed与Msum的模型精度Vsum满足如下不等式:
则称该联邦学习算法达到 _δ_ -精度损失.
联邦学习允许训练模型存在一定程度的性能偏差，但是为所有的参与方提供了数据的安全性和隐私保护。
联邦学习的流程如下所示
步骤1:系统初始化。首先由中心服务器发送建模任务，寻求参与客户端。客户端数据持有方根据自身需求，提出联合建模设想。在与其他合作数据持有方达成协议
后，联合建模设想被确立，各数据持有方进入联合建模过程。由中心服务器向各数据持有方发布初始参数。
步骤2:局部计算。联合建模任务开启并初始化系统参数后，各数据持有方将被要求首先在本地根据己方数据进行局部计算，计算完成后，将本地局部计算所得梯度脱敏后进行上传，以用于全局模型的一次更新。
步骤3:中心聚合。在收到来自多个数据持有方的计算结果后，中心服务器对这
些计算值进行聚合操作，在聚合的过程中需要同时考虑效率、安全、隐私等多方面的问题。比如，有时因为系统的异构性，中心服务器可能不会等待所有数据持有方的上传，而是选择一个合适的数据持有方子集作为收集目标，或者为了安全地对参数进行聚合，使用一定的加密技术对参数进行加密。
步骤4:模型更新。中心服务器根据聚合后的结果对全局模型进行一次更新，并将更新后的模型返回给参与建模的数据持有方。数据持有方更新本地模型，并开启下一步局部计算，同时评估更新后的模型性能，当性能足够好时，训练终止，联合建模结束。建立好的全局模型将会被保留在中心服务器端，以进行后续的预测或分
类工作。
## 形式化
联邦学习的训练目的可以看做是一个有限和优化问题，即最小化下式
N个参与方独立地处理N个局部模型，每个参与方在自己的私有数据集
都有各自的局部目标
在有监督的联邦学习环境下，fi的每个损失函数可以以下式进行计算
其中l代表使用局部参数wi得到的预测的损失
联邦学习的目标是为了得到一个全局模型，它可以在聚合了来自N个参与方的分布式训练结果后在测试集Dtest上泛化得很好。
当在第t轮时，中心服务器将当前的共享模型Gt发给n个选中的参与方，被选中的参与方i会在本地使用类似SGD的优化算法运行E个epoch在私有数据集Di以及学习率lr，计算函数fi，从而得到新的本地模型Lt+1
i.然后本地参与方会将模型更新，即下式的计算结果返回给中心服务器
中心服务器收到后，将会使用它自己的学习率平均所有的更新来生成新的全局模型Gt+1，如下式所示：
这个聚合过程会持续迭代直到联邦学习找到了最终的全局模型。
## 集中式后门攻击
通过联邦学习这一小节的介绍，我们知道，在每一轮中，中心服务器会将当前的全局模型分配给随机的参与方子集。他们每个人在本地进行训练，并向服务器提交一个更新的模型，服务器将更新的平均值放入新的全局模型中。但是，联邦学习在保证参与者隐私的同时，对参与者的本地数据和训练过程是不清楚的。所以联合学习非常容易受到模型中毒的影响，这是Bagdasaryan等人首次引入的一类新的中毒攻击，以前的投毒攻击只针对训练数据。模型中毒利用了联邦学习使恶意参与方会直接影响联合模型这一事实，使攻击效果比训练数据中毒更强大。攻击示意图如下所示
在联邦学习中，参与方可以(1)直接影响全局模型的权重，(2)以任何有利于攻击的方式进行训练，例如:在训练过程中，对其局部模型的权值进行随机修正，同时也可以将潜在防御的规避纳入其损失函数。所以对于攻击者而言，这是非常有利的。
在联邦学习过程中，既不能使用数据中毒防御，也不能使用异常检测，因为它们分别需要访问参与者的训练数据或他们提交的模型更新。但是中心服务器既不能观察训练数据，也不能在不侵犯参与者隐私的情况下观察基于这些数据的模型更新，此外最新版本的联邦学习采用了“安全聚合”，这可以防止任何人审计参与者的数据或更新，这些都给我们的攻击方案创造了有利条件。
## 形式化
正常情况下，联邦学习中全局模型的更新如下所示
而恶意参与方则尝试使用恶意模型X代替上式中的Gt+1
因为不是独立同分布的数据，每个局部模型可能会离当前的全局模型比较远，随着全局模型的收敛，这些偏差慢慢变小，即：
因此，攻击者可以按照如下方式求解需要提交的模型：