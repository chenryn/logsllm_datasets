main challenge for effective genetic search is how to deÔ¨Åne an
appropriate Ô¨Åtness function, which must measure the potential
usefulness of a candidate input. Genetic search, as well as other
heuristics for test input generation, can complement systematic
exploration using guided sampling [7].
The classic idea of symbolic execution [47] as well as
its more recent variants, e.g., where concrete inputs guide
symbolic execution [12, 35, 74], enable a form of white-box
test input generation that has received much recent attention
for Ô¨Ånding security bugs [36, 37, 46, 73]. Godefroid et
al.‚Äôs SAGE [36] introduced white-box fuzzing that executes
a given suite of inputs, monitors their execution paths, and
builds symbolic path condition constraints, which are sys-
tematically negated to explore their neighboring paths. SAGE
found several new bugs in Windows applications, including
media players and image processors. Grammar-based whitebox
fuzzing [34] uses a grammar to enumerate valid string inputs
by solving constraints over symbolic grammar tokens. A
security-focused application using a context-free fragment of
the JavaScript grammar to test the code generation module of
the Internet Explorer 7 JavaScript interpreter showed that the
use of the grammar provides enhanced code coverage. Similar
but independent work on CESE [51] uses symbolic grammars
with symbolic execution to create higher-coverage suites for
select UNIX tools, albeit in a non-security setting.
Kiezun et al.‚Äôs Ardilla [46] uses concolic execution to
generate test inputs that drive its dynamic taint analysis and
mutates the inputs using a library of attack patterns to create
SQL injection and cross-site scripting attacks. Halfond et
al. [37] show how symbolic execution can more precisely
identify parameter values that deÔ¨Åne the interfaces of Web
applications, and facilitate Ô¨Ånding vulnerabilities. Saxena et
al.‚Äôs Kudzu [73] uses a symbolic execution framework based
on a customized string constraint language and solver to Ô¨Ånd
code injection vulnerabilities in JavaScript clients.
Brumley et al. [10] proposed a white-box symbolic analysis
technique to guide differential
testing [59]. Their analysis
is driven by concrete executions in the spirit of dynamic
symbolic (aka concolic) execution [12, 35, 74]. They use
weakest preconditions [23] over select execution paths together
with constraint solving to compute inputs that likely cause
parsing discrepancies between different implementations of
protocols such as HTTP and NTP.
There are two basic differences between our methodology
and that of [10]. First, our black-box approach does not require
analyzing either the source, or the binary code. Second, the
need to solve path constraints limits the scalability of the
approach described in [10]. Generating even a single test
certiÔ¨Åcate using their technique requires symbolic analysis of
both the parsing code and the certiÔ¨Åcate validation code hidden
deep inside the program. SSL certiÔ¨Åcates are structurally more
complex than HTTP and NTP inputs, and, crucially, the certiÔ¨Å-
cate validation logic lies deeper in SSL/TLS implementations
than the X.509 parsing code. For example, a MiniWeb server
responding to a GET /index.html request (one of the case
studies in [10]) executes 246,910 instructions. By contrast, the
simplest of our test cases‚Äîan OpenSSL client processing a
certiÔ¨Åcate chain of length 1 with zero extensions‚Äîexecutes
27,901,961 instructions.
An interesting avenue for future research is to explore
whether the two approaches could be used in conjunction and,
in particular, whether generation of test SSL certiÔ¨Åcates can
beneÔ¨Åt from the fact that the technique of [10] performs a
directed search for likely behavioral differences.
More recent work by Ramos and Engler on UC-KLEE [63],
which integrates KLEE [11] and lazy initialization [45], ap-
plies more comprehensive symbolic execution over a bounded
exhaustive execution space to check code equivalence; UC-
KLEE has been effective in Ô¨Ånding bugs in different tools,
including itself. In principle, such goal-directed approaches
are very powerful:
they integrate the spirit of differential
testing with symbolic analysis to create formulas that explic-
itly capture behavioral differences of interest. However, the
resulting formulas in the context of structurally complex data
can be exceedingly complex since they represent destructive
updates in imperative code using a stateless logic. Scaling such
approaches to SSL/TLS implementations is an open problem.
In summary, while approaches based on symbolic execu-
tion have been successful in Ô¨Ånding bugs in many applications,
their central requirement‚Äîthe need to solve constraints for
each execution path explored in symbolic execution‚Äîis the
basic bottleneck that limits their scalability and applicability
for programs that operate on complex data types, such as
the structurally complex SSL certiÔ¨Åcates, and have complex
path conditions that can be impractical to solve. By contrast,
our test generation algorithm is not sensitive to the
implementation-level complexity of the programs being
tested. Instead, it focuses on the systematic exploration of
the space of likely useful inputs and thus reduces the overall
problem complexity by de-coupling the complexity of the input
space from that of the SSL/TLS implementations.
Srivastava et al. [76] use static differential analysis, which
to analyze
does not perform test generation or execution,
consistency between different
implementations of the Java
Class Library API and use the discrepancies as an oracle
to Ô¨Ånd Ô¨Çaws in the implementations of access-control logic.
While static analysis and dynamic analysis, such as testing,
are well-known to have complementary strengths, they can also
be applied in synergy [28]. For example, for testing SSL/TLS
implementations, static dataÔ¨Çow analysis could potentially
reduce the space of candidate inputs for the test generator by
focusing it to exercise fewer values or fewer combinations of
values for certain certiÔ¨Åcate extensions.
III. OVERVIEW OF SSL/TLS
A. SSL/TLS protocol
The Secure Sockets Layers (SSL) Protocol Version 3.0 [70]
and its descendants, Transport Layer Security (TLS) Protocol
117
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:02:03 UTC from IEEE Xplore.  Restrictions apply. 
Version 1.0 [64], Version 1.1 [67], and Version 1.2 [68], are the
‚Äúde facto‚Äù standard for secure Internet communications. The
primary goal of the SSL/TLS protocol is to provide privacy
and data integrity between two communicating applications.
In this paper, we focus on a particular security guarantee
promised by SSL/TLS: server authentication. Server authen-
tication is essential for security against network attackers. For
example, when SSL/TLS is used to protect HTTP communi-
cations (HTTPS), server authentication ensures that the client
(e.g., Web browser) is not mistaken about the identity of the
Web server it is connecting to. Without server authentication,
SSL/TLS connections are insecure against man-in-the-middle
attacks, which can be launched by malicious Wi-Fi access
points, compromised routers, etc.
The SSL/TLS protocol comprises the handshake protocol
and the record protocol. Server authentication is performed
entirely in the handshake protocol. As part of the hand-
shake, the server presents an X.509 certiÔ¨Åcate with its public
key [69]. The client must validate this certiÔ¨Åcate as described
in Section IV. If the certiÔ¨Åcate is not validated correctly,
authentication guarantees of SSL/TLS do not hold.
CertiÔ¨Åcate validation in SSL/TLS critically depends on
certiÔ¨Åcate authorities (CAs). Consequently, we analyze the
correctness of SSL/TLS implementations under the assumption
that the CAs trusted by the client correctly verify the identities
of the servers to whom they issue certiÔ¨Åcates. If this assump-
tion does not hold‚Äîe.g., a trusted CA has been compromised
or tricked into issuing false certiÔ¨Åcates [17, 22]‚ÄîSSL/TLS is
not secure regardless of whether the client is correct or not.
In summary, we aim to test if the implementations of
SSL/TLS clients correctly authenticate SSL/TLS servers in the
presence of a standard ‚Äúnetwork attacker,‚Äù who can control any
part of the network and run his own servers, possibly with
their own certiÔ¨Åcates, but does not control legitimate servers
and cannot forge their certiÔ¨Åcates.
B. SSL/TLS implementations
In this paper, we focus primarily on testing open-source
implementations of SSL/TLS. Our testing methodology can be
successfully applied to closed-source implementations, too (as
illustrated by our testing of Web browsers), but having access
to the source code makes it easier to identify the root causes
of the Ô¨Çaws and vulnerabilities uncovered by our testing.
We tested the following SSL/TLS implementations:
OpenSSL, NSS, CyaSSL, GnuTLS, PolarSSL, MatrixSSL,
cryptlib, OpenJDK, and Bouncy Castle. These implementa-
tions are distributed as open-source software libraries so that
they can be incorporated into applications that need SSL/TLS
for secure network communications.
Many vulnerabilities stem from the fact that applications
use these libraries incorrectly [31], especially when some
critical part of SSL/TLS functionality such as verifying the
server‚Äôs hostname is delegated by the SSL/TLS library to the
application. In this paper, however, we focus on Ô¨Çaws within
the libraries, not in the applications that use them, with one
exception‚ÄîWeb browsers.
HTTPS, the protocol for protecting Web sessions from
network attackers, is perhaps the most important application of











	




		


















	














	



		

		

	




	



	

	
	
	
Fig. 1: A sample X509 certiÔ¨Åcate chain.
SSL/TLS. Therefore, we extend our testing to Web browsers,
all of which must support HTTPS: Firefox, Chrome, Internet
Explorer, Safari, Opera, and WebKit (the latter is a browser
‚Äúengine‚Äù rather than a standalone browser). Web browsers
typically contain proprietary implementations of SSL/TLS,
some of which are derived from the libraries listed above.
For example, Firefox and Chrome use a version of NSS, while
WebKit has a GnuTLS-based HTTPS back end, among others.
IV. CERTIFICATE VALIDATION IN SSL/TLS
The only mechanism for server authentication in SSL/TLS
is the client‚Äôs validation of the server‚Äôs X.509 public-key
certiÔ¨Åcate presented during the handshake protocol. Client
authentication is less common (in a typical HTTPS browsing
session, only the server is authenticated). It involves symmetric
steps on the server side to validate the client‚Äôs certiÔ¨Åcate.
X.509 certiÔ¨Åcate validation is an extremely complex pro-
cedure, described in several semi-formal RFCs [64, 65, 66, 67,
68, 69, 70, 71]. Below, we give a very brief, partial overview
of some of the key steps.
Chain of trust veriÔ¨Åcation. Each SSL/TLS client trusts a
number of certiÔ¨Åcate authorities (CAs), whose X.509 certiÔ¨Å-
cates are stored in the client‚Äôs local ‚Äúroot of trust.‚Äù We will
refer to these trusted certiÔ¨Åcate authorities as root CAs, and
to their certiÔ¨Åcates as root certiÔ¨Åcates. The list of root CAs
varies from application to application and from OS to OS.
For example, the Firefox Web browser ships with 144 root
certiÔ¨Åcates pre-installed, while the Chrome Web browser on
Linux and MacOS relies on the OS‚Äôs list of root certiÔ¨Åcates.
Each X.509 certiÔ¨Åcate has an ‚Äúissuer‚Äù Ô¨Åeld that contains
the name of the certiÔ¨Åcate authority (CA) that
issued the
certiÔ¨Åcate. The certiÔ¨Åcate presented by the server (we‚Äôll call it
the leaf certiÔ¨Åcate) should be accompanied by the certiÔ¨Åcate
of the issuing CA and, if the issuing CA is not a root CA, the
certiÔ¨Åcates of higher-level CAs all the way to a root CA.
118
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:02:03 UTC from IEEE Xplore.  Restrictions apply. 
As part of certiÔ¨Åcate validation, the client must construct a
valid chain of certiÔ¨Åcates starting from the leaf certiÔ¨Åcate and
ending in a root certiÔ¨Åcate (see an example in Fig. 1). Below,
we list some of the checks involved in validating the chain.
These brief synopses are very informal and incomplete, please
refer to RFC 5280 [69] for the full explanation.
Each certiÔ¨Åcate in the chain must be signed by the CA
immediately above it and the root (‚Äúanchor‚Äù) of the chain must
be one of the client‚Äôs trusted root CAs.
The current time must be later than the value of each
certiÔ¨Åcate‚Äôs ‚Äúnot valid before‚Äù Ô¨Åeld and earlier than the value
of each certiÔ¨Åcate‚Äôs ‚Äúnot valid after‚Äù Ô¨Åeld, in the time zone
speciÔ¨Åed in these Ô¨Åelds. If no time zone is speciÔ¨Åed, then
Greenwich Mean Time (GMT) should be used.
If a CA certiÔ¨Åcate in an X.509 version 1 or version 2
certiÔ¨Åcate, then the client must either verify that it is indeed
a CA certiÔ¨Åcate through out-of-band means or reject
the
certiÔ¨Åcate [69, 6.1.4(k)]. The following checks apply only to
X.509 version 3 certiÔ¨Åcates.
the basic constraints extension:
For each CA certiÔ¨Åcate in the chain, the client must verify
‚Ä¢ The ‚ÄúCA bit‚Äù must be set. If the CA bit is not set, then
the current certiÔ¨Åcate cannot act as a root or intermediate
certiÔ¨Åcate in a certiÔ¨Åcate chain. The chain is not valid.
‚Ä¢ If the CA certiÔ¨Åcate contains a ‚Äúpath length‚Äù constraint,
the number of intermediate CAs between the leaf certiÔ¨Å-
cate and the current certiÔ¨Åcate must be less than the path
length. For example, if the CA certiÔ¨Åcate has path length
of 0, it can be used only to issue leaf certiÔ¨Åcates.
Every extension in a certiÔ¨Åcate is designated as critical
or non-critical. A certiÔ¨Åcate with a critical extension that the
client does not recognize or understand must be rejected.
If a CA certiÔ¨Åcate in the chain contains a name constraints
extension, then the subject name in the immediately following
certiÔ¨Åcate in the chain must satisfy the listed name constraints.
Name constraints are used to limit the subjects that a CA can
issue certiÔ¨Åcates for, by listing permitted or excluded subjects.
This extension is critical.
If a certiÔ¨Åcate in the chain contains a key usage extension,
the value of this extension must include the purpose that the
certiÔ¨Åcate is being used for. For example, the key usage of
an intermediate certiÔ¨Åcate must include keyCertSign (it
must also have the CA bit set in the basic constraints, as
described above). If a leaf certiÔ¨Åcate contains the server‚Äôs
RSA public key that will be used to encrypt a session key,
its key usage extension must include keyEncipherment.
CAs should mark this extension as critical.
Similar to key usage, if a certiÔ¨Åcate contains an extended
key usage extension, the value of this extension must include
the purpose that the certiÔ¨Åcate is being used for, e.g., server
authentication in the case of a leaf certiÔ¨Åcate.
If a certiÔ¨Åcate contains an Authority Key IdentiÔ¨Åer (AKI)
extension, then its value‚Äîcontaining the key identiÔ¨Åer and/or
issuer and serial number‚Äîshould be used to locate the public
key for validating the certiÔ¨Åcate. This extension is used when
the certiÔ¨Åcate issuer has multiple public keys.
If a certiÔ¨Åcate contains a CertiÔ¨Åcate Revocation List (CRL)
distribution points extension, the client should obtain CRL
information as speciÔ¨Åed by this extension.
The above list omits many important checks and subtleties
of certiÔ¨Åcate validation. For example, CA certiÔ¨Åcates may
contain policy constraints that limit their authority in various
ways [69, 4.2.1.11]. Policy constraints extension should be
marked as critical, although in practice few SSL/TLS imple-
mentations understand policy constraints.
Hostname veriÔ¨Åcation. After the chain of trust has been val-
idated, the client must verify the server‚Äôs identity by checking
if the fully qualiÔ¨Åed DNS name of the server it wants to talk to
matches one of the names in the ‚ÄúSubjectAltNames‚Äù extension
or the ‚ÄúCommon Name‚Äù Ô¨Åeld of the leaf certiÔ¨Åcate. Some SS-
L/TLS implementations perform hostname veriÔ¨Åcation, while
others delegate it to higher-level applications (see Table IX).
V. CURRENT TESTING PRACTICES FOR SSL/TLS
IMPLEMENTATIONS
Most SSL/TLS implementations analyzed in this paper
ship with several pre-generated X.509 certiÔ¨Åcates intended for
testing (Table I). These certiÔ¨Åcates differ only in a few Ô¨Åelds,
such as hashing algorithms (SHA-1, MD5, etc.), algorithms
for public-key cryptography (DSA, RSA, DifÔ¨Åe-Hellman, etc.),
and the sizes of public keys (512 bits, 1024 bits, etc.).
OpenSSL uses a total of 2 certiÔ¨Åcates to test client and server
authentication, respectively; the rest are intended to test other
functionalities such as certiÔ¨Åcate parsing.