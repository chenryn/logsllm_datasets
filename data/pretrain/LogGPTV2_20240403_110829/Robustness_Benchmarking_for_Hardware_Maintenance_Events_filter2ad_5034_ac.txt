3
3
3
3
3
3
3
5500
500
1000
6000
5000
5000
5000
5000
1250
3000
2000
1
64
1
4
3
2
2
2
6
1
1
5500
32000
1000
24000
15000
10000
10000
10000
7500
3000
2000
Table 2: System B FRU List and Maintenance Classes
the FRUs. This
by no means implies that we are
suggesting that the actual benchmark testing be skipped
in a  real benchmark production. 
As is evident from column 2 in the tables, more of
System B's FRUs are hot swappable than System A; this
is one of the key RAS enhancements of System B over
its predecessor. Column 3 lists the failure rate of each
FRU. The unit of the failure rate
is the number of
failures in 109 hours of operation time or FIT. To protect
company confidential
rate
information in column 3 is using artificial values for
illustrative purposes. We will address the issue of
information,
the
failure
confidential
information in Section 5. In both tables,
column 4 enumerates the number of each FRU type in the
system. Column 5 lists the combined failure rate for each
FRU type, which is the product of the number of each
type and its failure rate.
Tables 3 and 4 illustrate the steps for MRB-A
calculation for Systems A and B respectively. Column 2
in both these tables lists the combined failure rates of
FRUs in each maintenance class, derived from the total
failure rates of FRU types given in Tables 1 and 2. PMCi,
which is the percentage of the aggregate of failure rates
of FRUs in maintenance class i over the total system
Combined
Failure Rate
PMCi
MCFi
PMCi x MCFi
Maintenance Class 1
95500
80.93%
Maintenance Class 2
0
0.00%
Maintenance Class 3
22500
19.07%
1
10
100
MRB-A
0.81
0.00
19.07
19.88
Table 3: MRB-A Calculation for System A
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:30:11 UTC from IEEE Xplore.  Restrictions apply. 
Combined
Failure Rate
PMCi
MCFi
PMCi x MCFi
Maintenance Class 1
Maintenance Class 2
Maintenance Class 3
MRB-A
38500
24000
57500
32.08%
20.00%
47.92%
1
10
100
0.32
2.00
47.92
50.24
Table 4: MRB-A Calculation for System B
hardware failure rate, is shown in column 3 of both these
tables. By comparing these PMCi values, one can clearly
see that System B has a higher percentage of
its
maintenance events belonging to class 3 when compared
to System A. Finally, the system MRB-A is calculated as
the weighted average of MCFis, with PMCis being the
weights, as shown in the last column. Not surprisingly,
System B has a higher MRB-A benchmark rating than
System A, namely 50.24 for System B vs 19.88 for
System A. 
individual FRUs. Only an aggregate percentage of
failure rates of all FRUs within each of the three
maintenance classes is needed as shown in Tables 3
and 4 of Section 4.
 Rules. Rules need to be established to govern the
benchmark production to prevent benchmark gaming.
For example, there should be rules to govern system
configurations in order to
prevent vendors from
overloading certain type of FRUs in a system to make
their benchmark result look better. 
5. Issues
The following is a short list of important issues that
must be addressed in order to facilitate wide adoption of
the proposed benchmark.
 Failure Rate Prediction. This benchmark requires
assigning weights to the MCFs in a system, based on
the predicted failure rate in each maintenance class.
We think the industry standard Telcordia332 is well
suited for this purpose since it provides a consistent
estimate of hardware failure rate. It is to be noted that
we do not advocate calculating failure rates based on
field data because field data is often collected in an
uncontrolled fashion and the data collection process
vary from company to company,
therefore becomes
unsuitable for  benchmarking purposes. 
 Reporting. Vendors will perform the benchmark test
and publish the benchmark result. The reporting
requirement should balance the need for protecting
vendor proprietary information and the need for
sufficient disclosure of information as required by
public as well as competitors. Historically vendors
have been reluctant to publish failure data, predicted
or actual, of their products for fear of giving away
company secrets. This problem can be solved by not
requiring vendors to publish the failure rate data for
rate
calculation and maintenance
 Auditing. All benchmark results should be audited by
a certified third party. The auditor should verify the
failure
event
classification, as well as ensure that the benchmark is
produced in accordance with the rules established in
the benchmark specification. We envision a process
similar to the one that is used by TPC in which an
auditor must be qualified by the benchmarking
standard body and is paid by the vendor to perform
the audit. 
Unlike performance benchmarks such as TPC, MRB-A
is relatively straightforward to produce so we do not
think that cost is a major concern here. To the best of our
knowledge, all computer vendors perform hardware
failure rate prediction during their product design phase.
We estimate that maintenance event simulation will take
between 1-5 days depending the size of the server.
So
the cost for producing an MRB-A benchmark should be
much lower
the traditional
performance benchmarks. We are currently working on
the benchmark specification which will address the issues
mentioned in this section.
than producing some of
6. Summary and Future Work
In this paper, we proposed a benchmark, MRB-A, to
system robustness against
quantitatively characterize
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:30:11 UTC from IEEE Xplore.  Restrictions apply. 
of
the
overall
fault-induced hardware maintenance events. Until now,
the only way to compare robustness of systems against
maintenance events was to compare their feature lists.
These feature lists often do no match each other and leave
significant room for interpretation. MRB-A provides a
quantitative measurement
system
robustness against hardware repair events on a production
system, and thus facilitates an objective comparison of
systems with completely different feature lists. MRB-A
also provides system designers with the ability to measure
progress in a system's capability of handling fault induced
maintenance events.
the proposed
approach is the simple technique it employs to determine
the exhaustive maintenance load and the percentage of
maintenance events in each of the three maintenance
classes. MRB-A is currently used by Sun Microsystems
during product development.
The novelty of
To the best of our knowledge, MRB-A is the first
availability benchmark to address maintenance events and
it
is also the first completely portable availability
benchmark that can be used to compare different products
including those from different companies. Our next
effort in this area is to research the issues underlying the
development of
robustness benchmarks for non-fault
induced maintenance events as well as for software
maintenance events. As reported by a recent study,
maintenance events account for a majority of system
outages, and we hope that our work will inspire other
researchers
system
dependability against maintenance events. 
benchmarking
look
into
to
International Symposium on Fault-Tolerant Computing , June,
1995.
[4]. P. Koopman et al, "Comparing Operating Systems Using
Robust Benchmarks", Proceedings of the 16th Symposium on
Reliable Distributed Systems, pp. 72-79, Oct. 1997.
and
[5]. H. Madeira
“Dependability
Benchmarking: making choices in an n-dimensional problem
space.” Proceedings of the first workshop on Evaluating and
Architecting System Dependability, July, 2001.
P. Koopman,
[6]. K. Kanoun et al, “A Framework for Dependability
2002
Benchmarking”,
International Conference.
and
Networks, pp. F.7-F.8, June, 2002.
on Dependable Systems
Supplemental Volume
the
of
[7]. B. Miller et al, "An Empirical Study of Reliability of UNIX
Utilities", Comm. of the ACM, Vol 33, No. 12, pp. 32-43, Dec
1990.
[8]. A. Mukherjee and D. P. Siewiorek, "Measuring Software
IEEE
Dependability
Transactions on Software Engineering, Vol. 23, No. 6, June
1997.
Benchmarking",
Robustness
by
[9]. D. P. Siewiorek et al, "Development of a Benchmark to
Measure System Robustness", Proceedings of
the 1993
International Symposium on Fault-Tolerant Computing, pp.
88-97, June 1993.
[10].
"http://www.tpc.org/information/about/abouttpc.asp". Current.
Transaction
Processing
Council.
7. Acknowledgments
We wish to thank our respective management for their
support of this effort, and their continued commitment to
this work. Specifically, we wish to thank John
Bongiovanni, David Nelson-Gal, Ganesh Ramamurthy,
Roy Andrada, and Michael Chow. We owe a debt of
gratitude to Allan Packer, Jim Lewis, and William Bryson
for reviewing this document and providing thoughtful and
insightful feedback and commentary, and suggestions for
improvement.
8. References
[11]. T. Tsai et al, "An Approach Towards Benchmarking of
Fault Tolerant Commercial Systems", Proceedings of the 1996
Symposium on Fault-Tolerant Computing, pp. 314-323, June
1996.
[12]. J Zhu et al, "R-Cubed (R3): Rate, Robustness, and
Recovery – An Availability Benchmark Framework", Technical
Report Series #TR-2002-109, July 2002, Sun Microsystems,
Inc.
et
[1]. A. Avizienis
of
Dependability", UCLA CSD Report no. 010028, LAAS Report
no. 01-145, Newcastle University Report no. CS-TR-739,
2001.
"Fundamental Concepts
al,
[2]. A. Brown and D.A. Patterson, "Towards Availability
Benchmarks: A Case Study of Software RAID Systems",
USENIX 2000.
[3]. C. Dingman et al, "Measuring Robustness of a Fault
25th
Tolerant Aerospace
System", Proceedings
the
of
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:30:11 UTC from IEEE Xplore.  Restrictions apply.