title:Enhanced server fault-tolerance for improved user experience
author:Manish Marwah and
Shivakant Mishra and
Christof Fetzer
International Conference on Dependable Systems &Networks: Anchorage, Alaska, June 24-27 2008
Enhanced Server Fault-Tolerance for Improved User Experience
Manish Marwah
Hewlett-Packard Laboratories,
Palo Alto, CA 94304
Shivakant Mishra
Department of Computer Science,
University of Colorado, Boulder, CO 80309
Christof Fetzer
Department of Computer Science,
TU-Dresden, Dresden, Germany D-OI062
Abstract
Interactive applications such as email, calendar, and
maps are migratingfrom local desktop machines to data cen(cid:173)
ters due to the many advantages offered by such a computing
environment. Furthermore, this trend is creating a marked
increase in the deployment ofservers at data centers. To ride
the price/performance curves for CPU, memory and other
hardware, inexpensive commodity machines are the most cost
effective choicesfor a data center. However, due to low avail(cid:173)
ability numbers ofthese machines, the probability ofserver
failures is relatively high. Server failures can in turn cause
service outages, degrade user experience and eventually re(cid:173)
sult in lost revenue for businesses. We propose a TCP splice(cid:173)
based Web server architecture that seamlessly tolerates both
Web proxy and backend server failures. The client TCP con(cid:173)
nection and sessions are preserved, andfailover to alternate
servers in case ofserver failures is fast and client transpar(cid:173)
ent. The architecture provides support for both determinis(cid:173)
tic and non-deterministic server applications. A prototype of
this architecture has been implemented in Linux, and the pa(cid:173)
per presents detailed performance results for a PHP-based
webmail application deployed over this architecture.
1 Introduction
In recent years, computing applications and services are
moving away from local desktop machines to remote data
centers. This computing paradigm is attractive for a number
of reasons: (1) It frees users from the issues and costs related
to installing, maintaining and upgrading local software ap(cid:173)
plications; (2) It allows easy access to applications and data
from any location (using Internet connectivity); (3) It facili(cid:173)
tates sharing and collaboration among multiple users who are
geographically separated; and (4) It simplifies sending criti(cid:173)
cal client software updates such as bug and security fixes to
the clients (client scripts can be downloaded by the browser
each time they are used). In the future, more applications are
likely to migrate to remote data centers, effectively remote
desktops that people can access via thin clients.
In order to ride the performance/cost curve for CPU,
memory and other hardware, inexpensive commodity ma(cid:173)
chines are most cost effective for a data center. However,
their availability numbers are low (about three nines). Thus
use of commodity machines, rather than customized, hard(cid:173)
ened machines, leads to more server failures and service out(cid:173)
ages which in tum degrades user experience and results in
lost revenue for businesses. For example, if a user is brows(cid:173)
ing a map service like MSN, Google or Yahoo maps, a server
failure leading to an outage of more than afew seconds is de(cid:173)
tectable by a user and hence degrades user experience. How(cid:173)
ever, if server failures can be seamlessly handled, the low
availability numbers ofa server is not an problem.
Many emerging Web applications are highly interactive
(e.g., map browsing services) or even real time (e.g., stock
market ticker). Other applications - such as word process(cid:173)
ing and spreadsheets - that have traditionally resided on a
desktop are beginning to be hosted at a remote data center.
In order to ensure seamless user experience, these applica(cid:173)
tions put greater fault-tolerance demands on data centers. At
present, server failures in Web server farms are typically han(cid:173)
dled as follows: 1) The client detects a server failure (usu(cid:173)
ally by noticing an absence of response for some period of
time); 2) The client reissues the request; 3) The re-sent re(cid:173)
quest likely reaches a working server; 4) The new server
handles the request. This procedure can easily take tens of
seconds if not more. It is clear that these traditional mecha(cid:173)
nisms for handling server failures are no longer acceptable.
In particular, application response times have a direct impact
on user experience. It was recently reported [8] that Google
has re-architected parts of its Gmail application in order to
make the application faster - "[we are] profiling all parts of
the application, shaving milliseconds off wherever we can".
While this would clearly provide a better user experience dur(cid:173)
ing normal operation, it does not address the problems of pro(cid:173)
longed response times in case of server failures. In order to
minimize the impact of low availability commodity servers,
server failure recovery must be performed fast such that it is
seamless to the user.
In this paper, we describe the design and implementa-
tion of a Web server architecture that provides improved user
experience. In particular, it seamlessly tolerates failures of
intermediate proxies that perform content-based routing as
well as backend servers that process client requests. Fur(cid:173)
thermore, it provides support for handling non determinism
in server applications during server failures. To provide im(cid:173)
proved user experience, this architecture incorporates the fol-
1-4244-2398-9/08/$20.00 ©2008 IEEE
167
DSN 2008: Marwah et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:29:45 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems &Networks: Anchorage, Alaska, June 24-27 2008
lowing important features: (1) Proxy or server failure detec(cid:173)
tion is performed locally at the server end that is completely
transparent to a client. This ensures a fast failure detection,
in particular when a client is connecting over a wide area net(cid:173)
work (WAN). Fast failure detection by a client over a WAN
is problematic as it leads to increased traffic and is prone to
false positives. (2) Failover is significantly faster - at most
a few seconds, something a client will consider a minor net(cid:173)
work glitch. (3) All client sessions and states are preserved
during failover resulting again in a faster and seamless recov(cid:173)
ery.
The complete system architecture described in this paper
is based on some of our earlier work on enhancements to
TCP splicing mechanisms [11] and systems architectures for
transactional network interfaces [12]. There are three unique
contributions of this paper. First, the TCP splicing mecha(cid:173)
nism is adapted for seamless backend server failover. It al(cid:173)
lows for transparently redirecting current and future client
requests to an alternate backend server in case of original
backend server failures. Second, concepts of request transac(cid:173)
tionalization, tagging and logging have been introduced and
assimilated to provide support for fast failover and seamless
recovery. Finally, a prototype of the complete system archi(cid:173)
tecture has been built and experimented with in both LAN
and WAN (PlanetLab) settings. Fast failover and seamless
recovery are demonstrated by deploying a real-world appli(cid:173)
cation (RoundCube Webmail [16], an open-source webmail
client) over this architecture.
The rest of this paper is organized as follows. Section 2
describes some of our earlier work on which this work is built
as well as some related work. Section 3 provides a high-level
description of our complete system architecture. Section 4
describes important details including TCP re-splicing, trans(cid:173)
actionalization and tagging, and recovery mechanisms. Sec(cid:173)
tion 5 describes some salient features of our prototype im(cid:173)
plementation. Section 6 describes the details of RoundCube
Webmail deployment over our architecture, and the perfor(cid:173)
mance measured from this deployment under many different
operating scenarios. Finally, Section 7 concludes the paper
and discusses some future work.
2 Background
2.1 TCP splice
Web proxies are exceedingly used in Web server architec(cid:173)
tures for implementation of layer 7 (or content-aware) rout(cid:173)
ing, security policies, network management policies, usage
accounting, and Web content caching. An application level
Web proxy is inefficient since relaying data from a client to
a server involves transferring data between kernel-space and
user-space that results in additional context switches. TCP
Splice was proposed [9, 17] to enhance the performance of
Web proxies. It allows a proxy to relay data between a client
and a server by manipulating packet header information en(cid:173)
tirely in the kernel. This makes the latency and computa(cid:173)
tional cost at a proxy only a few times more expensive than
that of IP forwarding. There is no buffering required at the
proxy that performs the splicing, and, furthermore, the end(cid:173)
to-end semantics of a TCP connection are preserved between
the client and the server. Advantages of TCP splicing in Web
server architectures are further described in [15, 14, 5]. The
following steps are required in establishing a TCP splice:
• A client connects to a proxy. The proxy accepts the con(cid:173)
nection and receives the client request.
• The proxy performs authentication and other functions
as configured by the administrator, and then performs
layer 7 routing to select a backend server. It creates a
new TCP connection to the selected server.
• The proxy sends the client request to the server and
"splices" the client-proxy and the proxy-server TCP
connections.
• After the two TCP connections are spliced, the proxy
acts as a relay -
the packets coming from the client
are sent on to the server, after appropriate (in-kernel)
modification of the header that makes the server believe
that those packets are part of the original proxy-server
TCP connection); similarly, the packets received from
the server are relayed to the client after appropriate (in(cid:173)
kernel) header modifications.
2.2 Enhancements to TCP splice
The TCP splice mechanism described above suffers from
two major drawbacks:
(1) All traffic between a client and
a server (both directions) must pass through a proxy, thus
making the proxy scalability and performance bottlenecks;
and (2) this architecture is not fault-tolerant; if a proxy fails,
all the spliced TCP connections hosted on it fail as well, and
clients have to re-establish their HTTP connections and re(cid:173)
issue failed requests. This would be true even in the presence
of a backup proxy.
In order to address the scalability/performance bottle-
necks and fault-tolerance issues, we proposed two important
enhancements to the TCP splice mechanism [11]: (1) Repli(cid:173)
cated TCP splice: The splice state information is replicated
on multiple proxies allowing one TCP connection to use mul(cid:173)
tiple proxies. This distributed architecture provides both in(cid:173)
creased scalability and fault-tolerance; in fact, proxy fault(cid:173)
tolerance becomes trivial to implement as it only involves
detecting that a proxy has failed and then ceasing to send
packets to it. (2) Split TCP splice: The TCP splice function(cid:173)
ality is split into two unidirectional splices with packets in
the two directions being spliced at different machines. The
packets destined to the server are spliced at a proxy as usual,
however, response packets are spliced at the server and thus
bypass the proxy. Splicing state information is copied to the
server in order to achieve this. This further improves the scal(cid:173)
ability of the system particularly in cases where the response
is large.
2.3 Related work
Our architecture is most related to FT-TCP [2, 19], ST(cid:173)
TCP [10], Backdoors [4, 18] and other similar systems.
Unlike our architecture, FT-TCP and ST-TCP use an ac(cid:173)
tive backup which processes all requests sent to the primary
server. Furthermore, the applications are required to be deter(cid:173)
ministic. Our architecture provides greater scalability by not
requiring a dedicated backup and non-determinism is han(cid:173)
dled.
1-4244-2398-9/08/$20.00 ©2008 IEEE
168
DSN 2008: Marwah et al.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 12:29:45 UTC from IEEE Xplore.  Restrictions apply. 
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 2008
Backdoors [4, 18] requires a specialized NIC capable of
performing remote direct memory access (RDMA). This al(cid:173)
lows a backup to read state information from a failed primary.
Thus, Backdoors does not work if the failure impairs the pri(cid:173)
mary memory, or, access to it. Our architecture can tolerate
any server failure and no special hardware is required. Fur(cid:173)
thermore, Backdoors requires kernel modifications on pri(cid:173)
mary and backup machines. Although kernel modules are
needed on logger and proxy machines, we do not require ker(cid:173)
nel modification in backend server machines.
3 System architecture: An Overview
Figure 1 illustrates five logical components of our Web
server architecture. Note that these are the functional compo(cid:173)
nents of the system. In an actual instantiation of this architec(cid:173)
ture, some of these components can be resident on the same
machine. The five logical components are: (1) Stateless load
balancers: Distribute incoming client requests to the prox(cid:173)
ies; (2) Proxies: Perform layer 7 routing, TCP splicing, and,
re-splicing during recovery; (3) Backend servers: Process
client requests, send back responses, and, asynchronously
send application session state information to alternate back(cid:173)
end servers; (4) Loggers: Transparently log traffic, parse re(cid:173)
quests and responses into tagged transactions, detect failure
of backend servers, and assist in backend server recovery;
and (5) Auxiliary servers: Additional servers that backend
servers may contact for processing client requests.
I
Auxilary
Servers
I
Load BalanceraClient
I-Stateless
1.1
I loggerI
Proxies
Backend
Servers
Figure 1: Components of our Web Server Architecture.
Stateless load balancers distribute incoming client pack(cid:173)
ets among the proxies. As its name suggests, a load balancer
is completely stateless and a packet is distributed regardless
of the TCP connection to which it may belong. This also
implies that load balancer fault-tolerance is simple to imple(cid:173)
ment since there is no state to synchronize. The load balancer
could also be co-resident at a layer 2 switch or an IP router.
For new connections, a proxy performs layer 7 routing and
TCP splicing, and replicates the TCP splice among all prox(cid:173)
ies. Once a TCP splice has been replicated, subsequent client
requests can be handled (in-kernel header transformation and
forwarding to the appropriate backend server) by any proxy
in the proxy stage. A proxy failure is trivial to handle. The
recovery action comprises a load balancer detecting that a
proxy has failed and ceasing thereafter to send any packets to
that proxy [11]. In fact, multiple proxy failures are handled
similarly. In this paper, we extend the role of proxies to as(cid:173)
sist in recovery from backend server failure by participating
in state synchronization of the alternate backend server and
finally re-splicing the client TCP connection to that server.
A client request is handled at a backend server. A backend
server can itself process a number of client requests. How(cid:173)
ever, it may need to contact one or more additional servers in
some instances for further processing. For example, a back(cid:173)
end server may need to access a shared external database,
or an email store. We refer to these additional requests as
auxiliary (aux for short) requests, and the servers that han(cid:173)
dle these requests as aux servers. For certain client requests,
a backend server may need to issue multiple aux requests.
In order to correlate session state information and aux re(cid:173)
quests/responses with the appropriate transaction, a unique
transaction ID is assigned to each transaction. This ID is
computed from the client IP address and port number of the
TCP connection and the ordinality of the transaction on that
connection.
To facilitate seamless recovery, there are two points in
the system where IP packets are logged -
one is between
a proxy and a backend server, where client requests and cor(cid:173)
responding server responses are logged (front-end logger);
the other is between a backend server and an aux server (aux
logger), where aux requests and corresponding aux server
responses are logged. Although two different loggers are
shown in Figure 1, a single physical logger can be used for
logging at both of these locations. In addition to logging IP
packets, a logger performs a number of important functions