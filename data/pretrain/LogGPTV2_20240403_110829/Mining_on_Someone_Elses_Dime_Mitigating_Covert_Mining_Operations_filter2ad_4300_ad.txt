MineGuard Under an Active Adversary: In an attempt to throw oﬀ Mine-
Guard, a clever attacker could selectively create noise in the HPCs by running
computations in parallel that inﬂuence counters not involved in mining. This
would artiﬁcially inﬂate the values and modify patterns of certain HPCs that
are irrelevant to the mining signature and appear as a benign workload to the
classiﬁer. To check the eﬀectiveness of this scheme we performed an experiment
with Litecoin where we modiﬁed the miner’s code to run a computation in par-
allel that predominantly aﬀects the set of mining-orthogonal counters (HPCs
not showing signiﬁcance use during mining). We measured how increasing the
number of threads for the extra computation negatively impacts the total hash
rate along with the corresponding reduction in MineGuard’s detection accuracy.
Figure 7B captures this relationship for 100 diﬀerent runs of the aforementioned
experiment. As expected, increasing the number of threads for the camouﬂaging
computation severely degrades the hash rate (base hash rate is approximately
280000000
OS
200000000
120000000
260000000
VM
180000000
100000000
Mitigating Covert Mining Operations in Clouds and Enterprises
301
L1 Loads (count/100 ms)
False Positives
False Negatives
1
BFGMiner
301
CGMiner
(A)
601
901
CPUMiner
Number of Threads
(B)
Fig. 7. (A) Eﬀect of virtualization-induced noise on L1 Loads for various miners all
mining for Bitcoin. The x axis shows time in increments of 100 ms. (B) Degradation
of mining hash rate as the number of masking threads is increased. The hash rate falls
consistently while the detection rate remains constant throughout, with only a slight
increase in false negatives as 8 threads are used.
30 kH/s). However, it has very little impact on the detection rate meaning that
the exercise would not be of beneﬁt to the attacker. Granted, the experiment
covers only a small subset of the overall computation space available to the
attacker, we still feel that the impact suﬀered by the hash rate will be much
more severe compared to the hit taken by the classiﬁer in nearly all cases.
Feature (Counter) Selection: We now present a formal approach to feature
(counter) selection to determine the importance of each counter, both by itself
and in relation to other counters. When looking at each counter individually, we
use mutual information to determine its importance. The mutual information
(MI) of two random variables is a measure of the mutual dependence between
the two variables. More speciﬁcally, it quantiﬁes the “amount of information”
(in units such as bits or entropy) one random variable contributes to generating
a unique signature for the miner. When looking at multiple features together,
their importance as a whole is represented by joint mutual information (JMI),
a measure of the features’ combined entropy. JMI can then be used to rank
features from most important to least important. In turn, the ranking can be used
to choose the minimum number of features that provide the best classiﬁcation
accuracy.
Table 2 lists the 26 diﬀerent counters that were available on our system. To
obtain MI and JMI for each counter, we used FEAST, an open source toolbox
for feature selection algorithms [31]. The entropy (MI) of all 26 counters, both
in an OS setting and in a VM setting, is shown in Fig. 8. It can be seen that
features can be broadly divided into three categories. First, certain features like
feature ID 1 (clock cycles), 5 (bus cycles) and 8 (task clock) hold a signiﬁcant
amount of information in both OS and VM environments. Second, features like
feature ID 9 (page faults) and 10 (context switches) contribute negligibly to
the classiﬁcation process in both environments. Finally, the remaining features
provide varying amounts of information depending upon the environment. While
302
R. Tahir et al.
Table 2. HPCs used for CPU-based signatures along with their JMI rank and expla-
nation.
Name of counter
Counter ID OS rank VM rank Explanation
cycles
instructions
branches
branch-misses
bus-cycles
stalled-cycles-frontend
stalled-cycles-backend
task-clock
page-faults
context-switches
cpu-migrations
L1-dcache-loads
L1-dcache-load-misses
L1-dcache-stores
L1-dcache-store-misses
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
L1-dcache-prefetch-misses 16
L1-icache-load-misses
LLC-loads
LLC-stores
LLC-prefetches
dTLB-loads
dTLB-load-misses
dTLB-stores
dTLB-store-misses
iTLB-loads
iTLB-load-misses
17
18
19
20
21
22
23
24
25
26
4
6
2
16
8
1
11
3
26
24
25
13
21
7
14
18
15
12
20
9
5
17
10
23
19
22
4
6
19
15
1
5
16
3
26
24
25
14
13
7
8
17
10
11
2
20
12
21
9
22
23
18
# of CPU clock cycles
# of executed instructions
# of branch instructions
# of mispredicted branches
# of useful bus cycles
# of stalled cycles in frontend of
pipeline
# of stalled cycles in backend of
pipeline
CPU time in milliseconds
# of page faults
# of context switches
# of migrations of proﬁled app
# of loads at L1 data cache
# of load misses at L1 data cache
# of stores at L1 data cache
# of store misses at L1 data cache
# of misses at L1 cache that did not
beneﬁt from prefetching
# of instruction fetches missed in the
L1 instruction cache
# of loads at the Last Level Cache
# of loads that missed in the data
TLB
# of stores that queried the data TLB
# of stores at the Last Level Cache
# of prefetches at the Last Level
Cache
# of loads that queried the data TLB
# of stores that missed in the data
TLB
# of instruction fetches that queried
the instruction TLB
# of instruction fetches that missed in
the instruction TLB
the general trends are the same in both environments, the diﬀerences between
the two graphs present the importance of performing feature selection for each
environment.
We present feature ranking results for both OS and VM environments based
on JMI in Table 2. Feature rankings mimic the patterns observed in MI - certain
features like 2 (instructions) do not change rank while others like 3 (branches)
change rank signiﬁcantly. Another interesting observation is that system level
events like page faults and context switches have a low rank while purely
hardware-based events like loads and stores are ranked highly in both scenarios.
Classiﬁcation Accuracy: We now present results for MineGuard’s miner
detection performance in both closed and open world setting. A closed world
setting is a scenario in which every cryptocurrency that MineGuard can be
Mitigating Covert Mining Operations in Clouds and Enterprises
303
Fig. 8. The mutual information (entropy) contained within each hardware performance
counter for (A) an OS environment, (B) a VM environment.
requested to detect is a part of the signature database. The test sample may
vary from the signatures stored in the database but as we have previously shown,
miners have unique and consistent signatures, increasing the likelihood if the test
sample is from a miner, it will be matched to a miner in the signature database.
Table 3. Classiﬁcation results for three diﬀerent operating environments in a closed
world setting. Each result’s 95% conﬁdence interval is written in brackets.
Closed world scenario F-score (CI)
False positives (CI) False negatives (CI)
OS-Level
VM-Level
99.69% (0.13%) 0.22% (0.11%)
0.29% (0.25%)
99.69% (0.17%) 0.27% (0.14%)
0.26% (0.18%)
VM-Interference
99.15% (0.11%) 2.12% (0.29%)
0.04% (0.03%)
Open world scenario F-score (CI)
False positives (CI) False negatives (CI)
OS-Level
VM-Level
94.91% (1.02%) 4.50% (1.13%)
2.58% (1.64%)
93.58% (1.33%) 5.63% (1.61%)
4.52% (2.41%)
VM-Interference
95.82% (0.86%) 6.77% (1.45%)
2.53% (1.54%)
Table 3 shows our results for this scenario where all values are reported after
100 runs. Since MineGuard has been trained on every cryptocurrency in the
test set, it achieves an exceptionally high miner detection accuracy. It achieves
≈99.5% accuracy with a false positive rate (FPR) of 0.22% and false negative
rate (FNR) of 0.29% when classifying miners running solely in either the OS and
VM setting. This equates to near-perfect miner detection and implies that if a
known cryptocurrency is being mined in an OS or a VM, MineGuard will detect
it almost every time. When classifying miners running with other applications,
the average F-score drops to 99.15% and FPR increases to 2.12%, while FNR
remains at ≈0%. Even in an open world setting, where all test signatures are
unseen (i.e., miners in the test set are unknown to the classiﬁers), MineGuard still
achieves accuracy ≈95% for all three cases. Though the results are slightly worse
than a closed world setting, they are still satisfactory overall. Furthermore, as
we explain in Sect. 7, unseen signatures are rare as zero-day coins are an unlikely
scenario.
The results shown in Table 3, have been computed on a per sample basis. This
means that the classiﬁer treats each 2 s sample of HPC values as an independent
304
R. Tahir et al.
test vector rather than labeling all samples collectively as miner/non-miner.
An alternate way is to use per application classiﬁcation and treat all samples
collected from a running process as a single test vector. This approach has the
advantage that given the number of samples for a particular application, the
classiﬁcation can be done using various ratios. For example, if 5 samples for an
application are available, if one is categorized as miner the entire application
is labeled as a miner. Similarly, we can use a scheme where all samples need
to be classiﬁed as a miner or use a simple majority rule (3 out of 5 classiﬁed
as miner then app is miner). In each case, the corresponding F-score, FPR and
FNR would be diﬀerent. In Table 4, we present open world results for a simple