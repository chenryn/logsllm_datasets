We evaluate Sunlight by answering the following questions: (Q1) 
How accurate is Sunlight’s against ground truth, where it is avail­
able?  (Q2) How do different Stage 1 algorithm’s hypotheses com­
pare?  (Q3)  What  is  the  inﬂuence  of  p-value  correction  on  Sun­
light?  (Q4) How does scale affect conﬁdence?  As foreshadowing, 
we  show  that  Sunlight’s  high-conﬁdence  hypotheses  are  precise, 
and that the Logit (logistic regression) method is best suited among 
those we evaluated for maximizing hypothesis recall after p-value 
correction. Somewhat surprisingly, we show that the “winning” in­
ference algorithm at Stage 1 (XRay’s) is not the winner at the end 
of the pipeline, after correction is applied. Finally, we show that the 
same effect in also responsible for a trade-off between conﬁdence 
and scalability in the number of outputs. 
6.1  Methodology 
We evaluate Sunlight using the system’s split of observations into 
a training and a testing set, and leveraging the modularity of Sun­
light to measure the effectiveness of targeting detection at differ­
ent stages of the analysis pipeline.  We believe that our evaluation 
methodology, along with the metrics that we developed for it, rep­
resents a signiﬁcant contribution and a useful starting point for the 
evaluation of future transparency infrastructures, an area that cur­
rently lacks rigorous evaluations (see §2.2). 
A critical challenge in evaluating Sunlight and its design space 
is the lack of ground truth for targeting for most experiments.  For 
example, in Gmail, we do not know how ads are targeted; we can 
take guesses, but that is extremely error prone (see §6.6).  In other 
cases (such as for Amazon and Youtube recommendations), we can 
obtain the ground truth from the services.  For a thorough evalua­
tion, we thus decided to use a multitude of metrics, each designed 
for a different situation and goal. They are: 
1.	  hypothesis precision: proportion of high-conﬁdence hypothe­
ses that are true given some ground truth assessment. 
2.	  hypothesis recall: proportion of true hypotheses that are found 
from some ground truth assessment. 
3.	  ad prediction precision, the proportion of success in predict­
ing if an ad will be present in a training set account.4 
4.	  ad prediction recall: proportion of ads appearances that were 
correctly guessed when predicting if an ad will be present in 
a training set account. 
5.	  algorithm coverage:  proportion of low p-value hypotheses 
found  by  an  algorithm,  out  of  all  low  p-value  hypotheses 
found by any of the algorithms. 
4“Ad” in this section is short for the more generic output. 
Workload 
Gmail (one day) 
Website 
Website-large 
YouTube 
Amazon 
Proﬁles 
119 
200 
798 
45 
51 
Inputs  Outputs
4099 
327 
84 
4867 
19808 
263 
308 
64 
61 
2593 
Table 1: Workloads used to evaluate Sunlight 
We use the ﬁrst two metrics in cases where ground truth is avail­
able (§6.2) and with manual assessments (§6.6).  These are typi­
cally small scale experiments. We use the next two metrics in cases 
where ground truth is unavailable; this lets us evaluate at full scale 
and  on  interesting  targeting.  Finally,  we  use  the  last  metric  for 
comparison of various pipeline instantiations. 
Table 1 shows the datasets on which we apply these metrics. The 
ﬁrst three datasets come from the experiments described in the pre­
ceding section.  The Gmail dataset corresponds to one day’s worth 
of ads in the middle of our 33-day experiment.  The YouTube and 
Amazon datasets are from our prior work XRay [18]. They contain 
targeting observations for the recommendation systems of YouTube 
and Amazon, for videos and products respectively. They are small 
(about 60 inputs), and with inputs on very distinct topics, minimiz­
ing the chances for targeting on input combinations.  On the other 
hand the Gmail and Websites datasets are larger scale, with up to 
327 inputs and thousands outputs.  Moreover their inputs are not 
distinct, containing some redundancy because they include emails 
or websites on the same topics that are more likely to attract simi­
lar outputs.  They are thus more representative of experiments that 
would be conducted by investigators. 
In all the evaluation, we use XRay as our baseline comparison 
with prior art.  XRay is Sunlight’s most closely related system, in­
heriting  from  it  many  of  its  design  goals,  including  its  focus  on 
scalable,  generic,  and ﬁne-grained targeting detection.  We leave 
quantitative comparison with other systems for future work and re­
fer the reader to our analytical comparison in §8. 
6.2  Q1: Precision and recall on ground truth 
Dataset 
Amazon 
YouTube 
Precision 
Recall 
Sunlight  XRay  Sunlight  XRay
78% 
100% 
100% 
68% 
81% 
93% 
46% 
52% 
Table 2: Sunlight’s hypothesis precision & recall 
Hyp. 
count
142 
1349 
Sunlight favors ﬁnding reliable, validated targeting hypotheses 
over ﬁnding every potential targeting, so that investigators do not 
waste time on dead ends. This strategy is characterized by hypothe­
sis precision that should be very high, and hypothesis recall that we 
try to keep high without lowering precision. We measure these two 
metrics on two datasets from YouTube and Amazon from the XRay 
paper [18],  both containing ground truth (Amazon and YouTube 
inform users why they are shown certain recommendations).  This 
gives us a direct comparison with prior art, as well as an assessment 
of Sunlight’s hypothesis precision and recall on services provided 
ground truth for recommendation targeting.  Table 2 describes the 
results. We make two observations. First Sunlight’s hypothesis pre­
cision against ground truth is 100% (with a Logit Stage 1) on both 
Amazon and YouTube, while XRay’s best algorithm reaches only 
81% and 93% respectively.  This conﬁrms Sunlight’s high hypoth­
esis precision that makes a difference even on simple cases. 
Second hypothesis recall is higher for XRay. The Bayesian algo­
rithm reaches 68% on YouTube and 78% on Amazon while Logit 
yields 46% and 52% respectively.  This can be explained by the 
small size of these datasets:  when faced with little evidence, Sun­
light will return no hypothesis or low conﬁdence hypotheses,  fa­
voring precision over recall compared to XRay’s algorithms.  We 
believe this is a valuable trade-off when performing large scale ex­
logit 
bayes 
lm 
set_intersection 
logit 
bayes 
lm 
set_intersection 
logit 
bayesian 
lm 
set_intersection 
1 
0.8
 0.6
 0.4
 0.2
 0
 1 
0.8
 0.6
 0.4
 0.2
 0
all_hyps 
lo
w
lo
w
lo
w
_pvals
_pvals_
_pvals_
w/B
w/H
all_hyps 
lo
w
lo
w
lo
w
_pvals
_pvals_
_pvals_
w/B
w/H
all_hyps 
lo
w
lo
w
lo
w
_pvals
_pvals_
_pvals_
w/B
w/H
olm 
(a) Ad prediction precision, Gmail
Y
olm 
(b) Ad prediction recall, Gmail
Y
olm 
(c) Ad pred. precision, Websites
Y
Figure 6: Ad prediction precision and recall.  The x-axis shows different p-value correction methods, and the y-axis shows the proportion of precision and 
recall. (a) and (b) show ad prediction precision and recall on the Gmail dataset, (c) and (d) on the Websites dataset, for all algorithms. Both metrics increase 
when using stricter p-values, indicating better hypotheses. 
periments.  In the absence of ground truth, we need to be able to 
trust targeting hypotheses even at the cost of some recall. 
bayesian 
bayes 
logit 
logit 
lm 
set_intersection 
lm 
set_intersection 
1 
 0.8
 0.6
 0.4
 0.2
 0
logit 
bayesian 
lm 
set_intersection 
lo
w
lo
w
lo
w
all_hyps 
_pvals
_pvals_
_pvals_
w/B
Y
w/H
olm 
(d) Ad pred. recall, Websites 
1 
 0.8
 0.6
 0.4
 0.2
 0
1
0.8
0.6
 0.4
0.2
 0
This conﬁrms Sunlight’s focus on precision over recall on datasets 
with ground truth. We next study more complex targeting with in-
puts on redundant topics, but that do not provide ground truth. 
6.3	  Q2: Evaluating the analysis pipeline 
We now look inside the analysis pipeline to measure the effects 
of its different stages and to compare stage 1 algorithms.  In or­
der to measure algorithm’s performances we use their ad predic­
tion precision and recall described in § 6.1. Intuitively if the algo­
rithms detect targeting, they can predict where the ads will be seen 
in the testing set. Because ads do not always appear in all accounts 
that have the targeted inputs, we do not expect precision to always 
be 100%.  On the other hand, a targeting hypothesis formed using 
many inputs may easily yield high recall. 
Fig. 6 shows the precision and recall of those predictions on the 
Gmail and Website datasets, ﬁrst on all hypotheses and then after 
selecting higher and higher conﬁdence hypotheses. We make three 
observations. First, the precision is poor if we take every hypothe-
ses into account (see group labeled all_hyps).  Precision is below 
80% for both datasets, and even less than 60% for most algorithms. 
Restricting to just the low p-value hypotheses (without correction) 
somewhat increases ad presence precision (low_pvals group). 
Second,  correcting  the  p-values  for  multiple  testing  increases 
precision  as  well  as  recall.  The  best  algorithms  on  the  Gmail 
and Website datasets,  respectively,  reach a precision of 90% and 
84% after BY correction, and 93% and 91% after Holm correction 
(low_pvals_w/BY and low_pvals_w/Holm groups).  The precision 
is higher when with Holm because it is more conservative than BY. 
Third, the differences introduced by Stage 1 algorithms are re­
duced by ﬁltering out low-conﬁdence hypotheses. While the preci­
sion with all hypotheses (all_hyps group) can vary of up to 40 per­
centage points, different Stage 1 algorithms vary only by 1 or 2 per­
centage points after Holm correction (low_pvals_w/Holm group). 
The exception is with the BY correction (low_pvals_w/BY group), 
where the precision of Logit is noticeably higher than that of the 
other algorithms on the Website dataset. 
Thus, when selecting only high-conﬁdence hypotheses, Sunlight 
is  able  to  predict  the  presence  of  an  ad  with  high  precision  and 
recall.  Moreover, all Stage 1 algorithms generally yield accurate 
high-conﬁdence hypotheses, which suggests that we should maxi­
mize the number of hypotheses.  We next compare the number of 
high-conﬁdence hypotheses and how it is affected by correction. 
6.4  Q3: The effect of p-value correction 
Maximizing the number of high-conﬁdence hypotheses is maxi­
mizing coverage (see § 6.1), the proportion of all high-conﬁdence 
hypotheses found by a given algorithm. Fig. 7 shows for each Stage 
1  algorithm  the  coverage  on  the  Gmail  and  Website  datasets  for
 1 
 0.8
 0.6
 0.4
 0.2
0
lo
w
_pvals 
lo
w
lo
w
_pvals_
_pvals_
w/B
Y
w/H
olm 
lo
w
_pvals 
lo
w
lo
w
_pvals_
_pvals_
w/B
Y
w/H
olm 
(a) Coverage, Gmail