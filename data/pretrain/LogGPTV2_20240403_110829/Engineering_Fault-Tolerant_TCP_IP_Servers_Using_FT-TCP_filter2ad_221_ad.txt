connection) of the data packets sent by the client or ac-
knowledgment packets sent by the server. About 1 sec into
the experiment the primary host crashes and acknowledg-
ments from the server cease, which soon causes the client
to also stop transmission of data when its TCP window ﬁlls
up. About 300 ms later the client’s retransmission timer
goes off and it attempts to resend the packet that follows the
last acknowledged packet (shown as a dip in the line). For
the purposes of analysis we forced the recovery to take a
very long time—2.5 seconds—and so retransmissions pro-
ceed unacknowledged at exponentially increasing intervals
for three more rounds. By the fourth round, 4.8 seconds into
the experiment, the backup is ready, so the retransmission
succeeds and the ﬂow of data resumes.
The actual time when the backup recovered is indicated
by an ACK packet visible around 3.6 seconds. Unfortu-
nately, that ACK does not succeed in reviving the ﬂow of
data because it acknowledges an older packet that client
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:01:39 UTC from IEEE Xplore.  Restrictions apply. 
Setup
Clean TCP
Cold PRS
Hot PRS
Samba request lat. (µsec)
min.
2112
5394
5801
max.
2974
6989
7327
avg.
2181
5823
6183
TCP packet lat. (µsec)
Buffering lat. (µsec)
σ min.
237
827
745
154
299
260
avg.
748
2162
2337
max.
3475
7759
8417
σ min.
avg.
max.
σ
593
1186
1310
46
37
522
551
1617
3689
244
416
Table 1. Breakdown of latencies for short Samba requests
6
-
0
1
x
s
r
e
b
m
u
n
e
c
n
e
u
q
e
s
.
l
e
R
1.25
1.20
1.15
1.10
1.05
1.00
0.95
Client sent
Server acked
1.00
1.05
1.10
1.15
1.20
Time (sec)
6
-
0
1
x
s
r
e
b
m
u
n
e
c
n
e
u
q
e
s
.
l
e
R
0.72
0.70
0.68
0.66
0.64
0.62
0.60
0.58
0.56
Client sent
Server acked
1.00
1.50
2.00
Time (sec)
2.50
3.00
3.50
Figure 5. Behavior of FT-TCP for a short (100 ms)
promotion latency with permanent snooping
Figure 6. Behavior of FT-TCP for a long (2.5 sec)
promotion latency with permanent or reactive snoop-
ing
TCP already considers acknowledged. The length of this
retransmission gap between the actual time of recovery and
the time when the ﬂow of data revives depends on exactly
where in the retransmission cycle recovery happens to take
place: it can be very short if the next retransmission follows
soon after recovery, but it can also be very long (up to 64
seconds of maximum TCP retransmission period) if the ser-
vice recovers right after a retransmission. It is impossible
to avoid this gap if packets arriving immediately after the
crash are lost. In fact, a hot backup that can detect a failure
and recover well under the 200 ms will inevitably have to
wait that long for the ﬁrst retransmission to restart the ﬂow
of data. This effectively places a 200 ms lower limit—for
both hot and cold replication—on the guaranteed failover
time.
The only way to eliminate the retransmission gap is to
ensure that the backup receives all of the packets sent by
the client. That can be done by switching backup’s network
card into promiscuous mode at the beginning of the con-
nection and snooping packets off the network shared by the
client and the replicas. When the backup decides that the
primary failed it can process the snooped packets, acknowl-
edge them and thereby restart the ﬂow of data immediately,
as shown in Figure 5. With this method the failover time
is limited only by the failure detection delay. From Table 1
we can see that the average RTT for messages between the
replicas is about 0.5 ms (although it can be much less for
shorter messages). So a reasonable value for a failure detec-
tion timeout might be 1-2 ms. Unfortunately, FT-TCP im-
plementation relies on Linux kernel timers that have gran-
ularity of 10 ms, making that the minimal failure-detection
latency and consequently the minimal failover time for our
hot backup.
Although snooping helps ensure fastest possible failover
time, looking at every packet on a busy network may place
too heavy of a load on the backup machine. Therefore it
is worthwhile to consider a third approach, in which the
network card operates normally during failure-free opera-
tion, but goes into promiscuous mode whenever a failure
is detected (in fact, there is no harm in starting to snoop
whenever a failure is only suspected and not necessarily
conﬁrmed). We call this reactive snooping; the ﬁrst two
schemes are no snooping and permanent snooping, respec-
tively. Reactive snooping makes sense when the failure de-
tection latency is shorter than the TCP retransmission de-
lay (200 ms), but the promotion latency is longer. Starting
to snoop before the ﬁrst retransmission allows the backup
to collect all packets lost in the crash and restart the data
ﬂow as soon as the promotion is complete, as, for exam-
ple, happens around 3.4 seconds in Figure 6. There is no
point in reactive snooping with a backup that is promoted
quickly since it will get the ﬁrst retransmissions itself. With
short promotion latency the question is whether to snoop
permanently or not at all, and that is a trade-off between
good failure-free performance (which would be affected by
snooping) and short failover time.
The idea of using snooping to improve reliability at a
low cost has been around for a long time [9]. In [5] it was
used for primary-backup replication of a network ﬁle sys-
tem service. A fault-tolerant TCP system described in [8]
also relies on permanent snooping to obtain client packets
on the backup.
6. Conclusion
Our earlier work on FT-TCP [2] demonstrated the feasi-
bility of a server-side recovery approach to masking the fail-
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:01:39 UTC from IEEE Xplore.  Restrictions apply. 
ure of a TCP-based server from its clients. In this paper, we
addressed the question of how it performed in practice. To
do so, we applied FT-TCP to two existing services—Samba
and DSS—and determined its impact for both failure-free
execution and for executions with failures. We also exam-
ined how to reduce the failover time when recovering a TCP
connection. We chose as an embodiment a conﬁguration in
which the network link between the primary server and its
backups was fast, since this is by far the most common con-
ﬁguration used in practice. We found that:
• While it was necessary to modify the code of two
existing services to have them be recoverable using
FT-TCP, the modiﬁcations were few. For both ser-
vices, the nondeterminism was explicitly introduced
by the service: for Samba, nonces and ﬁle handles
are generated, and for DSS, session IDs are gener-
ated. This experience implies that adding a protocol-
speciﬁc “hook” might be useful for making it easier to
ensure that the backup makes the same nondeterminis-
tic choices that the primary does.
• The failure-free overhead of FT-TCP is very low and
the system does not impose any new scalability prob-
lems. The maximum throughput overhead that we
found was for large ﬁle transfers to a Samba server.
Such requests put a heavy load on the buffering mecha-
nism by sending it a large number of syscalls and pack-
ets. Even so, the overhead was under 2%.
• The performance of a hot backup with FT-TCP is
nearly indistinguishable from the performance of a
cold backup. For cold backups, we did not checkpoint
the service (meaning that it would be recovered from
its initial state); checkpointing could have a large im-
pact on server performance.
• The failover time of FT-TCP can be made very short,
but to do so requires the backup to capture the data sent
by the client immediately before the server failed. This
requires the backup to snoop on the incoming trafﬁc by
setting its network interface to promiscuous mode. For
servers that have a large promotion latency, the backup
need only start snooping when it suspects that the pri-
mary has failed, while if the promotion latency is un-
der 200 ms then the backup should start snooping as
soon as it starts executing. The use of snooping, how-
ever, only enhances performance, and is not required
for server-side recovery.
We have only looked at two services, and they are similar
in that they do not impose a large computational overhead
on the server processor. We are interested in the case where
the server does have a large computational overhead, but
such services are less common in practice.
References
[1] N. Aghdaie and Y. Tamir. Implementation and evaluation of
transparent fault-tolerant web service with kernel-level sup-
port. In Proc. IEEE Intl. Conf. on Computer Communica-
tions and Networks, 2002.
[2] L. Alvisi, T. Bressoud, A. El-Khashab, K. Marzullo, and
D. Zagorodnov. Wrapping server-side TCP to mask connec-
tion failures. In Proc. IEEE INFOCOM 2001, pages 329–
337, 2001.
[3] T. Bressoud and F. Schneider. Hypervisor-based fault tol-
erance. ACM Trans. on Computer Systems, 14(1):80–107,
1996.
[4] N. Budhiraja, K. Marzullo, F. Schneider, and S. Toueg.
Primary–backup protocols: Lower bounds and optimal im-
plementations. In Proc. 3rd IFIP Conf. on Dependable Com-
puting for Critical Applications, 1992.
[5] D. Dolev, D. Malki, and Y. Yarom. Warm backup using
snooping.
In Proc. 1st Intl. Workshop on Services in Dis-
tributed and Networked Environments (SDNE), pages 60–
65, 1994.
[6] E. Elnozahy, L. Alvisi, Y. Wang, and D. Johnson. A survey
of rollback-recovery protocols in message passing systems.
ACM Computing Surveys, 34(3):375–408, 2002.
[7] R. Nasika and P. Dasgupta. Transparent migration of dis-
tributed communicating processes.
In Proc. 13th ISCA
Intl. Conf. on Parallel and Distributed Computing Systems
(PDCS), 2000.
[8] M. Orgiyan and C. Fetzer. Tapping TCP streams. In Proc.
IEEE Intl. Symp. on Network Computing and Applications
(NCA2001), 2002.
[9] M. Powell and D. Presotto. Publishing: a reliable broadcast
In Proc. Symp. on Operating
communication mechanism.
Systems Principles, pages 100–109, 1983.
[10] G. Shenoy, S. Satapati, and R. Bettati. HydraNet-FT: Net-
In Proc. 20th Intl.
work support for dependable services.
Conf. on Distributed Computing Systems, 2000.
[11] A. Snoeren, D. Andersen, and H. Balakrishnan.
Fine-
grained failover using connection migration.
In Proc.
3rd USENIX Symp. on Internet Technologies and Systems
(USITS), pages 97–108, 2001.
[12] F. Sultan, K. Srinivasan, and L. Iftode. Transport layer sup-
port for highly-available network services. Technical Report
DCS-TR-429, Rutgers University, 2001.
[13] V. Zandy and B. Miller. Reliable network connections. In
Proc. ACM MobiCom, 2002.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:01:39 UTC from IEEE Xplore.  Restrictions apply.