that is in turn based on the SEAL library [58]. We use the code for
fully connected layers as it is from [2]. For convolution layers, we
parallelize the code, employ modulus-switching [58] to reduce the
ciphertext modulus (and hence ciphertext size), and implement the
strided convolutions proposed in Gazelle [43]. These optimizations
resulted in significant performance improvement of convolution
layers. E.g. for the first convolution layer10 of ResNet50, the runtime
decreased from 306s to 18s in the LAN setting and communication
decreased from 204 MiB to 76 MiB.
6.2 CrypTFlow integration
We integrate SCIOT and SCIHE as new cryptographic backends
into the CrypTFlow framework [1, 45]. CrypTFlow‚Äôs TensorFlow
frontend Athos outputs fixed-point DNNs that use 64-bit integers
and sets an optimal scale using a validation set. CrypTFlow required
a bitwidth of 64 to ensure that the probability of local truncation
errors in its protocols is small (Section 5.1.1). Since our protocols
are correct and have no such errors, we extend Athos to set both the
bitwidth and the scale optimally by autotuning on the validation set.
The bitwidth and scale leak information about the weights and this
leakage is similar to the prior works on secure inference [43, 45, 48‚Äì
51, 61].
Implementing faithful truncations using Œ†int,‚Ñì,ùë†
Trunc requires the par-
ties to communicate. We implement the following peephole op-
timizations in Athos to reduce the cost of these truncation calls.
Consider a DNN having a convolution layer followed by a ReLU
layer. While truncation can be done immediately after the convolu-
tion, moving the truncation call to after the ReLU layer can reduce
the cost of our protocol Œ†int,‚Ñì,ùë†
Trunc. Since the values after ReLU are
guaranteed to be all positive, the call to F int,‚Ñì
DReLU within it (step 2
in Algorithm 5) now becomes redundant and can be omitted. Our
optimization further accounts for operations that may occur be-
tween the convolutions and ReLU, say a matrix addition. Moving
the truncation call from immediately after convolution to after
ReLU means the activations flowing into the addition operation are
now scaled by 2ùë†, instead of the usual ùë†. For the addition operation
to then work correctly, we scale the other argument of addition
by ùë† as well. These optimizations are fully automatic and need no
manual intervention.
7 EXPERIMENTS
We empirically validate the following claims:
‚Ä¢ In Section 7.1, we show that our protocols for computing
ReLU activations are more efficient than state-of-the-art gar-
bled circuits-based implementations (Table 4). Additionally,
our division protocols outperforms garbled circuits when
computing average pool layers.
‚Ä¢ On the DNNs considered by prior work on secure inference,
our protocols can evaluate the non-linear layers much more
efficiently and decrease the total time (Table 5) as well as the
online time (Table 6).
10Layer parameters: image size 230 √ó 230, filter size 7 √ó 7, input channels 3, output
channels 64, and stride size 2 √ó 2
11
Figure 1: The left y-axis shows ( GC Time
Our Time). The right y-axis
shows the total number of ReLU layers corresponding to
each layer size in our benchmark set. The legend entries de-
note the input domain and the network setting.
‚Ä¢ We show the first empirical evaluation of 2-party secure
inference on ImageNet-scale benchmarks (Section 7.3). These
results show the trade-offs between OT and HE-based secure
DNN inference (Table 7).
We start with a description of our experimental setup and bench-
marks, followed by the results.
Experimental Setup. We ran our benchmarks in two network
settings, namely, a LAN setting with both machines situated in
West Europe, and a transatlantic WAN setting with one of the
machines in East US. The bandwidth between the machines is 377
MBps and 40 MBps in the LAN and the WAN setting respectively
and the echo latency is 0.3ms and 80ms respectively. Each machine
has commodity class hardware: 3.7 GHz Intel Xeon processor with
4 cores and 16 GBs of RAM.
Our Benchmarks. We evaluate on the ImageNet-scale bench-
marks considered by [45]: SqueezeNet [40], ResNet50 [37], and
DenseNet121 [38]. To match the reported accuracies, we need 37-
bit fixed-point numbers for ResNet50, whereas 32 bits suffice for
DenseNet121 and SqueezeNet (Appendix I). Recall that our division
protocols lead to correct secure executions and there is no accuracy
loss in going from cleartext inference to secure inference. Appen-
dix G provides a brief summary of these benchmarks.
7.1 Comparison with Garbled Circuits
We compare with EMP-toolkit [62], the state-of-the-art library
for Garbled Circuits (GC). Figure 1 shows the improvement of our
ReLU protocols over GC in both LAN and WAN settings. On the x-
axis, which is in log-scale, the number of ReLUs range from 20 to 220.
The histogram shows, using the right y-axis, the cumulative number
of layers in our benchmarks (SqueezeNet, ResNet50, DenseNet121)
which require the number of ReLU activations given on the x-axis.
We observe that these DNNs have layers that compute between 213
and 220 ReLUs. For such layers, we observe (on the left y-axis) that
0510152025303540455055600246810121416182022242602468101214161820# Layers in Our BenchmarksImprovement over GC# ReLUs (in powers of 2)need more#LayersZ2`-LANZn-LANZ2`-WANZn-WANBenchmark
SqueezeNet
ResNet50
DenseNet121
Benchmark
SqueezeNet
ResNet50
DenseNet121
Garbled Circuits
Our Protocols
LAN WAN Comm LAN WAN Comm
1.15
26.4
5.23
136.5
199.6
8.21
33.3
69.4
118.7
3.5
16.4
24.8
265.6
1285.2
1849.3
7.63
39.19
56.57
(a) over Z2‚Ñì
Garbled Circuits
Our Protocols
LAN WAN Comm LAN WAN Comm
1.77
51.7
8.55
267.5
383.5
12.64
525.8
2589.7
3686.2
16.06
84.02
118.98
50.4
124.0
256.0
5.6
28.0
41.9
(b) over Zùëõ
Table 4: Performance comparison with Garbled Circuits for
ReLU layers. Runtimes are in seconds and comm. in GiB.
our protocols are 2√ó‚Äì25√ó faster than GC ‚Äì the larger the layers
the higher the speedups, and gains are larger in the WAN settings.
Specifically, for WAN and > 217 ReLUs, the speedups are much
higher than the LAN setting. Here, the cost of rounds is amortized
over large layers and the communication cost is a large fraction
of the total runtime. Note that our implementations perform load-
balancing to leverage full-duplex TCP.
Next, we compare the time taken by GC and our protocols in
computing the ReLU activations of our benchmarks in Table 4. Our
protocol over Zùêø is up to 8√ó and 18√ó faster than GC in the LAN and
WAN settings respectively, while it is ‚âà 7√ó more communication
efficient. As expected, our protocol over Zùëõ has even better gains
over GC. Specifically, it is up to 9√ó and 21√ó faster in the LAN and
WAN settings respectively, and has ‚âà 9√ó less communication.
We also performed a similar comparison of our protocols with
GC for the Avgpool layers of our benchmarks, and saw up to 51√ó
reduction in runtime and 41√ó reduction in communication. We
report the concrete performance numbers and discuss the results
in more detail in Appendix H.
7.2 Comparison with Delphi
In this section, we compare with Delphi [49], which is the cur-
rent state-of-the-art for 2-party secure DNN inference that outper-
forms [12, 13, 17, 19, 22, 31, 43, 48, 56] in total time as well as the
time taken in online phase. It uses garbled circuits for non-linear
layers, and we show that with our protocols, the time taken to
evaluate the non-linear layers can be decreased significantly.
For a fair evaluation, we demonstrate these improvements on the
benchmarks of Delphi [49], i.e., the MiniONN (CIFAR-10) [48] and
ResNet32 (CIFAR-100) DNNs with ReLU activations (as opposed
to the ImageNet-scale benchmarks for which Delphi has not been
optimized). Similar to Delphi, we perform these computations with
a bitwidth of 41 in the LAN setting.
In Table 5, we report the performance of Delphi for evaluating
the linear and non-linear components of MiniONN and ResNet32
separately, along with the performance of our protocols for the
same non-linear computation11. The table shows that the time to
evaluate non-linear layers is the bulk of the total time and our
11Our non-linear time includes the cost of correct truncation.
12
Non-linear
Improvement
Delphi Ours
1.0
30.2
0.28
3.15
52.9
2.4
0.59
5.51
30.2√ó
12.3√ó
22.0√ó
9.3√ó
Benchmark Metric
Linear
MiniONN
ResNet32
Time
Comm.
Time
Comm.
10.7
0.02
15.9
0.07
Table 5: Performance comparison with Delphi [49] for non-
linear layers. Runtimes are in seconds and comm. in GiB.
Benchmark
Linear
MiniONN
ResNet32
< 0.1
< 0.1
Non-linear
Delphi Ours
0.32
3.97
6.99
0.63
Improvement
12.40√ó
11.09√ó
Table 6: Performance comparison with Delphi [49] for on-
line runtime in seconds.
protocols are 20√ó‚Äì30√ó faster in evaluating the non-linear layers.
Also note that we reduce the communication by 12√ó on MiniONN,
and require 9√ó less communication on ResNet32.
Next, we compare the online time of our protocols with the
online time of Delphi in Table 6. In the online phase, linear layers
take negligible time and all the time is spent in evaluating the non-
linear layers. Here, our protocols are an order of magnitude more
efficient than Delphi.
7.3 Evaluation on practical DNNs
With all our protocols and implementation optimizations in
place, we demonstrate the scalability of CrypTFlow2 by efficiently
running ImageNet-scale secure inference. Table 7 shows that both
our backends, SCIOT and SCIHE, are efficient enough to evaluate
SqueezeNet in under a minute and scale to ResNet50 and DenseNet121.
In the LAN setting, for both SqueezeNet and DenseNet121, SCIOT
performs better than SCIHE by at least 20% owing to the higher
compute in the latter. However, the quadratic growth of communi-
cation with bitlength in the linear-layers of SCIOT can easily drown
this difference if we go to higher bitlengths. Because ResNet50,
requires 37-bits (compared to 32 in SqueezeNet and DenseNet121)
to preserve accuracy, SCIHE outperforms SCIOT in both LAN and
WAN settings. In general for WAN settings where communication
becomes the major performance bottleneck, SCIHE performs bet-
ter than SCIOT: 2√ó for SqueezeNet and DenseNet121 and 4√ó for
ResNet50. Overall, with CrypTFlow2, we could evaluate all the 3
benchmarks within 10 minutes on LAN and 20 minutes on WAN.
Since CrypTFlow2 supports both SCIOT and SCIHE, one can choose
a specific backend depending on the network statistics [17, 53] to
get the best secure inference latency. To the best of our knowledge,
no prior system provides this support for OT and HE-based secure
DNN inference.
8 CONCLUSION AND FUTURE WORK
We have presented secure, efficient, and correct implementations
of practical 2-party DNN inference that outperform prior work [49]
by an order of magnitude in both latency and scale. We evaluate the
first secure implementations of ImageNet scale inference, a task that
previously required 3PC protocols [7, 45] (which provide weaker
security guarantees) or leaking intermediate computations [12]. In
Benchmark
SqueezeNet
ResNet50
DenseNet121
Protocol
SCIOT
SCIHE
SCIOT
SCIHE