(“faultdetection”OR“failuredetection”)
(“ML”OR“MachineLearning”)
(“faultlocalization”OR“failurelocalization”)
(“inference”OR“logic”OR“reasoning)
(“faultprediction”OR“failureprediction”)
(“supervised”OR“unsupervised”OR
(“faultprevention”OR“failureprevention”)
“semi-supervised”OR“reinforcement”)AND(“learning”)
(“log”OR“logs”OR“loganalysis”)
(“supportvectormachine”OR“SVM”)
(“metrics”OR“KPI”OR“keyperformanceindicator”)
(“tree”OR“tree-based”OR“trees”OR“forest”)
(“remediation”OR“recovery”)
((“bayesian”OR“neural”)AND“network”)
(“root-causeanalysis”OR“rootcauseanalysis”)
(((“hidden”AND“markov”)OR(“gaussian”
(“servicedeskautomation”)
AND“mixture”))AND“model”)
(“tracing”OR“trace”OR“traces”)
((“datacenter”OR“datacenter”)AND“management”)
– ThedocumentreferencesoneormoreAImethods.Thesementionscaneither
be part of the implementation or as part of its discussion/analysis (e.g. in a
survey). Any mention to AI algorithms employed by others (i.e. mentioned
in the related work section or as baseline comparison) that is not strictly the
focus of the document, is not considered valid;
– The document applies its concepts to some kind of IT system management.
We therefore exclude papers with no specific target domain or with a target
domain outside of IT Operations.
In terms of exclusion criteria, we define the following as exclusion rules:
– The language of the document is not English;
– The document is not accessible online;
– The document does not belong to the following categories: scientific article
(conference paper, journal article), book, white paper;
– The main topic of the document is one of the following: cybersecurity, indus-
trial process control, cyber-physical systems, and optical sensor networks.
For the special case of survey and review papers, we consider them relevant as
long while carrying out our mapping study, but we then exclude them from our
finalresultset,asthesearticlesareusefultofindotherconnectedworksthrough
references, but they do not constitute novel contributions to the field.
Database Search. For the search process, database search represents the first
andmostimportantstep,asitaimstoprovidethehighestnumberofresultsand
perform an initial screening of irrelevant papers. We perform database search in
three steps: keywording, query construction and result polling. For keywording
we use the PICO technique presented in [34] to derive a set of keywords for AI
and a set of keywords for IT Operations. The keywords are listed in Table1.
Then, following our scoping considerations, we construct queries so that they
return results where both AI and IT Operations are present. In particular, we
apply logic conjunction of keywords across all combinations of the two keyword
sets (e.g. “logistic regression” and “cloud computing”). This helps enforcing
114 P. Notaro et al.
precision in our search results. For keywords with synonyms and abbreviations,
we allow all equivalent expressions via OR disjunction. We also perform general
search queries, related to the topic as a whole (e.g. “AIOps”). Finally, we group
some queries with common terms to reduce the number of queries.
We select three online search databases that are appropriate for the scope of
investigation: IEEE Xplore, ACM Digital Library and arXiv. For each query we
restrict our analysis to the top 2000 results returned. We aggregate results from
all searches in one large set of papers, removing duplicates and annotating for
each item corresponding search metadata (e.g. number of hits, index position in
corresponding searches, etc.). The result from this step consists of 83817 unique
articles. For each item we collect the title, authors, year, publication venue,
contribution type and citation count (from Google Scholar).
Preliminary Filtering and Ranking-Based Selection. Inthefilteringstep
westartimproving thequality ofourselection ofpapers.First,papersareauto-
matically excluded based on publication venue, for those venues that are clearly
irrelevantfortopicreasons(e.g.meteorology).Wealsoexcludebasedontheyear
of publication (year <1990) as it precedes the advent of large-scale IT services.
By doing so, we can exclude approximately 8000 elements.
Usuallyatthispoint,afull-textanalysiswouldbeperformedonalltheavail-
ablepaperstoscreenrelevantcontributionsusingtheabovecitedselectionrules.
Although we partly filtered results, it is still not feasible to perform an exhaus-
tive selection analysis, even as simple as filtering by title. It is also impractical
to attempt an automated selection by content, as it is not clear how to perform
an efficient, high-recall, high-precision text classification without supervision.
Therefore, before proceeding with the rest of the search and selection steps, we
apply a ranking procedure on these intermediate results, so that we can priori-
tize investigation of more relevant papers. We apply the exclusion and inclusion
rules of Sect. 2.4 to the papers examined in ranking order.
This approximate procedure however raises the question of when it is con-
venient to stop our selection and discard the remaining items. To solve this, we
develop a new approach from our observations of ranked items. We base the
method on the following assumption: a considerable ratio of relevant papers can
be identified by ranking and selecting top results using different relevance cri-
teria (conference, position index in the query result set, number of hits in all
queries, etc.), but in this sorting scenario we also observe a long-tail distribu-
tionofrelevantdocuments,i.e.somerelevantpapersappearinthelastpositions
evenaftersortingwithourrelevanceheuristics(seeFig.1).Thisiscoherentwith
the known impossibility of performing exhaustive systematic literature reviews
and mapping studies, as completing the long tail provides less results at the
expenseofalargerresearcheffort.Weassumetheratioofrelevantpapersinthe
long tail to beconstant and comparable in magnitude to the number of relevant
papers when sampled randomly from the result set. Based on this assumption,
we proceed as follows:
A Systematic Mapping Study in AIOps 115
(a) (b)
Fig.1. Estimated relevance probability for collected papers (y-axis), as a function of
the index in the result set (x-axis, in thousands), with paper arranged: (a) in random
order(b)usingarelevanceheuristicbasedonsearchhits.Wecanobservehow,thanks
to the heuristic (b), the majority of relevant papers can be identified by examining
only a small fraction of the set (the top results on the left side).
– We start screening all papers in the result set, ranked according to different
relevance heuristics (e.g. number of hits in queries), and we observe the ratio
of relevant papers identified over time;
– We examine the same papers in random order, and measure the same ratio;
– When the two ratios are comparable, we assert we reached the tail of the
distribution of relevant papers and stop examining and selecting new papers.
As sorting criteria, we use the number of hits in the search performed in the
previous step, as well as other more complex heuristics, taking into account the
index position in result sets and the number of citations. When examining a
paper, we look into the full content to identify concepts related to our selection
criteria previously illustrated. As done previously with search results, we gather
relevant papers in one unique group. Using this stopping criterion, we conclude
this selection step when we have identified 430 relevant papers.
2.5 Additional Search Techniques
The “early stopping” criterion previously described, while allowing a feasible
and comprehensive selection strategy across thousands of contributions, has a
natural tendency towards discarding relevant papers. We also expect to miss
other relevant papers, not present in the initial set of 83817, because they were
not identified by our database search. To cover for these limitations, we apply
othersearchtechniquesinadditiontodatabasesearch.Differingfrombefore,we
here apply our selection criteria exhaustively for each document retrieved.
Reference Search. For each of the 430 relevant papers identified in the previ-
ousstep,wesearchinsidetheircitedreferences.Inparticular,weadoptbackward
116 P. Notaro et al.
Table 2. Sample k-shingles with relevance probability (and total occurrences).
k=1 k=2 k=3
tpc-w,1.00(13) defectprediction,1.00(34) softwaredefectprediction,1.00(22)
log-based,0.92(11) (work)loadprediction,1.00(32) diskfailureprediction,1.00(8)
sla,0.84(48) softwareaging,1.00(13) failurepredictionmodel,1.00(7)
stragglers,0.83(10) resourceallocation,1.00(6) cloudresourceprovisioning,1.00(5)
vm,0.83(59) hardwarefailures,0.89(8) automaticanomalydetection,0.88(7)
snowballsampling[18]:weincludeinourrelevantsetallpaperspreviouslycited
by a relevant paper whenever they fulfill the selection requirements mentioned
above. By doing so, we obtain 631 relevant elements, for a total of 1061.
Conference Search. Reference search allows to identify prominent contribu-
tions frequently mentioned by other authors. A drawback is the introduction of
bias towards specific research groups and authors. We also observed how ref-
erence search rewards specific tasks and research fields as they are typically
more cited. We therefore apply other search techniques to compensate for these
facts. We perform a manual search by inspecting papers published in relevant
conferences. These relevant conferences are identified via correlation with other
relevantpapersandhavealsobeenconfirmedbyexpertsinthefield.Welookat
the latest 3 editions of each conference, in an effort to compensate the sampling
of dated papers performed by reference search. We obtain 5 more papers with
this method.
Iterative Search Improvement. To conclude our search, we attempt at
improving our initial guess on IT Operations keywords via analysis of the avail-
able text content (text and abstract). Using our relevant paper set as positive
samples,weperformastatisticalanalysistoidentifyk-shingles(setsofk consec-
utive tokens) that appear often in relevant documents (Table2). In particular,
wemeasurethedocumentrelevanceprobabilitygiventhesetofshinglesobserved
in the available text content. We choose k = 1,...,5. We use these shingles as
keywords to construct new queries along with previously used AI keywords. We
here limit the collection to 20 results per query. Thanks to this step, we identify
20newrelevantpapers.Asaby-product,wegetincontactwithfrequentlycited
concepts and keywords in AIOps, later useful for taxonomy and classification.
2.6 Data Extraction and Categorization
After obtaining the result set of relevant papers (counting 1086 contributions),
we analyze the available information to draw quantitative results and answer
our research questions. We describe here the data extraction process and the
analysis techniques employed to gather insights and trends for the AIOps field.
First, we classify the relevant papers according to target components and
data sources. Target components indicate a high-level piece of software or hard-
wareinanITsystemthatthedocumenttriestoenhance(e.g.harddriveforhard
A Systematic Mapping Study in AIOps 117
Table 3. Selection of result papers grouped by data sources, targets and
(sub)categories.
DataSources Targets DataSources Targets
Ref. Cat. Ref. Cat.
secruoseRgnitseT stropeRtnedicnI secarTnoitucexE secruoseRgnitseT stropeRtnedicnI secarTnoitucexE
scirteMmetsyS atadOLS/sIPK cffiarTkrowteN scirteMmetsyS atadOLS/sIPK cffiarTkrowteN
edoCecruoS edoCecruoS edoCecruoS edoCecruoS
sgoLtnevE noitacilppA retnecataD sgoLtnevE noitacilppA retnecataD
ygolopoT erawdraH krowteN ygolopoT erawdraH krowteN
[27]   1.1 [15]   3.1
[32]    1.2 [10]     3.1
[16]    1.3 [6]    3.1
[41]     1.3 [28]   3.1
[29]   1.4 [30]    3.2
[47]   2.1 [49]    3.3
[14]    2.1 [1]     4.1
[12]    2.1 [33]     4.1
[46]    2.1 [5]    4.1
[8]    2.2 [44]    4.2
[11]    2.2 [4]     4.2
[17]     2.2 [19]    4.2
[35]     2.2 [9]    4.2
[24]    2.2 [36]    4.3
[37]      2.2 [7]     4.3
[45]   2.2 [26]    4.3
[43]    3.1 [2]    4.3
[42]   3.1 [39]    5.1
[40]     3.1 [48]    5.2
[21]    3.1 [25]     5.2
[22]     3.1 [38]     5.3
(Sub)CategoryLegend
1.1 SoftwareDefectPrediction 2.2 SystemFailurePrediction 4.2 RootCauseDiagnosis
1.2 FaultInjection 3.1 AnomalyDetection 4.3 RCA-Others
1.3 SoftwareRejuvenation 3.2 InternetTrafficClassification 5.1 IncidentTriage
1.4 Checkpointing 3.3 LogEnhancement 5.2 SolutionRecommendation
2.1 HardwareFailurePrediction 4.1 FaultLocalization 5.3 Recovery
diskfailureprediction).Wegroupcomponentsinfivehigh-levelcategories:code,
application, hardware, network and datacenter. Data sources provide an indica-
tionoftheinputinformationofthealgorithm(suchaslogs,metrics,orexecution
traces). Data sources are categorized in source code, testing resources, system
metrics, key performance indicators (KPIs), network traffic, topology, incident
reports, logs and traces. the “AI Method” axis denotes the actual algorithm
employed, with similar methods aggregated in bigger classes to avoid excessive
fragmentation (e.g. ‘clustering’ may contain both k-means and agglomerative
hierarchical clustering approaches). Table3 presents a selection of papers from
the result set with the corresponding target, source and category annotation.
Then, we use the result set to infer a taxonomy based on tasks and target
goals. The taxonomy is depicted in Fig.2. We divide in AIOps contributions in
failure management (FM), the study on how to deal with undesired behavior
118 P. Notaro et al.
Fig.2. Taxonomy of AIOps as observed in the identified contributions
Fig.3.Left:distributionofAIOpspapersinmacro-areasandcategories.Right:percent
distributionoffailuremanagementpapersbycategoryincorrespondingsub-categories.
in the delivery of IT services; and resource provisioning, the study of alloca-
tion of energetic, computational, storage and time resources for the optimal
delivery of IT services. Within each of these macro-areas, we further distinguish
approachesincategoriesbasedonthesimilarityofgoals.Infailuremanagement,
thesecategoriesarefailureprevention,onlinefailureprediction,failuredetection,
root cause analysis (RCA) and remediation. In resource provisioning, we divide
contributions in resource consolidation, scheduling, power management, service
composition,andworkloadestimation.Wefurtherchoosetoexpandouranalysis
ofFM(redboxofFig.2)byapplyingforthismacro-areaanadditionalsubcate-
gorizationbasedonspecificproblems.Examplesofsubcategoriesarecheckpoint-
ing for failure prevention, or fault localization for root cause analysis (see also
Table3).
A Systematic Mapping Study in AIOps 119
Fig.4.PublishedpapersinAIOpsbyyearandcategoriesfromthedescribedtaxonomy.
3 Results
We now discuss the results of our mapping study. We first analyze the distri-
bution of papers in our taxonomy. The left side of Fig.3 visualizes the distribu-
tion of identified papers by macro-area and category. Excluding papers treating
AIOps in general (8), we observe that more the majority of items (670, 62.1%)
are associated with failure management (FM), with most contributions concen-
trated in online failure prediction (26.4%), failure detection (33.7%), and root
cause analysis (26.7%); the remaining resource provisioning papers support in
large part resource consolidation, scheduling and workload prediction. On the
right side, we can observe that the most common problems in FM are software
defectprediction,systemfailureprediction,anomalydetection,faultlocalization
and root cause diagnosis. To analyze temporal trends present inside the AIOps
field, we measured the number of publications in each category by year of pub-
lication. The corresponding bar plot is depicted in Fig.4. Overall, we observe a
large, on-growing number of publications in AIOps. We can observe how failure
detection has gained particular traction in recent years (71 publications for the
2018–2019period)withacontributionsizelargerthantheentireresourceprovi-
sioning macro-area (69 publications in the same time frame). Failure detection
is followed by root cause analysis (39) and online failure prediction (34), while
failure prevention and remediation are the areas with the smallest number of
attested contributions (11 and 5, respectively).
4 Conclusion
In this paper, we presented our contribution towards better structuring the