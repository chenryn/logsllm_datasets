An important aspect of the auction is the scope of the
auction, by which we mean the set of ads that compete in
any given auction. As a general rule, the more ads that
compete, the higher the CPC. This is simply because the
more ads there are, the more probabilistically likely the next-
ranked ad will have a (B ×G×U ) closer to that of the clicked
ad. On the other hand, the larger the auction scope, the less
fair it is in the sense that very diﬀerent types of ads must
compete. A local pizza store may not wish to compete with
Mercedes for ad boxes.
The auction scope for search or contextual systems like
Bing and Google is the set of ads whose keywords match that
of the search or web page. Today’s ad exchanges, where ad-
vertisers bid in real time, typically for adboxes on premium
publishers, have a potentially much broader scope because
any advertiser can bid. The auction scope in a non-tracking
advertising system is tunable. It may be all ads in a client, or
all ads within an ad type (i.e. an interest). What’s more, in-
terests may be hierarchical (sports/tennis/clothing/shoes),
and may be more general or more speciﬁc, thus allowing for
substantial ﬂexibility in auction scope.
5.3 Attacks
In this section, we relax our assumption of honest-but-
curious players, and consider a number of malicious attacks
and defenses.
5.3.1 Client click fraud
The client can commit a form of click fraud by lying about
the value of ((Bn × Gn) × (Un/Uc)) (RaC) or (Un/Uc) (RaB
or Ra3). By inﬂating or deﬂating these values, the client
can cause advertisers to pay more or less, and cause pub-
lishers to earn more or less. At a high level, this is very
similar to normal forms of click fraud that occur today, and
in this sense our auctions do not allow fundamentally new
forms of click fraud. Privad describes how to defend against
click-fraud even with anonymizing brokers [12]. The same
method may be used here. The basic idea is that the proxy
tags reports with a per-report unique identiﬁer. If the bro-
ker suspects click fraud, it informs the proxy of the report
ID of the suspicious report.
If a given client is suspected
more times than some threshold, its reports can be tagged
by the proxy as coming from a suspected client.
In some
cases the broker may suspect click fraud simply because the
2nd price is impossibly high (i.e. higher than the 1st price
bid). In most cases, however, the broker may suspect click
fraud through statistical analysis of the reports for given
advertisers or publishers.
5.3.2 Proxy ﬁngerprints client user scores and re-
sulting ranking (RaB and Ra3)
It is diﬃcult but conceivable in RaB and Ra3 that the
proxy could determine user proﬁles through observation of
the client user scores and rankings. For instance, the proxy
could establish a number of fake clients that pretend to have
various proﬁle attributes, and establish ﬁngerprints of the re-
sulting user scores and rankings. One way to do this might
672be to determine (B × G) given user scores and correspond-
ing ad ranks, and use these values as the ﬁngerprint. The
proxy could then compare these ﬁngerprints with the cor-
responding ﬁngerprints of real clients. It could be that the
signal-to-noise ratio is high enough to successfully pull oﬀ
this attack. One way to prevent this would be to encrypt
user scores and rankings. The user scores could be encrypted
using the brokers (RaB) or 3rd-party’s (Ra3) public key, and
the rankings could be encrypted using symmetric keys cre-
ated by the clients and conveyed securely to the broker or
3rd-party. These symmetric keys would be frequently mod-
iﬁed to prevent the broker or 3rd-party from linking user
scores with the same client, and possibly launching a simi-
lar ﬁngerprint attack.
5.3.3 Broker manipulates [Bn × Gn] (RaB, Ra3)
A malicious broker could launch an attack on a non-
tracking advertising system to identify clients by inserting
unique IDs into the encrypted ﬁelds [B, G] or [B × G]. Once
a client is identiﬁed in this way, unlinkability is lost, and the
broker can build up client proﬁles. The broker can then po-
tentially identify the client through external means. There
is some cost to this approach, as the broker must “waste” ads
to do the tracking2. The homomorphic encryption variant
described in Section 4.4 defends against this attack. Because
the client multiplies the received encrypted ﬁelds with other
ﬁelds, the values generated by the broker are obscured.
5.4 Discussion
RaB, which was proposed in [12], is a weak system be-
cause it opens up a ﬁngerprinting attack at the broker. Ra3
solves this problem, though at the expense of requiring yet
another administratively distinct and non-colluding system.
Nevertheless, we consider it to be better than RaB.
If exposing the (B × G) product to the client is not a
problem for the broker and advertisers, then RaC is better
than Ra3 because it is simpler, incurs less overhead and
latency, and does not require the 3rd party. In addition, it
has no issues here with respect to user score churn, because
ranking takes place at ad view time. If exposing (B × G)
is a problem, however, then Ra3 appears to be a reasonable
approach.
6. EFFECT OF CHURN
Section 5.2 describes how various delays in all three auc-
tion systems may distort rankings and CPC computation.
How detrimental this delay is depends on how much churn
there is. Churn may aﬀect rank, CPC value, and ultimately
revenue. RaC is aﬀected by churn in B and G, while RaB
and Ra3 are additionally aﬀected by churn in U .
In this
section, we use trace data from Microsoft’s Bing advertising
platform to study in depth the eﬀect of auction delays on B
and G from both the advertiser and broker perspective. We
ﬁnd that, while churn exists, it has only a negligible impact
on broker revenue and advertiser costs.
6.1 What Causes Churn?
B × G for an ad changes when either B or G changes.
B can change in one of three ways: ﬁrst, the advertiser
2One might argue that the same attack can be launched
simply be creating unique Ad IDs transmitted in the clear.
However, this attack can at least be detected by third par-
ties, for instance running honey-farms of clients.
can manually update the bid; second, the ad network can
automatically update the bid (as directed by the advertiser);
third, a 3rd-party may update the bid on the advertiser’s
behalf. Each of these has diﬀerent churn characteristics:
Advertiser: Manual updates, we believe, cause very little
churn since they are reactive over a long feedback cycle.
Advertisers receive updated campaign information (i.e. how
many clicks, actual amount charged, budget left) at fairly
coarse intervals (few times a day). This limits the number
of informed changes to their campaigns.
Ad Network: The advertiser can invoke functionality pro-
vided by the ad network to optimize his bidding strategy.
For example, the ad network may allow the advertiser to
set a preferred rank (e.g. position 4), and the ad network
automatically lowers or raises the bid to satisfy the request
based on the market. Other examples may include auto-
matically modifying bids to meet a target number of im-
pressions per day (while still being charged only for clicks),
or modifying bids based on time of day etc. Some of this
functionality (e.g. modify bids based on time-of-day) can be
implemented in the client and would therefore not result in
any added churn. Other functionality (e.g. preferred rank)
tends to be implemented today as a periodic update (once
every few hours).
3rd Party: Search Engine Optimization (SEO) companies
optimize their client’s bidding strategy in real-time [6] e.g.
based on trending terms, real-time click-through rates, etc.
This could potentially result in high bid churn, however, due
to the premium nature of these SEO services, only a small
number of ads would be aﬀected.
Aside from changes in B, G can also change. Recall G
in our model is a function of what the broker knows: G is
computed based on the ad (past CTR, landing page quality,
etc.). G is largely a property of the ad itself, which we
don’t expect to change quickly or dramatically. In any event,
our Bing auction trace unfortunately does not allow us to
validate our assumption since it does not isolate user-derived
components of G from other components.
6.2 How Does Churn Affect Auctions?
Today auctions take place at the time when an ad is dis-
played to the user; ranking and CPC calculations can im-
mediately reﬂect any changes in B or G. Privacy preserving
auctions described in Section 4 are limited in terms of how
fast new B and G information can be incorporated. Since G
does not rapidly change over time or can be engineered to
remain relatively stable (e.g., using U to reﬂect short-term
changes in click probability), the main source of churn is
the changes in B. To understand the eﬀects of churn in B
values, we simulate auctions that use stale B information
for ranking and CPC computation, and then compare the
resulting ranking and CPC computation with auctions that
use up-to-date B information.
6.3 Dataset
For our trace driven simulations, we sampled around 2TB
of log data from Bing’s auction engine spanning a 48 hour
period starting September 1, 2010. The data covers over
150M auctions for over 18M unique ads shown to North
American Bing search users across all search topics. The
trace record for an auction lists all the ads that participated
in it (whether the ad was ultimately shown or not), the bids
corresponding to each ad, the corresponding quality scores,
673and which if any of the ads were ultimately clicked by the
user.
6.4 Methodology
We re-compute auction rankings and the CPC for each
auction in our dataset using stale bid information; we vary
staleness from 1 minute to 2 days.
Auction rankings are re-computed using bid and quality
data from the trace. Since our trace does not show when
the advertiser updated the bid, we infer the time based on
multiple auctions that a given ad participates in. If the bid
for the ad is the same for two consecutive auctions, we infer
that the bid did not change during that interval. If the bid
is diﬀerent, we infer that the bid changed sometime between
the two auctions; we use the mid-point as the time of change.
To simulate an auction at time T with stale information from
d minutes ago, we simply use the bids current as of time
T − d in our trace. The quality score in the trace is based on
user features (e.g. search query), which correspond to U in
private auctions; since the client always has the current value
of U we use the same quality score for simulated auctions as
in the trace.
CPCs are re-computed based on the re-computed auction
rankings (using the second-price formula of Equation 2). In
other words, for an adbox at time T in the trace, we compute
the ranking based on bid values recorded at time T − d
and populate the adbox using resulting ranking. If the user
clicked on an ad in this adbox, the bid of the next lower
ranked ad Bn that we use in the CPC computation is the
stale Bn taken at time T − d.
One limitation we face is that we cannot predict the
change in user behavior when auction rankings change. Con-
sider, for example, two ads A1 and A2 where in the trace
they are ranked 1 and 2, while in the simulated stale auction
they are ranked 2 and 1 respectively. If the user clicked A2
in the trace, what might we expect the user to click in our
simulation? One option is to model the user as clicking the
same ad he clicked in the trace; thus in this case the user
clicks A2 in the simulation. Another option is to model the
user as clicking the same position he did in the trace; in this
case the user clicks position 2 (A1 in the simulation).
In
reality, the user model is neither of these two extremes —
it is well-known that both ad content and rank eﬀect click-
through rates (CTR) [10]. To account for this, we simulate
5 user models: 1) same position, 2) 75% same position and
25% same ad, 3) 50%-50%, 4) 25% same position and 75%
same ad, and 5) same ad. Thus we establish an envelope of
possible user behavior to get a sense of the upper- and lower-
bounds of our simulation results. Note that always clicking
on the same ad is a strictly conservative estimate. This is
because an ad that was clicked in the trace but is not shown
to the user in our simulation (due to being ranked too low)
would not get clicked; at the same time, under the same-ad
model, an ad that was not shown in the trace (due to being
ranked too low) and was therefore not clicked would have
no chance of getting clicked even if it were to be shown to
the user in the simulation. This asymmetry biases the sim-
ulation towards fewer clicks (and therefore lower revenues).
The only user model immune to this limitation is the same-
position model.
A second limitation we face is that we cannot predict how
advertisers would change their bidding strategy in response
to auctions being based on stale information. Enterprising
)
%
(
e
u
n
e
v
e
r
r
e
k
o
r
B
n
i
e
g
n
a
h
C
 0.15
 0.1
 0.05
 0
-0.05
-0.1
-0.15
1m 2m 5m 15m 30m 1h
2h
3h
6h
12h 1d
36h 2d
Staleness
Figure 3: Change in broker revenue
advertisers or SEOs, may for instance, attempt to predict
what bid they might want to make 1hr hence, and enter it
into the system well in advance. Advance bidding would re-
duce the eﬀective staleness of information. For our purposes,
we assume the bidding strategy does not change.
6.5 Simulation Results
Overall our simulations show that there is no appreciable
change in broker revenue for using stale bid information;
even in the most conservative cases, the revenue is within
±0.1% of today. For advertisers, while stale bids aﬀect their
auction rankings, they do so in a balanced manner with cases
of higher-than-today rank canceling out cases of lower-than-
today rank resulting in zero net change.
Figure 3 plots the change in broker revenue compared to
today as a function of the staleness of information used and
the user model. The x-axis varies the stateless of bids from
1 minute to 2 days. The box-and-whisker plot varies the
user model with the top whisker showing the outcome where
the user clicks the same position, and the bottom whisker
showing when the user clicks the same ad; the top edge of
the box shows 75% same position and 25% same ad, and
vice versa for the bottom edge of the box; the line in the
middle shows the 50%-50% case.
The ﬁrst observation we make from Figure 3 is that under
a 50-50 user model, change in revenue is practically 0% even
with bid information as stale as up to 12 hours. Under the
75-25 and 25-75 models, the change is almost always between
±0.05%, and only in the extreme cases 100-0 or 0-100 does
it pass ±0.1%. More importantly, the change increases very
gradually. This is good news since it means a private adver-
tising system would not have a hard delay deadline beyond
which there would be disproportionate change in revenue.
Instead the system can strive to do the best it can, and re-
duce revenue change proportionally. The extremely gradual
rate of change also means that system design trade-oﬀs can
be biased towards scalability and other engineering goals
without much concern to revenue since it changes very little
in the ﬁrst place.
At ﬁrst blush the eﬀect of the “same-ad” user-model ap-
pears to be to reduce the revenue, but this is deceptive. As