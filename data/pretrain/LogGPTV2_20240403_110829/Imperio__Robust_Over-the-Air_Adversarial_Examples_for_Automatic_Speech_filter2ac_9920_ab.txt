this system is given. The DNN-HMM-based ASR system can be
divided into three parts: the feature extraction, which transforms
the raw input data into representative features, the DNN as the
acoustic model of the system, and the decoding step, which returns
the recognized transcription.
Feature Extraction. For the feature extraction, the raw audio
is divided into frames (e. g., 20 ms long) with a certain overlap
(e. g., 10 ms) between two neighboured frames. For each of these
frames, a discrete Fourier transform (DFT) is performed to retrieve
a frequency representation of the audio input. Next, the magnitude
and the logarithm of the resulting complex signal are calculated. The
result is a common representation of audio features in the frequency
domain. In SchÃ¶nherr et al.â€™s approach, this feature extraction is
integrated into the DNN, allowing them to directly modify the raw
audio data when computing adversarial examples (see Figure 3 for
an illustration).
Acoustic Model DNN. The features described above are used as
the input for the acoustic model DNN. Based on these, the DNN
calculates a matrix of so-called pseudo-posteriors, which describe
the probabilities for each of the phones of the languageâ€”English,
in this caseâ€”being present in each time step ğ‘¡ = 1 . . . ğ‘‡ .
Decoding. Finally, the pseudo-posteriors are used to calculate the
most likely transcription via Viterbi decoding and an HMM-based
language model.
This so-called hybrid approach, which realizes speech recogni-
tion through a search for the most likely path through a matrix of
phone posteriors, is easier to train and still achieves better results
in comparison to end-to-end approaches [22].
2.2 Adversarial Audio Examples
For the calculation of adversarial examples, the ASR system can be
described as the function
ğ‘¦ = arg max
Ëœğ‘¦
ğ‘ƒ(Ëœğ‘¦|ğ‘¥) = ğ‘“ (ğ‘¥),
(1)
mapping an audio signal ğ‘¥ to its corresponding, most likely tran-
scription ğ‘¦. An adversarial example is generated by modifying the
original input
ğ‘¥â€² = ğ‘¥ + ğ›¿,
ğ‘“ (ğ‘¥) â‰  ğ‘“ (ğ‘¥â€²).
such that
(2)
The added distortions ğ›¿ can also be restricted, e. g., via hearing
thresholds. In this work, only targeted attacks are considered, where
the target transcription ğ‘¦â€² != ğ‘“ (ğ‘¥â€²) is defined by the attacker. The
optimization can, therefore, be described as
ğ‘ƒ(ğ‘¦â€²|Ëœğ‘¥).
(3)
ğ‘¥â€² = arg max
Ëœğ‘¥
To calculate robust over-the-air adversarial examples, we base our
work on the approach proposed by SchÃ¶nherr et al. [29] and similar
works. The method can be divided into three steps: forced alignment,
gradient descent, and restriction of the perturbations via hearing
thresholds.
Forced Alignment. Forced alignment is typically used during the
training of the ASR systems when no exact alignments between the
audio and the transcription data are available. In our case, we utilize
this algorithm to find the best possible alignment of the original
audio input and the malicious target transcription.
Gradient Descent. For the attack, the feature extraction is inte-
grated into the DNN, so that the raw audio data can be updated
directly via gradient descent. For this purpose, the cross-entropy
loss is measured between the target outputâ€”the pseudo-posteriorsâ€”
and the actual DNN output, and is used to compute gradients for
the optimization algorithm.
Psychoacoustic Hearing Thresholds. The added noise is restricted
to those time-frequency ranges, where noise perceptibility is mini-
mal. For this, we use psychoacoustic hearing thresholds described
by Zwicker and Fastl [41].
2.3 Room Impulse Response
When an audio signal is transmitted through a room, as visualized
in Figures 1 and 4, the recorded signal can be approximated by
FeatureExtractionDNNDecoderrawaudiofeaturespseudo-posteriorstranscriptionISOLEMNLYSWEARTHATIAMUPTONOGOODACSAC 2020, December 7â€“11, 2020, Austin, USA
Lea SchÃ¶nherr, Thorsten Eisenhofer, Steffen Zeiler, Thorsten Holz, and Dorothea Kolossa
Figure 3: Augmented DNN, which gets the raw audio as input
and integrates the feature extraction into the recognizerâ€™s
DNN. This enables us to update the raw audio signal directly
via gradient descent.
convolving the roomâ€™s impulse response â„ with the original audio
signal ğ‘¥ as
(4)
Here, the convolution operator âˆ— is a shorthand notation for the
multi-path transmission model
ğ‘¥â„ = ğ‘¥ âˆ— â„.
ğ‘›âˆ‘ï¸
ğ‘¥â„(ğ‘›) =
ğ‘¥(ğ‘š) Â· â„(ğ‘› âˆ’ ğ‘š)
ğ‘š=ğ‘›âˆ’ğ‘€+1
with ğ‘› = 0, . . . , ğ‘ âˆ’ 1,
(5)
where ğ‘ is the length of the audio signal, ğ‘€ the length of the RIR â„,
and all ğ‘¥(ğ‘›) with ğ‘› < 0 are assumed to be zero.
In general, the RIR â„ depends on the size of the room, the posi-
tions of the source and the receiver, and other room characteristics
such as the sound reflection properties of the walls, any furniture,
people, or other contents of the room. Hence, the audio signal re-
ceived by the ASR system is never identical to the original audio,
and an exact RIR is practically impossible to predict. We describe a
possible solution for a sufficient approximation in Section 3.
2.4 Psychoacoustics
Psychoacoustics yields an effective measure of (in-)audibility, which
is also helpful for the calculation of inconspicuous audio adversar-
ial examples [27, 29]. Psychoacoustic hearing thresholds describe
how the dependencies between frequencies and across time lead
to masking effects in human perception [41]. Probably the best-
known example for an application of these effects is found in MP3
compression [18], where the compression algorithm uses empirical
hearing thresholds to minimize bandwidth or storage requirements.
For this purpose, the original input signal is transformed into a
smaller but lossy representation.
For an attack, the psychoacoustic hearing thresholds are used
to limit the changes in the audio signal to time-frequency-ranges,
where the added perturbations are not, or barely, perceptible by
humans. To calculate the hearing thresholds, we use the approach
described by SchÃ¶nherr et al. [29].
3 OVER-THE-AIR ADVERSARIAL EXAMPLES
Our goal is to compute robust audio adversarial examples, which
still work after transmission from a loudspeaker. For this, we simu-
late different RIRs and employ an iterative algorithm to compute
Figure 4: For the room simulation model, we assume a prob-
ability distribution over all possible rooms by defining rele-
vant simulation parameters like the room geometry, the re-
verberation time ğ‘‡60, and positions of source and receiver as
random variables. To optimize our over-the-air adversarial
examples, we sample from this distribution to get a variety
of possible RIRs.
adversarial examples robust against signal modifications that are
incurred during playback in a room.
3.1 Threat Model
Throughout the rest of this paper, we consider the following threat
model similar to prior work in this area. We assume a white-box
attack, where the adversary knows the internals of the ASR system,
including all its model parameters. This requirement is in line with
prior work on this topic [9, 29, 39]. Using this knowledge, the
attacker generates malicious audio samples offline before the actual
attack takes place, i. e., the attacker exploits the ASR system to create
an audio file that produces the desired recognition result, which
is then played via a loudspeaker. Additionally, we assume that the
trained ASR system, including the DNN, remains unchanged over
time. Finally, we assume that the adversarial examples are played
over the air. Note that we only consider targeted attacks, where
the target transcription is predefined (i.e., the adversary chooses
the target transcription). Finally, we assume a threat model where
a potential attacker can run an extensive search. Specifically, the
attacker is able to calculate a batch of potential adversarial examples
and select those examples that are especially robust.
3.2 Room Impulse Response Simulator
To simulate RIRs, we use the AudioLabs implementation based on
the image method from Allen and Berkley [2]. The simulator takes
as input the room dimensions, the reverberation time ğ‘‡60, and the
position of source and receiver and approximates the corresponding
RIR for the given parameters.
For our attack, we model cuboid-shaped rooms, which can be
described by their length ğ‘ğ‘¥, width ğ‘ğ‘¦, height ğ‘ğ‘§ defined as b =
FeatureExtractionDNNrawaudiopseudo-posteriorsx(0)......x(Nâˆ’1)Ï‡=fP(xh)P(y|x)=f(Ï‡)bzrzszbxrxsxbyrysyT60Imperio: Robust Over-the-Air Adversarial Examples for Automatic Speech Recognition Systems
ACSAC 2020, December 7â€“11, 2020, Austin, USA
Figure 6: To simulate any RIR and to update the time domain
audio signal directly, the RIR is integrated as an additional
layer into the DNN.
Figure 5: Simulated RIR for b = [8 m, 7 m, 2.8 m], s =
[3.9 m, 3.4 m, 1.2 m] and r = [1.4 m, 1.8 m, 1.2 m], and ğ‘‡60 =
0.4 in the time domain (top) and the frequency domain (bot-
tom).
[ğ‘ğ‘¥, ğ‘ğ‘¦, ğ‘ğ‘§]. In addition to this, we model the three-dimensional
source position s = [ğ‘ ğ‘¥, ğ‘ ğ‘¦, ğ‘ ğ‘§], receiver position r = [ğ‘Ÿğ‘¥, ğ‘Ÿğ‘¦, ğ‘Ÿğ‘§],
and the reverberation time ğ‘‡60, which is a standard measure for
the audio decay time, defined as the time it takes for the sound
pressure level to reduce by 60 dB. This results in ten freely selectable
parameters. All parameters are also sketched in Figure 4. Even
though this might seem like an overly simple model, we show that
the computed adversarial examples are indeed robust for real rooms
that are more complex.
In order to sample random RIRs, we interpret these ten parame-
ters to be random variables. We draw each value from a uniform
distribution between a minimum and a maximum allowed value.
For the room size and for ğ‘‡60, the minimum and the maximum
values can be chosen arbitrarily and are thus selected first. After
those parameters are drawn, the ranges for source and receiver
positions are drawn to guarantee that the source and the receiver
are located inside the room.
To simplify the notation, we use the 10-dimensional parameter
vector ğœƒ in the following to describe all of these parameters. The
RIR â„ can be considered as a sample of the distribution ğ»ğœƒ . An
example of a simulated RIR in the time and the frequency domain
is shown in Figure 5.
3.3 Robust Audio Adversarial Examples
Unlike earlier approaches that feed adversarial examples directly
into the ASR system [9, 29], we explicitly include characteristics of
the room, in the form of RIRs, in the optimization problem. This
hardens the adversarial examples to remain functional in an over-
the-air attack.
For the attack, we therefore extend the optimization criterion
given in (3) by
ğ‘¥â€² = arg max
Ëœğ‘¥
Eâ„âˆ¼ğ»ğœƒ [ğ‘ƒ(ğ‘¦â€²|Ëœğ‘¥â„)].
(6)
This approach is derived from the Expectation Over Transforma-
tion (EOT) approach in the visual domain, where it is used to con-
sider different two- and three-dimensional transformations, which
leads to robust real-world adversarial examples [4]. In our case, in-
stead of visual transformations, we use the convolution with RIRs,
drawn from ğ»ğœƒ , to maximize the expectation over varying RIRs, as
shown in Equation (6).
For the implementation, we use a DNN that already has been aug-
mented to include the feature extraction and prepend an additional
layer to the DNN. This layer simulates the convolution from the
input audio file with the RIR â„ to model the transmission through
the room. Integrating this convolution as an additional layer allows
us to apply gradient descent directly to the raw audio signal. A
schematic overview of the proposed DNN is given in Figure 6. The
first part (â€œConvolutionâ€) describes the convolution with the RIR â„.
Note that the RIR simulation layer is only used for the calculation
of adversarial examples and removed during testing, as the actual
RIR will then act during the transmission over the air. The center
and right part (â€œFeature extractionâ€ and â€œDNNâ€) show the feature
extraction and the acoustic model DNN, which is used to obtain
the pseudo-posteriors for the decoding stage.
The inclusion of the convolution as a layer in the DNN requires
it to be differentiable. Using (5), the derivative can be written as
ğœ•ğ‘¥â„(ğ‘›)
ğœ•ğ‘¥(ğ‘š) = â„(ğ‘› âˆ’ ğ‘š) âˆ€ğ‘›, ğ‘š.
ğœ•ğ‘“ğ‘(ğ‘¥â„) Â· ğœ•ğ‘“ğ‘(ğ‘¥â„)
ğœ•ğ¿(ğ‘¦, ğ‘¦â€²)
Â· ğœ•ğ‘“ (ğœ’)
ğœ•ğ‘“ (ğœ’)
This can be integrated for the calculation of the gradient âˆ‡ğ‘¥ as
âˆ‡ğ‘¥ =
(8)
where the function ğ‘“ğ‘(Â·) describes the feature extraction. This is
an extension of the approach in [29], where
ğœ•ğ‘¥â„
,
Â· ğœ•ğ‘¥â„
ğœ•ğ‘¥
(7)
(9)
âˆ‡ğ‘¥ =
ğœ•ğ¿(ğ‘¦, ğ‘¦â€²)
ğœ•ğ‘“ (ğœ’)
ğœ•ğ‘“ğ‘(ğ‘¥) Â· ğœ•ğ‘“ğ‘(ğ‘¥)
Â· ğœ•ğ‘“ (ğœ’)
ğœ•ğ‘¥
is defined for the calculation of adversarial examples via gradient
descent using the objective function ğ¿(Â·).
3.4 Over-the-Air Adversarial Examples
To assess the robustness of the hardened over-the-air adversarial
attack, the adversarial examples ğ‘¥â€² have to be played back via a
loudspeaker, and the recorded audio signals are used to determine
the accuracy. For the calculation, we implemented the optimization
as defined in (6), by sampling a new RIR â„ after every set of ğ‘„
gradient descent iterations. This simulates different rooms and
recording conditions. Therefore, the generated adversarial example
depends on the distribution ğ»ğœƒ , from which the RIR â„ is drawn.
After each gradient descent step, the audio signal ğ‘¥â€² is updated via
the calculated gradient âˆ‡ğ‘¥ at the learning rate ğ›¼.
ConvolutionFeatureExtractionDNNrawaudiopseudo-posteriorsx(n)...x(nâˆ’M+2)x(nâˆ’M+1)Ï‡=fP(xh)P(y|x)=f(Ï‡)xh=xâˆ—hhACSAC 2020, December 7â€“11, 2020, Austin, USA
Lea SchÃ¶nherr, Thorsten Eisenhofer, Steffen Zeiler, Thorsten Holz, and Dorothea Kolossa
olds Î¦, distribution ğ»ğœƒ
Algorithm 1 Calculation of robust adversarial examples.