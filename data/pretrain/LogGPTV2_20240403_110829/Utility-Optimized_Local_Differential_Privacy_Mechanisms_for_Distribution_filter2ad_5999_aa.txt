title:Utility-Optimized Local Differential Privacy Mechanisms for Distribution
Estimation
author:Takao Murakami and
Yusuke Kawamoto
Utility-Optimized Local Differential Privacy 
Mechanisms for Distribution Estimation
Takao Murakami and Yusuke Kawamoto, AIST
https://www.usenix.org/conference/usenixsecurity19/presentation/murakami
This paper is included in the Proceedings of the 28th USENIX Security Symposium.August 14–16, 2019 • Santa Clara, CA, USA978-1-939133-06-9Open access to the Proceedings of the 28th USENIX Security Symposium is sponsored by USENIX.Utility-Optimized Local Differential Privacy Mechanisms for
Distribution Estimation ∗
Takao Murakami
AIST
Yusuke Kawamoto
AIST
Abstract
LDP (Local Differential Privacy) has been widely studied to
estimate statistics of personal data (e.g., distribution underly-
ing the data) while protecting users’ privacy. Although LDP
does not require a trusted third party, it regards all personal
data equally sensitive, which causes excessive obfuscation
hence the loss of utility. In this paper, we introduce the notion
of ULDP (Utility-optimized LDP), which provides a privacy
guarantee equivalent to LDP only for sensitive data. We ﬁrst
consider the setting where all users use the same obfuscation
mechanism, and propose two mechanisms providing ULDP:
utility-optimized randomized response and utility-optimized
RAPPOR. We then consider the setting where the distinction
between sensitive and non-sensitive data can be different from
user to user. For this setting, we propose a personalized ULDP
mechanism with semantic tags to estimate the distribution of
personal data with high utility while keeping secret what is
sensitive for each user. We show theoretically and experimen-
tally that our mechanisms provide much higher utility than
the existing LDP mechanisms when there are a lot of non-
sensitive data. We also show that when most of the data are
non-sensitive, our mechanisms even provide almost the same
utility as non-private mechanisms in the low privacy regime.
1 Introduction
DP (Differential Privacy) [21,22] is becoming a gold standard
for data privacy; it enables big data analysis while protecting
users’ privacy against adversaries with arbitrary background
knowledge. According to the underlying architecture, DP
can be categorized into the one in the centralized model and
the one in the local model [22]. In the centralized model, a
“trusted” database administrator, who can access to all users’
personal data, obfuscates the data (e.g., by adding noise, gen-
eralization) before providing them to a (possibly malicious)
data analyst. Although DP was extensively studied for the
∗This
JP17K12667, and by Inria under the project LOGIS.
study was
supported by JSPS KAKENHI
JP19H04113,
centralized model at the beginning, the original personal data
in this model can be leaked from the database by illegal access
or internal fraud. This issue is critical in recent years, because
the number of data breach incidents is increasing [15].
The local model does not require a “trusted” administra-
tor, and therefore does not suffer from the data leakage is-
sue explained above. In this model, each user obfuscates her
personal data by herself, and sends the obfuscated data to
a data collector (or data analyst). Based on the obfuscated
data, the data collector can estimate some statistics (e.g., his-
togram, heavy hitters [45]) of the personal data. DP in the
local model, which is called LDP (Local Differential Pri-
vacy) [19], has recently attracted much attention in the aca-
demic ﬁeld [5, 12, 24, 29, 30, 39, 43, 45, 46, 50, 56], and has
also been adopted by industry [16, 23, 49].
However, LDP mechanisms regard all personal data as
equally sensitive, and leave a lot of room for increasing data
utility. For example, consider questionnaires such as: “Have
you ever cheated in an exam?” and “Were you with a prostitute
in the last month?” [11]. Obviously, “Yes” is a sensitive re-
sponse to these questionnaires, whereas “No” is not sensitive.
A RR (Randomized Response) method proposed by Man-
gat [37] utilizes this fact. Speciﬁcally, it reports “Yes” or “No”
as follows: if the true answer is “Yes”, always report “Yes”;
otherwise, report “Yes” and “No” with probability p and 1− p,
respectively. Since the reported answer “Yes” may come from
both the true answers “Yes” and “No”, the conﬁdentiality of
the user reporting “Yes” is not violated. Moreover, since the
reported answer “No” is always come from the true answer
“No”, the data collector can estimate a distribution of true
answers with higher accuracy than Warner’s RR [52], which
simply ﬂips “Yes” and ”No” with probability p. However,
Mangat’s RR does not provide LDP, since LDP regards both
“Yes” and “No” as equally sensitive.
There are a lot of “non-sensitive” data for other types of
data. For example, locations such as hospitals and home can
be sensitive, whereas visited sightseeing places, restaurants,
and coffee shops are non-sensitive for many users. Divorced
people may want to keep their divorce secret, while the oth-
USENIX Association
28th USENIX Security Symposium    1877
ers may not care about their marital status. The distinction
between sensitive and non-sensitive data can also be different
from user to user (e.g., home address is different from user to
user; some people might want to keep secret even the sight-
seeing places). To explain more about this issue, we brieﬂy
review related work on LDP and variants of DP.
Related work. Since Dwork [21] introduced DP, a number
of its variants have been studied to provide different types of
privacy guarantees; e.g., LDP [19], d-privacy [8], Pufferﬁsh
privacy [32], dependent DP [36], Bayesian DP [53], mutual-
information DP [14], Rényi DP [38], and distribution privacy
[31]. In particular, LDP [19] has been widely studied in the
literature. For example, Erlingsson et al. [23] proposed the
RAPPOR as an obfuscation mechanism providing LDP, and
implemented it in Google Chrome browser. Kairouz et al.
[29] showed that under the l1 and l2 losses, the randomized
response (generalized to multiple alphabets) and RAPPOR are
order optimal among all LDP mechanisms in the low and high
privacy regimes, respectively. Wang et al. [51] generalized
the RAPPOR and a random projection-based method [6], and
found parameters that minimize the variance of the estimate.
Some studies also attempted to address the non-uniformity
of privacy requirements among records (rows) or among items
(columns) in the centralized DP: Personalized DP [28], Het-
erogeneous DP [3], and One-sided DP [17]. However, obfus-
cation mechanisms that address the non-uniformity among
input values in the “local” DP have not been studied, to our
knowledge. In this paper, we show that data utility can be
signiﬁcantly increased by designing such local mechanisms.
Our contributions. The goal of this paper is to design obfus-
cation mechanisms in the local model that achieve high data
utility while providing DP for sensitive data. To achieve this,
we introduce the notion of ULDP (Utility-optimized LDP),
which provides a privacy guarantee equivalent to LDP only for
sensitive data, and obfuscation mechanisms providing ULDP.
As a task for the data collector, we consider discrete distribu-
tion estimation [2, 23, 24, 27, 29, 39, 46, 56], where personal
data take discrete values. Our contributions are as follows:
• We ﬁrst consider the setting in which all users use the
same obfuscation mechanism, and propose two ULDP
mechanisms: utility-optimized RR and utility-optimized
RAPPOR. We prove that when there are a lot of non-
sensitive data, our mechanisms provide much higher util-
ity than two state-of-the-art LDP mechanisms: the RR
(for multiple alphabets) [29, 30] and RAPPOR [23]. We
also prove that when most of the data are non-sensitive,
our mechanisms even provide almost the same utility as
a non-private mechanism that does not obfuscate the per-
sonal data in the low privacy regime where the privacy
budget is ε = ln|X| for a set X of personal data.
• We then consider the setting in which the distinction
between sensitive and non-sensitive data can be different
from user to user, and propose a PUM (Personalized
ULDP Mechanism) with semantic tags. The PUM keeps
secret what is sensitive for each user, while enabling the
data collector to estimate a distribution using some back-
ground knowledge about the distribution conditioned on
each tag (e.g., geographic distributions of homes). We
also theoretically analyze the data utility of the PUM.
• We ﬁnally show that our mechanisms are very promising
in terms of utility using two large-scale datasets.
The proofs of all statements in the paper are given in the
extended version of the paper [40].
Cautions and limitations. Although ULDP is meant to pro-
tect sensitive data, there are some cautions and limitations.
First, we assume that each user sends a single datum and
that each user’s personal data is independent (see Section 2.1).
This is reasonable for a variety of personal data (e.g., locations,
age, sex, marital status), where each user’s data is irrelevant
to most others’ one. However, for some types of personal
data (e.g., ﬂu status [48]), each user can be highly inﬂuenced
by others. There might also be a correlation between sensi-
tive data and non-sensitive data when a user sends multiple
data (on a related note, non-sensitive attributes may lead to
re-identiﬁcation of a record [41]). A possible solution to these
problems would be to incorporate ULDP with Pufferﬁsh pri-
vacy [32, 48], which is used to protect correlated data. We
leave this as future work (see Section 7 for discussions on the
case of multiple data per user and the correlation issue).
We focus on a scenario in which it is easy for users to
decide what is sensitive (e.g., cheating experience, location
of home). However, there is also a scenario in which users do
not know what is sensitive. For the latter scenario, we cannot
use ULDP but can simply apply LDP.
Apart from the sensitive/non-sensitive data issue, there are
scenarios in which ULDP does not cover. For example, ULDP
does not protect users who have a sensitivity about “informa-
tion disclosure” itself (i.e., those who will not disclose any
information). We assume that users have consented to infor-
mation disclosure. To collect as much data as possible, we can
provide an incentive for the information disclosure; e.g., pro-
vide a reward or point-of-interest (POI) information nearby
a reported location. We also assume that the data collector
obtains a consensus from users before providing reported data
to third parties. Note that these cautions are common to LDP.
There might also be a risk of discrimination; e.g., the data
collector might discriminate against all users that provide a
yes-answer, and have no qualms about small false positives.
False positives decrease with increase in ε. We note that LDP
also suffer from this attack; the false positive probability is
the same for both ULDP and LDP with the same ε.
In summary, ULDP provides a privacy guarantee equivalent
to LDP for sensitive data under the assumption of the data
independence. We consider our work as a building-block of
broader DP approaches or the basis for further development.
1878    28th USENIX Security Symposium
USENIX Association
2 Preliminaries
2.1 Notations
Let R≥0 be the set of non-negative real numbers. Let n be the
number of users, [n] = {1,2, . . . ,n}, X (resp. Y ) be a ﬁnite
set of personal (resp. obfuscated) data. We assume continuous
data are discretized into bins in advance (e.g., a location map
is divided into some regions). We use the superscript “(i)”
to represent the i-th user. Let X (i) (resp. Y (i)) be a random
variable representing personal (resp. obfuscated) data of the i-
th user. The i-th user obfuscates her personal data X (i) via her
obfuscation mechanism Q(i), which maps x ∈ X to y ∈ Y with
probability Q(i)(y|x), and sends the obfuscated data Y (i) to a
data collector. Here we assume that each user sends a single
datum. We discuss the case of multiple data in Section 7.
We divide personal data into two types: sensitive data and
non-sensitive data. Let XS ⊆ X be a set of sensitive data com-
mon to all users, and XN = X \ XS be the remaining personal
data. Examples of such “common” sensitive data x ∈ XS are
the regions including public sensitive locations (e.g., hos-
pitals) and obviously sensitive responses to questionnaires
described in Section 11.
S ⊆ XN (i ∈ [n]) be a set of sensitive
Furthermore, let X (i)
data speciﬁc to the i-th user (here we do not include XS into
X (i)
S because XS is protected for all users in our mechanisms).
X (i)
is a set of personal data that is possibly non-sensitive
S
for many users but sensitive for the i-th user. Examples of
such “user-speciﬁc” sensitive data x ∈ X (i)
are the regions
including private locations such as their home and workplace.
(Note that the majority of working population can be uniquely
identiﬁed from their home/workplace location pairs [25].)
S
S = ··· = X (n)
In Sections 3 and 4, we consider the case where all users
divide X into the same sets of sensitive data and of non-
sensitive data, i.e., X (1)
S = /0, and use the same
obfuscation mechanism Q (i.e., Q = Q(1) = ··· = Q(n)). In
Section 5, we consider a general setting that can deal with the
user-speciﬁc sensitive data X (i)
S and user-speciﬁc mechanisms
Q(i). We call the former case a common-mechanism scenario
and the latter a personalized-mechanism scenario.
We assume that each user’s personal data X (i) is inde-
pendently and identically distributed (i.i.d.) with a proba-