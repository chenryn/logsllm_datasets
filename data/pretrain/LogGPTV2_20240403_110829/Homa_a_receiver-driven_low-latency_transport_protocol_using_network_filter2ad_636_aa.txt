title:Homa: a receiver-driven low-latency transport protocol using network
priorities
author:Behnam Montazeri and
Yilong Li and
Mohammad Alizadeh and
John K. Ousterhout
Homa: A Receiver-Driven Low-Latency
Transport Protocol Using Network Priorities
Behnam Montazeri, Yilong Li, Mohammad Alizadeh†, and John Ousterhout
Stanford University, †MIT
ABSTRACT
Homa is a new transport protocol for datacenter networks. It pro-
vides exceptionally low latency, especially for workloads with
a high volume of very short messages, and it also supports large
messages and high network utilization. Homa uses in-network
priority queues to ensure low latency for short messages; priority
allocation is managed dynamically by each receiver and inte-
grated with a receiver-driven flow control mechanism. Homa
also uses controlled overcommitment of receiver downlinks to
ensure efficient bandwidth utilization at high load. Our imple-
mentation of Homa delivers 99th percentile round-trip times
less than 15 µs for short messages on a 10 Gbps network running
at 80% load. These latencies are almost 100x lower than the best
published measurements of an implementation. In simulations,
Homa’s latency is roughly equal to pFabric and significantly
better than pHost, PIAS, and NDP for almost all message sizes
and workloads. Homa can also sustain higher network loads
than pFabric, pHost, or PIAS.
CCS CONCEPTS
• Networks → Network protocols; Data center networks;
KEYWORDS
Data centers; low latency; network stacks; transport protocols
ACM Reference Format:
Behnam Montazeri, Yilong Li, Mohammad Alizadeh, and John Ouster-
hout. Homa: A Receiver-Driven Low-Latency Transport Protocol Us-
ing Network Priorities . In Proceedings of ACM SIGCOMM 2018
(SIGCOMM ’18). ACM, New York, NY, USA, 15 pages. https://doi.
org/10.1145/3230543.3230564
1
The rise of datacenter computing over the last decade has created
new operating conditions for network transport protocols. Mod-
ern datacenter networking hardware offers the potential for very
low latency communication. Round-trip times of 5 µs or less are
now possible for short messages, and a variety of applications
have arisen that can take advantage of this latency [20, 24, 26].
In addition, many datacenter applications use request-response
protocols that are dominated by very short messages (a few hun-
dred bytes or less). Existing transport protocols are ill-suited to
INTRODUCTION
Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made
or distributed for profit or commercial advantage and that copies bear this
notice and the full citation on the first page. Copyrights for components of this
work owned by others than ACM must be honored. Abstracting with credit is
permitted. To copy otherwise, or republish, to post on servers or to redistribute
to lists, requires prior specific permission and/or a fee. Request permissions
from permissions@acm.org.
SIGCOMM ’18, August 20-25, 2018, Budapest, Hungary
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5567-4/18/08. . . $15.00
https://doi.org/10.1145/3230543.3230564
these conditions, so the latency they provide for short messages
is far higher than the hardware potential, particularly under high
network loads.
Recent years have seen numerous proposals for better trans-
port protocols, including improvements to TCP [2, 3, 31] and
a variety of new protocols [4, 6, 14, 15, 17, 25, 32]. However,
none of these designs considers today’s small message sizes;
they are based on heavy-tailed workloads where 100 Kbyte mes-
sages are considered “small,” and latencies are often measured
in milliseconds, not microseconds. As a result, there is still no
practical solution that provides near-hardware latencies for short
messages under high network loads. For example, we know of
no existing implementation with tail latencies of 100 µs or less
at high network load (within 20x of the hardware potential).
Homa is a new transport protocol designed for small messages
in low-latency datacenter environments. Our implementation
of Homa achieves 99th percentile round trip latencies less than
15 µs for small messages at 80% network load with 10 Gbps
link speeds, and it does this even in the presence of competing
large messages. Across a wide range of message sizes and work-
loads, Homa achieves 99th percentile latencies at 80% network
load that are within a factor of 2–3.5x of the minimum possible
latency on an unloaded network. Although Homa favors small
messages, it also improves the performance of large messages
in comparison to TCP-like approaches based on fair sharing.
Homa uses two innovations to achieve its high performance.
The first is its aggressive use of the priority queues provided
by modern network switches. In order to make the most of the
limited number of priority queues, Homa assigns priorities dy-
namically on receivers, and it integrates the priorities with a
receiver-driven flow control mechanism like that of pHost [13]
and NDP [15]. Homa’s priority mechanism improves tail latency
by 2–16x compared to previous receiver-driven approaches.
In comparison to sender-driven priority mechanisms such as
PIAS [6], Homa provides a better approximation to SRPT (short-
est remaining processing time first); this reduces tail latency by
0–3x over PIAS.
Homa’s second contribution is its use of controlled over-
commitment, where a receiver allows a few senders to transmit
simultaneously. Slightly overcommitting receiver downlinks
in this way allows Homa to use network bandwidth efficiently:
Homa can sustain network loads 2–33% higher than pFabric [4],
PIAS, pHost, and NDP. Homa limits the overcommitment and
integrates it with the priority mechanism to prevent queuing of
short messages.
Homa has several other unusual features that contribute to its
high performance. It uses a message-based architecture rather
than a streaming approach; this eliminates head-of-line block-
ing at senders and reduces tail latency by 100x over streaming
transports such as TCP. Homa is connectionless, which reduces
SIGCOMM ’18, August 20-25, 2018, Budapest, Hungary
B. Montazeri et al.
connection state in large-scale applications. It has no explicit ac-
knowledgments, which reduces overheads for small messages,
and it implements at-least-once semantics rather than at-most-
once.
2 MOTIVATION AND KEY IDEAS
The primary goal of Homa is to provide the lowest possible
latency for short messages at high network load using current
networking hardware. We focus on tail message latency (99th
percentile), as it is the most important metric for datacenter
applications [2, 33]. A large body of work has focused on low
latency datacenter transport in recent years. However, as our re-
sults will show, existing designs are sub-optimal for tail latency
at high network load, particularly in networks with raw hard-
ware latency in the single-digit microseconds [9, 21, 28, 34].
In this section, we discuss the challenges that arise in such
networks and we derive Homa’s key design features.
2.1 Motivation: Tiny latency for tiny messages
State-of-the-art cut-through switches have latencies of at most a
few hundred nanoseconds [30]. Low latency network interface
cards and software stacks (e.g., DPDK [9]) have also become
common in the last few years. These advances have made it
possible to achieve one-way latencies of a few microseconds
in the absence of queuing, even across a large network with
thousands of servers (e.g., a 3-level fat-tree network).
Meanwhile, many datacenter applications rely on request-
response protocols with tiny messages of a few hundred bytes
or less. In typical remote procedure call (RPC) use cases, it is
almost always the case that either the request or the response
is tiny, since data usually flows in only one direction. The data
itself is often very short as well. Figure 1 shows a collection of
workloads that we used to design and evaluate Homa, most of
which were measured from datacenter applications at Google
and Facebook. In three of these workloads, more than 85% of
messages were less than 1000 bytes. In the most extreme case
(W1), more than 70% of all network traffic, measured in bytes,
was in messages less than 1000 bytes.
To our knowledge, almost all prior work has focused on work-
loads with very large messages. For example, in the Web Search
workload used to evaluate DCTCP [2] and pFabric [4] (W5 in
Figure 1), messages longer than 1 Mbyte account for 95% of
transmitted bytes, and any message shorter than 100 Kbytes
was considered “short.” Most subsequent work has used the
same workloads. To obtain these workloads, message sizes were
estimated from packet captures based on inactivity of TCP con-
nections beyond a threshold (e.g., 50 ms). Unfortunately, this ap-
proach overestimates message sizes, since a TCP connection can
contain many closely-spaced messages. In Figure 1, workloads
W1–W3 were measured explicitly in terms of application-level
messages, and they show much smaller sizes than workloads
W4 and W5, which were extracted from packet captures.
Unfortunately, existing datacenter transport designs cannot
achieve the lowest possible latency for tiny messages at high
network load. We explore the design space in the next section,
W1 Accesses to a collection of memcached servers
at Facebook, as approximated by the statistical
model of the ETC workload in Section 5 and
Table 5 of [5].
Search application at Google [29].
W2
W3 Aggregated workload from all applications
running in a Google datacenter [29].
W4 Hadoop cluster at Facebook [27].
W5 Web search workload used for DCTCP [2].
Figure 1: The workloads used to design and evaluate Homa.
Workloads W1–W3 were measured from application-level logs of
message sizes; message sizes for W4 and W5 were estimated from
packet traces. The upper graph shows the cumulative distribution of
message sizes weighted by number of messages, and the lower graph
is weighted by bytes. The workloads are ordered by average message
size: W1 is the smallest, and W5 is most heavy-tailed.
but consider, for example, designs that do not take advantage
of in-network priorities (e.g., HULL [3], PDQ [17], NDP [15]).
These designs attempt to limit queue buildup, but none of them
can eliminate queuing altogether. The state-of-the-art approach,
NDP [15], strictly limits queues to 8 packets, equivalent to
roughly 10 𝜇s of latency at 10 Gbps. While this queuing latency
has negligible impact in a network with moderate latency (e.g.,
RTTs greater than 50 µs) or for moderately-sized messages
(e.g., 100 KBytes), it increases by 5x the completion time of a
200-byte message in a network with 5 𝜇s RTT.
2.2 The Design Space
We now present a walk through the design space of low latency
datacenter transport protocols. We derive Homa’s four key de-
sign principles: (i) transmitting short messages blindly, (ii) using
in-network priorities, (iii) allocating priorities dynamically at
receivers in conjunction with receiver-driven rate control, and
(iv) controlled overcommitment of receiver downlinks. While
some past designs use the first two of these techniques, we show
that combining all four techniques is crucial to deliver the lowest
levels of latency at high network load.
We focus on message latency (not packet latency) since it
reflects application performance. A message is a block of bytes
of any length transmitted from a single sender to a single re-
ceiver. The sender must specify the size of a message when
020406080100110100100010000105106107Message/Flow Size (Bytes)Cumulative % of MessagesWorkLoadW1W2W3W4W5020406080100110100100010000105106107Message/Flow Size (Bytes)Cumulative % of BytesWorkLoadW1W2W3W4W5Homa: A Receiver-Driven Low-Latency Transport Protocol SIGCOMM ’18, August 20-25, 2018, Budapest, Hungary
presenting its first byte to the transport, and the receiver cannot
act on a message until it has been received in its entirety. Knowl-
edge of message sizes is particularly valuable because it allows
transports to prioritize shorter messages.
The key challenge in delivering short messages with low la-
tency is to eliminate queuing delays. Similar to prior work, we
assume that bandwidth in the network core is sufficient to accom-
modate the offered load, and that the network supports efficient
load-balancing [1, 10, 16], so that packets are distributed evenly
across the available paths (we assume simple randomized per-
packet spraying in our design). As a result, queueing will occur
primarily in the downlinks from top-of-rack switches (TORs)
to machines. This happens when multiple senders transmit si-
multaneously to the same receiver. The worst-case scenario is
incast, where an application initiates RPCs to many servers
concurrently and the responses all arrive at the same time.
There is no time to schedule every packet. An ideal scheme
might attempt to schedule every packet at a central arbiter, as
in Fastpass [25]. Such an arbiter could take into account all the
messages and make a global scheduling decision about which
packet to transmit from each sender and when to transmit it. The
arbiter could in theory avoid queues in the network altogether.
However, this approach triples the latency for short messages:
a tiny, single-packet message takes at least 1.5 RTTs if it needs
to wait for a scheduling decision, whereas it could finish within
0.5 RTT if transmitted immediately. Receiver-based scheduling
mechanisms such as ExpressPass [8] suffer the same penalty.
In order to achieve the lowest possible latency, short messages
must be transmitted blindly, without considering potential con-
gestion. In general, a sender must transmit enough bytes blindly
to cover the round-trip time to the receiver (including software
overheads on both ends); during this time the receiver can return
explicit scheduling information to control future transmissions,
without introducing additional delays. We refer to this amount
of data as RTTbytes; it is about 10 KB in our implementation of
Homa for 10 Gbps networks.
Buffering is a necessary evil. Blind transmissions mean that
buffering can occur when multiple senders transmit to the same
receiver. No protocol can achieve minimum latency without in-
curring some buffering. But, ironically, when buffering occurs,
it will increase latency. Many previous designs have attempted
to reduce buffering, e.g., with carefully-engineered rate control
schemes [2, 21, 34], reserving bandwidth headroom [3], or even
strictly limiting the buffer size to a small value [15]. However,
none of these approaches can completely eliminate the latency
penalty of buffering.
In-network priorities are a must. Given the inevitability of
buffering, the only way to achieve the lowest possible latency is
to use in-network priorities. Each output port in a modern switch