# Homa: A Receiver-Driven Low-Latency Transport Protocol Using Network Priorities

**Authors:** Behnam Montazeri, Yilong Li, Mohammad Alizadeh†, and John K. Ousterhout  
**Affiliations:** Stanford University, †MIT

## Abstract
Homa is a new transport protocol designed for datacenter networks, offering exceptionally low latency, particularly for workloads with a high volume of very short messages. It also supports large messages and high network utilization. Homa leverages in-network priority queues to ensure low latency for short messages, with priority allocation managed dynamically by each receiver and integrated with a receiver-driven flow control mechanism. Additionally, Homa uses controlled overcommitment of receiver downlinks to ensure efficient bandwidth utilization at high loads. Our implementation of Homa achieves 99th percentile round-trip times of less than 15 µs for short messages on a 10 Gbps network running at 80% load. These latencies are almost 100x lower than the best published measurements of other implementations. In simulations, Homa's latency is comparable to pFabric and significantly better than pHost, PIAS, and NDP for almost all message sizes and workloads. Homa can also sustain higher network loads than pFabric, pHost, or PIAS.

## CCS Concepts
- **Networks →** Network protocols; Data center networks

## Keywords
- Data centers
- Low latency
- Network stacks
- Transport protocols

## ACM Reference Format
Behnam Montazeri, Yilong Li, Mohammad Alizadeh, and John Ousterhout. "Homa: A Receiver-Driven Low-Latency Transport Protocol Using Network Priorities." In Proceedings of ACM SIGCOMM 2018 (SIGCOMM '18). ACM, New York, NY, USA, 15 pages. https://doi.org/10.1145/3230543.3230564

## Introduction
The rise of datacenter computing over the past decade has created new operating conditions for network transport protocols. Modern datacenter networking hardware offers the potential for very low-latency communication, with round-trip times of 5 µs or less for short messages. Various applications have emerged that can benefit from such low latency [20, 24, 26]. Many datacenter applications use request-response protocols dominated by very short messages (a few hundred bytes or less). However, existing transport protocols are not well-suited to these conditions, resulting in much higher latencies for short messages, especially under high network loads.

Recent years have seen numerous proposals for improved transport protocols, including enhancements to TCP [2, 3, 31] and various new protocols [4, 6, 14, 15, 17, 25, 32]. However, none of these designs adequately address today's small message sizes, as they are based on heavy-tailed workloads where 100 Kbyte messages are considered "small," and latencies are often measured in milliseconds rather than microseconds. Consequently, there is still no practical solution that provides near-hardware latencies for short messages under high network loads. For example, no existing implementation achieves tail latencies of 100 µs or less at high network loads (within 20x of the hardware potential).

Homa is a new transport protocol designed for small messages in low-latency datacenter environments. Our implementation of Homa achieves 99th percentile round-trip latencies of less than 15 µs for small messages at 80% network load with 10 Gbps link speeds, even in the presence of competing large messages. Across a wide range of message sizes and workloads, Homa achieves 99th percentile latencies at 80% network load that are within a factor of 2–3.5x of the minimum possible latency on an unloaded network. Although Homa favors small messages, it also improves the performance of large messages compared to TCP-like approaches based on fair sharing.

Homa uses two key innovations to achieve its high performance:
1. **Priority Queues:** Homa aggressively utilizes the priority queues provided by modern network switches. To make the most of the limited number of priority queues, Homa dynamically assigns priorities on receivers and integrates them with a receiver-driven flow control mechanism similar to those in pHost [13] and NDP [15]. This priority mechanism improves tail latency by 2–16x compared to previous receiver-driven approaches and provides a better approximation to SRPT (shortest remaining processing time first) than sender-driven priority mechanisms like PIAS [6], reducing tail latency by 0–3x.
2. **Controlled Overcommitment:** Homa allows a few senders to transmit simultaneously by slightly overcommitting receiver downlinks. This approach enables Homa to use network bandwidth efficiently, sustaining network loads 2–33% higher than pFabric [4], PIAS, pHost, and NDP. Homa limits the overcommitment and integrates it with the priority mechanism to prevent queuing of short messages.

Homa also features several other unique characteristics that contribute to its high performance:
- **Message-Based Architecture:** Unlike streaming approaches, Homa uses a message-based architecture, eliminating head-of-line blocking at senders and reducing tail latency by 100x over streaming transports like TCP.
- **Connectionless Design:** Homa is connectionless, reducing connection state in large-scale applications.
- **No Explicit Acknowledgments:** Homa omits explicit acknowledgments, reducing overheads for small messages.
- **At-Least-Once Semantics:** Homa implements at-least-once semantics rather than at-most-once.

## Motivation and Key Ideas
The primary goal of Homa is to provide the lowest possible latency for short messages at high network loads using current networking hardware. We focus on tail message latency (99th percentile), which is the most critical metric for datacenter applications [2, 33]. Despite significant research on low-latency datacenter transport, existing designs fall short for tail latency at high network loads, particularly in networks with raw hardware latency in the single-digit microseconds [9, 21, 28, 34].

### Motivation: Tiny Latency for Tiny Messages
State-of-the-art cut-through switches have latencies of just a few hundred nanoseconds [30]. Low-latency network interface cards and software stacks (e.g., DPDK [9]) have also become common, making one-way latencies of a few microseconds achievable in the absence of queuing, even across large networks with thousands of servers (e.g., a 3-level fat-tree network).

Many datacenter applications rely on request-response protocols with tiny messages of a few hundred bytes or less. In typical remote procedure call (RPC) use cases, either the request or the response is usually tiny, as data flows primarily in one direction. Figure 1 shows a collection of workloads used to design and evaluate Homa, most of which were measured from datacenter applications at Google and Facebook. In three of these workloads, more than 85% of messages were less than 1000 bytes. In the most extreme case (W1), more than 70% of all network traffic, measured in bytes, was in messages less than 1000 bytes.

To our knowledge, almost all prior work has focused on workloads with very large messages. For example, in the Web Search workload used to evaluate DCTCP [2] and pFabric [4] (W5 in Figure 1), messages longer than 1 Mbyte account for 95% of transmitted bytes, and any message shorter than 100 Kbytes was considered "short." Most subsequent work has used the same workloads, estimating message sizes from packet captures based on inactivity of TCP connections beyond a threshold (e.g., 50 ms). Unfortunately, this approach overestimates message sizes, as a TCP connection can contain many closely-spaced messages. In Figure 1, workloads W1–W3 were measured explicitly in terms of application-level messages, showing much smaller sizes than workloads W4 and W5, which were extracted from packet captures.

Existing datacenter transport designs cannot achieve the lowest possible latency for tiny messages at high network loads. We explore the design space in the next section, but consider, for example, designs that do not take advantage of in-network priorities (e.g., HULL [3], PDQ [17], NDP [15]). These designs attempt to limit queue buildup, but none can eliminate queuing altogether. The state-of-the-art approach, NDP [15], strictly limits queues to 8 packets, equivalent to roughly 10 µs of latency at 10 Gbps. While this queuing latency has negligible impact in a network with moderate latency (e.g., RTTs greater than 50 µs) or for moderately-sized messages (e.g., 100 KBytes), it increases the completion time of a 200-byte message by 5x in a network with 5 µs RTT.

### The Design Space
We now present a walk through the design space of low-latency datacenter transport protocols, deriving Homa's four key design principles:
1. **Blind Transmission of Short Messages:** Short messages must be transmitted without considering potential congestion.
2. **In-Network Priorities:** Use of in-network priorities to manage queuing.
3. **Dynamic Priority Allocation at Receivers:** Integration of priority allocation with receiver-driven rate control.
4. **Controlled Overcommitment of Receiver Downlinks:** Efficient use of bandwidth by allowing simultaneous transmissions from multiple senders.

While some past designs use the first two techniques, combining all four is crucial to deliver the lowest levels of latency at high network loads.

We focus on message latency (not packet latency) since it reflects application performance. A message is a block of bytes of any length transmitted from a single sender to a single receiver. The sender must specify the size of a message when presenting its first byte to the transport, and the receiver cannot act on a message until it has been received in its entirety. Knowledge of message sizes is valuable because it allows transports to prioritize shorter messages.

The key challenge in delivering short messages with low latency is to eliminate queuing delays. Similar to prior work, we assume that bandwidth in the network core is sufficient to accommodate the offered load, and that the network supports efficient load-balancing [1, 10, 16], so that packets are distributed evenly across available paths (we assume simple randomized per-packet spraying in our design). As a result, queuing will primarily occur in the downlinks from top-of-rack switches (TORs) to machines, especially during incast scenarios where multiple responses arrive simultaneously.

An ideal scheme might attempt to schedule every packet at a central arbiter, as in Fastpass [25]. Such an arbiter could avoid queues in the network altogether but would triple the latency for short messages. Receiver-based scheduling mechanisms such as ExpressPass [8] suffer the same penalty. To achieve the lowest possible latency, short messages must be transmitted blindly, without considering potential congestion. Generally, a sender must transmit enough bytes blindly to cover the round-trip time to the receiver (including software overheads on both ends); during this time, the receiver can return explicit scheduling information to control future transmissions without introducing additional delays. In our implementation of Homa for 10 Gbps networks, this amount of data (RTTbytes) is about 10 KB.

Buffering is a necessary evil. Blind transmissions mean that buffering can occur when multiple senders transmit to the same receiver. No protocol can achieve minimum latency without incurring some buffering. However, when buffering occurs, it will increase latency. Many previous designs have attempted to reduce buffering, e.g., with carefully-engineered rate control schemes [2, 21, 34], reserving bandwidth headroom [3], or strictly limiting the buffer size [15]. However, none of these approaches can completely eliminate the latency penalty of buffering.

In-network priorities are essential. Given the inevitability of buffering, the only way to achieve the lowest possible latency is to use in-network priorities. Each output port in a modern switch has multiple priority queues, and Homa dynamically allocates these priorities to ensure that short messages are prioritized.