0.1427
SP
0.9963
0.9931
0.9963
0.2004
0.3100
B1
1.0000
1.0000
1.0000
0.8244
0.6656
Recall
B2
0.0498
0.0753
0.1189
0.1000
0.2918
DP
0.5250
0.5263
0.5250
0.5250
0.5250
SP
0.9968
0.9954
0.9968
0.9977
0.8993
B1
0.0199
0.0227
0.0199
0.0138
0.0339
F1-score
DP
B2
0.6868
0.0315
0.6865
0.0271
0.0208
0.6868
0.0435
0.0149
0.0358
0.2244
SP
0.9966
0.9943
0.9966
0.3337
0.4610
Table 3: B1 and B2 are the best mechanisms from two families of mechanism. DP and SP are the mechanisms from Section 4.1.1
and Section 4.1.2 respectively. Going from red to blue the value decreases. ε = 0.1
but also its divergence. Thus, it indicates that our methodology is
even more appropriate for big data settings. On the other hand,
for anomalies, the errors of DP-mechanism are concentrated about
1/(1 + eε ) (Figure 7). This is in accordance with our theoretical
results and the assumption that the databases are typically sparse.
Next, we evaluated the performance over the normal records.
Here, both the SP and the DP mechanisms performed equally (Fig-
ure 8). For the same value of ε, every sensitive record in the data-
base has the same level of privacy under sensitive privacy as all the
records under differential privacy; thus the same level of accuracy
should be achievable under both the privacy notions. Here we see
again that datasets with larger sizes exhibit very small error.
To evaluate the performance over future queries, we picked
n records uniformly at random from the space of possible (val-
ues of) records for each dataset – n was set to be 20% of the size
of the dataset. Here too the SP-mechanism outperforms the DP-
mechanism significantly (Table 2). This is because most of the
randomly picked records are anomalous as per the (β, r )-anomaly,
which is due to the sparsity of the databases. This fact becomes very
clear when we compare the mean error over the random records to
the mean error over the anomalous records in the randomly picked
records (see the second and the last column of Table 2). Since the
probability of observing a mistake is extremely small (e.g., 1 in 1010
trials) , in Table 2, the mean is computed over the actual probability
of error of the mechanism instead of the estimated error.
We already saw that by increasing k we move the boundary be-
tween sensitive and non-sensitive records (Figure 3). So to observe
the effect of varying values of k on real world datasets, we car-
ried out experiments on the datasets with k = ⌊0.1β⌋, ⌊0.2β⌋, and
⌊0.3β⌋ – recall that a record is considered k-sensitive with respect
to a database if the record is normal or becomes normal under the
addition and (or) deletion of at most k records from the database.
Note that if k ≥ β + 1 then every record will be sensitive regardless
of the database. The results are provided in Figure 9. Here we con-
clude that even for the higher values of k SP-mechanism performs
reasonably well. Further, if the size of dataset is large enough, then
the loss in accuracy for most of the records is negligible.
We see that for Credit Fraud and APS Trucks datasets, differen-
tially private AIQ for some of the anomalous records give smaller
error. We explain this deviation using the Credit Fraud dataset as
an example. The above mentioned deviation in the error occurs
whenever the anomalous record is not unique (Figure 10a-b), which
is typically rare (Figure 10c). The reason DP-mechanism’s error
remains constant in most cases is that the anomalies lie in a very
sparse region of space and mostly do not have any duplicates (i.e.,
other records with the same value – xi ≈ 1).
Finally, to evaluate the overall performance of our SP-mechanism,
we computed precision, recall, and F1-score [1]. We also provide
a comparison with two different baseline mechanisms, B1, B2 in
addition to pareto optimal DP mechanism (see Table 3).
B1 and B2 are the best performing mechanisms (i.e., with the
highest F1-score) from two families of mechanisms. Each mech-
anism in each of the family is identified by a threshold t, where
0 ≤ t ≤ 1. Below, we describe the mechanisms from both the fam-
ilies for fixed ε, threshold t, record i ∈ X, and database x ∈ D.
The mechanism in the first family is given as B1,t (x ) = 1 if and
only if O(x ) + Lap(1/ε) > t × (||x||1 + Lap(1/ε)); here O(x ) gives
the number of anomalies in x and Lap(1/ε) is independent noise
from Laplace distribution of mean zero and scale 1/ε. The mech-
anism in the second family is given as B2,t (x ) = 1 if and only if
O(x ) + Lap(β/ε) > t × (||x||1 + Lap(1/ε)). Note that, the mechanism
from the first family are ε1-DP, where ε1 ≥ βε. This is due to the
fact that maxx,y∈D:||x−y||1=1 |O(x ) −O(y)| = β [15]. However the
mechanism from the second family are ε2-DP, where ε2 ≥ ε.
Our mechanism outperforms all the baselines. Furthermore, DP-
mechanism largely outperforms the rest of the baselines.
6 RELATED WORK
To our knowledge, there has been no work that formally explores
the privacy-utility trade-off in privately identifying anomalies,
where sensitive records (which include the normal records defined
in a data-dependent fashion) are protected against inference attacks
about their presence or absence in the database used.
Differential privacy [15, 17] has shaped the field of private data
analysis. This notion aims to protect everyone, and in a sense, many
of the DP mechanisms (e.g. Laplace mechanism) achieve privacy
by protecting anomalies; and in doing so perturb the information
regarding anomalies greatly. This adversely affects the accuracy
of anomaly detection and identification. Furthermore, differential
privacy is a special case of sensitive privacy (Section 4.1.1).
Variants of the notion of differential privacy address important
practical challenges. In particular, personalized differential privacy
[29], protected differential privacy [31], relaxed differential privacy
[6], and one-sided differential privacy [13] have a reversed order
of quantification compared to sensitive privacy. Sensitive privacy,
quantifies sensitive records and their privacy after quantifying the
database, which is in contrast to the previous work. Thus, under
sensitive privacy, it is possible for a record of some value to be
sensitive in one database and not in the other, while this cannot
be the case in the above mentioned definitions. On the other hand,
by labeling records independent to the database (as in the previous
work) one can solve a range of privacy problems such as counting
Session 3E: Privacy IICCS ’19, November 11–15, 2019, London, United Kingdom729queries and releasing histograms. Hence, this work solves the open
problem (in [31]). Next, we present an individual comparison with
each of the above mentioned previous work along with some other
relevant ones from the literature.
Protected differential privacy [31] proposes an algorithm for
social networks to search for anomalies that are fixed and are de-
fined independent of the database. This is not extensible to the case,
where anomalies are defined relative to the other records [31]. Sim-
ilarly, the proposed relaxed DP mechanism [6] is only applicable to
anomalies defined in data-independent manner.
One-sided differential privacy (OSDP) [13] is a general frame-
work, and is useful for the applications, where one can define the
records to be protected independent of the database. Note that the
notion of sensitive record in OSDP is different from the one con-
sidered here. Further, due to its asymmetric nature of the privacy
constrains, OSDP fails to protect against the inference about the
presence/absence of a sensitive record (in general), which is not
the case with sensitive privacy (see Appendix A.10.1).
Tailored differential privacy (TDP) [36] provides varying levels
of privacy for a record, which is given by a function, α, of the
record’s value and the database. However, the work is restricted
to releasing histograms, where outliers are provided more privacy.
Whereas our focus is identifying anomalies, where anomalies may
have lesser privacy. Further, the notion of anomaly used in the work
[36] is the simple (β, 0)-anomaly. Extending it to the case of r > 0 is
a non-trivial task since, here, changing a record in the database may
affect the label (outlyingness) of another record with a different
value. We also note that sensitive privacy is a specialized case of
tailored differential privacy (see Appendix A.10.2.)
Blowfish privacy (BP) [25] and Pufferfish privacy (PP) [33] are
general frameworks, and provide no concrete methodology or di-
rection to deal with anomaly detection or identification, where
anomalies are defined in a data-dependent fashion. Sensitive pri-
vacy is a specialized class of definitions under these frameworks.
Thus, in term of definition, our contribution in comparison with
OSDP [13], TDP [36], BP [25], and PP [33], is defining the the notion
of sensitive record and the sensitive neighborhood graph that is
appropriate and meaningful for anomalies (when defined relative
to the other records) and giving constructions and mechanisms for
identifying anomalies.
Finally, [4] proposed a method for searching outliers, which can
depend on data, but this is done in a rather restricted setting, which
has theoretical value (in [4] the input databases are guaranteed to
have only one outlier, a structure not present in the typical available
datasets; this is in addition to other input database restrictions
required by [4]).
Other relaxations of differential privacy such as [2] is specifically
for location privacy and [16] is to achieve fairness in classification
to prevent discrimination against individuals based on their mem-
bership in some group and as such are not applicable to the problem
we consider here.
7 KEY TAKEAWAYS AND CONCLUSION
This work is the first to lay out the foundations of the privacy-
preserving study of data dependent anomalies and develop general
constructions to achieve this. It is important to reiterate that the
formalization and conceptual development is independent of any
particular definition of anomaly. Indeed, the definition of sensitive
privacy (Definitions 3.1 and 3.2), and the constructions to achieve
it (Construction 1 and Construction 2) are general and work for an
arbitrary definition of anomaly (Theorem 4.1 and Theorem 4.7).
We noted earlier that sensitive privacy generalizes differential
privacy. Thus, the guarantees provided by sensitive privacy are sim-
ilar to that of differential privacy, and in fact, Construction 1 can be
employed to give differentially private mechanisms for computing
anomaly identification query or any binary function. However, in
general, the guarantee provided by sensitive privacy to any two
databases differing by one record could be correspondingly weaker
than that offered by differential privacy depending on the distance
between the databases in the sensitive neighborhood graph. There is
also a divergence in guarantees in terms of composition. In differen-
tial privacy, composition is only in terms of the privacy parameter,
ε. However, for sensitive privacy, composition needs to take into
account not only the privacy parameter ε, but also the sensitive
neighborhood graphs corresponding to the queries being composed.
Nevertheless, the composition and post-processing properties (Sec-
tion 3.1) hold regardless of the notion of the anomaly.
An extensive empirical study carried out over data from diverse
domains overwhelmingly supports the usefulness of our method.
The sensitively private mechanism consistently outperforms dif-
ferentially private mechanism with exponential gain in accuracy
in almost all cases. Although it is easy to come up with example
datasets where a differentially private mechanism also performs
well (e.g., (β, r )-AIQ for i and x when xi = Bx (i, r ) = β/2), the
experiments with real data show that such cases are unlikely to
occur in practice. Indeed, the experiments show that most of the
anomalies occur in the setting, where an ε-DP mechanism performs
the worst, that is, its error is close to 1/(1 + eε ) (a lower bound on
the error of any ε-DP mechanism and follows from Claim 1).
To conclude, in this paper, we develop methods for anomaly iden-
tification that provide a provable privacy guarantee to all records,
which is calibrated to their degree of being anomalous (in a data-
dependent sense), while enabling the accurate identification of
anomalies. We stress that the currently available methodologies for
protecting privacy in data analysis are fundamentally unsuitable
for the task at hand: they either fail to stop identity inference from
the data, or lack the ability to deal with the data-dependent defini-
tion of anomaly. Note that anomaly identification is only the first
step to tackling the problem of anomaly detection (finding all the
anomalous records in a dataset). In the future, we plan to tackle this