occasion.6
Here is the anonymous introduction protocol:
1. The owner collects a challenge from the certiﬁer, and
the user’s public key. He produces the following pro-
gram from ﬁgure 8 that is sent to the CPUF.
2. The owner decrypts the output from the CPUF, checks
the MAC, and passes Mesg5 on to the certiﬁer, along
with a copy of the program (only the part that partici-
pates in the MAC) encrypted with the certiﬁer’s public
key.
3. The certiﬁer decrypts the program, checks that it is the
ofﬁcial anonymous introduction program, then hashes
it to calculate CertSecret. He can then verify that
Mesg4 is authentic with the MAC. He ﬁnally signs
Mesg4, and sends the result to the owner.
4. The owner unblinds the message, and ends up with a
signed version of Mesg3. He can check the signature,
and the MAC in Mesg3 to make sure that the certiﬁer
isn’t communicating his identity to the user. He ﬁnally
sends the unblinded message to the user. This message
is in fact a version of Mesg3 signed by the certiﬁer.
5. The user checks the signature, and decrypts Mesg2
with his secret key to get a CRP.
Remarks:
• UserPubKey and CertChallenge must be en-
crypted, otherwise it is possible to correlate the mes-
sage that Alice sends to the CPUF with the certiﬁer’s
challenge or with the user’s public key.
6In this protocol, to avoid over-complication, we have assumed that
Alice does not need to know Bob’s public key in order to sign a message.
For real-world protocols such as the one that David Chaum describes in
[Cha85] this is not true. Therefore, an actual implementation of our anony-
mous introduction protocol might have to include the certiﬁer’s public key
in the program that is sent to the CPUF. In that case, it should be encrypted
to prevent correlation of messages going to the CPUF with a speciﬁc trans-
action with the certiﬁer.
/* Various values encrypted
with OwnerSecret. */
ESeed = ...
EPreChallengeSeed = ...
EUserPubKey = ...
ECertChallenge = ...
begin program
OwnerSecret = GetSecret(OwnerChallenge);
Seed = Decrypt(ESeed, OwnerSecret);
PreChallengeSeed =
Decrypt(EPreChallengeSeed, OwnerSecret);
UserPubKey =
Decrypt(EUserPubKey, OwnerSecret);
CertChallenge =
Decrypt(ECertChallenge, OwnerSecret);
CertSecret = GetSecret(CertChallenge);
PreChallenge =
Hash(UserPubKey, PreChallengeSeed);
NewChallenge = HashWithProg(PreChallenge);
ChangePersonality(Seed);
NewResponse = GetResponse(PreChallenge);
Mesg1 = (NewChallenge, NewResponse);
Mesg2 = PublicEncrypt(Mesg1, UserPubKey);
Mesg3 = (Mesg2, MAC(Mesg2, OwnerSecret));
Mesg4 = Blind(Mesg3, OwnerSecret);
Mesg5 = (Mesg4, MAC(Mesg4, CertSecret));
Mesg6 = EncryptAndMAC(Mesg5, OwnerSecret);
Output(Mesg6);
end program
Figure 8. The anonymous introduction pro-
gram.
• Seed must be encrypted to prevent the certiﬁer or the
user from knowing how to voluntarily get into the per-
sonality that the user is being shown.
• PreChallengeSeed must be encrypted to prevent
the certiﬁer from ﬁnding out the newly created chal-
lenge when he inspects the program in step 3.
• The encryption between Mesg5 and Mesg6 is needed
to prevent correlation of the message from the CPUF
to the owner and the message from the owner to the
certiﬁer.
Interestingly, we are not limited to one layer of encap-
sulation. A principal who has gained access to a personal-
ity of a CPUF through anonymous introduction can intro-
duce other parties to this PUF. In particular, he can send the
signed CRP that he received back to the certiﬁer and get
the certiﬁer to act as a certiﬁer for his personality when he
anonymously introduces the CPUF to other parties.
Proceedings of the 18th Annual Computer Security Applications Conference (ACSAC(cid:146)02) 
1063-9527/02 $17.00 ' 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:32:29 UTC from IEEE Xplore.  Restrictions apply. 
6. Applications
We believe there are many applications for which CPUFs
can be used, and we describe a few here. Other applications
can be imagined by studying the literature on secure copro-
cessors, in particular [Yee94]. We note that the general ap-
plications for which this technology can be used include all
the applications today in which there is a single symmetric
key on the chip.
6.1. Smartcard Authentication
The easiest application to implement is authentication.
One widespread application is smartcards. Current smart-
cards have hidden digital keys that can sometimes be ex-
tracted using many different kinds of attacks [And01]. With
a unique PUF on the smartcard that can be used to authen-
ticate the chip, a digital key is not required: the smartcard
hardware is itself the secret key. This key cannot be dupli-
cated, so a person can lose control of it, retrieve it, and con-
tinue using it. The smartcard can be turned off if the owner
thinks that it is permanently lost by getting the application
authority to forget what it knows of the secret signature that
is associated with the unique smartcard.
The following basic protocol is an outline of a protocol
that a bank could use to authenticate messages from PUF
smartcards. This protocol guarantees that the message the
bank receives originated from the smartcard. It does not,
however authenticate the bearer of the smartcard. Some
other means such as a PIN number or biometrics must be
used by the smartcard to determine if its bearer is allowed
to use it.
1. The bank sends the following program to the smart-
card, where R is a single use number and Challenge
is the bank’s challenge:
begin program
Secret = GetSecret(Challenge);
/* The smartcard somehow
*
* generates Message to send *
* to the bank
*/
Output(Message,
If the privacy of the smartcard’s message is a require-
ment, the bank can also encrypt the message with the same
key that is used for the MAC.
6.2. Certiﬁed execution
At present, computation power is a commodity that un-
dergoes massive waste. Most computer users only use a
fraction of their computer’s processing power, though they
use it in a bursty way, which justiﬁes the constant demand
for higher performance. A number of organizations, such
as SETI@home and distributed.net, are trying to tap that
wasted computing power to carry out large computations in
a highly distributed way. This style of computation is unre-
liable as the person requesting the computation has no way
of knowing that it was executed without any tampering.
With chip authentication, it would be possible for a cer-
tiﬁcate to be produced that proves that a speciﬁc computa-
tion was carried out on a speciﬁc chip. The person request-
ing the computation can then rely on the trustworthiness of
the chip manufacturer who can vouch that he produced the
chip, instead of relying on the owner of the chip.
There are two ways in which the system could be used.
Either the computation is done directly on the secure chip,
either it is done on a faster insecure chip that is being moni-
tored in a highly interactive way by supervisory code on the
secure chip.
To illustrate this application, we present a simple exam-
ple in which the computation is done directly on the chip. A
user, Alice, wants to run a computationally expensive pro-
gram over the weekend on Bob’s 128-bit, 300MHz, single-
tasking computer. Bob’s computer has a single chip, which
has a PUF. Alice has already established CRPs with the PUF
chip.
1. Alice sends the following program to the CPUF, where
Challenge is the challenge from her CRP:
begin program
Secret = GetSecret(Challenge);
/* The certified computation *
* is performed, the result *
* is placed in Result
*/
MAC((Message, R), Secret));
Output(Result,
end program
MAC(Result, Secret));
end program
2. The bank checks the MAC to verify the authenticity
and freshness of the message that it gets back from the
PUF.
2. The bank checks the MAC to verify the authenticity of
the message that it gets back from the PUF.
The number R is useful in the case where the smartcard
has state that is preserved between executions. In that case
it is important to ensure the freshness of the message.
Unlike the smartcard application, we did not include a
single use random number in this protocol. This is because
we are assuming that we are doing pure computation that
Proceedings of the 18th Annual Computer Security Applications Conference (ACSAC(cid:146)02) 
1063-9527/02 $17.00 ' 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:32:29 UTC from IEEE Xplore.  Restrictions apply. 
cannot become stale (any day we run the same computation
it will give the same result).
In this application, Alice is trusting that the chip in Bob’s
computer performs the computation correctly. This is eas-
ier to ensure if all the resources used to perform the com-
putation (memory, CPU, etc.) are on the PUF chip, and
included in the PUF characterization. We are currently re-
searching and designing more sophisticated architectures in
which the PUF chip can securely utilize off-chip resources
using some ideas from [Lie00] and a memory authentication
scheme that can be implemented in a hardware processor
[GSC+03].
There is also the possibility of a PUF chip using the ca-
pabilities of other networked PUF chips and devices using
certiﬁed executions. The PUF would have CRPs for each of
the computers it would be using, and perform computations
using protocols similar to the one described in this section.
6.3. Software licensing
We are exploring ways in which a piece of code could
be made to run only on a chip that has a speciﬁc identity
deﬁned by a PUF. In this way, pirated code would fail to
run. One method that we are considering is to encrypt the
code using the PUF’s responses on an instruction per in-
struction basis. The instructions would be decrypted inside
of the PUF chip, and could only be decrypted by the in-
tended chip. As the operating system and off-chip storage is
untrustworthy, special architectural support will be needed
to protect the intellectual property as in [Lie00].
7. Conclusion
In this paper we have introduced the notion of Controlled
Physical Random Functions (CPUFs) and shown how they
can be used to establish a shared secret with a speciﬁc phys-
ical device. The proposed infrastructure is ﬂexible enough
to allow multiple mutually mistrusting parties to securely
use the same device. Moreover, provisions have been made
to preserve the privacy of the device’s owner by allowing
her to show apparently different PUFs at different times.
We have also described two examples of how CPUFs can
be applied. They hold promise in creating smartcards with
an unprecedented level of security. They also enable these
smartcards or other processors to run user programs in a
secure manner, producing a certiﬁcate that gives the user
conﬁdence in the results generated. While we have not de-
scribed software licensing and intellectual property protec-
tion applications in this paper, the protocols for these appli-
cations will have some similarity to those described herein,
and are a subject of ongoing work.
References
[And01]
[Cha85]
Ross J. Anderson. Security Engineering: A
Guide to Building Dependable Distributed
Systems. John Wiley and Sons, 2001.
David Chaum. Security without identiﬁca-
tion: Transaction systems to make big brother
obsolete.
the ACM,
28:1030–1040, 1985.
Communications of
[GCvDD02] Blaise Gassend, Dwaine Clarke, Marten van
Dijk, and Srinivas Devadas. Silicon physical
random functions. In Proceedings of the 9th
ACM Conference on Computer and Commu-
nications Security, November 2002.
[GSC+03] Blaise Gassend, G. Edward Suh, Dwaine
Clarke, Marten van Dijk, and Srinivas De-
vadas. Caches and merkle trees for efﬁcient
memory authentication.
In Proceedings of
the 9th International Symposium on High-
Performance Computer Architecture, Febru-
ary 2003.
[Lie00]
David Lie et al. Architectural Support for
Copy and Tamper Resistant Software. In Pro-
ceedings of the 9th International Conference
on Architectural Support for Programming
Languages and Operating Systems (ASPLOS-
IX), pages 169–177, November 2000.
[MvOV96] Alfred J. Menezes, Paul C. van Oorschot, and
Scott A. Vanstone. Handbook of Applied
Cryptography. CRC Press, 1996.
[Rav01]
[Sch96]
[SW99]
P. S. Ravikanth. Physical One-Way Functions.
PhD thesis, Massachusetts Institute of Tech-
nology, 2001.
Bruce Schneier. Applied Cryptography. Wi-
ley, 1996.
S. W. Smith and S. H. Weingart. Building
a High-Performance, Programmable Secure
Coprocessor. In Computer Networks (Special
Issue on Computer Network Security), vol-
ume 31, pages 831–860, April 1999.
[Yee94]
Bennet S. Yee. Using Secure Coproces-
sors. PhD thesis, Carnegie Mellon University,
1994.
Proceedings of the 18th Annual Computer Security Applications Conference (ACSAC(cid:146)02) 
1063-9527/02 $17.00 ' 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:32:29 UTC from IEEE Xplore.  Restrictions apply.