Asura: A huge PCAP ﬁle analyzer for anomaly
packets detection using massive multithreading
Ruo Ando
National Institute of Informatics
DEF CON 26
Abstract—Recently, the inspection of huge trafﬁc log is im-
posing a great burden on security analysts. Unfortunately, there
have been few research efforts focusing on scalability in analyzing
very large PCAP ﬁle with reasonable computing resources. Asura
is a portable and scalable PCAP ﬁle analyzer for detecting
anomaly packets using massive multithreading. Asura’s parallel
packet dump inspection is based on task-based decomposition
and therefore can handle massive threads for large PCAP ﬁle
without considering tidy parameter selection in adopting data
decomposition. Asura is designed to scale out in processing large
PCAP ﬁle by taking as many threads as possible.
Asura takes two steps. First, Asura extracts feature vector
represented by associative containers of 
pair. By doing this, the feature vector can be drastically small
compared with the size of original PCAP ﬁles. In other words,
Asura can reduce packet dump data into the size of unique
¡sourceIP, destIP¿ pairs (for example, in experiment, Asura’s
output which is reduced in ﬁrst step is about 2a parallel clustering
algorithm is applied for the feature vector which is represented as
, V [i] where V [i] is aggregated ﬂow vector.
In second step, Asura adopts an enhanced Kmeans algorithm.
Concretely, two functions of Kmeans which are (1)calculating
distance and (2)relabeling points are improved for parallel
processing.
In experiment, in processing public PCAP datasets, Asura
can detect 750 packets which are labeled as malicious from
among 70 million (about 18GB) normal packets. In a nutshell,
Asura successfully found 750 malicious packets in about 18GB
packet dump. For Asura to inspect 70 million packets, it took
reasonable computing time of around 350-450 minutes with 1000-
5000 multithreading by running commodity workstation. Asura
will be released under MIT license and available at author’s
GitHub site on the ﬁrst day of DEF CON 26.
I. INTRODUCTION
The scale of network trafﬁc are growing year by year.
Besides, as the emergence of sophisticated cyber attack such as
APT (Advanced Persistent Threat), we are forced to cope with
the detailed packet inspection as well as checking other trafﬁc
logs. Currently, the inspection of huge trafﬁc log is imposing
a great burden on security analysts.
Trafﬁc irregularities or trafﬁc anomalies is usually described
as the result of one or more occurrences which changes the
normal ﬂow of data. Anomaly detection is the process of
ﬁnding patterns in the audit network trafﬁc log which do not
conform to legitimate or normal behavior. The design of an
anomaly detection method relies on a baseline model which
represents the normal behavior of network trafﬁc. The model
is generally trained using audit data extracted from trafﬁc for
which all items are labeled in advance as either normal or
anomalous. However, labelling trafﬁc data is often expensive
and time-consuming partly because it requires human experts.
II. OVERVIEW
A. Task based decomposition
If we want to transform code into a concurrent version, there
are two ways. First one is data decomposition, in which the
program cope with a large collection of data and can compute
every chunk of the data independently. Second one is is task
decomposition, in which the process is partitioned into a set of
independent tasks that threads can execute in any order. Data
decomposition has some drawbacks. For example, the size of
PCAP ﬁle varies according to the situation in which the ﬁle is
dumped. Besides each divided process is NOT homogeneous.
Asura adopts task-based parallel processing which is shown
in Figure1. Asura’s parallel packet dump inspection is based
on task-based decomposition and therefore can handle massive
threads for large PCAP ﬁle without considering tidy parameter
selection in adopting data decomposition. Asura is designed
to scale out in processing large PCAP ﬁle by taking as many
threads as possible. Assume that we have n threads and each
thread and each thread is associated with one PCAP ﬁle. By
doing this, Asura can take advantages of dynamic scheduling
for coping with a variety of kinds and size of PCAP ﬁle. Asura
consists of two thread sets: a master thread which enqueues the
name of PCAP ﬁle and a worker thread which processes the
each PCAP ﬁle. The master thread enqueues the list of PCAP
ﬁle, and passes it to the worker thread. More speciﬁcally, the
master thread traverses PCAP ﬁle directory and enqueue the
ﬁle name. When the queue is full, the master thread waits until
the worker thread processing packets consumes a ﬁle name
and removes it from the queue. The worker (packet processing
thread) dequeues a ﬁle name and parses the PCAP ﬁle. The
task of the worker thread is ﬂow vector aggregation using
associative container discussed later in III-A.
B. Feature selection and structures
In general, most system of the identiﬁcation of anomalies
on trafﬁc volume is based on features. The features could be
many: source and destination addresses, port numbers, static
signatures, statistics signatures and so on. The difﬁculty of
the anomaly trafﬁc identiﬁcation is to handle the huge trafﬁc
volume. The challenging part here is to select which header
item are worth and suitable to be learned. For simplicity, we
pick up the ﬁve items as follows:
Fig. 1. Task based decomposition
1) Source and destination IP addresses: unique identiers of
sender and receiver.
2) The number of captured packets: the ﬁelds is reﬂected
by the trafﬁc volume.
3) TLEN (total length of packets): the 16-bit ﬁeld deﬁnes
the entire packet volume in bytes, including both header
and data.
4) TTL (The time-to-live): the time-to-live value can be
referred as an upper bound on the time which an IP
datagram can be processed.
5) source and destination port: port number is checked for
the type of application.
Structure is shown as follows. For each ﬁelds, the numerical
ﬁelds of packet header are reduced using associative container
of map . The key stores number of packets.
In other words,
1: /* STRUCTURE II: reduced */
2: typedef struct _reduced {
3: map count;
4: map tlen;
5: map ttl;
6: map sport;
7: map dport;
8: pthread_mutex_t mutex;
9: } reduced_t;
10: reduced_t reduced;
Another challenging part is to transform code into a con-
current version with massive multithreading. For reduction, we
use the structure as follows.
1: /* STRUCTURE I: srcIP, destIP */
2: typedef struct _addrpair {
3: map m;
4: pthread_mutex_t mutex;
5: } addrpair_t;
6: addrpair_t addrpair;
Here pthread mutex t is the notation for mutual execution.
Mutual exclusion locks (mutexes) prevent multiple threads
from simultaneously executing critical sections of code which
Fig. 2. Flow vector computation by multithreading
access shared data (that is, mutexes are used to serialize the
execution of threads). All mutexes must be global. Threads
within the same process or within other processes can share
mutexes. Mutexes can synchronize threads within the same
process or in other processes. Mutexes can be used to synchro-
nize threads between processes if the mutexes are allocated in
writable memory and shared among the cooperating processes
,and have been initialized for this task.
Algorithm 1 Flow vector aggregation 1
Input: file{, V }
Output: Unique pair of , v[i]
1: while for i in each thread do
2:
lock(map )
3:
local map = map 
4:
unlock(map )
5:
while !end of file[i] do
6:
! = EMPTY do
9:
if ! = local map  then
10:
local map.insert()
11:
lock(map )
12:
map = localmap 
13:
unlock(map )
14:
i + +
15:
end if
16:
end while
17:
end while
18: end while
III. PROPOSAL METHOD
A. Extracting feature vector using associative container
Associative containers are a generic group of class templates
in the standard library of the C++ programming language
which cope with ordered associative arrays. The containers
are implemented in the current revision of the C++ standard:
set, map, multiset, multimap.
1) Key uniqueness: in map and set each key must be
unique.
Algorithm 2 Flow vector aggregation 2
Input: R1{, V }, R2{counter, sum}
Output: , V
1: while for i in each thread do
2:
while !end of file do
3:
{, v} ! = R1 local  then
7:
R1 local.  + = v
8:
lock(R1{, V })
9:
R1{, V } = R1 local
10:
unlock(R1{, V })
11:
end if
12:
i + +
13:
end for
14:
end while
15: end while
2) Element composition: in map and multimap each ele-
ment should be composed from a key and a mapped
value.
3) Element ordering: elements must follow a strict weak
ordering
Proposal method is divided into two steps. Two pseudocodes
are shown in the Algorithm 1 and 2. The core idea of our
algorithm is to divide data into chunks which includes the pair
. The implementation is based on associative
container which stores the unique pair of .
This procedure is shown in the pseudocode of Algorithm 1.
The second nested loop retrieves the pair  from
ﬁle stream at line 6. In the third nested loop, the program
traverse local map ¡S, D¿ for looking V[i] corresponding to
 The ﬁnal output of Algorithm 1 is the unique pair
of 
The purpose of Algorithm 2 is the contraction. The most
important line is 7. At line 7, edges between src and dest
are contracted by adding V[i]. More precisely, the edge
 and  are reduced
by adding V[i] to V. After ﬁguring out , V ,
contracted edges V are stored in global variable with the index
of . The second nested loop beginning at line 2
retrieves {, v} where s, d is single point with inter-
mediate points s. The third nested loop starting at line 5 ﬁnds
the corresponding {, v[j]}in , v[i]
at line 6. Similar to previous steps, {, V } is locked.
The ﬁnal output is , V .
B. Parallel Kmeans
The general design of Kmeans is that it iteratively partitions
a given dataset into k clusters. At ﬁrst, it choices k data point
as the initial centroids. It could be the ﬁrst data points or a
set of randomly selected k data points in the data set. We are
given an integer k and a set of n data points x 2 Rd. We
which to choose k centers C so as to minimize the potential
function,
φ =
n
X
x2D
minc2C k x − c k2
A clustering each centroid implicitly requires choosing these
centroids. Here we set one cluster to be the set of data points
which are closer to the center than to any therefore. It is known
that ﬁnding exact solution to the Kmeans problem is NP-hard.
For implementing our method, Kmeans are divided into four
steps: (1) initialization, (2) distance calculation, (3) relabeling
points, (4) centroid recalculation and (5)coverage condition.
Here the step of (3) relabeling points is explicitly appended.
The core idea of our algorithm is based data decomposition
which divide the dataset into j chunks. R = 1, ...j. We can
parallelize steps of (1) - (4). The pseudocode our our algorithm
is shown in Algorithm 3. Line 3 - 11 represents the step of
distance calculation of {, D} where R is data points,
S is cluster set, and D is the distance between R and S. The
table of distance is manipulated on shared memory, therefore
line 6-8 lock/unlock {, D} deﬁned as global variable.
The second step of relabeling clusters which ﬁnds the
minimum distance between R and S is depicted in line 14-26.
Precisely, this step aims to discover the unique pair of 
with minimum distance d. Aster calculating the minimum
distance, this pair  is stored in {, R}.
The third parallelized step of (4) is represented in line 27-
31. In this step, we have already relabeled the points R to
cluster S. Hence line 29 can be parallelized.
IV. EXPERIMENTAL RESULT
In experiment, in processing public PCAP datasets, Asura
can identiﬁed 750 packets which are labeled as malicious
from among 70 million (about 18GB) normal packets. In a
nutshell, Asura successfully found 750 malicious packets in
about 18GB packet dump. For Asura to inspect 70 million
packets, it took reasonable computing time of around 350-450
minutes with 1000-5000 multithreading by running commod-
ity workstation.
V. CONCLUSION
Recently, the inspection of huge trafﬁc log is imposing
a great burden on security analysts. Unfortunately, there
have been few research efforts focusing on scalability in
analyzing very large PCAP ﬁle with reasonable computing
resources. Asura is full-scratch (and painful) implementation
with POSIX Pthreads and C++ STL. PCAP ﬁles are NOT
organized in a regular pattern or the parsing of PCAP ﬁles
is different or unpredictable for each element in the stream.
Leveraging Pthreads which represents assembly language in
parallelism provides maximum ﬂexibility. Asura adopts task-
based implementation for processing huge, heterogeneous and
unpredictable PCAP ﬁle stream.
Currently, Asura has thousands lines of code. can process
about 75,000,000 - 80,000,000 packets in about 250-500
minutes. Drawbacks of adopting Pthreads are caused by lock
Algorithm 3 Parallel Kmeans
Input: 
Output: {, R}, {, D}
1: i = 0
2: for each threads j do
3:
for each r in R[j] do
4:
for each s in S do
5:
d[i] = distance(r, s)
6:
lock({, D})
7:
store , d[i]
8:
unlock({, D})
9:
i + +
10:
end for
11:
end for
12: end for
13: i = 0
14: for each threads j do
15:
for each r in R[j] do
16:
for each s in S do
17:
if d[i](r, s) , R})
23:
store , r
24:
unlock({, R})
25:
end for
26: end for
27: for each threads j do
28:
for each s in S do
29:
recalculate new centroid with {, R[j]}
30:
end for
31: end for
connection and context switch. For future work, other parallel
processing libraries will be promising technology for the
speedup of current Asura.
REFERENCES
[1] Linux Programmer’s Manual PTHREADS(7), http://man7.org/linux/man-
pages/man7/pthreads.7.html
[2] Threading Building Blocks, https://www.threadingbuildingblocks.org/
[3] Thrust, https://developer.nvidia.com/thrust
[4] Asura, https://github.com/RuoAndo/Asura