analyses. The Alexa list gives a solid choice of functional websites
frequently visited by users, and may be a good choice for a human
web-centered study. Through its link-counting, the Majestic list
also includes “hidden” links, and may include domains frequently
loaded, but not necessarily knowingly requested by humans. To
obtain a reasonably general picture of the Internet, we recommend
to scan a large sample, such as the “general population” used in §8,
i.e., the set of all com/net/org domains.
Consider Stability: With lists changing up to 50% per day,
insights from measurement results might not even generalise to
the next day. For most measurement studies, stability should be
increased by conducting repeated, longitudinal measurements. This
also helps to avoid bias from weekday vs. weekend lists.
Document List and Measurement Details: Studies should
note the precise list (e.g., Alexa Global Top 1M), its download date,
and the measurements date to enable basic replicability. Ideally, the
list used should be shared in a paper’s dataset.
9.2 Desired Properties for Top Lists
Based on the challenges discussed in this work, we derive various
properties that top lists should offer:
Consistency: The characteristic, mainly structure and stability,
of top lists should be kept static over time. Where changes are
Figure 8: HTTP/2 adoption over time for the Top 1k and Top 1M
lists and com/net/org domains.
We try to fetch the domains’ landing page via HTTP/2 by using
the nghttp2 library. We again www-prefix all domains in Alexa and
Majestic. In case of a successfully established HTTP/2 connection,
we issue a GET request for the / page of the domain. We follow up
to 10 redirects and if actual data for the landing page is transferred
via HTTP/2, we count the domain as HTTP/2-enabled. We probe
top lists on a daily basis and the larger zone file on a weekly basis.
We show HTTP/2 adoption in Figure 8. First, we observe that the
HTTP/2 adoption of all com/net/org domains is 7.84% on average
and thus significantly lower than for domains listed in Top 1M lists,
(up to 26.6% for Alexa) and even more so for Top 1k lists, which
show adoption around 35% or more.
One explanation is that, as shown above, popular domains are
more likely hosted on progressive infrastructures (e.g., CDNs) than
the general population.
We next investigate HTTP/2 adoption between top lists based
on Figure 8. Unsurprisingly, we observe HTTP/2 adoption differs
by list and by weekday for those lists with a weekday pattern (cf.,
§6.2). We also note the extremely different result when querying
the Top 1k lists as compared to the general population.
8.4 Takeaway
We have analysed the properties of top lists and the general popula-
tion across many layers, and found that top lists (i) generally show
significantly more extreme measurement results, e.g., protocol adop-
tion. This effect is pronounced to up to 2 orders of magnitude for
the Top 1k domains. Results can (ii) be affected by a weekly pattern,
e.g., the % of protocol adoption may yield a different result when
using a list generated on a weekend as compared to a weekday.
This is a significant limitation to be kept in mind when using top
lists for measurement studies.
9 DISCUSSION
We have shown in §3 that top lists are being frequently used in
scientific studies. We acknowledge that using top lists has distinct
advantages—they provide a set of relevant domains at a small and
stable size that can be compared over time. However, the use of
top lists also comes with certain disadvantages, which we have
explored in this paper.
First, while it is the stated purpose of a top list to provide a
sample biased towards the list’s specific measure of popularity,
these samples do not represent the general state in the Internet well:
2018-04-112018-04-142018-04-172018-04-202018-04-232018-04-262018-04-292018-05-022018-05-052018-05-080102030405060Share[%]Alexa1MAlexa1kUmbrella1MUmbrella1kMajestic1MMajestic1kc/n/oSignificance, Structure, and Stability of Internet Top Lists
IMC ’18, October 31-November 2, 2018, Boston, MA, USA
required due to the evolving nature of the Internet, these should be
announced and documented.
Transparency: Top list providers should be transparent about
their ranking process and biases to help researchers understand
and potentially control those biases. This may, of course, contradict
the business interests of commercial list providers.
Stability: List stability faces a difficult trade-off: While capturing
the ever-evolving trends in the Internet requires recent data, many
typical top list uses are not stable to changes of up to 50% per day.
We hence suggest that lists should be offered as long-term (e.g., a
90-day sliding window) and short-term (e.g., only the most recent
data) versions.
9.3 Ethical Considerations
We aim to minimise harm to all stakeholders possibly affected by
our work. For active scans, we minimise interference by following
best scanning practices [132], such as maintaining a blacklist, us-
ing dedicated servers with meaningful rDNS records, websites, and
abuse contacts. We assess whether data collection can harm individ-
uals and follow the beneficence principle as proposed by [133, 134].
Regarding list influencing in §7, the ethical implications of insert-
ing a test domain into the Top 1M domains is small and unlikely to
cause any harm. In order to influence Umbrella ranks, we generated
DNS traffic. For this, we selected query volumes unlikely to cause
problems with the OpenDNS infrastructure or the RIPE Atlas plat-
form. Regarding the RIPE Atlas platform, we spread probes across
the measurements as carefully as possible: 10k probes queried spe-
cific domains 100, 50, 10, and 1 times per day. In addition, 100, 1000,
and 5000 probes performed an additional 100 queries per day. Per
probe, that means 6,100 probes generated 261 queries per day (fewer
than 11 queries per hour), and another 3,900 probes generated 161
queries per day. Refer to Figure 5 to visualise the query volume.
That implies a total workload of around 2,220,000 queries per day.
As the OpenDNS service is anycasted across multiple locations, it
seems unlikely that our workload was a problem for the service.
10 RELATED WORK
We consider our work to be related to three fields:
Sound Internet Measurements: There exists a canon of work
with guidelines on sound Internet measurements, such as [132, 135–
137]. These set out useful guidelines for measurements in general,
but do not specifically tackle the issue of top lists.
Measuring Web Popularity: Understanding web popularity
is important for marketing as well as for business performance
analyses. A book authored by Croll and Power [138] warns site
owners about the potential instrumentation biases present in Alexa
ranks, specially with low-traffic sites. Besides that, there is a set
of blog posts and articles from the SEO space about anecdotal
problems with certain top lists, but none of these conduct systematic
analyses [4, 139].
Limitations of Using Top Lists in Research: Despite the fact
that top lists are widely used by research papers, we are not aware
of any study focusing on the content of popular lists. However, a
number of research papers mentioned the limitations of relying on
those ranks for their specific research efforts [45, 67]. Wählisch et
al. [90] discuss the challenges of using top lists for web measure-
ments. They demonstrate that results vary when including www
subdomains, and investigate root causes such as routing failures.
The aforementioned recent work by le Pochat et al. [114] focuses
on manipulating top lists.
11 CONCLUSION
To the best of our knowledge, this is the first comprehensive study of
the structure, stability, and significance of popular Internet top lists.
We have shown that use of top lists is significant among networking
papers, and found distinctive structural characteristics per list. List
stability has revealed interesting highlights, such as up to 50%
churn per day for some lists. We have closely investigated ranking
mechanisms of lists and manipulated a test domain’s Umbrella rank
in a controlled experiment. Systematic measurement of top list
domain characteristics and reproduction of studies has revealed
that top lists in general significantly distort results from the general
population, and that results can depend on the day of week. We
closed our work with a discussion on desirable properties of top
lists and recommendations for top list use in science. We share code,
data, and additional insights under
https://toplists.github.io
For long-term access, we provide an archival mirror at the TUM
University Library: https://mediatum.ub.tum.de/1452290.
Acknowledgements: We thank the scientific community for
the engaging discussions and data sharing leading to this publica-
tion, specifically Johanna Amann, Mark Allman, Matthias Wählisch,
Ralph Holz, Georg Carle, Victor le Pochat, and the PAM’18 poster
session participants. We thank the anonymous reviewers of the
IMC’18 main and shadow PCs for their comments, and Zakir Du-
rumeric for shepherding this work. This work was partially funded
by the German Federal Ministry of Education and Research under
project X-Check (grant 16KIS0530), by the DFG as part of the CRC
1053 MAKI, and the US National Science Foundation under grant
number CNS-1564329.
REFERENCES
[1] Alexa. Top 1M sites. https://www.alexa.com/topsites, May 24, 2018. http:
//s3.dualstack.us-east-1.amazonaws.com/alexa-static/top-1m.csv.zip.
[2] Cisco. Umbrella Top 1M List. https://umbrella.cisco.com/blog/blog/2016/12/14/
cisco-umbrella-1-million/.
[3] Majestic. https://majestic.com/reports/majestic-million/, May 17, 2018.
[4] Matthew Woodward. Ahrefs vs Majestic SEO – 1 Million Reasons Why Ahrefs
Is Better. https://www.matthewwoodward.co.uk/experiments/ahrefs-majestic-
seo-1-million-domain-showdown/, May 23, 2018.
[5] Alexa. The Alexa Extension. https://web.archive.org/web/20160604100555/http:
//www.alexa.com/toolbar, June 04, 2016.
[6] Alexa. Alexa Increases its Global Traffic Panel. https://blog.alexa.com/alexa-
panel-increase/, May 17, 2018.
[7] Alexa. Top 6 Myths about the Alexa Traffic Rank. https://blog.alexa.com/top-6-
myths-about-the-alexa-traffic-rank/, May 22, 2018.
[8] Alexa. What’s going on with my Alexa Rank? https://support.alexa.com/hc/en-
us/articles/200449614, May 17, 2018.
[9] Majestic. Majestic Million CSV now free for all, daily. https://blog.majestic.
com/development/majestic-million-csv-daily/, May 17, 2018.
[10] Quantcast. https://www.quantcast.com/top-sites/US/1.
[11] Statvoo. https://statvoo.com/top/sites, May 17, 2018.
[12] Google. Chrome User Experience Report. https://developers.google.com/web/
tools/chrome-user-experience-report/, May 15, 2018.
[13] SimilarWeb Top Websites Ranking. https://www.similarweb.com/top-websites.
[14] Vasileios Giotsas, Philipp Richter, Georgios Smaragdakis, Anja Feldmann,
Christoph Dietzel, and Arthur Berger.
Inferring BGP Blackholing Activity
in the Internet. In Proceedings of the 2017 Internet Measurement Conference, IMC
’17, November 2017.
IMC ’18, October 31-November 2, 2018, Boston, MA, USA
Scheitle et al.
[15] Srikanth Sundaresan, Xiaohong Deng, Yun Feng, Danny Lee, and Amogh Dhamd-
here. Challenges in Inferring Internet Congestion Using Throughput Measure-
ments. In Proceedings of the 2017 Internet Measurement Conference, IMC ’17,
November 2017.
[16] Zhongjie Wang, Yue Cao, Zhiyun Qian, Chengyu Song, and Srikanth V. Krish-
namurthy. Your State is Not Mine: A Closer Look at Evading Stateful Internet
Censorship. In Proceedings of the 2017 Internet Measurement Conference, IMC
’17, November 2017.
[17] Savvas Zannettou, Tristan Caulfield, Emiliano De Cristofaro, Nicolas Kourtelris,
Ilias Leontiadis, Michael Sirivianos, Gianluca Stringhini, and Jeremy Blackburn.
The Web Centipede: Understanding How Web Communities Influence Each
Other Through the Lens of Mainstream and Alternative News Sources.
In
Proceedings of the 2017 Internet Measurement Conference, IMC ’17, November
2017.
[18] Austin Murdock, Frank Li, Paul Bramsen, Zakir Durumeric, and Vern Paxson.
Target Generation for Internet-wide IPv6 Scanning. In Proceedings of the 2017
Internet Measurement Conference, IMC ’17, November 2017.
[19] Jan Rüth, Christian Bormann, and Oliver Hohlfeld. Large-scale Scanning of
TCP’s Initial Window. In Proceedings of the 2017 Internet Measurement Confer-
ence, IMC ’17, November 2017.
[20] Umar Iqbal, Zubair Shafiq, and Zhiyun Qian. The Ad Wars: Retrospective
Measurement and Analysis of Anti-adblock Filter Lists. In Proceedings of the
2017 Internet Measurement Conference, IMC ’17, November 2017.
[21] Johanna Amann, Oliver Gasser, Quirin Scheitle, Lexi Brent, Georg Carle, and
In
Ralph Holz. Mission Accomplished?: HTTPS Security After Diginotar.
Proceedings of the 2017 Internet Measurement Conference, IMC ’17, November
2017.
[22] Joe DeBlasio, Stefan Savage, Geoffrey M. Voelker, and Alex C. Snoeren. Trip-
wire: Inferring Internet Site Compromise. In Proceedings of the 2017 Internet
Measurement Conference, IMC ’17, November 2017.
[23] Shehroze Farooqi, Fareed Zaffar, Nektarios Leontiadis, and Zubair Shafiq. Mea-
suring and Mitigating Oauth Access Token Abuse by Collusion Networks. In
Proceedings of the 2017 Internet Measurement Conference, IMC ’17, November
2017.
[24] Janos Szurdi and Nicolas Christin. Email Typosquatting. In Proceedings of the
2017 Internet Measurement Conference, IMC ’17, November 2017.
[25] Enrico Bocchi, Luca De Cicco, Marco Mellia, and Dario Rossi. The Web, the
Users, and the MOS: Influence of HTTP/2 on User Experience. In International
Conference on Passive and Active Network Measurement, pages 47–59. Springer,
2017.
[26] Ilker Nadi Bozkurt, Anthony Aguirre, Balakrishnan Chandrasekaran, P Brighten
Godfrey, Gregory Laughlin, Bruce Maggs, and Ankit Singla. Why is the Internet
so Slow?! In International Conference on Passive and Active Network Measurement,
pages 173–187. Springer, 2017.
[27] Stephen Ludin. Measuring What is Not Ours: A Tale of 3rd Party Performance.
In Passive and Active Measurement: 18th International Conference, PAM 2017,
Sydney, NSW, Australia, March 30-31, 2017, Proceedings, volume 10176, page 142.
Springer, 2017.
[28] Kittipat Apicharttrisorn, Ahmed Osama Fathy Atya, Jiasi Chen, Karthikeyan
Sundaresan, and Srikanth V Krishnamurthy. Enhancing WiFi Throughput with
PLC Extenders: A Measurement Study. In International Conference on Passive
and Active Network Measurement, pages 257–269. Springer, 2017.
[29] Alexander Darer, Oliver Farnan, and Joss Wright. FilteredWeb: A framework
for the Automated Search-based Discovery of Blocked URLs. In Network Traffic
Measurement and Analysis Conference (TMA), 2017, pages 1–9. IEEE, 2017.
[30] Jelena Mirkovic, Genevieve Bartlett, John Heidemann, Hao Shi, and Xiyue Deng.
Do You See Me Now? Sparsity in Passive Observations of Address Liveness. In
Network Traffic Measurement and Analysis Conference (TMA), 2017, pages 1–9.
IEEE, 2017.
[31] Quirin Scheitle, Oliver Gasser, Minoo Rouhi, and Georg Carle. Large-scale
Classification of IPv6-IPv4 Siblings with Variable Clock Skew. In Network Traffic
Measurement and Analysis Conference (TMA), 2017, pages 1–9. IEEE, 2017.
[32] Paul Pearce, Ben Jones, Frank Li, Roya Ensafi, Nick Feamster, Nick Weaver, and
Vern Paxson. Global Measurement of DNS Manipulation. In Proceedings of the
26th USENIX Security Symposium (USENIX Security ’17), August 2017.
[33] Rachee Singh, Rishab Nithyanand, Sadia Afroz, Paul Pearce, Michael Carl
Tschantz, Phillipa Gill, and Vern Paxson. Characterizing the Nature and Dynam-
ics of Tor Exit Blocking. In Proceedings of the 26th USENIX Security Symposium
(USENIX Security ’17), August 2017.
[34] Tao Wang and Ian Goldberg. Walkie-Talkie: An Efficient Defense Against
In Proceedings of the 26th USENIX
Passive Website Fingerprinting Attacks.
Security Symposium (USENIX Security ’17), August 2017.
[35] Sebastian Zimmeck, Jie S Li, Hyungtae Kim, Steven M Bellovin, and Tony Jebara.
A Privacy Analysis of Cross-device Tracking. In Proceedings of the 26th USENIX
Security Symposium (USENIX Security ’17), August 2017.
[36] Taejoong Chung, Roland van Rijswijk-Deij, Balakrishnan Chandrasekaran,
David Choffnes, Dave Levin, Bruce M Maggs, Alan Mislove, and Christo Wilson.
A Longitudinal, End-to-End View of the DNSSEC Ecosystem. In Proceedings of
the 26th USENIX Security Symposium (USENIX Security ’17), August 2017.
[37] Katharina Krombholz, Wilfried Mayer, Martin Schmiedecker, and Edgar Weippl.
"I Have No Idea What I’m Doing" – On the Usability of Deploying HTTPS.
In Proceedings of the 26th USENIX Security Symposium (USENIX Security ’17),
August 2017.
[38] Adrienne Porter Felt, Richard Barnes, April King, Chris Palmer, Chris Bentzel,
and Parisa Tabriz. Measuring HTTPS Adoption on the Web. In Proceedings of
the 26th USENIX Security Symposium (USENIX Security ’17), August 2017.
[39] Ben Stock, Martin Johns, Marius Steffens, and Michael Backes. How the Web
Tangled Itself:Uncovering the History of Client-Side Web (In)Security. In Pro-
ceedings of the 26th USENIX Security Symposium (USENIX Security ’17), August
2017.
[40] Pepe Vila and Boris Köpf. Loophole: Timing Attacks on Shared Event Loops
in Chrome. In Proceedings of the 26th USENIX Security Symposium (USENIX
Security ’17), August 2017.
[41] Jörg Schwenk, Marcus Niemietz, and Christian Mainka. Same-Origin Policy:
In Proceedings of the 26th USENIX Security
Evaluation in Modern Browser.
Symposium (USENIX Security ’17), August 2017.
[42] Stefano Calzavara, Alvise Rabitti, and Michele Bugliesi. CCSP: Controlled
Relaxation of Content Security Policies by Runtime Policy Composition. In
Proceedings of the 26th USENIX Security Symposium (USENIX Security ’17), August
2017.
[43] Fang Liu, Chun Wang, Andres Pico, Danfeng Yao, and Gang Wang. Measuring
In Proceedings of the 26th
the Insecurity of Mobile Deep Links of Android.
USENIX Security Symposium (USENIX Security ’17), August 2017.
[44] Paul Pearce, Roya Ensafi, Frank Li, Nick Feamster, and Vern Paxson. Augur:
Internet-Wide Detection of Connectivity Disruptions. In IEEE Symposium on
Security and Privacy, 2017.
[45] Sumayah Alrwais, Xiaojing Liao, Xianghang Mi, Peng Wang, Xiaofeng Wang,
Feng Qian, Raheem Beyah, and Damon McCoy. Under the Shadow of Sun-