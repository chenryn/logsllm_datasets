### k-th Message Batch and Agreement

For the k-th message batch, two conditions must hold: 
1. Every correct process also delivers it.
2. The k-th message batch is the same at every process.

From these two conditions, we can easily deduce the properties of **Agreement** and **Total Order**. However, **Validity** requires a more detailed proof.

### Lemma 5
For all \( k > 0 \), for every process \( p \) and every correct process \( q \), if \( p \) executes round \( k \) until the end, then \( q \) also executes round \( k \) until the end, and \( \text{adeliver}_k^p = \text{adeliver}_k^q \).

#### Proof
We will prove this lemma by induction over \( k \).

**Base Case (k = 1):**
- It is straightforward to see that every correct process executes round 1 until the end.
- Due to consensus agreement, if \( p \) a-delivers messages in round 1, then \( \text{adeliver}_1^p = \text{adeliver}_1^q \).

**Inductive Step:**
- Assume the lemma holds for all \( k \) such that \( 1 \leq k < r \).
- We need to show that if \( p \) a-delivers messages in round \( r \), then \( q \) executes round \( r \) until the end.

If \( p \) a-delivers messages in round \( r \), then \( p \) returns from the invocation of `Consensus(r, *)` at line 8. Since there is at most a minority of faulty processes, at least one correct process \( u \) executes `Consensus(r, *)`. This implies that \( u \) w-broadcasts its estimate at line 6.

By the induction hypothesis, if \( p \) a-delivers messages in round \( r - 1 \), then \( q \) executes round \( r - 1 \) until the end. Thus, \( q \) eventually w-delivers the first message of stage \( r \) either:
- At line 7, or
- At line 15.

Without loss of generality, let \( \text{estimate}_u \) be the first message w-delivered by \( q \) in round \( r \). In both cases, \( q \) breaks from the corresponding wait statement and executes `Consensus(r, \text{estimate}_u)`. By consensus termination, \( q \) eventually executes round \( r \) until the end.

In case \( q \) breaks from the second wait statement (line 15), it does not block at the first wait statement (line 7) because it has already w-delivered the first round \( r \) message.

Next, we show that if \( p \) a-delivers messages in round \( r \), then \( \text{adeliver}_r^p = \text{adeliver}_r^q \). As shown in the first part of the lemma, \( q \) executes round \( r \) until the end. Thus, \( q \) a-delivers messages in \( \text{adeliver}_r^q \). Due to consensus agreement, \( \text{msgSet}_r^p = \text{msgSet}_r^q \). By the induction hypothesis, for all \( k \) such that \( 1 \leq k < r \), \( \text{adeliver}_k^p = \text{adeliver}_k^q \). Therefore, \( \text{adeliver}_r^p = \text{msgSet}_r^p - \bigcup_{k=1}^{r-1} \text{adeliver}_k^p \) and \( \text{adeliver}_r^q = \text{msgSet}_r^q - \bigcup_{k=1}^{r-1} \text{adeliver}_k^q \). Hence, \( \text{adeliver}_r^p = \text{adeliver}_r^q \).

### Lemma 6 (Agreement)
If a process a-delivers message \( m \), then all correct processes eventually a-deliver \( m \).

#### Proof
This follows directly from Lemma 5.

### Lemma 7 (Total Order)
If some process a-delivers message \( m' \) after it a-delivers \( m \), then any process that a-delivers \( m' \) does so only after a-delivering \( m \).

#### Proof
This follows from Lemma 5, the total ordering of natural numbers, and the fact that messages within a batch are delivered atomically in a deterministic order.

### Lemma 8 (Validity)
If a correct process a-broadcasts message \( m \), then eventually it a-delivers \( m \).

#### Proof
We prove this by contradiction. Suppose a correct process \( p \) a-broadcasts \( m \) but never a-delivers \( m \). By Lemma 6, no correct process a-delivers \( m \). Consider a process \( p \) that a-broadcasts \( m \). Consequently, \( p \) includes \( m \) in \( \text{estimate}_p \) and thus w-broadcasts \( m \). By the validity property of the ordering oracle, every correct process eventually w-delivers \( m \) at line 16 and thus includes \( m \) in its estimate. Since no correct process a-delivers \( m \), no correct process removes \( m \) from its estimate at line 12.

There is a time \( t \) so that all faulty processes have crashed before \( t \) and at which \( m \) is included in the estimate of every correct process. Let \( k \) be the lowest round number after \( t \). Every correct process w-broadcasts \( m \) in round \( k \), which implies that every value proposed to the \( k \)-th consensus instance necessarily contains \( m \). Due to the validity of consensus, \( m \) is included in the `msgSet` of every correct process. Thus, \( m \) is a-delivered by every correct process at round \( k \); a contradiction.

### Performance Evaluation

In this section, we provide a brief analytical and experimental comparison to outline the efficiency of our protocols compared to Paxos and WABcast. Table 1 compares the proposed protocols with Paxos [13] and WABcast [19] in terms of time complexity (where \( \delta \) is the maximum network delay), message complexity, resilience, and the oracle used for termination.

| No Collisions | Collisions |
|---------------|------------|
| **Protocol**  | **Paxos**  | **WABCast**  | **L-/P-Cons.**  |
| **Latency**   | 3δ         | 2δ ; ∞        | 2δ ; 3δ          |
| **# Messages**| n² + n + 1 | n² + n ; ∞    | n² + n ; 2n² + n |
| **Resilience**| f < n/2    | f < n/3       | f < n/3          |
| **Oracle**    | Ω          | W AB         | Ω/(cid:14)P      |

Compared to Paxos, L-/P-Consensus trade the maximum degree of resilience, i.e., \( f < n/2 \) for the lower time complexity of 2δ. In periods with collisions, WABcast might not terminate, whereas L-/P-Consensus have the same time complexity as Paxos, though with more messages. We expect the proposed protocols to be as efficient in terms of latency as WABcast when collisions are rare and to exhibit behavior similar to Paxos when collisions are frequent.

### Experimental Evaluation

We compared the performance of the proposed protocols with Paxos and WABcast. We measured the latency of atomic broadcast as a function of the throughput, where latency is defined as the shortest delay between a-broadcasting a message \( m \) and a-delivering \( m \). We implemented L-/P-Consensus and C-Abcast using the Neko [21] framework. The experiments were conducted on a cluster of 4 identical workstations (2.8GHz, 512MB) interconnected by a 100Mb Ethernet LAN. Different consensus algorithms were tested by exchanging the consensus module of C-Abcast. The WAB oracle implementation uses UDP packets, while the rest of the communication is TCP-based. We considered only stable runs in our experiments. To capture the performance of the tested protocols during periods with and without collisions, we varied the throughput between 20 msg/s and 500 msg/s.

Figure 2 shows the comparison of our protocols with WABcast. Both proposed protocols exhibit a similar latency as WABcast up to a throughput of 80 msg/s and outperform WABcast for all throughputs higher than 100 msg/s. Figure 3 summarizes the comparison with Paxos. When collisions predominate, the proposed protocols indeed have the same time complexity as Paxos. However, given their decentralized nature, our protocols need more messages. From a throughput of 300 msg/s upwards, Paxos slightly outperforms both protocols. For lower throughputs, L-/P-Consensus perform better than Paxos.

### Conclusion

One-step decision and zero-degradation express the ability to reach consensus in one and two communication steps, respectively, and protocols that satisfy them are optimal in this respect. We investigated whether these properties are inherently incompatible and showed that they cannot be both satisfied using the Ω failure detector. As shown in [11], any Ω-based protocol that decides in two communication steps in every well-behaved run is also zero-degrading. This implies that the failure detector employed by Fast Paxos [13] is strictly stronger than Ω. Subsequently, we proposed two approaches to circumvent the established impossibility result. The first approach relaxes one-step decision to hold only in stable runs. The second approach assumes a strictly stronger failure detector. For each approach, we developed a corresponding consensus protocol. While the proposed L-Consensus ensures one-step decision only in stable runs, the ability of P-Consensus to decide in one communication step is regardless of the failure detector output. To test the efficiency of the proposed protocols, we modified the atomic broadcast algorithm of [19] to use consensus. We compared the proposed consensus protocols with Paxos and WABcast both analytically and experimentally. The results of the experiments confirm the analytical evaluation, establishing the efficiency of our proposed protocols.

### Acknowledgments
We gratefully acknowledge the help and insights from Dr. Falk Fraikin, the DEEDS group, the funding support from Microsoft Research via the European PhD Fellowship, and also from the EU DECOS and ReSIST projects.

### References
[1] M. K. Aguilera et al. Failure detection and consensus in the crash-recovery model. Dist. Computing, vol. 13, 2, pp. 99-125, 2000.
[2] F. V. Brasileiro et al. Consensus in one communication step. Proc. of PACT, pp. 42-50, 2001.
[3] L. Camargos et al. Optimal and practical WAB-based consensus algorithms. UNISI TR IC-05-07 Apr. 2005.
[4] T. D. Chandra et al. The weakest failure detector for solving consensus. JACM, vol. 43, 3, pp. 685-722, 1996.
[5] T. D. Chandra and S. Toueg. Unreliable failure detectors for reliable distributed systems. JACM, vol. 43, 2, pp. 225-267, 1996.
[6] F. Chu. Reducing Ω to 3W. Inf. Processing Letters, vol. 67, 6, pp. 289-293, 1998.
[7] F. Cristian and C. Fetzer. The timed asynchronous distributed system model. Proc. FTCS, pp. 140-149, 1998.
[8] C. Dwork et al. Consensus in the presence of partial synchrony. JACM, vol. 35, 2, pp. 288-323, 1988.
[9] P. Dutta and R. Guerraoui. Fast indulgent consensus with zero degradation. Proc. EDCC-4, pp. 191-208, 2002.
[10] M. J. Fischer et al. Impossibility of distributed consensus with one faulty process. JACM, vol. 32, 2, pp. 374-382, 1985.
[11] R. Guerraoui and M. Raynal. The information structure of indulgent consensus. IEEE Trans. Computers, vol. 53, 12, pp. 453-466, 2004.
[12] I. Keidar and S. Rajsbaum. On the cost of fault-tolerant consensus when there are no faults. ACM SIGACT News, Online Vol. 32, 2001.
[13] L. Lamport. The part-time parliament. ACM Trans. Computer Systems, vol. 16, 2, pp. 133-169, 1998.
[14] L. Lamport. Lower bounds for asynchronous consensus. Future Directions in Dist. Computing, 2004.
[15] L. Lamport. Fast Paxos. MSR TR 2005-112, July 2005.
[16] N. A. Lynch. Distributed Algorithms. Morgan Kaufmann Publishers, 1996.
[17] A. Mostéfaoui and M. Raynal. Low cost consensus-based atomic broadcast. Proc. PRDC, pp. 45-54, 2000.
[18] F. Pedone and A. Schiper. Optimistic atomic broadcast: A pragmatic viewpoint. Journal of Theoretical Computer Science, vol. 291, 1, pp. 79-101, 2003.
[19] F. Pedone et al. Solving agreement problems with weak ordering oracles. Proc. of EDCC, pp. 44-61, 2002.
[20] F. B. Schneider. Implementing fault-tolerant services using the state machine approach: A tutorial. ACM Computing Surveys, vol. 22, 4, pp. 299-319, 1990.
[21] P. Urbán et al. Neko: A single environment to simulate and prototype distributed algorithms. Proc. of Information Networking, pp. 503-511, 2001.
[22] P. Dutta et al. The Overhead of Consensus Recovery. IC TR 200456, June 2004.