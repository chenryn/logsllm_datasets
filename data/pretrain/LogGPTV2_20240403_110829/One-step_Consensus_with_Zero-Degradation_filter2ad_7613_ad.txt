k-th message batch, then every correct process also delivers
it and b) that the k-th message batch is the same at every
process. From a) and b) we can easily deduce Agreement
and Total Order. Validity requires a more detailed proof.
Lemma 5 For all k > 0, every process p and every correct
process q, if p executes round k until the end then q executes
round k until the end and adeliverk
p = adeliverk
q .
p = adeliver1
Proof : We will prove the lemma by induction over k. First,
it is easy to see that every correct process executes round 1
until the end. Due to consensus agreement, if p a-delivers
messages in round 1 then adeliver1
q. Now as-
sume that the lemma holds for all k, 1 ≤ k < r. We ﬁrst
show that if p a-delivers messages in round r then q executes
round r until the end.
If p a-delivers messages in round
r, then p returns from the invocation of Consensus(r,∗) at
line 8. Since there is at most a minority of faulty processes,
at least one correct process u executes Consensus(r,∗).
This implies that u w-broadcasts its estimate at line 6. By
the induction hypothesis, if p a-delivers messages in round
r − 1, q executes round r − 1 until the end. Thus, q eventu-
ally w-delivers the ﬁrst message of stage r either a) at line 7
or b) at line 15. Without loss of generality, let estimateu
be the ﬁrst message w-delivered by q in round r. In both
cases q breaks from the corresponding wait statement and
executes Consensus(r, estimateu)3. By consensus termi-
nation, q eventually executes round r until the end.
3In case q breaks from the second wait statement (line 15) it does not
block at the ﬁrst wait statement (line 7) because it has already w-delivered
the ﬁrst round r message.
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 04:20:34 UTC from IEEE Xplore.  Restrictions apply. 
p = adeliverr
We now show that if p a-delivers messages in round
r then adeliverr
q. As shown in the ﬁrst
part of the lemma, q executes round r until the end.
Thus, q a-delivers messages in adeliverr
q. Due to con-
p = msgSetr
sensus agreement msgSetr
q. By the in-
duction hypothesis, ∀k, 1 ≤ k < r : adeliverk
p =
p = ∪r−1
q . As
k=1adeliverk
adeliverk
adeliverr = msgSetr − ∪r−1
q
k=1adeliverk, we get
−
adeliverr
∪r−1
(cid:1)
k=1adeliverk
k=1adeliverk
q = adeliverr
q.
k=1adeliverk
− ∪r−1
p = msgSetr
q
p = msgSetr
p
⇒ ∪r−1
Lemma 6 (Agreement) If a process a-delivers message m,
then all correct processes eventually a-deliver m.
Proof : Follows directly from Lemma 5.
(cid:1)
Lemma 7 (Total Order) If some process a-delivers mes-
(cid:1) only
sage m
after it a-delivers m.
(cid:1) after message m, then a process a-delivers m
Proof : Follows from lemma 5, the total odering of natu-
ral numbers, and the fact that messages within a batch are
(cid:1)
delivered atomically in a deterministic order.
Lemma 8 (Validity) If a correct process a-broadcasts
message m, then eventually it a-delivers m.
Proof : The proof is by contradiction. Suppose that a cor-
rect process a-broadcasts m but never a-delivers m. By
Lemma 6 no correct process a-delivers m. Consider a pro-
cess p that a-broadcasts a message m. Consequently, p in-
cludes m in estimatep and thus w-broadcasts m. By the
validity property of the ordering oracle, every correct pro-
cess eventually w-delivers m at line 16 and thus includes
m in its estimate. Since no correct process adelivers m,
no correct process removes m from its estimate at line 12.
There is a time t so that all faulty processes have crashed
before t and at which m is included in the estimate of
every correct process. Let k be the lowest round number
after t. Every correct process w-broadcasts m in k, which
implies that every value proposed to the k-th consensus in-
stance necessarily contains m. Due to validity of consen-
sus, m is included in the msgSet of every correct process.
Thus, m is a-delivered by every correct process at round k;
(cid:1)
a contradiction.
8 Performance Evaluation
In this section we provide a brief comparison both ana-
lytical and experimental to outline the efﬁciency of our pro-
tocols compared to Paxos and WABcast. Table 1 compares
the proposed protocols with Paxos [13] and WABCast [19]
in terms of time complexity (where δ is the maximum net-
work delay), message complexity, resilience, and the oracle
used for termination. In case of no collisions, WABCast as
well as L-/P-Consensus have the same time and message
Table 1: Comparison of various atomic broadcast protocols
No Collisions ; Collisions
Protocol
Paxos
WABCast
L-/P-Cons.
latency
3δ
2δ ; ∞
2δ ; 3δ
#messages
n2 + n + 1
n2 + n ; ∞
n2 + n ; 2n2 + n
Resil.
f < n/2
f < n/3
Oracle
Ω
W AB
Ω/(cid:14)P
complexity. Compared to Paxos, they trade the maximum
degree of resilience, i.e., f < n/2 for the lower time com-
plexity of 2δ. In periods with collisions, WABCast might
not terminate whereas L-/P-Consensus have the same time
complexity as Paxos though with more messages. We ex-
pect the proposed protocols to be as efﬁcient in terms of
latency as WABCast when collisions are rare and to exhibit
a behaviour similar to Paxos when collisions are frequent.
8.1 Experimental Evaluation
We compared the performance of the proposed pro-
tocols with Paxos and WABCast. We measured the la-
tency of atomic broadcast as a function of the throughput,
whereby latency is deﬁned as the shortest delay between a-
broadcasting a message m and a-delivering m. We imple-
mented L-/P-Consensus and C-Abcast using the Neko [21]
framework. The experiments were conducted on a cluster of
4 identical workstations (2.8GHz, 512MB) interconnected
by a 100Mb ethernet LAN. Different consensus algorithms
were tested by exchanging the consensus module of C-
Abcast. The WAB oracle implementation uses UDP pack-
ets whereas the rest of the communication is TCP-based.
We considered only stable runs in our experiments. In order
to capture the performance of the tested protocols during
periods with and without collisions, we varied the through-
put between 20msg/s and 500msg/s.
Figure 2 shows
]
s
m
[
y
c
n
e
t
a
l
n
a
e
m
 5
 4.5
 4
 3.5
 3
 2.5
 2
 1.5
 1
 0.5
 0
P-Consensus
L-Consensus
WABCast
 100
 200
 300
 400
 500
throughput [1/s]
Figure 2: L-/P-Cons. vs. WABcast (n = 4)
the comparison of our protocols with WABCast. Both pro-
posed protocols exhibit a similar latency as WABCast up to
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 04:20:34 UTC from IEEE Xplore.  Restrictions apply. 
P-Consensus
L-Consensus
Paxos
 2.6
 2.4
 2.2
 2
 1.8
 1.6
 1.4
 1.2
 1
]
s
m
[
y
c
n
e
a
t
l
n
a
e
m
 0.8
 0
 100
 200
 300
throughput [1/s]
 400
 500
Figure 3: L-/P-Cons. (n = 4) vs. Paxos (n = 3)
a throughput of 80msg/s and they outperform WABCast
for all throughputs higher than 100msg/s. Figure 3 sum-
marizes the comparison with Paxos. When collision pre-
dominate, the proposed protocols indeed have the same time
complexity as Paxos. However, given their decentralized
nature, our protocols need more messages. From a through-
put of 300msg/s upwards, Paxos slightly outperforms both
protocols. For lower throughputs, L-/P-Consensus perform
better than Paxos.
9 Conclusion
One-step decision and zero-degradation express the abil-
ity to reach consenus in one and two communication steps
respectively, and protocols that satisfy them are optimal in
this respect. We investigated if these properties are inher-
ently incompatible and showed that they cannot be both sat-
isﬁed using the Ω failure detector. As shown in [11], any Ω
based procol that decides in two communication steps in
every well-behaved run is also zero-degrading. This im-
plies that the failure detector employed by Fast Paxos [13]
is strictly stronger than Ω. Subsequently, we proposed two
approaches to circumvent the established impossiblity re-
sult. The ﬁrst approach relaxes one-step decision to hold
only in stable runs. The second approach assumes a strictly
stronger failure detector. For each approach we developed
a corresponding consensus protocol. While the proposed
L-Consensus ensures one-step decision only in stable runs,
the ability of P-Consensus to decide in one communication
step is regardless of the failure detector output. To be able
to test the efﬁciency of the proposed protocols we modiﬁed
the atomic broadcast algorithm of [19] to use consensus.
We compared the proposed consensus protocols with Paxos
and WABcast both analytically and experimentally. The re-
sults of the experiments conﬁrm the analytical evaluation
establishing the efﬁciency of our proposed protocols.
Acknowledgments: We gratefully acknowledge the
help and insights from Dr Falk Fraikin, the DEEDS group,
the funding support from Microsoft Research via the Euro-
pean PhD Fellowship; and also from the EU DECOS and
ReSIST projects .
References
[1] M. K. Aguilera et al. Failure detection and consensus in the
crash-recovery model. Dist. Computing, vol. 13, 2, pp. 99-
125, 2000.
[2] F. V. Brasileiro et al. Consensus in one communication step.
Proc. of PACT, pp. 42-50, 2001.
[3] L. Camargos et al. Optimal and practical WAB-based con-
sensus algorithms. UNISI TR IC-05-07 Apr. 2005.
[4] T. D. Chandra et al. The weakest failure detector for solving
consensus. JACM, vol. 43, 3, pp. 685-722, 1996.
[5] T. D. Chandra and S. Toueg. Unreliable failure detectors for
reliable distributed systems. JACM, vol. 43, 2, pp. 225-267,
1996.
[6] F. Chu. Reducing Ω to 3W. Inf. Processing Letters, vol. 67,
6, pp. 289-293, 1998.
[7] F. Cristian and C. Fetzer. The timed asynchronous distributed
system model. Proc. FTCS, pp. 140-149, 1998.
[8] C. Dwork et al. Consensus in the presence of partial syn-
chrony. JACM, vol. 35, 2, pp. 288-323, 1988.
[9] P. Dutta and R. Guerraoui. Fast indulgent consensus with zero
degradation. Proc. EDCC-4, pp. 191-208, 2002.
[10] M. J. Fischer et al.
Impossibility of distributed consensus
with one faulty process. JACM, vol. 32, 2, pp. 374-382, 1985.
[11] R. Guerraoui and M. Raynal. The information structure of
IEEE Trans. Computers, vol. 53, 12,
indulgent consensus.
pp. 453-466, 2004.
[12] I. Keidar and S. Rajsbaum. On the cost of fault-tolerant con-
sensus when there are no faults. ACM SIGACT News, Online
Vol. 32, 2001.
[13] L. Lamport. The part-time parliament. ACM Trans. Com-
puter Systems, vol. 16, 2, pp. 133-169, 1998.
[14] L. Lamport. Lower bounds for asynchronous consensus. Fu-
ture Directions in Dist. Computing, 2004.
[15] L. Lamport. Fast Paxos. MSR TR 2005-112, July 2005.
[16] N. A. Lynch. Distributed Algorithms. Morgan Kaufmann
Publishers, 1996.
[17] A. Most´efaoui and M. Raynal. Low cost consensus-based
atomic broadcast. Proc. PRDC, pp. 45-54, 2000.
[18] F. Pedone and A. Schiper. Optimistic atomic broadcast: A
pragmatic viewpoint. Journal of Theoretical Computer Sci-
ence, vol. 291, 1, pp. 79-101, 2003.
[19] F. Pedone et al. Solving agreement problems with weak or-
dering oracles. Proc. of EDCC, pp. 44-61, 2002.
[20] F. B. Schneider. Implementing fault-tolerant services using
the state machine approach: A tutorial. ACM Computing Sur-
veys, vol. 22, 4, pp. 299-319, 1990.
[21] P. Urb´an et al. Neko: A single environment to simulate and
prototype distributed algorithms. Proc. of Information Net-
working, pp. 503-511, 2001.
[22] P. Dutta et al. The Overhead of Consensus Recovery. IC TR
200456, June 2004.
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 04:20:34 UTC from IEEE Xplore.  Restrictions apply.