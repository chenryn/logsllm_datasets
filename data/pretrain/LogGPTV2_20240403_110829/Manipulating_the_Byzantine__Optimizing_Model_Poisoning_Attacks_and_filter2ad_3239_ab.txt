discards the gradients with the lowest scores. For any given
AGR, [17] proposes three defenses, one based on the loss of
model, one based on error of model, and one based on both
the loss and error. The combination of loss and error works
strictly better than either loss or error, hence, we consider the
defense based on the combination of loss and error.
3
D. Comparison with related works
Comparing our attacks. We compare our attacks with two
state-of-the-art model poisoning attacks, Fang [17] and LIE
(short for ‘A little is enough’) [4]. Fang attacks require the
knowledge of the server’s AGR, and without the knowledge,
they have no noticeable impact. Fang attack [17] formulates
a general optimization problem, but tailors it only to Krum
AGR. Furthermore, their solution of the optimization, even for
Krum AGR, is far from optimal (Figure 1-(a)). Next, unlike our
attacks on Trimmed-mean and Median AGRs, [17] proposes
only a heuristic attack to manipulate each dimension of the
collected gradients. Consequently, there are huge differences in
the impacts of ours and Fang attacks. Furthermore, our attacks
choose the most effective malicious direction tailored to the
given FL setting, which boosts their poisoning impacts. Note
that, Fang attacks are effective for the synthetic non-iid datasets
generated in [17]. But, as we will show, for iid and highly
imbalanced non-iid datasets, Fang attacks perform poorly.
LIE attacks [4] do not require the knowledge of AGR.
Hence, in order to circumvent detection by any AGR, LIE
attacks add a very small amounts of noises to a benign
aggregate. Comparatively, our novel AGR-agnostic attacks
leverage the knowledge of benign gradients in more principled
ways to boost their poisoning impacts (Figures 1-(b, c)).
Comparing our defense. Our attacks show that the con-
vergence guarantees of previous robust AGRs [8], [39] are
insufﬁcient to make FL robust to model poisoning. Hence,
our robust AGR aims to provide guarantees of the removal of
malicious updates. On a high level, our divide-and-conquer
(DnC) AGR uses singular value decomposition, a radically
different approach from the previous AGRs (Section II-C). Due
to its effective ﬁltering guarantees, the global model of DnC
converges to a better local optima and achieves signiﬁcantly
higher accuracies even under attack (Table IV).
III. THREAT MODEL OF MODEL POISONING ATTACKS
Here, we discuss various possible threat models of model
poisoning attacks on FL.
Adversary’s Objective. The goal of the adversary is to craft
malicious gradients such that when the malicious clients share
the malicious gradients with the central server, the accuracy of
the resulting global model reduces indiscriminately, i.e., on any
test input. This is also known as untargeted model poisoning
attack.
Adversary’s Capabilities. We assume that the adversary con-
trols up to m out of n total clients, called malicious clients.
We assume that the number of malicious clients is less than
the number of benign clients, i.e., (m/n) < 0.5; otherwise,
no Byzantine-robust AGR will be able to defeat poisoning
attacks. Following the previous works [6], [4], [3], [17], [37],
[19], we assume that
the adversary can access the global
model parameters broadcast in each epoch and can directly
manipulate the gradients on malicious devices.
Adversary’s Knowledge. We consider two important dimen-
sions of FL setting: knowledge of the gradient updates (simply
gradients) shared by the benign devices and knowledge of
Table I: Knowledge-based classiﬁcation of model poisoning adver-
saries in federated learning.
Type
agr-updates
agr-only
updates-only
agnostic
Gradients of
benign devices
(cid:88)

(cid:88)

Server’s AGR
algorithm
(cid:88)
(cid:88)


the AGR algorithm of the server. More speciﬁcally, we con-
sider four adversaries as shown in Table I. agr-updates
adversary is the strongest adversary who knows both the
gradients of benign devices and the server’s AGR. Although
agr-updates adversary has limited practical signiﬁcance, it
has been commonly used in previous works [17], [37], [4] to
understand the severity of the model poisoning threat. Further-
more, it allows the service provider (the server in this case) to
evaluate the robustness of its AGR algorithms. agr-only
adversary knows the server’s AGR, but does not have the
gradients of benign devices. To compute malicious gradients,
agr-only adversary uses benign gradients computed using
the benign data on malicious devices. updates-only ad-
versary has the gradients of benign devices, but does not
know the server’s AGR. We consider this adversary in order
to demonstrate the empirical upper bound of the severity of
our AGR-agnostic attacks. Finally, the agnostic adversary
does not have the gradients on benign devices or the server’s
AGR, and is the weakest possible adversary in FL.
Note that, none of the state-of-the-art untargeted model
poisoning attacks thoroughly consider these two dimensions:
Fang attacks [17] assume the complete knowledge of the
server’s AGR algorithm, while LIE attacks [4] assumes the
complete knowledge of the gradients of benign devices.
IV. OUR GENERIC FRAMEWORK FOR MODEL POISONING
In this section, we describe our generic framework to
mount model poisoning attacks on FL, followed by speciﬁc
optimizations for different AGRs and threat models, and ﬁnally
give an algorithm to solve the optimizations.
A. General optimization formulation
In each FL training epoch, the malicious and benign clients
share malicious and benign gradients, respectively, and then the
server updates the global model using an aggregate of all of
the gradients. To successfully mount an untargeted attack, our
general optimization problem aims to maximize the damage
to the global model in each FL epoch.
In order to maximize the damage to the global model, we
craft the malicious gradients, denoted by ∇m{i∈[m]}, such that
the aggregate computed by the server is far from a reference
benign aggregate, denoted by ∇b. A possible ∇b is the average
of the benign gradients that the adversary knows. For instance,
the agr-only adversary can compute m benign gradients
using the benign data on malicious devices. The ﬁnal malicious
gradient ∇m is a perturbed version of the benign aggregate ∇b,
i.e., ∇m = ∇b + γ∇p, where ∇p is a perturbation vector and
γ is a scaling coefﬁcient. Therefore, the objective of the full
4
Figure 1: Schematics of our attacks: (a) Our AGR-tailored attack, unlike Fang attack, ﬁne tunes the malicious gradient (∇b + γ∇p), using
optimal γ and dataset-optimized ∇p. (b) Our AGR-agnostic Min-Max attack ﬁnds its malicious gradient ∇m (red cross) whose maximum
distance from any other gradient is less than the maximum distance between any two benign updates (black arrows). (c) Our AGR-agnostic
Min-Sum attack ﬁnds ∇m (red cross) whose sum of distances from the other updates is less than the sum of distances of any benign gradient
from the other benign updates. Due to stricter constraints, ∇m of Min-Sum attack is closer to the benign aggregate, ∇b, than ∇m of Min-Max
attack. LIE attack computes very suboptimal ∇m due to extremely small amounts of noise additions.
argmax
knowledge agr-updates adversary is given by (1).
(cid:107)∇b − fagr(∇m{i∈[m]} ∪ ∇{i∈[m+1,n]})(cid:107)2
∇m
i∈[m] = ∇b + γ∇p; ∇b = favg(∇{i∈[n]})
γ,∇p
destroy the global model accuracy and signiﬁcantly outperform
the existing model poisoning attacks using one of these ∇p’s.
Hence, we leave investigating the optimal ∇p to future work.
(1)
where ∇{i∈[n]} are the benign gradients that the adversary
knows. Note that state-of-the-art robust AGRs [8], [39], [31]
are generally not differentiable. Hence, solving (1), i.e., ﬁnding
the optimal γ and ∇p, using gradient descent based optimiza-
tions is not trivial. Our idea to overcome this challenge is
to ﬁx the perturbation vector ∇p and ﬁnd the optimal γ, i.e.,
solve the modiﬁed objective in (2). Algorithm 1 (Section IV-D)
describes our algorithm to optimize γ.
argmax
γ
(cid:107)∇b − fagr(∇m{i∈[m]} ∪ ∇{i∈[m+1,n]})(cid:107)2
∇m
i∈[m] = ∇b + γ∇p; ∇b = favg(∇{i∈[n]})
(2)
Introducing perturbation vectors. A perturbation vector
is any malicious direction in the space of gradients that
the adversary can use to perturb ∇b and ﬁnd the malicious
gradients ∇m{i∈[m]}. In this work, we experiment with the
following three types of ∇p’s.
Inverse unit vector (∇p
uv). The intuition here is to compute the
malicious gradient by perturbing ∇b by a scaled unit vector
that points in the opposite direction of ∇b. Hence, we compute
∇p
uv as − ∇b
(cid:107)∇b(cid:107)2
Inverse standard deviation (∇p
std). The intuition here is that
the higher the variance of a dimension of benign gradients, the
higher the perturbation that the adversary can introduce along
the dimension. Hence, we compute ∇p
Inverse
as
−sign(favg(∇i∈[n])). The intuition here is similar to that of
(∇p
uv), but we observe that (∇p
sgn) is more effective for some
classiﬁcation tasks, e.g., MNIST.
std as −std(∇i∈[n]).
compute ∇p
(∇p
sgn).
sgn
.
sign
We
As we will show in Section VI-C, the appropriate choice
of perturbation vector ∇p is the key to an effective attack. For
instance, for Krum AGR, the attack using ∇p
uv increases the
accuracy of global model of MNIST, while the attack using
∇p
uv reduces the accuracy to random guessing for Purchase.
Finally, we note that our experiments show that our attacks
5
B. AGR-tailored attacks
In this
section, we
consider agr-updates and
agr-only adversaries, who know the server’s AGR algo-
rithm and tailor the general attack objective in (2) to the
known AGR. We consider the seven robust AGRs described
in Section II-C. For the clarity of presentation, we provide
the AGR-tailored optimizations for agr-updates adversary
with all the benign gradients ∇{i∈[n]}. The only change in
optimizations for agr-only adversary is to compute ∇b
using the benign gradients computed using the benign data
of the m malicious devices, i.e., ∇{i∈[m]}.
1) Krum: Krum1 selects a single gradient from its inputs
as its aggregate. Hence, a successful attack requires Krum
to select one of its malicious gradients,
i∈[m] =
fkrum(∇m{i∈[m]} ∪ ∇{i∈[m+1,n]}). Therefore, we modify (2)
to (3) for Krum. For each of the input gradients, Krum
computes a score that is the sum of distances of n − m − 2
nearest neighbors of the gradient. Therefore, to maximize the
chances of Krum selecting a malicious gradient, we keep all
the malicious gradients the same.
i.e., ∇m
argmax
γ
i∈[m] = fkrum(∇m{i∈[m]} ∪ ∇{i∈[m+1,n]})
∇m
∇m
i∈[m] = favg(∇{i∈[n]}) + γ∇p
(3)
2) Multi-krum: Multi-krum uses Krum iteratively to con-
struct a selection set S and computes average of the gradients
in the selection set as its aggregate. Our attack on Multi-krum
ensures that all of the malicious gradients are in selected S,
while maximizing the perturbation γ∇p used to compute the
malicious gradients. This strategy minimizes the number of
benign gradients in S, while maximizing γ∇p increases the
poisoning impact of malicious gradients on the ﬁnal aggregate.
Therefore, we modify (2) to (4) for Multi-krum; here |A| is
1We omit sufﬁx AGR, when it is clear from the context.
(b) Our AGR-agnostic Min-Max attack (c) Our AGR-agnostic Min-Sum attack LIE attackLIE attackOur attackﬁnal aggregateScaled maliciousperturbations(a) Our AGR-tailored attack (demonstrated for Krum)Fang attackﬁnal aggregateBenignaggregateLarge malicious gradientsrejected by KrumOur attackOur attackthe cardinality of A.
argmax
γ
m = |{∇ ∈ ∇m{i∈[m]}|∇ ∈ S}|
i∈[m] = favg(∇{i∈[n]}) + γ∇p
∇m
(4)
3) Bulyan: Our attack on Bulyan is similar to that on
Multi-krum, because Bulyan also computes a selection set in
the exact same fashion as Multi-krum. Furthermore, as the
distribution of perturbation γ∇p, and therefore, that of our
malicious gradients, is very similar to the distribution of the
benign updates. Hence, the Trimmed-mean based ﬁltering in
the second stage of Bulyan cannot effectively remove the
contribution of our malicious gradients, which makes our
attack effective.
4) Adaptive federated averaging: Our attack on AFA is
similar to that on Multi-krum, because similar to Multi-
krum, AFA computes a selection set and then computes
their weighted average. In AFA, the weight of each gradient
increases or remains constant if the gradient is selected, and
decreases if it is discarded. Hence, our attack on AFA aims
to maximize the number of malicious gradients in the ﬁnal
selection set of AFA.
The only change required to tailor our attack to AFA is
to use fafa to compute the selection set, S, in the attack
formulation in (4). Note that, to use fafa to compute S, we
need the weights of clients in each epoch; in our experiments,
we simply assume that all the clients have the same weights
in each epoch, while computing malicious gradients. Even this
loose assumption leads to highly impactful attacks. But, note
that if the exact weights are available, the attack impact can
improve further.
5) Trimmed-mean: For Trimmed-mean, we directly solve
the optimization described by (2), by ﬁxing the perturbation
∇p and keeping all the malicious updates the same. Hence, our
objective is to maximize the L2-norm of the distance between
the reference benign update ∇b and the aggregate computed
using Trimmed-mean on the set of benign and malicious
updates. This is formalized in (5).
argmax
γ
(cid:107)∇b − ftrmean(∇m{i∈[m]} ∪ ∇{i∈[m+1,n]})(cid:107)2
∇m
i∈[m] = favg(∇{i∈[n]}) + γ∇p
(5)
Note that in (5), we aim to compute γ that maximizes
the required L2-norm distance. As we demonstrate in our
evaluations, this extremely simple approach of crafting ma-
licious updates outperforms the complex approaches proposed
by Fang [17] attacks by very large margins for all the datasets.
6) Median: Similar to Trimmed-mean, Median computes
the aggregate of the collected updates for each dimension.
Therefore, our attack on Median is similar to that on Trimmed-
mean. The only change we introduce in (5) is that, for Median,
our optimization aims to maximize (cid:107)∇b − fmedian(∇m{i∈[m]} ∪
∇{i∈[m+1,n]})(cid:107)2.
7) Fang defenses: As described in Section II-C7, Fang
defenses are meta-AGRs that rely on existing defenses to detect
and remove malicious gradients. We argue that the simple
attack where the malicious gradients are computed by adding
an arbitrarily large vector to the average of benign gradients
sufﬁces to completely cripple any of the Fang defenses. We
illustrate this below.
Consider the Fang defense based on Multi-krum, called
Fang-Mkrum, and a set of b benign and m malicious gra-
dients, which we denote by G. For any malicious gradient
∇m computed using our attack, Fang-Mkrum computes two
aggregates: First, fmkrum(G), which is the average of b benign
gradients; this is because fmkrum can detect and remove all the
m malicious gradients due to their extremely large distances
from the rest of the gradients. Second, fmkrum(G − ∇m),
which is the average of b− 1 benign gradients, because fmkrum
removes all m − 1 malicious gradients and a single benign
gradient. For any benign gradient ∇b, Fang-Mkrum computes
two aggregates fmkrum(G) and fmkrum(G − ∇b). Similar to
malicious gradients, these aggregates are averages of b and
b − 1 benign gradients.
Hence, in theory, when adversary mounts our attack, Fang-
Mkrum assigns almost equal scores to all the benign and our
malicious gradients. This forces Fang-Mkrum to accept at least
few of the malicious gradients. As our malicious gradients are
arbitrarily large, even when Fang-Mkrum selects a few of them,
they signiﬁcantly corrupt the global model. Observe that all
the Fang defenses exhibit the same behavior, and hence, are
fundamentally broken.
For brevity, in this work, we only consider the Fang defense
that uses Trimmed-mean to discard malicious gradients. We
note that, our AGR-tailored attack on Fang defenses does not
depend on the robust AGR it uses.
C. AGR-agnostic attacks
consider
Now, we
the AGR-agnostic
adversaries,
updates-only and agnostic, who do not know
the server’s AGR algorithm. This is an important practical
consideration, because the FL platforms can conceal
the
details and/or parameters of their robust AGRs to protect the
security of the proprietary global models. Below, we ﬁrst
provide intuition behind our attacks and then propose two