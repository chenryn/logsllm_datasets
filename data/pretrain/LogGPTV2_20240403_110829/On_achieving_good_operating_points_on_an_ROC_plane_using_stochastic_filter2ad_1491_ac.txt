be used to predict the next anomaly score? To determine
the Markovian order, we use the conditional entropy based
measure proposed in [22].
Conditional entropy, H(B|A), of two discrete random vari-
ables A and B characterizes the information remaining in B
when A is already known. Phrased diﬀerently, conditional
entropy is ‘information about B not given by A.’ If A and
B are highly correlated, most of the information about B
is communicated through A and H(B|A) is small. On the
other hand, if pA and pB (which respectively represent the
probability mass functions of A ad B) are quite diﬀerent
then H(B|A) assumes a high value.1
To identify the order of correlation presence in the IDS
scoring random process, we deﬁne a Markov chain based
stochastic model as follows: Let the score at discrete time
instance n represent the realization of a random variable
derived from a stochastic process Xn. This process is a
1In the limiting cases, H(B|A) = 0 when A = B, while
H(B|A) = H(B) when A and B are independent.
Markov chain if it satisﬁes the Markov property, which is
deﬁned as:
Pr {Xn = j |Xn−1 = i, Xn−2 = in−2, . . . , X0 = i0}
= Pr {Xn = j |Xn−1 = i } = pj|i.
In other words, the probability of choosing a next state is
only dependent on the current state of the Markov chain.
In the present context, we can deﬁne a Markov chain
model Xn for an IDS’ scores by dividing all possible val-
ues of the score in multiple non-overlapping bins. Each bin
then represents a state of the Markov chain, while the set of
all bin indices ψ is its state space. Based on this state repre-
sentation, we can deﬁne a 1-st order Markov chain, X (1)
n , in
which each bin represents a state of the random process. The
transition probability matrix of the 1-st order Markov chain
P (1) can be computed by counting the number of times state
i is followed by state j. The resulting |ψ(1)| histograms can
be normalized to obtain the state-wise transition probability
mass functions as the rows of P (1).
We can ﬁnd the conditional probability of the 1-st order
Markov chain as:
H (1) = − X
i∈ψ(1)
π(1)
i X
j∈ψ(1)
j|i log2 (cid:16)p(1)
p(1)
j|i(cid:17) ,
(2)
i
where π(1)
is the average probability of being in state i which
is computed by counting the total number of times each state
is visited and then normalizing this frequency histogram.
The measure H (1) deﬁnes how much average information
is remaining in anomaly score Xn when it is predicted using
score Xn−1. If the present score is not highly correlated with
scores before Xn−1, H (1) will be relatively large implying
that information about Xn not provided by Xn−1 is high.
In such a case, generalizing the above discussion, we can
deﬁne a higher l-th order Markov chain, X (l)
n , in which each
state is an l-tuple  representing the values
taken by the random process in the last l time instances.
Aggregating multiple time instances in a single state allows
us to satisfy the Markov property, and hence a transition
probability matrix P (l) can be computed by counting the
number of times  is followed by state . The conditional entropy of X (l)
n deﬁned on
3191
0.8
0.6
0.4
0.2
y
p
o
r
t
n
E
l
a
n
o
i
t
i
d
n
o
C
0
2
Stide
SVM
KL−Divergence
Max−Entropy
TRW
PHAD
1
0.8
0.6
0.4
0.2
y
p
o
r
t
n
E
l
a
n
o
i
t
i
d
n
o
C
8
6
10
4
Markov Chain Order
(a) Network-based
12
14
0
2
6
4
Markov Chain Order
10
8
12
14
(b) Host-based
Figure 4: Conditional entropy of network- and host-based ADSs.
ψ(l) can then be computed using the same method as (2).
It is easy to observe that H (1) ≥ H (2) ≥ . . . ≥ H (l), as each
older anomaly score can either be independent of or provide
some information about the present score. The number of
previous scores required to accurately predict the next score
can then be determined by plotting H (l) as the function of
the Markov chain order, l = 1, 2, . . .. The order at which
the conditional entropy saturates deﬁnes the total number
of previous scores which have conveyed as much information
of the present score as possible.
Fig. 4(a) shows the conditional entropy of the network-
based ADSs. It can be observed that the Maximum Entropy
detector shows an exponentially decaying trend until order
4, after which the decay become somewhat linear. Therefore,
for the Maximum Entropy detector, four previous values are
suﬃcient to predict the next score. Similarly, TRW’s scores
can be predicted using approximately six prior scores. As
expected and explained earlier, PHAD exhibits very low con-
ditional entropy decay which is hardly visible on the y-axis
scale of Fig. 4(a). Hence PHAD’s scores can be predicted us-
ing only one prior score. Similarly, host-based detectors also
show the same decaying trend in Fig. 4(b). STIDE shows
the most steepest decay followed by SVM. The KL detector
shows behavior similar to the PHAD in the network domain;
i.e., the next ADS score can be predicted given very few past
score values. Therefore, we predict scores for STIDE, SVM
and KL detectors using eight, nine and one prior scores,
respectively.
At this point, in addition to having a generic method for
ascertaining the number of previous scores required for fu-
ture score prediction, we also know that an ADS’ scores
can be accurately modeled using high-order discrete Markov
chains. In the following section, we use this stochastic model
to develop a tracking algorithm that can accurately adapt
an ADS’ classiﬁcation threshold.
5. ADAPTIVE THRESHOLDING
ALGORITHM
Based on results of the last section, we now propose a sim-
ple and generic Markovian anomaly score predictor. This
Markovian predictor is in essence a variant of the stochastic
target tracking algorithm proposed in [8]. At this point, it is
important to reiterate that our rationale for anomaly score
prediction is that the predicted scores can be used to thresh-
old future scores in accordance with varying input charac-
teristics. The rest of the section describes the algorithm
and compares its accuracy with two well-known stochastic
prediction algorithms.
5.1 Algorithm
Let us subdivide an IDS’ anomaly score into k equal-sized
bins, where k is determined as a by-product of the condi-
tional entropy analysis of the last section. Speciﬁcally, the
Markovian order at which the decaying conditional entropy
saturates is chosen as the value of k. The size of each bin is
then calculated by taking the diﬀerence of the minimum and
the maximum anomaly score and dividing that diﬀerence by
k. The sizes of the ﬁrst and the last bin are kept ﬂexible
to accommodate any previously-unseen anomaly scores that
may be observed during real-time operation.
Let P (n) denote the k × k transition probability matrix of
the Markov chain predictor at time n, where p(n)
i|j represents
an entry at the ith row and jth column of P (n). Also, let
r(n) be the actual value of an ADS score observed at time
instance n and let ˆr(n) be the Markovian prediction from
the last time instance. Then the algorithm for adaptive
thresholding operates as follows:
ε(n) = |r(n) − ˆr(n)|,
r(n)±i|r(n) = ε(n) × p(n)
˜p(n+1)
˜p(n+1)
j|r(n)
i=1 ˜p(n+1)
i|r(n)
p(n+1)
j|r(n) .
Pk
ˆr(n+1) = max
j=1,...,k
p(n+1)
j|r(n) =
r(n)±i|r(n) , ∀i = 1, . . . , β,
, ∀j = 1, . . . , k, and
(3)
(4)
(5)
Equation (3) calculates the prediction error ε(n) from the
predicted and the observed score. Then, at each time step n,
(4) feeds the error ǫ(n) back into P (n) in order to adapt and
learn the varying traﬃc or host patterns. Using this feed-
back, the weight of a value near the observed value r(n) is
increased proportionally. Speciﬁcally, a tunable parameter β
is used to assign higher probability weights to states close to
the current Markov state r(n) by multiplying probabilities of
states r(n) + 1, . . . , r(n) + β and states r(n) − 1, . . . , r(n) − β
with ǫ(n). Thus higher error means that the probabilities
320l
i
s
e
u
a
V
e
c
n
e
g
r
e
v
D
−
L
K
20
15
10
5
0
0
Adaptive Threshold
Observed
Fixed Threshold
10
30
20
40
Time Window
50
60
Figure 5: K-L divergence thresholding for the
Maximum-Entropy detector using adaptive and
ﬁxed thresholds in an anomalous window.
of these states increase proportionally and the predicted
anomaly scores for the next time instance will likely drift
away from the current state.
In practice, we expect that
β ≪ k. The updated row of the transition probability ma-
trix is normalized to obtain a probability mass function for
state r(n). Finally, (5) predicts the the next anomaly score,
ˆr(n+1), as the state having the highest probability in the up-
dated transition probability matrix. This predicted anomaly
score is used as the adaptive threshold for time instance n+1.
5.2 Prediction Accuracy of the Adaptive
Thresholding Algorithm
For accuracy benchmarking, we compare the prediction
accuracy of the proposed algorithm with two well-known
predictors, namely Kalman ﬁlter and Holt-Winters [29]. Figs.
1(a) and (b) show the accuracies of the three predictors
(Markovian, Kalman, Holt-Winters) in tracking the input
(traﬃc and system call) trends observed in the LBNL and
UNM datasets; we use a value of β = 2 for these results. It
can be seen in Fig. 1 that, while all three predictors can fol-
low real-time measurements, the Markovian predictor tracks
these measurements much more accurately than the Kalman
ﬁlter and Holt-Winters predictors.
As a proof-of-concept example of the accuracy advan-
tages that can be provided by adaptive thresholding, Fig.
5 shows the Markovian-predicted thresholds in an anoma-
lous LBNL time window for the Maximum-Entropy detec-
tor; the threshold in this case is the K-L divergence of a
packet class that has been perturbed by the attack. It can
be clearly seen that the Markovian predictor estimates the
highly-varying K-L divergence values with remarkable ac-
curacy. Furthermore, note in Fig. 5 that selecting a ﬁxed
threshold may allow a few anomalies to go undetected, espe-
cially anomalies which do not cause signiﬁcant perturbations
in the actual network traﬃc. For instance, in the 60 second
output shown in Fig. 5, only 10 of these values cross the
ﬁxed threshold. In this experiment, the Maximum-Entropy
algorithm was detecting an anomaly if 12 or more values in
a 60 second window exceed the ﬁxed threshold. Hence, this
anomaly will not be detected by a ﬁxed threshold. Adaptive
thresholding, on the other hand, accurately predicts the K-L
divergence in the next window and the observed (perturbed)
divergence exceeds this threshold more than 20 times in a
60 second window, thereby allowing the Maximum-Entropy
detector to ﬂag the anomaly.
Furthermore, observe from Fig. 5 that for many seconds
the observed K-L values drop to 0. These low values give
a crafty attacker the leverage to introduce malicious traﬃc
that does not exceed the ﬁxed threshold of [0, 10]. How-
ever, an adaptive threshold immediately learns the change
and sets the threshold to 0, thus ensuring that no room is
available for such a mimicry attack.
6. PERFORMANCE EVALUATION
The adaptive thresholding algorithm proposed in the last
section is generic because it does not rely on the detection
features and principles of the underlying IDSs. Therefore, in
this section we incorporate the adaptive thresholding mod-
ule into the six ADSs evaluated in this work and evaluate
the accuracies and complexities of these adaptive ADSs. As
mentioned earlier, one should expect an accurate adaptive
thresholding algorithm to choose good accuracy points on
an ROC plane. In fact, as shown by the example provided
in the last section, in most cases we expect an adaptive
thresholding algorithm to provide accuracy improvements
over the original (non-adaptive) ADS. Moreover, threshold
adaptation should be provided with low complexity.
6.1 Accuracy Evaluation
Fig.
6 shows the ROC-based accuracy comparison of
the Maximum Entropy, TRW and PHAD detectors with
and without adaptive thresholding. It can be clearly seen
that for both datasets adaptive thresholding allows PHAD
to achieve dramatic accuracy improvements. In particular,
PHAD originally had very high false alarm rates. Adaptive-
PHAD prevents the false alarms caused by legitimate change
in traﬃc behavior, thereby inducing a considerable reduction
in the ADS’ false alarm rate. Signiﬁcant improvement was
not observed for the Adaptive-Maximum-Entropy detector
on the endpoint dataset because the original algorithm pro-
vided very high accuracy on the endpoints. Nevertheless, the
Adaptive-Maximum-Entropy detector provides good opera-
tional points on the ROC curve without any human inter-
vention. Note that non-adaptive Maximum Entropy failed
to maintain its performance across the LBNL dataset be-
cause the erratic traﬃc variations at an edge router intro-
duces signiﬁcant false alarms for the Maximum-Entropy de-
tector. Adaptive-Maximum-Entropy detector decreases this
false alarm rate. In contrast to the Maximum Entropy de-
tector, TRW provided good accuracy on the LBNL dataset