return variable of the platform API eventsSince, which is influ-
enced by the symbolic input timeAgo. However, as eventsSince
is closed-source, the concolic executor does not know how to set
timeAgo such that the value of the return variable of eventsSince
will make alreadySet be false. In contrast, Westworld applies se-
lective code-segment fuzzing to handle the particular code segment
(containing eventsSince and influenced by timeAgo) and can find
the appropriate value to explore both branches.
Manual Testing. We randomly selected 10 apps (2 from each cate-
gory in Table 2) and asked 5 human analysts to test them: one has
three years, two have two years, and another two have one year of
smart app development experiences. They are skilled in debugging
and are familiar with the SmartThings web interface for executing
apps. Each analyst tested 2 apps and spent 2 hours on each one.
Our studies were approved by IRB at our university.
The results show that 4 apps cannot be fully tested, and 6 apps
take more than one hour for testing. We interviewed all the analysts.
They mentioned three main problems for testing smart apps. First,
it is very troublesome to test smart apps using the SmartThings
web interface. When they modified the app configuration to change
the values of user inputs (e.g., in Figure 1) or mutated the values
of environment variables (e.g., in Figure 2), they needed to wait a
long time (more than 30s) until the app is reinstalled on the remote
cloud before executing the app. (2) Some environment variables
are defined in the source code (e.g., location.mode in Figure 4 at
Line 22). Through reading the source code, they were not sure
which variables are environment variables; more importantly, for
some environment variables, they did not know which values can be
assigned for testing. E.g., two analysts knew only two values (among
three) for the location mode; and one analyst did not know any value
for the location mode. This resulted in some paths being unexplored.
(3) They could not figure out the appropriate values for user inputs
and environment variables in order to trigger unexplored paths,
especially when some platform APIs are involved. Thus, a tool that
can automatically test smart apps is a critical need.
Precision. To evaluate the precision of our tool, we compare the
path condition of the execution path triggered by each test case to
the path condition used to generate the corresponding test case; no
path divergence occurs. Thus, all generated test cases are precise.
8.5 Efficiency
To evaluate the efficiency, we record the analysis time—the total
time used to finish automatic testing of an app—for each app in
Dataset-I. Figure 12 shows the result. In Figure 12(a), each point
represents the average analysis time for the apps in each category.
991Westworld: Fuzzing-Assisted Remote Dynamic Symbolic Execution of Smart Apps on IoT Cloud Platforms
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Figure 12: Efficiency results.
Figure 13: Breakdown of analysis time.
It shows that for the fuzzer, the analysis time grows very quickly
when the number of paths increases. W-van and W-boost are much
more efficient than the fuzzer.
In Figure 12(b), the blue diamond represents the analysis time
for each app when W-van is applied, and the red rectangle shows
that when W-boost is applied. The two black solid lines are the
polynomial trendlines of the analysis time. We can see that the
analysis time grows quickly when the number of paths increases
for W-van, while for W-boost, the time grows much slower.
We next seek to understand why W-boost performs better than
W-van. We divide the analysis time into three parts, as shown
in Figure 13: (A) data transmission time, used to send the testing
requests to the cloud and get the results back; (B) online execution
time, which includes b1) logging in the cloud, b2) locating an app,
b3) updating the app code, b4) triggering a simulator to install an
app, and b5) running an app on the cloud; and (C) offline analysis
time, used to generate test cases and instrument the app code.
We randomly select 20 apps, run each app ten times, and record
the time. (1) The average time for b1, b2, b3 and b4 is 1.74s, 6.95s,
8.42s, and 34.73s, respectively. The time cost of each is recorded
by our Web Interaction component. b1 and b2 are one-time effort,
while b3 and b4 are needed for each testing request. (2) The total
time taken by b5 and C for all testing requests of an app should be
approximately the same for W-van and W-boost. (3) The average
time for A is around 0.06s, which is very small compared to the cost
due to B. We can conclude that (i) the time cost due to b3 and b4 is
large, and (ii) W-boost is more efficient than W-van as the time
taken by b3, b4, and A is much saved—W-boost executes all test
cases of one generation through one testing request, while W-van
executes each test case through one request.
8.6 Bug Finding
Some bugs in IoT apps are hard to find manually; e.g., a division-by-
zero bug may be triggered by a particular user input. To demonstrate
the effectiveness in bug finding, we apply Westworld to four types
of bugs: (1) division by zero, (2) array out of bound, (3) null-pointer
dereference, and (4) dead code. The first three types of bugs, once
exploited, will make smart apps crash and smart home automation
undependable. The last one will increase the app size and analysis
cost if static analysis is applied. For a division-by-zero bug, when
a division formula (e.g., a/b) is found during path exploration and
Figure 14: Apps that contain null-pointer dereference bugs.
the denominator is represented as a symbolic expression, the path
constraint, b == 0, is added to each path condition when it is to
be resolved. A division-by-zero bug is found if the path condition
is resolvable. This is similar for other bugs with different path
constraints added. Due to space limit, we omit it here.
In Dataset-I, we found 4 apps with null-pointer dereference bugs,
which are caused by the “enum” and “required false” inputs. If the
value for such a variable is not specified, its value will be set to null
by the platform. When the variable is used to invoke a function, a
null pointer exception is triggered.
Figure 14 shows two smart apps containing the bug. (1) In the
GarageDoorOpen-TurnOnLight app, if a user does not specify a value
for the input threshold, when the toInteger function is called, a null
pointer exception is triggered. (2) In the HoneyImHome app, the
return variable s of the platform API getSunriseAndSunset is a
map. If the value for the input zipCode is not specified, the two fields
of s—sunrise and sunset—are null. As a result, riseTime is null, and
the access to a field of riseTime will trigger a null pointer exception.
We further create Dataset-III containing eight apps with different
bugs inserted, as shown in Table 3: (1) two apps contain division
by zero bugs, (2) four are inserted with dead code (due to infeasible
paths), (3) one contains an array out of bound bug, and (4) one
contains a null-pointer dereference bug. The evaluation shows that
Westworld can successfully find all the bugs.
(A) Datatransmission timeOur system(C) Offline analysis time(B) Online execution timeCloud(A) Datatransmission timeTesting requestResult992ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Lannan Luo, Qiang Zeng, Bokai Yang, Fei Zuo, and Junzhe Wang
Division by
Bugs
zero
Dead code
Array out of
bound
Null-pointer
dereference
Table 3: Eight inserted bugs.
# of apps
1
1
4
1
1
Description
A user input is converted to another value by a method and then used as a denominator.
A field of the state object (storing data of previous executions) is not initialized but used as a denominator.
Two path conditions are contradicted with each other. No solution can be found by the solver.
The platform API getChildApps returns a list of child apps associated with this smart app.
An index used to retrieve a child app is larger than the length of the list.
A field of the return variable weather of the platform API getTwcConditions is accessed.
The variable weather, which contains the weather data, however, may be null.
9 RELATED WORK
Symbolic Execution. Symbolic execution has been widely applied
to test applications like Windows programs [34, 74, 82], Linux pro-
grams [4, 10–12, 23, 74], Java programs [5, 8, 51, 59], and firmware [14,
26, 38, 68, 78, 85]. We propose remote dynamic symbolic execution
to test smart apps running in a remote execution environment.
Dynamic Symbolic Execution (DSE). It performs symbolic ex-
ecution dynamically [11, 12, 23, 34, 60]. Although DSE recovers
from imprecision caused by API calls, it sacrifices completeness. To
resolve it, we propose selective code-segment fuzzing that (1) identi-
fies part of the app code that causes missing execution paths, and
(2) fuzzes only this part of code to complement the path coverage.
Fuzzing Combined With Dynamic Symbolic Execution. A set
of approaches have been proposed to combine fuzzing and DSE.
Most of them are fuzzing-centric, in which the path exploration
is offloaded to the fuzzer, and DSE is selectively used to assist
fuzzing [7, 19, 45, 53, 57, 58, 67, 75, 77, 81, 83, 84]. E.g., Driller [67]
uses DSE to make the fuzzer “revive”. It aims to find bugs hidden
deep, but not complete path exploration. DeepFuzz [7] uses a similar
idea. DigFuzz [84] and MDPC [75] design a path prioritization
model to quantify each path’s difficulty and prioritize them for DSE.
QSYM [77] loosens precision of DSE for better performance.
Compared to fuzzing-centric approaches such as Driller, our
method is symbolic execution-centric. We made this SE-centric choice
due to the unique challenge: the communication cost between the
remote cloud and local analyzer, and request handling time can-
not be omitted. Thus, each testing request is expensive. Given a
path like (temp68), Driller cannot avoid generating a
lot of testing requests that repetitively take the same path, while
symbolic execution is good at this. Plus, our boosted generational
search further enhances the vanilla generational search by testing a
whole generation of inputs in one request to improve the efficiency.
Analysis of IoT Applications. A lot of researches have been made
on analyzing IoT or mobile apps [2, 15–17, 20, 21, 29, 30, 39, 43, 44,
46–48, 61, 73, 76, 79, 80]. FlowFence enforces sensitive data flow
control via opacified computation [29]. HAWatcher [30] extracts
semantics from IoT apps for anomaly detection. Centaur [51] also
needs to handle the difficulty due to the decoupled concrete execu-
tion and symbolic execution; specifically, it migrates the heap from
an Android system to the symbolic executor. Unlike prior work,
Westworld is the first system that enables DSE of IoT apps.
10 DISCUSSION AND LIMITATIONS
To demonstrate bug finding capability, Westworld is applied to
four types of bugs that result in crashes. Besides these, it can
also be applied to some sophisticated vulnerabilities, e.g., cross-
app interference (CAI) bugs [21, 22]. A set of work uses model
checking [17, 18, 56], or combines static analysis and NLP tech-
niques [28, 72], to detect CAI bugs. HomeGuard is the first work
that leverages classic symbolic execution and SMT solving for find-
ing CAI bugs [22]. To handle platform APIs, it uses manual function
modeling and thus causes imprecision. Westworld is the first DSE
system for analyzing IoT apps and attains precise analysis.
The proposed remote DSE is enabled by multiple ideas, such as
leveraging logging and messaging to collect path conditions, con-
verting environment data to symbolic inputs, boosted generational
search, and selective code-segment fuzzing. Extending remote DSE
to analyze other types of code running on remote proprietary plat-
forms is an interesting research direction.
Limitations. Westworld uses a constraint solver to generate
test cases, which can scale to complex constraints [6] but also has
limitations [25, 41]. Our evaluation shows Westworld can achieve
completeness for the apps under testing, mainly because existing
IoT apps usually have a small number of paths. We do not claim
Westworld guarantees completeness in general.
Selective code-segment fuzzing works well for analyzing smart
apps, as symbolic variables (such as temperature, home mode, and
switch state) usually do not have many discrete values. For general
programs which often have unbounded possible concrete values
for symbolic variables, however, the method does not work.
11 CONCLUSION
We have presented the first system that enables dynamic symbolic
execution (DSE) of smart apps. As most IoT platforms are cloud-
based proprietary execution environment, various challenges arise.
Exploiting the uniqueness of environment inputs, selective code-
segment fuzzing was proposed to assist DSE. Boosted generational
search was designed to accelerate the analysis. We implemented
Westworld, which performs fuzzing-assisted DSE-centric analysis
of smart apps. The evaluation shows that Westworld is effective
and efficient in path exploration and bug finding.
ACKNOWLEDGMENTS
We would like to thank our shepherd, Dr. Sébastien Bardin, and
the anonymous reviewers for their constructive suggestions and
comments. This work was supported in part by the US National
Science Foundation (NSF) under grants CNS-1815144, CNS-1850278,
CNS-1953073, CNS-1856380, CNS-2016415 and CNS-2107093. This
work was partially supported by an ASPIRE grant from the Office of
the Vice President for Research at the University of South Carolina.
993Westworld: Fuzzing-Assisted Remote Dynamic Symbolic Execution of Smart Apps on IoT Cloud Platforms
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
REFERENCES
[1] AFL. 2020. american fuzzy lop. https://lcamtuf.coredump.cx/afl/.
[2] Omar Alrawi, Chaz Lever, Manos Antonakakis, and Fabian Monrose. 2019. Sok:
Security evaluation of home-based iot deployments. In IEEE S&P. 208–226.
[3] Roberto Amadini, Mak Andrlon, Graeme Gange, Peter Schachte, Harald Sønder-
gaard, and Peter J Stuckey. 2019. Constraint Programming for Dynamic Symbolic
Execution of JavaScript. In International Conference on Integration of Constraint
Programming, Artificial Intelligence, and Operations Research. Springer, 1–19.
[4] Thanassis Avgerinos, Sang Kil Cha, Brent Lim Tze Hao, and David Brumley. 2011.
AEG: Automatic exploit generation. (2011).
[5] Daniel Balasubramanian, Zhenkai Zhang, Dan McDermet, and Gabor Karsai.
2019. Dynamic symbolic execution for the analysis of web server applications in
Java. In Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing.
2178–2185.
[6] Roberto Baldoni, Emilio Coppa, Daniele Cono D’elia, Camil Demetrescu, and
Irene Finocchi. 2018. A survey of symbolic execution techniques. ACM Computing
Surveys (CSUR) 51, 3 (2018), 1–39.
[7] Konstantin Böttinger and Claudia Eckert. 2016. Deepfuzz: Triggering vulner-
abilities deeply hidden in binaries. In International Conference on Detection of