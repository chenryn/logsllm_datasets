salient properties of this expansion: where (geographi-
cally or topologically) most of the expansion has taken
place, and how it has impacted clients.
There are interesting aspects of Google’s deployment
that we currently lack means to measure. In particu-
lar, we do not know the query volume from diﬀerent
clients, and we do not know the latency from clients to
servers (which may or may not correlate closely with
the geographic distance that we measure). We have left
exploration of these to future work. We do possess in-
formation about client aﬃnity to frontend servers, and
how this aﬃnity evolves over time (this evolution is a
function of improvements in mapping algorithms as well
as infrastructure rollout): we have left a study of this
to future work.
4. METHODOLOGY
In this section, we discuss the details of our measure-
ment methods for enumerating frontends, geolocating
them, and clustering them into serving sites.
4.1 Enumerating Frontends
Our ﬁrst goal is to enumerate the IP addresses of all
frontends within a serving infrastructure. We do not
attempt to identify when multiple IP addresses belong
to one computer, or when one address fronts for mul-
tiple physical computers. An IP addresses can front
hardware from a small satellite proxy to a huge data-
center, so careful accounting of public IP addresses is
not particularly meaningful.
Since most serving infrastructures use mapping al-
gorithms and DNS redirection, one way to enumerate
frontends is to issue DNS requests from multiple van-
tage points. Each request returns a frontend near the
querying vantage point. The completeness of this ap-
proach is a function of the number of vantage points.
We emulate access to vantage points around the world
using the proposed client-subnet DNS extension using
the EDNS extension mechansim (we call this approach
3
EDNS-client-subnet). As of May 2013, EDNS-client-
subnet is supported by Google, CacheFly, EdgeCast,
ChinaCache and CDN 77. We use a patch to dig 1
that adds support for EDNS-client-subnet, allowing the
query to specify the client preﬁx. In our measurements
of Google, we issue the queries through Google Public
DNS’s public recursive nameservers, which passes them
on to the service we are mapping. The serving infras-
tructure then return a set of frontends it believes are
best suited for clients within the client preﬁx.
EDNS-client-subnet allows our our single measure-
ment site to solicit the recommended serving infras-
tructure for all the Internet—we eﬀectively get vantage
points that are everywhere. We query using client pre-
ﬁxes drawn from 10 million routable /24 preﬁxes ob-
tained RouteViews BGP. Queries against Google us-
ing this approach take about a day to enumerate.
4.2 Client-centric Geolocation
Current geolocation approaches are designed for gen-
erality, making few or no assumptions about the target.
Unfortunately, this generality results in poor perfor-
mance when geolocating serving infrastructure. For ex-
ample, MaxMind’s free database [21] places all Google
frontends in Mountain View, the company’s headquar-
ters. General approaches such as CBG [10] work best
when vantage points are near the target [14], but fron-
tends in serving infrastructures are sometimes in remote
locations, far from public geolocation vantage points.
Techniques that use location hints in DNS names of
frontends or routers near frontends can be incomplete [12].
Our approach combines elements of prior work, adding
the observation that today’s serving infrastructures use
privileged data and advanced measurement techniques
to try to direct clients to nearby frontends [31]. While
we borrow many previously proposed techniques, our
approach is unique and yields better results.
We base our geolocation technique on two main as-
sumptions. First, a serving infrastructure tries to direct
clients to a nearby frontend, although some clients may
be directed to distant frontends, either through errors
or a lack of deployment density. Second, geolocation
databases have accurate locations for many clients, at
least at country or city granularity, but also have poor
granularity or erroneous locations for some clients.
Combining these two assumptions, our basic approach
to geolocation, called client-centric geolocation (CCG),
is to (1) enumerate the set of clients directed to a serv-
ing site, (2) query a geolocation database for the loca-
tions of those clients, and (3) assume the frontends are
located geographically close to most of the clients.
To be accurate, CCG must overcome challenges in-
herent in each of these three steps of our basic approach:
1. We do not know how many requests diﬀerent preﬁxes
1http://wilmer.gaa.st/edns-client-subnet/
send to a serving infrastructure. If a particular pre-
ﬁx does not generate much traﬃc, the serving infras-
tructure may not have the measurements necessary
to direct it to a nearby frontend, and so may direct
it to a distant frontend.
2. Geolocation databases are known to have problems
including erroneous locations for some clients and
poor location granularity for other clients.
3. Some clients are not near the frontend that serve
them, for a variety of reasons. For example, some
frontends may serve only clients within certain net-
works, and some clients may have lower latency paths
to frontends other than the nearest ones.
In other
cases, a serving infrastructure may direct clients to
a distant frontend to balance load or may mistak-
enly believe that the frontend is near the client. Or,
a serving infrastructure may not have any frontends
near a particular client.
We now describe how CCG addresses these challenges.
Selecting client preﬁxes to geolocate a frontend.
To enumerate frontends, CCG queries EDNS using all
routable /24 preﬁxes. However, this approach may not
be accurate for geolocating frontends, for the following
reason. Although we do not know the details of how a
serving infrastructure chooses which frontend to send a
client to, we assume that it attempts to send a client to
a nearby frontend and that the approach is more likely
to be accurate for preﬁxes hosting clients who query
the service a lot than for preﬁxes that do not query the
service, such as IP addresses used for routers.
To identify which client preﬁxes can provide more
accurate geolocation, CCG uses traceroutes and logs
of users of a popular BitTorrent extension [7]. Ono
issues traceroutes between connected pairs of users that
provided an additional 102,064 preﬁxes with unlikely
serving infrastructure mappings. From the user logs
we obtain a list of 2 million client preﬁxes observed
to participate in BitTorrent swarms with users. We
assume that a serving infrastructure is likely to also
observe requests from these preﬁxes.
Overcoming problems with geolocation databases.
CCG uses two main approaches to overcome errors and
limitations of geolocation databases. First, we exclude
locations that are clearly wrong. Second, we combine
a large set of client locations to locate each frontend
and assume that the majority of clients have correct lo-
cations that will dominate the minority of clients with
incorrect locations. To generate an initial set of client
locations to use, CCG uses a BGP table snapshot from
RouteViews [22] to ﬁnd the set of preﬁxes currently an-
nounced, and breaks these routable preﬁxes up into 10
million /24 preﬁxes.2 It then queries MaxMind’s Geo-
LiteCity database to ﬁnd locations for each /24 preﬁx.
2In Section 5.1, we verify that /24 is often the correct preﬁx
length to use.
4
CCG prunes three types of preﬁx geolocations as un-
trustworthy. First, it excludes preﬁxes for which Max-
Mind indicates it has less than city-level accuracy. This
heuristic excludes 1,966,081 of the 10 million preﬁxes
(216,430 of the 2 million BitTorrent client preﬁxes).
Second, it uses a dataset that provides coarse-grained
measurement-based geolocations for every IP address
to exclude preﬁxes that include addresses in multiple
locations [11]. Third, it issues ping measurements from
all PlanetLab locations to ﬁve responsive addresses per
preﬁx, and excludes any preﬁxes for which the Max-
Mind location would force one of these ping measure-
ments to violate the speed of light. Combined, these
exclude 8,396 of the 10 million preﬁxes (2,336 of the 2
million BitTorrent client preﬁxes).
With these problematic locations removed, and with
sets of preﬁxes likely to include clients, CCG assumes
that both MaxMind and the serving infrastructure we
are mapping likely have good geolocations for most of
the remaining preﬁxes, and that the large number of
accurate client geolocations should overwhelm any re-
maining incorrect locations.
Dealing with clients directed to distant frontends.
Even after ﬁltering bad geolocations, a client may be
geographically distant from the frontend it is mapped
to, for two reasons: the serving infrastructure may di-
rect clients to distant frontends for load-balancing, and
in some geographical regions, the serving infrastructure
deployment may be sparse so that the frontend nearest
to a client may still be geographically distant.
To prune these clients, CCG ﬁrst uses speed-of-light
constraints, as follows. It issues pings to the frontend
from all PlanetLab nodes and use the speed of light to
establish loose constraints on where the frontend could
possibly be [10]. When geolocating the frontend, CCG
excludes any clients outside of this region. This ex-
cludes 4 million out of 10 million preﬁxes (1.1 million
out of 2 million BitTorrent client preﬁxes). It then es-
timates the preliminary location for the frontend as the
weighted average of the locations of the remaining client
preﬁxes, then reﬁnes this estimate by calculating the
mean distance from the frontend to the remaining pre-
ﬁxes, and ﬁnds the standard deviation from the mean
of the client-to-frontend distances. Our ﬁnal ﬁlter ex-
cludes clients that are more than a standard deviation
beyond the mean distance to the frontend, excluding
392,668 out of 10 million preﬁxes (214,097 out of 2 mil-
lion BitTorrent client preﬁxes).
Putting it all together.
In summary, CCG works
as follows. It ﬁrst lists the set of preﬁxes directed to a
frontend, then ﬁlters out all preﬁxes except those ob-
served to host BitTorrent clients. Then, it uses Max-
Mind to geolocate those remaining client preﬁxes, but
excludes: preﬁxes without city-level MaxMind granu-
larity; preﬁxes that include addresses in multiple loca-
tions; preﬁxes for which the MaxMind location is not
in the feasible actual location based on speed-of-light
measurements from PlanetLab and M-Lab; and preﬁxes
outside the feasible location for the frontend. Its pre-
liminary estimate for the frontend location is the ge-
ographic mean of the remaining clients that it serves.
Calculating the distances from remaining clients to this
preliminary location, CCG further exclude any clients
more than a standard deviation beyond the mean dis-
tance in order to reﬁne our location estimate. Finally,
it locates the frontend as being at the geographic mean
of the remaining clients that it serves.
4.3 Clustering frontends
As we discuss later, CCG is accurate to within 10s
of kilometers.
In large metro areas, some serving in-
frastructures may have multiple serving sites, so we de-
velop a methodology to determine physically distinct
serving sites. We cluster by embedding each frontend
in a higher dimensional metric space, then clustering
the frontend in that metric space. Such an approach
has been proposed elsewhere [19, 34, 24] and our ap-
proach diﬀers from prior work in using better clustering
techniques and more carefully ﬁltering outliers.
In our technique, we map each frontend to a point
in high dimensional space, where the coordinates are
RTTs from landmarks (in our case, 250 PlanetLab nodes
at diﬀerent geographical sites). The intuition under-
lying our approach is that two frontends at the same
physical location should have a small distance in the
high-dimensional space.
Each coordinate is the smallest but one RTT of 8
consecutive pings, and we use the Manhattan distance
between two points for clustering.
In computing this
Manhattan distance, we (a) omit coordinates for which
we received fewer than 6 responses to pings and (b) omit
the highest 20% of coordinate distances to account for
outliers caused by routing failures, or by RTT measure-
ments inﬂated by congestion. Finally, we normalize this
Manhattan distance.
The ﬁnal step is to cluster frontends by their pairwise
normalized Manhattan distance. We use the OPTICS
algorithm [3] for this. OPTICS is designed for spa-
tial data, and, instead of explicitly clustering points,
it outputs an ordering of the points that captures the
density of points in the dataset. As such, OPTICS is
appropriate for spatial data where there may be no a
priori information about either the number of clusters
or their size, as is the case for our setting. In the out-
put ordering, each point is annotated with a reachabil-
ity distance: when successive points have signiﬁcantly
diﬀerent reachability distances, that is usually an indi-
cation of a cluster boundary. As we show in Section 5
this technique, which dynamically determines cluster
boundaries, is essential to achieving good accuracy.
5
Open resolver
EDNS-client-subnet
Beneﬁt
IPs
5182
7040
/24s
275
365
131
169
+36% +33% +29%
ASes Countries
59
60
+2%
Table 1: Comparison of Google frontends found by