title:A "hitchhiker's" guide to fast and efficient data reconstruction in
erasure-coded data centers
author:K. V. Rashmi and
Nihar B. Shah and
Dikang Gu and
Hairong Kuang and
Dhruba Borthakur and
Kannan Ramchandran
A “Hitchhiker’s” Guide to Fast and Efﬁcient Data
Reconstruction in Erasure-coded Data Centers
K. V. Rashmi1, Nihar B. Shah1, Dikang Gu2, Hairong Kuang2, Dhruba Borthakur2, and Kannan
Ramchandran1
1UC Berkeley
2Facebook
{rashmikv, nihar, kannanr}@eecs.berkeley.edu, {dikang, hairong, dhruba}@fb.com
ABSTRACT
Erasure codes such as Reed-Solomon (RS) codes are being
extensively deployed in data centers since they oﬀer signif-
icantly higher reliability than data replication methods at
much lower storage overheads. These codes however man-
date much higher resources with respect to network band-
width and disk IO during reconstruction of data that is miss-
ing or otherwise unavailable. Existing solutions to this prob-
lem either demand additional storage space or severely limit
the choice of the system parameters.
In this paper, we present Hitchhiker, a new erasure-coded
storage system that reduces both network traﬃc and disk IO
by around 25% to 45% during reconstruction of missing or
otherwise unavailable data, with no additional storage, the
same fault tolerance, and arbitrary ﬂexibility in the choice of
parameters, as compared to RS-based systems. Hitchhiker
“rides” on top of RS codes, and is based on novel encoding
and decoding techniques that will be presented in this paper.
We have implemented Hitchhiker in the Hadoop Distributed
File System (HDFS). When evaluating various metrics on
the data-warehouse cluster in production at Facebook with
real-time traﬃc and workloads, during reconstruction, we
observe a 36% reduction in the computation time and a 32%
reduction in the data read time, in addition to the 35% re-
duction in network traﬃc and disk IO. Hitchhiker can thus
reduce the latency of degraded reads and perform faster re-
covery from failed or decommissioned machines.
1.
INTRODUCTION
Data centers storing multiple petabytes of data have be-
come commonplace today. These data centers are typically
built out of individual components that can be unreliable,
and as a result, the system has to deal with frequent failures.
Various additional systems-related issues such as software
glitches, machine reboots and maintenance operations also
contribute to machines being rendered unavailable from time
to time. In order to ensure that the data remains reliable
and available despite frequent machine unavailability, data
is replicated across multiple machines, typically across mul-
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
SIGCOMM’14, August 17–22, 2014, Chicago, USA
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
Copyright 2014 ACM 978-1-4503-2836-4/14/08 ...$15.00
http://dx.doi.org/10.1145/2619239.2626325
Figure 1: High network usage during reconstruction
of erasure-coded data: reconstructing a single miss-
ing unit ‘a’ in this example requires transfer of twice
the amount of data through the top-of-rack (TOR)
switches.
tiple racks as well. For instance, the Google File System [10]
and the Hadoop Distributed File System (HDFS) [28] store
three copies of all data by default. Although disk storage
seems inexpensive for small data sizes, this assumption is
no longer true when storing multiple copies at the massive
scales of operation of today’s data centers. As a result, sev-
eral large-scale distributed storage systems [4,10] now deploy
erasure codes, that provide higher reliability at signiﬁcantly
lower storage overheads, with the most popular choice being
the family of Reed-Solomon (RS) codes [23].
An RS code is associated with two parameters: k and r.
A (k, r) RS code encodes k units of data into r parity units,
in a manner that guarantees the recoverability of all the k
data units from any k out of these (k + r) units. It thus
allows for tolerating unavailability of any r of its (k + r)
units. This collection of (k + r) units is called a stripe. In a
system deploying an RS code, the (k + r) units belonging to
a stripe are stored on distinct machines in order to maximize
diversity and tolerate maximum machine unavailability. In
addition, these machines are chosen from distinct racks in
order to maximize tolerance to rack failures. An example
of such a setting is depicted in Fig. 1, with an RS code
having parameters (k = 2, r = 2). Here {a, b} are the two
data units, which are encoded to generate two parity units,
(a+b) and (a+2b). The ﬁgure depicts these four units stored
across four machines on diﬀerent racks. The data-warehouse
cluster at Facebook employs an RS code with parameters
(k = 10, r = 4), thus resulting in a 1.4× storage overhead. In
contrast, for similar levels of reliability, a replication-based
system would require a 3× storage overhead.
We posit that the primary reason that makes RS codes
particularly attractive for large-scale distributed storage sys-
tems is its two following properties:
P1: Storage optimality. A (k, r) RS code entails the min-
imum storage overhead among all (k, r) erasure codes that
a b a + b a + 2b … … … … machine 1 machine 2 machine 3 machine 4 TOR TOR TOR TOR Router a + b b tolerate any r failures.1 This property is particularly rele-
vant for large-scale storage systems since disk space usage is
quickly becoming the dominant factor in these systems.
P2: Generic applicability. RS codes can be constructed
for arbitrary values of the parameters (k, r), thus allowing
complete ﬂexibility in the design of the system.
Although RS codes improve storage eﬃciency in data cen-
ters, they cause a signiﬁcant increase in the disk and network
traﬃc. This is due to the heavy download requirement dur-
ing reconstruction of any missing or otherwise unavailable
unit, as elaborated below. In a system that performs repli-
cation, a data unit can be reconstructed simply by copying
it from another existing replica. However, in an RS-coded
system, there is no such replica. To see the reconstruction
operation under an RS code, let us ﬁrst consider the exam-
ple of a (k = 2, r = 2) RS code as in Fig. 1. The ﬁgure
depicts the reconstruction of the ﬁrst data unit “a” (machine
1) from machines 2 and 3. Observe that this reconstruction
operation requires transfer of two units across the network.
In general, under a (k, r) RS code, reconstruction of a single
unit involves the download of k of the remaining units. An
amount equal to the logical size of the data in the stripe is
thus read from the disks and downloaded through the net-
work, from which the missing unit is reconstructed. This is
k times the size of the data to be reconstructed.
We
have
of RS-encoded
data.
performed
extensive measurements
on
Facebook’s data-warehouse cluster in production, which
consists of multiple thousands of nodes, and which stores
multiple Petabytes
These
measurements reveal that a median of more than 50
machine-unavailability events occur per day, and a median
of 95,500 blocks of RS-encoded data are recovered each day
(the typical size of a block is 256 Megabytes (MB)). The
reconstruction operations for RS-encoded data consume a
large amount of disk and cross-rack bandwidth: a median
of more than 180 Terabytes (TB) of data is transferred
through the top-of-rack switches every day for
this
purpose. For more details, the reader is referred to our
earlier work [20].
In addition to the increased toll on network and disk re-
sources, the signiﬁcant increase in the amount of data to be
read and downloaded during reconstruction aﬀects RS-based
storage systems in two ways. First, it drastically hampers
the read performance of the system in “degraded mode”, i.e.,
when there is a read request for a data unit that is missing
or unavailable. Serving such a request is called a ‘degraded
read’.
In a replication based system, degraded reads can
be performed very eﬃciently by serving it from one of the
replicas of the requisite data. On the other hand, the high
amount of data read and downloaded as well as the computa-
tional load for reconstructing any data block in an RS-based
system increases the latency of degraded reads. Second, it in-
creases the recovery time of the system: recovering a failed
machine or decommissioning a machine takes signiﬁcantly
longer than in a replication-based system. Based on con-
versations with teams from multiple enterprises that deploy
RS codes in their storage systems, we gathered that this in-
creased disk and network traﬃc and its impact on degraded
reads and recovery is indeed a major concern, and is one of
the bottlenecks to erasure coding becoming more pervasive
in large-scale distributed storage systems.
1In the parlance of coding theory, an RS code has the prop-
erty of being ‘Maximum Distance Separable (MDS)’ [16].
Storage optimality and generic applicability:
Storage requirement
Supported parameters
Fault tolerance
Same (optimal)
All
Same (optimal)
Data reconstruction:
Data downloaded (i.e., network traﬃc)
Data read (i.e., disk traﬃc)
Data read time (median)
Data read time (95th percentile)
Computation time (median)
Encoding:
Encoding time (median)
35% less
35% less
31.8% less
30.2% less
36.1% less
72.1% more
Table 1: Performance of Hitchhiker as compared to
Reed-Solomon-based system for default HDFS pa-
rameters.
The problem of decreasing the amount of data required to
be downloaded for reconstruction in erasure-coded systems
has received considerable attention in the recent past both
in theory and practice [7,11–14,17,19,21,22,26,27,29,30,32].
However, all existing practical solutions either demand ad-
ditional storage space [7, 12, 17, 21, 26, 27]), or are appli-
cable in very limited settings [11, 15, 30, 32]. For exam-
ple, [7, 12, 17] add at least 25% to 50% more parity units to
the code, thereby increasing the storage overheads, [21, 27]
necessitate a high redundancy of r ≥ (k − 1) in the system,
while [11,15,30,32] operate in a limited setting allowing only
two or three parity units.
In this paper, we present Hitchhiker, an erasure-coded
storage system that ﬁlls this void. Hitchhiker reduces both
network and disk traﬃc during reconstruction by 25% to 45%
without requiring any additional storage and maintaining the
same level of fault-tolerance as RS-based systems.2 Further-
more, Hitchhiker can be used with any choice of the sys-
tem parameters k and r, thus retaining both the attractive
properties of RS codes described earlier. Hitchhiker accom-
plishes this with the aid of two novel components proposed
in this paper: (i) a new encoding and decoding technique
that builds on top of RS codes and reduces the amount of
download required during reconstruction of missing or oth-
erwise unavailable data, (ii) a novel disk layout technique
that ensures that the savings in network traﬃc oﬀered by
the new code is translated to savings in disk traﬃc as well.
In proposing the new storage code, we make use of a re-
cently proposed theoretical framework [22] called the ‘Piggy-
backing framework’. In this paper, we employ this theoreti-
cal framework of piggybacking to design a novel erasure code
that reduces the amount of data required during reconstruc-
tion while maintaining the storage optimality and generic
applicability of RS codes. Our design oﬀers the choice of
either using it with only XOR operations resulting in sig-
niﬁcantly faster computations or with ﬁnite ﬁeld arithmetic
for a greater reduction in the disk and network traﬃc during
reconstruction. Interestingly, we also show that the XOR-
only design can match the savings of non-XOR design if the
underlying RS code satisﬁes a certain simple condition.
The proposed storage codes reduce the amount of down-
load required for data reconstruction, and this directly trans-
2It takes a free-ride on top of the RS-based system, retaining
all its desired properties, and hence the name ‘Hitchhiker’.
lates to reduction in network traﬃc. In the paper, we then
propose a novel disk layout which ensures that the savings
in network resources are also translated to savings in disk re-
sources. In fact, this technique is applicable to a number of
other recently proposed storage codes [11,15,19,21,26,27,29]
as well, and hence may be of independent interest. The pro-
posed codes also help reduce the computation time during
reconstruction as compared to the existing RS codes.
by
our measurements
Hitchhiker optimizes reconstruction of a single unit in a
stripe without compromising any of the two properties of
RS-based systems. Single unit reconstruction in a stripe is
the most common reconstruction scenario in practice, as
validated
from Facebook’s
data-warehouse cluster which reveal that 98.08% of all
recoveries involve recovering a single unit in a stripe (see
§6.5). Moreover, at any point in time, Hitchhiker can
alternatively perform the (non-optimized) reconstruction of
single or multiple units as
in RS-based systems by
connecting to any k of the remaining units. It follows that
any optimization or solution proposed outside the erasure
coding component of a storage system (e.g., [3, 5, 18]) can
be used in conjunction with Hitchhiker by simply treating
Hitchhiker as functionally equivalent
to an RS code,
thereby allowing for the beneﬁts of both solutions.
We have
ﬁle
distributed
open-source
implemented Hitchhiker
in the Hadoop
Distributed File System (HDFS). HDFS is one of the most
popular
systems with
widespread adoption in the industry. For example, multiple
tens of Petabytes are being stored via RS encoding in
HDFS at Facebook, a popular social-networking company.
We evaluated Hitchhiker on two clusters in Facebook’s
data centers, with the default HDFS parameters of (k =
10, r = 4). We ﬁrst deployed Hitchhiker on a test cluster
comprising 60 machines, and veriﬁed that the savings in the
amount of download during reconstruction is as guaranteed
by theory. We then evaluated various metrics of Hitchhiker
on the data-warehouse cluster in production consisting of
multiple thousands of machines, in the presence of ongoing
real-time traﬃc and workloads. We observed that Hitch-
hiker reduces the time required for reading data during re-
construction by 32%, and reduces the computation time dur-
ing reconstruction by 36%. Table 1 details the comparison
between Hitchhiker and RS-based systems with respect to
various metrics for (k = 10, r = 4).3 Based on our measure-
ments [20] of the amount of data transfer for reconstruction
of RS-encoded data in the data-warehouse cluster at Face-
book (discussed above), employing Hitchhiker would save
close to 62TB of disk and cross-rack traﬃc every day while
retaining the same storage overhead, reliability, and system
parameters.
In summary, we make the following contributions:
• Introduce Hitchhiker, a new erasure-coded storage system
that reduces both network and disk traﬃc during recon-