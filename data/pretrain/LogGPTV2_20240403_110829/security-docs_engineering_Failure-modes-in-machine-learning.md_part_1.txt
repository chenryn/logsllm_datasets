---
# This basic template provides core metadata fields for Markdown articles on learn.microsoft.com.
# Mandatory fields.
title: Failure Modes in Machine Learning
description: Machine Learning Threat Taxonomy
author: TerryLanfear
ms.author: terrylan
manager: rkarlin
ms.date: 11/11/2019
ms.topic: article
ms.prod: security
---

# Failure Modes in Machine Learning

Microsoft Corporation  
Berkman Klein Center for Internet and Society at Harvard University  
Ram Shankar Siva Kumar, David O’Brien, Jeffrey Snover, Kendra Albert, Salome Viljoen  
November 2019

## Introduction and Background

In the past two years, over 200 papers have been published on how machine learning (ML) can fail due to adversarial attacks on algorithms and data. The number of publications increases significantly if we include non-adversarial failure modes. This surge in research has made it challenging for ML practitioners, engineers, lawyers, and policymakers to keep up with the latest attacks and defenses of ML systems. As these systems become more pervasive, understanding their failure modes—whether caused by adversaries or inherent design flaws—becomes increasingly important.

This document aims to consolidate both intentional and unintentional failure modes into a single, comprehensive resource:

- **Intentional Failures:** These occur when an active adversary attempts to subvert the system, either to misclassify results, infer private training data, or steal the underlying algorithm.
- **Unintentional Failures:** These occur when an ML system produces a formally correct but unsafe outcome.

While other taxonomies and frameworks address either intentional or unintentional failure modes separately, our classification brings them together. This consolidation addresses several key needs:

1. **Common Vernacular:** Equip software developers, security incident responders, lawyers, and policymakers with a shared language to discuss ML failures. After developing the initial taxonomy, we collaborated with security and ML teams across Microsoft, 23 external partners, standards organizations, and governments to refine the framework. Our usability studies and stakeholder feedback highlighted the need to draw parallels between ML failures and traditional software attacks like data exfiltration, while also emphasizing the unique aspects of ML from a technology and policy perspective.

2. **Engineering Integration:** Provide a common platform for engineers to build upon and integrate into their existing software development and security practices. We aimed for the taxonomy to be more than just an educational tool; it should drive tangible engineering outcomes. Using this taxonomy, Microsoft updated its [Security Development Lifecycle (SDL)](https://www.microsoft.com/securityengineering/sdl/) process. Data scientists and security engineers now share a common language, enabling more effective threat modeling before deploying ML systems. Security Incident Responders also have a specific bug bar to triage ML-related threats, aligning with the standard vulnerability triage and response processes used by the Microsoft Security Response Center and all Microsoft product teams.

3. **Policy and Legal Framework:** Establish a common vocabulary for policymakers and lawyers to describe and regulate ML failures. This taxonomy is written for a broad, interdisciplinary audience, including policymakers focused on general ML/AI issues and specific domains such as misinformation and healthcare. We also highlight applicable legal interventions to address the identified failure modes.

For more information, see Microsoft's [Threat Modeling AI/ML Systems and Dependencies](/security/threat-modeling-aiml) and [SDL Bug Bar Pivots for Machine Learning Vulnerabilities](/security/engineering/bug-bar-aiml).

## How to Use This Document

This is a living document that will evolve with the changing threat landscape. We do not prescribe specific technological mitigations here, as defenses are scenario-specific and depend on the threat model and system architecture. The options presented for threat mitigation are based on current research and will likely evolve over time.

**For Engineers:**
- Browse through the overview of possible failure modes.
- Refer to the [threat modeling document](/security/threat-modeling-aiml) to identify threats, attacks, vulnerabilities, and plan countermeasures.
- Use the bug bar to map new ML vulnerabilities alongside traditional software vulnerabilities, providing a rating (e.g., critical, important). This can be easily integrated into existing incident response processes and playbooks.

**For Lawyers and Policymakers:**
- This document organizes ML failure modes and presents a framework to analyze key issues relevant for policy options.
- Categorize failures and consequences to help policymakers draw distinctions between causes, informing public policy initiatives to promote ML safety and security.
- Use these categories to evaluate how existing legal regimes may (or may not) adequately address emerging issues, and where civil liberties concerns should be prioritized.

## Document Structure

### Intentional Failure Modes
- **Definition:** A brief definition of the attack.
- **Example:** An illustrative example from literature.
- **Additional Fields:**
  1. **CIA Triad Compromise:** Which aspect of the Confidentiality, Integrity, or Availability (CIA) triad is compromised?
  2. **Knowledge Requirement:** Whether the attack requires blackbox or whitebox knowledge.
  3. **Access/Authorization Violation:** Commentary on whether the attacker violates traditional technological notions of access/authorization.

### Unintentional Failure Modes
- **Definition:** A brief definition of the failure.
- **Example:** An illustrative example from literature.

## Summary of Intentionally-Motivated Failures

| Scenario Number | Attack | Overview | Violates Traditional Access/Authorization? |
|-----------------|--------|----------|--------------------------------------------|
| 1               | Perturbation Attack | Attacker modifies the query to get an appropriate response. | No |
| 2               | Poisoning Attack | Attacker contaminates the training phase of ML systems to achieve a desired result. | No |
| 3               | Model Inversion | Attacker recovers the secret features used in the model through careful queries. | No |
| 4               | Membership Inference | Attacker infers if a given data record was part of the model’s training dataset. | No |
| 5               | Model Stealing | Attacker recovers the model through carefully-crafted queries. | No |
| 6               | Reprogramming ML System | Repurposes the ML system to perform an activity it was not programmed for. | No |
| 7               | Adversarial Example in Physical Domain | Attacker brings adversarial examples into the physical domain to subvert the ML system (e.g., 3D printing special eyewear to fool facial recognition). | No |
| 8               | Malicious ML Provider Recovering Training Data | Malicious ML provider queries the model used by a customer to recover the customer’s training data. | Yes |
| 9               | Attacking the ML Supply Chain | Attacker compromises the ML models during the download process. | Yes |
| 10              | Backdoor ML | Malicious ML provider backdoors the algorithm to activate with a specific trigger. | Yes |
| 11              | Exploit Software Dependencies | Attacker uses traditional software exploits (e.g., buffer overflow) to confuse or control ML systems. | Yes |

## Summary of Unintended Failures

| Scenario # | Failure | Overview |
|------------|---------|----------|
| 12         | Reward Hacking | Reinforcement Learning (RL) systems act in unintended ways due to a mismatch between stated and true rewards. |
| 13         | Side Effects | RL system disrupts the environment while trying to achieve its goal. |
| 14         | Distributional Shifts | The system performs well in one environment but fails to adapt to changes in another. |
| 15         | Natural Adversarial Examples | The ML system fails without attacker perturbations, often due to hard negative mining. |
| 16         | Common Corruption | The system cannot handle common corruptions and perturbations such as tilting, zooming, or noisy images. |
| 17         | Incomplete Testing | The ML system is not tested in realistic conditions, leading to unexpected failures. |

---

This structure and content provide a clear, organized, and professional overview of the different failure modes in machine learning, making it easier for various stakeholders to understand and address these issues.