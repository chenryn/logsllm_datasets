---
# This basic template provides core metadata fields for Markdown articles on learn.microsoft.com.
# Mandatory fields.
title: Failure Modes in Machine Learning
description: Machine Learning Threat Taxonomy
author: TerryLanfear
ms.author: terrylan
manager: rkarlin
ms.date: 11/11/2019
ms.topic: article
ms.prod: security
---
# Failure Modes in Machine Learning
    Microsoft Corporation
    Berkman Klein Center for Internet and Society at Harvard University
    Ram Shankar Siva Kumar
    David O’Brien
    Jeffrey Snover
    Kendra Albert
    Salome Viljoen
November 2019
## Introduction & Background
In the last two years, more than 200 papers have been written on how
Machine Learning (ML) can fail because of adversarial attacks on the
algorithms and data; this number balloons if we were to incorporate
non-adversarial failure modes. The spate of papers has made it difficult
for ML practitioners, let alone engineers, lawyers and policymakers, to
keep up with the attacks against and defenses of ML systems. However, as
these systems become more pervasive, the need to understand how they
fail, whether by the hand of an adversary or due to the inherent design
of a system, will only become more pressing. The purpose of this
document is to jointly tabulate both the of these failure modes in a
single place.
-   *Intentional failures* wherein the failure is caused by an active
    adversary attempting to subvert the system to attain her goals –
    either to misclassify the result, infer private training data, or to
    steal the underlying algorithm.
-   *Unintentional failures* wherein the failure is because an ML system
    produces a formally correct but completely unsafe outcome.
We would like to point out that there are other taxonomies and
frameworks that individually highlight intentional failure
modes[1],[2] and unintentional failure
modes[3],[4]. Our classification brings the two separate
failure modes together in one place and addresses the following needs:
1.  The need to equip software developers, security incident responders, lawyers, and policy makers with a common vernacular to talk about this problem. After developing the initial version of the taxonomy last year, we worked with security and ML teams across Microsoft, 23 external partners, standards organization, and governments to understand how stakeholders would use our framework. Based on this usability study and stakeholder feedback, we iterated on the framework.
    *Results:* When presented with an ML failure mode, we frequently observed that software developers and lawyers mentally mapped the ML failure modes to traditional software attacks like data exfiltration. So, throughout the paper, we attempt to highlight how machine learning failure modes are meaningfully different from traditional software failures from a technology and policy perspective.
2.  The need for a common platform for engineers to build on top of and to integrate into their existing software development and security practices. Broadly, we wanted the taxonomy to be more than an educational tool – we want it to effectuate tangible engineering outcomes.
    *Results:* Using this taxonomy as a lens, Microsoft modified its
    [Security Development Lifecycle](https://www.microsoft.com/securityengineering/sdl/) process for its entire organization.
    Specifically, data scientists and security engineers at Microsoft now
    share the common language of this taxonomy, allowing them to more effectively threat model their ML systems before
    deploying to production; Security Incident Responders also have a
    bug bar to triage these net-new threats specific to ML, the standard process for vulnerabilities triage and response used by the Microsoft Security Response Center and all Microsoft product teams.  
3.  The need for a common vocabulary to describe these attacks amongst policymakers and lawyers. We believe that this  for describing different ML failure modes and analysis of how their harms might be regulated is a meaningful first step towards informed policy.
    *Results:* This taxonomy is written for a wide interdisciplinary audience – so, policymakers who are looking at the issues from a general ML/AI perspective, as well as specific domains such as misinformation/healthcare should find the failure mode catalogue useful. We also highlight any applicable legal interventions to address the failure modes.
See also Microsoft's [Threat Modeling AI/ML Systems and Dependencies](/security/threat-modeling-aiml) and [SDL Bug Bar Pivots for Machine Learning Vulnerabilities](/security/engineering/bug-bar-aiml).
## How to use this document
At the outset, we acknowledge that this is a living document which will evolve over time with the threat landscape.
We also do not prescribe technological
mitigations to these failure modes here, as defenses are scenario-specific
and tie in with the threat model and system architecture under consideration.  Options presented for threat mitigation are based on current research with the expectation that those defenses will evolve over time as well.
For engineers, we recommend browsing through the overview of possible failure modes and jumping into the [threat modeling document](/security/threat-modeling-aiml). This way,
engineers can identify threats, attacks, vulnerabilities and use the
framework to plan for countermeasures where available. We then refer you
to the bug bar that maps these new vulnerabilities in the taxonomy
alongside traditional software vulnerabilities, and provides a rating
for each ML vulnerability (such as critical, important). This bug bar
is easily integrated into existing incident response processes/playbooks.
For lawyers and policy makers, this document organizes ML failure modes
and presents a framework to analyze key issues relevant for
anyone exploring policy options, such as the work done
here[5],[6]. Specifically, we have categorized failures and
consequences in a way that policy makers can begin to draw distinctions
between causes, which will inform the public policy initiatives to
promote ML safety and security. We hope that policy makers will use
these categories begin to flesh out how existing legal regimes may (not)
adequately capture emerging issues, what historical legal regimes or
policy solutions might have dealt with similar harms, and where we
should be especially sensitive to civil liberties issues.
## Document Structure
In both the *Intentional Failure Modes* and *Unintentional Failure
Modes* sections, we provide a brief definition of the attack, and
an illustrative example from literature.
In the *Intentional Failure Modes* section, we provide the additional
fields:
1. What does the attack attempt to compromise in the ML system – Confidentiality, Integrity or Availability? We define Confidentiality as assuring that the components of the ML system (data, algorithm, model) are accessible only by authorized parties; Integrity is defined as assuring that the ML system can be modified only by authorized parties; Availability is defined as an assurance that the ML system is accessible to authorized parties. Together, Confidentiality, Integrity and Availability is called the CIA triad. For each intentional failure mode, we attempt to identify which of the CIA triad is compromised.
2. How much knowledge is required to mount this attack – blackbox or whitebox? In Blackbox style attacks., the attacker does NOT have direct access to the training data, no knowledge of the ML algorithm used and no access to the source code of the model. The attacker only queries the model and observes the response. In a whitebox style attack the attacker has knowledge of either ML algorithm or access to the model source code.  
3. Commentary on if the attacker is violating traditional technological notion of access/authorization.
## Intentionally-Motivated Failures Summary
    Scenario Number
    Attack
    Overview
    Violates traditional technological notion of access/authorization?
    1
    Perturbation attack 
    Attacker modifies the query to get appropriate response
    No
    2
    Poisoning attack 
    Attacker contaminates the training phase of ML systems to get intended result
    No
    3
    Model Inversion
    Attacker recovers the secret features used in the model by through careful queries
    No
    4
    Membership Inference
    Attacker can infer if a given data record was part of the model’s training dataset or not
    No
    5
    Model Stealing
    Attacker is able to recover the model through carefully-crafted queries
    No
    6
    Reprogramming ML system
    Repurpose the ML system to perform an activity it was not programmed for
    No
    7
    Adversarial Example in Physical Domain 
    Attacker brings adversarial examples into physical domain to subvertML system e.g: 3d printing special eyewear to fool facial recognition system
    No
    8
    Malicious ML provider recovering training data 
    Malicious ML provider can query the model used by customer and recover customer’s training data 
    Yes
    9
    Attacking the ML supply chain
    Attacker compromises the ML models as it is being downloaded for use
    Yes
    10
    Backdoor ML 
    Malicious ML provider backdoors algorithm to activate with a specific trigger
    Yes
    11
    Exploit Software Dependencies
    Attacker uses traditional software exploits like buffer overflow to confuse/control ML systems
    Yes
## Unintended Failures Summary
    Scenario #
    Failure
    Overview
    12
    Reward Hacking
    Reinforcement Learning (RL) systems act in unintended ways because of mismatch between stated reward and true reward
    13
    Side Effects
    RL system disrupts the environment as it tries to attain its goal
    14
    Distributional shifts
    The system is tested in one kind of environment, but is unable to adapt to changes in other kinds of environment
    15
    Natural Adversarial Examples
    Without attacker perturbations, the ML system fails owing to hard negative mining
    16
    Common Corruption
    The system is not able to handle common corruptions and perturbations such as tilting, zooming, or noisy images.
    17
    Incomplete Testing
    The ML system is not tested in the realistic conditions that it is meant to operate in.