### 8.3 Toward a Fuzzing Benchmark Suite

Our assessment indicates a significant need for a well-defined, independently established benchmark suite for fuzz testing, similar to DaCapo [4] or SPEC10. This is a substantial undertaking that we do not aim to complete in this paper, but rather propose as a community-driven effort. However, we can offer some insights into what such a benchmark suite might entail.

First, the suite should include a variety of programs with clearly identifiable indicators of when specific bugs are discovered. This can be achieved by either synthetically introducing bugs (as in LAVA-M and CGC) or by using known bugs from previous versions (as in our ground truth assessment in Section 7.2). Clear knowledge of the ground truth helps avoid overcounting inputs corresponding to the same bug and allows for accurate evaluation of a tool's false positives and false negatives. We prefer using real programs with known bugs due to their higher ecological validity.

Second, the suite must be sufficiently large in both the number of programs and the size of those programs to represent the overall target population. The optimal number of programs remains an open question. For instance, CGC includes approximately 300 small programs, while the Google Fuzzer Suite has 25, and most papers use around 7. Our opinion is that 7 is too few, though the selection of these 7 programs could influence the results. A suite of 25 programs may be closer to the ideal number.

Finally, the testing methodology should incorporate measures to prevent overfitting. If a static benchmark suite becomes widely used, tools might develop heuristics and strategies that are overly specific to the benchmark programs. One approach to address this is to have a fixed standard suite and an "evolvable" part that changes frequently. This can be supported by setting up a recurring fuzzing competition, similar to the long-running series of SAT solving competitions [11]. An example of such an effort is Rode0day, a recurring bug-finding competition [12]. Since the target programs would not be known to researchers in advance, they would be motivated to develop general, reusable techniques. Each competition’s suite could be integrated into the static benchmark, at least partially, to enhance its robustness. A challenge here is to regularly develop new targets that are ecologically valid. For example, Rode0day uses automated bug insertion techniques, which could lead to overfitting (a concern also discussed for LAVA).

### 9. Conclusions and Future Work

Fuzz testing is a promising technology that has been instrumental in uncovering numerous important bugs and security vulnerabilities. This potential has spurred a growing number of researchers to develop new fuzz testing algorithms. The effectiveness of these algorithms is primarily evaluated through experiments, making it crucial to employ a well-founded experimental methodology. Specifically, a researcher should run their algorithm A on a diverse set of target programs, using a meaningful set of configuration parameters, including input seeds and duration (timeout), and compare it against a baseline algorithm B under the same conditions. Performance should be measured by the number of distinct bugs found, and multiple runs should be conducted to account for the inherent randomness in fuzzing, with performance judged via statistical tests.

In this paper, we surveyed 32 recent papers and analyzed their experimental methodologies. We found that no paper fully adheres to the methodology outlined above. Furthermore, our experiments using AFLFast [6] (as A) and AFL [1] (as B) illustrate why following this methodology is essential to avoid misleading or weakened conclusions. Our findings include:

- **Multiple Runs and Statistical Tests**: Most papers did not perform multiple runs, and those that did failed to use statistical tests to account for varying performance. Our experiments showed that run-to-run performance can vary substantially.
- **Bug Counting Heuristics**: Many papers measured fuzzer performance by counting "unique crashes" using heuristics like AFL’s coverage measure and stack hashes. Our experiments demonstrated that these heuristics can significantly over-count the number of bugs and may suppress bugs by incorrectly grouping crashing inputs.
- **Timeouts**: Many papers used short timeouts without justification. Our experiments indicated that longer timeouts are often necessary to fully assess an algorithm’s performance.
- **Seed Choices**: Many papers did not carefully consider the impact of seed choices on algorithmic improvements. Our experiments showed that performance can vary substantially depending on the seeds used.
- **Target Programs**: Papers varied widely in their choice of target programs. A growing number are using synthetic suites like CGC and LAVA-M, which have the advantage of independent definition and reliable bug counting. Other papers often selected small, disjoint sets of programs, making it difficult to compare results across studies. Our experiments showed that AFLFast performs well on its original targets but no better than AFL on other targets.

Overall, our experiments roughly matched the positive results of the original AFLFast paper [6], but expanding the scope to different seeds, longer timeouts, and different target programs weakened the evidence of AFLFast’s superiority. The questionable value of heuristic crash de-duplication further undermines confidence in claims of improvement. We believe many prior papers suffer from similar issues in their evaluations.

To draw more reliable conclusions, we recommend that fuzz testing evaluations should include:
- Multiple trials with statistical tests to distinguish distributions.
- A range of benchmark target programs with known bugs (e.g., LAVA-M, CGC, or old programs with bug fixes).
- Measurement of performance in terms of known bugs, rather than heuristics based on AFL coverage profiles or stack hashes; block or edge coverage can be used as a secondary measure.
- Consideration of various (well-documented) seed choices, including the empty seed.
- Timeouts of at least 24 hours, or else justification for shorter durations, with performance plotted over time.

We see at least three important lines of future work:
1. **Benchmark Suite Development**: There is a pressing need for a well-designed, well-assessed benchmark suite, as described in the previous section.
2. **Crash De-duplication Studies**: It would be valuable to conduct a larger study on the effectiveness of crash de-duplication methods in realistic fuzzing runs and potentially develop new, more effective methods.
3. **Algorithm Enhancements**: Exploring enhancements to the fuzzing algorithm inspired by the observation that no single fuzzing run found all true bugs in cxxfilt. Ideas from other search algorithms, like SAT solving "reboots" [46], could be applied.

### Acknowledgments

We thank Marcel Böhme and Abhik Roychoudhury for their help with AFLFast. We also thank the anonymous reviewers, Michelle Mazurek, Cornelius Aschermann, Luis Pina, Jeff Foster, Ian Sweet, the participants of the ISSISP’18 summer school, and our shepherd Mathias Payer for their helpful comments and suggestions. This research was supported in part by the National Science Foundation grants CNS-1563722 and CNS-1314857, and DARPA under contracts FA8750-15-2-0104 and FA8750-16-C-0022, and a Google Research Award.

### References

[References listed as provided in the original text]

This version of the text is more structured, coherent, and professional, with improved clarity and flow.