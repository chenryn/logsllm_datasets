extent to which these suites can be considered “real” or “general.”
8.3 Toward a Fuzzing Benchmark Suite
Our assessment leads us to believe that there is a real need for a
solid, independently defined benchmark suite, e.g., a DaCapo [4]
or SPEC10 for fuzz testing. This is a big enough task that we do
not presume to take it on in this paper. It should be a community
effort. That said, we do have some ideas about what the result of
that effort might look like.
First, we believe the suite should have a selection of programs
with clear indicators of when particular bugs are found, either
because bugs are synthetically introduced (as in LAVA-M and CGC)
or because they were previously discovered in older versions (as
in our ground truth assessment in Section 7.2). Clear knowledge
of ground truth avoids overcounting inputs that correspond to the
same bug, and allows for assessing a tool’s false positives and false
negatives. We lean toward using real programs with known bugs
simply because their ecological validity is more assured.
Second, the suite should be large enough (both in number of
programs, and those programs’ sizes) to represent the overall tar-
get population. How many programs is the right number? This is
an open question. CGC comprises ∼ 300 small programs; Google
Fuzzer Suite has 25; most papers used around 7. Our feeling is that
7 is too small, but it might depend on which 7 are chosen. Perhaps
25 is closer to the right number.
Finally, the testing methodology should build in some defense
against overfitting. If a static benchmark suite comes into common
use, tools may start to employ heuristics and strategies that are
not of fundamental advantage, but apply disproportionately to the
benchmark programs. One way to deal with this problem is to
have a fixed standard suite and an “evolvable” part that changes
relatively frequently. One way to support the latter is to set up a
fuzzing competition, similar to long-running series of SAT solving
competitions.11 One effort in this direction is Rode0day, a recur-
ring bug finding competition.12 Since the target programs would
not be known to fuzzing researchers in advance, they should be
incentivized to develop general, reusable techniques. Each compe-
tition’s suite could be rolled into the static benchmark, at least in
part, to make the suite even more robust. One challenge is to regu-
larly develop new targets that are ecologically valid. For example,
Rode0day uses automated bug insertion techniques to which a tool
could overfit (the issue discussed above for LAVA).
9 CONCLUSIONS AND FUTURE WORK
Fuzz testing is a promising technology that has been used to uncover
many important bugs and security vulnerabilities. This promise
has prompted a growing number of researchers to develop new
fuzz testing algorithms. The evidence that such algorithms work
is primarily experimental, so it is important that it comes from a
well-founded experimental methodology. In particular, a researcher
should run their algorithm A on a general set of target programs,
10https://www.spec.org/benchmarks.html
11http://www.satcompetition.org/
12https://rode0day.mit.edu/
using a meaningful set of configuration parameters, including the
set of input seeds and duration (timeout), and compare against
the performance of a baseline algorithm B run under the same
conditions, where performance is defined as the number of (distinct)
bugs found. A and B must be run enough times that the inherent
randomness of fuzzing is accounted for and performance can be
judged via a statistical test.
In this paper, we surveyed 32 recent papers and analyzed their
experimental methodologies. We found that no paper completely
follows the methodology we have outlined above. Moreover, results
of experiments we carried out using AFLFast [6] (as A) and AFL [1]
(as B) illustrate why not following this methodology can lead to
misleading or weakened conclusions. We found that
• Most papers failed to perform multiple runs, and those that did
failed to account for varying performance by using a statistical
test. This is a problem because our experiments showed that
run-to-run performance can vary substantially.
• Many papers measured fuzzer performance not by counting
distinct bugs, but instead by counting “unique crashes” using
heuristics such as AFL’s coverage measure and stack hashes.
This is a problem because experiments we carried out showed
that the heuristics can dramatically over-count the number
of bugs, and indeed may suppress bugs by wrongly grouping
crashing inputs. This means that apparent improvements may
be modest or illusory. Many papers made some consideration
of root causes, but often as a “case study” rather than a perfor-
mance assessment.
• Many papers used short timeouts, without justification. Our
experiments showed that longer timeouts may be needed to
paint a complete picture of an algorithm’s performance.
• Many papers did not carefully consider the impact of seed
choices on algorithmic improvements. Our experiments showed
that performance can vary substantially depending on what
seeds are used. In particular, two different non-empty inputs
need not produce similar performance, and the empty seed can
work better than one might expect.
• Papers varied widely on their choice of target programs. A
growing number are using synthetic suites CGC and/or LAVA-
M, which have the advantage that they are defined indepen-
dently of a given algorithm, and bugs found by fuzzing them
can be reliably counted (no crash de-duplication strategy is
needed). Other papers often picked small, disjoint sets of pro-
grams, making it difficult to compare results across papers. Our
experiments showed AFLFast performs well on the targets it
was originally assessed against, but performed no better than
AFL on two targets used by other papers.
Ultimately, our experiments roughly matched the positive results
of the original AFLFast paper [6], but by expanding the scope of
the evaluation to different seeds, longer timeouts, and different
target programs, evidence of AFLFast’s superiority, at least for the
versions we tested, was weakened. The fact that heuristic crash de-
duplication strategies are of questionable value further weakens our
confidence in claims of improvement. We believe the same could
be said of many prior papers—all suffer from problems in their
evaluation to some degree. As such, a key conclusion of this paper
is that the fuzzing community needs to start carrying out more
rigorous experiments in order to draw more reliable conclusions.
Specifically, we recommend that fuzz testing evaluations should
have the following elements:
• multiple trials with statistical tests to distinguish distributions;
• a range of benchmark target programs with known bugs (e.g.,
• measurement of performance in terms of known bugs, rather
than heuristics based on AFL coverage profiles or stack hashes;
block or edge coverage can be used as a secondary measure;
• a consideration of various (well documented) seed choices in-
• timeouts of at least 24 hours, or else justification for less, with
LAVA-M, CGC, or old programs with bug fixes);
cluding empty seed;
performance plotted over time.
We see (at least) three important lines of future work. First, we
believe there is a pressing need for well-designed, well-assessed
benchmark suite, as described at the end of the last section. Second,
and related, it would be worthwhile to carry out a larger study of
the value of crash de-duplication methods on the results of realistic
fuzzing runs, and potentially develop new methods that work better,
for assisting with triage and assessing fuzzing when ground truth is
not known. Recent work shows promise [42, 51]. Finally, it would
be interesting to explore enhancements to the fuzzing algorithm
inspired by the observation that no single fuzzing run found all
true bugs in cxxfilt; ideas from other search algorithms, like SAT
solving “reboots” [46], might be brought to bear.
Acknowledgments. We thank Marcel Böhme and Abhik Roy-
choudhury for their help with AFLFast. We thank the anonymous
reviewers, Michelle Mazurek, Cornelius Aschermann, Luis Pina,
Jeff Foster, Ian Sweet, the participants of the ISSISP’18 summer
school, and our shepherd Mathias Payer for helpful comments and
suggestions on drafts of this work. This research was supported in
part by the National Science Foundation grants CNS-1563722 and
CNS-1314857, and DARPA under contracts FA8750-15-2-0104 and
FA8750-16-C-0022, and a Google Research Award.
REFERENCES
[1] AFL 2018. American Fuzzing Lop (AFL). http://lcamtuf.coredump.cx/afl/.
[2] Andrea Arcuri and Lionel Briand. 2011. A Practical Guide for Using Statistical
Tests to Assess Randomized Algorithms in Software Engineering. In International
Conference on Software Engineering (ICSE).
[3] BFF 2018. CERT Basic Fuzzing Framework (BFF). https://github.com/CERTCC/
certfuzz.
[4] Stephen M. Blackburn, Robin Garner, Chris Hoffmann, Asjad M. Khang, Kathryn S.
McKinley, Rotem Bentzur, Amer Diwan, Daniel Feinberg, Daniel Frampton,
Samuel Z. Guyer, Martin Hirzel, Antony Hosking, Maria Jump, Han Lee, J. Eliot B.
Moss, Aashish Phansalkar, Darko Stefanović, Thomas VanDrunen, Daniel von
Dincklage, and Ben Wiedermann. 2006. The DaCapo Benchmarks: Java Bench-
marking Development and Analysis. In ACM SIGPLAN Conference on Object-
oriented Programming Systems, Languages, and Applications (OOPSLA).
[5] Marcel Böhme, Van-Thuan Pham, Manh-Dung Nguyen, and Abhik Roychoudhury.
2017. Directed Greybox Fuzzing. In ACM SIGSAC Conference on Computer and
Communications Security (CCS).
[6] Marcel Böhme, Van-Thuan Pham, and Abhik Roychoudhury. 2016. Coverage-
based Greybox Fuzzing As Markov Chain. In ACM SIGSAC Conference on Com-
puter and Communications Security (CCS).
[7] Guillaume Calmettes, Gordon B. Drummond, and Sarah L. Vowler. 2012. Making
due with what we have: use your bootstraps. Journal of Physiology 590, 15 (2012).
[8] Sang Kil Cha, Thanassis Avgerinos, Alexandre Rebert, and David Brumley. 2012.
Unleashing Mayhem on Binary Code. In IEEE Symposium on Security and Privacy
(S&P).
[9] Sang Kil Cha, Maverick Woo, and David Brumley. 2015. Program-Adaptive
Mutational Fuzzing. In IEEE Symposium on Security and Privacy (S&P).
[10] Peng Chen and Hao Chen. 2018. Angora: Efficient Fuzzing by Principled Search.
In IEEE Symposium on Security and Privacy (S&P).
[11] Yang Chen, Alex Groce, Chaoqiang Zhang, Weng-Keen Wong, Xiaoli Fern, Eric
Eide, and John Regehr. 2013. Taming Compiler Fuzzers. In ACM SIGPLAN Con-
ference on Programming Language Design and Implementation (PLDI).
[12] ConfIntv 2018. Confidence Intervals for a Median. http://www.ucl.ac.uk/
ich/short-courses-events/about-stats-courses/stats-rm/Chapter_8_Content/
confidence_interval_single_median.
[13] Jake Corina, Aravind Machiry, Christopher Salls, Yan Shoshitaishvili, Shuang
Hao, Christopher Kruegel, and Giovanni Vigna. 2017. DIFUZE: Interface Aware
Fuzzing for Kernel Drivers. In ACM SIGSAC Conference on Computer and Com-
munications Security (CCS).
[14] DARPA CGC 2018. Darpa Cyber Grand Challenge (CGC) Binaries. https://github.
[15] Brendan Dolan-Gavitt. 2018. Of Bugs and Baselines. http://moyix.blogspot.com/
com/CyberGrandChallenge/.
2018/03/of-bugs-and-baselines.html.
[16] Brendan Dolan-Gavitt, Patrick Hulin, Engin Kirda, Tim Leek, Andrea Mambretti,
William K. Robertson, Frederick Ulrich, and Ryan Whelan. 2016. LAVA: Large-
Scale Automated Vulnerability Addition. In IEEE Symposium on Security and
Privacy (S&P).
[17] Gordon B. Drummond and Sarah L. Vowler. 2012. Different tests for a difference:
how do we research? British Journal of Pharmacology 165, 5 (2012).
[18] FuzzerTestSuite 2018.
Fuzzer Test Suite.
https://github.com/google/
fuzzer-test-suite.
[19] Rahul Gopinath, Carlos Jensen, and Alex Groce. 2014. Code Coverage for Suite
Evaluation by Developers. In International Conference on Software Engineering
(ICSE).
[20] Gustavo Grieco, Martín Ceresa, and Pablo Buiras. 2016. QuickFuzz: an automatic
random fuzzer for common file formats. In International Symposium on Haskell.
[21] Gustavo Grieco, Martn Ceresa, Agustn Mista, and Pablo Buiras. 2017. QuickFuzz
Testing for Fun and Profit. J. Syst. Softw. (2017).
[22] Istvan Haller, Asia Slowinska, Matthias Neugschwandtner, and Herbert Bos. 2013.
Dowsing for Overflows: A Guided Fuzzer to Find Buffer Boundary Violations. In
USENIX Security Symposium.
[23] HyungSeok Han and Sang Kil Cha. 2017. IMF: Inferred Model-based Fuzzer. In
ACM SIGSAC Conference on Computer and Communications Security (CCS).
[24] Wookhyun Han, Byunggill Joe, Byoungyoung Lee, Chengyu Song, and Insik Shin.
2018. Enhancing Memory Error Detection for Large-Scale Applications and Fuzz
Testing. In Network and Distributed System Security Symposium (NDSS).
[25] Andrew Henderson, Heng Yin, Guang Jin, Hao Han, and Hongmei Deng. 2017.
VDF: Targeted Evolutionary Fuzz Testing of Virtual Devices. In Research in
Attacks, Intrusions, and Defenses (RAID).
[26] Michael Hicks. 2015. What is a bug? http://www.pl-enthusiast.net/2015/09/08/
what-is-a-bug/.
[27] Antonio Ken Iannillo, Roberto Natella, Domenico Cotroneo, and Cristina Nita-
Rotaru. 2017. Chizpurfle: A Gray-Box Android Fuzzer for Vendor Service Cus-
tomizations. In IEEE International Symposium on Software Reliability Engineering
(ISSRE).
[28] Laura Inozemtseva and Reid Holmes. 2014. Coverage is Not Strongly Correlated
with Test Suite Effectiveness. In International Conference on Software Engineering
(ICSE).
[29] Ulf Kargén and Nahid Shahmehri. 2015. Turning Programs Against Each Other:
High Coverage Fuzz-testing Using Binary-code Mutation and Dynamic Slicing.
In Foundations of Software Engineering (FSE).
[30] P. S. Kochhar, F. Thung, and D. Lo. 2015. Code coverage and test suite effectiveness:
Empirical study with real bugs in large systems. In IEEE International Conference
on Software Analysis, Evolution, and Reengineering (SANER).
[31] lcamtuf. 2018.
AFL quick start guide.
http://lcamtuf.coredump.cx/afl/
QuickStartGuide.txt.
[32] Caroline Lemieux and Koushik Sen. 2018. FairFuzz: A Targeted Mutation Strat-
IEEE/ACM International
egy for Increasing Greybox Fuzz Testing Coverage.
Conference on Automated Software Engineering.
[33] Yuekang Li, Bihuan Chen, Mahinthan Chandramohan, Shang-Wei Lin, Yang Liu,
and Alwen Tiu. 2017. Steelix: program-state based binary fuzzing. In Foundations
of Software Engineering (FSE).
[34] libFuzzer 2018. libFuzzer. https://llvm.org/docs/LibFuzzer.html.
[35] Ying-Dar Lin, Feng-Ze Liao, Shih-Kun Huang, and Yuan-Cheng Lai. 2015. Browser
fuzzing by scheduled mutation and generation of document object models. In
International Carnahan Conference on Security Technology.
[36] David Molnar, Xue Cong Li, and David A. Wagner. 2009. Dynamic Test Gener-
ation to Find Integer Bugs in x86 Binary Linux Programs. In USENIX Security
Symposium.
[37] Todd Mytkowicz, Amer Diwan, Matthias Hauswirth, and Peter F. Sweeney. 2009.
Producing Wrong Data Without Doing Anything Obviously Wrong!. In Inter-
national Conference on Architectural Support for Programming Languages and
Operating Systems (ASPLOS).
[38] R. Lyman Ott and Micheal T. Longnecker. 2006. Introduction to Statistical Methods
[39] Hui Peng, Yan Shoshitaishvili, and Mathias Payer. 2018. T-Fuzz: fuzzing by
and Data Analysis (with CD-ROM).
program transformation. In IEEE Symposium on Security and Privacy (S&P).
[40] Theofilos Petsios, Adrian Tang, Salvatore J. Stolfo, Angelos D. Keromytis, and
Suman Jana. 2017. NEZHA: Efficient Domain-Independent Differential Testing.
In IEEE Symposium on Security and Privacy (S&P).
[41] Theofilos Petsios, Jason Zhao, Angelos D. Keromytis, and Suman Jana. 2017.
SlowFuzz: Automated Domain-Independent Detection of Algorithmic Complexity
Vulnerabilities. In ACM SIGSAC Conference on Computer and Communications
Security (CCS).
[42] Van-Thuan Pham, Sakaar Khurana, Subhajit Roy, and Abhik Roychoudhury. 2017.
Bucketing Failing Tests via Symbolic Analysis. In International Conference on
Fundeamental Approaches to Software Engineering (FASE).
[43] Radamsa 2018. Radamsa. https://github.com/aoh/radamsa.
[44] Sanjay Rawat, Vivek Jain, Ashish Kumar, Lucian Cojocar, Cristiano Giuffrida, and
Herbert Bos. 2017. Vuzzer: Application-aware evolutionary fuzzing. In NDSS.
[45] Alexandre Rebert, Sang Kil Cha, Thanassis Avgerinos, Jonathan Foote, David
Warren, Gustavo Grieco, and David Brumley. 2014. Optimizing Seed Selection
for Fuzzing. In USENIX Security Symposium.
[46] Vadim Ryvchin and Ofer Strichman. 2008. Local Restarts. In International Con-
ference on Theory and Applications of Satisfiability Testing (SAT).
[47] Sergej Schumilo, Cornelius Aschermann, Robert Gawlik, Sebastian Schinzel, and
Thorsten Holz. 2017. kAFL: Hardware-Assisted Feedback Fuzzing for OS Kernels.
In USENIX Security Symposium.
[48] Konstantin Serebryany, Derek Bruening, Alexander Potapenko, and Dmitriy
Vyukov. 2012. AddressSanitizer: A Fast Address Sanity Checker.. In USENIX
Annual Technical Conference.
[49] Bhargava Shastry, Markus Leutner, Tobias Fiebig, Kashyap Thimmaraju, Fabian
Yamaguchi, Konrad Rieck, Stefan Schmid, Jean-Pierre Seifert, and Anja Feldmann.
2017. Static Program Analysis as a Fuzzing Aid. In Research in Attacks, Intrusions,
and Defenses (RAID).
[50] Nick Stephens, John Grosen, Christopher Salls, Andrew Dutcher, Ruoyu Wang,
Jacopo Corbetta, Yan Shoshitaishvili, Christopher Kruegel, and Giovanni Vigna.
2016. Driller: Augmenting Fuzzing Through Selective Symbolic Execution.. In
Network and Distributed System Security Symposium (NDSS).
[51] Rijnard van Tonder, John Kotheimer, and Claire Le Goues. 2018. Semantic Crash
Bucketing. In IEEE International Conference on Automated Software Engineering
(ASE).
[52] András Vargha and Harold D. Delaney. 2000. A Critique and Improvement of the
CL Common Language Effect Size Statistics of McGraw and Wong. Journal of
Educational and Behavioral Statistics 25, 2 (2000).
[53] Junjie Wang, Bihuan Chen, Lei Wei, and Yang Liu. 2017. Skyfire: Data-Driven
Seed Generation for Fuzzing. In IEEE Symposium on Security and Privacy (S&P).
[54] Weiguang Wang, Hao Sun, and Qingkai Zeng. 2016. SeededFuzz: Selecting and
Generating Seeds for Directed Fuzzing. In International Symposium on Theoretical
Aspects of Software Engineering (TASE).
[55] Maverick Woo, Sang Kil Cha, Samantha Gottlieb, and David Brumley. 2013. Sched-
uling Black-box Mutational Fuzzing. In ACM SIGSAC Conference on Computer
and Communications Security (CCS).
[56] Wen Xu, Sanidhya Kashyap, Changwoo Min, and Taesoo Kim. 2017. Designing
New Operating Primitives to Improve Fuzzing Performance. In ACM SIGSAC
Conference on Computer and Communications Security (CCS).
[57] Hyunguk Yoo and Taeshik Shon. 2016. Grammar-based adaptive fuzzing: Evalua-
tion on SCADA modbus protocol. In IEEE International Conference on Smart Grid
Communications.
[58] Bin Zhang, Jiaxi Ye, Chao Feng, and Chaojing Tang. 2017. S2F: Discover Hard-to-
Reach Vulnerabilities by Semi-Symbolic Fuzz Testing. In International Conference
on Computational Intelligence and Security.
[59] L. Zhang and V. L. L. Thing. 2017. A hybrid symbolic execution assisted fuzzing
method. In IEEE Region 10 Conference (TENCON).
[60] Zzuf 2018. Zzuf. http://caca.zoy.org/wiki/zzuf.