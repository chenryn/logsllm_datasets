title:A Large Scale Study of User Behavior, Expectations and Engagement
with Android Permissions
author:Weicheng Cao and
Chunqiu Xia and
Sai Teja Peddinti and
David Lie and
Nina Taft and
Lisa M. Austin
A Large Scale Study of User Behavior, Expectations 
and Engagement with Android Permissions
Weicheng Cao and Chunqiu Xia, University of Toronto; Sai Teja Peddinti, 
Google; David Lie, University of Toronto; Nina Taft, Google; Lisa M. Austin, 
University of Toronto
https://www.usenix.org/conference/usenixsecurity21/presentation/cao-weicheng
This paper is included in the Proceedings of the 30th USENIX Security Symposium.August 11–13, 2021978-1-939133-24-3Open access to the Proceedings of the 30th USENIX Security Symposium is sponsored by USENIX.A Large Scale Study of User Behavior, Expectations and
Engagement with Android Permissions
Weicheng Cao* Chunqiu Xia* Sai Teja Peddinti† David Lie* Nina Taft† Lisa M. Austin*
*University of Toronto
†Google
Abstract
We conduct a global study on the behaviors, expectations
and engagement of 1,719 participants across 10 countries
and regions towards Android application permissions. Partic-
ipants were recruited using mobile advertising and used an
application we designed for 30 days. Our app samples user
behaviors (decisions made), rationales (via in-situ surveys),
expectations, and attitudes, as well as some app provided ex-
planations. We study the grant and deny decisions our users
make, and build mixed effect logistic regression models to
illustrate the many factors that inﬂuence this decision making.
Among several interesting ﬁndings, we observed that users
facing an unexpected permission request are more than twice
as likely to deny it compared to a user who expects it, and
that permission requests accompanied by an explanation have
a deny rate that is roughly half the deny rate of app permis-
sion requests without explanations. These ﬁndings remain
true even when controlling for other factors. To the best of
our knowledge, this may be the ﬁrst study of actual privacy
behavior (not stated behavior) for Android apps, with users
using their own devices, across multiple continents.
1 Introduction
Permission requests in the Android system have two impor-
tant functions. First, they allow users to control a mobile ap-
plication’s ability to access resources and data on the phone.
Second, they are a mechanism that informs users about the
types of data that a mobile application might access. An im-
portant ramiﬁcation of this system is that developers could
interpret users’ decisions as hints on how to develop privacy
friendly applications. While many factors inﬂuence users’
decisions about which permissions they grant and which they
deny, this behavior could nevertheless be viewed as an oppor-
tunity to learn about unpopular permissions, which permis-
sions make sense to users, the reasons they grant permissions
and whether application-provided explanations affect users’
decisions. In this paper, we focus on the permissions An-
droid categorizes as “Dangerous”, namely those which must
be explicitly granted by the user to the application. Android
categorizes permissions into 11 permission groups (such as
Location, Camera, Microphone, etc.), which, for simplicity,
we simply refer to as “permissions” in this paper.
Many factors affect how users interact with Android per-
missions, such as behaviors, expectations, explanations of-
fered, and attitudes. Prior work usually focuses on one as-
pect of users at a time, such as behaviors [4, 20, 49], expec-
tations [19, 24, 48] or attitudes [19, 35], and do not seek to
analyze the interplay of these factors over the same set of
users. These prior studies used surveys, or provided users
with special devices, but it is preferable to obtain behavior
data “in-the-wild” (when users employ their own devices)
as opposed to experiments in a lab, as this captures more
naturally the choices users make in their daily lives. Finally,
even the largest published research studies to date that record
behavior on smartphones contain at most on the order of low
hundreds of participants from a single geographic region.
In order to overcome these challenges, we designed an
Android app, called PrivaDroid, and used it as our study in-
strument. PrivaDroid is designed to run in the background on
participants’ phones. It observes app installs, permission grant
and deny events, and launches in-situ surveys immediately
after these events. Together, the observations and surveys col-
lect data on participant decisions, rationales, expectations and
attitudes at the moment they act on their own personal devices.
In order to reach a broad base of participants, we designed
PrivaDroid to support all major Android versions from 6.0
to 10, translated PrivaDroid into 4 major languages and used
mobile advertising to recruit participants.
Our collection of decision rationales is similar to [4]; in
fact, we re-use the questions from this prior study, so we can
directly compare decision rationales. We expand beyond [4] in
multiple ways: 1) the prior study was done with US based par-
ticipants only, whereas our study includes participants from
10 countries and regions, and our app was deployed in 4 lan-
guages; 2) we collect which permissions a user expects an
app to ask for and thus can compare expectations against
behaviors; 3) we identify apps that provide explanations for
USENIX Association
30th USENIX Security Symposium    803
their permission requests and those that do not, and can thus
assess the impact on deny rate of providing explanations; and
4) we have users complete a privacy attitudes survey at the
end of our study, so that we may compare self-stated privacy
sensitivity with actual behavior.
The app was published on the Google Play Store from
September 2019 to August 2020 and advertised on several
online advertising platforms to recruit participants. To the best
of our knowledge, this is the ﬁrst cross-continent study on
Android permission decision making. Over the course of our
experiment, ∼1,700 participants joined from 10 countries and
successfully ﬁnished the 30 day study. In total, we observed
∼72K app installs and ∼36K permission decision events.
Nearly 1/3rd of these events were followed by an in-situ
survey that the participant completed. This is a much larger
scale study than [4] which was based on 157 participants.
Prior studies have advocated that explaining the reasons
for permission requests to users is critical to improve their
understanding, which in turn inﬂuences their grant and deny
choices [19, 22, 28]. In previous surveys, users state they
would be more comfortable granting permissions if explana-
tions were offered [40]. Our study allows us to examine actual
user behavior both in applications that offer explanations and
applications that do not.
Our contributions can be summarized as follows.
• We design and implement the PrivaDroid app to collect
behavioral data and perform experience sampling. We
translate PrivaDroid into Spanish, French and Chinese
(Traditional) and show that it is possible to use online
advertising to recruit participants from around the world.
• We compare the deny rate trends today to the study
done three years ago [4] and report which trends have
remained the same and which have evolved.
• We ﬁnd that some countries form cliques with statis-
tically similar deny rates, but also that deny rates may
differ signiﬁcantly between countries in different cliques.
• Using regression modeling we show there is a statisti-
cally signiﬁcant association between participants’ per-
missions decisions (grant/deny) and their run-time ex-
pectations, as well as with their install-time expectations.
We also employ these methods to show that deny rates
are lower when explanations are present. These ﬁnd-
ings remains true even when controlling for other factors
(such as country, attitudes, etc).
• We use a logistic regression model to study the inﬂuence
of 12 factors on users’ permission decision behavior.
Our model shows that nearly all of these parameters
have statistically signiﬁcant inﬂuence on users decisions.
This sheds light on the complexity of understanding user
decisions as many factors play a role.
• We compare privacy attitudes to behaviors and ﬁnd that
∼29% of our participants who say they are privacy sen-
sitive also exhibit low deny rates. Analysis shows that
these participants’ expectations about permissions tend
to be more accurate (matching app behavior), suggesting
that privacy sensitive users who grant many permissions
may be doing so with a better understanding of how and
why applications use permissions.
The rest of the paper is organized as follows. Section 2
discusses related work. Section 3 explains the participant
recruitment method, while Section 4 describes the design,
data collection and implementation of our PrivaDroid app.
Our ﬁndings are presented in Section 5. Section 6 describes
the limitations of our study and Section 7 concludes the paper.
The survey questions are listed in Appendix A.
2 Related Work
There is an extensive amount of existing research in the space
of Android permissions and privacy. Much of this research
documents user discomfort with permissions [35, 49] and
their frustration with what appears to them as unnecessary
permission requests [9, 20, 44, 46]. This can happen because
developers are not knowledgeable about permissions and this
results in mistakes [35, 38], or (mis)use of permissions in
unexpected ways [29, 39]. A recent study has shown that de-
velopers mostly use default conﬁgurations when integrating
ad/analytic libraries, and choose these libraries based on pop-
ularity and ease-of-use [26]. Many studies have found cases
where app permission requests are not related to the app’s
core functionality [1, 6, 19, 29, 31, 32, 34, 36, 46]. We do not
focus on developers in this work, but instead on users.
Research on user privacy expectations with permissions
has shown that users are concerned when they learn of the pos-
sible risks associated with permissions [11], or about applica-
tions collecting data when running in the background [13,42].
In [19], the authors studied user expectations around 4 re-
sources (GPS location, Device ID, network location, contact
list) based on an older model of Android. This study captured
resource requests users did not expect via an mTurk survey,
not based on decisions on personal devices as in our study.
Wijesekera et al. [48] captured user expectations by moni-
toring their apps for one week and showing users afterwards
what was collected and asking in-lab questions about whether
the participants expected that. This study reports that users
said they were more likely to deny permissions they didn’t
expect. Our results corroborate this ﬁnding, however we use
a very different mechanism as we captured actual decisions
made on personal devices, and at a much larger scale.
To help provide explanations or additional information so
users can make better choices, Harbach et al. [12] and Kel-
ley et al. [15] suggested providing more privacy information
and personal examples to help improve user comprehension.
804    30th USENIX Security Symposium
USENIX Association
Others categorized permissions to reduce the number of pri-
vacy/security decisions users need to make [10]. Some have
explored creating personalized privacy assistants [20], or sur-
facing nudges to assist users with decision making [2]. This
research focuses on supplementary features to help users
make decisions, whereas we focus on developer provided
explanations.
There is little work on app-provided permission explana-
tions. Tan et al. [40] conducted an online survey of smart-
phone users and showed that permission requests that include
explanations are signiﬁcantly more likely to be granted. They
also analyzed ∼4K iOS apps and showed that only 19% of
the permission requests included text within the dialogs to
explain the request. Liu et al. [21] analyzed ∼83K Android
apps and the extracted permission explanation messages, and
showed that less than 25% of apps provide explanations and
that the purposes stated in a signiﬁcant proportion of these ex-
planations were incorrect. We have made similar observations
in our analysis too: only 15% of apps in our data presented an
explanation to users for their permission requests, and having
an explanation reduced the permission deny rate from 15.4%
to 7.1%. While the prior work mentioned inﬂuence of permis-
sion explanations on the denial rates based on surveys, ours
is the ﬁrst to study user behavior on their own devices and
quantify the reduction in actual permission denial rates when
explanations are present.
Others have conducted cross-country studies [3, 5, 7, 14,
25, 27, 30, 33, 35] related to privacy. For example, Shklovski
et al. [35] conducted interviews and a survey across two
countries (Iceland and Denmark) to investigate how smart-
phone users feel about data access on their phones and if
they are willing to change their behavior after being informed
about tracking and data leakage. A multi-country survey [25]
showed that psychographics and various attributes of the
mobile app context are predictive of users’ privacy prefer-
ences. Schubauer et al. [33] examined app behavior on the
Google Play Store across three categories and 3 countries
(US, South Korea and Germany) and discovered that policy
changes aligned with privacy law changes (such as the Gen-
eral Data Protection Regulation) have impact on application
permission usage. Overall, there has been little research com-
paring users in different countries in terms of their attitudes
and behaviors related to Android app permissions. With the
exception of [33] that focuses on app design, the other prior
studies use interviews and surveys as their methodology. To
the best of our knowledge, we are the ﬁrst to compare actual
privacy behavior (not stated behavior), with users employing
their own devices, across multiple countries.
3 Participant Recruitment
Participant Composition. We recruited participants from 10
countries and territories, namely Canada, United States, Ar-
gentina, United Kingdom, France, Spain, South Africa, India,
Singapore, and Hong Kong. These countries were selected
using multiple criteria. First, we aimed to cover a diverse set
of regions thus selecting countries from 5 continents, cover-
ing 4 languages, and with different privacy legislation. Sec-
ond, we selected countries where we had access to native
speakers of the dominant language spoken, enabling us to
check our translations. Third, we focused on countries with
high smartphone penetration [37] and included two develop-
ing economies, South Africa and India. Finally, we aimed to
include countries covering a range of privacy views: India
previously had low privacy awareness and few concerns about
privacy [17,18] whereas France and Spain are reputed to have
strong concerns about privacy and are in a region (Europe)
with some of the strictest privacy laws (GDPR). This ensem-
ble of countries is similar to that in [5] which also includes 2
or 3 countries each from Europe, North America, Asia and 1
from South America.
Our aim was to recruit at least 100 participants from each
region with a nearly balanced split between males and fe-
males, hoping to obtain sufﬁcient data to compute statistically
signiﬁcant results. Because participants self-enrolled asyn-
chronously, and advertisements are sent out in large batches,
we could not control the number and gender of participants
who joined our study, and this created variance in participant
numbers across countries. We found that females were less
likely to join our study despite efforts to target more advertise-
ments at females. We did not control for other variables, such
as age, profession or income during the recruitment process,
mainly due to the inaccuracy in the advertisement network in-
ferred attributes for targeting our ads and partly due to ethical
concerns over targeting for age or income.
Advertising and Compensation. We use online advertising
to recruit participants as it allows us to ﬁnd participants across
many countries. Most recruitment agencies for user studies
only work in a single country, and international ones are
prohibitively expensive—particularly for large studies. We
selected three popular online advertising providers, namely
Google, Facebook and Reddit, so as to reach a broad audi-
ence. Initial experimentation with our app revealed that male
participants were more likely to join our experiment than fe-
males. To improve gender balance we targeted our advertising
towards female participants ﬁrst, and only started advertising
to males after we had more than 50 female participants.
We offered participants $10 USD if they stayed for 30 days
and completed the experiment. We initially selected Bitcoin
and PayPal as payment methods. However, Bitcoin was not
approved by our IRB, so we used PayPal for all participants.
Transparency and User Consent. This study was approved
by our institutional review board (IRB). Participants need to
give their consent before enrolling in our experiment. This
process happens after they install and open the PrivaDroid
Android app. The consent form enabled us to both gain con-
sent and allowed us to be transparent about our practices. It
USENIX Association
30th USENIX Security Symposium    805
contains the following key clauses. First, participants must
come from one of the speciﬁed countries and must be above
18 years of age. Second, participants must keep the accessibil-
ity service and app usage access enabled for our app during
the length of the experiment. Finally, we notify participants
that PrivaDroid collects no personally identiﬁable informa-
tion except for their Google advertising identiﬁer (a device
ID that we use to associate all the data coming from a single
device), and that we don’t use this advertising identiﬁer to
infer any other personal information (such as name, email,
etc). Participants must consent to these clauses.
Data Protection. To protect user privacy, access to the col-
lected raw data is controlled, and limited to only the subset of
authors (at the University of Toronto) directly involved in the
implementation and maintenance of PrivaDroid.