into a single PCIe transaction for better I/O bandwidth utilization,
(ii) in device driver: the amortized cost of per-packet bookkeeping
operations decreases, and (iii) in applications: processing multiple
packets achieves smaller instruction footprint with reduced function
call overheads and synchronization operations. We further improve
the batching approach of RouteBricks [19] as follows.
• We extend batch processing to the application (packet process-
ing) level. In contrast, Click [30] handles multiple packets at a
time only at NIC and device driver levels, but processes packets
one-by-one at the application level.
• To maximize beneﬁt from huge packet buffers, our packet I/O
engine performs aggressive software prefetch. The device driver
prefetches the packet descriptor and packet data of the next packet
into CPU cache in parallel while processing the packet. This
prefetch of consecutive packets eliminates the compulsory cache
miss latency.
When we pass batched RX packets to user-level applications, we
copy the data in the huge packet buffer into a consecutive user-level
buffer along with an array of offset and length for each packet. The
rationale for copying instead of zero-copy techniques (e.g., shared
huge packet buffer between kernel and user) is for better abstraction.
Copying simpliﬁes the recycling of huge packet buffer cells and
allows ﬂexible usage of the user buffer, such as manipulation and
split transmission of batched packets to multiple NIC ports. We ﬁnd
that the copy operation makes little impact on performance; it does
not consume additional memory bandwidth since the user buffer is
likely to reside in CPU cache and takes less than 20% of CPU cycles
out of total packet I/O processing.
Figure 5 shows the RX, TX, and minimal forwarding (RX + TX)
performance with 64B packets using a single CPU core. We see
that the throughput improves as the number of packets in a batch
increases, but the performance gain stalls after 32 packets. While
the packet-by-packet approach (with the batch size of 1) handles
only 0.78 Gbps, batch processing achieves 10.5 Gbps packet for-
warding performance with the batch size of 64 packets, resulting in
the speedup of 13.5.
4.4 Multi-core Scalability
Packet I/O on multi-core systems raises two performance issues:
(i) load balancing between CPU cores and (ii) linear performance
scalability with additional cores. To address these challenges, re-
199Figure 6: Performance of our packet I/O engine
caused by NUMA-blind data placement is almost hidden with multi-
gigabit trafﬁc, but not with multi-10G trafﬁc.
4.6 Packet I/O Performance
For the evaluation of our packet I/O engine, we implement a sim-
ple user-level program that repeatedly receives, transmits, and for-
wards packets without IP table lookup. Figure 6 summarizes the
performance of our packet I/O engine with all the techniques intro-
duced in this section. The results shown here are encouraging. The
TX performance comes close to the line rate, with 79.3 Gbps for
64B packets and 80.0 Gbps for 128B or larger packets. The RX
performance ranges from 53.1 Gbps to 59.9 Gbps depending on the
packet size. This performance asymmetry seems to stem from the
DMA inefﬁciency from a device to host memory, as described in
Section 3.2.
With RX and TX together, the minimal forwarding performance
(without IP table lookup) stays above 40 Gbps for all packet sizes.
By comparison, RouteBricks forwards 64B packets at 13.3 Gbps or
18.96 Mpps running in kernel mode on a slightly faster machine with
dual quad-core 2.83 GHz CPUs. Our server outperforms Route-
Bricks by a factor of 3 achieving 41.1 Gbps or 58.4 Mpps for the
same packet size even though our experiment is done in user mode.
Bars marked as “node-crossing” represent the case that all packets
received in one NUMA node are transmitted to ports in the other
node. Even for this worst case the throughput still stays above 40 Gbps,
implying our packet I/O engine is scalable with NUMA architecture.
We identify the bottleneck limiting the throughput around 40 Gbps,
which could reside in either CPU, memory bandwidth, or I/O. Though
we see the full CPU usage for 64B and 128B packets, CPU is not
the bottleneck. We run the same test with only four CPU cores
(two for each node) and get the same forwarding performance. As
implied in Figure 5, the CPU usage is elastic with the number of
packets for each fetch; the average batch size was 13.6 with 8 cores
and 63.0 with 4 cores. To see whether the memory bandwidth is
the bottleneck, we run the same experiments by having background
processes consume additional memory bandwidth, but we see the
same performance.
We conclude that the bottleneck lies in I/O. Considering the through-
put asymmetry of RX and TX and the fact that individual links (QPI
and PCIe) provide enough bandwidth, we suspect that the dual-IOH
problem described in Section 3.2 bounds the maximum packet I/O
performance around 40 Gbps in our system.
5. GPU ACCELERATION FRAMEWORK
Our GPU acceleration framework provide a convenient environ-
ment to write packet processing applications, maximizing synergy
with our highly optimized packet I/O engine. This section describes
the brief design of the framework.
Figure 7: PacketShader software architecture
5.1 Overall Structure
PacketShader is a software router framework that combines the
GPU acceleration template and our highly optimized packet I/O en-
gine. Figure 7 depicts the simpliﬁed architecture of PacketShader
(gray blocks indicate our implementation). PacketShader is a multi-
threaded program running in user mode. For packet I/O, it invokes
the packet API consisting of wrapper functions to kernel-level packet
I/O engine. A packet processing application runs on top of the frame-
work and is mainly driven by three callback functions (a pre-shader,
a shader, and a post-shader).
Currently, the performance of CUDA programs degrades severely
when multiple CPU threads access the same GPU, due to frequent
context switching overheads [29]. In order to avoid the pathological
case, PacketShader divides the CPU threads into worker and master
threads. A master thread communicates exclusively with a GPU in
the same node for acceleration, while a worker thread is responsible
for packet I/O and requests the master to act as a proxy for com-
munication with the GPU. Each thread has a one-to-one mapping
to a CPU core and is hard-afﬁnitized to the core to avoid context
switching and process migration costs [21].
PacketShader partitions the system into NUMA nodes so that each
of them could process packets independently. In each node a quad-
core CPU runs three worker threads and one master thread. Those
workers communicate only with the local master to avoid expensive
node-crossing communication. Once a worker thread receives pack-
ets, all processing is done by the CPU and the GPU within the same
node. The only exception is to forward packets to ports in the other
node, but this process is done by DMA, not CPU.
5.2 User-level Interface to Packet I/O Engine
PacketShader runs in user mode, not in kernel, to take advantage
of user-level programming: friendly development environments, re-
liability with fault isolation, and seamless integration with third-
party libraries (e.g., CUDA or OpenSSL). User-level packet process-
ing, however, imposes a technical challenge: performance. For ex-
ample, a user-mode Click router is reported to be three times slower
than when it runs in kernel space [31]. We list three major issues and
our solutions for high-performance packet processing in user mode.
Minimizing per-packet overhead: The simplest packet I/O scheme,
one system call for each packet RX and TX, introduces signiﬁcant
user-kernel mode switching overheads. PacketShader batches mul-
tiple packets over a single system call to amortize the cost of per-
packet system call overhead.
Better coupling of queues and cores: Existing user-level packet I/O
libraries, such as libpcap [26], exposes a per-NIC interface to user
200(a) Existing per-NIC queue scheme
(b) Our multiqueue-aware packet I/O scheme
Figure 8: User-level packet I/O interfaces
applications. This scheme signiﬁcantly degrades the performance
on systems equipped with multi-queue NICs and multi-core CPUs,
as shown in Figure 8(a). Kernel and user threads on multiple CPU
cores have to contend for the shared per-NIC queues. These mul-
tiplexing and de-multiplexing of received packets over the shared
queues cause expensive cache bouncing and lock contention.
PacketShader avoids this problem by using an explicit interface
to individual queues (Figure 8(b)). A virtual interface, identiﬁed
with a tuple of (NIC id, RX queue id), is dedicated to a user-level
thread so that the thread directly accesses the assigned queue. The
virtual interfaces are not shared by multiple cores, eliminating the
need of sharing and synchronization of a single queue. The user
thread fetches packets from multiple queues in a round-robin manner
for fairness.
Avoiding receive livelock: Click and other high-performance packet
processing implementations avoid the receive livelock problem [34]
by polling NICs to check packet reception rather than using inter-
rupts [17, 18, 30]. However, we note two problems with this busy-
wait polling: (i) Extended period of full CPU usage prevents the
chance to save electricity in an idle state, which can be up to a
few hundreds of watts for a modern commodity server. (ii) Polling
in kernel can starve user processes (e.g., a BGP daemon process)
running on the same CPU core.
Linux NAPI [44], a hybrid of interrupt and polling in Linux TCP/IP
stack, effectively prevents TCP/IP stack in kernel from starvation.
PacketShader can not directly beneﬁt from NAPI, however, since
NAPI protects only kernel context4. User context, which has the
lowest scheduling priority in the system, may always be preempted
by the kernel (hardware RX interrupt or softirq, the bottom-half han-
dling of received packets).
To avoid receive livelock problem in user context, PacketShader
actively takes control over switching between interrupt and polling.
In the interrupt-disabled state, PacketShader repeatedly fetches pack-
ets. When it drains all the packets in the RX queue, the thread blocks
and enables the RX interrupt of the queue. Upon receiving packets,
Figure 9: Basic workﬂow in PacketShader
the interrupt handler wakes up the thread, and then the interrupt is
disabled again. This scheme effectively eliminates receive livelock
as packet RX only happens with the progress of PacketShader.
5.3 Workﬂow
We deﬁne chunk as a group of packets fetched in a batch of packet
reception. The chunk size is not ﬁxed but only capped; we do not
intentionally wait for the ﬁxed number of packets. Chunk also works
as the minimum processing unit for GPU parallel processing in our
current implementation. By processing multiple packets pending in
RX queues, PacketShader adaptively balances between small par-
allelism for low latency and large parallelism for high throughput,
according to the level of offered load.
PacketShader divides packet processing into three steps: pre-shading,
shading5, and post-shading. Pre-shading and post-shading run on
worker threads and perform the actual packet I/O and other miscella-
neous tasks. Shading occurs on master threads and does GPU-related
tasks. Each step works as follows:
• Pre-shading: Each worker thread fetches a chunk of packets
from its own RX queues. It drops any malformed packets and
classiﬁes normal packets that need to be processed with GPU.
It then builds data structures to feed input data to the GPU. For
example, for IPv4 forwarding, it collects destination IP addresses
from packet headers and makes an array of the collected ad-
dresses. Then it passes the input data to the input queue of a
master thread.
• Shading: The master thread transfers the input data from host
memory to GPU memory, launches the GPU kernel, and takes
back the results from GPU memory to host memory. Then it
places the results back to the output queue of the worker thread
for post-shading.
• Post-shading: A worker thread picks up the results in its out-
put queue, and modiﬁes, drops, or duplicates the packets in the
chunk depending on the processing results. Finally, it splits the
packets in the chunk into destination ports for packet transmis-
sion.
Figure 9 shows how the worker and master threads collaborate on
GPU acceleration in PacketShader. Communication between threads
is done via the input queue of a master thread and output queues
of worker threads. Having per-worker output queues relaxes cache
bouncing and lock contention by avoiding 1-to-N sharing. However,
we do not apply the same technique to the input queue in order to
guarantee fairness between worker threads.
Another issue with communication between CPU cores is cache
4NAPI also indirectly saves user programs using TCP from starvation
because the packet arrival rate is suppressed by congestion control followed
by packet drop.
5In computer graphics, a shader is a small program running on GPUs for
rendering. A shader applies transformation to a large set of vertices or pixels
in parallel. We name PacketShader after this term since it enables GPUs to
process packets in parallel.
201(a) Chunk pipelining
(b) Gather/scatter
(c) Concurrent copy and execution
Figure 10: Optimizations on GPU acceleration
migration of the input and output data. The pre-shading step builds
the input data that the CPU core loads onto the private cache.
If
a master thread accesses the input data, corresponding cache lines
would migrate to the master thread’s CPU cache. Since cache mi-
gration is expensive on multi-core systems, we prevent the master
thread from accessing the input data itself. When the master thread
is notiﬁed of input data by a worker thread, it immediately transfers
it to GPU by initiating DMA without touching the data itself. We
apply the same policy to the output data as well.
PacketShader preserves the order of packets in a ﬂow with RSS.
RSS distributes received packets into worker threads on a ﬂow-basis;
packets in the same ﬂow are processed by the same worker thread.
PacketShader retains the initial order of packets in a chunk, and all
queues enforce First-In-First-Out (FIFO) ordering.
5.4 Optimization Strategies
PacketShader exploits optimization opportunities in the basic work-
ﬂow. First, we pipeline the processing steps to fully utilize each
worker thread. Second, we have the master thread process multiple
chunks fed from worker threads at a time to expose more parallelism
to GPU. Third, we run the PCIe data transfer in parallel to the GPU
kernel execution.
Chunk Pipelining: After a worker thread performs the pre-shading
step and passes the input data to its master, it waits until the master
process ﬁnishes the shading step. This underutilization of worker
threads degrades the system performance when the volume of input
trafﬁc is high.
To remedy this behavior, PacketShader pipelines chunk process-
ing in the worker threads as shown in Figure 10(a). Once a worker
thread passes input data to its master, it immediately performs the
pre-shading step for another chunk until the master returns the output
data of the ﬁrst chunk.
Gather/Scatter: As PacketShader has multiple worker threads per
master, the input queue can ﬁll with chunks if the GPU is overloaded.
Figure 10(b) shows how PacketShader processes multiple chunks in
the queue in the shading step; a master thread dequeues multiple
input data from its input queue and pipelines copies of input data