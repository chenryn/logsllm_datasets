The core objective of Maple is to offer an SDN programmer the
abstraction that a general-purpose program f deﬁned by the pro-
grammer runs “from scratch” on a centralized controller for ev-
ery packet entering the network, hence removing low-level details,
such as distributed switch ﬂow tables, from the programmer’s con-
ceptual model. A naive implementation of this abstraction, how-
ever, would of course yield unusable performance and scalability.
Maple introduces two key components in its design to efﬁciently
implement the abstraction. The ﬁrst is an optimizer, or tracing run-
time, which automatically discovers reusable (cachable) algorith-
mic policy executions at runtime, ofﬂoads work to switches when
possible, and invalidates cached policy executions due to environ-
ment changes. One can build a proactive rule installer on top of the
tracing runtime, for example, using historical packets, or develop a
static analyzer to proactively evaluate f. These are out of the scope
of this paper. The second is a run-time scheduler, or scheduler for
short, which provides Maple scalable execution of policy “misses”
generated by the many switches in a large network on multicore
hardware. Figure 1 illustrates the positions of the two components.
In addition, Maple allows a set of higher-level tools to be built
on top of the basic abstraction: (1) deployment portal, which im-
poses constraints on top of Maple, in forms such as best practices,
domain-speciﬁc analyzers, and/or higher-level, limited conﬁgura-
tion interfaces to remove some ﬂexibility of Maple; and (2) policy
composer, which an SDN programmer can introduce.
This paper focuses on the basic abstraction, the optimizer, and
the scheduler. This section sketches a high-level overview of these
three pieces, leaving technical details to subsequent sections.
3.1 Algorithmic Policy f
An SDN programmer speciﬁes the routing of each packet by pro-
viding a function f, conceptually invoked by our run-time sched-
Figure 1: Maple system components.
uler on each packet. An SDN programmer can also provide han-
dlers for other events such as switch-related events, but we focus
here on packet-arrival events. Although the function f may in prin-
ciple be expressed in any language, for concreteness we illustrate f
in a functional style:
f :: (PacketIn,Env) -> ForwardingPath
Speciﬁcally, f is given two inputs: PacketIn, consisting of a
packet (Packet) and the ingress switch/port; and Env, a handle
to an environment context. The objective of (Env) is to provide f
with access to Maple-maintained data structures, such as a network
information base that contains the current network topology.
The return value of a policy f is a forwarding path, which spec-
iﬁes whether the packet should be forwarded at all and if so how.
For multicast, this result may be a tree instead of a linear path. The
return of f speciﬁes global forwarding behavior through the net-
work, rather than hop-by-hop behavior.
Except that it must conform to the signature, f may use arbitrary
algorithms to classify the packets (e.g., conditional and loop state-
ments) and compute forwarding actions (e.g., graph algorithms).
3.2 Optimizer
Although a policy f might in principle follow a different execu-
tion path and yield a different result for every packet, in practice
many packets—and often many ﬂows—follow the same or simi-
lar execution paths in realistic policies. For example, consider the
example algorithmic policy in Section 2. f assigns the same path
to two packets if they match on source and destination MAC ad-
dresses and neither has a TCP port value 22. Hence, if we invoke f
on one packet, and then a second packet arrives, and the two pack-
ets satisfy the preceding condition, then the ﬁrst invocation of f is
reusable for the second packet. The key objective of the optimizer
is to leverage these reusable algorithm executions.
3.2.1 Recording reusable executions
The technique used by Maple to detect and utilize reusable exe-
cutions of a potentially complex program f is to record the essence
of its decision dependencies: the data accesses (e.g., reads and as-
sertions) of the program on related inputs. We call a sequence of
such data accesses a trace, and Maple obtains traces by logging
data accesses made by f.
As an example, assume that the log during one execution of an f
is follows: (1) the only data access of the program is to apply a test
on TCP destination port for value 22, (2) the test is true, and (3) the
program drops the packet. One can then infer that if the program
is again given an arbitrary packet with TCP destination port 22, the
program will similarly choose to drop the packet.
The key data structure maintained by the optimizer is a collection
of such data access traces represented as a trace tree. Figure 2 is a
trace tree from 3 traces of a program f. In one of these traces, the
program ﬁrst tests TCP destination port for 22 and the result is false
(the right branch is false). The program then reads the ﬁeld value
of Ethernet destination (=4) and Ethernet source (=6), resulting in
89tcpDst:22
true
drop
false
ethDst
2
drop
4
ethSrc
6
h6
s4
h4
1
s1
30
s2
3
s3
4
path 6->4: h6->s1/port 30->s2/port 3->s3/port 4 (h4)
Figure 2: An example trace tree. Diamond indicates a test
of some condition, circle indicates reading an attribute, and
rectangles contain return values. tcpDst denotes TCP destina-
tion; ethDst and ethSrc denote Ethernet destination and source.
Topology of the network is shown on the right.
the program’s decision to forward packets from host 6 to host 4
along the shortest path between these two hosts. For concreteness,
Figure 2 shows the details of how a path is represented. Assume
that host 6 is attached at switch s1/port 1, and host 4 at switch
s3/port 4. Figure 2 shows the detail of the path from host 6 to
host 4 as: from host 6 (s1/port 1) to output port 30 of s1, which is
connected to switch s2; s2 sends out at port 3, which is connected
to s3, where host 4 is attached at port 4. The trace tree abstracts
away the details of f but still retains its output decisions as well as
the decisions’ dependencies on the input packets.
3.2.2 Utilizing distributed ﬂow tables
Merely caching prior policy decisions using trace trees would
not make SDN scalable if the controller still had to apply these
decisions centrally to every packet. Real scalability requires that
the controller be able to “push” many of these packet-level deci-
sions out into the ﬂow tables distributed on the individual Openﬂow
switches to make quick, on-the-spot per-packet decisions.
To achieve this goal, the optimizer maintains, logically, a trace
tree for each switch, so that the leaves for a switch’s trace tree con-
tain the forwarding actions required for that switch only. For exam-
ple, for the trace tree shown in Figure 2, the switch-speciﬁc trace
tree maintained by Maple for switch s1 has the same structure, but
includes only port actions for switch s1 at the leaves (e.g., the right
most leave is labeled only port 30, instead of the whole path).
Given the trace tree for a switch, the optimizer compiles the trace
tree to a prioritized set of ﬂow rules, to form the ﬂow table of the
switch. In particular, there are two key challenges to compile an ef-
ﬁcient ﬂow table for a switch. First, the table size at a switch can be
limited, and hence it is important to produce a compact table to ﬁt
more cached policy decisions at the switch. Second, the optimizer
will typically operate in an online mode, in which it needs to contin-
uously update the ﬂow table as new decisions are cached. Hence, it
is important to achieve fast, efﬁcient ﬂow table updates. To address
the challenges, our optimizer introduces multiple techniques: (1) it
uses incremental compilation, avoiding full-table compilation; (2)
it optimizes the number of rules used in a ﬂow table, through both
switch-local and network-wide optimizations on switch tables; and
(3) it minimizes the number of priorities used in a ﬂow table, given
that the update time to a ﬂow table is typically proportional to the
number of priority levels [18].
3.2.3 Keeping trace trees, ﬂow tables up-to-date
Just as important as using distributed ﬂow tables efﬁciently is
keeping them up-to-date, so that stale policy decisions are not ap-
plied to packets. Speciﬁcally, the decision of f on a packet depends
on not only the ﬁelds of the packet, but also other variables. For ex-
ample, data accesses by f through the Env handle to access the net-
work information base will also generate dependency, which Maple
tracks. Hence, trace trees record the dependencies of prior policy
decisions on not only packet ﬁelds but also Maple-maintained en-
vironment state such as network topology and conﬁgurations.
A change to the environment state may invalidate some part of
the trace trees, which in turn may invalidate some entries of the
ﬂow tables. There is a large design spectrum on designing the in-
validation scheme, due to various trade-offs involving simplicity,
performance, and consistency. For example, it can be simple and
“safe” to invalidate more ﬂow table entries than necessary—though
doing so may impact performance. To allow extensible invalidation
design, Maple provides a simple invalidation/update API, in which
a ﬂexible selection clause can be speciﬁed to indicate the cached
computations that will be invalidated. Both system event handlers
and user-deﬁned functions (i.e., g in Figure 1) can issue these API
calls. Hence, user-deﬁned f can introduce its own (persistent) envi-
ronment state, and manage its consistency. Note that dependencies
that Maple cannot automatically track will require user-initiated in-
validations (see Section 4.2) to achieve correctness.
3.3 Multicore Scheduler
Even with efﬁcient distributed ﬂow table management, some frac-
tion of the packets will “miss” the cached policy decisions at switches
and hence require interaction with the central controller. This controller-
side processing of misses must scale gracefully if the SDN as a
whole is to scale.
Maple therefore uses various techniques to optimize the con-
troller’s scalability, especially for current and future multicore hard-
ware. A key design principle instrumental in achieving controller
scalability is switch-level parallelism: designing the controller’s
thread model, memory management, and event processing loops to
localize controller state relevant to a particular “client” switch. This
effectively reduces the amount and frequency of accesses to state
shared across the processing paths for multiple client switches.
While many of our design techniques represent “common sense”
and are well-known in the broader context of parallel/multicore
software design, all of the SDN controllers that we had access to
showed major scalability shortcomings, as we explore later in Sec-
tion 6. We were able to address most of these shortcomings through
a judicious application of switch-level parallelism principles, such
as buffering and batching input and output message streams, and
appropriate scheduling and load balancing across cores. Section 5
discusses scheduling considerations further.
4. MAPLE OPTIMIZER
This section details the optimizer, highlighting the construction
and invalidation of trace trees and methods for converting trace
trees to ﬂow tables. We choose to present out ideas in steps, from
basic ideas to optimizations, to make understanding easier.
4.1 Basic Concepts
Trace tree: A trace tree provides an abstract, partial representation
of an algorithmic policy. We consider packet attributes a1, . . . an
and write p.a for the value of the a attribute of packet p. We write
dom(a) for the set of possible values for attribute a: p.a ∈ dom(a)
for any packet p and attribute a.
DEFINITION 1
(TRACE TREE). A trace tree (TT) is a rooted
tree where each node t has a ﬁeld type t whose value is one of L
(leaf), V (value), T (test), or Ω (empty) and such that:
1. If type t = L, then t has a value t ﬁeld, which ranges over
possible return values of the algorithmic policy. This node
represents the behavior of a program that returns value t
without inspecting the packet further.
2. If type t = V, then t has an attr t ﬁeld, and a subtree t ﬁeld,
where subtree t is an associative array such that subtree t[v]
90Algorithm 1: SEARCHTT(t, p)
1 while true do
2
3
if type t = Ω then
return NIL;
4
5
6
7
8
9
10
11
12
13
return valuet;
t ← subtree t[p.attr t];
else if type t = L then
else if type t = V ∧ p.attr t ∈ keys(subtree t) then
else if type t = V ∧ p.attr t /∈ keys(subtree t) then
else if type t = T ∧ p.attr t = valuet then
else if type t = T ∧ p.attr t (cid:54)= valuet then
return NIL;
t ← t+;
t ← t−;
is a trace tree for value v ∈ keys(subtree t). This node rep-
resents the behavior of a program that if the supplied packet
p satisﬁes p.attr t = v, then it continues to subtree t[v].
3. If type t = T, then t has an attr t ﬁeld, a value t ﬁeld, such
that value t ∈ dom(attr t), and two subtree ﬁelds t+ and
t−. This node reﬂects the behavior of a program that tests
the assertion p.attr t = value t of a supplied packet p and
then branches to t+ if true, and t− otherwise.
4. If type t = Ω, then t has no ﬁelds. This node represents
arbitrary behavior (i.e., an unknown result).
Given a TT, one can look up the return value of a given packet, or
discover that the TT does not include a return value for the packet.
Algorithm 1 shows the SEARCHTT algorithm, which deﬁnes the
semantics of a TT. Given a packet and a TT, the algorithm traverses
the tree, according to the content of the given packet, terminating
at an L node with a return value or an Ω node which returns NIL.
Flow table (FT): A pleasant result is that given a trace tree, one
can generate an Openﬂow ﬂow table (FT) efﬁciently.
To demonstrate this, we ﬁrst model an FT as a collection of FT
rules, where each FT rule is a triple (priority, match, action),
where priority is a natural number denoting its priority, with the
larger the value, the higher the priority; match is a collection of
zero or more (packet attribute, value) pairs, and action denotes the
forwarding action, such as a list of output ports, or ToController,
which denotes sending to the controller. Matching a packet in an
FT is to ﬁnd the highest priority rule whose match ﬁeld matches
the packet. If no rule is found, the result is ToController. Note
that FT matches do not support negations; instead, priority ordering
may be used to encode negations (see below).
Trace tree to forwarding table: Now, we describe BUILDFT(t), a
simple algorithm shown in Algorithm 2 that compiles a TT rooted
at node t into an FT by recursively traversing the TT. It is simple
because its algorithm structure is quite similar to the standard in-
order tree traversal algorithm. In other words, the elegance of the
TT representation is that one can generate an FT from a TT using
basically simple in-order tree traversal.
Speciﬁcally, BUILDFT(t), which starts at line 1, ﬁrst initial-
izes the global priority variable to 0, and then starts the recur-
sive BUILD procedure. Note that each invocation of BUILD is pro-
vided with not only the current TT node t, but also a match param-
eter denoted m, whose function is to accumulate attributes read or
positively tested along the path from the root to the current node
t. We can see that BUILDFT(t) starts BUILD with m being any
(i.e., match-all-packets). Another important variable maintained by
BUILD is priority. One can observe an invariant (lines 8 and 16)
priority ← 0;
BUILD(t,any);
return;