and not thought anything about it.”
Removing paths through which users can close a credential-
entry window also raised suspicions among some partici-
pants on both Mac OS and Windows. For example, a Mac
OS participant reported that the spoofed window “didn’t
have the clean Apple ‘look’ [and] didn’t have an available can-
cel button.” Changing the appearance of the window close
box and removing the cancel button may have provided a
visual clue that the window was fake. Disabling such func-
tionality more subtly (e.g. by making active-looking but-
tons non-functional) might increase compromise rates, as
the only users who would be alerted to the diﬀerence would
be those who had already decided to close the window (and
thus would not be entering their credentials). Some fraction
of those users might then decide to enter their credentials if
they believed it was the only way to dismiss the window.
Finally, a handful of participants, familiar with research
372Table 2: Disaggregated data for the attack rates, per condition.
attack, including the 5% attack eﬃcacy results for scareware
campaigns previously reported by Cova et al. [3, 4].
Our participants were drawn from the population of users
of Mechanical Turk who accepted our HIT. This population
may diﬀer in important ways from the populations targeted
in certain attacks. For example, an attacker targeting a
software security company might compromise a smaller pro-
portion of users than were compromised in our experiments,
as such individuals may be more likely to detect spoofed
windows. Mechanical Turk users may be more or less likely
to be using personal (as opposed to a work) accounts and
thus the vigilance with which they protect their credentials
may diﬀer from the populations targeted in real attacks.
Some factors may have made our simulated attacks more
likely to result in a compromise than a real spooﬁng at-
tack. For example, participants may have recognized the
name of our institution in the initial consent disclosure and
assumed that researchers would not direct them to an un-
safe third-party site. Participants may also have mistyped
their credentials but reported that they had entered their
valid credentials. Additionally, convincing users to maxi-
mize their browser, or doing it for them, may be essential
to achieving the compromise rates we saw. However, we did
not see evidence of this when we compared the CredUI-D*
and CredUI conditions. Attackers may be unable to con-
vince users to maximize windows without causing suspicion.
It is also possible that attackers could achieve compromise
rates much higher than those we saw in our experiments. An
attacker who could provide a more compelling scenario for
entering credentials might be able to compromise many of
those users who would not be compromised by our treat-
ments. A real attacker need not repeat mistakes we made
when learning to spoof these interfaces, such as using the
word ‘Username’ instead of ‘User name’ in Windows and
failing to center the Mac OS credential-entry dialog. A real
attacker spooﬁng an installation of Silverlight might put a
Silverlight object on the page to detect whether Silverlight
was already installed and to identify the current version.
In considering the results of our study, one must also con-
studies, were not fooled by the study scenario and the sub-
terfuge of the fake site. One wrote: “I’m taking a psychology
study. I just ﬁgured it was part of the study.”
4.5 Follow-up experiment
We performed a second experiment to more tightly bound
our estimate of the eﬃcacy of one of our most eﬀective at-
tacks: UAC1. We collected data for 199 participants during
two solicitation periods on July 20 and 25 of 2012—a four-
fold increase in the number of participants per treatment.
In both sessions our participant quotas were met within a
matter of a few hours, in contrast to our earlier study which
was oﬀered over a greater diversity of times-of-day. Par-
ticipants’ demographic data were virtually identical in both
experiments. In the second experiment participants were an
average of 29 years old with a σ = 9.7 years (vs. 28 years
old, σ = 9.6), 53% were male (55% in the original), 77%
were caucasian (78% in the original), and the top two re-
ported occupations were again ‘Student’ (28% vs. 33% in
the original) and ‘Currently Unemployed’ (16% vs. 13% in
the original). Participants took in average 19 min 57 sec to
complete the study with a σ =8 min 26 sec, vs. 17 min 23
sec with a σ =18 min 15 sec.
Out of 199 participants, 52 entered at least one character,
and 41 of them (21% of the total) later admitted it was a real
password in either the ﬁrst or the second question described
earlier. Of those who did not proceed to enter characters
into the password ﬁeld and were asked why, 63 (32% of the
total) indicated password-theft as a concern, causing us to
categorize them as wise. The remaining 95 (47% of the toal)
were deemed oblivious to the attack. The most frequently
invoked reason for not entering a password was ‘concern that
the software could damage their computers’, checked by 104
participants (52% of the total), and ‘not wanting to install
new software’, checked by 86 participants (43% of the total).
In this follow-up experiment, 24 participants did not com-
plete the survey, 6 of them being from outside the US. Of the
18 remaining, 14 dropped before getting to the third game.
Only one of the four remaining participants returned to the
survey after having seen the spoofed dialog; we don’t have
evidence that the other three actually saw the dialog.
The compromise rate in the follow-up experiment (21%)
was lower than for the same treatment in the original ex-
periment (27%), although the diﬀerence is not signiﬁcant
(χ2(1) = 0.429, p = 0.5125). Figure 6 displays the 95% con-
ﬁdence intervals for the compromise rates observed in this
experiment.
5. LIMITATIONS
As with any experiment, our study has limitations that
may cause our results to diﬀer from the results of a real
Figure 6: Attack eﬃcacy for second experiment, along
with 95% conﬁdence intervals. 20.6±5.6% of participants
were compromised, 31.7±6.5% were wise to the attack, and
47.7±6.9% were oblivious to the attack.
373sider that the consequences of a compromise vary widely
based on what they are used for. If the user’s device blocks
all forms of remote access, the compromise of device creden-
tials may be of no consequence.
If the credentials are for
the user’s account on an enterprise network, and that enter-
prise oﬀers remote access to the network, computing, and
services (e.g. email, payroll, etc.), the consequences could
be signiﬁcant. If the user employs the same credentials for
other accounts, the consequences may extend even further.
6. RELATED DEFENSES
Operating system designers have been aware of the need
to defend against trusted-path vulnerabilities since at least
as far back as the early 1970’s when Saltzer and Schroeder
presented the need for a ‘secure’ path in the context of a sce-
nario in which a user grants permissions (capabilities): “one
thing is crucial–that there be a secure path from Doe, who
is authorizing the passing of the capability, to the program,
which is carrying it out.” More recently, Ka-Ping Yee de-
scribed trusted path as requiring “an unspoofable and faith-
ful communication channel between the user and any en-
tity trusted to manipulate authorities on the user’s behalf.”
Yee highlighted the secure attention sequence in Windows
(ctrl-alt-delete) as an example solution to the trusted path
problem for credential entry [31].
Many of the defenses that protect users from spooﬁng at-
tacks today rely on detecting bogus emails and blacklisting
software and websites; they do not address the underlying
trusted path problem. Such defenses are necessary because
preventing spooﬁng not only requires a technology to sup-
port a trusted path, but a change in user behavior to avoid
untrusted paths. Users must unlearn the habit of provid-
ing credentials into windows they they cannot authenticate.
This will require time and a clear set of rules that users can
apply to reliably diﬀerentiate the OS from other principals.
There are three major categories of solutions to establish
trusted paths: dedicated IO, visualizations of shared secrets,
and secure attention sequences.
6.1 Dedicated IO
One way to establish a trusted channel between the user
and an OS is to dedicate speciﬁc hardware, or portions of
hardware, to be used exclusively for that channel. For ex-
ample, a device could dedicate a screen and separate keypad
for use in authentication, as is sometimes done in payment
systems. Others employ a separate input and output de-
vice that the user may already have. For example, Parno et
al.’s Phoolproof Phishing scheme employs the user’s mobile
phone to externally conﬁrm websites when entering a pass-
word [20] and IBM’s Zone Trusted Information Channel pro-
vides an external trusted path for banking operations [24].
Enabling users to communicate securely with a single prin-
cipal need not necessarily require both a dedicated input and
output device. A dedicated output, such as an LED or ded-
icated screen region, may be suﬃcient to indicate when a
trusted path is present. A dedicated input device may be
suﬃcient to force a trusted path to be established, or may
itself be used for the sole purpose of entering credentials.
Many systems attempt to establish trusted paths by ded-
icating pixels within a window to host trust indicators that
indicate the presence of a trusted path. For example, the
“chrome” region in browsers is the portion of the browser
window that is not controlled by the website being rendered,
and has been used to host indicators that activate when a
connection is secure or that display the domain name of a
website. However, users can still be confused about whether
a window is real or fake. For example, Jackson et al. demon-
strated that users will trust spoofed chrome elements that
appear in a browser window that is itself spoofed, rendered
within the content region of a genuine browser window [13].
This is known as a picture-in-picture attack.
Operating systems sometimes use visual cues to diﬀeren-
tiate active windows (those ‘in focus’) from inactive ones,
in part to defend against picture-in-picture attacks. For ex-
ample, some systems render the frames of foreground win-
dows to appear darker than background windows. An astute
user might notice that our window in a picture-in-picture
attack remain active. Secure windows management systems
EROS [25] and Nitpicker [10] dim all windows except the
application currently in use and clearly label windows to
help prevent users from accidentally entering information
into the incorrect application. The results of our experi-
ment raise doubts as to whether dimming the screen is an
eﬀective way to establish a trusted path, and if an entire
window can be spoofed, the labels inside can be as well.
6.2 Visualizations of shared secrets
While operating systems allow other principals to use the
screen, they can usually ensure that they themselves can
render data to the screen without it being intercepted by
other applications. Thus, if the operating system and user
share a secret, the OS can display this secret with reasonable
conﬁdence that other principals will not learn it. Shared se-
cret schemes work much in the same way a dedicated output
device does, but instead of lighting up a dedicated set of pix-
els to signal a trusted path, the OS renders a representation
of the shared secret.
Tygar and Whitten propose “requir[ing] the consumer to
personalize the appearance of the software at the time the
trust relationship is formed” for this purpose [28]. Similarly,
Adelsbach et al.
suggest personalizing security indicators
in the browser interface [1]. Dhamija and Tygar’s Dynamic
Security Skins tool displays a user-selected photograph in
windows requesting or providing security information, al-
lowing users to verify that the window was produced by the
web browser and not a website [5]. In Herzberg and Jbara’s
Trustbar, users assign names or logos for each website, and
these are later shown to conﬁrm that the users are again at
the same website [11].
Other solutions use secrets that are not directly controlled
by the user. Ye et al. present a colored border for windows to
indicate when these windows are controlled by the browser.
The border format dynamically changes to match a browser-
controlled metadata window [30].
The security of shared secret schemes rests on the assump-
tions that users will be able to recognize the shared secret,
notice when the shared secret is absent, and realize there is
no trusted path when the shared secret is absent. Shared
secret schemes may be attacked by convincing users to dis-
regard an invalid or missing secret. For example, Schechter
et al. demonstrated an attack against the Passmark shared-
secret scheme used for online banking in which users were
told that their shared secret was temporarily unavailable due
to system maintenance [23].
3746.3 Secure attention sequences
Just as shared secrets leverage the operating systems’ ulti-
mate control over output devices, secure attention sequences
leverage their ability to capture and prioritize input events.
For example, on Windows the key combination of ctrl-alt-
delete is captured by the operating system, and triggers the
establishment of a trusted path to the OS, regardless of what
applications are running. Since Windows NT, the OS has
required that users unlock their computer with this sequence
of keys, a secure attention sequence, before logging into the
device. This sequence stops the execution of other processes,
ensuring the existence of a trusted path for the authentica-
tion process [12]. Alas, Windows does not explicitly tell
users not to enter their passwords without typing the se-
cure attention sequence, and legitimate applications often
ask users to do so.
A number of phishing prevention mechanisms have been
used as secure attention sequences, including a 2005 pro-
posal by Ross et al. [22]. Libonati et al. performed a ﬁeld
study to measure the eﬃcacy of secure attention sequences
in protecting web logins. No mechanism came close to being
foolproof, even though participants in the study knew that
they were being tested on their abilities to protect them-
selves from attacks, and given incentives to protect their
passwords [17].
In summary, solving the trusted path problem is daunting.
Providing dedicated input or output devices for authentica-
tion is impractical: It is costly, consumes device space, and
would require a redesign of myriad devices. Trusted chrome
has proven too easily spoofable. Establishing a trusted path
via users’ existing mobile devices for authentication simply
passes the buck onto another general computing platform,
which may also be vulnerable to spooﬁng attacks. Users
forget to enter secure attention sequences when they don’t
appear to be necessary.
the secure attention sequence to make the visual shared se-
cret appear. The secure attention sequence may make it
harder to trick the user into believing the visual shared se-