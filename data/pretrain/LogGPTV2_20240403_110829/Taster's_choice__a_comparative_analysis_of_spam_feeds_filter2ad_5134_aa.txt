title:Taster's choice: a comparative analysis of spam feeds
author:Andreas Pitsillidis and
Chris Kanich and
Geoffrey M. Voelker and
Kirill Levchenko and
Stefan Savage
Taster’s Choice: A Comparative Analysis of Spam Feeds
Andreas Pitsillidis∗
PI:EMAIL
Chris Kanich†
PI:EMAIL
Geoffrey M. Voelker∗
PI:EMAIL
Kirill Levchenko∗
PI:EMAIL
Stefan Savage∗
PI:EMAIL
∗
Department of Computer Science and Engineering
University of California, San Diego
†
Department of Computer Science
University of Illinois at Chicago
ABSTRACT
E-mail spam has been the focus of a wide variety of measure-
ment studies, at least in part due to the plethora of spam
data sources available to the research community. However,
there has been little attention paid to the suitability of such
data sources for the kinds of analyses they are used for. In
spite of the broad range of data available, most studies use
a single “spam feed” and there has been little examination
of how such feeds may diﬀer in content. In this paper we
provide this characterization by comparing the contents of
ten distinct contemporaneous feeds of spam-advertised do-
main names. We document signiﬁcant variations based on
how such feeds are collected and show how these variations
can produce diﬀerences in ﬁndings as a result.
Categories and Subject Descriptors
E.m [Data]: Miscellaneous; H.3.5 [Information Storage
and Retrieval]: On-line Information Services
General Terms
Measurement, Security
Keywords
Spam e-mail, Measurement, Domain blacklists
1.
INTRODUCTION
It is rare in the measurement of Internet-scale phenomena
that one is able to make comprehensive observations. Indeed,
much of our community is by nature opportunistic: we try
to extract the most value from the data that is available.
However, implicit in such research is the assumption that
the available data is suﬃcient to reach conclusions about
the phenomena at scale. Unfortunately, this is not always
the case and some datasets are too small or too biased to
be used for all purposes. In this paper, we explore this issue
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’12, November 14–16, 2012, Boston, Massachusetts, USA.
Copyright 2012 ACM 978-1-4503-1705-4/12/11 ...$15.00.
in the context of a common security measurement domain:
e-mail spam.
On the one hand e-mail spam is plentiful—everyone gets
it—and thus is deceptively easy to gather. At the same time,
the scale of the e-mail spam problem is enormous. Industry
estimates (admittedly based on unknown methodology) sug-
gest that spammers sent well over 100 billion e-mails each
day in 2010 [16]. If true, then even a spam corpus consisting
of 100,000 messages per day would constitute only one ten
thousandth of one percent of what occurred globally. Thus,
except for researchers at the very largest e-mail providers, we
are all forced to make extrapolations by many orders of mag-
nitude when generalizing from available spam data sources.
Further, in making these extrapolations, we must assume
both that our limited samples are suﬃciently unbiased to
capture the general behavior faithfully and that the behavior
is large enough to be resolved via our measurements (con-
cretely, that spam is dominated by small collections of large
players and not vice versa). However, we are unaware of any
systematic attempt to date to examine these assumptions
and how they relate to commonly used data sources.
To explore these questions, we compare contemporaneous
spam data from ten diﬀerent data feeds, both academic
and commercial, gathered using a broad range of diﬀerent
collection methodologies. To address diﬀerences in content,
we focus on the Internet domain names advertised by spam
messages in such feeds, using them as a key to identify like
messages. Using this corpus, corresponding to over a billion
messages distributed over three months, we characterize
the relationships between its constituent data sources. In
particular, we explore four questions about “feed quality”:
purity (how much of a given feed is actually spam?), coverage
(what fraction of spam is captured in any particular feed?),
timing (can a feed can be used to determine the start and
end of a spam campaign?) and proportionality (can one use
a single feed to accurately estimate the relative volume of
diﬀerent campaigns?).
Overall, we ﬁnd that there are signiﬁcant diﬀerences across
distinct spam feeds and that these diﬀerences can frequently
defy intuition. For example, our lowest-volume data source
(comprising just over 10 million samples) captures more spam-
advertised domain names than all other feeds combined (even
though these other feeds contain two orders of magnitude
more samples). Moreover, we ﬁnd that these diﬀerences in
turn translate into analysis limitations; not all feeds are good
for answering all questions. In the remainder of this paper,
we place this problem in context, describe our data sources
427and analysis, and summarize some best practices for future
spam measurement studies.
2. BACKGROUND
E-mail spam is perhaps the only Internet security phe-
nomenon that leaves no one untouched. Everybody gets
spam. Both this visibility and the plentiful nature of spam
have naturally conspired to support a vast range of empirical
measurement studies. Some of these have focused on how
to best ﬁlter spam [3, 5, 7], others on the botnets used to
deliver spam [11, 30, 42], and others on the goals of spam,
whether used as a vector for phishing [25], malware [12, 22]
or, most commonly, advertising [17, 18].
These few examples only scratch the surface, but impor-
tantly this work is collectively not only diverse in its analyses
aims, but also in the range of data sources used to drive
those same conclusions. Among the spam sources included in
such studies are the authors’ own spam e-mail [3, 44], static
spam corpora of varied provenance (e.g., Enron, TREC2005,
CEAS2008) [10, 26, 34, 41, 44], open mail proxies or relays [9,
28, 29], botnet output [11, 30], abandoned e-mail domains [2,
13], collections of abandoned e-mail accounts [39], spam au-
tomatically ﬁltered at a university mail server [4, 31, 35, 40],
spam-fed URL blacklists [24], spam identiﬁed by humans in
a large Web-mail system [42, 43], spam e-mail ﬁltered by a
small mail service provider [32], spam e-mail ﬁltered by a
modest ISP [6] and distributed collections of honeypot e-mail
accounts [36].
These data sources can vary considerably in volume —
some may collect millions of spam messages per day, while
others may gather several orders of magnitude fewer. In-
tuitively, it seems as though a larger data feed is likely to
provide better coverage of the spam ecosystem (although,
as we will show, this intuition is misleading). However, an
equally important concern is how diﬀerences in the manner
by which spam is collected and reported may impact the
kind of spam that is found.
To understand how this may be, it is worth ﬁrst reﬂecting
on the operational diﬀerences in spamming strategies. A
spammer must both obtain an address list of targets and
arrange for e-mail delivery. Each of these functions can be
pursued in diﬀerent ways, optimized for diﬀerent strategies.
For example, some spammers compile or obtain enormous
“low-quality” address lists [15] (e.g., based on brute force
address generation, harvesting of Web sites, etc.), many of
which may not even be valid, while others purchase higher
quality address lists that target a more precise market (e.g.,
customers who have purchased from an online pharmacy
before). Similarly, some spam campaigns are “loud” and
use large botnets to distribute billions of messages (with an
understanding that the vast majority will be ﬁltered [12])
while other campaigns are smaller and quieter, focusing on
“deliverability” by evading spam ﬁlters.
These diﬀerences in spammer operations in turn can inter-
act with diﬀerences in collection methodology. For example,
spam collected via MX honeypots (accepting all SMTP con-
nections to a quiescent domain) will likely contain broadly
targeted spam campaigns and few false positives, while e-mail
manually tagged by human recipients (e.g., by clicking on a
“this is spam” button in the mail client) may self-select for
“high quality” spam that evades existing automated ﬁlters,
but also may include legal, non-bulk commercial mail that is
simply unwanted by the recipient.
In addition to properties of how spam data is collected, how
the data is reported can also introduce additional limitations.
For example, some data feeds may include the full contents
of e-mail messages, but many providers are unwilling to do so
due to privacy concerns. Instead, some may redact some of
the address information, while, even more commonly, others
will only provide information about the spam-advertised
URLs contained with a message. Even within URL-only feeds
there can be considerable diﬀerences. Some data providers
may include full spam-advertised URLs, while others scrub
the data to only provide the fully-qualiﬁed domain name
(particularly for non-honeypot data, due to concern about
side-eﬀects from crawling such data). Sometimes data is
reported in raw form, with a data record for each and every
spam message, but in other cases providers aggregate and
summarize. For example, some providers will de-duplicate
identically advertised domains within a given time window,
and domain-based blacklists may only provide a single record
for each such advertised domain.
Taken together, all of these diﬀerences suggest that dif-
ferent kinds of data feeds may be more or less useful for
answering particular kinds of questions. It is the purpose of
this paper to put this hypothesis on an empirical footing.
3. DATA AND METHODOLOGY
In this work we compare ten distinct sources of spam data
(which we call feeds), ranging in their level of detail from full
e-mail content to only domain names of URLs in messages.
Comparisons between feeds are by necessity limited to the
lowest common denominator, namely domain names.
In
the remainder of this paper we treat each feed as a source
of spam-advertised domains, regardless of any additional
information available.
By comparing feeds at the granularity of domain names,
we are implicitly restricting ourselves to spam containing
URLs, that is, spam that is a Web-oriented advertisement in
nature, at the exclusion of some less common classes of spam
(e.g., malware distribution or advance fee fraud). Fortunately,
such advertising spam is the dominant class of spam today.1
3.1 Domains
Up to this point, we have been using the term “domain”
very informally. Before going further, however, let us say
more precisely what we mean: a registered domain—in this
paper, simply a domain—is the part of a fully-qualiﬁed
domain name that its owner registered with the registrar. For
the most common top-level domains, such as com, biz, and
edu, this is simply the second-level domain (e.g., “ucsd.edu”).
All domain names at or below the level of registered domain
(e.g., “cs.ucsd.edu”) are administered by the registrant, while
all domain names above (e.g., “edu”) are administered by
the registry. Blacklisting generally operates at the level
of registered domains, because a spammer can create an
arbitrary number of names under the registered domain
name to frustrate ﬁne-grained blacklisting below the level of
registered domains.
1One recent industry report [37] places Web-oriented advertising
spam for pharmaceuticals at over 93% of all total spam volume.
4283.2 Types of Spam Domain Sources
The spam domain sources used in this study fall into ﬁve
categories: botnet-collected, MX honeypots, seeded honey
accounts, human identiﬁed, and blacklists. Each category
comes with its own unique characteristics, limitations and
tradeoﬀs that we discuss brieﬂy here.
Botnet.
Botnet datasets result from capturing instances
of bot software and executing them in a monitored, controlled
environment such that the e-mail they attempt to send is
collected instead. Since the e-mail collected is only that sent
by the botnet, such datasets are highly “pure”: they have
no false positives under normal circumstances.2 Moreover,
if we assume that all members of a botnet are used in a ho-
mogeneous fashion, then monitoring a single bot is suﬃcient
for identifying the spamming behavior of the entire botnet.
Botnet data is also highly accessible since a researcher can
run an instance of the malware and obtain large amounts
of botnet spam without requiring a relationship with any
third-party security, mail or network provider [11]. Moreover,
since many studies have documented that a small number
of botnets are the primary source of spam e-mail messages,
in principle such datasets should be ideally suited for spam
studies [11, 21, 30]. Finally, these datasets have the advan-
tage of often being high volume, since botnets are usually
very aggressive in their output rate.
MX honeypot.
MX honeypot spam is the result of
conﬁguring the MX record for a domain to point to an SMTP
server that accepts all inbound messages. Depending on how
these domains are obtained and advertised, they may select
for diﬀerent kinds of spam. For example, a newly registered
domain will only capture spam using address lists created
via brute force (i.e., sending mail to popular user names at
every domain with a valid MX). By contrast, MX honeypots
built using abandoned domains or domains that have become
quiescent over time may attract a broader set of e-mail,