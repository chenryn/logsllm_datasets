by the momentum encoder for augmented inputs created from in-
puts in previous several mini-batches. Moreover, the dictionary is
dynamically updated during the pre-training of the image encoder.
Given a mini-batch of 𝑁 inputs, MoCo creates two augmented
inputs for each input in the mini-batch. The two augmented inputs
are respectively passed to the image encoder and the momentum
encoder. For simplicity, we use u𝑖 and u𝑗 to denote these two aug-
mented inputs. Given the two augmented inputs, the image encoder
ℎ, the momentum encoder ℎ𝑚, and the dictionary Γ, MoCo defines
a contrastive loss as follows:
ℓ(u𝑖)
= − log(
𝑒𝑥𝑝(𝑠𝑖𝑚(ℎ(u𝑖), ℎ𝑚(u𝑗))/𝜏) + 
𝑒𝑥𝑝(𝑠𝑖𝑚(ℎ(u𝑖), ℎ𝑚(u𝑗))/𝜏)
𝑒𝑥𝑝(𝑠𝑖𝑚(ℎ(u𝑖), z)/𝜏) ),
(1)
where 𝑒𝑥𝑝 is the natural exponential function, 𝑠𝑖𝑚 computes cosine
similarity between two vectors, and 𝜏 represents a temperature
parameter. The final contrastive loss is summed over the contrastive
loss ℓ(u𝑖) of the 𝑁 augmented inputs (i.e., 𝑢𝑖’s) corresponding to
the 𝑁 inputs. MoCo pre-trains the image encoder via minimizing
the final contrastive loss. Finally, the 𝑁 key vectors (i.e., ℎ𝑚(u𝑗)’s)
outputted by the momentum encoder for the 𝑁 augmented inputs
(i.e., 𝑢 𝑗’s) are enqueued to the dictionary while the 𝑁 key vectors
from the “oldest” mini-batch are dequeued.
SimCLR [10]: Similar to MoCo [20], SimCLR also tries to pre-
train an image encoder on unlabeled images. Given a mini-batch
of 𝑁 inputs, SimCLR creates two augmented inputs for each input
in the mini-batch via data augmentation. Given 2 · 𝑁 augmented
inputs (denoted as {u1, u2, · · · , u2·𝑁 }), SimCLR aims to pre-train
the image encoder such that it outputs similar (or dissimilar) feature
vectors for two augmented inputs that are created from the same
(or different) input(s). Formally, given a pair (u𝑖, u𝑗) of augmented
inputs created from the same input, the contrastive loss is defined
as follows:
ℓ𝑖 𝑗 = − log(
2·𝑁
𝑘=1 I(𝑘 ≠ 𝑖) · 𝑒𝑥𝑝(𝑠𝑖𝑚(𝑔(ℎ(u𝑖)), 𝑔(ℎ(u𝑘)))/𝜏) ), (2)
𝑒𝑥𝑝(𝑠𝑖𝑚(𝑔(ℎ(u𝑖)), 𝑔(ℎ(u𝑗)))/𝜏)
z∈Γ
where I is an indicator function, 𝑒𝑥𝑝 is the natural exponential
function, 𝑠𝑖𝑚 computes cosine similarity between two vectors, ℎ is
the image encoder, 𝑔 is the projection head, and 𝜏 is a temperature
parameter. The final contrastive loss is summed over the contrastive
loss ℓ𝑖 𝑗 of all 2 · 𝑁 pairs of augmented inputs, where each input
corresponds to two pairs of augmented inputs (u𝑖, u𝑗) and (u𝑗 , u𝑖).
SimCLR pre-trains the image encoder via minimizing the final
contrastive loss.
CLIP [38]: CLIP jointly pre-trains an image encoder and a text
encoder on unlabeled (image, text) pairs. In particular, the text en-
coder takes a text as input and outputs a feature vector for it. Given
a mini-batch of 𝑁 (image, text) pairs, CLIP creates an augmented
input image from each input image. For each augmented input
image, CLIP forms a correct (image, text) pair using the augmented
input image and the text that originally pairs with the input image
from which the augmented input image is created, and CLIP forms
(𝑁 − 1) incorrect pairs using the augmented input image and the
remaining (𝑁 − 1) texts. Therefore, there are 𝑁 correct pairs and
𝑁 · (𝑁 − 1) incorrect pairs in total. CLIP jointly pre-trains an image
Session 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2083encoder and a text encoder such that, for a correct (or incorrect)
pair of (image, text), the feature vector outputted by the image
encoder for the augmented input image is similar (or dissimilar) to
the feature vector outputted by the text encoder for the text.
Observation: We observe that these contrastive learning algo-
rithms try to pre-train an image encoder that outputs similar feature
vectors for two augmented inputs that are created from the same
input. Specifically, we can have this observation for MoCo [20] and
SimCLR [10] based on the definitions of their contrastive losses.
For CLIP, given an (image, text) pair, the feature vector outputted
by the image encoder for an augmented version of the image is
similar to the feature vector outputted by the text encoder for the
text. Therefore, the feature vectors outputted by the image encoder
for two augmented inputs created from the input image are similar
since both of them are similar to the feature vector outputted by
the text encoder for the given text. As we will discuss in Section 4,
our EncoderMI leverages this observation to infer members of an
image encoder’s pre-training dataset.
2.2 Training Downstream Classifiers
The image encoder can be used as a feature extractor for many
downstream tasks. We consider the downstream task to be image
classification in this work. In particular, suppose we have a labeled
dataset (called downstream dataset). We first use the image encoder
to extract feature vectors for inputs in the downstream dataset.
Then, we follow the standard supervised learning to train a classifier
(called downstream classifier) using the extracted feature vectors
as well as the corresponding labels. Given a testing input from
the downstream task, we first use the image encoder to extract
the feature vector for it and then use the downstream classifier to
predict a label for the extracted feature vector. The predicted label
is viewed as the prediction result for the testing input.
3 PROBLEM FORMULATION
3.1 Threat Model
Inferrer’s goal: Given an input image x, an inferrer aims to infer
whether it is in the pre-training dataset of an image encoder (called
target encoder). We call an input a member of the target encoder if
the input is in its pre-training dataset, otherwise we call the input a
non-member. The inferrer aims to achieve high accuracy at inferring
the members/non-members of the target encoder.
Inferrer’s background knowledge: We consider an inferrer has a
black-box access to the target encoder. We note that this is the most
difficult and general scenario for the inferrer. A typical application
scenario is that the encoder provider pre-trains an encoder and then
provides an API to downstream customers. The pre-training of an
encoder depends on three key dimensions, i.e., pre-training data
distribution, encoder architecture, and training algorithm (e.g., MoCo,
SimCLR). Therefore, we characterize the inferrer’s background
knowledge along these three dimensions.
• Pre-training data distribution. This background knowl-
edge characterizes whether the inferrer knows the distribu-
tion of the pre-training dataset of the target encoder. In partic-
ular, if the inferrer knows the distribution, we assume he/she
has access to a shadow dataset that has the same distribution
as the pre-training dataset. Otherwise, we assume the inferrer
has access to a shadow dataset that has a different distribution
from the pre-training dataset. Note that, in both cases, we
consider the shadow dataset does not have overlap with the
pre-training dataset. For simplicity, we use P to denote this
dimension of background knowledge.
• Encoder architecture. The inferrer may or may not know
the architecture of the target encoder. When the inferrer does
not know the target-encoder architecture, the inferrer can
assume one and perform membership inference based on
the assumed one. For instance, when the target encoder uses
ResNet architecture, the inferrer may assume VGG architec-
ture when performing membership inference. We use E to
denote this dimension of background knowledge.
• Training algorithm. This dimension characterizes whether
the inferrer knows the contrastive learning algorithm used
to train the target encoder. When the inferrer does not know
the training algorithm, the inferrer can perform membership
inference based on an assumed one. For instance, when the
training algorithm of the target encoder is MoCo [20], the
inferrer may perform membership inference by assuming the
training algorithm is SimCLR [10]. We use T to denote this
dimension of background knowledge.
We use a triplet B = (P, E, T) to denote the three dimensions
of the inferrer’s background knowledge. Each dimension in B can
be “yes” or “no”, where a dimension is “yes” ( or “no”) when the
corresponding dimension of background knowledge is available (or
unavailable) to the inferrer. Therefore, we have eight different types
of background knowledge in total. For instance, the inferrer knows
the pre-training data distribution, architecture of the target encoder,
and/or training algorithm when the encoder provider makes them
public to increase transparency and trust.
Inferrer’s capability: An inferrer can query the target encoder
for the feature vector of any input or an augmented input.
3.2 Membership Inference
Given the inferrer’s goal, background knowledge, and capability,
we define our membership inference against contrastive learning
as follows:
Definition 3.1 (Membership Inference against Contrastive Learn-
ing). Given a black-box access to a target encoder, the background
knowledge B = (P, E, T), and an input, membership inference
aims to infer whether the input is in the pre-training dataset of the
target encoder.
4 OUR METHOD
4.1 Overview
Recall that a target encoder is trained to output similar feature
vectors for the augmented versions of an input in the pre-training
dataset. Our EncoderMI is based on this observation. Specifically,
when an encoder is overfitted to its pre-training dataset, the encoder
may output more (or less) similar feature vectors for augmented
inputs created from an input in (or not in) the pre-training dataset.
Therefore, our EncoderMI infers an input to be a member of the
target encoder if the target encoder produces similar feature vectors
for the augmented versions of the input. Specifically, in EncoderMI,
Session 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2084an inferrer builds a binary classifier (called inference classifier),
which predicts member/non-member for an input based on some
features we create for the input. To distinguish with the feature
vectors produced by the target encoder, we call the features used
by the inference classifier membership features. Our membership
features of an input are based on the similarity scores between the
feature vectors of the input’s augmented versions produced by the
target encoder. Building our inference classifier requires a training
dataset (called inference training dataset) which consists of known
members and non-members. To construct an inference training
dataset, we split the inferrer’s shadow dataset into two subsets,
which we call shadow member dataset and shadow non-member
dataset, respectively. Then, the inferrer pre-trains an encoder (called
shadow encoder) using the shadow member dataset. We construct
an inference training dataset based on the shadow encoder and
shadow dataset. Specifically, each input in the shadow member (or
non-member) dataset is a member (or non-member) of the shadow
encoder, and we create membership features for each input in the
shadow dataset based on the shadow encoder. After building an
inference classifier based on the inference training dataset, we apply
it to infer members/non-members of the target encoder.
𝑠
4.2 Building Inference Classifiers
We first introduce how to train a shadow encoder on a shadow
dataset. Then, we discuss how to extract membership features for
an input. Finally, we discuss how to construct an inference training
dataset based on the shadow encoder and the shadow dataset, and
given the constructed inference training dataset, we discuss how
to build inference classifiers.
Training a shadow encoder: The first step of our EncoderMI
is to train a shadow encoder whose ground truth members/non-
members are known to the inferrer. For simplicity, we use ˜ℎ to
denote the shadow encoder. In particular, the inferrer splits its
shadow dataset D𝑠 into two non-overlapping subsets: shadow mem-
ber dataset (denoted as D𝑚
𝑠 ) and shadow non-member dataset (de-
noted as D𝑛𝑚
). Then, the inferrer pre-trains a shadow encoder
using the shadow member dataset D𝑚
𝑠 . If the inferrer has access
to the target encoder’s architecture (or training algorithm), the
inferrer uses the same architecture (or training algorithm) for the
shadow encoder, otherwise the inferrer assumes an architecture
(or training algorithm) for the shadow encoder. We note that each
input in the shadow member (or non-member) dataset is a member
(or non-member) of the shadow encoder.
Extracting membership features: For each input in the shadow
dataset, we extract its membership features based on the shadow en-
coder ˜ℎ. Our membership features are based on the key observation
that an encoder (e.g., target encoder, shadow encoder) pre-trained
by contrastive learning produces similar feature vectors for aug-
mented versions of an input in the encoder’s pre-training dataset.
Therefore, given an input x, we first create 𝑛 augmented inputs
using the data augmentation module A of the training algorithm
used to pre-train the shadow encoder. We denote the 𝑛 augmented
inputs as x1, x2, · · · , x𝑛. Then, we use the shadow encoder to pro-
duce a feature vector for each augmented input. We denote by ˜ℎ(x𝑖)
the feature vector produced by the shadow encoder ˜ℎ for the aug-
mented input x𝑖, where 𝑖 = 1, 2, · · · , 𝑛. Our membership features for
the input x consist of the set of pairwise similarity scores between
the 𝑛 feature vectors. Formally, we have:
M(x, ˜ℎ) = {𝑆( ˜ℎ(x𝑖), ˜ℎ(x𝑗))|𝑖 ∈ [1, 𝑛], 𝑗 ∈ [1, 𝑛], 𝑗 > 𝑖},
(3)
where M(x, ˜ℎ) is our membership features for an input x based on
encoder ˜ℎ, and 𝑆(·, ·) measures the similarity between two feature
vectors (e.g., 𝑆(·, ·) can be cosine similarity). Note that we omit the
explicit dependency of M(x, ˜ℎ) on 𝑆, A, and 𝑛 for simplicity. There
are 𝑛 · (𝑛 − 1)/2 similarity scores in M(x, ˜ℎ) and they tend to be
large if the input x is a member of the shadow encoder ˜ℎ.
Constructing an inference training dataset: Given the shadow
member dataset D𝑚
, and
the shadow encoder ˜ℎ, we construct an inference training dataset,
which is used to build an inference classifier. In particular, given
𝑠 , we extract its membership features M(x, ˜ℎ)
an input x ∈ D𝑚
and assign a label 1 to it; and given an input x ∈ D𝑛𝑚
, we extract
its membership features M(x, ˜ℎ) and assign a label 0 to it, where
the label 1 represents “member" and the label 0 represents “non-
member". Formally, our inference training dataset (denoted as E) is
as follows:
𝑠 , the shadow non-member dataset D𝑛𝑚
𝑠
𝑠
E ={(M(x, ˜ℎ), 1)|x ∈ D𝑚
𝑠 } ∪ {(M(x, ˜ℎ), 0)|x ∈ D𝑛𝑚
𝑠
}.
(4)
Building inference classifiers: Given the inference training dataset
E, we build a binary inference classifier. We consider three types
of classifiers, i.e., vector-based classifier, set-based classifier, and
threshold-based classifier. These classifiers use the membership fea-
tures differently. Next, we discuss them one by one.
• Vector-based classifier (EncoderMI-V). In a vector-based
classifier, we transform the set of membership featuresM(x, ˜ℎ)
of an input into a vector. Specifically, we rank the 𝑛 · (𝑛 − 1)/2
similarity scores in M(x, ˜ℎ) in a descending order. We apply
the ranking operation to the membership features of each
input in the inference training dataset E. Then, we train a
vector-based classifier (e.g., a fully connected neural network)
on E following the standard supervised learning procedure.
We use 𝑓𝑣 to denote the vector-based classifier. Moreover, we