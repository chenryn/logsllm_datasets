Coverage Histogram
Handles, w Specials
Handles, w/o Specials
w Specials
w/o Specials
1
1
0
e-1
5
1
0
e-1
4
1
0
e-1
3
1
0
e-1
2
1
0
e-1
1
1
0
e-1
0
0
e-0
1
0
e-9
1
0
e-8
1
0
e-7
1
0
e-6
1
0
e-5
1
0
e-4
1
0
e-3
1
0
e-2
1
0
e-1
Coverage classes
Figure 5. The coverage of executed test
cases compared to the set of input vectors
in different test conﬁgurations.
Test conﬁg
w/o Sp.
Han., w Sp.
Han., w/o Sp.
w Sp.
Avg. Cov.
Avg. Incomplete Cov.
4.422906 · 10−01
2.763518 · 10−01
4.389137 · 10−01
2.763809 · 10−01
7.086199 · 10−02
2.272866 · 10−02
6.748508 · 10−02
2.275769 · 10−02
Table 2. The average coverage per test con-
ﬁguration. Avg. Incomplete Coverage is the av-
erage coverage over all functions that have a
coverage < 1.
order of magnitude of f ’s test coverage. The Y-axis depicts
the number of functions per class.
The handle test types have no visible impact. But the av-
erage coverage of the two test conﬁgurations with handles
differs slightly from the average coverage for test conﬁg-
urations without handles as shown in table 2. The impact
of excluding some test cases via static analysis is visible.
In the two conﬁgurations where special test cases are ex-
cluded, more functions have a coverage with a lower order
of magnitude than in the conﬁgurations where all special
test cases are included. Some functions do not beneﬁt from
this exclusion because they perform indirect function calls
via function pointers. These functions were excluded from
the static analysis. For 76 of the 148 functions the number
of test cases could be reduced.
Autocannon is not only useful to generate protection hy-
potheses. It is a ﬂexible dependability benchmark for arbi-
trary C libraries. Figure 6 gives a summary of the bench-
mark results for the APR. It only covers the functions that
we have tested. More than half of the test cases resulted in
robust behavior. The gap between the number of robust and
unrobust test cases is about 70, 000. Figure 7 shows how
the robustness is distributed over the tested functions. The
majority of the functions is split in two sets: 67 functions
Robustness
s
l
l
a
C
n
o
i
t
c
n
u
F
#
 800000
 700000
 600000
 500000
 400000
 300000
 200000
 100000
 0
Crashes
Returned
Figure 6. Comparing number of crashed and
robust test cases.
Robustness per Function
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
 20
 40
 60
 80
 100
 120
 140
 160
 180
Function No.
s
l
l
a
C
l
l
A
/
s
l
l
a
C
d
e
h
s
a
r
C
Figure 7. Variation in percentage of unrobust
test cases per function.
crashed in at least 90% of the test cases and 66 functions
crashed in at most 10% of the test cases. In between are 40
functions. Functions without arguments were not tested.
To evaluate the correctness of the protection hypotheses,
we ﬁrst executed Apache with the protection wrapper and
logged all failed hypotheses. We assume that Apache does
call all APR functions with valid arguments as long as no
faults are injected by a third party. Even though it is possi-
ble, we did not manually correct any protection hypotheses.
We generated two wrappers: W was generated from fault
injection experiments with lower test coverage and WSA
was generated from fault injection experiments with higher
test coverage using static analysis. We switched the handle
test types on for both wrappers. The results are shown in
the ﬁrst two data rows of Table 3. W produced no false
positives. WSA had 10 functions leading to false positives
(6.17 % of all function calls). 3 of these functions were also
wrapped by W .
Additionally, we implemented a micro benchmark with
fault injection to test our protection in the presence of fail-
ures. Therefore, we run Apache with our protection wrap-
# Hypotheses
FP normal Run
Predicted
Microbench.
FP Microbench.
Crashes
with Specials without Specials
77
0%
56.81%
95
6.17%
51.39%
1.7% (17)
0.6% (6)
Table 3. Correctness of our Approach
per. We injected probabilistically bit-ﬂips [9] into argument
values of protected functions. Whenever a bit-ﬂip was in-
jected when calling a function f , we compared the result
of f ’s hypothesis and the behavior of executing f . Not all
bit-ﬂips resulted in a crash of f . Apache was restarted after
the bit-ﬂip to prevent further propagation of the fault. The
results show a trade-off between W and WSA (see Table 3).
W predicted more crashes than WSA but it also had more
false positives.
7 Related Work
In this paper, we combine dependability benchmarking
[10, 11] and automatic patch generation [8, 15, 14]. With
HEALERS [8] the second author already presented a gen-
eral approach to harden COTS libraries. But HEALERS
contains an inﬂexible type system that couples test types
and checks: that makes it very difﬁcult to extend. The map-
ping from argument types to test types is done via a pre-
deﬁned map. Therefore, HEALERS was only able to test
4 functions of the APR. All other functions contained un-
known argument types. HEALERS cannot tolerate contra-
dictions. All of these 4 functions produced a contradiction.
So HEALERS was unable to generate any protection hy-
pothesis for the APR.
A previous AutoPatch paper [15] presents an approach to
patch bad error handling. Our current work might introduce
unexpected error values when a hypothesis is evaluated to
false. The application might not be able to deal with this
unexpected errors and might itself behave unrobust or in-
secure. One can apply the bad error handling patching to
counter this problem.
Stelios et. al. introduced an approach to automatically
patch buffer overrun bugs in applications. They also evalu-
ated their approach with Apache. But because they ﬁx bugs
within the application and not at the interface to dynamic
libraries, their approach is not quantitative comparable to
ours. The approach needs some code that exploits the bug
to patch, for instance a zero-day exploit. The patch is based
in source code transformation. In contrast to this, we are
able to detect bugs and patch without any exploiting code
as input.
Our current work is based partly on Ballista [10, 11], a
dependability benchmark for POSIX implementations. We
have already presented in detail how we extended Ballista
to our test system Autocannon. The biggest difference is,
that Ballista is bound to a speciﬁc API while Autocannon is
more general and can test arbitrary APIs.
We also contribute to improving the availability. An un-
robust system has a lower availability than a robust system
in an adverse environment. By increasing the robustness we
increase the mean-time-to-failure. This is orthogonal to in-
creasing the mean-time-to-repair, for instance with micro-
reboots [3]. Both approaches can by used together to in-
crease availability.
8 Conclusion
We have presented a ﬂexible approach to hardening arbi-
trary libraries for robustness and security. Our contributions
are: (1) a new dependability benchmark that can measure
the robustness of arbitrary library utilizing static analysis
and (2) a table based approach to derive protection hypothe-
ses from the benchmark’s results. This is done by classify-
ing the benchmark’s test data by checks. The difference
to previous work is that our approach is easily extensible.
One can add new checks and test types for the dependabil-
ity benchmarks as needed.
Our protection hypotheses were able to predict up to
56.85 % of crashes in our evaluation. The drawback is that
our hypotheses misclassify a low number of robust argu-
ment values as unrobust. But we believe that we can over-
come this issue by adding more appropriate checks and test
types.
Acknowledgements We would like to thank Martin Kret-
zschmar for introducing us to LLVM.
References
[1] Apache Software Foundation. Apache portable runtime
project. http://apr.apache.org.
[2] D. Box and C. Sells. Essential . NET 1. The Common
Language Runtime, volume 1. Addison-Wesley Longman,
November 2002.
[3] G. Candea, S. Kawamoto, Y. Fujiki, G. Friedman, and
A. Fox. Microreboot – a technique for cheap recovery. In 6th
Symposium on Operating Systems Design and Implementa-
tion (OSDI), pages 31–44, December 2004.
[4] U. Drepper.
How to write shared libraries.
report, Red Hat,
nical
Park, NC, Tech. Rep.,
http://people.redhat.com/drepper/dsohowto.pdf.
Tech-
Inc., Research Triangle
vailable:
January 2005.
[5] D. Engler. Weird things that surprise academics try-
ing to commercialize a static checking tool.
Part
of an invited talk at SPIN05 and CONCUR05, 2005.
http://www.stanford.edu/ engler/spin05-coverity.pdf.
[6] D. Engler, D. Y. Chen, S. Hallem, A. Chou, and B. Chelf.
Bugs as deviant behavior: a general approach to inferring
In SOSP ’01: Proceedings of the
errors in systems code.
eighteenth ACM symposium on Operating systems princi-
ples, pages 57–72, New York, NY, USA, 2001. ACM Press.
[7] C. Fetzer and Z. Xiao. A ﬂexible generator architecture for
improving software dependability.
In Procceedings of the
Thirteenth International Symposium on Software Reliability
Engineering (ISSRE), pages 155–164, Annapolis, MD, Nov
2002.
[8] C. Fetzer and Z. Xiao. Healers: A toolkit for enhancing
the robustness and security ofexisting applications. In Inter-
national Conference on Dependable Systems and Networks
(DSN2003 demonstration paper), San Francisco, CA, USA,
June 2003.
[9] J.Arlat and Y.Crouzet. Faultload representativeness for de-
pendability benchmarking. In Workshop on Dependability
Benchmarking, pages 29–30, June 2002.
[10] P. Koopman and J. DeVale. Comparing the robustness of
posix operating systems.
In FTCS ’99: Proceedings of
the Twenty-Ninth Annual International Symposium on Fault-
Tolerant Computing, page 30, Washington, DC, USA, 1999.
IEEE Computer Society.
[11] P. Koopman and J. DeVale. The exception handling effec-
IEEE Trans. Softw.
tiveness of posix operating systems.
Eng., 26(9):837–848, 2000.
[12] C. Lattner and V. Adve. LLVM: A Compilation Frame-
work for Lifelong Program Analysis & Transformation. In
Proceedings of the 2004 International Symposium on Code
Generation and Optimization (CGO’04), Palo Alto, Califor-
nia, Mar 2004.
[13] K. Pattabiraman, G. P. Saggese, D. Chen, Z. Kalbarczyk,
and R. K. Iyer. Dynamic derivation of application-speciﬁc
error detectors and their implementation in hardware. In In-
proceedings of the Sixth European Dependable Computing
Conference (EDCC 2006), October 2006.
[14] S. Sidiroglou and A. D. Keromytis. Countering network
worms through automatic patch generation. Technical re-
port, Columbia University Computer Science Department,
2003.
[15] M. S¨ußkraut and C. Fetzer. Automatically ﬁnding and patch-
ing bad error handling.
In Inproceedings of the Sixth Eu-
ropean Dependable Computing Conference (EDCC 2006),
October 2006.
[16] M. S¨ußkraut and C. Fetzer. Learning library-level error re-
turn values from syscall error injection.
In Inproceedings
of the Sixth European Dependable Computing Conference
(EDCC 2006) [Fast Abstract], volume Proceedings Suple-
mental, 2006.
[17] D. van Heesch. Doxygen. http://www.doxygen.org.
[18] J. Yang, D. Evans, D. Bhardwaj, T. Bhat, and M. Das. Ter-
racotta: Mining temporal api rules from imperfect traces.
In 28 th International Conference on Software Engineering,
May 2006. http://www.cs.virginia.edu/terracotta/.