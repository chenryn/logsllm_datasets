ξi for each submodel ΞNum.
ξi as the
end while
dependency. The set ΞNum contains all submodels that
have neither rare events nor reward dependencies. From
Proposition 1, reward variable solution does not depend
on ΞNum. Thus we need only solve the state occupancy
probability for all submodels in ΞNum at the time of the
next rare event ﬁring. We do so by making the assumption
that the submodels enter steady state between ﬁrings of rare
events. This assumption seems appropriate for two reasons.
The ﬁrst is the high probability of a long inter-event time
between rare events. The second relates to the fact that for
the storage systems in which we are interested, systems tend
to enter into steady state instantly in the absence of rare
events. The scrub process, for example, is always in steady
state; the same holds true for many recovery, propagation,
or mitigation submodels. Simulation of the model M is
performed using Algorithm 2, a modiﬁed version of a
standard discrete event simulator.
The general improvement offered by this algorithm comes
from the reduction of events that must be simulated in order
to estimate the effect of rare events in the system. Bucklew
and Radeke [26] give a general rule of thumb that in order to
estimate the impact of an event with probability ρ, we must
process approximately 100/ρ simulations. Our method seeks
to reduce the number of events that must processed for each
computed trajectory by eliminating those events that cannot
impact ΘM without the ﬁring of a rare event.
The performance improvement offered by this algorithm
varies with the model and with the degree of dependence
of the state variables and events in the model. For models
whose resulting Ξ do not have the proper structure, our
proposed hybrid simulator may provide no improvement.
Between ﬁrings of a rare event, our method will produce
a speed-up proportional to the rate at which we remove
events from explicit simulation. Thus given E(cid:48) as the set
P
of all events ei ∈ ΞNum ∪ ej ∈ M such that ej /∈ Ξ, our
P
ei∈E λ(ei,ψi) ,∀ψi ∈ Ψ.
improvement is proportional to
This improvement is due to the removal of events that, while
enabled, cannot change the state of our model in a way that
inﬂuences our reward variables. For instance, a write process
may still be enabled, but in the absence of a UDE the write
process cannot result in the propagation of a UDE to parity.
ei∈E(cid:48) λ(ei,ψi)
Likewise while it is important to represent the position of the
scrub process, it cannot result in mitigation of a fault until a
fault is present to mitigate. By removing those events from
our simulator, we improve the efﬁciency of our solution.
B. Required Assumptions
A key assumption of our solution method is that the
storage sub-models reach steady state between rare events.
For the storage systems of interest, the initial transient period
is not part of the system lifetime during production. We
consider the initial state of any storage system to be fault-
free and with uniform distribution of ﬁles across the storage
system itself. The steady state of such a system can be
considered this fault-free condition, with the placement of
ﬁles described by the observed empirical distribution used
to model deduplication.
Other sub-models that are likely to be considered inde-
pendent, such as the model of the scrub process, are char-
acterized by periodic processes that are either unperturbed
by rare events in the system, or dormant in the absence of
faults. Thus, any system that is not currently composed with
another system that contains a failure can be said to be in
steady state.
C. Correctness of Reward Variables
In this section, we show the three types of reward vari-
ables deﬁned in Section II. We redeﬁne these reward vari-
ables for use with our decomposition algorithm and hybrid
solution method, described previously. We demonstrate that
the resulting reward variables are equivalent to those deﬁned
in Section II.
When solving for instant-of-time variables, we use the
same equation given in Section II:
θt = (cid:88)
ν∈P(S,N)
t +(cid:88)
e∈A
R(ν) · I ν
I(e) · I e
t
(6)
We evaluate this variable in the same fashion for the
that contains the necessary state variables to
submodel
establish each ν ∈ P(S, N) and each e ∈ A.
Proposition 2. For a given model M, a decomposed MDG
G(cid:48)
M , and an instant-of-time reward variable θt, solving
for θt using Equation 6, and the appropriate submodel
decomposition proposed by G(cid:48)
M yields the same result as
the original model M.
Proof: From Proposition 1 we know that the reward
variable θt is independent from a submodel ξj at time t if
no direct dependence exists in G(cid:48)
M from θt to a vertex in
g(cid:48)
j. Thus the solution at time t for θt for our decomposed
submodels is the same as the solution for M.
Because of our method of forming an MDG, reward
dependencies will exist between a variable and all state
Figure 8: Comparison of instant-of-time reward variable
solutions
variables and events, ensuring that the submodel is decom-
posed in such a way that the resulting submodel contains
everything necessary to evaluate θt.
To solve for interval-of-time variables using our hybrid
solution method, we must provide a new method of computa-
tion. Recall from Section II that an interval-of-time variable
θ[t,t+l] is deﬁned as follows:
θ[t,t+l] = (cid:88)
ν∈P(S,N)
[t,t+l] +(cid:88)
e∈A
R(ν)J ν
I(e)N e
[t,t+l]
(7)
We modify the computation of interval-of-time variables
to accommodate our solution technique by using multiple
random variables for J ν
[t,t+l] based on the
decomposition and re-composition of the underlying model,
as dictated by our solution techniques.
[t,t+l] and N e
As shown in Figure 8 we have a set of n model
decompositions that form intervals deﬁned by the times
d0, d1, . . . , dn−1 during the period [t, t + l]. For these in-
tervals, we create n + 1 random variables to replace J ν
and N e
[t,t+l]
[t,t+l]:
J ν
[t,d0], J ν
[t,d0], N e
[d0,d1], . . . , J ν
[d0,d1], . . . , N e
N e
[dn−1,t+l]
[dn−1,t+l]
Each of these random variables is equivalent to those from
the previous deﬁnition, but over a different interval of time.
The variables are distinguished by n+1 separate intervals
in the set
D = {[t, d0], [d0, d1], . . . , [dn−1, t + l]}
Based on those identities, we redeﬁne the method for calcu-
lating an interval-of-time variable for our solution method
as follows:
Y[t,t+l] = (cid:88)
(cid:88)
ν∈P(S,N)
d∈D
d +(cid:88)
(cid:88)
e∈A
d∈D
R(ν)J ν
I(e)N e
d
(8)
τττtd0d0d1d1t+ltt+lThe differences between that calculation and the one
shown in Section II are illustrated in Figure 8. In the
original model, we use a single random variable for each rate
and impulse reward. Using our methods, however, we need
one for each separate interval in D. The two methods are
actually equivalent, however, as the sum of the new indicator
variables yields the old indicator variables. The reason is that
the decomposition algorithm we have presented preserves
reward dependencies.
Proposition 3. For a given model M, a set of decomposed
MDGs ˆG(cid:48)
M over the interval [t, t+l], and an interval-of-time
reward variable Y[t,t+l], solving for Y[t,t+l] using Equation
8 and the appropriate submodel decompositions for each
interval in D yields the same result as the original model
M.
Speed-up
23.02
22.53
21.91
8 + p
8 + 2p
8 + 3p
Table I: Speed-up ratio of the run-times for various RAID
conﬁgurations.
that takes into account the subdivisions of the interval [t, t+l]
given by D.
Proposition 4. For a given model M, a set of decomposed
MDGs ˆG(cid:48)
M over the interval [t, t + l], and a time-averaged
interval-of-time reward variable θ(cid:48)
[t,t+l], solving for W[t,t+l]
using Equation 15 and the appropriate submodel decompo-
sitions for each interval in D yields the same result as the
original model M.
Proof: Starting from the deﬁnition from Equation 8, we
prove the equivalence of Y[t,t+l] and θ[t,t+l] by construction.
Y[t,t+l] = (cid:88)
= (cid:88)
ν∈P(S,N)
(cid:88)
R(ν)(cid:88)
d∈D
R(ν)J ν
d +(cid:88)
d +(cid:88)
e∈A
(cid:88)
I(e)(cid:88)
d∈D
J ν
ν∈P(S,N)
d∈D
e∈A
d∈D
I(e)N e
d
(9)
N e
d (10)
Given Proposition 1, which states that all state variables
and events required for calculating a reward variable are con-
tained within the submodel containing the reward variable
itself, we have that:
[t,t+l] = (cid:88)
[t,t+l] = (cid:88)
d∈D
J ν
N e
d∈D
J ν
d
N e
d
(11)
(12)
The sums of our new indicator variables are the original
indicator variables from Section II. Thus,
Y[t,t+l] = (cid:88)
ν∈P(S,N)
= θ[t,t+l].
[t,t+l] +(cid:88)
e∈A
R(ν)J ν