# 排错指南 - Pod
本文档介绍 Pod 的异常状态，可能原因和解决办法。
排查 Pod 异常的常用命令如下：
+ 查看 Pod 状态：`kubectl get pods  -n  -o wide`
+ 查看 Pod 的 yaml 配置：`kubectl get pods  -n  -o yaml`
+ 查看 Pod 的事件：`kubectl describe pods  -n `
+ 查看 Pod 容器日志：`kubectl logs -n   [-c ]`
## Pod 一直处于 Pending 状态
Pending 状态说明 Pod 还没有调度到某个 Node 上面。可以通过 `kubectl describe pods  -n ` 命令查看到 Pod 的事件，进而判断为什么没有调度。如：
    ``` bash
    $ kubectl describe pod mypod
    ...
    Events:
    Type     Reason            Age                From               Message
    ----     ------            ----               ----               -------
    Warning  FailedScheduling  12s (x6 over 27s)  default-scheduler  0/4 nodes are available: 2 Insufficient cpu.
    ```
可能的原因和解决方案：
1. 集群的 kube-scheduler 服务都挂掉了：
    可以在各节点运行 `ps -elf | grep kube-scheduler` 命令来验证。如果挂掉，可以使用 `systectl start kube-scheduler` 命令启动服务；
1. 节点没有打标签或打的标签值不匹配：
    如 Pod 指定了 nodeSelector、nodeAffinity、podAffinity 或 AntiAffinity 等标签选择器，但没有节点打对应的标签或打的标值不匹配。
    可以使用 `kubectl describe pods` 查看 Pod 的定义，使用 `kubectl get node --show-labels` 查看各 Node 的 lables 列表，使用 `kubectl label node lable_name=lable_value` 给 Node 打标签；
1. 节点资源不足：
    集群内所有的 Node 都不满足该 Pod request 的 CPU、Memory、GPU 或者临时存储空间等资源，或者 Node 上的 Pod 数量达到了上限(kubelet 的 --max-pods 参数指定)。
    可以通过 `kubectl describe node` 来查看各节点资源分配情况。然后通过删除不用的 Pod 来释放资源，或增加新的 Node 来增加资源。
    **案例**：
    1. 创建的 Pod 一直处于 Pending 状态，kubectl describe pods 显示 `No nodes are available that match all of the predicates: Insufficient pods (3).`
    原因：
    1. kubelet 默认最多能运行 110 个 Pods：
        ``` txt
        $ kubectl describe node m7-devops-128071|grep -A 7 'Capacity'
        Capacity:
        cpu:     40
        memory:  264040352Ki
        pods:    110
        Allocatable:
        cpu:     40
        memory:  263937952Ki
        pods:    110
        ```
    当所有 kubelet 运行的 Pods 数达到 110 后，新创建的 Pod 就会因为资源不足而无法调度，提示 Insufficient pods；
    解决方案：
    1. kubelet 的 --max-pods 选项可以指定运行的最大 Pod 数目，通过调大该参数，如 110 → 240， 可以解决该问题。
    1. 注意：因为 flanneld 配置的本节点 Pod 网段是 /24，所以一个 Node 最多运行 254 个 Pod（flannel、docker0 占用 2 个），--max-pods 不能超过该值；
1. 创建 Pod 基础容器 (sanbox) 失败：
    现象：
    1. 创建的 Pod 一直处于 Pending 状态；
        ``` bash
        [root@scriptk8c ~]# k get pods
        NAME                 READY     STATUS    RESTARTS   AGE
        dnsuutils-ds-7q6dr   0/1       Pending   0          3s
        ```
    1. kubectl describe pod 显示创建 sandbox pod 失败：
        ``` bash
        [root@scriptk8c ~]# k describe pods dnsuutils-ds-7q6dr|tail
        Tolerations:     node.alpha.kubernetes.io/notReady:NoExecute
                        node.alpha.kubernetes.io/unreachable:NoExecute
                        node.kubernetes.io/disk-pressure:NoSchedule
                        node.kubernetes.io/memory-pressure:NoSchedule
        Events:
        Type     Reason                  Age               From                     Message
        ----     ------                  ----              ----                     -------
        Normal   SuccessfulMountVolume   2m                kubelet, 172.27.129.237  MountVolume.SetUp succeeded for volume "default-token-jhdrm"
        Warning  FailedCreatePodSandBox  25s (x8 over 2m)  kubelet, 172.27.129.237  Failed create pod sandbox.
        Warning  FailedSync              25s (x8 over 2m)  kubelet, 172.27.129.237  Error syncing pod
        ```
    原因：
    1. kubelet 基础容器配置参数：--pod-infra-container-image=docker02:35000/rhel7/pod-infrastructure:latest
    1. 节点本地不存在这个 image，所以 kubelet 去 registry docker02:35000 拉取该镜像；
    1. registry 需要登录认证，而 kubelet 没有提供认证信息，所以获取 pod-infra-container-image 失败，进而导致创建 sandbox 失败；
    解决方案：
    1. 从其他节点导出 pod-infra-container-image ，再导入到该节点，这样 kubelet 就不会去 registry 拉取镜像了；
        ``` bash
        $ docker save registry.access.redhat.com/rhel7/pod-infrastructure -o rhel.tar
        $ docker load -i rhel.tar
        ```
    1. 或者让 kublet 使用节点的 docker 认证信息去 registry 拉取镜像：
        1. 在节点 root 账户下，执行 docker login docker02:35000 命令，输入账号、密码登录；
        1. 修改 kubelet 配置文件，添加环境变量: HOME=root，重启 kubelet:
            ``` bash
            [root@scriptk8c ~]# grep HOME /mnt/disk01/opt/k8s/etc/kubernetes/kubelet
            HOME=/root
            [root@scriptk8c ~]# systemctl restart kubelet
            [root@scriptk8c ~]#
            ```
    注意：方案 2 适用于 k8s 1.8 版本，1.11 支持更多的认证方式；
    参考：
    1. https://v1-8.docs.kubernetes.io/docs/concepts/containers/images/
    1. https://kubernetes.io/docs/concepts/containers/images/#configuring-nodes-to-authenticate-to-a-private-repository
1. Pod 使用 HostNetwork，对应的端口(HostPort)在节点上已被占用：
    **案例**：
    1. 集群中已经部署一个 node_exporter 的 Daemonset，它使用了 hostNetwork 和 hostPID ，端口为 9100。则再部署一个使用相同端口的 daemonset 时，一直不创建 pod（describe 时所有 field 均为 0）：
        ``` bash
        [root@m7-devops-128123 log]# kubectl get ds |grep -E "NAME|node-exporter"
        NAME                               DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
        my-prom-prometheus-node-exporter   0         0         0         0            0                     17m
        ```
    1. 查看所有 kube-xxx 组件的日志，均没有相关的异常日志。
    原因：
    1. Daemonset 由 DaemonSet Controller （非 scheduler ）负责调度。如果 DS Controller 发现没有 Node 满足调度条件，如 Node 资源不足、** HostPort 端口冲突**等，就不会为 Node 创建 POD；
    1. 本问题原因在于 Node_Exporter 使用了 hostNewtork，部署监听相同端口的 Daemonset 时出现 HostPort 端口冲突，故 K8S 不再创建 POD；
    解决方案：
    1. 换个 HostPort (Service 端口也需要更新) 或通常使用 Service 对外开放服务端口；
## Pod 一直处于 Waiting 或 ContainerCreating 状态
可能的原因和解决方案:
1. 创建 Pod 的基础容器 (sandbox) 失败，例如节点本地没有 pod-infrastructure 镜像，但是从 kubelet 拉取失败（如认证问题）：
    使用命令 `docker images |grep pod-infra` 确认节点 kubelet 参数 `--pod-infra-container-image` 对应的镜像，如 `registry.access.redhat.com/rhel7/pod-infrastructure:latest` 是否存在。如果不存在则手动拉取到节点（可能需要先登录 registry）。
1. Pod yaml 定义中请求的 CPU、Memory 太小或者**单位不对**，不足以成功运行 Sandbox。
    常见的错误是：
    1. Pod yaml 定义 request 或 limit Memory 值是**不带单位**（如 Gi, Mi, Ki 等）的数值，如 4，这时只分配和限制使用 4 bytes。
    2. 内存单位 M 写成了小写 m，如 1024m，表示 1.024 Byte；
    `kubectl descirbe pod` 显示：
        Pod sandbox changed, it will be killed and re-created。
    kubelet 日志报错: 
        to start sandbox container for pod ... Error response from daemon: OCI runtime create failed: container_linux.go:348: starting container process caused "process_linux.go:301: running exec setns process for init caused \"signal: killed\"": unknown
1. 拉取镜像失败：
    + 配置了错误的镜像；
    + Kubelet 无法访问镜像仓库（国内环境访问 gcr.io 需要特殊处理）；
    + 拉取私有镜像的 imagePullSecret 没有配置或配置有误；
    + 镜像太大，拉取超时（可以适当调整 kubelet 的 --image-pull-progress-deadline 和 --runtime-request-timeout 选项）；
## Pod 一直处于 ImagePullBackOff 状态