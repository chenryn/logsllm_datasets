A. Testbed Speciﬁcation and Code Size
We evaluate MALT on two physical machines. The target
server used an ASUS M2V-MX SE motherboard with an
AMD K8 northbridge and a VIA VT8237r southbridge. It has
a 2.2 GHz AMD LE-1250 CPU and 2GB Kingston DDR2
RAM. The target machine uses Windows XP SP3, CentOS 5.5
with kernel 2.6.24, and Xen 3.1.2 with CentOS 5.5 as domain
0. To simplify the installation, they are installed on three
separate hard disks, and the SeaBIOS manages the booting.
The debugging client
is a Dell Inspiron 15R laptop with
Ubuntu 12.04 LTS. It uses a 2.4GHz Intel Core i5-2430M
CPU and 6 GB DDR3 RAM. We use a USB-to-serial cable
to connect two machines.
We use cloc [58] to compute the number of lines of source
code. Coreboot and its SeaBIOS payload contain 248,421
lines. MALT adds about 1,500 lines of C code in the SMI
hander. After compiling the Coreboot code, the size of the
image is 1MB, and the SMI hander contains 3,098 bytes. The
debugger client contains 494 lines of C code.
B. Debugging with Kernels and Hypervisors
To demonstrate that MALT is capable of debugging kernels
and hypervsiors, we intentionally crash the OS kernels and do-
main 0 of a Xen hypervisor and then use MALT to debug them.
For the Linux kernel and domain 0 of the Xen hypervisor, we
simply run the command echo c > /proc/sysrq-trigger, which
performs a system crash by a NULL pointer dereference.
To force a Blue Screen of Death (BSOD) in Windows, we
create a new value named CrashOnCtrlScroll in the registry
key HKEY LOCAL MACHINE\System\CurrentControlSet
\Services\i8042prt\Parameters
to a
REG DWORD value of 0x01. Then,
the BSOD can be
initiated by holding the Ctrl key and pressing the Scroll
Lock key twice. After a system crashes, MALT can start a
debugging session by sending an SMI triggering message.
In our experiments, MALT is able to examine all the CPU
registers and the physical memory of the crashed systems.
and set
it
equal
C. Breakdown of Operations in MalT
In order to understand the performance of our debugging
system, we measure the time elapsed during particular oper-
ations in the SMI handler. We use the Time Stamp Counter
(TSC) to measure the number of CPU cycles elapsed during
each operation; we multiplied the clock frequency by the delta
in TSCs.
After a performance counter triggers an SMI, the system
hardware automatically saves the current architectural state
into SMRAM and begins executing the SMI handler. The ﬁrst
operation in the SMI handler is to identify the last running
process in the CPU. If the last running process is not the target
malware, we only need to conﬁgure the performance counter
register for the next SMI and exit from SMM. Otherwise, we
perform several checks. First, we check for newly received
messages and whether a breakpoint has been reached. If there
are no new commands and no breakpoints to evaluate, we
6565
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:04:38 UTC from IEEE Xplore.  Restrictions apply. 
BREAKDOWN OF SMI HANDLER (TIME: μs)
TABLE VII
Operations
SMM switching
Command and BP checking
Next SMI conﬁguration
SMM resume
Total
Mean
3.29
2.19
1.66
4.58
11.72
STD
0.08
0.09
0.06
0.10
95% CI
[3.27,3.32]
[2.15,2.22]
[1.64,1.69]
[4.55,4.61]
reconﬁgure the performance counter registers for the next
SMI. Table VII shows a breakdown of the operations in
the SMI handler if the last running process is the target
malware in the instruction-by-instruction stepping mode. This
experiment shows the mean, standard deviation, and 95%
conﬁdence interval of 25 runs. The SMM switching time takes
about 3.29 microseconds. Command checking and breakpoint
checking take about 2.19 microseconds in total. Conﬁguring
performance monitoring registers and SMI status registers for
subsequent SMI generation takes about 1.66 microseconds.
Lastly, SMM resume takes 4.58 microseconds. Thus, MALT
takes about 12 microseconds to execute an instruction without
debugging command communication.
D. Step-by-Step Debugging Overhead
In order to demonstrate the efﬁciency of our system,
we measure the performance overhead of the four stepping
methods on both Windows and Linux platforms. We use a
popular benchmark program, SuperPI [59] version 1.8, on
Windows and version 2.0 on Linux. SuperPI is a single-
threaded benchmark that calculates the value of π to a
speciﬁc number of digits and outputs the calculation time.
This tightly written, arithmetic-intensive benchmark is suitable
for evaluating CPU performance. Additionally, we use four
popular Linux commands, ls, ps, pwd, and tar, to measure
the overhead. ls is executed with the root directory; pwd
is executed under the home directory; and tar is used to
compress a hello-world program with 7 lines of C code. We
install Cygwin on Windows to execute these commands. First,
we run the programs and record their runtimes. Next we enable
each of the four stepping methods separately and record the
runtimes. SuperPI calculates 16K digits of π, and we use shell
scripts to calculate the runtimes of the Linux commands.
Table VIII shows the performance slowdown introduced by
the step-by-step debugging. The ﬁrst column speciﬁes four
different stepping methods; the following ﬁve columns show
the slowdown on Windows, which is calculated by dividing
the current running time by the base running time; and the
last ﬁve columns show the slowdown on Linux. It is evident
that far control transfer (e.g., call instruction) stepping only
introduces a 2x slowdown on Windows and Linux, which
facilitates coarse-grained tracing for malware debugging. As
expected, ﬁne-grained stepping methods introduce more over-
head. The instruction-by-instruction debugging causes about
973x slowdown on Windows for running SuperPI, which
demonstrates the worst-case performance degradation in our
four debugging methods. This high runtime overhead is due
to the 12-microsecond cost of every instruction (as shown
in Table VII) in the instruction-stepping mode. One way
to improve the performance is to reduce the time used for
SMM switching and resume operations by cooperating with
hardware vendors. Note that MALT is three times as fast as
Ether [4], [7] in the single-stepping mode.
Despite a three order-of-magnitude slowdown on Windows,
the debugging target machine is still usable and responsive
to user interaction. In particular, the instruction-by-instruction
debugging is intended for use by a human operator from
the client machine, and we argue that the user would not
notice this overhead while entering the debugging commands
(e.g., Read Register) on the client machine. We believe
that achieving high transparency at the cost of performance
degradation is necessary for certain types of malware analysis.
Note that the overhead in Windows is larger than that in
Linux. This is because (1) the semantic gap problem is solved
differently in each platform, and (2) the implementations of
the benchmark programs are different.
IX. DISCUSSION AND LIMITATIONS
Restoring a system to a clean state after each debugging
session is critical to the safety of malware analysis on bare
metal. In general, there are two approaches to restore a system:
reboot and bootless. The rebooting approach only needs to
reimage the non-volatile devices (e.g., hard disk or BIOS),
but it is slow. The bootless approach must manually reinitialize
the system state, including memory and disks, but takes less
time. BareBox [17] used a rebootless approach to restore the
memory and disk of the analysis machine; BareCloud [30]
used LVM-based copy-on-write to restore a remote storage
disk. For the rebootless approach, besides memory and disk
restoration, hardware devices also need to be restored. Modern
I/O devices now have their own processors and memory (e.g.,
GPU and NIC); quickly and efﬁciently reinitializing these
hardware devices is a challenging problem. MALT simply
reboots the analysis machine and reimages the disk and BIOS
by copying and reﬂashing. Additionally, the focus of MALT
is to address the debugging transparency problem, while fast
restoration (i.e., bootless approach) of a system increases the
efﬁciency of malware analysis. We leave this for future work.
MALT uses SMM as the foundation to implement various
debugging functions. Before 2006, computers did not lock
their SMRAM in the BIOS [42], and researchers used this ﬂaw
to implement SMM-based rootkits [41], [42], [43]. Modern
computers lock the SMRAM in the BIOS so that SMRAM
is inaccessible from any other CPU modes after booting. Wo-
jtczuk and Rutkowska demonstrated bypassing the SMRAM
lock through memory reclaiming [35] or cache poisoning [53].
The memory reclaiming attack can be addressed by locking the
remapping registers and Top of Low Usable DRAM (TOLUD)
register. The cache poisoning attack forces the CPU to execute
instructions from the cache instead of SMRAM by manipu-
lating the Memory Type Range Register (MTRR). Duﬂot also
independently discovered this architectural vulnerability [60],
but it has been ﬁxed by Intel adding SMRR [24]. Furthermore,
6666
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:04:38 UTC from IEEE Xplore.  Restrictions apply. 
STEPPING OVERHEAD ON WINDOWS AND LINUX (UNIT: TIMES OF SLOWDOWN)
TABLE VIII
Stepping Methods
Retired far control transfers
Retired near returns
Retired taken branches
Retired instructions
π
2
30
565
973
ls
2
21
476
880
ps
2
22
527
897
Windows
pwd
3
28
384
859
SUMMARY OF SMM ATTACKS AND SOLUTIONS
TABLE IX
SMM Attacks
Unlocked SMRAM [41], [42], [43]
SMRAM reclaiming [35]
Cache poisoning [53], [60]
Graphics aperture [61]
TSEG location [61]
Call/fetch outside of SMRAM [61], [63]
Solutions
Set D LCK bit
Lock remapping and TOLUD registers
SMRR
Lock TOLUD
Lock TSEG base
No call/fetch outside of SMRAM
Duﬂot et al. [61] listed some design issues of SMM, but
they can be ﬁxed by correct conﬁgurations in BIOS and
careful implementation of the SMI handler. Table IX shows
a summary of attacks against SMM and their corresponding
solutions. Wojtczuk and Kallenberg [62] recently presented
an SMM attack by manipulating UEFI boot script that allows
attackers to bypass the SMM lock and modify the SMI handler
with ring 0 privilege. The UEFI boot script is a data structure
interpreted by UEFI ﬁrmware during S3 resume. When the
boot script executes, system registers like BIOS NTL (SPI
ﬂash write protection) or TSEG (SMM protection from DMA)
are not set so that attackers can force an S3 sleep to take
control of SMM. Fortunately, as stated in the paper [62], the
BIOS update around the end of 2014 ﬁxed this vulnerability.
In MALT, we assume that SMM is trusted.
Butterworth et al. [64] demonstrated a buffer overﬂow
vulnerability in the BIOS updating process in SMM, but this
is not an architectural vulnerability and is speciﬁc to that
particular BIOS version. (Our SMM code does not contain
that vulnerable code). Since MALT adds 1,500 lines of C
code in the SMI handler, it is possible that our code has
bugs that could be exploited. Fortunately, SMM provides a
strong isolation from other CPU modes (i.e., it has its own
sealed memory). The only inputs from a user are through serial
messages, making it difﬁcult for malicious code to be injected
into our system.
We implement MALT on a single-core processor for com-
patibility with Coreboot, but SMM also works on multi-core
systems [24]. Each core has its own set of MSR registers,
which deﬁne the SMRAM region. When an SMI is generated,
all the cores will enter into SMM with their own SMI handler.
One simple way is to let one core execute our debugging code
and spin the other cores until the ﬁrst has ﬁnished. SMM-
based systems such as HyperSentry [37] and SICE [65] are
implemented on multi-core processors. In a multi-code system,
MALT can debug a process by pinning it to a speciﬁc core
while allowing the other cores to execute the rest of the system
normally. This will change thread scheduling for the debugged
tar
2
29
245
704
π
2
26
192
349
ls
3
41
595
699
Linux
ps
2
28
483
515
pwd
2
10
134
201
tar
2
15
159
232
Remote Debugger (“client”)
Debugging Target (“server”)
IDAPro
Tool
GDB
Client