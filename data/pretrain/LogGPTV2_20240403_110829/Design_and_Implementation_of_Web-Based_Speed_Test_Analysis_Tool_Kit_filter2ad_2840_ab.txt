and easy to use. Speciﬁcally, we have the three following design objectives.
A Uniﬁed and Conﬁgurable Interface. Our ﬁrst design objective is to provide
users with a uniﬁed and conﬁgurable interface for automatically executing all
web-based speed tests we support. This objective is two-fold: (i) a uniﬁed inter-
face to improve usability and repeatability of these platforms which allows fair
comparison among them, and (ii) a conﬁgurable interface to support measure-
ment server selection which eﬀectively exploits the resources of speed tests.
Lightweight and Extensive Data Collection. Our second design objective is to
enrich metrics we collect to allow correct interpretation of measurements. Due to
the run-time dynamics of speed tests, collecting information to monitor the local
environment, browser, and network traﬃc is essential for in-depth analysis of
network quality and possible sources of measurements inaccuracies. Lightweight
data collection also necessary mitigates potential interference with actual speed
test measurements.
In-depth Cross-layer Analysis. Our third design objective, closely related to the
second, is to perform cross-layer analysis of collected data. We aim at providing
a comprehensive and in-depth view across diﬀerent layers in the system.
4.2 Components of WebTestKit
With the three design objectives in mind, we propose WebTestKit, a lightweight
framework for automating speed tests with diﬀerent conﬁgurations and providing
in-depth analysis of test results. WebTestKit consists of three modules:
1. Measurement server exploration module discovers available measurement
servers in speed test infrastructures. Of the ﬁve platforms listed in Table 1, Xﬁn-
ity and Ookla speed tests support manual selection of measurement servers.
However, their full lists of servers are not publicly available like M-Lab. To
allow adjustably conﬁguring the targeted servers for diﬀerent needs, WebTestKit
ﬁrst identiﬁes RESTful APIs that the web interface uses to query measurement
servers by observing the associated HTTP transactions, and then uses these
RESTful APIs to retrieve measurement server information including hostnames,
IP addresses, and physical locations. As of October 2021, WebTestKit found 78
and 12 149 test servers in Xﬁnity and Ookla platforms, respectively.
2. Test execution module integrates a suite of tools to automate the execution
of speed tests and capture data from diﬀerent layers. The main challenge is
to minimize the interference of data collection with throughput measurements.
Our key insight is to capture only headers of measurement packets to minimize
overhead. We show (Sect. 5.1) that this module is lightweight and has minimal
impact on the throughput measurements.
88
R. Yang et al.
For all speed test platforms we support, WebTestKit provides a command-
line interface for programmably visiting and executing tests. To guarantee that
our results are identical to a user manually running the test with a Chromium
browser, we used an actual browser to render and interact with the speed test
platforms. Given a speed test platform and the measurement conﬁgurations (e.g.
target server), the interface will interact with the web page by clicking the ‘Start’
button, selecting measurement options, detecting the completion of the test, and
capturing the results displayed on the page.
In the browser, WebTestKit uses the performance trace function [37] to reveal
the resource timing API information [39] and event information received by
JavaScript XMLHttpRequest API [41]. We could have obtained such informa-
tion by capturing two types of internal messages (devtools .timeline and
blink.user timing). However, a recent update in Chrome (and Chromium) [14]
removed visibility of pre-ﬂight HTTP OPTIONS requests from performance trace.
Having only partial visibility of HTTP transactions could easily cause inaccura-
cies in matching the corresponding packets. Therefore, we employed Chrome’s
NetLog [36], which provides network layer information including TCP source
ports and the data transmission progress of all HTTP ﬂows at the socket level.
At the packet level, WebTestKit captures the ﬁrst 100 bytes of packets which
include the headers. It also collects CPU and memory usage of the end host every
second during the execution of tests using SoMeta [32].
3. Analysis module performs analysis of data collected in Test execution mod-
ule to generate an in-depth view of all HTTP transactions from application
level to packet level. WebTestKit conducts its analysis in a three-step fashion:
(i) identiﬁes URLs for measurement ﬂows, (ii) extracts information for HTTP
transactions, and (iii) locates HTTP messages in encrypted packet traces.
The module ﬁrst identiﬁes URLs of RESTful APIs or web objects correspond-
ing to download/upload tests by response/request sizes and crafts platform-
speciﬁc regular expressions to match URLs. Then, it uses the NetLog trace to
ﬁlter HTTP transactions associated with measurement ﬂows.
The second step is to extract events from NetLog and performance traces to
obtain timing information observed by the browser and JavaScript, respectively.
Since both traces have limited documentation, to correctly interpret the traces,
we used visualization tools (e.g., NetLog viewer [5] and about:tracing tool [37]).
For each HTTP transaction in the measurement ﬂows, WebTestKit extracts
the send/arrival times of HTTP requests/responses, and the sending/receiving
progress events of downloading/uploading large objects.
Finally, we developed a packet matching algorithm to locate HTTP messages
in encrypted traﬃc. This step is the key for our analysis because lacking visibil-
ity of HTTP messages at the packet level will make an in-depth analysis almost
impossible. For example, without such visibility, we cannot quantify the over-
head posed by diﬀerent layers from timing information of measurement traﬃc.
Although we could export SSL keys from the browser to decrypt the traﬃc, this
approach requires us to capture full-size packets, posing signiﬁcant overhead.
To make WebTestKit lightweight but capable of performing an in-depth anal-
ysis, we only capture packet headers and then locate packets containing HTTP
Design and Implementation of Web-Based Speed Test Analysis Tool Kit
89
messages by referring to the information extracted from NetLog (e.g. sequence
number, payload size). This task is challenging due to the use of HTTP persis-
tent connections and TLS encryption. An HTTP persistent connection reuses
a single TCP connection for multiple HTTP transactions. We used HTTP
request/response sizes to separate consecutive HTTP transactions in the same
TCP ﬂow. However, we could not directly apply the HTTP request/response
sizes obtained from NetLog to infer the total packet payload sizes, because TLS
encryption induces an overhead of 20–40 bytes to TLS records to include the
TLS Record header and padding bytes. The size of overhead depends on the
cipher suite negotiated in the TLS Handshake process.
Our packet-matching algorithm is designed to tackle these two challenges.
The algorithm ﬁrst calculates the TLS overhead by identifying the ﬁrst HTTP
request packet in the ﬁrst measurement ﬂow. Because all test platforms initially
perform latency or download tests using HTTP GET, the HTTP request is small
and should ﬁt in one packet. Therefore, the diﬀerence between the size of the
HTTP header and the TCP payload size is the TLS overhead, Stls. We assume
the overhead is constant across diﬀerent ﬂows in the same experiment since the
same cipher suite is used.
After the algorithm identiﬁes the TLS overhead, it locates the end of the
POST requests/GET responses, by estimating the size of the messages after
TLS encryption using Sbody + Stls × Nprog, where Sbody is the message body
size of the HTTP request/response, and Nprog is the number of progress update
events of HTTP transactions in NetLog. For HTTP POST requests, Nprog is
equal to the number of TLS records, which allows us to accurately calculate the
size of requests after TLS encryption. However, as the network socket aggregates
incoming response packets spanning multiple TLS records before passing data
to the browser, the number of update events is fewer than of TLS records,
implying an underestimation of the post-encrypted data size. To this end, the
algorithm determines to have found the end of GET responses when it observes
any outgoing data packet after receiving the expected amount of data (indicating
the next HTTP request) or the end of TCP connection.
4.3 Implementation
Our implementation of WebTestKit1 consists of ≈2k lines of JavaScript code
and ≈10k lines of Golang code. For test execution and data collection, it uses
puppeteer, a node.js library, to control a headless Chromium browser to pro-
grammably execute speed tests. It uses tcpdump to capture measurement pack-
ets, and SoMeta [32] to collect CPU and memory usage information on the end
host.
5 Testbed Evaluation
We set up a semi-controlled testbed to examine the resource consumption of
WebTestKit in four conﬁgurations and its impact on throughput measurements
1 Available at https://github.com/CAIDA/webtestkit.
90
R. Yang et al.
(Sect. 5.1), and the accuracy of the analysis module in matching packets to
HTTP transactions in measurement ﬂows (Sect. 5.2).
We performed two sets of experiments using (i) a server (Intel E3-Xeon 1270,
32 GB RAM, 1 Gbps Ethernet, Ubuntu 20.04), and (ii) a virtual machine (VM)
allocated with 2 vCPU and 8 GB RAM set up on this server to simulate a client
with low computational power. We performed speed tests with the ﬁve platforms
listed in Table 1 using WebTestKit. For consistency, Ookla tests used a server
hosted by 13D.net in Hong Kong, and Xﬁnity speed tests used servers in Seattle,
WA. We conﬁgured four scenarios in WebTestKit, denoted with (snaplen,D/N),
where snaplen is the snapshot length used in tcpdump (disabled when snaplen =
0), and D/N represents disabling/enabling NetLog. We ran each speed test in
each scenario 50 times in the VM.
5.1 Resource Overhead of WebTestKit
We studied resource usage and its impact on measurements results under diﬀer-
ent conﬁgurations of WebTestKit. Figure 1a and 1b show box-and-whisker plots
of CPU idle rate and reported download throughput of Ookla speed tests in the
VM, respectively. As expected, the control case (0,D) consumed the least CPU
resource. By default, WebTestKit adopted the leftmost scenario (100,N) in the
ﬁgures, which consumed 4.2% more CPU time in median than that of (0,D),
but 1.5% less than (65535,N), which captures full-size packets. Comparing sce-
narios (100,N) and (100,D), we found that enabling NetLog slightly increased
the median CPU usage by 2.4%. The download throughput followed a similar
pattern to the CPU idle rate. Capturing full-size packets had the lowest median
throughput (507 Mbps), 41 Mbps lower than the (100, N) scenario. Our results
showed that compared to full-size packet capture, the default conﬁguration of
WebTestKit had a signiﬁcantly lower impact on the throughput measurement.
(a) CPU idle rate.
(b) Download Throughput.
Fig. 1. Box-and-whisker plots for Ookla test results under four conﬁgurations.
5.2 Accuracy of Analysis Module
We evaluated the accuracy of the analysis module in identifying HTTP transac-
tions in encrypted packets. We used data collected in test scenario (65535,N) in
our testbed experiments from the server, so that we could decrypt the traﬃc to
Design and Implementation of Web-Based Speed Test Analysis Tool Kit
91
reveal HTTP messages as ground truth. We compared the packet sequence num-
bers of HTTP request and response headers inferred by our analysis module and
the actual ones observed in the decrypted traﬃc. We deﬁne Request/Response
matching accuracy (= # of correctly located HTTP requests/responses
) to quantify the
accuracy of our packet matching algorithm.
Total # of HTTP requests/responses
Fig. 2. Accuracy of the packet matching
algorithm for HTTP responses
Fig. 3. The number of diﬀerent HTTP
requests for ﬁve speed tests
We examined our results in Ookla and Xﬁnity speed tests, because we could
select the same servers in repeated trials. We found that WebTestKit achieved
100% matching accuracy in locating all HTTP requests. Figure 2 shows CDFs of
the response matching accuracy over 50 runs. We obtained very high accuracy
here as well, except the HTTP GET responses in Ookla tests, where the median
accuracy was 54.5%. The reason is that the Ookla server sent a TLS new session
ticket packet to the client right after receiving the probing GET requests. We
cannot easily distinguish this type of control packet from HTTP headers based
on the information in the TLS record protocol header. We found that these
mismatches had minimal impact on analyzing measurements. First, all these
incorrectly inferred HTTP responses were for latency measurements before the
actual download throughput tests, which had only a header indicating 200 OK
status. Second, the send times of the TLS packets were close to those of the
send HTTP response headers. Speciﬁcally, these TLS packets were only 3 or 4
packets ahead of the actual HTTP responses.
6 Use Cases
We present three use cases of WebTestKit: (i) characterizing the types and sizes
of HTTP transactions of 5 speed test platforms (Sect. 6.1), (ii) diagnosing high
variances in latency measurements (Sect. 6.2), (iii) and evaluating the inaccuracy
in latency measurement using HTTP request-response time (Sect. 6.3).
6.1 Characterizing Speed Tests with HTTP Transactions
Without access to the source code, speed tests’ methodologies remain opaque. We
used WebTestKit to characterize their implementations. We ran ﬁve speed tests
– Xﬁnity (CC), SpeedOf.Me (ME), Ookla (OK), Fast.com (FT), and Cloudﬂare
(CF) – 20 times with default settings in a workstation connected to a 1 Gbps
92
R. Yang et al.
campus network. Figure 3 shows the average number of each type of HTTP