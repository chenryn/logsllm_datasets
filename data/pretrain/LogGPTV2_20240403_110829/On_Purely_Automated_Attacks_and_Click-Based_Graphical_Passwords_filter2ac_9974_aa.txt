title:On Purely Automated Attacks and Click-Based Graphical Passwords
author:Amirali Salehi-Abari and
Julie Thorpe and
Paul C. van Oorschot
2008 Annual Computer Security Applications Conference
2008 Annual Computer Security Applications Conference
On Purely Automated Attacks and Click-Based Graphical Passwords ∗
Amirali Salehi-Abari, Julie Thorpe, and P.C. van Oorschot
School of Computer Science, Carleton University, Canada
{asabari, jthorpe, paulv}@scs.carleton.ca
Abstract
We present and evaluate various methods for purely au-
tomated attacks against click-based graphical passwords.
Our purely automated methods combine click-order heuris-
tics with focus-of-attention scan-paths generated from a
computational model of visual attention. Our method re-
sults in a signiﬁcantly better automated attack than pre-
vious work, guessing 8-15% of passwords for two repre-
sentative images using dictionaries of less than 224.6 en-
tries, and about 16% of passwords on each of these im-
ages using dictionaries of less than 231.4 entries (where the
full password space is 243). Relaxing our click-order pat-
tern substantially increased the efﬁcacy of our attack al-
beit with larger dictionaries of 234.7 entries, allowing at-
tacks that guessed 48-54% of passwords (compared to pre-
vious results of 0.9% and 9.1% on the same two images with
235 guesses). These latter automated attacks are indepen-
dent of focus-of-attention models, and are based on image-
independent guessing patterns. Our results show that au-
tomated attacks, which are easier to arrange than human-
seeded attacks and are more scalable to systems that use
multiple images, pose a signiﬁcant threat.
1 Introduction
Graphical passwords are an alternative to traditional text
passwords, whereby a user must remember an image (or
parts of an image) in place of a word. They are moti-
vated in part by the well-known fact that people are bet-
ter at remembering images than words [19]. There are
many different types of graphical passwords; among the
more popular approaches is click-based graphical pass-
words [31, 16, 2, 13, 6], which require users to click on
a sequence of points on one or more background images.
The most effective attack strategy to date on these schemes
appears to be human-seeded attacks [27], although such at-
∗
Version: Sept.15, 2008. The third author acknowledges NSERC fund-
ing under a Discovery Grant and as Canada Research Chair in Network
and Software Security.
tacks are more difﬁcult to arrange than attacks based on
purely automated means and do not scale well for systems
that use multiple images. In this paper, we pursue purely
automated approaches for guessing attacks.
We pursue heuristic-based strategies for purely auto-
mated dictionary generation (e.g., based on click-order pat-
terns), and strategies to prioritize these dictionaries using
image processing methods to identify points that users are
more likely to choose. We hypothesize that users will
choose click-points according to a click-order pattern, to
help remember the password as fewer “chunks” [8]. We
further examine use of the DIAG click-order pattern [27],
which captures arcs that are consistent in both horizontal
and vertical directions, and a subset of this pattern that we
call LINE that captures only horizontal and vertical lines.
We relax the rules on these deﬁnitions, showing that a
“lazy” approach to these click-order patterns is substantially
more effective.
We further hypothesize that users will choose click-
points based on their preference for certain points in the
image, and that their preference for certain points will be in-
ﬂuenced by how much they are naturally attracted to those
points. Attention is the cognitive process of selectively fo-
cusing on one aspect of the environment while ignoring oth-
ers, a mechanism that helps us prioritize sensory informa-
tion. There are two different categories of visual attention
models: bottom-up and top-down. Bottom-up visual atten-
tion captures how attention is drawn to the parts of a scene
or image that are salient or conspicuous. It is what natu-
rally draws us to look at the unexpected or different parts
of a scene, prioritizing them from the other consistent parts.
For example, if an image contains a large number of objects
that are blue, and only one is yellow, human attention will
instinctively focus on the yellow object. Top-down visual
attention is task-dependent, based on cognitive, volitional
control. With a priori knowledge about what object(s) to
look for, our attention is brought to the parts of the scene
containing those object(s). For example, if a user decides
that people with dark hair are of interest for some reason,
the user’s attention would shift between objects with fea-
tures that might indicate a dark-haired person.
1063-9527/08 $25.00 © 2008 IEEE
1063-9527/08 $25.00 © 2008 IEEE
DOI 10.1109/ACSAC.2008.18
DOI 10.1109/ACSAC.2008.18
101
111
Our contributions include the best purely automated at-
tacks to date against click-based graphical passwords, an
evaluation of how the model of Itti et al.
[15] relates to
user-selected click-based graphical passwords, and a new
spatial clustering algorithm. Two different hypotheses were
tested regarding how users might choose their click-points
relative to Itti’s model. Our methods were tested using the
same ﬁeld study database used by Thorpe et al. [27], allow-
ing us to compare performance. We found that a “lazy”
approach to click-order patterns produced a substantially
better automated attack than previous methods with com-
parable dictionary sizes and images [27, 10], guessing 48-
54% of passwords (compared to 0.9-9.1% previously) on
two different images used in a long-term ﬁeld study with a
dictionary of about 235 entries. Furthermore, we were able
to optimize this dictionary using Itti’s model, producing dic-
tionaries whose efﬁcacy is comparable to human-seeded at-
tacks [27]: one dictionary of 230.3 entries guessed 15.8% of
passwords on one image, and on a second image a dictio-
nary of 231.4 entries guessed 16.5% of passwords.
The remainder of this paper proceeds as follows. Section
2 discusses background and related work, including compu-
tational models of visual attention. We describe our purely
automated attack generation methods in Section 3, results
in Section 4, future work in Section 5, and conclusions in
Section 6.
2 Background and Related Work
We discuss computational models of visual attention in
Section 2.1, and terminology in Section 2.2. Many different
types of graphical passwords have been proposed to date
(see surveys [26, 20]). Here we give a brief overview fo-
cused on click-based graphical password schemes and other
work on modeling user choice in graphical passwords.
Click-based graphical password schemes require a user
to click on a set of points on one or more presented back-
ground images. In Blonder’s proposal [2] users must click
on a set of predeﬁned tap regions. In V-go, by PassLogix
[24], users must click on predeﬁned objects in the picture
in a speciﬁc sequence. In the Jansen et al. [16] variation
for PDAs, users click an ordered sequence of visible grid
squares imposed on a background image; the squares are
intended to help the user repeat their click-points in subse-
quent logins.
PassPoints [32, 31, 30] allows users to click a sequence
of ﬁve points anywhere on an image while allowing a degree
of error tolerance using robust discretization [1]. Various
studies have shown that PassPoints has acceptable usability
[30, 32, 31, 5]. visKey, a commercial system for the Pocket
PC, appears similar but allows the user to choose the num-
ber of click-points and set the error tolerance. In the Per-
suasive Cued Click-Points (PCCP) [4, 6] variation, a user
clicks on a single point on each of ﬁve images, guided par-
tially by a randomly placed viewport; each image displayed
(after the ﬁrst) is dependent on the previous click-point.
Two previous studies have examined the security of
PassPoints-style graphical passwords. Dirik et al. [10] ex-
amine the efﬁcacy of an automated tool for guessing Pass-
Points passwords. Their method, which does not draw on
a standard computational model of visual attention, uses
centroids of segments as guesses (but no corners). It was
tested against a database of single-session user choices for
two images. For the image with a reasonable level of detail,
their method guessed 8% of passwords with a dictionary of
232 entries compared to full space of 240 entries. As pre-
viously discussed, Thorpe et al. [27] examine both an auto-
mated method (based on stage 1 of Itti et al.’s [15] model
of visual attention; see Section 2.1), and a human-seeded
method (which uses click-point data from a set of users’
password choices).
User choice has been successfully modeled for other
graphical password schemes. Davis et al. [9] modeled
user choice for Faces and Story recognition-based graphical
passwords by training a dictionary using a large password
database. Van Oorschot et al. [28] model user choice in
“Draw-A-Secret” pure-recall graphical passwords [17] mo-
tivated by cognitive studies.
2.1 Models of Visual Attention
We conjecture that a signiﬁcant percentage of users will
choose points that draw their attention as components of
their click-based passwords, and thus that computational
models of visual attention may help pick out more probable
click-points. Computational models of bottom-up visual at-
tention are normally deﬁned by features of a digital image,
such as intensity, color, and orientation [15, 14].
Computational models of top-down visual attention can
be deﬁned by training [22]. The difﬁculty of these models
is that the top-down task must be pre-deﬁned (e.g., ﬁnd all
people in the image), and then a corpus of images that are
tagged with the areas containing the subject to ﬁnd (e.g.,
people) must be used for training. Navalpakkam et al. [21]
discuss an alternate method to create a top-down model,
based on Guided Search [33], which weighs visual feature
maps according to the top-down task. For example, with
a task of locating a red object, a red-sensitive feature map
would gain more weight, giving it a higher value in the re-
sulting saliency map. In both cases, assumptions regarding
what sort of objects people are looking for are required to
create such a model.
In this work, we focus on bottom-up visual attention, us-
ing Itti et al.’s [15] computational model of visual attention.
We use this particular model as it is quite well-known, and
there is empirical evidence that it captures people’s bottom-
102112
up visual attention [23]. The general idea behind this model
is that areas of an image will be salient (or visually “stand
out”) when they differ from their surroundings.
We now explain Itti’s model in further detail. Given
an input image, it outputs a focus-of-attention scan-path to
model the locations and the order in which a human might
automatically and unconsciously attend them.
It is com-
posed of two stages: (stage 1) construction of a saliency
map based on visual features, and (stage 2) the use of a
winner-take-all neural network with inhibition of return to
deﬁne a speciﬁc focus-of-attention scan-path, whose goal is
to replicate the order in which a user would scan the im-
age. Thorpe et al. [27] developed an automated attack for
click-based graphical passwords that focused only on a vari-
ation of stage 1, ordering an attack dictionary based on the
raw values of the resulting saliency map. The present paper
uses the entire model including stage 2.
In stage 1, the saliency map is created by decompos-
ing the original image into a set of 50 multi-level “feature
maps”, which extract spatial discontinuities based on color
opponency (either red-green or blue-yellow), intensity, or
orientation. Each level deﬁnes a different size of the center
and its surround, in order to account for conspicuous loca-
tions of various sizes. All feature maps are then combined
into a single saliency map.
In stage 2, a winner-take-all neural network detects the
point of highest salience (as indicated by the intensity value
of the saliency map), and draws the focus of attention to-
wards this location. Once an area has been attended to, in-
hibition of return will prevent an area from being the fo-
cus again for a period of time. Together, the winner-take-
all neural network with inhibition of return produces out-
put in the form of spatio-temporal attentional scan-paths,
which follow the order of decreasing saliency as deﬁned by
stage 1. Two different normalization types can be used with
the model: LocalMax and Iterative (cf. Figure 1). In Iter-
ative normalization, the neural network will ﬁnd the next
most salient area that has not been inhibited. In LocalMax
normalization, the neural network will have a bias towards
those areas that are closer to the previously attended loca-
tion. Each normalization type produces a different scan-
path; we study and compare the results of each as relates to
our work.
2.2 Terminology
We hypothesize that users are more likely to choose dis-
tinguishable points as click-points. We deﬁne a distinguish-
able point as a point on a digital image that can be easily dis-
tinguished and relocated by a user. General ways this could
be accomplished include: (1) by using referencable points
on the image (e.g., a corner), and (2) by using calculable
points that are based on other referencable parts of the im-
age (e.g., object centers). In related work, Thorpe et al. [27]
used corner detection to ﬁnd referencable points, and Dirik
et al. [10] used centroids to ﬁnd calculable points. Here, we
use both approaches to deﬁne a distinguishable points map
δ. We describe the details of each method below.
We use the following additional terminology. Suppose
that a user chooses a click-point c as part of her password.
The tolerable error or tolerance t is the error allowed (in
both vertical and horizontal directions) for a click-point en-
tered on a subsequent login to be accepted as c. This de-
ﬁnes a tolerance region (T-region) centered on c, which for
an implementation using t = 9 pixels, is a 19 × 19 pixel
square. A 19 × 19 T-region was used in the implementation
for collecting the database [27] used herein for evaluating
our results.
A window cluster is a square region of size n × n for
some positive integer n. A cluster is a set of one or more
points that lie within a window cluster. The center of a win-
dow cluster is representative of all the points within the win-
dow cluster. An alphabet is a set of distinct window centers.
Corner Detection. A corner can be deﬁned as the inter-
section of two edges, where an edge is deﬁned by the points
in a digital image where there are sharp changes in intensity
[11]. A corner can also be deﬁned as a point in whose lo-
cal neighborhood there are two dominant and different edge
directions [11].
We use the harris algorithm [12] as implemented by
Kovesi [18] for detecting corners. Harris corner detection
ﬁrst identiﬁes the edges and then those edges are blurred to
reduce the effect of any noise. Then an energy map is gen-
erated, based on the edges that contain local maxima and
minima. A local maximum indicates the presence of a cor-
ner. We run harris corner detection with the parameters:
σ = 1, θ = 1000 and r = 3, where σ is the standard devi-
ation of a smoothing Gaussian ﬁlter, θ is a threshold for the
maximum number of corners, and r is an inhibition radius,
measured in pixels around a detected corner.
Figure 2 shows the pool image where each detected cor-
ner is illustrated by a ‘+’. We also create a binary corners
map, which is a specialized type of binary map (i.e., a one-
to-one mapping from its pixels of value 0 or 1 to the pix-
els of the original image). In a binary corners map, when
a pixel is a corner in the original image, its corresponding
value is 1; otherwise it is 0.
Centroid Detection. To ﬁnd the centers of objects, we
ﬁrst partition the digital image into segments using image
segmentation, the goal of which is to change the representa-
tion of an image into something more meaningful and easier
to analyze [25]. We use the mean-shift segmentation algo-
rithm [7], which takes a feature (range) bandwidth, spatial
bandwidth, and a minimum region area (in pixels) as input.
We set these parameters to 7, 9, and 50 respectively, which
we found empirically to provide an acceptable segmentation
103113
(a) LocalMax normalization
(b) Iterative normalization
Figure 1. pool image with the ﬁrst 7 items in the scan-path.
Figure 2. Corner detection (left) and center detection (right) output for pool.
with the smallest resulting number of segments.
centers map and binary corners map.
(cid:2)
1
n(S)
After segmentation, we calculate the center of each seg-
ment (centroid) by calculating the arithmetic mean of each
coordinate of the segment’s points.
In other words, the
center (XS, YS) of segment S is calculated by XS =
i∈S yi, where xi and yi
are pixel coordinates, and n(S) denotes the total number
of pixels in segment S. The x coordinates of all points in
S are involved in calculating XS, not only those along the
maximum width.
i∈S xi and YS = 1
n(S)
(cid:2)
Figure 2(b) illustrates the resulting segments of the pool
image with different shading. The center of each segment
is denoted by a ‘+’. We also create a centers map, which is
a binary map of the same size of the corresponding image
where each pixel has the value 0 or 1. If a pixel is a center