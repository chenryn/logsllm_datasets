# On Purely Automated Attacks and Click-Based Graphical Passwords

**Authors:**
- Amirali Salehi-Abari
- Julie Thorpe
- Paul C. van Oorschot

**Conference:**
2008 Annual Computer Security Applications Conference (ACSAC)

**Title:**
On Purely Automated Attacks and Click-Based Graphical Passwords

**Abstract:**
We present and evaluate various methods for purely automated attacks against click-based graphical passwords. Our approach combines click-order heuristics with focus-of-attention scan-paths generated from a computational model of visual attention. This method significantly outperforms previous work, guessing 8-15% of passwords for two representative images using dictionaries of less than \(2^{24.6}\) entries, and about 16% of passwords on each image using dictionaries of less than \(2^{31.4}\) entries (where the full password space is \(2^{43}\)). Relaxing our click-order pattern substantially increased the efficacy of our attack, albeit with larger dictionaries of \(2^{34.7}\) entries, allowing us to guess 48-54% of passwords (compared to previous results of 0.9% and 9.1% on the same two images with \(2^{35}\) guesses). These latter automated attacks are independent of focus-of-attention models and are based on image-independent guessing patterns. Our results demonstrate that automated attacks, which are easier to arrange than human-seeded attacks and more scalable to systems that use multiple images, pose a significant threat.

## 1. Introduction
Graphical passwords offer an alternative to traditional text passwords by requiring users to remember an image or parts of an image instead of a word. One popular type of graphical password is the click-based graphical password, where users must click on a sequence of points on one or more background images. The most effective attack strategy to date on these schemes appears to be human-seeded attacks, although such attacks are more difficult to arrange and do not scale well for systems that use multiple images. In this paper, we explore purely automated approaches for guessing attacks.

We pursue heuristic-based strategies for purely automated dictionary generation, such as click-order patterns, and prioritize these dictionaries using image processing methods to identify points that users are more likely to choose. We hypothesize that users will choose click-points according to a click-order pattern to help remember the password as fewer "chunks." We examine the DIAG click-order pattern, which captures arcs consistent in both horizontal and vertical directions, and a subset of this pattern called LINE, which captures only horizontal and vertical lines. We also relax the rules on these definitions, showing that a "lazy" approach to these click-order patterns is substantially more effective.

We further hypothesize that users will choose click-points based on their preference for certain points in the image, influenced by how much they are naturally attracted to those points. Attention is the cognitive process of selectively focusing on one aspect of the environment while ignoring others, helping us prioritize sensory information. There are two categories of visual attention models: bottom-up and top-down. Bottom-up visual attention captures how attention is drawn to salient or conspicuous parts of a scene, while top-down visual attention is task-dependent, based on cognitive, volitional control.

Our contributions include the best purely automated attacks to date against click-based graphical passwords, an evaluation of how the model of Itti et al. [15] relates to user-selected click-based graphical passwords, and a new spatial clustering algorithm. We tested our methods using the same field study database used by Thorpe et al. [27], allowing us to compare performance. We found that a "lazy" approach to click-order patterns produced a substantially better automated attack than previous methods with comparable dictionary sizes and images, guessing 48-54% of passwords (compared to 0.9-9.1% previously) on two different images used in a long-term field study with a dictionary of about \(2^{35}\) entries. Furthermore, we optimized this dictionary using Itti’s model, producing dictionaries whose efficacy is comparable to human-seeded attacks: one dictionary of \(2^{30.3}\) entries guessed 15.8% of passwords on one image, and a dictionary of \(2^{31.4}\) entries guessed 16.5% of passwords on a second image.

The remainder of this paper is organized as follows: Section 2 discusses background and related work, including computational models of visual attention. Section 3 describes our purely automated attack generation methods, Section 4 presents the results, Section 5 outlines future work, and Section 6 provides conclusions.

## 2. Background and Related Work

### 2.1 Computational Models of Visual Attention
We conjecture that a significant percentage of users will choose points that draw their attention as components of their click-based passwords, and thus that computational models of visual attention may help pick out more probable click-points. Computational models of bottom-up visual attention are typically defined by features of a digital image, such as intensity, color, and orientation. Top-down visual attention models can be defined by training, but the task must be pre-defined, and a corpus of images tagged with the areas containing the subject to find must be used for training.

In this work, we focus on bottom-up visual attention, using Itti et al.’s [15] computational model. This model is well-known and has empirical evidence supporting its ability to capture people’s bottom-up visual attention. The general idea behind this model is that areas of an image will be salient when they differ from their surroundings. Given an input image, the model outputs a focus-of-attention scan-path to model the locations and order in which a human might automatically and unconsciously attend them. The model consists of two stages: (stage 1) construction of a saliency map based on visual features, and (stage 2) the use of a winner-take-all neural network with inhibition of return to define a specific focus-of-attention scan-path, replicating the order in which a user would scan the image.

### 2.2 Terminology
We hypothesize that users are more likely to choose distinguishable points as click-points. A distinguishable point is a point on a digital image that can be easily distinguished and relocated by a user. General ways to achieve this include using referencable points (e.g., a corner) and calculable points based on other referencable parts of the image (e.g., object centers).

- **Tolerance Region (T-region):** The error allowed (in both vertical and horizontal directions) for a click-point entered on a subsequent login to be accepted as the original click-point.
- **Window Cluster:** A square region of size \(n \times n\) for some positive integer \(n\).
- **Cluster:** A set of one or more points that lie within a window cluster.
- **Alphabet:** A set of distinct window centers.

#### Corner Detection
A corner can be defined as the intersection of two edges, where an edge is defined by the points in a digital image where there are sharp changes in intensity. We use the Harris algorithm [12] as implemented by Kovesi [18] for detecting corners. The Harris corner detection first identifies the edges, blurs them to reduce noise, and then generates an energy map. A local maximum in the energy map indicates the presence of a corner.

#### Centroid Detection
To find the centers of objects, we first partition the digital image into segments using image segmentation. We use the mean-shift segmentation algorithm [7], which takes a feature (range) bandwidth, spatial bandwidth, and a minimum region area (in pixels) as input. After segmentation, we calculate the center of each segment (centroid) by calculating the arithmetic mean of each coordinate of the segment’s points.

**Figure 2.** Corner detection (left) and center detection (right) output for pool.

## 3. Methodology
[Detailed description of the methodology, including the specific steps and algorithms used in the study.]

## 4. Results
[Detailed presentation of the results, including tables, figures, and statistical analysis.]

## 5. Future Work
[Discussion of potential future research directions and improvements.]

## 6. Conclusions
[Summary of the key findings and their implications, along with a brief discussion of the broader impact of the research.]

**Acknowledgments:**
The third author acknowledges NSERC funding under a Discovery Grant and as Canada Research Chair in Network and Software Security.

**References:**
[Complete list of references cited in the paper.]