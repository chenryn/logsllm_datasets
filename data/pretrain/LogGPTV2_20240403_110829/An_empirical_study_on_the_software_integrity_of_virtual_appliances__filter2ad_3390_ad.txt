per-VA whitelist entries (the hashes used to verify a speciﬁc
VA) to a global whitelist while removing any duplicate en-
tries; the size of the global whitelist was recorded after each
step. We repeated this process 10 times to get the average
size for each incremental step, and used these average values
to calculate the percentage values.
Intuitively, we see that the whitelist coverage percentage
grows as the number of VAs used to construct the whitelist
goes up. It does so more rapidly at ﬁrst, but then slows down
progressively. We can see that after 20% of the VAs are
used, about 40% of the total global whitelist is already con-
structed. The size of the ﬁnal global whitelist was 1,697,469
hash entries. The growth rate slows down because some
software packages are installed on multiple VAs, and thus do
not add any extra entries to the whitelist. Table 5 shows the
top 10 most commonly installed software packages as well
as their ﬁle counts (not counting the packages installed on
the base images). perl5.8.8, which has 3,115 ﬁles, was in-
stalled commonly across 27 VAs. python2.6 is another pop-
ular package that appeared on 22 VAs, containing 3,915 ﬁles.
Such packages are responsible for the decreasing growth rate
that we see on the graph.
4.5 Base image analysis
We found the base images for the VAs from the Ama-
zon appliance store by looking at their Linux distribution
238Table 4: Characteristics of the VA clusters
Cluster 1
Cluster 2
# VAs
137
14
Avg. % of unveriﬁed/missing
critical ﬁles
non critical ﬁles
68%
26%
22%
73%
Avg. % of integrity scores Avg. no. of integrity scores
3
99%
43%
2
0.5%
17%
1
0.3%
40%
1
1
171
3
418
205
2
2
77
Table 5: Most common software packages installed
Table 7: Example VAs that provide php 5.3
Software package
perl5.8.8
selinux-policy-2.4.6
python2.6
kernel-2.6.21.7-2.fc8xen
kernel-2.6.18-xenU-ec2-v1.0
python2.4
kernel-2.6.18-238.9.1.el5xen
ec2-ami-tools-1.3-56066
rubygems-1.3.3
# VAs # Files
27
22
22
15
15
12
12
11
11
3,115
23
3,915
251
325
3,038
99
165
307
Table 6: Base image characteristics: truncated average
values for all base images
All ﬁles Unveriﬁed/ missing ﬁles # software
38,928
386
5
3
385
2
0
1
0
and version information.
In most cases, the base images
were published by the distribution providers (e.g., Redhat,
CentOS) or directly by Amazon (with the Amazon Linux
AMI). It would be reasonable to expect that these base im-
ages would be clean and contain only veriﬁed ﬁles. Here we
analyze their characteristics and ﬁnd that it is indeed the
case.
To verify 151 VAs, we downloaded 47 base images, each
of which we used to verify about 3 derived images. Table 6
shows the average values for the base images. The average
number of unveriﬁed/missing ﬁles is 5, which is much smaller
than the number we saw for ILG A (see Table 4). Based
on that characteristic and the fact that the number of low-
integrity packages is suﬃciently small, we claim that the
base images are trusted and our assumption is valid.
4.6 On the usefulness of VA integrity assess-
ment
Findings 1 and 3 clearly demonstrate the need for a pri-
ori software integrity assessment of VAs. When a consumer
pays for a VA to use, she has the right to know that a VA
is conﬁgured properly and meets her software integrity ex-
pectations.
To illustrate the usefulness of software integrity evalua-
tion, in Table 7 we show example choices (among the 151
VAs studied) that a consumer would see when selecting a
VA that provides software package php-5.3. The table is
sorted by increasing number of unveriﬁed/missing ﬁles. The
variances in the integrity scores are quite signiﬁcant. VAs 5
and 7 both have more than 100 low-integrity and medium-
integrity packages (scores 2 and 1 ). Such VAs are part
of Cluster 2 (see Table 4). The ﬁrst two VAs have some
number of unveriﬁed/missing ﬁles but the counts for the
scores 2 and 1 are all 0. This is because all of their unveri-
ﬁed/missing ﬁles are of type conﬁguration ﬁles, not aﬀecting
the integrity scores.
High variances in the number of low-integrity and medium-
All ﬁles Unveriﬁed/
missing ﬁles
21
27
48
94
617
886
2,018
34,410
28,606
63,719
90,977
101,582
91,873
74,039
# soft-
ware
351
352
472
711
711
715
641
3
351
352
470
707
454
711
301
2
0
0
2
3
1
0
0
0
1
123
134
4
0
103
237
VA 1
VA 2
VA 3
VA 4
VA 5
VA 6
VA 7
integrity packages among VAs that provide similar functions
reinforces the need for a priori software integrity assessment.
Veriﬁcation reports would help a consumer avoid VAs that
are conﬁgured badly and choose ones with high-integrity
packages. Providers could also make use of the veriﬁcation
reports to admit VAs for standard workﬂows only when they
have high-integrity software packages. Compliance policies
can be deﬁned in terms of the integrity scores, allowing, for
example, only the VAs with 1 or fewer low-integrity packages
and 2 or fewer medium integrity packages to be published
(using the ILG B average as a guideline). A more ﬂexible
policy might state that a VA for well-known workﬂow with
more than 2 medium integrity packages can still be pub-
lished but must always be monitored with runtime monitor-
ing mechanisms when instantiated. Such policies would al-
low the repository providers to be more permissive (or open)
and still maintain a reliable infrastructure.
4.7
Implementation challenges
We faced several challenges when implementing the frame-
work and verifying Amazon VAs. This section highlights
some of these challenges and suggests workarounds.
4.7.1 Origin of unveriﬁed/missing ﬁles
Identifying the software package to which an unveriﬁed or
missing ﬁle belongs is not always trivial. Some packages get
installed manually (i.e., without using rpm), and the ﬁles get
located in diﬀerent directories. For such ﬁles (unknown to
rpm) that are unveriﬁed or missing, we manually identiﬁed
keywords from their absolute path and matched them with
software packages. The VMCVT prototype, while comput-
ing the integrity scores, used these keywords to ﬁgure out
which package’s score an unveriﬁed/missing ﬁle should in-
ﬂuence. As a workaround for a production VMCVT, the
publisher’s log (see Section 3) should list all the directories
under which a package’s ﬁles are installed. The VMCVT
would use that information to determine the origin of the
unveriﬁed/missing ﬁles when rating the packages.
4.7.2 Creating a whitelist for proprietary software
A small number of VAs had proprietary/commercial soft-
ware packages installed (e.g., Sugarcrm, Ideamax2 Gold,
IBMSoftware/ITMAgent), and we could not ﬁnd the source
239or binary ﬁles or signed MD5 checksums for them. Hence,
we selected one VA with a particular commercial software
package installed and created a whitelist for the package
by hashing the ﬁles from this VA instance, under the as-
sumption that it was a trusted source. Other VA instances
that also had the package installed were veriﬁed using this
whitelist. We suggest that in a real system publishers sub-
mit a software review request to the Conﬁguration Resolver
provider, asking for their software to be reviewed and added
to the whitelist.
4.7.3 Dealing with tmp and var ﬁles
We note that VAs typically have a lot of temporary ﬁles
such as those in the tmp or var directory, and that there is
no way to meaningfully verify their integrity. tmp ﬁles are
less worrying, since they do not aﬀect VA behavior, and we
ignored them. We were less certain about the contents of
var folders, though, and examined the types of ﬁles they
contained before deciding to ignore them. Table 9 (see Ap-
pendix C) shows the top 10 most common var ﬁles across
all the VAs we looked at. Most of the var ﬁles are log ﬁles
and rpm database ﬁles, which, we argue, can also be deleted
without aﬀecting the VA behavior.
5. SECURITY IMPLICATIONS
So far, we have studied the integrity levels of real-world
VAs in terms of their software identity, integrity, and com-
pleteness. Those software properties indicate the extent to
which a VA will behave and operate as expected, and are
also integral to ﬁguring out how secure a VA is. This sec-
tion discusses the security implications of measuring the VA
integrity levels and suggests a few ways to add useful secu-
rity metrics into the framework.
5.1 Expanding the integrity scores with black-
listing
There is room for improving the integrity scores to say
more about the trustworthiness of the software packages. We
suggest complementing our whitelist-based approach with
“blacklisting” via scanning of all unveriﬁed ﬁles for viruses:
score 0 is given to a package that fails an anti-virus check.
Packages with score 0 fall under the “malicious” category.
A VA might be planted with a small number of malicious
ﬁles (that are known to virus scanners), but overall has
a high percentage of cleanly installed packages (score 3 ).
Packages that contain those ﬁles will get score 0 though,
and that VA will be ﬂagged immediately as malicious.
If
those malicious ﬁles are new, clever and unknown to virus
scanners, the maximum score it can get through our system
is 2 , which indicates a package that might not behave in
an expected manner. Even though the integrity scores are
not a silver bullet solution, it will provide some indication
as to how trustworthy a package is whereas a virus scanner
(or any other malware detection solution) will likely miss it.
5.2 Checking the unveriﬁed ﬁles against virus
scanners
line tool that runs multiple anti-virus engines and returns
aggregated reports on the ﬁles. For a total of 111,981 un-
veriﬁed ﬁles (from all 151 VAs), VirustTotal ﬂagged 45 of
them as potentially malicious, which were spread out over
10 VAs (about 7% of the samples). Table 8 shows a sum-
mary of the viruses ﬂagged by VirusTotal.
Table 8: Viruses found from 111,981 unveriﬁed ﬁles
Report
Heuristic.BehavesLike.
Exploit.CodeExec.F
TROJ GEN.F47V0802
Win32.Trojan
Win32.TRCrypt.Upkm
Heuristic.LooksLike.
HTML.Suspicious-URL.H
#
Found
34
#
VAs
8
7
2
1
1
3
1
1
1
Files
word-
setkeycodes;
pt chown;
list-compress; ...
mksock; setcap
0000; 0001 (postgres
9.1)
batik-svg-dom-
1.7.jar (tomcat 6.0)
BookmarkFile.pod
(perl 5)
“Heuristic.BehavesLike.Exploit” is the most common re-
port, being ﬂagged 34 times across 8 VAs. To ensure that
those are not false alarms, we looked at the number of times
that a suspicious ﬁle (e.g., setkeycodes, ctrlaltdel) ap-
peared in the unveriﬁed ﬁles list across all the VAs, and the
number of times it was ﬂagged. For most of those ﬁles, there
was a diﬀerence between the two numbers, indicating that