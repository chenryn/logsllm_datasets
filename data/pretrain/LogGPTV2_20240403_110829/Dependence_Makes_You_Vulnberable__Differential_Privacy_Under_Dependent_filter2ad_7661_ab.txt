x=0 exp(− |S−0−0−0.5x|
x=0 exp(− |S−1−0.5−0.5x|
P (A([Di=0,Dj ])=S))
P (A([Di=1,Dj ])=S)) = max
exp(− |S−0−Dj|
exp(− |S−1−Dj|

≤ exp().
)dx
)dx
S
S
)
)

S
S


The above example exposes the following vulnerabilities
in DP: (1) The privacy Deﬁnition 1 does not account for the
dependence relations between the tuples in a database; (2)
Privacy mechanisms such as LPM rely on the independence of
the data tuples for providing privacy guarantees; and motivates
the DDP-adversary model which is formally deﬁned below.
DDP-adversary
We assume a setting, in which a trusted data curator
maintains a statistical database D = [D1, D2,··· , Dn] where
Di denotes the data from the ith user. In response to a query,
the curator computes a randomized query result A(D), with the
goal of providing statistical information about the dataset while
preserving the privacy of individual users. Both the users and
the data curator are assumed to be honest. The data recipient,
issuing the query, is our DDP-adversary. We associate the
following properties to this adversary who wants to use the
noisy query result to infer data Di:
1The global sensitivity for Example 1 computed according to Deﬁnition 2
is 1 since they only consider neighboring database which differ in one entry.
3
QuerySum,ijDDØøºß()1LapeAdd noiseNoisy()ijDD+IndependentPrivacy Guarantee()expe()exp1.5eDependentijDD^Smaller     means better privacy e[]0.50.50,1jiiDDXDX=+andarei.i.dinPrivacy GuaranteeTABLE I. Statistics of the Gowalla location dataset in our selected regions.
Mahattan,NY Brooklyn,NY Queens,NY San Jose,SF Oakland,SF Pasadena,LA Beverly Hill,LA Long Beach,LA
# of Users
# of Check-ins
# of Locations
997
11277
1641
507
7116
1680
811
9344
2392
1228
16347
2066
862
12647
2319
656
13114
1803
1083
19848
4383
825
9109
1486
Fig. 2. Gowalla’s social dataset colored according to the results
from K-means clustering of the location dataset. We can see
that the location dataset is inherently correlated and the social
dataset well represents such relationships.
Fig. 3. The distance between the location vectors of users.
A. Dataset Description
Sharing of
location information is often associated
with serious privacy threats [3], [35], [36]. Location data
(or mobility traces), can be easily linked with auxiliary
information sources (such as maps) to not only infer places
such as home and work location but also a user’s political
views, her medical conditions, etc.
We use the data collected from the location-based social
networking service Gowalla [9] for mounting our attack to
infer user’s location information. The locations correspond
to users’ check-ins at places. We obtained a subset of their
location dataset which had a total of 196, 591 users and
6, 442, 890 check-ins of these users over the period from
February 2009 to October 2010. Gowalla also maintains an
associated social network connecting its users. In fact, it is
this correlation that forms the basis of our inference attack.
The network data we obtained consisted of 950, 327 edges
connecting 196, 591 users. Considering the sparsity of the
location information, we decided to restrict our analysis to
users around three cities: New York, San Francisco and Los
Angeles. We selected these cities since they had the highest
number of active users. For our attack, we used data from
users who performed at least 10 check-ins at locations within
a 25km radius in any of the three cities. The resulting dataset
contains 6, 969 users, 98, 802 check-ins and 17, 770 locations
as shown in Table I. The corresponding selected social dataset
contains 47, 502 edges connecting these 6, 969 users.
in
user
check-ins
Constructing
the
i, we
Location
Pattern Dataset:
For
set
each
collect
her
a
C(i) = {ci0, ci1, ci2,··· , cimi
}, where each check-in
cik=[User ID, timestamp, GPS coordinates, place identiﬁer].
The GPS coordinates = [lat, lon] represents the latitude and
longitude of the location shared by the user. For the inference
attack, we only consider the GPS coordinates as effective
check-in records, and use them to extract a location pattern
vector for each user. To do so we compute the frequency
of visits to each location, and only keep the latitude and
longitude of those locations that correspond to the top-q
frequencies. Formally, the location pattern vector for user i is
deﬁned as:
di =(cid:0)lati1, loni1 , lati2 , loni2, lati3, loni3 ,··· , loniq
(4)
where (lati1 , loni1) is the coordinate of the most frequently
visited location of user i, (lati2 , loni2) and (lati3 , loni3) cor-
respond to the second and the third most frequently visited
location of user i, respectively. Without loss of generality, we
normalize each attribute in the location pattern dataset such
that its value lies within [0, 1].
(cid:1)
B. Differentially Private Data Release
The GPS coordinates describing a user’s check-ins are
generally considered to be distinct, but in reality they are
typically clustered around a limited number of points. We
consider a scenario where the data provider uses the classical
K-means clustering approach [20] on the Gowalla location
dataset to compute cluster centroids, applies DP mechanism
on the centroids, and publishes the perturbed centroids to
other applications or researchers. The input to the K-means
algorithm are points d1 . . . dn in the 2q-dimensional unit cube
[0, 1]2q. For our attack, we choose q = 6 while constructing
the location pattern in Eq. 4. To preserve the privacy of users’
sensitive data, the data provider perturbs the true centroids
µ = (µ1, µ2,··· ,(cid:101)µk) by using the LPM mechanism, and
releases the perturbed centroids (cid:101)µ = ((cid:101)µ1,(cid:101)µ2,··· ,(cid:101)µk) 2 for
preserving the privacy of each individual’s location pattern.
Fig. 2 depicts the structure of the Gowalla social network
dataset which is colored according to the K-means clustering
results of the Gowalla location dataset (users belonging to the
same community are giving the same color). We can see that
users’ location patterns are inherently correlated, and the social
dataset embeds relationships contained in the location dataset.
Fig. 3 further shows the cumulative distribution function of the
location pattern distance between different user pairs, where
we compute the distance for di and dj as
2In this paper, we use ˜a to represent the perturbed version of a, and ˆa to
represent the estimated value of a.
4
Manhattan,NewYorkQueens,NewYorkBrooklyn,NewYorkSanJose,SanFranciscoPasadena,LosAngelesLongBeach,LosAngelesBeverlyHills,LosAngelesOakland,SanFrancisco0100020003000400000.20.40.60.81Distance (Km)Cumulative Distribution Function  1−hop Friends2−hop Friends3−hop Friends4−hop Friendsq(cid:88)
dist ((latil, lonil), (latjl, lonjl))
(5)
Dist(di, dj) =
1
q
l=1
and dist ((latil, lonil), (latil, lonil)) represents the earth’s sur-
face distance between two coordinates3.
We ﬁnd that the distance between the location patterns
for closer friends is smaller. These observations from Fig. 2
and Fig. 3 not only imply that the location patterns of users are
correlated with each other, but also that their social network
can serve as an important external information source for an
adversary to infer a user’s sensitive records.
C. Inference Algorithm
munity centroids(cid:101)µ = [(cid:101)µ1,(cid:101)µ2,··· ,(cid:101)µk], has access to auxiliary
The adversary can observe the differentially private com-
information D−i (recall DDP-adversary in Section III) and also
to the social relationships among the users. Let the adversary’s
estimated value of Di be ˆDi. Using Bayes’ theorem,
the
posterior probability of ˆDi = ˆdi computed by the DDP-
adversary can thus be written as
P ( ˆDi = ˆdi|(cid:101)µ, D−i)
P ((cid:101)µ, D−i| ˆDi = ˆdi)P ( ˆDi = ˆdi)
P ((cid:101)µ, D−i)
P ((cid:101)µ|D−i, ˆDi = ˆdi)P (D−i| ˆDi = ˆdi)P ( ˆDi = ˆdi)
P ((cid:101)µ, D−i)
P ((cid:101)µ|D−i, ˆDi = ˆdi)P ( ˆDi = ˆdi|D−i)
∼ exp{−|(cid:101)µ − ˆµ|} · P ( ˆDi = ˆdi|D−i)
where exp{−|(cid:101)µ − ˆµ|} in Eq. 6 represents the Laplace
P ((cid:101)µ|D−i)
noise induced by the estimated centroids ˆµ. Such estimated
centroids are computed using the auxiliary D−i of other users
and each potential value ˆDi = ˆdi. P ( ˆDi = ˆdi|D−i) represents
the prior information of Di inferred from the auxiliary infor-
mation D−i of other users. Note that our inference attack can
be mounted with any amount of auxiliary information. For an
adversary with partial auxiliary information, the corresponding
estimated centroids ˆµ and prior information P ( ˆDi = ˆdi|D−i)
are computed based on these partial auxiliary information.
region of Di and compute exp{−|(cid:101)µ − ˆµ|}·P ( ˆDi = ˆdi|D−i)
To estimate ˆDi, an adversary can discretize the potential
for each potential value ˆdi. The adversary can then estimate
ˆDi which corresponds to the maximal posterior probability for
all the potential values ˆdi, i.e.,
exp{−|(cid:101)µ − ˆµ|} · P ( ˆDi = ˆdi|D−i)
ˆDi = argmax
(7)
(6)
=
=
=
ˆdi
The key challenge is for the adversary to compute the
prior information P ( ˆDi = ˆdi|D−i). We consider two different
types of adversaries: one which assumes that the tuples are in-
dependent and the other which utilizes the social relationships
between the users.
1) Attack 1 (Independent Tuple Assumption): First, we
consider an adversary who assumes the tuples within the
dataset are independent as in the standard differential privacy
model. To simplify our analysis without loss of generality,
we assume that Di
(i.e.,
P ( ˆDi = ˆdi|D−i) = P ( ˆDi = ˆdi)) with identical distributions.
3http://www.movable-type.co.uk/scripts/latlong.html
is independent of {Dj}n−1
j=0,j(cid:54)=i
Fig. 4. Location vector inference attack. (a) represents Attack 1
under independent tuple assumption and (b) represents Attack
2 under dependent tuple assumption.
Therefore, the auxiliary information D−i can serve as sampling
values of Di, which can be utilized to estimate the prior
probability P ( ˆDi = ˆdi).
Fig. 4 (a) shows the mechanism for inference attack under
the independent assumption. We discretize the estimated region
for ˆDi where each grid corresponds to a potential value ˆdi of
Di. The red squares (friends of user i) and the green triangles
(non-friends of user i) are location patterns of the other users
which also represent the sampling values of Di. Based on these
sampling values, we estimate the prior probability of ˆDi = ˆdi
by counting the number of values in D−i that fall into the grid
of ˆdi as
Pinde( ˆDi = ˆdi|D−i) =
|dj : dj ∈ grid(ˆdi)|
|dj : dj ∈ grid(ˆdk)|
(8)
(cid:80)
ˆdk
2) Attack 2 (Dependent Tuple Assumption): Next, we
consider a sophisticated adversary who assumes that tuples in
the dataset are dependent on each other. Such an assumption
is practical since the mobility traces from close friends are
likely to be similar as shown in Fig.3. For an adversary who
has access to the social relationships of the users, he can draw
circles, shown in red, in Fig. 4(b), to represent the dependent
relationships among users, and all the girds (corresponding to
each potential value ˆdi) within the red circles would be given
a higher weight. The prior probability for ˆDi = ˆdi would
thus be weighted based on the relationships of the users, and
the weighted prior probability under the dependent assumption
would become
Pde( ˆDi = ˆdi|D−i) =
(cid:80)
weight(ˆdi)|dj : dj ∈ grid(ˆdi)|
weight(ˆdk)|dj : dj ∈ grid(ˆdk)|
ˆdk
(9)
In Fig. 4(a), we can see that there are three sampling
values that belong to the grids corresponding to ˆDi = ˆdi(4)
and ˆDi = ˆdi(8). Therefore, we have Pinde( ˆDi = ˆd4|D−i) =
Pinde( ˆDi = ˆd8|D−i). However, in Fig. 4(b), the grid for ˆDi =
ˆdi(4) would have a much higher weight than the grid for ˆDi =
ˆdi(8). Therefore, we have Pde( ˆDi = ˆdi(4)|D−i) > Pde( ˆDi =
ˆdi(8)|D−i). As we know the location patterns of the user i’s
friends (shown as the red squares Fig. 4), it is more likely that
the location pattern Di of user i will be located closer to her
friends based on our observations in Fig. 3.
5
(a)(b)(cid:11)(cid:12)0ˆid(cid:11)(cid:12)1ˆid(cid:11)(cid:12)2ˆid(cid:11)(cid:12)3ˆid(cid:11)(cid:12)4ˆid(cid:11)(cid:12)5ˆid(cid:11)(cid:12)6ˆid(cid:11)(cid:12)7ˆid(cid:11)(cid:12)8ˆidcheck-in of users who are not friends of icheck-in of users who are friends of i(cid:11)(cid:12)0ˆid(cid:11)(cid:12)1ˆid(cid:11)(cid:12)2ˆid(cid:11)(cid:12)3ˆid(cid:11)(cid:12)4ˆid(cid:11)(cid:12)5ˆid(cid:11)(cid:12)6ˆid(cid:11)(cid:12)7ˆid(cid:11)(cid:12)8ˆidFig. 5. Performance ((a) the inference error and (b) the leaked information) for the inference attack.
(a)
(b)
D. Experimental Evaluation
We evaluate the performance for these two inference
attacks by measuring the following two metrics
Inference Error =
1
n
(cid:88)n
(cid:88)n
H(Di) − H(Di|(cid:101)µ, D−i)
Dist(di, ˆDi)
i=1
(10)
1
n
i=1
Leaked Information =
(11)
Dist(·) is deﬁned in Eq. 5 and H(·) denotes the entropy
(information) of a random variable [10]. H(Di) evaluates
the adversary’s prior information for Di without utilizing the
social relationships and is the entropy of the prior probability
in Eq. 8, H(Di|(cid:101)µ, D−i) evaluates the adversary’s posterior
information after the inference attack and is the entropy of
the posterior probability in Eq. 6 (combined with Eq. 8 under
the independent assumption or combined with Eq. 9 under the
dependent assumption). By evaluating the Leaked Information,
we can measure the privacy breaches in terms of change in an
adversary’s a-priori to a-posteriori beliefs.
We set the number of communities K = 8 in the K-
means algorithm (shown in Fig. 2) and discretize each city
(NY, SF, LA) into 20 × 20 grids. The prior information for
the adversary before the inference attack can be computed
as H(Di) = 8.38 bits according to Eq. 8. From the results
in Fig. 5, we can see that the attacker can exploit the social
relationships between users to make better inferences (shown
by smaller inference errors in Fig. 5(a) and more information
leakage in Fig. 5(b)). Furthermore, in Fig. 5(b), larger  (worse
privacy guarantee) results in smaller inference errors and more
information leakage, since the adversary has access to more
accurate centroids.
DDP-adversary with Partial Information: We also con-
sider a more realistic adversary that has access to partial
information of other users, e.g., Nprior = 3500 (roughly half
of all the other 6, 968 users) as in Fig. 5(b). By utilizing
social relationships, an adversary who only has access to partial
information of other users’ location data can still infer more
information than the DP adversary (recall Section III) who
has access to location information of all the other users but