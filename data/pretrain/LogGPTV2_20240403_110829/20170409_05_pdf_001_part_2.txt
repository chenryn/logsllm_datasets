anddesignchoicesintherestofthissection. time, TimescaleDB’s planner again knows how to opti-
mizequeriesgivenanyuseoftimeinthequerypredicate.
4.1 Scalingupandout
Parallelizing across chunks and servers. Chunks are
Transparent time/space partitioning. As discussed, dynamically created by the runtime and sized to opti-
thedatabasescalesbypartitioninghypertablesintwodi- mize performance in both cluster and single-node envi-
mensions: by time interval, and by a “partitioning key” ronments. When run as a cluster, chunks are placed on
oversomeprimaryindexforthedata(e.g., deviceiden- differentservers;onasinglemachine,chunkscanalsobe
tifiers for sensor data, locations, customers, users, etc.). automaticallyspreadacrossdisks. Witheitherapproach,
Eachtime/spacepartitioniscalledachunk,whichiscre- partitioning by space parallelizes inserts to recent time
atedandplacedonaserveranddiskautomaticallybythe intervals. Similarly, query patterns often slice across
system. AnillustrationofTimescaleDB’sarchitectureis timeorspace, soalsoenjoyperformanceimprovements
showninFigure4. throughsmartchunkplacement.
Everydatabasenodeknowsaboutthetimeandspace By default, chunks belonging to the same region of
ranges comprising data in each chunk, and each node partitionkeyspace,yetvaryingbytimeintervals,arecol-
builds local indexes on each individual chunk. As we locatedonthesameservers. Thisavoidsqueriestouching
4
PARTITION KEY furthersupporthighdataingestrates. Theseapproaches
improveperformancewhenemployedoneitherHDDsor
LATEST SSDs.
Q SPU AE CR EY ACROSS In-memoryindexes. Becausechunksareright-sizedto
servers,andthusthedatabaseneverbuildsmassivesingle
TIME
tables,TimescaleDBavoidsswappingindexestodisksfor
recenttimeintervals(wheremostwritesoccur). Yetitcan
CHUNKS
efficientlysupportanytypeofPostgresindexoncolumns,
OLDEST
frommoretraditionaltextornumericalcolumns,tomore
SERVER QUERY ACROSS TIME specialized indexes on array data types or GIS (spatial)
Figure 4: TimescaleDB architecture. Data is partitioned columns.
acrossbothtimeandspace,withtheresultingchunksorganized
Transactional support. TimescaleDB supports full
forcommonquerypatterns. Writesarelargelysenttothelatest
transactionsoverentrieswiththesamepartitionkey. Ina
chunks,whilequeriessliceacrossbothtimeandspace.
monitoringapplication,forexample,thisensurestransac-
tionalsemanticsonaper-devicebasisandguaranteesthat
allserverswhenperformingqueriesforasingleobjectin
multiple device measurements, which may each involve
space(e.g., aparticulardevice), whichhelpsreducetail
manyindividualsensormetrics,areatomicallyinserted.
latencyunderhigherqueryloads.
Supportfordatabackfill. EventhoughTimescaleDB’s
Right sizing chunks for single nodes. Even in single-
architecture is optimized for the scenario when most
nodesettings,chunkingstillimprovesperformanceover
writesaretothelatesttimeintervals,itfullysupportsthe
thevanillauseofasingledatabasetableforbothinserts
“backfill” of delayed data. Additionally, the database’s
and deletes. Right-sized chunks ensure that all of the
automated chunk management is also aware of the pos-
B-treesforatable’sindexescanresideinmemoryduring
sibility of backfill, so a modest amount of delayed data
inserts to avoid thrashing while modifying arbitrary lo-
canbeconfiguredtonot“overflow”achunk’ssizelimit
cations in those trees. Further, by avoiding overly large
tounnecessarilycreateadditionalundersizedchunksfor
chunks,wecanavoidexpensive“vacuuming”operations
thatinterval.
whenremovingdeleteddataaccordingtoautomatedre-
tentionpolicies,astheruntimecanperformsuchopera- Performancebenefitsforsinglenodes. Whiledatapar-
tionsbysimplydroppingchunks(internaltables),rather titioningistraditionallyseenasamechanismforscaling
than deleting individual rows. At the same time, avoid- out to many servers, TimescaleDB’s approach also pro-
ingtoo-smallchunksimprovesqueryperformancebynot videsmeaningfulperformanceimprovementsevenwhen
needingtoreadadditionaltablesandindexesfromdisk. employedonasinglemachine.
TimescaleDB performs such time/space partitioning To evaluate this behavior, we performed experiments
automatically based on table sizes, rather than an ap- whereclientsbothinsertindividualrowstothedatabases,
proach based on static time intervals more commonly aswellaslargebatchesofrowsinsingleoperations. Each
practiced (e.g., by creating a separate table per day). row includes 12 values in separate columns: a times-
When systems lack TimescaleDB’s transparent hyper- tamp, an indexed randomly-chosen primary id, and 10
tableabstraction,interval-basedpartitioningmightmake additional numerical metrics. Such batched inserts are
themanualtableselectionandjoinsatleasttractable(al- common practice for databases employed in more high-
thoughnoteasy). But,suchintervalsworkpoorlyasdata scaleproductionenvironments,e.g.,wheningestingdata
volumes change, e.g., a time interval appropriate for a fromadistributedqueuelikeKafka. Figure5illustrates
servicepullingdatafrom100devicesisnotappropriate theresults.
whenthatsystemscalesto100Kdevices. TimescaleDB In both scenarios, standard Postgres tables hit a per-
avoids the need to make this choice by managing data formance cliff after tens of millions of rows. Not only
partitioningautomatically. doesthroughputdropoff,butthevarianceincreasessig-
nificantly. In single-row inserts, the database achieves
4.2 Highdatawriterates
onlyhundredsofinsertspersecondquiteregularly. With
Batched commits. Writes are typically made to re- batchedinserts, the insertratefor Postgresconvergesto
cent time intervals, rather than old tables. This allows onlythousandsofinsertsperiod, arounda30xdecrease
TimescaleDBtoefficientlywritebatchinsertstoasmall fromitsperformanceattheonset.
number of tables as opposed to performing many small TimescaleDB, on the other hand, maintains constant
writes. Further,ourscale-outdesignalsotakesadvantage insert performance and low variance regardless of the
oftime-seriesworkloadstorecenttimeintervals,inorder databasesize,asindividualchunksremainappropriately
toparallelizewritesacrossmanyserversand/ordisksto sized. Wheninsertinginbatches,TimescaleDBstartsout
5
Insert batch size: 1, Cache: 4 GB memory Insert batch size: 10000, Cache: 16 GB memory
250,000
PostgreSQL PostgreSQL
16,000
TimescaleDB TimescaleDB
14,000 200,000 ]dnoces ]dnoces
12,000
150,000 10,000 / swor[ / swor[
8,000 100,000 etar etar
6,000 tresnI tresnI
4,000 50,000
2,000
0 0
0 50 100 150 200 250 0 50 100 150 200 250
Dataset size [millions of rows] Dataset size [millions of rows]
(a)Single-rowinserts,4GBcache (b)Batchedinserts,16GBcache
Figure5: InsertthroughputintoTimescaleDB’schunkedhypertable(red)comparedtoastandardtableinPostgreSQL(blue)on
asingledatabaseserver. ExperimentsrunusingPostgreSQL9.6.2onaAzurestandardDS4v2(8core)machinewithSSD-based
(premiumLRS)storage. Everyinsertedrowhas12columns. Trendlinesconstituteapolynomialfitoftheunderlyingdata;each
datapoint shows the average insert throughput over a 20s period. Throughput drops in PostgreSQL as tables grow large, while
hypertableinsertperformanceremainsconstantregardlessoftotaldatavolume.
atanaveragerateofabout140,000rowspersecond,much Whencoupledwithitsotheroptimizations,suchqueries
likevanillaPostgres. ButunlikePostgres,TimescaleDB touchonlythenecessarychunksandperformefficiently-
maintainsitsstableperformanceregardlessofdatasize. indexedqueriesoneachindividualchunk.
Minimizing ordered data from distinct chunks.
4.3 Optimizationsforcomplexqueries
TimescaleDBprovidesanumberofadditionalqueryop-
Intelligently selecting chunks needed to satisfy timizations that benefit both single-node and clustered
queries. Common queries to time-series data include deployments. Postgresalreadyenablesa“mergeappend”
(i) slicing across time for a given object (e.g., device), optimizationwhen combiningin-order datafrommulti-
(ii)slicingacrossmanyobjectsforagiventimeinterval, plechunks,whereaqueryefficientlycombinesdatafrom
or (iii) querying the last reported data records across (a these tables in sorted order, adding rows one-by-one to
subsetof)allobjectsorsomeotherdistinctobjectlabel. thequeryresultinthepropersortedorder,andthenstop-
Such queries to time or space, which may require scan- pingwhentheglobalqueryhasbeensatisfied(e.g.,based
ningovermanychunks(disks,servers),areillustratedin onitsLIMIT).Thisoptimizationensuresthatasubquery
Figure4. is only incrementally processed on a table if its result
Whileusersperformthesequeriesasifinteractingwith wouldbenefitthefinalglobalresultset. Thisisparticu-
a single hypertable, TimescaleDB leverages internally- larlybeneficialforquerieswithcomplexpredicates,such
managed metadata to only query those chunks that may thatfindingthe“next”itemthatmatchesthepredicatecan
possibly satisfy the query predicate. By aggressively involvescanningasignificantamountofdata.
pruningmanychunksandserverstocontactinitsquery TimescaleDB extends such merge-appends optimiza-
plan, TimescaleDB improves both query latency and tionstobringthesesignificantefficienciestotime-based
throughput. aggregates. Such aggregates appear quite regularly for
time-series analysis, such as “tell me the average of a
Minimizing scanning back in time: “LIMIT BY”
queries for distinct items. Similarly, for items like metricperhour,forthelast12hoursthatthedevicehas
unique devices, users, or locations, one often wants to reporteddata”whichexpressedasSQLinvolves“GROUP
askquestionslike“givemethelastreadingforeveryde- BYhourORDERBYhourDESCLIMIT12.” Thisway,
vice.” WhilethisquerycanbenativelyexpressedinSQL evenwithoutastricttime-rangespecifiedbytheuser(un-
usingwindowingoperators,suchaquerywouldturninto likeinmanytime-seriesdatabases),thedatabasewillonly
a full table scan for most relational databases. In fact, processthoseminimalsetofchunksanddataneededto
thisfulltablescancouldcontinuebacktothebeginning answerthisquery.
oftimetocapture“foreverydevice”oratbestsacrifice Parallelized aggregation. Much like its LIMIT push-
completenesswithsomearbitrarily-specifiedtimerange. down, TimescaleDB also pushes down aggregations for
To efficiently support such queries, TimescaleDB au- manycommonfunctions(e.g.,SUM,AVG,MIN,MAX,
tomatically tracks metadata about “distinct” items in COUNT) to the servers on which the chunks reside.
the database, as specified in the hypertable’s schema. Primarily a benefit for clustered deployments, this dis-
6
tributed query optimization greatly minimizes network tiontechniquesemployedbyPostgres,namely,streaming
transfersbyperforminglargerollupsorGROUP_BYsin replicationandcold/hotstandbys,aswellasbackups. It
situ on the chunks’ servers, so that only the computed alsousesPostgres’write-aheadlog(WAL)forconsistent
results need to be joined towards the end of the query, checkpointing. Inotherwords,eventhoughreplicationor
ratherthanrawdatafromeachchunk. backuppoliciescanbedefined(orcommandsissued)on
the hypertable, TimescaleDB performs these actions by
4.4 LeveragingtheRDBMSqueryplanner
replicatingorcheckpointingthehypertable’sconstituent
Because each node runs a full-fledged Postgres query chunks.
planneranddatamodel,deployingnewoptimizationsfor
Automated data retention policies. TimescaleDB al-
particularqueries,indexes,anddatatypesareeasy.
lows for easily defining data retention policies based
From the start, TimescaleDB supports Postgres’ full on time. For example, users can configure the sys-
SQL interface. In the current implementation, some tem to cleanup/erase data more than X weeks old.
queriesareoptimizedmorethanothers. However,subse- TimescaleDB’s time-interval-based chunking also helps
quentreleasesovertimewillincludeadditionalqueryop- make such retention policies more efficient, as the
timizations,allowingthedatabaseto“grow”withusers’ databasecanthenjustDROPitsinternaldatatablesthat
needs,withoutrequiringanychangesbyusers. are expired, as opposed to needing to delete individual
Join against relational data. Today, you can compute rowsandaggressivelyvacuumthereturningtables. For
joinsbetweenhypertablesandstandardrelationaltables, efficiency,thesepoliciescanbeimplementedlazily,i.e.,
whichareeitherstoreddirectlyinthedatabaseoraccessed individual records that are older than the expiry period
via foreign data wrappers to external databases. Future mightnotbeimmediatelydeleted. Rather,whenalldata
optimizationswillminimizedatamovementduringjoins. in a chunk becomes expired, then the entire chunk can
Mosttime-seriesdatabasestodaydonotsupportsuch justbedropped.
JOINs. That lack of support requires that users denor-
malize their data by storing additional metadata or la- 5 Conclusion and Status
belswitheverytime-seriesrecord. Thisapproachgreatly
expands data sizes, and makes updating metadata very Many time-series applications today are asking more
expensive. Alternatively,applicationwriters“silo”their complex questions of their data than before: analyzing
data between databases, and then require the applica- historical trends, monitoring current behavior, identify-
tion writer to perform this join between relational and ing possible problems, predicting future behavior, etc.
time-seriesdataoutsideofthedatabase,whichincreases The data, in turn, is being collected at higher volumes
overallsystemcomplexity. and velocities. In order to serve these applications, the
moderntime-seriesdatabaseneedstomarrybothscala-
Geo-spatial queries. TimescaleDB supports arbitrary
bilityandsupportforhighlyperformantcomplexqueries.
Postgresdatatypeswithinthetimeseries,includingGPS-
TimescaleDB achieves that combination through its
coordinate data by leveraging PostGIS, which provides
automatictime/spacepartitioning,optimizedqueryplan-
best-in-class support for geo-spatial data. Because a
ning,anddeepintegrationwithPostgreSQL.Atthesame
chunkcanuseanytypeofindexingonitsdata,GIN/GiST
time,TimescaleDBdeliversthisperformancethroughan
indexesaresupportedonGISdatarightoutofthebox.
easy-to-useinterface,thankstoitshypertableabstraction,
aswellasfullSQLsupport.
4.5 Flexiblemanagement
TimescaleDB is in active development by a team of
Becausechunksarenativedatabasetablesinternally,we
PhDsbasedinNewYorkCityandStockholm,backedby
can readily leverage the rich set of existing Postgres ca-
top-tier investors. An open-source single-node version
pabilitiesinTimescaleDB.
featuring scalability and full SQL support is currently
Toolingecosystem. TimescaleDBleveragesthedatabase availablefordownload. Aclusteredversionisinprivate
management tooling and features that have developed betawithselectcustomers. Wewelcomeanyfeedbackat
within the Postgres ecosystem over two decades. Users hello@timescale.com.
can connect to TimescaleDB via standard JDBC or
ODBC connectors and command-line tools. Yet
psql
Publication: v6–April7,2017
giventhewaythatTimescaleDB’spartitioningisimple-
mented, users typically only see their hypertables (on
which they can specify these management functions),
ratherthantheirconcomitantchunks.
Highly reliable (replication and backups).
TimescaleDB can reuse the battle-tested replica-
7