title:K2: Reading Quickly from Storage Across Many Datacenters
author:Khiem Ngo and
Haonan Lu and
Wyatt Lloyd
K2: Reading Quickly from Storage Across Many Datacenters
Khiem Ngo, Haonan Lu, Wyatt Lloyd
Princeton University
Abstract—The infrastructure available to large-scale and
medium-scale web services now spans dozens of geographically
dispersed datacenters. Deploying across many datacenters has
the potential to signiﬁcantly reduce end-user latency by serving
users nearer their location. However, deploying across many
datacenters requires the backend storage system be partially
replicated. In turn, this can sacriﬁce the low latency beneﬁts
of many datacenters, especially when a storage system provides
guarantees on what operations will observe.
We present the K2 storage system that provides lower latency
for large-scale and medium-scale web services using partial
replication of data over many datacenters with strong guaran-
tees: causal consistency, read-only transactions, and write-only
transactions. K2 provides the best possible worst-case latency for
partial replication, a single round trip to remote datacenters, and
often avoids sending any requests to far away datacenters using
a novel replication approach, write-only transaction algorithm,
and read-only transaction algorithm.
I. INTRODUCTION
The infrastructure necessary for large-scale web services
and available to medium-scale web services now includes
resources in many geographically dispersed datacenters. The
sheer size of large-scale services requires they deploy across
many datacenters—Google has 24 datacenters [27] and Face-
book has 15 datacenters [21]. Medium-scale services that can
be fully deployed in a few datacenters also have the option of
deploying across many datacenters due to the proliferation of
available locations on cloud platforms [6], [26], [44].
Deploying a service across many datacenters has the poten-
tial to signiﬁcantly decrease its latency for end-users through
increased proximity. For instance, users of a social network
in Australia can have signiﬁcantly faster interactions with the
service when their requests are handled entirely in Australia
instead of needing to go to another continent. This requires
that both the frontend web server that is handling the user’s
requests be in Australia and the backend storage system that
holds the data of the social network—e.g., friend lists, status
updates—be in Australia. Spreading a service’s frontend web
servers across many datacenters does not change the required
number of web servers because each does disjoint work; 100
servers in 1 datacenter and 10 servers in each of 10 datacenters
can handle the same number of user requests. Fully replicating
a backend storage system across many datacenters, however,
proportionally increases its costs because each replica in each
datacenter does the same work of storing and serving all
the data. This makes full replication across many datacenters
economically infeasible. Instead, the large-scale services that
must use many datacenters partially replicate their data by
storing only a subset of it in each datacenter [7], [20].
Partial replication of data, however, can eliminate the
latency beneﬁts of many datacenters and even increase a
service’s latency compared to full replication over a few
datacenters. The latency beneﬁt is eliminated if a datacenter
does not have the required data for a request and needs to go
to a far-away datacenter once. A service’s latency is worse if
the storage system needs to go to a far-away datacenter more
than once: it would have been faster to send the user’s request
to that far-away datacenter and handle all its backend accesses
there. We contend this is why medium-scale web services
typically stick to full replication over a few datacenters.
Further complicating matters are the guarantees the data
store provides. These guarantees include consistency, i.e., what
interleavings of write operations are visible to reads; and
transactions, i.e., what operations can appear to be grouped
into an atomic block. These guarantees enable and simplify
correct application logic, for example, by ensuring a referent
and reference appear in the correct order, as well as reduce
user-visible anomalies. In storage systems that provide such
guarantees over partially-replicated data, multiple round trips
to far-away datacenters would be common, leading to even
higher latency experienced by end-users and dwarﬁng the
beneﬁts of having many datacenters closer to them (§II).
We present K2, a storage system that provides lower la-
tency for large-scale and medium-scale web services using
partial replication of data over many datacenters. K2 provides
guarantees that achieve a sweet spot in the tradeoff between
the strength of abstraction and low latency: causal consistency,
read-only transactions, and write-only transactions. Prior work
that supports stronger guarantees is incompatible with low
latency; while prior work that achieves low latency does not
support any type of transactions.
K2 unlocks low latency for these guarantees by realizing
two design goals. First, it has at most one parallel round of
non-blocking requests to far-away datacenters. Second, it often
avoids sending any requests to far away datacenters. The ﬁrst
goal ensures K2 has latency no worse than fully replicating
across a few datacenters while the second goal provides lower
latency for most requests.
K2’s design includes several components that work together
to achieve these goals. First, K2 fully replicates metadata and
runs its algorithms primarily on that metadata. This enables it
to directly use an existing mechanism for causal consistency:
one-hop dependency checking [39]. Providing transactions,
©2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including
reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or
reuse of any copyrighted component of this work in other works. DOI 10.1109/DSN48987.2021.00034
however, remains challenging because existing mechanisms
do not achieve either of our design goals. Existing write-
only transactions algorithms cause cross-datacenter requests
to block and result in latency worse than a fully-replicated
system. To bound its worst case, K2 introduces a constrained
replication topology and a new write-only transaction algo-
rithm. Constraining replication ensures each datacenter knows
where to read consistent values. The new write-only transac-
tion algorithm decouples the visibility of data for local reads
from remote reads to ensure local reads remain consistent
while ensuring remote reads never block.
To provide local datacenter latency in the common case,
K2 integrates a small cache in each datacenter and introduces
a cache-aware read-only transaction algorithm. The algorithm
maximizes its ability to use cached data while ensuring con-
sistency and isolation. This enables read-only transactions to
often be handled locally with zero cross-datacenter requests.
K2 also uses the cache to provide low latency for write-only
transactions by committing them locally.
Our evaluation of K2 compares to an adaption of a fully
replicated system to work with partial replication and a con-
currently developed system that provides causal consistency
with transactions over partially replicated data. We ﬁnd that
K2 has signiﬁcantly lower latency than the baselines in all
evaluated settings.
In summary, the primary contribution of this paper is the
ﬁrst design that realizes the low latency beneﬁt of many
datacenters for the strong guarantees of causal consistency,
read-only, and write-only transactions. Read-only transactions
achieve low latency because they require zero cross-datacenter
requests in the common case and one round of non-blocking
requests in the worst case. K2’s design achieves this through its
novel replication approach, write-only transaction algorithm,
and read-only transaction algorithm.
II. BACKGROUND AND MOTIVATION
This section provides background, motivates partial replica-
tion, and identiﬁes our design goals.
A. Background
Figure 1 shows the general structure of web services. They
are distributed across several datacenters with frontends that
handle user requests by executing application code that reads
and writes data from a backend storage system. To match
common terminology, we use clients to refer to frontends and
servers to refer to backend storage servers.
Together,
the backend storage servers provide a set of
programming abstractions and accompanying guarantees to the
frontend application servers for manipulating an application’s
data. Stronger guarantees, such as transactions, simplify ap-
plication development by reducing the number of states and
edge cases that a programmer needs to reason about.
Targeted Guarantees. We target guarantees that provide a
sweet spot in the tradeoff between the strength of the abstrac-
tion provided by a storage system and its latency: causal con-
sistency, read-only transactions, and write-only transactions.
Fig. 1: Web services are made up of frontends and back-
end storage servers spread across datacenters. Large services
deploy across many datacenters, e.g., A–I. Medium services
often only deploy across a few datacenters, e.g., A–C. K2 im-
proves latency for large services and enables medium services
to also improve their latency by using many datacenters.
Stronger guarantees require cross-datacenter requests [10],
[15], [25], [37] and thus cannot reap the low latency beneﬁts
of many datacenters. Weaker guarantees are harder to program
against without being necessary for low latency.
This makes K2 suitable for the large set of applications
that prioritize low latency over the strongest guarantees (e.g.,
read-write transactions with strict serializability) such as social
networks, collaborative ﬁltering, and encyclopedias. It is even
suitable for sensitive applications such as access-control: K2’s
guarantees are sufﬁciently strong for Zanzibar, Google’s global
authorization system [45].
Causal consistency provides a partial order over operations
that ensures causality, which has three rules [2], [35]: if an
operation a happens before an operation b in the same thread
of execution, then a → b, meaning that b is causally after a;
if an operation a writes the value v to the variable x, and an
operation b reads the value v from x, then a → b; if a → b and
b → c, then a → c.
Read-only transactions are a group of read operations that
appear to all read from a single consistent state of the data
store. The read operations can span data that is spread across
many different shards of the data store. Grouping them in a
transaction in effect combines them into a single operation. In
K2, the group of reads will see the same, causally-consistent
state of the data store.
Write-only transactions are a group of write operations that
appear to all take effect at the same time. They can also span
data spread across many different shards. Write-only trans-
actions are fully isolated from other write-only transactions
and read-only transactions. Thus, a read-only transaction will
either see all or none of a write-only transaction.
Partial Replication. Large-scale web services like Google
and Facebook are typically deployed across many datacenters,
e.g., all 9 in Figure 1. Deploying across so many datacenters
necessitates partially replicating data to only a subset of the
datacenters [7], [20]. K2 is designed to provide lower latency
for such large-scale services for whom partial replication is
necessary. We are also motivated to provide lower latency for
the much larger number of medium-scale services for whom
partial replication over many datacenters is a deployment
option and not a requirement.
ADEBFGHCIBackend“Servers”Frontend“Clients”(a) User to far-away FE that
is near BE with all the data.
(b) User to nearby FE that
contacts the far-away BE
twice to ensure consistency.
(c) User to nearby FE that
goes to far-away BE at most
once. (K2’s worst case.)
(d) User to nearby FE that
can access all data locally.
(K2’s common case.)
Fig. 2: User latency for requests to frontend web servers (FE) that access backend storage (BE).
B. Partial Replication for the Many
Using many datacenters is not required for medium-scale
web services. Sufﬁcient resources are available on cloud
platforms to store all data and handle all user requests. These
services can place all frontend and backends in 1 datacenter,
e.g., on the West Coast (A). While West Coast users would
get low latency, those elsewhere like Japan would have high
latency from connecting to a far-away datacenter. A better
option is to place frontends and backends in 3 geographically
dispersed datacenters, e.g., West Coast (A), Europe (B), and
Japan (C). This reduces user latency by serving requests closer
to them. And, because data is typically replicated 3× for fault
tolerance, moving from 1 to 3 datacenters has little effect
on the cost of the deployment. Yet, it leaves many users
still connecting to far-away datacenters, e.g., Australian users.
Figure 2a shows such a connection to a far-away frontend.
One option would be to place frontends and fully replicated
backends in a large number of datacenters. Unfortunately,
this option is expensive. For instance, moving from 3 to 9
datacenters would roughly triple costs as the 6 additional
replicas of data are not necessary for fault tolerance.
An appealing alternative is to deploy a service with fron-
tends and partially replicated storage with 1/3 of the data in
each datacenter. The cost for such a deployment would be
roughly the same as using 3 datacenters with full replication.
Such a deployment, however, can result in higher latency for
end users if storage servers in the nearby datacenter contact
far-away datacenters multiple times. This is not a concern for
storage systems that provide eventual consistency because any
data can be returned with any other data.
In storage systems that provide stronger guarantees—e.g.,
causal consistency—multiple round trips to far-away data-
centers would be common. For instance, consider deploying
a storage system in a conﬁguration we call replicas across
datacenters (RAD) where 1/3 of each replica is placed in
each of the 9 datacenters. In this setting, the COPS [38] and
Eiger [39] systems would require as many 2 and 3 sequential
round-trips to far-away datacenters respectively. In COPS and
Eiger, a ﬁrst round optimistically reads data, and a second
round is required if the data returned in the ﬁrst round is
inconsistent. Eiger incurs an additional delay of one round-trip
between datacenters to check the status of pending updates if
the requested data is being modiﬁed by ongoing transactions.
Even 2 round-trips result in higher latency than a deployment
with full replicas in 3 datacenters as shown in Figure 2b.
Avoiding such scenarios motivates our ﬁrst design goal.
Design Goal 1: At Most One Round of Non-Blocking
Cross-Datacenter Requests. When a partially-replicated stor-
age system needs at most one round of cross-datacenter
requests that do not block its latency will at worst be similar