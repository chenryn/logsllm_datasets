## Environment info
  * `transformers` version: 4.10.0 (currently master)
  * Platform: TPU VM3.8 -- Ubuntu 20.04.2 LTS
  * Python version: 3.8.10
  * PyTorch version (GPU?): XLA - 1.8.1
  * Tensorflow version (GPU?): None
  * Using GPU in script?: None
  * Using distributed or parallel set-up in script?: Using `examples/pytorch/language-modeling/run_mlm_no_trainer.py` which is using Accelerator
### Who can help
@sgugger @patil-suraj
## Information
Model I am using (Bert, XLNet ...):
The problem arises when using:
  * the official example scripts: (give details below)
  * my own modified scripts: (give details below)
I have modified small things in `examples/pytorch/language-
modeling/run_mlm_no_trainer.py` and changes as follow (can be reached at
https://github.com/akalieren/transformers-master)
  1. Defined mp_fn to training script.
  2. Added `streaming_data=True` to Dataset Class
  3. Deleted `tpu_num_cores argument` from xla_spawn.py sys.args since it throw arrow.
The tasks I am working on is:
  * an official GLUE/SQUaD task: (give the name) Training MLM from scratch
  * my own task or dataset: (give details below)
## To reproduce
Steps to reproduce the behavior:
  1. Clone modified script  
`git clone https://github.com/akalieren/transformers-master`
  2. `export XRT_TPU_CONFIG="localservice;0;localhost:51011"`
  3. Install required libraries (I did not add extra installments to requirements.txt to highlight they are not stated in official example)
    pip install transformers-master
    pip install .
    pip install -r examples/pytorch/language-modeling/requirements.txt
    pip install accelerate
    pip install datasets[streaming]
  4. Run command
    python3 examples/pytorch/xla_spawn.py --num_cores 8 examples/pytorch/language-modeling/run_mlm_no_trainer.py   --model_type "roberta" --per_device_eval_batch_size 512 --per_device_train_batch_size 512 --max_train_steps 1000000 --preprocessing_num_workers 50 --pad_to_max_length  --tokenizer_name "./tokenizers/Roberta/"  --dataset_name='oscar'  --dataset_config_name='unshuffled_deduplicated_fr' --data_streaming=True --max_seq_length 512   --line_by_line=True
Note: Without xla_spawn, Accelerator use only one cores. Thats why I changed,
with 1 core it is running but slow
    2021-07-26 00:30:54.355600: E tensorflow/core/framework/op_kernel.cc:1693] OpKernel ('op: "TPURoundRobin" device_type: "CPU"') for unknown op: TPURoundRobin
    2021-07-26 00:30:54.355659: E tensorflow/core/framework/op_kernel.cc:1693] OpKernel ('op: "TpuHandleToProtoKey" device_type: "CPU"') for unknown op: TpuHandleToProtoKey
    07/26/2021 00:31:13 - INFO - run_mlm_no_trainer - Distributed environment: TPU
    Num processes: 8
    Process index: 0
    Local process index: 0
    Device: xla:1
    Use FP16 precision: False
    Downloading and preparing dataset oscar/unshuffled_deduplicated_tr (download: 9.68 GiB, generated: 26.43 GiB, post-processed: Unknown size, total: 36.10 GiB) to /home/akali/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_tr/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2...
    07/26/2021 00:31:20 - INFO - run_mlm_no_trainer - Distributed environment: TPU
    Num processes: 8
    Process index: 1
    Local process index: 1
    Device: xla:0
    Use FP16 precision: False
    07/26/2021 00:31:20 - INFO - run_mlm_no_trainer - Distributed environment: TPU
    Num processes: 8
    Process index: 5
    Local process index: 5
    Device: xla:0
    Use FP16 precision: False
    07/26/2021 00:31:20 - INFO - run_mlm_no_trainer - Distributed environment: TPU
    Num processes: 8
    Process index: 7
    Local process index: 7
    Device: xla:0
    Use FP16 precision: False
    07/26/2021 00:31:20 - INFO - run_mlm_no_trainer - Distributed environment: TPU
    Num processes: 8
    Process index: 6
    Local process index: 6
    Device: xla:0
    Use FP16 precision: False
    07/26/2021 00:31:21 - INFO - run_mlm_no_trainer - Distributed environment: TPU
    Num processes: 8
    Process index: 2
    Local process index: 2
    Device: xla:0
    Use FP16 precision: False
    07/26/2021 00:31:21 - INFO - run_mlm_no_trainer - Distributed environment: TPU
    Num processes: 8
    Process index: 4
    Local process index: 4
    Device: xla:0
    Use FP16 precision: False
    07/26/2021 00:31:23 - INFO - run_mlm_no_trainer - Distributed environment: TPU
    Num processes: 8
    Process index: 3
    Local process index: 3
    Device: xla:0
    Use FP16 precision: False
    0 examples [00:00, ? examples/s]07/26/2021 00:31:44 - INFO - datasets_modules.datasets.oscar.84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2.oscar - generating examples from = /home/akali/.cache/huggingface/datasets/downloads/657d72dc352d822d0496bb9f519cf0de87b87064d56024d9d1ac5585568125b1
    718146 examples [00:48, 14431.60 examples/s]07/26/2021 00:32:32 - INFO - datasets_modules.datasets.oscar.84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2.oscar - generating examples from = /home/akali/.cache/huggingface/datasets/downloads/f9b566f31181a53d426a2dc982a1b1de06cc92541de83cee688e5c57f4874300
    1471415 examples [01:36, 13302.22 examples/s]07/26/2021 00:33:21 - INFO - datasets_modules.datasets.oscar.84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2.oscar - generating examples from = /home/akali/.cache/huggingface/datasets/downloads/21f0672cc841442e067c7ea57471788dbd350f889acbd8028e75edb9efcacddb
    2229278 examples [02:24, 16466.88 examples/s]07/26/2021 00:34:09 - INFO - datasets_modules.datasets.oscar.84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2.oscar - generating examples from = /home/akali/.cache/huggingface/datasets/downloads/c027123c743fb1e0079bcd3be75f0ba6be89c6997f6b000e97c33f9c3d9c2742