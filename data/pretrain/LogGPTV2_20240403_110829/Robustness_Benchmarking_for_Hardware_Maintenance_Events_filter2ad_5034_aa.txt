# Robustness Benchmarking for Hardware Maintenance Events

## Authors
Ji Zhu, James Mauro, Ira Pramanick  
Sun Microsystems, Inc.  
{ji.zhu, james.mauro, ira.pramanick}@sun.com

## Abstract
This paper introduces a method to measure a specific aspect of system robustness: the handling of hardware maintenance events. Previous research in robustness benchmarking has primarily focused on injecting faults into a system and assessing its resilience. However, maintenance activities, such as replacing failed hardware components or installing software patches, can also cause system outages. This paper describes a benchmark for measuring a system's robustness against a class of hardware maintenance events.

## 1. Introduction
Availability benchmarking has been a significant area of research in both industry and academia. As society becomes increasingly dependent on computers, system availability is becoming a critical metric for evaluating computer systems. Benchmarks, traditionally used to measure system performance, provide a standardized metric for comparing different platforms, enabling IT organizations to make informed purchasing decisions. Additionally, benchmarks foster competition among vendors, driving technological improvements.

Past research on dependability and availability benchmarking has primarily focused on measuring system robustness under fault conditions. In this paper, we introduce a benchmark to measure system robustness during hardware maintenance events. The proposed benchmark is simple, portable, and can be used to evaluate the concurrent repair capability of different platforms. To the best of our knowledge, this is the first dependability benchmark focused on characterizing system behavior during maintenance events. Sun Microsystems is currently using this benchmark during the product architecting and development phases of its computer servers.

The rest of this paper is organized as follows: Section 2 provides background information and our motivation for this research. Section 3 details the benchmark methodology and metric. Section 4 presents an implementation of the benchmark on two Unix servers. Section 5 discusses relevant issues related to the benchmark, and Section 6 summarizes the findings and suggests future work.

## 2. Background and Motivation
Previous dependability benchmarking research [2, 3, 4, 7, 8, 11] has focused on measuring system behavior under fault conditions, typically using a workload-faultload model [5, 6]. In this model, a set of selected faults (faultload) is injected into a system running a representative workload. The system's fault and error handling capabilities are then assessed, and a benchmark score is established to summarize the overall robustness.

However, faults are not the only events that can cause system outages. Even a fault-tolerant system with 100% fault coverage can suffer an outage during the process of replacing a faulty component. For example, a system with N+1 redundant power supply modules will continue to function when a single module fails. However, if the failed module is not replaced, a second failure will result in a system outage. The maintenance event to replace the failed module may cause an outage unless the system supports hot-plugging. Other maintenance activities, such as hardware upgrades, configuration changes, and software patch installations, can also cause outages.

A study of approximately 6000 outages among 14 customers showed that maintenance events accounted for 79% of total outage events and 84% of total outage duration. Customers are equally concerned about planned outages (caused by maintenance events) and unplanned outages (caused by hardware and software faults). With many global corporations operating 24/7, there is no longer a window for planned outages. For e-commerce companies, any outage, whether planned or unplanned, is unacceptable.

Computer manufacturers have long recognized the importance of reducing downtime caused by planned maintenance and have invested in on-line maintenance technologies, such as hot-swappable components and Dynamic Reconfiguration (DR) capabilities. However, until now, there has been no method to quantitatively compare the on-line maintenance features of different servers. Our previous work [12] in establishing the R-Cubed (R3) framework addressed this gap by covering system maintenance events. While the main purpose of R3 was to establish a benchmarking framework, this paper aims to implement a benchmark based on R3 principles, specifically to measure a system's robustness during hardware repairs.

## 3. Benchmarking Methodology & Metric
Before describing the benchmark methodology and metric, it is important to define some key R3 concepts. A maintenance event, such as a repair action, system upgrade, or configuration change, is intended to restore or retain a functional unit in a specified state. Maintenance events can be categorized into two types:
- **Fault Induced:** A maintenance event directly resulting from a fault, such as replacing a faulty hardware component or installing a software patch.
- **Non-fault Induced:** A maintenance event not resulting from a fault but essential for system operation, such as a configuration change or an upgrade.

Robustness, as defined in [12], is a system's ability to detect and handle fault, maintenance, and system-external events, and the degree to which it remains available in the face of these events. The benchmark introduced in this paper measures a system's robustness against fault-induced hardware maintenance events (also known as hardware repairs), which are one of the most frequently performed maintenance actions on a production system.

### 3.1 Maintenance Event Classification
To develop a benchmark that is not tied to a particular vendor's system or layer, the classification must be general enough to apply to a wide range of systems. A two-class scheme based on system availability (available vs. unavailable) is too simplistic. Most systems today support partial on-line maintenance, where the system is unavailable only during part of the maintenance operation. The R3 framework proposes a three-level classification scheme:

- **Class 1 (Fully-Disruptive) Maintenance:** A maintenance event that requires the system to be unavailable for the entire duration of the activity. An example is the replacement of the main system interconnect.
- **Class 2 (Semi-Disruptive) Maintenance:** A maintenance event that requires the system to be unavailable during part, but not all, of the activity. An example is the phase I implementation of CPU module hot plug capability, where a reboot is required after insertion.
- **Class 3 (Non-Disruptive) Maintenance:** A maintenance event that does not require the system to be unavailable at any point. An example is the phase II implementation of CPU module hot plug capability, where the new CPU can be integrated without a reboot.

Each of these components is described in detail in the following subsections.