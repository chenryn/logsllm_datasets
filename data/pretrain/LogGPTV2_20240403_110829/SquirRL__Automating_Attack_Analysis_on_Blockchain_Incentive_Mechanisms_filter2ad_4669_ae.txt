agents. However, Figure 5 exhibits two surprising features that
require further examination before drawing such conclusions.
First, notice the non-smoothness in Figure 5. This is an
artifact of the OSM strategies solved in [56]. For hash powers
between 0.15 and 0.3, OSM learns one of multiple strategies
that are functionally equivalent in the single-agent setting.
Due to randomness in policy iteration, the solver may choose
different strategies for different hash powers, but they all have
the same relative reward [56]. In our setting, these choices
are not equivalent, and can lead to different rewards for the
OSM agent. This effect causes the non-smoothness in Figure 5,
but does not indicate incorrect results. The precise simulation
outcome that causes this effect is included in Appendix C.
Second, Figure 5 shows a counterintuitive effect: the excess
relative rewards of SquirRL can be negative; e.g., at hash power
0.2, our agent performs slightly worse than the honest agent.
Although we cannot guarantee SquirRL’s optimality, we observe
that even if our agent uses the honest strategy, its rewards are
still lower than those of the honest agent.
This is happening because to model a worst-case rushing
adversary, we implemented the honest agent as part of the
environment, as is standard in prior work [19], [45], [56]. Even
if the strategic agent uses the honest strategy, it is constrained
to choose its actions after the honest agent, which leads to
diminished rewards. Therefore, unlike in the single-agent setting,
a rushing adversary in a multi-agent setting can actually perform
worse than the honest party!
Although this phenomenon may seem like an artifact of our
timing model, we ﬁnd that it applies more generally to multi-
agent games with incomplete information. Consider a game
between A, B, and C, where each player must vote for an option
in the set {0,1}, and agent A (our “honest" agent) always votes
randomly. Suppose B and C know A’s vote vA (i.e., a rushing
model), and suppose C’s ﬁnal reward is equal to the number of
votes for the winning option, i.e. maxi∈{0,1} ∑x∈{A,B,C} 1{vx = i}.
Now if B employs a strategy where it always votes for 1− vA,
C’s reward is always 2. On the other hand, if A’s vote is not
visible to the other agents, then C’s expected reward under
an optimal strategy is strictly larger; with probability 1/4, all
agents will choose the same option, giving a reward of 3.
Hence, in general, a rushing adversary can lead to a strictly
lower expected reward for one of the agents. Coming back
to the selﬁsh mining setting, this suggests that the rushing
adversary model we (and others) posed may not be appropriate
for multi-strategic-agent settings. An attacker should be able
to mimic honest behavior perfectly, but the rushing adversary
model does not allow attackers in the multi-agent setting to
do that. This observation could be of broader interest to the
security community as the empirical and theoretical analysis
of multi-agent systems becomes more widespread [26], [33].
a) Solutions: Our ﬁndings suggest that honest agents
should not necessarily be implemented as part of the environ-
ment. Moving them outside the environment at least allows
strategic parties to mimic honest strategies as they are deﬁned
within the model. However, simply moving the honest agent
out of the environment poses new challenges, by preventing
agents from being able to react to honest actions. In reality,
block propagation times are generally much faster than block
mining speeds [16], so strategic miners should have time to
react to published blocks before the next block is released. Not
allowing this makes it difﬁcult to extract excess rewards.
To incorporate both constraints, we make two modeling
choices that are a notable departure from prior literature (1) we
model the honest party as an agent with a ﬁxed strategy outside
the environment (2) instead of assuming a block is mined at
every time slot, we have a block mining event every m turns
(in our experiments, we let m = 4; m = 2 sufﬁces to avoid the
rushing adversary issue, but we show results with m = 4 to
illustrate a more realistic setting where agents may take multiple
sequential actions between block mining events. The honest
party will always act in the turn following a block mining
event, giving attackers time to react before the next block is
10
α
-
d
r
a
w
e
r
e
v
i
t
a
l
e
R
α
-
d
r
a
w
e
r
e
v
i
t
a
l
e
R
α
-
d
r
a
w
e
r
e
v
i
t
a
l
e
R
Attacker hash power α
Attacker hash power α
Per-attacker hash power α
Fig. 5: SquirRL gives equal or higher rela-
tive rewards compared to OSM; however,
excess relative rewards can be negative.
Fig. 6: We can alleviate the problems with
the rushing adversary (Figure 5) by using
our more realistic model.
Fig. 7: In the multi-agent setting, we ob-
serve no gains from selﬁsh mining in the
presence of three parties.
mined. We refer to this as the "time-segmentation model". This
changes the POMG: the state space S now includes "time",
and so does the observation space Ω. Furthermore, Oi(s), the
observation agent i sees at state s also includes the time.
Training Methodology. The more realistic model introduces
a substantial new difﬁculty to the training process: a longer
time horizon over which to optimize. With sparser block
mining events, the agent must learn to plan. This is a widely-
acknowledged difﬁculty in DRL [52], and simply running the
default PPO conﬁguration from RLlib produces poor results. To
combat this difﬁculty, we leverage the existing structure in the
problem to modify the training methodology. Our modiﬁcations
are as follows:
• Train for longer: approximately 2M episodes
• Anneal m from 0 to the desired value of 4, increasing m
by 1 every 500K steps.
• We want
to detect vulnerabilities, so we bias the
agents towards selﬁsh mining by adding a bonus of
0.1 · max(2M − total episodes,0)/2M if the agent waits
between episode 500K and 1.6M. In general, biasing
agents towards dishonest behavior is a good choice when
analyzing the security of a system.
• At environment initialization, we run OSM agents in
place of SquirRL agents in the game for a random number
of block creation events. Then we use this leftover state
as the initial episode state. It is not necessary to use the
OSM strategy for initialization; any initialization of states
that gives sufﬁcient coverage over all states sufﬁces [3].
• Set the discount factor η = 0.997 rather than η = 0.99
to increase the incentive for the agent to plan ahead.
• Batch size of 1048576 steps.
In Sections VI-C and VI-D, we demonstrate that we obtain
physically realistic results under our new modeling choices, in
addition to obtaining other novel results.
C. OSM is not a Nash equilibrium
We apply this new time-segmented, non-rushing model to
obtain Figure 6. We now observe the more physically realistic
result that when Agent C is instantiated with SquirRL or OSM,
it always outperforms honest agents, unlike in the previous
model. Notice that as α → 0.5, the excess relative reward tends
to zero because the honest party’s hash power tends to 0, so
there is less excess reward to claim.
Furthermore, in a competition with OSM, agent C does better
using DRL (blue line) than OSM (orange line). This implies
that OSM is not actually a NE. In other words, the approach of
[45] to analyze restricted strategy sets is not sufﬁcient. If we
had restricted agents to either honest mine or to follow OSM,
then we might have (incorrectly) concluded from Figure 6’s
orange line that OSM is a NE, similarly to how [45] concludes
that semi-selﬁsh mining is a NE.
D. Selﬁsh mining may be unproﬁtable with k = 3 agents
Our next experiments involve training multiple strategic
agents against one another in a selﬁsh mining game under
the time-segmented, non-rushing model. Notice that these
experiments can only be run with DRL, as the environment
is both unknown and dynamic. We highlight one observation
from Figure 7: with three adaptive strategic agents, the agents
could not achieve reward better than honest mining.
Notice that the training modiﬁcations we detailed in Sub-
section VI-A all bias our agents to behave more selﬁshly.
In addition, we let γi = 1/3 for all i, the maximum possible
follower fraction: however, the equilibrium the agents settle
on is honest mining. Figure 13 illustrates in solid lines the
relative reward of each agent minus its hash power (α = 0.1733)
and in dotted lines the fraction of match actions, which is a
proxy for the agent’s strategy. Matching more often is a more
aggressive strategy; the honest strategy never matches. When
agents deviate from the honest strategy by matching more (e.g.,
agent 2 around iteration 100), they lose reward and quickly
revert to an honest strategy. These experiments suggest (but do
not prove) that honest mining is a Nash equilibrium for k = 3
strategic symmetric agents. Note this does not imply anything
for asymmetric agents. For example, consider agent A with
0.001% hash power, agent B with 0.001% hash power, and
agent C with 40% hash power, with the rest going to the honest
party. This effectively reduces to single-agent selﬁsh mining
with 40% attacker hash power, for which honest mining is not
a Nash equilibrium, as demonstrated in [56] and Section V. We
leave the exploration of asymmetric agents to future work.
11
0.050.100.150.200.250.300.350.400.450.030.020.010.000.010.02SquirRL (vs OSM) OSM (vs OSM) 0.10.20.30.40.50.0000.0050.0100.0150.0200.0250.0300.0350.040SquirRL (vs OSM)OSM (vs OSM)0.00.10.20.30.40.50.000.010.020.030.042 SquirRL agents3 SquirRL agentsFig. 8: Total relative reward, including
both voting and mining rewards.
Fig. 9: Relative reward from Casper FFG
(not including mining rewards).
Fig. 10: Policies of NE vs RL. P1’s hash
rate m1 = 0.1, and Pi uses hash power xi
to inﬁltrate the other pool.
VII. BEYOND SELFISH MINING
We have thus far focused on selﬁsh mining attacks. To show
the general applicability of SquirRL, we apply it to two problems
that are not selﬁsh mining: voting-based ﬁnality protocols [11]
and block withholding [18].
A. Casper the Friendly Finality Gadget (FFG)
In this section, we demonstrate a novel attack on the
Ethereum blockchain’s planned ﬁnalization protocol, called
Casper the Friendly Finality Gadget (FFG) [11]. Casper FFG is a
proof-of-stake (PoS), voting-based protocol for ﬁnalizing blocks
in proof-of-work (PoW) blockchains. Casper FFG includes an
incentive mechanism to ensure that participating nodes, or
validators do not deviate from the desired behavior. Our goal is
to use SquirRL to exploit these incentive mechanisms to amplify
an agent’s reward and/or subvert the integrity of the voting
process. Our attack illustrates how an agent can combine PoW
selﬁsh mining with PoS strategic voting to amplify her own
rewards. To the best of our knowledge, this is the ﬁrst attack
to combine selﬁsh mining with strategic voting in Byzantine
fault-tolerant (BFT)-style protocols. These experiments cannot
be solved with value/policy iteration because the state space is
continuous.
Casper FFG validators are meant to ﬁnalize the ﬁrst block (or
checkpoint) of every epoch, deﬁned as a chunk of consecutive
(cid:96) blocks on the same chain. Finalization occurs via voting.
When a checkpoint receives more than 2/3 of the votes, it is
justiﬁed. Here, the votes are weighted by the voters’ deposits.
If multiple checkpoints exist at the same height, a validator
should vote for the checkpoint on the longer chain. If two
consecutive checkpoints on the same chain are justiﬁed, the
ﬁrst checkpoint is ﬁnalized and it will remain in the canonical
chain forever. A chain that does not include every ﬁnalized
checkpoint in the system is considered invalid, even if it is the
longest (greatest-work) chain in the system.
The Casper FFG incentive mechanism is designed to ensure
that validators (a) participate in every epoch’s voting protocol,
and (b) vote for the same checkpoint if multiple options exist.
To achieve this, each validator v makes a deposit Dv into a
smart contract on the Ethereum chain to join the validator pool.
Roughly, if a checkpoint c at height h(c) is ﬁnalized, then all
the voters who voted for c see their deposit grow, whereas any
voters who voted for a different checkpoint c(cid:48) with h(c(cid:48)) = h(c)
will see their deposit shrink.
We implement a simpliﬁed version of the Casper FFG
incentive mechanism and voting process that captures the
essence of the protocol. At any given time step, we model
the voting process as either active or inactive. If the voting
process is inactive, then the actions and state transitions are
the same as in selﬁsh mining. If the voting process is active,