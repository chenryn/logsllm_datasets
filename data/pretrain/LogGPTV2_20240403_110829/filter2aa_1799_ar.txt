### Chapter 8: System Mechanisms

#### Executive Partitions and Work Queues

Each partition object, as described in Chapter 5 of Part 1, includes an executive partition. This partition is the part of the partition object that is relevant to the executive, specifically the system worker thread logic. The executive partition contains a data structure that tracks the work queue manager for each NUMA (Non-Uniform Memory Access) node within the partition. A work queue manager consists of:
- A deadlock detection timer
- A work queue item reaper
- A handle to the actual thread managing the queue

Additionally, the executive partition contains an array of pointers to up to eight possible work queues (EX_WORK_QUEUE). Each work queue is associated with an individual index and tracks:
- The minimum (guaranteed) and maximum number of threads
- The number of work items processed so far

Every system includes two default work queues:
- **ExPool**: Used by drivers and system components via the `ExQueueWorkItem` API.
- **IoPool**: Intended for use with the internal (non-exported) `ExQueueWorkItemToPrivatePool` API, which takes a pool identifier. The Store Manager (refer to Chapter 5 of Part 1 for more information) leverages this capability.

#### Dynamic Worker Thread Management

The executive dynamically adjusts the number of critical worker threads based on the changing workload of the system. When work items are being processed or queued, a check is made to determine if a new worker thread is needed. If so, an event is signaled, waking up the `ExpWorkQueueManagerThread` for the associated NUMA node and partition. An additional worker thread is created under the following conditions:
- The number of threads is below the minimum required for the queue.
- There are pending work items in the queue, or the last attempt to queue a work item failed.

Additionally, every second, the `ExpWorkQueueManagerThread` checks for potential deadlocks in each worker queue manager (i.e., for each NUMA node in each partition). If a deadlock is detected, an additional worker thread is created, regardless of any maximum thread limits, to clear out the potential deadlock. This detection is then disabled until it is deemed necessary to check again, such as when the maximum number of threads has been reached.

Since processor topologies can change due to hot add operations, the system also keeps track of new processors. Every 20 minutes (by default), the system checks if it should destroy any system worker threads. This process, called reaping, ensures that the system worker thread count does not get out of control. A system worker thread is reaped if it has been waiting for a long time, indicating that the current number of threads is sufficient to clear out the work items in a timely manner.

#### Experiment: Listing System Worker Threads

Due to changes in the architecture (which is no longer per-NUMA node and certainly no longer global), the `!exqueue` command can no longer be used to list system worker threads. However, since the `EPARTITION`, `EX_PARTITION`, and `EX_WORK_QUEUE` data structures are available in the public symbols, the debugger data model can be used to explore the queues and their contents.

For the main (default) system partition, you can use the following commands:

```plaintext
lkd> dx ((nt!_EX_PARTITION*)(*(nt!_EPARTITION**)&nt!PspSystemPartition)->ExPartition)->WorkQueueManagers[0]
```

This will display the `EX_WORK_QUEUE_MANAGER` for the first NUMA node in the partition. For example:

```plaintext
((nt!_EX_PARTITION*)(*(nt!_EPARTITION**)&nt!PspSystemPartition)->ExPartition)->WorkQueueManagers[0]
: 0xffffa483edea99d0 [Type: _EX_WORK_QUEUE_MANAGER *]
    [+0x000] Partition: 0xffffa483ede51090 [Type: _EX_PARTITION *]
    [+0x008] Node: 0xfffff80467f24440 [Type: _ENODE *]
    [+0x010] Event: [Type: _KEVENT]
    [+0x028] DeadlockTimer: [Type: _KTIMER]
    [+0x068] ReaperEvent: [Type: _KEVENT]
    [+0x080] ReaperTimer: [Type: _KTIMER2]
    [+0x108] ThreadHandle: 0xffffffff80000008 [Type: void *]
    [+0x110] ExitThread: 0x0 [Type: unsigned long]
    [+0x114] ThreadSeed: 0x1 [Type: unsigned short]
```

Alternatively, you can view the ExPool for NUMA Node 0, which currently has 15 threads and has processed almost 4 million work items:

```plaintext
lkd> dx ((nt!_EX_PARTITION*)(*(nt!_EPARTITION**)&nt!PspSystemPartition)->ExPartition)->WorkQueues[0][0],d
```

This will display the `EX_WORK_QUEUE` for the first work queue in the first NUMA node:

```plaintext
((nt!_EX_PARTITION*)(*(nt!_EPARTITION**)&nt!PspSystemPartition)->ExPartition)->WorkQueues[0][0],d
: 0xffffa483ede4dc70 [Type: _EX_WORK_QUEUE *]
    [+0x000] WorkPriQueue: [Type: _KPRIQUEUE]
    [+0x2b0] Partition: 0xffffa483ede51090 [Type: _EX_PARTITION *]
    [+0x2b8] Node: 0xfffff80467f24440 [Type: _ENODE *]
    [+0x2c0] WorkItemsProcessed: 3942949 [Type: unsigned long]
    [+0x2c4] WorkItemsProcessedLastPass: 3931167 [Type: unsigned long]
    [+0x2c8] ThreadCount: 15 [Type: long]
    [+0x2cc (30: 0)] MinThreads: 0 [Type: long]
    [+0x2cc (31:31)] TryFailed: 0 [Type: unsigned long]
    [+0x2d0] MaxThreads: 4096 [Type: long]
    [+0x2d4] QueueIndex: ExPoolUntrusted (0) [Type: _EXQUEUEINDEX]
    [+0x2d8] AllThreadsExitedEvent: 0x0 [Type: _KEVENT *]
```

To enumerate the worker threads associated with this queue, you can use:

```plaintext
lkd> dx -r0 @$queue = ((nt!_EX_PARTITION*)(*(nt!_EPARTITION**)&nt!PspSystemPartition)->ExPartition)->WorkQueues[0][0]
lkd> dx Debugger.Utility.Collections.FromListEntry(@$queue->WorkPriQueue.ThreadListHead, "nt!_KTHREAD", "QueueListEntry")
```

This will list the worker threads, such as:

```plaintext
[0x0] [Type: _KTHREAD]
[0x1] [Type: _KTHREAD]
...
[0xf] [Type: _KTHREAD]
```

Similarly, you can view the IoPool for NUMA Node 0, which would be the next index (1) on this NUMA Node (0):

```plaintext
lkd> dx ((nt!_EX_PARTITION*)(*(nt!_EPARTITION**)&nt!PspSystemPartition)->ExPartition)->WorkQueues[0][1],d
```

This will display the `EX_WORK_QUEUE` for the IoPool:

```plaintext
((nt!_EX_PARTITION*)(*(nt!_EPARTITION**)&nt!PspSystemPartition)->ExPartition)->WorkQueues[0][1],d
: 0xffffa483ede77c50 [Type: _EX_WORK_QUEUE *]
    [+0x000] WorkPriQueue: [Type: _KPRIQUEUE]
    [+0x2b0] Partition: 0xffffa483ede51090 [Type: _EX_PARTITION *]
    [+0x2b8] Node: 0xfffff80467f24440 [Type: _ENODE *]
    [+0x2c0] WorkItemsProcessed: 1844267 [Type: unsigned long]
    [+0x2c4] WorkItemsProcessedLastPass: 1843485 [Type: unsigned long]
    [+0x2c8] ThreadCount: 5 [Type: long]
    [+0x2cc (30: 0)] MinThreads: 0 [Type: long]
    [+0x2cc (31:31)] TryFailed: 0 [Type: unsigned long]
    [+0x2d0] MaxThreads: 4096 [Type: long]
    [+0x2d4] QueueIndex: IoPoolUntrusted (1) [Type: _EXQUEUEINDEX]
    [+0x2d8] AllThreadsExitedEvent: 0x0 [Type: _KEVENT *]
```

#### Exception Dispatching

In contrast to interrupts, which can occur at any time, exceptions are conditions that result directly from the execution of the program. Windows uses structured exception handling, which allows applications to gain control when exceptions occur. The application can then choose to handle the exception (e.g., by resuming the execution of the subroutine that raised the exception) or declare back to the system that the exception cannot be handled.

Structured exception handling is accessible through language extensions, such as the `__try` construct in C/C++. The system responds to exceptions by using the Interrupt Descriptor Table (IDT), where entries point to the trap handler for specific exceptions. Hardware interrupts are assigned entries later in the IDT, as mentioned earlier.