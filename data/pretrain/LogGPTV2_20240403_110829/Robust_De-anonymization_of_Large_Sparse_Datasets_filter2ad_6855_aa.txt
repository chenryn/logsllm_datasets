title:Robust De-anonymization of Large Sparse Datasets
author:Arvind Narayanan and
Vitaly Shmatikov
2008 IEEE Symposium on Security and Privacy
Robust De-anonymization of Large Sparse Datasets
Arvind Narayanan and Vitaly Shmatikov
The University of Texas at Austin
Abstract
We
present a
attacks
new class of
statistical de-
anonymization
high-dimensional
micro-data, such as individual preferences, recommen-
dations, transaction records and so on. Our techniques
are robust to perturbation in the data and tolerate some
mistakes in the adversary’s background knowledge.
against
We apply our de-anonymization methodology to the
Netﬂix Prize dataset, which contains anonymous movie
ratings of 500,000 subscribers of Netﬂix, the world’s
largest online movie rental service. We demonstrate
that an adversary who knows only a little bit about
an individual subscriber can easily identify this sub-
scriber’s record in the dataset. Using the Internet
Movie Database as the source of background knowl-
edge, we successfully identiﬁed the Netﬂix records of
known users, uncovering their apparent political pref-
erences and other potentially sensitive information.
1 Introduction
Datasets containing micro-data, that is, information
about speciﬁc individuals, are increasingly becoming
public in response to “open government” laws and to
support data mining research. Some datasets include
legally protected information such as health histories;
others contain individual preferences and transactions,
which many people may view as private or sensitive.
Privacy risks of publishing micro-data are well-
known. Even if identiﬁers such as names and Social
Security numbers have been removed, the adversary can
use background knowledge and cross-correlation with
other databases to re-identify individual data records.
Famous attacks include de-anonymization of a Mas-
sachusetts hospital discharge database by joining it with
a public voter database [25] and privacy breaches caused
by (ostensibly anonymized) AOL search data [16].
Micro-data are characterized by high dimensionality
and sparsity. Each record contains many attributes (i.e.,
columns in a database schema), which can be viewed as
dimensions. Sparsity means that for the average record,
there are no “similar” records in the multi-dimensional
space deﬁned by the attributes. This sparsity is empir-
ically well-established [7, 4, 19] and related to the “fat
tail” phenomenon: individual transaction and preference
records tend to include statistically rare attributes.
Our contributions. Our ﬁrst contribution is a formal
model for privacy breaches in anonymized micro-data
(section 3). We present two deﬁnitions, one based on the
probability of successful de-anonymization, the other on
the amount of information recovered about the target.
Unlike previous work [25], we do not assume a pri-
ori that the adversary’s knowledge is limited to a ﬁxed
set of “quasi-identiﬁer” attributes. Our model thus en-
compasses a much broader class of de-anonymization
attacks than simple cross-database correlation.
Our second contribution is a very general class of
de-anonymization algorithms, demonstrating the funda-
mental limits of privacy in public micro-data (section 4).
Under very mild assumptions about the distribution from
which the records are drawn, the adversary with a small
amount of background knowledge about an individual
can use it to identify, with high probability, this individ-
ual’s record in the anonymized dataset and to learn all
anonymously released information about him or her, in-
cluding sensitive attributes. For sparse datasets, such as
most real-world datasets of individual transactions, pref-
erences, and recommendations, very little background
knowledge is needed (as few as 5-10 attributes in our
case study). Our de-anonymization algorithm is robust
to the imprecision of the adversary’s background knowl-
edge and to perturbation that may have been applied to
the data prior to release. It works even if only a subset
of the original dataset has been published.
Our third contribution is a practical analysis of
containing anonymized
the Netﬂix Prize dataset,
movie ratings of 500,000 Netﬂix subscribers (sec-
tion 5). Netﬂix—the world’s largest online DVD rental
978-0-7695-3168-7 /08 $25.00 © 2008 IEEE
DOI 10.1109/SP.2008.33
111
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:12:32 UTC from IEEE Xplore.  Restrictions apply. 
service—published this dataset to support the Netﬂix
Prize data mining contest. We demonstrate that an ad-
versary who knows a little bit about some subscriber can
easily identify her record if it is present in the dataset,
or, at the very least, identify a small set of records which
include the subscriber’s record. The adversary’s back-
ground knowledge need not be precise, e.g., the dates
may only be known to the adversary with a 14-day error,
the ratings may be known only approximately, and some
of the ratings and dates may even be completely wrong.
Because our algorithm is robust, if it uniquely identiﬁes
a record in the published dataset, with high probability
this identiﬁcation is not a false positive.
2 Related work
Unlike statistical databases
[1, 3, 5], micro-
data include actual records of individuals even after
anonymization. A popular approach to micro-data pri-
vacy is k-anonymity [27, 9]. The data publisher de-
cides in advance which of the attributes may be available
to the adversary (these are called “quasi-identiﬁers”),
and which are the sensitive attributes to be protected.
k-anonymization ensures that each quasi-identiﬁer tu-
ple occurs in at least k records in the anonymized
database. This does not guarantee any privacy, because
the values of sensitive attributes associated with a given
quasi-identiﬁer may not be sufﬁciently diverse [20, 21]
or the adversary may know more than just the quasi-
identiﬁers [20]. Furthermore, k-anonymization com-
pletely fails on high-dimensional datasets [2], such as
the Netﬂix Prize dataset and most real-world datasets of
individual recommendations and purchases.
The de-anonymization algorithm presented in this pa-
per does not assume that the attributes are divided a pri-
ori into quasi-identiﬁers and sensitive attributes. Ex-
amples include anonymized transaction records (if the
adversary knows a few of the individual’s purchases,
can he learn all of her purchases?), recommendations
and ratings (if the adversary knows a few movies that
the individual watched, can he learn all movies she
watched?), Web browsing and search histories, and so
on. In such datasets, it is hard to tell in advance which
attributes might be available to the adversary; the adver-
sary’s background knowledge may even vary from indi-
vidual to individual. Unlike [25, 22, 14], our algorithm
is robust. It works even if the published records have
been perturbed, if only a subset of the original dataset
has been published, and if there are mistakes in the ad-
versary’s background knowledge.
Our deﬁnition of privacy breach is somewhat similar
to that of Chawla et al. [8]. We discuss the differences in
section 3. There is theoretical evidence that for any (san-
itized) database with meaningful utility, there is always
some auxiliary or background information that results
in a privacy breach [11]. In this paper, we aim to quan-
tify the amount of auxiliary information required and its
relationship to the percentage of records which would
experience a signiﬁcant privacy loss.
We are aware of only one previous paper that consid-
ered privacy of movie ratings. In collaboration with the
MovieLens recommendation service, Frankowski et al.
correlated public mentions of movies in the MovieLens
discussion forum with the users’ movie rating histories
in the internal MovieLens dataset [14]. The algorithm
uses the entire public record as the background knowl-
edge (29 ratings per user, on average), and is not robust
if this knowledge is imprecise, e.g., if the user publicly
mentioned movies which he did not rate.
While our algorithm follows the same basic scoring
paradigm as [14], our scoring function is more complex
and our selection criterion is nontrivial and an impor-
tant innovation in its own right. Furthermore, our case
study is based solely on public data and does not involve
cross-correlating internal Netﬂix datasets (to which we
do not have access) with public forums. It requires much
less background knowledge (2-8 ratings per user), which
need not be precise. Furthermore, our analysis has pri-
vacy implications for 500,000 Netﬂix subscribers whose
records have been published; by contrast, the largest
public MovieLens datasets contains only 6,000 records.
3 Model
Database. Deﬁne database D to be an N × M matrix
where each row is a record associated with some indi-
vidual, and the columns are attributes. We are interested
in databases containing individual preferences or trans-
actions. The number of columns thus reﬂects the total
number of items in the space we are considering, rang-
ing from a few thousand for movies to millions for (say)
the amazon.com catalog.
Each attribute (column) can be thought of as a dimen-
sion, and each individual record as a point in the multidi-
mensional attribute space. To keep our analysis general,
we will not ﬁx the space X from which attributes are
drawn. They may be boolean (e.g., has this book been
rated?), integer (e.g., the book’s rating on a 1-10 scale),
date, or a tuple such as a (rating, date) pair.
A typical reason to publish anonymized micro-data is
“collaborative ﬁltering,” i.e., predicting a consumer’s fu-
ture choices from his past behavior using the knowledge
112
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:12:32 UTC from IEEE Xplore.  Restrictions apply. 
of what similar consumers did. Technically, the goal is
to predict the value of some attributes using a combina-
tion of other attributes. This is used in shopping recom-
mender systems, aggressive caching in Web browsers,
and other applications [28].
Sparsity and similarity. Preference databases with
thousands of attributes are necessarily sparse, i.e., each
individual record contains values only for a small frac-
tion of attributes. For example, the shopping history of
even the most proﬂigate Amazon shopper contains only
a tiny fraction of all available items. We call these at-
tributes non-null; the set of non-null attributes is the sup-
port of a record (denoted supp(r)). Null attributes are
denoted ⊥. The support of a column is deﬁned anal-
ogously. Even though points corresponding to database
records are very sparse in the attribute space, each record
may have dozens or hundreds of non-null attributes,
making the database truly high-dimensional.
The distribution of per-attribute support sizes is typi-
cally heavy- or long-tailed, roughly following the power
law [7, 4]. This means that although the supports of the
columns corresponding to “unpopular” items are small,
these items are so numerous that they make up the bulk
of the non-null entries in the database. Thus, any attempt
to approximate the database by projecting it down to the
most common columns is bound to failure.1
Unlike “quasi-identiﬁers” [27, 9], there are no at-
tributes that can be used directly for de-anonymization.
In a large database, for any except the rarest attributes,
there are hundreds of records with the same value of this
attribute. Therefore, it is not a quasi-identiﬁer. At the
same time, knowledge that a particular individual has
a certain attribute value does reveal some information,
since attribute values and even the mere fact that a given
attribute is non-null vary from record to record.
The similarity measure Sim is a function that maps
a pair of attributes (or more generally, a pair of records)
to the interval [0, 1]. It captures the intuitive notion of
two values being “similar.” Typically, Sim on attributes
will behave like an indicator function. For example, in
our analysis of the Netﬂix Prize dataset, Sim outputs 1
on a pair of movies rated by different subscribers if and
only if both the ratings and the dates are within a certain
threshold of each other; it outputs 0 otherwise.
To deﬁne Sim over two records r1, r2, we “general-
ize” the cosine similarity measure:
(cid:1)
Sim(r1, r2) =
Sim(r1i, r2i)
|supp(r1) ∪ supp(r2)|
1The same effect causes k-anonymization to fail on high-
dimensional databases [2].
113
Figure 1. X-axis (x) is the similarity to
the “neighbor” with the highest similar-
ity score; Y-axis is the fraction of sub-
scribers whose nearest-neighbor similar-
ity is at least x.
Deﬁnition 1 (Sparsity) A database D is (, δ)-sparse
w.r.t. the similarity measure Simif
[Sim(r, r(cid:1)
Pr
r
) >  ∀r(cid:1) (cid:4)= r] ≤ δ
As a real-world example, in ﬁg. 1 we show that the
Netﬂix Prize dataset is overwhelmingly sparse. For the
vast majority of records, there isn’t a single record with
similarity score over 0.5 in the entire 500,000-record
dataset, even if we consider only the sets of movies rated
without taking into account numerical ratings or dates.
Sanitization and sampling. Database sanitization
methods include generalization and suppression [26, 9],
as well as perturbation. The data publisher may only re-
lease a (possibly non-uniform) sample of the database.
Our algorithm is designed to work against data that have
been both anonymized and sanitized.
If the database is published for collaborative ﬁlter-
ing or similar data mining purposes (as in the case of
the Netﬂix Prize dataset), the “error” introduced by san-
itization cannot be large, otherwise data utility will be
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:12:32 UTC from IEEE Xplore.  Restrictions apply. 
lost. We make this precise in our analysis. Our deﬁni-
tion of privacy breach allows the adversary to identify
not just his target record, but any record as long as it is
sufﬁciently similar (via Sim) to the target and can thus
be used to determine its attributes with high probability.
From the viewpoint of our de-anonymization algo-
rithm, there is no difference between the perturbation of
the published records and the imprecision of the adver-
sary’s knowledge about his target. In either case, there
is a small discrepancy between the attribute value(s) in
the anonymous record and the same value(s) as known
to the adversary. In the rest of the paper, we treat pertur-
bation simply as imprecision of the adversary’s knowl-
edge. The algorithm is designed to be robust to the latter.
Adversary model. We sample record r randomly from
database D and give auxiliary information or back-
ground knowledge related to r to the adversary.
It is
restricted to a subset of (possibly imprecise, perturbed,