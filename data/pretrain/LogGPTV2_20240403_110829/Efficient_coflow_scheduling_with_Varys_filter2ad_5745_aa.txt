title:Efficient coflow scheduling with Varys
author:Mosharaf Chowdhury and
Yuan Zhong and
Ion Stoica
Efﬁcient Coﬂow Scheduling with Varys
Mosharaf Chowdhury1, Yuan Zhong2, Ion Stoica1
1UC Berkeley, 2Columbia University
{mosharaf, istoica}@cs.berkeley.edu, PI:EMAIL
ABSTRACT
Communication in data-parallel applications often involves a col-
lection of parallel ﬂows. Traditional techniques to optimize ﬂow-
level metrics do not perform well in optimizing such collections,
because the network is largely agnostic to application-level require-
ments. The recently proposed coﬂow abstraction bridges this gap
and creates new opportunities for network scheduling. In this pa-
per, we address inter-coﬂow scheduling for two different objec-
tives: decreasing communication time of data-intensive jobs and
guaranteeing predictable communication time. We introduce the
concurrent open shop scheduling with coupled resources problem,
analyze its complexity, and propose effective heuristics to opti-
mize either objective. We present Varys, a system that enables
data-intensive frameworks to use coﬂows and the proposed algo-
rithms while maintaining high network utilization and guarantee-
ing starvation freedom. EC2 deployments and trace-driven simula-
tions show that communication stages complete up to 3.16× faster
on average and up to 2× more coﬂows meet their deadlines us-
ing Varys in comparison to per-ﬂow mechanisms. Moreover, Varys
outperforms non-preemptive coﬂow schedulers by more than 5×.
Categories and Subject Descriptors
C.2 [Computer-communication networks]: Distributed sys-
tems—Cloud computing
Keywords
Coﬂow; data-intensive applications; datacenter networks
1 Introduction
Although many data-intensive jobs are network-bound [7,9,15,23],
network-level optimizations [7, 8, 12, 25] remain agnostic to job-
speciﬁc communication requirements. This mismatch often hurts
application-level performance, even when network-oriented met-
rics like ﬂow completion time (FCT) or fairness improve [15].
Despite the differences among data-intensive frameworks [3, 4,
18,26,29,37,41], their communication is structured and takes place
between groups of machines in successive computation stages [16].
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
SIGCOMM’14, August 17–22, 2014, Chicago, IL, USA.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2836-4/14/08 ...$15.00.
http://dx.doi.org/10.1145/2619239.2626315.
Often a communication stage cannot ﬁnish until all its ﬂows have
completed [15, 19]. The recently proposed coﬂow abstraction [16]
represents such collections of parallel ﬂows to convey job-speciﬁc
communication requirements – for example, minimizing comple-
tion time or meeting a deadline – to the network and enables
application-aware network scheduling (§2). Indeed, optimizing a
coﬂow’s completion time (CCT) decreases the completion time of
corresponding job [15].
However, jobs from one or more frameworks create multiple
coﬂows in a shared cluster. Analysis of production traces shows
wide variations in coﬂow characteristics in terms of total size, the
number of parallel ﬂows, and the size of individual ﬂows (§4).
Simple scheduling mechanisms like FIFO and its variants [15, 19],
which are attractive for the ease of decentralization, do not perform
well in such an environment – one large coﬂow can slow down
many smaller ones or result in many missed deadlines.
Simply applying a shortest- or smallest-ﬁrst heuristic, the pre-
dominant way to solve most scheduling problems, is not sufﬁcient
either (§5). Inter-coﬂow scheduling is different from scheduling in-
dividual ﬂows [8, 25], because each coﬂow involves multiple par-
allel ﬂows. It also differs from related problems like scheduling
parallel tasks [9, 40] or caching parallel blocks [10]; unlike CPU
or memory, the network involves coupled resources – each ﬂow’s
progress depends on its rates at both source and destination. We
show that these coupled constraints make permutation schedules
– scheduling coﬂows one after another without interleaving their
ﬂows – suboptimal. Consequently, centralized scheduling becomes
impractical, because the scheduler might need to preempt ﬂows or
recalculate their rates at arbitrary points in time even when no new
ﬂows start or complete.
In this paper, we study the inter-coﬂow scheduling problem
for arbitrary coﬂows and focus on two objectives: improving
application-level performance by minimizing CCTs and guarantee-
ing predictable completions within coﬂow deadlines. We prove this
problem to be strongly NP-hard for either objective and focus on
developing effective heuristics. We propose a coﬂow scheduling
heuristic that – together with a complementary ﬂow-level rate allo-
cation algorithm – makes centralized coﬂow scheduling feasible by
rescheduling only on coﬂow arrivals and completions.
In the presence of coupled constraints, the bottleneck end-
points of a coﬂow determine its completion time. We propose the
Smallest-Effective-Bottleneck-First (SEBF) heuristic that greedily
schedules a coﬂow based on its bottleneck’s completion time. We
then use the Minimum-Allocation-for-Desired-Duration (MADD)
algorithm to allocate rates to its individual ﬂows. The key idea be-
hind MADD is to slow down all the ﬂows in a coﬂow to match the
completion time of the ﬂow that will take the longest to ﬁnish. As a
result, other coexisting coﬂows can make progress and the average
CCT decreases. While the combination of SEBF and MADD is not
necessarily optimal, we have found it to work well in practice.
For guaranteed coﬂow completions, we use admission control;
i.e., we do not admit any coﬂow that cannot meet its deadline with-
out violating someone else’s. Once admitted, we use MADD to
complete all the ﬂows of a coﬂow exactly at the coﬂow deadline for
guaranteed completion using the minimum amount of bandwidth.
We have implemented the proposed algorithms in a system
called Varys1 (§6), which provides a simple API that allows data-
parallel frameworks to express their communication requirements
as coﬂows with minimal changes to the framework. User-written
jobs can take advantage of coﬂows without any modiﬁcations.
We deployed Varys on a 100-machine EC2 cluster and evalu-
ated it (§7) by replaying production traces from Facebook. Varys
improved CCTs both on average (up to 3.16×) and at high per-
centiles (3.84× at the 95th percentile) in comparison to per-ﬂow
fair sharing. Hence, end-to-end completion times of jobs, specially
the communication-heavy ones, decreased. The aggregate network
utilization remained the same, and there was no starvation. In trace-
driven simulations, we found Varys to be 3.66× better than fair
sharing, 5.53× better than per-ﬂow prioritization, and 5.65× bet-
ter than FIFO schedulers Moreover, in EC2 experiments (simula-
tions), Varys allowed up to 2× (1.44×) more coﬂows to meet their
deadlines in comparison to per-ﬂow schemes; it marginally outper-
formed resource reservation mechanisms [11] as well.
We discuss current limitations of Varys and relevant future re-
search in Section 8 and compare Varys to related work in Section 9.
2 Background and Motivation
This section overviews the coﬂow abstraction (§2.1) and our con-
ceptual model of the datacenter fabric (§2.2), and illustrates the
advantages of using coﬂows (§2.3).
2.1 The Coﬂow Abstraction
A coﬂow [16] is a collection of ﬂows that share a common per-
formance goal, e.g., minimizing the completion time of the latest
ﬂow or ensuring that ﬂows meet a common deadline. We assume
that the amount of data each ﬂow needs to transfer is known before
it starts [8, 15, 19, 25]. The ﬂows of a coﬂow are independent in
that the input of a ﬂow does not depend on the output of another in
the same coﬂow, and the endpoints of these ﬂows can be in one or
more machines. Examples of coﬂows include the shufﬂe between
the mappers and the reducers in MapReduce [18] and the commu-
nication stage in the bulk-synchronous parallel (BSP) model [37].
Coﬂows can express most communication patterns between suc-
cessive computation stages of data-parallel applications (Table 1)
[16]. Note that traditional point-to-point communication is still a
coﬂow with a single ﬂow.
2.2 Network Model
In our analysis, we consider a network model where the entire
datacenter fabric is abstracted out as one non-blocking switch
[8, 11, 27, 33] interconnecting all the machines (Figure 1), and we
focus only on its ingress and egress ports (e.g., machine NICs).
This abstraction is attractive because of its simplicity, yet it is prac-
tical because of recent advances in full bisection bandwidth topolo-
gies [22, 32] and techniques for enforcing edge constraints into the
network [11, 21]. Note that we use this abstraction to simplify our
analysis, but we do not enforce it in our experiments (§7).
In this model, each ingress port has some ﬂows from one or more
coﬂows to various egress ports. For ease of exposition, we organize
them in Virtual Output Queues [31] at the ingress ports as shown
1Pronounced \'vä-ris\.
Communication in Data-Parallel Apps
Comm. in dataﬂow pipelines [3, 4, 26, 41]
Global communication barriers [29, 37]
Broadcast [3, 26, 41]
Aggregation [3, 4, 18, 26, 41]
Parallel read/write on dist. storage [13, 14, 17] Many One-to-One
Coﬂow Structure
Many-to-Many
All-to-All
One-to-Many
Many-to-One
F! F! F!
F! F!
F! F! F! F!
4F!
3F!
2F!
F!
Table 1: Coﬂows in data-parallel cluster applications.
Ingress Ports!
(Machine Uplinks)!
Egress Ports!
(Machine Downlinks)!
4!
1!
2!
2!
2!
1!
2!
3!
1!
2!
3!
DC Fabric!
Figure 1: Coﬂow scheduling over a 3 × 3 datacenter fabric with three
ingress/egress ports. Flows in ingress ports are organized by destinations
and color-coded by coﬂows – C1 in orange/light and C2 in blue/dark.
in Figure 1. In this case, there are two coﬂows C1 and C2; C1 has
three ﬂows transferring 1, 2, and 4 units of data, while C2 has two
ﬂows transferring 2 data units each.
2.3 Potential Beneﬁts of Inter-Coﬂow Scheduling
While the network cares about ﬂow-level metrics such as FCT
and per-ﬂow fairness, they can be suboptimal for minimizing the
time applications spend in communication. Instead of improving
network-level metrics that can be at odds with application-level
goals, coﬂows improve performance through application-aware
management of network resources.
Consider Figure 1. Assuming both coﬂows to arrive at the same
time, Figure 2 compares four different schedules. Per-ﬂow fairness
(Figure 2a) ensures max-min fairness among ﬂows in each link.
However, fairness among ﬂows of even the same coﬂow can in-
crease CCT [15]. WSS (Figure 2c) – the optimal algorithm in ho-
mogeneous networks – is up to 1.5× faster than per-ﬂow fairness
for individual coﬂows [15]; but for multiple coﬂows, it minimizes
the completion time across all coﬂows and increases the average
CCT. Recently proposed shortest-ﬂow-ﬁrst prioritization mecha-
nisms [8, 25] (Figure 2b) decrease average FCT, but they increase
the average CCT by interleaving ﬂows from different coﬂows. Fi-
nally, the optimal schedule (Figure 2d) minimizes the average CCT
by ﬁnishing ﬂows in the coﬂow order (C2 followed by C1). The
FIFO schedule [15, 19] would have been as good as the optimal if
C2 arrived before C1, but it could be as bad as per-ﬂow fair sharing
or WSS if C2 arrived later.
Deadline-Sensitive Communication Assume that C1 and C2
have the same deadline of 2 time units – C1 would never meet its
deadline as its minimum CCT is 4. Using per-ﬂow fairness or WSS,
both C1 and C2 miss their deadlines. Using earliest-deadline-ﬁrst
(EDF) across ﬂows [25], C2 meets its deadline only 25% of the
time. However, the optimal coﬂow schedule does not admit C1,
and C2 always succeeds.
Note that egress ports do not experience any contention in these
examples; when they do, coﬂow-aware scheduling can be even
more effective.
3 Varys Overview
Varys is a coordinated coﬂow scheduler to optimize either the per-
formance or the predictability of communication in data-intensive
applications. In this section, we present a brief overview of Varys
Flow Size @!
1!
3!
Both coﬂows end!
2!
4!
0!
2!
2!
1!
2!
C1!
C2!
4!
0!
/
W
B
C1!
P3!
C2!
P1!
P2!
Both coﬂows end!
P3!
!
s
s
e
r
g
n
I
@
W
B
/
1!
2!
C2 ends!
2!
2!
2!
Time!
C1 ends!
4!
n
I
@
W
B
/
P2!
P3!
Network Fabric!
Both coﬂows end!
4!
4!
Both coﬂows end!
Time!
P1!
2!
P2!
P3!
P1!
P2!
P3!
P1!
P2!
P3!
Both coﬂows end!
2!
Time!
4!
(a) Per-ﬂow fairness
Flow Size @!
2!
1!
3!
P1!
2!
4!
P2!
0!
2!
P3!
2!
Time!
1!
2!
4!
!
s
s
e
r
g
n
I
@
W
B
/
C1!
C2!
2!
Time!
4!
Sender!
Put!
P1!
P2!
P3!
C2 ends!
C1 ends!
4!
C2 ends!
C1 ends!
Both coﬂows end!
2!
4!
Time!
C2 ends!
P1!
P1!
(b) Per-ﬂow prioritization
P2!
P2!
C1 ends!
P3!
P3!
P1!
P2!
P3!
Time!
Time!
4!
4!
2!
2!
Receiver!
C1 ends!
Get!
C2 ends!
Varys 
Daemon!
Driver!
Reg!
Varys 
Daemon!
Varys 
Daemon!
Topology!
Monitor!
2!
Time!
Estimator!
Usage!
4!
Network Interface !
(Distributed) File System!
Coﬂow Scheduler!
Varys Master!
TaskName!
f!
Comp. Tasks calling!
Varys Client Library!
Figure 3: Varys architecture. Computation frameworks interact with Varys
through a client library.
C1 ends!
using existing techniques [17] to estimate current utilizations and
use remaining bandwidth (Rem(.)) during scheduling (§5.3).
We have implemented Varys in the application layer out of prac-
ticality – it can readily be deployed in the cloud, while providing
large improvements for both objectives we consider (§7).
Fault Tolerance Failures of Varys agents do not hamper job ex-
ecution, since data can be transferred using regular TCP ﬂows
in their absence. Varys agents store soft states that can be re-
built quickly upon restart. In case of task failures and consequent
restarts, corresponding ﬂows are restarted too; other ﬂows of the
same coﬂow, however, are not paused.
Scalability Varys reschedules only on coﬂow arrival and comple-
4!
tion events. We did not observe the typical number of concurrent
coﬂows (tens to hundreds [15, 33]) to limit its scalability. Varys
batches control messages at O(100) milliseconds intervals to re-
duce coordination overheads, which affect small coﬂows (§7.2).
Fortunately, most trafﬁc in data-intensive clusters are from large
coﬂows (§4). Hence, we do not use Varys for coﬂows with bottle-
necks smaller than 25 MB in size.
4 Coﬂows in Production
The impact of coﬂows on job durations and their network foot-
print have received extensive attention [15, 17, 33, 35, 39]. We fo-
cus on understanding their structural characteristics by analyzing
traces from a 3000-machine, 150-rack Hive/MapReduce data ware-
house at Facebook [17]. We highlight two attributes – wide vari-
ety in coﬂow structures and disproportionate footprint of few large
coﬂows – that motivate and guide Varys’s design.
4.1 Diversity of Coﬂow Structures
Because a coﬂow consists of multiple parallel ﬂows, it cannot be
characterized just by its size. We deﬁne the length of a coﬂow to
be the size of its largest ﬂow in bytes, its width to be the number
of parallel ﬂows, and its size to be the sum of all its ﬂows in bytes.
Furthermore, we consider skew, i.e., the coefﬁcient of variation of
its ﬂows in terms of their size.
The key takeaway from the CDF plots in Figure 4 is that coﬂows
vary widely in all four characteristics. We observe that while
more than 40% coﬂows are short (≤1 MB in length), ﬂows in
some coﬂows can be very large. Similarly, more than 60% nar-
row coﬂows (with at most 50 ﬂows) reside with coﬂows that con-
sist of millions of ﬂows. Furthermore, we see variations of ﬂow