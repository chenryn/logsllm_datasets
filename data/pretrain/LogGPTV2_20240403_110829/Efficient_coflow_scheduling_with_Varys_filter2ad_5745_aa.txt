# Efficient Coflow Scheduling with Varys

## Authors
- Mosharaf Chowdhury, UC Berkeley
- Yuan Zhong, Columbia University
- Ion Stoica, UC Berkeley

### Abstract
Data-parallel applications often involve a collection of parallel flows. Traditional techniques for optimizing flow-level metrics do not perform well in such scenarios because the network is largely agnostic to application-level requirements. The recently proposed coflow abstraction bridges this gap and creates new opportunities for network scheduling. In this paper, we address inter-coflow scheduling for two objectives: reducing communication time for data-intensive jobs and ensuring predictable communication times. We introduce the concurrent open shop scheduling problem with coupled resources, analyze its complexity, and propose effective heuristics to optimize either objective. We present Varys, a system that enables data-intensive frameworks to use coflows and the proposed algorithms while maintaining high network utilization and ensuring starvation freedom. EC2 deployments and trace-driven simulations show that communication stages complete up to 3.16× faster on average and up to 2× more coflows meet their deadlines using Varys compared to per-flow mechanisms. Moreover, Varys outperforms non-preemptive coflow schedulers by more than 5×.

### Categories and Subject Descriptors
C.2 [Computer-communication networks]: Distributed systems—Cloud computing

### Keywords
Coflow, data-intensive applications, datacenter networks

## 1. Introduction
Many data-intensive jobs are network-bound, yet network-level optimizations remain agnostic to job-specific communication requirements. This mismatch often degrades application-level performance, even when network-oriented metrics like flow completion time (FCT) or fairness improve. Despite differences among data-intensive frameworks, their communication is structured and occurs between groups of machines in successive computation stages. Often, a communication stage cannot finish until all its flows have completed. The recently proposed coflow abstraction represents such collections of parallel flows, conveying job-specific communication requirements to the network and enabling application-aware network scheduling. Optimizing a coflow’s completion time (CCT) reduces the corresponding job's completion time.

However, multiple coflows from one or more frameworks share a cluster. Production traces show wide variations in coflow characteristics, including total size, number of parallel flows, and individual flow sizes. Simple scheduling mechanisms like FIFO and its variants, which are attractive for decentralization, perform poorly in such environments—one large coflow can slow down many smaller ones or result in missed deadlines. Applying shortest- or smallest-first heuristics, the predominant approach for most scheduling problems, is also insufficient. Inter-coflow scheduling differs from individual flow scheduling and related problems like parallel task scheduling or caching parallel blocks, as each coflow involves multiple parallel flows, and the network involves coupled resources where each flow's progress depends on rates at both source and destination.

In this paper, we study the inter-coflow scheduling problem for arbitrary coflows, focusing on two objectives: improving application-level performance by minimizing CCTs and guaranteeing predictable completions within coflow deadlines. We prove this problem to be strongly NP-hard for either objective and develop effective heuristics. We propose a coflow scheduling heuristic that, together with a complementary flow-level rate allocation algorithm, makes centralized coflow scheduling feasible by rescheduling only on coflow arrivals and completions.

In the presence of coupled constraints, the bottleneck endpoints of a coflow determine its completion time. We propose the Smallest-Effective-Bottleneck-First (SEBF) heuristic, which greedily schedules a coflow based on its bottleneck’s completion time. We then use the Minimum-Allocation-for-Desired-Duration (MADD) algorithm to allocate rates to individual flows. The key idea behind MADD is to slow down all flows in a coflow to match the completion time of the longest flow, allowing other coexisting coflows to make progress and reducing the average CCT. While the combination of SEBF and MADD is not necessarily optimal, it works well in practice.

For guaranteed coflow completions, we use admission control, admitting only coflows that can meet their deadlines without violating others. Once admitted, we use MADD to complete all flows of a coflow exactly at the coflow deadline using the minimum amount of bandwidth.

We implemented the proposed algorithms in a system called Varys, which provides a simple API that allows data-parallel frameworks to express their communication requirements as coflows with minimal changes. User-written jobs can take advantage of coflows without modifications. We deployed Varys on a 100-machine EC2 cluster and evaluated it by replaying production traces from Facebook. Varys improved CCTs both on average (up to 3.16×) and at high percentiles (3.84× at the 95th percentile) compared to per-flow fair sharing. The aggregate network utilization remained the same, and there was no starvation. In trace-driven simulations, Varys outperformed fair sharing, per-flow prioritization, and FIFO schedulers. Additionally, Varys allowed up to 2× more coflows to meet their deadlines compared to per-flow schemes and marginally outperformed resource reservation mechanisms.

We discuss current limitations of Varys and relevant future research in Section 8 and compare Varys to related work in Section 9.

## 2. Background and Motivation
This section overviews the coflow abstraction (§2.1), our conceptual model of the datacenter fabric (§2.2), and the advantages of using coflows (§2.3).

### 2.1 The Coflow Abstraction
A coflow is a collection of flows that share a common performance goal, such as minimizing the completion time of the latest flow or ensuring that flows meet a common deadline. We assume the amount of data each flow needs to transfer is known before it starts. The flows of a coflow are independent, and their endpoints can be in one or more machines. Examples of coflows include the shuffle between mappers and reducers in MapReduce and the communication stage in the bulk-synchronous parallel (BSP) model. Coflows can express most communication patterns between successive computation stages of data-parallel applications.

### 2.2 Network Model
In our analysis, we consider a network model where the entire datacenter fabric is abstracted as a non-blocking switch interconnecting all machines, focusing on ingress and egress ports. This abstraction simplifies our analysis but is practical due to recent advances in full bisection bandwidth topologies and techniques for enforcing edge constraints. Each ingress port has flows from one or more coflows to various egress ports, organized in Virtual Output Queues.

### 2.3 Potential Benefits of Inter-Coflow Scheduling
While the network cares about flow-level metrics like FCT and per-flow fairness, these can be suboptimal for minimizing application-level communication time. Coflows improve performance through application-aware management of network resources. For example, in a 3 × 3 datacenter fabric, different schedules (per-flow fairness, per-flow prioritization, WSS, and optimal) show varying CCTs. The optimal schedule minimizes the average CCT by finishing flows in coflow order. Deadline-sensitive communication benefits from coflow-aware scheduling, as it can ensure that coflows meet their deadlines.

## 3. Varys Overview
Varys is a coordinated coflow scheduler to optimize either the performance or predictability of communication in data-intensive applications. Varys uses existing techniques to estimate current utilizations and remaining bandwidth during scheduling. We implemented Varys in the application layer, making it deployable in the cloud while providing significant improvements for both objectives. Varys agents store soft states that can be rebuilt quickly upon restart, and task failures do not pause other flows of the same coflow. Varys reschedules only on coflow arrival and completion events, and batches control messages to reduce coordination overheads.

## 4. Coflows in Production
The impact of coflows on job durations and network footprint has been extensively studied. We focus on understanding their structural characteristics by analyzing traces from a 3000-machine, 150-rack Hive/MapReduce data warehouse at Facebook. Two attributes—wide variety in coflow structures and disproportionate footprint of few large coflows—motivate and guide Varys’s design.

### 4.1 Diversity of Coflow Structures
Coflows vary widely in length, width, size, and skew. More than 40% of coflows are short (≤1 MB in length), but some flows can be very large. Similarly, more than 60% of narrow coflows (with at most 50 flows) coexist with coflows consisting of millions of flows. These variations highlight the need for flexible and efficient coflow scheduling.

---

This version of the text is more structured, coherent, and professional, with clear headings and a logical flow of ideas.