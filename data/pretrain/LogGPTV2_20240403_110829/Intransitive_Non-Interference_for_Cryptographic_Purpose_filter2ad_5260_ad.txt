non-interference notion.
Deﬁnition 4.1 (Downgraded information) Let a struc-
ture ( ˆM , S ) and a subset H ⊆ S of its speciﬁed ports
be given. Let r be a run of a conﬁguration of ( ˆM , S ).
Then the downgraded information at ports H in r, writ-
ten downH (r), is deﬁned by further restricting r(cid:5)H to
input ports, and to inputs that are pairs (L, m) of the
constant L and an arbitrary message m.
✸
probabilistic
Deﬁnition 4.2 (Intransitive
non-
interference with downgrading) Let a ﬂow policy
G = (∆,E) with ∆( ˆM ,S ) := {Si | i = 1, . . . , n} ∪ {¯S}
for a structure ( ˆM , S ) be given, and let one port set
H = Si be given as that of the trusted user H. Then
( ˆM , S ) fulﬁlls the policy G if the following holds
for all H, L with H (cid:10)❀ L, and all non-interference
conﬁgurations conf n in
H,L,I( ˆM , S ): If
P (b = b
∗
|
;
H,L ,k
H,L ∈ Confn in
r ← run conf n in
b := r(cid:5)pH bit!;
∗ := r(cid:5)p∗
b
L bit
?) ≥ 1
2
+ ns(k)
with ns(k) (cid:10)∈ SMALL, then there exists a distinguisher
D and a function ns(cid:4)(k) (cid:10)∈ SMALL such that
P (b = b
|
(cid:4)
;
H,L ,k
r ← run conf n in
b := r(cid:5)pH bit!;
(cid:4) ← D(downH (r), 1k)) ≥ 1
b
2
+ ns(cid:4)(k).
For the computational case, the distinguisher has to be
polynomial-time.
✸
The deﬁnition can be extended to predicates on ns, ns(cid:4)
like Deﬁnition 3.4.
5 Intransitive Non-Interference and Simu-
latability
In this section we investigate how intransitive non-
interference behaves under simulatability. In a nutshell,
we show that the relation “at least as secure as”, which
is the cryptographic analog of secure implementation,
does not destroy the properties from our deﬁnitions. As
deﬁning a cryptographic system usually starts with an
abstract speciﬁcation of what the system should do, pos-
sible implementations have to be proven to be at least
as secure as this speciﬁcation. The abstract speciﬁcation
usually consists of a monolithic idealized machine con-
taining neither cryptographic details nor probabilism.
Thus, its properties can typically be validated quite eas-
ily by formal proof systems, e.g., formal theorem prov-
ing or even automatic model checking. Hence it would
simplify life a lot if these properties would automatically
carry over to the real system. This is already known
for transitive non-interference [2]. In the following, we
show that it is also true for recognition non-interference
for trusted recipients (Deﬁnition 3.6). A similar preser-
vation theorem also holds for the remaining deﬁnitions,
but we omit the proofs due to space constraints.
Simulatability essentially means that whatever might
happen to honest users U of the real structure ( ˆM1, S )
can also happen to the same honest users with the ideal
structure ( ˆM2, S ), i.e., with the abstract speciﬁcation.
The structures have identical sets of speciﬁed ports.
Formally speaking, for every conﬁguration conf 1 of
( ˆM1, S ) there is a conﬁguration conf 2 of ( ˆM2, S ) with
the same users yielding indistinguishable views of U in
both systems [30]. This is written ( ˆM1, S ) ≥sec ( ˆM2, S )
(spoken “the real structure is at least as secure as the
ideal structure”). The indistinguishability of the views
of U is denoted by view conf 1
(U ) ≈ view conf 2
(U ).
The following theorem states that an intransitive non-
interference property is in fact preserved under the rela-
tion “at least as secure as”.
Theorem 5.1 (Preservation
of Recognition Non-
Interference for Trusted Recipients) Let a ﬂow policy
G = (∆,E) for a structure ( ˆM2, S ) be given, such
that this structure fulﬁlls Deﬁnition 3.6 for this policy.
Furthermore, let a structure ( ˆM1, S ) ≥sec ( ˆM2, S ) be
given. Then ( ˆM1, S ) also fulﬁlls Deﬁnition 3.6 for the
ﬂow policy G. This holds for the perfect, statistical, and
computational case.
Proof. First, G = (∆,E) is a well-deﬁned ﬂow pol-
icy also for ( ˆM1, S ) because the two structures have the
same set of speciﬁed ports. (This was the reason to de-
ﬁne the partition ∆ of all free ports via a partition Γ of
the speciﬁed ports.)
✷
Proceedings of the 2003 IEEE Symposium on Security and Privacy (SP(cid:146)03) 
1081-6011/03 $17.00 ' 2003 IEEE 
1
H,L,1
H,L,2
We now show that ( ˆM1, S ) fulﬁlls recognition non-
interference for trusted recipients for the policy G =
(∆,E).
Let arbitrary users H, L, a cut ˆC for H, L, and a
H,L,1 = ( ˆM1, S ,
non-interference conﬁguration conf n in
{H, L, T1, . . . , Tn}, A) ∈ Confn in
H,L,I( ˆM1, S ) be given.
Because of ( ˆM1, S ) ≥sec ( ˆM2, S ), there exists a con-
ﬁguration conf H,L,2 = ( ˆM2, S ,{H, L, T1, . . . , Tn}, A(cid:4))
≈
that
such
view conf H,L,2
the
set U := {H, L, T1, . . . , Tn} of users remains un-
changed in simulatability. It is easy to see that conf H,L,2
is again a non-interference conﬁguration by inspection
of the precise deﬁnition in [2]. Hence, we call it
conf n in
({H, L, T1, . . . , Tn})
that
view conf n in
({H, L, T1, . . . , Tn}).
Recall
H,L,2 in the following.
H,L,1
H,L,1,k
(U ), view conf n in
Because of view conf n in
(U ) ≈ view conf n in
For the statistical case, assume that L outputs the
2 + ns 1(k) for
bit b correctly with probability at least 1
ns 1 (cid:10)∈ SMALL in the conﬁguration conf n in
H,L,1. We have
to show that there exists a distinguisher D1 that can out-
put the bit b with a similar probability, given the view of
the users in the cut ˆC and the random tapes of T ˆC with
T ˆC being the set of users that lie in the same component
of the graph as L after applying the cut ˆC .
(U ),
there also exists ns 2 (cid:10)∈ SMALL such that the prob-
ability of a correct guess of L in conf n in
H,L,2 is at least
2 +ns 2(k). This follows with the deﬁnition of statistical
indistinguishability, which states that the statistical dis-
tance δ := |∆(view conf n in
(U ))|
is in SMALL. The predicate b = b
is a function of
such a view of U , and by a well-known lemma the sta-
tistical distance between a function of two random vari-
ables is bounded by the statistical distance between the
random variables themselves (see [2]), i.e., here by δ.
Now assume for contradiction that the probability of a
2 + s1(k) for
correct guess of L in conf n in
s1 ∈ SMALL. This would imply s1 + δ ≥ ns 2, in
contradiction to the fact that SMALL is closed under
addition and under making functions smaller.
As ( ˆM2, S ) fulﬁlls G by precondition, this result
about L’s guessing ability in the second structure implies
that there exists a distinguisher D2 that, given the view
( ˆC ), and the ran-
of the users in the cut, view conf n in
dom tapes of T ˆC , can output the bit b with probability at
2 (cid:10)∈ SMALL. Now we deﬁne
least 1
the distinguisher for the ﬁrst structure to be D1 := D2.
This is possible because the input of D2 only contains in-
formation collected by the users (their views and random
tapes), whose types are unchanged in simulatability. By
the same indistinguishability argument as above we see
( ˆC ) and the random tape
that D1, given view conf n in
of T ˆC , can also output the bit b with probability at least
H,L,2 is at most 1
2(k) with ns(cid:4)
2 + ns(cid:4)
∗
H,L,2,k
H,L,2,k
H,L,1,k
1
The proof
for
1(k) with ns(cid:4)
1 (cid:10)∈ SMALL. This ﬁnishes the
2 + ns(cid:4)
proof for the statistical case.
the perfect case follows with
SMALL := {0}. For the computational case we use
SMALL := NEGL. Here the contradiction proofs
showing that if the guessing probabilities of L and
D := D1 = D2, respectively, were signiﬁcantly differ-
ent in the real and ideal structure, the structures would
be distinguishable, is a bit different: One deﬁnes a
polynomial-time distinguishing machine D∗
(not to be
confused with D) in a standard way: On input a view
of the users, it computes what L or D would guess and
whether this is correct, and accordingly guesses whether
it has got a view from the real or the ideal structure. This
guess is correct with probability signiﬁcantly greater
2 by the closure properties of the class SMALL.
than 1
The omitted proofs of the corresponding preserva-
tion theorems for recognition non-interference (without
trusted users) and for downgrading are completely anal-
ogous, and that of blocking non-interference is even eas-
ier, with only one usage of indistinguishability.
6 Security of
Implementations with a
Cryptographic Firewall
To illustrate the usefulness of the preservation theo-
rem of the previous section, we now examine the realiza-
tion of intransitive ﬂow policies by cryptographic ﬁre-
walls, which we mentioned in several examples above.
A precise description of a ﬁrewall as such, i.e., a cryp-
tographic realization, was given in [2]; only the ﬁnal ﬁl-
tering rules were speciﬁc for transitive policies. Hence
we only give an informal description here, and only for
the secretary example.
The real ﬁrewall is a structure ( ˆM , S ) with ˆM :=
{Mi | i ∈ {c, s, g}} and where S contains two ports for
each machine Mi for inputs from and outputs to user I.
A machine Mb for the bad customers would also belong
to the intended structure, but as one cannot trust all bad
customers to use correct machines, the actual structure
joins that machine with the adversary machine A. This
is sketched in Figure 7.
The machines Mi perform secure message transmis-
sion, i.e., on input (send, m, j) from their respective
user, they sign the message with their secret signature
key along with the identity of the sender and the recipi-
ent, encrypt it with the public key of j, and send it to ma-
chine Mj over an insecure network. When Mj receives a
message from the network, it decrypts it and veriﬁes the
signature. If this fails, it discards the message, else it ap-
plies its ﬁltering rules. Here Mc only accepts messages
Proceedings of the 2003 IEEE Symposium on Security and Privacy (SP(cid:146)03) 
1081-6011/03 $17.00 ' 2003 IEEE 
b'
D
rand
view
S
M
s
b*
C
M
c
b
B
M
?
b
A
Figure 7. Sketch of the ﬁrewall system for
the secretary example. The bar in ma-
chine Mc denotes the ﬁltering function.
The good customers G and their machine
Mg are connected like S and Ms. A correct
machine Mb may or may not be present.
from Ms and Mg, i.e., outputs them to its user C, while
Ms and Mg accept messages from all machines. Note
that the correctness of this test depends on the preceding
signature test.
We want to prove that the structure ( ˆM , S ) fulﬁlls
the policy from Figure 2 in the computational case ac-
cording to Deﬁnition 3.6. As a proof for the real struc-
ture ( ˆM1, S ) would be quite complicated and error-
prone, we apply the preservation theorem to get rid of
cryptography-related details. In [2], an abstract speci-
ﬁcation ( ˆM2, S ) of the cryptographic ﬁrewall was pre-
sented and it was proved that the real structure is as se-
cure as the ideal structure. Hence we can use this for
our purposes. Roughly, instead of sending encrypted
and signed messages over an insecure network, the ab-
stract ﬁrewall just stores the messages in an internal ar-
ray and delivers them at the appropriate scheduling re-
quests. The internal test whether a message to C should