# Pinpoint: Problem Determination in Large, Dynamic Internet Services

**Authors:**  
Mike Y. Chen, Emre Kıcıman\*, Eugene Fratkin\*, Armando Fox\*, Eric Brewer  
*Computer Science Division, University of California, Berkeley*  
\* *Computer Science Department, Stanford University*  
*Contact: mchen, ekiciman, efratkin, afox, ebrewer*

## Abstract
Traditional problem determination techniques rely on static dependency models, which are challenging to generate accurately in today's large, distributed, and dynamic application environments, such as e-commerce systems. This paper introduces a dynamic analysis methodology that automates problem determination by 1) tagging numerous real client requests as they traverse the system and 2) using data mining techniques to correlate the success and failure of these requests to identify the most likely faulty components. To validate our methodology, we have implemented Pinpoint, a framework for root cause analysis on the J2EE platform that requires no prior knowledge of the application components. Pinpoint consists of three main parts: a communications layer for tracing client requests, a failure detector that uses traffic-sniffing and middleware instrumentation, and a data analysis engine. We evaluate Pinpoint by injecting faults into various application components and demonstrate that it accurately identifies faulty components with a low rate of false positives.

**Keywords:** Problem Determination, Problem Diagnosis, Root Cause Analysis, Data Clustering, Data Mining Algorithms

## 1. Introduction
Modern Internet services are expected to operate 24/7/365. Given the scale and rapid changes in these services, this is a significant challenge. Understanding how any given client request is being processed within a service is already difficult; determining the root cause of a failure is even more complex.

Internet services are large and dynamic systems. As new functionalities are added and components are replicated for performance and fault tolerance, the complexity of these systems increases. Additionally, as services become more dynamic, such as providing personalized interfaces and functionality, the way client requests are handled becomes increasingly varied. With the introduction of Internet-wide service frameworks like Microsoft’s .NET [20] and Hewlett-Packard’s E-Speak [15], the size and dynamics of typical Internet services will continue to grow.

A typical Internet service comprises many components across multiple tiers, including front-end load balancers, web servers, application components, and backend databases, as well as numerous (replicated) subcomponents within each tier [22]. Client requests are dynamically routed through this system. Current Internet services, such as Hotmail [8] and Google [7], are hosted on thousands of servers and continue to expand. The large size and dynamic behavior of these systems increase their complexity and the potential for failures due to unanticipated interactions among components. Few services achieve availability over 99.9% in a real-world operating environment.

### 1.1. Background
This paper focuses on problem determination: detecting system problems and isolating their root causes. Current root cause analysis techniques often fail to capture the dynamic complexity of large systems and require extensive manual input about the system [24, 4]. Most techniques, including event correlation systems, are based on static dependency models that describe the relationships among hardware and software components. These models are used to determine which components might be responsible for a given problem [5, 25, 6, 13].

The first major limitation of traditional dependency models is the difficulty in generating and maintaining an accurate model of a constantly evolving Internet service. The second major limitation is that they typically only model a logical system and do not distinguish among replicated components. However, large Internet services have thousands of replicated components, necessitating the ability to distinguish among them to find the specific instance at fault.

### 1.2. A Data Clustering Approach
We propose a new approach to problem determination that better handles large and dynamic systems by:
1. Dynamically tracing real client requests through the system. For each request, we record its success or failure and the set of components used to service it.
2. Using data clustering and statistical techniques to correlate the failures of requests with the components most likely to have caused them.

Tracing real requests through the system enables us to support problem determination in dynamic systems where using dependency models is not feasible. This tracing also allows us to distinguish between multiple instances of what would be a single logical component in a dependency model.

By performing data clustering to analyze the successes and failures of requests, we aim to find the combinations of components most highly correlated with request failures. By analyzing the components used in failed requests but not in successful ones, we achieve high accuracy with a relatively low number of false positives. This approach detects individual faulty components and faults due to interactions among multiple components. It is well-suited for large and dynamic Internet services because:
- Live tracing of client requests allows us to analyze both the logical and physical behavior of a system. Tracing does not require human intervention to adapt to system changes, making Pinpoint scalable for constantly evolving Internet services.
- Data clustering techniques allow us to quickly summarize the large amount of collected traces and correlate them with believed failures. The automated nature of Pinpoint means it does not require extra effort from service developers and operators to run on large services.

The Pinpoint approach makes two key assumptions about the system being measured. First, the system’s normal workload must exercise the available components in different combinations. Second, our data clustering approach assumes that requests fail independently and not due to the activities of other requests. These assumptions are generally valid in today’s large and dynamic Internet services. Service requests tend to be independent due to the nature of HTTP, and the highly replicated nature of Internet service clusters allows components to be recombined in many ways to avoid single points of failure.

We have implemented our approach in a prototype system called Pinpoint and used it to identify root causes in a prototype e-commerce environment based on the Java 2 Platform Enterprise Edition (J2EE) demonstration application, PetStore [21]. We used a workload that mimics the request distribution of the TPC web e-commerce ordering benchmark (TPC-WIPSo) [2]. We instrumented the J2EE server platform to trace client requests at every component and had a fault-injection layer to inject four types of faults. The results demonstrate the effectiveness of our approach. We were able to automatically identify the root causes of single-component failures 80-90% of the time with an average false positive rate of 40-50%, without any prior knowledge of the components and requests. This false positive rate is better than other common approaches that achieve similar accuracies.

The contributions of this paper are:
1. A dynamic analysis approach to problem determination that works effectively.
2. A framework that separates fault detection and problem determination from application-level components.

The rest of this paper describes our approach to automating problem determination and the experimental validation of this work. Sections 2 and 3 present a detailed design and implementation of the Pinpoint framework. Section 4 describes our experimental validation. Section 5 discusses the limitations of Pinpoint and related work, and future directions. We conclude in Section 6.

## 2. The Pinpoint Framework
To validate our data clustering approach to problem determination, we designed and implemented Pinpoint, a framework for problem determination in Internet service environments. Our framework, shown in Figure 1, provides three major pieces of functionality to aid developers and administrators in determining the root cause of failures:

- **Client Request Traces:** By instrumenting the middleware and communications layer between components, Pinpoint dynamically tracks which components are used to satisfy each individual client request.
- **Failure Detection:** Pinpoint provides both internal and external monitoring of a system to detect whether client requests are succeeding or failing. Internal fault-detection is used to detect assertion failures and exceptions, including errors that are later masked by the system. External fault-detection is used to detect end-to-end failures not otherwise detectable.
- **Data Clustering Analysis:** Pinpoint combines the data from tracking client requests with success and failure data for each request and feeds it into a data analysis engine to discover faulty components and component interactions.

### 2.1. Client Request Tracing
As a client request travels through the system, we record all the components it uses at various granularities. At a coarse granularity, we are interested in the machines and, depending on the size of the service, the clusters being used. At a finer granularity, we log individual software components, component versions, and, if practical, even individual data files (such as database tables and versions of configuration files). Our goal is to capture as much information about possible differentiating factors between successful and failed requests as is practical.

When a client request first arrives at the service, the request tracing subsystem assigns the request a unique ID and tracks it as it travels through the system. To avoid adding complexity and excessive load to the components being traced, the tracing subsystem generates simple log outputs in the form of <request ID, component ID> pairs. This information is collated into complete lists of all components each request touched.

By modifying the middleware beneath the application components, we can record the ID of every request that arrives at a specific component without having any knowledge of the applications and without modifying the components. When an application component makes a nested call to another component, the middleware records that another component is about to be used and forwards the request ID to the next component along with the call data. The changes required to implement this subsystem can often be restricted to the middleware software alone, thus avoiding modifications to application-level components. Whether this is possible depends on the specific middleware framework used and the details of the inter-component communication protocol.

### 2.2. Failure Detection
While the tracing subsystem is recording the components used by client requests, an orthogonal subsystem attempts to detect whether these requests are successfully completing. Although it is not possible to detect all failures, some are more easily noticeable from either inside or outside the service. Therefore, our framework allows for both internal and external failure detection.

**Internal Failure Detection:** Used to detect errors that might be masked before becoming visible to users. For example, a frontend failure that gets replaced by a hot swap may have no externally visible effects. Though these failures do not become visible to the users, system administrators should still track these errors to repair the systems. Internal failure detectors can also modify the middleware to track assertions and exceptions generated by application components.

**External Failure Detection:** Used to detect faults that will be visible to the user, such as complete service failures (e.g., network outages or machine crashes). External detection can also identify application-specific errors that generate user-visible messages.

Whenever a failure or success is detected, the detection subsystem logs this along with the ID of the client request. To be consistent with the logs of the client tracing subsystem, the two subsystems must either pass client request IDs between each other or use deterministic algorithms for generating request IDs based on the client request itself.

### 2.3. Data Analysis
Once the request traces and failure/success logs are available, they are fed into Pinpoint’s analysis subsystem. A sample input is shown in Table 1. The data analysis uses a data clustering algorithm to discover sets of components that are highly correlated with request failures.

Clustering algorithms structure data by grouping similar data points together. In our analysis, we calculate similarity based on how often components are and are not used together in requests. The details of the clustering algorithms used in our implementation are presented in Section 3.3.

Before running the clustering analysis, we prepare the data. During the logging stage, our requests are indexed by request ID, with each request ID matched to the components used in that request. We transpose this data for the clustering process and index by component, matching each to the requests it was used in. We also add a binary column indicating whether the request was successful or not. This transformed data is then fed into the clustering algorithm to identify the components most likely to be at fault.

**Table 1.** A sample input matrix for data analysis

| Client Request ID | Failure | Component A | Component B | Component C |
|-------------------|---------|-------------|-------------|-------------|
| 1                 | 0       | 1           | 1           | 0           |
| 2                 | 1       | 0           | 1           | 1           |
| 3                 | 1       | 0           | 0           | 0           |
| 4                 | 0       | 1           | 0           | 1           |

---

**Proceedings of the International Conference on Dependable Systems and Networks (DSN’02)**  
**0-7695-1597-5/02 $17.00 © 2002 IEEE**  
**Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:20:30 UTC from IEEE Xplore. Restrictions apply.**