title:Pinpoint: Problem Determination in Large, Dynamic Internet Services
author:Mike Y. Chen and
Emre Kiciman and
Eugene Fratkin and
Armando Fox and
Eric A. Brewer
Pinpoint: Problem Determination in Large, Dynamic Internet Services
Mike Y. Chen, Emre Kıcıman*, Eugene Fratkin*, Armando Fox*, Eric Brewer
Computer Science Division, University of California, Berkeley
*Computer Science Department, Stanford University
fmikechen, PI:EMAIL, femrek, fratkin, PI:EMAIL
Abstract
Traditional problem determination techniques rely on
static dependency models that are difﬁcult to generate ac-
curately in today’s large, distributed, and dynamic appli-
cation environments such as e-commerce systems. In this
paper, we present a dynamic analysis methodology that au-
tomates problem determination in these environments by 1)
coarse-grained tagging of numerous real client requests as
they travel through the system and 2) using data mining
techniques to correlate the believed failures and successes
of these requests to determine which components are most
likely to be at fault. To validate our methodology, we have
implemented Pinpoint, a framework for root cause analy-
sis on the J2EE platform that requires no knowledge of the
application components. Pinpoint consists of three parts:
a communications layer that traces client requests, a fail-
ure detector that uses trafﬁc-snifﬁng and middleware in-
strumentation, and a data analysis engine. We evaluate
Pinpoint by injecting faults into various application com-
ponents and show that Pinpoint identiﬁes the faulty compo-
nents with high accuracy and produces few false-positives.
Keywords: Problem Determination, Problem Diagnosis,
Root cause Analysis, Data Clustering, Data Mining Algo-
rithms
1. Introduction
Today’s Internet services are expected to be running
24x7x365. Given the scale and rate of change of these ser-
vices, this is no easy task. Understanding how any given
client request is being fulﬁlled within a service is difﬁcult
enough; understanding why a particular client request is
not working—determining the root cause of a failure—is
an even greater challenge.
Internet services are very large and dynamic systems.
The number of software and hardware components in these
systems increases as new functionalities are added and as
components are replicated for performance and fault toler-
ance, often increasing the complexity of the system. Ad-
ditionally, as services become more dynamic, e.g., to pro-
vide personalized interfaces and functionality, the way that
client requests are serviced becomes more and more varied.
With the introduction of Internet-wide service frameworks
encouraging programmatic interactions between distributed
systems, such as Microsoft’s .NET [20] and Hewlett-
Packard’s E-Speak [15], the size and dynamics of a typical
Internet service will only continue to increase.
Today, a typical Internet service has many components
divided among multiple tiers: front-end load balancers, web
servers, application components, and backend databases, as
well as numerous (replicated) subcomponents within each
[22]. As clients connect to these services, their requests are
dynamically routed through this system. Current Internet
services, such as Hotmail [8], a web-based email service,
and Google [7], a search engine, are already hosted on thou-
sands of servers and continue to grow.1 The large size of
these systems, together with the increase in their dynamic
behavior, means an increase in their complexity and more
potential for failures to occur due to unanticipated “interac-
tion” faults among components. That these failures actually
occur is evidenced by the fact that few services deliver avail-
ability over 99.9% in a real-world operating environment.
1.1. Background
The focus of this paper is problem determination: detect-
ing system problems and isolating their root causes. Current
root cause analysis techniques use approaches that do not
sufﬁciently capture the dynamic complexity of large sys-
tems, and they require people to input extensive knowl-
edge about the systems [24, 4]. Most root cause analysis
techniques, including event correlation systems, are based
on static dependency models describing the relationships
among the hardware and software components in the sys-
tem. These dependency models are used to determine which
components might be responsible for the symptoms of a
given problem [5, 25, 6, 13]. The ﬁrst major limitation
1Hotmail 7000+ [11], Google 8000+ [14]
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:20:30 UTC from IEEE Xplore.  Restrictions apply. 
of traditional dependency models is the difﬁculty of gen-
erating and maintaining an accurate model of a constantly
evolving Internet service. Their second major limitation is
that they typically only model a logical system, and do not
distinguish among replicated components. However, since
large Internet services have thousands of replicated compo-
nents, there is a need to distinguish among them to ﬁnd the
instance of the component that is at fault.
1.2. A Data Clustering Approach
We propose a new approach to problem determination
that better handles large and dynamic systems by:
1. Dynamically tracing real client requests through a sys-
tem. For each request, we record its believed success
or failure, and the set of components used to service it.
2. Performing data clustering and statistical techniques
to correlate the failures of requests to the components
most likely to have caused them.
Tracing real requests through the system enables us to
support problem determination in dynamic systems where
using dependency models is not possible. This tracing
also allows us to distinguish between multiple instances of
what would be a single logical component in a dependency
model.
By performing data clustering to analyze the successes
and failures of requests, we attempt to ﬁnd the combina-
tions of components that are most highly correlated with the
failures of requests, under the belief that these components
are causing the failures. By analyzing the components that
are used in the failed requests, but are not used in success-
ful requests, we provide high accuracy with relatively low
number of false positives. This analysis detects individual
faulty components, as well as faults occurring due to inter-
actions among multiple components. This approach is well
suited for large and dynamic Internet services because:
(cid:15) Live tracing of client requests allows us to analyze both
the logical and physical behavior of a system. Because
tracing does not require human intervention to adapt to
system changes, Pinpoint scales to constantly evolving
Internet services.
(cid:15) Data clustering techniques allow us to quickly sum-
marize the large amount of collected traces, and corre-
late them with believed failures. Because the Pinpoint
analysis is automated, it does not require extra effort
on the part of service developers and operators to run
on large services.
The Pinpoint approach does make two key assumptions
about the system being measured. First, the system’s nor-
mal workload must exercise the available components in
different combinations. For example, if two components
were always to be used together, a fault in one would
not be distinguishable from a fault in the other. Sec-
ondly, our data clustering approach assumes that requests
fail independently—they will not fail because of the activi-
ties of other requests. These assumptions are generally valid
in today’s large and dynamic Internet services. Service re-
quests tend to be independent of one another, due to the
nature of HTTP, and the highly replicated nature of Inter-
net service clusters allows components to be recombined in
many ways to avoid single-points of failure.
We have implemented our approach in a prototype sys-
tem called Pinpoint, and used Pinpoint to identify root
causes in a prototype e-commerce environment based on
the Java 2 Platform Enterprise Edition (J2EE) demonstra-
tion application, PetStore [21]. We use a workload that
mimics the request distribution of the TPC web e-commerce
ordering benchmark (TPC-WIPSo) [2]. We instrumented
J2EE server platform to trace client requests at every com-
ponent, and had a fault-injection layer that we used to inject
4 types of faults. The results demonstrate the power of our
approach. We were able to automatically identify the root
causes of single-component failures 80-90% of the time
with an average rate of 40-50% false positives without any
knowledge of the components and the requests. The rate of
false positives is better than other common approaches that
achieve similar accuracies.
The contributions of this paper are: 1) a dynamic analy-
sis approach to problem determination that works well and
2) a framework that enables separation of fault detection
and problem determination from application-level compo-
nents. The rest of this paper describes our approach to au-
tomating problem determination and the experimental val-
idation of this work. Sections 2 and 3 present a detailed
design and implementation of a framework, Pinpoint, that
uses our approach. Section 4 describes our experimental
validation. Section 5 discusses limitations of Pinpoint and
previous work and future work in this area. We conclude in
Section 6.
2. The Pinpoint Framework
To validate our data clustering approach to problem
determination, we designed and implemented Pinpoint, a
framework for problem determination in Internet service
environments. Our framework, shown in Figure 1, provides
three major pieces of functionality to aid developers and ad-
ministrators in determining the root cause of failures:
Client Request Traces: By instrumenting the middleware
and communications layer between components, Pin-
point dynamically tracks which components are used
to satisfy each individual client request.
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:20:30 UTC from IEEE Xplore.  Restrictions apply. 
Components
A
B
C
External
F/D
Communications Layer
(Tracing & Internal F/D)
Requests
#1
#2
#3
Fault
Log
1,Success
2, Fail
3, ...
1,A
1,C
2,B
...
Trace
Log
Statistical
Analysis
Detected
Faults
Figure 1. The Pinpoint Framework.
Failure Detection: Pinpoint provides both internal and ex-
ternal monitoring of a system to detect whether client
requests are succeeding or failing.
Internal fault-
detection is used to detect assertion failures and ex-
ceptions, including errors that are later masked by the
system. External fault-detection is used to detect end-
to-end failures not otherwise detectable.
Data Clustering Analysis: Pinpoint combines the data
from tracking client requests with success and failure
data for each request and feeds it into a data analysis
engine to discover faulty components and component
interactions.
2.1. Client Request Tracing
As a client request travels through the system, we are in-
terested in recording all the components it uses, at various
granularities. At a coarse granularity, we are interested in
the machines and, depending on the size of the service, the
clusters being used. At a ﬁner granularity, we are interested
in logging individual software components, component ver-
sions, and, if practical, even individual data ﬁles (such as
database tables, and versions of conﬁguration ﬁles). Our
goal is to capture as much information about possible dif-
ferentiating factors between successful and failed requests
as is practical.
When a client request ﬁrst arrives at the service, the re-
quest tracing subsystem is responsible for assigning the re-
quest a unique ID and tracking it as it travels through the
system. To avoid forcing extra complexity and excessive
load on the components being traced, the tracing subsystem
generates simple log outputs in the form of  pairs. This information is separately col-
lated into complete lists of all components each request
touched.
By modifying the middleware beneath the application
components we are interested in, we can record the ID of
every request that arrives at a speciﬁc component without
having any knowledge of the applications and without mod-
ifying the components. When an application component
makes a nested call to another component, the middleware
records that another component is about to be used, and
forwards the request ID to the next component along with
the call data. The changes required to implement this sub-
system can often be restricted to the middleware software
alone, thus avoiding modifying application-level compo-
nents. Whether this is possible depends on the speciﬁc mid-
dleware framework used and details of the inter-component
communication protocol.
2.2. Failure Detection
While the tracing subsystem is recording components
being used by client requests, an orthogonal subsystem is
attempting to detect whether these client requests are suc-
cessfully completing. Though it is not possible to detect all
failures that occur, some failures are more easily noticeable
from either inside or outside of the service. Therefore, our
framework allows for both internal and external failure de-
tection to be used.
Internal failure detection is used to detect errors that
might be masked before becoming visible to users. For ex-
ample, a frontend failure that gets replaced by a hot swap
may have no externally visible effects. Though these fail-
ures do not become visible to the users, system adminis-
trators should still be interested in tracking these errors to
repair the systems. Internal failure detectors also have the
option of modifying the middleware to track assertions and
exceptions being generated by application components.
External failure detection is used to detect faults that will
be visible to the user. This includes complete service fail-
ures, such as network outages or machine crashes. External
detection can also be used to identify application-speciﬁc
errors that generated user-visible messages.
Whenever a failure or success is detected, the detection
subsystem logs this along with the ID of the client request.
To be consistent with the logs of the client tracing subsys-
tem, the two subsystems must either pass client request ids
between each other, or use deterministic algorithms for gen-
erating request IDs based on the client request itself.
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:20:30 UTC from IEEE Xplore.  Restrictions apply. 
Client Request ID Failure Component A Component B Component C
1
2
3
4
0
1
1
0
1
1
0
0
0
1
1
0
0
0
0
1
Table 1. A sample input matrix for data analysis
2.3. Data Analysis
Once the request traces and failure/success logs are avail-
able, they are given to Pinpoint’s analysis subsystem. A
sample input is shown in Table 1. The data analysis uses
a data clustering algorithm to discover sets of components
that are highly correlated with failures of requests.
Clustering algorithms structure data by grouping similar
data points together. In our analysis, we calculate similarity
based on how often components are and are not used to-
gether in requests. The details of the clustering algorithms
we used in our implementation are presented in Section 3.3.
Before running this clustering analysis, we ﬁrst must pre-
pare the data. During the logging stage, our requests are in-
dexed by request ID, with each request ID matched to the
components used in that request. We transpose this data
for the clustering process and instead index by component,
matching each to the requests it was used in. We also add a