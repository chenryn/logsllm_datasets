k
c
y tr a
fr e
e
q m i n
n t
u
o
b it c
a m
s tr e
a
w
s
s
n
p ti o
Fig. 12: Graph showing how errors injected into the register
ﬁle propagate through the system.
calculations. While the register checkpoints store more state
than in our prior work [8], we have implemented this in an
area-neutral way by reducing numbers of entries stored, so this
is taken into account in our performance numbers.
D. Fault Injection Study
ParaMedic re-executes all computation by the main core on
checker cores, and so captures all observable errors that dual-
core lockstep would: the difference is that computation may
be checked in a different order from the dual-core lockstep
execution due to the exploitation of parallelism.
the end of each segment:
However, the way errors manifest may differ due to the
in dual-core
register checks at
lockstep,
these errors would instead appear the retirement
of an instruction, and in other sequential error detection
mechanisms [15], [4], [16], would either affect a future load
or store, or be masked entirely. By comparison, in ParaMedic
we must also check register ﬁles to ensure continuity between
parallel checked segments. We explore this in ﬁgure 12. Here,
we inject a random bit error in a random integer register at the
start of each checkpoint, to observe the error’s propagation. We
see that errors are split signiﬁcantly between load addresses,
store addresses and data, and register ﬁles. We also see that
up to half of all injected errors on the register ﬁle never affect
the resulting computation: they are in registers that are never
used by the application, and are overwritten by the time a
checkpoint is completed.
E. Summary
For single-threaded code without shared memory, the extra
slowdown from error correction is minimal, at
just 2.0%
compared with 1.9% for error detection. When we introduce
shared memory, mandating communication limitations that
prevent the escape of data from the L1 before it is committed
by using timestamps, this increases to 4.6%. However, it can be
reduced to 3.1% by using an additive-increase-multiplicative-
decrease (AIMD) scheme to dynamically scale the size of
timestamps, to reduce cache capacity evictions from data that
hasn’t yet been checked. When we extend this to multithreaded
code, the pattern is similar: overheads of 1% for error detection
increase to 1.5% for correction.
However,
ParaMedic rolls back speculative execution in the presence of
errors. There is, therefore, a clear comparison to be made with
transactional-memory systems [12]. Further, we can consider
the problem of error propagation as being akin to conﬂict
detection, both within-core to subsequent checkpoints, and
out-of-core when errors may propagate due to shared memory.
there are clear and necessary implementation
differences between our technique and typical hardware trans-
actional-memory (HTM) schemes. Most real-world HTM im-
plementations are best-effort [12], meaning forward progress
isn’t guaranteed. This is possible because the system can al-
ways fall back on sequential execution of transactions, whereas
there is no alternative to checking the code for correctness
when performing error correction. Whereas concurrent writes
in transactions are a cause for rollback, for error checking
we should expect addresses to be written to multiple times
in different checkpoints. This means that buffering in the
L1 is suitable for best effort hardware transactional memory
systems [29], but is not suitable for error correction; inter-
mediate writes need to be buffered as well for more ﬁne-
grained rollback and commit. Another difference is that we can
partition checkpoints as we wish for error checking, whereas
they are necessarily in ﬁxed points for atomic transactions.
Our error correction solution is, in transactional-memory
terms, an eager versioning, optimistic conﬂict-detecting system
up until the L1, and a lazy versioning, pessimistic conﬂict-
detecting system between cores [30]. Optimistic “conﬂict
detection” between checkpoints on the same core is necessary
to achieve the desired parallelism: if we were pessimistic, any
data accessed in multiple checkpoints would cause a stall until
the previous check had completed. Similarly, eager versioning,
where we use an undo log instead of using the L1 cache as
a buffer of data which can be thrown away, is necessary to
avoid the fates of all concurrent checks being tied together:
we need to be able to overwrite uncommitted data in the L1
for future transactions without stalling, and also to be able to
commit data within the L1 without forcing write-through to the
L2. Without an undo log, intermediate rollback states become
inaccessible. This would result in stalls while waiting for the
last in a set of checks to complete, reducing performance. As
all code is within a fault tolerance “transaction”, this means the
amount of sharing involved leaves lazy versioning impractical
from a performance perspective, unlike in typical hardware
transactional-memory implementations [12].
However, we disallow uncommitted data to leave the L1,
and prevent the sharing of uncommitted data between cores.
This, in effect, makes the transaction policy between cores
somewhat like a lazy versioning, pessimistic conﬂict-detection
system. We use lazy versioning in that the L2 is guaranteed
to contain correct data, and the L1 is used as a write buffer.
This makes rollback easier, as we don’t have to serialise undo
writes between different cores. For the same reason, we have
pessimistic conﬂict detection, in that uncommitted shared data
between cores is prevented by design, again to make rollback
211
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:52:36 UTC from IEEE Xplore.  Restrictions apply. 
simpler by avoiding error propagation. A more optimistic
solution might let potential errors propagate, and detect this at
commit time, but this would be signiﬁcantly more complicated
from a veriﬁcation and protocol perspective.
VIII. RELATED WORK
A wide design space exists for providing processor fault
tolerance: we compare the main categories here.
A. Hardware Redundancy
The two broad categories of hardware-only redundancy for
computation can be described as space-based and time-based.
Space-based schemes, such as lockstepping, duplicate hard-
ware for redundancy. Lockstepping is used in ARM’s Cortex-R
series [14], which are designed for high reliability applications.
Error correction can be achieved with lockstepping by using
three cores with a majority voting system [6]. Lockstepping
has also been used in other commercial designs such as the
IBM G5 [31] and Compaq Himalaya [32]. Traditional lock-
stepping calculates the redundant results immediately, meaning
fault propagation is trivial to prevent. However, it is often
desirable to have the second core trail the ﬁrst, as exempliﬁed
by Gupta et al. [33], to reduce cache misses and correlated
errors. This has the side effect of increasing communication
delay between multiple cores, as with our scheme.
Time-based schemes, by comparison, run the same code on
the same hardware at different times, typically with hardware
support to forward loads and stores between two threads. AR-
SMT [15] is an example of a time-based redundant-multi-
threading scheme without this forwarding, where the address
space is duplicated instead to achieve the same effect at high
overhead, whereas Reinhardt and Mukherjee [4] extend it with
a load forwarder similar to that used by heterogeneous parallel
error detection [8]. The overheads are still high, however:
Mukherjee et al. [16] estimate a 32% performance overhead,
along with the loss of a hardware context. Vijaykumar et
al. [19] use tighter coupling between the two threads to ensure
the checker thread executes before the main thread commits, to
provide correction as well as detection. Time-based techniques
tend to delay the checking of errors when compared with
lockstepping, which ﬁnds them immediately, and so techniques
such as SafetyNet [34] are necessary to roll back to consistent
states. With ParaMedic, we can reuse many of the architectural
elements from heterogeneous error detection [8], reducing the
hardware necessary for arbitrary error recovery, allowing us to
match the granularity of checking and recovery precisely, so
we can free state as soon as it is no longer needed. Further,
we can directly trap errors within a core’s cache, preventing
their escape to other parts of the system, or other systems,
and guarantee forward progress even in the presence of hard
errors, by tracking their behaviour within error detection.
B. Software Redundancy
It is also possible to achieve redundancy without any hard-
ware support, by re-executing code and comparing the re-
sults in software, albeit at a signiﬁcant performance penalty.
SWIFT [35] is a solution which runs two copies of each in-
struction within a single thread to compare the results. Khudia
and Mahlke [36] extend this by only repeating computation for
error-intolerant parts of an application. Wang et al. [37] run the
second execution in a separate thread, to make better use of
multicore and multithreaded systems. Mitropoulou et al. [38]
extend this by using a more efﬁcient queue to share results
between cores. However, for performance reasons hardware
support is highly beneﬁcial [4], [15], [16]. Hybrid schemes
such as CRAFT [17] have also been proposed, which use
compiler assistance to duplicate instructions, coupled with a
special hardware detection structure.
C. Heterogeneous Redundancy
Our prior work [8] presented a heterogeneous parallel er-
ror detection scheme. This work focuses on exploiting the
parallelism inherent in fault detection to reduce the power-
performance-area (PPA) overheads of error checking, by per-
forming the second run of a program on an array of smaller
cores. Other work to exploit heterogeneity in error detection
includes Austin’s DIVA [39], which uses a superscalar in-order
core to verify correctness of the execution of a larger out-of-
order superscalar core. Errors on the in-order core are left
unchecked through the assumption that it is implemented with
larger transistors that are less susceptible to errors. However,
extensive alterations to the main core’s microarchitecture are
necessary, including ECC on all register state.
Ansari et al. [40] utilise heterogeneity by pairing an older
and newer version of the same microarchitecture series on a
chip. If the newer core suffers from hard faults, it can be
repurposed to provide hints for the slower previous generation
core. By comparison, LaFrieda et al. [41] design for het-
erogeneity induced by manufacturing variability. They couple
cores dynamically for lockstep execution, based on proﬁling,
to maximise system performance by pairing similar cores.
IX. CONCLUSION
We have designed ParaMedic, an architecture for exploiting
parallelism for error correction. This involves coupling a
hardware undo log for rolling back errors, with using the
L1 cache as a buffer for forwarding of unchecked values to
future computation, without allowing it to escape to other
cores to restrict
the sphere of replication and thus avoid
ordering problems upon rollback of writes within the load-
store log. The system also allows recovery from hard faults,
by detecting when repeated errors occur, and thus triggering
hardware migration.
Performance is reduced relative to detection alone, but
typically this is very minor. With an adaptive technique to
set checkpoint lengths, the overheads increase from just 1.9%
with detection, to 3.1% with correction. We therefore have
provided a practical architecture to allow full error correction
to be implemented extremely efﬁciently for commodity out-
of-order superscalar systems.
212
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:52:36 UTC from IEEE Xplore.  Restrictions apply. 
[27] N. Binkert, B. Beckmann, G. Black, S. K. Reinhardt, A. Saidi, A. Basu,
J. Hestness, D. R. Hower, T. Krishna, S. Sardashti, R. Sen, K. Sewell,
M. Shoaib, N. Vaish, M. D. Hill, and D. A. Wood, “The gem5 simulator,”
SIGARCH Comput. Archit. News, vol. 39, 2011.
[28] A. Gutierrez, J. Pusdesris, R. G. Dreslinski, T. Mudge, C. Sudanthi, C. D.
Emmons, M. Hayenga, and N. Paver, “Sources of error in full-system
simulation,” in ISPASS, 2014.
[29] R. M. Yoo, C. J. Hughes, K. Lai, and R. Rajwar, “Performance
evaluation of Intel transactional synchronization extensions for high-
performance computing,” in SC, 2013.
[30] A. Mcdonald, “Architectures for transactional memory,” Ph.D. disserta-
tion, 2009.
[31] T. J. Slegel, R. M. Averill III, M. A. Check, B. C. Giamei, B. W.
Krumm, C. A. Krygowski, W. H. Li, J. S. Liptay, J. D. MacDougall,
T. J. McPherson, J. A. Navarro, E. M. Schwarz, K. Shum, and C. F.
Webb, “IBM’s S/390 G5 microprocessor design,” IEEE Micro, vol. 19,
1999.
[32] A. Wood, “Data integrity concepts, features, and technology,” Tandem
Division, Compaq Computer Corporation, White Paper, 1999.
[33] S. Gupta, S. Feng, A. Ansari, J. Blome, and S. Mahlke, “The stagenet
fabric for constructing resilient multicore systems,” in MICRO, 2008.
[34] D. J. Sorin, M. M. K. Martin, M. D. Hill, and D. A. Wood, “Safetynet:
Improving the availability of shared memory multiprocessors with global
checkpoint/recovery,” in ISCA, 2002.
[35] G. A. Reis, J. Chang, N. Vachharajani, R. Rangan, and D. I. August,
“Swift: Software implemented fault tolerance,” in CGO, 2005.
[36] D. S. Khudia and S. Mahlke, “Harnessing soft computations for low-
budget fault tolerance,” in MICRO, 2014.
[37] C. Wang, H. S. Kim, Y. Wu, and V. Ying, “Compiler-managed software-
based redundant multi-threading for transient fault detection,” in CGO,
2007.
[38] K. Mitropoulou, V. Porpodas,
“COMET:
Communication-optimised multi-threaded error-detection technique,” in
CASES, 2016.
and T. M.
Jones,
[39] T. M. Austin, “Diva: A reliable substrate for deep submicron microar-
chitecture design,” in MICRO, 1999.
[40] A. Ansari, S. Feng, S. Gupta, and S. Mahlke, “Necromancer: Enhancing
system throughput by animating dead cores,” in ISCA, 2010.
[41] C. LaFrieda, E. Ipek, J. F. Martinez, and R. Manohar, “Utilizing
dynamically coupled cores to form a resilient chip multiprocessor,” in
DSN, 2007.
ACKNOWLEDGEMENTS
This work was supported by the Engineering and Physical
Sciences Research Council (EPSRC), through grant references
EP/K026399/1 and EP/M506485/1, and Arm Ltd. Additional
data related to this publication is available in the data reposi-
tory at https://doi.org/10.17863/CAM.37963.
REFERENCES
[1] J. Srinivasan, S. V. Adve, P. Bose, and J. A. Rivers, “The impact of
technology scaling on lifetime reliability,” in DSN, 2004.
[2] S. E. Michalak, K. W. Harris, N. W. Hengartner, B. E. Takala, and S. A.
Wender, “Predicting the number of fatal soft errors in los alamos national
laboratory’s asc q supercomputer,” IEEE Transactions on Device and
Materials Reliability, 2005.
[3] S. Borkar and A. A. Chien, “The future of microprocessors,” Commu-
nications of the ACM, vol. 54, 2011.
[4] S. K. Reinhardt and S. S. Mukherjee, “Transient fault detection via
simultaneous multithreading,” in ISCA, 2000.
[5] C. Hernandez and J. Abella, “Timely error detection for effective
recovery in light-lockstep automotive systems,” IEEE Transactions on
Computer-Aided Design of Integrated Circuits and Systems, vol. 34,
2015.
[6] X. Iturbe, B. Venu, E. Ozer, and S. Das, “A triple core lock-step
(TCLS) ARM R(cid:2)Cortex R(cid:2)-R5 processor for safety-critical and ultra-
reliable applications,” in DSN-W, 2016.
[7] M. Rausand, Reliability of Safety-Critical Systems: Theory and Appli-
cations. Wiley, 2014.
neous cores,” in DSN, 2018.
iot-and-wearables/2.
[8] S. Ainsworth and T. M. Jones, “Parallel error detection using heteroge-
[9] http://www.anandtech.com/show/8542/cortexm7-launches-embedded-
[10] https://www.siﬁve.com/products/coreplex-risc-v-ip/e51/.
[11] http://www.anandtech.com/show/8718/the-samsung-galaxy-note-4-
exynos-review/6.
[12] T. Nakaike, R. Odaira, M. Gaudet, M. M. Michael, and H. Tomari,
“Quantitative comparison of hardware transactional memory for Blue
Gene/Q, zEnterprise EC12, Intel Core, and POWER8,” in ISCA, 2015.
[13] C. Bienia, “Benchmarking modern multiprocessors,” Ph.D. dissertation,
Princeton University, January 2011.
[14] N. Werdmuller, “Addressing functional safety applications with ARM
https://community.arm.com/groups/embedded/blog/2015/
Cortex-R5,”
01/22/addressing-functional-safety-applications-with-arm-cortex-r5,
2015.
[15] E. Rotenberg, “AR-SMT: A microarchitectural approach to fault toler-
ance in microprocessors,” in FTCS, 1999.
[16] S. S. Mukherjee, M. Kontz, and S. K. Reinhardt, “Detailed design and
evaluation of redundant multithreading alternatives,” in ISCA, 2002.
[17] G. A. Reis, J. Chang, N. Vachharajani, R. Rangan, D. I. August, and S. S.
Mukherjee, “Design and evaluation of hybrid fault-detection systems,”
in ISCA, 2005.
[18] E. Schuchman and T. N. Vijaykumar, “Blackjack: Hard error detection
with redundant threads on smt,” in DSN, 2007.
[19] T. N. Vijaykumar, I. Pomeranz, and K. Cheng, “Transient-fault recovery
using simultaneous multithreading,” in ISCA, 2002.
[20] T. K. Moon, Error Correction Coding: Mathematical Methods and
Algorithms. Wiley-Interscience, 2005.
[21] N. N. Sadler and D. J. Sorin, “Choosing an error protection scheme for
a microprocessor’s L1 data cache,” in ICCD, 2006.
[22] ARM Ltd., http://infocenter.arm.com/help/index.jsp?topic=/com.arm.
doc.ddi0500g/CHDEEHDD.html.
[23] D.-M. Chiu and R. Jain, “Analysis of the increase and decrease al-
gorithms for congestion avoidance in computer networks,” Computer
Networks and ISDN Systems, vol. 17, 1989.
[24] D. M. Tullsen, S. J. Eggers, and H. M. Levy, “Simultaneous multithread-
ing: Maximizing on-chip parallelism,” in ISCA, 1995.
[25] P. R. Luszczek, D. H. Bailey, J. J. Dongarra, J. Kepner, R. F. Lucas,
R. Rabenseifner, and D. Takahashi, “The hpc challenge (hpcc) bench-
mark suite,” in SC, 2006.
[26] M. R. Guthaus, J. S. Ringenberg, D. Ernst, T. M. Austin, T. Mudge, and
R. B. Brown, “MiBench: A free, commercially representative embedded
benchmark suite,” in WWC, 2001.
213
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:52:36 UTC from IEEE Xplore.  Restrictions apply.