## NumPy 相关**答疑 1：如何理解 NumPy 中 axis 的使用？**这里我引用文稿中的一段代码：    a = np.array([[4,3,2],[2,4,1]])print np.sort(a)print np.sort(a, axis=None)print np.sort(a, axis=0)  print np.sort(a, axis=1)  同学们最容易混淆的是 axis=0 和 axis=1 的顺序。你可以记住：axis=0代表跨行（实际上就是按列），axis=1 代表跨列（实际上就是按行）。如果排序的时候，没有指定 axis，默认axis=-1，代表就是按照数组最后一个轴来排序。如果axis=None，代表以扁平化的方式作为一个向量进行排序。所以上面的运行结果为：    [[2 3 4] [1 2 4]][1 2 2 3 4 4][[2 3 1] [4 4 2]][[2 3 4] [1 2 4我解释下 axis=0 的排序结果，axis=0代表的是跨行（跨行就是按照列），所以实际上是对 \[4, 2\] \[3, 4\] \[2,1\] 来进行排序，排序结果是 \[2, 4\] \[3, 4\] \[1,2\]，对应的是每一列的排序结果。还原到矩阵中也就是 \[\[2 3 1\], \[4, 4,2\]\]。
### **答疑 2：定义结构数组中的 S32 代表什么意思**？``{=html}我文稿中定义了一个结构数组 persontype。    import numpy as nppersontype = np.dtype({    'names':['name', 'age', 'chinese', 'math', 'english'],    'formats':['S32','i', 'i', 'i', 'f']})这里实际上用的是 numpy 中的字符编码来表示数据类型的定义，比如 i代表整数，f 代表单精度浮点数，S 代表字符串，S32 代表的是 32个字符的字符串。![](Images/3fc0ae449ba1ee8d10ac325c591488e8.png){savepage-src="https://static001.geekbang.org/resource/image/74/3c/74e6c7e0a9b9805f746703fbc55c763c.png"}如果数据中使用了中文，可以把类型设置为 U32，比如：    import numpy as nppersontype = np.dtype({    'names':['name', 'age', 'chinese', 'math', 'english'],    'formats':['U32','i', 'i', 'i', 'f']})peoples = np.array([(" 张飞 ",32,75,100, 90),(" 关羽 ",24,85,96,88.5), (" 赵云 ",28,85,92,96.5),(" 黄忠 ",29,65,85,100)], dtype=persontype)**答疑 3：PyCharm 中无法 import numpy 的问题**有些同学已经安装好了 numpy，但在 PyCharm 中依然无法使用numpy。遇到这个问题的主要原因是 PyCharm会给每一个新建的项目都是一个全新的虚拟环境。![](Images/5897780b87b92e33b2a557cee83c5c0a.png){savepage-src="https://static001.geekbang.org/resource/image/39/39/39ffb856f0937de79e2d3a1363537639.png"}在这个环境下，默认的包只有 pip、setuptools 和 wheel 这三个工具，你可以在File-\>Settings 里面找到这个界面。![](Images/dcaef677c6ff3cf86913a08ef59b8c2d.png){savepage-src="https://static001.geekbang.org/resource/image/7f/d8/7fbc03617a3d14b52ca62651be813cd8.png"}这说明 numpy 并没有配置到你创建的这个 Project下的环境中，需要手动点击右侧的 + 号，对 numpy 进行添加。![](Images/2bdac40a0d0e1519266e8225c08edc96.png){savepage-src="https://static001.geekbang.org/resource/image/a8/e1/a8d6063d8cdae8bdf90947f9b69f89e1.png"}\添加之后，你就可以正常运行程序，显示出结果了。**答疑 4：我不明白为什么打印出来的 name 会带一个 b？**这位同学的代码是这样的：    student = np.dtype([('name','S20'), ('age', 'i1'), ('marks', 'f4')]) a = np.array([('abc', 21, 50),('xyz', 18, 75)], dtype = student) print(a) print(a['name']) 结果： [(b'abc', 21, 50.) (b'xyz', 18, 75.)] [b'abc' b'xyz'我来解释一下。Python3 默认 str 是 Unicode 类型，所以要转成bytestring，会在原来的 str 前加上 b。如果你用 py2.7就不会有这个问题，py3 的 b 只是告诉你这里它转化成了 bytestring进行输出。**答疑 5：np.ceil 代表什么意思？**ceil 是 numpy 中的一个函数，代表向上取整。比如 np.ceil(2.4)=3。
## 数据分析思维培养及练习相关**答疑 1：Online Judge 的比赛题目，数学不好怎么办？**Vol1\~Vol32 的难度是逐渐增加吗的，怎么可以选出有易到难的题目？Online Judge 有一些简单的题目可以选择，选择使用人数多，且 accepted比例高的题目。另外它面向的是一些参加比赛的人员，比如高中的 NOI比赛，或者大学的 ACM 比赛。你也可以选择 leetcode 或者 pythontip进行训练。难度不一定是增加的，而是出题的先后顺序。难易程度，你可以看下提交的人数和Accepted 的比例。提交人数和 Accepted 比例越高，说明越简单。**答疑 2：加餐中小区宽带使用多台手机等设备，不会被检测到吗？**小区宽带和手机飞行是两种解决方案。用手机飞行不需要用到小区宽带。用小区宽带需要使用到交换机，这里可以自己来控制交换机，每次自动切换 IP。**答疑3：加餐中提到的一万个手机号。。。那怎么更换呢？也要一万台设备吗？**1万个手机号，主要用于账号注册，通常采用的是"卡池"这个设备。简单来说，卡池可以帮你做收发短信。一个卡池设备512 张卡，并发 32 路。有了卡池，还需要算法。你不能让这 512张卡每次操作都是有规律可循的，比如都是同步执行某项操作，否则微信、Facebook会直接把它们干掉。学过数据挖掘的人应该会知道，这 512张卡如果是协同操作，可以直接被算法识别出来。在微信、Facebook 看来，这512张卡实际上是同一个人，也就是"机器人"。所以卡池可以帮你做短信验证码，以便账号登录用。MIFI+SIM帮你做手机流量上网用。这是两套不同的设备。**答疑 4：听说企业里用 SQL 和 Excel进行数据分析的很多，这块该如何选择？**SQL 和 Excel做统计的工作多一些，涉及到编程的很少。如果你想在这个行业进一步提升，或者做一名算法工程师，那么你都要和Python 打交道。专栏里数据挖掘算法的部分，是用 Python 交付的。Excel 和SQL 很难做数据挖掘。如果想对数据概况有个了解，做一些基础分析，用 Excel 和 SQL 是 OK的。但是想进一步挖掘数据的价值，掌握 Python 还是挺有必要的。另外，如果你做的是数据可视化工作，在企业里会用到 tableau 或者 powerBI这些工具。数据采集你也可以选择第三方工具，或者自己用 Python 来编写。**答疑 5：学一些算法的时候比如SVM，是不是掌握它们的理论内容即可。不需要自己去实现，用的时候调用库即可？**是的，这些算法都有封装，直接使用即可。在 python 的 sklearn中就是一行语句的事。**答疑6：老师，我现在等于从零开始学数据挖掘，所谓的数学基础指的是把高数学到哪种境界啊？是像考研那样不管极限导数积分每种题型都要会解，还是只需要了解这些必备的高数基础的概念？**不需要求解每一道数学题，只需要具备高数基础概念即可！概率论与数理统计、线性代数、最优化方法和图论这些，我在算法中涉及的地方都会讲到，你暂时不用提前学习这些数学知识。我觉得最好的方式就是在案例中灵活运用，这样可以加深你对这些数学知识的理解。对于大部分从 0开始学数据挖掘的人来说，可以淡化公式，重点理解使用场景和概念。
## 爬虫相关问题**答疑 1：关于 Python 爬虫工具的推荐**我除了在专栏里讲到了**Requests、XPath 解析**，以及Selenium、PhantomJS。还有一些工具是值得推荐的。Scrapy 是一个 Python 的爬虫框架，它依赖的工具比较多，所以在 pip install的时候，会安装多个工具包。scrapy 本身包括了爬取、处理、存储等工具。在scrapy 中，有一些组件是提供给你的，需要你针对具体任务进行编写。比如在item.py 对抓取的内容进行定义，在 spider.py 中编写爬虫，在 pipeline.py中对抓取的内容进行存储，可以保存为 csv 等格式。这里不具体讲解 scrapy的使用。另外，Puppeteer 是个很好的选择，可以控制 Headless Chrome，这样就不用Selenium 和 PhantomJS。与 Selenium 相比，Puppeteer 直接调用 Chrome 的API 接口，不需要打开浏览器，直接在 V8 引擎中处理，同时这个组件是由Google 的 Chrome 团队维护的，所以兼容性会很好。**答疑 2：driver =webdriver.Chrome()，为什么输入这个代码就会报错了呢？**报错的原因是没有下载或正确配置 ChromeDriver 路径，正确的方法如下：1\. 下载 ChromeDriver，并放到 Chrome 浏览器目录中；下载地址：2\. 将 Chrome 浏览器目录添加到系统的环境变量 Path 中，然后再运行下试试.另外你也可以在代码中设置 ChromeDriver 的路径，方法如下：    chrome_driver = "C:\Users\cheny\AppData\Local\Google\Chrome\Application\chromedriver.exe"driver = webdriver.Chrome(executable_path=chrome_driver)**答疑 3：如果是需要用户登陆后才能爬取的数据该怎么用 python 来实现呢？**你可以使用 Python+Selenium 的方式完成账户的自动登录，因为 Selenium是个自动化测试的框架，使用 Selenium 的 webdriver就可以模拟浏览器的行为。找到输入用户名密码的地方，输入相应的值，然后模拟点击即可完成登录（没有验证码的情况下）。另外你也可以使用 cookie 来登录网站，方法是你登录网站时，先保存网站的cookie，然后在下次访问的时候，加载之前保存的 cookie，放到 requestheaders 中，这样就不需要再登录网站了。**答疑4：为什么我在豆瓣网查询图片的网址与你不一样？王祖贤&source=suggest 。**咱们访问豆瓣查询图片的网址应该是一样的。只是我给出的是 json 的链接。方法是这样的：用 Chrome 浏览器的开发者工具，可以监测出来网页中是否有json 数据的传输，所以我给出的链接是 json 数据传输的链接：**答疑 5：XHR 数据这个是如何查出来的，我使用 chrome 的开发者工具查看 XHR数据，但是查不到这部分，麻烦老师帮忙解答。**你需要使用浏览器的插件查看 XHR 数据，比如在 Chrome 的开发者工具。在豆瓣搜索中，我们对"王祖贤"进行了模拟，发现 XHR数据中有一个请求是这样的：王祖贤 &limit=20&start=0你可以看一下操作流程。```{=html}``````{=html}```
## 数据变换相关**答疑 1：数据规范化、归一化、标准化是同一个概念么？**数据规范化是更大的概念，它指的是将不同渠道的数据，都按照同一种尺度来进行度量，这样做有两个好处，一是让数据之间具有可比较性；另一个好处就是方便后续运算，因为数据在同一个数量级上规整了，在机器学习迭代的时候，也会加快收敛效率。数据归一化和数据标准化都是数据规范化的方式。不同点在于数据归一化会让数据在一个\[0,1\] 或者 \[-1,1\的区间范围内。而数据标准化会让规范化的数据呈现正态分布的情况，所以你可以这么记：归一化的"一"，是让数据在\[0,1\] 的范围内。而标准化，目标是让数据呈现标准的正态分布。**答疑 2：什么时候会用到数据规范化（Min-max、Z-Score 和小数定标）？**刚才提到了，进行数据规范化有两个作用：一是让数据之间具有可比较性，二是加快后续算法的迭代收敛速度。实际上你能看到 Min-max、Z-Score和小数定标规范化都是一种线性映射的关系，将原来的数值投射到新的空间中。这样变换的好处就是可以看到在特定空间内的数值分布情况，比如通过Min-max 可以看到数据在 \[0,1\] 之间的分布情况，Z-Score可以看到数值的正态分布情况等。不论是采用哪种数据规范化方法，规范化后的数值都会在同一个数量的级别上，这样方便后续进行运算。那么回过头来看，在数据挖掘算法中，是否都需要进行数据规范化呢？一般情况下是需要的，尤其是针对距离相关的运算，比如在K-Means、KNN以及聚类算法中，我们需要有对距离的定义，所以在做这些算法前，需要对数据进行规范化。另外还有一些算法用到了梯度下降作为优化器，这是为了提高迭代收敛的效率，也就是提升找到目标函数最优解的效率。我们也需要进行数据规范化，比如逻辑回归、SVM和神经网络算法。在这些算法中都有目标函数，需要对目标函数进行求解。梯度下降的目标是寻找到目标函数的最优解，而梯度的方法则指明了最优解的方向，如下图所示。![](Images/354b40c5674c74b3af9eddaffdd4396d.png){savepage-src="https://static001.geekbang.org/resource/image/55/a0/55fb82ec9ecac8516419edfcc5ecd1a0.jpg"}当然不是所有的算法都需要进行数据规范化。在构造决策树的时候，可以不用提前做数据规范化，因为我们不需要关心特征值的大小维度，也没有使用到梯度下降来做优化，所以数据规范化对决策树的构造结果和构造效率影响不大。除此之外，还是建议你在做数据挖掘算法前进行数据规范化。**答疑 3：如何使用 Z-Score 规范化，将分数变成正态分布？**我在专栏文稿中举了一个 Z-Score 分数规范化的例子，假设 A 与 B的考试成绩都为 80 分，A 的考卷满分是 100 分（及格 60 分），B的考卷满分是 500 分（及格 300 分）。这里假设 A 和 B的考试成绩都是成正态分布，可以直接采用 Z-Score 的线性化规范化方法。在专栏的讨论区中，有个同学提出了"Z-Score"的非线性计算方式，大家可以一起了解下：1.  先按公式计算出百分等级。百分等级（年级）=100-(100x 年级名次 -50)/    有效参加考试人数。这里百分等级是每个学生在该批学生中的相对位置，其中百分等级是按照正态分布图的所占面积比例求得的；2.  按照百分等级数去标准正态分布表中查询得出 Z-Score 值，这样最终得出的    Z 分便是标准的正态分布，能够将偏态转化成标准正态。因为在很多情况下，数值如果不是正态分布，而是偏态分布，直接使用 Z-Score的线性计算方式无法将分数转化成正态分布。采用以上的方法可以解决这一个问题，大家可以了解下。这里偏态分布指的是非对称分布的偏斜状态，包括了负偏态，也就是左偏态分布，以及正偏态，也就是右偏态分布。我发现大家对工具的使用和场景比较感兴趣，所以最后留两道思考题。第一道题：假设矩阵 a =np.array(\[\[4,3,2\],\[2,4,1\]\])，请你编写代码将矩阵中的每一列按照从小到大的方式进行排序。第二道题：你都用过哪些 Python爬虫工具，抓取过哪些数据，觉得哪个工具好用？欢迎你在评论分享你的想法，也欢迎你点击"请朋友读"，把它分享给你的朋友或者同事。![](Images/8b75105190797b2e4f7be2536b6543db.png){savepage-src="https://static001.geekbang.org/resource/image/48/96/48cb89aa8c4858bbc18df3b3ac414496.jpg"}
# 17 丨决策树（上）：要不要去打篮球？决策树来告诉你想象一下一个女孩的妈妈给她介绍男朋友的场景：女儿：长的帅不帅？妈妈：挺帅的。女儿：有没有房子？妈妈：在老家有一个。女儿：收入高不高？妈妈：还不错，年薪百万。女儿：做什么工作的？妈妈：IT 男，互联网公司做数据挖掘的。女儿：好，那我见见。在现实生活中，我们会遇到各种选择，不论是选择男女朋友，还是挑选水果，都是基于以往的经验来做判断。如果把判断背后的逻辑整理成一个结构图，你会发现它实际上是一个树状图，这就是我们今天要讲的**决策树**。
## 决策树的工作原理决策树基本上就是把我们以前的经验总结出来。我给你准备了一个打篮球的训练集。如果我们要出门打篮球，一般会根据"天气"、"温度"、"湿度"、"刮风"这几个条件来判断，最后得到结果：去打篮球？还是不去？![](Images/32a92537cc519199fd4145c5d283a75c.png){savepage-src="https://static001.geekbang.org/resource/image/dc/90/dca4224b342894f12f54a9cb41d8cd90.jpg"}\上面这个图就是一棵典型的决策树。我们在做决策树的时候，会经历两个阶段：**构造和剪枝**。**构造**什么是构造呢？构造就是生成一棵完整的决策树。简单来说，**构造的过程就是选择什么属性作为节点的过程**，那么在构造过程中，会存在三种节点：1.  根节点：就是树的最顶端，最开始的那个节点。在上图中，"天气"就是一个根节点；2.  内部节点：就是树中间的那些节点，比如说"温度"、"湿度"、"刮风"；3.  叶节点：就是树最底部的节点，也就是决策结果。``{=html}节点之间存在父子关系。比如根节点会有子节点，子节点会有子子节点，但是到了叶节点就停止了，叶节点不存在子节点。那么在构造过程中，你要解决三个重要的问题：1.  选择哪个属性作为根节点；2.  选择哪些属性作为子节点；3.  什么时候停止并得到目标状态，即叶节点。**剪枝**决策树构造出来之后是不是就万事大吉了呢？也不尽然，我们可能还需要对决策树进行剪枝。剪枝就是给决策树瘦身，这一步想实现的目标就是，不需要太多的判断，同样可以得到不错的结果。之所以这么做，是为了防止"过拟合"（Overfitting）现象的发生。"过拟合"这个概念你一定要理解，它指的就是模型的训练结果"太好了"，以至于在实际应用的过程中，会存在"死板"的情况，导致分类错误。欠拟合，和过拟合就好比是下面这张图中的第一个和第三个情况一样，训练的结果"太好"，反而在实际应用过程中会导致分类错误。![](Images/c8bba6c619254e353c45da2e6d8cbf91.png){savepage-src="https://static001.geekbang.org/resource/image/d3/df/d30bfa3954ffdf5baf47ce53df9366df.jpg"}\造成过拟合的原因之一就是因为训练集中样本量较小。如果决策树选择的属性过多，构造出来的决策树一定能够"完美"地把训练集中的样本分类，但是这样就会把训练集中一些数据的特点当成所有数据的特点，但这个特点不一定是全部数据的特点，这就使得这个决策树在真实的数据分类中出现错误，也就是模型的"泛化能力"差。泛化能力指的分类器是通过训练集抽象出来的分类能力，你也可以理解是举一反三的能力。如果我们太依赖于训练集的数据，那么得到的决策树容错率就会比较低，泛化能力差。因为训练集只是全部数据的抽样，并不能体现全部数据的特点。既然要对决策树进行剪枝，具体有哪些方法呢？一般来说，剪枝可以分为"预剪枝"（Pre-Pruning）和"后剪枝"（Post-Pruning）。预剪枝是在决策树构造时就进行剪枝。方法是在构造的过程中对节点进行评估，如果对某个节点进行划分，在验证集中不能带来准确性的提升，那么对这个节点进行划分就没有意义，这时就会把当前节点作为叶节点，不对其进行划分。后剪枝就是在生成决策树之后再进行剪枝，通常会从决策树的叶节点开始，逐层向上对每个节点进行评估。如果剪掉这个节点子树，与保留该节点子树在分类准确性上差别不大，或者剪掉该节点子树，能在验证集中带来准确性的提升，那么就可以把该节点子树进行剪枝。方法是：用这个节点子树的叶子节点来替代该节点，类标记为这个节点子树中最频繁的那个类。
## 如何判断要不要去打篮球？我给你准备了打篮球的数据集，训练数据如下：![](Images/05ca558bb62503bcacfd9729fb49fc28.png){savepage-src="https://static001.geekbang.org/resource/image/32/07/327eafa4a33e3e76ca86ac59195c0307.png"}\我们该如何构造一个判断是否去打篮球的决策树呢？再回顾一下决策树的构造原理，在决策过程中有三个重要的问题：将哪个属性作为根节点？选择哪些属性作为后继节点？什么时候停止并得到目标值？显然将哪个属性（天气、温度、湿度、刮风）作为根节点是个关键问题，在这里我们先介绍两个指标：**纯度**和**信息熵**。先来说一下纯度。你可以把决策树的构造过程理解成为寻找纯净划分的过程。数学上，我们可以用纯度来表示，纯度换一种方式来解释就是让目标变量的分歧最小。我在这里举个例子，假设有 3 个集合：-   集合 1：6 次都去打篮球；-   集合 2：4 次去打篮球，2 次不去打篮球；-   集合 3：3 次去打篮球，3 次不去打篮球。按照纯度指标来说，集合 1\> 集合 2\> 集合 3。因为集合 1 的分歧最小，集合3 的分歧最大。然后我们再来介绍信息熵（entropy）的概念，**它表示了信息的不确定度**。在信息论中，随机离散事件出现的概率存在着不确定性。为了衡量这种信息的不确定性，信息学之父香农引入了信息熵的概念，并给出了计算信息熵的数学公式：![](Images/acf25b0c935e02cff8a54f1a8396a987.png){savepage-src="https://static001.geekbang.org/resource/image/74/d5/741f0ed01c53fd53f0e75204542abed5.png"}\p(i\|t) 代表了节点 t 为分类 i 的概率，其中 log2 为取以 2为底的对数。这里我们不是来介绍公式的，而是说存在一种度量，它能帮我们反映出来这个信息的不确定度。当不确定性越大时，它所包含的信息量也就越大，信息熵也就越高。我举个简单的例子，假设有 2 个集合-   集合 1：5 次去打篮球，1 次不去打篮球；-   集合 2：3 次去打篮球，3 次不去打篮球。在集合 1 中，有 6 次决策，其中打篮球是 5 次，不打篮球是 1次。那么假设：类别 1 为"打篮球"，即次数为 5；类别 2为"不打篮球"，即次数为 1。那么节点划分为类别 1 的概率是 5/6，为类别 2的概率是 1/6，带入上述信息熵公式可以计算得出：![](Images/458c6e9a745da4c6fa8fd7b20670710f.png){savepage-src="https://static001.geekbang.org/resource/image/ab/a4/aba6bb24c3444923bfa2320119ce54a4.png"}\同样，集合 2 中，也是一共 6 次决策，其中类别 1 中"打篮球"的次数是3，类别 2"不打篮球"的次数也是 3，那么信息熵为多少呢？我们可以计算得出：![](Images/de5157a7467c6ef5840d2782f6096303.png){savepage-src="https://static001.geekbang.org/resource/image/e0/c2/e05fe27109330b49453b62505e37e4c2.png"}\从上面的计算结果中可以看出，信息熵越大，纯度越低。当集合中的所有样本均匀混合时，信息熵最大，纯度最低。我们在构造决策树的时候，会基于纯度来构建。而经典的"不纯度"的指标有三种，分别是信息增益（ID3 算法）、信息增益率（C4.5算法）以及基尼指数（Cart 算法）。我们先看下 ID3 算法。ID3算法计算的是**信息增益**，信息增益指的就是划分可以带来纯度的提高，信息熵的下降。它的计算公式，是父亲节点的信息熵减去所有子节点的信息熵。在计算的过程中，我们会计算每个子节点的归一化信息熵，即按照每个子节点在父节点中出现的概率，来计算这些子节点的信息熵。所以信息增益的公式可以表示为：![](Images/b1e67cd3a7ed894124b5c134f4a36018.png){savepage-src="https://static001.geekbang.org/resource/image/bf/34/bfea7626733fff6180341c9dda3d4334.png"}\公式中 D 是父亲节点，Di 是子节点，Gain(D,a) 中的 a 作为 D节点的属性选择。假设天气 = 晴的时候，会有 5 次去打篮球，5 次不打篮球。其中 D1 刮风 =是，有 2 次打篮球，1 次不打篮球。D2 刮风 = 否，有 3 次打篮球，4次不打篮球。那么 a 代表节点的属性，即天气 = 晴。你可以在下面的图例中直观地了解这几个概念。![](Images/babf69cd5eeeee2b617c7a7ff88163e9.png){savepage-src="https://static001.geekbang.org/resource/image/40/67/40810468abc4140f45f3a09a2d427667.jpg"}\比如针对图上这个例子，D 作为节点的信息增益为：![](Images/596515dd42052bc8023c6bca388d5e1e.png){savepage-src="https://static001.geekbang.org/resource/image/f2/82/f23f88a18b1227398c2ab3ef445d5382.png"}\也就是 D 节点的信息熵 -2 个子节点的归一化信息熵。2 个子节点归一化信息熵=3/10 的 D1 信息熵 +7/10 的 D2 信息熵。我们基于 ID3 的算法规则，完整地计算下我们的训练集，训练集中一共有 7条数据，3 个打篮球，4 个不打篮球，所以根节点的信息熵是：![](Images/11dc5d30afebcd1f54f0e5590710106f.png){savepage-src="https://static001.geekbang.org/resource/image/9f/9b/9f01e1d1e8082e55850676da50a84f9b.png"}如果你将天气作为属性的划分，会有三个叶子节点 D1、D2 和D3，分别对应的是晴天、阴天和小雨。我们用 + 代表去打篮球，-代表不去打篮球。那么第一条记录，晴天不去打篮球，可以记为1-，于是我们可以用下面的方式来记录 D1，D2，D3：D1(天气 = 晴天)={1-,2-,6+}D2(天气 = 阴天)={3+,7-}D3(天气 = 小雨)={4+,5-}我们先分别计算三个叶子节点的信息熵：![](Images/17607a1483d80f373bfd9443722c1324.png){savepage-src="https://static001.geekbang.org/resource/image/85/7f/8537ab10a1a3747d22059bfbbd2aa17f.png"}\因为 D1 有 3 个记录，D2 有 2 个记录，D3 有 2 个记录，所以 D中的记录一共是 3+2+2=7，即总数为 7。所以 D1 在 D（父节点）中的概率是3/7，D2 在父节点的概率是 2/7，D3 在父节点的概率是2/7。那么作为子节点的归一化信息熵 = 3/7\*0.918+2/7\*1.0+2/7\*1.0=0.965。因为我们用 ID3 中的信息增益来构造决策树，所以要计算每个节点的信息增益。天气作为属性节点的信息增益为，Gain(D , 天气)=0.985-0.965=0.020。。同理我们可以计算出其他属性作为根节点的信息增益，它们分别为 ：Gain(D , 温度)=0.128\Gain(D , 湿度)=0.020\Gain(D , 刮风)=0.020我们能看出来温度作为属性的信息增益最大。因为 ID3就是要将信息增益最大的节点作为父节点，这样可以得到纯度高的决策树，所以我们将温度作为根节点。其决策树状图分裂为下图所示：![](Images/01e6b9d6fb576623b40a0e6980c0bb92.png){savepage-src="https://static001.geekbang.org/resource/image/70/28/7067d83113c892a586e703d8b2e19828.jpg"}\然后我们要将上图中第一个叶节点，也就是D1={1-,2-,3+,4+}进一步进行分裂，往下划分，计算其不同属性（天气、湿度、刮风）作为节点的信息增益，可以得到：Gain(D , 天气)=0Gain(D , 湿度)=0Gain(D , 刮风)=0.0615我们能看到刮风为 D1的节点都可以得到最大的信息增益，这里我们选取刮风作为节点。同理，我们可以按照上面的计算步骤得到完整的决策树，结果如下：![](Images/14c1723d3bb5df84c48c3a0fc047d06c.png){savepage-src="https://static001.geekbang.org/resource/image/a6/3f/a6d66a655c5d82ea81299a7351694a3f.jpg"}\于是我们通过 ID3 算法得到了一棵决策树。ID3的算法规则相对简单，可解释性强。同样也存在缺陷，比如我们会发现 ID3算法倾向于选择取值比较多的属性。这样，如果我们把"编号"作为一个属性（一般情况下不会这么做，这里只是举个例子），那么"编号"将会被选为最优属性。但实际上"编号"是无关属性的，它对"打篮球"的分类并没有太大作用。所以 ID3有一个缺陷就是，有些属性可能对分类任务没有太大作用，但是他们仍然可能会被选为最优属性。这种缺陷不是每次都会发生，只是存在一定的概率。在大部分情况下，ID3都能生成不错的决策树分类。针对可能发生的缺陷，后人提出了新的算法进行改进。
## 在 ID3 算法上进行改进的 C4.5 算法那么 C4.5 都在哪些方面改进了 ID3 呢？**1. 采用信息增益率**因为 ID3 在计算的时候，倾向于选择取值多的属性。为了避免这个问题，C4.5采用信息增益率的方式来选择属性。信息增益率 = 信息增益 /属性熵，具体的计算公式这里省略。当属性有很多值的时候，相当于被划分成了许多份，虽然信息增益变大了，但是对于C4.5 来说，属性熵也会变大，所以整体的信息增益率并不大。**2. 采用悲观剪枝**ID3 构造决策树的时候，容易产生过拟合的情况。在 C4.5中，会在决策树构造之后采用悲观剪枝（PEP），这样可以提升决策树的泛化能力。悲观剪枝是后剪枝技术中的一种，通过递归估算每个内部节点的分类错误率，比较剪枝前后这个节点的分类错误率来决定是否对其进行剪枝。这种剪枝方法不再需要一个单独的测试数据集。**3. 离散化处理连续属性**C4.5可以处理连续属性的情况，对连续的属性进行离散化的处理。比如打篮球存在的"湿度"属性，不按照"高、中"划分，而是按照湿度值进行计算，那么湿度取什么值都有可能。该怎么选择这个阈值呢，**C4.5选择具有最高信息增益的划分所对应的阈值**。**4. 处理缺失值**针对数据集不完整的情况，C4.5 也可以进行处理。假如我们得到的是如下的数据，你会发现这个数据中存在两点问题。第一个问题是，数据集中存在数值缺失的情况，如何进行属性选择？第二个问题是，假设已经做了属性划分，但是样本在这个属性上有缺失值，该如何对样本进行划分？![](Images/7a1332d9c1b5ae42d19ffd2cae3fabba.png){savepage-src="https://static001.geekbang.org/resource/image/50/95/50b43c1820c03561f3ca3e627b454995.png"}\我们不考虑缺失的数值，可以得到温度 D={2-,3+,4+,5-,6+,7-}。温度 =高：D1={2-,3+,4+} ；温度 = 中：D2={6+,7-}；温度 = 低：D3={5-} 。这里 +号代表打篮球，- 号代表不打篮球。比如 ID=2时，决策是不打篮球，我们可以记录为 2-。所以三个叶节点的信息熵可以结算为：![](Images/e66c328f0acf4852673b5eead63174e2.png){savepage-src="https://static001.geekbang.org/resource/image/2a/a1/2a536b0362ec83286a497281b1acc2a1.png"}\这三个节点的归一化信息熵为 3/6\*0.918+2/6\*1.0+1/6\*0=0.792。针对将属性选择为温度的信息增益率为：Gain(D′, 温度)=Ent(D′)-0.792=1.0-0.792=0.208D′的样本个数为 6，而 D 的样本个数为 7，所以所占权重比例为 6/7，所以Gain(D′，温度) 所占权重比例为 6/7，所以：Gain(D, 温度)=6/7\*0.208=0.178这样即使在温度属性的数值有缺失的情况下，我们依然可以计算信息增益，并对属性进行选择。Cart 算法在这里不做介绍，我会在下一讲给你讲解这个算法。现在我们总结下ID3 和 C4.5 算法。首先 ID3算法的优点是方法简单，缺点是对噪声敏感。训练数据如果有少量错误，可能会产生决策树分类错误。C4.5在 ID3的基础上，用信息增益率代替了信息增益，解决了噪声敏感的问题，并且可以对构造树进行剪枝、处理连续数值以及数值缺失等情况，但是由于C4.5 需要对数据集进行多次扫描，算法效率相对较低。
## 总结前面我们讲了两种决策树分类算法 ID3 和C4.5，了解了它们的数学原理。你可能会问，公式这么多，在实际使用中该怎么办呢？实际上，我们可以使用一些数据挖掘工具使用它们，比如Python 的 sklearn，或者是Weka（一个免费的数据挖掘工作平台），它们已经集成了这两种算法。只是我们在了解了这两种算法之后，才能更加清楚这两种算法的优缺点。我们总结下，这次都讲到了哪些知识点呢？首先我们采用决策树分类，需要了解它的原理，包括它的构造原理、剪枝原理。另外在信息度量上，我们需要了解信息度量中的纯度和信息熵的概念。在决策树的构造中，一个决策树包括根节点、子节点、叶子节点。在属性选择的标准上，度量方法包括了信息增益和信息增益率。在算法上，我讲解了两种算法：ID3和 C4.5，其中 ID3 是基础的决策树算法，C4.5在它的基础上进行了改进，也是目前决策树中应用广泛的算法。然后在了解这些概念和原理后，强烈推荐你使用工具，具体工具的使用我会在后面进行介绍。![](Images/b14c846600ae0688945f04fe35c8fcb6.png){savepage-src="https://static001.geekbang.org/resource/image/d0/7a/d02e69930c8cf00c93578536933ad07a.png"}最后我们留一道思考题吧。请你用下面的例子来模拟下决策树的流程，假设好苹果的数据如下，请用ID3 算法来给出好苹果的决策树。![](Images/0ed45cd7e2fdde177cf15cee77ef359a.png){savepage-src="https://static001.geekbang.org/resource/image/0a/09/0a759fd725add916417c2c294600b609.png"}如果你觉得这篇文章有所价值，欢迎点击"请朋友读"，把它分享给你的朋友或者同事。![](Images/8b75105190797b2e4f7be2536b6543db.png){savepage-src="https://static001.geekbang.org/resource/image/48/96/48cb89aa8c4858bbc18df3b3ac414496.jpg"}