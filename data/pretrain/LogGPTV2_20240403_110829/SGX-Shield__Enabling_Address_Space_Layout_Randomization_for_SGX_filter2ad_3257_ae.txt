ASLR&DEP
Num sort
2.30%
String sort
31.67%
Fp emu.
29.77%
Assignment
3.89%
Idea
-0.67%
Huffman
25.96%
Neural net
22.20%
Lu decomp.
2.52%
Average
14.71%
TABLE V: Runtime performance overhead of SGX-Shield when
running nbench. Baseline column is a native run under SGX without
SGX-Shield and it is measured in microseconds. All columns in
SGX-Shield are represented with relative overheads in a percentage
compared to the baseline. RU-64 and RU-32 denote a size of
a randomization unit, 32- and 64-bytes, respectively. ASLR and
ASLR&DEP denote before and after applying SGX-Shield’s DEP and
SFI techniques, respectively. The average relative standard deviation
is 0.71% (the maximum is 2.45%).
we run SGX-Shield with various settings, changing the size of
randomization units and opt out software-DEP and SFI.
Table V shows the performance overhead imposed by
SGX-Shield in running nbench. An elapsed time in microsec-
onds is represented in the baseline column, while all other
columns under SGX-Shield are represented in a relative
overhead compared to the baseline in a percentage. In this
table, RU-32 and RU-64 denote the size of a randomization
unit, 32- and 64-bytes, respectively.
As the size of a randomization unit becomes smaller,
the performance overhead increases. More specifically, before
applying DEP and SFI, RU-32 imposes 6.75% more overhead
compared to RU-64 (i.e., from 1.05% to 7.80%). Once DEP
and SFI are applied, RU-32 imposes 7.82% more overhead (i.e.,
from 6.89% to 14.71%). This additional overhead is expected,
as SGX-Shield introduces more randomization units in RU-32,
and thus instruments more unconditional branches. Moreover,
a smaller randomization unit implies a negative impact on
code cache performance, as there will be more frequent control
transfers.
DEP and SFI techniques of SGX-Shield also slow the
execution of nbench. With RU-64, SGX-Shield shows 1.05%
overhead if DEP and SFI were not applied. If these were applied
together, SGX-Shield showed 6.89% overhead on average.
Similarly, with RU-32 SGX-Shield showed 7.80% and 14.71%
overhead on average before and after applying DEP and SFI,
respectively. In other words, the performance overhead of
SGX-Shield’s DEP and SFI is 5.84% and 6.91% in RU-32
and RU-64, respectively.
To better understand the performance
impacts of
SGX-Shield, we also counted the number of executed instruc-
tions in runtime while running the benchmarks. The perfor-
mance overhead that SGX-Shield imposes is directly related
to the number of executed instructions. To implement fine-
grained randomization, SGX-Shield instruments a terminator
instruction at the end of a randomization unit. This alone results
in 8.86% or 13.1% more executed instructions on average (See
Table VI). Moreover, to implement DEP and SFI, SGX-Shield
11
Benchmark
Baseline
RU-64
RU-32
SGX-Shield
ASLR&DEP
ASLR
5,245 K
6.55%
38,017 K 81.89%
7.07%
66,553 K
8.16%
301,104 K
5.80%
224,000 K
295,379 K
6.23%
8.76%
263,275 K
7.43%
7,967 K
16.49%
Num sort
String sort
Fp emu.
Assignment
Idea
Huffman
Neural net
Lu decomp.
Average
TABLE VI: The number of instructions executed in runtime while
running nbench
ASLR
21.38% 14.68%
274.49% 97.29%
23.37% 14.46%
7.73% 13.26%
6.47% 13.57%
12.07% 13.44%
21.56% 18.86%
9.22% 17.27%
47.04% 25.35%
ASLR&DEP
28.36%
314.47%
35.84%
15.95%
12.79%
19.14%
32.82%
21.73%
60.14%
SGX-Shield
RU-64
RU-32
Baseline
29 k
-
ASLR
37 k
5,663
212 KB 548 KB
131 KB 160 KB
68 KB 374 KB
ASLR&DEP
45 k
9,161
792 KB
193 KB
586 KB
ASLR&DEP
42 k
5,938
ASLR
39 k
8,430
584 KB 724 KB
177 KB 170 KB
391 KB 541 KB
# instr.
# RU
Binary size
— code+data
— metadata
TABLE VII: A static overhead of SGX-Shield to the nbench binary.
Note that the nbench benchmark suites contain a single binary, which
takes an argument to specify a certain testcase. # instr. denotes the
number of instructions in a binary; # RU. denotes the number of
randomization units that SGX-Shield generated. Binary size denotes
a size of a binary, including code and data as well as metadata;
code+data denotes a size of both code and data segments in a binary.
metadata denotes a size of symbal, relocation, and string table in a
binary.
instruments many memory accessing instructions. This further
increases the number of executed instructions by 30.55% and
34.79% for RU-64 and RU-32, respectively. The results show
that DEP and SFI have a stronger impact on the number of
executed instructions.
In terms of memory overhead, SGX-Shield actually imposes
fixed overhead due to over-estimation. More precisely, in the
current version of SGX-Shield, it imposes total 64 MB memory
overheads (i.e., 32 MB for code pages and 32 MB for data pages).
Looking into more detail on possible factors of memory
overhead, while SGX-Shield preserves the size of data objects,
it enlarges the size of code due to the randomization unit-level
ASLR and software-DEP. Particularly, ASLR also increases
the size of metadata including fine-grained symbol, string, and
relocation table entries. Table VII shows that the increased
binary size is mainly from more metadata.
Based on these evaluation results, we recommend that RU-
64 with DEP and SFI would be a reasonable configuration. The
address space layout showed fairly good randomness compared
to RU-32, and its runtime performance is 7.82% faster than
RU-32.
2) Macro-benchmark: In order to see how SGX-Shield
would work with real-world workloads, we ran an HTTPS
server within an enclave. We ported mbedTLS [4], which
is an open source Transport Layer Security (TLS) library.
mbedTLS also includes a sample HTTPS server, where its work
process can be broken into the following two parts: (1) SSL
handshaking, an initial and fixed cost for a request and (2)
reading an HTML file and then sending it to the client. mbedTLS
12
Fig. 9: Performance overheads in running an HTTPS server with
mbedTLS. Each bar represents the slowdown factor (%) of SGX-Shield
compared to the baseline (i.e., Intel SGX SDK for Linux): the bar
on the left (marked as A) shows the slowdown in the total elapsed
time for a request, and the bar on the right (marked as B) shows the
slowdown in the elapsed time for SSL handshaking. We ran 50 times
and report the median value. The median of the total elapsed time
in the baseline is 1.1 second, while the one for SSL handshaking is
0.359 second. The average relative standard deviation is 3.35% (the
maximum is 5.83%).
provides authentication and key sharing mechanisms for SSL
handshaking, and it also encrypts (or decrypts) messages on
sending (or receiving). We ran this HTTPS server, which serves
the HTML file of 12KB size. The average round-trip time (RTT)
between the server and the client was 175.5 ms, an average of
50 times running a ping command with 0.32% average relative
standard deviation.
Figure 9 shows the overheads of requesting the HTML
file from the HTTPS server. We computed the median of 50
times of the requests and plotted the slowdown factor as a
percentage between the baseline (i.e., Intel SGX SDK for
Linux) and SGX-Shield: the first bar (marked as A) represents
the slowdown in the total request, and the second bar (marked
as B) represents the slowdown during the SSL handshaking.
Overall, SGX-Shield imposed negligible overheads in the
total request time, from 1.8% to 2.7%, depending on the setting.
This performance number is much better compared to the micro-
evaluation results, and we suspect this is because the total
request time is dominated by the network latency, which is
independent of SGX-Shield’s extra work with instrumentation.
This can be supported by the increased slowdown numbers
in SSL handshaking, which is less related to the network
operations and more related to computational jobs, ranging from
3.1% to 7.6%. Similar to the micro-evaluation results, enabling
DEP incurred more slowdowns due to its extra instrumentation
for software-based DEP — 0.4% increases in RU-64 and 1.4%
increases in RU-32. According to these results, we believe
SGX-Shield would be fast enough to run realistic workloads
for SGX while providing ASLR security guarantees together.
VII. DISCUSSION ON CONTROLLED SIDE-CHANNEL
ATTACKS
The controlled side-channel [61] allows an attacker to infer
data values at runtime by observing coarse-grained execution
flows (i.e., a sequence of page faults). If an application running
within an enclave exhibits control-flow dependencies relying
on specific data values, this attack is indeed possible because
Total Request Time (A)SSL Handshaking (B)RU­64 ASLRRU­64ASLR&DEPRU­32 ASLRRU­32ASLR&DEP0%2%4%6%8%2.72.71.81.87.66.23.53.1Slowdown factor (%)page resources are managed by adversaries (i.e., the kernel) in
the SGX model.
While this attack is not directly related to breaking ASLR, it
can still be applied to partially infer address layout information.
In other words, although the kernel does not know memory
layouts after in-enclave loading, it may infer some layout
information by observing (or intentionally triggering) page
faults. For example, assume that there are four code objects (A,
B, C, and D) and a global variable (v), and A has the branch: if
v is 1, jump to B; if v is 0, jump to C, and then jump to D. In
this case, if the attacker observes the number of page faults as
2, she/he may conclude that x is 1. On the other hand, if the
number of page faults is 3, she/he may conclude that x is 0.
SGX-Shield’s design decisions on randomization units
effectively thwart this side-channel attack, as shown in §VI-A1.
Specifically, because SGX-Shield randomly places multiple
randomization units in a memory page, when the size of the
randomization unit is 32 bytes, an attacker needs to bruteforce
27 times (i.e., 4KB/32bytes = 212/25, as the memory page size
is 4 KB) to guess a single address value, while each failure
would end up crashing a target enclave program. Considering
the number of address values that need to be guessed in
practical memory corruption exploits (e.g., three or more in
ROP [17, 18, 60]), we believe this probabilistic defense against
the controlled side-channel would be effective and reasonable
in practice.
VIII. RELATED WORK
Secure systems based on Intel SGX. The early adoption
of Intel SGX focused on the cloud environment to cover
security problems from an untrusted cloud platform (e.g., cloud
provider). Haven [12] is a system to securely run the entire
library OS (LibOS) in an enclave as a guest OS to prevent access
from untrusted software with a malicious purpose. Similar to
SGX-Shield, the LibOS loads the actual target programs at
runtime. However, it does not support ASLR, which leads to
several threats as mentioned in §IV-A. Additionally, the TCB
of Haven is very large (more than 200MB) because of the
nature of OS, while the TCB of SGX-Shield is only 8KB
(1821 instructions in the x86-64 assembly). To reduce the TCB
size of the libOS approach, Scone [9] suggests the container
based sandbox in an enclave.
Starting from the cloud environment, the systems and
frameworks that apply Intel SGX to enhance security have
been proposed. VC3 [46] is a secure data processing framework
based on the Hadoop framework [1] to keep the confidentiality
and integrity of the distributed computations on the untrusted
cloud. VC3 suggests a self-integrity invariant that prevents
an enclave program from reading from or writing to the
non-enclave memory region. It basically aims to avoid data
leakages and memory corruptions. This technique is similar
to software-DEP or SFI of the SGX-Shield, but the one of
SGX-Shield aims to prevent an attack from injecting code to
execute. Ryoan [26] also adopts SFI in SGX to guarantee data
privacy between multiple distrusted parties. The S-NFV system
architecture [52] proposed a new design of the secure Network
Function Virtualization (NFV) system based on the Intel SGX.
Kim et al. [33] suggest security enhancements of the network
systems such as software-defined inter-domain routing and
peer-to-peer anonymity networks by adopting the Intel SGX.
OpenSGX [31] is a software platform that emulates the
SGX hardware and provides basic software components (e.g.,
system call interface and debugging support) and toolchains.
Security issues of Intel SGX. While Intel SGX provides
the protection of the program against access from privileged
software and hardware, several studies argue the vulnerabil-
ities of the enclave program as an open problem. Attackers
can perform various types of Iago attacks [19] through the
communication channel between the enclave program and the
external world. Moreover, potential side-channels [61] (e.g.,
page fault) exist that helps the untrusted privileged software
to guess the secret data of the program. To address this
problem, Shinde et al. [54] designed a defense mechanism
that enforces a program to access its input-dependent pages
in the same sequence regardless of the input variable. This
approach, called deterministic multiplexing, ensures that the OS
cannot distinguish the enclave execution.Since the performance
overhead of Shinde et al. [54] is too expensive in practice (4000