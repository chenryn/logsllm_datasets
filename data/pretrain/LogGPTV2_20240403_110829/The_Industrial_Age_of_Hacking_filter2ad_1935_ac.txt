### Detailed Schedule and Binary Selection

A detailed schedule is provided in Appendix B. We selected fifteen binaries from three popular public corpora of fuzzing targets: the Trail of Bits adaptations of the Cyber Grand Challenge binaries [38], the MIT Lincoln Laboratory Rode0day bug-injection challenges [12], and Google’s OSS-Fuzz project [32]. In some cases, we provided the source code. By the end of the study, each participant had investigated all fifteen binaries across three skill assessments. The list of binaries is available in Appendix F. These binaries represent a variety of practical challenges, including small versus large programs, pre-built versus complex build systems, and artificial versus natural bugs.

### Availability and Transparency

All the targets used in our skill assessments are freely available on the Internet. Additionally, an "answer key" for each target is available online, which may include a list of known bugs and, in some cases, a pre-built fuzzing harness. Our intention was to emphasize that open-source research is a key component of the vulnerability-discovery process and to acknowledge the importance of known n-day vulnerabilities.

### Time Constraints and Objectives

Participants were given exactly one hour to work on these targets, which was clearly insufficient for a deep dive into any of them. Their instructions emphasized two primary goals: (1) finding bugs and (2) creating fuzzing harnesses. The motivation for finding bugs is self-evident, as it aligns with the broader goal of vulnerability research. The goal of creating fuzzing harnesses is to encourage participants to use automation as a primary strategy for achieving the first goal.

### Target Selection

Selecting targets for this experiment was challenging. Klees et al. [20, §8] describe the difficulties in selecting targets to evaluate a fuzzing tool. We encountered similar challenges when evaluating our participants. After considering benchmarks from earlier work [12, 15, 20, 38], we decided to evaluate OpenWrt [10]. The packages available in OpenWrt are open source and serve diverse purposes, making them representative of modern, complex, and deployed software.

Before the participants began the vulnerability-discovery process, we ran a simple static analysis script to extract important information from every OpenWrt package. This included the package version, a listing of exported files, the results of running `file` [19, p. 46] on each item, and the intersection of each ELF file's exported symbols with a set of frequently misused standard library functions such as `strcpy` and `gets`.

For Software Diversification (SD), we selected two targets: `dropbear` and `uhttpd`. These services are installed and listening on a network socket by default, making them likely choices for a hacker performing SD. For Software Breadth (SB) targets, participants could select any software provided by the OpenWrt package manager, except for `dropbear` and `uhttpd`, to ensure both teams started fresh on those targets during SD. While this asymmetry might initially seem unfair, we argue that it reflects the real-world effectiveness of SB over SD, as committing to a single target is not the most efficient way to find bugs.

### Version Selection

To aid post-study analysis, we selected a four-year-old version of OpenWrt: 15.05.1. As others have noted [20], there is no good substitute for real bugs found. Unique crashes, as defined by program path or stack hash, do not necessarily correlate to unique bugs. By choosing an older version of OpenWrt, we hoped that participants would find bugs that were patched by the modern release (version 18.06.5). This allowed us to categorize crashes more precisely. Since all targets are open source, we will use their issue trackers to report any crashes still present in the modern version.

### Workflow and Tools

Both strategies, SD and SB, require tools to manage the vulnerability-discovery process. We used GitLab to manage our teams due to its feature set and open-source availability. For each vulnerability-discovery campaign, we created a GitLab project, and for each proposed target, we created a GitLab issue. We added the package information derived from our scripts to each issue's text.

Participants were directed to track their progress using a GitLab issue board, divided into lists related to each step in the vulnerability-discovery process. Each team's board contained lists for open, information gathering, program understanding, exploration, and journeyman. Many authors, including Newport [25], note the need for experts to be minimally interrupted, so we balanced the need for concentration, tracking progress, and recording important information. Participants were asked to:

- Drag a ticket from open to information gathering upon initiating work on a target.
- Append relevant articles, blogs, source repositories, corpora, and other information uncovered during their search.
- Move an issue from information gathering to program understanding once they create products worthy of committing to the target’s GitLab repository.
- Move an issue to the exploration list upon creating a working fuzzing harness.
- Move an issue to the journeymen list if progress becomes too difficult, with comments explaining the obstacles encountered.

Each participant was provided with an Internet-connected workstation equipped with tools such as Ghidra [18], AFL [42], Honggfuzz [37], Docker [3], and Mayhem [5]. We also provided a Docker container that emulates the OpenWrt 15.05.1 filesystem and services (adapted from other work [35]).

### Execution

Our experiment involved two iterations of the vulnerability-discovery process. During the first iteration, Team A applied SD, and Team B applied SB. Roughly every hour, we stopped work and asked participants to complete a survey (Appendix C). The teams then traded strategies for the second iteration, and we repeated the skill assessment after each iteration. Each day ended with an end-of-day survey (Appendix D), and the final day included an end-of-experiment survey (Appendix E).

For the next four business days, participants worked in their assigned strategy under the guidance of an investigator. We enforced the use of the assigned strategy by selecting only two targets for SD and approximately 1,000 targets for SB. The team lead encouraged SB participants to give up quickly and select targets they could reasonably accomplish in two hours of work. We gave participants an intermediate skill assessment before they traded strategies for the final four business days. On the final day, participants took the final skill assessment.

### Limitations

Our sampled population consisted solely of US Cyber Command personnel, but we posit that our results are applicable to other organizations. Both teams knew the software they would target for both weeks using our two strategies, which could have resulted in looking ahead at future targets. However, team leads mitigated this by focusing work. Our two team leaders also served as investigators and tried to mitigate any bias towards SB.

Other aspects of our study were difficult to control. Some participants missed work due to unforeseen emergencies, although the collective time for both teams appeared to be about equal. At times, our Internet connection became prohibitively slow, affecting both teams. Sometimes, participants' workstations would crash from unwieldy fuzz jobs, impacting our ability to collect and log data about their actions. We also discovered that our X11 monitoring tool did not capture time spent in the X11 lock screen.

### Human Research Standards and Data Collection

We obtained a DoD Human Research Protection Program (HRPP) determination before executing the research described in this paper. This included an examination by our Institutional Review Board (IRB). All recruitment was voluntary and minimized undue influence. Each participant was assigned a two-word pseudonym that was also their machine’s host name, Rocket.Chat user name, survey response name, and GitLab user name. Recorded data bore this pseudonym and was not linked to the participant’s real name. We collected skill assessments, surveys, GitLab commits, comments, and work products. We also collected data using `execsnoop`, which logged programs started by the participants, and `x11monitor`, which monitored the participants’ X11 cursor focus.

### Results

#### Surveys

We used the Mann-Whitney u-test p-value (MW) to compare the means of survey responses. This test is non-parametric and allows us to test the signed difference of means between two groups: SD and SB. B is the Bernoulli Trial as described by Papoulis et al. [30]. We assumed our sample of 12 was "large enough" and used an acceptance criteria of 0.020.

Hourly survey outcomes: When comparing subjects from both teams during the first week, those performing SB felt less surprised (MW=0.003), less frustrated (MW=3×10−4), and less doubtful (MW=0.004) than those performing SD. They also spent more time interacting with tools (MW=5×10−7) and more time harnessing (MW=0.002).

After the second week, we compared within-subjects on the team that transitioned from SD to SB. These subjects reported that SB left them spending less time on research (MW=1×10−4) and feeling less frustrated (MW=0.007), doubtful (MW=0.001), and confused (MW=0.009). SB found them interacting with tools (MW=0.008) and harnessing (MW=0.009) more.

End-of-experiment outcomes: Subjects felt SD was less effective than SB overall (B=0.019) and was a less effective use of their team’s skills (B=0.003). When asked which method they would prefer to lead, subjects were less likely to choose SD (B=0.003). Subjects felt breadth-first work was more independent but left them feeling less part of a team (B=0.003). The subjects claimed SB was less frustrating (B=0.003), and they unanimously said it was easier to get started with (B=2.400×10−4) and easier for a novice to contribute to (B=2.400×10−4). Subjects also unanimously claimed they learned something during the experiment (B=2.400×10−4). Subjects felt more prepared (MW=0.010) and more interested (MW=0.015) in hacking after the experiment than before. Every participant reported finding at least one bug (B=2.400×10−4).

#### Determining Number of Bugs

As Klees et al. discuss, many papers fail to provide control for randomness in fuzzing results [20]. Our approach was to collect subject harnesses and run each in three independent trials for 24 hours using the corpora and fuzzer selected by the harness creator. While Klees et al. also discuss finding "real bugs," the process of iteratively patching is extensive and time-consuming. As a compromise, we used the bug de-duplication mechanism in Mayhem [1, 5].

Statistical tests: We used MW to test the significance of mean difference in coverage and bug metrics. After using a total of 18,432 compute-hours to test each harness three independent times for 24 hours and two cores each, we collected the results. The following table shows the cumulative number of unique bugs found in each independent fuzzing trial Tx.

| Team | Method | Harnesses | T0 | T1 | T2 |
|------|--------|-----------|----|----|----|
| A    | SD     | 3         | 31 | 2  | 3  |
| A    | SB     | 42        | 42 | 23 | 40 |
| B    | SB     | 8         | 4  | 4  | 4  |
| B    | SD     | 61        | 12 | 8  | 4  |

Testing f (SD) < f (SB) with a p-value of (0.002 < 0.020).

In addition to finding more bugs, the categories of bugs found by SB were significantly more diverse and security-related than those found by SD. Both SB sessions found multiple out-of-bounds write primitives as described in the Common Weakness Enumeration (CWE) database [23], while none were found by SD. Both strategies found out-of-bounds reads [22], but SB found significantly more, some of which could lead to information disclosure. For bug-bounty hunters, this is important because bug criticality determines compensation [28].

#### Skill Assessment

After each assessment, we collected the participants' work products and notes and graded them to determine three objective measures: (1) number of working harnesses, (2) number of bugs found, and (3) number of bugs reproduced. A fuzzing harness was defined as working if, after a short while, it discovers new paths through the target program. A bug was defined as any program terminated by a signal that might result in a core dump, such as SIGABRT, SIGFPE, SIGILL, SIGSEGV, and SIGTRAP. Reproducing a bug required the participant to successfully run the program with the crashing input.

After collecting each objective measure, we combined them into a single score for each participant for analysis. We chose to weight each category equally, given the large scope of this study. A participant’s score is the sum of all measures: h + b + r.

Statistical tests and outcomes: Our assessment of participants before the study and after each strategy makes for a good candidate for the Friedman signed-rank test [33]. We chose this test because it does not require an assumption about the underlying distribution of our data. The Friedman test revealed no statistically significant mean difference between the three assessments. When testing all twelve participants, we received a p-value of 0.02024; for group one, 0.10782; and for group two, 0.12802. A larger sample of participants might reveal more significant results.

#### Ancillary Data

Browsing the web vs. strategy: Dividing the work time into hour-long windows to bin time spent with the X11 focus on Firefox (the pre-installed web browser) and grouping the values by strategy SD or SB was not significant according to the Wilcoxon signed-rank test [41]. The number of entries in Firefox’s history and the team’s strategy were also not significantly related.

Materials produced: Figure 5 describes the number of materials produced by both teams under both strategies. Both teams produced more materials under SB than SD: Team A produced 151 and 588 products under SD and SB, respectively; and Team B produced 177 and 387 products under SD and SB, respectively.

### Depth-First Strategy Discussion

This section, along with §5.6, records observations made during the experiment.