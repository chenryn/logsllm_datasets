a detailed schedule in Appendix B
from a pool of ﬁfteen. We took these binaries from three
popular public corpora of fuzzing targets: the Trail of Bits
adaptations of the Cyber Grand Challenge binaries [38],
the MIT Lincoln Laboratory Rode0day bug-injection chal-
lenges [12], and Google’s OSS-Fuzz project [32]. In some
cases, we provided source code. By the end, each subject
had investigated all ﬁfteen binaries over the course of three
skill assessments. We list the binaries in Appendix F. The
binaries we selected represent a variety of practical chal-
lenges varying across a number of dimensions, including
small versus large programs, pre-built versus complicated
build systems, and artiﬁcial versus natural bugs.
Each of the targets employed in our skill assessments
is freely available on the Internet. Also available on the
Internet is an “answer key” for each target including,
in some cases, a list of bugs and, in other cases, a
pre-built fuzzing harness. Our intention was to emphasize
that open source research is a key component of the
vulnerability-discovery process and to acknowledge that
known n-day vulnerabilities matter.
Subjects were given exactly one hour to make progress
on these targets; clearly not enough time for a deep-dive
into any of them. Their instructions emphasized two
goals: (1) ﬁnd bugs and (2) create fuzzing harnesses.
The motivation for ﬁnding bugs is self-evident, as it
aligns with the goal of vulnerability research in general.
The reason for the goal of creating fuzzing harnesses is
to put subjects in the mindset of using automation as a
primary strategy for achieving the ﬁrst goal.
Target selection Selecting targets for this experiment
was no easy task. Klees, et al. describe how selecting
targets to evaluate a fuzzing tool is difﬁcult [20, §8]. We
encountered many of the same challenges when evaluat-
ing our hackers. After considering using the benchmarks
in earlier work [12, 15, 20, 38], we decided on something
else altogether. We chose to evaluate OpenWrt [10]. The
packages available to OpenWrt are open source and serve
diverse purposes. Each of our targets was real and thus rep-
resentative of modern, complex, and deployed software.
Before the subjects began the vulnerability-discovery
process, we ran a simple static analysis script that
extracted some important
information from every
OpenWrt package. We collected each package’s version,
a listing of the ﬁles exported by the package, the results of
running file [19, p. 46] on each item in the package, and
the intersection of each ELF ﬁle’s exported symbols with
a set of frequently misused standard library functions
such as strcpy and gets.
For SD, we selected two targets: dropbear and uhttpd.
Because these services are installed and listening on a
network socket by default, they represent the most likely
choices for a hacker performing SD. For SB targets, we
allowed subjects to select any software the OpenWrt pack-
age manager provides, except for dropbear and uhttpd.
We excluded those two during SB so that both teams would
start fresh on those targets during SD. Two targets for SD
and a thousand for SB does present an asymmetry; upon
ﬁrst inspection, this might appear unfair, as (1) the true
number bugs in the underlying targets is biased and (2) the
two SD targets require more skill to analyze than the av-
erage of the SB targets. Thus the reader might claim, “of
course SB can ﬁnd more bugs, there are more bugs to ﬁnd!”
We agree. We argue this perceived unfairness is really
intuition that SB is more effective than SD, because our
selections represent real systems. Bugs exist, but over com-
mitting to a single target is not the easiest way to ﬁnd them.
In order to aid the post-study analysis, we selected a
four-year-old version of OpenWrt: 15.05.1. As others
mention [20], there is no good substitute for real bugs
found. Unique crashes as deﬁned by program path or
stack hash do not correlate to unique bugs. By choosing
USENIX Association
29th USENIX Security Symposium    1135
an older version of OpenWrt, we hoped that subjects
would ﬁnd bugs that were patched by version 18.06.5, the
modern release as of our experiment. This way, we could
take crashes and categorize them more precisely. Because
all targets are open source, we will use their issue trackers
to report crashes still present in the modern version.
Work ﬂow and tools Both strategies, SD and SB,
require tools to manage the execution of the vulnerability-
discovery process. We spent time during the orientation
describing these tools and the manner of their use.
We relied on GitLab to manage our teams due its
feature set and open-source availability. For each
vulnerability-discovery campaign, we created a GitLab
project, and for each proposed target we created a GitLab
issue. We added the package information derived from
our scripts to each issue’s text.
We directed our subject teams to track their progress us-
ing a GitLab issue board, divided into lists related to each
step in the vulnerability-discovery process. Each team’s
board contained one list (as deﬁned in [14]) for each of
open, information gathering, program understanding,
exploration, and journeyman. We depict a snapshot of
one such board in Figure 4. Many authors, including
Newport [25], note the need for experts to be minimally
interrupted, and this is why we did not include every step
of our vulnerability-discovery process in our issue boards.
Instead, we attempted to balance our subjects’ need for
concentration, our own need to track progress, and the
teams’ need to record important information. We felt a
reasonable compromise would ask subjects to:
• drag a ticket from open to information gathering
upon initiating work on a target;
• append to an issue relevant articles, blogs, source
repositories, corpora, and other information uncov-
ered during their search;
• move an issue from information gathering to pro-
gram understanding once they create products wor-
thy of committing to the target’s GitLab repository;
• move an issue to the exploration list upon creating
working fuzzing harness; and
• move an issue to the journeymen list if progress be-
comes too difﬁcult. In this case, comments will ex-
plain the obstacles encountered.
We gave each subject an Internet-connected work-
station co-located with their team members. The
workstations contained tools for our subjects, including:
Ghidra [18], AFL [42], Honggfuzz [37], Docker [3],
and Mayhem [5]. Each workstation also contained
monitoring software and was thus tied to our data
collection. We further allowed the subjects to use any
bug-ﬁnding tool they desired, but we encouraged them
to use dynamic-analysis tools. We also provided subjects
a Docker container that emulates the OpenWrt 15.05.1
ﬁlesystem and services (adapted from other work [35]).
4.3 Execution
Our experiment
involved two iterations of our
vulnerability-discovery process. During the ﬁrst it-
eration, Team A applied SD, and Team B applied SB.
Roughly each hour, we stopped work and asked the sub-
jects to complete a survey (Appendix C). The teams traded
their strategies for the second iteration, and we repeated
the skill assessment after each iteration. Each day ended
with an end-of-day survey (Appendix D), and the ﬁnal
day included an end-of-experiment survey (Appendix E).
For the next four business days, subjects on each
team—lead by an investigator—worked in their assigned
strategy. We enforced that each group use their assigned
strategy by selecting only two targets for SD and approx-
imately 1,000 targets for SB. The team lead encouraged
SB subjects to give up quickly and select targets that they
could reasonably accomplish in two hours of work. We
gave subjects the intermediate skill assessment before
they traded strategies for the ﬁnal four business days. On
the ﬁnal day, subjects took the ﬁnal skill assessment.
Limitations Our sampled population consisted solely
of US Cyber Command personnel, but we posit our
results are applicable to other organizations. Both teams
knew on day one the software they would target for both
weeks using our two strategies; this could have resulted in
looking ahead at a future target, but team leads mitigated
this by focusing work. Our two team leaders did double as
investigators, but they tried to mitigate any bias towards
SB as they guided their teams.
Other aspects of our study were difﬁcult if not
impossible to control. Some subjects missed work due
to unforeseen emergencies, although the collective time
for both teams appeared to be about equal. At times,
our Internet connection became prohibitively slow. This
affected both teams and seemed to persist during both
weeks of the study. Sometimes subjects would restart
their workstation or it would crash from an unwieldy fuzz
job. This affected our ability to collect and log data about
the participant’s actions. We also discovered during the
experiment that our X11 monitoring tool did not capture
1136    29th USENIX Security Symposium
USENIX Association
Target
Information gathering
Program understanding
Attack surface
Automated exploration
Promote to journeyman
Figure 4: The use of Gitlab to track the progress of a vulnerability-discovery campaign; we used a variant of Kanban
with bins that corresponded to groups of steps in our vulnerability-discovery process; each issue corresponds to a target
time spent in the X11 lock screen.
Human research standards and data collection
We obtained a DoD Human Research Protection Pro-
gram (HRPP) determination before executing the research
described by this paper. This included an examination by
our Institutional Review Board (IRB). All recruitment
was voluntary and minimized undue inﬂuence. We as-
signed each subject a two-word pseudonym that was also
their machine’s host name, their Rocket.Chat user name,
their survey response name, and their GitLab user name.
Recorded data bore this pseudonym, and it was in no way
linked to the subject’s real name. We collected skill assess-
ments, surveys, GitLab commits, comments, and work
products. We also collected data using execsnoop, which
logged programs started by the subjects, and x11monitor,
which monitored the subjects’ X11 cursor focus.
5 Results
Our analysis of the experiment’s results involves four
categories: survey questions, determining the number
of bugs found, measuring the subjects’ hacking skill,
and ancillary data. We present this analysis here before
commenting on our two strategies.
5.1 Surveys
We use Mann-Whitney u-test p-value (MW). That is, the
probability that the statement listed is not true given our
observation. We use this test to compare the means of
survey responses and conform to the necessary assump-
tions [24, §1.2] except that each entry is an independent
trial. This is violated because we sample each subject mul-
tiple times over the course of each method. We expect there
is variation within a single subject’s responses and thus
we conducted multiple samplings. Potentially, some other
tests such as repeated measures ANOVA [17] or Wilcoxon
signed-rank test [41] are more ﬁtting, but not quite right
and not the focus of this paper. We choose Mann-Whitney
mainly because it is a non-parametric test with minimal
assumptions about the data’s distribution and allows us to
test the signed difference of means between two groups:
SD and SB. B is the Bernoulli Trial as described by Pa-
poulis et al. [30]. We must assume our sample of 12 is
“large enough”. To balance the number of tests with our
small sample, we use an acceptance criteria of 0.020.
Hourly survey outcomes When comparing between
subjects from both teams during the ﬁrst week, subjects
performing SB felt less surprised (MW=0.003), less frus-
trated (MW=3×10−4), and less doubtful (MW=0.004)
than those performing SD. They also spent more time
interacting with tools (MW=5×10−7) and more time
harnessing (MW=0.002).
After the second week, we compared within-subjects
on the team that transitioned from SD to SB. These
subjects reported that SB left them spending less time
on research (MW=1×10−4) and feeling less frustrated
(MW=0.007), doubtful
(MW=0.001), and confused
(MW=0.009). SB found them interacting with tools
(MW=0.008) and harnessing (MW=0.009) more.
End-of-experiment outcomes Subjects felt SD was
less effective than SB overall (B=0.019) and was a less
effective use of their team’s skills (B=0.003). When asked
which method they would prefer to lead, subjects were less
likely to choose SD (B=0.003). Subjects felt breadth-ﬁrst
work was more independent but left them feeling less a
part of a team (B=0.003). The subjects claimed SB was
less frustrating (B=0.003), and they unanimously said it
was easier to get started with (B=2.400×10−4) and easier
for a novice to contribute to (B=2.400×10−4). Subjects
also unanimously claimed they learned something during
the experiment (B=2.400×10−4). Subjects felt more
prepared (MW=0.010) and more interested (MW=0.015)
USENIX Association
29th USENIX Security Symposium    1137
in hacking after the experiment than before. Every partic-
ipant reported ﬁnding at least one bug (B=2.400×10−4).
5.2 Determining number of bugs
As Klees et al. discuss in depth, many papers fail to
provide control for randomness in fuzzing results [20].
Our approach was to collect subject harnesses and run
each in three independent trials for 24 hours using the
corpora and fuzzer selected by the harness creator. While
Klees et al. also discuss ﬁnding “real bugs,” the process
of iteratively patching is extensive and time consuming.
As a compromise, we settled on an approximation. In lieu
of “real bugs,” we decided to use the bug de-duplication
mechanism in Mayhem [1, 5].
Statistical tests We use MW to test the signiﬁcance of
mean difference in coverage and bug metrics and conform
to all required assumptions [24, §1.2]. We chose this test
to measure the difference in bugs found by SD and SB
primarily for the reasons suggested by Klees [20, §4].
Bug outcomes After using a total 18,432 compute-
hours to test each harness three independent times for 24
hours and two cores each, we collected the results. The
following table shows the cumulative number of unique
bugs found in each independent fuzzing trial Tx.
Team Method Harnesses T0
3
31
42
4
8
42
61
12
A
A
B
B
SD
SB
SB
SD
T1
2
23
49
4
T2
3
40
40
4
Testing f (SD)  0.020); Within-subjects for team B,
(0.032 > 0.020). For the between-subject test of week
one, (0.032 > 0.020). However, combining both team’s
ﬁndings, we ﬁnd signiﬁcant evidence to claim f (SD) <
f (SB) with a p-value of (0.002 <0.020).
In addition to ﬁnding more bugs, the categories of
bugs found by SB are signiﬁcantly more diverse and
security-related than the bugs found in SD. Both SB
sessions found multiple out-of-bounds write primitives as
described in the Common Weakness Enumeration (CWE)
database [23], while none were found by SD. Both
strategies found out-of-bounds reads [22], but SB found
signiﬁcantly more and some that could lead to information
disclosure. For bug-bounty hunters, this is important
because bug criticality determines compensation [28].
5.3 Skill assessment
After each assessment, we collected the subjects’ work
products and notes and graded them with the goal of
determining three objective measures: (1) number of
working harnesses, (2) number of bugs found, and
(3) number of bugs reproduced. We deﬁned a fuzzing
harness as working if, after a short while, it discovers new
paths through the target program. We deﬁned a bug as any
program terminated by a signal that might result in a core
dump. Some commonly-encountered examples include:
SIGABRT, SIGFPE, SIGILL, SIGSEGV, and SIGTRAP.
Finally, it is possible for a subject to ﬁnd a bug—either
through static analysis or information gathering—but not
reproduce it. Reproducing a bug requires the subject to
successfully run the program with the crashing input.
After collecting each objective measure, we combined
them into a single score for each participant for the
purpose of analysis. While one could imagine assigning
differing weights to each category, those weights would
likely be chosen based on model ﬁtting from training data.
Perhaps a future researcher might use data from sources
like HackerRank [39]. Given the large scope of this study,
we chose to weight each category equally. A participant’s
score, then, is the sum of all measures: h+b+r.
Statistical tests and outcomes Our assessment of
subjects before the study and after each strategy makes for
a good candidate for the Friedman signed-rank test [33].
We chose this test over others such as repeated-measures
ANOVA [17] because this test does not require an
assumption about the underlying distribution of our data.
In our case, this is important because we neither know
the distribution of test scores nor think it reasonable
to assume the distribution is normal. We again use an
acceptance criteria of 0.020.
The Friedman test unfortunately revealed no statis-
tically signiﬁcant mean difference between the three
assessments. When testing all twelve participants, we
receive a p-value of 0.02024; for group one, 0.10782;
and for group two, 0.12802. A larger sample of subjects
might reveal more signiﬁcant results.
5.4 Ancillary data
Browsing the web vs. strategy Dividing the work
time into hour-long windows to bin time spent with the
X11 focus on Firefox (the pre-installed web browser)
and grouping the values by strategy SD or SB was not
signiﬁcant according to Wilcoxon signed-rank test [41].
The number of entries in Firefox’s history and the team’s
strategy were also not signiﬁcantly related.
1138    29th USENIX Security Symposium
USENIX Association
Materials produced Figure 5 describes the number of
materials produced by both teams under both strategies.
Both teams produced more materials under SB than SD:
Team A produced 151 and 588 products under SD and SB,
respectively; and Team B produced 177 and 387 products
under SD and SB, respectively.
5.5 Depth-ﬁrst strategy discussion
This section, along with §5.6, records observations made