2.36615
387.34
8.8351
33.1626
Power p-value
245.9
0.0016
0 0011
236 4
236.4
0.0011
0.0767
230.1
0.0012
223.5
217.7
0.0017
Freq.
0.0327
0 4226
0.4226
0.0025
0.1131
0.0301
Fig. 5. Permutation based ﬁltering
Fig. 6. Pruning using statistical features (TDSS bot)
based ﬁltering algorithm proposed in [25]. The main idea is
to identify how much of the signal energy can be attributed to
non-periodic or random mechanisms; then the only frequencies
of interest would be those with power higher than ones derived
from random processes. Given a time series sequence x(n),
the algorithm creates the random permutation x(cid:2)(n) of all N
elements of x(n). This shufﬂing process would destroy any
periodic patterns in x(n) while the ﬁrst order statistics and
characteristics not related to timing persist (e.g., amplitude).
information exhibited in x(cid:2)(n) is
Therefore any structural
likely from random noises and should be discarded.
Based on this observation, we perform the same perido-
gram analysis on the permuted signal x(cid:2)(n), record the maxi-
mum power in its frequency domain (denoted as pmax(x(cid:2)))
and use it as the threshold to identify potential candidate
frequencies in the original x(n). In other words, if a frequency
in x(n)’s periodogram carries less power than the threshold,
it is likely due to random noises and thus is eliminated from
the candidate frequency set.
One permutation may not completely destroy all periodic
structures. To provide a reasonable conﬁdence level C on the
power threshold, we repeat the above permutation process m
times, and use (C × m)-th highest power frequency (e.g., 19th
when C = 95% and m = 20) of pmax(x(cid:2)) as a good estimator
for the power threshold pT . The process is illustrated in Fig. 5.
The result of this process is a set of most promising
frequencies (denoted as F = {F1, F2, ...}) that potentially
correspond to true periodicities of x(n). If the set is empty, i.e.,
none of the frequency exceed the threshold, the original time
series x(n) will be rejected and considered as non-periodic.
C. Step 2: Pruning
After obtaining a set of candidate frequencies F and their
corresponding periods (denoted as P = {P1, P2, ...}) in the
time domain, the next step is to further ﬁlter out less feasible
candidates, and reduce the number of inputs to the more
expensive veriﬁcation step.
High-Frequency Noise. During our analysis, we observed
that candidate sets often include high-frequency noise com-
ponents. To identify such cases, we convert the timestamps
i.e., I = {i1, i2, ..., in−1}, where
T into an interval
ik = tk+1 − tk is the time interval between two consecutive
connections (Fig. 6 (a)). Analyzing the statistics of the interval
list often allows us to further prune impossible or unlikely
candidates. For example, any candidate period Pk ∈ P that
is smaller than the minimum interval, i.e., Pk < min(I),
list,
Interval List
[1,180,6,175,5,175,5,174,6,174,6,
174,6,174,6,175,5,175,5,175,5,17
4,6,174,6,174,6,175,3,2,175,4,176
,5,175,5,175,5,184,1,2,168,5,175,
5,175,5,176,4,175,5,175,5,175,5,1
75,4,176,5,175,1,4,175,5,175,5,17
6,5,175,4,176,5,175,5,175,5,175,5
,175,5,175,5,180,5,170,3,2,82,4]
GMM
analysis
GMM Components
Mean 
Weights
Data & Best Fit GMM
175.12
0.46
4.51
0.53
C
BIC vs. # components
82
0.01
Fig. 7. GMM for detecting multiple periods
is due to high frequency noise and can be removed from
the candidate set. Figure 6 shows an example of the TDSS
botnet trafﬁc and its interval list where the minimum interval
is 196 seconds. Therefore among all the candidate periods from
peridogram analysis in step 1, only 387.34 is larger than the
minimum interval and all others can be safely removed from
the candidate set.
Hypothesis Testing. Extending the threshold-based ap-
proach, we design a hypothesis testing algorithm to determine
the statistical signiﬁcance of the candidate periods given the
observed intervals. Given candidate period P ∈ P , we con-
struct our null hypothesis H0: P is the true period of the
original signal. Due to various noises in the real world scenario
(e.g., network delays, retransmission, context switching, etc.),
the observed intervals may not be exactly P. Instead, we can
model the distribution of the observed intervals as a random
variable N (P, σ2), i.e., the intervals are generated from an
underlying normal distribution with mean/variance (P, σ2).
The goal of hypothesis testing is to decide, given a pre-
deﬁned signiﬁcance level α and the set of observations,
the null hypothesis H0 in favor of the
whether to reject
alternative hypothesis H1: P is not the true period. In our
case, we applied a one-sample t-test on the observed intervals
and calculated the p-value. We reject P and remove it from
the candidate set if the p-value is smaller than the chosen
signiﬁcance level α = 5%. Take the TDSS botnet case in
Figure 6 as an example. All candidate periods except 387
seconds were pruned since their p-values are all considerably
smaller than 5%.
One of the major beneﬁts of applying hypothesis testing in
the pruning step is its conservativeness, i.e., the null hypothesis
is rejected only when there is signiﬁcant evidence against it.
Sampling Rate. Another pruning criterion is the sampling
rate. We ﬁlter out time series that are under-sampled. This
is particularly important in the rescaling and merging phase
(Sect. VII) where ﬁne-grained time series are aggregated
into coarser granularity for better scalability and periodicity
483
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:40:31 UTC from IEEE Xplore.  Restrictions apply. 
484
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:40:31 UTC from IEEE Xplore.  Restrictions apply. 
B. Novelty Analysis
The goal of this ﬁlter is to eliminate duplicate work by
the analyst. More speciﬁcally, it removes source/destination
pair candidates that have been reported for beaconing behavior
already, or at least whose destination has been involved in
pairs already reported, either from another candidate or from
a previous analysis run. The candidate is still logged and
reported, and is kept available for analyst review; however, it
will not be passed into the ranking algorithm for later manual
veriﬁcation and investigation.
This ﬁlter can be considered as “change detection,” consol-
idating cases of the same source/destination pairs and thereby
forwarding cases only when a destination has not been reported
before, or a source has not been reported before as beaconing
to that destination.
C. Language Model Score
A common strategy employed by botnets and malicious
servers to circumvent detection is domain name generation.
Bots algorithmically create a large pool of random names and
attempt to connect to them, hoping that some domains may
have been registered by attackers in advance as a rendezvous
point. To avoid collision with existing domains, randomly
generated names typically exhibit distinct patterns, e.g., combi-
nation of characters that rarely occur in popular domain names.
Such patterns often provide good indication of suspicious
nature.
is a probabilistic model
the algorithm ﬁrst extracts all
Motivated by this observation, we train a 3-gram language
model using Alexa top 1 million domain names [1]. An n-
gram model
that can be used to
predict
the next character in the sequence given previous
n − 1 characters. More speciﬁcally, given a training cor-
pus of domain names,
the
n-gram substrings c1c2 . . . cn, and compute the transitional
Count(c1c2...cn)
probability, P (cn|c1c2 . . . cn−1) =
Count(c1c2...cn−1) . This is
the probability of cn appearing after c1c2 . . . cn−1. With the
derived transitional probability from the training corpus, we
compute the score S of domain name D = c1c2 . . . cm
under the n-gram language model as S = log(P (D)), where
k=nP (ck|ck−n+1ck−n+2 . . . ck−1)3. A low score
P (D) = Πm
means abnormal or random patterns in a domain name, which
may warrant further investigation. For example, botnets using
domain generation algorithms (DGAs) often yield low scores.
The score of domain skmnikrzhrrzcjcxwfprgt.com is -45.166,
which is signiﬁcantly lower than -7.406378 of google.com.
D. Weighted Result Ranking
To assist analysts in prioritizing their investigation efforts,
we combine various indicators, e.g., periodicity strength, lan-
guage model score, and destination popularity, into a single
weighted ranking score. The weight of each indicator can be
empirically set based on the quality of the indicator and prefer-
ence of the analysts. For example, in the current implementa-
tion of BAYWATCH, we assign a higher weight to the language
model score for the domains with very low probabilities. We
also awarded a higher score to the connections with strong
periodicity, e.g., high ACF score, low standard deviation in
3Kneser-Ney smoothing is used for previously unseen n-grams.
TABLE II.
LIST OF FEATURES USED IN TRAINING CLASSIFIERS
Feature
series length
Deﬁnition
# intervals in series
period(s) most dominant period(s)
power
similar source
n-gram count
entropy
compressibility
power of most dominant period(s)
# sources sharing same destination
hist. of n-grams in symbolized series
entropy of symbolized series
compression ratio of symbolized series
the observed intervals, and periodic over long range of time,
since these regular patterns are of more interest to the analysts.
For the ﬁnal ranking of suspicious beaconing cases, we apply
a threshold to report only those cases that exceed the n-th
percentile of the score distribution.
VI.
INVESTIGATION AND VERIFICATION
In the aforementioned phases, we apply various ﬁltering
mechanisms to triage detected cases. For large networks, it
is highly likely that a large number of suspicious cases pass
through the triage for further manual investigation. Instead
of an exhaustive investigation, we propose the following al-
ternative: (a) sampling a small set of candidate cases and
manually investigating them, (b) using these cases and their
corresponding diagnosis results as “ground truth” to train a
classiﬁer, and (c) applying the trained classiﬁer to classify the
remaining candidate cases. For example, in our evaluation (see
Sect. VIII), we manually investigate a data set collected over
a one-month time interval to train a classiﬁer, and apply it to a
much larger data set spanning over a ﬁve-month time period.
Astute readers might ask why this approach is not applied
in earlier phases. The reasons are twofold. First, at this stage,
the candidate cases are much “cleaner” for training a classiﬁer
because many noisy cases have been pruned. Second, as a
desired side effect, the various ﬁltering mechanisms essentially
generate a rich set of features (e.g., dominant periods in
Sect. IV). Next we elaborate the feature set and the classiﬁer
we use in our prototype.
A. Feature Set
Recall that each candidate case is a tuple (cid:3)s, d, (i1, i2, . . .)(cid:4)
with s, d, and (i1, i2, . . .) representing the source, destination,
and a series of time intervals, respectively. For each case, we
generate the set of features as deﬁned in Table II. Among them,
the ﬁrst four features are fairly straightforward. We concentrate
our discussion on the remaining three.
Once we detect the most promising period(s) as in Sec-
tion IV, we “symbolize” the series of time intervals by
applying the following rule for each interval i:
(cid:2)
i → ‘x’
i → ‘y’
i → ‘z’
i appears in dominant period(s)
i = 0
otherwise
Since this simpliﬁed sequence comprises only three letters, we
can easily measure its entropy, its n-gram histogram (n = 3),
and its compressibility. In particular, the compressibility of
a symbolized series is measured by its compression ratio by
“gzip” under the highest compression level.
485
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:40:31 UTC from IEEE Xplore.  Restrictions apply. 
B. Classiﬁer
In our prototype, we adopt random forest [5] as a classiﬁer.
A random forest is a collection (or ensemble) of decision trees.
When classifying a new case, each decision tree assigns the
case to a single class (e.g., benign or malicious); the output of
the random forest is the mode of the outputs of the decision
trees. The random forest classiﬁer has a number of desired
features for our context: (a) built upon decision trees, it can
handle both numeric and categorical features; (b) it has been