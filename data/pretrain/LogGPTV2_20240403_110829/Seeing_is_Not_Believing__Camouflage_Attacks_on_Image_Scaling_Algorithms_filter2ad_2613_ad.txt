eters via exhaustive tests. An adversary can send a series of
m probing images {probeImgi}, (i = 1,2, ...,m), crafted by
the scaling attack method with various scaling parameters.
The attacker can infer the scaling parameters by watching
the classiﬁcation results. If one query returns with the correct
classiﬁcation result, the corresponding scaling parameters are
likely to be used by the target service. Then the attacker can
try to launch the attack with the inferred parameters. This
procedure is shown in Algorithm 3.
The inference efﬁciency can be increased by using a
complex attack image involving several sub-probing images.
These sub-probing images can only be recovered with their
corresponding scaling parameters.
Here we show one simple approach to achieve this goal.
First, the attacker collects n sub-probing images each be-
longing to different categories, and determines the input
size range SizeRange and the scaling algorithms AlgSet to
test. The search space S can be deﬁned as S = {Si} =
SizeRange⇥AlgSet = {sizei}⇥{algi}. Second, the attacker
chooses a large blank white image (with #FF as the pixel
value) as the background, and divides it into n non-overlapped
probing regions. Third, the attacker repeats the following pro-
cedure: he/she ﬁlls the j-th probing region of the blank image
with the j-th sub-probing image respectively, next scales it
with the scaling parameter S j
i , and then conducts the scaling
attack with the original blank image as the sourceImg and
the resized image as the targetImg. Finally, the attacker com-
bines all the output images to create the probeImgi. In this
way, when probeImgi is resized, the j-th sub-probing image
will be recovered if and only if the scaling parameter is set
as S j
i . Fig.6 gives an example of the probeImg. Note that the
larger n is, the fewer probeImgs are needed, but the recogni-
tion accuracy of sub-probing images might be reduced as the
area of each probing region decreases.
Image Crafting. After retrieving the possible input size
and scaling algorithm, the adversary can generate attack im-
ages as described in Section 5, and launch the scaling attack
to cloud vision services .
alg⇤
for size in SizeRange do
1: for alg in AlgSet do
2:
3:
4:
=
ScalingAttack(alg,
testImg = resize(targetImg, size)
probeImg
sourceImg,
testImg) ⇤ Can be recovered once resized into size
by alg.
if argmax( f (testImg)) == argmax( f (probeImg))
then
5:
Algorithm 3 Scaling Parameter Inference
Input: cloud vision API f , scaling algorithms AlgSet =
{alg1, alg2, ...}, input size range SizeRange = {size1, size2,
...}, source image sourceImg, target image targetImg
Output: the inferred input size size⇤ and scaling algorithm
Return size,alg
⇤ Get a feasible answer.
end if
end for
6:
7:
8:
9: end for
10: Return NULL, NULL ⇤ No match during the search.
Aliyun, and Tencent cloud vision services as our test beds9.
In our experiment, each probeImg contains four sub-
images (classiﬁed as “zebra”, “dog”, “rat” and “cat”) for dif-
ferent input parameters. For the input size, the SizeRange is
set from 201 to 300, while the scaling algorithm options in-
clude two libraries OpenCV and Pillow with Nearest, Bilinear
and Cubic interpolation methods. Considering the trade-off
between efﬁciency and recognition accuracy, we set n = 4.
Hence, the total amount of queries is 100 (#input size) * 2
(#scaling library) * 3 (#interpolation method) / 4 (#probing
region) = 150 (#probeImg). As we can see, the searching
space is extremely small and it consumes just up to several
minutes to obtain the results. We provide the scaling parame-
ter inference results and one scaling attack sample in Table
3.
Moreover, to verify the effectiveness of the proposed attack
strategy, we collected 935 images from Internet, including 17
categories except of sheep or sheep-like animals, and cropped
them into the 800*600 size holding the main object, as our
sourceImg dataset. Then for each of the 935 sourceImgs, we
generated one attack image with the same targetImg contain-
ing a sheep in the center, setting the scaling attack parameter
e = 0.01.
For Baidu, Aliyun and Tencent, all the attack images are
classiﬁed as “sheep” or “goat” with the highest conﬁdence
value compared with other classes, while for Azure the re-
sult becomes more complex. In our experiment we requested
the Azure cloud vision service API to respond with four fea-
6.2.2 Results
To show the feasibility of the scaling attack against black-
box cloud vision services, we choose Microsoft Azure, Baidu,
9 As part of the responsible disclosure etiquette, we have reported this
issue and received replies from these companies. The latter three have con-
ﬁrmed this problem as are now in the process of ﬁxing it. Microsoft Azure
has also acknowledged the issue and is discussing with us about possible
solutions.
452    28th USENIX Security Symposium
USENIX Association
(a) probeImg
(1024*1024)
(b) Scaled result
(OpenCV.Bilinear, 201*201)
(c) Scaled result
(OpenCV.Bilinear, 202*202)
(d) Scaled result
(OpenCV.Bilinear, 203*203)
(e) Scaled result
(OpenCV.Bilinear, 204*204)
Figure 6: An example of the probeImg. (a) is a probeImg containing 4 subﬁgures, and (b) to (e) are the results when the probeImg
is scaled under different scaling settings.
tures: “description”, “tags”, “categories”, and “color”. We
ﬁnd that for attack images, the word “sheep” may appear in
the “description” or “tags” with a conﬁdence value. Hence,
we computed the CDF (cumulative distribution function) of
attack images’ conﬁdence values of “tags” and “description”
respectively, and plot the two CDF curves in Fig.7 (we as-
sume the conﬁdence value as 0 if “sheep” is absent in the API
response). The result shows that for the “tags” feature, more
than 60% attack images are classiﬁed as “sheep” with a conﬁ-
dence value higher than 0.9, which implies the effectiveness
of our proposed attack.
Figure 7: The CDF curve of responses from Azure.
6.3 Deceiving Effect on Web Browsers
Web browsers provide the page zooming function to scale the
contents, including texts and images. Hence, an attacker may
be able to utilize scaling functions in web browsers to achieve
deceiving or phishing attacks.
We have evaluated such effect on several mainstream
browsers running on different platforms. We generated an
attack image (with a 672*224 sheep image as the source
image, a 224*224 wolf image as the target image, using
OpenCV.Bilinear as the scaling method, e = 0.01), and used
HTML tags to control its rendering size in browsers. The re-
sult is presented in Table 4, indicating the potential victims of
scaling attack are beyond the scope of deep learning computer
vision applications. One potential problem is that scaling at-
tacks can cause inconsistency between different screen reso-
lutions, when the browser’s auto/adaptive-zooming function
is enabled.
6.4 Factors that Might Interfere with Scaling
Attacks
Image processing applications often contain a complex pre-
processing pipeline. Besides scaling, an image processing
applications might use cropping, ﬁltering, and various other
image transformation actions. If these additional image pre-
processing actions occur prior to the scaling action, they might
pose additional challenges to scaling attacks.
The following list presents an overview of common image
preprocessing actions and discusses their potential impact on
scaling attacks.
• Cropping – truncate certain regions of the input image,
for the purpose of data augmentation or background re-
moving. Cropping usually changes the source image
aspect ratio, and if a scaling attack was designed under a
wrong dimension, the automatic generated image would
not scale to the right target image. Therefore, attackers
need to know precisely which region in the input is ex-
pected to be cropped. Only under some special cases,
such as the cropping preserves the aspect ratio and the
underneath algorithm is Nearest, deceiving effect can be
preserved. Certainly the degree of impact also depends
on the relative size being cropped. If the pixels that are
used to generate the targeted image are chopped, then
the effect of scaling attack is deﬁnitely affected.
• Filtering – is to blur or sharpen an image, adjust its color
palette. Image ﬁltering changes the pixel values and thus
directly interferes with scaling attacks, because the attack
is based on the manipulation of “average” values of
neighbor pixels used by the interpolation algorithms. For
simple scaling algorithms, such as Nearest, the output
image might still present deceiving effect as the result is
USENIX Association
28th USENIX Security Symposium    453
Service
Inferred Scale
Inferred Algorithm
Attack Image
Response
Table 3: Deceiving effect on four cloud vision services.
Azure
227*227
OpenCV.Bilinear
Baidu
256*256
OpenCV.Bicubic
Aliyun
224*224
OpenCV.Bilinear
Tencent
224*224
OpenCV.Bilinear
"captions":
{ "text": "a close up of a wolf",
"conﬁdence": 0.707954049 } }
Tags: ... { "name": "wolf",
"conﬁdence": 0.981169641}...
... "result":
{ "score": "0.938829",
"name": "Grey Wolf" },
{ "score": "0.0146997",
"name": "Mexico Wolf" }...
...
"Object":
{ "Grey Wolf": "49.37%",
"White Wolf": "29.93%",...}
...
"Tags":
{ "Grey Wolf": "88%",
"Eskimo": "15%"}...
Table 4: Proof-of-Concept sample image and the rendering effect under different browser settings. (The HTML ﬁle uses an IMG
tag to specify the image rendering size.)
Browsers
Original Image
Firefox, Edge
IE11
Chrome
Safari
Image
Size
Version
672*224
Firefox: 59.0.2, IE: 11.0.9600.18977
Chrome: 63.0.3239.84,
Edge: 41.16299.371.0
Safari: 8.0 (10600.1.25.1)
224*224
224*224
224*224
224*224
Firefox: 59.0.2
Edge: 41.16299.371.0
IE: 11.0.9600.18977 Chrome: 63.0.3239.84
Safari:
8.0 (10600.1.25.1)
like the original target with a ﬁltering effect. However,
for complex scaling algorithms, such as Bilinear and
Bicubic, the output image will likely not present as the
intend target image.
• Afﬁne transformations – is to rotate or mirror the in-
put image. Rotation in an arbitrary degree likely breaks
the calculation used by the automatic attack image craft-
ing. However, ﬂipping images in 180 degree, mirror im-
ages might have no impacts on the scaling attack which
mainly depends on the size of the inputs and the scaling
algorithms. Some scaling algorithms are orientation in-
dependent, i.e. the output is same regardless the scan of
pixels is from left to right or the opposite order. In those
cases, a ﬂip or mirror action would not affect scaling
attacks.
• Color transformations – to change the color space, like
convert an RGB image to grayscale. Color transforma-
tion can be considered as a special type of ﬁltering, and
thus the impact to scaling attack is similar to ﬁltering.
Although the above transformation actions all directly in-
terfere with scaling attacks, the interference can be overcome
by the attackers if they know these transformation details. In
fact, each of these operations can be described by a transfor-
mation matrix. Once an attacker ensures the exact content
of the transformation matrix and if there exists a correspond-
ing reverse transformation matrix, the attacker can applies
the reverse matrix to generated attack image before feeding
it to the targeted application. In the black-box case, the at-
tacker has to infer the transformation matrix. Therefore, these
transformation actions would increase the attack difﬁculty.
The deceiving effect of scaling attacks is also subject to
some native limitations, especially size and brightness, of
source and target images. An attacker needs to ﬁnd an ap-
propriate pair of the source and target images to achieve a
successful attack image.
• Size: Sizes of the source and target images decide how
many redundant pixels can be leveraged to launch the
attack. If the size differences between scaling input and
output are very close, the information attenuation due to
resampling may be insufﬁcient to achieve a successful
deceiving effect.
• Brightness: Brightness or color of the source and target
images decides how tight the constraints are. In the worst
case, it is hard to ﬁnd a feasible solution given a full
white source and a full black target. Even we generate
an attack image successfully, it is hard to deceive human
without noticing dark dots distributed in the white image.
6.5 Practical Attack Scenarios
This paper presents the risk of scaling attacks through a set
of limited experiments with proof-of-concept images. We
have shown these proof-of-concept images can achieve de-
ceiving effect in deep-learning based image applications, web
454    28th USENIX Security Symposium
USENIX Association
browsers, as well as cloud-based visual recognition and clas-
siﬁcation services. Although these proof-of-concept images,
such as the wolf-in-sheep set, do not cause any real damage,
we believe the risks of scaling attacks are real. This section
describes a few motivating scenarios to illustrate possible real
life threats.
• Data poisoning. Many image based applications rely
on label training sets and there are many large image
datasets, such as ImageNet [12], on the Internet. Many
deep learning developers rely on these datasets to train
their models. Although data poisoning as a concept is
known, developers and model trainers rarely consider
data poisoning is a real threat on these public datasets
since these datasets are public, and humans are expected
to notice obvious genre mistakes and a large set of mis-
labels. However, with scaling attacks, people with mali-
cious intent could conceal a hidden category of images
(e.g. wolf) while providing mistaken labels as another
category (e.g. sheep). We do not have evidence of such
activities, but we envision that scaling attacks deﬁnitely
make data poisoning more stealthy.
• Detection evasion and Cloaking. Content moderation
is one of the most widely used computer vision applica-
tions. Many vendors provide content ﬁltering services,
such as Google [11], Amazon AWS [3] and Microsoft
Azure [4]. ModerateContent claims that it is trusted by