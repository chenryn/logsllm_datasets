title:Patch Me If You Can: A Study on the Effects of Individual User Behavior
on the End-Host Vulnerability State
author:Armin Sarabi and
Ziyun Zhu and
Chaowei Xiao and
Mingyan Liu and
Tudor Dumitras
Patch Me If You Can: A Study on the Eﬀects
of Individual User Behavior on the End-Host
Vulnerability State
Armin Sarabi1(B), Ziyun Zhu2, Chaowei Xiao1, Mingyan Liu1,
and Tudor Dumitra¸s2
1 University of Michigan, Ann Arbor, USA
{arsarabi,xiaocw,mingynan}@umich.edu
2 University of Maryland, College Park, USA
{zhuziyun,tdumitra}@umiacs.umd.edu
Abstract. In this paper we study the implications of end-user behavior
in applying software updates and patches on information-security vul-
nerabilities. To this end we tap into a large data set of measurements
conducted on more than 400,000 Windows machines over four client-side
applications, and separate out the impact of user and vendor behavior on
the vulnerability states of hosts. Our modeling of users and the empirical
evaluation of this model over vulnerability states of hosts reveal a pecu-
liar relationship between vendors and end-users: the users’ promptness in
applying software patches, and vendors’ policies in facilitating the instal-
lation of updates, while both contributing to the hosts’ security posture,
are overshadowed by other characteristics such as the frequency of vul-
nerability disclosures and the vendors’ swiftness in deploying patches.
1 Introduction
Software vulnerabilities represent a valuable resource for attackers. Exploits
for these vulnerabilities can allow miscreants to control the vulnerable hosts
remotely. Unpatched vulnerabilities also present a threat for enterprises, as an
outward facing machine with an exploitable vulnerability can provide unautho-
rized access to the company’s internal network [26]. Moreover, the emergence
of exploit kits [14], makes it easy for attackers to compromise hosts in an auto-
mated fashion. To counter these threats, software vendors create and dissemi-
nate patches that users then install to remove vulnerabilities on their machines.
Vendors have also increased the automation of their software updating mecha-
nisms [9,13] in an attempt to accelerate the patching process to sidestep possible
tardiness on the part of the end users.
It follows that the vulnerability state of any given end-host at any given
time, reﬂected in the number of known but unpatched vulnerabilities, and
unpatched vulnerabilities with known exploits, is the result of a combination
of factors, including (1) the user’s updating behavior, (2) the software prod-
ucts’ patch release timeliness with respect to the disclosure of vulnerabilities,
c(cid:2) Springer International Publishing AG 2017
M.A. Kaafar et al. (Eds.): PAM 2017, LNCS 10176, pp. 113–125, 2017.
DOI: 10.1007/978-3-319-54328-4 9
114
A. Sarabi et al.
Table 1. Summary of ﬁndings. +/− indicate positive and negative impacts.
Findings
Implications
+ The user behavior can be summarized
using single parameter distributions
+ The geometric distribution provides a
good ﬁt, even for products with silent
updates
+ Silent updates lead to shorter
windows of vulnerability for end-hosts
(as expected)
− Even with silent updates, the majority
of hosts have long windows of
vulnerability
− Many hosts have long windows of
susceptibility to known exploits
Users’ willingness to patch does not
seem to depend on the type of
improvements in new releases
This simple model signiﬁcantly
simpliﬁes the analysis of the
relationship between user behavior and
the vulnerability state of their machines
The product vendors can improve the
vulnerability state by adopting a silent
updating mechanism
The large number of security ﬂaws in
client-side applications limits the
beneﬁts of silent updates
Exploit kits present a direct threat to
these hosts
(3) the update mechanisms employed to deploy patches on hosts, and (4) the
frequency at which vulnerabilities are disclosed and exploits are developed.
While the latter three elements have been extensively studied in the literature—
see e.g., [2–5,7,8,18,20,22,25] on vulnerability disclosure and patch releases,
[11,17,21,23,30] on patch deployment, and [4,6,14,24] on exploits—relatively
less is known about the impact of individual user behavior. Prior work in this
area has introduced several hypotheses on why users might delay patching vul-
nerabilities [15,16,29], and aggregated patching measurements for individual vul-
nerabilities over the general population and over selected groups of users [17].
In this paper, we present a broad ﬁeld study of individual user behavior,
including more than 400,000 users over a period of 3 years (01/2010 to 12/2012),
and their updating activities concerning 1,822 vulnerabilities across 4 software
products. The updating automation for these applications ranges from prompt-
ing users to install patched versions to silent updates, which require minimal
user interaction. Our goal is to understand (1) how users behave on an individ-
ual level, and (2) how diﬀerent updating behaviors relate to the vulnerability
state of their machines, and how this relationship diﬀers across products.
To achieve the above goal, we employ a combination of empirical analysis
and mathematical modeling. In summary, our main contributions are as follows.
We propose methods for quantifying the user updating behavior from ﬁeld mea-
surements of patch deployment. Furthermore, we conduct a systematic study
of vulnerability patching, from the perspective of individual users (rather than
individual vulnerabilities), and quantify the corresponding vulnerability state
of the users’ machines. Finally, building on insights from our measurements, we
Patch Me If You Can: A Study on the Eﬀects of Individual User Behavior
115
create and evaluate a parameterized model for individual patching behavior, and
discuss its implications for end-host security. Table 1 summarizes our ﬁndings.
2 Data Sets and Their Processing
We utilize a corpus of patch-deployment measurements collected by Nappa
et al., on user hosts that include average users, as well as professionals, software
developers, and security analysts, and mostly consist of Windows XP/Vista/7
machines[17]. These measurements were conducted by observing the installation
of subsequent versions of diﬀerent applications, and are derived from the WINE
data set [10]. The set of security ﬂaws aﬀecting each version are extracted from
NVD [19], using the CVE-ID of the vulnerability, resulting in 1,822 vulnera-
bilities. We analyze users’ patching behavior over 4 products: Google Chrome,
Mozilla Firefox, Mozilla Thunderbird, and Adobe Flash Player, and only include
hosts that have recorded more than 10 events for at least one application. This
results in a data set consisting of 11,017,973 events over 426,031 unique hosts,
99.3% of which are between 01/2010 and 12/2012.
Although an open vulnerability indicates that the application could be
exploited, few vulnerabilities are actually exploited in the wild. We collect exploit
data from (1) public descriptions of Symantec’s anti-virus signatures [28] and (2)
metadata about exploit kits from Contagiodump [12]. Combining both sources
of exploit information results in exploit release dates for 21 CVEs. The median
time between vulnerability disclosure and an exploit kit targeting it is 17 days.
For Firefox, Flash Player, and Thunderbird, we manually scrape release his-
tory logs, either provided on the vendor’s website, or collected by a third party,
to ﬁnd out when each version is released to the public. We have collected the
results along with the source for each entry in a single document [27].
2.1 Curated Data
Host state. Each update event corresponds to a (machine ID, product, version,
timestamp) tuple, indicating the installation of a software on the host. However,
the WINE database provides no information on when the product has been
removed, or if the user has installed multiple product lines in parallel (e.g. Firefox
3.6, and 4.0). We utilize the following heuristic to update the state of a machine
after each event. Assume that an event at time t signals the installation of
version v belonging to product line (cid:2), and we have detected the presence of
versions St− = {((cid:2)1, v1), . . . , ((cid:2)n, vn)} on the machine prior to the event. For
each (cid:2)i in St−, if there are no observations for the same line within 6 months
of the current event, we remove the ((cid:2)i, vi) pair from St−. We then add the
((cid:2), v) pair, or update the corresponding pair in St− if the same product line is
already installed on the host, to obtain the state St after the event. We then
take the union of vulnerabilities that aﬀect each version in St from NVD, as the
set of vulnerabilities present on the host. The subset of vulnerabilities that have
already been disclosed, or exploited, represent the machine’s security posture.
116
A. Sarabi et al.
Version release date. For Firefox, Flash Player, and Thunderbird, we can
obtain the oﬃcial release dates for each version by scraping version release notes
from the vendor, or release histories collected by a third party. For Chrome, we
tap into the patch measurement data to estimate release dates for each version.
In previous work, Nappa et al. [17] identify the release date automatically, by
selecting the ﬁrst date when the version appears in WINE. However, we found
that this approach can be unreliable in some cases. The binary that corresponds
to a new version might appear in the wild half a year before it is made available
on the release channel. We observe that on a release date there is usually a high
volume of patching events. We thus ﬁrst rank the dates by the count of patching
events, and then identify the patch release date as the earliest day among the
10 dates with top ranks. We compared the results from this method with the
release dates for Firefox and we found that they match for all the versions.
Purpose of updates. To determine if users are inﬂuenced by the purpose of
the updates, we identify four types of software releases: introducing new features
If eats, ﬁxing bugs Ibugs, patching security vulnerabilities Ivulns, or introducing
a new product line ImajV er. Using these four categories, we manually label the
versions for Firefox and Flash Player. Since the release notes are not available for
every build and they switched to silent updates on 2012-08-28 and 2012-03-28,
respectively, the number of versions we labeled is 30 and 39, respectively.
User updates. To study the frequency of irregular user behavior, we ﬁrst obtain
the number of events that result in the presence of more than one product line
on a host. For Chrome, Flash Player, Firefox, and Thunderbird, 0.9%, 4.9%,
1.2% and 0.3% of events lead to the installation of more than one product line.
For Flash Player, we further analyze the number of vulnerabilities associated
with each product line. On average, in the presence of multiple product lines,
79.5% of vulnerabilities come from the lowest product version installed on the
machine. Therefore, we take the lowest application version on the machine as its
current state, and only consider a new event as a version upgrade if it updates
the lowest installed version. Note that for evaluating whether a machine is prone
to a known vulnerability, we will still use the complete set of installed versions.
Finally, for each state transition that updates the lowest installed version,
we need to extract the user’s delay in applying the update. We ﬁrst take the
), and
timestamps for the current and previous events (denoted by T k
extract the ﬁrst time an update was released for the previously installed version
(denoted by Tr). The user’s delay is then Sk
, Tr). This
means that we measure the users’ delay from the day an update is available
for the installed version, or the product installation date, whichever comes last;
the latter takes eﬀect when the user installs an outdated version. Note that
successive versions do not necessarily follow a chronological order, as multiple
product lines are often developed in parallel. For each release, we take the next
version in the same line to be the update for that release. For end-of-life releases,
we pick the ﬁrst version in the subsequent line as the next logical update.
− max(T k−1
u := T k
u and T k−1
u
u
u
Figure 1a depicts a sample scenario for 4 successive releases of Firefox,
released at times t = 0, 35, 50, 75 (t = 0 corresponds to “2012-09-11”). Firefox
Patch Me If You Can: A Study on the Eﬀects of Individual User Behavior
117
6
4
2
s
e
i
t
i
l
i
b
a
r
e
n
u
V
l
0
0
15.0.1
16.0.1
16.0.2
17.0.0
Vulnerability
Disclosure
20
40
60
Time (days)
80
s
e
i
t
i
l
i
b
a
r
e
n
u
V
l
4
3
2
1
0
0
User update
20
40
60
Time (days)
80
(a)
(b)
Fig. 1. The number of vulnerabilities in successive Firefox versions (left) and following
a user’s update events (right). Each color represents a single version. (Color ﬁgure
online)
v15.0.1 is prone to 6 vulnerabilities, all of which are undisclosed at the time of
release. However, these vulnerabilities are made public at times t = 34, 36, 53, 76,
and patched in subsequent versions. Figure 1b illustrates a sample user in our
data set who installs these versions at t = 5, 37, 58, 84, respectively. Note that
with each update, the user inherits the set of vulnerabilities in the new release.
An update is made available for the ﬁrst version at time t = 35, and the user
u = 37, therefore the user’s delay for the
initiates a software update at time T 1
ﬁrst update event is S1
u = 2 days. Similarly, S2
u = 8 days, and S3
u = 9 days.
3 Analysis of User Behavior and Its Security Implications
3.1 Modeling a User’s Patching Delay
We assume that the user’s update delays are drawn from a probability distribu-