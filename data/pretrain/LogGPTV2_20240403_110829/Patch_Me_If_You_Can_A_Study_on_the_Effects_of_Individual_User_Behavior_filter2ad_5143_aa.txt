# Patch Me If You Can: A Study on the Effects of Individual User Behavior on the End-Host Vulnerability State

**Authors:**
- Armin Sarabi
- Ziyun Zhu
- Chaowei Xiao
- Mingyan Liu
- Tudor Dumitraș

**Affiliations:**
- 1 University of Michigan, Ann Arbor, USA
- 2 University of Maryland, College Park, USA

**Contact Information:**
- {arsarabi, xiaocw, mingyan}@umich.edu
- {zhuziyun, tdumitra}@umiacs.umd.edu

## Abstract

In this paper, we investigate the implications of end-user behavior in applying software updates and patches on information-security vulnerabilities. We analyze a large dataset of measurements from over 400,000 Windows machines across four client-side applications to separate the impact of user and vendor behavior on the vulnerability states of hosts. Our modeling and empirical evaluation reveal a complex relationship between vendors and end-users: while the promptness of users in applying software patches and the policies of vendors in facilitating updates both contribute to the security posture of hosts, these factors are overshadowed by other characteristics such as the frequency of vulnerability disclosures and the speed at which vendors deploy patches.

## 1. Introduction

Software vulnerabilities are a valuable resource for attackers. Exploits for these vulnerabilities can allow malicious actors to control vulnerable hosts remotely. Unpatched vulnerabilities also pose a significant threat to enterprises, as an outward-facing machine with an exploitable vulnerability can provide unauthorized access to the company's internal network [26]. The emergence of exploit kits [14] further automates the process of compromising hosts. To counter these threats, software vendors create and distribute patches that users install to remove vulnerabilities from their machines. Vendors have also increased the automation of their software updating mechanisms [9,13] to accelerate the patching process and mitigate potential delays by end-users.

The vulnerability state of any given end-host at any given time, reflected in the number of known but unpatched vulnerabilities and unpatched vulnerabilities with known exploits, is influenced by several factors:
1. The user’s updating behavior.
2. The timeliness of patch releases by software vendors relative to the disclosure of vulnerabilities.
3. The update mechanisms employed to deploy patches on hosts.
4. The frequency at which vulnerabilities are disclosed and exploits are developed.

While the latter three elements have been extensively studied in the literature (e.g., [2–5,7,8,18,20,22,25] on vulnerability disclosure and patch releases, [11,17,21,23,30] on patch deployment, and [4,6,14,24] on exploits), relatively less is known about the impact of individual user behavior. Prior work has introduced hypotheses on why users might delay patching vulnerabilities [15,16,29] and has aggregated patching measurements for individual vulnerabilities over the general population and selected groups of users [17].

In this paper, we present a broad field study of individual user behavior, including more than 400,000 users over a period of 3 years (January 2010 to December 2012), and their updating activities concerning 1,822 vulnerabilities across four software products. The updating automation for these applications ranges from prompting users to install patched versions to silent updates, which require minimal user interaction. Our goal is to understand:
1. How users behave on an individual level.
2. How different updating behaviors relate to the vulnerability state of their machines, and how this relationship varies across products.

To achieve this, we employ a combination of empirical analysis and mathematical modeling. Our main contributions are:
- Methods for quantifying user updating behavior from field measurements of patch deployment.
- A systematic study of vulnerability patching from the perspective of individual users, rather than individual vulnerabilities, and quantification of the corresponding vulnerability state of the users’ machines.
- A parameterized model for individual patching behavior, evaluated based on our measurements, and discussion of its implications for end-host security.

Table 1 summarizes our key findings.

## 2. Data Sets and Their Processing

We utilize a corpus of patch-deployment measurements collected by Nappa et al. [17], which includes average users, professionals, software developers, and security analysts, primarily using Windows XP/Vista/7 machines. These measurements were derived from the WINE dataset [10] and include 1,822 vulnerabilities extracted from the National Vulnerability Database (NVD) [19] using CVE-IDs. We analyze users' patching behavior over four products: Google Chrome, Mozilla Firefox, Mozilla Thunderbird, and Adobe Flash Player. Only hosts with more than 10 recorded events for at least one application are included, resulting in a dataset of 11,017,973 events over 426,031 unique hosts, with 99.3% of the data between January 2010 and December 2012.

Although an open vulnerability indicates that the application could be exploited, few vulnerabilities are actually exploited in the wild. We collect exploit data from public descriptions of Symantec’s anti-virus signatures [28] and metadata about exploit kits from Contagiodump [12]. Combining both sources results in exploit release dates for 21 CVEs, with a median time of 17 days between vulnerability disclosure and an exploit kit targeting it.

For Firefox, Flash Player, and Thunderbird, we manually scrape release history logs from vendor websites or third-party sources to determine when each version was released. The results and sources are documented in [27].

### 2.1 Curated Data

#### Host State
Each update event corresponds to a (machine ID, product, version, timestamp) tuple, indicating the installation of a software on the host. However, the WINE database does not provide information on when a product is removed or if multiple product lines are installed in parallel. We use the following heuristic to update the state of a machine after each event:

- Assume an event at time \( t \) signals the installation of version \( v \) belonging to product line \( \mathcal{L} \).
- Let \( S_{t-} = \{(\mathcal{L}_1, v_1), \ldots, (\mathcal{L}_n, v_n)\} \) be the set of versions detected on the machine prior to the event.
- For each \( \mathcal{L}_i \) in \( S_{t-} \), if there are no observations for the same line within 6 months of the current event, we remove the \( (\mathcal{L}_i, v_i) \) pair from \( S_{t-} \).
- We then add the \( (\mathcal{L}, v) \) pair or update the corresponding pair in \( S_{t-} \) if the same product line is already installed on the host to obtain the state \( S_t \) after the event.
- The union of vulnerabilities affecting each version in \( S_t \) from NVD represents the set of vulnerabilities present on the host. The subset of vulnerabilities that have already been disclosed or exploited represents the machine’s security posture.

#### Version Release Date
For Firefox, Flash Player, and Thunderbird, we obtain official release dates by scraping version release notes from vendor websites or third-party sources. For Chrome, we estimate release dates by ranking the dates by the count of patching events and identifying the patch release date as the earliest day among the top 10 ranked dates. This method matches the release dates for Firefox.

#### Purpose of Updates
To determine if users are influenced by the purpose of updates, we categorize software releases into four types: introducing new features (If eats), fixing bugs (Ibugs), patching security vulnerabilities (Ivulns), or introducing a new product line (ImajV er). We manually label the versions for Firefox and Flash Player, resulting in 30 and 39 labeled versions, respectively.

#### User Updates
To study the frequency of irregular user behavior, we analyze the number of events that result in the presence of more than one product line on a host. For Chrome, Flash Player, Firefox, and Thunderbird, 0.9%, 4.9%, 1.2%, and 0.3% of events lead to the installation of more than one product line. For Flash Player, 79.5% of vulnerabilities come from the lowest product version installed on the machine. Therefore, we take the lowest application version on the machine as its current state and consider a new event as a version upgrade only if it updates the lowest installed version. For evaluating whether a machine is prone to a known vulnerability, we use the complete set of installed versions.

Finally, for each state transition that updates the lowest installed version, we extract the user’s delay in applying the update. We measure the delay from the day an update is available for the installed version or the product installation date, whichever comes last. Successive versions do not necessarily follow a chronological order, as multiple product lines are often developed in parallel. For each release, we take the next version in the same line as the update for that release. For end-of-life releases, we pick the first version in the subsequent line as the next logical update.

Figure 1a depicts a sample scenario for four successive releases of Firefox, released at times \( t = 0, 35, 50, 75 \) (with \( t = 0 \) corresponding to “2012-09-11”). Firefox version 15.0.1 is prone to six vulnerabilities, all of which are undisclosed at the time of release. These vulnerabilities are made public at times \( t = 34, 36, 53, 76 \) and patched in subsequent versions. Figure 1b illustrates a sample user in our dataset who installs these versions at \( t = 5, 37, 58, 84 \), respectively. With each update, the user inherits the set of vulnerabilities in the new release. An update is made available for the first version at time \( t = 35 \), and the user initiates a software update at time \( T_1^u = 37 \), resulting in a user’s delay of \( S_1^u = 2 \) days. Similarly, \( S_2^u = 8 \) days, and \( S_3^u = 9 \) days.

## 3. Analysis of User Behavior and Its Security Implications

### 3.1 Modeling a User’s Patching Delay

We assume that the user’s update delays are drawn from a probability distribution.