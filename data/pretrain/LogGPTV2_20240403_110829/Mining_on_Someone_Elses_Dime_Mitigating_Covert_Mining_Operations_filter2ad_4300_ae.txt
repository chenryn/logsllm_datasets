majority scheme (though other settings can also be used such as classiﬁcation
based on 33% match or 75% match etc.) using per application testing.
Table 4. Classiﬁcation results for three diﬀerent operating environments in a open
world setting when all samples are treated collectively (per application processing).
Open world scenario F-score (CI)
False positives (CI) False negatives (CI)
OS-Level
VM-Level
93.85% (2.68%) 0.0% (0.0%)
9.70% (3.77%)
91.67% (3.16%) 0.0% (0.0%)
16.33% (5.83%)
VM-Interference
96.32% (1.75%) 0.0% (0.0%)
7.99% (4.10%)
The results show that the F-score is still high. The corresponding FPRs for
our simple majority scheme are zero in all cases, which eliminates the primary
concern in our setting as legitimate tenants would rarely be ﬂagged or shut down.
The reason for the 0% FPR is that previously, we were classifying each 2 s HPC
sample individually. In such a scenario there is a possibility that a particular
sample belonging to a non-miner exhibits HPC values matching those of a miner
(perhaps due to a hashing intensive phase in the execution). However, since now
we’re looking at all samples of a single application collectively, the chances of all
samples being hashing intensive (or even a majority of them) for a non-miner app
are rare and hence the 0% FPR. The corresponding FNRs are a bit high, however
this is less of a concern for the following reasons. First, since mining is commonly
a long-term activity the attacker will eventually get ﬂagged in a subsequent scan
even if he evades the classiﬁer once or twice. Second, if the attacker uses multiple
VMs to form a personal mining pool, then with high likelihood one of their VMs
will get ﬂagged (even if other VMs successfully evade the classiﬁer), which would
trigger MineGuard to immediately scan all other VMs that are part of the same
deployment again and if more VMs are caught, the cloud provider can do a more
comprehensive check of the entire deployment using VMI or other more invasive
tools.
Taken collectively, these results indicate that MineGuard is extremely adept
at identifying miners running in any operating environment. Even in the worse
case of detecting miners running in noisy environments, it achieves very high
accuracy.
Mitigating Covert Mining Operations in Clouds and Enterprises
305
Fig. 9. Accuracy of miner classiﬁcation in a VM environment, in terms of (A) average
F-score, (B) average false positive rate and average false negative rate, as the number
of features is increased.
Eﬀect of Signature Size on Accuracy: Figure 9 captures the relationship
between the size of the signature (number of top counters used) and the accu-
racy of detection in a VM environment for both open and closed world settings.
As shown in Fig. 9A, for the closed world scenario (triangles) even when only
2 counters are used, we achieve an average F-score above 99.5%, an average
false positive rate (FPR) below 0.5% and an average false negative rate (FNR)
of approximately 0, shown in Fig. 9B. This implies that MineGuard can actually
work with very small signature footprints speeding up all processes from proﬁl-
ing to matching. Similarly, in the open world case (circles) with only 3 counters
the average F-score is around 85% and jumps to 90% if we consider the top
7 counters. Increasing the size further brings marginal increases that ultimately
take the detection rate close to 95% for all 26 counters. An opposite downward
trend is observed in the average values of FP and FN for the open world case
as shown in Fig. 9B, with the rates declining all the way to roughly 5% when
the entire vector of HPCs is used. These numbers might appear a bit high, but
as we argue in Sect. 7 the open world case is highly unlikely as unseen mining
algorithms are an extremely rare possibility.
7 Discussion
We discuss a few aspects of our work in this section and explore potential limi-
tations.
Custom or Zero-Day Cryptocurrency: Is MineGuard vulnerable to zero-
day or custom coins? We believe it is not. By deﬁnition, zero-day or custom
coins do not exist because for a coin to have any dollar value, it ﬁrst needs to
have a known PoW algorithm, needs to be recognized by the cryptocommunity
as mathematically sound and has to be adopted at a moderate-to-large scale
for the core network to exist. Therefore, the ﬁrst time a new piece of malware
for some new coin makes an appearance in the wild, it would already be well-
known in the cryptocurrency community, giving cloud vendors enough time to
306
R. Tahir et al.
train MineGuard on the new coin’s signature, as its algorithm would be public
knowledge as mentioned above.
Evasion: An attacker can employ several techniques to try and evade Mine-
Guard’s detection scheme. First, they could employ known techniques of software
obfuscation. However, since we target the algorithm and not the implementation,
we believe that the attacker would have limited success (as shown in Sect. 6).
Second, the attacker could artiﬁcially manipulate the counters by performing
alternate computations that modify a distinct set of counters orthogonal to the
ones used in mining. Again, as we have shown in Sect. 6, this would severely
penalize the hash rate of the attacker while having very limited impact on his
chances of evading MineGuard. Thirdly, the attacker could attempt to stay under
the radar and mine at an extremely low hash rate. Theoretically, this is a limita-
tion since the attacker can evade MineGuard by sticking to low hash rates. How-
ever, we argue that the whole exercise becomes non-proﬁtable for the attacker
and nulliﬁes the whole point of mining on clouds. Furthermore, low hash rates
eliminate the original problem of resource abuse making it less of a nuisance.
Finally, the attacker could try to determine when the VM is being proﬁled by
MineGuard and stop mining temporarily. However, there are numerous issues
with this evasion scheme. First, since there is no measurable proﬁling overhead,
it is hard for an adversary to tell if their VM is being proﬁled. Second, instead
of monitoring the VMs in a round-robin fashion, the hypervisor can monitor the
VMs randomly, making it impossible to predict when a VM would be proﬁled.
Coin Diversity: We could not perform analysis on all cryptocurrencies available
in the market and chose to work with a popular subset (choosing coins with dis-
tinct algorithms and ignoring those which were forks of popular coins) as shown
in Table 1. Additionally, with the above restriction in mind we selected coins that
collectively comprise the largest share of market cap. Also, we justify our choice
by highlighting that most cryptocurrency exchanges, like Kraken [23], only deal
with the top 25–30 cryptocurrencies, as other altcoins have exceptionally low
dollar value and proﬁt margins from transactions are very low [22]. Moreover,
documented cases of cryptocurrency Trojans have been mostly limited to the top
10–15 coins [12,13,16,21,24]. Hence, attackers avoid wasting precious hashes on
less valuable coins, which is why we chose our subset of popularly used coins.
Nevertheless, we list this as a limitation, since the possibility, however minute,
of an excluded coin’s signature matching a cloud app still remains.
8 Related Work
Cloud abuse has become a hot topic of research. Recent eﬀorts [40,52] have been
geared towards developing a sound understanding of the problems and vulner-
abilities inherent to clouds. Others have demonstrated novel ways of exploiting
these vulnerabilities by building practical systems that are of value to attack-
ers, such as ﬁle sharing applications [51], unlimited storage banks [43] and
email-based storage overlays [48]. To mitigate these concerns, researchers have
Mitigating Covert Mining Operations in Clouds and Enterprises
307
proposed various Virtual Machine Introspection (VMI) approaches [36,42,45].
However, some of these are voluntary and require user participation [30], which
of course the attacker wants no part of, and others have a large overhead [41].
Furthermore, these VMI-based approaches are designed to observe the memory,
disk and processor state of customers’ VMs, which is a serious privacy concern
given the sensitive nature of customer data.
A diﬀerent yet related line of work attempts to describe the infrastructure
and mechanism of mining botnets. Huang et al. [39] present a thorough investi-
gation of mining ecosystems in the wild. They claim that mining is less proﬁtable
than other malicious activities, such as spamming or booter-renting (DDoS for
hire), and should be used as a secondary monetizing scheme. However, we believe
that it is unfair to compare mining proﬁts with other monetizing activities as the
price of coins varies substantially over time and as of this writing, the value of one
Bitcoin is a $1000 (and rising) as opposed to $100 in 2013, which demonstrates
that mining can generate an order of magnitude more revenue now. Further-
more, as mining uses an orthogonal set of resources (CPU/GPU and memory)
compared to DDoS attacks (network), we postulate that botnet-herders should
maximize their proﬁts by running various resource-disjoint monetizing activities
in parallel making a strong case for covert cryptomining. Indeed, Sophos Secu-
rity presented evidence that mining botnets could potentially generate around
$100,000 per day of proﬁts for herders [8].
Finally, there has been much research on detecting generic malware using
architectural and microarchitectural execution patterns, such as HPCs, with
diﬀering results. Demme et al. [35] built a system for detection of generic mal-
ware and demonstrate the feasibility of the design based on ARM (Android)
and Intel (Linux) platforms. Other researchers [38,50,55] have also used low-
level hardware features to promising success, furthering the work of Demme
et al. In addition to generic malware, HPCs have also been successfully used
to detect kernel-level rootkits [53], side-channel attacks [34], ﬁrmware modiﬁca-
tions [54] etc. However, none of these previous works try to accommodate the
noise introduced by virtualization, as we do in this work.
9 Conclusion
We present MineGuard, a userspace tool that prevents abuse of resources at
the hands of hackers interested in mining cryptocurrencies on others’ resources.
Whether the mining operation is local (restricted to one VM) or being con-
ducted in a pool of participating VMs, MineGuard can successfully detect and
shutdown the illegitimate mining “ring”. We empirically demonstrate that our
design imposes negligible overhead to legitimate tenants and can detect mining
in real-time with high precision. If multiple VMs are involved in mining, Mine-
Guard can collaborate with other MineGuard instances to expose the entire
footprint of the mining deployment. For detection, MineGuard uses signatures
based on Hardware Performance Counters for both CPU and GPU-based min-
ers. The fact that MineGuard runs on top of the hypervisor or the host OS
308
R. Tahir et al.
prevents miners running inside the VMs from subverting it despite root access
on the guest. We also account for the noise generated as a result of virtualization
to provide error correction for our detection mechanisms. In the future, we plan
to extend MineGuard to accurately detect other types of malwares in highly
multiplexed and virtualized environments.
References
1. Bitcoin Anonymizer TOR Wallet. https://torwallet.com/
2. CryptoNight. https://en.bitcoin.it/wiki/CryptoNight
3. CUDA Toolkit Documentation. https://tinyurl.com/z7bx3b3
4. Government employee caught mining using work supercomputer. https://tinyurl.
com/mrpqﬀd
5. ABC employee caught mining for Bitcoins on company servers (2011). https://
tinyurl.com/lxcujtx
6. Data Center Power and Cooling. CISCO White Paper (2011)
7. How to Get Rich on Bitcoin, By a System Administrator Who’s Secretly Growing
Them On His School’s Computers (2011). https://tinyurl.com/lwx8rup
8. The ZeroAccess Botnet - Mining and Fraud for Massive Financial Gain (2012).
https://tinyurl.com/ldgcfao
9. Online Thief Steals Amazon Account to Mine Litecoins in the Cloud (2013).
https://tinyurl.com/mzpbype
10. Harvard Research Computing Resources Misused for Dogecoin Mining Operation
(2014). https://tinyurl.com/n8pzvt6
11. How Hackers Hid a Money-Mining Botnet in the Clouds of Amazon and Others
(2014). https://tinyurl.com/mowzx73
12. List of Major Bitcoin Heists, Thefts, Hacks, Scams, and Losses (2014). https://
bitcointalk.org/index.php?topic=576337
13. Mobile Malware Mines Dogecoins and Litecoins for Bitcoin Payout (2014). https://
tinyurl.com/q828blg
14. NAS device botnet mined $600,000 in Dogecoin over two months (2014). https://
tinyurl.com/myglgoa
15. US Government Bans Professor for Mining Bitcoin with A Supercomputer (2014).
https://tinyurl.com/k3ww4rp
16. Adobe Flash Player Exploit Could Be Used to Install BitCoinMiner Trojan (2015).
https://tinyurl.com/lhxzloa
17. Cloud Mining Put to the Test- Is It Worth Your Money? (2015). https://tinyurl.
com/zquylbo
18. Developer Hit with $6,500 AWS Bill from Visual Studio Bug (2015). https://
tinyurl.com/zm3pzjq
19. Perf Tool Wiki (2015). https://tinyurl.com/2enxbko
20. Standard Performance Evaluation Corporation (2015). https://www.spec.org/
benchmarks.html
21. Trojan, C.: A Grave Threat to BitCoin Wallets (2016). https://tinyurl.com/
k73wdaq
22. Crypto-Currency Market Capitalizations (2016). https://coinmarketcap.com/
23. Kraken Bitcoin Exchange (2016). https://www.kraken.com/
24. Linux. Lady. 1 Trojan Infects Redis Servers and Mines for Cryptocurrency (2016).
urlhttps://tinyurl.com/ka9ae4c
Mitigating Covert Mining Operations in Clouds and Enterprises
309
25. Randomized Decision Trees: A Fast C++ Implementation of Random Forests
(2016). https://github.com/bjoern-andres/random-forest
26. Student uses university computers to mine Dogecoin (2016). https://tinyurl.com/
lubeqct
27. Supplemental Terms and Conditions For Google Cloud Platform Free Trial (2017).
https://tinyurl.com/ke5vs49
28. Akaike, H.: A new look at the statistical model identiﬁcation. IEEE TAC 19 (1974)
29. Marosi, A.: Cryptomining malware on NAS servers (2016)
30. Baek, H.W., Srivastava, A., van der Merwe, J.E.: Cloudvmi: virtual machine intro-
spection as a cloud service. In: 2014 IEEE International Conference on Cloud
Engineering (2014)
31. Brown, G., Pocock, A.C., Zhao, M., Luj´an, M.: Conditional likelihood maximisa-
tion: a unifying framework for information theoretic feature selection. In: JMLR
(2012)
32. Percival, C., Josefsson, S.: The Scrypt Password-Based Key Derivation Function.
IETF (2012)
33. Che, S., et al.: Rodinia: A benchmark suite for heterogeneous computing. In: Pro-
ceedings of the 2009 IEEE International Symposium on Workload Characterization
(2009)
34. Chiappetta, M., Savas, E., Yilmaz, C.: Real time detection of cache-based side-
channel attacks using hardware performance counters. IACR Cryptol. ePrint
Archive 2015, 1034 (2015)
35. Demme, J., Maycock, M., Schmitz, J., Tang, A., Waksman, A., Sethumadhavan,
S., Stolfo, S.J.: On the feasibility of online malware detection with performance
counters. In: The 40th Annual ISCA (2013)
36. Dinaburg, A., Royal, P., Sharif, M.I., Lee, W.: Ether: malware analysis via hard-
ware virtualization extensions. In: ACM CCS (2008)
37. Ferdman, M., Adileh, A., Ko¸cberber, Y.O., Volos, S., Alisafaee, M., Jevdjic, D.,
Kaynak, C., Popescu, A.D., Ailamaki, A., Falsaﬁ, B.: Clearing the clouds: a study
of emerging scale-out workloads on modern hardware. In: ASPLOS (2012)
38. Garcia-Serrano, A.: Anomaly detection for malware identiﬁcation using hardware
performance counters. CoRR (2015)
39. Huang, D.Y., Dharmdasani, H., Meiklejohn, S., Dave, V., Grier, C., McCoy, D.,
Savage, S., Weaver, N., Snoeren, A.C., Levchenko, K.: Botcoin: monetizing stolen
cycles. In: NDSS (2014)
40. Idziorek, J., Tannian, M.: Exploiting cloud utility models for proﬁt and ruin. In:
IEEE CLOUD (2011)
41. Jiang, X., Wang, X., Xu, D.: Stealthy malware detection and monitoring through
VMM-based “out-of-the-box” semantic view reconstruction. ACM Trans. Inf. Syst.
Secur. 13(2), 12:1–12:28 (2010)
42. Lengyel, T.K., Neumann, J., Maresca, S., Payne, B.D., Kiayias, A.: Virtual machine
introspection in a hybrid honeypot architecture. In: CSET (2012)
43. Mulazzani, M., Schrittwieser, S., Leithner, M., Huber, M., Weippl, E.R.: Dark
clouds on the horizon: using cloud storage as attack vector and online slack space.
In: 20th USENIX Security Symposium (2011)
44. National Science Foundation Oﬃce of
Inspector General: SEMIANNUAL
REPORT TO CONGRESS (2014)
45. Payne, B.D., Lee, W.: Secure and ﬂexible monitoring of virtual machines. In:
ACSAC (2007)
46. Sembrant, A.: Low Overhead Online Phase Predictor and Classiﬁer. Master’s the-
sis, UPPSALA UNIVERSITET (2011)
310
R. Tahir et al.
47. Sokolova, M., Lapalme, G.: A systematic analysis of performance measures for
classiﬁcation tasks. Inf. Process. Manage. 45, 427–437 (2009)
48. Srinivasan, J., Wei, W., Ma, X., Yu, T.: EMFS: email-based personal cloud storage.
In: NAS (2011)
49. Stratton, J.A., et al.: Parboil: A revised benchmark suite for scientiﬁc and com-
mercial throughput computing. In: IMPACT Technical report (2012)
50. Tang, A., Sethumadhavan, S., Stolfo, S.J.: Unsupervised anomaly-based malware
detection using hardware features. In: Stavrou, A., Bos, H., Portokalidis, G. (eds.)
RAID 2014. LNCS, vol. 8688, pp. 109–129. Springer, Cham (2014). doi:10.1007/
978-3-319-11379-1 6
51. Tinedo, R.G., Artigas, M.S., L´opez, P.G.: Cloud-as-a-gift: eﬀectively exploiting
personal cloud free accounts via REST apis. In: IEEE CLOUD (2013)
52. Vaquero, L.M., Rodero-Merino, L., Mor´an, D.: Locking the sky: a survey on IaaS
cloud security. Computing 91(1), 93–118 (2011)
53. Wang, X., Karri, R.: Numchecker: detecting kernel control-ﬂow modifying rootkits
by using hardware performance counters. In: The 50th Annual DAC (2013)
54. Wang, X., Konstantinou, C., Maniatakos, M., Karri, R.: Conﬁrm: Detecting
ﬁrmware modiﬁcations in embedded systems using hardware performance coun-
ters. In: ICCAD (2015)
55. Yuan, L., Xing, W., Chen, H., Zang, B.: Security breaches as PMU deviation:
detecting and identifying security attacks using performance counters. In: APSys
(2011)