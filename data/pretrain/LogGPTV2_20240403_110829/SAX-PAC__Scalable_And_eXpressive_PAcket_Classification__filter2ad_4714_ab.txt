fication time (i.e., time complexity) and memory footprint
(i.e., space complexity) [55]. The main idea is to split nodes
in the decision tree by “cutting” them along one or more
Figure 3: Rule partition.
dimensions. Starting from the root which contains all rules,
these algorithms iteratively split/cut the nodes until each
leaf contains fewer than a predefined number of rules. Given
a decision tree, classifying a packet reduces to walk the tree
from the root to a leaf, and then chose the highest priority
rule associated with that leaf.
Figure 2 illustrates this technique. The packet classifier
contains six rules (R0 to R5) in a two-dimensional space.
Figure 2(a) shows each rule as a rectangle in the space, and
represents the cuts as dashed lines. Figure 2(b) shows the
corresponding decision tree for this packet classifier. The
root of the tree contains all the six rules. First, we cut the
entire space (which represents the root) into four chunks
along dimension x. This leads to the creation of four children.
If a rule intersects a child’s chunk, it is added to that child.
For example, R1, R3 and R4 all intersect the first chunk (i.e.,
the first quarter in this space), and thus they are all added to
the first root’s child. If a rule intersects multiple chunks it is
added to each corresponding child, e.g., R1 is added to all the
four children. Next, we cut the chunk corresponding to each
of the four children along dimension y. As a result, each of
the nodes at the first level will end up with two children.
Rule partition. One challenge with "blindly" cutting a node
is that we might end up with a rule being replicated to a large
number of nodes [55]. In particular, if a rule has a large size
along one dimension, cutting along that dimension will result
3
R1R0R4R2R5R3XYR0,R1, R2, R3, R4,R5R1,R3,R4R0,R1,R4R1,R2,R4R1,R4,R5R3,R4R1R4R0,R1R4R1,R2R4,R5R1,R5(a) Packet classifier.(b) Decision tree.R0R2R5R3XY(a) Partition 1.(b) Partition 2.R1R4XYR0,R2,R3, R5R3R0R2R5R1, R4R1R4in that rule being added to many nodes. For example, rule
R1 in Figure 2(a) has a large size in dimension x. Thus, when
cutting along dimension x, R1 will end up being replicated
at every node created by the cut. Rule replication can lead to
decision trees with larger depths and sizes, which translate
to higher classification time and memory footprint.
One solution to address this challenge is to first partition
rules based on their "shapes". Broadly speaking, rules with
large sizes in a particular dimension are put in the same set.
Then, we can build a separate decision tree for each of these
partitions. Figure 3 illustrates this technique. The six rules
in Figure 2 are grouped into two partitions. One partition
consists of rules R1 and R4, as both these rules have large
sizes in dimension x. The other partition consists of the other
four rules, as these rules have small sizes in dimension x.
Figure 3(a) and Figure 3(b) show the corresponding decision
trees for each partition. Note that the resulting trees have
lower depth, and smaller number of rules per node as com-
pared to the original decision tree in Figure 2(b). To classify
a packet, we classify it against every decision tree, and then
choose the highest priority rule among all rules the packet
matches in all decision trees.
Summary. Existing solutions build decision trees by em-
ploying two types of actions: node cutting and rule partition.
These solutions mainly differ in the way they decide (i) at
which node to apply the action, (ii) which action to apply,
and (iii) how to apply it (e.g., along which dimension(s) to
partition).
3 A LEARNING-BASED APPROACH
In this section, we describe a learning-based approach for
packet classification. We motivate our approach, discuss the
formulation of classification as a learning problem, and then
present our solution.
3.1 Why Learn?
The existing solutions for packet classification rely on hand-
tuned heuristics to build decision trees. Unfortunately, this
leads to two major limitations.
First, these heuristics often face a difficult trade-off be-
tween performance and cost. Tuning such a heuristic for a
given set of rules is an expensive proposition, requiring con-
siderable human efforts and expertise. Worse yet, when given
a different rule set, one might have to do this all over again.
Addressing this challenge has been the main driver of a long
line of research over the past two decades [13, 30, 41, 47, 55].
Of course, one could build a general heuristic for a large
variety of rule sets. Unfortunately, such a solution would not
provide the best performance for a given set of rules.
Second, existing algorithms do not directly optimize for a
global objective. Ideally, a good packet classification solution
should optimize for (i) classification time, (ii) memory foot-
print, or (iii) a combination between the two. Unfortunately,
the existing heuristics do not directly optimize for any of
these objectives. At their core, these heuristics make greedy
decisions to build decision trees. At every step, they decide
on whether to cut a node or partition the rules based on
simple statistics (e.g., the size of the rules in each dimen-
sion, number of unique ranges in each dimension), which
are poorly correlated with the desired objective. As such, the
resulting decision trees are often far from being optimal.
As we will see, a learning-based approach can address
these limitations. Such an approach can learn to generate
an efficient decision tree for a specific set of rules without
the need to rely on hand-tuned heuristics. This is not to say
these heuristics do not have value; in fact they often contain
key domain knowledge that we show can be leveraged and
improved on by the learning algorithm.
3.2 What to Learn?
Classification is a central task in machine learning literature.
The recent success of using deep neural networks (DNNs) for
image recognition, speech recognition and language transla-
tion has been single-handedly responsible for the recent AI
"revolution" [10, 23, 51].
As such, one natural solution for packet classification
would be to replace a decision tree with a DNN. In particular,
such DNN will take as input the fields of a packet header and
output the rule matching that packet. Related to our problem,
prior work has shown that DNN models can be effectively
used to replace B-Trees for indexing [22].
However, this solution has two drawbacks. First, a DNN-
based classifier does not guarantee 100% accuracy. This is
because training a DNN is fundamentally a stochastic pro-
cess. Second, given a DNN packet classification result, it
is expensive to verify whether the result is correct or not.
Unlike the recently proposed learned index solution to re-
place B-Trees [22], the rules in packet classification are multi-
dimensional and overlap with each other. If a rule matches a
packet, we still need to check other rules to see if this rule
has the highest priority among all matched rules.
To avoid these drawbacks, in this paper we propose to
learn building decision trees for a given set of rules. Since
the result is still a decision tree, we can guarantee correct-
ness, and it will be easy to deploy the classifier with existing
systems (hardware and software) compared to a DNN.
3.3 How to Learn?
In this section, we show that the problem of building decision
trees maps naturally to RL. As illustrated in Figure 4(a), an
RL system consists of an agent that repeatedly interacts
with an environment. The agent observes the state of the
4
Neural Packet Classification
(a)
(b)
Figure 4: (a) Classic RL system. An agent takes an ac-
tion, At , based on the current state of the environment,
St , and applies it to the environment. This leads to a
change in the environment state (St +1) and a reward
(Rt +1). (b) NeuroCuts as an RL system.
environment, and then takes an action that might change
the environment’s state. The goal of the agent is to compute
a policy that maps the environment’s state to an action in
order to optimize a reward. As an example, consider an agent
playing chess. In this case, the environment is the board, the
state is the position of the pieces on the board, an action is
moving a piece on the board, and the reward could be 1 if
the game is won, and −1, if the game is lost.
This simple example illustrates two characteristics of RL
that are a particularly good fit to our problem. First, rewards
are sparse, i.e., not every state has associated a reward. For
instance, when moving a piece we do not necessary know
whether that move will result in a win or loss. Second, the
rewards are delayed; we need to wait until the end of the
game to see whether the game was won or lost.
To deal with large state and action spaces, recent RL so-
lutions have employed DNNs to implement their policies.
These solutions, called Deep RL, have achieved remarkable
results matching humans at playing Atari games [37], and
beating the Go world champion [46]. These results have en-
couraged researchers to apply Deep RL to networking and
systems problems, from routing, to congestion control, to
video streaming, and to job scheduling [4, 6, 16, 34, 35, 54,
62, 64, 65]. Building a decision tree can be easily cast as an
RL problem: the environment’s state is the current decision
tree, an action is either cutting a node or partitioning a set of
rules, and the reward is either the classification time, memory
footprint, or a combination of the two. While in some cases
there are legitimate concerns about whether Deep RL is the
right solution for the problem at hand, we identify several
characteristics that make packet classification a particularly
good fit for Deep RL.
First, when we take an action, we do not know for sure
whether it will lead to a good decision tree or not; we only
know this once the tree is built. As a result, the rewards in
our problem are both sparse and delayed. This is naturally
captured by the RL formulation.
Second, the explicit goal of RL is to maximize the reward.
Thus, unlike existing heuristics, our RL solution aims to
explicitly optimize the performance objective, rather than
using local statistics whose correlation to the performance
objective can be tenuous.
Third, one potential concern with Deep RL algorithms
is sample complexity. In general, these algorithms require
a huge number of samples (i.e., input examples) to learn a
good policy. Fortunately, in the case of packet classification
we can generate such samples cheaply. A sample, or rollout,
is a sequence of actions that builds a decision tree with the
associated reward(s) by using a given policy. The reason we
can generate these rollouts cheaply is because we can build
all these trees in software, and do so in parallel. Contrast this
with other RL-domains, such as robotics, where generating
each rollout can take a long time and requires expensive
equipment (i.e., robots).
4 NEUROCUTS DESIGN
4.1 NeuroCuts Overview
We introduce the design for NeuroCuts, a new Deep RL
formulation of the packet classification problem. Given a
rule set and an objective function (i.e., classification time,
memory footprint, or a combination of both), NeuroCuts
learns to build a decision tree that minimizes the objective.
Figure 4(b) illustrates the framing of NeuroCuts as an RL
system: the environment consists of the set of rules and
the current decision tree, while the agent uses a model (im-
plemented by a DNN) that aims to select the best cut or
partition action to incrementally build the tree. A cut action
divides a node along a chosen dimension (i.e., one of SrcIP,
DstIP, SrcPort, DstPort, and Protocol) into a number of
sub-ranges (i.e., 2, 4, 8, 16, or 32 ranges), and creates that
many child nodes in the tree. A partition action on the other
hand divides the rules of a node into disjoint subsets (e.g.,
based on the coverage fraction of a dimension), and creates a
new child node for each subset. The available actions for the
5
AgentEnvironmentactionAtstateStrewardRtRt+1St+1StateAgentNeural NetworkEnvironmentPacket ClassifierDecision TreeactionAtstateStrewardRtRt+1St+1current node are advertised by the environment at each step,
the agent chooses among them to generate the tree, and over
time the agent learns to optimize its decisions to maximize
the reward from the environment. Figure 5 visualizes the
learning process of NeuroCuts.
4.2 NeuroCuts Training Algorithm
Recall that the goal of an RL algorithm is to compute a policy
to maximize rewards from the environment. Referring again
to Figure 4, the environment defines the action space A and
state space S. The agent starts with an initial policy, evalu-
ates it using multiple rollouts, and then updates it based on
the results (rewards) of these rollouts. Then, it repeats this
process until satisfied with the reward.
We first consider a strawman formulation of decision tree
generation as a single Markov Decision Process (MDP). In
this framing, a rollout begins with a tree consisting of a
single node. This is the initial state, s0 ∈ S. At each step t,
the agent executes an action at ∈ A and receives a reward
rt; the environment transitions from the current state st ∈ S
to the next state st +1 ∈ S (i.e., the updated tree and next
node to process). The goal is to maximize the total reward
t γ trt where γ is a discounting
received by the agent, i.e.,
factor used to prioritize more recent rewards.
Design challenges. While at a high level this RL formula-
tion seems straightforward, there are three key challenges we
need to address before we have a realizable implementation.
The first is how to encode the variable-length decision tree
state st as an input to the neural network policy. While it is
possible to flatten the tree, say, into an 1-dimensional vector,
the size of such a vector would be very large (i.e., hundreds
of thousands of units). This will require both a very large
network model to process such input, and a prohibitively
large number of samples.
While recent work has proposed leveraging recurrent neu-
ral networks (RNNs) and graph embedding techniques [58,
60, 61] to reduce the input size, these solutions are brittle in
the face of large or dynamically growing graph structures
[66]. Rather than attempting to solve the state representa-
tion problem to deal with large inputs, in NeuroCuts we
instead take advantage of the underlying structure of packet
classification trees to design a simple and compact state rep-
resentation. This means that when the agent is deciding how
to split a node, it only observes a fixed-length representation
of the node. All needed state is encoded in the representation;
no other information about the rest of the tree is observed.
The second challenge is how to deal with the sparse and
delayed rewards incurred by the node-by-node process of
building the decision tree. While we could in principle return
a single reward to the agent when the tree is complete, it
would be very difficult to train an agent in such an envi-
ronment. Due to the long length of tree rollouts (i.e., many
thousands of steps), learning is only practical if we can com-
pute meaningful dense rewards.1 Such a dense reward for an
action would be based on the statistics of the subtree it leads
to (i.e., its depth or size).2 Unfortunately, it is not possible to
compute this until the subtree is complete. To handle this, we
take the somewhat unusual step of only computing rewards
for the rollout when the tree is completed, and setting γ = 0,
effectively creating a series of 1-step decision problems sim-
ilar to contextual bandits [25]. However, unlike the bandit
setting, these 1-step decisions are connected through the
dynamics of the tree building process.
Another way of looking at the dense reward problem is
that the process of building a decision tree is not really se-
quential but tree-structured (i.e., it is more accurately modeled
as a branching decision process [8, 18, 40]), and we need to
account for the reward calculations accordingly. In such a