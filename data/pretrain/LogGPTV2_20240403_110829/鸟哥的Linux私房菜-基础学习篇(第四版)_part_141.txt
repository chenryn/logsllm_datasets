size=1048576K mtime=Thu Jun 25 00:35:01 2015 # 某些时刻会出现这个东西！没关系的！
Continue creating array? y
mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md0 started.
# 详细的参数说明请回去前面看看啰！这里我通过 {} 将重复的项目简化！
# 此外，因为鸟哥这个系统经常在创建测试的环境，因此系统可能会抓到之前的 filesystem
# 所以就会出现如上前两行的讯息！那没关系的！直接按下 y 即可删除旧系统
[root@study ~]# mdadm --detail /dev/md0
/dev/md0: # RAID 的设备文件名
Version : 1.2
Creation Time : Mon Jul 27 15:17:20 2015 # 创建 RAID 的时间
Raid Level : raid5 # 这就是 RAID5 等级！
Array Size : 3142656 （3.00 GiB 3.22 GB） # 整组 RAID 的可用容量
Used Dev Size : 1047552 （1023.17 MiB 1072.69 MB） # 每颗磁盘（设备）的容量
Raid Devices : 4 # 组成 RAID 的磁盘数量
Total Devices : 5 # 包括 spare 的总磁盘数
Persistence : Superblock is persistent
Update Time : Mon Jul 27 15:17:31 2015
State : clean # 目前这个磁盘阵列的使用状态
Active Devices : 4 # 启动（active）的设备数量
Working Devices : 5 # 目前使用于此阵列的设备数
Failed Devices : 0 # 损坏的设备数
Spare Devices : 1 # 预备磁盘的数量
Layout : left-symmetric
Chunk Size : 256K # 就是 chunk 的小区块容量
Name : study.centos.vbird:0 （local to host study.centos.vbird）
UUID : 2256da5f:4870775e:cf2fe320:4dfabbc6
Events : 18
Number Major Minor RaidDevice State
0 252 5 0 active sync /dev/vda5
1 252 6 1 active sync /dev/vda6
2 252 7 2 active sync /dev/vda7
5 252 8 3 active sync /dev/vda8
4 252 9 - spare /dev/vda9
# 最后五行就是这五个设备目前的情况，包括四个 active sync 一个 spare ！
# 至于 RaidDevice 指的则是此 RAID 内的磁盘顺序
由于磁盘阵列的创建需要一些时间，所以你最好等待数分钟后再使用“ mdadm --detail /dev/md0 ”去查阅你的磁盘阵列详细信息！ 否则有
可能看到某些磁盘正在“spare rebuilding”之类的创建字样！通过上面的指令， 你就能够创建一个 RAID5 且含有一颗 spare disk 的磁盘阵列啰！
非常简单吧！ 除了指令之外，你也可以查阅如下的文件来看看系统软件磁盘阵列的情况：
[root@study ~]# cat /proc/mdstat
Personalities : [raid6] [raid5] [raid4]
md0 : active raid5 vda8[5] vda9[4]（S） vda7[2] vda6[1] vda5[0] 
上述的数据比较重要的在特别指出的第一行与第二行部分[3]：
第一行部分：指出 md0 为 raid5 ，且使用了 vda8, vda7, vda6, vda5 等四颗磁盘设备。每个设备后面的中括号 [] 内的数字为此磁盘在
RAID 中的顺序 （RaidDevice）；至于 vda9 后面的 [S] 则代表 vda9 为 spare 之意。
第二行：此磁盘阵列拥有 3142656 个block（每个 block 单位为 1K），所以总容量约为 3GB， 使用 RAID 5 等级，写入磁盘的小区块
（chunk） 大小为 256K，使用 algorithm 2 磁盘阵列演算法。 [m/n] 代表此阵列需要 m 个设备，且 n 个设备正常运行。因此本 md0 需要 4
个设备且这 4 个设备均正常运行。 后面的 [UUUU] 代表的是四个所需的设备 （就是 [m/n] 里面的 m） 的启动情况，U 代表正常运行，若
为 _ 则代表不正常。
这两种方法都可以知道目前的磁盘阵列状态啦！
格格式式化化与与挂挂载载使使用用 RAID
接下来就是开始使用格式化工具啦！这部分就需要注意喔！因为涉及到 xfs 文件系统的优化！还记得第七章的内容吧？我们这里的参数
为：
srtipe （chunk） 容量为 256K，所以 su=256k
共有 4 颗组成 RAID5 ，因此容量少一颗，所以 sw=3 喔！
由上面两项计算出数据宽度为： 256K*3=768k
所以整体来说，要优化这个 XFS 文件系统就变成这样：
[root@study ~]# mkfs.xfs -f -d su=256k,sw=3 -r extsize=768k /dev/md0
# 有趣吧！是 /dev/md0 做为设备被格式化呢！
[root@study ~]# mkdir /srv/raid
[root@study ~]# mount /dev/md0 /srv/raid
[root@study ~]# df -Th /srv/raid
Filesystem Type Size Used Avail Use% Mounted on
/dev/md0 xfs 3.0G 33M 3.0G 2% /srv/raid
# 看吧！多了一个 /dev/md0 的设备，而且真的可以让你使用呢！还不赖！
俗话说“天有不测风云、人有旦夕祸福”，谁也不知道你的磁盘阵列内的设备啥时会出差错，因此， 了解一下软件磁盘阵列的救援还是必
须的！下面我们就来玩一玩救援的机制吧！首先来了解一下 mdadm 这方面的语法：
[root@study ~]# mdadm --manage /dev/md[0-9] [--add 设设备备] [--remove 设设备备] [--fail 设设备备]
选项与参数：
--add ：会将后面的设备加入到这个 md 中！
--remove ：会将后面的设备由这个 md 中移除
--fail ：会将后面的设备设置成为出错的状态
设设置置磁磁盘盘为为错错误误 （（fault））
首先，我们来处理一下，该如何让一个磁盘变成错误，然后让 spare disk 自动的开始重建系统呢？
# 0. 先复制一些东西到 /srv/raid 去，假设这个 RAID 已经在使用了
[root@study ~]# cp -a /etc /var/log /srv/raid
[root@study ~]# df -Th /srv/raid ; du -sm /srv/raid/*
Filesystem Type Size Used Avail Use% Mounted on
/dev/md0 xfs 3.0G 144M 2.9G 5% /srv/raid
28 /srv/raid/etc <==看吧！确实有数据在里面喔！
51 /srv/raid/log
# 1. 假设 /dev/vda7 这个设备出错了！实际仿真的方式：
[root@study ~]# mdadm --manage /dev/md0 --fail /dev/vda7
mdadm: set /dev/vda7 faulty in /dev/md0 # 设置成为错误的设备啰！
/dev/md0:
.....（中间省略）.....
Update Time : Mon Jul 27 15:32:50 2015
State : clean, degraded, recovering
Active Devices : 3
Working Devices : 4
Failed Devices : 1 <==出错的磁盘有一个！
Spare Devices : 1
.....（中间省略）.....
Number Major Minor RaidDevice State
0 252 5 0 active sync /dev/vda5
1 252 6 1 active sync /dev/vda6
4 252 9 2 spare rebuilding /dev/vda9
5 252 8 3 active sync /dev/vda8
2 252 7 - faulty /dev/vda7
# 看到没！这的动作要快做才会看到！ /dev/vda9 启动了而 /dev/vda7 死掉了
上面的画面你得要快速的连续输入那些 mdadm 的指令才看的到！因为你的 RAID 5 正在重建系统！ 若你等待一段时间再输入后面的观
察指令，则会看到如下的画面了：
# 2. 已经借由 spare disk 重建完毕的 RAID 5 情况
[root@study ~]# mdadm --detail /dev/md0
....（前面省略）....
Number Major Minor RaidDevice State
0 252 5 0 active sync /dev/vda5
1 252 6 1 active sync /dev/vda6
4 252 9 2 active sync /dev/vda9
5 252 8 3 active sync /dev/vda8
2 252 7 - faulty /dev/vda7
看吧！又恢复正常了！真好！我们的 /srv/raid 文件系统是完整的！并不需要卸载！很棒吧！
将将出出错错的的磁磁盘盘移移除除并并加加入入新新磁磁盘盘
因为我们的系统那个 /dev/vda7 实际上没有坏掉啊！只是用来仿真而已啊！因此，如果有新的磁盘要替换，其实替换的名称会一样啊！
也就是我们需要：
1. 先从 /dev/md0 阵列中移除 /dev/vda7 这颗“磁盘”
2. 整个 Linux 系统关机，拔出 /dev/vda7 这颗“磁盘”，并安装上新的 /dev/vda7 “磁盘”，之后开机
3. 将新的 /dev/vda7 放入 /dev/md0 阵列当中！
# 3. 拔除“旧的”/dev/vda7 磁盘
[root@study ~]# mdadm --manage /dev/md0 --remove /dev/vda7
# 假设接下来你就进行了上面谈到的第 2, 3 个步骤，然后重新开机成功了！
# 4. 安装“新的”/dev/vda7 磁盘
[root@study ~]# mdadm --manage /dev/md0 --add /dev/vda7
[root@study ~]# mdadm --detail /dev/md0
....（前面省略）....
Number Major Minor RaidDevice State
0 252 5 0 active sync /dev/vda5
1 252 6 1 active sync /dev/vda6
4 252 9 2 active sync /dev/vda9
5 252 8 3 active sync /dev/vda8