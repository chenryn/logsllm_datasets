r
P
t
s
e
T
0
1
.
0
2
.
0
5
.
0
6
.
0
3
.
0
4
.
0
Epsilon attack-strength value
Candidate A (98.84)
Candidate B (96.13)
Candidate C (98.52)
Standard CNN(99.06)
Figure 7: Comparison of a Spartan Network vs its vanilla
CNN counterpart, against a FGSM attack with varying
epsilon-strength
Figure 8: Loss history during the training of a classical CNN
and a Spartan Network. We see a latency in the loss drop of
the Spartan Network’s training.
As the synthetic gradient allows the Spartan Network to know
the general direction of improvement, it can train even if the Heav-
iside step function does not yield a progressive, differentiable be-
haviour. After some time, the networks’ weights are trained enough
to go past the threshold when required, and the loss drops as sharply
as a standard CNN.
7.2.5 Resistance to Over-capacity. We note that in Fig. 7 Candidate
C’s precision begins to collapse at ϵ values close to 0.5. Note that,
at this level, the noise value is high enough to create a grey picture.
We see however that Candidate B manages to resist even after the
0.5 threshold. Upon further inspection, this Network has a 0.9 cutoff
threshold in its filters with close to 0 bias. This network focuses
on the brightest pixels. The attack did not supress enough bright
pixels to entirely corrupt the digits, and the Spartan Networks finds
the signal through the perturbation.
We report that out of the four given filters, the Candidate C
discarded two filters by making their activation impossible. The
network thus learns to ignore unnecessary capacity, by minimizing
its activation penalty. This resistance is comforted by the fact that
Candidate C learns that 1-bit black and white color is enough to
classify digits succesfully. This result is aligned with previous results
obtained by Xu et al. [43].
Spartan Networks thus have a reduced sensitivity to over-capacity.
We argue that this resistance is trading robustness for reduced per-
formance. This resistance assures that the hyperparameters intro-
duced by Spartan Networks do not slow the hyperperameter tuning
phase.
8 RISK EVALUATION OF A
ROBUSTNESS-PERFORMANCE TRADEOFF
Spartan Networks sacrifice a bit of precision for robustness. No-
tions of robustness-precision tradeoffs were already mentionned in
[17, 23].
To give a risk analysis of a situation, consider a Spartan Network
used in the context of a 4-digits check reading system, and consider
an adversarial check paper that automatically gets missclassified
as a 9999$ check. Let ∆C N→S N be the risk delta linked with going
from a "ConvNet" to a "SpartaNet". As the risk is the probability of
occurence times the impact, we get:
∆C N→S N = (peS N IeS N + ptS N ItS N ) − (peC N IeC N + ptC N ItC N )
Where I is an average impact value, p a probability of an event hap-
pening, SN and CN stand for Spartan Network and Convolutional
Network missclassification and t and e describe a theft scenario,
and an error scenario respectively.
As the Network does not change the Impact, only the probability,
we have:
∆C N→S N = (peS N − peC N )Ie + (ptS N − ptC N )It
Each of the above probabilities is a joint event of an scenario hap-
pening, and a missclassification happening. We consider the two
events disjoint.
∆C N→S N = (p(e, SN) − p(e, CN))Ie + (p(t, SN) − p(t, CN))It
arXiv Preprint, Dec 2018,
F. Menet et al.
We consider only abnormal check reading. α is the amount of mali-
cious bank checks in the overall non-normal checks. Let p(e, SN) −
p(e, CN) = ∆err , p(t, advSN) − p(t, advCN) = ∆adv, we get that:
∆C N→S N = (1 − α)∆err Ie + α ∆adv It
Diminishing the risks means that ∆C N→S N 
∆err Ie
∆err Ie − ∆adv It
Spartan Networks are less precise on clean samples. Thus, they
are more relevant in adversarial settings. This means that there is a
minimum amount of malicious checks that must be in use for the
bank to have an interest in them.
For instance, if we hypothesize that the attack is a ϵ = 0.3 FGSM
attack, we see in Fig. 7 that a Spartan Network is more robust to this
attack by 20%. However, we must also consider that our Network
comes with ∼ −0.5% precision on non-adversarial inputs. The risk
analysis must take into account the fact that non-malicious checks
are being misclassified more often than with a ConvNet.
Risk Management Implications. For example, if a non-malicious
error on a 4 digits check costs on average 50$, and a malicious error
costs 8999$ (1000$ → 9999$), Spartan Networks begin to reduce
the risk when around one erroneous check on 7200 is malicious.
We thus recommend that Spartan Networks be used in conjunc-
tion with attack detection, that can use Detection strategies. In
this setup, a classical, high-performance network would do the in-
ferrence until an attack is detected. When such an attack happens, a
Spartan Network could be used as a fallback network. In this adver-
sarial setting, their robustness would overcome their performance
issues.
9 CONCLUSION
We have presented Spartan Networks, deep neural networks that are
given a data-starving layer in order to select relevant features only.
The space-lowering effect allows the system to be more resistant
to adversarial examples. Filtering layers reduce performance, but
can, in specific threat models, be a cost-effective way to reduce an
attacker’s stealthiness and success rate. This robustness method is
made by trying to select relevant features, thus reinforcing deep
learning in its promise that feature selection can, in a long-term
view, be automated.
Contributions. Our contributions are the following:
• We introduce the concept of composite activation functions,
by separating the forward propagation and backwards prop-
agation functions in a Deep Neural Network.
• We introduce the idea of a self-adversarial layer, putting
an attack-agnostic layer inside the network to starve the
subsequent layers off of information.
• We create the first Spartan Network using the ideas above,
• We evaluate the performance-robustness tradeoff of Spartan
and test its robustness to black-box attackers.
Networks in a simple threat model.
Future Work. This work puts forward a lot of experiments and
we encourage our fellow researchers to explore on it. Our code will
be Open-Sourced to support this effort, and will be available at:
https://github.com/FMenet.
We also see that the possibilities for remplacement gradients are
endless: One could replace the gradient of a differentiable function
by another to improve the update dynamic while retaining a de-
sireable behaviour, like a gradient close to 1 to avoid exploding or
vanishing gradients.
We expect to use more powerful, deeper and wider architectures
to train on more complex datasets, but the current implementa-
tion of those networks is slow. We aim at producing a faster, more
efficient way to backpropagate efficiently with our replacement
gradients.
Image space topology allows for a smooth exploration of sample
space, as a small euclidian distance often means same semantics.
We leave the study of Spartan Networks on harder sample space
topologies for future work.
ACKNOWLEDGMENTS
We would like to thank all our colleagues from the SecSI lab, and
especially Ranwa Al-Mallah for useful comments and corrections.
REFERENCES
[1] [n. d.]. Fake News Detector AI | Detect Fake News Using Neural Networks.
http://www.fakenewsai.com/
[2] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, San-
jay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard,
Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg,
Dandelion Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike
Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul
Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals,
Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng.
2015. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems.
https://www.tensorflow.org/ Software available from tensorflow.org.
[3] Mahdieh Abbasi and Christian GagnÃľ. 2017. Robustness to Adversarial Examples
through an Ensemble of Specialists. arXiv:1702.06856 [cs] (Feb. 2017). http:
//arxiv.org/abs/1702.06856 arXiv: 1702.06856.
[4] N. Akhtar and A. Mian. 2018. Threat of Adversarial Attacks on Deep Learning in
Computer Vision: A Survey. IEEE Access 6 (2018), 14410–14430. https://doi.org/
10.1109/ACCESS.2018.2807385
[5] Anish Athalye, Nicholas Carlini, and David Wagner. 2018. Obfuscated gradients
give a false sense of security: Circumventing defenses to adversarial examples.
arXiv preprint arXiv:1802.00420 (2018).
[6] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat
Flepp, Prasoon Goyal, Lawrence D. Jackel, Mathew Monfort, Urs Muller, Jiakai
Zhang, Xin Zhang, Jake Zhao, and Karol Zieba. 2016. End to End Learning for
Self-Driving Cars. arXiv:1604.07316 [cs] (April 2016). arXiv: 1604.07316.
[7] Wieland Brendel and Matthias Bethge. 2017. Comment on "Biologically inspired
protection of deep networks from adversarial attacks". arXiv:1704.01547 [cs, q-bio,
stat] (April 2017). arXiv: 1704.01547.
[8] Jacob Buckman, Aurko Roy, Colin Raffel, and Ian Goodfellow. 2018. THERMOME-
TER ENCODING: ONE HOT WAY TO RESIST ADVERSARIAL EXAMPLES.
(2018), 22.
[9] Nicholas Carlini and David A. Wagner. 2017. Towards Evaluating the Robustness
of Neural Networks. 2017 IEEE Symposium on Security and Privacy (SP) (2017),
39–57.
[10] François Chollet et al. 2015. Keras. https://keras.io.
[11] Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua
Bengio. 2016. Binarized Neural Networks: Training Deep Neural Networks with
Weights and Activations Constrained to +1 or -1. arXiv:1602.02830 [cs] (Feb. 2016).
http://arxiv.org/abs/1602.02830 arXiv: 1602.02830.
[12] N. Das, M. Shanbhogue, S.-T. Chen, F. Hohman, L. Chen, M. E. Kounavis, and D. H.
Chau. 2017. Keeping the Bad Guys Out: Protecting and Vaccinating Deep Learning
with JPEG Compression. ArXiv e-prints (May 2017). arXiv:cs.CV/1705.02900
[13] H. Drucker and Y. Le Cun. 1991. Double backpropagation increasing generaliza-
tion performance. In IJCNN-91-Seattle International Joint Conference on Neural
Spartan Networks
arXiv Preprint, Dec 2018,
arXiv: 1511.04508.
[39] Aaditya Prakash, Nick Moran, Solomon Garber, Antonella DiLillo, and James A.
Storer. 2018. Deflecting Adversarial Attacks with Pixel Deflection. CoRR
abs/1801.08926 (2018).
[40] Andrew Slavin Ross and Finale Doshi-Velez. 2017. Improving the Adversarial
Robustness and Interpretability of Deep Neural Networks by Regularizing their
Input Gradients. arXiv:1711.09404 [cs] (Nov. 2017). http://arxiv.org/abs/1711.09404
arXiv: 1711.09404.
[41] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
Ian Goodfellow, and Rob Fergus. 2013. Intriguing properties of neural networks.
arXiv:1312.6199 [cs] (Dec. 2013). http://arxiv.org/abs/1312.6199 arXiv: 1312.6199.
[42] D. Tingle, Y.E. Kim, and D. Turnbull. 2010. Exploring automatic music annotation
with acoustically-objective tags. In Proceedings of the international conference on
Multimedia information retrieval. ACM, 55–62.
[43] Weilin Xu, David Evans, and Yanjun Qi. 2018. Feature Squeezing: Detecting
Adversarial Examples in Deep Neural Networks. arXiv:1704.01155 [cs] (2018).
https://doi.org/10.14722/ndss.2018.23198 arXiv: 1704.01155.
[44] Xiaojun Xu, Chang Liu, Qian Feng, Heng Yin, Le Song, and Dawn Song. 2017.
Neural Network-based Graph Embedding for Cross-Platform Binary Code Simi-
larity Detection. Proceedings of the 2017 ACM SIGSAC Conference on Computer
and Communications Security - CCS ’17 (2017), 363–376. https://doi.org/10.1145/
3133956.3134018 arXiv: 1708.06525.
Networks, Vol. ii. 145–150 vol.2. https://doi.org/10.1109/IJCNN.1991.155328
[14] Gamaleldin F. Elsayed, Shreya Shankar, Brian Cheung, Nicolas Papernot, Alex
Kurakin, Ian Goodfellow, and Jascha Sohl-Dickstein. 2018. Adversarial Examples
that Fool both Computer Vision and Time-Limited Humans. arXiv:1802.08195 [cs,
q-bio, stat] (Feb. 2018). http://arxiv.org/abs/1802.08195 arXiv: 1802.08195.
[15] Andre Esteva, Brett Kuprel, Roberto A Novoa, Justin Ko, Susan M Swetter, He-
len M Blau, and Sebastian Thrun. 2017. Dermatologist-level classification of skin
cancer with deep neural networks. Nature 542, 7639 (2017), 115.
[16] Reuben Feinman, Ryan R. Curtin, Saurabh Shintre, and Andrew B. Gardner. 2017.
Detecting adversarial samples from artifacts. arXiv preprint arXiv:1703.00410
(2017).
[17] Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S. Schoenholz, Maithra
Raghu, Martin Wattenberg, and Ian Goodfellow. 2018. Adversarial Spheres.
arXiv:1801.02774 [cs] (Jan. 2018).
arXiv:
1801.02774.
http://arxiv.org/abs/1801.02774
[18] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and
Harnessing Adversarial Examples. In arXiv:1412.6572 [cs, stat]. http://arxiv.org/
abs/1412.6572 arXiv: 1412.6572.
[19] Kathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes, and
Patrick McDaniel. 2017. On the (statistical) detection of adversarial examples.
arXiv preprint arXiv:1702.06280 (2017).
[20] Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes, and
Patrick McDaniel. 2016. Adversarial Perturbations Against Deep Neural Networks
for Malware Classification. Proceedings of the 2017 European Symposium on
Research in Computer Security (June 2016). http://arxiv.org/abs/1606.04435 arXiv:
1606.04435.
[21] Shixiang Gu and Luca Rigazio. 2014. Towards Deep Neural Network Architectures
Robust to Adversarial Examples. (Dec. 2014).
[22] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. 2017. BadNets: Identify-
ing Vulnerabilities in the Machine Learning Model Supply Chain. arXiv:1708.06733
[cs] (Aug. 2017). arXiv: 1708.06733.
[23] Chuan Guo, Mayank Rana, Moustapha Cisse, and Laurens van der Maaten. 2017.
Countering Adversarial Images using Input Transformations. arXiv:1711.00117
[cs] (Oct. 2017). http://arxiv.org/abs/1711.00117 arXiv: 1711.00117.
[24] Dan Hendrycks and Kevin Gimpel. 2016. Early methods for detecting adversarial
images. arXiv preprint arXiv:1608.00530 (2016).
[25] Weiwei Hu and Ying Tan. 2017. Black-Box Attacks against RNN based Malware
Detection Algorithms. arXiv:1705.08131 [cs] (May 2017). arXiv: 1705.08131.
[26] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. 1998. Gradient-based learning
applied to document recognition. Proc. IEEE 86, 11 (Nov. 1998), 2278–2324.
https://doi.org/10.1109/5.726791
[27] Xin Li and Fuxin Li. 2016. Adversarial examples detection in deep networks with
convolutional filter statistics. CoRR, abs/1612.07767 7 (2016).
[28] Jiajun Lu, Theerasit Issaranon, and David Forsyth. 2017. Safetynet: Detecting
and rejecting adversarial examples robustly. CoRR, abs/1704.00103 (2017).
[29] Yan Luo, Xavier Boix, Gemma Roig, Tomaso A. Poggio, and Qi Zhao.
2015. Foveation-based Mechanisms Alleviate Adversarial Examples. CoRR
abs/1511.06292 (2015).
[30] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu. 2017. Towards Deep Learning Models Resistant to Adversarial
Attacks. arXiv:1706.06083 [cs, stat] (June 2017). http://arxiv.org/abs/1706.06083
arXiv: 1706.06083.
[31] Dongyu Meng and Hao Chen. 2017. Magnet: a two-pronged defense against
adversarial examples. In Proceedings of the 2017 ACM SIGSAC Conference on
Computer and Communications Security. ACM, 135–147.
[32] Jan Hendrik Metzen, Tim Genewein, Volker Fischer, and Bastian Bischoff. 2017.
On detecting adversarial perturbations. arXiv preprint arXiv:1702.04267 (2017).
[33] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. 2015.
DeepFool: a simple and accurate method to fool deep neural networks.
arXiv:1511.04599 [cs] (Nov. 2015). arXiv: 1511.04599.
[34] A. Nayebi and S. Ganguli. 2017. Biologically inspired protection of deep networks
from adversarial attacks. ArXiv e-prints (March 2017). arXiv:stat.ML/1703.09202
[35] Nicolas Papernot, Fartash Faghri, Nicholas Carlini, Ian Goodfellow, Reuben Fein-
man, Alexey Kurakin, Cihang Xie, Yash Sharma, Tom Brown, Aurko Roy, Alexan-
der Matyasko, Vahid Behzadan, Karen Hambardzumyan, Zhishuai Zhang, Yi-Lin
Juang, Zhi Li, Ryan Sheatsley, Abhibhav Garg, Jonathan Uesato, Willi Gierke,
Yinpeng Dong, David Berthelot, Paul Hendricks, Jonas Rauber, and Rujun Long.
2018. Technical Report on the CleverHans v2.1.0 Adversarial Examples Library.
arXiv preprint arXiv:1610.00768 (2018).
[36] Nicolas Papernot and Patrick McDaniel. 2017. Extending Defensive Distillation.
arXiv:1705.05264 [cs, stat] (May 2017).
[37] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z. Berkay
Celik, and Ananthram Swami. 2016. Practical Black-Box Attacks against Machine
Learning. arXiv:1602.02697 [cs] (Feb. 2016). http://arxiv.org/abs/1602.02697 arXiv:
1602.02697.
[38] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami.
2015. Distillation as a Defense to Adversarial Perturbations against Deep Neural
Networks. arXiv:1511.04508 [cs, stat] (Nov. 2015). http://arxiv.org/abs/1511.04508