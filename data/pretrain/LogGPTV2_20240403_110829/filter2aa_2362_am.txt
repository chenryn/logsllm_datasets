Other scalar request fields that may contain aberrant values include,
but are not limited to, the Contact q value, the Timestamp value, and
the Via ttl parameter.
74
Quality Assurance and Testing
1IETF RFC 4475 “Session Initiation Protocol (SIP) Torture Test Messages.”
Most negative tests actually come in predefined test suites. The first such test
suites were released by the PROTOS research from the University of Oulu. PRO-
TOS researchers have provided free robustness testing suites for numerous proto-
cols since 1999, including tests for SIP released in 2002. One PROTOS test case
description in which the SIP method has been replaced with an increasing string of
“a” characters is shown below:
aaaaaaaaaaaaaaaaa sip: SIP/2.0
Via: SIP/2.0/UDP :;branch=z9hG4bK00003
From: 3 >;tag=3
To: Receiver >
Call-ID: @
CSeq:  INVITE
Contact: 3 >
Expires: 1200
Max-Forwards: 70
Content-Type: application/sdp
Content-Length: 
v=0
o=3 3 3 IN IP4 
s=Session SDP
c=IN IP4 
t=0 0
m=audio 9876 RTP/AVP 0
a=rtpmap:0 PCMU/8000
PROTOS uses a BNF-style grammar to model the entire communication proto-
col, and that can be seen in the generated test case descriptions as  elements
that represent changing values in the test cases. The test execution engine, or test
driver, will replace these fields with the dynamic values required during the execu-
tion of the test.
As you can see, the IETF approach is rather different from the PROTOS
approach. Instead of a limited coverage of tests for each test requirement, the PRO-
TOS SIP test suite contains more than 4,500 individual test cases that systemati-
cally add anomalies to different header elements of the protocol. Instead of one test
case per negative requirement, the test suite will execute a range of tests to try out
different unexpected values and exercise unusual corner cases. Test cases can be
configured with command-line options, and some dynamic functionality has been
implemented for protocol elements such as Content-Length, as shown above. PRO-
TOS tests were generated using a proprietary Mini-Simulation technology, which
basically can be thought of as a general-purpose fuzzing framework.2 In the IETF
torture test suite, the correct responses to error situations are defined, whereas
PROTOS ignores the responses and does not try to define the correct behavior
under corrupted or hostile situations. The approach of defining the responses to
3.2
Measuring Quality
75
2The PROTOS Mini-Simulation framework was later acquired by Codenomicon.
attacks limits the possible test coverage of torture tests and any other testing
approach based on test requirements and use cases. Most fuzzers behave the same
way as PROTOS suites did—i.e., the responses are rarely checked against any test
oracle.3 For those testers who have trouble thinking about using test cases in which
the response is unknown, it is important to note that the purpose of fuzzing is not
about verifying features, it is about finding crash-level defects.
3.2.2
Quality Is About Finding Defects
Quality assurance aims to reduce defects in software through two means. The first
way is by making it more difficult for people to introduce the defects in the first
place. The second, and more relevant means of defect reduction from the fuzzing
perspective, is using various methods of finding bugs. When integrating fuzzers into
quality assurance, you need to remember both these requirements.
Quality assurance should not only be about validating correctness. Sometimes
finding just one flaw is enough proof of the need for improvement, and whether it
takes fifty or five million tests to find it is irrelevant. If you find one flaw, you can
be sure that there are others. Bugs often appear in groups, and this is typical because
the same person (or team) tends to make similar mistakes in other places in their
code. A common process failure created by the traditional patch-and-penetrate race
is that when in a hurry, a person tends to focus all efforts on finding and fixing that
one specific flaw, when the same flaw could be apparent just 10 lines later. Even a
good programmer can make mistakes when in a hurry, or when having a bad day.
If a programmer does not pay attention to the entire module when fixing security
problems, he or she will most probably never have a chance to review that piece of
code again.
Quality assurance is hunting for bugs in software, by whatever means. This
should be the mental mode for testers: Testers are bug hunters. It is quite common
that in real-life software development, there might be no real bug hunters involved
in the testing process at all. The results of this type of destructive testing can be
annoying to some organizations that are more used to the positive thinking of “val-
idating and verifying” (V&V) functionality. Still, the ultimate purpose is not to
blame the designers and the programmers for the found flaws, but rather find and
remove as many problems as possible.
3.2.3
Quality Is a Feedback Loop to Development
Quality assurance is also used to validate the correctness of the development
process. For quality assurance people, the driving motivation is to be able to assist
developers in building better systems and potentially to improve the software devel-
opment process at the same time. A category of flaws that consistently appears and
is caught in the late phases of software development calls for a change in earlier
76
Quality Assurance and Testing
3A test oracle is the automated decision-making process that compares the received responses
against expected responses (input/output oracle) and makes the verdict if behavior was correct.
steps in the process. Security flaws are a good example of such a flaw category. If
buffer overflow vulnerabilities are consistently found in products ready to deploy,
the best solution is to radically improve the developer practices.
Note that many security flaws go by different names in different phases of the
software development process. During unit testing, a tester might presume that a
boundary value flaw is not critical and will label the bug as such. But the same
boundary value flaw is not critical and will not label the bug as such. Understand-
ing these links is critical, so that people use the same terminology and have the same
understanding of severity of bugs when discussing flaws.
3.2.4
Quality Brings Visibility to the Development Process
Quality assurance is a metric of the software development process. With good qual-
ity assurance processes, we are able to get visibility into the software development
process and the current status of the software. Integration of system units and soft-
ware modules is one measurement of the software process.
When a module is ready and tested, it can be labeled as completed. The soft-
ware industry is full of experiences in which the software has been 90% ready for
half of the development time. Security testing should also be an integral part of the
software development life cycle and not a delay at the end that adds to this miscon-
ception of “almost ready.” Knowing the place and time for security testing enables
product managers to understand the requirements of security testing from a time
(and money) perspective.
3.2.5
End Users’ Perspective
Quality assurance is a broad topic and we need to narrow it down to be able to
explain the selected parts in enough detail. Defining quality is a challenging task,
and different definitions apply to different categories of quality. For example, tests
that validate security properties can be very complex, and trust in their verdicts is
sometimes limited. The definition of quality depends on who is measuring it.
For many testers, the challenge is how to measure and explain the efficiency of
quality assurance so that the end customer will understand it. Quality assurance
needs to be measurable, but the customer of the quality assurance process has to be
able to define and validate the metrics used. In some cases, the customer has to also
be able to rerun and validate the actual tests.
Our purpose in this book is to look at quality from the security testing per-
spective, and also to look at quality assurance definitions mainly from the third-
party perspective. This, in most cases, means we are limited to black-box testing
approaches.
3.3
Testing for Quality
Testing does not equal quality assurance. The main goal of testing is to minimize
the number of flaws in released products. Testing is part of a typical quality assur-
ance process, but there are many other steps before we get to testing. Understanding
3.3
Testing for Quality
77
different quality assurance methods requires us to understand the different steps in
the software development life cycle (SDLC). There have been many attempts to
describe software development processes, such as the waterfall approach, iterative
development, and component-based development.
3.3.1
V-Model
V-model is not necessarily the most modern approach to describing a software
development process. In real life, software development rarely follows such a
straightforward process. For us, the V-model still offers an interesting view of the
testing side of things in the SDLC. Analyzing the software development from sim-
ple models is useful no matter what software development process is used. The
same functional methods and tools are used in all software development including
agile methods and spiral software development processes.
The traditional V-model is a very simplified graphical view of typical software
development practices. It maps the traditional waterfall development model into
various steps of testing. Note that we are not promoting the V-model over any other
software development model. You should not use the V-model in your real-life soft-
ware development without careful consideration. Let’s analyze the steps in typical
V-model system development, shown in Figure 3.1.
The phases on the left-hand side are very similar to the overly simplified school-
book waterfall model of software development. It goes through the different steps,
from gathering requirements to the various steps of design and finally to the pro-
gramming phase. To us, the goal of the V-model is to enforce natural system bound-
aries at various steps and to enforce test-driven development at different levels of
integration. The requirements step results in creation of the acceptance criteria used
in acceptance testing. The first set of specifications describes the system at a high
level and sets the functional criteria for system testing. Architectural design makes
decisions on high-level integration of components that will be used to test against
in integration testing. Finally, detailed design defines the most detailed testable units
and the test criteria of unit testing. The V-model does not consider the different
78
Quality Assurance and Testing
Figure 3.1
V-model for the system development life cycle.
purposes of testing; it only looks at the different levels of integration. There are
many specifics missing from the V-model when viewed from the security testing
perspective. Nevertheless, it is a good starting point when used in black-box testing
processes.
3.3.2
Testing on the Developer’s Desktop
Another set of quality assurance practices takes place even before we enter the test-
ing phase inside a typical waterfall model of software development. A majority of
bugs are caught in the programming phase. All the tools in the developer’s desktop
are tuned to catch human errors made during the programming and building
phases. When code is submitted to building, it typically goes through rigorous code
auditing. This can be either manual or automated. Also, manual programmers use
fuzzing as part of unit testing.
3.3.3
Testing the Design
Software inspections and peer reviews are static analysis approaches to assessing
various attributes in software development documentation and code. Verification
of the design phase requires a formal review and complex automation tools. The
formal methods employed can include mathematical proofs of encryption algo-
rithms and analyses of the message flows used. For example, when a new protocol
specification is being designed, the dynamic operation of the protocol message
exchange needs to be carefully analyzed from the security perspective.
State machines in complex interfaces can also act as a source of information for
black-box testing. Test automation can help in trying out a complex state machine
to ensure that there are no deadlocks or unhandled exceptional situations.4
3.4
Main Categories of Testing
Software quality assurance techniques such as testing can be based on either static
analysis or dynamic analysis. Static analysis is off-line analysis that is done to the
source code without any requirement to run the code. Dynamic analysis is a run-
time method that is performed while the software is executing. A good test process
can combine both of these approaches. For example, code optimization tools can
augment the code when the code is executed with information that can later be used
in a static analysis. There are many different ways that the methods of testing can
be partitioned.
3.4.1
Validation Testing Versus Defect Testing
Sommerville5 (2004) divides testing into validation testing and defect testing. The
purpose of validation testing is to show that the software functions according to
3.4
Main Categories of Testing
79
4An example of test automation framework that generates test cases from a state chart is the Con-
formiq Test Generator. www.conformiq.com/
5Ian Sommerville. Software Engineering, 8th ed. New York: Addison Wesley, 2006.
user requirements. On the other hand, defect testing intends to uncover flaws in the
software rather than simulate its operational use. Defect testing aims at finding
inconsistencies between the system and its specification.
3.4.2
Structural Versus Functional Testing
Another division of testing is based on access to the source code. These two cate-
gories are structural testing and functional testing.
Structural testing, or white-box testing, uses access to the source code to reveal
flaws in the software. Structural testing techniques can also be used to test the
object code. Structural testing can be based on either static or dynamic analysis, or
their combination. The focus is on covering the internals of the product in detail.
Various source code coverage techniques are used to analyze the depth of structural
testing. One example of white-box testing is called unit testing, which concentrates
on testing each of the functions as you see them in the code.
Functional testing, or black-box testing, tests the software through external
interfaces. Functional testing is always dynamic and is designed on the basis of var-
ious specification documents produced in different phases of the software develop-
ment process. A functional tester does not necessarily need to know the internals of
the software. Access to the code is unnecessary, although it can be helpful in design-
ing the tests.
Finally, gray-box testing is a combination of both the white-box and black-box
approaches, and it uses the internals of the software to assist in the design of the
tests of the external interfaces.
3.5
White-Box Testing
White-box testing has the benefit of having access to the code. In principle, the only
method of reaching 100% coverage (of some sort) in testing is with white-box tech-
niques. Different white-box testing techniques can be used to catch suspicious code
during the programming phase and also while the code is being executed. We will
next look at some relevant aspects of white-box testing techniques.
3.5.1
Making the Code Readable
A prerequisite for catching problems is to make the code more readable and thereby
easier to understand and debug. Good programming practices and coding conven-
tions can help in standardizing the code, and they will also help in implementing
various automated tools in the validation process. An example of such quality im-
provements is compile-time checks, which will detect use of insecure function calls
and structures.
3.5.2
Inspections and Reviews
Static analysis methods, such as various types of inspections and reviews, are widely
used, and they are critical to the development of good-quality software. Inspections
can focus on software development documents or the actual code. A requirement for
80
Quality Assurance and Testing
successful inspections and reviews is agreeing on a policy on how the code should be
implemented. Several industry standards from bodies like IEEE have defined guide-
lines on how and where inspections and reviews should be implemented.
3.5.3
Code Auditing
The simplest form of white-box testing is code auditing. Some people are more skilled
at noticing flaws in code, including security mistakes, than others. From the security
perspective, the most simple code auditing tools systematically search the code look-
ing for vulnerable functions, such as sprintf(), strcpy(), gets(), memcpy(), scanf(), sys-
tem(), and popen(), because they are often responsible for overflow problems. Such a
simplistic approach will necessarily reveal many false positives because these func-
tions can be used safely. More complex auditing tools analyze the entire program’s
structure, have models that represent common programming errors, and compare the
structure of the program to these models. Such tools will greatly reduce the number
of false positives as instead of just reporting the use of ‘strcpy,’ it analyzes whether the
input has been limited to it.
A code review can take place either off-line or during compilation. Some static
analysis tools check the compiled result of the code, analyzing weaknesses in the
assembly code generated during compilation of the software module. Compilers
themselves are also integrated with various quality-aware functionalities, issuing
warnings when something suspicious is seen in the code or in an intermediate repre-
sentation. As mentioned above, the most common problem encountered with code
auditing tools is the number of false-positive issues, which are security warnings that
do not pose a security risk. Another problem with all code-auditing practices is that
they can only find problems they are taught to find. For example, the exploitable secu-
rity flaw in the following code snippet from an X11 bitmap-handling routine might
easily be missed by even the most skilled code auditing people and tools:
01 / *Copyright 1987, 1998 The Open Group – Shortened for
presentation!
02
* Code to read bitmaps from disk files. Interprets
03
* data from X10 and X11 bitmap files and creates
04
* Pixmap representations of files.
05
* Modified for speedup by Jim Becker, changed image
06
* data parsing logic (removed some fscanf()s). Aug 5, 1988 */
07
08 int XReadBitmapFileData (_Xconst char *filename,
09
unsigned int *width,
/ *RETURNED */
10
unsigned int *height,
/ *RETURNED */
11
unsigned char **data,
/ *RETURNED */
12
int *x_hot,
/ *RETURNED */
13
int *y_hot)
/ *RETURNED */
14
15
unsigned char *bits = NULL;
/ *working variable */
16
int size;
/ *number of data bytes */
17
int padding;
/ *to handle alignment */
3.5
White-Box Testing