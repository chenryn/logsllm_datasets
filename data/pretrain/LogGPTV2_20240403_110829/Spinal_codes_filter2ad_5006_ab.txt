space of the spinal encoder gives rise to the second major difference:
the “constraint length” of this encoding goes all the way back to
the start of the message, because the state at the end of any stage
depends on all the input message bits in the message until that point.
The third key difference is that, whereas a convolutional encoder
has a constant ratio of the number of input to output bits (i.e., a
ﬁxed rate), the spinal code is rateless because one can generate
as many transmission symbols as desired using the RNG. h and
RNG together allow the spinal encoding to not only achieve good
separation between codewords, but also ratelessness.
3.2 Hash Function and RNG
We choose h uniformly based on a random seed from a pairwise
independent family of hash functions H [24]. This property guar-
antees that for two distinct hash inputs x and y, every pair of output
Figure 2: Uniform (left) and truncated Gaussian (right) con-
stellation mapping functions. Same average power; c = 6; trun-
cated Gaussian with β = 2.
values a and b is equally likely. This property is standard and attain-
able in practice. The encoder and decoder both know h, RNG, and
the initial value s0; if s0 is chosen pseudo-randomly, the resulting
symbol sequence is pseudo-random, providing resilience against
“bad” or adversarial input message sequences (one may view the use
of a pseudo-random s0 as analogous to a scrambler).
Because our requirements for RNG are similar to those for h,
one suitable choice for RNG is to combine h with a ν-to-c-bit shift
register.
3.3 Rateless Symbol Generation
The output of the encoder is delivered in a series of passes of
n/k symbols each, as depicted in Figure 1. The encoder produces
symbols xi,1 for the ﬁrst pass, where xi,1 is the output of a determin-
istic constellation mapping function acting on the ﬁrst c-bit number
generated by the ith RNG (seeded by si). It produces symbols xi,‘
for subsequent passes by generating additional outputs from each
of the random number generators. The encoder continues to loop
back and generate additional symbols until the receiver manages to
decode the message or the sender or receiver decides to give up on
the message.
Let b be a single c-bit input to the constellation mapping function.
For the BSC, the constellation mapping is trivial: c = 1, and the
sender transmits b. For the AWGN channel (with or without fading),
the encoder needs to generate I and Q under an average power
constraint. The constellation mapping function generates I and Q
independently from two separate RNG outputs of c bits each.
trols the truncation width. Very small corrections to P are omitted.
4. DECODING SPINAL CODES
In this section, we present an efﬁcient bubble decoder for spinal
codes. This is an approximate ML decoder whose time and space
complexity are polynomial in the number of bits in the message
being recovered. Later in this section and in Appendix 9, we show
that the polynomial-time approximation of ML decoding achieves
capacity over both the AWGN and BSC models. In §8, we show
experimentally that a linear-time bubble decoder achieves throughput
close to the Shannon limit, outperforming state-of-the-art rated codes
(LDPC) and recent rateless codes (Raptor and Strider).
We examine the two constellation mappings shown in Figure 2.
The ﬁrst is uniform, and the second produces a truncated Gaussian
via the standard normal CDF, Φ. In terms of the average power P,
√
Uniform: b → (u− 1/2)
Gaussian: b → Φ−1(γ + (1− 2γ)u)pP/2
where γ ≡ Φ(−β ) limits the Gaussian’s range to ±βpP/2. β con-
b + 1/2
u =
6P
2c
IQIQ4.1 The Problem
The central concept in ML spinal decoding is to search for the
encoded message that differs least from the received signal. Given a
vector of observations ¯y and an encoder function ¯x(M) that yields
the vector of transmitted symbols for a message M, the ML rule for
the AWGN channel is then
ˆM ∈ argmin
M0∈{0,1}n
k ¯y− ¯x(M0)k2.
(1)
That is, the receiver’s estimated message ˆM ∈ {0,1}n is the one that
produces an encoded vector ¯x( ˆM) closest to ¯y in ‘2 distance. For the
BSC, the only change is to replace the ‘2 Euclidean distance with
Hamming distance.
Because spinal codes are rateless, the lengths of vectors ¯x and ¯y
increase as the transmitter sends more symbols through the channel.
Suppose that the transmitter has sent N symbols up to the present.
The set of all transmitted words { ¯x(M0) for all M0 ∈ {0,1}n} forms
a dense cloud in N-dimensional space. Under Gaussian noise, nearby
points in the cloud are initially indistinguishable at the receiver. As
N increases, however, the average separation of the points increases.
Eventually, n/N (whose dimensions are bits/symbol) drops below
the Shannon capacity. The separation of the points then becomes
great enough that the correct message is the argmin, and decoding
terminates.
The brute-force approach to ML decoding is to conduct an exhaus-
tive search over all 2n possible messages. Thus, the key question is
whether it is possible to develop a practical and implementable spinal
decoder. Fortunately, the sequential structure of spinal codes and the
powerful mixing effect of h and RNG enable efﬁcient decoding, as
explained next.
4.2 Decoding over a Tree
Because the spinal encoder applies the hash function sequentially,
input messages with a common preﬁx will also have a common
spine preﬁx, and the symbols produced by the RNG from the shared
spine values will be identical. The key to exploiting this structure is
to decompose the total distance in (1) into a sum over spine values.
If we break ¯y into sub-vectors ¯y1, . . . , ¯yn/k containing symbols from
spine values si of the correct message, and we break ¯x(M0) for the
candidate message into n/k vectors ¯xi(s0
i), the cost function becomes:
k ¯yi − ¯xi(s0
k ¯y− ¯x(M0)k2 =
(2)
i)k2.
n/k
∑
i=1
A summand k ¯yi − ¯xi(si)k2 only needs to be computed once for all
messages that share the same spine value si. The following construc-
tion takes advantage of this property.
Ignoring hash function collisions (discussed in §8.5), decoding
can be recast as a search over a tree of message preﬁxes. The root of
this decoding tree is s0, and corresponds to the zero-length message.
Each node at depth d corresponds to a preﬁx of length kd bits, and
is labeled with the ﬁnal spine value sd of that preﬁx. Every node
has 2k children, connected by edges e = (sd,sd+1) representing a
choice of k message bits ¯me. As in the encoder, sd+1 is h(sd, ¯me).
By walking back up the tree to the root and reading k bits from each
edge, we can ﬁnd the message preﬁx for a given node.
To the edge incident on node sd, we assign a branch cost k ¯yd −
¯xd(sd)k2. Summing branch costs on the path from the root to a
node gives the path cost of that node, equivalent to the sum in (2).
The ML decoder ﬁnds the leaf with the lowest cost, and returns the
corresponding complete message.
The sender continues to send successive passes until the receiver
determines that the message has been decoded correctly. The re-
ceiver stores all the symbols it receives until the message is decoded
correctly. For instance, if six symbols have been received so far for
each spine value, then the vectors ¯xd in the cost computation above
have six components.
4.3 Bubble Decoding: Pruning the Tree
Suppose that M and M0 differ only in the ith bit. Comparing ¯x(M)
with ¯x(M0), we ﬁnd that symbols from spine values bi/kc, . . . ,n/k in
the two transmissions are completely dissimilar. If M is the correct
decoding, then M0 will have a larger path cost than M. The gap
will be largest when i is small. This observation suggests a helpful
intuition, which can be derived more formally from the proof of the
theorem in the appendix: alternate codewords with comparable path
costs to the ML decoding differ only in the last O(logn) bits.
Building on this idea, suppose that we have constructed the entire
ML decoding tree and computed the path costs for all of the leaves.
If we pick the best 100 leaves and trace them back through the
tree, we expect to ﬁnd that within a few times logn steps, they all
converge to a small number of common ancestors.
Consequently, our proposed bubble decoder accepts two param-
eters: the depth d, and the beam width B. Instead of searching the
entire decoding tree, we maintain B common ancestors, termed the
beam, and a partial decoding tree rooted on each ancestor. The
pseudo-code for the bubble decoder is:
Let T0 be the tree of nodes out to depth d from root.
beam ← {T0} # set of rooted trees
for i = 1 to n/k− d do
candidates ← bd ce # list of (tree,cost) tuples
for T ∈ beam do
for T0 ∈ subtrees(root(T )) do
Expand T0 from depth d − 1 to depth d.
Compute and store path_cost in expanded nodes.
cost ← min{path_cost(x)|x ∈ leaves(T0)}
candidates.append((T0, cost))
# get B lowest cost candidates, breaking ties arbitrarily
beam ← best(candidates,B)
return best(candidates,1)
These steps are depicted in Figure 3.
When d = 1, this method is the classical beam search (from AI),
also termed the M-algorithm in communication systems [1]. When
d = n/k− logk B, we recover the full ML decoder without any tree
pruning.
4.4 Tail Symbols
The bubble decoder ends up with a list of B messages, from which
it should produce the best one. One approach is to reconstruct and
validate all B messages, but the cost of doing so may be too high
when B is large. An alternative is to send multiple symbols from
sn/k in each pass (tail symbols), enough that if the correct candidate
is in the beam, it has the lowest path cost. Then, only the lowest path
cost message needs to be validated. We ﬁnd that adding even just
one tail symbol works well.
4.5 Decoding Time and Space Complexity
A single decoding attempt requires n/k − d steps. Each step
explores B2kd nodes at a cost of L RNG evaluations each, where L
is the number of passes. Each step selects the best B candidates in
O(B2k) comparisons using the selection algorithm. The overall cost
is O( n
k BL2kd) hashes and O( n
k B2k) comparisons.
Storage requirements are O(B2kd(k + ν)) for the beam and the
partial trees, plus O( n
k B(k + logB)) for message preﬁxes.
If B is polynomial in n, or if B is constant and d = O(logn),
the total number of states maintained and the time complexity of
operations remains polynomial in n. If both B and d are constant, the
complexity of the bubble decoder is linear in n. Our experimental
Figure 3: Sequential decoding process using the bubble decoder with B = 2, d = 2, k = 1. (a) At the beginning of step i, the partial
trees have depth d − 1. (b) Grow them out to depth d. (c) Propagate the smallest path costs back through the tree. (d) Select the B
best children, pruning the rest. Advance to the next step and repeat.
results are for such linear-time conﬁgurations, with B maximized
subject to a compute budget (§8.5).
In comparison, LDPC and Raptor decoders use several iterations
of belief propagation (a global operation involving the entire mes-
sage). Turbo decoders also require many full runs of the BCJR [2] or
Viterbi algorithm [40]. All told, LDPC, Raptor, and turbo decoders
perform several tens to thousands of operations per bit.
A spinal decoder with an appropriate choice of parameters per-
forms a comparable number of operations per bit to these codes,
achieves competitive throughput (§8), and is parallelizable (§7.2).
The spinal decoder has the additional advantage that the decoder
can run as symbols arrive because it operates sequentially over the
received data, potentially reducing decode latency.
4.6 Capacity Results
For the AWGN channel with the uniform constellation mapping,
we establish that a polynomial-time decoder achieves rates within a
small constant (≈ 0.25 bits/symbol) of capacity. The proof appears
in the appendix. A recent companion paper [3] states and establishes
capacity results for the AWGN channel with the Gaussian constella-
tion, and for the BSC: the spinal decoder achieves capacity under
these settings.
THEOREM 1
(AWGN CHANNEL PERFORMANCE). Let
Cawgn(SNR) be the AWGN channel capacity per channel use.
With the uniform constellation, a bubble decoder polynomial in n
achieves BER → 0 as n → ∞ for any number of passes L such that
h
i
Cawgn(SNR)− δ
L
> k,
with the degree of
(Cawgn(SNR)− δ − k/L) and
the polynomial
inversely proportional
to
δ ≡ δ (c,P∗, SNR) ≈ 3(1 + SNR)2−c +
1
2
log
.
(3)
(cid:16) πe
(cid:17)
6
This result suggests that with the uniform constellation mapping,
by selecting a large enough c = Ω(log(1 + SNR)), it is possible
2 log(πe/6) ≈ 0.25 of Cawgn(SNR). As
to achieve rates within 1
mentioned above, it is possible to close this theoretical gap with an
appropriately chosen Gaussian constellation mapping. In simula-
tion with ﬁnite n, however, we do not see signiﬁcant performance
differences between the Gaussian and uniform mappings.
Figure 4: Puncturing schedule. In each subpass, the sender
transmits symbols for spine values marked by dark circles;
shaded circles represent spine values that have already been
sent in a previous subpass.
5. PUNCTURING
In §3, the sender transmits one symbol per spine value per pass.
If it takes ‘ passes to decode the message, the rate achieved is k/‘
bits per symbol, with a maximum of k. Moreover, at moderate SNR,
when ‘ is a small integer, quantization introduces plateaus in the
throughput. Because the decoding cost is exponential in k, we cannot
simply increase k to overcome these disadvantages.
Spinal codes may be punctured to achieve both high and ﬁner-
grained rates, without increasing the cost of decoding. Rather than
sending one symbol per spine value per pass, the sender skips some
spine values and, if required, ﬁlls them in subsequent “subpasses”
before starting the next pass.
Figure 4 shows transmission schedule we implemented (others
are possible). Each pass is divided into eight subpasses (rows in
the ﬁgure). Within a subpass, only the spine values corresponding
to dark circles are transmitted. Decoding may terminate after any
subpass, producing a ﬁne-grained set of achievable rates. This
schedule nominally permits rates as high as 8k bits per symbol.
Puncturing does not change the decoder algorithm. For any miss-