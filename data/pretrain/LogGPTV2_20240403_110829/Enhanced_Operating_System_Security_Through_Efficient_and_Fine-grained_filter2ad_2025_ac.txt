structions to avoid giving rise to any new useful gadget.
It is easy to show that a sequence of nop instructions does
not yield any useful gadget on the x86 [54], but other
strategies may be necessary on other architectures.
Static data randomization. The data-transformation
pass randomly permutes all the static variables and read-
only data on the symbol table, as done before for func-
tions. We also employ the same padding strategy, except
random-sized dummy variables are used for the padding.
Buffer variables are also separated from other variables
to limit the power of buffer overﬂows. In addition, unlike
existing ASR solutions, we randomize the internal layout
of static data, when possible.
All the aggregate types in the C programming lan-
guage are potential candidates for layout randomization.
In practice, there are a number of restrictions. First, the
order of the elements in an array cannot be easily ran-
domized without changing large portions of the code and
resorting to complex program analysis techniques that
would still fail in the general case. Even when possi-
ble, the transformation would require indirection tables
that translate many sequential accesses into random array
accesses, sensibly changing the run-time cache behav-
ior and introducing overhead. Second, unions are cur-
rently not supported natively by LLVM and randomizing
their layout would introduce unnecessary complications,
given their rare occurrence in critical system data struc-
tures and their inherent ambiguity that already weakens
the assumptions made by an attacker. Finally, packed
structs cannot be randomized, since the code makes
explicit assumptions on their internal layout.
In light of these observations, our transformation fo-
cuses on randomizing the layout of regular struct
types, which are pervasively used in critical system data
structures. The layout randomization permutes the order
of the struct members and adds random-sized padding
between them. To support all the low-level programming
idioms allowed by C, the type transformations are oper-
ated uniformly for all the static and dynamic objects of
the same struct type. To deal with code which treats
nonpacked structs as implicit unions through pointer
casting, our transformation pass can be instructed to de-
tect unsafe pointer accesses and refrain from randomiz-
ing the corresponding struct types.
Layout randomization of system data structures is im-
portant for two reasons. First, it makes the relative dis-
tance/alignment between two struct members unpre-
dictable. For example, an overﬂow in a buffer allocated
inside a struct cannot make precise assumptions about
which other members will be corrupted by the overﬂow.
Second, this strategy is crucial to limit the assumptions
of an attacker in face of information leakage. Suppose an
attacker is armed with a reliable arbitrary kernel mem-
ory write generated by a missing pointer check. If the
attacker acquires knowledge on the location of the data
structure holding user credentials (e.g., struct cred on
Linux) for an attacker-controlled unprivileged process,
the offset of the uid member is normally sufﬁcient to
surgically override the user ID and escalate privileges.
All the existing ASR solutions fail to thwart this attack.
In contrast, our layout randomization hinders any precise
assumptions on the ﬁnal location of the uid. While brute
forcing is still possible, this strategy will likely compro-
mise other data structures and trigger a system crash.
Stack randomization. The stack randomization pass
performs two primary tasks. First, it randomizes the base
address of the stack to make the absolute location of any
stack object unpredictable.
In LLVM, this can be ac-
complished by creating a dummy alloca instruction—
which allocates memory on the stack frame of the cur-
rently executing function—at the beginning of the pro-
gram, which is later expanded by the code generator.
This strategy provides a portable and efﬁcient mecha-
nism to introduce random-sized padding for the initial
stack placement. Second, the pass randomizes the rel-
ative distance/alignment between any two objects allo-
cated on the stack. Prior ASR solutions have either ig-
nored this issue [39, 72] or relied on a shadow stack and
dynamically generated random padding [14], which in-
troduces high run-time overhead (10% in the worst case
in their experiments for user applications).
To overcome these limitations, our approach is com-
pletely static, resulting in good performance and code
which is statically veriﬁed by LLVM. In addition, this
strategy makes it realistic to use cryptographically ran-
dom number generators (e.g., /dev/random) instead
of pseudo-random generators to generate the padding.
While care should be taken not to exhaust the random-
ness pool used by other user programs, this approach
yields much stronger security guarantees than pseudo-
random generators, like recent attacks on ASR demon-
strate [24]. Our transformations can be conﬁgured to use
cryptographically random number generators for code,
data, and stack instrumentation, while, similar to prior
approaches [14], we always resort to pseudo-random
generation in the other cases for efﬁciency reasons.
When adopting a static stack padding strategy, great
care should be taken not to degrade the quality of the
randomization and the resulting security guarantees. To
randomize the relative distances between the objects in
a stack frame, we permute all the alloca instructions
used to allocate local variables (and function parame-
ters). The layout of every stack-allocated struct is also
randomized as described earlier. Nonbuffer variables are
all grouped and pushed to the top of the frame, close
Figure 2: The transformed stack layout.
to the base pointer and the return address. Buffer vari-
ables, in turn, are pushed to the bottom, with random-
sized padding (i.e., dummy alloca instructions) added
before and between them. This strategy matches our re-
quirements while allowing the code generator to emit a
maximally efﬁcient function prologue.
To randomize the relative alignment between any two
stack frame allocations of the same function (and thus
the relative alignment between their objects), we create
random-sized padding before every function call. Albeit
static, this strategy faithfully emulates dynamically gen-
erated padding, given the level of unpredictability intro-
duced across different function calls. Function calls in-
side loops are an exception and need to be handled sepa-
rately. Loop unrolling is a possible solution, but enforc-
ing this optimization in the general case may be expen-
sive. Our approach is instead to precompute N random
numbers for each loop, and cycle through them before
each function call. Figure 2 shows the randomized stack
layout generated by our transformation.
Dynamic data randomization. Our operating sys-
tem provides malloc/mmap-like abstractions to every
OS process. Ideally, we would like to create memory al-
location wrappers to accomplish the following tasks for
both heap and memory-mapped regions: (i) add random-
sized padding before the ﬁrst allocated object; (ii) add
random-sized padding between objects; (iii) permute the
order of the objects. For memory-mapped regions, all
these strategies are possible and can be implemented ef-
ﬁciently [39]. We simply need to intercept all the new
allocations and randomly place them in any available lo-
cation in the address space. The only restriction is for
ﬁxed OS component-speciﬁc virtual memory mappings,
which cannot be randomized and need to be explicitly
reserved at initialization time.
For heap allocations, we instrument the code to ran-
domize the heap base address and introduce random-
sized padding at allocation time. Permuting heap ob-
jects, however, is normally impractical in standard allo-
cation schemes. While other schemes are possible—for
example, the slab allocator in our memory manager ran-
domizes block allocations within a slab page—state-of-
Stack frameParametersPrevious frameSaved base pointerReturn addressLocal variablesNew stack frameInter-frame paddingPrevious frameReturn addressParametersSaved base pointerNonbuffer variablesIntra-frame paddingBuffer variablesthe-art allocators that enforce a fully and globally ran-
domized heap organization incur high overhead (117%
worst-case performance penalty) [53]. This limitation is
particularly unfortunate for kernel Heap Feng Shui at-
tacks [25], which aim to carefully drive the allocator into
a deterministic exploitation-friendly state. While random
interobject padding makes these attacks more difﬁcult, it
is possible for an attacker to rely on more aggressive ex-
ploitation strategies (i.e., heap spraying [59]) in this con-
text. Suppose an attacker can drive the allocator into a
state with a very large unallocated gap followed by only
two allocated buffers, with the latter vulnerable to under-
ﬂow. Despite the padding, the attacker can induce a large
underﬂow to override all the traversed memory locations
with the same target value. Unlike stack-based over-
ﬂows, this strategy could lead to successful exploitation
without the attacker worrying about corrupting other crit-
ical data structures and crashing the system. Unlike prior
ASR solutions, however, our design can mitigate these
attacks by periodically rerandomizing every OS process
and enforcing a new unpredictable heap permutation. We
also randomize (and rerandomize) the layout of all the
dynamically allocated structs, as discussed earlier.
Kernel modules randomization. Traditional load-
able kernel module designs share many similarities—
and drawbacks,
from a security standpoint—with
application-level shared libraries. The attack presented
in [61] shows that the data structures used for dynamic
linking are a major source of information leakage and
can be easily exploited to bypass any form of random-
ization for shared libraries. Prior work on ASR [67, 14]
discusses the difﬁculties of reconciling sharing with ﬁne-
grained randomization. Unfortunately, the inability to
perform ﬁne-grained randomization on shared libraries
opens up opportunities for attacks, including probing,
brute forcing [67], and partial pointer overwrites [23].
To overcome these limitations, our design allows only
statically linked libraries for OS components and inhibits
any form of dynamic linking inside the operating sys-
tem. Note that this requirement does by no means limit
the use of loadable modules, which our design simply
isolates in independent OS processes following the same
distribution and deployment model of the core operating
system. This approach enables sharing and lazy load-
ing/unloading of individual modules with no restriction,
while allowing our rerandomization strategy to random-
ize (and rerandomize) every module in a ﬁne-grained
manner. In addition, the process-based isolation prevents
direct control-ﬂow and data-ﬂow transfer between a par-
ticular module and the rest of the OS (i.e., the access is
always IPC- or capability-mediated). Finally, this strat-
egy can be used to limit the power of untrusted loadable
kernel modules, an idea also explored in prior work on
commodity operating systems [16].
6 Live rerandomization
Our live rerandomization design is based on novel auto-
mated run-time migration of the execution state between
two OS process variants. The variants share the same op-
erational semantics but have arbitrarily different memory
layouts. To migrate the state from one variant to the other
at runtime, we need a way to remap all the corresponding
global state objects. Our approach is to transform the bit-
code with another LLVM link-time pass, which embeds
metadata information into the binary and makes run-time
state introspection and automated migration possible.
Metadata transformation. The goal of our pass is to
record metadata describing all the static state objects in
the program and instrument the code to create metadata
for dynamic state objects at runtime. Access to these ob-
jects at the bitcode level is granted by the LLVM API. In
particular, the pass creates static metadata nodes for all
the static variables, read-only data, and functions whose
address is taken. Each metadata node contains three key
pieces of information: node ID, relocation information,
and type. The node ID provides a layout-independent
mechanism to map corresponding metadata nodes across
different variants. This is necessary because we random-
ize the order and the location of the metadata nodes (and
write-protect them) to hinder new opportunities for at-
tacks. The relocation information, in turn, is used by our
run-time migration component to locate every state ob-
ject in a particular variant correctly. Finally, the type is
used to introspect any given state object and migrate the
contained elements (e.g., pointers) correctly at runtime.
To create a metadata node for every dynamic state ob-
ject, our pass instruments all the memory allocation and
deallocation function calls. The node is stored before the
allocated data, with canaries to protect the in-band meta-
data against buffer overﬂows. All the dynamic metadata
nodes are stored in a singly-linked list, with each node
containing relocation information, allocation ﬂags, and a
pointer to an allocation descriptor. Allocation ﬂags de-
ﬁne the nature of a particular allocation (e.g., heap) to
reallocate memory in the new variant correctly at migra-
tion time. The allocation descriptors, in turn, are stat-
ically created by the pass for all the allocation sites in
the program. A descriptor contains a site ID and a type.
Similar to the node ID, the site ID provides a layout-
independent mechanism to map corresponding allocation
descriptors (also randomized and write-protected) across
different variants. The type, in contrast, is determined
via static analysis and used to correctly identify the run-
time type of the allocated object (e.g., a char type with
an allocation of 7 bytes results in a [7 x char] run-
time type). Our static analysis can automatically identify
the type for all the standard memory allocators and cus-
tom allocators that use simple allocation wrappers. More
advanced custom allocation schemes, e.g., region-based
memory allocators [11], require instructing the pass to
locate the proper allocation wrappers correctly.
The rerandomization process. Our OS processes
follow a typical event-driven model based on message
passing. At startup, each process initializes its state and
immediately jumps to the top of a long-running event-
processing loop, waiting for IPC messages to serve. Each
message can be processed in cooperation with other OS
processes or the microkernel. The message dispatcher,
isolated in a static library linked to every OS process,
can transparently intercept two special system messages
sent by the randomization manager (RM): sync and init.
These messages cannot be spoofed by other processes
because the IPC is mediated by the microkernel.
The rerandomization process starts with RM loading
a new variant in memory, in cooperation with the mi-
crokernel. Subsequently, it sends a sync message to the
designated OS process, which causes the current variant
to immediately block in a well-deﬁned execution point.
A carefully selected synchronization point (e.g., in main)
eliminates the need to instrument transient stack regions
to migrate additional state, thus reducing the run-time
overhead and simplifying the rerandomization strategy.
The new variant is then allowed to run and delivered an
init message with detailed instructions. The purpose of