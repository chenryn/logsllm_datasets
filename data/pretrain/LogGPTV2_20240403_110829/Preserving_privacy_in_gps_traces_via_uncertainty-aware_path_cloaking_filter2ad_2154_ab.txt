x 104
Figure 4: Fitting distance errors in tracking using an exponen-
tial function
3.2 Privacy Metric and Adversary Model
We observe that the degree of privacy risk strongly depends on
how long an adversary can follow a vehicle. To constitute a privacy
breach a trace must contain a privacy sensitive event (e.g., visited a
sensitive destination) and the adversary must be able to identify the
driver generating this trace. Both the probability that sensitive in-
formation is included and the probability of identiﬁcation increase
with longer traces. Identiﬁcation may be possible, for example, if
the vehicle returns to a known home or work location of a speciﬁc
individual.
Since consecutive location samples from a vehicle exhibit tem-
poral and spatial correlation, paths of individual vehicles can be re-
constructed from a mix of anonymous samples belonging to several
vehicles. This process can be formalized and automated through
target tracking algorithms [26]. These algorithms generally predict
the target position using the last known speed and heading informa-
tion and then decide which next sample to link to the same vehicle
through Maximum Likelihood Detection [44]. If multiple candi-
date samples exist, the algorithm chooses the one with the highest
a posteriori probability based on a probability model of distance
and time deviations from the prediction (in our evaluation, we as-
sume a strong adversary with a good model of these deviations).
If several of these samples appear similarly likely, no decision with
high certainty is possible and tracking stops.
Privacy Metrics. In consideration of this adversary model, we
measure the degree of privacy as the Mean Time To Confusion
(MTTC), the time that an adversary could correctly follow a trace.
Note that this includes time while a user remains stationary unless
otherwise speciﬁed. More speciﬁcally, the time to confusion is
the tracking time between two points where the adversary reached
confusion (i.e., could not determine the next sample with sufﬁcient
certainty). Inspired by the use of entropy in anonymous commu-
nication systems [40, 16], we use information theoretic metrics to
measure uncertainty or confusion in tracking.
For any point on the trace, Tracking Uncertainty is deﬁned as
H = − (cid:2)
pi log pi, where pi denotes the probability that location
sample i belongs to the vehicle currently tracked. Lower values of
H indicate more certainty or lower privacy. Given no other infor-
mation than the set of location samples, intuitively the probability
for a sample reported at time t is high, if the sample lies close to
the predicted position of the vehicle at time t and if no other sam-
ples at the same time are close to the vehicle. As one step further,
we can also express tracking conﬁdence C on adversary’s trial by
calculating (1 − H).
Empirically, we found that distances of the correct sample to
the predicted position appear monotonically decreasing in ﬁgure 4.
Therefore, we compute the probability pi for a given location sam-
ple by ﬁrst evaluating the exponential function
− di
µ
ˆpi = e
for every candidate sample and then normalizing all ˆpi to obtain
pi. The parameter μ can be interpreted as a distance difference that
can be considered very signiﬁcant. We obtain the value of μ from
empirical pdf of distance deviation in ﬁgure 4 which we ﬁt with
exponential function using unconstrained nonlinear minimization
(μ is 2094 meters).
The following algorithm is not dependent on the use of an ex-
ponential function for estimating the probability that a location
sample belongs to the same trace. It does assume, however, that
a publicly-known ’best’ tracking model exists and that the adver-
sary does not have any better tracking capabilities. In this paper,
we have empirically derived this probability model by ﬁtting an
exponential function.
Overall, the mean time to confusion can then be deﬁned as the
mean tracking time during which uncertainty stays below a confu-
sion threshold. If the uncertainty threshold is chosen high, tracking
times increase but so also does the number of false positives (fol-
lowing incorrect traces). Since the adversary cannot easily distin-
guish correct tracks and false positives, we assume that high uncer-
tainty thresholds will be used.
4. PATH PRIVACY-PRESERVING MECHA-
NISM
In this section, we present a method for preserving privacy in
GPS traces that can guarantee a level of privacy even for users
driving in low-density areas. Given a maximum allowable time
to confusion and an associated uncertainty threshold, the algorithm
can process a stream of received position samples to maintain the
tracking time bounds.
Since the algorithm must be aware of the positions of other ve-
hicles, we ﬁrst develop a centralized solution and then discuss how
reliance on a trustworthy privacy server may be relaxed. We ﬁrst
consider the stepwise tracking model without the possibility of path
reacquisition.
We observe that a speciﬁed maximum time to confusion (for a
given uncertainty level) can be guaranteed if the algorithm only
reveals location samples when (i) time since the last point of con-
fusion is less than the maximum speciﬁed time to confusion or (ii)
at the current time tracking uncertainty is above the threshold.
Algorithm 1 shows how this idea can be implemented. Note that
it describes processing of data from a single time interval, it would
be repeated for each subsequent time slot with the state in the ve-
hicle objects maintained. It takes as input the set of GPS samples
reported at time t (v.currentGPSSample updated for each vehicle),
the maximum time to confusion (confusionTimeout), and the asso-
ciated uncertainty threshold (confusionLevel). Its output is a set of
GPS samples that can be published while maintaining the speciﬁed
privacy guarantees.
The algorithm proceeds as follows. It ﬁrst identiﬁes the vehicles
that can be safely revealed because less time than confusionTime-
out has passed since the last point of confusion (line 12f.) Second,
it identiﬁes a set of vehicles that can be revealed because current
tracking uncertainty is higher than speciﬁed in confusionLevel (line
15-30). Finally, it updates the time of the last confusion point and
the last visible GPS sample for each vehicle (line 32ff., the latter
is needed for path prediction in the uncertainty calculation). This
step can only be performed when the set of revealed GPS samples
had been decided, since confusion should only be calculated over
the revealed samples.
The second step relies on several approximations. To reduce
computational complexity it calculates tracking uncertainty only
with the k closest samples to the prediction point, rather than with
all samples reported at time t. This is a conservative approxima-
tion, since uncertainty would increase if additional samples are
taken into account (see proof in appendix A). Further, it builds
a set of releaseCandidates since uncertainty should only be calcu-
privacy guarantee.
v.lastConfusionTime = t
v.predictedPos = v.lastVisible.position +
(t-v.lastVisible.time)*v.LastVisible.speed
Algorithm 1 Uncertainty-aware privacy algorithm
1: // Determines which location samples can be release while maintaining
2: releaseSet = releaseCandidates = {}
3: for all vehicles v do
4:
if start of trip then
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
// consider release of others dependent on uncertainty
v.dependencies = k vehicles closest to the predictedPos
if uncertainty(v.predictedPos, v.dependencies) > confusionLevel
then
// release all vehicles below time to confusion threshold
if t - v.LastConfusionTime = confusionLevel then
lated with released samples, but the set of released samples is not
determined yet. The algorithm subsequently prunes the candidate
set until only vehicles remain who meet the uncertainty threshold.
The key property to achieve after the pruning step is that ∀ v ∈
releaseCandidates. uncertainty(v.predictedPos, k closest neighbors
in releaseSet ∪ releaseCandidates) ≥ confusionLevel. The algo-
rithm uses the approximation of calculating the k closest neighbors
before the pruning phase, and ensuring during pruning that only
vehicles remain if all k neighbors are in the set. While this approx-
imation could be improved in order to release more samples, the
current version is sufﬁcient to maintain the privacy guarantee.
4.1 Algorithm Extensions for the Reacquisi-
tion Tracking Model
The algorithm described so far does not provide adequate pri-
vacy guarantees under the reacquisition tracking model because it
only ensures a single point of confusion after the maximum time to
confusion has expired. Recall that under the reacquisition model an
adversary skips samples with high confusion under certain condi-
tions and thus may be able to reacquire the correct trace even after
a point of confusion.
We observe that such reacquisitions are only possible over short
time-scales, since movements after more than several minutes be-
come too unpredictable. To verify this assumption, ﬁgure 5 shows
n
o
i
t
c
n
u
f
n
o
i
t
u
b
i
r
t
s
d
e
v
i
t
i
l
a
u
m
u
C
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
Empirical CDF
Original gap time
Reacquisition time
2
3
4
5
6
7
8
9
10
11
Time [min]
Figure 5: Cumulative distribution function of reacquisitions
the longest reacquisition and distribution of reacquisition length in
minutes, empirically obtained from our dataset. As expected, no
reacquisitions occur over gaps longer than 10 minutes. Thus, the
following extensions can prevent reacquisitions within a time win-
dow w. For the experiments reported in the following section we
set w = 10.
• After the confusionTimeout expires: In addition to main-
taining confusion from the last released position, it is calcu-
lated from every prior released location sample (of the same
vehicle) within the last w minutes. Samples can only be re-
leased if all these confusion values are above the confusion
threshold.
• Before the confusionTimeout expires: Every released sam-
ple must maintain confusion to any samples which are re-
leased during the last w minutes and before the confusion-
Timeout was last reset.
5. EXPERIMENTAL EVALUATION
In this section, we present the experimental evaluation of the pro-
posed privacy preserving techniques. Speciﬁcally, we demonstrate:
(1) the effectiveness of our proposed techniques for privacy protec-
tion in the analysis of GPS traces; (2) how our proposed privacy
preserving techniques can maintain the quality-of-service for the
trafﬁc monitoring application.
5.1 Experimental Setup
Experimental Data Sets. In our experiments, we used trace-
driven simulations for capturing real vehicle movements, density,
GPS inaccuracies, and road network artifacts. In the experiments,
we ﬁrst applied privacy preserving techniques on the GPS traces
and then tested the performance of privacy protection using target
tracking techniques on these privacy-preserved GPS traces.
Since target tracking typically is only effective for a short time
period, we conducted the targeting tracking experiments on 24-
hour GPS traces in two different user density scenarios: 500 probe
vehicles and 2000 probe vehicles on a 70km2 region. To create a
high density scenario, we overlay GPS Traces of different volunteer
drivers at the same time frame (24 hours) of different dates. A lim-
itation of this overlay method is that it generates similar routes by
aggregating GPS traces from the same set of drivers. Still, we be-
lieve that it provides insights into higher density deployments (we
will revisit this limitation in the discussion section.)
Evaluation Metrics. In our experiments, we applied the follow-
ing two metrics to evaluate our privacy preserving algorithms for
GPS traces.
Tracking Time. Minimizing tracking time reduces the risk that
an adversary can correlate an identity with sensitive locations. We
use time to confusion (TTC), which we deﬁned in section 3 as a
privacy metric, to measure the tracking duration. To better demon-
strate the bounded privacy protection of our proposed algorithm,
we report two statistics: the maximum value of TTC and the me-
dian value of TTC.
(Relative) Weighted Road Coverage. This metric provides an in-
dication of data quality for the trafﬁc monitoring applications. In-
deed, there is a tradeoff between privacy protection and the ﬂexible
use of data. In our case, a privacy preserving algorithm must pro-
vide reasonable privacy protection while delivering the same road
coverage for satisfying the need of the trafﬁc monitoring applica-
tions. In this paper, we use relative road coverage as we deﬁned in
section 2. In addition to this metric, we also provide the percentage
of released location sample compared to the original traces which
we consider 100%. Note that both metrics are normalized by values
of the original GPS traces.
x 106
4.74
4.73
4.72
4.71
4.7
4.69
4.68
4.67
2.8
2.9
3
3.1
3.2
3.3
3.4
3.5
x 105
(a) Snapshot of privacy-preserving GPS traces generated by
uncertainty-aware path cloaking at off-peak time (over 1.5
hours) in a high density scenario
x 106
4.74
4.73
4.72
4.71
4.7
4.69
4.68
4.67
2.8
2.9
3
3.1
3.2
3.3
3.4
3.5
x 105
(b) Snapshot of privacy-preserving GPS traces generated by
uncertainty-aware path cloaking algorithm at peak time (over
1.5 hour) in a high density scenario
Figure 6: Uncertainty-aware privacy algorithm removes more
samples in low-density areas, in which vehicles could be easily
tracked. Gray dots indicate released location samples, black
ones denote removed samples.
Snapshots of Privacy-preserving GPS Traces. Let us com-
pare the privacy-preserving GPS traces generated by the proposed
path cloaking algorithm with the original GPS traces. Figures 6(a)
and 6(b) show both in a high user density scenario for off-peak
(over 1.5 hours at 10am) and peak time (over 1.5 hour at 5pm),
respectively. Gray dots indicate released location samples while
black dots illustrate samples removed by path cloaking. We ob-
serve two characteristics from these traces. First, uncertainty-aware
path cloaking removes fewer location samples at peak time and sec-
ond, it retains more location samples within the presumably busier
downtown area. This illustrates how the algorithm, by virtue of its
design, retains information on busier roads where trafﬁc informa-
tion is most valuable.
5.2 Protection Against Target Tracking
The following target tracking experiment illustrates how the path
cloaking algorithm prevents an adversary from reconstructing an
individual’s path using the cleansed GPS traces. Speciﬁcally, we
compare our uncertainty-aware privacy algorithm and its with-reacq-
uisition version with random subsampling in terms of maximum
and median TTC for conﬁgurations that produce the same number
of released location samples (as a metric of data quality). We evalu-
ate the effectiveness of our proposed privacy preserving algorithms
by answering the following questions:
• Do uncertainty-aware privacy algorithms effectively limit track-
ing time (i.e., guarantee time-to-confusion)? Are these limits
maintained even in low-user density scenarios?
• How does the average tracking time allowed by path cloak-
ing compare to the subsampling baseline, at the same data
quality level.
• How are the results affected by the choice of data quality
metric (percentage of released location samples vs relative
weighted road coverage)?
Throughout the results presented in the following subsections,
one graph depicts many experiment trials, where one trial com-
prises the following steps. We ﬁrst apply a privacy algorithm to the
low-density (500 vehicle) or high-density (2000 vehicle) dataset
generated from the 233 original vehicle traces. We then remove
vehicle identiﬁers and execute the target tracking algorithm (see
Sec. 3) to measure tracking time for the ﬁrst 233 vehicles. For each
vehicle, we compute the tracking time starting from each sample
of the trace and report the maximum. One data point shown in the
graph then corresponds to the median or maximum over the 233 ve-
hicle tracking times computed for one trial. For each graph, these
trials are then repeated with different uncertainty thresholds for the
path cloaking algorithms and different probabilities of removal in
the subsampling algorithm.
5.2.1 Bounded Tracking Time without Reacquisition
First, we ascertain whether the uncertainty-aware privacy algo-
rithm guarantees bounded tracking under the no reacquisition track-
ing assumption . Figures 7(a) and 7(b) show the maximum and me-
dian tracking time plotted against the relative amount of released
location samples, respectively, for a high density scenario with
2000 vehicles in the 70km-by-70km area. Figure 7(a) shows re-
sults for the uncertainty-aware privacy algorithm (marked with +)
for varying uncertainty levels with timeout ﬁxed at 5 minutes and
for the random subsampling algorithm for varying probabilities of
removal. Since the conﬁguration parameters from these algorithms
are not directly comparable, the graph shows the percentage of re-
leased location samples on the x-axis, allowing comparison of TTC
at the same data quality level. Also note that graph compares the
algorithms in terms of maximum tracking time, to illustrate differ-
ences in tracking time variance and outliers. During tracking we
set the adversary’s uncertainty threshold to 0.4. This means that
the adversary will give up tracking if at any point the uncertainty
level rises above this threshold, because the correct trace cannot
be determined. A 0.4 uncertainty level corresponds to a minimum
probability of 0.92 for the most probable next location sample.
]