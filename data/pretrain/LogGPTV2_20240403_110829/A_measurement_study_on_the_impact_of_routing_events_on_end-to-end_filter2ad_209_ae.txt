the path used after the event. We analyze packet loss experienced
by vnl.cs.wust1.edu (shown as PlanetLab host A in Figure 14) when
the path between router R3 and the Beacon is repaired. Before the
event, the host reaches the Beacon via ISP 2 followed by router
R4 in ISP 1. Note that routers R1, R2, and R3 have only one
route entry to the Beacon before the event. After the event, the host
reaches the Beacon directly via ISP 2.
When the recovery event occurs, R3 receives the new path, and
sends it to both R2 and R1. Suppose that R3 waits for the expi-
ration of MRAI timer to send the new route to R1. At the same
time, suppose that R3 can send the new route to R2 because the
MRAI timer has just expired. Note that the MRAI timer is main-
tained for each BGP session. As a result, R2 obtains the new route
and switches to it. However, R2 cannot forward the new route to
other iBGP routers so that it will send a withdrawal message to R1
to poison its previous route. Suppose there is no delay for the with-
drawal, R1 loses its route entry to the Beacon until it obtains the
new route from R3.
In this example, host A can experience packet loss during a re-
covery event, while host B may not. However, if there is no phys-
ical link between R2 and R3, the iBGP session between the two
routers is via R1. Host B still can experience packet loss because
its packet to the Beacon is routed via R1, which can lose its route
entry. The logically fully meshed iBGP sessions are widely de-
ployed within large ISPs.
F
D
C
 1
 0.8
 0.6
 0.4
 0.2
 0
1
2
4
Number of loss bursts
3
5
Figure 15: Multiple loss bursts may be experienced by end
hosts during recovery events.
Table 6: Location of routing failures during recovery events
Class
Recovery-1
Recovery-2
ISP 1
90%
0
ISP 2 Other tier-1
0
0%
0%
59%
Non tier-1
10%
42%
5.4 Multiple Loss Bursts Caused by Routing
Failures
We ﬁnd that multiple packet loss bursts can occur during recov-
ery events. As shown in Figure 15, we identify that some hosts
can experience up to 5 loss bursts. Again, we focus on the ﬁrst
and second loss bursts, which contribute to the majority of packet
loss. About 16% of the ﬁrst loss bursts are identiﬁed as caused
by routing failures, while 8% of the second loss bursts are identi-
ﬁed as caused by routing failures. This means that one recovery
event can also cause multiple routing failures. According to our
measurement, we ﬁnd that more than half of the second routing
failures are transient forwarding loops, and those forwarding loops
last no more than 5 seconds. The reason is that when a route loses
its routing entry during a recovery event, which is the ﬁrst routing
failure, it can propagate the withdrawal message to its neighboring
ASes. When the neighbor receives the withdrawal message, it will
explore available routes if there is none in its routing table. During
the exploration, forwarding loops may occur.
5.5 Location of Routing Failures
We investigate the location of routing failures. Here, we present
the results for both recovery events. According to the DNS name
of the router from which a unreachable ICMP is sent, we identify
the AS to which the router belongs. Table 6 shows the location for
the ﬁrst loss due to routing failures. We observe that for recovery-
1 events, about 90% of routing failures occur within ISP 1, while
no routing failures are observed within ISP 2 during recovery-2
events. Compared with routing failures during failover events, we
ﬁnd similar results indicating that a very small number of routing
failures occur within ISP 2. On the other hand, routing failures do
occur within ISP 2’s neighbors, and the starting time of the ﬁrst
routing failures is just right after the time when the announcement
is advertised. For the second loss burst due to routing failures, we
ﬁnd that all of them occur within ISP 1’s and ISP 2’s neighbors,
and most of them are within ISP 2’s neighbors.
From the example shown in Figure 14, we know that routing
failures during recovery events depend on MRAI timer values. The
new route is advertised to some routers without delay, while it is
delayed for other routers due to the MRAI timer. As we mention
in Section 4, we ﬁnd that the MRAI timer applied in ISP 2 is very
small. This means that the new recovered route can be advertised
to all routers within ISP 2 with few delays. As a result, ISP 2
does not experience any routing failures. Similarly, the new route
can be advertised to its neighbors without delay. This explains the
Table 7: Connectivity of destination preﬁxes from a tier-1 ISP’s
customers.
Class
Single-homed preﬁxes
Multi-homed preﬁxes
A single upstream link Multiple upstream links
48%
29%
6%
17%
observation that the ﬁrst loss bursts during recovery-1 events are
right after the time when an announcement is sent, as shown in
Figure 10(b). On the other hand, during recovery-1 events, most of
the ﬁrst loss bursts caused by routing failures occur within ISP 1.
Those routing failures can be explained by the delay due to the
MRAI timer. Furthermore, the routes advertised by ISP 1 or ISP 2
can cause their neighbors to experience routing failures again.
6. REPRESENTATIVENESS OF THE
BEACON EXPERIMENTS
In this section, we discuss the representativeness of the Beacon
experiments. The Beacon has one link to each of its two tier-1
providers. In general, an AS can have multiple upstream providers
and/or can connect to a single provider via multiple links. There-
fore, the connectivity of destination preﬁxes in the Internet could be
much more complex than the Beacon topology. We ﬁrst character-
ize how destination preﬁxes are connected to upstream providers.
We then investigate how the results or insight gained on the Beacon
experiments can be applied in general. Finally, we discuss methods
to avoid transient routing failures.
6.1 Characterizing Connectivity of
Destination Preﬁxes
A preﬁx is single-homed if its origin AS advertises it to a single
upstream provider. A preﬁx is multi-homed if it is advertised to
multiple providers. On the other hand, a preﬁx can be advertised
to a provider through multiple links. We classify destination pre-
ﬁxes into four categories according to the characteristics of their
connectivity:
• Single-homed preﬁxes via a single upstream link.
• Single-homed preﬁxes via multiple upstream links.
• Multi-homed preﬁxes via a single upstream link.
• Multi-homed preﬁxes via multiple upstream links.
We characterize the connectivity of preﬁxes originated from cus-
tomers of a large tier-1 ISP. In particular, we examine how the ISP’s
customers advertise their preﬁxes to it. Within the ISP, we use a
BGP monitor that has iBGP sessions to some top-level backbone
routers and edge routers connecting to peer networks. A snapshot
of BGP routing table from each router is collected on a daily basis.
Thus, we are able to see all available routes for each destination
preﬁx that we examined. The results presented in this section are
based on data collected on January 15, 2006.
Table 7 shows that over half of preﬁxes originated by customers
of the tier-1 ISP are single-homed preﬁxes. In particular, 48% of all
preﬁxes are single-homed via a single upstream link. This is con-
sistent with the observation on a different tier-1 ISP in a previous
study [2]. We observe that 6% of preﬁxes are single-homed pre-
ﬁx via multiple upstream links, and the corresponding ﬁgures for
multi-homed preﬁxes are 29% and 17%, respectively. Since single-
homed preﬁxes via single upstream link do not have route redun-
dancy, in the remainder of the paper, we will focus on single-homed
preﬁxes via multiple upstream links and multi-homed preﬁxes. In
the next two subsections, we show that the insight gained from the
Beacon experiments can be applied to both multi-homed preﬁxes
with a single upstream link and preﬁxes with multiple upstream
links regardless of single-homed or multi-homed.
6.2 Routing Failures During Failover Events
Let’s ﬁrst look at an AS multihomed to a set of providers, which
could be tier-1 or non-tier-1 ISPs. Each of those providers will
learn more routes either from its peers or from its providers if it is
a non-tier-1 ISP. Thus, multihoming to a set of providers can in-
crease route diversity. However, multihoming only increases the
number of routes learned from peers or providers. Those routes
are less preferable than those from customers when the “prefer-
customer” routing policy is used. The higher preference of cus-
tomer routes forces those routes with lower preference invisible to
other routers within the provider’s network. Thus, during a failover
event, a multi-homed preﬁx via a single upstream link will expe-
rience routing failure just like the Beacon preﬁx, even though it
might have more providers than the Beacon.
On the other hand, connecting to a single provider at multiple
locations will help a customer to avoid routing failures. The rea-
son is that routes learned from the same customer typically have
the same local preference so that they are visible to other routers in
the provider’s network. However, routing failures might still occur
in this case. For example, BGP attributes, such as AS path length,
BGP MED, IGP weight, or router ID, can lead to only one route
available in a router. One possible method to avoid routing failures
is to use the “hot potato” routing policy. The “hot potato” rout-
ing policy is applied to routes coming from the same AS, and each
router selects the best route based on IGP distance. If routes learned
from its clients are always preferred over those from others [13], a
route reﬂector will see other available routes. However, applying
hot potato policy to routes from the same customer may not be efﬁ-
cient because customers typically have limited connections within
a geographic area.
6.3 Routing Failures During Recovery Events
In general, route diversity will be increased when a route is re-
covered. However, some routers may temporarily lose their routes
to the destination during route recovery. Here, we use an example
to show how route failures can occur in general during a recov-
ery event. Figure 16 illustrates an AS with k + m fully connected
routers. Suppose that routers 1, ..., k use routes learned from other
ASes to reach destination d. Here, the destination could be a single-
homed preﬁx via multiple upstream links or a multi-homed preﬁx
via a single or multiple link(s). Routers k + 1, ..., k + m use routes
from those k routers to reach d. Because of the fully meshed topol-
ogy, all routers will have k available routes to the destination.
When a new route is advertised to router k + m, the new route
may have the highest local preference. For example, routers 1, ...,
k learn their routes from peers, and router k + m learns the new
route from a customer. As a result, all routers will switch to the
new route after they learn it from router k + m. Suppose that router
k + m propagates the new route to routers 1, ..., k without any
delay, but postpones sending the new route to routers k + 1, ...,
k + m − 1 due to the MRAI timer. When routers 1, ..., k switch
to the new route, they will send withdrawal messages to routers
k + 1, ..., k + m to poison their previously advertised route. If the
withdrawal messages arrive at routers k + 1, ..., k + m earlier than
the new route does, routers k + 1, ..., k + m will temporarily lose
their routes to d.
As we have seen from the above example, both the location and
AS 1
k+1
k
k−1
k+2
k+3
k+m
2
1
Internet
restored path
d
Figure 16: Routers may lose existing routes during recovery
events.
preference of the recovered route can impact the occurrence of tran-
sient routing failures during recovery events. For example, if the
recovered route is learned from router k, then no router will lose
any route to d, and every router has k available routes during the
recovery event. Or, if the recovered route does not have higher local
preference, then only a subset of routers 1, 2, . . . , i (i < k) switch
to it.
6.4 Discussion
In summary, simply adding physical connectivity might not be
sufﬁcient in minimizing the impact of routing failures. Routing
policies, iBGP conﬁgurations, MRAI timer values, and failure lo-
cations can have signiﬁcant impact on the routing failures. We ob-
serve that the MRAI timer plays a crucial role during failover and
recovery events. Because the MRAI timer is conﬁgured on a per
BGP session basis and is often shared across preﬁxes, the delay for
the announcement is determined by BGP trafﬁc of the session. Dur-
ing the time periods of high BGP trafﬁc volume, routing updates are
most likely to be delayed. Of course, if the MRAI timer is small,
the alternate path can be quickly obtained so that the probability of
incurring routing failures is low. For example, in our measurement,
we observe that ISP 2 has very small MRAI timer value so that
it seldom has routing failures during recovery event. Clearly, ap-
plying MRAI timers at a coarser granularity such as session based
can save memory resources on routers, but it does have a negative
effect on routing. Furthermore, the value of the timer applied to
BGP updates can directly affect the failure duration. Our analysis
implies that there is a need to reevaluate the mechanism to which
MRAI timer is applied and the value of the timer. Another possible
way to minimize the impact of routing failures during failover and
recovery events is to store not only the best path but also the second
best one at each router [24]. This can potentially reduce the chance
that a router loses routes at the same time.
7. RELATED WORK
Similar to our work, a number of measurement studies have cor-
related routing instability and end-to-end performance [15, 23, 6,
16, 19, 1, 25]. Labovitz et al. studied BGP route instability, focus-
ing on the stability of paths between Internet Service Providers and
artiﬁcially injected routing failures to discover their effects on In-
ternet path performance [15]. Markopoulou et al. has characterized
failures that are correlated with IS-IS routing updates [19]. Feam-
ster et al. studied the location and duration of end-to-end path fail-
ures and correlated such failures with BGP routing instability [6].
Teixeira et al. measured the effects of intradomain routing insta-
bility, but did not examine how this instability affects end-to-end
performance [25]. Agarwal et al. correlated BGP routing changes
with packet traces at aggregate level from a large backbone ISP and
found that BGP routing instability usually has little impact on traf-
ﬁc shifts within a single AS [1]. Boutremans et al. used active and
passive measurements to study the impact of network congestion,
link failures, and IS-IS routing instability on voice over IP service
on a tier-1 backbone network [3]. In contrast to the above existing
work, our work focuses on how routing events such as link fail-
ures and repairs affect end-to-end Internet performance. Our work
is partly motivated by the work by Paxson, who identiﬁed Internet
failures, routing loops, and routing pathologies using end-to-end
traceroutes and discovered that routing instability can disrupt end-
to-end connectivity [20]. We take a step further by exploring the
root causes in the form of topological properties for the data plane
performance degradation due to routing changes.
There are also several studies on BGP routing instability [17,
16, 14, 12, 10, 11, 9, 5, 7]. For example, Labovitz et al. con-
ducted a series of empirical studies on characterizing interdomain
instability and the impact of policy and topology on convergence
delays [17, 16]. Alternatively, Grifﬁn et al. [14, 12] took theoreti-
cal approaches in explaining BGP dynamics observed empirically
on the Internet. Gao et al. [10, 9] proposed guidelines and models
for setting local routing policies in each AS to increase routing sta-
bility and reliability. These work is related to ours; however, they
mainly focused on characterizing BGP dynamics and identifying
root causes of such dynamics without further investigating in detail
the impact of such dynamics on end-to-end performance.
8. CONCLUSIONS
Despite the fact that an increasing number of Internet applica-
tions, such as VoIP and gaming, rely on high availability of end-
to-end paths, there is a lack of understanding of which and how
routing events affect end-to-end path performance. In this work,
we conduct extensive measurement involving both controlled rout-
ing updates through two tier-1 ISPs and active probes of a diverse
set of end-to-end paths on the Internet to illuminate the impact of
routing changes on data plane performance.
We ﬁnd that during failover and recovery event, routers can ex-
perience routing failures. Based on our measurement, routing fail-
ures contribute to end-to-end packet loss signiﬁcantly. During both
failover events and recovery events, multiple loss bursts are likely
observed and loss bursts can be signiﬁcantly longer than those ob-
served during recovery events. Multiple loss bursts can occur at dif-
ferent ASes. Furthermore, we show that common iBGP conﬁgura-
tion and MRAI timer values play a major role in causing packet loss
during routing events. Our study suggests that extending BGP to
accommodate routing redundancy may eliminate majority of end-
to-end path failures caused by routing events. The RCP architecture
introduced in [4] is a potential candidate for providing redundancy
within an AS.
Acknowledgements
We would like to thank the support from PlanetLab staff. We also
thank Pei Dan and anonymous reviewers for their constructive com-
ments. We are very grateful to Tim Grifﬁn for his valuable com-
ments on the experimental methodology and for initiating the Bea-
con project. The work is partially supported by NSF grants CNS-
0325868, CNS-0430204, ANI-0208116, ANI-0085848, and the Al-
fred P. Sloan Fellowship.
9. REFERENCES
[1] AGARWAL, S., CHUAH, C.-N., BHATTACHARYYA, S., AND DIOT, C. The
Impact of BGP Dynamics on Intra-Domain Trafﬁc. In Proceedings of ACM
SIGMETRICS (New York, NY, USA, June 2004).
[2] AGARWAL, S., NUCCI, A., AND BHATTACHARYYA, S. Measuring the Shared
Fate of IGP Engineering and Interdomain Trafﬁc. In ICNP (2005), pp. 236–245.
[3] BOUTREMANS, C., IANNACCONE, G., AND DIOT, C. Impact of link failures
on VoIP performance. In Proceedings of ACM NOSSDAV (May 2002).
[4] CAESAR, M., CALDWELL, D., FEAMSTER, N., REXFORD, J., SHAIKH, A.,
AND VAN DER MERWE, J. Design and implementation of a Routing Control
Platform. In Proc. Networked Systems Design and Implementation (2005).
[5] CHANG, D. F., GOVINDAN, R., AND HEIDEMANN, J. The temporal and
topological characteristics of BGP path changes. In Proceedings of IEEE ICNP
(November 2003).
[6] FEAMSTER, N., ANDERSEN, D., BALAKRISHNAN, H., AND KAASHOEK, M.
Measuring the Effects of Internet Path Faults on Reactive Routing. In
Proceedings of ACM SIGMETRICS (San Diego, CA, June 2003).
[7] FELDMANN, A., MAENNEL, O., MAO, Z. M., BERGER, A., AND MAGGS,
B. Locating Internet Routing Instabilities. In Proceedings of ACM SIGCOMM
(2004).
[8] GAO, L. On Inferring Autonomous System Relationships in the Internet.
IEEE/ACM Transactions On Networking 9, 6 (December 2001).
[9] GAO, L., GRIFFIN, T., AND REXFORD, J. Inherently Safe Backup Routing
with BGP. In Proceedings of IEEE INFOCOM (2001).
[10] GAO, L., AND REXFORD, J. A Stable Internet Routing without Global
Coordination. IEEE/ACM Transactions On Networking 9, 6 (December 2001),
681–692.
[11] GRIFFIN, T., AND WILFONG, G. T. A Safe Path Vector Protocol. In
Proceedings of IEEE INFOCOM (2000), pp. 490–499.
[12] GRIFFIN, T. G., SHEPHERD, F. B., AND WILFONG, G. The Stable Paths
Problem and Interdoman Routing. IEEE/ACM Transactions on Networking 10,
2 (April 2002), 232–243.
[13] GRIFFIN, T. G., AND WILFONG, G. On the correctness of IBGP conﬁguration.
In SIGCOMM ’02: Proceedings of the 2002 conference on Applications,
technologies, architectures, and protocols for computer communications
(2002), pp. 17–29.
[14] LABOVITZ, C., AND AHUJA, A. The Impact of Internet Policy and Topology
on Delayed Routing Convergence. In Proceedings of IEEE INFOCOM
(Anchorage, Alaska, April 2001).
[15] LABOVITZ, C., AHUJA, A., BOSE, A., AND JAHANIAN, F. Delayed Internet
routing convergence. IEEE/ACM Transactions on Networking 9, 3 (June 2001),
293–306.
[16] LABOVITZ, C., AHUJA, A., AND JAHANIAN, F. Experimental Study of
Internet Stability and Backbone Failures. In Proceedings of FTCS (1999),
pp. 278–285.
[17] LABOVITZ, C., MALAN, G. R., AND JAHANIAN, F. Internet Routing
Instability. IEEE/ACM Transactions on Networking 6, 5 (1998), 515–528.
[18] MAO, Z. M., BUSH, R., GRIFFIN, T., AND ROUGHAN, M. BGP Beacons. In
Proceedings of IMC (2003).
[19] MARKOPOULOU, A., IANNACCONE, G., BHATTACHARYYA, S., CHUAH, C.,
AND DIOT, C. Characterization of Failures in an IP Backbone, 2004.
[20] PAXSON, V. End-to-end routing Behavior in the Internet. IEEE/ACM
Transactions on Network 5, 5 (1997), 601–615.
[21] PEI, D., ZHAO, X., MASSEY, D., AND ZHANG, L. A Study of BGP Path
Vector Route Looping Behavior. In ICDCS (2004), pp. 720–729.
[22] PlanetLab. http://www.planet-lab.org.
[23] ROUGHAN, M., GRIFFIN, T., MAO, Z. M., GREENBERG, A., AND
FREEMAN, B. Combining Routing and Trafﬁc Data for Detection of IP
Forwarding Anomalies. In Proceedings of ACM SIGCOMM NeTs Workshop
(2004).
[24] SHAND, M., AND BRYANT, S. IP Fast Reroute Framework. Internet Draft
draft-ietf-rtgwg-ipfrr-framework-04.txt, October 2005.
[25] TEIXEIRA, R., SHAIKH, A., GRIFFIN, T., AND REXFORD, J. Dynamics of
hot-potato routing in IP networks, 2004.
[26] WANG, F., GAO, L., WANG, J., AND QIU, J. On Understanding of Transient
Interdomain Routing Failures. In Proceedings of IEEE ICNP (2005).
[27] Y. REKHTER AND T. LI. A border gateway protocol 4 (BGP-4). RFC 1771
(1995).