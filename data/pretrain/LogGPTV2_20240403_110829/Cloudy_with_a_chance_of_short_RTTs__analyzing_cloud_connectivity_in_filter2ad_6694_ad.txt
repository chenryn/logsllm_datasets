lower latency compared to Speedchecker. This occurs because
the majority of Atlas probes are dedicated hardware devices that
connect to the backbone via a wired last-mile. On the other hand,
Speedchecker measurements provide a more accurate representa-
tion of real Internet user connectivity as the used probes are end-
user Android devices connected via a wireless access medium.
4.3 Inter-Continental Latency
Previous results revealed that cloud connectivity can be signifi-
cantly longer in continents with limited datacenter deployment [22].
Therefore we now analyze if the latencies within these regions
improve if the users connect to datacenters in neighbouring (better-
provisioned) continents. The aim of the analysis is to investigate if
shortcomings of sparse geographical datacenter deployments can
be overcome by private and faster network backbones. We consider
two target regions for this analysis - Africa and South America -
68
‚àí100‚àí50050100DiÔ¨Äerenceinlatency[ms]0.000.250.500.751.00PercentileSpeedcheckerfasterAtlasfasterAFASEUNAOCSAIMC ‚Äô21, November 2‚Äì4, 2021, Virtual Event, USA
Dang and Mohan et al.
(a) Africa
(b) South America
Figure 6: Cloud access latency from probes in countries within (a) Africa and (b) South America to nearest cloud datacenters
within the same and in neighbouring continents.
since both host only a few datacenters, but are physically close to
well-served continents (North America and Europe, respectively).
Figure 6a shows the latency distributions of all measurements
recorded from African countries to nearest DCs within Africa, Eu-
rope, and North America. North African countries like Egypt (EG)
and Morocco (MA) have a relatively fast track to Europe due to
their physical proximity. Conversely, the path from these countries
to datacenters in South Africa is much longer, which manifests as
significantly higher access latency. Interestingly, we find that it is
faster for these countries to access North American datacenters via
undersea cables than in-land ones [16, 85]. Unsurprisingly, probes
from South Africa (ZA) have the quickest access to in-land cloud
since all three datacenters in the continents used for our measure-
ments are colocated in nearing regions. The most interesting results
are shown by Kenya (KE) - a country in the central east side of
Africa, which is (almost) equidistant from Europe and South Africa.
Here we observe that the lowest median latency is achieved when
accessing ZA datacenters, albeit with significant variation. On the
other hand, it takes longer to access datacenters in Europe from
Kenya, but the distribution appears to be a lot more stable.
Figure 6b plots similar results for VPs in South American coun-
tries connecting to DCs in Brazil and NA. Notice that the lowest
latencies from the continent are measured from probes in Brazil
(BR) and Argentina (AR) when accessing the in-continent datacen-
ters in BR. AR, being the furthest away, clocks the highest latency
towards NA datacenters. Results from Bolivia (BO) and Peru (PE)
are particularly interesting. Despite being geographically closer
to BR than NA, both countries have almost identical latency dis-
tributions for the two endpoint regions. This is a likely result of
high bandwidth submarine fiber cables connecting both countries
directly to North America [85]. Countries located in the north of the
continent, e.g., Colombia (CO), Ecuador (EC), and Venezuela (VE),
reach NA datacenters quicker than the SA ones. Once again, we
verify that cloud access latency is highly influenced by datacenter
distance (see BR and AR). However, our analysis also shows that
strong networking infrastructure can greatly help in case of local
datacenter scarcity (see BO and PE).
Takeaway ‚Äî Networking infrastructure can play an instrumental
role in bringing down latencies for regions with sparse datacenter
deployments. Remote countries (such as Bolivia, Peru, and Kenya)
can achieve similar performance connecting to datacenters in-
land or within neighbouring continents due to a well-provisioned
networking backbone. However, for most countries within SA,
Africa, and Asia, physical proximity to datacenters is the driving
factor affecting overall access latencies.
5 INFLUENCE OF WIRELESS LAST-MILE
We leverage our traceroute measurements to analyze the impact
of the wireless last-mile on cloud access. We infer the last-mile as
the link segment between probe IP address and first hop within
ISP AS. Since Speedchecker Android probes can either use WiFi
or cellular links for connecting to the Internet, we divide them
into two broad categories - home and cell. As the name suggests,
home VPs are user devices deployed in home networks that use
WiFi as wireless connectivity. We identify such VPs through their
network paths which traverse a private first-hop (home router)
before ingressing the ISP AS. Within this set, we breakup last-mile
latency into 1) wireless inclusive, i.e., from the probe to ISP (SC
home [USR-ISP]) and 2) wireless exclusive, i.e., home router to ISP
(SC home [RTR-ISP]). The SC cell category includes measure-
ments from VPs that have a direct one-hop link to ISP ASN. These
probes are, with high likelihood, user devices using cellular wire-
less medium to access the Internet, and the RTT of the last-mile
reflects latency between the device and the cellular tower. Keep in
mind that there are several caveats associated with our categoriza-
tion approach, which may impact the accuracy of our inferences.
Firstly, the first hop responding to our traceroutes might not be
the basestation itself (home or cellular). As a result, our inferred
last-mile may include part of ISP internal network in addition to
the wireless media. Similarly, for connections to the Internet via a
VPN or carrier-grade NATs (CGN) [71], private addresses will be
translated to public IPs; which would directly impact our home-cell
probe classification. Secondly, previous research has shown that
latency estimates from traceroutes can be inflated due to path
inconsistencies, probe processing from underpowered networking
devices, and so on. [32, 55, 80]. Such delays are hard to accurately
detect post-measurement, and thus may unduly impact our study.
Last-mile share of user path to cloud. Figure 7a shows the per-
centage share of wireless last-mile to the overall cloud access la-
tency for home probes (SC home [USR-ISP]) and cellular probes
(SC cell). Firstly, we find that the distribution of the latency share is
69
DZEGETKEMASNTNZA0100200300400500600Latency[ms]EUNAAFARBOBRCLCOECPEVE0100200300400Latency[ms]NASACloudy with a Chance of Short RTTs
IMC ‚Äô21, November 2‚Äì4, 2021, Virtual Event, USA
(a) Share of wireless last-mile to total cloud access latency for Speed-
checker probes.
(b) Absolute latency at the wireless last-mile for Speedchecker and
RIPE Atlas probes.
Figure 7: Impact of the wireless last-mile on the cloud access latency grouped by continents. SC home (USR-ISP) is the latency
between the VP and the ISP (via a home router), SC home (RTR-ISP) is the latency between the home router and the ISP, SC
cell is the latency between the VP and the first hop of cellular network, and Atlas is last-mile latency of RIPE Atlas probes.
quite similar for both access technologies, irrespective of the probe
location. Secondly, the wireless last-mile accounts for a significant
share of the total cloud access latency and is higher in continents
with more provisioned cloud deployment (i.e., NA and EU.). The
result is somewhat expected as not only the overall latency to reach
the nearest datacenter is significantly lower within these regions
(see ¬ß4.1), the latency due to transit is also quite low due to sig-
nificant deployment of cloud-owned WANs. As a result, the effect
of last-mile to overall cloud access latency is more pronounced
within these continents. In developing continents, such as Africa
and Asia, the percentage share of latency due to last-mile is much
smaller as paths to cloud traverse large geographical distances due
to relatively sparse datacenter deployment. We also observe that
the impact of the last-mile is higher in home probes than cellular
probes within developing regions compared to the rest of the globe.
Figure 19 in Appendix A.5 illustrates a similar percentage share
of last-mile access to end-to-end latency per probe, but only for
measurements towards the nearest cloud datacenter. The distribu-
tion trend remains fairly unchanged from Figure 7a and further
strengthens our inferences drawn above. However, we now find
that the latency due to the last-mile is more likely to be the primary
bottleneck ‚Äì as it exceeds the 50% share almost globally.
To understand the behaviour of the last-mile further, we com-
pare the absolute latency at the last-mile for both home and cellular
connections in Figure 7b. The plot also compares latency due to
the wired part of the home connection (SC home [RTR-ISP]) and
last-mile of probes in RIPE Atlas dataset (Atlas) (¬ß4.2). The re-
sult indicates that the nature of last-mile (cellular or WiFi) has
little influence on the overall cloud access latency across the globe.
The latency distribution of path between probe and ISP is similar
across continents as the median value hovers around 20‚Äì25 ms for
both home and cellular connection types. Interestingly, last-mile
(irrespective of the access technology) borders close to the MTP
threshold ( pairs with at least 10
samples. The result shows that both WiFi-based home probes and
cellular probes show similar variation across time, with the median
ùê∂ùë£ hovering around 0.5. Correlating the results with the absolute
latency achieved by home and cellular probes (Figure 7b) confirms
that all currently deployed wireless access technologies have simi-
lar behaviors and account for a significant portion of the latency to
the cloud. Figure 9 sheds more light on our results and shows ùê∂ùë£
of probes in two representative countries in each continent. Even
though the plot illustrates subtle stability differences in last-mile
delays across different countries, the state, and latencies due to the
wireless media is comparable (and significant) throughout the globe.
While new technologies like 5G promise to improve the last-mile
connectivity, preliminary studies measuring its current deployment
in-the-wild show minimal improvements over existing technolo-
gies [64, 65]. However, since 5G deployment is still in its nascent
stages, its performance is expected to improve in the future [69].
Takeaway ‚Äî Despite significant efforts to improve network con-
nectivity, the last-mile link continues to be the primary bottleneck
for cloud providers. As the coveted hop remains out of cloud op-
erator‚Äôs influence, latencies due to wireless will make support for
latency-critical applications difficult - unless the wireless media
improves significantly.
6 CLOUD & ISP INTERCONNECTIONS
Our study till now has focused on understanding the state of cloud
access across the globe and how user-side of the network impacts
overall latencies. As noted in ¬ß2.3, cloud providers have made sig-
nificant investments for shortening the path between tenant ISP
and their private WAN by setting up direct and private peering
agreements globally. In this section, we isolate the occurrences of
such undertakings by the cloud providers in our measurements and
analyze their possible impact on reducing user latencies.
6.1 Identifying ISP-Cloud Peering.
To accurately identify interconnections between VP ISPs and cloud
providers, we remove any unresponsive IP addresses and map the
remaining to their respective ASes using the methodology described
in ¬ß3.3. We identify and tag any IXPs on a path using CAIDA [17]
and PeeringDB [1] datasets, and remove them from AS-level topol-
ogy as they only act as points of traffic exchange. Further, we clas-
sify paths where the cloud and probe ISP AS are directly connected
neighbours as direct peering. Paths where an intermediate AS acts
as transit between cloud and VP ISP are tagged as private peering.
Finally, paths with more than one transit ASes are categorized as
public Internet. Please note that our peering relationship identifi-
cation may include several artifacts. Firstly, it is not guaranteed
that IXP hops will show up in traceroutes, and therefore we might
miss classify routes that traverse via IXPs as direct. Secondly, since
we conduct our measurements from probes outside of cloud and
ISP networks, our resulting traceroutes may not include router
hops within these WANs, thus resulting in mis-identification of
interconnections. A more complete approach for accurately iden-