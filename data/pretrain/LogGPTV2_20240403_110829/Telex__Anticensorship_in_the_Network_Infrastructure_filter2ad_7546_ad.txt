the tag and create an entry for the connection in a table
indexed by ﬂow. All future event handlers test whether
the ﬂow triggering the event is contained in this table, and
do nothing if it is not.
The Bro script then instructs the diversion component
(via a persistent TCP connection) to block the associated
ﬂow. As this does not affect the tap, our script still re-
ceives the associated packets, and the script is responsible
for actively forwarding them until the TLS Finished mes-
sages are observed. This allows the Bro script to inspect
each packet before forwarding it, while ensuring that any
delays in processing will not cause a packet that should
be blocked to make it through the router (e.g., a TLS Ap-
plicationData packet from NotBlocked.com to the client).
To derive the TLS shared secret from the key exchange,
our Bro script also stores the necessary parameters from
the TLS ServerKeyExchange message in the connection
table.
Once it observes the server’s TLS Finished handshake
message, our Bro script stops forwarding packets between
the client and the server (thus atomically severing traf-
ﬁc ﬂow between them) and sends the connection state,
which includes the TCP-level state (sequence number,
TCP options, windows, etc.), the key exchange parame-
ters, and the shared secret ksh to the proxy service compo-
nent. Our proof-of-concept implementation handles only
the TCP timestamp, selective acknowledgements (SACK),
and window scaling options, but other options could be
handled similarly. Likewise, we currently only support
TLS’s Difﬁe-Hellman key exchange, but RSA and other
key exchange methods could also be supported.
Proxy service
The proxy service component plays the
role of the TLS server and connects the client to blocked
websites. Our implementation consists of a user space
process called telex_relay and an associated kernel
module, which are responsible for decapsulating TLS
connection data and passing it to a local Squid proxy [25].
The telex_relay process is responsible for relaying
data from the client to the Squid proxy, in effect spooﬁng
the server side of the connection. We defer forwarding
of the last TLS Finished message until telex_relay has
initialized its connection state in order to ensure that all
application data is observed. We implement this delay by
including the packet containing TLS Finished message in
the state sent from our Bro script and leaving the task of
forwarding the packet to its destination to telex_relay,
thus avoiding further synchronization between the com-
ponents.
Similarly to telex_client, telex_relay is written
in about 1250 lines of C (again including shared TLS
utility code) and uses libevent to manage multiple connec-
tions. It reuses our modiﬁcations to OpenSSL in order to
substitute our shared secret for OpenSSL’s shared secret.
We implement relaying of packets between the client and
the Telex service straightforwardly, by registering event
handlers to read from one party and write to the other
using the usual send and recv system calls on the one
hand and SSL_read and SSL_write on the other.
To avoid easy detection, the relay’s TCP implementa-
tion must appear similar to that of the original TLS server.
Ideally, telex_relay would simply bind(2) to the ad-
dress of the original server and set the IP_TRANSPARENT
socket option, which, in conjunction with appropriate
ﬁrewall and routing rules for transparent proxying [29],
would cause its socket to function normally despite be-
ing bound to a non-local address. This would cause the
relay’s TCP implementation to be identical to that of the
operating system that hosts it. However, the TCP hand-
shake has already happened by the time our Bro script
redirects the connection to telex_relay, so we need a
method of communicating the state negotiated during the
handshake to the TCP implementation. Accordingly, we
modiﬁed the Linux 2.6.37 kernel to add a fake_accept
ioctl that allows a userspace application to create a seem-
ingly connected socket with arbitrary TCP state, including
endpoint addresses, ports, sequence numbers, timestamps,
and windows.
8 Evaluation
In this section, we evaluate the feasibility of our Telex
proxy prototype based on measurements of its perfor-
mance.
8.1 Model deployment
We used a small model deployment consisting of three
machines connected in a hub-and-spoke topology. Our
simulated router is the hub of our deployment, and the
two machines connected are the Telex station, and a web
server serving pages over HTTPS and HTTP. The Telex
our control, we used the Apache benchmark ab [1] to
have each of the clients simultaneously download a static
1-kilobyte page over HTTPS. To compare to Telex, we
then conﬁgured ab to download the same page through
the telex_client. Because the Telex tunnel itself is
encrypted with TLS, we conﬁgured ab to use HTTP,
not HTTPS, in this latter case. For the NotBlocked.com
used by telex_client, we used our server on port 443
(HTTPS) and for Blocked.com, we used our same server
on port 80 (HTTP).
We modiﬁed ab to ensure that only successful connec-
tions were counted in throughput numbers and to override
its use of OpenSSL’s SSL_OP_ALL option. This option
originally caused ab to send fewer packets than a default
conﬁguration of OpenSSL, allowing the TLS control to
perform artiﬁcially better at the cost of decreased security.
We used ab to perform batches of 1000 connections
(ab -n 1000); in each batch, we conﬁgured it to use a
variable number of concurrent connections. We repeated
each trial on our two clients (client A and client B) to get
a mean connection throughput for each client.
The results are shown in Figure 4; the performance
of the Telex tunnel lags behind that of TLS at low con-
currency, but catches up at higher concurrencies. The
observered performance is consistent with Telex introduc-
ing higher latency but similar throughput, which we posit
is due to Telex’s additional processing and network delay
(e.g., execution of the fake_accept ioctl). Both Telex
and TLS exhibit diminishing returns from more than 10
concurrent requests, and both start to plateau at 30 con-
current requests. Manual inspection of client machines’
CPU utilization conﬁrms that the tests are CPU bound by
50 concurrent connections.
8.4 Real-world experience
To test functionality on a real censor’s network, we ran a
Telex client on a PlanetLab [24] node located in Beijing
and attempted connections to each of the Alexa top 100
websites [2] using our model Telex station located at the
University of Michigan. As a control, we ﬁrst loaded these
sites without using Telex and noted apparent censorship
behavior for 17 of them, including 4 from the top 10: face-
book.com, youtube.com, blogspot.com and twitter.com.
The blocking techniques we observed included forged
RST packets, false DNS results, and destination IP black
holes, which are consistent with previous ﬁndings [15].
We successfully loaded all 100 sites using Telex. We also
compared the time taken to load the 83 unblocked sites
with and without Telex. While this metric was difﬁcult
to measure accurately due to varying network conditions,
we observed a median overhead of approximately 60%.
To approximate the user experience of a client in China,
we conﬁgured a web browser on a machine in Michigan
Figure 4: Client Request Throughput — We measured
the rate at which two client machines could complete
HTTP requests for a 1 kB page over a laboratory network,
using either TLS or our Telex prototype. The prototype’s
performance was competitive with that of unmodiﬁed
TLS.
station has a 2.93 GHz Intel Core 2 Duo E7500 processor
and 2 GB of RAM. The server has a 4-core, 2.26 GHz
Intel Xeon E55200 processor and 11 GB of RAM. The
router has a 3.40 GHz Intel Pentium D processor and 1 GB
of RAM. All of the machines in our deployment and tests
are running Ubuntu Server 10.10 and are interconnected
using Gigabit Ethernet.
8.2 Tagging performance
We evaluated our tagging implementation by generating
and verifying tags in bulk using a single CPU core on
the Telex station. We performed ten trials, each of which
processed a batch of 100,000 tags. The mean time to gen-
erate a batch was 18.24 seconds with a standard deviation
of 0.016 seconds, and the mean time to verify a batch was
9.03 seconds with a standard deviation of 0.0083 seconds.
This corresponds to a throughput of approximately 5482
tags generated per second and 11074 tags veriﬁed per
second. As our TLS throughput experiments show, tag
veriﬁcation appears very unlikely to be a bottleneck in
our system.
8.3 Telex-TLS performance
To compare the overhead of Telex, we used our model
deployment with two additional clients connected to the
router. Our primary client machine (client A) has a 2.93
GHz Intel Core 2 Duo E7500 processor and 2 GB of
RAM. The secondary client machine (client B) has a 3.40
GHz Intel Pentium D processor and 2 GB of RAM. For
01020304050Concurrent connections per client050100150200250Connection throughput (reqs./sec)Client A–TLSClient A–TelexClient B–TLSClient B–Telexto proxy its connections over an SSH tunnel to our Telex
client running in Beijing. Though each request traveled
from Ann Arbor to China and back before being for-
warded to its destination website (a detour of at least
32,000 km), we were able to browse the Internet uncen-
sored, and even to watch streaming YouTube videos.
Anecdotally, three of the authors have used Telex for
their daily Web browsing for about two months, from
various locations in the United States, with acceptable
stability and little noticeable performance degradation.
The system received additional stress testing because an
early version of the Telex client did not restrict incom-
ing connections to the local host, and, as a result, one
of the authors’ computers was enlisted by others as an
open proxy. Given the amount of malicious activity we
observed before the issue was corrected, our prototype
deployment appears to be robust enough to handle small-
scale everyday use.
9 Future Work
Maturing Telex from our current proof-of-concept to a
large-scale production deployment will require substantial
work. In this section, we identify four areas for future
improvement.
Trafﬁc shaping An advanced censor may be able to
distinguish Telex activity from normal TLS connections
by analyzing trafﬁc characteristics such as the packet and
document sizes and packet timing. We conjecture that this
would be difﬁcult to do on a large scale due to the large
variety of sites that can serve as NotBlocked and the dis-
ruptive impact of false positives. Nevertheless, in future
work we plan to adapt techniques from prior work [18]
to defend Telex against such analysis. In particular, we
anticipate using a dynamic padding scheme to mimic the
trafﬁc characteristics of NotBlocked.com. Brieﬂy, for
every client request meant for Blocked.com, the Telex
station would generate a real request to NotBlocked.com
and use the reply from NotBlocked.com to restrict the
timing and length of the reply from Blocked.com (as-
suming the Blocked.com reply arrived earlier). If the
NotBlocked.com data arrived ﬁrst, the station would send
padding as a reply to the client, including a command to
send a second “request” if necessary to ensure that the
apparent document length, packet size, and round trip
time remained consistent with that of NotBlocked.com.
Server mimicry Different service implementations
and TCP stacks are easily distinguished by their observ-
able behavior [21, Chapter 8]. This presents a substantial
challenge for Telex: to avoid detection when the Not-
Blocked.com server and the Telex station run different
software, a production implementation of Telex would
need to accurately mimic the characteristics of many com-
mon server conﬁgurations. Our prototype implementation
does not attempt this, and we have noted a variety of ways
that it deviates from TLS servers we have tested. These
deviations include properties at the IP layer (e.g. stale IP
ID ﬁelds), the TCP layer (e.g. incorrect congestion win-
dows, which is detectable by early acknowledgements),
and the TLS layer (e.g. different compression methods
and extensions provided by our more recent OpenSSL
version). While these speciﬁc examples may themselves
be trivial to ﬁx, convincingly mimicking a diverse popu-
lation of sites will likely require substantial engineering
effort. One approach would be for the Telex station to
maintain a set of userspace implementations of popular
TCP stacks and use the appropriate one to masquerade as
NotBlocked.com.
Station scalability Widescale Telex deployment will
likely require Telex stations to scale to thousands of con-
current connections, which is beyond the capacity of our
prototype. We plan to investigate techniques for adapt-
ing station components to run on multiple distributed
machines. Clustering techniques [31] developed for in-
creasing the scalability of the Bro IDS may be applicable.
Station placement
Telex raises a number of questions
related to Internet topography. How many ISPs would
need to participate to provide global coverage? Short of
this, where should stations be placed to optimally cover a
particular censor’s network? We leave accurate deploy-
ment modelling for future work.
Furthermore, we currently make the optimistic assump-
tion that all packets for the client’s connection to Not-
Blocked.com pass through some particular Telex station,
but this might not be the case if there are asymmetric
routes or other complications. Does this assumption hold
widely enough for Telex to be practically deployed? If
not, the system could be enhanced in future work to sup-
port cooperation among Telex stations on different paths,
or to support multi-headed stations consisting of several
routers in different locations diverting trafﬁc to common
recognition and relay components.
10 Conclusion
In this paper, we introduced Telex, a new concept in
censorship resistance. By moving anticensorship service
from the edge of the network into the core network infras-
tructure, Telex has the potential to provide both greater
resistance to blocking and higher performance than ex-
isting approaches. We proposed a protocol for stegano-
graphically implementing Telex on top of TLS, and we
supported its feasibility with a proof-of-concept imple-
mentation. Scaling up to a production implementation
will require substantial engineering effort and close part-
nerships with ISPs, and we acknowledge that worldwide
deployment seems unlikely without government partici-
pation. However, Internet access increasingly promises to
empower citizens of repressive governments like never be-
fore, and we expect censorship-resistant communication
to play a growing part in foreign policy.
Acknowledgments
We are grateful to the anonymous reviewers for their con-
structive feedback, and to Matthew Green for shepherding
the work to publication. We also wish to thank Michael
Bailey, Drew Dean, Robert K. Dick, Roger Dingledine,
Ed Felten, Manish Kirir, Z. Morley Mao, Nadia Heninger,
Peter Honeyman, Brent Waters, Florian Westphal, and
Xueyang Xu for thoughtful discussions. Ian Goldberg
gratefully acknowledges the funding support of NSERC
and MITACS.
References
[1] ab: Apache benchmark.
programipsets/ab.html.