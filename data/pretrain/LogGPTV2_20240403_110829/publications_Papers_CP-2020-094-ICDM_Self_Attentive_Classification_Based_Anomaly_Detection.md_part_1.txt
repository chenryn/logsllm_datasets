Self-Attentive Classification-Based Anomaly
Detection in Unstructured Logs
Sasho Nedelkoski∗, Jasmin Bogatinovski∗, Alexander Acker∗, Jorge Cardoso†, Odej Kao∗
∗Distributed and Operating Systems, TU Berlin, Berlin, Germany
{nedelkoski, jasmin.bogatinovski, alexander.acker, odej.kao}@tu-berlin.de
†Huawei Munich Research Center, Huawei Technologies, Munich, Germany
PI:EMAIL
Abstract—The detection of anomalies is essential mining task may arise. Log messages have free-form text structure written 0202
for the security and reliability in computer systems. Logs are a by the developers, which record a specific system event de-
commonandmajordatasourceforanomalydetectionmethodsin
scribingtheruntimesystemstatus.Specifically,alogmessage
almosteverycomputersystem.Theycollectarangeofsignificant
isacompositionofconstantstringtemplateandvariablevalues
events describing the runtime system status. Recent studies
have focused predominantly on one-class deep learning methods originating from logging instruction (e.g., print(”total of %i guA
on predefined non-learnable numerical log representations. The errors detected”, 5)) within the source code.
main limitation is that these models are not able to learn Acommonapproachforloganomalydetectionisone-class
log representations describing the semantic differences between
classification [10], where the objective is to learn a model 12
normal and anomaly logs, leading to a poor generalization of
that describes the normal system behaviour, usually assuming
unseen logs. We propose Logsy, a classification-based method
to learn log representations in a way to distinguish between that most of the unlabeled training data is non-anomalous and
]GL.sc[
normal data from the system of interest and anomaly samples that anomalies are samples that lie outside of the learned
from auxiliary log datasets, easily accessible via the internet. decision boundary. The massive log data volumes in large
The idea behind such an approach to anomaly detection is
systems have renewed the interest in the development of one-
that the auxiliary dataset is sufficiently informative to enhance
class deep learning methods to extract general patterns from
the representation of the normal data, yet diverse to regularize
against overfitting and improve generalization. We propose an non-anomalous samples. Previous studies have been focused
attention-based encoder model with a new hyperspherical loss mostlyontheapplicationoflongshort-termmemory(LSTM)-
function. This enables learning compact log representations based models [8], [9], [11]. They leverage log parsing [12], 1v04390.8002:viXra
capturing the intrinsic differences between normal and anomaly
[13] on the normal log messages and transform them into
logs. Empirically, we show an average improvement of 0.25 in
log templates, which are then utilized to train the models.
the F1 score, compared to the previous methods. To investigate
the properties of Logsy, we perform additional experiments The formulated task is to predict the next index of the
including evaluation of the effect of the auxiliary data size, the log template in the sequence t m+1 by utilizing the history
influenceofexpertknowledge,andthequalityofthelearnedlog of template indices H = t ,...,t . In other disciplines,
0 m
representations.Theresultsshowthatthelearnedrepresentation
numerous deep learning methods increase their performances
boosttheperformanceofthepreviousmethodssuchasPCAwith
by incorporating large amounts of data available through the
a relative improvement of 28.2%.
Index Terms—anomaly detection, log data, transformers, sys- internet.Acommonapproachtousethesedataisunsupervised
tems reliability learning.Innaturallanguageprocessing(NLP),word2vec[14]
andmorerecentlanguagemodelsBERT[15]arestandardand
I. INTRODUCTION responsibleforsignificantimprovementsinvariousNLPtasks.
Anomaly detection [1]–[3] is a data mining task of finding These models are pretrained on large corpora of text such as
observations in a corpus of data that differ from the expected Wikipediaandlaterfine-tunedontheparticulartaskordataset.
behaviour.Anomaliesinlargesystemssuchascloudandhigh- Recent studies in log anomaly detection [9], [11] utilize a
performance computing (HPC) platforms can impact critical pre-trained word embeddings to numerically represent the log
applications and a large number of users [4]. Owing to the templates instead of the integer log sequences [8], where they
inevitable weaknesses in software and hardware, systems are observe small improvements in the prediction of unseen logs.
prone to failures, which can potentially harm them to a large However, the learning of the sequence of template indices
extent [5], [6]. Timely and accurate detection of such threats and the enhanced log message embedding approaches still
is necessary for reliability, stable operation, and mitigation of havelargelimitationsintermsofgeneralizationforpreviously
losses in a complex computer system. unseen log messages. They tend to produce false predictions
Logs are an important data source for anomaly detection in owing to the imperfect log vector representations. For exam-
computersystems[7]–[9].Theyrepresentinteractionsbetween ple, learning sequence of indices fails to correctly classify a
data, files, services, or applications, and are typically utilized newly appearing log messages, and, the domain where the
by developers, and data-driven methods to understand system word vectors are pre-trained (e.g., Wikipedia) has essential
behaviours and to detect, localize, and resolve problems that differences from the language used in computer system de-
velopment. To partly mitigate some of these limitations, a 4) In another set of experiments, an investigation of the
possibility is to incorporate labeled data from operators and effects of variations in the amount of auxiliary data
perform life-long learning [16]. Yet, it still requires frequent for anomaly detection and inclusion of labelled data is
periodical retraining, updates, and costly expert knowledge to performed.
label the data, without addressing the problem of generaliza- 5) We provide an open-source implementation of the
tion on unseen logs that appear between retraining epochs. method.
Often,theassumptionforthenormaldatainanomalydetec-
II. RELATEDWORK
tionmethodsisthatitshouldbecompact[17].Thismeansthe
normal log messages should have vector representations with Asignificant amountof researchanddevelopment ofmeth-
closedistancesbetweeneachother,e.g.,concentratedwithina ods for log anomaly detection has been published in both
tight sphere, and the anomalies should be spread far from the industry and academia [8], [9], [11], [12], [20], [21].Super-
distributionofthenormalsamples.Weproposeanewanomaly vised methods were applied in the past to address the log
detection method that directly addresses the challenge of ob- anomaly detection problem. For example, [20] applied a
tainingrepresentativeandcompactnumericallogembeddings. support vector machine (SVM) to detect failures, where both
We train a neural network to learn log vector representations normal and anomalous samples are assumed to be available.
in a manner to separate the normal log data from the system For an overview of supervised approaches to log anomaly
of interest and log messages from auxiliary log datasets from detection we refer to Brier et al. [22]. However, obtaining
other systems, easily accessible via the internet. The concept system-specificlabelledsamplesiscostlyandoftenpractically
of such a classification approach to anomaly detection is that infeasible.
the auxiliary dataset helps learn a better representation of Several unsupervised learning methods have been proposed
the normal data while regularizing against overfitting. This aswell.Xuetal.[21]proposedusingthePrincipalComponent
ultimately leads to a better generalization in unseen logs. Analysis (PCA) method, where they assume that there are
For example, for a target system logs of interest T where different sessions in a log file that can be easily identified by
anomaly detection needs to be performed, as auxiliary data a session-id attached to each logentry. It first groups log keys
could be employed one or more datasets from an open-source bysessionandthencountsthenumberofappearancesofeach
logrepository(e.g.,[18]).Asaneuralnetworkarchitecture,we log key value inside each session. A session vector is of size
adopt the Transformer encoder with multi-head self-attention n, representing the number of appearances for each log key
mechanism [19], which learns context information from the in K in that session. A matrix is formed where each column
logmessageintheformoflogvectorrepresentations(embed- is a log key, and each row is one session vector. PCA detects
dings). We propose a hyperspherical learning objective that an abnormal vector (a session) by measuring the projection
enforcesthemodeltolearncompactlogvectorrepresentations length on the residual subspace of a transformed coordinate
of the normal log messages. This enforces for the normal system. The publicly available implementation allows for the
samplestohaveconcentrated(compact)vectorrepresentations term frequency-inverse document frequency (TF-IDF) repre-
aroundthecentreofahypersphere.Itenablesbetterseparation sentation of the log messages, utilized in our experiments as
between the normal and the anomaly data, where a distance a baseline. Lou et al. [23] proposed Invariant Mining (IM) to
from the centre of such a sphere is used to represent an mine the linear relationships among log events from log event
anomalyscore.Smalldistancescorrespondtonormalsamples, count vectors.
while large distances correspond to anomalies. The method The wide adoption of deep learning methods resulted in
also enables a direct log-to-vector transformation, which can variousnewsolutionsforlog-basedanomalydetection.Zhang
be used to improve the performances of previous related et al. [24] used LSTM to predict the anomaly of log sequence
methods. Additionally, it allows the operator to intervene and based on log keys. Similar to that, DeepLog [8] also use
correctmisclassifiedsamples,whichcouldbeusedforthenext LSTMtoforecast thenextlog eventandthen compareitwith
retraining of the model. the current ground truth to detect anomalies. Vinayakumar
The contributions of this study can be summarized in the et al. [25] trained a stacked-LSTM to model the operation
following points. log samples of normal and anomalous events. However, the
1) A new classification-based method for log anomaly input to the unsupervised methods is a one-hot vector of logs
detection utilizing self-attention and auxiliary easy- representing the indices of the log templates. Therefore, it
accessible data to improve log vector representation. cannot cope with newly appearing log events.
2) Modified objective function using hyperspherical deci- Some studies have leveraged NLP techniques to analyze
sion boundary, which enables compact data representa- log data based on the idea that log is a natural language
tions and distance-based anomaly score. sequence. Zhang et al. [24] proposed to use the LSTM model
3) The proposed approach is evaluated against three real and TF-IDF weight to predict the anomalous log messages.
anomaly detection datasets from HPC systems, Blue Bertero et al. [26] used word2vec and traditional classifiers,
Gene/L, Thunderbird, and Spirit. The method signifi- like SVM and Random Forest, to check whether a log
cantlyimprovestheevaluationscorescomparedtothose event is an anomaly or not. Similarly, LogRobust [9] and
in the previous studies. LogAnomaly [11] incorporate pre-trained word vectors for
learning of a sequence of logs where they train an attention- IV. SELF-ATTENTIVEANOMALYDETECTIONWITH
based Bi-LSTM model. CLASSIFICATION-BASEDOBJECTIVE
Different from all the above methods, we add domain In this section, we explain the proposed method in de-
bias on the anomalous distribution to improve detection [27]. tail. We provide formal definitions needed for explaining the
We provide such bias by employing easily accessible log method. We describe the data preprocessing, the neural net-
datasetsasanauxiliarydatasource.WeevaluateLogsyagainst work,thelogvectorrepresentations,andhowtheyareutilized
unsupervised approaches, as even it is a classification based in the modified objective function for anomaly detection.
approach, it does not use labels from the target system,
which as mentioned are often infeasible to obtain. From A. Preliminaries
the perspective of using labels of the target system it is an
We define a log as a sequence of temporally ordered
unsupervised approach.
unstructured text messages L = (x : i = 1,2,...), where
i
each message x is generated by a logging instruction (e.g.
i
printf(), log.info()) within the software source code, and i is
III. TOWARDSCLASSIFICATION-BASEDLOGANOMALY its positional index within the sequence. The log messages
DETECTION consistofaconstantandanoptionalvaryingpart,respectively
referred to as log template and variables.
The smallest inseparable singleton object within a log
Anomaly detection can be also viewed as density level set message is a token. Each log message consists of a finite
estimation [28]. Steinwart et al. [27] state that this can be sequence of tokens, r = (w : w ∈ V, j = 1,2,...,s ),
i j j i
interpreted as binary classification between the normal and where V is a set (vocabulary) of all tokens, j is the positional
the anomalous distribution and point out that the bias on index of a token within the log message x , and s is the
i i
theanomalousdistributionisessentialforimproveddetection. total number of tokens in x . We use |r | instead of s
i i i
Meaning that if we provide some information to the model of in following. For different x , |r | can vary. Depending on
i i
how anomalous data looks like, it will boost its performance. the concrete tokenization method, w can be a word, word
j
For instance, we may interpret the class assumption that piece, or character. Therefore, tokenization is defined as a
semi-supervised anomaly detection approaches require on the transformation function T :x→r.
anomalies, as such prior knowledge [17]. Moreover, specific Withrespecttoourproposedmethod,thenotionsofcontext
types of data can have an inherent properties that allows us and numerical vector representation (embedding vector) are
to make more informed prior assumptions such as the word additionally introduced. Given a token w , its context is
j
representations in texts [29]. Here the assumption is that each defined by a preceding and subsequent sequence of tokens,
word meaning depends on its context. i.e. a tuple of sequences: C(w ) = ((w ,w ,...,w ),
j 1 2 j−1
We assume that drawing realistic samples from some aux- (w ,w ,...,w )), where 0 ≤ j ≤ |r |. An embedding
j+1 j+2 |ri| i
iliary easy-access corpus of log data, can be much more vector is a d-dimensional real valued vector representation
informative for an added description of normal and anomalies s∈Rd of either a token or a log message.
compared to sampling noise, or no data used. The use of In the learned vector space, similar log messages should
auxiliarydataaddsextravaluetothemethod,whilepreserving be represented by closer embedding vectors while largely
the information from the normal data. different log messages should be distant. For example, the
PROBLEMDEFINITION.LetD ={(x 1,y 1),...,(x n,y n)} embeddingvectorsfor”Took10secondstocreateaVM”and
be the training logs from the system of interest where x ∈ ”Took9secondstocreateaVM”shouldhaveasmalldistance
i
Rd is a log message where it words are represented in in d-dimensional space, while vectors for ”Took 9 seconds to