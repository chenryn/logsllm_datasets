b
w
a
r
D
•
•
•
•
•
⋄
⋄
⋄
⋄
e
c
r
u
o
s
n
e
p
O
c
i
l
b
u
P
o
t
s
e
d
o
C
d
n
a
a
t
a
D
f
o
e
s
a
e
l
e
R
⋄
⋄
⋄
⋄
⋄
⋄
⋄
⋄
•
)
s
l
e
d
o
m
f
o
n
o
i
t
c
e
l
l
o
c
a
(
l
e
d
o
M
e
l
b
m
e
s
n
E
⋄
⋄
⋄
⋄
⋄
⋄
•
•
•
s
k
c
a
b
w
a
r
D
f
o
r
e
b
m
u
N
3
2
4
3
4
2
3
3
-
Table 1: Comparison between various previous works: Rows are var-
ious works in HPC-based malware detection and columns are de-
sign choices. The alternative shaded and white background repre-
sents diferent categories of tool/setup/model in malware detection
using HPCs. Red texts highlight drawbacks, and black texts express
the suggested tool/setup/model from this work. Solid dots (•) in-
dicate the use of that tool/setup/model (column) by the reference
(row), and hollow dimonds (⋄) indicate the non-use of that tool/se-
tup/model by the reference. Star (⋆) is our work. Our work avoids
the drawbacks discussed in the table, and quantitatively analyzes
how these drawbacks lead to the conclusion that HPCs can reliably
detect hardware.
the default use-case for current anti-malware suites. We denote the
drawbacks of DBI as Drawback I in Table 1.
While DBI is not feasible in online detection systems, other
methods in sampling HPCs can incur inaccurate measurements. A
plethora of previous works run the evaluated programs on VMs [3,
8ś11]. While VMs provide signiicant beneits to analyze unknown
programs (e.g., strong isolation guarantees), HPCs are limited and
shared resource between the host and all VMs. Thus, virtualizing
HPCs is a challenge in itself [16]. We conducted an experiment and
found out that none of the micro-architectural events that HPCs
monitor results in identical measurements in the VM and the host
machine. In our experiments, we measured HPC values of the SPEC
Benchmark Suite[17] 10 times both on a bare-metal machine (Intel
i7-6700 CPU with 8Gb RAM, Ubuntu 16.04 Linux, and VMWare
Workstation version 14.0.0 as the hypervisor), and subsequently in
a VM hosted on the same machine. We used 2 programs bzip and
hmmer from the SPEC Benchmark Suite as examples to show the
diference in measured HPC values from VM and bare-metal. We
used the perf [18] utility to sample HPC values at a maximum rate
of 100Hz in each case. We downsampled the measured HPC values
hmmer
bzip
]
%
[
s
t
n
e
v
E
f
o
e
l
i
t
n
e
c
r
e
P
100
75
50
25
0
0.0
0.2
0.4
0.6
0.8
1.0
Correlation Values
Figure 1: Correlation values between measured HPC values from
VM and bare-metal machine versus percentile of events. The plot
shows that none of the micro-architectural events has a correlation
value of 100% in hmmer and bzip.
from VM experiments to match the length of the sequence with
the measure HPC values from bare-metal experiments. We aver-
aged the measured HPC values to increase Signal-to-Noise Ratio
(SNR=mean/variation). We excluded the measured HPC values with
only zero values in both VM and bare-metal environment. Figure 1
shows the Cumulative Distribution Function (CDF) of correlation
values versus percentiles of events. The correlation values here re-
fer to the Pearson’s Correlation between the measured HPC values
gathered in VM and bare-metal environment. We observe that none
of the events has a correlation of 100% . No events have identical
HPC values, in fact, most of the events have low correlations. Thus,
the measured HPC values obtained in VM are substantially diferent
from HPC values obtained from the bare-metal environment that
real-life users have. To make matters worse, an evasive malware
can detect whether it is running in a VM and ceases to exhibit
malicious behavior (Kirat et al. [19]). These observations motivate
our experimental setup (ğ3) to run all experiments on bare-metal
systems. We label the use of VM in the experimental setups as
Drawback II in Table 1.
Due to inaccurate HPC measurements [20], previous works [3, 5ś
7] choose to maximize the measuring granularity by using HPCs
without time-multiplexing. Recall that as modern CPUs only have
6 (AMD) or 4 (Intel) registers for HPCs, malware detection meth-
ods must select the events from more than 100 available micro-
architectural events (130 in AMD Bulldozer and 196 in Intel Sky-
lake). Previous works [3, 5, 9ś11] have not provided a numerical
analysis on how micro-architectural events are selected. In our
experiments, we perform a Principal Component Analysis (PCA)
based approach to select the micro-architectural events. After the
selection of events, we use HPCs to track these events, and trans-
form the measured HPC values to examples in machine learning
models, i.e. feature extraction. We divide examples into training and
testing datasets for machine learning models (training-and-testing
split). Previous works [3, 5, 6, 8] have training-and-testing split
based on the examples (TTA1 in ğ 4.3) that the testing dataset can
have the same examples produced by programs in training dataset.
However, in real-life, it is unlikely that the oline training dataset
can include all the malware that a user might encounter. We mark
the use of data division based on examples as Drawback III in
Table 1.
In this work, we evaluate our model with 1,000 repetitions of 10-
fold cross-validations. The cross-validation examines the machine
learning models with diferent input training-and-testing exam-
ples, which prevent machine learning models from overitting3. We
observe that there is no cross-validation in some of the previous
works [3, 6, 7], while other works [8ś11] present insuicient cross-
validation, i.e. not every example in the dataset is validated. None of
these works report standard deviations of detection rates with cross-
validations. Without a substantial amount of cross-validation, we
cannot assert the reproducibility of detection rates, since a model
can have its high detection rates with speciic training and testing
datasets. We refer to no cross-validation or insuicient validations
as Drawback IV in Table 1.
The prevalence of the above-mentioned drawbacks motivates
us to perform rigorous, quantitative, and reproducible analytics
for HPC-based malware detection in Table 1. In order to perform
a fair comparison with works in Table 1, we use the following
machine learning models all used in previous works: Decision Tree
(DT), Random Forest (RF), K Nearest Neighbors (KNN), Neural Nets
(NN), Naive Bayes and AdaBoost. DT, RF, and KNN are designed to
identify outliers, which it the application of malware detection [21].
NN, NB, and AdaBoost (ensemble machine learning models) are
used in many previous works. We evaluate the detection rates of
all these machine learning models in our work and compare the
results with previous works.
Previous works reported their results with double decimal pre-
cision [3]. However, double decimal precision require at least 100
experiments in testing. With 10-fold cross-validation in the experi-
ments, the total number of programs (benignware and malware)
should be more than 1,000 programs. Thus, at least 1,000 programs
are required to evaluate the machine learning models within nu-
merical rounding error of less than 1%. As a result, we consider
the works with fewer than 1,000 programs as over-generalization
(training and testing with insuicient cross-validation), or over-
interpretation of the results (comparisons beyond rounding er-
rors) [3, 5ś8]. This insuicient number of programs in the experi-
ments is Drawback V in Table 1.
In addition to the drawbacks of the previous works, we found
that there is no public access to their data or codes, which presents a
direct comparison and examinations of the methods applied in these
works. To ease the reproducibility and advance the community’s
eforts to assess the utility of HPC-based malware detection, we
release all the code and data produced for this work under open-
source license.
We present all the tools/setups/models in various previous works
in Table 1. In Table 1, rows are various works in HPC-based malware
detection and columns are design choices of the tools/setups/models.
The alternative shaded and white background represents diferent
categories of tool/setup/model in malware detection using HPCs.
Red texts highlight drawbacks, and black texts express the sug-
gested tool/setup/model from this work. Solid dots (•) indicate the
use of that tool/setup/model (column) by the reference (row), and
hollow dimonds (⋄) indicate the non-use of that tool/setup/model
by the reference. Star (⋆) is our work. The last column counts the
drawbacks of the corresponding work. Table 1 shows that there are
at least 2 drawbacks in each work. Based on our work, we provide
3The model corresponds closely or exactly to a particular data and fail to predict other
data reliably.
3 guidelines for evaluating HPC-based malware detection in this
area. First, measurements of HPCs should be done in a bare-metal
environment, without VMs or any DBI. Second, building machine
learning models for malware detection requires the data division
by programs (TTA2 in ğ4.3) instead of division by traces (TTA1
in ğ4.3). Third, repeated cross-validations are required to prevent
overitting of machine learning models.
3 EXPERIMENTAL SETUP
In this section, we explain how we set up the experiments to gather
values of HPCs from benignware and malware. We ran our ex-
periments on a cluster with 15 machines as worker nodes, and a
master node to distribute jobs to measure and to collect data from
worker nodes. We dispatched our jobs to the worker nodes using
the Rabbitmq message system [22]. We collected the data back from
the worker nodes using a Samba [23] server on the master node.
We used Bindfs [24] to fuse the permission bits of Samba server
storage folder to be writable, not modiiable, not readable, and
not executable. Note that the Portable Operating System Interface
(POSIX) permission structure cannot provide the above-mentioned
permission bits. These permission bits allowed the worker nodes to
record the measured HPC values, while these permission settings
prevented malware from overwriting or deleting the measured HPC
values. On the worker nodes, we ran our experiments in Windows
7 32-bit operating system to be compatible with malware experi-
ments in other works [9ś11]. Previous works applied time-based
HPC sampling, i.e., they gathered values at a ixed sampling fre-
quency [3]. We used AMD CodeAnalyst APIs to build a time-based
HPC monitoring tool, Savitor, since AMD CodeAnalyst itself cannot
provide time-based measured HPC values [25].
3.1 Savitor (HPC measuring tool)
We designed Savitor, a tool that monitors a target process and
gathers HPC values related to the process. Savitor runs the target
process, pins the process to one core, reads the HPC values from that
core and then writes the measured HPCs values to iles on another
core, in order to reduce the noise during the sampling. Savitor
records 6 HPCs at a time, which is the maximum number of HPCs
that can be recorded on the AMD Bulldozer micro-architecture
without time-multiplexing. Savitor performs time-based sampling
and kills the target process at the end of each experiment. Following
the frequency limits in CodeAnalyst, we used the maximum possible
sampling frequency of 1 KHz for Savitor. Considering our limited
resources (time and hardware), we only ran each experiment of
both malware and benignware for 1 minute.
3.2 Malware and Benignware
For forming the set of malware, we downloaded 1,000 malware from
Virustotal [26], and performed a test run of those 1,000 malware on
worker nodes. After the test run, we identiied 962 malware which
could run for more than 1 minute and used them in our malware
experiments. According to AVClass tool [27], our dataset consisted
of 35 distinct malware families.
In order to collect benignware programs, we irst installed all the
packages and software from Futuremark [28], python performance
module [29], ninite.com [30], and Npackd [31] on the worker nodes.
Benignware that has a window
Run Monkey
Receive Job
Run Savitor
Run Program
(Malware/Benignware)
Reload Partition
Kill All Spawned Processes
Reset Environment
Malware
Benignware
Figure 2: Our worklow of benignware/malware experiments: The
worker node receives the dispatched jobs of experiments from the
master node. The worker node spawns a Savitor process, and then
Savitor runs the target process (benignware/malware). The dotted
arrow (d) means that the action does not always happen. If the ap-
plication has a window for interaction, we attach a monkey tester to
the window. The solid arrow (→) shows that actions always happen.
We reset the environment after each experiment. the worker node
kills any other processes spawned by the target process after each
benignware experiment. At the end of each malware experiment,
we reboot the machine into the Debian partition to reload a clean
Windows image.
After installation, we traversed all the iles in łStartup Menuž and
łC:\Program Filesž folder to include all the unique executable pro-
grams in our benignware dataset. We avoided the complication of
re-installation by excluding all the executable program iles with
łuninstallž in their names. We performed a test run of all these pro-
grams, and selected 1,382 benignware that could run for 1 minute.
To avoid the classiication bias, we matched the number of mal-
ware and benignware used in our experiments. Classiication bias
exists in classiication problems if the number of items in each
class is diferent. For example, in a classiication problem with two
classes, A and B, if class A makes up 80% of the data set and class B