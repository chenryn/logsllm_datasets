### Table 1: Comparison of Various Previous Works in HPC-Based Malware Detection

**Table Description:**
- Rows represent different works in HPC-based malware detection.
- Columns represent design choices.
- Alternating shaded and white backgrounds indicate different categories of tools, setups, or models used in malware detection with HPCs.
- Red text highlights drawbacks.
- Black text indicates the suggested tool, setup, or model from this work.
- Solid dots (•) indicate the use of a particular tool, setup, or model by the reference.
- Hollow diamonds (⋄) indicate the non-use of that tool, setup, or model by the reference.
- Star (⋆) denotes our work.

| Work | Design Choice 1 | Design Choice 2 | ... | Drawbacks |
|-------|-----------------|-----------------|-----|------------|
| ...   | ...             | ...             | ... | ...        |

### Introduction to HPC-Based Malware Detection

High-Performance Computing (HPC) is not the default use-case for current anti-malware suites. We denote the drawbacks of Dynamic Binary Instrumentation (DBI) as **Drawback I** in Table 1.

While DBI is not feasible for online detection systems, other methods of sampling HPCs can also lead to inaccurate measurements. Many previous works run evaluated programs on Virtual Machines (VMs) [3, 8–11]. Although VMs provide significant benefits such as strong isolation guarantees, HPCs are limited and shared resources between the host and all VMs, making virtualization a challenge [16].

### Experiment: Measuring HPC Values in VM vs. Bare-Metal

We conducted an experiment to measure HPC values of the SPEC Benchmark Suite [17] on both a bare-metal machine and a VM. The bare-metal machine used was an Intel i7-6700 CPU with 8GB RAM, running Ubuntu 16.04 Linux, and VMware Workstation version 14.0.0 as the hypervisor. We used two programs, `bzip` and `hmmer`, as examples to show the difference in measured HPC values.

- **Bare-Metal Setup:**
  - Machine: Intel i7-6700 CPU, 8GB RAM
  - OS: Ubuntu 16.04 Linux
  - Hypervisor: VMware Workstation 14.0.0

- **VM Setup:**
  - Hosted on the same machine as the bare-metal setup

We used the `perf` utility [18] to sample HPC values at a maximum rate of 100Hz. The measured HPC values from the VM were downsampled to match the length of the sequence from the bare-metal experiments. We averaged the measured HPC values to increase the Signal-to-Noise Ratio (SNR = mean/variation). We excluded HPC values that were zero in both environments.

**Results:**
- **Figure 1:** Correlation values between measured HPC values from VM and bare-metal machine versus percentile of events.
  - The plot shows that none of the micro-architectural events have a correlation value of 100% in `hmmer` and `bzip`.

### Drawbacks of Using VMs

The measured HPC values obtained in VMs are substantially different from those obtained in a bare-metal environment. Additionally, evasive malware can detect whether it is running in a VM and cease to exhibit malicious behavior [19]. These observations motivate our experimental setup to run all experiments on bare-metal systems. We label the use of VMs in experimental setups as **Drawback II** in Table 1.

### Inaccurate HPC Measurements and Event Selection

Due to inaccurate HPC measurements [20], previous works [3, 5–7] choose to maximize measuring granularity by using HPCs without time-multiplexing. Modern CPUs have only 6 (AMD) or 4 (Intel) registers for HPCs, and malware detection methods must select events from over 100 available micro-architectural events (130 in AMD Bulldozer and 196 in Intel Skylake).

- **Event Selection:**
  - Previous works [3, 5, 9–11] do not provide a numerical analysis on how micro-architectural events are selected.
  - In our experiments, we use Principal Component Analysis (PCA) to select the micro-architectural events.

After selecting the events, we use HPCs to track these events and transform the measured HPC values into features for machine learning models. We divide the examples into training and testing datasets. Previous works [3, 5, 6, 8] split the data based on examples (TTA1 in Section 4.3), which can result in the testing dataset having the same examples produced by programs in the training dataset. This is marked as **Drawback III** in Table 1.

### Cross-Validation and Reproducibility

In our work, we evaluate our model with 1,000 repetitions of 10-fold cross-validation. Cross-validation examines the machine learning models with different input training-and-testing examples, preventing overfitting. Some previous works [3, 6, 7] do not use cross-validation, while others [8–11] use insufficient cross-validation. None of these works report standard deviations of detection rates with cross-validation. We refer to no cross-validation or insufficient validation as **Drawback IV** in Table 1.

### Machine Learning Models and Data Division

To perform a fair comparison with the works in Table 1, we use the following machine learning models: Decision Tree (DT), Random Forest (RF), K Nearest Neighbors (KNN), Neural Networks (NN), Naive Bayes (NB), and AdaBoost. DT, RF, and KNN are designed to identify outliers, which is applicable to malware detection [21]. NN, NB, and AdaBoost (ensemble models) are used in many previous works.

- **Data Division:**
  - Previous works [3, 5–8] report results with double decimal precision, requiring at least 1,000 programs for evaluation.
  - We consider works with fewer than 1,000 programs as over-generalized or over-interpreted, marked as **Drawback V** in Table 1.

### Guidelines for Evaluating HPC-Based Malware Detection

Based on our work, we provide three guidelines for evaluating HPC-based malware detection:

1. **Bare-Metal Environment:**
   - HPC measurements should be done in a bare-metal environment, without VMs or any DBI.

2. **Data Division by Programs:**
   - Building machine learning models for malware detection requires data division by programs (TTA2 in Section 4.3) instead of by traces (TTA1 in Section 4.3).

3. **Repeated Cross-Validation:**
   - Repeated cross-validations are required to prevent overfitting of machine learning models.

### Experimental Setup

#### 3.1 Savitor (HPC Measuring Tool)

We designed Savitor, a tool that monitors a target process and gathers HPC values related to the process. Savitor runs the target process, pins it to one core, reads the HPC values from that core, and writes the measured HPC values to files on another core to reduce noise during sampling. Savitor records 6 HPCs at a time, the maximum number of HPCs that can be recorded on the AMD Bulldozer micro-architecture without time-multiplexing. Savitor performs time-based sampling and kills the target process at the end of each experiment. We used the maximum possible sampling frequency of 1 KHz, and each experiment ran for 1 minute.

#### 3.2 Malware and Benignware

- **Malware:**
  - Downloaded 1,000 malware samples from VirusTotal [26].
  - Identified 962 malware samples that could run for more than 1 minute.
  - Dataset consisted of 35 distinct malware families [27].

- **Benignware:**
  - Installed packages and software from Futuremark [28], Python performance module [29], ninite.com [30], and Npackd [31] on worker nodes.
  - Traversed all files in "Startup Menu" and "C:\Program Files" to include unique executable programs.
  - Excluded executables with "uninstall" in their names.
  - Selected 1,382 benignware samples that could run for 1 minute.

**Figure 2: Workflow of Benignware/Malware Experiments**

- **Worker Node:**
  - Receives dispatched jobs from the master node.
  - Spawns a Savitor process.
  - Runs the target process (malware/benignware).
  - Resets the environment after each experiment.
  - Kills any spawned processes after each benignware experiment.
  - Reboots the machine into the Debian partition to reload a clean Windows image after each malware experiment.

### Conclusion

Our work addresses several drawbacks in previous HPC-based malware detection studies. By conducting rigorous, quantitative, and reproducible analytics, we provide guidelines for future research. We release all code and data under an open-source license to facilitate reproducibility and advance the community's efforts in assessing the utility of HPC-based malware detection.