o
T
e
v
i
t
a
l
e
R
1.0
0.8
0.6
0.4
0.2
0.0
1 3
6
XBrainM
Deployment
9
12 15 18 21 24
Time (in month)
(a) Prediction recall and precision of XBrainM
in the production environment.
(b) Monthly DCNU rate before and after
XBrainM deployment. Y-axis is in relative scale
due to conﬁdential reasons.
(c) Monthly toal DCNU time before and after
XBrainM deployment. Y-axis is in relative scale
due to conﬁdential reasons.
Fig. 9: Online results.
overhead with 2500+ features is negligible.
E. Parameter Tuning and Leading Time
We next perform a set of experiments to evaluate the impact
of different parameters and show the leading time of the
successful predictions with our model.
Impact of decision threshold. For binary classiﬁcation, the
decision threshold decides the conﬁdence level of the alerts.
As shown in Fig. 8(a), a strict decision threshold is likely to
be associated with a low recall and a high precision and vice
versa. By carefully tuning the decision threshold, we can strike
a good balance between precision and recall. Fig. 8(a) shows
the variance of recall, precision, F1-score, and NURR with the
varying threshold. As shown in the ﬁgure, when we reach the
best NURR 63%, the recall and precision are 79% and 49%.
As to F1-score, the threshold 0.8 achieves the best F1-score
70% with 64% recall and 75% precision. Tuning the threshold
with F1-score is 8% worse NURR than tunning with NURR
directly. That is to say, a model tuned by NURR can reduce
8% more node unavailability when it is deployed online.
AUC (Area Under The Curve)-ROC (Receiver Operating
Characteristics) curve is one of the most popular evaluation
metrics for checking the performance of the classiﬁcation
model. It measures the quality of predictions irrespective of
what classiﬁcation threshold is chosen. As shown in Fig. 8(b),
our model is accurate and robust as the curve is close to the
upper left corner, and the percentage of AUC to 92.7%.
Impact of the prediction interval. We select prediction
interval from a set of candidates {5 minutes, 1 hour, 3 hours,
1 day}. Generally, a shorter prediction interval can reach a
higher recall, but it may involve too much system overhead.
On the other hand, a longer window brings less overhead, but
it fails to predict some failures promptly. Table VI shows the
prediction performance with different prediction intervals. As
we can see from the table, precision is relatively stable, but
recall keeps decreasing as the prediction interval increases. In
the list, a 5-minute prediction interval is most suited to DCNU
prediction in XBrainM.
Leading time. Fig. 8(c) shows the distribution of the leading
time of DCNUs which are correctly predicted. The X-axis
TABLE VI: Performances on different Prediction Interval.
Prediction Interval
5 minutes
1 hour
3 hours
1 day
Recall
79.34%
57.57%
46.69%
39.75%
Precision
48.65%
48.41%
48.13%
48.28%
F1
NURR
60.31% 63.03%
45.68%
52.59%
36.99%
47.40%
43.60%
31.51%
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:28:25 UTC from IEEE Xplore.  Restrictions apply. 
284
is the log-scale time-axis measured in minute, while Y-axis
represents the fraction of the predictions whose leading time
exceeds the value of the X-axis. As we can see from the
ﬁgure, more than 89% of all leading time is longer than 15
minutes, leaving sufﬁcient time for VM migration. Actually, in
our cloud, more than 95% of the leading time is sufﬁcient for
VM migration as our system specially optimizes the process
of live migration.
F. Online Results
XBrainM has been deployed in the production environment
of ECS system for over a year and signiﬁcantly improved the
overall service availability.
Online recall and precision. As described earlier, we use
stress testing to conﬁrm whether the predicted faulty nodes
will become unavailable or not. Then, we compute the online
recall and precision based on the stress testing results. As
shown in Fig. 9(a), XBrainM consistently achieved more than
60% monthly recall and precision in one year.
DCNU rate and total DCNU time. In the production
environment, DCNU rate is the percentage of nodes that
experienced DCNU among all nodes, and the total DCNU time
is used to evaluate the total unavailability caused by DRAM
faults. Fig. 9(b) and 9(c) illustrate the monthly DCNU rate
and monthly total DCNU time in one year before and after
XBrainM deployment. The Y-axis is in relative scale due to
business conﬁdential reasons. As shown in the ﬁgures, both
the DCNU rate and total DCNU time have a signiﬁcant (over
40% relative) reduction after XBrainM being deployed, which
indicates XBrainM effectively avoids a considerable number
of DCNUs. As to the yearly average, XBrainM signiﬁcantly
improves the availability and reliability of our cloud by
reducing 57% of DCNU rate and 69% of total DCNU time.
VIII. RELATED WORK
During the past years, many studies have been carried out on
failure prediction in real production systems. Li et al. [26], [27]
performed empirical and realistic evaluation of memory errors
on an Internet service farm. They observed different failures
on cells, rows, columns, and whole chips. Zivanovic et al. [45]
presented the memory failure characteristics from a large-scale
ﬁeld study in Google’s server ﬂeet. It was observed that the
probabilities of both CEs and UEs increase in the presence of
prior errors. Hwang et al. [21] presented an expanded study
on the error data collected from Google’s server ﬂeet and the
Blue Gene clusters. It was found that along those cells in the
band of nearby rows and columns, the probability of error
occurrence increases signiﬁcantly. Sridharan et al. [37], [38]
argued that DRAM failure rate is a better indicator of DIMM
health than error rate.
Dattatraya et al. [10] studied the memory error data for
more than 1 billion compute node-hours over 8 years. It was
observed that MCEs from memory and cache constitute a
large fraction of total hardware failures. Patwari et al. [34]
performed an investigation on spatial behaviors of DRAM
errors across an entire cluster. It was shown that some suscep-
tible regions are more vulnerable to errors than other regions.
Meza et al. [30] performed an investigation on memory errors
in the servers at Facebook over fourteen months. It was
observed that the hardware and software overheads to handle
such errors cause a kind of denial of service attack on some
servers. However, this work only quantiﬁes the importance
of the features, no prediction results are given. Du and Li
[13] proposed a method to predict DRAM errors in micro-
level components, such as cell rows and columns. The method
is based on a kernel function that measures the similarity
between the current observation and a certain previous ob-
servation in history.
Several studies proposed to use machine learning to predict
DRAM UEs before they occur [2], [16], [18]. They typically
train an ML model from historical data and then perform
predictions for further actions such as job migration to mitigate
the failure impact. Giurgiu et al. [18] processed the raw data
using feature selection, pre-branching, and data imputation,
ﬁnally built a sliding window classiﬁcation model with ran-
dom forests and achieved decent performance in experiments.
However, the best recall reported with the model is only 31%.
Boixaderas et al. [2] used random forest to predict DRAM
uncorrected errors in the MareNostrum3 supercomputer. In-
stead of using the traditional metrics such as precision, recall,
and F1-score, this work proposed the cost-aware prediction to
measure the system cost with a low precision (no more than
2%, our precision is much higher).
Recently, building AI solutions to solve system problems
has become a hot topic. A few studies proposed to enhance
system availability in cloud or HPC by predicting node failures
in advance [6], [9], [28], [29], [43]. They apply the most
popular machine learning models in their system, such as
LSTM, random forest, etc. Failure prediction on other im-
portant components of the computing nodes or systems were
also studied, such as disk failure prediction [17], [41], [42],
switch failures prediction [44] and GPU error prediction [31].
Differing from those studies, our task is to prevent DCNUs.
To mitigate the memory errors proactively, the software
solution of memory page ofﬂining was studied in [8], [14],
[21], [30], [39]. As a feature on Intel ICE Lake platform,
Partial Cache Line Sparing was studied in [15].
IX. CONCLUSION
In this paper, we proposed to predict DCNU relevant to UEs,
CE storms, and DIMM communication losses. We designed
novel spatio-temporal features based on the observation that
DCNUs have strong relevance to temporal statistics and spatial
patterns of CEs, to train a ML model. We then developed
XBrainM, which ensembles the ML model and traditional
rule-based approaches to predict DCNUs. Ofﬂine results show
that our prediction approach achieves over 40% more node
unavailability reduction rate than existing methods. Online
results in the production environment show that XBrainM
signiﬁcantly reduces 57% of DCNU rate and 69% of total
DCNU time.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:28:25 UTC from IEEE Xplore.  Restrictions apply. 
285
REFERENCES
[1] Further memory fault modeling. https://link.springer.com/content/pdf/
bbm%3A978-0-306-47972-4%2F1.pdf.
[2] Isaac Boixaderas, Darko Zivanovic, Sergi Mor´e, Javier Bartolome, David
Vicente, Marc Casas, Paul M Carpenter, Petar Radojkovi´c, and Eduard
Ayguad´e. Cost-aware prediction of uncorrected dram errors in the ﬁeld.
In SC, pages 1–15, 2020.
[3] Leo Breiman. Random forests. Machine learning, 45(1):5–32, 2001.
[4] Paris Carbone, Asterios Katsifodimos, Stephan Ewen, Volker Markl, Seif
Haridi, and Kostas Tzoumas. Apache ﬂink: Stream and batch processing
in a single engine. Bulletin of the IEEE Computer Society Technical
Committee on Data Engineering, 36(4), 2015.
[5] Tianqi Chen, Tong He, Michael Benesty, Vadim Khotilovich, Yuan Tang,
Hyunsu Cho, Kailong Chen, et al. Xgboost: extreme gradient boosting.
R package version 0.4-2, 1(4):1–4, 2015.
[6] Yujun Chen, Xian Yang, Qingwei Lin, Hongyu Zhang, Feng Gao,
Zhangwei Xu, Yingnong Dang, Dongmei Zhang, Hang Dong, Yong Xu,
et al. Outage prediction and diagnosis for cloud service systems.
In
WWW, pages 2659–2665, 2019.
[7] Alibaba Cloud. Elastic compute service (ecs) service level agreement,
2021. https://www.alibabacloud.com/help/en/doc-detail/42436.htm.
[8] Carlos HA Costa, Yoonho Park, Bryan S Rosenburg, Chen-Yong Cher,
and Kyung Dong Ryu. A system software approach to proactive
memory-error avoidance. In SC, pages 707–718, 2014.
[9] Anwesha Das, Frank Mueller, and Barry Rountree. Aarohi: Making
real-time node failure prediction feasible. In IPDPS, pages 1092–1101,
2020.
[10] Harish Dattatraya Dixit, Fan Lin, Bill Holland, Matt Beadon, Zhengyu
Yang, and Sriram Sankar. Optimizing interrupt handling performance
for memory failures in large scale data centers. In ICPE, pages 193–201,
2020.
[11] Timothy J Dell. A white paper on the beneﬁts of chipkill-correct ecc
IBM Microelectronics division, 11:1–23,
for pc server main memory.
1997.
[12] Catello Di Martino, Zbigniew Kalbarczyk, Ravishankar K Iyer, Fabio
Baccanico, Joseph Fullop, and William Kramer. Lessons learned from
the analysis of system failures at petascale: The case of blue waters. In
DSN, pages 610–621, 2014.
[13] Xiaoming Du and Cong Li. Memory failure prediction using online
learning. In MEMSYS, pages 38–49, 2018.
[14] Xiaoming Du and Cong Li. Combining error statistics with failure
prediction in memory page ofﬂining. In MEMSYS, pages 127–132, 2019.
[15] Xiaoming Du and Cong Li. Dpcls: Improving partial cache line sparing
with dynamics for memory error prevention. In ICCD, pages 197–204,
2020.
[16] Xiaoming Du, Cong Li, Shen Zhou, Mao Ye, and Jing Li. Predicting
uncorrectable memory errors for proactive replacement: An empirical
study on large-scale ﬁeld data.
In European Dependable Computing
Conference, pages 41–46, 2020.
[17] Sandipan Ganguly, Ashish Consul, Ali Khan, Brian Bussone, Jacque-
line Richards, and Alejandro Miguel. A practical approach to hard
disk failure prediction in cloud platforms: Big data model for failure
management in datacenters. In International Conference on Big Data
Computing Service and Applications, pages 105–116, 2016.
[18] Ioana Giurgiu, Jacint Szabo, Dorothea Wiesmann, and John Bird.
In
Predicting dram reliability in the ﬁeld with machine learning.
Middleware, pages 15–21, 2017.
[19] Baptiste Gregorutti, Bertrand Michel, and Philippe Saint-Pierre. Cor-
Statistics and
relation and variable importance in random forests.
Computing, 27(3):659–678, 2017.
[20] Saurabh Gupta, Tirthak Patel, Christian Engelmann, and Devesh Tiwari.
Failures in large scale systems: long-term measurement, analysis, and
implications. In SC, pages 1–12, 2017.
[21] Andy A Hwang, Ioan A Stefanovici, and Bianca Schroeder. Cosmic
rays don’t strike twice: Understanding the nature of dram errors and the
implications for system design. ASPLOS, 47(4):111–122, 2012.
[22] IDG.
2020 cloud computing survey, 2020.
https://www.idg.com/
tools-for-marketers/2020-cloud-computing-study/.
intel
[23] Intel.
data
2002.
e7500-chipset-mch-x4-single-device-data-correction-note.pdf.
device
validation,
https://www.intel.com/content/dam/doc/application-note/
chipset mch
sddc)
Intel
correction
implementation
e7500
(x4
single
and
x4
[24] Scott Levy, Kurt B Ferreira, Nathan DeBardeleben, Taniya Siddiqua,
Vilas Sridharan, and Elisabeth Baseman. Lessons learned from memory
errors observed over the lifetime of cielo. In SC, pages 554–565, 2018.
[25] Sebastien Levy, Randolph Yao, Youjiang Wu, Yingnong Dang, Peng
Huang, Zheng Mu, Pu Zhao, Tarun Ramani, Naga Govindaraju, Xukun
Li, et al. Predictive and adaptive failure mitigation to avert production
cloud vm interruptions. In OSDI, pages 1155–1170, 2020.
[26] Xin Li, Michael C Huang, Kai Shen, and Lingkun Chu. An empirical
study of memory hardware errors in a server farm. In HotDep, 2007.
[27] Xin Li, Michael C Huang, Kai Shen, and Lingkun Chu. A realistic eval-
uation of memory hardware errors and software system susceptibility.
In ATC, pages 75–88, 2010.
[28] Yangguang Li, Zhen Ming Jiang, Heng Li, Ahmed E Hassan, Cheng He,
Ruirui Huang, Zhengda Zeng, Mian Wang, and Pinan Chen. Predicting
node failures in an ultra-large-scale cloud computing platform: an aiops
solution. ACM Transactions on Software Engineering and Methodology,
29(2):1–24, 2020.
[29] Qingwei Lin, Ken Hsieh, Yingnong Dang, Hongyu Zhang, Kaixin Sui,
Yong Xu, Jian-Guang Lou, Chenggang Li, Youjiang Wu, Randolph Yao,
et al. Predicting node failure in cloud service systems. In ESEC/FSE,
pages 480–490, 2018.
[30] Justin Meza, Qiang Wu, Sanjeev Kumar, and Onur Mutlu. Revisiting
memory errors in large-scale production data centers: Analysis and
modeling of new trends from the ﬁeld. In DSN, pages 415–426, 2015.
[31] Bin Nie, Ji Xue, Saurabh Gupta, Tirthak Patel, Christian Engelmann,
Evgenia Smirni, and Devesh Tiwari. Machine learning models for gpu
error prediction in a large scale hpc system.
In DSN, pages 95–106,
2018.
[32] William S Noble. What is a support vector machine? Nature biotech-
nology, 24(12):1565–1567, 2006.
[33] Muddapu Parvathi, N Vasantha, and KSatya Parasad. Modiﬁed march
International Journal of
c-algorithm for embedded memory testing.
Electrical and Computer Engineering, 2(5):571, 2012.
[34] Ayush Patwari, Ignacio Laguna, Martin Schulz, and Saurabh Bagchi.
Understanding the spatial characteristics of dram errors in hpc clusters.
In ACM Workshop on Fault-Tolerance for HPC at Extreme Scale, pages
17–22, 2017.
[35] Bianca Schroeder and Garth A Gibson. A large-scale study of fail-
IEEE Transactions on
ures in high-performance computing systems.
Dependable and Secure Computing, 7(4):337–350, 2009.
[36] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian
Advances in neural
optimization of machine learning algorithms.
information processing systems, 25, 2012.
[37] Vilas Sridharan, Nathan DeBardeleben, Sean Blanchard, Kurt B Ferreira,
Jon Stearley, John Shalf, and Sudhanva Gurumurthi. Memory errors in
modern systems: The good, the bad, and the ugly. ASPLOS, 43(1):297–
310, 2015.
[38] Vilas Sridharan and Dean Liberty. A study of dram failures in the ﬁeld.
In SC, pages 1–11, 2012.
[39] Dong Tang, Peter Carruthers, Zuheir Totari, and Michael W Shapiro.
Assessment of the effect of memory page retirement on system ras
against hardware faults. In DSN, pages 365–370, 2006.
[40] Raymond E Wright. Logistic regression. 1995.
[41] Jiang Xiao, Zhuang Xiong, Song Wu, Yusheng Yi, Hai Jin, and Kan
Hu. Disk failure prediction in data centers via online learning. In ICPP,
pages 1–10, 2018.
[42] Yong Xu, Kaixin Sui, Randolph Yao, Hongyu Zhang, Qingwei Lin,
Yingnong Dang, Peng Li, Keceng Jiang, Wenchi Zhang, Jian-Guang
Lou, et al. Improving service availability of cloud systems by predicting
disk error. In ATC, pages 481–494, 2018.
[43] Yang Yang, Jing Dong, Chao Fang, Ping Xie, and Na An. Fp-ste: A
novel node failure prediction method based on spatio-temporal feature
extraction in data centers. Computer Modeling in Engineering &
Sciences, 123(3):1015–1031, 2020.
[44] Shenglin Zhang, Ying Liu, Weibin Meng, Zhiling Luo, Jiahao Bu, Sen
Yang, Peixian Liang, Dan Pei, Jun Xu, Yuzhi Zhang, et al. Preﬁx: Switch
failure prediction in datacenter networks. Proceedings of the ACM on
Measurement and Analysis of Computing System, 2(1):1–29, 2018.
[45] Darko Zivanovic, Pouya Esmaili Dokht, Sergi Mor´e, Javier Bartolome,
Paul M Carpenter, Petar Radojkovi´c, and Eduard Ayguad´e. Dram errors
in the ﬁeld: a statistical approach. In MEMSYS, pages 69–84, 2019.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:28:25 UTC from IEEE Xplore.  Restrictions apply. 
286