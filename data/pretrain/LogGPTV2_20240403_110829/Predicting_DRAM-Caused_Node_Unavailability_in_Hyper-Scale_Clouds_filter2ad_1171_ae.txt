### Online Results

XBrainM has been deployed in the production environment of the ECS system for over a year and has significantly improved overall service availability.

#### Online Recall and Precision
To confirm whether the predicted faulty nodes will become unavailable, we use stress testing. Based on the results of these tests, we compute the online recall and precision. As shown in Fig. 9(a), XBrainM consistently achieved more than 60% monthly recall and precision over the course of one year.

#### DCNU Rate and Total DCNU Time
In the production environment, the DCNU rate is defined as the percentage of nodes that experienced DCNU among all nodes, and the total DCNU time is used to evaluate the total unavailability caused by DRAM faults. Figures 9(b) and 9(c) illustrate the monthly DCNU rate and monthly total DCNU time before and after XBrainM deployment. The Y-axis is in relative scale due to business confidentiality. Both the DCNU rate and total DCNU time show a significant reduction (over 40% relative) after XBrainM was deployed, indicating that XBrainM effectively avoids a considerable number of DCNUs. On average, XBrainM reduces the DCNU rate by 57% and the total DCNU time by 69% annually, thereby significantly improving the availability and reliability of our cloud.

### Parameter Tuning and Leading Time

We conducted a series of experiments to evaluate the impact of different parameters and to demonstrate the leading time of successful predictions with our model.

#### Impact of Decision Threshold
For binary classification, the decision threshold determines the confidence level of the alerts. As shown in Fig. 8(a), a strict decision threshold is typically associated with low recall and high precision, and vice versa. By carefully tuning the decision threshold, we can achieve a good balance between precision and recall. Figure 8(a) illustrates the variance of recall, precision, F1-score, and NURR with varying thresholds. When we reach the best NURR of 63%, the recall and precision are 79% and 49%, respectively. For the F1-score, a threshold of 0.8 achieves the best F1-score of 70% with 64% recall and 75% precision. Tuning the threshold with F1-score results in an 8% worse NURR compared to tuning with NURR directly. This means that a model tuned by NURR can reduce 8% more node unavailability when deployed online.

#### AUC-ROC Curve
The AUC-ROC curve is a widely used evaluation metric for assessing the performance of classification models. It measures the quality of predictions irrespective of the chosen classification threshold. As shown in Fig. 8(b), our model is accurate and robust, with an AUC of 92.7%.

#### Impact of Prediction Interval
We selected prediction intervals from a set of candidates: 5 minutes, 1 hour, 3 hours, and 1 day. Generally, a shorter prediction interval can achieve higher recall but may involve more system overhead. Conversely, a longer window reduces overhead but may fail to predict some failures promptly. Table VI shows the prediction performance with different prediction intervals. Precision remains relatively stable, while recall decreases as the prediction interval increases. A 5-minute prediction interval is most suitable for DCNU prediction in XBrainM.

| Prediction Interval | Recall | Precision | F1 | NURR |
|---------------------|--------|-----------|----|------|
| 5 minutes           | 79.34% | 48.65%    | 60.31% | 63.03% |
| 1 hour              | 57.57% | 48.41%    | 45.68% | 52.59% |
| 3 hours             | 46.69% | 48.13%    | 36.99% | 47.40% |
| 1 day               | 39.75% | 48.28%    | 43.60% | 31.51% |

#### Leading Time
Figure 8(c) shows the distribution of the leading time of DCNUs that were correctly predicted. The X-axis is a log-scale time-axis measured in minutes, while the Y-axis represents the fraction of predictions whose leading time exceeds the value on the X-axis. More than 89% of all leading times are longer than 15 minutes, providing sufficient time for VM migration. In our cloud, more than 95% of the leading times are sufficient for VM migration, thanks to our system's optimized live migration process.

### Related Work

Over the past years, numerous studies have been conducted on failure prediction in real production systems. Li et al. [26], [27] performed empirical and realistic evaluations of memory errors in an Internet service farm, observing different types of failures. Zivanovic et al. [45] presented the characteristics of memory failures from a large-scale field study in Google’s server fleet, noting that the probabilities of both CEs and UEs increase in the presence of prior errors. Hwang et al. [21] expanded on error data collected from Google’s server fleet and the Blue Gene clusters, finding that the probability of error occurrence increases significantly along cells in the band of nearby rows and columns. Sridharan et al. [37], [38] argued that the DRAM failure rate is a better indicator of DIMM health than the error rate.

Dattatraya et al. [10] studied memory error data for over 1 billion compute node-hours over 8 years, observing that MCEs from memory and cache constitute a large fraction of total hardware failures. Patwari et al. [34] investigated the spatial behaviors of DRAM errors across an entire cluster, showing that some regions are more susceptible to errors. Meza et al. [30] studied memory errors in Facebook’s servers over fourteen months, observing that the hardware and software overheads to handle such errors can cause a denial of service attack on some servers. Du and Li [13] proposed a method to predict DRAM errors in micro-level components using a kernel function to measure similarity.

Several studies have proposed using machine learning to predict DRAM UEs before they occur [2], [16], [18]. They typically train ML models from historical data and perform predictions for actions like job migration to mitigate failure impacts. Giurgiu et al. [18] used feature selection, pre-branching, and data imputation to build a sliding window classification model with random forests, achieving decent performance. However, the best recall reported with the model is only 31%. Boixaderas et al. [2] used random forest to predict DRAM uncorrected errors in the MareNostrum3 supercomputer, proposing a cost-aware prediction to measure system costs with low precision.

Recently, building AI solutions to solve system problems has become a hot topic. Studies have proposed enhancing system availability in cloud or HPC by predicting node failures in advance [6], [9], [28], [29], [43]. These studies apply popular ML models such as LSTM and random forest. Failure prediction on other important components, such as disk failure prediction [17], [41], [42], switch failures prediction [44], and GPU error prediction [31], have also been studied. Our task, however, is to prevent DCNUs. To proactively mitigate memory errors, the software solution of memory page offlining has been studied [8], [14], [21], [30], [39]. Additionally, Partial Cache Line Sparing, a feature on Intel ICE Lake platforms, has been studied [15].

### Conclusion

In this paper, we proposed to predict DCNUs related to UEs, CE storms, and DIMM communication losses. We designed novel spatio-temporal features based on the observation that DCNUs have strong relevance to temporal statistics and spatial patterns of CEs, to train an ML model. We then developed XBrainM, which ensembles the ML model and traditional rule-based approaches to predict DCNUs. Offline results show that our prediction approach achieves over 40% more node unavailability reduction rate than existing methods. Online results in the production environment show that XBrainM significantly reduces 57% of the DCNU rate and 69% of the total DCNU time.