t
n
e
t
u
p
h
g
u
o
r
h
T
i
l
c
(
 50
 40
 30
 20
 10
 0
 0.0001
 0.01
 1
 100
 10000
Database table width-height ratio
Fig. 4: Use of bandwidth-efﬁcient DPFs gives a 32× speed-up
over the naïve constructions, in which a client’s request is as
large as the database.
10GB
1GB
100MB
10MB
1MB
100kB
10kB
1kB
)
s
e
t
y
b
(
r
e
f
s
n
a
r
t
t
a
a
D
100 B
10 
No DPF
Server - Recv
Server - Send
Audit - Recv
Audit - Send
1k
1M
10M 100M
100 
Database table size (# of 160-byte rows)
100k
10k
Fig. 5: The total client and server data transfer scales sub-
linearly with the size of the database.
DPF keys to the length of the database table. The dashed
line in Figure 3 indicates this upper bound (605 MB/s), as
determined using an AES benchmark written in Go. That line
indicates the maximum possible throughput we could hope to
achieve without aggressive optimization (e.g., writing portions
of the code in assembly) or more powerful machines. Migrat-
ing the performance-critical portions of our implementation
from Go to C (using OpenSSL) might increase the throughput
by a factor of as much as 6×, since openssl speed reports
AES throughput of 3.9 GB/s, compared with the 605 MB/s we
obtain with Go’s crypto library. At very small table sizes, the
speed at which the server can set up TLS connections with the
clients limits the overall throughput to roughly 900 requests
per second.
Figure 4 demonstrates how the request throughput varies as
the width of the table changes, while the number of bytes in
the table is held constant at 10 MB. This ﬁgure demonstrates
√
the performance advantage of using a bandwidth-efﬁcient
O(
L) DPF (Section IV) over the naïve DPF (Section III-A).
Using a DPF with optimal table size yields a throughput of
38.4 requests per second. The extreme left and right ends
of the ﬁgure indicate the performance yielded by the naïve
construction, in which making a write request involves sending
a (1 × L)-dimension vector to each server. At the far right
extreme of the table, performance drops to 0.05 requests per
second, so DPFs yield a 768× speed-up.
Figure 5 indicates the total number of bytes transferred by
one of the database servers and by the audit server while
processing a single client write request. The dashed line at
the top of the chart indicates the number of bytes a client
would need to send for a single write request if we did not
use bandwidth-efﬁcient DPFs (i.e., the dashed line indicates
the size of the database table). As the ﬁgure demonstrates,
the total data transfer in a Riposte cluster scales sub-linearly
with the database size. When the database table is 2.5 GB in
size, the database server transfers only a total of 1.23 MB to
process a write request.
B. s-Server Protocol
In some deployment scenarios, having strong protection
against server compromise may be more important than perfor-
mance or scalability. In these cases, the s-server Riposte pro-
tocol provides the same basic functionality as the three-server
protocol described above, except that it maintains privacy even
if s − 1 out of s servers collude or deviate arbitrarily from
the protocol speciﬁcation. We implemented the basic s-server
protocol but have not yet implemented the zero-knowledge
proofs necessary to prevent malicious clients from corrupting
the database state (Section V-B). These performance ﬁgures
thus represent an upper bound on the s-server protocol’s
performance. Adding the zero-knowledge proofs would require
L) elliptic curve operations per server in
an additional Θ(
an L-row database. The computational cost of the proofs
would almost certainly be dwarfed by the Θ(L) elliptic curve
operations required to update the state of the database table.
The experiments use the DDH-based seed-homomorphic
pseudo-random generator described in Section IV-D and they
use the NIST P-256 elliptic curve as the underlying algebraic
group. The table row size is ﬁxed at 160 bytes.
√
Figure 6 demonstrates the performance of an eight-server
Riposte cluster as the table size increases. At a table size of
1,024 rows, the cluster can process one request every 3.44
seconds. The limiting factor is the rate at which the servers
can evaluate the DDH-based pseudo-random generator (PRG),
since computing each 32-byte block of PRG output requires a
costly elliptic curve scalar multiplication. The dashed line in
the ﬁgure indicates the maximum throughput obtainable using
Go’s implementation of P-256 on our servers, which in turn
dictates the maximum cluster throughput. Processing a single
request with a table size of one million rows would take nearly
one hour with this construction, compared to 0.3 seconds in
the AES-based three-server protocol.
Figure 7 shows how the throughput of the Riposte cluster
changes as the number of servers varies. Since the workload
is heavily CPU-bound, the throughput only decreases slightly
as the number of servers increases from two to ten.
C. Discussion: Whistleblowing and Microblogging with
Million-User Anonymity Sets
Whistleblowers, political activists, or others discussing sen-
sitive or controversial issues might beneﬁt from an anonymous
microblogging service. A whistleblower, for example, might
333333
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:07:16 UTC from IEEE Xplore.  Restrictions apply. 
Actual throughput
Maximum EC throughput
 100
 10
 1
 0.1
)
c
e
s
/
s
t
s
e
u
q
e
r
t
n
e
t
u
p
h
g
u
o
r
h
T
i
l
c
(
 0.01
10 
100 
1k
10k
Database table size (# of 160-byte rows)
Fig. 6: Throughput of an eight-server Riposte cluster using the
(8, 7)-distributed point function.
)
c
e
s
/
s
t
s
e
u
q
e
r
t
t
u
p
h
g
u
o
r
h
T
n
e
i
l
c
(
 9
 8
 7
 6
 5
 4
 3
 2
 1
16-row table
64-row table
2 
3 
4 
5 
6 
7 
8 
9 
10 
Number of servers
Fig. 7: Throughput of Riposte clusters using two different
database table sizes as the number of servers varies.
want to anonymously blog about an instance of bureaucratic
corruption in her organization. The utility of such a system
depends on the size of the anonymity set it would provide:
if a whistleblower is only anonymous amongst a group of
ten people, it would be easy for the whistleblower’s employer
to retaliate against everyone in the anonymity set. Mounting
this “punish-them-all” attack does not require breaking the
anonymity system itself, since the anonymity set is public.
As the anonymity set size grows, however,
the feasibility
of the “punish-them-all” attack quickly tends to zero. At an
anonymity set size of 1,000,000 clients, mounting an “punish-
them-all” attack would be prohibitively expensive in most
situations.
Riposte can handle such large anonymity sets as long as (1)
clients are willing to tolerate hours of messaging latency, and
(2) only a small fraction of clients writes into the database in
each time epoch. Both of these requirements are satisﬁed in the
whistleblowing scenario. First, whistleblowers might not care
if the system delays their posts by a few hours. Second, the
vast majority of users of a microblogging service (especially
in the whistleblowing context) are more likely to read posts
than write them. To get very large anonymity sets, maintainers
of an anonymous microblogging service could take advantage
of the large set of “read-only” users to provide anonymity for
the relatively small number of “read-write” users.
The client application for such a microblogging service
would enable read-write users to generate and submit Riposte
write requests to a Riposte cluster running the microblogging
service. However, the client application would also allow read-
only users to submit an “empty” write request to the Riposte
cluster that would always write a random message into the
ﬁrst row of the Riposte database. From the perspective of the
servers, a read-only client would be indistinguishable from a
read-write client. By leveraging read-only users in this way,
we can increase the size of the anonymity set without needing
to increase the size of the database table.
To demonstrate that Riposte can support very large
anonymity set sizes—albeit with high latency—we conﬁgured
a cluster of Riposte servers with a 65,536-row database table
and left it running for 32 hours. In that period, the system
processed a total of 2,895,216 write requests at an average
rate of 25.19 requests per second. (To our knowledge, this is
the largest anonymity set ever constructed in a system that
offers protection against trafﬁc analysis attacks.) Using the
techniques in Section III-B, a table of this size could handle