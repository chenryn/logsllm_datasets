==== Lasso Regression
Lasso 详解见: 
Lasso 这些年研究较多，衍生出了一些其他的方法如 group lasso 等， 除了用在 Generialized linear 中，也会被用在时序预测 如ARMIA的定阶中，或者高维的feature selection里。
这里简单介绍下Lasso 最基本的应用numeric prediction。模型和LinearRegression一致，区别在于目标函数的不同:
image::images/ml-lasso-model-1.png[]
从公式可以看出相比于 Linear regression的目标函数，Lasso 多了一个L1 正则化项， 用于惩罚模型训练过程中过大的参数值，从这点也可以看出在Lasso的训练过程中，实际上是带有一定的特征选择能力的。
为什么要对参数进行正则化（即对较大参数值进行惩罚）， 中给出了一个很好的实验，实验中使用多项式回归，分别测试了当特征的指数从1-15时，结果的变化和参数值的变化，可以看出当特征的阶数较大时，训练数据出现了明显的过拟合情况，同时参数值也随着模型复杂度的变化 有着指数倍的提升，反过来，指数过大的坏处是？过拟合。
训练过程和Linear Regression 相同，优化目标函数`(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1`，  X为样本特征，y为样本实际目标变量值，w为参数。
Lasso 和 Ridge 都属于 Generialized Linear Model中Elastic net的一类。
Syntax: 
    fit Lasso [params set]  from  [into model_name]
Parameters:
* : model 中的 y 目标变量名，必选
*  : model中的x , 特征列表， 个数必须大于等于1
* [into model_name]: fit后模型保存名字， 可选， 可用于后续apply命令
* [params set]: 算法相关参数，可选
** alpha : float, optional
+
Constant that multiplies the L1 term. Defaults to 1.0. alpha = 0 is equivalent to an ordinary least square, solved by the LinearRegression object. For numerical reasons, using alpha = 0 with the Lasso object is not advised. Given this, you should use the LinearRegression object.
** fit_intercept : boolean
+
whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered).
** normalize : boolean, optional, default False
+
If True, the regressors X will be normalized before regression. This parameter is ignored when fit_intercept is set to False. When the regressors are normalized, note that this makes the hyperparameters learnt more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use preprocessing.StandardScaler before calling fit on an estimator with normalize=False.
** max_iter : int, optional
+
The maximum number of iterations
** tol : float, optional
+
The tolerance for the optimization: if the updates are smaller than tol, the optimization code checks the dual gap for optimality and continues until it is smaller than tol.
** warm_start : bool, optional
+
When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution.
** positive : bool, optional
+
When set to True, forces the coefficients to be positive.
** selection : str, default ‘cyclic’
+
If set to ‘random’, a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to ‘random’) often leads to significantly faster convergence especially when tol is higher than 1e-4.
** random_state : int, RandomState instance, or None (default)
+
The seed of the pseudo random number generator that selects a random feature to update. Useful only when selection is set to ‘random’.
对同样的波士顿放假数据我们尝试用Lasso进行训练，并和LinearRegression效果进行对比, 因为lasso 算法本身具有特征选择的能力，所以这边我们会多产生些高阶的特征
  tag:housing | limit 400 | where !empty(json.MEDV) |eval square_rm = json.RM * json.RM | eval triple_rm = json.RM * json.RM * json.RM | eval square_tax = json.TAX * json.TAX | eval triple_tax = json.TAX * json.TAX * json.TAX | eval square_b = json.B * json.B | eval triple_b = json.B * json.B * json.B |  fit Lasso json.MEDV from json.CRIM, json.ZN,  json.INDUS, json.CHAS, JSON.NOX, json.RM, json.AGE, json.DIS, json.RAD,json.TAX,json.PTRATIO,json.B,json.LSTAT,square_rm, triple_rm,square_tax,  triple_tax, square_b, triple_b into lasso_triple | rename json.MEDV as actual_price, predicted_json.MEDV as predicted_price | table actual_price, predicted_price 
对剩余数据进行apply
 tag:housing |eval time = tolong(context_id) | sort by +time | limit 100 | where !empty(json.MEDV) | eval square_rm = json.RM * json.RM | eval triple_rm = json.RM * json.RM * json.RM | eval square_tax = json.TAX * json.TAX | eval triple_tax = json.TAX * json.TAX * json.TAX | eval square_b = json.B * json.B | eval triple_b = json.B * json.B * json.B |  apply lasso_triple| eval residual = (predicted_json.MEDV-json.MEDV) *  (predicted_json.MEDV-json.MEDV) | stats count() as cnt , sum(residual) as sum_residual | eval mse = sum_residual/cnt
image::images/ml-lasso-example-mse.png[]
最终可以看到结果的mse是7.44。比之前的 LinearRegression 有了更大的提升。
==== Ridge
详解见: 
image::images/ml-ridge-model.png[]
Ridge 和 Lasso 十分的类似，主要区别在于 Ridge 使用L2 正则化而Lasso 使用L1 正则化， 介绍可参考 Lasso
Syntax: 
    fit Ridge [params set]  from  [into model_name]
Parameters:
* : model 中的 y 目标变量名，必选
*  : model中的x , 特征列表， 个数必须大于等于1
* [into model_name]: fit后模型保存名字， 可选， 可用于后续apply命令
* [params set]: 算法相关参数，可选
** alpha : {float, array-like}, shape (n_targets)
+
Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to C^-1 in other linear models such as LogisticRegression or LinearSVC. If an array is passed, penalties are assumed to be specific to the targets. Hence they must correspond in number.
** fit_intercept : boolean
+
Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered).
** max_iter : int, optional
+
Maximum number of iterations for conjugate gradient solver. For ‘sparse_cg’ and ‘lsqr’ solvers, the default value is determined by scipy.sparse.linalg. For ‘sag’ solver, the default value is 1000.
** normalize : boolean, optional, default False
+
If True, the regressors X will be normalized before regression. This parameter is ignored when fit_intercept is set to False. When the regressors are normalized, note that this makes the hyperparameters learnt more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use preprocessing.StandardScaler before calling fit on an estimator with normalize=False.
** solver : {‘auto’, ‘svd’, ‘cholesky’, ‘lsqr’, ‘sparse_cg’, ‘sag’}
+
Solver to use in the computational routines:
*** ‘auto’ chooses the solver automatically based on the type of data.
*** ‘svd’ uses a Singular Value Decomposition of X to compute the Ridge coefficients. More stable for singular matrices than ‘cholesky’.
*** ‘cholesky’ uses the standard scipy.linalg.solve function to obtain a closed-form solution.
*** ‘sparse_cg’ uses the conjugate gradient solver as found in scipy.sparse.linalg.cg. As an iterative algorithm, this solver is more appropriate than ‘cholesky’ for large-scale data (possibility to set tol and max_iter).
*** ‘lsqr’ uses the dedicated regularized least-squares routine scipy.sparse.linalg.lsqr. It is the fastest but may not be available in old scipy versions. It also uses an iterative procedure.
*** ‘sag’ uses a Stochastic Average Gradient descent. It also uses an iterative procedure, and is often faster than other solvers when both n_samples and n_features are large. Note that ‘sag’ fast convergence is only guaranteed on features with approximately the same scale. You can preprocess the data with a scaler from sklearn.preprocessing.
+
All last four solvers support both dense and sparse data. However, only ‘sag’ supports sparse input when fit_intercept is True.
** tol : float
+
Precision of the solution.
** random_state : int seed, RandomState instance, or None (default)
+
The seed of the pseudo random number generator to use when shuffling the data. Used only in ‘sag’ solver.
继续使用上面的房价预测数据,和Lasso采用同样的特征, mse 为8.4比lasso稍差，尝试对参数进行调整  
* alpha=0.05  mse 为8.1
* alpha=0.005 mse 为7.8
* alpha=0.001 mse 为7.76
考虑到alpha控制着惩罚的力度，值越小力度越小，因为我们本身的特征就很少，同事多项式特征最多也就3阶，所以overfitting的情况很少，所以惩罚力度没必要很大，不过目前看来，单纯靠控制参数并没法让算法有着很大的提升，这时候我们可以考虑对数据进行变化，如加入更多的特征，对数据进行标准化预处理等等。
==== ElasticNet
之前介绍过Lasso 是LinearRegression + L1 正则化，Ridge 是 LinearRegression + L2正则化。ElasticNet 则是 Linear regression with combined L1 and L2 priors as regularizer.
Syntax: 
    fit ElasticNet [params set]  from  [into model_name]
Parameters:
* : model 中的 y 目标变量名，必选
*  : model中的x , 特征列表， 个数必须大于等于1
* [into model_name]: fit后模型保存名字， 可选， 可用于后续apply命令
* [params set]: 算法相关参数，可选
** alpha : float, optional
+
Constant that multiplies the penalty terms. Defaults to 1.0. See the notes for the exact mathematical meaning of this parameter.``alpha = 0`` is equivalent to an ordinary least square, solved by the LinearRegression object. For numerical reasons, using alpha = 0 with the Lasso object is not advised. Given this, you should use the LinearRegression object.
** l1_ratio : float
+
The ElasticNet mixing parameter, with 0  from  [into model_name]
Parameters:
** kernel : string or callable, default=”linear”
+
Kernel mapping used internally. A callable should accept two arguments and the keyword arguments passed to this object as kernel_params, and should return a floating point number.
** gamma : float, default=None
+
Gamma parameter for the RBF, laplacian, polynomial, exponential chi2 and sigmoid kernels. Interpretation of the default value is left to the kernel; see the documentation for sklearn.metrics.pairwise. Ignored by other kernels.
** degree : float, default=3
+
Degree of the polynomial kernel. Ignored by other kernels.
** coef0 : float, default=1
+
Zero coefficient for polynomial and sigmoid kernels. Ignored by other kernels.