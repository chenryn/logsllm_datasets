)
c
e
s
/
c
e
s
/
s
g
e
s
(
r
e
w
o
P
 30
 25
 20
 15
 10
 5
 0
 0  10  20  30  40  50  60  70  80  90  100
Window (packets)
(c) RTT vs. window size. RTT was es-
sentially constant at small window sizes.
Above a window of 30, each additional
packet in TCP’s window was added to a
standing queue at the bottleneck, and the
RTT increased linearly with window size.
(d) Power vs. window size. Power reached
a maximum at the point where the bot-
tleneck crossed over from under-full (the
link had idle time) to over-full (there was
standing data in the queue), in some sense
the ideal TCP operating point.
Fig. 3. Plots of scan results
The results in the generated report are grouped hierarchically. The base of the
report shows test parameters and conditions. Test results are grouped into the
following categories: local host (client) conﬁguration, path measurements, and
tester (server) consistency checks. Path measurements are further divided into
data rate, loss rate, network buﬀering, duplex mismatch tests, and suggestions
for alternate test parameters.
Test results are labeled and color-coded for easy reading. All failing tests
(red) include a “corrective action” (starting with “>”) indicating what needs
to be ﬁxed and how to ﬁx it. In general, failing tests are guaranteed to be
performance show-stoppers - the application will fail to meet the target data
rate over the full end-to-end path as long as there are failing tests. The help for
Pathdiag: Automated TCP Diagnosis
157
Test conditions
Tester: (none) (192.88.115.171) [?]
Target: (none) (xxx.xxx.xxx.xxx) [?]
Logfile base name: xxxxx.xxxx.xxx:xxxx-xx-xx-xx:xx:xx [?]
This report is based on a 90 Mb/s target application data rate [?]
This report is based on a 20 ms Round-Trip-Time (RTT) to the target application [?]
The Round Trip Time for this path section is 2.518223 ms.
The Maximum Segment Size for this path section is 1448 Bytes. [?]
Target host TCP configuration test: Fail! [?]
Warning: TCP connection is not using SACK. [?]
Critical Failure: Received window scale is 2, it should be 3. [?]
The maximum receiver window (128k) is too small for this application (and/or some tests). [?]
Diagnosis: The target (client) is not properly configured. [?]
>  See TCP tuning instructions at http://www.psc.edu/networking/projects/tcptune/ [?]
Path measurements [?]
Data rate test: Pass! [?]
Pass data rate check: maximum data rate was 93.900110 Mb/s [?]
Loss rate test: Pass! [?]
Pass: measured loss rate 0.000848% (117889 packets between loss events). [?]
FYI: To get 90 Mb/s with a 1448 byte MSS on a 20 ms path the total end-to-end loss budget is 0.002029% (49275 packets between losses). [?]
Suggestions for alternate tests
FYI: This path may even pass with a more strenuous application: [?]
    Try rate=90 Mb/s, rtt=30 ms
    Try rate=93 Mb/s, rtt=29 ms
Or if you can raise the MTU: [?]
    Try rate=90 Mb/s, rtt=192 ms, mtu=9000 bytes
    Try rate=93 Mb/s, rtt=184 ms, mtu=9000 bytes
Network buffering test: Warning! [?]
This test did not complete due to other problems with the path, target or tester.
>  Correct other problems first, and then rerun this test. [?]
Estimated queue size is at least: Pkts: 64 Bytes: 92672 
This is probably an underestimate of the actual queue size. [?]
This corresponds to a 7.737751 ms drain time. [?]
To get 90 Mb/s with on a 20 ms path, you need 225000 bytes of buffer space. [?]
Tester validation: Pass! [?]
No internal tester problems were detected.
Tester version: $Id: xxxxx.xxxx.xxx:xxxx-xx-xx-xx:xx:xx.html,v 1.1 2007/08/06 18:40:24 mathis Exp $
Fig. 4. Sample report from the same data as Figure 3
passing tests (green) indicates any caveats about limitations of the tests. Tests
that are inconclusive for some reason yield orange warning messages. These
include ﬂaws that might not cause performance problems and tests that did not
complete due to other failing tests. Messages in black are informational and are
of most value to expert users. The analysis engine can detect 21 diﬀerent failure
conditions and 16 possible warnings.
Host Conﬁguration. The host conﬁguration tests conﬁrm that TCP settings
on the client are appropriate for target parameters. Pathdiag checks the options
negotiated on the SYN and SYN-ACK. The Window Scale option [13] must have
negotiated an appropriate value or it is ﬂagged as a critical failure. It also checks
if TCP Selective Acknowledgments (SACK) [18] or TCP Timestamps [13] are
enabled.
A key test is whether the TCP receive buﬀer is larger than the target window.
Since many modern operating systems adaptively size their receive buﬀers [9,
5, 23], it is necessary to check the announced receive window at extreme points
of the measurements (peak data rate or window size). There are several corner
cases that the analysis engine needs to consider. If the ﬂow is limited by the
receive window, and the maximum observed receive window for the entire run
is less than the target window then the receiver never announced enough buﬀer
space for the path, and the buﬀer is too small. If the receiver reduced its window
at the extreme points, then receiver is not fast enough, which is a diﬀerent
problem. Also, since the section of the path under test is normally shorter than
158
M. Mathis et al.
the target path, it might be correct for an adaptive receive window to have a
maximum size that is smaller than the target window. In this case, pathdiag
cannot make a strong conclusion about the receive buﬀer size. However, in most
default conﬁgurations, hosts that announce suﬃcient receiver window for the
queue space test and pass the window scale check will also have suﬃcient receive
buﬀer space.
Path Measurements. Pathdiag tests three parameters of the local path: max-
imum data rate, background loss rate, and bottleneck queue size. It also has a
special-case test for Ethernet duplex mismatch [21], which is only invoked if the
signature is detected.
Normally, the data rate test fails only if the tested path is not short enough,
the user is mistaken about the properties of the path, or there is a serious problem
such as a media-type negotiation failure. With a short RTT, TCP can overcome
most ﬂaws with only a minor performance reduction. As a consequence, most
ﬂaws do not mask other ﬂaws when the path is short enough, so a single test
run can detect multiple ﬂaws.
Pathdiag measures the background, non-congested loss rate at a window that
is slightly smaller than the window necessary to cause congestion on the link. A
failure is reported if the measured loss rate is greater than the rate calculated
by a TCP performance model [19] applied to the speciﬁed RTT and data rate
for the target application.
A warning is issued if the measured bottleneck queue buﬀer space is less than
the bandwidth-delay product of the target path. It is only a warning because
pathdiag cannot determine if the small buﬀer will cause signiﬁcantly reduced
performance for the target path. The test reﬂects the traditional full-BDP rule
for sizing router buﬀers for TCP [22]. Recent results show that smaller buﬀers
are adequate for aggregated ﬂows [1,8], but for single ﬂows there are some situ-
ations that might cause full window-sized bursts. In particular, TCP slow-start
naturally requires cwnd/2 buﬀer space at the bottleneck to avoid prematurely
transitioning to congestion avoidance [6]. Furthermore, if the bottleneck employs
active queue management (AQM) [2] such as RED [7], pathdiag is likely to mea-
sure the threshold for dropping packets rather than the actual buﬀer space used
to absorb bursts. We are planning future work in this area.
Tester Consistency Checks. Occasionally, either the traﬃc generator or path-
diag itself might be a bottleneck. For example, there may be unanticipated users
on the server. Pathdiag checks for this and other exceptional events, and reports
them as problems with the tester.
2.3 The Server Framework
The reports generated by pathdiag are ordinary web pages. They can be book-
marked and the URL forwarded to experts for additional analysis. The on-line
documentation stresses this feature [15]. Even relatively na¨ıve users can generate
diagnostic reports that clearly identify a problematic subsystem, and then for-
ward them to people with the resources and authority to take corrective action.
Pathdiag: Automated TCP Diagnosis
159
Web archival of pathdiag reports is also critical to our ongoing improvement of
the tool. We periodically scan various deployed diagnostic servers and retrieve the
reports from the analysis engine and the raw data from the measurement engine.
We inspect selected reports to conﬁrm they agree with our manual analysis
of the raw data. If we discover ﬂaws that are not reported clearly, we make
improvements to the analysis engine. We test the improved analysis engine by
reapplying it to our collection of measurement data, and inspect the re-generated
reports that diﬀer from the original reports. In this manner every user contributes
to our pool of test data, and to reﬁnement of the tool. Our archive currently
holds more than 7000 diagnostic reports.
The sever does not expose any private information about the user except
the name and IP address of the client machine. No user information or system
version information is explicitly exposed, though some operating systems may
be deduced by their performance properties.
3 Strengths and Weaknesses
Symptom scaling makes traditional tools currently used for network diagnostics,
such as ttcp and iperf, completely insensitive to ﬂaws on short paths. Network
experts use these tools over long paths to test for the existence of ﬂaws, but
actually locating the ﬂaws is often a diﬃcult trial and error process. As described
above, Pathdiag’s deﬁning characteristic is its ability to compensate for symptom
scaling. As such, it works best when run on short path sections, and is most useful
for debugging problems close to end systems.
Pathdiag fundamentally relies on active measurement3 and must send a sig-
niﬁcant amount of bulk data to measure the loss rate at the scale of the target
application. In this way it is diﬀerent than many bandwidth-estimation tools
that obtain results by measuring dispersion of short bursts of traﬃc without
sending suﬃcient traﬃc to measure the loss rate at a scale relevant to AIMD
congestion control.
The measurement algorithms used by pathdiag assume that other traﬃc across
the tested path is relatively unvarying. Though it will not result in a false pass,
highly variable levels of cross traﬃc may yield inconsistent results, especially in
measurements of bottleneck buﬀer size and measured throughput. This can be
largely mitigated by testing a shorter section of the path.
One fairly basic limitation is that the underlying diagnostic TCP stream is
unidirectional, and TCP is intrinsically diﬃcult to instrument from the receiving
end. There are a number of potential solutions to this, which we hope to address
in future work. Users can test the reverse path by running pathdiag at the other
end of the test path. This can be done most easily by using the Internet2 Network
Performance Toolkit [3] live boot CD to run a temporary server on almost any
PC.
3 For specialized uses, pathdiag can be run from the command line as a standalone
tool without the web server server framework. One special use is to manually attach
it to a bulk TCP stream belonging to another application.
160
M. Mathis et al.
Pathdiag cannot diagnose application problems, since the target application
does not participate in the testing process. It is often very diﬃcult to write
applications that can attain high data rates even on ideal long networks. Some
application problems are addressed in related work [10, 20].
4 Closing
Pathdiag is designed to improve TCP performance for the Research and Edu-
cation masses—those with a need for high performance but without the time
or expertise to individually diagnose network problems. It is particularly well
suited for testing at the edges of the network, which is usually where the ma-
jority of performance-reducing ﬂaws occur. Since it compensates for symptom
scaling, pathdiag is able to isolate these near-edge ﬂaws that are very diﬃcult to
diagnose using conventional local diagnostics.
In its most common form, deployment of pathdiag is fairly straightforward. A
single well-connected test server is in a position to provide coverage for an entire
campus or metropolitan network. It is our intent that pathdiag test servers will
ultimately yield signiﬁcant beneﬁts to both the users and administrators of high-
performance networks.
References
1. Appenzeller, G., Keslassy, I., McKeown, N.: Sizing router buﬀers. In: Proc. of ACM
SIGCOMM 2004, October 2004, pp. 281–292 (2004)
2. Braden, B., et al.: Recommendations on queue management and congestion avoid-
ance in the internet. In: RFC 2309 (April 1998)
3. Carlson, R.: Network performance toolkit,
http://e2epi.internet2.edu/network-performance-toolkit.html
4. Carpenter, B., Brim, S.: Middleboxes: Taxonomy and issues. In: RFC 3234 (Febru-
ary 2002)
5. Fisk, M., Feng, W.: Dynamic right-sizing is TCP. In: 2nd Annual Los Alamos
Computer Science Institute Symposium (LACSI 2001) (October 2001)
6. Floyd, S.: Limited slow-start for TCP with large congestion windows. In: RFC
3742 (March 2004)
7. Floyd, S., Jacobson, V.: Random early detection gateways for congestion avoidance.
IEEE ACM Transactions on Networking 1(4), 397–413 (1993)
8. Ganjali, Y., McKeown, N.: Update on buﬀer sizing in internet routers. ACM
9. Heﬀner, J.: High bandwidth TCP queuing,
CCR 36(4), 67–70 (2006)
http://www.psc.edu/∼jheffner/papers/senior thesis.pdf
10. Heﬀner, J., Mathis, M.: Applications and the speed of light: How well do applica-
tions perform on long perfect networks (2007), Web paper:
http://www.psc.edu/networking/projects/applight/
11. Internet2 Land Speed Record, http://www.internet2.edu/lsr/
12. Internet2 NetFlow Weekly Reports, http://netflow.internet2.edu/weekly/
13. Jacobson, V., Braden, B., Borman, D.: TCP extensions for high performance. In:
RFC 1323 (May 1992)
14. Mathis, M.: Windowed ping: an IP layer performance diagnostic. Computer Net-
Pathdiag: Automated TCP Diagnosis
161
works and ISDN Systems 27(3), 449–459 (1994)
15. Mathis, M., et al.: NPAD diagnostics servers: Automatic diagnostic server for trou-
bleshooting end-systems and last-mile network problems (2007), Web paper:
http://www.psc.edu/networking/projects/pathdiag/
16. Mathis, M., Heﬀner, J., Raghunarayan, R.: TCP extended statistics MIB. In: RFC
4898 (May 2007)
17. Mathis, M., Heﬀner, J., Reddy, R.: Web100: Extended TCP instrumentation for
research, education and diagnosis. Computer Communications Review 33(3), 69–79
(2003)
18. Mathis, M., Mahdavi, J., Floyd, S., Romanow, A.: TCP selective acknowledgement
options. In: RFC 2018 (October 1996)
19. Mathis, M., Semke, J., Mahdavi, J.: The macroscopic behavior of the TCP conges-
tion avoidance algorithm. Computer Communications Review 27(3), 67–82 (1997)
20. Rapier, C., Stevens, M.: High performance SSH/SCP - HPN-SSH (2007),
http://www.psc.edu/networking/projects/hpn-ssh/
21. Shalunov, S., Carlson, R.: Detecting duplex mismatch on ethernet. In: Dovrolis, C.
(ed.) PAM 2005. LNCS, vol. 3431, pp. 135–148. Springer, Heidelberg (2005)
22. Villamizar, C., Song, C.: High performance TCP in ANSNET. Computer Commu-
nications Review 24(5), 45–60 (1994)
23. New networking features in Windows Server 2008 and Windows Vista (2008),
http://technet.microsoft.com/en-us/library/bb726965.aspx