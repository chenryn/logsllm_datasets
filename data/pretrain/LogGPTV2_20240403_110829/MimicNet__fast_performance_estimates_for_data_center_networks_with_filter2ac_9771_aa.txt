title:MimicNet: fast performance estimates for data center networks with
machine learning
author:Qizhen Zhang and
Kelvin K. W. Ng and
Charles W. Kazer and
Shen Yan and
Jo√£o Sedoc and
Vincent Liu
MimicNet: Fast Performance Estimates for Data Center
Networks with Machine Learning
Qizhen Zhang, Kelvin K.W. Ng, Charles W. Kazer¬ß, Shen Yan‚Ä†, Jo√£o Sedoc‚ãÑ, and Vincent Liu
{qizhen,kelvinng,liuv}@seas.upenn.edu, ¬ßPI:EMAIL, ‚Ä†PI:EMAIL, ‚ãÑPI:EMAIL
University of Pennsylvania, ¬ßSwarthmore College, ‚Ä†Peking University, ‚ãÑNew York University
ABSTRACT
At-scale evaluation of new data center network innovations is
becoming increasingly intractable. This is true for testbeds, where
few, if any, can afford a dedicated, full-scale replica of a data center.
It is also true for simulations, which while originally designed
for precisely this purpose, have struggled to cope with the size of
today‚Äôs networks.
This paper presents an approach for quickly obtaining accurate
performance estimates for large data center networks. Our system,
MimicNet, provides users with the familiar abstraction of a packet-
level simulation for a portion of the network while leveraging
redundancy and recent advances in machine learning to quickly
and accurately approximate portions of the network that are not
directly visible. MimicNet can provide over two orders of magnitude
speedup compared to regular simulation for a data center with
thousands of servers. Even at this scale, MimicNet estimates of the
tail FCT, throughput, and RTT are within 5% of the true results.
CCS CONCEPTS
‚Ä¢ Networks ‚Üí Network simulations; Network performance mod-
eling; Network experimentation; ‚Ä¢ Computing methodologies ‚Üí
Massively parallel and high-performance simulations;
KEYWORDS
Network simulation, Data center networks, Approximation, Ma-
chine learning, Network modeling
ACM Reference Format:
Qizhen Zhang, Kelvin K.W. Ng, Charles W. Kazer¬ß, Shen Yan‚Ä†, Jo√£o Sedoc‚ãÑ,
and Vincent Liu. 2021. MimicNet: Fast Performance Estimates for Data Cen-
ter Networks with Machine Learning. In ACM SIGCOMM 2021 Conference
(SIGCOMM ‚Äô21), August 23‚Äì27, 2021, Virtual Event, USA. ACM, New York,
NY, USA, 18 pages. https://doi.org/10.1145/3452296.3472926
1 INTRODUCTION
Over the years, many novel protocols and systems have been pro-
posed to improve the performance of data center networks [5‚Äì
7, 12, 19, 33, 39]. Though innovative in their approaches and promis-
ing in their results, these proposals suffer from a consistent chal-
lenge: the difficulty of evaluating systems at scale. Networks, highly
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGCOMM ‚Äô21, August 23‚Äì27, 2021, Virtual Event, USA
¬© 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-8383-7/21/08...$15.00
https://doi.org/10.1145/3452296.3472926
287
Figure 1: Accuracy for MimicNet‚Äôs predictions of the FCT
distribution for a range of data center sizes. Accuracy is
quantified via the Wasserstein distance (W1) to the distri-
bution observed in the original simulation. Lower is better.
Also shown are the accuracy of a flow-level simulator (Sim-
Grid) and the accuracy of assuming a small (2-cluster) simu-
lation‚Äôs results are representative.
interconnected and filled with dependencies, are particularly chal-
lenging in that regard‚Äîsmall changes in one part of the network
can result in large performance effects in others.
Unfortunately, full-sized testbeds that could capture these effects
are prohibitively expensive to build and maintain. Instead, most pre-
production performance evaluation comprises orders of magnitude
fewer devices and fundamentally different network structures. This
is true for (1) hardware testbeds [47], which provide total control of
the system, but at a very high cost; (2) emulated testbeds [43, 54, 56],
which model the network but at the cost of scale or network effects;
and (3) small regions of the production network, which provide
‚Äòin vivo‚Äô accuracy but force operators to make a trade-off between
scale and safety [48, 59]. The end result is that, often, the only way
to ascertain the true performance of the system at scale is to deploy
it to the production network.
We note that simulation was originally intended to fill this gap.
In principle, simulations provide an approximation of network be-
havior for arbitrary architectures at an arbitrary scale. In practice,
however, modern simulators struggle to provide both simultane-
ously. As we show in this paper, even for relatively small networks,
packet-level simulation is 3‚Äì4 orders of magnitude slower than real-
time (5 min of simulated time every ‚àº3.2 days); larger networks
can easily take months or longer to simulate. Instead, researchers
often either settle for modestly sized simulations and assume that
performance translates to larger deployments, or they fall back to
approaches that ignore packet-level effects like flow approximation
techniques. Both sacrifice substantial accuracy.
In this paper, we describe MimicNet, a tool for fast performance
estimation of at-scale data center networks. MimicNet presents to
users the abstraction of a packet-level simulator; however, unlike
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.748163264128W1 to Ground TruthNetwork Size (#Clusters)Small-scaleFlow-levelMimicNetSIGCOMM ‚Äô21, August 23‚Äì27, 2021, Virtual Event, USA
Q. Zhang et al.
existing simulators, MimicNet only simulates‚Äîat a packet level‚Äî
the traffic to and from a single ‚Äòobservable‚Äô cluster, regardless of the
actual size of the data center. Users can then instrument the host and
network of the designated cluster to collect arbitrary statistics. For
the remaining clusters and traffic that are not directly observable,
MimicNet approximates their effects with the help of deep learning
models and flow approximation techniques.
Figure 2: OMNeT++ performance on leaf-spine topologies of
various size. Even for these small cases, 5 mins of simulation
time can take multiple days to process. Results were similar
for ns-3 and other simulation frameworks.
‚Ä¢ An architecture for composing Mimics into a generative model
of a full-scale data center network. For a set of complex protocols
and real-world traffic patterns, MimicNet can match ground-
truth results orders of magnitude more quickly than otherwise
possible. For large networks, MimicNet even outperforms flow-
level simulation in terms of speed (in addition to producing
much more accurate results).
‚Ä¢ A customizable hyperparameter tuning procedure and loss func-
tion design that ensure optimality in both generalization and a
set of arbitrary user-defined objectives.
‚Ä¢ Implementations and case studies of a wide variety of network
protocols that stress MimicNet in different ways.
The framework is available at: https://github.com/eniac/MimicNet.
2 MOTIVATION
Modern data center networks connect up to hundreds of thousands
of machines that, in aggregate, are capable of processing hundreds
of billions of packets per second. They achieve this via scale-out
network architectures, and in particular, FatTree networks like the
one in Figure 3 [4, 18, 50]. In the canonical version, the network
consists of Top-of-Rack (ToR), Cluster, and Core switches. We refer
to the components under a single ToR as a rack and the components
under and including a group of Cluster switches as a cluster. A large
data center might have over 100 such clusters.
The size and complexity of these networks make testing and
evaluating new ideas and architectures challenging. Researchers
have explored many potential directions including verification [15,
26, 27, 35, 57], emulation [52, 54, 56], phased rollouts [48, 59], and
runtime monitoring [20, 58]. In reality, all of these approaches have
their place in a deployment workflow; however, in this paper, we fo-
cus on a critical early step: pre-deployment performance estimation
using simulation.
2.1 Background on Network Simulation
The most popular simulation frameworks include OMNeT++ [34],
ns-3 [42], and OPNET [1]. Each of these operates at a packet-level
and are built around an event-driven model [53] in which the op-
erations of every component of the network are distilled into a
sequence of events that each fire at a designated ‚Äòsimulated time.‚Äô
288
As a preview of MimicNet‚Äôs evaluation results, Figure 1 shows the
accuracy of its Flow-Completion Time (FCT) predictions for various
data center sizes and compares it against two common alternatives:
(1) flow-level simulation and (2) running a smaller simulation and
assuming that the results are identical for larger deployments. For
each approach, we collected the simulated FCTs of all flows with
at least one endpoint in the observable cluster. We compared the
distribution of each approach‚Äôs FCTs to that of a full-fidelity packet-
level simulation using a ùëä1 metric. The topology and traffic pattern
were kept consistent, except in the case of small-scale simulation
where that was not possible (instead, we fixed the average load and
packet/flow size). While MimicNet is not and will never be a perfect
portrayal of the original simulation, it is 4.1√ó more accurate than
the other methods across network sizes, all while improving the
time to results by up to two orders of magnitude.
To achieve these results, MimicNet imposes a few carefully cho-
sen restrictions on the system being modeled: that the data center is
built on a classic FatTree topology, that per-host network demand is
predictable a priori, that congestion occurs primarily on fan-in, and
that a given host‚Äôs connections are independently managed. These
assumptions provide outsized benefits to simulator performance
and the scalability of its estimation accuracy, while still permitting
application to a broad class of data center networking proposals,
both at the end host and in the network.
Concretely, MimicNet operates as follows. First, it runs a sim-
ulation of a small subset of the larger data center network. Using
the generated data, it trains a Mimic‚Äîan approximation of clus-
ters‚Äô ‚Äònon-observable‚Äô internal and cross-cluster behavior. Then,
to predict the performance of an ùëÅ cluster simulation, it carefully
composes a single observable cluster with ùëÅ ‚àí 1 Mimic‚Äôed clusters
to form a packet-level generative model of a full-scale data center.
Assisting with the automation of this training process is a hyper-
parameter tuning stage that utilizes arbitrary user-defined metrics
(e.g., FCT, RTT, or average throughput) and MimicNet-defined met-
rics (e.g., scale generalizability) rather than traditional metrics like
L1/2 loss, which are a poor fit for a purely generative model.
This entire process‚Äîsmall-scale simulation, model training/tu-
ning, and full-scale approximation‚Äîcan be orders of magnitude
faster than running the full-scale simulation directly, with only
a modest loss of accuracy. For example, in a network of a thou-
sand hosts, MimicNet‚Äôs steps take 1h3m, 7h10m, and 25m, respec-
tively, while full simulation takes over a week for the same net-
work/workload. These results hold across a wide range of network
configurations and conditions extracted from the literature. This
paper contributes:
‚Ä¢ Techniques for the modeling of cluster behavior using deep-
learning techniques and flow-level approximation. Critical to
the design of the Mimic models are techniques to ensure the
scalability of their accuracy, i.e., their ability to generalize to
larger networks in a zero-shot fashion.
0.00010.0010.010.1010203040506070SimulationSeconds/Second#ofToRs/AggsSingle ThreadOne MachineTwo MachinesFour MachinesMimicNet: Fast Performance Estimates for DCNs with ML
SIGCOMM ‚Äô21, August 23‚Äì27, 2021, Virtual Event, USA
Compared to evaluation techniques such as testbeds and emulation,
these simulators provide a number of important advantages:
‚Ä¢ Arbitrary scale: Decoupling the system model from both hard-
ware and timing constraints means that, in principle, simula-
tions can encompass any number of devices.
‚Ä¢ Arbitrary extensions: Similarly, with full control over the simu-
lated behavior, users can model any protocol, topology, design,
or configuration.
‚Ä¢ Arbitrary instrumentation: Finally, simulation allows the collec-
tion of arbitrary information at arbitrary granularity without
impacting system behavior.
In return for the above benefits, simulators trade-off varying
levels of accuracy compared to a bare-metal deployment. Even so,
prior work has demonstrated their value in approximating real
behavior [5, 6, 33, 46, 55].
2.2 Scalability of Today‚Äôs Simulators
While packet-level simulation is easy to reason about and extend,
simulating large and complex networks is often prohibitively slow.
One reason for this is that discrete-event simulators, in essence,
take a massive distributed system and serialize it into a single event
queue. Thus, the larger the network, the worse the simulation
performs in comparison.
Parallelization. A natural approach to improving simulation speed
is parallelization, for instance, with the parallel DES (PDES) tech-
nique [17]. In PDES, the simulated network is partitioned into
multiple logical processes (LPs), where each process has its own
event queue that is executed in parallel. Eventually, of course, the
LPs must communicate. In particular, consistency demands that a
process cannot finish executing events at simulated time ùë° unless
it can be sure that no other process will send it additional events
with ùë°ùëí < ùë°. In these cases, synchronization may be necessary.
Parallel execution is therefore only efficient when the LPs can
run many events before synchronization is required, which is typi-
cally not the case for highly interconnected data center networks.
In fact, simulation performance often decreases in response to par-
allelization (see Figure 2). Many frameworks instead recommend
running several instances with different configurations [14]. This
trivially provides a proportional speedup to aggregate simulation