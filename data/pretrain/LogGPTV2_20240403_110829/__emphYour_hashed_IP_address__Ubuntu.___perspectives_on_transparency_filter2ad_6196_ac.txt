Thirteen companies provided information regarding inferred
segments and six companies chose not to provide this data, de-
spite the fact that the privacy policy mentions that segments are
inferred. Nine companies provided demographic information they
inferred from the users’ online profiles we analyzed. If the compa-
nies provided access to demographic information, most of them
shared the user’s location(s) or the inferred age. In general, users
do not think that companies trying to learn their age is a severe
privacy problem [41]. Two companies provided health-related in-
formation (e. g., Health & Fitness > Diets & Nutrition). No company
provided racial information, which is in line with most privacy
policies stating such information is not inferred.
While most companies state in their privacy policy that they
track users around the web, we only found six companies that list
the websites on which they tracked the user. Google only provided
the visits to websites the company controls (e. g., YouTube), while it
is known that they track users across tons of sites [20].
Legal Requirements. Two researchers—both with a strong un-
derstanding of the online ad ecosystem—were assigned the task to
classify if companies disclose “why” and “what” data is collected.
The final inter-rater agreement for classification shows substantial
agreement (Cohen’s κ = 0.77 for “why” and κ = 0.74 for “what”
data is collected; agreement > 90 % for both categories). In the rare
cases of discrepancies, a third person with a similar background
was consulted to resolve such cases. We found that five companies
do not disclose why and five companies do not disclose what they
collect data. It is worth noting that some companies only vaguely
explain why they collect data (e. g.,  “advertising
technology allows Business Partners to target advertisements to users
[...]”) or how this is actually implemented. In three cases neither
why nor what data is collected is given.
We computed the Flesch-Kincaid grade for all privacy policies.
The score suggests that on average users need 12.58 (with SD 1.21)
years of education (senior high school student in the US). Data
provided by the U.S. Census Bureau shows that 12 % of all adult US
citizens did not obtain a high school diploma [54].
Summary. The field of analyzed transparency tools is heteroge-
neous with regard to the type of data provided, the way of access,
and information about sharing activities. Some companies’ reports
lack of explanations of data collection and usage, and they do not
provide all data representations that they claim to have. As the
tools provided data in different forms and levels of granularity, it
is worth analyzing to which extent such data helps users to assess
the privacy implications of a company’s activities.
4 PERCEPTION OF TRANSPARENCY TOOLS
Previous work has focused on the transparency of targeted ads
themselves [43] or the accuracy of inferred interest segments [5, 42].
In this study, we try to get a better understanding of the users’
expectations and needs when it comes to transparency in online
advertisement.
4.1 Method
To get a better understanding of the users’ side of transparency
tools, we run an online survey which focuses on two aspects, First,
we wanted to understand to what extent users can identify who
is collecting their data because otherwise, they would not be able
to request it. Second, the ways companies provide access to data
differs from approaches studied in the past. Our goal was to under-
stand how different types of data disclosures found in the field help
participants understand the privacy implications of a company.
Our study focuses on the ability of users to understand the
provided data as it is the most important mechanism to provide
transparency, while other aspects like completeness or the compre-
hensibility of how certain information was inferred also play an
important role in the value of these tools.
Prior to the analysis of users’ perceptions (see Appendix A), we
asked questions about their attitude and understanding of online
advertisements. We also asked them about their general view and
usage of the tools to get access to personal data.
To test if users can identify which ad network is responsible for
an advertisement, we present two screenshots of websites, one of
which contained a standard ad banner(see Fig. 1a—top image in
Fig 1) and the other an advertisement with links to articles distin-
guished as “Recommendations” (see Fig. 1b—bottom image in Fig 1).
The ad banner contains a link to an opt-out program but does not
directly show the name of the third party. Users would have to
hover over the ad with the mouse and check the URL displayed in
the browser’s status bar to identify the ad network—we included
this URL in the screenshot but did not highlight it. The recommen-
dation contains a reference to the third party that generated the ad
(i. e., “Recommended by Outbrain”).
The remainder of the survey focuses on the users’ expectations
and understanding of personal data provided by ad companies upon
request. To assess this, we took screenshots of nine real-world
profiles that were provided to us upon request.
We grouped the nine profiles into three categories based on their
content: (1) technical data, (2) tracking data, and (3) segment data—
a definition of these categories can be found in Section 4.2. The
order of these categories was randomized for each participant. To
reduce the length of the survey, we did not differentiate between
segment and demographic data. The influence of this data on the
perception of online ads has been studied before [41]. Instead, we
differentiated between more abstract clickstream (tracking) and
detailed technical data. Our analysis of existing transparency tools
(see Section 3) showed that disclosing data in this form is common.
The disclosure of raw data is also related to the new right to data
706(a) Article recommendation including the company providing it (top
right corner)
participants if these profiles would help them to better assess the
privacy impact of the companies. In our survey, all questions (aside
from optional open-ended questions) provide a “I prefer not to an-
swer.” answer option. Following each section of categories, we asked
participants general questions regarding their personal views on
provided data and preferences which data representation and the
category they prefer.
In general, we used Pearson’s chi-squared test to test the inde-
pendence of two variables and the Pearson correlation coefficient
to determine a linear correlation between two variables. For both
tests, we used a significance level of α = 0.5. Furthermore, we
assigned the value 5 to the most positive answer in a Likert scale, 1
to the most negative answer option, and consequentially three to
the natural option (e. g., “Strongly Agree” = 5, “Undecided” = 3, and
“Strongly Disagree” = 1).
We conducted a pre-study (n = 50) with a similar survey struc-
ture as described above. In the pre-study, we focused on how users
might use data access to their benefit (i. e., how they would use the
provided information). However, due to usability problems, users
did not give useful feedback on this (e. g., P-6 stated:“No, this is
gibberish to me”). Thus, we dropped this question for the final study.
The full questionnaire is shown in Appendix A.
To recruit participants, we used Amazon’s Mechanical Turk
(MTurk) [3] and only accepted participants with high task comple-
tion rates (≥ 97 %) and permanent residents of the US. Furthermore,
we only accepted participants who were at least 18 years old and
asked for their consent to participate in the survey. In the intro-
duction of the survey, we disclosed our names, affiliations, and
all sponsors. We used a self-hosted LimeSurvey [34] instance to
conduct the survey. Participants received 2 USD for completing the
survey which took them around 15 minutes on average (median =
13 min). All answers were saved pseudonymously using the ran-
dom unique string, used by MTurk to pay the workers. After the
payment process with MTurk had been completed, we deleted the
identifier to increase participants’ anonymity level.
(b) Traditional ad banner
Figure 1: Article recommendation (top) and ad banner (bot-
tom (red frames) referenced in Q10 and Q11 (see Appen-
dix A). The article recommendation discloses the company
providing it.
portability and the perception of this form of data for transparency
has not yet been studied.
Each section of categories starts with a brief introduction to the
data shown, followed by three different examples of profiles. Par-
ticipants had to assess four statements regarding their understand-
ing and the presentation of the data. For most questions, we used
5-point Likert scales and, for the remainder, we used “Yes/No” ques-
tions and a prioritization question (see Appendix A). We provide
a 5-point Likert scale (ranging from “Strongly Agree” to “Strongly
Disagree”) and an “I prefer not to answer” answer option for each
statement. The order of the profiles is also randomized within each
category. At the end of each section of categories, we asked the
4.2 Results
The survey was conducted in February 2019 with 490 participants,
using Amazon’s Mechanical Turk to recruit them. and in the fol-
lowing we describe the main results.
Participant Demographics. 54 % of the participants are male, the
majority (46 %) of participants are between 25 and 34 years old, and
holds either a high school diploma (33 %) or a bachelor’s degree
(52 %). The full demographic information in our study, compared
to the general adult US population provided by the U.S. Census
Bureau [54], can be found in Table 2. Our sample is biased as more
participants identified as male, have a better formal education, and
are younger than the general population. A majority of participants
(90 %) use some form of privacy protection online, at least from time
to time (ad blocker: 50 %; private browsing: 52 %; delete cookies:
71 %; opt-out: 31 %; none: 10 %). A recent study found that 37 % of
Internet users use an ad blocker, especially younger individuals [19].
We assume that we observed more ad blocker usage since our
sample is skewed towards younger participants.
707Table 2: Participant Demographics. ⋆: The census data does
not account for non-binary individuals. ♣: The census data
combines these categories.
Amount
%
US pop.
Gender
Male
Female
Non-binary
Age
18–24
25–34
35–44
45–54
55–64
Education
None
High School
Bachelor’s
Pro./Master’s/Ph.D.
264
224
1
41
218
112
66
40
1
161
255
69
54 %
46 %
0 %
8 %
46 %
23 %
14 %
8 %
0 %
33 %
52 %
14 %
49 %
51 %
— ⋆
16 %
22 %
20 %
21 %
21 %
12 %
51 %
18 %
11 %♣
Attitude towards online advertising. The general view of partici-
pants on online advertising is quite neutral. Participants who see
ads that suit their interests still evaluate them slightly negatively
(mean: 2.7 with SD 0.4 and the hypothesis test yielded p ≤ .0005),
which is in line with previous work [18] (Q1 and Q2). Other studies
also found that users find ads “creepy” or “intrusive” [51]. In our
study users expressed such views too, but at the same time they
did not evaluate ads negatively (e. g., P-02 stated in Q9:“They [ads]
are creepy, a product is merely mentioned in my house then I see ads
for it the next time I’m online” but at the same time stated her views
on ads as “Moderately satisfied”).
The neutral view on online ads is likewise observed in an open-
ended question (Q9) and on a Likert scale (85 % of participants
choose one the following answer options almost balanced (Q2):
“Moderately satisfied” (= 4), “Neither satisfied nor dissatisfied” (= 3),
or “Moderately dissatisfied” (= 2) with a mean of 3.07 and SD 0.5
(Q. In total, 73 % of participants “agreed” (47 %) or “strongly agreed”
(26 %) with the statement that access to personal data is useful to
assess privacy implications of the usage of their data (Q5), but only
19 participants requested their data, mostly from big companies
like Google or Facebook (Q4).
In Q6, 60 % of participants stated they were “not” (10 %) or “some-
what knowledgeable” (50 %) about online advertisements while 30 %
stated that they were “very knowledgeable” (5 %) or “knowledge-
able” (25 %). We used a multiple-choice question to test this self-
assessment (Q7). This question contained seven statements on the
online advertising industry—four of which are correct and three
incorrect. Each (multiple-choice) answer option was selected 357
times on average (SD 51) regardless of whether it was true or false.
This indicates that users often do not understand online advertising
as well as they think they do. Furthermore, some participants made
statements containing false information about online advertising:
“I honestly expect some ad companies to illegally collect my facial
expressions and sounds in my environment through cameras and mi-
crophones. I always expect them to access other apps and histories of
everything that I do” (P-152). While there was no public report on
this happening in practice at the time of the survey, after finishing
the study, multiple reports surfaced that big Internet companies
employed humans to categories conversations recorded by smart
home devices [12, 27, 49]. The most common misconceptions were
that ad companies have access to all purchased products (64 %) and
the full browsing history (66 %).
65 % of all participants stated (Q22) they thought that compa-
nies did not provide all collected information upon request, only
13 % thought they would do that, and 22 % had no opinion on this
topic. This mistrust might be based on misconceptions about what
companies can collect or relate to public reporting on data leakage
scandals (e. g., Facebook providing data to Cambridge Analytica and
not informing users properly [48]).
Identifying Data Collectors. We tried to assess if users can under-
stand what personal data is used when they see a specific ad and
whom they have to ask to get access to this data (Q10 and Q11).
To do so, we showed users two screenshots of websites contain-
ing ads (see Figure 1 in Appendix A); one contained information
regarding the company providing the ad, while the other did not.
We asked users whom they would have to contact if they wanted
to understand why they see this specific ad. We provided different
answer options (multiple choice): (1) the website on which the ad
was shown, (2) the company name of the advertised product, and
(3) the actual ad network providing the ad.
In the case of the ad that contained (Q11) the ad network’s name,
46 % of users answered the question correctly, 28 % named the adver-
tised company, 17 % named the visited website, and the remaining
9 % did not know whom they have to ask. For the ad banner that
did not directly include the ad network’s name but showed it in the
link when hovering over the ad (Q10), 43 % named the advertised
company as the contact company, 24 % named the visited website,
and only 24 % correctly knew whom they should have to contact.
In conclusion, only a minority of participants was able to identify
the correct company to contact, but we did not find a significant
correlation between users’ self-assessed knowledge about online
advertising and their answers (Pearson-correlation: r = 0.56 with