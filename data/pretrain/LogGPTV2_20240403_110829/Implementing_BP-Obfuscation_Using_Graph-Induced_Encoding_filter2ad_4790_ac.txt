matrix that depends on R.
(2) Set (cid:174)v = (cid:174)u − A(cid:174)p mod q.
(3) Sample a small solution (cid:174)z to the equation G(cid:174)z = (cid:174)v (mod q),
(4) Output (cid:174)x = (cid:174)p +
according to a spherical discrete Gaussian.
(cid:17) (cid:174)z mod q.
Note that this indeed yields a solution to A(cid:174)x = (cid:174)u (mod q), since
A(cid:174)x = A
= A(cid:174)p + G(cid:174)z = A(cid:174)p + (cid:174)v = (cid:174)u
(mod q).
Also, if (cid:174)p is chosen relative to covariance matrix Σp and (cid:174)z is chosen
from a spherical discrete Gaussian with parameter σz, then the co-
variance matrix of the resulting (cid:174)x will be Σx = Σp +σ
RT |I
z ·(cid:16) R
2
I
(cid:17)×(cid:16)
(cid:17).
(cid:16) R
(cid:17)(cid:174)z
(cid:17)
I
(cid:16)(cid:174)p +
(cid:16) R
I
Session D1:  Functional Encryption and ObfuscationCCS’17, October 30-November 3, 2017, Dallas, TX, USA787Parameters: σ , e, k and the pi’s
Input: u ∈ Z
1. for i = 0 to k − 1, for j = 0 to e − 1
2.
3.
4.
5. Output (cid:174)z ′
t ← Dpi Z+u,σ // Sample a σ-short t ∈ Z, t = u (mod pi)
Set (cid:174)z ′[j + i · e] := t
Set u := (u − t)/pi // Update u after each iteration
Figure 3: The G-sampling procedure
I
2
z
(cid:16) R
(cid:17) (R|I). (This means that σx must
Thus, to get a solution (cid:174)x sampled from a spherical Gaussian with
(large enough) parameter Sx = σx I, we need to set the covariance
matrix for (cid:174)p to Σp = σ
x I − σ
2
be sufficiently large relative to σz and the singular values of R, so
that Σp is positive definite.)
4.2 The Gadget Matrix G
The “Gadget matrix” in [18] is based on binary representation, but
for our implementation we use a mixed radix representation instead
(which makes CRT-based processing easier, see below). Specifically,
we use a list of small co-prime factors (in the range from 71 to 181),
which we denote here by p1, p2, . . . , pk.
We also have a parameter e that specifies how many times we
repeat each factor (by default e = 3). Thus our big modulus is
, where we take k large enough to give us as many bits
in q as we need.
Given all these co-prime factors, we define the vector (cid:174)д as the
mixed-radix vector with the pi’s appearing e times each. That is,
the first entry is (cid:174)д[0] = 1, and for each following entry we have
(cid:174)д[i + 1] = (cid:174)д[i] · p⌊i/e⌋:
q =k
i =1 pe
i
(cid:0)1, p1, . . . , pe−1
1
,
. . .
pe1 , p2pe1 , . . . , pe−1
P∗, pk P∗, . . . , pe−1
2 pe1 ,
k P∗(cid:1).
(cid:174)дT =
where P∗ =
i  i
// Update v[i] as an integer
v[i] := (v[i] − t)/pi
v[m] :=(cid:2)(v[m] − t) · p−1
(cid:3)
pe
m
i
3.
4.
5.
6.
7. Output (cid:174)z ′
Figure 4: The G-sampling procedure in CRT representation
vulnerability due to this deviation. We also note that any one-
dimensional sampling implementation can be plugged in to our
code without changing anything else, but this is one aspect that we
did not experiment with.
I
2
z
RT |I
(cid:17)(cid:16)
(cid:16) R
x I − σ
2
Multi-dimensional ellipsoidal Gaussians. Recall that the perturba-
tion vector (cid:174)p in the Micciancio-Peikert procedure is drawn from an
ellipsoidal discrete Gaussian distribution with covariance matrix
Σp = σ
Peikert [19] showed how to sample from this distribution by first
(cid:17). Prior work due to Gentry et al. [13] and
computing(cid:112)Σp, a rather expensive operation. Instead, we devised
and implemented a sampling procedure for the ellipsoidal discrete
Gaussian that is somewhat similar to the GPV sampler in [13] but
can work directly with the covariance matrix rather than its square
root. Specifically, we choose each entry in (cid:174)p from a one-dimensional
discrete Gaussian distribution, conditioned on the previous entries.
To approximate the conditional mean and variance, we use the
corresponding values from the continuous case. Specifically, for a
vector (x, y) with covariance matrix Σ and mean µ we have:
= ΣY ,Y − ΣY ,X Σ−1
ΣY |X
µY |X =x = µY + ΣY ,X Σ−1
X,X ΣX,Y
X,X (x − µX )
(3)
where ΣX,X , ΣX,Y , ΣY ,X , ΣY ,Y are the four quadrants of the covari-
ance matrix Σ, and µX , µY are the two parts of the mean vector µ.
(Note that when x is one-dimensional, as in our case, then Σ−1
is
X,X
just 1/σ
2
x .) Below we show that this procedure yields the right prob-
ability distribution up to a negligible error, as long as the singular
values of the matrix Σp are all ω(log(λn)). 5
Analysis of the ellipsoidal Gaussian sampler. Below we use DZn,
√
Σ
to denote the n-dimensional discrete Gaussian distribution with
covariance matrix Σ. For index sets A, B ⊂ [n], we define ΣA,B to
be the submatrix obtained by restricting Σ to the rows in A and
the columns in B. (When the set only has one element, we sim-
ply write, e.g., ΣA,i instead of ΣA, {i }.) We will be interested in
the kth diagonal entry Σk,k, the top-left (k − 1) × (k − 1) subma-
trix Σ[k−1],[k−1], and the submatrices Σk,[k−1] and Σ[k−1],k. (Note
that Σk,[k−1] = ΣT
k,[k−1], since Σ is positive definite and therefore
symmetric.)
5The proof below works when the underlying one-dimensional sampling is from a
discrete Gaussian, and will incur noticeable deviation when using rounded continuous
Gaussian.
7
(cid:169)(cid:173)(cid:173)(cid:173)(cid:171) Σ−1
−1 =
Σ
[n−1],[n−1] + (cid:174)vn (cid:174)vT
n /Sn −(cid:174)vn/Sn
1/Sn
−(cid:174)vT
n /Sn
(cid:170)(cid:174)(cid:174)(cid:174)(cid:174)(cid:172) .
We then define
Sk := Σk,k − Σk,[k−1]Σ
(cid:174)vk := Σ
−1
[k−1],[k−1]Σk,[k−1] .
−1
[k−1],[k−1]Σ[k−1],k , and
(4)
(Note that Sk ∈ R is a scalar and (cid:174)vk ∈ Rk−1 is a (k −1)-dimensional
vector. For convenience, we also define S1 := Σ1,1 and (cid:174)v1 := (cid:174)0.)
Sk is known as the Schur complement of Σ[k−1],[k−1] in Σ[k],[k],
a very well-studied object [21]. In particular, the following claim
shows that the kth coordinate of a (continuous) Gaussian with
covariance Σ conditioned on the first k − 1 coordinates taking the
value (cid:174)x ′ ∈ Rk−1 is exactly the Gaussian with variance Sk and mean
⟨(cid:174)vk , (cid:174)x ′⟩, as we discussed above.
Claim: For any vector (cid:174)x ∈ Rn and symmetric matrix Σ ∈ Rn×n,
let Sn and (cid:174)vn be defined as above, and let (cid:174)x ′ ∈ Rn−1 be the first
n − 1 coordinates of (cid:174)x. Then, if Sn and Σn,n are non-zero and
Σ[n−1],[n−1] is invertible, then (cid:174)xT Σ−1(cid:174)x = (xn − ⟨(cid:174)vn, (cid:174)x ′⟩)2/Sn +
(cid:174)x′T Σ[n−1],[n−1](cid:174)x ′ .
Proof. Note that
(One can check this by simply multiplying by Σ. We note that the
identity does not hold when Σ is not symmetric.) The result then
follows by directly computing (cid:174)xT Σ−1(cid:174)x.
□
We will also need the following well-known fact, which follows
immediately from the Poisson summation formula.
Lemma 4.1. For any s ≥ 1 and any x ∈ R,
(1 − 2−s2) · ρs(Z) ≤ ρs(Z − x) ≤ ρs(Z) .
Here, ρs(x) is the one-dimensional Gaussian function with pa-
rameter s, and for a set of points X, ρs(X) is the sum of ρs over
these points. We now prove the correctness of our sampler.
, where
a positive-definite matrix Σ ∈ Rn×n.
Theorem 4.2. Consider the following procedure that takes as input
(1) Set (cid:174)z ← (cid:174)0.
(2) For k = 1, . . . , n, compute Sk and (cid:174)vk as defined above.
(3) For k = 1, . . . , n, sample zk from µk + DZ−µk,
√
Sk
(4) Return (cid:174)z.
µk := ⟨(cid:174)vk , (cid:174)z ⟩, and set (cid:174)z ← (cid:174)z + zk · (cid:174)ek .
Then, the output vector (cid:174)z is within statistical distance n2−S of DZ,
where S := mink Sk .
Proof. Note that the first coordinate of (cid:174)z is distributed exactly