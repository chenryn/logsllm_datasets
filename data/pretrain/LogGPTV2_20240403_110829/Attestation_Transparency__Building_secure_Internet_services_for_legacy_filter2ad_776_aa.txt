title:Attestation Transparency: Building secure Internet services for legacy
clients
author:Jethro G. Beekman and
John L. Manferdelli and
David A. Wagner
Attestation Transparency: Building secure Internet
services for legacy clients
Jethro Beekman
John Manferdelli
David Wagner
Electrical Engineering and Computer Sciences
University of California at Berkeley
Technical Report No. UCB/EECS-2016-12
http://www.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-12.html
March 15, 2016
Copyright © 2016, by the author(s).
All rights reserved.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission.
Acknowledgement
We thank Jon McCune and our anonymous reviewers for their feedback.
This work was supported by Intel through the ISTC for Secure Computing,
AFOSR under MURI award FA9550-12-1-0040, and NSF under CCF-
0424422.
Attestation Transparency
Building secure Internet services for legacy clients
Jethro G. Beekman
University of California, Berkeley
PI:EMAIL
John L. Manferdelli
Google
PI:EMAIL
David Wagner
University of California, Berkeley
PI:EMAIL
ABSTRACT
Internet services can provide a wealth of functionality, yet
their usage raises privacy, security and integrity concerns
for users. This is caused by a lack of guarantees about what
is happening on the server side. As a worst case scenario,
the service might be subjected to an insider attack. We
use remote attestation of the server to obtain guarantees
about the programming of the service. On top of that, we
augment Certiﬁcate Transparency to distribute information
about which services exist and what they do. Combined,
this creates a platform that allows legacy clients to obtain
security guarantees about Internet services.
CCS Concepts
•Security and privacy → Trusted computing; Soft-
ware security engineering; •Computer systems or-
ganization → Client-server architectures; •Networks →
Cloud computing;
Keywords
Remote attestation; Secure enclaves; Certiﬁcate Trans-
parency; Cloud computing
1.
INTRODUCTION
End users increasingly perform important computing ac-
tivities online in the cloud. This is convenient for them
but the guarantees they get about those activities are sig-
niﬁcantly reduced from an ideal desktop-computing model
where applications are run on trusted machines, inaccessi-
ble to adversaries, using software installed and maintained
by knowledgeable trusted personnel known to the end user.
On well-managed desktop machines, users can be conﬁdent
that the software they use is the version they expect, with
known behavior and mechanisms to prevent unauthenticated
access to their data and unauthorized modiﬁcation to the
software itself.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
ASIA CCS’16, May 30-June 03, 2016, Xi’an, China
c(cid:13) 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-4233-9/16/05. . . $15.00
DOI: http://dx.doi.org/10.1145/2897845.2897895
1
Cloud services typically do not provide similar guarantees,
which raises privacy, security and integrity concerns. Who
will have access to my data, intentionally or unintentionally?
Will the service continue to work properly tomorrow? Can
the service read my data and use it for purposes I didn’t
have in mind? Will my data still exist in the same form
tomorrow? Could a malicious system administrator at the
service read or modify my data? If a system administrator’s
credentials are breached, could an attacker gain access to
my data? For current Internet services, the answers to these
questions are often unsatisfying.
On the other hand, cloud services provide many beneﬁts
that desktop users don’t get. Cloud services provide avail-
ability through redundancy and replication and they remove
the burden of maintenance from the user. In addition, the
sheer number of available services seems to be surpassing
the number of applications available on the desktop.
The ideas presented in this paper aim to combine beneﬁts
from the cloud-based service model with some of the guar-
antees with which desktop computer users are familiar. We
focus especially on defending against insider attacks. For
instance, the insider might be a malicious system adminis-
trator, a system administrator whose credentials have been
compromised, or even a government order that compels the
service to provide access to user data [15]. In addition, we
deﬁne a policy-based mechanism that allows users to choose
which security properties they expect in a secure service.
Our basic approach is to use transparency to deter unau-
thorized access. We use hardware support for sealed storage
to ensure that user data is stored in encrypted form, and not
directly accessible by insiders. Then, we build mechanisms
to ensure that insiders cannot modify or introduce backdoors
into the software without detection. Our design makes use
of recent advances in security technology such as hardware-
supported remote attestation and Certiﬁcate Transparency.
Services store a hash of the software that they are running in
a public audit log, and (conceptually) clients verify that the
server is running code that matches a hash in the public au-
dit log. This means that insiders cannot make undetectable
changes to the server-side code; at least in principle, any
such changes can be detected through examination of the
public audit log. One challenge is how to ensure that these
servers can be used from legacy clients, such as existing web
browsers. We show how to achieve this goal by building on
Certiﬁcate Transparency.
We claim the following contributions:
Insider attack protection We show how to build In-
ternet services that are unalterable secure services. Such
services provide integrity and conﬁdentiality of computation
and user data, are secure against insider attacks, and their
functionality can be remotely veriﬁed.
Policy and update mechanism Although we
de-
scribe services as unalterable, in fact, updates will often
be desirable to ﬁx bugs, improve usability or security or
add functionality. Also, instances of a secure service should
not be locked to a single platform. We demonstrate how
these goals can be achieved, without user-visible downtime
for updates, and without compromising security or privacy.
Finally, secure services may—under policy control speciﬁed
by a user—be authorized to collaborate with other user
secure services while preserving the security promises; we
show how this can be done as well.
Legacy client support
In a manner similar to the way
Certiﬁcate Transparency protects against CAs secretly is-
suing bad certiﬁcates, Attestation Transparency protects
against service providers secretly changing the services they
provide. This transparency provides a public record link-
ing domain names to service implementations. Today’s TLS
clients immediately reap the beneﬁts of the transparency
framework, except in some cases when they are the victim
of a targeted attack involving misissued certiﬁcates.
In the following section we review existing building blocks
we use to build our framework. We provide a high-level
overview of the paper in §3. In §4 we discuss the design of
unalterable secure services while §5 describes how we aug-
ment Certiﬁcate Transparency to allow veriﬁcation of these
services. We evaluate our design in §6 and elaborate on po-
tential use cases in §7. We then wrap up with related work
in §8 and the conclusion (§9).
2. BACKGROUND
2.1 Secure Enclaves
We deﬁne a secure enclave as an isolated process, executed
on a platform that provides conﬁdentiality and integrity of
code and data as well as sealing and attestation. In general,
these technologies allow initializing an isolated and perhaps
encrypted block of memory with a known program. Access
to application memory is restricted by hardware and exter-
nal access to the software is similarly restricted to identiﬁed
entry points into the code. The software loaded in an enclave
is also measured1, allowing the hardware to attest to an-
other party that the expected software was properly loaded
and initialized and that the enclave software is isolated from
other software running on the computer. The platform also
provides a way to encrypt data so that the encrypted data
can only be decrypted by this particular instance of the code
running on this particular hardware. Diﬀerent technologies
provide such secure enclaves, including Intel SGX [4, 27],
IBM SecureBlue++ [8], TPM-based Flicker [26], and per-
haps ARM Trustzone [38].
2.1.1 Attestation primitive
Attestation is a mechanism that allows software to make
statements that can be veriﬁed remotely by communicating
parties, using attested statements. When talking about se-
cure enclaves, some hardware-based root of trust, H, will
1A measurement is typically a cryptographic hash of the
software as loaded together with any conﬁguration informa-
tion which may aﬀect the software behavior.
attest that it is running a program with identity (measure-
ment) I. In order for such a program to communicate with
the outside world securely, it will need an encryption key K,
and a way to securely announce to the outside world that it
controls that key. Attestation provides such a mechanism:
the hardware makes a statement of the form2
A(I, K) = (cid:28) H says “H runs I which says
[K speaks for I]”(cid:29) .
Platforms providing secure enclaves often provide ways for
an entity I1 to endorse a particular program with identity
I2. For example, I1 might cryptographically sign I2, and
this signature can be veriﬁed as part of loading I2. Such an
attestation is of the form
A(I1 : I2, K) = (cid:28) H says “H runs I2 which says
[K speaks for I2]” and “I1 endorses I2”(cid:29) .
If the platform can not verify the endorsement itself, a sim-
ilar statement can still be formed by including the endorse-
ment directly, as in
A(I1 : I2, K) = (cid:28) H says “H runs I2 which says [K
speaks for I2]”(cid:29) and (cid:28) I1 says “I1 endorses I2”(cid:29) .
Sealing and encryption primitives
2.1.2
The general secure enclave concept does not include secure
persistent storage. This is generally solved by using an un-
trusted persistent store and storing data only in encrypted
form. This provides a form of secure persistent storage. We
discuss some limitations of this scheme (such as rollback at-
tacks) in §7.2.
Encrypting data in such a way that only a particular in-
stance of a secure enclave can access it is called sealing.
Generally, this is achieved by using a symmetric encryption
key derived from both the program and hardware identity.
The sealing operation turns the message m into the sealed
text s = Eseal(m), while unsealing is m = Dseal(s).
In this paper we also use authenticated encryption, written
c = E(K, m) and m = D(K, c).
2.2
Intel Software Guard Extensions
Intel Software Guard Extensions (SGX) [4, 17, 19, 27] are
a recently announced hardware technology and instruction
set extension providing secure enclaves. A special set of in-
structions can measure and encrypt a memory region before
transferring execution control to it. The trusted computing
base of SGX-based secure enclaves encompasses only the
processor hardware, its microcode and the enclave image it-
self. In particular, the operating system is not part of the
TCB. Data stored in memory regions belonging to the en-
clave is encrypted before it leaves the processor, so that the
memory bus is also not part of the TCB. The security of
this system is predicated on the correct functioning of the
processor hardware and the SGX instruction set.
2.2.1 Attestation
SGX-enabled hardware can generate reports:
integrity-
protected statements about the enclave generated by the
hardware:
Reportlocal = MAC (Ienclave(cid:107)Isigner(cid:107)Duser) .
2Following the Taos language [39].
2
The MAC key is diﬀerent for each processor and private to
the enclave that requested the report—only that enclave on
the same processor can verify the report. Ienclave is the mea-
surement of the code of the enclave the report is generated
of and Isigner is the public key that was used to sign that en-
clave before loading it. Duser is an arbitrary value that can
be speciﬁed by the enclave when requesting the attestation
report. This can be used to bind data to the attestation.
A special secure enclave provided by Intel, called the quot-
ing enclave, can replace the MAC with a signature:
Reportremote = Sign (Ienclave(cid:107)Isigner(cid:107)Duser) .
The signature private key is private to the processor and
cannot be used improperly or for any purpose. The corre-
sponding public key is published by the vendor, and a third
party can use it to verify that the report was created by
actual Intel hardware.
2.2.2
A special instruction can generate an enclave-speciﬁc seal-
Sealed storage
ing key. The key is derived as
Kderived = H (Ienclave(cid:107)Kdevice(cid:107) . . .)
where Kdevice is a hardware-embedded secret unique to this
device. The enclave can use this key to encrypt data which
can only be decrypted by the same enclave running the same
code on the same hardware.
A diﬀerent key can also be derived as Kderived =
H (Isigner(cid:107)Kdevice(cid:107) . . .). This key can be used to transfer
data between enclaves running on the same hardware that
were signed by the same public key. In this work, we don’t
use this key since it gives too much control to the signer.
2.3 Certiﬁcate Transparency
The Certiﬁcate Transparency (CT) framework [22], as the
name implies, aims to provide transparency to the issuance
of TLS certiﬁcates. CT makes all legitimate TLS certiﬁcates
a matter of public record, making it trivial to identify misis-
sued certiﬁcates. The framework consists of several parts.
Public append-only logs A CT log server maintains
a log of all certiﬁcates submitted to it. The log is structured
as a Merkle tree which allows eﬃcient veriﬁcation of addi-
tions to the log. When submitting a certiﬁcate to the log
server, the server will return a Signed Certiﬁcate Timestamp
(SCT). The SCT is a promise that the server will include the
certiﬁcate in the log within a certain time limit, the maxi-
mum merge delay. The SCT can be used as proof to other
parties that a certiﬁcate is part of the public record.
Monitors A monitor watches one or more CT log
servers for suspicious changes. For example, a domain owner
might know that all its certiﬁcates are issued by a particular
CA. If a certiﬁcate for their domain issued by a diﬀerent CA
appears in a log, the monitor raises an alarm. The admin-
istrator can then act upon that alarm, e.g., by demanding
the revocation of the phony certiﬁcate.
Auditors An auditor watches one or more CT log
servers for consistency.
It checks that the Merkle tree is
updated consistently and that certiﬁcates are included as
promised by SCTs. If it detects any inconsistency, it raises
an alarm. The CT log owner will then need to explain the
discrepancy or risk being shut down.
Browsers Once the CT framework is fully operational,
TLS clients such as browsers can demand proof from TLS
servers that the server’s certiﬁcate appears in a log. TLS
servers can provide this proof in the form of SCTs.
If a
certiﬁcate does not appear in the logs, that is suspicious,
and the client can choose to abort the connection attempt.
3. OVERVIEW
We want application service providers on the Internet to
be able to host secure services. These services must be able
to store and handle user data securely. By secure, we mean
that the data’s conﬁdentiality and integrity is preserved, in
the face of attacks within the scope of the threat model