title:How to Accurately and Privately Identify Anomalies
author:Hafiz Salman Asif and
Periklis A. Papakonstantinou and
Jaideep Vaidya
How to Accurately and Privately Identify Anomalies
Jaideep Vaidya
Rutgers University
Periklis A. Papakonstantinou
Rutgers University
Rutgers University
Hafiz Asif
PI:EMAIL
PI:EMAIL
PI:EMAIL
ABSTRACT
Identifying anomalies in data is central to the advancement of
science, national security, and finance. However, privacy concerns
restrict our ability to analyze data. Can we lift these restrictions and
accurately identify anomalies without hurting the privacy of those
who contribute their data? We address this question for the most
practically relevant case, where a record is considered anomalous
relative to other records.
We make four contributions. First, we introduce the notion of
sensitive privacy, which conceptualizes what it means to privately
identify anomalies. Sensitive privacy generalizes the important con-
cept of differential privacy and is amenable to analysis. Importantly,
sensitive privacy admits algorithmic constructions that provide
strong and practically meaningful privacy and utility guarantees.
Second, we show that differential privacy is inherently incapable
of accurately and privately identifying anomalies; in this sense, our
generalization is necessary. Third, we provide a general compiler
that takes as input a differentially private mechanism (which has
bad utility for anomaly identification) and transforms it into a sen-
sitively private one. This compiler, which is mostly of theoretical
importance, is shown to output a mechanism whose utility greatly
improves over the utility of the input mechanism. As our fourth
contribution we propose mechanisms for a popular definition of
anomaly ((β, r )-anomaly) that (i) are guaranteed to be sensitively
private, (ii) come with provable utility guarantees, and (iii) are em-
pirically shown to have an overwhelmingly accurate performance
over a range of datasets and evaluation criteria.
CCS CONCEPTS
• Security and privacy → Privacy-preserving protocols; • Com-
puting methodologies → Anomaly detection.
KEYWORDS
privacy; anomaly identification; differential privacy; outlier detec-
tion
ACM Reference Format:
Hafiz Asif, Periklis A. Papakonstantinou, and Jaideep Vaidya. 2019. How
to Accurately and Privately Identify Anomalies. In 2019 ACM SIGSAC Con-
ference on Computer and Communications Security (CCS ’19), November
11–15, 2019, London, United Kingdom. ACM, New York, NY, USA, 18 pages.
https://doi.org/10.1145/3319535.3363209
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CCS ’19, November 11–15, 2019, London, United Kingdom
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6747-9/19/11...$15.00
https://doi.org/10.1145/3319535.3363209
1 INTRODUCTION
At the forefront of today’s research in medicine and natural sci-
ences is the use of data analytics to discover complex patterns from
vast amounts of data [11, 23, 39]. While this approach is incredibly
useful, it raises serious privacy-related ethical and legal concerns
[5, 7, 20, 21] because inferences can be drawn from the analysis
of the person’s data to the person’s identity, causing a privacy
breach [19, 24, 26, 27, 37]. In this work, we focus specifically on the
problem of identifying anomalous records, which has fundamental
applications in many domains and is also crucial for scientific ad-
vancements [1, 3, 30, 40, 42]. For example, to treat cancer, we must
tell if a tumor is malignant; to stop bank fraud, we must flag the
suspicious transactions; and to counter terrorism, we must iden-
tify the individuals exhibiting extreme behavior. Note that in such
settings, it is imperative to accurately identify the anomalies, e.g.,
it is critical to identify the fraudulent transactions. However, in
all these situations, it is still essential to protect the privacy of the
normal (i.e., non-anomalous) records [7, 21] (e.g., customers with a
legitimate transaction or patients with a benign tumor) while not
sacrificing accuracy (e.g., labeling a malignant tumor as benign).
We solve the problem of accurate, private, and algorithmic anom-
aly identification (i.e., labeling a record as anomalous or normal
by an algorithm) with an emphasis on reducing false negative –
labeling an anomaly as normal – rate. The current methods for
protecting privacy work well for doing statistics and other ag-
gregate tasks [17, 18], but they are inherently unable to identify
anomalous records accurately. Furthermore, the modern methods
of anomaly identification label a record as anomalous (or normal)
based on its degree of dissimilarity from the other existing records
[1, 3, 8, 35]. Consequently, the labeling of a record as anomalous is
specific to a dataset, and knowing that a record is anomalous can
leak a significant amount of information about the other records.
This type of privacy leakage is the core obstacle that any privacy-
preserving anomaly identification method must overcome. This
work is the first to develop methods (in a general setting where
anomalies are data-dependent) to accurately identify if a record is
anomalous while simultaneously guaranteeing privacy by making
it statistically impossible to infer if a non-anomalous record was
included in the dataset.
We formalize a notion of privacy appropriate for anomaly detec-
tion and identification and develop general constructions to achieve
this. Note that we assume a trusted curator, who performs the anom-
aly identification. If the data is distributed and the trusted curator
is not available, one can employ secure multiparty computation to
simulate the trusted curator [9], where now the same methodology
as in the previous setting can be used.
Although the privacy definitions and constructions we develop
are not tied to any specific anomaly definition, we instantiate them
for a specific kind of anomaly: (β, r )-anomaly [35], which is a widely
prevalent model for characterizing anomalies and generalizes many
Session 3E: Privacy IICCS ’19, November 11–15, 2019, London, United Kingdom719Figure 1: (a) x and y differ by one record, the “ε axis” is for the privacy parameter, the “P (M (x ) (cid:44) f (x )) axis” is for the minimum
error over all ε-DP mechanisms M on x for a give error on y on the “P (M (y) (cid:44) f (y)) axis”. The graph depicts the tradeoff
between the errors committed on x and y. (b) this plot is for ε = 1 and otherwise is the same but for different x’s and y’s.
other definitions of anomalies [3, 22, 34, 35]. These technical instan-
tiations naturally extend to the other well-known variants of this
formalization [1]. Under this anomaly definition, a record (which
lives in a metric space) is considered anomalous if there are at most
β records similar to it, i.e., within distance r. The parameters β and
r are given by domain experts [35] or found through exploratory
analysis by possibly using differentially private methods [17, 18]
(since these parameters can be obtained by minimizing an aggre-
gate statistic, e.g., risk or average error) to protect privacy in this
process.
1.1 Why do we need a new privacy notion?
We consider the trusted curator setting for the privacy. The trusted
curator has access to the database, and it answers the anomaly iden-
tification queries using a mechanism. The privacy of an individual
is protected if the output of an anomaly identification mechanism
is unaffected by the presence or the absence of the individual’s
record in the database (which is the input to the mechanism). This
is the notion of privacy (i.e. protection) of a record that we consider
here; it protects the individual against any risk incurred due to the
presence of its information and was first formalized in the seminal
work of differential privacy [15, 17] (where privacy is quantified by
a parameter ε > 0: the smaller the ε, the higher the privacy) and can
informally be stated as follows: a randomized mechanism that takes
a database as input is ε-differentially private if for any two input
databases differing by one record, the probabilities (correspond-
ing to the two databases) of occurrence of any event are within
a multiplicative factor eε (i.e., are almost the same in all cases).
Unfortunately, simply employing differential privacy does not ad-
dress the need for both privacy and practically meaningful accuracy
guarantees in our case. For example, providing privacy equally to
everyone severely degrades accuracy in identifying anomalies. For
a database, the addition of a record in a region which is sparse in
terms of data points creates an anomaly. Conversely, the removal
of an anomalous record typically removes the anomaly altogether.
Therefore, the accuracy achievable for anomaly identification via
differential privacy is limited as explained below.
Differential privacy for binary functions f : D → {0, 1}, such
as the anomaly identification, comes with inherent limitations that
can be explained through the graph of Figure 1a. Fix any mecha-
nism M that is supposed to compute f , with the property that this
mechanism is differentially private. The mere fact that f is binary
and M is differentially private has the following effect. For any two
databases x and y that differ in one record say that f (x ) = 0 and
f (y) = 1. Now, a simple calculation shows that the differential pri-
vacy constraints create a tradeoff: whenever M makes a small error
in computing f (x ) then it is forced to err a lot when computing on
its “neighbor” y and vice-versa. Moreover, the higher the privacy
requirements are (i.e. for smaller ε) the stricter this tradeoff is, as
depicted on Figure 1a. Formally, we state this fact as follows.
Claim 1. Fix ε > 0, f : D → {0, 1}, and ε-DP M : D → {0, 1}
arbitrarily. For every x and y, if f (x ) (cid:44) f (y) and ||x − y||1 = 1, then
P (M (x ) (cid:44) f (x )) ≥ 1/(1 + eε ) or P (M (y) (cid:44) f (y)) ≥ 1/(1 + eε ).
What happens to this inherent tradeoff when x and y differ in
more than one record? As shown on Figure 1b this tradeoff is re-
laxed. We note that for deriving the tradeoff, there was nothing
specific to the ℓ1 metric (used for differential privacy), but instead
we could have used any metric over the space of databases; other
works that considered general metrics are e.g., [25, 33]. Our work
proposes a distance metric which is appropriate for anomaly identi-
fication, in conjunction to an appropriate relaxation of differential
privacy. This way we will lay out a practically meaningful (but also
amenable to analysis) privacy setting.
1.2 What do we want from the new notion?
We want to relax differential privacy since affording protection
for everyone severely degrades the accuracy for anomaly identifi-
cation. One possible relaxation, suitable for the problem at hand,
is providing protection only for a subset of the records. We note
that such a relaxation is backed by privacy legislation, e.g., GDPR
allows for giving up privacy for an illegal activity [21]. Protecting
a prefixed set of records, which is decided independent of the data-
base, works when anomalies are defined independent of the other
Session 3E: Privacy IICCS ’19, November 11–15, 2019, London, United Kingdom720records. However, for a data-dependent anomaly definition, such
a notion of privacy fails to protect the normal records. Here the
problem arises due to the fixed nature of the set that is database-
specific. In the case of a data-dependent definition of anomaly, if
we wish to provide privacy guarantee to the normal – call them
sensitive – records that are present in the database, then specifying
the set of sensitive records itself leaks information and can lead to
a privacy breach. Thus, sensitive records must be defined based on
a more fundamental premise to reduces such dependencies. This
notion of sensitive record plays a pivotal role in defining a notion
of privacy, named sensitive privacy, which is appropriate for the
problem identifying anomaly.
We remark that although anomaly identification method provide
binary labeling, they assign scores to represent how outlying a
record is [1, 3]; thus these models (implicitly or explicitly) assign a
records a degree of outlyingness with respect to the other records,
which the following discussion takes into account.
An appropriate notion of privacy in our setting must allow a
privacy mechanism to have the following two important properties.
First, the more outlying (or non-outlying) a record is, the higher
the accuracy the privacy mechanism can achieve for anomaly iden-
tification, which is in contrast to DP (Figure 2c). Second, all the
sensitive records should have DP like privacy guarantee for the
same value of privacy parameter.
The mechanisms that are private under sensitive privacy achieve
both the properties (see Figure 2, which gives the indicative experi-
mental results on the example data; see Section A.1 for the details
on the experiment and the values of the parameters). Furthermore,
it has an additional property: in a typical setting, the anomalies do
not lose privacy altogether; instead the more outlying a record is
the lesser privacy it has (Figure 2d).
1.3 How do we define the new privacy notion?
To define privacy, we need a metric space over the databases since
a private mechanism needs to statistically blur the distinction be-
tween databases that are close in the metric space. While differential
privacy uses the || · ||1 − metric, we utilize a different metric over
databases, which can be defined using the notion of sensitive record.
Informally, we say a record is sensitive with respect to a database if
it is normal or becomes normal under a small change—we formalize
this in Section 3. We argue that this notion of sensitive record is
quite natural, and it is inspired from the existing anomaly detection
literature [1, 3]. Since, by definition, an anomalous record signifi-
cantly diverges from other records in the database [1, 3], a small
change in the database should not affect the label of an anomalous
record. Given the definition of sensitive record, a graph over the
databases is defined by adding an edge between two databases if
and only if they differ in a sensitive record. The metric over the
databases is now given by the shortest path length between the
databases in this graph. This metric space has the property that
databases differing by a sensitive record are closer compared to
the databases differing in a non-sensitive record. We use the pro-
posed metric space to define sensitive privacy, which enables us to
fine-tune the tradeoff between accuracy and privacy.
Figure 2: (b), (c) is for the same data, and (d), (e) is for the
same data. (a) gives the density plot of the distribution of the
example data. z1 and z2 axes give the coordinate of a point
(record). (b) and (c) resp. show the accuracy (on vertical axis)
for anomaly identification (AId) via sensitively private (SP)
and DP mechanisms for the data. The plots give the interpo-
lated results to clarify the relationship of outlyingness and
accuracy. (d) and (e) give the privacy (on vertical axis) for