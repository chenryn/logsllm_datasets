terfaces, and a source route has the eﬀect of overriding these
arrangements. Why should they be enthusiastic about this?
Since source routes don’t work eﬀectively today, researchers
propose even more indirect ways of getting around provider-
selected routing, such as exploiting hosts as intermediate
forwarding agents. (This kind of overlay network is a tool
in the tussle, certainly.) Another, perhaps simpler, approach
is to compensate the provider for carrying the packets. But
this idea tends to upset designers as well as customers, be-
cause they fear they will end up in an onerous “pay by the
byte” situation, which does not seem to have much market
appeal.
• The design for provider-level source routing must in-
corporate a recognition of the need for payment. There
4In particular, today’s loose source routes, even if widely
implemented, would provide only a small portion of what is
needed.
351must be enough generality in the payment schemes
that the market can select an outcome that works for
all parties. (Remember, we are not designing the out-
come, only the playing ﬁeld for the tussle.)
• Overlay architectures should be evaluated for their
ability to isolate tussles and provide choice. A com-
parison is warranted between overlay architectures and
integrated global schemes to understand how each bal-
ances the relative control that providers and consumers
have, and whether economic distortion is greater in one
or the other.
3.2 Trust
One of the most profound and irreversible changes in the
Internet is that by and large, many of the users don’t trust
each other. The users of the Internet no longer represent
a single community with common motivation and shared
trust. There are parties with adverse interests, and some
genuine “bad guys” out there. This implies that mechanisms
that regulate interaction on the basis of mutual trust should
be a fundamental part of the Internet of tomorrow.5
Most users would prefer to have nothing to do with the
bad guys. They would like protection from system penetra-
tion attacks, DoS attacks, and so on. This is a profound
tussle, between people who want to be left alone, and peo-
ple who want to bother them. Since host security today is
of variable and mostly poor quality, this desire for protec-
tion leads to ﬁrewalls. Firewalls change the Internet from a
system with transparent packet carriage between all points
(what goes in comes out), to a “that which is not permitted
is forbidden” network. This is a total reversal of the In-
ternet philosophy, but pure transparency is not what most
users long for. For over ten years, Internet purists have been
bemoaning the fact that ﬁrewalls inhibit innovation and the
introduction of new applications (ﬁfteen years ago they were
called “mail gateways”), but ﬁrewalls have not gone away.
The principle of “design for choice” would imply that users
should be able to choose with whom they interact, and users
should be able to choose the level of transparency they oﬀer
to other users. The principle of “tussle isolation” suggests
that these mechanisms should not be overloaded on to any
other mechanism, but should be separated. Further, one
should consider if within the broad topic of trust, there are
separable issues.
The ﬁrst topic is control over which parties are willing to
exchange packets with each other.
• In the abstract, there is a technical question as to
whether each end-node can implement suﬃcient trust-
related controls within itself, or whether delegation
of this control to a remote point inside the network
is required—a “trust-aware ﬁrewall”. As a practical
matter, the market calls for ﬁrewalls. Firewalls of the
future must be designed so that they apply constraints
based on who is communicating, as well as what proto-
cols are being run and where in the network the parties
are. Such a device would imply the design of new pro-
tocols and interfaces, to allow the end node and the
control point to communicate about the desired con-
5A thoughtful analysis of trust that has shaped our thinking
is provided by [10].
trols.6 Issues of choice arise: who gets to pick which
ﬁrewall a user uses?
• To prevent DoS attacks, protocols could be changed so
that end-nodes do not have to establish state or oth-
erwise invest eﬀort until they have veriﬁed that they
want to talk to the party initiating communication.
This concept is challenging, ﬁrst because of a diﬃ-
cult balance between cost and function, and second,
because the idea of “ﬁrst packet trust veriﬁcation” is
at odds with the layered model of protocols, in which
transport establishes a connection before any higher
level information is exchanged.
Another tussle about ﬁrewalls is worth noting. Who gets
to set the policy in the ﬁrewall? The end user may certainly
have opinions, but a network administrator may as well.
Who is “in charge”? There is no single answer, and we bet-
ter not think we are going to design it. All we can design is
the space for the tussle. But this illustrates the point about
visibility of decision-making. If a system administrator has
installed control rules in a ﬁrewall that aﬀect an end user,
should that end user be able to download and examine these
rules? One way to help preserve the end to end character of
the Internet is to require that devices reveal if they impose
limitations on it. However, there is no obvious way to en-
force this requirement, so it becomes a courtesy, not a real
requirement.
Another dimension of trust is the fact that most users
don’t trust many of the parties they actually want to talk
to. We connect to web sites but are suspicious that they are
gathering information on us, stealing our credit cards, not
going to deliver what they promised, and so on. In this case,
the solution is more complex; we depend on third parties to
mediate and enhance the assurance that things are going
to go right. Credit card companies limit our liability to
$50, or sometimes nothing, in case of dispute. Public key
certiﬁcate agents provide us with certiﬁcates that assure us
we are talking to the party we think we are. Web sites assess
and report the reputation of other sites. The fact of these
third parties contrasts with our simple model of two-party
end-to-end communication among trusting parties. Each
individual interaction may be two-party end-to-end, but the
application design is not.
• An important engineering principle for future applica-
tions is that there should be explicit ability to select
what third parties are used to mediate an interaction,
and to act as an agent for the end-user in improving
his trust in the operation. The parties must be able
to choose, so they can select third parties that they
trust.7
Another space in which trust is eroding is that users less
and less trust the software they have to run. They suspect
6The IETF is working on such standards, e.g. the MIDCOM
working group.
7An interesting debate relevant to this topic centers on the
IETF proposal to charter the Open Pluggable Edge Services
(OPES) working group, and the IAB deliberation on policy
concerns. The IAB has focused on issues of whether one end
or both have to concur with the insertion of an intermediate
node in the communication, and what tools the user should
have to detect and recover from a faulty node.
352their operating system and browser of gathering information
on them and passing it on without their knowledge, or turn-
ing them in for software license violations. There are web
sites that claim to look at the outgoing data stream from
the user’s machine and detect and remove any information
that is leaking out.
• This problem may best be dealt with using non-techni-
cal means—regulation, public opinion and so on. Just
because a problem manifests in a technical space, it
does not mean it has to be solved there. But it is an
interesting exercise to consider whether there are tech-
nical means to protect a user from software running on
their own machine. The history of mandatory security
controls and security kernels suggests that this prob-
lem is thorny.
3.2.1 The role of identity
One obvious point about trust is that if communication is
to be mediated based on trust, then as a preliminary step,
parties must be able to know to whom they are talking.
Otherwise, one has little basis for judging how much to trust
others.
One could take this as a call for the imposition of a global
namespace of Internet users, with attached trust assess-
ments. We believe this is a bad idea. It is hard to imagine
a global system that is really trustworthy. More impor-
tantly, there are lots of ways that parties choose to identify
themselves to each other, many of which will be private to
the parties, based on role rather than individual name, etc.
What is needed is a framework that translates these diverse
ways into lower level network actions that control access.
This implies a framework for talking about identity, not a
single identity scheme. We suggest that such a framework
could usefully share and arbitrate information across many
layers of the protocol stack.
The need to know to whom we are talking will challenge a
current precept of the Internet, which is that it is permissible
to be anonymous on the Internet. There is a fundamental
tussle between the ideas of anonymous action, and the idea
that in a society where “that which is not forbidden is per-
mitted”, one can be held accountable for ones actions. A
possible outcome of this tension is that while it will be pos-
sible to act anonymously, many people will choose not to
communicate with you if you do, or will attempt to limit
what you do.8 A compromise outcome of this tussle might
be that if you are trying to act in an anonymous way, it
should be hard to disguise this fact. This illustrates the
observation that one must think about whether the conse-
quences of choice are visible, or can be hidden.
3.3 The tussles of openness
One of the most profound fears for the Internet today is
that it will lose its “open” qualities: the openness to inno-
vation that permits a new application to be deployed, the
openness of access that allows a user to point their Web
browser at any content they please, the openness that al-
lows a user to select the servers and services that best meet
their needs.
8An analog is the current situation with Caller ID, where
a sender can block the caller’s information, but the receiver
can refuse to accept calls from a sender that does.
The openness to innovation—to new applications and new
uses—has perhaps been the most critical success factor for
the Internet. But openness is not an unalloyed virtue for
service providers. Openness often equates to competition,
which creates the fear factor that demands costly investment
and drives proﬁts to a minimum. Many telephone company
executives remember the good old monopoly days, with a
comfortable regulated rate of return and no fear. And many
current ISPs may long for a return to those less open, high
margin days, if they could only ﬁgure out how to get there.
The keys are closed or proprietary interfaces and vertical
integration.
Motivations concerning open vs. proprietary systems have
much to do with economics. Economists have studied the
motivation of providers with various degrees of market power
to choose open or proprietary interfaces; see [6]. Industry
understands that interfaces, or lack thereof, can shape a
market.9 There is probably a whole paper on the tussles
surrounding open vs. closed systems. However, as a start-
ing point, the ﬁrst exercise should be to speculate about
whether these various openness tussles can be modularized
and disentangled, and what this means for mechanism de-
sign.
Vertical integration—the bundling together of infrastruc-
ture and higher-level services—requires the removal of cer-
tain forms of openness. The user may be constrained to
use only certain providers of content, or to pay to run cer-
tain protocols, and so on. However, vertical integration has
nothing to do with a desire to block innovation. Even in a
market with a high degree of vertical integration, innovation
that brings new value to the customer is likely to beneﬁt all
parties. So it would be wise to separate the tussle of vertical
integration, about which many feel great passion, from the
desire to sustain innovation.
The technical characteristic of the network that has fos-
tered innovation is transparent packet carriage—the ability
to deploy a new protocol without having to modify the inside
of the network. But transparency is not the same thing as
openness, though they are related. With this brief motiva-
tion, we consider some old design principles of the Internet,
including the principle that is usually equated with trans-
parency, the end to end arguments.
4. REVISITING OLD PRINCIPLES
4.1 The future of the end to end arguments
One of the most respected and cited of the Internet design
principles is the end to end arguments, which state that
mechanism should not be placed in the network if it can be
placed at the end node, and that the core of the network
should provide a general service, not one that is tailored to
a speciﬁc application [11]. There are two general dimensions
to the arguments: innovation and reliability.
9While technical network designers may not think about
open interfaces as a tool to drive market structure, indus-
trial players understand this fully. When then Senator Gore
announced his vision for a National Information Infrastruc-
ture (NII) in the early 1990s, at least two organizations pro-
duced requirement documents for the “critical interfaces”
that would permit the NII to have a suitable structure [4,
5, 2].
353Innovation: If the core of the network has been tailored to
one speciﬁc application, this may inhibit the deployment of
other applications. If the core of the network must be modi-
ﬁed to deploy a new application, this puts a very high hurdle
in front of any unproven idea, and almost by deﬁnition, a
new idea is unproven.
Reliability and robustness: If bits of applications are “in
the network”, this increases the number of points of failure
that can disable the application. The more simple the core
of the network, the more reliable it is likely to be.
The simplest application of the end to end arguments pro-
duces a network that is transparent : packets go in, and they
come out, and that is all that happens in the network. This
simple idea was very powerful in the early days of the Inter-
net, but there is much fear that it seems to be eroding, for
many of the reasons discussed above:
• The lost of trust calls for less transparency, not more,
and we get ﬁrewalls.
• The desire for control by the ISP calls for less trans-
parency, and we get application ﬁltering, connection
redirection, and so on.
• The desire of third parties to observe a data ﬂow (e.g.
wiretap) calls for data capture sites in the network.
• The desire to improve important applications (e.g. the
Web), leads to the deployment of caches, mirror sites,
kludges to the DNS and so on.