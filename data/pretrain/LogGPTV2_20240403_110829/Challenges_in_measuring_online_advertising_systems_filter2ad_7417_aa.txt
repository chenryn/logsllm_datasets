title:Challenges in measuring online advertising systems
author:Saikat Guha and
Bin Cheng and
Paul Francis
Challenges in Measuring Online Advertising Systems
Saikat Guha
Microsoft Research India
Bangalore, India
PI:EMAIL
Bin Cheng, Paul Francis
Max Planck Institute for Software Systems
Kaiserslautern-Saarbruecken, Germany
{bcheng,francis}@mpi-sws.org
ABSTRACT
Online advertising supports many Internet services, such as
search, email, and social networks. At the same time, there
are widespread concerns about the privacy loss associated
with user targeting. Yet, very little is publicly known about
how ad networks operate, especially with regard to how they
use user information to target users. This paper takes a
ﬁrst principled look at measurement methodologies for ad
networks.
It proposes new metrics that are robust to the
high levels of noise inherent in ad distribution, identiﬁes
measurement pitfalls and artifacts, and provides mitigation
strategies. It also presents an analysis of how three diﬀer-
ent classes of advertising — search, contextual, and social
networks, use user proﬁle information today.
Categories and Subject Descriptors
C.4 [Performance of Systems]: Measurement techniques;
K.4.1 [Computers and Society]: Public Policy Issues—
Privacy
General Terms
Experimentation, Measurement
Keywords
Advertising, Privacy, Behavioral Targeting, Contextual, Churn,
Similarity, Google, Facebook
1.
INTRODUCTION
Online advertising is a key economic driver in the Inter-
net economy, funding a wide variety of websites and services.
At the same time, ad networks gather a great deal of user
information, for instance users’ search histories, web brows-
ing behaviors, online social networking proﬁles, and mobile
locations [6–8]. As a result, there are widespread concerns
about loss of user privacy. In spite of all this, very little is
publicly known about how ad networks use user information
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’10, November 1–3, 2010, Melbourne, Australia.
Copyright 2010 ACM 978-1-4503-0057-5/10/11 ...$10.00.
to target ads to users. For instance, Google recently started
allowing advertisers to target ads based not just on keywords
and demographics, but on user interests as well [9]. Know-
ing how well Google and others are able to determine user
characteristics is an important consideration in the ongoing
public debate about user privacy.
If an ad network is able to accurately target users, we can
deduce that the ad network is able to determine user charac-
teristics (though the inverse does not follow). Given then the
goal of determining how well ad networks can target users,
the high-level methodology is straightforward. Create two
clients that emulate diﬀerent values of a given user charac-
teristic (i.e. location or gender), and then measure whether
the two clients receive diﬀerent sets of ads as a result. If the
ads are identical, we can trivially conclude the ad network
doesn’t use that user characteristic for targeting (although it
might still be storing the data). But the outcome is unclear
if the two sets are diﬀerent: the diﬀerence may genuinely be
due to the diﬀerence in characteristic, or it may be due to
noise.
As it turns out, the level of noise in measuring ads is
extremely high. Even queries launched simultaneously from
two identically conﬁgured clients on the same subnet can
produce wildly diﬀerent ads over multiple timescales. As we
show later, some of this noise is systemic (e.g. DNS load-
balancing), and can therefore be eliminated through proper
experiment design. Other noise has a temporal component,
which likely reﬂects ad churn, the constant process of old ads
being deactivated and new ads being activated. We design
a metric that mitigates the noise from churn.
Overall this paper makes two contributions. First, we
present the detailed design of a measurement methodology
for measuring online advertising that is robust to the high
levels of noise inherent in today’s systems. We present a set
of guidelines for researchers that wish to study advertising
systems. Second, we present an analysis of the key factors
that determine ad targeting on Google and on Facebook.
2. MEASUREMENT METHODOLOGY
In this section we present the detailed design of our mea-
surement methodology. We face four key challenges: 1) com-
paring individual ads, 2) collecting a representative snapshot
of ads, 3) quantifying diﬀerences between snapshots while
being robust to noise, and 4) avoiding measurement arti-
facts arising from the experiment design.We present our de-
sign decisions and justify each using measurement data. The
methodology we design applies to text-ads in any context
including ads in search results, contextual ads on webpages
81Instance A:
Instance B:
Red Prom Dresses
Win a Free Dress for Prom 2010.
Find New Trends; Great Prices!
DavidsProm.com
Red Prom Dresses
Beautiful Designer Prom Dresses
to Fit Every Figure; Price Range.
DavidsProm.com
MD5(RedirURL): 8ebc. . . 45dc
Dest: . . . detail.jsp?i=2462
MD5(RedirURL): 3646. . . 85d3
Dest: . . . detail.jsp?i=2462
Approach
RedirURL
DisplayURL
Title + DisplayURL
Title + Summary
All Fashion
Dresses only
% FP % FN % FP % FN
0
7
0
0
38
13
45
68
1
12
0
0
52
10
50
69
Instance C:
Instance D:
Baby Doll Prom Dresses
Win a Free Dress for Prom 2010.
Find New Trends; Great Prices!
DavidsProm.com
Red Prom Dresses
Shop JCPenney For Colorful Prom
Gowns; Dresses From Top Designers.
JCPenney.com/dresses
MD5(RedirURL): 99d0. . . f0bf
Dest: . . . detail.jsp?i=2203
MD5(RedirURL): c12d. . . ce2c
Dest: . . . X6.aspx?ItemId=17bd2fb
Table 1: Examples that complicate uniquely identifying ads.
and webmail systems, and ads on online social networking
pages; additionally, many qualitative aspects of our design
may also apply to banner ads.
2.1 Comparing Individual Ads
Before we can compare sets of ads, we ﬁrst need the ability
to identify the same ad in diﬀerent scenarios. The problem is
hard because ad networks typically do not reveal the unique
ID for the ad (except for Facebook), and some (like Bing)
even go so far as to obfuscate or encrypt parameters in the
click URL denying scraping based measurement approaches
any visibility into internal parameters. The only data avail-
able consistently across ad networks is the content of the ad,
and even there the extensive abilities to customize it (e.g.
for Google), makes it hard to identify diﬀerent instances of
the same ad. Since slight variations of the same ad defeat
simple equality tests, heuristics must be used and their false
positive and false negative behavior must be understood.
Consider, for example, the four ads illustrated in Table 1.
Instances A and B are semantically equivalent, mutually ex-
clusive (i.e. never both served for the same request), and
lead to the same page on the advertiser’s website (despite
having diﬀerent redirect URLs1). Instances A and C appear
to be from the same template, but are semantically diﬀerent
and lead to diﬀerent pages.
Instances A and D are from
diﬀerent advertisers altogether.
Ideally, instances A and B would be considered equivalent,
and diﬀerent from both C and D. This cannot be achieved
with simple equality tests on any single ad attribute (title,
summary, display URL1, redirect URL); other examples not
presented here demonstrate the presence of false positives
or false negatives for equality tests on combinations of at-
tributes.
Experiment 1: Since comparison errors are unavoidable,
we analyze false positives and false negatives of diﬀerent ap-
proaches to comparing ads in order to pick the best approach
as well as provide a bound on analysis errors. We consider
4 approaches: 1) equality of the redirect URL, 2) equality
of the display URL, 3) equality of the ad title and display
URL, and 4) equality of the ad title and summary text with
all occurrences of the search keywords masked. Note: we
cannot consider the destination URL for the ad since that is
1Display URL (e.g. DavidsProm.com) is the URL displayed
to the user. Redirect URL (RedirURL), is the URL the user
is actually redirected to when he clicks the ad. The redirect
URL is longer and less user-friendly, containing deep path
information and URL parameters. The destination URL
(Dest) where the user is eventually taken to may be diﬀerent
from the redirect URL when multiple redirects are involved.
Table 2: False positives (FP) and false negatives (FN) of various
approaches for uniquely identifying ads.
revealed only after the ad is clicked (and clicking on the ad
in an automated matter constitutes fraud). We apply each
approach to all pairs of ads in two datasets of ads scraped
from Google search results. For estimating false positives,
we manually check a sampling of pairs ﬂagged as equal by
the comparison approach. For false negatives, we manually
check pairs ﬂagged as diﬀerent by the comparison approach;
to focus manual analysis on pairs likely to be false negatives,
we examine those ﬂagged as equal by one of the other ap-
proaches, thus providing a lower bound. The ﬁrst dataset
contains ads for search queries related to fashion in general
(clothing, shoes, accessories, etc.), while the second dataset
restricts the queries to only dress-related.
Table 2 summarizes the false positives and false negatives
for each comparison approach based on manual analysis of
100 ad-pairs ﬂagged by each approach. Except when com-
paring display URLs, there are (almost) no false positives.
The false positives for display URLs arise from some stores
using the same display URL for a wide range of products
(e.g. target.com instead of target.com/mens). The display
URL does, however, have signiﬁcantly lower false negatives
in both datasets. This is because advertisers often have mul-
tiple variations of the same ad. This includes diﬀerent title,
summary, and a diﬀerent redirect URL to measure how each
variation performs; comparing ads based on these is there-
fore more error prone. The display URL, in contrast, is more
stable across variations of the same ad.
False negatives have the eﬀect of over-counting the num-
ber of unique ads and in the process, increasing noise. False
positives, on the other hand do not increase noise, but under-
count the number of unique ads. In this paper we perform
all analysis relative to a control experiment (aﬀected equally
by false positives or negatives) to mitigate the eﬀect of over-
counting or under-counting. This leaves added noise (for
false negatives) as the only diﬀerentiating factor. In this pa-
per we therefore use the display URL, which has signiﬁcantly
lower false negatives and slightly higher false positives, to
compute uniqueness of ads.
2.2 Taking a Snapshot
There typically are more ads than can be displayed on
a single page (search result, or adbox in a webpage). A
single query therefore reveals incomplete information. The
actual subset returned depends on the ad network, but likely
takes into account the ad value (based on auctions), and
frequency capping (not showing the same ad to the same
user too often). Reloading the page multiple times typically
reveals more ads, but runs the risk of capturing inaccurate
snapshots in the presence of ad churn.
Experiment 2: To determine how many times the page
should be reloaded to capture accurate and complete snap-
shots we perform the following experiment. We reload the
Google search results for a given query every 5 seconds for
5 minutes; the experiment is repeated for over 200 diﬀerent
82)
s
d
a
e
u
q
n
U
i
(
F
D
C
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
 10
 20
 30
 40
 50
 60
# reloads
Figure 1: Reloading the page initially reveals more ads that
didn’t originally ﬁt. Beyond a point, however, ad churn overtakes
diminishing returns of reloading the page. 95-5 error bars.
search queries (chosen randomly from a set of 5000 keywords
scraped from Google Product Search2).
Figure 1 plots the CDF of unique ads for the median
search query, with error bars marking oﬀ the 95th and 5th
percentiles. The graph comprises a steep increase (5 reloads)
where we discover most ads that didn’t ﬁt the ﬁrst time,
after which point diminishing return kicks in.
Instead of
ﬂattening out, however, beyond 10 requests the graph be-
comes linear. This gradual linear increase, we believe, is
evidence of a constant rate of ad churn where new ads are
constantly activated and old ones deactivated. The high
rate of churn (1–4% per minute depending on the search
keyword) is consistent with [5] where we found that roughly
only 60% of ads are stable (hour-to-hour, and day-to-day)
while the rest change rapidly. The knee of the graph (around
10 reloads) represents the point at which the diminishing re-
turns of reloading the page is overtaken by ad churn.
In our experiments we therefore balance completeness and
churn by reloading the page 10 times when collecting snap-
shots. We also keep track of the number of times each ad was
seen (if multiple reloads contain the ad) and the positions
where the ad was seen.
2.3 Quantifying Change
The simplest approach to comparing two snapshots is to
compute the set overlap. The Jaccard index quantiﬁes this
overlap as |A∩B|
|A∪B| where A and B are the sets of unique ads in
the two snapshots; 0 implies no overlap and 1 implies iden-
tical snapshots. While simple, the Jaccard index is highly
susceptible to noise (ﬂeeting ads); each unique ad is weighed
equally whether it was seen only once or seen many times.
The typical way of dealing with noise is to look at aggregate
behavior. Taking the union of multiple snapshots, however,
makes the situation worse by also aggregating the noise.
The extended Jaccard index (also called cosine similar-
ity) addresses this limitation by interpreting the two sets
as vectors in n-dimensional space (where each set element
deﬁnes one dimension), and the coeﬃcient of the vector in
that dimension is some weight function (w) based on the
¯A· ¯B
k ¯Akk ¯Bk where ¯A = [wA,e];
element. The metric is deﬁned as
wA,e is some non-zero weight if ad e exists in A, or 0 if it
doesn’t. As before, the metric evaluates to 0 for dissimilar
snapshots, and to 1 for identical snapshots. We explore three
approaches to picking the non-zero weight: 1) the number of
page reloads containing the ad, 2) the logarithm of the same,
and 3) the number of page reloads scaled by the “value” of
the ad; the value, deﬁned in [4], is based on the ad’s position
with ads near the top of the page getting more weight than
2http://www.google.com/products
e
r
o
c
s
y
t
i
r
a
l
i
m
S
i
e
r
o
c
s
y
t
i
r