proximity. To achieve this, we generate 20 random walks
of length 10 for every node, and generate a 128-dimension
embedding for each node based on a context window size of
2. This means each node will only consider a neighborhood
of 2-hop neighbors in the embedding process. Our anomaly
detection threshold is set at δ = 0.1.
Graph Learning with Global View (GL-GV). This is
our second graph learning variant which is very similar to the
ﬁrst, however this time conﬁgured to have a more global view
of the graph. This means our embeddings and link predictor
will be optimized for nodes that are further apart in our graph.
To that end we used the same conﬁguration as previously,
however now setting the window size to 5. This means nodes
will consider at most 5-hop neighbors during the embedding
and link prediction process, which will give the algorithm a
much broader view of the graph.
Local Outlier Factor (LOF) [2]. For a non-graph-
based machine learning comparison, we implement the LOF
anomaly detection algorithm. The LOF is a density-based
anomaly detection approach, where relative local densities
are compared between each sample, and those which are very
different from their neighbors are considered anomalous. In
order to generate features for this algorithm, we 1-hot encode
the authentication events into an authentication vector con-
taining a dimension for all authenticating entities. For each
event, the dimensions corresponding to the various authenti-
cating entities for that particular record will be set to 1, and all
other dimensions will be 0. We then apply the LOF algorithm
to these vectors to identify anomalies.
Isolation Forest (IF) [15]. This is a second non-graph-
based machine learning comparison technique. The Isolation
Forest algorithm identiﬁes samples that can be easily isolated
from the dataset by simple decision trees as being anomalous.
This is applied to the same authentication vectors as in the
previous LOF method.
Unknown Authentication (UA). This is a more traditional
rule-based heuristic which simply identiﬁes all ﬁrst-time au-
thentication events as anomalous. During the training period,
a list of known authentications is generated for each authen-
ticating entity in the network. During the testing phase, any
authentication event which was not seen during the training
phase is considered as anomalous. After an anomalous result
is generated the ﬁrst time, the authentication event is added
to the set of known authentications for the particular entity.
This way we do not generate repeated anomalies for the same
event.
Failed Login (FL). This is a second traditional rule-based
heuristic which considers all failed login events as anomalous.
As this technique does not requiring any training data, we
only evaluate it on the test portion of the datasets.
4.3 Detection Analysis
Next we apply the six different algorithms discussed previ-
ously and evaluate their ability to detect malicious authenti-
cation in our two datasets. For all techniques, we report the
number of true positives (TP), false positives (FP), as well as
the true positive rate (TPR), and false positive rate (FPR).
PicoDomain. First we apply all techniques to the simulated
PicoDomain dataset. We split the dataset into training and
testing, with the training data consisting of authentication
activity before the APT attack began, and the testing data
containing all other activity. As this is a small dataset focused
on malicious activity, the majority of the time period contains
malicious events. As a result, there was only roughly 20%
clean training data available. Thus our ﬁnal train/test split on
this data was about 20%/80%. For all 6 detection techniques,
we only generate accuracy metrics on the testing dataset.
Table 2 shows the results for all six techniques. Not unsur-
prisingly, the UA detector performed very well, with 100%
TPR, and only 1.5% FPR. This means all of the lateral move-
ment associated with the APT campaign involved systems
which did not have authentication activity during the training
period, a characteristic that is likely only to hold in relatively
small and simulated environments. We can also see that the
failed login (FL) rule generated very few results, and only
managed to detect a single event associated with the malicious
activity. This is due to the fact that the APT campaign did
not involve any brute-force password guessing attempts. The
single failed login is likely due to user error during the attack.
Both ML techniques (LOF and IF) struggled to detect ma-
licious events, with TPRs well below 50%, and FPRs as high
USENIX Association
23rd International Symposium on Research in Attacks, Intrusions and Defenses    263
Table 2: Anomaly Detection Results on PicoDomain Dataset
UA
FL
LOF
IF
Algorithm TP
129
1
41
34
102
102
GL-LV
GL-GV
FP TPR (%)
11
15
19
62
0
0
100
0.8
32
26
80
80
FPR (%)
1.5
2.0
2.5
8.3
0.0
0.0
as 8.3%. This indicates that the pure authentication activity
between entities, without the additional information present
in the graph topology, is not sufﬁcient for detecting lateral
movement.
Our graph learning techniques, GL-LV and GL-GV, per-
formed much better than the comparison ML techniques,
achieving 80% TPR. This shows the strength of the graph
topology for the detection of lateral movement. Additionally,
the graph-learning approaches were able to reduce the FPR
to 0% compared with the 1.5% of the UA detector. A low
false positive rate is critical for anomaly detection techniques,
as will be made clear by the next experiment on the LANL
dataset. Interestingly, we see that the global view and local
view had no effect on the performance. This again is likely
due to the extremely small scale of this dataset. The aver-
age shortest path between any two nodes in the PicoDomain
graph is slightly over 2 hops. This means the additional visi-
bility that the GL_GV detector provides will not contribute
signiﬁcantly more information on the graph structure.
LANL. Here we apply the same 6 detectors to the LANL
Comprehensive Cyber Security Events dataset. In a similar
manner, we split the data into training and testing sets. The
training set consists of 40 days on which no malicious activity
is reported, and the testing set of 18 days with malicious
activity. This is equivalent to roughly 70% training data, and
30% testing data. Due to the large scale of this dataset, it was
necessary that we perform an additional down sampling for
the two ML techniques LOF and IF, which was accomplished
by removing timestamps from the training and testing dataset,
and removing duplicate events. The TPR and FPR for these
two techniques have been adjusted to account for this.
Table 3 shows the results for the six anomaly detectors.
FP
Table 3: Anomaly Detection Results on LANL Dataset
Algorithm TP
TPR (%) FPR (%)
542
31
87
65
503
635
530082
116600
169460
299737
146285
107960
4.4
1.0
9.6
16.9
1.2
0.9
UA
FL
LOF
IF
GL-LV
GL-GV
72
4
12
9
67
85
The impact of scale is readily evident in these results, with a
signiﬁcant number of false positives for all detectors, despite
reasonably small false-positive rates.
We can see that the UA detector performs again reasonably
well, with a signiﬁcant 72% of the malicious authentication
events detected. However, with this real-world dataset, we can
see how noisy this detector is, with a FPR of 4.4% resulting
in over 500,000 false positives. The FL detector again fails
to perform, indicating that for APT style campaigns, simple
failed login attempts are not suitable detectors. Similarly, both
ML approaches generated many false positives, and few true
positives, again showing that simple authentication events
without the added information in the authentication graph are
insufﬁcient for malicious authentication detection.
The two graph learning techniques were able to provide
the best TPR at the least FPR. The GL-LV detector, although
returning less true positives than the simple UA detector,
was still able to detect 67% of the malicious activity, at only
1.2% FPR compared to 4.4% by the UA detector. The best
performing predictor on this dataset is the GL_GV detector,
which was able to detect the most malicious authentication
events with a TPR of 85%, while maintaining the lowest FPR
of 0.9%. For this dataset, the increased context window of
the GL-GV over the GL-LV contributed signiﬁcantly to the
added performance. The average shortest path between any
two nodes in the LANL graph is roughly 4 hops. This explains
why, in this case, the broader view of the GL_GV detector
was able to capture more information from the graph structure
in the node embeddings, resulting in a better performing link
predictor.
It is important to note here that all of the previous experi-
ments were performed on commodity server hardware. Specif-
ically, we utilized a server with two Intel Xeon CPU E5-2683
CPUs, and 512 GB of ram. This provided enough memory
and compute power to run any of the detectors discussed
on the full 58-day LANL dataset in under 6 hours. We be-
lieve that the techniques used here would be supported by the
infrastructure already available to our network defenders.
4.4 Reducing False Positives
As we can see from the previous experiment, and speciﬁ-
cally Table 3, the effect of false positives on the datasets of
the scale found in the real-world can be very detrimental.
Even for the best performing detector, the GL_GV detector,
a false positive rate of 0.9% resulted in over 100,000 indi-
vidual false positive results in the test data. As these results
will ultimately be used by cyber analysts to investigate the
threats, it is important that we do our best to keep the false
positives to a minimum. In this section, we present some of
our observations of the data and results, and design several
ﬁlters to further reduce the false positive rate by nearly 40%,
while reducing true positives by less than 1%.
264    23rd International Symposium on Research in Attacks, Intrusions and Defenses
USENIX Association
Figure 5: Impact of various approaches in reducing the number of false positives returned on the LANL dataset.
Observation 1: The malicious authentication events are
predominantly ﬁrst authentication events.
This observation was made based on the fact that the simple
unknown authentication (UA) detector performed very well
at identifying the malicious events. However, its false positive
rate was far too high to use on its own. Based on this obser-
vation, we use the inverse of this detector as a false positive
ﬁlter. More precisely, all anomalies generated by the graph
learning approach are passed through a ﬁlter based on the
known authentication events. We discard any of the anoma-
lous authentication events that were previously seen during
the training period. This ﬁlter corresponds to the "Known
Auth" ﬁlter in Figure 5. We can see that we achieved about a
10% reduction in false positives, while reducing true positives
by less than 1%.
Observation 2: The malicious authentication events are
predominantly based on user interactions.
Our authentication graph includes interactions between
users and computers, but also interactions between purely
computers. Some of the interactions are possibly associated
with the red team exercise, however, the labeling scheme uti-
lized by LANL only labeled authentication events involving
user accounts as being malicious. Without further details on
exactly what the red team activity entailed, it is impossible
to label other interactions as malicious or benign that could
have been associated with the red team exercise. Based on
this, we modify our anomaly detection algorithm, and again
add a new ﬁlter where the results that are generated and do
not involve at least one user account are discarded. This ﬁlter
corresponds to the "User Only" ﬁlter in Figure 5. We can see
this had a signiﬁcant impact on the results, reducing false
positives by over 20% from the original, while not reducing
the true positives at all.
Observation 3: The malicious authentication events are
predominantly related to speciﬁc user accounts and systems.
This observation makes sense from a practical standpoint.
When an adversary gains access to a network, it is unlikely
that they have multiple initial footholds. Typically a single
foothold would be established, and then access throughout
the network would expand from there. This means that all
of the malicious edges in our authentication graph should be
close together, or even form a connected component in the
graph. Based on this observation, we build a third ﬁlter, where
all of the anomalous results are chained together based on
their shared nodes and edges. Any anomalous results which
do not form a chain with at least one other anomalous event
is discarded. This ﬁlter corresponds to the "Anomaly Chain"
ﬁlter in Figure 5. This resulted again in about a 20% reduction
in false positives from the original, and no reduction in true
positives.
To summarize, the last bars labeled as "Combined" in Fig-
ure 5 represent the results when combining all of the previous
ﬁlters together. We can see this resulted in the best perfor-