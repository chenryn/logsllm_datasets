(4)
where K is a constant that depends on second-order statis-
3/2 for periodic losses); see
tics of the loss process (K =
[1] and cited references.
(cid:2)
3.4 Statistical bandwidth sharing
Consider now the impact of random ﬂuctuations in the
number of TCP connections.
In the rest of the section,
all ﬁgures are bubble diagrams with each bubble depicting
the realized throughput of a ﬂow as a function of its size.
The size given on the x-axis is also represented by the cross-
section of the bubbles in order to reﬂect the relative weights
of mice and elephants.
Non-limiting receive window, same RTT
Figure 2 presents results for a 1 Mbit/s link shared by users
all having the same RTT. The curved envelope appearing in
the top left hand corner is due to the rate limiting eﬀect of
slow start as described in Section 3.2. The ﬁgure shows that
ﬂow throughput for mice tends to be highly variable while
that for elephants is more stable. Note that the dispersion
of realized throughputs is roughly symmetrical about a size
invariant mean of around 500 Kbit/s.
When several permanent TCP connections use the same
bottleneck link, the eﬀect of congestion avoidance is to
share the link bandwidth between them. Assuming all ﬂows
experience the same loss probability p, relation (4) suggests
that bandwidth is shared in inverse proportion to RTT. We
have veriﬁed this by an ns simulation of 20 connections shar-
ing a link of 10 Mbit/s, 10 with an RTT of 50 ms and 10
with an RTT of 100 ms, together with 1 Mbit/s of on-oﬀ
UDP traﬃc (included to attenuate undesirable synchroniza-
tion eﬀects due to the artiﬁcial homogeneity of simulated
connections). Results in Table 1 conﬁrm that on average
the connections indeed share bandwidth in proportion to
RTT and that there is no wasted bandwidth. For these
results we have discarded the initial slow start phase be-
fore calculating the average throughput over a simulated
duration of 1 minute.
RTT
(ms)
50
100
Throughput of TCP ﬂows
(Kbit/s)
Total
(Mbit/s)
646 570 578 605 577 629 642 592 535 667
273 277 376 352 248 320 311 306 252 288
6.04
3.00
Table 1: Bandwidth sharing of a 10 Mbit/s link
with persistent UDP and TCP ﬂows
It is possible to invert (4) to derive a relation p(B). In
other words, if B is known then we can deduce the packet
loss rate p. Now, if we assume that TCP is eﬃcient in using
all the link capacity C and that each connection receives a
(cid:3)
share inversely proportional to its RTT, we can deduce the
packet loss rate (p = (
K/Crtti)2). This observation
is signiﬁcant in that it suggests that it is not necessary to
take account of the complex packet arrival process discussed
in Section 2. The loss rate and the multifractal scaling
behavior both result from the way the congestion avoidance
algorithm shares link bandwidth.
Figure 2: Throughput of TCP transfers (1 Mbit/s
link, demand of 500 Kbit/s)
Limiting receive window, same RTT
Figure 3 corresponds to traﬃc on a link of 10 Mbit/s. In
this case, ﬂow throughput is limited mainly by slow start
and the size of the receive window. An assumed RTT of 100
ms (40 ms maximum queuing delay + 60 ms ﬁxed delay)
with the 40-packet receive window would give a maximum
bandwidth of 3.2 Mbit/s for a very long connection. Real-
ized throughput is mainly limited by the envelope (2) and
(3) which is not strictly deﬁned, however, due to the fact
that RTT is not constant.
Limiting receive window, different RTT
Figure 4 relates to a 10 Mbit/s link shared by two classes
of connections distinguished by their maximum RTT: 50
ms and 100 ms, respectively. Each class contributes a de-
mand of 4 Mbit/s. The darker bubbles correspond to the
longer RTT connections which, as expected, achieve lower
throughput. This discrimination is due both to the diﬀer-
ent slow start envelopes (2) and (3) and to the diﬀerent
shares obtained in congestion avoidance according to (4).
larger ﬂows. We assume here that fair sharing is realized
immediately for all ﬂows whatever their size. Let π(n) be
the probability n ﬂows are in progress at an arbitrary in-
stant and let R(s) be the expected response time of a ﬂow
of size s. Let ρ = λσ/C denote the link load and assume
ρ < 1.
With Poisson ﬂow arrivals, the number of ﬂows in progress
behaves like the number of customers in an M/G/1 proces-
sor sharing queue [22] and we have immediately the well
known results:
π(n) = ρn
R(s) =
(1 − ρ),
s
C(1 − ρ)
.
(5)
(6)
Figure 3: Throughput of TCP transfers (10 Mbit/s
link, demand of 5 Mbit/s)
Figure 4: Throughput of TCP transfers with diﬀer-
ent RTT (10 Mbit/s link, demand of 8 Mbit/s)
The throughput of elephants of both types converges to a
value somewhat less than 2 Mbit/s.
Prior to discussing the signiﬁcance of these results with
respect to statistical bandwidth performance achieved in a
network, we present in the next two sections a number of
mathematical models which help to explain the observed
behavior.
4. MODELING STATISTICAL BANDWIDTH
SHARING
Consider an isolated link of capacity C and assume ﬂows
arrive as a Poisson arrival process of rate λ with a size
drawn independently from a common distribution of mean
σ. Flows are modeled as a ﬂuid whose rate adjusts instantly
in response to changes in the number of ﬂows in progress.
4.1 Fair sharing bottleneck
Note that R(s)/s is the mean of the inverse of the through-
put received by ﬂows of size s. Thus γ(s) = s/R(s) is
the harmonic mean throughput of ﬂows of size s. For the
present system γ(s) is constant and equal to C(1 − ρ). The
latter expression thus also represents the ratio expected size
to expected response time, a measure of overall throughput
performance 2.
It may be veriﬁed from the results of Figure 2 that γ(s)
provides a good approximation for the mean throughput
achieved by TCP, C(1 − ρ) in this case being precisely 500
Kbit/s. While the mean value constitutes a useful esti-
mate for the throughput of long ﬂows, the distribution for
shorter ﬂows is more widely dispersed. This is not surpris-
ing since the throughput of mice is essentially determined
by the (highly variable) number of ﬂows present at their
arrival. On the other hand, the throughput of elephants,
which use all the capacity not used by other, shorter ﬂows,
is approximately equal to the residual capacity C(1 − ρ).
Figure 5 presents results comparable to those of Figure
2 derived from a simulation of the considered ﬂuid system.
Comparison of the ﬁgures conﬁrms that the ﬂuid model
yields approximately the same behavior as TCP induced
statistical sharing with the notable exception of the impact
of slow start on the throughput of short ﬂows.
The above formulas are insensitive to the nature of the
ﬂow size distribution. This is a highly signiﬁcant result
since it shows that ﬁrst order bandwidth sharing perfor-
mance is largely independent of this traﬃc characteristic.
4.2 Fair sharing with limited rate
The maximum throughput of ﬂows on a network link is
frequently limited by external constraints such as the user’s
modem speed, server capacity, bandwidth on other network
links or the TCP receive window size, as illustrated in Fig-
ure 3. Assume all users have a common maximum rate limit
r < C. This bandwidth sharing model can be recognized as
a generalization of the processor sharing queue considered
by Cohen in [9]. Corresponding results derived therein for
π(n) and R(s) are as follows:
n! ( ρC
m!
ρn−m,
π(n) = (1 − ρ)f (ρ) ×
r )n−m,
(cid:1)
for n < m,
for n ≥ m,
(cid:5)
− m)(1 − ρ))
(7)
(8)
(cid:4)
R(s) = s
1
r +
f (ρ)
C(1 − ρ)
(1 − (
C
r
If all ﬂows have the same RTT, TCP tends to share band-
width equally among the ﬂows in progress, at least for the
2For a discussion of alternative measures of throughput per-
formance see [21].
This may be veriﬁed on the simulation results of Figure 4,
for instance. The rate limit imposed by the receive win-
dow W/rtt is equal to 6.4 Mbit/s if rtt = 50 ms and 3.2
Mbit/s if rtt = 100 ms. Corresponding mean throughputs
derived from (8) for each rate limit are 1.9 Mbit/s and 1.6
Mbit/s, respectively. These values correspond reasonably
well to the throughput of elephants as shown in the ﬁgure.
4.3 Unequal sharing
In practice, bottleneck bandwidth is not shared perfectly
fairly. One reason is the impact of diﬀerent round trip
times (see Fig. 4). Another is the fact that some ﬂows may
be transported by more than one TCP connection (e.g.,
with HTTP 1.0). To fully explore the implications of un-
equal sharing is beyond present scope. We note simply that
evaluations using simulation and a discriminatory processor
sharing model [14] reveal the following (see [6, 29]):
• discrimination in realized throughput is signiﬁcant mainly
at loads close to saturation;
• size dependent throughput γ(s) is roughly the same
for all s except for the very largest documents whose
throughput tends to C(1 − ρ);
• throughput performance is roughly insensitive with
respect to the document size distribution;
• naturally, the access rate r plays a signiﬁcant equal-
izing role when r (cid:13) C.
These observations suggest that the broad conclusions we
draw from idealized equal sharing models are likely to be
true also under more realistic assumptions of discriminatory
sharing.
Figure 5: Throughput of ﬂows (1 Mbit/s fair shar-
ing link, 500 Kbit/s demand)
where m denotes the integer part of C/r and
f (ρ) =
(1 − ρ)
(cid:3)m−1
( Cρ
k=0 ( Cρ
r )m/m!
r )k/k! + ( Cρ
r )m/m!
is the probability the link is saturated.
Again, the mean throughput γ(s) = s/R(s) does not de-
pend on the ﬂow size s. Figure 6 shows how γ depends on
ρ with a rate limit r = C/10. It is clear from the ﬁgure
that throughput on a high capacity link for which r (cid:13) C
is equal to r except when the oﬀered load ρ is very close to
1. We have the approximation: γ ≈ min(r, C(1 − ρ)).
t
u
p
h
g
u
o
r
h
t
d
e
t
c
e
p
x
E
1
0.8
0.6
0.4
0.2
0
0
Without rate limit
With rate limit
0.2
0.4
0.6
Offered load
0.8
1
Figure 6: Expected normalized throughput against
oﬀered load in case of fair sharing
Figure 7: Throughput of ﬂows (1 Mbit/s unequal
sharing link, 500 Kbit/s demand, 10:1 bias)
The above results are again insensitive to the ﬂow size dis-
tribution. Unfortunately, this convenient property is lost if
we wish to account for the fact that the rate limit is gener-
ally diﬀerent for every ﬂow and can vary during the trans-
fer, or that bandwidth sharing is not perfectly fair. It is
likely, however, that (8) still provides a good approxima-
tion for the expected response time, at least for elephants.
Figure 7 illustrates the imprecise discrimination realized
when ﬂows of two classes of equal intensity share a bot-
tleneck link with 50% load: class 1 ﬂows receive 10 times
the rate of concurrent class 2 ﬂows. Despite this clear bias,
the throughput performance realized by the two classes is
quite close. The mean throughput predicted by the model
of [14] is 690 Kbit/s for class 1 and 390 Kbit/s for class 2
[6]. Statistical variations obscur this diﬀerence, particularly
for mice.
4.4 A transparent backbone link
We ﬁnally consider the case where the link capacity C
is very large compared to the external rate limits and such
that it is virtually transparent. By this we mean the prob-
ability of the sum of external rate limits of all ﬂows in
progress exceeding link capacity C is negligibly small. This
assumption is reasonable for the large, moderately loaded
links of major backbone providers.
The number of ﬂows in progress is now unconstrained
by the considered link which appears as an M/G/∞ queue.
Flow duration is thus an independent random variable. Let
θ be the mean duration and ν the mean number of ﬂows in
progress. By Little’s law we have: ν = λθ. The number of
ﬂows in progress has the Poisson distribution:
π(n) =
νn
n!
e−ν , for n = 0, 1, ....
(9)
Formula (9) is true for any ﬂow duration distribution and
thus for an arbitrary ﬂow size distribution and rate limit.
5. ACCOUNTING FOR THE REAL FLOW
ARRIVAL PROCESS
In this section we show that most of the analytical results
derived above for Poisson ﬂow arrivals also apply under
much more realistic traﬃc assumptions. Our models apply
to a fair sharing bottleneck link, with or without a common
rate limit, or a transparent backbone link, as introduced in
the previous section.
5.1 A stochastic network
The succession of document transfers and think-times
constituting a session may be represented as a customer
visiting two stations in a stochastic network of the kind
considered notably by Kelly [19]3 (see Figure 8). The ﬁrst
and last station to be visited is the link, and successive
visits to the link are separated by a visit to a think-time
station. Outside arrivals are Poisson and every customer
eventually leaves the network.
Poisson 
session arrivals
end of session 
 flows 
link
think 
time
Figure 8: Flow arrivals modelled as a stochastic
network
An essential characteristic of a customer in a stochastic
network is its “class”. This is a versatile attribute which
3Broadly equivalent results can be derived using the alter-
native formalisms of [2] or [9].
allows us to distinguish diﬀerent kinds of customer as well