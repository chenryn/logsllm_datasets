extracted from user accounts and their activities [8].
Yang et al. used ground-truth provided by RenRen to train
an SVM classiﬁer in order to detect fake accounts [27]. Using
simple features, such as frequency of friend requests, fraction
of accepted requests, and per-account clustering coefﬁcient, the
authors were able to train a classiﬁer with 99% true-positive
rate (TPR) and 0.7% false-positive rate (FPR).
Stringhini et al. utilized honeypot accounts to collect data
describing various user activities in OSNs [28]. By analyzing
the collected data, they were able to build a ground-truth for
real and fake accounts, with features similar to those outlined
above. The authors trained two random forests (RF) classiﬁers
to detect fakes in Facebook and Twitter, ending up with 2%
FPR and 1% false-negative rate (FNR) for Facebook, and 2.5%
FPR and 3% FNR for Twitter.
Wang et al. used a click-stream dataset provided by Ren-
Ren to cluster user accounts into “similar” behavioral groups,
corresponding to real or fake accounts [29]. Using the METIS
clustering algorithm [30] with both session and clicks features,
such as average clicks per session, average session length, the
percentage of clicks used to send friend requests, visit photos,
and share content, the authors were able to calibrate a cluster-
based classiﬁer with 3% FPR and 1% FNR.
Even though feature-based detection scales to large OSNs,
it is still relatively easy to circumvent. This is the case because
it depends on features describing activities of known fakes
in order to identify unknown ones. In other words, attackers
can evade detection by adversely modifying the content and
activity patterns of their fakes, leading to an arms race [31]–
[33]. Also, feature-based detection does not provide any formal
security guarantees and often results in a high FPR in practice.
This is partly attributed to the large variety and unpredictability
of behaviors of users in adversarial settings [13].
With Íntegro, we employ feature-based detection to identify
unknown victims in a non-adversarial setting. The dataset used
to train a victim classiﬁer includes features of only known real
accounts that have either accepted or rejected friend requests
send by known fakes. As real accounts are controlled by benign
users who are not adversarial, a feature-based victim account
classiﬁer is harder to circumvent than a similarly-trained fake
account classiﬁer. As we discuss in Section IV, we only require
victim classiﬁcation to be better than random guessing in order
to outperform the state-of-the-art in fake account detection.
Graph-based detection. As a response to the lack of formal
security guarantees in feature-based detection, the state-of-the-
art in fake account detection utilizes a graph-based approach
instead. In this approach, an OSN is modeled as a graph, with
nodes representing user accounts and edges between nodes rep-
resenting social relationship. Given the assumption that fakes
can establish only a small number of attack edges, the subgraph
induced by the set of real accounts is sparsely connected to
fakes, that is, the cut which crosses over attack edges is sparse.2
Graph-based detection mechanisms make this assumption, and
attempt to ﬁnd such a sparse cut with formal guarantees [34]–
[36]. For example, Tuenti employs SybilRank to rank accounts
according to their perceived likelihood of being fake, based on
structural properties of its social graph [13].
Yu et al. were among the ﬁrst to analyze the social graph
for the purpose of identifying fake accounts in OSNs [16], [17].
The authors developed a technique that labels each account as
either fake or real based on multiple, modiﬁed random walks.
This binary classiﬁcation is used to partition the graph into two
smaller subgraphs that are sparsely interconnected via attack
edges, separating real accounts from fakes. They also proved
that in the worst case O(|Ea| log n) fakes can be misclassiﬁed,
where |Ea| is the number of attack edges and n is the number
2A cut is a partition of nodes into two disjoint subsets. Visually, it is a line
that cuts through or crosses over a set of edges in the graph (see Fig. 2).
3
Yang el al. studied the cyber criminal ecosystem on Twit-
ter [44]. They found that victims fall into one of three cate-
gories. The ﬁrst are social butterﬂies who have large numbers
of followers and followings, and establish social relationships
with other accounts without careful examination. The second
are social promoters who have large following-follower ratios,
larger following numbers, and a relatively high URL ratios in
their tweets. These victims use Twitter to promote themselves
or their business by actively following other accounts without
consideration. The last are dummies who post few tweets but
have many followers. These victims are actually dormant fake
accounts at an early stage of their abuse.
III.
INTUITION, GOALS, AND MODEL
We now introduce Íntegro, a fake account detection system
that is robust against social inﬁltration. We ﬁrst present the
intuition behind our design, followed by its goals and model.
A. Intuition
Some users are more likely to become victims than others.
If we can train a classiﬁer to accurately predict whether a user
is a victim with some probability, we can then identify the
cut which separates fakes from real accounts in the graph. As
victims are benign users who are not adversarial, the output
of this classiﬁer represents a reliable information which we
can integrate in the graph. To ﬁnd the cut which crosses over
mostly attack edges, we can deﬁne a graph weighting scheme
that assigns edges incident to predicted victims lower weights
than others, where weight values are calculated from prediction
probabilities. In a weighted graph, the sparsest cut is the cut
with the smallest volume, which is the sum of weights on edges
across the cut. Given an accurate victim classiﬁer, such a cut
is expected to cross over some or all attack edges, effectively
separating real accounts from fakes, even if the number of
attack edges is large. We ﬁnd this cut using a ranking scheme
that ideally assigns higher ranks to nodes in one partition of
the cut than others. This ranking scheme is inspired by similar
graph partitioning algorithms proposed by Spielman et al. [45],
Yu [34], and Cao et al. [13].
B. Design goals
Íntegro aims to help OSN operators in detecting fake ac-
counts using a meaningful user ranking scheme. In particular,
Íntegro has the following design goals:
• High-quality user ranking (effectiveness). The system should
consistently assign higher ranks to real accounts than fakes. It
should limit the number of fakes that might rank similar to or
higher than real accounts. The system should be robust against
social inﬁltration under real-world attack strategies. Given a
ranked list of users, a high percentage of the users at the bottom
of the list should be fake. This percentage should decrease as
we go up in the list.
• Scalability (efﬁciency). The system should have a practical
computational cost which allows it to scale to large OSNs. It
should deliver ranking results in only few minutes. The system
should be able to extract useful, low-cost features and process
large graphs on commodity machines, in order to allow OSNs
to deploy it on their existing computer clusters.
Fig. 2: System model. In this ﬁgure, the OSN is represented as a graph
consisting of 14 users. There are 8 real accounts, 6 fake accounts,
and 5 attack edges. The cut, represented by a dashed-line, partitions
the graph into two regions, real and fake. Victim accounts are real
accounts that are directly connected to fakes. Trusted accounts are
accounts that are known to be real and not victims. Each account has
a feature vector representing basic account information. Initially, all
edges have a unit weight, so user B for example has a degree of 3.
C. System model
As illustrated in Fig. 2, we model an OSN as an undirected
graph G = (V, E), where each node vi ∈ V represents a
user account and each edge {vi, vj} ∈ E represents a bilateral
social relationship among vi and vj. In the graph G, there are
n = |V | nodes and m = |E| edges.
Attributes. Each node vi ∈ V has a degree deg(vi) that is
equal to the sum of weights on edges incident to vi. Moreover,
vi has a feature vector A(vi), where each entry aj ∈ A(vi)
describes a feature or an attribute of the account vi. Each edge
{vi, vj} ∈ E has a weight w(vi, vj) ∈ (0, 1], which is initially
set to w(vi, vj) = 1.
Regions. The node set V is divided into two disjoint sets, Vr
and Vf , representing real and fake accounts, respectively. We
refer to the subgraph induced by Vr as the real region Gr,
which includes all real accounts and the friendships between
them. Likewise, we refer to the subgraph induced by Vf as the
fake region Gf . The regions are connected by a set of attack
edges Ea between victim and fake accounts. We assume the
OSN operator is aware of a small set of trusted accounts Vt,
which are known to be real accounts and are not victims.
IV. SYSTEM DESIGN
We now describe the design behind Íntegro. We start with
a short overview of our approach, after which we proceed with
a detailed description of each system component.
A. Overview
Íntegro extracts low-cost features from user-level activities
in order to train a classiﬁer to identify unknown victims in the
social graph. We refer to these accounts as potential victims,
as there are probabilities attached to their labels. Íntegro then
calculates new edge weights from prediction probabilities such
that edges incident to identiﬁed victims have lower weights
than others. Finally, Íntegro ranks user accounts based on the
landing probability of a modiﬁed random walk that starts from
a trusted account picked at random. The walk is “short,” as it is
5
Real!Trusted!Victim!Fake!Attack!edge!Real region!Fake region! Gender      #Friends                       #Posts!Male!3!… !10!Feature vector of B!B(terminated early before it converges. The walk is “supervised,”
as it is biased towards traversing nodes which are reachable via
higher-weight paths. This short, supervised random walk has
a higher probability to stay in the real region of the graph, as
it is highly unlikely to escape into the fake region in few steps
through low-weight attack edges. Accordingly, Íntegro assigns
most of the real accounts a higher rank than fakes.
B. Identifying potential victims
For each user vi, Íntegro extracts a feature vector A(vi)
from its recent user-level activities. A subset of feature vectors
is selected to train a binary classiﬁer to predict whether each
user is a victim and with what probability. As attackers have
no control over victims, such a victim classiﬁer is inherently
more resilient to adversarial attacks than similarly-trained fake
account classiﬁer. Let us consider one concrete example. In the
“boiling-frog” attack [31], fake accounts can force a classiﬁer
to tolerate abusive activities by slowly introducing similar
activities to the OSN. Because the OSN operator has to retrain
deployed classiﬁers in order to capture new behaviors, a fake
account classiﬁer will learn to tolerate more and more abusive
activities, until
the attacker can launch a full-scale attack
without detection [7]. For victim prediction, on the other hand,
this is possible only if the accounts used for training have been
hijacked. This situation can be avoided by manually verifying
the accounts, as described in Section II-C.
Feature engineering. Extracting and selecting useful features
from user activities can be both challenging and time consum-
ing. For efﬁciency, we seek features that can be extracted in
O(1) time per user. One candidate location for low-cost feature
extraction is the proﬁle page of user accounts, where features
are readily available (e.g., a Facebook proﬁle page). However,
these features are expected to be statistically “weak,” which
means they may not strongly correlate with whether a user is
a victim or not (i.e., the label). As we explain later, we require
the victim classiﬁer to be better than random in order to deliver
robust fake account detection. This requirement, fortunately, is
easy to satisfy. In particular, we show in Section V that an OSN
operator can train and cross-validate a victim classiﬁer that is
up to 52% better than random, using strictly low-cost features.
Supervised learning. For each user vi, Íntegro computes a vul-
nerability score p(vi) ∈ (0, 1) that represents the probability
of vi to be a victim. For a ﬁxed operating threshold α ∈ (0, 1)
with a default value of α = 0.5, we say vi is a potential victim
if p(vi) ≥ α. To compute vulnerability scores, Íntegro uses
random forests (RF) learning algorithm [46] to train a victim
classiﬁer, which given A(vi) and α, decides whether the user
vi is a victim with a score p(vi). We picked this learning
algorithm because it is both efﬁcient and robust against model
over-ﬁtting [47]. It takes O(n log n) time to extract n low-cost
feature vectors, each consisting of O(1) features, and train a
victim classiﬁer. It also takes O(n) to evaluate node scores,
given the trained classiﬁer and users’ feature vectors.
C. Integrating victim predictions and ranking users
To rank users, Íntegro computes the probability of a modi-
ﬁed random walk to land on each user vi after k steps, where
the walk starts from a trusted user account picked at random.
For simplicity, we refer to the probability of a random walk to
land on a node as its trust value, so the probability distribution
of the walk at each step can be modeled as a trust propagation
process [48]. In this process, a weight w(vi, vj) represents the
rate at which trust may propagate from either side of the edge
{vi, vj} ∈ E. We next describe this process in detail.
Trust propagation. Íntegro utilizes the power iteration method
to efﬁciently compute trust values [49]. This method involves
successive matrix multiplications where each element of the
matrix is the transition probability of the random walk from
one node to another. Each iteration computes the trust distri-
bution over nodes as the random walk proceeds by one step.
Let Tk(vi) denote the trust collected by each node vi ∈ V
after k iterations. Initially, the total trust, denoted by τ ≥ 1,
is evenly distributed among the trusted nodes in Vt:
T0(vi) =
if vi ∈ Vt,
otherwise.
(1)
The process then proceeds as follows:
(cid:26)τ /|Vt|
(cid:88)
0
Tk(vi) =
{vi,vj}∈E
Tk−1(vj) · w(vi, vj)
deg(vj)
,
(2)
where in iteration k, each node vi propagates its trust Tk−1(vi)
from iteration k−1 to each neighbour vj, proportionally to the
ratio w(vi, vj)/ deg(vi). This is required so that the sum of the
propagated trust equals Tk−1(vi). The node vi then collects the
trust propagated similarly from each neighbour vj and updates
its trust Tk(vi). Throughout this process, τ is preserved such
that for each iteration k ≥ 1 we have:
Tk−1(vi) =
Tk(vi) = τ.
(3)
(cid:88)
vi∈V
(cid:88)
vi∈V
Our goal is to ensure that most real accounts collect higher
trust than fake accounts. That is, we seek to limit the portion of
τ that escapes the real region Gr and enters the fake region Gf .
To achieve this property, we make the following modiﬁcations.