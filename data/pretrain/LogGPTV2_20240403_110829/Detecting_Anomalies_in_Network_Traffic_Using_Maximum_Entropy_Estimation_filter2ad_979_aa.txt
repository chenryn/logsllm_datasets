title:Detecting Anomalies in Network Traffic Using Maximum Entropy Estimation
author:Yu Gu and
Andrew McCallum and
Donald F. Towsley
Detecting Anomalies in Network Trafﬁc Using Maximum Entropy Estimation
Yu Gu, Andrew McCallum, Don Towsley
Department of Computer Science,
University of Massachusetts,
Amherst, MA 01003
Abstract
We develop a behavior-based anomaly detection method
that detects network anomalies by comparing the current
network trafﬁc against a baseline distribution. The Max-
imum Entropy technique provides a ﬂexible and fast ap-
proach to estimate the baseline distribution, which also
gives the network administrator a multi-dimensional view
of the network trafﬁc. By computing a measure related to
the relative entropy of the network trafﬁc under observation
with respect to the baseline distribution, we are able to dis-
tinguish anomalies that change the trafﬁc either abruptly
or slowly.
In addition, our method provides information
revealing the type of the anomaly detected. It requires a
constant memory and a computation time proportional to
the trafﬁc rate.
1 Introduction
Malicious abuses of the Internet are commonly seen in to-
day’s Internet trafﬁc. Anomalies such as worms, port scans,
denial of service attacks, etc. can be found at any time
in the network trafﬁc. These anomalies waste network re-
sources, cause performance degradation of network devices
and end hosts, and lead to security issues concerning all In-
ternet users. Thus, accurately detecting such anomalies has
become an important problem for the network community
to solve.
In this paper, we develop a network anomaly detection
technique based on maximum entropy and relative entropy
techniques. Our approach exploits the idea of behavior-
based anomaly detection. We ﬁrst divide packets into
classes along multiple dimensions. A maximum entropy
baseline distribution of the packet classes in the benign
trafﬁc is determined by learning a density model from a
set of pre-labeled training data. The empirical distribution
of the packet classes under observation is then compared
to this baseline distribution using relative entropy as the
metric.
If the two distributions differ, we show that the
packet classes primarily responsible for the difference con-
tain packets related to an anomaly.
The maximum entropy approach described in this work
exhibits many advantages. First, it provides the adminis-
trators a multi-dimensional view of the network trafﬁc by
classifying packets according to a set of attributes carried
by a packet. Second, it detects anomalies that cause abrupt
changes in the network trafﬁc, as well as those that increase
trafﬁc slowly. A large deviation from the baseline distribu-
tion can only be caused by packets that make up an unusual
portion of the trafﬁc. If an anomaly occurs, no matter how
slowly it increases its trafﬁc, it can be detected once the
relative entropy increases to a certain level. Third, it pro-
vides information about the type of the anomaly detected.
Our method requires only a constant amount of memory
and consists solely of counting the packets in the trafﬁc,
without requiring any per ﬂow information.
Our approach divides into two phases. Phase one is to
learn the baseline distribution and phase two is to detect
anomalies in the observed trafﬁc.
In the ﬁrst phase, we
ﬁrst divide packets into multi-dimensional packet classes
according to the packets’ protocol information and desti-
nation port numbers. These packet classes serve as the
domain of the probability space. Then, the baseline dis-
tribution of the packet classes is determined by learning a
density model from the training data using Maximum En-
tropy estimation. The training data is a pre-labeled data
set with the anomalies labeled by a human and in which
packets labeled as anomalous are removed. During the
second phase, an observed network trafﬁc trace is given
as the input. The relative entropy of the packet classes in
the observed trafﬁc trace with respect to the baseline dis-
tribution is computed. The packet classes that contribute
signiﬁcantly to the relative entropy are then recorded. If
certain packet classes continue to contribute signiﬁcantly
to the relative entropy, anomaly warnings are generated and
the corresponding packet classes are reported. This corre-
sponding packet class information reveals the protocols and
the destination port numbers related to the anomalies.
USENIX Association
Internet Measurement Conference 2005  
345
We test the approach over a set of real trafﬁc traces. One
of them is used as the training set and the others are used
as the test data sets. The experimental results show that our
approach identiﬁes anomalies in the trafﬁc with low false
negatives and low false positives.
The rest of the paper is organized as follows. In Sec-
tion 2, we review related work. Section 3 describes how
we classify the packets in the trafﬁc. In Section 4, we in-
troduce the Maximum Entropy estimation technique.
In
Section 5, we describe how to detect anomalies in the net-
work trafﬁc based on the baseline distribution. Section 6
gives experimental results and Section 7 discusses the im-
plementation of the algorithm and related practical issues.
The last section summarizes the whole paper.
2 Related work
A variety of tools have been developed for the purpose
of network anomaly detection. Some detect anomalies by
matching the trafﬁc pattern or the packets using a set of
predeﬁned rules that describe characteristics of the anoma-
lies. Examples of this include many of the rules or policies
used in Snort [12] and Bro [10]. The cost of applying these
approaches is proportional to the size of the rule set as well
as the complexity of the individual rules, which affects the
scalability of these approaches. Furthermore they are not
sensitive to anomalies that have not been previously de-
ﬁned. Our work is a behavior based approach and requires
little computation.
A number of existing approaches are variations on the
In [2], Brutlag uses the Holt
change detection method.
Winter forecasting model to capture the history of the net-
work trafﬁc variations and to predict the future trafﬁc rate
in the form of a conﬁdence band. When the variance of
the network trafﬁc continues to fall outside of the conﬁ-
dence band, an alarm is raised. In [1], Barford et al. use
wavelet analysis to remove from the trafﬁc the predictable
ambient part and then study the variations in the network
trafﬁc rate. Network anomalies are detected by applying a
threshold to a deviation score computed from the analysis.
In [14], Thottan and Ji take management information base
(MIB) data collected from routers as time series data and
use an auto-regressive process to model the process. Net-
work anomalies are detected by inspecting abrupt changes
in the statistics of the data. In [15], Wang et al. take the
difference in the number of SYNs and FINs (RSTs) col-
lected within one sampling period as time series data and
use a non-parametric Cumulative Sum (CUSUM) method
to detect SYN ﬂooding by detecting the change point of
the time series. While these methods can detect anoma-
lies that cause unpredicted changes in the network trafﬁc,
they may be deceived by attacks that increase their trafﬁc
slowly. Our work can detect anomalies regardless of how
slowly the trafﬁc is increased and report on the type of the
anomaly detected.
There is also research using approaches based on infor-
mation theory. In [7], Lee and Xiang study several informa-
tion theoretic measures for intrusion detection. Their study
uses entropy and conditional entropy to help data partition-
ing and setting parameters for existing intrusion detection
models. Our work detects network trafﬁc anomalies that
cause unusual changes in the network trafﬁc rate or content.
In [13], Staniford et al. use information theoretic measures
to help detect stealthy port scans. Their feature models are
based on maintaining probability tables of feature instances
and multi-dimensional tables of conditional probabilities.
Our work applies a systematic framework, Maximum En-
tropy estimation, to estimate the baseline distribution, and
our approach is not limited to locating port scans.
Maximum Entropy estimation is a general technique that
has been widely used in the ﬁelds of machine learning,
information retrieval, computer vision, and econometrics,
etc. In [11], Pietra et al. present a systematic way to in-
duce features from random ﬁelds using Maximum Entropy
technique. In [9], McCallum builds, on [11], an efﬁcient
approach to induce features of Conditional Random Fields
(CRFs). CRFs are undirected graphical models used to cal-
culate the conditional probability of values on designated
output nodes given values assigned to other designated in-
put nodes. And in [8], Malouf gives a detailed comparison
of several Maximum Entropy parameter estimation algo-
rithms. In our work, we use the L-BFGS algorithm imple-
mented by Malouf to estimate the parameters in the Maxi-
mum Entropy model.
3 Packet classiﬁcation
In this section, we describe how we divide packets in the
network trafﬁc into a set of packet classes. Our work fo-
cuses on anomalies concerning TCP and UDP packets. In
order to study the distribution of these packets, we divide
them into a set of two-dimensional classes according to the
protocol information and the destination port number in the
packet header. This set of packet classes is the common do-
main of the probability spaces in this work.
In the ﬁrst dimension, packets are divided into four
classes according to the protocol related information. First,
packets are divided into the classes of TCP and UDP pack-
ets. Two other classes are further split from the TCP packet
class according to whether or not the packets are SYN and
RST packets.
In the second dimension, packets are divided into 587
classes according to their destination port numbers. Port
numbers often determine the services related to the packet
exchange. According to the Internet Assigned Numbers
Authority [6], port numbers are divided into three cate-
gories: Well Known Ports (0 ∼ 1023), Registered Ports
(1024 ∼ 49151), and Dynamic and/or Private Ports
346
Internet Measurement Conference 2005 
USENIX Association
(49152 ∼ 65535). In our work, packets with a destination
port in the ﬁrst category are divided into classes of 10 port
numbers each. Since packets with port number 80 comprise
the majority of the network trafﬁc, they are separated into
a single class. This produces 104 packet classes. Packets
with destination port in the second category are divided into
482 additional classes, with each class covering 100 port
numbers with the exception of the class that covers the last
28 port numbers from 49124 to 49151. Packets with des-
tination port numbers larger than 49151 are grouped into a
single class. Thus, in this dimension, packets are divided
into a total of 104 + 482 + 1 = 587 classes.
Altogether, the set of two-dimensional classes consists
of 4 ∗ 587 = 2348 packet classes. These packet classes
comprises the probability space in this paper. We estimate
the distribution of different packets in the benign trafﬁc ac-
cording to this classiﬁcation, and use it as the baseline dis-
tribution to detect network trafﬁc anomalies.
4 Maximum Entropy estimation of
the
packet classes distribution
Maximum Entropy estimation is a framework for obtaining
a parametric probability distribution model from the train-
ing data and a set of constraints on the model. Maximum
Entropy estimation produces a model with the most ’uni-
form’ distribution among all the distributions satisfying the
given constraints. A mathematical metric of the uniformity
of a distribution P is its entropy:
P (ω) log P (ω).
(1)
H(P ) = −X
ω∈Ω
Let Ω be the set of packet classes deﬁned in the previous
section. Given a sequence of packets S = {x1, . . . , xn} as
the training data, the empirical distribution ˜P over Ω in this
training data is
˜P (ω) = P 11(xi ∈ ω)
n
,
(2)
where 11(X) is an indicator function that takes value 1 if X
is true and 0 otherwise.
Suppose we are given a set of feature functions F =
{fi}, and let fi be an indicator function fi : Ω 7→ {0, 1}.
By using Maximum Entropy estimation, we are looking for
a density model P that satisﬁes EP (fi) = E ˜P (fi) for all
fi ∈ F and has maximum entropy. In [11], it has been
proved that under such constraints, the Maximum Entropy
estimate is guaranteed to be (a) unique, and (b) the same
as the maximum likelihood estimate using the generalized
Gibbs distribution, having the following log-linear form
P (ω) =
1
Z
exp(X
i
For each feature fi, a parameter λi ∈ Λ determines its
weight in the model, Λ is the set of parameters for the fea-
ture functions. Z is a normalization constant that ensures
that the sum of the probabilities over Ω is 1. The difference
between two given distributions P and Q is commonly de-
termined using the relative entropy or Kullback-Leibler (K-
L) divergence:
D(P||Q) = X
ω∈Ω
P (ω) log P (ω)
Q(ω) .
Maximizing the likelihood of the distribution in the form of
(3) with respect to ˜P is equivalent to minimizing the K-L
divergence of ˜P with respect to P
P = arg min
P
D( ˜PkP )
P (ω) 
11(xi∈ω) ∝ exp(−D( ˜PkP )).
as
Y
ω∈Ω
For the sake of efﬁciency, feature functions are often se-
lected to express the most important characteristics of the
training data in the learned log-linear model, and in return,
the log-linear model expresses the empirical distribution
with the fewest feature functions and parameters.
The Maximum Entropy estimation procedure consists of
two parts: feature selection and parameter estimation. The
feature selection part selects the most important features
of the log-linear model, and the parameter estimation part
assigns a proper weight to each of the feature functions.
These two parts are performed iteratively to reach the ﬁ-
nal model. In the following, we describe each part in turn.
More details can be found in [11].
4.1 Feature selection
The feature selection step is a greedy algorithm which
chooses the best feature function that minimizes the dif-
ference between the model distribution and the empirical
distribution from a set of candidate feature functions.
Let Ω be the set of all packet classes, ˜P the empirical
distribution of the training data over Ω, and F a set of can-
didate feature functions. The initial model distribution over
Z , Z = |Ω|, which is a uniform distribution
Ω is P0(ω) = 1
over Ω.
Now let Pi be a model with i feature functions selected
Pi(ω) =
1
Z
exp(
λj fj(ω)).
(4)
iX
j=1
and we want to select the i + 1st feature function. Let g be
a feature function in F\{f1, . . . fi} to be selected into the
model and λg be its weight, then let
Z0 exp(X
1
i
λifi(ω)).
(3)
Pi,λg ,g(ω) =
λifi(ω)) exp(λgg),
(5)
USENIX Association
Internet Measurement Conference 2005  
347
and let
GPi(λg, g) = D( ˜P||Pi) − D( ˜P||Pi,λg ,g)
= λgE ˜P (g) − log EPi(exp(λgg)),
where EP (g) is the expected value of g with respect to the
distribution of P . GPi(λg, g) is a concave function with
respect to λg, and
GPi(g) = sup
λg
GPi(λg, g)
(6)
is the maximum decrease of the K-L divergence that can be
attained by adding g into the model. The feature function g
with the largest gain GPi(g) is selected as the i+1st feature
function to the model.
In [11], it is also shown that for indicator candidate fea-
ture functions, there are closed form formulas related to
the maxima of GPi(λg, g), which makes it computation-
ally easier. For more details on feature selection, please
refer to [11] and [4].
4.2 Parameter estimation
After a new feature function is added to the log-linear
model, the weights of all feature functions are updated.
Given a set of training data and a set of selected feature
functions {fi}, the set of parameters is then estimated.