Prior work, using ns-2, has shown that unpaced TCP traffic
outcompetes paced traffic in terms of throughput [2, 86]. They rec-
ommend pacing at a rate of (ùëêùë§ùëõùëë +1)/ùëÖùëáùëá , which is implemented
by Linux. These fairness concerns suggest that spillover may be
nonzero, which implies that there would be congestion interference
in an A/B test.
We ran pacing A/B tests in our lab to measure whether this
interference still exists and if it would impact the results of an
A/B test. Figure 2b shows the results. Paced traffic (the treatment)
Figure 3: Experiments where 10 TCP connections using Cu-
bic or BBR share a 10 Gb/s link. Throughput is the same if
everyone uses either algorithm, but A/B tests suggest that
both are improvements.
obtains 50% lower throughput than unpaced traffic (the control) in
any A/B test, regardless of allocation. In each A/B test, we observed
essentially no reduction in retransmissions for pacing.
Applying usual A/B testing practice to these results might have
led us to decide not to deploy pacing. However, if we did deploy pac-
ing, we would be pleasantly surprised to see no impact on through-
put and a large decrease in retransmissions. The A/B tests also miss
that pacing is good for other traffic: the spillovers from pacing are
an increase in throughput and a decrease in retransmissions.
Pacing highlights the importance of estimating TTE when ex-
perimenting with networking algorithms. It is not obvious that
pacing changes the way connections compete with each other: we
expected it would smooth out bursts and cause lower RTT and loss
with no impact on throughput. Without careful experiment design,
an experimenter could be easily misled into thinking that pacing is
not useful, or waste effort chasing a non-existent bug.
5
84
IMC ‚Äô21, November 2‚Äì4, 2021, Virtual Event, USA
Spang et al.
3.3 Test 3: Congestion Control Algorithms
There has been extensive study of the fairness of congestion control
algorithms (e.g. [5, 15, 16, 23, 43, 44, 56, 71, 81, 82, 84, 85]). A treat-
ment algorithm is often said to be unfair if it gets a larger share of
throughput when competing against a control algorithm. In terms
of our metrics, this would be if the spillover on control traffic is a
decrease in throughput.
An A/B test will not accurately measure the TTE for an unfair
algorithm. The treatment algorithm will take throughput away
from the control, making the control perform worse than if the
treatment were not present. Most widely-used congestion control
algorithms are known to be unfair to at least some other algorithms
in certain settings. The resulting biases undermine A/B tests on
new congestion control algorithms at scale.
As an example, it‚Äôs been widely reported that BBR is unfair to
Cubic in certain situations [16, 43, 44, 71, 84, 85]. This unfairness
suggests congestion interference, so we ran simulated A/B tests
in our lab. We ran ten long-lived TCP connections, and allocated
some fraction of them to BBR and the rest to Cubic. Figure 3 shows
our results. If we were interested in deploying BBR in this setting
and ran a 10% allocation, we would see a huge improvement in
throughput. If instead we were interested in deploying Cubic and
ran a 10% allocation, we would also see a huge improvement! But
in this setting there is no difference in throughput between a global
allocation to either BBR or Cubic.
4 PAIRED LINK EXPERIMENT WITH
BITRATE CAPPING
In response to the increased network usage during the beginning of
the COVID-19 pandemic, Netflix worked with various governments
to reduce load on the Internet, and rolled out a bitrate capping
program which reduced video quality [30]. This program capped
the video bitrate delivered to clients, while preserving the video
resolution based on their subscription plans. It was observed that
between March and June 2020, capping the bitrate reduced Netflix
traffic in many countries by 25%, and reduced congestion for a
number of ISPs.
In this section, we will describe a controlled experiment we
ran to accurately measure the effects of bitrate capping. Given
that bitrate capping reduced Netflix traffic by 25%, we suspected it
would decrease congestion. Our preceding lab studies also led us
to suspect that standard A/B tests may give biased results. So our
goals with this experiment were to:
(1) Measure the impact of bitrate capping on network perfor-
mance and video quality of experience, by estimating TTE
and spillover effects.
(2) Estimate the bias of na√Øve A/B tests on these measurements,
and
this bias.
(3) Evaluate whether alternate experiment designs would reduce
These are challenging goals to accomplish simultaneously. To
evaluate the bias of a na√Øve A/B test and newer experimental designs,
we need to measure what happens when all traffic is treated. But
if we treat all traffic, we have nothing to compare against! We
could run sequential experiments and compare their results, but
this makes strong assumptions about how the system behaves over
6
85
Figure 4: Diagram of the paired link experiment.
time. These would be useful assumptions to make when running
alternate experiment designs, and we wanted to use this experiment
to evaluate these assumptions.
In this section we describe the experiment we ran to achieve
these goals. In Netflix‚Äôs network, there are a pair of 100 Gb/s peering
links to an ISP. The links are reliably congested during peak hours,
and are statistically very similar. We treat these two links as ‚Äúparallel
universes,‚Äù and can compare the outcomes of different experiments
to investigate A/B test biases and congestion interference.
Our results are striking and sobering. Bitrate capping reduced
congestion at the cost of slightly lower video quality, and improved
the performance of uncapped traffic. This was almost completely un-
detected by na√Øve A/B tests which underestimated some treatment
effects, failed to detect others, and, as we will see, even inferred the
wrong direction of improvement for certain metrics.
4.1 Paired peering links
Netflix has a location with a pair of identical clusters, replicated for
scale and redundancy. Each cluster is identically configured with
a router and a number of cache servers. Each router connects to a
partner ISP via a 100 Gb/s peering link. This setup is depicted in
Figure 4.
During peak viewing hours, demand from users connecting via
this ISP increases until eventually a large standing queue builds up
on both links. Latency increases, and throughput and video quality
decrease. The congestion has a large impact on the quality observed
by traffic, and we suspected strong congestion interference between
connections sharing the same link.
A priori, we are not guaranteed that the two links will be similar
to each other, since the system is optimized to serve video and not
to run experiments. The content available on the two clusters is
not identical, and different traffic is routed to the servers across
each link. To validate statistical similarity between the two links,
we collected data on both links during a week-long baseline period,
comprising over five million sessions: 50.8% on link 1, and 49.2%
on link 2. Netflix collects client- and server-side data on video
performance. We looked at 24 important metrics including ones
related to network performance (throughput, RTT, etc...) and video
QoE (perceptual quality, stability, etc...). For each metric, we used
the analysis approach described in Appendix B to compare links 1
and 2. We will discuss the most relevant subset of these metrics.
We obtained the following results, reported as means and 95%
confidence intervals. Relative to link 2, link 1 had 5% (0.5%-10%)
more overall bytes sent, a 2% (0.1%-3%) higher video stability metric,
ISP‚Ä¶ServersRouterLink 1 TreatmentLink 2‚Ä¶ServersRouterControlUnbiased Experiments in Congested Networks
IMC ‚Äô21, November 2‚Äì4, 2021, Virtual Event, USA
and 0.1% (0.03%-0.25%) lower perceptual quality. The largest differ-
ences were related to rebuffers. Rebuffers are moments when video
playback is interrupted because the client is unable to download
a piece of video from the server. Relative to link 2, link 1 had 20%
(13-27%) more sessions with rebuffers; there were four additional
metrics related to rebuffers that also exhibited similar differences.
All other metrics did not have statistically significant differences.
Notably, we did not see differences in most metrics we will discuss
in our experiment below, including RTT, throughput, video bitrate,
cancelled starts, or packet retransmissions.
Traffic on these links is not perfectly balanced, but it is clearly
quite similar. Although the pre-existing differences in rebuffers is
large, it is important to note that in absolute terms rebuffers are rare.
Given the similarity in other metrics, we believe they are caused
by some other difference, such as the content served on the two
links. Nevertheless, we carefully discuss our experimental findings
regarding rebuffers in Section 4.3, where our observations suggest
this difference in fact causes us to underestimate the extent to which
na√Øve A/B tests are biased.
Being able to run an experiment like this is an extremely un-
usual situation. Operators work hard to avoid persistent congestion,
so it is rare to have a pair of congested peering links. It is even
rarer for the traffic to be balanced, and to be able to run separate
experiments on each link. Netflix has hundreds of locations and
thousands of peering links worldwide, but only two were suitable
for this experiment.
4.2 Experiment design and analysis
We now describe the experiment we ran. Our goal was to estimate
the effects when most traffic was capped, the TTE, and compare this
to the results of A/B tests. We also wanted to measure the spillover
of capped traffic on uncapped traffic.
To accomplish this, we ran a pair of A/B tests on the two links.
On link 1, we allocated 95% of flows to treatment (ùëù = 0.95). On link
2, we allocated 5% to treatment. Computing the na√Øve ÀÜùúè(ùëù) estima-
tor on sessions within each link allows us to calculate ÀÜùúè(0.95) and
ÀÜùúè(0.05). By comparing the mean of the 95% treatment sessions on
link 1 to the 95% control sessions on link 2, we obtain an approximate
estimate of TTE. By comparing the mean of the 5% control sessions
on link 1 to the 95% control sessions on link 2, we can obtain an
approximate estimate of the spillover of capping. With this design,
we ran A/B tests simultaneously on the pair of links. The experi-
ment ran for five days, and included about fourteen million video
sessions. We analyzed the experiment using techniques described
in Appendix B.
In practice, network experiments are usually run in one of two
settings. The first is an initial experiment with a relatively low level
of initial treatment allocation, corresponding to the 5% A/B test.
The second is a long-term holdback test, where almost all traffic
is treated. We might na√Øvely hope that by treating more traffic, we
would reduce congestion interference, and this corresponds to the
95% A/B test.
This experiment may at first appear a bit odd. We are measuring
the difference in behavior when almost all traffic is capped and
almost all is uncapped. This is an interesting quantity which tells us
a lot about the behavior of bitrate capping during congestion, but it
is only an approximation to TTE. The most straightforward way to
estimate TTE in this network would be to cap 100% of sessions on
link 1 as treatment, and uncap 100% of sessions on link 2 as control.
We could then compare the means of each group to estimate TTE.
However, if we did this, we would have no instances where capped
and uncapped traffic shared a link, and we would be unable to
compare the results to an A/B test or measure spillover. We could
run other experiments other times on the links and compare the
results, but we would be making strong assumptions about time
invariance. This would require careful experimental design and
analysis, and one of our goals here was to validate these designs.
Putting it another way: one of our goals is to test the SUTVA
assumption, and check whether treatment effects as measured by
A/B tests give good predictions of what happens when an algorithm
is widely deployed. If SUTVA holds, as in Figure 1a, spillover must
be zero, and there must be no difference between the results of the
two A/B tests and the approximate TTE we measure. If there is any
difference between these quantities in our experiments, SUTVA
cannot hold. Knowing that SUTVA does not hold, we would not
expect slightly increasing the fraction of capped traffic to fix this
problem.
4.3 Results
Our results can be summarized as follows: bitrate capping substan-
tially reduced congestion and improved performance of uncapped
traffic, and yet the na√Øve estimator would have largely failed to
detect this.
Figure 5 reports our estimates of treatment effects and 95% confi-
dence intervals for several important video streaming and network
metrics. We report the results of 5% and 95% Na√Øve A/B test results
(i.e., ÀÜùúè(0.05) and ÀÜùúè(0.95)), as well as our estimate of approximate
TTE and our estimate of spillover. The na√Øve estimators are also
wrong about the direction of improvement for minimum RTT and
average throughput, and the magnitude of average play delay and
video bitrate. The spillover is non-zero for most metrics.
Taking the example of average throughput, the two na√Øve A/B
tests predicted a 5% decrease in throughput, which na√Øvely suggests
that capping increased congestion. However, the TTE tells a very
different story: that capping increased average throughput by 12%.
Spillover shows that capping also benefited other traffic sharing
the link: control traffic on the mostly capped link had 16% higher
throughput than that on the mostly uncapped link.
These results can be explained by the way bitrate capping re-
duced congestion. There was significantly less capped traffic, so
it took a larger number of users for the link to become congested.