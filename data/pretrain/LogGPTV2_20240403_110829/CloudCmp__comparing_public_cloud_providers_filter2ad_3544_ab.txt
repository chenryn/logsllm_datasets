themselves and with the shared services offered by a cloud. The
performance of the network is vital to the performance of dis-
tributed applications. Within a cloud, the intra-datacenter network
often has quite different properties compared to the inter-datacenter
network. Providers vary in the type of network equipment (NICs,
switches) as well as in their choice of routing (layer 2 vs.
layer
3) and conﬁguration such as VLANs. All providers promise high
intra-datacenter bandwidth (typically on the order of hundreds of
Mbps to Gbps), approximating a private data center network.
To compare the performance of intra-cloud networks, we use
path capacity and latency as metrics. We use TCP throughput as a
measure of path capacity because TCP is the dominant trafﬁc type
of cloud applications. Path capacity impacts data transfer through-
put and congestion events can lead to errors or delayed responses.
Path latency impacts both TCP throughput [30] and end-to-end re-
sponse time. Together, these metrics provide insight into how a
provider’s intra-cloud network is provisioned.
None of the providers charge for trafﬁc within their data centers.
Inter-datacenter trafﬁc is charged based on the volume crossing the
data center boundary. Since all providers charge similar amounts,
comparing cost of network transfers becomes a moot point.
3.3.4 Wide-area Network
The wide-area network is deﬁned as the collection of network
paths between a cloud’s data centers and external hosts on the In-
ternet. All the providers we study offer multiple locations to host
4customer applications. Requests from an end user can be served
by an instance close to that user to reduce latency. Uniquely, Ap-
pEngine offers a DNS-based service to automatically map requests
to close-by locations. The others require manual conﬁguration.
We use the optimal wide-area network latency to compare
providers’ wide-area networks. The optimal wide-area network la-
tency is deﬁned as the minimum latency between a vantage point
and any data center owned by a provider. We use locations of Plan-
etLab nodes as vantage points. The more the data centers offered by
a provider and the closer they are to population centers, the smaller
the optimal network latency. The metric is useful for customers
because it corresponds to the network latency an application may
experience given an ideal mapping. For AppEngine, which pro-
vides automatic mapping of requests to locations, we also measure
how close its automatic mapping is to the optimal mapping.
4.
IMPLEMENTATION
In this section, we describe the implementation details of Cloud-
Cmp and highlight the practical challenges we address.
4.1 Computation Metrics
Benchmark tasks. As described above, we would like a suite
of benchmark tasks that stresses various aspects of the compute
infrastructure offered by cloud providers. In traditional computa-
tion performance measurement, any benchmark suite, such as the
SPEC CPU2006 benchmarks [16], would ﬁt this bill. However,
the context of cloud computing poses new constraints. For exam-
ple, AppEngine only provides sand-boxed environments for a few
cross-platform programming languages, and applications have to
be singled-threaded and ﬁnish within limited time.
To satisfy those constraints and be fair across different
providers, we modiﬁed a set of Java-based benchmark tasks from
SPECjvm2008 [17], a standard benchmark suite for Java virtual
machines. We choose Java because it is supported by all cloud
providers. The benchmark suite includes several CPU intensive
tasks such as cryptographic operations and scientiﬁc computations.
We augment it with memory and I/O intensive tasks. Each bench-
mark task runs in a single thread and ﬁnishes within 30 seconds so
as to be compatible with all providers.
Benchmark ﬁnishing time. We run the benchmark tasks on each
of the virtual instance types provided by the clouds, and measure
their ﬁnishing time. Some instances offer multiple CPU cores for
better parallel processing capability. For these instances, we also
evaluate their multi-threading performance by running instances of
the same benchmark task in multiple threads simultaneously, and
measuring the amortized ﬁnishing time of the task. The number
of threads is set to be equivalent to the number of available CPU
cores.
Cost per benchmark. For cloud providers that charge based on
time, we compute the cost of each benchmark task using the task’s
ﬁnishing time and the published per hour price. For AppEngine that
charges by CPU cycles, we use its billing API to directly obtain the
cost.
Scaling latency. We write our own scripts to repeatedly request
new virtual instances and record the time from when the instance
is requested to when it is available to use. We further divide the
latency into two segments: a provisioning latency and a booting
latency. The former measures the latency from when an instance is
requested to when the instance is powered on. The latter measures
the latency from the powered-on time to when the instance is ready
to use. The separation of the two is useful for a cloud provider to
pinpoint the performance bottleneck during instance allocation.
4.2 Storage Metrics
Benchmark tasks. Along with each storage service, comes an API
to get, put or query data from the service. Most APIs are based on
HTTP. To use the APIs, we wrote our own Java-based client based
on the reference implementations from the providers [3,9,21]. The
client has a few modiﬁcations over the reference implementations
to improve latency. It uses persistent HTTP connections to avoid
SSL and other connection set up overheads. It also skips a step in
some implementations in which a client ﬁrst sends a request header
and waits an RTT until the server returns an HTTP 100 (Continue)
message before proceeding with the request body. For comparison,
we also tested other non-Java-based clients such as wget and C#-
based clients. To avoid the potential impact of memory or disk bot-
tlenecks at the client’s instance, our clients mimic streaming work-
load that processes data as it arrives without retaining it in memory
or writing it to disk.
Regarding the benchmark workload, we vary the size of the data
fetched to understand the latency vs. throughput bottlenecks of the
storage service. We vary the number of simultaneous requests to
obtain maximum achievable throughput as well as measuring per-
formance at scale. We vary the size of the working sets to observe
both in- and out-of-cache performance. Because performance will
be impacted by load on the client and in the network, we repeat
each experiment at different times across different locations to get
representative results. We also study the impact of the different
client implementations described above.
Response time. The response time for an operation is the time
from when the client instance begins the operation to when the last
byte reaches the client.
Throughput. The throughput for an operation is the maximum
rate that a client instance obtains from the storage service.
Time to Consistency. We implement a simple test to estimate the
time to consistency. We ﬁrst write an object to a storage service
(the object can be a row in a table, a blob, or a message in a queue).
We then repeatedly read the object and measure how long it takes
before the read returns correct result.
Cost per operation. Similar to cost per benchmark task, we use
the published prices and billing APIs to obtain the cost per storage
operation.
4.3 Network Metrics
We use standard tools such as iperf [8] and ping to measure the
network throughput and path latency of a provider. To measure
intra-cloud throughput and latency, we allocate a pair of instances
(in the same or different data centers), and run those tools between
the two instances. Some providers further divide instances within
the same data center into zones for a ﬁner-grained control of in-
stance location (e.g., not placing all instances in the same failure
domain).
In this case, we also deploy inter-zone and intra-zone
instances respectively to measure their throughput and latency.
To prevent TCP throughput from being bottlenecked by ﬂow
control, we control the sizes of the TCP send and receive windows.
Our measurements show that with a 16MB window, a single TCP
ﬂow is able to use up the available capacity along the paths mea-
sured in this paper. Larger window sizes do not result in higher
throughput. For comparison, we also measure the throughput ob-
tained by TCP clients that use the default window size conﬁgured
by the instance’s operating system.
To measure the optimal wide-area network latency, we instanti-
ate an instance in each data center owned by the provider and ping
these instances from over 200 vantage points on PlanetLab [13].
5i
i
e
m
T
g
n
h
s
n
F
d
e
z
i
i
i
l
a
m
r
o
N
 1.4
 1.2
 1
 0.8
 0.6
 0.4
 0.2
 0
single thread
multiple threads
i
e
m
T
g
n
h
s
n
F
d
e
z
i
i
i
1
.
1
C
2
.
1
C
3
.
1
C
1
.
2
C
2
.
2
C
3
.
2
C
4
.
2
C
3
C
1
.
4
C
2
.
4
C
3
.
4
C
4
.
4
C
i
l
a
m
r
o
N
 1.4
 1.2
 1
 0.8
 0.6
 0.4
 0.2
 0
single thread
multiple threads
1
.
1
C
2
.
1
C
3
.
1
C
1
.
2
C
2
.
2
C
3
.
2
C
4
.
2
C
3
C
1
.
4
C
2
.
4
C
3
.
4
C
4
.
4
C
(a) CPU
(b) Memory
i
i
e
m
T
g
n
h
s
n
F
d
e
z
i
i
i
l
a
m
r
o
N
 1.4
 1.2
 1
 0.8
 0.6
 0.4
 0.2
 0
single thread
multiple threads
1
.
1
C
2
.
1
C
3
.
1
C
1
.
2
C
2
.
2
C
3
.
2
C
4
.
2
C
1
.
4
C
2
.
4
C
3
.
4
C
4
.
4
C
(c) Disk I/O
Figure 1: The ﬁnishing time of benchmark tasks on various cloud instances. The time values are normalized using the longest ﬁnishing time to
emphasize each provider’s relative performance. We show both single-threaded and multi-threaded results. The multi-threaded and I/O results are
missing for C3 because it does not support multi-threading or accessing the local disk.
For each vantage point, the optimal latency to a provider is the
smallest RTT to a data center of the provider. AppEngine automat-
ically replicates our application to multiple instances at different
data centers. By querying the DNS name corresponding to our ap-
plication from each of the PlanetLab vantage points, we collect the
IP addresses of the instance that AppEngine’s automatic mapping
service maps the request from each vantage point. Each of these
IP addresses, we conjecture, corresponds to the virtual IP address
of the front-end load balancer at a data center. We then ping all
of these IP addresses from each of the PlanetLab vantage points to
identify the best mapping that AppEngine might have achieved.
5. RESULTS
In this section, we present the comparison results between the
four providers: AWS, Azure, AppEngine, and CloudServers. Due
to legal concerns, we anonymize the identities of the providers in
our results, and refer to them as C1 to C4. The data was collected
over a two-month period from March to May 2010. We have made
the data available for download from the project website [4].
5.1 Elastic Compute Cluster
We ﬁrst measure the computation performance of different types
of instances offered by cloud providers. Our naming convention
refers to instance types as provider.i where i denotes the tier of
service with lower numerals corresponding to lower cost and com-
putational speed. For instance, C1.1 is the cheapest and slowest
instance type offered by C1.
Table 3 summarizes the instances we measure. We test all in-
stance types offered by C2 and C4, and the general-purpose in-
stances from C1. C1 also provides specialized instances with
more memory or CPU or high bandwidth network, which we do
not include in this study, as they have no counterparts from other
providers. C3 does not offer different instance types, so we use its
default environment to run the benchmark tasks.
Cloud
Provider
C1
C2
C3
C4
< 1
2
4
4
4
4
4
4
4
4
Instance Number
Type
of Cores
C1.1
C1.2
C1.3
C2.1
C2.2
C2.3
C2.4
C2.5
C2.6
C2.7
default
C4.1
C4.2
C4.3