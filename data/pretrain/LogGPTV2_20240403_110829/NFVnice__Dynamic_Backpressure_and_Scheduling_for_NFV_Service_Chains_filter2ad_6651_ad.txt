connected back-to-back with dual-port 10Gbps DPDK compatible
NICs to avoid any switch overheads.
We make use of DPDK based high speed traffic generators, Moon-
gen [12] and Pktgen [38] as well as Iperf3 [11], to generate line
rate traffic consisting of UDP and TCP packets with varying num-
bers of flows. Moongen and Pktgen are configured to generate 64
byte packets at line rate (10Gbps), and vary the number of flows as
needed for each experiment.
We demonstrate NFVnice’s effectiveness as a user-space solution
that influences the NF scheduling decisions of the native Linux
kernel scheduling policies, i.e., Round Robin (RR) for the Real-time
scheduling class, SCHED_NORMAL (termed NORMAL henceforth)
and SCHED_BATCH (termed BATCH) policies in the CFS class.
Different NF configurations (compute, I/O) and service chains with
varying workloads (traffic characteristics) are used. For all the bar
plots, we provide the average, the minimum and maximum values
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
S. Kulkarni et al.
Table 3: Packet drop rate per second
NORMAL
BATCH
RR(1ms)
RR(100ms)
Default NFVnice Default NFVnice Default NFVnice Default NFVnice
3.58M
2.02M
0.86M
2.92M
0.53M
0.03M
0
11.5K
11.2K
12.3K
2M
0.9M
0
12K
0
12K
NF1
NF2
Table 4: Scheduling Latency and Runtime of NFs
RR(100ms)
NORMAL
RR(1ms)
BATCH
measured in ms Default NFVnice Default NFVnice Default NFVnice Default NFVnice
NF1-Avg. Delay
NF1-Runtime
NF2-Avg. Delay
NF2-Runtime
NF3-Avg. Delay
NF3-Runtime
0.112
128.723
0.008
848.922
0.025
1014.218
1.613
143.754
0.255
803.185
0.009
1047.968
0.003
312.703
1.144
836.940
0.149
826.203
0.002
657.825
0.065
602.285
0.045
623.797
1.022
-
0.570
-
0.885
-
0.924
-
0.537
-
0.703
-
0.730
-
0.612
-
0.479
-
0.809
-
0.473
-
0.646
-
Figure 7: Performance of NFVnice in a service chain of 3 NFs
with different computation costs
observed across the samples collected every second during the
experiment. In all cases, the NFs are interrupt driven, woken up by
NF manager when the packets arrive while NFs voluntarily yield
based on NFVnice’s policies. Also, when the transmit ring out of
an NF is full, that NF suspends processing packets until room is
created on the transmit ring.
4.2 Overall NFVnice Performance
We first demonstrate NFVnice’s overall performance, both in through-
put and in resource (CPU) utilization for each scheduler type. We
compare the default schedulers to our complete NFVnice system,
or when only including the CPU weight allocation tool (which
we term cgroups) or the backpressure to avoid wasted work at
upstream NFs in the service chain.
4.2.1 NF Service Chain on a Single Core: Here, we first consider
a service chain of three NFs; with computation cost Low (NF1, 120
cycles), Medium (NF2, 270 cycles), and High (NF3, 550 cycles). All
NFs run on a single shared core.
Figure 7 shows that NFVnice achieves an improvement of as
much as a factor of two times in throughput (especially over the RR
scheduler). We separately show the contribution of the cgroups and
backpressure features. By combining these, NFVnice improves the
overall throughput across all three kernel scheduling disciplines.
Table 3 shows the number of packets dropped at either of the up-
stream NFs, NF1 or NF2, after processing (an indication of truly
wasted work). Without NFVnice, the default schedulers drop mil-
lions of packets per second. But with NFVnice, the packet drop
rate is dramatically lower (near zero), an indication of effective
avoidance of wasted work and proper CPU allocation.
Table 5: Throughput, CPU utilization and wasted work in
chain of 3 NFs on different cores
NFVnice
Svc. rate Drop rate CPU Util
11% ±3%
0.82Mpps
64% ±1%
0.72Mpps
150Kpps
-
0.6Mpps
0.6Mpps
70Kpps
100%
-
175% ±3%
100%
100%
100%
300%
Default
Drop rate CPU Util
NF1
NF2
(∼550cycles)
(∼2200cycles)
(∼4500cycles)
Aggregate
NF3
Svc. rate
5.95Mpps
-
1.18Mpps
4.76Mpps
0.6Mpps
0.6Mpps
0.58Mpps
-
We also gather perf-scheduler statistics for the average schedul-
ing delay and runtime of each of the NFs. From Table 4, we can see
that i) with NFVnice the run-time for each NF is apportioned in a
cost-proportional manner (NF1 being least and NF3 being most),
unlike the NORMAL scheduler that seeks to provide equal alloca-
tions independent of the packet processing costs. ii) the average
scheduling delay with NFVnice for the NFs (that is the time taken
to begin execution once the NF is ready) is lower for the NFs with
higher processing time (which is exactly what is desired, to avoid
making a complex NF wait to process packets, and thus avoiding
unnecessary packet loss). Again this is better than the behaviour
of the default NORMAL or RR schedulers 2 .
Figure 8: Different NF chains (Chain-1 and Chain-2, of
length three), using shared instances for NF1 and NF4.
4.2.2 Multi-core Scalability: We next demonstrate the benefit
of NFVnice with the NFs in a chain across cores, with an NF being
pinned to a separate, dedicated core for that NF. We use these
experiments to demonstrate the benefits of NFVnice, namely: a)
avoiding wasted work through backpressure; and b) judicious re-
source (CPU cycles) utilization through scheduling. When NFs are
pinned to separate cores, there is no specific role/contribution for
Figure 9: Multi-core chains: Performance of NFVnice for two
different service chains of 3 NFs (each NF pinned to a differ-
ent core), as shown in Fig. 8.
2Even though, for this experiment, RR(100ms) performs as well as NFVnice, it
performs very poorly with variable per-packet processing costs, as seen in 4.3.1 and
for chains with heterogeneous computation costs, as in 4.3.2 scenarios.
NFVnice: Dynamic Backpressure and Scheduling for NF Chains
SIGCOMM ’17, August 21-25, 2017, Los Angeles, CA, USA
Table 6: Throughput, CPU utilization and wasted work in
a chain of 3 NFs (each NF pinned to a different core) with
different NF computation costs
Default
Drop
Rate
(pps)
2.86M 78.6% ±0.4
NFVNice
Drop
Rate
(pps)
82.1% ±0.5
CPU
Util.%
CPU
Util.%
Svc.Rate
0
NF1
(∼270cycles)
NF2
(∼120cycles)
NF3
(∼4500cycles)
NF4
(∼300cycles)
(pps)
3.26M
3.26M
6.522M
3.26M
3.26M
-
-
0.582M
0.582M
3.26M
0.582M
3.842M
Chain1
Chain2
Aggregate
Chain1
Chain2
Aggregate
Chain1
Chain2
Aggregate
Chain1
Chain2
Aggregate
Svc.Rate
(pps)
6.498M
0.583M
7.08M
6.498M
-
-
6.498M
0.582M
0.582M
6.498M
0.582M
7.08M
∼0
52.8% ±1.2
2.68M 100% ±0
0
60% ±0.7
∼0
58% ±0.7
<100
100% ±0
0
84% ±0.7
the vanilla OS schedulers, and for such an experiment we use the
default scheduler (NORMAL).
First, we consider the chain of 3 NFs, NF1 (Low, 550 cycles), NF2
(Medium, 2200 cycles) and NF3 (High, 4500 CPU cycles). Compared
to the default scheduler (NORMAL), NFVnice plays a key role in
avoiding the wasted work and efficiently utilizing CPU cycles. Ta-
ble 5 shows that NFVnice’s CPU utilization by NF1 and NF2 on
their cores is dramatically reduced, going down from 100% to ˜11%