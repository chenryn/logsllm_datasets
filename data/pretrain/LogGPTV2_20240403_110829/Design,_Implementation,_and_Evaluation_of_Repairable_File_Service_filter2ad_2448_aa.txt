title:Design, Implementation, and Evaluation of Repairable File Service
author:Ningning Zhu and
Tzi-cker Chiueh
Design, Implementation, and Evaluation of Repairable File Service
Ningning Zhu and Tzi-Cker Chiueh
State University of New York at Stony Brook
Stony Brook, NY 11794
{nzhu, chiueh}@cs.sunysb.edu
Abstract
loads.
The data contents of an information system may be cor-
rupted due to security breaches or human errors. This
project focuses on intrusion tolerance techniques that speed
up the process of repairing a damaged ﬁle system. The pro-
posed system, called Repairable File Service (or RFS), is
speciﬁcally designed to facilitate the repair of compromised
network ﬁle servers. An architectural innovation of RFS is
that it is decoupled from and requires no modiﬁcations on
the shared ﬁle server that is being protected. RFS supports
ﬁne-grained logging to allow roll-back of any ﬁle update
operation, and keeps track of inter-process dependencies to
quickly determine the extent of system damage after an at-
tack/error. Compared with the current practice of manual
post-intrusion damage repair, RFS signiﬁcantly reduces the
mean time to repair and thus improves the overall system
availability. Performance overhead of RFS is less than 6%.
1. Introduction
Despite a large amount of research efforts on computer
security, there is no such thing as an un-breakable sys-
tem. According to Gartner Group’s estimate, on the aver-
age more than 50% of the cost associated with a computer
security break-in is attributed to the productivity/revenue
loss due to data loss or service disruption, and/or additional
workloads required to repair the damages that intruders left
behind.
Even in absence of malicious attacks, humans make mis-
takes, which in turn lead to data damages or service outages.
James Reason [17, 5] did a study on the types of human er-
rors and concluded that human errors, even on simple tasks,
are unavoidable.
In another study [8, 6], an analysis on
PSTN and Internet sites’ operational statistics shows con-
sistently that operator errors are the leading cause of system
failures, as compared with software/hardware faults or over-
From an economic perspective, the total cost of owner-
ship (TCO) of an information system is widely believed to
be 5 to 10 times the initial acquisition cost of the associ-
ated hardware and software, and about one third to one half
of the TCO is related to recovering from or preparing for
system failures [8, 7].
In summary, an information system may be damaged due
to malicious attacks or honest human errors, and it is fun-
damentally difﬁcult either to stop all security breaches or
to completely prevent mistakes. So the next best thing one
can hope for is to restore the damaged systems back to their
functional state as soon as possible and with minimum data
loss. The focus of this paper is Repairable File Service
(RFS), which turns an existing shared ﬁle server into “re-
pairable” in the sense that it improves the precision and per-
formance of the damage repair process. More concretely,
an RFS signiﬁcantly decreases the damage repair time of a
LAN-based shared ﬁle server after the very ﬁrst attack or
error event is identiﬁed, by automating the identiﬁcation of
corrupted ﬁle blocks and the restoration of these blocks.
Regardless of the type of data that is being repaired, fun-
damentally a repairable information system needs to per-
form two tasks: (1) Maintaining all the raw data so that
every update is undoable, and (2) Keeping track of the de-
pendencies among update operations so that only the data
affected by attacks or errors are rolled back to their last
known consistent state. The ﬁrst task, update operation log-
ging, is supported by most existing ﬁle or database systems
to a varying extent, differing in logging granularities, fre-
quencies, or types of operations logged. The second task,
dependency tracking, is relatively unusual, and is more im-
portant because in practice, the most time consuming part
of a system damage repair process is to identify the parts of
the system that are compromised directly or indirectly by
the attacks or errors. Once they are identiﬁed, rolling them
back to a consistent state when all the raw data is available
can be largely automated.
Given the very ﬁrst process (or a few processes) (called
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:30:00 UTC from IEEE Xplore.  Restrictions apply. 
the root process hereafter) that starts an attack or a hu-
man error, RFS performs contamination analysis to deter-
mine the extent of ﬁle data corruption that needs to be re-
paired. These root processes are either outputs of an intru-
sion detection system, or results of administrators manually
inspecting the system log ﬁles. A process is considered con-
taminated if it is a root process, it’s a child of a contaminated
process, or it accesses contaminated ﬁle system data. At the
repair time, once a process is contaminated, RFS will re-
move all the modiﬁcations it makes to the ﬁle system. On
the other hand, the updates made by non-contaminated pro-
cesses, even though they start after the attack, will be pre-
served.
Here are several assumptions or limitations of current
RFS system. First we assume ﬁle server only get compro-
mised through network request sent by client. RFS can not
protect the server from being contaminated through local
access. Second assumption is that RFS depends on external
resource to get the pids of root processes. Third assump-
tion is that ﬁle dependency is useful although imperfect. It
is possible that one process is not contaminated even after
reading contaminated data.
It is also possible that a pro-
cess is contaminated through other communication chan-
nels (IPC, network etc). A precise contamination analysis
requires comprehensive process logging and high-level pro-
gram semantics analysis which is beyond the scope of this
paper. As future work, we need to evaluate the effectiveness
of our current approach or explore more precise contamina-
tion analysis.
In Section 2, we review previous research efforts related
to fast system damage repair. Section 3 describes RFS’s
system architecture, and the rationale of its design deci-
sions. Section 4 details the implementation of the current
RFS prototype. Section 5 presents the results of a perfor-
mance study on the RFS prototype. Section 6 concludes
this paper with a summary of major research contributions
and an outline of on-going work.
2. Related Work
Most previous work related to RFS focused only on the
[9] described the
update logging problem. Santry et al.
Elephant ﬁle system that allows users to deﬁne retention
policies which specify what ﬁles/directories should be ver-
sioned when they are subject to deletion/modiﬁcation op-
erations. However, Elephant does not log each and ev-
ery update so that it can not repair every possible dam-
age. The restoration process is not automatic. Wylie et al.
[10, 12, 11] described a Survivable Storage Systems which
is composed of PASIS and Self-securing Storage. The PA-
SIS architecture addresses the server fault tolerance and
availability problems, including the connections to those
servers. Self-securing storage addresses client and user ac-
count problems by transparently auditing accesses and ver-
sioning data within each storage server. Self-securing stor-
age (S4) is a network-attached object-based store that main-
tains previous versions of storage objects in a way transpar-
ent to both the operating system and applications. The main
rationale behind this design decision is that S4’s designers
assumed that the operating system itself could also be com-
promised. In contrast, RFS chooses to work in a way that
is completely transparent to the network ﬁle server and the
underlying disk storage subsystems, because one of its main
design goals is to minimize disruption to the existing infor-
mation technology infrastructure. Moreover, RFS supports
fast damage repair, which neither S4 nor PASIS supports.
Venti [22] uses a cryptographic hash of a disk block’s
contents as the disk block’s unique ID for reads and writes,
and supports a write-once policy that never overwrites or
deletes data. The content-derived ID also gives Venti a nat-
ural way to detect and remove duplicate disk blocks. Venti
and S4 share with RFS the same “logging every update”
philosophy. However, neither Venti nor S4 by itself ad-
dresses the most time-consuming element of damage repair,
i.e., identiﬁcation of damaged data sets by intrusions or er-
rors.
There have been several research projects on post-
intrusion database damage repair. Ammann et al. [13, 14]
proposed a transaction model and protocols that allow nor-
mal transactions to proceed against a database some por-
tions of which are known to be damaged as a result of
an intrusion. The proposal is largely a theoretical exer-
cise without detailed system-level considerations. Peng Liu
[15, 14, 19] described a concrete intrusion tolerant database
system which can continue its transaction processing ser-
vice even in the presence of active attacks. It logs database
updates in terms of SQL-based transactions. Fastrek [20]
is a dependency tracking approach for adding intrusion re-
silience in Database systems which shares similar principles
yet designed for DBMS instead of ﬁle system. The main
difference is that DBMS already maintained undo record
for each transaction for the abort purpose which can be used
by Fastrek for recovery, RFS has to maintain such undo log
by itself. Another difference is that Fastrek maintains the
inter-transaction dependency graph on the ﬂy to achieve fast
recovery. Since ﬁle system operation granularity is smaller
than transaction granularity, the overhead incurred and per-
formance penalty would be too high for ﬁle system. So in
normal status, RFS just keeps raw information about depen-
dency, and during repair time, only those entries which are
related with corruption need to be analyzed.
David Patterson [8, 6] advocates a radical shift from a
performance-dominated research focus to what he refers to
as Recovery Oriented Computing (ROC), where the focus
is on improving system availability by reducing MTTR and
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:30:00 UTC from IEEE Xplore.  Restrictions apply. 
eventually the overall system ownership cost. One of the ap-
plications that is currently being pursued, an undoable email
system [8], shares a similar approach with RFS but focuses
speciﬁcally on email message protection rather than general
ﬁles as in RFS.
RFS is different from traditional ﬁle system back-up
tools such as Amanda [1] in that RFS logs every ﬁle sys-
tem update and thus can undo every possible update. RFS
is different from ﬁle signature check-pointing tools such as
Tripwire [2] in that it can more accurately identify the ex-
tent of damage through process dependencies that are con-
stantly tracked at run time. RFS is different from ﬁle/disk
mirroring systems in that the latter does not offer any fast
restoration capability in the presence of attacks or errors.
File/disk mirroring simply makes the same mistake twice
with respect to an update initiated by an attack or error!
3. Repairable File Service
3.1. Overview
Repairable File Service (RFS) is not a ﬁle system on its
own. It is designed to be a general framework for protect-
ing shared ﬁle servers from irrevocable damages caused by
errors or attacks. In the normal operating mode, RFS main-
tains a ﬁle update log and an inter-process dependency log.
In the repair mode, RFS ﬁrst determines the exact extent
of system damage, and then performs selective roll-back of
those data blocks that are considered contaminated. The
design of RFS centers around the two fundamental tasks as-
sociated with any repairable information system and driven
by a set of design goals:
• RFS should operate in a way that does not require any
modiﬁcations to the shared ﬁle server that it is protect-
ing,
• RFS should not
introduce signiﬁcant performance
overhead to the ﬁle access path that disrupts the inter-
actions between the shared ﬁle server and its clients,
and
• The system architecture of RFS should be sufﬁciently
modular that the components which are independent of
the underlying network ﬁle access protocol (e.g., NFS)
should be reusable across different network ﬁle access
protocols.
RFS focuses on speeding up the system repair process
after an intrusion. It itself does not perform intrusion de-
tection. At the time of damage repair RFS assumes that the
processes which start an attack are already identiﬁed, either
through an intrusion detection system, or through manual
inspection of the system log. Given these processes, RFS
can partially or completely automate the subsequent dam-
age repair.
3.2. System Architecture
The system architecture of RFS is shown in Figure 1.
The request interceptor logs all ﬁle update requests sent to
the protected network ﬁle server from its clients. By logging
these updates, RFS can undo the effect of each and every
one of these updates if necessary. In this paper, the origi-
nal NFS request (and sometimes with its reply together) is
called a redo record. A collection of redo record is called
a redo log. The compensation operation of redo record is
called undo record. A collection of undo record is called a
undo log.
The client-side system call logger records the process
identity, parent-child relationship among the processes, as
well as enough information to map a client-side ﬁle system-
related syscall to a server-side ﬁle update log entry. Note
that actual data in read and write syscall is not logged. The
contamination analysis analyzes the client/server logs to
determine the extent of damage. The repair engine per-
forms undo operations according to the result of the con-
tamination analysis.
NFS
Client
syscall
logger
NFS
Client
syscall
logger
RFS server
Request
Interceptor
 Log Collection,
Contamination
Analysis and 
Repair Engine
Mirror
NFS Server
Protected
NFS server
Figure 1. The system architecture of RFS includes a request
interceptor that logs all ﬁle update requests sent to the protected
shared ﬁle server (in this case an NFS server), a client-side
system call logger (embedded in the NFS clients), a central log
collection, analysis and repair engine, and a mirror ﬁle server
that runs the same protocol as the protected ﬁle server.
There are two reasons why separate per-client logs are
needed in addition to the ﬁle update log. First, due to NFS
client caching, the ﬁle update log does not include all ﬁle
reads and writes on clients. Although the repair granularity
of RFS is an NFS request and RFS can not undo each write
syscall, logging each syscall supports more precise contam-
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:30:00 UTC from IEEE Xplore.  Restrictions apply. 
ination analysis. Secondly, ﬁle update log does not contain
information about the processes which issue these updates
which is important for ﬁle contamination analysis.
RFS is separate from the shared ﬁle server it protects.
The decoupled architecture makes it easier to apply the RFS
framework to different network ﬁle system technologies,
such as SUN’s NFS protocol [4] or Microsoft Server Mes-
sage Block (SMB) [3] or CIFS [18] protocol. It also greatly
simpliﬁes the logistics of deploying the RFS technology be-
cause no modiﬁcation is required to the protected network
ﬁle servers. In addition, RFS box is invisible to both the
network ﬁle server and its clients, it only monitors NFS traf-
ﬁc. There is little chance for it to be attacked.
In the event of a failure, forward recovery rolls the sys-
tem state back to the most recent clean snapshot, and selec-
tively applies the non-contaminated redo operations to keep
useful work. Backward recovery, on the other hand, rolls
the current system state back by undo contaminated opera-
tions until the entire system is clean. With backward recov-
ery, it is possible to avoid the overhead of checkpointing,
and the repair time is proportional to the duration before
intrusion or mistakes are detected. With forward recovery,
recent damages take long time to repair, unless the system
pays the overhead of frequent checkpointing. We assume
that with current intrusion detection techniques, intrusions
and errors can mostly be detected within a short period of
time after their occurrence. RFS uses backward recovery
approach, and logs ﬁle system updates as an undo log.
To construct an undo operation, RFS needs the before
image of the block(s)/metadata of ﬁle system updates. A
naive approach to retrieve the before image is to issue a ﬁle
read or get attribute to the protected shared ﬁle server before
updating the ﬁle server. However, this approach has two
serious drawbacks. This approach adds signiﬁcant delay
to the original update operation and substantially increases
the load on the shared ﬁle server. Moreover, if the original
ﬁle update request actually fails, all the efforts to get the
associated before image are wasted.
RFS takes an alternative approach to the before image
problem: It assumes the existence of a separate mirror ﬁle
server (Figure 1), which services the requests to get be-
fore image. Typically the load on the mirror ﬁle server is
lower than the protected shared ﬁle server because it does
not need to service non-update ﬁle system requests(read,
getattr, lookup, access, readdir etc), and because before im-
age accesses can be serviced asynchronously and scheduled
for better disk access efﬁciency.
RFS decouples ﬁle system requests interception from
undo record construction The outputs of request interceptor
are redo records, which are then transformed by log collec-
tion into undo records.
4. RFS Prototype Implementation
We chose NFS server as the target for the ﬁrst RFS proto-
type, whose detailed software architecture is shown in Fig-
ure 2. In this section, we describe the implementation issues
of the RFS prototype and their solutions.
4.1. Client-Side Logging
The client-side system call logger logs the ﬁle system-
related system calls for each process, as well as system
calls related to process creation and termination, e.g. fork()
and exit(). Contamination analysis assumes individual pro-
cesses as units of repair, update operations of a process are
either all preserved or all removed. It is essential for RFS to
determine the initiating process of each NFS request. Un-
fortunately, NFS requests only contain user ID , but not in-
formation about the process that issues the requests. Actu-
ally due to client caching and request aggregation, there is
no one-to-one mapping between an NFS request and a lo-
cal system call. RFS solves this problem by inserting an
RPC message entry to the per-client system call log before
the associated NFS request is sent out. By analyzing the
client-side system call log entries immediately before the
request’s corresponding RPC message entry, RFS can de-
termine the process or processes that are responsible for a
particular NFS request. The implementation is similar to
unix syslogd, only difference is that instead of being stored