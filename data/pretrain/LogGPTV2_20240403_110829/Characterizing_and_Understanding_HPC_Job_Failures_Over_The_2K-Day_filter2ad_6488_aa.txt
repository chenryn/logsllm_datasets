title:Characterizing and Understanding HPC Job Failures Over The 2K-Day
Life of IBM BlueGene/Q System
author:Sheng Di and
Hanqi Guo and
Eric Pershey and
Marc Snir and
Franck Cappello
Characterizing and Understanding HPC Job Failures
over The 2K-day Life of IBM BlueGene/Q System
Sheng Di,∗ Hanqi Guo,∗ Eric Pershey,∗ Marc Snir,† Franck Cappello∗†
∗Argonne National Laboratory, IL, USA
PI:EMAIL, PI:EMAIL, PI:EMAIL, PI:EMAIL
†University of Illinois at Urbana-Champaign, IL, USA
PI:EMAIL
Blue Joule (UK), also adopt the same Blue Gene/Q system
architecture and they are still in operation.
Abstract—An in-depth understanding of the failure features of
HPC jobs in a supercomputer is critical to the large-scale system
maintenance and improvement of the service quality for users. In
this paper, we investigate the features of hundreds of thousands
of jobs in one of the most powerful supercomputers, the IBM
Blue Gene/Q Mira, based on 2001 days of observations with a
total of over 32.44 billion core-hours. We study the impact of the
system’s events on the jobs’ execution in order to understand the
system’s reliability from the perspective of jobs and users. The
characterization involves a joint analysis based on multiple data
sources, including the reliability, availability, and serviceability
(RAS) log; job scheduling log; the log regarding each job’s
physical execution tasks; and the I/O behavior log. We present 22
valuable takeaways based on our in-depth analysis. For instance,
99,245 job failures are reported in the job-scheduling log, a large
majority (99.4%) of which are due to user behavior (such as
bugs in code, wrong conﬁguration, or misoperations). The job
failures are correlated with multiple metrics and attributes, such
as users/projects and job execution structure (number of tasks,
scale, and core-hours). The best-ﬁtting distributions of a failed
job’s execution length (or interruption interval) include Weibull,
Pareto, inverse Gaussian, and Erlang/exponential, depending on
the types of errors (i.e., exit codes). The RAS events affecting job
executions exhibit a high correlation with users and core-hours
and have a strong locality feature. In terms of the failed jobs, our
similarity-based event-ﬁltering analysis indicates that the mean
time to interruption is about 3.5 days.
I. INTRODUCTION
Since many of today’s science research problems are too
complicated to resolve by theoretical analysis, scientists have
to perform large-scale (or extreme-scale) simulations on su-
percomputers. Large-scale simulations, however, have a high
likelihood of encountering failures during lengthy execution
[1]. In order to improve the service quality of HPC systems,
it is critical to deeply understand the features and behaviors
of failed jobs and their correlation with system reliability.
In this paper, we characterize job failures in one of the
most powerful supercomputers, the IBM Blue Gene/Q Mira,
which is deployed at Argonne National Laboratory. Our study
is based on a set of system logs spanning 5.5 years (from
04/09/2013 to 09/30/2018). The IBM Blue Gene/Q Mira was
ranked as the third fastest supercomputer in 2013, and it
is still ranked as the 21st in the world based on the latest
TOP500 report. Understanding the job failure features on
this supercomputer has a broad signiﬁcance because multiple
supercomputers, such as Sequoia (USA), Vulcan (USA), and
1
Studying the job failure features in a large-scale system is
nontrivial in that it involves numerous messages logged from
across multiple data sources and many messages are heavily
duplicated [2]. In our work, we performed a joint analysis
by leveraging four different data sources: the reliability, avail-
ability, and serviceability (RAS) log; task execution log; job
scheduling log; and I/O behavior log. The RAS log is the most
important system log related to system reliability such as node
failure, power outages, and coolant issues. In the 5.5 years
of observation, the RAS log has 80,665,723 messages, which
have three severity levels (Fatal, Warn, and Info). In the Mira
system, users who wish to run a high-performance computing
(HPC) application or simulation must submit a job to the
Cobalt system [3] (a job-scheduling system similar to Torque
[4]); the submitted job is then split into multiple tasks during
the whole execution. The user-submitted jobs are called user
jobs or Cobalt jobs in the following text. The job-scheduling
log records the status for each job, such as queuing status,
running status, number of nodes or cores used, completion
time, and exit status. In our study, the job-scheduling log
involves up to 32.44 billion core-hours, and this is the largest
compute resource usage in a resilience study up to date, to
the best of our knowledge. The task execution log contains
detailed information such as what physical execution block
was assigned to the job and which rank ran into errors if the
job failed. To determine the jobs’ I/O behaviors, we analyze
the I/O characterization logs produced by Darshan [5], [6],
such as the number of bytes read/written by each job and the
potential correlation with job failures. We combined all four
data sources to better understand the behavior of a failed job
and how the fatal system events affect the job execution.
Our characterization/analysis results have been approved by
the Mira system administrator who is an expert in log analysis.
Based on our in-depth study of HPC jobs running on the IBM
Blue Gene/Q Mira system, we address the following questions.
These questions are critical to large-scale system maintenance,
in-depth understanding of HPC job failures, and improvement
of the resource provisioning quality.
• Analysis of generic job features: What are the statistical
features of the HPC jobs from the perspective of a
long-term period of observation of Mira? Speciﬁcally,
we characterize the distribution of execution time, the
best-ﬁt distribution type of execution time by maximum
likelihood estimation (MLE), jobs’ I/O behaviors, and
resource usage of jobs across users and projects. This
characterization indicates the job features in IBM Blue
Gene/Q systems, in comparison with other systems with
different architectures [7]–[9].
• Analysis of failed jobs’ features: What are the statistical
features of the failed jobs on the petascale system from a
long-term view? To address this question, we provide an
in-depth, comprehensive study of the correlation of the
speciﬁc job exit statuses and other important attributes
using multiple logs from the IBM Blue Gene/Q Mira;
this approach is in contrast with general characterization
work [2], [7] focusing mainly on the system level. Our
work also differs from the existing application resilience
study in [8], which was focused on a statistical analysis
of job failures on Blue Waters [10]. Speciﬁcally, not
only do we characterize the distribution of failed jobs,
but we also explore the best-ﬁt distribution type of the
execution lengths based on speciﬁc exit statuses. We
also identify the relationship between job failure status
and other critical attributes, such as the execution scale,
users/projects, job’s execution tasks, jobs’ I/O behaviors,
job execution time, and resource allocation locations.
• Impacts of fatal system events to job executions: How
do the fatal system events impact the job executions from
the perspective of both job scheduling system and user
job executions? In contrast to related work [2] focusing
only on the correlation among system events (i.e., RAS
events) [11], we investigate the correlation between the
system’s RAS events and job executions in this paper.
This new analysis is important to system administrators,
application users, and fault tolerance researchers because
it indicates the system’s reliability from the perspective of
users and jobs. On the one hand, system administrators
can better understand fatal system events and diagnose
the issues more effectively by taking into account their
impact on the users. On the other hand, application users
and researchers can get a more accurate estimation of the
mean time to interruption (MTTI) such that more efﬁcient
fault tolerance strategies can be developed accordingly.
The remainder of this paper is organized as follows. In
Section II, we discuss related work. In Section III, we describe
the IBM Blue Gene/Q Mira system and the data sources
(logs). In Section IV, we describe our analysis methodology.
In Section V, we characterize the features of job executions
in Mira and investigate the failure properties. In Section VI,
we analyze the correlation between the system’s fatal events
and job executions and their locality features. In Section VII,
we conclude the paper with a brief discussion of future work.
II. RELATED WORK
Although researchers have analyzed supercomputer’s relia-
bility, their analysis results cannot be applied to our context
because of different systems or architectures. Li et al. [12], for
2
example, used a tool called CrashFinder to analyze the faults
causing long-latency crashes in user programs; they conducted
their experiments on an Intel Xeon E5 machine with simulated
faults injected by the open fault injector LLFI [13]. Siddiqua
et al. [14], [15] characterized DRAM/SRAM faults collected
over the lifetime of the LANL Cielo supercomputer [16]. Nie
et al. [17], [18] characterized and quantiﬁed different kinds of
soft-errors on the Titan supercomputer’s GPU nodes and also
developed machine learning methods to predict the occurrence
of GPU errors. Sridharan et al. [19], [20] examined the impact
of errors and aging on DRAM and identiﬁed a signiﬁcant
intervendor effect on DRAM fault rates based on the LANL
Cielo system and the ORNL Jaguar system [21]. Martino et
al. [7] studied hardware/ﬁrmware errors on Blue Waters [10],
showing that its processor and memory protection mechanisms
(x8 and x4 Chipkill, ECC, and parity) are robust.
Arguably, some large-scale system failure studies [9], [22]–
[26] have been conducted on the IBM Blue Gene series of
supercomputers; however, their analyses generally focus on
speciﬁc issues such as memory errors, temperature, power,
and soft-error behaviors or on small or medium-sized super-
computers. Hwang et al. [27], for example, characterized the
DRAM errors and their implications for the system design,
based on four supercomputers: IBM Blue Gene/L, IBM Blue
Gene/P, SciNet, and a Google data center. Di et al. [2] char-
acterized the resilience features of fatal system events for the
IBM Blue Gene/Q Mira, but the study was based on a single
data source (RAS event log). Zheng et al. [28] provided a
coanalysis of RAS logs and job logs on a Blue Gene/P system
[29]; their study, however, was based on an older, smaller
cluster (163k cores) with a short logging period (273 days).
By comparison, we provide a much more comprehensive, ﬁne-
grained analysis of the correlation between various job failure
types and multiple attributes (such as users and projects, job
execution structure, locality, job’s I/O behaviors, and RAS
events) as well as the best distribution ﬁtting for job length
and locality features.
Our analysis also differs from the characterization work in
[8], which focused on applications running on Blue Waters
[10]. That work studied application resilience (including both
CPU usage and GPU usage) and categorized failure reasons in
terms of resource usage such as core-hours. However, it did
not characterize the detailed correlation between exit codes
with speciﬁc key attributes (such as user names, number of
tasks per job, and MLE-based best distribution ﬁtting on job
length) using contingency tables. Moreover, we also charac-
terize failures vs. I/O behaviors and the detailed correlation
between job failures and speciﬁc fatal system events as well
as the locality features. Their analysis was based on Cray-
series systems whose architecture and users differ from those
of Mira, such that their results cannot be applied to our study
simply: for example, in their study on Blue Waters, only 14%
of failed applications are due to timeout, whereas 55.8% failed
jobs are attributed to timeout on Mira. Our study also involves
far more core-hours than that work did: 32.44 billion core-
hours vs. 6.8 billion (=2.12E8 node-hours×32) core-hours.
III. BACKGROUND
In this section, we describe the Blue Gene/Q supercomputer
Mira and its system logs used in our study. An organizational
diagram of the Blue G/Q system can be found in IBM BG/Q
administrator guide [11] (see Fig. 1-2 in that document).
A. Mira
Mira is a 10-petaﬂops IBM Blue Gene/Q system operated
by Argonne National Laboratory. Mira consists of 49,152 com-
pute nodes across 48 racks. Each rack contains two midplanes,
each of which has 32 compute cards (or compute nodes). Every
compute node has a PowerPC A2 1600 MHz processor with
16 active cores and 16G DDR3 memory, bringing the total to
786,432 cores for the entire machine. Each compute rack has
an I/O drawer, each coming with 8 I/O cards, 8 PCIe Gen2
x8 slots, optical modules, link module, and fan assembly.
The Mira system uses IBM’s 5D torus network with 2
GB/s chip-to-chip links for connecting the nodes and uses
a single network for point-to-point, collective, and barrier
communication (in contrast to prior generations of Blue Gene
systems). Each node has 10 links with 2 Gb/s bandwidth, with
an additional 11th link for communication with the I/O nodes.
Links between the midplanes are optical and within the mid-
plane are electrical. The 48 compute racks are denoted by R00-
R0F, R10-R1F, and R20-R2F; and each rank is composed of
two midplanes (denoted M0 and M1). The compute resources
are allocated to jobs in the granularity of midplanes. Each
compute resource assignment exhibits an allocation block,
which is represented as x1x2x3x4x5–y1y2y3y4y5, where xi
and yi denote the ﬁrst and last node index in the ith dimension
of the 5D torus network, respectively.
B. Data Source Description: Job, Task, and I/O Behavior Logs
In our study, we combine four system logs—RAS log, job
scheduling log, task execution log, and I/O log—to explore
the features of the user jobs based on their execution statuses.
All four logs are available to download from the Argonne
Leadership Computing Facility (ALCF) website [30].
1) RAS Log: The RAS log is one of the most important
system logs because it indicates system reliability. Each item
in the RAS log is represented as a speciﬁc event with one
of three severity levels (INFO, WARN, or FATAL). The fatal
events are the most important category because they imply
potential system errors [11]. Of the 14 ﬁelds, only a few (such
as message ID, task ID, and timestamp) are critical to our
study; other ﬁelds (such as record ID) either are not needed
in our analysis or can be derived from other ﬁelds already
included (e.g., category value is determined by message ID).
2) Cobalt Job Log: The job-scheduling log, or Cobalt
log, is another critical log. It contains detailed information
about the submitted jobs, including submission, scheduled,
and completion timestamps, the number of nodes or cores
requested or used, physical execution tasks during execution,
applications and projects, and termination status.
The Cobolt job log comprises 57 ﬁelds, of which 15 ﬁelds
are selected in our study, as listed in Table I. Other ﬁelds
either are not needed (e.g., machine name is always valued as
“mira”) or can be derived from the 15 key ﬁelds: for instance,
job name is always named as cobalt jobID.mira; start date id
(such as 20130401) can be derived from start timestamp (such
as 2013-04-01 00:01:11.000000); and the # cores used is equal
to 16×nodes used because each node has 16 cores.
Field
cobalt jobID
queued timestamp
start timestamp
end timestamp
user ID
project ID
queue name
wall time
runtime
nodes used
nodes requested
location
exit code
mode
TABLE I
KEY FIELDS OF THE COBALT JOB LOG
Examples
67928,67931
2013-03-31 21:16:27.000000
2013-03-31 21:36:43.000000
2013-04-01 00:07:38.000000
50587932556210
3041172680929
prod-short,backﬁll,prod-long
4200,9000,21600
4200,9000,21600
1024,2048,32768
1024,2048,32768
MIR-48400-7B771-1024
0,143,137,139
script,c1,c4
1,9,12,21
Description
user’s job ID
job submission moment
start execution moment
end execution moment
hashed user name
hashed project name
queue name
requested wall time
execution length
number of nodes used
number of nodes requested
execution block
exist status
execution mode
number of execution tasks
num of tasks
During the 5.5 years (2,001 days: from 04/09/2013 to
09/30/2018), there are a total of 377,531 jobs, with a rather
nonuniform distribution on the number of jobs submitted each
day. Speciﬁcally, the minimum and maximum job-submission
counts within one day are 0 and 1,788, respectively. Note
that the minimum compute resource allocation per job (i.e.,
the number of cores actually used per job) is required to be
no less than one midplane (i.e., 8,192 cores), leading to a
huge total number of core-hours consumed—speciﬁcally, up to
32.44 billion core-hours in total in the Mira system during the
2,001 days—arguably the largest amount in any job resilience
study to the best of our knowledge.
3) Task Execution Log: The task execution log involves
physical execution information such as the allocation block,
the detailed execution status, and the corresponding Cobalt
job ID. We can combine the RAS log and Cobalt job log to
do a joint analysis.
Each job may go through two stages—queuing and
execution—throughout its lifetime. The job execution stage is
composed of one or multiple consecutive or parallel execution
phases each handled by a particular task. A task is a ﬁner
execution unit to complete the work for a job. Hence, by in-
vestigating the task execution log, one can understand the job’s
detailed execution history. The task execution log comprises 21
ﬁelds. Our study involves mainly 7 of these ﬁelds (as listed in
Table II) because other ﬁelds are not relevant to our study. The
whole log includes over 2.6 million task execution records,
which means that each job involves 7 tasks on average.
4) Darshan I/O Characterization Log: The I/O log used in
our analysis was generated by the lightweight I/O behavior-
monitoring tool Darshan [5], [6] (which received an R&D100
research award in 2018). The Darshan log records the I/O
behavior of Cobalt jobs in the system (since 01/01/2014), in-
cluding properties such as patterns of access within ﬁles. This
characterization can shed important light on the I/O behavior
of applications at extreme scale. Based on the Darshan log, we
3
KEY FIELDS OF THE TASK EXECUTION LOG
TABLE II
Field
taskID
userID
location
start timestamp
end timestamp
cobalt jobID
exit signal
err text
Examples
184483
82945435253412
Description
ID of task
user’s ID
allocated execution block MIR-48400-7B771-1024
starting moment
ending moment
ID of user’s job
exit status of task
description of exit status
2013-04-01 00:04:31.760919
2013-04-01 00:07:18.536998
67928,67931
9, 15
“abnormal termination by
signal 9 from rank 13461”
analyze the I/O behaviors of the user jobs and their potential
correlations with the execution status.
The log comprises a total of 149 ﬁelds. The key ﬁelds
include the total number of bytes read/written, highest offset
in the ﬁle that was read/written, and number of POSIX/MPI
reads/writes. Other ﬁelds either are not needed for our study
(e.g., the value of MACHINE NAME and CP DEVICE are
always mira and 0, respectively) or can be derived from other
information (e.g., RUN DATE ID can be derived from the job
execution time stamp).
IV. JOB FAILURE FEATURE ANALYSIS METHODOLOGY
In this section, we describe our analysis method, which
combines four data sources—RAS events, user jobs, execution
tasks, and I/O behaviors. Fig. 1 illustrates the process.
(calculated by the Cobalt log) with its wall time request.
The failed jobs with overlong execution times are cate-
gorized as timeout jobs.
• Bug: The jobs with serious termination signals, such as
SIGABRT (due to abort assertion or double-free of mem-
ory) and SIGSEGV (segmentation fault), are grouped in
the bug category.
• Kill: Some jobs were killed in the middle of execution
(with signal 9), although they were not due to a code
bug or execution time exceeding wall time. We group
such types of jobs in the kill category.
• IO: We observed a number of abnormal tasks are related
to ﬁles stored on I/O nodes, so we group them in the
IO category. Note that the corresponding failed jobs are
not due to ﬁle system issues but to user mistakes in
managing ﬁles or operations. The following are examples
of task failure messages: “Load failed on Q1G-I4-J00:
Changing to working directory failed, errno 2 No such
ﬁle or directory”, “Load failed on Q0H-I2-J06: Reading
data from application executable failed, errno 21 Is a
directory”, and “Load failed on Q0H-I4-J06: No authority
to application executable, errno 13 Permission denied”.
• RAS: According to the task execution log, some tasks
were killed because of system reliability (i.e., RAS
events). We classify the corresponding jobs in the RAS
category. These jobs are critical to understanding the
impact of the system events on the job executions.
• Unknown: A few jobs terminate with unknown reason
(e.g., missing messages).
• SIGILL/SIGTRAP/SIGFPE: Three more types of ter-
mination signals exist, which we put in the categories
SIGILL, SIGTRAP, and SIGFPE, respectively. They cor-
respond to signals 4, 5, and 8, to be detailed later.
Fig. 1.
Illustration of System Logs and Joint Analysis
First, we need to identify the abnormal job termination as
well as the different failure types, based on the Cobalt job-
scheduling log and the task execution log. Accurately extract-
ing the jobs’ exit statuses (normal or failure) is nontrivial
because the nonzero exit status values recorded in the Cobalt
log refer to all the possible failed jobs from the perspective
of the scheduling system. In fact, we observe that many jobs
terminate with nonzero exit codes according to the Cobalt log
yet the description ﬁeld of the task execution log indicates that
they terminate normally from the perspective of the users. The
nonzero status values in the Cobalt log could be due to the
users’ customized exit statuses or to missing exit codes.
To ensure that our analysis is based on the correct termina-
tion status values of the jobs, we map all task executions to
their corresponding jobs and determine each job’s exit status
by the termination signals of the execution task(s) and corre-
sponding descriptions in the task execution log. We categorize
the failed jobs for several types of jobs as follows.
• Timeout: We ﬁrst select the failed jobs with at least
one abnormal execution task(s) (recorded in the task
execution log) and then compare its real execution time
In order to reveal the mean time to interruption (MTTI)