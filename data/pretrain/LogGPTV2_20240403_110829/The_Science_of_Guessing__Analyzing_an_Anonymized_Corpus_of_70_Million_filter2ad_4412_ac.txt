by a Yahoo! manager and machine-generated entropy. The
experiment was approved by Yahoo!’s legal team as well
as the responsible ethics committee at the University of
Cambridge. We deployed our experiment on a random subset
of Yahoo! servers for a 48 hour period from May 23–25,
2011, observing 69,301,337 unique users and constructing
separate histograms for 328 different predicate functions. Of
these, many did not achieve a sufﬁcient sample size to be
useful and were discarded.
V. EFFECTS OF SAMPLE SIZE
In our mathematical treatment of guessing difﬁculty, we
assumed complete information is available about the under-
lying probability distribution of passwords X . In practice, we
will need to approximate X with empirical data.9 We assume
that we have M independent samples X1, . . . , XM
← X
and we wish to calculate properties of X .
The simplest approach is to compute metrics using the
distribution of samples directly, which we denote ˆX .10 As
9It possible that an attacker knows the precise distribution of passwords
in a given database, but typically in this case she or he would also know
per-user passwords and would not be guessing statistically.
R
10We use the hat symbolˆfor any metric estimated from sampled data.
101214161820222426lgM510152025metricvalue(bits)ˆH0ˆ˜GˆH1ˆ˜µ0.25ˆ˜λ10ˆH∞shown in Figure 3, this approach produces substantial and
systematic under-estimates of most metrics, most promi-
nently ˆH0 = lg ˆN which increases nearly continuously with
increasing sample size M indicating that new passwords
are still being seen often even at our massive sample size.
The maximum-likelihood estimation of the growth rate d ˆN
dM
has been shown to be exactly V (1,M )
M , the proportion of
passwords in the sample observed only once [42].11 This
can be seen because in in exactly V (1,M )
of all possible
orderings that the sample may have been collected will
the last observation have been a new item. For our full
sample, V (1,M )
= 42.5%, indicating that a larger sample
would continue to ﬁnd many new passwords and hence
larger estimates for H0, H1, G1 etc. Similarly, for a random
subsample of our data, many passwords will be missed and
estimates of these metrics will decrease.
M
M
Interpreting hapax legomena is a fundamental problem in
statistics and there are there are no known non-parametric
techniques for estimating the true distribution size N [42].
This is a not merely a theoretical restriction; in the case of
passwords determining that apparently pseudorandom pass-
words really are 128-bit random strings would require an ut-
terly intractable sample size many times greater 2128. Good-
Turing techniques [43] aren’t helpful for the distribution-
wide statistics we are interested in; they can only estimate
the cumulative probability of all unobserved events (the
“missing mass”) and provide damped maximum-likelihood
estimates of the probability of individual events.
Fortunately, in practice we can usefully approximate our
guessing metrics from reasonably-sized samples;
though
these estimations implicitly rely on assumptions about the
underlying nature of the password distribution. As seen in
Figure 3, partial guessing metrics which rely only on the
more-frequent items in the distribution are the easiest to
approximate, while those which rely on a summation over
the entire distribution such as H0, H1 and ˜µα, ˜Gα for large
values of α will be the most difﬁcult.
A. The region of stability
We can reliably estimate pi for events with observed
frequency fi (cid:29) 1 due to the law of large numbers.
Estimating H∞ requires estimating only p1, the probability
of the most common password, which was 1.08% in our data
set. Gaussian statistics can be used to estimate the standard
error of the maximum-likelihood estimate ˆpi:
(cid:114)
(cid:114)
error(ˆpi) =
pi(1 − pi)
M
1
pi ≈
·
fi
M 2 ·
M
fi
=
1
√fi
For our data set, this gives a standard error of under 0.1 bit
in ˆH∞ for M ≥ 214. This argument extends to ˆ˜λβ for small
Figure 4.
Estimated guessing curves with reduced sample size M.
Subsamples were computed randomly without replacement, to simulate
having stopped the collection experiment earlier. After the maximum
conﬁdence point α6; there are two (almost indistinguishable) dashed plots
representing the 1st and 99th percentiles from 1,000 random samples.
values of β and in practice we can measure resistance to
online guessing with relatively modest sample size.
Reasoning about the error in ˆ˜µα and ˆ˜Gα for values of α
which represent realistic brute-force attacks is more difﬁcult.
Fortunately, we observe that for our password data set the
number of events V (f, M ) which occur f times in a sample
of size M is very consistent for small f and provides a
reasonable estimate of the number of events with probability
f−0.5
M ≤ p ≤ f +0.5
This enables a useful heuristic that ˜µα and ˜Gα will be
well approximated when α is small enough to only rely
on events occurring greater than some small frequency f.
Calling αf the cumulative estimated probability of all events
occurring at least f times, we took 1,000 random samples
of our corpus with M = 219 and observed the following
values in the 1st and 99th percentiles:
M in our full data set.12
f
αf
˜µαf − ˆ˜µαf
˜Gαf − ˆ˜Gαf
6
7
8
0.162–0.163
0.157–0.180
0.155–0.176
0.153–0.154
0.125–0.148
0.123–0.146
0.145–0.146
0.103–0.127
0.101–0.126
We observed very similar values for larger values of M.
Thus, we will use ˆ˜µα, ˆ˜Gα directly for α ≤ α6 for random
subsamples of our data. The utility of this heuristic is seen
in Figure 3, where it accurately predicts the point at which
˜µ0.25 stabilizes, and in Figure 4, where it marks the point
below which ˜µα is inaccurate for varying M.
11Events observed only once in a sample are called hapax legomena in
linguistics, Greek for “said only once.”
12V (f, M ) will almost always overestimate this value because more low-
probability events will be randomly over-represented than the converse.
545
0.00.20.40.60.81.0successrateα0510152025α-work-factor˜µα(bits)M=69,301,337(full)M=10,000,000(sampled)M=1,000,000(sampled)M=500,000(sampled)B. Parametric extension of our approximations
Estimating ˜µα and ˜Gα for higher α requires directly
assuming a model for the underlying password distribution.
Passwords have been conjectured to follow a power-law
distribution13 where:
Pr [p(x) > y] ∝ y1−a
(12)
Unfortunately, using a power-law distribution is problematic
for two reasons. First, estimates for the scale parameter a
are known to decrease signiﬁcantly with sample size [42].
Using maximum-likelihood ﬁtting techniques [44] for our
observed count data we get the following estimates:
M 69M 10M 1M 100k
4.21
ˆa
3.23
2.99
3.70
A second problem is this model ﬁts our observed, integer
counts. To correctly estimate ˜µα from samples, we need to
model the presence of passwords for which pi · M  0, c > 0, g  0.99) the hypothesis that our sample
was drawn from the modeled distribution.
Our goal is to accurately compare statistics for differently-
sized subsamples of our data. Doing so using our empirical
precision estimates directly is accurate only under the as-
sumption that two different subpopulations have each chosen
a distribution of passwords which our model ﬁts equally
well.15 If some deﬁnable population of user-generated pass-
words form a very different underlying distribution (for
example, uniform or exponential), then our model might
produce much more variable estimates. When analyzing
our data in Section VI we thus make a weaker claim
only that different demographic subsamples of users are
signiﬁcantly different from the global population of users if
our extrapolation produces estimates which are outside the
1st or 99th percentile of estimates observed for similarly-
sized random samples as listed in this section.
VI. ANALYSIS OF YAHOO! DATA
A. External comparison
We ﬁrst compare our collected data to several known
data sets. To the author’s knowledge, there have been two
large-scale leaks of password data suitable for statistical
analysis:16 the 2009 RockYou leak and a 2011 leak of
roughly 500k passwords from the gaming website Battleﬁeld
Heroes.17 Guessing metrics for these distributions and our
collected data are listed in Table III. All three distributions,
despite being taken from substantially different populations,
agree to within 1 bit for estimates of online attacks (H∞
and ˜λ10), and within 2 bits for ofﬂine attacks ( ˜G0.25 and
˜G0.5).
We plot
the guessing curve for our collected data in
Figure 6 along with that of the RockYou distribution. We
15Supporting this assumption, we ﬁnd that our model produces similarly
accurate estimates for subsamples of the RockYou distribution, the only
other large password data set to which we have access.
16A prominent 2010 leak revealed nearly 1M passwords from the blog-
ging site Gawker, but these were salted via the Unix crypt() function,
preventing full analysis of the distribution.
17The Battleﬁeld Heroes passwords were hashed with MD5, but without
any salt, making analysis of the distribution possible.
547
Yahoo! (2011)
RockYou (2009)
Battleﬁeld Heroes (2011)
COMPARISON OF YAHOO! DATA WITH LEAKED DATA SETS
ˆH∞ ˆ˜λ10
9.1
6.5
8.9
6.8
7.7
9.8
ˆ˜G0.25
17.6
15.9
16.5
ˆ˜G0.5
21.6
19.8
20.0
M
69301337
32603388
548774