### Experiment Approval and Deployment
The experiment was approved by Yahoo!'s legal team and the responsible ethics committee at the University of Cambridge. It was conducted by a Yahoo! manager and utilized machine-generated entropy. The experiment was deployed on a random subset of Yahoo! servers over a 48-hour period from May 23 to May 25, 2011. During this time, we observed 69,301,337 unique users and constructed separate histograms for 328 different predicate functions. Many of these histograms did not achieve a sufficient sample size to be useful and were discarded.

### Effects of Sample Size
In our mathematical treatment of guessing difficulty, we assumed complete information about the underlying probability distribution of passwords \( X \). In practice, we approximate \( X \) using empirical data. We assume we have \( M \) independent samples \( X_1, X_2, \ldots, X_M \) drawn from \( X \) and wish to calculate properties of \( X \).

The simplest approach is to compute metrics using the distribution of samples directly, denoted as \( \hat{X} \). As shown in Figure 3, this approach often produces substantial and systematic under-estimates of most metrics, particularly \( \hat{H}_0 = \log_2 \hat{N} \), which increases nearly continuously with increasing sample size \( M \). This indicates that new passwords are still being discovered even at large sample sizes.

The maximum-likelihood estimation of the growth rate \( \frac{d\hat{N}}{dM} \) has been shown to be exactly \( \frac{V(1, M)}{M} \), the proportion of passwords in the sample observed only once [42]. For our full sample, \( V(1, M) = 42.5\% \), indicating that a larger sample would continue to find many new passwords, leading to larger estimates for \( H_0, H_1, G_1 \), etc. Similarly, for a random subsample of our data, many passwords will be missed, and estimates of these metrics will decrease.

Interpreting hapax legomena (events observed only once in a sample) is a fundamental problem in statistics, and there are no known non-parametric techniques for estimating the true distribution size \( N \) [42]. This is not just a theoretical restriction; determining that apparently pseudorandom passwords are truly 128-bit random strings would require an intractably large sample size, much greater than \( 2^{128} \). Good-Turing techniques [43] are not helpful for the distribution-wide statistics we are interested in; they can only estimate the cumulative probability of all unobserved events (the "missing mass") and provide damped maximum-likelihood estimates of the probability of individual events.

Fortunately, in practice, we can usefully approximate our guessing metrics from reasonably-sized samples, though these estimations implicitly rely on assumptions about the underlying nature of the password distribution. As seen in Figure 3, partial guessing metrics that rely only on the more frequent items in the distribution are the easiest to approximate, while those that rely on a summation over the entire distribution, such as \( H_0, H_1 \), and \( \tilde{\mu}_\alpha, \tilde{G}_\alpha \) for large values of \( \alpha \), will be the most difficult.

### The Region of Stability
We can reliably estimate \( p_i \) for events with observed frequency \( f_i \gg 1 \) due to the law of large numbers. Estimating \( H_\infty \) requires estimating only \( p_1 \), the probability of the most common password, which was 1.08% in our dataset. Gaussian statistics can be used to estimate the standard error of the maximum-likelihood estimate \( \hat{p}_i \):

\[
\text{error}(\hat{p}_i) = \sqrt{\frac{p_i (1 - p_i)}{M}} \approx \frac{1}{\sqrt{f_i}}
\]

For our dataset, this gives a standard error of under 0.1 bit in \( \hat{H}_\infty \) for \( M \geq 2^{14} \). This argument extends to \( \hat{\tilde{\lambda}}_\beta \) for small values of \( \beta \), and in practice, we can measure resistance to online guessing with relatively modest sample sizes.

Reasoning about the error in \( \hat{\tilde{\mu}}_\alpha \) and \( \hat{\tilde{G}}_\alpha \) for values of \( \alpha \) representing realistic brute-force attacks is more difficult. Fortunately, we observe that for our password dataset, the number of events \( V(f, M) \) occurring \( f \) times in a sample of size \( M \) is very consistent for small \( f \) and provides a reasonable estimate of the number of events with probability \( \frac{f - 0.5}{M} \leq p \leq \frac{f + 0.5}{M} \).

This enables a useful heuristic that \( \tilde{\mu}_\alpha \) and \( \tilde{G}_\alpha \) will be well approximated when \( \alpha \) is small enough to only rely on events occurring greater than some small frequency \( f \). Calling \( \alpha_f \) the cumulative estimated probability of all events occurring at least \( f \) times, we took 1,000 random samples of our corpus with \( M = 2^{19} \) and observed the following values in the 1st and 99th percentiles:

| \( f \) | \( \alpha_f \) | \( \tilde{\mu}_{\alpha_f} - \hat{\tilde{\mu}}_{\alpha_f} \) | \( \tilde{G}_{\alpha_f} - \hat{\tilde{G}}_{\alpha_f} \) |
|--------|---------------|--------------------------------------------------|-------------------------------------------------|
| 6      | 0.162–0.163   | 0.153–0.154                                      | 0.145–0.146                                     |
| 7      | 0.157–0.180   | 0.125–0.148                                      | 0.103–0.127                                     |
| 8      | 0.155–0.176   | 0.123–0.146                                      | 0.101–0.126                                     |

We observed very similar values for larger values of \( M \). Thus, we will use \( \hat{\tilde{\mu}}_\alpha \) and \( \hat{\tilde{G}}_\alpha \) directly for \( \alpha \leq \alpha_6 \) for random subsamples of our data. The utility of this heuristic is seen in Figure 3, where it accurately predicts the point at which \( \tilde{\mu}_{0.25} \) stabilizes, and in Figure 4, where it marks the point below which \( \tilde{\mu}_\alpha \) is inaccurate for varying \( M \).

### Parametric Extension of Our Approximations
Estimating \( \tilde{\mu}_\alpha \) and \( \tilde{G}_\alpha \) for higher \( \alpha \) requires directly assuming a model for the underlying password distribution. Passwords have been conjectured to follow a power-law distribution [13]:

\[
\Pr[p(x) > y] \propto y^{1-a}
\]

Unfortunately, using a power-law distribution is problematic for two reasons. First, estimates for the scale parameter \( a \) are known to decrease significantly with sample size [42]. Using maximum-likelihood fitting techniques [44] for our observed count data, we get the following estimates:

| \( M \)        | 69M         | 10M         | 1M          | 100k         |
|----------------|-------------|-------------|-------------|--------------|
| \( \hat{a} \)  | 4.21        | 3.23        | 2.99        | 3.70         |

A second problem is that this model fits our observed, integer counts. To correctly estimate \( \tilde{\mu}_\alpha \) from samples, we need to model the presence of passwords for which \( p_i \cdot M < 1 \). We propose a parametric extension of the power-law model, adding a constant term \( c \) and a geometric tail \( g \):

\[
\Pr[p(x) > y] \propto y^{1-a} + c \cdot g^y
\]

Using maximum-likelihood fitting techniques, we fit this model to our data and obtained the following parameters: \( \hat{a} = 3.98 \), \( \hat{c} = 0.002 \), \( \hat{g} = 0.99 \). This model fits the data well, with a Kolmogorov-Smirnov test (D = 0.005, p-value > 0.99) supporting the hypothesis that our sample was drawn from the modeled distribution.

Our goal is to accurately compare statistics for differently-sized subsamples of our data. Doing so using our empirical precision estimates directly is accurate only under the assumption that two different subpopulations have each chosen a distribution of passwords that our model fits equally well. If some definable population of user-generated passwords forms a very different underlying distribution (e.g., uniform or exponential), our model might produce more variable estimates. When analyzing our data in Section VI, we thus make a weaker claim that different demographic subsamples of users are significantly different from the global population of users if our extrapolation produces estimates outside the 1st or 99th percentile of estimates observed for similarly-sized random samples as listed in this section.

### Analysis of Yahoo! Data
#### External Comparison
We first compare our collected data to several known datasets. To the author's knowledge, there have been two large-scale leaks of password data suitable for statistical analysis: the 2009 RockYou leak and a 2011 leak of roughly 500,000 passwords from the gaming website Battlefield Heroes. Guessing metrics for these distributions and our collected data are listed in Table III. All three distributions, despite being taken from substantially different populations, agree to within 1 bit for estimates of online attacks (\( H_\infty \) and \( \tilde{\lambda}_{10} \)), and within 2 bits for offline attacks (\( \tilde{G}_{0.25} \) and \( \tilde{G}_{0.5} \)).

We plot the guessing curve for our collected data in Figure 6 along with that of the RockYou distribution. Supporting this assumption, we find that our model produces similarly accurate estimates for subsamples of the RockYou distribution, the only other large password dataset to which we have access.

| Dataset                 | \( \hat{H}_\infty \) | \( \hat{\tilde{\lambda}}_{10} \) | \( \hat{\tilde{G}}_{0.25} \) | \( \hat{\tilde{G}}_{0.5} \) | \( M \)       |
|-------------------------|---------------------|---------------------------------|-----------------------------|-----------------------------|---------------|
| Yahoo! (2011)           | 9.1                 | 6.5                             | 17.6                        | 21.6                        | 69,301,337    |
| RockYou (2009)          | 8.9                 | 6.8                             | 15.9                        | 19.8                        | 32,603,388    |
| Battlefield Heroes (2011)| 7.7                 | 9.8                             | 16.5                        | 20.0                        | 548,774       |

**Note:**
- The Battlefield Heroes passwords were hashed with MD5 but without any salt, making analysis of the distribution possible.
- A prominent 2010 leak revealed nearly 1 million passwords from the blogging site Gawker, but these were salted via the Unix `crypt()` function, preventing full analysis of the distribution.