you are using Git it is one of two possible solutions.
[ 440 ]
Tips and Tricks Chapter 16
A Git submodule is basically a reference to another Git repository from within another.
Hence, it does not actually contain the code of the submodule—it simply contains a
reference to it that can be cloned and updated as required. Suppose that you are writing a
playbook to install Apache 2 on a server and you decide that, rather than write your own
module, you are going to use Jeff Geerling's Apache 2 role from GitHub (https:/​/​github.
com/​geerlingguy/​ansible-​role-​apache):
1. Before you start, you will need to have your playbook directory structure created
and checked into your version control system. Then, ensure you have the roles/
directory in your playbook structure as normal and change to this directory:
$ mkdir roles
$ cd roles
2. Now, check out the code you want as a submodule, providing the Git tool with a
directory name to clone it to—in our case, we'll call it jeffgeerling.apache2:
$ git submodule add
https://github.com/geerlingguy/ansible-role-apache.git
jeffgeerling.apache2
3. Once this is done, you will notice that you have a new file at the root of your
working copy called .gitmodules. You will need to add this and the directory
created by the submodule add command to your repository:
$ git add ../.gitmodules jeffgeerling.apache2
$ git commit -m "Added Apache2 submodule as role"
$ git push
That's all there is to it—you now have this role stored within your playbook
directory structure, but as far as Git is concerned, it is stored elsewhere. The
whole process should look something like what's shown in the following
screenshot:
[ 441 ]
Tips and Tricks Chapter 16
To update this submodule at any time, you must change into the directory you created for
it earlier and then run a standard git pull command. The beauty of this is that, as far as
Git is concerned, the submodule is just another repository and so you can run all of the
usual subcommands you are used to such as push, pull, status, and so on.
The only thing to add is that, when you clone the playbook directory for the first time from
your Git server, although it will be aware of the submodule, it doesn't actually check out
the code. Hence, when you clone for the first time, you must run the following commands:
$ git clone 
$ cd 
$ git submodule init
$ git submodule update
From here, you can use the working copy and submodule exactly as described previously.
[ 442 ]
Tips and Tricks Chapter 16
The other way to solve the problem of role code reuse is to make use of the ansible-
galaxy tool. We saw ansible-galaxy in action in Chapter 2, Automating Your IT
Infrastructure with Ansible, where we demonstrated it as a way of cloning publicly available
roles from the Ansible Galaxy web site (https:/​/​galaxy.​ansible.​com/​). However,
ansible-galaxy can also clone roles from a valid Git URL.
Suppose we wanted to achieve exactly what we have just done with the Apache 2 role, but
without using Git submodules. Instead, we create a file called requirements.yml in the
base directory of the playbook structure.
To clone the role we just used, our requirements.yml file would need to look like this:
---
- src: https://github.com/geerlingguy/ansible-role-apache.git
scm: git
You can, of course, have more than one requirement in this file—just specify them as a
standard YAML list. When you have completed this file, you can then download the roles
to your working copy using this command:
$ ansible-galaxy install -r requirements.yml --roles-path roles
This clones the Git repository referenced by the src parameter in requirements.yml into
the roles/ directory. Note that we do not customize the directory name, so the one from
the Git repository is used for the role name (in this case, ansible-role-apache). The
following screenshot shows an example of this being completed:
[ 443 ]
Tips and Tricks Chapter 16
Unlike the submodules, ansible-galaxy does not actually clone the repository as a
working copy; hence, you cannot simply change into its directory and run git
pull command to update it to the latest version. Instead, requirements.yml should
remain in your working copy, and in the future, to update, you would run the following
command:
$ ansible-galaxy install -r requirements.yml --roles-path roles --force
The --force parameter instructs ansible-galaxy to download the role even if it is
already downloaded, hence overwriting the version you have already installed.
We have only scratched the surface of what can be achieved with
requirements.yml—you can download from private repositories,
ensure you only ever download a specific Git version, and more—this is
left as an exercise for you to investigate.
Hence, you have two completely different yet equally valid ways to efficiently reuse roles
by storing them individually in a source control system. By considering everything in this
section, including the decision to use AWX or Ansible Tower, you should have a robust and
scalable automation architecture built around Ansible.
In the next section, we will address another facet of Ansible that has so far not received a
great deal of attention by virtue of our simple example structure, and yet is vital to its
operation—the inventory.
Inventories – maintaining a single source of
truth
We have worked hard throughout this book to build an automation architecture that
implements good practices for your enterprise. For example, when it comes to managing
your Ansible playbooks and roles, we have strongly encouraged the use of version control
systems and including roles from source control so that there is always a single source of
truth for your Ansible code.
[ 444 ]
Tips and Tricks Chapter 16
However, in our examples throughout this book, we have worked with very simple, static
inventory files that feature, at most, a handful of hosts. Naturally, your enterprise won't
look like this—the whole goal of automation is to be able to handle a large infrastructure of
hundreds of machines with ease and grace and to be able to cope with changes in that
infrastructure efficiently and effectively.
Most enterprises that begin their automation journey are not starting from scratch—it is
anticipated that many who read this book will already have a Linux estate of some size that
they need to manage more effectively, and so will already have a list of machines that need
automation applying to them.
This completes our problem statement—imagine that you have an estate of Linux servers
comprised of hundreds of machines and have built up a scalable automation system using
Ansible and AWX/Ansible Tower, with all code stored in version control and roles actively
being reused. Why then would you want to manually type out those hundreds of
hostnames into a flat-text inventory file?
Further, whenever a new Linux machine is commissioned (or an old one
decommissioned)—a not uncommon task in this age of virtualization—imagine having to
manually edit that inventory file and ensuring it is in sync with what your estate actually
looks like.
In short, this situation is not acceptable. It is not scalable and will very quickly become
unmanageable. How can you, to pick one example, be confident that all of the servers in
your estate have had the CIS Benchmark applied to them if you are not confident that your
inventory contains all your servers to begin with?
Thankfully, Ansible includes a solution to this too, in the form of dynamic inventory script,
and we will look at the anatomy of these in the next section.
Working with Ansible dynamic inventories
To keep the examples in this book simple and to focus on the automation code being
written, we have made use of the simple inifile format of inventory that Ansible
supports. However, Ansible can ingest inventory data in JSON format, which can be passed
to it by any executable script.
[ 445 ]
Tips and Tricks Chapter 16
Almost every Linux machine these days will exist within some ecosystem, be it a public
cloud provider such as AWS or Azure, a private cloud environment such as OpenStack, or
a traditional virtualization environment such as VMware or oVirt. All of these systems
already know what their inventory is, although they do not use that term as such. For
example, if you run a set of Linux virtual machines in Amazon EC2 or OpenStack, both
systems know exactly what those machines are and what they are called. Similarly, if you
spin them up in VMware or oVirt, the hypervisor managers know what machines are
running and what they are called.
In essence, what we are saying is that just about every infrastructure management system
already has a kind of inventory that Ansible can use. Our task is to extract that inventory
and convert it into the JSON format that Ansible understands so that it can use it.
Thankfully, the developers and contributors involved in the Ansible project have already
developed dynamic inventory scripts that cover a wide array of systems. If you look at the
project's Ansible repository (https:/​/​github.​com/​ansible/​ansible/​tree/​devel/​contrib/
inventory), you will see all of the currently available inventory scripts. Most of them are
written in Python, but you can write it in any language that your operating system can
execute—it can even be a shell script if you wish!
In short, if you need a dynamic inventory, there is a good chance that it already exists and
you can make use of the existing script. If you are making use of AWX/Ansible Tower, all of
these scripts along with their required libraries are all pre-installed, which makes it
incredibly easy to get started.
If, however, you are using Ansible in the shell, note that many of the scripts will require
additional libraries to function. For example, the ec2.py script for producing a dynamic
inventory from Amazon EC2 requires the boto Python library, which may not be pre-
installed. For example, we could download and run the ec2.py script by executing these
commands:
$ wget
https://raw.githubusercontent.com/ansible/ansible/devel/contrib/inventory/e
c2.py
$ chmod +x ec2.py
$ ./ec2.py
[ 446 ]
Tips and Tricks Chapter 16
We would expect the preceding commands to fail because we have not configured the
dynamic inventory script with our AWS account data—however, if you perform this
without checking the prerequisites (such as the boto library), you will be presented with an
error such as this:
The exact fix for this will be dependent on your operating system—on Ubuntu Server 18.04,
I can fix this by running this:
$ sudo apt install python-boto
On CentOS 7, you will need the EPEL repositories configured, and then you can install it
using a command like this:
$ sudo yum install python2-boto
Each dynamic inventory script will have different pre-requisites—some might not even
have any! In addition to the dependencies, you must also configure the script as it will (at a
bare minimum) require authentication parameters so that it can query the upstream source
for the inventory. You will find that the configuration file is alongside the dynamic
inventory script—hence, for our example ec2.py script, you could download the example
configuration file using the following command:
$ wget
https://raw.githubusercontent.com/ansible/ansible/devel/contrib/inventory/e
c2.ini
[ 447 ]
Tips and Tricks Chapter 16
Both the template configuration file and the comments at the beginning of the dynamic
inventory script provide a great deal of documentation and information on how the scripts
work and how to make use of them. Be sure to read these when implementing these scripts
as it will save you a lot of time when implementing them.
It is not anticipated that everyone reading this book will have an AWS account to test
dynamic inventory scripts against, so completing this exercise is left for you.
Finally, it should be noted that, although many dynamic inventory scripts have been
provided, there will be systems that do not have a dynamic inventory script available yet.
Perhaps you have your own in-house Configuration Management System (CMS)—in this
instance, as long as you can extract the data from it, you can write your own dynamic
inventory plugin. The Ansible project provides you with some guidance and example code
to get you started here: https:/​/​docs.​ansible.​com/​ansible/​latest/​dev_​guide/
developing_​inventory.​html.
The beauty of open source software is that you can even contribute it back to the Ansible
project so that others may benefit from your work (if you so wish). In short, just as you
should always reuse your role code and ensure it is version controlled, so should you make
use of dynamic inventories wherever possible.
Before we finish our look at dynamic inventory scripts, we will complete a simple worked
example that anyone can try in their environment.
Example – working with the Cobbler dynamic
inventory
Cobbler is an open source provisioning system that provides a framework for managing
your PXE-based installs. It is embedded in the Spacewalk project (and Red Hat Satellite
Server 5.x) and can be used standalone if you require a management framework for your
PXE boot environment (rather than managing it by hand as we did in Chapter 6, Custom
Builds with PXE Booting).
Although the actual use of Cobbler is beyond the scope of this book, it serves as an excellent
example for our dynamic inventory section of this book because it is extremely easy to get
up and running with.
[ 448 ]
Tips and Tricks Chapter 16
If you are considering the use of Katello for patch management, as
discussed in Chapter 9, Patching with Katello, note that Katello also
provides a robust framework for managing PXE-based installs and it is
recommended you investigate this for this purpose so that you are using
one tool for both processes. This supports our principle of commonality
discussed in Chapter 1, Building a Standard Operating Environment on
Linux. You would use the foreman.py dynamic inventory script to work
with Katello in your environment.
To get started with this example, you will need a demo system that Cobbler is packaged
for—at the time of writing, there are no native packages for Ubuntu Server 18.04, so we will
install our Cobbler server on CentOS 7. Your dynamic inventory script can be run from an
Ubuntu Server machine, though—the only requirement is that it can communicate with
your Cobbler server on the network:
1. To get started, install the minimum required Cobbler packages on your CentOS 7
system using the following command:
$ sudo yum -y install cobbler cobbler-web
2. The default configuration for Cobbler should be fine for our simple dynamic
inventory test purposes, so we will start the server with this command:
$ sudo systemctl start cobblerd.service
3. Next, we will create distro and profile for our systems—when using Cobbler
for actual PXE-based installs, distro describes the operating system and
specifies items such as the kernel and initial RAMDisk to be used. These
commands should work on your CentOS 7 test system, but be aware that if you
don't have these specific kernel files installed, you must change these to reference
the kernel you have installed:
$ sudo cobbler distro add --name=CentOS --
kernel=/boot/vmlinuz-3.10.0-957.el7.x86_64 --
initrd=/boot/initramfs-3.10.0-957.el7.x86_64.img
$ sudo cobbler profile add --name=webservers --distro=CentOS
[ 449 ]
Tips and Tricks Chapter 16
4. It appears that Cobbler does not function with the out of the box SELinux policy
that runs on CentOS 7—in a production environment, you would modify the
policy to support Cobbler correctly. For the sake of this simple demo, you can
simply disable SELinux using this command:
$ sudo setenforce 0
Just don't do this in a production environment!
5. With our prerequisite steps completed, we can now commence adding our actual
systems to the Cobbler inventory. We will add two frontend web servers to our
webservers group using the following commands:
$ cobbler system add --name=frontend01 --profile=webservers --dns-
name=frontend01.example.com --interface=eth0
$ cobbler system add --name=frontend02 --profile=webservers --dns-
name=frontend02.example.com --interface=eth0
The --dns-name parameter should be an actual resolvable DNS name in your
test environment for this test to work—I am adding them to /etc/hosts on my
Ansible server for this test but, again, in a production environment, you would
not do this.
6. Cobbler is now set up and has an inventory of two hosts in a group (profile)
called webservers. Now, we can move back to our Ansible server. On this
machine, download the Cobbler dynamic inventory script and its associated
configuration file by running this:
$ wget
https://raw.githubusercontent.com/ansible/ansible/devel/contrib/inv
entory/cobbler.py
$ wget
https://raw.githubusercontent.com/ansible/ansible/devel/contrib/inv
entory/cobbler.ini
$ chmod +x cobbler.py
7. Now, edit the configuration file, cobbler.ini—near the top of this file, you will
see a few lines that look like this:
[cobbler]
host = http://PATH_TO_COBBLER_SERVER/cobbler_api
Change the PATH_TO_COBBLER_SERVER string to the hostname or IP address of
the machine you just installed Cobbler on. That's all there is to it!
[ 450 ]
Tips and Tricks Chapter 16
8. Now, you can run Ansible and use an ad hoc command to test your dynamic
inventory—simply run this:
$ ansible webservers -i cobbler.py -m ping
You will observe that we are telling Ansible to only perform this action on the webservers
group from the inventory specified by the -i parameter—which, in this case, is our Cobbler
dynamic inventory script. If all has gone well, your output should look something like this
screenshot:
In this case, the deprecation warning is about the output from the Cobbler dynamic
inventory script, which suggests it might need updating to work with Ansible 2.10 onward.
However, we can see that Ansible can extract the inventory from the Cobbler server and
use it for our simple ad hoc command—this would work just as well with a whole
playbook!
[ 451 ]
Tips and Tricks Chapter 16
Play with the Cobbler server; try adding and removing systems and see how Ansible
retrieves the up to date inventory each and every time. Using other dynamic inventory
scripts can be a little more involved, but it is not complicated provided you refer to the
documentation and examples that ship with each. The time spent learning this will more
than pay off later in terms of making your life easier and your inventories more accurate.
In the final section of this chapter, we will look a little deeper at ad hoc commands and how
they can help you with one-off tasks.
Running one-off tasks with Ansible
In the previous chapter, we used the ansible webservers -i cobbler.py -m
ping command to test connectivity to all of the servers in the webservers group of our