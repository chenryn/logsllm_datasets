This is tough for me to reliably reproduce, but I've seen it a half-dozen
times and think it needs an issue. Elasticsearch can still lose inserted
documents when a single primary node is isolated by a simple network
partition. Nothing fancy, no overlapping partitions required. Sometimes it's
only a few documents, from immediately following the partition:
    {:valid? false,
     :lost "#{640 643 645 649}",
     :recovered
     "#{140..142 144 146 148..152 155 157 159..162 164..168 171 174..175 177 179..185 187 190..192 194 197 199..200 202 205..208 214 224 234 238 240..247 249..254 256..264 266..282 285 296 317 342 363 388 431}",
     :ok
     "#{0..137 140..142 144 146 148..152 155 157 159..162 164..168 171 174..175 177 179..185 187 190..192 194 197 199..200 202 205..208 210..212 214 224 234 238 240..247 249..254 256..264 266..639 641..642 644 646..648 650..654}",
     :recovered-frac 96/655,
     :unexpected-frac 0,
     :unexpected "#{}",
     :lost-frac 4/655,
     :ok-frac 598/655}
Other times, the cluster seems to get really confused, and loses over half of
the acknowledged records, distributed throughout the history:
    {:valid? false,
     :lost
     "#{53..55 126..129 131..142 144..167 169..189 191..212 214..235 237..260 262..281 283..305 307..327 329..352 354..367 374 376}",
     :recovered
     "#{46..47 49 52 60 428..430 432..435 437..440 442..445 447..450 452..455 457..473}",
     :ok
     "#{0..47 49 52 60 428..430 432..435 437..440 442..445 447..450 452..455 457..473}",
     :recovered-frac 15/158,
     :unexpected-frac 0,
     :unexpected "#{}",
     :lost-frac 118/237,
     :ok-frac 91/474}
The logs aren't particularly informative here, but I have noticed some
interesting messages which seem correlated with data loss: for instance:
    [2015-04-02][WARN ][index.shard] [n4] [jepsen-index][0] suspect illegal state: trying to move shard from primary mode to replica mode
and
    [2015-04-02 17:19:54,370][DEBUG][action.index             ] [n2] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
which I think is a hardcoded message.
Again, these are tough to reproduce--only 1 in 10 runs or so--so I'll keep
working on finding a failure schedule that reliably triggers the fault.