f
(x) = relu(bκ) =
(6)
This denotes a horizontal line (i.e., the blue segment on the left
of x1 in Figure 10.
Case (II): x1 ≤ x ≤ x2, f
Figure 9) and f
(k +1)
θ
(k +1)
γ
(x) is an ascending line (in blue in
(x) = 0 is a horizontal line (in orange),
(k +2)
κ
f
(x) = r elu(w(γ ,κ) · f
(k +1)
γ
(x) + bκ)
= r elu(w(γ ,κ) · w(α ,γ ) · x + w(γ ,κ) · (w(β,γ ) · vβ + bγ ) + bκ)
= r elu(w(γ ,κ) · w(α ,γ ) · x + cκ), with cκ = w(γ ,κ) · (w(β,γ ) · vβ + bγ ) + bκ
(cid:26)w(γ ,κ) · w(α ,γ ) · x + cκ
x3 ≤ x ≤ x2
x1 ≤ x  x4,
both NSFs become a straight line. Hence, our sampling algorithm
starts from the value of α observed during the model execution,
and proceeds towards the two ends (e.g., one direction is to make
x larger and the other is to make it smaller). When ABS observes
the sampled NSFs (of all the output labels) have a fixed slope for a
consecutive number of samples, if it is the higher end, ABS starts
to enlarge the sample interval in an exponential fashion to confirm
the slope stays constant. If so, ABS terminates for the higher end.
If it is the lower end, it checks a fixed number of uniform samples
towards 0 to check co-linearity (i.e., if the slope stays the same).
Identifying Appropriate Sampling Interval. Sampling with a small
interval leads to high overhead, whereas sampling with a large
interval may miss peaks that suggest trojaned behavior. We develop
an adaptive sampling method. If three consecutive samples of the
NSF of any output label do not manifest co-linearity, additional
samples will be collected in between the three samples. Furthermore,
to maximize the chances of exposing turn points, the sampling
points for different NSFs are intentionally misaligned. Consider
Figure 12. It shows two NSFs, NSFA and NSFC. There is a very sharp
peak on NSFA missed by the samples on NSFA (as the three circles
are co-linear). However, since the sampling points of NSFC are not
the same as NSFA, but rather having some constant offset from the
samples points of NSFA, the lack of co-linearity of the samples on
NSFC causes ABS to collect additional samples, which expose the
peak in NSFA. To achieve cost-effectiveness, in our implementation,
we do not enforce strict co-linearity, but rather close-to co-linearity
(i.e., the slope differences are small). The formal definition of the
sampling algorithm is elided.
4.2 Identifying Compromised Neuron
Candidates
The stimulation analysis identifies the NSFs for each neuron in the
model. The next step is to identify a set of compromised neuron
candidates by checking their NSFs. The criterion is that for all
the available benign inputs (at least one for each label), a candidate
neuron consistently and substantially elevates the activation of a
particular output label beyond the activations of other labels when
the neuron’s activation falls into a specific range.
Figure 8: Stimulation
Analysis: Model
Figure 9: Stimulation
Analysis: Layer Lk +1
Figure 10: Stimulation
Analysis: Layer Lk +2
Figure 11: Stimulation
Analysis: Layer Lk +3
Figure 12: Mis-aligned
Sampling
Algorithm 1 describes the procedure of selecting a most likely
candidate. It can be easily extended to select a set of most likely
candidates. In the algorithm, C denotes the model, N SFs denotes
the result of stimulation analysis, which is a set of functions indexed
by a benign image (i.e., the image used to execute the model to
generate all the concrete neuron activations), the neuron, and the
output label; base_imдs denotes the set of benign images used in
our analysis. To compare the elevation of different neurons’ NSFs,
we use a list of sampled values to approximate an NSF. The loop in
lines 4-19 identifies the most likely candidate neuron. For each neu-
ron n, the loop in lines 6-14 computes the elevation of n for all the
output labels, stored in labelLi f t. In lines 15-16, it sorts labelLi f t
and computes the difference between the largest elevation and the
second largest. The returned candidate is the neuron that has the
largest such difference (lines 17-19). Note that we do not simply
return the one with the largest elevation. The reason will be ex-
plained later. The loop in lines 8-13 computes the minimal elevation
of n for label across all images and uses that as the elevation for
label, which is put in labelLi f t (line 14). The elevation for an image
imд is computed by the peak of the NSF and the activation value
observed when running C on imд (line 11).
label Lif t = []
for label in C .labels do
min_imд_v = ∞
for imд in base_imдs do
max_n = 0
max_v = 0
for n in C .neurons do
Algorithm 1 Compromised Neuron Candidate Identification
1: function identify_candidate(C, N S F s, base_imдs)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
if imд.label == label then
imд_v = max(N S F s[label, n, imд](x)) − C(imд)[label]
if imд_v  max_v then
min_imд_v = imд_v
continue
max_v = n_v
max_n = n
return max_n
Using Maximum Elevation Difference Instead of Maximum
Elevation. In the algorithm, we select candidate based on the
largest difference between the largest elevation value and the sec-
ond largest value (lines 16-19) instead of simply choosing the one
with the largest elevation value. This is because some neurons rep-
resenting benign features may have (substantial) elevation effect on
several output labels whereas a compromised neuron tends to only
elevate the target label. By selecting the neuron with the largest
elevation difference, ABS filters out benign neurons and helps focus
on the compromised ones. Figure 13(a) presents the NSFs of a be-
nign neuron on the left and a compromised neuron on the right for
(a) Examples for Using Elevation Difference Instead Elevation
(b) Examples for Using Minimal Elevation Across Images
Figure 13: NSFs to Illustrate Selection of Compromised Neu-
ron Candidates; x axis denotes neuron activation; y denotes output
activation; each line denotes an NSF
a NiN model [36] on the CIFAR-10 [31] dataset. Each line denotes
one output label. Observe that both neurons lift the value of output
label 0. However the benign neuron also lifts label 7 at a similar
scale. Intuitively, it suggests that the benign neuron represents a
common feature shared by labels 0 and 7.
Using the Minimum Elevation Across All Images As Label
Elevation. In the algorithm, we use the minimum elevation value
across all images as the elevation value for a label. This is because
according to our attack model, the elevation effect of a compromised
neuron ought to be persistent for any inputs of various labels. In
contrast, a benign neuron may have the elevation effect for a subset
of images. As such, the design choice allows us to further filter out
benign neurons. Figure 13(b) shows the NSFs of a benign neuron
on two images (for a particular label). Observe that the elevation
(i.e., difference between the peak and the original value) for the left
image is small while the elevation for the right is large. ABS uses
the left elevation as the elevation for the label.
4.3 Validating Compromised Neuron
Candidates by Generating Trojan Triggers
After acquiring candidates, ABS further identifies the real compro-
mised neurons by generating trojan trigger. Ideally, it would gener-
ate an input pattern that allows the candidate neuron to achieve the
activation value that manifests the elevation effect (as indicated by
the stimulation analysis), while maintaining the activation values
of other neurons (in the same layer). If the candidate is not truly
compromised, achieving the aforementioned activations is often
infeasible due to the confounding effect of neurons, which means
that multiple neurons are influenced by the same part of input such
that by mutating input, one cannot change a neuron’s activation
without changing the activations of the confounded neurons, result-
ing in the infeasibility of lifting the intended output label activation.
Figure 14: Feature Space Attack
In a well-trained model, benign neurons are often substantially con-
founded. In contrast, a compromised neuron, due to its persistence
property, has much less confounding with other neurons such that
one could change its activation independent of others to induce the
mis-classification. An example can be found in Appendix A.
Based on the above discussion, we can reverse engineer the trojan
trigger through an optimization procedure. Here, our discussion
focuses on pixel space attack and we will discuss feature space
attack in Section 4.4. Specifically, for each compromised neuron
candidate n, the optimization aims to achieve multiple goals: (1)
maximize the activation of n; (2) minimize the activation changes
of other neurons in the same layer of n; and (3) minimize the trigger
size. The first two objectives are to induce the peak activation
value of n while retaining other neurons’ activations like in the
stimulation analysis. We use an input mask to control the trigger
region and trigger size. Intuitively, an mask is a vector that has
the same size of an input. Its value is either 0 or 1. Stamping a
trigger on an input x can be achieved by x ◦ (1 − mask) + triддer ◦
mask with ◦ the Hadamard product operation. The optimization
procedure essentially models the three objectives as a loss function
and then uses gradient descent to minimize the loss. It is described
in Algorithm 2.
Function reverse_engineer_trigger() in lines 5-21 is the main
procedure. Parameter model denotes the model; l and n denote
the layer and the candidate neuron, respectively; e denotes the
number of epochs; lr denotes the learning rate; max_mask_size
is the maximum size of trigger; apply denotes the function that
applies the trigger to an image. It could be either pixel_apply() in
lines 1-2 for pixel space attack or feature_apply() in lines 3-4 for
feature space attack. The latter will be explained in the next section.
Line 6 initializes triддer and mask, with triддer usually initialized
to be the same as the input image and mask usually initialized to a
random array. Line 7 defines the perturbed input x′ by applying the
trigger to x. In lines 8-12, we define components of the loss function.
In line 8, we define f1 to be the activation value of the candidate
neuron. We want to maximize f1 such that its weight in the loss
function (line 15) is negative. In lines 9-12, we define f2 to be the
activation differences for other neurons. We want to minimize f2
such that its weight in line 15 is positive. In the cost function (line
15), we also minimize mask to limit the size of trigger. Lines 16 and
17 add a large penalty to the loss if the mask size (measured by
sum(mask)) is larger than the threshold max_mask_size, which is
the bound of trigger size. The loop in lines 14 to 21 performs the
iterative optimization.
4.4 Handling Feature Space Attack
Unlike pixel space attack, feature space attack does not have a
fixed pixel pattern that can trigger targeted mis-classification. In-
stead, it plants hard-to-interpret features in training data. The pixel
level mutation caused by such feature injection is usually input
x = x ◦ (1 − mask) + triддer ◦ mask return x
x = stack([x, maxpool(x), minpool(x), avдpool(x)]) · triддer return x
triддer, mask = initialize(x)
x′def
= apply(x, triддer, mask)
def
= model .layers[l].neurons[n](x′)
f1
def
= model .layers[l].neurons[: n](x′)
f2
+model .layers[l].neurons[n + 1 :](x′)
−model .layers[l].neurons[: n](x)