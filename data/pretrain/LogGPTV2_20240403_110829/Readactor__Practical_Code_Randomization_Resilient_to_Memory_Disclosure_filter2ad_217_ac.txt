

	

	

	


	

	

	
Direct disclosure
possible
Adversary
Read access
prevented by
Readactor
Figure 6: We rewrite switch-case tables to be executable
instructions, rather than data embedded in executable code.
A. Fine-grained Code DiversiÔ¨Åcation
Our compiler supports several Ô¨Åne-grained code random-
ization techniques: function permutation, basic-block insertion,
NOP (no-operation) insertion, instruction schedule random-
ization, equivalent instruction substitution, register allocation
randomization, and callee-saved register save slot reordering.
The last
technique randomizes the stack locations that a
function uses to save and restore register values that it must
preserve during its execution. In our prototype implementation
of Readactor, we use function permutation [37], register
allocation randomization, and callee-saved register save slot
reordering [49]. We selected these transformations because
they permute the code layout effectively, have low performance
impact, and make the dataÔ¨Çow between gadgets unpredictable.
Our prototype implementation performs Ô¨Åne-grained code
randomization at compile-time. With additional implementation
effort, we can make the compiler emit binaries that randomize
themselves at load-time [8, 44, 65]. Self-randomizing binaries
eliminate the need to generate and distribute multiple distinct
binaries, which improves the practicality of diversity. However,
the security properties of compile-time and load-time solutions
are largely similar. Hence, we focus on how to randomize pro-
grams and how to protect the code from disclosure irrespective
of when randomization happens.
B. Code and Data Separation
To increase efÔ¨Åciency, compilers sometimes intersperse code
and data. Since Readactor enforces execute-only permissions
for code pages, we must prevent the compiler from embedding
data in the code it generates. That is, we must generate Harvard-
architecture compatible code. If we do not strictly separate code
and data, we run the risk of raising false alarms as a result of
benign attempts to read data from code pages.
We found that the LLVM compiler only emits data in the
executable .text section of x86 binaries when optimizing a
switch-case statement. LLVM emits the basic block address
corresponding to each switch-case in a table after the current
function. As shown in the left part of Figure 6, the switch
statement is then implemented as a load from this table and
an indirect branch to the loaded address.
Our compiler translates switch statements to a table of
direct branches rather than a list of code pointers that an
attacker can read. Each direct branch targets the Ô¨Årst basic
block corresponding to a switch-case. The switch statement
is then generated as an indirect branch into the sequence
of direct branches rather than an indirect load and branch
sequence. This entirely avoids emitting the switch-case pointers
as data, thereby making LLVM generate x86 code compatible
with execute-only permissions. Figure 6 shows how code
pointers (addr_case1. . . addr_case3) are converted to
direct jumps in an example switch-case statement. We quantify
the impact of this compiler transformation in Section X.
While examining x86 binaries on Linux, we noticed that
contemporary linkers include both the readable ELF header
data and executable code on the Ô¨Årst page of the mapped ELF
Ô¨Åle. Hence, we created a patch for both the BFD and Gold
linkers to start the executable code on a separate page from
the readable ELF headers and to adjust the page permissions
appropriately. This separation allows the ELF loader to map
the headers as readable while mapping the Ô¨Årst code page as
execute-only.
C. Code-Pointer Hiding
Making code non-readable prevents the original JIT-ROP
attack but not indirect JIT-ROP. In the latter attack, an attacker
combines pointer harvesting with partial a priori knowledge of
the code layout, e.g., the layout of individual code pages or
functions (cf. Section III). To thwart indirect JIT-ROP, we hide
code pointers so they are no longer stored in readable memory
pages.
We protect against the sources of indirect code disclosure
identiÔ¨Åed in Section V by adding a level of indirection to code
pointers. The two steps in code-pointer hiding are (i) creating
trampolines for each instruction reachable through an indirect
branch and (ii) replacing all code pointers in readable memory
with trampoline pointers. We use two kinds of trampolines:
jump trampolines and call trampolines, to protect function
addresses and call sites respectively.
We generate a jump trampoline for each function that has
its address taken. Figure 7 shows how we replace a vtable and
function pointer with pointers to jump trampolines. For example,
when a call is made through funcPtr_trampoline, execution is
redirected to the original target of the call: Function_B.
The call trampolines that hide return addresses on the stack
are shown in Figure 8. Normally, a call to Method_A will
push the address of the following instruction (call_site_1) onto
the stack. The Readactor compiler moves the call into a call
trampoline such that the return address that is pushed onto
the stack points to the call trampoline rather than the calling
function. When the callee returns to the call trampoline, a direct
branch transfers execution back to the original caller. Like jump
trampolines, call trampolines cannot be read by attackers and
therefore do not leak information about the function layout.
A Ô¨Ånal source of indirect code leakage is related to C++
exception handling. When an exception occurs, the C++ runtime
library must unwind the stack, which is the process of walking
back up the call chain and destroying locally allocated objects
until an appropriate exception handler is found. Modern C++
768
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:06:24 UTC from IEEE Xplore.  Restrictions apply. 
Legacy Application
Readacted Application
Data page (read-write)
Code page (read-execute)
Data page (read-write)
DiversiÔ¨Åed
Code page (execute-only)
vTable:
vTable pointer
‚Ä¶
C++ object:
Function pointer
‚Ä¶
Method_A:
asm_ins
asm_ins
Function_B:
asm_ins
asm_ins
vTable:
vTable_trampoline
‚Ä¶
C++ object:
funcPtr_trampoline
‚Ä¶
Indirect disclosure possible
Adversary
Data reference to code
Code reference to code
Read access prevented 
by Readactor
Function_B:
asm_ins
asm_ins
Method_A:
asm_ins
asm_ins
Trampolines:
JUMP Method_A
JUMP ‚Ä¶
JUMP Function_B
Figure 7: Hiding code pointers stored in the heap and in C++ vtables. Without Readactor, pointers to functions and methods may
leak (left). With Readactor, only pointers to jump trampolines may leak and the layouts of functions and jump trampolines are
randomized (right).
Legacy Application
Readacted Application
Stack page (read-write)
Code page (read-execute)
Stack page (read-write)
Return_addr_1
CALL Method_A
call_site_1:
Ret_trampoline_1
DiversiÔ¨Åed
Code page (execute-only)
JUMP Function_B_tramp
call_site_2:
Return_addr_2
CALL Function_B
call_site_2:
Ret_trampoline_2
JUMP Method_A_tramp
call_site_1:
Indirect disclosure possible
Adversary
Data reference to code
Code reference to code
Read access prevented 
by Readactor
Trampolines:
CALL Method_A
JUMP call_site_1
CALL Function_B
JUMP call_site_2
Figure 8: Hiding return addresses stored on the machine stack. Without Readactor, each activation frame on the stack leaks the
location of a function (left). With Readactor, calls go through call trampolines so the return addresses pushed on the stack can
only leak trampoline locations ‚Äì not return sites (right).
compilers implement efÔ¨Åcient exception handling by generating
an exception handling (EH) table that informs the unwinding
routine of the stack contents. These data tables are stored in
readable memory during execution and contain the range of
code addresses for each function and the information to unwind
each stack frame. During stack unwinding, the C++ runtime
library locates the exception handling entry for each return
address encountered on the stack. Since our call trampolines
push the address of a trampoline onto the stack rather than the
real return address, the runtime will try to locate the address
of the call trampoline in the exception handling tables. Hence,
we need to replace the real function bounds in the EH table
with the bounds of the trampolines for that function.
Our prototype compiler implementation does not rewrite the
EH table to refer to trampolines; however, doing so is merely a
matter of engineering effort. No aspect of our approach prevents
us from correctly supporting C++ exception handling. We
found that disabling C++ exception handling was not a critical
limitation in practice, since many C++ projects, including
Chromium, choose not to use C++ exceptions for performance
or compatibility reasons.
Handwritten assembly routines are occasionally used to
optimize performance critical program sections where standard
C/C++ code is insufÔ¨Åcient. To prevent this assembly code from
leaking code pointers to the stack, we can rewrite it to use
trampolines at all call sites. Additionally, we can guarantee
that no code pointers are stored into readable memory from
assembly code. Our current implementation does not perform
such analysis and rewriting of handwritten assembly code but
again, doing so would involve no additional research.
While code pointers are hidden from adversaries, trampoline
pointers are stored in readable memory as shown on the right-
hand sides of Figures 7 and 8. Therefore, we must carefully
consider whether leaked trampoline pointers are useful to
adversaries. If the layout of trampolines is correlated with
the function layout, knowing the trampoline pointers informs
769
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:06:24 UTC from IEEE Xplore.  Restrictions apply. 
Readacted 
JIT Compiler
Function
Compilation
Generated 
Code Exec.
Function
Optimization
Generated 
Code Exec.
Garbage Collection
‚Ä¶
Modify Code 
Execute Code
Modify Code
Execute Code
Modify Code
Execute-only
Readable-writable
Figure 9: Timeline of the execution of a JIT-compiled program. Execution switches between the JIT compiler and generated code.
adversaries of the code layout. To ensure there is no correlation
between the layout of trampolines and functions, we permute
the list of trampolines. We also insert dummy entries into
the list of trampolines that consist of privileged instructions.
Hence, any attempts to guess the contents of the trampoline
list by executing them in a row will eventually trigger an
exception [19].
Each trampoline entry contains a direct jump that consists of
an opcode and an immediate operand encoding the destination
of the jump. Because we permute the order of functions,
the value of the immediate operand is randomized as a side
effect. This makes the contents of trampolines unpredictable to
attackers and prevents use of any unintended gadgets contained
in the immediate operands of direct jumps.
An attacker can use trampoline pointers just as they are
used by legitimate code: to execute the target of the trampoline
pointer without knowledge of the target address. Because we
only create trampolines for functions that have their address
taken and for all return sites, our code mechanism restricts
the attacker to reuse only function-entry gadgets and call-
preceded gadgets. Note that CFI-based defenses constrain
attackers similarly and some CFI implementations use trampo-
line mechanisms similar to ours [63, 67, 69]. Coarse-grained
CFI defenses are vulnerable to gadget stitching attacks where
adversaries construct ROP chains out of the types of gadgets
that are reachable via trampoline pointers [20, 29]. Although
gadget stitching attacks against Readactor will be hard to
construct because the required trampoline pointers may be
non-trivial to leak, we still included protection against such
attacks. We observe that gadget chains (including those that
bypass CFI) pass information from one gadget to another via
stack slots and registers. Because we perform register allocation
randomization and callee-saved register save slot reordering,
the attacker cannot predict how data Ô¨Çows through the gadgets
reachable via trampolines.
Modern processors have deep pipelines and fetch instruc-
tions long before they may be needed. On the one hand,
pipelining can hide the cost of the additional indirection
introduced by trampolines. On the other hand, we must ensure
that our use of trampolines to hide code pointers does not
increase the number of costly branch predictor misses that stall
the processor pipeline until the correct instructions are fetched.
Thanks to the use of a dedicated return-address stack in modern
branch predictor units, targets of function returns are rarely
mispredicted as long as function calls and returns are paired. By
preserving this pairing, we ensure that our trampolines do not
introduce additional branch predictor misses. We evaluate the
performance impact of jump and call trampolines in Section X.
VIII. READACTOR ‚Äì JIT COMPILER PROTECTION
Web pages embed JavaScript code that must be executed
by the browser. The most effective way to execute JavaScript
(and other so called dynamic languages) is via JIT compilation,
which all major web browsers support. What sets just-in-time
compilers apart from their ahead-of-time counterparts is that
code generation happens dynamically at runtime. Consequently,
our compile-time techniques described in Section VII will
not protect dynamically generated code. To make our defense
practical and comprehensive, we extended our execute-only
memory to support dynamically compiled code. This section
describes how this was achieved for the V8 JavaScript engine
which is part of the Chromium web browser. We believe that
the method we used generalizes to other JIT compilers.
Modern JavaScript engines are tiered, which means that they
contain several JIT compilers. The V8 engine contains three JIT
compilers: a baseline compiler that produces unoptimized code
quickly and two optimizing compilers called CrankShaft and
TurboFan. Having multiple JIT compilers lets the JavaScript
engine focus the optimization effort on the most frequently
executed code. This matters because the time spent on code
optimization adds to the overall running time.
JIT engines cache the generated code to avoid continually
recompiling the same methods. Frequently executed methods
in the code cache are recompiled and replaced with optimized
versions. When the code cache grows beyond a certain size, it
is garbage collected by removing the least recently executed