### Analysis of Alarms and Error Status

The analysis revealed that 3.5% of the occurrences were alarms, while 19.3% were error statuses. Exceptions were more commonly associated with low-level errors (e.g., segmentation faults) rather than errors affecting system call parameters. In contrast, the error status mechanism is designed to validate the correctness of system calls. Some corrupted parameters altered the frequency of task activations, leading to a higher number of alarms. The impact of the injected faults was relatively lower in this campaign, as indicated by the high percentage of successful experiments (72%).

### Impact of Analysis Strategy on Failure and Error Detection

As discussed in Section 4, the failure percentages and error detection coverage can vary significantly based on the strategy used for analyzing the experiments, particularly those where both failure and error detection events are observed. The results presented above considered the order of these events. However, if the order of events is disregarded, the results can be interpreted differently, giving priority either to error detection or to failure events. This alternative analysis is illustrated in Figure 5.

Figure 5 shows the failure rates for the previous campaigns using three different analysis strategies: 
1. **First Event** (considering the order of events, as in Figure 4),
2. **Priority to Error Detection** (giving priority to error detection events, as in [9, 10]),
3. **Priority to Failure** (giving priority to failure events, as in [11]).

Failure rates include the classes 'Deadline Missed', 'Incorrect Results', 'Application Hang', and 'System Hang'. The complementary percentages correspond to error detection coverage, which includes the classes 'Alarm', 'Error Status', and 'Exception'. The 'Correct' class is not represented.

The **First Event** and **Priority to Error Detection** strategies provide similar results because, in most cases, error detection occurred before a failure. Specifically, error detection preceded a failure 98.3% of the time in the mSCH campaign, 98.4% in the mTIM campaign, and 95.3% in the pTIM campaign. Conversely, when using the **Priority to Failure** strategy, the failure rates increase significantly, as many experiments where a failure was preceded by an error detection are now counted as failures. The total percentage of such experiments was 80.2% in the mSCH campaign, 87% in the mTIM campaign, and 24.3% in the pTIM campaign. Generally, the pTIM campaign is less affected by the analysis strategy due to the fewer instances of both failure and error detection events (only 25.5% of the total).

### Impact on Real-Time Properties

This section presents an example of how MAFALDA-RT analyzes the timing properties of a real-time system in the presence of faults. We compare the experimentally observed response times of tasks in each campaign with the analytically predicted values from Table 1 (see also [13]).

Figure 6 illustrates this comparison. Note that we only represent response times when a measurement could be made, i.e., when tasks were not hung. Figure 6a shows the maximum response times observed before fault activation, while Figure 6b shows those observed after fault activation. The "Before" interval spans from the start of the experiment to the fault activation, and the "After" interval spans from the fault activation to the end of the experiment. The "Predicted" curve represents the analytically predicted worst-case response times from Table 1. Curves P6&+, P7,0, and S7,0 represent the maximum observed response times in the mSCH, mTIM, and pTIM campaigns, respectively.

In the absence of faults (Figure 6a), all observed response times were within their predicted worst-case values, regardless of the fault injection campaign. The curves of the observed values follow the same trend as the predicted values, indicating that, before fault activation, the type of campaign has no significant effect on response times and confirms the accuracy of the analytically predicted results.

However, the presence of faults significantly impairs the system's timing predictability. After fault activation (Figure 6b), some tasks exceeded their predicted worst-case response times in certain campaigns. Specifically, the maximum response time of periodic tasks (i.e., CH4 Sensor, CO Sensor, Air-Flow Sensor, and Water-Flow Sensor) was highly sensitive to faults affecting the timers component of the microkernel (P7,0 curve). Timers are used to compute the release times of periodic tasks, and errors in this component can alter task frequencies, resulting in increased response times. The sporadic task HLW Handler was not affected because it is activated by an external interrupt, not a timer. Additionally, the CO Sensor task experienced a very high response time in the pTIM campaign due to an incorrect task arrival, causing it to be released beyond its period interval. The root cause was a corrupted parameter in the system call responsible for releasing periodic tasks at the beginning of their periods. When faults were injected into the scheduling component, the response times were not affected, as most measured response times were from experiments in the 'Correct' class. Very few response times could be extracted from other classes because the application often hung immediately after fault activation, as explained in Section 5.2.

It is worth noting that MAFALDA-RT observed other types of abnormal overheads, not only in the application (e.g., affecting computation times) but also in the microkernel (e.g., affecting context switches and system calls).

### Summary of Benefits

The main benefits of the timing property analysis conducted by MAFALDA-RT are:
- Validation of worst-case scheduling analyses in practice, whether or not they involve dependability requirements.
- Identification of the primary sources of overhead in tasks or the software executive in the presence of faults.
- Complementing worst-case measurements obtained through other techniques, such as static code analysis.

### Conclusion

MAFALDA-RT provides a comprehensive environment for characterizing both the performance and dependability features of real-time systems based on COTS microkernels, suitable for a wide range of application domains. It supports features for eliminating temporal intrusiveness caused by SWIFI instrumentation (both fault injection and observation) and for analyzing a comprehensive set of measurements (including timing aspects) at both the kernel and application task levels.

The demonstration illustrated the development of fault injection campaigns and the provided facilities for analyzing results. MAFALDA-RT is being extended with wrapping capabilities to facilitate the integration of COTS microkernels into real-time systems. These wrappers aim to guarantee timing and functional properties through error confinement and recovery mechanisms based on a formal description of the system [14].

### Acknowledgments

The work reported in this paper was partially carried out in the framework of LIS1 and is currently partially financed by the DSOS project (IST-1999-11585). Manuel Rodríguez was supported in part by THALES. The authors would like to thank Jean-Charles Fabre for his inspiring role and suggestions.

### References

[1] H. Madeira, D. Costa, and M. Vieira, “On the Emulation of Software Faults by Software Fault Injection,” in Proc. DSN 2000, New York, NY (USA), pp. 417-426, 2000.

[2] G. A. Kanawati, N. A. Kanawati, and J. A. Abraham, “FERRARI: A Tool for the Validation of System Dependability Properties,” in Proc. FTCS-22, Boston, MA (USA), pp. 336-344, 1992.

[3] J. Carreira, H. Madeira, and J. G. Silva, “Xception: A Technique for the Experimental Evaluation of Dependability in Modern Computers,” IEEE Transactions on Software Engineering, vol. 24, no. 2, pp. 125-136, 1998.

[4] J. Arlat, J.-C. Fabre, M. Rodríguez, and F. Salles, “Dependability of COTS Microkernel-Based Systems,” IEEE Transactions on Computers, vol. 51, no. 2, pp. 138-163, 2002.

[5] J. L. Aidemark, J. P. Vinter, P. Folkesson, and J. Karlsson, “GOOFI - A Generic Fault Injection Tool,” in Proc. DSN 2001, Göteborg, Sweden, pp. 83-88, 2001.

[6] PostgreSQL, http://www.postgresql.org/.

[7] S. Dawson, F. Jahanian, T. Mitton, and T. Tung, “Testing of Fault-Tolerant and Real-Time Distributed Systems via Protocol Fault Injection,” in Proc. FTCS-26, Sendai, Japan, pp. 404-414, 1996.

[8] J. C. Cunha, M. Z. Rela, and J. G. Silva, “Can Software Implemented Fault-Injection be Used on Real-Time Systems?,” in Proc. 3rd European Dependable Computing Conference (EDCC-3), Prague, Czech Republic, pp. 209-226, 1999.

[9] P. Chevochot and I. Puaut, “Experimental Evaluation of the Fail-Silent Behavior of a Distributed Real-Time Run-Time Support built from COTS Components,” in Proc. DSN 2001, Göteborg, Sweden, 2001.

[10] A. Steininger and C. Scherrer, “Identifying Efficient Combinations of Error Detecting Mechanisms Based on Results of Fault Injection Experiments,” IEEE Transactions on Computers, vol. 51, no. 2, pp. 235-239, 2002.

[11] E. Marsden and J.-C. Fabre, “Failure Mode Analysis of CORBA Service Implementations,” in Proc. IFIP/ACM International Conference on Distributed Systems Platforms (Middleware 2001), Heidelberg, Germany, 2001.

[12] Chorus Systems, “CHORUS/ClassiX release 3 - Technical Overview,” Technical Report no. CS/TR-96-119.12, Chorus Systems, 1997 (www.sun.com/chorusos).

[13] A. Burns and A. J. Wellings, Real-time Systems and their Programming Languages, Addison Wesley, 1997.

[14] M. Rodríguez, J.-C. Fabre, and J. Arlat, “Formal Specification for Building Robust Real-time Microkernels,” in Proc. 21st IEEE Real-Time Systems Symposium (RTSS 2000), Orlando, Florida (USA), pp. 119-128, 2000.

1. Located at LAAS, the Laboratory for Dependability Engineering (LIS) was a Cooperative Laboratory between five industrial companies (Airbus France, Astrium, Électricité de France, Technicatome, THALES) and LAAS-CNRS.