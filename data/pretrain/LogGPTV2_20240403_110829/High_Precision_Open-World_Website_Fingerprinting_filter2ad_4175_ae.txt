TPR
0.54
0.60
0.87
0.9
Original
FPR contribution
0.9%
0.7%
0.6%
1.4%
TPR
0.11
0.15
0.02
0.06
With PO
FPR contribution
0%
0%
0%
0%
Active client. we want to test the precision of an attacker
against an active client who is generating random events
through page browsing, rather than a static client who visits a
page and does not act on it. We chose two sites for this task; for
the ﬁrst site “AJAX”, the active client randomly scrolls down
the web page, causing more content to be loaded because of
its AJAX code; in the other site “LINKS”, the active client
randomly clicks on links to browse topics. For each site, we
generated two classes, corresponding to a fast client (AJAX1,
LINKS1) and a slow client (AJAX2, LINKS2). Exact details
are in the Appendix.
We replaced 4 of the original 100 classes with these new
classes and asked Ha-kFP to classify the new problem
precisely. (The data set size is the same at 100x200+80000).
We present the results in Table VIII. We show TPR for the
four chosen classes before and after optimization, as well as
what portion of the false positives they each contributed to.
Since there are 100 classes, each class on average contributes
to 1% of the false positives.
We found that AJAX1 and AJAX2 were especially dif-
ﬁcult to classify, as their true positive rate was low even
in the original case. This was probably due to the highly
random network activity. None of these four classes were
especially responsible for false positives in the modiﬁed data
set. After optimization, the true positive rate of each class
was reduced below the overall mean true positive rate (0.38),
suggesting that
the four classes were somewhat harder to
classify correctly. Overall, however, the classiﬁer still achieved
π20 > 0.99 and π1000 > 0.85, about as good as the original
data set. The four new classes were difﬁcult to classify, but
they did not end up hurting precision.
V. DISCUSSION
A. Does Website Fingerprinting scale?
We have shown that optimized classiﬁers can be highly
precise in our large open world. Yet, the reader may comment
that our experimental open world is tiny compared to the actual
open world (with over a billion pages). Can these results be
extended to the actual open world?
Since any experiment’s size is invariably limited when
compared to the space of all potential inputs, what is crucial is
to ensure that the experiment procedure correctly simulates a
realistic attack scenario. Our open-world experiment is correct
because we are pessimistically simulating an attacker who has
Fig. 6: 20-precision for Wa-kNN on a data set of 100x200+N
elements using 10-fold cross-validation, where N is the size
of the open-world class ranging from 100 to 80,000.
no knowledge whatsoever of the testing open world; at most,
he is only allowed to train on his own toy open world, which
does not intersect with the client’s testing open world. The
difference in size between the training and testing worlds does
not affect the correctness of this procedure. For this reason,
we only need to ensure that the classiﬁer should never be
tested on the same non-sensitive pages it has trained on.
To achieve this experimentally, we visited each non-
sensitive page only once, and split the training and testing set
(generally using 10-fold cross validation, though the training
set was smaller for some attacks). We also ﬁltered out pages
with similar domain names so that they would not both appear
in our non-monitored set. Since the classiﬁer has no knowledge
whatsoever of the non-sensitive pages in the testing set, its
decision that a testing element should be classiﬁed as non-
sensitive is not based on any speciﬁc knowledge about the
page it came from. In other words, the actual open world —
and its larger size — has no impact on the classiﬁer’s success.
Some previous work increased the size of the open world
and found that accuracy and precision decreased [19], [20]. We
argue that this was only the case because they did not keep r
constant and implicitly increased r when increasing the open
world size. To show this, we also increased the size of the
open world, but we measured π20 (20-precision), holding the
base rate of visiting non-sensitive pages constant. We evaluated
optimized Wa-kNN on a data set of 100x200+N elements,
where N, the open world size, varied from 100 to 80,000,
with 10-fold cross-validation. We show the results in Figure 6,
which shows that a larger open world set size will increase
π20 from 0.28 to 0.93, as may be expected of a competent
classiﬁer. Other classiﬁers show similar results, though we
speciﬁcally picked Wa-kNN as it does not have to wrestle
with class size imbalance.
B. What is the value of r?
The value of r parametrically captures an aspect of client
behavior: the ratio between the probability a client would visit
a non-sensitive page compared to that of a sensitive page. We
never assume that the attacker knows or needs to estimate the
value of r (his strategy is the same no matter what the client
does). For simplicity of presentation we decided to focus on
two scenarios, r = 20 and r = 1000. Here we justify the
 0 0.2 0.4 0.6 0.8 1 0 10000 20000 30000 40000 50000 60000 70000 80000π20Nchoice of these parameters by presenting data from real web
browsers.
We wrote a small executable to read browser history ﬁles
(Firefox and Chrome) and distributed it to volunteering ac-
quaintances. We created two lists of 100 web pages each,
respectively the top 100 web pages, and 100 sensitive web
pages that are banned in certain countries including the U.K.,
India, Australia, New Zealand, U.S., and Russia.5 The latter
set mostly consists of ﬁle sharing sites and streaming services,
and we selected the 100 most popular ones (by Alexa rank)
amongst those. All clients were fully informed of the above
and how the data would be presented, and approval was
obtained from the relevant institutional ethics review boards.
We included a hash as a rudimentary way to stop participants
from editing their data.
Since Tor Browser keeps no history and no logins between
sessions, users generally have to visit the home page of a web
site (sometimes to log in) before visiting any of its other web
pages. This is not the case in normal browsing, so we took this
into consideration to avoid under-counting the number of page
visits (thus overestimating r). We counted the number of page
visits in two different ways. First, the “Site” method counts
all page visits corresponding to that site, not just the speciﬁc
page. This likely over-counts the number of actual visits to
these pages. Second, the “Session” method also counts all page
visits to that site, but only once in a given session. We deﬁne a
session to be any continuous series of page visits with less than
5 minutes of inactivity between two consecutive visits. For
example, if a user visits ten proﬁle pages on facebook.com
in one session without going to the home page (because they
are already logged in), this counts as ten visits in the “Site”
method and one visit in the “Session” method. The latter
more realistically represents private browsing and Tor Browser.
Exact page visits are counted in any case. While it can be
argued that r extracted from normal browsing would differ
from r extracted from Tor Browser users, we have no way of
obtaining the latter and the former nevertheless represents real
browsing activity.
We show values of r in Table IX. We see that r values
for the top sites generally varied from 10 to 30, while
the sensitive values varied more signiﬁcantly from several
hundred to several thousand. If we treat the “Session” value as
more realistic, then monitoring these particular sensitive pages
would probably require the attacker to succeed at r = 1000 to
r = 2000. The value of 54338 for one data set is questionable
because it corresponds to a single recorded visit of a sensitive
page; a few more visits would signiﬁcantly decrease the value.
For presentation, we chose to ﬁx r to two speciﬁc values
earlier, r = 20 and r = 1000 to represent popular pages
and sensitive pages respectively. Here, we want to vary r to
examine its effects on r-precision. Since our classiﬁers are
not dependent on r, we take the TPR, WPR, and FPR of the
best previous result, Ha-kFP using conﬁdence-based PO, and
5We intentionally did not include any pages banned in China as it does not
represent what our participants would consider sensitive.
TABLE IX: Values of r for several participants, based on two
data sets and two methods of counting r, “Site” and “Session”.
# of pages
127095
54338
116566
13161
Site
6
10
5
20
Top pages
Session
15
33
14
20
Sensitive pages
Session
Site
281
1896
54338
2089
485
2534
274
258
Fig. 7: Lower bound for πr by the Wilson method when we
vary r from 1 to 10000. Note that the x-axis is logarithmic.
re-calculate the conservative lower bound of r-precision using
the Wilson method while varying r. We show these results in
Figure 7.
First, there is only a very slight decrease of r-precision
from r = 20 (π20 > 0.96) to r = 100 (π100 > 0.95). r = 100
represents a moderately difﬁcult scenario, since the base rate
we measure is the sum of 100 pages. For example, if 10% of
the population regularly visit a page, and if they visit 30 pages
per day, the pages they visit once per month would match the
rate of the sensitive pages examined by the r = 100 scenario.
Our success with the r = 20 scenario can be extended to the
r = 100 scenario. Second, there is a more signiﬁcant drop
in r-precision at r = 10000. This is in fact an experimental
limitation, as the value is obtained by the conservative Wilson
method. At r = 2000, we still achieve π2000 > 0.78 with our
current methods and data set.
C. Is Website Fingerprinting realistic?
Generally, WF works make several assumptions to present
results. These assumptions include the use of a cold cache,
freshness of the training set and the lack of noise that could
impede classiﬁcation. Multiple works [11], [31] have noted
that violating these assumptions would cause recall to drop.
We note that assuming the use of a cold cache is reasonable
for Tor (as it does not keep cache in disk), and Wang et al. [31]
have shown that the training set can be kept sufﬁciently fresh
for classiﬁcation. Here, we discuss issues related to the last
assumption, the lack of noise.
In machine learning, noise could refer to several different
aspects of data. First, there may be label noise — mislabeled
elements in the training set. This is irrelevant to our WF
attack model as the attacker visits speciﬁc pages himself to
collect training data. Second, perturbations to the data may
be introduced by differences between the experimental setting
 0 0.2 0.4 0.6 0.8 1 1 10 100 1000 10000πrrnot precise considering the realistically low base rates at
which people normally visit sensitive web pages. This implied
that WF would only succeed in identifying highly popular
web pages. We formulate r-precision (πr), the percentage of
sensitive classiﬁcations that are correct if the client visits r
times as many non-sensitive pages as sensitive pages. We
address a confusion between experimental data set sizes and
real world data set sizes sometimes found in previous work,
and show the importance of distinguishing between wrong
positives and false positives in calculating precision.
As no previous attack was precise under r = 1000, we
present three classes of POs to improve the precision of WF
classiﬁers. Our conﬁdence-based POs ask classiﬁers to output
the degree of conﬁdence they have in their classiﬁcations, and
we reject classiﬁcations that are not conﬁdent enough. Our
distance-based POs reject classiﬁcations of testing elements
that are too far from the assumed class. Our ensemble-based
POs reject elements for which a chosen set of classiﬁers
did not unanimously agree on the assigned class. While
conﬁdence-based Ha-KFP performed the best at π1000 >
0.86, distance-based POs were able to allow almost any attack
to achieve high π20 precision, and ensemble-based POs were
nearly as precise as conﬁdence-based Ha-kFP with the added
beneﬁt of requiring no parametrization.
Previous authors have shown that a number of problems in
realistic scenarios would lower recall, such as noisy packet
sequences, poor training sets and multi-tab browsing; solu-
tions have been proposed in previous works. We evaluated
one particular scenario involving active users with random
behavior to show that we can still achieve high precision, but
we do not know if other realistic problems and their solutions
would affect precision. One way to deﬁnitely demonstrate the
practicality of OWF would be to create a private Tor guard
that performs website ﬁngerprinting on consenting Tor clients,
telling them which sensitive web pages they visited and asking
them if it is correct.
On the ﬂip side, defenses should also be designed to
minimize the attacker’s precision. One approach to designing
defenses is to create “anonymity sets” of web pages that look
the same to the attacker after padding. This approach provides
an upper bound on the maximum recall of any attacker, but not
precision. We should consider re-designing such defenses to
provide an upper bound on the maximum r-precision instead.
Our data and code can be found at:
https://github.com/OpenWF/openwf.git
and the real client setting, such as network conditions and
browser settings. Much of these differences are ameliorated
when experimenting on Tor, where network conditions are
already random and the browser cannot be easily conﬁgured
differently. The browser also does not keep cookies across
sessions, including login cookies; for example, a user must
login to a social media page every time she starts the browser
again, thus going through the easily-ﬁngerprinted front page.
Third, user actions may cause data to load in a different
way compared to the experimental client (which is static
and does not act beyond loading a page). For example, the
user may be listening to music in another tab, which causes
network activity. Wang et al. [31] showed that classiﬁcation
is only impeded when the bandwidth rate of noise is very
high, possibly from a video or a ﬁle download. Our work
addresses several types of active users to ﬁnd that they will not
impede precision, though we cannot claim to have a complete
investigation of all types of user activity.
VI. RELATED WORK
As the presentation of our results has already included
descriptions of much previous work, we offer only a brief
overview of related work in this section focusing on WF
attacks. Cheng et al. [5], Sun et al. [26], Hintz et al. [9], and
Bissias et al. [1] were some of the ﬁrst to show successful
classiﬁers to determine which page someone is visiting based
on trafﬁc patterns. Later works referred to this trafﬁc analysis
problem as website ﬁngerprinting.
The original paper on Tor considered trafﬁc analysis to be a
serious threat [6], though no attack had been successful on Tor