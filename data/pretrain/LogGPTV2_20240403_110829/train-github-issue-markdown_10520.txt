# Checklist
  * I have checked the issues list  
for similar or identical feature requests.
  * I have checked the pull requests list  
for existing proposed implementations of this feature.
  * I have checked the commit log  
to find out if the if the same feature was already implemented in the  
master branch.
  * I have included all related issues and possible duplicate issues  
in this issue (If there are none, check this box anyway).
## Related Issues and Possible Duplicates
#### Related Issues
  * #5830
  * https://groups.google.com/g/celery-users/c/IGOBnp4n0gU
#### Possible Duplicates
  * None
# Brief Summary
Currently, the only way to use `joblib` and `loky` (in some extent
`multiprocessing` too) is to use `-P threads` instead of `-P processes`.
Since `-P threads` is using `ThreadPoolExecutor` from the stdlib under the
hood, the task in the same worker is not really running in parallel (but
async). This is because of the Python GIL.
The problem becomes even more important if the workload is split between the
main code and the subprocesses (executed by `joblib`).
Only the subprocesses are executed in parallel but not the main code. It adds
a clear bottleneck that is not ideal.
# Architectural Considerations
I am not comfortable enough with Celery to propose an implementation. Feel
free to throw your ideas below in this ticket.
# Potential workaround
A potential workaround is to have plenty of Celery worker replicas that
execute only one task (concurrency of 1). So the task will have all the CPU
time available for it and be free to use `joblib` at its convenience.
But having all the Celery workers with a concurrency of 1 also imply some
overhead in the underlying infrastructure that is not really wanted ideally.