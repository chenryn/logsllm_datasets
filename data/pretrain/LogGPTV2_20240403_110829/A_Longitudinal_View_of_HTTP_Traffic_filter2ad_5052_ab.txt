Figure 7 shows the distribution of requests per server hostname. As discussed in § 5 all
requests to a server name such as “www.cnn.com” are grouped together in this plot no
matter what IP address handles the request. We see that across the years in our study the
distribution of requests per hostname is similar. Many server hostnames are accessed
infrequently—with the median being less than 10 requests per year. However, a small
number of server hostnames are quite popular—with the maximum number of requests
per hostname exceeding one million per year. We also observe that there is change in
the top hostnames over time. We determined the top ten hostnames per year and ﬁnd
that four hostnames are within the top ten for all four years of our survey, one hostname
is in the top ten in three of the years, two hosts appear in the list for two years and
17 hostnames appear only once across the four years.
Next we turn to object popularity. Figure 8 shows the distribution of the number of
requests per object. The distributions are similar across the years. As seen in the plot,
request popularity ﬁts exceedingly well to a Zipf-like distribution. Across more than
3 orders of magnitude, the fall-off in the distribution matches closely to a Pareto dis-
tribution with α = 1. Around 90% of the objects are accessed only one time.2 Further,
only a small number of objects are fetched more than 10 times. That said, there are
some popular objects that are requested thousands of times over the course of a year.
Next, ﬁgure 9 shows the distribution of the unique number of objects (determined by
URL) per hostname over time. We again observe stability across the years of our study.
This plot shows that one-third of the hostnames we encounter (regardless of year) serve
only a single unique object. Further, two-thirds of the hostnames serve ten or fewer
objects. Similar to many other aspects of web trafﬁc a small number of hostnames
serve many web objects. For instance, roughly 5% of the hostnames provide more than
100 objects and hostnames top out at providing over one million objects.
Object popularity has a direct bearing on the usefulness of caching. We use our data
to investigate both visible end-host caching and also potential savings from a network-
based cache with our results illustrated in ﬁgure 10. First, we look at the number of
2 As unique sets of parameters at the end of a URL generally yield distinct outputs, we consider
the entire URL, including parameters, as a distinct object.
A Longitudinal View of HTTP Trafﬁc
227
2006
2007
2008
2009
100GB
s
e
t
y
B
10GB
1GB
1
10
100
10K 100K
1M
1K
Objects
100MB
2006
Total Bytes/month from GET Responses
Total Cacheable Bytes/month (GET)
Total Bytes Saved from HTTP 304s (GET)
2007
2008
2009
Year
F
D
C
C
 1
 0.1
 0.01
 0.001
 0.0001
 1e-05
Fig. 9. Unique objects per hostname
Fig. 10. Cond. GET and caching savings
bytes visibly saved by end-host caches using conditional GET requests. These requests
call for a client re-requesting an object in the browser cache to include the timestamp of
this cached object in the GET request. If the object has been updated the server will re-
send it and otherwise will send a 304 (“not modiﬁed”) response to the client, signaling
that the version the client has is correct. As shown in the ﬁgure the number of bytes
fetched would increase by roughly 10% without these conditional GET requests. The
use of conditional GETs across the dataset increases in roughly the same manner as the
overall number of bytes downloaded.
We next investigate the number of additional bytes that could beneﬁt from caching
at the institute-level—i.e., with some shared proxy cache at the border shared by all
users. We ﬁrst identify unique objects using the URL (including hostname) and object
size. Ideally we would include a stronger notion such as a hash of the content itself, but
our logs do not contain such information. We crunched 5.75 days worth of full-payload
packet traces from January 2010 to assess the accuracy of our heuristic. We form tuples
of URLs and sizes, (u, s), from the packet traces. In addition, for each we compute an
MD5 hash of the object. For each (u, s) we record the number of MD5s observed. We
ﬁnd that 1.4% of the 846K (u, s) pairs in the trace have multiple MD5s. Further, we
ﬁnd this represents 6.7% of the bytes fetched over the course of the trace. Therefore,
while we assume that if the object size stays the same the object has not changed this
is wrong in a small number of cases. In addition, we calculate the amount of cachable
data for each month in isolation and without any notion of timing out the objects or
imposing a limit on the number or size of objects held in the cache. Therefore, our
results are an upper bound on how an actual institute-wide cache would work with real-
world constraints and a better understanding of the uniqueness of objects. As shown
in ﬁgure 10 the percentage of bytes that could be cached by a network-wide proxy
is 10–20% across most of our study. The number of cachable bytes does increase in
2009—when generally more than 25% of the GET requests could plausibly be handled
by a cache. In 2009 to get the caching beneﬁt suggested in the plot would require over
10 GB of storage space—which is quite modest in modern servers. We note that our
caching results may be quite different if the population of users was larger.
228
T. Callahan, M. Allman, and V. Paxson
5 Server Structure
Web trafﬁc is composed of a series of HTTP transactions that in turn utilize TCP for
reliable data transfer. HTTP uses one transaction per web object. Early HTTP used a
separate TCP connection for each HTTP transaction, however with persistent HTTP
connections an arbitrary number of HTTP transactions can be conducted over a single
TCP connection. Figure 11 shows the total number of TCP connections we observe for
the sampled week of each month of our dataset, as well as the average number of HTTP
transactions per TCP connection. (Note, since these results depend on a solid under-
standing of connections, we do not include data before November 2006 as discussed
in § 2.) The plot shows a fairly stable number of connections and average transactions
per connection rate except for one impulse between November and December 2007. At
that point we observe the re-use of TCP connections dropped by an order of magnitude.
This results in a dramatic increase in the number of web connections observed. Note,
ﬁgure 1 shows that the overall number of HTTP requests does not differ greatly across
this event. That is, the same amount of content is being transferred, but the particulars
of the underlying delivery has changed.
We believe this change in delivery pattern is due to a software change on the web
clients. To verify this we determined the top 100 servers in each month by the overall
number of requests (regardless of number of connections). For each server we then
calculate the average number of transactions per connection. We ﬁnd 66 servers to be
common across the top 100 lists from the two months. We then calculate the difference
in the average number of requests per connection for each of these 66 servers and ﬁnd
that in all but one instance the average drops in December. And, in over 70% of the
cases the average requests per connection drops by at least 10 requests. This indicates
that the use of persistent connections has dropped across the board and is not caused
by some popular server curtailing support for persistent connections or some heavy-
hitter client. We therefore conclude that this is a client policy change that is quite likely
caused by a institute-wide web browser upgrade.3
We next turn our attention to how web sites are structured to serve content. Figure 12
shows the distribution of the number of hostnames each server IP address takes on over
the course of each year in our study. CDN hosts can accommodate a wide range of
logical hostnames using a server with a single IP address. The distributions are similar
across the years with around 80% of the server IP addresses we encounter mapping to a
single server hostname. While roughly 10% of the server IPs map to two hostnames we
observe a small percentage (≈5%) of server IPs accommodating three or more IP ad-
dresses. Further, there are a handful of IP addresses that serve trafﬁc for a large number
of hostnames. Our data shows that the maximum number of hostnames observed for a
single IP address is 477, 878, 1784 and 1353 for the years 2006–2009 respectively. This
shows a deﬁnite increase over time. (Note, since the data only covers half of 2009 the
number may well increase when the entire year is considered.)
3 The ICSI system administrators report a minor version upgrade of Firefox (from 2.0.0.8 to
2.0.0.10) during this timeframe. While we ﬁnd nothing in the Firefox change-log that indicates
a difference in the use of persistent HTTP connections we believe the defaults more than likely
changed in 2.0.0.10 given the observed behavior so dramatically changes at the time of the
upgrade.
A Longitudinal View of HTTP Trafﬁc
229
10M
1M
100K
10K
1K
100
10
1
Total Number of Connections
Average Requests per Connection
F
D
C
C
2007
2008
Year
2009
 1
 0.1
 0.01
 0.001
 0.0001
 1e-05
1
2006
2007
2008
2009
10
100
1K
Hostnames
Fig. 11. Conns. / week and requests / conn
Fig. 12. Unique hostnames per IP
In addition to using one IP address to serve content from multiple server hostnames
CDNs also use multiple IP addresses to serve content from a common server hostname.
Generally this is done to transmit content from a server that is close to the requesting
client and/or for load balancing purposes. Figure 13 shows the distribution of the num-
ber of IP addresses used for each hostname observed for each year in our dataset. The
ﬁgure shows that 80–90% of the hostnames are served by one IP address. Another 5–
10% of the server hostnames are handled by two IP addresses. Approximately 5% of the
server hostnames are associated with three or more IP addresses over the course of the
year. While relatively rare we ﬁnd many hostnames that have dozens to hundreds of IP
addresses over the course of a year. In particular, we note that we observe a maximum
of 144, 183, 340 and 388 IP addresses for a single hostname in 2006–2009 respectively.
This shows an increase in the number of hosts brought to bear to deliver content over
the course of our study for some hostnames. While this trend is not general across all
hostnames we believe it may indicate larger and more dynamic CDN behavior.
We next wanted to assess the degree to which content providers are relying on the
content delivery networks (CDNs) to deliver their data. In this initial exploration we
focus on the Akamai CDN but intend to broaden our consideration as part of our fu-
ture work. A colleague provided us with a list of partial Akamai hostnames manually
gathered for another project [15]. The partial hostnames in this set were determined
from downloading known Akamai-based web pages from 300-400 locations around the
world. The hostnames represent 12K Akamai IP addresses [15] which represents an
undercount of Akamai’s footprint (e.g., Akamai is cited as having 40K servers in [13]).
Therefore, our accounting of Akamai trafﬁc is highly likely to be an underestimate. We
correlate the Akamai hostnames and the DNS logs produced in conjunction with the
web logs by Bro. For each web log we use the corresponding DNS log to ﬁnd resolu-
tions for the Akamai names and record the associated IP addresses. We can then easily
assign web trafﬁc as Akamai trafﬁc or non-Akamai trafﬁc.
Figure 14 shows the percentage of the bytes fetched in response to GET requests
that are handled by Akamai servers. Across the time period of our study we ﬁnd that
in general 15–30% of the bytes are delivered by Akamai.4 However, recall that this is
4 While we see a dip to 3% at the end of 2006, this is caused by the trafﬁc spike at the same
time—as shown in ﬁgure 3—which represents trafﬁc caused by a single client to a non-Akamai
server. This client excluded, 13% of the trafﬁc involves Akamai.
230
T. Callahan, M. Allman, and V. Paxson
F
D
C
C
 1
 0.1
 0.01
 0.001
 0.0001
 1e-05
1
2006
2007
2008
2009
10
Unique IPs
100
)
%
(
i
a
m
a
k
A
m
o
r
f
s
e
t
y
b
T
E
G
50
45
40
35
30
25
20
15
10
5
0
2006
2007
2008
Year
2009
Fig. 13. Unique IPs per hostname
Fig. 14. Trafﬁc using Akamai CDN servers
a lower bound due to our classiﬁcation methodology. Also note that over the years of
our study ICSI users accessed Akamai served content from over 9K distinct Akamai
servers. The number of Akamai servers observed in a each year in our dataset is: 2.5K,
3.4K, 4.5K and 3K for 2006–2009 respectively. (Note, the 2009 count is for only half
the year and so may well increase if the entire year is considered.)
6 Related Work
The large body of literature dedicated to empirical evaluations of web trafﬁc is too vast
to catalog in the space available here. A good overview, including pointers to much of
the literature, appears in [9]. The topics that have been studied are diverse and numer-
ous, with some examples being: characterization and modeling work [5,3], performance
analysis [6], analysis of web applications [17], analysis of web technologies [14], as-
sessments of web caching [16,4] and studies of the HTTP protocol itself [11,8]. Our
work is quite similar, but serves to add additional longitudinal data to the community’s
body of work.
7 Summary
In this paper we have employed a three and a half year longitudinal dataset of web
activity to assess web operations from numerous angles. This study represents both a
reappraisal of previous work and a broadening of the viewpoint in the temporal dimen-
sion. We ﬁnd that some aspects of web trafﬁc have been fairly static over time (e.g.,
distribution of transaction types) while others have changed (e.g., average size of GET
and POST transactions). We also develop a view of the structure of the web, including
an initial understanding of the behavior of browser caches and the impact of content
distribution networks, which we ﬁnd to be more prominent as time progresses. While
there are obviously more aspects of web operations to assess than could be ﬁt in this
initial paper, we believe our contribution will be useful in grounding the community’s
mental models and experiments in long-term empirical observation.
A Longitudinal View of HTTP Trafﬁc
231
References
1. Al-Qudah, Z., Rabinovich, M., Allman, M.: Web Timeouts and Their Implications. In: Pas-
sive and Active Measurement Conference (April 2010)
2. Allman, M., Avrachenkov, K., Ayesta, U., Blanton, J., Hurtig, P.: Early Retransmit for
TCP and SCTP, Internet-Draft draft-ietf-tcpm-early-rexmt-01.txt (work in progress) (January
2009)
3. Arlitt, M., Williamson, C.: Internet Web Servers: Workload Characterization and Implica-
tions. IEEE/ACM Transactions on Networking (October 1997)
4. Barford, P., Bestavros, A., Bradley, A., Crovella, M.: Changes in Web Client Access Patterns:
Characteristics and Caching Implications. In: Proc. WWW, vol. 2 (1999)
5. Barford, P., Crovella, M.: Generating Representative Web Workloads for Network and Server
Performance Evaluation. In: ACM SIGMETRICS, July 1998, pp. 151–160 (1998)
6. Barford, P., Crovella, M.: Measuring Web Performance in the Wide Area. In: Performance
Evaluation Review: Special Issue on Network Trafﬁc Measurement and Workload Charac-
terization (August 1999)
7. Fielding, R., Gettys, J., Mogul, J.C., Frystyk, H., Berners-Lee, T.: Hypertext Transfer Proto-
col – HTTP/1.1. RFC 2068 (January 1997)
8. Krishnamurthy, B., Arlitt, M.: PRO-COW: Protocol compliance on the web: A longitudinal
study. In: USENIX Symp. on Internet Technologies and Sys. (2001)
9. Krishnamurthy, B., Rexford, J.: Web Protocols and Practice. Addison-Wesley, Reading
(2001)
10. Mondal, A., Kuzmanovic, A.: Removing Exponential Backoff from TCP. ACM Computer
Communication Review 38(5) (October 2008)
11. Nielsen, H., Gettys, J., Baird-Smith, A., Prud’hommeaux, E., Lie, H., Lilley, C.: Network
Performance Effects of HTTP/1.1, CSS1, and PNG. In: ACM SIGCOMM (September 1997)
12. Paxson, V.: Bro: A System for Detecting Network Intruders in Real-Time. Computer Net-
works 31(23–24) (1999)
13. Qureshi, A., Weber, R., Balakrishnan, H., Guttag, J., Maggs, B.: Cutting the Electric Bill for
Internet-Scale Systems. In: ACM SIGCOMM (August 2009)
14. Schneider, F., Agarwal, S., Alpcan, T., Feldmann, A.: The New Web: Characterizing AJAX
Trafﬁc. In: Passive and Active Measurement Conf. (2008)
15. Triukose, S., Wen, Z., Rabinovich, M.: Content Delivery Networks: How Big is Big Enough?
In: ACM SIGMETRICS poster (2009)
16. Wills, C., Mikhailov, M.: Studying the impact of more complete server information on Web
caching. In: Proc. of the 5th Intl. Web Caching and Content Delivery Workshop (2000)
17. Zink, M., Suh, K., Gu, Y., Kurose, J.: Watch Global, Cache Local: YouTube Network Traces
at a Campus Network - Measurements and Implications. In: Proc. Fifteenth Annual Multi-
media Computing and Networking, ACMMCN (2008)