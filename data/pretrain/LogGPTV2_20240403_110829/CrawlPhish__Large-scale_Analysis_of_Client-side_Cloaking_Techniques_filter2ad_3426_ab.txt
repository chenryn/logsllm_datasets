Cloaking type
Requirement
User Interaction
Fingerprinting
Bot Behavior
Pop-up
Click on alert/notiﬁcation window
Mouse Detection Move mouse over browser
Click Through
Cookie
Referrer
User-Agent
Timing
Randomization
Pass Click Through on browser
Check document.cookie
Check document.referrer
Check navigator.userAgent
Render webpage after certain time
using sleep()/Date.getTime()
Show content randomly using
Math.random()
Fig. 1: Typical operation of client-side cloaking in phishing
websites.
III. OVERVIEW
TABLE II: Summary of the client-side cloaking technique
types identiﬁed in this work.
bots). Consequently, cloaked phishing websites may have a
longer life span than ones without: by delaying or avoiding
detection, the attackers who launch these websites maximize
their return-on-investment [43]. Because client-side evasion
techniques enable complex interactions between potential vic-
tims and phishing websites, they may be more effective in
hampering automated detection than traditional server-side
cloaking, and, thus, pose a threat to potential victim users.
B. Challenges in Analyzing Client-side Cloaking
Unlike server-side code, the client-side code (JavaScript) of
websites can trivially be obtained through crawling. Therefore,
malicious websites typically leverage code obfuscation meth-
ods such as string array encoding, object key transformation,
dead code injection, and even full encryption [17, 31]. Attack-
ers also can dynamically generate and execute code (e.g., using
eval) to hide malicious behaviors. Such obfuscation methods
pose a challenge for static code analysis approaches, which are
otherwise favored for their efﬁciency.
Other types of obfuscation also seek to prevent dynamic
analysis approaches from detecting malicious behaviors. Ma-
licious JavaScript code often targets speciﬁc versions of web
browsers and operating systems by ﬁngerprinting them [18].
Such attacks are difﬁcult to discover because detection systems
require extensive resources to reveal the conditions that trigger
attacks [17]. Besides, external and inter-block dependencies,
which require recording states in different execution paths, can
be obstacles that thwart the analysis of JavaScript code [34].
Furthermore, scripts may execute in an event-driven manner
to necessitate external triggers to initiate malicious behavior
while otherwise appearing benign [34].
All of the aforementioned anti-analysis methods can po-
tentially be leveraged by phishing websites’ implementations
of client-side cloaking techniques. Given the difﬁculty of
analyzing such cloaking, the security community struggles to
thoroughly understand the impact and prevalence of phish-
ers’ tactics, and,
to appropriately mitigate
them. When we consider the scale on which phishing attacks
occur [9],
the consequences of the corresponding gaps in
detection and mitigation can be signiﬁcant.
thus, may fail
Client-side cloaking techniques can help phishing websites
evade detection by anti-phishing entities [43], yet prior studies
have not investigated them in detail, despite evidence that
sophisticated phishing websites—such as those with client-
side cloaking—are responsible for a majority of real-world
damage due to phishing [46].
We discover eight different types of JavaScript cloaking
techniques across three high-level categories: User Interaction,
Fingerprinting, and Bot Behavior (summarized in Table II).
Cloaking techniques in the User Interaction category show
phishing content only if visitors interact with a phishing
website (e.g., by moving the mouse or clicking a speciﬁc
button). Phishing websites with Fingerprinting identify visitors
by inspecting the conﬁguration of browsers or web requests.
Finally, phishing websites with Bot Detection identify anti-
phishing crawlers based on factors such as how long the web
page stays open and whether the web request is repeated
after failing initially. We elaborate on each cloaking type
in Section VI-A.
We aim to comprehensively understand and characterize the
landscape of client-side cloaking techniques used by phishing
websites in the wild through an automated methodology for
analyzing them. To this end, we design,
implement, and
evaluate CrawlPhish: a framework that automatically detects
and analyzes client-side cloaking within phishing websites.
Figure 2 provides an overview of the CrawlPhish architecture.
CrawlPhish is composed of the following components:
1 Crawling and pre-processing (§IV-A): CrawlPhish ﬁrst
collects web page source code (along with any external ﬁle
inclusions) by visiting live phishing website URLs recently
reported to anti-phishing feeds. We then ﬁlter URLs that
cannot be retrieved as well as URLs without any JavaScript
code.
2 Feature extraction (§IV-B): CrawlPhish adapts a state-
of-the-art code analysis method, forced execution [34], to
execute JavaScript regardless of branch conditions, and
extracts all possible execution paths in which evasion
techniques could be implemented. We then derive (1)
visual features of the rendered web pages, by means of
screenshots, and (2) code structure features such as web
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:32:33 UTC from IEEE Xplore.  Restrictions apply. 
1111
Phishing Website VisitedSame PayloadVisitorIdentiﬁcationPhishing or Benign Content Shown Based on Visitor IdentiﬁcationFig. 2: CrawlPhish architecture.
API calls, event listeners, and the Abstract Syntax Tree
(AST) for each path.
3 Cloaking detection (§IV-C): CrawlPhish analyzes the vi-
sual features corresponding to each execution path to detect
if cloaking exists, and it stores the corresponding code
structure features of every such path.
4 Cloaking categorization (§IV-D): Using the code struc-
ture features, CrawlPhish categorizes the cloaking tech-
niques used by phishing websites based on their semantics.
After presenting CrawlPhish and the resulting analysis of
cloaking techniques, we evaluate our approach, as described
below, to ensure that our methodology can help improve user
security by enhancing the ability of anti-phishing systems to
detect and bypass attackers’ evasion techniques.
§V. Detection of cloaked phishing websites: We ﬁrst eval-
uate the effectiveness of CrawlPhish on the dataset of
112,005 phishing websites that we crawled. We show that
CrawlPhish can detect the presence of client-side cloaking
with very low false-negative and false-positive rates (1.75%
and 1.45%, respectively).
§VI. Cloaking categorization: We measure the prevalence
of client-side cloaking techniques in the wild and char-
acterize eight different types in three high-level categories.
Also, we evaluate CrawlPhish to show that it can reliably
categorize the semantics of each cloaking technique. We
compare the ﬁndings from our crawled dataset with an
additional dataset of 100,000 phishing websites. Moreover,
we analyze the source code that CrawlPhish collected
to identify and group related cloaking implementations.
Tracking the deployment and evolution of such code can
be indicative of sophisticated phishing kits, which can help
security researchers pinpoint the threat actor and track the
associated attack volume.
§VII. Impact of cloaking techniques: We deploy 150 arti-
ﬁcial phishing websites to empirically demonstrate that
all
three categories of evasion can successfully bypass
detection by the anti-phishing backends used in major web
browsers. Separately, we conduct a user study to show that
human users remain likely to interact with cloaked phishing
pages. Through these experiments, we show that client-side
cloaking poses a real-world threat.
Dataset. In our evaluation, we use two different datasets.
(1) APWG Dataset: CrawlPhish collected the source code of
28,973 phishing websites from June to December 2018 and
100,000 websites from May to November 2019 using the Anti-
Phishing Working Group (APWG) URL feed [51].
(2) Public Dataset: Phishing website source code from
September
to December 2019 from various well-known
sources, shared publicly by a security researcher [24].
Ethics. We ensured that our experiments did not cause any
disruption to legitimate Internet infrastructure or negatively
impact any human users. Our crawling (Section IV-A) did not
negatively affect any legitimate websites because CrawlPhish
pruned those websites before initiating analysis. The user study
in Section VII-B underwent
the IRB review and received
approval. During this study, we did not ask for or acquire any
Personally Identiﬁable Information (PII) from participants. In
addition, no human users ever saw any of the artiﬁcial phishing
websites discussed in Section VII-A, nor were these websites
conﬁgured to collect any data that may have been submitted.
IV. CRAWLPHISH DESIGN
The design goal of CrawlPhish is to detect and categorize
client-side cloaking techniques in an automated manner while
overcoming the JavaScript code analysis challenges discussed
in Section II-B.
A. Crawling & Pre-processing
To collect the source code of live phishing websites to detect
and classify client-side evasion methods that are currently
employed in the wild, CrawlPhish ﬁrst obtains URLs of known
phishing websites in real-time.
In our deployment, CrawlPhish continuously ingested URLs
from the APWG eCrime Exchange database—a curated clear-
inghouse of phishing URLs maintained by various organiza-
tions engaged in anti-phishing. Because this database receives
frequent updates and tracks phishing URLs that
target a
diverse range of brands, it is well-suited for phishing website
analysis.1 Note, however, that the inclusion of a URL in the
database does not mean that it was adequately mitigated (e.g.,
1 Although the goal of cloaking is to evade detection by automated anti-
phishing systems, such evasion will often delay detection rather than outright
prevent it. Phishing websites may also be detected by other means (e.g.,
manual review) [46]. Thus, we expected the AWPG database to contain a
representative sampling of any client-side cloaking that might be used in the
wild.
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:32:33 UTC from IEEE Xplore.  Restrictions apply. 
1112
ForcedExecutionEngineScreenshotsEventAPIASTCode StructureLivePhishingURLsIPsUser-AgentsCrawler① Crawling & Pre-processing② Feature Extraction③ Cloaking DetectionyesCloaking Type④ Type CategorizationHAR FilesPrune DataPhishingWebsitesVisual Similarity Matches?CloakednoUncloakedyesCloakingTechnique DatabaseManualInspectionnoPruneBlankPagesCode Similarity Matches?OriginalWebpagethrough timely blacklisting) [45]. Hence, websites found to
use sophisticated client-side cloaking still warrant scrutiny.
Next, CrawlPhish downloads source code by visiting each
phishing website URL (shortly after being ingested) us-
ing a programmatically controlled web browser. Speciﬁcally,
CrawlPhish stores source code using HAR ﬁles [57], which
capture all HTTP requests/responses between our client and
the server, and ensure that all dependencies (such as linked
scripts) are preserved for each website. In case of a failed
request, CrawlPhish switches between different conﬁgurations
of IP addresses and user-agents in an effort to circumvent
potential server-side cloaking techniques used by phishing
websites [44]. 4,823 of the 128,973 websites we crawled
(3.74%) showed different response status codes after we
switched request conﬁgurations.
Finally, CrawlPhish ﬁlters out URLs that contain blank
pages or non-phishing websites. Such websites were either
already taken down [7] or were false-positive detections by
the time of crawling. We found 0.53% of URLs within the
APWG Dataset to be false positives. Therefore, CrawlPhish
excludes data in the following cases:
i. empty websites: servers respond with no content.
ii. error websites: requests for URLs were denied because
the phishing websites were already taken down, or used
server-side cloaking which we could not bypass.
iii. non-phishing websites: mistakenly reported URLs, which
CrawlPhish ﬁlters based on a manually curated whitelist
of reputable domains.
B. Feature Extraction
Cloaked content detection. Client-side cloaking techniques
used in phishing websites can be more diverse than server-
side cloaking because they can not only ﬁngerprint visitors
based on conﬁgurations of browsers and systems, but may also
require visitors to interact with websites. To effectively detect
client-side cloaking techniques, CrawlPhish adapts J-Force: a
forced execution framework implemented in the WebKitGTK+
browser that executes JavaScript code along all possible paths,
crash-free, regardless of the possible branch conditions, event
handlers, and exceptions [34]. We modiﬁed J-Force to whitelist
(avoid force-executing) well-known JavaScript libraries, such
as Google Analytics or jQuery,
to expedite execution by
ignoring the benign content changes that such libraries could
introduce.
Execution time limit. We select a time limit for each invoca-
tion of forced execution by CrawlPhish to avoid failures due
to long-running scrips (e.g., due to heavy branching or long-
running loops). Note that this time limit is in addition to other
anti-timeout features implemented in the forced execution
framework, as discussed in Section IX-B.
As a starting point, we chose an execution limit of 300
seconds. We conducted an experiment by force-executing
2,000 randomly selected phishing websites in our crawled
dataset to record the execution time. We found that 1.75%
of phishing websites contained JavaScript code that exceeded
the time limit. Execution ﬁnished as quickly as 12.56 seconds,
the median execution time was 13.82 seconds, the average
execution time was 29.96 seconds, and the standard deviation
was 54.89 seconds. Based on this experiment, we chose a
ﬁnal execution limit of 195 seconds (three standard deviations
above the mean) so that CrawlPhish could efﬁciently analyze
the majority of phishing websites.
Feature extraction. To facilitate detection of (the existence
of) cloaking and categorization of the corresponding cloaking
type, CrawlPhish extracts both visual and code structure
features from each phishing website. Each phishing website’s
visual features consist of the set of all web page screenshots
(in our implementation, at a resolution of 2,495×1,576 pixels)
captured after every possible execution path is explored by
forced execution. In our dataset, each website generated 46.3
screenshots on average. CrawlPhish compares the screenshots
of each execution path within one website against the original
screenshot to detect if cloaking exists, because the presence of
cloaking will result in signiﬁcant visual layout changes [59].
The code structure features include web API calls, web event
listeners, and ASTs, which can characterize different types of
cloaking techniques and reveal how the cloaking techniques
are implemented. Using forced execution, CrawlPhish can
reveal and extract the web APIs and events contained in every
code block, even if the code is obfuscated. CrawlPhish can
then classify the cloaking types in a website using the code
structure features.
Code structure features used. According to preliminary
analysis which we conducted by manually inspecting cloaking
techniques in a sampling of phishing websites in our dataset,
different client-side cloaking techniques each have substan-
tially different features. For example, a cloaking technique that
checks mouse movement waits for an onmousemove event,
then performs DOM substitution or redirection. However, a
cloaking technique that checks screen size would ﬁrst access
the screen.height property. Therefore, as CrawlPhish
executes a code block via forced execution, it records the web
APIs and events that are invoked in the code block.
In addition, we found that the same semantic types of client-
side cloaking techniques have many different implementations.
CrawlPhish distinguishes between different implementations
of each type of cloaking technique by comparing ASTs.
Even though JavaScript code is often obfuscated, the AST
feature is still useful because most phishing websites are
deployed using phishing kits, so the corresponding websites,
with the same phishing kit origin, share the same source code
structure [54]. Furthermore, by computing the AST similarity,
we can trace the origin of the cloaking technique by ﬁnding
similar implementations earlier in phishing pages.
C. Cloaking Detection
CrawlPhish examines the visual similarity between force-
executed screenshots and a screenshot of the website rendered
in an unmodiﬁed version of WebKitGTK+ (i.e., as would
be shown during a normal browser visit) to detect if cloak-
ing exists. Because phishers implement JavaScript cloaking
techniques to evade detection by anti-phishing systems, they
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:32:33 UTC from IEEE Xplore.  Restrictions apply. 
1113
remove suspicious attributes in websites (e.g., login forms)
or outright redirect to a benign website. Therefore, the visual
content shown when the cloaking condition is not satisﬁed will
differ signiﬁcantly from that of the malicious page.
For example, consider a phishing website that asks visitors
to click on a button in a pop-up window prior to showing
the phishing content. After forced execution, two different
execution paths result in two different screenshots: one as
an initial benign-looking page (Figure 4a), and the other
with phishing content (Figure 4b). Therefore, we consider a
phishing website as cloaked if any of the screenshots taken
during forced execution noticeably differ from the original one.
CrawlPhish can also reveal phishing content hidden behind
multiple layers of cloaking. Consider a phishing website with
a cloaking technique that (1) detects mouse movement and (2)
checks the referrer such that the malicious content will appear
only if both requirements are met. CrawlPhish will explore
the execution path that shows the malicious content by force-
executing it, regardless of the branching conditions. Therefore,
after each screenshot is compared with the screenshot of the
original page, CrawlPhish determines that a cloaking technique
exists because one of the screenshots will differ.
Removal of blank pages after forced execution. Screenshots
of pages rendered by force-executed paths may be blank,
which can be caused by (1) negative branches from cloaking
techniques (such as mouse movement detection) that require
user input or (2) execution paths that take longer to ﬁnish
than the execution time limit. In the latter case, CrawlPhish
can mislabel a website as cloaked if an initial screenshot is
compared to an empty page caused by unﬁnished execution
paths. For example, phishers may trigger an inﬁnite loop if
they identify that a visit is from an anti-phishing system.
In this case, CrawlPhish cannot ﬁnish forced execution and