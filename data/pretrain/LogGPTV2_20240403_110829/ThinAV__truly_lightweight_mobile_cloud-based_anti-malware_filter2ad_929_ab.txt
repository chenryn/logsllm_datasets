tained in that ﬁle. Unfortunately, these scanning services
are based on proprietary anti-malware engines, and as such,
the exact details of the engines underpinning these services
are very closely held trade secrets. Therefore, the exact ca-
pabilities and limitations of these services with respect to
threat detection are not publicly known.
The currently implemented modules and their descrip-
tions:
vice with a 10MB ﬁle size limit.
• Kaspersky – Oﬀers a free service that uses a propri-
etary anti-malware engine for scanning ﬁles that are
1MB or smaller in size.
• VirusChief – A multi-engine anti-malware scanning ser-
• VirusTotal – Scans uploaded ﬁles up to 20MB in size
with 42 diﬀerent scanning engines. VirusTotal has a
public API to interact with its services.
• ComDroid - While not an anti-malware engine, Com-
Droid can identify potential vulnerabilities in Android
apps by performing static code analysis. The Com-
Droid module identiﬁes scanned applications as being
“at risk” as opposed to being “infected”.
The ThinAV server is currently conﬁgured to select scan-
ning services based on the average amount of time each mod-
ule takes to scan a ﬁle. We found the fastest service to be
Kaspersky, followed by VirusChief, ComDroid, and Virus-
Total.
If any scanning module returns an error from an
attempted online scan, then the next module in the prior-
ity sequence is selected. If all four scanning modules fail, a
general error code is returned to the device that originated
the request. Performance measurements for the scanning
modules are discussed in the following section.
4. THINAV EVALUATION
This section presents the results of our evaluation of Thi-
nAV. All development and testing was done on the Android
212
Number of Apps
Mean App Size
Median App Size
Minimum App Size
Maximum App Size
Proportion of Apps <1 MB
Proportion of Apps <10 MB
Proportion of Apps <20 MB
1022
2.65 MB
1.78 MB
0.02 MB
37.06 MB
34.64 %
97.16 %
99.51 %
Table 1: General ﬁle size characteristics of the An-
droid test data set.
emulator provided in the SDK. The emulator enabled rapid
development on diﬀerent versions of the Android operating
system, and allowed for changes to be made to the Android
source code.
Working on the emulator presents evaluation issues with
respect to network performance. Because ThinAV is heavily
reliant on the network, link speed has a direct impact on
performance. On a mobile device like a cell phone, the speed
of the cellular connection can vary based on the location of
the user, radio interference, the load on the cellular network,
as well as other factors. Due to the challenges involved in
cellular network measurements, we use the results of Gass
et al. [10].
4.1 Data Set
The evaluation process was performed with a collection of
apps downloaded from the oﬃcial Google Play Store (known
as the Android Market at the time of data collection) using
a custom crawler. We downloaded the top 50 free apps (as
ranked by user ratings) in each application category on Jan-
uary 3, 2012. The majority of package downloads were suc-
cessful, with 28 downloads causing repeated failures. This
resulted in 1,022 apps spread across 21 application cate-
gories, with each category having between 46 and 50 pack-
ages. Table 1 summarizes the key ﬁle size statistics of the
data set.
4.2 Malware Detection
We ﬁrst uploaded the entire data set to the VirusTotal
scanning service to to conﬁrm that VirusTotal, and therefore
other scanning services are capable of correctly receiving and
scanning Android applications. This initial scan also allowed
us to obtain a baseline of detection. That is, to see how many
apps in our initial data set contain malware.
VirusTotal ﬂagged several possible instances of malware
in the data set downloaded from the Google Play Store. Of
the 1,022 apps uploaded, 1,019 were scanned (three apps
were skipped due to size restrictions) and 27 were ﬂagged
as malware by at least one scanning engine. One package
was ﬂagged as malware by four diﬀerent engines, nine pack-
ages were ﬂagged by two engines, and the remaining seven-
teen packages were ﬂagged as malware by a single engine.
Table 2 provides details on some of the commonly ﬂagged
samples. The most commonly identiﬁed sample was from
the Adware.Airpush family. However, the majority of these
samples were identiﬁed by a single scanning engine (DrWeb),
which raises the possibility of this being a false positive.
The next most common sample was Plankton, which was
identiﬁed by a variety of scanning engines. The remaining
Sample Name
Adware.Airpush(2, 3)
Plankton (A, D, G)
SmsSend (151, 261)
Rootcager
Trojan
Adware
Malware Type Count Detection
Engine(s)
DrWeb,
Kaspersky
Kaspersky,
Comodo,
NOD32,
Trend
Micro
DrWeb
Symantec
Dialer
Trojan
15
6
2
2
Table 2: Most frequent samples of malware detected
in Google Market data set. Detection engine refers
to which VirusTotal scanning engines detected the
sample.
malware samples had far fewer occurrences in the data set.
While the test data set only suggested that 6 of the AV
engines used by VirusTotal are capable of detecting Android
malware, we later conﬁrmed that as many as 26 (more than
half of the VirusTotal scanning engines) are capable of de-
tecting some form of Android speciﬁc malware.
4.3 AV Scanning Module Performance
We developed a testing program which uploaded ﬁles of
diﬀerent sizes to each of the scanning services at speciﬁc time
intervals. This program was designed to measure the re-
sponse time of each service. The program submitted 12 ﬁles
(of sizes 0 KB, 1 KB, 2 KB, 4 KB, 8 KB, 16 KB, 32 KB, 64
KB, 128 KB, 256 KB, 512 KB and 1023 KB) for scanning in
random order over an 8 day window. The ﬁles were created
by a script which produces ﬁles of a speciﬁed size ﬁlled with
pseudo-random bits with the expectation that ﬁles gener-
ated in such a way would have an extremely low probability
of being ﬂagged as malware by one of the scanning services,
or exist in the service cache. Test ﬁles were uploaded in a
pseudo-random order every time the test program was run
to overcome any penalty that might be incurred against the
ﬁrst ﬁle being uploaded due to DNS lookups.
Results.
For each of the three scanning services, several hundred
response time measurements were recorded. A cursory re-
view of the data showed a handful of extreme outliers for
each service. Any measurement beyond two standard devia-
tions of the mean was classiﬁed as an outlier. This threshold
was chosen because it eliminated the most extreme results,
while retaining the vast majority of the data.
A comparison of the measurements from the three services
shows a clear diﬀerence of nearly an order of magnitude be-
tween the performance of VirusTotal and the other two scan-
ning services. The average response times from Kaspersky
and VirusChief range from 1.54 – 14.49 seconds and 6.82
– 28.70 seconds respectively, while the response times from
VirusTotal range from 1.21 – 229.28 seconds (though the
latter range becomes 148.92 – 229.28 seconds, when only
non-zero ﬁle sizes are considered). The upload portion of
VirusTotal shows response times similar to Kaspersky with
response times ranging from 1.94 – 11.74 seconds.
With the outliers removed, the response time data was
plotted (see Figures 2, 3, and 4) in an attempt to determine
the correlation between ﬁle size and service response time
for the three scanning services. Each of the ﬁgures shows
the upload ﬁle size plotted versus the response time for each
of the three scanning services, and Figure 5 graphs the up-
load and response speed of VirusTotal. Kaspersky and Vir-
usChief both show a similar positive correlation between ﬁle
size and response time, with the VirusChief data being posi-
tively shifted on the y-axis (and therefore slower) by roughly
ﬁfteen seconds. VirusTotal, on the other hand, shows little
if any relationship between ﬁle size and response time. The
trend of the VirusTotal response time data is slightly neg-
ative and has a much larger y-intercept than either of the
other two scanning services. Conversely, the upload portion
of the VirusTotal scan shows a trend very similar to Kasper-
sky. With this data, we produced a set of linear equations
which approximate the performance of the scanning services
as a function of the number of bytes in the ﬁle (see Table
3).
)
s
d
n
o
c
e
s
(
e
m
i
t
e
s
n
o
p
s
e
R
30 
25 
20 
15 
10 
5 
0 
0 
200000 
400000 
600000 
800000 
1000000 
1200000 
File size (bytes) 
Figure 2: Scan response time versus ﬁle upload size
for the Kaspersky virus scanner service.
)
s
d
n
o
c
e
s
(
e
m
T
e
s
n
o
p
s
e
R
i
45 
40 
35 
30 
25 
20 
15 
10 
5 
0 
0 
200000 
400000 
600000 
File size (bytes) 
800000 
1000000 
1200000 
Figure 3: Scan response time versus ﬁle upload size
for the VirusChief virus scanner service.
Discussion.
It is not surprising that Kaspersky, which only scans with
a single anti-virus engine, returns the fastest results, and
VirusChief, which scans with six anti-virus engines is roughly
ﬁfteen seconds slower than Kaspersky when scanning a sim-
ilarly sized ﬁle. We attribute the erratic VirusTotal perfor-
mance to the company’s prioritization of scanning requests.
VirusTotal assigns the lowest priority to requests that are
sent via their formal API, and the response appears to not
be dependent on the size of the uploaded ﬁle, but rather on
how busy the VirusTotal scanning service is at any given
213
)
s
d
n
o
c
e
s
(
e
m
i
t
e
s
n
o
p
s
e
R
900 
800 
700 
600 
500 
400 
300 
200 
100 
0 
0 
200000 
400000 
600000 
800000 
1000000 
1200000 
File size (bytes) 
)
s
(
e
m
T
i
1000 
900 
800 
700 
600 
500 
400 
300 
200 
100 
0 
0 
2000 
4000 
6000 
8000 
10000 
12000 
App Size (in KB) 
Figure 4: Scan response time versus ﬁle upload size
for the VirusTotal virus scanner service.
Figure 6: Response time of the ComDroid service
as a function of package size.
)
s
d
n
o
c
e
s
(
e
m
i
t
e
s
n
o
p
s
e
R
30 
25 
20 
15 
10 
5 
0 
0 
200000 
400000 
600000 
800000 
1000000 
1200000 
File size (bytes) 
Figure 5: Upload response time versus ﬁle upload
size for the VirusTotal virus scanner service when
uploading a ﬁle and not polling for a scan result.
moment. This distinction is made much clearer when com-