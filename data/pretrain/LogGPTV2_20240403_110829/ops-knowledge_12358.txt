User: I have a scenario where text delimited files arrives from different servers(around 10) to hadoop system every 30 minutes.
Each file has around 2.5 million records and may not arrive at the same time, I am looking for an approach where these file can be processed every 30 minutes. 
My questions are:

How to handle files arriving at different times?
I want the data to be aggregated across 10 files. Should such large files be combined or processed separately?

I want this solution to be implemented in python but solutions using any tools/techniques in hadoop would be appreciated.
Assistant: How to handle files arriving at different times?

Doesn't matter unless your data is time-sensitive. If so, then your raw data should include the timestamp at which the record was written. 

Should such large files be combined or processed separately?

Large, separate files are best. Take note of the HDFS block size. This size depends on your installation.  

I want this solution to be implemented in python

You're welcome to use Spark Streaming for watch a directory for files, or Oozie+Spark to just schedule regular batches but other tools are arguably simpler. 
Some you can research 

Apache NiFi
Streamsets Data Collector
Apache Flume

Flume will require you to install agents on those 10 external servers. 
Each of the listed services can read data in near-real time, so you don't explicitly need 30 minute batches.