with input, /dev/random and /dev/urandom. In particular, we showed several attacks proving
that these generators are not robust according to our deﬁnition, and do not accumulate entropy
properly. These attacks are due to the vulnerabilities of the entropy estimator and the internal
mixing function of the generators.
Security Analysis of OpenSSL and Java Generators In [CR14], we gave an analysis
of real-life generators using the security model of robustness against memory attacks and we
demonstrated how it can help to identify new vulnerabilities. In particular, we showed that a
full internal state corruption is not necessary to compromise a lot of concrete implementation
of real-life generators, instead only a partial one may be suﬃcient. We characterized how a
generator can be attacked in order to produce a predictible output and we identiﬁed how many
bits of the internal state are required to mount an attack against the generator. In this aim,
we characterized and gave a new security analysis of pseudo-random number generators with
input implementations from widely used providers in real-life applications: OpenSSL, OpenJDK,
Android, Bouncycastle and IBM. To our knowledge, while intensively used in practice, these
generators had not been evaluated with recent security models. Our analysis revealed new
vulnerabilities of these generators due to the implementation of their internal state in several
ﬁelds that are not updated securely during generators operations.
This work is fully described in Chapter 7.
— 7 —
Chapter 1. Introduction
— 8 —
Chapter 2
Preliminaries
Throughout this thesis we refer to discrete probability distributions. For notations, deﬁnitions
and theorems presented in this Chapter, we refer to [GB01,Sho06,BR06,Vad12].
2.1 Probabilities
Random Variable. Let X be a random variable over a sample set S. Then X deﬁnes a
probability distribution PX : S → [0, 1], where PX(x) := Pr[X = x] called the distribution of
the random variable X. In this thesis, we will denote by X both the random variable X and
$← X when x is sampled according
the distribution of the random variable X and we denote x
to X. The support of a random variable X is the set supp(X) = {x : Pr[X = x] > 0}. If A is
an algorithm, then A(X) denotes the random variable that samples x
$← X and returns A(x).
Uniform Distribution. Let X be a random variable over a non empty ﬁnite sample set S.
If ∀x ∈ S, PX(x) = 1|S|, the random variable X is said uniformly distributed over S, that we
$← S. Let n > 0 be an integer, the uniform distribution over the sample set {0, 1}n is
denote X
denoted Un.
Independence. Let X and Y be two random variables. Then X and Y are independent if for
all x and y,
Pr[(X = x) and (Y = y)] = Pr[X = x] · Pr[Y = y].
Let {Xi}i∈I be a ﬁnite family of random variables. The family is pairwise-independent if for all
i, j ∈ I such that i 6= j, the random variables Xi and Xj are independent.
2.2 Indistinguishability
P
x | Pr[X = x] − Pr[Y = x]|.
Statistical Indistinguishability. Let n > 0 be an integer and let X and Y be two random
variables over the sample set {0, 1}n. The statistical distance between X and Y is equal to:
SD(X, Y ) = 1
2
Theorem 1 shows that the statistical distance is a distance. In particular, it satisﬁes the triangle
inequality, that will be useful to build reductions between security notions.
Theorem 1 (Statistical Distance Properties [Sho06]). Let n > 0 be an integer and let X, Y and
Z be random variables over the sample set {0, 1}n. Then: 0 ≤ SD(X, Y ) ≤ 1, SD(X, X) = 0,
SD(X, Y ) = SD(Y, X) and SD(X, Z) ≤ SD(X, Y ) + SD(Y, Z).
— 9 —
Chapter 2. Preliminaries
Theorem 2 will also be useful when we build reductions between security notions. In particular,
it implies that if the statistical distance between two random variables X and Y is small, no
eﬃcient algorithm can distinguish between them.
Theorem 2 (Statistical Distance Properties [Sho06]). Let n > 0 be an integer and let X and
Y be random variables over the sample set {0, 1}n. Then for every subset T ⊆ {0, 1}n, we have
SD(X, Y ) ≥ | Pr[X ∈ T] − Pr[Y ∈ T]|.
Finally, the random variables X and Y are said ε-close if SD(X, Y ) ≤ ε.
Computational Indistinguishability. Let X and Y be two random variables over {0, 1}n,
let t be an integer and let A be a probabilistic algorithm running in time t, that takes as input
a bitstring in {0, 1}n. Note that the running time t includes both the computation time and the
pre-computation time (e.g. memory setting). The t-computational distance between the two
random variables X and Y is equal to CDt(X, Y ) = maxA≤t | Pr[A(X) = 1] − Pr[A(Y ) = 1]|
where the notation maxA≤t denotes that the maximum is over all A running in time at most t.
Theorems 1 and 2 can be stated for the computational distance.
The random variables X and Y are said (t, ε)-close if for any probabilistic algorithm A running
within time t, CDt(X, Y ) ≤ ε. When t = ∞, meaning A is unbounded, then X and Y are
ε-close.
2.3 Hash Functions
Let p and m be integers, such that m < p. A hash function is a function h : {0, 1}p → {0, 1}m.
Pairwise Independence. A family of hash functions H = {h : {0, 1}p → {0, 1}m} is pairwise-
independent:
1. ∀x ∈ {0, 1}p, h(x) is uniformly distributed in {0, 1}m, when h
2. ∀x1 6= x2 ∈ {0, 1}p, the random variables h(x1) and h(x2) are independent, when h
The two above conditions can be combined as follow: ∀x1, x2 ∈ {0, 1}p,∀y1, y2 ∈ {0, 1}m,
$← H.
$← H.
Pr
$←H
h
[h(x1) = y1 and h(x2) = y2] = Pr
$←H
h
1
22m
=
[h(x1) = y1] · Pr
$←H
h
[h(x2) = y2]
Universality. A hash functions family H = {h : {0, 1}p → {0, 1}m} is ε-universal if for any
inputs x1 6= x2 ∈ {0, 1}p we have:
[h(x1) = h(x2)] ≤ ε.
Pr
$←H
h
2.4 Game Playing Framework
In this work, we focus on giving precise security properties for systems.
In cryptography, a
scheme has reductionist security (or provable security), as opposed to heuristic security, if its
security requirements can be stated formally in an adversarial model where the capabilities of
the adversary are formally described with clear assumptions. This formal description includes
the potential accesses of the adversary to the system and its computational resources. In this
approach, the security of a cryptographic scheme is based on algorithmic problems that are
— 10 —
2.5. Shannon Entropy, Min-Entropy
supposed to be hard to solve. The scheme is secure as long as the underlying algorithmic
problems are diﬃcult and the security of the scheme is proven by reduction to the security of
the underlying algorithmic problems.
For our security deﬁnitions and proofs we use the code-based game playing framework of [BR06].
A security game involves a challenger and an adversary, denoted A. The adversary will always
be modelled with a probabilistic algorithm running in time t. The challenge of the adversary is
to distinguish between two experiments, which are both indexed by a Boolean bit b.
Interactions between the challenger and the adversary are modeled with procedures. To describe
procedures, we use the expression ’proc. ’. When some parameters are adversarially chosen, they
are used as input to the procedures. When procedures generate some outputs (as a result of a
computation, for example):
• The output is given with a directive named OUTPUT when the output is given to the
adversary and the security games continues.
• The output is given with a directive named RETURN when the output is the result of the
security game (which is therefore terminated).
A security game GAME has an initialize procedure, procedures to respond to adversary oracle
queries, and a ﬁnalize procedure. A security game GAME is executed with an adversary A as
follows. First, challenger executes procedure initialize, and its outputs are given as inputs to A.
Then A executes, its oracle queries being answered by the corresponding procedures of GAME.
In this description, A can be restricted to a limited number or order of oracle queries. When A
terminates, its output becomes the input to the ﬁnalize procedure.
The output of the ﬁnalize procedure is called the output of the security game GAME, and we
denote the output of the adversary as GAMEA. Finally we denote the event that this output
takes value y as GAMEA ⇒ y and we deﬁne the advantage of A in GAME as
AdvGAMEA
= 2 × Pr[GAMEA ⇒ 1] − 1.
Our convention is that Boolean ﬂags are assumed initialized to false and that the running time
of the adversary A is deﬁned as the total running time of the game with the adversary in
expectation, including the procedures of the game.
To prove a reduction from the security of a scheme to the intractability of an algorithmic problem,
we deﬁne sequences of security games as follows: the ﬁrst game is the game that deﬁnes the
security of the scheme, the last game is the game that deﬁnes the intractability of the algorithmic
problem, and the games in between describe successive transitions from the two games. We then
estimate the distance between the successive security games and the estimation of the reduction
uses Theorems 1 and 2 as the properties of the computational distance will ensure that we can
bound the (global) distance between the two games by the sum of all distances.
2.5 Shannon Entropy, Min-Entropy
We now model the concept of ’how random’ is the distribution of a random variable. We will
consider that a phenomenon is described by a random variable and we want to model ’how ran-
dom’ its distribution is. We will name sources of randomness or sources the random variables
that will be used because they ’look random’ or they ’contain a certain amount of randomness’.
Hence a source on {0, 1}p is a random variable on {0, 1}p.
We need a tool to estimate the ’amount of randomness’ that is contained in a given source. In
— 11 —
Chapter 2. Preliminaries
doing so, we will be able to formalize that we can ’extract k bits of randomness’ from a source
that contains ’n bits of randomness’, for k ≤ n. This idea is captured with the notion of entropy,
that is given in Deﬁnition 1.
The ﬁrst notion of entropy (the Shannon entropy) is described in the seminal paper of Shan-
non [Sha48]. Consider a sequence of random variables X1,··· , Xn, of distribution probabilities
p1,··· , pn. Shannon shows that entropy is the only function that satisﬁes the three properties:
(a) it shall be continuous (b) if pi = 1
n, then it shall be maximal (when every outcome is equally
like the uncertainty is greatest and hence so is the entropy) and (c) it should be additive. This
leads to the notion of ’Shannon entropy’, denoted H1 below.
The second notion of entropy (the min-entropy) was ﬁrst used as a measure of randomness in
the seminal work of Chor and Goldreich [CG85], as explained in the survey of Shaltiel [Sha02].
This notion is very close to the notion of randomness extractor, a notion that we will describe
in Section 2.6, in the sense that a necessary condition to extract randomness from distributions
is that they shall have high min-entropy.
Deﬁnition 1 (Entropy). Let X be a random variable on a sample set S.
• The Shannon entropy of X is H1(X) = Ex∈S[− log Pr[X = x]].
• The min-entropy of X is H∞(X) = minx∈S{− log Pr[X = x]}.
2128−1. Then H1(X) = 127, 006 and H∞(X) = 7.
First note that if X is uniform on (e.g.) {0, 1}128, then H∞(X) = H1(X) = 128. However
when X is not uniform, the two notions give diﬀerent values. Let us illustrate this with one
example. Consider the discrete random variable X deﬁned on {0, 1}128, where Pr[X = 0] = 2−7
and Pr[X = y, y 6= 0] = 1−2−7
Hence the two notions of entropy describe a diﬀerent deviation to the uniform distribution: the
estimated Shannon entropy H1(X) is close to 128, whereas its min-entropy H∞(X) is on the
opposite very low and is such that Pr[X = 0] = 2−H∞(X), setting a direct relation between
H∞(X) and the set of non-uniformity.
Let us now illustrate why the Shannon Entropy can not be used for cryptographic purposes.
Suppose now that we use directly the source X to generate a 128-bits encryption key for a sym-
metric algorithm (AES for example). Recall that an encryption scheme is a triple (key, enc, dec),
where key is a probabilistic algorithm for key generation, enc is the encryption algorithm and
dec is the decryption algorithm. For each key K sampled by key and for all x, we have that
enc(K, dec(K, x)) = x. Consider the (simple) security game ENC described in Figure 2.1, where
the key sampling algorithm key is the algorithm that samples a key of distribution X. Consider
proc. enc-ror(m0, m1)
c0 ← enc(K, m0)
c1 ← enc(K, m1)
OUTPUT cb
proc. initialize()
K $← X;
$← {0, 1}
b
proc. ﬁnalize(b∗)
IF b = b∗ RETURN 1
ELSE RETURN 0
Figure 2.1 – Procedures in Security Game ENC
now an adversary A in game ENC. As the key K is sampled from the random variable X, with
probability 2−7, we have that K = 0. Hence with probability 2−7, A can distinguish between
— 12 —
2.6. Randomness Extractors
enc(K, m0) and enc(K, m1), for all m0 and m1 and for all encryption scheme (key, enc, dec). How-
ever, it is expected that the outputs c0 and c1 are distinguishable with probability 2−128. If one
considers Shannon entropy in place of min-entropy, one could have argued that the probability of
distinguishing between c0 and c1 is close to 128, which is wrong. Hence there is a direct relation
between the min-entropy of a random variable and the advantage for an adversary in distinguish-
ing between two computations in a security game where the random variable is used as a source
of randomness. In other words, H1 measures the amount of randomness that a source contains
on average, (as justiﬁed with the use of the expectation E) while H∞ measures the amount of
randomness on the worst-case, which are typically the cases that an adversary will use to break
a security scheme. This justiﬁes the notion of k-sources (or distributions with min-entropy at
least k) as the formalization of the notion of sources ’containing k bits of randomness’.
Deﬁnition 2 (k-source). A source X is a k-source if H∞(X) ≥ k.
2.6 Randomness Extractors
Randomness is concretely generated from sources which are potentially biased, where the only
known information is that they potentially contain some amount of randomness, or, as formalized
in Deﬁnition 2, they are k-sources. We therefore need a map that extracts the randomness that
is actually contained in these sources, and produces an output which is close to uniform. These
maps are named extractors.
Let ﬁrst illustrate this idea with the two following examples:
• Extractor for Independent Sources. Consider a sequence of independent sources of
bits Xi ∈ {0, 1} where for all i, Pr[Xi = 1] = δ (i.e., all sources are biased with the
same bias). Consider the following map Extract : {0, 1} × {0, 1} → {0, 1} ∪ {∅}, where:
Extract(0, 0) = Extract(1, 1) = ∅, Extract(1, 0) = 1 and Extract(0, 1) = 0. Then the output
of Extract is uniformly distributed. This map is known as the ’Von Neumann extractor’,
as described in [VN51].
• Extractor for Independent-Bit Sources. Consider a sequence of independent sources
of bits Xi ∈ {0, 1}, where for all i, Pr[Xi = 1] = δi, Pr[Xi = 0] = 1 − δi, 0 < δ ≤ δi ≤ 1 − δ
(i.e., all sources are biased with diﬀerent bounded biases). Consider the following function
Extract : {0, 1}p → {0, 1}, where Extract(x1,··· , xp) = x1 ⊕ ··· ⊕ xp. Then if p sources
2 + (1 − 2δ)p],
are used, each bit output by Extract has bias in the interval [ 1
hence:
2 − (1 − 2δ)p, 1
| Pr[yi = 0|y1,··· , yi−1] − Pr[yi = 1|y1,··· , yi−1]| < (1 − 2δ)p.
Therefore the outputs of Extract are indistinguishable for large p. This function is described
by Santha and Vazirani in [SV84] and is also referred to as the ’parity extractor’.
The previous ’parity extractor’ is an example of deterministic extractors, as deﬁned by Nisan
and Zuckerman in [NZ93]. A deterministic extractor is formalized in Deﬁnition 3. Note that
formally, the ’Von Neumann extractor’ is not an extractor, as the deﬁnition supposes that an
output is generated for any input (the output ∅ is not possible).
Deﬁnition 3 (Deterministic Extractors). Let p and m be integers, such that p ≥ m. Let C be a
class of sources on {0, 1}p. An ε-deterministic extractor for C is a function Extract : {0, 1}p →
{0, 1}m, such that for every X ∈ C, Extract(X) and Um are ε-close.
Note that Deﬁnition 3 requires that the function Extract works for all the sources X that belong
to the class C. Therefore, if one wants to extract randomness from sources, these distributions
— 13 —
Chapter 2. Preliminaries
do not need to be known: for example, in case of the ’parity extractor’ presented before, the
bias δ do not need to be known. Moreover the link between min-entropy and extraction comes
directly from Deﬁnition 3, as a necessary condition to extract m bits of randomness from a
distribution X is that H∞(X) ≥ m.
Let us now describe another class of sources, named Santha-Vazirani sources, or δ-Unpredictable-
bit sources. These sources are also described in [SV84]. Consider the sequence of sources of bits