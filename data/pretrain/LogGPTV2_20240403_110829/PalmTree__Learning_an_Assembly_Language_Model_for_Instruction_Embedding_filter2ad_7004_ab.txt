Asm2Vec [10] looks into instructions to a very limited degree. It
considers an instruction having one opcode and up to two operands.
In other words, each instruction has up to three tokens, one for
opcodes, and up to two for operands. A memory operand with
an expression will be treated as one token, and thus it does not
understand how a memory address is calculated. It does not take
into account other complexities, such as prefix, a third operand,
implicit operands, EFLAGS, etc.
1 ; prepare the third argument for function call
2 mov rdx , rbx
3 ; prepare the second argument for function call
4 mov rsi , rbp
5 ; prepare the first argument for function call
6 mov rdi , rax
7 ; call memcpy () function
8 call memcpy
9 ; test rbx register ( this instruction is reordered )
10 test rbx , rbx
11 ; store the return value of memcpy () into rcx register
12 mov rcx , rax
13 ; conditional jump based on EFLAGS from test instruction
14 je
0 x40adf0
Listing 2: Instructions can be reordered
2.2.2 Noisy Instruction Context. The context is defined as a small
number of instructions before and after the target instruction on
the control-flow graph. These instructions within the context often
have certain relations with the target instruction, and thus can help
infer the target instruction’s semantics.
Session 12A: Applications and Privacy of ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3238Table 1: Summary of Approaches
Name
DeepVSA [14]
Instruction2Vec [41]
InnerEye [43]
Asm2Vec [10]
PalmTree (this work)
Encoding
1-hot encoding on raw-bytes
manually designed
word2vec
PV-DM
BERT
Internal Structure
Context
Disassembly Required
no
yes
no
partial
yes
no
no
control flow
control flow
control flow & data flow
no
yes
yes
yes
yes
While this assumption might hold in general, compiler optimiza-
tions tend to break this assumption to maximize instruction level
parallelism. In particular, compiler optimizations (e.g., “-fschedule-
insns”, “-fmodulo-sched”, “-fdelayed-branch” in GCC) seek to avoid
stalls in the instruction execution pipeline by moving the load from
a CPU register or a memory location further away from its last
store, and inserting irrelevant instructions in between.
Listing 2 gives an example. The test instruction at Line 10 has
no relation with its surrounding call and mov instructions. The
test instruction, which will store its results into EFLAGS, is moved
before the mov instruction by the compiler, such that it is further
away from the je instruction at Line 14, which will use (load) the
EFLAGS computed by the test instruction at Line 10. From this
example, we can see that contextual relations on the control flow
can be noisy due to compiler optimizations.
Note that instructions also depend on each other via data flow
(e.g., lines 8 and 12 in Listing 2). Existing approaches only work on
control flow and ignore this important information. On the other
hand, it is worth noting that most existing PTMs cannot deal with
the sequence longer than 512 tokens [33] (PTMs that can process
longer sequences, such as Transformer XL [8], will require more
GPU memory), as a result, even if we directly train these PTMs
on instruction sequences with MLM, it is hard for them capture
long range data dependencies which may happen among different
basic blocks. Thus a new pre-training task capturing data flow
dependency is desirable.
2.3 Summary of Existing Approaches
Table 1 summarizes and compares the existing approaches, with
respect to which encoding scheme or algorithm is used, whether dis-
assembly is required, whether instruction internal structure is con-
sidered, and what context is considered for learning. In summary,
raw-byte encoding and manually-designed encoding approaches
are too rigid and unable to convery higher-level semantic infor-
mation about instructions, whereas the existing learning-based
encoding approaches cannot address challenges in instruction in-
ternal structures and noisy control flow.
3 DESIGN OF PALMTREE
3.1 Overview
To meet the challenges summarized in Section 2, we propose PalmTree,
a novel instruction embedding scheme that automatically learns a
language model for assembly code. PalmTree is based on BERT [9],
and incorporates the following important design considerations.
First of all, to capture the complex internal formats of instruc-
tions, we use a fine-grained strategy to decompose instructions: we
consider each instruction as a sentence and decompose it into basic
tokens.
Then, in order to train the deep neural network to understand
the internal structures of instructions, we make use of a recently
proposed training task in NLP to train the model: Masked Language
Model (MLM) [9]. This task trains a language model to predict the
masked (missing) tokens within instructions.
Moreover, we would like to train this language model to cap-
ture the relationships between instructions. To do so, we design a
training task, inspired by word2vec [28] and Asm2Vec [10], which
attempts to infer the word/instruction semantics by predicting two
instructions’ co-occurrence within a sliding window in control
flow. We call this training task Context Window Prediction (CWP),
which is based on Next Sentence Prediction (NSP) [9] in BERT. Es-
sentially, if two instructions i and j fall within a sliding window in
control flow and i appears before j, we say i and j have a contextual
relation. Note that this relation is more relaxed than NSP, where
two sentences have to be next to each other. We make this design
decision based on our observation described in Section 2.2.2: in-
structions may be reordered by compiler optimizations, so adjacent
instructions might not be semantically related.
Furthermore, unlike natural language, instruction semantics
are clearly documented. For instance, the source and destination
operands for each instruction are clearly stated. Therefore, the data
dependency (or def-use relation) between instructions is clearly
specified and will not be tampered by compiler optimizations. Based
on these facts, we design another training task called Def-Use Pre-
diction (DUP) to further improve our assembly language model.
Essentially, we train this language model to predict if two instruc-
tions have a def-use relation.
Figure 1 presents the design of PalmTree. It consists of three
components: Instruction Pair Sampling, Tokenization, and Lan-
guage Model Training. The main component (Assembly Language
Model) of the system is based on the BERT model [9]. After the
training process, we use mean pooling of the hidden states of the
second last layer of the BERT model as instruction embedding. The
Instruction Pair Sampling component is responsible for sampling
instruction pairs from binaries based on control flow and def-use
relations.
Then, in the second component, the instruction pair is split into
tokens. Tokens can be opcode, registers, intermediate numbers,
strings, symbols, etc. Special tokens such as strings and memory
offsets are encoded and compressed in this step. After that, as intro-
duced earlier, we train the BERT model using the following three
tasks: MLM (Masked Language Model), CWP (Context Window
Prediction), and Def-Use Prediction (DUP). After the model has
been trained, we use the trained language model for instruction
Session 12A: Applications and Privacy of ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3239Figure 1: System design of PalmTree. Trm is the transformer encoder unit, C is the hidden state of the first token of the sequence
(classification token), Tn (n = 1 . . . N ) are hidden states of other tokens of the sequence
embedding generation. In general, the tokenization strategy and
MLM will help us address the first challenge in Section 2.2, and
CWP and DUP can help us address the second challenge.
In Section 3.2, we introduce how we construct two kinds of
instruction pairs. In Section 3.3, we introduce our tokenization pro-
cess. Then, we introduce how we design different training tasks to
pre-train a comprehensive assembly language model for instruction
embedding in Section 3.4.
3.2 Input Generation
We generate two kinds of inputs for PalmTree. First, we disassem-
ble binaries and extract def-use relations. We use Binary Ninja2
in our implementation, but other disassemblers should work too.
With the help of Binary Ninja, we consider dependencies among
registers, memory locations, and function call arguments, as well as
implicit dependencies introduced by EFLAGS. For each instruction,
we retrieve data dependencies of each operand, and identify def-use
relations between the instruction and its dependent instructions.
Second, we sample instruction pairs from control flow sequences,
and also sample instruction pairs based on def-use relations. Instruc-
tion pairs from control flow are needed by CWP, while instruction
pairs from def-use relations are needed by DUP. MLM can take both
kinds of instruction pairs.
3.3 Tokenization
As introduced earlier, unlike Asm2Vec [10] which splits an in-
struction into opcode and up to two operands, we apply a more
fine-grained strategy. For instance, given an instruction “mov rax,
qword [rsp+0x58]”, we divide it into “mov”, “rax”, “qword”, “[”,
“rsp”, “+”, “0x58”, and “]”. In other words, we consider each instruc-
tion as a sentence and decompose the operands into more basic
elements.
2https://binary.ninja/
We use the following normalization strategy to alleviate the
OOV (Out-Of-Vocabulary) problem caused by strings and constant
numbers. For strings, we use a special token [str] to replace them.
For constant numbers, if the constants are large (at least five digits
in hexadecimal), the exact value is not that useful, so we normal-
ize it with a special token [addr]. If the constants are relatively
small (less than four digits in hexadecimal), these constants may
carry crucial information about which local variables, function ar-
guments, and data structure fields that are accessed. Therefore we
keep them as tokens, and encode them as one-hot vectors.
3.4 Assembly Language Model
In this section we introduce how we apply the BERT model to our
assembly language model for instruction embedding, and how we
pre-train the model and adopt the model to downstream tasks.
3.4.1 PalmTree model. Our model is based on BERT [9], the state-
of-the-art PTM in many NLP tasks. The proposed model is a multi-
layer bidirectional transformer encoder. Transformer, firstly intro-
duced in 2017 [39], is a neural network architecture solely based
on multi-head self attention mechanism. In PalmTree, transformer
units are connected bidirectionally and stacked into multiple layers.
Figure 2: Input Representation
We treat each instruction as a sentence and each token as a
word. Instructions from control flow and data flow sequences are
1: mov  rbp, rdi2: mov  ebx, 0x13: mov  rsi, rbp4: mov  rdx, rbx5: call  memcpy6: mov  [rcx+rbx], 0x07: mov  rcx, rax8: mov  [rax], 0x2e1326Instruction Pair Samplingmov rdx, rbxmov [rcx+rbx], 0x0[CLS]movrdxrbx[SEP]mov[rcx+rbx]0x0[SEP]Raw InstructionsDFGInstruction TokensInstruction Pair SamplingTokenizationAssembly Language Model24578136E[CLS]TrmE1ENTrmTrmTrmTrmTrmCT1TN............MLM: internal formatsMasked Language ModelE[CLS]TrmE1ENTrmTrmTrmTrmTrmCT1TN............CWP: contextual dependencyE[CLS]TrmE2ENTrmTrmTrmTrmTrmCT2TN............Def-Use PredictionDUP: data flow dependencyContextWindowData Flow Pairs2446Control Flow PairsContext Window PredictionTokenizationAnother nodenot in the segmentmov[CLS]ebx0x1[SEP]movrdxrbx[SEP]InputES1ES1ES1ES1ES1ES2ES2ES2ES2SegmentmovE0ebx0x1[SEP]movrdxrbx[SEP]PositionEmovE[CLS]Eebx0x1[SEP]movrdxrbx[SEP]TokenE8E7E6E5E4E3E2E1[SEP]rbxrdxmov[SEP]0x1E[SEP]ErbxErdxEmovE[SEP]E0x1Session 12A: Applications and Privacy of ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3240concatenated and then fed into the BERT model. As shown in Fig-
ure 2, the first token of this concatenated input is a special token –
[CLS], which is used to identify the start of a sequence. Secondly,
we use another token [SEP] to separate concatenated instructions.
Furthermore, we add position embedding and segment embedding
to token embedding, and use this mixed vector as the input of the
bi-directional transformer network, as shown in Figure 2. Position
embedding represents different positions in the input sequence,
while segment embedding distinguishes the first and second in-
structions. Position embedding and segment embedding will be
trained along with token embeddings. These two embeddings can
help dynamically adjust token embeddings according to their loca-
tions.
3.4.2 Training task 1: Masked Language Model. The first task we
use to pre-train PalmTree is Masked Language Model (MLM), which
was firstly introduced in BERT [9]. Here is an example shown
in Figure 3. Assuming that ti denotes a token and instruction
I = t1, t2, t3, ..., tn consists of a sequence of tokens. For a given
input instruction I, we first randomly select 15% of the tokens to
replace. For the chosen tokens, 80% are masked by [MASK] (mask-
out tokens), 10% are replaced with another token in the vocabulary
(corrupted tokens), and 10% of the chosen tokens are unchanged.
Then, the transformer encoder learns to predict the masked-out
and corrupted tokens, and outputs a probability for predicting a
particular token ti = [MASK] with a softmax layer located on the
top of the transformer network:
p( ˆti|I) =
K
exp(wi Θ(I)i)
k =1 exp(wk Θ(I)i)
where ˆti denotes the prediction of ti. Θ(I)i is the ith hidden vector
of the transformer network Θ in the last layer, when having I as
input. and wi is weight of label i. K is the number of possible
labels of token ti. The model is trained with the Cross Entropy loss
function:
LMLM = − 
log p(ˆt|I)
where m(I) denotes the set of tokens that are masked.
ti ∈m(I)
(1)
(2)
Figure 3: Masked Language Model (MLM)
Figure 3 shows an example. Given an instruction pair “mov ebx,
0x1; mov rdx, rbx”, we first add special tokens [CLS] and [SEP].
Then we randomly select some tokens for replacement. Here we
select ebx and rbx. The token ebx is replaced by the [MASK] token
(the yellow box). The token rbx is replaced by the token jz (another
token in the vocabulary, the red box). Next, we feed this modified
instruction pair into the PalmTree model. The model will make a
prediction for each token. Here we care about the predictions of
the yellow and red boxes, which are the green boxes in Figure 3.
Only the predictions of those two special tokens are considered in
calculating the loss function.
3.4.3 Training task 2: Context Window Prediction. We use this train-
ing task to capture control flow information. Many downstream
tasks [5, 14, 40, 43] rely on the understanding of contextual rela-
tions of code sequences in functions or basic blocks. Instead of
predicting the whole following sentence (instruction) [18, 38], we
perform a binary classification to predict whether the two given
instructions co-occur within a context window or not, which makes
it a much easier task compared to the whole sentence prediction.
However, unlike natural language, control flows do not have strict
dependencies and ordering. As a result, strict Next Sentence Pre-
diction (NSP), firstly proposed by BERT [9], may not be suitable
for capturing contextual information of control flow. To tackle this
issue, we extend the context window, i.e., we treat each instruction
w steps before and w steps after the target instruction in the same
basic block as contextually related. w is the context windows size.
In Section C.3, we evaluate the performance of different context
window sizes, and pick w = 2 accordingly. Given an instruction I
and a candidate instruction Icand as input, the candidate instruction
can be located in the contextual window of I, or a negative sample
randomly selected from the dataset. ˆy denotes the prediction of
this model. The probability that the candidate instruction Icand is
a context instruction of I is defined as
1
p( ˆy|I , Icand) =
1 + exp(Θ(I ∥ Icand)cls)
(3)
where Icand ∈ C, and C is the candidate set including negative
and positive samples. Θcls is the first output of the transformer
network in the last layer. And “∥” means a concatenation of two
instructions. Suppose all instructions belongs to the training set D,
then the loss function is:
log p( ˆy|I , Icand)
(4)
LCW P = −
I ∈D
Figure 4: Context Window Prediction (CWP)
Here is an example in Figure 4. We use the input mentioned
above. We feed the unchanged instruction pairs into the PalmTree
model and pick the first output vector. We use this vector to predict
whether the input are located in the same context window or not.
In this case, the two instructions are next to each other. Therefore
the correct prediction would be “true”.
3.4.4 Training task 3: Def-Use Prediction. To further improve the
quality of our instruction embedding, we need not only control
flow information but also data dependency information across in-
structions. Sentence Ordering Prediction (SOP), first introduced
by Lan et al. [19], is a very suitable choice. This task can help the
PalmTree model to understand the data relation through DFGs,
and we call it Def-Use Prediction (DUP).
Given an instruction pair I1 and I2 as input. And we feed I1 ∥ I2
as a positive sample and I2 ∥ I1 as a negative sample. ˆy denotes the