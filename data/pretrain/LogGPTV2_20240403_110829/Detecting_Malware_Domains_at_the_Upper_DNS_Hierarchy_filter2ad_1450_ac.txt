cious activities.
3Domain names up to the LOW trustworthiness score, where LOW
trustworthiness score follows the deﬁnition by DNSWL [9]. More de-
tails can be found at http://www.dnswl.org/tech.
6
Discussion While none of the features used alone
may allow Kopis to accurately discriminate between
malware-related and legitimate domain names, by com-
bining the features described above we can achieve a
high detection rate with low false positives, as shown in
Section 5.
We would like to emphasize that the features com-
puted by Kopis, particularly the Requester Diversity and
Requester Proﬁle features, are novel and very differ-
ent from the statistical features proposed in Notos [3]
and Exposure [4], which are heavily based on IP repu-
tation information. Unlike Notos and Exposure, which
leverage RDNS-level DNS trafﬁc monitoring, Kopis ex-
tracts statistical features speciﬁcally chosen to harvest
the “malware signal” as seen from the upper DNS hi-
erarchy, and to cope with the coarser granularity of the
DNS trafﬁc observed at the AuthNS and TLD level. Fur-
thermore, we show in Section 5 that, unlike previous
work, Kopis is able to detect malware-related domains
even when no IP reputation information is available.
The Requester Diversity and Requester Proﬁle fea-
tures can operate without any historical IP address rep-
utation information. These two sets of features can be
computed practically and on-the-ﬂy at each authoritative
or TLD server. The main reason why we identify the
six Resolved-IP Reputation features is to harvest part of
the already established IP reputation in IPv4. This will
help the overall system to reduce the false positives (FPs)
and at the same time maintain a very high true positives
(TPs). We will elaborate more in Section 5 on the differ-
ent operational modes of Kopis.
5 Evaluation
In this section, we report the results of our evaluation of
Kopis. First, we describe how we collected our datasets
and the related ground truth. We then present results re-
garding the detection accuracy of Kopis for authoritative
NS- and TLD-level deployments. Finally, we present a
case study regarding how Kopis was able to discover a
previously unknown DDoS botnet based in China.
5.1 Datasets
Our datasets were composed of the DNS trafﬁc obtained
from two major domain name registrars between the
dates of 01-01-2010 up until 08-31-2010 and a country
code top level domain (.ca) between the dates of 08-26-
2010 up until 10-18-2010. In the case of the two domain
name registrars we were also able to observe the answers
returned to the requester of each resolution. Therefore, it
is easy for us to identify the IP addresses for the A-type
of DNS query trafﬁc. In the case of the TLD we obtained
data only for 52 days and had to passively reconstruct the
Figure 5: General observations from the datasets. Plot
(i) shows the difference between the raw lookup volume
vs. the query tuples that Kopis uses over a period of 107
days. Plots (ii), (iii) and (iv) show the number of unique
CCs, ASs and CIDRs (in which the RDNSs resides) for
each domain name that was looked up during one day.
IP addresses corresponding to the A-type of lookups
observed.
An interesting problem arises when we work with the
large data volume from major authorities and the .ca
TLD servers. According to a sample monitoring pe-
riod of 107 days we can see from Figure 5 (i) that the
daily number of lookups to the authorities was on aver-
age 321 million. This was a signiﬁcant problem since
it would be hard to process such a volume of raw data,
especially if the temporal information from these daily
observations were important for the ﬁnal detection pro-
cess. On the same set of raw data we used a data reduc-
tion process that maintained only the query tuples (as de-
ﬁned in Section 4). This reduced the daily observations,
as we can observe from Figure 5 (i), to a daily average
of 12,583,723 unique query tuples. The signal that we
missed with this reduction was the absolute lookup vol-
ume of each query tuple in the raw data. Additionally, we
missed all time sensitive information regarding the peri-
ods within a day that each query tuple was looked up. As
we will see in the following sections, this reduction does
not affect Kopis’ ability to model the proﬁle of benign
and malware-related domains.
Figures 5 (ii), (iii) and (iv) report the number of
CIDR (i.e., BGP preﬁxes), Autonomous Systems (AS),
Country Code (CC), respectively, for the RDNSs (or re-
questers) that looked up each domain name every day.
The domains are sorted based on counts of ASs, CCs
and CIDRs corresponding to the RDNSs that look them
up (from left to right with the leftmost having the largest
count). We observe that roughly the ﬁrst 100,000 do-
7
main names were the only domains that exhibit any di-
versity among the requesters that looked them up. We
can also observe that the ﬁrst 10,000 domain names are
those that have some signiﬁcant diversity. In particular
only the ﬁrst 10,000 domain names were looked up by
at least ﬁve CIDRs, or ﬁve ASs or two different CCs. In
other words, the remaining domains were looked up from
very few RDNSs, typically in small sets of networks and
a small number of countries. Using this observation we
created statistical vectors only for domain names in the
sets of the 100,000 most diverse domains from the point
of view of the RDNS’s CC, AS and CIDR.
5.2 Obtaining the Ground Truth
We collected more than eight months of DNS trafﬁc from
two DNS authorities and the .ca TLD. All query tuples
derived from these DNS authorities were stored daily and
indexed in a relational database. Due to some monitor-
ing problems we missed trafﬁc from 3 days in January, 9
days in March and 6 days in June 2010.
Some of our statistical features require us to map each
observed IP address to the related CIDR (or BGP preﬁx)
AS number and country code (Section 4). To this end,
we leveraged Team CYMRU’s IP-to-ASN mapping [28].
Kopis’ knowledge base contained malware informa-
tion from two malware feeds collected since March 2009.
We also collected public blacklisting information from
various publicly available services (e.g., Malwaredo-
mains [16], Zeus tracker [31]). Furthermore, we col-
lected information regarding domain names residing in
benign networks from DNSWL [9] but also the address
space from the top 30 Alexa [2] domains veriﬁed using
the assistance of the Dihe’s IP address index browser [8].
Overall, we were able to label 225,429 unique RRs that
correspond to 28,915 unique domain names. From those
we had 1,598 domain names labeled as legitimate and
27,317 domain names labeled as malware-related. All
collected information was placed in a table with ﬁrst and
last seen timestamps. This was important since we com-
puted all IPR features for day n based only on data we
had until day n. Finally, we should note that we labeled
all the data based on black-listing and white-listing in-
formation collected until October 31st 2010.
5.3 Model Selection
As described in Section 3, Kopis uses a machine learn-
ing algorithm to build a detector based on the statistical
proﬁles of resolution patterns of legitimate and malware-
related domains. As with any machine-learning task, it
is important to select the appropriate model and impor-
tant parameters. For Kopis, we need to identify the min-
imal observation window of historic data necessary for
Figure 6: ROCs from datasets with different sizes assem-
bled from different time windows.
training. The observation window here is the number of
epochs from which we assemble the training dataset.
In Figure 6, we see the detection results from four
different observation windows. The ROCs in Figure 6
were computed using 10-fold cross validation. The clas-
siﬁer that produced these results was a random forest
(RF) classiﬁer under a two, three, four and ﬁve day
training window. The selection of the RF classiﬁer was
made using a model selection process [10], a common
method used in the machine learning community, which
identiﬁed the most accurate classiﬁer that could model
our dataset. Besides the RF, during model selection we
also experimented with Naive Bayes, k-nearest neigh-
bors (IBK), Support Vector Machines, MLP Neural Net-
work and random committee (RC) classiﬁers [10]. The
best detection results reported during the model selection
were from the RF classiﬁer. Speciﬁcally, the RF classi-
ﬁer achieved a T Prate = 98.4% and a F Prate = 0.3%
using a ﬁve day observation window. When we increased
the observation window beyond the mark of ﬁve days we
did not see a signiﬁcant improvement in the detection re-
sults.
We should note that this parameter and model method-
ology should be used every time Kopis is being deployed
in a new AuthNS or TLD server because the character-
istics of the domains, and hence the resolution patterns,
may vary in different AuthNS and TLD servers, and dif-
ferent patterns or proﬁles may best ﬁt different parameter
values and classiﬁers.
5.4 Overall Detection Performance
In order to evaluate the detection performance of Kopis
and in particular the validity and strength of its statistical
features and classiﬁcation model, we conducted a long-
term experiment with ﬁve months of data. We used 150
8
of the T Prates and F Prates for the RF classiﬁer over
the entire evaluation period. The average, minimum and
maximum F Prates for the RF were 0.5% (8 domains),
0.2% (3 domains) and 1.1% (18 domains), respectively,
while the average, minimum and maximum T Prates
were 99.1% (27,072 domains), 98.1% (27.071 domains)
and 99.8% (27,262 domains), respectively. The RF clas-
siﬁer’s F Prates were almost consistently around 0.6% or
less. The T Prate of the RF classiﬁer, with the exception
of six days, was above 96% and typically in the range
of 98%. With the IBK classiﬁer being the exception, the
RF and RC classiﬁers had similar longterm detection ac-
curacy. This experiment showed that Kopis overall has
a very high T Prate and very low F Prate against all new
and previously unclassiﬁed malware-related domains.
As described in Section 4, we deﬁne three main types
of features. Next we show how Kopis would oper-
ate if trained on datasets assembled by features from
each family, ﬁrst separately and then combined. To de-
rive the results from the experiments, we used as in-
put the 150 datasets created in the previously described
longterm evaluation mode. Then, for each one of these
150 datasets, we isolated the features from the RD,
RP and IPR feature families into three additional types
of datasets.
In Figure 7 and Figure 8 we present the
longterm detection rates obtained using 10-fold cross
validation of these three different types of datasets. Ad-
ditionally, we present the detection results from:
• The combination of RP and RD features (RD+RP
Features).
• The combination of RD, RP and the features
from the IPR feature family that describe the Au-
tonomous System properties of the IP address that
each domain name d points at (RD+RP+IRP(AS)
Features).
• The detection results from the combination of all
features combined (All Features).
The longterm F Prates and T Prates in Figure 7 and
Figure 8 respectively, we show the detection accura-
cies from each different feature set. One may tend to
think that the IPR (IP reputation) features hold a signiﬁ-
cantly stronger classiﬁcation signal than the combination
of RD and RP features, mainly because there are many
resources that currently contribute to the quantiﬁcation
and improvement of IP reputation (i.e., spam block lists,
malware analysis, dynamic DNS reputation etc.). How-
ever, Figure 7 and Figure 8 show that with respect to both
the F Prates and T Prates, the combination of the RD and
RP sets of features performs almost equally to the IPR
features used in isolation from the remaining features.
At the same time, using all features performs much bet-
ter than using each single feature subset in isolation. This
Figure 7: The distribution of T Prate for combination of
features and features families in comparison with Kopis
observed detection accuracy.
Figure 8: The distribution of F Prate for combinations of
features and features families in comparison with Kopis
observed detection accuracy.
different datasets created over a period of 155 days (ﬁrst
15 days for bootstrap). These datasets were composed by
using a ﬁfteen-day sliding window with a one-day step
(i.e., two consecutive windows overlap by 14 days). We
then used 10-fold cross validation4 to obtain the F Prates
and T Prates from every dataset. We picked three clas-
siﬁcation algorithms, namely, RF, RC, and IBK, which
performed best in the model selection process (described
in Section 5.3) because we wanted to use their detection
rates during the long-term experiment.
In Figure 7 and Figure 8 we observe the distribution
4To avoid overﬁtting our dataset we report the evaluation results us-
ing 10-fold cross validation that implies that 90% of dataset is used for
training and 10% for testing — in each of the 10 folds. This technique
is known [14] to yield a fair estimation of classiﬁcation performance
over a dataset.
9
 0.86 0.88 0.9 0.92 0.94 0.96 0.98 1 0 20 40 60 80 100 120 140TP RateIPR FeaturesRD+RP FeaturesAll Features 0.86 0.88 0.9 0.92 0.94 0.96 0.98 1 0 20 40 60 80 100 120 140TP RateDaysRP FeaturesRD FeaturesRD+RP+AS(IPR) 0.005 0.01 0.015 0.02 0.025 0.03 0 20 40 60 80 100 120 140FP RateIPR FeaturesRD+RP FeaturesAll Features 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 20 40 60 80 100 120 140FP RateDaysRP FeaturesRD FeaturesRD+RP+AS(IPR)Figure 9: T Prates for different observation periods using
an 80/20 train/test dataset split.
Figure 10: F Prates for different observation periods us-
ing an 80/20 train/test dataset split.
shows that the combination of the RP and RD features
contribute signiﬁcantly to the overall classiﬁcation accu-
racy and can enable the correct classiﬁcation of domains
in environments where IP reputation is absent or in cases
where we cannot reliably compute IP reputation features
“on-the-ﬂy” (e.g., in some TLD-level deployments).
5.5 New and Previously Unclassiﬁed Do-
mains
While the experiments described in Section 5.4 showed
that Kopis can achieve very good overall detection accu-
racy, we also wanted to evaluate the “real-world value”
of Kopis, and in particular its ability to detect new and
previously unclassiﬁed malware domains. To this end,
we conducted a set of experiments in which we trained
Kopis based on one month of labeled data from which
we randomly excluded 20% of both benign and malware-
related domains (i.e., we assumed that we did not know
anything about these domain names during training).
This excluded 997 benign and 4,792 malware-related
unique, deduplicated domain names from the training
datasets. Then we used the next three weeks of data as
an evaluation dataset, which contained the domains ex-
cluded from the training set mentioned above, as well as
all other newly seen domain names. In other words, the
classiﬁcation model learned using the training data was
not provided with any knowledge whatsoever about the
domains in the evaluation dataset.