title:Geo-replicated storage with scalable deferred update replication
author:Daniele Sciascia and
Fernando Pedone
Geo-replicated storage with scalable deferred update replication
Daniele Sciascia
University of Lugano (USI)
Switzerland
Fernando Pedone
University of Lugano (USI)
Switzerland
Abstract—Many current online services are deployed over
geographically distributed sites (i.e., datacenters). Such dis-
tributed services call for geo-replicated storage, that is, storage
distributed and replicated among many sites. Geographical
distribution and replication can improve locality and avail-
ability of a service. Locality is achieved by moving data
closer to the users. High availability is attained by replicating
data in multiple servers and sites. This paper considers a
class of scalable replicated storage systems based on deferred
update replication with transactional properties. The paper
discusses different ways to deploy scalable deferred update
replication in geographically distributed systems, considers the
implications of these deployments on user-perceived latency,
and proposes solutions. Our results are substantiated by a series
of microbenchmarks and a social network application.
Keywords-Database replication, scalable data store, fault
tolerance, high performance, transactional systems
I. INTRODUCTION
that
Many current online services are deployed over geograph-
ically distributed sites (i.e., datacenters). Such distributed
services call for geo-replicated storage,
is, storage
distributed and replicated among many sites. Geographic
distribution and replication can improve locality and avail-
ability of a service. Locality is achieved by moving the data
closer to the users and is important because it improves user-
perceived latency. High availability is attained by deploying
the service in multiple replicas; it can be conﬁgured to
tolerate the crash of a few nodes within a datacenter or
the crash of multiple sites, possibly placed in different
geographical locations.
In this paper, we consider a class of scalable replicated
storage systems based on deferred update replication. De-
ferred update replication is a well-established approach (e.g.,
[1], [2], [3], [4]). The idea behind a scalable deferred
update replication (SDUR) protocol is conceptually simple:
the database is divided into partitions and each partition
is fully replicated by a group of servers [5]. To execute a
transaction, a client interacts with (at most) one server per
partition and there is no coordination among servers during
the execution of the transaction—essentially, the technique
relies on optimistic concurrency control [6]. When the client
wishes to commit the transaction, he atomically broadcasts
This work was supported in part by the Swiss National Science Founda-
tion under grant number 127352.
the transaction’s updates (and some meta data) to each par-
tition involved in the transaction. Atomic broadcast ensures
that servers in a partition deliver the updates in the same
order and can certify and possibly commit the transaction in
the same way. Certiﬁcation guarantees that the transaction
can be serialized with other concurrent transactions within
the partition. Transactions are globally serialized using a
two-phase commit-like protocol: servers in the involved
partitions exchange the outcome of certiﬁcation (i.e., the
partition’s vote) and if the transaction passes the certiﬁcation
test successfully at all partitions it is committed; otherwise
it is aborted.
transactions,
Scalable deferred update replication offers very good
performance, which under certain workloads grows propor-
tionally with the number of database partitions [5], but it is
oblivious to the geographical location of clients and servers.
While the actual location of clients and servers is irrelevant
for the correctness of SDUR, it has important consequences
on the latency perceived by the clients. SDUR distinguishes
between local transactions, those that access data in a single
partition, and global
those that access data
in multiple partitions. Intuitively, a local transaction will
experience lower latency than a global transaction since
it does not require the two-phase commit-like termination
needed by global transactions. Moreover, in a geographically
distributed environment, the latency gap between local and
global transactions is likely wider since the termination of
global transactions may involve servers in remote regions,
subject to longer communication delays. This is not the
case for local transactions whose partition servers are within
the same region. Applications can exploit these tradeoffs
by distributing and replicating data to improve locality and
maximize the use of local transactions.
Although local
transactions are “cheaper” than global
transactions when considered individually, in mixed work-
loads global transactions may hinder the latency advantage
of local transactions. This happens because within a par-
tition, the certiﬁcation and commitment of transactions is
serialized to ensure determinism, a property without which
replicas’ state would diverge. As a consequence, a local
transaction delivered after a global transaction will expe-
rience a longer delay than if executed in isolation. We have
assessed this phenomenon in a geographically distributed
environment and found that even a fairly low number of
978-1-4799-0181-4/13/$31.00 ©2013 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:54:41 UTC from IEEE Xplore.  Restrictions apply. 
global transactions in the workload is enough to increase
the average latency of local transactions by up to 10 times.
This paper makes the following contributions. First, it
revisits scalable deferred update replication and discusses
how it can be deployed in geographically distributed sys-
tems. Second, it experimentally assesses the performance
of these deployments, using Amazon’s elastic compute in-
frastructure, and quantiﬁes the problems mentioned above.
Third, it proposes two extensions to address the limitations
of the original protocol and presents a detailed experimen-
tal analysis of their effectiveness. Our experimental study
considers a series of microbenchmarks and a social network
application, prototypical of current online services deployed
over geographically distributed sites.
The remainder of the paper is structured as follows.
Section II presents our system model and some deﬁnitions.
Section III recalls the scalable deferred update replication
approach. Section IV discusses how to deploy SDUR in a
geographically distributed system, points out performance
issues with these deployments, and details solutions to the
problems. Section V describes our prototype and some
optimizations. Section VI evaluates the performance of the
protocol under different conditions. Section VII reviews
related work and Section VIII concludes the paper.
II. SYSTEM MODEL AND DEFINITIONS
In this section, we deﬁne our system model and introduce
some deﬁnitions used throughout the paper.
A. Processes and communication
We consider a distributed system composed of an un-
bounded set C = {c1, c2, ...} of client processes and a set
S = {s1, ..., sn} of server processes. Set S is divided into
P disjoint groups, S1, ..., SP . The system is asynchronous:
there is no bound on messages delays and on relative process
speeds. We assume the crash-stop failure model (e.g., no
Byzantine failures). A process, either client or server, that
never crashes is correct, otherwise it is faulty.
Processes communicate using either one-to-one or one-
to-many communication. One-to-one communication uses
primitives send(m) and receive(m), where m is a message.
Links are quasi-reliable: if both the sender and the receiver
are correct, then every message sent is eventually received.
One-to-many communication relies on atomic broadcast,
implemented within each group p. Atomic broadcast
is
deﬁned by primitives abcast(p, m) and adeliver(p, m) and
ensures two properties: (1) if message m is delivered by a
server in p, then every correct server in p eventually delivers
m; and (2) no two messages are delivered in different order
by their receivers.
While several atomic broadcast algorithms exist [7], we
use Paxos to implement atomic broadcast within a group
of servers [8]. Paxos requires a majority of correct servers
within a group and additional assumptions to ensure liveness,
notably a leader-election oracle at each group [8].
B. Databases, transactions and serializability
The database is a set D = {x1, x2, ...} of data items. Each
data item x is a tuple !k, v, ts", where k is a key, v its value,
and ts its version—we assume a multiversion database. The
database is divided into P partitions and each partition p
is replicated by servers in group Sp. Hereafter we assume
that atomic broadcast can be solved within each partition.
For brevity, we say that server s belongs to partition p
meaning that s ∈ Sp, and that p performs an action (e.g.,
sending a message) with the meaning that some server in p
performs the action. For each key k, we denote partition(k)
the partition to which k belongs.
A transaction is a sequence of read and write operations
on data items followed by a commit or an abort operation.
We represent a transaction t as a tuple !id , rs, ws" where id
is a unique identiﬁer for t, rs is the set of data items read
by t (readset(t)) and ws is the set of data items written
by t (writeset(t)). The set of items read or written by t is
denoted by Items(t). The readset of t contains the keys
of the items read by t; the writeset of t contains both
the keys and the values of the items updated by t. We
assume that transactions do not issue “blind writes”, that
is, before writing an item x, the transaction reads x. More
precisely, for any transaction t, writeset(t) ⊆ readset(t).
Transaction t is said to be local if there is a partition p such
that ∀(k, −) ∈ Items(t) : partition(k) = p. If t is not a
local transaction, then we say that t is global. The set of
partitions that contain items read or written by t is denoted
by partitions(t).
The isolation property is serializability: every concurrent
execution of committed transactions is equivalent to a serial
execution involving the same transactions [9].
III. SCALABLE DEFERRED UPDATE REPLICATION
Scalable deferred update replication (SDUR) is an ex-
tension of deferred update replication that accounts for
partitioned data. In this section, we recall how SDUR works.
A. Transaction execution
In SDUR, the lifetime of a transaction is divided in two
phases: (1) the execution phase and (2) the termination
phase. The execution phase starts when the client issues
the ﬁrst transaction operation and ﬁnishes when the client
requests to commit or abort the transaction, when the ter-
mination phase starts. The termination phase ﬁnishes when
the transaction is completed (i.e., committed or aborted).
During the execution phase of a transaction t, client c
submits each read operation of t to a server s in the partition
p that contains the item read. This assumes that clients are
aware of the partitioning scheme.1 When s receives a read
command for x from c, it returns the value of x and its cor-
responding version. The ﬁrst read determines the database
1Alternatively, a client can connect to a single server s and submit all its
read requests to s, which will then route them to the appropriate partition.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:54:41 UTC from IEEE Xplore.  Restrictions apply. 
snapshot at partition p the client will see upon executing
other read operations for t. Therefore, reads within a single
partition see a consistent view of the database. Transactions
that read from multiple partitions must either be certiﬁed at
termination to check the consistency of snapshots or request
a globally-consistent snapshot upon start; globally-consistent
snapshots, however, may observe an outdated database since
they are built asynchronously by servers. Write operations
are locally buffered by c and only propagated to the servers
during transaction termination.
Read-only transactions
a globally-
consistent snapshot and commit without certiﬁcation. Update
transactions must pass through the termination phase to
commit, as we describe next.
execute
against
B. Transaction termination
To request the commit of t, c atomically broadcasts to
each partition p accessed by t the subset of t’s readset and
writeset related to p, denoted readset(t)p and writset(t)p.
Client c uses one broadcast operation per partition—running
a system-wide atomic broadcast would result
in a non-
scalable architecture. Upon delivering t’s readset(t)p and
writeset(t)p, a server s in p certiﬁes t against transac-
tions delivered before t in p—since certiﬁcation within a
partition is deterministic, every server in p will reach the
same outcome for t. If t passes certiﬁcation, it becomes a
pending transaction in s; otherwise s aborts t. If t is a local
transaction, it will be committed after s applies its changes
to the database. If t is a global transaction, s will send the
outcome of certiﬁcation, the partition’s vote, to the servers
in partitions(t) and wait for the votes from partitions(t).
If each partition votes to commit t, s applies t’s updates to
the database (i.e., commit); otherwise s aborts t.
The certiﬁcation of a local transaction checks whether the
transaction can be serialized according to its delivery order.
If transactions ti and tj executed concurrently in partition p
and ti is delivered before tj , tj will pass certiﬁcation with
respect to ti if readset(tj)p∩writeset(ti)p = ∅. Logically, in
a serial execution where ti executed before tj , tj would see
any of ti’s updates. Since ti and tj executed concurrently,
certiﬁcation allows tj to commit only if tj did not read any
item updated by ti. This relatively simple certiﬁcation test is
possible thanks of the totally ordered delivery of transactions
within a partition, implemented by atomic broadcast.
The certiﬁcation of global transactions is more complex
due to the absence of total order across partitions. If ti
and tj are concurrent global transactions that read from
partitions px and py, it may happen that ti
is delivered
before tj at px and tj is delivered before ti at py. Simply
certifying that tj does not read any item updated by ti at
px and ti does not read any item updated by tj at py does
not ensure serializability.2 To enforce serializable executions
without system-wide total order, SDUR uses a more strict
certiﬁcation test for global transactions: If global transaction
ti executed concurrently with transaction tj
in partition
p, and tj is delivered before ti, ti will pass certiﬁcation
with respect to tj if readset(tj)p ∩ writeset(ti)p = ∅ and
readset(ti)p ∩ writeset(tj)p = ∅. Intuitively, this means that
if ti and tj pass certiﬁcation, they can be serialized in any
order—thus, it does not matter if ti is delivered before tj at
one partition and tj is delivered before ti at another partition.
IV. SCALABLE DEFERRED UPDATE REPLICATION IN
GEO-REPLICATED ENVIRONMENTS
SDUR is oblivious to the geographical location of clients
and servers. In this section, we revisit our system model
considering a geographically distributed environment, dis-
cuss possible deployments of SDUR in these settings, point
out performance issues with these deployments, and propose
solutions to overcome the problems.
A. A system model for geo-replication
We assume client and server processes grouped within
datacenters (i.e., sites) geographically distributed over dif-
ferent regions. Processes within the same datacenter and
within different datacenters in the same region experience
low-latency communication. Messages exchanged between
processes located in different regions are subject to larger
latencies. A partition replicated entirely in a datacenter can
tolerate the crash of some of its replicas. If replicas are
located in multiple datacenters within the same region, then
the partition can tolerate the crash of a whole site. Finally,
catastrophic failures (i.e., the failure of all datacenters within
a region) can be addressed with inter-region replication.
Replication across regions is mostly used for locality,
since storing data close to the clients avoids large delays
due to inter-region communication [10], [11]. We account
for client-data proximity by assuming that each database par-
tition p has a preferred server [10], denoted by pserver (p),
among the servers that contain replicas of p. Partition p can
be accessed by clients running at any region, but applications
can reduce transaction latency by carefully placing the
preferred server of a partition in the same region as the
partition’s main clients.
B. SDUR in geographically distributed systems
We now consider two deployments of SDUR in a geo-
graphically distributed system. The ﬁrst deployment (“WAN
1” in Figure 1) places a majority of the servers that replicate
a partition in the same region, possibly in different datacen-
ters. A local transaction executed against the preferred server
2To see why, let ti read x and write y and tj read y and write x, where
x and y are items in px and py, respectively. If ti is delivered before tj
at px and tj is delivered before ti at py, both pass certiﬁcation at px and
py, but they cannot be serialized.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:54:41 UTC from IEEE Xplore.  Restrictions apply. 
of partition P1 (s1 in the ﬁgure) will terminate in 4δ, where δ
is the maximum communication delay among servers in the
same region. A global transaction that accesses partitions
P1 and P2, executed against server s1, will be subject to
4δ + 2∆, where ∆ is maximum inter-region delay.
The second deployment (“WAN 2”) distributes the servers
of a partition across regions. This deployment can tolerate
catastrophic failures, as we discuss next. The termination of
a local transaction will experience 2δ + 2∆ since Paxos will
no longer run among servers in the same region. Global
transactions are more expensive than local
transactions,
requiring 3δ + 3∆ to terminate.3 In both deployments, a
global transaction that executes at P1 (respectively, P2) will
read items from P2 (P1) within 2δ.
Deployments one and two tolerate the failure of servers in
a partition as long as a majority of the servers is available in
the partition (see Section II). The ﬁrst deployment, however,
does not
tolerate the failure of all servers in a region,
since such an event would prevent atomic broadcast from
terminating in some partitions.
C. Performance considerations
In SDUR,
terminating transactions are totally ordered
within a partition. If ti is delivered before tj in partition
p, ti will be certiﬁed before tj . If ti and tj pass certiﬁcation
(in all concerned partitions), ti’s updates will be applied to
the database before tj ’s. While this mechanism guarantees
deterministic transaction termination, it has the undesirable
effect that tj may have its termination delayed by ti. This is
particularly problematic in SDUR if ti is a global transaction
and tj is a local transaction since global transactions may
take much longer to terminate than local transactions.
The consequences of global transactions on the latency
of local transactions depend on the difference between the
expected latency of local and global transactions. For exam-
ple, in WAN 1 local transactions are expected to terminate
much more quickly than global transactions, which is not
the case in WAN 2. Thus, global transactions can have a
more negative impact on local transactions in WAN 1 than in
WAN 2. We have assessed this phenomenon experimentally
(details in Section VI) and found that in WAN 1, global
transactions can increase the latency of local transactions
by up to 10 times. In the next section, we discuss two
techniques that reduce the effects of global transactions on
the latency of local transactions in SDUR.
D. Delaying transactions
In our example in the previous section, if tj is a local
transaction delivered after a global transaction ti at server
s, tj will only terminate after s has received votes from all
partitions in partitions(ti) and completed ti.
3Note that we do not place server s4 in Region 1 because this would
result in Region 2 having no preferred server.
We can reduce ti’s effects on tj as follows. When s
receives ti’s termination request (message 1 in Figure 1), s
forwards ti to the other partitions (message 2) but delays the
broadcast of ti at p by ∆ time units. Delaying the broadcast
of ti in p increases the chances that tj is delivered before ti
but does not guarantee that tj will not be delivered after ti.
Note that if ∆ is approximately the time needed to reach
a remote partition (message 2 in Figure 1), then delaying
the broadcast of ti at p by ∆ will not increase ti’s overall
latency.
E. Reordering transactions
The idea behind reordering is to allow a local transaction
tj to be certiﬁed and committed before a global transaction
ti even if ti is delivered before tj . This is challenging for
two reasons: First, when tj is delivered by some server s
in partition p, s may have already sent ti’s vote to other
partitions. Thus, reordering tj before ti must not invalidate
s’s vote for ti. For example, assume ti reads items x and y
and writes item y and s voted to commit ti. If tj updates
the value of x, then s cannot reorder tj before ti since that
would change s’s vote for ti from commit to abort. Second,
the decision to reorder transactions must be deterministic,
that is, if s decides to reorder tj , then every server in p
must reach the same decision.
We ensure that at partition p local transaction tj can
be reordered with previously delivered pending transactions
ti0 , ..., tiM using a reordering condition similar to the one
presented in [1], originally devised to reduce the abort rate
of concurrent transactions. In our context, we deﬁne that tj
can be serialized at position l if the following holds:
(a) ∀k, 0 ≤ k < l: writeset(tik ) ∩ readset(tj) = ∅ and
(b) ∀k, l ≤ k ≤ M : writeset(tj) ∩ readset(tik ) = ∅.
If there is a position l that satisﬁes the constraints above,
tj passes certiﬁcation and is “inserted” at position l, which
essentially means that it will become the l-th transaction
to be applied to the database, after transactions ti0 , ..., til−1
have completed. If more than one position meets the cri-
teria, servers choose the leftmost position that satisﬁes the
conditions above since that will minimize tj ’s delay.
Consider now an execution where ti is pending at server
s when tj is delivered and let ti read and write item x and
tj read and write item y. Thus, s can reorder tj before ti
in order to speed up tj ’s termination. At server s′, before tj
is delivered s′ receives all votes for ti and commits ti. The
result is that when s′ delivers tj , it will not reorder tj before
ti since ti is no longer a pending transaction at s′. Although
ti and tj modify different data items, servers must commit
them in the same order to avoid non-serializable executions.4
To guarantee deterministic reordering of transactions, we
introduce a reordering threshold of size k per pending
4For example, a transaction that reads x and y at s could observe that tj
commits before ti and another transaction that reads x and y at s′ could
observe that ti commits before tj .
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:54:41 UTC from IEEE Xplore.  Restrictions apply. 
Local transaction
termination
➀
➆ ➀
Global transaction
termination
Deployment
in WAN 1
➆
➂
➃
➅
➂
➃
➄
➅
Region 1
Deployment
in WAN 2
Region 1
Region 2
preferred sites for P1 and P2
➁
➂
➃
Region 2
Region 3
Region 2