响应延时：单路读写响应延时0.4ms，RDMA网络响应延时小于0.2ms；
●
二三异步：第三个数据副本异步完成，极大提升了延时的稳定性；
●
QoS流控：根据前台业务负载情况控制后台IO流量，保证写入性能；
●
快速Failover：存储集群单机failover优化为5秒，达到业界领先水平；
●
高可用部署：单集群四Rack部署，将数据可靠性提升到10个9。
●
同时，在数据库方面我们也做了大量优化，最重要的是降低计算节点和存储节点
的网络传输量，以此来降低网络延迟对于数据库性能的影响。第一是redo log sync
12 > 9年双11：互联网技术超级工程
优化，将数据库吞吐提升了100%。第二是由于盘古支持原子写功能，所以我们关闭
了数据库的Double Write Buffer，高压力下数据库吞吐提升20%，网络带宽节省了
100%。
双11数据库混部技术
容器化和存储计算分离，使得数据库无状态化，具备调度能力。在双11高峰，
通过将共享存储挂载到不同的计算集群（离线集群），实现数据库的快速弹性。
阿里新一代数据库技术
阿里最早是商业数据库，然后我们做去IOE，研发出阿里MySQL分支AliSQL
和分布式中间件TDDL。2016年，我们开始研发阿里新一代数据库技术，我们把它
命名为X-DB，X代表追求极限性能，挑战无限可能的含义。
阿里的业务场景对于数据库有很高的要求：
数据要可扩展；
●
持续可用、数据要强一致；
●
数据量大、重要程度高；
●
数据有明显的生命周期特性，冷热数据特点鲜明；
●
交易、库存，支付等业务，操作逻辑简单，要求高性能。
●
因此，定义新一代数据库就要包含几个重要特点：具备数据强一致、全球部署能
力；内置分布式、高性能、高可用能力；具备自动数据生命周期管理能力。
新智能  9年双11：互联网技术超级工程
X-DB核心技术之二：Batching & Pipelining。X-DB在事务提交时，必须保
证日志在数据库节点的多数派收到并提交，这是保证数据强一致基础，由于事务在
提交时必须需要跨网络，这一定会导致延时增加，要保证高延时下的吞吐是非常困
难的。Batching & Pipelining技术保证尽可能批量提交，数据可以乱序接收和确认，
最终保证日志顺序提交。可以在高延时的情况下，保持很高的吞吐能力。
新智能  9年双11：互联网技术超级工程
典型应用场景
同城跨AZ部署替代传统主备模式，我们把原来主备模式变成三节点，解决跨
AZ数据质量问题和高可用问题。跨AZ数据强一致，单AZ不可用数据零丢失、单
AZ不可用秒级切换、切换自封闭，无第三方组件。相对主备模式零成本增加。
跨Region部署，用更底层的数据库技术解决异地多活问题，三地六副本（主备
模式）降低为三地五副本（三地五节点四数据），对于业务来说，可以享受跨Region
新智能  9年双11：互联网技术超级工程
交易卖家库的性能瓶颈解决方案
随着双11交易量增长，近两年交易买家库和卖家库的同步延时一直比较大，导
致商户不能及时处理双11订单；且卖家库有大量复杂的查询，性能差。我们曾经通
过为大卖家设置独立队列、同步链路合并操作和卖家库限流等进行优化，但仍然没有
完全解决问题。
ESDB是基于ES打造的分布式文档数据库，我们在ElasticSearch的基础上，
支持了SQL接口，应用可以从MySQL无缝迁移到ESDB；针对大卖家，提供动态
二级散列功能，彻底解决了数据同步的性能瓶颈，而且ESDB还可以提供复杂的查
询能力。
数据库监控系统演进
数据库监控系统的技术挑战具体有以下四点：
1.海量数据：平均每秒1000万项监控指标，峰值1400万；
2.复杂的聚合逻辑：地域、机房、单元、业务集群、数据库主备等多维度数据聚合；
3.实时性要求高：监控盯屏需要立即看到上一秒的监控数值；
4.计算资源：占用尽可能少的资源进行采集和计算。
整个链路经历三代架构：第一代Agent + MySQL；第二代Agent + datahub +
分布式NoSQL；第三代Agent +实时计算引擎+ HiTSDB
新智能  9年双11：互联网技术超级工程
CloudDBA在今年双11也做了一些探索，通过对全量SQL以及监控数据的分
析，我们实现了SQL自动优化（慢SQL调优）、空间优化（无用表无用索引分析）、
访问模型优化（SQL和KV）和存储空间增长预测等功能。
展望明年双11
展望明年的双11，我总结了三个关键词：Higher，Faster，Smarter
Higher意味着更高的交易峰值，背后其实是更低成本的追求，用极致的弹性能
力支持更高的峰值，给用户最好的购物体验，希望有一天可以做到不限流。
Faster是我们技术人一直不变的追求，更快的应用系统、更快的数据库，更快
的存储，更快的硬件等等。天下武功，唯快不破。
Smarter是机器智能在双11中的应用，不管是数据库、调度、个性化推荐甚至
客服等方面，我们都希望机器智能可以得到更多的应用，产生更大的技术突破。
新智能  9年双11：互联网技术超级工程
的4.7亿每秒。
实时计算在阿里巴巴内部应用广泛。随着新经济体的发展，技术的革新和用户需
求的提升，人们越来越需要实时计算的能力，它的最大好处就是能够基于实时变化数
据更新大数据处理的状态和结果。接下来，举两个例子来阐释实时计算在阿里内部应
用的场景：
1.双11大屏
每年双11阿里都会聚合有价值的数据展现给媒体，GMV大屏是其中之一。整
个GMV大屏是非常典型的实时计算场景，每条交易数据经过聚合展现在大屏之上。
从DataBase写入一条数据开始，到数据实时处理写入HBase，最后展现在大屏之
上，整个过程的链路十分长。整个应用也存在着诸多挑战：
大屏展现需要秒级延迟，这需要实时计算延迟在亚秒级别
●
数据库产生的大量数据需要在一个Job中聚合完成
●