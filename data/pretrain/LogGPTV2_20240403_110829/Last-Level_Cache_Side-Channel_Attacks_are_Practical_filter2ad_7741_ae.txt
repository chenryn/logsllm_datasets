automated. In Section VII-H we present the results of
running the attack on the experimental platforms.
D. Collect cache-set activity trace patterns
The purpose of this step is to create trace patterns
for the cache-set activities during the victim multiplica-
tion operations. A trace pattern is a bit vector of length
m, the number of multiplications in an exponentiation.
Each bit describes whether a cache set is accessed or not
in the multiplication operation. In the absence of noise,
if a cache set is used for a multiplier, we expect the
trace pattern of the cache set to match the usage pattern
of that multiplier. However, noise is present, both due to
system activity and due to capture errors. Nevertheless,
we still expect the trace pattern of the cache set to be
similar to the usage pattern of the multiplier.
To collect trace patterns, we ﬁrst reuse the method
of Section VI-B to identify a cache set used for the
multiplication code. Next, we scan cache-set activities
for every cache set, each scan lasts for 35,000 time slots
(of 5,000 cycles each). For each scan, we simultane-
ously monitor the scanned cache set and the cache set
containing the multiplication code. Within each time
slot, we prime both cache sets, and at the end of the
616
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:06:03 UTC from IEEE Xplore.  Restrictions apply. 
time slot we probe for activity in both sets. We use
the information obtained from the cache set containing
the multiplication code to identify when the victim
performs the exponentiation and when the multiplication
operations start and end. We then check for activity
in the scanned cache set during these multiplications
to construct the trace patterns. We leave the detailed
description on how we handle noise in the cache set
containing multiplication code to Appendix A.
Note that because a decryption takes about 7,000
time slots, each scan of 35,000 time slots contains 4–
5 complete exponentiations. Therefore, we can get 4–5
samples of trace patterns in each scan. We refer readers
to Appendix B for a discussion of the number of trace
patterns we need to collect.
E. Filter out unexpected trace patterns
Since the size of the multipliers is much smaller than
the size of the LLC, most of the trace patterns created
during the scan do not correspond to cache sets used for
multipliers. To reduce the amount of data we process in
the next phase, we leverage the statistical knowledge of
the multiplier usage patterns to ﬁlter out trace patterns
that are unlikely to be a multiplier.
As discussed in Section VII-B, during the ﬁrst few
multiplication operations, g[0] is accessed twice and
other multipliers are accessed once in sequence. A
multiplier is expected to be used 10 times during the
subsequent exponentiation phase. Therefore, we can
discard trace patterns that show too little or too much
activity ( 20 multiplications). Furthermore, we
remove trace patterns that do not show activity within
the ﬁrst few multiplications.
F. Clustering trace patterns
In the absence of noise, the trace patterns for cache
sets used for the same multiplier are identical to the
multiplier usage pattern, so usage patterns of all cache
sets containing the same multiplier would be identical.
With moderate noise we cannot expect
them to be
identical, but they should at least be similar. We rely on
this similarity to group trace patterns for a multiplier
together, thereby converging towards the actual usage
of that multiplier in multiplication operations. Because
our attack collects enough trace patterns, we are highly
likely to capture multiple trace patterns for each multi-
plier.
To group similar trace patterns, we use a hierarchical
clustering algorithm [17] with the edit distance [25]
between the trace patterns as the measure of similarity.
The clustering algorithm is quadratic in the number of
trace patterns. Figure 9 demonstrates a cluster of trace
patterns, representing the usage pattern of a multiplier.
G. Identify corresponding multipliers for clusters
The ﬁnal step is recovering usage patterns of all the
pre-computed multipliers, which directly exposes the
exponent, as we discussed in Section VII-B. We ﬁnd that
identifying the clusters representing usage of multipliers
is fairly straightforward. According to the statistical
information on multiplier usage (Section VII-B), we
can easily locate those clusters that match expectations.
Therefore, we focus on error correction and on mapping
clusters to corresponding multipliers. Unlike previous
steps, which are automated, this step requires manual
processing.
To explain how we capture errors, we look at a
sample cluster in Figure 9, showing a cluster with
15 trace patterns. Each horizontal
line represents a
trace pattern of the multiplier cache set, indexed by
the multiplication sequence. The shaded areas are the
multiplication indices in which the scanned cache set
shows activity. The solid vertical lines show the ground-
truth activity, i.e., the usage pattern for the multiplier,
as obtained from the victim’s key. Red marks indicate
activity detected due to noise in the scanned cache line.
Because the noise is independent of the multiplication
activity, it can be easily identiﬁed by comparing all the
trace patterns in the cluster.
In the ﬁgure we can see another noise effect: the
further we advance in the exponentiation, the more the
trace patterns deviate from the ground truth. We believe
that this deviation is caused by short pauses in the victim
operation which result in our attack interpreting a single
multiplication operation as two separate multiplications.
While we do try to correct these (Appendix A), our ﬁx
is, evidently, not perfect.
For correcting errors, we process the trace patterns
from left to right. We re-align all the trace patterns in
a cluster based on their common positions that have
access to the corresponding multiplier. This removes
the spurious accesses as we progress through the trace
patterns.
Lastly, we assign trace patterns to pre-computed
multipliers, g[i], by comparing the patterns in the ﬁrst
few multiplications with the knowledge of the pre-
computation phase in Algorithm 4. For the cluster in
Figure 9, the trace patterns indicate that the ﬁrst multi-
plication operation of that multiplier occurs at the third
index. According to the sequence of multiplications
in the pre-computation stage, we conclude that
this
cluster represents usage patterns of multiplier g[1]. By
processing all clusters with the same technique, we
are able to identify usage patterns of all pre-computed
multipliers.
617
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:06:03 UTC from IEEE Xplore.  Restrictions apply. 
s
n
r
e
t
t
a
p
e
c
a
r
T
 15
 14
 13
 12
 11
 10
 9
 8
 7
 6
 5
 4
 3
 2
 1
 3  12
 78
 116
 154  172
 204
 273  289  305
 377  391
 492
Sequence of multiplications within the exponentiations
Fig. 9: A cluster of traces
H. Results
We test
the attack ﬁve times on each platform,
each time with a different key. In each attack,
the
victim runs in one VM, repeatedly executing the GnuPG
program, decrypting a short text ﬁle. The attacker runs
continuously in another VM, observing activity in the
shared LLC. Table II summarizes the results.
TABLE II: Results of attack on sliding window.
Online attack time
Ofﬂine analysis time
Manual processing time
Observed exponentiations
Interesting exponentiations
Average cluster size
Minimum cluster size
server
27m
60s
10m
79,900
1,035
20.4
12
desktop
12m
30s
10m
33,600
734
17.7
5
Most of the attack time is spent on the online attack,
collecting observations of cache sets. Due to the larger
cache size, we collect more observations on the server
platform. We ﬁlter out over 97% of the observations
because they do not match the expected activity of a
multiplier, leaving 700–1000 interesting traces, which
we pass to the clustering algorithm. This ofﬂine phase
takes less than a minute,
leaving us with a list of
clusters. The average cluster size is around 20 traces,
with a minimum of 5. In all test cases, we require about
10 minutes of an expert to do the manual processing of
the clusters to completely recover the actual usage of
multipliers and recover the key.
VIII. RELATED WORK
A. PRIME+PROBE
This technique has been used for attacks against
several processor caches, including the L1 data cache
[28, 31, 34, 47], L1 instruction cache [1, 2, 4] and the
branch prediction cache [3]. All these caches are core-
private, and the attacks exploit either hyper-threading or
time multiplexing of the core.
Zhang et al. [47] use PRIME+PROBE to implement
a cross-VM attack on the square-and-multiply imple-
mentation of GnuPG version 1.4.13. The attack relies
on exploiting a weakness in the Xen scheduler and on
having a non-zero probability of the spy and victim
time-sharing the same core. The attack requires six
hours of constant decryptions for collecting enough data
to break the key. In contrast, we use the LLC as an
attack vector, which is used by all cores, and do not
need to trick the scheduler to share a processor core,
resulting in a much faster attack.
B. LLC based covert channel
Percival [31] describes an L2 covert channel with
a capacity of 100 KiB/s, but does not explain how the
attack recovers the address mapping. Ristenpart et al.
[32] experiment with L2 covert channels in a cloud
environment, achieving a bandwidth of about 0.2 b/s.
Xu et al. [43] extend this attack, reporting an L2-based
channel with a capacity of 233 b/s. By focusing on a
small group of cache sets, rather than probing the whole
cache, Wu et al. [42] achieve a transfer rate of over
190 Kb/s. By accurately mapping the cache sets, our
attack achieves a much higher bandwidth (up to 1.2
Mb/s) than prior work.
C. LLC based side channel attacks
Due to the low channel capacity, an LLC-based side
channel typically only leaks course-grain information.
For example, the attacks of Ristenpart et al. [32] leak in-
formation about co-residency, trafﬁc rates and keystroke
timing. Zhang et al. [46] use an L2 side channel to detect
non-cooperating co-resident VMs. Our attack improves
on this work by achieving a high granularity that enables
leaking of cryptographic keys.
Yarom and Falkner [45] show that when attacker and
victim share memory, e.g. shared libraries, the technique
of Gullasch et al. [16] can achieve an efﬁcient cross-
VM, cross-core, LLC attack. The same technique has
been used in other scenarios [5, 22, 35, 44, 48]. Our
attack removes the requirement for sharing memory, and
is powerful enough to recover the key from the latest
GnuPG crypto software which uses the more advanced
618
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:06:03 UTC from IEEE Xplore.  Restrictions apply. 
sliding window technique for modular exponentiation,
which is impossible using FLUSH+RELOAD attacks.
In concurrent work Irazoqui et al. [23] describes the
use of large pages for mounting a synchronous LLC
PRIME+PROBE attack against the last round of AES.
IX. MITIGATION
A. Fixing GnuPG
One approach to ﬁxing GnuPG for preventing in-
formation leaks is exponent blinding, which splits the
exponent
into two parts. Modular exponentiation is
performed on each part, followed by combining the
results [9].
An alternative is a constant-time implementation that
does not contain any conditional statements or secret-
dependent memory references. Techniques for constant-
time implementations have been explored, for example,
in Bernstein et al. [6]. These approaches can be tricky
to get right, and recent work has demonstrated that
a “constant-time” implementation of OpenSSL is still
susceptible to timing attacks at
least on the ARM
architecture [10].
When reporting the vulnerability to the GnuPG
team, we also provided a patch, which changes the
exponentiation algorithm to a ﬁxed-window exponen-
tiation, and ensures that the access patterns to the mul-
tipliers do not depend on exponent bits. This measure
falls short of a constant-time implementation because
the implementation of the multiplication and modular
reduction are not constant time. While it is possible to
leak information from these implementations [12, 13],
we are not aware of a micro-architectural attack that can
exploit this weakness.
B. Avoiding resource contention
While ﬁxing GnuPG is clearly desirable, this does
not address the general issue of maintaining isolation
and preventing information leaks in a multi-tenant en-
vironment. Since the root cause of LLC attacks is
resource contention, the most effective countermeasure
is to eliminate the resource contention. This can be
achieved with different granularity.
1) Avoid co-residency: This is the coarse-grained
partitioning of the resource: simply disallowing VMs
from different tenants to be hosted on the same pro-
cessor package, which prevents sharing of the LLC
among the attacker and the victim VMs. However, this
approach is fundamentally at odds with the core moti-
vation of cloud computing: reducing cost by increasing
resource utilization through sharing. Given the steady
increase in core counts, the economics will shift further
in favor of sharing.
2) Cache partitioning: Cache partitioning is a form
of ﬁne-grained resource partitioning. There are several
approaches to partition the cache.
One approach is to partition the cache by sets. This
can be achieved through page coloring, where frames of
different color are guaranteed to map to different cache
sets [26]. The VMM can manage the allocation of host-
physical memory so that VMs from different tenants are
mapped to frames of disjoint colors. Coloring frames
complicates the VMM’s resource management and leads
to memory wastage due to fragmentation. It is also
incompatible with the use of large pages, and thus
foregoes their performance beneﬁts.
STEALTHMEM [24] proposes a smarter way to uti-
lize the page coloring technology by only reserving very
few colors, known as stealth pages, for each physical
core to store the security-sensitive code and data. It
ensures that the security-sensitive code and data will not
have cache conﬂicts with other code and data. However,
this approach does not eliminate the LLC-based covert
channel.
The latest Intel processors provide cache allocation
technology (CAT), which partitions the cache by ways
[21]. CAT deﬁnes several classes of service (COS), and
each COS can be allocated a subset of ways in each
cache set. This can be used to partition the LLC between
COSes. The architecture presently supports up to four