measure the degree centrality of the nodes in the aforemen-
tioned graph, with pruning (Figure 4d) and without pruning
(Figure 4c). As we can see, the degree of nodes increases
signiﬁcantly after node deletions without pruning. Low degree
centrality is desirable in advanced persistent attacks (APT).
Additionally, it decreases the chances of detection and take
down, because of maintaining a low proﬁle and avoiding to
raise the alarm. For example, Stuxnet only infected maximum
of three other nodes [59], to slow down its spread and avoiding
detection.
To better understand the effect of size, we simulate a small
botnet of size 5000 [57] and a medium botnet of size 15000.
Figure 5 depicts the aforementioned metrics. As we can see in
Figures 5a and 5b, the self-repairing graph remains connected
even when a large portion (90%-95%) of the nodes are deleted,
compared to a normal graph (a graph with no self-repairing
mechanism). Note that, in a normal graph after 60% node
deletion, the number of partitions increases sharply. As we
can see in Figures 5c and 5d, the degree centrality slightly
increases in the DDSR compared to a normal graph, since
the healing process ensures that the degree of the nodes stays
within a speciﬁed range. However, as we remove the nodes
in a normal graph, the diameter increases until the graph is
partitioned, where the diameter is inﬁnite. In OnionBot, as
the nodes are deleted and the number of nodes decreases, the
diameter of the graph also decreases accordingly (Figures 5e
and 5f).
VI. MITIGATION OF BASIC ONIONBOTS
In this section, we look at different mitigation strategies
against OnionBots. Mitigation and detection can take place at
different levels, such as host level or network level. Host level
remediation techniques include the use of anti-virus software
and frameworks such as the Security Behavior Observatory
(SBO) [60]. Because of the scaling limitation of such tech-
niques, and the fact that the compromised hosts are rarely
updated or patched, we focus on the network level strategies.
Many of the current detection and mitigation mechanisms
are IP-based, and rely on the network trafﬁc patterns or DNS
queries to distinguish legitimate trafﬁc from malicious trafﬁc.
However, current solutions do not work with OnionBots, since
7575
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:50:16 UTC from IEEE Xplore.  Restrictions apply. 
(a)
(b)
(c)
(d)
(e)
(f)
Fig. 5: Graphs depicting the number of connected components, average degree centrality, and graph diameter, after incremental
node deletions, in a 10-regular graph of 5000 (left side) and 15000 (right side) nodes.
map these addressees to their corresponding IP addresses and
take down the infected hosts. Since the proposed construction
offers a self-repairing low-degree, low-diameter network, even
after taking over a large portion of the bots, the botnet remains
functional. As Figure 6 shows, an adversary needs to take down
about 40% of the bots simultaneously, to even partition the
network into two subgraphs. Note that, it means there is not
enough time for the graph to self-repair. As we can see, the
conventional solutions that ignore the privacy infrastructure
construction of OnionBots are not effective. Therefore, we
need to adapt our detection and mitigation methods, and
integrate them into the foundation of such infrastructures. In
this section we divide the network level mitigations into two
categories; techniques that are generic to Tor, and schemes
that are speciﬁc to OnionBots. In particular, we propose a
new OnionBot speciﬁc mitigation method, called Sybil Onion
Attack Protocol (SOAP).
A. Targeting OnionBots Through Privacy Infrastructures
Generic mitigations targeting Tor are based on denying
access to the bots through the HSDirs. As described be-
fore,
the list of HSDirs can be calculated by any en-
tity who knows the .onion address (in case there is no
descriptor-cookie). Hence, an adversary can inject her
relay into the Tor network such that it becomes the relay re-
sponsible for storing the bot’s descriptors. Since the ﬁngerprint
of relays is calculated from their public keys, this translates
Fig. 6: Partitioning of graphs after removing nodes. The
network becomes partitioned after removing on average about
40% of the nodes, in 10-regular graphs of size n=1000, to
n=15000.
the Tor trafﬁc is encrypted, non IP-based, and there are no
conventional DNS queries to resolve the .onion addresses.
Furthermore, even if an adversary captures a bot sample (e.g.,
by using honeypots or other similar techniques), and recovers
the .onion address of its peers, he is still unable to directly
7676
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:50:16 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 7: Soaping attack: node T is under attack by the compromised node C and its clones. In each step one of the clones
initiates the peering process with the T, until it is contained. After several iterations, the network is partitioned, and the botnet
is neutralized.
into ﬁnding the right public key [8]. Nevertheless, it should be
noted that an adversary needs to position herself at the right
position in the ring at least 25 hours before (it takes 25 hours
to get the HSDir ﬂag). It is difﬁcult to mitigate against many
bots, since the adversary requires the computation power, and
a prior knowledge of the .onion addresses. Furthermore, it
disrupts the operation and user experience of the Tor network.
A more long term approach involves making changes to the
Tor, such as use of CAPTACHAs, throttling entry guards and
reusing failed partial circuits, as described in [61]. Having said
that, these mitigations are limited in their preventive power,
open the door to censorship, degrade Tor’s user experience, and
are not effective against advanced botnets such as OnionBot.
B. Sybil Onion Attack Protocol (SOAP)
We devised a mitigation mechanism that uses OnionBots’
very own capabilities (e.g., the decoupling of IP address and
the host) against them. We ﬁrst overview the attack here, and
then provide a step by step explanation as depicted in Figure 7.
To attack the botnet and neutralize it, we ﬁrst need to ﬁnd the
bots’ .onion addresses. This can be done either by detecting
and reverse engineering an already infected host, or by using
a set of honeypots. Although this is not a trivial task, and
requires a signiﬁcant amount of effort, it allows us to inﬁltrate
the botnet, traverse its network, and identify the other bots.
After identifying the bots’ .onion address, we run many
hidden services, disclosing a subset of these as neighbors to
each peer we encounter, so gradually over time our clone nodes
dominate the neighborhood of each bot and contain it. Note
that, we can run all of the clones on the same machine because
of the decoupling between the IP address and the host.
Figure 7 depicts the soaping attack in different steps.
Node T is the target of the soaping attack, nodes Ni, are its
neighboring bot nodes, and nodes C are the adversary (e.g.,
the authorities), and his clones, which are represented with
small black circles. In step 1, the botnet is operating normally,
and none of T ’s neighbors are compromised. In step 2, one
of its peers, N4, is compromised. Then, N4 (now depicted as
C), makes a set of clones (the small black circles). In step
3, a subset of C’s clones, start the peering process with T ,
and declare their degree to be a small random number, which
changes to avoid detection (e.g., d=2). Doing so increases
the chances of being accepted as a new peer, and replacing
an existing peer of T . In step 4, T forgets about one of its
neighbors with the highest degree, N3, and peers with one the
7777
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:50:16 UTC from IEEE Xplore.  Restrictions apply. 
clones. The clones repeat this process until T has no more
benign neighbors (steps 5-8). As a result, T is surrounded by
clones and is contained (step 9). As we can see after many
iterations the adversary can partition the network into a set of
contained nodes, and neutralizing the botnet.
VII. RELATED WORK
In this section, we look at other work that examine alterna-
tive botnet constructions. However, they still rely on traditional
models, which makes them vulnerable to the current detection
and mitigation techniques, once their design is known.
Kartaltepe et al. [62],
investigate a new generation of
botnets that use online social networks, such as Twitter as their
C&C infrastructure. An instance of such malware, Naz, gets
its commands by making GET requests to the RSS feed of its
botmaster on Twitter. The tweets contain the base64 encoding
of shortened URLs (e.g., bit.ly) that redirect the bot to the
compressed malicious payload.
Nagaraja et al. [63], propose Stegobot, a botnet
that
communicates over probabilistic communication channels. It
spreads via social networks, and steals information from its
victims.
Stranerger et al. [52], introduce a botnet communication
protocol, called Overbot. Their design leverages Kademila
peer-to-peer protocol, a distributed hash table (DHT) used by
many peer-to-peer applications. They investigate the possibil-
ities of using the existing protocol to design stealth C&C
channels. The bot uses the 160-bit hash values in a search
request to announce it’s sequence number, which is encrypted
with the public key of the botmaster. Later, this sequence
number is used to send commands to the bot.
Nappa et al. [64], propose a parasitic botnet protocol
that exploits Skype’s overlay network. Skype provides a
widespread resilient network with a large install base for C&C
infrastructure. The communications between the master and
the bots are encrypted using adhoc schemes. The protocol
broadcasts messages to all peers in the network, similar to
the algorithms used in Gnutella. Once each peer receives a
new message it passes it to all of its neighbors.
Vogt et al. [65], examine the possibility of creating a super-
botnet by splitting a large botnet into many smaller botnets.
Each smaller botnet in the super-botnet network, stores some
routing information on how to communicate with the rest of the
network. They use a tree-structured infection process, where
each new zombie learns how many additional host it should
infect and add to its botnet. This design results in a connected
graph, with many densely connected cliques.
Lee and Kim [66], explore the design and mitigation
of botnets that use URL shortening services (USSes) for
alias ﬂuxing. A botmaster uses the USSes to hide and
obfuscate IP address of the C&C by using a dictionary
of 256 words for each part of an IPv4. For example,
10.15.43.89 can be mapped to “Brown.Fox.Jumps.Over.” Then
this expression is transformed into a search query, such
as google.com/q?=Brown+Fox+Jumps+Over. Using the URL
shortening service, bots can ﬁnd the corresponding IP address
by using the same dictionary.
Wang et al. [56], design a hybrid peer-to-peer botnet,
which is composed of servant and client bots. Their botnet
communicates with a ﬁxed number of peers contained in each
bot to limit the node exposure. The botmaster can control,
monitor and update the bots by sending the messages through
servant bots, and getting the reports from a sensor host. These
messages are encrypted using individualized predeﬁned or
dynamically generated keys.
Xu et al. [67] study the use of DNS for C&C using two
communication modes to piggyback messages over the DNS
messages, codeword and tunneled. In the codeword mode,
the bot makes a query (e.g., codeword.example.com) and
the server replies with an appropriate answer (e.g., the IP
address of a victim for DoS attack). In the tunneled mode the
client encodes its data using a base32 encoding and sends a
CNAME query. After receiving the query, the server uses base32
encoding to construct the corresponding CNAME reply.
VIII. CONCLUSION
Privacy infrastructures such as Tor had a tremendous im-
pact on society, protecting users anonymity and rights to access
information in the face of censorship. It also opened the door
to abuse and illegal activities, such as ransomware [7], and a
marketplace for drugs and contraband [68], [6]. In this work we
envisioned OnionBots, and investigated the potential of sub-
verting privacy infrastructures (e.g., Tor hidden services) for
cyber attacks. We presented the design of a robust and stealthy
botnet that lives symbiotically within these infrastructures to
evade detection, measurement, scale estimation and observa-
tion. It is impossible for Internet Service Providers (ISP) to
effectively detect and mitigate such botnet, without blocking
all Tor access. Additionally, OnionBots rely on a resilient self-
healing network formation that is simple to implement, yet it
has desirable features such as low diameter and low degree.
Such botnets are robust to partitioning, even if a large fraction
of the bots are simultaneously taken down. In the scenario of
a gradual take down of nodes, the network is also able to self-
repair, even after up to 90% node deletions. More importantly,
we developed soaping, a novel mitigation attack that neutral-
izes the OnionBots. We also suggested mitigations that act at
the Tor level. There are still many challenges that need to be
preemptively addressed by the security community, such as
the byzantine behavior of OnionBots [69], [70]. We hope that
this work ignites new ideas to proactively design mitigations
against the new generations of crypto-based botnets.
REFERENCES
[1] R. A. Rodr´ıguez-G´omez, G. Maci´a-Fern´andez, and P. Garc´ıa-Teodoro,
“Survey and taxonomy of botnet research through life-cycle,” ACM
Computing Surveys (CSUR), vol. 45, no. 4, August 2013.
[2] S. S. Silva, R. M. Silva, R. C. Pinto, and R. M. Salles, “Botnets: A
-
inevitable
tor,”
survey,” Computer Networks, vol. 57, 2013.
“Know your enemy: Fast-ﬂux service networks,” http://www.honeynet.
org/papers/ff/, July 2007.
“The
with
the-inevitable-move-64-bit-zeus-enhanced-with-tor/,
2013.
“Chewbacca
ware,”
chewbacca-a-new-episode-of-tor-based-malware/, December 2013.
enhanced
http://securelist.com/blog/events/58184/
December
64-bit
move