## Questions & Help

I would like to use `mixed_precision` in my TensorFlow model and have found the `tf.keras.mixed_precision.experimental.Policy` class. I added the following line before loading the pre-trained BERT model:

```python
tf.keras.mixed_precision.experimental.set_policy("mixed_float16")
```

However, when I run the code, I encounter the following error:

```
InvalidArgumentError: cannot compute AddV2 as input #1 (zero-based) was expected to be a half tensor but is a float tensor [Op:AddV2] name: tf_bert_model_1/bert/embeddings/add/
```

This error occurs at the line where I build the network with dummy inputs:

```python
ret = model(model.dummy_inputs, training=False)
```

I am unsure if I am using `tf.keras.mixed_precision.experimental.set_policy` correctly. According to the TensorFlow documentation, policies can be passed to the `dtype` argument of layer constructors, or a global policy can be set with `tf.keras.mixed_precision.experimental.set_policy`. 

I would like to know if it is possible to use Automatic Mixed Precision (AMP) with TensorFlow-based transformer models and, if so, how to do it properly. Any help would be greatly appreciated.

[error.txt]