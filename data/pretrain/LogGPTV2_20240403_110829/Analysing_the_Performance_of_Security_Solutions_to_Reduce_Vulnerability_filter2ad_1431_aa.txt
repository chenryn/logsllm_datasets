title:Analysing the Performance of Security Solutions to Reduce Vulnerability
Exposure Window
author:Yolanta Beres and
Jonathan Griffin and
Simon Shiu and
Max Heitman and
David Markle and
Peter Ventura
2008 Annual Computer Security Applications Conference
2008 Annual Computer Security Applications Conference
Analysing the Performance of Security Solutions to Reduce Vulnerability 
Yolanta Beres, Jonathan Griffin, Simon Shiu 
Max Heitman, David Markle, Peter Ventura 
yolanta.beres, jonathan.griffin, simon.shiu 
max.heitman, david.markle, peter.ventura 
Exposure Window 
CITI 
@citi.com
HP Labs 
@hp.com 
Abstract 
In this paper we present a novel approach of using 
mathematical  models  and  stochastic  simulations  to 
guide  and  inform  security  investment  and  policy 
change  decisions. 
investigate 
vulnerability  management  policies,  and  explore  how 
effective  standard  patch  management  and  emergency 
escalation  based  policies  are,  and  how  they  can  be 
combined with earlier, pre-patch mitigation measures 
to reduce the potential exposure window. 
In  particular,  we 
The  paper  describes  the  model  we  constructed  to 
represent typical vulnerability  management  processes 
in  large  organizations,  which captures  the  external 
threat environment and the internal security processes 
and decision  points.  We  also  present  the  results  from 
the  experimental  simulations,  and  show  how  changes 
in security solutions and policies, such as speeding up 
patch  deployment  and  investing  in  early  mitigation 
measures, affect the overall exposure window in terms 
of the time it takes to reduce the potential risk.  
We believe that this type of mathematical modelling 
and  simulation-based  approach  provides  a  novel  and 
useful  way  of  considering 
investment 
decisions, which is quite distinct from traditional risk 
analysis.  
1. Introduction 
security 
security  operations 
typical 
organization  has  a  number  of  security  controls 
(policies)  such  as  patching,  antivirus,  client  side 
firewalls,  etc.,  that  work  together  in  minimizing  the 
exposure  of  the  organization’s  systems  to  security 
threats.  But  it  is  notoriously  difficult  to  evaluate  how 
well security processes are protecting an organization, 
and even harder to estimate in advance the impact of a 
change in security mechanism, investment choice or a 
change in policy. Examination of historical data would 
give only partial answers; e.g., it is certainly possible 
in  a 
team 
The 
1063-9527/08 $25.00 © 2008 IEEE
1063-9527/08 $25.00 © 2008 IEEE
DOI 10.1109/ACSAC.2008.42
DOI 10.1109/ACSAC.2008.42
23
33
is 
and 
team 
to track how long the deployment of a particular patch 
takes across a set of systems, but without the context of 
an  external  threat  environment,  the  historical  data 
(such  as  it  is)  cannot  help  determine  if  the  systems 
were  left  exposed  for  too  long  –  thus  yielding 
unacceptable risk.  
vulnerability 
One  of  the  main  tasks  faced  by  the  security 
operations 
patch 
management.    With  thousands  of  systems  running 
popular  business  operating  systems  such  as  Windows 
in  a  medium  to  large  size  organization,  all  of  these 
systems  may  potentially 
to  be 
deployed.    However,  deploying  patches  across  all  of 
these  systems  in  a  timely  manner  is  not  simple.    In 
addition to the time spent for the patch assessment and 
patch testing, the security operations team often faces 
restrictions placed by the business in terms of allowed 
system downtime and minimal business disruption.  
require  patches 
Overall,  the  security  operations  team  needs  to 
consider if the current practice in patch deployment is 
exposing  the  organization  to  an  unacceptable  risk  of 
potential  exploitation,  explore  the  improvements  that 
the 
can  be  made 
effectiveness  of  other  mitigations 
to 
patching.  From  what  we  have  observed,  beyond 
examining  historical  data,  security  operations  staff 
have  few  tools  to  help  them  understand  trade-offs  or 
test different vulnerability  mitigation strategies before 
putting them in practice.  
the  process,  and  evaluate 
in  addition 
In this paper we propose using a simulation-based, 
predictive  modelling  approach  to  help  explore  the 
space of outcomes for a range of patching policies and 
mitigation  mechanisms.  In  particular,  we  analyse  the 
effects of adjusting the patching policies based on the 
criticality of vulnerabilities and the likelihood of them 
being  exploitable.  We  additionally  investigate  the 
impact of the deployment of other mitigations that are 
signature  and  behaviour  based,  to  explore  to  what 
extent they help minimize the exposure window.  
in 
The  predictive  modeling  approach  provides  some 
advantages  over  a  purely  analytical  approach  as  it 
offers  more  opportunities  to  naturally  explore,  via 
experimental results, the dependencies among different 
decisions  in  the  patch  management  process  and  the 
impact  of  change  in  vulnerability  and  exploit  rates. 
This  type  of  approach  requires  first  constructing  a 
model of the dynamics of the operational environment, 
such  as  the  patching  process,  which  are  explored 
empirically  using  executable  models  (e.g.,  discrete 
event simulation) to derive probability distributions of 
potential  outcomes.  If  the  behaviour  of  the  model 
correctly matches the relevant behaviour characteristics 
of  the  underlying  environment,  we  are  able  to  draw 
inferences about the effects from experiments and thus 
provide support for adopting specific policies.  
Our  paper  is  organized  as  follows.  Section  2 
reviews related  work in this  area. Section 3 describes 
our  modeling  approach  and  the  advantages  of  using 
executable  models  in  the  form  of  discrete  event 
simulations.  In  section  4,  we  present  how  we  have 
constructed  the  model,  capturing  the  specific  tasks 
undertaken  by  security  operations  and  the  speed  at 
which  these  tasks  are  progressed.  Section  4  also 
describes  how  we  selected  the  parameters  from  the 
external environment for use in our model. In section 5 
we  describe  our  design  of  a  set  of  simulation 
experiments using the constructed model together with 
detailed analysis of the results obtained. The first half 
of  our  analysis  aims  to  determine  if  the  standard 
patching  policy  used  by  many  security  operations 
centers are effective at reducing vulnerability exposure 
to  an  acceptable  level.  The  second  half  evaluates  the 
impact  of  including  additional  mitigations  which  can 
be put into effect earlier, and also the impact of faster 
patch  deployment. 
  Our  paper  finishes  with  a 
discussion of the implications of our analysis for future 
work and presents some final conclusions. 
2. Related work 
In  recent  years  there  has  been  a  lot  of  research 
interest  in  the  topic  of  security  economics,  exploring 
the trade-offs between costs of security and loss from 
exposure.  Anderson [1] argued that security problems 
should  be  viewed  from  an  economic  view  point  in 
addition to the technical challenges. Gordon and Loeb 
[7,  8]  examined  how  trade-offs  between  cost  and 
potential loss should be balanced when evaluating the 
optimal  investment  for  a  single  security  policy.  This 
economic viewpoint to security is critical for our work, 
as we aim to analyze both cost and exposure risk trade-
offs of different security policies.  
2434
in 
those  attacks.  Since 
Another  stream  of  research  relevant  to  our  work 
here is the analysis of the vulnerability life-cycle, and 
their  past  and  future  trends.  The  earlier  work  by 
McHugh  and  Arbaugh  [10]  introduces  a  life-cycle 
model  for  system  vulnerabilities.  Looking  at  CERT 
data  on  attacks  related  to  specific  vulnerabilities, 
McHugh  and  Arbaugh  noted  that  many  systems  were 
still  being  attacked  months  or  even  years  after  the 
patches  became  available  for 
the  vulnerabilities 
exploited 
then,  security 
operations  teams  in  many  organizations  streamlined 
their  patch  management  processes  so  that  fewer 
numbers  of  systems  remain  vulnerable  for  longer. 
However,  the  rate  at  which  new  vulnerabilities  are 
discovered and exploited is constantly increasing, thus 
requiring  constant  reassessment  of  existing  patching 
practices. The work by Frei et al [6] is important here 
for helping understand past and current trends of how 
vulnerabilities  have  been 
and  how 
mitigations  have  been  handled  by  software  vendors. 
This work looked at the generic set of vulnerabilities as 
disclosed  by  CERT,  Security  Focus,  and  others.  In 
building our model of the external threat environment, 
as described in section 4.2, we have used some of the 
distribution  functions  derived  by  the  authors  of  this 
paper,  such  as  for  exploit  availability  rate.  The  set  of 
vulnerabilities  that  large  organizations  care  about 
usually  represents  only  a  subset  of  all  the  known 
vulnerabilities  that  could  be  relevant.    Thus,  further 
analysis  is  necessary  to  assess  the  sensitivity  of  the 
distributions functions across the various sets. 
exploited 
There  has  also  been  some  work  examining  the 
trade-offs of different patching policies. Beattie, et al. 
[2] explores the factors affecting when it is best time to 
apply  a  patch  so 
that  organizations  minimize 
disruptions  caused  by  a  defective  patch.  In  contrast, 
rather than looking just at a single patch and adjusting 
a  single  point  in  time  to  start  patch  deployment,  we 
model the overall patch management process that also 
takes into account the time taken to apply the patches 
across  all  the  vulnerable  systems  in  an  organization; 
this can be considerable for a large organization.  
The  work  by  Zhang,  et  al.  [18]  provides  an 
analytical  framework  for  cost-benefit  analysis  of 
different patching policies. They assume that patching 
lead  time  (the  time  taken  to  apply  patch  across  the 
systems)  is  negligible  or  very  small  comparing  to  the 
overall  patching  lifecycle,  which  we  argue  is  not  the 
case in large organizations  with thousands of systems 
requiring the same patch. The authors also assume that 
the  costs  associated  with  vulnerability  exploitations 
can be estimated with relative ease by an organization, 
which in reality is very difficult to determine with any 
accuracy.  
oil, 
gas, 
steel, 
automobile, 
The simulation-based stochastic modeling approach 
has  had  little  application  in  the  computer  security 
world,  but  it  has  been  extensively  used  in  the 
aerospace, 
and 
telecommunications 
industries.  Recently  we  have 
applied this approach to also examine security policies 
for  USB  memory  sticks  [13],  and  in  earlier  work  we 
have examined resource contention issues such as staff 
utilization for network security operations tasks [14]. 
3. Probabilistic modelling approach 
With  the  stochastic  modeling  approach  we  seek  to 
model  the  system of interest  in order to answer some 
specific questions.  The modeling process usually starts 
with observations of the key stages and decision points 
of  the  processes  involved.  These  observations  induce 
the  definition  of  a  mathematical  process  model,  of 
which  properties  may  be  deduced.  The  mathematical 
consequences  of  the  definition  of  the  model  are 
interpreted  as  to  how  well  they  help  explain  reality 
and,  experimentally,  the  extent  of  their  validity  is 
observed. This cycle is repeated until a  model judged 
to  capture  enough  of  the  decision  making  situation  is 
obtained. 
Pictorially  the  way  that  our  models  are  built  and 
used is represented using activity diagrams. We found 
that in real life situations, it is important to have such a 
high  level  representation  of  the  model  as  it  helps 
discuss  the  models  with  the  specialists  who  run  the 
actual system.  
The  activity  diagrams  are  then  converted  into 
executable code, which can be used in the Monte-Carlo 
approach based discrete event simulations. In this stage 
either  extensions  to  general  purpose  programming 
language  can  be  used  to  capture  the  model  or 
specialized simulation tools.  
In  our  case  we  have  used  a  specialized  simulation 
oriented language Demos2K [3, 5], which implements 
a framework based on the mathematical foundations of 
a  synchronous  calculus  of  resources  and  processes, 
together with an associated modal logic [15]. Because 
of  these  strong  mathematical  foundations  and  sound 
semantics,  we  have  assurance  that  simulations  based 
on  the  models  developed  in  Demos2k  language  are 
robust and reliable – thus, meaningful observations can 
be 
repeated 
experimental  simulations  in  the  specially  developed 
experimental  environment  [11],  where  statistically 
significant information is gathered.  
taken.  The  code 
is  executed  via 
The  mathematical  framework  behind  Demos2K 
programming  language  revolves  around  four  key 
concepts:  resources,  capturing  the  essentially  static 
components  of  the  system;  processes,  capturing  the 
2535
dynamic components of the system; location, capturing 
the spatial distribution and connectivity of the system; 
and environment within which a system functions.  A 
full description of this mathematical framework can be 
found in [15]. 
the 
vulnerability 
4. 
Constructing 
management model 
on 
principles 
recommendations 
The  need  for  having  an  explicit  patching  and 
vulnerability  handling  policy, 
together  with  a 
systematic  process  for  handling  patches,  has  been 
recognized  for  almost  a  decade.  Both  the  early  ISO 
standard  17799:2000  [9]  and  NIST  publications  [12] 
provide 
and 
methodologies that can be used for accomplishing this. 
However,  the  actual  implementation  of  the  patch 
management  process  differs  from  one  organization  to 
another, often depending upon the resources that could 
be  dedicated  to  this  process,  the  number  of  systems 
that  the  organization  has,  and  its  risk  appetite.  To 
construct  a  model  of  the  patch  management  process, 
we  have  examined  patching  lifecycles  across  several 
large  organizations  with  production  environments 
typically of more than 100,000 systems.  
In this section we describe the different entities and 
environment parameters that influence the vulnerability 
patching process. There are two parts of the model in 
this  context:  (1)  the  external  environment  parameters, 
and  (2)  the  internal  workflows  and  decision  points  in 
the  vulnerability  assessment,  mitigation  and  patching 
process. Before we cover the model itself we will first 
describe  the  vulnerability  timeline  we  use,  as  this 
affects  how  we  examine  and  present  results  from 
simulations  based  on  the  model. The  results  from  the 
simulations  will  be  examined  in  section  5,  but  in 
designing the model that is presented in this section we 
had  two  main  questions  in  mind  that  we  wanted  to 
explore  via  experimental  runs:  (1)  how  good  are  the 
standard  patching  processes  at  minimizing 
the 
vulnerability exposure, in terms of the time it takes and 
the  percentage  of  systems  exposed  at  certain  time 
points;  (2)  what  impact  do  different  policy  decisions 
make,  such  as  introducing  shorter  patching  timelines, 
compared 
additional  mitigation 