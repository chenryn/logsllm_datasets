# Title: Analyzing the Performance of Security Solutions to Reduce Vulnerability Exposure Window

## Authors:
- Yolanta Beres
- Jonathan Griffin
- Simon Shiu
- Max Heitman
- David Markle
- Peter Ventura

## Affiliations:
- CITI: yolanta.beres, jonathan.griffin, simon.shiu @citi.com
- HP Labs: max.heitman, david.markle, peter.ventura @hp.com

## Abstract
In this paper, we present a novel approach that leverages mathematical models and stochastic simulations to guide and inform security investment and policy change decisions. We investigate vulnerability management policies, exploring the effectiveness of standard patch management and emergency escalation-based policies. Additionally, we examine how these policies can be combined with early, pre-patch mitigation measures to reduce the potential exposure window.

The paper describes a model we constructed to represent typical vulnerability management processes in large organizations, capturing both the external threat environment and internal security processes and decision points. We also present the results from experimental simulations, demonstrating how changes in security solutions and policies, such as accelerating patch deployment and investing in early mitigation measures, affect the overall exposure window in terms of the time it takes to reduce potential risk.

We believe that this type of mathematical modeling and simulation-based approach provides a novel and useful way of considering security investment decisions, distinct from traditional risk analysis.

## 1. Introduction
A typical organization employs various security controls, such as patching, antivirus, and client-side firewalls, to minimize the exposure of its systems to security threats. However, evaluating the effectiveness of these security processes is notoriously difficult, and estimating the impact of changes in security mechanisms, investment choices, or policies is even more challenging. Historical data alone offers only partial answers; for example, while it is possible to track the time taken to deploy a particular patch across a set of systems, without the context of the external threat environment, historical data cannot determine if the systems were left exposed for too long, thus yielding unacceptable risk.

One of the main tasks faced by security operations teams is patch management. In medium to large-sized organizations, thousands of systems running popular business operating systems like Windows may require patches. Deploying patches across all these systems in a timely manner is not simple, given the time required for patch assessment and testing, and the restrictions placed by the business regarding system downtime and minimal disruption.

Overall, the security operations team needs to consider whether current patch deployment practices expose the organization to an unacceptable risk of potential exploitation, explore improvements that can be made to the process, and evaluate the effectiveness of other mitigations in addition to patching. Beyond examining historical data, security operations staff have few tools to help them understand trade-offs or test different vulnerability mitigation strategies before implementing them.

In this paper, we propose a simulation-based, predictive modeling approach to explore the outcomes of various patching policies and mitigation mechanisms. Specifically, we analyze the effects of adjusting patching policies based on the criticality of vulnerabilities and the likelihood of their exploitation. We also investigate the impact of deploying additional signature and behavior-based mitigations to explore their effectiveness in minimizing the exposure window.

The predictive modeling approach offers several advantages over a purely analytical approach by providing more opportunities to naturally explore, via experimental results, the dependencies among different decisions in the patch management process and the impact of changes in vulnerability and exploit rates. This approach requires first constructing a model of the dynamics of the operational environment, which are explored empirically using executable models (e.g., discrete event simulation) to derive probability distributions of potential outcomes. If the model's behavior correctly matches the relevant characteristics of the underlying environment, we can draw inferences about the effects from experiments and provide support for adopting specific policies.

Our paper is organized as follows. Section 2 reviews related work in this area. Section 3 describes our modeling approach and the advantages of using executable models in the form of discrete event simulations. Section 4 details how we constructed the model, capturing the specific tasks undertaken by security operations and the speed at which these tasks are progressed. Section 5 presents the design of a set of simulation experiments using the constructed model, along with a detailed analysis of the results obtained. The first half of our analysis aims to determine if the standard patching policies used by many security operations centers are effective at reducing vulnerability exposure to an acceptable level. The second half evaluates the impact of including additional mitigations that can be put into effect earlier and the impact of faster patch deployment. Our paper concludes with a discussion of the implications of our analysis for future work and some final conclusions.

## 2. Related Work
In recent years, there has been significant research interest in the topic of security economics, exploring the trade-offs between the costs of security and the losses from exposure. Anderson [1] argued that security problems should be viewed from an economic perspective in addition to the technical challenges. Gordon and Loeb [7, 8] examined how trade-offs between cost and potential loss should be balanced when evaluating the optimal investment for a single security policy. This economic viewpoint is critical for our work, as we aim to analyze both cost and exposure risk trade-offs of different security policies.

Another stream of research relevant to our work is the analysis of the vulnerability life cycle and its past and future trends. McHugh and Arbaugh [10] introduced a life-cycle model for system vulnerabilities. By analyzing CERT data on attacks related to specific vulnerabilities, they noted that many systems were still being attacked months or even years after patches became available. Since then, security operations teams in many organizations have streamlined their patch management processes to reduce the number of vulnerable systems. However, the rate at which new vulnerabilities are discovered and exploited is constantly increasing, necessitating constant reassessment of existing patching practices. The work by Frei et al. [6] is important here for understanding past and current trends in how vulnerabilities have been exploited and how mitigations have been handled by software vendors. In building our model of the external threat environment, we have used some of the distribution functions derived by the authors of this paper, such as for exploit availability rates. Further analysis is necessary to assess the sensitivity of these distribution functions across various sets of vulnerabilities.

There has also been some work examining the trade-offs of different patching policies. Beattie et al. [2] explored the factors affecting the optimal time to apply a patch to minimize disruptions caused by defective patches. In contrast, we model the overall patch management process, taking into account the time required to apply patches across all vulnerable systems in an organization, which can be considerable for large organizations.

Zhang et al. [18] provided an analytical framework for cost-benefit analysis of different patching policies. They assumed that the patching lead time (the time taken to apply a patch across systems) is negligible or very small compared to the overall patching lifecycle, which we argue is not the case in large organizations with thousands of systems requiring the same patch. The authors also assumed that the costs associated with vulnerability exploitations can be estimated with relative ease, which is very difficult to determine accurately in reality.

The simulation-based stochastic modeling approach has had limited application in the computer security world but has been extensively used in aerospace, oil, gas, steel, automobile, and telecommunications industries. Recently, we have applied this approach to examine security policies for USB memory sticks [13], and in earlier work, we have examined resource contention issues such as staff utilization for network security operations tasks [14].

## 3. Probabilistic Modeling Approach
With the stochastic modeling approach, we seek to model the system of interest to answer specific questions. The modeling process typically starts with observations of the key stages and decision points involved. These observations induce the definition of a mathematical process model, whose properties can be deduced. The mathematical consequences of the model's definition are interpreted to explain reality, and experimentally, the extent of their validity is observed. This cycle is repeated until a model that captures enough of the decision-making situation is obtained.

Pictorially, our models are built and used with activity diagrams. In real-life situations, it is important to have such a high-level representation of the model to facilitate discussions with the specialists who run the actual system.

The activity diagrams are then converted into executable code, which can be used in Monte Carlo-based discrete event simulations. In this stage, either extensions to general-purpose programming languages or specialized simulation tools can be used. In our case, we have used a specialized simulation-oriented language, Demos2K [3, 5], which implements a framework based on the mathematical foundations of a synchronous calculus of resources and processes, together with an associated modal logic [15]. Due to these strong mathematical foundations and sound semantics, we have assurance that simulations based on the models developed in Demos2k are robust and reliable, allowing for meaningful observations to be taken. The code is executed via repeated experimental simulations in a specially developed experimental environment [11], where statistically significant information is gathered.

The mathematical framework behind Demos2K revolves around four key concepts: resources, capturing the essentially static components of the system; processes, capturing the dynamic components; location, capturing the spatial distribution and connectivity of the system; and environment, within which the system functions. A full description of this mathematical framework can be found in [15].

## 4. Constructing the Vulnerability Management Model
The need for an explicit patching and vulnerability handling policy, along with a systematic process for handling patches, has been recognized for almost a decade. Both the early ISO standard 17799:2000 [9] and NIST publications [12] provide principles and methodologies for accomplishing this. However, the actual implementation of the patch management process varies from one organization to another, often depending on the resources dedicated to the process, the number of systems, and the organization's risk appetite. To construct a model of the patch management process, we have examined patching lifecycles across several large organizations with production environments typically comprising more than 100,000 systems.

In this section, we describe the different entities and environment parameters that influence the vulnerability patching process. There are two parts of the model in this context: (1) the external environment parameters, and (2) the internal workflows and decision points in the vulnerability assessment, mitigation, and patching process. Before covering the model itself, we will first describe the vulnerability timeline we use, as this affects how we examine and present results from simulations based on the model. The results from the simulations will be examined in Section 5, but in designing the model presented in this section, we had two main questions in mind that we wanted to explore via experimental runs: (1) how effective are standard patching processes at minimizing vulnerability exposure in terms of the time it takes and the percentage of systems exposed at certain time points, and (2) what impact do different policy decisions make, such as introducing shorter patching timelines, compared to additional mitigation measures.