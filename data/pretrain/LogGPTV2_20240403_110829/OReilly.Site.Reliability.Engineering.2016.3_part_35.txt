cost of introducing another RPC and slight additional latency.
If caching has a significant effect on the service,5 you may want to use one or some of
the following strategies:
• Overprovision the service. It’s important to note the distinction between a latency
cache versus a capacity cache: when a latency cache is employed, the service can
sustain its expected load with an empty cache, but a service using a capacity
cache cannot sustain its expected load under an empty cache. Service owners
should be vigilant about adding caches to their service, and make sure that any
new caches are either latency caches or are sufficiently well engineered to safely
function as capacity caches. Sometimes caches are added to a service to improve
performance, but actually wind up being hard dependencies.
• Employ general cascading failure prevention techniques. In particular, servers
should reject requests when they’re overloaded or enter degraded modes, and
testing should be performed to see how the service behaves after events such as a
large restart.
• When adding load to a cluster, slowly increase the load. The initially small
request rate warms up the cache; once the cache is warm, more traffic can be
added. It’s a good idea to ensure that all clusters carry nominal load and that the
caches are kept warm.
Always Go Downward in the Stack
In the example Shakespeare service, the frontend talks to a backend, which in turn
talks to the storage layer. A problem that manifests in the storage layer can cause
problems for servers that talk to it, but fixing the storage layer will usually repair both
the backend and frontend layers.
However, suppose the backends cross-communicate amongst each other. For exam‐
ple, the backends might proxy requests to one another to change who owns a user
when the storage layer can’t service a request. This intra-layer communication can be
problematic for several reasons:
5 Sometimes you find that a meaningful proportion of your actual serving capacity is as a function of serving
from a cache, and if you lost access to that cache, you wouldn’t actually be able to serve that many queries. A
similar observation holds for latency: a cache can help you achieve latency goals (by lowering the average
response time when the query is servable from cache) that you possibly couldn’t meet without that cache.
Slow Startup and Cold Caching | 275
• The communication is susceptible to a distributed deadlock. Backends may use
the same thread pool to wait on RPCs sent to remote backends that are simulta‐
neously receiving requests from remote backends. Suppose backend A’s thread
pool is full. Backend B sends a request to backend A and uses a thread in backend
B until backend A’s thread pool clears. This behavior can cause the thread pool
saturation to spread.
• If intra-layer communication increases in response to some kind of failure or
heavy load condition (e.g., load rebalancing that is more active under high load),
intra-layer communication can quickly switch from a low to high intra-layer
request mode when the load increases enough.
For example, suppose a user has a primary backend and a predetermined hot
standby secondary backend in a different cluster that can take over the user. The
primary backend proxies requests to the secondary backend as a result of errors
from the lower layer or in response to heavy load on the master. If the entire sys‐
tem is overloaded, primary to secondary proxying will likely increase and add
even more load to the system, due to the additional cost of parsing and waiting
on the request to the secondary in the primary.
• Depending on the criticality of the cross-layer communication, bootstrapping the
system may become more complex.
It’s usually better to avoid intra-layer communication—i.e., possible cycles in the
communication path—in the user request path. Instead, have the client do the
communication. For example, if a frontend talks to a backend but guesses the
wrong backend, the backend should not proxy to the correct backend. Instead,
the backend should tell the frontend to retry its request on the correct backend.
Triggering Conditions for Cascading Failures
When a service is susceptible to cascading failures, there are several possible distur‐
bances that can initiate the domino effect. This section identifies some of the factors
that trigger cascading failures.
Process Death
Some server tasks may die, reducing the amount of available capacity. Tasks might die
because of a Query of Death (an RPC whose contents trigger a failure in the process),
cluster issues, assertion failures, or a number of other reasons. A very small event
(e.g., a couple of crashes or tasks rescheduled to other machines) may cause a service
on the brink of falling to break.
276 | Chapter 22: Addressing Cascading Failures
Process Updates
Pushing a new version of the binary or updating its configuration may initiate a cas‐
cading failure if a large number of tasks are affected simultaneously. To prevent this
scenario, either account for necessary capacity overhead when setting up the service’s
update infrastructure, or push off-peak. Dynamically adjusting the number of in-
flight task updates based on the volume of requests and available capacity may be a
workable approach.
New Rollouts
A new binary, configuration changes, or a change to the underlying infrastructure
stack can result in changes to request profiles, resource usage and limits, backends, or
a number of other system components that can trigger a cascading failure.
During a cascading failure, it’s usually wise to check for recent changes and consider
reverting them, particularly if those changes affected capacity or altered the request
profile.
Your service should implement some type of change logging, which can help quickly
identify recent changes.
Organic Growth
In many cases, a cascading failure isn’t triggered by a specific service change, but
because a growth in usage wasn’t accompanied by an adjustment to capacity.
Planned Changes, Drains, or Turndowns
If your service is multihomed, some of your capacity may be unavailable because of
maintenance or outages in a cluster. Similarly, one of the service’s critical dependen‐
cies may be drained, resulting in a reduction in capacity for the upstream service due
to drain dependencies, or an increase in latency due to having to send the requests to
a more distant cluster.
Request profile changes
A backend service may receive requests from different clusters because a frontend
service shifted its traffic due to load balancing configuration changes, changes in the
traffic mix, or cluster fullness. Also, the average cost to handle an individual payload
may have changed due to frontend code or configuration changes. Similarly, the data
handled by the service may have changed organically due to increased or differing
usage by existing users: for instance, both the number and size of images, per user, for
a photo storage service tend to increase over time.
Triggering Conditions for Cascading Failures | 277
Resource limits
Some cluster operating systems allow resource overcommitment. CPU is a fungible
resource; often, some machines have some amount of slack CPU available, which
provides a bit of a safety net against CPU spikes. The availability of this slack CPU
differs between cells, and also between machines within the cell.
Depending upon this slack CPU as your safety net is dangerous. Its availability is
entirely dependent on the behavior of the other jobs in the cluster, so it might sud‐
denly drop out at any time. For example, if a team starts a MapReduce that consumes
a lot of CPU and schedules on many machines, the aggregate amount of slack CPU
can suddenly decrease and trigger CPU starvation conditions for unrelated jobs.
When performing load tests, make sure that you remain within your committed
resource limits.
Testing for Cascading Failures
The specific ways in which a service will fail can be very hard to predict from first
principles. This section discusses testing strategies that can detect if services are sus‐
ceptible to cascading failures.
You should test your service to determine how it behaves under heavy load in order
to gain confidence that it won’t enter a cascading failure under various circumstances.
Test Until Failure and Beyond
Understanding the behavior of the service under heavy load is perhaps the most
important first step in avoiding cascading failures. Knowing how your system
behaves when it is overloaded helps to identify what engineering tasks are the most
important for long-term fixes; at the very least, this knowledge may help bootstrap
the debugging process for on-call engineers when an emergency arises.
Load test components until they break. As load increases, a component typically han‐
dles requests successfully until it reaches a point at which it can’t handle more
requests. At this point, the component should ideally start serving errors or degraded
results in response to additional load, but not significantly reduce the rate at which it
successfully handles requests. A component that is highly susceptible to a cascading
failure will start crashing or serving a very high rate of errors when it becomes over‐
loaded; a better designed component will instead be able to reject a few requests and
survive.
Load testing also reveals where the breaking point is, knowledge that’s fundamental to
the capacity planning process. It enables you to test for regressions, provision for
worst-case thresholds, and to trade off utilization versus safety margins.
278 | Chapter 22: Addressing Cascading Failures
Because of caching effects, gradually ramping up load may yield different results than
immediately increasing to expected load levels. Therefore, consider testing both grad‐
ual and impulse load patterns.
You should also test and understand how the component behaves as it returns to
nominal load after having been pushed well beyond that load. Such testing may
answer questions such as:
• If a component enters a degraded mode on heavy load, is it capable of exiting the
degraded mode without human intervention?
• If a couple of servers crash under heavy load, how much does the load need to
drop in order for the system to stabilize?
If you’re load testing a stateful service or a service that employs caching, your load
test should track state between multiple interactions and check correctness at high
load, which is often where subtle concurrency bugs hit.
Keep in mind that individual components may have different breaking points, so load
test each component separately. You won’t know in advance which component may
hit the wall first, and you want to know how your system behaves when it does.
If you believe your system has proper protections against being overloaded, consider
performing failure tests in a small slice of production to find the point at which the
components in your system fail under real traffic. These limits may not be adequately
reflected by synthetic load test traffic, so real traffic tests may provide more realistic
results than load tests, at the risk of causing user-visible pain. Be careful when testing
on real traffic: make sure that you have extra capacity available in case your automatic
protections don’t work and you need to manually fail over. You might consider some
of the following production tests:
• Reducing task counts quickly or slowly over time, beyond expected traffic
patterns
• Rapidly losing a cluster’s worth of capacity
• Blackholing various backends
Test Popular Clients
Understand how large clients use your service. For example, you want to know if
clients:
• Can queue work while the service is down
• Use randomized exponential backoff on errors
Testing for Cascading Failures | 279
• Are vulnerable to external triggers that can create large amounts of load (e.g., an
externally triggered software update might clear an offline client’s cache)
Depending on your service, you may or may not be in control of all the client code
that talks to your service. However, it’s still a good idea to have an understanding of
how large clients that interact with your service will behave.
The same principles apply to large internal clients. Stage system failures with the larg‐
est clients to see how they react. Ask internal clients how they access your service and
what mechanisms they use to handle backend failure.
Test Noncritical Backends
Test your noncritical backends, and make sure their unavailability does not interfere
with the critical components of your service.
For example, suppose your frontend has critical and noncritical backends. Often, a
given request includes both critical components (e.g., query results) and noncritical
components (e.g., spelling suggestions). Your requests may significantly slow down
and consume resources waiting for noncritical backends to finish.
In addition to testing behavior when the noncritical backend is unavailable, test how
the frontend behaves if the noncritical backend never responds (for example, if it is
blackholing requests). Backends advertised as noncritical can still cause problems on
frontends when requests have long deadlines. The frontend should not start rejecting
lots of requests, running out of resources, or serving with very high latency when a
noncritical backend blackholes.
Immediate Steps to Address Cascading Failures
Once you have identified that your service is experiencing a cascading failure, you
can use a few different strategies to remedy the situation—and of course, a cascading
failure is a good opportunity to use your incident management protocol (Chap‐
ter 14).
Increase Resources
If your system is running at degraded capacity and you have idle resources, adding
tasks can be the most expedient way to recover from the outage. However, if the ser‐
vice has entered a death spiral of some sort, adding more resources may not be suffi‐
cient to recover.
280 | Chapter 22: Addressing Cascading Failures
Stop Health Check Failures/Deaths
Some cluster scheduling systems, such as Borg, check the health of tasks in a job and
restart tasks that are unhealthy. This practice may create a failure mode in which
health-checking itself makes the service unhealthy. For example, if half the tasks aren’t
able to accomplish any work because they’re starting up and the other half will soon
be killed because they’re overloaded and failing health checks, temporarily disabling
health checks may permit the system to stabilize until all the tasks are running.
Process health checking (“is this binary responding at all?”) and service health check‐
ing (“is this binary able to respond to this class of requests right now?”) are two con‐
ceptually distinct operations. Process health checking is relevant to the cluster
scheduler, whereas service health checking is relevant to the load balancer. Clearly
distinguishing between the two types of health checks can help avoid this scenario.
Restart Servers
If servers are somehow wedged and not making progress, restarting them may help.
Try restarting servers when:
• Java servers are in a GC death spiral
• Some in-flight requests have no deadlines but are consuming resources, leading
them to block threads, for example
• The servers are deadlocked
Make sure that you identify the source of the cascading failure before you restart your
servers. Make sure that taking this action won’t simply shift around load. Canary this
change, and make it slowly. Your actions may amplify an existing cascading failure if
the outage is actually due to an issue like a cold cache.
Drop Traffic
Dropping load is a big hammer, usually reserved for situations in which you have a
true cascading failure on your hands and you cannot fix it by other means. For exam‐
ple, if heavy load causes most servers to crash as soon as they become healthy, you
can get the service up and running again by:
1. Addressing the initial triggering condition (by adding capacity, for example).
2. Reducing load enough so that the crashing stops. Consider being aggressive here
—if the entire service is crash-looping, only allow, say, 1% of the traffic through.
3. Allowing the majority of the servers to become healthy.
4. Gradually ramping up the load.
Immediate Steps to Address Cascading Failures | 281
This strategy allows caches to warm up, connections to be established, etc., before
load returns to normal levels.
Obviously, this tactic will cause a lot of user-visible harm. Whether or not you’re able
to (or if you even should) drop traffic indiscriminately depends on how the service is
configured. If you have some mechanism to drop less important traffic (e.g., prefetch‐
ing), use that mechanism first.
It is important to keep in mind that this strategy enables you to recover from a cas‐
cading outage once the underlying problem is fixed. If the issue that started the cas‐
cading failure is not fixed (e.g., insufficient global capacity), then the cascading failure
may trigger shortly after all traffic returns. Therefore, before using this strategy, con‐
sider fixing (or at least papering over) the root cause or triggering condition. For
example, if the service ran out of memory and is now in a death spiral, adding more
memory or tasks should be your first step.
Enter Degraded Modes
Serve degraded results by doing less work or dropping unimportant traffic. This
strategy must be engineered into your service, and can be implemented only if you
know which traffic can be degraded and you have the ability to differentiate between
the various payloads.
Eliminate Batch Load
Some services have load that is important, but not critical. Consider turning off those
sources of load. For example, if index updates, data copies, or statistics gathering con‐
sume resources of the serving path, consider turning off those sources of load during
an outage.
Eliminate Bad Traffic
If some queries are creating heavy load or crashes (e.g., queries of death), consider
blocking them or eliminating them via other means.
282 | Chapter 22: Addressing Cascading Failures
Cascading Failure and Shakespeare
A documentary about Shakespeare’s works airs in Japan, and explicitly points to our
Shakespeare service as an excellent place to conduct further research. Following the
broadcast, traffic to our Asian datacenter surges beyond the service’s capacity. This
capacity problem is further compounded by a major update to the Shakespeare ser‐
vice that simultaneously occurs in that datacenter.
Fortunately, a number of safeguards are in place that help mitigate the potential for
failure. The Production Readiness Review process identified some issues that the