wj
k(cid:54)=i
n(cid:88)
(cid:0)dk,|k − i|n
(cid:33)
(cid:1) − Ci
(cid:0)di,|i − k|n
(cid:1) .
(cid:0)di,|i − k|n
µjwj
(3)
(4)
5Noise in the data results in diﬀerent Yi for diﬀerent sets of
discounts; the ISP can use an average of these Yi.
Algorithm 2: Price determination.
Data: Estimated waiting functions.
Result: Optimized TDP prices.
Start with a set of discounts for the next n periods, determined
with initial waiting function estimates;
while TDP is oﬀered do
for k = 1 → n do
Choose the discount for the nth period after period k so
as to minimize Γ1 + Γ2 in (3-4).
if k == n then
Run the waiting function estimation (Algorithm 1)
to ﬁnd updated waiting function parameters.
Figure 4: Static TDP vs. dynamic TDP.
The ISP attempts to minimize Γ1 + Γ2 with respect to the
discount variables di, i = 1, 2, . . . , n [10]. For our trial,
TUBE oﬀers day-ahead pricing: when TDP is ﬁrst intro-
duced, the ISP publishes a full day of time-dependent prices
to users. After each subsequent period, a new price for the
period one day ahead of the current one is published. We
thus have the online price determination in Algorithm 2.
To test the eﬃcacy of TUBE’s feedback loop, we com-
pare simulated results of our feedback loop with results with
static TDP, i.e., time-dependent prices that do not change
from day to day. Figure 4 shows the results over three days.
We see that the peak periods under static TDP are about
8 MB above the maximum capacity, while those under dy-
namic TDP are only 4 MB above the maximum capacity.
The ISP underestimates users’ reaction to TDP in setting
the static prices; thus, many users shift to lower-price peri-
ods, resulting in a new peak period. Dynamic TDP, how-
ever, adjusts to this underestimation by correcting the prices
oﬀered, thus ﬂattening out the peak usage.
2.4 Graphical User Interface
Users react to the prices oﬀered through our GUIs, which
are designed to be simple and intuitive. We include the
following components in our design:
Price display: Users check the prices for the next 24
hours on the GUI home screen. Each price is color-coded
by its discount rate, e.g., red ( 0 s.t. pj+h < pj
money.
(cid:110)
νi : ∃ (cid:80)h−1
(cid:110)(cid:80)h−1
(cid:111)
(cid:111)
;
q=0 uk,j+q : pj+h < pj
l = argmaxj
Block app k from period l to l + h − 1, inclusive;
// Users can choose whether or not a notification
is sent at the beginning of period l + h to say
that the app is no longer blocked.
Update the projected usage values;
h ← h + 1;
Recalculate the predicted amount spent;
if app k does not have the lowest delay tolerance then
if the user prefers to defer the app with next-highest
delay tolerance by 1 period rather than defer app k by
h + 1 periods then
h ← 1;
νk ← 0;
The user will exceed the budget under all allowable
scheduling constraints;
change data with the server. Figure 5a shows TUBEOpt’s
component diagram, including the price optimization com-
ponents discussed in Sections 2.2 and 2.3; the shaded blocks
are part of our current implementation. All of the blocks
represent dynamic modules and can be reloaded on the ﬂy.
In the following discussion, we detail TUBEOpt’s scala-
bility of usage monitoring and the computational overhead
for its user behavior estimation and price computation.
Improving Scalability of Usage Monitoring
3.1.1
To measure individual usage, we assigned a unique IP ad-
dress to each user and created a Netfilter rule. When
TUBEOpt records the usage, it retrieves the byte and packet
counts from each rule. Unfortunately, this approach scales
linearly with the number of users; each user requires one
rule, and the computational cost increases linearly with the
number of rules. While ipset can improve performance by
combining multiple rules into one hash table, its use here is
limited, as usage with ipset is tracked for the hash table,
not for the individual rules. To improve scalability, we there-
fore implemented a separate kernel module that hooks the
LOCAL_IN of the IPSEC/VPN interface (ipsec0). It cre-
ates a hash table and records the usage for each IP address,
requiring only O(1) running time.
3.1.2 Computational Overhead
We next examine the computational overhead of using
Matlab and Python to estimate user behavior and compute
the optimized TDP prices. TUBE requires the runtime of
these codes to be relatively short, since each period’s price
computation should ﬁnish before the next TDP period, at
which time the server advertises the newly computed price.
Therefore, we evaluate the overhead of these two computa-
tions as we increase the optimization complexity.
We measure the computational overhead (the total run-
time in Matlab) as we increase the number of periods from
12 to 144 (2 hour to 10 minute periods). Table 2 shows
252(a) TUBEOpt design
(b) TUBEApp design
Figure 5: Building blocks of TUBE: (a) TUBEOpt on the server side and (b) TUBEApp on the mobile device.
The shaded blocks in (a) are TDP-speciﬁc modules, while those in (b) require system-level modiﬁcation.
Number of Periods
Behavior Estimation
Price Calculation
12
12.76
1.67
24
200.0
1.69
48
959.6
1.70
96
1967
1.81
144
15040
1.84
Table 2: Runtime of the behavior estimation and
price calculation in seconds.
Type
Status bar App usage
iPhone
Android
Windows
No
Yes
Yes
No
Yes
Yes
Daemon Code size
support
(# lines)
Partial
Yes
Yes
25K
5.4K
5.3K
Number of Application Types
Number of Periods
2
12
24
48
0.21
3.33
15.99
4
12.99
47.08
197.22
8
21.52
75.47
215.42
Table 3: Runtime of the behavior estimation (mins).
the measured running time of the behavior estimation and
price calculation. Even with 144 periods, the price calcu-
lation is quite fast (1.84 seconds); the estimation algorithm
performs adequately (4.2 hours), as it runs only once a day
for day-ahead TDP.
We also measure the eﬀect of adding multiple traﬃc classes
to the behavior estimation algorithm. Table 3 shows the al-
gorithm running time on our Intel Xeon server. The com-
putation with 48 periods and 8 application types still takes
less than 4 hours (215.42 minutes), which is more than fast
enough, as the estimation runs once a day. Our estimation
uses one month of simulated data, which was generated by
perturbing the usage predicted from given waiting functions
by up to 50%. The running times were averaged across ﬁve
computations with random data and starting points. With
more powerful hardware and optimized code, signiﬁcant fur-
ther acceleration can be achieved.
3.2 TUBEApp
We implemented TUBEApp on the iOS, Android, and
Windows platforms, although all trial participants were iOS
users due to these devices’ popularity on our campus. We
therefore focus our discussion on the iOS implementation.
We consider both the manual and autopilot modes, as shown
in Fig. 5b’s design and Fig. 6’s GUI screenshots. Due to
the iOS platform’s closed nature, implementing the shaded
blocks in Fig. 5b, e.g., monitoring each application’s usage,
requires jailbreaking the devices. We demo our app in [26].
3.2.1 OS Limitations
The Windows, Android, and iOS platforms each have var-
ious limitations; the iOS platform oﬀers the most restric-
tions. While all platforms support showing the prices oﬀered
and the device’s aggregate data usage, TUBEApp’s autopi-
lot mode additionally requires 1) measuring the volume of
Table 4: TUBEApp on diﬀerent platforms.
each application’s usage, 2) displaying the price for the cur-
rent period on a status bar, and 3) allowing and blocking