ential Privacy. IEEE Access 7 (2019), 89390–89399.
[67] Zhilu Zhang and Mert R Sabuncu. 2018. Generalized cross entropy loss for
training deep neural networks with noisy labels. arXiv preprint arXiv:1805.07836
(2018).
[68] Jun Zhou, Chaochao Chen, Longfei Zheng, Xiaolin Zheng, Bingzhe Wu, Ziqi
Liu, and Li Wang. 2020. Privacy-Preserving Graph Neural Network for Node
Classification. arXiv preprint arXiv:2005.11903 (2020).
A DEFERRED THEORETICAL ARGUMENTS
A.1 Theorem 3.1
Proof. Let M(x) denote the multi-bit encoder (Algorithm 1)
applied on the input vector x. Let x∗ = M(x) be the encoded vector
corresponding to x. We need to show that for any two input features
x1 and x2, we have Pr[M(x1)=x∗]
it can be easily seen that 𝑥∗
when 𝑖 ∉ S with probability 1 − 𝑚
According to Algorithm 1, for any dimension 𝑖 ∈ {1, 2, . . . , 𝑑},
𝑖 = 0 occurs
𝑖 ∈ {−1, 0, 1}. The case 𝑥∗
Pr[M(x2)=x∗] ≤ 𝑒𝜖.
𝑑 , therefore:
1 − 𝑚/𝑑
1 − 𝑚/𝑑
Pr [M(x1)𝑖 = 0]
Pr [M(x2)𝑖 = 0] =
= 1 ≤ 𝑒𝜖, ∀𝜖 > 0
(15)
According to Algorithm 1, in the case of 𝑥∗
𝑖 ∈ {−1, 1}, we see that
the probability of getting 𝑥∗
𝑑 · 𝑒𝜖/𝑚
𝑑 ·
𝑖 = 1 ranges from 𝑚
𝑒𝜖/𝑚+1
depending on the value of 𝑥𝑖. Analogously, the probability of 𝑥∗
𝑖 =
−1 also varies from 𝑚
𝑑 ·
𝑒𝜖/𝑚+1 to 𝑚
𝑒𝜖/𝑚
1
𝑒𝜖/𝑚+1. Therefore:
Pr [M(x1)𝑖 ∈ {−1, 1}]
Pr [M(x2)𝑖 ∈ {−1, 1}] ≤ max Pr [M(x1)𝑖 ∈ {−1, 1}]
min Pr [M(x2)𝑖 ∈ {−1, 1}]
𝑑 ·
𝑑 ·
𝑑 ·
𝑒𝜖/𝑚+1 to 𝑚
≤ 𝑒𝜖/𝑚
𝑒𝜖/𝑚
𝑒𝜖/𝑚+1
𝑒𝜖/𝑚+1
≤
𝑚
𝑚
1
1
Consequently, we have:
Pr [M(x1) = x∗]
Pr [M(x2) = x∗] =
=
𝑖
𝑖
𝑖=1
(cid:3)
Pr(cid:2)M(𝑥1)𝑖 = 𝑥∗
𝑑
(cid:3)
Pr(cid:2)M(𝑥2)𝑖 = 𝑥∗
Pr(cid:2)M(𝑥1) 𝑗 = 0(cid:3)

Pr(cid:2)M(𝑥2) 𝑗 = 0(cid:3)
× 

≤ 
𝑘 ∈{−1,1}
𝑥∗
𝑘 ∈{−1,1}
𝑘 |𝑥∗
𝑗 |𝑥∗
𝑗 =0
=
𝑘 ∈{−1,1}
𝑥∗
≤ 𝑒𝜖
Pr [M(𝑥1)𝑘 ∈ {−1, 1}]
Pr [M(𝑥2)𝑘 ∈ {−1, 1}]
Pr [M(𝑥1)𝑘 ∈ {−1, 1}]
Pr [M(𝑥2)𝑘 ∈ {−1, 1}]
𝑒𝜖/𝑚
(16)
(17)
(18)
(19)
(20)
which concludes the proof. In the above, (18) and (19) follows from
applying (15) and (16), respectively, and (20) follows from the fact
that exactly 𝑚 number of input features result in non-zero output.
□
A.2 Proposition 3.2
We first establish the following lemma and then prove Proposi-
tion 3.2:
𝑖
(cid:19)
(cid:18)
and
𝑚
𝑑
(cid:3) =
Lemma A.1. Let x∗ be the output of Algorithm 1 on the input vector
E(cid:2)𝑥∗
(cid:3) =
𝑉 𝑎𝑟(cid:2)𝑥∗
E(cid:2)𝑥∗
(cid:3) = E(cid:2)𝑥∗
x. For any dimension 𝑖 ∈ {1, 2, . . . , 𝑑}, we have:
· 𝑒𝜖/𝑚 − 1
2 · 𝑥𝑖 − 𝛼
𝑒𝜖/𝑚 + 1 ·
𝛽 − 𝛼
(cid:34)𝑚
(cid:18)
2 · 𝑥𝑖 − 𝛼
𝛽 − 𝛼
| 𝑠𝑖 = 1(cid:3) Pr(𝑠𝑖 = 1)
| 𝑠𝑖 = 0(cid:3) Pr(𝑠𝑖 = 0) + E(cid:2)𝑥∗
· 𝑒𝜖/𝑚 − 1
𝑒𝜖/𝑚 + 1 ·
Proof. For the expectation, we have:
(cid:19)(cid:35)2
− 1
− 1
(21)
(22)
𝑚
𝑑
−
𝑑
𝑖
𝑖
𝑖
𝑖
=
𝑚
𝑑
· (2 E [𝑡𝑖] − 1)
− 1
(cid:3)2
𝑖
Since 𝑡𝑖 is a Bernoulli random variable, we have:
𝑖
𝑖
·
1
2
1
=
=
𝑚
𝑑
𝑚
𝑑
𝑚
𝑑
− 1
(cid:33)
(cid:35)
E [𝑡𝑖] =
(cid:3) =
E(cid:2)𝑥∗
· 𝑒𝜖/𝑚 − 1
𝑒𝜖/𝑚 + 1
𝑉 𝑎𝑟(cid:2)𝑥∗
Combining (23) and (24) yields:
𝑒𝜖/𝑚 + 1 + 𝑥𝑖 − 𝛼
𝛽 − 𝛼
(cid:34)
(cid:32)
· 𝑒𝜖/𝑚 − 1
𝑒𝜖/𝑚 + 1 + 𝑥𝑖 − 𝛼
(cid:34) 1 − 𝑒𝜖/𝑚
𝛽 − 𝛼
𝑒𝜖/𝑚 + 1
· 𝑒𝜖/𝑚 − 1
𝑒𝜖/𝑚 + 1 + 2 · 𝑥𝑖 − 𝛼
(cid:18)
(cid:19)
·
𝛽 − 𝛼
𝑒𝜖/𝑚 + 1
· 𝑒𝜖/𝑚 − 1
2 · 𝑥𝑖 − 𝛼
𝑒𝜖/𝑚 + 1 ·
𝛽 − 𝛼
(cid:1)2(cid:105) − E(cid:2)𝑥∗
(cid:104)(cid:0)𝑥∗
(cid:3) = E
(cid:3)2
For the variance, we have:
(cid:1)2 | 𝑠𝑖 = 0(cid:105) Pr(𝑠𝑖 = 0)
(cid:104)(cid:0)𝑥∗
(cid:1)2 | 𝑠𝑖 = 1(cid:105) Pr(𝑠𝑖 = 1) − E(cid:2)𝑥∗
(cid:104)(cid:0)𝑥∗
𝑖 = ±1, and thus(cid:0)𝑥∗
(cid:34)𝑚
(cid:19)(cid:35)2
· 𝑒𝜖/𝑚 − 1
𝑒𝜖/𝑚 + 1 ·
(cid:104)𝑥′
· 𝑒𝜖/𝑚 + 1
𝑒𝜖/𝑚 − 1 · E
(cid:1)2
2 · 𝑥𝑖 − 𝛼
𝛽 − 𝛼
(cid:105)
Given 𝑠𝑖 = 1, we have 𝑥∗
combining with (25), we get:
any dimension 𝑖 ∈ {1, 2, . . . , 𝑑}.
Proof. We need to show that E
Now we prove Proposition 3.2.
𝑉 𝑎𝑟(cid:2)𝑥∗
(cid:105) + 𝛼 + 𝛽
= E
+ E
𝑑(𝛽 − 𝛼)
(cid:104)𝑥∗
(cid:3) =
(cid:104)𝑥′
− 1
𝑚
𝑑
2𝑚
(cid:18)
(cid:105)
−
=
𝑣,𝑖
𝑣,𝑖
𝑣,𝑖
2
E
𝑑
𝑖
𝑖
𝑖
𝑖
𝑖
𝑖
(cid:35)
(23)
(24)
(25)
(26)
□
(27)
= 1. Therefore,
= 𝑥𝑣,𝑖 for any 𝑣 ∈ V and
Session 7A: Privacy Attacks and Defenses for ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea2143Applying Lemma A.1 yields:
(cid:105)
(cid:104)𝑥′
𝑣,𝑖
E
𝑑
(cid:34)𝑚
(cid:19)
− 1
+ 𝛼 + 𝛽
2
𝑒𝜖/𝑚 − 1
𝑒𝜖/𝑚 + 1
+ 𝛼 + 𝛽
2
𝑒𝜖/𝑚 + 1
𝑒𝜖/𝑚 − 1
𝑑(𝛽 − 𝛼)
=
2𝑚
(cid:18)
+ 𝛼 + 𝛽
2
2 𝑥𝑣,𝑖 − 𝛼
𝛽 − 𝛼
𝛽 − 𝛼
2
= 𝑥𝑣,𝑖 − 𝛼 − 𝛽 − 𝛼
2
=
(cid:19)(cid:35)
(cid:18)
2 𝑥𝑣,𝑖 − 𝛼
𝛽 − 𝛼
− 1
)
𝑧
(
𝑓
20
15
10
5
0
0
2
4
6
8
10
𝑧
= 𝑥𝑣,𝑖
□
A.3 Proposition 3.3
Proof. According to (4), the variance of 𝑥′
𝑣,𝑖 can be written in
terms of the variance of 𝑥∗
𝑉 𝑎𝑟(cid:104)𝑥′
(cid:105)
𝑉 𝑎𝑟(cid:104)𝑥′
=
𝑣,𝑖
𝑣,𝑖
2𝑚
Applying Lemma A.1 yields:
=
2𝑚
𝑣,𝑖 as:
· 𝑒𝜖/𝑚 + 1
𝑒𝜖/𝑚 − 1
(cid:35)2
(cid:34) 𝑑(𝛽 − 𝛼)
(cid:105)
(cid:34) 𝑑(𝛽 − 𝛼)
· 𝑒𝜖/𝑚 + 1
(cid:34)𝑚
𝑒𝜖/𝑚 − 1
×(cid:169)(cid:173)(cid:171)𝑚
· 𝑒𝜖/𝑚 − 1
(cid:32) 𝛽 − 𝛼
(cid:33)2
𝑒𝜖/𝑚 + 1 ·
· 𝑒𝜖/𝑚 + 1
(cid:18)
(cid:20) 𝛽 − 𝛼
𝑒𝜖/𝑚 − 1
2 · 𝑥𝑣,𝑖 − 𝛼
(cid:33)2
(cid:32) 𝛽 − 𝛼
𝛽 − 𝛼
· 𝑒𝜖/𝑚 + 1
𝑒𝜖/𝑚 − 1
𝑑
𝑚
𝑑
𝑚
(cid:18)
−