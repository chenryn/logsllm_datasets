ential Privacy. IEEE Access 7 (2019), 89390â€“89399.
[67] Zhilu Zhang and Mert R Sabuncu. 2018. Generalized cross entropy loss for
training deep neural networks with noisy labels. arXiv preprint arXiv:1805.07836
(2018).
[68] Jun Zhou, Chaochao Chen, Longfei Zheng, Xiaolin Zheng, Bingzhe Wu, Ziqi
Liu, and Li Wang. 2020. Privacy-Preserving Graph Neural Network for Node
Classification. arXiv preprint arXiv:2005.11903 (2020).
A DEFERRED THEORETICAL ARGUMENTS
A.1 Theorem 3.1
Proof. Let M(x) denote the multi-bit encoder (Algorithm 1)
applied on the input vector x. Let xâˆ— = M(x) be the encoded vector
corresponding to x. We need to show that for any two input features
x1 and x2, we have Pr[M(x1)=xâˆ—]
it can be easily seen that ğ‘¥âˆ—
when ğ‘– âˆ‰ S with probability 1 âˆ’ ğ‘š
According to Algorithm 1, for any dimension ğ‘– âˆˆ {1, 2, . . . , ğ‘‘},
ğ‘– = 0 occurs
ğ‘– âˆˆ {âˆ’1, 0, 1}. The case ğ‘¥âˆ—
Pr[M(x2)=xâˆ—] â‰¤ ğ‘’ğœ–.
ğ‘‘ , therefore:
1 âˆ’ ğ‘š/ğ‘‘
1 âˆ’ ğ‘š/ğ‘‘
Pr [M(x1)ğ‘– = 0]
Pr [M(x2)ğ‘– = 0] =
= 1 â‰¤ ğ‘’ğœ–, âˆ€ğœ– > 0
(15)
According to Algorithm 1, in the case of ğ‘¥âˆ—
ğ‘– âˆˆ {âˆ’1, 1}, we see that
the probability of getting ğ‘¥âˆ—
ğ‘‘ Â· ğ‘’ğœ–/ğ‘š
ğ‘‘ Â·
ğ‘– = 1 ranges from ğ‘š
ğ‘’ğœ–/ğ‘š+1
depending on the value of ğ‘¥ğ‘–. Analogously, the probability of ğ‘¥âˆ—
ğ‘– =
âˆ’1 also varies from ğ‘š
ğ‘‘ Â·
ğ‘’ğœ–/ğ‘š+1 to ğ‘š
ğ‘’ğœ–/ğ‘š
1
ğ‘’ğœ–/ğ‘š+1. Therefore:
Pr [M(x1)ğ‘– âˆˆ {âˆ’1, 1}]
Pr [M(x2)ğ‘– âˆˆ {âˆ’1, 1}] â‰¤ max Pr [M(x1)ğ‘– âˆˆ {âˆ’1, 1}]
min Pr [M(x2)ğ‘– âˆˆ {âˆ’1, 1}]
ğ‘‘ Â·
ğ‘‘ Â·
ğ‘‘ Â·
ğ‘’ğœ–/ğ‘š+1 to ğ‘š
â‰¤ ğ‘’ğœ–/ğ‘š
ğ‘’ğœ–/ğ‘š
ğ‘’ğœ–/ğ‘š+1
ğ‘’ğœ–/ğ‘š+1
â‰¤
ğ‘š
ğ‘š
1
1
Consequently, we have:
Pr [M(x1) = xâˆ—]
Pr [M(x2) = xâˆ—] =
=
ğ‘–
ğ‘–
ğ‘–=1
(cid:3)
Pr(cid:2)M(ğ‘¥1)ğ‘– = ğ‘¥âˆ—
ğ‘‘
(cid:3)
Pr(cid:2)M(ğ‘¥2)ğ‘– = ğ‘¥âˆ—
Pr(cid:2)M(ğ‘¥1) ğ‘— = 0(cid:3)

Pr(cid:2)M(ğ‘¥2) ğ‘— = 0(cid:3)
Ã— 

â‰¤ 
ğ‘˜ âˆˆ{âˆ’1,1}
ğ‘¥âˆ—
ğ‘˜ âˆˆ{âˆ’1,1}
ğ‘˜ |ğ‘¥âˆ—
ğ‘— |ğ‘¥âˆ—
ğ‘— =0
=
ğ‘˜ âˆˆ{âˆ’1,1}
ğ‘¥âˆ—
â‰¤ ğ‘’ğœ–
Pr [M(ğ‘¥1)ğ‘˜ âˆˆ {âˆ’1, 1}]
Pr [M(ğ‘¥2)ğ‘˜ âˆˆ {âˆ’1, 1}]
Pr [M(ğ‘¥1)ğ‘˜ âˆˆ {âˆ’1, 1}]
Pr [M(ğ‘¥2)ğ‘˜ âˆˆ {âˆ’1, 1}]
ğ‘’ğœ–/ğ‘š
(16)
(17)
(18)
(19)
(20)
which concludes the proof. In the above, (18) and (19) follows from
applying (15) and (16), respectively, and (20) follows from the fact
that exactly ğ‘š number of input features result in non-zero output.
â–¡
A.2 Proposition 3.2
We first establish the following lemma and then prove Proposi-
tion 3.2:
ğ‘–
(cid:19)
(cid:18)
and
ğ‘š
ğ‘‘
(cid:3) =
Lemma A.1. Let xâˆ— be the output of Algorithm 1 on the input vector
E(cid:2)ğ‘¥âˆ—
(cid:3) =
ğ‘‰ ğ‘ğ‘Ÿ(cid:2)ğ‘¥âˆ—
E(cid:2)ğ‘¥âˆ—
(cid:3) = E(cid:2)ğ‘¥âˆ—
x. For any dimension ğ‘– âˆˆ {1, 2, . . . , ğ‘‘}, we have:
Â· ğ‘’ğœ–/ğ‘š âˆ’ 1
2 Â· ğ‘¥ğ‘– âˆ’ ğ›¼
ğ‘’ğœ–/ğ‘š + 1 Â·
ğ›½ âˆ’ ğ›¼
(cid:34)ğ‘š
(cid:18)
2 Â· ğ‘¥ğ‘– âˆ’ ğ›¼
ğ›½ âˆ’ ğ›¼
| ğ‘ ğ‘– = 1(cid:3) Pr(ğ‘ ğ‘– = 1)
| ğ‘ ğ‘– = 0(cid:3) Pr(ğ‘ ğ‘– = 0) + E(cid:2)ğ‘¥âˆ—
Â· ğ‘’ğœ–/ğ‘š âˆ’ 1
ğ‘’ğœ–/ğ‘š + 1 Â·
Proof. For the expectation, we have:
(cid:19)(cid:35)2
âˆ’ 1
âˆ’ 1
(21)
(22)
ğ‘š
ğ‘‘
âˆ’
ğ‘‘
ğ‘–
ğ‘–
ğ‘–
ğ‘–
=
ğ‘š
ğ‘‘
Â· (2 E [ğ‘¡ğ‘–] âˆ’ 1)
âˆ’ 1
(cid:3)2
ğ‘–
Since ğ‘¡ğ‘– is a Bernoulli random variable, we have:
ğ‘–
ğ‘–
Â·
1
2
1
=
=
ğ‘š
ğ‘‘
ğ‘š
ğ‘‘
ğ‘š
ğ‘‘
âˆ’ 1
(cid:33)
(cid:35)
E [ğ‘¡ğ‘–] =
(cid:3) =
E(cid:2)ğ‘¥âˆ—
Â· ğ‘’ğœ–/ğ‘š âˆ’ 1
ğ‘’ğœ–/ğ‘š + 1
ğ‘‰ ğ‘ğ‘Ÿ(cid:2)ğ‘¥âˆ—
Combining (23) and (24) yields:
ğ‘’ğœ–/ğ‘š + 1 + ğ‘¥ğ‘– âˆ’ ğ›¼
ğ›½ âˆ’ ğ›¼
(cid:34)
(cid:32)
Â· ğ‘’ğœ–/ğ‘š âˆ’ 1
ğ‘’ğœ–/ğ‘š + 1 + ğ‘¥ğ‘– âˆ’ ğ›¼
(cid:34) 1 âˆ’ ğ‘’ğœ–/ğ‘š
ğ›½ âˆ’ ğ›¼
ğ‘’ğœ–/ğ‘š + 1
Â· ğ‘’ğœ–/ğ‘š âˆ’ 1
ğ‘’ğœ–/ğ‘š + 1 + 2 Â· ğ‘¥ğ‘– âˆ’ ğ›¼
(cid:18)
(cid:19)
Â·
ğ›½ âˆ’ ğ›¼
ğ‘’ğœ–/ğ‘š + 1
Â· ğ‘’ğœ–/ğ‘š âˆ’ 1
2 Â· ğ‘¥ğ‘– âˆ’ ğ›¼
ğ‘’ğœ–/ğ‘š + 1 Â·
ğ›½ âˆ’ ğ›¼
(cid:1)2(cid:105) âˆ’ E(cid:2)ğ‘¥âˆ—
(cid:104)(cid:0)ğ‘¥âˆ—
(cid:3) = E
(cid:3)2
For the variance, we have:
(cid:1)2 | ğ‘ ğ‘– = 0(cid:105) Pr(ğ‘ ğ‘– = 0)
(cid:104)(cid:0)ğ‘¥âˆ—
(cid:1)2 | ğ‘ ğ‘– = 1(cid:105) Pr(ğ‘ ğ‘– = 1) âˆ’ E(cid:2)ğ‘¥âˆ—
(cid:104)(cid:0)ğ‘¥âˆ—
ğ‘– = Â±1, and thus(cid:0)ğ‘¥âˆ—
(cid:34)ğ‘š
(cid:19)(cid:35)2
Â· ğ‘’ğœ–/ğ‘š âˆ’ 1
ğ‘’ğœ–/ğ‘š + 1 Â·
(cid:104)ğ‘¥â€²
Â· ğ‘’ğœ–/ğ‘š + 1
ğ‘’ğœ–/ğ‘š âˆ’ 1 Â· E
(cid:1)2
2 Â· ğ‘¥ğ‘– âˆ’ ğ›¼
ğ›½ âˆ’ ğ›¼
(cid:105)
Given ğ‘ ğ‘– = 1, we have ğ‘¥âˆ—
combining with (25), we get:
any dimension ğ‘– âˆˆ {1, 2, . . . , ğ‘‘}.
Proof. We need to show that E
Now we prove Proposition 3.2.
ğ‘‰ ğ‘ğ‘Ÿ(cid:2)ğ‘¥âˆ—
(cid:105) + ğ›¼ + ğ›½
= E
+ E
ğ‘‘(ğ›½ âˆ’ ğ›¼)
(cid:104)ğ‘¥âˆ—
(cid:3) =
(cid:104)ğ‘¥â€²
âˆ’ 1
ğ‘š
ğ‘‘
2ğ‘š
(cid:18)
(cid:105)
âˆ’
=
ğ‘£,ğ‘–
ğ‘£,ğ‘–
ğ‘£,ğ‘–
2
E
ğ‘‘
ğ‘–
ğ‘–
ğ‘–
ğ‘–
ğ‘–
ğ‘–
(cid:35)
(23)
(24)
(25)
(26)
â–¡
(27)
= 1. Therefore,
= ğ‘¥ğ‘£,ğ‘– for any ğ‘£ âˆˆ V and
Session 7A: Privacy Attacks and Defenses for ML CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea2143Applying Lemma A.1 yields:
(cid:105)
(cid:104)ğ‘¥â€²
ğ‘£,ğ‘–
E
ğ‘‘
(cid:34)ğ‘š
(cid:19)
âˆ’ 1
+ ğ›¼ + ğ›½
2
ğ‘’ğœ–/ğ‘š âˆ’ 1
ğ‘’ğœ–/ğ‘š + 1
+ ğ›¼ + ğ›½
2
ğ‘’ğœ–/ğ‘š + 1
ğ‘’ğœ–/ğ‘š âˆ’ 1
ğ‘‘(ğ›½ âˆ’ ğ›¼)
=
2ğ‘š
(cid:18)
+ ğ›¼ + ğ›½
2
2 ğ‘¥ğ‘£,ğ‘– âˆ’ ğ›¼
ğ›½ âˆ’ ğ›¼
ğ›½ âˆ’ ğ›¼
2
= ğ‘¥ğ‘£,ğ‘– âˆ’ ğ›¼ âˆ’ ğ›½ âˆ’ ğ›¼
2
=
(cid:19)(cid:35)
(cid:18)
2 ğ‘¥ğ‘£,ğ‘– âˆ’ ğ›¼
ğ›½ âˆ’ ğ›¼
âˆ’ 1
)
ğ‘§
(
ğ‘“
20
15
10
5
0
0
2
4
6
8
10
ğ‘§
= ğ‘¥ğ‘£,ğ‘–
â–¡
A.3 Proposition 3.3
Proof. According to (4), the variance of ğ‘¥â€²
ğ‘£,ğ‘– can be written in
terms of the variance of ğ‘¥âˆ—
ğ‘‰ ğ‘ğ‘Ÿ(cid:104)ğ‘¥â€²
(cid:105)
ğ‘‰ ğ‘ğ‘Ÿ(cid:104)ğ‘¥â€²
=
ğ‘£,ğ‘–
ğ‘£,ğ‘–
2ğ‘š
Applying Lemma A.1 yields:
=
2ğ‘š
ğ‘£,ğ‘– as:
Â· ğ‘’ğœ–/ğ‘š + 1
ğ‘’ğœ–/ğ‘š âˆ’ 1
(cid:35)2
(cid:34) ğ‘‘(ğ›½ âˆ’ ğ›¼)
(cid:105)
(cid:34) ğ‘‘(ğ›½ âˆ’ ğ›¼)
Â· ğ‘’ğœ–/ğ‘š + 1
(cid:34)ğ‘š
ğ‘’ğœ–/ğ‘š âˆ’ 1
Ã—(cid:169)(cid:173)(cid:171)ğ‘š
Â· ğ‘’ğœ–/ğ‘š âˆ’ 1
(cid:32) ğ›½ âˆ’ ğ›¼
(cid:33)2
ğ‘’ğœ–/ğ‘š + 1 Â·
Â· ğ‘’ğœ–/ğ‘š + 1
(cid:18)
(cid:20) ğ›½ âˆ’ ğ›¼
ğ‘’ğœ–/ğ‘š âˆ’ 1
2 Â· ğ‘¥ğ‘£,ğ‘– âˆ’ ğ›¼
(cid:33)2
(cid:32) ğ›½ âˆ’ ğ›¼
ğ›½ âˆ’ ğ›¼
Â· ğ‘’ğœ–/ğ‘š + 1
ğ‘’ğœ–/ğ‘š âˆ’ 1
ğ‘‘
ğ‘š
ğ‘‘
ğ‘š
(cid:18)
âˆ’