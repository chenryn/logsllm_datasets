analysis  is  done,  we  then  show  how  identified  weak  passwords 
can be effectively modified to be strong. 
We  next  review  some  classical  mathematical  notions  that  have 
been proposed to measure strengths of passwords. After this, we 
review the probabilistic password cracking [3] approach that we 
use for doing the password analysis component of AMP. 
2.2  Classical Measures of Password Strength 
Entropy  as  a  way  to  measure  the  uncertainty  arising  from  a 
probability distribution was suggested by Claude Shannon [24] in 
an effort to explore the uncertainty of letters and words in English.  
Definition  2.1  Shannon  Entropy:  Let  X  be  a  discrete  random 
variable with probability mass function p(x) = Pr{X=x}, x(cid:143)X. The 
entropy H(X) of such a random variable X is defined by: 
H(X)
                          (2.1) 
The notion of guessing entropy was introduced in [25].  
Definition 2.2 Guessing Entropy: Assume that the probabilities 
pi are denoted in a decreasing form p1 (cid:149)(cid:3)(cid:83)2  (cid:149)(cid:3)(cid:171)(cid:3)(cid:149)(cid:3)(cid:83) n. Then the 
guessing entropy denoted by G(X) is: 
 = 
 G(X)
                                    (2.2) 
The idea here is that in an optimal attack, the attacker would try 
the highest probability passwords first and thus guessing entropy 
measures the expected number of tries until success. However, it 
was shown by Verheul [2] that simply having a high value of the 
guessing entropy itself is not sufficient since a distribution with a 
high  value  of  guessing  entropy  is  possible,  even  with  the 
probability of the first potential guess p1 being very high and thus 
easily broken. A third notion is thus often used which is called the 
min entropy (H(cid:146)(X) = - log2 p1) [2]. If the min entropy is high then 
the probability of the first password is small. At the time of much 
of  this  research,  investigators  really  had  no  idea  of  the  actual 
distribution of passwords or how to determine the above entropy 
values in any realistic setting. This began to change with hackers 
posting  large  numbers  of  revealed  passwords  on-line.  An 
important  development  at  this  stage  was  the  probabilistic 
password cracking work based on training a context-free grammar 
and  using  this  grammar  as  an  effective  model  to  simulate  an 
optimal  password  cracking  attack  (trying  the  highest  probability 
passwords first). 
2.3  Probabilistic Password Cracking 
We used the password cracking system of Weir et al. [3] for our 
work.  The  authors  used  probabilistic  context-free  grammars  to 
model  the  derivation  of  real  user  passwords  and  the  way  users 
create their passwords. The goal was to generate realistic guesses 
in  decreasing  order  of  probability  where  the  probabilities  are 
derived  through  training  on  large  sets  of  revealed  passwords.  In 
[3]  password  string  components  consisting  of  alphabet  symbols 
are  denoted  as  L,  digits  as  D,  special  characters  as  S  and 
capitalization as M. The authors also associate a number to show 
the 
the  password 
“football123!$” would be L8D3S2. Such strings are called the base 
structures. There are two steps in this password cracking approach 
[3]:  the  first  is  generating  the  context-free  grammar  from  a 
training  set  of  disclosed  real  user  passwords  and  the  second  is 
generating  the  actual  guesses  in  probabilistic  order  using  the 
grammar.  
the  substring.  For  example, 
length  of 
2.3.1  Training  
The  observed  base  structures  and  their  frequencies  are  derived 
from  the  training  set  of  passwords.  Information  about  the 
probability  of  the  digits,  special  characters  and  capitalization 
(case) are also obtained from the training set. This information is 
used  to  generate  the  probabilistic  context  free  grammar.  The 
probability of any string derived from the start symbol is then the 
product  of  the  probabilities  of  the  productions  used  in  its 
derivation. See Table 1. Using this grammar, for example, we can 
derive password “987dog!” with probability 0.04992. 
S (cid:198)(cid:198) D3L3S1(cid:198) 987L3S1(cid:198)987dogS1(cid:198)987dog! 
The  learning  phase  does  not  actually  include  determining 
probabilities of the alphabet strings since these are not considered 
to be a sufficient sample even for large training sets. Instead, for 
example,  the  L3  part  of  the  guess  comes  from  a  dictionary  with 
probability  equal  to  one  over  the  number  of  words  of  length  3. 
Furthermore,  probability  smoothing  can  be  used  to  give  an 
appropriately  low  probability  value  to  digits,  special  symbols, 
case and base structures that do not arise in the training set. 
2.3.2  Generating Password Guesses 
The  guess  generation  phase  generates  the  possible  password 
guesses  in  decreasing  probability  order  using  the  context-free 
grammar obtained from the previous step. Note that this order is 
what  we  need  to  compute  the  guessing  entropy.  Multiple 
dictionaries can be used with probabilities associated to each.  
Table 1. Example probabilistic CFG 
Left Hand 
Side 
S(cid:198) 
S(cid:198) 
D3(cid:198) 
D3(cid:198) 
S1(cid:198) 
S1(cid:198) 
S2(cid:198) 
S2(cid:198) 
S2(cid:198) 
L3(cid:198) 
L3(cid:198) 
Right Hand 
Side 
D3L3S1 
S2L3 
123 
987 
! 
# 
** 
!@ 
!! 
dog 
cat 
Probability 
0.8 
0.2 
0.76 
0.24 
0.52 
0.48 
0.62 
0.21 
0.17 
0.5 
0.5 
3.  ANALYZING PASSWORD STRENGTH 
For a password to be strong we need to make sure that it cannot be 
easily broken. For memorability we start with the assumption that 
the original password chosen by the user is a memorable password 
for that particular user. Our first step is to evaluate the user chosen 
password  for  strength  based  on  the  probability  of  that  password 
being  able  to  be  cracked.  For  this  we  use  the  probabilistic 
password cracking system trained on a comprehensive set of real 
passwords.  In  fact,  we  are  able  to  determine  a  threshold  value 
below  which  a  password  would  be  considered  as  strong.  This 
allows  us  to  build  a  reject  function  that  will  accept  a  strong 
password but reject a  weak password. Weak passwords are then 
modified by AMP to make them strong. An overview of different 
components of the AMP system is illustrated in Figure 1. In the 
preprocessing phase, we train the system on real user passwords 
using  the  same  technique  used  for  training  a  probabilistic 
password  cracker.  This  results  in  a  probabilistic  context  free 
grammar  that  can  generate  guesses  in  highest  probability  order. 
We  assume  that  the  training  set  used  in  this  step  is  a 
comprehensive set of passwords (and a sufficiently large sample 
111
set) that can be used as a model of realistic passwords. Once we 
have this we need to determine the threshold value. 
Figure 1. Flow chart of AMP design. 
3.1  Setting the Threshold 
We now turn to the specific issue of distinguishing between weak 
and strong passwords. A strong password is one for which it takes 
an attacker an appropriately long cracking time (ct) to crack that 
password  (in  hours).  Our  assumption  is  that  in  an  online  attack, 
the attacker would use the optimal strategy of trying the highest 
probability  passwords  in  decreasing  order  of  probability.  We 
define  the  threshold  value  (thp)  as  that  probability  such  that 
passwords with probability less than thp are strong and those that 
are  greater  than  or  equal  to  thp  are  weak.  Because  we  use  our 
probabilistic context-free grammar (plus appropriate dictionaries) 
as  our  model  of  the  realistic  password  distribution,  we  can 
determine the number of guesses g(thp) the attacker would make 
before trying a password with a value equal to the threshold value 
thp. Let r be the rate-per-hour of the guesses (based on the hash 
type, cracking system speed, etc.). We thus have g(thp) = ct * r. 
We now need to determine thp given g(thp) since we will use this 
value to decide whether a given password is strong or weak. There 
are two ways that we can determine the threshold. 
In the first approach, we run the probabilistic password cracker of 
the pre-processing phase (once) and generate a table that produces 
guesses and their probabilities at various time intervals. Although 
this  approach  is  accurate  and  straightforward,  it  is  not  always 
feasible to reach the desired number of guesses due to time and 
resources as we may have to run our guesser for a very long time. 
Table  2  shows  the  threshold  table  produced  by  running  a  guess 
generator trained on a set of over 1 million passwords. If we set 
the threshold at 2.96 × 10-13 and the probability of a password is 
less than this threshold, we know that it will take at least 1 day to 
crack that password using an optimal password cracking strategy. 
The second approach gives us only a lower bound for the number 
of  guesses  g(thp)  until  we  reach  a  given  value  thp  but  it  only 
requires  using  the  context-free  grammar  and  does  not  require 
actually generating all the guesses. Thus it is very fast. Although 
it only gives a bound, it is conservative with respect to ensuring 
that  a  proposed  password  is  strong.  The  algorithm  starts  with  a 
threshold thp and estimates the number of elements in each base 
structure i (with probability pi) that are greater than this value. By 
doing  a  binary  search  we  can  find  a  password  with  an  index  in 
each  component  of  the  base  structure  whose  probability  is  the 
closest  one  to  thp/pi,  and  thus  calculate  the  number  of  guesses 
with probability greater than this value. We do this for each base 
structure. This will also give us  a table of the probabilities with 
the associated total number of guesses such as Table 2. 
112
Table 2. Thresholds for the training_pswd_checker grammar 
Total number of 
guesses 
Probability 
values 
Time (in hours) 
(On 2.4GHz 
Intel Core 2 Duo, 
MD5 hash) 
1  
8  
12 
16  
24  
48  
72  
96  
104  
the  probabilistic  context-free  grammar  of 
1.31 × 10-11 
1.59 × 10-12 
1.20  × 10-12 
6.37 × 10-13 
2.96 × 10-13 
9.94 × 10-14 
6.70 × 10-14 
5.29 × 10-14 
4.70 × 10-14 
1,800,000,000 
14,400,000,000 
21,600,000,000 
28,800,000,000 
43,200,000,000 
86,400,000,000 
129,600,000,000 
172,800,000,000 
187,200,000,000 
3.2  The AMP Reject Function 
AMP  starts  with  asking  users  to  enter  their  chosen  password. 
Using 
the  pre-
processing  phase,  we  calculate  the  probability  of  the  chosen 
password as follows:  parse the given password to its components. 
For  example  if  the  password  is  Alice123!,  we  parse  it  to 
L5M5D3S1.  Next,  we  find  the  probability  of  the  base  structure 
L5D3S1 along with the probabilities of alice, 123, ! and the mask 
ULLLL. The product of these probabilities is the probability of the 
user’s  password.  This  probability  pu  is  compared  with  the 
threshold  value  to  accept  or  reject  the  password.  An  issue  that 
might  occur  in  this  phase  is  not  being  able  to  determine  the 
probability  of  pu  from  the  context-free  grammar.  This  could 
happen if the base structure or some other components of the user-
chosen  password  is  not  included  in  the  derived  context-free 
grammar. Here is how we handle some of these situations for each 
component:  If  the  base  structure  of  the  user-chosen  password  is 
not  included  in  the  context-free  grammar,  we  can  either  assume 
that the password is strong enough and accept it, or we can find 