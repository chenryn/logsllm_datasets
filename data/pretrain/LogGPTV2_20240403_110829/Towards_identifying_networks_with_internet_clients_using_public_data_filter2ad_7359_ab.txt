worldwide that anycast routes to different PoPs. The query dig
@8.8.8.8 o-o.myaddr.l.google.com -t TXT returns which PoP
is reached. We run our measurements from AWS and Vultr cloud
VMs around the world to cover 22 Google Public DNS PoPs (red dots
in Figure 1), out of 45 listed by Google. We tested all AWS regions
and reached 16 PoPs, plus 6 more from Vultr. Our measurements
1Other major public resolvers such as Cloudflare‚Äôs do not display this behavior.
Figure 1: Density of active prefixes identified by our cache probing.
Figure 2: CDF of cache hits where the prefix is ‚â§ ùëã km from the PoP.
include PoPs in the United States (seven states) and Canada (two
provinces), Asia (five countries/regions), Europe (five countries),
South America (two countries), and Australia. The 22 PoPs we cover
account for 95% of Google Public DNS queries to Microsoft services.
We verified with Google that some PoPs we did not reach are not
active, and 18 of the 23 PoPs we do not cover do not issue any
queries to Microsoft, suggesting they may be inactive. Figure 5 in
Appendix A.1 shows the PoPs we do not measure.
To reduce measurements, we do not query for each prefix at
each Google Public DNS PoP, instead querying for a prefix only at
its most likely PoPs. Since anycast routes most clients to nearby
PoPs for Google Public DNS [23], we use MaxMind to map each /24
prefix to a geolocation (which should be accurate enough for the
user prefixes of interest to us). We first query each PoP with 78,637
prefixes selected randomly out of all the public IPv4 address space
for which MaxMind indicates an error radius smaller than 200 km.
For each PoP, we then determine the radius that contains 90% of
those prefixes that return a cache hit for at least one of the four
most popular domains in the Alexa top global sites list that both
support ECS and have TTLs greater than one minute. We consider
this radius to be a likely service radius for that PoP. In the rest of
our measurements, we query a PoP only for prefixes that MaxMind
places as possibly within the PoP‚Äôs service radius (combining the
MaxMind location and error radius for the prefix). Figure 2 justifies
this approach‚Äîfor three PoPs with diverse geographies, the service
radius ranges from 478 km to 3273 km. Using per-PoP service radii
results in an average of 2.4 million /24 prefixes to probe at each
PoP, compared to an average 4.4 million if we used the maximum
service radius of 5,524 km (used for Zurich) for all PoPs.
Probing details. We select the four most popular domains from
the Alexa top sites global list that both support ECS and have
TTLs greater than one minute (as of 9/22/2021): www.google.com
(rank 1), www.youtube.com (rank 2), facebook.com (rank 7), and
755
Probed Google Public DNS PoP0101102103104105Prefix count050010001500200025003000350040004500Distance to Google Public DNS PoP (km)0.00.10.20.30.40.50.60.70.80.91.0Cumulative fraction ofprefixes with cache hits90th PercentilePoP locationGroningen, NLDalles, USCharleston, USIMC ‚Äô21, November 2‚Äì4, 2021, Virtual Event, USA
Weifan Jiang, Tao Luo, Thomas Koch, Yunfan Zhang, Ethan Katz-Bassett, and Matt Calder
www.wikipedia.org (rank 13). We also include one popular do-
main hosted by Microsoft Azure Traffic Manager that supports
ECS and has a TTL of 5 minutes, which we will use to validate our
methodology. We issued cache probes for 120 hours at a rate of 50
prefixes per second per domain at each PoP, looping over the list of
assigned prefixes continuously. Since Google Public DNS employs
multiple independent cache pools at each PoP [31], we issue 5 re-
dundant queries for each ‚ü®PoP, prefix, domain‚ü© combination to
increase the likelihood that our queries can cover multiple caches.
We queried Google Public DNS through DNS over TCP instead
of UDP, as probing the same domains repeatedly using UDP trig-
gers a rate limit much lower than the normal 1,500 QPS limit. We
consider a prefix active if Google Public DNS returns a cache hit
for any domain indicating the prefix with return scope > 0 for an
ECS query (a return scope of 0 indicates the cache entry was for
the whole address space rather than for a particular prefix). In the
future (¬ß6), we will assess how queries for different domains with
varying popularity, TTLs, and user bases can be combined to obtain
a rich picture of the types and relative activity levels for a network.
Strengths and Limitations. Google Public DNS cache prob-
3.1.2
ing can be replicated by anyone, without requiring any privileged
access or data. It directly measures (likely) active client prefixes,
rather than measuring activity from recursive resolvers or another
proxy of client activity. It also lends itself to developing rich signals
by combining observations across time and domains. However, the
approach has limitations. It measures active use of Google Public
DNS (and not of other recursive resolvers), which is popular (¬ß3.1)
but may have skewed adoption along various dimensions. DNS‚Äôs
use of recursive resolvers and caching also introduces complexities
in comparing activity levels across ECS prefixes. It is likely impos-
sible to quantify how much a DNS record was used from cache
within its TTL, and cross-prefix comparisons are tricky because of
differences in addressing (including NAT) within an ECS prefix.
3.2 Crawling DNS for Chromium Queries
We call our second approach DNS logs. We look for DNS queries
matching a signature of the Chromium web browser codebase,
which is part of browsers including Chrome, Microsoft Edge, Brave,
and Opera. The number of Chromium DNS queries from a prefix is
intuitively an indicator of the level of client activity.
3.2.1 Methodology. Chromium detects DNS interception by query-
ing for random strings of 7-15 lowercase letters [35]. Chromium
sends queries when the browser starts, and when the device‚Äôs IP
address or DNS configuration changes. Because these queries often
have no valid TLD appended (e.g., COM), they should not result
in cache hits at recursive resolvers, so the queries go to a DNS
root server [35]. To separate Chromium queries from others (e.g.,
‚Äúsdhfjssf‚Äù vs. ‚Äúcolumbia‚Äù), we use the heuristic that randomly gener-
ated strings likely have few collisions. Using empirical simulations,
we found Chromium queries would collide fewer than 7 times per
day across all roots with 99% probability.
We look for queries matching this pattern queried less often than
our daily threshold in the DITL traces, 2 days of traces of queries to
most root DNS servers [15]. The queries in the traces contain the
IP address of the querier, which is generally the recursive resolver
used by the Chromium client. We consider a matching query to be
a strong indicator that a recursive resolver with that IP address is
used by a user of a Chromium browser. We process J, H, M, A, K
and D root, the roots that offer un-anonymized, complete traces in
the most recent available (2020) DITL.
3.2.2
Strengths and Limitations.
A direct, precise signal with global coverage. Using Chromium
queries as a proxy for client activity provides per-resolver counts
proportional to the number of clients, assuming counts over large
populations are proportional to the number of clients. Counting
Chromium queries offers truly global coverage‚Äîif a recursive re-
solver forwards Chromium queries to the roots, they are in DITL.
Most major browsers use Chromium, and the market share is grow-
ing. Counting Chromium queries can be done by many researchers
(through access to DNS-OARC, or via collaboration with a root
deployment, some of which are hosted by academic institutions).
But . . . it‚Äôs not perfect. First, IP addresses seen in root DNS packet
traces are of recursive resolvers, and so Chromium queries provide
a signal of client activity at the recursive resolver level rather than
at the prefix or AS level.
Second, user activity and the presence of Chromium queries
are not perfectly correlated. The analysis excludes users of Safari
and Firefox. Chromium queries are (only) executed each time the
browser starts up, and each time the system‚Äôs IP address or DNS
configuration changes [35]. Also, DITL traces are only available
yearly and do not contain all root letters, so time-based analysis
is not possible from DITL alone. Moreover, the implementation of
this feature is subject to change. Since Chromium queries cause a
considerable load on the root DNS, the Chromium team has shown
interest in reducing the number of DNS queries going to the root
DNS [36]. We verified in September 2021 with B root that a few
percent of all B root queries are Chromium queries, although that
number is only 30% of what it was in 2020.
4 VALIDATION & CROSS-COMPARISON
Datasets. We compare client activity indicators obtained using
cache probing and DNS logs to measures of activity used in
prior work. First, we compare our results to APNIC user estimates
(APNIC), which use a heuristic based on Google Ad volumes to
generate user population estimates by AS. APNIC is publicly avail-
able, so it is useful to see how these new methodologies augment
existing, widely accessible methods of estimating activity. Second,
we compare three private datasets which contain measures of client
activity for two popular Microsoft Azure services: CDN and DNS
Traffic Manager. These services are used by billions of users in tens
of thousands of ASes and hundreds of countries/regions daily. The
first measure (Microsoft clients) is proportional to the number
of times clients access the CDN, aggregated by client IP address.
The second (Microsoft resolvers) is a count of client IP addresses
that the CDN observes using each recursive resolver, aggregated
by recursive resolver IP address. The third (cloud ECS prefixes)
is the set of ECS prefixes observed in DNS queries for authoritative
records of Traffic Manager, Azure‚Äôs DNS-based load balancing for
cloud tenants. We aggregate the data by prefix and by AS.
756
Towards Identifying Networks with Internet Clients Using Public Data
IMC ‚Äô21, November 2‚Äì4, 2021, Virtual Event, USA
DNS activity is a good proxy for web client activity. To demon-
strate that DNS-based techniques like ours can be a good proxy for
identifying Internet clients, we compare a full day of (1) /24 prefixes
from Microsoft clients (no ECS) with (2) ECS prefixes from cloud
ECS prefixes. The CDN sees HTTP requests from prefixes includ-
ing ones responsible for 97.2% of the DNS queries. Prefixes seen in
the ECS queries are responsible for 92% of the HTTP requests to
the CDN. This large overlap shows that client prefixes with DNS
activity usually have HTTP(S) activity, and our techniques will be
able to identify most prefixes with web activity (the goal) if they are
able to identify most prefixes with DNS activity (what they measure).
Not all prefixes that query CDNs are human users: both services
see some bot, crawler, and machine-to-machine traffic.
Cache probing recovers most DNS activity. We compare the results
of our cache probing for a popular Microsoft Azure domain with
ground truth ECS data observed at its authoritative resolver. Our
cache probing includes 91% of the ground truth ECS/24 prefixes,
showing that our approach can uncover the vast majority of a
service‚Äôs client population that is using Google Public DNS.
AS-level results. We consider the overlap in ASes detected as
hosting clients by our two techniques and the ones we compare to.
Although our methods identify activity at finer granularities, we
start with AS-level comparisons to compare with APNIC. Table 3 in
Appendix B.1 presents pairwise comparisons. In total, 66,804 ASes
were in at least one dataset, with 64,766 of those (97%) being in the
Microsoft clients. The APNIC dataset, despite being widely used,
misses 64% of the ASes observed as hosting Microsoft clients.
Our techniques perform better, missing only 40.1% (DNS logs) and
44.5% (cache probing), and recovering 74.2% and 81.9% of the ASes
observed by APNIC. DNS logs detects about the same number of
ASes as cache probing (39,652 vs 36,989), and the overlap between
them is fairly low‚Äîcombined, they detect 51,859 ASes. The low
overlap could result from DNS logs measuring some ASes that
host clients‚Äô recursive resolvers but not clients. The low overlap
means that combining our datasets yields more overlap with others.
For example, cache probing ‚à™ DNS logs observes 21,866 ASes
in APNIC (93.8%) and 50,006 ASes in Microsoft clients (77.2%).
To help understand the ASes our techniques detect as hosting web
clients but that APNIC does not consider as hosting customers,
we consider what types of ASes they are, according to ASdb [38].
Of all 29,973 ASes detected by our methods but absent in APNIC,
ASdb categorizes 27,773 (92.7%). Of these, 10,998 (39.5%) are Internet
Service Providers (ISPs). Outside the ISPs, 4,823 (17.4%) are host-
ing/cloud providers (which may reflect non-human web clients),
and 1,723 (6.2%) are schools, which likely host human users.
The ASes we miss are generally small. ASes that at least one
of our techniques identifies as hosting clients account for 98.8%
of the Microsoft clients queries (compared to 92% for APNIC).
Table 4 in Appendix B.2 presents pairwise comparisons by volume.
Although our DNS logs technique only identifies 74.2% of the AP-
NIC ASes, those ASes account for 97.6% of the world‚Äôs Internet
population, per APNIC estimates. ASes that include prefixes that
cache probing detected as active also account for 97.6% of AP-
NIC‚Äôs Internet population. Figure 3 breaks this analysis down per
country. In most countries, cache probing uncovers client activity
in ASes that APNIC identifies as hosting all or almost all of the
Figure 3: Fraction of a country‚Äôs Internet users (according to APNIC)
in ASes where our cache probing technique identified client activity.
We identified most eyeballs in most countries, including‚âà 100% in the
U.S., 99% in India, and 98% in China.
Internet users. Many of the larger countries (in terms of # of users)
where cache probing coverage is worse are in South America,
even though our probes covered all Google PoPs in South America
and most in the southern United Stages (Appendix A.1).
Despite these gaps, our cache probing results in global cover-
age: Figure 1 plots the MaxMind geolocations of prefixes where
cache probing detects activity. For each prefix with return scope
larger than /24, we make the simplifying assumption that all its /24
subprefixes are active. For (rare) return scopes smaller than /24, we
assume the entire /24 prefix it belongs to is active. Our measure-
ments infer more activity in some regions than others, e.g. Europe
is more active than China, although we cannot easily differentiate
how much this is a result of differences in prefix allocation policies,
Google Public DNS use, popularity of the domains we probe, or
coverage of our vantage points. Within a region, the distribution of
active prefixes often roughly follows the distribution of population.
For example, activity in the US and Brazil is densest near coasts.
Figure 4: Fraction of AS‚Äôs prefixes detected as active by cache probing.
Prefix-level measurements reveal variations within and across ASes.
Figure 4 depicts the fraction of /24 prefixes announced by each AS
that our cache probing technique detects as active (out of all the
AS announces [1]). When Google Public DNS returns a cache hit for
a prefix with scope bigger than /24, we know at least one /24 in the
prefix has client activity, but we cannot infer exactly which or how
many. So, we estimate upper and lower bounds. The lower bound
757
01011021031041051061071081091010Number of Internet users in country (by APNIC)0.00.10.20.30.40.50.60.70.80.91.0Fraction of APNIC Internet populationseen by cache probingBrazilBoliviaArgentinaPeruEcuadorParaguayUruguayColombiaChileVenezuelaSuriname51015202530Number of countries0.00.10.20.30.40.50.60.70.80.91.0Fraction of /24 prefixes detected as active0.00.10.20.30.40.50.60.70.80.91.0Cumulative fraction of ASesLower boundUpper boundIMC ‚Äô21, November 2‚Äì4, 2021, Virtual Event, USA
Weifan Jiang, Tao Luo, Thomas Koch, Yunfan Zhang, Ethan Katz-Bassett, and Matt Calder
cache probing
9712.2K (100.0%)
650.5K (94.0%)
9712.2K (99.6%)
6614.4K (74.7%)
932.6K (96.4%)
DNS logs
650.5K (6.7%)
692.2K (100.0%)
692.2K (7.1%)
661.2K (7.5%)
419.8K (43.4%)
cache probing ‚à™ DNS logs Microsoft clients Microsoft resolvers
932.6K (9.6%)
419.8K (60.6%)
954.1K (9.8%)
940.5K (10.6%)
967.7K (100.0%)
6614.4K (68.1%)
661.2K (95.5%)
6647.8K (68.2%)
8849.9K (100.0%)
940.5K (97.2%)
9712.2K (100.0%)
692.2K (100.0%)