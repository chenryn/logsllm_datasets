# 通过predict方法进行模型调用    response = predictor.predict(data)甚至对于大批量数据的场景，你还可以借助 **transformer机制**，离线地进行模型的批量调用。    transformer = estimator.transformer(instance_count=1, instance_type='ml.p2.xlarge')    
# 批量调用数据    transformer.transform('s3://my-bucket/batch-transform-input')    transformer.wait()    
# 随后可以从transformer.output_path中下载结果数据你看，SageMaker的这些功能还是很贴心的，它大大地简化了模型的部署和推理调用，相当程度地解决了模型开发者未必熟悉的Web 服务的工程性问题。好了，对于机器学习基础设施服务，我们就讨论到这里。你可以通过下面的SageMaker架构流程图，来加深对这个支撑体系的印象。这一类云服务在你构建自有模型时，能帮你提供**一站式的解决方案**，能在多个核心环节给予你鼎力的支持。![](Images/3cc8dc45438385614d2f0d916934ca8e.png)savepage-src="https://static001.geekbang.org/resource/image/de/76/defd655d3a7650a1008d86f3d4cce276.jpg"}AWS SageMaker 架构流程图（来自slate-object="mark"}AWS 官网slate-object="mark"}slate-object="inline"）slate-object="mark"}课堂总结与思考当今世界的现代应用程序，如果其中没有一点 AI的元素，恐怕都会不好意思发布了。与这样的趋势相匹配，云平台都希望成为构建和运行新一代智能应用程序的最佳平台。你可以根据自己的需求，直接使用云上 AI服务中琳琅满目的内置模型，也可以利用云上机器学习平台，来高效地构建你自己的智能模型。云上AI 平台还提供了很好的管理手段，来保存和使用公司的数据和模型资产，让你的AI工作朝着规范化、工程化的方向发展。AI技术博大精深，你当然需要通过专门的课程去系统学习。而我们这一讲的价值在于，主要关注了云对于AI任务在不同层面的结合点和支撑点，包括模型、数据、算法、计算资源、部署推理等等。通过今天的介绍，希望你在云上实践时，能够知道如何按图索骥，在某个细分的AI场景进行深入的尝试。期待你的下一个智能应用。最后作为惯例，我们还是要谈一谈这里的风险，主要仍旧是**厂商绑定的问题**。如果你比较关注可迁移性，那么在使用云上 AI服务时，你就需要注意甄别，哪些是云的自有生态，哪些是开源组件。当然，云厂商其实也在不断升级，努力地让云上AI服务从完全内置的黑盒到逐渐走向开放和兼容。最终让每一个环节能够拆开单独使用并且互相解耦，这是未来的一个发展趋势。补充：我们也不要"谈绑色变"，绑定是正常的商业选择，也常会给用户带来效率的提升。云计算的很多服务和开源世界的若即若离，其本质是在生态发展和客户黏性，在技术普惠和商业利益中，不断进行着博弈和平衡。slate-object="mark"}**这一讲，我留给你的课后问题是：**1.  作为模型构建的重要组成部分，还有一个"调参"（Hyperparameter    Tuning）的阶段，它也是一件困难而又麻烦的事情。你知道调参具体是指什么意思吗？在这方面，云上    AI 服务能够提供帮助吗？        2.  前面我们谈到了可迁移性的问题，它不仅是指代码，也包括训练好的模型。那么，外部训练好的模型能够放置到云上    AI    服务中吗？在云上训练好的模型，又能不能取下来，放到本地环境中运行呢？        欢迎你给我留言和参与讨论。如果你觉得今天的内容有帮助，也欢迎把这篇文章分享给你的朋友。好了，至此我们 PaaS篇的内容就全部结束了。我是何恺铎，感谢你的阅读和陪伴。我还有许多想说的话，让我们在结束语中再见。