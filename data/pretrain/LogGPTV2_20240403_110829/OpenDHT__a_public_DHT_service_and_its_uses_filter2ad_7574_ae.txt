7368
8035
933
872
940
770
804
531
328
311
249
131
Table 3: Queuing times in milliseconds for each of the clients in the multiple size and TTL tests. Sizes are in bytes; TTLs are in minutes.
A “bid” of 1.0 indicates that a client is putting often enough to ﬁll 1/15th of the disk in an otherwise idle system.
non-starvation property of the algorithm allows it to intelligently
choose which data to let expire and which to renew.
As new clients arrive, the put rate is further subdivided. One
maximum TTL after clients stop arriving, each client is allocated
its fair share of the storage available on disk.
Our second experiment demonstrates fairness and high utiliza-
tion when clients issue puts with various sizes and TTLs. In ad-
dition, it also shows that clients putting at or below their fair rate
experience only slight queuing delays. The maximum TTL in this
experiment is one hour, giving a disk capacity of 3.4 MB (3600×
1000 bytes).
We consider three tests, each consisting of 15 clients divided
into three groups, as shown in Table 3. All the clients in a group
have the same total demand, but have different put frequencies, put
sizes, and TTLs; e.g., a client submitting puts with maximum size
and half the maximum TTL puts twice as often as a client in the
same group submitting puts with the maximum size and TTL.
The clients in Groups 2 and 3 put at the same rate in each test.
The clients in Group 3 are light users. Each of these users demands
only 1/30th of the available storage. For example, Client 11 sub-
mits on average a 1000-byte, maximum-TTL put every 30 seconds.
As the fair share of each client is 1/15th of the disk, the puts of
the clients from Group 3 should be always accepted. The clients in
Group 2 are moderate users, putting at exactly their fair share. For
example, Client 6 submits on average one 1000-byte, maximum-
TTL put every 15 seconds.
The clients in Group 1 put at a different rate in each test. In Test
1, they put as the same rate as the clients in Group 2. Since clients
in Groups 1 and 2 put at their fair share while the clients in Group 3
put below their fair share, the system is underutilized in this test.
In Tests 2 and 3, the clients of Group 1 put at twice and three times
their fair rate, respectively. Thus, in both these tests the system is
overutilized.
Figure 5 and Table 3 summarize the results for this experiment.
Figure 5 shows the storage allocated to each client versus time.
As expected, in the long term, every client receives its fair share of
storage. Moreover, clients that submit puts with short TTLs acquire
storage more quickly than other clients when the disk is not full yet.
This effect is illustrated by the steep slopes of the lines representing
the allocations of some clients at the beginning of each test. This
behavior demonstrates the beneﬁt of using the admission control
test to rate-limit new put requests: looking back at Figure 3, one
can see that many puts with short TTLs can be accepted in a mostly-
empty disk without pushing the value of f (τ) over C.
Table 3 shows the queuing delays experienced by each client.
This delay is the time a put waits from the moment it arrives at the
node until it is accepted. There are three points worth noting. First,
as long as the system is underutilized every client experiences very
low queuing delays. This point is illustrated by Test 1.
Second, even when the system is overutilized, the clients that is-
sue puts at below or at their fair rate experience low queuing delays.
For example, the clients in Group 3 (i.e., Clients 11-15) which issue
puts below their fair rate experience average queuing delays of at
most 531 ms, while the clients in Group 2 (i.e., Clients 6-10) which
issue puts at their fair rate experience average queuing delays no
larger than 1 second. One reason clients in Group 3 experience
lower queuing delays than clients in Group 2 is the use of param-
eter α in the computation of the start times (Eqn. 2). Since clients
in Group 3 have fewer puts stored than those in Group 2, there are
simply more cases when the start times of puts of clients in Group 3
r
o
)
s
(
t
n
u
o
C
e
r
u
l
i
a
F
y
c
n
e
t
a
L
t
e
G
 1000
 100
 10
 1
 0.1
Median Latency
95th Percentile Latency
Failures
PlanetLab
V3 Rollout
09/28
10/12
10/26
11/09
11/23
12/07
12/21
01/04
01/18
02/01
Figure 6: Long-running performance and availability of OpenDHT . See text for description.
are computed based on the system virtual time (i.e., v(·)−α) rather
than on the ﬁnish times of the previous puts.
Third, clients that are above the fair rate must wait their turn
more often, and thus experience higher, but not unreasonable, queu-
ing delays.
5. DEPLOYMENT AND EVALUATION
In this section we evaluate both the performance and the usability
of OpenDHT.
Much of OpenDHT’s routing and storage layers builds on prior
efforts. We use the Bamboo DHT implementation for our routing
layer [29] and implement a soft-state storage layer atop it simi-
lar to that described in Cates’ thesis [8]. As such, in evaluating
OpenDHT’s performance in Section 5.1, we do not focus on the
detailed behavior of the underlying DHT routing or storage algo-
rithms, both of which have been evaluated over short periods else-
where [8, 10, 29]. Rather, we focus on the long-running perfor-
mance of OpenDHT in terms of data durability and put/get latency.
Although DHTs are theoretically capable of achieving high dura-
bility, we are aware of no previous long term studies of real (not
simulated) deployments that have demonstrated this capability in
practice.
As discussed in Section 3.2, the ReDiR library presents appli-
cations with a lookup interface. Since each ReDiR lookup is im-
plemented using at least one get operation, a lookup in ReDiR can
be no faster than a get in the underlying DHT. We quantify the
performance of ReDiR lookups on PlanetLab in Section 5.2. This
in situ performance evaluation is both novel (no implementation
of ReDiR was offered or evaluated in [19]) and essential, as the
validity of our claim that OpenDHT can efﬁciently support oper-
ations beyond put/get rests largely on the performance penalty of
ReDiR vs. standard lookup and routing interfaces.
Finally, OpenDHT’s usability is best demonstrated by the spec-
trum of applications it supports, and we describe our early experi-
ence with these in Section 5.3.
5.1 Long-Running Put/Get Performance
In this section we report on the latency of OpenDHT gets and the
durability of data stored in OpenDHT.
Measurement Setup OpenDHT has been deployed on PlanetLab
since April 2004, on between 170 and 250 hosts. From August
2004 until February 2005 we continuously assessed the availabil-
ity of data in OpenDHT using a synthetic put/get workload.5 In
this workload, a client puts one value into the DHT each second.
5During the PlanetLab Version 3 rollout a kernel bug was intro-
duced that caused a large number of hosts to behave erratically until
it was ﬁxed. We were unable to run OpenDHT during this period.
Value sizes are drawn randomly from {32, 64, 128, 256, 512, 1024}
bytes, and TTLs are drawn randomly from {1 hour, 1 day, 1 week}.
The same client randomly retrieves these previously put data to as-
sess their availability; each second it randomly selects one value
that should not yet have expired and gets it. If the value cannot be
retrieved within an hour, a failure is recorded. If the gateway to
which the client is connected crashes, it switches to another, resub-
mitting any operations that were in ﬂight at the time of the crash.
Results Figure 6 shows measurements taken over 3.5 months of
running the above workload. We plot the median and 95th per-
centile latency of get operations on the y axis. The black impulses
on the graph indicate failures. Overall, OpenDHT maintains very
high durability of data; over the 3.5 months shown, the put/get test
performed over 9 million puts and gets each, and it detected only
28 lost values. Get latency is good, although there is some room for
improvement. Some of our high latency is due to bugs; on Febru-
ary 4 we ﬁxed a bug that was a major source of the latency “ramps”
shown in the graph. On April 22 (not shown) we ﬁxed another and
have not seen such “ramps” since. Other high latencies are caused
by Internet connectivity failures; the three points where the 95th
percentile latency exceeds 200 seconds are due to the gateway be-
ing partially partitioned from the Internet. For example, on January
28, the PlanetLab all-pairs-pings database [32] shows that the num-
ber of nodes that could reach the gateway dropped from 316 to 147
for 20–40 minutes. The frequency of such failures indicates that
they pose a challenge DHT designers should be working to solve.
5.2 ReDiR Performance
We consider three metrics in evaluating ReDiR performance:
(1) latency of lookups, (2) ReDiR’s bandwidth consumption, and
(3) consistency of lookups when the registered nodes external to
OpenDHT churn. The ﬁrst two quantify the overhead due to build-
ing ReDiR over a put/get interface, while consistency measures
ReDiR’s ability to maintain correctness despite its additional level
of indirection relative to DHTs such as Chord or Bamboo.
Measurement Setup To evaluate ReDiR we had 4 PlanetLab
nodes each run n/4 ReDiR clients for various n, with a ﬁfth Planet-
Lab node performing ReDiR lookups of random keys. We selected
an OpenDHT gateway for each set of clients running on a particular
PlanetLab node by picking 10 random gateways from a list of all
OpenDHT gateways, pinging those ten, and connecting to the one
with lowest average RTT. We used a branching factor of b = 10
in all of our experiments, with client registration occurring every
30 seconds, and with a TTL of 60 seconds on a client’s (IP, port)
entries in the tree. Each trial lasted 15 minutes.
Results Our ﬁrst experiment measured performance with a stable
set of n clients, for n ∈ {16,32,64,128,256}. Figure 7 shows a
 1
 0.8
 0.6
 0.4
 0.2
n
o
i
t
c
a
r
f
e
v
i
t
a
l
u
m
u
C
 0
 0.01
OpenDHT gets
ReDiR lookups
 0.1
 1
 10
 100
Latency (seconds)
Figure 7: Latency of ReDiR lookups and OpenDHT gets.
t
n
e
t
s
i
s
n
o
C
t
n
e
c
r
e
P
 100
 98
 96
 94
 92
Lookup consistency
Bytes per lookup
Bytes/sec/client
 1
 2
 4
 8
 16
Median session time (min)
 800
 600
 400
 200
 0
 32
s
e
t
y
B
Figure 8: Percentage of ReDiR lookups that are consistent, bytes
transferred per lookup, and bytes/sec per registration process.
CDF of ReDiR lookup latency, based on 5 trials for each n. We
compare to the latency of the OpenDHT gets performed in the pro-
cess of ReDiR lookups. The average lookup uses ≈ 1.3 gets, indi-
cating that our tree depth estimation heuristic is effective. We have
veriﬁed this result in a simple simulator for up to 32,768 clients, the
results of which match our PlanetLab results closely within their
common range of n. Bandwidth use is quite low; even at the high-
est churn rate we tested, the average client registration process uses
≈ 64 bytes per second and a single lookup uses ≈ 800 bytes.
We next measured consistency as the rate of client churn varies.
We used 128 clients with exponentially distributed lifetimes. When-
ever one client died, a new client joined. We use Rhea et al.’s
deﬁnition of consistency [29]: ten lookups were performed simul-
taneously on the same key, the majority result (if any) is considered
consistent, and all others are inconsistent.
Figure 8 plots consistency as a function of median client life-
time. We show the mean and 95% conﬁdence intervals based on 15
trials. Despite its layer of indirection, ReDiR is competitive with
the implementation of Chord evaluated in [29], although Bamboo
performs better at high churn rates (note, however, that the exper-
iments of [29] were performed on ModelNet, whereas ours were
performed on PlanetLab).
In summary, these results show that lookup can be implemented
using a DHT service with a small increase in latency, with consis-
tency comparable to other DHTs, and with very low bandwidth.
5.3 Applications
We cannot directly quantify the utility of OpenDHT’s interface,
so in this section we instead report on our experience with build-
ing applications over OpenDHT. We ﬁrst give an overview of the
various OpenDHT-based applications built by us and by others. We
then describe one application—FreeDB Over OpenDHT (FOOD)—
in greater detail. FOOD is a DHT-based implementation of FreeDB,
the widely deployed public audio-CD indexing service. As FreeDB
is currently supported by a set of replicated servers, studying FOOD
allows us to compare the performance of the same application built
in two very different ways. We end this section with a brief discus-
sion of common feature requests from application-builders; such
requests provide one way to identify which aspects of OpenDHT
matter most during development of real applications.
5.3.1 Generality: Overview of Applications
OpenDHT was opened up for experimentation to “friends and
family” in March 2004, and to the general public in December
2004. Despite its relative infancy, OpenDHT has already been
adopted by a fair number of application developers. To gain ex-
perience ourselves, we also developed four different OpenDHT ap-
plications. Table 4 lists the known OpenDHT applications. We
make a number of observations: