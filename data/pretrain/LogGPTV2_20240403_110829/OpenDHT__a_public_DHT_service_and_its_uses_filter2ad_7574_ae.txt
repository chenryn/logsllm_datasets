### Table 3: Queuing Times in Milliseconds for Each Client in the Multiple Size and TTL Tests
Sizes are in bytes; TTLs are in minutes.

- **7368**
- **8035**
- **933**
- **872**
- **940**
- **770**
- **804**
- **531**
- **328**
- **311**
- **249**
- **131**

A “bid” of 1.0 indicates that a client is putting data frequently enough to fill 1/15th of the disk in an otherwise idle system. The non-starvation property of the algorithm allows it to intelligently choose which data to let expire and which to renew.

As new clients arrive, the put rate is further subdivided. One maximum TTL after clients stop arriving, each client is allocated its fair share of the storage available on disk.

### Experiment 2: Fairness and High Utilization
Our second experiment demonstrates fairness and high utilization when clients issue puts with various sizes and TTLs. Additionally, it shows that clients putting at or below their fair rate experience only slight queuing delays. The maximum TTL in this experiment is one hour, giving a disk capacity of 3.4 MB (3600 × 1000 bytes).

#### Test Setup
We consider three tests, each consisting of 15 clients divided into three groups, as shown in Table 3. All clients in a group have the same total demand but differ in put frequencies, put sizes, and TTLs. For example, a client submitting puts with the maximum size and half the maximum TTL will put twice as often as a client in the same group submitting puts with the maximum size and TTL.

- **Group 2 and Group 3 Clients**: These clients put at the same rate in each test.
- **Group 3 Clients**: Light users, each demanding only 1/30th of the available storage. For instance, Client 11 submits on average a 1000-byte, maximum-TTL put every 30 seconds.
- **Group 2 Clients**: Moderate users, putting at exactly their fair share. For example, Client 6 submits on average one 1000-byte, maximum-TTL put every 15 seconds.
- **Group 1 Clients**: Put at different rates in each test. In Test 1, they put at the same rate as the clients in Group 2. Since clients in Groups 1 and 2 put at their fair share while the clients in Group 3 put below their fair share, the system is underutilized in this test. In Tests 2 and 3, the clients of Group 1 put at twice and three times their fair rate, respectively, leading to overutilization.

### Results
Figure 5 and Table 3 summarize the results for this experiment.

- **Figure 5**: Shows the storage allocated to each client over time. As expected, in the long term, every client receives its fair share of storage. Clients that submit puts with short TTLs acquire storage more quickly than other clients when the disk is not full yet. This behavior demonstrates the benefit of using the admission control test to rate-limit new put requests.
- **Table 3**: Shows the queuing delays experienced by each client. There are three key points:
  1. **Underutilized System**: When the system is underutilized, every client experiences very low queuing delays, as illustrated by Test 1.
  2. **Overutilized System**: Even when the system is overutilized, clients that issue puts at or below their fair rate experience low queuing delays. For example, clients in Group 3 (i.e., Clients 11-15) experience average queuing delays of at most 531 ms, while clients in Group 2 (i.e., Clients 6-10) experience average queuing delays no larger than 1 second.
  3. **Above Fair Rate**: Clients that put above their fair rate must wait their turn more often, resulting in higher, but not unreasonable, queuing delays.

### 5. Deployment and Evaluation
In this section, we evaluate both the performance and usability of OpenDHT.

#### 5.1 Long-Running Put/Get Performance
We report on the latency of OpenDHT gets and the durability of data stored in OpenDHT.

**Measurement Setup**:
- **Deployment**: OpenDHT has been deployed on PlanetLab since April 2004, on between 170 and 250 hosts.
- **Workload**: From August 2004 until February 2005, we continuously assessed the availability of data in OpenDHT using a synthetic put/get workload. A client puts one value into the DHT each second. Value sizes are randomly drawn from {32, 64, 128, 256, 512, 1024} bytes, and TTLs are randomly drawn from {1 hour, 1 day, 1 week}. The same client randomly retrieves these previously put data to assess their availability; each second it randomly selects one value that should not yet have expired and gets it. If the value cannot be retrieved within an hour, a failure is recorded.

**Results**:
- **Figure 6**: Shows measurements taken over 3.5 months. We plot the median and 95th percentile latency of get operations. Overall, OpenDHT maintains very high durability of data; over the 3.5 months, the put/get test performed over 9 million puts and gets each, detecting only 28 lost values. Get latency is good, although there is room for improvement. Some high latencies are due to bugs, and others are caused by Internet connectivity failures.

#### 5.2 ReDiR Performance
We evaluate ReDiR performance based on three metrics:
1. **Latency of lookups**
2. **ReDiR’s bandwidth consumption**
3. **Consistency of lookups when the registered nodes external to OpenDHT churn**

**Measurement Setup**:
- **Clients**: 4 PlanetLab nodes each run n/4 ReDiR clients for various n, with a fifth PlanetLab node performing ReDiR lookups of random keys.
- **Gateway Selection**: An OpenDHT gateway for each set of clients was selected by picking 10 random gateways, pinging them, and connecting to the one with the lowest average RTT.
- **Branching Factor**: b = 10, with client registration occurring every 30 seconds and a TTL of 60 seconds on a client’s (IP, port) entries in the tree. Each trial lasted 15 minutes.

**Results**:
- **Figure 7**: Shows the CDF of ReDiR lookup latency based on 5 trials for each n. The average lookup uses ≈ 1.3 gets, indicating that our tree depth estimation heuristic is effective.
- **Bandwidth Use**: Even at the highest churn rate, the average client registration process uses ≈ 64 bytes per second, and a single lookup uses ≈ 800 bytes.
- **Consistency**: Measured as the rate of client churn varies. Figure 8 plots consistency as a function of median client lifetime, showing mean and 95% confidence intervals based on 15 trials. Despite its layer of indirection, ReDiR is competitive with other DHTs in terms of consistency and bandwidth use.

#### 5.3 Applications
We report on our experience with building applications over OpenDHT.

**Overview of Applications**:
- **OpenDHT was opened** for experimentation to “friends and family” in March 2004 and to the general public in December 2004. Despite its relative infancy, OpenDHT has been adopted by several application developers. We also developed four different OpenDHT applications, listed in Table 4.

**Case Study: FreeDB Over OpenDHT (FOOD)**:
- **Description**: FOOD is a DHT-based implementation of FreeDB, a widely deployed public audio-CD indexing service. Comparing FOOD to the current replicated server implementation of FreeDB allows us to evaluate the performance of the same application built in two very different ways.
- **Common Feature Requests**: Feedback from application builders provides insights into the aspects of OpenDHT that matter most during the development of real applications.

This structured and detailed approach ensures clarity and coherence in presenting the experimental setup, results, and evaluation of OpenDHT.