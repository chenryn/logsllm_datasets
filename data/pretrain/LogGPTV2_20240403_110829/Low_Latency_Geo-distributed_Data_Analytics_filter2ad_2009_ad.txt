lines.
6.2 EC2 Deployment
We used four workloads to evaluate Iridium on EC2.
(1) Conviva Video Analytics: We use queries from
Conviva, a video delivery and monitoring company. Data
from clients (e.g., the edge/CDN serving them, their
ISP and network characteristics) are analyzed to mod-
ify the parameters of video sessions (e.g., codec, buﬀer
sizes) to improve performance (re-buﬀering ratio [21]).
The queries contain a mixture of aggregation (“reduce”)
and table-joins. Every query has 160GB input.
(2) Microsoft Bing Edge Dashboard: Microsoft’s
Bing service maintains a running dashboard of its edge
servers deployed worldwide. The queries aggregate data
from 40, 000 raw counters ﬁltered by a range of location
(lat/long, city), user-id, etc. values to produce aver-
age and 90th percentiles. This is also an example of a
streaming query that we execute using Spark Stream-
ing’s “mini-batch” model [60] in every time period.
(3) TPC-DS Benchmark: The TPC-DS benchmark
is a set of decision support queries [12] based on those
used by retail product suppliers such as Amazon. These
OLAP queries examine large volumes of data (215GB)
each, and are characterized by a mixture of compute
and disk/network load, the latter of relevance to us.
(4) AMPLab Big-Data: The big-data benchmark [4]
is derived from workloads and queries from [45] with
identical schema of the data. The suite contain a mix
of Hive and Spark queries. The queries contain simple
scans, aggregations, joins, and UDF’s.
In our inter-region EC2 experiment, we use band-
widths naturally available to the instances on the sites.
In our 30-site setup, we vary the bandwidths between
100Mb/s to 2Gb/s (Linux Traﬃc Control [10]), hop-
ing to mimic the heterogeneous bandwidths across edge
clusters and DCs available for analytics frameworks.
Figure 5a plots our average gains for the four work-
loads across eight-region EC2 regions. Gains compared
to the in-place and centralized baselines range from 64%
to 92% (3× to 14×). Singapore, Tokyo and Oregon
(US) had 2.5× higher bandwidth than Virgina (US)
and Frankfurt, and 5× higher bandwidth than Sydney,
020406080100Iridium vs. CentralizedIridium vs. In-placeConvivaBing EdgeTPC-DSBig-DataReduction (%) in Query Response Time3x3x3x3x5x5x5x5x----14x14x14x14x6x6x6x6x----9x9x9x9x3x3x3x3x020406080100Iridium vs. CentralizedIridium vs. In-place4444xxxxConvivaBing EdgeTPC-DSBig-DataReduction (%) in Query Response Time3x3x3x3x----10x10x10x10x4x4x4x4x----19x19x19x19x3x3x3x3x----7x7x7x7xIridium vs.
In-place Centralized
Iridium vs.
Core
Core + Query Lag
Core + Query Lag
+ Contention
Core + Contention
26%
41%
59%
45%
32%
46%
74%
53%
Table 2: Progression of Iridium’s gains as additional
features of considering query lag and contention be-
tween query/data movements are added to the basic
heuristic. (Facebook workload)
Sao Paulo and Ireland. Iridium automatically adjusts its
data and task placement away from these sites to avoid
unduly congesting their links during query execution.
Our gains are similar (but a bit higher) with our 30-
site setup at 3 × −19×. Note that since the Bing Edge
query is a streaming operation executed as a series of
“mini-batch” queries, we report the gains per batch.
Gains compared to the centralized baseline is higher
than with the in-place baseline for all but the Conviva
workload. This is because the intermediate data size
is closer to the input size (α) in the Conviva workload,
which makes central aggregation less hurtful. In addi-
tion, the Conviva and Big-data queries also have a more
intensive map stage (during which we do just as good
as our baselines via data locality) that relatively brings
down the opportunity and gains for Iridium. Finally,
the Conviva and Bing Edge queries have lesser skew in
their map outputs which limits the value of Iridium’s task
placement compared to the in-place baseline.
Overheads: Micro-benchmarks show that our data
placement iterations are eﬃcient to implement, ﬁnish-
ing in under 100ms for up to 200 sites; the LP used
(§3.1) calculates fraction of tasks (ri), thus the number
of tasks do not matter in its calculation.
6.3 Trace-driven Simulation
In this section, we present simulation results based
on the production trace from Facebook’s Hadoop clus-
ter. We use bandwidth values 100Mb/s to 2Gb/s in our
simulator, similar to §6.2, but we also present a result
when the bandwidths are higher with lower heterogene-
ity later ({10, 50} Gb/s), indicative of only large DCs.
Compared to baselines of leaving data in place and
central aggregation, Iridium improves average response
time by 59% and 74%, respectively.
Table 2 shows the progression in gains as we start
with the basic data placement heuristic, and incremen-
tally add the usage of query lag in the score for datasets,
and consideration of contention between query/data traf-
ﬁc. The basic heuristic itself starts with fairly moderate
gains, but jumps by a factor of 1.5× when we consider
the lag, and a further 1.5× with modeling contentions
with query traﬃc. The ﬁnal result is also signiﬁcantly
better than adding either one of the features. These
results seek to underline the use of query lag in dif-
ferentiating between datasets, and avoiding contention
with running queries.
(a) In-place baseline
(b) Centralized baseline
Figure 6: CDF of Iridium’s gains with the Facebook
workload. We also compare our two techniques—
task placement and data placement—standalone.
Also, keeping data in place is a more stringent base-
line than the common approach of central aggregation.
This is because reduction in data in intermediate stages
(α 1020406080100%QueriesImprovement (%)Dataset Popularity (#Access)100[51 –100] 020406080100%QueriesImprovement (%)Query Size (#Tasks)1000020406080100%QueriesImprovement (%)Cross-site Data Skew (Coefficient of variation)2follows: min z, s.t. (cid:80)
the time it takes to send data from site i to site j;
Tij(rj) = Sirj/Bij. The LP to compute z, the mini-
mal shuﬄe duration, and the corresponding ri’s is as
i ri = 1 and ∀i (cid:54)= j : Tij(rj) ≤ z.
Redesigning the data placement heuristic, however, is
more challenging and requires careful consideration.
Local minima and greedy approach: As we alluded
to in §4, the joint problem of data and task placement
is non-convex. This means that the greedy approach
adopted by our heuristic may get stuck in local minima.
Overcoming them requires exploring potential options
that could increase query response time temporarily be-
fore bringing it down. While our gains are signiﬁcant
even with the greedy solution, depending on the lag and
bandwidth available for moving data, one could con-
ceive a larger move with much more signiﬁcant gain.
Extending our heuristic to overcome local minima is
part of future work.
8. RELATED WORK
1) Distributed databases: While big-data frameworks
currently operate only within a single cluster, work on
distributed databases has been a popular topic [22, 27];
see surveys in [40, 44]. Google Spanner [28] is an in-
stance of a distributed database deployed at scale. Our
problem is simpler because we do not have to deal with
concurrency and deadlocks; data processing systems are
typically append-only. This gives us more freedom to
move data across sites. JetStream [46], a stream pro-
cessing system for OLAP cubes, uses data aggregation
and adaptive ﬁltering to support data analytics. How-
ever, unlike Iridium, JetStream does not support arbi-
trary SQL queries and does not optimize data and task
placement. Recent work [53, 54] optimize for WAN
bandwidth usage across sites. As we showed, this can
lead to poor query response times. In contrast, Iridium
optimizes for query response time and WAN usage using
a budget (§4.4 and §6.5).
2) Reducing data analytics responses: There is a large
body of work on improving query response time in data
parallel systems [20, 23, 29]. These systems improve