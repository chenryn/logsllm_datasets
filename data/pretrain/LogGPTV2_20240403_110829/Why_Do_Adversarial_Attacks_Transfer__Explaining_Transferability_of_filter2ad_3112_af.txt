every path of the tree, causing misclassiﬁcation.
Is gradient alignment an effective transferability metric?
In Fig. 8, we report on the left the gradient alignment com-
puted between surrogate and target models, and on the right
the Pearson correlation coefﬁcient ρ(ˆδ,δ) between the per-
turbation optimized against the surrogate (i.e., the black-box
perturbation ˆδ) and that optimized against the target (i.e., the
white-box perturbation δ). We observe immediately that gradi-
ent alignment provides an accurate measure of transferability:
the higher the cosine similarity, the higher the correlation
(meaning that the adversarial examples crafted against the
two models are similar). We correlate these two measures in
Fig. 6c, and show that such correlation is statistically signif-
icant for both Pearson and Kendall coefﬁcients. In Fig. 6d
we also correlate gradient alignment with the ratio between
the test error of the target model in the black- and white-box
setting (extrapolated from the matrix corresponding to ε = 1
in the bottom row of Fig. 7), as suggested by our theoretical
derivation. The corresponding permutation tests conﬁrm sta-
tistical signiﬁcance. We ﬁnally remark that gradient alignment
is extremely fast to evaluate, as it does not require simulating
any attack, but it is only a relative measure of the attack trans-
330    28th USENIX Security Symposium
USENIX Association
SVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLNNHNNLRFHRFLSVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLNNHNNL.15.08.13.11.11.10.04.04.07.07.49.51.22.13.20.18.15.15.06.07.12.12.53.54.19.09.17.14.12.12.04.05.09.09.50.52.21.11.19.16.14.13.05.05.10.10.52.53.07.04.06.05.08.06.02.02.03.03.42.43.13.07.12.10.13.12.03.03.06.06.48.50.19.11.17.15.13.13.06.06.11.11.52.53.22.13.20.17.15.14.07.07.12.12.53.54.20.10.18.15.13.12.05.05.10.10.51.52.21.11.19.16.13.13.05.05.10.10.52.53targeterror.03.02.02.02.03.03.01.01.01.02.02.02whitebox.96.19.89.601.00.83.17.10.34.19transferrate.16.21.18.19.11.15.19.20.18.19SVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLNNHNNLRFHRFLSVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLNNHNNL.52.35.51.50.51.46.22.17.37.29.52.53.72.66.73.74.79.78.49.47.63.55.56.57.62.46.62.62.66.61.32.26.48.37.53.54.68.55.69.69.74.71.39.33.55.45.54.55.18.07.16.12.27.18.04.04.07.07.48.48.43.25.43.41.69.63.17.13.28.21.51.52.63.56.65.65.71.68.46.40.57.48.55.56.70.64.71.72.77.76.51.49.64.56.56.58.66.54.67.67.68.65.38.33.54.44.54.55.68.56.69.69.72.70.40.35.56.46.54.55targeterror.03.02.02.02.03.03.01.01.01.02.02.02whitebox1.00.791.00.981.001.00.81.66.90.76transferrate.41.64.51.57.18.39.57.64.55.58SVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLNNHNNLRFHRFLSVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLNNHNNL.98.97.99.991.001.00.92.93.97.94.59.581.001.001.001.001.001.00.991.001.001.00.75.76.99.991.001.001.001.00.97.97.99.97.61.621.001.001.001.001.001.00.98.99.99.99.64.65.62.54.64.67.89.88.47.44.54.46.51.51.93.93.96.981.001.00.83.83.93.87.55.541.00.991.001.001.001.00.99.991.00.99.69.691.001.001.001.001.001.001.001.001.001.00.77.771.00.991.001.001.001.00.98.99.99.99.63.641.001.001.001.001.001.00.99.991.00.99.64.64targeterror.03.02.02.02.03.03.01.01.01.02.02.02whitebox1.001.001.001.001.001.001.001.001.001.00transferrate.91.96.93.94.60.86.95.96.93.94SVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLNNHNNLRFHRFLSVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLNNHNNL.09.05.08.07.07.06.02.02.03.05.43.45.28.14.26.22.19.17.07.07.13.14.53.54.12.06.11.09.10.09.03.03.04.06.47.49.19.09.18.15.15.13.04.04.08.08.50.52.08.04.07.05.11.07.02.02.03.04.43.45.15.07.13.10.21.15.03.03.05.06.47.49.19.10.17.15.13.12.06.06.10.11.53.53.25.13.23.20.17.16.08.08.14.14.53.54.20.10.18.15.14.12.05.05.11.10.52.53.24.12.22.20.16.15.07.07.13.13.53.53targeterror.03.02.02.02.03.03.01.01.01.02.02.02whitebox.96.19.89.601.00.83.17.10.31.21transferrate.12.23.14.18.12.16.19.22.19.21SVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLNNHNNLRFHRFLSVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLNNHNNL.27.11.25.23.24.19.06.06.12.12.49.49.80.70.82.81.88.87.53.50.68.59.56.57.39.18.40.37.46.37.10.09.19.18.50.51.63.41.66.64.76.70.25.20.43.35.53.53.23.08.21.15.46.27.04.04.08.08.47.49.52.23.51.47.89.81.15.11.26.23.51.52.63.51.65.65.71.68.48.38.56.48.55.55.76.66.77.77.85.83.58.53.69.60.57.57.65.49.67.66.72.68.40.33.60.46.54.54.75.63.77.77.82.80.52.47.67.58.56.56targeterror.03.02.02.02.03.03.01.01.01.02.02.02whitebox1.00.791.00.981.001.00.81.66.90.73transferrate.22.69.31.51.21.43.57.68.56.66SVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLNNHNNLRFHRFLSVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLNNHNNL.83.72.89.90.94.96.58.59.71.60.53.521.001.001.001.001.001.001.001.001.001.00.75.73.94.86.97.97.99.99.71.73.87.75.55.55.99.981.001.001.001.00.95.95.99.95.62.60.71.56.74.75.98.97.46.42.57.50.51.51.97.92.98.981.001.00.81.81.92.82.56.54.99.991.001.001.001.00.99.99.99.98.72.691.001.001.001.001.001.001.001.001.001.00.78.75.99.981.001.001.001.00.98.981.00.98.67.641.001.001.001.001.001.001.001.001.001.00.75.72targeterror.03.02.02.02.03.03.01.01.01.02.02.02whitebox1.001.001.001.001.001.001.001.001.001.00transferrate.73.96.82.92.64.86.94.96.94.95Figure 8: Gradient alignment and perturbation correlation
for evasion attacks on MNIST89. Left: Gradient alignment
R (Eq. 18) between surrogate (rows) and target (columns)
classiﬁers, averaged on the unmodiﬁed test samples. Right:
Pearson correlation coefﬁcient ρ(δ, ˆδ) between white-box and
black-box perturbations for ε = 5.
ferability, as the latter also depends on the complexity of the
target model; i.e., on the size of its input gradients.
SVML
ε = 1.7
SVMH
ε = 0.45
SVM-RBFL
ε = 1.1
SVM-RBFH
ε = 0.85
ε = 2.35
ε = 0.95
ε = 2.9
ε = 2.65
Figure 9: Digit images crafted to evade linear and RBF SVMs.
The values of ε reported here correspond to the minimum
perturbation required to evade detection. Larger perturbations
are required to mislead low-complexity classiﬁers (L), while
smaller ones sufﬁce to evade high-complexity classiﬁers (H).
5.1.2 Android Malware Detection
The Drebin data [1] consists of around 120,000 legitimate and
around 5000 malicious Android applications, labeled using
the VirusTotal service. A sample is labeled as malicious (or
positive, y = +1) if it is classiﬁed as such from at least ﬁve
out of ten anti-virus scanners, while it is ﬂagged as legitimate
(or negative, y = −1) otherwise. The structure and the source
code of each application is encoded as a sparse feature vector
consisting of around a million binary features denoting the
presence or absence of permissions, suspicious URLs and
other relevant information that can be extracted by statically
analyzing Android applications. Since we are working with
sparse binary features, we use the (cid:96)1 norm for the attack.
We use 30,000 samples to learn surrogate and target clas-
siﬁers, and the remaining 66,944 samples for testing. The
Figure 10: White-box evasion attacks on DREBIN. Evasion
rate against increasing maximum perturbation ε.
classiﬁers and their hyperparameters are the same used for
MNIST89, apart from (i) the number of hidden neurons for
NNH and NNL, set to 200, (ii) the weight decay of NNL, set
to 0.005; and (iii) the maximum depth of RFL, set to 59.
We perform feature selection to retain those 5,000 fea-
tures which maximize information gain, i.e., |p(xk = 1|y =
+1)− p(xk = 1|y = −1)|, where xk is the kth feature. While
this feature selection process does not signiﬁcantly affect the
detection rate (which is only reduced by 2%, on average, at
0.5% false alarm rate), it drastically reduces the computa-
tional complexity of classiﬁcation.
In each experiment, we run white-box and black-box eva-
sion attacks on 1,000 distinct malware samples (randomly
selected from the test data) against an increasing number of
modiﬁed features in each malware ε ∈ {0,1,2, . . . ,30}. This
is achieved by imposing the (cid:96)1 constraint (cid:107)x(cid:48)−x(cid:107)1 ≤ ε. As in
previous work, we further restrict the attacker to only inject
features into each malware sample, to avoid compromising
its intrusive functionality [3, 11].
To evaluate the impact of the aforementioned evasion at-
tack, we measure the evasion rate (i.e., the fraction of malware
samples misclassiﬁed as legitimate) at 0.5% false alarm rate
(i.e., when only 0.5% of the legitimate samples are misclas-
siﬁed as malware). As in the previous experiment, we report
the complete security evaluation curve for the white-box at-
tack case, whereas we report only the value of test error for
the black-box case. The results, reported in Figs. 10, 11, 12,
and 13, along with the statistical tests in Table 1 (third and
fourth columns) conﬁrm the main ﬁndings of the previous
experiments. One signiﬁcant difference is that random forests
are much more robust in this case. The reason is that the (cid:96)1-
norm attack (differently from the (cid:96)2) only changes a small
number of features, and thus the probability that it will change
features in all the ensemble trees is very low.
5.2 Transferability of Poisoning Attacks
For poisoning attacks, we report experiments on handwritten
digits and face recognition.
USENIX Association
28th USENIX Security Symposium    331
SVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLNNHNNLSVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLNNHNNL0.140.350.190.290.130.250.260.320.280.320.320.880.420.630.260.500.680.830.670.790.180.450.250.370.180.320.350.420.360.410.260.640.350.510.240.430.490.590.510.580.120.260.160.230.180.280.210.250.210.240.220.490.290.410.270.470.390.460.400.440.250.690.330.500.210.400.670.750.580.660.300.830.390.580.250.470.750.870.660.780.260.680.340.510.220.410.570.670.650.680.300.810.390.580.240.460.670.790.680.80SVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLNNHNNLSVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLNNHNNL.31.47.35.42.24.37.41.45.39.44.44.89.51.67.34.57.75.86.72.82.34.54.38.47.28.42.46.52.45.50.39.68.46.58.32.51.58.66.57.64.24.34.27.32.35.39.30.33.29.32.35.56.40.50.39.55.49.55.48.52.39.76.45.59.30.50.74.80.66.74.43.86.49.65.32.55.80.90.73.83.37.73.44.57.29.49.65.74.68.72.42.83.49.64.32.53.74.84.72.82051015202530ε0.00.20.40.60.81.0EvasionRateWhite-boxevasionattack(DREBIN)SVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLNNHNNL(a)
(b)
(c)
(d)
Figure 11: Evaluation of our metrics for evasion attacks on DREBIN. See the caption of Fig. 6 for further details.
5.2.1 Handwritten Digit Recognition
We apply our optimization framework to poison SVM, logis-
tic, and ridge classiﬁers in the white-box setting. Designing
efﬁcient poisoning availability attacks against neural networks
is still an open problem due to the complexity of the bilevel
optimization and the non-convexity of the inner learning prob-
lem. Previous work has mainly considered integrity poisoning
attacks against neural networks [5, 20, 41], and it is believed
that neural networks are much more resilient to poisoning
availability attacks due to their memorization capability. Poi-
soning random forests is not feasible with gradient-based
attacks, and we are not aware of any existing attacks for
this ensemble method. We thus consider as surrogate learn-
ers: (i) linear SVMs with C = 0.01 (SVML) and C = 100
(SVMH); (ii) logistic classiﬁers with C = 0.01 (logisticL)
and C = 10 (logisticH); (iii) ridge classiﬁers with α = 100
(ridgeL) and α = 10 (ridgeH); and (iv) SVMs with RBF kernel
with γ = 0.01 and C = 1 (SVM-RBFL) and C = 100 (SVM-
RBFH). We additionally consider as target classiﬁers: (i) ran-
dom forests with 100 base trees, each with a maximum depth
of 6 for RFL, and with no limit on the maximum depth for
RFH; (ii) feed-forward neural networks with two hidden lay-
ers of 200 neurons each and ReLU activations, trained via
cross-entropy loss minimization with different regularization
(NNL with weight decay 0.01 and NNH with no decay); and
(iii) the Convolutional Neural Network (CNN) used in [7].
We consider 500 training samples, 1,000 validation sam-
ples to compute the attack, and a separate set of 1,000 test
samples to evaluate the error. The test error is computed
against an increasing number of poisoning points into the
training set, from 0% to 20% (corresponding to 125 poisoning
points). The reported results are averaged on 10 independent,
randomly-drawn data splits.
How does model complexity impact poisoning attack suc-
cess in the white-box setting? The results for white-box poi-
soning are reported in Fig. 14. Similarly to the evasion case,
high-complexity models (with larger input gradients, as shown
in Fig. 15a) are more vulnerable to poisoning attacks than
their low-complexity counterparts (i.e., given that the same
learning algorithm is used). This is also conﬁrmed by the sta-
tistical tests in the ﬁfth column of Table 1. Therefore, model
complexity plays a large role in a model’s robustness also
against poisoning attacks, conﬁrming our analysis.
How do poisoning attacks transfer between models in
black-box settings? The results for black-box poisoning are
reported in Fig. 16. For poisoning attacks, the best surrogates
are those matching the complexity of the target, as they tend
to be better aligned and to share similar local optima, except
for low-complexity logistic and ridge surrogates, which seem
to transfer better to linear classiﬁers. This is also witnessed
by gradient alignment in Fig. 17, which is again not only
correlated to the similarity between black- and white-box per-
turbations (Fig. 15c), but also to the ratio between the black-
and white-box test errors (Fig. 15d). Interestingly, these error
ratios are larger than one in some cases, meaning that attack-
ing a surrogate model can be more effective than running a
white-box attack against the target. A similar phenomenon has
been observed for evasion attacks [33], and it is due to the fact
that optimizing attacks against a smoother surrogate may ﬁnd
better local optima of the target function (e.g., by overcoming
gradient obfuscation [2]). According to our ﬁndings, for poi-
soning attacks, reducing the variability of the loss landscape
(V) of the surrogate model is less important than ﬁnding a
good alignment between the surrogate and the target. In fact,
from Fig. 15b it is evident that increasing V is even beneﬁcial
for SVM-based surrogates (and all these results are statisti-
cally signiﬁcant according to the p-values in the sixth column
of Table 1). A visual inspection of the poisoning digits in
Fig. 18 reveals that the poisoning points crafted against high-
complexity classiﬁers are only minimally perturbed, while
the ones computed against low-complexity classiﬁers exhibit
larger, visible perturbations. This is again due to the presence
of closer local optima in the former case. Finally, a surprising
result is that RFs are quite robust to poisoning, as well as
NNs when attacked with low-complexity linear surrogates.
The reason may be that these target classiﬁers have a large
capacity, and can thus ﬁt outlying samples (like the digits
crafted against low-complexity classiﬁers in Fig. 18) without
affecting the classiﬁcation of the other training samples.
332    28th USENIX Security Symposium
USENIX Association
10−1100Sizeofinputgradients(S)0.20.40.60.81.0Evasionrate(ε=5)SVMlogisticridgeSVM-RBFNN10−210−1Variabilityoflosslandscape(V)0.30.40.50.6Transferrate(=30)0.00.20.40.60.8Gradientalignment(R)0.00.10.20.30.40.50.6ρ(ˆδ,δ)(=30)P:0.91,p-val:<1e-10K:0.74,p-val:<1e-100.00.20.40.60.8Gradientalignment(R)0.20.40.60.8Black-towhite-boxerrorratio(=5)P:0.69,p-val:<1e-10K:0.48,p-val:<1e-10(a) ε = 5
(b) ε = 10
(c) ε = 30
Figure 12: Black-box (transfer) evasion attacks on DREBIN. See the caption of Fig. 7 for further details.
Figure 13: Gradient alignment and perturbation correlation
(at ε = 30) for evasion attacks on DREBIN. See the caption
of Fig. 8 for further details.
5.2.2 Face Recognition
The Labeled Face on the Wild (LFW) dataset consists of faces
of famous peoples collected on Internet. We considered the