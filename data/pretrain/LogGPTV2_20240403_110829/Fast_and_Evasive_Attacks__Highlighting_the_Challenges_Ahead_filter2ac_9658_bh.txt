to the malware name. Please remember that we classify a samples as unique with
the help of the MD5 sum. This means that 1136 diﬀerent samples are detected
as Worm.Padobot.M.
4 Future Work
In this section we want to give an overview of further work in the area of ne-
penthes and large-scale honeynet deployments. An extension of the nepenthes
platform to support UDP-based exploits is straightforward. Most of these ex-
ploits are “single-shot” attempts that just send one UDP packet. Therefore it is
only necessary to capture the payload and analyze it, we do not need to emulate
any service at all. However, if the exploit requires interaction with the honey-
pot, we can use the same concept as for TCP-based exploits: we just emulate
the necessary parts and trick the exploit.
The current nepenthes platform is another building block towards an auto-
mated system to eﬀectively stop remote control networks. Such networks are
needed by attackers to coordinate automated activity, e.g. to send commands
to a large number of compromised machines. An example of such a remote con-
trol network is a botnet, i.e., a network of compromised machines that can be
remotely controlled by an attacker. The whole process of stopping such a net-
work is depicted in Figure 6. With the help of nepenthes, we can now automate
step 1 to a high degree. Without supervision, this platform can collect malware
that currently propagates within a network. We are currently working on step
2 - an automated mechanism to extract the sensitive information of a remote
control network from a given binary. With the help of honeypots, we can au-
tomate this step to a certain degree. In addition, we explore possible ways to
use sandbox-like techniques to extract this information during runtime. Thirdly,
we can use static binary analysis, but it seems like this approach cannot be
automated easily. Step 3 in the whole process can be automated as outlined in
[6]: we impersonate as a legal victim and inﬁltrate the network. This allows us
to study the attacker and his techniques, collect more information about other
victims, or learn about new trends. Finally, step 4 can be automated to a limited
degree with the help of techniques such as stooping the communication channel
between victims and remote control server, or other ways to shut-down the main
server itself [8]. This step also needs some further research, but it seems viable
that this can also be automated to a high degree. The whole process would then
allow us to automatically defend against these kind of attacks in a pro-active
manner. An automated system is desirable since this kind of attacks is a growing
threat within the attacker community.
We are currently in the process of deploying a network intrusion detection
system (NIDS) based on nepenthes. In cooperation with SurfNET, we want
to explore feasible ways of using honeypots as a new kind of IDS. The goals
182
P. Baecher et al.
Fig. 6. Four steps to stop remote control networks
of this project are manifold: one the one hand, the system should enable us to
understand the types and amount of malicious traﬃc within a LAN. In addition,
it should stop spreading worms and other kinds of malware. The literature in
this ﬁeld shows some ways how to achieve this goal with honeypots [14]. On the
other hand, the solution must be scalable and easy to manage and maintain.
Zero-maintenance of the individual sensors is desirable and a missing feature
of existing solutions. Our current experience shows that nepenthes scales well
to a couple of thousand honeypots with just one physical machine. In addition,
a hierarchical setup can be used to distribute load if an even larger setup is
needed. The nepenthes platform can also scale to high-speed networks due to
its limited amount of memory resource and only moderate amount of processing
resources needed. Furthermore, the proposed NIDS should have close to no false
positives. Up to now, we did not have any false positives with our nepenthes
setup, so this goal seems to be reachable. This is mainly due to the assumption
of honeypots: all network traﬃc is suspicious. False negatives of our platform
generate a log-entry and all captured information about network traﬃc that
could not be handled are saved. This way, all possible information to help in
avoiding false negatives is already available for analysis by a human.
Finally, an empirical analysis of the eﬀectiveness of a distributed nepenthes
setup is desirable. Nepenthes oﬀers the possibility of distributed deployment
as outlined in Section 2.2 and a recent study concludes that distributed worm
monitoring oﬀers several advantages in regards to detection time [15]. Those
results are obtained with the help of captured packet traces. With the help of
nepenthes, the results could be veriﬁed on live data. Additionally, such a study
would reveal to what degree a certain piece of malware spreads locally.
5 Conclusion
In this paper we introduced the nepenthes platform. This is a new kind of
honeypot-based system that specializes in large-scale malware collection. Ne-
penthes inherits the scalability of low-interaction honeypots but at the same
time oﬀers a high degree of expressiveness. This goal is reached by emulating
only the vulnerable parts of a service. This leads to an eﬃcient and eﬀective so-
lution that oﬀers many advantages compared to other honeypot-based solutions.
The main advantage is the ﬂexibility: an ordinary honeypot solution has to use
a ﬁxed conﬁguration. If an incoming exploit targets another conﬁguration, this
The Nepenthes Platform: An Eﬃcient Approach to Collect Malware
183
exploit will fail. In contrast to this, one instance of nepenthes can be exploited
by a wide array of exploits since nepenthes is ﬂexible in the emulation process.
It can decide during runtime which oﬀset is the correct one to get successfully
exploited. Several other factors like virtual ﬁlesystem and shell emulation con-
tribute further to the enhanced scalability. With only one physical machine we
are able to listen to more than 16,000 IP addresses in parallel.
We have collected millions of malware samples currently spreading in the wild.
A further examination of more than 14,000 unique and valid binaries showed that
current anti-virus engines have some limitations and fail to detect all malware
propagating in the wild. Moreover, we presented some ideas how nepenthes could
be used as the basic block of an automated system to stop botnets or as part of
a next-generation network intrusion detection system.
References
1. K. Anagnostakis, S. Sidiroglou, P. Akritidis, K. Xinidis, E. Markatos, and
In Pro-
A. Keromytis. Detecting Targeted Attacks Using Shadow Honeypots.
ceedings of the 14th USENIX Security Symposium, 2005.
2. Michael Bailey, Evan Cooke, Farnam Jahanian, Jose Nazario, and David Watson.
The Internet Motion Sensor: A Distributed Blackhole Monitoring System. In Pro-
ceedings of the 12th Annual Network and Distributed System Security Symposium
(NDSS 05), 2005.
3. Edward Balas and Camilo Viecco. Towards a Third Generation Data Capture Ar-
chitecture for Honeynets. In Proceeedings of the 6th IEEE Information Assurance
Workshop, West Point, 2005. IEEE.
4. Team Cymru: The Darknet Project. Internet: http://www.cymru.com/Darknet/,
Accessed: 2006.
5. David Dagon, Cliﬀ Zou, and Wenke Lee. Modeling Botnet Propagation Using Time
Zones. In Proceedings of the 13th Annual Network and Distributed System Security
Symposium (NDSS 06), 2006.
6. Felix Freiling, Thorsten Holz, and Georg Wicherski. Botnet Tracking: Exploring a
Root-Cause Methodology to Prevent Distributed Denial-of-Service Attacks. In 10th
European Symposium On Research In Computer Security, ESORICS05, Milano,
Italy, September 12-14, 2005, Proceedings, Lecture Notes in Computer Science.
Springer, 2005.
7. Thorsten Holz. A Short Visit to the Bot Zoo. IEEE Security & Privacy, 3(3):76–79,
2005.
2003.
8. Thorsten Holz. Spying With Bots. USENIX ;login:, 30(6):18–23, 2005.
9. Xuxian Jiang and Dongyan Xu. Collapsar: A vm-based architecture for network
attack detention center. In Proceedings of 13th USENIX Security Symposium, 2004.
10. Bill McCarty. Automated Identity Theft. IEEE Security & Privacy, 1(5):89–92,
11. David Moore, Colleen Shannon, Geoﬀrey M. Voelker, and Stefan Savage. Network
Telescopes. Technical Report TR-2004-04, CAIDA, 2004.
12. David Moore, Geoﬀrey M. Voelker, and Stefan Savage. Inferring Internet Denial-of-
Service Activity. In Proceedings of the 10th USENIX Security Symposium, August
2001.
ternet: http://www.few.vu.nl/∼porto/argos/, Accessed: 2006.
13. Georgios Portokalidis. Argos: An Emulator for Capturing Zero-Day Attacks. In-
184
P. Baecher et al.
14. Niels Provos. A Virtual Honeypot Framework. In Proceedings of 13th USENIX
Security Symposium, pages 1–14, 2004.
15. Moheeb Abu Rajab and Andreas Terzis. On the Eﬀectiveness of Distributed Worm
Monitoring. In Proceedings of the 14th USENIX Security Symposium, 2005.
16. Yoichi Shinoda, Ko Ikai, and Motomu Itoh. Vulnerabilities of Passive Internet
Threat Monitors. In Proceedings of the 14th USENIX Security Symposium, 2005.
17. Stuart Staniford, David Moore, Vern Paxson, and Nicholas Weaver. The Top Speed
of Flash Worms. In ACM Workshop on Rapid Malcode (WORM), 2004.
18. Symantec. Mantrap. Internet: http://www.symantec.com/, Accessed: 2006.
19. Nicolas Vanderavero, Xavier Brouckaert, Olivier Bonaventure, and Baudouin Le
Charlier. The HoneyTank : a scalable approach to collect malicious Internet traﬃc.
In Proceedings of the International Infrastructure Survivability Workshop, 2004.
20. Michael Vrable, Justin Ma, Jay Chen, David Moore, Erik Vandekieft, Alex C. Sno-
eren, Geoﬀrey M. Voelker, and Stefan Savage. Scalability, Fidelity, and Contain-
ment in the Potemkin Virtual Honeyfarm. In Proceedings of the ACM Symposium
on Operating System Principles (SOSP), 2005.
21. Kathy Wang. Honeyclient. Internet: http://honeyclient.org, Accessed: 2006.
22. Yi-Min Wang, Doug Beck, Chad Verbowski, Shuo Chen, Sam King, Xuxian Jiang,
and Roussi Roussev. Automated web patrol with strider honeymonkeys: Finding
web sites that exploit browser vulnerabilities. In Proceedings of the 13th Network
and Distributed System Security Symposium (NDSS 06), 2006.
23. Vinod Yegneswaran, Paul Barford, and Dave Plonka. On the Design and Use of In-
ternet Sinks for Network Abuse Monitoring. In Proceedings of the 7th International
Symposium on Recent Advances in Intrusion Detection (RAID), 2004.
Automatic Handling of Protocol Dependencies
and Reaction to 0-Day Attacks with ScriptGen
Based Honeypots
Corrado Leita1, Marc Dacier1, and Frederic Massicotte2
1 Institut Eurecom, Sophia Antipolis, France
{leita, dacier}@eurecom.fr
2 Communications Research Centre, Ottawa, Canada
PI:EMAIL
Abstract. Spitzner proposed to classify honeypots into low, medium
and high interaction ones. Several instances of low interaction exist, such
as honeyd, as well as high interaction, such as GenII. Medium interaction
systems have recently received increased attention. ScriptGen and Role-
Player, for instance, are as talkative as a high interaction system while
limiting the associated risks. In this paper, we do build upon the work
we have proposed on ScriptGen to automatically create honeyd scripts
able to interact with attack tools without relying on any a-priori knowl-
edge of the protocols involved. The main contributions of this paper are
threefold. First, we propose a solution to detect and handle so-called
intra-protocol dependencies. Second, we do the same for inter-protocols
dependencies. Last but not least, we show how, by modifying our initial
reﬁnement analysis, we can, on the ﬂy, generate new scripts as new at-
tacks, i.e. 0-day, show up. As few as 50 samples of attacks, i.e. less than
one per platform we have currently deployed in the world, is enough to
produce a script that can then automatically enrich all these platforms.
1 Introduction
Honeypots are powerful systems for information gathering and learning. L.Spitzner
in [1] has deﬁned a honeypot as “a resource whose value is being in attacked or
compromised. This means, that a honeypot is expected to get probed, attacked
and potentially exploited. Honeypots do not ﬁx anything. They provide us with
additional, valuable information”. In [1] honeypots are classiﬁed according to the
degree an attacker can interact with the operating system.
In high interaction honeypots, the attacker interacts with real operating sys-
tems usually deployed through virtual emulators. This ensures a very reliable
source of information, but also brings some major drawbacks. High interaction
honeypots are real hosts and therefore can be compromised: the maintenance
cost and the risk involved in them is high. Also, the amount of resources re-
quired to deploy such honeypots is usually substantial.
In low interaction honeypots such as honeyd [2], the attacker interacts with sim-
ple programs that pretend to behave as a real operating system through very simple
D. Zamboni and C. Kruegel (Eds.): RAID 2006, LNCS 4219, pp. 185–205, 2006.
c(cid:1) Springer-Verlag Berlin Heidelberg 2006
186
C. Leita, M. Dacier, and F. Massicotte
approaches. Honeyd uses a set of scripts to implement responders to the most com-
mon services. Given a request, these scripts try to produce a response that mimics
the behavior of the emulated server. This approach has two major drawbacks. On
the one hand, the manual generation of these scripts is a tedious and sometimes
impossible task due to the unavailability of protocol speciﬁcations. On the other
hand, they are often not able to correctly handle complex protocols, limiting the
length of the conversation that the honeypot is able to carry on with the client.
Since many exploits deliver the malicious payload only after an exchange of sev-
eral packets with the server, low interaction honeypots are often not able to carry
on the conversation long enough to discriminate between diﬀerent types of activi-
ties. For instance, in our experience within the Leurre.com project [3,4,5,6,7,8], due
to the lack of emulation scripts we have been able to observe only the ﬁrst request
of many interesting activities such as the spread of the Blaster worm [9]. But since
Blaster sends the exploit in the second request of its dialog on port 135, we have
never been able to observe such a payload. Therefore it becomes very diﬃcult to
distinguish Blaster’s activity from other activities targeting the same port using
solely the payload as a discriminating factor.
The lack of emulation scripts led us to investigate the feasibility of automat-
ically generating emulators starting from samples of protocol interaction using
the ScriptGen framework [10]. We showed how it was possible to take advantage
of the statistical diversity of a large number of training samples to rebuild a par-
tial notion of semantics. This can be done in a completely protocol-independent
way: no assumption is made on the protocol behavior, nor on its semantics. Our
ﬁrst results showed how ScriptGen had been able to successfully carry on a small
segment of conversation with the clients, proving the validity of the method but
also showing the need to improve emulation.
In this paper we take a big step forward, showing how it is possible to dra-
matically increase the emulation quality by coupling the seminal work presented
in [10] with a number of novel contributions. Speciﬁcally, this paper presents i)
an innovative algorithm to infer dependencies in the content of protocol mes-
sages (intra-protocol dependencies) without requiring the knowledge of protocol
semantics; ii) a new algorithm to generate relations in the interaction of multi-
ple TCP sessions (inter-protocols dependencies); iii) a proxying algorithm that
allows a ScriptGen honeypot to automatically build a training set to reﬁne its
knowledge of the protocol reacting to the detection of new activities.
This paper is organized as follows: section 2 gives an overview on the current
state of the art in the ﬁeld; section 3introduces the main concepts and contributions
of this paper; section 4 gives an in-depth description of the novel contributions to
the ScriptGen framework; section 5 shows the experimental validation performed
on the new ScriptGen emulators; section 6 concludes the paper.
2 State of the Art
The contributions of this paper put their roots in a seminal work presented in
[10]. ScriptGen is a method that aims at building protocol emulators in a com-
pletely automated and protocol-independent way. This is possible through an
Automatic Handling of Protocol Dependencies
187
algorithm detailed in [10] called region analysis. Region analysis uses bioinfor-
matics algorithms [11] as primitives to rebuild protocol semantics and to raise
the training data to a higher level of abstraction. This is done in a completely
protocol-independent fashion: no assumption is made on the protocol seman-
tics or on the protocol behavior. This allows us to build emulators for protocols
whose speciﬁcation is not available or partially unknown. In [10] we validated
the approach, and we identiﬁed a number of limitations that were preventing
ScriptGen emulators from correctly carrying on complete conversations with a
client.
Shortly after our initial publication, Cui et al. presented the results of a sim-
ilar approach, named RolePlayer [12], carried out in parallel to ours. These
authors have the same goals in mind but have imposed diﬀerent constraints
on themselves. RolePlayer uses as input two cleaned and well-chosen scripts.
These scripts are training samples of the conversation that must be emulated.
As ScriptGen does, RolePlayer uses bioinformatics algorithms to align bytes and
delimit ﬁelds inside the protocol byte stream. RolePlayer gives semantic value
to the various ﬁelds using additional information (IP addresses, host names used
in the conversation) and a simple “cookbook” of rules to give an interpreta-
tion to the various ﬁelds. This “cookbook” is a set of heuristics deduced from
observations made on various known protocols.
The RolePlayer approach oﬀers a very elegant solution but it is worth noting
that it is orthogonal to ScriptGen’s philosophy and shows a number of limita-
tions. First of all, the usage of only two scripts in the alignment phase requires
carefully chosen samples in order to avoid false deductions. This process can be
easily done by a human operator, but an automatic preparation of the training
set does not appear straightforward. Furthermore, it appears that the design of
well behaved samples precludes the usage of this technique for online creation
of scripts as we propose to do it in section 4.3. To accomplish the same pur-
pose, ScriptGen performs the analysis on a statistically signiﬁcant number of
samples. ScriptGen exploits the statistical diversity of the samples to minimize
false deductions without requiring any sort of human intervention. As we will
show in this paper, this property is extremely interesting when implementing
automated learning of new activities. In fact, we will show in this paper how
ScriptGen is able to react to 0-day attacks, exploiting its characteristics to learn
the behavior of the new activity. It does so by building in a completely auto-
mated fashion a new training set and using it to reﬁne its knowledge of the
protocol. For this to be possible, no human intervention must be necessary; the
process must be totally automated. ScriptGen, being completely automated and
protocol-agnostic, fulﬁlls these requirements. As opposed to that, the additional
manual input required by RolePlayer to generate the emulators is a severe limi-
tation with respect to this objective. Also, RolePlayer takes advantage of a set
of heuristics that are deduced from the knowledge of existing protocols. Even if
these heuristics might be valid for a certain number of protocols, they restrict
the generality of the method itself by taking into consideration only the number
of well-known protocols for which these assumptions hold. Finally, RolePlayer as
188
C. Leita, M. Dacier, and F. Massicotte
Fig. 1. Simple example of semantic abstraction
described in [12] seems to be able to replay only a single script at a time. It does
not oﬀer a structure to handle in parallel diﬀerent protocol functional ﬂows. A
ScriptGen emulator instead is able to map diﬀerent activities to diﬀerent paths
of the internal protocol state machine.
A completely diﬀerent approach is instead followed in the context of the mw-
collect project [13,14], that has recently merged with the nepenthes project.
These tools use a set of vulnerability modules to attract bots, analyze their shell