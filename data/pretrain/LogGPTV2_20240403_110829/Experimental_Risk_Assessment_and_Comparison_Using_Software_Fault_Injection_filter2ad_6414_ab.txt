)
βα
x
+
(
)
(2) 
Using the logit it is possible to obtain a simple form 
of a multivariate logistic regression model based on the 
relationship  presented  in  equation  (3)  where  several 
independent variables (in our case, static metrics) can 
be used.  
(
logit 
prob
)
=
ln
prob
−
prob
1
+=
ββα
+
x
11
x
22
++
..
β
nnx
 (3) 
To estimate the prob(f)  we need to identify  which 
metrics are relevant, since the chosen metrics strongly 
depend  on  the  system  characteristics,  operational 
profile, the risk type, and the particular aspects that are 
being evaluated. When there is more than one metric 
available,  we  need  to  select  which  of  them  is  best 
suited to the evaluation of the software complexity. 
The  size  (in  terms  of  lines  of  code  -  LoC)  of  the 
component was emphasized as an example of the direct 
relationship  of  measures  of  software  complexity  and 
measures of software quality [31] and it is one of the 
first and most common forms of software complexity 
measurement.  LoC  was  used  to  compose  the  field 
observation  and  was  combined  with  empirical  fault 
density  provided  by  Rome  Laboratory  [33].  We  start 
by  considering  the  cyclomatic  complexity  (Vg)  as 
regressors  for  prob(f).  Vg  measures  the  control  flow 
complexity  of  a  program  and  is  dependent  on  the 
number  of  predicates  (logical  expression  such  as  if, 
while, etc). Vg is also one of the most used software 
metrics and it is independent from the language.    
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:32:36 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007The  accuracy  of  the  results  obtained  in  the  first 
experiments was evaluated through the analysis of bug 
reports  available  from  open  software  initiatives  (see 
[29]). Based on that study, we added the following new 
metrics  to  achieve  a  better  approximation  for  the 
estimated  fault  density  when  compared  to  the  bugs 
observed  in  field:  number  of  parameters,  number  of 
returns, maximum nesting depth, program length, and 
vocabulary  size  [14].  Halstead’s  metrics  and  Vg 
measure two distinct program attributes [31] leading to 
a better fault prediction capability [24].  
According  to  equation  (3)  and  considering  six 
metrics, the probability that a component has a residual 
fault  (prob(f(X1,  X2,  X3,  X4,  X5,  X6  ))  can  then  be 
expressed as in equation (4). This equation allows us to 
use any number of metrics we consider appropriate to 
calculate  the  probability  of  the  existence  of  residual 
fault. For that purpose we only need to add one more 
term  (Bi  Xi)  for  each  metric  to  be  considered  in  the 
equation.  
=
prob
f
)(
++
+
βα
β
X
X
)
...
6
6
1
1
βα
++
β
+
X
X
...
6
1
1
exp(
+
exp(
          (4) 
In  the  above  equation,  Xi  represents  the  product 
metrics  (independent  variables)  and  α  and  βi  the 
estimated logistic regression coefficients.  
In  order  to  obtain  the  coefficients  for  logistic 
regression [16] we proceed as follows: 
•  Evaluate the complexity metrics of each module. 
•  Adopt fault density ranges accepted by the industry 
community  as  a  preliminary  estimation  of  fault 
densities.  In  our  work  we  use  the  empirical  fault 
density  reported  by  Rome  Laboratory  [33]  as  a 
starting  estimation  for  the  logistic  regression.  We 
used this preliminary estimation to replace the field 
observation used in any regression analysis. 
1
)
6
•  Use the binomial distribution. Taking into account 
prob  as  0.1  fault  per  KLoC  and  LoC  metric  for 
each  module  we  get  the  number  of  lines  with 
residual faults in the module i as a binomial random 
variable  with  parameters  LoCi  and  prob,  and 
defines  a  preliminary  fault  density  for  module  i. 
This  preliminary  density  is  then  refined  with  the 
contribution of the other metrics using regression. 
The  binomial  distribution  is  used  as  we  consider 
the  existence  of  a  fault  is  independent  from  the 
existence  of  other 
remaining 
component program lines. 
•  Apply the regression using the value obtained from 
natural  logarithm  of  the  preliminary  fault  density 
and 
the 
coefficients. 
the  chosen  metrics  aim 
to  obtain 
•  Estimate the probability of fault of each component 
by  using  the  computed  coefficients  in the  logistic 
equation presented in (4). 
faults 
the 
in 
f
/
f
g
)
(
)
(
(*)
(5) 
the 
prob
i
When  necessary  to  estimate  the  probability  of 
residual  fault  in  a  set  of  components  (the  case  of  a 
large  component  formed  by  several  sub-components) 
we  have  to  use  the  prob(f)  estimated  for  each  sub-
component  combined  with  the  complexity  weight  of 
each sub-component in the global component. This is 
obtained  by  equation  (5),  where  Metricsi    represents 
any of the available metrics for each component i. One 
can  choose  the  metric  that  best  represent  the  system 
characteristics (for example, maximum nesting depth if 
the system has several nested structures).  
∑
prob
2.2. Failure Cost Estimation Through Injection 
of Software Faults 
Metrics
i
Metrics
i
∑=
instruction  sequences 
We  use  the  G-SWFIT  [11]  technique  to  inject  the 
software  faults.  G-SWFIT  is  based  on  a  set  of  fault 
injection operators that reproduce directly in the target 
executable  code 
that 
represent  most  common  types  of  high-level  software 
faults. These  fault  injection  operators resulted  from  a 
field study that analyzed and classified more than 500 
real  software  faults  discovered  in  several  programs, 
identifying  the  most  common  (the  “top-N”)  types  of 
software  faults  [11].  Table  1  shows  the  12  most 
frequent types of faults found in [11]. We use these 12 
fault types in the present paper. The representativeness 
of  the  faults  injected  ensures  that  the  fault  injection 
experiments represent the activation of  faults that are 
likely to exist in the component. 
type  could 
The locations where the injection are performed are 
selected by the analysis of the target code (done by the 
G-SWFIT tool), which allows the identification of the 
places  where  a  given  fault 
indeed 
realistically  exist,  and  avoids  using  locations  where 
faults  of  the  intended  type  could  not  exist.  For 
example,  MIFS  fault  type  in  Table  1  can  only  be 
injected  in  target  code  locations  that  represent  an  IF 
structure. The analysis of the target code is  based  on 
the  knowledge  of  how  the  high-level  constructs  are 
translated  into  low-level  instruction  sequences  [11]. 
The distribution of the number of fault injected in each 
component  is  based  in  our  previous  proposal  [29]. 
Furthermore,  for  large  components  with  a  very  large 
number  of  fault 
internally 
distributed according to the distribution show in Table 
1 (third column). For small components with a small 
number of fault locations is not possible to follow the 
distribution in Table 1. In this case, faults, inside the 
component, are injected using the best approximation 
for the distribution in Table 1. 
locations,  faults  are 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:32:36 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007Fault types 
Description 
% of total observed  
ODC classes  
Table 1 – Most frequent fault types found in [11]
   MIFS 
   MFC 
   MLAC 
   MIA 
   MLPC 
   MVAE 
   WLEC 
   WVAV 
   MVI 
   MVAV 
   WAEP 
   WPFV 
Missing "If (cond) { statement(s) }" 
Missing function call 
Missing "AND EXPR" in expression used as branch condition 
Missing "if (cond)" surrounding statement(s) 
Missing small and localized part of the algorithm 
Missing variable assignment using an expression 
Wrong logical expression used as branch condition 
Wrong value assigned to a value 
Missing variable initialization 
Missing variable assignment using a value 
Wrong arithmetic expression used in parameter of function call 
Wrong variable used in parameter of function call 
Total faults coverage 
9.96 % 
8.64 % 
7.89 % 
4.32 % 
3.19 % 
3.00 % 
3.00 % 
2.44 % 
2.25 % 
2.25 % 
2.25 % 
1.50 % 
50.69 % 
Algorithm 
Algorithm 
Checking 
Checking 
Algorithm 
Assignment 
Checking 
Assignment 
Assignment 
Assignment 
Interface 
Interface 
The evaluation of the cost of component failures is 
done  by  injecting  one  fault  at  each  time.  After  the 
injection  of  each  fault,  the  cost  is  measured  as  the 
impact  of  the  fault  injected  in  the  component  in  the 
whole  system.  This 
in  the 
following failure modes: Hang – when the application  
is  not  able  to  terminate  in  the  pre-determinate  time; 
Crash – the application terminates abruptly before the 
workload complete; Wrong – the workload terminates 
but the results are not correct; Correct – there are no 
errors reported and the result is correct.  
translated 
impact 
is 
Considering the four failure modes proposed, only 
the “Correct” failure mode means that the system has 
delivered  correct  service  after  the  injected  fault. This 
means that we could in fact reduce the failure mode to 
only two: correct or incorrect behavior. However, we 
decide  to  keep  the  four  failure  modes  to  have  more 
detailed information on the impact of each fault. 
When  a  software  fault  is  injected  in  a  given 
component,  that  fault  may  or  may  not  cause  faulty 
behavior  in  the  component.  Furthermore,  only  a 
fraction of faults that cause erroneous behavior in the 
component will cause the system to fail, depending on 
the system architecture and the operational profile (the 
remaining faults are tolerated or have no visible effect). 
This  means  that  the  results  measured  by  using  fault 
injection already include two important terms:   
 cost(f) = prob(fa) * c(failure) 
(6) 
where  prob(fa)  is  the  probability  of  fault  activation 
(and consequent deviation in the component behavior) 
and  c(failure)  is  the  consequence  of  a  failure,  for 
example, the probability that the system crash. 
3. Case Study 
We present a case-study to show the experimental 
risk  estimation  as  proposed  in  our  approach.  The 
case-study is a C application implemented for both the 
RTEMS and RTLinux operating systems and allows us 
to  assess  the  risk  for  each  of  these  alternative 
components. The metrics of each software component 
were obtained with the Resource Standard Metrics [35] 
and CMT++ tools [41]. 
The  case-study  is  a  satellite  data  handling  system 
named  Command  and  Data  Management  System 
(CDMS).  A  satellite  data  handling  system 
is 
responsible for managing all data transactions between 
ground  systems  and  a  spacecraft.  The  CDMS 
application was developed in C and runs on top of two 
alternative  real-time  operating  systems:  RTEMS  [36] 
and  RTLinux  [23].  This  is  particularly  relevant  as  it 
allows  us  to  show  an  interesting  utilization  of  the 
proposed  approach,  which  is  to  help  developers  in 
choosing between alternative off-the-shelf components 
(RTEMS and RTLinux in this case).  
Metrics 
C. Complexity 
N. Parameters 
N. Returns 