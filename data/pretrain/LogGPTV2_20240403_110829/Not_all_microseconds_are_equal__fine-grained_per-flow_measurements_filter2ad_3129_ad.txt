ISP traces. The other two are real router traces with synthetic
trafﬁc (jointly referred to as WISC).
The per-ﬂow counters are updated the same way as before in RLI
estimator. Because this estimator does not use both values, it is not
as accurate as the RLI estimator as we shall discuss in our evalua-
tion.
Shrinkage estimation. While linear interpolation is a simple means
to approximate the delay, linearity may not always be the best choice.
We therefore considered possible reﬁnement of the delay estima-
tors, in particular using Shrinkage Estimation [11]. This is a stan-
dard method to improve accuracy of an estimator X by making a
convex combination of it with some ﬁxed point X0, possibly 0, to
form X(cid:2) = λX + (1 − λ)X0 for some λ ∈ [0, 1]. In our case, the
approach is to shrink individual delay values towards an EWMA
estimate of the mean delay. It turned out that Shrinkage Estimation
provided only a very small improvement in estimation accuracy in
the results reported in §4. We consider exploring other non-linear
interpolation schemes as part of our future work.
4. EVALUATION METHODOLOGY
We evaluate our architecture in order to answer the following
questions. (1) How accurate is our architecture in estimating per-
ﬂow latencies under different settings? (2) How does our archi-
tecture compare with previous solutions such as trajectory sam-
pling [14] and Multiﬂow estimator [25]? (3) What are the over-
heads involved in our measurement architecture? (4) Does the ar-
chitecture cause any interference with regular packets? In order
to answer these questions, we build a custom simulator that uses
packet header traces as input (using either synthetic or anonymized
IP addresses), and implements the reference packet generator and
latency estimator modules to simulate the architecture.
4.1 Data sets used for evaluation
Ideally, we need real packet traces with high-resolution times-
tamps of ingress and egress interfaces of a production router. Un-
fortunately, we have found no such public datasets, neither in the
target setting of a data center or trading network, nor otherwise.
The only known study to us, although in a backbone ISP context,
is the one by Papagiannaki et al. where they conducted extensive
studies of delays in real routers by collecting GPS synchronized
packet header traces from the Sprint network [27, 19], but the data
set itself is not public. In their study, the authors observe that packet
delays follow a Weibull distribution. However, our method exploits
the correlations of the delay across different packets, which are not
modeled in their study.
In the absence of ideal traces, we resort to two other types of
traces. First, we used traces of the passage of synthetic trafﬁc
across a real router collected by the authors of [33] in their evalua-
tion of a new active probing tool. Trafﬁc is synthetically generated
using the Harpoon trafﬁc generator [32] over a dumbbell topology
with an OC-3 bottleneck link. Even though the trafﬁc sources are
synthetic, they are subject to real router forwarding paths, queue-
ing and other behavior, and thus are quite realistic. The router em-
ployed the RED queueing policy during collection of these traces.
33CHIC
SANJ
WISC
 1
 0.8
 0.6
 0.4
 0.2
F
D
C
 0
10-3
10-2
CHIC
SANJ
WISC
 1
 0.8
 0.6
 0.4
 0.2
F
D
C
100
101
 0
10-3
10-2
CHIC
SANJ
 1
 0.8
 0.6
 0.4
 0.2
F
D
C
100
101
 0
10-3
10-2
10-1
Relative error
(a) High utilization (≈ 88%)
10-1
(b) Moderate utilization (≈ 55%)
Relative error
10-1
100
101
Relative error
(c) Low utilization (≈ 22%)
Figure 5: CDF of mean per-ﬂow delay estimates using RLI estimator for different utilizations and different traces.
Ingress and egress timestamps are recorded for each packet, hence
the delay incurred in traversing the router is computed by subtrac-
tion. The data set referred to as WISC consists of two traces with
different utilization levels, summarized in Table 2.
Second, we used backbone header traces published by CAIDA
[31] that include actual packet arrival times of real packets at an
interface, and then simulate the passage of these packet arrivals
through a queue. The traces are summarized in Table 2. Each trace
records packet arrivals during a 600 second period on OC-192 (i.e.,
10 Gbps) backbone links of a tier-one ISP. The IP addresses in the
trace were anonymized. The traces denoted as CHIC and SANJ
represent those collected at Chicago, IL and San Jose, CA respec-
tively. For the experiments described below, we classiﬁed packets
according to the standard 5-tuple comprising source and destination
IP addresses, port numbers and protocol ﬁelds. (Note the distinc-
tion with the 2-tuple keys employed in §2.)
4.2 Simulator
While the ideal prototype implementation would use a real router,
router architectures are typically proprietary making it hard to make
any changes. Further, to demonstrate the effectiveness of our archi-
tecture, we mainly need sender and receiver timestamps for indi-
vidual packets that pass through a real router; the internal details
of routers are not important for our evaluation. Thus, we build
a simulator by extending an open-source NetFlow platform called
YAF [8] for our simulation. NetFlow, the de facto passive mea-
surement solution, already supports ﬂow-level collection of basic
statistics such as number of packets, bytes, etc. Thus, extending
YAF automatically provided us with the ﬂow creation, ﬂow update,
and ﬂow expiry mechanisms in regular NetFlow. We added support
for the injection of reference packets from the sender side, the in-
terpolation buffer at the receiver, and latency estimator along with
three additional counters we maintain for the latency estimates on a
per-ﬂow basis. We implement the adaptive reference packet injec-
tion algorithm based on keeping track of the utilization as described
in Algorithm 1. Since most real routers use RED, we simulate RED
queue management strategy.
In the queueing model employed for simulation with CHIC and
SANJ, we control the packet loss and delay by conﬁguring queue
length and drain rate. We ﬁx the drain rate in terms of bytes per
second. By ﬁxing this one parameter, the drain rate, we can au-
tomatically control both the delay as well as the loss distribution.
Following the guidelines in [18], we chose a queue size of 10,000,
minth = 4, 000 and maxth = 9, 000, queue weight wq = 0.002
and maximum drop probability, maxp = 1
50 for all traces. Note
that while our simulation is open-loop, i.e., we do not see TCP
backoff effects even when we drop packets using RED, both WEB468
and WEB700 traces are generated by conﬁguring a real router with
RED, and as such, will expose all the relevant TCP backoff dynam-
ics associated with RED.
For WISC traces, since we cannot easily inject reference packets
into the simulation, we rely on a simple packet marking scheme that
denotes the nearest regular packet as a reference packet whenever
it needs to be injected. Compared to adjusting the packet times-
tamps to simulate the injection of a reference packets, our packet
marking scheme is much less intrusive. We believe that it does not
affect the accuracy of our architecture, because the delays are still
real packet delays. Effectively, the reference packet times are just
slightly offset from what they would be in an actual realization.
4.3 Other solutions for comparison
Trajectory sampling. First, we consider trajectory sampling pro-
posed by Dufﬁeld et al. to sample packet trajectories [14]. While
the original intent is different, we can add a timestamp with each
packet label sampled at a router, and aggregate samples that belong
to a given ﬂow for latency estimates. The estimator just computes
the difference of timestamps at two adjacent locations (similar to
the naive timestamp idea discussed in §3).
Multiﬂow estimator. Second, we consider a new estimator called
Multiﬂow estimator (MFE) proposed by Lee et al. in [25]. MFE
exploits the fact that NetFlow already maintains timestamps of start
and end packets for each (sampled) ﬂow. Two adjacent routers us-
ing consistent hash-based sampling will collect same ﬂow records
with same start and end packets, giving two delay samples. Given
that the simple averaging of just these two samples is not an ac-
curate estimator, MFE computes the average of all delay samples
(referred to as background samples) that may potentially belong to
other ﬂows within the start and end of the ﬂow. The spirit behind
this estimator is grounded in a similar observation as ours in §2.
5. RESULTS
We divide our results into three major parts: First, we evaluate
the accuracy of our RLI estimator, both mean and standard devia-
tion estimates, for different traces and different utilizations. Sec-
ond, we compare our architecture with other solutions such as the
trajectory sampling and MFE described in §4.3. Finally, we evalu-
ate the overheads involved in our architecture.
5.1 Accuracy of RLI
To evaluate the accuracy of RLI, we primarily focus on the rela-
tive error (deﬁned as |true− estimated|/true) of mean and stan-
dard deviation estimations of each ﬂow with the ground truth.
Accuracy of mean latency. We plot the cumulative distribution
function (CDF) of the relative error of mean delay estimates for all
the ﬂows in Figure 5 for different utilizations and traces. In our
evaluation, we consider the WEB468 as a moderate utilization sce-
nario with about 55% link utilization, while WEB700 comprises
the high utilization scenario (about 88% utilization). We do not
have access to a lower utilization trace in the WISC data set, hence
we do not show the curve for WISC in Figure 5(c). For high and
moderate utilizations, we can observe that median relative error of
latency estimates among all ﬂows is around 10-12%. The 75%ile
relative error is also less than 20% in these two cases. For low uti-
lization, median relative error of estimates is around 30%. Across
34CHIC
SANJ
WISC
CHIC
SANJ
WISC
 1
 0.8
 0.6
 0.4
 0.2
F
D
C
10-5
10-4
10-3
10-2
10-1
True per-flow delay (second)
(a) Binned by delay
CHIC
SANJ
 1
 0.8
 0.6
 0.4
 0.2
 0
10-6
 1
 0.8
 0.6
 0.4
 0.2
r
o
r
r
e
e
v
i
t
l
a
e
r
e
g
a
r
e
v
A
r
o
r
r
e
e
v
i
t
l
a
e
r
e
g
a
r
e
v
A
 0
10-3
 1
 0.8
 0.6
 0.4
 0.2
 0
10-6
r
o
r
r
e
e
v
i
t
l
a
e
r
e
g
a
r
e
v
A
10-2
10-1
Relative error
100
101
(a) CDF
CHIC