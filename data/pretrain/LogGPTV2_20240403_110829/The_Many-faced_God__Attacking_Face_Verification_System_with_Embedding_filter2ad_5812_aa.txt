title:The Many-faced God: Attacking Face Verification System with Embedding
and Image Recovery
author:Mingtian Tan and
Zhe Zhou and
Zhou Li
The Many-faced God: Attacking Face Verification System with
Embedding and Image Recovery
Mingtian Tan, Zhe Zhou∗
{18210240176,zhouzhe}@fudan.edu.cn
Fudan University
Zhou Li
PI:EMAIL
University of California, Irvine
ABSTRACT
Face verification system (FVS), which can automatically verify a
person’s identity, has been increasingly deployed in the real-world
settings. Key to its success is the inclusion of face embedding, a
technique that can detect similar photos of the same person by deep
neural networks.
We found the score displayed together with the verification result
can be utilized by an adversary to “fabricate” a face to pass FVS.
Specifically, embeddings can be reversed at high accuracy with
the scores. The adversary can further learn the appearance of the
victim using a new machine-learning technique developed by us,
which we call embedding-reverse GAN. The attack is quite effective
in embedding and image recovery. With 2 queries to a FVS, the
adversary can bypass the FVS at 40% success rate. When the query
number raises to 20, FVS can be bypassed almost every time. The
reconstructed face image is also similar to victim’s.
ACM Reference Format:
Mingtian Tan, Zhe Zhou and Zhou Li. 2021. The Many-faced God: Attacking
Face Verification System with Embedding and Image Recovery. In Annual
Computer Security Applications Conference (ACSAC ’21), December 6–10, 2021,
Virtual Event, USA. ACM, New York, NY, USA, 14 pages. https://doi.org/10.
1145/3485832.3485840
1 INTRODUCTION
Machine learning (ML), especially Deep Learning based on Deep
Neural Networks (DNN), has transformed many important appli-
cation domains, like computer vision, language processing and
speech recognition. In certain tasks, DNN can achieve far better
result comparing to the human expert, thanks to its capability of
modeling the complex relation between input and output domains.
Apart from high accuracy, ML is easy to implement, which also
contributed to its popularity. Usually a deep learning model costs
developers only several hundreds of Python codes but can already
produce satisfied accuracy.
Face verification is such an application scenario supported by ML.
State-of-the-art face embedding schemes like Facenet can achieve
over 99% accuracy. Motivated by such result, face verification sys-
tems (FVS) powered by face embedding are widely deployed at
∗Zhe Zhou is the Corresponding Author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
© 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-8579-4/21/12...$15.00
https://doi.org/10.1145/3485832.3485840
places like border control [8, 17, 44, 47], company entrance [2, 29]
and mobile device [4, 50]. Its success is mainly attributed to its
convenient workflow: after the user enrolls in FVS with her ID and
face images, next time, the same user can be quickly verified based
on the embedding.
Unfortunately, our study revealed a severe confidentiality issue
of the deployed FVS. By carefully probing a targeted FVS with a
set of faces and observing the responses, not only an attacker can
“create” a face that successfully passes the check of FVS, she can
also recover the face image of a victim enrollee. Our attack does
not exploit any specific bug of FVS system nor require access to
enrollee’s image (in fact, the face image is never stored by FVS).
In particular, our attack is based on three unique insights about
FVS and face embedding: 1) No matter what the verification result
is, information about the victim enrollee leaks to the attacker. 2)
Such information can be accumulated so that the face embedding,
the internal representation created by the embedding model about
a user, can be recovered. 3) The face embedding is highly sensi-
tive, because an attacker can reconstruct the input image with
high fidelity under its guidance. Below we elaborate the three
insights.
1) Information leakage from FVS. For some FVS, every time the
verification result (“pass” or “fail”) is displayed to an attested person,
the score is also displayed for the debugging purposes, reflecting
how far/close the person’s image to the enrollee’s. The score is
directly related to the distance on the embedding space.
Every time the similarity to attested person is displayed, the
system leaks a small portion of information about the claimed user’s
face. The similarity could help attackers to recover the embedding
(a vector representing a face) of the profile photo once enough
information is collected.
2) Embedding recovery from leakage. At the first glance, re-
versing the victim enrollee’s face from FVS score seems infeasible,
as the information contained by it is negligible. However, the in-
formation can be accumulated, when the attacker probes FVS with
different images. One of our key findings is that when the number of
inquiry images equals to the dimensions of the embedding model, the
victim’s embedding can be recovered without error, mainly because
an embedding is a high-dimension vector which still obeys alge-
braic geometry theorems. By formulating the embedding distances
with equations on Euclidean space, the root of equations corre-
sponds to the exact embedding. Furthermore, we found through a
dimension-reduction approach based on PCA (Principal Compo-
nent Analysis), the adversary can issue much less queries to recover
a similar embedding.
3) Image recovery with embedding. With the embedding, the
attacker is supposed to reconstruct the victim’s photo. However,
face embedding is a complex, non-linear and lossy mapping from
17ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Mingtian Tan, Zhe Zhou and Zhou Li
an input sample. Reversing such mapping is quite challenging,
which has not been resolved by prior works. We propose a novel
approach based on generative adversarial network (GAN) for this
task. Classical GAN models reconstruct images from noise input
or a pre-defined label but none of them deal with unseen input.
Therefore, we design a new embedding-reverse GAN (or erGAN)
with the generator and loss function tailored to the embedding
input.
Major results. We evaluate the embedding recovery (named Em-
bRev) and face recovery (named ImgRev) modules. The overall
result proves learning faces enrolled in an FVS just from scores is
feasible and the attack is practical.
For EmbRev, we evaluated over 13,000 images contained by the
LFW dataset on 4 different embedding models. When the query
number equals to the embedding dimensions (e.g., 128 queries when
attacking Facenet-128), the embedding can be recovered nearly
perfectly (the error margin is due to floating-point precision). When
reducing the query number to half, the error distance is still small, at
only 0.1 in average, which is far smaller than the distance threshold
of the targeted models (e.g., 1.28 for Facenet-128). In fact, only
2 queries are sufficient to help the adversary byass FVS at
40% chances under whitebox setting. We also found EmbRev
achieves consistent performance when the image is distorted or
some digits of the FVS score are hidden.
For ImgRev, we evaluated with the images from CelebA dataset.
Our result shows the images reversed from the perfect embed-
dings can pass all 4 evaluated embedding models with over 90%
success rate. Furthermore, based on FID metrics, the quality of
our recovered images are considered quite satisfactory (e.g., 34
for Clarifai-1024), considering that adding a pair of eye glasses
easily raises FID over 47 [63]. When the recovered embedding
contains errors, e.g., due to the reduced query number, the result
maintains the same level. The consequence of ImgRev is severe.
Take a face verification based door entrance system as an exam-
ple, an attacker can claim to be an arbitrary enrollee (victim) and
pass the entrance with the recovered photo. Moreover, ImgRev
eventually can help attackers infer similar photos to all enrollees’
photos stored in the FVS database, leading to outstanding privacy
leakage.
We have reported our discoveries to stake-holders like Clar-
ifai. The code of this project will be released at a GitHub
repo1.
We summarize our contributions as follow:
• We identified that the confidentiality of FVS enrollees is un-
der threat when the adversary probes the FVS with different
images.
• We presented a new attack against face embedding. Our
attack is able to recover a sensitive face embedding with
only a few to dozens of queries.
• We developed a new DNN model based on GAN, which is
able to reconstruct an image close to victim’s from a recov-
ered embedding.
• We evaluated our attack with state-of-the-art embedding
models and real-world face dataset.
1https://github.com/BennyTMT/DL_Privacy
(a) A self-service FVS in Chinese Entry & Exit Bu-
reau.
(b) A FVS
app [14].
Figure 1: Two examples of FVS. To notice, the similarity
scores are displayed.
2 BACKGROUND
2.1 Face Verification
A face verification system (FVS) takes a digital image or video
through camera as input and matches it with the database of face
images to verify the claimed identity. It has been widely deployed
by government for surveillance and border control [8, 17, 44, 47],
enterprise for attendance tracking [2, 29] and mobile device for
owner authentication [4, 50]. When the verification process is initi-
ated, the face detection module discovers the face region and sends
it to the face matching module, which computes a score between
the captured face image and the enrolled face images to decide
whether the person can be authenticated.
However, as previous work identified [37], face verification is
vulnerable under media-based facial forgery (MFF) attack, where
the adversary captures the victim’s face (e.g., from social network)
ahead and replays the crafted photo/video. To detect such forged
face, liveness detection system was proposed. It either uses sensors
(e.g., accelerometer and gyroscope) or challenge-response protocol
(e.g., asking the user to smile) to assign a liveness score about
the inputted image/video [34, 36, 55, 57]. Yet, its effectiveness is
questionable when the adversary can wear a mask with the victim’s
face printed [16]. In this work, we focus on bypassing FVS with
static image. To bypass live detection system, the methods proposed
previously [16] can be leveraged.
Face embedding. The accuracy of face verification highly depends
on the face matching module. Specifically, it should give high simi-
larity score to the face images of the same person but low score to
those of different persons. Nowadays, face embedding models like
Clarifai [10] (online service) and Facenet (open-source implementa-
tion) [3, 51] are integrated to build the face matching module. Face
embedding is a Deep Convolutional Neural Network trained with
face images collected from a pool of participants (each participant
can have multiple images). Given an image, the face embedding
model will map it to a vector of N dimensions (e.g., 128 or 512 for
Facenet [51] and 1024 for Clarifai [10]), which is also called embed-
ding. The deployed FVS usually uses pre-trained model (e.g., trained
with public face dataset like CASIA-WEBFACE[68]). In enrollment
stage, FVS stores the embedding and its user ID (e.g., employee)
into the biometric database, which is kept as secret. In verification
stage, the embedding of the attested person will be compared to
18The Many-faced God: Attacking Face Verification System with Embedding and Image Recovery
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
the embedding of his enrolled profile photo under the provided ID.
The similarity is typically computed using L2 distance or Cosine
distance and the person is authenticated if the similarity is over a
threshold. In addition to face verification, face embedding has also
been leveraged to find similar persons and techniques like Locality
Sensitive Hashing (LSH) [21] can be leveraged.
2.2 Adversary Model
The primary goal of our adversary is to impersonate another person
who has enrolled in a deployed FVS and bypass the check. Specifi-
cally, the adversary intends to forge a face image with minimum
distance to victim on the embedding plane. Such attack can deal
great damage to public safety. For example, an enlisted terrorist
can escape into country’s border which deploys self-service FVS
(shown in Figure. 1a). The secondary goal of the adversary is to
learn the appearance of a victim without her consent, which vio-
lates her privacy. In other words, the forged image should also look
realistic, with minimum distance to the victim on the image plane.
Adversarial examples do not meet this goal as they do not need to
look similar to victims but attackers.
Our attack consists of five steps. (1) We assume victim’s ID has
been obtained, e.g., through searching public ID database. The ad-
versary comes to the FVS, enters the ID of the targeted victim, and
initiates the face verification process. (2) The verification result
(it should be “fail”) and score are displayed, which leaks informa-
tion about the victim. To gain more information, multiple scores
according to different attempts are collected, which can be done by
showing different face images or recruiting a group of people to
approach the FVS. (3) The adversary reconstructs the embedding of
the victim with the tested faces and their scores. (4) Victim’s face
image is recovered through a generative model. (5) The adversary
prints out the generated face image (e.g., as a mask) and wears it to
bypass FVS. Figure 2 illustrates our attack process.
Leakage of FVS score. The calculated distance, from some FVS de-
velopers’ perspective, is not sensitive. An example we encountered
is a self-service machine that was deployed at the Chinese entry and
exit bureau (the counterpart of immigration or boarder inspection
of some countries, and also part of the police system). This machine
authenticates users with their faces before other tasks. The ma-
chine directly displays the similarity on the screen (see Figure 1a).
Another example is an app that directly shows the matching score
on its UI to users, as shown by Figure 1b. No matter if similarity,