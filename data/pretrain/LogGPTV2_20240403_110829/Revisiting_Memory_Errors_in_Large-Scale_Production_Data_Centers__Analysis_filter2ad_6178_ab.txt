1
/
0
1
3
1
/
1
1
3
1
4
1
/
/
4
1
4
1
4
1
4
1
4
1
4
1
4
1
/
/
2
3
/
4
/
/
/
/
5
6
7
8
1
2
1
Fig. 1: Timeline of correctable and uncorrectable errors.
Month
First, correctable errors occur relatively commonly each
month, affecting 2.08% of servers on average. Though such
errors do not corrupt data, they do reduce machine performance
due to the hardware required to reconstruct the correct data.
While a single correctable error may not be very noticeable, a
large number of correctable errors could lead to performance
degradation. We examine the distribution of the number of
correctable errors among machines at the end of this section.
To compare against prior work, we measured the correctable
error incidence rate over the course of twelve months (7/13
up to and including 7/14, excluding 1/14) and found that,
cumulatively across all months, around 9.62% of servers ex-
perience correctable memory errors. This is much lower than
the yearly correctable error incidence rate reported in work
from the ﬁeld seven years ago (32.2% in Table 1 in [44])
and comparable with the 5.48% to 9.10% failure rate reported
in more recent work [48] from two years ago. Thus, though
the overall correctable error incidence rate may have decreased
over the better part of a decade of device improvements, our
measurements corroborate the trend that memory errors are still
a widespread problem in the ﬁeld.
In addition, we ﬁnd that the correlation between a server
having a correctable error in a given month, depending on
whether there were correctable errors observed in the previous
month is 31.4% on average. In comparison, prior work from
the ﬁeld found around a 75% correlation in correctable errors
between two consecutive months [44]. Our lower observed
amount of correlation is partially due to how memory errors
are handled in the servers we evaluate: servers were ﬂagged for
memory repair if they had more than 100 correctable errors per
week, whereas prior work (e.g., [44]) only replaced components
with uncorrectable errors. Under our more aggressive/proactive
repair policy, we ﬁnd that on average around 46% of servers
that have errors end up being repaired each month. As a result,
in contrast to prior work, we ﬁnd that a majority (69.6%) of the
machines that report errors each month are not repeat offenders
from the previous month.
Second, the rate of uncorrectable errors is much smaller
than the rate of correctable errors, with uncorrectable errors
affecting 0.03% of servers each month on average. Recall
1Correctable error data for January 2014 (1/14) is not available. Note that
if a server has multiple errors in multiple months, it will be represented in
multiple data points.
417417
that uncorrectable errors cause a server to crash, increasing
downtime and potentially causing data loss. Therefore, it is
desirable to decrease the rate of uncorrectable errors as much
as possible.
Schroeder et al. conjectured that repair policies “where a
DIMM is replaced once it experiences a signiﬁcant number of
correctable errors, rather than waiting for the ﬁrst uncorrectable
error” could reduce the likelihood of uncorrectable errors [44].
To test this hypothesis in the ﬁeld on our systems that are
repaired with more than 100 correctable errors, we compare
the rate of uncorrectable errors relative to the rate of correctable
errors, in order to control for the change in rate of correctable
errors between the two studies. Interestingly, in Schroeder et
al.’s study, uncorrectable error rate was only 25:0(cid:2) smaller than
the correctable error rate, while in our study it is 69:3(cid:2) smaller.
If more aggressive repair policies indeed lead to higher server
reliability, then our results suggest that uncorrectable error rate
can be lowered by up to 2:8(cid:2) (i.e., 69:3(cid:2) / 25:0(cid:2)). This is
achieved by repairing around 46% of the machines with errors
(those with more than 100 correctable errors). System designers
must decide whether the beneﬁt in reduction of potential data
loss is worth the increase in repair rate.
Third, the incidence error rate for correctable errors ﬂuctu-
ates little (its standard deviation is ˙0:297%) and is relatively
stable over the fourteen months that we examined. Uncor-
rectable errors also remain low in comparison to correctable
errors (with a standard deviation of ˙0:018%). We attribute
the low standard deviation in error behavior over time to the
large population size that we examine.
Figure 2 (left) shows the distribution of correctable errors
among servers that had at least one correctable error. The x axis
is the normalized device number, with devices sorted based on
the number of errors they had during a month. The y axis shows
the total number of errors a server had during the month in log
scale. Notice that the maximum number of logged errors is in
the millions. We observe that a small number of servers have
a large number of errors. For example, the top 1% of servers
with the most errors have over 97.8% of all observed correctable
errors. We also ﬁnd that the distribution of number of errors
among servers is similar to that of a power-law distribution
with exponent (cid:3)2:964. Prior work observed that some failed
devices, such as the memory controller or bus, can account for
a large number of errors (e.g., [45]), though the full distribution
of errors has not been quantiﬁed before.
Fig. 2: The distribution of memory errors among servers with errors (left)
resembles a power-law distribution. Memory errors also follow a Pareto
distribution among servers with errors (right).
Figure 2 (right) shows the probability density distribution of
correctable errors. The x axis is the number of errors per month
and the y axis is the probability of a server having at least that
many errors per month. A Pareto distribution (a special case
of the power law) has been ﬁt to the measured data. Similarly
to many past works that have found decreasing hazard rates in
the behavior of systems (e.g., Unix process lifetimes [14], sizes
of ﬁles transferred through the Web [8, 9], sizes of ﬁles stored
in Unix ﬁle systems [17], durations of FTP transfers in the
Internet [42], CPU requirements for supercomputing jobs [43],
and memory access latencies [22]), we ﬁnd that the distribution
of errors across servers follows a Pareto distribution, with a
decreasing hazard rate. This means, roughly, that the more
errors a server has had so far, the more errors it is expected
to have.2
Quantifying the skewed distribution of correctable errors is
important as it can help diagnose the severity of a memory
failure relative to the population. For comparison, Schroeder
et al. reported a mean error rate of 22,696 correctable errors
per server per year (Table 1 in [44]), or 1,891 correctable
errors per server per month. Without knowing the underlying
distribution, however, it is not clear whether all servers had such
a large number of errors each month or whether this average is
dominated by a small number of outliers (as we observe here).
If we compute the mean error rate as in prior work, we
observe 497 correctable errors per server per month. However, if
we examine the error rate for the majority of servers (by taking
the median errors per server per month), we ﬁnd that most
servers have at most 9 correctable errors per server per month.3
In this case, using the mean value to estimate the value for
the majority overestimates by over 55(cid:2). We therefore conclude
that, for memory devices, the skewed nature in which errors
are distributed among devices call for the full distribution to be
examined. Doing so reveals that memory errors follow a power-
law distribution, which can be used to accurately assess the
severity of machine failures. Therefore, we hope future studies
that use error data from the ﬁeld take into account the new
distribution we observe and openly provide.
In addition, we found that hardware scrubbing detected
13.1% of the total number of errors. While we did not monitor
how many servers employed scrubbing, we observed that 67.6%
of the servers with errors detected at least one error through
scrubbing. We do not have detailed memory access information,
so the interaction between scrubbing and different workloads is
not clear, and requires further examination.
B. Component Failure Analysis
Memory errors can occur due to failures in a DRAM device
as well as if the memory controller in the processor fails
or if logic associated with transmitting data on a memory
channel fails. While prior work examined DRAM chip-level
failures ([16, 47, 48]) and memory controller/channel failures
([45]) separately, no prior work has comprehensively examined
failures across the entire memory system.
We adopted a methodology for classifying component fail-
ures similar to prior work (e.g., [16, 47, 48, 45]). We examined
all of the correctable errors across the ﬂeet each month. We
began by determining each correctable error’s corresponding
processor socket, memory channel, bank, row, column, and byte
offset. Then, we grouped errors based on the component that
failed and caused the error to occur. For grouping errors by
components, we used the following criteria:
Socket. If there were > 1 K errors across > 1 memory
channel connected to the same processor socket, we classiﬁed
those errors as being caused by a socket failure. The > 1 K error
threshold was chosen so as to ensure that the failures we classify
are not due to a small number of independent cell failures. To
make sure this was the case, we cross-referenced repair logs of
the servers classiﬁed with failed sockets and found that 50% of
them had a large number of errors that required replacing the
2Note that one can take advantage of this property to potentially predict
which servers may have errors in the future. We leave this for future work.
For more information on the Pareto distribution, decreasing hazard rate, and
their properties, we refer the reader to [22, 13].
3Concurrent work by Sridharan et al. [46] makes a similar observation,
though we quantify and provide a model for the full distribution of errors
per server.
418418
processor to eliminate the errors and 50% contained intermittent
bursts of errors that caused the server to become unresponsive
for long periods of time – both of these are characteristics of
failed sockets that can generate a large number of machine
check exceptions, as observed in prior work [45].
Channel. After excluding the above errors, if there were
> 1 K errors across > 1 DRAM banks connected to the same
memory channel, we classiﬁed the channel as having failed.
Similar to sockets, we cross-referenced repair logs for servers
classiﬁed with failed channels and found that 60% of the servers
with failed channels did not have any logged repair action (re-
placing or reseating the DIMM), suggesting that these failures
were transient, potentially caused by temporary misalignment
of the transmission signal on the channel. The other 40% of
servers required DIMMs to be replaced, suggesting permanent
failures related to the channel transmission logic (e.g., the I/O
circuitry) within the DIMM.
Bank. After excluding the above errors, we repeated the
procedure for banks, classifying a bank as having failed if it
had > 1 K errors across > 1 row. Note that our study examines
monthly failure trends, and we assume that multiple row failures
in the same bank in the same month may be more indicative
of a bank failure than multiple independent row failures in the
bank.
Row. After excluding the above errors, we classiﬁed a row
as having failed if > 1 column in the same row had errors.
Column. After excluding the above errors, we classiﬁed
a column as having failed if > 1 error occurred in the same
column.
Cell. After excluding the above errors, we classiﬁed a cell
as having failed if > 1 error occurred in the same byte within
60 seconds. We chose this amount of time because we found
that 98.9% of errors at a particular byte address had another
error at the same address within 60 seconds if they ever had an
error at the same byte address again in the same day.
Spurious. After excluding the above errors, we are left
with what we term spurious errors. These errors are isolated to
individual cells that do not share a common failed component
and do not repeat in a short amount of time. Potential causes of
these errors include alpha particle strikes from the atmosphere
or chip packaging [34] and cells with weak or variable charge
retention times [30, 21, 20].
Figure 3 shows the fraction of logged errors each month that
are attributed to different types of failures. Error bars show the
standard deviation between months.
s
r
o
r
r
e
f
o
n
o
i
t
c
a
r
F
0
0
1
.
0
5
0
.
0
0
0
.
s
r
e
v
r
e
s
f
o
n
o
i
t
c
a
r
F
0
0
1
.
0
5
0
.
0
0
0
.
k
n
a
B
w
o
R
t
e
k
c
o
S
l
e
n
n
a
h
C
l
l
e
C
n
m
u
o
C
l
s
u
o
i
r
u
p
S
k
n
a
B
w
o
R
t
e
k
c
o
S
l
e
n
n
a
h
C
l
l
e
C
n
m
u
o
C
l
s
u
o
i
r
u
p
S
Fig. 3: How errors are distributed
among different memory compo-
nents. Error bars signify the varia-
tion in total errors from month to
month.
Fig. 4: The fraction of
failed
servers that had each type of mem-
ory component failure.
Sockets and channels generate the most errors when they
fail, 63.8% and 21.2% of all errors each month, respectively.
This is because when these components fail,
they affect a
large amount of memory. Compared to a prior work that
419419
examined socket (memory controller) and channel failures [45]
(but did not examine DRAM chip-level failures), we ﬁnd that
our systems have 2:9(cid:2) more socket errors and 5:3(cid:2) more
channel errors. This could be due to differences in the server
access patterns to memory or how quickly servers crash when
experiencing these types of failures.
That sockets and channels cause a large number of errors
when they fail helps explain the skew in the distribution of
errors among servers (Figure 2, left). For example, servers