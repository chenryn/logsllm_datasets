periodically recalculated oﬀ-line. Calculation is (rela-
tively) slow given that hist() calls return large sets.
• Block/IP Value Caching: Similarly, block and IP
reputations can be cached after the ﬁrst cache miss.
Cache hit rates are expected to be high because (1)
an email with multiple recipients (i.e., a carbon copy)
is received multiple times but with the same source IP
address, and (2) source IP addresses are non-uniformly
distributed. For the 6.1 million (non-intra-network,
non-blacklisted) emails in the working data-set, there
are 364k unique IP senders and 176k unique ‘blocks.’
• Cache Consistency: Caches at all levels need to be
ﬂushed when the blacklists are updated (every 30 min-
utes), to avoid inconsistencies involving the arrival of
3This minimal history requirement was of beneﬁt to this
study. Reputations must warm-up before their use is appro-
priate. Indeed, collection of blacklist data began in 5/2009,
three months before the ﬁrst classiﬁcations.
4Although it was found unnecessary, h could be optimized
on an interval basis, much like re-training a classiﬁer. How-
ever, experiments showed minor variations of the parameter
to be inconsequential.
166
new listings. As far as time-decay is concerned, a dis-
crepancy of up to 30 minutes is inconsequential when
considering a 10-day half-life.
• Whitelisting: There is no reason to calculate reputa-
tion in trusted IP addresses, such as one’s own server.
Of course, whitelists could also be utilized in a feed-
back loop to alleviate false-positives stemming from
those entities whose emails are misclassiﬁed.
 1e+07
 9e+06
 8e+06
 7e+06
 6e+06
d
e
t
s
i
l
l
k
c
a
B
s
P
I
.
m
u
N
XBL (y1)
REP (y2)
n
o
i
t
a
t
u
p
e
R
S
A
e
g
a
r
e
v
A
Using these optimizations, the PreSTA implementation is
capable of scoring 500k emails an hour, with average email
latency on the order of milliseconds5. Latency and band-
width are minimal concerns. Instead, it is the oﬀ-line pro-
cessing supporting this scoring which is the biggest resource
consumer. Even so, the implementation is comfortably han-
dled by a commodity machine and could easily run adjacent
to an email server. Pertinent implementation statistics, such
as cache performance, are available in Sec. 6.4.
5.7 Reputation Classiﬁcation
Extraction of a binary classiﬁcation (i.e., spam or ham)
is based on a threshold strategy. Emails evaluated above
the threshold are considered ham, and those below are con-
sidered spam. Finding an appropriate threshold can be
diﬃcult, especially as dimensionality grows, as is the case
when classifying multiple reputation values. Further, a ﬁxed
threshold is insuﬃcient due to temporal ﬂuctuations; as
large groups (botnets) of spamming IPs arise and fall over
time, the distinction between good and bad may shift.
A support vector machine (SVM) [13] is employed to de-
termine thresholds. SVM is a form of supervised learn-
ing that provides a simple and eﬀective means to classify
multiple reputation values. The algorithm maps reputation
triples (a feature for each spatial dimension) from an email
training set into 3-dimensional space. It then determines the
surface (threshold) that best divides spam and ham data-
points based on the training labels. This same threshold
is then applied during classiﬁcation. The SVM routine is
tuned via a cost metric that is correlated to the eventual
false-positive rate of the classiﬁer.
The classiﬁer is adjusted (re-trained) every 4 days to han-
dle dynamism. A subset of emails received in the previous
4 days are trained upon, and the resulting classiﬁer is used
for the next 4 day interval. The aﬀect of diﬀerent training
periods has not been extensively studied. Clearly, large pe-
riods are not desired; the reputation of distant emails may
not speak to the classiﬁcation of current ones. Too short a
period is poor because it requires extensive resources to re-
train so frequently. Analysis found 4-day re-training to be a
good compromise. However, the re-training period need not
be ﬁxed, and future work will explore re-training rates that
adjust based on various environmental factors.
At each re-training, 10,000 emails (5% of the non-intra-
network, non-blacklisted email received every 4 days) were
used, and emails were labeled as spam/ham based on the
Proofpoint score. In a more general use case, there would
be some form of client feedback correlated across many ac-
counts that can classify spam post-delivery and train various
spam detectors. Since we do not have access to such user
behavior, correlation statistics, or any external spam ﬁlters,
5Statistics are based on a single-threaded implementation.
Concurrency and other programming optimizations would
likely improve PreSTA’s performance and scalability.
08/01/09
09/01/09 10/01/09
11/01/09 12/01/09
Figure 5: XBL Size Relative to Global Rep.
the provided Proofpoint values are assumed.
Post-training, the false-positive (FP) rate of the classi-
ﬁer is estimated by measuring the error over the training
set (assuming one does not over-ﬁt the training data). The
estimated FP-rate is a good indicator of the true FP-rate,
and the SVM cost parameter is adjusted to tune the ex-
pected FP-rate. All classiﬁer statistics and graphs hereafter
were produced with a 0.5% tolerance for false-positives (over
the classiﬁcation set), as this simpliﬁes presentation. This
FP-rate (0.5%) is a reasonable setting given that blacklists
are widely accepted and achieved a 0.74% FP-rate over the
same dataset. Additionally, these rates are somewhat in-
ﬂated given the decision to exclude intra-network emails,
which are unlikely to contribute false-positives (the black-
list FP-rate was reduced one-third to 0.46% with their in-
clusion). In Sec. 6.5, the trade-oﬀ between the FP-rate and
spam blockage is examined in greater depth.
6. EXPERIMENTAL ANALYSIS
Experimental analysis begins by examining component
reputations individually. From there, two case studies are
presented which exemplify how PreSTA produces metrics
outperforming traditional blacklists in both spatial and tem-
poral dimensions. Finally, the detection results of the
PreSTA spam ﬁlter are presented.
To best simulate a real email server load, it is assumed
emails arrive in the order of their timestamps and are eval-
uated relative to this ordering. Additionally, cache popula-
tion/ﬂushing and classiﬁcation re-training are performed at
the relative time-intervals outlined in the previous section.
6.1 Blacklist Relationship
In examining how reputations quantify behavior, we apply
a simple intuition: One would expect to see a clear push-pull
relationship between an entity’s reputation and the number
of corresponding entries on the blacklist. To conﬁrm this
hypothesis, the size of the XBL blacklist6 was graphed over
time and compared to the average reputation of all ASes.
Results are presented in Fig. 5. An inverse relationship is
observed, conﬁrming the hypothesis. When the number of
listings decreases, reputation increases – and vice versa.
6.2 Component Reputation Analysis
In order for component reputations (IP, block, and AS) to
be useful in spam detection they must be behavior predictive.
That is, the reputations of ham emails should exceed those
6The XBL is the driving force behind reputation. The SBL
is also a contributor, but is orders of magnitude smaller.
167
)
s
e
i
r
e
s
y
b
(
s
l
i
a
m
%
 20
 15
 10
 5
 0
 Ham
 Spam
 Ham
 Spam
 60
 40
 20
 Ham
 Spam
 100
 80
 60
 40
 20
 0.9
 0.8
(a) IP Reputation
 1
 0
 0.99
 0.995
 1
 0
 0.99
 0.995
 1
(b) Block Reputation
(c) ASN Reputation
Figure 6: CDFs of Component Reputations
t
h
g
u
a
C
m
a
p
S
"
L
B
e
h
t
e
v
o
b
A
"
f
o
%
 60
 50
 40
 30
 20
 10
 0
ASN Rep.
Block Rep.
IP Rep.
All (SVM)
08/19/09
09/11/09
10/04/09
10/28/09
11/20/09
12/13/09
Start Date of Classification (4-day blocks)
Figure 7: Component Reputation Performance
of spam emails. This relationship is visualized in the CDFs
of Fig. 6. All component reputations behave as expected.
Fig. 6 also displays the beneﬁt of multiple spatial groupings.
While 90% of spam emails come from IPs that had ideal
reputation (i.e., a reputation of 1) at the time of receipt,
this is true for just 46% of blocks, and only 3% of AS.
The CDFs of Fig. 6 imply that each component reputa-
tion is, in and of itself, a metric capable of classifying some
quantity of spam. However, it is desirable to show that each
granularity captures unique spam, so that the combination
of multiple reputations will produce a higher-order classi-
ﬁer of greater accuracy. In Fig. 7, the eﬀectiveness of each
component reputation is presented. The percentage of spam
caught is “above the blacklist,” or more precisely, the per-
centage of spam well-classiﬁed by the reputation value that
was not identiﬁed by the blacklist alone7. Crucially, the
combined performance (the top line of Fig. 7), exceeds that
of any component, so each spatial grouping catches spam the
others do not. On the average, PreSTA is able to capture
25.7% of spam emails not caught by traditional blacklists.
We are also interested in determining which grouping pro-
vides the best classiﬁcation. AS-level reputation is the most
stable of the components, individually capable of classifying
an additional 10-15% of spam above the blacklist. However,
during periods of increased PreSTA performance, it is often
the block and IP levels that make signiﬁcant contributions.
This is intuitive; AS-level thresholding must be conservative.
Given their large size, the mis-classiﬁcation of an AS could
result in an unacceptable increase in the FP-rate. Mean-
while, the cost associated with a mis-prediction is far less
for block and IP groupings.
These results suggest that considering more spatial di-
mensions should increase performance, that is, when there
are non-overlapping classiﬁcations. However, there are di-
minishing returns. Each additional component reputation
requires increased resources in evaluation and classiﬁcation.
7Given the inclusion of traditional blacklist ﬁltering, the pri-
mary concern is those emails that are not actively listed.
168
i
d
e
v
e
c
e
R
s
m
a
p
S
f
o
%
 100
 80
 60
 40
 20
 0
9/9/09 - 10/3/09 :: IP-204.xxx.9.154 History
BL
SPAM(y1)
REP(y2)
 1
 0.8
 0.6
 0.4
 0.2
 0
n
o
i
t
t
a
u
p
e
R
l
e
v
e
l
-
P
I