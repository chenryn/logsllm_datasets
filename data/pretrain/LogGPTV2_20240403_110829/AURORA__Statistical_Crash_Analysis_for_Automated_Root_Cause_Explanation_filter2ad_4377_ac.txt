To demonstrate the practical feasibility of the proposed ap-
proach, we implemented a prototype of AURORA. We brieﬂy
explain important implementation aspects in the following,
the full source code is available at https://github.com/
RUB-SysSec/aurora.
Input Diversiﬁcation. For the purpose of exploring inputs
close to the original crash, we use AFL’s crash exploration
mode [58]. Given a crashing input, it ﬁnds similar inputs that
still crash the binary. Inputs not crashing the program are not
fuzzed any further. We modiﬁed AFL (version 2.52b) to save
these inputs to the non-crashing set before discarding them
from the queue.
Monitoring Input Behavior. To monitor the input behav-
ior, we implemented a pintool for Intel PIN [40] (version 3.7).
Relying on Intel’s generic and architecture-speciﬁc inspection
APIs, we can reliably extract relevant information.
Explanation Synthesis. The explanation synthesis is writ-
ten in Rust. It takes two folders containing traces of crashes
and non-crashes as input. Then, it reconstructs the joined
control-ﬂow graph and then synthesizes and evaluates all
predicates. Finally, it monitors and ranks the predicates as de-
scribed before. To monitor the execution of the predicates, we
set conditional breakpoints using the ptrace syscall. In a ﬁnal
step, we use binutils’ addr2line [36] to infer the source
ﬁle, function name and line for each predicate. If possible, all
subsequent analysis parts are performed in parallel. Overall,
about 5,000 lines of code were written for this component.
6 Experimental Evaluation
Based on the prototype implementation of AURORA, we now
answer the following research questions:
RQ 1: Is AURORA able to identify and explain the root cause
of complex and highly exploitable bug classes such as
type confusions, use-after-free vulnerabilities and heap
buffer overﬂows?
RQ 2: How close is the automatically identiﬁed explanation
to the patch implemented by the developers?
RQ 3: How many predicates are related to the fault?
To answer these research questions, we devise a set of ex-
periments where we analyze various types of software faults.
For each fault, we have manually analyzed and identiﬁed the
242    29th USENIX Security Symposium
USENIX Association
root cause; furthermore, we considered the patches provided
by the developers.
6.1 Setup
All of our experiments are conducted within a cloud VM with
32 cores (based on Intel Xeon Silver 4114, 2.20 GHz) and
224 GiB RAM. We use the Ubuntu 18.04 operating system.
To facilitate deterministic analysis, we disable address space
layout randomization (ASLR).
We selected 25 software faults in different well-known ap-
plications, covering a wide range of fault types. In particular,
we picked the following bugs:
• ten heap buffer overﬂows, caused by an integer overﬂow
(#1 mruby [1]), a logic ﬂaw (#2 Lua [2], #3 Perl [3]
and #4 screen [4]) or a missing check (#5 readelf [5],
#6 mruby [6], #7 objdump [7], #8 patch [8]), #9 Python
2.7/3.6 [9] and #10 tcpdump [10])
• one null pointer dereference caused by a logic ﬂaw
(#11 NASM [11])
• three segmentation faults due to integer overﬂows
(#12 Bash [12] and #13 Bash [13]) or a race condition
(#14 Python 2.7 [14])
• one stack-based buffer overﬂow (#15 nm [15])
• two type confusions caused by missing checks
(#16 mruby [16] and #17 Python 3.6 [17])
• three uninitialized variables caused by a logic ﬂaw
(#18 Xpdf [18]) or missing checks (#19 mruby [19] and
#20 PHP [20])
• ﬁve
use-after-frees, caused by a
free
(#21 libzip [21]), logic ﬂaws (#22 mruby [22],
#23 NASM [23] and #24 Sleuthkit [24]) or a missing
check (#25 Lua [25])
double
These bugs have been uncovered during recent fuzzing runs or
found in the bug tracking systems of well-known applications.
Our general selection criteria are (i) the presence of a proof-
of-concept ﬁle crashing the application and (ii) a developer-
provided ﬁx. The former is required as a starting point for
our analysis, while the latter serves as ground truth for the
evaluation.
For each target, we compile two binaries: One instrumented
with AFL that is used for crash exploration and one non-
instrumented binary for tracing purposes. Note that some of
the selected targets (e. g., #1, #5 or #19) are compiled with
sanitizers, ASAN or MSAN, because the bug only manifests
when using a sanitizer. The targets compiled without any san-
itizer are used to demonstrate that we are not relying on any
sanitizers or requiring source code access. The binary used
for tracing is always built with debug symbols and without
sanitizers. For the sake of the evaluation, we need to mea-
sure the quality of our explanations, as stated in the RQ 1
and RQ 2. Therefore, we use debug symbols and the applica-
tion’s source code to compare the identiﬁed root cause with
the developer ﬁx. To further simplify this process, we derive
source line, function name and source ﬁle for each predicate
via addr2line. This does not imply that our approach by
any means requires source code: all our analysis steps run on
the binary level regardless of available source code. Experi-
ments using a binary-only fuzzer would work the exact same
way. However, establishing the ground truth would be more
complex and hence we use source code strictly for evaluation
purposes.
For our evaluation, we resort to the well-known AFL fuzzer
and run its crash exploration mode for two hours with the
proof-of-concept ﬁle as seed input. We found that this is
enough time to produce a sufﬁciently large set of diverse in-
puts for most targets. However, due to the highly structured
nature of the input languages for mruby, Lua, nm, libzip,
Python (only #17) and PHP, AFL found less than 100 in-
puts within two hours. Thus, we repeat the crash exploration
with 12 hours instead of 2 hours. Each input found during
exploration is subsequently traced. Since some inputs do not
properly terminate, we set a timeout of ﬁve minutes after
which tracing is aborted. Consequently, we do lose a few in-
puts, see Table 4 for details. Similarly, our predicate ranking
component may encounter timeouts. As monitoring inputs
with conditional breakpoints is faster than tracing an input,
we empirically set the default timeout to 60 seconds.
6.2 Experiment Design
An overview of all analysis results can be found in Table 1.
Recall that in practice the crashing cause and root cause of a
bug differ. Thus, for each bug, we ﬁrst denote its root cause as
identiﬁed by AURORA and veriﬁed by the developers’ patches.
Subsequently, we present the crashing cause, i. e., the reason
reported by ASAN or identiﬁed manually. For each target,
we record the best predicate score observed. Furthermore, we
investigate each developer ﬁx, comparing it to the root cause
identiﬁed by our automated analysis. We report the number
of predicates an analyst has to investigate before ﬁnding the
location of the developers’ ﬁx as Steps to Dev. Fix. We ad-
ditionally provide the number of source code lines (column
SLOC) a human analyst needs to inspect before arriving at the
location of the developer ﬁx since these ﬁxes are applied on
the source code level. Note that this number may be smaller
than the number of predicates as one line of source code usu-
ally translates to multiple assembly instructions. Up to this
day, no developer ﬁx was provided for bug #23 (NASM). Hence,
we manually inspected the root cause, identifying a reason-
able location for a ﬁx. Bug #11 has no unique root cause; the
bug was ﬁxed during a major rewrite of program logic (20
ﬁles and 500 lines changed). Thus, we excluded it from our
analysis.
To obtain insights into whether our approach is actually
capable of identifying the root cause even when it is sepa-
rated from the crashing location by the order of thousands
of instructions, we design an experiment to measure the dis-
USENIX Association
29th USENIX Security Symposium    243
Table 1: Results of our root cause explanations. For 25 different bugs, we note the target, root and crashing cause as well as whether the target has been compiled
using a sanitizer. Furthermore, we provide the number of predicates and source lines (SLOC) a human analyst has to examine until the location is reached where
the developers applied the bug ﬁx (denoted as Steps to Dev. Fix). Finally, the number of true and false positives (denoted as TP and FP) of the top 50 predicates
are shown. * describes targets where no top 50 predicates with a score above or equal to 0.9 exist.
Target
Root Cause
Crash Cause
Sanitizer Best Score
#1
#2
#3
#4
#5
#6
#7
#8
#9
#10
#11
#12
#13
#14
#15
#16
#17
#18
#19
#20
#21
#22
#23
#24
#25
mruby
Lua
Perl
screen *
readelf
mruby
objdump
patch
Python
tcpdump
NASM
Bash
Bash
Python
nm *
mruby
Python
Xpdf
mruby
PHP
libzip *
mruby
NASM *
Sleuthkit
Lua
int overﬂow
logic ﬂaw
logic ﬂaw
logic ﬂaw
missing check
missing check
missing check
missing check
missing check
missing check
logic ﬂaw
int overﬂow
int overﬂow
race condition
missing check
missing check
missing check
logic ﬂaw
missing check
missing check
double free
logic ﬂaw
logic ﬂaw
logic ﬂaw
missing check
ASAN
ASAN
ASAN
ASAN
ASAN
ASAN
heap buffer overﬂow
heap buffer overﬂow
heap buffer overﬂow
heap buffer overﬂow
heap buffer overﬂow
heap buffer overﬂow
heap buffer overﬂow
heap buffer overﬂow
heap buffer overﬂow
heap buffer overﬂow
nullptr dereference
segmentation fault
segmentation fault
segmentation fault
stack buffer overﬂow
type confusion
type confusion
uninitialized variable
ASAN
uninitialized variable MSAN
uninitialized variable MSAN
ASAN
use-after-free
use-after-free
ASAN
use-after-free
use-after-free
use-after-free
ASAN
ASAN
-
-
-
-
-
-
-
-
-
-
-
-
0.998
1.000
1.000
0.999
1.000
1.000
0.981
0.997
1.000
0.994
1.000
0.992
0.999
1.000
0.980
1.000
1.000
0.997
1.000
1.000
1.000
1.000
0.957
1.000
1.000
Steps to Dev. Fix
#Predicates
1
1
13
26
7
1
3
1
46
1
-
10
9
13
1
33
215
16
16
42
1
9
1
2
3
Top 50
#SLOC TP FP
0
0
7
0
0
38
2
0
6
0
0
22
15
23
0
0
43
0
0
21
0
8
9
2
0
1
1
10
16
5
1
3
1
28
1
-
6
6
13
1
15
141
11
5
19
1
6
1
2
3
50
50
43
30
50
12
48
50
44
50
50
28
35
27
35
50
7
50
50
29
39
42
14