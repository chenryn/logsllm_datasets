0
0
Total Outgoing Bytes per TCP Connection
n
o
i
t
c
n
u
F
n
o
i
t
i
u
b
i
r
t
s
D
e
v
i
t
l
a
u
m
u
C
Average
Chicago
Germany
India
Singapore
10
20
30
40
50
60
70
80
Data (KB)
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
TCP Connection Duration
Average
Chicago
Germany
India
Singapore
5
10
15
20
25
30
35
40
Duration (s)
Figure 8: Bytes transferred per TCP connection.
Figure 9: TCP connection duration.
3.2.2 TCP Connection Characterization
In this section we study different characteristics of users’ TCP
connections to the Yahoo! CDN.
Figure 8 shows the distribution of the total number of bytes trans-
ferred to the user per connection. We see that about 90% of connec-
tions download less than 64KB of data. Note that TCP Reno, the
default TCP ﬂavor at many operating systems including Linux, has
64KB as the default value for TCP’s initial ssthresh (Slow Start
Threshold). One important consequence of this is that 90% of
the connections, barring loss, would never enter TCP’s congestion
avoidance phase and the transfer is done entirely in the slow-start
phase.
Figure 9 shows the distribution of connection duration at the four
servers. The different knees in the graphs correspond to the server’s
connection timeout setting, which reﬂects differences in the local
server conﬁgurations. Note that this does not correspond to active
transfer times; due to the typical size of objects requested, and the
small number of objects in a persistent connection, as we will see
in Section 3.2.3, a typical connection is idle most of the time.
To study packet loss for these connections, we use a metric that
is measurable from the packet traces we collected, i.e. retransmis-
sion rate. This retransmission rate is an upper bound on the packet
loss rate. Since most users use selective acknowledgments, retrans-
missions establish a tight upper bound. Figure 10 shows the dis-
tribution of packet retransmission rates per connection. Note that,
in India for example, over 70% of the connections see no retrans-
missions; however, over 17% of connections have retransmit rates
above 10%. Similarly, in Figures 4 and 5, we see that some sub-
nets experience very little retransmissions, while others experience
substantial retransmission rates that sometimes reach 50%. As we
see in Section 4.1.2, overall page load time is extremely sensitive
to the packet-loss rate, especially during connection setup, where a
SYN timeout is on the order of seconds. This is compounded for
networks where RTTs are signiﬁcantly higher, and it is not uncom-
mon to see total page load time in the range of 10-120 seconds for
the Yahoo! frontpage.
n
o
i
t
c
n
u
F
n
o
i
t
i
u
b
i
r
t
s
D
e
v
i
t
l
a
u
m
u
C
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0
Packet Retransmit Rate from CDN to User
Average
Chicago
Germany
India
Singapore
5
10
15
20
25
30
35
40
Packet Retransmit Rate (%)
Figure 10: Packet retransmission rate.
3.2.3 HTTP Workload Characterization
In this section we study different properties of the downloaded
web objects at the Yahoo! CDN. We can see the distribution of
requested object sizes in Figure 11, which shows around 90% of
objects are smaller than 25KB (17 segments). Figure 12 shows
the distribution of the number of HTTP requests per connection to
see the effect of persistent connections. The mean is only about
2.4 requests per connection, with the majority of connections re-
questing only one object. The reason for having a small number of
requests per connection in spite of typical web pages having tens of
objects is because web browsers typically use multiple concurrent
TCP connections per domain per web page. Putting together Fig-
ures 11 and 12 tells us that even when requesting multiple objects
back-to-back, objects are so small and so few that a typical connec-
574tion does not have enough time to take advantage of opening up the
congestion window. Hence, most of the time is spent ramping up
in slow-start.
In Figure 13, we show the distribution of the time between HTTP
requests within the same connection (so-called “think-time”). We
observed that the overall majority, about 80% of back-to-back re-
quests, occur in under one second, and therefore unlikely to be the
result of user-clicks, but rather the browser fetching objects.
Linux 2.6 kernels also provide a setting called tcp_slow_start_-
after_idle, which resets the congestion window to the ICW and
moves TCP back to the slow-start phase if there is no data to send
after a given idle period, deﬁned as one retransmission timeout
(RTO). This is on by default. In Figure 13, we also plot the dis-
tribution of the difference between the inter-request time and our
estimate of of the RTO, calculated using the standard Jacobson es-
timator: RT O = RT Taverage + 4 ∗ RT Tvariance. We ﬁnd that
approximately 10% of back-to-back object requests are seperated
by more than one RTO. All these users have to go through slow
start again when downloading the following object spending ex-
pensive network round trips to probe the network for bandwidth
again.
t
n
u
o
C
n
o
i
t
c
e
n
n
o
C
8
7
6
5
4
3
2
1
0
x 106
HTTP Requests per TCP Connection
Mean = 2.24 Req
90% <= 4 Req
1
2
3
4
5
6
Requests
7
8
9
10
Figure 12: HTTP requests per connection.
n
o
i
t
c
n
u
F
n
o
i
t
i
u
b
i
r
t
s
D
e
v
i
t
l
a
u
m
u
C
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
Requested Object Size
Average
Chicago
Germany
India
Singapore
10
20
30
40
50
60
Object Size (KB)
Figure 11: HTTP object size distribution.
Figure 14 shows the distribution of the fraction of the total num-
ber of bytes downloaded via objects with certain sizes. For exam-
ple, we note that 50% of the total bytes downloaded come from
objects with sizes of 60KB or greater. Looking at Figure 11, we
note that less than 5% of the web objects downloaded have sizes of
60 KB or more. Hence, one can conclude that less than 5% of the
web objects downloaded account for 50% of the bytes downloaded.
4.
IMPACT OF DIFFERENT OPTIMIZA-
TIONS ON WEB PAGE LOAD TIME
In Section 3, we saw that RTTs are generally high. They are even
higher for the up-and-coming segment of users – mobile users. For
CDN operators, little can be done about this to signiﬁcantly change
the picture. This means that the bigger opportunity to reduce the
web page load time lies in reducing the RTT multiplier.
In this
section, we study different optimizations to reduce this multiplier.
Two of these optimizations attempt to reduce the number of round
trips during TCP slow start.
The ﬁrst optimization is increasing TCP ICW size, which at-
tempts to make TCP slow start begin transmitting data at a rate
closer to the maximum available bandwidth. Hence, it can use
fewer round trips to reach optimal window size that achieves the
maximum transmission rate. Section 4.1 studies the effects of us-
ing larger ICW sizes.
The second optimization is opening up the congestion window at
a higher rate during slow start. TCP slow start increases the conges-
tion window by one for every acknowledgment received. However,
delayed acknowledgments, which is pervasively deployed in the In-
ternet, makes the receiver send an acknowledgment for every other
packet received. This causes TCP congestion window to increase
by a factor of 1.5 instead of 2 (as originally intended) per round
trip. To remedy this problem, Appropriate Bytes Counting (ABC)
was introduced. ABC increases the window based on the number
of bytes acknowledged instead of just counting the number of ac-
knowledgments received. In Section 4.2, we study the effectiveness
of ABC in reducing page load time.
Finally,
in Section 4.3 we study the effectiveness of HTTP
pipelining in reducing the RTT multiplier. Moreover, we study its
interplay with increasing the ICW size.
For these different optimizations, we experimentally evaluated
their effectiveness using live trafﬁc from real users. Moreover, we
used macro benchmarks to evaluate their overall effects on web
page load times in a closed laboratory environment. In these macro
benchmarks, we constructed our synthetic workloads based on
measurements from the live traces.
4.1 Initial Congestion Window (ICW)
We have seen that the vast majority of Yahoo! CDN connections
transfer very few, and very small objects, which means that TCP
spends most of its time in the slow-start phase. For this reason,
improving the efﬁciency of this phase is crucial. When a new TCP
connection starts probing for available bandwidth, the Linux TCP
implementation follows RFC 3390 [7], which speciﬁes an ICW of
3 segments for networks having a MSS of 1460 bytes, which is the
575n
o
i
t
c
n
u
F
n
o
i
t
u
b
i
r
t
s
D
e
v
i
t
a
u
m
u
C
l
i
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55
0.5
−1
Think Time and RTO
Think Time
Think Time − RTO
0
1
2
3
4
5
Think Time (s)
)
%
(
s
e
t
y
B
f
o
e
g
a
t
n
e
c
r
e
P
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0
Bytes Transferred in Objects Larger than X
Average
Chicago
Germany
India
Singapore
20
40
60
80
100
120
140
160
Object Size (KB)
Figure 13:
(RTO).
Inter-request time and retransmission timeout
Figure 14: Percentage of overall bytes transferred across all
objects vs. object size.
most common MSS. By increasing this ICW, small objects stand
to be transferred in fewer RTTs, which when compounded across
all objects on a page can cut down the total page load time signiﬁ-
cantly.
Obviously, TCP at the server end would not send more unac-
knowledged data than is allowed by the client’s advertised receive
window so as not to overﬂow the client’s receive buffer (The re-
ceive window is dynamically allocated on most operating systems
and advertised throughout the connection). Luckily, on popular op-
erating systems (except Linux which has a much smaller receive
window), the initial receive window is quite large (64KB-256KB),
which would allow for utilizing a larger ICW. According to [11],
more than 90% of user connections have receive windows large
enough to utilize an ICW of 10. Hence, increasing TCP’s ICW can
be beneﬁcial.
4.1.1 Evaluation Using Live Trafﬁc
To test the effects of increasing the ICW size, we chose one of
our CDN sites – Singapore. We chose it due to the diversity of
its connections qualities as seen in Figure 5. There, we varied this
setting and captured traces for each ICW size.
We show in Figure 15 the distribution of object transfer time
normalized by the connection’s average RTT. This normalization
allows us to compare transfer times of objects over connections
having different RTTs. We observe a reduction of 32% in object
transfer time overall at the 80th percentile when going from an
ICW of 3 to 16. However, increasing the ICW sees diminishing
returns beyond that point. Because the effects of increasing the
ICW would be more evident during the beginning of a TCP con-
nection, we show in Figure 16 the same metric for the ﬁrst HTTP
request only, where the improvement is a 38% reduction in transfer
time. Subsequent HTTP requests beneﬁt less as the TCP window
is usually opened up at the ﬁrst request. Note that in both Fig-
ures 15 and 16 there is a fraction of the objects that are down-
loaded in under one average RTT. The reason for this is that the
average RTT is measured across all round trips measured in the
lifetime of a connection. By examining the traces, we observed
that sometimes round trip times vary signiﬁcantly within a single
connection. Hence, within a single connection an RTT can be less
than the connection’s average RTT.
A potential side effect of increasing the ICW is an increase in
retransmission rates, which we did observe as shown in Figure 17.
Note that while over 70% of connections see no retransmissions,
increasing the ICW from 3 to 16 increases the retransmit rate from
17% to 25% for about 10% of connections. As we will see later in
this section, this will have a signiﬁcant impact on the overall page
load time as it is highly sensitive to packet-loss.
To study the effects of increasing the ICW size on the tail of the
distribution of object transfer times on a per-subnet basis, we show
in Figure 18 the 80th percentile of the average object transfer times
per network preﬁx. In this ﬁgure, for each subnet, 7 data points,
corresponding to 7 different ICW sizes, are presented. Each point
represents the average object transfer time normalized by the av-
erage RTT. In the ﬁgure, we can observe that most subnets beneﬁt
from increasing the ICW and see dramatically lower object trans-
fer times. We also note that for these 20% of the objects, maximum
beneﬁt is achieved at an ICW of 16. After that, for ICW of 32, ob-
ject transfer time goes higher. In contrast, other objects are not hurt
by larger ICW sizes as seen in Figure 15. Note that we chose the
80th percentile to show that a signiﬁcant portion of the connections
in the tail of the distribution can suffer from using larger ICW size.
4.1.2 Studying Page Load time
While the previous section studied traces of individual HTTP
requests in the wild, we also wanted to capture the effects of tun-
ing the TCP ICW size on the overall page load time. Since a full
web page encompasses many objects, any straggling object down-
load will delay the overall web page download time, especially if
this straggling download is for the HTML ﬁle. Studying whole
page load times in the wild is very difﬁcult though. This is because
when using packet traces, there is no notion of a full page down-
load as the objects in a page likely span multiple connections that
are difﬁcult to tie together in a postmortem analysis. Moreover,
these objects are often at different servers. Hence, no one server
576initcwnd 1
initcwnd 3
initcwnd 5
initcwnd 7
initcwnd 10
initcwnd 16
initcwnd 32
s
T
T
R
n
i
i
e
m
T
r
e
f
n
s
a
r
T
t
c
e
b
O
j
10
8
6
4
2
0
1
2
3
4
5
6
7
8
9
0
1
1
1
2
1
3
1
4
1
5
1
6
1
7
1
8
1
9
1
0
2
1
2