### Moderately Vulnerable DRAM Chips (see Table 3)

The results vary significantly depending on the DRAM setup. In our experiments, we found that in highly vulnerable environments such as A_2, successful indiscriminate damage to the model was achieved in 24 out of 25 trials. Conversely, in less vulnerable environments like C_1, the number of successful attacks decreased to just one, with the remaining 24 attempts timing out. It is important to note that a timeout does not indicate a negative result or a crash. In fact, the single successful attack in C_1 resulted in one of the most significant relative accuracy drops (RADs) observed in the entire experiment, with RADs of 0.9992 and 0.9959 for Top-1 and Top-5, respectively. Overall, the mean number of effective attacks for this Rowhammer variant across different DRAM setups was 15.6 out of 25. The distribution of accuracy drops for Top-1 and Top-5 is illustrated in Figure 10.

### 5.3 Synopsis

In this section, we analyzed the outcomes of surgical and blind attacks against large DNN models, demonstrating how Rowhammer can be effectively deployed as an attack vector. These results align with our earlier findings in Section 4, where we estimated that at least 40% of a modelâ€™s parameters are vulnerable to single-bit corruptions. Due to this large attack surface, we showed in Section 5.1 that a Rowhammer-enabled attacker, equipped with knowledge of the network's parameters and powerful memory massaging primitives [44, 62, 67], can carry out precise and effective indiscriminate attacks within a few minutes in our simulated environment. Additionally, the resilience of code regions to spurious bit-flips allowed us to successfully execute blind attacks against the ImageNet-VGG16 model, even when the model was hidden from the attacker.

### 6 Discussion

#### Potential Mitigations

We discuss and evaluate potential mitigations to protect DNN models against single-bit attacks. Two key research directions are: restricting activation magnitudes and using low-precision numbers. Prior work on defenses against Rowhammer attacks has suggested system-level defenses [10, 27] that often require specific hardware support [6, 26]. However, these solutions have not been widely adopted due to the need for infrastructure-wide changes. Moreover, even if the infrastructure is robust to Rowhammer, adversaries can exploit other vectors to corrupt models. Therefore, we focus on solutions that the victim can apply directly to their models.

#### 6.1 Restricting Activation Magnitudes

In Section 4.3, we found that the ratio of vulnerable parameters varies based on the DNN's inherent properties. For example, using PReLU activation functions allows a model to propagate negative extreme activations. By opting for activation functions that bound the output within a specific range, such as Tanh or HardTanh [25], we can reduce the likelihood of indiscriminate damage from bit-flips. ReLU-6 [28] offers two key advantages: 1) the victim can substitute existing ReLU functions with ReLU-6 without retraining, and 2) ReLU-6 allows the victim to control the level of permitted activation by modifying the bounds, minimizing performance loss. For instance, in our ImageNet-AlexNet experiments, we set the limits to [0, max], where max is defined adaptively by the maximum activation from each layer (ReLU-A).

**Experiments:**
We evaluated three DNN models: MNIST-B, ImageNet-AlexNet, and ImageNet-VGG16. We tested four activation functions: ReLU (default), Tanh, ReLU-6, and ReLU-A (only for AlexNet and VGG16). We also compared two training methods: training from scratch (Scr) and substituting the existing activation function (Sub). Table 5 summarizes the effectiveness of these approaches. Restricting activation magnitudes with Tanh and ReLU-6 reduced the number of vulnerable parameters, especially in the MNIST models, from 50% to 1.4-2.4% without significant performance loss. For AlexNet and VGG16, the reduction was from 47.34% to 2.8% and 41.13% to 11.67%, respectively. However, some parameters remained vulnerable, particularly in the last layer. Using ReLU-A, we could trade accuracy for fewer vulnerable parameters, as shown in Table 5 and Figure 11.

**Takeaways:**
Restricting activation magnitudes allows defenders to balance the trade-off between relative accuracy drop and reducing vulnerable parameters. This approach enables ad-hoc defenses without retraining the network from scratch. However, the remaining vulnerable parameters indicate that Rowhammer attackers can still inflict damage, albeit with a reduced success rate.

#### 6.2 Using Low-Precision Numbers

Another mitigation strategy is to represent model parameters as low-precision numbers through quantization and binarization. In Section 4.3, we found that the vulnerability exploits the bitwise representation of corrupted parameters to induce dramatic changes. Using low-precision numbers, such as 8-bit quantized integers, limits the impact of bit-flips. For example, an 8-bit integer can be increased by at most 128 by flipping the most significant bit (MSB). Popular deep learning frameworks like TensorFlow support training models with low-precision numbers, allowing victims to deploy quantized or binarized models.

**Experiments:**
To validate this approach, we used three DNN models: the MNIST-L5 (baseline) and its quantized and binarized versions. We applied 8-bit quantization to convert model parameters into integers between 0 and 255, and XNOR-Net [43] for binarization, converting parameters to -1 and 1, except in the first convolutional layer. Our results, summarized in Table 6, show that using low-precision parameters reduces the percentage of vulnerable parameters from 49% (Baseline) to 0-2% (surprisingly 0% with 8-bit quantization). Most vulnerable parameters were found in the first convolutional and classification layers.

**Takeaways:**
Using low-precision parameters effectively reduces the vulnerability. However, training large models from scratch with low-precision numbers can be computationally intensive, potentially taking a week on a supercomputing cluster.

### 7 Related Work

#### DNN Resilience to Perturbations

Prior work has explored the graceful degradation of DNN models under parameter perturbations. Techniques such as network quantization [3, 5] reduce model size and inference time with minimal performance loss. This property has also been used to improve DNN security, such as injecting watermarks for ownership verification [1], adding Gaussian noise to mitigate test-time adversarial attacks [69], and fine-tuning parameters to remove malicious backdoors [37].

#### Indiscriminate Poisoning Attacks on DNNs

Recent studies in adversarial machine learning have demonstrated various attack scenarios to inflict indiscriminate damage on models. One well-studied vector is indiscriminate poisoning attacks [8], where adversaries inject malicious data into the training set to degrade model performance. Previous work suggests that such attacks may require a significant number of poisonous instances [40]. For example, Steinhardt et al. [54] showed that an attacker needs to craft 3% of the total training instances to achieve an 11% accuracy drop on the IMDB dataset. Defenses based on robust outlier removal techniques can filter out poison injection [14, 54]. Targeted poisoning attacks [49, 55] aim to cause targeted damage without affecting overall accuracy. In this paper, we analyze a test-time vulnerability that does not require the adversary to contact the training data.