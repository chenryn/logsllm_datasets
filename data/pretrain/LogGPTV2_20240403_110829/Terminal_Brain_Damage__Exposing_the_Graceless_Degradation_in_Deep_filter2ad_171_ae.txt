moderately vulnerable DRAM chips (see Table 3). Depending
on the DRAM setup, we obtain fairly different results. We
found A_2 obtains successful indiscriminate damages to the
model in 24 out of 25 experiments while, in less vulnerable
environments such as C_1, the number of successes decreases
to only one while the other 24 times out. However, it is im-
USENIX Association
28th USENIX Security Symposium    507
in total. This shows that the model effectively dominates the
memory footprint of the victim process and conﬁrms ﬁndings
from our earlier analysis that bit-ﬂips in non-vulnerable model
elements have essentially no noticeable impact.
5.3 Synopsis
Throughout the section, we analyzed the outcome of surgi-
cal and blind attacks against large DNN models and demon-
strated how Rowhammer can be deployed as a feasible at-
tack vector against these models. These results corroborate
our ﬁndings in Sec 4 where we estimated at least 40% of a
model’s parameters to be vulnerable to single-bit corruptions.
Due to this large attack surface, in Sec 5.1, we showed that
a Rowhammer-enabled attacker armed with knowledge of
the network’s parameters and powerful memory massaging
primitives [44, 62, 67] can carry out precise and effective in-
discriminate attacks in a matter of, at most, few minutes in our
simulated environment. Furthermore, this property, combined
with the resiliency to spurious bit-ﬂips of the (perhaps idle)
code regions, allowed us to build successful blind attacks
against the ImageNet-VGG16 model and inﬂict “terminal
brain damage” even when hiding the model from the attacker.
6 Discussion
In this section, we discuss and evaluate some potential mitiga-
tions to protect against single-bit attacks on DNN models. We
discuss two research directions towards making DNN models
resilient to bit-ﬂips: to restrict activation magnitudes and to
use low-precision numbers. Prior work on defenses against
Rowhammer attacks suggest system-level defenses [10, 27]
that often even require speciﬁc hardware support [6, 26].
Yet they have not been widely deployed since they require
infrastructure-wide changes from cloud host providers. More-
over, even though the infrastructure is robust to Rowhammer
attacks, an adversary can leverage other vectors to exploit
bit-ﬂips attacks to corrupt a model. Thus, we focus on the
solutions that our victim can apply to his models.
6.1 Restricting Activation Magnitudes
In Sec 4.3, we found that the vulnerable parameter ratio
changes based on inherent properties of a DNN; for instance,
using PReLU activation function allows a model to propagate
negative extreme activations. Hence, if we opt for an activa-
tion function that always bounds the output within a speciﬁc
range, a bit-ﬂip is hard to cause indiscriminate damage. There
are several functions, such as Tanh or HardTanh [25], that sup-
presses the activations; however, using ReLU-6 [28] function
provides two key advantages over the others: 1) the victim
only needs to substitute the existing activation functions from
ReLU to ReLU-6 without re-training, and 2) ReLU-6 allows
Figure 9: The successful runs of a blind attack execution
over three different DRAM setups (A_2-most, I_1-least,
and C_1-moderately vulnerable). We report the success in
terms of # f lips and #hammer attempts required to obtain an
indiscriminate damage to the victim model. We observe the
successes within few hammering attempts.
Figure 10: The distribution of relative accuracy drop for
Top-1 and Top-5. We compute them over the effective # f lips
in our experiments on the ImageNet-VGG16 model.
portant to note that a timeout does not represent a negative
result—a crash. Contrarily, while C_1 only had a single suc-
cessful attack, it also represents a peculiar case corroborating
the analysis presented in Sec 4. The corruption generated in
this single successful experiment was induced by a single bit-
ﬂip, which caused one of the most signiﬁcant RADs detected
in the entire experiment, i.e., 0.9992 and 0.9959 in Top-1 and
Top-5. Regardless of this edge case, we report a mean of 15.6
out of 25 effective attacks for this Rowhammer variant over
the different DRAM setups. Moreover, we report the distribu-
tion of accuracy drops for Top-1 and Top-5 in Figure 10. In
particular, the median drop for Top-1 and Top-5 conﬁrms the
claims made in the previous sections, i.e., the blind attacker
can expect [RAD > 0.1] on average.
Interestingly, when studying the robustness of the victim
process to Rowhammer, we discovered it to be quite resilient
to spurious bit-ﬂips. We registered only 6 crashes over all
the different DRAM conﬁgurations and experiments—300
508    28th USENIX Security Symposium
USENIX Association
I_1DRAMConfigurationAccuracyDropTop-50.00.20.40.60.81.0Top-1Network
Train
Base acc.
# Params
Vulnerability
Network
Method
Base acc.
# Params
Vulnerability
Base (ReLU)
Base (ReLU6)
Base (Tanh)
Base (ReLU6)
AlexNet (ReLU)
AlexNet (ReLU6)
AlexNet (ReLUA)
VGG16 (ReLU)
VGG16 (ReLU6)
VGG16 (ReLUA)
Scr
Scr
Scr
Sub
-
Sub
Sub
-
Sub
Sub
98.13
98.16
97.25
95.71
56.52 / 79.07
39.80 / 65.82
56.52 / 79.07
64.28 / 86.56
38.58 / 64.84
64.28 / 86.56
21,840
20,000
(61M)
20,000
(138M)
10,972 (50.2%)
313 (1.4%)
507 (2.3%)
542 (2.4%)
9.467 (47.34%)
560 (2.8%)
1,063 (5.32%)
8,227 (41.13%)
2,339 (11.67%)
2,427 (12.14%)
Table 5: Effectiveness of restricting activation.
the victim to control the level of permitted activation by mod-
ifying the bounds, e.g., using other limits instead of 6, which
minimizes the performance loss by bounding the activation.
What the victim can do is to monitor the activation values
over the validation set and to decide the limits that only sup-
presses the abnormal activation by bit-ﬂips. For example, in
our experiments with ImageNet-AlexNet, we set the limits to
[0,max], where max is deﬁned adaptively by looking at the
maximum activation from each layer (ReLU-A).
Figure 11: The vulnerability of DNN models using differ-
ent criterions. We illustrate the ImageNet-AlexNet (red lines)
and -VGG16 (black lines) cases with ReLU-6 and ReLU-A.
Experiments. We use three DNN models in Sec 4: the
MNIST-B, ImageNet-AlexNet, and ImageNet-VGG16 mod-
els. We evaluate four activation functions: ReLU (default),
Tanh, ReLU-6, and ReLU-A (only for AlexNet and VGG16),
and two training methods: training a model from scratch (Scr)
or substituting the existing activation into another (Sub). We
use the notation as the network names with the activation in
parenthesis, e.g., AlexNet (ReLU-6). For larger models, we
use the same speed-up heuristics in Sec 4.2; SV, SB, and SP.
Table 5 shows the effectiveness of our proposal. For each
network (Column 1), we list the training method, the base
L5
L5
L5
-
8-bit Quantized
XNOR Binarized
99.24
99.03
98.39
62,598
62,600
62,286
30,686 (49.0%)
0 (0.0%)
623 (1.0%)
Table 6: Effectiveness of using low-precision.
accuracy, the number of examined parameters, and the vul-
nerability (Column 2-5). We found that restricting activation
magnitudes with Tanh and ReLU-6 in some instances can
reduce the vulnerability; For instance, in the MNIST mod-
els, we observed that the number of vulnerable parameters is
reduced from 50% to 1.4-2.4% without incurring in signiﬁ-
cant performance loss. Further, we discovered that ReLU-6
achieves a similar effect without re-training of a model like
Tanh. However, there are the vulnerable parameters after the
restrictions since we cannot apply the ReLU-6 function to the
last layer. In AlexNet and VGG16, the decrease in the number
of vulnerable parameters is also generally signiﬁcant, namely
from 47.34% to 2.8% and 41.13% to 11.67%. However, we
observe the models suffer from large accuracy drops caused
by restricting the activation. To minimize the loss, we control
the bounds of activation in AlexNet (ReLU-A) and VGG16
(ReLU-A) by choosing the maximum activation from each
layer. With the ReLU-A, we can trade accuracy for the number
of vulnerable parameters as we show in Table 5. Neverthe-
less, it is interesting to see that by employing ReLU-A, while
the number of vulnerable parameters remains signiﬁcant, the
RAD also suffers from the new activation function limiting
the possible effects of the corruption. In Figure 11, the dashed
lines are for ReLU-6, the dashed-dot lines are for ReLU-A,
and the straight lines are for ReLU. We found the ReLU-A
lines are between the ReLU and ReLU-6 in AlexNet.
Takeaways. Our experimental results with restricting acti-
vation magnitudes suggest that: this mechanism 1) allows a
defender to control the trade-off between the relative accuracy
drop and reducing the vulnerable parameters and 2) enables
ad-hoc defenses to DNN models, which does not require train-
ing the network from scratch. However, the remaining number
of vulnerable parameters shows that the Rowhammer attacker
still could inﬂict damage, with a reduced success rate.
6.2 Using Low-precision Numbers
Another direction is to represent the model parameters as low-
precision numbers by using quantization and binarization.
In Sec 4.3, we found that the vulnerability exploits the bit-
wise representation of the corrupted parameter to induce the
dramatic chances in the parameter value. Our intuition is to
use low-precision numbers hard to be increased dramatically
by a bit-ﬂip; for example, an integer expressed as the 8-bit
quantized format can be increased at most 128 by a ﬂip in the
USENIX Association
28th USENIX Security Symposium    509
MSB (8th bit). Thus, the attacker only can increase a model
parameter with such a restricted bound. Training models us-
ing low-precision numbers are supported by the popular deep
learning frameworks such as TensorFlow9. The victim can
train and deploy the model with the quantized or binarized
parameters by utilizing the frameworks.
the resilience to structural changes has lead to pruning tech-
niques [4, 20, 35] which improve the efﬁciency of a DNN
model by removing unimportant neurons along with their pa-
rameters. In our work, we study the graceless degredation of
DNNs under hardware fault attacks that induce single bit-ﬂips
to individual parameters.
Experiments. To validate our intuition, we use 3 DNN mod-
els: the MNIST-L5 (baseline) and its quantized and binarized
models. When we quantize the MNIST-L5 model, we use the
8-bit quantization in [7, 64] which converts the model param-
eters in all layers into integers between 0 and 255. For the
binarization, we employ the method in XNOR-Net [43] which
converts the model parameters to -1 and 1, except the ﬁrst
convolutional layer. Using the trained models, we evaluate the
vulnerability to single bit-ﬂips, and report the accuracy, total
parameters, and vulnerability, without the speed-up heuristics.
Table 6 shows the effectiveness of using low-precision
parameters. For each network (Column 1), we note the quan-
tization method, the accuracy, the number of vulnerable pa-
rameters, and their percentage (Column 2-5). We found that
using low-precision parameters reduces the vulnerability; in
all cases, the percentage of vulnerable parameters are reduced
from 49% (Baseline) to 0-2% (surprisingly 0% with the quan-
tization). We focus on analyzing which layer has vulnerable
parameters in the binarization model. We found that mostly
the parameters in the ﬁrst convolutional (150 parameters) and
classiﬁcation (last) layers (420 parameters) are vulnerable to
a bit-ﬂip, which corroborates what observed in Sec 4.3.
Takeaways. Even though we showed the elimination of
the vulnerability through 8-bit quantization, in a real-world,
training the large model such as [65] from scratch can take a
week on a supercomputing cluster.
7 Related Work
DNN’s resilience to perturbations. Prior work has uti-
lized the graceful degredation of DNN models under parame-
ter perturbations in a wide range of applications. For example,
network quantization [3, 5], by quantizing a DNN model’s
high-precision parameter into low-precision, reduces the size
and inference time of a model with negligible performance
penalty. This property has also been used as a primitive for
improving the security of DNNs. For example, modifying the
parameter slightly to inject a watermark to allow model own-
ers to prove ownership [1]; adding Gaussian noise to model
parameter for reducing the reliability of test-time adversarial
attacks on DNNs [69]; and ﬁne-tuning the parameters for
mitigating the malicious backdoors in a model [37]. Further,
9https://www.tensorflow.org/lite/performance/post_training_
quantization
Indiscriminate poisoning attacks on DNNs. Recent work
on adversarial machine learning has demonstrated many at-
tack scenarios to inﬂict indiscriminate damage on a model.
One of the well-studied vectors is indiscriminate poisoning
attacks [8] in which the adversary, by injecting malicious data
in the victim’s training set, aims to hurt the model. Previ-
ous studies suggest that such attack might require signiﬁcant
amount of poisonous instances [40]. For example, Steinhardt
et al. [54] shows that, with IMDB dataset, an attacker needs
to craft 3% of the total training instances to achieve 11% of
accuracy drop compared to the pristine model. Further, the
defenses based on robust outlier removal techniques could
render poison injection ineffective by ﬁltering it out [14, 54].
Moreover, to achieve targeted damages without harming the
model’s overall accuracy, targeted poisoning attacks [49, 55]
have been studied. In this paper, we analyze a test-time vul-
nerability that does not require the adversary’s contact to the