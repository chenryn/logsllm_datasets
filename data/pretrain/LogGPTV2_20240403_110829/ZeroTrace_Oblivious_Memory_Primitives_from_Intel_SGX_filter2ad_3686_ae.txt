B. Evaluation of our Core Memory Controller
We ﬁrst evaluate performance of ZeroTrace for the core
memory controller component, conﬁgured to resist software-
based side channel attacks from an active adversary (Sec-
tion II-B). Figure 3 shows the time taken by a single access
request in contrast with the number of data blocks N in the
system, for DRAM and HDD untrusted storage systems. For
the points using the ORAM recursion technique, we use a
position map of size 500 KB within the EPC pages and always
set the recursion ORAM block size to 64 B (a processor cache
line). When recursion is not used, the position map (which
3Sequential access patterns maximize stash pressure [43]. Since we use a
static stash size (Section IV), this does not effect our response time.
10
Fig. 3: Representative result. Shows the number of data blocks vs.
time per request, with data blocks of size 1 KB with Path ORAM as
the the underlying ORAM for ZeroTrace.
Fig. 5: Comparison of Circuit ORAM and Path ORAM as the ORAM
schemes for ZeroTrace under passive and active adversarial models.
Each ORAM uses a data block size of 8 bytes.
Fig. 4: Detailed performance breakdown for ZeroTrace with Path
ORAM as the underlying ORAM, given a 1 KB block size. Total time
per request is the sum of controller and storage (DRAM or HDD)
times. The ORAM spills to disk given ≥ 107 blocks.
Fig. 6: Performance as a function of data ORAM block size for a
dataset with N = 107 blocks, using recursion and DRAM as the
storage backend.
is unbounded in size,) is streamed through the EPC, paging
as necessary, incurring the overhead of paging EPC pages as
mentioned in III-B6. From Figure 3, we see recursive ORAM
pays off for large datasets. This matches the theory [43] and
our system uses whichever conﬁguration achieves the best
performance, depending on public parameters.
1) Performance breakdown: Figure 4 breaks down the time
taken to run oblivious enclave code in the memory controller,
vs. the time spent servicing untrusted memory requests. We
compare two ways to cache ORAM in DRAM when capacity
spills to disk: automatic OS caching and manual tree top caching
(Section IV-E2) and ﬁnd that tree top caching signiﬁcantly
improves performance. For sufﬁciently large ORAMs, disk
time dominates access time. This issue isn’t fundamental; our
system can use an SSD to improve disk latency. For smaller
ORAMs, which will be common in the data-structure/plug-
and-play setting, the oblivious controller is the bottleneck,
given fast untrusted DRAM. Hence, to improve performance in
the context of our proposed plug-and-play memory controller,
we designed and implemented an oblivious variant of Circuit
ORAM (Section III-E) to serve as the backend ORAM scheme.4
Figure 5 compares ZeroTrace between Circuit ORAM
and Path ORAM backends, under both active and passive
adversarial models. Contrary to expectation, Circuit ORAM
does not perform signiﬁcantly better than Path ORAM given
a small (word-level) block size, which will be common in
a data-structure setting. The primary reason for this is SGX
ECALL/OCALLs have a large constant overhead of 0.015ms
in addition to the taking time proportional to the path length.
4We note that Circuit ORAM was designed to be asymptotically efﬁcient
when coded in an oblivious manner, but it still needs to be written in terms of
CMOV in our setting.
11
Circuit ORAM requires three path fetch and stores from
the server for each access, the ORAM controller logic for
Circuit ORAM is about 2-3x faster than that of Path ORAM,
however the overhead of moving these three paths in and out
of the enclave memory controller throttles Circuit ORAM’s
performance. Moreover this overhead is aggravated by recursion
as well, since Circuit ORAM pays this cost for each level of
recursion.
Breaking this down further, Figure 6 shows the controller
request time varying the data block size, between Path and
Circuit ORAM. For small data block sizes, the curve is ﬂat
because the cost of recursion dominates. In Figure 6, we see
that despite the aforementioned limitation, Circuit ORAM’s
eviction circuit begins to outperform Path ORAM signiﬁcantly
at larger block sizes. This is because the cost of obliviously
moving blocks becomes dominant at larger block sizes, and Path
ORAM’s eviction procedure has to perform signiﬁcantly more
of these oblivious move operations than Circuit ORAM. The
reason for these additional move operations in Path ORAM is
two fold; ﬁrst, recollect that Path ORAM has to iterate over the
entire stash for each bucket on a fetched path while performing
oblivious updates as explained in Section IV-D5c, whereas
Circuit ORAM makes a single stash + path pass. Second, as
mentioned in Section III-E, Circuit ORAM requires a smaller
stash size of O(1) as opposed to ω(log N ) blocks required
by Path ORAM. 5 Additionally, we note that scaling block
sizes has a discretized performance effect since we work with
the blocks at a granularity of 64 B registers. A block of 1
KB performs 16 iterations of CMOV instructions within an
oupdate function, whereas a block of 8 B performs a single
CMOV instruction.
We show a detailed performance breakdown for ZeroTrace
while varying the underlying ORAM scheme, data block size
and storage backend in the table in Figure 7. The table illustrates
the overhead of I/O for Circuit ORAM as mentioned in Section
III-E. From this table, it is clear that if the application requires
HDD backends, ZeroTrace should use Path ORAM instead
of Circuit ORAM, whereas in the plug-and-play memory
setting Path ORAM outperforms Circuit ORAM at small block
sizes and vice versa at large block sizes.6 Thus, being able
to ﬂexibly change the underlying ORAM scheme based on
public initialization parameters allows ZeroTrace to optimize
its performance. Additionally, as mentioned before if the
application requires weaker security guarantees, ZeroTrace can
revert to passive-only protection to optimize its performance
(as seen in Figure 5).
C. Evaluation of Data-Structure Modules
We now evaluate a library of oblivious data-structures,
which uses our core memory controller as a primitive. Data-
structures expose two function calls to client applications:
a) Initialize(N, size): Informs the ZeroTrace memory
controller enclave to provision storage for N size-Byte blocks.
b) Access(op, req): Performs the operation op, given
arguments as a tuple req, whose format changes based on the
data-structure. Enclaves are required to sanitize this input to
ensure proper formatting.
5In our implementation we use a static stash size of 10 for Circuit ORAM
and 90 for Path ORAM.
6We see from Figure 6 that the switch over point is at block size 100 bytes.
Fig. 8: Evaluation of our oblivious memory controller library for
Set/Dictionary/List/Array. Array is a direct call to our core memory
controller, which uses ORAM recursion to be asymptotically
efﬁcient.
c) Data-structures
supported:
Our
current
implementation supports oblivious arrays, sets, dictionaries
and lists. Array is a passthrough interface to our oblivious core
memory controller, suppporting the same interface read(addr)
and write(addr, data). Sets support the operations insert(data),
support
delete(data)
put(tag, data) and get(tag). Lists support insert(index, data)
and remove(index). These options are implemented obliviously
in the enclave followed by the necessary ORAM lookups.
contains(data). Dictionaries
and
d) Implementation and results: In our current implemen-
tation, each data-structure maintains a primitive array which
stores information used to lookup the data block stored by the
memory controller. For example, sets and dictionaries use the
array to store cryptographic hashes of data blocks, which map
array indices to addresses in the memory controller. (Given our
interface for set, above, the data storage is simply the array of
hashes. Thus, set does not have a datasize.) The data-structure
logic obliviously scans the array in O(N ) time, to ﬁnd the
block, and then makes a single memory controller access to
fetch the block. Figure 8 shows the performance for these
data structures. While our design is efﬁcient for reasonably
sized data-structures (≤ 105 elements), the O(N ) time scan
dominates for larger datasets. The O(N ) effect can be improved
with optimized data-structures from Wang et al. [51], which
makes use of ORAMs and can use our core memory controller
as a primitive as well.
VII. RELATED WORK
Our work is the ﬁrst demonstration of a completely oblivious
data structures library built on a real secure hardware platform.
For this project, we rely on research in several foundational
areas:
1) Oblivious RAMs and Secure Hardware: Research in
ORAM began with the seminal work by Goldreich and
Ostrovsky [13], and has culminated in practical constructions
with logarithmic bandwidth overhead [33], [43], [49]. In the
context of ORAM, our work moves the ORAM controller close
to storage, exploiting the fact that ORAM bandwidth overhead
12
Underlying ORAM Block Size Backend Controller Time Backend Time
Total Time
Path ORAM
Path ORAM
Path ORAM
Path ORAM
Circuit ORAM
Circuit ORAM
Circuit ORAM
Circuit ORAM
1024
1024
8
8
8
8
1024
1024
DRAM
DRAM
HDD
HDD
DRAM
DRAM
HDD
HDD
1.2141
5.9938
1.223
5.9921
1.304
3.3242
1.327
3.3359
0.0048
0.0152
40.2137
43.8868
0.0167
0.0645
132.5139
137.4236
1.2189
6.0091
41.4367
49.8789
1.3207
3.3887
133.8409
140.7595
Fig. 7: Performance numbers for ZeroTrace under different parametrizations of underlying ORAM controller, data block size and backend
storages. All timings are in ms. Experiments have N = 107 blocks, and all experiments that use HDD backends in this table make use of Tree
Top Caching. Note that the controller time is also inclusive of time spend by the controller in recursion and time taken by the overheads of
ecall/ocall.
occurs between ORAM controller and storage and not between
client and ORAM controller. This idea has been explored by
combining homomorphic encryption with ORAM [10], and by
the ORAM-based systems Oblivistore [41] and ObliviAd [3]
(which assume hypothetical secure hardware). The latter two
works have a weaker threat model than this paper: our goal
is to protect against all remote software attacks, whereas the
latter two focus only on hiding ORAM protocol-level access
patterns.
Another similar direction of research is secure hardware
projects such as Phantom [24], Aegis [44] and Ascend [11].
Phantom is a secure processor that obfuscates it’s memory
access patterns by using PathORAM intrinsically for all its
memory accesses. Aegis is aimed at incorporating privacy and
integrity guarantees for physical attacks (in addition to software
attacks) against the processor. (It makes use of PUF - Physically
Unclonable Functions to create Physical Random Functions).
Ascend is a secure coprocessor7 that aims at achieving secure
computations for a cloud server against semi-honest adversary.
It is designed to perform oblivious computations to which end
it obfuscates its instruction execution such that it appears to
spend the same time/energy/effort for the execution of each
instruction independent of the underlying instruction.
While Phantom achieves similar security goals as that of
ZeroTrace, there are several differences between our project and
such secure hardware projects. First, since these projects rely on
custom hardware that are uncommon commercially (typically
unavailable), deployability of these projects are dubious at best.
Intel SGX (and therefore ZeroTrace) is commercially available
and already present on all Intel processors from Skylake series
onwards. Secondly, these secure processors are innately tied to
providing oblivious accesses to just DRAM, however ZeroTrace
is extremely ﬂexible with respect to the underlying storage
support. Additionally, ZeroTrace also offers security ﬂexibility,
which allows applications to trade their higher level of security
for performance efﬁciency when required.
2) Systems: A number of systems investigate the question
of protecting applications running in enclaves. Raccoon [32]
provides oblivious program execution via an integration with an
ORAM and control-ﬂow obfuscation techniques. In particular,
they obfuscate programs by ensuring that all possible branches
7An additional processor that sits alongside the main server, for performing
secure computation.
are executed, regardless of the input data. This approach
conceptually differs from ours since we provide oblivious
building blocks for sensitive data with strict underlying security
guarantees. Also, because of how the control-ﬂow techniques
are enforced in Racoon, it assumes a trusted operating system
(Section 3, [32]). In our design, obliviousness is guaranteed
even when an adversary compromises the entire software stack
including the OS. Finally, while Racoon can run on an Intel
SGX-enabled processor, the architectural limitations of SGX
are not taken into consideration in their design.
GhostRider [22] proposed a software-hardware hybrid