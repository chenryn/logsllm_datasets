cient Approach to Combat a Broad Range of Memory Error Exploits, 12th USENIX
Security Symposium, 2003.
3. F. Buchholz, T. Daniels, J. Early, R. Gopalakrishna, R. Gorman, B. Kuperman,
S. Nystrom, A. Schroll, and A. Smith, Digging For Worms, Fishing For Answers,
ACSAC 2002.
4. Sung-Bae Cho, and Sang-Jun Han, Two Sophisticated Techniques to Improve
HMM-Based Intrusion Detection Systems, RAID 2003.
On Random-Inspection-Based Intrusion Detection
183
5. Scott Coull, Joel Branch, Boleslaw K. Szymanski, and Eric Breimer, Intrusion
Detection: A Bioinformatics Approach, ACSAC 2003.
6. Crispin Cowan, Calton Pu, and Heather Hinton, Death, Taxes, and Imperfect Soft-
ware: Surviving the Inevitable, theNew Security Paradigms Workshop 1998
7. Dorothy E. Denning, An intrusion detection model, IEEE Transactions on Software
Engineering, 13-2:222, Feb 1987.
8. Henry H. Feng, Oleg Kolesnikov, Prahlad Fogla, Wenke Lee, and Weibo Gong,
Anomaly Detection Using Call Stack Information, IEEE Symposium on Security
and Privacy, 2003.
9. S. Forrest, S. Hofmeyr, A. Somayaji, and T. Longstaﬀ, A sense of self for UNIX
processes, IEEE Symposium on Security and Privacy, 1996.
10. S. Forrest, A. Somayaji, and D. Ackley, Building Diverse Computer Systems, Pro-
ceeding: 6 workshop on Hot Topics in Operating Systems, IEEE Computer Society
Press, pp. 67-72.
11. Tal Gar nkel, Traps and pitfalls: Practical problems in in system call interposition
based security tools, Proc. Network and Distributed Systems Security Symposium,
February 2003.
12. Anup K. Ghosh, Christoph Michael, and Michael Schatz, A Real-Time Intrusion
Detection System Based on Learning Program Behavior, RAID 2000.
13. Jonathon T. Giﬃn, Somesh Jha, and Barton P. Miller, Detecting manipulated re-
mote call streams, 11th USENIX Security Symposium, 2002.
14. Jonathon T. Giﬃn, Somesh Jha, and Barton P. Miller, Eﬃcient context-sensitive
intrusion detection, 11th Network and Distributed System Security Symposium,
2004.
15. S. A. Hofmeyr, A. Somayaji, and S. Forrest, Intrusion detection using sequences of
system calls, Journal of Computer Security, Vol. 6, 1998, pp. 151–180.
16. Ruiqi Hu and Aloysius K. Mok, Detecting Unknown Massive Mailing Viruses Using
Proactive Methods, RAID 2004.
17. A. Jones and S. Li, Temporal Signatures of Intrusion Detection, ACSAC 2001.
18. Gaurav S. Kc, Angelos D. Keromytis, and Vassilis Prevelakis. Countering Code-
Injection Attacks With Instruction-Set Randomization. 10th ACM International
Conference on Computer and Communications Security (CCS), pp. 272 - 280.
October 2003.
19. V. Kiriansky, D. Bruening, and S. Amarasinghe, Secure execution via program
shepherding, 11th USENIX Security Symposium, 2002.
20. C. Ko, Logic Induction of Valid Behavior Speciﬁcations for Intrusion Detection,
IEEE Symposium on Security and Privacy, 2000.
21. C. Kruegel, D. Mutz, F. Valeur ,and G. Vigna, On the Detection of Anomalous Sys-
tem Call Arguments, 8th European Symposium on Research in Computer Security
(ESORICS), 2003.
22. Christopher Kruegel, Darren Mutz, William Robertson, and Fredrik Valeur,
Bayesian Event Classiﬁcation for Intrusion Detection, ACSAC 2003.
23. Lap Chung Lam and Tzi-cker Chiueh, Automatic Extraction of Accurate
Application-Speciﬁc Sandboxing Policy, RAID 2004.
24. T. Lane and C. Brodley, Temporal Sequence Learning and Data Reduction for
Anomaly Detection, ACM Trans. Info. and Sys. Security, 1999.
25. W. Lee and S. Stolfo, Data Mining Approaches for Intrusion Detection, 7th
USENIX Security Symposium, 1998.
26. p62 wbo PI:EMAIL, Jamie Butler, and p62 wbo PI:EMAIL,
Bypassing 3rd Party Windows Buﬀer Overﬂow Protection, Phrack, Issue #62, of
July 10, 2004
184
S.P. Chung and A.K. Mok
27. R. Sekar, M. Bendre, D. Dhurjati, and P. Bollineni, A Fast Automaton-based
Method for Detecting Anomalous Program Behaviors, Proceedings of the 2001 IEEE
Symposium on Security and Privacy.
28. R. Sekar, A. Gupta, J. Frullo, T. Shanbhag, A. Tiwari, H. Yang, and S. Zhou, Spec-
iﬁcation based anomaly detection: a new approach for detecting network intrusions,
ACM Computer and Communication Security Conference, 2002.
29. skape, Understanding Windows Shellcode,
http://www.hick.org/code/skape/papers/win32-shellcode.pdf
30. A. Somayaji, S. Forrest, Automated Response Using System-Call Delays, 9th Usenix
Security Symposium, 2000.
31. Kymie M. C. Tan, and Roy A. Maxion, “Why 6?” Deﬁning the Operational Limits
of Stide, an Anomaly-Based Intrusion Detector, IEEE Symposium on Security and
Privacy 2002.
32. Kymie M. C. Tan, Kevin S. Killourhy, and Roy A. Maxion, Undermining an
Anomaly-Based Intrusion Detection System Using Common Exploits, RAID 2002
33. Thomas Toth, Christopher Krugel, Accurate Buﬀer Overﬂow Detection via Ab-
stract Payload Execution, RAID 2002.
34. P. Uppuluri and R. Sekar, Experiences with Speciﬁcation-Based Intrusion Detec-
tion, RAID 2001
35. D. Wagner and P. Soto, Mimicry Attacks on Host-Based Intrusion Detection Sys-
tems, ACM Conference on Computer and Communications Security, 2002.
36. D. Wagner and D. Dean, Intrusion Detection via Static Analysis, IEEE Symposium
on Security and Privacy, 2001.
37. Christina Warrender, Stephanie Forrest, and Barak Pearlmutter, Detecting intru-
sions using system calls: alternative data models, IEEE Symposium on Security
and Privacy, 1999.
38. A. Wespi, M. Dacier and H. Debar, Intrusion detection using variable-length audit
39. Matthew M. Williamson, Throttling Viruses: Restricting propagation to defeat ma-
trail patterns, RAID, 2000.
licious mobile code, ACSAC 2002.
40. Haizhi Xu, Wenliang Du and Steve J. Chapin, Context Sensitive Anomaly Moni-
toring of Process Control Flow to Detect Mimicry Attacks and Impossible Paths,
RAID 2004.
Environment-Sensitive Intrusion Detection
Jonathon T. Gifﬁn1, David Dagon2, Somesh Jha1, Wenke Lee2, and Barton P. Miller1
1 Computer Sciences Department, University of Wisconsin
2 College of Computing, Georgia Institute of Technology
{giffin, jha, bart}@cs.wisc.edu
{dagon, wenke}@cc.gatech.edu
Abstract. We perform host-based intrusion detection by constructing a model
from a program’s binary code and then restricting the program’s execution by
the model. We improve the effectiveness of such model-based intrusion detection
systems by incorporating into the model knowledge of the environment in which
the program runs, and by increasing the accuracy of our models with a new data-
ﬂow analysis algorithm for context-sensitive recovery of static data.
The environment—conﬁguration ﬁles, command-line parameters, and
environment variables—constrains acceptable process execution. Environment
dependencies added to a program model update the model to the current envi-
ronment at every program execution.
Our new static data-ﬂow analysis associates a program’s data ﬂows with
speciﬁc calling contexts that use the data. We use this analysis to differentiate
system-call arguments ﬂowing from distinct call sites in the program.
Using a new average reachability measure suitable for evaluation of call-stack-
based program models, we demonstrate that our techniques improve the precision
of several test programs’ models from 76% to 100%.
Keywords: model-based anomaly detection, Dyck model, static binary analysis,
static data-ﬂow analysis.
1 Introduction
A host-based intrusion detection system (HIDS) monitors a process’ execution to iden-
tify potentially malicious behavior. In a model-based anomaly HIDS or behavior-based
HIDS [3], deviations from a precomputed model of expected behavior indicate possible
intrusion attempts. An execution monitor veriﬁes a stream of events, often system calls,
generated by the executing process. The monitor rejects event streams deviating from
the model. The ability of the system to detect attacks with few or zero false alarms relies
entirely upon the precision of the model.
Static analysis builds an execution model by analyzing the source or binary code
of the program [5, 20, 10, 14]. Traditionally, static analysis algorithms are conservative
and produce models that overapproximate correct execution. In particular, previous sta-
tically constructed models allowed execution behaviors possible in any execution en-
vironment. Processes often read the environment—conﬁguration ﬁles, command-line
parameters, and environment variables known at process load time and ﬁxed for the
entire execution of the process. The environment can signiﬁcantly constrain a process’
A. Valdes and D. Zamboni (Eds.): RAID 2005, LNCS 3858, pp. 185–206, 2006.
c(cid:2) Springer-Verlag Berlin Heidelberg 2006
186
J.T. Gifﬁn et al.
execution, disabling entire blocks of functionality and restricting the process’ access.
If the process can generate the language of event sequences Le given the current en-
vironment e, then previous program models constructed from static analysis accepted
the language Ls = ∪i∈ELi for E the set of all possible environments. Ls is a super-
set of Le and may contain system call sequences that cannot be generated by correct
execution in environment e.
These overly general models may fail to detect attacks. For example, versions of
the OpenSSH secure-shell server prior to 3.0.2 had a design error that allowed users to
alter the execution of the root-level login process [19]. If the conﬁguration ﬁle setting
“uselogin” was disabled, then the ssh server disabled the vulnerable code. However, an
attacker who has subverted the process can bypass the “uselogin” checks by directly
executing the vulnerable code. Previous statically constructed models allowed all paths
in the program, including the disabled path. By executing the disabled code, the attacker
can undetectably execute root-level commands.
In this paper, we make statically constructed program models sensitive to the execu-
tion environment. An environment-sensitive program model restricts process execution
behavior to only the behavior correct in the current environment. The model accepts a
limited language of event sequences Lv, where Le ⊆ Lv ⊆ Ls. Event sequences that
could not be correctly generated in the current environment are detected as intrusive,
even if those sequences are correct in some other environment. In the OpenSSH exam-
ple, if “uselogin” was disabled, then the model disallows system calls and system-call
arguments reachable only via the vulnerable code paths. The model detects an entire
class of evasion attacks that manipulate environment data, as described in Sect. 7.4.
Environment dependencies characterize how execution behavior depends upon en-
vironment values. Similar to def-use relations in static data-ﬂow analysis [15], an en-
vironment dependency relates values in the environment, such as “uselogin”, to values
of internal program variables. When an environment-sensitive HIDS loads a program
model for execution enforcement, it customizes the model to the current environment
based upon these dependencies. In this paper, we manually identify dependencies. Our
long-term goal is to automate this procedure, and in Sect. 5.3 we postulate that auto-
mated identiﬁcation will not be an onerous task.
Environment sensitivity works best with system-call argument analysis. Our static
analyzer includes powerful data-ﬂow analysis to recover statically known system-call
arguments. Different execution paths in a program may set a system-call argument dif-
ferently. Our previous data-ﬂow analysis recovered argument values without calling
context, in that the analysis algorithm ignored the association between an argument
value and the call site that set that value [9,10]. In this work, we encode calling context
with argument values to better model the correct execution behavior of a program. A
system-call argument value observed at runtime must match the calling context leading
up to the system call. Additionally, the data-ﬂow analysis now crosses shared object
boundaries, enabling static analysis of dynamically-linked executables.
Although environment-sensitive program modeling is the primary focus of our work,
we make an additional contribution: a new evaluation metric. The existing standard
metric measuring model precision, average branching factor, poorly evaluates models
that monitor a program’s call stack in addition to the system-call stream [5, 8]. We
Environment-Sensitive Intrusion Detection
187
instead use context-free language reachability to move forward through stack events to
discover the next set of actual system calls reachable from the current program location.
Our new average reachability measure fairly evaluates the precision of program models
that include function call and return events. Using the average reachability measure, we
demonstrate the value of whole-program data-ﬂow analysis and environment-sensitive
models. On four test programs, we improved the precision of context-sensitive models
from 76% to 100%.
In summary, we believe that this paper makes the following contributions:
– Static model construction of dynamically-linked executables. In particular, the sta-
tic analyzer continues data-ﬂow analysis across shared-object boundaries by learn-
ing the API by which programs call library code, as described in Sect. 4.1.
– Context-sensitive encoding of recovered system-call arguments, detailed in Sect. 4.2.
Combined with whole-program analysis, this technique improved argument recov-
ery by 61% to 100% in our experiments.
– A formal deﬁnition of environment-sensitive program models and methods to en-
code environment dependencies into statically constructed program models. Envi-
ronment sensitivity and static system-call argument recovery improved the preci-
sion of program models by 76% to 100%. Section 5 presents this work.
– An extension to the commonly-used average branching factor metric suitable for
program models that require update events for function calls and returns (Sect. 6).
The average reachability measure provides a fairer comparison of call-stack-based
models and other models that do not monitor the call stack.
2 Related Work
In 1994, Fix and Schneider added execution environment information to a programming
logic to make program speciﬁcations more precise [7]. The logic better speciﬁed how
a program would execute, allowing for more precise analysis of the program in a proof
system. Their notion of environment was general, including properties such as sched-
uler behavior. We are proposing a similar idea: use environment information to more
precisely characterize expected program behavior in a program model. As our models
describe safety properties that must not be violated, we focus on environment aspects
that can constrain the safety properties.
Chinchani et al. instrumented C source-code with security checks based upon envi-
ronment information [1]. Their deﬁnition of environment primarily encompassed low-
level properties of the physical machine on which a process executes. For example,
knowing the number of bits per integer allowed the authors to insert code into a pro-
gram to prevent integer overﬂows. This approach is speciﬁc to known exploit vectors
and requires source-code editing, making it poorly suited for our environment-sensitive
intrusion detection.
One aspect of our current work uses environment dependencies and static analysis to
limit allowed values to system-call arguments. This speciﬁc problem has received prior
attention.
Static analysis can identify constant, statically known arguments. While extracting
execution models from C source code, Wagner and Dean identiﬁed arguments known
188
J.T. Gifﬁn et al.
callsite 1
unlink
call
callsite 2
call unlink
arg ∈ {“/home/user/testﬁle”}
arg ∈ {“/tmp/Mx.*”}
callsite 1
unlink
call
callsite 2
unlink
call
arg ∈ {“/home/user/testﬁle”}
arg is unknown
libc:
unlink
entry
arg ∈ {“/home/user/testﬁle”,
“/tmp/Mx.*”}
unlink
kernel trap
(A)
libc:
unlink
entry
arg is unknown
unlink
kernel trap
(B)
Fig. 1. Prior static argument recovery. Argument values recovered along different execution paths
join together when the execution paths converge. (A) The association between a speciﬁc argument
value and an execution path is lost. (B) If an argument value cannot be statically recovered on
any execution path leading to a system call, all other recovered values must be discarded. The
argument is completely unconstrained.
statically [20]. In earlier work, we used binary code analysis to recover arguments in
SPARC executables [9, 10]. These efforts suffered from several problems:
– Earlier binary data-ﬂow analysis required statically-linked executables. In this pa-
per, we use data-ﬂow analysis to learn the API for a shared object. When analyzing
an executable, we continue data-ﬂow analysis anywhere the library API is used.
– Values recovered were not sensitive to calling context. This forces two inaccura-
cies. First, the association between a system-call argument value and the execution
path using that value is lost (Fig. 1A). An attacker could undetectably use a value
recovered on one execution path on any other execution path to the same system
call. Second, if any execution path set an argument in a way not recoverable stati-
cally, all values recovered along all other execution paths must be discarded for the
analysis to be safe (Fig. 1B). Our current work avoids these two inaccuracies by
encoding calling context with recovered values.
– Static analysis cannot recover values set dynamically. In this paper, we make a
distinction between dynamic values set at load time and values set by arbitrary user
input. Environment dependencies augment static analysis and describe how values
set when the operating system loads a process ﬂow to system-call arguments.
Dynamic analysis learns a program model by generalizing behavior observed during
a training phase. Kruegel et al. [13] and Sekar et al. [16] used dynamic analysis to learn
constraints for system-call arguments. These constraints will include values from the
environment that are used as part of a system-call argument, which forces a tradeoff.
The training phase could modify environment values to learn a general model, but such a
model fails to constrain later execution to the speciﬁc environment. Conversely, training
could use only the current environment. If the environment ever changes, however, then
the model no longer characterizes correct execution and retraining becomes necessary.
By including environment dependencies described in this paper, learning could be done
only for arguments not dependent upon the environment. Environment dependencies
Environment-Sensitive Intrusion Detection
189
would resolve the remaining arguments to the current environment every time the model
was subsequently loaded.
Environment-sensitive models are well suited to the model-carrying code execution
design. Sekar et al. proposed that unknown, untrusted executables can include models
of their execution [16]. A consumer of the executable can use a model checker to verify
that the model does not violate their security policy and an execution monitor to limit
the program’s execution to that allowed by the model. The code producer must build the
program model, but they cannot know any consumer’s speciﬁc execution environment.
To avoid false alarms, the model must be general to suit all possible environments. Such
a general model may not satisfy a consumer’s security policy. If the code producer
adds environment dependencies to the model shipped with the code, the model will
automatically adapt to every consumer’s unique environment. With the environment
constraints, the model is increasingly likely to satisfy a consumer’s security policy.
3 Overview
Model-based anomaly detection has two phases: construction of the program model and