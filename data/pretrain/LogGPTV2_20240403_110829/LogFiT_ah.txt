|---|---|---|---|
| HDFS |176.04 |18.63 |146 |
| BGL |128.66 |15.73 |6046 |
| Thunderbird |1445.70 |126.63 |15557 |
Table I 
PER-PARAGRAPH WORD AND SENTENCE STATISTICS FOR THE DATASETS.similarly chunked into paragraphs, each consisting of the consecutive sentences belonging to a time window of 30 seconds, following the approach discussed in [5], [7]. Furthermore, a log paragraph is considered an anomaly if it includes at least one log sentence that is tagged as an anomalous event. The full Thunderbird dataset consists of 10,000,000 log sentences, of which 4,934 are anomalies.For each of the datasets mentioned, a total of 5,000 log paragraphs were used to train LogFiT (sampled from 10,000 log paragraphs set aside for training). A separate tuning set consisting of 4,500-5,000 normal plus 500-1,000 anomaly paragraphs were used to tune the LogFiT model’s hyper parameters. Furthermore, a separate evaluation set consisting of 4,500-5,000 normal and 500-1,000 anomaly samples was set aside to evaluate the performance of the tuned LogFiT model. No random shuffling is done when splitting the datasets into subsets - the sequential order of the logs is preserved; this is to prevent information leakage during model training. Lastly, to test the models’ ability to handle variation in the syntactic structure of the input log data, the evaluation set is dynamically modified during model evaluation, so that the top 10% most commonly occurring verbs are replaced with their WordNet lemmas.Baselines. The performance of the LogFiT model is compared against the following baselines.
• DeepLog [4]. The DeepLog model uses an LSTM-based architecture to train an anomaly detection model using sequences of normal log templates. Anomalies are detected by letting the model predict the next log template given its n preceding log templates. The top-k accuracy of the prediction is the anomaly score. The DeepLog implementation from the logdeep library1was used to obtain the results reported in the experiments section. It is worth noting that the performance metrics reported in the DeepLog paper cannot be reproduced using this implementation of DeepLog.• LogBERT [5]. The LogBERT model uses a BERT-based architecture to learn the patterns of normal log templates, using masked log key prediction and centroid distance minimisation to fit the model to the training data. Anomalies are detected by allowing the model to predict n randomly masked log keys in a sequence, and then using the top-k accuracy and centroid distance to compute an anomaly score. If either the top-k accuracy or centroid distance threshold is exceeded, the input data is considered an anomaly. The publicly available LogBERT source code2is used to obtain the results reported in1Available from https://github.com/donglee-afar/logdeep 2Available from https://github.com/HelenGuohx/logbert
the paper. It is worth noting that the performance metrics reported in the LogBERT paper cannot be reproduced.
	Implementation Details. LogFiT was implemented using Python 	and 	leveraged 	several 	well-known 	libraries 	to accelerate the development and evaluation of the model.• Pre-processing of the raw log data was done using simple regular expressions to replace unimportant (because they are too specific) details such as IP addresses and port numbers, numeric values, file paths, block IDs (and other types of identifiers). Python and its extensive data analysis tools were used for this task.• Pytorch - Pytorch is a Python-based deep learning framework developed by Facebook Research. Pytorch allows researchers to build and train complex neural network models using a Pythonic syntax that is intuitive and easy to use. LogFiT uses Pytorch as interface to the underlying hardware-accelerated tensor runtime.• FastAI - FastAI is a high-level deep learning framework that has been designed to run on top of Pytorch. This framework focuses on efficient model training workflows and incorporates the latest deep learning techniques and best practices. LogFiT uses FastAI to implement its training and evaluation strategies in concise Python code.• HuggingFace - HuggingFace is a Python library and ecosystem for building and sharing Transformers-based models. LogFiT uses the HuggingFace library to jump start model development by inheriting from pre-trained Transformer models available from the HuggingFace model hub.The source code implementing the LogFiT model, datasets and model checkpoints will be made available online. 