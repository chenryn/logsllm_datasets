### Application Architecture and Functionality

The application is responsible for handling socket communications, implementing an interface for persistent storage, and hosting an enclave that runs the server-side LCM (Lightweight Consistency Mechanism) protocol and the Key-Value Store (KVS). When a client request is received, the server application collects the messages in a bounded queue (batch). Once the queue reaches its limit or no more client requests are available, the server application performs an ecall (enclave call) and passes the batched invoke messages to the LCM protocol. The LCM protocol processes the batch and returns the reply messages along with the encrypted application and protocol state. The server application then writes the states to disk and forwards the reply messages to the clients.

To achieve crash tolerance, the server application must write the state synchronously to disk (using `fsync`), which can significantly decrease performance. Our prototype implementation of the LCM-protected key-value store consists of approximately 4000 lines of code (sloc), with around 2200 sloc dedicated to the enclave components. The remaining code is for the untrusted server implementation, including storage and network code. The KVS client and the LCM client-library add an additional 1600 sloc to the prototype.

### Evaluation

We evaluated the overhead of LCM using a set of benchmarks with YCSB (Yahoo! Cloud Serving Benchmark) and compared it against an SGX-protected key-value store without rollback and forking protection. We also compared LCM's performance against a trusted monotonic counter approach and an unprotected Redis.

#### Experiment Setup

The experiments were conducted on a Dell Optiplex 7040 desktop machine with an i7-6700 Intel CPU, which is SGX-capable. The server was equipped with 8 GB of memory, a 1 Gbps network connection, and an SSD drive. Clients were simulated on a virtual machine with 24 virtual CPUs and 8 GB of memory, running YCSB as the workload generator using Oracle Java (JRE 8, build 1.8.0_111-b14). All machines ran Ubuntu Linux 14.04.4 Server with the generic 4.4.0-47 Linux kernel.

YCSB, an extensible tool for benchmarking key-value stores, supports various key-value stores such as Redis and Cassandra. For our evaluation, we used workload A, which includes a 50/50 mix of PUT and GET operations. Each reported data point was taken over a period of 30 seconds. We integrated the KVS client, including the LCM client-library, with YCSB. As a baseline, we used our KVS (see Section V-C) protected with SGX. For comparison with Redis and our native KVS implementation, we used Stunnel to secure the communication with the clients. Redis, originally designed for private networks, does not support TLS connections. Both LCM and the SGX-based KVS prototype use AES-GCM encryption with 128-bit keys for secure communication with the clients. To simplify the evaluation process, we used predefined encryption keys.

#### Enclave Memory

In a preliminary experiment, we evaluated the memory consumption of the SGX key-value store by inserting one million objects and measuring the enclave heap allocation using `sgx-gdb`. Each object had a key size of 40 bytes and a value size of 100 bytes. For 300,000 objects, we measured an allocation of 93 MB of enclave memory, whereas we expected only about 40 MB. It turned out that the KVS implementation based on `std::map` comes with a memory overhead of about 134%. Specifically, each string key-value pair consumed about 280 bytes, while the map data structure allocated an additional 48 bytes per object for maintaining an internal search structure. Additionally, we measured the latency of PUT and GET operations for different numbers of objects. Given the limited EPC (Enclave Page Cache), we expected a performance drop when the number of objects increased and the SGX driver started swapping EPC pages. We observed that the latency increased up to 240% when the KVS held more than 300,000 objects. Due to space constraints, we do not show the graph. We assume that this hardware restriction will be addressed in future CPU releases and thus chose our further evaluation workloads to fit within the EPC.

#### LCM Protocol Message Overhead

We first studied how the LCM protocol message overhead affects throughput. As described in Section IV-B, LCM sends additional metadata, such as the sequence number and hash chain value, along with a client request. Our LCM implementation adds 45 bytes to an operation invocation and 46 bytes to a result. This overhead remains constant regardless of the operation and result sizes. To evaluate this, we ran the experiment with 8 clients for 1000 objects of varying sizes (100 to 2500 bytes). Figure 4 shows that the throughput of LCM behaves similarly to the plain SGX KVS. As expected, LCM introduces an overhead, but it decreases with larger object sizes. Specifically, for objects of 100 bytes, the throughput is 20.12% lower, and for objects of 2500 bytes, it is 10.96% lower compared to the plain SGX KVS.

![Throughput with Different Object Sizes](figure4.png)

#### Throughput with Different Numbers of Clients

We also studied the overall throughput of LCM by increasing the number of clients. This workload used up to 32 clients, 1000 objects with a fixed size of 100 bytes, and 40-byte keys. In this experiment, we compared two variants of LCM against a KVS without SGX ("Native"), an SGX-secured KVS ("SGX"), Redis, and an SGX-secured KVS with an emulated trusted monotonic counter ("SGX+TMC"). We ran LCM and SGX without batching enabled and with batching of up to 16 operations. We configured Redis to use an append log strategy for persistence and disabled `fsync` (synchronous disk writes) for both Redis and our KVS prototypes. As shown in Figure 5, the throughput of Redis and the Native KVS scales almost linearly. In contrast, LCM and SGX reach saturation with 8 clients. We observed that the SGX KVS achieves 0.42x to 0.78x the throughput of the Native KVS. LCM, on the other hand, achieves 0.67x to 0.95x the throughput of the SGX KVS, and with batching, even 0.72x to 0.98x. The reason is that LCM and SGX are single-threaded applications and perform the encryption of every client request inside the enclave. Although Redis and the Native KVS are also single-threaded, they leverage Stunnel, which uses multiple processes to encrypt/decrypt client communication, making secure communication a bottleneck.

![Throughput with Different Numbers of Clients](figure5.png)

#### Performance Impact of Trusted Monotonic Counter

In this experiment, we investigated the performance impact of Trusted Monotonic Counters (TMC) when used to protect against rollback and forking attacks. The current version (Version 1.6) of the SGX driver and SDK do not yet support Intel’s TMC on Linux. However, on Windows, they are available via the Intel Management Engine (ME), which stores the counter in non-volatile memory. We measured an average latency of 60 ms to increment an SGX TMC on Windows, whereas [38] reported a latency of 2000 μs.