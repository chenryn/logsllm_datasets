application handles the socket communications, implements an
interface for persistent storage, and hosts an enclave running
the server-side LCM protocol and the KVS. When the server
application receives a client request, it collects the message in
a bounded queue (batch). Once the queue has reached limit
or no more client request are available, the server application
performs an ecall and passes the batched invoke messages to
the LCM protocol, which, in turn, has processed the batch,
it returns the reply messages and encrypted application and
protocol state. The server application then, writes the states
to disk and forwards the replay messages to the clients. Note
in order to achieve crash tolerance, the server application has
to write the state synchronously to disk (fsync), this clearly
decreases the performance. Our prototype implementation of
a LCM-protected key-value store comprises about 4000 sloc,
where enclave components comprise around 2200 sloc. The
rest is for the untrusted server implementation including the
storage and network code. The KVS client and the LCM
client-library add additional 1600 sloc to the prototype.
VI. EVALUATION
We evaluated the overhead of LCM with a set of bench-
marks using YCSB and compare it against a SGX protected
key-value store without rollback and forking protection. Fur-
thermore, we compare the LCM performance against a trusted
monotonic counter approach and an unprotected Redis.
A. Experiment setup
The experiments use a Dell Optiplex 7040 desktop ma-
chine with an i7-6700 Intel CPU that is SGX-capable to run
the server. It is equipped with 8 GB of memory, 1 Gbps
network connection and an SSD drive. We simulate clients
on a virtual machine with 24 virtual CPUs and 8 GB of
memory, running YCSB as workload generator using Oracle
Java (JRE 8, build 1.8.0_111-b14). All machines run Ubuntu
Linux 14.04.4 Server with the generic 4.4.0-47 Linux kernel.
The evaluation is driven by YCSB [16], an extensible tool for
benchmarking key-value stores. It supports many different key-
value stores, such as Redis (http://redis.io), Cassandra (https:
//cassandra.apache.org/) and many more. YCSB comes with a
set of core workloads spanning different application scenarios.
For the evaluation we use workload A with a mix of 50/50
165
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:58:47 UTC from IEEE Xplore.  Restrictions apply. 
PUT and GET operations and show the overall throughput
of all clients. Every reported data point
is taken over a
period of 30 seconds. We integrated the KVS client including
the LCM client-library with YCSB. As a baseline for our
experiments we use our KVS (see Sec. V-C) protected with
SGX. For the comparison with Redis and our native KVS
implementation we use Stunnel (https://www.stunnel.org) to
secure the communication with the clients. Redis has been
originally designed for deployment in private networks, thus,
it does not support TLS connections. LCM and the SGX-
based KVS prototype establish a secure communication with
the clients by using AES-GCM encryption with 128 bit keys.
In order to simplify the evaluation process we use predeﬁned
encryption keys.
B. Enclave memory
In a preliminary experiment we evaluated the memory
consumption of the SGX key-value store. We inserted one
million objects and measured the enclave heap allocation
using sgx-gdb. Each object with a key size of 40 byte
and 100 byte values. For 300000 objects we measured an
allocation of 93 MB enclave memory whereas we expected
only about 40 MB. It turned out that the KVS implementation
based on std::map comes with an
memory overhead of about 134%. In particular, the string
key-value pairs consume about 280 byte whereas the map
data structure allocates additional 48 byte for each object
for maintaining an internal search structure. Moreover, we
measured the latency of PUT and GET operations for different
number of objects. As the EPC is limited, we expected a
performance drop when the number of objects increases and
the SGX driver starts swapping EPC pages as also reported
in [2], [8]. We observed that the latency increases up to 240%
when the KVS holds more than 300000 objects. We refrain
from showing the graph due to page reasons. We assume
that this hardware restriction will be addressed in future CPU
releases and thus choose our further evaluation workloads to
ﬁt into the EPC.
C. LCM protocol message
We ﬁrst study how the LCM protocol message overhead
affects the throughput. As described in Sec. IV-B, LCM sends
additional metadata, such as the sequence number and hash
chain value, along with a client request. In particular, our
LCM implementation adds 45 byte to an operation invocation
and 46 byte to a result. This overhead remains constant for
varying operation and result sizes. In order to evaluate this,
we run the experiment with 8 clients for 1000 objects of
size 100 to 2500 byte. Fig. 4 shows that the throughput of
LCM behaves similar to the plain SGX KVS. As expected,
we observe that LCM introduces an overhead but it decreases
with bigger object sizes. In particular, for objects with the size
of 100 byte the throughput is 20.12% and for objects with size
of 2500 byte it is 10.96% lower compared to the plain SGX
KVS.
(cid:1)
(cid:1)
(cid:1)
(cid:1)
]
]
c
c
e
e
s
s
/
/
s
s
p
p
o
o
k
k
[
[
t
t
u
u
p
p
h
h
g
g
u
u
o
o
r
r
h
h
T
T
20
20
15
15
10
10
5
5
(cid:1) SGX
(cid:1) SGX
LCM
LCM
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
100
100
500
500
1000
1000
1500
1500
2000
2000
2500
2500
Object Size
Object Size
Fig. 4. Throughput with different object sizes with async disk writes.
60
60
50
50
40
40
30
30
20
20
10
10
]
]
c
c
e
e
s
s
/
/
s
s
p
p
o
o
k
k
[
[
t
t
u
u
p
p
h
h
g
g
u
u
o
o
r
r
h
h
T
T
SGX
SGX
SGX with batching
SGX with batching
Native
Native
LCM
LCM
LCM with batching
LCM with batching
Redis TLS
Redis TLS
SGX + TMC
SGX + TMC
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
1
1
(cid:1)
(cid:1)
(cid:1)
(cid:1)
2
2
(cid:1)
(cid:1)
(cid:1)
(cid:1)
8
8
(cid:1)
(cid:1)
(cid:1)
(cid:1)
4
4
# of Clients
# of Clients
(cid:1)
(cid:1)
(cid:1)
(cid:1)
16
16
(cid:1)
(cid:1)
(cid:1)
(cid:1)
32
32
Fig. 5. Throughput with different numbers of clients with async disk writes.
D. The throughput of LCM
We also study the overall throughput of LCM by increasing
the number of clients. This workload uses up to 32 clients,
1000 objects with a ﬁxed size of 100 byte. Each object key
is 40 byte. In this experiment we compare two variants of
LCM against a KVS without SGX (“Native”), SGX-secured
KVS (“SGX”), Redis, and SGX-secured KVS with emulated
trusted monotonic counter (“SGX+TMC”). We run LCM and
SGX without batching enabled and with batching of up to 16
operations. We conﬁgured Redis to use an append log strategy
for persistence. We also disabled fsync (synchronous disk
writes) for Redis as well as for our KVS prototypes. As Fig. 5
shows, the throughput of Redis and the Native KVS scale
almost linear. In contrast, LCM and SGX reach saturation
already with 8 clients. We observed that the SGX KVS reaches
0.42x – 0.78x the throughput of the Native KVS. LCM, on the
other hand, reaches 0.67x – 0.95x the throughput of the SGX
KVS, with batching even 0.72x – 0.98x. The reason is, LCM
and SGX are single threaded applications and perform the
encryption of every client request inside the enclave. Although,
Redis and Native KVS are also single threaded, they leverage
Stunnel that uses multiple processes to encrypt/decrypt client
communication. That way, secure communication becomes a
bottleneck.
E. The performance impact of Trusted Monotonic Counter
In this experiment we investigate the performance impact
of Trusted Monotonic Counters (TMC) when used to protect
against rollback and forking attacks. The current version
(Version 1.6) of the SGX driver and SDK do not yet support
Intel’s Trusted Monotonic Counter [23] on Linux. However,
on Windows [22] they are available provided by the Intel
management engine (ME) that stores the counter in non-
volatile memory. We measured an average latency of 60ms to
increment a SGX TMC on Windows, whereas [38] reported
166
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:58:47 UTC from IEEE Xplore.  Restrictions apply. 
2000
2000
1500
1500
(cid:1)
(cid:1)
(cid:1)
(cid:1)
SGX
SGX
SGX with batching
SGX with batching
Native
Native
LCM
LCM
LCM with batching
LCM with batching
Redis TLS
Redis TLS
SGX + TMC
SGX + TMC
]
]
c