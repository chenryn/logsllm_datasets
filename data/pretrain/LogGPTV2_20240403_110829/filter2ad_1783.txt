title:Poster: Effective Layers in Coverage Metrics for Deep Neural Networks
author:Leo Hyun Park and
Sangjin Oh and
Jaeuk Kim and
Soochang Chung and
Taekyoung Kwon
Poster: Effective Layers in Coverage Metrics for Deep Neural
Networks
Leo Hyun Park , Sangjin Oh, Jaeuk Kim, Soochang Chung, and Taekyoung Kwon
Yonsei University, Seoul, Korea
{dofi,dhtkdwls537,freak0wk,warpstar,taekyoung}@yonsei.ac.kr
ABSTRACT
Deep neural networks (DNNs) gained in popularity as an effective
machine learning algorithm, but their high complexity leads to the
lack of model interpretability and difficulty in the verification of
deep learning. Fuzzing, which is an automated software testing
technique, is recently applied to DNNs as an effort to address these
problems by following the trend of coverage-based fuzzing. How-
ever, new coverage metrics on DNNs may bring out the question
of which layer to measure the coverage in DNNs. In this poster, we
empirically evaluate the performance of existing coverage metrics.
By the comparative analysis of experimental results, we compile
the most effective layer for each of coverage metrics and discuss a
future direction of DNN fuzzing.
CCS CONCEPTS
• Security and privacy → Software and application security.
KEYWORDS
deep neural network; coverage; fuzzing; adversarial examples
ACM Reference Format:
Leo Hyun Park , Sangjin Oh, Jaeuk Kim, Soochang Chung, and Taekyoung
Kwon. 2019. Poster: Effective Layers in Coverage Metrics for Deep Neural
Networks. In 2019 ACM SIGSAC Conference on Computer& Communications
Security (CCS ’19), November 11–15, 2019, London, United Kingdom. ACM,
New York, NY, USA, 3 pages. https://doi.org/10.1145/3319535.3363286
1 INTRODUCTION
Deep neural networks (DNNs), which have become the mainstream
of machine learning algorithms, demonstrate outstanding perfor-
mance in various fields, including but not limited to NLP, computer
vision, and voice recognition. The complicated structure is a key
factor in the success of DNNs. It enables the network to be expres-
sive, however, also brings down the interpretability and makes it
difficult for people to understand the grounds for the prediction
results.
A general approach of verifying DNNs is measuring the predic-
tion performance of networks with a test data set, but this approach
has severe limitations [4]. First, it highly depends on the quality
of the data set, which may not represent all of the samples in the
real world. Second, it does not consider a realistic threat to DNNs,
such as adversarial examples which may target a blind spot of a
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
CCS ’19, November 11–15, 2019, London, United Kingdom
© 2019 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-6747-9/19/11.
https://doi.org/10.1145/3319535.3363286
(a) L2 constraint = 5
(b) L2 constraint = 50
Figure 1: Adversarial examples found by DNN fuzzing
DNN by applying small perturbation to an original sample, leading
to misclassification [5]. Recent studies have shown that DNNs can
easily be fooled by imperceptible perturbations [1].
Lately, a fuzzing technique has been proposed as a promising
analysis method for verification of DNNs, involving adversarial
examples [6]. Fuzzing is originally used as an automated software
testing tool, automatically generating inputs for finding bugs. One
of the trends in software fuzzing is coverage-guided fuzzing, which
is based on code coverage information to check various states of a
target program. DNNs can be seen as software and similarly applied
to fuzz testing, but such an approach may require different coverage
metrics due to the neural-based nature of deep learning models.
Research in fuzzing DNNs is in its early-stage and therefore
finding appropriate coverage metrics for DNNs is still in progress.
While developing a new coverage metric for effective coverage-
guided fuzzing on DNNs, we were strongly motivated to analyze
each layer’s activation and find out which layer’s coverage is effec-
tive in verifying target models. Odena et al. [2] claimed that using
activation values from "only the logits, or the layer before the logits"
can be effective as a coverage metric. We could think of two re-
search questions from here: First, can the performance of logit layer
coverage be comparable to the entire coverage? Second, what are
other possible single layers that could be used as a better coverage
metric? In previous studies, even in TensorFuzz [2], we could not
find any empirical evaluations for the two research questions.
In this study, we empirically evaluate two existing DNN cover-
age metrics to find out which layers are effective in guiding DNN
fuzzing. We first design a fuzzing system for deep neural networks
(§2), considering the difference from a traditional software (§2.1)
and components of the system (§2.2). We briefly describe the exist-
ing coverage metrics for DNNs, neuron coverage [3] and activation
vector [2] (§2.3). We evaluate each of coverage metrics by layers,
and measure the coverage information and the number of crashes
(§3). In §4, we discuss the implication of our study and future work.
2 FUZZING DEEP NEURAL NETWORK
2.1 Applying Fuzzing to Deep Neural Network
Both traditional fuzzing on software and DNN fuzzing try to find
crashes. Their difference is in the definition of a crash; a crash in
DNN generally means a misclassification while the one in software
means an unexpected termination of a process. The crash of a DNN
Original Label : 7Predicted Label : 3Original Label : 0Predicted Label : 5Original Label : 7Predicted Label : 3Original Label : 0Predicted Label : 5PosterCCS ’19, November 11–15, 2019, London, United Kingdom2681Figure 2: An overview of fuzzing system
may be caused by insufficient training, but we focus on those caused
by adversarial examples in an attacker’s view. Given an original
input x and a mutated input x′ that is slightly perturbed from x,
indicating the predicted class of some input as C , if Cx (cid:44) Cx′
satisfies, we call x′ an adversarial example.
While elaborate adversarial attacks utilize gradients to find the
degree of perturbation [1], DNN fuzzing applies a mutation-based
approach that applies random noise to the original sample. There-
fore, how to select an original sample is an important factor. In
coverage-guided fuzzing, a target model’s coverage of some input
sample is measured, then if the coverage reaches a new state of
the target, the sample is added to an input corpus. Repetitively, the
fuzzing routine picks one of the samples in the corpus and applies
mutation to it.
2.2 Components of Fuzzing System
We introduce the structure of coverage-guided and mutation-based
fuzzing for DNNs with components that we implemented. Figure 2
shows an overview of our fuzzing system.
Corpus stores seed corpus elements to be mutated and manages
them. The element contains various information of an input sample
such as image data, coverage, and prediction result.
Sample Function selects one to be mutated from seed elements
in the corpus. Although we can increase the probability that recent
seed element would be selected, we assign same probabilities to all
seeds.
Mutation Function generates random noise and applies it to the
selected seed. Mutation constraint based on Lp-norm is inevitable
since it is very important to make perturbation that is impercept-
able to human. We utilize L2-norm, the euclidean distance, as a
constraint.
Model is a target of DNN fuzzing, similar to software in traditional
fuzzing. We conduct an analysis in a white-box manner. Once a mu-
tated input sample is fed, we can retrieve the neuron information.
Coverage Function derives a coverage from neuron information
based on a proper coverage metric.
Objective Function decides whether an input sample achieved an
objective. In our work, the objective of DNN fuzzing is satisfied
when the sample is an adversarial example.
Updater identifies the novelty of a derived coverage from the cov-
erage function compared to the previous coverage. If the coverage
is new, we add the sample to the corpus.
2.3 Coverage Metrics for Deep Neural Network
Traditional fuzzing can utilize software features such as branch,
function, and basic block for coverage criterion. However, DNN
fuzzing utilizes the activation value of a neuron since DNNs lack
software features. Coverage metrics of DNN fuzzing differ in the
utilization method of neuron activation values.
Table 1: Neuron or filter output size of neural networks
Layer
Size
Fully Connected Network
Dense 2
Dense 1
Dense 0
256
512
Convolutional Neural Network
128
Layer Conv 0
Size
26*26*200
Pool 0
13*13*200
Conv 1
11*11*64
Pool 1 Dense0
5*5*64
256
Logit
10
Logit
10
2.3.1 Neuron Coverage. Pei et al. proposed DeepXplore, a DNN
analysis framework based on neuron coverage [3]. They considered
a neuron with an activation value exceeding a certain threshold
as an activated neuron. They measured the ratio of the number of
activated neurons to the number of total neurons for the coverage.
Neuron coverage has a few number of findable states and tried
to observe all activations. DeepXplore was not a fuzzing system
but a framework measuring coverage of an original and modified
input. Therefore, we apply neuron coverage to our fuzzing system
in Section 3.
2.3.2 Activation Vector. Odena et al. proposed TensorFuzz, a DNN
fuzzing system based on activation vector [2]. Contrary to neuron
coverage, they used the activation value itself and created a vector
with the elements. They utilized approximate nearest neighbor
algorithm and considered a new activation vector with enough
distance from others as a new state. We are interested in one of
their mentions: coverage of the logits or the layer before the logits
may give a good result.
3 EVALUATION
Referring to the mention of Odena et al. [2], we performed fuzzing
on various layers for each coverage metrics. We also provide a com-
parative analysis of layers to find effective layers for each metric.
Our experiments were performed on Intel (R) Xeon(R) Gold 6134
CPU @ 3.20 GHz, 190 GB RAM, Nvidia GTX Titan V and Ubuntu
18.04.1 LTS. We also used Python 3.5 and tensorflow 1.8.0. We
constructed two target Deep Neural Network models for fuzzing,
Fully Connected Network (FC) and Convolutional Neural Network
(CNN). The construction of networks are described in Table 1. We
trained them with 50,000 training images of MNIST handwritten
dataset. The accuracy of 10,000 test samples was 97.34% for FC and
99.06% for CNN. We used 1,000 MNIST test samples as initial seeds
and performed fuzzing with 100,000 iterations. We have two L2
constraints, 5 and 50. We performed the test for five times for each
condition, then derived their average performance.
3.1 Layer Analysis on Neuron Coverage
As DeepXplore [3] can be applied to CNN, we performed fuzzing
on the CNN for the neuron coverage. Figure 3 describes the trend of
the number of detected crashes during fuzzing iterations. Overall,
fuzzing with the L2 constraint 50 which permits stronger perturba-
tion found more crash than the constraint 5. We found that mea-
suring dense0, the layer before the logit layer, found more crashes
than other layers, even more than measuring whole layers. For
neuron coverage, the logit layer performed much worse than mea-
suring whole layers and similar to intermediate layers. As shown
InputCorpusSampleFunctionMutationFunctionDNNModelCoverage FunctionObjectiveFunctionUpdaterAdversarialExampleNewCoverageDifferentPredictionPosterCCS ’19, November 11–15, 2019, London, United Kingdom2682Figure 3: The number of crashes on convolutional neural
network (fuzzing with neuron coverage)
Figure 6: The number of corpus elements on fully connected
network (fuzzing with neuron coverage)
Comparing two coverage metrics, neuron coverage found more
crashes than activation vector. It does not indicate the neuron cov-
erage is better, because we set the distance threshold of activation
vector to 500. When the threshold is low, there was no difference
among the conditions, the number of detected crashes for all con-
ditions was near to the number of iterations. Even with the high
threshold, activation vector could find much more states of DNN
than the neuron coverage.
4 DISCUSSIONS AND FUTURE WORK
It is important to find an effective layer for measuring the cover-
age since DNN layers have different features. We could derive the
answers for our two research questions from the experiments, and
the results were interesting; an intermediate layer guides fuzzing
better than not only other single layers, but also than measuring
whole layers for both coverage metrics. However, we cannot yet
define the ground-truth about an effective layer that can be applied
to every network. Therefore, we need to continue this analysis on
other coverage metrics and more sophisticated networks. We are
designing a new coverage metric which might detect more states
than the existing metrics, so the new metric will be analyzed in the
same manner as this work.
ACKNOWLEDGMENTS
This work was supported by the National Research Foundation of
Korea (NRF) grant funded by the Korea government (MSIT) (No.
NRF-2019R1A2C1088802).
REFERENCES
[1] Nicholas Carlini and David Wagner. 2017. Towards evaluating the robustness
of neural networks. In 2017 IEEE Symposium on Security and Privacy (SP). IEEE,
39–57.
[2] Augustus Odena and Ian Goodfellow. 2018. Tensorfuzz: Debugging neural net-
works with coverage-guided fuzzing. arXiv preprint arXiv:1807.10875 (2018).
[3] Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. 2017. Deepxplore: Au-
tomated whitebox testing of deep learning systems. In Proceedings of the 26th
Symposium on Operating Systems Principles. ACM, 1–18.
[4] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean
Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al.
2015. Imagenet large scale visual recognition challenge. International journal of
computer vision 115, 3 (2015), 211–252.
[5] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
Ian Goodfellow, and Rob Fergus. 2013. Intriguing properties of neural networks.
arXiv preprint arXiv:1312.6199 (2013).
[6] Xiaofei Xie, Lei Ma, Felix Juefei-Xu, Minhui Xue, Hongxu Chen, Yang Liu, Jianjun
Zhao, Bo Li, Jianxiong Yin, and Simon See. 2019. DeepHunter: a coverage-guided
fuzz testing framework for deep neural networks. In Proceedings of the 28th ACM
SIGSOFT International Symposium on Software Testing and Analysis. ACM, 146–157.
Figure 4: The number of corpus elements on convolutional
neural network (fuzzing with neuron coverage)
Figure 5: The number of crashes on fully connected network
(fuzzing with neuron coverage)
in Figure 4, measuring dense0 also found more states of the net-
work by adding more corpus elements than other single layers, but
slightly less than whole layers. In summary, we can consider the
layer dense0 the most effective layer for neuron coverage.
3.2 Layer Analysis on Activation Vector
TensorFuzz [2] mainly focused on verifying FC, so we also per-
formed fuzzing on FC for activation vector. The fuzzing results on
activation vector was more surprising than the neuron coverage.
As shown in Figure 5, when the constraint is 50, all coverage condi-
tions, except the logits with the worst performance, detected the
similar number of crashes. When the constraint is 5, layer dense0
and dense1 found more crashes than logits and whole layers. The
results about the number of corpus elements was similar to the
crash. In Figure 6, all conditions found the similar number of states
with constraint 5, adding the similar number of corpus elements,
but dense1 outperformed other layers while iteration goes further.
PosterCCS ’19, November 11–15, 2019, London, United Kingdom2683