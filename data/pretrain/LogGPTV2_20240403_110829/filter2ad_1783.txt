# Poster: Effective Layers in Coverage Metrics for Deep Neural Networks

**Authors:**  
Leo Hyun Park, Sangjin Oh, Jaeuk Kim, Soochang Chung, and Taekyoung Kwon  
Yonsei University, Seoul, Korea  
{dofi, dhtkdwls537, freak0wk, warpstar, taekyoung}@yonsei.ac.kr

## Abstract
Deep neural networks (DNNs) have become a popular and effective machine learning algorithm. However, their high complexity leads to a lack of model interpretability and difficulty in verification. Fuzzing, an automated software testing technique, has recently been applied to DNNs to address these issues by following the trend of coverage-based fuzzing. This raises the question of which layers in DNNs are most effective for measuring coverage. In this poster, we empirically evaluate the performance of existing coverage metrics. Through comparative analysis, we identify the most effective layer for each coverage metric and discuss future directions for DNN fuzzing.

## CCS Concepts
- **Security and Privacy → Software and Application Security**

## Keywords
- Deep neural network
- Coverage
- Fuzzing
- Adversarial examples

## ACM Reference Format
Leo Hyun Park, Sangjin Oh, Jaeuk Kim, Soochang Chung, and Taekyoung Kwon. 2019. Poster: Effective Layers in Coverage Metrics for Deep Neural Networks. In 2019 ACM SIGSAC Conference on Computer & Communications Security (CCS '19), November 11–15, 2019, London, United Kingdom. ACM, New York, NY, USA, 3 pages. https://doi.org/10.1145/3319535.3363286

## 1. Introduction
Deep neural networks (DNNs) have become the mainstream of machine learning algorithms, demonstrating outstanding performance in various fields such as natural language processing (NLP), computer vision, and voice recognition. The complex structure of DNNs is a key factor in their success, enabling them to be highly expressive. However, this complexity also reduces interpretability and makes it difficult to understand the basis of prediction results.

A common approach to verifying DNNs is to measure their prediction performance using a test dataset. This method, however, has significant limitations. First, it heavily depends on the quality of the dataset, which may not represent all real-world samples. Second, it does not account for realistic threats such as adversarial examples, which can target a DNN's blind spots by applying small perturbations to original samples, leading to misclassification [5]. Recent studies have shown that DNNs can be easily fooled by imperceptible perturbations [1].

Recently, fuzzing has been proposed as a promising method for DNN verification, including the detection of adversarial examples [6]. Fuzzing is originally used as an automated software testing tool to generate inputs for finding bugs. One trend in software fuzzing is coverage-guided fuzzing, which uses code coverage information to check various states of a target program. DNNs can be treated as software and similarly applied to fuzz testing, but this approach may require different coverage metrics due to the unique nature of deep learning models.

Research in DNN fuzzing is still in its early stages, and finding appropriate coverage metrics for DNNs is an ongoing effort. While developing new coverage metrics for effective coverage-guided fuzzing on DNNs, we were motivated to analyze the activation of each layer and determine which layer's coverage is most effective for verifying target models. Odena et al. [2] suggested that using activation values from "only the logits, or the layer before the logits" can be effective as a coverage metric. This led us to two research questions: 
1. Can the performance of logit layer coverage be comparable to the entire coverage?
2. What other single layers could be used as better coverage metrics?

In previous studies, including TensorFuzz [2], we did not find any empirical evaluations addressing these questions. In this study, we empirically evaluate two existing DNN coverage metrics to identify the most effective layers for guiding DNN fuzzing.

## 2. Fuzzing Deep Neural Networks

### 2.1 Applying Fuzzing to Deep Neural Networks
Both traditional fuzzing on software and DNN fuzzing aim to find crashes. The difference lies in the definition of a crash: in DNNs, a crash generally means a misclassification, whereas in software, it means an unexpected termination of a process. A DNN crash can be caused by insufficient training, but we focus on those caused by adversarial examples from an attacker's perspective. Given an original input \( x \) and a mutated input \( x' \) that is slightly perturbed from \( x \), if the predicted class \( C_x \neq C_{x'} \), then \( x' \) is an adversarial example.

While sophisticated adversarial attacks use gradients to find the degree of perturbation [1], DNN fuzzing applies a mutation-based approach that adds random noise to the original sample. Therefore, selecting an original sample is crucial. In coverage-guided fuzzing, the coverage of a target model for some input sample is measured. If the coverage reaches a new state, the sample is added to the input corpus. The fuzzing routine repeatedly picks one of the samples in the corpus and applies mutations.

### 2.2 Components of the Fuzzing System
We introduce the structure of coverage-guided and mutation-based fuzzing for DNNs, along with the components we implemented. Figure 2 provides an overview of our fuzzing system.

- **Corpus:** Stores seed corpus elements to be mutated and manages them. Each element contains information about the input sample, such as image data, coverage, and prediction result.
- **Sample Function:** Selects one element from the seed corpus to be mutated. We assign equal probabilities to all seeds.
- **Mutation Function:** Generates random noise and applies it to the selected seed. An Lp-norm constraint, such as L2-norm, is used to ensure the perturbation is imperceptible to humans.
- **Model:** The target DNN for fuzzing, similar to software in traditional fuzzing. We conduct a white-box analysis, retrieving neuron information once a mutated input sample is fed.
- **Coverage Function:** Derives coverage from neuron information based on a proper coverage metric.
- **Objective Function:** Decides whether an input sample achieves the objective. In our work, the objective is satisfied when the sample is an adversarial example.
- **Updater:** Identifies the novelty of the derived coverage compared to the previous coverage. If the coverage is new, the sample is added to the corpus.

### 2.3 Coverage Metrics for Deep Neural Networks
Traditional fuzzing can use software features like branches, functions, and basic blocks for coverage criteria. However, DNN fuzzing uses neuron activation values since DNNs lack these features. Different DNN coverage metrics vary in how they utilize neuron activation values.

#### 2.3.1 Neuron Coverage
Pei et al. [3] proposed DeepXplore, a DNN analysis framework based on neuron coverage. They considered a neuron with an activation value exceeding a certain threshold as activated. They measured the ratio of activated neurons to the total number of neurons for coverage. Neuron coverage has a limited number of findable states and aims to observe all activations. Although DeepXplore was not a fuzzing system, it measured the coverage of original and modified inputs. We apply neuron coverage to our fuzzing system in Section 3.

#### 2.3.2 Activation Vector
Odena et al. [2] proposed TensorFuzz, a DNN fuzzing system based on activation vectors. Unlike neuron coverage, they used the activation values themselves to create a vector. They utilized an approximate nearest neighbor algorithm and considered a new activation vector with sufficient distance from others as a new state. They mentioned that coverage of the logits or the layer before the logits might yield good results.

## 3. Evaluation
Referring to Odena et al. [2], we performed fuzzing on various layers for each coverage metric. We also provided a comparative analysis of layers to find the most effective ones for each metric. Our experiments were conducted on an Intel (R) Xeon(R) Gold 6134 CPU @ 3.20 GHz, 190 GB RAM, Nvidia GTX Titan V, and Ubuntu 18.04.1 LTS. We used Python 3.5 and TensorFlow 1.8.0. We constructed two target DNN models for fuzzing: a Fully Connected Network (FC) and a Convolutional Neural Network (CNN). The network structures are described in Table 1. We trained them with 50,000 training images from the MNIST handwritten dataset. The accuracy of 10,000 test samples was 97.34% for FC and 99.06% for CNN. We used 1,000 MNIST test samples as initial seeds and performed fuzzing with 100,000 iterations. We used two L2 constraints, 5 and 50, and repeated the test five times for each condition, deriving their average performance.

### 3.1 Layer Analysis on Neuron Coverage
As DeepXplore [3] can be applied to CNNs, we performed fuzzing on the CNN for neuron coverage. Figure 3 shows the trend of the number of detected crashes during fuzzing iterations. Overall, fuzzing with the L2 constraint of 50, which allows stronger perturbations, found more crashes than the constraint of 5. We found that measuring dense0, the layer before the logit layer, found more crashes than other layers, even more than measuring whole layers. For neuron coverage, the logit layer performed much worse than measuring whole layers and was similar to intermediate layers. As shown in Figure 4, measuring dense0 also found more states of the network by adding more corpus elements than other single layers, but slightly less than whole layers. In summary, dense0 appears to be the most effective layer for neuron coverage.

### 3.2 Layer Analysis on Activation Vector
TensorFuzz [2] focused on verifying FC, so we also performed fuzzing on FC for activation vector. The fuzzing results for the activation vector were more surprising than for neuron coverage. As shown in Figure 5, when the constraint is 50, all coverage conditions, except the logits, detected a similar number of crashes. When the constraint is 5, dense0 and dense1 found more crashes than logits and whole layers. The results for the number of corpus elements were similar to the crash results. In Figure 6, all conditions found a similar number of states with the constraint of 5, adding a similar number of corpus elements, but dense1 outperformed other layers as the iteration progressed.

## 4. Discussions and Future Work
Identifying an effective layer for measuring coverage is important because DNN layers have different features. Our experiments provided answers to our two research questions: an intermediate layer guides fuzzing better than both other single layers and whole layers for both coverage metrics. However, we cannot yet define a ground truth for an effective layer that can be applied to every network. Therefore, we need to continue this analysis with other coverage metrics and more sophisticated networks. We are designing a new coverage metric that might detect more states than existing metrics, and we will analyze the new metric in the same manner as this work.

## Acknowledgments
This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korean government (MSIT) (No. NRF-2019R1A2C1088802).

## References
[1] Nicholas Carlini and David Wagner. 2017. Towards evaluating the robustness of neural networks. In 2017 IEEE Symposium on Security and Privacy (SP). IEEE, 39–57.

[2] Augustus Odena and Ian Goodfellow. 2018. TensorFuzz: Debugging neural networks with coverage-guided fuzzing. arXiv preprint arXiv:1807.10875 (2018).

[3] Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. 2017. DeepXplore: Automated whitebox testing of deep learning systems. In Proceedings of the 26th Symposium on Operating Systems Principles. ACM, 1–18.

[4] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. 2015. ImageNet large scale visual recognition challenge. International journal of computer vision 115, 3 (2015), 211–252.

[5] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. 2013. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199 (2013).

[6] Xiaofei Xie, Lei Ma, Felix Juefei-Xu, Minhui Xue, Hongxu Chen, Yang Liu, Jianjun Zhao, Bo Li, Jianxiong Yin, and Simon See. 2019. DeepHunter: A coverage-guided fuzz testing framework for deep neural networks. In Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis. ACM, 146–157.

![Figure 1: Adversarial examples found by DNN fuzzing](figure1.png)
(a) L2 constraint = 5  
(b) L2 constraint = 50

![Figure 2: An overview of the fuzzing system](figure2.png)

![Figure 3: The number of crashes on convolutional neural network (fuzzing with neuron coverage)](figure3.png)

![Figure 4: The number of corpus elements on convolutional neural network (fuzzing with neuron coverage)](figure4.png)

![Figure 5: The number of crashes on fully connected network (fuzzing with neuron coverage)](figure5.png)

![Figure 6: The number of corpus elements on fully connected network (fuzzing with neuron coverage)](figure6.png)