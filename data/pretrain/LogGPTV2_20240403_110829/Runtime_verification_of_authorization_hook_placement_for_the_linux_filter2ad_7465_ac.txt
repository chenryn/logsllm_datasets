Instrumented Calls
Logging Module
/
p
r
o
c
Authorisation Filter
LSM Hooks
Context Filter
Breakpoints
Linux
Kernel
Control-Flow Filter
Instrumented Calls
Figure 3: Implementation Architecture
4.1 Collecting Runtime Information
4.1.1 Log Contents
Table 2 shows the information collected during runtime analy-
sis. Controlled operations are identiﬁed by the tuple (instruction
pointer, object type, member, access). A controlled operation ID is
assigned to each unique combination. Authorizations are uniquely
identiﬁed by (LSM hook, policy operation). Like controlled opera-
tions, a unique authorization ID is assigned to each. Function entry
and exit are recorded as well. The function entry address uniquely
identiﬁes the function.
For each controlled operation or authorization performed, the log
must include the identity of the object (e.g., inode) involved. Object
identities (OIDs) are deﬁned per object type, for example, inodes
are identiﬁed by (device ID, inode number) while tasks are identi-
ﬁed by process-ID. OIDs are only required to be unique within a
context.
We use the concept of a context to mean the processing of a ker-
nel event (e.g., a system call). Authorizations are obviously only
valid in the context in which they are executed, therefore, the log
entries must also include the context of controlled operations and
authorizations.
4.1.2 Collection Overview
Figure 3 presents an overview of the tool. Creation of the log
involves three stages: the required information must be generated,
it must be collected, and it must be written to the log.
Information is generated in three different ways. First, autho-
rization information is generated by the LSM hooks. Second, con-
trolled operation details are generated by compiling the kernel with
a modiﬁed version of GCC that identiﬁes controlled operations,
and instruments the kernel with calls to a handler function before
all such operations. Control-ﬂow information is also generated by
instrumenting the kernel at compile-time. Third, context informa-
tion is generated by placing breakpoints in the kernel. These three
methods are discussed in more detail in the following sections.
Four kernel modules are loaded to receive the information shown
in Figure 3. These modules perform coarse-grained ﬁltering, and
arrange the information into the correct format, before passing the
record to the logging module. The logging module assigns a con-
text ID to the incoming records and writes the information into a
buffer.
4.1.3 Authorization Information
Hooks to log authorization information are already provided by
the LSM patch, so little additional implementation is required. The
authorization ﬁlter is simply an LSM module that adds a log entry
for each authorization. These log entries identify the authorization
that was performed (e.g., RMDIR PARENT, RMDIR TARGET)
and the object authorized.
4.1.4 Controlled Operations
To log controlled operations, we ﬁrst have to locate controlled
operations in the kernel, and then provide a mechanism for detect-
ing the execution of these operations.
Identifying controlled operations in the kernel requires source
analysis. Rather than a direct source-code analysis (which is difﬁ-
cult), we chose to identify controlled operations by analyzing GCC’s
intermediate tree representation. As Linux depends on GCC exten-
sions, a source-code analysis would require using the GCC parser,
therefore making use of the tree it already builds seems logical. To
identify controlled operations, we traverse the tree looking for ex-
pressions in which members of mediated data types are accessed 2.
When a controlled operation is detected we insert a call to a func-
tion __controlled_op that includes the object, type, member,
and access, before the statement in which the expression exists. If
the expression is the condition statement of a loop, then a call is
inserted before the loop and at the end of each iteration. This call
contains all the information required to identify the controlled op-
eration and allow the handler to extract the identity of the object.
A couple of accesses cause problems for this approach. First, it
is possible to modify a structure member by taking the address of
a member, storing it to a pointer, and changing the member via the
pointer. Since the initial access is a read into the pointer variable,
it is possible that we may miss the subsequent write. Rather than
performing more extensive source analysis to identify these cases,
we simply detect when aliasing occurs. Second, it is also possible
that we miss accesses to controlled data structures when they are
cast to a non-controlled type. This is also detected. Our initial
analysis shows that these cases occur in a small number of ways
(although for the ﬁrst, a large number of times), so they can be
handled as special cases.
4.1.5 Control Flow
Control ﬂow information is generated by compiling the kernel
with the -finstrument-functions switch provided by GCC-
3.0. This option causes the compiler to insert calls to handler func-
tions at the entry and exit of every function. These handler func-
tions then pass the information to the appropriate module.
4.1.6 Context Information
As there may be multiple execution contexts in the kernel at
anytime, all log entries must contain a context ID, so the analysis
can tell which entries relate to one another. Unfortunately, no key
is available that will uniquely identify a single execution context,
therefore, we must choose a non-unique key and deﬁne an approach
to distinguish contexts with the same key.
2These are COMPONENT REF nodes where the resultant type of
the ﬁrst operand is a mediated type.
230We chose the base of the current kernel stack as the non-unique
key as we need a key that is at least unique among concurrently ac-
tive executions, and it would seem impossible for this property to
be violated for the stack. While it is unique among concurrently ac-
tive executions, the kernel stack is not unique per-context for three
reasons: all system-calls from the same process use the same ker-
nel stack, once a process dies its kernel stack may be allocated to
a new process, and interrupts execute with the kernel stack of the
process they interrupt. The critical property here is that although
the context key is not unique, contexts with the same key are never
interleaved. Therefore, by recording the beginning and end of a
context (and the associated key), we can unambiguously assign log
entries to contexts.
Fortunately, there are only a few points where a context can
begin (all located in entry.S), and a roughly equal number of
places that contexts can end. The exit system call is an excep-
tional case since it never returns, therefore, the schedule() call
in do_exit() is also identiﬁed as a context exit point. To gener-
ate this information at run time, the context ﬁlter inserts breakpoint
instructions into the (memory-image of the) kernel at all entry and
exit points. When a breakpoint is executed, the context ﬁlter cre-
ates a log entry containing the context key, and whether this is the
beginning or end of a context.
4.1.7 Performance
We did a simple performance check to determine the perfor-
mance degradation in the instrumented kernel. On an unmodiﬁed
Linux kernel, LMBench conﬁgured for a “fast benchmark” took 3
minutes and 4 seconds to run. The instrumented kernel took 3 min-
utes and 24 seconds to run the same benchmark for a degradation of
slightly over 10%. We believe that this overhead is quite acceptable
for such analyses. In this test, as in the results collection described
above, we sample 1 out of 20 system calls. The reason for this is to
keep the log growth rate lower than the disk throughput rate. Since
these benchmarks perform the same system calls many times, we
did not notice that we “lost” any security-relevant information. If
necessary, a policy for determining when to drop a log entry can be
devised.
4.2 Log Analysis
We have also built a tool that enables log analysis for identify-
ing sensitivities in authorization requirements as described in Sec-
tion 3.1. The tool enables speciﬁcation of rules for extracting the
desired log entries, called log ﬁltering rules, and computes the au-
thorization sensitivities given the extracted entries. We can gener-
ate two types of displays for sensitivities: (1) authorization graphs
that show the sensitives between each authorization and controlled
operation and (2) sensitivity class lists that show the aggregation of
controlled operations by authorizations and sensitivity attribute.
While the analysis tool enables ﬂexible analysis, we have found
that an optimistic approach is the easiest to manage. That is, we
write rules to identify sensitivities at the highest level attribute, sys-
tem call. If all the controlled operations in the system call execution
have the same authorizations (i.e., are system call sensitive), then
we only have to verify that the authorizations are correct. If not, we
examine whether system call inputs are responsible for the sensitiv-
ity. Analysis for system call input sensitivity is somewhat ad hoc,
since there are a large number of possible inputs, but very few have
an effect on authorizations. Authorization graphs are useful for this
task because they give an overall view of the authorization status.
After tuning the log ﬁltering rules to handle system call input sen-
sitivities, we then generate partitions (i.e., sensitivity class lists) for
controlled operations to do the remaining sensitivity analysis.
# Path sensitive rule for operation at
0xc014f046
1 = (+,id type,CONTEXT) (+,di cfm eax,READ)
2 (D,1) = (+,id type,CNTL OP)
(+,di dfm ip,0xc014f046)
3 (D,1) = (+,id type,SEC CHK)
# Member sensitive rule for inode member
i flock read access
1 = (+,id type,CONTEXT) (+,di cfm eax,READ)
2 (D,1) = (+,id type,CNTL OP)
(+,di dfm class,OT INODE)
(+,di dfm member,i flock)
(+,di dfm access,OP READ)
3 (D,1) = (+,id type,SEC CHK)
# Input sensitive rule for open for read
access, but not path walk
1 = (+,id type,CONTEXT) (+,di cfm eax,OPEN)
(+,co ecx,RDONLY)
2 (D,1) = (+,id type,FUNC)
(+,di ffm ip,path walk)
3 (D,1)(N,2) = (+,ALL,0,0)
Figure 4: Example authorization sensitivity ﬁltering rules
4.2.1 Log Filtering Rules
The log ﬁltering tool takes an execution log and set of ﬁltering
rules as input, and outputs the log entries that match the rules. The
rule language is currently rather low-level, as we have been con-
cerned more with demonstrating feasibility rather than creating a
nice high-level rule language. However, we demonstrate the rule
language to give a sense of the types of analyses that are possible.
A rule base is deﬁned by a set of rules that deﬁne matching re-
quirements. A rule consists of: (1) an index; (2) a dependency
speciﬁcation; (3) a set of statements. The index identiﬁes the rule
within the rule base. The dependency states relationships to other
rules by index. We can state that a rule can only match entries that
are also matched by another rule, D; i, where i is the index of the
other rule. Also, we can state that a dependency that a rule does
not include entries matched by another rule i, as ; i. Lastly, the
statements describe the matching conditions for entries. These are
speciﬁed by identifying the entry type (id type), and then matching
type-speciﬁc attributes. Entry types include: events (CONTEXT),
authorizations (SEC_CHK), functions (FUNC), and controlled op-
erations (CNTL_OP).
Figure 4 shows some example rules. The path sensitive rule ﬁnds
all authorizations in the context of a read system call when a con-
trolled operation at the speciﬁed address is run. The ﬁrst line col-
lects all context entries for a read system call (i.e., the start of the
system call). The second line collects all entries of controlled oper-
ations at the speciﬁed location. The (D,1) means that this state-
ment is dependent on statement 1, so only entries within the read
system call context will be collected. The third line collects all au-
thorizations within the read system call context. In this case, each
execution of this controlled operation should have the same autho-
rizations or there is a violation of the path insensitivity invariant
that prohibits a controlled operation from having multiple sets of
legal authorizations.
The function sensitive rule collects all authorizations and con-
trolled operations of “read inode member i ﬂock” within a read
231DFN d 0 FILE f dentry -1
DFN d 0 FILE f dentry 1
DFN d 0 FILE f vfsmnt -1
DFN d 0 FILE f op -1
...
SFN(ALWAYS) d 0 FILE READ
-----------------------
DFN d 1 SUPERBLOCK s blocksize -1
DFN d 1 SUPERBLOCK s type -1
...
DFN d 1 TASK state -1
DFN d 1 TASK state 0
DFN d 1 TASK flags -1
...
SFN() NONE
-----------------------
DFN o 0 INODE i blocks -1
DFN o 0 INODE i blocks 1
DFN o 0 INODE i version -1
...
SFN(ALWAYS) o 0 FILE READ
-----------------------
DFN o 1 INODE i dnotify mask -1
SFN() NONE
-----------------------
Figure 6: Sensitivity class list for read system call with the
following ﬁelds: (1) entry type (DFN or SFN); (2) sensitivity (d
for datatype and  for object); (3) class number; (4) datatype;
(5) member; (6) access identiﬁer.
ation (data type, member offset, operation type) information. The
authorization nodes include the authorization, command, and func-
tion containing the authorization. Always edges are indicated by a
solid line and sometimes edges are indicated by a dashed line. If
no edge exists between a controlled operation and an authorization,
then that authorization is never performed for that operation.
By visually analyzing this graph we can identify whether the in-
variants described in Section 3.2 hold for the current graph or not.
In this case, the sometimes relation between fput and its autho-
rizations may indicate a problem. Also, the fact that different sets
of authorizations are made for the same ﬁeld (member offset 480
which happens to be f_owner) may be indicative of a problem.
Manual investigation is then required to identify whether any in-
consistency is due to an error or a legitimate sensitivity.
4.2.3 Sensitivity Class Lists
The sensitivity class lists show the partition of the controlled op-
erations by sensitivity level in which authorizations are consistent
and the authorization requirements at those levels. This partition
is computed using the algorithm described in Section 3.2. The
sensitivity class lists provide a different view than the authoriza-
tion graphs of the same authorization results. Whereas an autho-
rization graph shows the relationship between each individual con-
trolled operation and authorization, the sensitivity class lists show
the collection of controlled operations with the same authorization
requirements. The sensitivity class lists makes more obvious the