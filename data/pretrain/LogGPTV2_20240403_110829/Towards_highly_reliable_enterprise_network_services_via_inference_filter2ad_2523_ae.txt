By Ferret’s computations, 87% of the problems were caused by
only 16 components (out of the 358 components that can fail inde-
pendently). We were able to corroborate the 3 most notable prob-
lems marked in the ﬁgure with external evidence. The Server1 in-
cident was caused by a front-end web server with intermittent but
recurring performance issues. In the Server2 incident, another web
server was having problems accessing its SQL backend. The third
incident was due to recurring congestion on a link between R1 and
the rest of the enterprise network. In Figure 16, when Ferret is un-
able to determine a single root cause due to lack of information,
it will provide a list of most likely suspects. For example in the
Server1 incident, there are 4 dots which represent the web server,
the last links to and from the web server, and the router to which
the web server is directly connected.
Figure 17: 5-minute averages of link utilization reported by
SNMP. Oscillations around 14:00 correspond to observed per-
formance issue.
Sherlock can also discover problems that might be overlooked
by using traditional threshold-based techniques. For instance, in the
Server2 incident, both the web server and SQL backend were func-
tioning normally and traditional threshold-based techniques would
not raise any alerts. Only requests requiring interaction between the
web server and the SQL backend experience poor performance, but
this is caught is by Sherlock.
In a fourth incident, some clients were experiencing intermittent
poor performance when accessing a web server in the data center
while other clients did not report any problem. Ferret identiﬁed a
suspect link on the path to the data center that was shared by only
those clients that experienced poor performance. Figure 17 shows
the MRTG [11] data describing the bandwidth utilization of the
congested link. Ferret’s conclusion on when the link was troubled
matches the spikes in link utilization between 12:15 and 17:30.
However, an SNMP-based solution would have trouble detecting
this performance incident. First, the spikes in the link utilization
are always less than 40% of the link capacity. This is common with
SNMP counters, since those values are 5-minute averages of the
actual utilization and may not reﬂect instantaneous high link uti-
lization. Second, the 60% utilization at 11:00 and 18:00 did not
lead to any user-perceived problems, so there is no threshold set-
ting that catches the problem while avoiding false alarms. Finally,
due to scalability issues, administrators are unable to collect rele-
vant SNMP information from all the links that might run into con-
gestion.
6.2.2 Comparing Sherlock with Prior Approaches
Sherlock differs from prior fault localization approaches in its
use of multi-level inference graph instead of two-level bipartite
graph, and its use of probabilistic dependencies. Comparing Sher-
lock with prior approaches allows us to evaluate the impact of these
design decisions.
To perform the comparison, we need a large set of observations
for which the actual root causes of the problems are known. Be-
cause it is infeasible to create such a set of observations using a
testbed, we conduct experiments with simulations. We ﬁrst cre-
ated a topology and its corresponding inference graph that exactly
matches that of the production network. Then we randomly set the
state of each root cause to be troubled or down and perform a prob-
abilistic walk through the inference graph to determine the state of
all the observation nodes. Repeating this process 1,000 times pro-
duced 1,000 sets of observations for which we know the actual root
causes. We then compare different techniques on their ability to
identify the correct root cause given the 1,000 observation sets.
Figure 18 shows that by using multi-level inference graphs, Fer-
ret is able to correctly identify up to 32% more faults than Shrink,
which uses two-level bipartite graphs. Figure 9 and Figure 14 show
that multi-level dependencies do exist in real systems, and repre-
senting this type of dependency using bipartite graphs does lose im-
portant information. SCORE [7] uses a deterministic dependency
Ferret on the
Inference Graph
Shrink (Two-level
Noisy Or)
SCORE (Minimal
Set Cover)
0
58.61
52.59
20
40
% of Dataset successfully Localized
60
80
90.66
100
Figure 18: Multi-level probabilistic model allows Ferret to cor-
rectly identify 30% more faults than approaches based on
two-level probabilistic models (Shrink) or deterministic mod-
els (SCORE).
 10000
 1000
 100
 10
 1
)
s
(
e
m
T
i
 0.1
 100
Time To Localize < .004*Nodes
25 min
 1000
 100000
Number of Nodes in the Inference Graph
 10000
 1e+06
Figure 19: The time taken by Ferret to localize a fault grows
linearly with the number of nodes in the Inference Graph.
model in which a dependency either exists or not. For example,
the caching of names makes DNS a weak dependency. If such
weak dependencies are included, the SCORE model causes many
false-positives, yet excluding these dependencies results in false-
negatives.
6.2.3 Time to Localize Faults
We now study how long it takes Ferret to localize faults in large
enterprise networks. In the following simulations, we use a topol-
ogy which is the same as the one in our ﬁeld deployment. We then
add more clients and servers to the topology and use the measure-
ment results in Figure 15 to determine the number of unique clients
that would access a server in a given time window. The experi-
ments were run on an AMD Athlon 1.8GHz machine with 1.5GB
of RAM. Figure 19 shows that the time it takes to localize injected
faults grows almost linearly with the number of nodes in the Infer-
ence Graph. The running time of Ferret is always less than 4 ms
times the number of nodes in the Inference Graph. With an Infer-
ence Graph of 500,000 nodes that contains 2,300 clients and 70
servers, it takes Ferret about 24 minutes to localize an injected
fault. Note that Ferret is easily parallelizable (see pseudo-code in
Algorithm 1) and implementing it on a cluster would signiﬁcantly
reduce the running time.
Impact of Errors in Inference Graph
6.2.4
Sometimes, errors are unavoidable when constructing inference
graphs. For example, service-level dependency graphs might con-
tain false positives or false negatives. Traceroutes might also report
the wrong intermediate routers. To understand how sensitive Ferret
is to errors in inference graphs, we compare the results of Ferret on
correct inference graphs with those on perturbed inference graphs.
We deliberately introduce four types of perturbation into in-
ference graphs: First, for each observation node in the inference
graph, we randomly add a new parent. Second, for each observation
node, we randomly swap one of its parents with a different node.
Third, for each edge in the inference graph, we randomly change
d
e
d
e
e
c
c
u
S
s
e
s
a
C
t
t
e
s
a
a
D
%
 92
 90
 88
 86
 84
 82
 80
 78
 76
 74
 1
Perturbing DepGraph Edge Probs.
Adding Edges in DepGraph
Swapping Edges in DepGraph
Perturbing Paths
% of Objects (Edges, Probs, Paths) Perturbed
 10
 100
Figure 20: Impact of errors in inference graph on Ferret’s abil-
ity to localizing faults.
its weight. Fourth, for each network-level path, we randomly add
an extra hop or permute its intermediate hops. The ﬁrst three types
of perturbation correspond to errors in service-level dependency
graphs and the last type corresponds to errors in traceroutes.
We use the same inference graph as the one in the ﬁeld deploy-
ment and perturb it in the ways that are described above. Figure 20
shows how Ferret behaves in the presence of each type of pertur-
bation. Each point in the ﬁgure represents the average of 1,000 ex-
periments. Note that Ferret is reasonably robust to all four types of
errors. Even when half the paths/nodes/weights are perturbed, Fer-
ret correctly localizes faults in 74.3% of the cases. Perturbing the
edge weights seems to have the least impact while permuting the
paths seems to be most harmful.
6.2.5 Modeling Redundancy Techniques
Specialized meta-nodes have important roles modeling load-
balancing and redundancy, such as ECMP, NLB, and failover. With-
out these nodes, the fault localization algorithm may come up with
unreasonable explanations for observations reported by clients. To
evaluate the impact specialized meta-nodes, we again used the
same inference graph as the one in the ﬁeld deployment. We cre-
ated 24 failure scenarios where the root cause of each of the fail-
ures is a component connected to a specialized meta-node (e.g. a
primary DNS server or an ECMP path). We then used Ferret to
localize these failures both on inference graphs using specialized
meta-nodes and on inference graphs using noisy-max meta-nodes
instead of specialized meta-nodes.
The key points of our evaluations are:
In 14 cases where the root cause was a secondary server or a
backup path, there is no difference between the two approaches. In
the remaining 10 cases where a primary server or path failed, Fer-
ret correctly identiﬁed the root cause in all 10 of the cases when
using specialized meta-nodes. In contrast, when not using special-
ized meta-nodes Ferret identiﬁed the wrong root cause in 4 cases.
6.3 Summary of Results
• First, we corroborated the inferred service-level dependency
graphs of ﬁfteen servers with our administrators and found them
to be mostly correct except for a few false-positives. Our algo-
rithm is able to discover service dependencies in a few hours
during a normal business day.
• Second, service dependencies vary widely from one server to
another and the inference graph of an enterprise network may
contain hundreds to thousands of nodes, justifying the need for
an automatic approach.
• Third, in a ﬁeld deployment we show that the Sherlock system
is effective at identifying performance problems and narrowing
down the root-cause to a small number of suspects. Over a ﬁve
day period, Sherlock identiﬁed over 1,029 performance prob-
lems in the network, and narrowed down more than 87% of the
blames to just 16 root causes out of the 350 potential ones. We
also validated the three most signiﬁcant outages with external
evidence. Further, Sherlock can help localize faults that may be
overlooked by using existing approaches.
• Finally, our simulations show that Sherlock is robust to noise in
the Inference Graph and its multi-level probabilistic model helps
localize faults more accurately than prior approaches that use a
two-level probabilistic model.
7. DISCUSSION
To save money and datacenter space, many enterprises are con-
solidating multiple servers onto a single piece of hardware via vir-
tual machines (VMs). We expect Sherlock techniques to be unaf-
fected by this trend, as most VM technologies (e.g. Xen, VMware,
VSS) assign each virtual server its own IP address, with the host
machine implementing a virtual Ethernet switch or IP router that
multiplexes the VMs to the single physical network interface. To
all the algorithms described in this paper, each VM appears as a
separate host, with the hosts joined together by a network element.
One source of failures that we have not modeled is the software
running on hosts. For example, if a buggy patch were installed on
the hosts in a network, it could cause correlated failures among the
hosts. Unless the inference graph models this shared dependency
on the patch, then blame for the failures will be incorrectly placed
on some component that is widely shared (e.g., the DNS service).
Extending our inference graph to these common failure modes will
be an important next step.
Using Sherlock as a research tool, we are now conducting a lon-
gitudinal study of the distributed applications used by our organi-
zation to determine how many different types of applications ex-
ist, whose dependencies we can automatically extract, and whose
we cannot. We expect to ﬁnd convoluted systems and protocols for
which Sherlock will not be able to extract the correct dependency
graph. However, we are hopeful as this paper has shown Sherlock’s
success on variety of common application types.
8. CONCLUSIONS
In this paper we describe Sherlock, a system that helps IT admin-
istrators localize performance problems across network and ser-
vices in a timely manner without requiring modiﬁcations to existing
applications and network components.
In realizing Sherlock, we make three important technical con-
tributions: (1) We introduce a multi-level probabilistic inference
model that captures the large sets of relationships between hetero-
geneous network components in enterprise networks. (2) We devise
techniques to automate the construction of the inference graph by
using packet traces, traceroute measurements, and network conﬁg-
uration ﬁles. (3) We describe an algorithm that uses an Inference
Graph to localize the root cause of the network or service problem.
We evaluate our algorithms and mechanisms via testbeds, simu-
lations and ﬁeld deployment in a large enterprise network. Our key
ﬁndings are: (1) service dependencies are complicated and con-
tinuously evolving over time thus justifying a need for automatic
approaches to discovering them. (2) Our service dependency infer-
ence algorithm is able to successfully discover dependencies for
a wide variety of unmodiﬁed services in a timely manner, (3) our
fault localization algorithm shows great promise in that it narrows
down the root cause of the performance problem to a small num-
ber of suspects helping IT administrators in their constant quest to
track down frequent user complaints, and ﬁnally, (4) comparisons
to other state-of-art techniques show that our fault localization al-
gorithm is robust to noise and it localizes performance problems
more quickly and accurately.
Acknowledgements
We would like to thank Jitu Padhye, Ratul Mahajan, Parveen Patel,
Emre Kiciman, Lun Li and the anonymous reviewers for useful
comments on the paper, and Geoffry Nordlund for helping with the
experimental setup and validation of the results.
9. REFERENCES
[1] M. K. Aguilera, J. C. Mogul, J. L. Wiener, P. Reynolds, and
A. Muthitacharoen. Performance Debugging for Distributed
Systems of Black Boxes. In SOSP, Oct. 2003.
[2] W. Aiello, C. Kalmanek, P. McDaniel, S. Sen, O. Spatscheck,
and J. V. der Merwe. Analysis of Communities of Interest in
Data Networks. In PAM, Mar. 2005.
[3] P. Barham, A. Donnelly, R. Isaacs, and R. Mortier. Using
Magpie for Request Extraction and Workload Modelling. In
OSDI, Dec. 2004.
[4] M. Y. Chen, A. Accardi, E. Kıcıman, J. Lloyd, D. Patterson,
A. Fox, and E. Brewer. Path-based failure and evolution
management. In NSDI’04, Mar. 2004.
[5] J. Dunagan, N. J. A. Harvey, M. B. Jones, D. Kostic,
M. Theimer, and A. Wolman. FUSE: Lightweight
Guaranteed Distributed Failure Notiﬁcation. In OSDI, 2004.
[6] S. Kandula, D. Katabi, and J.-P. Vasseur. Shrink: A Tool for
Failure Diagnosis in IP Networks. In Proc. MineNet
Workshop at SIGCOMM, 2005.
[7] R. R. Kompella, J. Yates, A. Greenberg, and A. Snoeren. IP
Fault Localization Via Risk Modeling. In Proc. of NSDI,
May 2005.
[8] D. J. C. MacKay. Information Theory, Inference, and
Learning Algorithms. Cambridge University Press, 2003.
[9] R. Mahajan, N. Spring, D. Wetherall, and T. Anderson.
User-level Internet Path Diagnosis. In SOSP, Oct. 2003.
[10] Microsoft Operations Manager.
http://www.microsoft.com/mom/.
[11] Multi Router Trafﬁc Grapher. http://www.mrtg.com/.
[12] K. P. Murphy, Y. Weiss, and M. I. Jordan. Loopy Belief
Propagation for Approximate Inference: An Empirical
Study. In Uncertainity in Artiﬁcial Intelligence, 1999.
[13] HP Openview. http://www.openview.hp.com/.
[14] R. Pang, M. Allman, M. Bennett, J. Lee, V. Paxson, and
B. Tierney. A First Look at Modern Enterprise Trafﬁc. In
IMC, Oct. 2005.
[15] J. Pearl. Probabilistic Reasoning in Intelligent Systems:
Networks of Plausible Inference. Morgan Kaufmann, 1988.
[16] P. Reynolds, J. L. Wiener, J. C. Mogul, M. K. Aguilera, and
A. Vahdat. WAP5: Black-box Performance Debugging for
Wide-area Systems. In WWW, May 2006.
[17] I. Rish, M. Brodie, and S. Ma. Efﬁcient Fault Diagnosis
Using Probing. In AAAI Spring Symposium on Information
Reﬁnement and Revision for Decision Making, March 2002.
[18] J. Sommers, P. Barford, N. Dufﬁeld, and A. Ron. Improving
Accuracy in End-to-end Packet Loss Measurement. In
SIGCOMM, 2005.
[19] IBM Tivoli. http://www.ibm.com/software/tivoli/.
[20] http://www.winpcap.org/.
[21] S. Yemini, S. Kliger, E. Mozes, Y. Yemini, and D. Ohsie.
High Speed and Robust Event Correlation. In IEEE
Communications Magazine, 1996.