### Optimized Text

According to Ferret's computations, 87% of the identified problems were attributed to just 16 out of the 358 independently failing components. We were able to validate the three most significant issues with external evidence. The Server1 incident was caused by a front-end web server experiencing intermittent but recurring performance issues. In the Server2 incident, another web server had trouble accessing its SQL backend. The third incident was due to recurring congestion on a link between R1 and the rest of the enterprise network. Figure 16 illustrates that when Ferret cannot pinpoint a single root cause due to insufficient information, it provides a list of the most likely suspects. For example, in the Server1 incident, four dots represent the web server, the last links to and from the web server, and the router to which the web server is directly connected.

**Figure 17: 5-minute averages of link utilization reported by SNMP. Oscillations around 14:00 correspond to observed performance issues.**

Sherlock can also uncover problems that might be overlooked by traditional threshold-based techniques. For instance, in the Server2 incident, both the web server and SQL backend were functioning normally, and thus, traditional methods would not raise any alerts. Only requests requiring interaction between the web server and the SQL backend experienced poor performance, which Sherlock was able to detect.

In a fourth incident, some clients experienced intermittent poor performance when accessing a web server in the data center, while other clients did not report any issues. Ferret identified a suspect link on the path to the data center that was shared only by those clients experiencing poor performance. Figure 17 shows the MRTG [11] data describing the bandwidth utilization of the congested link. Ferret's conclusion on when the link was troubled matches the spikes in link utilization between 12:15 and 17:30. However, an SNMP-based solution would struggle to detect this performance issue. First, the spikes in link utilization are always below 40% of the link capacity, which is common with SNMP counters, as they provide 5-minute averages and may not reflect instantaneous high link utilization. Second, the 60% utilization at 11:00 and 18:00 did not lead to any user-perceived problems, making it challenging to set a threshold that catches the problem without causing false alarms. Finally, due to scalability issues, administrators cannot collect relevant SNMP information from all potentially congested links.

### 6.2.2 Comparing Sherlock with Prior Approaches

Sherlock differs from previous fault localization approaches in its use of multi-level inference graphs instead of two-level bipartite graphs and its reliance on probabilistic dependencies. This comparison helps evaluate the impact of these design decisions.

To perform the comparison, we need a large set of observations for which the actual root causes are known. Since creating such a set using a testbed is infeasible, we conducted experiments with simulations. We first created a topology and its corresponding inference graph that exactly matched that of the production network. Then, we randomly set the state of each root cause to be troubled or down and performed a probabilistic walk through the inference graph to determine the state of all observation nodes. Repeating this process 1,000 times produced 1,000 sets of observations for which we knew the actual root causes. We then compared different techniques on their ability to identify the correct root cause given the 1,000 observation sets.

**Figure 18: Multi-level probabilistic model allows Ferret to correctly identify up to 32% more faults than Shrink, which uses two-level bipartite graphs. Figures 9 and 14 show that multi-level dependencies exist in real systems, and representing these dependencies using bipartite graphs loses important information. SCORE [7] uses a deterministic dependency model where a dependency either exists or not. For example, DNS caching makes it a weak dependency. Including such weak dependencies in the SCORE model leads to many false positives, while excluding them results in false negatives.**

### 6.2.3 Time to Localize Faults

We now study how long it takes Ferret to localize faults in large enterprise networks. In the following simulations, we used a topology identical to the one in our field deployment. We added more clients and servers to the topology and used the measurement results in Figure 15 to determine the number of unique clients that would access a server in a given time window. The experiments were run on an AMD Athlon 1.8GHz machine with 1.5GB of RAM. Figure 19 shows that the time it takes to localize injected faults grows almost linearly with the number of nodes in the Inference Graph. The running time of Ferret is always less than 4 ms per node in the Inference Graph. With an Inference Graph of 500,000 nodes containing 2,300 clients and 70 servers, it takes Ferret about 24 minutes to localize an injected fault. Note that Ferret is easily parallelizable (see pseudo-code in Algorithm 1), and implementing it on a cluster would significantly reduce the running time.

### 6.2.4 Impact of Errors in Inference Graph

Sometimes, errors are unavoidable when constructing inference graphs. For example, service-level dependency graphs might contain false positives or false negatives, and traceroutes might report incorrect intermediate routers. To understand how sensitive Ferret is to these errors, we compared the results of Ferret on correct inference graphs with those on perturbed inference graphs. We introduced four types of perturbations into the inference graphs: 

1. Randomly adding a new parent for each observation node.
2. Randomly swapping one of the parents of each observation node with a different node.
3. Randomly changing the weight of each edge in the inference graph.
4. Randomly adding an extra hop or permuting the intermediate hops for each network-level path.

The first three types of perturbations correspond to errors in service-level dependency graphs, and the last type corresponds to errors in traceroutes. Using the same inference graph as in the field deployment, we perturbed it in the ways described above. Figure 20 shows how Ferret behaves in the presence of each type of perturbation. Each point in the figure represents the average of 1,000 experiments. Note that Ferret is reasonably robust to all four types of errors. Even when half the paths, nodes, or weights are perturbed, Ferret correctly localizes faults in 74.3% of the cases. Perturbing the edge weights seems to have the least impact, while permuting the paths appears to be the most harmful.

### 6.2.5 Modeling Redundancy Techniques

Specialized meta-nodes play a crucial role in modeling load-balancing and redundancy techniques such as ECMP, NLB, and failover. Without these nodes, the fault localization algorithm may produce unreasonable explanations for observations reported by clients. To evaluate the impact of specialized meta-nodes, we used the same inference graph as in the field deployment. We created 24 failure scenarios where the root cause of each failure was a component connected to a specialized meta-node (e.g., a primary DNS server or an ECMP path). We then used Ferret to localize these failures on inference graphs using specialized meta-nodes and on inference graphs using noisy-max meta-nodes instead.

The key points of our evaluations are:

- In 14 cases where the root cause was a secondary server or a backup path, there was no difference between the two approaches.
- In the remaining 10 cases where a primary server or path failed, Ferret correctly identified the root cause in all 10 cases when using specialized meta-nodes. In contrast, when not using specialized meta-nodes, Ferret identified the wrong root cause in 4 cases.

### 6.3 Summary of Results

1. **Service Dependency Validation**: We corroborated the inferred service-level dependency graphs of fifteen servers with our administrators and found them to be mostly correct, except for a few false positives. Our algorithm can discover service dependencies within a few hours during a normal business day.
2. **Complexity and Evolution of Service Dependencies**: Service dependencies vary widely from one server to another, and the inference graph of an enterprise network may contain hundreds to thousands of nodes, justifying the need for an automatic approach.
3. **Effectiveness in Field Deployment**: In a field deployment, Sherlock effectively identified over 1,029 performance problems in the network over five days, narrowing down more than 87% of the issues to just 16 root causes out of the 350 potential ones. We validated the three most significant outages with external evidence. Additionally, Sherlock can help localize faults that may be overlooked by existing approaches.
4. **Robustness and Accuracy**: Our simulations show that Sherlock is robust to noise in the Inference Graph, and its multi-level probabilistic model helps localize faults more accurately than prior approaches that use a two-level probabilistic model.

### 7. Discussion

Many enterprises are consolidating multiple servers onto a single piece of hardware via virtual machines (VMs) to save money and data center space. We expect Sherlock's techniques to remain unaffected by this trend, as most VM technologies (e.g., Xen, VMware, VSS) assign each virtual server its own IP address, with the host machine implementing a virtual Ethernet switch or IP router that multiplexes the VMs to the single physical network interface. To the algorithms described in this paper, each VM appears as a separate host, with the hosts joined together by a network element.

One source of failures that we have not modeled is the software running on hosts. For example, if a buggy patch is installed on the hosts in a network, it could cause correlated failures among the hosts. Unless the inference graph models this shared dependency on the patch, the blame for the failures will be incorrectly placed on some component that is widely shared (e.g., the DNS service). Extending our inference graph to include these common failure modes will be an important next step.

Using Sherlock as a research tool, we are conducting a longitudinal study of the distributed applications used by our organization to determine the number of different types of applications, whose dependencies we can automatically extract, and whose we cannot. We expect to find convoluted systems and protocols for which Sherlock will not be able to extract the correct dependency graph. However, this paper has shown Sherlock's success with a variety of common application types, giving us hope for future developments.

### 8. Conclusions

In this paper, we describe Sherlock, a system that helps IT administrators localize performance problems across networks and services in a timely manner without requiring modifications to existing applications and network components.

In realizing Sherlock, we make three important technical contributions:

1. **Multi-Level Probabilistic Inference Model**: We introduce a multi-level probabilistic inference model that captures the large sets of relationships between heterogeneous network components in enterprise networks.
2. **Automated Inference Graph Construction**: We devise techniques to automate the construction of the inference graph using packet traces, traceroute measurements, and network configuration files.
3. **Fault Localization Algorithm**: We describe an algorithm that uses an Inference Graph to localize the root cause of the network or service problem.

We evaluated our algorithms and mechanisms via testbeds, simulations, and field deployment in a large enterprise network. Our key findings are:

1. **Complex and Evolving Service Dependencies**: Service dependencies are complicated and continuously evolving, justifying the need for automatic approaches to discovering them.
2. **Successful Dependency Discovery**: Our service dependency inference algorithm successfully discovers dependencies for a wide variety of unmodified services in a timely manner.
3. **Effective Fault Localization**: Our fault localization algorithm narrows down the root cause of performance problems to a small number of suspects, helping IT administrators track down frequent user complaints.
4. **Comparative Robustness and Accuracy**: Comparisons to other state-of-the-art techniques show that our fault localization algorithm is robust to noise and localizes performance problems more quickly and accurately.

### Acknowledgements

We would like to thank Jitu Padhye, Ratul Mahajan, Parveen Patel, Emre Kiciman, Lun Li, and the anonymous reviewers for their useful comments on the paper, and Geoffry Nordlund for his assistance with the experimental setup and validation of the results.

### References

[1] M. K. Aguilera, J. C. Mogul, J. L. Wiener, P. Reynolds, and A. Muthitacharoen. Performance Debugging for Distributed Systems of Black Boxes. In SOSP, Oct. 2003.

[2] W. Aiello, C. Kalmanek, P. McDaniel, S. Sen, O. Spatscheck, and J. V. der Merwe. Analysis of Communities of Interest in Data Networks. In PAM, Mar. 2005.

[3] P. Barham, A. Donnelly, R. Isaacs, and R. Mortier. Using Magpie for Request Extraction and Workload Modelling. In OSDI, Dec. 2004.

[4] M. Y. Chen, A. Accardi, E. Kıcıman, J. Lloyd, D. Patterson, A. Fox, and E. Brewer. Path-based failure and evolution management. In NSDI’04, Mar. 2004.

[5] J. Dunagan, N. J. A. Harvey, M. B. Jones, D. Kostic, M. Theimer, and A. Wolman. FUSE: Lightweight Guaranteed Distributed Failure Notification. In OSDI, 2004.

[6] S. Kandula, D. Katabi, and J.-P. Vasseur. Shrink: A Tool for Failure Diagnosis in IP Networks. In Proc. MineNet Workshop at SIGCOMM, 2005.

[7] R. R. Kompella, J. Yates, A. Greenberg, and A. Snoeren. IP Fault Localization Via Risk Modeling. In Proc. of NSDI, May 2005.

[8] D. J. C. MacKay. Information Theory, Inference, and Learning Algorithms. Cambridge University Press, 2003.

[9] R. Mahajan, N. Spring, D. Wetherall, and T. Anderson. User-level Internet Path Diagnosis. In SOSP, Oct. 2003.

[10] Microsoft Operations Manager. http://www.microsoft.com/mom/.

[11] Multi Router Traffic Grapher. http://www.mrtg.com/.

[12] K. P. Murphy, Y. Weiss, and M. I. Jordan. Loopy Belief Propagation for Approximate Inference: An Empirical Study. In Uncertainty in Artificial Intelligence, 1999.

[13] HP Openview. http://www.openview.hp.com/.

[14] R. Pang, M. Allman, M. Bennett, J. Lee, V. Paxson, and B. Tierney. A First Look at Modern Enterprise Traffic. In IMC, Oct. 2005.

[15] J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann, 1988.

[16] P. Reynolds, J. L. Wiener, J. C. Mogul, M. K. Aguilera, and A. Vahdat. WAP5: Black-box Performance Debugging for Wide-area Systems. In WWW, May 2006.

[17] I. Rish, M. Brodie, and S. Ma. Efficient Fault Diagnosis Using Probing. In AAAI Spring Symposium on Information Refinement and Revision for Decision Making, March 2002.

[18] J. Sommers, P. Barford, N. Dufﬁeld, and A. Ron. Improving Accuracy in End-to-end Packet Loss Measurement. In SIGCOMM, 2005.

[19] IBM Tivoli. http://www.ibm.com/software/tivoli/.

[20] http://www.winpcap.org/.

[21] S. Yemini, S. Kliger, E. Mozes, Y. Yemini, and D. Ohsie. High Speed and Robust Event Correlation. In IEEE Communications Magazine, 1996.