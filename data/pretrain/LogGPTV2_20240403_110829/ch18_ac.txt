def nonlin(x,deriv=False):def nonlin(x,deriv=False):
  if(deriv==True):
    return (x*(1–x))
  return (1/(1+np.exp(-x)))
#sigmoid_function显示上述代码在操作中。
Sigmoid 函数
我们在#biological_neuron_and_an_artificial_neuro看到的人工神经元有不同的输入（x1...xn）具有不同的权重（w1...wn）然后，这些输入的加权总和通过 Sigmoid 或 Heaside 步长函数f传递，如下所示：
# Initialize the dataset as a matrix with input Data:
X = np.array([[0,0,1],
             [0,1,1],
             [1,0,1],
             [1,1,1]])[1,1,1]])
# Output Data with one output neuron each:
Y = np.array([[1],
              [0.7],
              [1],
              [0]])
# Seed to make them deterministic
np.random.seed(1)
# Create synapse matrices.
synapse0 = 2 * np.random.random((3, 4)) - 1
synapse1 = 2 * np.random.random((4, 1)) - 1接下来，我们将数据集初始化为包含输入数据（x）的矩阵。每行是一个不同的训练示例，每列表示不同的神经元。输出数据（y）每个输出神经元，我们将它们播种，使它们具有确定性；这将产生具有相同起点的随机数（可用于调试），因此，每次运行程序时，都可以获得相同的生成数序列。为了完成这个简单的神经网络，我们创建了两个突触矩阵并初始化神经网络的权重。我们刚刚创建了一个具有两层权重的神经网络：
# Training code (loop)
for j in xrange(100000):
    # Layers layer0,layer1,layer2
    layer0 = X
    # Prediction step
    layer1 = nonlin(np.dot(layer0, synapse0))
    layer2 = nonlin(np.dot(layer1, synapse1))# Get the error rate
    layer2_error = Y - layer2
    # Print the average error
    if(j % 10000) == 0:
        print "Error:" + str(np.mean(np.abs(layer2_error)))
    # Multiply the error rate
    layer2_delta = layer2_error * nonlin(layer2, deriv=True)
    # Backpropagation
    layer1_error = layer2_delta.dot(synapse1.T)
    # Get layer1's delta
    layer1_delta = layer1_error * nonlin(layer1, deriv=True)# Gradient Descent
    synapse1 += layer1.T.dot(layer2_delta)
    synapse0 += layer0.T.dot(layer1_delta)
上例中的训练代码涉及更多，我们为给定数据集优化网络。第一层（layer0 ）只是我们的输入数据。预测步骤在每个图层及其突触之间执行矩阵乘法。然后，我们在矩阵上运行 Sigmoid 函数以创建下一个图层。通过layer1中的数据除以layer2产生预测数据，对于layer2我们可以使用减法将其与预期输出数据进行比较，以获得错误率。然后，我们继续按设定的时间间隔打印平均误差，以确保每次错误都会下降。
我们将误差率乘以 Sigmoid 的斜率，在layer2中的值和 做反向传播，这是在神经网络上执行梯度下降的主要算法。首先，每个节点的输出值在正向传递中计算（和缓存）。然后，在图形的向后传递中计算与每个参数误差的局部导数。它是"错误向后传播"的缩写，即导致 layer1 上的错误的原因， layer2 并将 layer2 增量乘以突触 1 的转置。接下来，我们将 layer1 通过将误差乘以 Sigmoid 函数的结果，并通过梯度下降这是一种通过计算相对于模型参数的损耗梯度来最小化损失的技术，条件在训练数据上。非正式地，梯度下降迭代地调整参数，逐渐找到权重和偏置的最佳组合，以尽量减少损耗。来获得的增量，也就是一个一阶迭代优化算法，用于查找函数的最小值，最后更新权重。现在，我们每个层都有增量，我们可以使用它们来更新突触速率，以便每次迭代时都能更降低错误率。这将生成以下内容：
Error:0.434545246367
Error:0.00426490134801
Error:0.00285547679431
Error:0.00226684843815
Error:0.00192718684831
Error:0.00170049171577
Error:0.00153593455208
Error:0.00140973826096Error:0.00140973826096
Error:0.00130913223749
Error:0.00122657710472
这意味着随着神经网络的学习，误差越来越接近于零。如果我们打印每个 layer2 和我们的目标：
print "Output after training"
print layer2
Output after training
[[ 0.99998867]
 [ 0.69999105]
 [ 0.99832904]
 [ 0.00293799]]
print "Initial Objective"
print Y
Initial Objective
[[ 1. ]
 [ 0.7]
 [ 1. ]
 [ 0. ]]
我们已经成功地创建了一个神经网络，仅使用NumPy和一些数学，并训练它接近初始目标，通过使用反向传播和梯度下降。这在更大的场景中非常有用，我们教神经网络识别模式，如异常检测、声音、图像，甚至我们平台中的某些事件，稍后将会看到。使用 TensorFlow 和 TensorBoard
  正如我们现在所看到的，Google 的 TensorFlow 只不过是我们刚刚看到的 NumPy。主要区别在于 TensorFlow 首先构建了要完成的所有操作的图形，然后在调用“会话”时，它“运行”图形。它可以扩展，并通过 CUDA 来使用 GPU。另一个库 Keras 简化了 TensorFlow 的编码（参见#tensordot9）。
在数学中，张量是描述几何矢量、标量和其他张量之间的线性关系的几何对象。此类关系的基本示例包括点积、交叉产品和线性贴图。（来源：https://en.wikipedia.org/wiki/Tensor。）
请记住，TensorFlow 本身没有“神经元”，但它确实喜欢线性代数。神经元只能算是过去对生物智能的一种模拟，其实一切都只是矩阵数学，或者从抽象角度来说算是张量数学，这是 TensorFlow 名字的由来。这个模型使 TensorFlow 非常灵活，并使其能够实现之前难以企及的高效计算。可以从 pip 包管理器安装 TensorFlow：
pip install tensorflow
要测试安装，可以运行以下命令集：
import tensorflow as tf
a = tf.constant(1.0)
b = tf.constant(2.0)
c = a + b
sess = tf.Session()
print(sess.run(c))
生成的操作以#resulting_operation来展示。
生成的操作
忽略有关 CPU 的任何警告。我们的 TensorFlow 运行的结果应该是 3.0 。
前面的测试虽然不是很令人印象深刻，但显示了 TensorFlow 在实际在会话中运行之前如何声明内容。
以下 Python代码，https://www.oreilly.com/learning/hello-tensorflow；感谢 Aaron Schumacher 的分享。虽然不使用真实数据集，但使用 TensorFlow 进行一些数据神经元训练：1  import tensorflow as tf
2 
3  x = tf.constant(1.0, name='input')
4  w = tf.Variable(0.8, name='weight')
5  y = tf.multiply(w, x, name='output')