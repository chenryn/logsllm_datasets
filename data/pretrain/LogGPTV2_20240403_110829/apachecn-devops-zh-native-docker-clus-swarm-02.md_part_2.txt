因此，我们创建了一个集群，其中有一个主节点、三个从节点，并且启用了 TLS 并准备接受容器。
我们可以确保从主节点并列出集群中的节点。我们现在将使用`swarm list`命令:
```
$ docker run swarm list token://$TOKEN
```
![Re-architecting the example of Chapter 1 with token](img/image_02_008.jpg)
## 令牌限制
令牌尚未被弃用，但可能很快就会被弃用。标准要求 Swarm 中的每个节点都应该有互联网连接，这不是很方便。此外，对 Docker Hub 的访问使得这种技术依赖于 Hub 的可用性。实际上，它将集线器作为单点故障。然而，使用 token，我们能够更好地理解幕后的情况，我们遇到了 Swarm v1 命令:`create`、`manage`、`join`和`list`。
现在是时候进一步了解真正的发现服务和共识算法了，这是容错系统的一个基本原则。
# 木筏
共识是分布式系统中的一种算法，它迫使系统中的代理在一致的价值观上达成一致，并选出一个领导者。
一些著名的共识算法是 Paxos 和 Raft。Paxos 和 Raft 提供了类似的性能，但 Raft 不太复杂，更容易理解，因此在分布式商店实现中变得非常受欢迎。
作为共识算法，Consul 和 Etcd 实现 Raft，而 ZooKeeper 实现 Paxos。实现 Raft 的 CoreOS Etcd Go 库作为一个依赖项包含在 SwarmKit 和 Swarm Mode 中(在`vendor/`中)，因此在本书中我们将更加关注它。
筏在《奥加罗，驱逐》一文中有详细描述，在[https://ramcloud.stanford.edu/raft.pdf](https://ramcloud.stanford.edu/raft.pdf)有售。在接下来的部分中，我们将总结它的基本概念。
## 筏理论
Raft 在设计时考虑到了简单性，与 Paxos 相比，它真正实现了这一目标(甚至有学术出版物对此进行了论证)。就我们的目的而言，Raft 和 Paxos 的主要区别在于，在 Raft 中，消息和日志仅由集群领导者发送给其对等方，这使得算法更易于理解和实现。我们将在理论部分使用的示例库是 CoreOS Etcd 提供的 Go 库，可在[https://github.com/coreos/etcd/tree/master/raft](https://github.com/coreos/etcd/tree/master/raft)获得。
Raft 集群由节点组成，这些节点必须以一致的方式维护复制的状态机，无论如何:新节点可以加入，旧节点可能崩溃或变得不可用，但这个状态机必须保持同步。
为了实现这一故障感知目标，Raft 集群通常由奇数个节点组成，例如三个或五个，以避免大脑分裂。当剩下的节点分裂成无法就领导人选举达成一致的群体时，就会出现分裂。如果有奇数个节点，他们最终可以以多数同意一个领导者。如果是偶数，选举可以以 50%-50%的结果结束，这是不应该发生的。
回到 raft，Raft 集群在`raft.go`中被定义为一个类型 Raft 结构，并且包括诸如领导者 UUID、当前术语、指向日志的指针以及用于检查法定人数和选举状态的实用程序等信息。让我们通过分解集群组件 Node 的定义来逐步说明所有这些概念。节点被定义为`node.go`中的一个接口，在这个库中被规范地实现为`type node struct`。
```
type Node interface {
 Tick()
 Campaign(ctx context.Context) error
 Propose(ctx context.Context, data []byte) error
 ProposeConfChange(ctx context.Context, cc pb.ConfChange) error
 Step(ctx context.Context, msg pb.Message) error
 Ready() <-chan Ready
 Advance()
 ApplyConfChange(cc pb.ConfChange) *pb.ConfState
 Status() Status
 ReportUnreachable(id uint64)
 ReportSnapshot(id uint64, status SnapshotStatus)
 Stop()
}
```
每个节点保持一个任意长度的刻度(增加`Tick()`，表示时间或纪元的术语或周期，这是当前运行的时刻。在每个术语中，节点可以处于以下状态类型之一:
*   领导者
*   候选人
*   属下
在正常情况下，只有一个领导者，所有其他节点都是追随者。领导者为了让我们尊重它的权威，会定期向它的追随者发送心跳信息。当追随者注意到心跳消息不再到达时，他们明白领导者不再可用，因此他们增加自己的价值并成为候选人，然后试图通过运行`Campaign()`成为领导者。他们从投票给自己开始，试图达到法定人数。当一个节点实现了这一点，一个新的领导者就当选了。
`Propose()`是一种建议将数据追加到日志中的方法。日志是 Raft 中用于同步集群状态的数据结构，也是 Etcd 中的另一个重要概念。它保存在一个稳定的存储(内存)中，当日志变大时，它能够压缩日志以节省空间(快照)。领导者确保日志始终处于一致状态，并且只有在确定该信息已经通过其大多数追随者复制时，才会提交新数据以附加到其日志(主日志)中，因此达成了一致。有一种`Step()`方法，将状态机推进到下一步。
`ProposeConfChange()`是允许我们在运行时更改集群配置的方法。它被证明在任何情况下都是安全的，这要归功于它的两阶段机制，该机制确保了每一个可能的多数都同意这一变化。`ApplyConfChange()`将此更改应用于当前节点。
然后就是`Ready()`。在节点接口中，该函数返回一个只读通道，该通道返回准备读取、保存到存储和提交的消息的封装规范。通常，在调用就绪并应用其条目后，客户端必须调用`Advance()`，以通知就绪已取得进展。实际上，`Ready()`和`Advance()`是 Raft 通过避免日志、日志内容和状态同步中的不一致来保持高一致性的方法的一部分。
这就是 Raft 实现在 CoreOS 的 Etcd 中的样子。
## 筏在实践中
如果你想举手练习 Raft，一个好主意是使用来自 Etcd 的`raftexample`，开始一个三人集群。
由于 Docker 撰写 YAML 文件是自描述的，下面的示例是准备运行的撰写文件:
```
version: '2'
services:
 raftexample1:
 image: fsoppelsa/raftexample
 command: --id 1 --cluster 
          http://127.0.0.1:9021,http://127.0.0.1:9022,
          http://127.0.0.1:9023 --port 9121
 ports:
 - "9021:9021"
 - "9121:9121"
 raftexample2:
 image: fsoppelsa/raftexample
 command: --id 2 --cluster    
          http://127.0.0.1:9021,http://127.0.0.1:9022,
          http://127.0.0.1:9023 --port 9122
 ports:
 - "9022:9022"
 - "9122:9122"
 raftexample3:
 image: fsoppelsa/raftexample
 command: --id 3 --cluster 
          http://127.0.0.1:9021,http://127.0.0.1:9022,
          http://127.0.0.1:9023 --port 9123
 ports:
 - "9023:9023"
 - "9123:9123"
```
该模板创建了三个 Raft 服务(`raftexample1`、`raftexample2`和`raftexample3`)。每一个都运行一个 raftexample 的实例，通过用`--port`公开 API，并用`--cluster`使用静态集群配置。
您可以通过以下方式在 Docker 主机上启动:
```
docker-compose -f raftexample.yaml up
```
现在你可以玩了，例如通过杀死领导者，观察新的选举，通过 API 为其中一个容器设置一些值，移除容器，更新值，重新启动容器，检索这个值，并注意它被正确升级。
与 API 的交互可以通过 curl 完成，如[https://github . com/coreos/etcd/tree/master/contrib/raft example](https://github.com/coreos/etcd/tree/master/contrib/raftexample)所述:
```
curl -L http://127.0.0.1:9121/testkey -XPUT -d value
curl -L http://127.0.0.1:9121/testkey
```
我们把这个练习留给更热情的读者。
### 类型
当您试图采用 Raft 实现时，选择 Etcd 的 Raft 库以获得最高性能，选择 Consul(来自 Serf 库)以实现易用且更容易的实现。
# Etcd
Etcd 是一个高度可用、分布式和一致的键值存储，用于共享配置和服务发现。使用 Etcd 的一些著名项目有 FlighKit、Kubernetes 和 Fleet。
Etcd 可以在网络分裂的情况下优雅地管理主机选举，并且可以容忍节点故障，包括主机。应用，在我们的例子中是 Docker 容器和 Swarm 节点，可以读写数据到 Etcd 的键值存储中，例如服务的位置。
## 用 Etcd 重新构建第 1 章的例子
我们再次创建了一个具有一个管理器和三个节点的示例，这次是通过说明 Etcd。
这一次，我们需要一个真正的发现服务。我们可以通过在 Docker 内部运行 Etcd 服务器来模拟一个非 HA 系统。我们创建了一个由四个主机组成的集群，名称如下:
*   `etcd-m`将是 Swarm 主机，也将托管 Etcd 服务器
*   `etcd-1`:第一个 Swarm 节点
*   `etcd-2`:第二个 Swarm 节点
*   `etcd-3`:第三个 Swarm 节点
操作员通过连接到`etcd-m:3376`，将像往常一样在三个节点上操作 Swarm。
让我们从用机器创建主机开始:
```
for i in m `seq 1 3`; do docker-machine create -d virtualbox etcd-$i; 
done
```
现在我们将在 `etcd-m`上运行 Etcd 主程序。我们使用来自 CoreOS 的`quay.io/coreos/etcd`官方图片，遵循可在[获得的文档。](https://github.com/coreos/etcd/blob/master/Documentation/op-guide/clustering.md)
首先，在终端中，我们获取`etcd-m`外壳变量:
```
term0$ eval $(docker-machine env etcd-m)
```
然后，我们在单主机模式下运行 Etcd 主机(也就是说，没有容错能力，等等):