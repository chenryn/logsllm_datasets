tocol speciﬁcation [35], the OBI informs the controller
about the implementations available for each supported
abstract processing block. The controller can then spec-
ify the exact implementation it would like the OBI to
use, or let the OBI use its default settings and choose
the implementation itself. The OpenBox protocol also
allows injecting new custom blocks from the controller
to the OBI, as described in detail in Section 3.2.1.
Table 1 lists some of the fundamental abstract pro-
cessing blocks deﬁned by the OpenBox protocol. Each
block has its own conﬁguration parameters and addi-
tional information, as described in Section 3.2.
2.2 Merging Multiple Graphs
Our framework allows executing multiple network func-
tions at a single data plane location. For example, pack-
513
Read Packets Header Classifier Drop Alert Output Read Packets Header Classifier Drop Alert Regex Classifier Regex Classifier Regex Classifier Output Figure 3: A na¨ıve merge of the two processing
graphs shown in Figure 2.
Figure 4: The result of our graph merge algo-
rithm for the two processing graphs shown in
Figure 2.
ets may have to go through a ﬁrewall and then through
an IPS. We could simply use multiple processing graphs
at such locations, making packets traverse the graphs
one by one, as shown in Figure 3. In this section we
show how to merge multiple graphs while preserving
the correct processing order and results.
Consider two network functions as shown in Figure 2,
running at the same physical location in the data plane.
We would like to merge the two graphs into one, such
that the logic of the ﬁrewall is ﬁrst executed on packets,
followed by the execution of the IPS logic. Addition-
ally, we would like to reduce the total delay incurred on
packets by both NFs by reducing the number of blocks
each packet traverses. The desired result of this pro-
cess is shown in Figure 4: We would like packets to go
through one header classiﬁcation instead of two, and
execute the logic that corresponds to the result of this
classiﬁcation.
2.2.1 Graph Merge Algorithm
Our graph merge algorithm must ensure that correct-
ness is maintained: a packet must go through the same
path of processing steps such that it will be classiﬁed,
modiﬁed and queued the same way as if it went through
the two distinct graphs. We also want to make sure that
static operations such as alert or log will be executed
on the same packet, at the same state, as they would
without merging. Our goal in this process is to reduce
the per-packet latency, so we would like to minimize the
length of paths between input and output terminals in
the graph.
In order to model the merge algorithm, we classify
blocks into ﬁve classes:
(current, port) ← Q.poll()
if current is a classiﬁer, modiﬁer or shaper then
if start is null then
start ← current
for each outgoing connector c from current do
(cid:46) Mark start of path
else
end for
continue
end ← current
if start and end are mergeable classiﬁers then
(cid:46) start is not null - end of path
merged ← merge(start, end)
for each output port p of merged do
Clone the path from start’s correct
successor for port p to end (exclusive)
Mark clone of last block before end
Clone the sub-tree from end’s correct
successor for port p
Rewire connectors from merged port p
to the clones and between clones
end if
if c.dst not in Q then
Add (c.dst, c.srcP ort) to Q
Algorithm 1 Path compression algorithm
1: function compressPaths(G = (V, E), root ∈ V )
Require: G is normalized
Q ← empty queue
2:
Add (root, −1) to Q
3:
start ← null
4:
5:
while Q is not empty do
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33:
34:
35:
36:
37:
38:
39:
40:
41:
42:
43:
44:
45:
46:
47:
48:
49:
50:
51:
end if
52:
end while
53:
54:
return G
55: end function
end for
if graph G was changed then
end for
current ← merged
Add (c.dst, c.srcP ort) to Q
end if
end for
continue
if c.dst not in Q then
end if
end if
end if
else
(cid:46) Not mergeable classiﬁers
if start and end are classiﬁers then
Treat start and end as a single classiﬁer
Find next classiﬁer, modiﬁer or shaper
and mark the last block before it.
end if
for each outgoing connector c from current do
Find mergeable blocks from c to a marked
block. Merge and rewire connectors
Restart compressPaths(G, current)
else
(cid:46) Skip statics, terminals
for each outgoing connector c from current do
• Terminals (T): blocks that start or terminate the
processing of a packet.
• Classiﬁers (C): blocks that, given a packet, clas-
sify it according to certain criteria or rules and
output it to a speciﬁc output port.
• Modiﬁers (M): blocks that modify packets.
• Shapers (Sh): blocks that perform traﬃc shap-
ing tasks such as active queue management or rate
limiting.
514
Header Classifier Drop Alert (IPS) Regex Classifier Regex Classifier Regex Classifier Output Read Packets Header Classifier Drop Alert (Firewall) Read Packets Header Classifier Drop Alert  (IPS) Regex Classifier Regex Classifier Regex Classifier Output Alert  (Firewall) Alert  (Firewall) Alert  (Firewall) Alert  (Firewall) • Statics (St): blocks that do not modify the packet
or its forwarding path, and in general do not be-
long to the classes above.
We use these classes in our graph merge algorithm in
order to preserve the correctness of the merged graph:
We can change the order of static blocks, or move classi-
ﬁers before static blocks, but we cannot move classiﬁers
across modiﬁers or shapers, as this might lead to incor-
rect classiﬁcation. We can merge classiﬁers, as long as
we pay special attention to the rule set combination and
output paths. We can also merge statics and modiﬁers
in some cases. The right column in Table 1 speciﬁes the
class of each block.
Our algorithm works in four stages. First, it nor-
malizes each processing graph to a processing tree, so
that paths do not converge.1 Then, it concatenates the
processing trees in the order in which the correspond-
ing NFs are processed. Note that a single terminal in
the original processing graph may correspond to several
leaves in the processing tree. A copy of the subsequent
processing tree will be concatenated to each of these
leaves. Nevertheless, the length of any path in the tree
(from root to leaf) is exactly the same as it was in the
original processing graph, without normalization.2
While the number of blocks in the merged tree can
increase multiplicatively,3 in practice this rarely hap-
pens, and most importantly, the number of blocks in
the graph has no eﬀect on OBI performance. The sig-
niﬁcant parameter is the length of paths, as longer paths
mean greater delay. Moreover, two graphs need not be
merged if the overheads are too high. The controller is
responsible for avoiding such a merger.
As the processing tree is in fact a collection of paths,
the third stage in our algorithm is re-ordering and merg-
ing blocks along a path. This is shown in Algorithm 1.
As mentioned before, the algorithm works by examin-
ing the class of the blocks and deciding whether blocks
can be merged (Line 7).
Perhaps the most interesting case is merging two clas-
siﬁer blocks. Speciﬁcally, classiﬁer blocks of the same
type can support merging by having their own merge
logic. The merge should resolve any conﬂicts according
to the ordering and priorities of the two input applica-
tions (if applicable) and on the priority of the merged
rules (Lines 18-29).
For example, in our implementation, the HeaderClas-
siﬁer block is mergeable: it implements a speciﬁc Java
interface and a mergeWith(...) method, which cre-
ates a cross-product of rules from both classiﬁers, or-
1The process of graph normalization may theoretically lead to
an exponential number of blocks. This only happens with a cer-
tain graph structure, and it never happened in our experiments.
However, if it does, our system rolls back to the na¨ıve merge.
2The process of graph concatenation requires careful handling
of special cases with regard to input and output terminals. We
address these cases in our implementation. However, due to space
considerations, we omit the technical details from the paper.
3For graphs G1 = (V1, E1) and G2 = (V2, E2), the number of
blocks in the merged graph is up to |V1|2(1 + |V2|2)
Figure 5: Sample OpenBox network with dis-
tributed data plane processing (as in Figure 6).
ders them according to their priority, removes dupli-
cate rules caused by the cross-product and empty rules
caused by priority considerations, and outputs a new
classiﬁer that uses the merged rule set. After merging
classiﬁer blocks, our algorithm rewires the connectors
and clones the egress paths from the classiﬁers such
that packets will correctly go through the rest of the
processing blocks. The merge algorithm is then applied
recursively on each path, to compress these paths when
possible. See the paths from the header classiﬁer block
in Figure 4 for the outcome of this process in our ex-
ample.
It is also possible to merge static and modiﬁer blocks,
if they are of the same class and type, and their param-
eters do not conﬂict. For example, two instances of a
rewrite header block can be merged in constant time if
they modify diﬀerent ﬁelds, or the same ﬁeld with the
same value (lines 38-39 in Algorithm 1).
The last stage of our algorithm takes place after the
merge process is completed. It eliminates copies of the
same block and rewires the connectors to the remaining
single copy, so that eventually the result is a graph as
shown in Figure 4, and not necessarily a tree. Note that
the diameter of the merged processing graph, as shown
in Figure 4, is shorter (six blocks) than the diameter of
the graph we would have obtained from a na¨ıve merge
(seven blocks, see Figure 3).
The correctness of the process stems from the follow-
ing: First, any path a packet would take on the na¨ıvely
merged graph exists, and will be taken by the same
packet, on the normalized and concatenated graph. Sec-
ond, when merging classiﬁers we duplicate paths such
that the previous property holds. Third, we only elimi-
nate a copy of a block if the remaining copy is pointing
to exactly the same path (or its exact copy).
3. OPENBOX FRAMEWORK ARCHIT-
ECTURE
In this section we describe the OpenBox framework
in detail by dividing it into layers, as shown in Fig-
515
OpenBox Controller OpenBox Applications OBI VM HW OBI OBI VM 1 2 3 4 5 6 A B ure 1:
from OpenBox service instances (OBIs) in the
data plane at the bottom, through the OpenBox proto-
col and the OpenBox controller (OBC), to the applica-
tions at the top.
3.1 Data Plane
The OpenBox data plane of consists OpenBox service
instances (OBIs), which are low-level packet processors.
An OBI receives a processing graph from the controller
(described in Section 3.3). The OBI applies the graph
it was assigned on packets that traverse it. It can also
answer queries from the controller and report its load
and system information.
OBIs can be implemented in software or hardware.
Software implementations can run in a VM and be pro-
visioned and scaled on demand. An OBI provides im-
plementations for the abstract processing blocks it sup-
ports, and declares its implementation block types and
their corresponding abstract block in the Hello message
sent to the OBC. The controller may use a speciﬁc im-
plementation in the processing graph it sends to the
OBI, or use the abstract block name, leaving the choice
of exact implementation to the OBI.
An OBI may be in charge of only part of a processing
graph. In this case, one or more additional OBIs should
be used to provide the remaining processing logic. A
packet would go through a service chain of all corre-
sponding OBIs, where each OBI attaches metadata (us-
ing some encapsulation technique [12, 19, 37] see also
Section 3.4) to the packet before sending it to the next
OBI. Upon receiving a packet from a previous OBI, the
current OBI decodes the attached metadata and acts