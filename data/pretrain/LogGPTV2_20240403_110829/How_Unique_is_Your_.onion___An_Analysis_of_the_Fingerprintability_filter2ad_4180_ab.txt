of onion services will land first on homepage more often than for
regular sites before logging in or further interacting with the site.
In this paper, we focus only on onion services because a 2015
study showed that the website fingerprinting adversary can distin-
guish between visits to onion services and regular websites with
high accuracy [17]. Even though Panchenko et al.’s study shows that
website fingerprinting does not scale to the Web, website finger-
printing has been identified as a potential threat for onion services
for two reasons [8]: first, in contrast to the Web’s size, the onion
service space’s size may be sufficiently small for an adversary to
build a fingerprint database for all existing onion services; second,
onion services tend to host sensitive content and visitors of these
sites may be subject to more serious, adverse consequences.
2.2 State-of-the-art attacks
We have selected three classifiers proposed in recent prior work for
our study because they represent the most advanced and effective
website fingerprinting attacks to date. Each attack uses different
classification algorithms and feature sets, although they have some
features in common. The details of each classifier are as follows:
Wang-kNN [26]: Wang et al. proposed an attack based on a k-
Nearest Neighbors (k-NN) classifier that used more than 3,000 traffic
features. Some of the most relevant features are the number of
outgoing packets in spans of 30 packets, the lengths of the first 20
packets, and features that capture traffic bursts, i.e., sequences of
packets in the same direction. They also proposed an algorithm to
tune the weights of the custom distance metric used by the k-NN
that minimizes the distance among instances that belong to the
same site. They achieved between 90% to 95% accuracy on a closed-
world of 100 non-onion service websites [26]. Kwon et al. evaluated
their own implementation of the attack for 50 onion service sites
and obtained 97% accuracy.
CUMUL [21]: Panchenko et al. designed CUMUL, an attack based
on a Radial Basis Function kernel (RBF) SVM. Each feature instance
is a 104-coordinate vector formed by the number of bytes and pack-
ets in each direction and 100 interpolation points of the cumulative
sum of packet lengths (with direction). They report success rates
that range between 90% and 93% for 100 regular sites. In addition,
they collected the largest and most realistic dataset of non-onion
service websites, including inner pages of websites and popular
−EntryTornetworkClienta.onionb.onionc.onionAdversarylinks extracted from Twitter. They conclude that website finger-
printing does not scale to such large dataset, as classification errors
increase with the size of the world.
k-Fingerprinting (k-FP) [11]: Hayes and Danezis’s k-FP attack
is based on Random Forests (RF). Random Forests are ensembles of
decision trees that are randomized and averaged to reduce overfit-
ting. In the open-world, they use the leafs of the random forest to
encode websites. This allows them to represent websites in function
of the outputs of the random forest, capturing the relative distance
to pages that individual trees have confused with the input page.
The instances extracted from the random forest are then fed into a
k-NN classifier for the actual classification. The study uses a set of
175 features that includes variations of features in the literature as
well as timing features such as the number of packets per second.
Hayes and Danezis evaluated the attack on a limited set of 30 onion
services and obtained 90% classification accuracy [11].
In the following subsection we provide an overview of prior
results on features that has inspired the feature selection made by
these three attacks.
2.3 Feature analysis for website fingerprinting
We consider two types of features: network-level and site-level fea-
tures. Network-level features are extracted from the stream of TCP
packets and are the typical features used in website fingerprinting
attacks. Site-level features are related to the web design of the site.
These features are not available in the network traffic meta-data,
but the adversary still has access to them by downloading the site.
Most website fingerprinting feature analyses have focused on
network-level features and have evaluated their relevance for a
specific classifier [5, 10, 22]. In particular, Hayes and Danezis [11]
perform an extensive feature analysis by compiling a comprehen-
sive list of features from the website fingerprinting literature as
well as designing new features. In order to evaluate the importance
of a feature and rank it, they used the random forest classifier on
which their attack is based.
Unlike prior work, our network-level feature analysis is classifier-
independent, as we measure the statistical variance of features
among instances of the same website (intra-class variance) and
among instances of different websites (inter-class variance).
2.4 Website fingerprinting defenses
Dyer et al. presented BuFLO, a defense that delays real messages
and adds dummy messages to make the traffic look constant-rate,
thus concealing the features that website fingerprinting attacks
exploit. They conclude that coarse-grained features such as page
load duration and total size are expensive to hide with BuFLO and
can still be used to distinguish websites [10].
There have been attempts to improve BuFLO and optimize the
padding at the end of the page download to hide the total size of the
page [4, 6]. These defenses however incur high latency overheads
that make them unsuitable for Tor. To avoid introducing delays,
a website fingerprinting defense based solely on adding dummy
messages was proposed by Juarez et al. [16]. These defenses aim at
crafting padding to obfuscate distinguishing features exploited by
the attack. Instead, we look at sites and examine what makes them
more or less fingerprintable.
There are defenses specifically designed for Tor that operate
at the application layer [8, 20, 23]. However, these defenses do
not account either for feature analyses that can help optimize the
defense strategy. Our study is the first to analyze the features at both
the website and network layers. Based on our results, we discuss
ways to reduce the fingerprintability of onion service sites and
inform the design of server and client-side website fingerprinting
defenses without requiring any changes to the Tor protocol itself.
3 DATA COLLECTION AND PROCESSING
We used the onion service list offered by ahmia.fi, a search engine
that indexes onion services. We first downloaded a list of 1,363
onion service websites and found that only 790 of them were online
using a shell script based on torsocks. We crawled the homepage
of the 790 online onion services.
Prior research on website fingerprinting collected traffic data by
grouping visits to pages into batches, visiting every page a number
of times each batch [15, 27]. All visits in a batch used the same
Tor instance but Tor was restarted and its profile wiped between
batches, so that visits from different batches would never use the
same circuit. The batches were used as cross-validation folds in the
evaluation of the classifier, as having instances collected under the
same circuit in both training and test sets gives an unfair advantage
to the attacker [15, 27].
In this study, we used the same methodology to collect data,
except that we restarted Tor on every visit to avoid using the same
circuit to download the same page multiple times. We ran the crawl
on a cloud based Linux machine from a data center in the US in
July 2016. The crawl took 14 days to complete which allowed us to
take several snapshots of each onion service in time.
We used Tor Browser version 6.0.1 in combination with Selenium
browser automation library 2. For each visit, we collected network
traffic, HTML source code of the landing page, and HTTP request-
response headers. We also saved a screenshot of each page.
We captured the network traffic traces using the dumpcap 3 com-
mand line tool. After each visit, we filtered out packets that were
not destined to the Tor guard node IP addresses. Before each visit,
we downloaded and processed the Tor network consensus with
Stem 4 to get the list of current guard IP addresses.
The HTML source code of the index page was retrieved using Se-
lenium’s page_source property. The source code and screenshots
are used to extract site-level features, detect connection errors and
duplicate sites. The HTTP requests and response headers are stored
using a custom Firefox browser add-on. The add-on intercepted all
HTTP requests, including the dynamically generated ones, using
the nsIObserverService of Firefox 5.
Finally, we collected the logs generated by Tor Browser binary
and Tor controller logs by redirecting Tor Browser’s process output
to a log file.
2http://docs.seleniumhq.org/
3https://www.wireshark.org/docs/man-pages/dumpcap.html
4https://stem.torproject.org/
5https://developer.mozilla.org/en/docs/Observer_Notifications#HTTP_requests
3.1 Processing crawl data
We ran several post-processing scripts to make sure the crawl data
was useful for analysis.
Remove offline sites. Analyzing the collected crawl data, we
removed 573 sites as they were found to be offline during the crawl.
Remove failed visits. We have also removed 14481 visits that
failed due to connection errors, possibly because some onion sites
have intermittent uptime and are reachable temporarily.
Outlier removal. We used Panchenko et al.’s outlier removal
strategy to exclude packet captures of uncommon sizes compared
to other visits to the same site [21]. This resulted in the removal of
5264 visits.
Duplicate removal. By comparing page title, screenshot and
source code of different onion services, we found that some onion
service websites are served on multiple .onion addresses. We elim-
inated 159 duplicate sites by removing all copies of the site but
one.
Threshold by instances per website. After removing outliers
and errored visits, we had an unequal number of instances across
different websites. Since the number of training instances can affect
classifier accuracy, we set all websites to have the same number of
instances. Most datasets in the literature have between 40 and 100
instances per website and several evaluations have shown that the
accuracy saturates after 40 instances [21, 27]. We set the threshold
at 70 instances which is within the range of number of instances
used in the prior work. Choosing a greater number of instances
would dramatically decrease the final number of websites in the
dataset. We removed 84 sites for not having a sufficient number of
instances and removed 9,344 extra instances.
Feature Extraction. Following the data sanitization steps out-
lined above, we extract features used by the three classifiers. Further,
we extract site level features using the HTML source, screenshot,
HTTP requests and responses. Site level features are explained in
Section 6.
In the end, the dataset we used had 70 instances for 482 different
onion sites.
4 ANALYSIS OF WEBSITE CLASSIFICATION
ERRORS
This section presents an in-depth analysis of the successes and
failures of the three state-of-the-art website fingerprinting methods.
This analysis helps identify which pages are the most fingerprint-
able and which are more likely to confuse the classifiers, giving
insight into the nature of the errors produced by the classifiers.
4.1 Classifier Accuracy
Even though the classification problem is not binary, we binarize
the problem by using a one-vs-rest binary problem for each site: a
True Positive (TP) is an instance that has been correctly classified
and False Positive (FP) and False Negative (FN) are both errors with
respect to a fixed site w; a FP is an instance of another site that
has been classified as w; a FN is an instance of w that has been
classified as another site.
In the closed world we measure the accuracy using the F1-Score
(F1). The F1-Score is a complete accuracy measure because it takes
into account both Recall (TPR) and Precision (PPV). More precisely,
the F1-Score is the harmonic mean of Precision and Recall: if either
is zero, the F1-Score is zero as well, and only when both achieve
their maximum value, the F1-Score does so too.
Note that there are the same total number of FPs and FNs, since a
FP of wy that actually belongs to wx is at the same time a FN of wx .
Thus, in the closed world the total F1-Score equals both Precision
and Recall. However, when we focus on a particular site, the FP
and FN for that site are not necessarily the same (see Table 2).
Table 1: Closed world classification results for our dataset of
482 onion services (33,740 instances in total).
k-NN CUMUL
k-FP
TPR 69.97%
30.03%
FPR
80.73%
19.27%
77.71%
22.29%
We have applied the classifiers to our dataset of 482 onion ser-
vices and evaluated the classification using 10-fold cross-validation.
Cross-validation is a standard statistical method to evaluate whether
the classifier generalizes for instances that it has not been trained
on. In most cases, ten is the recommended number of folds in the
machine learning literature and the standard in prior website fin-
gerprinting work. The results for each classifier are summarized in
Table 1 where we report the total number of TPs and FPs and the
average accuracy obtained in the 10-fold cross-validation. Thus, we
note that using TPR as an accuracy metric is sound in the closed
world but, in the open world, TPR is a partial measure of accuracy,
as it does not take into account Precision.
As we see in Table 1, while CUMUL and k-FP achieve similar
accuracies, the k-NN-based attack is the least accurate. Even though
these results are in line with other studies on website fingerprinting
for onion services [8], we found some discrepancies with other
evaluations in the literature. For 50 sites, Hayes and Danezis obtain
over 90% accuracy with k-FP [11], and Kwon et al. obtained 97%
with k-NN [17]. However, for the same number of sites and even
more instances per site, our evaluations of k-FP and k-NN only
achieve 80% maximum accuracy. Since our results show that some
sites are more fingerprintable than others, we believe the particular
choice of websites may account for this difference: we randomly
picked 50 sites from our set of 482 sites and even though Kwon et
al. also used onion URLs from ahmia.fi, they do not explain how
they picked the URLs for their evaluation.
4.2 Classifier Variance
In order to determine which features cause a site to be finger-
printable, we look into two types of sites: i) sites that are easy to
fingerprint, i.e., sites that consistently cause the least amount of
errors across all classifiers; and ii) sites that are difficult to finger-
print, namely sites that are most frequently misclassified across all
three classifiers. In the following sections, we compare the features
of these two types of sites and look for evidence that explains their
different degree of fingerprintability.
Table 2: The top five onion services by number of misclassi-
fication for each attack (repeating services in bold).
URL (.onion) TP
4
3
3
2
1
2
2
2
2
1
4
3
3
2
2
4fouc. . .
ykrxn. . .
wiki5k. . .
ezxjj. . .
newsi. . .
zehli. . .
4ewrw. . .
harry. . .
sqtlu. . .
yiy4k. . .
ykrxn. . .
t4is3. . .
wiki5. . .
jq77m. . .
newsi. . .
FP
84
62
77
76
87
15
29
29
35
14
62
42
55
54
63
FN
66
67
67
68
69
68
68
68
68
69
66