of cohorts used in the collection design must be carefully
selected and changed over time, to avoid privacy implica-
tions; otherwise, cohorts may be so small as to facilitate
the tracking of clients, or clients may report as part of dif-
ferent cohorts over time, which will reduce their privacy.
RAPPOR responses can even aﬀect client anonymity, when
they are collected on immutable client values that are the
same across all clients:
if the responses contain too many
bits (e.g., the Bloom ﬁlters are too large), this can facilitate
tracking clients, since the bits of the Permanent randomized
responses are correlated. Some of these concerns may not
apply in practice (e.g., tracking responses may be infeasi-
ble, because of encryption), but all must be considered in
RAPPOR collection design.
In particular, longitudinal privacy protection guaranteed
by the Permanent randomized response assumes that client’s
value does not change over time. It is only slightly violated if
the value changes very slowly. In a case of rapidly changing,
correlated stream of values from a single user, additional
measures must be taken to guarantee longitudinal privacy.
The practical way to implement this would be to budget ∞
over time, spending a small portion on each report. In the
RAPPOR algorithm this would be equivalent to letting q
get closer and closer to p with each collection event.
Because diﬀerential privacy deals with the worst-case sce-
nario, the uncertainty introduced by the Bloom ﬁlter does
not play any role in the calculation of its bounds. Depend-
ing on the random draw, there may or may not be multiple
candidate strings mapping to the same h bits in the Bloom
ﬁlter. For the average-case privacy analysis, however, Bloom
ﬁlter does provide additional privacy protection (a ﬂavor of
k-anonymity) because of the diﬃculty in reliably inferring a
client’s value v from its Bloom ﬁlter representation B [4].
7 Related Work
Data collection from clients in a way that preserves their
privacy and at the same time enables meaningful aggregate
inferences is an active area of research both in academia and
industry. Our work ﬁts into a category of recently-explored
problems where an untrusted aggregator wishes to learn the
“heavy hitters” in the clients’ data—or run certain types of
learning algorithms on the aggregated data—while guaran-
teeing the privacy of each contributing client, and, in some
cases, restricting the amount of client communication to the
untrusted aggregator [7, 16, 18, 20]. Our contribution is to
suggest an alternative to those already explored that is in-
tuitive, easy-to-implement, and potentially more suitable to
certain learning problems, and to provide a detailed statisti-
cal decoding methodology for our approach, as well as exper-
imental data on its performance. Furthermore, in addition
to guaranteeing diﬀerential privacy, we make explicit algo-
rithmic steps towards protection against linkability across
reports from the same user.
It is natural to ask why we built our mechanisms upon
randomized response, rather than upon two primitives most
commonly used to achieve diﬀerential privacy: the Laplace
and Exponential mechanisms [12, 21]. The Laplace mech-
anism is not suitable because the client’s reported values
may be categorical, rather than numeric, in which case di-
rect noise addition does not make semantic sense. The Ex-
ponential mechanism is not applicable due to our desire to
implement the system in a local model, where the privacy
is ensured by each client individually without a need for a
trusted third party. In that case, the client does not have
suﬃcient information about the data space in order to do
the necessary biased sampling required by the Exponential
mechanism. Finally, randomized response has the additional
beneﬁt of being relatively easy to explain to the end user,
making the reasoning about the algorithm used to ensure
privacy more accessible than other mechanisms implement-
ing diﬀerential privacy.
Usage of various dimensionality reduction techniques in
order to improve the privacy properties of algorithms while
retaining utility is also fairly common [1, 17, 20, 22]. Al-
though our reliance on Bloom ﬁlters is driven by a desire
to obtain a compact representation of the data in order to
lower each client’s potential transmission costs and the de-
sire to use technologies that are already widely adopted in
practice [6], the related work in this space with regards to
privacy may be a source for optimism as well [4]. It is con-
ceivable that through a careful selection of hash functions, or
choice of other Bloom ﬁlter parameters, it may be possible
to further raise privacy defenses against attackers, although
we have not explored that direction in much detail.
The work most similar to ours is by Mishra and San-
dler [24]. One of the main additional contributions of our
work is the more extensive decoding step, that provides both
experimental and statistical analyses of collected data for
queries that are more complex than those considered in their
work. The second distinction is our use of the second ran-
domization step, the Instantaneous randomized response, in
order to make the task of linking reports from a single user
diﬃcult, along with more detailed models of attackers’ ca-
pabilities.
The challenge of eliminating the need for a trusted aggre-
gator has also been approached with distributed solutions,
that place trust in other clients [11]. In this manner, dif-
ferentially private protocols can be implemented, over dis-
tributed user data, by relying on honest-but-curious proxies
or aggregators, bound by certain commitments [2, 8].
Several lines of work aim to address the question of lon-
gitudinal data collection with privacy. Some recent work of
considers scenarios when many predicate queries are asked
against the same dataset, and it uses an approach that,
rather than providing randomization for each answer sep-
arately, attempts to reconstruct the answer to some queries
based on the answers previously given to other queries [25].
The high-level idea of RAPPOR bears some resemblance
to this technique–the Instantaneous randomized response is
reusing the result of the Permanent randomized response
step. However, the overall goal is diﬀerent—rather than
answering a diverse number of queries, RAPPOR collects
reports to the same query over data that may be changing
over time. Although it does not operate under the same local
model as RAPPOR, recent work on pan-private streaming
and on privacy under continual observation introduces addi-
tional ideas relevant for the longitudinal data collection with
privacy [13, 14].
8 Summary
RAPPOR is a ﬂexible, mathematically rigorous and practi-
cal platform for anonymous data collection for the purposes
of privacy-preserving crowdsourcing of population statistics
on client-side data. RAPPOR gracefully handles multiple
data collections from the same client by providing well-deﬁned
longitudinal diﬀerential privacy guarantees. Highly tunable
parameters allow to balance risk versus utility over time,
depending on one’s needs and assessment of likelihood of
diﬀerent attack models. RAPPOR is purely a client-based
privacy solution. It eliminates the need for a trusted third-
party server and puts control over client’s data back into
their own hands.
Acknowledgements
The authors would like to thank our many colleagues at
Google and its Chrome team who have helped with this
work, with special thanks due to Steve Holte and Moti Yung.
Thanks also to the CCS reviewers, and many others who
have provided insightful feedback on the ideas, and this
paper, in particular, Frank McSherry, Arvind Narayanan,
Elaine Shi, and Adam D. Smith.
9 References
[1] C. C. Aggarwal and P. S. Yu. On privacy-preservation
of text and sparse binary data with sketches. In
Proceedings of the 2007 SIAM International Conference
on Data Mining (SDM), pages 57–67, 2007.
[2] I. E. Akkus, R. Chen, M. Hardt, P. Francis, and
J. Gehrke. Non-tracking web analytics. In Proceedings of
the 2012 ACM Conference on Computer and
Communications Security (CCS), pages 687–698, 2012.
[3] Y. Benjamini and Y. Hochberg. Controlling the false
discovery rate: A practical and powerful approach to
multiple testing. Journal of the Royal Statistical Society
Series B (Methodological), 57(1):289–300, 1995.
[4] G. Bianchi, L. Bracciale, and P. Loreti. ‘Better Than
Nothing’ privacy with Bloom ﬁlters: To what extent? In
Proceedings of the 2012 International Conference on
Privacy in Statistical Databases (PSD), pages 348–363,
2012.
[5] B. H. Bloom. Space/time trade-oﬀs in hash coding with
allowable errors. Commun. ACM, 13(7):422–426, July
1970.
[6] A. Z. Broder and M. Mitzenmacher. Network
applications of Bloom ﬁlters: A Survey. Internet
Mathematics, 1(4):485–509, 2003.
[7] T.-H. H. Chan, M. Li, E. Shi, and W. Xu. Diﬀerentially
private continual monitoring of heavy hitters from
distributed streams. In Proceedings of the 12th
International Conference on Privacy Enhancing
Technologies (PETS), pages 140–159, 2012.
[8] R. Chen, A. Reznichenko, P. Francis, and J. Gehrke.
Towards statistical queries over distributed private user
data. In Proceedings of the 9th USENIX Conference on
Networked Systems Design and Implementation (NSDI),
pages 169–182, 2012.
[10] C. Dwork. A ﬁrm foundation for private data analysis.
Commun. ACM, 54(1):86–95, Jan. 2011.
[11] C. Dwork, K. Kenthapadi, F. McSherry, I. Mironov,
and M. Naor. Our data, ourselves: Privacy via
distributed noise generation. In Proceedings of 25th
Annual International Conference on the Theory and
Applications of Cryptographic Techniques
(EUROCRYPT), pages 486–503, 2006.
[12] C. Dwork, F. McSherry, K. Nissim, and A. Smith.
Calibrating noise to sensitivity in private data analysis.
In Proceedings of the 3rd Theory of Cryptography
Conference (TCC), pages 265–284, 2006.
[13] C. Dwork, M. Naor, T. Pitassi, and G. N. Rothblum.
Diﬀerential privacy under continual observation. In
Proceedings of the 42nd ACM Symposium on Theory of
Computing (STOC), pages 715–724, 2010.
[14] C. Dwork, M. Naor, T. Pitassi, G. N. Rothblum, and
S. Yekhanin. Pan-private streaming algorithms. In
Proceedings of The 1st Symposium on Innovations in
Computer Science (ICS), pages 66–80, 2010.
[15] J. Hsu, M. Gaboardi, A. Haeberlen, S. Khanna,
A. Narayan, B. C. Pierce, and A. Roth. Diﬀerential
privacy: An economic method for choosing epsilon. In
Proceedings of 27th IEEE Computer Security
Foundations Symposium (CSF), 2014.
[16] J. Hsu, S. Khanna, and A. Roth. Distributed private
heavy hitters. In Proceedings of the 39th International
Colloquium Conference on Automata, Languages, and
Programming (ICALP) - Volume Part I, pages 461–472,
2012.
[17] K. Kenthapadi, A. Korolova, I. Mironov, and
N. Mishra. Privacy via the Johnson-Lindenstrauss
transform. Journal of Privacy and Conﬁdentiality,
5(1):39–71, 2013.
[18] D. Keren, G. Sagy, A. Abboud, D. Ben-David,
A. Schuster, I. Sharfman, and A. Deligiannakis.
Monitoring distributed, heterogeneous data streams:
The emergence of safe zones. In Proceedings of the 1st
International Conference on Applied Algorithms
(ICAA), pages 17–28, 2014.
[19] D. Kifer and A. Machanavajjhala. No free lunch in
data privacy. In Proceedings of the ACM SIGMOD
International Conference on Management of Data
(SIGMOD), pages 193–204, 2011.
[20] B. Liu, Y. Jiang, F. Sha, and R. Govindan.
Cloud-enabled privacy-preserving collaborative learning
for mobile sensing. In Proceedings of the 10th ACM
Conference on Embedded Network Sensor Systems
(SenSys), pages 57–70, 2012.
[9] Chromium.org. Design Documents: RAPPOR
(Randomized Aggregatable Privacy Preserving Ordinal
Responses). http://www.chromium.org/developers/
design-documents/rappor.
[21] F. McSherry and K. Talwar. Mechanism design via
diﬀerential privacy. In Proceedings of the 48th Annual
IEEE Symposium on Foundations of Computer Science
(FOCS), pages 94–103, 2007.
[22] D. J. Mir, S. Muthukrishnan, A. Nikolov, and R. N.
Wright. Pan-private algorithms via statistics on sketches.
In Proceedings of Symposium on Principles of Database
Systems (PODS), pages 37–48, 2011.
[23] I. Mironov. On signiﬁcance of the least signiﬁcant bits
for diﬀerential privacy. In Proceedings of ACM
Conference on Computer and Communications Security
(CCS), pages 650–661, 2012.
[24] N. Mishra and M. Sandler. Privacy via pseudorandom
sketches. In Proceedings of Symposium on Principles of
Database Systems (PODS), pages 143–152, 2006.
[25] A. Roth and T. Roughgarden. Interactive privacy via
the median mechanism. In Proceedings of the 42nd ACM
Symposium on Theory of Computing (STOC), pages
765–774, 2010.
[26] R. Tibshirani. Regression shrinkage and selection via
the Lasso. Journal of the Royal Statistical Society, Series
B, 58:267–288, 1994.
[27] S. L. Warner. Randomized response: A survey
technique for eliminating evasive answer bias. Journal of
the American Statistical Association, 60(309):pp. 63–69,
1965.
[28] Wikipedia. Randomized response.
http://en.wikipedia.org/wiki/Randomized_response.
APPENDIX
Observation 1
For a, b ≥ 0 and c, d > 0 : a+b
Proof. Assume wlog that a
c+d ≤ max( a
c , b
d ).
c ≥ b
d , and suppose the state-
c . Then ac + bc > ac + ad or
ment is false, i.e., a+b
bc > ad, a contradiction with assumption that a
c+d > a
c ≥ b
d .
Deriving Limits on Learning
We consider a Basic One-time RAPPOR algorithm to estab-
lish theoretical limits on what can be learned using a par-
ticular parameter conﬁguration and a number of collected
reports N . Since the Basic One-time RAPPOR is more ef-
ﬁcient (lossless) than the original RAPPOR, the following
provides a strict upper bound for all RAPPOR modiﬁca-
tions.
Decoding for the Basic RAPPOR is quite simple. Here,
we assume that f = 0. The expected number that bit i is
set in a set of reports, Ci, is given by
E(Ci) = qTi + p(N − Ti),
where Ti is the number of times bit i was truly set (was the
signal bit). This immediately provides the estimator
ˆTi =
Ci − pN
q − p
.
It can be shown that the variance of our estimator under
the assumption that Ti = 0 is given by
p(1 − p)N
(q − p)2 .
V ar( ˆTi) =
Determining whether Ti is larger than 0 comes down to
statistical hypothesis testing with H0 : Ti = 0 vs H1 : Ti >
0. Under the null hypothesis H0 and letting p = 0.5, the
standard deviation of Ti equals
sd( ˆTi) =
We reject H0 when
√
N
2q − 1
.
ˆTi > Q × sd( ˆTi)
√
Q
N
2q − 1
,
>
where Q is the critical value from the standard normal distri-
bution Q = Φ−1(1− 0.05
M ) (Φ−1 is the inverse of the standard
Normal cdf). Here, M is the number of tests; in this case,
it is equal to k, the length of the bit array. Dividing by M ,
the Bonferroni correction, is necessary to adjust for multiple
testing to avoid a large number of false positive ﬁndings.
Let x be the largest number of bits for which this condition
is true (i.e., rejecting the null hypothesis). x is maximized
when x out of M items have a uniform distribution and a
combined probability mass of almost 1. The other M − x
bits have essentially 0 probability. In this case, each non-
zero bit will have frequency 1/x and its expected count will
be E( ˆTi) = N/x ∀i.
Thus we require
√
Q
N
2q − 1
N
x
>
,
N
.
where solving for x gives
√
x ≤ (2q − 1)
Q