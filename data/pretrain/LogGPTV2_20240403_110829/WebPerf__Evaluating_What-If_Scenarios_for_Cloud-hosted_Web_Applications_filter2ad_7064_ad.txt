Given a node, Algorithm 1 recursively computes the latency
distribution for every path to it (by applying ProbAdd on
latency estimation of nodes on the path), and applies Prob-
Max or ProbMin to those paths, depending on whether the
paths have W henAny edges or not.
5. WebPerf EXTENSIONS
This section describes two extensions to WebPerf. We
have implemented them, and we evaluate them in §6.
5.1 End-to-end Latency Prediction
WebPerf’s cloud latency estimates can be extended to es-
timate end-to-end latency of a web request. End-to-end la-
tency consists of three components: (1) cloud-side latency
given by the prediction algorithm above, (2) network latency
between an application’s cloud frontend and the client, and
(3) client-side latency within client’s browser, which can be
predicted by tools such as WebProphet [25]. Accurately
modeling network latency is outside the scope of the pa-
per. For simplicity, we model network latency using the RTT
distribution between the client’s browser and the application
frontend. WebPerf uses a combination of two techniques to
estimate end-to-end latencies: end-to-end tracking and prob-
abilistic estimation.
End-to-end tracking. To match a client side webpage re-
266
Algorithm 2 END-TO-END LATENCY PREDICTION
1: INPUT: A webpage W and a latency vector ¯L of all objects in
W )
2: Function E2ELatencyPrediction (W, ¯L)
3: DG ← dependency graph of objects in W
4: S ← {}
5: while more sample to collect do
6:
7:
8:
for every object O in W do
9:
if O involves web request from cloud frontend then
ωN et ← a Monte Carlo sample from RTTDistribu-
tion
ωCloud ← a Monte Carlo sample from CloudLaten-
cyPredict() for O
ω ← ωCloud + ωN et
ω ← a Monte Carlo sample from FetchTime(O)
else
10:
11:
12:
13:
14:
15: return Distribution of S
OUTPUT: Predicted latency distribution of W
Update O’s latency in ¯L with ω
S ← S∪ ClientPrediction(DG, ¯L)
quest with corresponding HTTP requests seen by the cloud
frontend, WebPerf automatically instruments the webpage
so every HTTP request from it contains a unique id. WebPerf
also instruments the HTTP module at the webserver to match
the request with the cloud-side processing.
Probabilistic estimation. WebPerf uses Monte Carlo sim-
ulation to add cloud, network, and client latency distribu-
tions to produce an end-to-end latency distribution. It uses
a WebProphet-like tool called ClientPrediction that, given
a client-side dependency graph DGclient with causal depen-
dencies of various objects downloaded by the browser and
a latency vector ¯L of download latency of all objects in the
webpage, can estimate the page load time.
Algorithm 2 uses ClientPrediction for end-to-end predic-
tion as follows. It performs the following steps until a suf-
ﬁcient number of end-to-end latency samples are collected
(e.g., until the conﬁdence bound is within a target). For each
object O in the webpage, if it comes from a frontend HTTP
call, it samples a random value from the cloud-side latency
distribution and a random value from the network RTT dis-
tribution and adds them to estimate the download time for
O. On the other hand, if O comes from external sources
or if it is a static object, its download time is sampled from
a FetchTime distribution. (The FetchTime distribution is
proﬁled ofﬂine. If no such distribution is available for an ob-
ject, its value in ¯L can be used as the sampled value.) The
sampled download time is then updated in ¯L and passed to
ClientPrediction to produce a sample of end-to-end latency.
Finally, the algorithm returns the distribution of all samples.
5.2 Optimal Proﬁling
WebPerf’s primary bottleneck is estimating baseline la-
tencies of I/O calls appearing in the target workload (i.e.,
set of requests). Given a time budget T for measuring n re-
quests, one straightforward way is to allocate T /n time for
each request. However, this is not optimal since modeling
some APIs would need more samples than modeling others,
due to their different variabilities in latencies. WebPerf uses
an algorithm to decide how to allocate the time budget op-
timally to measure different APIs. The process is similar to
optimal experiments design [31], a statistical technique that
allows us to select the most useful training data points, and
has been used for performance prediction for large-scale ad-
vanced analytics [44].
Suppose the developer is interested in what-if analysis of
her application for n different requests: r1, r2, . . . , rn. Let
us ﬁrst start with the simpliﬁed assumption that each request
ri contains exactly one I/O call ci. We need to determine
ni, 1 ≤ i ≤ n, the number of times ri should be measured
to build a proﬁle of its I/O call ci. We have two goals. First,
the total measurement time3 for all requests must be within
a given budget T . Suppose executing request ri takes ti sec-
i=1 niti ≤ T . Second, the total (or average)
standard error of measurements is minimized. Suppose ex-
ecuting request ri takes ti seconds on average, with a stan-
√
dard deviation of σi. Then, the standard error of ni measure-
ments of ri is given by σi/
ni. Thus, we want to minimize
ni. In addition to the above two goals, we also
want each request to be measured at least k times, in order
to get meaningful statistics. The above problem can be for-
mulated as the following integer program:
onds; then(cid:80)n
(cid:80)n
i=1 σi/
√
(cid:88)
s.t. (cid:88)
min
1≤i≤n
1≤i≤n
σi√
ni
niti ≤ T, ni ≥ k
∀i = 1, . . . , n
ti
√
) 2
The problem is in NP, since a simpler version of the prob-
lem can be reduced to the Knapsack problem. We propose
a linear analytical approximation algorithm based on La-
grange multipliers [6] to solve it. We can derive the value
) 2
for each ni = (T ( σi
3 ), which is a real
2ti
number larger than 0. The intuition behind this is that we
need more samples for those APIs with higher variance but
take less time. We approximate them by rounding to closest
integers, which gives the optimal number of requests.
3 )/((cid:80)
i ( σi
We have generalized our solution [23] to the more realistic
setting where a request can contain multiple I/O calls and an
I/O call can appear in multiple requests.
6. EVALUATION
2
We have implemented WebPerf for Microsoft Azure web
apps, and we use this to evaluate WebPerf.
6.1 Methodology
Implementation. The WebPerf binary instrumenter is im-
plemented as a NuGet package [29] that a developer can ob-
tain from Azure Site Extension Gallery [4] and install to her
web app as a site extension. The Proﬁler is implemented as
a collection of web applications and the proﬁle dictionary
is stored in an Azure table. The WebPerf What-if Engine
is implemented as a cloud service with a web interface that
allows the developer to specify her web app, workload, and
what-if scenarios. The engine communicates with the Instru-
menter (included in the target web app by the developer) and
the Proﬁler through HTTP requests. The WebPerf prediction
component is written in python (∼ 20K LOC).
3The problem can also be formulated with other costs.
267
Experimental Setup. We have evaluated WebPerf with six
third party Azure web applications, listed in Table 3, for
which we could ﬁnd cloud-side source code or binaries.
The key application for which we provide detailed results
is SocialForum,4 a production-ready Microsoft web appli-
cation. It provides Instagram-like social network functional-
ity and allows users to create new accounts, create/join fo-
rums, post/share/tag/search pictures and comments, etc. It
uses ﬁve different Azure services: Azure Blob Storage for
storing large data such as images; Azure Table for storing
relational data; Azure Redis Cache for caching and storing
key-value pairs; Azure Service Bus for queueing background
processing tasks and Azure Search Service for searching fo-
rums. The index page of the SocialForum website consists
of more than 20 objects, and the corresponding HTTP re-
quest at the cloud side has a dependency graph consisting of
116 async I/O calls to Redis Cache, Table storage, and Blob
storage, with many executing in parallel.
For lack of space we omit the detailed architecture of re-
maining ﬁve applications in Table 3; these are discussed in
[23]. They all are of modest complexity, as hinted by various
Azure resources they use. The requests that we use to all six
applications are also fairly complex, with large dependency
graphs. On average, a dependency graph has 182 nodes, 180
async-await edges, and 62 synchronization points.
6.2 Cloud Latency Prediction
We ﬁrst evaluate accuracy of WebPerf’s predicted cloud
latency under six what-if scenarios. For each scenario and
for each application, WebPerf predicts a distribution of the
cloud latency of loading the index page. To quantify the
accuracy of WebPerf’s prediction, we compare predicted
latency distribution with ground truth latency distribution
measured by actually deploying the applications under tar-
get what-if conﬁgurations. Given predicted and ground
truth latency CDFs F1 and F2, we compute the relative
error distribution—the distribution of vertical deviations
(|F1(x) − F2(x)|) of two CDFs, and report statistics such
as maximum, mean, and median of the deviations.
(Note
that the maximum of the relative error distribution is the D
statistic used for Kolmogorov–Smirnov test for comparing
two distributions.) Ideally, the relative error statistics should
be close to zero.
We present detailed results only for SocialForum; results
for other applications are summarized in Figure 12. Unless
indicated otherwise, SocialForum is deployed in an Azure
US West datacenter and clients load its speciﬁed page in a
Chrome browser from California.
Scenario 1: What-if the Redis cache is upgraded from
the original Standard C0 tier to Standard C2 tier? Fig-
ure 11(a) shows distributions of original latency (with C0
Redis), ground truth latency (with C2 Redis), and predicted
latency (for C2 Redis). The result shows that upgrading Re-
dis from C0 to C2 signiﬁcantly changes cloud latency, and
hence simply using the baseline performance of C0 tier as
4A pseudonym for the actual product.
Application
SocialForum
SmartStore.Net [39]
Description
A production-ready Microsoft application that provides Instagram-like
social functionalities and allows users to create new accounts, create/join
forums, post/share/tag/search pictures and comments, etc.
An open source e-commerce solution that includes all essential features to
easily create a complete online shopping website. It offers a rich set of
features to handle products, customers, orders, payments, inventory,
discounts, coupons, newsletter, blogs, news, boards and much more.
A classiﬁed advertisement website, similar to craigslist.org
ContosoAds [12]
EmailSubscriber [15] An email subscription service, similar to mailchimp.com, that allows
ContactManager [11] An online contact management web application, similar to
users to subscribe, unsubscribe, and send mass emails to mailing lists
CourseManager [13]
zoho.com/contactmanager, that allows users organize to contacts
A course management website, similar to coursera.org, that allows
instructor course creation, student admission and homework assignments.
Azure services used
Blob storage, Redis cache, Service
bus, Search, Table
SQL
Blob storage, Queue, SQL, Search
Blob storage, Queue, Table
Blob storage, SQL
Blob storage, SQL
Table 3— Third party applications used in our case studies and the Azure services they talk to.
a prediction for the new tier will be inaccurate. As shown,
WebPerf’s prediction is very accurate: median, average, and
maximum relative errors are 0.8% , 2.7% , and 18.3% re-
spectively. We also used WebPerf to predict performance
for two additional scenarios: upgrading Redis tier from C0
to C6 and upgrading the front-end web server tier from A1
to A3. The median relative errors of predictions for these
two scenarios are 0.8% and 1.7% respectively.
Scenario 2: What-if the front-end of SocialForum is repli-
cated to two locations: US East and Asia East? The backend
still remains at US West. We conﬁgured WebPerf’s proﬁle
dictionary with latency models for SocialForum’s backend
APIs when frontend and backend are deployed in the same
datacenter (e.g., US West). The models were then added
with RTT distribution between backend and new frontend
location (e.g., US East). This helped us avoid proﬁling la-
tency models for all possible combinations of frontend and
backend locations. WebPerf’s end-to-end prediction is quite
accurate for both the locations, with median, mean, and max-
imum relative errors < 4%, < 2%, and < 15% respectively.
Scenario 3: What-if SocialForum’s reads/writes data of size
X from/to blob storage? We used X = 14KB, 134KB,
6.8MB, 12MB and 20MB. We conﬁgured WebPerf with of-
ﬂine proﬁles of blob storage API latencies for contents of
different sizes. Figure 11(c) shows the CDFs of predicted
and ground truth latency distributions for X = 6.8M B. The
median errors for all values of X are below 9%.
Scenario 4: What-if other collocated applications interfere
with SocialForum? Azure does not guarantee performance
isolation for free tiers. We deployed SocialForum in a free
tier and let other collocated applications create CPU pres-
sure, using CPU loads of 10%, 20%, . . . , 80%. The median
relative errors for all the scenarios were < 9%.
Scenario 5: What-if N users concurrently load SocialFo-
rum webpage? For this scenario, we conﬁgured WebPerf
with proﬁles for API latencies for n concurrent API calls, for
different values of n. WebPerf uses the dependency graph
of the web request to determine nc, the maximum number
of concurrent execution of a I/O call c during each web re-
quest. Then, WebPerf uses c’s latency proﬁle under N × nc
concurrent calls. We used WebPerf to predict latency under
N = 10, 20, 30, 40, 50, and 60 concurrent requests. Fig-
ure 11(d) shows the CDFs of distributions of ground truth
and predicted cloud latencies for 30 concurrent requests. For
all values of N, median prediction errors were < 10%. end-
to-end median prediction errors were < 8%.
Scenario 6: What-if a replicated frontend fails? For this
scenario, we replicated SocialForum’s front end on two web
servers and placed them behind a load balancer. We then
used WebPerf to predict cloud latencies if one of the web
servers dies. WebPerf assumes that when one web server
fails, all user requests are routed to the live web server and
hence its load effectively doubles. Thus, WebPerf predicts
response times under a 2× concurrent user requests (similar
to the last scenario). We conducted the experiments with 10,
30, and 60 concurrent user requests. In all the cases, median
relative prediction error was < 9%.
Other applications: We conducted the above predictions
for all the applications in Table 3. For consistency, we used
the same set of scenarios across apps.
In Scenario 1, we
upgraded their frontends from the lowest tier to a mid tier.
In Scenario 2, we replicated the frontend to Asia East. In
Scenario 3, front-end retrieves 6MB data from backend blob
storage. In Scenario 4, background CPU load is 70%. Sce-
nario 5 uses 30 concurrent connections. Scenario 6 uses 60
concurrent connections and one of the two frontend fails.
Figure 12(a) shows the median relative prediction errors
for all applications and scenarios. Overall, errors are small
(< 7%), indicating that WebPerf is able to predict cloud la-
tencies of a wide range of applications under the what-if sce-
narios we considered.
6.3 End-to-end Latency Prediction
We now evaluate how well WebPerf predicts end-to-end
latency (§ 5.1). We used all applications and scenarios
used for cloud latency prediction above. To quantify pre-
diction error, we obtained ground truth end-to-end latencies
by accessing the index page of the applications in a Chrome
browser from California. The cloud application was hosted
in an Azure US West datacenter, and we used the network
RTT distribution between the client and server as the net-
work latency model. The relative error of the predictions
(Figure 12(b)) for all applications and scenarios is, overall,