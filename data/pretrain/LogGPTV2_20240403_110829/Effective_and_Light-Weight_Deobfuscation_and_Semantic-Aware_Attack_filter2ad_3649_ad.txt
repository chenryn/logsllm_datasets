try to match the pre-trained OOA rules. The results cannot only
show the malicious scores but also the semantics of the scripts.
6 EVALUATION
6.1 Evaluation Methodology
In this paper, we first evaluate the performance of our subtree-based
deobfuscation, which is divided into three parts. First, we evaluate
Figure 8: Semantic-aware detection workflow
5 SEMANTIC-AWARE POWERSHELL
ATTACK DETECTION
Semantic-aware detection has many advantages over signature-
based detection. Among all, the most significant one is that semantic-
aware detection is hard to be evaded by polymorphic variants. Be-
sides more robust attack detection, it also allows explanations and
classifications of the malicious behaviors, which is highly desired
in malware analysis and forensics [29].
For binary program analysis, researchers usually use several
kinds of graphs, e.g., control-flow graphs [22] and dependency
graphs [28], instead of API sets to represent semantics. This is
because APIs used in the binary program only contain low-level
semantics and thus can be ambiguous. In contrast, as shown in
Table 3, APIs in the PowerShell language contain a higher level of
semantics, and thus semantics of PowerShell scripts can already be
understood easily by commands and functions sets. Considering
that API sets can be processed much more efficiently compared to
graphs, in our system design we adopt API sets instead of graphs
for PowerShell semantics detection.
As shown in Figure 8, our detection system can be divided into
two phases: training and detection, which are detailed below.
5.1 Training Phase
First, using the parser described in §4.2, we can get a set of AST
nodes corresponding to each deobfuscated script. As discussed in
§4.2, only several kinds of nodes, such as InvokeMemberExpression-
Ast, CommandAst, etc, need to be considered. Then we extract their
values and normalize them. Our normalization includes: (1) con-
verting to lowercase, (2) deleting irrelevant characters, (3) check-
ing alias. For example, for a script that downloads a program and
launches it, the following set can be extracted: {’new-object’,
’downloadfile’, ’start-process’}.
Objective-oriented Association Mining We employ an clas-
sic classification based on OOA mining [68] on itemsets of com-
mands for detection. The OOA mines association frequency pat-
terns that are specifically related to a pre-defined objective. Those
frequent patterns are called OOA rules and carry the underlying
semantics of the data.
DeobfuscatedScripts- z, r- z, x, y, s, t- x, s, r- z, x, y, s, t- z, x, y, r, tParserZ:5-r:1x:3y:3s:2t:2r:1t:1x:1s:1r:1FP-treeCommand querydatabase- z, r- z, x, y, t- x, s- x, r- ...Calculateconfidenceand Support- z, rOOA rulesdatabaseMatchingParserDetectionReportDetection phaseTraining phaseFrequentitemsetswhether we can find the minimum subtrees involved in obfusca-
tion, which can directly determine the quality of the deobfuscation.
This is dependent on the classifier and thus we cross-validate the
classifier with manually-labelled ground truth. Second, we verify
the quality of the entire obfuscation by comparing the similar-
ity between the deobfuscated scripts and the original scripts. In
this evaluation, we modify the AST-based similarity calculation
algorithm provided by [39]. Third, we evaluate the efficiency of de-
obfuscation by calculating the average time required to deobfuscate
scripts obfuscated by different obfuscation methods.
Next, we evaluate the benefit of our deobfuscation method on
PowerShell attack detection. In §2, we find that obfuscation can
evade most of the existing anti-virus engine. In this section, we
compare the detection results for the same PowerShell scripts before
and after applying our deobfuscation method. In addition, we also
evaluate the effectiveness of the semantic-based detection algorithm
in Section 5.
6.1.1 PowerShell Sample Collection. To evaluate our system, we
create a collection of malicious and benign, obfuscated and non-
obfuscated PowerShell samples. We attempt to cover all possible
download sources that can have PowerShell scripts, e.g., GitHub,
security blogs, open-source PowerShell attack repositories, etc.,
instead of intentionally making selections among them.
Benign Samples: To collect benign PowerShell Scripts, we down-
load the top 500 repositories on GitHub under PowerShell language
type using Chrome add-on Web Scraper [12]. We then find out the
ones with PowerShell extension ’.ps1’ and manually check them
one by one to remove attacking modules. After this process, 2342
benign samples are collected in total.
Malicious Samples: The malicious scripts we use to evaluate de-
tection are based on recovered scripts which consist of two parts.
1) 4098 unique real-world attack samples collected from security
blogs and attack analysis white papers [55]. Limited by the method
of data collection, the semantics of the samples are relatively simple.
Most of the samples belong to the initialization or execution phase.
2) To enrich the collection of malicious scripts, we pick other 43
samples from 3 famous open source attack repositories, namely,
PowerSploit [9], PowerShell Empire [1] and PowerShell-RAT [43].
Obfuscated Samples: In addition to the collected real-world ma-
licious samples, which are already obfuscated, we also construct
obfuscated samples through the combination of obfuscation meth-
ods and non-obfuscated scripts. More specifically, we deploy four
kinds of obfuscation methods in Invoke-Obfuscation, mentioned in
§2.3, namely, token-based, string-based, hex-encoding and security
string-encoding on 2342 benign samples and 75 malicious. After
this step, a total of 9968 obfuscated samples are generated.
Script Similarity Comparison. Deobfuscation can be regarded
6.1.2
as the reverse process of obfuscation. In the ideal case, deobfuscated
scripts should be exactly the same as the original ones. However, in
practice, it is difficult to achieve such perfect recovery for various
reasons. However, the similarity between the recovered script and
the original script is still a good indicator to evaluate the overall
recovery effect.
To measure the similarity of scripts, we adopt the methods of
code clone detection. This problem is widely studied in the past
decades [50]. Different clone granularity levels apply to different
intermediate source representations. Match detection algorithms
are a critical issue in the clone detection process. After a source
code representation is decided, a carefully selected match detec-
tion algorithm is applied to the units of source code representa-
tion. We employ suffix tree matching based on ASTs [40]. Both
the suffix tree and AST are widely used in similarity calculation.
Moreover, such combination can be used to distinguish three types
of clones, namely, Type 1(Exact Clones), Type 2(Renamed Clones),
Type 3(Near Miss Clones), which fits well for our situation.
To this end, we parse each PowerShell script into an AST. Most
of the code clone detection algorithm is line-based. However, lines
wrapping is not reliable after obfuscation. We utilize subtrees in-
stead of lines. We serialize the subtree by pre-order traversal and
apply suffix tree works on sequences. Therefore, each subtree in one
script is compared to each subtree in the other script. The similarity
between the two subtrees is computed by the following formula:
n = 2 × s/(2 × s + l + r).
where s represents the number of shared nodes, l stands for the
number of different nodes in subtree 1, and r represents the number
of different nodes in subtree 2.
We only take subtree pairs with similarity greater than 0.7. To
avoid repeatedly counting, once one subtree is picked, its ancestor
nodes are ignored. Besides, to avoid coincidence, subtrees with
fewer than 7 nodes will not be considered. Finally, the similarity
scores between two scripts are calculated by the following formula:
N = 2 × S/(2 × S + L + R).
where S is the summary of s, L represents the number of different
nodes in tree 1, R stands for the number of different nodes in tree 2.
Detailed pseudo code for calculating similarity can be found in §A.
6.2 Evaluation Results
In this section, we evaluate the effectiveness and efficiency of our
approach using the collected PowerShell samples described earlier
(§6.1.1). The experiment results are obtained using a PC with In-
tel Core i5-7400 Processor 3.5 GHz, 4 Cores, and 16 Gigabytes of
memory, running Windows 10 64-bit Professional.
6.2.1 Obfuscation Detection Accuracy. Accurate localization of ob-
fuscated script pieces is a prerequisite for our deobfuscation. For
obfuscation detection, we apply a logistic regression with gradient
descent binary classifier based on three levels of features mentioned
in §4.3. To train the classifier, we manually select 1250 subtrees
as obfuscated samples (500 from token-based obfuscated samples,
250 from each of the other three obfuscated samples). As for the
unobfuscated samples, we randomly pick the same number of sub-
trees whose root are PipelineAst from unobfuscated scripts. All
subtrees mentioned above are selected from 250 original scripts
and corresponding 1000 obfuscated samples. The remaining 2167
original scripts and corresponding 8668 obfuscated samples are
selected as the testing set.
As a comparison, PSDEM [41] uses a series of regular expressions
combined with some syntactic information to locate obfuscated
script pieces. For example, to identify the $StrOrder in Figure 1,
PSDEM will extract the following regex: "-f operator" in Figure
1, which is a common string operation widely used for obfuscation.
Among the four obfuscation schemes targeted in our approach, PS-
DEM can cover S1 and S2 using "-f operator" and "replace()",
but cannot cover S3 and S4.
Table 4: The accuracy of obfuscation detection
Obfuscation detection approachs
Our approach
PSDEM [41]
TPR
100%
49.9%
FPR
1.8%
22.2%
Results. We apply both our subtree classifier and PSDEM’s regex
on the testing set. As long as there is one match for one script, we
regard it as a obfuscated case. To improve our obfuscation detection
performance, we also employ emulation result to check the detec-
tion result as mentioned in §4. The results are shown in table 4. As
shown, our approach can achieve 100% true-positive rate(TPR) with
false-positive rate as low as 1.8% on the testing set. In comparison,
PSDEM only has 49.9% true-positive rate(FPR) since it fails on all
the samples obfuscated with S3 and S4. At the same time, it has a
22.2% false positive rate, which is much higher than our approach.
Based on the results, we find that this is mainly due to that the
regexes can only be used to locate functions commonly used in
obfuscation but not to determine whether the functions are used
for obfuscation or a regular scenario, which indicates the inherent
limitation of regex based obfuscation detection.
6.2.2 Recovery Quality. Next, we evaluate the overall recovery
quality by comparing the similarity between obfuscated sample
scripts and original ones before and after deobfuscation using the
methodology described in §6.1.2. In this experiment, we use all
obfuscated samples in the training set mentioned above. The results
are shown in Table 5.
Table 5: The average similarities of deobfuscated and origi-
nal ASTs
Obfuscation schemes
Obfuscated
S1
S2
S3
S4
Overall
1.8%
0.1%
0.01%
0.004%
0.5%
Deobfuscated
(our approach)
71.5%
79.0%
82.9%
85.2%
79.7%
Deobfuscated
(PSDEM[41])
70.6%
79.5%
0.01%
0.004%
37.5%
Results. As shown, after deobfuscation using our approach,
the average similarity increases significantly from 0.5% to 79.8%,
which is about 160 times higher. Among all, the similarities for
scripts recovered from S2, S3 and S4 are higher that those for S1.
This is because these three schemes are script block-based, which
can completely preserve the structure inside the script block after
deobfuscation and thus achieve higher similarity scores. Note that,
as indicated by the similarity scores, the deobfuscated scripts are not
exactly the same as the original ones. This is mainly because syntax-
level changes in the obfuscation processes, e.g., using variables to
save intermediate values, thus does not affect the semantics-aware
attack detection and understanding of functionality as shown later
in §6.2.4. A real-world sample is analyzed in appendix §B.
In comparison, for S1 and S2, which PSDEM can cover, PSDEM
has a similar recovery quality with our approach. However, for ob-
fuscation techniques that PSDEM cannot support, there is nothing
PSDEM can do. Moreover, PSDEM does not provide an automatic
Figure 9: Average deobfuscation time for obfuscated scripts.