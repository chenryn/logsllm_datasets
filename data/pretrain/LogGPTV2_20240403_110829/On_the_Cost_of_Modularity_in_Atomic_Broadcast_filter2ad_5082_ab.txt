q and r decide to start another consensus. In other words,
process p may send its proposal for consensus instance k+1
just after having sent the decision of consensus instance k.
In the monolithic implementation the successive consen-
sus instances are run within the atomic broadcast module.
If consensus instance k decides in the ﬁrst round (which is
the case in good runs), then the coordinator of consensus in-
stances k and k + 1 (in its ﬁrst round) are the same process.
In this case, the decision of consensus k and the proposal
of consensus k + 1 are sent together as a single message
(denoted “proposal k + decision k − 1” in Figure 6).
This ﬁrst optimization in the monolithic atomic broad-
cast stack allows a better use of network resources: instead
of sending one small message (tag DECISION) followed by
a larger message (the proposal of consensus k +1) the small
message is piggybacked on the larger one.
4.2 Piggybacking Messages Abcast on ack
Messages
In the modular implementation of atomic broadcast, a
process abcasting a message m starts by sending m to all
other processes. Whenever this message is received, it is
added to the set of proposed messages for the next consen-
sus instance. In good runs, this is inefﬁcient for the follow-
ing reason: every process delivers m, but only the coordina-
tor of the next consensus execution actually needs m (in or-
pqrtimeconsensus(m)adeliver(m’)abcast(m)diffuseadeliver(m’)adeliver(m’)order(m’)abcast(m’)adeliver(m)adeliver(m)adeliver(m)Consensus KConsensus K+1timeqrp(coord)proposalackdecisionproposalackdecision37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 20074.3 Reducing the Message Complexity of
Reliable Broadcast
Consensus decisions have to be reliably broadcast to
all processes. In the modular implementation, the reliable
2 c) messages
broadcast algorithm requires (n − 1) · (b n+1
to be sent on the network for each reliable broadcast to n
processes.
In the monolithic implementation, the cost of the deci-
sion diffusion is reduced to n messages: the decision is sim-
ply sent to all processes without any additional retransmis-
sions (in good runs). The reduction relies on the knowledge
that the successive consensus instances are executed on the
same set Π of processes (and thus, messages in consensus
k + 1 can serve as acknowledgments for messages sent in
consensus k). With this knowledge, the decision of consen-
sus execution k is acknowledged by the messages sent by
non-coordinators to the coordinator in consensus execution
k + 1.
since it considerably reduces from (n − 1) · (b n+1
the number of messages sent by reliable broadcast.
Again, this optimization reduces the network congestion
2 c) to n
5 Performance Evaluation
We now evaluate and compare the performance of our
two (optimized) implementations of atomic broadcast. We
speciﬁcally focus on the case of a system with three and
seven processes, supporting one, respectively three, fail-
ures. This system size might seem small. However, atomic
broadcast is usually used for relatively small degrees of
replication.
If a a larger degree of replication is needed,
then alternatives that provide weaker consistency should be
considered [1].
The section starts by presenting the parameters consid-
ered. An analytical evaluation of the two implementations
is then presented, followed by the experimental evaluation
of these implementations.
5.1 Metrics, Workload, Faultload
The following paragraphs describe the benchmarks (i.e.
the performance metrics and the workloads) that were used
to evaluate the performance of the atomic broadcast algo-
rithms.
Performance Metrics. We use two performance metrics
to evaluate the algorithms: early latency and throughput.
For a single abcast message, the early latency L is deﬁned
as follows. Let t0 be the time at which the abcast(m) event
completes and let ti be the time at which adeliver(m) oc-
curs on process pi, with i ∈ 1, . . . , n. The early latency L
is then deﬁned as L
def= (mini ti) − t0.
Figure 6. Consecutive consensus executions
in the monolithic implementation of atomic
broadcast.
der to propose m for consensus). This can not be optimized
in a modular stack, since the atomic broadcast module can-
not access information that is speciﬁc to the consensus mod-
ule (such as the identity of the coordinator). Furthermore, to
preserve modularity, atomic broadcast can not disseminate
messages abcast by the application within consensus mes-
sages. This is shown in Figure 7: messages m and m0 are
ﬁrst sent (in the diffuse step), then consensus is executed (in
the order step).
A more efﬁcient solution, which can only be imple-
mented in the monolithic stack, is to combine ack messages
with the sending of messages m and m0 (see solid arrows in
Fig. 7). This is done as follows. The sender of m directly
sends m to the (initial) coordinator of the next consensus ex-
ecution. Furthermore, instead of sending m as a standalone
message to the coordinator, it can be piggybacked on the
ack message of the consensus algorithm (denoted “ack +
diffusion” in Figure 6). If the coordinator changes (i.e. if a
suspicion occurs), message m is again piggybacked on the
estimate sent to the new coordinator.
The gain of this optimization is twofold. Firstly, it re-
duces network congestion by avoiding an unnecessary dif-
fusion of abcast messages to all processes: messages are
only sent to the coordinator. Secondly, similarly to the
ﬁrst optimization presented above, it allows a more efﬁcient
use of network resources thanks to the aggregation of small
messages with bigger ones.
Figure 7. Diffusion of two messages m and
m0, followed by their ordering. The diffu-
sion and ordering steps cannot be merged in
the modular implementation of atomic broad-
cast.
timeqrp(coord)proposal k+decision k-1ack+diffusion(            ,[m])ack+diffusionproposal k+2+decision k+1abcast(m)abcast(m’)abcast(m’’)(ack,[m’])(ack,[m’’])(            ,[m’, m’’])adeliver(m)adeliver(m)adeliver(m)proposal k+1+decision k(ack,[])(ack,[])adeliver(m’)adeliver(m’’)adeliver(m’)adeliver(m’’)adeliver(m’)adeliver(m’’)(            ,[])DECISIONDECISIONDECISIONpqrtimersend(m)adeliver(m’)abcast(m)diffuseadeliver(m’)adeliver(m’)orderrsend(m’)abcast(m’)adeliver(m)adeliver(m)adeliver(m)proposalackdecision37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007The throughput T is deﬁned as follows. Let ri be
the rate at which adeliver events occur on a process pi,
with i ∈ 1, . . . , n. The throughput T is then deﬁned as
T def= 1
i=1 ri and is expressed in messages per second
n
(or msgs/s).
Pn
In our performance evaluation, the mean for L and T is
computed over many messages and for several executions.
For all results, we show 95% conﬁdence intervals.
Workloads and Faultload. The early latency L and the
throughput T are measured for a certain workload, which
speciﬁes how many abcast events are generated per time
unit. We chose a simple symmetric workload where all pro-
cesses abcast messages of a ﬁxed size s at a constant rate r
(with s and r varying from experiment to experiment). The
global rate of atomic broadcast events is called the offered
load Toﬀered, which is expressed in messages per second.
We then evaluate the dependency between, on one hand,
the early latency L and the throughput T and, on the other
hand, the offered load Toﬀered and the size of the messages.
Furthermore, both implementations of the atomic broad-
cast protocol use the same ﬂow-control mechanism that
blocks further abcast events when necessary. More pre-
cisely, the ﬂow-control mechanism ensures that, on aver-
age, M = 4 messages are ordered per consensus execution.
This value of M optimizes performance of both modular
and monolithic stacks. We ensure that the system stays in
a stationary state by verifying that the latencies of all pro-
cesses stabilize over time.
Finally, we only evaluate the performance of the algo-
rithms in good runs, i.e. without any process failures or
wrong suspicions. The latency and throughput of the im-
plementations is measured once the system has reached a
stationary state (at a sufﬁciently long time after the startup).
The parameters that inﬂuence the latency and the through-
put are n (the number of processes), the implementation
(modular or monolithic) the offered load Toﬀered and the
size of the messages that are abcast.
5.2 Analytical Evaluation
As shown in Section 3, the Chandra-Toueg atomic broad-
cast algorithm reduces to a sequence of consensus execu-
tions. We assume a workload high enough so that consensus
execution k + 1 starts directly after execution k.4 This con-
dition is met if the offered load Tof f ered is greater than the
number of consensus executions that the system can execute
per second (i.e., if d is the average duration of a consensus
execution, we have T > d−1).
We now analyze two aspects of the performance of the
two implementations: (1) the number of messages that are
4Otherwise, there is no point in optimizing the algorithms.
sent and (2) the total amount of data that needs to be sent to
solve atomic broadcast.
5.2.1 Number of Sent Messages
In both the modular and monolithic implementations of
atomic broadcast, sets of unordered abcast messages are or-
dered in consensus executions. We assume that, on average,
M messages are ordered per consensus execution. In the
experimental evaluation, this is ensured by our ﬂow-control
mechanism. We now derive the number of messages that
need to be sent in both stacks in order to adeliver these M
messages.
Modular Implementation.
In the modular implementa-
tion of atomic broadcast, the M unordered messages are
ﬁrst sent to all processes in the system, generating M · (n−
1) messages on the network. These messages are then re-
ceived by the coordinator of the consensus algorithm that
sends a proposal to all processes (n − 1 messages). All
processes reply by sending an ack message to the coordi-
nator (n − 1 messages), which then reliably broadcasts the
decision to all processes (which necessitates an additional
(n − 1) · b n+1
To adeliver the M abcast messages, the modular imple-
2 c)
mentation thus needs to send (n − 1)(M + 2 + b n+1
messages.
2 c messages).
Monolithic Implementation.
In the monolithic imple-
mentation of atomic broadcast, the M unordered messages
are not immediately sent to all processes. Instead, they are
piggybacked on the ack messages of the previous consensus
execution. The coordinator starts the consensus execution
by sending both the decision of the previous consensus and
a new proposal in the same message. This message is sent
to all processes (n − 1 messages). The other processes then
reply by sending an ack message to the coordinator (n − 1
messages).
To adeliver the M abcast messages, the monolithic im-
plementation thus only needs to send 2 · (n − 1) messages.
In the case of a system of n = 3 processes for example,
with an average of M = 4 messages ordered per consensus
execution5, this means that the monolithic implementation
needs 4 messages to order these 4 abcast messages (assum-
ing of course that a previous consensus execution allows
some piggybacking of messages). In the case of the modu-
lar stack, 16 messages are needed for the same result.
5This value of M corresponds to the one that we chose for our experi-
mental evaluation.
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 20075.2.2 Total Amount of Sent Data
We assume that abcast messages all have a size of l bytes.
We further assume that messages sent by the algorithm that
have a constant size (e.g. ack messages and tag DECISION
in the modular implementation) only represent a negligible
part of the sent data. As above, we analyze how much data
is sent on average per consensus execution (i.e., to adeliver
M abcast messages).
Modular Implementation.
In the modular implementa-
tion, abcast messages are sent to all other processes. The
M messages of size l are thus sent to n − 1 processes. The
coordinator then adds these messages to a consensus pro-
posal (sent to the n − 1 non-coordinator processes) which
thus has a size of M · l on average. The total amount of
data exchanged per consensus in the modular stack is then
Datamod = 2(n − 1)M · l bytes.
Monolithic Implementation.
In the monolithic imple-
mentation, the processes do not diffuse their abcast message
to everyone and instead only send them to the coordinator
(by piggybacking them on ack messages). On average, M
n
abcast messages of size l are piggybacked by each one of
the n− 1 non-coordinator processes during a consensus ex-
ecution. The coordinator then creates a proposal with the M
messages ( M
n abcast
by the other processes) of size l that is sent to the n−1 other
processes. The total amount of data sent per consensus exe-
cution is thus on average Datamono = (n− 1)(1 + 1
n)M · l
bytes.
The overhead of the modular implementation with re-
n messages abcast by itself and (n−1) M
spect to the monolithic implementation is therefore
overhead =
Datamod − Datamono
Datamono
= n − 1
n + 1
In a system with n = 3 processes, the modular implementa-
tion needs to send 50% more data than the monolithic one.
In the case of n = 7, the overhead reaches 75%.
5.3 Experimental Evaluation
The paragraphs above presented an analytical evaluation
of the two atomic broadcast implementations from the per-
spective of two performance aspects. These two aspects are
however not sufﬁcient to completely characterize the per-
formance cost of the modular implementation versus the
monolithic one. Indeed, the analysis above focuses on as-
pects related to the network communication of the two im-
plementations, whereas processing times for example are
not at all taken into account. The experimental evaluation
of the two stacks ﬁlls this gap.
Figure 8. Early latency vs. offered load for ab-
cast messages of size 16384 bytes.
The following paragraph presents the system setup used
in the experiments. Then, a performance comparison is pre-
sented between the modular and monolithic stacks.
5.3.1 System Setup
The benchmarks were run on a cluster of machines running
SuSE Linux (kernel 2.6.11). Each machine has a Pentium 4
processor at 3.2 GHz and 1 GB of RAM. The machines are
interconnected by Gigabit Ethernet (which is exclusively
used by the cluster machines) and run Sun’s 1.5.0 Java Vir-
tual Machine. The machines were dedicated to the perfor-
mance benchmarks and had no other load on them.
The atomic broadcast algorithm was implemented
(twice) in Fortika ver. 0.46 [18, 19]. Fortika is a group com-
munication toolkit with various well-known off-the-shelf
protocol modules. These protocol modules can then be
composed using different protocol frameworks. The current
experiments were run with the Cactus protocol framework
[4, 24].
5.3.2 Performance Results
Latency of Atomic Broadcast. Figure 8 shows the evolu-
tion of the early latency (vertical axis) of atomic broadcast
using the two implementations as the offered load (horizon-
tal axis) increases. Results are shown for a system size of
n = 3 (two bottom curves) and n = 7 (two top curves),
with abcast messages of size 16384 bytes. Note that chang-
ing the size of messages does not signiﬁcantly affect the
results.