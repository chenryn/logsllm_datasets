ORAM tree with the other rows of the resulting matrices.
2) Πspdz Scheme: Our Πspdz scheme operates on the gen-
eral (cid:96)-server setting with the pre-computation model.
Retrieval. As presented in Figure 11, Πspdz employs the
SPDZ-PIR protocol, instead of RSS-PIR or XOR-PIR in Πrss,
to implement the private retrieval phase.
Eviction.
Figure 12 depicts the eviction phase of Πspdz,
which is similar to that of Πrss, except that it employs matrix
multiplication protocol by SPDZ to implement the oblivious
pick and drop operations. In this case, the client creates and
sends to the servers the authenticated shares of the permutation
matrices, instead of only the additive shares as in Πrss.
3) MACAO Security: We now present
the security of
MACAO schemes. The MACAO eviction follows the push-
down principle proposed in Circuit-ORAM [66] so that it has
the same overﬂow probability as follows.
Lemma 3 (Stash overﬂow probability). Let the bucket size
Z ≥ 2. Let st(MACAO[s]) be a random variable denoting the
stash size of a MACAO scheme after an access sequence s.
Then, for any access sequence s, Pr[st(MACAO[s]) ≥ R] ≤
14 · e−R.
Proof: (see [66])
The security of MACAO can be stated as follows.
Theorem 1 (MACAO security). MACAO framework is statis-
tically (information-theoretically) secure by Deﬁnition 2.
Πspdz.Evict():
Parameters: Same as Figure 10, except (cid:96) ≥ 2.
Inputs: The client has input α and every Si has inputs(cid:0)(cid:74)α(cid:75)i, (cid:104)T(cid:105)i
Client: (cid:0)b, (I0, . . . , IH )(cid:1) ← Execute lines 1–3 in Figure 10.
2. Send(cid:0)(cid:104)b(cid:105)i, ((cid:104)I0(cid:105)i, . . . , (cid:104)IH(cid:105)i)(cid:1) to Si for 0 ≤ i < (cid:96)
1. ((cid:104)Ih(cid:105)0, . . . , (cid:104)Ih(cid:105)(cid:96)−1) ← AuthCreate(Ih, (cid:96)) for 0 ≤ h ≤ H
(cid:1).
the bucket P(v, h) of (cid:104)T(cid:105)i with (cid:104)b(cid:105)i, respectively.
in Figure 3, where the client
Server: For each level h of the eviction path v starting from the root:
3. Every Si forms authenticated shared matrices (cid:104)Bh(cid:105)i by concatenating
4. All parties execute the SPDZ-based matrix multiplication protocol
inputs α and every Si has inputs
5. Every Si interprets the last row of (cid:104)Bh × Ih(cid:105)i as the holding block (cid:104)b(cid:105)i
for the next level h + 1, and updates the buckets P(v, h) of (cid:104)T(cid:105)i with
the other rows.
(cid:1). Let (cid:104)Bh × Ih(cid:105)i be the output of Si.
(cid:0)(cid:74)α(cid:75)i, (cid:104)Ih(cid:105)i, (cid:104)Bh(cid:105)i
Fig. 12: Πspdz eviction.
Proof: (see Appendix)
D. Cost Analysis
analyze
We
the
asymptotic
cost of our proposed
MACAO schemes. We treat some parameters as constant
including the ﬁnite ﬁeld (p), the bucket size Z and the number
of servers (cid:96) (i.e., (cid:96) = 3 in Πrss and (cid:96) ≥ 2 in Πspdz). Following
the tree ORAM literature (e.g., [59], [64], [66]), we consider
the statistical security parameter as a function of database size,
i.e., λ = O(log N ). Let L = Z(H + 1) = O(log N ) be the
length of the path in the MACAO structure, and C = |b|/|Fp|
be the number of chunks per data block.
Client-server communication. In the Πrss retrieval, the client
sends six L-bit binary strings and receives six 2|b|-bit replies
if using XOR-PIR. If using RSS-PIR, the client sends six
(L|Fp|)-bit queries, and receives three 2|b|-bit replies. In the
Πrss eviction, the client sends to each server two authenticated
shares of a data block and (H + 1) permutation matrices of
size (Z + 1) × (Z + 1), where each element is log2 p bits.
Since L = O(log N ) and p is ﬁxed, the client communication
cost per Πrss access is O(|b| + log N ). Πspdz has a similar
asymptotic bandwidth cost as Πrss because they only differ in
the ﬁxed number of authenticated shares per server (2 vs. 1),
and the number of servers (cid:96) (yet (cid:96) is ﬁxed).
Server-server communication. In Πrss, servers communicate
with each other only in the eviction phase, where two authenti-
cated shares of the entire eviction path is transmitted from one
server to the others. Hence, the server-server communication
is 4L((cid:96)−1)|b| = O(|b| log N ). In Πspdz, servers need to com-
municate with each other not only in the eviction but also in
the retrieval phase. For each retrieval/eviction operation, every
server sends the authenticated shares of the entire path and the
client queries/matrices to all the others. Thus, its total server-
server communication is 2L(|Fp| + |b| + (Z + 1)2 + |b|) =
O(|b| log N ).
Client computation.
In Πrss retrieval, the client generates
4L random bits and performs XOR on L-bit data and |b|-
bit data four times if using XOR-PIR. If using RSS-PIR, the
client generates ((cid:96) − 1)L|Fp| random bits. In both cases, the
client additionally performs 2C additions (for block and MAC
recovery) and C multiplications (for MAC comparison) over
Fp ﬁeld. For each Πrss eviction, the client generates L((cid:96) −
1)(Z + 1)2 log2 p random bits, performs 2L(Z + 1)2((cid:96) − 1)
additions and L(Z + 1)2 multiplications over Fp. The cost of
Πspdz is similar to that of Πrss using RSS-PIR, but with (cid:96) ≥ 2.
Server computation.
In Πrss retrieval, each server performs
XOR operations on 2|b|-bit strings approximately L times if
using XOR-PIR. If using RSS-PIR, each server performs 6LC
modular multiplications, 4LC additions over Fp. Each Πrss
eviction incurs 6LC multiplications and 16LC additions over
Fp with 4LC log2 p random bits being generated.
Client storage. In both Πrss and Πspdz, the position map is of
size N (log2 N + log2 log2 N ). We follow the eviction in [66],
which requires the client to maintain a stash of size O(|b|λ) for
negligible overﬂow probability. In total, the asymptotic client
storage overhead is O(N (log N + log log N ) + |b| log N ).
Server Storage. An ORAM tree with N leaves can store 2N
data blocks. Each node in the tree can store Z blocks. In Πrss
and Πspdz schemes, each server stores one and two authenti-
cated shares of the ORAM tree, respectively. Therefore, the
storage overhead per server in Πrss and Πspdz is 4Z|b|N and
2Z|b|N bits = O(N ), respectively.
E. Extensions
In this section, we describe some tricks that can be applied
to our MACAO schemes.
1) Reducing Bandwidth Overhead: Since our ORAM
framework relies on XOR secret sharing and additive secret
sharing as the main building blocks,
the retrieval queries
and eviction data can be created and distributed in a more
communication-efﬁcient manner. The client can generate the
authenticated shares of retrieval queries, data blocks and
permutation matrices using a Pseudo-Random Function (PRF)
instead of a truly random function. To reduce the commu-
nication overhead,
the client can create random seeds for
such pseudo-random generator using a truly random function,
and securely send the seeds to ((cid:96) − 1) servers so that they
can generate their own shares themselves. Since the client
only needs to send the shares to one server, this strategy
can signiﬁcantly reduce the client bandwidth overhead. The
price to pay for this is the reduction of the security level
from information-theoretic to computational due to the pseudo-
random generation function. In Πrss scheme, we can further
apply this trick to reduce the server-server communication
overhead in the eviction phase. After performing the local
computation (e.g., line 1 in Figure 2), every server can generate
and send the seeds to other servers to let them calculate their
re-shared values. In this case, every server only needs to send
a shared matrix (instead of four) to one other server.
2) Reducing Client Storage Overhead: In our framework,
the client maintains two major components including a position
map of size O(N (log N + log log N ) and a stash S of size
O(|b| log N ). While the position map can be stored remotely
on the server-side using the recursion and meta-data techniques
[59], [22], we present two solutions to remove S at the client
as follows. The ﬁrst solution is to store S remotely at the
server-side in the form of authenticated shares ((cid:74)S(cid:75),(cid:74)αS(cid:75)),
and leverage homomorphic matrix multiplication protocols to
obliviously pick and drop the block from/into S. We treat S
as an additional level of the ORAM tree appended to the root
as suggested in [66]. Thus, when executing the PIR protocol
in the retrieval phase, we need to include the stash level, and
therefore, the retrieval query length (and the number of data
9
blocks in the path) will be (Z(H + 1) + λ) = O(log N ). In
the eviction phase, to obliviously put a block b into S[x], the
client creates a unit vector v = (v0, . . . , vλ−1) where vx = 1
and vy = 0 for all 0 ≤ y (cid:54)= x < λ. The client creates and sends
authenticated shares (cid:104)b(cid:105) and (cid:104)v(cid:105), and every server performs
(cid:104)S(cid:105) ← (cid:104)b(cid:105)×(cid:104)v(cid:105)(cid:62) +(cid:104)S(cid:105). To obliviously pick a block at S[x(cid:48)],
the client creates a unit vector v(cid:48) = (v0, . . . , vλ−1), where
ex(cid:48) = 1 and ey = 0 for all 0 ≤ y (cid:54)= x < λ. The client creates
and sends authenticated shares of v(cid:48) and the servers perform
(cid:104)b(cid:105) ← (cid:104)v(cid:48)(cid:105) × (cid:104)S(cid:105)(cid:62) and (cid:104)S(cid:105) ← (cid:104)S(cid:105) − (cid:104)v(cid:48)(cid:105) × (cid:104)S(cid:105)(cid:62).
The other method is to implement
the triplet eviction
principle proposed in [22] using homomorphic properties of
additive shares as similar to [33]. Since this approach requires
the bucket size to be of size O(log N ) for negligible overﬂow,
the computation and communication in the retrieval phase will
be increased by a factor of O(log N ).
IMPLEMENTATION AND PERFORMANCE EVALUATION
V.
A. Implementation
We fully implemented both MACAO schemes in §IV-C
all the extensions in §IV-E. In Πspdz, we implemented only
the online phase of the SPDZ-based matrix multiplication
protocol since we assume that authenticated shares of Beaver-
like matrix triples were pre-computed sufﬁciently in the ofﬂine
phase. The implementation was written in C++ consisting of
more than 25K lines of code for the MACAO controllers at the
client- and server-side. We used three main external libraries:
(i) Shoup’s NTL library v9.10.0 [60] for the server computa-
tion; (ii) ZeroMQ library [2] for the network communication;
(iii) pthread for server computation parallelization. Our
implementation made use of SIMD instructions to optimize
the performance of bit-wise operations and vectorized compu-
tations on Intel x64 architecture. For the reduced bandwidth
trick, we used tomcrypt library [1] to implement seeded
pseudo-random number generators using sober128 stream
cipher. Each server stored a 128-bit secret seed shared with
the client. In Πrss scheme, each server stored two extra 128-
bit secret seeds shared with the other two servers, which are
used to re-share the local computation during the RSS-based
matrix multiplication in the eviction phase.
Our
implementation is available at https://github.com/
thanghoang/MACAO.
B. Performance Evaluation
We ﬁrst describe our conﬁguration and evaluation method-
ology followed by the main experimental results.
1) Conﬁguration and Methodology: For the server-side, we
employed three c5.4xlarge Amazon EC2 instances each
equipped with 3.00 GHz 16-core Intel Xeon 8124M CPU,
16 GB RAM and 8 TB networked Elastic Block Storage (EBS)-
based SSD. For the client, we used a Macbook Pro with
2.6 GHz 6-core Intel Core i7 8850H CPU and 32 GB RAM.
Database. We used a random database of size ranging from
1 GB to 1 TB. We selected two standard data block sizes
including 4 KB and 256 KB as these are commonly used in
small-scale and large-scale ﬁle systems, respectively.
Network. We used a standard home Internet setting for
client-server communication. Speciﬁcally, the laptop client was
10
connected to the Internet via WiFi with 54.5 Mbps download,
5.72 Mbps upload throughput and 20ms round-trip latency
to Amazon EC2 servers. The server instances were set up
geographically close to each other, which resulted in the inter-
server throughput of 1 Gbps with 3ms round-trip latency.
Parameter choice and counterpart selection. We selected
S3ORAM and Path-ORAM as the main counterparts for
MACAO since S3ORAM is the most efﬁcient multi-server
ORAM scheme (but with no malicious security), while the
others are state-of-the-art single-server ORAM schemes. Al-
though Onion-ORAM offers similar properties to MACAO
(e.g., constant bandwidth, malicious security), we did not
explicitly compare our framework with it because the delay of
Onion-ORAM was shown signiﬁcantly higher than S3ORAM
and Path-ORAM in [33]. The main performance evaluation
metric is the end-to-end delay, which captures the processing
time at client- and server-side (e.g., I/O, computation) as well
as the network communication among parties. We conﬁgured
the system parameters for all schemes as follows so that they
achieve the same failure probability of 2−80.
• MACAO: We selected the bucket size Z = 2, stash size
|S| = 80 and performed two deterministic evictions per access
as suggested in [66]. We selected a 59-bit prime ﬁeld for
the computational advantage of 64-bit architecture and the
optimization of NTL library. In this experiment, we demon-
strated the performance of Πrss with the RSS-PIR protocol
only. It is because this protocol allows to further enable secure
computation on the accessed block, and its delay is higher than
XOR-PIR (so the comparison with counterparts will be more
conservative).
• S3ORAM: We used the open-sourced implementation in
[34]. We selected Z = 74 and eviction frequency A = 37.
Notice that these selected parameters are more appropriate than
the ones in [33], which suffer from the bucket overﬂow failure.
Similar to MACAO, we selected a 59-bit prime ﬁeld.
• Path-ORAM: We implemented a prototype of Path-ORAM.
We selected Z = 4 and |S| = 80 [64]. We used Intel AES-NI
library to accelerate cryptographic operations. We used AES
with counter mode (CTR) for encryption and decryption. We
created the MAC tag for each node in the Path-ORAM tree
using AES-128 CMAC.
• Ring-ORAM: We selected standard parameters as suggested
in [53] (i.e., Z = 16 and A = 20).
• Circuit-ORAM: We selected Z = 2, |S| = 80 and performed
two evictions per access as in [66]. Similar to Path-ORAM,
we used AES-128 CMAC for authentication and AES-CTR
for encryption with Intel AES-NI.
2) Setup Delay: We ﬁrst discuss the time to set up nec-
essary MACAO components (e.g., authenticated shares of the
ORAM trees, position maps) on the client machine. The delay
grew linearly to the database size. Speciﬁcally, it took around
370 s to 357,601 s to construct Πrss components for 1 GB to
1 TB database with 4 KB block size. For Πspdz components,
it took 244 s to 241,553 s, which was round 1.5 times faster
than Πrss since Πspdz only needs 2 servers (vs. 3 in Πrss). With
255 KB block size, the setup delay was 142 s to 135,927 s for
Πspdz components, and 209 s to 215,482 s for Πrss components.
We note that we did not measure the preprocessing cost to
generate multiplication triples for Πspdz scheme as it is out-
of-scope of this paper. We refer curious readers to [36] for its
detailed benchmarks.
1.2
1
)
c
e
s
(
y
a
l
e
D
0.8
0.6
0.4
0.2
20
Πrss
Πprf
spdz