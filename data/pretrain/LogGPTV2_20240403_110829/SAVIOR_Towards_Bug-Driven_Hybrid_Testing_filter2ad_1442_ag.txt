### References

1. Lezama, “Towards optimization-safe systems: analyzing the impact of undefined behavior,” in *ACM SIGOPS 24th Symposium on Operating Systems Principles (SOSP '13)*, Farmington, PA, USA, November 3-6, 2013, pp. 260–275.

2. Q. Yi, Z. Yang, S. Guo, C. Wang, J. Liu, and C. Zhao, “Eliminating path redundancy via postconditioned symbolic execution,” *IEEE Transactions on Software Engineering*, vol. 44, no. 1, pp. 25–43, 2018.

3. W. You, X. Wang, S. Ma, J. Huang, X. Zhang, X. Wang, and B. Liang, “Profuzzer: On-the-fly input type probing for better zero-day vulnerability discovery,” in *Proceedings of the 27th USENIX Security Symposium*. Berkeley, CA, USA: USENIX Association, 2018, pp. 745–761.

4. I. Yun, S. Lee, M. Xu, Y. Jang, and T. Kim, “QSYM: A practical concolic execution engine tailored for hybrid fuzzing,” in *Proceedings of the 27th USENIX Security Symposium*. Berkeley, CA, USA: USENIX Association, 2018, pp. 745–761.

5. L. Zhao, Y. Duan, H. Yin, and J. Xuan, “Send hardest problems my way: Probabilistic path prioritization for hybrid fuzzing,” in *Proceedings of the Network and Distributed System Security Symposium (NDSS)*, 2019.

### Supplementary Figures and Evaluation Data

#### Appendix A

**A. Program Instrumentation**

Figure 10 shows the UBSan-instrumented LLVM IR for the `objdump` defect in our motivating example. The source code is presented in Figure 2. In Figure 10, we highlight the instrumentation with `!saviorBugNum` metadata for bug-driven prioritization.

**B. UBSan Label Reduction**

In the process of vulnerability labeling, SAVIOR also reduces labels that can be confirmed as false positives. Table IX shows the results of label reduction on our benchmark programs.

| **Program** | **Total UBSan Labels** | **Removed UBSan Labels** | **Percentage** |
|-------------|------------------------|--------------------------|----------------|
| tcpdump     | 13,926                 | 1,924                    | 13.8%          |
| tiff2ps     | 1,768                  | 57                       | 3.2%           |
| readelf     | 2,476                  | 99                       | 4.0%           |
| xmllint     | 5,258                  | 195                      | 3.7%           |
| djpeg       | 9,391                  | 573                      | 6.1%           |
| tiff2pdf    | 3,126                  | 80                       | 2.6%           |
| jasper      | 3,838                  | 228                      | 5.9%           |
| objdump     | 9,025                  | 346                      | 3.8%           |
| **Average** | **6,106**              | **438**                  | **5.36%**      |

**Table IX:** Number of UBSan labels removed in our benchmark programs. On average, 5.36% of the labels are reduced.

**C. LAVA-M Evaluation**

In the evaluation with LAVA-M, bug-guided verification helps identify a group of LAVA bugs that are not listed. Table X shows the IDs of these LAVA bugs.

| **Program** | **LAVA Bugs**                                                                                   |
|-------------|--------------------------------------------------------------------------------------------------|
| base64      | 274, 521, 526, 527                                                                               |
| uniq        | 227                                                                                             |
| md5sum      | 281, 287                                                                                        |
| who         | 1007, 1026, 1034, 1038, 1049, 1054, 1071, 1072, 117, 12, 125, 1329, 1334, 1339, 1345, 1350, 1355, 1361, 1377, 1382, 1388, 1393, 1397, 1403, 1408, 1415, 1420, 1429, 1436, 1445, 1450, 1456, 1461, 16, 165, 169, 1718, 1727, 1728, 173, 1735, 1736, 1737, 1738, 1747, 1748, 1755, 1756, 177, 181, 185, 189, 1891, 1892, 1893, 1894, 1903, 1904, 1911, 1912, 1921, 1925, 193, 1935, 1936, 1943, 1944, 1949, 1953, 197, 1993, 1995, 1996, 2, 20, 2000, 2004, 2008, 2012, 2014, 2019, 2023, 2027, 2031, 2034, 2035, 2039, 2043, 2047, 2051, 2055, 2061, 2065, 2069, 2073, 2077, 2079, 2081, 2083, 210, 214, 2147, 218, 2181, 2189, 2194, 2198, 2219, 222, 2221, 2222, 2223, 2225, 2229, 2231, 2235, 2236, 2240, 2244, 2246, 2247, 2249, 2253, 2255, 2258, 226, 2262, 2266, 2268, 2269, 2271, 2275, 2282, 2286, 2291, 2295, 2302, 2304, 24, 2462, 2463, 2464, 2465, 2466, 2467, 2468, 2469, 2499, 2500, 2507, 2508, 2521, 2522, 2529, 2681, 2682, 2703, 2704, 2723, 2724, 2742, 2796, 2804, 2806, 2814, 2818, 2823, 2827, 2834, 2838, 2843, 2847, 2854, 2856, 2919, 2920, 2921, 2922, 294, 2974, 2975, 298, 2982, 2983, 2994, 2995, 3002, 3003, 3013, 3021, 303, 307, 3082, 3083, 3099, 312, 316, 3189, 3190, 3191, 3192, 3198, 3202, 3209, 321, 3213, 3218, 3222, 3237, 3238, 3239, 3242, 3245, 3247, 3249, 325, 3252, 3256, 3257, 3260, 3264, 3265, 3267, 3269, 327, 334, 336, 338, 3389, 3439, 346, 3466, 3468, 3469, 3470, 3471, 3487, 3488, 3495, 3496, 350, 3509, 3510, 3517, 3518, 3523, 3527, 355, 359, 3939, 4, 4024, 4025, 4026, 4027, 4222, 4223, 4224, 4225, 4287, 4295, 450, 454, 459, 463, 468, 472, 477, 481, 483, 488, 492, 497, 501, 504, 506, 512, 514, 522, 526, 531, 535, 55, 57, 59, 6, 61, 63, 73, 77, 8, 81, 85, 89, 974, 975, 994, 995, 996 |

**Table X:** IDs of unlisted bugs in LAVA-M that are triggered with bug-guided verification.

**D. Real World Benchmark Evaluation**

For a better reference of our evaluation with real-world programs, we summarize the number of triggered violations at the end of 224 hours in Table XII. Additionally, we compare the UBSan violations triggered by SAVIOR and other fuzzers. The results are summarized in Table XI. Generally, these fuzzers explore a similar group of UBSan violations. More importantly, for most cases, SAVIOR triggers a super-set of the violations made by the other fuzzers (particularly AFL and AFLGO). This indicates that SAVIOR has a better thoroughness in vulnerability finding.

| **Program** | **AFL** | **AFLGO** | **ANGORA** | **DRILLER** | **QSYM** | **SAVIOR** |
|-------------|---------|-----------|------------|-------------|----------|------------|
| tcpdump     | +5/-43  | +15/-28   | +1/-7      | +0/-6       | +0/-0    | +0/-0      |
| tiff2ps     | +0/-13  | +0/-76    | +0/-6      | +0/-6       | +0/-0    | +0/-0      |
| readelf     | +0/-7   | +0/-71    | +0/-6      | +0/-6       | +0/-0    | +0/-0      |
| xmllint     | +0/-6   | +0/-8     | +0/-6      | +0/-6       | +0/-0    | +0/-0      |
| djpeg       | +0/-0   | +0/-9     | +0/-0      | +0/-0       | +0/-0    | +0/-0      |
| tiff2pdf    | +0/-7   | +0/-6     | +0/-0      | +0/-0       | +0/-0    | +0/-0      |
| jasper      | +2/-13  | +2/-7     | +0/-7      | +0/-7       | +0/-0    | +0/-0      |
| objdump     | +14/-18 | +4/-13    | +10/-18    | +16/-20     | +7/-30   | +0/-0      |

**Table XI:** Difference between violations triggered by SAVIOR and other fuzzers. (+X/-Y) means X violations are triggered by the fuzzer but not by SAVIOR and Y violations are triggered by SAVIOR but not by that fuzzer.

| **Program** | **AFL** | **AFLGO** | **ANGORA** | **DRILLER** | **QSYM** | **SAVIOR** | **Total** |
|-------------|---------|-----------|------------|-------------|----------|------------|-----------|
| tcpdump     | 87      | 59        | 43         | 113         | 102      | 128        | 367       |
| tiff2ps     | 3       | 10        | 7          | 8           | 8        | 16         | 335       |
| readelf     | 14      | 16        | 14         | 16          | 15       | 22         | 297       |
| xmllint     | 12      | 12        | 3          | 12          | 12       | 18         | 389       |
| djpeg       | 141     | 134       | 134        | 141         | 141      | 141        | 377       |
| tiff2pdf    | 13      | 13        | 9          | 10          | 13       | 17         | 465       |
| jasper      | 33      | 31        | 23         | 26          | 26       | 44         | 465       |
| objdump     | 64      | 60        | 64         | 63          | 60       | 79         | 465       |

**Table XII:** Number of unique UBSan violations triggered by different fuzzers in 24 hours. In particular, SAVIOR triggers 43.4% and 44.3% more violations than DRILLER and QSYM, respectively.

### Appendix B

**Technical Discussion and Future Work**

In this section, we discuss the limitations of our current design, insights we learned, and possible future directions.

**Over-approximation in Vulnerability Labeling:**

As explained in Section III, SAVIOR leverages sound algorithms to label vulnerabilities where over-approximation may introduce many false-positive labels. This imprecision can weaken the performance of SAVIOR’s prioritization. A straightforward reaction to this issue is to eliminate as many dummy labels as possible. In our design, we utilize a rule-based scheme to filter those false-positive labels in Section III-B. In the future, we plan to include more precise static analysis for finer-grained label pruning. For instance, the STACK system developed by Wang et al. [69, 70] and the approach proposed by Hathhorn et al. [45] can be incorporated into SAVIOR, which are complementary to UBSan in identifying code snippets that may lead to undefined behavior.

**Prediction in Vulnerability Detection:**

Once reaching a potentially vulnerable program location in concolic execution, SAVIOR extracts the guarding predicates of the vulnerability label. However, these predicates may contradict the current path condition. In such cases, SAVIOR terminates the exploration of the labeling site immediately, as continuing the analysis cannot contribute to any valuable test input. Moreover, in many cases, we can predict whether an execution path can trigger a vulnerability or not by studying the runtime information of previous executions. To achieve this goal, we need a method to backwardly summarize path constraints from the labeled site to its predecessors in the explored paths. The core technique of this summary is the weakest precondition [44] (derived from Hoare Logic), which has been applied to both sequential and concurrent program analysis domains [22, 43, 71].

### Conclusion

This document provides a detailed overview of the SAVIOR system, including its methodology, evaluation, and future work. The system demonstrates significant improvements in vulnerability detection and prioritization, particularly when compared to existing fuzzers. Future work will focus on refining the label pruning and prediction mechanisms to further enhance the system's effectiveness.