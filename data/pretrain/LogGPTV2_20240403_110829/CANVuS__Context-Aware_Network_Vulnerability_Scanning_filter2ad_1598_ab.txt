e
x
t
t
M
a
n
a
g
e
m
e
n
t
l
i
M
d
d
e
w
a
r
e
t
t
N
e
w
o
r
k
S
a
e
D
a
a
b
a
s
e
t
t
Query
Trigger
Scanning
Subsystem
Vuln.
DB
CANVuS: Context-Aware
Network Vulnerability
Scanning
Other Context Aware
Applications
Fig. 1. Our context-aware network vulnerability scanning (CANVuS) architecture. The enterprise
network is monitored by using data from existing physical infrastructure devices, network service
appliances, and generic network probes. These heterogeneous sources of data are combined into
a uniﬁed view of the network context which can be queried by context-aware applications or
can have triggers automatically executed in response to certain contextual changes. In the case
of CANVuS, contextual changes that indicate possible conﬁguration changes are used to more
efﬁciently scan network devices.
long term storage, smart storage management, graceful degradation, etc. The CANVuS
application does not require extensive historical data, although we acknowledge that
other context-aware applications will indeed require these functionalities and we look
to leverage existing work in this direction for future versions of our architecture [34].
In the next three subsections, we ﬁrst describe categories of monitoring points or
data sources, from which creates a view of network context. Using this understanding,
we then present the design of the Context Manager, which converts data from network
monitors to the network state database in a uniform representation. Finally, we pro-
vide an example of what the network state database would look like for context-aware
applications.
4.1 Sources of Data
Inferring network states and state changes is a challenging problem because the re-
quired information is distributed across many devices, network services, and applica-
tions. Thus, the key to capture the states and changes is to monitor the targets from a
network perspective and approximate the context by aggregating network events from
various data sources, which may lay in different layers of the network stack. To de-
termine what data sources to use for event collection and integration, we ﬁrst need to
understand what types of network activities could be monitored and how they relate to
changes in the network.
144
Y. Xu et al.
In this architecture, the monitors distributed across the network fall into three cate-
gories. The ﬁrst category is monitors deployed in infrastructure devices, such as switches,
routers and wireless access points. The advantage of having these types of monitors is
that they provide detailed knowledge about the entire network, as well as the host infor-
mation, with high resolution. This is a direct result of these devices providing the core
hardware infrastructure for the network. For example, by querying the switches with
SNMP, the system knows exactly when a particular physical machine is plugged into
the network and when it leaves. However, because of the importance of these devices,
tradeoffs are involved between the ﬁdelity of data and the overhead (or difﬁculty) of the
data collection.
The second category of monitors is for network services, which may include DNS,
DHCP and even Kerberos services. The data observed by this second category of mon-
itors provides additional detailed states about the hosts. Services in this class are the
ones that are deployed in almost every enterprise and are critical to the operation of
the network. For example, the data generated for DHCP service helps to distinguish
between conﬁguration changes for the same host and MAC-to-IP binding changes for
different hosts. Depending on the types of the network, monitors in this category may
require access to the syslog that are local to each service.
The ﬁnal category consists of passive probes, which are deployed along with packet
taps. These probes perform realtime analysis of network trafﬁc that can be either generic
TCP/UDP ﬂows or application/OS speciﬁc to generate events. By monitoring TCP/UDP
ﬂows, the system gains the knowledge of which hosts are available and what services
are running and being used by clients. In addition, using Deep Packet Inspecting (DPI)
for an application protocol or OS ﬁngerprint helps inventory the speciﬁc versions of
applications (e.g., HTTP, SSH) or operating systems running in the network. Deviating
version information for the same host directly indicates conﬁguration changes for that
host. For example, one could use Bro’s protocol analysis to do connection reconstruc-
tion and application version ﬁngerprinting [22] (which we used in our implementation
of CANVuS). However, the visibility of passive probes is limited to the network trafﬁc
they have access to. In other words, silent applications/services will be missed, which is
also the limitation of passive vulnerability scanners [26, 8, 17]. Thus, the passive probe
is merely an additional type of monitor for collecting context data for observing a com-
plete view of the network.
We acknowledge that host-based monitors exist for clients, but we decided that they
are beyond the scope of this work. Intuitively, host-based monitors can provide the most
accurate inventory information that effectively renders vulnerability scanning useless.
Unfortunately, they are very difﬁcult, if not impossible, to be enforced in large enter-
prise networks due to scalability and administrative reasons. However, as some tradi-
tional host-based programs, like antivirus software, are moved into the cloud [18], we
may be able to build another type of monitors for potential in-cloud security services.
The key advantage is that the in-cloud version of the services already have the visibil-
ity into the end hosts because they provide functionalities that used to be local to the
hosts. Thus, the system will be able to obtain more ﬁne-grained information about host
activities without unacceptable modiﬁcations or performance penalties to the end hosts.
CANVuS: Context-Aware Network Vulnerability Scanning
145
4.2 Context Manager
The Context Manager, a layer between data monitors and the network state database,
infers network context (states and state changes) from aggregated data collected from
various monitors and translates this to a uniform model. Speciﬁcally, the network states
are a collection of simple facts about the target network, and they keep evolving as
underlying hosts and services change. For example, these facts may include what hosts
are available at the moment, what are the MAC addresses for a set of IPs in a certain
period of time, or when is the ﬁrst time a particular host connected to the network.
Existing libraries are used to read data from syslog, SNMP and Netﬂow that then get
ﬁltered and processed into a uniform representation of the network’s state that is then
inserted into the network state database. These modules can come from a programming
languages standard library (e.g., Python’s syslog module) or from third party libraries
(e.g., PySNMP [12]). Additionally, the Context Manager supports a ﬂexible framework
for adding additional plugins to read input from new data monitoring sources and trans-
late this data into a uniform model. These plugins can be thought of as data adapters
that convert the source inputs canonical data format into a representation used in our
architecture.
4.3 Network State Database
The network state database provides underlying model for which context-aware appli-
cations are written upon. The applications use standard database triggers or the pro-
grammable querying interfaces to interact with the database. The types of data that
may be represented can be found in Table 1. This table shows how for that the data
is uniformly represented across typical network abstraction layers, sources, and their
respective data formats.
Table 1. Example contents of the network state database
Network Layer Data Source Data Format
Description
Link Layer
Network Layer
Transport Layer
Switch
Switch
Bro
Router
Router
Bro
DNS Server
Bro
Application Layer DHCP Server
Bro
Bro
SNMP
SNMP
Ethernet
SNMP
Netﬂow
TCP/IP
Syslog
UDP/IP
Syslog
UDP/IP
TCP/IP
Mapping between a MAC address and a Physical Switch Port
Mapping between a MAC address and an IP address
Mapping between a MAC address and an IP address
Network allocation
New connection established
New connection established
Name resolution
Name resolution
Mapping between a MAC address and an IP address
Mapping between a MAC address and an IP address
Application version ﬁngerprint
5 CANVuS
In this section, we describe the implementation of CANVuS, a vulnerability scanning
system, based on our context-aware architecture. It was implemented in Python to con-
nect the network state database and a vulnerability scanner. In our implementation, we
146
Y. Xu et al.
used Nessus [25]. Ideally, if all host conﬁguration changes produce network artifacts,
the need for network vulnerability scanning of these events would be straightforward.
However, not all host changes have network evidence that can be captured by at least
one of the monitors. Thus, CANVuS uses both query and callback interfaces from the
network state database to leverage the context information and to maintain a history of
scanning results in its own vulnerability database.
During the initialization phase, CANVuS queries the database for all available hosts
as scanning candidates. Due to the constraints in hardware and network resources, all
hosts are not scanned concurrently. Instead, candidates are queued for pending scans,
whose size depends on the network conditions, the conﬁgured policies, and the amount
of available physical resources. A scheduling strategy is needed here to select the
next candidate to scan. For example, each candidate could be weighted based on their
triggering events and scheduled accordingly. In the current implementation, a simple
FIFO strategy is applied. At the same time, CANVuS registers a callback function with
database triggers so that a new candidate will be appended to the pending queue when a
change happens, unless the same target is already in the queue. To conduct actual scan-
ning operations, Nessus is used, yet is modiﬁed to change its target pool from a static
conﬁguration ﬁle to the queue discussed above.
Conversely, if a scanned host has no events ﬁred after N seconds since its last scan,
and there is no evidence (including both the context information and former scanning
results) indicating that it becomes unavailable, it will be added to the queue for another
scan. Thus, each host is effectively equipped with its own timer. Once it expires, CAN-
VuS will query the network state database and its vulnerability database to determine
if further scanning is necessary. Clearly, the value of the timer is a conﬁgurable policy
up to the decision of the operators. In addition, when registering callback functions,
instead of simply subscribing all possible changes in the database, CANVuS deﬁnes a
set of event-speciﬁc policies to ﬁlter the ones that are less relevant to host conﬁguration
changes.
The choices of polices involve tradeoffs and depend on the objectives of the admin-
istrators who manage this system. The purpose of our current implementation is not to
provide a reference system for production use. Instead, we aim to ﬁgure out what events
are more effective in detecting changes and what policies are more appropriate with the
given network conditions and administrative requirements. Therefore, the policies used
for our experiment were set to be as aggressive as possible so that an optimal solution
could be determined by ﬁltering unnecessary scans after the experiment was complete.
More speciﬁcally, the default policy is that every single events triggers a scan. The only
exception is the TCP event, since there are too many of them for each host, an active
timeout is enforced to prevent them from overwhelming the system. On the other hand,
if scans are being constantly triggered by inbound connections to ports that scanners fail
to discover, a negative timeout is also enforced to suppress this type of event being ﬁred
over and over again. Further details regarding the revision of our trigger implementation
and policy decisions are presented in the evaluation.
The vulnerability database is the central storage of vulnerability data for all of the
hosts in the network. It keeps track of the result for every scan conducted against each
CANVuS: Context-Aware Network Vulnerability Scanning
147
host. In addition to the raw results generated by our modiﬁed Nessus scanner, each scan
record contains following information:
– The time when this scan is triggered.
– The time when the backend scanner starts and ﬁnishes the scan.
– A list of open ports and the vulnerabilities on each port.
– A map from open ports to services.
– Operating system ﬁngerprint (optional).
– The type of triggering event.
As more results are inserted into the vulnerability database, the information can be used
in policy evaluation for further scans. Additionally, this information may be queried by
administrators at any time for risk assessment or used by other security applications.
6 Evaluation
In this section, we evaluate CANVuS in a large academic network. The basic metrics
used throughout this section is the number of scans conducted, which represents the re-
source consumption or overhead, and the latency of detecting conﬁguration changes,
which represents the system efﬁcacy. Ideally, CANVuS should outperform periodic
scanning with fewer number of scans by implicitly avoiding examining unallocated
IP addresses and unavailable hosts. Moreover, CANVuS should also achieve lower de-
tection latency as many host conﬁguration changes create network evidence that trigger
scans timely in our architecture.
We begin by discussing our experimental methodology. Then we show how CAN-
VuS outperforms existing models in terms of the number of scans required and the de-
tection latency. Next, we explore the impact of timeout values to the CANVuS system.
The following section evaluates the contribution of various data sources to CANVuS
and their correlations with observed changes on the hosts. We conclude the evaluation
by discussing the scalability requirements of the context-aware architecture.
6.1 Experimental Methodology
The target network for the experiment is a college-level academic network with one
/16 and one /17 subnet. There are two measurement points for the experiments. One is
the core router for the entire college network. Because it has the access to the trafﬁc
between the Internet and the college network, there were two monitors built on it:
– TCP connection monitor: records the creation of new TCP connections.
– Application version monitor: records the version strings in protocol headers.
The second measurement point is a departmental network within the college that has
the visibility into both the inbound/outbound trafﬁc and more ﬁne-grained inter-switch
trafﬁc. As result, the following monitors were deployed:
– ARP monitor: records the ARP messages probing for newly assigned IPs.
– DHCP monitor: records DHCP acknowledgment events.
148
Y. Xu et al.
– DNS monitor: records queries to certain software update sites.
– TCP connection monitor: as described above.
– Application version monitor: as described above.
This choice of measurement points and event monitors enables CANVuS to cover the
network stack from the link layer to the application layer. Moreover, it also allows us to
analyze the effectiveness of individual monitors with different visibility.
Based on these event monitors, CANVuS was deployed to scan the college network
using a 12-hour active/inactive timeout and 1-hour negative timeout. In addition, an-
other instance of a Nessus scanner was deployed for comparison purposes. It was con-
ﬁgured to constantly enumerate the entire college network (a.k.a. the loop scanner).
Both scanners were restricted to allow a maximum of 256 concurrent scans. The exper-
iment lasted for 16 days in March, 2010, during which time the loop scanner completed
46 iterations. And it was interrupted by a power outage for a couple of hours at the
end of the ﬁrst week. Since we expect the system to be running long term, occasional
interrupts would not have a major impact to the experiment results.
We performed a direct comparison between CANVuS and the loop scanner across
both dimensions of resource consumption and detection latency. During the current
exceptionally aggressive experiment, the loop scanner took less then 9 hours to ﬁnish
one iteration. In realistic deployments, we envision using larger values and scanning less
aggressively. Thus, the result of the loop scanner is only considered to represent the best
performance that traditional periodic scanning systems can achieve in detection latency.
More realistic values are represented below by sampling multiple 9-hour periods.
Ground truth for the experiments was established by identifying the period in which
an observable network change occurred. Speciﬁcally, the scanning records from both
scanners are ﬁrst grouped together based on the MAC (if available) or IP address of
each target host, and then each group of records are sorted by the time when each
scan started. In each sorted group, a change is deﬁned as two consecutive scan records
(scans of unavailable hosts are ignored) with different sets of open ports. In addition,
we assume that the change always happens immediately after the former scan ﬁnishes.
Subsequent discussions that require the knowledge about ground truth are all based
on this model unless otherwise noted. We approximated the ground truth in this way
because it is infeasible to gain the local access to a large number of hosts in the target
network to collect the real ground truth. As a result, we will not be able to analyze the
true positive rate of CANVuS, and the average latencies for both scanners represent the
upper bound, or the worst case.
6.2 CANVuS Evaluation