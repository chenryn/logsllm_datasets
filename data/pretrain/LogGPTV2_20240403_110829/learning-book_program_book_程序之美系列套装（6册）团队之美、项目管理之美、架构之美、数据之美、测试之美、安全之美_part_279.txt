"Read key,value pairs from file."
for line in file (name) :
yield line.split(sep)
def avoid_long_words (word,N) :
"Estimate the probability of an unknown word."
return 10./ (N*10**len(word))
N=1024908267229##Number of tokens in corpus
Pw=Pdist (datafile (*vocab_common*) , N, avoid_long_words) )
注意，Pw[w]是单词w的原始计数，而Pw(w)是它的概率。本章描述
的所有程序都能够通过http：/norvig.com/ngrams获得。
因此，该模型分割的效果如何呢？以下是一些实例：
>>>segment ('choosespain*)
['choose', 'spain']
>>>segment ('thisisatest')
['this'，'is'，'a'，'test']
>>>
segment ('wheninthecourseofhumaneventsitbecomesnecessary')
，'becomes','necessary']
>>>segment ('whorepresents*)
['who', 'represents']
>>>segment ('expertsexchange')
1581
---
## Page 1583
['experts'， 'exchange']
>>>segment ('speedofart')
['speed'，'of'，'art']
>>>segment ('nowisthetimeforallgood*)
['now'，'is'，'the'，'time'，'for'，'all'，'good']
>>>segment ('itisatruthuniversallyacknowledged')
>>>
segment ('itwasabrightcolddayinaprilandtheclockswerestrikin
gthirteen*)
，.tde,u.,ep..Poo.ubq.,，.seM,,T.]
nd','the'，'clocks',
'were'，'striking', 'thirteen*]
>>>
segment ('itwasthebestoftimesitwastheworstoftimesitwastheag
eofwisdomitwastheage
offoolishness*)
'，*worst'，'of'，'times',
，'age'，'of'，
' foolishness')
>>>
segment ('asgregorsamsaawokeonemorningfromuneasydreamshefou
ndhimselftransformed
inhisbedintoagiganticinsect ')
['as'，'gregor'，'samsa'，'awoke'，'one'，'morning'，'from',
'uneasy'，'dreams'，'he',
'found'，‘himself'，'transformed'，'in'，'his'，'bed'，'into'
，'a'，'gigantic',
1582
---
## Page 1584
insect*]
>>>
segment (′inaholeinthegroundtherelivedahobbitnotanastydirty
wetholefilledwiththe
endsofwormsandanoozysmellnoryetadrybaresandyholewithnothin
ginittositdownonortoeat
itwasahobbitholeandthatmeanscomfort')
'a'，'hobbit'，'not'，'a',
ds'，'of'，'worms'，'and',
ues，,xeq，,xp，，,，,ou,us，zoo，ue
dy', 'hole', 'with',
'nothing'，'in'，'it'，'to'，'sitdown'，'on'，'or'，'to'，'ea
t'，'it'，'was'，'a'，
'hobbit'，'hole'，'and'，'that'，'means'，'comfort']
>>>
segment ('faroutintheunchartedbackwatersoftheunfashionablee
ndofthewesternspiral
armofthegalaxyliesasmallunregardedyellowsun')
['far'，'out'，'in'，'the'，'uncharted'，'backwaters'，'of'，
'the', 'unfashionable',
'galaxy'，'lies'，'a',
[,uns,,mott,papeb,,un,eus
看到程序正确地对一些生僻的单词如"Samsa"和"oozy"进行分割，你
可能会很高兴。对于“Samsa"在10000亿个单词中出现了42000次，
“o0zy”出现了13000次，你应该不会太惊讶。分割的整体结果看起来
不错，但是有两个错误：un'、'regarded'应该是一个单词，
而'sitdown'应该是两个单词。尽管如此，分词的准确率是
157/159=98.7%，还是不错的。
1583
---
## Page 1585
第一个错误是由于"umregarded"在我们的三十多万单词的词汇里（它
在所有单词词汇中的位置是1005493，计数值是7557）。如果我们
把它放入词汇中，我们发现之前的分词是正确的。
>>>Pw[*unregarded']=7557
>>>
segment ('faroutintheunchartedbackwatersoftheunfashionablee
ndofthewesternspiral
armofthegalaxyliesasmallunregardedyellowsun')
1'，'arm'，‘of'，'the'，'galaxy',，'lies'，'a'，'small'，'unre
garded', ' yellow', ' sun']
这并没有证明我们解决了问题：我们需要放回所有其他的干扰单
词，而不仅仅是我们需要的那个；而且我们需要返回所有的测试案
例，确保增加其他的单词并没有混淆了任何其他的结果。
第二个错误是虽然"sit"和"down"是频繁词（概率分别为0.003%和
0.04%），但是这两个单词的概率乘积刚好略小于"sitdown"这个单
词本身的概率。根据二元计数，两个单词序列“sitdown"的概率比其
分别的概率乘积约大100倍。我们可以通过对二元建模来解决这个
问题；也就是说，考虑每个单词的概率，给定前一个单词的概率
是：
P(W1: n) =IIk=1: nP(Wk/Wk-1)
当然，整个二元单词表无法全部装载到内存中。如果我们只保留出
现100000次以上的二元单词，结果就有250000多的入口项，这也无
法装载到内存中。然而，我们可以通过Count(sitdown)/Count(sit)来
估计概率P(downsit)。如果一个二元单词在表中不出现，我们就通
过单元值来计算。在一个单词的前序单词给定的情况下，我们可以
定义这个单词的条件概率cPw如下：
def cPw (word, prev) :
1584
---
## Page 1586
"The conditional probability P(wordlprevious-word)."
try:
return P2w[prev+'*+word]/float (Pw[prev])
except KeyError:
return Pw (word)
P2w=Pdist (datafile ('count2w'), N)
（细心的人会注意到cPw不是概率分布，因为对于一个给定的先验
单词，所有单词的概率之和会超过1。这种方法的技术名称是“愚蠢
回退"(supidbackof)，但其实际效果很不错，因此我们不用担
心。）我们现在来比较包含先验单词“to”、“sitdown"和"sitdown"的
概率：
>>>cPw ('sit'， 'to')
*cPw （'down'，'sit')/cPw('sitdown'，'to')
1 6 9 8 . 0 0 0 23 3 01 9 9 2 6 3
我们发现"sitdown”比"sitdown”的概率高1698倍，因为"sitdown”是
一个高频的二元项，而且“tosit"词频很高，而"tositdown"词频不
高。
这看起来振奋人心：让我们用二元模型来实现新版的分词。但要实
现这种新版分词，我们还需要解决另外两个问题：
1.当给一个由n个单词构成的序列增加一个新的单词时，它调用
Pwords来对这n+1个概率值进行乘法运算。但是分词时，原有的n个
概率值已经做过一次运算处理了。将每个元素的运算结果也就是概
率值保存起来，这样添加一个单词的时候就只需要另外执行一次运
算，因此分词也将更为高效。
2.可能存在算术运算向下溢出问题。如果我们把Pwords应用于一个
出现了61次"blah"之后的序列，我们将得到5.2x10-321的概率，而如
果再加上一个"blah”，其概率为0.0。我们能够表示的最小正浮点数
1585
---
## Page 1587
是4.9x10-324；任何小于该值的概率都置为0.0。为了避免向下溢
出，最简单的解决方案是对这些数值使用对数运算而非直接进行乘
法运算。
我们将定义“segment2”（分词2），它和分词有三个方面的区别：第
一，它使用了条件二元语言模型，cPw，而不是单元模型Pw。第
二，函数特征是不同的。“segment2"除了需要把“文本"作为一个参
数传递给函数之外，还需要将这段文本的一个单词也传递给函数。
在句子的开始，前一个单词是句子的起始标记；其返回值也
不只是一个单词的序列，而是包含了两项内容：分词的概率和单词
序列。我们返回概率，这样可以把它保存起来（通过记录的装饰器
memo)，不需要重新计算；这解决了问题：（1）的效率低问题。合
并函数接受四个输入：第一个单词、剩下的单词以及它们分别的概
率。合并函数会把第一个单词添加到其他单词后面，并对所有的概
率值进行乘法运算。但为了解决问题（2）我们引入了第三个区
别：不对概率值直接进行乘法运算而是对概率的对数执行加法运
算。
以下“segment2"的代码：
from math import log10
def segment2(txt,prev=''):
"Return (log P(words), words), where words is the best
segmentation."
if not text:return 0.o, []
candidates=[combine (logl0(cw (first,prev) ) ,
first, segment2 (rm, first))
for first,rem in splits (text))
return max (candidates)
def combine (Pfirst, first, (Pem, rem) ) :
"Combine first and rem results into
one (probability, words) pair. "
1586
---
## Page 1588
return Pfirst+Prem, [first]+rem
segment2执行了O(nL)次的递归调用，每次调用需要考虑O(L)次的切
分，因此整个算法的复杂度是O（nL2）。实际上，这就是Viterbi算
法，memo隐含地创建Viterbi表。
segment2正确地对"sitdown"例子进行分割，第一个版本中能正确切
分的例子，在这个版本中也仍然是正确的。但这两个版本对
“unregarded"的分词都不正确。
我们能否在性能上有所提升？很有可能。我们可以对未知单词创建