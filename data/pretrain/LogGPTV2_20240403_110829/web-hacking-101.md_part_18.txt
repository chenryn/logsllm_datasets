# Application Logic Vulnerabilities

## Takeaways
Javascript source code provides you with actual source code from a target you can explore. This is great because your testing goes from blackbox, having no idea what the back end is doing, to whitebox (though not entirely) where you have insight into how code is being executed. This doesn't mean you have to walk through every line, the POST call in this case was found on line 20570 with a simple search for POST.

## 8. Accessing PornHub's Memcache Installation
**Difficulty:** Medium  
**Url:** stage.pornhub.com  
**Report Link:** https://hackerone.com/reports/119871  
**Date Reported:** March 1, 2016  
**Bounty Paid:** $2500

**Description:**  
Prior to their public launch, PornHub ran a private bug bounty program on HackerOne with a broad bounty scope of *.pornhub.com which, to most hackers means all sub domains of PornHub are fair game. The trick is now finding them.

In his blog post, Andy Gill @ZephrFish explains why this is awesome, by testing the existing of various sub domain names using a list of over 1 million potential names, he discovered approximately 90 possible hacking targets.

Now, visiting all of these sites to see what's available would take a lot of time so he automated the process using the tool Eyewitness (included in the Tools chapter) which takes screenshots from the URLs with valid HTTP / HTTPS pages and provides a nice report of the sites listening on ports 80, 443, 8080 and 8443 (common HTTP and HTTPS ports).

According to his write up, Andy slightly switched gears here and used the tool Nmap to dig deeper in to the sub domain stage.pornhub.com. When I asked him why, he explained, in his experience, staging and development servers are more likely to have misconfigured security permissions than production servers. So, to start, he got the IP of the sub domain using the command nslookup:

```
nslookup stage.pornhub.com
Server: 8.8.8.8
Address: 8.8.8.8#53
Non-authoritative answer:
Name: stage.pornhub.com
Address: 31.192.117.70
```

I've also seen this done with the command, ping, but either way, he now had the IP address of the sub domain and using the command `sudo nmap -sSV -p- 31.192.117.70 -oA stage__ph -T4 &` he got:

```
Starting Nmap 6.47 ( http://nmap.org ) at 2016-06-07 14:09 CEST
Nmap scan report for 31.192.117.70
Host is up (0.017s latency).
Not shown: 65532 closed ports
PORT STATE SERVICE VERSION
80/tcp open http nginx
443/tcp open http nginx
60893/tcp open memcache
Service detection performed. Please report any incorrect results at http://nmap.org/submit/
Nmap done: 1 IP address (1 host up) scanned in 22.73 seconds
```

Breaking the command down:
* the flag -sSV defines the type of packet to send to the server and tells Nmap to try and determine any service on open ports
* the -p- tells Nmap to check all 65,535 ports (by default it will only check the most popular 1,000)
* 31.192.117.70 is the IP address to scan
* -oA stage__ph tells Nmap to output the findings in its three major formats at once using the filename stage__ph
* -T4 defines the timing for the task (options are 0-5 and higher is faster)

With regards to the result, the key thing to notice is port 60893 being open and running what Nmap believes to be memcache. For those unfamiliar, memcache is a caching service which uses key-value pairs to store arbitrary data. It's typically used to help speed up a website by service content faster. A similar service is Redis.

Finding this isn't a vulnerability in and of itself but it is a definite red flag (though installation guides I've read recommend making it inaccessible publicly as one security precaution). Testing it out, surprising PornHub didn't enable any security meaning Andy could connect to the service without a username or password via netcat, a utility program used to read and write via a TCP or UDP network connection. After connecting, he just ran commands to get the version, stats, etc. to confirm the connection and vulnerability.

However, a malicious attacker could have used this access to:
* Cause a denial of service (DOS) by constantly writing to and erasing the cache thereby keeping the server busy (this depends on the site setup)
* Cause a DOS by filling the service with junk cached data, again, depending on the service setup
* Execute cross-site scripting by injecting a malicious JS payload as valid cached data to be served to users
* And possibly, execute a SQL injection if the memcache data was being stored in the database

## Takeaways
Subdomains and broader network configurations represent great potential for hacking. If you notice that a program is including *.SITE.com in it's scope, try to find subdomains that may be vulnerable rather than going after the low hanging fruit on the main site which everyone may be searching for. It's also worth your time to familiarize yourself with tools like Nmap, eyewitness, knockpy, etc. which will help you follow in Andy's shoes.

## 9. Bypassing Twitter Account Protections
**Difficulty:** Easy  
**Url:** twitter.com  
**Report Link:** N/A  
**Date Reported:** Bounty awarded October 2016  
**Bounty Paid:** $560

**Description:**  
In chatting with Aaron Ullger, he shared the following Twitter vulnerability with me so I could include it and share it here. While the report isn't disclosed (at the time of writing), Twitter did give him permission to share the details and there's two interesting takeaways from his finding.

In testing the account security features of Twitter, Aaron noticed that when you attempted to log in to Twitter from an unrecognized IP address / browser for the first time, Twitter may ask you for some account validation information such as an email or phone number associated with the account. Thus, if an attacker was able to compromise your username and password, they would potentially be stopped from logging in to and taking over your account based on this additional required information.

However, undeterred, after Aaron created a brand new account, used a VPN and tested the functionality on his laptop browser, he then thought to use his phone, connect to the same VPN and log into the account. Turns out, this time, he was not prompted to enter additional information - he had direct access to the "victim's" account. Additionally, he could navigate to the account settings and view the user's email address and phone number, thereby allowing him desktop access (if it mattered).

In response, Twitter validated and fixed the issue, awarding Aaron $560.

## Takeaways
I included this example because it demonstrates two things - first, while it does reduce the impact of the vulnerability, there are times that reporting a bug which assumes an attacker knows a victim's user name and password is acceptable provided you can explain what the vulnerability is and demonstrate it's severity. Secondly, when testing for application logic related vulnerabilities, consider the different ways an application could be accessed and whether security related behaviours are consistent across platforms. In this case, it was browsers and mobile applications but it also could include third party apps or API endpoints.

## Summary
Application logic based vulnerabilities don't necessarily always involve code. Instead, exploiting these often requires a keen eye and more thinking outside of the box. Always be on the lookout for other tools and services a site may be using as those represent a new attack vector. This can include a Javascript library the site is using to render content.

More often than not, finding these will require a proxy interceptor which will allow you to play with values before sending them to the site you are exploring. Try changing any values which appear related to identifying your account. This might include setting up two different accounts so you have two sets of valid credentials that you know will work. Also look for hidden / uncommon endpoints which could expose unintentionally accessible functionality.

Also, be sure to consider consistency across the multiple ways the service can be accessed, such as via the desktop, third party apps, mobile applications or APIs. Protections offered via one method may not be consistently applied across all others, thereby creating a security issue.

Lastly, be on the lookout for new functionality - it often represents new areas for testing! And if/when possible, automate your testing to make better use of your time.

# 21. Getting Started

This chapter has been the most difficult to write, largely because of the variety of bug bounty programs that exist and continue to be made available. To me, there is no simple formula for hacking but there are patterns. In this chapter, I've tried to articulate how I approach a new site, including the tools that I use (all of which are included in the Tools chapter) and what I've learned of others. This is all based on my experience hacking, interviewing successful hackers, reading blogs and watching presentations from DefCon, BSides, and other security conferences.

But before we begin, I receive a lot of emails asking me for help and guidance on how to get started. I usually respond to those with a recommendation that, if you're just starting out, choose a target which you're likely to have more success on. In other words, don't target Uber, Shopify, Twitter, etc. That isn't to say you won't be successful, but those programs have very smart and accomplished hackers testing them daily and I think it'll be easier to get discouraged if that's where you spend your time when you're just beginning.

I know because I've been there. Instead, I suggest starting out with a program that has a broad scope and doesn't pay bounties. These programs often attract less attention because they don't have financial incentives. Now, I know it won't be as rewarding when a bug is resolved without a payment but having a couple of these under your belt will help motivate you to keep hacking and as you improve, you'll be invited to participate in private programs which is where you can make some good money.

With that out of the way, let's get started.

## Information Gathering

As you know from the examples detailed previously, there's more to hacking that just opening a website, entering a payload and taking over a server. There are a lot of things to consider when you're targeting a new site, including:

* What's the scope of the program? All sub domains of a site or specific URLs? For example, *.twitter.com, or just www.twitter.com?
* How many IP addresses does the company own? How many servers is it running?
* What type of site is it? Software as a Service? Open source? Collaborative? Paid vs Free?
* What technologies are they using? Python, Ruby, PHP, Java? MSQL? MySQL, Postgres, Microsoft SQL? Wordpress, Drupal, Rails, Django?

These are only some of the considerations that help define where you are going to look and how you're going to approach the site. Familiarizing yourself with the program is a first step. To begin, if the program is including all sub domains but hasn't listed them, you're going to need to discover them. As detailed in the tools section, KnockPy is a great tool to use for this. I recommend cloning Daniel Miessler's SecLists GitHub repository and using the subdomains list in the /Discover/DNS folder. The specific command would be:

```
knockpy domain.com -w /PATH_TO_SECLISTS/Discover/DNS/subdomains-top1mil-110000.txt
```

This will kick off the scan and save a csv file with the results. I recommend starting that and letting it run in the background. Next, I recommend using Jason Haddix's (Technical Director of Bugcrowd and Hacking ProTips #5 interviewee) enumall script, available on GitHub under his Domain repo. This requires Recon-ng to be installed and configured but he has setup instructions in his readme file. Using his script, we'll actually be scrapping Google, Bing, Baidu, etc. for sub domain names. Again, let this run in the background and it'll create a file with results.

Using these two tools should give us a good set of subdomains to test. However, if, after they're finished, you still want to exhaust all options, IPV4info.com is a great website which lists IP addresses registered to a site and associated subdomains found on those addresses. While it would be best to automate scrapping this, I typically will browse this manually and look for interesting addresses as a last step during my information gathering.

While the subdomain enumeration is happening in the background, next I typically start working on the main site of the bug bounty program, for example, www.drchrono.com. Previously, I would just jump into using Burp Suite and exploring the site. But, based on Patrik Fehrenbach's advice and awesome write ups, I now start the ZAP proxy, visit the site and then do a Forced Browse to discover directories and files. Again, I let this run in the background. As an aside, I'm using ZAP because at the time of writing, I don't have a paid version of Burp Suite but you could just as easily use that.

Having all that running, it's now that I actually start exploring the main site and familiarizing myself with it. To do so, ensure you have the Wappalyzer plug installed (it's available for FireFox, which I use, and Chrome). This allows us to immediately see what technologies a site is using in the address bar. Next, I start Burp Suite and use it to proxy all my traffic. If you are using the paid version of Burp, it's best to start a new project for the bounty program you'll be working on.

At this stage, I tend to leave the defaults of Burp Suite as is and begin walking through the site. In other words, I leave the scope completely untouched so all traffic is proxied and included in the resulting history and site maps. This ensures that I don't miss any HTTP calls made while interacting with the site. During this process, I'm really just exploring while keeping my eyes out for opportunities, including:

### The Technology Stack

What is the site developed with, what is Wappalyzer telling me? For example, is the site using a Framework like Rails or Django? Knowing this helps me determine how I'll be testing and how the site works. For example, when working on a Rails site, CSRF tokens are usually embedded in HTML header tags (at least for newer versions of Rails). This is helpful for testing CSRF across accounts. Rails also uses a design pattern for URLs which typically corresponds to /CONTENT_TYPE/RECORD_ID at the most basic. Using HackerOne as an example, if you look at reports, their URLs are www.hackerone.com/reports/12345. Knowing this, we can try to pass record IDs we shouldn't have access to. There's also the possibility that developers may have inadvertently left json paths available disclosing information, like www.hackerone.com/reports/12345.json.

I also look to see if the site is using a front end JavaScript library which interacts with a backend API. For example, does the site use AngularJS? If so, I know to look for Angular Injection vulnerabilities and include the payload {{4*4}}[[5*5]] when submitting fields (I use both because Angular can use either and until I confirm which they use, I don't want to miss opportunities). The reason why an API returning JSON or XML to a template is great is because sometimes those API calls unintentionally return sensitive information which isn't actually rendered on the page. Seeing those calls can lead to information disclosure vulnerabilities as mentioned regarding Rails.

Lastly, and while this bleeds into the next section, I also check the proxy to see things like where files are being served from, such as Amazon S3, JavaScript files hosted elsewhere, calls to third party services, etc.

### Functionality Mapping

There's really no science to this stage of my hacking but here, I'm just trying to understand how the site works. For example:

* I set up accounts and note what the verification emails and URLs look like, being on the lookout for ways to reuse them or substitute other accounts.
* I note whether OAuth is being used with other services.
* Is two factor authentication available, how is it implemented - with an authenticator app or does the site handle sending SMS codes?
* Does the site offer multiple users per account, is there a complex permissions model?
* Is there any inter-user messaging allowed?
* Are any sensitive documents stored or allowed to be uploaded?
* Are any type of profile pictures allowed?
* Does the site allow users to enter HTML, are WYSIWYG editors used?

These are just a few examples. During this process, I'm really just trying to understand how the platform works and what functionality is available to be abused. I try to picture myself as the developer and imagine what could have been implemented incorrectly or what assumptions could have been made, prepping for actual testing. I try my best not to start hacking right away here as it's really easy to get distracted or caught up trying to find XSS, CSRF, etc. vulnerabilities submitting malicious payloads everywhere. Instead, I try to focus on understanding and finding areas that may provide higher rewards and may...