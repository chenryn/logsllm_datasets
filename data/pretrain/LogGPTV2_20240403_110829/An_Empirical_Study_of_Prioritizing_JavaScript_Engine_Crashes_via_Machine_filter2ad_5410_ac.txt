log2
log2
(2)
10
10
10
10
10
10
10
10
10
50
100
500
1000
5000
10000
10
10
10
10
Parameters
(3)
...
True
True
True
True
True
True
True
False
True
True
True
True
True
True
True
True
True
True
True
...
(4)
gini
gini
gini
gini
gini
gini
gini
gini
gini
gini
gini
gini
gini
gini
gini
gini
gini
gini
entropy
(5)
2
5
10
20
50
100
100
100
100
100
100
100
100
100
100
100
500
1000
100
Accuracy
...
0.719 ± 0.095
0.724 ± 0.060
0.737 ± 0.041
0.742 ± 0.123
0.77 ± 0.118
0.76 ± 0.0093
0.77 ± 0.077
0.783 ± 0.101
0.788 ± 0.121
0.77 ± 0.077
0.77 ± 0.077
0.765 ± 0.075
0.765 ± 0.075
0.765 ± 0.075
0.765 ± 0.075
0.742 ± 0.029
0.756 ± 0.085
0.77 ± 0.056
0.747 ± 0.068
...
(1) max_features (2) n_estimators (3) warm_start (4) criterions (5) max_depth
refer the interested readers to the technical documentation for the
scikit-learn package [4] for more details.
Table 4 illustrates an example of the grid search result conducted
on a DTC model. The grid search generates all possible combina-
tions for a given set of hyperparameter values and then calculates
the accuracy of the model trained with the corresponding parame-
ters. Each column in the first five columns represents a parameter,
and the last column is the accuracy. Each row shows each parame-
ter candidate. In Table 4, the candidate [loд2, 10,True, дini, 100] in
bold produces the best average accuracy. As such, each parameter
of CRScope is determined by the grid search, producing the best
performance.
5 EXPERIMENTAL DESIGN
We evaluated CRScope by answering three key questions:
classifying security bugs from given crash-dumps?
• RQ1: How effective are Exploitable and AddressSanitizer in
• RQ2: Is CRScope effective in classifying security bugs from
given crash-dumps? If so, how much more accurate is CRScope
in comparison to the previous tools?
• RQ3: What feature types are most significant in classifying
security bugs?
To answer these questions, we evaluated six different classifiers
trained on features that CRScope extracts. The following sections
explain our cross-validation methodology and the method for han-
dling imbalanced training instances.
5.1 Cross-Validation
Our evaluation goal is to check whether a CRScope model can be
trained on past bugs and their crash-dump files, and whether it is
able to classify an unforeseen crash-dump. All browser vendors
accept a security bug report only if the reported bug exists in the
latest versions of their browsers or JS engines. Conservatively, a
Figure 1: Four-fold time series cross-validation in CRScope
model should be capable of classifying a crash on the latest JS
engines, which the model has not observed at all.
Cross-validation is a prevalent technique for evaluating a ma-
chine learning model. A standard n-fold cross-validation shuf-
fles crash instances to create training and test sets so that crash
instances on a later JS engine version will be used for predict-
ing a past bug, which was reported ahead of the release of the
JS engine. This is a common pitfall of the previous research on
hardware-counter based malware detection via machine learning,
as Zhou et al. pointed out [68]. That is, a standard n-fold cross-
validation is not applicable for evaluating CRScope because our
dataset is by nature a time series.
We designed our evaluation to check whether CRScope is able to
classify future security bugs. We sorted all security and non-security
bugs by their commit dates, which we used in their target binaries.
We then arranged their crash instances in ascending order and
divided these crashes into five bins so that each bin had the same
number of crash instances. From those five bins, we conducted the
4-fold cross-validation. The first cross-validation used the first and
second bins as the training and test sets, respectively. Its successive
cross-validation used all bins from the previous test as the training
set and used the next bin as the testing set, as shown in Figure 1.
Specifically, with five bins {1, 2, 3, 4, 5}, we leveraged four cross-
validation sets of {Traininд : 1 / T estinд : 2}, {1, 2 / 3}, {1, 2, 3 / 4},
and {1, 2, 3, 4 / 5}.
5.2 Balancing instances
Balancing security-related and non-security crashes is vital to avoid-
ing the classification bias towards a majority class [68]. When
building and evaluating a model on imbalanced data such that the
number of instances per each class is not equally distributed, clas-
sifier models are more likely to classify a new observation to the
majority class because the probability of instances belonging to the
majority class is significantly high.
Tripathi et al. evaluated Support Vector Machine (SVM) mod-
els in classifying exploitable crashes [60]. However, they used a
total of 523 crashes with 166 exploitable and 357 non-exploitable
samples. They trained and evaluated their model using this imbal-
anced dataset. Thus, their evaluation results were biased towards
non-exploitable crashes.
We also have an imbalanced dataset. To address this problem,
it is feasible to increase the frequency of the minority class, or
to decrease the frequency of the majority class. To avoid miss-
ing any crash instances due to under-sampling, we performed
over-sampling to balance security and non-security crashes, when
Session 9: FuzzingAsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand651Table 5: Evaluation of Exploitable
Table 6: Evaluation of AddressSanitizer
Our verdict
Exploitable
Exploitable
exploitable
probably-exploitable
Not-Exploitable
probably-not-exploitable
Ignored
unknown
Crash instances
Security
269 (73.10%)
40 (10.87%)
15 (4.08%)
44 (11.96%)
Not-security
197 (49.50%)
140 (35.18%)
52 (13.07%)
9 (2.26%)
preparing the cross-validation set. We applied the random over-
sampling algorithm [34], which duplicates random records from
the minority class.
6 EXPERIMENTAL EVALUATION
6.1 Performance of Exploitable and ASan (RQ1)
Exploitable. Exploitable [8] is a gdb extension. It analyzes the cur-
rent execution state of a target process when the gdb pauses the
crashing process with a crash-dump. It then predicts the exploitabil-
ity of this crash-dump by leveraging pre-defined heuristics. Each
label becomes the sole factor in determining the exploitability of a
target crash. This determination is classified as either exploitable,
probably-exploitable, probably-not-exploitable, or unknown.
Table 5 describes the evaluation results of Exploitable on 339
PoCs and their 766 crashes. The second column represents a label
that Exploitable predicts. The third and fourth columns represent
the true labels of tested crashes. The first column shows our inter-
pretation of the Exploitable reports. We ignored the unknown label
for Exploitable because we were unable to make a determination
with the information provided. Hence, we conservatively excluded
these 53 crash instances when computing the precision, recall, and
accuracy of Exploitable.
Approximately 84% of security crashes were labeled exploitable
or probably-exploitable, which we considered to be security-related.
That is, Exploitable achieved 0.95 recall for security bugs. However,
the resulting accuracy was 0.51 and the precision was 0.48 due
to false positives and false negatives, which are marked bold in
the table; 85% of non-security bugs were classified as exploitable
(false positives), and 4% of security bugs were classified as not-
exploitable (false negatives). If a browser vendor used Exploitable to
prioritize security bugs, the engineering cost of vetting these false
positives would be wasted. Furthermore, there still exist 53 crashes
that Exploitable is unable to classify.
AddressSanitizer. AddressSanitizer [56] is an open-source mem-
ory error detector from Google, which is designed to detect memory-
related bugs such as use-after-free and buffer overflows. It is an
instrumentation tool, which requires a target binary to compile via
clang with the -fsanitize=address option. After an instrumented JS
engine crashes when running a given PoC, AddressSanitizer reports
a memory-error class. Note that ClusterFuzz has used this class as
an indicator for classifying new browser bugs [20].
Table 6 shows our evaluation results for AddressSanitizer on the
collected PoCs and their crashes. The second column is a memory-
error class reported by AddressSanitizer. For the first column, we
Our verdict
AddressSanitizer
Exploitable
stack-buffer-overflow
heap-use-after-free
stack-buffer-underflow
invalid-free
stack-use-after-return
use-after-poison
Not-Exploitable
alloc-dealloc-mismatch
memory-leaks
stack-overflow
Crash instances
Security
15 (6.47%)
4 (1.72%)
4 (1.72%)
1 (0.43%)
1 (0.43%)
0 (0.00%)
16 (6.90%)
1 (0.43%)
0 (0.00%)
Not-security
1 (0.47%)
4 (1.87%)
0 (0.00%)
2 (0.93%)
0 (0.00%)
1 (0.47%)
10 (4.67%)
3 (1.40%)
5 (2.34%)
Ignored
invalid-memory-access
not-segv
148 (63.79%)
42 (18.10%)
143 (66.82%)
45 (21.03%)
clustered the reported memory-error classes into three groups: Ex-
ploitable, Not-exploitable, and Ignored. For this clustering, we lever-
aged the criteria from ClusterFuzz [21]. It classifies the severity of
a bug when AddressSanitizer emits one of the following labels: Bad-
cast, Heap-buffer-overflow, Heap-double-free, Heap-use-after-free,
Stack-buffer-overflow, Stack-use-after-return, or Use-after-poison.
Otherwise, we referenced the Common Weakness Enumeration
(CWE) list [39] to decide whether each memory-error class was ex-
ploitable. For instance, CWE-762 indicates alloc-dealloc-mismatch
and its official CWE description states that exploiting this bug is
rarely likely to cause unauthorized code execution [40]. We ignored
378 crash instances with the invalid-memory-access and not-segv
labels when computing the precision, recall, accuracy, and false neg-
atives of AddressSanitizer because we were unable to make further
decisions whether or not they are security-related.
As Table 6 shows, AddressSanitizer achieved 0.76 precision. How-
ever, the accuracy is 0.63 and the recall is 0.60, thus producing 40%
of false negatives. Also, AddressSanitizer was unable to detect 190 of
the crashes (42%) triggering security bugs. It becomes evident that
the tool is designed for detecting general memory errors, rather
than for classifying security bugs in JS engines.
6.2 CRScope Performance (RQ2)
We evaluated CRScope on crash-dump files from Chakra, V8, and
SpiderMonkey. Table 7 summarizes the averaged accuracy, pre-
cision, recall, F-1 score, and AUC for the model that appears in
the “Selected model” row. We selected the model with the highest
AUC among six classifiers. We also evaluated each of the classifi-
cation models, which were trained and validated on crash-dumps
only from each JS engine listed between the third and sixth head
columns. “All” represents the evaluation result of the model with
crash instances from all three JS engines.
Table 7 demonstrates that CRScope is effective in classifying
security crashes of JS engines. For SpiderMonkey crashes, the DTC
model of CRScope achieved 0.93 accuracy, 0.95 precision, 0.92 recall,
0.93 F1-score, and 0.93 AUC; this showed better performance than
any of the other models. Figure 2 also shows the ROC curves of
the six models for each JS engine. In general, CRScope is good at