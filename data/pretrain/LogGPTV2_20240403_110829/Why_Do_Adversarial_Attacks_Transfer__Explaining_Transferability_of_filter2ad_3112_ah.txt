and conclusions contained in this document are those of the
authors and should not be interpreted as representing the ofﬁ-
cial policies, either expressed or implied, of the Combat Capa-
bilities Development Command Army Research Laboratory
336    28th USENIX Security Symposium
USENIX Association
100101Sizeofinputgradients(S)0.200.250.300.350.40Testerror(5%poisoning)SVMlogisticridgeSVM-RBF10−510−4Variabilityoflosslandscape(V)0.240.260.280.300.320.340.360.38Transferrate(20%poisoning)0.20.40.60.8Gradientalignment(R)0.20.40.60.8ρ(ˆδ,δ)(20%poisoning)P:0.45,p-val:<1e-3K:0.27,p-val:<1e-20.20.40.60.8Gradientalignment(R)0.40.60.81.01.21.4Black-towhite-boxerrorratio(20%poisoning)P:0.31,p-val:0.01K:0.19,p-val:0.03SVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLRFHRFLNNHNNLCNNSVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFL.39.17.18.16.19.16.19.15.23.22.18.19.28.20.20.18.18.22.18.18.18.23.23.20.18.29.41.18.35.19.38.20.30.17.23.23.22.21.29.37.23.35.31.38.30.28.20.23.23.22.24.30.44.19.42.21.43.26.26.16.23.23.19.20.28.38.23.37.32.39.32.25.19.23.23.23.23.28.33.17.22.17.25.16.35.15.22.22.18.18.29.19.19.18.18.21.18.18.18.23.23.19.19.29targeterror.14.17.13.15.16.15.14.14.21.22.16.17.26whitebox.27.19.37.31.40.32.34.18transferrate.21.20.26.28.27.28.22.20SVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLRFHRFLNNHNNLCNNSVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFL.43.17.22.16.24.16.23.16.22.23.18.18.30.22.24.21.19.24.20.20.21.25.24.20.20.30.42.20.39.25.43.24.33.19.23.24.25.24.32.42.30.40.36.41.35.31.27.25.25.26.28.32.45.21.43.24.44.28.32.18.23.24.22.22.32.41.25.40.34.40.33.27.22.24.24.24.22.30.37.18.28.18.31.17.40.16.23.24.19.22.30.22.21.20.19.23.20.20.22.24.24.24.25.30targeterror.14.17.13.15.16.15.14.14.21.22.16.17.26whitebox.38.23.40.38.42.35.39.20transferrate.22.22.29.32.29.30.25.23SVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLRFHRFLNNHNNLCNNSVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFL.47.19.27.18.29.18.30.17.24.24.24.23.33.23.30.22.23.26.24.22.26.27.27.28.26.35.45.29.43.35.47.33.37.27.27.28.32.34.36.46.41.45.42.44.42.35.39.30.31.33.33.36.46.27.44.33.45.33.38.22.26.27.31.31.36.44.31.45.39.44.38.30.28.27.27.29.29.36.40.20.33.21.37.19.44.18.26.25.28.28.37.26.29.25.25.27.26.25.29.28.28.30.30.34targeterror.14.17.13.15.16.15.14.14.21.22.16.17.26whitebox.44.30.44.43.44.39.46.28transferrate.25.26.35.38.34.34.29.28SVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFLSVMHSVMLlogisticHlogisticLridgeHridgeLSVM-RBFHSVM-RBFL.76.40.34.39.37.62.41.36.41.24.11.28.09.58.19.22.34.11.18.31.16.54.14.14.38.26.30.64.28.70.26.31.37.10.17.32.25.53.15.11.61.55.51.68.49.83.56.58.43.20.15.30.16.59.39.24.36.22.13.33.11.60.25.29or the U.S. Government. The U.S. Government is authorized
to reproduce and distribute reprints for Government purposes
not withstanding any copyright notation here on. We would
also like to thank Toyota ITC for funding this research.
References
[1] D. Arp, M. Spreitzenbarth, M. Hübner, H. Gascon, and
K. Rieck. Drebin: Efﬁcient and explainable detection
of android malware in your pocket. In 21st NDSS. The
Internet Society, 2014.
[12] A. Demontis, P. Russu, B. Biggio, G. Fumera, and
F. Roli. On security and sparsity of linear classiﬁers for
adversarial settings. In A. Robles-Kelly et al., editors,
Joint IAPR Int’l Workshop on Structural, Syntactic, and
Statistical Patt. Rec., vol. 10029 of LNCS, pp. 322–332,
Cham, 2016. Springer International Publishing.
[13] Y. Dong, F. Liao, T. Pang, X. Hu, and J. Zhu. Boosting
adversarial examples with momentum. In CVPR, 2018.
[14] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining
and harnessing adversarial examples. In ICLR, 2015.
[2] A. Athalye, N. Carlini, and D. A. Wagner. Obfuscated
gradients give a false sense of security: Circumventing
defenses to adversarial examples. In ICML, vol. 80 of
JMLR W&CP, pp. 274–283. JMLR.org, 2018.
[15] K. Grosse, N. Papernot, P. Manoharan, M. Backes, and
P. D. McDaniel. Adversarial examples for malware
detection. In ESORICS (2), vol. 10493 of LNCS, pp.
62–79. Springer, 2017.
[3] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. Šrndi´c,
P. Laskov, G. Giacinto, and F. Roli. Evasion attacks
against machine learning at test time. In H. Blockeel et
al., editors, ECML PKDD, Part III, vol. 8190 of LNCS,
pp. 387–402. Springer Berlin Heidelberg, 2013.
[4] B. Biggio, B. Nelson, and P. Laskov. Poisoning attacks
against support vector machines. In J. Langford and
J. Pineau, editors, 29th Int’l Conf. on Machine Learning,
pp. 1807–1814. Omnipress, 2012.
[5] B. Biggio and F. Roli. Wild patterns: Ten years after the
rise of adversarial machine learning. Pattern Recogni-
tion, 84:317–331, 2018.
[6] C. M. Bishop. Pattern Recognition and Machine Learn-
ing (Information Science and Stats). Springer, 2007.
[7] N. Carlini and D. A. Wagner. Adversarial examples are
not easily detected: Bypassing ten detection methods. In
B. M. Thuraisingham et al., editors, 10th ACM Workshop
on Artiﬁcial Intelligence and Security, AISec ’17, pp.
3–14, New York, NY, USA, 2017. ACM.
[8] N. Carlini and D. A. Wagner. Towards evaluating the
robustness of neural networks. In IEEE Symp. on Sec.
and Privacy, pp. 39–57. IEEE Computer Society, 2017.
[9] X. Chen, C. Liu, B. Li, K. Lu, and D. Song. Targeted
backdoor attacks on deep learning systems using data
poisoning. ArXiv e-prints, abs/1712.05526, 2017.
[10] H. Dang, Y. Huang, and E.-C. Chang. Evading classiﬁers
by morphing in the dark. In 24th ACM SIGSAC Conf.
on Computer and Comm. Sec., CCS, 2017.
[11] A. Demontis, M. Melis, B. Biggio, D. Maiorca, D. Arp,
K. Rieck, I. Corona, G. Giacinto, and F. Roli. Yes, ma-
chine learning can be more secure! a case study on
android malware detection. IEEE Trans. Dependable
and Secure Computing, In press.
[16] T. Gu, B. Dolan-Gavitt, and S. Garg. Badnets: Identify-
ing vulnerabilities in the machine learning model supply
chain. In NIPS Workshop on Machine Learning and
Computer Security, vol. abs/1708.06733, 2017.
[17] A. Ilyas, L. Engstrom, A. Athalye, and J. Lin. Black-box
adversarial attacks with limited queries and information.
In J. Dy and A. Krause, editors, 35th ICML, vol. 80, pp.
2137–2146. PMLR, 2018.
[18] M. Jagielski, A. Oprea, B. Biggio, C. Liu, C. Nita-
Rotaru, and B. Li. Manipulating machine learning:
Poisoning attacks and countermeasures for regression
learning. In IEEE Symp. S&P, pp. 931–947. IEEE CS,
2018.
[19] A. Kantchelian, J. D. Tygar, and A. D. Joseph. Eva-
sion and hardening of tree ensemble classiﬁers.
In
33rd ICML, vol. 48 of JMLR W&CP, pp. 2387–2396.
JMLR.org, 2016.
[20] P. W. Koh and P. Liang. Understanding black-box pre-
dictions via inﬂuence functions. In Proc. of the 34th
Int’l Conf. on Machine Learning, ICML, 2017.
[21] Y. Liu, X. Chen, C. Liu, and D. Song. Delving into
transferable adversarial examples and black-box attacks.
In ICLR, 2017.
[22] C. Lyu, K. Huang, and H.-N. Liang. A uniﬁed gradient
regularization family for adversarial examples. In IEEE
Int’l Conf. on Data Mining (ICDM), vol. 00, pp. 301–
309, Los Alamitos, CA, USA, 2015. IEEE CS.
[23] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and
A. Vladu. Towards deep learning models resistant to
adversarial attacks. In ICLR, 2018.
[24] S. Mei and X. Zhu. Using machine teaching to identify
optimal training-set attacks on machine learners. In 29th
AAAI Conf. Artiﬁcial Intelligence (AAAI ’15), 2015.
USENIX Association
28th USENIX Security Symposium    337
[25] M. Melis, A. Demontis, B. Biggio, G. Brown, G. Fumera,
and F. Roli.
Is deep learning safe for robot vision?
Adversarial examples against the iCub humanoid. In
ICCVW ViPAR, pp. 751–759. IEEE, 2017.
[26] S.-M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and
In
P. Frossard. Universal adversarial perturbations.
CVPR, 2017.
[27] L. Muñoz-González, B. Biggio, A. Demontis, A. Pau-
dice, V. Wongrassamee, E. C. Lupu, and F. Roli. To-
wards poisoning of deep learning algorithms with back-
gradient optimization. In B. M. Thuraisingham et al.,
editors, 10th ACM Workshop on AI and Sec., AISec ’17,
pp. 27–38, New York, NY, USA, 2017. ACM.
[28] B. Nelson, M. Barreno, F. J. Chi, A. D. Joseph, B. I. P.
Rubinstein, U. Saini, C. Sutton, J. D. Tygar, and K. Xia.
Exploiting machine learning to subvert your spam ﬁl-
ter. In LEET ’08, pp. 1–9, Berkeley, CA, USA, 2008.
USENIX Association.
[29] A. Newell, R. Potharaju, L. Xiang, and C. Nita-Rotaru.
On the practicality of integrity attacks on document-
level sentiment analysis. In AISec, 2014.
[30] J. Newsome, B. Karp, and D. Song. Paragraph: Thwart-
ing signature learning by training maliciously. In RAID,
pp. 81–105. Springer, 2006.
[31] N. Papernot, P. McDaniel, and I. Goodfellow. Trans-
from phenomena
ferability in machine learning:
to black-box attacks using adversarial samples.
arXiv:1605.07277, 2016.
[32] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B.
Celik, and A. Swami. Practical black-box attacks against
machine learning. In ASIA CCS ’17, pp. 506–519, New
York, NY, USA, 2017. ACM.
[33] N. Papernot, P. D. McDaniel, and I. J. Goodfellow.
Transferability in machine learning: from phenomena
to black-box attacks using adversarial samples. ArXiv
e-prints, abs/1605.07277, 2016.
[34] R. Perdisci, D. Dagon, W. Lee, P. Fogla, and M. Sharif.
Misleading worm signature generators using deliberate
noise injection. In IEEE Symp. Sec. & Privacy, 2006.
[35] A. S. Ross and F. Doshi-Velez. Improving the adver-
sarial robustness and interpretability of deep neural net-
works by regularizing their input gradients. In AAAI.
AAAI Press, 2018.
[36] B. I. Rubinstein, B. Nelson, L. Huang, A. D. Joseph,
S.-h. Lau, S. Rao, N. Taft, and J. D. Tygar. Antidote: un-
derstanding and defending against poisoning of anomaly
detectors. In 9th ACM SIGCOMM Internet Measure-
ment Conf., IMC ’09, pp. 1–14, NY, USA, 2009. ACM.
[37] P. Russu, A. Demontis, B. Biggio, G. Fumera, and
F. Roli. Secure kernel machines against evasion attacks.
In 9th ACM Workshop on AI and Sec., AISec ’16, pp.
59–69, New York, NY, USA, 2016. ACM.
[38] M. Sharif, S. Bhagavatula, L. Bauer, and M. K. Reiter.
Accessorize to a crime: Real and stealthy attacks on
state-of-the-art face recognition. In ACM SIGSAC Conf.
on Comp. and Comm. Sec., pp. 1528–1540. ACM, 2016.
[39] C. J. Simon-Gabriel, Y. Ollivier, B. Schölkopf, L. Bottou,
and D. Lopez-Paz. Adversarial vulnerability of neural
networks increases with input dimension. ArXiv, 2018.
[40] J. Sokoli´c, R. Giryes, G. Sapiro, and M. R. D. Rodrigues.
Robust large margin deep neural networks. IEEE Trans.
on Signal Proc., 65(16):4265–4280, 2017.
[41] O. Suciu, R. Marginean, Y. Kaya, H. D. III, and T. Dumi-
tras. When does machine learning FAIL? Generalized
transferability for evasion and poisoning attacks. In 27th
USENIX Sec., pp. 1299–1316, 2018. USENIX Assoc.
[42] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Er-
han, I. Goodfellow, and R. Fergus. Intriguing properties
of neural networks. In ICLR, 2014.
[43] F. Tramèr, N. Papernot, I. Goodfellow, D. Boneh, and
P. McDaniel. The space of transferable adversarial ex-
amples. ArXiv e-prints, 2017.
[44] D. Varga, A. Csiszárik, and Z. Zombori. Gradient Regu-
larization Improves Accuracy of Discriminative Models.
ArXiv e-prints ArXiv:1712.09936, 2017.
[45] N. Šrndic and P. Laskov. Practical evasion of a learning-
based classiﬁer: A case study. In IEEE Symp. Sec. and
Privacy, SP ’14, pp. 197–211, 2014. IEEE CS.
[46] B. Wang and N. Z. Gong. Stealing hyperparameters in
machine learning. In 2018 IEEE Symposium on Security
and Privacy (SP), pp. 36–52. IEEE, 2018.
[47] L. Wu, Z. Zhu, C. Tai, and W. E. Enhancing the trans-
ferability of adversarial examples with noise reduced
gradient. ArXiv e-prints, 2018.
[48] H. Xiao, B. Biggio, G. Brown, G. Fumera, C. Eckert,
and F. Roli. Is feature selection secure against training
data poisoning? In F. Bach and D. Blei, editors, JMLR
W&CP - 32nd ICML, vol. 37, pp. 1689–1698, 2015.
[49] W. Xu, Y. Qi, and D. Evans. Automatically evading
classiﬁers a case study on PDF malware classiﬁers. In
NDSS. Internet Society, 2016.
[50] F. Zhang, P. Chan, B. Biggio, D. Yeung, and F. Roli.
Adversarial feature selection against evasion attacks.
IEEE Trans. on Cybernetics, 46(3):766–777, 2016.
338    28th USENIX Security Symposium
USENIX Association