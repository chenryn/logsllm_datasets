number  of  open  bugs  at  noon  Friday  was  largely  a 
meaningless number), the activity consumed in changing bug 
status crowded out more useful work. This is what you don’t 
want  if  you  use  metrics as a management  tool  instead of a 
blame-assignment tool. 
Triangulated metrics, in which several factors are used to 
arrive at a combined attribute, are more resistant to gaming 
as  well  as  potentially  providing  better  measurement 
(navigators, for example, require both latitude and longitude 
to  “fix”  their  position  in  the  ocean:  neither  latitude  nor 
longitude is sufficient. Nor is a GPS system sufficient to fix 
your position if you drop it overboard). 
For example, consider the area of producing patches for 
software vulnerabilities: most particularly, producing patches 
for security vulnerabilities. Most development organizations 
want fairly speedy closure of bugs, particularly for customer-
reported  bugs  of  high  severity  (that  is,  in  which  customer 
systems  are  down  or  their  ability  to  function  is  impaired). 
Therefore,  many  development  organizations  keep  track  of 
their backlog of vulnerabilities, and “age” them. The utility 
of aging is similar to that of other business functions that do 
aging (accounts receivable, for example). It isn’t necessarily 
the  total  number  of  bugs  that  is  a  problem  (after  all,  pre-
release, you want developers to find bugs, log them, and fix 
them), but an aged backlog of bugs in a production product 
that is both growing and aging raises a red flag. 
Developers  may  be  measured  on  how  fast  they  close 
bugs as well as by the absolute number of outstanding bugs. 
These  metrics  may  be  used  for  a  number of  goals,  such as 
ensuring that no critical (high severity) bugs are outstanding 
before  a  product  shipment  milestone  (e.g.,  major  product 
release or a patch set release) and that the most critical bugs 
are  addressed  quickly.  Both  of  these  are  worthy  goals. 
However, in the case of security vulnerabilities, the picture is 
a little more complex. 
Security  vulnerabilities  are  different  than  non-security 
vulnerabilities in that speed of closure is not as important as 
ensuring  that  the  root  cause  of  the  bug  is  addressed  (or, 
“fixed  completely”),  and,  moreover  that  the  bug  is  fixed 
correctly  the  first  time.  One  could  argue  that  these  are 
important  attributes  in  any  bug,  but  they  are  particularly 
critical in security bugs because of their nature. Specifically: 
•  Security  bugs  generally  require  the  vendor  to 
produce  more  patches  than  is  the  case  for  non-
security bugs  
•  Security patches are generally more broadly applied 
•  Security  bugs  that  are  not  fixed  correctly  may 
by customers than non-security patches 
(perversely) increase risk to customers 
Regarding  the  first  point,  many  vendors’  vulnerability 
handling policies include some notion of treating customers 
equally – more specifically, protecting customers to the same 
degree.  As a result, the vendor produces security patches on 
all  (or  mostly  all)  supported  systems  so  that  customers  are 
protected to the same degree, rather then telling customers to 
upgrade to the newest version (which often takes months and 
not  days  to  do).  The  record  at  Oracle  for  a  single-issue 
security  patch  (that  is,  a  patch  that  fixed  a  single  security 
issue  on  all  affected  versions  and  platforms)  is  78  patches. 
The  cost 
those  patches  easily  exceeded 
$1,000,000  and  that  was  exclusive  of  costs  to  Oracle 
Corporation  to  patch  their  own  product  instances  and 
exclusive  of  costs  to  customers  of  patching  their  product 
instances.  
to  produce 
Regarding  the  second  point,  customers  generally  apply 
security  patches  more  broadly  then  other  (non-critical) 
patches for, say, performance issues. That is, customers don’t 
wait  until  they  encounter  a  security  problem  (i.e.,  by 
experiencing a data breach) before applying the patch if they 
believe  their  software  installations  are  affected  by  an  issue 
and there is no other mitigation. They may also be required 
by  regulatory  pressures  to  apply  security  patches  more 
broadly than is the case for non-security patches.  
Regarding the third point, either a reporter of a security 
bug  or  other  third  parties  may  find  other  instances  of  a 
vulnerability if it is not fixed correctly. For example, suppose 
a  third  party  security  researcher  reports  that  a  particular 
sequence  of  characters  (Ctrl-X)  inputted  to  an  application 
causes  a  core  dump  (and  therefore,  anyone  sending  that 
sequence of characters to an application could cause a denial 
189
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:06:29 UTC from IEEE Xplore.  Restrictions apply. 
•  The fix was not complete or correct 
•  A  customer  may  have  applied  a  costly  emergency 
patch,  yet  is  still  vulnerable  to  a  variant  of  the 
weakness 
•  And  may  either  experience  an  exploit  or  have  to 
apply another expensive emergency patch 
of service attack). The real security problem is not that Ctrl-
X  causes  a  core  dump;  it’s  that  a  failure  to  validate  input 
correctly  leads  to  a  denial  of  service  attack.  Should  a 
developer only modify the code to handle Ctrl-X gracefully, 
the likelihood is that the reporter will log additional bugs in 
succeeding  weeks  regarding  Ctrl-Y,  Ctrl-Z,  and  so  forth. 
Even if the reporter does not try additional combinations to 
force  a  core  dump,  should  the  vendor  release  a  security 
patch,  it  may  well  be  decompiled  (by  other  researchers)  to 
figure  out  “What  changed”  to  determine  “How  can  we 
exploit the vulnerability?” The original coding error is now 
compounded  –  and  the  risk  to  customers  increased  – 
because: 
In short, if you only measure developers on how fast they 
close a security bug, their incentive is to fix the exact issue 
reported and not look at the root cause. You get the behavior 
you measure for, but not what customers actually need. 
As a result of the above three factors that make security 
vulnerabilities  different  from  non-security  vulnerabilities,  it 
is  critical  to  capture  metrics  that  measure  the  desired 
outcome and incentivize the correct behavior. A more robust 
metric than “closing the bug quickly” is to triangulate what is 
being  measured  to  factor  in  “completeness  of  fix”  and 
specifically,  capture  whether  a  bug  is  ever  reopened  (i.e., 
because the developer made a mistake and the security bug 
was  not  fixed  correctly  the  first  time).  Incentivizing  the 
correct  behavior  can  be  facilitated  by  using  normal  bug 
correction  mechanisms  to  remind  developers  that  “security 
bugs are different.”  
Oracle uses the bug database to both “flag” security bugs 
specifically  and  add  notations  to  the  bug  text  as  to  the 
requirements  for  “fix  completely”  (a  URL  to  the  secure 
coding  standards  and  “case  law”  on  what  “fix  completely” 
means  in  the  context  of  security  bugs).  The  purpose  of 
metrics is to manage better; therefore, reminding developers 
of  their  responsibilities  vis-à-vis  fixing  security  bugs  (and 
that correctness is as much or more important than a fast fix) 
is  considered  more  effective  than  merely  tracking  how 
poorly  the  organization  is  doing  regarding  meeting  that 
requirement. 
Another triangulation example involves the development 
equivalent  of  solving  the  worst  problems  first.  Assigning  a 
severity to security bugs using a standard measure, such as 
Common  Vulnerability  Scoring  System  (CVSS),  allows 
“bucketing”  of  bugs  by 
severity.  Capturing  aging 
information (e.g., aged backlog – which bugs are 0-30 days 
old, 30-60, 60-90 days old, and so forth) can be refined by 
using aging buckets to help measure not merely the status of 
the  backlog,  but  whether  more  critical  issues  are  being 
addressed first. Most development organizations have to use 
some kind of scoring or vulnerability rating anyway since no 
organization  can  reasonably  fix  all  security  bugs  of  all 
severity on all old product versions. Also, it is important to 
customers (as well as efficient resource allocation) to ensure 
that at least the most severe issues (that are likely to become 
public or that are already public) are fixed for affected and 
supported platforms. 
IV.  MIRROR MIRROR ON THE WALL, WHICH PRODUCT IS 
THE LEAST BUGGIEST OF ALL? 
Metrics  gaming  becomes  an  issue  when  metrics  moves 
from a management function to a reporting function, or from 
private measurement to public disclosure. The temptation to 
“game”  metrics  is  never  greater  than  in  instances  where 
numbers  will  be  used  for  comparative  purposes  or 
competitive purposes, most especially when they are public. 
Many  vendors  routinely  take  pot  shots  at  one  another 
over a metric that can broadly be categorized as “number of 
published  security  vulnerabilities  in  competing  products.”  
Absent much transparency or disclosure in how products are 
developed  or  “how  buggy  the  code  base  actually  is,” 
“published  vulnerabilities”  becomes  the  security  metric 
vendors  use  against  one  another  (and  that  customers  rely 
upon as a pseudo-security quality metric). The metric can be 
compiled in as simple a fashion as counting the number of 
line  items  in  security  advisories  that  a  competitor  releases 
during a year in a competing product. The metric as used is 
inherently flawed for reasons that will become clear below. 
First of all, a metric “comparison,” to be relevant, needs 
to compare apples to apples, not apples to road apples. There 
are  few  equivalent  ways  to  measure  security  defects  in 
software,  and  the  ones  that  are  equivalent  sometimes  fail 
because  of  lack  of  transparency  and  disclosure  (as  noted 
earlier, public metrics are likely to not only measure different 
things but potentially, actually report different (i.e., “prettied 
up”)  numbers  since  they  are  not  audited  or  otherwise 
independently verified).  
An  obvious  comparative  metric  for  multiple  bodies  of 
code  is  “defects  per  thousand  lines  of  code  (KLOC).” 
Comparing  defects  per  KLOC  in  two  bodies  of  code, 
assuming  the  sample  is  large  enough,  is a  fair comparison. 
However,  defects  per  KLOC  is  not  a  comparison  of 
“security”  per  se;  rather,  it  is  a  quality  metric  that  may 
nonetheless  have  security 
is,  some 
percentage  of  the  defects  per  KLOC  may  be  security 
vulnerabilities, but absent knowing how many are, it’s not a 
security comparison at all, merely a quality metric. 
implications.  That 
Another  aspect  of  comparing  security  vulnerabilities  is 
that  not  all  security  vulnerabilities  are  of  comparable 
severity.  Clearly  severity  is  important  when  talking  about 
security  vulnerabilities,  since  some  defects  are  far  more 
relevant  than  others.  A  low  severity,  or  non-exploitable,  or 
not-easily-exploitable  security  defect  is  not  in  the  same 
category  as  one  in  which  anybody  can  (remotely)  become 
SYSADMIN,  for  example.    Note:  it  is  true  that  lower 
severity  security  vulnerabilities  may  be  exploited  in  a  so-
called combined attack, but it is difficult to anticipate these 
and, absent an combined attack exploit in the wild, vendors 
are  better  off  using  an  objective  (stand-alone)  severity 
ranking for resource allocation purposes. 
190
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:06:29 UTC from IEEE Xplore.  Restrictions apply. 
actually 
a 
involved 
counting 
In  fact,  security  vulnerabilities  are  inherently  non-
comparable  unless  they  are  “rated”  according  to  a  severity 
ranking  such  as  the  CVSS.  While  there  is  inevitably  some 
judgment 
they 
nonetheless  enable  some  “apples  to  apples”  comparisons 
between  comparable  products  to  the  extent  all  product 
vulnerabilities are assigned a CVSS score by vendors.  
in  assigning  CVSS  scores, 
Non-comparability of product factoring or product size is 
also  a  consideration.  For  example,  comparing  numbers  of 
security vulnerabilities in competing products is not always 
useful to the extent that the products are either significantly 
different as regards features and functions, the size of code 
base, or in the way the product is factored (that is, extra-cost 
or  additional  product  functionality  may  be  bundled  in  one 
product where it is factored as a separate product by another 
vendor).  It  is,  in  a  way,  like  comparing  termite  inspection 
reports  from  a  20,000  square  foot  house  and  a  doghouse. 
You might know how many absolute numbers of bugs there 
are  in  each  structure,  but  it  doesn’t  tell  you  whether  the 
McMansion  can  truly  be  considered  “buggier”  than  the 
doghouse, or vice versa. 
Another difficulty  with  comparing  number of  published 
security vulnerabilities in products is that there is no standard 
(or  objective)  way  of 
security 
vulnerabilities. If a change to one function addresses the root 
cause  of  several  reported  security  bugs  (involving,  say, 
failure  to  validate  input  correctly),  is  that  to  be  counted  as 
one  security  bug  or  several  security  bugs?  A  common 
criticism  of  many  vendors  is  that  they  do  not,  upon 
reporting  vulnerabilities,  address 
addressing  externally 
“nearby”  vulnerabilities.  For 
reported 
vulnerability in an Apache mod should lead one to analyze 
other  Apache  mods  for  similar  problems.  Should  a  vendor 
who does so report “related” vulnerabilities in the same mod 
as part of a security advisory, the disclosure of what else was 
found (if the vendor discloses it) is likely to be held against 
the  vendor.  Ironically,  “correct”  behavior  (“look  for  other 
issues and fix them”) is likely to rebound more negatively on 
the vendor (at least in the short run) than would be the case if 
the vendor either doesn’t look for related issues, or looks for 
and fixes additional issues without disclosing what else was 
found and fixed. (One can argue about “silent fixing” vs. full 
disclosure, but the bottom line is that finding and fixing more 
issues earlier is generally in customers’ best interests.) 
To  that  point,  using  published  numbers  of  security 
vulnerabilities  as  a  competitive  comparison  is  inherently 
flawed as it relies to a large extent on self-disclosure by the 
vendor  and  thus  is  “rigged  to  be  gamed.”  Absent  knowing 
what  a  vendor’s  vulnerability  disclosure  policies  are  (for 
example,  do  they  self-disclose  issues  they  find  and  fix 
themselves?),  it  is  not  possible  to  make  any  meaningful 
comparison  based  on  “number  of  publicly  disclosed 
vulnerabilities.”  One  can  assume  that  vendors  find  some 
portion  of  security  vulnerabilities  themselves:  do  they 