our various loss types (Figure 2d) – higher detection rate at
low FPR brought about primarily by multi-objective binary
losses, a net AUC increase and a detection bump at 10−3
FPR brought about by the count loss, and a reduced vari-
ance brought about by many loss functions. To convince
ourselves that this is not a coincidence, we also trained a
network using only Poisson and vendor auxiliary losses but
no attribute tags (cf. Figure 3). As expected, we see that this
curve exhibits similar general shape and AUC characteristics
that occur when training with all loss terms, but the variance
appears slightly increased.
In the variance reduction sense, we can view our auxiliary
losses as regularizers. This raises a question: are improve-
ments 1 and 2 actually occurring for the reasons that we hy-
pothesize or are they merely naive result of regularization?
5.2 Representation or Regularization?
While the introduction of some kinds of auxiliary targets ap-
pears to improve the model’s performance on the main task,
it is less clear why this is the case. The reduction in variance
produced by the addition of extra targets suggests one po-
tential alternative explanation for the observed improvement:
rather than inducing a more discriminative representation in
the hidden layers of the network, the additional targets may
be acting as constraints on the network, reducing the space of
viable weights for the ﬁnal trained network, and thus acting
as a form of additional regularization. Alternatively, the ad-
dition of extra targets may simply be accelerating training by
amplifying the gradient; while this seems unlikely given our
use of a validation set to monitor approximate convergence,
we nevertheless also investigate this possibility.
To evaluate these hypotheses, we constructed three addi-
tional targets (and associated loss functions) that provided
uninformative targets to the model: i) a pseudo-random tar-
get that is approximately independent of either the input fea-
tures or the malware/benign label; ii) an additional copy of
the main malware target transformed to act as a regression
target; and iii) an extra copy of the main malware target.
The random target approach attempts to directly evaluate
whether or not an additional pseudo-random target might im-
prove network performance by ‘using up’ excess capacity
that might otherwise lead to overﬁtting. We generate pseudo-
random labels for each sample based off of the parity of a
hash of the ﬁle contents. While this value is effectively ran-
dom and independent of the actual malware/benignware la-
bel of the ﬁle, the use of a hash value ensures that a given
sample will always produce the same pseudo-random tar-
get. This target is ﬁt via standard binary cross-entropy loss
against a sigmoid output,
Lrnd(X,Y ) = − 1
M
M
∑
i=1
y(i) log frnd(x(i))+
(1− y(i))log
(cid:16)
(cid:17)
1− frnd(x(i))
,
(9)
(cid:16)
x(i)(cid:17)
where frnd
ﬁt to the random target y(i).
is the output of the network which is being
In contrast,
the duplicated regression target evaluates
whether further constraining the weights without requiring
excess capacity to model additional independent targets has
an effect on the performance on the main task. The model
is forced to adopt an internal representation that can satisfy
two different loss functions for perfectly correlated targets,
thus inducing a constraint that does not add additional infor-
mation. To do this, we convert our binary labels (taking on
values of 0 and 1 for benign and malware, respectively) to
-10 and 10, and add them as additional regression targets ﬁt
via mean squared error (MSE). Taking y(i) as the ith binary
target and fMSE(x(i)) as the regression output of the network,
we can express the MSE loss as:
(cid:16)
(cid:16)
x(i)(cid:17)− 20
(cid:16)
Lmse(X,Y ) =
M
∑
i=1
fmse
(cid:17)(cid:17)2
y(i) − 0.5
.
(10)
Finally, in the case of the duplicated target, the model ef-
fectively receives a larger gradient due to a duplication of the
loss. The loss for this label uses the same cross-entropy loss
as for the main target, obtained by substituting fdup(x(i)) for
fmal(x(i)) in equation 1 as the additional model output that
is ﬁt to the duplicated target. Note that we performed two
variants of the duplicated target experiment: one in which
both the dense layer prior to the main malware target and
the dense layer prior to the duplicated target were trainable,
and one in which the dense layer for the duplicate target was
frozen at its initialization values to avoid the trivial solution
where the pre-activation layer for both the main and dupli-
cate target were identical.
In both cases, the results were
equivalent; only results for the trainable case are shown.
Both frnd and fdup are obtained by applying a dense layer
followed by a sigmoid activation function to the intermediate
output of the input sample from the shared base layer (h in
Figure 1), while fmse(x(i)) is obtained by passing the inter-
mediate representation of the input sample h through a fully
connected layer with no output non-linearity.
Results of all three experiments are shown in Figure 4. In
no case did the performance of the model on the main task
improve statistically signiﬁcantly over the baseline. This
suggests that auxiliary tasks must encode relevant informa-
tion to improve the model’s performance on the main task.
For each of the three auxiliary loss types in Figure 4, there is
USENIX Association
28th USENIX Security Symposium    313
output as an auxiliary objective. They observed that adding
targets in this fashion increased performance both on the de-
tection task and on the malware family classiﬁcation task.
Our work builds upon theirs in several respects. First, while
their work used 4000-dimensional dynamic features derived
from Windows API calls, we extend multi-target approaches
to lower-dimensional static features on a larger data set. In
this respect, our work pioneers a more scalable approach,
but lacks the advantages of dynamic features that their ap-
proach provides. Second, we demonstrate that improvements
from multi-target learning also occur using far more targets,
and we introduce heterogeneous loss functions, i.e., binary
cross-entropy and Poisson, whereas their work employs only
two categorical cross-entropy losses. Finally, our work in-
troduces loss-weighting to account for potentially missing
labels, which may not be problematic for only two targets
but become more prevalent with additional targets.
Despite the lack of attention from the ML-Sec commu-
nity, multi-target learning has been applied to other areas of
ML for a long time. The work of Abu-Mostafa [2] predates
most explicit references to multi-task learning by introducing
the concept of hints, in which known invariances of a solu-
tion (e.g., translation invariance, invariance under negation)
can be incorporated into network structure and used to gen-
erate additional training samples by applying the invariant
operation to the existing samples, or – most relevant to our
work – used as an additional target by enforcing that samples
modiﬁed by an invariant function should be both correctly
classiﬁed and explicitly classiﬁed identically. Caruna [5]
ﬁrst introduced multi-task learning in neural networks as a
“source of inductive bias” (also reframed as inductive trans-
fer in [4]), in which more difﬁcult tasks could be combined
in order to exploit similarities between tasks that could serve
as complementary signals during training. While his work
predates the general availability of modern GPUs, and thus
the models and tasks he examines are fairly simple, Caruna
nevertheless demonstrates that jointly learning related tasks
produces better generalization on a task-by-task basis than
learning them individually. It is interesting to note that in
[5] he also demonstrated that learning multiple copies of the
same task can also lead to a modest improvement in perfor-
mance (which we did not observe in this work, possibly due
to the larger scale and complexity of our task).
Kumar and Duame [17] consider a reﬁnement on the basic
multi-task learning approach that leads to clustering related
tasks, in an effort to mitigate the potential of negative trans-
fer in which unrelated tasks degrade performance on the tar-
get task. Similarly, the work of Rudd et al. [22] explores the
use of domain-adaptive weighting of tasks during the train-
ing process.
Multi-target learning has been applied to extremely com-
plex image classiﬁcation tasks, including predicting charac-
ters and ngrams within unconstrained images of text [14],
joint facial landmark localization and detection [19], image
Figure 4: ROC curves comparing classiﬁcation capabilities
of models on the malware target when either random (green
dotted line), regression (red dashed line), or duplicated tar-
gets (magenta dashed and dotted line) are added as auxiliary
losses. Note that with the exception of the regression loss
– which appears to harm performance – there is little dis-
cernible difference between the remaining ROC curves. The
baseline is shown as a blue solid line for comparison.
no additional information provided by the auxiliary targets:
the random target is completely uncorrelated from any in-
formation in the ﬁle (and indeed the ﬁnal layer is ultimately
dominated by the bias weights and produces a constant out-
put of 0.5 regardless of the inputs to the layer), while the
duplicated and MSE layers are perfectly correlated with the
ﬁnal target. In either case, there is no incentive for the net-
work to develop a richer representation in layers closer to
the input; the ﬁnal layer alone is sufﬁcient given an adequate
representation in the core of the model.
6 Related Work
Applications of ML to computer security date back to the
1990’s [21], but large-scale commercial deployments of deep
neural networks (DNNs) that have led to transformative per-
formance gains are a more recent phenomenon. Several
works from the ML-Sec community have leveraged DNNs
for statically detecting malicious content across a variety of
different formats and ﬁle types [25, 26, 23]. However, these
works predominantly focus on applying regularized cross-
entropy loss functions between single network outputs and
malicious/benign labels per-sample, leaving the potential of
multiple-objective optimization largely untapped.
A notable exception, which we build upon in this work, is
[11], in which Huang et al. add a multiclass label for Mi-
crosoft’s malware families to their classiﬁcation model us-
ing a categorical cross-entropy loss function atop a softmax
314    28th USENIX Security Symposium
USENIX Association
tagging and retrieval [12, 31], and attribute prediction [1, 22]
where a common auxiliary task is to challenge the network
to classify additional attributes of the image, such as manner
of dress for full-body images of people or facial attributes
(e.g., smiling, narrow eyes, race, gender). While a range of
neural network structures are possible, common exemplars
include largely independent networks with a limited number
of shared weights (e.g., [1]), a single network with minimal
separation between tasks (e.g., [22]), or a number of parallel
single-task classiﬁers in which the weights are constrained to
be similar to each other. A more complex approach may be
found in [24], in which the sharing between tasks is learned
automatically in an online fashion.
Other, more distantly connected domains of ML research
reinforce the intuition that learning on disparate tasks can im-
prove model performance. Work in semi-supervised learn-
ing, such as [16] and [20], has shown the value of additional
reconstruction and denoising tasks to learn representations
valuable for a core classiﬁcation model, both through reg-
ularization and through access to a larger dataset than is
available with labels. The widespread success of transfer
learning is also a testament to the value of training a sin-
gle model on nominally distinct tasks. BERT [8], a recent
example from the Natural Language Processing literature,
shows strong performance gains from pre-training a model
on masked-word prediction and predictions of whether two
sentences appear in sequence, even when the true task of in-
terest is quite distinct (e.g. question answering, translation).
Multi-view learning (see [32] for a survey) is a related ap-
proach in which multiple inputs are trained to a single tar-
get. This approach also arguably leads to the same general
mechanism for improvement: the model is forced to learn
relationships between sets of features that improve the per-
formance using any particular set. While this approach often
requires all sets of features to be available at test time, there
are other approaches, such as [28], that relax this constraint.
7 Conclusion
In this paper, we have demonstrated the effectiveness of
auxiliary losses for malware classiﬁcation. We have also
provided experimental evidence which suggests that perfor-
mance gains result from an improved and more informa-
tive representation, not merely a regularization artifact. This
is consistent with our observation that improvements occur
as additional auxiliary losses and different loss types are
added. We also note that different loss types have differ-
ent effects on the ROC; multi-label vendor and semantic at-
tribute tag losses have greatest effect at low false positive
rates (≤ 10−3), while Poisson counts have a substantial net
impact on AUC, the bulk of which stems from detection
boosts at higher FPR.
While we experimented on PE malware in this paper, our
auxiliary loss technique could be applied to many other prob-
lems in the ML-Sec community, including utilizing a label
on format/ﬁle type for format-agnostic features (e.g., ofﬁce
document type in [23]) or ﬁle type under a given format, for
example APKs and JARs both share an underlying ZIP for-
mat; a zip-archive malware detector could use tags on the
ﬁle type for auxiliary targets. Additionally, tags on topics
and classiﬁcations of embedded URLs could serve as auxil-
iary targets when classifying emails or websites.
One open question is whether or not multiple auxiliary
losses improve each others’ performances as well as the main
task’s. If the multiple outputs of operational interest (such
as the tagging output) can be trained simultaneously while
also increasing (or at least not decreasing) their joint accu-
racy, this could lead to models that are both more compact
and more accurate than individually deployed ones. In addi-
tion to potential accuracy gains, this has signiﬁcant potential
operational beneﬁts, particularly when it comes to model de-
ployment and updates. We defer a more complete evaluation
of this question to future work.
While this work has focused on applying auxiliary losses
in the context of deep neural networks, there is nothing math-
ematically that precludes using them in conjunction with a
number of other classiﬁer types. Notably, gradient boosted
classiﬁer ensembles, which are also popular in the ML-Sec
community could take very similar auxiliary loss functions
even though the structure of these classiﬁers is much dif-
ferent. We encourage the ML-Sec research community to
implement multi-objective ensemble classiﬁers and compare
with our results. Our choice of deep neural networks for
this paper is infrastructural more than anything else; while
several deep learning platforms, including PyTorch, Keras,
and Tensorﬂow among others easily support multiple ob-
jectives and custom loss functions, popular boosting frame-
works such as lightGBM and XGBoost have yet to imple-
ment this functionality.
The analyses conducted herein used metadata that can nat-
urally be transformed into a label source and impart addi-
tional information to the classiﬁer with no extra data collec-
tion burden on behalf of the threat intelligence feed. More-
over, our auxiliary loss technique does not change the under-
lying feature space representation. Other types of metadata,
e.g., the ﬁle path of the malicious binary or URLs extracted
from within the binary might be more useful in a multi-view
context, serving as input to the classiﬁer, but this approach
raises challenges associated with missing data that our loss
weighting scheme trivially addresses. Perhaps our weight-
ing scheme could even be extended, e.g., by weighting each
sample’s loss contribution according to certainty/uncertainty
in that sample’s label, or re-balancing the per-task loss ac-
cording to the expected frequency of the label in the target
distribution. This could open up novel applications, e.g., de-
tectors customized to a particular user endpoints and remove
sampling biases inherent to multi-task data.
USENIX Association
28th USENIX Security Symposium    315
8 Acknowledgments
This research was sponsored by Sophos PLC. We would
additionally like to thank Adarsh Kyadige, Andrew Davis,
Hillary Sanders, and Joshua Saxe for their suggestions that
greatly improved this manuscript.
References
[1] ABDULNABI, A. H., WANG, G., LU, J., AND JIA,
K. Multi-task cnn model for attribute prediction. IEEE
Transactions on Multimedia 17, 11 (2015), 1949–1959.
[2] ABU-MOSTAFA, Y. S. Learning from hints in neural
networks. J. Complexity 6, 2 (1990), 192–198.
[3] ANDERSON, H. S., AND ROTH, P. Ember: an open
dataset for training static pe malware machine learning
models. arXiv preprint arXiv:1804.04637 (2018).
[4] CARUANA, R. A dozen tricks with multitask learning.
In Neural networks: tricks of the trade. Springer, 1998,
pp. 165–191.
[5] CARUNA, R. Multitask learning: A knowledge-based
source of inductive bias. In Machine Learning: Pro-
ceedings of the Tenth International Conference (1993),
pp. 41–48.
[6] CHOLLET, F., ET AL. Keras, 2015.
[7] CLEVERT, D.-A., UNTERTHINER, T., AND HOCHRE-
ITER, S. Fast and accurate deep network learning
arXiv preprint
by exponential linear units (elus).
arXiv:1511.07289 (2015).
[8] DEVLIN,
J., CHANG, M.-W., LEE, K., AND
TOUTANOVA, K. Bert: Pre-training of deep bidirec-
tional transformers for language understanding. arXiv
preprint arXiv:1810.04805 (2018).