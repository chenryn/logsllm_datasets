streams originate from the same person.
When an OR later replies to Alice with a relay cell, it en-
crypts the cell’s relay header and payload with the single key
it shares with Alice, and sends the cell back toward Alice
along the circuit. Subsequent ORs add further layers of en-
cryption as they relay the cell back to Alice.
To tear down a circuit, Alice sends a destroy control cell.
Each OR in the circuit receives the destroy cell, closes all
streams on that circuit, and passes a new destroy cell forward.
But just as circuits are built incrementally, they can also be
torn down incrementally: Alice can send a relay truncate cell
to a single OR on a circuit. That OR then sends a destroy cell
forward, and acknowledges with a relay truncated cell. Alice
can then extend the circuit to different nodes, without signal-
ing to the intermediate nodes (or a limited observer) that she
has changed her circuit. Similarly, if a node on the circuit
goes down, the adjacent node can send a relay truncated cell
back to Alice. Thus the “break a node and see which circuits
go down” attack [4] is weakened.
4.3 Opening and closing streams
When Alice’s application wants a TCP connection to a given
address and port, it asks the OP (via SOCKS) to make the
connection. The OP chooses the newest open circuit (or cre-
ates one if needed), and chooses a suitable OR on that circuit
to be the exit node (usually the last node, but maybe others
due to exit policy conﬂicts; see Section 6.2.) The OP then
opens the stream by sending a relay begin cell to the exit node,
using a new random streamID. Once the exit node connects
to the remote host, it responds with a relay connected cell.
Upon receipt, the OP sends a SOCKS reply to notify the ap-
plication of its success. The OP now accepts data from the
application’s TCP stream, packaging it into relay data cells
and sending those cells along the circuit to the chosen OR.
There’s a catch to using SOCKS, however—some applica-
tions pass the alphanumeric hostname to the Tor client, while
others resolve it into an IP address ﬁrst and then pass the IP
1Actually, the negotiated key is used to derive two symmetric keys: one
2With 48 bits of digest per cell, the probability of an accidental collision
for each direction.
is far lower than the chance of hardware failure.
address to the Tor client. If the application does DNS resolu-
tion ﬁrst, Alice thereby reveals her destination to the remote
DNS server, rather than sending the hostname through the Tor
network to be resolved at the far end. Common applications
like Mozilla and SSH have this ﬂaw.
With Mozilla, the ﬂaw is easy to address:
the ﬁltering
HTTP proxy called Privoxy gives a hostname to the Tor
client, so Alice’s computer never does DNS resolution. But
a portable general solution, such as is needed for SSH, is an
open problem. Modifying or replacing the local nameserver
can be invasive, brittle, and unportable. Forcing the resolver
library to prefer TCP rather than UDP is hard, and also has
portability problems. Dynamically intercepting system calls
to the resolver library seems a promising direction. We could
also provide a tool similar to dig to perform a private lookup
through the Tor network. Currently, we encourage the use of
privacy-aware proxies like Privoxy wherever possible.
Closing a Tor stream is analogous to closing a TCP stream:
it uses a two-step handshake for normal operation, or a one-
step handshake for errors. If the stream closes abnormally,
the adjacent node simply sends a relay teardown cell. If the
stream closes normally, the node sends a relay end cell down
the circuit, and the other side responds with its own relay end
cell. Because all relay cells use layered encryption, only the
destination OR knows that a given relay cell is a request to
close a stream. This two-step handshake allows Tor to support
TCP-based applications that use half-closed connections.
4.4 Integrity checking on streams
Because the old Onion Routing design used a stream cipher
without integrity checking, trafﬁc was vulnerable to a mal-
leability attack: though the attacker could not decrypt cells,
any changes to encrypted data would create corresponding
changes to the data leaving the network. This weakness al-
lowed an adversary who could guess the encrypted content to
change a padding cell to a destroy cell; change the destination
address in a relay begin cell to the adversary’s webserver; or
change an FTP command from dir to rm *. (Even an ex-
ternal adversary could do this, because the link encryption
similarly used a stream cipher.)
Because Tor uses TLS on its links, external adversaries
cannot modify data. Addressing the insider malleability at-
tack, however, is more complex.
We could do integrity checking of the relay cells at each
hop, either by including hashes or by using an authenticating
cipher mode like EAX [6], but there are some problems. First,
these approaches impose a message-expansion overhead at
each hop, and so we would have to either leak the path length
or waste bytes by padding to a maximum path length. Sec-
ond, these solutions can only verify trafﬁc coming from Al-
ice: ORs would not be able to produce suitable hashes for
the intermediate hops, since the ORs on a circuit do not know
the other ORs’ session keys. Third, we have already accepted
that our design is vulnerable to end-to-end timing attacks; so
tagging attacks performed within the circuit provide no addi-
tional information to the attacker.
Thus, we check integrity only at the edges of each stream.
(Remember that in our leaky-pipe circuit topology, a stream’s
edge could be any hop in the circuit.) When Alice negotiates
a key with a new hop, they each initialize a SHA-1 digest with
a derivative of that key, thus beginning with randomness that
only the two of them know. Then they each incrementally
add to the SHA-1 digest the contents of all relay cells they
create, and include with each relay cell the ﬁrst four bytes of
the current digest. Each also keeps a SHA-1 digest of data
received, to verify that the received hashes are correct.
To be sure of removing or modifying a cell, the attacker
must be able to deduce the current digest state (which de-
pends on all trafﬁc between Alice and Bob, starting with their
negotiated key). Attacks on SHA-1 where the adversary can
incrementally add to a hash to produce a new valid hash don’t
work, because all hashes are end-to-end encrypted across the
circuit. The computational overhead of computing the digests
is minimal compared to doing the AES encryption performed
at each hop of the circuit. We use only four bytes per cell
to minimize overhead; the chance that an adversary will cor-
rectly guess a valid hash is acceptably low, given that the OP
or OR tear down the circuit if they receive a bad hash.
4.5 Rate limiting and fairness
Volunteers are more willing to run services that can limit
their bandwidth usage. To accommodate them, Tor servers
use a token bucket approach [50] to enforce a long-term aver-
age rate of incoming bytes, while still permitting short-term
bursts above the allowed bandwidth.
Because the Tor protocol outputs about the same number
of bytes as it takes in, it is sufﬁcient in practice to limit only
incoming bytes. With TCP streams, however, the correspon-
dence is not one-to-one: relaying a single incoming byte can
require an entire 512-byte cell. (We can’t just wait for more
bytes, because the local application may be awaiting a reply.)
Therefore, we treat this case as if the entire cell size had been
read, regardless of the cell’s fullness.
Further, inspired by Rennhard et al’s design in [44], a cir-
cuit’s edges can heuristically distinguish interactive streams
from bulk streams by comparing the frequency with which
they supply cells. We can provide good latency for interactive
streams by giving them preferential service, while still giving
good overall throughput to the bulk streams. Such prefer-
ential treatment presents a possible end-to-end attack, but an
adversary observing both ends of the stream can already learn
this information through timing attacks.
4.6 Congestion control
5 Rendezvous Points and hidden services
Even with bandwidth rate limiting, we still need to worry
about congestion, either accidental or intentional. If enough
users choose the same OR-to-OR connection for their cir-
cuits, that connection can become saturated. For example,
an attacker could send a large ﬁle through the Tor network
to a webserver he runs, and then refuse to read any of the
bytes at the webserver end of the circuit. Without some con-
gestion control mechanism, these bottlenecks can propagate
back through the entire network. We don’t need to reimple-
ment full TCP windows (with sequence numbers, the abil-
ity to drop cells when we’re full and retransmit later, and
so on), because TCP already guarantees in-order delivery of
each cell. We describe our response below.
Circuit-level throttling: To control a circuit’s bandwidth
usage, each OR keeps track of two windows. The packaging
window tracks how many relay data cells the OR is allowed to
package (from incoming TCP streams) for transmission back
to the OP, and the delivery window tracks how many relay
data cells it is willing to deliver to TCP streams outside the
network. Each window is initialized (say, to 1000 data cells).
When a data cell is packaged or delivered, the appropriate
window is decremented. When an OR has received enough
data cells (currently 100), it sends a relay sendme cell towards
the OP, with streamID zero. When an OR receives a relay
sendme cell with streamID zero, it increments its packaging
window. Either of these cells increments the corresponding
window by 100. If the packaging window reaches 0, the OR
stops reading from TCP connections for all streams on the
corresponding circuit, and sends no more relay data cells until
receiving a relay sendme cell.
The OP behaves identically, except that it must track a
packaging window and a delivery window for every OR in
the circuit. If a packaging window reaches 0, it stops reading
from streams destined for that OR.
Stream-level throttling: The stream-level congestion con-
trol mechanism is similar to the circuit-level mechanism. ORs
and OPs use relay sendme cells to implement end-to-end ﬂow
control for individual streams across circuits. Each stream
begins with a packaging window (currently 500 cells), and
increments the window by a ﬁxed value (50) upon receiv-
ing a relay sendme cell. Rather than always returning a relay
sendme cell as soon as enough cells have arrived, the stream-
level congestion control also has to check whether data has
been successfully ﬂushed onto the TCP stream; it sends the
relay sendme cell only when the number of bytes pending to
be ﬂushed is under some threshold (currently 10 cells’ worth).
These arbitrarily chosen parameters seem to give tolerable
throughput and delay; see Section 8.
Rendezvous points are a building block for location-hidden
services (also known as responder anonymity) in the Tor net-
work. Location-hidden services allow Bob to offer a TCP ser-
vice, such as a webserver, without revealing his IP address.
This type of anonymity protects against distributed DoS at-
tacks: attackers are forced to attack the onion routing network
because they do not know Bob’s IP address.
Our design for location-hidden servers has the following
goals. Access-control: Bob needs a way to ﬁlter incoming
requests, so an attacker cannot ﬂood Bob simply by mak-
ing many connections to him. Robustness: Bob should be
able to maintain a long-term pseudonymous identity even in
the presence of router failure. Bob’s service must not be tied
to a single OR, and Bob must be able to migrate his service
across ORs. Smear-resistance: A social attacker should not
be able to “frame” a rendezvous router by offering an ille-
gal or disreputable location-hidden service and making ob-
servers believe the router created that service. Application-
transparency: Although we require users to run special soft-
ware to access location-hidden servers, we must not require
them to modify their applications.
We provide location-hiding for Bob by allowing him to
advertise several onion routers (his introduction points) as
contact points. He may do this on any robust efﬁcient key-
value lookup system with authenticated updates, such as a
distributed hash table (DHT) like CFS [11].3 Alice, the client,
chooses an OR as her rendezvous point. She connects to one
of Bob’s introduction points, informs him of her rendezvous
point, and then waits for him to connect to the rendezvous
point. This extra level of indirection helps Bob’s introduc-
tion points avoid problems associated with serving unpopular
ﬁles directly (for example, if Bob serves material that the in-
troduction point’s community ﬁnds objectionable, or if Bob’s
service tends to get attacked by network vandals). The ex-
tra level of indirection also allows Bob to respond to some
requests and ignore others.
5.1 Rendezvous points in Tor
The following steps are performed on behalf of Alice and Bob
by their local OPs; application integration is described more
fully below.
service.
• Bob generates a long-term public key pair to identify his
• Bob chooses some introduction points, and advertises
them on the lookup service, signing the advertisement
with his public key. He can add more later.
• Bob builds a circuit to each of his introduction points,
and tells them to wait for requests.
3Rather than rely on an external infrastructure, the Onion Routing net-
work can run the lookup service itself. Our current implementation provides
a simple lookup system on the directory servers.
• Alice learns about Bob’s service out of band (perhaps
Bob told her, or she found it on a website). She retrieves
the details of Bob’s service from the lookup service. If
Alice wants to access Bob’s service anonymously, she
must connect to the lookup service via Tor.
• Alice chooses an OR as the rendezvous point (RP) for
her connection to Bob’s service. She builds a circuit
to the RP, and gives it a randomly chosen “rendezvous
cookie” to recognize Bob.
• Alice opens an anonymous stream to one of Bob’s intro-
duction points, and gives it a message (encrypted with
Bob’s public key) telling it about herself, her RP and ren-
dezvous cookie, and the start of a DH handshake. The
introduction point sends the message to Bob.
• If Bob wants to talk to Alice, he builds a circuit to Al-
ice’s RP and sends the rendezvous cookie, the second
half of the DH handshake, and a hash of the session key
they now share. By the same argument as in Section 4.2,
Alice knows she shares the key only with Bob.
• The RP connects Alice’s circuit to Bob’s. Note that RP
• Alice sends a relay begin cell along the circuit. It arrives
• An anonymous stream has been established, and Alice
can’t recognize Alice, Bob, or the data they transmit.
at Bob’s OP, which connects to Bob’s webserver.
and Bob communicate as normal.
When establishing an introduction point, Bob provides the
onion router with the public key identifying his service. Bob
signs his messages, so others cannot usurp his introduction
point in the future. He uses the same public key to establish
the other introduction points for his service, and periodically
refreshes his entry in the lookup service.
The message that Alice gives the introduction point in-
cludes a hash of Bob’s public key and an optional initial au-
thorization token (the introduction point can do prescreening,
for example to block replays). Her message to Bob may in-
clude an end-to-end authorization token so Bob can choose
whether to respond. The authorization tokens can be used
to provide selective access: important users can get uninter-
rupted access. During normal situations, Bob’s service might
simply be offered directly from mirrors, while Bob gives
out tokens to high-priority users. If the mirrors are knocked
down, those users can switch to accessing Bob’s service via
the Tor rendezvous system.
Bob’s introduction points are themselves subject to DoS—
he must open many introduction points or risk such an at-
tack. He can provide selected users with a current list or fu-