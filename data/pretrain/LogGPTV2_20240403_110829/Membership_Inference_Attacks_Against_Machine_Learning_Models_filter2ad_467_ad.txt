the largest
respectively) is 0.6 and 0.2 for CIFAR-10 and CIFAR-100,
respectively. The accuracy is low, indicating that the models
are heavily overﬁtted on their training sets. Figure 4 shows
the results of the membership inference attack against the
CIFAR models. For both CIFAR-10 and CIFAR-100,
the
attack performs much better than the baseline, with CIFAR-
100 especially vulnerable.
Table I shows the training and test accuracy of the models
constructed using different machine learning platforms for the
purchase dataset with 100 classes. Large gaps between training
and test accuracy indicate overﬁtting. Larger test accuracy
indicates better generalizability and higher predictive power.
Figure 5 shows the results of the membership inference
attack against the black-box models trained by Google’s and
Amazon’s machine learning platforms. Figure 7 compares
precision of the attacks against these models with the attacks
against a neural-network model trained on the same data. Mod-
els trained using Google Prediction API exhibit the biggest
leakage.
For the Texas hospital-stay dataset, we evaluated our attack
against a Google-trained model. The training accuracy of the
target model is 0.66 and its test accuracy is 0.51. Figure 6
shows the accuracy of membership inference. Precision is
mostly above 0.6, and for half of the classes, it is above 0.7.
Precision is above 0.85 for more than 20 classes.
For the location dataset, we evaluated our attacks against
a Google-trained model. The training accuracy of the target
model is 1 and its test accuracy is 0.66. Figure 8 shows the
accuracy of membership inference. Precision is between 0.6
and 0.8, with an almost constant recall of 1.
E. Effect of the shadow training data
Figure 8 reports precision of the attacks trained on the
shadow models whose training datasets are noisy versions of
the real data (disjoint from the target model’s training dataset
but sampled from the same population). Precision drops as the
amount of noise increases, but the attack still outperforms the
11
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:20:35 UTC from IEEE Xplore.  Restrictions apply. 
ML Platform
Google
Amazon (10,1e-6)
Amazon (100,1e-4)
Neural network
Training
0.999
0.941
1.00
0.830
Test
0.656
0.468
0.504
0.670
TABLE I: Training and test accuracy of the models constructed using
different ML-as-a-service platforms on the purchase dataset (with 100
classes).
Location Dataset, Google, Membership Inference Attack
Real Data
Noisy Data 10%
Noisy Data 20%
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
s
e
s
s
a
C
l
f
o
n
o
i
t
c
a
r
F
e
v
i
t
l
a
u
m
u
C
 0
 0
 0.2
 0.4
 0.6
Precision
 0.8
 1
Fig. 8: Empirical CDF of the precision of the membership inference
attack against the Google-trained model for the location dataset.
Results are shown for the shadow models trained on real data and for
the shadow models trained on noisy data with 10% and 20% noise
(i.e., x% of features are replaced with random values). Precision of
the attack over all classes is 0.678 (real data), 0.666 (data with 10%
noise), and 0.613 (data with 20% noise). The corresponding recall
of the attack is 0.98, 0.99, and 1.00, respectively.
baseline and, even with 10% of the features in the shadows’
training data replaced by random values, matches the original
attack. This demonstrates that our attacks are robust even
if the attacker’s assumptions about the distribution of the
target model’s training data are not very accurate.
Figure 9 reports precision of the attacks when the attacker
has no real data (not even noisy) for training his shadow mod-
els. Instead, we used the marginal distributions of individual
features to generate 187, 300 synthetic purchase records, then
trained 20 shadow models on these records.
We also generated 30, 000 synthetic records using the
model-based approach presented in Algorithm 1. In our ex-
periments with the purchase dataset where records have 600
binary features, we initialize k to kmax = 128 and divide it
by 2 when rejmax = 10 subsequent proposals are rejected.
We set its minimum value kmin = 4. In the sampling phase,
we set the minimum conﬁdence threshold confmin to 0.2.
For our ﬁnal set of sampled records, the target model’s
conﬁdence in classifying the records is 0.24 on average (just
a bit over our threshold confmin = 0.2). On average, each
synthetic record needed 156 queries (of proposed records)
during our hill-climbing two-phase process (see Section V-C).
We trained 8 shadow models on this data.
Figure 9 compares precision of the attacks when shadow
models are trained on real data versus shadow models trained
12
Purchase Dataset, Google, Membership Inference Attack
Real Data
Marginal-Based Synthetic
Model-Based Synthetic
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
s
e
s
s
a
C
l
f
o
n
o
i
t
c
a
r
F
e
v
i
t
a
u
m
u
C
l
 0
 0
 0.2
 0.4
 0.6
Precision
 0.8
 1
Fig. 9: Empirical CDF of the precision of the membership inference
attack against the Google-trained model for the purchase dataset.
Results are shown for different ways of generating training data for
the shadow models (real, synthetic generated from the target model,
synthetic generated from marginal statistics). Precision of the attack
over all classes is 0.935 (real data), 0.795 (marginal-based synthetic
data), and 0.896 (model-based synthetic data). The corresponding
recall of the attack is 0.994, 0.991, and 0.526, respectively.
on synthetic data. The overall precision is 0.935 on real data
compared to 0.795 for marginal-based synthetics and 0.895
for model-based synthetics. The accuracy of the attack using
marginal-based synthetic data is noticeably reduced versus real
data, but is nevertheless very high for most classes. The attack
using model-based synthetic data exhibits dual behavior. For
most classes its precision is high and close to the attacks
that use real data for shadow training, but for a few classes
precision is very low (less than 0.1).
The reason for the attack’s low precision on some classes
is that the target classiﬁer cannot conﬁdently model the dis-
tribution of data records belonging to these classes—because
it has not seen enough examples. These classes are under-
represented in the target model’s training dataset. For example,
each of the classes where the attack has less than 0.1 precision
contributes under 0.6% of the target model’s training dataset.
Some of these classes have fewer than 30 training records (out
of 10, 000). This makes it very difﬁcult for our algorithm to
synthesize representatives of these classes when searching the
high-dimensional space of possible records.
For the majority of the target model’s classes, our attack
achieves high precision. This demonstrates that a membership
inference attack can be trained with only black-box access
to the target model, without any prior knowledge about
the distribution of the target model’s training data if the
attacker can efﬁciently generate inputs that are classiﬁed by
the target model with high conﬁdence.
F. Effect of the number of classes and training data per class
The number of output classes of the target model contributes
to how much the model leaks. The more classes, the more
signals about the internal state of the model are available to
the attacker. This is one of the reasons why the results in Fig. 4
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:20:35 UTC from IEEE Xplore.  Restrictions apply. 
Purchase Dataset, Google, Membership Inference Attack
 1
 0.9
 0.8
 0.7
 0.6
 0.5
i
i
n
o
s
c
e
r
P
k
c
a
t
t
A
2
20
10
50
Number of Classes
100
Fig. 10: Precision of the membership inference attack against differ-
ent purchase classiﬁcation models trained on the Google platform.
The boxplots show the distribution of precision over different classi-
ﬁcation tasks (with a different number of classes).
are better for CIFAR-100 than for CIFAR-10. The CIFAR-
100 model is also more overﬁtted to its training dataset. For
the same number of training records per class, the attack
performs better against CIFAR-100 than against CIFAR-10.
For example, compare CIFAR-10 when the size of the training
dataset is 2, 000 with CIFAR-100 when the size of the training
dataset is 20, 000. The average number of data records per
class is 200 in both cases, but the attack accuracy is much
better (close to 1) for CIFAR-100.
that
To quantify the effect
the number of classes has
on the accuracy of the attack, we trained target models
using Google Prediction API on the purchase dataset with
{2, 10, 20, 50, 100} classes. Figure 10 shows the distribution
of attack precision for each model. Models with fewer classes
leak less information about
their training inputs. As the
number of classes increases, the model needs to extract more
distinctive features from the data to be able to classify inputs
with high accuracy. Informally, models with more output
classes need to remember more about their training data, thus
they leak more information.
Figure 11 shows the relationship between the amount of
training data per class and the accuracy of membership infer-
ence. This relationship is more complex, but, in general, the
more data in the training dataset is associated with a given
class, the lower the attack precision for that class.
Table II shows the precision of membership inference
against Google-trained models. For the MNIST dataset, the
training accuracy of the target model is 0.984 and its test
accuracy is 0.928. The overall precision of the membership
inference attack is 0.517, which is just slightly above random
guessing. The lack of randomness in the training data for each
class and the small number of classes contribute to the failure
of the attack.
For the Adult dataset, the training accuracy of the target
model is 0.848 and its test accuracy is 0.842. The overall
precision of the attack is 0.503, which is equivalent to random
13
Dataset
Adult
MNIST
Location
Purchase (2)
Purchase (10)
Purchase (20)
Purchase (50)
Purchase (100)
TX hospital stays
Training
Accuracy
0.848
0.984
1.000
0.999
0.999
1.000
1.000
0.999
0.668
Testing
Accuracy
0.842
0.928
0.673
0.984
0.866
0.781
0.693
0.659
0.517
Attack
Precision
0.503
0.517
0.678
0.505
0.550
0.590
0.860
0.935
0.657
TABLE II: Accuracy of the Google-trained models and the corre-
sponding attack precision.
guessing. There could be two reasons for why membership
inference fails against this model. First, the model is not
overﬁtted (its test and train accuracies are almost the same).
Second, the model is a binary classiﬁer, which means that the
attacker has to distinguish members from non-members by
observing the behavior of the model on essentially 1 signal,
since the two outputs are complements of each other. This
is not enough for our attack to extract useful membership
information from the model.
G. Effect of overﬁtting
The more overﬁtted a model, the more it leaks—but only
for models of the same type. For example,
the Amazon-
trained (100, 1e− 4) model that, according to Table I, is more
overﬁtted leaks more than the Amazon-trained (10, 1e − 6)
model. However, they both leak less than the Google-trained
model, even though the Google model is less overﬁtted than
one of the Amazon models and has a much better predictive
power (and thus generalizability) than both Amazon models.
Therefore, overﬁtting is not the only factor that causes
a model to be vulnerable to membership inference. The
structure and type of the model also contribute to the problem.
In Figure 11, we look deeper into the factors that contribute
including how overﬁtted the
to attack accuracy per class,
model is and what fraction of the training data belongs to each
class. The (train-test) accuracy gap is the difference between
the accuracy of the target model on its training and test data.
Similar metrics are used in the literature to measure how
overﬁtted a model is [18]. We compute this metric for each
class. Bigger gaps indicate that the model is overﬁtted on its
training data for that class. The plots show that, as expected,
bigger (train-test) accuracy gaps are associated with higher
precision of membership inference.
VII. WHY OUR ATTACKS WORK
Table II shows the relationship between the accuracy of
our membership inference attack and the (train-test) gap of
the target models. Figure 12 also illustrates how the target
models’ outputs distinguish members of their training datasets
from the non-members. This is the information that our attack
exploits.
Speciﬁcally, we look at how accurately the model predicts
the correct label as well as its prediction uncertainty. The ac-
curacy for class i is the probability that the model classiﬁes an
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:20:35 UTC from IEEE Xplore.  Restrictions apply. 
Purchase Dataset, 10-100 Classes, Google, Membership Inference Attack
Purchase Dataset, 10-100 Classes, Google, Membership Inference Attack
i
i
n
o
s
c
e
r
P
k
c
a
t
t
A
 1