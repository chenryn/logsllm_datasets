### Optimized Text

The largest accuracy gap for the CIFAR-10 and CIFAR-100 datasets is 0.6 and 0.2, respectively, indicating that the models are heavily overfitted to their training sets. Figure 4 illustrates the results of the membership inference attack on these CIFAR models. The attack performs significantly better than the baseline, with CIFAR-100 being particularly vulnerable.

Table I presents the training and test accuracies of models constructed using different machine learning platforms for the purchase dataset, which contains 100 classes. Large discrepancies between training and test accuracies suggest overfitting. Higher test accuracy indicates better generalizability and predictive power. Figure 5 shows the results of the membership inference attack on black-box models trained by Google's and Amazon's machine learning platforms. Figure 7 compares the precision of these attacks with those against a neural network model trained on the same data. Models trained using Google Prediction API exhibit the most significant leakage.

For the Texas hospital-stay dataset, we evaluated our attack against a Google-trained model. The target model has a training accuracy of 0.66 and a test accuracy of 0.51. Figure 6 displays the accuracy of the membership inference. Precision is mostly above 0.6, and for half of the classes, it exceeds 0.7. For more than 20 classes, precision is above 0.85.

For the location dataset, we also evaluated our attacks against a Google-trained model. The training accuracy of the target model is 1, while its test accuracy is 0.66. Figure 8 shows the accuracy of the membership inference. Precision ranges from 0.6 to 0.8, with an almost constant recall of 1.

### Effect of Shadow Training Data
Figure 8 reports the precision of attacks trained on shadow models whose training datasets are noisy versions of the real data (disjoint from the target model’s training dataset but sampled from the same population). As the noise increases, precision drops, but the attack still outperforms the baseline. Even with 10% of the features in the shadow models' training data replaced by random values, the attack matches the original performance. This demonstrates the robustness of our attacks, even if the attacker's assumptions about the distribution of the target model’s training data are not very accurate.

Figure 9 compares the precision of attacks when shadow models are trained on real data versus synthetic data. The overall precision is 0.935 on real data, 0.795 for marginal-based synthetic data, and 0.895 for model-based synthetic data. The attack using marginal-based synthetic data shows reduced accuracy compared to real data but remains high for most classes. The model-based synthetic data attack exhibits dual behavior: high precision for most classes but very low for a few underrepresented classes. These underrepresented classes have fewer than 30 training records, making it challenging to synthesize representatives of these classes.

### Effect of the Number of Classes and Training Data per Class
The number of output classes in the target model affects the amount of information leaked. More classes provide more signals about the internal state of the model, contributing to better attack performance. This is why the results in Figure 4 are better for CIFAR-100 than for CIFAR-10. The CIFAR-100 model is also more overfitted to its training dataset. For the same number of training records per class, the attack performs better against CIFAR-100 than against CIFAR-10.

To quantify this effect, we trained target models using Google Prediction API on the purchase dataset with {2, 10, 20, 50, 100} classes. Figure 10 shows the distribution of attack precision for each model. Models with fewer classes leak less information. As the number of classes increases, the model needs to extract more distinctive features, leading to more information leakage.

Figure 11 illustrates the relationship between the amount of training data per class and the accuracy of membership inference. Generally, more data per class in the training dataset is associated with lower attack precision for that class.

### Effect of Overfitting
Overfitting contributes to information leakage, but it is not the only factor. For example, the Amazon-trained (100, 1e−4) model, which is more overfitted, leaks more than the Amazon-trained (10, 1e−6) model. However, both leak less than the Google-trained model, despite the Google model being less overfitted and having better predictive power. Therefore, the structure and type of the model also play a role.

Figure 11 delves into the factors contributing to attack accuracy per class, including the (train-test) accuracy gap and the fraction of training data per class. Bigger gaps indicate overfitting, and as expected, larger gaps are associated with higher precision of membership inference.

### Why Our Attacks Work
Table II and Figure 12 illustrate the relationship between the accuracy of our membership inference attack and the (train-test) gap of the target models. Specifically, we examine how accurately the model predicts the correct label and its prediction uncertainty. The accuracy for class \(i\) is the probability that the model correctly classifies an input from that class. The plots show that the target models’ outputs distinguish members of their training datasets from non-members, which is the information our attack exploits.