adding the perturbation on the speech part; and (b) adding
the perturbation on the non-speech part.
in speaker recognition or the convolution neural network struc-
ture used in speech command recognition). However, the extracted
feature-map of each frame is usually aggregated via a statistic pool-
ing layer or a fully-connected layer before feeding it to the classifier
or softmax layer where the final prediction is made. Therefore, by
adding adversarial perturbation to only part of the speech signal,
the adversary could influence the feature-map extracted from the
corresponding speech frames and in turn potentially affect the final
recognition result.
To further verify the feasibility of adding a short segment of
adversarial perturbation to fool the model, we conducted a prelimi-
nary experiment on the speaker recognition system (i.e., X-vectors
system [44]) using 10 enrolled speakers of the VCTK corpus dataset
[48]. Then we solved the optimal adversarial perturbation through
the same optimization process formulated in Equation 2, where
we use the categorical cross entropy as the loss function. In this
experiment, as shown in Figure 3, we intentionally generate two
adversarial perturbations acting on the speech-part and non-speech-
part of the audio input respectively, to force the model recognize
Spk-1 (i.e., the actual label of the audio input) as the target label Spk-
8. The added adversarial perturbation is only about 0.16 seconds
and can be applied to either speech-part signals or non-speech-part
signals (i.e., natural pauses between words). The initial success of
this attack shows the potential of synchronization-free adversarial
perturbation: adding a very short perturbation regardless of its
position on the audio input to fool the model.
Subsecond Synchronization-free Adversarial Perturbation Gen-
4.1.2
eration. With the inherent temporal constraint bypassed, we now
discuss how to generate a subsecond audio adversarial perturba-
tion δ ∈ [−1, 1]l (l ≪ n) that is robust to various unsynchronized
conditions. Inspired by the Expectation Over Transformation (EOT)
technique [6] proposed for synthesizing robust visual adversarial
examples, we incorporate the time shifting caused under unsyn-
chronized conditions into the adversarial perturbation generation
process. Instead of directly solving the adversarial perturbation
acting at a specific timestamp, we seek to minimize the expected
effective loss when the perturbation is shifted by a delay of τ, where
τ obeys the uniform distribution between the time interval 0 and
n − l. Therefore, Equation 2 would become:
(3)
minimize
where Shi f t(δ, τ) shifts the adversarial perturbation δ by time τ. To
maximize the degree of synchronization-free, we sample random
(cid:2)L(cid:0)f(cid:0)x + Shi f t(δ, τ)(cid:1), yt
(cid:1) + α · ||δ||2(cid:3),
τ∼U (0,n−l)
E
4.2 Universal Audio Adversarial Perturbation
In the practical steaming-speech attack scenario, prior knowledge
on the audio input is usually inaccessible. We therefore need to
further circumvent the constraint on the prior knowledge of audio
input to enable the attack to be launched in a real-time manner.
Our goal is to find a universal subsecond adversarial perturbation
δ ∈ [−1, 1]l computed from a relatively small set of training data
samples to force the model to recognize arbitrary new audio input
(e.g., streaming audio input) as the target label with high probability.
Let µ denote the distribution of the training data samples, which are
a set of utterances spoken by the actual speaker for attacking speaker
recognition systems or a set of utterances of the actual command
for attacking speech command classification systems. We thus aim
to find a universal perturbation to fool the model on almost all the
audio inputs sampled from µ and alter the classification result to the
desired target class with a high probability (i.e.,∀x ∼ µ, P(cid:0)f (x +δ) =
(cid:1) → 1). It worth noting that, unlike existing untargeted universal
prediction (i.e., ∀x ∼ µ, P(cid:0)f (x +δ) (cid:44) f (x)(cid:1) → 1), we seek to launch
yt
attacks [35, 47] where the goal is to fool the classifier to make false
a targeted universal attack, which allows the adversary to pick the
target class that they desire. We believe this is a more harmful type
of attack as it offers the adversary the ability to control the attack
outcome (e.g., making their voice to be recognized as a specific
speaker with access privilege).
E
x∼µ,τ∼U (0,n−l)
(cid:2)L(cid:0)f(cid:0)x +Shi f t(δ, τ)(cid:1), yt
To coin such an attack, we used a penalty-based method to find
the universal adversarial perturbation by optimizing the following
expectation function over a set of training data samples sampled
from the distribution µ:
minimize
Let D = {(x1, y1), ...,(xk , yk)} be a set of training data sampled
from µ. To approximate the true data distribution with its sample
distribution, we iterate through each data sample to update the
adversarial perturbation δ by applying gradients calculated from
Equation 3. The process is repeated for several epochs until the
desired attack success rate is reached.
(cid:1) +α·||δ||2(cid:3) . (4)
To ensure the validity of the audio produced from the above pro-
cess, we need to enforce a box constraint on the output adversarial
perturbation as well as the adversarial example: δ ∈ [−1, 1]l , x′ =
x + δ ∈ [−1, 1]n. This can be achieved through projected gradient
descent, which clips the value of the audio after each iteration to be
within the set range. However, this could yield non-optimal results
in complicated update steps [9], especially in our case where multi-
level clipping is needed. To mitigate this issue, we introduce a new
variable z, where δ = tanh(z). This changes the box-constrained
optimization on δ to unconstrained optimization on z. The pseu-
docode of the proposed algorithm is described in Algorithm 1,
00.511.5Time (s)−0.6−0.4−0.20.00.20.40.6AmplitudeAudio InputAdversarial Perturbation00.511.5Time (s)−0.6−0.4−0.20.00.20.40.6AmplitudeAudio InputAdversarial Perturbationsired target class yt , hyperparameters α , β
Algorithm 1 Universal and Synchronization-free Adversarial Per-
turbation Generation (SGD is used for simplicity)
Input: Data samples D = {(x1, y1), ..., (xk , yk)}, target model f (·), de-
Output: Universal and Synchronization-free Adversarial Perturbation δ
1: Randomly initialize z ← [0, 1]l
2: for number of epochs do
3:
4:
5:
6:
7:
8:
τ ← U (0, n − l)
for each data sample (xi , yi) ∈ D do
δ ← tanh(z)
δ ← Shif t(δ , τ)
x′ ← clip(x + δ , [−1, 1])
Ltot al ← L(cid:0)f (x′), yt
▷ Get constrained audio perturbation
▷ Time shifting
▷ Craft adversarial example
▷ Compute total
(cid:1) + α · ||δ ||2
▷ Sample random time delay
▷ Update perturbation via z
loss via Equation 3
z ← z − β · ∂Ltot al
∂z
9:
10:
11: end for
end for
where stochastic gradient descent (SGD) [27] is used for simplicity.
In practice, this method could work with other gradient-based op-
timization algorithms, such as RMSProp [18], Adam [28], Nadam
[15], or AdaGuard [16]. We empirically choose to use Adam for
faster convergence.
4.3 Environmental Sound Mimicking
To further make the attack less suspicious, we tailored the gener-
ated adversarial perturbation according to environmental sounds.
Specifically, the adversary can craft adversarial perturbation to
sound like any situational sound that would normally appear in
the environment (e.g., bird singing, car horns, or HVAC nosies). For
a chosen environmental sound template ˆδ, we introduce another
term to Equation 4 to penalize the shape difference between the
adversarial perturbation and the sound template:
(cid:2)L(cid:0)f(cid:0)x + Shi f t(δ, τ)(cid:1), yt
(cid:1)
+ α · ||δ||2 + γ · dist(δ, ˆδ)(cid:3),
(5)
minimize
E
x∼µ,τ∼U (0,n−l)
where dist(δ, ˆδ) denotes the measured distance between the two
audio signal according to a chosen distance metric. As shown in Ap-
pendix A.3, after comparing 4 different metrics, we chose to use the
L2 distance between the two time-series signals for environmental
sound mimicking.
4.4 Robust Adversarial Perturbation for
Over-the-air Attack
In practice, the crafted adversarial perturbation played by a loud-
speaker will experience heavy distortions incurred by signal atten-
uation, multi-path effect, and ambient noises when propagating
over the air. Such an inevitable audio distortion would make the
perturbation lose its effectiveness, with a high possibility. In this
subsection, we enhance the robustness of the generated adversarial
perturbation to enable physical over-the-air attacks by incorporat-
ing the effects of speaker & microphone limitation, absorption and
reverberation and ambient noise into the adversarial perturbation
generation process.
minimize
E
x∼µ,τ∼U (0,n−l)
4.4.1 Using Band-pass Filter to Cope with Speaker & Microphone
Limitations. Audio devices such as loudspeakers and microphones
are normally designed to work in the human audible frequency
range (e.g., 20Hz to 20kHz). However, due to hardware limitations,
most loudspeakers and microphones do not respond to these fre-
quencies uniformly, which would cause the relative amplification
in some frequency ranges and attenuation in others, in turn affect-
ing the performance of the attack. For instance, we measure the
frequency response by playing a chirp signal ranging from 2Hz to
20kHz through the Bose Companion 2 speakers and recording from
an omni-directional microphone as shown in Appendix Figure 13.
We can observe a clear attenuation in lower frequencies (i.e., below
50Hz). To mitigate this effect, we imposed a band-pass filter during
the adversarial perturbation generation process as a constrain to
limit the adversarial perturbation to be in the valid frequency range:
(cid:2)L(cid:0)f (ˆx), yt
(cid:1)
+ α · ||δ||2 + γ · dist(δ, ˆδ)(cid:3),
(cid:0)Shi f t(δ, τ)(cid:1) and
BPF
(6)
BPF
50∼8000H z
50∼8000H z
where ˆx = x +
denotes the
band-pass filter operation. We chose 8kHz as the upper cutoff fre-
quency because it is the Nyquist frequency of 16kHz sampling rate,
which is commonly used in intelligent audio systems (e.g., Google
Assistant SDK [42]).
4.4.2 Using Room Impulse Response to Cope with Absorption and
Reverberation. During the over-the-air propagation, the surround-
ing environment will lead to absorption and reverberation, causing
the received audio signal to be very different from the original trans-
mitted signal. The room impulse response (RIR) models the transfer
function between the sound source and the received sound at the
microphone end, thus it can be used to emulate the over-the-air
distortions in the adversarial perturbation optimization process to
enhance the attack’s robustness. The RIR can vary largely according
to different room layouts, the position of the sound source and the
microphone, and the absorbent nature of each reflective surfaces.
Thus, we use a group of real RIRs collected in various environments
to improve the robustness of the generated adversarial perturbation
regardless of the launching condition and environment. Specifically,
we utilized the REVERB challenge database [29], the RWCP sound
scene database [34], and the Aachen impulse response database [26],
resulting in a total number of 218 physically measured RIRs under
different room layout (e.g., small, medium, large room). Through
integrating these RIRs, Equation 6 becomes:
(cid:2)L(cid:0)f (ˆx), yt
(cid:1) + α · ||δ||2
+ γ · dist(δ, ˆδ)(cid:3),
(cid:0)Shi f t(δ, τ)(cid:1) ⊗ h, and h is the RIR sampled
(7)
50∼8000H z
where ˆx = x + BPF
from the collected RIR distribution H. It’s worth noting that if the
adversary can anticipate the layout of the attacking environment
(e.g., rough room size) where the attack would be launched, the
adversary can further improve the attack performance by narrow-
ing down the available RIRs to a specific subset that is specifically
collected to reflect the attack environment.
4.4.3 Mitigating the Effect of Ambient Noise. In practice, ambient
noise is inevitable during recording and is highly variable, rang-
ing from continuous environmental white/pink noise (e.g., traffic
minimize
E
x∼µ,τ∼U (0,n−l),h∼H
Figure 4: Performance of the digital attack on the speaker
recognition model.
Figure 5: Performance of the digital attack on the speech
command recognition model.
minimize
x∼µ,τ∼U (0,n−l),h∼H ,w∼W
E
noise, rain sound, engine noise and air conditioning noise) to sud-
den sounds (e.g., phone ring, extraneous speech). Taking this into
account, we utilized a set of collected ambient noise and randomly
sampled an individual noise at each optimization step to solve:
(cid:2)L(cid:0)f (ˆx), yt
(cid:1) + α · ||δ||2
+ γ · dist(δ, ˆδ)(cid:3),
(cid:0)Shi f t(δ, τ)(cid:1) ⊗ h + w and w is the ambient
50∼8000H z
where ˆx = x + BPF
noise sampled from the noise dataset W . Specifically, we use 92
isotropic noise samples collected in different room layouts (e.g.,
small, medium, large) from the RWCP sound scene database [34]
as our noise dataset. Similarly to RIR, instead of using a generic
dataset, a sophisticated adversary can gather his own customized
noise dataset for a specific attack scenario (e.g., living room, office,
airport, and mall) to further improve the attack performance.
(8)
5 EVALUATION OF DIGITAL ATTACK
5.1 Experimental Methodology
Adversarial Perturbation Generation. We implemented AdvPulse
on the Tensorflow [1] platform and generated digital adversarial
perturbations according to Equation 5 using an NVIDIA 2080Ti
GPU. As for the attack configuration, we set α = 0.01, β = 0.001,
γ = 0.01, and chose the adversarial perturbation duration to be 0.5
seconds. The impact of the perturbation durations on the attack