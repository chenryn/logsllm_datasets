in the same DCelll but in two diﬀerent DCelll−1s. Since
there are gl DCelll−1 subnetworks inside this DCelll, it can
always choose a DCelll−1 (e.g., the one nearest to n1 but
diﬀerent from the one n2 is in). There must exist a link,
denoted as (p1, p2), that connects this DCelll−1 and the
one where n1 resides. Local-reroute then chooses p2 as its
proxy and re-routes packets from n1 to the selected proxy p2.
Upon receiving the packet, p2 simply uses DCellRouting to
route the packet to dst. In DCellRouting, the new and the
original pathes converge when they both reach DCelll or
above.
Local-route is eﬃcient in handling link failures. This is be-
cause most links in a path are low-level links using DCellRouting.
When local re-route is used to bypass a failed level-l link,
the path length increases on average by the average path
length in a DCelll−1. This is because local re-route needs
to route the packet into the proxy DCelll−1 to bypass the
failed link.
Local-reroute is not loop free. But loops can happen only
when there are multiple link failures and the re-routes form
a ring, hence is of very low probability. We discuss how to
remove looped packets from DCell in Section 4.3.3. Local-
reroute alone cannot completely address node failures. This
is because it is purely based on DCell topology and does
not utilize any kind of link or node states. We illustrate
is in the failed i2, a packet will not be able to reach dst no
matter how we local-reroute or jump-up. To remove packets
from the network, we introduce two mechanisms as our ﬁnal
defense. First, a retry count is added in the packet header.
It decrements by one when a local-reroute is performed. A
packet is dropped when its retry count reaches zero. Sec-
ond, each packet has a time-to-live (TTL) ﬁeld, which is
decreased by one at each intermediate node. The packet
is discarded when its TTL reaches zero. When a packet is
dropped, a destination unreachable message is sent back to
the sender so that no more packets to the destination will
be injected.
4.3.4 DFR: DCell Fault-tolerant Routing
Our DFR protocol uses all three techniques for fault-tolerant
routing. The detailed procedure of DFR is shown in Figure
5. Denote the receiver node as self.uid. Upon receiving a
packet, a node ﬁrst checks whether it is the destination. If
so, it delivers the packet to upper layer and returns (line
1). Otherwise, it checks the proxy ﬁeld of the packet.
If
the proxy value of the packet matches the node, implying
that the packet has arrived at the proxy, we then clear the
proxy ﬁeld (line 2). Let dcn dst denote our DCellRouting
destination. When the proxy ﬁeld of the packet is empty,
dcn dst is the destination of the packet; otherwise, it is the
proxy of the packet (lines 3-4). DCellRouting is then used
to compute a path from the current node to dcn dst (line 5),
and to ﬁnd the ﬁrst link with level > b from the path (with
F irstLink, line 6). If we cannot ﬁnd such a link (indicating
that dcn dst and the receiver are in the same DCellb), we
set dij dst, which is the destination to be used in Dijkstra
routing within this DCellb, to dcn dst. Once found, such a
link is denoted as (n1, n2). We know that n1 and n2 are in
two diﬀerent DCelll−1s but in the same DCelll, where l is
the level of (n1, n2). We then check the status of (n1, n2). If
(n1, n2) failed, we perform local-rerouting; otherwise, we set
dij dst to n2, the last hop in our DijkstraRouting (line 10).
Once dij dst is chosen, we use DijkstraRouting to perform
intra-DCell routing and obtain the next hop. If the next hop
is found, we forward the packet to it and return (line 13).
However, If we cannot ﬁnd a route to dij dst and the desti-
nation of the packet and the receiver are in the same DCellb,
we drop the packet and return (lines 14-15); otherwise, we
local-reroute the packet.
When we need to reroute a packet, we use SelectP roxy
to select a link to replace the failed link (n1, n2). In case
we cannot ﬁnd a route to n1 inside DCellb, we treat it as
equivalent to (n1, n2) failure. The idea of SelectP roxy is
simple. Our preferred choice is to ﬁnd a link that has the
same level as (n1, n2). When rack failure occurs, we increase
the link level by 1 to ‘jump-up’. Once we determine the level
for proxy selection, we use a greedy algorithm to choose the
proxy. We choose the node that has an alive link with our
preferred level and is the closest one to self.uid. In the ex-
ample of Figure 4, link (p1, p2) is chosen, node p1 is in the
same DCellb with the current receiver m2 and p2 is chosen
as the proxy.
5.
INCREMENTAL EXPANSION
It is unlikely that a full DCell-based DCN is constructed
at one time. Servers need to be added into data centers
incrementally. When new machines are added, it is desirable
that existing applications should not be interrupted, thus
Figure 4: DFR: Fault tolerant routing in DCell.
the problem via an example. Consider from src to dst there
is sub DCellRouting path {(q1, q2), (q2, q3)}. The level of
(q1, q2) is 1 and the level of (q2, q3) is 3. Now q1 ﬁnds that
(q1, q2) is down (while actually q2 failed). Then, no matter
how we re-route inside this DCell2, we will be routed back
to the failed node q2!
In the extreme case, when the last
hop to dst is broken, the node before dst is trapped in a
dilemma: if dst fails, it should not perform local-reroute; if
it is a link failure, it should perform local-reroute. To solve
the problem faced by pure local-reroute, we next introduce
local link-state.
4.3.2 Local Link-state
With local link-state, we use link-state routing (with Dijk-
stra algorithm) for intra-DCellb routing and DCellRouting
and local reroute for inter-DCellb routing. In a DCellb, each
node uses DCellBroadcast to broadcast the status of all its
(k + 1) links periodically or when it detects link failure. A
node thus knows the status of all the outgoing/incoming
links in its DCellb. b is a small number specifying the size
of DCellb. For example, a DCellb has 42 or 1806 servers
when b is 1 or 2 and n = 6.
Figure 4 illustrates how local link-state routing works to-
gether with local re-route. Use node m2 as an example.
Upon receiving a packet, m2 uses DCellRouting to calculate
the route to the destination node dst. It then obtains the
ﬁrst link that reaches out its own DCellb (i.e., (n1, n2) in
the ﬁgure). m2 then uses intra-DCellb routing, a local link-
state based Dijkstra routing scheme, to decide how to reach
n1. Upon detecting that (n1, n2) is broken, m2 invokes local-
reroute to choose a proxy. It chooses a link (p1, p2) with the
same level as (n1, n2) and sets p2 as the proxy. After that,
m2 routes the packet to p2. When p2 receives the packet,
it routes the packet to dst. Note that we handle the failure
of (n1, n2) successfully, regardless of a link failure or a node
failure at n2.
Jump-up for Rack Failure
4.3.3
We now introduce jump-up to address rack failure. As-
sume the whole DCellb, i2, fails in Figure 4. Then the packet
will be re-routed endlessly around i2, since all the re-routed
paths need to go through r1. The idea of jump-up can also be
illustrated in Figure 4. Upon receiving the rerouted packet
(implying (n1, n2) has failed), p2 checks whether (q1, q2) has
failed or not. If (q1, q2) also fails, it is a good indication that
the whole i2 failed. p2 then chooses a proxy from DCells
with higher level (i.e., it jumps up). Therefore, with jump-
up, the failed DCell i2 can be bypassed. Note that when dst
if ((n1, n2) fails) goto local-reroute;
else dij dst = n2;
if (pkt.dst == self.uid) { deliver(pkt); return; }
if (self.uid == pkt.proxy) pkt.proxy =NULL;
if (pkt.proxy! =NULL) dcn dst = pkt.proxy;
else dcn dst = pkt.dst;
path = DCellRouting(self.uid, dcn dst);
(n1, n2) = FirstLink(path, b);
if ((n1, n2) ==NULL) dij dst = dcn dst;
else
DFR(pkt) /*pkt is the received packet*/
1
2
3
4
5
6
7
8
9
10
11 next hop = DijkstraRouting(pkt, dij dst);
12 if (next hop! =NULL)
13
14 else if (self.uid and pkt.dst are in a same DCellb))
15
local-reroute:
16 pkt.retry −−;
17 if (pkt.retry == 0){drop(pkt); return;}
18 pkt.proxy = SelectProxy(uid, (n1, n2)))
19 return DFR(pkt);
forward pkt to next hop and return;
drop pkt and return;
Figure 5: Pseudocode for DFR.
requiring that: (1) re-wiring should not be allowed, and (2)
addresses of existing machines should not change. A direct
consequence of these requirements is that, the number of
servers in a DCell0, denoted as n, should be ﬁxed.
A straightforward way to gradually build DCell is the
bottom-up approach. When a DCell0 is full, we start to
build a DCell1. When a DCellk−1 is full, we start to build
a DCellk. This way, neither re-addressing nor re-wiring is
needed when new machines are added. The system grows
incrementally. However, this approach may generate interim
structure that is not fault-tolerant. For example, when the
number of nodes in the system is 2 × ti−1, it will form two
full DCelli−1s connected by a single link. If this link fails,
the network is partitioned into two parts.
In this work, we propose a top-down approach to incre-
mentally build a DCell. When constructing a DCellk, we
start from building many incomplete DCellk−1s and make
them fully connected. Hence, even interim structure is fault
tolerant. In our approach, we require that the minimal quan-
tum of machines added at one time be much larger than one.
In this paper, we use DCell1 as the basic adding unit. This
does not pose any diﬃculty in reality since servers are added
in racks in data centers. A DCell1 has 20, 30, 42 servers
when n = 4, 5, 6. It can be readily placed into a rack, which
typically has 20-80 servers. We also ﬁx k at the planning
stage of a data center. A choice of k = 3 accommodates
millions of servers.
The AddDCell procedure is shown in Figure 6.
It adds
a DCell1 and runs recursively. When adding a new level-1
DCell d1, AddDCell starts from DCellk and recursively ﬁnds
the right DCell2 for the new DCell1. The right sub-DCell
for d1 is found via calling sub-routine GetIndex, which sim-
ply checks the number of its DCelll−1. If the number is less
than (t1 + 1), it allocates a new empty DCelll−1. If all the
DCelll−1s are full, a new DCelll−1 is created; Otherwise,
it ﬁnds the ﬁrst none-full DCelll−1. Then d1 is added into
the sub-DCell. After d1 is added, all the nodes in d1 are
connected to their existing neighbors (not shown in Figure
6).
AddDCell (pref , l, d1) /*d1 is the DCell1 to add*/
if (l == 2)
i is the largest index of the existing DCell1s;
assign preﬁx [pref , i + 1] to d1;
return;
id = GetIndex (pref , l);
AddDCell([pref, id], l − 1, d1);
return;
GetIndex (pref , l){ /*get the DCelll−1 to add d1*/
m = the number of DCelll−1 subnetworks;
if (m  2) built by AddDCell either
is a complete graph at level i, or becomes a complete graph
at level i after we remove its DCelli−1 subnetwork that has
the largest preﬁx.
An incrementally expanding DCell is highly fault toler-
ant because of good connectivity. When building a DCellk
(k > 1) using AddDCell, the number of DCellk−1 networks
is at least min(t1 + 1, θ), where θ is the number of added
DCell1s. This result, together with Theorem 7, demon-
strates the good connectivity achieved by AddDCell.
AddDCell also ﬁts naturally with our DFR routing algo-
rithm. When a server cannot forward a packet to a sub-
DCell, it just treats the next link as a failed one (no matter
whether it is due to failure or an incomplete DCell). Using
DCell1 as the adding unit also ensures that the intra-DCell
link-state routing of DFR works well since DCells at levels
1 and 2 are always fully connected.
6. SIMULATIONS
In this section, we use simulations to evaluate the perfor-
mance of DFR under server node, rack, and link failures.
We compare DFR with the Shortest-Path Routing (SPF),
which oﬀers a performance bound. The results are obtained
by averaging over 20 simulation runs.
6.1 DFR in a Full DCell
In our simulation, diﬀerent types of failures are randomly
generated. A randomly selected node routes packets to all
the other nodes. We study both the path failure ratio and
the average path length for the found paths. In all the sim-
ulations, we set the intra-DCell routing level b = 1 and each
Figure 7: Path failure ration vs. node failure ratio.
Figure 8: Path failure ratio vs. link failure ratio.
DCell1 is a rack. We vary the (node/rack/link) failure ratios
from 2% to 20%. The networks we use are a DCell3 with
n = 4 (176,820 nodes) and a DCell3 with n = 5 (865,830
nodes).
Figure 7 plots the path failure ratio versus the node failure
ratio under node failures. We observe that, DFR achieves
results very close to SPF. Even when the node failure ratio
is as high as 20%, DFR achieves 22.3% path failure ratio for
n = 4 while the bound is 20%. When the node failure ratio
is lower than 10%, DFR performs almost identical to SPF.
Moreover, DFR performs even better as n gets larger.
Since DFR uses local reroute to bypass failed links, one
concern is that local reroute might increase the path length.
Table 2 shows that, the diﬀerence in path length between
DFR and SPF increases very slowly as the node failure ra-
tio increases. We also have studied the standard deviations
of the path lengths under DFR and SPF. The standard de-
viation of DFR also increases very slowly as the node failure
increases. When the node failure rate is as high as 20%, the
standard deviation is still less than 5.
We have also studied the eﬀect of rack failure. We ran-
domly select DCell1s and let all the nodes and links in those
DCell1 fail. Table 2 shows that, the impact of rack failure
on the path length is smaller than that of node failure. This
is because when a rack fails, SPF also needs to ﬁnd alterna-
tive paths from higher-level DCells. Our jump-up strategy
is very close to SPF routing in this case. The path failure
ratio is not shown here since it is very similar to the node
failure case in Figure 7.
Figure 8 plots the path failure ratio under link failures,