switches in testbed are implemented based on DPDK, the software-
based processing would introduce uncertain latency and further
affect the evolution of sending rate and queue length. So, there are
some transient divergences between experiments and simulations.
But the overall evolutions keep good consistence. These results
validate our simulator, therefore we use it for further evaluations.
6.2.2 Deadlock case study. As shown in Figure 11, we use a
3-layer fat-tree [1] with k = 4 as our case-study topology, and
employ the shortest-path-first routing algorithm. Three link failures,
marked with dashed lines, make the network prone to deadlock.
Four flows are introduced into the network: F1 : H0 → H8, F2 :
H4 → H12, F3 : H9 → H1 and F4 : H13 → H5. Then there is a
four-hop CBD in the network: SC1 → SA3 → SC2 → SA7 → SC1.
In simulations, the input buffer size is 300KB, the link capacity is
10Gbps and the propagation delay is 1µs. In PFC, XOF F is 280KB
and XON is 277KB. In buffer-based GFC, B1 is set to 281KB, and
Bn+1 − Bn = ( 1
2)n × 19KB. In time-based GFC, B0 is set to 159KB.
The results are presented in Figure 12 and Figure 13. In both
PFC and CBFC, the network falls into deadlock. While employing
GFC, deadlock is avoided and each flow shares 5Gbps of bandwidth
normally.
Victim flow: As aforementioned, when deadlock occurs, it would
not only stop all flows associated with the CBD but also prevent
other CBD-irrelevant flows. In other words, a local deadlock may
disable a large part of network. Here we add a new flow (F5) into
the network. This flow travels from H4 to H5 with the forwarding
path H4 → SE3 → H5, which does not pass through the CBD. We
conduct above simulations again. The throughput of F5 are shown
in Figure 14(a) and Figure 14(b). In both CBFC and PFC, deadlock
occurs and the throughput of F5 goes to zero. The root cause is
that deadlock pauses all flows entering the CBD, and such pausing
behavior will propagate hop-by-hop back to the sources of these
flows. Then irrelevant flows sharing part of this path are victimized.
H0H1H2H3SE1SE2H4H5H6H7H8H9H10H11H12H13H14H15SA1SA2SC1SC2SC3SC4SE3SE4SA3SA4SE5SE6SA5SA6SE7SE8SA7SA8 0 50 100 150 200 250 300 0 200 400 600 800 1000 0 2 4 6 8 10Length (KB)Rate (Gbps)Time (us)Queue lengthInput rate 0 50 100 150 200 250 300 0 200 400 600 800 1000 0 2 4 6 8 10Length (KB)Rate (Gbps)Time (us)Queue lengthInput rate 0 50 100 150 200 250 300 0 1000 2000 3000 4000 5000 0 2 4 6 8 10Length (KB)Rate (Gbps)Time (us)Queue lengthInput rate 0 50 100 150 200 250 300 0 500 1000 1500 2000 0 2 4 6 8 10Length (KB)Rate (Gbps)Time (us)Queue lengthInput rate 0 2 4 6 8 10 0 500 1000 1500 2000Output Rate (Gbps)Time (us)Buﬀer-based GFCPFC 0 2 4 6 8 10 0 1000 2000 3000 4000 5000 6000Output Rate (Gbps)Time (us)Time-based GFCCBFCSIGCOMM ’19, August 19–23, 2019, Beijing, China
Kun Qian, Wenxue Cheng, Tong Zhang, Fengyuan Ren
Figure 15: Empirical traffic pattern.
Table 1: Statistical results of deadlock cases
(a) Deadlock-free cases
(b) Deadlock-prone cases
Figure 16: Average available bandwidth.
Scale
k = 4
k = 8
k = 16
32
12
2
GFC
0
0
0
PFC Buffer-based
CBFC Time-based
GFC
0
0
0
32
12
2
By employing GFC, deadlock is eliminated and F5 can share its
deserving 5Gbps throughput normally.
6.2.3
Large-scale simulations. In this part we evaluate the over-
all performance of GFC in large-scale networks. The fat-tree topol-
ogy with k = 4, 8 and 16 is used. The settings of buffer size and link
capacity remain unchanged, so do the parameters in different flow
control mechanisms. In this set of simulations, the failure probabil-
ity of each link is 5%. We randomly generate 10000 networks under
each scale and employ the shortest-path-first routing algorithm. As
shown in Figure 15, the input workload is based on empirically ob-
served enterprise traffic patterns [57]. Each host randomly chooses
a destination in different racks to start a new flow. Once this flow
is finished, the host repeats the above process. In each selected
network, we run different flow control mechanisms, respectively.
Since the topologies and routing algorithm are determined, we
can filter out cases which are prone to generate CBD in advance.
In order to judge whether deadlock occurs or not under these sce-
narios, we repeat each simulation for 100 times. For a certain flow
control in a specific topology, if deadlock occurs in any of these 100
simulations, we consider this scenario as a deadlock case. Results
for all flow control mechanisms under each network scale are listed
in Table 1. Results show that deadlock only occurs under PFC and
CBFC. With the increase of network scale, less deadlock cases come
out. Since large-scale topology possesses more optional forwarding
paths, random link failures are harder to generate a CBD. It is worth
noticing that PFC and CBFC cause the same number of deadlock
cases under all network scales. Actually, through our careful check,
they fall into deadlock in the same topologies. By employing GFC
schemes, deadlock is avoided in all network scales.
Overall performance: Operators may have interest in whether
deploying GFC would introduce side-effect to the overall network
performance (e.g., wasting available bandwidth, increasing latency),
especially when there is no CBD in the network. To answer this
question, 100 CBD-free cases under each network scale are ran-
domly selected to conduct further simulations. We record the aver-
age throughput of servers and the average slowdown, which is the
(a) Deadlock-free cases
(b) Deadlock-prone cases
Figure 17: Average slowdown (normalized to minimum in
each scale).
actual flow completion time (FCT) divided by the shortest possible
time for a same-sized flow to finish in an unloaded network. The
throughput values are worked out through counting sent bytes ev-
ery 100µs. When there is no CBD in network, those failed links only
constrain forwarding selections. Under these scenarios, different
kinds of flow control all work for port-level rate adjustment only,
which would introduce small difference on overall performance.
In Figure 16(a) and Figure 17(a), the statistical values are similar
among different flow control mechanisms (buffer-based/time-based
GFC and PFC/CBFC). It confirms that GFC does not introduce
extra bandwidth waste or FCT increase. The standard deviation
of throughput is smaller in buffer-based/time-based GFC than in
PFC/CBFC because GFC adjusts input rate at a much finer granu-
larity. When a deadlock is generated by employing PFC and CBFC,
the entire network is further affected and disabled, which leads to
zero average bandwidth and infinite FCT as shown in Figure 16(b)
and Figure 17(b). By employing GFC, deadlock is eliminated. Fur-
thermore, compared with results under deadlock-free cases, the
differences in the macroscopic performance are small. Although
specific combination of flows still generate CBD when deploying
GFC, these flows can continuously pass through the CBD. Once any
flow in this combination is finished, the CBD is naturally broken
and there is no further side-effect on overall network utilization. A
more detailed case study is given in the following part to conduct
an in-depth investigation.
Case study: We select one of deadlock-prone simulations in
fat-tree network (k = 16) as an example. The evolutions of aver-
age throughput under buffer-based GFC and PFC are depicted in
Figure 18. After initialization, all servers send traffic at line rate.
Quickly the switch queues build up and flow control mechanisms
take effect to restrict sending rates. Since the destinations of flows
84
 0 0.2 0.4 0.6 0.8 1101102103104105106107108109CDFFlow Size (B) 0 1 2 3 4 5k=4k=8k=16Throughput (Gbps)Network ScaleBuﬀer-based GFCPFCTime-based GFCCBFC 0 1 2 3 4 5k=4k=8k=16Throughput (Gbps)Network ScaleBuﬀer-based GFCPFCTime-based GFCCBFC 0 0.5 1 1.5 2k=4k=8k=16Normalized SlowdownNetwork ScaleBuﬀer-based GFCPFCTime-based GFCCBFC 0 0.5 1 1.5 2k=4k=8k=16Normalized SlowdownNetwork ScaleBuﬀer-based GFCPFCTime-based GFCCBFCGentle Flow Control: Avoiding Deadlock in Lossless Networks
SIGCOMM ’19, August 19–23, 2019, Beijing, China
Figure 18: Throughput evolution.
Figure 19: Occupied bandwidth.
Figure 20: Interaction with DCQCN.
are randomly selected, it takes time to generate the specific flow
combinations to fill up CBD under PFC. During 0 ∼ 8.5ms, both flow
control mechanisms work well. However, at 8.5ms, deadlock occurs
under PFC. Then the throughput of the entire network rapidly goes
to zero. Three main reasons lead to this scenario: (1) The appear-
ance of deadlock prohibits the forwarding of all flows associated
with the CBD. (2) Deadlock pressures congestion back to sources,
further making many victim flows cease. (3) Although many irrel-
evant servers can still send packets when deadlock occurs, they
will randomly select new destinations after current flows finishing.
Once the new forwarding path passes through any “dead” links,
this server will also be paused and more links will become “dead”.
Through a transient evolution (from 8.5ms to 8.7ms), the entire
network traps into deadlock ultimately. By employing buffer-based
GFC, the sending rate can always be controlled well and no dead-
lock occurs. The results of other network scales and of time-based
GFC are similar.
Occupied bandwidth: We provide further evaluations about
the consumed bandwidth by employing GFC. In time-based GFC,
Message Generator remains unchanged, so the occupied bandwidth
keeps the same as in CBFC. In buffer-based GFC, Message Generator
would generate more feedback messages, which is suspicious to
damage available bandwidth. So this part mainly evaluates the
introduced bandwidth consumption of buffer-based GFC. We add
a counting module on each port to record the number of received
feedback messages every 500µs. The network topology is fat-tree
(k = 16) and traffic pattern is randomly generated as in the former
set of simulations. We conduct 100 simulations and record occupied
bandwidth of all ports. The CDF of results is presented in Figure 19.
The average bandwidth consumption is 0.21%, and in 99% cases
the occupied bandwidth is less than 0.4%. Frequent generations
of feedback messages occur only when the sending rate changes
dramatically (e.g., 9Gbps → 1Gbps), which happens rarely. In our
simulations, only up to 0.49% of bandwidth occupation is observed.
7 DISCUSSION
Cooperation with congestion control. In practical lossless net-
works, both flow control and congestion control are deployed to
manage network traffic. Existing congestion control mechanisms
tend to explicitly (e.g., QCN [26] and DCQCN [59]) or implicitly (e.g.,
TIMELY [42]) control the queue length to a low level, so low latency
and high throughput can be achieved at the same time. Since GFC
only decreases the sending rate when queue length is very large,
the introduction of GFC would not change the steady-state results
of congestion control. The duty of GFC is avoiding packet loss and
deadlock in critical situations. In common scenarios, congestion
control is taking charge of managing proper network sharing and
keeping the queue occupation below the active threshold of GFC.
To better understand the interaction between GFC and conges-
tion control, we build a small simulation using the dumbbell topol-
ogy, which contains 8 senders (H1 ∼ H8) and 1 receiver (H9). All
settings of buffer-based GFC are consistent with aforementioned
simulations. DCQCN is implemented as a representative congestion
control. The ECN threshold is 40KB. α = 0.5, д = 1/256, N = 50µs
and K = 55µs in DCQCN. All senders start a long flow to the re-
ceiver at the same time. The results are shown in Figure 20. We
monitor three metrics: (1) the ingress queue length of switch port
connected with H1, (2) the flow sending rate in H1, which is con-
trolled by DCQCN (denoted by “DCQCN rate”), and (3) the rate
of output queue in H1, which is controlled by buffer-based GFC
(denoted by “GFC rate”). Notice that, in this scenario, the minimum
value of (2) and (3) will determine the final sending rate of H1. At
the beginning, the input queue length increases quickly owing to
the incast traffic. Although DCQCN detects the congestion, it needs
several turns to decrease the flow rate. Therefore, queue length
keeps rising and triggers GFC. GFC rapidly limits the output rate of
H1 to 1.25Gbps, so the queue length stops increasing further. Then
DCQCN continuously decreases the sending rate to remove backlog.
Once the DCQCN rate is smaller than GFC rate, the sending rate
of H1 is fully controlled by DCQCN and GFC is actually disabled.
Then DCQCN processes the normal evolution to find the appro-
priate flow rate. This simulation shows that GFC only works as a
safeguard and introduces small influence on the overall evolution
of end-to-end congestion control.
Multiple priority queues. In CEE, different priorities usually rep-
resent the precedence for occupying bandwidth. In order to prevent
low priority queues from long-time starvation, which may intro-
duce the risk of buffer exhausting, the output queue scheduling
should be enabled to assign minimal output bandwidth to each pri-
ority. The weighted output queue scheduling is available on most
commodity switches [11, 34]. As long as the queue obtains a positive
output rate, GFC can adjust the input rate to match it. In InfiniBand,
different priority queues fairly share the output bandwidth, so GFC
can work as expected without additional configurations.
Routing loop. Since GFC keeps all routing configurations unmodi-
fied, routing loop, if exists, would trap packets in itself and make the
sending rate gradually decreases to zero. However, eliminating self-
loop is the basic requirement of network routing and there are many
85
 0 2 4 6 8 10 0 2 4 6 8 10 12 14Bandwidth (Gbps)Time (ms)Buﬀer-based GFCPFC 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5CDFOccupied Bandwidth (%) 0 50 100 150 200 250 300 0 2 4 6 8 10 0 2 4 6 8 10Length (KB)Rate (Gbps)Time (ms)Queue lengthDCQCN rateGFC rateSIGCOMM ’19, August 19–23, 2019, Beijing, China
Kun Qian, Wenxue Cheng, Tong Zhang, Fengyuan Ren
efficient mechanisms deployed in data centers to achieve it. In layer-
2 network, spanning tree protocol is widely deployed to efficiently
avoid the generation of self-loop. Furthermore, novel mechanisms
are proposed when designing new data center architectures [44]
and efficient rerouting mechanism is designed to guarantee loop-
freedom during rule update [5]. Therefore, although routing loop
can formulate deadlock, it is, to the best of our knowledge, not
observed in real-world deployment. Practical deadlocks are formu-
lated by a combination of flows each passing through part of the
CBD [22, 52]. This scenario can be well handled by GFC.
The granularity of rate limiting. Owing to the implementation
constraint, the granularity of rate limiting cannot be infinitely small.
When the target sending rate is smaller than the minimum rate
unit, GFC needs to alternate sending rate between zero and the
minimum rate unit to match it, which may still introduce hold and
wait. However, in commodity switches, the minimum rate unit is
8Kbps [12, 33], which is small enough in general scenarios. For
example, in 10/40/100Gbps network, it requires at least 6 cascaded
16:1 incast in the same priority to force the sending rate to be less
than 8Kbps (0.59/2.38/5.96 Kbps), which hardly happens in practice.
Lossless Ethernet vs lossy Ethernet. Considering many side-
effects (e.g., head-of-line blocking, victim flow and deadlock) caused
by guaranteeing zero packet loss (employing PFC), there is a trend
to build RMDA over lossy network. IRN is the representative of this
work [43]. By implementing a smarter retransmission mechanism,
the performance of long flows would not be penalized a lot by
packet loss. However, for short flows, which are widespread in data
centers, any packet loss would double their FCT. Furthermore, in
some services (e.g., IPC), zero packet loss is a critical requirement.
Any packet retransmission would make a flow miss its deadline. As
mentioned in [43], IRN experiences significant packet loss (8.5%)
without PFC, thus cannot support these services well. Therefore,
zero packet loss on layer-2 network is still an attractive property
for optimizing network performance.
8 RELATED WORK
CBD-free routing. To avoid network deadlock, previous work fo-
cuses on designing central routing algorithms to avoid CBD occur-
rence under any traffic pattern [7, 13–15, 17, 18, 21, 50, 51, 54–56, 58].
Autonet [51] proposes Up*/Down* routing, which can eliminate
CBD in tree-like topologies. Then, more methodologies are designed
to guarantee CBD-free in other generic topologies. However, these
solutions introduce great limitations to network configurations
and disable some intrinsic advantages (e.g., balancing load among
multiple paths). Furthermore, completely obeying the centralized
routing decisions at all time is impractical. When link failure occurs
in network, distributed rerouting would be activated to maintain
network connectivity, which still introduces the risk of generating
CBD.
Other work presents dynamic-routing-based mechanisms to
avoid CBD [16]. In normal situations, packets can be forwarded
through the optimal paths. When buffers are full, preset escape
paths are activated to make sure no CBD exists. The implementa-
tion requires switches to reserve an independent buffer for each
escape path, which would consume a large amount of precious
storage resource. This defence mechanism is conservative since it
will activate escape paths even when the normal congestion fills up
buffer. However, forwarding packets through escape (non-optimal)