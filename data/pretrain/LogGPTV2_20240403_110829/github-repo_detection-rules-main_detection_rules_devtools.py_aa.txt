# Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one
# or more contributor license agreements. Licensed under the Elastic License
# 2.0; you may not use this file except in compliance with the Elastic License
# 2.0.
"""CLI commands for internal detection_rules dev team."""
import dataclasses
import functools
import io
import json
import os
import re
import shutil
import subprocess
import textwrap
import time
import typing
import urllib.parse
from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import click
import pytoml
import requests.exceptions
import yaml
from elasticsearch import Elasticsearch
from eql.table import Table
from semver import Version
from kibana.connector import Kibana
from . import attack, rule_loader, utils
from .beats import (download_beats_schema, download_latest_beats_schema,
                    refresh_main_schema)
from .cli_utils import single_collection
from .docs import IntegrationSecurityDocs, IntegrationSecurityDocsMDX
from .ecs import download_endpoint_schemas, download_schemas
from .endgame import EndgameSchemaManager
from .eswrap import CollectEvents, add_range_to_dsl
from .ghwrap import GithubClient, update_gist
from .integrations import (SecurityDetectionEngine,
                           build_integrations_manifest,
                           build_integrations_schemas,
                           find_latest_compatible_version,
                           find_latest_integration_version,
                           load_integrations_manifests)
from .main import root
from .misc import PYTHON_LICENSE, add_client, client_error
from .packaging import (CURRENT_RELEASE_PATH, PACKAGE_FILE, RELEASE_DIR,
                        Package, current_stack_version)
from .rule import (AnyRuleData, BaseRuleData, DeprecatedRule, QueryRuleData,
                   RuleTransform, ThreatMapping, TOMLRule, TOMLRuleContents)
from .rule_loader import RuleCollection, production_filter
from .schemas import definitions, get_stack_versions
from .utils import (dict_hash, get_etc_path, get_path, load_dump,
                    load_etc_dump, save_etc_dump)
from .version_lock import VersionLockFile, default_version_lock
RULES_DIR = get_path('rules')
GH_CONFIG = Path.home() / ".config" / "gh" / "hosts.yml"
NAVIGATOR_GIST_ID = '1a3f65224822a30a8228a8ed20289a89'
NAVIGATOR_URL = 'https://ela.st/detection-rules-navigator'
NAVIGATOR_BADGE = (
    f'[![ATT&CK navigator coverage](https://img.shields.io/badge/ATT&CK-Navigator-red.svg)]({NAVIGATOR_URL})'
)
def get_github_token() -> Optional[str]:
    """Get the current user's GitHub token."""
    token = os.getenv("GITHUB_TOKEN")
    if token is None and GH_CONFIG.exists():
        token = load_dump(str(GH_CONFIG)).get("github.com", {}).get("oauth_token")
    return token
@root.group('dev')
def dev_group():
    """Commands related to the Elastic Stack rules release lifecycle."""
@dev_group.command('build-release')
@click.argument('config-file', type=click.Path(exists=True, dir_okay=False), required=False, default=PACKAGE_FILE)
@click.option('--update-version-lock', '-u', is_flag=True,
              help='Save version.lock.json file with updated rule versions in the package')
@click.option('--generate-navigator', is_flag=True, help='Generate ATT&CK navigator files')
@click.option('--add-historical', type=str, required=True, default="no",
              help='Generate historical package-registry files')
@click.option('--update-message', type=str, help='Update message for new package')
def build_release(config_file, update_version_lock: bool, generate_navigator: bool, add_historical: str,
                  update_message: str, release=None, verbose=True):
    """Assemble all the rules into Kibana-ready release files."""
    config = load_dump(config_file)['package']
    add_historical = True if add_historical == "yes" else False
    if generate_navigator:
        config['generate_navigator'] = True
    if release is not None:
        config['release'] = release
    if verbose:
        click.echo(f'[+] Building package {config.get("name")}')
    package = Package.from_config(config, verbose=verbose, historical=add_historical)
    if update_version_lock:
        default_version_lock.manage_versions(package.rules, save_changes=True, verbose=verbose)
    package.save(verbose=verbose)
    if add_historical:
        previous_pkg_version = find_latest_integration_version("security_detection_engine", "ga", config['name'])
        sde = SecurityDetectionEngine()
        historical_rules = sde.load_integration_assets(previous_pkg_version)
        historical_rules = sde.transform_legacy_assets(historical_rules)
        docs = IntegrationSecurityDocsMDX(config['registry_data']['version'], Path(f'releases/{config["name"]}-docs'),
                                          True, historical_rules, package, note=update_message)
        docs.generate()
        click.echo(f'[+] Adding historical rules from {previous_pkg_version} package')
        package.add_historical_rules(historical_rules, config['registry_data']['version'])
    if verbose:
        package.get_package_hash(verbose=verbose)
        click.echo(f'- {len(package.rules)} rules included')
    return package
def get_release_diff(pre: str, post: str, remote: Optional[str] = 'origin'
                     ) -> (Dict[str, TOMLRule], Dict[str, TOMLRule], Dict[str, DeprecatedRule]):
    """Build documents from two git tags for an integration package."""
    pre_rules = RuleCollection()
    pre_rules.load_git_tag(pre, remote, skip_query_validation=True)
    if pre_rules.errors:
        click.echo(f'error loading {len(pre_rules.errors)} rule(s) from: {pre}, skipping:')
        click.echo(' - ' + '\n - '.join([str(p) for p in pre_rules.errors]))
    post_rules = RuleCollection()
    post_rules.load_git_tag(post, remote, skip_query_validation=True)
    if post_rules.errors:
        click.echo(f'error loading {len(post_rules.errors)} rule(s) from: {post}, skipping:')
        click.echo(' - ' + '\n - '.join([str(p) for p in post_rules.errors]))
    rules_changes = pre_rules.compare_collections(post_rules)
    return rules_changes
@dev_group.command('build-integration-docs')
@click.argument('registry-version')
@click.option('--pre', required=True, help='Tag for pre-existing rules')
@click.option('--post', required=True, help='Tag for rules post updates')
@click.option('--directory', '-d', type=Path, required=True, help='Output directory to save docs to')
@click.option('--force', '-f', is_flag=True, help='Bypass the confirmation prompt')
@click.option('--remote', '-r', default='origin', help='Override the remote from "origin"')
@click.option('--update-message', default='Rule Updates.', help='Update message for new package')
@click.pass_context
def build_integration_docs(ctx: click.Context, registry_version: str, pre: str, post: str,
                           directory: Path, force: bool, update_message: str,
                           remote: Optional[str] = 'origin') -> IntegrationSecurityDocs:
    """Build documents from two git tags for an integration package."""
    if not force:
        if not click.confirm(f'This will refresh tags and may overwrite local tags for: {pre} and {post}. Continue?'):
            ctx.exit(1)
    rules_changes = get_release_diff(pre, post, remote)
    docs = IntegrationSecurityDocs(registry_version, directory, True, *rules_changes, update_message=update_message)
    package_dir = docs.generate()
    click.echo(f'Generated documents saved to: {package_dir}')
    updated, new, deprecated = rules_changes
    click.echo(f'- {len(updated)} updated rules')
    click.echo(f'- {len(new)} new rules')
    click.echo(f'- {len(deprecated)} deprecated rules')
    return docs
@dev_group.command("bump-pkg-versions")
@click.option("--major-release", is_flag=True, help="bump the major version")
@click.option("--minor-release", is_flag=True, help="bump the minor version")
@click.option("--patch-release", is_flag=True, help="bump the patch version")
@click.option("--new-package", type=click.Choice(['true', 'false']), help="indicates new package")
@click.option("--maturity", type=click.Choice(['beta', 'ga'], case_sensitive=False),
              required=True, help="beta or production versions")
def bump_versions(major_release: bool, minor_release: bool, patch_release: bool, new_package: str, maturity: str):
    """Bump the versions"""
    pkg_data = load_etc_dump('packages.yml')['package']
    kibana_ver = Version.parse(pkg_data["name"], optional_minor_and_patch=True)
    pkg_ver = Version.parse(pkg_data["registry_data"]["version"])
    pkg_kibana_ver = Version.parse(pkg_data["registry_data"]["conditions"]["kibana.version"].lstrip("^"))
    if major_release:
        major_bump = kibana_ver.bump_major()
        pkg_data["name"] = f"{major_bump.major}.{major_bump.minor}"
        pkg_data["registry_data"]["conditions"]["kibana.version"] = f"^{pkg_kibana_ver.bump_major()}"
        pkg_data["registry_data"]["version"] = str(pkg_ver.bump_major().bump_prerelease("beta"))
    if minor_release:
        minor_bump = kibana_ver.bump_minor()
        pkg_data["name"] = f"{minor_bump.major}.{minor_bump.minor}"
        pkg_data["registry_data"]["conditions"]["kibana.version"] = f"^{pkg_kibana_ver.bump_minor()}"
        pkg_data["registry_data"]["version"] = str(pkg_ver.bump_minor().bump_prerelease("beta"))
        pkg_data["registry_data"]["release"] = maturity
    if patch_release:
        latest_patch_release_ver = find_latest_integration_version("security_detection_engine",
                                                                   maturity, pkg_data["name"])
        # if an existing minor or major does not have a package, bump from the last
        # example is 8.10.0-beta.1 is last, but on 9.0.0 major
        # example is 8.10.0-beta.1 is last, but on 8.11.0 minor
        if latest_patch_release_ver.minor != pkg_kibana_ver.minor:
            latest_patch_release_ver = latest_patch_release_ver.bump_minor()
        if latest_patch_release_ver.major != pkg_kibana_ver.major:
            latest_patch_release_ver = latest_patch_release_ver.bump_major()
        if maturity == "ga":
            pkg_data["registry_data"]["version"] = str(latest_patch_release_ver.bump_patch())
            pkg_data["registry_data"]["release"] = maturity
        else:
            # passing in true or false from GH actions; not using eval() for security purposes
            if new_package == "true":
                latest_patch_release_ver = latest_patch_release_ver.bump_patch()
            pkg_data["registry_data"]["version"] = str(latest_patch_release_ver.bump_prerelease("beta"))
            pkg_data["registry_data"]["release"] = maturity
    click.echo(f"Kibana version: {pkg_data['name']}")
    click.echo(f"Package Kibana version: {pkg_data['registry_data']['conditions']['kibana.version']}")
    click.echo(f"Package version: {pkg_data['registry_data']['version']}")
    save_etc_dump({"package": pkg_data}, "packages.yml")
@dataclasses.dataclass
class GitChangeEntry:
    status: str
    original_path: Path
    new_path: Optional[Path] = None
    @classmethod
    def from_line(cls, text: str) -> 'GitChangeEntry':
        columns = text.split("\t")
        assert 2  Path:
        return self.new_path or self.original_path
    def revert(self, dry_run=False):
        """Run a git command to revert this change."""
        def git(*args):
            command_line = ["git"] + [str(arg) for arg in args]
            click.echo(subprocess.list2cmdline(command_line))
            if not dry_run:
                subprocess.check_call(command_line)
        if self.status.startswith("R"):
            # renames are actually Delete (D) and Add (A)
            # revert in opposite order
            GitChangeEntry("A", self.new_path).revert(dry_run=dry_run)
            GitChangeEntry("D", self.original_path).revert(dry_run=dry_run)
            return
        # remove the file from the staging area (A|M|D)
        git("restore", "--staged", self.original_path)
    def read(self, git_tree="HEAD") -> bytes:
        """Read the file from disk or git."""
        if self.status == "D":
            # deleted files need to be recovered from git
            return subprocess.check_output(["git", "show", f"{git_tree}:{self.path}"])
        return self.path.read_bytes()
@dev_group.command("unstage-incompatible-rules")
@click.option("--target-stack-version", "-t", help="Minimum stack version to filter the staging area", required=True)
@click.option("--dry-run", is_flag=True, help="List the changes that would be made")
@click.option("--exception-list", help="List of files to skip staging", default="")
def prune_staging_area(target_stack_version: str, dry_run: bool, exception_list: list):
    """Prune the git staging area to remove changes to incompatible rules."""
    exceptions = {
        "detection_rules/etc/packages.yml",
    }
    exceptions.update(exception_list.split(","))
    target_stack_version = Version.parse(target_stack_version, optional_minor_and_patch=True)
    # load a structured summary of the diff from git
    git_output = subprocess.check_output(["git", "diff", "--name-status", "HEAD"])
    changes = [GitChangeEntry.from_line(line) for line in git_output.decode("utf-8").splitlines()]
    # track which changes need to be reverted because of incompatibilities
    reversions: List[GitChangeEntry] = []
    for change in changes:
        if str(change.path) in exceptions:
            # Don't backport any changes to files matching the list of exceptions
            reversions.append(change)
            continue
        # it's a change to a rule file, load it and check the version
        if str(change.path.absolute()).startswith(RULES_DIR) and change.path.suffix == ".toml":
            # bypass TOML validation in case there were schema changes
            dict_contents = RuleCollection.deserialize_toml_string(change.read())
            min_stack_version: Optional[str] = dict_contents.get("metadata", {}).get("min_stack_version")
            if min_stack_version is not None and \
                    (target_stack_version < Version.parse(min_stack_version, optional_minor_and_patch=True)):
                # rule is incompatible, add to the list of reversions to make later
                reversions.append(change)
    if len(reversions) == 0:
        click.echo("No files restored from staging area")
        return
    click.echo(f"Restoring {len(reversions)} changes from the staging area...")
    for change in reversions:
        change.revert(dry_run=dry_run)
@dev_group.command('update-lock-versions')
@click.argument('rule-ids', nargs=-1, required=False)
def update_lock_versions(rule_ids):
    """Update rule hashes in version.lock.json file without bumping version."""
    rules = RuleCollection.default()
    if rule_ids:
        rules = rules.filter(lambda r: r.id in rule_ids)
    else:
        rules = rules.filter(production_filter)
    if not click.confirm(f'Are you sure you want to update hashes for {len(rules)} rules without a version bump?'):
        return
    # this command may not function as expected anymore due to previous changes eliminating the use of add_new=False
    changed, new, _ = default_version_lock.manage_versions(rules, exclude_version_update=True, save_changes=True)
    if not changed:
        click.echo('No hashes updated')
    return changed
@dev_group.command('kibana-diff')
@click.option('--rule-id', '-r', multiple=True, help='Optionally specify rule ID')
@click.option('--repo', default='elastic/kibana', help='Repository where branch is located')
@click.option('--branch', '-b', default='main', help='Specify the kibana branch to diff against')
@click.option('--threads', '-t', type=click.IntRange(1), default=50, help='Number of threads to use to download rules')
def kibana_diff(rule_id, repo, branch, threads):
    """Diff rules against their version represented in kibana if exists."""
    from .misc import get_kibana_rules
    rules = RuleCollection.default()
    if rule_id:
        rules = rules.filter(lambda r: r.id in rule_id).id_map
    else:
        rules = rules.filter(production_filter).id_map
    repo_hashes = {r.id: r.contents.sha256(include_version=True) for r in rules.values()}
    kibana_rules = {r['rule_id']: r for r in get_kibana_rules(repo=repo, branch=branch, threads=threads).values()}