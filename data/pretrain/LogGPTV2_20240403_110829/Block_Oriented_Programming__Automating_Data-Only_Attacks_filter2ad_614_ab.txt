assume that the attacker has access to an information leak, i.e., a
vulnerability that allows her to read any value from any memory
address. We call this an Arbitrary memory Read Primitive (ARP).
Note that the ARP is optional and only needed to bypass orthogonal
probabilistic defenses.
We also assume that there exists an entry point, i.e., a location
that the program reaches naturally after completion of all AWPs
(and ARPs). Thus BOPC does not require code pointer corruption
to reach the entry point. Determining an entry point is considered
to be part of the vulnerability discovery process. Thus, finding this
entry point is orthogonal to our work.
Note that these assumptions are in line with the threat model of
control-flow hijack mitigations that aim to prevent attackers from
exploiting arbitrary read and write capabilities. These assumptions
are also practical. Orthogonal bug finding tools such as fuzzing
often discover arbitrary memory accesses that can be abstracted to
the required arbitrary read and writes, placing the entry point right
after the AWP. Furthermore, these assumptions map to real bugs.
Web servers, such as nginx, spawn threads to handle requests and a
bug in the request handler can be used to read or write an arbitrary
memory address. Due to the request-based nature, the adversary
can repeat this process multiple times. After the completion of the
state injection, the program follows an alternate and disjoint path
to trigger the injected payload.
These assumptions enable BOPC to inject a payload into a tar-
get binary’s address space, modifying its memory state to execute
the payload. BOPC assumes that the AWP (and/or ARP) may be
triggered multiple times to modify the memory state of the target
binary. After the state modification completes, the SPL payload
executes without using the AWP (and/or ARP) further. This sepa-
rates SPL execution into two phases: state modification and payload
execution. The AWP allows state modification, BOPC infers the
required state change to execute the SPL payload.
4 DESIGN
Figure 1 shows how BOPC automates the analysis tasks necessary
to leverage AWPs to produce a useful exploit in the presence of
strong defenses, including CFI. First, BOPC provides an exploit
programming language, called SPL, that enables analysts to define
exploits independent of the target program or underlying architec-
ture. Second, to automate SPL gadget discovery, BOPC finds basic
blocks from the target program that implement individual SPL
statements, called functional blocks. Third, to chain basic blocks
together in a manner that adheres with CFI and shadow stacks,
BOPC searches the target program for sequences of basic blocks
that connect pairs of neighboring functional blocks, which we call
dispatcher blocks. Fourth, BOPC simulates the BOP chain to produce
a payload that implements that SPL payload from a chosen AWP.
The BOPC design builds on two key ideas: Block Oriented Pro-
gramming and Block Constraint Summaries. First, defenses such as
CFI impose stringent restrictions on transitions between gadgets,
so an exploit no longer has the flexibility of setting the instruc-
tion pointer to arbitrary values. Instead, BOPC implements Block
Oriented Programming (BOP), which constructs exploit programs
called BOP chains from basic block sequences in the valid CFG of
a target program. Note that our CFG encodes both forward edges
(protected by CFI) and backward edges (protected by shadow stack).
(1) SPL Payload
(4) Stitching
BOP gadgets
(2) Selecting
functional blocks
(3) Searching for
dispatcher blocks
Figure 1: Overview of BOPC’s design.
Functional
Dispatcher
BOP
Gadget
Simple loop
void payload () {
__r0 = 0;
LOOP :
__r0 += 1;
if ( __r0 != 128)
goto LOOP ;
Spawn a shell
void payload () {
string prog = "/ bin / sh \0 ";
int64 * argv = {& prog , 0 x0 };
__r0 = & prog ;
__r1 = & argv ;
__r2 = 0;
returnto 0 x446730 ;
execve ( __r0 , __r1 , __r2 );
}
}
Table 1: Examples of SPL payloads.
or processor architectures. SPL is a dialect of C. Compared to min-
DOP [34], SPL allows use of both virtual registers and memory for
operations and declaration of variables/constants. Table 1 shows
some sample payloads. Overall, SPL has the following features:
Figure 2: BOP gadget structure. The functional part consists
of a single basic block that executes an SPL statement. Two
functional blocks are chained together through a series of
dispatcher blocks, without clobbering the execution of the
previous functional blocks.
For BOP, gadgets are chains of entire basic blocks (sequences of
instructions that end with a direct or indirect control-flow transfer),
as shown in Figure 2. A BOP chain consists of a sequence of BOP
gadgets where each BOP gadget is: one functional block that imple-
ments a statement in an SPL payload and zero or more dispatcher
blocks that connect the functional block to the next BOP gadget in
a manner that complies with the CFG.
Second, BOPC abstracts each basic block from individual in-
structions into Block Constraint Summaries, enabling blocks to be
employed in a variety of different ways. That is, a single block
may perform multiple functional and/or dispatching operations by
utilizing different sets of registers for different operations. That is,
a basic block that modifies a register in a manner that may fulfill
an SPL statement may be used as a functional block, otherwise it
may be considered to serve as a dispatcher block.
BOPC leverages abstract Block Constraint Summaries to apply
blocks in multiple contexts. At each stage in the development of
a BOP chain, the blocks that may be employed next in the CFG
as dispatcher blocks to connect two functional blocks depend on
the block summary constraints for each block. There are two cases:
either the candidate dispatcher block’s summary constraints indi-
cate that it will modify the register state set and/or the memory
state by the functional blocks, called the SPL state, or it will not,
enabling the computation to proceed without disturbing the effects
of the functional blocks. A block that modifies a current SPL state
unintentionally, is said to be a clobbering block for that state. Block
summary constraints enable identification of clobbering blocks at
each point in the search.
An important distinction between BOP and conventional ROP
(and variants) is that the problem of computing BOP chains is NP-
hard, as proven in Appendix B. Conventional ROP assumes that
indirect control-flows may target any executable byte in memory
while BOP must follow a legal path through the CFG for any chain
of blocks, resulting in the need for automation.
4.1 Expressing Payloads
BOPC provides a programming language, called SPloit Language
(SPL) that allows analysts to express exploit payloads in a com-
pact high-level language that is independent of target programs
• It is Turing-complete;
• It is architecture independent;
• It is close to a well known, high level language.
Compared to existing exploit development tools [30, 53, 55], the
architecture independence of SPL has important advantages. First,
the same payload can be executed under different ISAs or operat-
ing systems. Second, SPL uses a set of virtual registers, accessed
through reserved volatile variables. Virtual registers increase flex-
ibility, which in turn increases the chances of finding a solution:
virtual registers may be mapped to any general purpose register
and the mapping may be changed dynamically.
To interact with the environment, SPL defines a concise API
to access OS functionality. Finally, SPL supports conditional and
unconditional jumps to enable control-flow transfers to arbitrary
locations. This feature makes SPL a Turing-complete language, as
proven in Appendix C. The complete language specifications are
shown in Appendix A in Extended Backus–Naur form (EBNF).
The environment for SPL differs from that of conventional lan-
guages. Instead of running code directly on a CPU, our compiler
encodes the payload as a mapping of instructions to functional
blocks. That is, the underlying runtime environment is the target
binary and its program state, where payloads are executed as side
effects of the underlying binary.
4.2 Selecting functional blocks
To generate a BOP chain for an SPL payload, BOPC must find a
sequence of blocks that implement each statement in the SPL pay-
load, which we call functional blocks. The process of building BOP
chains starts by identifying functional blocks per SPL statement.
Conceptually, BOPC must compare each block to each SPL state-
ment to determine if the block can implement the statement. How-
ever, blocks are in terms of machine code and SPL statements are
high-level program statements. To provide flexibility for matching
blocks to SPL statements, BOPC computes Block Constraint Sum-
maries, which define the possible impacts that the block would
have on SPL state. Block Constraint Summaries provide flexibility
in matching blocks to SPL statements because there are multiple
possible mappings of SPL statements and their virtual registers to
the block and its constraints on registers and state.
The constraint summaries of each basic block are obtained by
isolating and symbolically executing it. The effect of symbolically
(a)
(b)
(c)
Figure 3: Visualisation of BOP gadget volatility, rectangles:
SPL statements, dots: functional blocks (a). Connecting any
two statements through dispatcher blocks constrains re-
maining gadgets (b), (c).
executing a basic block creates a set of constraints, mapping input
to the resultant output. Such constraints refer to registers, memory
locations, jump types and external operations (e.g., library calls).
To find a match between a block and an SPL statement the block
must perform all the operations required for that SPL statement.
More specifically, the constraints of the basic block should contain
all the operations required to implement the SPL statement.
4.3 Finding BOP gadgets
BOPC computes a set of all potential functional blocks for each
SPL statement or halts if any statement has no blocks. To stitch
functional blocks, BOPC must select one functional block and a
sequence of dispatcher blocks that reach the next functional block
in the payload. The combination of a functional block and its dis-
patcher blocks is called a BOP gadget, as shown in Figure 2. To build
a BOP gadget, BOPC must select exactly one functional block from
each set and find the appropriate dispatcher blocks to connect to a
subsequent functional block.
However, dispatcher paths between two functional blocks may
not exist either because there is no legal path in the CFG between
them, or the control flow cannot reach the next block due to un-
satisfiable runtime constraints. This constraint imposes limits on
functional block selection, as the existence of a dispatcher path
depends on the previous BOP gadgets.
BOP gadgets are volatile: gadget feasibility changes based on the
selection of prior gadgets for the target binary. This is illustrated in
Figure 3. The problem of selecting a suitable sequence of functional
blocks, such that a dispatcher path exists between every possible
control flow transfer in the SPL payload, is NP-hard, as we prove
in Appendix B. Even worse, an approximation algorithm does not
exist.
As the problem is unsolvable in polynomial time in the general
case, we propose several heuristics and optimizations to find solu-
tions in reasonable amounts of time. BOPC leverages basic block
proximity as a metric to “rank” dispatcher paths and organizes this
information into a special data structure, called a delta graph that
provides an efficient way to probe potential sequences of functional
blocks.
4.4 Searching for dispatcher blocks
While each functional block executes a statement, BOPC must
chain multiple functional blocks together to execute the SPL pay-
load. Functional blocks are connected through zero or more basic
Function_1:
...
call Function_2
...
Function_2:
...
B:
A:
2
3
call Function_2
retn
...
retn
4
1
Figure 4: Existing shortest path algorithms are unfit to mea-
sure proximity in the CFG. Consider the shortest path from
A to B. A context-unaware shortest path algorithm will mark
the red path as solution, instead of following the blue arrow
upon return from Function_2, it follows the red arrow (3).
blocks that do not clobber the SPL state computed thus far. Finding
such non-clobbering blocks that transfer control from one func-
tional statement to another is challenging as each additional block
increases the constraints and path dependencies. Thus, we propose
a graph data structure, called the delta graph, to represent the state
of the search for dispatcher blocks. The delta graph stores, for each
functional block for each SPL statement, the shortest path to the
next candidate block. Stitching arbitrary sequences of statements is
NP-hard as each selected path between two functional statements
influences the availability of further candidate blocks or paths, we
therefore leverage the delta graph to try likely candidates first.
The intuition behind the proximity of functional blocks is that
shorter paths result in simpler and more likely satisfiable con-
straints. Although this metric is a heuristic, our evaluation (Sec-
tion 6) shows that it works well in practice.
The delta graph enables quick elimination of sets of functional
blocks that are highly unlikely to have dispatcher blocks and thus
constitute a BOP gadget. For instance, if there is no valid path in the
CFG between two functional blocks (e.g., if execution has to traverse
the CFG “backwards”), no dispatcher will exist and therefore, these
two functional blocks cannot be part of the solution.
The delta graph is a multi-partite, directed graph that has a set
of functional block nodes for every payload statement. An edge
between two functional blocks represents the minimum number
of executed basic blocks to move from one functional block to the
other, while avoiding clobbering blocks. See Figure 7 for an example.
Indirect control-flow transfers pose an interesting challenge
when calculating the shortest path between two basic blocks in a
CFG: while they statically allow multiple targets, at runtime they
are context sensitive and only have one concrete target.
Our context-sensitive shortest path algorithm is a recursive ver-
sion of Dijkstra’s [11] shortest path algorithm that avoids all clob-
bering blocks.. Initially, each edge on the CFG has a cost of 1. When
it encounters a basic block with a call instruction, it recursively
calculates the shortest paths starting from the calling function’s en-
try block, BE (a call stack prevents deadlocks for recursive callees).
If the destination block, BD, is inside the callee, the shortest path
is the concatenation of the two individual shortest paths from the
beginning to BE and from BE to BD. Otherwise, our algorithm finds
Long path with simple constraints
a , b , c , d , e = input () ;
// point A
if (a == 1) {
if (b == 2) {
if (c == 3) {
if (d == 4) {
if (e == 5) {
// point B