### Browser Statistics Collection

Browser statistics were collected over a 5-week period from the CIS Department web server at the University of Pennsylvania. As shown in the figure, up to six versions of a particular client can be expected. We anticipate that this distribution will stabilize around final release versions, and we expect to minimize the number of different versions that need to be checked based on their popularity.

### Filtering and Anomaly Detection

#### IXP1200-based Firewall/Load-Balancer

We first evaluated the performance of an IXP1200-based firewall/load-balancer. The IXP1200 evaluation board used in this study has two Gigabit Ethernet interfaces and eight Fast Ethernet interfaces. The Gigabit Ethernet interfaces connect to the internal and external networks, while the Fast Ethernet interfaces communicate with the sensors. A set of client workstations generated traffic through the firewall, which then forwarded the traffic to the sensors for processing. The sensors determined whether the traffic should be dropped, redirected to the shadow honeypot, or forwarded to the internal network.

Previous studies [38] have reported forwarding rates of at least 1600 Mbit/s for the IXP1200 when used as a simple forwarder/router, which is sufficient to saturate a Gigabit Ethernet interface. Our measurements show that despite the added cost of load balancing, filtering, and coordinating with the sensors, the firewall can still handle the Gigabit Ethernet interface at line rate.

To gain further insight into the actual overhead of our implementation, we conducted a second experiment using Intel’s cycle-accurate IXP1200 simulator. We assumed a clock frequency of 232 MHz for the IXP1200 and an IX bus configured to be 64-bit wide with a clock frequency of 104 MHz. In the simulated environment, we obtained detailed utilization measurements for the microengines of the IXP1200. The results, shown in Table 10, indicate that even at line rate and under worst-case traffic conditions, the implementation is quite efficient, with the microengines operating at 50.9%-71.5% of their processing capacity. These results provide further insight into the scalability of our design.

#### PC-based Sensor Performance

We also measured the throughput of the PC-based sensors that cooperate with the IXP1200 for analyzing traffic and performing anomaly detection. For this experiment, we used a 2.66 GHz Pentium IV Xeon processor with hyper-threading disabled. The PC had 512 Mbytes of DDR DRAM at 266 MHz, and the PCI bus was 64-bit wide and clocked at 66 MHz. The host operating system was Linux (kernel version 2.4.22, Red-Hat 9.0).

We used LAN traces to stress-test a single sensor running a modified version of Snort, which, in addition to basic signature matching, provides the hooks needed to coordinate with the IXP1200, as well as the APE and payload sifting heuristics. We replayed the traces from a remote system through the IXP1200 at different rates to determine the maximum loss-free rate (MLFR) of the sensor. For this experiment, we connected a sensor to the second Gigabit Ethernet interface of the IXP1200 board.

The measured throughput of the sensor for signature matching using APE and Earlybird is shown in Table 1. The throughput per sensor ranges between 190 Mbit/s (APE) and 268 Mbit/s (payload sifting), while standard signature matching can be performed at 225 Mbit/s. This means that we need at least 4-5 sensors behind the IXP1200 for each of these mechanisms. Note, however, that these results are conservative and based on unoptimized code, and thus only serve to provide a rough estimate of the cost of anomaly detection.

### False Positive vs. Detection Rate Trade-offs

We determined the workload generated by the AD heuristics by measuring the false positive rate. We also considered the trade-off between false positives and detection rate to demonstrate how the AD heuristics could be tuned to increase detection rate in our shadow honeypot environment. We used the payload sifting implementation from [8] and the APE algorithm from [48]. The APE experiment corresponds to a tightly-coupled shadow server scenario, while the payload sifting experiment examines a loosely-coupled shadow honeypot scenario that can be used for worm detection.

We ran the modified Snort sensor implementing APE and payload sifting on packet-level traces captured on an enterprise LAN with roughly 150 hosts. The traces contained several instances of the Welchia worm. APE was applied to the URIs contained in approximately one billion HTTP requests gathered by monitoring the same LAN.

Figure 11 demonstrates the effects of varying the distinct destinations threshold of the content sifting AD on the false positives (measured in requests to the shadow services per minute) and the (Welchia worm) detection delay (measured in the ratio of hosts in the monitored LAN infected by the time of detection). Increasing the threshold means more attack instances are required for triggering detection, thereby increasing the detection delay and reducing false positives. It is evident that to achieve a zero false positive rate without shadow honeypots, the system must operate with parameters that yield a suboptimal detection delay.

The detection rate for APE is the minimum sled length it can detect and depends on the sampling factor and the MEL parameter (the number of valid instructions that trigger detection). A high MEL value reduces false positives due to random valid sequences but also makes the heuristic blind to sleds of smaller lengths. Figure 12 shows the effects of the MEL threshold on the false positives. APE can be used in a tightly coupled scenario, where suspect requests are redirected to instrumented server instances. The false positives (measured in requests to the shadow services per minute by each of the normal services under maximum load) can be handled easily by a shadow honeypot. APE alone has false positives for the entire range of acceptable operational parameters; it is the combination with shadow honeypots that removes the problem.

### Limitations

There are three limitations of the shadow honeypot design presented in this paper:

1. **Effectiveness of Rollback Mechanism**: The rollback mechanism's effectiveness depends on the proper placement of calls to `transaction()` for committing state changes and the latency of the detector. The detector used in this paper can instantly detect attempts to overwrite a buffer, preventing system corruption. However, other detectors may have higher latency, making the placement of commit calls critical for recovery from attacks. Depending on the detector latency and the cost of implementing rollback, different approaches may be necessary. These trade-offs are thoroughly examined in the fault-tolerance literature (c.f. [14]).

2. **Loosely Coupled Client Shadow Honeypot**: This type of honeypot is limited to protecting against relatively static attacks. It cannot effectively emulate user behavior that may be involved in triggering the attack, such as through DHTML or JavaScript. The loosely coupled version is also weak against attacks that depend on local system state on the user’s host, which is difficult to replicate. This is not a problem with tightly coupled shadows, as they accurately mirror the state of the real system. In some cases, it may be possible to mirror state on loosely coupled shadows, but this was not considered in the experiments presented in this paper.

3. **Feedback from Shadow Honeypot**: We have not explored in depth the use of feedback from the shadow honeypot to tune the anomaly detection components. Although this is likely to lead to substantial performance benefits, we need to be careful to prevent attackers from launching blinding attacks, such as "softening" the anomaly detection component through a barrage of false positives before launching a real attack.

### Related Work

Much of the work in automated attack reaction has focused on the problem of network worms, which have reached epidemic proportions. For example, the system described in [56] detects worms by monitoring probes to unassigned IP addresses ("dark space") or inactive ports and computing statistics on scan traffic, such as the number of source/destination addresses and the volume of captured traffic. By measuring the increase in the number of source addresses seen in a unit of time, it is possible to infer the existence of a new worm when as little as 4% of the vulnerable machines have been infected. A similar approach for isolating infected nodes inside an enterprise network [41] is taken in [15], where it was shown that as few as four probes may be sufficient in detecting a new port-scanning worm. [54] describes an approximating algorithm for quickly detecting scanning activity that can be efficiently implemented in hardware.

[34] describes a combination of reverse sequential hypothesis testing and credit-based connection throttling to quickly detect and quarantine local infected hosts. These systems are effective only against scanning worms (not topological, or "hit-list" worms) and rely on the assumption that most scans will result in non-connections. As such, they are susceptible to false positives, either accidentally (e.g., when a host is joining a peer-to-peer network such as Gnutella, or during a temporary network outage) or on purpose (e.g., a malicious web page with many links to images in random or unused IP addresses). Furthermore, it may be possible for several instances of a worm to collaborate in providing the illusion of several successful connections, or to use a list of known repliers to blind the anomaly detector. Another algorithm for finding fast-spreading worms using two-level filtering based on sampling from the set of distinct source-destination pairs is described in [50].

[55] correlates DNS queries/replies with outgoing connections from an enterprise network to detect anomalous behavior. The main intuition is that connections due to random-scanning (and, to a degree, hit-list) worms will not be preceded by DNS transactions. This approach can be used to detect other types of malicious behavior, such as mass-mailing worms and network reconnaissance.

[17] describes an algorithm for correlating packet payloads from different traffic flows to derive a worm signature that can then be filtered [23]. The technique is promising, although further improvements are required to allow it to operate in real-time. Earlybird [36] presents a more practical algorithm for doing payload sifting and correlates these with a range of unique sources generating infections and destinations being targeted. However, polymorphic and metamorphic worms [46] remain a challenge; Spinelis [39] shows that it is an NP-hard problem. Buttercup [25] attempts to detect polymorphic buffer overflow attacks by identifying the ranges of possible return memory addresses for existing buffer overflow vulnerabilities. Unfortunately, this heuristic cannot be employed against some of the more sophisticated overflow attack techniques [26]. Furthermore, the false positive rate is very high, ranging from 0.01% to 1.13%. Vigna et al. [51] discuss a method for testing detection signatures against mutations of known vulnerabilities to determine the quality of the detection model and mechanism.

In [52], the authors describe a mechanism for pushing vulnerability patches to workstations, which can help mitigate the impact of new vulnerabilities.