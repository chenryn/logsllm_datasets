read only
not mapped
 Guest VM
Xen Slice
Shared Service
Secure Monitor
Object
Viewer
 Guest VM
Xen Slice
Shared Service Secure Monitor
Fig. 3. Memory access permissions of different components of Nexen.
The security monitor does not have its own address space.
It is shared by all internal domains. Every domain can directly
request its services, but no one is able to tamper with the
monitor’s inner data. Shared service and every Xen slice have
their own address space. Shared service has its own piece of
data and code. Xen slices share the same code while each has
its own data.
3) Controlling Privileged Instructions: Privileged instruc-
tion is the second class of privilege controlled by Nexen. Many
5
special instructions may potentially violate the memory access
policies or even directly harm other internal domains than the
caller. Execution of these instructions must undergo careful
examination. We treat them as “privileged” instructions.
Nexen limits the instruction set each internal domain can
execute to avoid abuse of privileged instructions. No direct
execution of privileged instructions is allowed outside the
security monitor. Internal domains request
the monitor to
execute the instructions for them. The monitor enforces a
sound sanity check to prevent unauthorized use and malicious
use of these instructions.
4) Control Flow: To support the interaction between in-
ternal domains, Nexen provides a secure call gate for domain
switching:
•
nx_entry(int domain_type, int dom_id)
The domain_type argument speciﬁes whether the switch target
is a Xen slice or the shared service. When domain type is Xen
slice, ‘dom_id’ shows the ID of the target Xen slice.
The call gate guarantee the following features:
•
•
•
Non-bypassable: It is impossible to switch to another
internal domain without calling the gate.
Unforgeable: Any call gate not called from its ex-
pected position is not accepted. Each call gate is bound
to both its return address and its domain_type
argument, which must be hard-coded.
Atomic: Switches of control ﬂow and address space
are atomic. Any attempt
to switch to an internal
domain’s address space while redirecting the control
ﬂow to another one will fail the return address check
and be rejected.
Even if the attacker has successfully exploited a vulnerability
leading to privilege escalation, she will only gain full control
inside one internal domain, which normally is a Xen slice. To
do any meaningful attack, the attacker must try to intrude into
another domain through the call gates, which is much harder
than the initial exploit.
We carefully restrict control ﬂows to minimize the possi-
bility for internal domains to attack each other.
Guest VM
Xen Slice
Nexen
Shared Srv
Gate Keeper
                 Secure Monitor
Fig. 4. The control ﬂow between components in Nexen.
the hypervisor to VMs. On entering the hypervisor, the gate
keeper records the trap frame and dispatch the control ﬂow
to the correct internal domain. On exiting the hypervisor, the
gate keeper checks VMCS and other necessary running states
to make sure no policy is violated. If the reason of entering the
hypervisor is an interrupt, the control ﬂow will be transferred
to the shared service. The shared service will deal with the
interrupt itself or dispatch it to a VM and exit to current VM’s
Xen slice. For other reasons, mainly a hypercall or a code
emulation, the control ﬂow directly goes to current VM’s Xen
slice.
A Xen slice usually deal with the request itself. It may use
some services provided by shared service or security monitor.
The control ﬂow will always return to the Xen slice except
for scheduling. Eventually, the control ﬂow goes to the gate
keeper to return to the VM.
We explicitly forbid switching between Xen slices since
no such a need exists. This eliminates the possibility that a
malicious Xen slice can directly intrude into another Xen slice
or further attack the bound VM. The only way to switch control
ﬂow from one VM and its Xen slice to another is scheduling.
Since the scheduler doesn’t receive any input generated by
a guest, and the context after the switch is decided by the
target VM and its Xen slice, an attacker can hardly intrude
through this way, which is also evidenced by the scheduler’s
low vulnerability ratio even in unmodiﬁed Xen.
Communication between Xen slices and shared service is
strictly limited to prevent a malicious Xen slice from attacking
the shared service. The only situations where control ﬂow is
allowed to transfer from a Xen slice to shared service are
discussed in the next subsection. We’ll show that all these
transfers are secure enough and can hardly be used as an attack
surface.
E. Decomposing into Slices
The monitor and slicing services provide powerful isolation
and code integrity properties for Xen. They form the founda-
tion for Nexen to apply security relevant partitioning of Xen.
Our deconstruction strategy is to apply the principle of
least-privilege: minimize the authority of each domain in the
system. The best solution would be to create a complete Xen
slice for each VM. Unfortunately, there is functionality that
interacts across much of the Xen system. Another challenge is
to select partitions that minimize the interface between the
shared service and per-VM slices to minimize API abuse.
Signiﬁcant data structures must also be wisely arranged to
avoid destructive corruption.
As such Nexen must deconstruct Xen in a way that
intelligently partitions functionality—maximize the value of
least-privilege while minimizing cross-domain interactions.
To manage this we identify functionality that is shared and
place it in the Nexen shared service domain, which operates
at a slightly higher privilege level than per-VM slices. The
higher privilege enables special data and cross-domain calling
privileges solely for the shared service domain.
There is a small piece of code called gate keeper that
controls all exits from VMs to the hypervisor and entries from
Another high level idea of our strategy is to derive our
partitioning from the vulnerability analysis. Much like device
6
driver isolation was motivated by the high degree of vulnera-
bility of drivers, we identify the Xen functionality that is most
likely to be corrupted and place that into the per-VM slices.
On the other side we identify core components that must be in
the shared service domain due to the nature of their operations.
To sum up, we follow three principles in decomposition
work to enhance the security of the system. The ﬁrst one
is to avoid inserting dangerous functionalities into the shared
service, which is very intuitive. The second one is to avoid
runtime communication. Components in shared service can
be safe even if it contains relatively more vulnerabilities as
long as guests are not able to invoke them actively during
runtime. The third one is to separate mechanism from policy.
Complex calculation and decision making can be processed by
untrusted code. But the ﬁnal security sensitive operation must
be performed by trusted code. This principle can effectively
reduce the size of trusted code and alleviate the burden of
sanity check.
1) Signiﬁcant Component Decisions: In decomposing Xen
there are several signiﬁcant decisions to make either because
the functionality is pervasive or it
is a highly vulnerable
component. In this section we provide further analysis of
our deconstruction. The Scheduler, Memory Management, and
Event Channels are all or partly placed in Nexen shared service.
Due to its high complexity and vulnerability, Code Emulation
and I/O are split across per-VM slices. Note that this is not an
exhaustive list.
Scheduler determines which VCPU currently runs on one
CPU. Each VCPU has a credit used to calculate its priority. It
burns as a VCPU runs. Each CPU has a runqueue recording
all VCPUs and their priority. The queue is periodically sorted
to maintain the priority order of VCPUs. There are also some
global parameters controlling the speed of burning credits, the
rate limit of scheduling and other issues. When a scheduling
operation happens,
the VCPU with the highest priority is
picked to run in next round.
We can observe that all signiﬁcant data is naturally closed
for scheduler’s inner use. Credit burning, runqueue sorting
and scheduling are all triggered by timer interrupts without
any interference from guests. Further, CPU is usually shared
between VMs and is not suitable to be assigned to any Xen
slice. Hence, we put the whole scheduler inside the shared
service. No Xen slice is allowed to modify the state of the
scheduler.
There are occasions where a guest wants to yield, sleep
or wake its VCPU. We open these three interfaces to Xen
slice. Their only input
is the VCPU pointer used to ﬁnd
its corresponding data structure inside the scheduler, whose
validity will be checked on the shared service’s side. Nothing
changes except for the guest’s own VCPU’s existence in the
runqueue after these operations. Hardly can any malicious data
be delivered through this interface, nor can any dangerous
behavior be performed.
Event Channel delivers various events between guest
VMs, the hypervisor and hardware. Each VM has its own event
channel bucket, which contains a number of event channel
ports. A port can be bound to an interrupt or another VM’s
port. VMs maintain their own buckets while the hypervisor
helps to deliver the events.
Since event channel buckets are tightly bound to VMs, we
put each VM’s bucket in its own Xen slice to avoid abuse from
other VMs. Interrupts related to one VM will be forwarded to
its Xen slice, so delivery of events bound to interrupts are
done by Xen slices. When a VM send event to another VM, it
will have difﬁculty writing the pending bit in the target VM’s
port for the lack of writing permission. We proxy this request
through the shared service, which is another interface open
to Xen slices. This request is safe enough because the only
information provided by the sender is the target VM’s port
number, which is easy to examine.
Memory Management contains everything related to
memory operation, mainly memory allocation and memory
mapping update. The core data structures are page tables and
a page_info region maintaining the usage state of each
physical page. Allocator’s free page list is built based on the
linked list ﬁeld in page_info structure. Other information
in page_info is referred to and updated during memory
mapping update.
Allocator is only used during booting and domain building,
so we keep it in the shared service and do not expose any inter-
face to Xen slices. Memory mapping update is mostly related
to only one VM. Xen slices are allowed to manage their bound
VM’s memory mapping update. One challenge here is that
page_info region is required by both functionalities, each
in a different type of internal domain. Fortunately, memory
region of Xen and each VM has a clear boundary. We map the
region in every internal domain, but only grant to each Xen
slice the writing permission of their own page information.
Both functions work nicely in this way. Apart from that, no
internal domain has the permission to write page tables. After
making the page table update decision, they must request the
monitor to perform the operation. Such updates are carefully
checked to make sure no policy is violated.
Code Emulation and I/O are related to CPU features more
than memory. These parts of the hypervisor provide virtual
CPU and devices for VMs by catching and emulating VM’s
privileged operations. The emulation code sometimes runs the
privileged instructions itself to get necessary data. This whole
process is extremely error-prone. Attackers may directly cause
an exception, corrupting memory in another module, or steal
sensitive data from other VMs through the misuse of hardware
features.
We run the emulation code in each VM’s Xen slice and
grant VM’s VCPU running state to the Xen slice. Although
the misused hardware features are largely out of our control,
attempt to corrupt other parts of the system must go back
to memory. Even if the attacker has successfully raised her
privilege to that of the host, Nexen can still enforce the memory
access policies. The attacker will be isolate in the Xen slice and
achieve nothing worthwhile. If the attacker chooses to trigger
a deadly exception, the handlers are modiﬁed so that only
the Xen slice, instead of the whole system, is destroyed. We
are able to do this because Nexen’s memory isolation gives
the guarantee that no crucial data are corrupted during the
exception.
7
TABLE VI.
CONTENT OF SHARED SERVICE
Shared service content
Scheduler
Allocator part of memory management
Interrupt handlers
Domain building
Event delivery of event channel
IV. Nexen IMPLEMENTATION
We implemented a prototype of Nexen based on Xen
version 4.5 for Intel x86-64 architecture. This section describes
how we address three main technical challenges: First, how
to enforce inter-domain isolation and memory access policy?
Second, how to control privileged instructions? Third, how to
monitor the interposition between the hypervisor and VMs?
A. Isolation between Internal Domains
We used an isolation mechanism based on memory map-
ping. Each internal domain resides in its own virtual address
space. The permission bits in page table entries are set accord-
ing to memory access policies. In this way, Nexen controls
what every internal domain is allowed to read and write.
To achieve this, Nexen interposes memory mapping updates
and enforces a set of carefully selected invariants to provide
ﬂexibility in applying policies on any memory region. Addi-
tionally, Nexen allows interaction between internal domains
while retaining the isolation.
1) Control Memory Mapping Update: Nexen maps all
page-table-pages as read-only in all address spaces and enables
the Write Protection (WP) bit, forbidding mapping updates.
The security monitor’s internal data is also mapped read-only
to avoid modiﬁcation from malicious domains. As control
ﬂows into the monitor the WP-bit is ﬂipped so that the monitor
can update page tables and maintain its internal data structures.
On exiting the monitor, WP-bit
is enabled. Interrupts are
disabled while executing in the monitor to make sure no entity
can hijack the control ﬂow when the WP-bit is turned off.
In this way, the monitor completely controls all mappings.
Memory mapping updates in internal domains are replaced
by calls into the monitor, but the logic for their management
remains in the domain.
2) Enforcing Memory Invariants: Before each memory
mapping update,
the security monitor needs to do sanity
checking to enforce certain invariants. These invariants are
independent of policies and have the highest priority in all
rules. They keep the memory layout and the most signiﬁcant
data structures of the system intact in any condition. The
invariants of each type of memory are shown in Table VII.
The monitor maintains an internal data structure recording
the usage of each physical page. Invariants are mostly based
on the memory page’s usage type and owner.
3) Enforcing Memory Isolation Policies: To provide a
ﬂexible way to protect internal domains’ data, memory of
Protected Data type can have various policies. The monitor
has a special allocator inside itself for allocating protected
data, through which policies can be speciﬁed for each memory
region. The allocator has an inner memory pool recording the
address, size and policy related information for every allocated
and free memory region. When building a domain, the shared
service will ask for memory from the allocator for the new Xen
slice’s data structures. The policy is given to the monitor along
with the request. All the information is recorded in the memory
pool. When the new address space is eventually created, the
monitor will traverse the memory pool and modify mappings
for recorded memory regions according to their policy. As is
described in the invariants, mappings for protected data will
not be changed once the owner VM has started. So no one can
trick the monitor into exposing other domain’s data.
Frequently used policies are:
• Grant: Data is granted to a Xen slice. They can be
modiﬁed by its owner Xen slice. In other internal
domains they are mapped read-only.
• Half Grant: Data is granted to a Xen slice. They can
be modiﬁed by the owner Xen slice and the shared
service. In other Xen slices they are mapped read-
only.
• Grant and Hide: Data is granted to a Xen slice. They
can be modiﬁed by owner Xen slice. In other internal
domains they are unmapped.
•
•
Limit: Data is granted to the shared service. In all
Xen slices they are mapped read-only.
Limit and Hide: Data is granted to the shared service.
In all Xen slices they are unmapped.
4) Securing Call Gate: Nexen provides a gate allowing the
switch between internal domains. It is a function call with the
target internal domain’s type and id as arguments. The function
itself switches the address space to that of the target domain.
The return address determines the target domain’s entry point.
To make sure the switches of control ﬂow and address
space happen atomically, we turn off interrupt during the
execution of the gate and check the validity of the function
call’s return address. All gates are placed at ﬁxed places in the
code. Calling through a function pointer is forbidden. All valid
return addresses for an internal domain are collected with the
help of the compiler and stored in a table. At each call of the
gate, the return address is searched in the table. Only if it is a
valid one recorded in the table will the address space switch
be executed. The table is sorted and search with binary search
to speed up the process.
B. Conﬁning Privileged Instructions
To ensure isolation, Nexen must control the execution of
sensitive instructions. Nexen achieves this with two methods:
“monopolize” and “hide”. Internal domains are deprivileged
from direct access to sensitive instructions. Similar to memory
mapping validations, any uses of sensitive instructions are
forwarded to the security monitor, which ensures all invariants
to maintain isolation.
A “monopolized” instruction only has one instance within
the monitor’s code. Under this constraint, the instruction is
still visible to internal domains. The attacker can either call the
monitor’s interface or directly jump to the instruction if she has
successfully hijacked control ﬂow. If the bad consequence of
the instruction does not occur immediately after the execution,
8
TABLE VII.
PROTECTION INVARIANTS FOR DIFFERENT TYPES OF MEMORY PAGES
Page Type
Guest Memory
Sensitive Guest Memory
Host Code
Monitor Data