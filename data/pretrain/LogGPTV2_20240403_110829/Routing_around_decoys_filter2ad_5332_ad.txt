D
0
3
.
0
5
1
.
0
0
0
.
0
blocked
notblocked
40
50
60
70
80
90
40
45
50
55
60
65
40
60
80
100
120
140
40
Latency (ms)
(a) Amazon
Latency (ms)
(b) Gmail
Latency (ms)
(c) Facebook
50
45
Latency (ms)
55
(d) blocked.telex.cc
Figure 5: Comparing distribution of latencies from notblocked.telex.cc to (a) Amazon (b) Gmail (c) Facebook and (d) blocked.telex.cc
s
e
v
i
t
i
s
o
P
e
u
r
T
0
.
1
8
.
0
6
.
0
4
.
0
2
.
0
0
.
0
s
e
v
i
t
i
s
o
P
e
u
r
T
0
.
1
8
.
0
6
.
0
4
.
0
2
.
0
0
.
0
DB size 10
DB size 25
DB size 50
s
e
v
i
t
i
s
o
P
e
u
r
T
0
.
1
8
.
0
6
.
0
4
.
0
2
.
0
0
.
0
DB w/o filter
DB w/filter
s
e
v
i
t
i
s
o
P
e
u
r
T
0
.
1
8
.
0
6
.
0
4
.
0
2
.
0
0
.
0
DB w/o filter
DB w/filter
DB w/o filter
DB w/filter
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
False Positives
False Positives
False Positives
False Positives
(a) ROC curves for all
(b) Database size 10
(c) Database size 25
(d) Database size 50
Figure 7: Comparing ROC curves for different database sizes in (a), and comparing ROC curves with and without ﬁltering entries based on
inter K-S scores in (b)-(d).
actions performed by the Telex station itself is causing some of the
noticeable differences in latency measurements.
So far, all experiments have been run from a single machine
which resides approximately 25 to 30 milliseconds away from the
Telex station and servers. One possibility is that the further away
the client is, the more noisy the connection will be, hiding any over-
head or differences in path which are incurred by using Telex. Us-
ing PlanetLab, we selected 40 hosts, ranging from 50 to 250 mil-
liseconds away from the Telex station and the overt destination,
then ran the same previous experiments for each host, using the
set of nearby servers from the previous experiments, along with
blocked.telex.cc. These experiments were run sequentially
instead of in parallel, in order to minimize any extra workload on
the Telex station.
Figure 8: Surface plot of K-S score depending on client distance
from Telex and distance between overt and covert destinations.
The results of these runs can be seen in Figure 8. Note that none
of the Kolmogorov-Smirnov scores that were calculated were be-
low 0.26, even when all the hosts were using blocked.telex.
cc as the covert destination. In addition, we do not see any gen-
eral trend of lower scores for hosts located further away as we had
initially thought. Instead, we seem to only see background noise
in the Kolmogorov-Sminov scores, with no relation to the distance
of the host at all. Additionally, we looked at the latencies for each
host when connecting to blocked.telex.cc and found that the
range of scores returned was between 0.25 and 0.8. This still almost
completely falls out of the range of values you would expect to see,
as the CDF for the overt comparisons shown in Figure 6 show a
range of 0.08 to 0.26.
5.3 Fingerprinting Covert Destinations
As we have seen, comparing distributions of latencies was an ef-
fective method for determining whether a client was either directly
connecting to the overt destination or if they were using Telex and
communicating with some unknown covert destination. In this sec-
tion, we show how similar techniques can be used to ﬁngerprint
covert destinations, allowing a warden to identify with which sites
a client is communicating.
The attack works as follows: ﬁrst the warden selects a set of
covert destinations to be included in the database. Then, since
the warden has the ability to enumerate all decoy routers (see Sec-
tion 4.1), they can build a database of latency distributions using
each decoy router. When a client makes a connection, the war-
den uses any of the previously mentioned detection methods to de-
termine if the client is using Telex, and then examines the path
to identify the decoy router being used. After doing so, the war-
den compares the latency distributions for that decoy router against
the observed latencies. As before, the Kolmogorov-Smirnov test
is used to compare latency distributions, using a threshold on the
d-value to decide when to accept or reject a sample. For our exper-
iments, we used the latency distributions captured for the Alexa top
100 sites, and for each threshold value we would randomly select
a ﬁxed size of the samples to be in the database, using 50 of the
100 captured latencies to include in the database, while the other
9250 were used to test for true positive rates. This was repeated 100
times for each threshold value to calculate the average true positive
and false positive rates. Figure 7a contains the results from these
experiment, showing the ROC curve for databases of size 10, 25
and 50, with AUC values of 0.868, 0.707 and 0.537 respectively.
As noted, these experiments randomly chose destinations to be
included in the database. However, a warden can build a database
in a more intelligent manner to improve the true positive rate while
keeping the false positive rate low. By setting a lower bound thresh-
old on the Kolmogorov-Smirnov score that any pair of entries can
have, the database is built while ensuring that no two distributions
are too similar. This way, the warden will be less likely to incor-
rectly classify an observed latency distribution. It should be noted
that the larger the database is, the lower the threshold value will
need to be, otherwise it will be impossible to ﬁnd enough entries
that are different enough from all the others. For our experiments,
we used threshold values of 0.8, 0.7 and 0.35 for database sizes 10,
25 and 50. Figures 7b-7d show the results after applying a thresh-
old on the database entries. We can see there is a signiﬁcant im-
provement in the ROC curves, particularly for the larger database
sizes.
s
e
v
r
u
c
C
O
R
r
o
f
C
U
A
8
.
0
4
.
0
0
.
0
DB size 10
DB size 25
DB size 50
0
10
20
30
40
50
Number of latency samples collected
Figure 9: AUC of the ROC curve for all database sizes using dif-
ferent number of samples to compare to database entries.
So far, when comparing latency distributions, we have assumed
that the warden has access to a somewhat large number of samples.
This might not always be practical, so we tested the effect varying
the number of samples had on the ROC curves. For the experi-
ments, we restricted the size of the samples in the database to 50,
while using the threshold method to ensure no two distributions in
the database were too similar. We then repeated the previous ex-
periments, creating an ROC curve while restricting the size of all
samples used to compare to the database, then calculating the AUC
for these ROC curves. Figure 9 shows the results from these ex-
periments. We can see that having about 12 samples is enough to
be able to consistently match distributions against the database. In
fact, when restricting the size of the database to 10 distributions,
even having just a few latency measurements was enough to gener-
ate ROC curves with AUC values above 0.8.
5.4 Timing Conclusions
As we have seen, a warden is able to infer a great deal of infor-
mation by simply making latency measurements of connections it
sees and comparing them to expected distributions. First, by com-
paring the distribution of latencies the warden would expect to see
to the overt destination to those it observes from a client, a war-
den can deﬁnitively run a conﬁrmation attack to tell if the client
is using Telex or actually communicating with the overt destina-
tion. Even when a client is using Telex to communicate with a
covert destination that is, for all practical purposes, running on the
same machine, the overhead from the Telex station performing the
man-in-the-middle actions is enough for a warden to be able to dis-
tinguish the latency distributions. Furthermore, we showed how a
warden can construct databases of latency distributions of chosen
covert destinations, which can be used by the warden to identify
with which covert destination the client is communicating. By in-
telligently building the database and limiting the size, the warden is
able to execute this with a remarkably high true positive rate while
in many cases keeping the false positive rate under 10%.
6. COUNTERMEASURES AND THEIR
LIMITATIONS
It is clear that a warden is able to launch attacks against decoy
routing systems if the containment of the warden is incomplete.
Sadly, achieving good containment for a warden is difﬁcult, even
for smaller, less well-connected ones, as discussed in Section 4.2.
Path diversity provides far too many alternative routes to be slowed
by small deployments of decoy routers. This raises an obvious
question: what does a successful deployment look like? As dis-
cussed previously, a decoy routing system needs to cover all paths
to a large enough set of destinations such that it is economically or
functionally infeasible for the warden to block these destinations.
But how would we best go about doing this? In a graph, a set of
vertices that partition the remaining vertices into two disconnected
sets is called a vertex separator. Finding an optimal vertex sepa-
rator is NP-complete, with good approximations existing only for
certain classes of graphs. We will instead focus on straightforward
constructions of vertex separators that, while not optimal, will pro-
vide the best properties for decoy routing systems.
One immediate option is to surround the warden with a “ring”
of decoy routers. The question is how many ASes would that en-
compass? Clearly the answer depends on how close to the warden
this ring is built. If it is built close to the warden, the ring will be
smaller than if it is built further out. For China, Syria, Iran, and
Egypt, we consulted AS relationships from CAIDA to measure the
size of this ring at various depths. We deﬁne an AS’s depth from a
warden to be its minimum distance, in AS hops, from that warden.
Hence, while there might be both a two hop and three hop path to
a given AS, we consider it at a depth of two, not a depth of three.
The sizes of the rings built by selecting all transit ASes at a given