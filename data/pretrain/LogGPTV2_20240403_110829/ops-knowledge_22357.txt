User: I have the usual large set of dependent jobs and want to run them effectively in a PBS cluster environment.  I have been using Ruffus and am pretty happy with it, but I also want to experiment a bit with other approaches.  
One that looks interesting in python is jug.  However, it appears that jug assumes that the jobs are homogeneous in their requirements.  I have some jobs that require 8GB RAM while others require only 100MB; some can consume all processors and some are single-threaded.  I'm aiming for being able to quickly assemble a pipeline, run it and have it "update" based on dependencies, and log reasonably so that I can see what jobs still need to be run.  Is anyone using jug or other similar system with these types of requirements?
Assistant: Author of jug here.
Jug does handle the dependencies very well. If you change any of the inputs or intermediate steps, running jug status will tell you the state of the computation.
There is currently no way to specify that some tasks (what jug calls jobs) should have multiple processes allocated to them. In the past, whenever I had tasks which were to be run in multiple threads, I was forced to take a worst-case-scenario approach and allocate all processes to the jug execute process.
This means, of course, that single-threaded tasks will take up all the processes. Since the bulk of the computation was in the multi-threaded tasks, it was acceptable.