should be independent of its share in the past.
In the absence of this memoryless property, ﬂows may experi-
ence starvation. For example, with virtual clock, if one ﬂow uses a
flow 1 flow 2 flow 3 (a)                                (b) Link CPU 0% 100% 50% Link CPU 0% 100% 50% job 1 job 2 r1 r2 0% 100% 50% 3link at full rate for one minute, and a second ﬂow becomes active,
then only the second ﬂow is serviced for the next minute, until their
virtual clocks equalize. Thus, the ﬁrst ﬂow starves for a minute.2
The concept of virtual time was proposed to address this pit-
fall
[24]. Instead of measuring real time, virtual time measures
the amount of work performed by the system. Informally, a virtual
time unit is the time it takes to send one bit of a unit-weight ﬂow
in the ﬂuid-ﬂow system. Thus, it takes l virtual time units to send
a packet of length l. Thus, virtual time progresses faster than real-
time when fewer ﬂows are active. In general, assuming a ﬂow with
weight w, it takes l/w virtual time units to send the packet in the
ﬂuid-ﬂow system.
Virtual time turns out to be expensive to compute exactly, so a va-
riety of algorithms have been proposed to implement FQ efﬁciently
by approximating it [10, 24, 18, 29, 9]. One of the main algorith-
mic challenges we address in our work is to extend this concept to
multiple resources that are consumed at different rates over time.
4. ANALYSIS OF EXISTING POLICIES
We initially explored two natural scheduling algorithms for mid-
dleboxes. The ﬁrst solution, called bottleneck fairness, turns out to
lack both strategy-proofness and the sharing guarantee. The sec-
ond, called per-resource fairness, performs fair sharing indepen-
dently at each resource. This would happen naturally in routers
that queue packets as they pass between different resources and
serve each queue via fair sharing. We initially pursued per-resource
fairness but soon discovered that it is not strategy-proof.
4.1 Bottleneck Fairness
In early work on resource scheduling for software routers, Egi et
al. [14] point out that most of the time, only one resource is con-
gested. They therefore suggest that the system should dynamically
determine which resource is congested and perform fair sharing on
that resource. For example, a middlebox might place new packets
from each ﬂow into a separate queue and serve these queues based
on the packets’ estimated CPU usage if CPU is a bottleneck, their
memory bandwidth usage if memory is a bottleneck, etc.
This approach has several disadvantages. First, it is not strategy-
proof. As we showed in Section 3.1, a ﬂow can nearly double its
share by artiﬁcially increasing its resource consumption to shift the
bottleneck.
Second, when neither resource is a clear bottleneck, bottleneck
fairness can rapidly oscillate, affecting the throughput of all ﬂows
and keeping some ﬂows below their share guarantee. This can hap-
pen readily in middleboxes where some ﬂows require expensive
processing and some do not. For example, consider a middlebox
with two resources, CPU and link bandwidth, that applies IPsec en-
cryption to ﬂows within a corporate VPN but forwards other trafﬁc
to the Internet. Suppose that an external ﬂow has a resource proﬁle
of h1, 6i (bottlenecking on bandwidth), while an internal ﬂow has
h7, 1i. If both ﬂows are backlogged, it is unclear which resource
should be considered the bottleneck.
Indeed, assume the system decides that the ﬁrst resource is the
bottleneck and tries to divide it evenly between the ﬂows. As a
result, the ﬁrst resource will process seven packets of ﬂow 1 for
every single packet of ﬂow 2. Unfortunately, this will congest the
second resource right away, since processing seven packets of ﬂow
1 and one packet of ﬂow 2 will generate a higher demand for re-
source 2 than resource 1, i.e., 7h1, 6i + h7, 1i = h14, 43i. Once
2A workaround for this problem would be for the ﬁrst ﬂow to never
use more than half of the link capacity. However, this leads to
inefﬁcient resource utilization during the ﬁrst minute.
Figure 5: Example of oscillation in Bottleneck Fairness [14].
Note that ﬂow 3 stays below 1
3 share of both resources.
resource 2 becomes the bottleneck, the system will try to divide this
resource equally. As a result, resource 2 will process six packets of
ﬂow 2 for each packet of ﬂow 1, which yields an overall demand of
h1, 6i + 6h7, 1i = h43, 12i. This will now congest resource 1, and
the process will repeat.
Such oscillation is a problem for TCP trafﬁc, where fast changes
in available bandwidth leads to bursts of losses and low throughput.
However, bottleneck fairness also fails to meet share guarantees for
non-TCP ﬂows. For example, if we add a third ﬂow with resource
proﬁle h1, 1i, bottleneck fairness always keeps its share of both
3 , as shown in Figure 5. This is because there is
resources below 1
no way, while scheduling based on one resource, to increase all the
3 before the other gets congested.
ﬂows’ share of that resource to 1
4.2 Per-Resource Fairness (PF)
A second intuitive approach is to perform fair sharing indepen-
dently at each resource. For example, suppose that incoming pack-
ets pass through two resources: a CPU, which processes these pack-
ets and then an output link. Then one could ﬁrst schedule packets
to pass through the CPU in a way that equalizes ﬂows’ CPU shares,
by performing fair queuing based on packets’ processing times, and
then place the packets into buffers in front of the output link that
get served based on fair sharing of bandwidth.
Although this approach is simple, we found that it is not strategy-
proof. For example, Figure 6(a) shows two ﬂows with resource
proﬁles h4, 1i and h1, 2i that share two resources. The labels of the
packets show when each packet uses each resource. For simplicity,
we assume that the resources are perfectly divisible, so both ﬂows
can use a resource simultaneously. Furthermore, we assume that
the second resource can start processing a packet only after the ﬁrst
one has ﬁnished it, and that there is only a 1-packet buffer for each
ﬂow between the resources. As shown in Figure 6(a), after the
initial start, a periodic pattern with a length of 7 time units emerges.
7 of the
As a result, ﬂow 1 gets resource shares h 4
7 of the second resource. Meanwhile, ﬂow 2 gets
ﬁrst resource and 1
7i.
resource shares h 3
7 , 6
Suppose ﬂow 1 artiﬁcially increases its resource consumption
to h4, 2i. Then per-resource fair queuing gives the allocation in
3i and ﬂow 2’s share is
Figure 6(b), where ﬂow 1’s share is h 2
3i. Flow 1 has thus increased its share of the ﬁrst resource by
3 , 2
h 1
16%, while decreasing ﬂow 2’s share of this resource by 22%.
This behavior surprised us, because fair queuing for a single re-
source is strategy-proof. Intuitively, ﬂow 1 “crowds out” ﬂow 2
at the second resource, which is the primary resource that ﬂow 2
7i, i.e., it gets 4
7 , 1
3 , 1
4(a) Allocation with resource proﬁles h4, 1i and h1, 2i.
(b) Allocation with resource proﬁles h4, 2i and h1, 2i.
Figure 6: Example of how ﬂows can manipulate per-resource
fairness. A shaded box shows the consumption of one packet
on one resource. In (b), ﬂow 1 increases per-packet resource
3 as
use from h4, 1i to h4, 2i to get a higher share of resource 1 ( 2
7).
opposed to 4
needs, by increasing its share, and this causes the buffer for ﬂow
2’s packets between the two resources to be full more of the time.
This leaves more time for ﬂow 1 at the ﬁrst resource.
We found that the amount by which ﬂows can raise their share
with per-resource fairness is as high as 2⇥, which provides sub-
stantial incentive for applications to manipulate the scheduler. We
discuss an example in Section 8.2.1. We have also simulated other
models of per-resource fairness, including ones with bigger buffers
and ones that let multiple packets be processed in parallel (e.g., as
on a multicore CPU), but found that they give the same shares over
time and can be manipulated in the same way.
Finally, from a practical viewpoint, per-resource fairness is hard
to implement in systems where there is no buffer between the re-
sources, e.g., a system where scheduling decisions are only taken
at an input queue, or a processing function that consumes CPU and
memory bandwidth in parallel (while executing CPU instructions).
Our proposal, DRFQ, is directly applicable in these settings.
5. DOMINANT RESOURCE FAIR QUEUING
The goal of this section is to develop queuing mechanisms that
multiplex packets to achieve DRF allocations.
Achieving DRF allocations in middleboxes turns out to be al-
gorithmically more challenging than in the datacenter context. In
datacenters, there are many more resources (machines, CPUs, etc)
than active jobs, and one can simply divide the resources across
the jobs according to their DRF allocations at a given time. In a
packet system, this is not possible because the number of packets
in service at a given time is usually much smaller than the num-
ber of backlogged ﬂows. For example, on a communication link,
at most one packet is transmitted at a time, and on a CPU, at most
one packet is (typically) processed per core. Thus, the only way
to achieve DRF allocation is to share the resources in time instead
of space, i.e., multiplex packets from different ﬂows to achieve the
DRF allocation over a longer time interval.
This challenge should come as no surprise to networking re-
searchers, as scheduling a single resource (link bandwidth) in time
was a research challenge receiving considerable attention for years.
Efforts to address this challenge started with idealized models (e.g.,
ﬂuid ﬂow [10, 24]), followed by a plethora of algorithms to accu-
rately and efﬁciently approximate these models [29, 18, 9].
We begin by describing a unifying model that accounts for re-
source consumption across different resources.
5.1 Packet Processing Time
The mechanisms we develop generalize the fair queueing con-
cepts of virtual start and ﬁnish times to multiple resources, such
that these times can be used to schedule packets.
To do this, we ﬁrst ﬁnd a metric that lets us compare virtual
times across resources. Deﬁning the unit of virtual time as the time
it takes to process one bit does not work for all resource types.
For example, the CPU does not consume the same amount of time
to process each bit; it usually takes longer to process a packet’s
header than its payload. Furthermore, packets with the same size,
but belonging to different ﬂows, may consume different amounts
of resources based on the processing functions they go through.
For example, a packet that gets handled by the IPSec encryption
module will consume more CPU time than a packet that does not.
To circumvent these challenges, we introduce the concept of
i .
packet processing time. Denote the k:th packet of ﬂow i as pk
i,j, is
The processing time of the packet pk
the time consumed by resource j to process packet pk
i , normalized
to the resource’s processing capacity. Note that processing time is
not always equal to packet service time.3 Consider a CPU with four
cores, and assume it takes a single core 10 µs to process a packet.
Since the CPU can process four such packets in parallel, the nor-
malized time consumed by the CPU to process the packet (i.e., the
processing time) is 2.5 µs. However, the packet’s service time is 10
µs. In general, the processing time is the inverse of the throughput.
In the above example, the CPU throughput is 400, 000 packets/sec.
We deﬁne a unit of virtual time as one µsec of processing time
for the packet of a ﬂow with weight one. Thus, by deﬁnition, the
processing time, and by extension, the virtual time, do not depend
on the resource type. Also, similarly to FQ, the unit of virtual time
does not depend on number of ﬂows backlogged. Here, we assume
that the time consumed by a resource to process a packet does not
depend on how many other packets, if any, it processes in parallel.
i at resource j, denoted sk
We return to processing time estimation in §5.7 and §7.1.
5.2 Dove-Tailing vs. Memoryless Scheduling
The multi-resource scenario introduces another challenge. Spec-
iﬁcally, there is a tradeoff between dove-tailing and memoryless
scheduling, which we explain next.
Different packets from the same ﬂow may have different pro-
cessing time requirements, e.g., a TCP SYN packet usually requires
more processing time than later packets. Consider a ﬂow that sends
a total of 10 packets, alternating in processing time requirements
h2, 1i and h1, 2i, respectively. It is desirable that the system treats
this ﬂow the same as a ﬂow that sends 5 packets, all with processing
time h3, 3i. We refer to this as the dove-tailing requirement.4
Our dove-tailing requirement is a natural extension of fair queu-
ing for one resource. Indeed, past research in network fair queuing
attempted to normalize the processing time of packets of different
length. For example, a ﬂow with 5 packets of length 1 KB should
be treated the same as a ﬂow with 10 packets of length 0.5 KB.
3Packet service time is the interval between (1) the time the packet
starts being processed and (2) the time at which its processing ends.
4Dove-tailing can occur in practice in two ways. First, if there is a
buffer between two resources (e.g., the CPU and a link), this will
allow packets with complementary resource demands to overlap in
time. Second, if two resources need to be consumed in parallel
(e.g., CPU cycles and memory bandwidth), there can still be dove-
tailing from processing multiple packets in parallel (e.g., multiple
cores can be working on packets with complementary needs).
r1 time r2 p1 p2 p3 p5 p6 p7 p8 p9 p4 p10 p11 1 3 2 4 5 6 7 9 8 10 11 12 13 14 p1 p2 p3 p4 p5 p6 p7 p8 p9 p10 p11 p12 p1 p2 p2 p1 p3 p3 p4 flow 1 flow 2 r1 r2 p1 p1 p2 p3 p2 p3 p4 p5 p6 p7 p8 p9 p10 p11 p5 p7 p9 p4 p6 p8 p10 time 1 3 2 4 5 6 7 9 8 10 11 12 13 14 p1 p1 p2 p2 p3 p4 p3 5At the same time, it is desirable for a queuing discipline to be
memoryless; that is, a ﬂow’s current share of resources should not
depend on its past share. Limiting memory is important to prevent
starving ﬂows when new ﬂows enter the system, as discussed in
Section 3.3.
Unfortunately, the memoryless and dove-tailing properties can-
not both be fully achieved at the same time. Dove-tailing requires
that a ﬂow’s relative overconsumption of a resource be compen-
sated by its past relative underconsumption of a resource, e.g., pack-
ets with proﬁle h1, 2i and h2, 1i. Thus, it requires the scheduler to
have memory of past processing time given to a ﬂow.
Memoryless and dove-tailing are at the extreme ends of a spec-
trum. We gradually develop DRFQ, starting with a simple algo-
rithm that is fully memoryless, but does not provide dove-tailing.
We thereafter extend that algorithm to provide full dove-tailing but
without being memoryless. Finally, we show a ﬁnal extension in
which the amount of dove-tailing and memory is conﬁgurable. The
latter algorithm is referred to as DRFQ, as the former two are spe-
cial cases. Before explaining the algorithms, we brieﬂy review
Start-time Fair Queuing (SFQ) [18], which our work builds on.
5.3 Review: Start-time Fair Queuing (SFQ)
SFQ builds on the notion of virtual time. Recall from Section 3.3
that a virtual time unit is the time it takes to send one bit of a unit-
weight ﬂow in the ﬂuid-ﬂow system that fair queuing approximates.
Thus, it takes l virtual time units to send a packet of length l with
weight 1, or, in general, l/w units to send a packet with weight
w. Note that the virtual time to send a packet is always the same
regardless of the number of ﬂows; thus, virtual time progresses
slower in real-time when more ﬂows are active.
i be the k-th packet of ﬂow i. Upon pk
Let pk
time based schedulers assign it a start and a ﬁnish time S(pk
F (pk
i ), respectively, such that
i ’s arrival, all virtual
i ) and
F (pk
i ) = S(pk
i ) +
L(pk
i )
wi
,
(1)
i ) is the length of packet pk
where L(pk
i in bits, and wi is the weight
of ﬂow i. Intuitively, functions S and F approximate the virtual
times when the packet would have been transmitted in the ﬂuid
ﬂow system.
In turn, the virtual start time of the packet is:
S(pk
i ) = max⇣A(pk
i ), F (pk 1
i
)⌘ ,
(2)
i ) is the virtual arrival time of pk
where A(pk
be the (real) arrival time of packet pk
the virtual time at real time ak
i , i.e., V (ak
i ).
i . In particular, let ak
i
i ) is simply
i . Then, the A(pk
Fair queueing algorithms usually differ in (1) how V (t) (virtual
time) is computed and (2) which packet gets scheduled next.
While there are many possibilities for both choices, SFQ pro-
ceeds by (1) assigning each packet a virtual time equal to the start
time of the packet currently in service (that is, V (t) is the start time
of the packet in service at real time t) and (2) always scheduling
the packet with the lowest virtual start time. We discuss why these
choices are attractive in middleboxes in Section 5.7.
5.4 Memoryless DRFQ
In many workloads, packets within the same ﬂow have similar
resource requirements. For such workloads, a memoryless DRFQ
scheduler closely approximates DRF allocations.
Assume a set of n ﬂows that share a set of m resources j, (1 
j  m), and assume ﬂow i is given weight wi, (1  i  n).
Throughout, we will use the notation introduced in Table 1.
Notation Explanation
pk
i
ak
i
sk
i,j
S(p)