0
0
0
0
0
0
e
f
a
S
e
r
o
F
0
0
0
0
0
0
0
0
0
0
0
0
0
0
Table 7: Sensor counts, as evaluated from different Android
sandbox systems. Very different from table 5, the sandboxes
exhibit very few sensors.
5.3 Hardware and software component eval-
uation
None of the measured sandbox systems made effort to emulate
hardware sensors. As shown in Table 7, only CopperDroid han-
dles accelerometer events, and no sandbox handles any other sen-
sor type. Likewise, the sandboxes report exact, unchanging values
for the battery level (0) and scale (100) as used in the emulator.
5.4 System design evaluation
We elected to only measure coarse data in regard to potentially
sensitive items such as address book contacts and call logs. With
the wide distribution of our test applications to public sandboxes
and Google Play, we wanted to reduce the risk of incidentally re-
ceiving any private information. For this reason, we only collected
the total counts of contacts, call logs, and applications installed.
Even so, these total counts are likely enough to discern if a de-
vice is realistically in use as the sandboxes report very low values.
Better heuristics could certainly be developed over time. Andrubis
reports 63 applications installed and Foresafe reports 46 apps, but
neither sandbox has any of the Google services apps such as Google
Talk, YouTube, or Google Service Framework installed. Andrubis
shows 5 contacts and 3 call logs, indicating that some thought has
been given to making Andrubis look like a device in use. The stan-
dard emulator has no contacts and no call logs, which is also what
Foresafe shows.
In addition to the duration shown in the various sandbox re-
ports, we attempted to measure the execution time. We did this
by creating an application that embedded the system timestamp
(System.currentTimeMillis()) in repeated UDP packets
sent out from the sandbox. By comparing the largest and smallest
value received, we can approximate the duration of execution. For
instance, Foresafe may report a “Processing Time” of 106 seconds,
yet for the same submission we observe timestamp values indicat-
ing an execution time of 68 seconds. Similarly, When Andrubis
reports an analysis duration of 306 seconds we observe 248 sec-
onds and when CopperDroid reports 731 seconds, we observe 399
seconds. The additional time listed by each sandbox may include
some operations other than exclusively executing the application.
6. DISCUSSION
Some of the emulation detections presented in this work are
relatively simple to mitigate.
For instance, unexpected val-
ues could be returned for many of the API methods listed in
Table 1.
Some security tools, such as DroidBox [4], have
started to take this step. However, DroidBox does not raise the
bar very high as the new values are set to a known constant.
For example, the TelephonyManager device identiﬁer is al-
ways 357242043237517, and the subscriber identiﬁer is always
310005123456789 [5]. When other security tools make use of soft-
ware packages such as DroidBox, they become subject to evasion
due to DroidBox detections. Such is the case for Andrubis. A sand-
box system designer must be thoughtful about how these detections
are mitigated. As demonstrated in section 4, reﬂection and runtime
exec can be used to observe many system values in different ways,
a good mitigation will handle all such cases.
Perhaps the easiest mitigations to counter are those that rely on
the high-level state of the device, such as the number of contacts
and the call log. Populating a sandbox with a copious address book
makes such a detection considerably more difﬁcult for an attacker.
Yet other detection mechanisms we have presented are far more
difﬁcult to counter. Techniques rooted in detecting virtualization
itself, such as those we presented via timing, present a difﬁcult hur-
dle for mobile sandbox system designers as they essentially require
to redesign the emulator to obtain timing measurements that closely
resemble those obtained on actual hardware. While it may be pos-
sible to reduce the wide discrepancy we have observed through
our measurements, one can easily imagine the next step of this
arms race would be to build up a hardware proﬁle based on var-
ious measurements (CPU, graphics, ...) over several benchmarks
rather than the simple Pi calculation we relied upon. We conjecture
it could be possible to pinpoint the exact hardware used with such
a technique—and of course, to detect any emulation.
7. CONCLUSION
As with many malware-related technologies, the detection of dy-
namic analysis systems is one side of an arms race. The primary
reason emulator detection is more applicable here than for PCs is
that practical use cases have developed for virtualizing general pur-
pose computers – a phenomenon that has yet to occur for mobile
devices. Virtualization is not broadly available on consumer mo-
bile platforms. For this reason, we believe that mobile-oriented
detection techniques will have more longevity than corresponding
techniques on the PC.
We have presented a number of emulator and dynamic analysis
detection methods for Android devices. Our detections are rooted
in observed differences in hardware, software and device usage.
From an implementation perspective, the detection techniques re-
quire little or no access beyond what a typical application would
normally be granted. Such detections can signiﬁcantly raise the bar
for designers of dynamic analysis systems as they must universally
mitigate all detections. Of those, hardware differences appear to be
the most vexing to address: a very simple evaluation of the frame-
per-second rate immediately led us to identify malware sandboxes,
without requiring advanced permissions. Likewise, accelerometer
values would yield deﬁnitive clues that the malware is running in a
sandboxed environment. Whether concealing such hardware prop-
erties can be done in practice remains an open problem, on which
we hope the present paper will foster research, lest malware sand-
boxes be easily detected and evaded.
456Acknowledgments
This research was partially funded by the National Science Foun-
dation under ITR award CCF-0424422 (TRUST).
8. REFERENCES
[1] AMAT: Android Malware Analysis Toolkit. http:
//sourceforge.net/projects/amatlinux/.
[2] Andrubis. http://anubis.iseclab.org/.
[3] CopperDroid. http://copperdroid.isg.rhul.ac.
uk/copperdroid/.
[4] DroidBox.
https://code.google.com/p/droidbox/.
[5] Droidbox device identiﬁer patch.
https://code.google.com/p/droidbox/
source/browse/trunk/droidbox23/
framework_base.patch?r=82.
[6] Foresafe. http://www.foresafe.com/scan.
[7] mobile-sandbox. http://mobilesandbox.org/.
[8] Monitoring the Battery Level and Charging State | Android
Developers. http://developer.android.com/
training/monitoring-device-
state/battery-monitoring.html.
[9] North American Numbering Plan Adminstration search.
www.nanpa.com/enas/area_code_query.do.
[10] SandDroid. http://sanddroid.xjtu.edu.cn/.
[11] Using the Android Emulator | Android Developers.
http://developer.android.com/tools/
devices/emulator.html.
[12] U. Bayer, P. Comparetti, C. Hlauschek, C. Kruegel, and
E. Kirda. Scalable, behavior-based malware clustering. In
NDSS, 2009.
[13] T. Blasing, L. Batyuk, A. Schmidt, S. Camtepe, and
S. Albayrak. An android application sandbox system for
suspicious software detection. In MALWARE’10, 2010.
[14] D. J. Chaboya, R. A. Raines, R. O. Baldwin, and B. E.
Mullins. Network intrusion detection: automated and manual
methods prone to attack and evasion. Security & Privacy,
IEEE, 4(6):36–43, 2006.
[15] X. Chen, J. Andersen, Z. M. Mao, M. Bailey, and J. Nazario.
Towards an understanding of anti-virtualization and
anti-debugging behavior in modern malware. In Dependable
Systems and Networks With FTCS and DCC, 2008. IEEE
International Conference on, pages 177–186, 2008.
[16] H. Dreger, A. Feldmann, V. Paxson, and R. Sommer.
Operational experiences with high-volume network intrusion
detection. In Proc. CCS, pages 2–11. ACM, 2004.
[17] M. F. and P. Schulz. Detecting android sandboxes, Aug 2012.
https://www.dexlabs.org/blog/btdetect.
[18] A. Felt, M. Finifter, E. Chin, S. Hanna, and D. Wagner. A
survey of mobile malware in the wild. In Proc. SPSM, pages
3–14. ACM, 2011.
[19] P. Ferrie. Attacks on more virtual machine emulators.
Symantec Technology Exchange, 2007.
[20] P. Fogla and W. Lee. Evading network anomaly detection
systems: formal reasoning and practical techniques. In Proc.
CCS, pages 59–68. ACM, 2006.
[21] M. Handley, V. Paxson, and C. Kreibich. Network intrusion
detection: Evasion, trafﬁc normalization, and end-to-end
protocol semantics. In Proc. USENIX Security, 2001.
[22] T. Holz and F. Raynal. Detecting honeypots and other
suspicious environments. In Information Assurance
Workshop, 2005. IAW’05. Proceedings from the Sixth Annual
IEEE SMC, pages 29–36. IEEE, 2005.
[23] P. G. Kelley, S. Consolvo, L. F. Cranor, J. Jung, N. Sadeh,
and D. Wetherall. A conundrum of permissions: Installing
applications on an android smartphone. In USEC’12, pages
68–79. Springer, 2012.
[24] B. Lau and V. Svajcer. Measuring virtual machine detection
in malware using dsd tracer. Journal in Computer Virology,
6(3):181–195, 2010.
[25] H. Lockheimer. Android and Security, Feb 2012.
http://googlemobile.blogspot.com/2012/
02/android-and-security.html.
[26] A. Moser, C. Kruegel, and E. Kirda. Exploring multiple
execution paths for malware analysis. In Security and
Privacy, 2007. SP’07. IEEE Symposium on, 2007.
[27] D. Mutz, G. Vigna, and R. Kemmerer. An experience
developing an ids stimulator for the black-box testing of
network intrusion detection systems. In Computer Security
Applications Conference, 2003. Proceedings. 19th Annual,
pages 374–383. IEEE, 2003.
[28] J. Oberheide and C. Miller. Dissecting the android bouncer.
SummerCon2012, New York, 2012.
[29] T. Ooura. Improvement of the pi calculation algorithm and
implementation of fast multiple precision computation.
Transactions-Japan Society for Industrial and Applied
Mathematics, 9(4):165–172, 1999.
[30] R. Paleari, L. Martignoni, G. F. Roglia, and D. Bruschi. A
ﬁstful of red-pills: How to automatically generate procedures
to detect cpu emulators. In Proc. WOOT, volume 41,
page 86. USENIX, 2009.
[31] N. J. Percoco and S. Schulte. Adventures in bouncerland.
Black Hat USA, 2012.
[32] T. H. Ptacek and T. N. Newsham. Insertion, evasion, and
denial of service: Eluding network intrusion detection.
Technical report, DTIC Document, 1998.
[33] T. Raffetseder, C. Krügel, and E. Kirda. Detecting system
emulators. In Information Security. Springer, 2007.
[34] J. Rutkowska. Red pill... or how to detect vmm using
(almost) one cpu instruction. Invisible Things, 2004.
[35] T. Strazzere. Dex education 201 anti-emulation, Sept 2013.
http://hitcon.org/2013/download/Tim\
%20Strazzere\%20-\%20DexEducation.pdf.
[36] T. Vidas and N. Christin. Sweetening android lemon
markets: measuring and combating malware in application
marketplaces. In Proc. 3rd CODASPY, pages 197–208.
ACM, 2013.
[37] T. Vidas, D. Votipka, and N. Christin. All your droid are
belong to us: A survey of current android attacks. In Proc.
WOOT. USENIX, 2011.
[38] T. Vidas, C. Zhang, and N. Christin. Toward a general
collection methodology for android devices. DFRWS’11,
2011.
[39] C. Willems, T. Holz, and F. Freiling. Toward automated
dynamic malware analysis using cwsandbox. Security &
Privacy, IEEE, 5(2):32–39, 2007.
[40] Y. Zhou and X. Jiang. Dissecting android malware:
Characterization and evolution. In Proc. IEEE Symp. on
Security and Privacy, 2012.
[41] Y. Zhou, Z. Wang, W. Zhou, and X. Jiang. Hey, you, get off
of my market: Detecting malicious apps in ofﬁcial and
alternative android markets. In Proc. NDSS, 2012.
457a c t u a l
r e f l e c t i o n t o o b t a i n a h a n d l e t o t h e h i d d e n a n d r o i d . o s . S y s t e m P r o p e r t i e s
C l a s s L o a d e r
C l a s s  S y s t e m P r o p e r t i e s = c l . l o a d C l a s s ( " a n d r o i d . o s . S y s t e m P r o p e r t i e s " ) ;
c l = t h e A c t i v i t y . g e t B a s e C o n t e x t ( ) . g e t C l a s s L o a d e r ( ) ;
@ S u p p r e s s W a r n i n g s ( " r a w t y p e s " )
C l a s s [ ] p a r a m T y p e s = { S t r i n g . c l a s s } ;
Method g e t = S y s t e m P r o p e r t i e s . g e t M e t h o d ( " g e t " , p a r a m T y p e s ) ;
O b j e c t [ ] p a r a m s = { s } ;
r e t = ( S t r i n g ) g e t . i n v o k e ( S y s t e m P r o p e r t i e s , p a r a m s ) ;
9. APPENDIX
S t r i n g r e t = " " ;
t r y {
Log . e ( " r e f l e c t . " +s , g e t P r o p V i a R e f l e c t ( s ) ) ;
l o g A p r o p ( " r o . s e c u r e " ) ;
l o g A p r o p ( " r o . p r o d u c t . name " ) ;
l o g A p r o p ( " r o . d e b u g g a b l e " ) ;
l o g A p r o p ( " s t a t u s . b a t t e r y . l e v e l _ r a w " ) ;
l o g A p r o p ( " r o . b u i l d . h o s t " ) ;
l o g A p r o p ( " r o . b u i l d . t a g s " ) ;
l o g A p r o p ( " n e t . g p r s . l o c a l −i p " ) ;
l o g A p r o p ( " n e t . e t h 0 . gw" ) ;
l o g A p r o p ( " n e t . d n s 1 " ) ;
l o g A p r o p ( " gsm . o p e r a t o r . n u m e r i c " ) ;
l o g A p r o p ( " r o . k e r n e l . qemu " ) ;
l o g A p r o p ( " r o . k e r n e l . qemu . g l e s " ) ;
l o g A p r o p ( " r o . k e r n e l . a n d r o i d . qemud " ) ;
1 p r i v a t e v o i d l o g R u n t i m e S y s t e m P r o p s R e f l e c t ( ) {
2
3
4
5
6
7
8
9
10
11
12
13
14
15 }
16 p r i v a t e v o i d l o g A p r o p ( S t r i n g s ) {
17
18 }
19
20 / / t h e
21 p r i v a t e S t r i n g g e t P r o p V i a R e f l e c t ( S t r i n g s ) {
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37 }
38
39 p r i v a t e v o i d l o g R u n t i m e S y s t e m P r o p s E x e c ( ) {
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59 }
}
c a t c h ( E x c e p t i o n e r r )
{
}
i n p u t . c l o s e ( ) ;
}
r e t u r n r e t ;
e r r . p r i n t S t a c k T r a c e ( ) ;
t r y
{
} c a t c h ( E x c e p t i o n e ) {
e . p r i n t S t a c k T r a c e ( ) ;
}
i n p u t = new B u f f e r e d R e a d e r ( new I n p u t S t r e a m R e a d e r ( p . g e t I n p u t S t r e a m ( ) ) ) ;
S t r i n g l i n e ;
j a v a . l a n g . P r o c e s s p = R untime . g e t R u n t i m e ( ) . e x e c ( " g e t p r o p " ) ;
B u f f e r e d R e a d e r
w h i l e
{
( ( l i n e = i n p u t . r e a d L i n e ( ) )
! = n u l l )
s p l i t = l i n e . i n d e x O f ( " ] :
/ / q u i c k l i n e p a r s i n g
i n t
S t r i n g k = l i n e . s u b s t r i n g ( 1 , s p l i t ) ;
S t r i n g v = l i n e . s u b s t r i n g ( s p l i t + 4 , l i n e . l e n g t h ( ) −1) ;
Log . e ( " r u n p r o p . " +k , v ) ;
[ " ) ;
Figure 11: This listing uses reﬂection (top) and a runtime exec (bottom) to obtain runtime SystemProperties information. In
either case the information is logged to the system log, for detection purposes this value would be evaluated in accordance with
the detection techniques presented in earlier sections. This code listing obtains information in completely different ways than those
detailed in the earlier sections of the paper without using the ofﬁcial API and without requiring additional permissions. This code
obtains values for the battery level, the Build conﬁguration, network IP settings, and cellular provider MCC and MNC.
458