True Positive Rate (TPR) or Recall.
As we see in the table, k-FP outperforms the other
techniques by a small margin followed by CUMUL and,
lastly, k-NN, the least accurate technique. These accuracies
are consistent with existing evaluations of these techniques
on onion service sites [6]. We also evaluated the techniques
on existing datasets [29] to make sure that we are able to
reproduce previous evaluations on regular sites and that we do
not introduce errors that stem from our methodology; we did
not ﬁnd any major discrepancies from previous results.
On the other hand, Table IV shows the accuracies the
techniques achieved when applied on the cell traces collected
from our middle relay. In this table we see that the classiﬁers
are ranked in the same order as in the client: k-FP being the
most accurate and k-NN the least accurate. With respect to
the accuracies obtained at the client for the same classiﬁer,
TABLE IV.
10-FOLD CROSS-VALIDATED ACCURACIES FOR THE THREE
STATE-OF-THE-ART ATTACKS ON OUR DATASET OF CELL TRACES
COLLECTED AT THE middle relay
Num sites
10
50
100
500
1,000
k-NN
(%)
91% ± 0.03
73% ± 0.01
68% ± 0.01
64% ± 0.00
59% ± 0.00
k-FP
(%)
100% ± 0.00
91% ± 0.01
76% ± 0.02
72% ± 0.01
56% ± 0.01*
CUMUL
(%)
99% ± 0.03
86% ± 0.03
76% ± 0.02
66% ± 0.01
63% ± 0.01
* Due to RAM constraints we were not able to evaluate k-FP using the
optimal parameters for 1,000 sites which reduced classiﬁer accuracy.
we see some interesting differences: while k-NN has a few
percent points decrease in accuracy with respect to the entry
link scenario, both k-FP and CUMUL perform a few percent
points better when they are applied at the middle. This is
plausible because each classiﬁer uses different features that
may be more or less robust to timing and order differences
between both positions. The accuracy improvement can be
explained by the fact that we used TCP traces at the client,
whereas the middle dataset includes cell traces, conveying a
higher amount of information and including less noise than
TCP traces [30].
Another interesting observation is that discarding all third-
party circuits for training and testing does not impact classiﬁer
accuracy. We attribute this result to the low prevalence of
third party embedded content in onion services (it has been
found on a large dataset of onion services to be less than 20%
overall [6]).
2) Open world scenario: The preceding analysis and re-
sults are applicable to an idealized closed-world scenario
where we try to identify known and trained-on onion websites.
We now consider a more realistic and challenging open-world
setting—one where unseen and unknown onion websites may
be introduced at testing time. We now present an enhancement
of our website ﬁngerprinting techniques for this scenario.
In other recent open-world evaluations, the classiﬁer is only
trained on a small fraction of the web pages in the world. In
this setting, the user may visit any page in the world, including
pages of which the classiﬁer is not aware. These works deﬁne
the open world evaluation as a binary problem: the positive
class is formed by a set of monitored pages that the classiﬁer
aims to identify and the negative class by the remaining
non-monitored pages [4], [15], [20], [21], [26], [29]. During
training, the classiﬁer is shown examples of both monitored
and non-monitored pages; however, the majority of the pages
in the non-monitored set are not present in the training set.
In this paper we have approached the open world dif-
ferently. There is no strong support to believe that the non-
monitored samples used for training necessarily represent the
whole world of non-monitored pages because the sample that is
taken to train the classiﬁer is small compared to the population,
i.e., all pages that could possibly be visited. This sample may
bias the classiﬁer toward a speciﬁc choice of non-monitored
pages selected for training or not actually help the classiﬁer
discriminate monitored from non-monitored sites. Instead, we
propose to model the open world as a one-class classiﬁcation
problem: the classiﬁer only takes instances for the monitored
class and draws a boundary between the monitored pages and
all other pages.
9
0.5
0.4
R
P
T
ν = 0.2
0
0
0.2
0.64
FPR
1
·10−2
Fig. 6. ROC curve to optimize the ν parameter. We can see that ν = 0.2
makes a reasonable trade-off between TPR and FPR. To deal with extreme
base rates, it is possible to minimize the FPR at the expense of the TPR.
In particular, we have taken the monitored set
to be
composed by one single web page—the best-case scenario for
the adversary in such an open world. We have collected 5,000
instances of a popular social network service (SNS) that is
deployed as an onion site and use 3,750 of the samples to
draw the decision boundary and 1,250 for testing. We have
used 200,000 instances of 2,500 different sites (80 instances
per site) for testing the one-class classiﬁer for onion service
pages that are not the SNS.
For the one-class classiﬁer we have used sklearn’s
implementation of one-class SVM with a radial basis function.
The one-class SVM is parameterized on ν which deﬁnes an
upper bound on the fraction of training errors; ν can be
used to adjust the trade-off between False Positives and True
Positives. We plotted the ROC curve to ﬁnd a value of ν that
maximizes the number of True Positives while keeping a low
False Positive Rate. In Figure 6 we can see that ν = 0.2
achieves such a compromise, providing a FPR lower than 1%
while the TPR is slightly higher than 40%.
We have chosen a subset of CUMUL’s features because
they are also used in an SVM for the closed world prob-
lem [26]. After analyzing different subsets, we found that a
combination of the ﬁrst and last interpolation points of the
cumulative sum can separate SNS instances from the rest. In
particular, we used the second interpolation point (CUMUL’s
5th feature), describing the ﬁrst region of packets in the
original
trace, and the 87th one (CUMUL’s 90th feature),
which described mid- and end-regions of the trace.
The results are presented in Figure 7 which shows a
projection of the classiﬁcation space on these two features. The
orange plus marks are the SNS’s training instances, the purple
empty circles are the SNS’s testing instances, and black ﬁlled
dots are “Others” instances. The decision boundary learned by
the classiﬁer is depicted by the black line.
As we observe in Figure 7, there are many testing samples
of SNS that fall outside the boundary. This is because we tuned
the classiﬁer to minimize the number of False Positives—
instances of non-SNS pages that are classiﬁed as SNS. This
way we achieve a False Positive rate below 1%, but this
has a cost of a large number of False Negatives: the True
Positive rate is 40%. The reason we have optimized for low
FPR instead of TPR becomes is because we are interested in
realistic deployments where the base rate of the positive class
becomes relevant, as we discuss next in Section VI-E.
E. Precision is in the detail
The base rate is the probability that a site is visited,
and can also be interpreted as site popularity. Previous work
has discussed the importance of the effect of the base rate
Train error
TPR
FPR
Precision
1
e
t
a
R
0.5
0
0.005
0.012
0.05
0.1
0.25
SNS’s base rate (log scale)
0.96
0.4
0.2
1 · 10−2
0.5
Fig. 8.
Performance metrics for one-class open world for the SNS’s base
rates ranging from 0.5% to 50%. The vertical dashed line shows the point in
which Precision is 50%.
1
0.5
0
0
12
50
75
Onion service index
Fig. 7. Projection over two CUMUL features of the one-class classiﬁcation
instances. The plus sign marks are instances used for training the classiﬁer, the
circle marks are SNS instances used to test the positive class and the cross
marks are instances that belong to non-SNS sites used to test the negative
class. The black line shows the boundary that was learned by the classiﬁer
that minimizes False Positives.
s
r
o
r
r
e
f
o
n
o
i
t
c
a
r
F
of the positive class (i.e., the SNS) on the Precision of the
classiﬁer [20], [21]. Precision is proportional to the number of
samples that our classiﬁer detected as SNS that are actually
SNS. In other words, Precision is an estimate of the probability
that the classiﬁer was correct when it guessed SNS.
Prior work has pointed out that if the base rate of the
positive class (i.e., the SNS) is orders of magnitude lower than
the negative class (i.e., Others), the False Positive Rate (FPR)
has to be negligible so that the classiﬁer can perform with
sufﬁcient Precision [20]. Since we cannot estimate the base rate
of the SNS’s onion site directly for ethical and privacy reasons,
we have evaluated the Precision of our one-class classiﬁer for
several hypothetical base rates.
In Figure 8, we show the Precision, the TPR, the FPR
and training error (i.e., ν). In the graph we see that all of the
metrics are ﬁxed for the whole range of considered base rates,
while precision decreases exponentially when the base rate of
the SNS tends to zero. The vertical dashed line indicates the
base rate (1%) where Precision is 50%; the point where the
classiﬁer is correct only half of the time. These results are
comparable to previous work that evaluated the precision of
the CUMUL classiﬁer and achieved similar results [26].
We further analyzed the sites that the classiﬁer misclas-
siﬁed most often. The distribution of errors over the sites is
shown in Figure 9. We observe that 80% of the errors are
concentrated over 12 of the sites and only 3% of the total
number of sites are responsible for 100% the misclassiﬁcation.
Based on this observation, we argue that it is possible that even
for 1% FPR (see Figure 8), the classiﬁer may have greater
precision if those 12 sites that are responsible for most of
the errors are less popular. Note that we assumed a uniform
distribution of the sites that belong to the Others sites. Further,
it may be possible to design dedicated classiﬁers that learn
to distinguish between the SNS and each of these 12 sites
individually in order to reduce the number of False Positives
that the classiﬁer incurs overall.
Fig. 9.
Sites that were confused with the SNS at least once during the
classiﬁcation (2,443 sites had zero errors). Note that the distribution is heavily
skewed and 80% of all the errors are concentrated in the 0.5% (12) of the
sites (see vertical dashed line) and 3% of the sites (75) include all the errors.
We manually checked the 12 sites that were misclassiﬁed
as the SNS some weeks after we crawled them. Five of the
sites were ofﬂine and one of them had been seized by the
German Federal Criminal Police. The remaining sites were
up and are of a diverse nature: one is a personal homepage,
two are movie streaming sites, another is a porn site, one is
a hacking page and, surprisingly, the last one is the download
page for the SecureDrop anonymous whistle-blowing software
(secrdrop5wyphb5x.onion) run by the Freedom of the
Press. We believe that they were confused with the SNS’s
onion service page due to similarities in page size.
VII. ONION SERVICE POPULARITY MEASUREMENT
We showed in Section V how a relay can predict that it
is serving as a middle (in the R-C-M1 position) and on a
rendezvous onion service circuit with high conﬁdence, and we
showed in Section VI how website ﬁngerprinting techniques
can be used to accurately predict which onion service webpage
is visited. In this section, we validate our previous results
and show the practicality of the techniques that we developed
through a privacy-preserving measurement of the popularity of
a social networking site (SNS) accessible as an onion service.
A. Measurement Goals and Methodology
Tor circuit and website ﬁngerprinting techniques have
thus far been discussed in the literature in the context of
client deanonymization attacks. The goal of the measurement
study in this section is to show how to use the classiﬁcation
techniques presented in the previous sections not for client
deanonymization, but to predict accesses to and safely measure
10
−2−1012−2−1012Feature5(scaled)Feature90(scaled)OtherstestSNStrainSNStestthe popularity of an onion service SNS. In this study, we
seek to: (i) develop a reusable framework for safe onion
service popularity prediction and measurement; (ii) validate
our classiﬁcation techniques from the previous sections by
running them in real time in a realistic public Tor network
environment on live Tor trafﬁc (something that has never been
done before to the best of our knowledge); and (iii) show how
our proof-of-concept measurement framework can be used to
discover the popularity of an onion service in the open-world.
Note that doing this measurement safely is a primary goal that
is further discussed below in Section VII-D.
Achieving these goals involves several components. First,
we run middle relays in the Tor network that provide resources
to Tor users. Second, our relays must predict which circuits in
which they are serving are onion service circuits (speciﬁcally,
rendezvous circuits since those are used to access web content).
Third, our relays must predict when they are in a middle
position of circuits in which they are serving (speciﬁcally,
the R-C-M1 position since our classiﬁers were trained for
that position). Finally, our relays must predict which of the
predicted rendezvous circuits in which they predict the R-C-
M1 position are used to access the SNS. We next explain the
tools that we built, modiﬁed, and used to realize these goals.
B. Measurement Tools
We enhance PrivCount [19], an existing privacy-preserving
Tor measurement tool,8 to use a new prediction library that we
developed to allow us to make predictions in real time on real
Tor relay trafﬁc.
1) PrivCount Overview: PrivCount is a distributed mea-
surement
tool, based on the secret-sharing variant of
PrivEx [11], that can be used to safely measure Tor. PrivCount
works by extracting events from a Tor process and then
counting the occurrence of those events across a set of relays.
PrivCount consists of a tally server, several share keepers,
and several data collectors (one for each relay in a set of
measurement relays). The tally server acts as a centralized,
but untrusted, entity that