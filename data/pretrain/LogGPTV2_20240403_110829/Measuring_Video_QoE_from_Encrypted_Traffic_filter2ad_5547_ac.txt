518pared to the other two classes. Moreover, the confusion
matrix in Table 4 indicates that very few sessions have
been misclassiﬁed as mildly or severly problematic.
These indicators show that healthy video sessions are
streamed in signiﬁcantly better network conditions as
opposed to the problematic ones. This is translated to
higher BDP and close to zero packet retransmissions for
the vast majority of the instances. Additionally, healthy
conditions allow higher quality streams with fewer or no
quality switches. The combination of these character-
istics allow the algorithm to easily distinguish healthy
videos from problematic ones.
The separation of problematic sessions can be more
challenging however, which can be veriﬁed from respec-
tive values in the confusion matrix. Here, in contrast
to the healthy cases, there is a much higher number
of misclassiﬁcations between the videos with mild stalls
and those with severe stalls. In these cases, the chunk
size often is not suﬃcient to indicate the amount of
stalling. The reason for this is that frequently the min-
imum video quality is already selected due to limited
bandwidth and therefore the minimum chunk size or its
standard deviation will not contribute signiﬁcant infor-
mation for detecting the amount of stalling that took
place during a video session.
percentiles. As a result, the total number of features we
end up with is equal to 210.
The chunk average size is calculated from the sizes of
all the individual chunks in a video. The size of a chunk
has a strong correlation with the respective quality of
the video segment. The chunk size delta represents the
diﬀerence in the size of consecutive chunks while the
chunk time delta corresponds to the inter-arrival time
of video chunks. These parameters are indicators of
representation switches which in turn aﬀect the average
representation of the session and will be discussed in
more detail in Section 4.3.
Figure 3 presents a video session with a representa-
tion switch from 144p to 480p. Each point in the plot
represents a video chunk, while the labels above the
points indicate the segments’ resolutions. The x axis
corresponds to the video session relative time and the
y axis to the size of the video segments. In this exam-
ple there is a representation switch from 144p to 480p
at t = 22 of the time line. This is translated to a sig-
niﬁcant increase for both chunk ∆t and chunk ∆size,
which indicates that they can be relevant indicatiors of
quality switches.
Class
no stalls
mild stalls
severe stalls
weighted avg.
TP Rate FP Rate Precision Recall
0.977
0.809
0.793
0.935
0.977
0.809
0.793
0.935
0.111
0.035
0.009
0.09
0.965
0.816
0.887
0.934
Table 3: Classiﬁer’s output for the stall detec-
tion model
original label
predicted label
no stalls
mild stalls
severe stalls
no stalls mild stalls
97.76%
14.7%
4.2%
2.06%
80.9%
16.5%
severe stalls
0.18%
4.4%
79.3%
Table 4: Stall detection confusion matrix
4.2 Average Representation Detection
Feature Construction
In order to detect the average representation of videos
with higher accuracy, in addition to the 10 features that
are already available in the dataset, we construct ﬁve
new ones, i.e. the chunk average size, the chunk size
delta, the chunk time delta, the average throughput
and the throughput cumulative sum. The chunk res-
olution is only used for the ground truth and labelling
of the instances and not for the construction of the pre-
dictive model. Hence, we have a total of 14 features
from which we extract the following statistics, mini-
mum, mean, maximum, std. deviation and 5th, 10th,
15th, 20th, 25th, 50th, 75th, 80th, 85th, 90th and 95th
Figure 3: ∆t and ∆size in a video session with a
representation switch
The average throughput is calculated from the indi-
vidual throughputs of all the chunks, while the cusum is
their cumulative sum. The later is used as an indicator
of variations in throughput.
Labelling
For the detection of the average representation of a
video session, it is necessary to categorize the videos
in three main categories based on their average resolu-
tion, low (LD), standard (SD) and high deﬁnition (HD).
Given that in our dataset all the observed resolutions
take only a few standard values, i.e. 144p, 240p, 360p,
480p, 720p and 1080p, we label all videos with resolu-
tions 144p and 240p as LD, 360p and 480p as SD and
all videos with higher resolution as HD.
In the dataset 57% of the videos have LD average
quality, 38% have SD quality and only 5% have HD.
This is an expected ﬁnding in our case where videos
020406080100120140160180relative time (s)0100200300400500600700800segment size (KB)240p∆t∆size240p∆t∆size240p∆t∆size240p∆t∆size240p∆t∆size360p∆t∆size360p∆t∆size360p∆t∆size360p∆t∆size360p∆t∆size360p∆t∆size360p∆t∆size360p∆t∆size360p∆t∆size360p∆t∆size360p∆t∆size360p∆t∆size360p∆t∆size360p∆t∆size360p∆t∆size360p∆t∆size360p∆t∆size360p∆t∆size360p∆t∆size519are streamed using limited mobile data plans and on
handheld devices that often come whith smaller screens
which leads users to opt for LD and SD video qualities.
However, we need to also account for cases where
there are representation changes during the playback.
For these videos, we calculate the average representa-
tion µ from the resolutions of all the segments. We
proceed to label the instances in the dataset following
the rule below for calculating the Representation Qual-
ity RQ.
 HD :
SD :
LD :
RQ =
µ > 480
480 ≥ µ ≥ 360
µ < 360
Feature Selection
The FS is again performed with the aid of CfsSubsetE-
val and Best First. After the selection there are 15
features remaining out of the initial 210. These fea-
tures are listed in Table 5, ranked by their respective
information gain.
We observe that statistics derived from the chunk size
are the ones with the highest rank and represent the
vast majority of the 15 features. This is a meaning-
ful and expected result since the chunk sizes are highly
correlated with the diﬀerent representation qualities.
Moreover, the list of features also contains the BDP
and the BIF which are proportional to the amount of
bytes that can be delivered by the network but also the
throughput cusum which is related to the throughput
variations throughout the video session.
info. gain
0.41
0.39
0.38
0.37
0.33
0.32
0.22
0.21
0.2
0.19
0.16
0.15
0.06
0.05
0.03
feature
chunk size 75%
chunk size 85%
chunk size 90%
chunk size 50%
chunk size max
chunk avg size mean
BIF avg max
cumsum throughput min
chunk ∆size max
chunk size std
chunk ∆size std
chunk ∆t 25%
BDP 90%
BIF maximum min
RTT minimum min
Table 5: Features used for the Average Repre-
sentation detection.
Training and Testing the Predictive Model
The model to predict the average representation quality
is again built using ML and Random Forest. The train-
ing is done with balanced classes and then the trained
model is tested on the entire set. The obtained over-
all accuracy in this case is 84.5%. The accuracy for
each class is provided in Table 6 and the corresponding
confusion matrix in Table 7.
Class
TP Rate FP Rate Precision Recall
LD
SD
HD
weighted avg.
0.9
0.768
0.756
0.841
0.206
0.106
0.003
0.156
0.845
0.82
0.945
0.841
0.9
0.768
0.756
0.841
Table 6: Classiﬁer’s output for the average rep-
resentation model
original label
predicted label
LD
SD
HD
SD
HD
LD
90%
9.9% 0.1%
22.7% 76.8% 0.5%
6.8% 18.2% 75%
Table 7: Average representation confusion ma-
trix
The accuracies in the later table reveal that our model
is able to predict the average quality of LD videos with
very high accuracy but with slightly reduced accuracy in
the case of SD and HD videos. Nevertheless, the overall
but also the individual accuracies remain in high levels,
which verify the model’s good performance.
When further investigating the accuracy loss how-
ever, we identify that its caused by the increased num-
ber of misclassiﬁcations that occur in the SD and HD
classes. More speciﬁcally, a considerable amount of SD
video sessions is falsely detected as LD, while 18% of
HD videos are identiﬁed as SD.
This behavior is attributed to the quality downscales
that happen during a video session. As a result one
part of the video is streamed in higher quality and the
part after the downscale is streamed with lower quality.
The diﬀerences in chunk sizes between the two qualities
of a session lead to the incorrect classiﬁcation of the
video. Of course the eﬀects of this phenomenon cannot
be observed for LD videos since there is no lower qual-
ity to downgrade to and chunk sizes remain consistent
throughout the session.
4.3 Representation Quality Switch Detec-
tion
Adaptive streaming can adjust the representation of
the video during playback in order to compensate for
changes in the network conditions and reduce the likeli-
hood of playback buﬀer outages that lead to stalls. The
duration and frequency of the representation changes,
also known as Presentation Quality Switch Rate (PQSR),
as well as the amplitude of the switch can have a nega-
tive impact on the perceived QoE.
Filtering
During the start-up phase, many content providers em-
ploy a fast start mechanism that allows them to ﬁll the
playout buﬀer and start the playback as fast as possible,
eﬀectively reducing the start-up delay. This short ini-
tial part of a video session may have very diﬀerent char-
520acteristics in terms of segment sizes, inter-segment ar-
rival times and throughput when compared to the much
longer steady phase.
To reduce the noise introduced by the start-up phase
in the detection of resolution variations, we remove the
ﬁrst ten seconds of all video sessions in our dataset.
Given that this initial section represents a very small
fraction of the entire video session (the average session
duration is approximately 180 seconds), we can safely
remove it to reduce the noise introduced by the start-up
phase while maintaining more than 95% of the session.
Labelling
In order to build a model for quality switching detec-
tion, it is necessary to ﬁrst quantify the switches in
terms of frequency and amplitude. To this end, we de-
ﬁne two metrics, the time spent in each representation
tr, the frequency of representation switches F and the
switch amplitude A.
The switching frequency F is simply calculated as
the total number of switches that were observed in a
video. The lower the value this metric has, the better
the quality of the corresponding video is.
Finally, equation 2 which is based on the work of
Yin et al.[16], expresses the switch amplitude A as the
normalized sum of all the amplitudes of representation
switches between consecutive segments rk and rk+1.
Again, A is analogous to the degradation of QoE since
large representation changes which lead to poor QoE
will return higher values of A.
K−1(cid:88)
k=1
A =
1
K − 1
|rk+1 − rk|
(2)
The two metrics are then combined to a single indi-
cator of the representation variation Var using linear
combination. Next, each instance in the dataset is clas-
siﬁed in one of three main categories, no variation, mild
variation and high variation, based on the value of Var.
Change Detection
During the study of the sessions with many represen-
tation changes, we observe that whenever the adaptive
algorithm enforces a change in the representation of the
video, a new start-up phase is initiated for the new rep-
resentation. During this phase, the size and inter-arrival