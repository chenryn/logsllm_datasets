with, and slightly larger variance in size than correctly classified
ones. To rank features by taking into account both intra- and inter-
class variance, we use the relative difference between the inter-
and intra-class variance, where we define relative difference as:
d(x, y) = (x − y)/((x + y)/2). This formula normalizes the differ-
ences by their mean to values between 0 and 2, where features
with a relative difference close to 0 are similar and features with a
relative difference close to 2 are far apart. This allows features of
different scales to be compared. We consider features that are close
to 2 better predictors, as they have a relatively higher inter-class
variance than intra-class variance.
Many of the features that appear as most predictive for the con-
sidered classifiers are directly related to the size of a site (e.g., the
number of packets). Further, the misclassifications described in Sec-
tion 4 show that the smaller sites are more likely to be misclassified.
In addition to running feature analysis on the entire dataset, we
also look only at the small sites to determine which other features
have predictive value.
We start with an analysis of the network-level features used
by the three fingerprinting attacks detailed in Section 2 and ana-
lyzed in Section 4. Most traditional applications of feature analysis
aim to reduce the dimensionality of the data to more efficiently
classify instances. Instead, the goal of our feature analysis is to
determine which features can be modified to trick a classifier into
misclassifying an instance. Unlike many adversarial machine learn-
ing problems with the same goal, this analysis lacks knowledge of
the specific classifier (or even the classification algorithm) used for
fingerprinting, as there are many different classifiers in the litera-
ture to consider, and the site should ideally be hard to classify for all
of them. In addition to the wide variety of classification techniques
available in the current literature, novel classification techniques
could be easily developed by an adversary.
Therefore, the network-level feature analysis we present here is
classifier-independent. That is, we use only information about the
feature values themselves and do not use classification methods
to determine the importance of the features. Figure 7 shows the
relationship between how likely a site is to be fingerprinted vs its
size. All of the larger sites have high fingerprintability scores, while
the scores of smaller sites are much more varied.
Figure 7: Larger sites are easily fingerprinted while results
are mixed for smaller sites. Note also the vertical clusters
of sites with low fingerprintability that are similar in size.
Incoming packet size (in bytes) is plotted in log scale.
In a website fingerprinting attack, only features based on the
traffic traces are available to the adversary. Each attack uses a
distinct set of features derived from these traces and as a result the
exact feature analysis varies.
This analysis is classifier independent, meaning no classification
techniques were performed on the dataset prior to this analysis
and the results do not rely on any specific classification algorithm
or task. We cannot, however, perform any feature analysis that is
completely independent from the website fingerprinting methods,
as the types of features we analyze rely on the features chosen by
each method. For each attack, however, we can determine which
features are most predictive.
5.2 Network-Level Feature Results
Here we analyze which network-level features are the best predic-
tors in state-of-the-art website fingerprinting attacks.
5.2.1 CUMUL. The first group of features we consider come
from the CUMUL attack. There are two types of features used in
CUMUL: direct size features (Table 3) and interpolated features. The
interpolated features are formed by the number of bytes and packets
in each direction and 100 interpolation points of the cumulative sum
of packet lengths (with direction). We calculate the inter and intra-
class variance for each of these features. The direct size features
are the most important to classification (Table 3). We found that
the interpolated features are more predictive at the end of the trace
than the beginning, with the minimum relative difference (0.37)
being from the very first interpolated feature and then increasing
to the greatest relative difference (1.51) being the last interpolated
feature from the very end of the trace.
Feature Name
Total Size of all Outgoing Packets
Total Size of Incoming Packets
Number of Incoming Packets
Number of Outgoing Packets
Relative Diff
1.605
1.520
1.525
1.500
Table 3: Network-Level Feature Variance Analysis for CU-
MUL Method. These features had a higher relative differ-
ence than most of the interpolated features and alone are
great predictors.
5.2.2
k-fingerprinting. The next group of features we look at
come from the k-fingerprinting attack. The features used in the
k-fingerprinting attack are more varied as well as more straightfor-
ward than those in CUMUL. They include not only features that
give information about the size and number of packets, but also the
timing of the packets. The features with the highest inter-class to
intra-class variance ratio are shown in Table 4.
The feature analysis we present here is similar to the original
analysis presented with the method by the authors, but without
the use of any classification technique. Further, we also look at
which features are more predictive for small sites, as we see that
misclassifications are much more common for smaller sites.
Table 4 shows that features correlated to the total size of a site
(e.g. # of outgoing packets) have the highest relative difference and
thus are among the top features. This result is consistent with the
analysis done by Hayes and Danezis[11] on the same set of features.
When only smaller sites are analyzed however, standard devia-
tion features become important. In Section 4, we show that large
sites are easily identified, and the fact that size features are very
predictive is not at all unexpected. However, that standard devia-
tion features are top features for the smaller sites implies that the
dynamism of the site makes a difference, as small dynamic sites are
generally the least fingerprintable.
5.2.3
kNN. The last set of features are those of the kNN attack.
Like with the other classifiers, we find that the most important
features are those that relate to the size of the traffic flow. In this
case, we find that almost all of the top predictive features (with the
highest relative difference) are related to “packet ordering” – which
in practice acts as proxy for the size of the flow.
The packet ordering feature is computed as follows: for each
outgoing packet oi, feature fi is the total count of all packets sent or
received before it. Essentially, these features measure the ordering
of incoming and outgoing packets.Note that not all sites, however,
have the same number of outgoing packets. Therefore if the end of
Feature name
All Sites
Percent incoming vs outgoing
Average concentration of packets
# of outgoing packets
Sum of concentration of packets
Average order in
Smallest 10% of Sites
Percent incoming vs outgoing
Average concentration of packets
Standard deviation of order in
# of packets
# of packets per second
Relative Diff
1.895
1.775
1.740
1.740
1.720
1.951
1.944
1.934
1.927
1.927
Table 4: Network-level feature analysis for kFP method.
the number of outgoing packets is less than some n (we use n = 500
to be consistent with the original implementation), the rest of the
features are filled in with zero or null values. Similarly, some sites
may have over n outgoing packets. If this is the case, the packets
over the nth packet are ignored. Similar to the features used in
CUMUL, we observed that the later features in this sequence are
more important, this is because for most sites (size < n) they are
zero and thus these features are a proxy for the total size of the site.
The only other feature-type with high relative difference be-
tween inter and intra-class variance is the number of packets (1.96),
a direct measure of the size of the site.
6 SITE-LEVEL FEATURE ANALYSIS
In website fingerprinting attacks, the adversary records the network
traffic between a user and Tor, and analyzes its features to identify
the site that was visited. Network-level features and their relative
contribution to fingerprintability are, however, not informative for
onion service designers who may want to craft their site to be
robust against website fingerprinting attacks. To gain insight into
which design choices make sites vulnerable to attacks, and how
websites can be designed with increased security, we need to look
at the features at a site-level.
In this section we investigate which site-level features corre-
late with more and less fingerprintable sites. Site-level features are
those that can be extracted from a web page itself, not from the
traffic trace. Driven by adversarial learning, we investigate the task
of causing misclassifications for any set of network-level features
and any classification method. This information can help sites de-
sign their web pages for low fingerprintability, and also assist in
developing more effective server-side defenses.
6.1 Methodology
Site-level features are extracted and stored by our data collection
framework as explained in Section 3. The list of all site-level features
considered can be found in Table 6 (in the Appendix).
We build a random forest regressor that classifies easy- and hard-
to-fingerprint sites, using the fingerprintability scores (the F1
scores from the ensemble classifier described in Section 4) as labels,
and considering site-level features. We then use the fingerprint-
ability regressor as a means to determine which site-level features
better predict fingerprintability.
In this section we aim to understand which site-level features
are more prevalent in the most and least fingerprintable sites. For
the sake of this feature analysis, we remove the middle tier of sites,
defined as those with a fingerprintability score in (0.33, 0.66). 44
sites in our dataset were assigned a mid-ranged F1-score, leaving
438 sites for this analysis.
The next challenge is that the high and low-fingerprintability
classes are unbalanced, because of the disproportionately higher
number of easily identifiable sites compared to the amount of sites
that are hard to identify. Recall that a full 47% of sites in our dataset
have a fingerprintability score greater than 95%. A regressor trained
with such unbalanced priors will be biased to always output a predic-
tion for of “very fingerprintable,” or values close to 1, and therefore
any analysis on the results would be meaningless. To perform the
feature analysis, we remove randomly selected instances from the
set of more fingerprintable sites, so that it is balanced in size with
that of low fingerprintability.
We train a random forest regressor using the features from Ta-
ble 6. We use the feature weights from the regression to determine
which of these site-level features are most predictive of sites that are
easily fingerprinted. We use the information gain from the random
forest regression to rank the importance of the site-level features
in making websites more or less fingerprintable.
While in its current state this regression is only useful for fea-
ture analysis, this could be extended into a tool that allows sites to
compute their fingerprintability score, and be able to determine if
further action is needed to protect their users from website finger-
printing attacks.
see that standard deviation features are also important, implying
that sites that are more dynamic are harder to fingerprint.
Additionally, Table 5 shows how different the easy- and hard-to-
fingerprint sets of sites are in terms of total HTTP download size, a
straightforward metric for the size of a site. The median site size for
the 50 most fingerprintable sites is almost 150 times larger than the
median size of the harder to classify sites. The standard deviation of
the total site size for the most and least fingerprintable sites, relative
to their size, is similarly distinct, showing the most fingerprintable
sites are less dynamic than the 50 least fingerprintable sites. That
is, they are less likely to change between each visit.
Total HTTP Download Size
Median Std Dev
(normalized by total size)
Median Size
50 Most
0.00062
50 Least
0.04451
Table 5: Differences in the most and least fingerprintable
sites. The 50 most fingerprintable sites are larger and less
dynamic than the 50 least fingerprintable sites.
438110
2985
While the smallest sites are less fingerprintable, some are still
easily identified. Figure 9 shows the distribution of sizes consid-
ering only the smallest sites, distinguished by whether they have
a high or low fingerprintability score. We can see that the least
fingerprintable sites are clustered in fewer size values, while the
most fingerprintable are more spread, meaning that there are fewer
sites of the same size that they can be confused with.
6.2 Results
Figure 8: Most important features by information gain. Fea-
tures related to the size of a site are important.
Figure 8 shows the results of the analysis. We see that features
associated with the size of the site give the highest information gain
for determining fingerprintability when all the sites are considered.
Among the smallest sites, which are generally less identifiable, we
Figure 9: Distribution of sizes for the most and least fin-
gerprintable sites, considering only the sites smaller than
25,000 bytes.
7 IMPLICATIONS FOR ONION SERVICE
DESIGN
Overall, our analysis showed that most onion services are highly
vulnerable to website fingerprinting attacks. Additionally, we found
that larger sites are more susceptible to website fingerprinting
attacks. Larger sites were more likely to be perfectly classified by
all attacks while many smaller sites were able to evade the same
attacks by inducing misclassifications.
We also observed that the small sites that are harder to iden-
tify also have a high standard deviations for many site-level and
network-level features, implying that dynamism plays a role in why
these sites are less identifiable. While our results show that small
size is necessary, it is not sufficient. As a result, our recommendation
for onion service designers is “make it small and dynamic.”
Most website fingerprinting defenses rely on some form of padding,
that is, adding spurious traffic and therefore increasing the down-
load size. Our analysis, however, shows that this type of defense
may not be robust when features such as download size become
sparse. Often, these defenses are tested against a single attack with
a single feature set and a specific classification algorithm. We see,
though, that classification errors do not always coincide for dif-
ferent attacks, and argue that any website fingerprinting defense
needs to be tested against a range of state-of-the-art attacks, prefer-
ably relying on different algorithms and feature sets, in order to
provide more general guarantees of its effectiveness.
As a case study, we consider the results that our ensemble classi-
fier achieved in identifying SecureDrop sites. These sites are onion
services that are running the SecureDrop software, a whistleblower
submission system that allows journalists and media publishers to
protect the identities of their sources. Given the sensitive nature
of the service that they provide and the nation-state adversaries
that they may realistically face, these SecureDrop sites have strong
anonymity requirements.
Our dataset contained a SecureDrop site owned by ‘Project On
Gov’t Oversight’ (POGO)6. The SecureDrop site had an F1-Score
of 99%, meaning that it is much more vulnerable to website finger-
printing attacks than the average onion service site.
There were other SecureDrop sites present in our initial dataset,
associated with The New Yorker, The Intercept and ExposeFacts.
These sites were flagged as duplicates of the POGO SecureDrop
site and thus removed during the data processing stage. Since they
were identified as duplicates, all these SecureDrop sites have very
similar characteristics and can thus be expected to be identifiable
at a similarly high rates as the POGO site. In particular, we noted
that these pages embed images and use scripts and CSS styles that
make them large and therefore distinguishable.
It can be argued that the existence of various similar SecureDrop
sites creates an anonymity set and makes some sites cover up for
each other. On the other hand however, it may be enough for the
adversary to ascertain that the user is visiting a SecureDrop site
for the anonymity of the source to be compromised.
We did a small, manual analysis of some of the most and least
fingerprintable sites (by F1 score) to see if there were any strong
correlations with content. We found that pages at the bottom end of
6https://securedrop.pogo.org
the spectrum were smaller and simpler (a hidden wiki, a listing of a
directory, nginx config page, etc.) whereas the most fingerprintable
pages were larger and more complex (a bitcoin faucet site, a forum,
the weasyl art gallery site, propublica, a Russian escort service site).
Pages in the middle of the spectrum varied, but were often login