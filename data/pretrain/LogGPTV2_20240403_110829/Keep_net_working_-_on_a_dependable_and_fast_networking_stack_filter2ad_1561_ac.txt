server detects a crash of its neighbor, it tells the database to
abort all requests to this server. While the database removes
the requests, it executes the associated abort actions. Abort
actions are highly application speciﬁc. For instance, servers in
a storage stack are likely to clean up and propagate the abort
further until an error is returned to the user-space processes.
On the other hand, servers in a networking stack may decide
to retransmit a packet or drop it, which we discuss in the
following Section V.
The channels allow a component to be transparently
replaced by a newer version on the ﬂy as long as the interface
to the rest of the system stays unchanged. Since a new
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:17:40 UTC from IEEE Xplore.  Restrictions apply. 
Component
Drivers
IP
UDP
Packet ﬁlter
TCP
Ability to restart
No state, simple restart
Small static state, easy to restore
Small state per socket, low frequency of change, easy
to store safely
Static conﬁguration, easy to restore, information about
existing connections is recoverable
Large, frequently changing state for each connection,
difﬁcult to recover. Easy to recover listening sockets
COMPLEXITY OF RECOVERING A COMPONENT
Table I
of this state and the frequency at which it changes determines
how easily we can recover from a failure (Table I).
After drivers, the simplest component to restart is IP. It has
very limited (static) state, basically the routing information,
which we can save in any kind of permanent storage and
restore after a crash. ARP and ICMP are stateless. To recover
UDP, however, we need to know the conﬁguration of the
sockets, a 4-tuple of source and destination address and ports.
Fortunately, this state does not change very often. The packet
ﬁlter has two kinds of state. The more static portion is its
conﬁguration by the user which is as simple to recover as
IP state. However, there is also dynamic state. For instance,
when a ﬁrewall blocks incoming trafﬁc it must not stop
data on established outgoing TCP connections after a restart.
In NewtOS, the ﬁlter can recover this dynamic state, for
instance, by querying the TCP and UDP servers.
The biggest challenge is recovering TCP. Besides the 4-
tuple part of the state, it has a frequently changing part
for congestion avoidance and reliable transport. In fact,
all unacknowledged data are part of this state. Although
preserving such state for recovery is difﬁcult, research in this
area shows how to design such system components [9].
In our design, we isolate the parts that are difﬁcult to
recover (TCP) from from those we know how to restart, thus
improving overall dependability. The ability to recover most
of the network stack (even if we cannot recover all) is much
better than being able to recover none of it and vastly better
than a bug bringing the entire system to a grinding halt.
Note that not being able to recover the state of TCP means
only that existing connections break. Users can immediately
establish new ones.
NewtOS survives attacks similar to the famous ping of
death [14] without crashing the entire system. Also, it does
not become disconnected when the packet ﬁlter crashes,
neither does it become vulnerable to attacks after it restarts
since its conﬁguration is preserved.
In addition, it is possible to update each component
independently without stopping the whole system as long as
the interface to the rest of the system remains unchanged. In
fact, shutting down a component gracefully makes restarting
much simpler as it can save its state and announce the restart
to other parts of the stack in advance. We are conﬁdent that
all servers of our network stack can converge to a consistent
Figure 2. Decomposition and isolation in multiserver systems
incarnation of a server in our system inherits the old version’s
address space, the channels remain established.
V. DEPENDABLE NETWORKING STACK
The network stack is a particularly critical part of current
OSs, where often extreme performance is as important as high
reliability since downtime may have a high cost. In addition,
the network stack is very complex and frequently contains
critical bugs, as witnessed recently by the vulnerability in
Microsoft systems [4]. Thus, we selected the networking
stack as the most interesting subsystem in the OS to evaluate
our design.
In contrast to monolithic OSs that are very good in
performance but do not address reliability at all, we opted for
an extreme design. In case of a fatal error in the stack, the
rest of the system keeps working. As we shall see, the system
can often ﬁx the problem automatically and seamlessly. In
situations when it cannot, the user can take an action like
saving data to disk and reboot which is still more than a
user can do when the whole system halts.
Our stack goes even well beyond what is currently found
in other multi-server systems. For instance, Herder et al. [22]
showed in the original Minix 3 how to repair faulty network
userspace drivers at runtime by restarting them. However, net-
work drivers are near-stateless components and the network
protocols know how to recover from packet loss. Any fault
in IP, say, would crash the entire stack. However, because
the network stack itself is stateful, it was possible to restart
it, but not to recover the state. We decompose the network
stack in even more smaller (and simpler) separate processes,
which increases isolation, but also the amount of IPC.
Figure 2 shows how we split up the stack into multiple
components. The dashed box represents what is usually a
single server in a multiserver system and the boxes inside
are the servers in NewtOS. We draw a line between the IP
layer and the transport protocols. Our IP also contains ICMP
and ARP. For security reasons, the networking stack usually
contains a packet ﬁlter which we can also isolate into a
standalone process. Again, such an extreme decomposition
is practical only if we do not signiﬁcantly compromise
performance.
Each of the components has at least some state and the size
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:17:40 UTC from IEEE Xplore.  Restrictions apply. 
ProcessProcessProcessPMPFMMNetDrvMicrokernelNetwork StackSATADrvSYSCALLVFSExt2TCPIP / ICMPNetDrvUDPstate for an update since they satisfy the conditions presented
by Giuffrida et al. in [18].
In November 2011, Microsoft announced a critical vulner-
ability [4] in the UDP part of Windows networking stack.
The vulnerability allows an intruder to hijack the whole
system. In this respect, NewtOS is much more resilient. First,
hijacking an unprivileged component does not automatically
open doors to the rest of the system. Second, we are able
to replace the buggy UDP component without rebooting.
Given the fact that most Internet trafﬁc is carried by the
TCP protocol, this trafﬁc remains completely unaffected by
the replacement, which is especially important for server
installations. Incidentally, restartability of core components
proved very valuable during development of the system since
each reboot takes some time and it resets the development
environment.
A. The Internals
Nowadays, multigigabit networks present a challenge for
many software systems, therefore we want to demonstrate that
a multiserver system handles multigigabit rates. We replaced
the original Minix 3 stack by lwIP [13] because lwIP is easier
to split and modify. Although lwIP is primarily designed for
size rather than high performance (it targets mostly embedded
devices), it is a clean and portable implementation of the
TCP/IP protocol suite. We use the NetBSD packet ﬁlter (PF)
and we heavily modiﬁed the driver for the family of Intel
PRO/1000 gigabit network adapters.
To separate the IP part, we only had to change the
place where lwIP does the routing for outgoing packets.
Although virtually all gigabit network adapters provide
checksum ofﬂoading and TCP segmentation ofﬂoading (TSO
- NIC breaks one oversized TCP segment into small ones),
lwIP does not support it out of the box. We changed the
lwIP internals to support these optimizations. Although this
improves the performance of lwIP dramatically, the TCP
code requires a complete overhaul if we want it to be as
efﬁcient as, say, the Linux network stack. Even so, we will
show that the performance of our design is competitive.
We did not port the network stack from Linux or any BSD
ﬂavor because these depend on the monolithic environment
(memory management, etc.) and changing the stack to our
needs would likely severely compromise its performance.
Figure 3 shows the placement of PF within the stack.
Placing PF in a T junction makes it easier to support both
post and pre-routing rules, and to restart PF on a crash (see
Section V-D). In addition, in this design IP remains the only
component that communicates with drivers. Although this
setup puts more performance pressure on the IP server since
it must hand off each packet to another component three
times, IP is not the performance bottleneck of the stack, even
with the extra work.
Figure 3. Asynchrony in the network stack
B. Combining Kernel IPC and Channels IPC
In our current implementation, the servers which interface
with user space and drivers need to combine channel IPC with
kernel IPC, as the kernel converts interrupts to messages to
the drivers. Similarly, the system calls from user applications
are also kernel IPC messages. Therefore we combine the
kernel call, which monitors memory writes, with a non-
blocking receive, that is, before we really block or after we
wake up, we check if there is a pending message. Whenever
there is one, we deliver it when we return from the kernel
call. Of course, we do not block at all if we ﬁnd a message.
Because kernel IPC from other cores is accompanied by an
interprocessor interrupt (IPI) when the destination core is
idle, the IPI breaks the memory write monitoring even if no
write to the monitored location occurred. Note that unlike
in a monolithic design where system calls are kernel calls,
system calls in a multiserver system are implemented as
messages to servers.
To detach the synchronous POSIX system calls from the
asynchronous internals of NewtOS, the applications’ requests
are dispatched by a SYSCALL server. It is the only server
which frequently uses the kernel IPC. Phrased differently, it
pays the trapping toll for the rest of the system. Nonetheless,
the work done by the SYSCALL server is minimal, it merely
peeks into the messages and passes them to the servers
through the channels. The server has no internal state, and
restarting it in the case of a failure is trivial. We return errors
to the system calls and ignore old replies from the servers.
Figure 3 shows connections of the SYSCALL (SC) server to
the rest of the network stack. We use these connections only
for control messages. The actual data bypass the SYSCALL
as opening a socket also exports shared memory buffer to
the applications where the servers expect the data.
Our C library implements the synchronous calls as mes-
sages to the SYSCALL server, which blocks the user process
on receive until it gets a reply. Although this is a convenient
way to implement POSIX system calls, some applications
may prefer other arrangements. Extending the channels from
inside the system to the user space allows applications to
bypass the overhead of the synchronous calls by opening
channels directly to the servers.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:17:40 UTC from IEEE Xplore.  Restrictions apply. 
SCTCPUDPIPPFDRVDRVDRVProcProcProcSynchronousAsynchronousSynchronous IPCAsynchronous ChannelsC. Zero Copy
By using channels, shared pools and rich pointers, we can
pass data through the system without copying it from com-
ponent to components as is traditionally done in multiservers.
Any server that knows the pool described in the pointer, can
translate the rich pointer into a local one to access the data.
Because modern network interface cards (NICs) assemble
packets from chunks scattered in memory, monolithic systems
pass packets from one networking layer to another as a chain
of these chunks. Every protocol prepends its own header. The
payload is similarly scattered, especially when the packets are
large (for example, when the network allows jumbo frames
or TSO is enabled). In NewtOS, we pass such a chain as an
array allocated in a shared pool ﬁlled with rich pointers.
We emphasize that zero copy makes crash recovery much
more complicated. Unlike a monolithic system where we can
free the data as soon as we stop using them, in our case, the
component that allocated the data in a pool must free them.
This means that we must report back when it is safe to free
the data—almost doubling the amount of communication.
Worse, after a server recovers from a crash, the other servers
must ﬁnd out what data are still in use and which should be
freed. To the best of our knowledge, ours is the ﬁrst system
capable of restarting components in a multiserver system
stack with zero copy communication throughout.
To further improve reliability, we make the data in the
pools immutable (like in FBufs [12]). Phrased differently, we
export all pools read-only. Therefore each component which
needs to change data must create a new copy. For instance,
this is done by IP when it places a partial checksum in the
TCP and UDP headers of outgoing packets. As the headers
are tiny, we combine them with IP headers in one chunk.
D. Crash Recovery
Before we can recover from a crash, we must detect it.
In NewtOS, as in Minix 3, all system servers are children
of the same reincarnation server which receives a signal
when a server crashes, or resets it when it stops responding
to periodic heartbeats. More details on crash detection in
Minix 3 are presented in [20].
A transparent restart is not possible unless we can preserve
the server’s state and we therefore run a storage process
dedicated to storing interesting state of other components as
key and value pairs. We start each server either in fresh start
or in restart mode so the process knows whether it should
try to recover its state or not. It can request the original state