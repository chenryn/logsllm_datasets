### 2.3.2 Execution-based Analysis through VM-based Page Rendering

A drive-by download attack occurs when a web page exploits a flaw in the victim’s browser. In the worst case, such an attack can allow the attacker to install and run arbitrary software on the victim’s computer. Our execution-based approach to detecting these attacks is adapted from a technique we developed in our earlier spyware measurement study [28], where we used virtual machines (VMs) to determine whether a web page contained malicious content. We summarize this technique below.

Our detection method is based on the assumption that malicious web content will attempt to break out of the security sandbox implemented by the browser. For example, simply rendering a web page should not cause any of the following side effects:
- Creation of new processes other than known helper applications
- Modifications to the file system outside of safe folders such as the browser cache
- Registry modifications
- Browser or OS crashes

If we observe any of these unacceptable conditions, it indicates that the web page contains malicious content.

To analyze a web page, we use a “clean” VMware [45] virtual machine with unnecessary services disabled. We direct an unmodified browser running in the VM to fetch and render the web page. Since other services are disabled, any side effects observed must be caused by the browser rendering the web page. We monitor the guest OS and browser using “triggers” installed to detect sandbox violations, including those listed above. If a trigger fires, we declare the web page unsafe. This mechanism is described in more detail in [28].

This technique is behavior-based rather than signature-based. We do not attempt to characterize vulnerabilities; instead, we execute or render content to look for evidence of malicious side effects. Given a sufficiently comprehensive set of trigger conditions, we can detect zero-day attacks that exploit vulnerabilities not yet identified.

### 2.4 Limitations

While our approach is effective, it has several challenges and limitations. First, the overhead of cloning a VM, rendering content within it, and detecting trigger conditions can be high. In Section 3, we describe several optimizations to mitigate this overhead and evaluate their success in Section 4. Second, our trigger monitoring system should be located outside the VM to prevent tampering or disabling by the malware it is attempting to detect. Although we have not done so, we believe we could modify our implementation to use techniques such as VM introspection [18] to achieve this. Third, pre-executing web content on-the-fly raises several correctness and completeness issues, which we discuss below.

#### 2.4.1 Non-determinism

With SpyProxy in place, web content is rendered twice: once in the VM’s sandboxed environment and once on the client. For our technique to work, all attacks must be observed by the VM; the client must never observe an attack that the VM-based execution missed. This will be true if the web content is deterministic and follows the same execution path in both environments. Therefore, SpyProxy is ideal for deterministic web pages designed to be downloaded and displayed to the user as information.

However, highly interactive web pages, similar to general-purpose programs, may have execution paths that depend on non-deterministic factors such as randomness, time, unique system properties, or user input. An attacker could use non-determinism to evade detection. For example, a malicious script could flip a coin to decide whether to carry out an attack, defeating SpyProxy 50% of the time.

As a more pertinent example, if a website relies on JavaScript to control ad banner rotation, it is possible that the VM worker will see a benign ad while the client will see a malicious ad. However, much of Internet advertising today is served from ad networks like DoubleClick or Advertising.com, where a web page makes an image request to the server, and any non-determinism in picking an ad happens on the server side. In this case, SpyProxy will return the same ad to both the VM worker and the client. Generally, only client-side non-determinism could cause problems for SpyProxy.

Potential solutions for handling non-determinism in SpyProxy include logging non-deterministic events in the VM and replaying them on the client, although this likely requires extensive browser modifications. Another approach is to rewrite the page to make it deterministic, but this is an open problem and unlikely to generalize across content types. The results of VM-based rendering can be shipped directly to the client using a remote display protocol, avoiding client-side rendering altogether, but this would break the integration between the user’s browser and the rest of their computing environment. None of these approaches seem simple or satisfactory. As a result, we consider malicious non-determinism a fundamental limitation to our approach. In our prototype, we evaluated its practical impact on SpyProxy’s effectiveness, and our results in Section 4 demonstrate that our system detected all malicious web pages examined, despite the majority containing non-determinism. We recognize that in the future, an adversary could introduce non-determinism to evade detection by SpyProxy.

#### 2.4.2 Termination

Our technique requires that the web page rendering process terminates so that we can decide whether to block content or forward it to the user. SpyProxy uses browser interfaces to determine when a web page has been fully rendered. Unfortunately, some scripts depend on timer mechanisms or user input, and determining when or whether a program will terminate is generally not possible.

To prevent “timebomb-based” attacks, we speed up the virtual time in the VM [28]. If the rendering times out, SpyProxy pessimistically assumes the page has caused the browser to hang and considers it unsafe. Post-rendering events, such as those triggered by user input, are not currently handled by SpyProxy but could be supported with additional implementation. For example, we could keep the VM worker active after rendering and intercept the events triggered by user input to forward them to the VM for pre-checking. This interposition could be accomplished by inserting runtime checks similar to BrowserShield [33].

#### 2.4.3 Differences Between the Proxy and Client

In theory, the execution environment in the VM and on the client should be identical, so that web page rendering follows the same execution path and produces the same side effects in both executions. Differing environments might lead to false positives or false negatives. In practice, malware usually targets a broad audience, and small differences between the two environments are not likely to matter. For our system, it is sufficient that harmful side effects produced at the client are a subset of those produced in the VM. This implies that the VM system can be partially patched, making it applicable for all clients with a higher patch level. Currently, SpyProxy uses unpatched Windows XP VMs with an unpatched IE browser, making SpyProxy conservative and blocking threats even if the client is patched to defend against them.

There is a possibility that a patch could contain a bug, causing a patched client to be vulnerable to an attack to which the unpatched SpyProxy is immune [24]. We assume this is a rare occurrence and do not attempt to defend against it.

### 2.5 Client-side vs. Network Deployment

SpyProxy has a flexible implementation: it can be deployed in the network infrastructure or serve as a client-side proxy. There are trade-offs involved in choosing one over the other. A network deployment allows clients to benefit from the workloads of other clients through caching of both data and analysis results. On the other hand, a client-side approach removes the bottleneck of a centralized service and the latency of an extra network hop. However, clients would be responsible for running virtualization software necessary to support SpyProxy’s VM workers. Many challenges, such as latency optimizations or non-determinism issues, apply in both scenarios.

While designing our prototype and carrying out our evaluation, we decided to focus on the network-based SpyProxy. In terms of effectiveness, the two approaches are identical, but obtaining good performance with a network deployment presents more challenges.

### 3 Performance Optimizations

The simple proxy architecture described in Section 2 effectively detects and blocks malicious web content but performs poorly. For a given web page request, the client browser will not receive or render any content until the proxy has downloaded the full page from the remote web server, rendered it in a VM worker, and ensured no triggers have fired. Accordingly, many of the optimizations that web browsers perform to minimize perceived latency, such as pipelining the transfer of embedded objects and the rendering of elements within the main web page, cannot occur.

To mitigate the cost of VM-based checking in our proxy, we implemented a set of performance optimizations that either enable the browser to perform its normal optimizations or eliminate proxy overhead altogether.

#### 3.1 Caching the Result of Page Checks

Web page popularity follows a Zipf distribution [6]. Thus, a significant fraction of requests generated by a user population are repeated requests for the same web pages. Web proxy caches take advantage of this fact to reduce web traffic and improve response times [1, 13, 15, 21, 52, 53]. Web caching studies generally report hit rates as high as 50%.

Given this, our first optimization is caching the result of our security check so that repeated visits to the same page incur the overhead of our VM-based approach only once. In principle, the hit rate in our security check cache should be similar to that of web caches.

This basic idea faces complications. The principle of complete mediation warns against caching security checks, as changes to the underlying security policy or resources could lead to caching an incorrect outcome [34]. In our case, if any component in a web page is dynamically generated, different clients may be exposed to different content. However, in our architecture, our use of the Squid proxy ensures that no confusion can occur: we cache the result of a security check only for objects that Squid also caches, and we invalidate pages from the security cache if any of the page’s objects are invalid in the Squid cache. Thus, we generate a hit in the security cache only if all of the web page content will be served out of the Squid proxy cache. Caching checks for non-deterministic pages is dangerous, and we take the simple step of disabling the security cache for such pages.

#### 3.2 Prefetching Content to the Client

In the unoptimized system, the web client will not receive any content until the entire web page has been downloaded, rendered, and checked by SpyProxy. As a result, the network between the client and the proxy remains idle while the page is being checked. If the client has a low-bandwidth network connection, or if the web page contains large objects, this idle time represents a wasted opportunity to begin downloading content to the client.

To rectify this, SpyProxy includes additional components and protocols that overlap several steps. Specifically, a new client-side component acts as a SpyProxy agent. The client-side agent prefetches content from SpyProxy and releases it to the client browser once SpyProxy informs it that the web page is safe. This improves performance by transmitting content to the client in parallel with checking the web page in SpyProxy. Because we do not give any web page content to the browser before the full page has been checked, this optimization does not erode security.

In our prototype, we implemented the client-side agent as an IE plugin. The plugin communicates with the SpyProxy front end, spooling web page content and storing it until SpyProxy grants authorization to release the content to the browser.

#### 3.3 Staged Release of Content

Although prefetching allows content to be spooled to the client while SpyProxy is performing its security check, the user’s browser cannot begin rendering any of that content until the full web page has been rendered and checked in the VM worker. This degrades responsiveness, as the client browser cannot take advantage of its performance optimizations that render content well before the full page has arrived.

We therefore implemented a “staged release” optimization. The goal of staged release is to present content considered safe for rendering to the client browser in pieces; as soon as the proxy believes that a slice of content (e.g., an object or portion of an HTML page) is safe, it simultaneously releases and begins transmitting that content to the client.

Figure 2 depicts the process of staged release. A page consists of a root page (typically containing HTML) and a set of embedded objects referred to from within the root page. As a web browser downloads and renders more of the root page, it learns about embedded objects and begins downloading and rendering them.

Without staged release, the proxy releases no content until the full web page and its embedded objects have been rendered in the VM. With staged release, once the VM has rendered an embedded object, it releases that object and all of the root page content that precedes its reference. If the client browser evaluates the root page in the same order as the VM browser, this is safe to do.