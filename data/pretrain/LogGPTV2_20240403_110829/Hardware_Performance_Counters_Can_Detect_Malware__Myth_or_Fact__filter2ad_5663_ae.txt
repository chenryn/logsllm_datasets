ekj
jt h event. Each v
is a 32 × 1 eigenvector. By multiplying each
example with the v eigenvector, we reduce the dimensions from
192 (6 events × 32 bins) to 12 (6 events × 2 components).
In summary, we list following steps to select the events:
• Run 7 programs 32 times and measure 130 micro-architectural
events.
• Divide the total run time of each program into 32 intervals
and sum the measured HPC values in each interval into a
separate bin.
• Sum the bins across diferent runs of the identical measure-
ments.
• Apply PCA on 130 events with 7 programs.
• Compute the approximation errors for 7 programs.
• Find 6 events with the least approximation errors.
Four of the selected events in our experiments align with other
works that do not provide any analysis of their selection of events [3,
5, 9ś11]. We observe that one event is related to data cache load and
store references. Two other micro-architectural events are related
to load and store operations, which have also been used in other
works. It is not clear how load and store operations deterministi-
cally contain the information of malicious behavior. Any statistics
of memory behavior should be legitimate in program execution,
since the memory accesses inheretly exist in every program. In
our selection of events, we include the events in kernel mode to
capture complete program behavior. The remaining 3 selected hard-
ware events related to cache lush behavior, system management
interrupts and CPUID instructions. However, we cannot infer any
reasons why these instructions/operations by the kernel can be
mapped to any malicious user-level behavior.
4.3 Classiication Models
In ğ4.2, we selected the 6 events to monitor and formulate the
eigenvector matrix in Equation 6. With this method, we can extract
features from the measured HPC values to get examples for ma-
chine learning models, i.e. traces in our datasets of benignware and
malware.
To have the same number of measurements on the same pro-
gram samples as in ğ4.2, we run each benignware program and
each malware program 32 times, and collect 61,568 (2 × 962 × 32),
30,784 for benignware and 30,784 for malware, measured HPC val-
ues (1,026 CPU hours). We sum the measured HPC values into 32
histogram bins (as described in ğ4.2) for each of 6 events. Each ex-
ample of histogram binned HPC values has 192 (6 events × 32 bins)
features. By multiplying each example with the v eigenvector in
Equation 6, we reduce the dimensions from 192 (6 events × 32 bins)
to 12 (6 events × 2 components). To this end, we convert the mea-
sured HPC values into histogram bins, and then transform them
into traces.
Using the reduction of dimensions (ğ4.2), the input matrix A30,784×192
(30,784 examples and 192 features) of benignware or malware is
transformed to lower-dimensional space as A′
30,784×12 (30,784 ex-
amples and 12 features). For training and testing of the machine
learning models, we are going to separate the examples in matrix
A′ into training and testing datasets (training-and-testing split). In
our experiments, we consider 2 Training-and-Testing Approaches
(TTA) to divide our dataset into training set and testing set. The
two approaches are as follows:
TTA1 Dividing 30,784 traces with a split of 90:10 ratio, resulting in
27,704 traces (90% of 30,784 traces) as training dataset and
3,078 traces (10% of 30,784 traces) as testing dataset both in
benignware and malware experiments.
TTA2 Dividing 962 programs with a split of 90:10 ratio, result-
ing in traces of 866 programs (90% of programs) as training
dataset and traces of 96 programs (10% of programs) as test-
ing dataset both in benignware and malware experiments.
In the irst training-and-testing approach (TTA1), we randomly
choose 27,704 traces as training dataset and 3,078 traces as test-
ing dataset both in benignware and malware experiments. In this
approach, the traces resulting from the same program sample can
appear in both training and testing datasets. As a result, such an ap-
proach corresponds to a highly optimistic and unrealistic scenario
where the testing programs (benignware or malware) are available
during training. Given that thousands of new malware appearing
everyday, it is impossible to include all the malware that user may
encounter. Hence, TTA1 should not be applied in training machine
learning models for malware detection.
In the second training-and-testing approach (TTA2), we ran-
domly choose traces of 866 programs as training dataset and traces
of 96 programs as testing dataset both in benignware and malware
experiments. TTA2 corresponds to a realistic case where during
training model, we do not have access to the exact programs, benign
or malicious, that users run in the real life. To validate across our
models, we perform 10-fold cross-validations 1,000 times. For each
10-fold cross-validation, we randomly shule the dataset to ensure
diference across 1,000 rounds. In each 10-fold cross-validation,
each example in the dataset is used in training 9 times and testing
once. This ensures the identical times of training and testing for
every single example, compared to randomly shuling the data
and validating the machine learning models. With 1,000 10-fold
cross-validations, we can ensure that the standard deviations of
detection rates increase no more with more rounds of validations.
In our experiments, we perform training and testing with both
TTA1 and TTA2. We compare the detection results in terms of
precision, recall, F1-score, and Area Under Curve (AUC) in both ap-
proaches. We use the implementations of machine learning models
in scikit-learn package [35]: DT, RF, NN, KNN, AdaBoost, and Naive
Bayes. The seed for randomness in machine learning initialization
and division of data comes from the random number generator
ł/dev/urandomž. During training, we set the parameters of the ma-
chine learning models as described below to prevent the machine
learning models from underitting due to default limitations in com-
putational resources set by scikit-learn. We used default values for
the remaining parameters in scikit-learn.
• DT: We set the maximum depth as 100 to classify the malware
and benignware. The number of levels is suicient to avoid
any underitting of the model.
• RF: We set the maximum depth to be the same as in the DT.
We enable a maximum 200 estimators in the RF. The number
of estimators is suicient to avoid any underitting of the
model.
• NN: The network we use here is a Multilayer Perceptron
(MLP) neural network having 4 layers with 100 neurons
in each layer. We apply łtanhž function as the activation
function. We use L2 regularization on the parameters in the
NN.
• KNN: We choose 5 as the number of nearest neighbors, and
perform experiments with K value varying from 1 to 20.
When K equals to 5, the F1-score reaches the highest detec-
tion rates.
• AdaBoost: Adaboost is an Ensemble classiier, which utilizes
a collection of estimators. Adaboost its a sequence of classi-
iers on the training data. The predictions are decided based
on a majority vote [36]. The default value of the number of
estimators is 50. We use 200 estimators instead of 50, since
our test experiments show that 200 estimators have a higher
detection rate for AdaBoost.
• Naive Bayes: We use the same number of malware and be-
nignware traces in the model. The prior probability is 50%.
Table 3: Detection Rates with TTA1 and TTA2: Red means the value is less than 50% and bold means that the value is more than 90%
TTA1
TTA2
Models
Precision[%] Recall[%]
F1-Score[%] AUC[%]
Precision[%] Recall[%]
F1-Score[%] AUC[%]
Decision Tree
Naive Bayes
Neural Net
AdaBoost
Random Forest
Nearest Neighbors
83.04
70.36
82.41
78.61
86.4
84.84
83.75
7.97
75.4
71.73
83.34
82.37
83.39
14.32
78.75
75.01
84.84
83.59
89.65
58.11
84.41
80.57
91.84
89.26
83.21
56.72
91.34
75.78
84.36
82.7
77.44
5.425
22.16
65.6
78.44
77.88
80.22
9.903
35.66
70.32
81.29
80.22
87.36
58.38
66.43
77.96
89.94
86.98
1.0
0.8
0.6
0.4
0.2
e
t
a
R
e
v
i
t
i
s
o
P
e
u
r
T
ROC Curve
Decision Tree: 89.65%
Neural Net: 84.41%
AdaBoost: 80.57%
Random Forest: 91.84%
Nearest Neighbors: 89.26%
1.0
0.8
0.6
0.4
0.2
e
t
a
R
e
v
i
t
i
s
o
P
e
u
r
T
ROC Curve
Decision Tree: 87.36%
Neural Net: 66.43%
AdaBoost: 77.96%
Random Forest: 89.94%
Nearest Neighbors: 86.98%
0.0
0.0
0.2
0.4
0.6
0.8
1.0
False Positive Rate
0.0
0.0
0.2
0.4
0.6
0.8
1.0
False Positive Rate
(a)
(b)
Figure 5: Receiver Operating Characteristic (ROC) curve of 5 models. (a) The AUC of DT, NN, AdaBoost, RF, and KNN using (TTA1) is 89.65%,
84.41%, 80.57%, 91.84%, and 89.26%, respectively. (b) The AUC of DT, NN, AdaBoost, RF, and KNN using (TTA2) is 87.36%, 66.43%, 77.96%, 89.94%,
and 86.98%, respectively.
5 EXPERIMENTAL RESULTS
In this section, we show our results with the experiments to detect
malware using HPCs and contrast the ones obtained in previous
works. We report malware detection rates in terms of precision,
recall, F1-score, and Area Under Curve (AUC) in Receiver Operating
Characteristic (ROC) plots. We use the positive label to denote
malware and the negative label to denote benignware. True positive
samples (T+) are malware programs that are classiied as malware.
False positive samples (F+) are benign programs that are classiied
as malware. False negative samples (F−) are malware programs that
are classiied as benignware. Precision is deined as the number of
true positive samples (T+) divided by the number of all the positive
samples, (T+ + F+) in Equation 7. Recall is deined as the number
of true positive samples (T+) divided by the sum of the number of
true positive (T+) and the number of false negative (F−) samples in
Equation 7. The F1-score is the harmonic mean of precision and
recall in Equation 8.
Precision =
F 1 − score =
T+
Recall =
T+
T+ + F+
2 × Recall × Precision
T+ + F−
Precision + Recall
=
2T+
2T+ + T− + F−
(7)
(8)
The ROC curve represents how the true positive rate varies with
diferent thresholds for the false positive rate. We can reach 100%
true positive rate only if we accept a false positive rate of 100%.
Conversely, if we want to achieve a 0% false positive rate, then
that leads to 0% true positive rate. By changing the false positive
rate threshold, we can trade-of the false positive rate with the true
positive rate. Thus we use AUC of ROC curve to measure how
efective classiiers are at various false positive rate thresholds.
5.1 Malware Detection
In this section, we report the detection rates (precision, recall, and
F1-score) with 2 diferent data divisions, TTA1 and TTA2. TTA1
is the division of data according to the traces; while TTA2 is the
division of data according to the programs, as deined in ğ4.3.
5.1.1 Results from TTA1 Experiments. We train and test traces
using various machine learning models and determine the detection
rates (precision, recall, and F1-score) with TTA1. Then we plot the
ROC curves and compute the AUCs. Table 3 shows the precision,
recall, F1-score, and the AUCs of ROC curves. Any results with a
value larger than 90% and smaller than 50% are set in bold and red,
respectively. Figure 5 shows the ROC curves and the AUCs of ROC
for diferent machine learning models.
DT uses the diferent features to classify examples at diferent
tree branches. RF uses a collection of DTs to perform classiications.
KNN determines the classes of each examples by comparing the
number of examples within predeined distances. DT, RF, and KNN
models target classifying outliers in the dataset [37], which it our
malware detection problem. According to our results, the detection
rates of precision, recall, and F1-score are higher in DT, RF, and KNN
models than any other models. The F1-scores in DT, RF, and KNN
models are 83.39%, 84.84%, and 83.59%. Figure 5 shows the higher
true positive rates of RF and DT with diferent thresholds, compared
to other models. Accordingly, the AUCs in DT, RF, and KNN are
89.65%, 91.84%, and 89.26%. Figure 5(a) shows that the AUCs of
ROC curves in DT and RF are the highest in various thresholds of
false positive rates.
AdaBoost model leverages a collection of machine learning mod-
els. AdaBoost and NN model are designed to classifying clusters
of examples. They perform worse in terms of detection rates com-
pared to DT, RF, and KNN, as these models are designed to classify
outliers. The F1-scores in AdaBoost and NN are 75.01% and 78.75%.
In Figure 5(a), AdaBoost and NN models are also worse than DT
and RF models. The AUC values for AdaBoost and NN are 80.57%
and 84.41%.
The classiication of Naive Bayes is only based on the probabil-
ities of the occurrences of malware and benignware, which is a
poor assumption to design classiiers [38]. In our design, we use the
prior probability (50%) to design the Naive Bayes classiier. Naive
Bayes model has many false negatives, which causes the F1-score
value to be as low as 14.32%. The AUC of Naive Bayes is 58.11%.
As a result, Naive Bayes classiies examples between malware and
benignware with low detection rates.
5.1.2 Results from TTA2 Experiments. We perform another ex-
periment of training and testing using various machine learning
models to show the detection rates (precision, recall, and F1-score)
with TTA2. The F1-scores of DT, RF, KNN, Naive Bayes, AdaBoost,
and NN models are 80.22%, 81.29%, 80.22%, 9.903%, 70.32%, and
35.66% using TTA2, compared to 83.39%, 84.84%, 83.59%, 14.32%,
75.01%, and 78.75% using TTA1 in Table 3. By using TTA2, the
detection rates are lower compared to the scenario using TTA1.
Figure 5(b) shows the ROC curves and the AUCs of ROC for dif-
ferent machine learning models using . The AUCs of ROC of DT, RF,
KNN, Naive Bayes, AdaBoost, and NN models are 87.36%, 89.94%,
86.98%, 58.38% 77.96%, and 66.43% using TTA2 in Figure 5(b), com-
pared to 89.65%, 91.84%, 89.26%, 58.11%, 80.57%, and 84.41% using
TTA1 in Figure 5(a).
Demme et al. showed precision varying from 25% ∼ 100% [3]
among diferent families of malware, without any recall values
reported using TTA1. The median precision among all the families
of malware is around 80%, with TTA1. Precision value of 80%
corresponds to the False Discovery Rate5 of 20%. Consider that a
default Windows 7 installation has 1,323 executable iles, an AV
system with a 20% False Discovery Rate would lag 264 of these iles
incorrectly as malware ś clearly such a detection system would not
be practical. As a result, such a malware detection method is not
usable in real-life systems. With thousands of malware reported
everyday, the oline training of malware detection cannot capture
the same malware program that a user may encounter. In real-life
cases, the malware detection rates of HPC-based malware detection
would be those in columns of TTA2 of Table 3 and Figure 5(b).
These results show that high detection rates and robustness in
5False Discovery Rate (F+ /(F+ + T+)
detection are over-estimated due to division of data during training.
Our comparison using TTA1 and TTA2 shows that using TTA2
can cause the precisions to be even lower. Thus, prior works could
have even worse precisions by using TTA2. In the next subsection,
we will show that the results presented in this subsection are not
an exception.
5.2 Cross-Validation
Cross-validation is a common practice in machine learning for