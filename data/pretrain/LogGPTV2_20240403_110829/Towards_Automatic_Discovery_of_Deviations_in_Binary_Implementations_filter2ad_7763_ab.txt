deviation inputs(cid:13)
1 - Formula Extraction Phase
2 - Deviation Detection Phase
Candidate
deviation inputs(cid:13)
Different?
Yes
Deviation
Program
binary P1
Program
binary P2
Output
state 1
Output
state 2
3 - Validation Phase(cid:13)
Figure 1: Overview of our approach.
an input x, such as an HTTP GET request. For each
implementation, we log an execution trace of the binary
as it processes the input, and record what output state it
reaches, such as halting or sending a reply. We assume
that the execution from both binaries reaches semanti-
cally equivalent output states; otherwise we have already
found a deviation! For each implementation P1 and
P2, we then use this information to produce a boolean
formula over the input, f1 and f2 respectively, each of
which is satisﬁed for inputs that cause the binary to reach
the same output state as the original input did.
Next, in the deviation detection phase, we use a solver
(such as a decision procedure) to ﬁnd differences in the
two formulas f1 and f2. In particular, we ask the solver
if (f1 ∧ ¬f2) ∨ (f2 ∧ ¬f1) is satisﬁable. When satisﬁable
the solver will return an example satisfying input. We
call these inputs the candidate deviation inputs.
Finally, in the validation phase we evaluate the can-
didate deviation inputs obtained in the formula extrac-
tion phase on both implementations and check whether
the implementations do in fact reach semantically differ-
ent output states. This phase is necessary because the
symbolic formula might not include all possible execu-
tion paths, then an input that satisﬁes f1 is guaranteed to
make P1 reach the same semantically equivalent output
state as the original input x but an input that does not
satisfy f1 may also make P1 reach a semantically equiv-
alent output state. Hence, the generated candidate devia-
tion inputs may actually still cause both implementations
to reach semantically equivalent output states.
If the implementations do reach semantically different
output states, then we have found a deviation triggered by
that input. This deviation is useful for two things: (1) it
may represent an implementation error in at least one of
the implementations, which can then be checked against
the protocol speciﬁcation to verify whether it is truly an
error; (2) it can be used as a ﬁngerprint to distinguish
between the two implementations.
Iteration. We can iterate this entire process to examine
other input types. Continuing with the HTTP example,
we can compare how the two implementations process
other types of HTTP requests, such as HEAD and POST,
by repeating the process on those types of requests.
3 Design
In this section, we describe the details of the three phases
in our approach, the formula extraction phase, the devia-
tion detection phase, and the validation phase.
3.1 Formula Extraction Phase
3.1.1 Intuition and Overview
The goal of the formula extraction phase is that given an
input x such that P1(x) = P2(x) = s, where s is the
output state when executing input x with the two given
programs, we would like to compute two formulas, f1
and f2, such that,
and
f1(x) = true ⇒ P1(x) = s
f2(x) = true ⇒ P2(x) = s,
This matches well with the technique of weakest precon-
dition (WP) [19, 26]. The weakest precondition, denoted
216
16th USENIX Security Symposium
USENIX Association
wp(P, Q), is a boolean formula f over the input space I
of P such that if f (x) = true, then P (x) will terminate
in a state satisfying Q. In our setting, the post-condition
is the protocol output state, and the weakest precondition
is a formula characterizing protocol inputs, which will
cause the implementation to reach the speciﬁed protocol
output state.
Unfortunately, calculating the weakest precondition
over an entire real-world binary program can easily re-
sult in a formula that is too big to solve. First, there may
be many program paths which can lead to a particular
output state. We show that we can generate interesting
deviations even when considering a single program path.
Second, we observe that in many cases only a small sub-
set of instructions operate on data derived from the origi-
nal input. There is no need to model the instructions that
do not operate on data derived from the original input,
since the result they compute will be the same as in the
original execution. Therefore we eliminate these instruc-
tions from the WP calculation, and replace them with
only a series of assignments of concrete values to the rel-
evant program state just before an instruction operates on
data derived from the input.
Hence, in our design, we build the symbolic formula
in two distinct steps. We ﬁrst execute the program on the
original input, while recording a trace of the execution.
We then use this execution trace to build the symbolic
formula.
3.1.2 Calculating the Symbolic Formula
In order to generate the symbolic formula, we perform
the following steps:
1. Record the execution trace of the executed program
path.
2. Process the execution trace. This step translates the
execution trace into a program B written in our sim-
pliﬁed intermediate representation (IR).
3. Generate the appropriate post-condition Q.
4. Calculate the weakest precondition on B by:
(a) Translating B into a single assignment form.
(b) Translating the (single assignment) IR pro-
gram into the guarded command language
(GCL). The GCL program, denoted Bg, is
semantically equivalent to the input IR state-
ments, but appropriate for the weakest precon-
dition calculation.
(c) Computing the weakest precondition f =
wp(Bg, Q) in a syntax-directed fashion on the
GCL.
The output of this phase is the symbolic formula f .
Below we describe these steps in more detail.
Step 1: Recording the execution trace. We generate
formulas based upon the program path for a single ex-
ecution. We have implemented a path recorder which
records the execution trace of the program. The exe-
cution trace is the sequence of machine instructions ex-
ecuted, and for each executed instruction, the value of
each operand, whether each operand is derived from the
input, and if it is derived from the input, an identiﬁer for
the original input stream it comes from. The trace also
has information about the ﬁrst use of each input byte,
identiﬁed by its offset in the input stream. For example,
for data derived from network inputs, the identiﬁer spec-
iﬁes which session the input came from, and the offset
speciﬁes the original position in the session data.
Step 2: Processing the execution trace. We process
the execution trace to include only relevant instructions.
An instruction is relevant if it operates on data derived
from the input I. For each relevant instruction, we:
• Translate the x86 instruction to an easier-to-analyze
intermediate representation (IR). The generated IR
is semantically equivalent to the original instruc-
tion.
The advantage of our IR is that it allows us to per-
form subsequent steps over the simpler IR state-
ments, instead of the hundreds of x86 instructions.
The translation from an x86 instruction to our IR
is designed to correctly model the semantics of the
original x86 instruction, including making other-
wise implicit side effects explicit. For example, we
insert code to correctly model instructions that set
the eflags register, single instruction loops (e.g.,
rep instructions), and instructions that behave dif-
ferently depending on the operands (e.g., shifts).
Our IR is shown in Table 1. We translate x86 in-
struction into this IR. Our IR has assignments (r :=
v), binary and unary operations (r := r1
2bv and
r := 2uv where 2b and 2u are binary and unary
operators), loading a value from memory into a reg-
ister (r1 := ∗(r2)), storing a value (∗(r1) := r2),
direct jumps (jmp `) to a known target label (label
`i), indirect jumps to a computed value stored in a
register (ijmp r), and conditional jumps (if r then
jmp `1 else jmp `2).
• Translate the information logged about the operands
into a sequence of initialization statements. For
each operand:
– If it is not derived from input, the operand is
assigned the concrete value logged in the ex-
ecution trace. These assignments effectively
model the sequences of instructions that we do
not explicitly include.
USENIX Association
16th USENIX Security Symposium
217
Instructions
i
::=
∗(r1) := r2|r1 := ∗(r2)|r := v|r := r1
|r := 2uv | label li | jmp ` | ijmp r
| if r jmp `1 else jmp `2
2bv
Operations 2b
2u
v
τ
Operands
Reg. Types
::= +, −, ∗, /, (cid:28), (cid:29), &, |, ⊕, ==, ! =, <, ≤ (Binary operations)
::= ¬, ! (unary operations)
::=
::=
n (an integer literal) | r (a register) | ` (a label)
reg64 t | reg32 t | reg16 t | reg8 t | reg1 t (number of bits)
Table 1: Our RISC-like assembly IR. We convert x86 assembly instructions into this IR.
– For operands derived from input, the ﬁrst time
we encounter a byte derived from a particu-
lar input identiﬁer and offset, we initialize the
corresponding byte of the operand with a sym-
bolic value that uniquely identiﬁes that input
identiﬁer and offset. On subsequent instruc-
tions that operate on data derived from that
particular input identiﬁer and offset, we do
not initialize the corresponding operand, since
we want to accurately model the sequence of
computations on the input.
The output of this step is an IR program B consisting
of a sequence of IR statements.
Step 3: Setting the post-condition. Once we have
generated the IR program from the execution trace, the
next step is to select a post-condition, and compute the
weakest precondition of this post-condition over the pro-
gram, yielding our symbolic formula.
The post-condition speciﬁes the desired protocol out-
put state, such as what kind of response to a request
message is desired. In our current setting, an ideal post-
condition would specify that “The input results in an ex-
ecution that results in an output state that is semantically
equivalent to the output state reached when processing
the original input.” That is, we want our formula to be
true for exactly the inputs that are considered “seman-
tically equivalent” to the original input by the modeled
program binary.
In our approach, the post-condition speciﬁed the out-
put state should be the same as in the trace. In order to
make the overall formula size reasonable, we add addi-
tional constraints to the post-condition which constraint
the formula to the same program path taken as in the
trace. We do this by iterating over all conditional jumps
and indirect jumps in the IR, and for each jump, add a
clause to the post-condition that ensures that the ﬁnal for-
mula only considers inputs that also result in the same
destination for the given jump. For example, if in the
trace if e then `1 else `2 was evaluated and the
next instruction executed was `2, then e must have eval-
uated to false, and we add a clause restricting e = false
to the post-condition.
In some programs, there may be multiple paths that
reach the same output state. Our techniques can be gen-
eralized to handle this case, as discussed in Section 6. In
practice, we have found this post-condition to be sufﬁ-
cient for ﬁnding interesting deviations. Typically, inputs
that cause the same execution path to be followed are
treated equivalently by the program, and result in equiv-
alent output states. Conversely, inputs that follow a dif-
ferent execution path often result in a semantically dif-
ferent output state of the program. Although more com-
plicated and general post-conditions are possible, one in-
teresting result from our experiments is that the simple
approach was all that was needed to generate interesting
deviations.
Step 4: Calculating the weakest precondition. The
weakest precondition (WP) calculation step takes as in-
put the IR program B from Step 2, and the desired post-
condition Q from Step 3. The weakest precondition, de-
noted wp(B, Q), is a boolean formula f over the input
space such that if f (x) = true, then B(x) will terminate
in a state satisfying Q. For example, if the program is
B : y = x + 1 and Q : 2 < y < 5, then wp(B, Q) is
1 < x < 4.
We describe the steps for computing the weakest pre-
condition below.
Step 4a: Translating into single assignment form. We
translate the IR program B from the previous step into
a form in which every variable is assigned at most once.
(The transformed program is semantically equivalent to
the input IR.) We perform this step to enable additional
optimizations described in [19, 29, 36], which further re-
duce the formula size. For example, this transformation
will rewrite the program x := x+1; x := x+1; as
x1 := x0+1; x2 := x1+1;. We carry out this
transformation by maintaining a mapping from the vari-
able name to its current incarnation, e.g., the original
variable x may have incarnations x0, x1, and x2. We
iterate through the program and replace each variable use
with its current incarnation. This step is similar to com-
puting the SSA form of a program [39], and is a widely
used technique.
Step 4b: Translating to GCL. The translation to GCL
takes as input the single assignment form from step 4a,
218
16th USENIX Security Symposium
USENIX Association
and outputs a semantically equivalent GCL program Bg.
We perform this step since the weakest precondition is
calculated over the GCL language [26]. The result-
ing program Bg is semantically equivalent to the input
single-assignment IR statements. The weakest precondi-
tion is calculated in a syntax-directed manner over Bg.
The GCL language constructs we use are shown in
Table 2. Although GCL may look unimpressive, it is
sufﬁciently expressive for reasoning about complex pro-
grams [24, 26, 28, 29] 1. Statements S in our GCL pro-
grams will mirror statements in assembly, e.g., store,
load, assign, etc. GCL has assignments of the form
lhs := e where lhs is a register or memory location, and
e is a (side-effect) free expression. assume e assumes
a particular (side-effect free) expression is true. An as-
sume statement is used to reason about conditional jump
predicates, i.e., we add “assume e” for the true branch of
a conditional jump, and “assume ¬e” for the false branch
of the conditional jump. assert e asserts that e must be
true for execution to continue, else the program fails. In
other words, Q cannot be satisﬁed if assert e is false.
skip is a semantic no-op. S1; S2 denotes a sequence
where ﬁrst statement S1 is executed and then statement
S2 is executed. S1
2S2 is called a choice statement, and
indicates that either S1 or S2 may be executed. Choice
statements are used for if-then-else constructs.
For example, the IR:
i f
( x0 < 0 ) {
x1 := x0 − 1 ;
} e l s e {
x1 := x0 + 1 ;
}