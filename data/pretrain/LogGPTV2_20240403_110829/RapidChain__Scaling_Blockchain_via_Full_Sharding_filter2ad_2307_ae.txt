let ρ1j , ..., ρmj be the shares node j receives from the previous step. Node j computes its share of r
l =1 ρl j. Finally, nodes exchange their shares of r and reconstruct the result using
the Lagrange interpolation technique [35]. The random generation protocol consists of two phases:
sharing and reconstruction. The sharing phase is more expensive in practice but is executed in
advance before the start of the epoch.
Any new node who wishes to join the system can contact any node in any committees at any
time and request the randomness of this epoch as a fresh PoW puzzle. The nodes who solve the
puzzle will send a transaction with their solution and public key to the reference committee. If the
solution is received by the reference committee before the end of the current epoch, the solution is
accepted and the reference committee adds the node to the list of active nodes for the next epoch.
19
4.6.3 Committee Reconﬁguration
Partitioning the nodes into committees for scalability introduces a new challenge when dealing with
churn. Corrupt nodes could strategically leave and rejoin the network, so that eventually they can
take over one of the committees and break the security guarantees of the protocol. Moreover, the
adversary can actively corrupt a constant number of uncorrupted nodes in every epoch even if no
nodes join/rejoin.
One approach to prevent this attack is to re-elect all committees periodically faster than the
adversary’s ability to generate churn. However, there are two drawbacks to this solution. First,
re-generating all of the committees is very expensive due to the large overhead of the bootstrapping
protocol (see Section 5). Second, maintaining a separate ledger for each committee is challenging
when several committee members may be replaced in every epoch.
To handle this problem, RapidChain uses a modiﬁed version of the Cuckoo rule [8, 62], which we
refer to as the bounded Cuckoo rule, to re-organize only a subset of committee members during the
reconﬁguration event at the beginning of each epoch. This modiﬁcation is to ensure that committees
are balanced with respect to their sizes as nodes join or leave the network. In the following, we ﬁrst
describe the basic Cuckoo rule algorithm and then, proceed to the bounded cuckoo rule.
Cuckoo Rule. To map the nodes to committees, we ﬁrst map each node to a random position in
[0, 1) using a hash function. Then, the range [0, 1) is partitioned into k regions of size k/n, and a
committee is deﬁned as the group of nodes that are assigned to O(log n) regions, for some constants k.
Awerbuch and Scheideler [8] propose the Cuckoo rule as a technique to ensure the set of committees
created in the range [0, 1) remain robust to join-leave attacks. Based on this rule, when a node wants
to join the network, it is placed at a random position x ∈ [0, 1), while all nodes in a constant-sized
interval surrounding x are moved (or cuckoo’ed) to new random positions in (0, 1]. Awerbuch and
Scheideler prove that given  < 1/2− 1/k in a steady state, all regions of size O(log n)/n have O(log n)
nodes (i.e., they are balanced) of which less than 1/3 are faulty (i.e., they are correct), with high
probability, for any polynomial number of rounds.
A node is called new while it is in the committee where it was assigned when it joined the system.
At any time after that, we call it an old node even if it changes its committee. We deﬁne the age
of a k-region as the amount of time that has passed after a new node is placed in that k-region. We
deﬁne the age of a committee as the sum of the ages of its k-regions.
Bounded Cuckoo Rule. At the start of each epoch, once the set of active nodes who remain in the
protocol for the new epoch is deﬁned, the reference committee, CR, deﬁnes the set of the largest m/2
committees (who have more active members) as the active committee set, which we denote by A.
We refer to the remaining m/2 committees with smaller sizes as the inactive committee set, denoted
by I. Active committees accept new nodes that have joined the network in the previous epoch, as
new members of the committee. However, inactive committees only accept the members who were
part of the network before, to join them. Both active and inactive committees fulﬁll any other
responsibilities they have in the protocol (such as consensus on blocks and routing transaction)
indiﬀerently. For each new node, the reference committee, CR, chooses a random committee Ca from
the set A and adds the new node to Ca. Next, CR evicts (cuckoos) a constant number of members
from every committee (including Ca) and assigns them to other committees chosen uniformly at
random from I.
In Section 6.5, we show two invariant properties which are maintained for each committee during
the reconﬁguration protocol: At any moment, the committees are balanced and honest. The ﬁrst
property ensures a bounded deviation in the sizes of the committees. The second property ensures
that each committee maintains its honest majority.
20
4.6.4 Node Initialization and Storage
Once a node joins a committee, it needs to download and store the state of the new committee.
We refer to this as the node initialization process. Moreover, as transactions are processed by the
committee, new data has to be stored by the committee members to ensure future transactions can
be veriﬁed. While RapidChain shards the global ledger into smaller ones each maintained by one
committee, the initialization and storage overhead can be large and problematic in practice due to
the high throughput of the system. One can employ a ledger pruning/checkpointing mechanism,
such as those described in [45, 42], to signiﬁcantly reduce this overhead. For example, a large
percentage of storage is usually used to store transactions that are already spent.
In Bitcoin, new nodes download and verify the entire history of transactions in order to ﬁnd/verify
the longest (i.e., the most diﬃcult) chain1.
In contrast, RapidChain is a BFT-based consensus
protocol, where the blockchain maintained by each committee grows based on member votes rather
than the longest chain principle [31]. Therefore, a new RapidChain node initially downloads only
the set of unspent transactions (i.e., UTXOs) from a suﬃcient number of committee members in
order to verify future transactions. To ensure the integrity of the UTXO set received by the new
node, the members of each committee in RapidChain record the hash of the current UTXO set in
every block added to the committee’s blockchain.
5 Evaluation
Experimental Setup. We implement a prototype of RapidChain in Go2 to evaluate its performance
and compare it with previous work. We simulate networks of up to 4,000 nodes by oversubscribing a
set of 32 machines each running up to 125 RapidChain instances. Each machine has a 64-core Intel
Xeon Phi 7210 @ 1.3GHz processor and a 10-Gbps communication link. To simulate geographically-
distributed nodes, we consider a latency of 100 ms for every message and a bandwidth of 20 Mbps
for each node. Similar to Bitcoin Core, we assume each node in the global P2P network can accept
up to 8 outgoing connections and up to 125 incoming connections. The global P2P overlay is only
used during our bootstrapping phase. During consensus epochs, nodes communicate through much
smaller P2P overlays created within every committee, where each node accepts up to 16 outgoing
connections and up to 125 incoming connections.
Unless otherwise mentioned, all numbers reported in this section refer to the expected behavior
of the system when less than half of all nodes are corrupted. In particular, in our implementation
of the intra-consensus protocol of Section 4.3, the leader gossips two diﬀerent messages in the same
iteration with probability 0.49. Also, in our inter-committee routing protocol, 49% of the nodes do
not participate in the gossip protocol (i.e., remain silent).
To obtain synchronous rounds for our intra-committee consensus, we set ∆ (see Section 3 for
the deﬁnition) conservatively to 600 ms based on the maximum time to gossip an 80-byte digest to
all nodes in a P2P network of 250 nodes (our largest committee size) as shown in Figure 4 (left).
Recall that synchronous rounds are required only during our consensus protocol to agree on a hash
of the block resulting in messages of up to 80 bytes size including signatures and control bits.
We assume each block of transaction consist of 4,096 transactions, where each transaction consists
of 512 bytes resulting in a block size of 2 MB. To implement our IDA-based gossiping protocol to
gossip 2-MB blocks within committees, we split each block into 128 chunks and use the Jerasure
library [3] to encode messages using erasure codes based on Reed-Solomon codes [59] with the
1Bitcoin nodes can, in fact, verify the longest chain by only downloading the sequence of block headers via a method called
simpliﬁed payment veriﬁcation described by Nakamoto [54].
2https://golang.org
21
Figure 4: Latency of gossiping an 80-byte message for diﬀerent committee sizes (left); Impact of block size
on throughput and latency (right)
decoding algorithm of Berlekamp and Welch [10].
Choice of Block Size. To determine a reasonable block size, we measure the throughput and
latency of RapidChain with various block sizes between 512 KB and 8,192 KB for our target network
size of 4,000 nodes. As shown in Figure 4 (right), larger block sizes generally result in higher
throughput but also in higher conﬁrmation latency. To obtain a latency of less than 10 seconds
common in most mainstream payment systems while obtaining the highest possible throughput,
we set our block size to 2,048 KB, which results in a throughput of more than 7,000 tx/sec and a
latency of roughly 8.7 seconds.
Throughput Scalability. To evaluate the impact of sharding, we measure the number of user-
generated transactions processed per second by RapidChain as we increase the network size from
500 to 4,000 nodes. As the network size increases, we choose larger committee sizes so that the
failure probability of each epoch is at most 2 · 10−6 (i.e., protocol fails after more than 1,300 years)
in each experiment. For our target network size of 4,000, we consider a committee size of 250 which
results in an epoch failure probability of less than 6 · 10−7 (i.e., time-to-failure of more than 4,580
years). As shown in Figure 5 (left), since the required committee sizes increase slowly as n increases,
the total number of shards still increases with the network size. Moreover, doubling the network size
increases the capacity of RapidChain by 1.5-1.7 times. This is an important measure of how well
the system can scale up its processing capacity with its main resource, i.e., the number of nodes.
We also evaluate the impact of our pipelining technique for intra-committee consensus (as de-
22
275325375425475525100125150175200225250Latency (ms)Committee SizeMaximumAverageMedian73848.840.005.0010.0015.0020.0025.0030.0035.0001000200030004000500060007000800090001282565121024204840968192Latency (sec)Transactions per SecondBlock Size (KB)ThroughputLatencyscribed in Section 4.3) by comparing the throughput with pipelining (i.e., 7,384 tx/sec) with the
throughput without pipelining (i.e., 5,287 tx/sec) for n = 4, 000, showing an improvement of about
1.4x.
Transaction Latency. We measure the latency of processing a transaction in RapidChain using two
metrics: conﬁrmation latency and user-perceived latency. The former measures the delay between
the time that a transaction is included in a block by a consensus participant until the block is
added to a ledger and its inclusion can be conﬁrmed by any (honest) participant.
In contrast,
user-perceived latency measures the delay between the time that a user sends a transaction, tx, to
the network until the time that tx can be conﬁrmed by any (honest) node in the system.
Figure 5 (right) shows both latency values measured for various network sizes. While the client-
perceived latency is roughly 8 times more than the conﬁrmation latency, both latencies remain about
the same for networks larger than 1,000 nodes. In comparison, Elastico [47] and OmniLedger [42]
report conﬁrmation latencies of roughly 800 seconds and 63 seconds for network sizes of 1,600 and
1,800 nodes respectively.
Reconﬁguration Latency. Figure 6 (left) shows the latency overhead of epoch reconﬁguration
which, similar to [42], happens once a day. We measure this latency in three diﬀerent scenarios,
where 1, 5, or 10 nodes join RapidChain for various network sizes and variable committee sizes
(as in Figure 5 (left)). The reconﬁguration latency measured in Figure 6 (left) includes the delay
of three tasks that have to be done sequentially during any reconﬁguration event: (1) Generating
epoch randomness by the reference committee; (2) Consensus on a new conﬁguration block proposed
by the reference committee; and (3) Assigning new nodes to existing committee and redistributing
a certain number of the existing members in the aﬀected committees. For example, in our target
network of 4, 000 nodes, out of roughly 372 seconds for the event, the ﬁrst task takes about 4.2
seconds (1%), the second task takes 71 seconds (19%), and the third task takes 297 seconds (80%).
For other network sizes, roughly the same percentages are measured.
As shown in Figure 6 (left), the reconﬁguration latency increases roughly 1.5 times if 10 nodes
join the system rather than one node. This is because when more nodes join the system, more nodes
are cuckooed (i.e., redistributed) among other committees consequently. Since the churn of diﬀerent
nodes in diﬀerent committees happen in parallel, the latency does not increase signiﬁcantly with
more joins. Moreover, the network size impacts the reconﬁguration latency only slightly because
churn mostly aﬀects the committees involved in the reconﬁguration process. In contrast, Elastico [47]
cannot handle churn in an incremental manner and requires re-initialization of all committees. For a
network of 1,800 nodes, epoch transition in OmniLedger [42] takes more than 1,000 seconds while it
takes less than 380 second for RapidChain. In practice, OmniLegder’s epoch transition takes more
than 3 hours since the distributed random generation protocol used has to be repeated at least 10
times to succeed with high probability. Finally, it is unclear how this latency will be aﬀected by the
number of nodes joining (and hence redistributing node between committees) in OmniLedger.
Impact of Cross-Shard Batching. One of the important features of RapidChain is that it allows
batching cross-shard veriﬁcation requests in order to limit the amount of inter-committee communi-
cations to verify transactions. This is especially crucial when the number of shards is large because,
as we show in Section 6.7, in our target network size of 4,000 nodes with 16 committees, roughly
99.98% of all transactions are expected to be cross-shard, meaning that at least one of every trans-
action’s input UTXOs is expected to be located in a shard other than the one that will store the
transaction itself. Since transactions are assigned to committees based on their randomly-generated
IDs, transactions are expected to be distributed uniformly among committees. As a result, the size
of a batch of cross-shard transactions for each committee for processing every block of size 2 MB
23
Figure 5: Throughput scalability of RapidChain (left); Transaction latency (right)
is expected to be equal to 2 MB/16 = 128 KB. Figure 6 (right) shows the impact of batching
cross-shard veriﬁcations on the throughput of RapidChain for various network sizes.
Storage Overhead. We measure the amount of data stored by each node after 1,250 blocks (about
5 million transactions) are processed by RapidChain. To compare with previous work, we estimate
the storage required by each node in Elastico and OmniLedger based on their reported throughput
and number of shards for similar network sizes as shown in Table 2.
Protocol
Elastico [47]
Network
Size
1,600 nodes
OmniLedger [42]
RapidChain
RapidChain
1,800 nodes
1,800 nodes
4,000 nodes
Storage
(esti-
2,400 MB
mated)
750 MB (estimated)
267 MB
154 MB
Table 2: Storage required per node after processing 5 M transactions without ledger pruning
Overhead of Bootstrapping. We measure the overheads of our bootstrapping protocol to setup
committees for the ﬁrst time in two diﬀerent experiments with 500 and 4,000 nodes. The measured
latencies are 2.7 hours and 18.5 hours for each experiment respectively. Each participant in these
24
15482423332346765127603069337384024681012141618010002000300040005000600070008000500[145]1000[175]1500[190]2000[200]2500[225]3000[225]3500[230]4000[250]Number of ShardsTransactions per SecondNumber of Nodes[Committee Size]ThroughputNumber of Shards32.267.969.169.870.070.470.670.78.048.498.648.728.758.808.838.847.88.18.48.79.09.39.69.9010203040506070805001000150020002500300035004000Confirmation Latency (sec)User-Perceived Latency (sec)Number of NodesUser-Perceived LatencyConfirmation LatencyFigure 6: Reconﬁguration latency when 1, 5, or 10 nodes join each committee (left); Impact of batching
cross-shard veriﬁcations (right)
two experiments consumes a bandwidth of roughly 29.8 GB and 86.5 GB respectively. Although
these latency and bandwidth overheads are substantial, we note that the bootstrapping protocol
is executed only once, and therefore, its overhead can be amortized over several epochs. Elastico
and OmniLedger assume a trusted setup for generating an initial randomness, and therefore, do not
report any measurements for such a setup.
6 Security and Performance Analysis
6.1 Epoch Security
We use the hypergeometric distribution to calculate the failure probability of each epoch. The
cumulative hypergeometric distribution function allows us to calculate the probability of obtaining
no less than x corrupt nodes when randomly selecting a committee of m nodes without replacement
from a population of n nodes containing at most t corrupt nodes. Let X denote the random variable
corresponding to the number of corrupt nodes in the sampled committee. The failure probability
for one committee is at most
Pr(cid:2)X ≥ (cid:98)m/2(cid:99)(cid:3) =
m
x =(cid:98)m/2(cid:99)
(cid:1)
.
(cid:0)t
x