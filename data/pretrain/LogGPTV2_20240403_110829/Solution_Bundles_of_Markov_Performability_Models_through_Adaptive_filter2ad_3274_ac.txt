Otherwise, if the parameter θj takes values in the interval
[lb, ub] a natural choice may be to consider discretizing
ub + lb
ub − lb
(ij )
θ
j
2
2
+
:=
cos
, ij = 1, . . . , nj,
(7)
which are often called Chebyshev points of [lb, ub]. Indeed,
the representation in the Chebyshev basis of the polynomial
interpolant on these points can be found efﬁciently using a
discrete cosine transform in O(nj log nj).
Classical results on the Lebesgue constant for Chebsyhev
interpolation of a continuous function [lb, ub] → R guarantee
that this result is almost accurate as the best polynomial
approximation on the interval, up to a factor that grows as
log(nj). In addition, if the function has higher regularity (as it is
in this case assuming that Q depends smoothly on θ1, . . . , θp,
as in Section II-C) a very quick convergence rate can be
guaranteed. For further details, the reader is referred to [20].
In practice, the choice of points will have little to no inﬂuence
on the proposed solution method (and this will become clear
in Section IV-D). From now on, it is assumed that this choice
has been made once and for all.
(cid:8)
(cid:7)
π(ij − 1)
nj − 1
IV. PROPOSED METHOD
This section describes the numerical method used to con-
struct a compressed representation of all the evaluations of the
measure on a (p + 1)-dimensional lattice.
More precisely, from now on it is assumed that the discretiza-
tions of the time variable t and the parameters θ1, . . . , θp are
chosen a priori as
(1)
t
(2)
 0.
h=1
and therefore restating the problem as solving k linear systems
with shifted copies of Q. Since Q is an M-matrix with
eigenvalues with negative real part, it is natural to choose
an approximant ξ(z) that guarantees |ξ(z) − ez| ≤  for any
z ∈ R−, the negative real semiaxis, for a properly chosen . If
the eigenvalues of Q are sufﬁciently close to the real axis, a
k(cid:4)
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:18:14 UTC from IEEE Xplore.  Restrictions apply. 
385
low error  can be rigorously guaranteed through the use of
Bernstein ellipses (see [20] for the details, whose discussion
is beyond the scope of the manuscript).
An appropriate choice for the approximant ξ, i.e., a choice of
αh ∈ R and zh ∈ C for h = 1, . . . , k, is given by the rational
function obtained using Gonchar poles [21], which guarantee
−k (the development of such
a convergence with rate  (cid:2) 9.28
estimates has been long and difﬁcult, and is well summarized in
the review paper about the 1/9-conjecture [21]). Precomputed
versions of such poles through the Remez algorithm are used
in this work for 1 ≤ k ≤ 16, which allow to reach the same
accuracy of double precision ﬂoating point arithmetic.
) requires
the solution of n1 · k linear systems, which has the sought
asymptotic complexity, but may have a non-negligible cost in
practice. In Section IV-D we discuss how to further optimize
this step.
C. Low-rank structure and adaptive cross approximation
(i1)
Obtaining the evaluations m(t(i0), θ
1
, . . . , θ
(ip)
p
It is now essential to note that the entries Mi0,...,ip of the
M tensor, deﬁned in Equation (5), are the evaluations of a
smooth (analytic) function over a (p + 1)-dimensional lattice.
This fact can be used to prove that M is well-approximated
by a low-rank tensor, i.e.,
M ≈ ˆM :=
(8)
where ⊗ denotes the tensor product [18], [22] and ˆMh,j ∈ Rnj
corresponds to the evaluation of the j-th ﬁber of M.
ˆMh,0 ⊗ . . . ⊗ ˆMh,p
k(cid:4)
pivoth
h=1
,
It is useful to restrict the attention to the special case p = 1
ﬁrst, as in the running example shown in Figure 3, where
Equation (8) is just a low-rank factorization of the matrix
M. In this case, several techniques are available to obtain an
accurate factorization as in Equation (8) effectively (to name a
few, roughly sorted from the more expensive to the cheapest:
the SVD [23], QR factorizations with pivoting [24], randomized
sketching [25], Golub-Kahan-Lanczos bidiagonalization [23]).
In general, it is convenient to consider a particular technique,
that allows to obtain a factorization as in
named ACA,
Equation (8) by evaluating O(k) rows and columns of the
matrix [26]. The procedure can be interpreted as a partial
Lower Upper Factorization (LU) [23], and its convergence
properties derived through this factorization.
In a nutshell, the ACA works as follows, for a sequence of
steps h = 1, 2, . . . , k:
h.1) A sufﬁciently large (in modulus) entry in M is chosen,
and is ﬁxed as the current pivot.
h.2) The row ˆMh,r and the column ˆMh,c containing the pivot
are used to construct the unique rank-1 approximation
−1 · ˆMh,r of M that coincides with
ˆMh := ˆMh,c · pivot
the computed ﬁbers on the selected row and column5.
h.3) M is (implicitly) replaced with M − ˆMh, and the
procedure is continue as long as M is large enough.
5Note that, up to reshaping the rank-1 term, this is exactly ˆMh,0 ⊗ ˆMh,1
described in (8). ˆMh actually corresponds to fh(t)· gh,1(θ1) of Equation (6).
h.2)
h.3)
−1
−1
≈
≈
= ˆM1
= ˆM2
(9)
−1
≈
−1
+
= ˆM1 + ˆM2
Fig. 4. The above picture represents two steps of ACA for p = 1. In h.2), the
ﬁrst pivot (the black square) is selected, and used in the rank-1 approximation
in the right hand side. In h.3), the residual M − ˆ
M1 is considered, which
has a zero column and row; another pivot is chosen, and the two rank-1
contributions are combined in (9).
At the end, one has accumulated k rank-1 approximations
ˆM1, . . . , ˆMk such that
M ≈ ˆM1 + ··· + ˆMk
(9)
which can be expanded in tensor products as in (8). The
procedure is summarized pictorially in Figure 4. For the running
example shown in Figure 3, the columns of M comprise
measure evaluations where λ is ﬁxed and time varies, and
the rows those where time is ﬁxed and λ varies.
The difﬁcult part is choosing the pivot at item h.1) in a
way that produces a stable algorithm. For instance, choosing
the largest modulus entry in the current ﬁber as pivot makes
the algorithm equivalent to LU with partial pivoting [18], and
hence very stable in practice.
Generalizing this idea to more than two variables is difﬁcult
in general, since the theory is much weaker. In particular,
the connection with the LU factorization with partial pivoting
cannot be claimed any more, and the low-rank approximation
problem for tensors is much more challenging than the one for
matrices (and this is the reason that sparked the creation of
several alternative low-rank deﬁnition and formats for tensors
[27]). Nevertheless, a tensor analogue of the ACA procedure
can still be formulated, as follows:
• A large enough pivot is determined in the tensor (the
original one, or the current residual of the approximation)
• All the ﬁbers6 that contain the pivot are computed, and
the unique rank-1 tensor ˆM1 that coincides with M on
these ﬁbers is determined; M is replaced by M − ˆM1,
and the procedure is iterated.
Clearly, the “difﬁcult” part is again the ﬁrst item; two strategies
have been experimented in this work:
1) A ﬁxed number of entries is sampled from the tensor, and
the element of maximum modulus is chosen as pivot.
2) Given the current ﬁber, the maximum along it is chosen
as ﬁrst pivot estimate. Then, another ﬁber through that
pivot is chosen and the new candidate pivot is set as the
maximum along the new ﬁber; the procedure is repeated
a ﬁxed number of times.
6The ﬁbers for tensor play the role of rows and columns for matrices.
386
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:18:14 UTC from IEEE Xplore.  Restrictions apply. 
In the tests, procedure 2) has shown to be quite effective,
and always converged in the considered tests. We remark that
the underlying theory in this case is much weaker, and we
currently expect that the proposed method may need to be
adjusted in particular cases for p ≥ 2. A possible modiﬁcation
is to restructure the method in a recursive way, where the
2(cid:9), and
tensor M is seen as n0 . . . nl × nl+1 . . . np with l = (cid:8) p
the rows and columns in the ACA for this matrix are again
approximated with an l-dimensional recursive ACA, with a
base case at p ≤ 2. This approach, described in [9], would be
more expensive, but also more reliable. Nevertheless, the need
for this more reﬁned algorithm has never been encountered in
the case studies considered in Section V.
D. Effective computation of parameters ﬁber
The use of a rational approximant to compute the entries of
the j-th ﬁber still requires solving k · nj linear systems with
shifted versions of Q. This can easily become the bottleneck
in the computation. Hence, we have modiﬁed the procedure to
take advantage of the fact that these points are evaluations of
a smooth function of a single parameter θj, as follows:
1) An interval [lb, ub] enclosing all values of the parameter
is determined.
2) The measure is evaluated at the Chebyshev points, deﬁned
as in Equation (7) of degree k (instead of degree nj as
used in Equation (7)).
3) The interpolant Chebyshev polynomial of degree k − 1 is
determined from these evaluations using a discrete cosine
transform [20].
4) The measure is evaluated at new points, by setting k :=
2k − 1 (this only requires k − 1 new evaluations, since
the points are nested). If the previous approximant was
accurate enough, the procedure is stopped, otherwise it is
started again from item 3).
At the end of the above procedure, one has a high-quality
approximant to the 1D function under consideration, which
can be used to evaluate the function at all the required points.
The above algorithm makes the cost of the ﬁber evaluation
almost independent of the number of points under consideration;
the only part that depends on nj is the ﬁnal evaluation of the
Chebyshev series. The number of solutions of linear systems
with matrices (zhI + tQ), which are the dominating part in
the cost, only depends on the smoothness of the evaluated
function.
E. Evaluating accumulated measures
The algorithm described can be adapted to evaluate m(t, θ)
when it is an accumulated measure, exploiting
(cid:9) t
τ Q
π0e
0
r dτ = tπ0ϕ1(tQ)r = dot(b, r),
where
ϕ1(z) :=
ez − 1
z
, z ∈ C,
and noticing that the upper extreme of integration is t. The
solution b(t) of Equation (2) can then be expressed as b(t) =
tπ0ϕ1(tQ). Using the rational approximation of ez constructed
from Gonchar poles one may derive a rational approximation
for ϕ1(z) by computing
ˆξ(z) :=
ξ(z) − 1
z
k(cid:4)
=
h=1
−1
αhz
h
zh + z
.
This can be used in the analogous scheme to transform the
evaluation of the ﬁbers of M into a solution of a sequence of
linear systems with shifted versions of Q, whose number is
greatly reduced by exploiting the strategy of Section IV-D.
It can be noted that the accuracy attainable with this new
approximation may be lower than the one for the matrix
exponential, since one can only guarantee that
|zϕ1(z) − z ˆξ(z)| ≤ ,
which is a much looser bound for |z| (cid:10) 1. To overcome this,
the truncation threshold  may be chosen as a smaller value.
Alternatively, an optimal Remez approximation may be found