the conversion in this paper.  
Figure 2: Detectable points. 
4.2  Location-Independent Human-Assisted 
Memorability 
To facilitate labeling detectable points, we have implemented a tool 
to display detectable points on an image, with different categories 
marked  with  different  colors.  Detectable  points  are  all  initially 
assigned with M-index = 6. A user can move or adjust a circle to 
select detectable points inside the circle to label or adjust their M-
indices according to the IPM model. We add a new category called 
textual  points  for  detectable  points  that  are  a  part  of  texture.  A 
texture  is  a  region  comprising  similar  and  repetitive  structures, 
possibly  with  a  certain  degree  of  randomness.  Both  cross-points 
and  rectangle’s  centers  shown  in  Figure  1(C)  are  textual  points. 
Textural points are unmemorable and thus removed.  
Salient objects are first identified as follows: looking at the image 
far  away  such  that  image  details  are  lost;  objects  that  are  still 
identifiable are considered as salient objects. Once salient objects 
are  identified,  the  M-index  of  a  point  on  a  salient  object  is  then 
reduced by 2, the weight for a salient object. Then HSC and MSC 
objects  are  identified.  The  M-index  of  a  point  on  an  identified 
object is reduced by the weight of the object, i.e., 2 or 1 for a HSC 
or MSC object. Finally, HSC and MSC points are identified, and 
their  M-indices  are  reduced  in  a  similar  manner.  The  tool  also 
allows a user to assign a point’s M-index directly. Unfinished work 
can be saved and then resumed at a later time. Pressing “Submit” 
button ends labeling of an image. 
We collected 1200 images in our empirical study of PCCP security 
(see Section 7). Three computer science undergraduates were hired 
to label detectable points in these images. They had not used any 
click-based  graphical  password  before,  and  were  otherwise  not 
involved in this work. They were trained with exemplary samples 
of each category in advance. The task of labeling the 1200 images 
was  divided  among  the  three  people.  Each  image  was  labeled  by 
one  person.  The  work  was  done in  10 days.  The  average  time  to 
label an image was 3 minutes and 32 seconds. 
There was no cross-validation in this labeling process, which might 
result in some inconsistent or inaccurate labels. However, it turns 
out  that  the  labelling  quality  we  achieved  was  sufficient  for  our 
studies, as evidenced by experimental results we present later.  
4.3  Human-Assisted Memorability 
Salience  is  location-dependent:  humans  naturally  look  at  objects 
near the center of an image [27]. This behavior is accounted for in 
[27]  with  a  Gaussian  blob  centered  at  the  image  center,  which  is 
also applied to adjust human labelling results obtained in Section 
4.2.  The  following  empirical  rule  is  applied:  human  labeled  M-
index is adjusted by subtracting the Gaussian blob normalized to an 
output range of [0, 1]. A detectable point at the image center is thus 
12215.2  Location-Independent Automated 
Memorability  
The automated memorability described above depends on a point’s 
location  since  salience  is  location-dependent.  However,  we  can 
eliminate this dependence simply by removing the center feature, 
the only location-dependent feature in the Judd et al.’s model [27]. 
The resulting M-index is the sought one. 
5.3  Comparison with Prior Art 
Prior  art [13,16-18]  has  conceptually  used  the  first  two  modules, 
but is inadequate for predicting memorability of a point due to lack 
of memory decay  mechanism  fulfilled in the third module, as we 
mentioned  in  Section  3.1.  This  inadequacy  also  manifests  in  the 
following real example. Figure 3 shows an image and its salience 
map, wherein a more salient area is brighter. We can see that leaves 
have  rather  large  salience  values.  As  Figure  2  shows,  many 
detectable points can be found in leaves. Their M-indices given by 
Eq.  (1)  without  using  any  dissimilarity  factor  will  be  small, 
meaning  these  detectable  points  are  determined  memorable. 
Actually they are textural points and thus unmemorable. The prior 
art  is  significantly  inaccurate  in  evaluating  the  memorability  of 
points in textual regions.  
Figure 3: Image (left) and its salience map (right). 
In  contrast,  by  incorporating  color  and  structural  dissimilarity 
factors into Eq. (1) to model the memory decay mechanism in our 
IPM  model,  an  M-index  derived  by  the  attention  model  is 
significantly  boosted  (i.e.,  much  less  memorable)  for  a  point  in 
leaves  due  to  very  low  dissimilarity  for  both  color  and  gradient 
distributions in leaves. Even though detectable points are still found 
in leaves, they are ranked among least memorable points (i.e., with 
largest  M-indices),  and  thus  are  likely  excluded  from  our 
dictionaries, which is what we seek for.  
When a point’s neighborhood is very dissimilar, the dissimilarity 
factors in Eq. (1) also decrease its M-index returned by the attention 
model (meaning the point is more memorable). In this way, relative 
memorability levels of detectable points are reordered by both color 
and structural dissimilarities, resulting in better and more accurate 
dictionaries than before.  
In addition to our conceptual innovation of introducing a memory 
decay  mechanism,  we  have  developed  a  new  corner  detection 
algorithm  that  outperforms  prior  art,  and  have  applied  a  better 
attention model than the one used in prior art. 
Our approach offers the first automatic approximation to the IPM 
model,  and  distances  itself  from  prior  art,  both  conceptually  and 
technically.  We  note  that  this  is  not  necessarily  the  optimal 
automatic  approximation.  We  encourage  computer  vision 
researchers to search for better techniques. 
6.  A DEFENSIVE APPLICATION: 
GRAPHICAL HONEYWORDS 
Honeywords  were  recently  introduced  to  improve  the  security  of 
hashed  text  passwords  [20].  For  each  account,  one  or  more 
honeywords  (false  passwords)  are  cryptographically  hashed  and 
stored together with the real password hash. A legitimate user never 
uses  the  honeywords  associated  with  her  account.  An  adversary 
steals a file of hashed passwords, and can invert the hash function. 
If  the  adversary  cannot  tell  passwords  and  honeywords  apart,  he 
may  use  a  honeyword  in  a  login  attempt,  which  will  sets  off  an 
alarm of password compromises. Honeywords are also applicable 
to systems that do not store password hashes, but store encrypted 
or plain passwords instead.  
Graphical  passwords  are  stored  in  a  similar  manner  as  text 
passwords.  For  click-based  graphical  passwords,  a  password 
breach  may  allow  adversaries  to  access  or  deduce  either  a 
password’s  click-points  or  their  tolerance  squares,  depending  on 
whether password discretization has been used or not, and on which 
discretization scheme has been used. In the latter case, a tolerance 
square, which is very small, reflects the characteristics of the points 
inside.  Graphical  honeywords  would  serve  exactly  the  same 
purpose  for  graphical  passwords  as  text  honeywords  for  text 
passwords. 
The  IPM  model  can  be  applied  to  generate  honeywords  for  any 
click-based  graphical  password  scheme.  We  chose  PassPoints  in 
our empirical study for two reasons:  
• 
It  is  more  challenging  to  design  a  honeyword  scheme  for 
PassPoints  than  for  either  CCP  or  PCCP,  since  the  former 
exhibits click patterns, which the latter lacks but that can be 
exploited by  adversaries  to  tell  passwords  and honeywords 
apart.  
•  To reach the same discriminative power, PassPoints requires 
a much smaller training set than CCP or PCCP does.  
Thus, PassPoints facilitates a good feasibility study of our approach 
without loss of generality. 
6.1  Honeywords: Desired Properties  
To  be  effective  and  practical,  a  honeyword  generation  system 
should have the following desired properties: 
1.  Efficiency.  Internet  applications  may  have  many  accounts 
and  thus  need  to  generate  a  large  number  of  honeywords 
efficiently.  
2.  Indistinguishability. 
should 
Honeywords 
be 
indistinguishable  from  passwords  to  prevent  adversaries 
from  telling  them  apart.  This  indistinguishability  manifests 
in several ways: 
a.  Honeywords 
semantic 
characteristics as passwords. For PassPoints, this means 
that honeywords should have a tendency to use hotspots 
and  exhibit  click  patterns  that  PassPoints  passwords 
exhibit.  
should  have 
the 
same 
b.  Passwords may exhibit certain variations due to human 
imprecision. For example, a hotspot is likely chosen as a 
click-point  for PassPoints, but  different  passwords  may 
contain  a  slightly  different  point  due  to  human  click 
variations. For a system that click-points can be deduced 
in  password  breaches,  honeywords  should  also  exhibit 
the same variations.  
c.  Statistically 
indistinguishable.  Honeywords 
should 
exhibit a probability distribution indistinguishable from 
that  of  passwords  to  avoid  being  distinguished  by 
statistical  analysis.  For  example,  PassPoints  passwords 
1222containing  hotspots  and  click  patterns  are  more  likely 
chosen  by  users  than  other  passwords.  Honeywords 
should also exhibit this skewed probability distribution.  
Quality of generated honeywords can be evaluated by comparing 
with the ideal case that honeywords are truly indistinguishable from 
passwords. In this ideal case, the best one can do is a random guess, 
resulting  in  a  success  probability  of 1/((cid:24) + 1),  where (cid:24)  is  the 
number of honeywords per password. This probability can be made 
arbitrarily  small  by  selecting (cid:24)  appropriately  but  at  the  cost  of 
complexity and storage space. Without loss of generality, (cid:24) was set 
to 1 in our empirical studies.  
6.2  Generating Honeywords for PassPoints  
6.2.1  Overview  
In our honeyword generation, Property 2.a is achieved by utilizing 
the  IPM  model  (Section  6.2.2)  to  select  points  and  to  rank-order 
honeyword candidates, referred to as words, and by adjusting the 
rank  order  with  click  patterns  (Section  6.2.3)  so  that  generated 
honeywords  exhibit  both  hotspots  and  click  patterns.  To  fulfill 
Property  2.c,  each  word  is  assigned  a  drawing  probability 
according to which words are drawn (i.e., selected) in generating 
honeywords,  and  the  drawing  probability  distribution  is  made 
similar  to  that  of  real  passwords  (Section  6.2.4)  to  avoid 
distinguishing  honeywords  and  passwords  statistically.  We  raise 
the  challenge  we  face  by  requiring  our  honeyword  generation  to 
fulfill Property 2.b in order to be applicable regardless of password 
discretization  methods.  To  achieve  this,  each  drawn  word  is 
perturbed (Section 6.2.5) before outputting as a honeyword. 
the  automated  memorability  or 
Either 
the  human-assisted 
memorability can be used as an approximation of the IPM model 
for generating honeywords. Either resulting system can generate a 
large  number  of  honeywords  efficiently.  Thus  Property  1  is 
fulfilled. Note that human labeling is needed for the human-assisted 
memorability, but such labeling is a one-shot, manageable effort. 
For  example,  it  took  less  than  7 minutes  to  label  the  two  images 
shown in Figure 4 that were used in our case study. After labeling, 
a large number of honeywords can be generated automatically and 
efficiently.  We  also  note  that  honeyword  generation  with  the 
human-assisted  memorability 
is  significantly  different  from 
generating honeywords with humans. It is a substantially easier task 
for  a  user  to  rank  clickable  image  points  by  their  memorability 
levels  than  to  create  a  large  number  of  distinct  passwords  as 
honeywords.  More 
importantly,  manually  creating  many 
honeywords is exhausting for a user, which deviates from people’s 
normal password-creating behaviors. Honeywords created this way 
would  highly  likely  have  a  distinct  probability  distribution  from 
real  passwords,  resulting  in  honeywords  distinguishable  from 
passwords. 
6.2.2  Building a Dictionary with IPM Model 
Detectable points are found from an image, and their M-indices are 
calculated  using  the  IPM  model  to  form  a  set  of  distinguishable 
points. For a simple image, this set might be too small. In this case, 
indistinguishable points of low M-indices (i.e., more  memorable) 
are  added  to  the  set  so  that  the  set  is  large  enough  in  order  to 
generate a large variation of honeywords. On the other hand, when 
a simple image is used, security-conscious users may be forced to 
choose less memorable points in creating their passwords in order 
to avoid hotspots. 
The  points  in  the  set  are  then  traversed  to  form  a  dictionary  of 
words;  each  word  is  a  sequence  of  5  distinct  points  with  the 
minimum  distance  between  any  pair  of  points  exceeding  a 
threshold  required  by  PassPoints.  Like  in  attacking  PCCP  to  be 
described in Section 7.1, each word is assigned an M-index that is 
the sum of the M-indices of its constituent points.  
6.2.3  Adjustment by Click Patterns 
Since  PassPoints  passwords  exhibit  click  patterns,  so  should 
honeywords. Two heuristic click patterns are considered: Line and 
Regular. Line includes any sequence of 5 click-points that follow a 
directional  line.  Regular  includes  any  sequence  of  5  click-points 
that follow consistent directions such as left to right, clock-wise.  
Patterns are used to adjust M-indices of the words in the dictionary 
so  that  words  exhibiting  a  pattern  have  a  higher  chance  to  be 
selected as honeywords, i.e., their M-indices are lowered. Like in 
Sections  5  and  6,  this  adjustment  is  achieved  by  multiplying  a 
fractional  factor  for 
the  automated  memorability,  and  by 
subtracting a subtrahend for the human-assisted memorability. As 
described in detail in Appendix 11.4, this factor or subtrahend is a 
confidence level ranging from 0 to 1, which is an empirical metric 
to measure evaluation reliability that the word exhibits the pattern. 
For each word in the dictionary, Line is checked first. Regular is 
checked if the word does not exhibit the Line pattern. If an entry 
exhibits neither pattern, its M-index remains unchanged.  
6.2.4  Approximating Password Distribution 
The drawing probability assigned to each word should preserve the 
word order by their M-indices. In addition, the drawing probability 
distribution  should  be  similar  to  the  probability  distribution  of 
passwords so that honeywords cannot be distinguished statistically 
from  passwords.  Since  no  probability  distribution  of  PassPoints 
passwords is available, we approximate it with the distribution of 
attacking PassPoints, shown in Figure 6 in Section 7.5: the drawing 
probability of a word with M-index (cid:27) is calculated empirically as 
follows: 
(cid:28)((cid:27)) =  (cid:29) ∙ exp (−" ∙ #$%&’
%(cid:20)($%&’),        (2) 
where )*+ and ),- are the minimum and maximum M-index of 
the  words  in  the  dictionary,  respectively, (cid:29)  is  a  normalization 
factor,  and  " ≥ 0  is  a  parameter.  Eq.  (2)  is  a  monotonically 
decreasing function of M-index, thus preserves the M-index order 
of words. 