# Measuring Bandwidth Between PlanetLab Nodes

**Authors:**
- Sung-Ju Lee
- Puneet Sharma
- Sujata Banerjee
- Sujoy Basu
- Rodrigo Fonseca

**Affiliations:**
- **Sung-Ju Lee, Puneet Sharma, Sujata Banerjee, Sujoy Basu:** Hewlett-Packard Laboratories, Palo Alto, CA
  - Email: {sjlee, puneet, sujata, basus}@hpl.hp.com
- **Rodrigo Fonseca:** University of California, Berkeley, CA
  - Email: [PI:EMAIL]

## Abstract
In the absence of end-to-end Quality of Service (QoS) guarantees in existing networks, applications requiring specific performance levels often rely on periodic network path measurements. Key metrics include latency, bandwidth, and loss rates. While latency has been extensively studied, bandwidth measurement has received less attention. This paper presents a comprehensive analysis of bandwidth measurements between PlanetLab nodes, highlighting various trends and insights. We evaluate several existing bandwidth measurement tools, discussing the challenges in selecting and using them on the PlanetLab platform.

## 1. Introduction
The lack of end-to-end QoS and multicast support in the best-effort networking infrastructure has led to the development of application-level intermediaries such as web caches and service replicas to address performance issues. It is crucial not only to provide these intermediary services but also to connect end-clients to the intermediary that can meet their QoS requirements and offer the best performance. For example, web applications might select the nearest content cache, while online multiplayer game players might choose the least loaded game server. Additionally, there have been efforts to build overlays by connecting application-level intermediaries for composable and personalized web and media services. These services often require QoS metrics like bandwidth and delay, necessitating periodic or on-demand end-to-end path measurements.

Significant research has been conducted on active and passive network measurement techniques, particularly in large-scale networks. However, measuring all possible network paths periodically or on-demand is inefficient and incurs high overhead. Therefore, developing scalable measurement and inference techniques with minimal probing is essential.

Key network metrics of interest include end-to-end latency, bandwidth, and loss rates, while application-level metrics include HTTP response times and media streaming rates. Many studies have focused on scalable network distance estimation, primarily using the triangular inequality heuristic. However, this heuristic does not apply to bandwidth, making it challenging to identify nodes providing maximal bandwidth without extensive probing.

Although numerous bandwidth measurement techniques exist, few large-scale studies have been conducted. Most studies are associated with validating new measurement tools. In this paper, we present results from a large-scale bandwidth measurement study on the PlanetLab infrastructure. Our objectives are to understand the bandwidth characteristics of network paths connecting PlanetLab nodes and to gain insights into potential trends for scalable bandwidth estimation. We focus on the first goal in this paper, assessing the capabilities of available tools and reporting our findings to assist other researchers in making informed tool choices.

## 2. Measurement Study
PlanetLab is an ideal platform for bandwidth measurement due to its open, globally distributed nature, with hundreds of nodes spanning over 25 countries. Although the interdomain connectivity of PlanetLab hosts may not fully represent the global Internet, understanding PlanetLab topology is crucial for designing experiments and drawing informed conclusions. Several studies have focused on PlanetLab's connectivity and inter-node latency. In this paper, we examine the bottleneck capacity between PlanetLab nodes.

### 2.1 Methodology
Our methodology involved deploying a selected bandwidth measurement tool on responsive PlanetLab nodes using standard tools, executing a script to run the measurements, and then shipping the collected data to a central node for analysis.

We conducted two sets of measurements at different times:
- **Set 1:** August 2004
- **Set 2:** January 2005 (after PlanetLab version 3 rollout)

Despite over 500 deployed nodes, only about half consistently responded. The first step was to select a suitable tool, which we describe below. Pairwise latency measurements for a few hundred nodes can be completed quickly, but pairwise capacity measurements require careful coordination due to cross-traffic detection, resulting in a longer process.

We evaluated several bandwidth measurement/estimation tools, aiming to find one that is reasonably accurate, low-overhead, and easily deployable on PlanetLab. Our evaluation included:
- **Per-hop capacity estimation tools:** Excessive probing traffic.
- **Pathchar and pchar:** Incompatible with newer Linux systems.
- **Clink:** Compatibility issues with PlanetLab's kernel version.
- **End-to-end capacity estimation tools:**
  - **bprobe:** Only runs on SGI Irix.
  - **SProbe:** Attractive but returns estimates for less than 30% of measurements.
  - **pathrate:** Successfully ran and provided estimates.
  - **Available bandwidth estimation tools:**
    - **cprobe:** Does not run on Linux.
    - **pathload:** Invalid argument error on connect.
    - **IGI:** Poor accuracy and high variance.
    - **Spruce:** Requires knowledge of path capacity.
    - **pathChirp and STAB:** Failed after PlanetLab version 3 rollout.

### 2.2 Measurement Analysis
For the first set of measurements, we used pathrate (version 2.4.0) as it was the only tool that consistently provided reliable estimates. Each pathrate run took approximately 30 minutes. Pathrate provides high and low estimates for bottleneck capacity. In the first experiment, some low capacity estimates were negative, which was corrected in the modified version (v2.4.1b) used in the second set of measurements. We report only the high capacity estimate to avoid calculation errors.

- **Set 1 (August 11, 2004 - September 6, 2004):**
  - **Nodes:** 279
  - **Paths:** 12,006
  - **Minimum Capacity:** 0.1 Mbps
  - **Maximum Capacity:** 1210.1 Mbps
  - **Average Capacity:** 63.44 Mbps
  - **Median Capacity:** 24.5 Mbps
  - **Standard Deviation:** 119.22 Mbps

- **Set 2 (January 5, 2005 - January 18, 2005):**
  - **Nodes:** 178
  - **Paths:** 21,861
  - **Minimum Capacity:** 0.3 Mbps
  - **Maximum Capacity:** 682.9 Mbps
  - **Average Capacity:** 64.03 Mbps
  - **Median Capacity:** 91.4 Mbps
  - **Standard Deviation:** 43.78 Mbps

Table 2 shows the distribution of end-to-end capacities, and Figure 1 visualizes this distribution. We observed that certain nodes had very low measured bandwidths. For example, in Set 1, paths with `freedom.ri.uni-tuebingen.de` as the source had an average bandwidth of 4.61 Mbps. In Set 2, `200-102-209-152.paemt7001.t.brasiltelecom.net.br` as the source had an average capacity of 0.42 Mbps, and as the destination, 0.41 Mbps. Conversely, `planetlab1.eurecom.fr` as the destination had an average bandwidth of 3.85 Mbps, with one path from `planetlab1.inria.fr` showing 199.2 Mbps. Without this outlier, the average bandwidth would be 2.13 Mbps. Some nodes, like `planet1.ottawa.canet4.nodes.planetlab.org`, showed high average bandwidths, such as 508.46 Mbps.

## 3. Conclusion
This paper presents a large-scale bandwidth measurement study on the PlanetLab infrastructure, focusing on understanding the bandwidth characteristics of network paths. We assessed the capabilities of several available tools and reported our findings, which can help researchers make informed tool choices for future studies. Future work will involve testing additional tools and further refining our understanding of bandwidth trends on PlanetLab.