title:Measuring Bandwidth Between PlanetLab Nodes
author:Sung-Ju Lee and
Puneet Sharma and
Sujata Banerjee and
Sujoy Basu and
Rodrigo Fonseca
Measuring Bandwidth Between PlanetLab Nodes
Sung-Ju Lee1, Puneet Sharma1, Sujata Banerjee1, Sujoy Basu1, and Rodrigo Fonseca2
1 Hewlett-Packard Laboratories, Palo Alto, CA
{sjlee, puneet, sujata, basus}@hpl.hp.com
2 University of California, Berkeley, CA
PI:EMAIL
Abstract. With the lack of end-to-end QoS guarantees on existing networks, ap-
plications that require certain performance levels resort to periodic measurements
of network paths. Typical metrics of interest are latency, bandwidth and loss rates.
While the latency metric has been the focus of many research studies, the band-
width metric has received comparatively little attention. In this paper, we report
our bandwidth measurements between PlanetLab nodes and analyze various trends
and insights from the data. For this work, we assessed the capabilities of several
existing bandwidth measurement tools and describe the difﬁculties in choosing
suitable tools as well as using them on PlanetLab.
1
Introduction
The lack of end-to-end QoS and multicast support in the underlying best effort network-
ing infrastructure has spurred a trend towards providing application level intermediaries
such as web-caches and service replicas to mitigate the performance issues. It is not
only important to provide intermediary services but also to connect the end-clients to
the intermediary that can meet the client QoS requirements and provide the best perfor-
mance. For instance, web applications might want to select the nearest content cache
while the online multiplayer game players might want to choose the least loaded game
server. There also have been attempts to build overlays by connecting application level
intermediaries for composable and personalized web and media services. Normally, such
services have QoS requirements such as bandwidth and delay. Hence, building and main-
taining such overlays requires periodic or on-demand measurement of end-to-end paths.
Motivated by these trends, there have been signiﬁcant research studies on active and
passive network measurement techniques, and measurement studies from many large
scale networks [1]. Clearly, periodic or on-demand measurement of all possible network
paths will incur a high overhead and is inefﬁcient. Thus a key concern is the development
of scalable measurement and inference techniques which require minimum probing and
yet provide the required measurement accuracy.
The primary network metrics of interest are end-to-end latency, bandwidth and loss
rates while application level metrics are HTTP response times, media streaming rates,
and so on. Many studies have focused on scalable network distance estimation, mainly
using the triangular inequality heuristic [17]. However, similar triangular inequality does
not apply to bandwidth, and hence it is much more difﬁcult to identify the nodes that
provide the maximal bandwidth without probing to each node.
C. Dovrolis (Ed.): PAM 2005, LNCS 3431, pp. 292–305, 2005.
c(cid:1) Springer-Verlag Berlin Heidelberg 2005
Measuring Bandwidth Between PlanetLab Nodes
293
Although there is a plethora of bandwidth measurement techniques, there have been
only a few large-scale bandwidth or bottleneck capacity measurement studies. In fact,
most of these studies are in conjunction with the validation of a new bandwidth mea-
surement tool. New bandwidth measurement tools continue to be developed, aiming
for better accuracy with faster measurements. In this paper, we present results from a
large scale bandwidth measurement study on the PlanetLab infrastructure. Our goals
are (i) to understand the bandwidth characteristics of network paths connecting Plan-
etLab nodes and (ii) to ultimately obtain insights into potential trends that will enable
scalable bandwidth estimation. We primarily focus on the ﬁrst goal in this paper. We do
not develop a new bandwidth estimation tool nor evaluate and compare the accuracy of
various tools. Rather, we assess the capabilities of a number of available tools from a
PlanetLab deployment standpoint and report our ﬁndings in the hope that it will help
other researchers to make an informed choice of a tool.
In the next section, we describe our methodology and the tools we assessed for this
study, followed by an analysis of the data we collected. Section 3 concludes the paper.
2 Measurement Study
PlanetLab is an attractive platform for bandwidth measurement as it is an open, globally
distributed network service platform with hundreds of nodes spanning over 25 countries.
PlanetLab has gained the status of the de facto standard for conducting large scale
Internet experiments. Although the interdomain connectivity of the PlanetLab hosts may
not represent the global Internet [2], the characterization of PlanetLab topology is still
of utmost importance for designing experiments on PlanetLab and drawing informed
conclusions from the results. Several measurement studies have been conducted on
PlanetLab topology, mostly focusing on the connectivity and the inter-node latency. In
this paper, we study the bottleneck capacity between the PlanetLab nodes.
2.1 Methodology
Our methodology consisted of deploying the bandwidth1 measurement tool on a selected
set of responsive nodes on PlanetLab using standard PlanetLab tools and then executing
a script to run the measurements. The collected data is then shipped back to a central
node on our site for analysis.
We performed two sets of measurements at two different time periods. The ﬁrst set
(referred to as Set 1 in rest of the paper) was measured and collected starting in August
of 2004, and the second (referred to as Set 2) in January of 2005. Between the two
measurements periods, PlanetLab went through a major version change. The second set
of experiments were performed after the version 3 rollout on PlanetLab.
Although there are over 500 deployed nodes on PlanetLab, only a little over half the
nodes consistently responded when we started the measurement process. A crucial ﬁrst
step was to select a tool; we describe the selection process below. Conducting pair-wise
latency measurements for a few hundred nodes is a relatively quick process for which
1 We use the terms bandwidth and capacity inter-changeably throughout the paper.
294
S.-J. Lee et al.
measurements can be run in parallel and ﬁnishes in the order of minutes. However,
pair-wise capacity measurements for a few hundred nodes needs to be well coordinated
because the capacity measurement tools often do not give accurate results when cross
trafﬁc is detected. Thus the measurement process for all pairs can take much longer and
is of the order of days to even weeks.
There are a large number of bandwidth measurement/estimation tools available,
with several new tools recently introduced. This in itself is an indication that accurate
bandwidth measurement/estimation remains a hard problem even after many years of
research and there is room for further improvements [9]. For the details of the various
tools and their measurement accuracy, please use our bibliography or available survey
articles [15,18]. For the purposes of our study, our goal was to ﬁnd a reasonably accurate
but low overhead tool that is easily deployable on the PlanetLab platform. Note that the
purpose of this study is not to do an accuracy comparison of these tools. After some
narrowing of the choices, we evaluated the following tools as described below. We merely
present our experiences with different tools in the evaluation process.
We were hesitant to use per-hop capacity estimation tools as they generate excessive
probing trafﬁc overhead. Moreover, we could not build pathchar [10] or pchar [14] as
they can not be built on newer Linux systems. Currently, PlanetLab runs kernel version
2.4.22, but pathchar supports up to 2.0.30 and pchar up to 2.3. When we tested Clink [5]
on PlanetLab, the experiment were simply “hung" without making any progress. We
suspect this is also because of a Linux version compatibility issue.
As for end-to-end capacity estimation tools, bprobe [3] works only on SGI Irix.
SProbe [21, 22] is an attractive tool as it only requires to be run on the source machine,
and hence can measure capacities to hosts where the user does not have account access. In
addition, SProbe is included in the Scriptroute [23] tool that runs as a service on PlanetLab
hosts. One key feature of SProbe is that when it detects cross trafﬁc, instead of making a
poor estimate of the capacity, it does not report any value. When we ran SProbe between
PlanetLab hosts, less than 30% of the measurements returned the capacity estimate. The
authors of the tool had a similar experience on their trials with Napster and Gnutella peers.
As we have access to all the PlanetLab hosts, we can deploy and run pathrate [4]. Unless
the network hosts are down or we could not login to the hosts for various reasons, we
were able to measure capacity between PlanetLab nodes using pathrate. It was the only
capacity estimation tool we could successfully run and obtain estimates on PlanetLab.
We also tested several available bandwidth estimation tools. Similar to bprobe,
cprobe [3] does not run on Linux. One of the most popular tools is pathload [8]. When
we tested pathload on PlanetLab nodes however, we ran into an invalid argument error
on connect. This very issue was also recently brought up in PlanetLab user mailing
list. We were able to run IGI (Initial Gap Increasing) [7] without any run-time errors.
However, the tool showed poor accuracy with high variance in the estimation of the same
pair on sequential attempts, and also reported unusually high estimates (ten times larger
than the estimated capacity by pathrate). Spruce [24] has shown to be more accurate than
pathload and IGI. However, Spruce requires the knowledge of the capacity of the path
to estimate available bandwidth. We also tested pathChirp [19] and it ran successfully
with reasonable accuracy in our ﬁrst set of measurements performed in August 2004.
However, after the version 3 rollout of PlanetLab, pathChirp, along with STAB [20],
Measuring Bandwidth Between PlanetLab Nodes
295
Table 1. End-to-end capacity statistics
Set 1
279
Set 2
178
Number of nodes
Measurement period 8/11/04∼9/6/04 1/5/05∼1/18/05
PlanetLab version
Number of pairs
Minimum capacity
Maximum capacity
Average capacity
Median capacity
Standard deviation
version 3
21,861
0.3 Mbps
682.9 Mbps
64.03 Mbps
91.4 Mbps
43.78 Mbps
version 2
12,006
0.1 Mbps
1210.1 Mbps
63.44 Mbps
24.5 Mbps
119.22 Mbps
developed by the same authors of pathChirp, failed to work on PlanetLab. After a few
chirps, the tool stops running and hangs. We are communicating with the authors of
pathChirp to resolve the issue.
In our future work, we are planning to test tools such as ABwE [16], CapProbe [11],
pathneck [6], and MultiQ [12].
2.2 Measurement Analysis
For the ﬁrst set of measurements, we show the capacity measurements from pathrate
(version 2.4.0) as it was the only capacity estimation tool we were able to successfully
run in a consistent manner. Each pathrate run on average took approximately 30 minutes.
Pathrate returns two estimates, a high estimate and a low estimate, for bottleneck capacity
between a pair of source and destination nodes. In the ﬁrst experiment pathrate returned
negative values for low capacity estimate in certain measurements. When we reported
this to the authors of the pathrate tool, they kindly debugged the calculation error and
the modiﬁed version (v2.4.1b) was used in the second set of measurements. To avoid
this calculation error, we only report the high capacity estimate of the pathrate in this
paper.
The ﬁrst set of measurements was initiated on August 11th, 2004 and completed
on September 6th, 2004. The second set was measured between January 5th, 2005 and
January 18th, 2005. On our ﬁrst attempt in August 2004, we tried measuring capacities
between all PlanetLab nodes of the then nearly 400 nodes. However, many of the nodes
did not respond consistently, and many of the pathrate capacity estimates were not
returned. Ultimately, in the ﬁrst set, we collected bottleneck capacity data on 12,006
network paths from 279 nodes. In the second set of experiments performed in January of
2005, we preﬁltered 178 nodes (and no more than two nodes per site) that consistently
responded. It could be one of the reasons why the experiments were ﬁnished in a shorter
time compared with the ﬁrst set of measurement experiments. In the second set of
measurement we managed to collect data on 21,861 paths.
We ﬁrst look at the statistics of the end-to-end capacity over all paths (source-
destination node pairs) measured. It is important to note that given two nodes A and
B, capacity measurements in both directions, i.e., source destination node pairs (A,B)
296
S.-J. Lee et al.
Table 2. End-to-end capacity distribution
Capacity (C)
Set 1
Set 2
Number of paths Percentage (%) Number of paths Percentage (%)
C < 20 Mbps
20 Mbps ≤ C < 50 Mbps
50 Mbps ≤ C < 80 Mbps
80 Mbps ≤ C < 120 Mbps
120 Mbps ≤ C < 200 Mbps
200 Mbps ≤ C < 500 Mbps
500 Mbps ≤ C
4013
4246
674
2193
207
392
281
33.42
35.37
5.61
18.27
1.72
3.27
2.34
6733
1910
1303
11744
139
21
11
30.8
8.74
5.96
53.72
0.64
0.096
0.05
1400
1200
1000
800
600
400
200
y
t
i
c
a
p
a
C
0
0
2000
4000
6000
8000
Node Pair ID
700
600
500
400
300
200
100
y
t
i
c
a
p
a
C
10000
12000
14000
0
0
0.5
1
1.5
Node Pair ID
2
2.5
x 104
(a) Set 1.
(b) Set 2.
Fig. 1. Bandwidth capacity for all pairs measured
and (B,A) may not both be available. Table 1 shows that the average bandwidth between
PlanetLab hosts is nearly 64 Mbps. Table 2 shows the distribution, Figure 1 visualizes this
distribution (notice the different scaling of y-axis between two subﬁgures) and Figure 2
shows the cumulative distribution function (notice the different scaling of x-axis between
two subﬁgures). On further analysis, we observed that when certain nodes were the source
or the destination, the bandwidth measured was very low. In the ﬁrst set of measure-
ments for instance, for paths with freedom.ri.uni-tuebingen.de as the source, the average
bandwidth was 4.61 Mbps. We noticed a similar behavior in the second set as when 200-
102-209-152.paemt7001.t.brasiltelecom.net.br was the source, the average capacity was
0.42 Mbps and when it was the destination, the average capacity was 0.41 Mbps. On the
other hand, when planetlab1.eurecom.fr was the destination, the average bandwidth was
3.85 Mbps, with the path from planetlab1.inria.fr having 199.2 Mbps of bandwidth. With-
out this measurement of 199.2 Mbps, the average bandwidth with planetlab1.eurecom.fr
as the destination is 2.13 Mbps. We also noticed nodes with high average bandwidth.
For instance, measurements from planet1.ottawa.canet4.nodes.planetlab.org showed an
average of 508.46 Mbps.
Measuring Bandwidth Between PlanetLab Nodes
297
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
F
D
C
0
0
200
400
600
800
Capacity
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2