each batch. Then we train ﬁve classiﬁers from prior work,
P, H, D, W, T (described in Table 3), using single tab traces
of Alexa Top 100 webpages and test it using the multitab
traces we collected. We consider a classiﬁer successful if it
can identify either the foreground page or the background
page.
We observe a dramatic drop in the accuracy for all the
classiﬁers with respect to the accuracy obtained with the
control crawl (when the classiﬁers are trained and tested us-
ing single tab traces), even when the delay between the ﬁrst
to identify the foreground page. The accuracies obtained us-
ing this deﬁnition halved the accuracies showed in Table 5
(Acc test) and Figure 5.
4.6 Tor Browser Bundle Versions
In this section we evaluate the eﬀect of diﬀerent TBB
versions and properties of TBB on WF. We evaluate the
impact of having diﬀerent TBB versions for training and
testing.
In practice, many TBB versions coexist, largely
because of the lack of an auto-update functionality. It may
be diﬃcult for the attacker to know the exact version that is
being used by the user. We also changed the conﬁguration
of Tor in the torrc to see how deviating from the default
conﬁguration may aﬀect the success of the attack.
TBB Versions
We evaluate diﬀerent combinations of TBB versions 2.4.7,
3.5 and 3.5.2.1 for the control (training) and the test crawls.
Table 6 shows the accuracy of classiﬁer W when it trained on
traces from Alexa Top 100 sites collected using TBB in the
column and tested on the traces from the same sites collected
using the TBB in the rows.
For versions 3.5 and 3.5.2.1 we observe high accuracies of
W independently of the training and testing choices. This
may imply that the countermeasure based on request ran-
domization integrated in the TBB [24] may not be eﬀective.
On the other hand, when we evaluate 2.4.7 we observe low
accuracies for combinations with 3.5. This is probably due
to the major diﬀerences between the two versions. Version
3.5.2.1 is only a subversion of the TBB 3.5 which does not
incorporate as many changes as the diﬀerence between 3.5
and 2.4.7.
TBB
2.4.7 (Train)
3.5 (Train)
3.5.2.1 (Train)
2.4.7 (Test)
62.70% (±2.8%)
16.25% (±4.51%)
6.51% (±1.15%)
3.5 (Test)
29.93% (±2.54%)
76.38% (±4.97%)
66.75% (±3.68%)
3.5.2.1 (Test)
12.30% (±1.47%)
72.43% (±3.22%)
79.58% (±2.45%)
Table 6: Entry in row X, column Y corresponds to the
Acc Test (Step 2) and standard deviation (in parenthe-
ses) obtained by training in TBB version X and testing in
TBB version Y . The conﬁguration for these experiments is:
ntrain = 9, ntest = 1, Ttrain = 36, ttest = 4, m = 10 and
k = 100.
TBB properties
We vary the following properties: UseEntryGuards and Nu-
mEntryGuards. UseEntryGuards indicates the policy for en-
try guard selection.
It can take the following two values:
enabled, Tor selects three entry guards for a long period of
time; or disabled, picks one entry guard at random every
time it builds a new circuit. NumEntryGuards sets the num-
ber of entry guards that will be available for the construction
of a circuit (Default: 3). Note that even though we specify
a value for these variables, we clean the Tor data directory
after each batch crawl and, therefore, entry guards possibly
change across batches.
We trained and tested on the control crawl for three dif-
ferent pairs of values (only Step 1), listed in Table 7. The
default conﬁguration is to choose an entry guard from a list
of three possible entry guards (shown in the ﬁrst row of Ta-
ble 7). We also evaluated the setting used by Wang and
Goldberg [32], which consists in disabling UseEntryGuards
(second row in Table 7). Finally, we enabled UseEntry-
Figure 5: Average accuracies of P, H, D, T classiﬁers for a delay
of 0.5 sec between the loading start times of the foreground
page and the background page. Light gray bars represent the
accuracies of the control crawls (Step 1). We plot in darker
colour the accuracies obtained by training in the control
and testing in the multitab crawl (Step 2). Green intervals
indicate the standard deviation of the accuracy.
page and the background page was of 0.5 seconds (Figure 5
and Table 5). We also observe a drop in the accuracy while
we increase the size of the world, although the change in the
accuracy was similar for all classiﬁers ((Figure 5).
We notice very similar accuracies for classiﬁers P and T in
this experiment. These two classiﬁers are built using the
same set of features but diﬀerent learning models. This
might imply that the speciﬁc learning model is not as im-
portant for a successful attack as the feature selection. Dyer
et al. reported a similar observation between Naive Bayes
and SVM classiﬁers.
We also vary the time gap between the two pages to ac-
count for diﬀerent delays between opening the two tabs.
Since the W classiﬁer is based on an edit-distance, we ex-
pect the distance between the observed traﬃc trace and the
trace of any of the two loaded pages to be smaller with re-
spect to shorter delays, since there would be less overlap
between the traﬃc traces of the two loaded pages. However,
we do not observe a signiﬁcant evidence that may support
this hypothesis in the evaluation for 0.5, 3 and 5 seconds of
delay (Table 5) . The average page load for the test crawl for
the 5 second gap experiment is 15 seconds, leaving on aver-
age 30% of the original trace uncovered by the background
traﬃc. Even in this case, the accuracy with respect to the
control crawl drops by over 68%.
Delay
0.5 sec
3 sec
5 sec
Acc test
9.8% (±3.1%)
7.9% (±0.8%)
8.23% (±2.32%)
Acc control
77.08% (±2.72%)
77.08% (±2.72%)
77.08% (±2.72%)
Table 5: Average accuracies and standard deviations (in
parentheses) of classiﬁer W for diﬀerent delays of starting the
background page load. The parameters for this experiment
are: ntrain = 9, ntest = 1, Ttrain = 36, ttest = 4, m = 10
and k = 100.
So far we showed the results obtained when the adver-
sary is able to identify either the foreground page or the
background page. We also consider the case where the user
utilizes a countermeasure such as Camouﬂage [23]. In that
case, the user is not interested in the contents of the back-
ground page thus the adversary is successful only if he is able
PHD16TPHD32TPHD64TPHD100T020406080100Accuracy(%)6750413410854115227421736154481710766158453713644SizeoftheworldGuards but used a list of only one possible entry guard (third
row in Table 7).
Entry guard conﬁg.
NumEntryGuards = 3
UseEntryGuards = 1
UseEntryGuards = 0
NumEntryGuards = 1
UseEntryGuards = 1
Acc control
64.40% (±3.60%)
62.70% (±2.80%)
70.38% (±11.70%)
Table 7: Accuracy for diﬀerent entry guard conﬁgurations.
For these experiments we used the following parameters:
ntrain = 9, ntest = 1, Ttrain = 36, ttest = 4, m = 10
and k = 100.
In Table 7 we summarize the results for these three dif-
ferent conﬁgurations using classiﬁer W. We can see that the
standard deviation increases signiﬁcantly for the case where
we ﬁx one entry guard. Even though we ﬁx the entry guard
for all circuits in a batch, since we remove the Tor data direc-
tory after each batch, we force the entry guard to change.
On the other hand, allowing Tor to pick a diﬀerent entry
guard for each circuit results in a more balanced distribu-
tion because it is more likely that the same entry guards are
being used in each single batch, thus there is lower variance
across batches. We must clarify that these results are not
concluding and there may be a diﬀerent explanation for such
diﬀerence in standard deviation.
4.7 Network
Another important variable that is being ruled out by the
assumptions described in the previous section is the Inter-
net connection. We suspect that it is unrealistic to assume
the adversary is able to train using the same exact Inter-
net connection as the user, especially in the non-targeted
attack. For that, he might need more capabilities than the
ones included in the basic model.
In this section we study the eﬀect of diﬀerent network
locations on the accuracy of the W classiﬁer. To that end,
we crawled in three networks located in cities in diﬀerent
continents: Leuven, New York and Singapore.
Loc. Train Loc. Test
New York
Singapore
Singapore New York
Leuven
Leuven
Acc test
8.83% (±2.87%)
9.33% (±0.98%)
68.53% (±3.24%)
Acc control
66.95% (±2.87%)
66.95% (±2.87%)
76.40% (±5.99%)
Table 8: Accuracy for diﬀerent network locations. The Acc
test (Step 2) is calculated by training on data from Location
Train and testing in data from Location Test. The param-
eters for the setting of these experiments are: ntrain = 9,
ntest = 1, Ttrain = 36, ttest = 4, m = 10 and k = 100.
Our results show that the accuracy drop between the
crawls training on Leuven and testing in one of the other
two locations is relatively greater than the accuracy drop
observed in the experiments between Singapore and New
York. Since the VM in Leuven is located within a university
network and the other two VMs in data centers belonging
to the same company, we attribute this diﬀerence to the
fact that data center Internet connections tend to be closer
to the Internet backbone. This could account for similar
properties in the connection of the VMs in New York and
Singapore that helped the classiﬁer matching training and
testing instances.
4.8 The importance of false positives
In this section we evaluate the open-world scenario in
which an adversary monitors a small subset of the total
number of pages that a user can visit, thus cannot train
a classiﬁer using every possible page.
Open-world
In the open-world scenario the adversary monitors a small
number of pages and trains a classiﬁer on traﬃc traces of
both monitored and non-monitored pages. Following the
approach described by Wang et al. [32], we assume the ad-
versary monitors four pages: google.com, facebook.com,
wikipedia.org and twitter.com and the rest of the pages
in the Alexa Top 100 URLs are not monitored. We train
a classiﬁer using 36 traces for each of the Alexa Top 100
URLs, including the URLs of our monitored pages. To show
the accuracy of the classiﬁer in a true open-world scenario,
we test it using four traces for each of the monitored sites
plus one trace for each of the sites ranging from Alexa rank
151 to 34,710. For the classiﬁcation, we assume the attacker
is only interested in learning whether the user is visiting a
monitored page or not, and not the exact URL that the user
is visiting. did not train on.
The classiﬁer W oﬀers an accuracy over 90%, a true positive
rate (TPR) of 80% that is almost constant, and the false
positive rate (FPR) tends to 2.6% when we increase the size
of the world (Figure 6).
The base rate fallacy
Prior WF studies used accuracy-based metrics to measure
the success of the WF attack in the open-world. This ap-
proach however neglects the base rate or prior, that is the
probability of a user visiting a monitored page a priori. As
it has been pointed out recently within the Tor commu-
nity [25], this constitutes a bias in the evaluation of the WF
attack called the base rate fallacy. Despite reporting high
accuracies and low FPR when the prior is low the success of
the attack can be signiﬁcantly lower.
100
10−1
10−2
TPR
FPR
BDR
(uniform)
0K
10K
20K
30K
Size of the world
Figure 6: BDR in a uniformly distributed ∼35K open-world.
In contrast to prior work, we measure the success of the
attack in the open-world using the Bayesian detection rate
(BDR). The BDR is deﬁned as the probability that a traﬃc
trace actually corresponds to a monitored webpage given
that the classiﬁer recognized it as monitored.
Using the Bayes theorem, the BDR is expressed as
P (M | C) =
P (C | M ) P (M )
P (M ) P (C | M ) + P (¬M ) P (C | ¬M )
,
where M and C are the random variables of a webpage being
monitored and a webpage being detected by the classiﬁer as
monitored respectively. We use the TPR as an approxima-
tion of P (C | M ) and the FPR to estimate P (C | ¬M ).
In Figure 6, we show the BDR with assuming a uniform
|Monitored|
|World|
distribution of web pages (P (M ) =
) along with
the TPR and FPR of the classiﬁer W for diﬀerent sizes of the
world. We observe that the BDR tends to zero as the size
of the world increases. For a world of size 30K, which is a
rather small world compared to the total size of the Web, the
BDR is 0.4%. This means that there was a 0.4% probability
that the classiﬁer made a correct classiﬁcation, and 99.6%
of the times the adversary would wrongly conclude that the
user was accessing a monitored page.
1
0.8
0.6
0.4
0.2
0
0K
10K
20K
30K
Size of the world
TPR
FPR
BDR
(domains)
BDR
(popular)
BDR