regression took 45 minutes. Although we do not fully explore
the effects of different data sizes or algorithm parameters on
training time, we note that the following factors can increase
the training time: a higher number of iterations, a larger
training set (both with respect to number of examples and total
number of nonzero features), a smaller regularization factor λ
(which increases the amount of data communicated throughout
the cluster by decreasing the sparsity of the partial gradients
and weight vectors), and a smaller number of cluster machines.
For example, if we wanted to train on a larger number
of examples, we could lower the number of itertations and
increase the regularization factor to limit the training time.
Being aware of these tradeoffs can help practitioners who want
to retrain the classiﬁer daily.
Cost. Using our deployment of Monarch as a model, we
provide a breakdown of the costs associated with running
Monarch on AWS for a month long period, shown in Table 9.
Each of our components executes on EC2 spot instances that
have variable prices per hour according to demand, while
storage has a ﬁxed price. URL aggregation requires a single
instance to execute, costing $178 per month. For a throughput
of 638,000 URLs per day, 20 machines are required to
constantly crawl URLs and collect features, costing $882 per
month. Besides computing, we require storage as feature data
457
accumulates from crawlers. During a one month period, we
collected 1TB worth of feature data, with a cost of $.10 per
GB. However, for live execution of Monarch that excludes
the requirement of log ﬁles for experimentation, we estimate
only 700GB is necessary to accommodate daily re-training at
a monthly cost of $70. We can discard all other data from
the system after it makes a classiﬁcation decision. Finally,
daily classiﬁer retraining requires a single hour of access to 50
Double-Extra Large instances, for a total of $527 per month.
In summary, we estimate the costs of running a URL ﬁltering
service using Monarch with a throughput of 638,000 URLs
per day to be approximately $1,600 per month. We can reduce
this cost by limiting our use of cloud storage (switching from
JSON to a compressed format), as well as by reducing the
processing time per URL by means of better parallelism and
code optimizations.
We estimate the cost of scaling Monarch to a large web
service, using Twitter as an example. Twitter users send 90
million tweets per day, 25% (22.5 million) of which contain
URLs [45]. After whitelisting, deploying Monarch at that scale
requires a throughput of 15.3 million URLs per day. The
URL aggregation component is already capable of processing
incoming URLs at this capacity and requires no additional
cost. The crawlers and storage scale linearly, requiring 470
instances for feature collection and approximately 15 TB of
storage for a week’s worth of data, costing $20,760 and $1,464
per month respectively. The classiﬁer training cost remains
$527 per month so long as we use the same size of training
sample. Alternatively, we could reduce the number of training
iterations or increase the regularization factor λ to train on
more data, but keep training within one hour. This brings the
total cost for ﬁltering 15.3 million URLs per day to $22,751
per month.
6.3. Comparing Email and Tweet Spam
We compare email and tweet spam features used for
classiﬁcation and ﬁnd little overlap between the two. Email
spam consists of a diverse ecosystem of short-lived hosting
infrastructure and campaigns, while Twitter is marked by
longer lasting campaigns that push quite different content. We
capture these distinctions by evaluating two properties: feature
overlap between email and tweet spam and the persistence of
features over time for both categories. Each experiment uses
900,000 samples aggregated from email spam, tweet spam,
and non-spam, where we use non-spam as a baseline.
Overlap. We measure feature overlap as the log odds ratio that
a feature appears in one population versus a second population.
Speciﬁcally, we compute |log(p1q2/p2q1)|, where pi is the
likelihood of appearing in population i and qi = 1− pi. A log
odds ratio of 0 indicates a feature is equally likely to be found
in two populations, while an inﬁnite ratio indicates a feature
is exclusive to one population. Figure 4 shows the results of
the log odds test (with inﬁnite ratios omitted). Surprisingly,
90% of email and tweet features never overlap. The lack of
Fig. 4: Overlap of features. Email and Twitter spam share only 10%
of features in common, indicating that email spammers and Twitter
spammers are entirely separate actors.
correlation between the two indicates that email spammers are
entirely separate actors from Twitter spammers, each pushing
their own campaigns on distinct infrastructure. Consequently,
the classiﬁer must learn two separate sets of rules to identify
both spam types.
Equally problematic, we ﬁnd 32% of tweet spam features
are shared with non-spam, highlighting the challenge of classi-
fying Twitter spam. In particular, 41% of IP features associated
with tweet spam are also found in nonspam, a result of shared
redirects and hosting infrastructure. In contrast, only 16% of
email spam IP features are found in non-spam, allowing a
clearer distinction to be drawn between the two populations.
Persistence. We measure feature persistence as the time delta
between the ﬁrst and last date a feature appears in our data
set, shown in Figure 5. Email spam is marked by much shorter
lived features compared to tweet spam and non-spam samples.
Notably, 77% of initial URL features appearing in email
disappear after 15 days. The same is true for 60% of email
DNS features, compared to just 30% of IP features associated
with email spam hosting. Each of these results highlights the
quick churn of domains used by email campaigns and the long
lasting IP infrastructure controlled by email spammers. This
same sophistication is unnecessary in Twitter, where there is
no pressure to evade blacklists or spam ﬁltering.
6.4. Spam Infrastructure
Email spam has seen much study towards understanding the
infrastructure used to host spam content [18], [46]. From our
feature collection, we identify two new properties of interest
that help to understand spam infrastructure: redirect behavior
used to lead victims to spam sites, and embedding spam
content on benign pages.
Redirecting to spam. Both Twitter and email spammers use
redirects to deliver victims to spam content. This mechanism
458
Domain
bit.ly
t.co
tinyurl.com
ow.ly
goo.gl
su.pr
fb.me
dlvr.it
os7.biz
is.gd
Email spam Twitter spam
1%
0%
3%
0%
0%
0%
0%
0%
0%
0%
41%
4%
4%
4%
3%
3%
2%
2%
1%
1%
TABLE 10: Top 10 URL shortening services abused by spammers.
Feature Category
Initial URL
Final URL
Top-level Window Redirect URL
Content Redirect URL
Frame Content URL
Link URLs
Source URLs
% Blacklisted % Exclusive
0.05%
2.62%
4.41%
1.35%
6.87%
7.03%
42.51%
16.60%
23.33%
34.25%
3.99%
14.85%
28.28%
100%
TABLE 11: Breakdown of the locations of blacklisted URLs. We
mark a page as spam if it makes any outgoing request to a blacklisted
URL.
of spam popups, sounds, and video to play. Table 11 shows
a breakdown of the locations containing blacklisted URLs
speciﬁcally for Twitter. The column labeled exclusive indicates
the percent of URLs that can be blacklisted exclusively based
on a URL in that location. For example, 0.05% of Twitter spam
can be blacklisted using only an initial URL posted to the
site. Since the category source URLs is a superset of all other
URLs, 100% of pages can be blacklisted; however, looking
exclusively at URLs which are not part of other categories,
we ﬁnd that 42.51% of source URLs lead to blacklisting. This
indicates a page included an image, stylesheet, plugin, script,
or dynamically retrieved content via JavaScript or a plugin that
was blacklisted. These scenarios highlight the requirement of
analyzing all of a webpages content to not overlook spam
with dynamic page behavior or mash-up content that includes
known spam domains.
7. Discussion
In this section we discuss potential evasive attacks against
Monarch that result from running a centralized service. While
we can train our system to identify spam and have shown the
features we extract are applicable over time, classiﬁcation ex-
ists in an adversarial environment. Attackers can tune features
to fall below the spam classiﬁcation threshold, modify content
after classiﬁcation, and block our crawler. We do not propose
solutions to these attacks; instead, we leave to future work an
in depth study of each attack and potential solutions.
Feature Evasion. When Monarch provides a web service
with a classiﬁcation decision, it also provides attackers with
immediate feedback for whether their URLs are blocked. An
attacker can use this feedback to tune URLs and content
in an attempt to evade spam classiﬁcation, as discussed in
Fig. 5: Persistence of URL features. Email spam features are shorter
lived compared to tweet spam, a result of short-lived campaigns and
domain churn.
is dominated by tweet spam where 67% of spam URLs in
our data set use redirects, with a median path length of 3.
In contrast, only 20% of email spam URLs contain redirects,
with a median path length of 2. Further distinctions between
email and tweet spam behavior can be found in the abuse
of public URL shorteners. Table 10 shows the top ten URL
shortening services used for both email and tweet spam. The
majority of email spam in our data set redirects through
customized infrastructure hosted on arbitrary domains, while
Twitter spammers readily abuse shortening services provided
by bit.ly, Twitter, Google, and Facebook. Despite efforts by
URL shorteners to block spam [8], [9], we ﬁnd that widespread
abuse remains prevalent.
Apart from the use of redirectors to mask initial URLs,
we also examine domains that are commonly traversed as
shortened URLs resolve to their ﬁnal landing page. The top
two destinations of URLs shortened by bit.ly are publicly
available services provided by google.com and blogspot.com.
Together, these two domains account for 24% of the spam ﬁrst
shortened by bit.ly. In the case of google.com, spam URLs
embed their ﬁnal landing page behind an arbitrary redirector
operated by Google. This masks the ﬁnal spam landing site
from bit.ly, rendering blacklisting performed by the service
obsolete. The second most common service, blogspot.com, is
abused for free spam hosting rather than as a redirector. Each
blog contains scam advertisements and other solicitations.
By relying on Blogspot, spammers can evade domain-based
blacklists that lack the necessary precision to block spam
hosted alongside benign content.
Each of these are prime examples of web services currently
being abused by spammers and serve as a strong motivation
for the need of a system like Monarch.
Page content. Another phenomenon we frequently observe
in Twitter spam is the blacklisting of content within a page.
For the majority of sites, this is a web advertisement from
a questionable source. We have observed popular news sites
with non-spam content displaying ads that cause a variety
459
previous studies [47]–[49], but not without consequences and
limitations. The simplest changes an attacker can make are
modiﬁcations to page content: HTML,
links, and plugins.
Known spam terms can be transformed into linguistically
similar, but lexically distinct permutations to avoid detection,
while links and plugins can be modiﬁed to imitate non-spam
pages. Page behavior poses a more difﬁcult challenge; by
removing pop-up windows and alert prompts, a spammer po-
tentially reduces the effectiveness of eliciting a response from
victims. Finally, hosting infrastructure, redirects, and domains,
while mutable, require a monetary expense for dynamism. We
leave evaluating how susceptible our classiﬁcation system is to
evasion to future work, but note that email spam classiﬁcation
and intrusion prevention systems both exist
in adversarial
environments and maintain wide-spread adoption.
Time-based Evasion. In Monarch’s current implementation,
feature collection occurs at the time a URL is submitted to
our system; URLs are not re-crawled over time unless they
are resubmitted. This raises the potential for an attacker to
change either page content or redirect to new content after a
URL has been classiﬁed. For this attack to succeed, a URL’s
redirects and hosting infrastructure must appear benign during
classiﬁcation and allow subsequent modiﬁcation. An attacker
that simply masks his ﬁnal landing page, but re-uses known
hostile redirect infrastructure may still be identiﬁed by the
classiﬁer. Furthermore, static shorteners such as bit.ly cannot
be used because the landing page cannot be changed after
shortening. To circumvent both of these limitations, an attacker
can rely on mutable content hosted on public infrastructure
typically associated with non-spam pages, such as Blogspot,
LiveJournal, and free web hosting. In this scenario, an at-
tacker’s blog contains non-spam content during classiﬁcation
and is subsequently modiﬁed to include spam content or a
JavaScript redirect to a new hostile landing page.
Crawler Evasion. Rather than an attacker modifying content
to evade classiﬁcation, an adversary can alter HTTP and DNS
behavior to prevent our crawler from ever reaching spam
pages. Potential attacks include relying on browser user-agent
detection or other forms of browser ﬁngerprinting [50] to
forward our crawler to non-hostile content and regular users to
a hostile copies. Alternatively, the IP addresses of Monarch’s
crawlers can be learned by an attacker repeatedly posting
URLs to our service and tracking the IPs of visitors. A list of
crawler IP addresses can then be distributed as a blacklist, with
attackers either blocking access or redirecting our crawlers to
non-spam content.
8. Related Work
Web Service-speciﬁc Defenses. Of the threats facing web
services, social networks in particular have garnered particular
attention. In a recent study, Gao et al. showed that 10% of links
posted to Facebook walls are spam while 97% of accounts
participating in the campaigns are compromised users [4]. The
same is true of Twitter, where at least 8% of links posted are
spam and 86% of the accounts involved are compromised [5].
To counter this threat, a number of service-speciﬁc solutions
have been proposed to classify spam accounts based on post
frequency, the number of URLs an account posts, and the
ability of an account to acquire friends [12]–[14]. Accuracy
for these techniques varies between 70–99% for detecting
spam accounts, with sample sizes ranging between 200–1500
spam accounts. However, many of the metrics used to detect
spammers can be readily evaded. Spammers can randomize the
template behind spam messages posted to social networks, post
benign messages to skew the ratio of URLs to posts, befriend
other spammers to mimic the average friend counts of regular
users, and generate dormant spam accounts prior to the onset
of a spam campaign to avoid heuristics targeting account age.
While Twitter and Facebook rely on similar account heuris-