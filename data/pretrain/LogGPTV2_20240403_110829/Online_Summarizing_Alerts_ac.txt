Online Summarizing Alerts through Semantic and Behavior Information 	ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
Figure 5: The structure of ACT.
| groups, correlated alerts to 𝑒𝑖, denoted as 𝑅𝑖 (|𝑅𝑖 | = 𝑐𝑖), and re-maining uncorrelated alerts, denoted as 𝐻𝑖 (|𝐻𝑖 | = 𝑜𝑖). Then, the real relationship between two alerts, 𝑒𝑖 and 𝑒𝑗 (1 ≤ 𝑗 ≤ 𝑛), is for-mally defined as 𝑃𝑖,𝑗 = [1, 0]⊤if 𝑒𝑖 and 𝑒𝑗 are correlated, otherwise 𝑃𝑖,𝑗 = [0, 1]⊤. Finally, we adopt Equation (5), which measures the difference between the determination of ACT and the ground truth,as the loss function of ACT. | 
 | incident 1 | 
 | incident 2 | incident 2 | newly reported alert |
|---|---|---|---|---|---|---|
| groups, correlated alerts to 𝑒𝑖, denoted as 𝑅𝑖 (|𝑅𝑖 | = 𝑐𝑖), and re-maining uncorrelated alerts, denoted as 𝐻𝑖 (|𝐻𝑖 | = 𝑜𝑖). Then, the real relationship between two alerts, 𝑒𝑖 and 𝑒𝑗 (1 ≤ 𝑗 ≤ 𝑛), is for-mally defined as 𝑃𝑖,𝑗 = [1, 0]⊤if 𝑒𝑖 and 𝑒𝑗 are correlated, otherwise 𝑃𝑖,𝑗 = [0, 1]⊤. Finally, we adopt Equation (5), which measures the difference between the determination of ACT and the ground truth, as the loss function of ACT. |  |incident 1 |  | | |newly reported alert || groups, correlated alerts to 𝑒𝑖, denoted as 𝑅𝑖 (|𝑅𝑖 | = 𝑐𝑖), and re-maining uncorrelated alerts, denoted as 𝐻𝑖 (|𝐻𝑖 | = 𝑜𝑖). Then, the real relationship between two alerts, 𝑒𝑖 and 𝑒𝑗 (1 ≤ 𝑗 ≤ 𝑛), is for-mally defined as 𝑃𝑖,𝑗 = [1, 0]⊤if 𝑒𝑖 and 𝑒𝑗 are correlated, otherwise 𝑃𝑖,𝑗 = [0, 1]⊤. Finally, we adopt Equation (5), which measures the difference between the determination of ACT and the ground truth, as the loss function of ACT. |  |incident 1 |  | | |newly reported alert || groups, correlated alerts to 𝑒𝑖, denoted as 𝑅𝑖 (|𝑅𝑖 | = 𝑐𝑖), and re-maining uncorrelated alerts, denoted as 𝐻𝑖 (|𝐻𝑖 | = 𝑜𝑖). Then, the real relationship between two alerts, 𝑒𝑖 and 𝑒𝑗 (1 ≤ 𝑗 ≤ 𝑛), is for-mally defined as 𝑃𝑖,𝑗 = [1, 0]⊤if 𝑒𝑖 and 𝑒𝑗 are correlated, otherwise 𝑃𝑖,𝑗 = [0, 1]⊤. Finally, we adopt Equation (5), which measures the difference between the determination of ACT and the ground truth, as the loss function of ACT. |  |incident 1 |  | | | || groups, correlated alerts to 𝑒𝑖, denoted as 𝑅𝑖 (|𝑅𝑖 | = 𝑐𝑖), and re-maining uncorrelated alerts, denoted as 𝐻𝑖 (|𝐻𝑖 | = 𝑜𝑖). Then, the real relationship between two alerts, 𝑒𝑖 and 𝑒𝑗 (1 ≤ 𝑗 ≤ 𝑛), is for-mally defined as 𝑃𝑖,𝑗 = [1, 0]⊤if 𝑒𝑖 and 𝑒𝑗 are correlated, otherwise 𝑃𝑖,𝑗 = [0, 1]⊤. Finally, we adopt Equation (5), which measures the difference between the determination of ACT and the ground truth, as the loss function of ACT. |  |incident 1 |  | | | |ICSE ’22, May 21–29, 2022, Pittsburgh, PA, USA
Table 2: Details of Experimental Datasets
| Datasets | Datasets | Time Span | #Alerts | #Alert Types |
|---|---|---|---|---|
| Bank A Bank B |Bank A Bank B |2018/11/23 ∼ 2019/02/02 2019/03/01 ∼ 2020/08/06 |50947  500000 |2794  51 |
| 8.2 |Baselines |Baselines |50947  500000 |2794  51 |As shown in Table 3, we compare our approaches with seven ap-proaches, SeqKrimp, GoKrimp, CSC, SWIFT, Jaccard, Word2Vec, and LDA. SeqKrimp, GoKrimp, CSC, and SWIFT are all frequent pattern mining approaches. They generically summarize an event sequence by mining frequent patterns that minimize the description length of the event sequence [24]. SeqKrimp [12], GoKrimp [12], and CSC [10] are offline approaches, while SWIFT [28] is an online approach. As frequent patterns reveal co-occurrence relations be-tween alerts, we consider CSC, SeqKrimp, GoKrimp, SWIFT as a type of approaches that summarize alerts by behavior information.Jia Chen, Peng Wang, and Wei WangIn LDA, the number of topics is set to 9. For the CBOW model in ASR and Word2Vec, the size of the word embedding is 512 and the epoch is set to 100. For ABR, the size of the behavior representation is 600, the sample granularity for the occurrence series, 𝛼, is set to 1 minute, the sample length for the occurrence series, 𝛽, is set to 6 hours for Bank A and 13 hours for Bank B, and the epoch is set to 5. For ACT, in Linear Transformation, the first layer has 50 neurons, and the second layer has 30 neurons. In Dimension Reduction, the first layer has 20 neurons, and the second layer has 2 fixed neurons. The epoch of ACT is set to 60. The window size, 𝑤, in online summarizing, is set to 5 minutes. For approaches that require training, Table 4 demonstrates their time cost in the training stage. Although our approaches require the longest training time, they all take less than 10 minutes to finish training, which is actually acceptable since the training stage is conducted offline.Table 4: Training Time of Experimental Approaches
Table 3: Information of Experimental Approaches Bank ASR Training Time (s)
ABR 	ACT 	Word2Vec LDA
| Approach | Approach | Semantic | Behavior | Online |
|---|---|---|---|---|
| Approach |Approach |Information |Information |Online |
| SeqKrimp GoKrimp CSC  SWIFT  Jaccard  LDA  Word2Vec ASR  ABR  OAS |SeqKrimp GoKrimp CSC  SWIFT  Jaccard  LDA  Word2Vec ASR  ABR  OAS |× × × √× √ √ √ |√ √ √ √ |× × √× √ √ √ √ √ √ || SeqKrimp GoKrimp CSC  SWIFT  Jaccard  LDA  Word2Vec ASR  ABR  OAS |SeqKrimp GoKrimp CSC  SWIFT  Jaccard  LDA  Word2Vec ASR  ABR  OAS |× × × √× √ √ √ |× × × √× √ |× × √× √ √ √ √ √ √ |
| SeqKrimp GoKrimp CSC  SWIFT  Jaccard  LDA  Word2Vec ASR  ABR  OAS |SeqKrimp GoKrimp CSC  SWIFT  Jaccard  LDA  Word2Vec ASR  ABR  OAS |√× |× × × √× √ |× × √× √ √ √ √ √ √ || SeqKrimp GoKrimp CSC  SWIFT  Jaccard  LDA  Word2Vec ASR  ABR  OAS |SeqKrimp GoKrimp CSC  SWIFT  Jaccard  LDA  Word2Vec ASR  ABR  OAS | | | || Since Jaccard [13, 31], Word2Vec[19], and LDA [2, 33] are widely used to measure the semantic relevance of alerts, we thus com-pare our approaches with such three approaches. In addition, we also individually evaluate the ability of ASR and ABR, respectively. Specifically, to summarize alerts online by ASR, we adopt the online summarizing strategy in Section 7. For the newly generated alert, 𝑒𝑖, instead of ACT, we find its most relevant alert during [𝑡𝑖 −𝑤,𝑡𝑖] by the cosine similarity between semantic representations. Then, if the maximum cosine similarity is larger than a fixed threshold, we then add 𝑒𝑖 into the incident of the most relevant alert. Otherwise, we form a new incident for 𝑒𝑖. The same is true for ABR. |Since Jaccard [13, 31], Word2Vec[19], and LDA [2, 33] are widely used to measure the semantic relevance of alerts, we thus com-pare our approaches with such three approaches. In addition, we also individually evaluate the ability of ASR and ABR, respectively. Specifically, to summarize alerts online by ASR, we adopt the online summarizing strategy in Section 7. For the newly generated alert, 𝑒𝑖, instead of ACT, we find its most relevant alert during [𝑡𝑖 −𝑤,𝑡𝑖] by the cosine similarity between semantic representations. Then, if the maximum cosine similarity is larger than a fixed threshold, we then add 𝑒𝑖 into the incident of the most relevant alert. Otherwise, we form a new incident for 𝑒𝑖. The same is true for ABR. |Since Jaccard [13, 31], Word2Vec[19], and LDA [2, 33] are widely used to measure the semantic relevance of alerts, we thus com-pare our approaches with such three approaches. In addition, we also individually evaluate the ability of ASR and ABR, respectively. Specifically, to summarize alerts online by ASR, we adopt the online summarizing strategy in Section 7. For the newly generated alert, 𝑒𝑖, instead of ACT, we find its most relevant alert during [𝑡𝑖 −𝑤,𝑡𝑖] by the cosine similarity between semantic representations. Then, if the maximum cosine similarity is larger than a fixed threshold, we then add 𝑒𝑖 into the incident of the most relevant alert. Otherwise, we form a new incident for 𝑒𝑖. The same is true for ABR. |Since Jaccard [13, 31], Word2Vec[19], and LDA [2, 33] are widely used to measure the semantic relevance of alerts, we thus com-pare our approaches with such three approaches. In addition, we also individually evaluate the ability of ASR and ABR, respectively. Specifically, to summarize alerts online by ASR, we adopt the online summarizing strategy in Section 7. For the newly generated alert, 𝑒𝑖, instead of ACT, we find its most relevant alert during [𝑡𝑖 −𝑤,𝑡𝑖] by the cosine similarity between semantic representations. Then, if the maximum cosine similarity is larger than a fixed threshold, we then add 𝑒𝑖 into the incident of the most relevant alert. Otherwise, we form a new incident for 𝑒𝑖. The same is true for ABR. |Since Jaccard [13, 31], Word2Vec[19], and LDA [2, 33] are widely used to measure the semantic relevance of alerts, we thus com-pare our approaches with such three approaches. In addition, we also individually evaluate the ability of ASR and ABR, respectively. Specifically, to summarize alerts online by ASR, we adopt the online summarizing strategy in Section 7. For the newly generated alert, 𝑒𝑖, instead of ACT, we find its most relevant alert during [𝑡𝑖 −𝑤,𝑡𝑖] by the cosine similarity between semantic representations. Then, if the maximum cosine similarity is larger than a fixed threshold, we then add 𝑒𝑖 into the incident of the most relevant alert. Otherwise, we form a new incident for 𝑒𝑖. The same is true for ABR. || 8.3 |Setup |Setup |Setup |Setup || We implement ASR, ABR, ACT, OAS, Jaccard, LDA, and Word2Vec with Python 3.6 and PyTorch 1.5.1 [22]. For LDA and Word2Vec, we use a popular open-source NLP toolkit, Gensim [23]. As for SeqKrimp, GoKrimp, CSC, and SWIFT, we use a Java-based open-source toolkit [28]. All experiments are conducted on a 4.20GHz× 8 Intel i7-7700K PC with 16 GB memory running ubuntu. The source code of our approaches is available at 5281/zenodo.5998339. |We implement ASR, ABR, ACT, OAS, Jaccard, LDA, and Word2Vec with Python 3.6 and PyTorch 1.5.1 [22]. For LDA and Word2Vec, we use a popular open-source NLP toolkit, Gensim [23]. As for SeqKrimp, GoKrimp, CSC, and SWIFT, we use a Java-based open-source toolkit [28]. All experiments are conducted on a 4.20GHz× 8 Intel i7-7700K PC with 16 GB memory running ubuntu. The source code of our approaches is available at 5281/zenodo.5998339. |We implement ASR, ABR, ACT, OAS, Jaccard, LDA, and Word2Vec with Python 3.6 and PyTorch 1.5.1 [22]. For LDA and Word2Vec, we use a popular open-source NLP toolkit, Gensim [23]. As for SeqKrimp, GoKrimp, CSC, and SWIFT, we use a Java-based open-source toolkit [28]. All experiments are conducted on a 4.20GHz× 8 Intel i7-7700K PC with 16 GB memory running ubuntu. The source code of our approaches is available at 5281/zenodo.5998339. |We implement ASR, ABR, ACT, OAS, Jaccard, LDA, and Word2Vec with Python 3.6 and PyTorch 1.5.1 [22]. For LDA and Word2Vec, we use a popular open-source NLP toolkit, Gensim [23]. As for SeqKrimp, GoKrimp, CSC, and SWIFT, we use a Java-based open-source toolkit [28]. All experiments are conducted on a 4.20GHz× 8 Intel i7-7700K PC with 16 GB memory running ubuntu. The source code of our approaches is available at 5281/zenodo.5998339. |We implement ASR, ABR, ACT, OAS, Jaccard, LDA, and Word2Vec with Python 3.6 and PyTorch 1.5.1 [22]. For LDA and Word2Vec, we use a popular open-source NLP toolkit, Gensim [23]. As for SeqKrimp, GoKrimp, CSC, and SWIFT, we use a Java-based open-source toolkit [28]. All experiments are conducted on a 4.20GHz× 8 Intel i7-7700K PC with 16 GB memory running ubuntu. The source code of our approaches is available at 5281/zenodo.5998339. |Bank A Bank B 9.977 
24.667 7.707 
330.388 16.425 
260.671 7.656 
24.658 5.005 
57.214
Thresholds for experimental approaches are all adjusted to achieve the maximum valid compression ratio. For each dataset, according to the timestamp, we take the first 80% as the training data, and the last 20% as the testing data.