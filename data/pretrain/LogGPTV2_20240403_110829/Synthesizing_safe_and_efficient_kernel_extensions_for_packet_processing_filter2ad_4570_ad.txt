Number of basic blocks
All
5
5
6
5
4
13
13
20
5
18
5
15
19
4
24
6
21
4
247
Benchmark
(1) xdp_exception
(2) xdp_redirect_err
(3) xdp_devmap_xmit
(4) xdp_cpumap_kthread
(5) xdp_cpumap_enqueue
(6) sys_enter_open
(7) socket/0
(8) socket/1
(9) xdp_router_ipv4
(10) xdp_redirect
(11) xdp1_kern/xdp1
(12) xdp2_kern/xdp1
(13) xdp_fwd
(14) xdp_pktcntr
(15) xdp_fw
(16) xdp_map_access
(17) from-network
(18) recvmsg4
(19) xdp-balancer
Avg. of all benchmarks
Table 1: K2’s improvements in program compactness across benchmarks from the Linux kernel (1–13), Facebook (14, 19), hXDP [52] (15, 16),
and Cilium (17, 18). “DNL” means that the program variant did not load as it was rejected by the kernel checker.
79
10
1,201
1,170
1,848
519
6
9
898
523
472
157
6,137
288
826
27
6,871
3,350
167,428
10,096
914
3,455
354,154
228,101
739,416
100,811
2,851,203
614,569
342,009
69,628
4,312,839
904,934
10,251,406
1,240,505
11.11%
11.11%
19.44%
25.00%
19.23%
16.67%
6.90%
6.25%
10.81%
18.60%
8.20%
8.97%
17.42%
13.64%
9.72%
13.33%
25.64%
13.83%
9.26%
13.95%
K2
16
16
29
18
21
20
27
30
99
35
56
71
128
19
65
26
29
81
1,607
-Os
18
18
36
24
26
24
29
32
111
43
61
78
155
22
72
30
39
94
1,771
18
18
36
24
26
24
29
32
111
43
61
78
155
22
72
30
39
94
1,811
-O1
18
19
36
24
26
24
32
35
139
45
72
93
170
22
85
30
43
98
DNL
We use two server-class machines on CloudLab [66] to set up
a high-speed traffic generator (T-Rex [7]) and a device-under-test
(DUT). Our setup is visualized in Fig. 2. The DUT runs a subset of our
benchmark BPF programs that attach to the network device driver
using the XDP hook [83]. The servers house 10-core Intel Broadwell
(E5-2640v4) 2.4 GHz processors with a PCIe 3.0 bus and 64 GB of
memory. The servers are equipped with Mellanox ConnectX-4 25G
adapters. Test traffic moves from the traffic generator to the DUT
and back to the traffic generator to form a loop, in the spirit of the
benchmarking methodology outlined in RFC 2544 [1], allowing us
to measure both the packet-processing throughput and the round-
trip latency to forward via the DUT. Within the CloudLab network,
the two machines connect over a Mellanox switch.
We tuned the DUT following instructions from the XDP bench-
marking configurations described in [83]. Specifically, we set up
Linux Receive-Side Scaling (RSS) [53], IRQ affinities for NIC receive
queues [86], PCIe descriptor compression, the maximum MTU for
the Mellanox driver to support BPF, and the RX descriptor ring
size for the NIC. Our configurations and benchmarking scripts are
publicly available from the project web page [120].
We report program throughput as the maximum loss-free for-
warding rate (MLFFR [1]) of a single core. This is measured by
increasing the offered load from the traffic generator slowly and
recording the load beyond which the packet loss rate rises sharply.
We measure throughput in millions of packets per second (Mpps) at
64-byte packet size. We use the minimum packet size since network-
related CPU usage is proportional to packets per second rather than
bytes per second, and XDP programs can easily saturate 100 Gbit/s
on a single core with larger packet sizes [83]. Since latency varies
with the load offered by the traffic generator, we report the latencies
of the program variants at four separate offered loads: (i) low (load
smaller than the throughput of the slowest variant), (ii) medium
(load equal to the throughput of the slowest variant), (iii) high (load
equal to the throughput of the fastest variant), and (iv) saturating
(load higher than the throughput of all known variants). We aver-
age the results of 3 trials, with each result obtained after waiting
60 seconds or until the numbers stabilize.
K2’s measured improvements in throughput and latency over the
best clang-compiled variant of the same program are summarized in
Table 2 and Table 3. K2 provides small improvements in throughput
ranging from 0–4.75%, while K2’s latency benefits range from 1.36%–
55.03%. These benefits arise from target-specific optimizations with
the latency cost function. (Appendix H shows detailed pictures of
packet latency at varying loads.) More work remains before fully
attaining the potential benefits of synthesis (§11).
Benchmark
xdp2
xdp_router_ipv4
xdp_fwd
xdp1
xdp_map_access
xdp-balancer
-O1
8.855
1.496
4.886
16.837
14.679
DNL
-O2/-O3
9.547
1.496
4.984
16.85
14.678
3.292
K2
9.748
1.496
5.072
17.65
15.074
3.389
Gain
2.11%
0.00%
1.77%
4.75%
2.70%
2.94%
Table 2: Throughput reported as the maximum loss-free forward-
ing rate (MLFFR) in millions of packets per second per core (§8).
10
Synthesizing Safe and Efficient Kernel Extensions for Packet Processing
arXiv, July 14, 2021
Reduction
Low
9
1
4.4
3
Reduction
Medium
11.91%
5.51%
5.93%
3.88%
9.5
1.5
5
3.3
clang
29.148
63.323
32.272
38.650
Benchmark
xdp2
5.89%
xdp_router_ipv4
1.48%
xdp_fwd
2.46%
xdp-balancer
1.36%
Table 3: Average latencies (in microseconds) of the best clang and K2 variants at different offered loads (in millions of packets per second).
We consider 4 offered loads: low (smaller than the slowest throughput of clang or K2), medium (the slowest throughput among clang and K2),
high (the highest throughput among clang and K2), and saturating (higher than the fastest throughput of clang or K2).
clang
89.523
84.450
87.291
237.701
clang
103.872
619.291
192.936
296.405
K2
30.237
76.929
71.645
55.741
K2
25.676
59.834
30.358
37.152
K2
97.754
610.119
188.199
292.376
K2
40.259
76.929
71.645
119.497
clang
51.157
84.450
87.291
73.319
Reduction
Saturating
Reduction
40.89%
8.91%
17.92%
23.97%
High
9.7
1.5
5
3.4
55.03%
8.91%
17.92%
49.73%
10.3
1.8
5.2
3.7
Safety of synthesized programs. We loaded the XDP program out-
puts produced by K2 into the kernel. All 38 out of the 38 programs
found by K2’s search were successfully accepted by the kernel
checker, even without K2’s safety post-processing (§6). Table 5 in
Appendix F lists the programs we loaded into the kernel.
Benefits of equivalence-checking optimizations. We show the ben-