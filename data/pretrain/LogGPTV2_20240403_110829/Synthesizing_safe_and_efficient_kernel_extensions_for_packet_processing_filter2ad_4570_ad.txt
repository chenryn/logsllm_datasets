### Number of Basic Blocks

| Benchmark | Number of Basic Blocks |
|-----------|------------------------|
| xdp_exception | 5 |
| xdp_redirect_err | 5 |
| xdp_devmap_xmit | 6 |
| xdp_cpumap_kthread | 5 |
| xdp_cpumap_enqueue | 4 |
| sys_enter_open | 13 |
| socket/0 | 13 |
| socket/1 | 20 |
| xdp_router_ipv4 | 5 |
| xdp_redirect | 18 |
| xdp1_kern/xdp1 | 5 |
| xdp2_kern/xdp1 | 15 |
| xdp_fwd | 19 |
| xdp_pktcntr | 4 |
| xdp_fw | 24 |
| xdp_map_access | 6 |
| from-network | 21 |
| recvmsg4 | 4 |
| xdp-balancer | 247 |

**Table 1: K2’s Improvements in Program Compactness Across Benchmarks**

- **Linux Kernel (1–13)**
- **Facebook (14, 19)**
- **hXDP [52] (15, 16)**
- **Cilium (17, 18)**

"DNL" indicates that the program variant did not load as it was rejected by the kernel checker.

### K2's Improvements in Program Compactness

| Benchmark | K2 | -Os | -O1 | DNL |
|-----------|----|-----|-----|-----|
| xdp_exception | 16 | 18 | 18 | 18 |
| xdp_redirect_err | 16 | 18 | 18 | 18 |
| xdp_devmap_xmit | 29 | 36 | 36 | 36 |
| xdp_cpumap_kthread | 18 | 24 | 24 | 24 |
| xdp_cpumap_enqueue | 21 | 26 | 26 | 26 |
| sys_enter_open | 20 | 24 | 24 | 24 |
| socket/0 | 27 | 29 | 29 | 32 |
| socket/1 | 30 | 32 | 32 | 35 |
| xdp_router_ipv4 | 99 | 111 | 111 | 139 |
| xdp_redirect | 35 | 43 | 43 | 45 |
| xdp1_kern/xdp1 | 56 | 61 | 61 | 72 |
| xdp2_kern/xdp1 | 71 | 78 | 78 | 93 |
| xdp_fwd | 128 | 155 | 155 | 170 |
| xdp_pktcntr | 19 | 22 | 22 | 22 |
| xdp_fw | 65 | 72 | 72 | 85 |
| xdp_map_access | 26 | 30 | 30 | 30 |
| from-network | 29 | 39 | 39 | 43 |
| recvmsg4 | 81 | 94 | 94 | 98 |
| xdp-balancer | 1,607 | 1,771 | 1,811 | DNL |

**Improvement Percentage:**
- K2: 11.11% to 25.64%
- -Os: 13.83% to 13.95%

### Experimental Setup

We used two server-class machines on CloudLab [66] to set up a high-speed traffic generator (T-Rex [7]) and a device-under-test (DUT). The setup is visualized in Fig. 2. The DUT runs a subset of our benchmark BPF programs, which attach to the network device driver using the XDP hook [83]. The servers are equipped with 10-core Intel Broadwell (E5-2640v4) 2.4 GHz processors, a PCIe 3.0 bus, and 64 GB of memory. They also have Mellanox ConnectX-4 25G adapters. Test traffic moves from the traffic generator to the DUT and back to the traffic generator, forming a loop. This setup allows us to measure both packet-processing throughput and round-trip latency.

### Tuning and Configuration

The DUT was tuned following instructions from the XDP benchmarking configurations described in [83]. Specifically, we set up Linux Receive-Side Scaling (RSS) [53], IRQ affinities for NIC receive queues [86], PCIe descriptor compression, the maximum MTU for the Mellanox driver to support BPF, and the RX descriptor ring size for the NIC. Our configurations and benchmarking scripts are publicly available from the project web page [120].

### Throughput and Latency Measurement

- **Throughput:** We report program throughput as the maximum loss-free forwarding rate (MLFFR [1]) of a single core, measured by increasing the offered load from the traffic generator slowly and recording the load beyond which the packet loss rate rises sharply. Throughput is measured in millions of packets per second (Mpps) at 64-byte packet size.
- **Latency:** Latency varies with the load offered by the traffic generator. We report latencies at four separate offered loads: (i) low (load smaller than the throughput of the slowest variant), (ii) medium (load equal to the throughput of the slowest variant), (iii) high (load equal to the throughput of the fastest variant), and (iv) saturating (load higher than the throughput of all known variants). Results are averaged over 3 trials, with each result obtained after waiting 60 seconds or until the numbers stabilize.

### K2's Measured Improvements

**Table 2: Throughput (MLFFR in Mpps/core)**

| Benchmark | -O1 | -O2/-O3 | K2 | Gain (%) |
|-----------|-----|---------|----|----------|
| xdp2 | 8.855 | 9.547 | 9.748 | 2.11% |
| xdp_router_ipv4 | 1.496 | 1.496 | 1.496 | 0.00% |
| xdp_fwd | 4.886 | 4.984 | 5.072 | 1.77% |
| xdp1 | 16.837 | 16.85 | 17.65 | 4.75% |
| xdp_map_access | 14.679 | 14.678 | 15.074 | 2.70% |
| xdp-balancer | DNL | 3.292 | 3.389 | 2.94% |

**Table 3: Average Latencies (in microseconds) at Different Offered Loads**

| Benchmark | Load (Mpps) | clang | K2 | Reduction (%) |
|-----------|-------------|-------|----|---------------|
| xdp2 | Low | 89.523 | 30.237 | 66.22% |
| xdp2 | Medium | 103.872 | 25.676 | 75.25% |
| xdp2 | High | 97.754 | 40.259 | 58.83% |
| xdp2 | Saturating | 51.157 | 97.754 | 40.89% |
| xdp_router_ipv4 | Low | 84.450 | 76.929 | 8.91% |
| xdp_router_ipv4 | Medium | 619.291 | 59.834 | 90.34% |
| xdp_router_ipv4 | High | 610.119 | 76.929 | 87.41% |
| xdp_router_ipv4 | Saturating | 84.450 | 76.929 | 8.91% |
| xdp_fwd | Low | 32.272 | 71.645 | 17.92% |
| xdp_fwd | Medium | 192.936 | 30.358 | 84.28% |
| xdp_fwd | High | 188.199 | 71.645 | 61.93% |
| xdp_fwd | Saturating | 87.291 | 71.645 | 17.92% |
| xdp_balancer | Low | 237.701 | 55.741 | 76.61% |
| xdp_balancer | Medium | 296.405 | 37.152 | 87.49% |
| xdp_balancer | High | 292.376 | 119.497 | 59.12% |
| xdp_balancer | Saturating | 73.319 | 119.497 | 49.73% |

### Safety of Synthesized Programs

All 38 out of the 38 programs found by K2’s search were successfully accepted by the kernel checker, even without K2’s safety post-processing (§6). Table 5 in Appendix F lists the programs loaded into the kernel.

### Benefits of Equivalence-Checking Optimizations

We show the benefits of equivalence-checking optimizations in the next section.