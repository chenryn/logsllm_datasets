Mapping Functions. We use three functions to map QoS metrics to user
QoE: speciﬁcally, a linear 1(·) function, a logarithm function of the form of (2),
and an exponential function of the form of (3). While the rationale behind (2)
and (3) come from the Weber-Fetchner law and the IQX hypothesis, we stress
that many works still directly compare PLT statistics, which is analogous to a
simplistic linear mapping. We carefully calibrate the model parameters using
the non-linear least squares Marquardt-Levenberg algorithm. In Fig. 3(b) we
contrast how these diﬀerent mappings correlate to QoE for a relevant subset of
the QoS metrics: speciﬁcally, we select the most widely used metric (PLT) as well
as those metrics exhibiting the worst (DOM) and the best (IIAAT F ) correlation
with user QoE. We also compare results with the reference obtained by default
ITU-T models for slow/medium/fast network conditions using the PLT metric.
Among the default ITU-T models, the model for medium networking condi-
tions shows the stronger correlation to QoE in our dataset. This can be explained
by users’ expectation of network performance, since the experimental network
conditions mirror that of Internet Web access. It is worth noting that the uncal-
ibrated ITU-T medium model is still better than a linear mapping of PLT to
QoE. We observe across all metrics in our dataset that the exponential mapping
is superior to logarithmic, which is in turn superior to simply using a linear
mapping to estimate QoE. It is easy to observe that our proposed metrics based
on the AATF time (particularly, IIAAT F ) consistently yields the strongest cor-
relation with MOS, across all functions.
4.3 Machine Learning
We evaluate diﬀerent machine learning techniques to learn regression models that
predict user QoE. Note that the learned function f(·) maps a vector x to MOS,
compared to the expert models where x is a scalar metric. We evaluate the per-
formance of three state-of-the-art machine learning algorithms: Support Vector
Regression (SVR), Classiﬁcation And Regression Tree (CART), and AdaBoost
with CART (BOOST) implemented using the sci-kit learn Python module.
40
D. N. da Hora et al.
Parameter Tuning. We tune the hyper-parameters of the ML algorithms using
grid optimization. Namely, we select the best combination of parameters  ∈
[10−2, 1], γ ∈ [10−3, 10] and C ∈ [1, 104] for SVR, minimum number of samples
per leaf ∈ [1, 10] and tree depth ∈ [1, 10] for CART and BOOST, and number
of boosted trees ∈ [10, 103] for BOOST. Grid optimization outputs  = 0.3,
γ = 10−3, and C = 104 for SVR, and suggests 4 samples per leaf and tree depth
of 2 for both CART and BOOST, and 102 trees for BOOST.
Feature Selection. We employ three strategies for building predictors using
diﬀerent sets of features from our dataset. The ﬁrst baseline strategy considers
as features the 9 raw metrics deﬁned in Sect. 4.2. The second strategy feeds the
ML model with the output of the 3 expert models computed on the 9 raw met-
rics, for an extended set of 27 features (notice that since one mapping function is
linear, there are 18 additional features beyond the raw ones). Finally, as perfor-
mance upper bound, we perform an exhaustive search of feature subsets from the
extended set, to select the combination that minimizes the Root Mean Squared
Error (RMSE) of the predictor. The selected combinations include few features
(3–5 out of 9) that vary across ML algorithms, although the sets consistently
include II P LT (all algorithms) AATF and IIAAT F (all but one).
Results. We evaluate ML predictors using leave-one-out cross-validation.
Figure 4 shows the (a) correlation and (b) RMSE between MOS and the ML
model, for the full set of algorithms and feature selection strategies. We also
report, as a reference, the performance of the best expert model (exponential,
II AAT F ), a traditional model (logarithmic, P LT ), and the worst expert model
(linear, DOM). Similar considerations hold for both correlation (the higher
the better) or RMSE (the lower the better): BOOST presents a small advan-
tage over CART trees, although SVR outperforms them both. Yet, the picture
clearly shows that SVR results are on par with the best expert model, with a
small advantage arising in the optimistic case of an exhaustive search for feature
selection.
Fig. 4. Comparison of ML algorithm using diﬀerent feature sets against reference
expert models, for correlation and RMSE metrics
Narrowing the Gap Between QoS Metrics and Web QoE
41
4.4 Discussion
We believe that there is further room for improvement. Notably, we argue that,
due to the variety of Webpages, the attempt to build a one-size-ﬁt-all model is
doomed to fail. To show this, we report in Fig. 5 an extreme example, where
(a) we build a model per Webpage and (b) contrast the RMSE results in the
per-page vs. all-pages model cases: it is immediate to see that RMSE drastically
decreases under ﬁne grained models – the gap is comparably larger than what
could be reasonably achieved by further reﬁning the metrics deﬁnition, or by the
use of more complex expert (or learned) models. Clearly, given the sheer number
of Webpages, it would be highly unrealistic to attempt to systematically build
such ﬁne-grained models. At the same time, we believe that due to the high skew
of Web content, it would be scalable to (i) build per-page models for only very
popular pages (e.g. the top-1000 Alexa) and (ii) build per-class models for the
rest of pages, by clustering together pages with similar characteristics. Whereas
our dataset currently includes few pages to perform a full-blown study, we believe
that crowdsourcing eﬀorts such as Gao et al. [15] and systematic share of dataset
can collectively assist the community to achieve this goal.
(a) Black: one model for all pages, Gray:
one model per page
(b) Lines: one model for all pages, Bars:
one model per page
Fig. 5. Discussion: one model for all pages vs. one model per page
5 Conclusions
This paper narrows the gap between QoS and QoE for Web applications. Our
contributions are, ﬁrst, to motivate, deﬁne and implement a simple yet eﬀective
method to compute an Approximated ATF time (AATF) [5], which is also use-
ful to narrow the time-horizon of time-integral metrics [15]. Second, we carry
on a large campaign to collect a dataset of nearly 9,000 user subjective feed-
back, which we use for our analysis and make available to the community [28].
Finally, we systematically compare expert vs. data-driven models based on a set
of QoS metrics, which include the ATF time approximation and variants. In a
nutshell, our results suggest that whereas considering PLT metric with linear
mapping should be considered a discouraged practice. Using (i) an exponential
IQX mapping, (ii) over time-integral metrics considering ByteIndex progress of
42
D. N. da Hora et al.
image-content only, and (iii) narrowing the time-horizon to the AATF time, pro-
vides a sizeable improvement of Web QoE estimation. Finally, we found that (iv)
calibrated expert models can provide estimations on par with state-of-the-art ML
algorithms.
Acknowledgments. We are grateful to our shepherd Mike Wittie and to the anony-
mous reviewers, whose useful comments helped us improving our work. This work
has been carried out at LINCS (http://www.lincs.fr) and beneﬁted from support of
NewNet@Paris, Ciscos Chair “Networks for the Future” at Telecom ParisTech
and the EU Marie curie ITN program METRICS (grant no. 607728).
References
1. https://googlewebmastercentral.blogspot.fr/2010/04/using-site-speed-in-web-
search-ranking.html
2. http://googleresearch.blogspot.fr/2009/06/speed-matters.html
3. https://sites.google.com/a/webpagetest.org/docs/using-webpagetest/metrics/
speed-index
4. Alexa Internet Inc. http://www.alexa.com
5. Approximate ATF chrome extension. https://github.com/TeamRossi/ATF
6. Bampis, C.G., Bovik, A.C.: Learning to predict streaming video QoE: distortions,
rebuﬀering and memory. CoRR, abs/1703.00633 (2017)
7. Belshe, M., Peon, R., et al.: Hypertext Transfer Protocol Version 2 (HTTP/2).
RFC 7540 (2015)
8. Bocchi, E., De Cicco, L., et al.: Measuring the quality of experience of web users.
In: ACM SIGCOMM CCR (2016)
9. Bocchi, E., De Cicco, L., Mellia, M., Rossi, D.: The web, the users, and the MOS:
inﬂuence of HTTP/2 on user experience. In: Kaafar, M.A., Uhlig, S., Amann, J.
(eds.) PAM 2017. LNCS, vol. 10176, pp. 47–59. Springer, Cham (2017). https://
doi.org/10.1007/978-3-319-54328-4 4
10. Brutlag, J., Abrams, Z., et al.: Above the fold time: Measuring web page perfor-
mance visually (2011)
11. Butkiewicz, M., Madhyastha, H.V., et al.: Characterizing web page complexity and
its impact. IEEE/ACM Trans. Netw. 22(3), 943 (2014)
12. Charonyktakis, P., Plakia, M., et al.: On user-centric modular QoE prediction
for VoIP based on machine-learning algorithms. IEEE Trans. Mob. Comput. 15,
1443–1456 (2016)
13. Erman, J., Gopalakrishnan, V., et al.: Towards a SPDY’ier mobile web? In: ACM
CoNEXT, pp. 303–314 (2013)
14. Fiedler, M., Hoßfeld, T., et al.: A generic quantitative relationship between quality
of experience and quality of service. IEEE Netw. 24(2), 36 (2010)
15. Gao, Q., Dey, P., et al.: Perceived performance of top retail webpages in the wild:
insights from large-scale crowdsourcing of above-the-fold QoE. In: Proceedings of
ACM Internet-QoE Workshop (2017)
16. Google: SPDY, an experimental protocol for a faster web. https://www.chromium.
org/spdy/spdy-whitepaper
17. ITU-T: Estimating end-to-end performance in IP networks for data application
(2014)
Narrowing the Gap Between QoS Metrics and Web QoE
43
18. Kelton, C., Ryoo, J., et al.: Improving user perceived page load time using gaze.
In: Proceedings of USENIX NSDI (2017)
19. Langley, A., Riddoch, A., et al.: The QUIC transport protocol: design and internet-
scale deployment. In: Proceedings of ACM SIGCOMM (2017)
20. Minutes of TPAC Web Performance WG meeting. https://www.w3.org/2016/09/
23-webperf-minutes.html
21. Qian, F., Gopalakrishnan, V., et al.: TM3: ﬂexible transport-layer multi-pipe mul-
tiplexing middlebox without head-of-line blocking. In: ACM CoNEXT (2015)
22. Schatz, R., Hoßfeld, T., Janowski, L., Egger, S.: From packets to people: quality
of experience as a new measurement challenge. In: Biersack, E., Callegari, C.,
Matijasevic, M. (eds.) Data Traﬃc Monitoring and Analysis. LNCS, vol. 7754, pp.
219–263. Springer, Heidelberg (2013). https://doi.org/10.1007/978-3-642-36784-
7 10
23. Spetebroot, T., Afra, S., et al.: From network-level measurements to expected
quality of experience: the Skype use case. In: M & N Workshop (2015)
24. Varvello, M., Blackburn, J., et al.: EYEORG: a platform for crowdsourcing web
quality of experience measurements. In: Proceedings of ACM CoNEXT (2016)
25. Varvello, M., Schomp, K., et al.: Is The Web HTTP/2 Yet?. In: Proceedings of
PAM (2016)
26. Wang, X.S., Balasubramanian, A., et al.: How speedy is SPDY? In: USENIX NSDI,
pp. 387–399. USENIX Association, Seattle (2014)
27. Wang, X.S., Krishnamurthy, A., et al.: Speeding up web page loads with Shandian.
In: USENIX NSDI (2016)
28. Web QoE dataset. https://newnet.telecom-paristech.fr/index.php/webqoe/