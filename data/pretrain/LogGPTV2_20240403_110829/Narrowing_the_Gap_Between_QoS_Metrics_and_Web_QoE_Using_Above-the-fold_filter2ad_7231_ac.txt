### Mapping Functions
We utilize three distinct functions to map Quality of Service (QoS) metrics to user Quality of Experience (QoE): a linear function, a logarithmic function as defined in Equation (2), and an exponential function as defined in Equation (3). The rationale for using the logarithmic and exponential functions is derived from the Weber-Fechner law and the IQX hypothesis, respectively. However, it is important to note that many studies still rely on directly comparing Page Load Time (PLT) statistics, which is akin to a simplistic linear mapping. We meticulously calibrate the model parameters using the non-linear least squares Marquardt-Levenberg algorithm.

In Figure 3(b), we compare how these different mappings correlate with QoE for a relevant subset of QoS metrics. Specifically, we select the most widely used metric (PLT) and those metrics that exhibit the worst (DOM) and best (IIAATF) correlation with user QoE. Additionally, we compare our results with the reference models provided by ITU-T for slow, medium, and fast network conditions using the PLT metric. Among the ITU-T models, the one for medium network conditions shows the strongest correlation with QoE in our dataset. This can be attributed to users' expectations of network performance, as the experimental network conditions mirror typical Internet Web access. It is worth noting that even the uncalibrated ITU-T medium model outperforms a simple linear mapping of PLT to QoE. Across all metrics in our dataset, the exponential mapping consistently outperforms the logarithmic mapping, which in turn outperforms the linear mapping for estimating QoE. Our proposed metrics based on the AATF time, particularly IIAATF, consistently yield the strongest correlation with Mean Opinion Score (MOS) across all functions.

### Machine Learning
We evaluate various machine learning techniques to develop regression models that predict user QoE. The learned function \( f(\cdot) \) maps a vector \( x \) to MOS, as opposed to expert models where \( x \) is a scalar metric. We assess the performance of three state-of-the-art machine learning algorithms: Support Vector Regression (SVR), Classification and Regression Tree (CART), and AdaBoost with CART (BOOST), implemented using the scikit-learn Python module.

#### Parameter Tuning
We tune the hyper-parameters of the ML algorithms using grid optimization. For SVR, we select the best combination of parameters \( \epsilon \in [10^{-2}, 1] \), \( \gamma \in [10^{-3}, 10] \), and \( C \in [1, 10^4] \). For CART and BOOST, we optimize the minimum number of samples per leaf \( \in [1, 10] \) and tree depth \( \in [1, 10] \). For BOOST, we also tune the number of boosted trees \( \in [10, 10^3] \). The optimal parameters are: \( \epsilon = 0.3 \), \( \gamma = 10^{-3} \), and \( C = 10^4 \) for SVR; 4 samples per leaf and tree depth of 2 for both CART and BOOST; and 100 trees for BOOST.

#### Feature Selection
We employ three strategies for building predictors using different sets of features from our dataset:
1. **Baseline Strategy**: Uses the 9 raw metrics defined in Section 4.2.
2. **Extended Set Strategy**: Feeds the ML model with the output of the 3 expert models computed on the 9 raw metrics, resulting in an extended set of 27 features.
3. **Exhaustive Search Strategy**: Performs an exhaustive search of feature subsets from the extended set to minimize the Root Mean Squared Error (RMSE) of the predictor. The selected combinations include a few features (3â€“5 out of 9) that vary across ML algorithms, but consistently include II P LT, AATF, and IIAATF.

#### Results
We evaluate the ML predictors using leave-one-out cross-validation. Figure 4 shows the (a) correlation and (b) RMSE between MOS and the ML model for the full set of algorithms and feature selection strategies. We also report, as a reference, the performance of the best expert model (exponential, IIAATF), a traditional model (logarithmic, PLT), and the worst expert model (linear, DOM). Both correlation (the higher the better) and RMSE (the lower the better) indicate that BOOST has a slight advantage over CART, although SVR outperforms both. However, SVR results are on par with the best expert model, with a small advantage arising in the optimistic case of an exhaustive search for feature selection.

### Discussion
We believe there is further room for improvement. Given the variety of web pages, attempting to build a one-size-fits-all model is likely to fail. To illustrate this, we present an extreme example in Figure 5, where (a) we build a model per web page and (b) contrast the RMSE results in the per-page versus all-pages model cases. It is evident that RMSE drastically decreases under fine-grained models, with a gap larger than what could be achieved by refining metrics or using more complex models. While it is unrealistic to build such fine-grained models for all web pages, it is feasible to (i) build per-page models for very popular pages (e.g., top-1000 Alexa) and (ii) build per-class models for the rest by clustering pages with similar characteristics. Although our current dataset includes only a few pages, we believe that crowdsourcing efforts and systematic data sharing can help achieve this goal.

### Conclusions
This paper aims to narrow the gap between QoS and QoE for web applications. Our contributions include:
1. Motivating, defining, and implementing a simple yet effective method to compute Approximated ATF time (AATF), which is also useful for narrowing the time-horizon of time-integral metrics.
2. Conducting a large-scale campaign to collect a dataset of nearly 9,000 user subjective feedback, which we use for our analysis and make available to the community.
3. Systematically comparing expert and data-driven models based on a set of QoS metrics, including the AATF time approximation and variants.

Our results suggest that while using PLT with a linear mapping should be discouraged, using (i) an exponential IQX mapping, (ii) time-integral metrics considering ByteIndex progress of image content, and (iii) narrowing the time-horizon to the AATF time, provides a significant improvement in web QoE estimation. Finally, we found that (iv) calibrated expert models can provide estimations on par with state-of-the-art ML algorithms.

### Acknowledgments
We are grateful to our shepherd Mike Wittie and the anonymous reviewers for their valuable comments. This work was carried out at LINCS and benefited from the support of NewNet@Paris, Cisco's Chair "Networks for the Future" at Telecom ParisTech, and the EU Marie Curie ITN program METRICS (grant no. 607728).

### References
[References listed as in the original text]

This version of the text is more structured, coherent, and professional, with improved clarity and flow.