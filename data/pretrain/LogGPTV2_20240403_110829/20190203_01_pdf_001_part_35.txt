### Recommendation for Snapshot Replication

It is recommended to perform a snapshot replication on any master node where the filtering criteria have been modified. 

1. **Confirmation Box:**
   - Click the **Ok** button in the confirmation box to proceed with updating the filter rule selections.
   - Click the **Cancel** button to return to the **Filter Rules** tab if you need to modify your filter rule selections.

   **Figure 6-67: Change Filter Rule Confirmation**

   Copyright © 2010 - 2018 EnterpriseDB Corporation. All rights reserved. 373
   EDB Postgres Replication Server User’s Guide

2. **Update Confirmation:**
   - If you clicked **Ok** in the previous step, a confirmation message will appear indicating that the filter rules were successfully updated.

   **Figure 6-68: Successful Update of Filter Rules**

   - If you clicked **Cancel** in the previous step, the **Filter Rules** tab will reopen, allowing you to modify your filter rule selections by repeating Step 4 or abort the updates by clicking the **Cancel** button in the **Filter Rules** tab.

3. **Snapshot Replication:**
   - It is strongly recommended to perform a snapshot replication on the master node where the filtering criteria have been changed. This ensures that the content of the master node tables is consistent with the updated filtering criteria. For more information on performing a snapshot replication, see Section 6.5.1.

   **Note:** The master definition node, which serves as the source of table content for the snapshot, should contain a superset of all the data in the other master nodes of the multi-master replication system. This ensures that the target of the snapshot receives all the data that satisfies the updated filtering criteria. Conversely, if the master definition node contains only a subset of the data, a snapshot to another master node may not result in the complete set of required data.

### Switching the Master Definition Node

After setting up the multi-master replication system, you can switch the role of the master definition node to another master node.

#### Steps:

1. **Ensure Publication Server is Running:**
   - Ensure that the publication server, which is the parent of the master nodes in the replication system, is running and registered in the xDB Replication Console. For instructions on starting and registering a publication server, see Section 5.2.1.

2. **Select the New Master Definition Node:**
   - Select the **Publication Database** node corresponding to the master node you wish to set as the new master definition node.

   **Figure 6-69: Selecting the Master Node to Set as the Master Definition Node**

3. **Set as MDN:**
   - Right-click the **Publication Database** node and choose **Set as MDN**.

   **Figure 6-70: Setting the Master Definition Node**

4. **Confirm the Change:**
   - In the **Set as MDN** confirmation box, click the **Yes** button.

   **Figure 6-71: Set as MDN Confirmation**

5. **Verification:**
   - The selected master node is now the master definition node.

   **Figure 6-72: Database Promoted to Master Definition Node**

6. **Property Window:**
   - The value **Yes** in the **MDN** field of the **Property** window indicates that this database is the master definition node.

   **Note:** The new master definition node will be moved to the top of the replication tree in the xDB Replication Console.

   **Figure 6-73: Master Definition Node (MDN) Indicated by ‘Yes’ in the Property Window**

7. **Synchronization Replication:**
   - Perform a synchronization replication to ensure that the new master definition node is synchronized with the other master nodes. For directions on performing a synchronization replication, see Section 6.5.2.

### Ensuring High Availability

In a multi-master replication system, master nodes can reside on separate physical hosts. If any master node goes offline, the remaining master nodes continue to synchronize transactions, ensuring consistency. When an offline master node is brought back online, pending transactions are synchronized with the other master nodes, ensuring no transaction data is lost.

Each master node serves as a backup for the others, providing consistent publication data to applications. The complete configuration information (control schema and its objects) is stored in each publication database, ensuring that the configuration information is always available even if a master node goes offline.

#### Significance of the Controller Database

The controller database has special significance in the operation of the replication system. At any given time, one of the publication databases is designated as the controller database. This can be identified in the xDB Replication Console or the xDB Replication Configuration file.

- **Controller Database Identification:**
  - In the xDB Replication Console, the **Controller database** field in the **Property** window is set to **Yes** if the master node is the current controller database.
  - In the xDB Replication Configuration file, the authentication and connection parameters are set to the controller database. For more information, see Section 2.3.1.3.

- **Configuration Updates:**
  - Any changes made to the replication system configuration using the xDB Replication Console or CLI are first updated in the control schema of the controller database and then replicated to the other publication databases.

- **Replication History:**
  - Replication history may take longer to replicate from the controller database to the other publication databases. If access to the controller database fails, some replication history may be lost. For more information on replication history, see Section 7.4.

- **Maintaining Access:**
  - It is crucial to maintain access to the controller database whenever the replication system is in use. If access cannot be maintained, such as during scheduled maintenance or unexpected network/system problems, automatic switchover or manual switching of the controller database can be performed.

#### Automatic Switchover of the Controller Database

If the controller database becomes inaccessible, the xDB Replication Server automatically connects to another online publication database to act as the new controller database. The controller database authentication and connection information is updated in the xDB Replication Configuration file, ensuring no disruption in the operation of the xDB Replication Server.

#### Switching an Active Controller Database

If the database server hosting the controller database needs to be taken offline for maintenance, the controller database role can be switched to another publication database. This can be done using the xDB Replication Console or the xDB Replication Server CLI. After the switch, the former controller database can be taken offline, and pending transactions will be applied when it is brought back online.

#### Restarting with an Alternate Controller Database

If the current controller database is inaccessible, the xDB Replication Configuration file can be edited to designate a new controller database. After modifying the file, restart the publication server and subscription server if necessary. For instructions on starting the servers, see Sections 5.2.1 and 5.3.1.

### Optimizing Performance

Various publication server configuration options are available to optimize the performance of multi-master replication systems. Most configuration options for single-master replication systems are also applicable to multi-master systems, except for database-specific options like those for Oracle. For a detailed explanation of how to set these options, see Section 10.4.1.

#### Additional Configuration Options for Multi-Master Replication Systems

- **uniquenessConflictDetection:**
  - Determines whether uniqueness conflicts are detected at data load time or deferred to when data is applied against a target master node. Possible values are `EAGER` and `LAZY`. Set to `EAGER` if there is a high probability of duplicate inserts across master nodes.
  - Default value is `LAZY` when the number of master nodes is two. For more than two master nodes, conflict detection is always performed in `EAGER` mode.

- **skipConflictDetection:**
  - Controls whether to skip conflict detection during synchronization replication. The default is `false` and should be changed only if the probability of data conflict across master nodes is zero. Setting this to `true` can improve replication time if each master node operates on an independent set of data.

- **deadlockRetryCount:**
  - Controls the number of times the publication server attempts to retry application of changes after detecting a deadlock. Set to `0` to turn off this option. The default value is `1`.

- **deadlockWaitTime:**
  - Sets the wait time in milliseconds before the publication server attempts to retry application of changes on the target master node. The default value is `1000` milliseconds.

### Common Operations

This chapter describes configuration and maintenance operations common to both single-master and multi-master replication systems. The xDB Replication Console is used to illustrate the steps, but the same operations can be performed using the xDB Replication Server Command Line Interface (CLI). For CLI commands, see Chapter 8.

#### Selecting Tables with the Wildcard Selector

When selecting tables for creating a publication or adding/deleting tables from an existing publication, the wildcard selector can be used to simplify the process, especially when dealing with a large number of tables.

- **Wildcard Selector Patterns:**
  - Pattern matching is used to filter tables based on a pattern similar to the SQL `LIKE` clause. Wildcards include `?` (single-character), `%` (multi-character), `[abc...]` (list), `[a-d]` (range), and `!pattern` (exclusive pattern).

- **Using the Wildcard Selector:**
  - Invoke the Wildcard Selector dialog box from the calling dialog box (e.g., Create Publication, Add Tables, Remove Tables). Use the patterns to filter and select the desired tables.

For more details on pattern definitions and examples, refer to the help screen in the Wildcard Selector dialog box.

Copyright © 2010 - 2018 EnterpriseDB Corporation. All rights reserved.