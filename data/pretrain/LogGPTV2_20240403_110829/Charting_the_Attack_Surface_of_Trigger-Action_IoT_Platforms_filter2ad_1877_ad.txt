Binary
Binary
Binary
Binary
Binary
Binary
Binary
Binary
Binary
Binary
Description
Semantic similarity scores of ⟨A, T ⟩ verb-pairs.
Semantic similarity scores of ⟨A, T ⟩ object-pairs.
Is the trigger verb a synonym of the action verb?
Is trigger verb (move) a hypernym1 of action verb (walk)?
Can the trigger verb (eat) be caused by the action verb (feed)?
Does the trigger verb (wake) entail2 the action verb(sleep)?
Is the trigger object a synonym of the action object?
Is trigger object (publication) a hypernym of action object (book)?
Is the trigger object (lock) a meronym3 of action object (door)?
Is the trigger object (door) a holonym4 of the action object (lock)?
Does action object property match the trigger object property?
Do the verb-particles5 match between action verb and trigger verb,
if the verbs are multi-word expression (turn of )?
Usually, root deines the main task while Direct Object and Com-
pound together deine the object, in addition to Modiiers that de-
ine the properties of the object (Figure 9). However, sometimes
the clausal complement to the root verb describes the main task
instead. For example, in the trigger description łThis Trigger ires
every time an audio event is detectedž, the root verb łirež is not
the main task, but łdetectž is, which is a clausal complement to
łirež. Moreover, łaudio eventž is a passive nominal subject to łdetectž
instead of the Direct Object relationship. So, there are a few other
dependency relations, e.g., Nominal and Passive Nominal subjects
and Clausal Complements [21], that we track to detect syntactic ele-
ments in order to accommodate the variability in unstructured text.
These grammatical dependencies comprise the syntactic elements
of interest for the remainder of our analysis.
After performing POS tagging, parsing and extracting the rel-
evant syntactic elements, we also attempt to detect and exclude
the Named Entities [9] from each text description. In preliminary
experimentation, we found that this was necessary because named
entities appearing in extracted object descriptions often seemed to
encode similarity between dissimilar objects. For example, WeMo
Humidiier and WeMo Lighting are likely to be unrelated in spite
of a shared Named Entity WeMo. We therefore decide to exclude
named entities to avoid bias when calculating object similarity.
Semantic Feature Extraction. After extracting the relevant
6.1.2
text elements, we then encode the semantic relationship between
the syntactic elements of the action and trigger as a vector of (contin-
uous and binary) numerical features. These features are calculated
by processing the syntactic elements of A and T in a pairwise fash-
ion (i.e., verb-verb, object-object). Intuitively, if the elements of the
trigger and action description have related semantics, it is likely
that there exists a dependency between them. A summary of the
feature vector is given in Table 2.
Continuous Feature Computation: We leverage the Word Vector
Embedding technique to calculate Verb Similarity and Object Simi-
larity features, which maps words from a vocabulary into vectors of
real numbers. These vector representations are able to encode ine-
grained semantic regularities using vector arithmetic [64]. Based
1Hypernym: generic term used to designate a class of speciic instances.
2Entailment: the trigger verb cannot happen unless the action verb happens.
3Meronym: a constituent part, the substance of, or member of some object.
4Holonym: The name of the object of which the meronym names a part.
5The verb-particles, i.e., Of or On are tagged diferently by the POS-Tagger than On
as a preposition.
on vector arithmetic, we then use the word embedding tools (e.g.,
word2vec [16], GloVe [18]) to calculate a real number score repre-
senting the semantic similarity between the two syntactic elements.
We calculate pairwise semantic similarity scores for each pair of
verbs and objects extracted from A and T . To calculate the simi-
larity score for multi-word elements, we calculate a phrase vector
as the average of the vectors of the component words [47]. Let
phrase P be composed of words (w1, w2, . . . , wn ) with vector em-
beddings (uw1 , uw2 , . . . , uwn ). The vector for P is then deined as:
uP := 1
i =1 uwi . Finally, the semantic similarity score for the
action and trigger phrases is calculated as the cosine similarity
between the two vectors.
n Pn
Binary Semantic Feature Computation: We are ultimately inter-
ested in speciic causal relationships between actions and triggers,
but our continuous features relect any relationship between the
syntactic elements. As a result of this broader focus, the similarity
scores may underweight the relationship between two elements
within the context of IoT; for example, word2vec(lock, door) with the
Wikipedia-trained model we used yields a middling similarity score
of 0.53, but in the IoT domain it is highly likely that a change in lock
state suggests a change in door state. To correct for this, we also cal-
culate a series of binary features for each action trigger pair, which
we deine to capture generic semantic relationships that we found
were commonly relevant to action-trigger lows during manual
coding of our IFTTT dataset. For example, multi-word expressions
(e.g., łturn onž) are commonly found in descriptions, but the verb
particle on is often tagged as a preposition by the POS tagger, so
we introduce a feature that tests if the verb particles match. These
features are calculated using the lexical database Babelnet [69],
annotated and interlinked with semantic relations.
6.2 Classiication Problem
We cast information low detection as a supervised binary classiica-
tion problem between an action and trigger pair ⟨A,T ⟩, where both
T and A belong to the same service S. Each ⟨A,T ⟩ pair is labeled
such that 1 signiies the existence of a low from A to T while 0
signiies the contrary. We divide the dataset into training and test
sets by service so that the classiier is unable to leverage service-
speciic semantics when classifying test samples. We use 4 diferent
classiication algorithms - Support Vector Machine, Random Forest,
Multilayer Perceptron and Logistic Regression. We use Grid Search
with Cross Validation to search the hyperparameter space to op-
timize the classiier performance for a high recall (i.e., maximize
proportion of actual positives identiied correctly) value. The deci-
sion of recall optimization comes from the intuition that it is safer
to admit false lows than exclude true lows.
One issue with our dataset is that it is highly imbalanced because
there are more spurious non-lows than true lows, i.e., the number
of positive examples is far less than the number of negative exam-
ples. We use two diferent techniques to combat this problem. First,
we use class-weights inversely proportional to the percentage of
class examples in the training set. This assigns a higher misclassii-
cation penalty to training instances of the minority class. Second,
we use Random Oversampling to balance the data by randomly
oversampling the minority class. We do not use undersampling of
majority class since we have a limited sized dataset.
Session 7A: Internet of ThingsCCS ’19, November 11–15, 2019, London, United Kingdom1446Table 3: A summary of the classiication performance (percentages).
Classiier
Accuracy
AUC
Recall
SVM (RBF Kernel)
Random Forest
Multilayer Perceptron
Logistic Regression
80.2
85.7
86.8
83.1
79.8
80.5
82.7
79.5
90.7
88.2
88.6
84.4
FP
Rate
22.3
15.2
16.4
20.4
FN
Rate
9.2
11.8
11.4
15.6
False Flow
Reduction
72
78.7
77.6
74.3
6.3 Classiication Performance
Based on the methodology described above, we now evaluate the
overall accuracy of our NLP-aided information low analysis tool.
6.3.1 Experimental Setup. Our feature extraction tool was imple-
mented using the Stanford CoreNLP [12] library for POS tagging
during syntactic element extraction, the FastText [27] project’s
Wikipedia dataset word vectors to calculate similarity scores, and
Babelnet [69] to extract binary semantic features.
The classiier’s training and test sets were derived by randomly
selecting 512 services from our IFTTT dataset, which is described
in greater detail in Section 7.1. We divided this into 374 services
for training and 138 services for testing. Because we were unable
to purchase, conigure, and run all of the devices and services on
IFTTT, our labeled ground truth is based not on applet invocations,
but on manual coding by two of the authors; we consider the poten-
tial limitations of this approach in Section 8. Each coding decision
entailed examining the text descriptions of the service to determine
whether it was possible for a given action to lead to the invocation
of a given trigger. The existence or absence of a low was usually
obvious; occasionally the coders needed to look up the functionality
of a service if they were not familiar with it. Manual coding en-
tailed an author spending approximately 40 hours manually coding
the intra-service lows, followed by a second author spending 5
hours on reliability coding [82]. for a total of 45 hours of human
efort. There were a small number (less than 10 in total out of 512
services) of discrepancies identiied by the reliability coder, which
were easily resolved between the two coders through a brief discus-
sion. While this strategy for deriving intra-service lows is already
tedious, we argue that it will shortly become entirely untenable
as IoT platforms continue to grow in popularity. There is already
evidence that this expansion of IFTTT is underway ś during a 5
month window in 2017, the platforms services, triggers and actions
grew by 11%, 31%, and 27% respectively [63].
6.3.2 Results. We used the training set to train the classiier and
the test set to compute the accuracy and AUC score. Then we fed
the entire training set and test set together to our tool and computed
recall, error rates and the amount of false low reduction. These
results are summarized in Table 3. We compare the performance
of our NLP-aided tool using diferent classiication models against
the baseline naïve strategy used in our preliminary experiments,
which conservatively assumes a low exists between all actions and
triggers of a service. Compared to this baseline which generates 6637
lows, our NLP-based tool with SVM classiier minimizes the FN rate
to 9.2% while causing an overall reduction in graph complexity of
72%. This inding demonstrates that an NLP-based approach is a
irst step towards overcoming the opacity of IoT platforms.
6.3.3 Discussion. In light of the large number of false dependencies
that exist using the naïve information low strategy, we feel that
our error rates are promising. Here, a false positive signiies that
our attack surface model is overly conservative, encoding a low
between two rules that does not actually exist, while a false negative
fails to identify a legitimate low.
We identify two error sources that can be directly attributed to
our methodology. First, our approach depends on an accurate text
description of the rule behavior; in cases where the trigger/action
do not contain verbs that explain the behavior, we are unable to
identify the low (True Error). In a few cases, the classiier’s decision
boundary detected ⟨A,T ⟩ pairs with high verb or object similarity
as a non-low, or pairs with lower similarity scores as a low (Clas-
siication Error). However, we did not want to overit our model
to the dataset, so we restrained from ine-tuning the classiier to
address this.
The larger sources of error in our system can be attributed to
limitations in the underlying NLP tools we employed. (1) Text de-
scriptions that generated complex syntax trees (with uncommon
grammatical relations) led to false positives because we were unable
to track the language elements indicating a non-low (Syntax Tree
Complexity). (2) The POS-tagger sometimes labeled words incor-
rectly, leading to errors; for example, the łonž in łTurn onž might
be detected as preposition instead of a verb-particle, third-person
verbs sometimes detected as plural nouns, or the word everytime
is detected as a verb (POS Tagger Error). (3) Parsing errors by the
CoreNLP parser module produced incorrect dependency trees, lead-
ing to incorrect feature vectors (Dependency Parsing Error). (4) De-
scriptions that contained complex object modiiers led to some false
positives, e.g., łThis Action will create a regular post on your Blogger
blogž and łThis Trigger ires every time you publish a new post on
your Blogger blog with a speciic labelž (Complex Object Modiier). (5)
Word embeddings often assign high similarity score to contextually
similar verb pairs, for example łopen-closež, łactivate-deactivatež,
thus confusing the classiier to record a false positive. (6) A signii-
cant source of error was that the word embedding models we used
were not trained for the IoT domain, but a more general vocabulary
(i.e., Wikipedia). This was especially problematic when novel words
(e.g., łcool-modež) were encountered.
These error sources could potentially be addressed in future work
through advancements in these techniques or by training NLP tools
speciically for the IoT domain. Alternately, our method could be
augmented with prediction uncertainty analysis and quantiication
techniques [43] to request human intervention when the classiier
is not conident enough in its prediction.
7 EVALUATION
Having generated an information low graph of IoT deployments
using our NLP-aided analysis tool, we are now able to leverage
iRuler to identify inter-rule vulnerabilities within real-world IoT
platforms. In this section, we examine the potential for inter-rule
vulnerabilities within the IFTTT ecosystem.
7.1 Dataset
We conduct our evaluation on a dataset crawled from the IFTTT
website in October 2018 using the methodology introduced by Ur
et al. in [86]. The data we collect is entirely public and includes
only metadata about the published applets and services ś all user
Session 7A: Internet of ThingsCCS ’19, November 11–15, 2019, London, United Kingdom1447s
n
o
i
t
l
a
o
v
f
i
o
#
e
g
a
r
e
v
A
Looping
Conflict
Conflict_2
Reverting
Duplication
Duplication_2
 100
 10
 1
 0.1
 0.01
s
n
o
i
t
l
a
o
v
f
i
o
#
e
g
a
r
e
v
A
Looping
Conflict
Conflict_2
Reverting
Duplication
Duplication_2
 100
 10
 1
 0.1
 0.01
s
n
o