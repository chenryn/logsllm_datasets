---
## Page 285
248
8Chapter 7 Memory
• What processes are blocked waiting on swap-ins?
What memory mappings are being created system-wide?
* What is the system state at the time of an OOM kill?
• What application code paths are allocating memory?
• What types of objects are allocated by applications?
 Are there memory allocations that are not freed after a while? (They could indicate
potential leaks.)
These can be answered with BPF by instrumenting software events or tracepoints for faults and
syscalls; kprobes for kernel memory allocation functions; uprobes for library, runtime, and
application allocators; USDT probes for libc allocator events; and PMCs for overflow sampling
of mermory accesses. These event sources can also be mixed in one BPF program to share context
between different systems.
Memory events including allocations, memory mappings, faults, and swapping, can all be instru
mented using BPF Stack traces can be fetched to show the reasons for many of these events.
Event Sources
Table 7-1 lists the event sources for instrumenting memory.
Table 7-1 Event Sources for Instrumenting Memory
Event Type
Event Source
User memory allocations
uprobes on allocator functions and libc USDT probes
Kernel memory allocations
kprobes on allocator functions and kmem tracepoints
Heap expansions
brk syscall tracepoints
Shared memory functions
syscall tracepoints
Page faults
kprobes, software events, and exception tracepoints
suoge8 a8e
migration tracepoints
Page compaction
compaction tracepoints
vmscan tracepoints
Memory access cycles
PMCs
Here are the USDT probes available in libc:
+ bpftrace -1 usdt:/lib/x86_641inux-gnu/1ibc2.27.so
[.--]
usdt1/lib/x86_641inux-gnu/1ibc=2.27,so:1ibcimemory_nal1opt_arena_max
usdt:/1ib/x85_6411nux=gnu/1ibc=2.27,so:1ibc:menory_na11opt_axena_test
usdt:/lib/x86_641inux-gnu/1ibc=2.27,so:1ibc:memory_tunalble_tcache_nax_bytes
---
## Page 286
7.1Background249
usdit:/1ib/x86_641inux-gnu/1ibc2.27,so:1ibc:memory_tunable_tcache_count
usdit:/1ib/x85_6411nux=gnu/1ibc=2.27,so:1ibc:menory_tunable_tcache_unsorted_11nit
usdt:/1ib/x86_641inux-gnu/1ibc2:27,so:1ibc:memory_na11opt_trim_threshold
usdt:/1ib/x85_6411nux=gnu/1ibc=2.27,so:1ibc:menory_na11opt_top_pad
usdt:/1ib/x86_641inux-gnu/1ibc2.27,so:1ibcimemory_na1lopt_mma_thresho1d
usdt:/1ib/x85_6411nux=gnu/1ibc=2 27, so:1ibc:menory_na11opt_mmap_nax
usdt:/lib/x86_641inux=gnu/1ibc=2.27,so:1ibc:memory_na1lopt_perturb
usdt:/1ib/x85_64=1Lnux=gna/1ibc=2 27,so:1ibc:menoxy_heap_nev
usdt: /1ib/x86_64=1inux=gnu/1ibe=2,27,so:Libcimemory_sbrk_less
usdt:/1ib/x85_6411nux=gnu/1ibc=2 27,so:1ibc:menory_arena_reuse
usdt:/lib/x86_641inux-gnu/1ibc=2.27,so:1ibcimemory_aren.a_reuse_wa.it
xou"eueae/.xoueu:oqTT:os*.Z*zoqtt/nu6xmuTt-998x/qTt/:apsn
usdt1/lib/x86_64-1inux-gnu/libc-2.27,ao:libcimemory_arena_reuse_free_liat
usdt: /1ib/x85_6411nux=gnu/1ibc=2.27,so:1ibc:menory_arena_retxy
usdit:/1ib/x86_641inux-gnu/1ibc2 27,so:1ibc:memory_besp_free
usdt:/1ib/x85_6411nux=gnu/1ibc=2 27, so:1ibc:menory_heap_1e.sa
usdt:/1ib/x86_64=1inux=gnu/1ibc=2. 27,so:1ibc:memory_besp_more
usdt:/lib/x86_641inux=gnu/1ibc=2,27,so:1ibc:memory_na11opt_free_dyn_thresho1ds
usdt: /11b/x86_64-11nux-gnu/1ibc-2,27, so:1ibc:menoxy_sbk_m0re
usdt:/1ib/x85_64=11nux=gnu/1ibc=2.27,so:1ibc:menory_na11oc_retry
usdit:/lib/x86_641inux=gnu/1ibc-2: 27,so:1ibc:memory_memalign_retr)
usdt:/1ib/x86_64=1Lnux=gnu/1ibc=2.27,so:1ibc:menory_rea11oc_zetzy
usdt:/1ib/x86_64=1inux=gnu/1ibc=2 27,so:1Libc:memory_ca11oc_ret.ry
usdt:/11b/x85_6411nux=gnu/1ibc=2, 27,so:1ibc:menory_na11opt
usdt:/1ib/x86_641inux=gnu/1ibc=2 27,so:1ibc:memory_ma11opt_mxfast
These probes provide insight into the internal operation of the libc allocator.
Overhead
As mentioned earlier, memory allocation events can occur millions of times per second. Although
BPF programs are optimized to be fast, calling them millions of times per second can add up
10 times (10x), depending on the rate of events traced and the BPF program used.
to significant overhead, slowing the target software by more than 10%, and in some cases by
To work around this overhead, Figure 7-2 shows which paths are frequent by using bold arrows
and which are infrequent by using lighter arrows. Many questions about memory usage can be
answered, or approximated, by tracing the infrequent events: page faults, page outs, brk() calls,
and mmap( calls. The overhead of tracing these events can be negligible.
One reason to trace the malloc() calls is to show the code paths that led to malloc(). These code
paths can be revealed using a different technique: timed sampling of CPU stacks, as covered in
Chapter 6. Searching for *malloc? in a CPU flame graph is a coarse but cheap way to identify the
p uoung a e o upu no nb uog s u sed apo
---
## Page 287
250
0Chapter 7Memory
The performance of uprobes may be greatly improved in the future (10x to 100x) through
dynamic ibraries involving user-to-user-space jumps rather than kernel traps (see Section 2.8.4
in Chapter 2)
7.1.3 Strategy
If you are new to memory performance analysis, here is a suggested overall strategy to follow:
1. Check system messages to see if the OOM killer has recently killed processes (e.g., using
dmesg(1)
2. Check whether the system has swap devices and the amount of swap in use; also check
whether those devices have active I/O (e.g., using swap(1), iostat(1), and vmstat(1).
3. Check the amount of free memory on the system and system-wide usage by caches
(e-g-, tree(1))
4. Check per-process memory usage (e.g., using top(1) and ps(1).
5. Check the page fault rate and examine stack traces on page faults, which can explain RSS
growth.
6. Check the files that were backing page faults.
7. Trace brk() and mmap() calls for a different view of memory usage.
8. Browse and execute the BPF tools listed in the BPF tools section of this chapter.
9. Measure hardware cache misses and memory accesses using PMCs (especially with PEBS
enabled) to determine functions and instructions causing memory I/O (e.g., using perf(1)
The following sections explain these tools in more detail.
7.2TraditionalTools
Traditional performance tools provide many capacity-based memory usage statistics, including
how much virtual and physical memory is in use by each process and system-wide, with some
breakdlowns such as by process segment or slab. Analyzing memory usage beyond basics such as
the page fault rate required built-in instrumentation for each allocation by the allocation library,
runtime, or application; or a virtual machine analyzer like Valgrind could be used; this latter
approach can cause the target application to run over 10 times slower while instrumented. BPF
tools are more efficient and cost smaller overheads.
Even where they are not sufficient on their own to solve issues, traditional tools can provide clues
to direct your use of BPF tools. The traditional tools listed in Table 7-2 have been categorized here
based on their source and measurement type,
---
## Page 288
7.2 Traditional Tools
251
Table 7-2Traditional Tools
1001
Type
Description
dnesg
Kemel log
OOM killer event details
uodtxs
Kemel statistics
Swap device usage
free
Kemel statistics
Systemwide memory usage
ps
Kermel statistics
Process statistics, including memory usage
pmap
Kemel statistics
Process memory usage by segment
vms ta t
Kemel statistics
Various statistics, including memory
sar
Kermel statistics
Can show page fault and page scanner rates
perf
Software events, hardware
Memory-related PMC statistics and event sampling
statistics, hardware sampling
The following sections summarize the key functionality of these tools. Refer to their man pages
and other resources, including Systems Performance [Gregg 13b], for more usage and explanations.
Chapter 14 includes slabtop(1) for kernel slab allocations.
7.2.1
Kernel Log
The kernel out-of-memory killer writes details to the system log, viewable using dmesg(1), for
each time it needs to kill a process. For example:
+ dnesg
[2156747,865271] run Lnvoked oon=ki1lex: gfp_nask=0x24201ca, order=0, con_score_adj=0
[...]
[2156747, 865330] MernInfo:
[2156747,865333] active_anon:3773117 inactive_anon:20590 isolated_anon:0
[2156747, 865333]
actlve_Cile:3 Inactlve_flle:0 1solated_flle:0
[2156747, 865333]
umerictable:0 dirty:0 vriteback:0 unstable:0
[E22598*4+.95121
slab_reclaimable:3980 slab_unreclalnable:5811
[2156747, 865333]
mapped:36 shmem:20596 pagetables:10620 bounce10
[2156747, 865333] free:18748 frea_pcp:45s free_cma:0
[...]
[2156747,865385] [ p1d ]
uidtgid total_m
rss nr_ptes nx_pnds svapents
aueu peasosuco
[015 1106c598*&c9512]
D
510
4870
67
1.5
0 upstart-udev-br
524]
L
524
12944
237
-1000 sys tend-udevd
28
[..-]
[2156747,65574] 0ut of memory: Ki1l process 23409 (perl) score 329 or sacrifice chi1d
[2156747,865583] K111ed procega 23409 (per1) total=vn:5370580xB, anon=rss:5224980kB,
file-rss:4kB
---
## Page 289
252
Chapter 7 Memory
Isage, the process table, and the target
process that was sacrificed.
You should always check dmesg(1) before geting into deeper memory analysis.
7.2.2
Kernel Statistics
Kernel statistics tools use statistical sources in the kernel, often exposed via the /proc interface
(e.g-, /proc/meminfo, /proc/swaps). An advantage of these tools is that the metrics are usually
always enabled by the kernel, so there is little adlditional overhead involved in using them. They
can also often be read by non-root users.
swapon
swapon(1) can show whether swap devices have been configured and how much of their volume
is in use. For example:
5 suapon
BAME
TYFE
0IX& 0350 3215
/dev/dn-2 part1t.ion 9808
0B
-2
Many systems nowadays do not have swap configuredl, and if this is the case, swapon(1) does not
This output shows a system with one swap partition of 980 Mbytes, which is not in use at all.
print any output.
If a swap device has active I/O, it can be seen in the *si* and *so? columns in vmstat(1), and as
device I/O in iostat(1).
free
The free(1) tool summarizes memory usage and shows available free memory system-wide. This
example uses n. for Mbytes:
S free -B
tota1
used
free
shared buff/cache
available
Men:
189282
183022
1103
5156
4716
Sxap1
0
0
0
The output from free(1) has improved in recent years to be less confusing; it now includes
an °available* column that shows how much memory is available for use, including the file
system cache. This is less confusing than the “free* column, which only shows memory that is
completely unused. If you think the system is running low on memory because “fre° is low, you
need to consider *available° instead.
The file system cached pages are seen in the *buff/cache* column, which sums two types: I/O
buffers and file system cached pages. You can view these pages in separate columns by using the
-v option (wide).
---
## Page 290
7.2  Traditional Tools
253
This particular example is from a production system with 184 Gbytes of total main memory, of
which about 4 Gbytes is currently available. For more breakdowns of system-wide memory,
cat /proc/meminfo.
ps
The ps(1) process status command can show memory usage by process:
xne sd ;
USER
PID ICPU 1MEN
VS2
RSS 7TY
STAT START
TIME COMAND
[.]
root
2499 0.0 0.0
30028
2720?
S.s
Jan25
0:00 /usx/sbln/cron -f
root
2703
0,0
0
0
I
04:13
0:00 [kxorker/41:0]
pcp
2951
0.0
0.0 116716
3572?
Jan25
0:00 /usx/11b/pcp/bin/pmwe
root
2992
0.00.0
0
 0
I
Jan25
0:00 [kxorker/17:2]
root
TLE
0.0
0.0
D
0?
I
Jan25
0 :05 [kxorker/0:3]
vxV
3785 1970 95.7 213734052 185542800 2 S1
Jan25 15123:15 /apps/java/bin/jav8. 
[..-]
This output has columns for:
 %MEM: The percentage of the system’s physical memory in use by this process
• VSZ: Virtual memory size
 RSS: Resident set size: the total physical memory in use by this process
This output shows that the java proces is consuming 95.7% of the physical memory on the
system. The ps(1) command can print custom columns to focus only on memory statistics
(e-g- ps -eo pid, pnem, vsz, ss). These statistics and more can be found in the /proc files:
/proc/PID/status.
pmap
The pmap(1) command can show process memory usage by adldress space segment. For example:
S8LE K- deud 
3785:
/apps/java/bin/ ava
-Dnop
-XX: +UseG1GC
XX:+Paralle1RefProcEnabled XX:+Exp11citGCIn
Address
Kbytes
RSS
Dirty Mode
Mapping
0000000000400000
4
0
 -x--
Java
0000000000400000
0
0
 r-x--
java
000000000600000
0000000000600000
4
4
4 =*-
Java
0
java
０0000０0０006c20０0
00LS
ZL5S
5572 rv=
[anon ]
00000000006c2000
0
0-
[anonJ
[..-]
---
## Page 291
254
Chapter 7 Memory
00007f2ce5e61000
0
0
-- libjvn.s0
00007f2ce 6061000
832
B32
832 rv=
11bjvn.80
00007f2ce 6061000
 0
 0
0c
1ibjvn.s0
[. - -]
ffffffffff60o000
0
{ r=7=*
(anon ]
rrrerrrrrr6ooooo
0
 r-x-
[anon ]
total kB
213928940 185743916 185732800
This view can identify large memory consumers by libraries or mapped files. This extended (x)