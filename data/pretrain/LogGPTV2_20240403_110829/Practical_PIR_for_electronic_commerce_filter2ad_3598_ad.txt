of the SPIR relies on successful veriﬁcation of the subsequent re-
ceipt proof (which is acceptable, since the servers do not respond
to the query until they have veriﬁed both proofs).
Refreshing the wallet.
Before performing subsequent queries, the user must refresh his
wallet with the bank. To do so, the user sends the tuple of val-
ues (ς, ReceiptP , wallet) to the bank, who veriﬁes that ς is
a valid signature on wallet(cid:107)ReceiptP . If so, the user and the
bank run the credential issuing protocol for the credential system
(see §2) that represents the wallet. At the end of this protocol, the
user has a new unlinkable (even to the bank) wallet wallet(cid:48) en-
coding the same tier as wallet and a balance equal to the price
committed to in ReceiptP subtracted from the balance encoded
in wallet. The user may similarly recharge his wallet with addi-
tional funds by ﬁrst ‘purchasing’ a receipt that encodes a negative
price using, for example, a prepaid credit card. Note that in this
procedure, the bank does not learn the balance in the new or the old
wallet, or the price encoded in the receipt; in fact, the bank cannot
even distinguish between a user that is refreshing his wallet and one
who is recharging it.
Supporting access control lists.
We now describe a simple modiﬁcation to implement access con-
trol lists atop our PSPIR construction. The idea is to impose a max-
imum balance bmax on users’ wallets, and then require all users to
prove that their new balance does not exceed bmax each time they re-
fresh or recharge their wallets with the bank. The bank will refuse
to issue any wallet without such a proof, thus ensuring that no user’s
balance ever exceeds bmax. The remainder of the protocol remains
unchanged, except that a price of ⊥ in (cid:126)pπ is treated as a price of
bmax + 1, which, by our restriction above, no user can afford. Thus,
this simple modiﬁcation effectively prevents users in price tier π
from purchasing any record marked as ⊥ in (cid:126)pπ.
4.3 Bookkeeping
This section discusses our approach to bookkeeping. At a high
level, our idea uses the additive and multiplicative homomorphic
properties of Shamir secret shares to maintain and compute on
shares of aggregate counts of the number of times each database
record is retrieved (and at what price). In Goldberg’s original PIR
construction, the database servers do not maintain any state infor-
mation — the only information they store is the actual database
contents. We augment the database with two additional columns of
state information; i.e., we require the database servers to store two
w-bit words of state (Shamir secret shares) per database record,
which are the length-r vectors of shares (cid:126)cj = (cid:104)[c1]q, . . . , [cr]q(cid:105)
and (cid:126)dj = (cid:104)[d1]q, . . . , [dr]q(cid:105).
Our initial idea was for servers to maintain a running sum of all
queries they witness between successive bookkeeping operations.
Unfortunately, this naive solution requires all servers to be involved
in all queries, since if server j aggregates a query but server j(cid:48) does
not, their shares will be inconsistent and interpolate to an unpre-
dictable value that does not reﬂect the actual number of queries per
record. However, requiring all servers to participate in all queries
hurts availability since the failure of a single database server would
render the system inoperable. (It also hurts efﬁciency by requiring
additional bandwidth and computational power be devoted to each
query.) Moreover, even if all servers aggregate all queries, this only
enables them to track the number of times each record is retrieved,
but not the prices paid for them. We solve the ﬁrst problem by hav-
ing the user reveal which subset Q of database servers are involved
in each query, and then use this knowledge to convert the Shamir
secret shares into additive shares; therefore, all servers must be on-
line to compute on the shared bookkeeping data, but they do not
need to be online during every query. To solve the second prob-
lem, we have the user (querying for record β under price tier π)
683After each bookkeeping operation, database server j reinitial-
izes the auxiliary vectors (cid:126)cj and (cid:126)dj back to the length-r zero vec-
tor, chooses a new private signing key xj for BLS signature gen-
eration, and publishes the corresponding veriﬁcation key (ˆg, ˆgxj ).
By having each server choose its signing key independently, every
subset Q of k servers has a unique public veriﬁcation key pkQ for
(k, k)-threshold BLS signatures; this key is easily computable us-
ing Q via the expression pkQ =(cid:81)
λQ,j =(cid:81)
(cid:0)ˆgxIj(cid:1)λQ,j mod q where
Ii∈Q−{Ij} Ij · (Ij − Ii)−1 mod q. Users then reveal Q
during each query, and the servers append an unambiguous string
representation of Q to the message resulting in the signature σ.
The servers accept the signature as valid if and only if veriﬁcation
succeeds using veriﬁcation key pkQ, where Q is the set of servers
encoded in the message. Similarly, the servers encode Q into the
signature ς on the user’s receipt, and the user transmits Q along
with the receipt to the bank.
Ij∈Q
include the additional vector of shares (cid:126)j of (pπβ · (cid:126)1β) along with
his query.5 The servers convert both vectors into additive shares and
aggregate them into (cid:126)cj and (cid:126)dj, which are then vectors of shares of
the number of times each record was retrieved, and the total price
paid for those retrievals, respectively.
This ensures that each server involved in a query sees a consistent
set Q of other servers; however, a malicious user may still disrupt
the bookkeeping process by neglecting to send the tuple (σ, V ) to
any nontrivial subset of Q on Step 12 of the SPIR protocol. The ser-
vers that do receive (σ, V ) will aggregate the user’s query into (cid:126)cj
and (cid:126)dj, while those that do not will not (thus resulting in inconsis-
tent shares among the servers in Q). We therefore rely on the bank
to facilitate atomicity to the query aggregation process. Instead of
returning the regular query response (cid:126)ρj·D, the servers use the PRG
(seeded with their shared secret and the user’s wallet) to produce a
common Zq element for blinding, Γ, and return (cid:126)ρj · D + Γ. When
the user sends (ς, Q, wallet, ReceiptP ) to the bank, the bank
1) veriﬁes that ς is a valid signature on wallet(cid:107)ReceiptP(cid:107)Q
using pkQ, 2) computes and sends Γ to the user, and 3) notiﬁes each
server in Q that the transaction involving wallet is complete. At
this point, all servers can safely update their aggregate shares.
Top-K replication.
Given the above modiﬁcations, supporting top-K replication is
straightforward. When a query ( (cid:126)C, S, wallet, (cid:126)ρj, wj, Q) arrives
at server Ij, it temporarily stores (cid:126)ρj. Upon notiﬁcation of the
query’s success from the bank, server Ij accumulates the query
by computing (cid:126)cj = (cid:126)cj + λQ,j · (cid:126)ρj.Computing the top-K records
from these shares is then a straightforward application of Burkhart
and Dimitropoulos’ [14] privacy-preserving top-K (PPTK) algo-
rithm. The algorithm outputs the top-K largest shares in (cid:126)cj without
revealing any additional information about the value of any share.
After the top K are revealed, the servers replicate these to a smaller
database and reinitialize (cid:126)cj to the length-r zero vector.
Multiple-payee PSPIR.
Supporting multiple payees in the tiered pricing model is slightly
more involved than is supporting top-K replication. Due to space
constraints, we only address the simpler case of a single-tiered pric-
ing scheme, and then brieﬂy outline how to extend this approach to
a system with multiple price tiers (and access control lists). Full de-
5Note that (cid:126)j (cid:54)= pπβ · (cid:126)ρj mod q, which would reveal the price
pπp to the servers; rather, (cid:126)j is an independently chosen vector of
shares. Of course, (cid:126)j − pπβ · (cid:126)ρj yields shares of the length-r zero
vector, which is the property we exploit to let the user prove the
well-formedness of (cid:126)j.
tails of the more general construction are available in the extended
version of this paper [36].
The single-tier case simpliﬁes the construction in two impor-
tant ways: ﬁrst, the user need not send or prove statements about
the additional vector of shares (cid:126)j as above and, second, the ser-
vers only need to store a single additional column of auxiliary in-
formation. Consider the above PSPIR construction with a sin-
gle price list (cid:126)p = (cid:104)p1, . . . , pr(cid:105) for all users, and a set of m CPs
CP = {CP1, . . . , CPm}. For ease of presentation, we deﬁne
r1 = 0 and rm+1 = r, and assume that CPi (1 ≤ i ≤ m)
owns the records at indices ri + 1 through ri+1. The amount
payable to CPi is then computed by summing the additive shares
(cid:80)ri+1
i=ri+1 pi · [ci]q.
With tiered pricing, the computation is conceptually similar but
requires cooperation from the user. The user sends the vector of
shares (cid:126)j described above along with his query. The server chooses
a random length-r vector of challenges (cid:126)c ∈ (Zq)r (via Fiat-Shamir)
and computes the T dot products [ei]q = ( (cid:126)j − (cid:126)pi (cid:0) (cid:126)ρj)· (cid:126)c, where
(cid:0) denotes component-wise multiplication of two vectors, for 1 ≤
i ≤ T . If (cid:126)j is well-formed, then [ei]q = [0]q for i = π; otherwise,
[ei]q (cid:54)= [0]q for any 1 ≤ i ≤ T with overwhelming probability.
The user then proves that [ei]q = [0]q for i = π using a proof
similar to the existing proof of correctness for the receipt. Once
convinced, server j is convinced that (cid:126)j is a vector of commitments
to the correct price, and can safely aggregate (cid:126)j into (cid:126)dj.
Bookkeeping frequency.
Bookkeeping necessarily leaks some information about user que-
ries.
In the extreme case, where only a single user queries the
database between bookkeeping operations, bookkeeping may com-
pletely reveal that user’s query. At the other end of the spectrum,
when every record is accessed hundreds or thousands of times be-
tween bookkeeping operations, the information leakage is minimal
and likely not at all invasive to any user’s privacy. However, pro-
longing the period between bookkeeping limits its usefulness in the
case of top-K replication, and may be economically unacceptable in
the case of multiple-payee PSPIR. Thus, a great deal of discretion
is necessary on the part of the database servers in determining how
often to run the bookkeeping protocols. For databases with con-
sistently high usage, a simple bookkeeping schedule such as once
per week or once per month may sufﬁce, whereas those databases
with lower usage may need to wait until the servers answer some
threshold number of queries. In general, the bookkeeping policy is
highly dependent both on the characteristics of the database and the
business logic of its operators. We leave an in-depth investigation
of this privacy-utility tradeoff as an important area for future work.
5.
IMPLEMENTATION & EVALUATION
We implemented the protocols described in this paper using Ben
Lynn’s Pairing-Based Cryptography (PBC) [43] library with Aniket
Kate’s PBCWrapper [39] package for C++ Wrapper classes, Vic-
tor Shoup’s NTL [55] with the GNU Multi-Precision Arithmetic
Library [29] for multi-precision arithmetic, and OpenSSL [50] for
hash functions (we use SHA-256). All experiments use a value of
κ = 40 for the soundness parameter. Our PSPIR implementation
is built atop Ian Goldberg’s implementation of his PIR protocols,
Percy++ [32]. For our evaluation, we implemented the protocol as
a standalone add-on to Percy++, but we will later integrate it with
the Percy++ library. We used the BigInteger-based version of Mar-
tin Burkhart’s SEPIA library [13] and his PPTK [14] protocol for
our top-K replication benchmarks. All measurements were taken
in Ubuntu Linux 10.04.1 LTS running on a machine with Dual Intel
684Figure 1: Query execution time for Percy++ with and without PSPIR (k = 4, t = 2). The percent compute time attributable to the PSPIR
enhancements decreases monotonically from ≈ 86% for a 1 GB database down to ≈ 33% for a 44 GB database. Percy++ starts carrying the extra
overhead of disk reads after a 28 GB database, which exceeds available RAM. The w = 8 plot shows the execution time for Percy++ using its
performance-optimal parameter choices, whereas the w = 160 plot shows Percy++ with parameters needed to ensure the security of PSPIR. The
Percy++ with PSPIR plot shows the combined cost of Percy++ with w = 160 and the PSPIR enhancements. Error bars are plotted for all data
points, but are small and may therefore be difﬁcult to see. For comparison, downloading a 44 GB database in OT-based schemes takes over 11 hours
at 9 Mbps, which is the average Internet bandwidth in Canada and the US [49].
Xeon E5420 2.50 GHz CPUs and 32 GB memory. The value of q
(the order of the pairing groups and the modulus for the polynomial
operations) was 160 bits long.
5.1 Experiments
We measured the performance of the PSPIR and top-K replica-
tion protocols for various values of the PIR parameters n (the size
of the database), b (the size of each record in the database), k (the
number of servers participating in each query), and t (the number
of servers that can collude without affecting query privacy).
Experiment 1. Our ﬁrst experiment measures the computational
overhead added to Percy++ by the PSPIR enhancements. We gen-
erated databases of sizes ranging from 1 GB to 44 GB contain-
ing random data and took measurements for both Percy++ and the
Percy++ with PSPIR.
√
In Figure 1, we plot the results for parameters k = 4, t = 2, and
160n, which is the communication-optimal record size for
b =
this PIR scheme. We observe that PSPIR results in a moderate in-
crease in compute times, with the percent compute time attributable
to the PSPIR enhancements decreasing monotonically from about
86% for a 1 GB database down to just 33% for a 44 GB database.
The upward bump just before 30 GB marks the point after which
the database no longer ﬁts in available memory. From that point
on, every query bears more overhead from disk reads. In terms of
communication overhead, PSPIR increases Percy++’s query size,
which is itself just k times the size of the retrieved record, by a
multiplicative factor of about 5. However, it increases each server’s
response by only 46 bytes, which corresponds to two G1 elements
(BLS signature shares).
The PSPIR compute time scales linearly with k and r = n/b.
In our implementation, the cost is independent of t except for a
one-time preprocessing step at the client. (The size of the long-
term polynomial commitment public key and Percy++’s client-side
compute time are also linear in t. This latter cost consistently ac-
counted for less than one percent of overall compute time in our
experiments.) The bottleneck operation for the client is comput-
ing commitments, while for the server it is processing the actual
PIR query. In both cases, the bottleneck operation increases with
r. Top-K replication or bucketization [47] can enable one to trade
some privacy to support larger values of r with little to no addi-
tional compute time or communication overhead.
Experiment 2. Our next experiment evaluates the impact of top-
K replication; i.e., it studies the performance gains for the users
when all queries for the K most popular records go to the smaller
replicated database. In this experiment, we assume that all records
are physically replicated to a second set of database servers, thus
increasing the maximum database size for which both the top-K
and non-top-K records ﬁt in physical memory. Alternatively, the
database servers could simply publish a list of indices for the top-
K records and allow users to perform PIR on just this subset of
the database; this would result in identical performance when the
entire database ﬁts in physical memory and somewhat lower per-
formance otherwise. All trials of the top-K experiment used a
query distribution that we generated at random using a bounded
Pareto distribution that satisﬁes the 80/20 rule; i.e., about 80% of
queries are for just 20% of the database records, with the number
of queries per record bounded between 0 and 10,000. As such, we
use K = (cid:98)r/5(cid:99). We chose an 80/20 distribution because such dis-