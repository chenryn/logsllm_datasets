title:Stealing Hyperparameters in Machine Learning
author:Binghui Wang and
Neil Zhenqiang Gong
2018 IEEE Symposium on Security and Privacy
Stealing Hyperparameters in Machine Learning
Binghui Wang, Neil Zhenqiang Gong
ECE Department, Iowa State University
{binghuiw, neilgong}@iastate.edu
Abstract‚ÄîHyperparameters are critical
in machine learn-
ing, as different hyperparameters often result in models with
signiÔ¨Åcantly different performance. Hyperparameters may be
deemed conÔ¨Ådential because of their commercial value and the
conÔ¨Ådentiality of the proprietary algorithms that the learner uses
to learn them. In this work, we propose attacks on stealing the
hyperparameters that are learnt by a learner. We call our attacks
hyperparameter stealing attacks. Our attacks are applicable to a
variety of popular machine learning algorithms such as ridge
regression, logistic regression, support vector machine, and neural
network. We evaluate the effectiveness of our attacks both
theoretically and empirically. For instance, we evaluate our
attacks on Amazon Machine Learning. Our results demonstrate
that our attacks can accurately steal hyperparameters. We also
study countermeasures. Our results highlight the need for new
defenses against our hyperparameter stealing attacks for certain
machine learning algorithms.
Many
popular
INTRODUCTION
I.
supervised machine
learning
[19]
,
(ML)
algorithms‚Äìsuch as ridge regression (RR)
logistic
regression (LR) [20], support vector machine (SVM) [13],
and neural network (NN) [16]‚Äìlearn the parameters in a
model via minimizing an objective function, which is often
in the form of loss function + Œª √ó regularization term.
Loss function characterizes how well
the model performs
over
regularization term is used to
prevent overÔ¨Åtting [7], and Œª balances between the two.
Conventionally, Œª is called hyperparameter. Note that there
could be multiple hyperparameters if the ML algorithm adopts
multiple regularization terms. Different ML algorithms use
different loss functions and/or regularization terms.
the training dataset,
Hyperparameters are critical for ML algorithms. For the
same training dataset, with different hyperparameters, an ML
algorithm might learn models that have signiÔ¨Åcantly different
performance on the testing dataset, e.g., see our experimental
results about the impact of hyperparameters on different ML
classiÔ¨Åers in Figure 16 in the Appendix. Moreover, hyperpa-
rameters are often learnt through a computationally expensive
cross-validation process, which may be implemented by pro-
prietary algorithms that could vary across learners. Therefore,
hyperparameters may be deemed conÔ¨Ådential.
Our work: In this work, we formulate the research problem of
stealing hyperparameters in machine learning, and we provide
the Ô¨Årst systematic study on hyperparameter stealing attacks
as well as their defenses.
Hyperparameter stealing attacks. We adopt a threat
model in which an attacker knows the training dataset, the
ML algorithm (characterized by an objective function), and
(optionally) the learnt model parameters. Our threat model
is motivated by the emerging machine-learning-as-a-service
¬© 2018, Binghui Wang. Under license to IEEE.
DOI 10.1109/SP.2018.00038
36
(MLaaS) cloud platforms, e.g., Amazon Machine Learning [1]
and Microsoft Azure Machine Learning [25], in which the
attacker could be a user of an MLaaS platform. When the
model parameters are unknown, the attacker can use model
parameter stealing attacks [54] to learn them. As a Ô¨Årst step
towards studying the security of hyperparameters, we focus
on hyperparameters that are used to balance between the loss
function and the regularization terms in the objective function.
Many popular ML algorithms‚Äìsuch as ridge regression, logistic
regression, and SVM (please refer to Table I for more ML
algorithms)‚Äìrely on such hyperparameters. It would be an in-
teresting future work to study the security of hyperparameters
for other ML algorithms, e.g., the hyperparameter K for KNN,
as well as network architecture, dropout rate [49], and mini-
batch size for deep neural networks. However, as we will
demonstrate in our experiments, an attacker (e.g., user of an
MLaaS platform) can already signiÔ¨Åcantly Ô¨Ånancially beneÔ¨Åt
from stealing the hyperparameters in the objective function.
We make a key observation that the model parameters
learnt by an ML algorithm are often minima of the corre-
sponding objective function. Roughly speaking, a data point is
a minimum of an objective function if the objective function
has larger values at the nearby data points. This implies that
the gradient of the objective function at the model parameters
is close to 0 (0 is a vector whose entries are all 0). Our attacks
are based on this key observation. First, we propose a general
attack framework to steal hyperparameters. SpeciÔ¨Åcally, in our
framework, we compute the gradient of the objective function
at the model parameters and set it to 0, which gives us a
system of linear equations about the hyperparameters. This
linear system is overdetermined since the number of equa-
tions (i.e., the number of model parameters) is usually larger
than the number of unknown variables (i.e., hyperparameters).
Therefore, we leverage the linear least square method [30],
a widely used method to derive an approximate solution of
an overdetermined system, to estimate the hyperparameters.
Second, we demonstrate how we can apply our framework to
steal hyperparameters for a variety of ML algorithms.
Theoretical and empirical evaluations. We evaluate our
attacks both theoretically and empirically. Theoretically, we
show that 1) when the learnt model parameters are an exact
minimum of the objective function, our attacks can obtain
the exact hyperparameters; and 2) when the model parameters
deviate from their closest minimum of the objective function
with a small difference, then our estimation error is a lin-
ear function of the difference. Empirically, we evaluate the
effectiveness of our attacks using six real-world datasets. Our
results demonstrate that our attacks can accurately estimate the
hyperparameters on all datasets for various ML algorithms.
For instance, for various regression algorithms, the relative
estimation errors are less than 10
‚àí4 on the datasets.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:29:54 UTC from IEEE Xplore.  Restrictions apply. 
Moreover, via simulations and evaluations on Amazon Ma-
chine Learning, we show that a user can use our attacks to learn
a model via MLaaS with much less economical costs, while
not sacriÔ¨Åcing the model‚Äôs testing performance. SpeciÔ¨Åcally,
the user samples a small fraction of the training dataset, learns
model parameters via MLaaS, steals the hyperparameters using
our attacks, and re-learns model parameters using the entire
training dataset and the stolen hyperparameters via MLaaS.
Rounding as a defense. One natural defense against our
attacks is to round model parameters, so attackers obtain
obfuscated model parameters. We note that rounding was
proposed to obfuscate conÔ¨Ådence scores of model predictions
to mitigate model inversion attacks [14] and model stealing
attacks [54]. We evaluate the effectiveness of rounding using
the six real-world datasets.
less than around 10
First, our results show that rounding increases the relative
estimation errors of our attacks, which is consistent with our
theoretical evaluation. However, for some ML algorithms, our
attacks are still effective. For instance, for LASSO (a popular
regression algorithm) [53], the relative estimation errors are
‚àí3 even if we round the model
still
parameters to one decimal. Our results highlight the need
to develop new countermeasures for hyperparameter stealing
attacks. Second, since different ML algorithms use different
regularization terms, one natural question is which regulariza-
tion term has better security property. Our results demonstrate
that L2 regularization term can more effectively defend against
our attacks than L1 regularization term using rounding. This
implies that an ML algorithm should use L2 regularization
in terms of security against hyperparameter stealing attacks.
Third, we also compare different loss functions in terms of
their security property, and we observe that cross entropy loss
and square hinge loss can more effectively defend against our
attacks than regular hinge loss using rounding. The cross-
entropy loss function is adopted by logistic regression [20],
while square and regular hinge loss functions are adopted by
support vector machine and its variants [21].
In summary, our contributions are as follows:
‚Ä¢ We provide the Ô¨Årst study on hyperparameter stealing attacks
to machine learning. We propose a general attack framework
to steal the hyperparameters in the objective functions.
‚Ä¢ We evaluate our attacks both theoretically and empirically.
Our empirical evaluations on several real-world datasets
demonstrate that our attacks can accurately estimate hyper-
parameters for various ML algorithms. We also show the
success of our attacks on Amazon Machine Learning.
‚Ä¢ We evaluate rounding model parameters as a defense against
our attacks. Our empirical evaluation results show that
our attacks are still effective for certain ML algorithms,
highlighting the need for new countermeasures. We also
compare different regularization terms and different loss
functions in terms of their security against our attacks.
II. RELATED WORK
Existing attacks to ML can be roughly classiÔ¨Åed into
four categories: poisoning attacks, evasion attacks, model
inversion attacks, and model extraction attacks. Poisoning
attacks and evasion attacks are also called causative attacks
and exploratory attacks [2], respectively. Our hyperparameter
stealing attacks are orthogonal to these attacks.
Poisoning attacks:
In poisoning attacks, an attacker aims to
pollute the training dataset such that the learner produces a bad
classiÔ¨Åer, which would mislabel malicious content or activities
generated by the attacker at testing time. In particular, the
attacker could insert new instances, edit existing instances, or
remove existing instances in the training dataset [38]. Existing
studies have demonstrated poisoning attacks to worm signature
generators [34], [41], [35], spam Ô¨Ålters [31], [32], anomaly
detectors [43], [24], SVMs [5], face recognition methods [4],
as well as recommender systems [27], [57].
Evasion attacks:
In these attacks [33], [22], [3], [51], [52],
[17], [26], [23], [37], [56], [9], [44], [28], [36], an attacker aims
to inject carefully crafted noise into a testing instance (e.g., an
email spam, a social spam, a malware, or a face image) such
that the classiÔ¨Åer predicts a different label for the instance.
The injected noise often preserves the semantics of the original
instance (e.g., a malware with injected noise preserves its ma-
licious behavior) [51], [56], is human imperceptible [52], [17],
[37], [28], or is physically realizable [9], [44]. For instance,
Xu et al. [56] proposed a general evasion attack to search
for a malware variant that preserves the malicious behavior of
the malware but is classiÔ¨Åed as benign by the classiÔ¨Åer (e.g.,
PDFrate [47] or Hidost [50]). Szegedy et al. [52] observed
that deep neural networks would misclassify an image after we
inject a small amount of noise that is imperceptible to human.
Sharif et al. [44] showed that an attacker can inject human-
imperceptible noise to a face image to evade recognition or
impersonate another individual, and the noise can be physically
realized by the attacker wearing a pair of customized eyeglass
frames. Moreover, evasion attacks can be even black-box, i.e.,
when the attacker does not know the classiÔ¨Åcation model. This
is because an adversarial example optimized for one model is
highly likely to be effective for other models, which is known
as transferability [52], [28], [36].
We note that Papernot et al. [39] proposed a distillation
technique to defend against evasion attacks to deep neural
networks. However, Carlini and Wagner [10] demonstrated
that this distillation technique is not as secure to new evasion
attacks as we thought. Cao and Gong [8] found that adversarial
examples, especially those generated by the attacks proposed
by Carlini and Wagner, are close to the classiÔ¨Åcation boundary.
Based on the observation, they proposed region-based classi-
Ô¨Åcation, which ensembles information in the neighborhoods
around a testing example (normal or adversarial) to predict
its label. SpeciÔ¨Åcally, for a testing example,
they sample
some examples around the testing example in the input space
and take a majority vote among the labels of the sampled
examples as the label of the testing example. Such region-
based classiÔ¨Åcation signiÔ¨Åcantly enhances the robustness of
deep neural networks against various evasion attacks, without
sacriÔ¨Åcing classiÔ¨Åcation accuracy on normal examples at all. In
particular, an evasion attack needs to add much larger noise in
order to construct adversarial examples that successfully evade
region-based classiÔ¨Åers.
inversion attacks:
Model
In these attacks [15], [14], an
attacker aims to leverage model predictions to compromise
user privacy. For instance, Fredrikson et al. [15] demonstrated
37
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:29:54 UTC from IEEE Xplore.  Restrictions apply. 
that model inversion attacks can infer an individual‚Äôs private
genotype information. Furthermore, via considering conÔ¨Ådence
scores of model predictions [14], model inversion attacks can
estimate whether a respondent in a lifestyle survey admitted to
cheating on its signiÔ¨Åcant other and can recover recognizable
images of people‚Äôs faces given their name and access to the
model. Several studies [45], [48], [42] demonstrated even
stronger attacks, i.e., an attacker can infer whether a particular
instance was in the training dataset or not.
Model extraction attacks: These attacks aim to steal param-
eters of an ML model. Stealing model parameters compro-
mises the intellectual property and algorithm conÔ¨Ådentiality of
the learner, and also enables an attacker to perform evasion
attacks or model inversion attacks subsequently [54]. Lowd
and Meek [29] presented efÔ¨Åcient algorithms to steal model
parameters of linear classiÔ¨Åers when the attacker can issue
membership queries to the model through an API. Tram`er
et al. [54] demonstrated that model parameters can be more
accurately and efÔ¨Åciently extracted when the API also produces
conÔ¨Ådence scores for the class labels.
III. BACKGROUND AND PROBLEM DEFINITION
A. Key Concepts in Machine Learning
We introduce some key concepts in machine learning (ML).
In particular, we discuss supervised learning, which is the focus
of this work. We will represent vectors and matrices as bold
lowercase and uppercase symbols, respectively. For instance,
x is a vector while X is a matrix. xi denotes the ith element
of the vector x. We assume all vectors are column vectors in
this paper. xT (or XT ) is the transpose of x (or X).
Decision function: Supervised ML aims to learn a decision
function f, which takes an instance as input and produces label
of the instance. The instance is represented by a feature vector;
the label can be continuous value (i.e., regression problem) or
categorical value (i.e., classiÔ¨Åcation problem). The decision
function is characterized by certain parameters, which we
call model parameters. For instance, for a linear regression
problem, the decision function is f (x)=wT x, where x is the
instance and w is the model parameters. For kernel regression
problem, the decision function is f (x)=wT œÜ(x), where œÜ is
a kernel mapping function that maps an instance to a point in
a high-dimensional space. Kernel methods are often used to
make a linear model nonlinear.
Learning model parameters in a decision function: An
ML algorithm is a computational procedure to determine the
model parameters in a decision function from a given training
dataset. Popular ML algorithms include ridge regression [19],
logistic regression [20], SVM [13], and neural network [16].
Training dataset. Suppose the learner is given n instances
X = {xi}n
m√ón. For each instance xi, we have a
label yi, where i = 1, 2,¬∑¬∑¬∑ , n. yi takes continuous value
for regression problems and categorical value for classiÔ¨Åcation
problems. For convenience, we denote y = {yi}n
i=1. X and y
form the training dataset.
i=1 ‚àà R
Objective function. Many ML algorithms determine the
model parameters via minimizing a certain objective function
TABLE I: Loss functions and regularization terms of various
ML algorithms we study in this paper.
Category
Regression
Logistic Regression
SVM
Neural Network
ML Algorithm
RR
LASSO
ENet
KRR
L2-LR
L1-LR
L2-KLR
L1-BKLR
SVM-RHL
SVM-SHL
KSVM-RHL
KSVM-SHL
Regression
ClassiÔ¨Åcation
Loss Function
Least Square
Least Square
Least Square
Least Square
Cross Entropy
Cross Entropy
Cross Entropy
Cross Entropy
Regular Hinge Loss
Square Hinge Loss
Regular Hinge Loss
Square Hinge Loss
Least Square
Cross Entropy
Regularization
L2
L1
L2 + L1
L2
L2
L1
L2
L1
L2
L2
L2
L2
L2
L2
over the training dataset. An objective function often has the
following forms:
Non-kernel algorithms: L(w) = L(X, y, w) + ŒªR(w)
Kernel algorithms: L(w) = L(œÜ(X), y, w) + ŒªR(w),
where L is called loss function, R is called regularization term,
œÜ is a kernel mapping function (i.e., œÜ(X) = {œÜ(xi)}n
i=1),
and Œª > 0 is called hyperparameter, which is used to
balance between the loss function and the regularization term.
Non-kernel algorithms include linear algorithms and nonlinear
neural network algorithms. In ML theory, the regularization
term is used to prevent overÔ¨Åtting. Popular regularization terms
include L1 regularization (i.e., R(w)=||w||1 =
i |wi|) and
L2 regularization (i.e., R(w)=||w||2
i w2
i ). We note that,
some ML algorithms use more than one regularization terms
and thus have multiple hyperparameters. Although we focus
on ML algorithms with one hyperparameter in the main text
of this paper for conciseness, our attacks are applicable to
more than one hyperparameter and we show an example in
Appendix B.
2 =
(cid:2)
(cid:2)
An ML algorithm minimizes the above objective function
for a given training dataset and a given hyperparameter, to
get the model parameters w, i.e., w = argminL(w). The
learnt model parameters are a minimum of the objective
function. w is a minimum if the objective function has larger
values at the points near w. Different ML algorithms adopt
different loss functions and different regularization terms. For
instance, ridge regression uses least-square loss function and
L2 regularization term. Table I lists the loss function and
regularization term used by popular ML algorithms that we
consider in this work. As we will demonstrate, these different
loss functions and regularization terms have different security
properties against our hyperparameter stealing attacks.
(cid:2)n
For kernel algorithms, the model parameters are in the form
i=1 Œ±iœÜ(xi). In other words, the model parameters are
w =
a linear combination of the kernel mapping of the training
instances. Equivalently, we can represent model parameters
using the parameters Œ± = {Œ±i}n
Learning hyperparameters via cross-validation: Hyper-
parameter is a key parameter in machine learning systems;
a good hyperparameter makes it possible to learn a model
that has good generalization performance on testing dataset.
In practice, hyperparameters are often determined via cross-
i=1 for kernel algorithms.
38
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:29:54 UTC from IEEE Xplore.  Restrictions apply. 

	


 





	
 	








Fig. 1: Key concepts of a machine learning system.
validation [21]. A popular cross-validation method is called K-
fold cross-validation. SpeciÔ¨Åcally, we can divide the training
dataset into K folds. Suppose we are given a hyperparameter.
For each fold, we learn the model parameters using the
remaining K‚àí1 folds as a training dataset and tests the model
performance on the fold. Then, we average the performance
over the K folds. The hyperparameter is determined in a
search process such that the average performance in the cross-
validation is maximized. Learning hyperparameters is much
more computationally expensive than learning model parame-
ters with a given hyperparameter because the former involves
many trials of learning model parameters. Figure 1 illustrates
the process to learn hyperparameters and model parameters.
i
}ntest
i=1 , whose labels are {ytest
Testing performance of the decision function: We often
use a testing dataset to measure performance of the learnt
model parameters. Suppose the testing dataset consists of
}ntest
{xtest
i=1 , respectively. For
regression, the performance is often measured by mean square
i ‚àí
error (MSE), which is deÔ¨Åned as MSE = 1
i=1 (ytest
ntest
f (xtest
))2. For classiÔ¨Åcation, the performance is often mea-
sured by accuracy (ACC), which is deÔ¨Åned as ACC =
i=1 I(ytest
=
), otherwise it is 0. A smaller MSE or a higher ACC
)), where I is 1 if ytest
ntest
f (xtest
implies better model parameters.
= f (xtest
(cid:2)ntest
(cid:2)ntest
1
i
i
i
i
i
i
B. Problem DeÔ¨Ånition
Threat model: We assume the attacker knows the train-
ing dataset,
the ML algorithm, and (optionally) the learnt
model parameters. Our threat model is motivated by machine-
learning-as-a-service (MLaaS) [6], [1], [18], [25]. MLaaS is an
emerging technology to aid users, who have limited computing
power and machine learning expertise, to learn an ML model
over large datasets. SpeciÔ¨Åcally, a user uploads the training
dataset to an MLaaS platform and speciÔ¨Åes an ML algorithm.
The MLaaS platform uses proprietary algorithms to learn the
hyperparameters, then learns the model parameters, and Ô¨Ånally
certain MLaaS platforms (e.g., BigML [6]) allow the user to
download the model parameters to use them locally. Attackers
could be such users. We stress that when the model parameters
are unknown, our attacks are still applicable as we demonstrate
in Section V-B3. SpeciÔ¨Åcally, the attacker can Ô¨Årst use model
parameter stealing attacks [54] to learn them and then perform
our attacks. We note that various MLaaS platforms‚Äìsuch as
Amazon Machine Learning and Microsoft Azure Machine
Learning‚Äìmake the ML algorithm public. Moreover, for black-
box MLaaS platforms such as Amazon Machine Learning and
Microsoft Azure Machine Learning, prior model parameter
stealing attacks [54] are applicable.
We deÔ¨Åne hyperparameter stealing attacks as follows:
DeÔ¨Ånition 1 (Hyperparameter Stealing Attacks): Suppose
an ML algorithm learns model parameters via minimizing
an objective function that
is in the form of loss function