RE-TARGETABLE GRAMMAR BASED TEST 
CASE GENERATION
JOE ROZNER / @JROZNER
TESTING PARSERS IS 
HARD
2
HOW WE GOT HERE
▸ Mostly black box (ish) implementation of complex languages (context-free-ish) 
▸ ~35k lines of grammar in total (ANTLR) 
▸ Implemented from incomplete, inaccurate, and contradictory documentation 
▸ Radically different parsing algorithm(s) from original implementations 
▸ Lack of public test cases for most dialects
3
PROBLEM AREAS FOR 
PARSING
OVERFIT/UNDERFIT GRAMMAR DEFINITION
▸ Does the parser accurately recognize the language? 
▸ Lack of access to real grammars 
▸ Poor documentation 
▸ Differences in parsing algorithms means differences in grammar deﬁnition 
▸ Ambiguity, recursion, precedence differences 
▸ Universally proving CFG equivalence is undecidable
TREE GENERATION FLAWS
▸ Does the parse tree accurately represent the sentence? 
▸ Shows the syntactic relationship between tokens 
▸ Most parser generators require manual tree construction 
▸ Typically this stage translates string data to it’s real type representation
6
lexer_body_part: ast::Operation = { 
      => ast::unroll_quantifier(q, ast::Operation::Token(r)), 
     ".."  => ast::Operation::Range((l, r)), 
      => ast::unroll_quantifier(q, ast::Operation::StringLiteral(r)), 
    "("  ")"  => ast::unroll_quantifier(q, ast::Operation::Group(r)), 
    "."  => ast::unroll_quantifier(q, ast::Operation::Any), 
    CharacterClass  => ast::unroll_quantifier(q, ast::Operation::CharacterClass), 
};
UNSAFE/INCORRECT VALIDATED INPUT
▸ Have we validated that the input is safe and correct? 
▸ Correct parsing proves validity but doesn’t ensure future proper handling 
▸ Syntactic/Semantic correctness doesn’t ensure safety 
▸ Opaque handling of tokens is fairly common due to language complexity 
▸ Once you’re past the parser it’s back to smashing the stack/logic ﬂaws/etc
8
HOW DO WE TEST THIS?
GETTING MORE TEST CASES
▸ Write by hand 
▸ Crawl the web/open source project pulling out examples 
▸ Automatically generate test cases with a fuzzer
10
STYLES OF FUZZING
INSTRUMENTATION + RANDOM MUTATION
▸ Focus on path exploration and code coverage 
▸ No concept of syntax/semantics 
▸ Wont necessarily provide lot’s of coverage for variations of a speciﬁc parse tree 
▸ Might spend a lot of time on uninteresting/non-relevant code paths 
▸ Not immediately clear how to build a proper test harness 
▸ Example of this strategy is AFL (American Fuzzy Lop) 
▸ https://lcamtuf.blogspot.com/2014/11/pulling-jpegs-out-of-thin-air.html
12
“THE FIRST IMAGE, HIT AFTER 
ABOUT SIX HOURS ON AN 8-CORE 
SYSTEM…”
“…CERTAIN TYPES OF ATOMICALLY EXECUTED 
CHECKS WITH A LARGE SEARCH SPACE MAY 
POSE AN INSURMOUNTABLE OBSTACLE TO THE 
FUZZER…”
if (strcmp(header.magic_password, "h4ck3d by p1gZ”)) 
  goto terminate_now;
“IN PRACTICAL TERMS, THIS MEANS THAT AFL-
FUZZ WON'T HAVE AS MUCH LUCK ‘INVENTING’ 
PNG FILES OR NON-TRIVIAL HTML DOCUMENTS 
FROM SCRATCH…”
INSTRUMENTATION + SOLVING
▸ Focus on path exploration and code coverage 
▸ Instrument the code and solve for new paths 
▸ Still doesn’t care about syntax/semantics 
▸ Still not clear how to build a more customer test harness 
▸ Not necessarily easy to gate off speciﬁc paths that are uninteresting 
▸ Example of this is KLEE
17
GRAMMAR BASED
▸ Uses a grammar to generate syntactically correct sentences 
▸ Typically provide their own grammar language 
▸ Mostly targeted at regular/context-free text based languages 
▸ Example of this is Mozilla Dharma 
▸ https://github.com/MozillaSecurity/dharma
18
PROBLEMS WITH TRADITIONAL TEST CASE GENERATION
▸ Inﬂexibility with using test cases 
▸ Inﬂexibility with providing feedback 
▸ Existing tools solve many cases but as you deviate they become less useful
HOW CAN WE DO BETTER?
▸ Easy to build ﬂexible test harnesses 
▸ Directly use grammar deﬁnition without manual translation 
▸ Expressive enough for regular, context-free, and context-sensitive languages 
both text and binary 
▸ Embeddable into and usable from any language 
▸ As much code re-use as possible to avoid duplication
20
SYNFUZZ
A GRAMMAR BASED TEST CASE GENERATION 
PLATFORM
Synfuzz
ANTLR
BISON
Test 1
EBNF
Ragel
Test 2
Test 1
Test 1
Test 3
Values
Quantiﬁcation
Logical
Grouping
CharLiteral
RepeatN
Choice
Sequence
Byte
Many
Not
JoinWith
String
Many1
SepBy
CharRange
Range
SepBy1
Optional
let mut f = File::open("bnf.g4").unwrap(); 
let mut buf = String::new(); 
f.read_to_string(&mut buf).unwrap(); 
let rules = antlr4::generate_rules(&buf).unwrap(); 
let r = rules.read().unwrap(); 
let root = r.get("rulelist").unwrap(); 
let generated = root.generate(); 
let s = String::from_utf8_lossy(&generated); 
println!("{}", s);
DEMO
DESIGNING TEST 
HARNESSES
DOES IT CRASH?
1.Start process 
2.Generate input and feed it 
3.Listen for SIGSEGV/SIGABRT
27
OVERFIT
1.Generate test case 
2.Find oracle that speciﬁes whether a syntax or runtime error 
3.Feed test case 
4.Categorization 
1.If failure and syntax error parser is overﬁt 
2.If failure and runtime error may or may not be overﬁt
28
mysql> select * from a where id ^^^^^ 3; 
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that 
 corresponds to your MySQL server version for the right syntax to use near '^^^^ 3' at line 1
mysql> select * from a where fakecolumn = 3; 
ERROR 1146 (42S02): Table 'mysql.a' doesn't exist
UNDERFIT
1.Generate test case from reference implementation grammar 
2.Parse with re-implementation 
3.Categorization 
1.If fails re-implementation is underﬁt 
2.If succeeds re-implementation is correct
30
WHAT’S READY TODAY
▸ Combinator library for regular and context-free grammars 
▸ ANTLR4 frontend 
▸ https://www.github.com/jrozner/synfuzz 
▸ https://www.github.com/jrozner/rust-antlr4
31
▸ Cycle detection + forced progression 
▸ Expose a C-compatible API + language bindings 
▸ Better negation logic 
▸ Context-Sensitive/Introspective generators 
▸ Bit level values 
▸ Additional frontends 
▸ Grammar coverage information
WHAT’S NEXT?
32
QUESTIONS?
PI:EMAIL / @JROZNER 
HTTPS://WWW.GITHUB.COM/JROZNER/SYNFUZZ