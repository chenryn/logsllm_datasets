to end learning for self-driving cars. CoRR, abs/1604.07316, 2016.
[3] Nicholas Carlini and David A. Wagner. Towards evaluating the robus-
In 2017 IEEE Symposium on Security and
tness of neural networks.
Privacy (SP), pages 39–57, 2017.
[4] Swarat Chaudhuri, Sumit Gulwani, and Roberto Lublinerman. Con-
tinuity analysis of programs. In Proceedings of the 37th Annual ACM
Symposium on Principles of Programming Languages (POPL), pages
57–70, 2010.
[5] Swarat Chaudhuri, Sumit Gulwani, Roberto Lublinerman, and Sara
Navidpour. Proving programs robust. In Proceedings of the 19th ACM
SIGSOFT symposium and the 13th European conference on Foundations
of software engineering (ESEC/FSE), pages 102–112. ACM, 2011.
[6] P. Cousot and R. Cousot. Abstract interpretation frameworks. Journal
of Logic and Computation, 2(4):511–547, 1992.
[7] Patrick Cousot and Radhia Cousot. Abstract interpretation: A uniﬁed
lattice model for static analysis of programs by construction or approx-
imation of ﬁxpoints.
In Proceedings of the 4th ACM Symposium on
Principles of Programming Languages (POPL), pages 238–252, 1977.
[8] Patrick Cousot and Nicolas Halbwachs. Automatic discovery of linear
restraints among variables of a program.
In Proceedings of the 5th
ACM Symposium on Principles of Programming Languages (POPL),
pages 84–96, 1978.
[9] Ivan Evtimov, Kevin Eykholt, Earlence Fernandes, Tadayoshi Kohno,
Bo Li, Atul Prakash, Amir Rahmati, and Dawn Song. Robust physical-
world attacks on machine learning models. CoRR, abs/1707.08945,
2017.
[10] Khalil Ghorbal, Eric Goubault, and Sylvie Putot. The zonotope abstract
domain taylor1+. In Proceedings of the 21st International Conference
on Computer Aided Veriﬁcation (CAV), pages 627–633, 2009.
[11] Khalil Ghorbal, Eric Goubault, and Sylvie Putot. A logical product
approach to zonotope intersection. In Proceedings of the 22Nd Interna-
tional Conference on Computer Aided Veriﬁcation (CAV), 2010.
[12] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining
and harnessing adversarial examples. CoRR, abs/1412.6572, 2014.
[13] Eric Goubault and Sylvie Putot. Robustness analysis of ﬁnite precision
implementations. In Programming Languages and Systems - 11th Asian
Symposium (APLAS), pages 50–57, 2013.
[14] Kathrin Grosse, Nicolas Papernot, Praveen Manoharan, Michael Backes,
and Patrick D. McDaniel. Adversarial perturbations against deep neural
networks for malware classiﬁcation. CoRR, abs/1606.04435, 2016.
[15] Shixiang Gu and Luca Rigazio. Towards deep neural network architec-
tures robust to adversarial examples. CoRR, abs/1412.5068, 2014.
[16] Warren He, James Wei, Xinyun Chen, Nicholas Carlini, and Dawn
Song. Adversarial example defense: Ensembles of weak defenses are
not strong. In USENIX (WOOT 17). USENIX Association, 2017.
[17] Ruitong Huang, Bing Xu, Dale Schuurmans, and Csaba Szepesv´ari.
Learning with a strong adversary. CoRR, abs/1511.03034, 2015.
[18] Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. Safety
veriﬁcation of deep neural networks. In Computer Aided Veriﬁcation,
29th International Conference (CAV), pages 3–29, 2017.
[19] David H Hubel and Torsten N Wiesel. Receptive ﬁelds, binocular
interaction and functional architecture in the cat’s visual cortex. The
Journal of physiology, 160(1):106–154, 1962.
[20] Bertrand Jeannet and Antoine Min´e. Apron: A library of numerical
In Computer Aided Veriﬁcation,
abstract domains for static analysis.
21st International Conference (CAV), pages 661–667, 2009.
[21] Guy Katz, Clark W. Barrett, David L. Dill, Kyle Julian, and Mykel J.
Kochenderfer. Reluplex: An efﬁcient SMT solver for verifying deep
neural networks.
In Computer Aided Veriﬁcation, 29th International
Conference (CAV), pages 97–117, 2017.
[22] Alex Krizhevsky. Learning multiple layers of features from tiny images.
Technical report, 2009.
[23] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.
Imagenet
classiﬁcation with deep convolutional neural networks. In Proceedings
of the 25th International Conference on Neural Information Processing
Systems (NIPS), pages 1097–1105, 2012.
[24] Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial
examples in the physical world. CoRR, abs/1607.02533, 2016.
[25] Yann Lecun, Lon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-
based learning applied to document recognition. In Proceedings of the
IEEE, pages 2278–2324, 1998.
[26] Yann Lecun, Larry Jackel, Bernhard E. Boser, John Denker, H.P. Graf,
Isabelle Guyon, Don Henderson, R. E. Howard, and W. Hubbard.
Handwritten digit recognition: Applications of neural network chips and
automatic learning. IEEE Communications Magazine, 27(11), 1989.
[27] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris
Tsipras, and Adrian Vladu. Towards deep learning models resistant
to adversarial attacks. In International Conference on Learning Repre-
sentations (ICLR), 2018.
[28] Rupak Majumdar and Indranil Saha. Symbolic robustness analysis. In
Proceedings of the 30th IEEE Real-Time Systems Symposium (RTSS),
pages 355–363, 2009.
[29] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Fros-
sard. Deepfool: A simple and accurate method to fool deep neural
networks. In 2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 2574–2582, 2016.
[30] Anh Mai Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks
are easily fooled: High conﬁdence predictions for unrecognizable ima-
ges. In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pages 427–436, 2015.
[31] Nicolas Papernot, Patrick D. McDaniel, Xi Wu, Somesh Jha, and
Ananthram Swami. Distillation as a defense to adversarial perturbations
against deep neural networks.
In IEEE Symposium on Security and
Privacy (SP), pages 582–597, 2016.
[32] Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. Deepxplore:
Automated whitebox testing of deep learning systems. In Proceedings
of the 26th Symposium on Operating Systems Principles (SOSP), pages
1–18, 2017.
[33] Corneliu Popeea and Wei-Ngan Chin. Inferring disjunctive postconditi-
ons. In Proceedings of the 11th Asian Computing Science Conference
on Advances in Computer Science: Secure Software and Related Issues
(ASIAN), pages 331–345, 2007.
[34] Luca Pulina and Armando Tacchella. An abstraction-reﬁnement appro-
In Computer Aided
ach to veriﬁcation of artiﬁcial neural networks.
Veriﬁcation, 22nd International Conference (CAV), 2010.
[35] Sara Sabour, Yanshuai Cao, Fartash Faghri, and David J. Fleet. Ad-
versarial manipulation of deep representations. CoRR, abs/1511.05122,
2015.
[36] Sriram Sankaranarayanan, Franjo Ivancic, Ilya Shlyakhter, and Aarti
In Static
Gupta. Static analysis in disjunctive numerical domains.
Analysis, 13th International Symposium, SAS), pages 3–17, 2006.
[37] Karsten Scheibler, Leonore Winterer, Ralf Wimmer, and Bernd Becker.
Towards veriﬁcation of artiﬁcial neural networks. In Methoden und Bes-
chreibungssprachen zur Modellierung und Veriﬁkation von Schaltungen
und Systemen (MBMV) 2015, pages 30–40, 2015.
[38] Uri Shaham, Yutaro Yamada, and Sahand Negahban. Understanding
adversarial training: Increasing local stability of neural nets through
robust optimization. CoRR, abs/1511.05432, 2015.
[39] Gagandeep Singh, Markus P¨uschel, and Martin Vechev. Fast polyhedra
the 44th ACM Symposium on
abstract domain.
Principles of Programming Languages (POPL), pages 46–59, 2017.
In Proceedings of
[40] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna,
Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. Intriguing properties
of neural networks. CoRR, abs/1312.6199, 2013.
[41] Pedro Tabacof and Eduardo Valle. Exploring the space of adversarial
In 2016 International Joint Conference on Neural Networks
images.
(IJCNN), pages 426–433, 2016.
[42] Florian Tram`er, Alexey Kurakin, Nicolas Papernot, Dan Boneh, and
Patrick McDaniel. Ensemble adversarial training: Attacks and defenses.
arXiv preprint arXiv:1705.07204, 2017.
[43] Florian Tram`er, Nicolas Papernot, Ian J. Goodfellow, Dan Boneh, and
Patrick D. McDaniel. The space of transferable adversarial examples.
CoRR, abs/1704.03453, 2017.
[44] Zhenlong Yuan, Yongqiang Lu, Zhaoguo Wang, and Yibo Xue. Droid-
sec: deep learning in android malware detection. In ACM SIGCOMM
2014 Conference, pages 371–372, 2014.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 05:59:05 UTC from IEEE Xplore.  Restrictions apply. 
16
A. CAT function representations of the Convolutional Layer and the Max Pooling Layer
APPENDIX
In this section, we provide the formal deﬁnitions of the matrices and vectors used to represent the convolutional layer and
the max pooling layer as CAT functions.
Convolutional Layer. Recall that for ﬁlters W k ∈ R
n×m×r → R
p(cid:2)
ConvF (x) : R
ConvF (x)i,j,k = ReLU(
p×q×r, bk ∈ R for 1 ≤ k ≤ t, we have
q(cid:2)
(m−p+1)×(n−q+1)×t
r(cid:2)
i(cid:2),j(cid:2),k(cid:2) · x(i+i(cid:2)−1),(j+j(cid:2)−1),k(cid:2) + bk),
W k
i(cid:2)=1
j(cid:2)=1
k(cid:2)=1
for 1 ≤ i ≤ m − p + 1, 1 ≤ j ≤ n − q + 1 and 1 ≤ k ≤ t. Reshaping both the input and the output vector such that they have
only one index, we obtain
n·m·r → R
(cid:2)
F (x) : R
Conv
(cid:2)
F (x)(n−q+1)·t·(i−1)+t·(j−1)+k = ReLU(
Conv
(m−p+1)·(n−q+1)·t
p(cid:2)
q(cid:2)
r(cid:2)
i(cid:2)=1
j(cid:2)=1
k(cid:2)=1
i(cid:2),j(cid:2),k(cid:2) · xn·r·(i+i(cid:2)−2)+r·(j+j(cid:2)−2)+k(cid:2) + bk),
W k
for 1 ≤ i ≤ m − p + 1, 1 ≤ j ≤ n − q + 1 and 1 ≤ k ≤ t. The function Conv
(cid:2)
F is ReLU after an afﬁne transformation,
((m−p+1)·(n−q+1)·t)×(n·m·r) and a vector bF ∈ R
therefore there is a matrix W F ∈ R
(m−p+1)·(n−q+1)·t such that
ConvF (x)
v
(cid:2)
F (xv
= Conv
) = ReLU(W F · xv
+ bF
) = FCW F ,bF (x).
The entries of W F and bF
are obtained by equating
FC(el)(n−q+1)·t·(i−1)+t·(j−1)+k = ReLU(W F
(cid:2)
F (el)(n−q+1)·t·(i−1)+t·(j−1)+k = ReLU(
Conv
for standard basis vectors el with (el)i = [l = i] for 1 ≤ l ≤ n and 1 ≤ i ≤ n · m · r. This way, we obtain
i(cid:2),j(cid:2),k(cid:2) · [l = n · r · (i + i(cid:2) − 2) + r · (j + j(cid:2) − 2) + k(cid:2)
W k
(n−q+1)·t·(i−1)+t·(j−1)+k,l + bF
(n−q+1)·t·(i−1)+t·(j−1)+k) with
k(cid:2)=1
j(cid:2)=1
i(cid:2)=1
p(cid:2)
q(cid:2)
r(cid:2)
] + bk),
p(cid:22)
q(cid:22)
r(cid:22)
j(cid:2)=1
k(cid:2)=1
W F
(n−q+1)·t·(i−1)+t·(j−1)+k,l =
i(cid:2)=1
bF
(n−q+1)·t·(i−1)+t·(j−1)+k = bk,
i(cid:2),j(cid:2),k(cid:2) · [l = n · r · (i + i(cid:2) − 2) + r · (j + j(cid:2) − 2) + k(cid:2)
W k
] and
for 1 ≤ i ≤ m − p + 1, 1 ≤ j ≤ n − q + 1 and 1 ≤ k ≤ t. Note that here, [ϕ] is an Iverson bracket, which is equal to 1 if ϕ
holds and equal to 0 otherwise.
m×n×r → R
q ×r partitions the input vector into disjoint blocks of size
Max Pooling Layer. Recall that MaxPoolp,q : R
p × q × 1 and replaces each block by its maximum value. Furthermore, MaxPool
q ·r is obtained from
(cid:2)
p,q : R
(cid:2)
(cid:2)
p,q(xv) = MaxPoolp,q(x)v. We will represent MaxPool
MaxPoolp,q by reshaping both the input and output: MaxPool
p,q as a
composition of CAT functions,
m·n·r → R
p × n
p · n
m
m
(cid:2)
p,q = f m
MaxPool
q ·r ◦ . . . ◦ f1 ◦ f MP.
p · n
brought into the same order as the output from each block appears in the output vector.
Here, f MP rearranges the input vector such that values from the same block are adjacent. Values from different blocks are
(cid:19)(cid:23)
Note that ((i − 1) mod p) + 1, (j − 1) mod q) + 1, 1) are the indices of input value xi,j,k within its respective block and
i−1
are the indices of the unique value in the output vector whose value depends on xi,j,k. Recall that
p
+ 1, k
j−1
q
+ 1,
(cid:20)
(cid:24)
(cid:23)
(cid:24)
the permutation matrix M representing a permutation π is given by Mπ(i) = ei.
The CAT function f MP is a linear transformation f MP(xv) = W MP · xv where the permutation matrix W MP is given by
p (cid:18)+(cid:17) j−1
for 1 ≤ i ≤ m, 1 ≤ j ≤ n and 1 ≤ k ≤ r.
r·p·q·( n
q (cid:17) i−1
W MP
q (cid:18))+p·q·(k−1)+q·((i−1) mod p)+((j−1) mod q)+1 = en·r·(i−1)+r·(j−1)+k,
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 05:59:05 UTC from IEEE Xplore.  Restrictions apply. 
17
For each 1 ≤ i ≤ m
p · r, the CAT function fi selects the maximum value from a (p · q)-segment starting from the ith
component of the input vector. The function fi consists of a sequence of cases, one for each of the p · q possible indices of
the maximal value in the segment:
p · n
fi(x) = case (xi ≥ xi+1) ∧ . . . ∧ (xi ≥ xi+p·q−1) : W (i,i) · x,
case (xi+1 ≥ xi) ∧ . . . ∧ (xi+1 ≥ xi+p·q−1) : W (i,i+1) · x,
case (xi+p·q−1 ≥ xi) ∧ . . . ∧ (xi+p·q−1 ≥ xi+p·q−2) : W (i,i+p·q−1) · x.
...
The matrix W (i,k) ∈ R
the value xk and is given by
(m·n·r−(p·q−1)·i)×(m·n·r−(p·q−1)·(i−1)) replaces the segment xi, . . . , xi+p·q−1 of the input vector x by
⎧⎨
⎩ ej,
W (i,k)
j
=
ek,
ej+p·q−1,
if 1 ≤ j ≤ i − 1
if j = i
if i + 1 ≤ j ≤ m · n · r − (p · q − 1) · i
.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 05:59:05 UTC from IEEE Xplore.  Restrictions apply. 
18