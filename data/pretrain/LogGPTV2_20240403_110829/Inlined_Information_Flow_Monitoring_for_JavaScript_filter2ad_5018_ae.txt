### Table 3: Selected Benchmark Results

| Benchmark | Uninstrumented (s) | Instrumented (s) | Slowdown |
|-----------|--------------------|------------------|----------|
| 0.0025    | 0.0661             | 27×              |
| 0.9267    | 6.143              | 663×             |
| 1.355     | 1.355              | 1×               |
| 0.7866    | 0.7866             | 1×               |
| 0.5132    | 0.5949             | 1.16×            |
| 0.5057    | 0.5057             | 1×               |
| No        | Yes                | Slowdown         |
| 342×      | 116×               | 315×             |
| 101×      | 8×                 | 231×             |
| 7.6×      |                    |                  |

### Listing 4: Original Code
```javascript
while (a > 10) {
    a -= b * 0.1;
    b = b > 10 ? b : --b;
}
```

### Listing 5: Transformed from Listing 4
```javascript
xxenter(1);
while (xxToBooleanBox(xxupdate(0, xxopgt(a, xprimlow(10))).v)) {
    xxassignVarOp(xxopsub, a, xxopmul(b, xprimlow(0.1)));
    xxassignVar(b, xxexit(xxToBooleanBox(xxpush(xxopgt(b, xprimlow(10)))).v ? b : xxprefixdec(b)));
}
xxexit();
```

### Listing 6: Original Code
```javascript
function f(x) { x(1); }
f(function g(y) { alert(y); g(y); });
```

### Listing 7: Transformed from Listing 6
```javascript
var f = xxinitVar();
xxassignVar(f, xxfunlow(function(x) { xxinvokeFunction(x, xprimlow(1)); }));
xxinvokeFunction(f, xxfunlow(function g(y) { 
    xxinvokeFunction(alert, y);
    xxinvokeFunction(xxfunlow(g), y);
}));
```

### Script Description
This second script contains a variable declaration that holds the list of upcoming shows. The crawler script reads this list, renders it as HTML, and includes it in the web page. The policy allows the crawler to read the user's playlist, history, and Zip-code for better targeting. However, for privacy and security reasons, the crawler should not access the user's name, authentication credentials, or other sensitive information.

Such a policy might be provided by the radio service to distinguish itself as more privacy-aware. Alternatively, a hardened browser designed for government agencies may have default policies concerning authentication credentials. Our goal is not to motivate organizations to specify such policies.

### Third-Party Authentication Service
A third-party authentication provider integrates a login form via an IFrame. It verifies authentication credentials with an asynchronous request to the auth server, which returns an authentication token. This token is then communicated back to the hosting page via `postMessage`. The website is not allowed to see the authentication credentials.

### Currency Converter
This web-based currency converter pulls conversion rates from the server each time to ensure timeliness. The policy is that neither the original nor the resulting amounts are disclosed to the server. A malicious version, however, discloses these amounts.

### Experiment Setup
We accessed both benign and malicious versions of the mashups through an inlining proxy. The proxy inlined the monitor that enforced the intended policies. We interacted with the applications, observed their behavior, and checked for any security violations.

### Findings
The monitor could run the benign versions of the case studies with one declassification (payment) and upgrade (ads) annotation each. All attacks from the malicious versions were successfully stopped, consistent with earlier findings [28]. No new kinds of vulnerabilities were found.

### 4.2 Performance Benchmarks
Synthetic performance benchmarks are commonly used to measure JavaScript runtime performance. At the time of writing, SunSpider, Octane, and Kraken were the most widely recognized and used. However, these benchmarks are contentious because they focus on numerical computations and algorithms on data structures. An alternative approach is to construct benchmarks from widely used websites, but we opted out due to heavy use of `eval`, which complicates reliable performance measurements and disables optimization in JIT compilers.

To get a sense of performance overhead, we chose SunSpider and Kraken, which can be run stand-alone easily. We also used them as an extra test of transparency, as they check the correctness of results. These benchmarks, while not representative of web apps, especially security-critical ones, constitute the worst-case scenario for the monitor.

We selected a subset of SunSpider and Kraken to maximize diversity and minimize additional API support. SunSpider tests are heavy on mathematical computations, bitwise operations, and string processing, while Kraken focuses on signal processing, cryptography, and JSON-related benchmarks. We performed two types of measurements using the Benchmark.js library to factor out interpreter and RTS startup times and execution time fluctuations due to garbage collection and JIT compilation. We compared the running time of benchmark code instrumented with the monitor following a trivial policy to the uninstrumented one.

We observed a 101-364× slowdown in mean running time depending on the benchmark and inlining optimization. We have a mode that trades off some transparency for up to a 40% speedup by omitting the `with` statement in the monitor structure (Section 3.4). See Table 3 and [15, 14] for more details.

We also compared our performance to the closest related work, JSFlow [28], discussed in Section 5. It was impossible to use the benchmarking library with JSFlow without invasive modifications. Instead, we used the Unix `time` command to measure one run of the benchmark without instrumentation, with our monitor, and with JSFlow. This approach does not account for measurement fluctuations (observed to be about 1%) and includes the interpreter startup time. In this test, the inlined monitor exhibited a 15.6× slowdown, and JSFlow was 1680× slower compared to the original. Detailed data and instructions to reproduce the experiments are available in [15, 14]. The numbers for JSFlow are consistent with the authors' observations reported in [28] and private communication.

All measurements were done in Node.js v0.10.25 with all performance optimizations enabled.

While the performance results look uninspiring, let us put them in perspective. First, the results are from running synthetic, computationally intensive benchmarks on a state-of-the-art JIT compiler designed to run these benchmarks fast. Second, the performance analyses in closely related work either report overheads for hard-to-reproduce "macro benchmarks" or compare performance slowdown against interpreters that don't use JIT.

Due to the reactive nature of our case-study applications, reliable performance measurement requires additional infrastructure. One promising approach would be to adapt them, as well as examples from previous work, for performance testing by removing reactivity.

### 5. Related Work
Fragoso-Santos and Rezk implemented an inlined NSU-based monitor for a subset of ECMAScript 3rd edition and a small but challenging subset of the DOM API [54, 2], including "live collections." No performance data is provided. The JS subset is very small, omitting non-syntactic control flow, exceptions, the `with` and `for-in` statements, the `in` and `new` operators, as well as flow via the standard library and implicit type conversions. This allows for inlining to use "shadow" variables and fields to store labels, which is relatively simple and should cater for performance. The authors provide formal proofs of soundness and transparency.

The complexities of JS and IF strongly motivate formal verification for assurance of monitors. Several lines of work on JS IFC monitoring provide proofs of soundness (and transparency in some cases). However, these proofs are for formalizations that idealize (in varying degrees) from the implemented systems. We are aware of no verified implementation of an information flow monitor for JS. To make verification more tractable, it is attractive to minimize the complexities of JS by distilling to a small core. This is difficult to achieve for full JS, as discussed in detail by [24].

Just et al [33] added an IF monitor to the WebKit JS interpreter. The monitor appears to use the NSU approach and includes full JS support, including `eval`. The authors appear to be the first to recognize the importance of using control-flow graphs for accurate and sound tracking of implicit flows due to unstructured control flow (break, continue, return, exceptions). Implicit flows due to element existence are also discussed. Although exceptions are discussed, the static analysis does not deal with exceptions; the authors question whether the approach using control-flow graphs will work with exceptions. Performance tests using synthetic benchmarks have been performed, reporting a 2-3× slowdown compared with a non-JIT interpreter. A qualitative experiment with a short JS program that didn't use the browser API has also been performed.

Building on [33], Bichhawat et al [11] modified the JS bytecode interpreter to track IF, handling implicit flows using immediate post-dominator analysis of intra-procedural control flow graphs built on-the-fly. The semantics of the bytecode are formalized and used to give a formal proof of soundness. The monitor implements permissive upgrade [5], including its non-trivial extension to arbitrary security lattices [10]. It is also the only one to implement sparse labeling [4]. With sparse labeling, they report 0-125% runtime overhead in synthetic benchmarks (SunSpider) with the average being 45%, and 7-42% overheads in macro benchmarks (websites), with the average of 29%. The overheads are calculated for an unmodified interpreter that does not use JIT compilation. The JIT compiler is much faster on synthetic benchmarks. However, the JIT shows about the same performance as the interpreter on macro benchmarks (websites), chosen by the authors. Rajani et al [47] extended the system to support the full DOM API as well as flows via event handling. These extensions are formalized and proved sound.

Magazinius et al [38] were the first to point out in print that an inlined monitor can deal with `eval` by applying the inlining transformation to each string passed to `eval`. They prove soundness of the inlining transformation for a small imperative language with `eval` but lacking objects, exceptions, lambdas, dynamic access to the runtime environment, or other challenging JS features. Lack of unstructured control allows them to track PC level elegantly using lexically-scoped `let`-expressions rather than an explicit stack. They experimented with manual transformation of programs that use `eval` and implemented automatic transformation for a small subset of JS. In their experiments doing the inlining manually, the inlined monitor adds an overhead of 20%-1700% depending on the browser.

Hedin and Sabelfeld [29] formalized an NSU monitor for a core subset of ECMAScript 5 and discussed IF for its various features, including references, object structure, `eval`, and exceptions. They proved that monitoring ensures TINI and proved the partial-correctness form of transparency: if a monitored program terminates without IF exception, then erasing security labels from its final state yields the outcome that would have been obtained by running the program without the monitor. The monitor was later extended with support for browser APIs and implemented [28], though no formalization of the extension is provided. The implementation, JSFlow, is a custom JS interpreter written in JS. Like ours, it can be used without modifying the browser and is comparable in language and API support. The treatment of control flow is coarse-grained. Hedin et al [27] improved the monitor by runtime static analysis to predict potential write targets, for which the monitor upgrades labels. They proved soundness, a key point being that the monitor relies on NSU for soundness. They demonstrated, for the chosen static analysis, increases in permissiveness. The monitor is also in a position to upgrade labels at the right time, which sidesteps a complication (delayed upgrades [12]) with upgrades as code annotations.

Optimization of programs in JavaScript is tricky: even employing sophisticated program specialization techniques yields modest results. In [57], Thiemann reports that specializing JSFlow for the input, essentially yielding a compiler, gains only 1.8× speedup compared to the original.

De Groef et al [26] built a modification of the Firefox web browser that features SME [20]. An advantage of SME is that it avoids the restrictions of NSU. However, to avoid multiple executions of the entire software stack, the tool considers any use of web API as IO, thus treating DOM flow-insensitively. Performance overhead has been shown to be as low as 20% in IO-intensive applications (and as high as 200% on synthetic tests for a simple two-level policy), while the average memory overhead was 88%. The overhead is compared to the unmodified version of Firefox. Austin and Flanagan’s SME approach has also been implemented for JS as a Firefox add-on [6]. They present a performance result, using one of the SunSpider benchmarks, that compares favorably with other SME and does not suffer as much degradation when the number of security levels increases. Soundness for these variations has been proved in [20, 6] and also in [46], which also proves transparency.

Jang et al [32] implemented taint tracking for JS in web pages and used it for a large-scale empirical study of privacy violations. The monitor is implemented by rewriting and resembles ours in some respects, such as boxing and a stack of levels for indirect flow (not implicit). As the authors point out, the monitor does not track implicit flow and thus misses some information flows. They report slowdowns of 3-8× compared to the original, depending on optimizations. Dhawan and Ganapathy [21] implemented taint tracking in JS browser extensions.

Recent work has shown that taint tracking can be made sufficiently performant for use in production scenarios, offering an average of 25% overhead [39]. Complementing work on JS, Bauer et al [9] formalized and implemented coarse-grained taint tracking end-to-end in the Chromium browser.

Yip et al [61] implemented a reference monitor called BFlow as a plug-in for Firefox. The monitor tracks levels of data at the grain of protection zones, which are groups of frames where data has the same sensitivity. The data labels need to arrive from a BFlow web server. Tracking of information labels is very coarse-grained and restrictive: once a script in a frame has accessed data with a certain label, all data originating from that frame will be assigned at least that label. Consequently, no frame can handle both sensitive and public information at the same time, which prevents most information leaks.