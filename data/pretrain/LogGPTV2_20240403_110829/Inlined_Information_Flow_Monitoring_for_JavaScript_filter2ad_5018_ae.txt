0.0025s
0.0661s
0.9267s
6.143s
1.355s
0.7866s
0.5132s
0.5949s
0.5057s
No
No
No
No
No
Yes
Yes
Slowdown
342×
116×
315×
101×
8×
231×
7.6×
Table 3: Selected benchmark results
638while ( a > 10) { a -= b * 0.1; b = b > 10 ? b : --b ;}
Listing 4: Original
xxenter (1) ;
while ( x x T o B o o l e a n B o x ( xxupdate (0 , xxopgt (a , xx p r i m l ow (10) ) ) ) . v ) {
x x a s s i g n V a r O p ( xxopsub , a , xxopmul (b , x x p r i ml o w (0.1) ) ) ;
x x a s s i g n V a r (b , xxexit ( x x T o B o o l e a n B o x ( xxpush ( xxopgt (b , x x pr i m l o w (10) ) ) ) . v ? b :
x x p r e f i x d e c ( b ) ) ) ;
}
xxexit () ;
function f ( x ) { x (1) ;}; f ( function g ( y ) { alert ( y ) ; g ( y ) ;}) ;
Listing 5: Transformed from Listing 4
Listing 6: Original
var f = x x i ni t V a r () ;
x x a s s i g n V a r (f , xxfunlow ( function ( x ) { x x i n v o k e F u n c t i o n (x , x x p r i ml o w (1) ) ;}) ) ;
x x i n v o k e F u n c t i o n (f , xxfunlow ( function g ( y ) { x x i n v o k e F u n c t i o n ( alert , y ) ;
x x i n v o k e F u n c t i o n ( xxfunlow ( g ) , y ) ;}) ) ;
Listing 7: Transformed from Listing 6
This second script contains a declaration of a variable con-
taining the list of upcoming shows, which is read by the
crawler script, rendered as HTML and included in the web
page. The policy is that, for better targeting, the crawler
script is allowed to read the user’s playlist and history, as
well the Zip-code. However, for privacy and security reasons,
the crawlers should not read the user’s name, authentication
credentials or any other sensitive information.
Such a policy might be provided by the radio service in
hopes to distingish itself as more privacy-aware than others.
Perhaps more likely is that a hardened browser designed
to meet requirements of some government agency may have
default policies concerning authentication credentials. It is
not our aim in this work to solve the problem of motivating
organizations to specify policies.
Third-party authentication service. A 3rd-party au-
thentication provider gives the ability to integrate a login
form via an IFrame. It veriﬁes the authentication creden-
tials with an asynchronous request to the auth server. The
server returns an authentication token, which is, in turn,
communicated back to the hosting page via postMessage. A
web-site that would like to authenticate a user is not allowed
to see the authentication credentials.
Currency converter. This web-based currency con-
verter is not a mashup, but still has an IF policy. The
conversion is done on the client side. But, in an eﬀort to
be timely, it pulls the conversion rates from the server each
time. The policy is that neither the original, nor the result-
ing amounts are disclosed to the server — and the malicious
version does just that.
Experiment setup. We accessed the mashups —both
the benign and malicious versions— in the browser through
an inlining proxy. The proxy inlined the monitor that en-
forced the intended policies for the applications. We then
interacted with the applications and observed their behavior
and whether there were any security violations.
Findings. The monitor could run the benign versions
of case-studies with one declassiﬁcation (payment) and up-
grade (ads) annotation each. The attacks from malicious
versions were all successfully stopped. This is consistent
with earlier ﬁndings in [28]. We have not found any new
kinds of vulnerabilities.
4.2 Performance benchmarks
Synthetic performance benchmarks are the common way
of measuring performance of JS run-times. At the time of
writing SunSpider, Octane and Kraken were the most widely
recognized and used. Yet, such benchmarks are also con-
tentious [48] because they focus on numerical computations
and algorithms on data-structures. Automatic construction
of benchmarks from widely used websites has been proposed
as an alternative [49], but we opted out of using it due to
heavy use of eval for implementation which complicates
reliable performance measurements in our implementation.
Also, use of eval disables optimization in all the JIT com-
pilers we’ve studied.
We still wanted to get a sense of performance overheads,
so we’ve chosen SunSpider and Kraken, which could be run
stand-alone easily.
In addition, we use them as an extra
test of transparency, since they check correctness of results.
We recognize that these benchmarks are not representative
of web apps —particularly the security critical ones— and
constitute the worst-case scenario for the monitor.
We select a subset of SunSpider and Kraken in an at-
tempt to maximize diversity, but minimize the additional
API support required. SunSpider tests are heavy on math-
ematical computations, bitwise operations and string pro-
cessing, while most of Kraken is about signal processing and
cryptography, along with an implementation of A∗ and two
JSON-related benchmarks. We perform two kinds of mea-
surements. First, we use the Benchmark.js library to mea-
sure performance accurately, factoring out interpreter and
RTS startup times and ﬂuctuations in the execution time
639due to garbage collection, JIT compilation etc. We compare
the running time of benchmark code instrumented with the
monitor following a trivial policy to the uninstrumented one.
We observed 101−364× slowdown in the mean running time
depending on the benchmark and the inlining optimization.
We have a mode that trades oﬀ some transparency for up to
40% speedup by omitting the with statement in the monitor
structure (Sec. 3.4). See table 3, as well as [15, 14] for more
details.
We also compare our performance to the closest related
work, JSFlow [28], which is discussed in section 5. It was im-
possible to use the benchmarking library with JSFlow with-
out invasive modiﬁcations. Instead we used the Unix time
command to measure one run of the benchmark without in-
strumentation, with our monitor and with JSFlow. This ap-
proach does not account for measurements ﬂuctuations (ob-
served to be about 1%) and includes the interpreter startup
time. In this test the inlined monitor exhibits a 15.6× slow-
down and JSFlow is 1680× slower compared to the original.
Detailed data as well as instructions on how to reproduce the
experiments is available in [15, 14]. The numbers for JSFlow
are consistent with the authors’ observations reported in [28]
and private communication.
All measurements were done in Node.js v0.10.25 with all
the performance optimizations enabled.
While the performance results look uninspiring,
let us
put them in perspective. First, the results are from run-
ning synthetic, computationally intensive benchmarks on a
state-of-the-art JIT compiler that was designed to run these
particular benchmarks fast. Second, the performance analy-
ses in closely related work either report overheads for hard-
to-reproduce “macro benchmarks” or compare performance
slowdown against interpreters that don’t use JIT.
Due to reactive nature of our case-study applications reli-
able measurement of performance requires additional infras-
tructure. One promising approach would be to adapt them,
as well as examples from previous work, for performance
testing by removing reactivity.
5. RELATED WORK
Fragoso-Santos and Rezk implement an inlined NSU-based
monitor for a subset of ECMAScript 3rd edition and a small
but challenging subset of the DOM API [54, 2] including
“live collections”. No data on performance is provided. The
JS subset is very small, omitting non-syntactic control-ﬂow,
exceptions, the with and for-in statements, the in and new
operators, as well as ﬂow via the standard library and im-
plicit type conversions. Thus there are relatively few possi-
bilities for an attack on the monitor, and fewer peculiarities
of the IF semantics. This allows for inlining to use “shadow”
variables and ﬁelds to store labels, which is relatively sim-
ple and should cater for performance. The authors provide
formal proofs of soundness and transparency.
The complexities of JS and of IF strongly motivate for-
mal veriﬁcation for assurance of monitors. Several lines of
work on JS IFC monitoring provide proofs of soundness (and
transparency in some cases). However, these proofs are all
for formalizations that idealize (in varying degrees) from the
implemented systems. We are aware of no veriﬁed imple-
mentation of an information ﬂow monitor for JS. To make
veriﬁcation more tractable, it is attractive to minimize the
complexities of JS by distilling to a small core. This is dif-
ﬁcult to achieve for full JS, as discussed in detail by [24].
Just et al [33] have added an IF monitor to the WebKit
JS interpreter. The monitor appears to use the NSU ap-
proach and includes full JS support including eval. The
authors appear to be the ﬁrst to recognize the importance
of using control-ﬂow graphs for accurate and sound track-
ing of implicit ﬂows due to unstructured control ﬂow (break,
continue, return, exceptions). Implicit ﬂows due to element
existence are also discussed. Although exceptions are dis-
cussed, the static analysis does not deal with exceptions;
in fact the authors question whether the approach using
control-ﬂow graphs will work with exceptions. Performance
tests using synthetic benchmarks have been performed re-
porting 2× to 3× slowdown compared with a non-JIT inter-
preter. A qualitative experiment with a short JS program
that didn’t use the browser API has been performed as well.
Building on [33], Bichhawat et al [11] modify the JS byte-
code interpreter to track IF, handling implicit ﬂows using
immediate post-dominator analysis of intra-procedural con-
trol ﬂow graphs built on-the-ﬂy. Semantics of the bytecode
is formalized and used to give a formal proof of soundness.
The monitor implements permissive upgrade [5], including
its non-trivial extension to arbitrary security lattices [10]. It
is also the only one to implement sparse labeling [4]. With
sparse labeling, they report 0 to 125% run-time overhead in
synthetic benchmarks (SunSpider) with the average being
45%, and 7 to 42% overheads in macro benchmarks (web
sites), with the average of 29%. The overheads are calcu-
lated for an unmodiﬁed interpreter that does not use JIT
compilation. The JIT compiler is much faster on synthetic
benchmarks. However the JIT shows about the same perfor-
mance as the interpreter on macro benchmarks (web sites),
chosen by the authors. Rajani et al [47] extend the system
to support the full DOM API as well as ﬂows via event han-
dling. These extensions are formalized and proved sound.
Magazinius et al [38] were the ﬁrst to point out in print
that an inlined monitor can deal with eval by applying the
inlining transformation to each string passed to eval. They
prove soundness of the inlining transformation for a small
imperative language with eval but lacking objects, excep-
tions, lambdas, dynamic access to the runtime environment,
or other challenging JS features. Lack of unstructured con-
trol allows them to track PC level elegantly using lexically-
scoped let-expressions rather than an explicit stack. They
experiment with manual transformation of programs that
use eval, and they implement automatic transformation for
a small subset of JS. In their experiments doing the inlining
manually, the inlined monitor adds an overhead of 20%–
1700% depending on the browser.
Hedin and Sabelfeld [29] formalize an NSU monitor for
a core subset of ECMAScript 5 and discuss IF for its vari-
ous features including references, object structure, eval, and
exceptions. They prove that monitoring ensures TINI and
they prove the partial-correctness form of transparency:
if
a monitored program terminates without IF exception, then
erasing security labels from its ﬁnal state yields the out-
come that would have been obtained by running the pro-
gram without monitor. The monitor is later extended with
support for browser APIs and implemented [28], though no
formalization of the extension is provided. The implementa-
tion, JSFlow, is a custom JS interpreter written in JS. Like
ours it can be used without modifying the browser and is
comparable in language and API support. The treatment
of control-ﬂow is coarse-grained. Hedin et al [27] improve
640the monitor by runtime static analysis to predict potential
write targets, for which the monitor upgrades labels. They
prove soundness, a key point being that the monitor relies
on NSU for soundness. They demonstrate, for the chosen
static analysis, increases in permissiveness. The monitor is
also in a position to upgrade labels at the right time, which
sidesteps a complication (delayed upgrades [12]) with up-
grades as code annotations.
Optimization of programs in JavaScript is tricky: even
employing sophisticated program specialization techniques
yields modest results. In [57] Thiemann reports that special-
izing JSFlow for the input, essentially yielding a compiler,
gains only 1.8× speedup compared to the original.
De Groef et al [26] have built a modiﬁcation of the Firefox
web browser that features SME [20]. An advantage of SME
is that avoids the restrictions of NSU. However, in order to
avoid multiple executions of the entire software stack, the
tool considers any use of web API as IO, thus treating DOM
ﬂow-insensitively. Performance overhead has been shown to
be as low as 20% in IO-intensive applications (and as high
as 200% on synthetic tests for a simple two-level policy),
while the average memory overhead was 88%. The overhead
is compared to the unmodiﬁed version of Firefox. Austin
and Flanagan’s SME approach has also been implemented
for JS, as Firefox add-on [6]. They present a performance
result, using one of the SunSpider benchmarks, that com-
pares favorably with other SME and do not suﬀer as much
degradation when the number of security levels increases.
Soundness for these variations has been proved in [20, 6]
and also in [46] which also proves transparency.
Jang et al [32] implement taint tracking for JS in web
pages and use it for a large-scale empirical study of pri-
vacy violations. The monitor is implemented by rewriting
and resembles ours in some respect, such as boxing and a
stack of levels for indirect ﬂow (not implicit). As the au-
thors point out, the monitor does not track implicit ﬂow
and thus misses some information ﬂows. They report slow-
downs of 3×-8× compared to the original, depending on op-
timizations. Dhawan and Ganapathy [21] implement taint
tracking in JS browser extensions.
Recent work has shown that taint tracking can be made
suﬃciently performant for use in production scenarios, of-
fering an average of 25% overhead [39],
Complementing work on JS, Bauer et al [9] formalize and
implement coarse grained taint tracking end-to-end in the
Chromium browser.
Yip et al [61] implement a reference monitor called BFlow
as a plug-in for Firefox. The monitor tracks levels of data
at the grain of protection zones which are groups of frames
where data has the same sensitivity. The data labels need
to arrive from a BFlow web server. Tracking of information
labels is very coarse-grained and restrictive: once a script
in a frame has accessed data with a certain label, all data
originating from that frame will be assigned at least that
label. Consequently, no frame can handle both sensitive and
public information at the same time, which prevents most