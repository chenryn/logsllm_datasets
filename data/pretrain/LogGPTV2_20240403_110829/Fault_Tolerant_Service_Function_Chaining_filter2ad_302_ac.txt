In a piggyback log, we replace the sequence number with a
dependency vector that represents the effects of a transaction on
state partitions. If the transaction does not access a state partition,
the head uses a “don’t-care” value for this partition in the piggyback
log. The head obtains the sequence number of other partitions from
the head’s dependency vector before incrementing their sequence
numbers.
Each successor replica keeps a dependency vector MAX that
tracks the latest piggyback log that it has replicated in order, i.e.,
it has already received all piggyback logs prior to MAX. In case
SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
Figure 3: Data dependency vectors. The head and the replica
run two threads and maintain a dependency vector for three state
partitions.
a packet is lost, a replica requests its predecessor to retransmit
missing piggyback logs.
Upon receiving a packet, a replica compares the piggybacked
dependency vector with its MAX. The replica ignores state parti-
tions with “don’t care” from this comparison. Once all prior piggy-
back logs have been received and applied, the replica applies and
replicates the piggyback log. For other state partitions, the replica
increments their associated sequence numbers in MAX.
Example: Figure 3 shows an example of using data dependency
vectors in the head and a successor replica with two threads. The
head and the replica begin with the same dependency vector for
three state partitions. First, the head performs a packet transaction
that writes to state partition 1 and increments the associated se-
quence number. The piggyback log belonging to this transaction
contains “don’t care” value for state partitions 2 and 3 (denoted
by x), since the transaction did not read or write these partitions.
Second, the head performs another transaction and forwards the
packet with a piggyback log.
Third, as shown the second packet arrives to the replica before
the first packet. Since the piggybacked dependency vector is out of
order, the replica holds the packet. Fourth, the first packet arrives.
Since the piggybacked vector is in order, the replica applies the
piggyback log and updates its local dependency vector accordingly.
Fifth, by applying the piggyback log of the first packet, the replica
now can apply the piggyback log of the held packet.
5 FTC FOR A CHAIN
Our protocol for a chain enables each middlebox to replicate the
chain’s state while processing packets. To accomplish this, we ex-
tend the original chain replication protocol [58] during both normal
operation and failure recovery. FTC supports middleboxes with dif-
ferent functionalities to run across the chain, while the same process
must be running across the nodes in the original chain replication
protocol. FTC’s failure recovery instantiates a new middlebox at the
failure position to maintain the chain order, while the traditional
protocol appends a new node at the end of a chain.
Figure 4 shows our protocol for a chain of n middleboxes. Our
protocol can be thought of as running n instances (per middlebox)
of the protocol developed earlier in § 4. FTC places a replica per
each middlebox. Replicas form n replication groups, each of which
provides fault tolerance for a single middlebox.
Viewing a chain as a logical ring, the replication group of a mid-
dlebox consists of a replica and its f succeeding replicas. Instead
of being dedicated to a single middlebox, a replica is shared among
f + 1 middleboxes and maintains a state store for each of them.
Among these middleboxes, a replica is the head of one replication
group and the tail of another replication group. A middlebox and
its head are co-located in the same server. For instance in Figure 4,
if f = 1 then the replica r1 is in the replication groups of middle-
boxes m1 and mn, and r2 is in the replication groups of m1 and m2.
Subsequently, the replicas rn and r1 are the head and the tail of
middlebox mn.
FTC adds two additional elements, the forwarder and buffer at
the ingress and egress of a chain. The forwarder and buffer are also
multithreaded, and are collocated with the first and last middle-
boxes. The buffer holds a packet until the state updates associated
with all middleboxes of the chain have been replicated. The buffer
also forwards state updates to the forwarder for middleboxes with
replicas at the beginning of the chain. The forwarder adds state
updates from the buffer to incoming packets before forwarding the
packets to the first middlebox.
5.1 Normal Operation of Protocol
Figure 4 shows the normal operation of our protocol. The forwarder
receives incoming packets from the outside world and piggyback
messages from the buffer. A piggyback message contains middlebox
state updates. As the packet passes through the chain, a replica
detaches and replicates the relevant parts of the piggyback message
and applies associated state updates to its state stores. A replica
ri tracks the state updates of a middlebox mi and updates the pig-
gyback message to include these state updates. Replicas at the
beginning of the chain replicate for middleboxes at the end of the
chain. The buffer withholds the packet from release until the state
updates of middleboxes at the end of the chain are replicated. The
buffer transfers the piggyback message to the forwarder that adds
it to incoming packets for state replication.
The forwarder receives incoming packets from outside world
and piggyback messages from the buffer. A piggyback message
consists of a list of piggyback logs and a list of commit vectors. The
tail of each replication group appends a commit vector to announce
the latest state updates that have been replicated f + 1 times for
the corresponding middlebox.
Each replica constantly receives packets with piggyback mes-
sages. A replica detaches and processes a piggyback message before
the packet transaction. As mentioned before, each replica is in the
replication group of f preceding middleboxes. For each of them,
the replica maintains a dependency vector MAX to track the latest
piggyback log that it has replicated in order. The replica processes
a relevant piggyback log from the piggyback message as described
in § 4.3. Once all prior piggyback logs are applied, the replica repli-
cates the piggyback log, applies state updates to the associated state
store, and updates the associated dependency vector MAX.
W(1)0,x,x1,x,42,3,50,3,41,3,4The headA replica1234The head’s dependency vector:122,3,50,3,41,3,4The replica’s dependency vector: 455held0,3,4 ≥0,x,x0,3,4 ≥1,x,41,3,4 ≥1,x,4R(i) denotes reading state partition i.W(i) denotes writing state partition i.R(1),W(3)x in a vector shows “don’t care”Fault Tolerant Service Function Chaining
SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
Figure 4: Normal operation for a chain. A middlebox and its head replica reside in the same server. The forwarder and buffer are located
in the first and last servers. 1○ The forwarder appends a state message containing the state updates of the last f middleboxes in the chain
from the buffer to an incoming packet, and forwards this packet to the first replica. 2○ Each replica applies the piggybacked state updates,
allows the middlebox to process the packet, and appends new state updates to the packet. 3○ Replicas at the beginning of the chain replicate
for middleboxes at the end of the chain. The buffer holds packets and releases them once state updates from the end of the chain are replicated.
4○ The buffer transfers the piggyback message to the forwarder that adds it to incoming packets for state replication.
Once the middlebox finishes the packet transaction, the replica
updates and reattaches the piggyback message to the packet, then
forwards the packet. For the replication group where the replica
is the head, it adds a piggyback log containing the state updates
of processing the packet. If the replica is a tail in the replication
group of a middlebox m, it removes the piggyback log belonging
to middlebox m to reduce the size of the piggyback message. The
reason is that a tail replicates the state updates of m for f +1-th time.
Moreover, it attaches its dependency vector MAX of middlebox m
as a commit vector. Later by reading this commit vector, the buffer
can safely release held packets. Successor replicas also use this
commit vector to prune obsolete piggyback logs.
To correctly release a packet, the buffer requires that the state
updates of this packet are replicated, specifically for each middlebox
with a preceding tail in the chain. The buffer withholds a packet
from release until an upcoming packet piggybacks commit vectors
that confirm meeting this requirement. Upon receiving an upcom-
ing packet, the buffer processes the piggybacked commit vectors to
release packets held in the memory.
Specifically, let m be a middlebox with a preceding tail, and
V2 be the end of updated range from a piggyback log of a held
packet belonging to m. Once the commit vector of each m from an
upcoming packet shows that all state updates prior to and including
V2 have been replicated, the buffer releases the held packet and
frees its memory.
Other considerations: There may be time periods that a chain
receives no incoming packets. In such cases, the state is not propa-
gated through the chain, and the buffer does not release packets. To
resolve this problem, the forwarder keeps a timer to receive incom-
ing packets. Upon the timeout, the forwarder sends a propagating
packet carrying a piggyback message it has received from the buffer.
Replicas do not forward a propagating packet to middleboxes. They
process and update the piggyback message as described before
and forward the packet along the chain. The buffer processes the
piggyback message to release held packets.
Some middlebox in a chain can filter packets (e.g., a firewall may
block certain traffic), and consequently the piggybacked state is not
passed on. For such a middlebox, its head generates a propagating
packet to carry the piggyback message of a filtered packet.
Finally, if the chain length is less than f + 1, we extend the chain
by adding more replicas prior to the buffer. These replicas only
process and update piggyback messages.
Packet with Piggyback MessageReplicaPiggyback MessagePacket!"!"!"!"MiddleboxState storeBuﬀer!"!"!"!"Forw.!"!"!"!"!!!!1234SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
5.2 Failure Recovery
Handling the failure of the forwarder or the buffer is straightfor-
ward. They contain only soft state, and spawning a new forwarder
or a new buffer restores the chain.
The failure of a middlebox and its head replica is not isolated,
since they reside on the same server. If a replica fails, FTC repairs f +
1 replication groups as each replica replicates for f +1 middleboxes.
The recovery involves three steps: spawning a new replica and a
new middlebox, recovering the lost state from other alive replicas,
and steering traffic through the new replica.
After spawning a new replica, the orchestrator informs it about
the list of replication groups in which the failed replica was a
member. For each of these replication group, the new replica runs
an independent state recovery procedure as follows. If the failed
replica was the head of a replication group, the new replica retrieves
the state store and the dependency vector MAX from the immediate
successor in this replication group. The new replica restores the
dependency matrix of the failed head by setting each of its row to the
retrieved MAX. For other replication groups, the new replica fetches
the state from the immediate predecessors in these replication
groups.
Once the state is recovered, the new replica notifies the orchestra-
tor to update routing rules to steer traffic through the new replica.
For simultaneous failures, the orchestrator waits until all new repli-
cas confirm that they have finished their state recovery procedures
before updating routing rules.
6 IMPLEMENTATION
FTC builds on the ONOS SDN controller [7] and Click [34]. We
use the ONOS controller as our NFV orchestrator to deploy middle-
boxes built on Click. Click is a popular toolkit to develop virtual
middleboxes using modular elements. The forwarder and buffer are
implemented as Click elements. Our implementation consists of 1K
lines of Java for the orchestrator and 6K lines of C++ for FTC and
middleboxes.
A replica consists of control and data plane modules. The control
module is a daemon that communicates with the orchestrator and
the control modules in other replicas. In failover, the control module
spawn a thread to fetch state per each replication group. Using a
reliable TCP connection, the thread sends a fetch request to the
appropriate member in the replication group and waits to receive
state.
The data plane module processes piggyback messages, sends and
receives packets to and from a middlebox, constructs piggyback
messages, and forwards packets to a next element in the chain
(the data-plane module of the next replica or the buffer). In our
experiments, the data plane also routes packets through a chain by
rewriting their headers.
FTC appends the piggyback logs to the end of a packet, and
inserts an IP option to notify our runtime that a packet has a piggy-
back message. As a piggyback message is appended at the end of a
packet, its process and construction can be performed in-place, and
there is no need to actually strip and reattach it. Before sending a
packet to the middlebox, the relevant header fields (e.g., the total
length in IP header) is updated to not account for the piggyback
message. Before forwarding the packet to next replica, the header
Middlebox State reads State writes Chain Middleboxes in chain
Ch-n Monitor1 → · · · → Monitorn
Ch-Gen Gen1 → Gen2
MazuNAT Per packet Per flow
SimpleNAT Per packet Per flow
Monitor Per packet Per packet Ch-Rec Firewall → Monitor → SimpleNAT
Gen No
Firewall N/A
Table 1: Experimental middleboxes and chains
Per packet
N/A
is updated back to reconsider the piggyback message. For middle-
boxes that may extend the packet, the data plane module operates
on the copy of a piggyback message.
7 EVALUATION
We describe our setup and methodology in § 7.1. We micro bench-
mark the overhead of FTC in § 7.2. We measure the performance
of FTC for middleboxes in § 7.3 and for chains in § 7.4. Finally, we
evaluate the failure recovery of FTC in § 7.5.
7.1 Experimental Setup and Methodology
We compare FTC with NF, a non fault-tolerant baseline system, and
FTMB, our implementation of [51]. Our FTMB implementation is a
performance upper bound of the original work that performs the
logging operations described in [51] but does not take snapshots.
Following the original prototype, FTMB dedicates a server in which
a middlebox master (M) runs, and another server where the fault
tolerant components input logger (IL) and output logger (OL) ex-
ecute. Packets go through IL, M, then OL. M tracks accesses to
shared state using packet access logs (PALs) and transmits them
to OL. In the original prototype, no data packet is released until
all corresponding dropped PALs are retransmitted. Our prototype
assumes that PALs are delivered on the first attempt, and packets
are released immediately afterwards. Further, OL maintains only
the last PAL.
We used two environments. The first is a local cluster of 12
servers. Each server has an 8-core Intel Xeon CPU D-1540 clocked
at 2.0 Ghz, 64 GiB of memory, and two NICs, a 40 Gbps Mellanox
ConnectX-3 and a 10 Gbps Intel Ethernet Connection X557. The
servers run Ubuntu 14.04 with kernel 4.4 and are connected to
10 and 40 Gbps top-of-rack switches. We use MoonGen [17] and
pktgen [57] to generate traffic and measure latency and throughput,
respectively. Traffic from the generator server, passed in the 40 Gbps
links, is sent through middleboxes and back to the generator. FTC
uses a 10 Gbps link to disseminate state changes from buffer to
forwarder.
The second environment is the SAVI distributed Cloud [30] com-