as an optimization problem that aims to change the global
model update the most along the opposite direction of g, by
optimizing the poisoned local model updates sent from the
malicious clients to the server. Different aggregation rules lead
to different instantiations of the optimization problem. Fang et
al. applied the framework to optimize local model poisoning
attacks for Krum (called Krum attack) as well as Trim-mean
and Median (called Trim attack).
Scaling attack [5]: This attack aims to corrupt the global
model to predict attacker-chosen target labels for attacker-
chosen target
testing examples, while the predicted labels
for other testing examples are unaffected (i.e., the normal
testing error rate remains the same). For instance, the attacker-
chosen target testing examples can be normal testing examples
embedded with a predeﬁned backdoor trigger (e.g., a logo, a
speciﬁc feature pattern). To achieve the goal, the Scaling attack
adds trigger-embedded training examples with the attacker-
chosen target label to the local training data of malicious
clients. The local model updates on malicious clients are then
computed based on the local training datasets augmented with
the trigger-embedded examples. However, the poisoned local
model updates may have limited impact on the global model
update because it is aggregated over all clients’ local model
updates. For instance, in FedAvg, the effect of the attack will
be diluted after the averaging [5]. Therefore, the attack further
scales the poisoned local model updates on malicious clients
by a factor that is much larger than 1. The scaled poisoned
local model updates are then sent to the server.
III. PROBLEM SETUP
Attack model: We follow the attack model
in previous
works [5], [7], [15]. Speciﬁcally, an attacker controls some
malicious clients, which can be fake clients injected by the at-
tacker or genuine ones compromised by the attacker. However,
the attacker does not compromise the server. The malicious
clients can send arbitrary local model updates to the server in
each iteration of the FL training process. Typically, an attacker
has the following partial knowledge about an FL system:
local training data and local model updates on the malicious
clients, loss function, and learning rate. We notice that the
Scaling attack [5] only requires such partial knowledge. The
Krum and Trim attacks [15] are also applicable in this partial-
knowledge setting. However, they are stronger in the full-
knowledge setting [15], where the attacker knows everything
about the FL training process, including the local training data
and local model updates on all clients in each iteration, as well
as the FL’s aggregation rule. Therefore, we consider such full-
knowledge setting to show that our method can defend against
strong attacks. Moreover, the attacker can perform adaptive
attacks to FLTrust, which we discuss in Section V.
Defense goals: We aim to design an FL method that achieves
Byzantine robustness against malicious clients without sacri-
ﬁcing the ﬁdelity and efﬁciency. In particular, we treat FedAvg
under no attacks as a baseline to discuss ﬁdelity and efﬁciency,
i.e., our method should be robust against malicious clients
while being as accurate and efﬁcient as FedAvg under no
attacks. Speciﬁcally, we aim to design a Byzantine-robust FL
method that achieves the following defense goals:
•
•
•
Fidelity. The method should not sacriﬁce the classi-
ﬁcation accuracy of the global model when there is
no attack. In particular, under no attacks, the method
should be able to learn a global model that is as
accurate as the global model learnt by FedAvg, a
popular FL method in non-adversarial settings.
Robustness. The method should preserve the classiﬁ-
cation accuracy of the global model in the presence of
malicious clients performing strong poisoning attacks.
In particular, we aim to design a method that can learn
a global model under attacks that is as accurate as
the global model learnt by FedAvg under no attacks.
Moreover, for targeted attacks, our goal further in-
cludes that the global model is unlikely to predict the
attacker-chosen target labels for the attacker-chosen
target testing examples.
Efﬁciency. The method should not incur extra compu-
tation and communications overhead, especially to the
clients. Clients in FL are often resource-constrained
devices. Therefore, we aim to design a method that
does not increase the workload of the clients, com-
pared to FedAvg under no attacks.
4
Fig. 2: Illustration of our aggregation rule, which is applied in each iteration of FLTrust.
Existing Byzantine-robust FL methods such as Krum,
Trim-mean, and Median do not satisfy the ﬁdelity and robust-
ness goals. Moreover, Krum does not satisfy the efﬁciency goal
because it requires the server to compute pairwise distances
of the clients’ local model updates, which is computationally
expensive when the number of clients is large.
Defender’s knowledge and capability: We consider the
defense is performed on the server side. The server does not
have access to the raw local training data on the clients, and
the server does not know the number of malicious clients.
However, the server has full access to the global model as well
as the local model updates from all clients in each iteration.
Moreover, the server itself can collect a clean small training
dataset (we call it root dataset) for the learning task. We require
the root dataset to be clean from poisoning. The server can
collect a clean root dataset by manual labeling. For instance,
Google enlists its employees to type with Gboard to create
the root dataset for its federated next-word prediction [1];
when the learning task is digit recognition, the service provider
can hire human workers to label some digits. Since we only
require a small root dataset, e.g., 100 training examples, it is
often affordable for the server to perform manual collection
and labeling. The root dataset may or may not follow the
same distribution as the overall training data distribution of the
learning task. Our experimental results show that our method
is effective once the root dataset distribution does not deviate
too much from the overall training data distribution.
IV. OUR FLTRUST
A. Overview of FLTrust
In our FLTrust, the server itself collects a small clean
training dataset (called root dataset) and maintains a model
(called server model) for it just like how a client maintains a
local model. In each iteration, our FLTrust follows the general
three steps of FL discussed in Section II-A. However, our
FLTrust is different from existing FL methods in Step II and
Step III. Speciﬁcally, in Step II, each client trains its local
model in existing FL methods, while the server also trains its
server model via ﬁne-tuning the current global model using
the root dataset in FLTrust. In Step III, existing FL methods
only consider the clients’ local model updates to update the
global model, which provides no root of trust. On the contrary,
FLTrust considers both the server model update and the clients’
local model updates to update the global model.
Speciﬁcally, an attacker can manipulate the directions of
the local model updates on the malicious clients such that the
global model is updated towards the opposite of the direction
along which it should be updated; or the attacker can scale
up the magnitudes of the local model updates to dominate
the aggregated global model update. Therefore, we take both
the directions and the magnitudes of the model updates into
consideration. In particular, FLTrust ﬁrst assigns a trust score
(TS) to a local model update based on its direction similarity
with the server model update. Formally, our trust score of
a local model update is its ReLU-clipped cosine similarity
with the server model update. Then, FLTrust normalizes each
local model update by scaling it to have the same magnitude
as the server model update. Such normalization essentially
projects each local model update to the same hyper-sphere
where the server model update lies in the vector space, which
limits the impact of the poisoned local model updates with
large magnitudes. Finally, FLTrust computes the average of
the normalized local model updates weighted by their trust
scores as the global model update, which is used to update the
global model.
B. Our New Aggregation Rule
Our new aggregation rule considers both the directions and
magnitudes of the local model updates and the server model
update to compute the global model update. Figure 2 illustrates
our aggregation rule.
ReLU-clipped cosine similarity based trust score: An
attacker can manipulate the directions of the local model
updates on the malicious clients such that the global model
update is driven to an arbitrary direction that the attacker
desires. Without root of trust, it is challenging for the server
to decide which direction is more “promising” to update the
global model. In our FLTrust, the root trust origins from the
direction of the server model update. In particular,
if the
direction of a local model update is more similar to that of
the server model update, then the direction of the local model
update may be more “promising”. Formally, we use the cosine
similarity, a popular metric to measure the angle between two
vectors, to measure the direction similarity between a local
model update and the server model update.
5
Local model updates 𝒈𝒈1,𝒈𝒈2& server model update 𝒈𝒈0ReLU-clipped cosine similarity basedtrust scoreNormalizing the magnitudes of local model updatesAggregation𝒈𝒈2𝒈𝒈1𝜃𝜃2𝒈𝒈0⟨𝒈𝒈1,𝒈𝒈0⟩𝒈𝒈1𝒈𝒈0⟨𝒈𝒈2,𝒈𝒈0⟩𝒈𝒈2𝒈𝒈0𝑇𝑇𝑇𝑇1=ReLU𝑐𝑐1=𝑐𝑐1𝑇𝑇𝑇𝑇2=ReLU𝑐𝑐2=0𝒈𝒈1=𝒈𝒈0𝒈𝒈1𝒈𝒈1𝒈𝒈1𝒈𝒈2𝒈𝒈2=𝒈𝒈0𝒈𝒈2𝒈𝒈2𝒈𝒈=1𝑇𝑇𝑇𝑇1+𝑇𝑇𝑇𝑇2(𝑇𝑇𝑇𝑇1𝒈𝒈1+𝑇𝑇𝑇𝑇2𝒈𝒈2)𝑐𝑐1=cos𝜃𝜃1=𝑐𝑐2=cos𝜃𝜃2=𝒈𝒈2𝒈𝒈1𝜃𝜃2𝒈𝒈0𝒈𝒈05:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
Algorithm 1 ModelUpdate(w, D, b, β, R)
Output: Model update.
1: w0 ← w.
2: for r = 1, 2,··· , R do
3:
4:
5: end for
6: return wR − w.
Randomly sample a batch Db from D.
wr ← wr−1 − β∇Loss(Db; w).
However, the cosine similarity alone faces a challenge.
Speciﬁcally, if a local model update and the server model
update are in opposite directions, their cosine similarity is neg-
ative, which still has negative impact on the aggregated global
model update (see our experimental results in Section VI-B).
Therefore, we exclude such local model updates from the
aggregation by clipping the cosine similarity. In particular, we
use the popular ReLU operation for clipping. Formally, our
trust score is deﬁned as follows:
T Si = ReLU (ci),
(2)
where T Si is the trust score for the ith local model update gi,
and ci is the cosine similarity between gi and the server model
(cid:104)gi,g0(cid:105)
update g0, i.e., ci =
||gi||·||g0||. ReLU is deﬁned as follows:
ReLU (x) = x if x > 0 and ReLU (x) = 0 otherwise.
Normalizing the magnitudes of local model updates: An
attacker can also scale the magnitudes of the local model
updates on the malicious clients by a large factor such that they
dominate the global model update. Therefore, we normalize the
magnitude of each local model update. Without root of trust,
it is challenging to decide what quantity we should normalize
to. However, the server has the root dataset to bootstrap trust
in FLTrust. Therefore, we normalize each local model update
such that it has the same magnitude as the server model update.
Such normalization means that we rescale local model updates
to be the same hyper-sphere where the server model update lies
in the vector space. Formally, we have the following:
¯gi =
||g0||
||gi|| · gi,
(3)
where gi is the local model update of the ith client in the
current iteration, ¯gi is the normalized local model update of
the ith client, g0 is the server model update, and || · || means
(cid:96)2 norm of a vector. Our normalization ensures that no single
local model update has too much impact on the aggregated
global model update. Note that our normalization also enlarges
a local model update with a small magnitude to have the
same magnitude as the server model update. This is based on
the intuition that local model updates with small magnitudes
are more likely from benign clients, and thus enlarging their
magnitudes helps reduce the impact of the poisoned local
model updates from the malicious clients, leading to a better
global model (see our experimental results in Section VI-B).
Aggregating the local model updates: We compute the
average of the normalized local model updates weighted by
their trust scores as the global model update:
n(cid:88)
T Si · ¯gi
1
n(cid:80)
g =
T Sj
i=1
j=1
6
Algorithm 2 FLTrust
Input: n clients with local
training datasets Di, i =
1, 2,··· , n; a server with root dataset D0; global learning
rate α; number of global iterations Rg; number of clients
τ sampled in each iteration; local learning rate β; number
of local iterations Rl; and batch size b.
Output: Global model w.
1: w ← random initialization.
2: for r = 1, 2,··· , Rg do
3:
4:
// Step I: The server sends the global model to clients.
The server randomly samples τ clients C1, C2,··· , Cτ
from {1, 2,··· , n} and sends w to them.
// Step II: Training local models and server model.
// Client side.
for i = C1, C2,··· , Cτ do in parallel
gi = M odelU pdate(w, Di, b, β, Rl).
Send gi to the server.
end for
// Server side.
g0 = M odelU pdate(w, D0, b, β, Rl).
the local model updates.
// Step III: Updating the global model via aggregating
for i = C1, C2,··· , Cτ do
(cid:107)gi(cid:107)(cid:107)g0(cid:107)
(cid:16) (cid:104)gi,g0(cid:105)
(cid:17)
.
T Si = ReLU
||g0||
||gi|| · gi.
τ(cid:80)
¯gi =
end for
g =
w ← w − α · g.
τ(cid:80)
T SCj
i=1
j=1
1
T SCi · ¯gCi.
19:
20: end for
21: return w.
n(cid:88)
1
ReLU (cj)
i=1
ReLU (ci) · ||g0||
||gi|| · gi,
(4)
=
n(cid:80)
j=1
where g is the global model update. Note that if the server
selects a subset of clients in an iteration, the global model
update is aggregated from the local model updates of the
selected clients. In principle, the server model update can
be treated as a local model update with a trust score of 1
and the global model update can be weighted average of the
clients’ local model updates together with the server model
update. However, such variant may negatively impact
the
global model because the root dataset is small and may not
have the same distribution as the training data, but the server
model update derived from it has a trust score of 1, reducing
the contributions of the benign clients’ local model updates
(see our experimental results in Section VI-B). Finally, we
update the global model as follows:
w ← w − α · g,
(5)
where α is the global learning rate.
C. Complete FLTrust Algorithm
Algorithm 2 shows our complete FLTrust method. FLTrust
performs Rg iterations and has three steps in each iteration.
In Step I, the server sends the current global model to the
clients or a subset of them. In Step II, the clients compute the
local model updates based on the global model and their local
training data, which are then sent to the server. Meanwhile, the
server itself computes the server model update based on the
global model and the root dataset. The local model updates
and the server model update are computed by the function
ModelUpdate in Algorithm 1 via performing stochastic gra-
dient descent for Rl iterations with a local learning rate β.
In Step III, the server computes the global model update by
aggregating the local model updates and uses it to update the
global model with a global learning rate α.
D. Formal Security Analysis
the parameter space of the global model, D = (cid:83)n
As we discussed in Section II-A, the optimal global model
w∗ is a solution to the following optimization problem:
w∗ = arg minw∈Θ F (w) (cid:44) ED∼X [f (D, w)], where Θ is
i=1 Di is
the joint training dataset of the n clients, X is the training
data distribution, f (D, w) is the empirical loss function on
the training data D, and F (w) is the expected loss function.
Our FLTrust is an iterative algorithm to ﬁnd a global model to
minimize the empirical loss function f (D, w). We show that,
under some assumptions, the difference between the global
model learnt by FLTrust under attacks and the optimal global
model w∗ is bounded. Next, we ﬁrst describe our assumptions
and then describe our theoretical results.
Assumption 1. The expected loss function F (w) is µ-strongly
convex and differentiable over the space Θ with L-Lipschitz
continuous gradient. Formally, we have the following for any
F ((cid:98)w) ≥ F (w) + (cid:104)∇F (w), (cid:98)w − w(cid:105) +
(cid:107)∇F (w) − ∇F ((cid:98)w)(cid:107) ≤ L(cid:107)w − (cid:98)w(cid:107) ,
µ