networks,” in Nat Mach Intell 2, 2020, pp. 665—-673.
[17] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” in
Advances in Neural Information Processing Systems, 2014.
[18] T. Gu, B. Dolan-Gavitt, and S. Garg, “Badnets: Identifying vulnerabil-
ities in the machine learning model supply chain,” in Proceedings of
the NIPS Workshop on Mach. Learn. and Comp. Sec, 2017.
[19] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770–778.
[20] S. Hong, V. Chandrasekaran, Y. Kaya, T. Dumitra¸s, and N. Papernot,
“On the effectiveness of mitigating data poisoning attacks with gradient
shaping,” arXiv preprint arXiv:2002.11497, 2020.
[21] A. Ilyas, S. Santurkar, D. Tsipras, L. Engstrom, B. Tran, and A. Madry,
“Adversarial examples are not bugs, they are features,” in Advances in
Neural Information Processing Systems, 2019, pp. 125–136.
[22] M. Jagielski, A. Oprea, B. Biggio, C. Liu, C. Nita-Rotaru, and B. Li,
“Manipulating machine learning: Poisoning attacks and countermea-
sures for regression learning,” in 2018 IEEE Symposium on Security
and Privacy (SP).
IEEE, 2018, pp. 19–35.
[23] M. Kearns and M. Li, “Learning in the presence of malicious errors,”
SIAM Journal on Computing, vol. 22, no. 4, pp. 807–837, 1993.
[24] N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P.
Tang, “On large-batch training for deep learning: Generalization gap
and sharp minima,” International Conference on Learning Representa-
tions, 2017.
[25] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
International Conference on Learning Representations, 2015.
[26] M. Kloft and P. Laskov, “Online anomaly detection under adversarial
impact,” in Proceedings of the thirteenth international conference on
artiﬁcial intelligence and statistics, 2010, pp. 405–412.
[27] ——, “Security analysis of online centroid anomaly detection,” The
Journal of Machine Learning Research, vol. 13, no. 1, 2012.
[28] P. W. Koh and P. Liang, “Understanding black-box predictions via in-
ﬂuence functions,” in Proceedings of the 34th International Conference
on Machine Learning-Volume 70.
JMLR. org, 2017, pp. 1885–1894.
[29] A. Krizhevsky, G. Hinton et al., “Learning multiple layers of features
from tiny images,” (Technical Report), 2009.
[30] A. Krogh and J. A. Hertz, “A simple weight decay can improve gen-
eralization,” in Advances in Neural Information Processing Systems,
1992, pp. 950–957.
[31] S. Laine and T. Aila, “Temporal ensembling for semi-supervised learn-
ing,” International Conference on Learning Representations, 2017.
[32] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” nature, vol. 521,
no. 7553, pp. 436–444, 2015.
[33] D.-H. Lee, “Pseudo-label: The simple and efﬁcient semi-supervised
learning method for deep neural networks,” in Workshop on challenges
in representation learning, ICML, vol. 3, 2013, p. 2.
[34] K. Liu, B. Dolan-Gavitt, and S. Garg, “Fine-pruning: Defending against
backdooring attacks on deep neural networks,” in International Sym-
posium on Research in Attacks, Intrusions, and Defenses. Springer,
2018, pp. 273–294.
[35] X. Liu, S. Si, X. Zhu, Y. Li, and C.-J. Hsieh, “A uniﬁed framework
for data poisoning attack to graph-based semi-supervised learning,”
Advances in Neural Information Processing Systems, 2020.
[36] Y. Liu, X. Ma, J. Bailey, and F. Lu, “Reﬂection backdoor: A natural
backdoor attack on deep neural networks,” in European Conference on
Computer Vision. Springer, 2020, pp. 182–199.
[37] D. Mahajan, R. Girshick, V. Ramanathan, K. He, M. Paluri, Y. Li,
A. Bharambe, and L. van der Maaten, “Exploring the limits of weakly
supervised pretraining,” in Proceedings of the European Conference
on Computer Vision (ECCV), 2018, pp. 181–196.
[38] G. J. McLachlan, “Iterative reclassiﬁcation procedure for constructing
an asymptotically optimal rule of allocation in discriminant analysis,”
Journal of the American Statistical Association, vol. 70, no. 350, pp.
365–369, 1975.
[39] T. Miyato, S.-i. Maeda, M. Koyama, and S. Ishii, “Virtual adversarial
training: a regularization method for supervised and semi-supervised
learning,” IEEE transactions on pattern analysis and machine intelli-
gence, vol. 41, no. 8, pp. 1979–1993, 2018.
1590    30th USENIX Security Symposium
USENIX Association
[40] B. Nelson, M. Barreno, F. J. Chi, A. D. Joseph, B. I. Rubinstein,
U. Saini, C. A. Sutton, J. D. Tygar, and K. Xia, “Exploiting machine
learning to subvert your spam ﬁlter.” LEET, vol. 8, pp. 1–9, 2008.
[58] R. Taori, A. Dave, V. Shankar, N. Carlini, B. Recht, and L. Schmidt,
“Measuring robustness to natural distribution shifts in image classiﬁca-
tion,” Advances in Neural Information Processing Systems, 2020.
[41] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng,
“Reading digits in natural images with unsupervised feature learning,”
Workshop on Deep Learning and Unsupervised Feature Learning, 2011.
[42] A. Oliver, A. Odena, C. A. Raffel, E. D. Cubuk, and I. Goodfellow,
“Realistic evaluation of deep semi-supervised learning algorithms,” in
Advances in Neural Information Processing Systems, 2018.
[43] S. J. Pan and Q. Yang, “A survey on transfer learning,” IEEE Trans-
actions on knowledge and data engineering, vol. 22, no. 10, pp. 1345–
1359, 2009.
[44] D. S. Park, Y. Zhang, Y. Jia, W. Han, C.-C. Chiu, B. Li, Y. Wu, and Q. V.
Le, “Improved noisy student training for automatic speech recognition,”
arXiv preprint arXiv:2005.09629, 2020.
[45] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg et al.,
“Scikit-learn: Machine learning in python,” the Journal of machine
Learning research, vol. 12, pp. 2825–2830, 2011.
[46] A. Radford, L. Metz, and S. Chintala, “Unsupervised representation
learning with deep convolutional generative adversarial networks,”
arXiv preprint arXiv:1511.06434, 2015.
[47] B. Recht, R. Roelofs, L. Schmidt, and V. Shankar, “Do ImageNet clas-
siﬁers generalize to ImageNet?” in Proceedings of the 36th Interna-
tional Conference on Machine Learning, ser. Proceedings of Machine
Learning Research, K. Chaudhuri and R. Salakhutdinov, Eds., vol. 97.
PMLR, 09–15 Jun 2019, pp. 5389–5400.
[48] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
Z. Huang, A. Karpathy, A. Khosla, M. Bernstein et al., “Imagenet large
scale visual recognition challenge,” International journal of computer
vision, vol. 115, no. 3, pp. 211–252, 2015.
[49] R. Schuster, C. Song, E. Tromer, and V. Shmatikov, “You autocom-
plete me: Poisoning vulnerabilities in neural code completion,” in 30th
{USENIX} Security Symposium ({USENIX} Security 21), 2021.
[50] H. Scudder, “Probability of error of some adaptive pattern-recognition
machines,” IEEE Transactions on Information Theory, vol. 11, no. 3,
pp. 363–371, 1965.
[51] A. Shafahi, W. R. Huang, M. Najibi, O. Suciu, C. Studer, T. Dumitras,
and T. Goldstein, “Poison frogs! targeted clean-label poisoning attacks
on neural networks,” in Advances in Neural Information Processing
Systems, 2018, pp. 6103–6113.
[52] R. Shokri, M. Stronati, C. Song, and V. Shmatikov, “Membership infer-
ence attacks against machine learning models,” in 2017 IEEE Sympo-
sium on Security and Privacy (SP).
IEEE, 2017, pp. 3–18.
[53] K. Sohn, D. Berthelot, C.-L. Li, Z. Zhang, N. Carlini, E. D. Cubuk,
A. Kurakin, H. Zhang, and C. Raffel, “Fixmatch: Simplifying semi-
supervised learning with consistency and conﬁdence,” Advances in
Neural Information Processing Systems, 2020.
[54] K. Sohn, Z. Zhang, C.-L. Li, H. Zhang, C.-Y. Lee, and T. Pﬁster, “A
simple semi-supervised learning framework for object detection,” arXiv
preprint arXiv:2005.04757, 2020.
[55] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut-
dinov, “Dropout: a simple way to prevent neural networks from over-
ﬁtting,” The journal of machine learning research, vol. 15, no. 1, pp.
1929–1958, 2014.
[56] J. Steinhardt, P. W. W. Koh, and P. S. Liang, “Certiﬁed defenses for
data poisoning attacks,” in Advances in neural information processing
systems, 2017, pp. 3517–3529.
[57] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow,
and R. Fergus, “Intriguing properties of neural networks,” International
Conference on Learning Representations, 2014.
[59] A. Tarvainen and H. Valpola, “Mean teachers are better role models:
Weight-averaged consistency targets improve semi-supervised deep
learning results,” in Advances in Neural Information Processing Sys-
tems, 2017, pp. 1195–1204.
[60] A. Turner, D. Tsipras, and A. Madry, “Label-consistent backdoor at-
tacks,” arXiv preprint arXiv:1912.02771, 2019.
[61] V. Vanhoucke,
[Online].
2019.
the-quiet-semi-supervised-revolution-edec1e9ad8c
“The
quiet
Available:
semi-supervised
revolution,”
https://towardsdatascience.com/
[62] V. Vapnik, “Principles of risk minimization for learning theory,” in
Advances in Neural Information Processing Systems, 1992.
[63] B. Wang, Y. Yao, B. Viswanath, H. Zheng, and B. Y. Zhao, “With great
training comes great vulnerability: Practical attacks against transfer
learning,” in 27th USENIX Security Symposium (USENIX Security 18),
2018, pp. 1281–1297.
[64] J. H. Ward Jr, “Hierarchical grouping to optimize an objective function,”
Journal of the American statistical association, vol. 58, no. 301, pp.
236–244, 1963.
[65] C. Xie, M. Tan, B. Gong, J. Wang, A. L. Yuille, and Q. V. Le, “Ad-
versarial examples improve image recognition,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2020, pp. 819–828.
[66] Q. Xie, Z. Dai, E. Hovy, M.-T. Luong, and Q. V. Le, “Unsupervised
data augmentation for consistency training,” Advances in Neural Infor-
mation Processing Systems, 2020.
[67] Q. Xie, M.-T. Luong, E. Hovy, and Q. V. Le, “Self-training with
noisy student improves imagenet classiﬁcation,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), June 2020.
[68] Y. Yao, H. Li, H. Zheng, and B. Y. Zhao, “Latent backdoor attacks on
deep neural networks,” in Proceedings of the 2019 ACM SIGSAC Con-
ference on Computer and Communications Security, 2019, pp. 2041–
2055.
[69] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals, “Understand-
ing deep learning requires rethinking generalization,” International
Conference on Learning Representations, 2017.
[70] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, “mixup: Beyond
empirical risk minimization,” International Conference on Learning
Representations, 2018.
[71] C. Zhu, W. R. Huang, A. Shafahi, H. Li, G. Taylor, C. Studer, and
T. Goldstein, “Transferable clean-label poisoning attacks on deep neural
nets,” Proceedings of Machine Learning Research. Volume 97., 2019.
[72] X. J. Zhu, “Semi-supervised learning literature survey,” University of
Wisconsin-Madison Department of Computer Sciences, Tech. Rep.,
2005.
A Semi-Supervised Learning Methods Details
We begin with a description of three state-of-the-art methods:
• MixMatch [3] generates a label guess for each unla-
beled image, and sharpens this distribution by increas-
ing the softmax temperature. During training MixMatch
penalizes the L2 loss between this sharpened distribution
and another prediction on the unlabeled example. As ad-
ditional regularization, MixMatch applies MixUp [70],
weight decay [30], an exponential moving average of
model parameters, trained with the Adam optimizer [25].
USENIX Association
30th USENIX Security Symposium    1591
• UDA [66] at its core behaves similarly to MixMatch,
and generates label guesses and matches the sharpened
guess as MixMatch does. Instead of applying standard
weak augmentation, UDA was the ﬁrst method to show
that strong augmentation can effectively increase the
accuracy of semi-supervised learning algorithms. UDA
again also contains a number of implementation details,
including a cosine decayed learning rate, an additional
KL-divergence loss regularizer, and training signal an-
nealing.
• FixMatch [53] is a simpliﬁcation of MixMatch and
UDA. FixMatch again generates a guessed label and
trains on this label, however it uses hard pseudo-labels
instead of a sharpened label distribution and only trains
on examples that exceed a conﬁdence threshold. By care-
fully tuning parameters, FixMatch is able to remove from
MixMatch the MixUp regularization and Adam training,
and remove from UDA all of the details above (KLD
loss, training signal annealing). Because it is a simpler
methods, it is easier to determine optimal hyperparame-
ter choices and it is able to achieve higher accuracy.
We also describe the four prior methods:
• Pseudo Labels [33] is one of the early semi-supervised
learning methods that gave high accuracy, and is built
on by most others. This technique introduced the label
guessing procedure, and exclusively works by generating
a guessed label for each unlabeled example, and then
training on that guessed label.
• Virtual Adversarial Training [39] proceeds by guess-
ing a label for each unlabeled example using the current
model weights. Then, it minimizes two terms. First, it
minimizes the entropy of the guessed label, to encourage
conﬁdent predictions on unlabeled examples. Then, it
generates an adversarial perturbation δ for each unla-
beled example and encourages the prediction f (xu) to
be similar to the prediction f (xu + δ).
• Mean Teacher [59] again generates a pseudo label for
each unlabeled example and trains on this pseudo label.
However, instead of generating a pseudo label using
the current model weights, it generates the pseudo label
using an exponential moving average of prior model
weights.
• PiModel [31] relies heavily on consistency regular-
ization. This technique introduces the augmentation-
consistency as used in prior techniques, however this
paper uses network dropout instead of input-space per-
turbations to regularize the model predictions.
B Additional Defense Figures
Figure 9: Our defense reliably detects poisoning attacks using
FixMatch on CIFAR-10. In all cases, we perfectly separate the
standard training data from the injected poisoned examples.
Figure 10: Our defense reliably detects poisoning attacks
using MixMatch on CIFAR-10. In all cases, we perfectly
separate the standard training data from the injected
Figure 11: Our defense reliably detects poisoning attacks
using MixMatch on SVHN. In all cases, we perfectly separate
the standard training data from injected poisoned examples.
Figure 12: Our defense reliably detects poisoning attacks us-
ing FixMatch on SVHN. In all cases, we perfectly separate the
standard training data from the injected poisoned examples.
1592    30th USENIX Security Symposium
USENIX Association
10−510−410−310−210−110−0Mean Influence of 5 Nearest Neighbors   10−210−110−0Frequency (log scaled)10−510−410−310−210−110−0Mean Influence of 5 Nearest Neighbors   10−210−110−0Frequency (log scaled)10−510−410−310−210−110−0Mean Influence of 5 Nearest Neighbors   10−210−110−0Frequency (log scaled)10−510−410−310−210−110−0Mean Influence of 5 Nearest Neighbors   10−210−110−0Frequency (log scaled)10−510−410−310−210−110−0Mean Influence of 5 Nearest Neighbors   10−210−110−0Frequency (log scaled)10−510−410−310−210−110−0Mean Influence of 5 Nearest Neighbors   10−210−110−0Frequency (log scaled)10−510−410−310−210−110−0Mean Influence of 5 Nearest Neighbors   10−210−110−0Frequency (log scaled)10−510−410−310−210−110−0Mean Influence of 5 Nearest Neighbors   10−210−110−0Frequency (log scaled)