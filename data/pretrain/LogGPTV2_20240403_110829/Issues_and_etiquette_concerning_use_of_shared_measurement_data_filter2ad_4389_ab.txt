analysis for which it was provided. Researchers must not
treat privately shared data as a general resource that can be
analyzed at will without the explicit consent of the provider.3
Note: In general it makes sense for researchers to keep data
they have collected to address concerns with their analysis,
or to follow up on questions generated by peer review or sub-
sequent publication. However, it also makes sense to delete
someone else’s data—provided for a speciﬁc purpose—as
soon as it is no longer needed, such that the data does not fall
into the wrong hands, or lead to the temptation for reuse in
another context. The tension between these competing goals
is fairly fundamental, and therefore we encourage providers
and users to explicitly address data retention as part of an
Acceptable Use policy.
• If a researcher wishes to employ data for another task, they
should seek permission from the provider. First and fore-
most, the provider may not wish their data to be used for the
new purpose the researcher has in mind. In fact, the provider
might already be engaged in research on the new topic them-
selves, for which being “scooped” by someone using their
own data would prove highly frustrating.
In addition, the provider may have transformed the shared
data in a way that can (sometimes invisibly) render the re-
searcher’s results incorrect. For instance, consider a packet
trace for which the provider removed large bulk transfers
corresponding to backup trafﬁc, because these consumed a
great amount of space in the trace yet had little bearing on
the analysis for which the provider originally made the data
available. However, if that data were subsequently studied
for network utilization, it would show the network much less
loaded than actually was the case.
Finally, the sensitivity “threat model” the provider had in
mind when originally providing the trace may differ in the
context of the new form of analysis, for which the researcher
using the data has a strong obligation to honor the provider’s
concerns.
3In our experience, reusing data can prove a signiﬁcant temptation,
due to the general difﬁculty of obtaining rich, apt datasets.
3.3 De-anonymizing Data
More than any other activity, efforts to de-anonymize shared
measurement data have the potential to cause serious problems
for future data release across the community.
Careless de-
anonymization efforts can violate privacy, increase a site’s expo-
sure to security problems, or potentially embarrass a data provider.
In addition, careless reporting on such activities from the research
community itself—the very people the data provider is trying to
help—can profoundly change the threat model applied to future
data release. Thus, it is important to appreciate that such activities
can have a chilling effect across the community, rendering poten-
tial providers not even involved with a de-anonymized dataset quite
reluctant to release data in the future.
Therefore, ﬁrst
that de-
anonymization of measurement data should not be undertaken as
sport.
and foremost we
emphasize
That said, there are scholarly reasons to attempt to de-anonymize
measurement data. If a researcher can illustrate how to leverage a
modicum of information to untangle an anonymization scheme, and
doing so points to better anonymization techniques, then such an ef-
fort can comprise a signiﬁcant beneﬁt for the community. However,
researchers wishing to engage in this sort of analysis must proceed
carefully. First, they should undertake such an activity only with
the consent of the data provider—either because such activity is
part of the normal Acceptable Use policy provided with the data, or
per a speciﬁc arrangement with the data provider. Second, report-
ing on such an investigation should refrain from openly publishing
the speciﬁc, de-anonymized data.
The data provider often holds the ground truth, such that they
can inform researchers whether their de-anonymization schemes
succeeded.4 Therefore, as a part of the scientiﬁc process, an in-
vestigator attempting to de-anonymize data for scholarly purposes
should try to verify their results with the data provider. A natu-
ral hesitation to approaching providers in this way can arise be-
cause of a perceived conﬂict-of-interest: the data provider may sim-
ply indicate that the researcher did not properly de-anonymize the
data—regardless of whether the researcher was accurate or not—
in the hopes that the researcher will thus not publicize their ef-
forts, and hence keep unrevealed any information problematic for
the provider. On the other hand, the data provider has a vested inter-
est in understanding ﬂaws in their anonymization scheme, that they
might ﬁx the problems before releasing more data. In addition, if
the provider and researcher follow the general advice in this note,
then there will already be an understanding of what the researcher
is doing, and therefore likely a working relationship such that a re-
ﬂexive “nope, not right” reaction from the provider becomes less
likely.
We also note that reports of de-anonymization techniques should
not directly un-mask details of a dataset (e.g., IP addresses). Sig-
niﬁcantly better is to describe the process and note that the provider
has veriﬁed that it indeed recovers sensitive data. (Of course, pro-
viding a ﬁx for the problem in terms of a more secure way to per-
form the anonymization is also quite useful.) We encourage review-
ers, program committees and editors to require authors to follow
this path, rather than publishing sensitive details of datasets.
To avoid the thorny issues of dealing with and reporting on oth-
ers’ data, a different approach for researchers studying attacks on
anonymization is to focus on the anonymization techniques rather
than the anonymized data. That is, the researchers can re-apply
the anonymization used by a particular data provider, but to data
4This is not always true, as some data providers (partially)
anonymize using keys that they subsequently discard [8].
that they themselves capture. They then assess the possible attacks
against the new data. Such an approach can completely factor out
a provider’s sensitive data from the investigation. On the other
hand, collecting data from a variety of sources inevitably yields
different artifacts. Therefore, without using the provider’s data,
the researcher may not get as full a picture of the strength of the
provider’s anonymization techniques.
3.3.1 Case Study
As a concrete example, [7] reports on an investigation into de-
anonymizing several recently released datasets. The authors of
the study do this with an eye towards enhancing the community’s
understanding of anonymization techniques and where they break
down. The study provides a useful illustration of several of the
items discussed above.
The study reports apparent mappings between the IP addresses
in anonymized datasets and the real IP addresses, as inferred by
the authors’ techniques. However, the authors did not approach the
data providers regarding this attempt, for fear of creating a con-
ﬂict of interest [11], illustrating the uncertain community culture
regarding how to undertake such studies.5 The down-side to the
study not including such interactions is that the authors were un-
able to compare their results with ground truth—and thus, in fact,
ended up incorrectly de-anonymizing nearly all of the IP address
mappings reported in the paper for our LBNL dataset [2], to the
detriment of assessing the underlying scientiﬁc issues. On the other
hand, the authors’ de-anonymization scheme clearly has signiﬁcant
merit, since in addition they employed the same anonymization
techniques as used on the LBNL traces on their own packet traces
(in line with the approach for which we advocate above), for which
their de-anonymization techniques worked well. Taken together,
the above two points also nicely illustrate why a breadth of data
can be highly beneﬁcial when analyzing measurement data.
The ﬁnal point we draw from this example is that the concerns
expressed in this section are not theoretical. Exposing a pur-
ported mapping of anonymized IP addresses to real IP addresses
risks making further release of data from LBNL more difﬁcult.
While the general form of the techniques presented in [7] is well
known, and was in fact taken into account by LBNL [13], the threat
model used at LBNL was of a malicious attacker—not the research
community—scouring the data for information. At a minimum,
the actions of the research community will need to be explained
and defended to LBNL’s decision makers (likely the CIO) before
additional data gains approval for release.
As noted in § 1, our understanding of the issues with data release
and use has evolved over time. In this case, we failed to accompany
our data with a discussion of expectations regarding use of the data,
and the terms of our commitment to work with researchers study-
ing it. We believe the community’s understanding of these issues
is also evolving. Therefore, we should aim to understand the dif-
ferent perspectives of the involved researchers, and from this work
towards a common data-sharing culture for the community.
3.4 Notiﬁcation and Acknowledgment
Earlier we discussed how data providers should consider explic-
itly stating what sort of notiﬁcation they would like when a re-
searcher uses their data or when it later appears in a publication,
and what sort of acknowledgment any such publications should in-
clude. Naturally, data users should honor these requests. In ad-
dition, if data users are uncertain regarding the provider’s desired
5Furthermore, the authors of [7] perceived a double conﬂict-of-
interest in this case, because the data providers were also the au-
thors of the anonymization techniques.
notiﬁcation policies, they should attempt to contact the provider
to learn them. In the absence of explicit guidance, best is to as-
sume that the provider desires notiﬁcation and an acknowledgment
in publications, including the location of the data, if publicly avail-
able.
The above points may seem somewhat obvious, but we note them
here to frame an additional facet of such notiﬁcation/credit of which
data users are often unaware: some data providers ﬁnd it highly
beneﬁcial—either internal to their organization, or when interact-
ing with their funders—to tabulate the uses that researchers have
made of the released data. Since data release entails signiﬁcant
work for the provider in gaining the institute’s approval, obtaining
funding to support an altruistic activity, and anonymizing the data it
behooves the community to give providers the necessary “ammo”
for presenting a case that such release has broad beneﬁt, and re-
sults in positive public recognition for both the institute and those
responsible for the data release.6
4.
INTERACTIONS
In the previous two sections we have discussed what data
providers should furnish to users (§ 2) and what responsibilities
the users of the data have to the provider (§ 3).
In this section,
we explore issues regarding subsequent interactions between data
providers and data users. Analyzing measurement data is often a
messy process, whereby additional context can often shed much
light on the observed phenomena. Unfortunately, often when using
someone else’s data, the amount of additional context is in short
supply. In addition, anonymizing data—nearly always a require-
ment when sharing—tends to introduce additional blind spots into
the analysis process, which can leave researchers using shared data
with lingering questions.
We advocate that researchers should ask data providers explicit
questions when such situations arise, rather than making indepen-
dent assumptions or assertions about the data. When doing so,
researchers should temper their questions to the data providers to
only those that are vital to their analysis. Of course, data providers
may or may not be able to answer the questions for a variety
of reasons (e.g., lack of time/energy, lack of additional context,
privacy/competitive concerns, etc.). Further, seemingly mundane
questions can result in a large amount of analysis to ﬁnd an accept-
able answer. While we do not wish to put providers on the hook
for answering every question that comes their way, we suggest that
providers make reasonable efforts to answer reasonable questions
about data sets they release, to help foster an effective culture for
sharing measurement data.
As discussed in § 3.3, one natural place where puzzles arise con-
cerns working on de-anonymizing data. As sketched above, this
activity should only be conducted under mutual agreement between
the provider and the researcher. As part of this agreement, the
parties should discuss information the provider will convey when
checking the de-anonymized data against ground truth.
In the case where researchers ultimately must make assumptions
about the data because they cannot get answers from the provider,
they should explicitly note these assumptions when reporting on the
data analysis. In addition, they should frame their efforts to work
with the data provider.
In turn, peer reviewers should expect communication on key
points of the analysis between providers and researchers, and resist
cases where researchers have seemingly not made efforts to vali-
date their assumptions with data providers.7 We stress, however,
that we advocate applying such a standard only for key analysis
points; not for minor or tangential aspects of the data analysis.
5. SUMMARY
Our goal for this note is to help evolve the community’s under-
standing about the care required when releasing measurement data,
and the sensitivity of others using such data. We advocate that data
providers be explicit in terms of a dataset’s acceptable use, and
researchers thoughtful in the reporting of potentially sensitive in-
formation gleaned from others’ data. Data providers should also
convey what interactions they desire or will accommodate, and re-
searchers should comply with such interactions.
In general, measurement is a painfully laborious undertaking,
and therefore there is great beneﬁt in leveraging others’ efforts in
the form of shared measurement data. But providing such data is
not without its own signiﬁcant labors. Thus, it behooves the re-
search community to foster a culture to support such sharing as
best we can.
6. ACKNOWLEDGMENTS
We thank Paul Barford, Fabian Monrose and Mike Reiter for
their discussion of the issues presented in this note, and for feed-
back on a draft version. This work was supported in DHS Award
HSHQPA4X03322 and NSF Awards ITR/ANI-0205519 and NSF-
0433702. Any opinions, ﬁndings, and conclusions or recommenda-
tions expressed in this material are those of the authors or origina-
tors and do not necessarily reﬂect the views of the National Science
Foundation.
7. REFERENCES
[1] Community Resource for Archiving Wireless Data At
Dartmouth (CRAWDAD). http://crawdad.cs.dartmouth.edu/.
[2] Enterprise tracing project. http://www.icir.org/
enterprise-tracing/.
[3] Protected Repository for the Defense of Infrastructure
against CyberThreats. http://www.predict.org/.
[4] M. Allman, E. Blanton, and W. Eddy. A Scalable System for
Sharing Internet Measurements. In Passive and Active
Measurement Workshop, Mar. 2002.
[5] E. Blanton. tcpurify, May 2004.
http://irg.cs.ohiou.edu/~eblanton/tcpurify/.
[6] E. Blanton. Personal communication, Apr. 2007.
[7] S. Coull, C. Wright, F. Monrose, M. Collins, and M. Reiter.
Playing Devil’s Advocate: Inferring Sensitive Information
from Anonymized Network Traces . In Proceedings of the
Network and Distributed System Security Symposium, Feb.
2007.
[8] J. Heidemann. Personal communication, Apr. 2007.
[9] k. claffy, M. Crovella, T. Friedman, C. Shannon, and
N. Spring. Community-Oriented Network Measurement
Infrastructure (CONMI) Workshop Report. ACM Computer
Communication Review, 36(2):41–48, Apr. 2006.
[10] G. Minshall. tcpdpriv, Aug. 1997.
http://ita.ee.lbl.gov/html/contrib/tcpdpriv.html.
[11] F. Monrose and M. Reiter. Personal communication, Apr.
2007.
[12] R. Pang, M. Allman, M. Bennett, J. Lee, V. Paxson, and
B. Tierney. A First Look at Modern Enterprise Trafﬁc. In
6For instance, we would advocate that preparing and releasing a
broadly useful dataset be considered a valuable scholarly activity
when the researchers involved are evaluated.
7Researchers can indicate communication by explaining the out-
come and citing “personal communication” with the data providers.
More speculatively, program chairs and editors may wish to contact
data providers themselves with speciﬁc questions about the dataset.
ACM SIGCOMM/USENIX Internet Measurement
Conference, Oct. 2005.
[13] R. Pang, M. Allman, V. Paxson, and J. Lee. The Devil and
Packet Trace Anonymization. ACM Computer
Communication Review, 36(1), Jan. 2006.
[14] R. Pang and V. Paxson. A High-Level Programming
Environment for Packet Trace Anonymization and
Transformation. In ACM SIGCOMM, Aug. 2003.
[15] V. Paxson. Internet Trafﬁc Archive. http://ita.ee.lbl.gov/.
[16] V. Paxson. Strategies for Sound Internet Measurement. In
ACM SIGCOMM Internet Measurement Conference, Oct.
2004.
[17] C. Shannon, D. Moore, K. Keys, M. Fomenkov, B. Huffaker,
and kc claffy. The Internet Measurement Data Catalog. ACM
Computer Communication Review, 35(5), Oct. 2005.
[18] J. Xu, J. Fan, M. H. Ammar, and S. B. Moon.
Preﬁx-Preserving IP Address Anonymization:
Measurement-Based Security Evaluation and a New
Cryptography-Based Scheme. In Proc. of the 10th IEEE
International Conference on Network Protocols, 2002.