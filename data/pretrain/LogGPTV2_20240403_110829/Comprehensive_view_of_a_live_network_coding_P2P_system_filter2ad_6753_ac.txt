4.7 Server Efﬁciency
Minimizing the download time, especially of the early nodes, re-
quires optimal utilization of the server resources. Indeed, no down-
load can ﬁnish before the server serves each byte of the ﬁle at least
once, which is equivalent to eliminating duplicate transmissions
(until, of course, a full copy of the ﬁle has been served). Network
coding ensures optimal resource utilization since every block gen-
erated by the server can be used (with high probability) in place of
any other block to reconstruct the original ﬁle.
However, combining the entire ﬁle to construct a single encoded
block is impractical due to high encoding and decoding costs. In-
stead, in our implementation, we divide the blocks of the ﬁle into
groups (similar to the generations in [6]) and we perform coding
only inside the groups. Typically, each group is composed of 50-
100 blocks. By restricting the number of blocks required to pro-
duce an encoded block reduces the encoding operations, and, also,
results in sparse matrices that can be decoded efﬁciently. The dis-
advantage of our approach is that some segments may become un-
popular. This problem is similar to the optimal scheduling of the
block propagation. In the following, we argue experimentally that
segment scheduling is a much easier problem than block schedul-
ing and that the penalty of group coding is small.
To study the effect of group coding, we deﬁne server efﬁciency
as the number of useful (i.e. unique) blocks that are served by the
server in time sufﬁcient to serve one full copy of the ﬁle (under opti-
mal scheduling). We measure server efﬁciency as a ratio and notice
that the efﬁciency of network coding is 1.0; similarly, the efﬁciency
of an ideal block propagation scheme is also 1.0. We estimate the
efﬁciency ratio of group network coding and of unencoded trans-
fers using the traces we have collected in our experiments.
We concentrate on the block transfers that took place from the
beginning of the download until the server has transmitted a num-
ber of blocks equal to the number of blocks in the ﬁle (e.g. 1500
blocks in Trial-4). Observe that we also include block transfers
between nodes. We assume that the underlying topology and peer
matching remains the same as we evaluate different block schedul-
ing techniques. We measure the number of unique blocks trans-
mitted by the server. In the case of group network coding, the ﬁrst
b blocks from a particular segment are unique, where b equals the
number of blocks per group; the rest blocks transmitted in that seg-
ment are redundant. We have performed 5 runs for each of six al-
Table 3: Utilization of server resources.
Method
Group coding (50 blocks per group)
Group coding (100 blocks per group)
No coding, Local rarest, 4 neighbors
No coding, Local rarest, 8 neighbors
No coding, Random
Optimal network coding
Efﬁciency
0.9552
0.9598
0.8863
0.9445
0.7625
1.0000
gorithms for block scheduling and we report the average efﬁciency
results in Table 3.
First it is interesting to node that the efﬁciency of the system
did not suffer when restricting coding over a segment of size 50-
100 blocks. We used network coding inside a given segment, but
scheduling decisions across different segments were made at ran-
dom. In fact, the efﬁciency was very close to 1; that of an optimal
coding scheme spanning the whole ﬁle.
For the case of local rarest with no coding we have varied the size
of the neighborhood used to calculate the rarest ﬁrst block from 4 to
8 nodes. From Table 3, we observe that a random block choice pol-
icy performs very poorly; almost 24% of the tranmissions are redu-
dant. We also observe that group coding incurs a penalty of 4.5%,
compared to an optimal network coding system, however, it still
performs better than no coding. In the case of no coding, improving
the nodes’ knwoledge about blocks in other parts of the network,
improves the efﬁciency of the server. If the number of neighbors
is large enough (8 in our example), then unencoded block transfers
result in similar efﬁciency as group coding. This is due to the lim-
ited number of participants in our experiments. We expect that in
larger networks, the local rarest heuristic will be a poor estimator
of the state of the network (in the same spirit that local rarest with
4 neighbors performed worse than local rarest with 8 neighbors).
We conjecture that the number of neighbors that need to be sam-
pled to get a reliable estimate of the state of the network is not con-
stant with the size of the network. Hence, without encoding, we
may need a large number of connections to guarantee efﬁcient uti-
lization of the server resources. On the other hand, group network
coding results in good performance even if the average node degree
is small (in the case of group network coding in our experiments,
the peers could not see the state of their neighbors).
Figure 9: Evolution of the second eigenvalue λ2 for the (overlay)
topologies of Trials 1 and 4.
nections to account for the bandwidth asymmetry. Compared to
current p2p systems, our system treats upload and download con-
nections separately.
A standard metric for measuring the global connectivity proper-
ties of a graph is the second eigenvalue λ2 of the stochastic normal-
ization of the adjacency metric of the graph [7, 16]. Unlike other
connectivity metrics, such as the clustering coefﬁcient, λ2 mea-
sures global connectivity properties and has been associated with
various good properties of the graph (such as conductance and scal-
ing [13], efﬁcient sampling [14], and others). Topologies with good
connectivity have second eigenvalue bounded away from 1.
In Figure 9, we plot the changes in λ2 for Trials 1&4. Note that
values above 0.85 indicate graphs with poor connectivity charac-
teristics.
From this ﬁgure we can see that Trial 1 had consistently bad con-
nectivity properties due to the peer matching algorithm used. We
observe that this algorithm creates clusters of nodes, with few inter-
cluster connections, and, hence, graphs with poor connectivity. A
closer examination of the data also revealed bad fairness properties
since fast links were monopolized by a few peers in the system.
In Trials 3&4, the topology construction algorithm periodically
dropped one of its peers at random. We experimented with various
values of the frequency of dropping connections and with various
policies for choosing the connections to be dropped. We will now
show the results for the case where we drop one peer after ﬁve
blocks are uploaded (assuming that the peer-set was at its max-
imum). We also tested preferentially dropping slow peers, and
got similar results. Our motivation is to force rewiring of connec-
tions and, hence, construct overlay topologies with better proper-
ties [14, 15, 25, 31]. However, even doing so, we still observed
cluster formation. This can be seen in table 4 where we show the
breakdown of the connection statistics for Trial 3 and Trial 4. Ob-
serve, that during Trial 3 there are a large number of connections
established that do not result in any block transfer since they are
immediately terminated with a ’too busy’ message. The reason be-
ing that peers that were just dropped in one part of the network try
to contact other peers in other parts of the network, however, they
get continuously rejected since other nodes have their connection
budget used and are not accepting more connections.
Figure 8: Amount of time spent at each stage of the download.
4.8 Download Progress
Anecdotal evidence suggests that downloaders in current peer-to-
peer systems perceive slow performance in the beginning of the
download, because they do not have anything to offer (or whatever
they have is already popular in the system),2 and toward the end of
the download, because they cannot ﬁnd the last missing blocks. In
fact, [30] shows that if one plots the number of users downloading
a given portion of the ﬁle (e.g.
the ﬁrst 5%, the next 5%, etc),
it follows a U-shape, with users spending a large amount of time
to obtain the ﬁrst and last portions of the ﬁle. This problem is
more acute when the number of seeds is small, or when the size of
the cloud is very large. Network coding can be used to solve this
problem.
In Fig.8 we plot the average time spent obtaining each 1% of
ﬁle for all users in Trial-4. For example, the 50th column is the
elapsed time it took to go from 49% of the ﬁle downloaded to 50%
of the ﬁle. The height of the column shows how much of the overall
download time was spent getting that each one percent. Observe the
absence of a U-shape in the graph by using Network Coding. The
reason is that each encoded block is unique and useful to any node.
Thus, newly arriving nodes can easily obtain useful blocks that in
turn they can exchange with other nodes, and nodes at the end of
the download do not need to wait long periods before ﬁnding their
missing blocks.
5. TOPOLOGY CONSTRUCTION
Topology construction and maintenance are critical components of
any mesh network. Topologies with poor connectivity and connec-
tivity algorithms that bias the connections result in low network
efﬁciency and may raise some fairness issues (if only some nodes
monopolize the resources of the fast peers and/or the server).
During our four trials, we have experimented with a variety of
topology construction algorithms. The ﬁrst two trials used a sim-
ple random construction algorithm, where each new node picks a
random subset of nodes out of those in the system (registered at the
tracker) and connect to them. Upload and download connections
were treated separately, and the set of uploading nodes could be
different than the set of downloading nodes. Each node attempts to
open new connections until their number reaches a certain target (6
for upload, and 8 for download). The maximum number of allowed
download connections is higher than the maximum for upload con-
2Recall that many P2P systems implement tit-for-tat algorithms to
discourage free-riders.
Total Connect Attempts
Failed (e.g. NATs)
Established
Downloaded Block
Too Busy (cluster)
Trial 3
88,179
72,371
32,033
12,687
14,905
Trial 4
42,218
35,233
14,960
12,131
0
Table 4: Connection Statistics
To avoid this problem, in the Trial 4 we allowed a given peer’s
peer-set to grow temporarily by some threshold (e.g. max-peer-set
+ 2), creating an “elastic” maximum. This enabled dropped peers to
get immediately accepted by peers in other pars of the network. The
accepting peer would then be in a situation where he would have
more peers than the maximum allowed max-peer-set, and would try
to come back to the max value by randomly dropping other excess
peers.
Figure 9 shows the connectivity graph in this scenario. We can
see that compared to Trial 1, Trial 4 resulted in very well connected
graphs with low λ2, and this was achieved at the cost of slightly
higher churn. Similarly, from Table 4, we can also observe that
compared to Trial 3, Trial 4 ensures that peers can ﬁnd other ac-
cepting peers in the network much easier (note the zero ’too busy’
messages).
6. CONNECTIVITY
The wide deployment of Network Address Translation (NAT) de-
vices and ﬁrewalls reduces peer-to-peer network performance. Peers
behind NATs and ﬁrewalls, which we shall collectively call un-
reachable peers, cannot receive inbound connections. (We exclude
from our deﬁnition peers behind NATs and ﬁrewalls conﬁgured to
allow incoming connections.) Unreachable peers cannot exchange
content with each other, and, hence, cannot take advantage of the
network capacity that exists between them.3 Both their download
performance and the overall system throughput is reduced as a re-
sult.
Based on the observed peer performance and the percentage of
unreachable nodes, we calculate a) the optimal throughput of the
system assuming all nodes are reachable, and b) the optimal through-
put of the system taking into consideration the set of unreachable
peers. The optimal throughput at time t is computed as the sum
of the peak upload rate of all active peers at time t. To com-
pute the system throughput taking into consideration unreachable
nodes, we replayed the traces collected during the trials, calcu-
lating the optimal throughput given the existing connectivity con-
straints. To this extend, for each time t we ﬁrst saturate the upload
(or, download capacities) of the plausible connections between un-
reachable nodes and reachable nodes. Then, we saturate the re-
maining upload/download capacities of reachable nodes by match-
ing them with each other. This matching is optimal. Our compu-
tation of the optimal throughput does not assume an upper limit on
the number of connections per node, which can overestimate the
computed optimal throughput.
In Fig. 10 we plot the optimal throughput with full node con-
nectivity and with the actual connectivity seen during two different
trials. During the ﬁrst trial considered (Fig. 10(left)), the average
number of unreachable peers was quite high, more than 75%; the
second trial had less than 60% unreachable peers, possible due to
our efforts to educate the users of the performance beneﬁts of con-
ﬁguring their NAT boxes and ﬁrewalls.
Observe the large discrepancy between the maximum system
throughput with and without considering the unreachable peers in
the ﬁrst trial around time 30hr. After examining the connectiv-
ity pattern of users, we realized that at this speciﬁc time, the sys-
tem reached high percentages of unreachable peers (more than 85-
90%).
In Fig. 10(right), we present the results for Trial 4 (with less than
60% unreachable nodes). We observe that the throughput under
partial connectivity is fairly close to that achieved with full con-
nectivity, which implies that the system performs surprisingly well
even with a large number of unreachable peers. We attribute the
resilience of the network to two factors: a) the aggregate upload
capacity of the high-capacitated, globally-reachable nodes can be
saturated by the aggregate download capacity of the unreachable
nodes, and b) the aggregate upload capacity of the unreachable
nodes can be saturated by the very high download capacity of the
few well-connected and fast nodes.4 We have validated both as-
sumptions analytically and experimentally, but, due to space con-
straints, we omit the details.