 10000
 1000
 100
 10
 1
               1.0e+06                                               1.5e+06                                               2.0e+06
time (cycles)
Fig. 7. The histogram of wbinvd execution time over 1,000,000 measure-
ments.
INSTRUCTION LATENCIES (IN CYCLES) CAUSED BY DISABLING CACHING.
TABLE IV
Instructions
nop
load
store
load; lfence
Caching enabled
1.00
1.01
1.01
14.82
Caching disabled
901
1266
978
2265
Slowdown
901×
1253×
968×
153×
invalidation using the wbinvd instruction is difﬁcult. Instead,
we measure the execution time of wbinvd to approximate
the latency of cache invalidation. This is reasonable because
wbinvd is a serialized instruction. Speciﬁcally, we conducted
the following experiments: We run wbinvd in a tight loop for
1,000,000 times and measure the execution time of each loop,
which is shown in Fig. 7. We observe that in some cases the
latency is as high as 2 × 106 cycles, which typically happens
early in the experiments, while most of the times the latency
is only 1 × 106 cycles. We believe this is because dirty cache
lines need to be written back to the memory in the ﬁrst few
tests, but later tests usually encounter already-empty caches.
Effects of disabling caching. The attacker can disable caching
on a logical core by setting the CD bit of control registers.
According to Intel Software Developer’s Manual [26, Chapter
8.7.13.1], “the CD ﬂags for the two logical processors are
ORed together, such that when any logical processor sets its
CD ﬂag, the entire cache is nominally disabled.” This allows
the adversary to force an enclave thread to enter the no-ﬁll
caching mode. According to Intel’s manual [26, Sec. 11.5.3
and Table 11-5], after setting the CD bit, the caches need to
be ﬂushed with wbinvd instruction to insure system memory
coherency. Otherwise, cache hits on reads will still occur and
data will be read from valid cache lines. The adversary can also
disable caching of the entire PRM by setting the PRMRR [32,
Chapter 6.11.1], as “all enclave accesses to the PRMRR region
always use the memory type speciﬁed by the PRMRR, unless
the CR0.CD bit on one of the logical processors on the
core running the enclave is set.” It is worth noting that the
PRMRR_BASE and PRMRR_MASK MSRs are set in an early
booting stage, and cannot be updated after the system boots.
We measured the latency of the nop, load, store instruc-
tions, and the load;lfence instruction sequence, respec-
tively, in tight loops (averaged over 10,000,000 measurements)
with the caching enabled and disabled. The results are shown
in Table IV. The slowdowns were calculated by comparing
the latency with caching disabled and enabled. It can be seen
that the slowdowns of nop, load, and store instructions
are around 1000×. But
the slowdown of load;lfence
instruction sequence is only two orders of magnitude. This
result leads to the non-linear distortion of T1 when caching
are disabled (see Fig. 2), which is also shown in Table III:
store,v and T 1
T 0
store,v are on the same order of magnitude
when caching is enabled but become drastically different when
caching is disabled (i.e., 1.32e+5 vs. 1.34e+4).
Discussion. A prerequisite of observing data races in the
memory is that the load operations miss L1/L2/LLC caches.
This may be achieved using one of the following mechanisms:
• Evicting the shared variable to memory on-the-ﬂy. The
the
adversary could leverage two approaches to evict
shared variable to memory: (1) Flushing cache content
using the wbinvd instruction. However, as the latency of
the instruction (on the order of 106 cycles) is too large
(see Fig. 7), it cannot effectively evict the shared variable
to memory. In fact, during the execution of the wbinvd
instruction, caches can still be ﬁlled normally. We have
empirically conﬁrmed that co-location tests that happen
during the execution of the wbinvd instruction are not
affected. (2) Evicting the cache content using PRIME-
PROBE techniques. However, according to our measure-
ment study, the time needed to PRIME one cache set in
LLC is at least 40 × wLLC cycles (wLLC is the number
of cache lines in one LLC slides), which is signiﬁcantly
larger than the interval between the pre-load instructions
and the actual load instruction (i.e., 1 cycle). Even if
the adversary could distribute the task of cache PRIMEs
to multiple threads running on different CPU cores, which
is by itself challenging due to cache conﬂicts among these
threads, the gap of speed should be huge enough to prevent
such attacks. We will empirically verify this artifact in
Sec. V-C.
• Disabling caching. We have examined several approaches
to disable caching: First, the adversary can disable caching
by editing PRMRR, which will be effective after system
reboots. Second, the adversary can interrupt the co-location
tests before the load instructions and ﬂush the cache
content using the wbinvd instruction or PRIME-PROBE
operations (though interruption of the co-location tests will
be detected and thus restart the co-location tests). Third,
the adversary can disable the caching of the two physical
cores on which T0 and T1 executes by setting the CD bits
of the control registers. However, none of this methods
can pass the co-location tests. This is because we use
load instructions as paddings in thread T0, and use load
followed by lfence instructions as paddings in thread T1.
If caching is disabled, the slowdown of “load; lfence”
is much smaller than the other instructions, since the former
188
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:38:29 UTC from IEEE Xplore.  Restrictions apply. 
store,v = c0 · 1.32 × 105, R0
already serializes the load operations (see Table IV). As
a result, the relative speed of the two threads changes
v/R1
signiﬁcantly (see Table III). Particularly, as R0
v is no
longer close to 1, the co-location tests will not pass.
• Altering CPU frequency when caching is disabled. We
further consider the cases of changing CPU frequency
after disabling caching by setting the CD bits. Suppose
the frequency change slows down thread T0 and T1 by
a factor of c0 and c1, respectively, which are constant.
v = c0 · 1.35 × 105,
Then T 0
v = c1 · 2.57× 104, according
store,v = c1 · 1.34× 104, R1
T 1
to Table III. Then, based upon Equa. (1), the data race
probabilities of T0 and T1 are ˆp0 = min( c0·1.32×105
c1·2.57×104 , 1)
c0·1.35×105 , 1) respectively. Since ˆp0 · ˆp1 ≤
and ˆp1 = min( c1·1.34×104
c0·1.35×105 ≈ 0.51, we can see that
c1·2.57×104 · c1·1.34×104
c0·1.32×105
the
√
probability for a thread to observe the data race will not
0.51 ≈ 71.4%, which has a near zero probability
exceed
to pass our co-location test.
• Nonlinear CPU frequency changes. The only remaining
possibility for the adversary to fool the co-location test is
to change the CPU frequency nonlinearly so that T 0
store,v,
load,v, T 1
T 0
load,v change independently. However,
the CPU frequency transition latency we could achieve on
our testbed is between 20μs and 70μs (measured using the
method proposed by Mazouz et al. [33]), which is on the
same order of magnitude as R1
v when caching is disabled
(and thus much larger than R1
v when caching is enabled),
making it very difﬁcult, if not impossible, to introduce
desired nonlinear frequency change during the co-location
tests.
In summary, when the data races take place in the memory
through any of the methods we discussed above, the attacker
cannot achieve high probability of observing data races in both
T0 and T1. The hypothesis tests will fail in all cases.
C. Empirical Security Evaluation
store,v, T 1
We empirically evaluated the accuracy of the co-location
tests. As the primary goal of the co-location test is to raise
alarms when the two threads are not co-located, we deﬁne
a false positive as a false alarm (i.e., the co-location test
fails) when the two threads are indeed scheduled on the same
physical core, and a false negative as a missed detection (i.e.,
the co-location test passes) of the threads’ separation.
False positive rates. A False positive of the co-location tests
is approximately the combined type I error of two hypothesis
tests (from T0 and T1, respectively). We run the same code
shown in Fig. 2 on four different processors (i.e., i7-6700,
E3-1280 v5, i7-7700HQ, and i5-6200U) without modiﬁcation.
The empirical probabilities of passing unit tests by T0 and
T1 on these processors are listed in Table V. These values
are estimated by conducting 25, 600, 000 unit tests. Then with
parameter n = 256 and the corresponding values of p0 and
p1, we run co-location tests with α = 0.01, α = 0.001,
α = 0.0001, respectively. The false positive rates are reported
in Table V. Although the empirical values are close to the
TABLE V
EVALUATION OF FALSE POSITIVE RATES.
CPU
i7-6700
E3-1280 V5
i7-7700HQ
i5-6200U
p0
0.969
0.963
0.965
0.968
p1
0.968
0.948
0.950
0.967
false positive rates (α =)
1e−4
0.01
4e−5
0.005
5e−5
0.004
2e−4
0.005
3e−4
0.006
0.001
5e−4
4e−4
5e−4
0.001
theoretical values of α, there are cases where the empirical
values are 3× the theoretical ones (i.e., on i5-6200U with
α = 0.0001). This is probably because of the lack of true ran-
domness and independence in our statistical tests (explained
in Sec. IV-B). However, these values are on the same order of
magnitude. We believe it is reasonable to select a desired α
value to approximate false positives in practice.
False negative rates. A false negative of the co-location test
is approximately the type II error of the hypothesis test. We
particularly evaluated the following four scenarios:
1. The adversary simply places the two threads on two
physical cores without interfering with their execution.
2. The adversary simply places the two threads on two
physical cores, and further reduces the frequency of the
two physical cores to 800 Mhz.
3. The adversary simply places the two threads on two
physical cores, and further disabling caching on the cores
on which the two threads run, by setting the CD ﬂag.
4. The adversary simply places the two threads on two
physical cores, and creates 6 threads that concurrently
PRIME the same LLC cache set to which the shared variable
V is mapped.
We run 100, 000 co-location tests for every scenarios. The
tests were conducted on the i7-6700 processor, with parameter
n = 256, p0 = 0.969, p1 = 0.968, α = 0.0001. Results
are shown in Table VI. Column 2 and 3 of the table show
ˆp0 and ˆp1, the probability of passing unit tests under the
considered scenarios, respectively. We can see that in all cases,
the probabilities of observing data races from T0 and T1 are
very low (e.g.., 0.03% to 2.2%). In all cases, the co-location
tests fail, which suggests we have successfully detected that
the two threads are not co-located. We only show results with
α = 0.0001 because larger α values (e.g., 0.01 and 0.001) will
lead to even lower false negative rates. In fact, with the data
collected in our experiments, we could not achieve any false
negatives even with a much smaller α value (e.g., 1e−100).
This result suggests it is reasonable to select a rather small
α value to reduce false positives while preserving security
guarantees. We leave the decision to the user of HYPERRACE.
VI. PROTECTING ENCLAVE PROGRAMS WITH HYPERRACE
In this section, we introduce the overall design and imple-
mentation of HYPERRACE that leverages the physical core
co-location test presented in the previous sections.
189
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:38:29 UTC from IEEE Xplore.  Restrictions apply. 
TABLE VI
EVALUATION OF FALSE NEGATIVE RATES.
Scenario
1
2
3
4
ˆp0
0.0004
0.0003
0.0153
0.0013
ˆp1
0.0007
0.0008
0.0220
0.0026
false negative rates
(α = 1e−4)
0.000
0.000
0.000
0.000
A. Safeguarding Enclave Programs
HYPERRACE is a compiler-assisted tool that compiles a pro-
gram from source code into a binary that runs inside enclaves
and protects itself from Hyper-Threading side-channel attacks
(as well as other same-core side-channel attacks).
At the high-level, HYPERRACE ﬁrst inserts instructions to
create a new thread (i.e., the shadow thread) at runtime, which
shares the same enclave with the original enclave code (dubbed
the protected thread). If the enclave program itself is already
multi-threaded, one shadow thread needs to be created for each
protected thread.
HYPERRACE then statically instruments the protected
thread to insert two types of detection subroutines at proper
program locations, so the subroutines will be triggered period-
ically and frequently at runtime. The ﬁrst type of subroutines
is designed to let the enclave program detect AEXs that take
place during its execution. The second type of subroutines
performs the aforementioned physical-core co-location tests.
The shadow thread is essentially a loop that spend most of its
time waiting to perform the co-location test.
At runtime, the co-location test is executed ﬁrst when the
protected thread and the shadow thread enter the enclave,
so as to ensure the OS indeed has scheduled the shadow
thread to occupy the same physical core. Once the test passes,
while the shadow thread runs in a busy loop, the protected
thread continues the execution and frequently checks whether
an AEX has happened. Once an AEX has been detected,
which may be caused by either a malicious preemption or
a regular timer interrupt, the protected thread will instruct
the shadow thread to conduct another co-location test and,
if passes, continue execution.
AEX detection. HYPERRACE adopts the technique introduced
by Gruss et al. [30] to detect AEX at runtime,
through
monitoring the State Save Area (SSA) of each thread in the
enclave. Speciﬁcally, each thread sets up a marker in its SSA,
for example, writing 0 to the address within SSA that is
reserved for the instruction pointer register RIP. Whenever an
AEX occurs, the current value of RIP overrides the marker,
which will be detected by inspecting the marker periodically.
When an AEX is detected, the markers will be reset to value
0. A co-location test will be performed to check co-location of
the two threads, because AEX may indicate a privilege-level
switch—an opportunity for the OS kernel to reschedule one
thread to a different logical core. By the end of the co-location
test, AEX detection will be performed again to make sure no
AEX happened during the test.
Co-location test. To check the co-location status, HYPER-
RACE conducts the physical-core co-location test described
in Sec. IV between two threads. Since the shared variable in
the test is now in the enclave memory, the adversary has no
means to inspect or modify its value. Once the co-location
status has been veriﬁed, subsequent co-location tests are only
needed when an AEX is detected.
B. Implementation of HYPERRACE
HYPERRACE is implemented by extending the LLVM
framework. Speciﬁcally, the enclave code is complied using
Clang [34], a front-end of LLVM [35] that translates C code
into LLVM intermediate representation (IR). We developed
an LLVM IR optimization pass that inserts the AEX detection
code (including a conditional jump to the co-location test rou-
tine if an AEX is detected) into every basic block. Further, we
insert one additional AEX detection code every q instructions
within a basic block, where q is a parameter we could tune.
Checking AEX in every basic block guarantees that secret-
dependent control ﬂows are not leaked due to side-channel
attacks; adding additional checks prevents data-ﬂow leakage.
We will evaluate the effects of tuning q in Sec. VII.
The shadow thread is created outside the enclave and system
calls are made to set the CPU afﬁnity of the protected thread
and the shadow thread prior to entering the enclave. We use
spin locks to synchronize the co-location test routines for
the protected thread and the shadow thread. Speciﬁcally, the
shadow thread waits at the spin lock until the protected thread