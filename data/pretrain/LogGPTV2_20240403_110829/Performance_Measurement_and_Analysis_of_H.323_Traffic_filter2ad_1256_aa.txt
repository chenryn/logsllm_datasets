title:Performance Measurement and Analysis of H.323 Traffic
author:Prasad Calyam and
Mukundan Sridharan and
Weiping Mandrawa and
Paul Schopis
Performance Measurement and Analysis of
H.323 Traﬃc(cid:1)
Prasad Calyam1, Mukundan Sridharan2, Weiping Mandrawa1, and
Paul Schopis1
1 OARnet, 1224 Kinnear Road,Columbus, Ohio 43212.
{pcalyam,wmandraw,pschopis}@oar.net
2 Department of Computer and Information Science,
The Ohio State University, Columbus, OH 43210.
PI:EMAIL
Abstract. The popularity of H.323 applications has been demonstrated
by the billions of minutes of audio and video traﬃc seen on the Internet
every month. Our objective in this paper is to obtain Good, Acceptable
and Poor performance bounds for network metrics such as delay, jitter
and loss for H.323 applications based on objective and subjective quality
assessment of various audio and video streams. To obtain the necessary
data for our analysis we utilize the H.323 Beacon tool we have developed
and a set of Videoconferencing tasks performed in a LAN and also with
end-points located across multiple continents, connected via disparate
network paths on the Internet.
1 Introduction
H.323 [1] is an umbrella standard that deﬁnes how real-time multimedia com-
munications, such as audio and video-conferencing, can be exchanged on packet-
switched networks (Internet). With the rapid increase in the number of individu-
als in industry and academia using H.323 audio and video-conferencing systems
extensively, the expectation levels for better audio and video performance have
risen signiﬁcantly. This has led to the need to understand the behavior of audio
and video traﬃc as it aﬀects end user perceived quality of the H.323 applications
over the Internet. Several studies have been conducted [2,3,4] and many approa-
ches [5,6,7] have been proposed to determine the performance quality measures
of H.323 applications. Many of the previous studies used pre-recorded audio and
video streams and aimed at obtaining quality measures either based solely on
network variations or on various audiovisual quality assessment methods.
Our primary focus in this paper is to understand how the various levels of
network health, characterized by measuring delay, jitter and loss, can aﬀect end
user perception of audiovisual quality. By systematically emulating various net-
work health scenarios and using a set of Videoconferencing ’Tasks’ we determine
performance bounds for delay, jitter and loss. The obtained performance bounds
(cid:1) This work was supported in part by The Ohio Board of Regents and Internet2
C. Barakat and I. Pratt (Eds.): PAM 2004, LNCS 3015, pp. 137–146, 2004.
c(cid:1) Springer-Verlag Berlin Heidelberg 2004
138
P. Calyam et al.
are mapped to end-users perceptions of the overall audiovisual quality and are
then categorized into Grades such as ’Good’, ’Acceptable’ and ’Poor’. We show
that end-users are more sensitive to variations in jitter than variations in de-
lay or loss. The results of this paper could provide ISPs and Videoconferencing
Operators a better understanding of their end-user’s experience of audiovisual
quality for any given network health diagnostics.
To obtain the necessary data to support the various conclusions in this paper,
we utilized the H.323 Beacon tool
[8] we have developed and a set of Video-
conferencing tasks. Over 500 one-on-one subjective quality assessments from
Videoconferencing Users and the corresponding H.323 traﬃc traces were collec-
ted during our testing, which featured numerous network health scenarios in an
isolated LAN environment and on the Internet. The collected traces provided
objective quality assessments. The Internet testing involved 26 Videoconferen-
cing end-points; each performing 12 Videoconferencing tasks and located across
multiple continents, connected via disparate network paths that included re-
search networks, commodity networks, cable modem connections, DSL modem
connections and Satellite networks.
The rest of the paper is organized as follows: Section 2 provides a background
pertaining to this paper, Section 3 describes our testing methodology, Section
4 discusses our analysis of the performance bounds for delay, jitter and loss and
Section 5 concludes the paper.
2 Background
2.1 H.323 System Architecture
There are numerous factors that aﬀect the performance assessments of H.323 ap-
plications. These factors can be subdivided into 3 categories: 1. Human factors
2. Device factors 3. Network factors. First, Human factors refer to the perception
of quality of the audio and video streams and also the human error due to neg-
ligence or lack of training which results in performance bottlenecks which then
aﬀect the performance assessments. Secondly, essential devices such as H.323
terminals, Multipoint Control Units (MCUs), gatekeepers, ﬁrewalls and Net-
work Address Translators (NATs) frequently contribute towards performance
degradations in H.323 systems. Thirdly, the network dynamics caused by route
changes, competing traﬃc and congestion cause performance bottlenecks that
aﬀect performance assessments. In this paper we are interested in studying as-
pects of the Human factors, which deal with end-user perception of audiovisual
quality and the Network factors, which contribute to any network’s health. The
reader is referred to [9] for details related to Device factors.
2.2 Audiovisual Quality Assessment Metrics
There are two popular methods to assess audiovisual quality: Subjective qua-
lity assessment and Objective quality assessment. Subjective quality assessment
Performance Measurement and Analysis of H.323 Traﬃc
139
Fig. 1. Voice Quality Classes
involves playing a sample audiovisual clip to a number of participants. Their
judgment of the quality of the clip is collected and used as a quality metric.
Objective quality assessment does not rely on human judgment and involves
automated procedures such as signal-to-noise ratio (SNR) measurements of ori-
ginal and reconstructed signals and other sophisticated algorithms such as Mean
Square Error (MSE) distortion, Frequency weighted MSE, Segmented SNR, Per-
ceptual Analysis Measurement System (PAMS) [10], Perceptual Evaluation of
Speech Quality (PESQ) [11], and Emodel [5], to determine quality metrics. The
problem with subjective quality assessment techniques is that human perception
of quality is based on individual perception, which can vary signiﬁcantly bet-
ween a given set of individuals. The problem with objective quality assessment
techniques is that they may not necessarily reﬂect the actual end-user experi-
ence. There have been studies [12] that show that when objective and subjective
quality assessment are performed simultaneously, the results are comparable.
In our study, we employ both the subjective and objective quality assessment
methods to determine end-user perception of audiovisual quality for various net-
work health scenarios. To obtain subjective quality assessment scores from the
participants, we extended the slider methodology presented in [7] and develo-
ped our own slider that was integrated into our H.323 Beacon tool. Participants
ranked the audiovisual quality on a scale of 1 to 5 for various Videoconferencing
tasks using what is basically the Mean Opinion Score (MOS) ranking technique.
To obtain objective quality assessment scores we utilized the Telchemy VQMon
tool
[12] that implements the Emodel and uses traﬃc traces obtained for the
various Videoconferencing tasks as an input for its analysis. The Emodel is a well
established computational model that uses transmission parameters to predict
the subjective quality. It uses a psycho-acoustic R-scale whose values range from
0 to 100 and can be mapped to MOS rankings and User Satisfaction as shown
in Fig. 1. Though the Emodel fundamentally addresses objective quality asses-
sment of voice, our collected data shows reasonable correlation of the subjective
quality assessment scores for audiovisual quality provided by the participants
and the objective quality assessment scores provided by VQMon. The reader is
referred to [2,5,12] for more details relating to Emodel components.
140
P. Calyam et al.
2.3 Network Performance Metrics
The variables that aﬀect the MOS rankings the most in H.323 system deploym-
ents are the dynamic network changes caused by route ﬂuctuations, competing
traﬃc and congestion. The network dynamics can be characterized by 3 network
metrics viz. delay, jitter and loss as speciﬁed in [13].
Delay is deﬁned as the amount of time that a packet takes to travel from the
sender’s application to the receiver’s destination application. The components
that contribute to the end-to-end delay include: (a) compression and transmis-
sion delay at the sender (b) propagation, processing and queuing delay in the
network and (c) buﬀering and decompression delay at the receiver. The value
of one-way delay needs to be stringent for H.323 audio and video traﬃc to su-
stain good interaction between the sender and receiver ends. It is recommended
by [14] that delay bounds for the various grades of perceived performance in
terms of human interaction can be deﬁned as: Good (0ms-150ms), Acceptable
(150ms-300ms), Poor (> 300ms).
Jitter is deﬁned as the variation in the delay of the packets arriving at the
receiving end. It is caused due to congestion at various points in the network,
varying packet sizes that result in irregular processing times of packets, out of
order packet delivery, and other such factors. Excessive jitter may cause packet
discards or loss in the playback buﬀer at the receiving end. The playback buﬀer
is used to deal with the variations in delay and facilitate smooth playback of
the audio and video streams. There have been some indications in [15] about
jitter bounds, which have been veriﬁed to be approximately correct in our earlier
work [9] and are also supported by the studies conducted in this paper. However,
there have not been well deﬁned rules of thumb to suggest the accurate jitter
bounds in terms of the various grades of H.323 application performance. Our
studies suggest the following jitter values to be reasonably reliable estimates to
determine the grade of perceived performance: Good (0ms-20ms), Acceptable
(20ms-50ms), Poor (> 50ms).
Lastly, loss is deﬁned as the percentage of transmitted packets that never
reach the intended destination due to deliberately discarded packets (RED,
TTL=0) or non-deliberately by intermediate links (layer-1), nodes (layer-3) and
end-systems (discards due to late arrivals at the application). Though popu-
lar experience suggests loss levels greater than 1% can severely aﬀect audio-
visual quality, there have not been well deﬁned loss bounds in terms of the
various grades of H.323 application performance. Our studies in this paper sug-
gest the following loss values to be reasonably reliable estimates to determine
the grade of perceived performance: Good (0%-0.5%), Acceptable (0.5%-1.5%),
Poor (> 1.5%).
3 Test Methodology
3.1 Design of Experiments
Our approach in determining the performance bounds for delay, jitter and loss
in terms of Good, Acceptable and Poor grades of performance, is to view the
Performance Measurement and Analysis of H.323 Traﬃc
141
network health as an outcome of the combined interaction of delay, jitter and loss.
Our reasoning is that all three network parameters co-exist for every path in the
Internet at any given point of time; regulating any one of these parameters aﬀects
the other parameters and ultimately the Quality of Service (QoS) perceived by
the end-user in terms of H.323 application performance. A real-world example
is illustrated in [16] where, resolving a loss problem in an Intercampus DS3 led
to a decrease in the observed loss but unexpectedly led to an increase in the
overall jitter levels in the network. This shows that network health needs to be
sustained in a stable state where the network delay, jitter and loss are always
within the Good performance bounds. In our design of experiments, we employ
a full factorial design for the 3 factors, i.e. we emulate 27 scenarios that cover
every possible permutation involving the various delay, jitter and loss levels.
We performed extensive LAN tests that covered all of the 27 possibilities and
selected 9 scenarios shown in Table 1 of Section 4 for Internet measurements
whose results in essence reﬂected the results of the 27 scenarios. The reason to use
the 9 scenarios is that it is impractical and non-scalable to have each participant
judging quality 27 times in a single subjective assessment test session.
For each of the 9 Internet test scenarios a Videoconferencing task was as-
signed. A Videoconferencing task could be any activity that takes place in a
routine Videoconference. A casual conversation, an intense discussion, or a class
lecture would qualify as a Videoconferencing task. There is signiﬁcant litera-
ture recommending strategies for tasks that could be part of Subjective and
Objective assessments of audiovisual quality [6,7]. All of them recommend that
in addition to passive viewing for assessments of audiovisual quality, the par-
ticipants must be presented with realistic scenarios. Key guidelines proposed
in the above literature were followed in task creation, participant training for
scoring the audiovisual quality, task ordering and overall environment setup for
the assessment. A subset of the Videoconferencing tasks performed by the test
participants involved the audio and video loop back feature of the H.323 Beacon
tool. The loopback feature enables local playback of remote H.323 Beacon client
audio or video recorded at a remote H.323 Beacon server. The reader is referred
to [8] for more details relating to the H.323 Beacon.
3.2 Test Setup
To obtain the performance bounds for delay, jitter and loss and to aﬃrm our
conclusions, we chose a two phase approach. In the ﬁrst phase we performed
extensive testing by emulating all the 27 scenarios in a LAN environment and
obtained the performance bounds for delay, jitter and loss as stated in Section
2.3. In the second phase, we used the 9 scenarios described in Section 3.1 and
performed the Internet tests. In both the phases of testing, we conducted one-
on-one testing with participants at each of the LAN/Internet sites and collected
traﬃc traces and objective and subjective quality assessments. Fig. 2 shows
the overall test setup and Fig.3 shows the participating sites in the testing and
their last mile network connections. NISTnet [17] network emulator was used to
create the various network health scenarios by introducing delay, jitter and loss
142
P. Calyam et al.
Fig. 2. Overall Test Setup
♦
xx
♦
♦