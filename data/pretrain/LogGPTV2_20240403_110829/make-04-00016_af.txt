qreport the average times over five runs for experiments with several log 
sources—i.e., Auditlog (AD), Apache for web logs (AP), Exim for mail transfer agent logs 
(EX), Syslog for Linux system logs (SY), and Authlog for authentication logs (AT)—for asingle host in Figure 11. We used the data set from the first web server (i.e., mail.cup.com) 
Version April 1, 2022 submitted to Journal Not Specified in this evaluation. Note that we only extracted 1000k log lines from Authlog due to the 18 of 22
small original file size (less than 1.2k log lines).
| avg. time (sec) | 12 | AD | EX | SY | AD | EX | SY | AD | SY | AD | AD | Reading ||---|---|---|---|---|---|---|---|---|---|---|---|---|
| avg. time (sec) |12 |AD |EX |SY |AD |EX |SY |AD |SY |AD |AD |Extraction |
| avg. time (sec) |10 |AD |EX |SY |AD |EX |SY |AD |SY |AP |AP |Extraction |
| avg. time (sec) |10 |AD |EX |SY |AD |EX |SY |AD |SY |AP |AP |RDF Mapping |
| avg. time (sec) |8 |AD |EX |SY |AD |EX |SY |AD |SY |EX |SY |Compression |
| avg. time (sec) |8 |AD |EX |SY |AD |EX |SY |AP |SY |EX |SY |Compression || avg. time (sec) |6 |AD |EX |SY |AD |EX |SY |AP |SY |EX |SY |Compression |
| avg. time (sec) |4 |AD |EX |SY |AP |EX |SY |EX |SY |EX |SY |Compression |
| avg. time (sec) |2 |AP |EX |SY |AT |EX |SY |5,000 |SY |7,000 |7,000 |Compression |
| avg. time (sec) |0 |AP |EX |SY |AT |EX |SY |5,000 |SY |7,000 |7,000 |Compression |
| avg. time (sec) |0 |AP |EX |SY |3,000 |EX |SY |5,000 |SY |7,000 |7,000 |Compression || avg. time (sec) |1,000 |AP |EX |SY |3,000 |EX |SY |5,000 |SY |7,000 |7,000 |Compression |
number of log lines
Figure 11. Average log graph generation time for n log lines with a single host (36 extracted properties) Figure 11. Average log graph generation time for n log lines with a single host (36 extracted
properties).properties).
host in Figure 11. We used the data set from the first web server (i.e., mail.cup.com) in this 	549 evaluation. Note that we only extracted 1000k log lines from Authlog due to the small 	550
original file size (less than 1.2 k log lines). 	551
	We found that the performance for log graph extraction differs across the log sources. 	552 Constructing a log graph from Auditlog (AD) data resulted in the longest processing times 	553 followed by Apache, Exim, Syslog and Authlog. The overall log processing time scales 	554| Mach. Learn. Knowl. Extr. 2022, 4 | avg. time (sec) | 8 | AD | EX | SY | AD | EX | SY | AD | SY | EX | SY | Compression |
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
| Mach. Learn. Knowl. Extr. 2022, 4 |avg. time (sec) |6 |AD |EX |SY |AD |EX |SY |AP |SY |EX |SY |Compression |
| Mach. Learn. Knowl. Extr. 2022, 4 |avg. time (sec) |6 |AD |EX |SY |AD |EX |SY |EX |SY |EX |SY |Compression || Mach. Learn. Knowl. Extr. 2022, 4 |avg. time (sec) |4 |AD |EX |SY |AP |EX |SY |EX |SY |EX |SY |Compression |
| Mach. Learn. Knowl. Extr. 2022, 4 |avg. time (sec) |2 |AP |EX |SY |AT |EX |SY |5,000 |SY |7,000 |7,000 |389 |
| Mach. Learn. Knowl. Extr. 2022, 4 |avg. time (sec) |0 |AP |EX |SY |AT |EX |SY |5,000 |SY |7,000 |7,000 |389 |
| Mach. Learn. Knowl. Extr. 2022, 4 |avg. time (sec) |0 |AP |EX |SY |3,000 |EX |SY |5,000 |SY |7,000 |7,000 |389 || Mach. Learn. Knowl. Extr. 2022, 4 |avg. time (sec) |1,000 |AP |EX |SY |3,000 |EX |SY |5,000 |SY |7,000 |7,000 |389 |
number of log lines
| Figure 11. Average log graph generation time for n log lines with a single host (36 extracted properties) We found that the performance for log graph extraction differs across the log sources. Constructing a log graph from Auditlog (AD) data resulted in the longest processing timeshost in Figure 11. We used the data set from the first web server (i.e., mail.cup.com) in this followed by Apache, Exim, Syslog and Authlog. The overall log processing time scalesevaluation. Note that we only extracted 1000k log lines from Authlog due to the small linearly with the number of extracted log lines. Typically, the log extraction phase accounts for the largest proportion (>80%) of the overall log processing time. Furthermore, we found original file size (less than 1.2 k log lines). that the increase in log processing time with a growing number of extracted log lines is moderate, which suggests that the approach scales well to a large number of log lines. We found that the performance for log graph extraction differs across the log sources. Constructing a log graph from Auditlog (AD) data resulted in the longest processing times followed by Apache, Exim, Syslog and Authlog. The overall log processing time scales linearly with the number of extracted log lines. Typically, the log extraction phase accounts for the largest proportion (> 80%) of the overall log processing time. Furthermore, we found that the increase in log processing time with a growing number of extracted log lines is moderate, which suggests that the approach scales well to a large number of log lines.of log properties (36), followed by Apache (12), Exim (11), Authlog (11), and Syslog (6). Dynamic Log Graph Generation Figure 12 shows an evaluation of log graph generation performance with respect to the complexity of the log source. We use the Auditlog for this evaluation as it has the highest As discussed in the first part of the evaluation, execution times are mainly a function of the length of text in the log source and the granularity of the extraction patterns (i.e., number of log properties. Overall, the log graph generation performance grows linearlylog properties). As can be seen in Table 4, the log sources are heterogeneous and exhibit with the number of extracted log properties. Hence, queries that involve a smaller subset
different levels of complexity. In our setup, Auditlog, for instance, has the largest number of properties (e.g., only user or IP address rather than all information that could potentiallyof log properties (36), followed by Apache (12), Exim (11), Authlog (11), and Syslog (6). be extracted) will typically have smaller generation times. | Figure 11. Average log graph generation time for n log lines with a single host (36 extracted properties) We found that the performance for log graph extraction differs across the log sources. Constructing a log graph from Auditlog (AD) data resulted in the longest processing timeshost in Figure 11. We used the data set from the first web server (i.e., mail.cup.com) in this followed by Apache, Exim, Syslog and Authlog. The overall log processing time scalesevaluation. Note that we only extracted 1000k log lines from Authlog due to the small linearly with the number of extracted log lines. Typically, the log extraction phase accounts for the largest proportion (>80%) of the overall log processing time. Furthermore, we found original file size (less than 1.2 k log lines). that the increase in log processing time with a growing number of extracted log lines is moderate, which suggests that the approach scales well to a large number of log lines. We found that the performance for log graph extraction differs across the log sources. Constructing a log graph from Auditlog (AD) data resulted in the longest processing times followed by Apache, Exim, Syslog and Authlog. The overall log processing time scales linearly with the number of extracted log lines. Typically, the log extraction phase accounts for the largest proportion (> 80%) of the overall log processing time. Furthermore, we found that the increase in log processing time with a growing number of extracted log lines is moderate, which suggests that the approach scales well to a large number of log lines.of log properties (36), followed by Apache (12), Exim (11), Authlog (11), and Syslog (6). Dynamic Log Graph Generation Figure 12 shows an evaluation of log graph generation performance with respect to the complexity of the log source. We use the Auditlog for this evaluation as it has the highest As discussed in the first part of the evaluation, execution times are mainly a function of the length of text in the log source and the granularity of the extraction patterns (i.e., number of log properties. Overall, the log graph generation performance grows linearlylog properties). As can be seen in Table 4, the log sources are heterogeneous and exhibit with the number of extracted log properties. Hence, queries that involve a smaller subset
different levels of complexity. In our setup, Auditlog, for instance, has the largest number of properties (e.g., only user or IP address rather than all information that could potentiallyof log properties (36), followed by Apache (12), Exim (11), Authlog (11), and Syslog (6). be extracted) will typically have smaller generation times. | Figure 11. Average log graph generation time for n log lines with a single host (36 extracted properties) We found that the performance for log graph extraction differs across the log sources. Constructing a log graph from Auditlog (AD) data resulted in the longest processing timeshost in Figure 11. We used the data set from the first web server (i.e., mail.cup.com) in this followed by Apache, Exim, Syslog and Authlog. The overall log processing time scalesevaluation. Note that we only extracted 1000k log lines from Authlog due to the small linearly with the number of extracted log lines. Typically, the log extraction phase accounts for the largest proportion (>80%) of the overall log processing time. Furthermore, we found original file size (less than 1.2 k log lines). that the increase in log processing time with a growing number of extracted log lines is moderate, which suggests that the approach scales well to a large number of log lines. We found that the performance for log graph extraction differs across the log sources. Constructing a log graph from Auditlog (AD) data resulted in the longest processing times followed by Apache, Exim, Syslog and Authlog. The overall log processing time scales linearly with the number of extracted log lines. Typically, the log extraction phase accounts for the largest proportion (> 80%) of the overall log processing time. Furthermore, we found that the increase in log processing time with a growing number of extracted log lines is moderate, which suggests that the approach scales well to a large number of log lines.of log properties (36), followed by Apache (12), Exim (11), Authlog (11), and Syslog (6). Dynamic Log Graph Generation Figure 12 shows an evaluation of log graph generation performance with respect to the complexity of the log source. We use the Auditlog for this evaluation as it has the highest As discussed in the first part of the evaluation, execution times are mainly a function of the length of text in the log source and the granularity of the extraction patterns (i.e., number of log properties. Overall, the log graph generation performance grows linearlylog properties). As can be seen in Table 4, the log sources are heterogeneous and exhibit with the number of extracted log properties. Hence, queries that involve a smaller subset
different levels of complexity. In our setup, Auditlog, for instance, has the largest number of properties (e.g., only user or IP address rather than all information that could potentiallyof log properties (36), followed by Apache (12), Exim (11), Authlog (11), and Syslog (6). be extracted) will typically have smaller generation times. | Figure 11. Average log graph generation time for n log lines with a single host (36 extracted properties) We found that the performance for log graph extraction differs across the log sources. Constructing a log graph from Auditlog (AD) data resulted in the longest processing timeshost in Figure 11. We used the data set from the first web server (i.e., mail.cup.com) in this followed by Apache, Exim, Syslog and Authlog. The overall log processing time scalesevaluation. Note that we only extracted 1000k log lines from Authlog due to the small linearly with the number of extracted log lines. Typically, the log extraction phase accounts for the largest proportion (>80%) of the overall log processing time. Furthermore, we found original file size (less than 1.2 k log lines). that the increase in log processing time with a growing number of extracted log lines is moderate, which suggests that the approach scales well to a large number of log lines. We found that the performance for log graph extraction differs across the log sources. Constructing a log graph from Auditlog (AD) data resulted in the longest processing times followed by Apache, Exim, Syslog and Authlog. The overall log processing time scales linearly with the number of extracted log lines. Typically, the log extraction phase accounts for the largest proportion (> 80%) of the overall log processing time. Furthermore, we found that the increase in log processing time with a growing number of extracted log lines is moderate, which suggests that the approach scales well to a large number of log lines.of log properties (36), followed by Apache (12), Exim (11), Authlog (11), and Syslog (6). Dynamic Log Graph Generation Figure 12 shows an evaluation of log graph generation performance with respect to the complexity of the log source. We use the Auditlog for this evaluation as it has the highest As discussed in the first part of the evaluation, execution times are mainly a function of the length of text in the log source and the granularity of the extraction patterns (i.e., number of log properties. Overall, the log graph generation performance grows linearlylog properties). As can be seen in Table 4, the log sources are heterogeneous and exhibit with the number of extracted log properties. Hence, queries that involve a smaller subset
different levels of complexity. In our setup, Auditlog, for instance, has the largest number of properties (e.g., only user or IP address rather than all information that could potentiallyof log properties (36), followed by Apache (12), Exim (11), Authlog (11), and Syslog (6). be extracted) will typically have smaller generation times. | Figure 11. Average log graph generation time for n log lines with a single host (36 extracted properties) We found that the performance for log graph extraction differs across the log sources. Constructing a log graph from Auditlog (AD) data resulted in the longest processing timeshost in Figure 11. We used the data set from the first web server (i.e., mail.cup.com) in this followed by Apache, Exim, Syslog and Authlog. The overall log processing time scalesevaluation. Note that we only extracted 1000k log lines from Authlog due to the small linearly with the number of extracted log lines. Typically, the log extraction phase accounts for the largest proportion (>80%) of the overall log processing time. Furthermore, we found original file size (less than 1.2 k log lines). that the increase in log processing time with a growing number of extracted log lines is moderate, which suggests that the approach scales well to a large number of log lines. We found that the performance for log graph extraction differs across the log sources. Constructing a log graph from Auditlog (AD) data resulted in the longest processing times followed by Apache, Exim, Syslog and Authlog. The overall log processing time scales linearly with the number of extracted log lines. Typically, the log extraction phase accounts for the largest proportion (> 80%) of the overall log processing time. Furthermore, we found that the increase in log processing time with a growing number of extracted log lines is moderate, which suggests that the approach scales well to a large number of log lines.of log properties (36), followed by Apache (12), Exim (11), Authlog (11), and Syslog (6). Dynamic Log Graph Generation Figure 12 shows an evaluation of log graph generation performance with respect to the complexity of the log source. We use the Auditlog for this evaluation as it has the highest As discussed in the first part of the evaluation, execution times are mainly a function of the length of text in the log source and the granularity of the extraction patterns (i.e., number of log properties. Overall, the log graph generation performance grows linearlylog properties). As can be seen in Table 4, the log sources are heterogeneous and exhibit with the number of extracted log properties. Hence, queries that involve a smaller subset
different levels of complexity. In our setup, Auditlog, for instance, has the largest number of properties (e.g., only user or IP address rather than all information that could potentiallyof log properties (36), followed by Apache (12), Exim (11), Authlog (11), and Syslog (6). be extracted) will typically have smaller generation times. | Figure 11. Average log graph generation time for n log lines with a single host (36 extracted properties) We found that the performance for log graph extraction differs across the log sources. Constructing a log graph from Auditlog (AD) data resulted in the longest processing timeshost in Figure 11. We used the data set from the first web server (i.e., mail.cup.com) in this followed by Apache, Exim, Syslog and Authlog. The overall log processing time scalesevaluation. Note that we only extracted 1000k log lines from Authlog due to the small linearly with the number of extracted log lines. Typically, the log extraction phase accounts for the largest proportion (>80%) of the overall log processing time. Furthermore, we found original file size (less than 1.2 k log lines). that the increase in log processing time with a growing number of extracted log lines is moderate, which suggests that the approach scales well to a large number of log lines. We found that the performance for log graph extraction differs across the log sources. Constructing a log graph from Auditlog (AD) data resulted in the longest processing times followed by Apache, Exim, Syslog and Authlog. The overall log processing time scales linearly with the number of extracted log lines. Typically, the log extraction phase accounts for the largest proportion (> 80%) of the overall log processing time. Furthermore, we found that the increase in log processing time with a growing number of extracted log lines is moderate, which suggests that the approach scales well to a large number of log lines.of log properties (36), followed by Apache (12), Exim (11), Authlog (11), and Syslog (6). Dynamic Log Graph Generation Figure 12 shows an evaluation of log graph generation performance with respect to the complexity of the log source. We use the Auditlog for this evaluation as it has the highest As discussed in the first part of the evaluation, execution times are mainly a function of the length of text in the log source and the granularity of the extraction patterns (i.e., number of log properties. Overall, the log graph generation performance grows linearlylog properties). As can be seen in Table 4, the log sources are heterogeneous and exhibit with the number of extracted log properties. Hence, queries that involve a smaller subset
different levels of complexity. In our setup, Auditlog, for instance, has the largest number of properties (e.g., only user or IP address rather than all information that could potentiallyof log properties (36), followed by Apache (12), Exim (11), Authlog (11), and Syslog (6). be extracted) will typically have smaller generation times. | Figure 11. Average log graph generation time for n log lines with a single host (36 extracted properties) We found that the performance for log graph extraction differs across the log sources. Constructing a log graph from Auditlog (AD) data resulted in the longest processing timeshost in Figure 11. We used the data set from the first web server (i.e., mail.cup.com) in this followed by Apache, Exim, Syslog and Authlog. The overall log processing time scalesevaluation. Note that we only extracted 1000k log lines from Authlog due to the small linearly with the number of extracted log lines. Typically, the log extraction phase accounts for the largest proportion (>80%) of the overall log processing time. Furthermore, we found original file size (less than 1.2 k log lines). that the increase in log processing time with a growing number of extracted log lines is moderate, which suggests that the approach scales well to a large number of log lines. We found that the performance for log graph extraction differs across the log sources. Constructing a log graph from Auditlog (AD) data resulted in the longest processing times followed by Apache, Exim, Syslog and Authlog. The overall log processing time scales linearly with the number of extracted log lines. Typically, the log extraction phase accounts for the largest proportion (> 80%) of the overall log processing time. Furthermore, we found that the increase in log processing time with a growing number of extracted log lines is moderate, which suggests that the approach scales well to a large number of log lines.of log properties (36), followed by Apache (12), Exim (11), Authlog (11), and Syslog (6). Dynamic Log Graph Generation Figure 12 shows an evaluation of log graph generation performance with respect to the complexity of the log source. We use the Auditlog for this evaluation as it has the highest As discussed in the first part of the evaluation, execution times are mainly a function of the length of text in the log source and the granularity of the extraction patterns (i.e., number of log properties. Overall, the log graph generation performance grows linearlylog properties). As can be seen in Table 4, the log sources are heterogeneous and exhibit with the number of extracted log properties. Hence, queries that involve a smaller subset
different levels of complexity. In our setup, Auditlog, for instance, has the largest number of properties (e.g., only user or IP address rather than all information that could potentiallyof log properties (36), followed by Apache (12), Exim (11), Authlog (11), and Syslog (6). be extracted) will typically have smaller generation times. | Figure 11. Average log graph generation time for n log lines with a single host (36 extracted properties) We found that the performance for log graph extraction differs across the log sources. Constructing a log graph from Auditlog (AD) data resulted in the longest processing timeshost in Figure 11. We used the data set from the first web server (i.e., mail.cup.com) in this followed by Apache, Exim, Syslog and Authlog. The overall log processing time scalesevaluation. Note that we only extracted 1000k log lines from Authlog due to the small linearly with the number of extracted log lines. Typically, the log extraction phase accounts for the largest proportion (>80%) of the overall log processing time. Furthermore, we found original file size (less than 1.2 k log lines). that the increase in log processing time with a growing number of extracted log lines is moderate, which suggests that the approach scales well to a large number of log lines. We found that the performance for log graph extraction differs across the log sources. Constructing a log graph from Auditlog (AD) data resulted in the longest processing times followed by Apache, Exim, Syslog and Authlog. The overall log processing time scales linearly with the number of extracted log lines. Typically, the log extraction phase accounts for the largest proportion (> 80%) of the overall log processing time. Furthermore, we found that the increase in log processing time with a growing number of extracted log lines is moderate, which suggests that the approach scales well to a large number of log lines.of log properties (36), followed by Apache (12), Exim (11), Authlog (11), and Syslog (6). Dynamic Log Graph Generation Figure 12 shows an evaluation of log graph generation performance with respect to the complexity of the log source. We use the Auditlog for this evaluation as it has the highest As discussed in the first part of the evaluation, execution times are mainly a function of the length of text in the log source and the granularity of the extraction patterns (i.e., number of log properties. Overall, the log graph generation performance grows linearlylog properties). As can be seen in Table 4, the log sources are heterogeneous and exhibit with the number of extracted log properties. Hence, queries that involve a smaller subset
different levels of complexity. In our setup, Auditlog, for instance, has the largest number of properties (e.g., only user or IP address rather than all information that could potentiallyof log properties (36), followed by Apache (12), Exim (11), Authlog (11), and Syslog (6). be extracted) will typically have smaller generation times. | Figure 11. Average log graph generation time for n log lines with a single host (36 extracted properties) We found that the performance for log graph extraction differs across the log sources. Constructing a log graph from Auditlog (AD) data resulted in the longest processing timeshost in Figure 11. We used the data set from the first web server (i.e., mail.cup.com) in this followed by Apache, Exim, Syslog and Authlog. The overall log processing time scalesevaluation. Note that we only extracted 1000k log lines from Authlog due to the small linearly with the number of extracted log lines. Typically, the log extraction phase accounts for the largest proportion (>80%) of the overall log processing time. Furthermore, we found original file size (less than 1.2 k log lines). that the increase in log processing time with a growing number of extracted log lines is moderate, which suggests that the approach scales well to a large number of log lines. We found that the performance for log graph extraction differs across the log sources. Constructing a log graph from Auditlog (AD) data resulted in the longest processing times followed by Apache, Exim, Syslog and Authlog. The overall log processing time scales linearly with the number of extracted log lines. Typically, the log extraction phase accounts for the largest proportion (> 80%) of the overall log processing time. Furthermore, we found that the increase in log processing time with a growing number of extracted log lines is moderate, which suggests that the approach scales well to a large number of log lines.of log properties (36), followed by Apache (12), Exim (11), Authlog (11), and Syslog (6). Dynamic Log Graph Generation Figure 12 shows an evaluation of log graph generation performance with respect to the complexity of the log source. We use the Auditlog for this evaluation as it has the highest As discussed in the first part of the evaluation, execution times are mainly a function of the length of text in the log source and the granularity of the extraction patterns (i.e., number of log properties. Overall, the log graph generation performance grows linearlylog properties). As can be seen in Table 4, the log sources are heterogeneous and exhibit with the number of extracted log properties. Hence, queries that involve a smaller subset
different levels of complexity. In our setup, Auditlog, for instance, has the largest number of properties (e.g., only user or IP address rather than all information that could potentiallyof log properties (36), followed by Apache (12), Exim (11), Authlog (11), and Syslog (6). be extracted) will typically have smaller generation times. | 549 550 551 552 553 554 555 556 557 558
559
560 561 562 563 564
573 574 575
565 576 566 577 567 578 568 579 569 580 570 581 582 
571
572 |
|---|---|---|---|---|---|---|---|---|---|| 12 |12 |12 |12 |12 |12 |12 |1k lines  3k lines |1k lines  3k lines |549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 573 574 575 565 576 566 577 567 578 568 579 569 580 570 581 582  571 572 || 	10  avg. time (sec)  	8 6 4 Version April 1, 2022 submitted to Journal Not Specified 2 |	10  avg. time (sec)  	8 6 4 Version April 1, 2022 submitted to Journal Not Specified 2 |	10  avg. time (sec)  	8 6 4 Version April 1, 2022 submitted to Journal Not Specified 2 |	10  avg. time (sec)  	8 6 4 Version April 1, 2022 submitted to Journal Not Specified 2 |	10  avg. time (sec)  	8 6 4 Version April 1, 2022 submitted to Journal Not Specified 2 |	10  avg. time (sec)  	8 6 4 Version April 1, 2022 submitted to Journal Not Specified 2 |	10  avg. time (sec)  	8 6 4 Version April 1, 2022 submitted to Journal Not Specified 2 |5k lines  7k lines 19 of 22 |5k lines  7k lines 19 of 22 |549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 573 574 575 565 576 566 577 567 578 568 579 569 580 570 581 582  571 572 || 0 |0 |0 |0 |0 |0 |0 |0 |0 |549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 573 574 575 565 576 566 577 567 578 568 579 569 580 570 581 582  571 572 |
| 3 |3 |3 |6 |12 |18 |24 |30 |36 |549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 573 574 575 565 576 566 577 567 578 568 579 569 580 570 581 582  571 572 || 	We performed full property extraction (i.e., all 36 identified properties) against 5k, 10k, 15k, and 20k log-lines, respectively, and compare the original size of raw log data, the Figure 12. Dynamic log graph generation time. Experiments carried out on AuditLog data on a single host. generated RDF graph in TURTLE39format (.ttl), and the compressed graph output in HDT Figure 12. Dynamic log graph generation time38 format. Figure 12 shows an evaluation of log graph generation performance with respect to the complexity of the log source. We use the Auditlog for this evaluation as it has the highest Graph Compression to 5.4 MB for the uncompressed RDF graph. 20k log lines (4 MB raw log) compresses to about 1.87 MB from 21.4 MB uncompressed generated RDF graph. Overall, the compressed Figure 13 shows the performance for log graph compression on the Auditlog dataset. version is typically less than half the size of the original raw log and 10x smaller than We performed full property extraction (i.e., all 36 identified properties) against 5k, 10k, 15k, and 20k log-lines, respectively, and compare the original size of raw log data, the generated RDF graph. The resulting graph output would be even smaller for fewer the generated RDF graph in TURTLE [19] format (.ttl), and the compressed graph output in extracted properties, minimizing resource requirements (i.e. storage/disk space). Graph Compression the HDT format. 25 Figure 13 shows the performance for log graph compression on the Auditlog dataset. Original raw log data |	We performed full property extraction (i.e., all 36 identified properties) against 5k, 10k, 15k, and 20k log-lines, respectively, and compare the original size of raw log data, the Figure 12. Dynamic log graph generation time. Experiments carried out on AuditLog data on a single host. generated RDF graph in TURTLE39format (.ttl), and the compressed graph output in HDT Figure 12. Dynamic log graph generation time38 format. Figure 12 shows an evaluation of log graph generation performance with respect to the complexity of the log source. We use the Auditlog for this evaluation as it has the highest Graph Compression to 5.4 MB for the uncompressed RDF graph. 20k log lines (4 MB raw log) compresses to about 1.87 MB from 21.4 MB uncompressed generated RDF graph. Overall, the compressed Figure 13 shows the performance for log graph compression on the Auditlog dataset. version is typically less than half the size of the original raw log and 10x smaller than We performed full property extraction (i.e., all 36 identified properties) against 5k, 10k, 15k, and 20k log-lines, respectively, and compare the original size of raw log data, the generated RDF graph. The resulting graph output would be even smaller for fewer the generated RDF graph in TURTLE [19] format (.ttl), and the compressed graph output in extracted properties, minimizing resource requirements (i.e. storage/disk space). Graph Compression the HDT format. 25 Figure 13 shows the performance for log graph compression on the Auditlog dataset. Original raw log data |	We performed full property extraction (i.e., all 36 identified properties) against 5k, 10k, 15k, and 20k log-lines, respectively, and compare the original size of raw log data, the Figure 12. Dynamic log graph generation time. Experiments carried out on AuditLog data on a single host. generated RDF graph in TURTLE39format (.ttl), and the compressed graph output in HDT Figure 12. Dynamic log graph generation time38 format. Figure 12 shows an evaluation of log graph generation performance with respect to the complexity of the log source. We use the Auditlog for this evaluation as it has the highest Graph Compression to 5.4 MB for the uncompressed RDF graph. 20k log lines (4 MB raw log) compresses to about 1.87 MB from 21.4 MB uncompressed generated RDF graph. Overall, the compressed Figure 13 shows the performance for log graph compression on the Auditlog dataset. version is typically less than half the size of the original raw log and 10x smaller than We performed full property extraction (i.e., all 36 identified properties) against 5k, 10k, 15k, and 20k log-lines, respectively, and compare the original size of raw log data, the generated RDF graph. The resulting graph output would be even smaller for fewer the generated RDF graph in TURTLE [19] format (.ttl), and the compressed graph output in extracted properties, minimizing resource requirements (i.e. storage/disk space). Graph Compression the HDT format. 25 Figure 13 shows the performance for log graph compression on the Auditlog dataset. Original raw log data |	We performed full property extraction (i.e., all 36 identified properties) against 5k, 10k, 15k, and 20k log-lines, respectively, and compare the original size of raw log data, the Figure 12. Dynamic log graph generation time. Experiments carried out on AuditLog data on a single host. generated RDF graph in TURTLE39format (.ttl), and the compressed graph output in HDT Figure 12. Dynamic log graph generation time38 format. Figure 12 shows an evaluation of log graph generation performance with respect to the complexity of the log source. We use the Auditlog for this evaluation as it has the highest Graph Compression to 5.4 MB for the uncompressed RDF graph. 20k log lines (4 MB raw log) compresses to about 1.87 MB from 21.4 MB uncompressed generated RDF graph. Overall, the compressed Figure 13 shows the performance for log graph compression on the Auditlog dataset. version is typically less than half the size of the original raw log and 10x smaller than We performed full property extraction (i.e., all 36 identified properties) against 5k, 10k, 15k, and 20k log-lines, respectively, and compare the original size of raw log data, the generated RDF graph. The resulting graph output would be even smaller for fewer the generated RDF graph in TURTLE [19] format (.ttl), and the compressed graph output in extracted properties, minimizing resource requirements (i.e. storage/disk space). Graph Compression the HDT format. 25 Figure 13 shows the performance for log graph compression on the Auditlog dataset. Original raw log data |	We performed full property extraction (i.e., all 36 identified properties) against 5k, 10k, 15k, and 20k log-lines, respectively, and compare the original size of raw log data, the Figure 12. Dynamic log graph generation time. Experiments carried out on AuditLog data on a single host. generated RDF graph in TURTLE39format (.ttl), and the compressed graph output in HDT Figure 12. Dynamic log graph generation time38 format. Figure 12 shows an evaluation of log graph generation performance with respect to the complexity of the log source. We use the Auditlog for this evaluation as it has the highest Graph Compression to 5.4 MB for the uncompressed RDF graph. 20k log lines (4 MB raw log) compresses to about 1.87 MB from 21.4 MB uncompressed generated RDF graph. Overall, the compressed Figure 13 shows the performance for log graph compression on the Auditlog dataset. version is typically less than half the size of the original raw log and 10x smaller than We performed full property extraction (i.e., all 36 identified properties) against 5k, 10k, 15k, and 20k log-lines, respectively, and compare the original size of raw log data, the generated RDF graph. The resulting graph output would be even smaller for fewer the generated RDF graph in TURTLE [19] format (.ttl), and the compressed graph output in extracted properties, minimizing resource requirements (i.e. storage/disk space). Graph Compression the HDT format. 25 Figure 13 shows the performance for log graph compression on the Auditlog dataset. Original raw log data |	We performed full property extraction (i.e., all 36 identified properties) against 5k, 10k, 15k, and 20k log-lines, respectively, and compare the original size of raw log data, the Figure 12. Dynamic log graph generation time. Experiments carried out on AuditLog data on a single host. generated RDF graph in TURTLE39format (.ttl), and the compressed graph output in HDT Figure 12. Dynamic log graph generation time38 format. Figure 12 shows an evaluation of log graph generation performance with respect to the complexity of the log source. We use the Auditlog for this evaluation as it has the highest Graph Compression to 5.4 MB for the uncompressed RDF graph. 20k log lines (4 MB raw log) compresses to about 1.87 MB from 21.4 MB uncompressed generated RDF graph. Overall, the compressed Figure 13 shows the performance for log graph compression on the Auditlog dataset. version is typically less than half the size of the original raw log and 10x smaller than We performed full property extraction (i.e., all 36 identified properties) against 5k, 10k, 15k, and 20k log-lines, respectively, and compare the original size of raw log data, the generated RDF graph. The resulting graph output would be even smaller for fewer the generated RDF graph in TURTLE [19] format (.ttl), and the compressed graph output in extracted properties, minimizing resource requirements (i.e. storage/disk space). Graph Compression the HDT format. 25 Figure 13 shows the performance for log graph compression on the Auditlog dataset. Original raw log data |	We performed full property extraction (i.e., all 36 identified properties) against 5k, 10k, 15k, and 20k log-lines, respectively, and compare the original size of raw log data, the Figure 12. Dynamic log graph generation time. Experiments carried out on AuditLog data on a single host. generated RDF graph in TURTLE39format (.ttl), and the compressed graph output in HDT Figure 12. Dynamic log graph generation time38 format. Figure 12 shows an evaluation of log graph generation performance with respect to the complexity of the log source. We use the Auditlog for this evaluation as it has the highest Graph Compression to 5.4 MB for the uncompressed RDF graph. 20k log lines (4 MB raw log) compresses to about 1.87 MB from 21.4 MB uncompressed generated RDF graph. Overall, the compressed Figure 13 shows the performance for log graph compression on the Auditlog dataset. version is typically less than half the size of the original raw log and 10x smaller than We performed full property extraction (i.e., all 36 identified properties) against 5k, 10k, 15k, and 20k log-lines, respectively, and compare the original size of raw log data, the generated RDF graph. The resulting graph output would be even smaller for fewer the generated RDF graph in TURTLE [19] format (.ttl), and the compressed graph output in extracted properties, minimizing resource requirements (i.e. storage/disk space). Graph Compression the HDT format. 25 Figure 13 shows the performance for log graph compression on the Auditlog dataset. Original raw log data |	We performed full property extraction (i.e., all 36 identified properties) against 5k, 10k, 15k, and 20k log-lines, respectively, and compare the original size of raw log data, the Figure 12. Dynamic log graph generation time. Experiments carried out on AuditLog data on a single host. generated RDF graph in TURTLE39format (.ttl), and the compressed graph output in HDT Figure 12. Dynamic log graph generation time38 format. Figure 12 shows an evaluation of log graph generation performance with respect to the complexity of the log source. We use the Auditlog for this evaluation as it has the highest Graph Compression to 5.4 MB for the uncompressed RDF graph. 20k log lines (4 MB raw log) compresses to about 1.87 MB from 21.4 MB uncompressed generated RDF graph. Overall, the compressed Figure 13 shows the performance for log graph compression on the Auditlog dataset. version is typically less than half the size of the original raw log and 10x smaller than We performed full property extraction (i.e., all 36 identified properties) against 5k, 10k, 15k, and 20k log-lines, respectively, and compare the original size of raw log data, the generated RDF graph. The resulting graph output would be even smaller for fewer the generated RDF graph in TURTLE [19] format (.ttl), and the compressed graph output in extracted properties, minimizing resource requirements (i.e. storage/disk space). Graph Compression the HDT format. 25 Figure 13 shows the performance for log graph compression on the Auditlog dataset. Original raw log data |	We performed full property extraction (i.e., all 36 identified properties) against 5k, 10k, 15k, and 20k log-lines, respectively, and compare the original size of raw log data, the Figure 12. Dynamic log graph generation time. Experiments carried out on AuditLog data on a single host. generated RDF graph in TURTLE39format (.ttl), and the compressed graph output in HDT Figure 12. Dynamic log graph generation time38 format. Figure 12 shows an evaluation of log graph generation performance with respect to the complexity of the log source. We use the Auditlog for this evaluation as it has the highest Graph Compression to 5.4 MB for the uncompressed RDF graph. 20k log lines (4 MB raw log) compresses to about 1.87 MB from 21.4 MB uncompressed generated RDF graph. Overall, the compressed Figure 13 shows the performance for log graph compression on the Auditlog dataset. version is typically less than half the size of the original raw log and 10x smaller than We performed full property extraction (i.e., all 36 identified properties) against 5k, 10k, 15k, and 20k log-lines, respectively, and compare the original size of raw log data, the generated RDF graph. The resulting graph output would be even smaller for fewer the generated RDF graph in TURTLE [19] format (.ttl), and the compressed graph output in extracted properties, minimizing resource requirements (i.e. storage/disk space). Graph Compression the HDT format. 25 Figure 13 shows the performance for log graph compression on the Auditlog dataset. Original raw log data |549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 573 574 575 565 576 566 577 567 578 568 579 569 580 570 581 582  571 572 || 38 |20 |20 |20 |20 |20 |20 |Generated RDF Graph (.ttl) |Generated RDF Graph (.ttl) |549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 573 574 575 565 576 566 577 567 578 568 579 569 580 570 581 582  571 572 || 38 |Experiments carried out on AuditLog data on a single host. |Experiments carried out on AuditLog data on a single host. |Experiments carried out on AuditLog data on a single host. |Experiments carried out on AuditLog data on a single host. |Experiments carried out on AuditLog data on a single host. |Experiments carried out on AuditLog data on a single host. |Compressed RDF Graph (.hdt) |Compressed RDF Graph (.hdt) |549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 573 574 575 565 576 566 577 567 578 568 579 569 580 570 581 582  571 572 || size (MB) |size (MB) |15  10 |15  10 |15  10 |15  10 |15  10 |15  10 |15  10 |549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 573 574 575 565 576 566 577 567 578 568 579 569 580 570 581 582  571 572 |
5