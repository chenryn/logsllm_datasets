### Optimized Text

#### Failures and I/O Usage
For System A, the overall failure likelihood increases with the total remote I/O rate in shared single jobs, as shown in Fig. 5b. Recall that resource usage in shared mode is the aggregated usage across all jobs running on the node at a given time. In this context, failures are triggered by interference among multiple jobs competing for the remote storage service. 

At a high level, contention for remote resources with elements outside the node is predominant in non-shared environments, while contention with other jobs executing on the same node is dominant in shared environments. Consequently, there is a negative correlation between failure rate and I/O usage in non-shared environments, and a positive correlation in shared environments.

For System B, no correlation is observed between failure rate and total remote I/O usage rate for single-node jobs. The failure rate distribution is flat across the entire range of remote I/O usage. This is expected because the peak read/write rate by a single node is 9.6 GB/s (limited by the network interface card capacity), whereas the file system's peak bandwidth is much higher at 1.1 TB/s.

However, for multi-node jobs in System B, the failure rate is positively correlated with tail usage rate, with a sharp peak near 46 MB/s (Fig. 5c). It is challenging to pinpoint why contention in the NFS occurs precisely at this traffic volume. We hypothesize that at this volume, aggregated over all nodes, there is significant contention at the NFS. Although its rated capacity is 1.1 TB/s, its actual operating limit is much lower due to random access, similar to local I/O. Upon deeper investigation, we find that most jobs failing at this peak have an exit code of 107, indicating that they cannot connect to the NFS. Notably, 33% of all system-related job failures during the study period are due to unreachability of the remote file system.

To understand the impact of these failures, we clustered all jobs with exit code 107 using the DBSCAN algorithm [22] with parameters ε=300s and min job failure count=25. A total of 26 clusters were identified, as shown in Fig. 5d. The median time between these burst failures (more than 25 jobs failing within 300s) is 3.3 days. The unreachability of the remote file system can be attributed to: (i) remote file system failure (2 clusters), (ii) network failure (4 clusters), and (iii) congestion in the network and NFS (remaining 20 clusters).

#### Implications for System Design
Bandwidth to the parallel file system remains a critical issue for large-scale systems. When increasing bandwidth is not feasible, it is essential to carefully monitor net usage and stagger I/O requests from different applications during periods of contention.

### Prediction of Remote I/O Usage
The analysis shows that contention at remote storage increases job failure likelihood, especially in shared environments where contention with other jobs on the same node is significant. Predicting the remote I/O requirement of a job can enable better scheduling strategies, such as scheduling jobs with high remote I/O at different times. Table VI presents the results of different I/O usage predictors. The Maximum Cosine Similarity model, with a MAPE of 22.3%, outperforms the others.

### Network Usage
#### Relation of Job Failure with Network Usage
We omitted all plots in this section to save space. No significant correlation was found for non-shared multi-node jobs in System A and single-node jobs in System B (refer to Table V). However, non-shared single jobs in System A and multi-node jobs in System B show a negative correlation with tail I/O usage rate. This implies that system issues such as contention or poor network connectivity cause jobs to fail with low tail network usage. For shared single-node jobs in System A, a positive correlation is observed.

#### Prediction of Network Usage
A network usage prediction model can help minimize contention. Table VI shows the performance of four different models on the test dataset. All models have a promising MAPE of < 18%, with the MCS-based model performing the best at 13.0%.

### Job Node-Seconds
#### Relation of Job Failure with Job Node-Seconds
This section analyzes the effect of total node-seconds on job failure rates. Prior studies have found a positive correlation between failure rate and total execution time [14], [19]. For example, [19] found a linearly increasing relationship (in log-log scale) between the probability of application failure and application node hours for extreme-scale Blue Waters applications. Here, we examine if a similar relationship holds for System A and System B.

For System A, the relation has a negative slope for all three categories of jobs—non-shared single (Fig. 6a), non-shared multi, and shared single. This counter-intuitive relationship can be explained by the observation that many novice users submit jobs to System A. The allocation model allows any faculty member who purchases even one asset in the system to authorize researchers from her group to execute on the cluster. Therefore, many poorly written jobs that make huge demands on system resources (such as loading large datasets into memory) and fail quickly. Conversely, jobs that have executed for a while are less likely to run into such problems.

For System B, the trend is the opposite. Long-running jobs put pressure on system resources and have a higher likelihood of failing due to system issues. These codes are more mature, and long-running jobs are exposed to more faults in space or time or both. Interestingly, a significant fraction of all failed jobs fail with job execution times less than 1 minute—34% for System A and 45% for System B. These short-lived jobs cause inefficient cluster usage due to the constant overhead of scheduling, initiating, and terminating execution.

#### Implications for System Design
With mature, demanding codes, the job failure rate increases with larger jobs. However, with naive usage models, the failure rate is high for short jobs.

#### Prediction of Job Runtime
While a runtime prediction model cannot prevent job failures, it can significantly improve scheduling quality. Users often overestimate their runtime (by more than 9X on median), leading to longer queue times. Table VI shows the results of different runtime prediction models on the test dataset. All predictors have a MAPE of less than 17%, with the median predictor outperforming the others.

### Application of the Analyses
#### Predicting Job Failures
The previous section shows that job failure rates are correlated with certain resource usages. We explore whether impending failures can be predicted to take mitigation actions, such as taking a checkpoint and migrating the process. We use various ML models, including linear regression, logistic regression, and decision trees, and find that gradient-boosted decision trees perform the best based on recall and precision scores. The output is converted to a binary decision using a threshold, indicating whether the job will fail or not. These models are implemented using the XGBoost package in Python (refer to Figure 7). Figure 8 shows the precision-recall curves for different execution environments and job types, generated by varying the threshold. The model performance for non-shared single job types in System B is not usable, as the number of jobs failing due to system issues is insignificant (0.1%, refer to Table IV). Precision scores for multi-node jobs are better than those for single-node jobs, likely due to the higher diversity in single-node jobs.

Our job failure prediction model can reduce resource wastage by triggering a checkpoint when failure is imminent. The application registers a callback, which is invoked by our system upon this event. Since the failure prediction model is not perfect, we recommend combining our ML model with the optimal periodic checkpointing method, such as those given by Young [82] or Vaidya [76], to achieve the best savings. In our implementation, a checkpoint is taken whenever either method suggests it.

Figure 9 shows the schemes of optimal periodic checkpointing and ML + optimal periodic checkpointing methods. Savings are defined as the time that would have been wasted due to job failure and no checkpointing, minus the overhead of checkpointing, expressed as a percentage of the total execution time of the job. In the case of periodic checkpointing, work done between the last checkpoint and the failure is lost. Our ML-based checkpointing minimizes this wastage by forcing a checkpoint whenever it predicts a failure with a probability above a threshold. Monitoring happens periodically (every 5 minutes).

Table VIII presents the normalized area under the curve of Figure 10, normalized with respect to jobs with no wastage due to failures. We present results with TS = 60s, a reasonable value for real production jobs. For jobs with higher TS values, we recommend using data compression techniques, such as the one by Islam [34], which reduced large-scale application checkpointing overhead to less than 60s. With SSDs, this overhead can be further reduced to TS = 10s [3].

---

This optimized text aims to provide a clear, coherent, and professional presentation of the information.