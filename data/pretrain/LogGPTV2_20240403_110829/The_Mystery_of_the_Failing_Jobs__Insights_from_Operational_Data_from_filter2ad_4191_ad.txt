fail with low I/O usage rate. However, for shared single jobs
for System A, the overall failure likelihood increases as total
remote I/O rate increases as shown in Fig. 5b. Recollect that
the resource usage in shared mode is the aggregated resource
usage across all jobs running on the node at the given time.
Here failure is triggered by interference among multiple jobs
contending for the remote storage service. At a high level,
the contention for remote resources with executing elements
outside the node is predominant in non-shared environment,
while the contention with other jobs executing on that same
node is dominant for a shared environment. Therefore there is
a negative correlation of failure rate in the non-shared environ-
ment while a positive correlation in the shared environment.
In case of System B, no correlation is observed between
failure rate and total remote I/O usage rate for single-node
(a) System A non-shared single
(b) System A shared single
(a) System A non-shared single
(b) System A shared single
Fig. 4: Failure rate vs total tail local IO rates
(c) System B non-shared multi
(d) Sys B — exit code=107
Fig. 5: Failure rate vs total tail network ﬁle system I/O rates
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:28:14 UTC from IEEE Xplore.  Restrictions apply. 
164
jobs. Moreover, the failure rate distribution is ﬂat for the whole
remote I/O range. This is expected for System B since the peak
read/write rate by a single node is 9.6 GB/s (limited by the
network interface card capacity) whereas the peak bandwidth
supported by the ﬁle system is much higher at 1.1 TB/sec.
However, for multi-node jobs, we observe that the failure rate
is positively correlated with tail usage rate with a sharp peak
close to 46 MB/sec (Fig. 5c). It is difﬁcult to explain why
contention in the NFS happens precisely at this trafﬁc volume
going out of one node. However, we hypothesize that at this
volume, aggregated over all the nodes, there is contention at
the NFS. Though its rated capacity is 1.1 TB/s, its actual
operating limit is much lower due to random access (as shown
for local I/O). On investigating deeper and analyzing all the
failed jobs around the peak, we ﬁnd that most of the jobs
in that peak failed with an exit code 107. Exit code 107 is
returned when ‘transport endpoint is not connected’ indicating
jobs are not able to connect to the NFS. Tellingly, 33% of all
system-related job failures across the study period are due to
unreachability of remote ﬁle system.
To understand the failures caused by unreachability of
remote ﬁle system and its system-wide impact, all jobs with
exit code 107 are clustered using the DBSCAN algorithm [22]
with parameters =300s and min job failure count=25.  is the
maximum radius of the neighborhood from point p, where p
represents a failed job. A total of 26 clusters are found using
this approach and are shown in Fig. 5d along with the total
number of failures that belong to the cluster. The median time
between occurrence of these burst failures (i.e., more than 25
jobs failing within a duration of 300s) is 3.3 days. A remote ﬁle
system may become unreachable due to: (i) remote ﬁle system
failure (2 clusters), (ii) network failure (4 clusters) and, (iii)
congestion in the network and NFS (remaining 20).
Implications for System Design: Bandwidth to the parallel
ﬁle system continues to be a problem for large-scale systems.
Where it is not possible to increase that bandwidth, it is
needed to carefully monitor its net usage and to stagger the
IO requests from different applications at times of contention.
2) Prediction of remote IO usage based on user proﬁle::
The above analysis shows that contention at remote storage
increases job failure likelihood. Additionally, contention with
other jobs executing on that same node increases failure
likelihood for a shared environment. Hence, if the remote IO
requirement of a job can be predicted, it can enable a better
scheduling strategy such as scheduling jobs with high remote
IO at different times. Table VI shows the results of different IO
usage predictors. We observe the Maximum Cosine Similarity
model with M AP E = 22.3%, outperforms others.
D. Network
1) Relation of job failure with network usage:: We omitted
all plots of this section to save space. No signiﬁcant correlation
is found for non-shared multi-node jobs of System A and
(non-shared) single-node jobs of System B (refer Table V).
However, non-shared single jobs for System A and multi jobs
for System B are negatively correlated with tail I/O usage rate.
(a) System A non-shared single
(b) System B non-shared multi
Fig. 6: Failure rate vs node-seconds.
As discussed earlier, negative correlation implies system issues
such as contention or poor network connectivity is causing jobs
to fail with low tail network usage. Similar to remote I/O, we
observe overall positive correlation for shared single-node jobs
for System A.
2) Prediction of network usage based on user proﬁle::
Network usage prediction model can help in minimizing
the contention in network. Hence, here we explore different
network usage prediction models. Table VI shows the per-
formance of 4 different kinds of models on the test dataset.
All models have promising M AP E of < 18%. Among these,
MCS based model has the best performance of 13.0%.
E. Job Node-seconds
1) Relation of job failure with job node-seconds:: This
section analyzes the effect of the total node-seconds of a
job (deﬁned in Sec.II) on the job failure rate. Prior studies
have found positive correlation of failure rate with the total
execution time of an application [14], [19]. For example, [19]
found that for extreme scale Blue Waters applications, both
of CPU and CPU+GPU kind, there was a linearly increasing
relationship (in log-log scale) of the probability of application
failure and the application node hours. Here we analyze to see
if a similar relationship holds for System A and System B.
We ﬁnd that for System A, the relation has a negative slope
for all three categories of jobs—non-shared single (Fig. 6a),
non-shared multi, and shared single (negative correlation with
distribution similar to Fig. 6a for both non-shared multi and
shared single—plot omitted to save space). This seemingly
counter-intuitive relation can be explained by the observation
that many novice users submit jobs to System A. The allocation
model is that any faculty member who purchases even one
asset in the system can authorize researchers from her group
to execute on the cluster. Therefore, many poorly written jobs
get submitted, which on startup make huge demands on system
resources (such as loading huge datasets into memory) and
fail quickly. Hence, the high failure rate for low job execution
times. On the other hand, jobs that have executed for a while
are unlikely to run into such problems.
The trend for System B is the opposite. Here, long-running
jobs do put pressure on the system resources and hence have
a higher likelihood of failing due to system issues. Here
the codes are more mature and the long-running jobs are
exposed to more faults in space or time or both. Interestingly,
a signiﬁcant fraction of all failed jobs fail with job execution
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:28:14 UTC from IEEE Xplore.  Restrictions apply. 
165
Memory (GB)
Local IO (MB/s)
Network IO (MB/s)
Network (MB/s)
Node-seconds (s)
log10(x)
log10(x)
log10(x)
log10(x)
Gradient boosted
decision trees
(XGBoost)
Failure 
Probability
Fig. 7: ML model for predicting impending failure of a job.
Wastage
Monitoring
Window
Optimal Periodic
Checkpointing
t = 0
t = TOCP
t = 2TOCP
ML + Optimal Periodic
Checkpointing
Failure
Predicted
Failure
Fig. 9: Periodic vs ML + Periodic
Fig. 8: PR curves
Fig. 10: [Base case: Periodic] ML model + periodic checkpointing
savings as a percentage of the total time that would have been
wasted in the baseline case due to job failure and no checkpointing.
Mean time between failures (MTBF in seconds) and time to save
information at a checkpoint (Ts in seconds) used to compute
optimal checkpointing frequency are given in the bottom right of
each plot. Jobs with runtimes up to 5 hours are considered.
time less than 1 minute—34% for System A and 45% for
System B. These cause inefﬁcient cluster usage due to the
constant overhead of scheduling, initiating, and terminating
execution.
Implications for System Design: With mature demanding
codes, the job failure rate goes up with larger jobs. However,
with na¨ıve usage models, the failure rate is high for short jobs.
2) Prediction of job runtime based on user proﬁle: While
a runtime prediction model cannot help in preventing a job
failure, it can improve the quality of scheduling signiﬁcantly
(as we show in Section VI-B). We observe that users overesti-
mate their runtime (more than 9X on median) leading to longer
queue time since the scheduler takes requested walltime as
input. Hence, we explore different runtime prediction models
here. Table VI shows the results of the different models on the
test dataset. All predictors have M AP E less than 17% with
the median predictor outperforming others.
VI. APPLICATION OF THE ANALYSES
A. Predicting Job Failures
The previous section shows that the job failure rate is
correlated with usage for certain resources. The question we
pose ourselves is, can we predict impending failures to take
mitigation actions, such as, taking a checkpoint and migrating
TABLE VII: Symbols used for saving calculation
Symbol: Description
x: Total number of failures — y: Number of failures predicted
P: Precision of the model — R: Recall of the model
Tf : Mean time between node failure (MTBF)
Ts: Time to save information at a checkpoint
(cid:2)
Tw:
TsTf [82] (Work time between checkpoints)
TOCP: Ts + Tw (Optimal checkpointing period)
Tr: Runtime of the job
NOCP: (cid:3)Tr/TOCP(cid:4) (Number of periodic checkpoints taken)
SOCP: Saving by the optimal periodic checkpointing method
SML: Saving by the ML method
ST: Saving by the ML + periodic method
the process. This motivates us to explore different ML models
to predict job failure using the different resource usages for a
job as input features. We divide our dataset into training and
test sets in the ratio 7:3. After trying different ML models such
as linear regression, logistic regression, and decision trees, we
ﬁnd that gradient-boosted decision tree performs the best based
on recall and precision scores. The output is converted to a
binary decision using a threshold, whether the job will fail or
not. These gradient-boosted decision tree based models have
been implemented using the XGBoost package in Python
(refer Figure 7). Fig. 8 shows the precision-recall curves for
different execution environments and job types generated by
varying the threshold. We observe that model performance
corresponding to the non-shared single job types of System B
is not usable, expectedly since the number of jobs belonging
to failure category due to system issues is insigniﬁcant (0.1%,
refer Table IV). Additionally, precision scores of multi node
jobs are better than that of single node jobs. This we suspect
is because the diversity in the single node jobs is signiﬁcantly
higher than in the multi node jobs.
Our
job failure prediction model can reduce resource
wastage on these systems by triggering a checkpoint when
failure is imminent. The application registers a callback which
is invoked by our system upon this event. Since the failure
prediction model is not perfect, our ML model by itself may
not give the optimal savings. So, we recommend to combine
our ML model with the optimal periodic checkpointing method
such as one given by Young [82] or Vaidya [76] to obtain the
optimal savings. In our implementation, combination means
taking a checkpoint whenever either method suggests taking a
checkpoint—there are of course other ways of combining the
two methods, which we do not explore in this work.
Fig. 9 shows the schemes of optimal periodic checkpointing
and ML + optimal periodic checkpointing methods. We deﬁne
savings as the time that would have been wasted in the baseline
case due to job failure and no checkpointing (and that is saved
due to the checkpointing) minus the overhead of checkpoint-
ing, expressed as a percentage of the total execution time
of the job. In the case of periodic checkpointing, whenever
a failure occurs, the amount of work done between the last
checkpoint to the failure is lost. Our ML-based checkpointing
will minimize this wastage by forcing a checkpoint whenever
it predicts a failure and the probability is above a threshold.
Recall that monitoring happens periodically (5 minutes for
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:28:14 UTC from IEEE Xplore.  Restrictions apply. 
166
TABLE VIII: [Base case: Periodic] Normalized area under the
curve of Figure 10 (normalized with respect to jobs with no wastage
execution due to failures). We present results with TS = 60s a
reasonable value for real production jobs. For jobs with higher TS
values, we recommend to use data compression techniques such
as the one by Islam [34] (which reduced large-scale application
checkpointing overhead to less than 60s). With SSD, this overhead
can be further reduced to TS = 10s [3].
System
Periodic ML ML+Periodic
MTBF=1e4,
TS=60 sec
MTBF=1e5,
TS=60 sec
MTBF=1e6,
TS=60 sec
MTBF=1e6,
TS=10 sec
shared single
non-shared single
non-shared multi
non-shared multi
shared single
non-shared single
non-shared multi
non-shared multi
shared single
non-shared single
non-shared multi
non-shared multi
shared single
non-shared single
non-shared multi
non-shared multi
0.81
0.66
0.30
0.60
0.82
0.89
0.90
0.91
0.82
0.89
0.90
0.91
0.82
0.89