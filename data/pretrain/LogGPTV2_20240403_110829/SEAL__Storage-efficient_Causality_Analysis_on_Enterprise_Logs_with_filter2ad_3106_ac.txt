10000011111100100100110100 (26 bits). In this example,
32 bits are sufﬁcient to store the Golomb codeword.
3.5 Query and Decompression
As deﬁned by QFC, decompression is only necessary when
the relation between the time range speciﬁed in the query and
in the edge cannot be determined. If there are no intersections
of these two ranges, decompression can be skipped. In our
back-tracking queries, the above property holds for two rea-
sons. First, due to the order preservation property of Golomb
coding, it is unnecessary to decode all Golomb codes in the
database to answer a query with a timestamp constraint. The
speciﬁed timestamp can be simply encoded by Golomb code,
and used as the new constraint issued to the database. Sec-
ond, the minimum starttime t0
start is recorded in a merged
edge. Hence, if we back-track for events whose starttime
is smaller than some given tquery, then all individual edges of
an combined edge with t0
start > tquery will be rejected. There-
fore, the database does not need to decompress and can safely
reject this combined edge.
Here we use the example shown in Figure 3 to demon-
strate how the query and decompression work. Assume a
query tries to initiate back-tracking on E to ﬁnd the prior
causal events whose starttime is less than tquery = 65. First,
tquery will be Golomb coded into Gol(65). And the database
needs to ﬁnd events such that Gol(t0
start )  0.
In large graphs, it is hard to sample nodes in the entire
graph according to some distribution as we do not know the
number of nodes and the node degrees. To overcome such
difﬁculty, the Smooth algorithm can be modiﬁed such that the
sampled nodes are obtained by random walk [17]. However,
it makes some assumptions that do not readily ﬁt the depen-
dency graph problem: (i) The graph needs to be irreducible
and aperiodic. However, the dependency graph naturally con-
tains disconnected components. (ii) The sample complexity
needs to be high enough to pass the mixing time and approach
the steady-state distribution, which varies depending on the
structure of the graph.
To overcome these issues, two techniques are used in Al-
gorithm 2. First, random walk with escaping [7] jumps to a
random new node with probability p jump and stays on the ran-
dom walk path with probability 1− p jump (see Line 4). There-
fore, we can reach different components of the graph. Second,
thinning [41] takes one sample every θ samples as in Line
7. We obtain θ groups of thinned samples. If the samples are
indexed by 0,1,2, . . ., then in our algorithm the j-th group, de-
noted by S j, contains samples indexed by j, j + θ, j + 2θ, . . .,
for 0 ≤ j ≤ θ−1. Each group produces its own estimate (Line
13), and the ﬁnal estimate is the average of these groups (Line
14). Since the sample distribution is not uniform, we can-
not directly use the estimator of Equation (2). The sampled
degrees need to be re-weighted using the Hansen-Hurwitz
technique [32] to correct the bias towards the high degree
nodes, corresponding to the term dv + c in the numerator and
the denominator of Line 13. Note that due to the difﬁculty to
sample a node from the entire graph, the sample distribution
is not speciﬁed in Lines 2 and 9.
Algorithm 2 Average degree estimation.
Input: undirected graph H, sample size r, coarse aver-
age degree estimator c, thinning parameter θ, jumping
probability p jump
Output: average degree estimator ˆd
1: S j ← /0, j = 0,1, . . . ,θ− 1
2: Randomly sample a node vpre of H
3: for i = 0 to r− 1 do
4:
5:
6:
rnd ∼ Bernoulli(p jump)
if rnd = 0 then
Uniformly sample a neighbor v of vpre assuming
vpre also has c added self loops
Si mod θ ← Si mod θ ∪{v}
else
7:
8:
9:
10:
11:
12: end for
13:
ˆd j =
ˆd = 1
14:
Randomly sample a node v of H
end if
vpre ← v
∑v∈S j 1/(dv+c) , j = 0,1, . . . ,θ− 1
∑v∈S j dv/(dv+c)
θ ∑θ−1
ˆd j
j=0
4 Architecture
Design rationale. Figure 5 shows the architecture of SEAL
and how it is integrated into the log ingestion and analysis
pipeline. SEAL resembles the design [77] at the very high
level. In [77], the compression system mainly includes three
elements: computing components, caches, and the database.
In this work, we redesign those elements according to our
algorithm for both the compression system and the query
system. The compression system receives online data streams
of system events, encodes the data, and saves them into the
database. The query system takes a query, applies the query
transformation and recovers the result with post-processing,
and returns the result. The information ﬂow follows closely
the deﬁnition of QFC in Section 3.2 and includes the structure
and property compression Rs,Rp, the query transformation F,
and the post-processing P, which are explained in details in
Sections 3.3 – 3.5.
Due to the current monitoring system structure of our indus-
2994    30th USENIX Security Symposium
USENIX Association
trial collaborator, SEAL is solely deployed at the server-side by
the data aggregator. Note that, alternatively, one can choose
to compress the data at the host end before sending them to
the data aggregator. Since there are no cross-host events in
FileEvent, the compression ratio will be identical for both
choices.
Online compression. While ofﬂine compression can achieve
an optimized compression ratio with full visibility to the data,
it will add a long waiting time before a query can be processed.
Given that causality analysis could be requested any time of
the day, ofﬂine compression is not a viable option. As such,
we choose to apply online compression.
The online compression system is built by the following
main components: (i) The optional compression ratio esti-
mator. If the estimated ratio as described in Section 3.6 is
more than the given threshold, data is passed through the fol-
lowing components. Otherwise, data is directly stored in the
database. (ii) Caching. It organizes and puts the most recent
data steam into a cache. When the cache is ﬁlled, the data
will be compressed. The cache size is conﬁgurable, called
chunk size. (iii) Graph structure compression. It merges and
encodes all the edges that satisfy the edge merge pattern as in
Section 3.3. It also generates the node mapping between the
individual nodes and the new nodes, shown in Table 2. (iv)
Edge property compression. It encodes each event timestamp
entry using delta coding and Golomb codes as in Section 3.4.
Next, we remark on some design choices. The conﬁgurable
chunk size provides a tradeoff between the memory cost and
the compression ratio. The larger the chunk size, the more
edges can be combined. We found in our experiments as in
Section 5 that 134 MB per host is a large enough chunk size
offering sufﬁciently high compression capability.
Query. The query system comprises three main components.
(i) Query transformation. Given a query Q, SEAL transforms
it into another query Q(cid:48) that the compressed database can pro-
cess. In particular, it needs to transform the queried timestamp
and the srcid constraints, if there are any. The timestamp con-
straint is encoded into a Golomb codeword, which is used as
the new constraint as in Section 3.5. If a srcid is given, then
this individual node is mapped to all the corresponding new
nodes using the node map. (ii) Querying. The transformed
query Q(cid:48) is issued to the database and the answer is obtained.
(iii) Post-processing. The combined edges are decompressed
from delta codes, the timestamp constraint is checked, the
merged node is mapped to individual nodes, and the valid
individual edges are returned as described in Section 3.5.
Note that, in the query transformation component, if dstid
is a query constraint, then no node mapping is required since
only source nodes are merged during compression. In our
work, we focus on back-tracking, where srcid is not a query
constraint, hence the query transformation is simpliﬁed. More-
over, the node mapping progress is fast due to the small num-
ber of objects compared to the events.
For each given destination ID, at most one combined edge
Figure 5: The SEAL architecture of online compression and
querying.
will be returned as an answer in each chunk (containing 105
to 106 events depending on the chunk size). This observation
combined with the fact that the dependency graph is much
smaller after compression effectively controls the query over-
head in our experiments.
To quickly access the node map as in Table 2, it is cached
using a hash map. Given that the number of nodes is much
smaller than the number of edges, the memory size of this
hash map is a small fraction of the database size.
5 Evaluation
5.1 Experiment Setup
Our evaluation about compression is primarily on a dataset
of system logs collected from 95 hosts by our industrial part-
ner, which we call DSind. This dataset contains 53,172,439
events and takes 20GB in uncompressed form. For querying
evaluations, we select a subset of DSind covering 8 hosts, with
46,308 events and a total size of 8 GB. As DSind does not have
ground-truth labels of attacks, we use another data source un-
der the DARPA Transparent Computing program [16]. The
logs are collected on machines with OS instrumented, and a
red team carried out simulated APT attacks. Multiple datasets
are contained, and each one corresponds to a simulated at-
tack. We use CARDETs dataset, which simulates an attack
on Ngnix server, with a total of 1,183M events (27% write,
25.8% read and 47.2% execute), and we term this dataset
DSdtc. Since our system focuses on event merging, we only
compress the edges and a subset of the attributes, with 233GB
data size.
We implemented SEAL using JAVA version 11.0.3. We
use JDBC (the Java Database Connectivity) to connect to
PostgreSQL Database version Ubuntu 11.3-1.pgdg18.04+1.
For DSind, we run our system on Ubuntu 14.04.2, with 64 GB
memory and Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHZ.
To run the queries, one machine with AMD Ryzen 7 2700X
USENIX Association
30th USENIX Security Symposium    2995
DataEstimatorCachingStructureCompressionPropertyCompressionDatabaseQueryTransformationQueryingPost-ProcessorResultsDatabaseNode Map Node Map Eight-Core Processor and 16GB memory is used. For DSdtc,
we run the system on Ubuntu 16.04, with 32 GB memory and
Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz.
Section 2.3 compares the designs between SEAL and other
systems, and demonstrates when other systems introduce er-
rors to attack investigation. In this section, we quantify the
difference, and select the method of Full Dependency (FD)
preservation [37] as the comparison target, which strikes a
good balance between reduction rate and preservation of anal-
ysis results. Under FD, A node u is reachable to v if either
there is an edge e = (u,v) or there is a causality dependency
eu → ev, where eu is an outgoing edge of u, and ev is an in-
coming edge of v. We implement a relaxed FD constraint,
where repeated edges (between any pair nodes) are merged
such that the reachability for any pair of nodes in the graph
is maintained. The corresponding compression ratio is better
than FD since it is a relaxation. We compare the relaxed FD
with our method SEAL.