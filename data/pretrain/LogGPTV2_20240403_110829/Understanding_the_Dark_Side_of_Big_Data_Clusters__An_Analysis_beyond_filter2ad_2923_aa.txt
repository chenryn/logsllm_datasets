title:Understanding the Dark Side of Big Data Clusters: An Analysis beyond
Failures
author:Andrea Rosà and
Lydia Y. Chen and
Walter Binder
2015 45th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
2015 45th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
Understanding the Dark Side of Big Data
Clusters: An Analysis beyond Failures
Andrea Ros`a
Faculty of Informatics
Universit`a della Svizzera italiana
Lugano, Switzerland
Email: PI:EMAIL
Lydia Y. Chen
Cloud Server Technologies Group
IBM Research Lab Zurich
R¨uschlikon, Switzerland
Email: PI:EMAIL
Walter Binder
Faculty of Informatics
Universit`a della Svizzera italiana
Lugano, Switzerland
Email: PI:EMAIL
Abstract—Motivated by the high system complexity of today’s
datacenters, a large body of related studies tries to understand
workloads and resource utilization in datacenters. However, there
is little work on exploring unsuccessful job and task executions.
In this paper, we study three types of unsuccessful executions
in traces of a Google datacenter, namely fail, kill, and eviction.
The objective of our analysis is to identify their resource waste,
impacts on application performance, and root causes. We ﬁrst
quantitatively show their strong negative impact on CPU, RAM,
and DISK usage and on task slowdown. We analyze patterns
of unsuccessful jobs and tasks, particularly focusing on their
interdependency. Moreover, we uncover their root causes by
inspecting key workload and system attributes such as machine
locality and concurrency level. Our results help in the design of
low-latency and fault-tolerant big-data systems.
I.
INTRODUCTION
In today’s multi-tenancy and multi-purpose datacenters,
“big-data” is becoming the key application, featuring high
fanout jobs and performance dependency on data locality [1].
An increasing number of management systems aim at enhanc-
ing various aspects of big-data performance, in particular low-
latency requirements [2], resource efﬁciency for cost minimiza-
tion [3], and scalability on different system sizes [1]. However,
as failures in software or hardware are more norm than excep-
tion in large-scale datacenters [4], unsuccessful executions of
jobs prevail in a big-data cluster [5] and potentially turn into
a critical performance impediment. Therefore, it is imperative
to address the dependability issues of big-data applications,
starting from a deep understanding of unsuccessful executions
observed by the scheduler.
A large body of characterization studies [6]–[8] tries to
ﬁrst understand the workload demand and resource efﬁciency
observed in datacenters, using rich ﬁeld data collected by op-
erators such as Google. Unfortunately, their analyses overlook
job and task failures and shed no light on the corresponding
resource demands and impact on system efﬁciency. Existing
reliability analyses for big-data clusters are mainly conducted
from the perspective of hardware components [9], [10] and
software bugs [11], [12], overlooking unsuccessful executions
in big-data systems. Recently, a few studies [13]–[15] analyze
failures in a Google datacenter [16], providing a general
statistical summary of unsuccessful executions, particularly
focusing on jobs [13], machines [14] and task preemption [15].
However, the prior art overlooks the performance impact of un-
successful executions, their root causes, and the dependencies
80
70
60
50
]
%
[
40
30
20
10
0
Jobs
Tasks
Events
70.5
57.7
47.5
40.7
29.1
20.2
23.5
8.8
~0 ~0
Eviction
1.6 0.4
Fail
Kill
Finish
Type
Job, task, and event distribution per execution type: eviction, fail,
Fig. 1.
kill, and ﬁnish.
between tasks and jobs.
The objective of this paper is to provide a better under-
standing of the performance impact of unsuccessful executions,
their characteristics, and their relationship with application and
machine attributes in multi-purpose and multi-tenancy datacen-
ters. Our analysis is based on traces collected from schedulers
and machines in a sizeable datacenter [16], where each job is
composed of multiple tasks, which in turn experience multiple
events. All events are classiﬁed into four different types, i.e.,
eviction, fail, kill, and ﬁnish. In contrast to ﬁnish, we consider
the ﬁrst three types as unsuccessful executions, which do not
directly produce useful results and do not contribute to the
completion of jobs or tasks.
As a motivation of the urgency of investigating unsuccess-
ful executions, we present the distribution of different types
of executions collected at the job, task and event levels in
Figure 1. Finish only accounts for 47.5% of events, whereas
the remainder is distributed on fail, kill, and eviction with the
percentages being 23.5%, 20.2%, and 8.8% respectively. In
contrast, jobs and tasks have a different trend, showing the
majority of unsuccessful executions to be kill, followed by fail.
At all levels, one can clearly see that unsuccessful executions
are signiﬁcant, amounting to 52.5%, 42.3% and 29.5% of
the total events, jobs and tasks respectively. Both jobs and
tasks have a high probability to be executed without successful
ﬁnish and cause signiﬁcant resource waste and slowdown of
application performance.
Visual inspection of Figure 1 guides our analysis to ques-
tions concerning different types of unsuccessful executions,
particularly the quantitative performance impact, the patterns
978-1-4799-8629-3/15 $31.00 © 2015 IEEE
978-1-4799-8629-3/15 $31.00 © 2015 IEEE
DOI 10.1109/DSN.2015.37
DOI 10.1109/DSN.2015.37
207
207
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:48:35 UTC from IEEE Xplore.  Restrictions apply. 
and dependencies between jobs, tasks and events, and the re-
lationship with application and system attributes. The building
blocks of our analysis are the statistics of different types of
unsuccessful job/task/event executions, in correlation with key
job and system attributes, while highlighting the differences
and similarities between unsuccessful executions at the job,
task and event levels.
The challenges of the proposed analysis stem from the
complex nature of such big clusters, i.e., jobs with a high
number of task fanout and tasks subjected to multiple events,
and are magniﬁed by the size of the system and the resulting
trace volume. Owing to limited description on the trace and
no access to the actual system, our characterization study
resorts to a black-box approach that may fall short in providing
concise analysis. Moreover, our ﬁndings are to a certain extent
bounded to the trace available to us, i.e., the Google trace. To
the best of our knowledge, this is the only public available trace
providing detailed information related to the dependability of
job and task executions in a large scale datacenter.
Our study consists of three parts: (1) spatial and temporal
performance impact of unsuccessful executions, (2) patterns
and predictive models of task, job and event types, and (3) the
analysis on the root causes of unsuccessful executions. For
the performance impact, we focus on how different resources,
such as CPU, RAM, and DISK, are consumed by all four
types of events, and how tasks and jobs are slowed down by
unsuccessful executions. To derive patterns, we leverage prob-
abilistic models and statistical distributions of inter-type times.
Finally, to identify application and system attributes that lead to
a high frequency of certain event types, we compare the rate of
unsuccessful executions relative to different levels of resource
requirement, machine capacity, resource utilization/reservation,
task priority, machine locality, job size, machine concurrency,
and job/task execution time.
The contributions of this paper are numerous:
to the
best of our knowledge, this is the ﬁrst ﬁeld analysis of its
kind on multiple types of executions at the level of events,
tasks, and jobs in big-data clusters, quantifying the negative
performance impact and uncovering the complex dependencies
between tasks and jobs. Our analysis provides several crucial
observations and insights to derive predictive models for ﬁnish,
kill, eviction, and fail jobs/tasks/events. Moreover, by applying
a black-box approach to correlate applications and various
system parameters, we reveal the mechanisms used by the
underlying resource management policies and shed light on
fault-tolerant system design.
The outline of this work is as follows. Section II provides
an overview of the dataset and the data-collection method-
ology. The performance impact of unsuccessful executions is
detailed in Section III. We present models and patterns for
tasks, jobs and events in Section IV. In Section V, we reveal
the root causes of unsuccessful executions. Section VI presents
related work, followed by the summary and conclusions in
Section VII.
II. DATA COLLECTION
Our analysis is based on the Google cluster trace [16],
which represents a rich heterogeneous workload mix executing
on a large heterogenous cluster for 29 days, from May 1st to
Fig. 2.
Illustration of the type classiﬁcation at job, task and event levels.
May 29th, 2011. Table I summarizes the cluster conﬁguration,
where all values are normalized by the maximum amount of
machine capacity. The cluster is composed of 12585 machines.
DISTRIBUTION OF MACHINES [%] AND THEIR CAPACITY AT
TABLE I.
THE BEGINNING OF THE TRACE. RESOURCES ARE NORMALIZED BETWEEN
[0, 1].
%
53.47
30.74
7.95
6.32
1
CPU RAM % CPU RAM % CPU RAM
0.97
0.5
0.5
0.5
0.5
0.06
1
0.5
0.25
0.75
0.25
0.12
0.03
0.99
0.43
0.04
0.25
0.5
0.5
0.03
0.02
0.01
0.5
1
0.5
A. Job vs. Task vs. Event Types
job,
Note that the workload in this trace can be studied at
three levels:
task and event. The complex nature of
jobs, tasks and events increases the difﬁculty of our analysis
tremendously. Prior to explaining the attributes of interests and
our data collection process, we ﬁrst explain how jobs, tasks,
and events are correlated and how they can be classiﬁed into
four types of executions, namely kill, eviction, fail, and ﬁnish,
supported by the illustration of Figure 2.
Users submit jobs to the cluster. A single job is composed
by one or more tasks,
the minimum running entities. A
task runs on a single machine at a time. A single task can
experience multiple events. Events can be of four different
types, i.e., fail, kill, eviction and ﬁnish. After an event, a task is
descheduled from the system and might be resubmitted. Each
job and task is associated with a single type. To facilitate the
analysis, we assign types to tasks based on their last event type.
For example, a task can be submitted four times, be evicted
the ﬁrst three times, and ﬁnish successfully the last time. We
assign ﬁnish as its task type, as the last event experienced by
the task was ﬁnish. On the contrary, a job can be submitted
only once and its type is given directly by the trace. Indeed,
although its tasks can be classiﬁed into different types based
on their last events, there is no clear description given on how
to relate task and job types. Section IV is completely devoted
to analyzing this relationship. Note that throughout the paper,
for ease of readability, we also use the typeset of fail, kill,
eviction and ﬁnish as adjective, i.e., type events/tasks/jobs.
As for the reasons behind these event types, the trace
tasks are subjected to
provides the following explanation:
eviction by the scheduler due to preemption by higher-priority
tasks, overcommitment of resources or hardware failure, they
can be killed either by the users or by the scheduler1, or can fail
1For example, because a job/task on which this job/task is dependent
terminates unsuccessfully, or because the task uses more resources than
requested.
208208
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:48:35 UTC from IEEE Xplore.  Restrictions apply. 
after an internal error. Unfortunately, the trace does not provide
detailed information on why a task, job or event terminated
with a given type.
1) Job and Task Life Cycle: Clearly, tasks have a quite
complex life cycle, as they can go through multiple events. In
this work, we use the following terminology when referring
to the task life cycle. As soon as tasks enter the cluster,
they are submitted to the system and wait for a scheduling
decision. Later, tasks are scheduled on machines and begin
their execution, which can be either successful or unsuccessful.
In both cases, tasks complete their execution with a given
event type. At this point, tasks can either go through the
same life cycle (being submitted again) or leave the system
permanently. In the latter case, we set the task type equal to
the last event type. Intuitively, we call the timestamps of these
events submission time, scheduling time and completion time.
Jobs follow the same life cycle, with the following differences:
(1) job scheduling time coincides with the ﬁrst scheduling of
any of its tasks, and (2) jobs can be submitted to the system
only once.
B. Job and Task Attributes
To understand the performance impact, patterns, and root
causes of unsuccessful executions, we analyze the following
attributes at the job, task and event level, provided to us by
the trace:
•
•
Resources: the trace provides three kinds of resources,
i.e., CPU, RAM, and DISK. They can be speciﬁed by
users upon task submission, measured at run time by
the proﬁler or equipped in machines. The ﬁrst kind
of resource is termed requested, the second kind is
called used, while we use the term machine capacity to
identify the third kind. CPU expresses the maximum
number of cores that tasks can use, whereas RAM
and DISK specify the amount of volatile and mass
memory, respectively. All resources are normalized
between [0, 1] by the largest machine capacity of any
machine in the system.
Priority: users assign each task a priority value,
ranging between [0, 11], where high values represent
important tasks. There is no priority information pro-
vided at the job level.
Execution time of jobs, tasks and events in the system.
Job size: number of tasks that compose a job.
•
•
• Machine locality: the ratio between the number of
unique machines required by all events belonging to
the same job and the job size. This metric implicitly
reﬂects data locality.
C. Data-Filtering Methodology
We face several challenges in gathering the attributes
outlined above, because of their different logging granularities,
structures, and even missing information. To better sanitize the
data, we use the following ﬁltering steps prior to proceeding to
our core analysis: 1) ﬁltering out jobs with no tasks; 2) ﬁltering
out jobs with missing information; 3) ﬁltering out jobs and
tasks still in execution at the end of the trace; 4) ﬁltering out
209209
]
%
[
45
40
35
30
25
20
15
10
5
0
Resubmission
Queue
Running
Eviction
Fail
Kill
Finish
Type
Fig. 3. Machine time distribution per event type.
jobs and tasks that arrive or start before the beginning of the
trace2. Overall, our proposed procedure ﬁlters no more than
0.89% of jobs and 1.36% of tasks, for a total of 667624 jobs
and ∼25M tasks after ﬁltering.
III. PERFORMANCE IMPACT OF UNSUCCESSFUL
EXECUTIONS
Our initial investigation showed that there exists a signiﬁ-
cant number of unsuccessful executions, i.e., eviction, fail and
kill. Those events not only occupy resources while leading to
non-meaningful work for a signiﬁcant amount of time, but also
possibly degrade the task execution times. As the workload is
highly heterogeneous in such clusters [6], it is not clear how
different types of machine resources are consumed by different
events. The ﬁrst objective of this study is to quantify the impact
of unsuccessful executions on the entire system performance.
We focus on how machine time and resources are wasted, by
means of a temporal vs. spatial resource analysis from the
perspective of single tasks as well as the entire cluster. As
tasks may be resubmitted to the cluster after an unsuccessful
execution without resuming from where they were previously
killed, evicted or failed, we consider these times and resources
as “wasted”.
We ﬁrst present results on wasted machine time in Sec-
tion III-A, followed by wasted machine resources, i.e., CPU,
memory and disk, in Section III-B.
A. Temporal Impact: Machine Time Waste
In this section, we try to disclose how machine time is
distributed across different event types. As illustrated previ-
ously, events consume machine time at different stages of their
execution. Consequently, we divide the time spent by them into
three categories: (1) resubmission time: the interval between
the completion time of the preceding event and the submission
time of the current event; (2) queue time: the interval between
submission and scheduling time; (3) running time: the interval
between scheduling and completion time. We compute the
machine time per event type and normalize the result by the
aggregate machine time of all events in the cluster. We present
the normalized time distribution in Figure 3.
Wasted machine time due to unsuccessful executions is