(cid:16) p
(cid:17), where p is the probability of the user being at
For each explanatory variable, logistic regression assigns a co-
efficient that estimates the change in the log odds-ratio of the
dependent variable. In other words, the coefficients estimate the
change in ln
risk. For example, if the coefficient of the binary variable indicating
whether a user reported to have an anti-virus installed is estimated
to be β > 0, then the odds of visiting malicious sites are e β times
higher for users who report to have an anti-virus on their device
than those who do not. Alternately, if β is negative, then the odds
of being at risk decrease for users who have an anti-virus.
5.1 Experiment Design
We use the three-month worth of HTTP requests we collected, and
the survey responses to build the logistic regression. Using τ = 2
0.000.250.500.751.000.00.10.20.30.40.5CDF over usersUnexposed (τ=0)Unexposed (τ=2)Unexposed (τ=22)Unexposed (τ=max)Exposed (τ=0)Exposed (τ=2)Exposed (τ=22)Exposed (τ=max)0.000.250.500.751.000.00.10.20.30.40.5Unexposed (τ=0)Unexposed (τ=2)Unexposed (τ=22)Unexposed (τ=max)Exposed (τ=0)Exposed (τ=2)Exposed (τ=22)Exposed (τ=max)Session 8A: Web 1CCS’18, October 15-19, 2018, Toronto, ON, Canada1494to have an anti-virus were 2.51 times more likely than others to
visit malicious URLs. We hypothesize that these users have a false
sense of security that leads them to engage in a high-risk behavior—
something that was previously observed by others [14]. It may also
be the case that the users installed an anti-virus in response to a
past exposure. Finally, users who achieved full proactive awareness
score on RSeBIS were 0.82 times as likely as those who achieved
the lowest score to visit malicious URLs, supporting prior evidence
that the scale correlates with certain security behavior [11, 21, 22].
Our analysis provides insights into how self-reported informa-
tion correlates with actual exposure to malicious content, and how
different types of information collected via self-reporting differ
in their importance. Yet, the best model we could build using the
data we asked about can explain only 5% of the variance (i.e., the
2 = 0.05). Thus, despite being rather diverse and certainly helpful,
R
the self-reported information we considered may be insufficient
by itself to fully explain exposure to malicious content online. We
have to complement it with other features, as shown next.
6 LONG-TERM PREDICTION OF EXPOSURE
Now we move from exploring the relationship between self-reported
information and exposure to predicting exposure. In particular, we
evaluate predictive models that leverage self-reported answers to
surveys, past behavioral observations, or both to predict user ex-
posure over a relatively long time period (specifically, one month).
Here, as opposed to the previous section, models that rely solely on
survey responses help us estimate how accurately can self-reported
data predict exposure. Additionally, models that rely on past behav-
ioral observations of users help set a baseline to evaluate the utility
of self-reported data for prediction.
6.1 Classifier Design
Self-Reported Features. From the survey responses, we extract
seven features for prediction. Six of the features are the same as
described in Sec. 5.1. The seventh denotes users’ self-confidence in
their security knowledge (computed via summing the responses to
the Likert-scale questions then normalizing to the [0, 1] range).
Past-Behavior Features. The main premise when using Past-Beha-
vior Features is that past behavior is indicative of future one (e.g.,
users who visited malicious domains in the past are likely to visit
malicious domains in the future [10]). The features that we develop
are motivated by the findings of Sec. 4 regarding the differences
between exposed and unexposed users. Some of the features quan-
tify the user’s amount of activity per day: average number of ses-
sions and HTTP requests, average number of bytes uploaded and
downloaded, and average session length in seconds and number of
requests. Two features are used to estimate the amount of past expo-
sure: One feature indicates prior exposure, and another quantifies
the fraction of previously exposed sessions. 24 features summarize
the level of activity during different hours of the day, and 99 features
describe how previous HTTP requests are distributed among the 99
topics of DigitalArts [39]. Finally, one feature reports the fraction
of request to domains not in the Alexa top 100,000 websites. The
premise behind the last feature is that top websites are unlikely to
be malicious or to link to malicious content. Overall, we use 132
Past-Behavior Features.
Table 2: Summary of the classifiers’ design.
Classifier
Input
Long term (Sec. 6)
Random Decision Forest
Past-Behavior
and/or
Self-Reported features
Output
Class balance
Probability estimate of
user exposure
18:82
Short term (Sec. 7)
Conv. neural network
Contextual, and, possi-
bly, Past-Behavior and/or
Self-Reported features
Probability estimate of
exposure within session
0.1:99.9
(a) Threshold effect
(b) Feature set effect
Figure 7: ROC curves for long-term predictions. Left: the effect of
the threshold (as a number s of previously exposed sessions) using
all (PS) features. Right: the effect of the various feature sets, using
a threshold of s = 4 sessions.
Prediction Algorithm. We use Random Decision Forests [37], a
popular machine-learning algorithm, to perform predictions. For
best performance, we set the number of classification trees to 50,
the maximum height to 5, and weigh negative examples by ×0.1
the weight of positive ones. We also experimented with other clas-
sification algorithms, but they did not perform as well. Table 2
summarizes the classifier’s design.
6.2 Experiment Design
Essentially, the question that we aim to answer is: based on users’
self-reported data and/or browsing behavior in a specific month,
can we predict exposure in the following month? To answer this
question, we use our data to train Random Decision Forests to
predict exposure in May (τ = 2) using observations made in April,
and evaluate how well these models can predict exposure in June
via observations made in May. We perform ten-fold cross-validation
rounds in which we use 90% of users for training and 10% of users
for testing.
6.3 Experiment Results
Fig. 7 shows the overall classifier performance as a function of two
parameters. Using our complete feature set (Self-Reported and Past
Behavior), Fig. 7(a) shows the influence of the session threshold s. s
is used to denote the number of distinct exposed sessions a user has
to endure to be considered as exposed. Intuitively, the higher s, the
more conservative the notion of user exposure. In particular, with
s = 1, a user might have landed on an exposed webpage by accident;
with s = 5, on the other hand, the user is repeatedly getting exposed.
0.000.250.500.751.000.000.250.500.751.00False Positive RateTrue Positive Rates=1s=2s=3s=4s=50.000.250.500.751.000.000.250.500.751.00False Positive RateTrue Positive RateSPPSSession 8A: Web 1CCS’18, October 15-19, 2018, Toronto, ON, Canada1495Unsurprisingly, for long-term predictions, a higher s seems to work
better. Fig. 7(b) shows the impact of both feature sets: S denotes the
impact of self-reported features, P, that of past-behavior features,
and PS the impact of considering both sets. As expected, the whole
set (PS) achieves the best results, but more interestingly, the impact
of the Self-Reported Features by themselves is very modest. While,
in isolation, Self-Reported Features might be useful (e.g., if user
monitoring is not an option), the Past-Behavior Features achieve
much better performance on their own; and combining them with
the Self-Reported Features does not yield a great improvement.
Our long-term prediction system performs slightly worse than
Canali et al.’s system [10]. However, we use a far more restricted
set of features: our data collection is less intrusive, we only col-
lect text/html content, and most importantly, our features are con-
siderably cheaper to compute (e.g., most can be computed using
counters).
Regardless of the accuracy of long-term predictions, they are
not a panacea—recall from Sec. 4 that many users are actually only
exposed once or twice (Finding 2). This begs for an additional set of
classification primitives, that attempt to prevent exposure on much
shorter time intervals.
7 WITHIN-SESSION EXPOSURE PREDICTION
We next explore the feasibility of predicting whether a user will
engage in risky behavior (namely, visiting a malicious website)
during a single browsing session. Thus, we aim to use contextual
features that describe the session at some stage, to predict whether
the user will be exposed at a later stage in the session. Each time the
user makes an HTTP request, the contextual features are updated to
describe the new state of the session, and another prediction attempt
is made. As Sec. 6 and prior work have shown, users’ behavioral
features (e.g., number of websites visited [50]) can help predict user
exposure risk over long periods of time (e.g., three months [10]).
We next show how to use similar behavioral features to predict
exposure on much shorter time scales.
7.1 Classifier Design
The prediction problem we aim to solve poses two key challenges.
First, the dataset is imbalanced—there are about 1,733 benign ses-
sions for each exposed session. Learning from imbalanced datasets
is hard, as classifiers which predict the majority class most of the
time, albeit useless, would be favored by training algorithms due to
their high accuracy [36]. We address this challenge by undersam-
pling the majority class during training. Other methods propose to
oversample the minority class (e.g., SMOTE [13] and ADASYN [35]),
but we did not find them useful. Second, browsing sessions contain
large numbers of HTTP requests. Thus, the prediction algorithm
and the feature computation need to be highly efficient to be de-
ployed in real-time. We thus design features that can be efficiently
updated after each HTTP request, and rely on a compact neural
network for prediction.
Contextual Features. From the observations presented in Sec. 4,
we develop a set of session-level features to predict whether a
session will become exposed. In essence, the Contextual Features are
similar to the Past-Behavior Features from Sec. 6, but are computed
over the course of the session, rather than over the entire historical
observations we have about the user.
Since exposed sessions tend to have a higher amount of activity,
features quantifying magnitude of activity are a good proxy for our
predictions. These include session length (in seconds), number of
HTTP requests, and amount of bytes transferred during the session.
Malicious domains are less likely to be (directly) linked to from
top-domains. So, we use the fraction of HTTP requests to non-top
domains (i.e., outside the Alexa top-100,000) as another feature.
Exposed sessions are more likely to occur in the weekends, and
late in the day (Finding 5). We thus use a feature to indicate whether
the session is taking place during the weekend, and 24 features to
indicate the hours in which the session has been active.
Six binary features indicate the operating system and the browser
observed in the session (as learned from the user-agent strings in
HTTP headers): Two of the features indicate Android and iOS
(the two most popular mobile OSes [80]), two features indicate
the use of Chrome and Safari (the most popular browsers [77]),
and two features are used to indicate other OSes and browsers
(e.g., Firefox OS). While our exploratory data analysis and prior
work [49] did not indicate stark differences in the risk profile of
different systems (e.g., iOS vs. Android), these features could help
capture the subtle differences between systems (e.g., how often
browser blacklists are updated) that could affect users’ exposure.
Last, 99 features describe the topics of domains visited in the
session. As prior work has shown, and in line with Finding 6, cer-
tain website categories (e.g., online streaming [61]) tend to exhibit
more malicious activity than others. Hence, visits to such websites
are likely to increase the likelihood of exposure. The features we
developed reflect the distribution of domains in the HTTP requests
among the 99 Digital Arts categories [39].
All 135 contextual features we use can be computed efficiently
throughout sessions via counters and table lookups. Some of the
models presented in this section also rely on the Past-Behavior
and Self-Reported Features from Sec. 6. For a given session, we
compute the Past-Behavior Features using the user’s history up to
the beginning of that session. Once the session is over, we update
the Past-Behavior Features based on the observations made during
the session.
Prediction Algorithm. We use convolutional deep neural net-
works (DNNs) to perform predictions [30]. DNNs can achieve state-
of-the-art performance on many different tasks ranging from image
classification [7] to speech recognition (e.g., [70, 89]) to playing
Go [16]. Besides their high performance, DNNs are practically use-
ful when training with large datasets as their parameters can be
learned iteratively using small batches of data.
We train DNNs using data from the early parts of our collection
and evaluate them using data from later parts. As the goal is to pre-
dict exposure to malicious content before the exposure takes place,
HTTP requests in exposed sessions that are made to or after the
visits of malicious URLs are disregarded. We create a feature vector
for each HTTP request in the training set. We assign a positive label
(i.e., 1) to HTTP requests that belong to exposed sessions, and a
negative label (i.e., 0) to the remaining requests. During the training
process we do not use HTTP requests in exposed sessions that are
made more than one minute before the moment of exposure. This
Session 8A: Web 1CCS’18, October 15-19, 2018, Toronto, ON, Canada1496apparently improves performance, possibly because requests that
are made long before exposure takes place are unlikely to play an
influential role in the exposure.
Additionally, both in training and testing, we disregard the first
nine HTTP requests of each session. These requests are needed to
create context about the browsing sessions and to bootstrap the
Contextual Features. In other words, we perform predictions only
for sessions with more than nine requests. Consequently, we only
keep sessions with more than nine requests for training and testing
(these include ∼96% of all exposed sessions). Using Selenium [18]
and tPacketCapture [79], we crawled the Alexa top 100 HTTP-only
websites on an Android phone, and found that, on average, two
website visits correspond to ten HTTP requests. So, our proposed
system would begin analyzing a user browsing activity after she
has visited two websites, on average.
In each training epoch of the DNN, to address dataset imbalance,
we only use a random subset of the negative examples for training,
such that the number of positive and negative samples is equal [36].
At test time, we classify a session as exposed only if the feature vec-
tor describing the state of the session at a certain stage is classified
as positive.
The DNNs we use are sequential, and consist of three convo-
lutional layers, each followed by a Rectified Linear activation, a
fully-connected layer, and a softmax layer. The convolutions use
5× 128 kernels, and are applied with a stride of one. In training, the
DNNs’ the cross-entropy loss is minimized. This is a standard choice
of architecture and training strategy [29, 30, 68]. Before selecting
the convolutional architecture, we initially tested a neural network
architecture consisting of fully-connected layers only, finding it to
perform less accurate predictions.
We normalize the DNNs’ inputs to a [0, 1] range using statistics
learned from the training data, as normalization helps to speed up
training and improve performance [17]. We set the learning rate
to 5 × 10−5, the batch size to 128, and the number of epochs to 50.
We performed a grid search to set the hyper parameters, optimize
depth of the DNNs, and the size of the convolution kernels. We
implement the DNNs in Keras [43] (with a TensorFlow backend [1]).
Table 2 summarizes the predictor’s design.
7.2 Experiment Design
We trained several DNNs to predict whether sessions will become