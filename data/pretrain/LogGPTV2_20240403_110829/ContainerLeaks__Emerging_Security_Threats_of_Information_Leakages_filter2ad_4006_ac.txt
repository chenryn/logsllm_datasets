(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
values and are unique for every host machine. Similarly,
energy uj in the RAPL sysfs interface is the accumulated energy
counter in micro joules. The data read from channels in this
group change at real time, but are still unique to represent
a host. We rank the channels in this group based on their
growth rates. A faster growth rate indicates a lower chance of
duplication.
The metric V demonstrates whether the data change with
time. With this feature available, two containers can make
snapshots of this pseudo ﬁle periodically at the same time.
Then they can determine co-residence by checking whether
two data snapshot traces match with each other. For example,
starting from the same time, we can record MemFree in
/proc/meminfo from two containers every second for one minute.
If these two 60-point data traces match with each other, we
are conﬁdent that these two containers run on the same host.
Each channel contains a different capacity of information for
inferring co-residence, which can be naturally measured via the
joint Shannon entropy. We deﬁne the entropy H in Formula (1).
Each channel C contains multiple independent data ﬁelds Xi,
and n represents the number of independent data ﬁelds. Each Xi
has possible values {xi1,··· , xim}. We rank the capability of
revealing co-residence for the nine channels (for which U=False
and V=True) based on their entropy results in Table II.
H[C(X1, · · · , Xn)] =
n(cid:2)
[− m(cid:2)
i=1
j=1
p(xij ) log p(xij )].
(1)
The metric M indicates whether the container tenants can
manipulate the data. We mark a channel (cid:2) if tenants can
directly implant specially-crafted data into it. For example, we
can create a timer in a program with a special task name inside
a container. This task name and its associated timer will appear
in /proc/timer list. Another container can search for this special
task name in the timer list to verify co-residence. We mark a
channel (cid:4)(cid:3) if tenants can only indirectly inﬂuence the data in
this channel. For example, an attacker can use taskset command
to bond a computing-intensive workload to a speciﬁc core, and
check the CPU utilization, power consumption, or temperature
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:19:41 UTC from IEEE Xplore.  Restrictions apply. 
/proc/interrupts
/proc/sys/fs/inode-nr
/proc/sys/fs/file-nr
TABLE II: LEAKAGE CHANNELS FOR CO-RESIDENCE VERIFICATION.
Rank
Leakage Channels
U V M
/proc/sched_debug
/proc/timer_list
/proc/schedstat
/proc/softirqs
/proc/locks
/proc/uptime
/proc/stat
/proc/sys/kernel/random/boot_id
/sys/devices/system/node/node#/numastat
/sys/class/powercap/.../energy_uj2
/sys/devices/system/.../usage3
/sys/devices/system/.../time4
/proc/sys/fs/dentry-state
(cid:2) (cid:3) (cid:3)
/sys/fs/cgroup/net_prio/net_prio.ifpriomap (cid:2) (cid:3) (cid:3)
(cid:2) (cid:2) (cid:2)
(cid:2) (cid:2) (cid:2)
(cid:2) (cid:2) (cid:2)
(cid:2) (cid:2) (cid:4)(cid:3)
(cid:2) (cid:2) (cid:4)(cid:3)
(cid:2) (cid:2) (cid:4)(cid:3)
(cid:2) (cid:2) (cid:4)(cid:3)
(cid:2) (cid:2) (cid:4)(cid:3)
(cid:2) (cid:2) (cid:4)(cid:3)
(cid:2) (cid:2) (cid:4)(cid:3)
(cid:2) (cid:2) (cid:4)(cid:3)
(cid:2) (cid:2) (cid:4)(cid:3)
(cid:2) (cid:2) (cid:4)(cid:3)
(cid:2) (cid:2) (cid:4)(cid:3)
(cid:2) (cid:2) (cid:4)(cid:3)
(cid:3) (cid:2) (cid:4)(cid:3)
(cid:3) (cid:2) (cid:4)(cid:3)
(cid:3) (cid:2) (cid:4)(cid:3)
(cid:3) (cid:2) (cid:4)(cid:3)
(cid:3) (cid:2) (cid:4)(cid:3)
(cid:3) (cid:2) (cid:4)(cid:3)
(cid:3) (cid:2) (cid:4)(cid:3)
(cid:3) (cid:2) (cid:4)(cid:3)
(cid:3) (cid:2) (cid:3)
(cid:3) (cid:3) (cid:3)
(cid:3) (cid:3) (cid:3)
(cid:3) (cid:3) (cid:3)
/proc/sys/kernel/random/entropy_avail
/proc/sys/kernel/.../max_newidle_lb_cost6
/proc/modules
/sys/devices/system/node/node#/meminfo
/sys/devices/platform/.../temp#_input5
/proc/loadavg
/sys/devices/system/node/node#/vmstat
/proc/fs/ext4/sda#/mb_groups
/proc/cpuinfo
/proc/version
/proc/zoneinfo
/proc/meminfo
Low High
from another container. Those entries could be exploited by
advanced attackers as covert channels to transmit signals.
For those channels that do not have these U V M properties,
we consider them hard to be exploited. For example, most
servers in a cloud data center probably install the same OS
distribution with the same module list. Although /proc/modules
leaks the information of loaded modules on the host, it is
difﬁcult to use this channel to infer co-resident containers.
IV. SYNERGISTIC POWER ATTACK
At ﬁrst glance,
the leaked information discovered in
Section III seems difﬁcult to exploit. Because both procfs and
sysfs are mounted read-only inside the containers, malicious
tenants can only read such information, but modiﬁcation is not
allowed. We argue that attackers can make better decisions by
learning the runtime status of the host machine.
In this section, we present a potential synergistic power
attack in the scope of power outage threats that may impact the
2/sys/class/powercap/intel-rapl:#/intel-rapl:#/energy uj
3/sys/devices/system/cpu/cpu#/cpuidle/state#/usage
4/sys/devices/system/cpu/cpu#/cpuidle/state#/time
5/sys/devices/platform/coretemp.#/hwmon/hwmon#/temp# input
6/proc/sys/kernel/sched domain/cpu#/domain#/max newidle lb cost
)
W
W
(
(
r
r
e
e
w
o
P
1200
0
1150
1
1100
1050
1050
1000
0
0
Sampling interval:1s
Sampling interval:1s
ng inter
50
100
Time (s)
me (s)
150
200
2000
Sampling interval:30s
1
2
3
Time (day)
4
5
6
7
1150
1100
1050
1000
)
W
(
r
e
w
o
P
950
0
Fig. 2: The power consumption for 8 servers in one week.
reliability of data centers. We demonstrate that adversaries can
exploit these information leakages discovered by us to amplify
the attack effects, reduce the attack costs, and facilitate attack
orchestration. All experiments are conducted in a real-world
container cloud.
A. Attack Ampliﬁcation
The key to launching a successful power attack is to
generate a short-time high power spike that can surpass the
power facility’s supply capacity. As we mentioned in II-C,
the root cause of power attacks is the wide adoption of
power oversubscription, which makes it possible for power
spikes to surpass the safe threshold. In addition, a rack-level
power capping mechanism can only react in minute-level time
granularity, leaving space for the occurrence of a short-time
high power spike. In the most critical situation, the overcharging
of power may trip the branch circuit breaker, cause a power
outage, and ﬁnally bring down the servers. The heights of power
spikes are predominantly determined by the resources that are
controlled by attackers. Existing power attacks maximize the
power consumption by customizing power-intensive workloads,
denoted as power viruses. For example, Ganesan et al. [15],
[16] leveraged genetic algorithms to automatically generate
power viruses that consume more power than normal stress
benchmarks. However, launching a power attack from scratch
or being agnostic about the surrounding environment wastes
unnecessary attacking resources.
In a real-world data center, the average utilization is around
20% to 30%, as reported by Barroso et al. [9]. With such
low utilization, the chance of tripping the circuit breaker by
indiscriminately launching power attacks is extremely low.
However, although the average utilization is low, data centers
still encounter power outage threats under peak demands [37].
This indicates that the power consumption of physical servers
ﬂuctuates enormously with the changing workloads. To conﬁrm
this assumption, we conduct an experiment to monitor the
whole-system power consumption (via the RAPL leakage
channel in case study II of Section III) of eight physical servers
in a container cloud for one week. We present the result in
Figure 2. We ﬁrst average the power data with a 30-second
interval and observe drastic power changes on both Day 2
and Day 5. Furthermore, we pick a high power consumption
region in Day 2 and average the data at the interval of one
second (which is a typical time window for generating a power
spike). The peak power consumption could reach 1,199 Watts
(W). In total, there was a 34.72% (899W ∼ 1,199W) power
difference in this one-week range. We anticipate that the power
consumption difference would be even larger if we could
242
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:19:41 UTC from IEEE Xplore.  Restrictions apply. 
Synergistic
Periodical
250
200
150
100
50
)
W
(
r
e
w
o
P
0
0
200
No attack
1 Container
2 Containers
3 Containers
800
1000
400
600
Time (s)
Fig. 3: The power consumption of 8 servers under attack.
Fig. 4: The power consumption of a server under attack.
monitor it for a longer time period, such as on a holiday
like Black Friday, when online shopping websites hosted on a
cloud may incur a huge power surge.
For a synergistic power attack in a container cloud, instead
of indiscriminately starting a power-intensive workload, the
adversaries can monitor the whole-system power consumption
through the RAPL channel and learn the crests and troughs of
the power consumption pattern at real time. Therefore, they
can leverage the background power consumption (generated
by benign workloads from other tenants on the same host)
and superimpose their power attacks when the servers are at
their peak running time. This is similar to the phenomenon
of insider trading in the ﬁnancial market—the one with more
insider information can always trade at the right time. The
adversaries can boost their power spikes, by adding on already-
high power consumption, to a higher level with the “insider”
power consumption information leaked through the RAPL
channel.
B. Reduction of Attack Costs
From the attackers’ perspective, they always intend to
maximize attack outcomes with the lowest costs. Running
power-intensive workloads continuously could deﬁnitely catch
all the crests of benign power consumption. However, it may
not be practical for real-world attacks for several reasons.
First, it is not stealthy. To launch a power attack, the attacker
needs to run power-intensive workloads. Such behavior has
obvious patterns and could be easily detected by cloud providers.
Second, utilization-based billing models are now becoming
more popular. More cloud services provide ﬁner-grained prices
based on CPU/memory utilization and the volume of network
trafﬁc. For instance, Elastic Container provides containers with
CPU metering-based billing for customers [3]. IBM Cloud
provides billing metrics for computing resources in the cloud
[4]. Amazon EC2 [1] offers Burstable Performance Instances
that could occasionally burst but do not fully run most of the
time. The VMware OnDemand Pricing Calculator [5] even
gives an estimate for different utilization levels. For example,
it charges $2.87 per month for an instance with 16 VCPUs
with an average of 1% utilization, and $167.25 for the same
server with full utilization. Under these cloud billing models,
continuous power attacks may ﬁnally lead to an expensive bill.
For synergistic power attacks, monitoring power consump-
tion through RAPL has almost zero CPU utilization. To achieve
the same effects (the height of power spikes), synergistic power
attacks can signiﬁcantly reduce the attack costs compared to
continuous and periodic attacks. In Figure 3, we compare the
attack effects of a synergistic power attack with a periodic attack
(launching power attacks every 300 seconds). Synergistic power
attacks can achieve a 1,359W power spike with only two trials
in 3,000 seconds, whereas periodic attacks were launched nine
times and could only reach 1,280W at most.
C. Attack Orchestration
Different from traditional power attacks, another unique
characteristic of synergistic power attack is its attack orches-
tration. Assume an attacker is already controlling a number
of container instances. If these containers scatter in different
locations within a data center, their power additions on multiple
physical servers put no pressure on power facilities. Existing
power-capping mechanisms can tolerate multiple small power
surges from different locations with no difﬁculty. The only
way to launch a practical power attack is to aggregate all
“ammunition” into adjacent locations and attack a single power
supply simultaneously. Here we discuss in depth on the
orchestration of attacking container instances.
As we mentioned in Section III, by exploiting multiple
leakage channels7, attackers can aggregate multiple container
instances into one physical server. Speciﬁcally in our experiment
on CC1, we choose to use timer list to verify the co-residence
of multiple containers. The detailed veriﬁcation method is
explained in Section III-C. We repeatedly create container
instances and terminate instances that are not on the same
physical server. By doing this, we succeed in deploying three
containers on the same server with trivial effort. We run four
copies of Prime [7] benchmark within each container to fully
utilize the four allocated cores. The results are illustrated
in Figure 4. As we can see, each container can contribute
approximately 40W power. With three containers, an attacker
can easily raise the power consumption to almost 230W, which
is about 100W more than the average power consumption for
a single server.
We also ﬁnd /proc/uptime to be another interesting leakage
channel. The uptime includes two data entries, the booting time
of the physical server and the idle time of all cores. In our
experiment, we ﬁnd that some servers have similar booting
times but different idle times. Typically servers in data centers
do not reboot once being installed and turned on. A different
idle time indicates that they are not the same physical server,
7Typically, if a channel is a strong co-residence indicator, leveraging this
one channel only should be enough.
243
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:19:41 UTC from IEEE Xplore.  Restrictions apply. 
cgroup
Initialization
cpuacct
cgroup
perf_event
cgroup
perf_event