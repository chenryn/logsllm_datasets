(FPR=3.5%)
TPR
0.923
from di(cid:130)erent perspectives, we also include the area under the ROC
curve (AUC) score [7], in addition to the G-mean score that we
already used in Section 5.2. Overall, Marmite achieves an average
AUC of 0.960 (with a standard deviation of 0.01), and an average
G-mean of 0.944 (with a standard deviation of 0.008). (cid:140)is shows
that Marmite can o(cid:130)er high accuracy with stable performance
over time since the standard deviation of both AUC and G-mean
scores over the measurement period of 7 days are small. We further
con(cid:128)rm this (cid:128)nding by testing Marmite on a dataset collected six
months later in Section 6.1.
We then wanted to understand the true positive rate (TPR) and
false positive rate (FPR) reported by Marmite on labeled data.
(cid:140)ese values are important, because they give us a feeling of how
well Marmite would perform if ran in the wild. Table 1 reports the
results of this experiment. We start by se(cid:138)ing a fairly high false
positive rate of 3.5%, and measure that in this se(cid:138)ing Marmite
has a TPR of 0.923 (which corresponds to a false negative rate of
7.7%) on average. By decreasing the false positive rate, the TPR
decreases but remains high. With a FPR of 1%, Marmite reports a
TPR of 0.786 (false negative rate of 21.4%) on average. By decreasing
the false positive rate even further, to 0.5%, Marmite reports and
average TPR of 0.690 (false negative rate of 31%). (cid:140)is result shows
that Marmite could be set to have very low false positives and still
be useful in practice, (cid:131)agging a signi(cid:128)cant amount of malware. In
addition, in Section 6.1 we show that whitelisting can be used to
further reduce false positives. We also show that false negatives
in the wild are lower than what reported by these validation tests,
which were only performed on ground truth of known malicious
and benign (cid:128)les. Marmite, regardless the size of the graph, can also
maintain comparable accuracy in terms of both AUC and G-mean
scores thanks to recursively propagated evidence across the whole
graph. Details can be found in Section 5.5.
5.5 Modifying the Observation Interval
Our hypothesis is that Marmite, regardless the size of the graph,
should maintain comparable accuracy in terms of both AUC and G-
mean scores thanks to recursively propagated evidence across the
whole graph. To check if this is the case, we build three download
graphs from observation periods of one hour (00:00 - 01:00), two
hours (00:00 - 02:00), and three hours (00:00 - 03:00), extracted from
the data collected on a one day subset of the dataset D1. We use the
same parameters as in the rest of the paper. We list the results of
this experiment in Table 2 and compare the results to those obtained
on the data extracted over an entire day. As we can see, even if
the graphs for shorter time intervals are signi(cid:128)cantly smaller, both
AUC and G-mean scores from all three experiments remain similar
to those for the full day data: we have an AUC of 0.975 (G-mean
0.919) for the one-hour interval, an AUC of 0.977 (G-mean 0.923) for
Table 2: Performance of Marmite using one hour, two hour,
and three hour observation intervals on one day of data. As
it can be seen, Marmite reports good results both in terms of
AUC and G-Mean for all interval lengths.
Observation interval
00:00 - 01:00
00:00 - 02:00
00:00 - 03:00
00:00 - 23:59
#nodes
147,721
261,686
355,787
2,256,984
#edges
196,297
359,602
498,745
3,364,215
AUC
0.975
0.977
0.978
0.972
G-mean
0.919
0.925
0.934
0.957
6 EVALUATION
In the previous section we validated Marmite by testing it on
ground truth data collected over a period of 10 days (dataset D1). In
this section we evaluate it against dataset D2, which was collected
six months later. We (cid:128)rst show that Marmite is still e(cid:130)ective in
detecting malware without need for re-tuning a(cid:137)er this period
of time, signi(cid:128)cantly growing the amount of detected malware
samples compared to the ones used for seeding. We also show that
Marmite is able to detect malware before VirusTotal.
6.1 Malware Detection In the Wild
We ran Marmite on the entire 30-day dataset D2. On average, we
seeded the system with 111,449 benign (cid:128)les and 7,657 malicious
ones every day. We used four di(cid:130)erent modes of operation for
Marmite, which means that for each of them we set the parame-
ters that reported 3.5%, 2%, 1%, and 0.5% FPR in the ground truth
validation from Section 5.4. (cid:140)e overall results for our experiments
are reported in Table 3.
For each of the se(cid:138)ings, we carefully ve(cid:138)ed the results provided
by Marmite. In particular, we (cid:128)rst checked whether the detections
performed were either con(cid:128)rmed by Symantec’s internal systems
or if the SHA2 hashes of the detected (cid:128)les appeared as malicious
in VirusTotal. We considered these detections as true positives
by Marmite. Note that we split detections between malware and
PUP, based on Symantec’s feedback. As we show in Section 7.4,
it is o(cid:137)en di(cid:129)cult to distinguish the two types of operations. As
it can be seen in Table 3, the fraction of detections performed by
Marmite is generally high. For the 3.5% se(cid:138)ing we can con(cid:128)rm 94%
of the detections as either malware or PUP. (cid:140)is number gradually
increases as we make detection stricter, peaking at 98% for the
se(cid:138)ings that lead to a 0.5% FPR during the validation phase.
For the false positive analysis, we looked at (cid:128)les whose SHA2s
are known as benign by Symantec. As it can be seen in Table 3,
false positives decrease as we make Marmite stricter on which (cid:128)les
it considers as malware. For the 3.5% case, Marmite reports a 5.8%
false positive rate in the wild. (cid:140)is number decreases steadily as the
system becomes stricter, up to arriving at 1.1% for the 0.5% se(cid:138)ing.
In general, we can observe that Marmite’s results in the wild are
slightly worse than they were by looking at labeled data only (as we
did in Section 5.4). As we explained, however, we expect many of
97Table 3: Summary of detection performance by Marmite on our 30-day measurement data. “Setting” reports the FPR obtained
when setting the same parameter values during the validation phase.
Se(cid:138)ing
0.005
0.01
0.02
0.035
Tot.
Prediction
1,684,439
2,104,897
2,496,526
2,864,481
Con(cid:128)rmed
Malware
1,540,877
1,757,027
1,899,359
1,985,959
Con(cid:128)rmed
PUP
111,913
277,091
481,170
707,545
FPs before
Whitelisting
24,546 (1.1% FPR)
53,949 (2.3% FPR)
89,175 (3.8% FPR)
134,657 (5.8% FPR)
FPs a(cid:137)er
Whitelisting
14,978 (0.6% FPR)
34,676 (1.5% FPR)
51,277 (2.2% FPR)
67,752 (3.0% FPR)
FNs
Unknown
593,643 (26.2% FNR)
383,634 (15.6% FNR)
246,175 (9.1% FNR)
163,940 (5.5% FNR)
7,101
16,827
26,811
36,298
these false positives to be systematic and not change signi(cid:128)cantly
over time, so that they can be easily removed by using a whitelist.
In Section 7.1 we show a detailed example of these systematic false
positives. To test this hypothesis, we compile a whitelist of known
benign (cid:128)les from January 2016, and apply it to our results obtained
six months later, in June. As Table 3 reports, the whitelist is able to
reduce false positives signi(cid:128)cantly. For the 3.5% se(cid:138)ing, the false
positive rate of the (cid:128)ltered dataset is only 3%, while for the 0.5%
se(cid:138)ing it becomes 0.6%.
While false positives in the wild turn out to be slightly higher
than in the validation phase, false negatives are lower, indicating
that Marmite is able to comparatively detect more malware than
it was present in our ground truth. As it can be seen in Table 3, the
false negative rate for the 3.5% se(cid:138)ing is 5.5%, while it was 7.6%
on the ground truth. Similarly, for the 0.5% case the FNR is 26.2%,
while it was 31% during the validation. Finally, for a small number
of (cid:128)les none of our sources could con(cid:128)rm whether these (cid:128)les were
benign or malicious. We list them as “unknown” in Table 3.
Based on the results reported in this section, we can see that
Marmite is able to e(cid:130)ectively detect malware, and does not require
frequent retraining, as the results in the wild six months a(cid:137)er
the tuning of the systems are generally in line with the original
validation results. Depending on how strict the operator wants
to be in making detections, Marmite can increase the original
knowledge of malware from the seed (cid:128)les between 11 times (in the
3.5% case) and 6 times (in the 0.5% case).
Figure 5: Number of malicious (cid:128)les detected by
Marmite in advance and average early detection
time.
6.2 Early Detection of Unknown Malware
We estimate that if Marmite was run in production it would have
been able to detect these (cid:128)les as malicious before other antivirus
programs. To understand how relevant this early detection would
have been, we looked at the average days passed between when
Marmite (cid:131)agged a (cid:128)le as malicious and when VirusTotal (cid:131)agged
it as such too. Note that we consider a malware sample as detected
by VirusTotal if it is (cid:131)agged by one of the top (cid:128)ve AV products plus
any other two [24]. For evaluation purpose in this section, we set
FPR=3.5%. Figure 5 reports a summary of the number of malware
samples detected by Marmite that were unknown to VirusTotal
at the moment, together with how much later (in days) these (cid:128)les
appeared on VirusTotal as malicious. On average, Marmite was
able to detect 1,870 (cid:128)les as malware 6.46 days before VirusTotal on
a daily basis.
7 LESSONS LEARNED AND CASE STUDIES
In this section, we provide some interesting case studies that we
encountered while operating Marmite. Note that we remove FQDN
nodes from all the (cid:128)gures to make them easier to read.
7.1 Malicious programs download legitimate
libraries.
In this section, we carry out a detailed case study on PUPs drop-
ping both benign DLLs and further PUPs. (cid:140)is is a typical ex-