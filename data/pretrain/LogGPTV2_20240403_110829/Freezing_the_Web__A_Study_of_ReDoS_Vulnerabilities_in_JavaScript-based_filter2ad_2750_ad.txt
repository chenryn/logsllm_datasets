the ReDoS payloads have a cumulative effect and even a
small delay in the main loop can cause signiﬁcant harm
for availability.
We remind the reader that the above experiment uses
the smallest payload in our data set, forwarded. There-
fore, if we show that even this exploit poses a threat to
availability, we can conclude that the rest of the exploits
also do. For more severe ReDoS vulnerabilities, e.g. in
ua-parser-js, there is even no need to evaluate the im-
pact on availability. As described in the Section 2, one
single such payload is enough to completely block the
server for as long as the matching takes. Considering
that with 50–60 characters we predict a CPU computa-
tion time in the order of years, such vulnerabilities are a
very serious threat to availability.
4.4 Response Time vs. Matching Time
Our methodology relies on the assumption that small
changes in the server computation time have an effect
on clients. To validate this assumption we again use
the forwarded package and the commercial web server
setup from the previous section. We use 1,000 pay-
loads smaller than 8,000 characters. The largest one of
these payloads produces a matching time smaller than
100 milliseconds on our local machine. We measure
the time spent by the server in the forwarded package
and the time it takes for a request to be served at the
client level. We then plot the relation between these two
time measurements in Figure 9. The correlation between
both measurements is 0.99, i.e., very strong. The strong
correlation shows that the delays introduced by the net-
work layer are relatively constant over time and that the
server computation time is the dominant component in
the response time measured at the client-side. Of course,
the observed value depends on the chosen web server
provider and the current server load, but we can safely
conclude that measuring time at the client level is a good
enough estimation of the server-side computation time.
4.5 Dimensioning Exploits
Choosing an appropriate size for the payload is a cru-
cial part in our methodology and distinguishes our study
from a real DoS attack on websites. The goal of this step
is to ﬁnd a payload size that is large enough to check
whether a website is vulnerable to a speciﬁc attack, but
small enough to only block the website for a negligible
amount of time. To this end, we locally run each exploit
ﬁve times with a payload of increasing size and stop the
process when the matching time exceeds two seconds.
We consider ﬁve target matching times, 100ms, 200ms,
500ms, 1s, and 2s, and choose the payload size that pro-
duces the closest matching time to the target time.
Figure 10 shows the values for each target time and
vulnerable module. For example, for the platform vul-
nerability, we obtain a matching time of 200ms with
a payload of 11,000 characters. The useragent and
ua-parser-js packages, whose matching times grow
at a much faster rate, requiring less than 1,500 characters
to cause a delay of 2s.
4.6 Vulnerable Sites
The goal of the next step is to assess to what extent real
websites suffer from ReDoS vulnerabilities. Based on
the ﬁve payload sizes for each exploit, we create attack
payloads and random payloads for each exploit and pay-
load size. We send these payloads to the 2,846 real web-
sites that are running an Express webserver (Section 3.1).
We warm up the connection three times and then mea-
sure ﬁve response times for both random and malicious
inputs. Using the methodology described in Section 3.4,
we then decide based on the measured response times
whether a site is vulnerable.
If for some reason, we
could not send three or more out of the ﬁve payloads to
a speciﬁc website, we consider that website to be non-
vulnerable.
Overall, we observe that 339 sites suffer from at
USENIX Association
27th USENIX Security Symposium    369
 50 100 150 200 250 300 350 400 450 500 0 50 100 150 200 250 300 350 400 450Response time (ms)Matching time (ms)Exploit
fresh
forwarded
ua-parser-js
useragent
mobile-detect
platform
charset
content
Affected sites
241
99
41
16
9
8
3
0
Figure 12: Number of websites affected by speciﬁc vul-
nerabilities.
4.7 Prevalence of Speciﬁc Vulnerabilities
Figure 12 shows the number of websites affected by each
vulnerability. Perhaps unsurprisingly, the vulnerabilities
in fresh and forwarded have most impact, since these
two modules are part of the Express framework. One
of them needs to be activated using a conﬁguration op-
tion, while the other module is enabled by default. One
may ask why not all Express analyzed websites suffer
from this problem. The reason is the way we dimension
our payloads: Many Express instances limit the header
size, and hence we cannot send large enough payloads
to conﬁrm that the sites are vulnerable. The other six
vulnerabilities affect websites with a frequency that is
roughly proportional to the popularity of the respective
modules. For example, the vulnerability in the popular
useragent affects more websites than the vulnerabil-
ity in the less used charset module. To our initial sur-
prise, we cannot conﬁrm any site vulnerable due to the
content module. After more careful consideration, we
realized that there are two more popular alternatives for
parsing the Content-Header and the content package
seems to be more popular among users of the hapi.js
framework, which is a competitor of Express.
From an attacker’s perspective, the distribution of vul-
nerabilities is great news, because exploits are portable
across websites and knowing a vulnerabilities is sufﬁ-
cient to attack various websites. Likewise, the distribu-
tion is also good news for the community, showing that
one can lower the risk of ReDoS in multiple websites by
ﬁxing a relatively small set of popular packages.
Inﬂuence of Popularity
4.8
Are ReDoS vulnerabilities a problem of less popular
sites? In Figure 13, we show how the vulnerable sites
are distributed across the Alexa top one million sites.
For each point p on the horizontal axis, the vertical axis
shows the number of exploitable sites with popularity
rank ≤ p. For example, there are 61 vulnerable sites
in the top 100,000 websites, with one site in top 1,000
and nine in top 10,000. As can be observed from the
distribution, the vulnerabilities are roughly equally dis-
tributed among the top one million sites. There is even
(a) Response time for an vulnerable site.
(b) Response time for a non-vulnerable site.
Figure 11: Effect of increasing payload sizes on the re-
sponse time of two websites.
least one of the eight vulnerabilities.
66 sites actu-
ally suffer from two vulnerabilities and six sites even
from three. This result shows that ReDoS attacks are
a widespread problem that affects a large number of
real-world websites. Given that our methodology is de-
signed to underestimate the number of affected sites,
e.g., because we consider only eight exploits, the actual
number of ReDoS-vulnerable sites is likely to be even
higher. Moreover, we expect the growing popularity of
JavaScript on the server side to further increase the prob-
lem in the future.
To illustrate our methodology for deciding whether a
In
site is vulnerable, consider two example websites.
Figure 11, we plot for each of the ﬁve payload sizes the
response time for malicious and random inputs. The ﬁg-
ure shows the mean and the conﬁdence intervals for a
vulnerable site in Figure 11a and for a non-vulnerable
site in Figure 11b. The response time grows signiﬁ-
cantly faster for the malicious payloads in the vulnera-
ble site, reaching slightly more than two seconds for the
ﬁfth payload. In contrast, for the non-vulnerable site, the
response time for both malicious and random payloads
seems to grow linearly. Since the conﬁdence interval for
the response times in Figure 11b overlap, we classify this
website as non-vulnerable. By inspecting other websites
classiﬁed as vulnerable by our methodology, we observe
patterns similar to Figure 11a. Therefore, we conclude
that our criteria for deciding if a website is vulnerable
are valid.
370    27th USENIX Security Symposium
USENIX Association
 0 500 1000 1500 2000 2500P1P2P3P4P5Response time (ms)Payload numberRandomMalicious 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800P1P2P3P4P5Response time (ms)Payload numberRandomMaliciousonly, the number of websites that accept larger payloads
decreases over time. This is surprising since for other ex-
ploits like mobile-detect there seem to be more web-
sites to accept 10,000 characters long headers. We be-
lieve this observation to be due to the fact that some
websites refuse to process many requests from the same
user in a short period of time. For instance, our largest
payload is sent after approximately 50 other requests of
smaller size and the site refuses to serve it. This is a well
known network-level protection against DoS, but there
seem to be only around 200 websites to implement it.
However, limiting the number of requests is no silver bul-
let against denial of service attacks, especially when the
attacker has the resources to deploy a distributed denial
of service attack.
4.10 Threats to Validity
One threat to validity for our study is that we rely on time
measurements performed over the network to estimate
the likelihood of a ReDoS vulnerability. One may argue
that these measurements should not be trusted and that
pure chance made us observe some larger slowdowns
for malicious payloads. We address this threat in mul-
tiple ways: we show that for commercial web hosting
servers there is a high correlation between response time
and server CPU time, we repeat measurements multiple
times, and we draw conclusions only from statistically
signiﬁcant differences.
Another potential concern is that the exploits we cre-
ated are too generic and happen to cause slowdown in
another regular expression than the one we created them
for. We believe that this situation would only impact our
ability to tell which module is used on the server-side and
not the impact of a ReDoS attack. Moreover, ﬁve of our
exploits rely on a speciﬁc sequence of characters in the
payload to the effective. These sequences of highly con-
textual characters need to be present in the beginning or
at the end of the exploit. Removing any of them would
make the exploit unusable. Therefore, we believe that
at least for these vulnerabilities it is very likely that our
exploits indeed trigger the intended regular expression.
5 Discussion
In this section, we discuss the potential of a large-scale
DoS attack on Node.js websites and some defenses we
recommend to minimize the impact of such an event.
Finally, we describe an unexpected implication of our
study: that algorithmic complexity attacks can be used
for software ﬁngerprinting.
Figure 13: Cumulative distribution function showing the
popularity of vulnerable sites. Each point on the graph
shows how many sites among the top x sites suffer from
at least one vulnerability.
Figure 14: Number of websites that accept a payload of
a speciﬁc size. Note the logarithmic x-scale.
a slight tendency toward more vulnerabilities among the
more popular websites. This tendency can be explained
by the trend we have seen in Figure 4, that server-side
JavaScript tends to be more popular among popular web-
sites. Overall, we can conclude that ReDoS vulnerabili-
ties are a general problem that affects sites independent
of their popularity ranking.
4.9 Use of Mitigation Techniques
As mentioned before, some websites refuse to process a
request whose header size exceeds a certain size. In Fig-
ure 14 we plot for each exploit how many websites accept
a payload of a given size. As can be observed, most web-
sites accept headers that are smaller than 10,000 charac-
ters, but only few websites accept headers that are, for
instance, 40,000 characters long. As we have shown in
Section 4.3, 10,000 characters are enough to do harm
even with the least serious vulnerability. Therefore, the
current limits that the websites apply on the header size
are insufﬁcient and they do not provide adequate protec-
tion against DoS.
Another interesting trend to observe in Figure 14 is
that even for the most harmful exploit, useragent, for
which we require payloads between 38 and 42 characters
USENIX Association
27th USENIX Security Symposium    371
 0 50 100 150 200 250 300 350100,000200,000300,000400,000500,000600,000700,000800,000900,000Number of vulnerable websites Popularity ranking 0 500 1000 1500 2000 2500 3000 10 100 1000 10000 100000Number of websitesHeader sizefreshforwardedua-parser-jsuseragentmobile-detectplatformcharsetcontentImpact of a Large-scale Attack
5.1
Compared to a regular DoS attack, a ReDoS vulnerabil-
ity enables an attacker to launch an attack with fewer re-
sources. As shown in Section 4.3, even the least harmful
vulnerabilities we identify can be a lethal weapon when
used as part of a large-scale DoS attack, because the at-
tacker can send payloads that hang the loop for hundreds
of milliseconds, several seconds, or even more, depend-
ing on the vulnerability. We remind the reader that with
just eight standard attack vectors we could affect hun-
dreds of websites.
It is worth emphasizing once again that this issue
would not be as serious in a traditional thread-based
web server, such as Apache. This is because the match-
ing would be done in a thread serving the individual
client. In contract, in an event-based system, the match-
ing is done in the main loop and spending a few seconds
matching a regular expression is equivalent to completely
blocking the server for this amount of time.
A large-scale ReDoS attack against Node.js-based
sites is a bleak scenario for which, as we have shown,
many websites are not prepared. To limit this risk, we
have been working with the maintainers of vulnerable
modules to ﬁx vulnerabilities. In addition, we urgently
call for the adoption of multiple layers of defense, as out-
lined in the following.
5.2 Defenses
First of all, to limit the effect of a payload delivered
through an HTTP header, the size of the header should
be limited. For more than 15% sites, we could success-
fully deliver headers longer than 25,000 characters. We
are not aware of any benign use cases for such large
HTTP headers. Therefore, a best practice in Node.js ap-
plications should be to limit the size of request headers.
This kind of defense would mitigate the effects of some
potential attacks, but is limited to vulnerabilities related
to HTTP headers. In contrast, vulnerabilities related to
other inputs received from the network, e.g., the body of
an HTTP request, would remain exploitable.
Another defense mechanism could be to use a more
sophisticated regular expression engine that guarantees
linear matching time. The problem is that these en-
gines do not support advanced regular expression fea-
tures, such as look-ahead or back-references. Davis et
al. [11] advocate for a hybrid solution that only calls
the backtracking engine when such advanced features are
used, and to use a linear time algorithm in all other cases.
This is an elegant solution that is already adopted by lan-
guages like Rust9. However, it would not completely
solve the problem, since some regular expressions with
9https://github.com/rust-lang/regex
advanced features may still contain ReDoS vulnerabil-
ities. For instance, during our vulnerability study, we
found the following regular expression:
/ (?= . *\ b A n d r o i d \ b )(?= . *\ bMobile \ b ) / i
This expression from the ismobilejs module contains
both lookahead and has super-linear complexity in a
backtracking engine.
We also recommend that Node.js augments its regu-
lar expression APIs with an additional, optional time-
out parameter. Node.js will stop any matching of regular
expressions that takes longer than the speciﬁed timeout.
This solution is far from perfect, but it is relatively easy
to implement and adopt, has been successfully deployed
in other programming languages [25], and may also be
feasible for Node.js [14].
Additionally, we advocate that our work should be
used as a roadmap for penetration testing sessions per-
formed on Node.js websites. First, the tester audits the
list of package dependencies, identiﬁes any known Re-
DoS vulnerability in these packages or analyzes all the
contained regular expressions. Second, the tester creates
payloads for all the vulnerable regular expressions iden-
tiﬁed in the ﬁrst step. Third, the tester tries to deliver
these payloads using standard HTTP requests.
Finally, better tools and techniques should be created
to help developers reason about ReDoS vulnerabilities in
server-side JavaScript. Both static and dynamic analysis
tools can aid in understanding the complexity of regular
expressions and their performance. A good starting point
could be porting existing solutions that were created for
other languages, e.g. [43].
5.3 Fingerprinting Web Servers
Part of our methodology could be used to ﬁngerprint web
servers to predict some of the third-party modules used
by a website. This ability can be useful for an attacker in
at least two ways. First, the attacker may try to temper
with the development process of that module by intro-
ducing backdoors that can then be exploited in the live
website. Given that npm modules often depend on sev-
eral others, the vulnerability can even be hidden in a
dependent module. Second, the attacker may exploit a
more serious vulnerability present in the same module.
To show how this scenario may happen, consider the
dns-sync vulnerability, identiﬁed in Section 4.1. The