Card(D)
IG(P) =
p∑
i=1
Card(Di)
Card(D)
log
So the corrected gain is:
CorrG(P) =
G(P)
IG(P)
(2.6)
(2.7)
In this corrected gain, we can see that the denominator increases
with the number of partitions which is the expected behaviour.
2.2.3 Regression trees
The case of regression is similar to classiﬁcation, the only thing to
change is the formula of the gain. The main idea is to minimize the
variance in each leaf. This corresponds to minimizing the mean square
error at each split. Using the same notations the gain becomes:
G(P) = ∑
y∈D
( ˆy − y)2 − p∑
i=1
Card(Di)
Card(D)
∑
y∈Di
( ˆyi − y)2
This minimizes the variance in each leaf because the prediction at each
level is:
Card(D)∑
yj
j=1
ˆy =
1
Card(D)
ˆyi =
1
Card(Di)
Card(Di)∑
j=1
yj
CHAPTER 2. BACKGROUND
9
which leads to the exact deﬁnition of the variance of D and Di in the
gain. The prediction at each level is given by the mean of all sam-
ples. So, decision trees that uses this method for prediction in their
leaf only produce step functions. This can be overcome by adding a
simple logistic regression or a naive Bayes [14] [15] on each leaf to be
more accurate.
The algorithm described above is mainly the starting point of all re-
search around decision trees. The ﬁrst problem I tried to address was
to better handle categorical values. With this algorithm, when mak-
ing the decision to split on a categorical attribute, it creates as many
branches as the number of values of this attribute. This leads to larger
trees, that makes them harder to read by a human. This drop is the
issue I want to ﬁrst tackle.
2.3 Overview of categorical split methods
As seen in the previous section the split on categorical attributes in not
really optimal is the sense that it produces one branch per category
regardless of the output, so it often produces branches with few sam-
ples which are often useless. Our idea was to reduce the number of
branches when encountering a categorical feature by making different
groups of values. Nevertheless, it is really hard to ﬁnd a simple way to
group categories. Categorical splits differ from real-valued one since
there is no order in it, so we cannot apply the same strategy. Further-
more testing all possible partitions is computationally hard because
the number of possibilities quickly becomes intractable. An attribute
with p different values leads to Bp partitions where Bp is the pth num-
ber of Bell [16] deﬁned by the recursive relation:
One can show that:
∀n ∈ N,
(p − 1)! ≤ Bp ≤ p!
(cid:18) i
(cid:19)
Bi
p
p∑
i=0
Bp+1 =
(cid:113)
B0 = 1
(2.8)
(2.9)
10
CHAPTER 2. BACKGROUND
We will see in this section four common techniques currently used to
improve this particular point and avoiding any explosion of complex-
ity. Table 2.1 presents an example of a dataset that we will use to illus-
trate the various splitting techniques.
Index
1
2
3
4
5
6
7
8
9
10
Animal
Disease
Dog
Dog
Dog
Cat
Cat
Frog
Frog
Frog
Salamander
Salamander
Yes
Yes
No
Yes
Yes
No
No
Yes
No
No
Table 2.1: Example dataset (each row represents one sample)
2.3.1 One hot encoding or One Versus All
The idea is to turn each categorical feature into dummy features. This
means that each categorical feature is transformed into a list of k fea-
tures where k is the number of different values of this particular fea-
ture. Each new feature correspond to the question: is this categorical
value equal to a given value. Even if it does not compute all possible
partitions of categorical values this method is widely used, for instance
in open source library like scikit-learn [10] or caret [11]. If we take the
previous example then we obtain the dataset represented in table 2.2.
Once this preprocessing is done, the idea is to consider each new
attribute as a continuous one and apply the continuous method on
them. This method is also called one versus all, since this encoding
creates splits that isolate one attribute against the others. In this simple
example, we can see that it fails to capture the the best split which is
(Dog, Cat), (Frog, Salamander). This split is the best because it is the
one which leads to the lowest error rate on prediction. The algorithm
will need two steps to capture this subtlety. Nevertheless, this solution
has the advantage of being easily implementable and of having a low
CHAPTER 2. BACKGROUND
11
Index Is Dog Is Cat
Is Frog Is Salamander Disease
1
2
3
4
5
6
7
8
9
10
1
1
1
0
0
0
0
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0
0
0
1
1
1
0
0
0
0
0
0
0
0
0
0
1
1
Yes
Yes
No
Yes
Yes
No
No
Yes
No
No
Table 2.2:
(each line represent one datapoint)
Toy dataset
transformed into dummy variables
complexity. Only p splits are tested where p is the number different
values of the categorical attribute.
Furthermore, an interesting feature with categorical features is to
groups the different values. We will now see two other methods which
can create this kind of group.
2.3.2 Push Left By Purity
The push left by purity method aims to reduce the number of tested
splits compared to the brute-force method while being a good heuristic
to determine the best split. In this case, the idea is to start with all the
samples on the "right", and start passing each attribute to the left one
by one. The criterion to pass an attribute to the left it is that it produces
the best split. If we take the previous example 2.1 there are 4 possible
splits at the ﬁrst step 2.3. The table 2.3 gives the four possible splits at
the ﬁrst step with the example:
In the case when two splits are in a tie, a random one is selected.
Therefore, in this example the best split is split 2. Now, the best ob-
tained split is stored together with the corresponding gain and the op-
eration is repeated. This leads to the following splits.
Then the algorithm stops because passing another value to the left
will give the exact same partitions as in the ﬁrst step. The best split is
returned and selected, here split 2 in step 2. The global algorithm is
12
CHAPTER 2. BACKGROUND
Split 1
Split 2
Split 3
Split 4
Attribute right Cat, Dog, Fro
Attribute left
Index right
Index left
(1,2,3,4,5,6,7,8)
Sal
(9,10)
Gain
Best split
0
No
Cat, Dog, Sal
Cat, Fro, Sal
Dog, Fro, Sal
Fro
Dog
Cat
(1,2,3,4,5,9,10)
(1,2,3,6,7,8,9,10)
(4,5,6,7,8,9,10)
(6, 7, 8)
1
Yes
(4, 5)
0
No
(1, 2, 3)
0
No
Table 2.3: Possible split in step 1 of the push left by purity algorithm
(Fro stands for Frog and Sal for Salamander)
Attribute right
Attribute left
Index right
Index left
Gain
Best split
Split 1
Cat, Dog
Sal, Fro
(1,2,3,4,5)
(6, 7, 8, 9, 10)
Split 2
Cat, Frog
Sal, Dog
(1,2,3,6,7,8)
(4, 5, 9, 10)
Split 3
Dog, Frog
Sal, Cat
(3,4,5,6,7,8)
(1, 2, 3, 9, 10)
0
No
1
Yes
0
No
Table 2.4: Possible splits in step 2 of the push left by purity algorithm
(Fro stands for Frog and Sal for Salamander)
described in (Algorithm 1).
This algorithm is valuable because it was one of the earliest to pro-
vide a way to group several values of a categorical feature. This prop-
erty is interesting in term of explainability since it can capture relations
or correlations between different categorical values. It also reduces by
a lot the number of nodes in a decision tree based on a dataset with
many categorical values. According to the litterature, this algorithm
provides a good heuristic for the binary categorical split. It is proven
that this method ﬁnds the best result for a binary classiﬁcation task.
However, no bounds have been found on the optimality for a general
classiﬁcation problem. The push left by purity method tests p2 dif-
ferent splits which makes it longer to execute than the one versus all
strategy.
CHAPTER 2. BACKGROUND
13
2.3.3 Majority class
The majority class in a dataset is the class which contains the largest
number of samples. The idea, here, is to reduce again the number
of tested split. Facing the algorithm seen in the previous subsection,
one can think about how to pass a value from the right partition to
the left partition while testing less splits. One simple answer could
be to look at the probability to be in the class which has the largest
number of samples given a speciﬁc value of an attribute. This lead to
the algorithm described in Algorithm 2.
We will not show in this section how it works with the example
because this algorithm is almost identical to the push left by purity
algorithm.
In the edge case when two classes have the same num-
ber of occurrences, a random class could be chosen or the class with
the biggest variance. In practice, the second method gives better re-
sults. In term of complexity, only p splits are tested, computing the
majority class probability is quick and already necessary to compute
the entropy anyway so it does not add much time. This method has
the same advantage as the push left by purity method that is to say
that it provides groups of values for categorical features. If the prob-
lem is a binary classiﬁcation this algorithm is strictly equivalent to the
push left by purity algorithm.
2.3.4 Covariance split
The last method we will see in this subsection is based on the same
principle as the Principal Component Analysis [17] and could be seen
as an extension of the majority class algorithm. It was developed by
Don Coppersmith and released in 1999 [18]. The idea is to group the
different values which are close in term of distribution. Formally, let
{ai}i∈[1,p] be the set of values of a categorical attribute, {ci}i∈[1,m] the
set of class and D = {(Xi, yi)}i∈[1,N] the dataset. The ﬁrst step is to
(cid:19)
compute the global probability distribution in the whole dataset:
Card((Xi, yi) ∈ D|yi = m)
πglobal =
N
(cid:18)Card((Xi, yi) ∈ D|yi = 1
(cid:32)Card((Xi, yi) ∈ D|Xi = aj, yi = 1
, ...,
N
Nj
πaj =
And then compute the probability distribution for each value of the
categorical feature:
Card((Xi, yi) ∈ D|Xi = aj, yi = m)
, ...,
Nj
(cid:33)
14
CHAPTER 2. BACKGROUND
Where Nk = Card((Xi, yi) ∈ D|Xi = ak). Now that all the probabil-
ity distributions are computed, the global covariance matrice is com-
puted.
C =
m∑
i=1
(πai − πglobal)T(πai − πglobal)
1
Ni
Let (λbest, ebest) be the largest eigenvalue of matrix C and its associated
eigenvector. The idea is to compute a score for each attribute ai which
corresponds to the coordinate of πai ) on the e direction:
si =
The last step is to order the attribute in the ascending order according
to the (si)i∈[1,m] and test all partitions with the shape:
Pk = ({a1, ...ak},{ak+1, ...ap}), k ∈ [1, m − 1]
This algorithm returns the best partition over all the Pk. The global
algorithm is summarized in Algorithm 3. Applying this algorithm on
the toy example described in 2.1 leads to the results described in 2.5.
π(Yes) π(No) Sample count
Cat
Dog
Frog
Salamander
Total
1
2/3
1/3
0
1/2
0
1/3
2/3
1
1/2
2
3
3
2
10
Table 2.5: Possible splits in step 2 of the push left by purity algorithm
This lead to the covariance matrix described in 2.3.4:
(cid:19)
(cid:18) 2
3 − 2
− 2
3
2
3
3
M =
, λ ∈ {1, 0}, e ∈
(cid:26)(cid:18) 1−1
(cid:19)