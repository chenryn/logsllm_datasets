以下是优化后的参考文献列表，使其更加清晰、连贯和专业：

1. Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. 2019. Certified Adversarial Robustness via Randomized Smoothing. In *International Conference on Machine Learning* (ICML). PMLR, 1310–1320.

2. Francesco Croce and Matthias Hein. 2020. Minimally Distorted Adversarial Examples with a Fast Adaptive Boundary Attack. In *International Conference on Machine Learning* (ICML). PMLR, 2196–2205.

3. Guneet S Dhillon, Kamyar Azizzadenesheli, Zachary C Lipton, Jeremy Bernstein, Jean Kossai, Aran Khanna, and Anima Anandkumar. 2018. Stochastic Activation Pruning for Robust Adversarial Defense. (2018).

4. Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li. 2018. Boosting Adversarial Attacks with Momentum. In *CVPR*.

5. Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu. 2019. Evading Defenses to Transferable Adversarial Examples by Translation-Invariant Attacks. In *CVPR*.

6. Martin Ester, Hans-Peter Kriegel, Jörg Sander, and Xiaowei Xu. 1996. A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise. In *KDD*, 226–231.

7. Reuben Feinman, Ryan R Curtin, Saurabh Shintre, and Andrew B Gardner. 2017. Detecting Adversarial Samples from Artifacts. *arXiv* preprint arXiv:1703.00410 (2017).

8. Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and Harnessing Adversarial Examples. In *3rd International Conference on Learning Representations* (ICLR 2015), San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings. http://arxiv.org/abs/1412.6572

9. Kathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes, and Patrick McDaniel. 2017. On the (Statistical) Detection of Adversarial Examples. *arXiv* preprint arXiv:1702.06280 (2017).

10. Shixiang Gu and Luca Rigazio. 2015. Towards Deep Neural Network Architectures Robust to Adversarial Examples. In *International Conference on Learning Representations* (ICLR) Workshop.

11. Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. 2019. BadNets: Evaluating Backdooring Attacks on Deep Neural Networks. *IEEE Access* 7 (2019), 47230–47244. https://doi.org/10.1109/ACCESS.2019.2909068

12. Chuan Guo, Mayank Rana, Moustapha Cisse, and Laurens Van Der Maaten. 2018. Countering Adversarial Images Using Input Transformations. (2018).

13. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for Image Recognition. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition* (CVPR). 770–778.

14. Warren He, James Wei, Xinyun Chen, Nicholas Carlini, and Dawn Song. 2017. Adversarial Example Defense: Ensembles of Weak Defenses Are Not Strong. In *11th USENIX Workshop on Offensive Technologies* (WOOT 17).

15. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the Knowledge in a Neural Network. *arXiv* preprint arXiv:1503.02531 (2015).

16. Ruitong Huang, Bing Xu, Dale Schuurmans, and Csaba Szepesvári. 2016. Learning with a Strong Adversary. In *International Conference on Learning Representations* (ICLR), 2016.

17. Nathan Inkawhich, Kevin J Liang, Lawrence Carin, and Yiran Chen. 2020. Transferable Perturbations of Deep Feature Distributions. In *International Conference on Learning Representations* (ICLR 2020).

18. Nathan Inkawhich, Kevin J Liang, Binghui Wang, Matthew Inkawhich, Lawrence Carin, and Yiran Chen. 2020. Perturbing Across the Feature Hierarchy to Improve Standard and Strict Blackbox Attack Transferability. *arXiv* preprint arXiv:2004.14861 (2020).

19. Nathan Inkawhich, Wei Wen, Hai Helen Li, and Yiran Chen. 2019. Feature Space Perturbations Yield More Transferable Adversarial Examples. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition* (CVPR). 7066–7074.

20. Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning Multiple Layers of Features from Tiny Images. (2009).

21. Alexey Kurakin, Ian Goodfellow, and Samy Bengio. 2017. Adversarial Examples in the Physical World. In *5th International Conference on Learning Representations* (ICLR 2017), Workshop track.

22. Alexey Kurakin, Ian Goodfellow, and Samy Bengio. 2017. Adversarial Machine Learning at Scale. In *5th International Conference on Learning Representations* (ICLR 2017).

23. Yann LeCun, Lawrence D Jackel, Léon Bottou, Corinna Cortes, John S Denker, Harris Drucker, Isabelle Guyon, Urs A Muller, Eduard Sackinger, Patrice Simard, et al. 1995. Learning Algorithms for Classification: A Comparison on Handwritten Digit Recognition. *Neural Networks: The Statistical Mechanics Perspective* (1995), 261–276.

24. Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. 2019. Certified Robustness to Adversarial Examples with Differential Privacy. In *2019 IEEE Symposium on Security and Privacy* (SP). IEEE, 656–672.

25. Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. 2019. Certified Adversarial Robustness with Additive Noise. In *NeurIPS 2019*.

26. Jiajun Lu, Theerasit Issaranon, and David Forsyth. 2017. SafetyNet: Detecting and Rejecting Adversarial Examples Robustly. In *Proceedings of the IEEE International Conference on Computer Vision* (ICCV). 446–454.

27. Shiqing Ma and Yingqi Liu. 2019. NIC: Detecting Adversarial Samples with Neural Network Invariant Checking. In *Proceedings of the 26th Network and Distributed System Security Symposium* (NDSS 2019).

28. Xingjun Ma, Bo Li, Yisen Wang, Sarah M Erfani, Sudanthi Wijewickrema, Grant Schoenebeck, Dawn Song, Michael E Houle, and James Bailey. 2018. Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality. (2018).

29. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. 2018. Towards Deep Learning Models Resistant to Adversarial Attacks. In *6th International Conference on Learning Representations* (ICLR 2018), Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net. https://openreview.net/forum?id=rJzIBfZAb

30. Dongyu Meng and Hao Chen. 2017. MagNet: A Two-Pronged Defense against Adversarial Examples. In *Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security* (CCS '17). Association for Computing Machinery, New York, NY, USA, 135–147. https://doi.org/10.1145/3133956.3134057

31. Jeet Mohapatra, Ching-Yun Ko, Sijia Liu, Pin-Yu Chen, Luca Daniel, et al. 2020. Rethinking Randomized Smoothing for Adversarial Robustness. *arXiv* preprint arXiv:2003.01249 (2020).

32. Nina Narodytska and Shiva Prasad Kasiviswanathan. 2017. Simple Black-Box Adversarial Perturbations for Deep Networks. In *CVPR Workshops*.

33. Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. 2017. Practical Black-Box Attacks against Machine Learning. In *Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security*. 506–519.

34. Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram Swami. 2016. The Limitations of Deep Learning in Adversarial Settings. In *2016 IEEE European Symposium on Security and Privacy* (EuroS&P). IEEE, 372–387.

35. Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. 2016. Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks. In *2016 IEEE Symposium on Security and Privacy* (SP). IEEE, 582–597.

36. Pouya Samangouei, Maya Kabkab, and Rama Chellappa. 2018. Defense-GAN: Protecting Classifiers against Adversarial Attacks using Generative Models. (2018).

37. Uri Shaham, Yutaro Yamada, and Sahand Negahban. 2018. Understanding Adversarial Training: Increasing Local Stability of Supervised Models through Robust Optimization. *Neurocomputing* 307 (2018), 195–204.

38. Shawn Shan. 2021. Gotta Catch’Em All: Using Honeypots to Catch Adversarial Attacks on Neural Networks. https://github.com/Shawn-Shan/trapdoor, note = Accessed on May 1st, 2021. (2021).

39. Shawn Shan, Emily Wenger, Bolun Wang, Bo Li, Haitao Zheng, and Ben Y. Zhao. 2020. Gotta Catch’Em All: Using Honeypots to Catch Adversarial Attacks on Neural Networks. In *Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security* (CCS '20). Association for Computing Machinery, New York, NY, USA, 67–83. https://doi.org/10.1145/3372297.3417231

40. Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K Reiter. 2016. Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition. In *Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security*. 1528–1540.

41. Yang Song, Taesup Kim, Sebastian Nowozin, Stefano Ermon, and Nate Kushman. 2018. PixelDefend: Leveraging Generative Models to Understand and Defend Against Adversarial Examples. (2018).

42. Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. 2012. Man vs. Computer: Benchmarking Machine Learning Algorithms for Traffic Sign Recognition. *Neural Networks* 32 (2012), 323–332.

43. Jiawei Su, Danilo Vasconcellos Vargas, and Kouichi Sakurai. 2019. One Pixel Attack for Fooling Deep Neural Networks. *IEEE Transactions on Evolutionary Computation* 23, 5 (2019), 828–841.

44. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. 2014. Intriguing Properties of Neural Networks. *2nd International Conference on Learning Representations* (ICLR) 2014.

45. Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. 2017. Ensemble Adversarial Training: Attacks and Defenses. *arXiv* preprint arXiv:1705.07204 (2017).

46. Jonathan Uesato, Brendan O’Donoghue, Pushmeet Kohli, and Aäron van den Oord. 2018. Adversarial Risk and the Dangers of Evaluating Against Weak Attacks. In *Proceedings of the 35th International Conference on Machine Learning* (ICML 2018), Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018 (Proceedings of Machine Learning Research), Jennifer G. Dy and Andreas Krause (Eds.), Vol. 80. PMLR, 5032–5041. http://proceedings.mlr.press/v80/uesato18a.html

47. Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y. Zhao. 2019. Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks. In *2019 IEEE Symposium on Security and Privacy* (SP 2019), San Francisco, CA, USA, May 19-23, 2019. IEEE, 707–723. https://doi.org/10.1109/SP.2019.00031

48. Lior Wolf, Tal Hassner, and Itay Maoz. 2011. Face Recognition in Unconstrained Videos with Matched Background Similarity. In *CVPR 2011*. IEEE, 529–534.

49. Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan Yuille. 2018. Mitigating Adversarial Effects through Randomization. (2018).

50. Cihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, Jianyu Wang, Zhou Ren, and Alan L Yuille. 2019. Improving Transferability of Adversarial Examples with Input Diversity. In *CVPR*.

51. Weilin Xu, David Evans, and Yanjun Qi. 2018. Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks. In *25th Annual Network and Distributed System Security Symposium* (NDSS 2018), San Diego, California, USA, February 18-21, 2018. The Internet Society. http://wp.internetsociety.org/ndss/wp-content/uploads/sites/25/2018/02/ndss2018_03A-4_Xu_paper.pdf

52. Valentina Zantedeschi, Maria-Irina Nicolae, and Ambrish Rawat. 2017. Efficient Defenses Against Adversarial Attacks. In *Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security*. 39–49.

53. Stephan Zheng, Yang Song, Thomas Leung, and Ian Goodfellow. 2016. Improving the Robustness of Deep Neural Networks via Stability Training. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition* (CVPR). 4480–4488.

**附录**

**A. 数据集和网络**

在我们的实验评估中使用的数据集和神经网络列于表11。用于MNIST、GTSRB和CIFAR10的网络详细信息分别列于表12、表13和表14。YouTube Face使用的网络是标准的ResNet50 [25]。

**表11：在我们的评估中使用的数据集和网络**

| 任务 | 数据集 | 模型架构 |
| --- | --- | --- |
| 手写数字识别 | MNIST | 2个卷积层，2个全连接层 |
| 交通标志识别 | GTSRB | 6个卷积层，2个全连接层 |
| 图像分类 | CIFAR10 | ResNet20 |
| 人脸识别 | YouTube Face | ResNet50 |

**表12：MNIST的模型架构。Size表示卷积层（核大小 × 滤波器数量）或最大池化层（下采样大小）或全连接层（隐藏神经元数量）。**

| 层（类型） | Size | 激活函数 |
| --- | --- | --- |
| conv_1 (Conv2D) | 5 × 5 × 16 | ReLU |
| max_pool_1 (MaxPooling2D) | 2 × 2 | - |
| conv_2 (Conv2D) | 5 × 5 × 32 | ReLU |
| max_pool_2 (MaxPooling2D) | 2 × 2 | - |
| dense (Dense) | 512 | ReLU |
| dense_1 (Dense) | 10 | Softmax |

**表13：GTSRB的模型架构。Size表示卷积层（核大小 × 滤波器数量）或最大池化层（下采样大小）或全连接层（隐藏神经元数量）。**

| 层（类型） | Size | 激活函数 |
| --- | --- | --- |
| conv_1 (Conv2D) | 3 × 3 × 32 | ReLU |
| conv_2 (Conv2D) | 3 × 3 × 32 | ReLU |
| conv_3 (Conv2D) | 3 × 3 × 64 | ReLU |
| conv_4 (Conv2D) | 3 × 3 × 64 | ReLU |
| conv_5 (Conv2D) | 3 × 3 × 128 | ReLU |
| conv_6 (Conv2D) | 3 × 3 × 128 | ReLU |
| max_pool_1 (MaxPooling2D) | 2 × 2 | - |
| max_pool_2 (MaxPooling2D) | 2 × 2 | - |
| max_pool_3 (MaxPooling2D) | 2 × 2 | - |
| dense_1 (Dense) | 512 | ReLU |
| dense_2 (Dense) | 43 | Softmax |

**A.2 TeD中的后门设置**

在我们的实验评估中，后门注入的参数设置与[51]相同。对于除YouTube Face以外的所有数据集，当仅向DNN模型注入一个后门时，后门触发器是一个6 × 6的图案，放置在图像的右下角，后门注入比例设置为0.1，即后门模型用受污染的数据训练，这些数据占总训练数据的10%。当向DNN模型注入多个后门时，后门触发器由五个3 × 3的小块组成，随机分布在整张图像上，后门注入比例设置为0.5。在这两种情况下，合并透明度都设置为0.1，这意味着后门触发器的强度为10%。