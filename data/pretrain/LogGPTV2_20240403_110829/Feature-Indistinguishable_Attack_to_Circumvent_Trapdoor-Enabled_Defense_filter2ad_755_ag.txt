[13] Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. 2019. Certied adversarial
robustness via randomized smoothing. In International Conference on Machine
Learning. PMLR, 1310–1320.
[14] Francesco Croce and Matthias Hein. 2020. Minimally distorted adversarial exam-
ples with a fast adaptive boundary attack. In International Conference on Machine
Learning. PMLR, 2196–2205.
[15] Guneet S Dhillon, Kamyar Azizzadenesheli, Zachary C Lipton, Jeremy Bernstein,
Jean Kossai, Aran Khanna, and Anima Anandkumar. 2018. Stochastic activation
pruning for robust adversarial defense. (2018).
[16] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and
Jianguo Li. 2018. Boosting adversarial attacks with momentum. In CVPR.
[17] Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu. 2019. Evading Defenses to
Transferable Adversarial Examples by Translation-Invariant Attacks. In CVPR.
[18] Martin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu, et al. 1996. A density-
based algorithm for discovering clusters in large spatial databases with noise.. In
kdd, Vol. 96. 226–231.
[19] Reuben Feinman, Ryan R Curtin, Saurabh Shintre, and Andrew B Gardner. 2017.
Detecting adversarial samples from artifacts. arXiv preprint arXiv:1703.00410
(2017).
[20] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and
Harnessing Adversarial Examples. In 3rd International Conference on Learning
Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track
Proceedings, Yoshua Bengio and Yann LeCun (Eds.). http://arxiv.org/abs/1412.6572
[21] Kathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes, and
Patrick McDaniel. 2017. On the (statistical) detection of adversarial examples.
arXiv preprint arXiv:1702.06280 (2017).
[22] Shixiang Gu and Luca Rigazio. 2015. Towards deep neural network architec-
tures robust to adversarial examples. In International Conference on Learning
Representations, (ICLR) Workshop.
[23] Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. 2019. BadNets:
Evaluating Backdooring Attacks on Deep Neural Networks. IEEE Access 7 (2019),
47230–47244. https://doi.org/10.1109/ACCESS.2019.2909068
[24] Chuan Guo, Mayank Rana, Moustapha Cisse, and Laurens Van Der Maaten. 2018.
Countering adversarial images using input transformations. (2018).
[25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 770–778.
[26] Warren He, James Wei, Xinyun Chen, Nicholas Carlini, and Dawn Song. 2017.
Adversarial example defense: Ensembles of weak defenses are not strong. In 11th
USENIX workshop on oensive technologies (WOOT 17).
[27] Georey Hinton, Oriol Vinyals, and Je Dean. 2015. Distilling the knowledge in
a neural network. arXiv preprint arXiv:1503.02531 (2015).
[28] Ruitong Huang, Bing Xu, Dale Schuurmans, and Csaba Szepesvári. 2016. Learning
with a strong adversary. In International Conference on Learning Representations
(ICLR), 2016.
[29] Nathan Inkawhich, Kevin J Liang, Lawrence Carin, and Yiran Chen. 2020. Trans-
ferable perturbations of deep feature distributions. In International Conference on
Learning Representations, ICLR 2020.
[30] Nathan Inkawhich, Kevin J Liang, Binghui Wang, Matthew Inkawhich, Lawrence
Carin, and Yiran Chen. 2020. Perturbing across the feature hierarchy to improve
standard and strict blackbox attack transferability. arXiv preprint arXiv:2004.14861
(2020).
[31] Nathan Inkawhich, Wei Wen, Hai Helen Li, and Yiran Chen. 2019. Feature space
perturbations yield more transferable adversarial examples. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. 7066–7074.
[32] Alex Krizhevsky, Georey Hinton, et al. 2009. Learning multiple layers of features
from tiny images. (2009).
[33] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. 2017. Adversarial examples
in the physical world. In 5th International Conference on Learning Representations
ICLR 2017, Workshop track.
[34] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. 2017. Adversarial machine
learning at scale. In 5th International Conference on Learning Representations ICLR
2017.
[35] Yann LeCun, Lawrence D Jackel, Léon Bottou, Corinna Cortes, John S Denker,
Harris Drucker, Isabelle Guyon, Urs A Muller, Eduard Sackinger, Patrice Simard,
et al. 1995. Learning algorithms for classication: A comparison on handwritten
digit recognition. Neural networks: the statistical mechanics perspective (1995),
261–276.
[36] Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman
Jana. 2019. Certied robustness to adversarial examples with dierential privacy.
In 2019 IEEE Symposium on Security and Privacy (SP). IEEE, 656–672.
[37] Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. 2019. Certied
adversarial robustness with additive noise. In NeurIPS 2019.
[38] Jiajun Lu, Theerasit Issaranon, and David Forsyth. 2017. Safetynet: Detecting and
rejecting adversarial examples robustly. In Proceedings of the IEEE International
Conference on Computer Vision. 446–454.
[39] Shiqing Ma and Yingqi Liu. 2019. Nic: Detecting adversarial samples with neural
network invariant checking. In Proceedings of the 26th Network and Distributed
System Security Symposium (NDSS 2019).
[40] Xingjun Ma, Bo Li, Yisen Wang, Sarah M Erfani, Sudanthi Wijewickrema, Grant
Schoenebeck, Dawn Song, Michael E Houle, and James Bailey. 2018. Characteriz-
ing adversarial subspaces using local intrinsic dimensionality. (2018).
[41] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu. 2018. Towards Deep Learning Models Resistant to Adversarial
Attacks. In 6th International Conference on Learning Representations, ICLR 2018,
Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings.
OpenReview.net. https://openreview.net/forum?id=rJzIBfZAb
[42] Dongyu Meng and Hao Chen. 2017. MagNet: A Two-Pronged Defense against
Adversarial Examples. In Proceedings of the 2017 ACM SIGSAC Conference on
Computer and Communications Security (CCS ’17). Association for Computing
Machinery, New York, NY, USA, 135–147. https://doi.org/10.1145/3133956.3134
057
[43] Jeet Mohapatra, Ching-Yun Ko, Sijia Liu, Pin-Yu Chen, Luca Daniel, et al. 2020.
Rethinking randomized smoothing for adversarial robustness. arXiv preprint
arXiv:2003.01249 (2020).
[44] Nina Narodytska and Shiva Prasad Kasiviswanathan. 2017. Simple black-box
adversarial perturbations for deep networks. In CVPR Workshops.
[45] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay
Celik, and Ananthram Swami. 2017. Practical black-box attacks against machine
learning. In Proceedings of the 2017 ACM on Asia conference on computer and
communications security. 506–519.
[46] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik,
and Ananthram Swami. 2016. The limitations of deep learning in adversarial
settings. In 2016 IEEE European symposium on security and privacy (EuroS&P).
IEEE, 372–387.
[47] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami.
2016. Distillation as a defense to adversarial perturbations against deep neural
networks. In 2016 IEEE symposium on security and privacy (SP). IEEE, 582–597.
[48] Pouya Samangouei, Maya Kabkab, and Rama Chellappa. 2018. Defense-gan:
Protecting classiers against adversarial attacks using generative models. (2018).
[49] Uri Shaham, Yutaro Yamada, and Sahand Negahban. 2018. Understanding adver-
sarial training: Increasing local stability of supervised models through robust
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3172optimization. Neurocomputing 307 (2018), 195–204.
[50] Shawn Shan. 2021. Gotta Catch’Em All: Using Honeypots to Catch Adversarial
Attacks on Neural Networks. https://github.com/Shawn-Shan/trapdoor, note =
Accessed on May 1st, 2021. (2021).
[51] Shawn Shan, Emily Wenger, Bolun Wang, Bo Li, Haitao Zheng, and Ben Y. Zhao.
2020. Gotta Catch’Em All: Using Honeypots to Catch Adversarial Attacks on
Neural Networks. In Proceedings of the 2020 ACM SIGSAC Conference on Computer
and Communications Security (CCS ’20). Association for Computing Machinery,
New York, NY, USA, 67–83. https://doi.org/10.1145/3372297.3417231
[52] Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K Reiter. 2016. Ac-
cessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition.
In Proceedings of the 2016 acm sigsac conference on computer and communications
security. 1528–1540.
[53] Yang Song, Taesup Kim, Sebastian Nowozin, Stefano Ermon, and Nate Kushman.
2018. Pixeldefend: Leveraging generative models to understand and defend
against adversarial examples. (2018).
[54] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. 2012. Man vs.
computer: Benchmarking machine learning algorithms for trac sign recognition.
Neural networks 32 (2012), 323–332.
[55] Jiawei Su, Danilo Vasconcellos Vargas, and Kouichi Sakurai. 2019. One pixel
IEEE Transactions on Evolutionary
attack for fooling deep neural networks.
Computation 23, 5 (2019), 828–841.
[56] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
Ian Goodfellow, and Rob Fergus. 2014. Intriguing properties of neural networks.
2nd International Conference on Learning Representations (ICLR) 2014.
[57] Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh,
and Patrick McDaniel. 2017. Ensemble adversarial training: Attacks and defenses.
arXiv preprint arXiv:1705.07204 (2017).
[58] Jonathan Uesato, Brendan O’Donoghue, Pushmeet Kohli, and Aäron van den
Oord. 2018. Adversarial Risk and the Dangers of Evaluating Against Weak
Attacks. In Proceedings of the 35th International Conference on Machine Learning,
ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018 (Proceedings
of Machine Learning Research), Jennifer G. Dy and Andreas Krause (Eds.), Vol. 80.
PMLR, 5032–5041. http://proceedings.mlr.press/v80/uesato18a.html
[59] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao
Zheng, and Ben Y. Zhao. 2019. Neural Cleanse: Identifying and Mitigating
Backdoor Attacks in Neural Networks. In 2019 IEEE Symposium on Security
and Privacy, SP 2019, San Francisco, CA, USA, May 19-23, 2019. IEEE, 707–723.
https://doi.org/10.1109/SP.2019.00031
[60] Lior Wolf, Tal Hassner, and Itay Maoz. 2011. Face recognition in unconstrained
videos with matched background similarity. In CVPR 2011. IEEE, 529–534.
[61] Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan Yuille. 2018.
Mitigating adversarial eects through randomization. (2018).
[62] Cihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, Jianyu Wang, Zhou Ren, and
Alan L Yuille. 2019. Improving transferability of adversarial examples with input
diversity. In CVPR.
[63] Weilin Xu, David Evans, and Yanjun Qi. 2018. Feature Squeezing: Detecting
Adversarial Examples in Deep Neural Networks. In 25th Annual Network and
Distributed System Security Symposium, NDSS 2018, San Diego, California, USA,
February 18-21, 2018. The Internet Society. http://wp.internetsociety.org/ndss/wp-
content/uploads/sites/25/2018/02/ndss2018_03A-4_Xu_paper.pdf
[64] Valentina Zantedeschi, Maria-Irina Nicolae, and Ambrish Rawat. 2017. Ecient
defenses against adversarial attacks. In Proceedings of the 10th ACM Workshop on
Articial Intelligence and Security. 39–49.
[65] Stephan Zheng, Yang Song, Thomas Leung, and Ian Goodfellow. 2016. Improving
the robustness of deep neural networks via stability training. In Proceedings of
the ieee conference on computer vision and pattern recognition. 4480–4488.
A APPENDIX
A.1 Datasets and Networks
The datasets and neural networks used in our experimental evalua-
tion are listed in Table 11. The detail of the networks used for MNIST,
GTSRB, and CIFAR10 are listed in Tables 12, 13, and 14, respectively.
The network for YouTube Face is the standard ResNet50 [25]. It is
too long to list here.
A.2 Trapdoor Settings in TeD
In our experimental evaluation, trapdoors are injected with the
same parameter settings as in [51]. For all datasets except YouTube
Face, when only one trapdoor is injected into a DNN model, the
trapdoor trigger is a 6 × 6 pattern placed at the right bottom corner
Table 11: Datasets and networks used in our evaluation.
Task
Handwritten digit recognition
Trac sign recognition
Image classication
Face Recognition
Dataset
MNIST
GTSRB
CIFAR10
YouTube Face
Model Arch.
2 Conv, 2 Dense
6 Conv, 2 Dense
ResNet20
ResNet50
Table 12: The model architecture for MNIST. Size stands for
lter size (kernel size × # of lters) for Conv2D, downsam-
pling size for MaxPooling2D, or # of hidden neurons for
Dense layer.
Layer (type)
conv_1 (Conv2D)
max_pool_1 (MaxPooling2D)
conv_2(Conv2D)
max_pool_2 (MaxPooling2D)
dense (Dense)
dense_1 (Dense)
5 × 5 × 16
5 × 5 × 32
Size
2 × 2
2 × 2
512
10
Activation
ReLU
ReLU
-
-
ReLU
Softmax
Table 13: The model architecture for GTSRB. Size stands for
lter size (kernel size × # of lters) for Conv2D, downsam-
pling size for MaxPooling2D, or # of hidden neurons for
Dense layer.
Layer (type)
conv_1 (Conv2D)
conv_2 (Conv2D)
conv_3 (Conv2D)
conv_4 (Conv2D)
conv_5 (Conv2D)
conv_6 (Conv2D)
dense_1 (Dense)
dense_2 (Dense)
Size
2 × 2
3 × 3 × 32
3 × 3 × 32
3 × 3 × 64
3 × 3 × 64
3 × 3 × 128
3 × 3 × 128
2 × 2
2 × 2
512
43
Activation
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
ReLU
Softmax
max_pool_1 (MaxPooling2D)
max_pool_2 (MaxPooling2D)
max_pool_3 (MaxPooling2D)
of an image, and the trapdoor is injected with the injection ratio set
to 0.1, i.e., the trapdoored model is trained with the contaminated
data taking 10% of the total training data. When multiple trapdoors
are injected into a DNN model, a trapdoor trigger consists of ve
3 × 3 small pieces placed randomly across the whole image, and
trapdoors are injected with the injection ratio set to 0.5. In both
cases, the merge transparency is set to 0.1, which means that the