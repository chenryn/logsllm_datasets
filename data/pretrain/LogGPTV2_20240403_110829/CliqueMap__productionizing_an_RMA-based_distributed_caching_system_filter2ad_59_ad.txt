6.4 R=2/Immutable Mode
Google has a rich ecosystem of durable storage systems [14–16]
which utilize persistent storage media to avoid data loss. To reduce
lookup latencies relative to persistent storage and DRAM require-
ments relative to R=3.2, we added an R=2 mode to CliqueMap,
in which an immutable corpus is loaded from an external system
of record. Because the data is immutable, only one replica need
be consulted for most operations, although the second replica still
serves in the event of a failed backend task. As such, R=2/Immutable
resembles CliqueMap R=1, in terms of its network behaviors, and
has data availability properties that tolerate single-backend failures.
6.5 Disaggregating Local State
CliqueMap was initially intended to displace RPC-based KVCS for
CPU efficiency improvement of applications that accessed a dis-
tributed corpus. However, we found that CliqueMap’s latency was
sufficiently low that some serving stacks that relied on serving data
shards from local memory could instead access those corpora over
the network from CliqueMap. Importantly, remote access allows
these serving tasks to become stateless. Statelessness allows com-
pute to scale independently from DRAM (holding data), leading
to overall improved resource efficiency. Increased strategic focus
on disaggregation occurred well after initial launch and minor fea-
tures enabling such use cases were added, e.g., customizeable hash
functions.
7 Evaluation
This section first presents measurements of CliqueMap production
workloads, and then, using synthetic workloads, we present con-
trolled experiments that highlight CliqueMap behaviors in specific
scenarios, including the impact of previously discussed production-
ization features.
7.1 Production Workloads
We highlight two production serving workloads: Ads (Figure 8),
from a datacenter in The Dalles, Oregon (USA), and Geo (Figure 9),
serving road traffic predictions from a datacenter in Lenoir, North
Carolina (USA).
The Ads workload is a portion of the serving pipeline for Google’s
advertising business on third-party Internet properties. Advertising
data is keyed by topic and fetched on-demand from CliqueMap cells
(R=3.2) when participating in an auction process; late responses
to such auctions are discarded and hence response time is critical
to revenue opportunity. The graph supports CliqueMap’s design
100
CliqueMapClientPonyExpress088017602640CPU-ns/op2xRSCARMSG0+1d+2d+3d+4d+5d+6d+7d02.5K5K7.5KOpLatency(us)0250K500K750KOpRate(ops/sec)50pLatency90pLatency99pLatency99.9pLatencyGETRateSETRate(Writes)SETRate(Backfill)CliqueMap: Productionizing an RMA-Based Distributed Caching System
SIGCOMM ’21, August 23–28, 2021, Virtual Event, USA
Figure 9: Geo Workload.
Figure 11: Preferred backend selection benefits under varied
server host load; normalized to no-load.
Figure 10: Ads and Geo Object Size Distribution.
Figure 12: SCAR vs. 2×R performance under varied client load.
decisions to optimize for a GET rate that greatly exceeds the SET
rate. Ads fetch tends to be highly batched; batch sizes reach 30-300
KV pairs in the 99.9th percentile tail, which makes the client the
bottleneck due to response incast, pushing 99.9% GET tail latency
into the vicinity of 5ms.
Geo is a serving workload that provides estimates of traffic condi-
tions on roadways throughout the world. It feeds into, e.g., driving
directions suggested to end-users. The corpus is keyed by a road seg-
ment identifier, and stores a compact representation of utilization of
the road segment in question. Like Ads, lookups are highly batched,
usually consisting of tens of segments at a time. The underlying
model experiences a high update rate. The Geo workload serves
highly diurnal GET traffic intermixed with a background update rate
for the corpus (SETs), originating from different client jobs. Despite
the 3x variation in GET rate over the course of a day, 99.9% tail
latency varies minimally. Like most CliqueMap workloads, GET
performance is critical; less so for SETs.
These workloads are typified by values of different sizes; Fig-
ure 10 plots the CDF. For both workloads, objects tend to be small,
typically at most a few KB (importantly, smaller than our typical
MTU size), but there is a tail of larger objects.
7.2 Controlled Experiments
Next, we discuss the quantifiable behaviors of the CliqueMap design
through a set of controlled experiments using synthetic workloads.
7.2.1 Preferred Backend Selection Benefits
To highlight the effect of quoruming to reduce tail latency in R=3.2,
we set up a synthetic workload with a small, 3-backend R=3.2
CliqueMap cell, configured to use 2×R. Synthetic clients repeat-
edly GET the same 4KB-sized K/V pair. We then place one of three
backends under load from an antagonist, which offers ∼95Gbps of
competing demand through its NIC. Figure 11 plots the resulting
normalized median and tail latencies.
Takeaway: Preferred backend selection in R=3.2 tolerates a single
slow server; there is almost no elevation in latency (within noise
margins). In comparison, R=1 is obliged to rely on load to a slow
server, and hence both median and tail suffer due to the overtaxed
backend.
7.2.2 SCAR and Incast
§6.3 outlines our addition of SCAR to Pony Express, and because
of its advantages we deploy SCAR to most production cells. But
not all tail latency metrics improved when we introduced SCAR, as
SCAR has a potential downside: when deployed with R=3.2, SCAR
solicits three full copies of the datum, whereas 2×R solicits only one,
plus three IndexEntries. That is, SCAR transiently incasts its client,
which can be problematic when batch sizes or values are large.
Figure 12 plots the behavior of SCAR and 2×R when fetching
relatively large (64KB) values, with and without competing load
applied to the client (which exacerbates the incast condition). The
difference in median GET duration is evident; because SCAR trans-
fers 195KB per op (3× 64KB values and 3× 1KB Buckets), it begins
to lag behind 2×R’s 67KB transfer (1× 64KB value and 3× 1KB
Buckets), despite SCAR’s single round-trip advantage. The precise
constants leading to this effect vary with technology generation;
older, slower hardware observes this effect at smaller value/batch
sizes.
Takeaway: Deploy SCAR when values/batch sizes are small rela-
tive to NIC speeds. It’s acceptable to redundantly fetch data when
individual KV sizes are small.
7.2.3 Maintenance
Maintenance, binary upgrades, and reconfigurations are ubiquitous,
and hence CliqueMap performance during these events is critical. We
next highlight the performance of CliqueMap GETs during repairs
and data migrations.
101
0+1d+2d+3d+4d+5d+6d+7d05K10K15KOpLatency(us)02M4M6MOpRate(ops/sec)50pLatency90pLatency99pLatency99.9pLatencyGETRateSETRate103105KVObjectSize(B)0.000.250.500.751.00CDFAdsGeoR=3.2NoExternalLoadR=3.2WithExternalLoadR=1NoExternalLoadR=1WithExternalLoad012NormalizedLatency50pLatency99pLatency2XRSCAR050100150MedianLatency(us)WithExternalLoadNoExternalLoadSIGCOMM ’21, August 23–28, 2021, Virtual Event, USA
A. Singhvi et al.
Figure 13: CliqueMap Planned Maintenance via Spares at con-
sistent 100K GET/sec.
Figure 14: CliqueMap Unplanned Maintenance via Repairs at
consistent 100K GET/sec.
In this scenario, we load an R=3.2 CliqueMap cell with 100K
GET/sec from ten clients, and then inject artificial planned and
unplanned maintenance events. In the former case, CliqueMap is no-
tified of impending primary (non-spare) backend restart, and hence,
can utilize warm spare backends gracefully; in the latter case, only
post-restart repairs are possible. Figure 13 plots GET latency per-
centiles and RPC byte rates (from repairs and sparing) during the
planned event. We inject the planned restart at 13:52:00. Immedi-
ately, the notified primary backend transfers its data to a spare (RPC
traffic), and exits by 13:53:30. At 13:54:05, the spare returns the
transferred data to the newly-restarted primary (RPC traffic again).
Takeaway: Warm sparing effectively hides planned maintenance
from clients. Throughout the event, we see virtually no change in the
client-observed latencies; fewer than 1 op in 1000 observes degraded
performance, e.g., from retries.
Finally, we forcibly crash a backend to simulate an unplanned
maintenance event (Figure 14), at 14:22:00 (not evident from graph).
The new backend restarts on another host by 14:23:30, and we
observe a significant burst of RPC activity as repairs are performed.
Latency fluctuates slightly during this interval, and even experiences
a downward trend, as the clients perform less total work when the
cell is degraded–after observing a connection failure, clients only
send two out of three operations per GET, as they await reconnect.
Takeaway: Repairs augment warm sparing and have little perfor-
mance impact under realistic load levels.
7.2.4 RMA Deployment Characterization
We next characterize the performance of moderate-sized CliqueMap
cells under controlled load in homogeneous RMA deployments. We
use a 950-host testbed with synthetic load generation, as the details
of transport operation are difficult to isolate and study in situ in
production workloads. Our testbed is equipped with Skylake-class
CPUs and connected with a fabric capable of 50Gbps sustained and
102
Figure 15: Pony Express scale out as a heatmap, wherein darker
red indicates a larger fraction of machines scaled out to, on av-
erage, the number of cores reflected on the right axis. Lines de-
marcate 50th/90th/99th CliqueMap latencies on the left axis.
100Gbps burst per host. To highlight the networking behaviors at
scale, we operate a 500-backend R=1 CliqueMap cell; when using
Pony Express [31] we enable SCAR, but use 2×R fetches when
using 1RMA [34]. We use a fixed value size of 4KB, which, with
framing and metadata, allows a GET response to fit within a single
5KB-MTU frame.
Pony Express Load Ramp. Pony Express can scale out to addi-
tional CPU for network Tx/Rx activities. We configure each client
and backend to spread its load among four Pony Express engines.
Engines are single-threaded and may time-multiplex a single core
or each scale out to their own core in response to load. Figure 15
plots the GET latency percentiles as we ramp request rate to 400M
GET/sec (among 10K client tasks, which is 800K ops/sec/backend).
We overlay the degree of Pony Express scale-out on the right axis,
as a heat map, wherein darker red indicates a larger fraction of ma-
chines scaled out to, on average, the number of cores reflected on
the right axis.
The scale-out plot reveals two bands, respectively corresponding
to hosts occupied by only CliqueMap clients (average 10.6 clients
per host) and those also hosting a backend (500 such systems with
one backend each). Hosts occupied by both CliqueMap backends and
clients (co-tenant) are busier on average, and hence Pony Express
scales out on these hosts first, reaching ∼3.5 CPU/host. But as load
continues to rise, client-only hosts also surpass scaling thresholds,
and begin to scale out at 20:00, and en masse by 20:10, reaching ∼1.5
CPU/host. The client-side scale-out process significantly reduces tail
latency even as load continues to ramp up, because receive transfer
parallelism is achieved within individual clients.
Takeaway: The combination of CliqueMap and Pony Express
has significant capacity headroom. For near-term technologies, we
don’t expect significant design changes needed to realize further
performance, because with tuning we can drive our system’s op rate
in a single cell well beyond the current demand.
1RMA Load Ramp. Figure 16 plots a similar experiment using
1RMA. 1RMA offers a different set of tradeoffs—in contrast to Pony
Express, 1RMA’s serving path is entirely hardware. However, 1RMA
doesn’t offer the SCAR primitive, and hence each lookup operation
must use 2×R and incur two fabric round-trip times (RTTs) per
operation. 1RMA also significantly optimizes interaction between
the NIC and the server memory system via PCIe, so the application-
visible RTT for 1RMA is lower than with more traditional packet-
oriented systems.
13:5213:5313:5413:5504080120OpLatency(us)00.4M0.8M1.2MRPCBytes/sec50pLatency99.9pLatencyRPCBytes/sec14:2314:2414:2514:2604080120OpLatency(us)05M10M15MRPCBytes/sec50pLatency99.9pLatencyRPCBytes/secTransport CPU Scale-outCliqueMap Latency (us)19:5020:0020:1020:2020:3005010015020025030035040000.511.522.533.5419:40Client-only scale-outCo-tenant scale-outPony Express scale outCliqueMap: Productionizing an RMA-Based Distributed Caching System
SIGCOMM ’21, August 23–28, 2021, Virtual Event, USA
Figure 16: 1RMA Ramp: Fabric+PCIe timestamps during load
ramp. Lines demarcate 50th/90th/99th/99.9th percentiles.
Figure 18: CliqueMap latencies under varying mixes of GETs
and SETs, under fixed value size 4KB.
Figure 17: 1RMA Ramp - GET Latencies. Lines demarcate
50th/90th/99th percentiles.
Figure 19: CliqueMap CPU cost under varying mixes of GETs
and SETs, under fixed value size 4KB.
We again ramp load from 0 to 400M GET/sec. The figure over-
lays a heatmap of the timestamps emitted by the 1RMA NIC, a
hardware measurement of the combined latency of fabric and re-
mote PCIe. At peak, this workload demands only ∼32Gbps from
server-side PCIe on average and hence we expect its latency to
not be substantially elevated. Combined fabric/PCIe latency rises
marginally with load, still well short of saturating the network. End-
to-end GET latency, shown in Figure 17, is dominated by CPU time
spent in the CliqueMap client, as depicted by the mostly unchanging
latency distribution of CliqueMap GETs themselves. Perhaps sur-
prisingly, the highest latency is observed at the lowest load, an effect
we often see when our testbed is otherwise idle, due to power-saving
C-state transitions at low load. By roughly 250K GET/sec/client,
delays from C-state transitions have disappeared entirely and total
latency remains insensitive to load.
Takeaway: RMA Infrastructure heterogeneity means there’s no
single optimal lookup method–the choice depends on the underlying
infrastructure, and hence a system’s ability to evolve over time mat-
ters. Counter-intuitively, despite requiring two fabric round-trips per
GET, the simple and generic 2×R fetch strategy can outperform the
SCAR-based strategy under load in this testbed, as the all-hardware
1RMA serving path incurs no software bottleneck on the serving
side. Because CliqueMap can leverage a variety of transports and
fetch algorithms atop them, CliqueMap can provide users a relatively
uniform performance envelope, taking advantage of scale-out to do
so in environments lacking significant offload.
7.2.5 Workload Variance
Figure 18 plots latency and Figure 19 plots CPU usage of
CliqueMap backends under varying mixes of GETs and SETs. These
are unlike our previous graphs, which differentiate GET from SET
performance. It is no surprise that greater percentages of RPC-based
SETs incur greater overheads and worse typical latency, as progres-
sively more of the workload is unable to use RMA.
Figure 20: CliqueMap performance under varying value sizes.
We consider the effect of value size at fixed GET rate in Figure 20.
For values sizes common in our production workloads, individual
GET and SET performance are dominated by fixed costs–i.e., costs
per op, not costs per byte–as our value sizes tend to be small (Figure
10).
Takeaway: CliqueMap delivers on its intent of providing nominal
lookup latencies across diverse workloads.
8 Related Work
Since its inception, CliqueMap’s goal has been to deliver the perfor-
mance of state-of-the-art KVCSs in the literature (e.g., Pilaf [33],
HERD [23], MICA [29], FaRM-KV [17, 18], and others [36, 39, 40])
to Google datacenters, adapting ideas as needed to meet the practical
requirements of our environment. These requirements differ among
hyperscale operators. For instance, the ubiquity of Stubby RPCs
and ALTS [2] at Google means that third party solutions (e.g., mem-
cached) aren’t directly applicable. Rather, as with Twemcached [41],
such solutions require a fair amount of investment to reach produc-