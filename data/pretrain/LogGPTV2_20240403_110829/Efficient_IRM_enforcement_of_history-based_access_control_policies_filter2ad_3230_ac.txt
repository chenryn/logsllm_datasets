Theorem 3. The four-step optimization procedure is safe,
unobtrusive and eﬀective.
Proof. Since only guaranteed preconditions and dead ef-
fects are eliminated, the feasibility of a path is not altered
by the optimization procedure. Safety and unobtrusive-
ness thus follow. Eﬀectiveness follows from the fact that
the procedure performs only precondition and eﬀect elimi-
nation.
Discussion.
By adopting conjunctive preconditions and constant ef-
fects, rather than unconstrained precondition and eﬀect ex-
pressions, we have obtained an elegant and informed opti-
mization procedure. First, a function of the form fL(S) =
S ⊕ L for a ﬁxed set L of generalized literals is a monotone
function [25]. Our representation is thus readily amenable
to guaranteed set analysis. Second, the syntactic restric-
tion allows the analyses to deduce more information about
guaranteed sets (see (3)) and live sets (see (7)) than an un-
constrained representation.
cannot be readily eliminated.
Challenge #4 By (11), precondition checks following a re-
turn node cannot be readily eliminated.
In the next section, we discuss a distributed optimization
protocol that would allow an untrusted code producer to
assist a distrusting code consumer in addressing the above
challenges.
5. A DISTRIBUTED OPTIMIZATION
PROTOCOL
5.1 Cooperative Optimization without
Assuming Trust
Consider a program distribution scenario inspired by [24],
in which an untrusted code producer P distributes a pro-
gram P to a code consumer C for execution. Suppose C
employs IRM to enforce a history-based access control pol-
icy, while P, eager to promote the usage of P, oﬀers to help
boost the optimization eﬀectiveness of C. How can C securely
accept the contribution of P? We propose the following dis-
tributed optimization protocol .
Stage 1: C publishes, over an untrusted media, a security
policy π = hΠ, {δa}, αi, where Π is a set of state vari-
ables, {δa} a family of operators for Π-states, and α a
procedure that computes, for a program P, an associa-
tive array op[·] mapping every program point in P to
an operator from {δa}.
Stage 2: P submits π and an untrusted program P to an
untrusted oracle, which generates a set D of opti-
mization directives. D contains annotations designed
to inform C of how aggressive optimization can be
achieved.
Stage 3: P ships the package hP, Di to C via an untrusted
channel.
Stage 4: C performs the steps below before executing P:
Step 1 - Guaranteed set analysis.
We replace data ﬂow equation (11) by the following:
GUAout [n] = pre
GUAout [n] = symtbl (E−1
inv (n)).post
for n ∈ {nentry } (15)
for n ∈ Nret
(16)
(The expression symtbl (E−1
inv (n)) refers to the callee’s proce-
dure interface for n ∈ Nret .) Rather than indiscriminately
taking guaranteed sets to be ∅ at the entry node and the
return nodes, the interface components pre and post now
inform guaranteed set analysis, thereby creating more op-
portunities for precondition elimination, and thus address-
ing Challenges 1 & 4. This works so long as pre and post
are trustworthy annotations.
Step 3 - Liveness analysis.
Phase 1: Use procedure α to construct operator as-
We replace data ﬂow equation (13) by the following:
signment op[·] for P.
Phase 2: Update op[·] as follows: (a) D is exploited
to optimize op[·] aggressively. (b) As D cannot
be fully trusted, blindly following the optimiza-
tion directives may destroy the safety of the opti-
mization procedure. Additional “guards” are in-
jected into op[·], so that fraudulent annotations
are detected when P is executed.
Phase 3: Inject op[·] into P.
The protocol is particularly appropriate for a C that is com-
putationally constrained (e.g., IRM via load-time binary
rewriting), and a P having access to a computationally pow-
erful oracle (e.g., oﬄine certiﬁcation service). In the sequel,
we specialize the protocol for addressing the four optimiza-
tion challenges outlined in Sect. 4.2.
5.2 Procedure Interfaces
We postulate that the code producer attaches a proce-
dure interface to every procedure it ships. Each procedure
interface is a quadruple hpre, post, dead in , dead out i, where:
pre: a set of literals guaranteed by the caller to be estab-
lished at the call node.
post: a set of literals guaranteed by the procedure to be
established at the exit node.
dead in : a set of propositions guaranteed by the procedure
to be dead at the entry node.
dead out : a set of propositions guaranteed by the caller to be
dead at the return node.
The main procedure must have an interface of h∅, ∅, Π, Πi.
Interfaces of other procedures can be generated by the code
producer using an appropriate whole-program analysis (see
[41, Appendix A] for a complete algorithm).
5.3 Using Procedure Interfaces as Optimiza-
tion Directives
The code consumer treats the procedure interfaces as op-
timization directives. Speciﬁcally, C uses the interfaces to
perform more accurate analyses in Step 1 and Step 3 of
the four-step optimization procedure. To see this, assume
there is a symbol table function symtbl that maps every call
node to the procedure interface of the callee.
LIVin [n] = (Π \ dead out ) ∪ vars(op[n].pre)
for n ∈ {nexit }
LIVin [n] = (Π \ symtbl (n).dead in ) ∪ vars(op[n].pre)
for n ∈ Ncall
(17)
(18)
(The subexpression vars(op[n].pre) does not concern us for
now, because, by setting op[n] initially to h∅, ∅i for n 6∈
Ninstr , the subexpression is essentially ∅. It becomes indis-
pensable when op[n] is not empty, as is the case once (19),
(20) and (21) have been introduced.) If the interface compo-
nents dead in and dead out are trustworthy, then they inform
liveness analysis at the exit node and the call nodes, thereby
addressing Challenges 2 & 3.
5.4 Guarding Against Fraudulent Procedure
Interfaces
But the procedure interfaces are not to be trusted! They
could cause essential monitoring logic to be optimized away.
To prevent this, Steps 2 and 4 of the four-step optimization
procedure are adapted as follows.
Step 2 - Precondition elimination.
This step now involves two subtasks. First, associate an
auxiliary operator to the exit node and each call node:
op[n] := op guard (symtbl (n).pre)
op[n] := op guard (post)
for n ∈ Ncall
for n ∈ {nexit }
(19)
(20)
where, given a set S of literals, op guard (S) is the eﬀect-less
operator hS, ∅i. The injected operators guarantee that the
assumptions made in data ﬂow equations (15) and (16) are
veriﬁed at run time.
The second subtask is precondition elimination, which is
performed also on the newly introduced operators:
op[n].pre := op[n].pre \ GUAin [n]
for n ∈ Ninstr ∪ Ncall ∪ {nexit }
(21)
Step 4 - Effect elimination.
Again, this step is now divided into two subtasks. First,
an auxiliary operator is assigned to every entry and return
node.
op[n] := op assert (GUAout [n], dead in )
for n ∈ {nentry }
(22)
op[n] := op assert (GUAout [n], symtbl (E−1
inv (n)).dead out )
for n ∈ Nret
(23)
where, given a set S of literals and a set P of proposi-
tions, op assert (S, P ) is the precondition-less operator h∅, (S ∩
lits(P )) ∪ {?p | p ∈ P \vars(S)}i. The operator assigns a
value to each of the propositions in P . For each proposition
in P that also appear in a literal in S, the assigned value
is speciﬁed by the literal. For each proposition in P that
does not appear in a literal in S, the assigned value is unde-
ﬁned. Essentially, the operator forces all propositions in P
to become dead at run time, and serves as a “guard” for the
assumptions made in (17) and (18).
The second subtask is eﬀect elimination, which is also per-
formed on the newly introduced auxiliary operators.
op[n].eﬀ := op[n].eﬀ ∩ gen-lits(LIVout [n])
detection will be optimized away. Conservative procedure
interfaces can be generated by the interface generation algo-
rithm described in [41, Appendix A].
5.5 Accommodating Java-Style Language
Constructs
Exception handling.
In [41, Sect. 6.1] we provide details on how our optimiza-
tion procedure can be extended to accommodate Java-style
exception handling constructs. As a highlight, procedure
interfaces must now assume the form hpre, post, esc, dead in ,
dead out , dead fail i, where the new components have the fol-
lowing roles:
esc: a set of literals guaranteed by the procedure to be es-
tablished when an exception escapes the procedure
dead fail : a set of propositions guaranteed to be dead by han-
dlers of exceptions escaping from the procedure
for n ∈ Ninstr ∪ Nret ∪ {nentry }
(24)
Method overriding.
Theorem 4. The revised optimization procedure is safe.
Proof. Suppose the value of op[n] has been updated
from h∅, ∅i to op guard (S) for some set S of literals. As the
operator is eﬀect-less, every infeasible path containing n re-
mains infeasible. The introduction of op guard (S) in updates
(19) and (20) thus preserves safety.
Now, suppose the value of op[n] has been updated from
h∅, ∅i to op assert (GUAout [n], P ), for some set P of proposi-
tions. Consider an eﬀect asserted by the auxiliary operator.
If the eﬀect is of the form ?p, then it only causes future pre-
condition checks to fail, but never establishes any precondi-
tion (recall that a precondition cannot be used to check if
a proposition is undeﬁned). If the eﬀect is a literal, and it
establishes a precondition, then the precondition is already
guaranteed prior to the assertion of the literal.
In either
case, infeasible paths remain infeasible. Updates (22) and
(23) thus preserve safety.
In other words, if the code producer attempts to mislead
the code consumer by sending fraudulent procedure inter-
faces, the fraud will be detected by the IRM at run time.
Thm. 4 therefore guarantees the security of the distributed
optimization protocol.
The interface hpre, post, dead in , dead out i of a procedure
proc is said to be conservative iﬀ all the following hold:
(a) pre ⊆ GUAin [n] for every call node n for which proc is
the callee, (b) post ⊆ GUAin [n] for the exit node n of proc,
(c) dead in ⊆ Π \ LIVout [n] for the entry node n of proc, and
(d) dead out ⊆ Π \ LIVout [n] for every return node n for which
proc is the callee.
Theorem 5. With conservative interfaces, the revised op-
timization procedure is unobtrusive and eﬀective.
Proof. If all procedure interfaces are conservative, then
the updates (21) and (24) will completely remove the pre-
conditions and eﬀects of the auxiliary operators introduced
in (19), (20), (22) and (23).
In other words, if the code producer is honest about the
optimization directives, then the run-time checks for fraud
In the presence of dynamic method dispatching, the code
consumer must verify that method overriding honors certain
constraints among method interfaces. Given method inter-
faces I = hpre, post, esc, dead in , dead out , dead fail i and I ′ =
fail i, we write I ′ ⊑ I
hpre ′, post ′, esc′, dead ′
iﬀ all of the following hold:
out , dead ′
in , dead ′
pre ⊇ pre ′
dead in ⊆ dead ′
in
post ⊆ post ′
dead out ⊇ dead ′
out
esc ⊆ esc′
dead fail ⊇ dead ′
fail
The constraints follow the usual contravariant pattern of
function subtyping [26]. To preserve safety, the code con-
sumer must verify that I ′ ⊑ I whenever a method with
interface I ′ overrides a method with interface I. Since ⊑ is
transitive, only direct method overrides need to be veriﬁed.
The interface generation algorithm in [41, Appendix A] can
be used by the code producer to generate method interfaces
guaranteed to satisfy the above. Details on the treatment of
method overriding can be found in [41, Sect. 6.2].
6.
IMPLEMENTATION EXPERIENCE
We developed a Java prototype for the revised optimiza-
tion procedure (Sect. 5), with Java bytecode as the target
language. Our prototype handles the entire Java bytecode
language. The prototype was developed in Soot [37], a
framework for Java bytecode manipulation and optimiza-
tion. Soot provides facilities for converting Java bytecode
into more manageable internal representations, performing
control ﬂow analysis to construct control ﬂow graphs, as well
as providing infrastructure code for iterative, intraprocedu-
ral data ﬂow analyses. Speciﬁcally, our prototype consists
of three components: (1) a modular optimization procedure,
which applies the revised four-step optimization procedure
to a CFG and an operator assignment, (2) an instrumenta-
tion module that converts a CFG and an operator assign-
ment to Java bytecode, and (3) a method interface genera-
tor, which is a whole-program analysis built on top of the
modular optimization procedure [41, Appendix A].
Soot’s built-in control ﬂow analyzer has been adopted to
construct control ﬂow graphs in the presence of exceptions.
Name/Version Description
BCEL/5.2
BcVer/1.0
JavaCC/4.0
JavaTar/2.5
ProGuard/4.2
SableCC/3.2
framework for manip-
ulating Java bytecode
prints classﬁle version
parser generator
tar-style archiving tool
classﬁle shrinker, op-
timizer, obfuscater &
pre-veriﬁer
parser generator
#classes #methods
384
11
137
15
447
3184
120
2091
176
4211
285
2366
Figure 4: Benchmarking suite.
Although Soot provides “hooks” for programmers to cus-
tomize the control ﬂow analyzer so that more accurate ex-
ception ﬂows can be obtained, we refrain from following that
trail, as precise exception escape analysis is outside of the
scope of this work. We however modiﬁed the code base of
the Soot data ﬂow analysis framework to accommodate the
complex data ﬂow equations caused by exception handling
(see [41, Sect. 6.1] for details).
7. EMPIRICAL EVALUATION
We employed our prototype to empirically assess the de-
gree to which an IRM enforcement mechanism can bene-
ﬁt from the four-step optimization procedure (Sect. 4), as
well as the further improvements brought about by adopt-
ing method interfaces as optimization directives in a dis-
tributed optimization protocol (Sect. 5). To benchmark
our optimization schemes against production-quality con-
trol ﬂow graphs, we selected a suite of open source Java
applications for our experiments (see Fig. 4). We inten-