space
EVRANGE B
Host application
space
Figure 2: Per-enclave page tables
bles, which are stored in the enclave’s memory (Figure 2).
This makes private the page table dirty and accessed bits,
which can reveal memory access patterns at page granu-
larity. Exposing an enclave’s page tables to the untrusted
OS leaves the enclave vulnerable to attacks such as [55].
The enclave’s virtual address space outside EVRANGE
is used to access its host application’s memory, via the
page tables set up by the OS. Sanctum’s hardware exten-
sions implement dual page table lookup (§ 5.2), and make
sure that an enclave’s page tables can only point into the
enclave’s memory, while OS page tables can only point
into OS memory (§ 5.3).
Sanctum supports multi-threaded enclaves, and en-
claves must appropriately provision for thread state data
structures. Enclave threads, like their SGX cousins, run at
the lowest privilege level (user level in RISC-V), meaning
a malicious enclave cannot compromise the OS. Specif-
ically, enclaves may not execute privileged instructions;
address translations that use OS page tables generate page
faults when accessing supervisor pages.
The per-enclave metadata used by the security monitor
is stored in dedicated DRAM regions (metadata regions),
each managed at the page level by the OS, and each in-
cludes a page map that is used by the security monitor to
verify the OS’ decisions (much like the EPC and EPCM
in SGX, respectively). Unlike SGX’s EPC, the metadata
region pages only store enclave and thread metadata. Fig-
ure 3 shows how these structures are weaved together.
Sanctum considers system software to be untrusted, and
governs transitions into and out of enclave code. An en-
clave’s host application enters an enclave via a security
monitor call that locks a thread state area, and transfers
control to its entry point. After completing its intended
task, the enclave code exits by asking the monitor to un-
lock the thread’s state area, and transfer control back to
the host application.
Enclaves cannot make system calls directly: we cannot
trust the OS to restore an enclave’s execution state, so the
860  25th USENIX Security Symposium 
USENIX Association
4
Page Map Entries
Enclave
Type
Invalid
0
⋮
⋮
C1C000
C1C000
Enclave
Enclave
⋮
⋮
Thread
C1C000
C1C000 Thread
⋮
C1C000 Thread
C1C000 Thread
⋮
Metadata Region
Page Map
Enclave info
Mailboxes
⋮
Thread 1 state
⋮
Thread 2 state
Enclave memory
Page tables
⋮
Thread 1 stack
Thread 1 fault 
handler stack
Thread 2 state
⋮
Runtime code
Application code
Application data
Enclave info
Initialized?
Debugging enclave?
Running thread count
Mailbox count
First mailbox
DRAM region bitmap
Measurement hash
Thread state
Lock
AEX state valid?
Host application PC
Host application SP
Enclave page table
base
Entry point (PC)
Entry stack pointer (SP)
Fault handler PC
Fault handler SP
Fault state (R0 … R31)
AEX state (R0 … R31)
Runtime code
Fault handler
Enclave entry
Enclave exit
Syscall proxying
Figure 3: Enclave layout and data structures
enclave’s runtime must ask the host application to proxy
syscalls such as ﬁle system and network I/O requests.
Sanctum’s security monitor is the ﬁrst responder for
interrupts: an interrupt received during enclave execution
causes an asynchronous enclave exit (AEX), whereby the
monitor saves the core’s registers in the current thread’s
AEX state area, zeroes the registers, exits the enclave, and
dispatches the interrupt as if it was received by the code
entering the enclave.
Unlike SGX, resuming enclave execution after an AEX
means re-entering the enclave using its normal entry point,
and having the enclave’s code ask the security monitor to
restore the pre-AEX execution state. Sanctum enclaves
are aware of asynchronous exits so they can implement
security policies. For example, an enclave thread that
performs time-sensitive work, such as periodic I/O, may
terminate itself if it ever gets preempted by an AEX.
The security monitor conﬁgures the CPU to dispatch
all faults occurring within an enclave directly to the en-
clave’s designated fault handler, which is expected to be
implemented by the enclave’s runtime (SGX sanitizes
and dispatches faults to the OS). For example, a libc
runtime would translate faults into UNIX signals which,
by default, would exit the enclave. It is possible, though
not advisable for performance reasons (§ 6.3), for a run-
time to handle page faults and implement demand paging
securely, and robust against the attacks described in [55].
Unlike SGX, we isolate each enclave’s data throughout
the system’s cache hierarchy. The security monitor ﬂushes
per-core caches, such as the L1 cache and the TLB, when-
ever a core jumps between enclave and non-enclave code.
Last-level cache (LLC) isolation is achieved by a simple
partitioning scheme supported by Sanctum’s hardware
extensions (§ 5.1).
Sanctum’s strong isolation yields a simple security
model for application developers: all computation that
executes inside an enclave, and only accesses data inside
the enclave, is protected from any attack mounted by soft-
ware outside the enclave. All communication with the
outside world, including accesses to non-enclave memory,
is subject to attacks.
We assume that the enclave runtime implements the
security measures needed to protect the enclave’s com-
munication with other software modules. For example,
any algorithm’s memory access patterns can be protected
by ensuring that the algorithm only operates on enclave
data. The runtime can implement this protection simply
by copying any input buffer from non-enclave memory
into the enclave before computing on it.
The enclave runtime can use Native Client’s approach
[57] to ensure that the rest of the enclave software only
interacts with the host application via the runtime to miti-
gate potential security vulnerabilities in enclave software.
The lifecycle of a Sanctum enclave closely resembles
the lifecycle of its SGX equivalent. An enclave is created
when its host application performs a system call asking
the OS to create an enclave from a dynamically loadable
module (.so or .dll ﬁle). The OS invokes the security
monitor to assign DRAM resources to the enclave, and
to load the initial code and data pages into the enclave.
Once all the pages are loaded, the enclave is marked as
initialized via another security monitor call.
Our software attestation scheme is a simpliﬁed version
of SGX’s scheme, and reuses a subset of its concepts.
The data used to initialize an enclave is cryptographically
hashed, yielding the enclave’s measurement. An enclave
can invoke a secure inter-enclave messaging service to
send a message to a privileged attestation enclave that can
access the security monitor’s attestation key, and produces
the attestation signature.
5 Hardware Modiﬁcations
5.1 LLC Address Input Transformation
Figure 4 depicts a physical address in a toy computer with
32-bit virtual addresses and 21-bit physical addresses,
4,096-byte pages, a set-associative LLC with 512 sets and
64-byte lines, and 256 KB of DRAM.
The location where a byte of data is cached in the
USENIX Association  
25th USENIX Security Symposium  861
5
Address bits covering the maximum addressable physical space of 2 MB
31
Address bits used by 256 KB of DRAM
Cache Set Index
DRAM Stripe
Index
DRAM Region
Index
Cache
Line Oﬀset
20
17
15
18
Cache Tag
Physical page number (PPN)
14
12
11
5
6
Page Oﬀset
Virtual Address
Virtual Page Number (VPN)
Address Translation Unit
0
Physical Page 
Number (PPN)
20
S2
18
15
17
DRAM Region
Index
14
12
S1
12
11
5
6
Page Oﬀset
0
Figure 4: Interesting bit ﬁelds in a physical address
0KB
256KB
No cache address shift - 8 x 4 KB stripes per DRAM region
1-bit cache address shift - 4 x 8 KB stripes per DRAM region
2-bit cache address shift - 2 x 16 KB stripes per DRAM region
3-bit cache address shift - a DRAM region is one 32 KB stripe
Region 3
Region 7
Region 1
Region 5
Region 2
Region 6
Region 0
Region 4
Figure 5: Address shift for contiguous DRAM regions
LLC depends on the low-order bits in the byte’s physical
address. The set index determines which of the LLC lines
can cache the line containing the byte, and the line offset
locates the byte in its cache line. A virtual address’s low-
order bits make up its page offset, while the other bits are
its virtual page number (VPN). Address translation leaves
the page offset unchanged, and translates the VPN into
a physical page number (PPN), based on the mapping
speciﬁed by the page tables.
We deﬁne the DRAM region index in a physical ad-
dress as the intersection between the PPN bits and the
cache index bits. This is the maximal set of bits that im-
pact cache placement and are determined by privileged
software via page tables. We deﬁne a DRAM region to
be the subset of DRAM with addresses having the same
DRAM region index. In Figure 4, for example, address
bits [14 . . .12] are the DRAM region index, dividing the
physical address space into 8 DRAM regions.
In a typical system without Sanctum’s hardware exten-
sions, DRAM regions are made up of multiple continuous
DRAM stripes, where each stripe is exactly one page
long. The top of Figure 5 drives this point home, by
showing the partitioning of our toy computer’s 256 KB
of DRAM into DRAM regions. The fragmentation of
Cache 
Address 
Shifter
Cache Unit
Input
S1
18
20
17
S2
Cache Tag
DRAM Region
Index
15
14
12
11
6
5
Cache
Line Oﬀset
0
Cache Set Index
Address bits used by 256 KB of DRAM
Physical address
Figure 6: Cache address shifter, 3 bit PPN rotation
DRAM regions makes it difﬁcult for the OS to allocate
contiguous DRAM buffers, which are essential to the efﬁ-
cient DMA transfers used by high performance devices.
In our example, if the OS only owns 4 DRAM regions,
the largest contiguous DRAM buffer it can allocate is 16
KB.
We observed that, up to a certain point, circularly shift-
ing (rotating) the PPN of a physical address to the right
by one bit, before it enters the LLC, doubles the size of
each DRAM stripe and halves the number of stripes in a
DRAM region, as illustrated in Figure 5.
Sanctum takes advantage of this effect by adding a
cache address shifter that circularly shifts the PPN to
the right by a certain amount of bits, as shown in Figures
6 and 7. In our example, conﬁguring the cache address
shifter to rotate the PPN by 3 yields contiguous DRAM
regions, so an OS that owns 4 DRAM regions could hy-
pothetically allocate a contiguous DRAM buffer covering
half of the machine’s DRAM.
The cache address shifter’s conﬁguration depends on
the amount of DRAM present in the system. If our exam-
ple computer could have 128 KB - 1 MB of DRAM, its
cache address shifter must support shift amounts from 2
to 5. Such a shifter can be implemented via a 3-position
variable shifter circuit (series of 8-input MUXes), and a
ﬁxed shift by 2 (no logic). Alternatively, in systems with
known DRAM conﬁguration (embedded, SoC, etc.), the
shift amount can be ﬁxed, and implemented with no logic.
5.2 Page Walker Input
Sanctum’s per-enclave page tables require an enclave page
table base register eptbr that stores the physical address
of the currently running enclave’s page tables, and has
similar semantics to the page table base register ptbr
pointing to the operating system-managed page tables.
These registers may only be accessed by the Sanctum
security monitor, which provides an API call for the OS
862  25th USENIX Security Symposium 
USENIX Association
6
CPU Die
Tile
Core
Tile
Core
Tile
Core
Tile
Core
L1 Cache
L1 Cache
L1 Cache
L1 Cache
Cache
Address
Shifter
Cache
Address
Shifter
Cache
Address
Shifter
Cache
Address
Shifter
TileLinkIO Network
Cache
Address
Shifter
DMA
Transfer
Filter
Cached
TileLinkIO
Device
LLC
Coherence 
Manager
LLC Cache
Slice
Cache
Address
Un-Shifter
Coherence 
Manager