title:The Industrial Age of Hacking
author:Timothy Nosco and
Jared Ziegler and
Zechariah Clark and
Davy Marrero and
Todd Finkler and
Andrew Barbarello and
W. Michael Petullo
The Industrial Age of Hacking
Timothy Nosco, United States Army; Jared Ziegler, National Security Agency; 
Zechariah Clark and Davy Marrero, United States Navy; Todd Finkler, 
United States Air Force; Andrew Barbarello, United States Navy; 
W. Michael Petullo, United States Army
https://www.usenix.org/conference/usenixsecurity20/presentation/nosco
This paper is included in the Proceedings of the 29th USENIX Security Symposium.August 12–14, 2020978-1-939133-17-5Open access to the Proceedings of the 29th USENIX Security Symposium is sponsored by USENIX.The Industrial Age of Hacking
Tim Nosco
United States Army
Jared Ziegler
National Security Agency
Zechariah Clark
United States Navy
Davy Marrero
United States Navy
Todd Finkler
United States Air Force
Andrew Barbarello
United States Navy
W. Michael Petullo
United States Army
Abstract
There is a cognitive bias in the hacker community to select
a piece of software and invest signiﬁcant human resources
into ﬁnding bugs in that software without any prior indi-
cation of success. We label this strategy depth-ﬁrst search
and propose an alternative: breadth-ﬁrst search. In breadth-
ﬁrst search, humans perform minimal work to enable au-
tomated analysis on a range of targets before committing
additional time and effort to research any particular one.
We present a repeatable human study that leverages
teams of varying skill while using automation to the great-
est extent possible. Our goal is a process that is effective
at ﬁnding bugs; has a clear plan for the growth, coaching,
and efﬁcient use of team members; and supports measur-
able, incremental progress. We derive an assembly-line
process that improves on what was once intricate, manual
work. Our work provides evidence that the breadth-ﬁrst
approach increases the effectiveness of teams.
1
Introduction
Can we build a better vulnerability discovery process?
Many researchers have proposed tools that aim to aid
human work, including approaches that apply symbolic
execution, fuzzing, taint tracing, and emulation to the
problem of bug ﬁnding. These techniques automate bug
ﬁnding in the sense that, with some up-front cost, they
carry out a search over time of software states with little
need for human intervention. The goal of each reﬁnement
or invention is to increase the effectiveness of tools when
they are used on real software. Yet ﬁnding vulnerabilities
at scale still appears out of reach, partly due to the human
effort required to effectively setup automated tools.
Our work focuses on human processes that build on
a foundation of automation. We choose to focus on au-
tonomous technologies (as opposed to other vulnerability
discovery techniques such as static analysis) because
we view them as holding great promise for scalability.
However, we by no means discourage the use of other
techniques, either alone or in connection with autonomy.
We propose a minor change to Votipka’s process [40]
by creating a deliberate software selection step we call
targeting. We encourage novice hackers to perform
a breadth-ﬁrst search of potential software targets to
accomplish only the essential-but-preliminary tasks that
allow automated analysis. We suggest bringing in more
experienced hackers to perform a deeper but more costly
analysis of select software only once novices have tried
and failed with automation. Our approach focuses the
most experienced practitioners on hard problems by
delegating other work to hackers with less experience;
they, in turn, generate work artifacts that are useful for
informing more advanced analysis. Due to the volume of
targets, all hackers have the opportunity to select software
suitable for their skill level, and team members have a
clear path for knowledge growth and coaching.
This paper describes our vulnerability-discovery
process along with the repeatable experiment that we
used to assess it. We found substantial evidence to claim a
breadth-ﬁrst search makes a superior targeting strategy in
the presence of automation. We also measured signiﬁcant
improvement in the conﬁdence of subjects who applied
our process to a vulnerability-discovery campaign.
After surveying related work in §2, we introduce our
process in §3. §3.1 describes a depth-ﬁrst strategy, and
§3.2 describes our breadth-ﬁrst strategy. §4 lays out the
design and execution of our experiment: the application
of our process with two teams of hackers applying two
strategies during two successive weeks. §5 describes our
results, and §6 concludes.
2 Related work
Votipka, et al. studied the interplay between testers, who
investigate software prior to release and hackers, who
investigate software after release. They derived from their
study a common vulnerability discovery process, which
we build on here [40, §V].
Manès, et al. provide a survey of many of the techniques
found in fuzzing tools [21]. For example, Mayhem [5]
and Driller [36] address the path explosion problem
in symbolic execution. Klees, et al. survey the fuzzing
literature to comment on the required procedure for good
scientiﬁc and evidence based research [20].
USENIX Association
29th USENIX Security Symposium    1129
l
e
v
e
l
l
l
i
k
S
Master
Journeyman
Apprentice
Automation
g
n
i
h
c
a
o
C
Figure 1: Practitioners, divided into apprentices, journey-
men, and master hackers; each represents a higher level
of skill and experience, and each mentors the level below
Avgerinos, et al. mention analysis at scale, speciﬁcally
how scaling analysis to thousands of software artifacts
makes any per-program manual labor impractical [1, §6.4].
Babic, et al. discuss a method to harness library code auto-
matically and at great scale [2]. Sawilla and Ou proposed
ASSETRANK, an algorithm that reveals the importance of
vulnerabilities present in a system [31]. The strategy we
propose builds on OSS-Fuzz’s idea of passing indicators
of vulnerability to human experts for remediation [32].
In this study, we extend Votipka’s vulnerability
discovery process, use modern tools referenced by Manès,
accept some amount of manual labor to make ﬁnding bugs
in real software artifacts tractable, and use statistical tests
to extrapolate our observations to the hacker community.
3 Vulnerability discovery process
We aim to discover ways to increase the effectiveness of
teams built on a foundation of automation (i.e., fuzzing
and related technologies) whose goal is to ﬁnd bugs in soft-
ware. Most interesting to us are bugs exploitable in a way
that circumvents a system’s security. We consider both
published and novel bugs, focusing on employed software
where vulnerabilities—published (n-day) or not (0-day)—
are the main concern. Here we describe our vulnerability
discovery process, based on Votikpa’s work. We also intro-
duce distinct two strategies that our experiment compared.
Observations led us to divide bug ﬁnders into three
categories: apprentices, journeymen, and masters, as
depicted in Figure 1. Collectively, we refer to these three
groups as hackers. Maximizing the productivity of each
skill level while enabling a progression from apprentice
to master over time was a key motivator to our process.
An apprentice hacker has a general computing
background and a basic understanding of how to apply
some number of automated software analysis tools. At the
core of an apprentice’s tool set are fuzzers. Apprentices
have limited experience in modifying software, and they
do not yet have a command of the internal workings of
the various build systems used for software development.
A journeyman hacker adds the ability to manipulate a
program to work with his tools. A journeyman can modify
source code or use binary patching to deal with obstacles
that thwart fuzzing, such as checksums, encryption, or
non-deterministic functionality. A journeyman routinely
modiﬁes targets to expose their attack surfaces.
The highest skill level, master, adds the ability to
manipulate or create tools in order to better investigate a
target program. Many existing tools were written by mas-
ters in need of a specialized approach to a particular piece
or class of software. We will use Alice as an apprentice,
James as a journeyman, and Meghan as a master hacker.
Other actors include leaders, who make targeting
decisions based on the work of hackers; analysts, who cor-
relate technical work with other resources such as blogs
and Common Vulnerabilities and Exposures (CVE); and
system support personnel, who manage automation jobs
and computing resources. Motivated by our observations
of the skill levels that comprise vulnerability-discovery
teams, we added a targeting step to Votipka’s vulnerability
discovery process [40], as shown in Figure 2.
Targeting Targeting selects software for investigation.
The term target is common among bug ﬁnders because
software targets are subject to an unusually careful
inspection that resembles an attack [28]. The goal of the
targeting phase is to divide a complex system or group
of complex systems into targets that can be individually
studied in later phases of the vulnerability-discovery
process. Even monolithic software artifacts decompose
into multiple targets: for example, a browser decomposes
into media libraries, TLS and networking libraries, an
HTML/CSS renderer, a JavaScript engine, and so on.
Experience shows that many or most teams have multiple
or many targets under consideration.
Only cursory information focused on how to perform
this division should be collected during the targeting
phase. Examples include the pervasiveness of existing
security research focused on the target; the availability
of target source code, bug trackers, and public developer
forums; and the impact of ﬁnding a vulnerability in the
target. The availability of the target itself; its dependencies
(e.g., software, hardware, and supporting resources);
and the tools necessary to interact with the target—both
automatically and manually—are other considerations.
The predicted Proﬁt of a vulnerability-ﬁnding effort is
proportional to the Likelihood and V alue of success and
inversely proportional to the projected T ime investment
and required Skill level.
P = (L×V )−(T ×S)
This model guides targeting and subsequent decisions
about how to proceed while maximizing return on
1130    29th USENIX Security Symposium
USENIX Association
Leader: Validates
and prioritizes
queue of targets.
Analyst: Reviews
CVEs. Hacker:
Gathers code and
reviews project.
Hacker: Builds and executes program. Reviews
features. Enumerates components and I/O channels.
Writes fuzzing harness. Labels lines of research as
suitable for apprentice, journeyman, or master.
Targeting
*
Start
. . .
. . .
. . .
Information
gathering
†
Program
understanding
†
Attack surface
analysis
Obstacle?
†
Yes
Leader: Determines bug’s
value and likelihood;
compares against projected
investment.
Hacker: Performs root-cause analysis to
produce a report on the likelihood of poten-
tial payoff and projected time investment
to achieve payoff. Documents results.
Yes
Worth-
while?
No
Stop
Reporting
†
Vulnerability
recognition
†
No
Automated
exploration
†
System Support:
Manages fuzzing
jobs and computing
resources.
Figure 2: Our vulnerability-discovery process adds targeting (*) to the steps of Votipka, et al. (†) [40, §V].
investment.
Not all hackers are created equal, and building expertise
in software security can take years of effort, experience,
and coaching [28]. A targeting strategy ought to boost
overall productivity across all skill levels. We wanted to
derive a sufﬁciently large number of software targets to
allow hackers of varying skill levels to select work that
aligns with both their ability and interest.
Ultimately, we arrived at a strategy that coupled the
freedom of target choice with a “fail fast” team culture
and an incentive for producing rapid results. Thus our tar-
geting phase allows teams to self-organize, and it enables
a more effective use of journeyman- and master-level
hackers’ scarce time. We describe a depth-ﬁrst strategy
in §3.1 and our favored breadth-ﬁrst strategy in §3.2.
Information gathering The ﬁrst steps individual hack-
ers and analysts take during the vulnerability-discovery
process is to collect additional information about the
target, this time with an eye toward decision making
during later phases. Key among this information are
general details about the target’s development, prevalence,
and known current or previous defects, along with any
security research already complete [40].
Existing analysis can quickly advance the understand-
ing of obstacles, along with the methods of overcoming
them. For instance, work to fuzz the OpenSSH dæ-
mon [26] describes eleven non-trivial techniques to
harness targets for fuzzing. When considering a team of
mixed proﬁciency, descriptive guides such as this allow a
novice to begin work that would otherwise require a more
experienced hacker.
Scenario A Alice begins investigating a piece of soft-
ware that provides an NTP service. She notes the ver-
sion in common use, reviews the National Vulnerabil-
ity Database for known vulnerabilities, and records the
primary programming language used in the project.
Program understanding Hackers next focus on
gaining knowledge of the target’s operation and design.
Of interest is how the target is used as it was intended,
more advanced use cases and conﬁguration options, and
the general design of the target software. Information
gathered during this phase can come from documentation,
source code, online forums, users, developers [40], and
other sources. Program understanding and the next phase,
attack surface analysis, make up an iterative cycle within
the vulnerability discovery process; Figure 2 illustrates
this with the Obstacle decision point.
Scenario A (cont.) Alice installs the NTP service by
downloading its source code from an online repository
and running ./configure; make. She references the
usage instructions to interact with the software.
Scenario B Working on a separate project, James
compiles a browser after reading preliminary notes by
Alice. This takes some work as his Linux distribution
did not provide a required library. He identiﬁes the
browser’s JavaScript engine and HTML renderer, and
he notes the libraries used to decode various media
USENIX Association
29th USENIX Security Symposium    1131
formats. James also notes that the default build makes
use of Address Space Layout Randomization (ASLR),
non-executable stacks, and stack canaries.
Attack surface analysis
Investigating a program’s at-
tack surface involves devising ways to provide input to
portions of the target program. In many cases, this takes
the form of a fuzzing harness, also known as a driver appli-
cation [21], which directs the inputs a fuzzer generates to a
portion of the program’s attack surface. The practical exe-
cution of this phase diverges among hackers of varied skill.
Our process asks apprentices to apply known tools until
an obstacle prevents them from further process. Their strat-
egy is to give up quickly when progress stops, document
their successful work, and move on to the next target.
Journeymen consume the documentation produced
by the apprentices, allowing them to immediately
apply higher-order analysis and continue the program
understanding–attack surface analysis cycle.
Projects that reach the master level either are ex-
ceptionally important or have exceeded other hackers’
ability to exploit despite clear indications of buggy
behavior. A master should always enter the program
understanding–attack surface analysis cycle with a
plethora of documentation and other products generated
by apprentices and journeymen. The master’s time is thus
spent doing tasks only a master could perform.
Some literature suggests that to even begin vulnerability
discovery, a person must already have the skill we describe
as a master’s: “Although fuzzing tools are more common,
people typically do not use off-the-shelf tools; they prefer
making their own fuzzers . . . [11]” We found counterex-
amples where apprentices and journeymen were able to
progress through every phase of vulnerability discovery.
In other cases, they provided clear value to later work by