### Signature Learning by Training Maliciously
In *Recent Advances in Intrusion Detection*, Vol. 4219. Springer Berlin Heidelberg, Berlin, Heidelberg. [DOI: 10.1007/11856214_5](https://doi.org/10.1007/11856214_5). Series Title: Lecture Notes in Computer Science.

### References

1. **Sinno Jialin Pan and Qiang Yang. 2009.** A survey on transfer learning. *IEEE Transactions on Knowledge and Data Engineering* 22, 10 (2009), 1345‚Äì1359.
2. **Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. 2017.** Practical black-box attacks against machine learning. In *Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security*. 506‚Äì519.
3. **F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011.** Scikit-learn: Machine Learning in Python. *Journal of Machine Learning Research* 12 (2011), 2825‚Äì2830.
4. **R. Perdisci, D. Dagon, Wenke Lee, P. Fogla, and M. Sharif. 2006.** Misleading worm signature generators using deliberate noise injection. In *2006 IEEE Symposium on Security and Privacy (S&P‚Äô06)*. IEEE, Berkeley/Oakland, CA. [DOI: 10.1109/SP.2006.26](https://doi.org/10.1109/SP.2006.26).
5. **Neehar Peri, Neal Gupta, W Ronny Huang, Liam Fowl, Chen Zhu, Soheil Feizi, Tom Goldstein, and John P Dickerson. 2020.** Deep k-nn defense against clean-label data poisoning attacks. In *European Conference on Computer Vision*. Springer, 55‚Äì70.
6. **Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019.** Language models are unsupervised multitask learners. *OpenAI Blog* 1, 8 (2019), 9.
7. **Elan Rosenfeld, Ezra Winston, Pradeep Ravikumar, and Zico Kolter. 2020.** Certified robustness to label-flipping attacks via randomized smoothing. In *International Conference on Machine Learning*. PMLR, 8230‚Äì8241.
8. **Benjamin IP Rubinstein, Blaine Nelson, Ling Huang, Anthony D Joseph, Shing-hon Lau, Satish Rao, Nina Taft, and J Doug Tygar. 2009.** Antidote: Understanding and defending against poisoning of anomaly detectors. In *Proceedings of the 9th ACM SIGCOMM Conference on Internet Measurement*. 1‚Äì14.
9. **Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. 2018.** Mobilenetv2: Inverted residuals and linear bottlenecks. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*. 4510‚Äì4520.
10. **Lea Schonherr, Katharina Kohls, Steffen Zeiler, Thorsten Holz, and Dorothea Kolossa. 2018.** Adversarial attacks against automatic speech recognition systems via psychoacoustic hiding. *arXiv preprint arXiv:1808.05665* (2018).
11. **Roei Schuster, Tal Schuster, Yoav Meri, and Vitaly Shmatikov. 2020.** Humpty Dumpty: Controlling word meanings via corpus poisoning. In *2020 IEEE Symposium on Security and Privacy (SP)*. IEEE, 1295‚Äì1313.
12. **Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, and Tom Goldstein. 2018.** Poison frogs! Targeted clean-label poisoning attacks on neural networks. In *Advances in Neural Information Processing Systems*. 6103‚Äì6113.
13. **Yanyao Shen and Sujay Sanghavi. 2019.** Learning with bad training data via iterative trimmed loss minimization. In *International Conference on Machine Learning*. PMLR, 5739‚Äì5748.
14. **Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017.** Membership inference attacks against machine learning models. In *2017 IEEE Symposium on Security and Privacy (SP)*. IEEE, 3‚Äì18.
15. **Karen Simonyan and Andrew Zisserman. 2015.** Very deep convolutional networks for large-scale image recognition. In *3rd International Conference on Learning Representations (ICLR 2015)*, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, Yoshua Bengio and Yann LeCun (Eds.). [arXiv:1409.1556](http://arxiv.org/abs/1409.1556).
16. **David Solans, Battista Biggio, and Carlos Castillo. 2020.** Poisoning attacks on algorithmic fairness. *arXiv preprint arXiv:2004.07401* (2020).
17. **Congzheng Song and Vitaly Shmatikov. 2020.** Overlearning reveals sensitive attributes. In *International Conference on Learning Representations*. [OpenReview](https://openreview.net/forum?id=SJeNz04tDS).
18. **N. Srndic and P. Laskov. 2014.** Practical evasion of a learning-based classifier: A case study. In *Proc. IEEE Security and Privacy Symposium*.
19. **Octavian Suciu, Radu Marginean, Yigitcan Kaya, Hal Daume III, and Tudor Dumitras. 2018.** When does machine learning FAIL? Generalized transferability for evasion and poisoning attacks. In *27th USENIX Security Symposium (USENIX Security 18)*. 1299‚Äì1316.
20. **Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. 2014.** Intriguing properties of neural networks. In *International Conference on Learning Representations*. [arXiv:1312.6199](http://arxiv.org/abs/1312.6199).
21. **Mingxing Tan and Quoc Le. 2019.** EfficientNet: Rethinking model scaling for convolutional neural networks. In *International Conference on Machine Learning*. PMLR, 6105‚Äì6114.
22. **Brandon Tran, Jerry Li, and Aleksander Madry. 2018.** Spectral signatures in backdoor attacks. In *Proceedings of the 32nd International Conference on Neural Information Processing Systems (NIPS'18)*. Curran Associates Inc., Montr√©al, Canada, 8011‚Äì8021.
23. **Alexander Turner, Dimitris Tsipras, and Aleksander Madry. 2019.** Clean-Label Backdoor Attacks. In *ICLR*.
24. **Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017.** Attention is all you need. In *Advances in Neural Information Processing Systems*. 5998‚Äì6008.
25. **Akshaj Kumar Veldanda, Kang Liu, Benjamin Tan, Prashanth Krishnamurthy, Farshad Khorrami, Ramesh Karri, Brendan Dolan-Gavitt, and Siddharth Garg. 2020.** NNoculation: Broad spectrum and targeted treatment of backdoored DNNs. *arXiv:2002.08313* [cs] (Feb. 2020). [arXiv:2002.08313](http://arxiv.org/abs/2002.08313).
26. **Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y. Zhao. 2019.** Neural Cleanse: Identifying and mitigating backdoor attacks in neural networks. In *2019 IEEE Symposium on Security and Privacy (SP)*. IEEE, San Francisco, CA, USA, 707‚Äì723. [DOI: 10.1109/SP.2019.00031](https://doi.org/10.1109/SP.2019.00031).
27. **Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020.** Transformers: State-of-the-art natural language processing. In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations*. Association for Computational Linguistics, Online, 38‚Äì45. [DOI: 10.18653/v1/2020.emnlp-demos.6](https://doi.org/10.18653/v1/2020.emnlp-demos.6).
28. **Huang Xiao, Battista Biggio, Gavin Brown, Giorgio Fumera, Claudia Eckert, and Fabio Roli. 2015.** Is feature selection secure against training data poisoning? In *International Conference on Machine Learning*. 1689‚Äì1698.
29. **Saining Xie, Ross Girshick, Piotr Doll√°r, Zhuowen Tu, and Kaiming He. 2017.** Aggregated residual transformations for deep neural networks. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*. 1492‚Äì1500.
30. **Weilin Xu, Yanjun Qi, and David Evans. 2016.** Automatically evading classifiers. In *Proceedings of the 2016 Network and Distributed Systems Symposium*. 21‚Äì24.
31. **Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.** XLNet: Generalized autoregressive pretraining for language understanding. In *Advances in Neural Information Processing Systems*. 5754‚Äì5764.
32. **Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. 2018.** Privacy risk in machine learning: Analyzing the connection to overfitting. In *2018 IEEE 31st Computer Security Foundations Symposium (CSF)*. IEEE, 268‚Äì282.
33. **Zhifei Zhang, Yang Song, and Hairong Qi. 2017.** Age progression/regression by conditional adversarial autoencoder. In *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*. IEEE.

### Appendix: Defenses

#### A.1 Proof for Section 7
In this section, we describe a theoretical model in which subpopulation attacks are impossible to defend against. The model is closely related to two existing theoretical models:

1. **Corrupted Clusterable Dataset Model**: This model was used to analyze the robustness to label noise of neural networks trained with early stopping [Reference 40].
2. **Subpopulation Mixture Model**: This model was used to justify the importance of memorization in learning and explain the gap between differentially private learning and non-private learning [Reference 20, STOC 2020].

Our model generalizes the first model and simplifies the second one. The key components of our model are:
- **Datasets**: Consisting of potentially noisy subpopulations of data.
- **Classifiers**: Assigning a uniform class to each cluster.

**Definition A.1 (Noisy ùëò-Subpopulation Mixture Distribution)**: A noisy ùëò-subpopulation mixture distribution \( D \) over \( X \times Y \) consists of \( k \) subpopulations \(\{D_i\}_{i=1}^k\) with distinct, known supports over \( X \), unknown mixture weights, and labels drawn from subpopulation-specific Bernoulli distributions.

- **Supports**: \(\{X_i \subset X\}_{i=1}^k\). By distinct supports, we mean that \(\forall i, j \in [k], i \neq j, \text{Supp}_X(D_i) \cap \text{Supp}_X(D_j) = \emptyset\).
- **Function**: There exists a function \( C_D : X \to [k] \) returning the subpopulation of the given sample.
- **Mixture Weights**: \(\{\alpha_i\}_{i=1}^k\); note that \(\sum_{i=1}^k \alpha_i = 1\).
- **Full Distribution**: \( D = \sum_{i=1}^k \alpha_i D_i \).

**Definition A.2 (ùëò-Subpopulation Mixture Learner)**: A subpopulation mixture learner \( A \) takes as input a dataset \( D \) of size \( n \) and the subpopulation function \( C_D \) of a noisy \( k \)-subpopulation mixture distribution \( D \), and returns a classifier \( f : [k] \to \{0, 1\} \). On a fresh sample \( x \), the classifier returns \( f(C_D(x)) \). The learner is locally dependent if \( f(i) \) depends only on \(\{y | (x, y) \in D \cap \text{Supp}_X(D_i)\}\).

The learners considered in both [40] (particular shallow wide neural networks) and [20] (\( k \)-nearest neighbors, mixture models, overparameterized linear models) are locally dependent.

**Theorem A.3**: For any dataset \( D \) of size \( n \) drawn from a noisy \( k \)-subpopulation mixture distribution and subpopulation function \( C_D \), there exists a subpopulation poisoning attack \( D_p \) of size \( \leq n/k \) that causes all locally dependent \( k \)-subpopulation mixture learners \( A \) that minimize 0-1 loss in binary classification to return:
\[ A(D \cup D_p) = A(D) \]
with probability \( \frac{1}{2} \).

**Proof**:
- Assume without loss of generality that \( K = \arg \min_i |D \cap \text{Supp}_X(D_i)| \), and write \( D_K = D \cap \text{Supp}_X(D_i) \).
- Our attack operates by taking \( D_K \) and flipping all of its labels, producing \( D_p^K \).
- Suppose we provide the learner with the original dataset \( D \) and the returned classifier is \( f \). On the other hand, when we provide the learner with the poisoned dataset \( D' = D \cup D_p^K \), it returns \( f' \).
- Datasets \( D \) and \( D' \) differ in \( |D_K| \) records, which is \( \leq n/k \) according to the pigeonhole principle (\( K \) being the smallest subpopulation).
- From the properties assumed about the learner \( A \), we have:
  \[ A(D') = A(D) \]
  with probability \( > \frac{1}{2} \), which implies that learners \( f \) and \( f' \) return the same label for subpopulation \( K \): \( f(K) = f'(K) \).
- However, because \( f \) and \( f' \) are locally dependent and make their decisions based on subpopulations to minimize the 0-1 loss, adding enough points in \( D' \) to flip the decision results in:
  \[ f'(K) = 1 - f(K) \]
- These two statements result in a contradiction, proving the first part of the theorem.

**Improving Bounds on the Smallest Subpopulation**:
- For a sample from a noisy \( k \)-subpopulation mixture distribution, consider the smallest subpopulation \( j = \arg \min_i \alpha_i \) and its mixture coefficient \( \alpha = \alpha_j \).
- Use the multiplicative Chernoff bound with \( \delta > 0 \):
  \[
  \Pr[|D \cap \text{Supp}_X(D_j)| > 2\alpha n] \leq \exp\left(-\frac{\alpha n \delta^2}{2 + \delta}\right)
  \]
- Setting \( \mu = \alpha_j n = \alpha n \) and \( \delta = 1 \) in the above Chernoff bound gives:
  \[
  \Pr[X > (1 + \delta)\mu] \leq 1 - \exp(-5) = 99.3\%
  \]
- The pigeonhole principle requires a poisoning attack of size \( 200 \).

#### A.2 Experimental Defenses Addendum
The version of TRIM/ILTM used in our experiments can be found in Algorithm 4.

**Figure 2** illustrates a potential failure mode for availability attack defenses, related to the theoretical impossibility result but using logistic regression, a model not captured by our theoretical results. The figure portrays three logistic regression models:
- **Unpoisoned Model**
- **Poisoned Model**
- **Poisoned Model with TRIM Applied**

Trained on a synthetic binary classification dataset consisting of three subpopulations, each subpopulation has 20 data points, and 30 points are used to poison the second subpopulation. The difference between Figures 2a and 2b indicates that the subpopulation attack is fairly successful.

**Experiment Results**:
| Population | Fraction Trimmed (FT) | Original Target Attack | Test Attack Target |
|------------|-----------------------|------------------------|--------------------|
| 1          | 1%                    | 0.757                  | 0.984              |
| 1          | 2%                    | 0.757                  | 0.984              |
| 1          | 5%                    | 0.757                  | 0.984              |
| 1          | 10%                   | 0.757                  | 0.984              |
| 2          | 1%                    | 0.619                  | 0.890              |
| 2          | 2%                    | 0.619                  | 0.890              |
| 2          | 5%                    | 0.619                  | 0.890              |
| 2          | 10%                   | 0.619                  | 0.890              |
| 3          | 1%                    | 0.710                  | 0.890              |
| 3          | 2%                    | 0.710                  | 0.890              |
| 3          | 5%                    | 0.710                  | 0.890              |
| 3          | 10%                   | 0.710                  | 0.890              |

This table shows the performance of the models under different levels of trimming, indicating the effectiveness of the attack and the limitations of the defense mechanisms.