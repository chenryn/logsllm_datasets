Signature Learning by Training Maliciously. In Recent Advances in Intrusion
Detection. Vol. 4219. Springer Berlin Heidelberg, Berlin, Heidelberg. https:
//doi.org/10.1007/11856214_5 Series Title: Lecture Notes in Computer Science.
[49] Sinno Jialin Pan and Qiang Yang. 2009. A survey on transfer learning. IEEE
Transactions on knowledge and data engineering 22, 10 (2009), 1345‚Äì1359.
[50] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay
Celik, and Ananthram Swami. 2017. Practical black-box attacks against machine
learning. In Proceedings of the 2017 ACM on Asia conference on computer and
communications security. 506‚Äì519.
[51] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M.
Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cour-
napeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine
Learning in Python. Journal of Machine Learning Research 12 (2011), 2825‚Äì2830.
[52] R. Perdisci, D. Dagon, Wenke Lee, P. Fogla, and M. Sharif. 2006. Misleading worm
signature generators using deliberate noise injection. In 2006 IEEE Symposium on
Security and Privacy (S&P‚Äô06). IEEE, Berkeley/Oakland, CA. https://doi.org/10.
1109/SP.2006.26
[53] Neehar Peri, Neal Gupta, W Ronny Huang, Liam Fowl, Chen Zhu, Soheil Feizi,
Tom Goldstein, and John P Dickerson. 2020. Deep k-nn defense against clean-
label data poisoning attacks. In European Conference on Computer Vision. Springer,
55‚Äì70.
[54] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI
Blog 1, 8 (2019), 9.
[55] Elan Rosenfeld, Ezra Winston, Pradeep Ravikumar, and Zico Kolter. 2020. Certi-
fied robustness to label-flipping attacks via randomized smoothing. In Interna-
tional Conference on Machine Learning. PMLR, 8230‚Äì8241.
[56] Benjamin IP Rubinstein, Blaine Nelson, Ling Huang, Anthony D Joseph, Shing-
hon Lau, Satish Rao, Nina Taft, and J Doug Tygar. 2009. Antidote: understanding
and defending against poisoning of anomaly detectors. In Proceedings of the 9th
ACM SIGCOMM conference on Internet measurement. 1‚Äì14.
[57] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-
Chieh Chen. 2018. Mobilenetv2: Inverted residuals and linear bottlenecks. In
Proceedings of the IEEE conference on computer vision and pattern recognition.
4510‚Äì4520.
[58] Lea Schonherr, Katharina Kohls, Steffen Zeiler, Thorsten Holz, and Dorothea
Kolossa. 2018. Adversarial attacks against automatic speech recognition systems
via psychoacoustic hiding. arXiv preprint arXiv:1808.05665 (2018).
[59] Roei Schuster, Tal Schuster, Yoav Meri, and Vitaly Shmatikov. 2020. Humpty
Dumpty: Controlling word meanings via corpus poisoning. In 2020 IEEE Sympo-
sium on Security and Privacy (SP). IEEE, 1295‚Äì1313.
[60] Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer,
Tudor Dumitras, and Tom Goldstein. 2018. Poison frogs! targeted clean-label poi-
soning attacks on neural networks. In Advances in Neural Information Processing
Systems. 6103‚Äì6113.
[61] Yanyao Shen and Sujay Sanghavi. 2019. Learning with bad training data via
iterative trimmed loss minimization. In International Conference on Machine
Learning. PMLR, 5739‚Äì5748.
[62] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. Mem-
bership inference attacks against machine learning models. In 2017 IEEE Sympo-
sium on Security and Privacy (SP). IEEE, 3‚Äì18.
[63] Karen Simonyan and Andrew Zisserman. 2015. Very Deep Convolutional Net-
works for Large-Scale Image Recognition. In 3rd International Conference on Learn-
ing Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track
Proceedings, Yoshua Bengio and Yann LeCun (Eds.). http://arxiv.org/abs/1409.1556
[64] David Solans, Battista Biggio, and Carlos Castillo. 2020. Poisoning Attacks on
Algorithmic Fairness. arXiv preprint arXiv:2004.07401 (2020).
[65] Congzheng Song and Vitaly Shmatikov. 2020. Overlearning Reveals Sensi-
tive Attributes. In International Conference on Learning Representations. https:
//openreview.net/forum?id=SJeNz04tDS
[66] N. Srndic and P. Laskov. 2014. Practical Evasion of a Learning-Based Classifier:
A Case Study. In Proc. IEEE Security and Privacy Symposium.
[67] Octavian Suciu, Radu Marginean, Yigitcan Kaya, Hal Daume III, and Tudor Dumi-
tras. 2018. When does machine learning {FAIL}? generalized transferability for
evasion and poisoning attacks. In 27th {USENIX} Security Symposium ({USENIX}
Security 18). 1299‚Äì1316.
[68] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
Ian Goodfellow, and Rob Fergus. 2014. Intriguing properties of neural networks.
In International Conference on Learning Representations. http://arxiv.org/abs/1312.
6199
[69] Mingxing Tan and Quoc Le. 2019. Efficientnet: Rethinking model scaling for
convolutional neural networks. In International Conference on Machine Learning.
PMLR, 6105‚Äì6114.
[70] Brandon Tran, Jerry Li, and Aleksander Madry. 2018. Spectral signatures in
backdoor attacks. In Proceedings of the 32nd International Conference on Neural In-
formation Processing Systems (NIPS‚Äô18). Curran Associates Inc., Montr√©al, Canada,
8011‚Äì8021.
[71] Alexander Turner, Dimitris Tsipras, and Aleksander Madry. 2019. Clean-Label
Backdoor Attacks. In ICLR.
[72] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information processing systems. 5998‚Äì6008.
[73] Akshaj Kumar Veldanda, Kang Liu, Benjamin Tan, Prashanth Krishnamurthy,
Farshad Khorrami, Ramesh Karri, Brendan Dolan-Gavitt, and Siddharth Garg.
2020. NNoculation: Broad Spectrum and Targeted Treatment of Backdoored
DNNs. arXiv:2002.08313 [cs] (Feb. 2020). http://arxiv.org/abs/2002.08313 arXiv:
2002.08313.
[74] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao
Zheng, and Ben Y. Zhao. 2019. Neural Cleanse: Identifying and Mitigating
Backdoor Attacks in Neural Networks. In 2019 IEEE Symposium on Security and
Privacy (SP). IEEE, San Francisco, CA, USA, 707‚Äì723. https://doi.org/10.1109/SP.
2019.00031
[75] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe
Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu,
Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
and Alexander Rush. 2020. Transformers: State-of-the-Art Natural Language
Processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural
Language Processing: System Demonstrations. Association for Computational
Linguistics, Online, 38‚Äì45. https://doi.org/10.18653/v1/2020.emnlp-demos.6
[76] Huang Xiao, Battista Biggio, Gavin Brown, Giorgio Fumera, Claudia Eckert, and
Fabio Roli. 2015. Is feature selection secure against training data poisoning?. In
International Conference on Machine Learning. 1689‚Äì1698.
[77] Saining Xie, Ross Girshick, Piotr Doll√°r, Zhuowen Tu, and Kaiming He. 2017.
Aggregated residual transformations for deep neural networks. In Proceedings of
the IEEE conference on computer vision and pattern recognition. 1492‚Äì1500.
[78] Weilin Xu, Yanjun Qi, and David Evans. 2016. Automatically evading classifiers.
In Proceedings of the 2016 Network and Distributed Systems Symposium. 21‚Äì24.
[79] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov,
and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for language
understanding. In Advances in neural information processing systems. 5754‚Äì5764.
[80] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. 2018. Privacy
risk in machine learning: Analyzing the connection to overfitting. In 2018 IEEE
31st Computer Security Foundations Symposium (CSF). IEEE, 268‚Äì282.
[81] Zhifei Zhang, Yang Song, and Hairong Qi. 2017. Age Progression/Regression by
Conditional Adversarial Autoencoder. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR). IEEE.
A APPENDIX FOR DEFENSES
A.1 Proof for Section 7
In this section, we will describe a theoretical model in which sub-
population attacks are impossible to defend against. The model is
closely related to two existing theoretical models. First is the cor-
rupted clusterable dataset model of [40], which was used to analyze
the robustness to label noise of neural networks trained with early
stopping. The second relevant model is the subpopulation mixture
model of [20] (appearing at STOC 2020), which they used to justify
the importance of memorization in learning and explain the gap be-
tween differentially private learning and nonprivate learning. Our
model generalizes the [40] model, and simplifies the [20] model.
The two key components of our model are the datasets, which
consist of potentially noisy subpopulations of data, and the classi-
fiers, which assign a uniform class to each cluster. [40] show that
the neural network architecture and training procedure they use
produces this set of classifiers if label noise is not too large. [20]
shows that overparameterized linear models, k-nearest neighbors,
and mixture models are examples of these classifiers, conjecturing
(based on empirical evidence) that neural networks are as well.
Definition A.1 (Noisy ùëò-Subpopulation Mixture Distribution). A
noisy ùëò-subpopulation mixture distribution D over X √ó Y consists
ùëñ=1, with distinct, known, supports over X,
of ùëò subpopulations {Dùëñ}ùëò
(unknown) mixture weights, and labels drawn from subpopulation-
specific Bernoulli distributions.
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea3118We write the supports of each subpopulation {Xùëñ ‚äÇ X}ùëò
ùëñ=1. By
distinct supports, we mean that‚àÄùëñ, ùëó ‚àà [ùëò] with ùëñ ‚â† ùëó, ùëÜùë¢ùëùùëùX(Dùëñ)‚à©
ùëÜùë¢ùëùùëùX(Dùëó) = ‚àÖ. Furthermore, because the supports are known,
there exists a function ùê∂D : X ‚Üí [ùëò] returning the subpopulation
of the given sample.
ùëñ=1; note
ùëñ=1. The full distribution can be written as
thatùëñ ùõºùëñ = 1. The subpopulation-specific label distributions are
D =ùëñ ùõºùëñDùëñ.
We write the unknown mixture weights as ùú∂ = {ùõºùëñ}ùëò
written {Bernoulli(ùëùùëñ)}ùëò
Notice that the existence of ùê∂D implies that the classification task
on D is exactly the problem of estimating the correct label for each
subpopulation. We formalize this by introducing ùëò-subpopulation
mixture learners.
Definition A.2 (ùëò-Subpopulation Mixture Learner). A subpopula-
tion mixture learner A takes as input a dataset ùê∑ of size ùëõ, and
the subpopulation function ùê∂D of a noisy ùëò-subpopulation mixture
distribution D, and returns a classifier ùëì : [ùëò] ‚Üí {0, 1}. On a fresh
sample ùë•, the classifier returns ùëì (ùê∂D(ùë•)). We call the learner locally
dependent if ùëì (ùëñ) depends only on {ùë¶|(ùë•, ùë¶) ‚àà ùê∑ ‚à© ùëÜùë¢ùëùùëùX(Dùëñ)},
that is, only labels from those data points which belong to the
specific subpopulation.
The learners considered in both [40] (particular shallow wide
neural networks) and [20] (ùëò-nearest neighbors, mixture models,
overparameterized linear models) are locally dependent.
For our main theorem, we consider learners that are binary
classifiers, and select a label that minimizes the cumulative error
on each subpopulation using the 0-1 loss.
Theorem A.3. For any dataset ùê∑ of size ùëõ drawn from a noisy
ùëò-subpopulation mixture distribution and subpopulation function ùê∂D,
there exists a subpopulation poisoning attack ùê∑ùëù of size ‚â§ ùëõ/ùëò that
causes all locally dependent ùëò-subpopulation mixture learners A that
minimize 0-1 loss in binary classification return:
A(ùê∑ ‚à™ ùê∑ùëù) = A(ùê∑) with probability 
1
2 .
Assume without loss of generality that ùêæ = arg minùëñ |ùê∑ ‚à©
ùëÜùë¢ùëùùëùX(Dùëñ)|, and write ùê∑ùêæ = ùê∑ ‚à©ùëÜùë¢ùëùùëùX(Dùëñ). Our attack operates
ùêæ.
by taking ùê∑ùêæ and flipping all of its labels, producing ùê∑ùëù
Suppose we provide the learner with the original dataset ùê∑ and
the returned classifier is ùëì . On the other hand, when we provide
the learner with the poisoned dataset ùê∑‚Ä≤ = ùê∑||ùê∑ùëù
ùêæ, it returns ùëì ‚Ä≤.
Datasets ùê∑ and ùê∑‚Ä≤ differ in |ùê∑ùêæ| records, which is ‚â§ ùëõ/ùëò, accord-
ing to the pigeonhole principle (ùêæ being the smallest subpopulation).
From the properties we assumed about the learner A , we have:
A(ùê∑‚Ä≤) = A(ùê∑), with probability >
1
2 ,
which implies that learners ùëì and ùëì ‚Ä≤ return the same label for
subpopulation ùêæ: ùëì (ùêæ) = ùëì ‚Ä≤(ùêæ).
On the other hand, the learners ùëì and ùëì ‚Ä≤ are locally dependent,
and make their decisions only based on subpopulations to minimize
the 0-1 loss. Because we added in ùê∑‚Ä≤ enough points in the subpop-
ulation ùêæ to flip the decision, it turns out that: ùëì ‚Ä≤(ùêæ) = 1 ‚àí ùëì (ùêæ).
But these two statements result in a contradiction, which proves
the first part of the theorem.
We now turn to the second part of the theorem statement, to
improve the bounds on the size of the smallest subpopulation. This
argument is particularly powerful when the mixture distribution
does not have uniform weights.
For a sample from a noisy ùëò-subpopulation mixture distribution,
consider the smallest subpopulation ùëó = arg minùëñ ùõºùëñ and its mixture
coefficient ùõº = ùõº ùëó.
ùëñ=1 Bernoulli(ùõº ùëó).
We use the following multiplicative Chernoff bound, with ùõø > 0:
The number of points in subpopulation ùëó isùëõ
(cid:19)
‚àíùõø2ùúá
2ùõø
Pr[|ùê∑ ‚à© ùëÜùë¢ùëùùëùX(Dùëó)| > 2ùõºùëõ] ‚â§ exp(cid:16)‚àí ùõºùëõ
Setting ùúá = ùõº ùëóùëõ = ùõºùëõ and ùõø = 1 in the above Chernoff bound gives
Pr[ùëã > (1 + ùõø)ùúá]  1 ‚àí exp(‚àí5) = 99.3%, whereas the pigeonhole principle requires
a poisoning attack of size 200.
A.2 Experimental Defenses Addendum
The version of TRIM/ILTM we use in our experiments can be found
in Algorithm 4.
We illustrate a potential failure mode for availability attack de-
fenses in Figure 2; this failure mode is related to the theoretical
impossibility result, but uses logistic regression, a model not cap-
tured by our theoretical results. The figure portrays three logistic
regression models - an unpoisoned model, a poisoned model, and
a poisoned model with TRIM applied - trained on a synthetic bi-
nary classification dataset consisting of three subpopulations. Each
subpopulation consists of 20 data points, and 30 points are used to
poison the second subpopulation. The difference between Figures
2a and 2b indicates that the subpopulation attack is fairly successful,
Session 11D: Data Poisoning and Backdoor Attacks in ML CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea3119Experiment
Pop: 1 - FT: 1%
Pop: 1 - FT: 2%
Pop: 1 - FT: 5%
Pop: 1 - FT: 10%
Pop: 2 - FT: 1%
Pop: 2 - FT: 2%
Pop: 2 - FT: 5%
Pop: 2 - FT: 10%
Pop: 3 - FT: 1%
Pop: 3 - FT: 2%
Pop: 3 - FT: 5%
Pop: 3 - FT: 10%
Original Target Attack Test Attack Target
0.757
0.757
0.757
0.757
0.619
0.619
0.619
0.619
0.710
0.710
0.710
0.710
0.984
0.984
0.984
0.984
0.890
0.890
0.890
0.890
0.890
0.890
0.890