settings:
(1) The memory required by our sketch depends mainly on the
desired approximation error and is sublinear with respect to
N , the number of entries in the dataset.
(2) The sketch can be efficiently constructed in a single pass
through the data, without auxiliary data structures or addi-
tional storage.
(3) The sketches are mergeable - given a sketch S1 of dataset
D1 and S2 of D2, we can obtain a sketch of the combined
dataset D1 ∪ D2 by simply adding the counters of S1 and
S2 [2].
It is well-known that these properties are sufficient to enable fast,
distributed implementations of the algorithm [2]. However, these
properties also enable privacy in the distributed setting. Thanks to
federated learning, which requires private aggregation of gradients,
a large number of solutions have recently been proposed for the
private aggregation problem. Our sketch is compatible with most of
these solutions. There are two approaches of interest: multi-party
protocols and local noise addition.
Local Noise Addition: This scenario is very well-studied be-
cause federated learning requires private centralized aggregation
of gradient updates. In the federated learning setting, users add
noise to their gradient updates locally before sending the private
update to an aggregator. This may be done by adding Gaussian or
Binomial noise to obtain local (ϵ, β)-differential privacy as in [1].
These approaches directly apply to our sketch in the same way that
they have been applied to the Count Sketch by [27].
Secure Aggregation: If we wish to perform aggregation with-
out a trusted central server, we can also use multi-party protocols
to securely compute the sum of count values [7]. However, we must
generate noise via a distributed privacy mechanism such as the
one proposed in [16] to ensure that these sums preserve privacy
(in addition to being computed securely). Recent work by Choi et.
al. addresses the specific case of multi-party based aggregation for
sketches and is directly applicable to our sketch. To ensure that
the count values are protected by ϵ-differential privacy, Choi et. al.
add quantized gamma noise to each count before running the MPC
protocol so that the sum of gamma noises is Laplace-distributed,
then run the MPC protocol to merge the sketches. Our sketches fit
into this framework because we only need to run the MPC protocol
R × W times, once for each count.
In summary, our sketch is compatible with a large amount of
existing work on private sum aggregation. If RACE sketches are
constructed in distributed settings, we do not lose the ability to
release private sketches.
Although RACE can only estimate LSH kernels, the space of
LSH kernels is large enough to be useful for machine learning. For
example, RACE performed well on classification tasks for ϵ > 10−1,
providing a competitive utility tradeoff for practical privacy budgets.
For linear regression, RACE outperforms objective perturbation, a
well-established and general method for private learning [8], but not
AdaSSP or AdaOPS, which are specialized methods that only work
for linear regression but deliver the best possible performance [43].
Therefore, RACE can perform regression and classification with
good (though not necessarily state-of-the-art) accuracy.
Function Release at Scale: Our most important result is that
the private RACE sketch is orders of magnitude faster than com-
peting algorithms for function release. Although PFDA and the
Bernstein mechanism have the strongest theoretical error bounds,
they required days to produce the experimental results in Figure 3
while RACE and PrivBayes required a few seconds. This is a serious
barrier in practice - it would require > 2128 ≈ 1038 computations
and over one billion exabytes of memory to run the Bernstein mech-
anism on the UCI gas dataset. As a result, we were unable to run
these methods on the covtype dataset without sampling or dimen-
sionality reduction. The graphs in Table 4, which are much larger,
are utterly infeasible for all methods other than RACE, which only
requires a few minutes to run. Our sketch has a small memory
footprint, inexpensive streaming updates and a fast distributed
implementation.
8 DISCUSSION
Our experiments show that RACE can privately release useful
function summaries for many machine learning problems. Our
sketch beats interpolation-based methods for private function re-
lease when the dataset has more than a few dimensions and when
fD is not smooth. In our experiments (Figure 3), the Bernstein
mechanism outperforms RACE on the SF dataset but fails to cap-
ture the nuances of the NYC salary distribution, which has sharp
peaks. RACE preserves the details of fD because the Laplace noise
can only make local changes to each hash partition of the RACE
structure. For example, if we were to generate an unusually large
Laplace noise, only queries in the problematic partition are affected.
If we perturb one or two of the most important weights of a series
estimator, the changes propagate to all queries.
Our sketch is also convenient to use and deploy in a produc-
tion environment because it is simple to implement and construct.
Unlike other algorithms, which require complex construction pro-
cesses that model correlations between features or release functions
on high-dimensional lattices, RACE only requires a 2D array of inte-
gers and a hash function. Simplicity is a strong practical advantage
because it substantially reduces the likelihood that an incorrect
algorithm implementation will lead to a breach of privacy. The sim-
plicity of the sketch also enables straightforward ways to perform
hyperparameter selection, since we only need to choose the width
W and height R of the 2D array. Although we discuss principled
methods to select R in Section 3.3, we found in our experiments that
these choices are not critical. Any R ∈ [100, 1k] with W > 1k will
provide good results for many function release problems. Finally,
the private RACE sketch is the only function release method that
can be constructed in parallel and distributed settings. Although
Session 12A: Applications and Privacy of ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3263one could possibly merge synthetic point sets or compute Bern-
stein coefficients in parallel, neither operation has the error-stable
merging property enjoyed by our sketch. Therefore, we believe that
the private RACE sketch can make private function release a viable
tool for real-world applications.
9 CONCLUSION
We have presented a differentially private sketch for a variety of
machine learning tasks. RACE is competitive with the state of the
art on tasks including density estimation, classification, and linear
regression. The sketches can be constructed in the distributed one-
pass streaming setting and are highly computationally efficient. At
the same time, they offer good performance and make an efficient
use of the privacy budget. Given the utility, simplicity and speed of
the algorithm, we expect that RACE will enable private machine
learning in large-scale settings.
ACKNOWLEDGEMENTS
This work was supported by National Science Foundation IIS-
1652131, BIGDATA-1838177, AFOSR-YIP FA9550-18-1-0152, ONR
DURIP Grant, and the ONR BRC grant on Randomized Numerical
Linear Algebra.
REFERENCES
[1] Naman Agarwal, Ananda Theertha Suresh, Felix Xinnan X Yu, Sanjiv Kumar,
and Brendan McMahan. 2018. cpsgd: Communication-efficient and differentially-
private distributed sgd. In Advances in Neural Information Processing Systems.
7564–7575.
[2] Pankaj K Agarwal, Graham Cormode, Zengfeng Huang, Jeff Phillips, Zhewei
Wei, and Ke Yi. 2012. Mergeable summaries. In Proceedings of the 31st ACM
SIGMOD-SIGACT-SIGAI symposium on Principles of Database Systems. 23–34.
[3] Francesco Aldà and Benjamin IP Rubinstein. 2017. The bernstein mechanism:
Function release under differential privacy. In Thirty-First AAAI Conference on
Artificial Intelligence.
[4] Noga Alon, Yossi Matias, and Mario Szegedy. 1999. The space complexity of
approximating the frequency moments. Journal of Computer and system sciences
58, 1 (1999), 137–147.
[5] Matej Balog, Ilya Tolstikhin, and Bernhard Schölkopf. 2018. Differentially Private
Database Release via Kernel Mean Embeddings. In International Conference on
Machine Learning. 414–422.
[6] Raef Bassily, Albert Cheu, Shay Moran, Aleksandar Nikolov, Jonathan Ullman, and
Steven Wu. 2020. Private Query Release Assisted by Public Data. In Proceedings
of the 37th International Conference on Machine Learning (Proceedings of Machine
Learning Research, Vol. 119), Hal DaumÃľ III and Aarti Singh (Eds.). PMLR, 695–
703. http://proceedings.mlr.press/v119/bassily20a.html
[7] Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan
McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. 2017. Prac-
tical secure aggregation for privacy-preserving machine learning. In Proceedings
of the 2017 ACM SIGSAC Conference on Computer and Communications Security.
1175–1191.
[8] Kamalika Chaudhuri, Claire Monteleoni, and Anand D Sarwate. 2011. Differen-
tially private empirical risk minimization. Journal of Machine Learning Research
12, Mar (2011), 1069–1109.
[9] Flavio Chierichetti and Ravi Kumar. 2012. LSH-Preserving Functions and Their
Applications. In Proceedings of the Twenty-Third Annual ACM-SIAM Symposium
on Discrete Algorithms (Kyoto, Japan) (SODA âĂŹ12). Society for Industrial and
Applied Mathematics, USA, 1078âĂŞ1094.
[10] Benjamin Coleman, Benito Geordie, Li Chou, RA Leo Elworth, Todd J Treangen,
and Anshumali Shrivastava. 2019. Diversified RACE Sampling on Data Streams
Applied to Metagenomic Sequence Analysis. bioRxiv (2019), 852889.
[11] Benjamin Coleman, Gaurav Gupta, John Chen, and Anshumali Shrivastava. 2020.
STORM: Foundations of End-to-End Empirical Risk Minimization on the Edge.
arXiv preprint arXiv:2006.14554 (2020).
[12] Benjamin Coleman and Anshumali Shrivastava. 2020. Sub-linear RACE Sketches
for Approximate Kernel Density Estimation on Streaming Data. In Proceedings of
the 2020 World Wide Web Conference. International World Wide Web Conferences
Steering Committee.
[13] Andrew R Conn, Katya Scheinberg, and Luis N Vicente. 2009. Introduction to
derivative-free optimization. Vol. 8. Siam.
[14] Graham Cormode, Somesh Jha, Tejas Kulkarni, Ninghui Li, Divesh Srivastava,
and Tianhao Wang. 2018. Privacy at scale: Local differential privacy in practice.
In Proceedings of the 2018 International Conference on Management of Data. 1655–
1658.
[15] Cynthia Dwork. 2006. Differential Privacy. In 33rd International Colloquium on
Automata, Languages and Programming, part II (ICALP 2006) (33rd international
colloquium on automata, languages and programming, part ii (icalp 2006) ed.)
(Lecture Notes in Computer Science, Vol. 4052). Springer Verlag, 1–12. https:
//www.microsoft.com/en-us/research/publication/differential-privacy/
[16] Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and
Moni Naor. 2006. Our data, ourselves: Privacy via distributed noise generation. In
Annual International Conference on the Theory and Applications of Cryptographic
Techniques. Springer, 486–503.
[17] Cynthia Dwork, Aaron Roth, et al. 2014. The algorithmic foundations of differ-
ential privacy. Foundations and Trends® in Theoretical Computer Science 9, 3–4
(2014), 211–407.
[18] Úlfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova. 2014. Rappor: Random-
ized aggregatable privacy-preserving ordinal response. In Proceedings of the 2014
ACM SIGSAC conference on computer and communications security. 1054–1067.
[19] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. 2015. Model inversion
attacks that exploit confidence information and basic countermeasures. In Pro-
ceedings of the 22nd ACM SIGSAC Conference on Computer and Communications
Security. 1322–1333.
[20] Matthew Fredrikson, Eric Lantz, Somesh Jha, Simon Lin, David Page, and Thomas
Ristenpart. 2014. Privacy in pharmacogenetics: An end-to-end case study of
personalized warfarin dosing. In 23rd {USENIX} Security Symposium ({USENIX}
Security 14). 17–32.
[21] Aristides Gionis, Piotr Indyk, Rajeev Motwani, et al. 1999. Similarity search in
high dimensions via hashing. In Vldb, Vol. 99. 518–529.
[22] Rob Hall, Alessandro Rinaldo, and Larry Wasserman. 2013. Differential privacy
for functions and functional data. Journal of Machine Learning Research 14, Feb
(2013), 703–727.
[23] Moritz Hardt, Katrina Ligett, and Frank McSherry. 2012. A simple and practical
algorithm for differentially private data release. In Advances in Neural Information
Processing Systems. 2339–2347.
[24] Piotr Indyk and Rajeev Motwani. 1998. Approximate nearest neighbors: towards
removing the curse of dimensionality. In Proceedings of the thirtieth annual ACM
symposium on Theory of computing. ACM, 604–613.
[25] George H. John and Pat Langley. 1995. Estimating Continuous Distributions in
Bayesian Classifiers. In Proceedings of the Eleventh Conference on Uncertainty in
Artificial Intelligence (Montréal, Qué, Canada) (UAIâĂŹ95). Morgan Kaufmann
Publishers Inc., San Francisco, CA, USA, 338âĂŞ345.
[26] Jure Leskovec and Andrej Krevl. 2014. SNAP Datasets: Stanford Large Network
Dataset Collection. http://snap.stanford.edu/data.
[27] Tian Li, Zaoxing Liu, Vyas Sekar, and Virginia Smith. 2019. Privacy for free:
Communication-efficient learning with differential privacy using sketches. arXiv
preprint arXiv:1911.00972 (2019).
[28] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. 2020. Federated
IEEE Signal Processing
learning: Challenges, methods, and future directions.
Magazine 37, 3 (2020), 50–60.
[29] Chen Luo and Anshumali Shrivastava. 2018. Arrays of (locality-sensitive) count
estimators (ACE): Anomaly detection on the edge. In Proceedings of the 2018
World Wide Web Conference. International World Wide Web Conferences Steering
Committee, 1439–1448.
[30] Ashwin Machanavajjhala, Xi He, and Michael Hay. 2017. Differential privacy in
the wild: A tutorial on current practices & open challenges. In Proceedings of the
2017 ACM International Conference on Management of Data. 1727–1730.
[31] Ardalan Mirshani, Matthew Reimherr, and Aleksandra Slavković. 2019. For-
mal Privacy for Functional Data with Gaussian Perturbations. In Proceedings
of the 36th International Conference on Machine Learning (Proceedings of Ma-
chine Learning Research, Vol. 97). PMLR, Long Beach, California, USA, 4595–4604.
http://proceedings.mlr.press/v97/mirshani19a.html
[32] Wahbeh Qardaji, Weining Yang, and Ninghui Li. 2013. Understanding hierarchical
methods for differentially private histograms. Proceedings of the VLDB Endowment
6, 14 (2013), 1954–1965.
[33] Daniel Rothchild, Ashwinee Panda, Enayat Ullah, Nikita Ivkin, Ion Stoica,
Vladimir Braverman, Joseph Gonzalez, and Raman Arora. 2020. Fetchsgd:
Communication-efficient federated learning with sketching. In International
Conference on Machine Learning. PMLR, 8253–8265.
[34] Or Sheffet. 2017. Differentially private ordinary least squares. In International
Conference on Machine Learning. 3105–3114.
[35] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. Mem-
bership inference attacks against machine learning models. In 2017 IEEE Sympo-
sium on Security and Privacy (SP). IEEE, 3–18.
Session 12A: Applications and Privacy of ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3264[36] Anshumali Shrivastava and Ping Li. 2014. Asymmetric LSH (ALSH) for sublinear
time maximum inner product search (MIPS). In Advances in Neural Information
Processing Systems. 2321–2329.
[37] Alexander Strehl and Joydeep Ghosh. 2000. Impact of similarity measures on
web-page clustering. In AAAI-2000: Workshop of Artificial Intelligence for Web
Search.
[38] Apple Differential Privacy Team. 2017. Learning with privacy at scale. Apple
Machine Learning Journal 1, 8 (2017).
[39] Reihaneh Torkzadehmahani, Peter Kairouz, and Benedict Paten. 2019. Dp-cgan:
Differentially private synthetic data and label generation. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. 0–0.
[40] Jonathan Ullman. 2013. Answering N2+o(1) Counting Queries with Differential
Privacy is Hard. In Proceedings of the Forty-Fifth Annual ACM Symposium on
Theory of Computing (Palo Alto, California, USA) (STOC 2013). Association for
Computing Machinery, New York, NY, USA, 361âĂŞ370. https://doi.org/10.1145/
2488608.2488653
[41] Jalaj Upadhyay. 2018. The price of privacy for low-rank factorization. In Advances
in Neural Information Processing Systems. 4176–4187.
[42] Jingdong Wang, Ting Zhang, Nicu Sebe, Heng Tao Shen, et al. 2017. A survey on
learning to hash. IEEE transactions on pattern analysis and machine intelligence
40, 4 (2017), 769–790.
[43] Yu-Xiang Wang. 2018. Revisiting differentially private linear regression: optimal
and adaptive prediction & estimation in unbounded domain (Proceedings of
the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence). Monterey,
California USA, 93–103.
[44] Yu-Xiang Wang, Stephen Fienberg, and Alex Smola. 2015. Privacy for free: Pos-
terior sampling and stochastic gradient monte carlo. In International Conference
on Machine Learning. 2493–2502.
[45] Ziteng Wang, Kai Fan, Jiaqi Zhang, and Liwei Wang. 2013. Efficient algorithm for
privately releasing smooth queries. In Advances in Neural Information Processing
Systems. 782–790.
[46] Larry Wasserman and Shuheng Zhou. 2010. A statistical framework for differen-
tial privacy. J. Amer. Statist. Assoc. 105, 489 (2010), 375–389.
[47] David P Woodruff. 2014. Sketching as a Tool for Numerical Linear Algebra.
Theoretical Computer Science 10, 1-2 (2014), 1–157.
[48] Liyang Xie, Kaixiang Lin, Shu Wang, Fei Wang, and Jiayu Zhou. 2018. Differ-
entially private generative adversarial network. arXiv preprint arXiv:1802.06739
(2018).
[49] Jia Xu, Zhenjie Zhang, Xiaokui Xiao, Yin Yang, Ge Yu, and Marianne Winslett.
2013. Differentially private histogram publication. The VLDB Journal 22, 6 (2013),
797–822.
[50] Jun Zhang, Graham Cormode, Cecilia M Procopiuc, Divesh Srivastava, and Xi-
aokui Xiao. 2017. Privbayes: Private data release via bayesian networks. ACM
Transactions on Database Systems (TODS) 42, 4 (2017), 1–41.
Session 12A: Applications and Privacy of ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3265