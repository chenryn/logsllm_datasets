### Memory and Construction Efficiency

1. **Memory Requirements**: The memory required by our sketch is primarily determined by the desired approximation error and scales sublinearly with respect to \( N \), the number of entries in the dataset.

2. **Efficient Construction**: The sketch can be constructed efficiently in a single pass through the data, without the need for auxiliary data structures or additional storage.

3. **Mergeability**: The sketches are mergeable. Given a sketch \( S_1 \) of dataset \( D_1 \) and a sketch \( S_2 \) of dataset \( D_2 \), we can obtain a sketch of the combined dataset \( D_1 \cup D_2 \) by simply adding the counters of \( S_1 \) and \( S_2 \) [2].

These properties are well-known to enable fast, distributed implementations of the algorithm [2]. Additionally, they facilitate privacy in distributed settings. Federated learning, which requires private aggregation of gradients, has led to numerous solutions for the private aggregation problem. Our sketch is compatible with most of these solutions. Two notable approaches are multi-party protocols and local noise addition.

### Privacy in Distributed Settings

**Local Noise Addition**: This approach is well-studied due to its use in federated learning, where private centralized aggregation of gradient updates is required. In this setting, users add noise (e.g., Gaussian or Binomial) to their gradient updates locally before sending them to an aggregator. This process ensures local \((\epsilon, \beta)\)-differential privacy as described in [1]. These methods can be directly applied to our sketch, similar to how they have been applied to the Count Sketch by [27].

**Secure Aggregation**: For scenarios where a trusted central server is not available, multi-party protocols can be used to securely compute the sum of count values [7]. To ensure privacy, noise must be generated via a distributed privacy mechanism, such as the one proposed in [16]. Recent work by Choi et al. addresses the specific case of multi-party based aggregation for sketches and is directly applicable to our sketch. To achieve \(\epsilon\)-differential privacy, Choi et al. add quantized gamma noise to each count before running the MPC protocol, ensuring that the sum of gamma noises is Laplace-distributed. Our sketches fit into this framework, requiring the MPC protocol to be run \( R \times W \) times, once for each count.

### Compatibility with Existing Work

In summary, our sketch is compatible with a wide range of existing work on private sum aggregation. When RACE sketches are constructed in distributed settings, the ability to release private sketches is maintained.

### Performance and Utility

Although RACE can only estimate LSH kernels, the space of LSH kernels is sufficiently large to be useful for machine learning. For example, RACE performed well on classification tasks for \(\epsilon > 10^{-1}\), providing a competitive utility tradeoff for practical privacy budgets. For linear regression, RACE outperforms objective perturbation, a well-established method for private learning [8], but not specialized methods like AdaSSP or AdaOPS, which deliver the best possible performance for linear regression [43]. Therefore, RACE can perform regression and classification with good, though not necessarily state-of-the-art, accuracy.

### Function Release at Scale

Our most significant result is that the private RACE sketch is orders of magnitude faster than competing algorithms for function release. While PFDA and the Bernstein mechanism have the strongest theoretical error bounds, they require days to produce results, whereas RACE and PrivBayes require only a few seconds. For instance, running the Bernstein mechanism on the UCI gas dataset would require over \(2^{128} \approx 10^{38}\) computations and more than one billion exabytes of memory, making it impractical. Consequently, we were unable to run these methods on the covtype dataset without sampling or dimensionality reduction. For larger datasets, such as those in Table 4, only RACE can handle the task within a few minutes. Our sketch has a small memory footprint, inexpensive streaming updates, and a fast distributed implementation.

### Discussion

Our experiments show that RACE can privately release useful function summaries for many machine learning problems. RACE outperforms interpolation-based methods for private function release when the dataset has more than a few dimensions and when \( f_D \) is not smooth. For example, while the Bernstein mechanism outperformed RACE on the SF dataset, it failed to capture the nuances of the NYC salary distribution, which has sharp peaks. RACE preserves the details of \( f_D \) because the Laplace noise can only make local changes to each hash partition of the RACE structure. In contrast, perturbing one or two of the most important weights in a series estimator can propagate changes to all queries.

### Practical Advantages

Our sketch is also convenient to use and deploy in a production environment due to its simplicity. Unlike other algorithms that require complex construction processes to model correlations between features or release functions on high-dimensional lattices, RACE only requires a 2D array of integers and a hash function. This simplicity reduces the likelihood of privacy breaches due to incorrect implementations and simplifies hyperparameter selection, as we only need to choose the width \( W \) and height \( R \) of the 2D array. Although principled methods to select \( R \) are discussed in Section 3.3, our experiments show that any \( R \in [100, 1k] \) with \( W > 1k \) provides good results for many function release problems. Finally, the private RACE sketch is the only function release method that can be constructed in parallel and distributed settings, making it a viable tool for real-world applications.

### Conclusion

We have presented a differentially private sketch for various machine learning tasks. RACE is competitive with the state of the art in tasks such as density estimation, classification, and linear regression. The sketches can be constructed in a distributed one-pass streaming setting and are highly computationally efficient, offering good performance and efficient use of the privacy budget. Given its utility, simplicity, and speed, we expect RACE to enable private machine learning in large-scale settings.

### Acknowledgements

This work was supported by the National Science Foundation IIS-1652131, BIGDATA-1838177, AFOSR-YIP FA9550-18-1-0152, ONR DURIP Grant, and the ONR BRC grant on Randomized Numerical Linear Algebra.

### References

[References listed here as provided in the original text.]

---

This optimized version aims to improve clarity, coherence, and professionalism, while maintaining the technical details and key points of the original text.