examples (useful for practitioners) and the insights that they
bring about this approach to using CII-based arguments.
We study these new scenarios in the context of the method
and assumptions of our previous papers [6,7]:
1) applying conservative Bayesian inference (CBI) [9–11]:
a use of Bayesian inference that aims to avoid the risk of
unwittingly over-optimistic assessment. While Bayesian infer-
ence requires its user to specify a full “prior distribution”, CBI
does not. Instead, it uses, as its input, limited constraints on
the prior, which are easier for experts to argue on the basis of
the evidence. This improves trust that the dependability claims
are the result of the actual evidence, rather than artefacts of
assumptions made for mathematical convenience.
2) treating the common situation in which the claim of
interest concerns a probability of failure per demand (pfd).
Speciﬁcally, the failure process is a Bernoulli process: failures
on successive demands are independent events with the same
probability (the pfd). Bernoulli processes are in common use
[1,12]. They give a useful model for many systems where the
main concern is design faults, and/or for limited periods of
operation. We expect similar results to hold for systems with
failure processes in continuous time, and that the approach can
be extended to other forms of reliability functions.
To this general picture, the present work adds the aforemen-
tioned new scenarios of CII evidence, solutions of the related
extremization problems for applying CBI, and illustrations of
the impact of the various forms of evidence on conﬁdence
in claims. We illustrate how the impact of evidence – on
conﬁdence in a claim – can vary signiﬁcantly, depending on
the nature and strength of the evidence. In particular, we
highlight situations where additional CII evidence improves
conﬁdence in pfd bounds, and situations where it does not.
In the rest of this paper, Sec. II discusses related work, and
Sec. III reviews CBI. Sec. IV then gives example scenarios in
which our models may be applied, and of the different formal
CII statements that apply. Sec. V presents the formal statistical
models, with illustrative examples shown in Sec. VI. Sec. VII
gives a sensitivity analysis, by comparing scenarios that differ
in prior knowledge and supporting evidence for CII. Sec. VIII
gives a ﬁnal discussion of our results.
II. RELATED WORK
Bayesian methods are well established in reliability and
safety [12]. They support combining various forms of evidence
and direct reliability predictions. The results that we use here
derive mostly from work on the effects of software faults, or
other causes of systematic failure. Software reliability assess-
ment has long been argued to require a statistical approach,
and Bayesian methods are well-suited for it [13–15].
Demonstrating that a system meets its dependability re-
quirements, on the sole basis of observed good behaviour
(few or zero failures over many demands), is in some cases
extremely challenging: it would require observing infeasibly
many failure-free runs by the system being assessed, or require
improbably strong prior evidence [3,4,11]. However, in many
situations, suitable application of Bayesian inference does sup-
port strong claims – e.g. situations with more modest reliability
requirements, or with justiﬁable estimates of the probability
that certain subsystems will not fail (i.e. a probability of
“perfection”, or of pfd = 0), or with architectural information
to support white-box assessment – [6,16,17].
The present work uses conservative Bayesian inference
(CBI). This approach is suitable for various safety assessment
contexts and produces posterior measures of reliability that
are “guaranteed-to-be-conservative”, but no more conservative
than prior evidence and the observed failure behaviour of the
system will allow. For instance, having seen the system suc-
cessfully handle n demands from its operational environment,
CBI gives conservative values for 1) the probability that the
system fails on the next demand [9], 2) the probability that
the system “survives” the next m demands [10], and 3) the
posterior conﬁdence in an upper bound on the system’s pfd
[11]. CBI has also been applied when (rare) failures occur
among many correctly handled demands [11,18].
The inference program in CBI applications is the same.
An assessor (a) chooses a posterior measure of reliability,
(b) speciﬁes an appropriate likelihood function to characterise
any observed failure/success behaviour, (c) translates prior
evidence into mathematical statements (we will call
these
“prior knowledge” statements, PKs), (d) considers all prior
distributions consistent with these PKs, (e) selects, from this
set of priors, a prior that gives the most conservative value
for the posterior measure of interest (this need not be unique
[7]). For brevity, at times we will use wording like “prior
evidence implies a certain effect on a posterior measure”,
where “prior evidence” needs to be read as “the PKs justiﬁed
by the prior evidence”. The inference program just outlined is
closely related to robust Bayesian analysis – a general frame-
work for investigating the sensitivity of posterior measures to
uncertainties in the inputs of Bayesian inference [19,20].
Unsurprisingly, adding more constraints to limit the set of
priors makes CBI’s predictions less conservative. Indeed, in
the limiting case in which evidence is enough to justify a
speciﬁc prior distribution, CBI reduces to ordinary Bayesian
inference. But even under less extreme circumstances, the
extent
to which stronger prior evidence can temper CBI
conservatism has been studied, e.g., when evidence supporting
an estimate of the prior probability of pfd being 0 [21], or close
to it [10,22], can be included in the assessment.
Similarly, for cases where, as in the present paper, claims
for a new system rely on evidence about an older system, we
previously showed how justiﬁable “probability of perfection”
evidence can result in less conservative claims than if such
evidence were unavailable [7]. We also studied the case of
an autonomous vehicle assessed under new environmental
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:23:48 UTC from IEEE Xplore.  Restrictions apply. 
452
conditions, given knowledge of the system’s operation under
a different environment in the past [6]. The number of failure-
free miles that need to be driven in the new environment –
to support a given claim with, say, 90% conﬁdence – can be
much less than the number needed to make the same claim if
evidence from that previous environment is unavailable.
III. REVIEW: CBI EXAMPLE
We recall an application of CBI [11]. A Bernoulli process
represents the failure behaviour of a system on a succession
of demands. Let X be the system’s unknown pfd. The system
is observed to successfully handle n demands. The Bernoulli
failure process implies that the probability of observing this
sequence of successes – the likelihood function – takes the
form L(x) = (1 − x)n. Let p be a pfd upper bound with
respect to which an assessor seeks to make a claim. If the
assessor has evidence to support a prior distribution of X,
then the posterior conﬁdence that the system pfd X is better
than p (given that the system survived those n demands) is:
E[ L(X)1X(cid:54)p ]
P (X (cid:54) p | n) =
P (X (cid:54) p, n successes)
=
(1)
where 1S is an indicator function – it equals 1 when predicate
S is true, and 0 otherwise.
But available evidence is typically insufﬁcient
to fully
justify a given prior for X. Evidence may, instead, support
relatively weaker prior claims, such as those proposed below.
Prior Knowledge 1. certainty that the system pfd X is no
better than some pl (cid:62) 0. That is, P (X (cid:62) pl) = 1 .
Prior Knowledge 2. θ× 100% conﬁdence that the system pfd
X meets, or surpasses, a pfd ε. That is, P (X (cid:54) ε) = θ.
Here ε, the “engineering goal”, is meant as a pfd that has high
probability θ of being achieved by system developers required
to build a system with pfd better than p (where p (cid:62) ε).
If one has evidence to support prior knowledge 1 and 2,
the following proposition (proved in [11]) shows that such
knowledge allows one to conservatively gain conﬁdence in a
required pfd bound p upon observing failure-free operation.
Proposition 1. Let D be the set of all probability distributions
for the pfd X of a system (i.e. all distributions over [0, 1]).
Consider the optimization problem
P (n successes)
E[ L(X) ]
infD P (X (cid:54) p | n)
(where ε (cid:54) p), subject to the constraint that there is evidence
the system satisﬁes prior knowledge 1 and 2.
The prior distribution in Fig. 1 solves this problem because,
upon using this prior, P (X < p | n) = infD P (X (cid:54) p | n).
IV. EVIDENCE AND ARGUMENTS FOR IMPROVEMENT
CLAIMS
The ﬁrst, critical step for this form of argument
is to
examine which evidence supports an “improvement” argu-
ment, and translate it into a formal, mathematical CII claim.
Fig. 1: A conservative prior cumulative distribution function.
This subsidiary claim will support the linking between the
reasoning about A and about B, so that evidence collected
about A supports conﬁdence regarding B.
Different evidence may justify different forms of CII claims,
thus different ﬁnal conﬁdence on B. Lack of rigour at this
stage could invalidate any conclusions, despite the rest of the
reasoning being a provably correct series of deductive steps.
We give some examples of what basis a CII claim may have
in evidence; what mathematical form it could then take; and
what factors may prevent absolute conﬁdence in the claim,
thus requiring that it be considered true only with a certain
probability, which we will call φ, with 0 < φ < 1.
1) A and B are two systems to be used in the same
operational environment. B is a newly developed, plug-in
replacement for the older system A. B is built to the same
speciﬁcation as A, but by newer methods known to yield better
reliability. For instance, its software has been developed with
methods known to be less error-prone, and veriﬁed through
better methods, by better staff. One then expects B to be more
reliable than A, as produced by a better technology, thus less
likely to have design faults. However, such beliefs concern the
generality of systems produced by the two different processes,
not A and B speciﬁcally: it is possible, though improbable,
that B is worse than A, as B turns out to be an unusually poor
result of a high-quality process; and/or A an unusually good
result of the relatively worse process that produced it. These
unlikely scenarios determine an amount of doubt (1 − φ).
2) system B is an improvement on system A, identical
except that some known defects have been removed (e.g., some
failure-prone hardware parts were made more reliable, known
design faults were ﬁxed). This would mean that whatever the
true reliability of A, B would probably be more reliable in the
same environment; some of the failures that occur in A due
to those defects will not occur in B. This would not be 100%
guaranteed of course: bug ﬁxes sometimes introduce new bugs,
and occasionally reduce reliability. So, the CII claim that can
be made is that whatever the true pfd of A, B’s pfd is better or
not worse, with conﬁdence φ limited by how often in similar
circumstances (system complexity, change approval processes)
ﬁxes have actually reduced reliability.
3) system B is obtained by adding to system A some safety
protection: e.g., A is a safety system and B adds another
independent safety monitor with authority to effect the safety
action (“1-out-of-n” scheme). This way of building B ensures
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:23:48 UTC from IEEE Xplore.  Restrictions apply. 
453
(b) PK1 and PK2 for A and B
(a) PK1 for A and B; PK2 for A
Fig. 2: How the different forms of prior knowledge that we deﬁne constrain joint prior distributions of (cid:104)XA, XB(cid:105), by associating probabilities
to regions of the Cartesian plane of (xA, xB) values. The forms of prior knowledge are: (a) a marginal probability θ that version A meets
the engineering goal, [XA (cid:54) ε]; (b) identical marginal probabilities θ for both versions; (c) probabilities that B is no worse than A,
i.e. P (XB (cid:54) XA, XA (cid:54) α) = ϕ and P (XB (cid:54) XA) = φ (where 0 < ϕ (cid:54) φ and ε (cid:54) α); (d) within the events [pl (cid:54) XA (cid:54) ε]
and [α (cid:54) XA (cid:54) 1], the ratio between the probabilities for regions above and below the diagonal is
1−φ , for any M, φ, θ such that
0 < M < (1 − φ)(1 − θ).
(d) PK1 and PK4
(c) PK1 and PK3
φ
that the set of demands on which B fails unsafely, UB, is a
subset of those where A fails unsafely, UB ⊆ UA. Hence,
pfd B =
P (D) = pfd A
(2)
(cid:88)
P (D) (cid:54) (cid:88)
D∈UB
D∈UA
despite our not knowing either the sets UB, UA or the probabil-
ities associated to them. Thus, the evidence supports a claim
(cid:54) pfd A.
that no matter what the true value of pfd A, pfd B
This claim can usually be considered true with very high
conﬁdence; however, again, there are conceivable, unlikely
scenarios in which adding safety elements increases pfd, and
the historical frequency of such events will determine the
doubt (1 − φ).
4) A is a testing environment, for a demand-operated con-
trol system that is to be deployed in an operational environ-
ment B. Environment A has been made “stressful” through two
precautions: (a) making the statistical distribution of demands
(sequences of inputs to the system), conditional on each type of
demand, the same as in B; but (b) giving higher probabilities to
types of demands that are known to cause failures with higher
probabilities, due to known limits of the system hardware.
Let us call qi the pfd conditional on a demand belonging to
type i, for each one of n types of demands; and tA
the
probabilities of demands of that type in environments A and B
respectively. Precaution (a) ensures that no qi value changes
between A and B; and (b) ensures that demand types with
higher qi have higher probabilities in A than in B: for those
i . The pfd values in the two environments are then
i, tA
i
as in the equalities below [23]:
(cid:62) tB
i , tB