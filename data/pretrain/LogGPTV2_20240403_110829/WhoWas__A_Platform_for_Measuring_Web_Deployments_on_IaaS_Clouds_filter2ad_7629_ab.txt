(though we will admittedly be targeting smaller ranges). One round
of scanning visits all the IP addresses input to WhoWas. We chose
small defaults to balance burden on infrastructure with the temporal
granularity of WhoWas measurements.
Figure 1: The main components of WhoWas.
We say an IP address is responsive if it responds to any of the
network probes. Otherwise it is unresponsive.
HTTP requests. For any IP address that responds as being open
on port 80 or port 443, the scanner submits the IP to the web-
page fetcher, which will randomly pick up a worker process from a
worker pool to gather further information from this IP. The worker
generates a URL by concatenating the IP address to either “http://”
(should port 80 and 443 have been seen as open) or “https://” (if
only port 443 was open). A GET request is submitted to the re-
sultant URL concatenating with “robots.txt”. Then, a GET may be
sent to the URL depending on the content of robots.txt. At most
two GETs are made per round of scanning for a given IP address.
The fetcher records any received HTTP response code, response
header, or error information in a MySQL database. Each round
of scanning uses a distinct database table, with a timestamp in
the table name to indicate the start of the round.
If the type of
the response content is “application/*”, “audio/*”, “image/*” or
“video/*”, the fetcher foregoes downloading the content as our
analysis engine cannot yet process non-text data. For text con-
tent (e.g., HTML, JSON, XML) we store the ﬁrst 512 KB of content
into the database. The fetcher does not follow links in the collected
content and does not execute active content. The fetcher is built
using the Python Requests library [25].
An IP is available in a round if the HTTP(S) request for the URL
without robots.txt succeeded, and unavailable otherwise. Note that
unresponsive IPs are also unavailable.
Feature extraction. After a round of scanning is ﬁnished,
WhoWas invokes a separate process to extract various features from
the results. For each webpage successfully fetched, we extract and
insert into the database: (1) the back-end technology from the “x-
powered-by” ﬁeld in HTTP response headers; (2) a description
of a webpage from the “description” tag in the returned HTML;
(3) a string that consists of all ﬁeld names in the HTTP response
header (sorted alphabetically and separated by “#”); (4) the length
of the returned HTML; (5) the title string from the HTML; (6) the
web template (e.g., Joomla!, WordPress, or Drupal) indicated by
“generator” tags in the HTML; (7) the server type as reported in
HTTP response headers; (8) the keywords as indicated in a key-
words tag in the HTML; and (9) any Google Analytics ID found
in the HTML. We mark entries as unknown when they are missing
from the HTML or HTTP headers.
We additionally compute (10) a simhash [26–28] over the HTML
of the returned webpage. This algorithm has been used previously
for webpage comparison: two webpages are similar if their ﬁnger-
prints have low Hamming distance. We use 96-bit hashes.
Limitations. We brieﬂy highlight a few limitations of our platform.
First, it can only detect a portion of web-facing instances hosted in
the cloud. For example, the scanner fails to successfully discover a
web server instance if the owner conﬁgures ﬁrewalls to only allow
trafﬁc from a given source; the instance is not bound to a public
Cloud IPs1.ScannerIP/Domain whitelist2.Fetcher3.FeatureGeneratorDBData Collection and ProcessingInputAnalysis Engine……ClusteringVPC lookup103IP address (e.g., instances in some EC2 VPC networks); and/or the
ports used for SSH or web services are not set to the default.
A second limitation is that we can visit a website using only the
IP address instead of the intended domain. PaaS providers or web
hosting providers may use one instance for multiple websites or
web applications, and not allow requests without speciﬁc URLs.
In this case, they will return 404 pages or other customized pages
to the fetcher. Even so, for some IPs, we can still identify their
ownership by looking at unique features in the returned contents
(e.g., if the 404 response contains the domain name of the website).
A third limitation is the granularity achieved by WhoWas. By
only querying once per day, or even once per several days, we
can only measure changes in status with that granularity. Network
packet drops or other temporary failures will have WhoWas mark
an IP as unresponsive or unavailable for the entire measurement pe-
riod. More generally, EC2 and Azure (as well as other clouds) have
per-hour charging models, meaning that we might expect an hourly
granularity to better reﬂect the rapidity of changes.
Increasing the scan rate could be easily accommodated via par-
allelism. However, while fetching webpage content at such granu-
larities might not represent a burden for large enterprise tenants (let
alone the IaaS provider), it is impossible to know whether WhoWas
is also interacting with tenants expecting very little, or even no,
network trafﬁc2. For these reasons we err on the side of having a
relatively conservative scan rate.
5. ANALYSIS TOOLS
As described thus far, WhoWas can perform scans, fetch pages,
and store them. To enable richer analyses, we build in extra tools
to aid users. We ﬁrst include a clustering algorithm to associate IP
addresses that are likely to be hosting the same webpage or, at least,
be operated by the same owner. We also build in some knowledge
of the semantics associated with particular IP addresses by way of
cloud cartography techniques.
Clustering. After collecting webpages for a period of time, we use
clustering to help identify IP addresses that are likely to be host-
ing the same web applications. Features used for clustering are the
title, template, server software, keywords, Google Analytics ID,
and simhash. Other extracted features are not used for clustering
because they are unstable (e.g., HTML length) or redundant. We
then use a 2-level hierarchical clustering, using the features for IPs
across all rounds of measurement. The clustering is run across all
rounds, so that each entry in a cluster corresponds to an 
pair. In the ﬁrst level clustering, we cluster solely on the ﬁrst ﬁve
features. At this level, strict equality is used for comparison, so
each ﬁrst-level cluster only has webpages with the same title, tem-
plate, server, keywords, and Analytics ID.
In the second level clustering we use the simhashes. The distance
measure here is the Hamming distance between two simhashes,
which can take on a value from 0 (identical) to 96 (very dissimi-
lar) since we use 96-bit hashes. We use a simple tuning procedure
to pick a distance threshold for clustering based on the gap statistic,
which is a common method for estimating the number of clusters
when doing unsupervised clustering [29]. Each resulting second-
level cluster is given a unique random identiﬁer, and we associate
to each IP’s round record their cluster assignment.
If using this clustering approach to identify IPs belonging to the
same website, an immediate limitation is that signiﬁcant changes to
the features (e.g., due to an update to the page or underlying server
software) will result in IPs being placed into distinct clusters. We
2Indeed, we received emails from a handful of tenants that did not
realize their web servers were publicly accessible! See also §7.
therefore provide some additional heuristics to help catch changes
in features that may not be indicative of change of ownership of
an IP address. In a post-processing step after clustering, we merge
clusters using the following algorithm.
Consider two records in the database:
a, f(cid:48)
DB[IP, T, simhash, cluster, fa, fb, fc, fd, fe],
DB[IP (cid:48), T (cid:48), simhash(cid:48), cluster(cid:48), f(cid:48)
c, f(cid:48)
d, f(cid:48)
e]
where we store the IP, the timestamp (T), the simhash of the
content of the page and the cluster to which the IPs belong, and
the other features (fx) namely: title, server, template, keywords,
and Google Analytics ID. We merge two records if the following
conditions hold: the IPs are the same, the simhashes differ at most
by 3 bits (used also in [30]), one feature is the same between the
two records, and the two clusters are temporally ordered.
b, f(cid:48)
The 2-level clustering and merging heuristic above strive to iden-
tify web applications that are being used across distinct IPs or
across time on the same IP address.
It was the culmination of
a manual reﬁnement process, where we started with using just
simhashes and then added top level features to help improve clus-
tering results (as measured by manual inspection of samples). This
was made easy by the programmatic WhoWas interface. The inter-
face also makes it easy to modify, for example, the approach above
to cluster with other goals in mind, such as simply ﬁnding related
content (dropping the server feature) or only using Analytics IDs.
After merging, some clusters may still include IP addresses ac-
tually operated by different owners. For example, sites that lack
a Google Analytics ID but reply with a default Apache page all
end up in the same cluster. To not have such clusters bias analyses
further in the paper, we perform some manual cleaning of the clus-
ters. First, we write a script to remove the top-level clusters (and
associated second-level clusters) whose title contains expressions
indicating that WhoWas failed to fetch useful content.
(e.g., “not
found” and “error”.) Then, for second-level clusters with over 20 IP
addresses associated with them on average per day, we check the ti-
tles of their top-level clusters, and exclude them if they correspond
to default server test pages. (e.g., with title “welcome-apache”.)
Unless speciﬁed otherwise, from now on we refer to second-level
clusters after merging and cleaning as simply clusters.
Cloud cartography. Previous work highlighted the utility of so-
called cloud cartography, in which one uses measurements to as-
sociate cloud public IP addresses to types of infrastructure. For
example, this has been used to estimate in EC2 the type [31] and
availability zone [2,31] of a given instance. Different than previous
works, we use cartography to help disambiguate distinct network-
ing conﬁgurations associated with public EC2 IPs.
Recall that Amazon EC2 provides two ways to launch instances:
classic and VPC. For classic instances, one public IP address rep-
resents one VM instance. VPC instances can instead have multiple
public IPs associated to them. If we see a cluster of N IPs, if these
IPs are VPC it may therefore be the case that these IPs are associ-
ated with fewer than N VM instances.
We would therefore like to have WhoWas be able to differentiate
between VPC and classic instances. We use DNS interrogation to
differentiate between public IP addresses used for classic instances
and those used for VPC instances: Amazon’s internal DNS always
returns a public IP address when resolving the public domain name
associated to a VPC IP address (c.f., [32]).
In more detail, we have the following procedure for a one-
time measurement to label each public IP in a region with VPC
or classic.
In that region, we launch an instance in the classic
network, and then perform a DNS query for each public IP in
this region. For example, say the IP address is 1.2.3.4, then we
104Region
USEast
USWest_Oregon
EU
AsiaTokyo
AsiaSingapore
USWest_NC
AsiaSydney
SouthAmerica
VPC preﬁxes % all IPs in region
13.7
36.4
20.8
32.0
33.9
22.5
33.3
31.9
280
256
124
98
82
72
64
56
Table 2: Breakdown of public IP preﬁxes in EC2 by VPC
form the EC2-style public domain name3 “ec2-1-2-3-4.compute-
1.amazonaws.com” and perform a DNS lookup from the instance.
If the DNS response contains a start-of-authority (SOA) record
(meaning no DNS information for that DNS name), then there is
no active instance on this IP and also the IP is labeled as classic.
If instead the response contains an IP address that is in EC2’s pub-
lic IP address space, then the IP is using VPC; otherwise, the IP is
using classic networking.
After performing such queries for all EC2 public IP addresses
(with a suitably low rate limit), we obtain a map of IP preﬁxes as
being used for VPC or classic. Table 2 shows the results in terms
of the number of /22 preﬁxes (c.f., [33]) used for VPC compared to
all IPs associated to that region.
We include in WhoWas the resulting /22 map to add to any public
EC2 IP address round record an indicator of whether the IP uses
VPC or classic networking.
6. DATA COLLECTION
We seed WhoWas with the Amazon EC2 and Azure public IP
address ranges as of Sep. 10, 2013 and use them as the target IP
addresses [23, 24]. There are 4,702,208 target IP addresses in EC2
and 495,872 in Azure. The probing on EC2 began on Sep. 30, 2013
and lasted for 3 months, and the probing on Azure began on Oct. 31,
2013 and lasted for 2 months. In October and November, a round
of measurements was performed every 3 days. We then increased
the frequency to one round per day in December. The ﬁnal round
of measurement was on Dec. 31, 2013. In the end, we collected
a total of 51 rounds of scanning in EC2 and 46 in Azure. The
number of rounds are similar despite an extra month of probing on
EC2 because of the slower rate in the ﬁrst two months and because
at the beginning we occasionally stopped the probing for updates
on our infrastructure.
We run WhoWas from a pair of machines, one for EC2 and one
for Azure. The machines had the same conﬁguration: Ubuntu 12.10
i386 server, 8 core Intel i7 @ 3.40 GHz CPU, 32 GB memory, and
2 TB hard drive for storing data. The only software dependencies
are Python Requests and Python MySQLdb libraries. The worker
number for the crawler is set to 250 by default. The timeout for
HTTP(S) connections is set to 10 seconds.
The resulting data set. We collected about 900 GB of data in total.
Afterwards, we ran the analysis tools described in §5 to cluster the
IP addresses and also label IP addresses as being VPC or classic.
Recall that clusters contain  pairs. As shown in Table 4,
an average of 64.7% of the responsive EC2 IP addresses (60.6%
for Azure) are available in each round of probing. Table 3 shows
a break down of the ports open on responsive IPs. For the avail-
3Recall as per §2 that these differ slightly on a per-region and per-
zone basis, but that it is easy to determine from looking at examples
from one’s own accounts.
provider
EC2
Azure
port 22-only 80-only 443-only 80&443
30.6
28.4
25.9
9.3
38.0
45.8
5.5
16.5
Table 3: Average percentage of responsive IPs per measurement
round that open a given port(s)
provider
EC2
Azure
code 200 4xx 5xx other
64.7 28.0 7.2 0.10
60.6 30.2 9.2 0.02
Table 4: Average percentage of responsive IPs per measurement
round that respond with a given HTTP status code.
able IP addresses, Table 5 shows the top 5 text content types in the