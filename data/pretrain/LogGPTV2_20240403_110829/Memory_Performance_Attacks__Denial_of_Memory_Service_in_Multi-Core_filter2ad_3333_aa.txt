title:Memory Performance Attacks: Denial of Memory Service in Multi-Core
Systems
author:Thomas Moscibroda and
Onur Mutlu
Memory Performance Attacks:
Denial of Memory Service in Multi-Core Systems
Thomas Moscibroda Onur Mutlu
Microsoft Research
{moscitho,onur}@microsoft.com
Abstract
We are entering the multi-core era in computer science.
All major high-performance processor manufacturers have in-
tegrated at least two cores (processors) on the same chip —
and it is predicted that chips with many more cores will be-
come widespread in the near future. As cores on the same chip
share the DRAM memory system, multiple programs execut-
ing on different cores can interfere with each others’ memory
access requests, thereby adversely affecting one another’s per-
formance.
In this paper, we demonstrate that current multi-core proces-
sors are vulnerable to a new class of Denial of Service (DoS)
attacks because the memory system is “unfairly” shared among
multiple cores. An application can maliciously destroy the
memory-related performance of another application running on
the same chip. We call such an application a memory perfor-
mance hog (MPH). With the widespread deployment of multi-
core systems in commodity desktop and laptop computers, we
expect MPHs to become a prevalent security issue that could
affect almost all computer users.
We show that an MPH can reduce the performance of an-
other application by 2.9 times in an existing dual-core system,
without being signiﬁcantly slowed down itself; and this prob-
lem will become more severe as more cores are integrated on
the same chip. Our analysis identiﬁes the root causes of unfair-
ness in the design of the memory system that make multi-core
processors vulnerable to MPHs. As a solution to mitigate the
performance impact of MPHs, we propose a new memory sys-
tem architecture that provides fairness to different applications
running on the same chip. Our evaluations show that this mem-
ory system architecture is able to effectively contain the neg-
ative performance impact of MPHs in not only dual-core but
also 4-core and 8-core systems.
1 Introduction
For many decades, the performance of processors has in-
creased by hardware enhancements (increases in clock
frequency and smarter structures) that improved single-
thread (sequential) performance. In recent years, how-
ever,
the immense complexity of processors as well
as limits on power-consumption has made it increas-
ingly difﬁcult to further enhance single-thread perfor-
mance [18]. For this reason, there has been a paradigm
shift away from implementing such additional enhance-
ments. Instead, processor manufacturers have moved on
to integrating multiple processors on the same chip in
a tiled fashion to increase system performance power-
In a multi-core chip, different applications
efﬁciently.
can be executed on different processing cores concur-
rently,
thereby improving overall system throughput
(with the hope that the execution of an application on
one core does not interfere with an application on an-
other core). Current high-performance general-purpose
computers have at least two processors on the same chip
(e.g. Intel Pentium D and Core Duo (2 processors), Intel
Core-2 Quad (4), Intel Montecito (2), AMD Opteron (2),
Sun Niagara (8), IBM Power 4/5 (2)). And, the industry
trend is toward integrating many more cores on the same
chip. In fact, Intel has announced experimental designs
with up to 80 cores on chip [16].
The arrival of multi-core architectures creates signif-
icant challenges in the ﬁelds of computer architecture,
software engineering for parallelizing applications, and
operating systems. In this paper, we show that there are
important challenges beyond these areas. In particular,
we expose a new security problem that arises due to the
design of multi-core architectures – a Denial-of-Service
(DoS) attack that was not possible in a traditional single-
threaded processor.1 We identify the “security holes”
in the hardware design of multi-core systems that make
such attacks possible and propose a solution that miti-
gates the problem.
In a multi-core chip, the DRAM memory system is
shared among the threads concurrently executing on dif-
ferent processing cores. The way current DRAM mem-
ory systems work, it is possible that a thread with a
particular memory access pattern can occupy shared re-
sources in the memory system, preventing other threads
from using those resources efﬁciently.
In effect, the
1While this problem could also exist in SMP (symmetric shared-
memory multiprocessor) and SMT (simultaneous multithreading) sys-
tems, it will become much more prevalent in multi-core architectures
which will be widespreadly deployed in commodity desktop, laptop,
and server computers.
USENIX Association
16th USENIX Security Symposium
257
memory requests of some threads can be denied service
by the memory system for long periods of time. Thus,
an aggressive memory-intensive application can severely
degrade the performance of other threads with which it
is co-scheduled (often without even being signiﬁcantly
slowed down itself). We call such an aggressive appli-
cation a Memory Performance Hog (MPH). For exam-
ple, we found that on an existing dual-core Intel Pentium
D system one aggressive application can slow down an-
other co-scheduled application by 2.9X while it suffers
a slowdown of only 18% itself. In a simulated 16-core
system, the effect is signiﬁcantly worse:
the same ap-
plication can slow down other co-scheduled applications
by 14.6X while it slows down by only 4.4X. This shows
that, although already severe today, the problem caused
by MPHs will become much more severe as processor
manufacturers integrate more cores on the same chip in
the future.
There are three discomforting aspects of this novel se-
curity threat:
• First, an MPH can maliciously destroy the memory-
related performance of other programs that run on
different processors on the same chip. Such Denial
of Service in a multi-core memory system can ulti-
mately cause signiﬁcant discomfort and productiv-
ity loss to the end user, and it can have unforeseen
consequences. For instance, an MPH (perhaps writ-
ten by a competitor organization) could be used to
fool computer users into believing that some other
applications are inherently slow, even without caus-
ing easily observable effects on system performance
measures such as CPU usage. Or, an MPH can result
in very unfair billing procedures on grid-like com-
puting systems where users are charged based on
CPU hours [9].2 With the widespread deployment
of multi-core systems in commodity desktop, laptop,
and server computers, we expect MPHs to become a
much more prevalent security issue that could affect
almost all computer users.
• Second, the problem of memory performance attacks
is radically different from other, known attacks on
shared resources in systems, because it cannot be
prevented in software. The operating system or the
compiler (or any other application) has no direct con-
trol over the way memory requests are scheduled in
the DRAM memory system. For this reason, even
carefully designed and otherwise highly secured sys-
tems are vulnerable to memory performance attacks,
unless a solution is implemented in memory system
2In fact, in such systems, some users might be tempted to rewrite
their programs to resemble MPHs so that they get better performance
for the price they are charged. This, in turn, would unfairly slow down
co-scheduled programs of other users and cause other users to pay
much higher since their programs would now take more CPU hours.
hardware itself. For example, numerous sophisti-
cated software-based solutions are known to prevent
DoS and other attacks involving mobile or untrusted
code (e.g. [10, 25, 27, 5, 7]), but these are unsuited
to prevent our memory performance attacks.
• Third, while an MPH can be designed intentionally, a
regular application can unintentionally behave like an
MPH and damage the memory-related performance
of co-scheduled applications, too. This is discomfort-
ing because an existing application that runs with-
out signiﬁcantly affecting the performance of other
applications in a single-threaded system may deny
memory system service to co-scheduled applications
in a multi-core system. Consequently, critical appli-
cations can experience severe performance degrada-
tions if they are co-scheduled with a non-critical but
memory-intensive application.
The fundamental reason why an MPH can deny memory
system service to other applications lies in the “unfair-
ness” in the design of the multi-core memory system.
State-of-the-art DRAM memory systems service mem-
ory requests on a First-Ready First-Come-First-Serve
(FR-FCFS) basis to maximize memory bandwidth uti-
lization [30, 29, 23]. This scheduling approach is suit-
able when a single thread is accessing the memory sys-
tem because it maximizes the utilization of memory
bandwidth and is therefore likely to ensure fast progress
in the single-threaded processing core. However, when
multiple threads are accessing the memory system, ser-
vicing the requests in an order that ignores which thread
generated the request can unfairly delay some thread’s
memory requests while giving unfair preference to oth-
ers. As a consequence, the progress of an application
running on one core can be signiﬁcantly hindered by an
application executed on another.
In this paper, we identify the causes of unfairness in
the DRAM memory system that can result in DoS attacks
by MPHs. We show how MPHs can be implemented and
quantify the performance loss of applications due to un-
fairness in the memory system. Finally, we propose a
new memory system design that is based on a novel def-
inition of DRAM fairness. This design provides memory
access fairness across different threads in multi-core sys-
tems and thereby mitigates the impact caused by a mem-
ory performance hog.
The major contributions we make in this paper are:
• We expose a new Denial of Service attack that
can signiﬁcantly degrade application performance on
multi-core systems and we introduce the concept of
Memory Performance Hogs (MPHs). An MPH is an
application that can destroy the memory-related per-
formance of another application running on a differ-
ent processing core on the same chip.
258
16th USENIX Security Symposium
USENIX Association
• We demonstrate that MPHs are a real problem by
evaluating the performance impact of DoS attacks on
both real and simulated multi-core systems.
• We identify the major causes in the design of the
DRAM memory system that result in DoS attacks:
hardware algorithms that are unfair across different
threads accessing the memory system.
• We describe and evaluate a new memory system de-
sign that provides fairness across different threads
and mitigates the large negative performance impact
of MPHs.
2 Background
We begin by providing a brief background on multi-
core architectures and modern DRAM memory systems.
Throughout the section, we abstract away many details
in order to give just enough information necessary to
understand how the design of existing memory systems
could lend itself to denial of service attacks by explicitly-
malicious programs or real applications. Interested read-
ers can ﬁnd more details in [30, 8, 41].
2.1 Multi-Core Architectures
Figure 1 shows the high-level architecture of a process-
ing system with one core (single-core), two cores (dual-
core) and N cores (N-core). In our terminology, a “core”
includes the instruction processing pipelines (integer and
ﬂoating-point), instruction execution units, and the L1
instruction and data caches. Many general-purpose com-
puters manufactured today look like the dual-core sys-
tem in that they have two separate but identical cores.
In some systems (AMD Athlon/Turion/Opteron, Intel
Pentium-D), each core has its own private L2 cache,
while in others (Intel Core Duo, IBM Power 4/5) the L2
cache is shared between different cores. The choice of a
shared vs. non-shared L2 cache affects the performance
of the system [19, 14] and a shared cache can be a pos-
sible source of vulnerability to DoS attacks. However,
this is not the focus of our paper because DoS attacks at
the L2 cache level can be easily prevented by providing
a private L2 cache to each core (as already employed by
some current systems) or by providing “quotas” for each
core in a shared L2 cache [28].
Regardless of whether or not the L2 cache is shared,
the DRAM Memory System of current multi-core sys-
tems is shared among all cores.
In contrast to the L2
cache, assigning a private DRAM memory system to
each core would signiﬁcantly change the programming
model of shared-memory multiprocessing, which is com-
monly used in commercial applications. Furthermore,
in a multi-core system, partitioning the DRAM memory
system across cores (while maintaining a shared-memory
programming model) is also undesirable because:
1. DRAM memory is still a very expensive resource
in modern systems. Partitioning it requires more
DRAM chips along with a separate memory con-
troller for each core, which signiﬁcantly increases the
cost of a commodity general-purpose system, espe-
cially in future systems that will incorporate tens of
cores on chip.
2. In a partitioned DRAM system, a processor access-
ing a memory location needs to issue a request to the
DRAM partition that contains the data for that loca-
tion. This incurs additional latency and a communi-
cation network to access another processor’s DRAM
if the accessed address happens to reside in that par-
tition.
For these reasons, we assume in this paper that each core
has a private L2 cache but all cores share the DRAM
memory system. We now describe the design of the
DRAM memory system in state-of-the-art systems.
2.2 DRAM Memory Systems
A DRAM memory system consists of three major com-
ponents: (1) the DRAM banks that store the actual data,
(2) the DRAM controller (scheduler) that schedules com-
mands to read/write data from/to the DRAM banks, and
(3) DRAM address/data/command buses that connect the
DRAM banks and the DRAM controller.
2.2.1 DRAM Banks
A DRAM memory system is organized into multiple
banks such that memory requests to different banks can
be serviced in parallel. As shown in Figure 2 (left), each
DRAM bank has a two-dimensional structure, consisting
of multiple rows and columns. Consecutive addresses in
memory are located in consecutive columns in the same
row.3 The size of a row varies, but it is usually between
1-8Kbytes in commodity DRAMs. In other words, in a
system with 32-byte L2 cache blocks, a row contains 32-
256 L2 cache blocks.
Each bank has one row-buffer and data can only be read
from this buffer. The row-buffer contains at most a sin-
gle row at any given time. Due to the existence of the
row-buffer, modern DRAMs are not truly random access
(equal access time to all locations in the memory array).
Instead, depending on the access pattern to a bank, a
DRAM access can fall into one of the three following
categories:
1. Row hit: The access is to the row that is already in
the row-buffer. The requested column can simply
be read from or written into the row-buffer (called
a column access). This case results in the lowest
latency (typically 30-50ns round trip in commodity
3Note that consecutive memory rows are located in different banks.
USENIX Association
16th USENIX Security Symposium
259
CHIP
CHIP
CHIP
CORE
CORE 1
CORE 2
CORE 1
CORE N
L2 CACHE
L2 CACHE
L2 CACHE
L2 CACHE
L2 CACHE
. . .
DRAM MEMORY
CONTROLLER
...
DRAM BANKS
DRAM BUS
DRAM MEMORY
CONTROLLER
...
DRAM BANKS
DRAM Memory System
DRAM Memory System
. . .
DRAM MEMORY
CONTROLLER
DRAM BUS
DRAM BUS
...
DRAM BANKS
DRAM Memory System
Figure 1: High-level architecture of an example single-core system (left), a dual-core system (middle), and an N-core
system (right). The chip is shaded. The DRAM memory system, part of which is off chip, is encircled.
L2 Cache 0
Requests
L2 Cache N−1
Requests
Row Address
0
n
m
u
l
o
C
r
e
d
o
c
e
D
s
s
e
r
d
d
A
w
o
R
ROW 0
ROW 1
1
−
C
n
m
u
l
o
C
ROW R−2
ROW R−1
ROW BUFFER
Column Address
Column Decoder
Address
Data
To/From Cores
s
u
B
a
t
a
D
p
i
h
C
−
n
O
s
u
B
a
t
a
D
M
A
R
D
BANK 0
REQUEST
BUFFER
Bank 0
Scheduler
. . .
Crossbar
. . .
. . .
Memory Request
Buffer
Memory Access
Scheduler
BANK B−1
REQUEST
BUFFER
Bank B−1
Scheduler
DRAM Bus Scheduler
Selected Address and DRAM Command
DRAM Address/Command Bus
To DRAM Banks
To/From DRAM Banks
Figure 2: Left: Organization of a DRAM bank, Right: Organization of the DRAM controller
DRAM, including data transfer time, which trans-
lates into 90-150 processor cycles for a core run-
ning at 3GHz clock frequency). Note that sequen-
tial/streaming memory access patterns (e.g. accesses
to cache blocks A, A+1, A+2, ...) result in row hits
since the accessed cache blocks are in consecutive
columns in a row. Such requests can therefore be
handled relatively quickly.
2. Row conﬂict: The access is to a row different from
the one that is currently in the row-buffer.
In this
case, the row in the row-buffer ﬁrst needs to be writ-
ten back into the memory array (called a row-close)
because the row access had destroyed the row’s data
in the memory array. Then, a row access is per-
formed to load the requested row into the row-buffer.
Finally, a column access is performed. Note that this
case has much higher latency than a row hit (typically
60-100ns or 180-300 processor cycles at 3GHz).
3. Row closed: There is no row in the row-buffer. Due
to various reasons (e.g.
to save energy), DRAM
memory controllers sometimes close an open row in
the row-buffer, leaving the row-buffer empty. In this
case, the required row needs to be ﬁrst loaded into the