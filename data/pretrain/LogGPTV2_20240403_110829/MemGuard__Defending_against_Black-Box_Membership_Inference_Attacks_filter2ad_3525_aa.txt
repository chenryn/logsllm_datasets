title:MemGuard: Defending against Black-Box Membership Inference Attacks
via Adversarial Examples
author:Jinyuan Jia and
Ahmed Salem and
Michael Backes and
Yang Zhang and
Neil Zhenqiang Gong
9
1
0
2
c
e
D
8
1
]
R
C
.
s
c
[
3
v
4
9
5
0
1
.
9
0
9
1
:
v
i
X
r
a
MemGuard: Defending against Black-Box Membership
Inference Attacks via Adversarial Examples
Jinyuan Jia
ECE Department
Duke University
PI:EMAIL
Ahmed Salem
CISPA Helmholtz Center for
Information Security
ahmed.salem@cispa.saarland
Michael Backes
CISPA Helmholtz Center for
Information Security
backes@cispa.saarland
Yang Zhang
CISPA Helmholtz Center for
Information Security
zhang@cispa.saarland
Neil Zhenqiang Gong
ECE Department
Duke University
PI:EMAIL
ABSTRACT
In a membership inference attack, an attacker aims to infer whether
a data sample is in a target classifier’s training dataset or not. Specif-
ically, given a black-box access to the target classifier, the attacker
trains a binary classifier, which takes a data sample’s confidence
score vector predicted by the target classifier as an input and pre-
dicts the data sample to be a member or non-member of the target
classifier’s training dataset. Membership inference attacks pose
severe privacy and security threats to the training dataset. Most
existing defenses leverage differential privacy when training the
target classifier or regularize the training process of the target clas-
sifier. These defenses suffer from two key limitations: 1) they do not
have formal utility-loss guarantees of the confidence score vectors,
and 2) they achieve suboptimal privacy-utility tradeoffs.
In this work, we propose MemGuard, the first defense with for-
mal utility-loss guarantees against black-box membership inference
attacks. Instead of tampering the training process of the target clas-
sifier, MemGuard adds noise to each confidence score vector pre-
dicted by the target classifier. Our key observation is that attacker
uses a classifier to predict member or non-member and classifier
is vulnerable to adversarial examples. Based on the observation,
we propose to add a carefully crafted noise vector to a confidence
score vector to turn it into an adversarial example that misleads the
attacker’s classifier. Specifically, MemGuard works in two phases.
In Phase I, MemGuard finds a carefully crafted noise vector that
can turn a confidence score vector into an adversarial example,
which is likely to mislead the attacker’s classifier to make a random
guessing at member or non-member. We find such carefully crafted
noise vector via a new method that we design to incorporate the
unique utility-loss constraints on the noise vector. In Phase II, Mem-
Guard adds the noise vector to the confidence score vector with a
certain probability, which is selected to satisfy a given utility-loss
budget on the confidence score vector. Our experimental results on
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CCS ’19, November 11–15, 2019, London, United Kingdom
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6747-9/19/11...$15.00
https://doi.org/10.1145/3319535.3363201
three datasets show that MemGuard can effectively defend against
membership inference attacks and achieve better privacy-utility
tradeoffs than existing defenses. Our work is the first one to show
that adversarial examples can be used as defensive mechanisms to
defend against membership inference attacks.
CCS CONCEPTS
• Security and privacy; • Computing methodologies → Ma-
chine learning;
KEYWORDS
Membership inference attacks; adversarial examples; privacy-preserving
machine learning
ACM Reference Format:
Jinyuan Jia, Ahmed Salem, Michael Backes, Yang Zhang, and Neil Zhen-
qiang Gong. 2019. MemGuard: Defending against Black-Box Membership
Inference Attacks via Adversarial Examples. In 2019 ACM SIGSAC Con-
ference on Computer and Communications Security (CCS ’19), November
11–15, 2019, London, United Kingdom. ACM, New York, NY, USA, 16 pages.
https://doi.org/10.1145/3319535.3363201
1 INTRODUCTION
Machine learning (ML) is transforming many aspects of our soci-
ety. We consider a model provider deploys an ML classifier (called
target classifier) as a black-box software or service, which returns
a confidence score vector for a query data sample from a user. The
confidence score vector is a probability distribution over the pos-
sible labels and the label of the query data sample is predicted as
the one that has the largest confidence score. Multiple studies have
shown that such black-box ML classifier is vulnerable to member-
ship inference attacks [43, 56, 58, 59]. Specifically, an attacker trains
a binary classifier, which takes a data sample’s confidence score
vector predicted by the target classifier as an input and predicts
whether the data sample is a member or non-member of the target
classifier’s training dataset. Membership inference attacks pose se-
vere privacy and security threats to ML. In particular, in application
scenarios where the training dataset is sensitive (e.g., biomedical
records and location traces), successful membership inference leads
to severe privacy violations. For instance, if an attacker knows her
victim’s data is used to train a medical diagnosis classifier, then the
attacker can directly infer the victim’s health status. Beyond privacy,
membership inference also damages the model provider’s intellec-
tual property of the training dataset as collecting and labeling the
training dataset may require lots of resources.
Therefore, defending against membership inference attacks is
an urgent research problem and multiple defenses [42, 56, 58] have
been explored. A major reason why membership inference attacks
succeed is that the target classifier is overfitted. As a result, the
confidence score vectors predicted by the target classifier are dis-
tinguishable for members and non-members of the training dataset.
Therefore, state-of-the-art defenses [42, 56, 58] essentially regular-
ize the training process of the target classifier to reduce overfitting
and the gaps of the confidence score vectors between members and
non-members of the training dataset. For instance, L2 regulariza-
tion [58], min-max game based adversarial regularization [42], and
dropout [56] have been explored to regularize the target classifier.
Another line of defenses [1, 6, 12, 24, 30, 60, 66, 70] leverage differen-
tial privacy [13] when training the target classifier. Since tampering
the training process has no guarantees on the confidence score
vectors, these defenses have no formal utility-loss guarantees on
the confidence score vectors. Moreover, these defenses achieve sub-
optimal tradeoffs between the membership privacy of the training
dataset and utility loss of the confidence score vectors. For instance,
Jayaraman and Evans [25] found that existing differentially private
machine learning methods rarely offer acceptable privacy-utility
tradeoffs for complex models.
Our work: In this work, we propose MemGuard, the first defense
with formal utility-loss guarantees against membership inference
attacks under the black-box setting. Instead of tampering the train-
ing process of the target classifier, MemGuard randomly adds noise
to the confidence score vector predicted by the target classifier
for any query data sample. MemGuard can be applied to an ex-
isting target classifier without retraining it. Given a query data
sample’s confidence score vector, MemGuard aims to achieve two
goals: 1) the attacker’s classifier is inaccurate at inferring member
or non-member for the query data sample after adding noise to the
confidence score vector, and 2) the utility loss of the confidence
score vector is bounded. Specifically, the noise should not change
the predicted label of the query data sample, since even 1% loss
of the label accuracy may be intolerable in some critical applica-
tions such as finance and healthcare. Moreover, the confidence
score distortion introduced by the noise should be bounded by a
budget since a confidence score vector intends to tell a user more
information beyond the predicted label. We formulate achieving
the two goals as solving an optimization problem. However, it is
computationally challenging to solve the optimization problem as
the noise space is large. To address the challenge, we propose a
two-phase framework to approximately solve the problem.
We observe that an attacker uses an ML classifier to predict
member or non-member and classifier can be misled by adversarial
examples [10, 19, 31, 47–50, 62]. Therefore, in Phase I, MemGuard
finds a carefully crafted noise vector that can turn the confidence
score vector into an adversarial example. Specifically, MemGuard
aims to find a noise vector such that the attacker’s classifier is likely
to make a random guessing at inferring member or non-member
based on the noisy confidence score vector. Since the defender does
not know the attacker’s classifier as there are many choices, the de-
fender itself trains a classifier for membership inference and crafts
the noise vector based on its own classifier. Due to transferabil-
ity [31, 32, 47, 62] of adversarial examples, the noise vector that
misleads the defender’s classifier is likely to also mislead the at-
tacker’s classifier. The adversarial machine learning community has
developed many algorithms (e.g., [10, 19, 31, 35, 39, 40, 50, 63]) to
find adversarial noise/examples. However, these algorithms are in-
sufficient for our problem because they did not consider the unique
constraints on utility loss of the confidence score vector. Specifically,
the noisy confidence score vector should not change the predicted
label of the query data sample and should still be a probability
distribution. To address this challenge, we design a new algorithm
to find a small noise vector that satisfies the utility-loss constraints.
In Phase II, MemGuard adds the noise vector found in Phase
I to the true confidence score vector with a certain probability.
The probability is selected such that the expected confidence score
distortion is bounded by the budget and the defender’s classifier
is most likely to make random guessing at inferring member or
non-member. Formally, we formulate finding this probability as
solving an optimization problem and derive an analytical solution
for the optimization problem.
We evaluate MemGuard and compare it with state-of-the-art
defenses [1, 42, 56, 58] on three real-world datasets. Our empir-
ical results show that MemGuard can effectively defend against
state-of-the-art black-box membership inference attacks [43, 56]. In
particular, as MemGuard is allowed to add larger noise (we measure
the magnitude of the noise using its L1-norm), the inference accura-
cies of all evaluated membership inference attacks become smaller.
Moreover, MemGuard achieves better privacy-utility tradeoffs than
state-of-the-art defenses. Specifically, given the same average confi-
dence score distortion, MemGuard reduces the attacker’s inference
accuracy at inferring member/non-members by the most.
In summary, our key contributions are as follows:
• We propose MemGuard, the first defense with formal utility-
loss guarantees against membership inference attacks under
the black-box setting.
• We propose a new algorithm to find a noise vector that sat-
isfies the unique utility-loss constraints in Phase I of Mem-
Guard. Moreover, in Phase II, we derive an analytical solution
of the probability with which MemGuard adds the noise vec-
tor to the confidence score vector.
• We evaluate MemGuard on three real-world datasets. Our
results show that MemGuard is effective and outperforms
existing defenses.
2 RELATED WORK
2.1 Membership Inference
Membership inference attacks: The goal of membership infer-
ence is to determine whether a certain data sample is inside a
dataset. Homer et al. [23] proposed the first membership inference
attack in the biomedical setting, in particular on genomic data.
Specifically, they showed that an attacker can compare a user’s
genomic data with the summary statistics of the target database,
such as mean and standard deviation, to determine the presence
of the user in the database. The comparison can be done by using
statistical testing methods such as log-likelihood ratio test. Later,
several works performed similar membership inference attacks
against other types of biomedical data such as MicroRNA [4] and
DNA methylation [20]. Recently, Pyrgelis et al. [52, 53] further
showed that membership inference can also be performed effec-
tively against location databases. In particular, they showed that an
attacker can infer whether a user’s location dataset was used for
computing a given aggregate location dataset.
Membership inference attacks against ML models: Shokri et
al. [58] introduced membership inference in the ML setting. The
goal here is to determine whether a data sample is in the training
dataset of a target black-box ML classifier. To achieve the goal, the
attacker trains binary ML classifiers, which take a data sample’s
confidence score vector predicted by the target classifier as input
and infer the data sample to be a member or non-member of the
target classifier’s training dataset. We call these classifiers attack
classifiers and they are trained using shadow classifiers. Specifically,
the attacker is assumed to have a dataset coming from the same
distribution as the target classifier’s training dataset and the at-
tacker uses the dataset to train shadow classifiers, each of which
aims to replicate the target classifier. Then, the attacker trains the
attack classifiers by using the confidence score vectors predicted
by the shadow classifiers for some members and non-members of
the shadow classifiers’ training datasets.
Salem et al. [56] recently proposed new membership inference
attacks for black-box target classifiers, which relax the assumptions
of the attacks proposed by Shokri et al. from both model and data
angles. For instance, they showed that the attacker can rank the
entries in a confidence score vector before feeding it into an attack
classifier, which improves the attack effectiveness. Moreover, they
showed that it is sufficient for the attacker to train just one shadow
classifier. These results indicate that membership inference threat
is even larger than previously thought.
More recently, Nasr et al. [43] proposed membership inference
attacks against white-box ML models. For a data sample, they cal-