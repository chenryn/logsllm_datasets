```
docker run -d -p 2379:2379 -p 2380:2380 -p 4001:4001 \
--name etcd quay.io/coreos/etcd \
-name etcd-m -initial-advertise-peer-urls http://$(docker-machine 
    ip etcd-m):2380 \
-listen-peer-urls http://0.0.0.0:2380 \
-listen-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001 \
-advertise-client-urls http://$(docker-machine ip etcd-m):2379 \
-initial-cluster-token etcd-cluster-1 \
-initial-cluster etcd-m=http://$(docker-machine ip etcd-m):2380
-initial-cluster-state new
```
我们在这里要做的是在守护进程(`-d`)模式下启动 Etcd 映像，并公开端口`2379` (Etcd 客户端通信)、`2380` (Etcd 服务器通信)、`4001`()，并指定以下 Etcd 选项:
*   `name`:节点的名称，在这种情况下我们选择 etcd-m，作为托管这个容器的节点的名称
*   `initial-advertise-peer-urls`在这个静态配置中是地址:集群的端口
*   `listen-peer-urls`
*   `listen-client-urls`
*   `advertise-client-urls`
*   `initial-cluster-token`
*   `initial-cluster`
*   `initial-cluster-state`
我们可以使用`etcdctl cluster-health`命令行实用程序来确保这个单节点 Etcd 集群是健康的:
```
term0$ docker run fsoppelsa/etcdctl -C $(dm ip etcd-m):2379 
    cluster-health
```
![Re architecting the example of Chapter 1 with Etcd](img/image_02_009.jpg)
这表明 Etcd 至少已经启动并运行，因此我们可以使用它来设置 Swarm v1 集群。
我们在同一台`etcd-m`主机上创建了 Swarm 管理器:
```
term0$ docker run -d -p 3376:3376 swarm manage \
-H tcp://0.0.0.0:3376 \`
etcd://$(docker-machine ip etcd-m)/swarm
```
这暴露了从主机到容器的通常的`3376`端口，但是这一次使用发现服务的`etcd://` URL 启动管理器。
我们现在加入节点，`etcd-1`、`etcd-2`和`etcd-3`。
像往常一样，我们可以为每个终端提供一台机器:
```
term1$ eval $(docker-machine env etcd-1)
term1$ docker run -d swarm join --advertise \
$(docker-machine ip etcd-1):2379 \
etcd://$(docker-machine ip etcd-m):2379
term2$ eval $(docker-machine env etcd-2)
term1$ docker run -d swarm join --advertise \
$(docker-machine ip etcd-2):2379 \
etcd://$(docker-machine ip etcd-m):2379
term3$ eval $(docker-machine env etcd-3)
term3$ docker run -d swarm join --advertise \
$(docker-machine ip etcd-3):2379 \
etcd://$(docker-machine ip etcd-m):2379
```
通过加入`-advertise`，我们命令本地节点加入 Swarm 集群，使用在`etcd-m`上运行和公开的 Etcd 服务。
我们现在转到`etcd-m`并通过调用 Etcd 发现服务来查看我们集群的节点:
![Re architecting the example of Chapter 1 with Etcd](img/image_02_010.jpg)
我们已经按照预期将三台主机加入到集群中。
# 动物园管理员
ZooKeeper 是另一个广泛使用的高性能分布式应用协调服务。Apache ZooKeeper 最初是 Hadoop 的一个子项目，但现在是一个顶级项目。这是一个高度一致、可扩展和可靠的键值存储，可用作 Docker Swarm v1 集群的发现服务。如前所述，ZooKeeper 使用的是 Paxos，而不是 Raft。
与 Etcd 类似，当 ZooKeeper 形成一个具有仲裁的节点集群时，它有一个领导者，其余的节点是追随者。在内部，动物园管理员使用自己的 ZAB，动物园管理员广播协议，以保持一致性和完整性。
# 领事
我们将在这里看到的最后一个发现服务是 Consul，这是一个用于发现和配置服务的工具。它提供了一个允许客户端注册和发现服务的应用编程接口。与 Etcd 和 ZooKeeper 类似，Consul 是一个带有 REST API 的键值存储。它可以执行健康检查以确定服务可用性，并通过 Sef 库使用 Raft 共识算法。当然，与 Etcd 和 Zookeeper 类似，Consul 可以通过领袖选举形成高可用性的法定人数。其会员管理系统基于`memberlist`，一个高效的 Gossip 协议实现。
## 用 Consul 重新设计第 1 章的例子
我们现在将创建另一个 Swarm v1，但在本节中，我们将在云提供商 DigitalOcean 上创建机器。为此，您需要一个访问令牌。但是如果你没有 DigitalOcean 账号，可以用`--driver virtualbox`替换`--driver digitalocean`，在本地运行这个例子。
让我们从创建领事大师开始:
```
$ docker-machine create --driver digitalocean consul-m
$ eval $(docker-machine env consul-m)
```
我们从这里开始第一个特工。虽然我们称它为代理，但实际上我们将在服务器模式下运行它。我们使用服务器模式(`-server`)并使其成为引导节点(`-bootstrap`)。有了这些选项，执政官将不执行领导者选择，因为它将强迫自己成为领导者。
```
$ docker run -d --name=consul --net=host \
consul agent \
-client=$(docker-machine ip consul-m) \
-bind=$(docker-machine ip consul-m) \
-server -bootstrap
```
在高可用性达到法定人数的情况下，第二个和第三个必须从`-botstrap-expect 3`开始，以允许它们形成高可用性集群。
现在，我们可以使用`curl` 命令来测试我们的领事仲裁是否成功启动。
```
$ curl -X GET http://$(docker-machine ip consul-m):8500/v1/kv/
```
如果它没有显示任何错误，那么领事工作正常。
接下来，我们将在数字海洋上再创建三个节点。
```
$ for i in `seq 1 3`; do docker-machine create -d digitalocean 
    consul-$i; 
    done
```
让我们开始掌握并使用 Consul 作为发现机制:
```
$ eval $(docker-machine env consul-m)
$ docker run -d -p 3376:3376 swarm manage \
-H tcp://0.0.0.0:3376 \
consul://$(docker-machine ip consul-m):8500/swarm
$ eval $(docker-machine env consul-1)
$ docker run -d swarm join \
 --advertise $(docker-machine ip consul-1):2376 \
 consul://$(docker-machine ip consul-m):8500/swarm
$ eval $(docker-machine env consul-2)
$ docker run -d swarm join \
 --advertise $(docker-machine ip consul-2):2376 \
 consul://$(docker-machine ip consul-m):8500/swarm
$ eval $(docker-machine env consul-3)
$ docker run -d swarm join \
 --advertise $(docker-machine ip consul-3):2376 \
 consul://$(docker-machine ip consul-m):8500/swarm
```
以下是我们在运行`swarm list`命令时得到的结果:所有节点都加入了 Swarm，因此示例正在运行。
```
$ docker run swarm list consul://$(docker-machine ip consul-m):8500/swarm                                       time="2016-07-01T21:45:18Z" level=info msg="Initializing discovery without TLS"
104.131.101.173:2376
104.131.63.75:2376
104.236.56.53:2376
```
# 走向分散的发现服务
Swarm v1 架构的局限性在于它使用了一个集中式的外部发现服务。这种方法使每个代理都可以与外部发现服务进行对话，发现服务服务器的负载可能会呈指数级增长。从我们的实验来看，对于一个 500 节点的集群，我们建议形成一个高可用性发现服务，至少有三台中高规格的机器，比如 8 个内核和 8 GB 内存。
为了正确地解决这个问题，SwarmKit 和 Swarm Mode 使用的发现服务在设计时考虑了分散化。Swarm 模式在所有节点上使用相同的发现服务代码库 Etcd，没有单点故障。
# 总结
在本章中，我们熟悉了共识和发现服务的概念。我们知道它们在编排集群中扮演着重要的角色，因为它们提供诸如容错和安全配置等服务。在查看两个具体的 Raft 发现服务实现 Etcd 和 Consul 之前，我们详细分析了一个共识算法，例如 Raft，将事情付诸实践并使用它们重新构建基本示例。在下一章中，我们将开始探索使用嵌入式 Etcd 库的 SwarmKit 和 Swarm。