Shannon entropy, and Mi is the ith feature of the dataset.
Intuitively, the information gain can be thought of as
measuring the amount of information that the observa-
tion of the value of feature i gives about the class label
associated with the example.
To reduce the total size and sparsity of the feature vec-
tor, we retained only those features that individually had
non-zero information gain.
(These features can be re-
ferred to as IG-CSFS throughout the rest of the paper.)
Note that, as H(A|Mi) ≤ H(A), information gain is al-
ways non-negative. While the use of information gain
on a variable-per-variable basis implicitly assumes inde-
pendence between the features with respect to their im-
pact on the class label, this conservative approach to fea-
ture selection means that we only use features that have
demonstrable value in classiﬁcation.
To validate this approach to feature selection, we ap-
plied this method to two distinct sets of source code ﬁles,
and observed that sets of features with non-zero informa-
tion gain were nearly identical between the two sets, and
the ranking of features was substantially similar between
the two. This suggests that the application of information
gain to feature selection is producing a robust and con-
sistent set of features (see Section 4 for further discus-
sion). All the results are calculated by using CSFS and
IG-CSFS. Using IG-CSFS on all experiments demon-
strates how these features generalize to different datasets
that are larger in magnitude. One other advantage of IG-
CSFS is that it consists of a few hundred features that
result in non-sparse feature vectors. Such a compact rep-
resentation of coding style makes de-anonymizing thou-
sands of programmers possible in minutes.
3.3.2 Random Forest Classiﬁcation
We used the random forest ensemble classiﬁer [7] as
our classiﬁer for authorship attribution. Random forests
are ensemble learners built from collections of decision
trees, each of which is grown by randomly sampling
N training samples with replacement, where N is the
number of instances in the dataset. To reduce correla-
tion between trees, features are also subsampled; com-
monly (logM) + 1 features are selected at random (with-
out replacement) out of M, and the best split on these
(logM) +1 features is used to split the tree nodes. The
number of selected features represents one of the few
tuning parameters in random forests: increasing the num-
ber of features increases the correlation between trees in
the forest which can harm the accuracy of the overall en-
semble, however increasing the number of features that
can be chosen at each split increases the classiﬁcation ac-
curacy of each individual tree making them stronger clas-
siﬁers with low error rates. The optimal range of number
of features can be found using the out of bag (oob) error
estimate, or the error estimate derived from those sam-
ples not selected for training on a given tree.
During classiﬁcation, each test example is classiﬁed
via each of the trained decision trees by following the bi-
nary decisions made at each node until a leaf is reached,
and the results are then aggregated. The most populous
class can be selected as the output of the forest for simple
classiﬁcation, or classiﬁcations can be ranked according
to the number of trees that ‘voted’ for a label when per-
forming relaxed attribution (see Section 4.3.4).
We employed random forests with 300 trees, which
empirically provided the best trade-off between accuracy
and processing time. Examination of numerous oob val-
ues across multiple ﬁts suggested that (logM) + 1 ran-
dom features (where M denotes the total number of fea-
tures) at each split of the decision trees was in fact op-
timal in all of the experiments (listed in Section 4), and
was used throughout. Node splits were selected based on
the information gain criteria, and all trees were grown to
the largest extent possible, without pruning.
The data was analyzed via k-fold cross-validation,
where the data was split into training and test sets strat-
iﬁed by author (ensuring that the number of code sam-
ples per author in the training and test sets was identi-
cal across authors). k varies according to datasets and
is equal to the number of instances present from each
author. The cross-validation procedure was repeated 10
times, each with a different random seed. We report the
average results across all iterations in the results, ensur-
ing that they are not biased by improbably easy or difﬁ-
cult to classify subsets.
4 Evaluation
In the evaluation section, we present the results to the
possible scenarios formulated in the problem statement
and evaluate our method. The corpus section gives an
overview of the data we collected. Then, we present the
main results to programmer de-anonymization and how
it scales to 1,600 programmers, which is an immediate
privacy concern for open source contributors that prefer
to remain anonymous. We then present the training data
requirements and efﬁcacy of types of features. The ob-
fuscation section discusses a possible countermeasure to
programmer de-anonymization. We then present possi-
ble machine learning formulations along with the veriﬁ-
cation section that extends the approach to an open world
problem. We conclude the evaluation with generalizing
the method to other programming languages and provid-
ing software engineering insights.
260  24th USENIX Security Symposium 
USENIX Association
6
4.1 Corpus
One concern in source code authorship attribution is that
we are actually identifying differences in coding style,
rather than merely differences in functionality. Consider
the case where Alice and Bob collaborate on an open
source project. Bob writes user interface code whereas
Alice works on the network interface and backend ana-
lytics. If we used a dataset derived from their project,
we might differentiate differences between frontend and
backend code rather than differences in style.
In order to minimize these effects, we evaluate our
method on the source code of solutions to programming
tasks from the international programming competition
Google Code Jam (GCJ), made public in 2008 [2]. The
competition consists of algorithmic problems that need
to be solved in a programming language of choice. In
particular, this means that all programmers solve the
same problems, and hence implement similar functional-
ity, a property of the dataset crucial for code stylometry
analysis.
The dataset contains solutions by professional pro-
grammers, students, academics, and hobbyists from 166
countries. Participation statistics are similar over the
years. Moreover, it contains problems of different dif-
ﬁculty, as the contest takes place in several rounds. This
allows us to assess whether coding style is related to pro-
grammer experience and problem difﬁculty.
The most commonly used programming language was
C++, followed by Java, and Python. We chose to inves-
tigate source code stylometry on C++ and C because of
their popularity in the competition and having a parser
for C/C++ readily available [32]. We also conducted
some preliminary experimentation on Python.
A validation dataset was created from 2012’s GCJ
competition. Some problems had two stages, where the
second stage involved answering the same problem in a
limited amount of time and for a larger input. The so-
lution to the large input is essentially a solution for the
small input but not vice versa. Therefore, collecting both
of these solutions could result in duplicate and identical
source code. In order to avoid multiple entries, we only
collected the small input versions’ solutions to be used in
our dataset.
The programmers had up to 19 solution ﬁles in these
datasets. Solution ﬁles have an average of 70 lines of
code per programmer.
To create our experimental datasets that are discussed
in further detail in the results section;
(i) We ﬁrst partitioned the corpus of ﬁles by year of com-
petition. The “main” dataset includes ﬁles drawn from
2014 (250 programmers). The “validation” dataset ﬁles
come from 2012, and the “multi-year” dataset ﬁles come
from years 2008 through 2014 (1,600 programmers).
(ii) Within each year, we ordered the corpus ﬁles by the
round in which they were written, and by the problem
within a round, as all competitors proceed through the
same sequence of rounds in that year. As a result, we
performed stratiﬁed cross validation on each program ﬁle
by the year it was written, by the round in which the pro-
gram was written, by the problems solved in the round,
and by the author’s highest round completed in that year.
Some limitations of this dataset are that it does not al-
low us to assess the effect of style guidelines that may
be imposed on a project or attributing code with mul-
tiple/mixed programmers. We leave these interesting
questions for future work, but posit that our improved re-
sults with basic stylometry make them worthy of study.
4.2 Applications
In this section, we will go over machine learning task
formulations representing ﬁve possible real-world appli-
cations presented in Section 2.
4.2.1 Multiclass Closed World Task
This
section presents our main experiment—de-
anonymizing 250 programmers in the difﬁcult scenario
where all programmers solved the same set of prob-
lems.
The machine learning task formulation for
de-anonymizing programmers also applies to ghostwrit-
ing detection. The biggest dataset formed from 2014’s
Google Code Jam Competition with 9 solution ﬁles to
the same problem had 250 programmers. These were the
easiest set of 9 problems, making the classiﬁcation more
challenging (see Section 4.3.6). We reached 91.78%
accuracy in classifying 250 programmers with the Code
Stylometry Feature Set. After applying information gain
and using the features that had information gain, the
accuracy was 95.08%.
We also took 250 programmers from different years
and randomly selected 9 solution ﬁles for each one of
them. We used the information gain features obtained
from 2014’s dataset to see how well they generalize.
We reached 98.04% accuracy in classifying 250 pro-
grammers. This is 3% higher than the controlled large
dataset’s results. The accuracy might be increasing be-
cause of using a mixed set of Google Code Jam prob-
lems, which potentially contains the possible solutions’
properties along with programmers’ coding style and
makes the code more distinct.
We wanted to evaluate our approach and validate our
method and important features. We created a dataset
from 2012’s Google Code Jam Competition with 250
programmers who had the solutions to the same set of
9 problems. We extracted only the features that had pos-
itive information gain in 2014’s dataset that was used as
USENIX Association  
24th USENIX Security Symposium  261
7
the main dataset to implement the approach. The classi-
ﬁcation accuracy was 96.83%, which is higher than the
95.07% accuracy obtained in 2014’s dataset.
The high accuracy of validation results in Table 5 show
that we identiﬁed the important features of code stylom-
etry and found a stable feature set. This feature set does
not necessarily represent the exact features for all pos-
sible datasets. For a given dataset that has ground truth
information on authorship, following the same approach
should generate the most important features that repre-
sent coding style in that particular dataset.
A = #programmers, F = max #problems completed
N = #problems included in dataset (N ≤ F)
A = 250 all years
F ≥ 9 all years
N = 9
A = 250 from 2014
F = 9 from 2014
A = 250 from 2012
F = 9 from 2014
N = 9
N = 9
Average accuracy after 10 iterations with IG-CSFS features
95.07%
96.83%
98.04%
Table 5: Validation Experiments
4.2.2 Mutliclass Open World Task
The experiments in this section can be used in software
forensics to ﬁnd out the programmer of a piece of mal-
ware. In software forensics, the analyst does not know if
source code belongs to one of the programmers in the
candidate set of programmers.
In such cases, we can
classify the anonymous source code, and if the majority
number of votes of trees in the random forest is below a
certain threshold, we can reject the classiﬁcation consid-
ering the possibility that it might not belong to any of the
classes in the training data. By doing so, we can scale
our approach to an open world scenario, where we might
not have encountered the suspect before. As long as we
determine a conﬁdence threshold based on training data
[30], we can calculate the probability that an instance
belongs to one of the programmers in the set and accord-
ingly accept or reject the classiﬁcation.
We performed 270 classiﬁcations in a 30-class prob-
lem using all the features to determine the conﬁdence
threshold based on the training data. The accuracy was
96.67%. There were 9 misclassiﬁcations and all of them
were classiﬁed with less than 15% conﬁdence by the
classiﬁer. The class probability or classiﬁcation conﬁ-
dence that source code fragment C is of class i is cal-
culated by taking the percentage of trees in the random
forest that voted for that particular class, as follows2:
P(Ci) =
∑ j Vj(i)
|T| f
(2)
8
Where Vj(i) =1 if the jth tree voted for class i and
0 otherwise, and |T| f denotes the total number of trees
in forest f . Note that by construction, ∑i P(Ci) =1 and
P(Ci) ≥ 0 ∀ i, allowing us to treat P(Ci) as a probability
measure.
There was one correct classiﬁcation made with 13.7%
conﬁdence. This suggests that we can use a threshold be-
tween 13.7% and 15% conﬁdence level for veriﬁcation,
and manually analyze the classiﬁcations that did not pass
the conﬁdence threshold or exclude them from results.
We picked an aggressive threshold of 15% and to vali-
date it, we trained a random forest classiﬁer on the same
set of 30 programmers 270 code samples. We tested on
150 different ﬁles from the programmers in the training
set. There were 6 classiﬁcations below the 15% threshold
and two of them were misclassiﬁed. We took another set
of 420 test ﬁles from 30 programmers that were not in the
training set. All the ﬁles from the 30 programmers were
attributed to one of the 30 programmers in the training
set since this is a closed world classiﬁcation task, how-
ever, the highest conﬁdence level in these classiﬁcations
was 14.7%. The 15% threshold catches all the instances
that do not belong to the programmers in the suspect set,
gets rid of 2 misclassiﬁcations and 4 correct classiﬁca-
tions. Consequently, when we see a classiﬁcation with
less than a threshold value, we can reject the classiﬁca-
tion and attribute the test instance to an unknown suspect.
4.2.3 Two-class Closed World Task
Source code author identiﬁcation could automatically
deal with source code copyright disputes without requir-
ing manual analysis by an objective code investigator.
A copyright dispute on code ownership can be resolved
by comparing the styles of both parties claiming to have
generated the code. The style of the disputed code can
be compared to both parties’ other source code to aid in
the investigation. To imitate such a scenario, we took
60 different pairs of programmers, each with 9 solution