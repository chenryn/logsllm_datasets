When a Kafka index task successfully publishes some segments then it persists
the latest consumed offsets info in the metadata store. Now if the supervisor
is stopped or stops because of some reason and restarted at a later point and
the earliest msg offset in Kafka is greater than persisted offset in the
metadata (probably because earlier messages were dropped by kafka brokers or
something else) then Kafka Index task will get `OffsetOutOfRangeException` and
keep on retrying the same offset indefinitely.
One way to solve this issue is to reset the consumer offset if the persisted
offset is less than the earliest offset at the Kafka broker. Although I think
in this case the persisted offset information would need to be delete from
metadata store otherwise the new tasks will fail to publish segment because of
consecutive offset check. Any thoughts ?