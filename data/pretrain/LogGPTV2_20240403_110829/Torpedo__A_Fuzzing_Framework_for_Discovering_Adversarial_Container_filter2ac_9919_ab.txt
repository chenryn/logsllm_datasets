the container, allowing the container to share the host kernel.
Examples include the default runtime packaged with Docker,
runc [10], and the Red Hat crun [24], which is written in C for
setting up the environment before the container process starts.
Sandboxed runtimes introduce a translation layer between the
container and the host kernel, and gVisor [12] is a secure
runtime that reduces the attack space on the host kernel by
implementing a large portion of the syscall interface with
a smaller number of syscalls. At the kernel level, containers
depend on multiple independent Linux kernel components (e.g.,
namespace and cgroups) to enforce isolation among user-space
instances. Particularly, cgroups (i.e., control groups) are the
key features for controlling and limiting the total amount of
system resources for containers. We next discuss the cgroups
mechanism in detail.
A. Linux Control Group
Modern Linux OS features cgroups as a highly ﬂexible and
conﬁgurable way to control the dynamic computing resource
allocation, including (CPU) runtime, memory, input/output
(I/O), and network bandwidth. cgroups quantitatively limit
the amount of resources assigned to a container, thus ideally, it
is designed to prevent one or a particular group of containers
from draining all the available computing resources of other
containers or the host machine. Typically,
the cgroups
mechanism partitions groups of processes into hierarchical
groups with controlled behaviors, and relies on different
resource controllers (or named as subsystems) to limit, account
for, and isolate various types of system resources.
The control groups mechanism is one keystone constituting
containerization platforms, enforcing both cross-container
isolation and container-to-host isolation on multiple types of
system resources. As mentioned above, cgroups specify the
resource allowance for one or a set of containers. For instance,
by specifying the CPU usage share of one container as 512
and another container as 1024, the latter one is provisioned
to get roughly double amount of CPU time compared to the
ﬁrst one. Nevertheless, enforced cgroups, none of these two
containers can starve the other one, even if they are competing
the same CPU core. Similarly, cgroups helps to prevent
containers from draining resource over the host machine. The
cpu controller can provide a hard limit on the maximum amount
of resource utilized by a container, by specifying a quota
and period. Each container can only consume up to “quota”
microseconds within each given “period” in microseconds. For
a container set with 50,000 “quota” and 50,000 “period”, it can
consume up to the total CPU cycles of one CPU core. More
importantly, cgroups have an inheritance mechanism, ensuing
that all child processes inherit the exactly same cgroups
attributes from their parent processes, which guarantees that
all child processes will be conﬁned under the same cgroups
policies. Overall, cgroups provide a ﬂexible mechanism
to specify and enforce the resource quota for containers,
smoothly enabling the “pay-as-you-go” scheme for real-world
cloud platforms. More importantly, a correctly designed and
implemented cgroup mechanism shall prevent most cross-
container or container-to-host attack vectors such as Denial-of-
Service (DoS) attacks in the ﬁrst place.
B. Container Attacks by Abusing Resource Allocation
Despite the encouraging and ﬂexible enforcement provided
by Linux namespaces and cgroups, we have observed various
real-world exploitations toward containerization platforms. In a
multi-tenant environment where multiple containers belonging
to different tenants run on the same physical machine, malicious
containers might turn other co-resident containers or the host
into mal-functional. For instance, a malicious container can
drain most of the CPU computing resources and starve other
containers or even the host OS.
Ideally, the resource consumed by a container is limited by
cgroups. However, previous work [29], [45] demonstrates
that inherited cgroups conﬁnement via process creation cannot
always guarantee consistent and fair resource accounting, and
it is possible to break the resource rein of cgroups. Gao et
al. [29] designed a set of exploiting strategies to generate out-
of-band workloads on another process (in a different cgroup)
on behalf of a constrained original (malicious) process. The
consequence is huge: Gao et al. [29] demonstrated that, by
escaping the resource limit of cgroups, a container can
consume system resources (e.g., CPU) as much as 200× of
its limit, and signiﬁcantly degrade the performance of other
co-resident containers to only 5%.
Defer Work to the Kernel. The ﬁrst type of strategy is to
defer or delegate workload to the kernel, as all kernel threads
are attached to the root cgroup. The amount of resources
consumed by those workloads would be counted to the target
kernel thread, instead of the initiating user-space process (i.e.,
the container). The Linux kernel by default runs multiple kernel
threads, including kworker for handling workqueue tasks [1]
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:18:21 UTC from IEEE Xplore.  Restrictions apply. 
403
and ksoftirqd for serving softirqs. Also, a container process
can exploit kernel threads as proxies to spawn new processes
(which are still attached to the root cgroup), and thus escape
the resource control. One feasible solution is to exploit the
usermode helper API, which provides a simple interface
for creating a process in the user-space. In both cases, the
corresponding consumed resources would not be limited by
any cgroups.
Deferring Work to Other Process cgroups. The second
type of strategy is to delegate workload to other userspace
processes, various system daemons and services, which are
all attached to other cgroups than a containerized process. For
example, a malicious container can exploit multiple system
processes (e.g., systemd) maintained by the Linux server for
purposes like process management, system information logging,
debugging, or container engine processes, which are required
to run on the host to support and manage container instances.
The corresponding consumed resources would not be charged
to the initiating process (i.e., malicious container), and thus
the cgroups mechanism can be escaped.
However, we consider the existing research has never fulﬁlled
its potential by conducting a systematic and comprehensive
study on resource allocation. Previous works largely rely on
manual analysis and thus can only ﬁnd limited exploiting
methods. We thus intend to develop a system to automatically
uncover those vulnerabilities in containerization platforms.
III. PROBLEM FORMULATION AND APPROACH
In this section, we formulate the research problem and
discuss the opportunities to address it with fuzzing.
A. Problem Formulation
In general, containerization platforms are designed to deliver
a conﬁned provision for container instances, in terms of both
static and dynamic computing resources. Container instances
should not go over a pre-deﬁned amount of static computing
resources. More importantly, the provision of runtime resources
should not be changed no matter how the resource is accessed
by other container instances; violation of such provision may
be due to bugs or inherent design limits, revealing chances of
conducting exploitations.
Threat Model. We consider standard multi-tenant environ-
ments where multiple containers belonging to different tenants
share the same physical machine. All containers are conﬁned
with proper resource isolation and thus can only consume
limited resources (e.g., CPU cycles, memory, etc.). The attacker
can control one or more containers by using the provided
service normally and legitimately. The malicious container
then attempts to cause system-wide impacts by consuming
more resources than allocated.
Formulation. The aforementioned research problem is formu-
lated as below. Let H represent a physical machine which
hosts n container instances C = c1, c2, . . . , cn, running with
different containerization platform combinations (denoted as
P = p1, p2, . . . , ps). Once deployed, remote users can com-
municate with the deployed application by constantly feeding
inputs and imposing one of the workloads W = w1, w2, . . . , wt.
To prevent inter-container exploitation, the container manager
enforces the following holistic requirement:
∀ci ∈ C,∀wj ∈ W,∀pk ∈ P : Ri,j,k ≤ Alloc(ci, wj, pk) (1)
where function Alloc denotes the amount of computing
resources (e.g., CPU, Memory, I/O bandwidth) provisioned
for a container instance ci, and Ri,j,k denotes the total
amount of resources consumed by that container. In general,
this requirement speciﬁes that a container should not be
capable of consuming more resources than allocated by the
containerization platforms.
The above formulation indicates the supposed resource
consumption by each container. However, it is challenging
to monitor out-of-band workloads for each container, as many
processes are shared among all containers. Thus, we can make
a generalization about the total resource utilization of the
host (RH) for an arbitrary set of containerized workloads.
Particularly, the total resource consumed by the host should be
less than the summation of allocated resources of all containers.
RH ≤ (cid:2)
Alloc(ci, wi, pk)
(2)
Although satisfying the above requirement guarantees a fair
resource provision, this requirement is still too strict for most
real-world cases. Each OS has some amount of unavoidable
overhead associated with creating and executing a containerized
workload. Therefore, we use  as a small drifting and reﬁne
Constraint 2 such that not only containerization platforms with
strict enforcement are deemed safe, but also with small changes
 are safe:
RH +  ≤ (cid:2)
Alloc(ci, wj, pk)
(3)
where  can be conﬁgured by the users.
B. Resource-Guided Fuzz Testing
Feedback-driven fuzz testing has been widely used to
automatically generate tests to detect software faults [80]. The
strength of feedback-driven fuzz testing lies in its capability
to beneﬁt from the “genetic algorithm” to gradually identify
and retain inputs that can maximize the fuzzing objective. We
leverage the fuzzing scheme to analyze container instances
running on the same host. Overall, while applications within
different containers usually have different functionalities, the
container instances themselves, once conﬁgured and launched,
should always be conﬁned by the speciﬁed resource provision.
In that sense, if one or more containers exhibit observable
violations, then it means that workloads exposed over containers
provoke vulnerabilities of the tested, which should be remedied
by developers.
We aim to record the violations w.r.t. the testing oracle
measurements observed over container instance executions. For
each iteration of testing, it mutates the test inputs to guide
the fuzzer in ﬁnding inputs that maximize predeﬁned feedback
(in this case it is the resource allocation driftings). In general,
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:18:21 UTC from IEEE Xplore.  Restrictions apply. 
404
(cid:7)(cid:25)(cid:27)(cid:31)(cid:30)(cid:1)(cid:24)(cid:31)(cid:30)(cid:14)(cid:30)(cid:21)(cid:26)(cid:25)
(cid:7)(cid:25)(cid:27)(cid:31)(cid:30)(cid:29)
(cid:1)(cid:4)(cid:5)(cid:8)(cid:7)(cid:6)
(cid:2)(cid:8)(cid:3)(cid:8)(cid:3)
(cid:8)(cid:18)(cid:18)(cid:27)(cid:1)(cid:21)(cid:25)(cid:27)(cid:31)(cid:30)(cid:29)(cid:1)(cid:30)(cid:20)(cid:14)(cid:30)(cid:1)
(cid:16)(cid:14)(cid:31)(cid:29)(cid:18)(cid:1)(cid:25)(cid:18)(cid:33)(cid:1)(cid:27)(cid:14)(cid:30)(cid:30)(cid:18)(cid:28)(cid:25)(cid:29)
(cid:6)(cid:18)(cid:18)(cid:17)(cid:15)(cid:14)(cid:16)(cid:22)
(cid:4)(cid:26)(cid:25)(cid:30)(cid:14)(cid:21)(cid:25)(cid:18)(cid:28)(cid:1)(cid:7)(cid:25)(cid:29)(cid:30)(cid:14)(cid:25)(cid:16)(cid:18)
(cid:10)(cid:28)(cid:26)(cid:16)(cid:18)(cid:29)(cid:29)
(cid:12)(cid:34)(cid:29)(cid:30)(cid:18)(cid:24)(cid:1)(cid:11)(cid:18)(cid:29)(cid:26)(cid:31)(cid:28)(cid:16)(cid:18)
(cid:12)(cid:34)(cid:29)(cid:30)(cid:18)(cid:24)(cid:1)(cid:4)(cid:14)(cid:23)(cid:23)(cid:29)
(cid:4)(cid:26)(cid:17)(cid:18)(cid:1)
(cid:4)(cid:26)(cid:32)(cid:18)(cid:28)(cid:14)(cid:19)(cid:18)
(cid:11)(cid:18)(cid:29)(cid:26)(cid:31)(cid:28)(cid:16)(cid:18)(cid:1)
(cid:13)(cid:30)(cid:21)(cid:23)(cid:21)(cid:35)(cid:14)(cid:30)(cid:21)(cid:26)(cid:25)
(cid:6)(cid:3)(cid:5)(cid:4)(cid:2)(cid:1)(cid:3)
(cid:4)(cid:26)(cid:25)(cid:30)(cid:14)(cid:21)(cid:25)(cid:18)(cid:28)(cid:1)(cid:5)(cid:25)(cid:19)(cid:21)(cid:25)(cid:18)(cid:1)(cid:2)(cid:4)(cid:12)(cid:6)(cid:10)(cid:7)(cid:14)(cid:3)
(cid:11)(cid:31)(cid:25)(cid:30)(cid:21)(cid:24)(cid:18)(cid:1)(cid:2)(cid:14)(cid:16)(cid:11)(cid:3)(cid:2)(cid:1)(cid:6)(cid:14)(cid:16)(cid:11)(cid:2)(cid:1)(cid:8)(cid:5)(cid:9)(cid:15)(cid:12)(cid:14)(cid:3)
(cid:9)(cid:12)(cid:1)(cid:8)(cid:18)(cid:28)(cid:25)(cid:18)(cid:23)(cid:1)(cid:2)(cid:6)(cid:8)(cid:14)(cid:12)(cid:16)(cid:13)(cid:15)(cid:3)
Fig. 1: Overview of the proposed approach.
feedback-driven approaches form a search campaign inspired
by evolutionary biology, which aims to gradually converge test
cases with high chances of success. Hence, the input creation
and mutation would consider the collected feedback. It deﬁnes
the requirement in this research as follows:
δ = RH − (cid:2)
max
wm
Alloc(ci, wm, pk)
(4)
where δ denotes the difference of allocated resources between
the allocated and the real consumed. A large δ indicates a
higher chance of conducting cgroup escape exploitations. As
shown in Equation 4, we compute the resource allocation
differences for each mutated input: inputs will be kept in a
queue for further mutation and usage in case it leads to new
(interesting) differences in the tested container instances.
IV. TORPEDO DESIGN
The goal of TORPEDO is to develop an unsupervised
coverage-guided fuzzer supporting multiple containers to be
tested on different container runtimes in parallel. Figure 1
depicts a bird’s eye view. It is similar to SYZKALLER, which
fuzzes pools of virtual machines, but is actually signiﬁcantly
different from it. Instead of spawning VM, TORPEDO creates
containers with arbitrary resource restrictions and runtimes
(e.g., runC, crun, gVisor) directly, thus reducing the amount of
resource overhead incurred by additional isolation mechanisms
in VMs.
For the general architecture, a manager binary serves as
an entrypoint for the fuzzer and a central collection point for
the program corpus and execution statistics. Each manager
spawns a number of fuzzer processes and communicates
with the fuzzers over gRPC. The fuzzer binary then runs
inside a container, and is responsible for generating and
manipulating programs through various lifetime stages. It
repeatedly mutates programs to determine variants that generate
new coverage. The executor then executes a serialized program
while collecting coverage information about each call. It
implements a translation layer to forward commands directly
Algorithm 1 Fuzz testing. Report all discrepant workloads
across container instances C starting from a corpus I. R denotes
the resource (e.g., CPU cycles) for testing.
1: function TORPEDO(I, C, R)
(cid:4) discrepant workload set
∗)
S ← ∅
O ← CONFIGCONTAINER(C)
for 1 ... MAX ITER do
i ← POPQUEUE(I)
∗ ← MUTATE(i)
i
W ← GENWORKLOAD(i
R ← ∅
for coni, wi ∈ (C,W) do
r ← RUN(coni, wi)
R ← R (cid:3){r}
I ← I (cid:3){i
∗}
for oracle ∈ O do
S ← S (cid:3){i
if NEWPATTERN(R) then
patterns
oracle
return S
2:
3:
4:
5: