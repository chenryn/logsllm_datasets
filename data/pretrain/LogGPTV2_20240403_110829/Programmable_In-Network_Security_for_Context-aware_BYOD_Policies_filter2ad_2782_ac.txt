which requires matching a context ﬁeld against a list of keys
bit by bit. It could be a range match, which compares a con-
text ﬁeld against a range of values in TCAM (Ternary Content
Addressable Memory). By default, Poise uses 4-byte headers
for exact matches, and 2-byte headers for range matches. Con-
text matches can also be performed against a user-speciﬁed
constant list that deﬁnes membership, e.g., a set of devices
owned by the sales department. For a list with k items [a1,
a2,· · · , ak], our compiler will construct a match/action table
with k entries, where each entry corresponds to an item in
the list. The actions associated with the entries depend on the
mode of access deﬁned in the policy program.
For instance, consider the P4 snippet in Figure 4(c), which
shows a match/action table generated from a constant list of
two entries: Bob and Alice. The table implements an exact
match on the device ID ﬁeld. If the context match is suc-
cessful, then the device will be granted access; unsuccessful
matches indicate that the context fails the membership test,
and these requests will be denied access.
Compiling stateful monitors. The Poise compiler generates
a read/write register for each stateful monitor in the policy, as
well as code components for detecting monitored events and
updating the monitor values. Such monitors are implemented
as a number of registers in P4, which are supported in switch
SRAM. Updates to the registers are linespeed, so they can
be performed on a per-packet basis. Speciﬁcally, for each
incoming packet, the generated code checks whether this cor-
responds to an event of interest, using either a context match,
or a match over a membership list. If this event should be
monitored, the code additionally updates the monitor register
and records the event timestamp. If a long time has elapsed
after the previous event took place, then this register is cleared
to indicate that the monitored event is absent. As discussed
before, monitors enable network-wide policies that make co-
ordinated security decisions—a policy can test if a monitored
event is detected, and make decisions accordingly.
Concretely, the snippet in Figure 4(d) shows an example.
It instantiates a 32-bit register to hold the monitor value, and
updates the register when the admin context is active in a
packet. The code associates a timestamp to this monitor, and
resets the monitor upon timeout.
Compiling actions. An action will be taken on each packet
to represent the ﬁnal decision made on its context. In P4, de-
cisions are represented by attaching special metadata ﬁelds to
a packet, which will be recognized and processed by a trafﬁc
manager, which schedules packets to be sent on the correct
outgoing port(s) or dropped. Logging a packet is achieved by
setting the outgoing port to be the switch CPU.
Compiler optimizations. Programmable data planes have
three types of notable constraints. Stages: There is a ﬁxed
number of hardware stages, and a packet can only match
against one single context table per stage. Tables: A single
stage can only hold a ﬁxed number of tables. Memory: Each
stage has a limited amount of memory.
The Poise compiler performs two types of optimizations,
which are particularly useful when Poise needs to compose
many policies together. (a) If multiple policies check against
the same context signal, our compiler will perform table dedu-
600    29th USENIX Security Symposium
USENIX Association
plication to eliminate redundant context tables and save mem-
ory. (b) If a policy performs more context checks than the
number of available stages, Poise will collapse the policy by
recirculating context packets to traverse the stages multiple
times, triggering different tables at each recirculation. This
addresses the switch constraint that a packet can only trigger
a single table per stage. Our optimization creates the illusion
of a larger number of stages with the cost of slightly increased
latency for recirculated packets. We refer interested readers
to Appendix A.2 for more details.
Summary. So far, we have described the basic compilation
algorithm as if each packet is tagged with context information.
This makes it easy for a switch to access a packet’s context
without keeping state, but it results in high trafﬁc overhead.
Next, we will relax this assumption by the design of a stateful,
efﬁcient, programmable in-network security primitive.
5 The In-Network Security Primitive
Poise has a security primitive that runs in a programmable
switch, which is dynamic, efﬁcient, and programmable.
Goal: A dynamic and efﬁcient security primitive. The in-
network primitive should ideally allow the level of protection
to be adjusted between per-packet and per-ﬂow granularities,
by supporting a tunable frequency of context packets for each
connection. At one end of the spectrum, per-ﬂow granularity
of protection degenerates into a static security mechanism that
does not support context changes within a connection. Thus
the protection is very coarse-grained, especially for long-lived
connections that persist for an extended period of time (e.g.,
push-based mobile services, such as email [93]). At the other
end, per-packet granularity is extremely ﬁne-grained, but it
may incur unnecessary resource waste unless context changes
from packet to packet. As a concrete example, if there are
20 context ﬁelds across policies, then each client needs to
send 20 × 4/500 = 16% extra trafﬁc, assuming typical 500-
byte packets and 4-byte context ﬁelds. The Poise primitive
supports a property that we call subﬂow-level security, which
achieves a tunable tradeoff between security granularity and
overhead when enforcing context-aware security.
Property: Subﬂow-level security. We state this property
more formally below. Consider a sequence of packets in the
same ﬂow ci, pi1, · · · , pik , ci+1, where c represents a context
packet and p a data packet. Subﬂow-level security requires
that decisions made on the context packet ci should be applied
to subsequent data packets pi j , i j ∈ [i1, ik], but fresh decisions
should be made for data packets that follow ci+1. The deci-
sion granularity can be tuned by f , the frequency of context
packets. This results in an overhead of s · f , where s is the size
of context packets. For instance, assuming 80-byte context
packets and a frequency of one context packet per ten seconds,
the overhead would be as low as 8 bytes per second.
Challenges. Designing a primitive that supports subﬂow-
level security, however, requires tackling three key challenges.
FullConn
Decision
Key (3-tuple)
Val
Idx
Decision
10.0.0.2:22:TCP
10.0.0.9:80:UDP
10.0.0.7:ff:TCP
10.0.0.6:80:UDP
1
2
0
3
0
1
2
3
1	(Allow)
0	(Drop)
1	(Allow)
2	(DPI)
Cache
M/A	tables
Hash
3-tuple	
Decision	
0xFE32
10.0.0.1:80:TCP
0	(Drop)
Registers
0x88EA
10.0.0.2:22:TCP
1	(Allow)
0xBC42
10.0.0.7:52:UDP 1	(Allow)
0x4A52
10.0.0.9:A7:UDP 2	(DPI)
Figure 5: The key/value store with example entries.
(a) Keeping per-ﬂow state requires a prohibitive amount of
memory, but modern switches only have O(10MB) SRAM.
Poise addresses this by approximating per-ﬂow state using a
on-chip key/value store. (b) Buffering control plane updates is
necessary for handling new ﬂows. Although context changes
can be entirely handled by the data plane, new ﬂows require
installing match/action entries from the switch CPU, which
takes time. Before updates are fully populated, Poise uses
another hardware data structure akin to a cache to make con-
servative decisions for buffered ﬂows. (c) Mitigating DoS
attacks that could arise due to the interaction between data
and control planes. This defends against malicious clients that
craft special context packets to degrade the performance of
selected clients, or even the entire network. In the next three
subsections, we detail each of these techniques.
5.1 Approximating per-ﬂow state
The key problem in the ﬁrst challenge stems from the fact
that the switch needs to process data packets without contexts
attached to them. Therefore, when a switch processes a con-
text packet, it needs to remember the decision and apply it to
subsequent data packets in the same connection, until the next
context packet refreshes the decision. A naïve design would
require keeping per-ﬂow state on the switch, which leads to
high memory overhead.
To address this, Poise approximates per-ﬂow state using a
key/value store consisting of two data structures, FullConn
and Decision, as shown in Figure 5. The FullConn schema
is [sip, sport, proto]→idx. The match key is the
source IP/port and protocol for the client, and the value is an
index to a register array R. The indexed register R[idx] holds
the decision made on the latest context packet within this
connection, and it can be refreshed entirely in the data plane.
Insertions to this key/value store require control plane involve-
ment, but they are relatively infrequent and only needed for
new connections. Since the match key does not include the
destination IP/port, this introduces some inaccuracy when
a client reuses a source port across connections. Therefore,
for short-lived connections, data packets may see slightly
outdated decisions. To ensure that such inaccuracy does not
USENIX Association
29th USENIX Security Symposium    601
Match-action tables
Stateful registers
Switch Control Plane
Hit: update decision
Insert new connection
Context 
packet in Make BYOD
decision
FullConn
Decision
Miss
Update 
Cache
Cache
BF
Evicted deny entry 
(a) The logic for processing context packets
Data packet in
FullConn
Decision
Hit
Miss
Cache
Miss
Hit
Drop
Enforce BYOD
decision
Collision 
Miss: recirculate
BF
Hit
Drop
(b) The logic for processing data packets
Figure 6: Poise uses a combination of match/action tables and
stateful registers to process context and data packets.
misclassify a “deny” as an “allow”, we blacklist the source IP
addresses that have recently violated the enterprise policy: all
connections from these clients would be blocked temporarily.
5.2 Buffering control plane updates
Insertions to FullConn requires control plane involvement, so
they take much longer than updating policy decisions for an
existing connection. As a result, when data packets in a new
connection arrive at the switch, the FullConn match/action
table may not have been populated with the corresponding
entry yet. To address this, Poise uses a level of indirection
by creating a small hardware Cache to buffer decisions for
pending table updates, which resides on the data plane and
can be updated at linespeed. All decisions in Cache are up-to-
date, since writes to this cache are immediately effective; but
this table has a smaller capacity. The FullConn table takes
more time to update, but it holds more connections.
The cache design. As shown in Figure 5, Cache has a
ﬁxed number of entries. Our implementation uses 216 entries,
which corresponds to the output size of a CRC-16 hash func-
tion. Each entry is of the form h→[sip,sport,proto,dec],
where h is the CRC hash of the ﬂow’s three tuple, i.e.,
h=CRC(sip,sport,proto), and dec is the decision made
based on the context packet. The size of Cache is 216 × (7 +
1)=0.38 MB memory. When Poise receives a context packet
from a new connection (Figure 6a), it immediately adds the
entry to Cache, and then invokes the control plane API to
insert the match/action entry in FullConn. Since CRC func-
tions are not collision resistant, different connections may be
mapped to the same entry; hence, we evict old entries upon
collision. When a data packet comes in (Figure 6b), Poise
ﬁrst matches it against the FullConn table and applies the
decision upon success. If there is no entry for this packet,
then Poise indexes the Cache table instead. Upon a cache
hit, the corresponding decision is applied to the data packet.
Upon a cache miss, one of two situations has happened: a)
the switch has not seen a context packet from this client, or
b) the entry for this client has been evicted due to collision.
Poise distinguishes between these cases using the following
cache eviction algorithm.
Handling cache evictions. Upon collision, we always re-
place the existing entry with the new one. This is because
Poise has already invoked the control plane to install the cor-
responding entry in FullConn, which will complete in time.
Therefore, if a packet does not match any entry in FullConn
and experiences a collision in Cache, we use a special instruc-
tion to recirculate the packet inside the data plane to delay its
processing. Recirculated packets are sent back to the switch
ingress to be matched against the FullConn table one more
time. This recirculation is repeated up to k times, where the
latency is chosen to be larger than the expected time for the
control plane to populate an entry. If a packet has reached
this threshold, and the FullConn table still has not been pop-
ulated, then we consider this to be case a) above and drop the
packet.
Early denies. To reduce the amount of recirculated packets,
we make early decisions to drop a packet if its context is
evaluated to a “deny”. Speciﬁcally, when evicting an entry
from Cache, we add its source IP address into a blacklist
Bloom ﬁlter (BF in Figure 6) if the decision is to drop. Source
addresses in BF represent devices that have violated the policy
recently and need to be blacklisted for a period of time. If a
packet cannot ﬁnd an entry in either Cache or FullConn, but
hits BF, we drop it without recirculation. Since Bloom ﬁlters
can only produce false positives, but never false negatives, we
will always correctly reject an illegal connection. However, we
might err on the conservative side and reject legal connections
as well, if the BF produces a false positive. This is a rare
case, however, as this will only happen during the window in
which FullConn has not been populated, the Cache entry has
been evicted, and the BF happens to produce a false positive.
Nevertheless, Poise periodically clears this Bloom ﬁlter to
reduce false positive rates, which grow with the number of
contained elements. When the BF is being cleared, packets
will be recirculated until the operation completes.
5.3 Handling denial-of-service attacks
Since Poise requires extra processing inside the network, we
need to ensure that it does not introduce new attack vectors.
Speciﬁcally, we have identiﬁed two potential denial-of-service
attack vectors and hardened the primitive against them.
Total residency attacks. Different from stateless, IP-based
routing, Poise keeps state in the FullConn table. Therefore,
an attacker could initiate many new connections and try to
a) overwhelm the FullConn table and b) constantly involve
the switch CPU to install new entries. A defense, for instance,
602    29th USENIX Security Symposium