they generally target speciﬁc architectures, and are designed
to reﬂect the eﬀects and capacities of speciﬁc perturbation
techniques.
For the purpose of this paper, it is suﬃcient to know that
there exist two broad classes of fault models. The ﬁrst class
captures faults that modify the dataﬂow, for instance by set-
ting a particular register to a default value (the null fault
model) or to a constant but unknown value (the constant
fault model), or by setting part of the register to a constant
value (the zero high-order bits fault model and its variants).
In practice, it is often important to consider models that
combine several kinds of faults; for instance, one can con-
sider a fault model which allows null faults on small registers,
1022and constant faults on larger registers. Such faults are con-
sidered for example in [20], where the authors also justify
their practical feasibility. The second class captures faults
that modify the control ﬂow, for instance by skipping an
instruction (the instruction skip model), by forcing a condi-
tional instruction to enter into a speciﬁc branch (the branch
fault model), or by forcing the execution of a loop to be in-
terrupted before the guard is set to false, or continued after
it is set to false (the loop fault model). These models are
classic and are considered in [39], for instance. Both mod-
els overlap, in the sense that one can sometimes achieve the
same eﬀect by a dataﬂow fault attack, or by a control ﬂow
fault attack.
Fault policies. Instead of hardcoding the diﬀerent fault
models, our tool allows users to specify ﬁne-grained fault
policies that delineate very precisely the space of faulted
implementations by describing which faults can be injected
in the program. Fault policies are program speciﬁc, and are
given by two sets of replacement clauses.
The ﬁrst set consists of variable replacement clauses of the
form (x, e) where x is a program variable and e is an expres-
sion; such a clause says that one can replace the variable
x by the expression e in the course of program execution.
These declarations can be used to model data faults; for in-
stance, the null fault on x is captured by the clause (x, 0),
whereas the zero high order bits fault on x that sets r bits
to zero is captured by the clause (x, msbr(x)).
The second set consists of command replacement clauses
of the form (c, c(cid:48)), where c and c(cid:48) are commands; such a
clause says that one can replace the command c by the com-
mand c(cid:48) in the course of program execution. These declara-
tions can be used to model control ﬂow faults; for instance,
instruction skip faults on an assignment c is captured by
the clause (c, skip), whereas branch faults are captured by
the clause (if b then c1 else c2, ci) where i = 1 if the goal
is to force execution to go into the true branch, and i = 2,
otherwise. By convention, we require that all instruction re-
placements do not increase the set of modiﬁed variables, i.e.
the set of modiﬁed variables of a command c(cid:48) is a subset of
the set of modiﬁed variables of the command c it replaces.
This is the case for all control ﬂow attacks described above,
and is essential for the completeness of our tool.
Although it is useful in practice, fault policies do not cur-
rently include a mechanism to impose any locality constraint
on the clauses, i.e.
replacements may occur anywhere in
the program. This can easily be circumvented by writing
programs in pseudo-SSA form, for instance by adding sub-
scripts for the diﬀerent occurences of the same variable in
the program.
Finally, fault policies may also include some upper bounds
on the number of times a clause can be used to fault an
implementation. This is useful to constrain the space of
faulted implementations and to match physical constraints.
Discussion. There is a direct relation between fault models
and fault policies, in the sense that every fault model deter-
mines a unique fault policy for each program. However,
many fault attacks require multiple faults and can only be
captured by hybrid fault models, that combine several sim-
pler ones. An example of hybrid fault model is one that
considers null faults on variables that denote small registers
(for instance, variables that store values smaller than 28),
and constant faults on variables representing larger regis-
ters.
It would be interesting to develop a high-level language
for describing hybrid fault models, and a compiler for gen-
erating automatically fault policies from high-level speciﬁca-
tions. However, building the compiler requires a signiﬁcant
amount of infrastructure, including the ability to automat-
ically infer program invariants:
for the example discussed
above, the compiler would need to infer that the value held
by a variable x is always smaller than 28 in order to gener-
ate the clause (x, 0). We leave the design of this high-level
language and the implementation of the compiler for future
work, and require for now that fault policies (albeit in some
edulcorated form) are given as input to the tool.
4.3 Algorithm
Our tool takes as input a (non-faulted) implementation
written in the programming language of EasyCrypt, a fault
condition, a fault policy, and optionally a precondition ψ.
It outputs a set of faulted implementations that satisfy the
fault condition and are valid faults of the original implemen-
tation with respect to the fault model considered. The core
of the tool is an algorithm that interleaves the computation
of weakest preconditions, logical simpliﬁcations, and genera-
tion of faults. For simplicity, we describe a non-deterministic
and ineﬃcient version of the algorithm, whereas the imple-
mentation uses a more eﬃcient implementation, and some
caching and early pruning techniques for the smart explo-
ration of the search space. We initially explain how the
algorithm works on straightline programs, i.e. programs
without loops, conditionals, and procedure calls. Then, we
explain how to extend the algorithm to procedure calls and
loops. First, we deﬁne the notion of faulted instruction.
Faulted commands. The fault policy determines for each
command c of the program a set of faulted instances, consist-
ing of commands c(cid:48) that can be obtained from c according
to the fault policy. All commands are faulted instances of
themselves, and moreover the command c(cid:48) is a faulted in-
stance of c if there exists an instruction replacement clause
(c, c(cid:48)). Moreover, there are some speciﬁc rules for each con-
struct of the language.
• x ← e[e1, . . . , en/y1, . . . , yn] is a faulted instance of
x ← e, provided for i = 1 . . . n, the replacements of yi
by ei are allowed by the fault policy.
• the commands while b do c(cid:48), and if b then c(cid:48); while b do c,
and while b(cid:48) do c; if b then c(cid:48) are all faulted instances of
while b do c, where c(cid:48) is a faulted instance of c, and b(cid:48)
is a guard that forces exactly one less iteration of the
loop body.
The last clause captures faults on the ﬁrst and last iteration
of a loop, and can be extended to model faults on the ﬁrst
and last k iterations of a loop, for k ≥ 1.
Straightline programs. The algorithm is given as input
a fault policy, and manipulates triples of the form (c, φ,(cid:98)c).
a new triple (c(cid:48), φ(cid:48),(cid:98)c(cid:48)) as follows;
Initially, the algorithm is given the triple (c, φ, skip) consist-
ing of the program being analyzed against fault attacks, the
fault condition, and the empty statement. At each iteration,
the algorithm consumes the last command of c and outputs
1. c is decomposed into a sequence c(cid:48); i, where i is the last
command of the program (necessarily an assignment or
10232. the algorithm checks whether i aﬀects φ, i.e.
a random sampling). If c is empty, then the algorithm
checks if φ is a consequence of the precondition, and
returns(cid:98)c if this is the case and nothing otherwise;
algorithm breaks to the next iteration with (c(cid:48), φ,(cid:98)c(cid:48)),
where (cid:98)c(cid:48) = i;(cid:98)c;
if any
of the variables modiﬁed by i occur in φ. If not, the
3. if some variable modiﬁed by i occurs in φ, then the
algorithm chooses non-deterministically a faulted in-
stance i(cid:48) of i;
4. the algorithm computes the weakest precondition of
i(cid:48) on φ. For instance, the rules for computing weak-
est preconditions of deterministic and random assign-
ments are:
WP(x ← e, φ) = φ{x ← e}
WP(x $← d, φ) = ∀v ∈ dom(d), φ{x ← v},
where dom(d) is the set of values that have a non-
zero probability in d. Note that the weakest precon-
dition computation takes an assertion and returns an
assertion. This is achieved by viewing probabilistic as-
signments as non-deterministic assignments over the
domain of the distribution from which the assignment
is sampled;
5. the algorithm applies logical simpliﬁcations to the as-
sertion φ output by the weakest precondition compu-
tation. The output is a new assertion φ(cid:48) that has fewer
free variables than φ;
6. the algorithm proceeds to the next iteration with state
(c, φ(cid:48),(cid:98)c(cid:48)), where (cid:98)c(cid:48) = i(cid:48);(cid:98)c.
Breaking to the next iteration in step 2 and performing log-
ical simpliﬁcations in Step 5 may in fact signiﬁcantly prune
the search space, without ruling out any potential attacks:
computing the weakest precondition on a command i whose
left-hand-side does not appear in the fault condition never
changes that fault condition, whichever fault may be se-
lected. Indeed, our algorithm is sound and relatively com-
plete for straightline code, in the sense that, given an oracle
that can decide logical implications, the algorithm would re-
turn all faulted versions c(cid:48) of c such that the Hoare triple
{ψ}c(cid:48){φ} is valid. In practice, logical implications are veri-
ﬁed using SMT solvers, and hence the implementation might
actually fail to ﬁnd a valid fault attack.
Procedure calls. Our tool deals with programs that make
non-recursive procedure calls by entering into the code of the
procedure when reaching a call. This is intuitively equiva-
lent to inlining all procedure calls and applying the non-
procedural analysis to the inlined code. Although it is cer-
tainly possible to develop more sophisticated approaches, in-
cluding ones that deal with recursive procedure calls, based
on state-of-the-art techniques, our elementary approach has
the advantage of simplicity and is suﬃcient for most imple-
mentations of cryptographic algorithms.
Loops. Dealing with loops is the main source of complex-
ity for our tool, as computing weakest preconditions re-
quires knowing some useful loop invariants, i.e. assertions
that hold throughout all iterations of the loop body. We
provide two elementary mechanisms for dealing with loops:
an invariant generator, and an algorithm for turning (user-
provided) invariants for non-faulted loops into invariants for
their faulted instances. There is admittedly signiﬁcant scope
for improving these mechanisms, in particular by building
upon recent developments in invariant generation; we leave
this avenue for future work.
Pruning. We use two main pruning techniques for improv-
ing the eﬃciency of the search algorithm. First, since SMT
solvers are a clear performance bottleneck, we cache all SMT
queries and their result. Second, we maintain a table of all
intermediate statements (c, φ,(cid:98)c), and abort execution when-
ever the algorithm computes a triple which coincides in the
ﬁrst and second component with an element of the table.
5. APPLICATIONS
Using our tool, we are able to discover many attacks on
implementations of RSA-CRT and ECDSA signatures. Sev-
eral of these attacks are new, and of independent interest.
In this section, we review in some detail the most relevant
attacks we ﬁnd.
5.1 RSA-CRT signatures
We consider a CRT-based implementation of RSA that
uses the Montgomery ladder (Figure 2) for modular expo-
nentiation and the CIOS algorithm (Figure 1) for modu-
lar multiplication. We consider implementations using both
Garner’s recombination algorithm (Figure 4) and the stan-
dard CRT recombination with optimizations (Figure 3). Most
of the attacks we ﬁnd involve faults injected during the last
call to CIOS in the ladder (line 13, Figure 2), which takes
the result of the exponentiation back into its classical rep-
resentation. We assume that the parameter x in CIOS is
stored in a shift-register, used to extract its individual dig-
its in base b.
Finding multiples of p or q. Using the fault condition from
Proposition 1, and allowing null faults on small variables
(that contain integers mod b) we recover the most basic
and eﬃcient attack of [19], which sets q(cid:48) to 0 during the
ﬁnal call to CIOS( ¯Sq, 1).
In addition, the tool also ﬁnds
several variants of the fault, indicating which (combinations
of) variables can be set to 0 to fulﬁll the fault condition.
For example, setting both uj and xj to 0 throughout the
computation still yields a null result.
This attack and its variants only work when the ﬁnal call
to CIOS occurs with 1 as second argument. This is not
always the case when CRT recombination is used, since the
call to CIOS can be used to optimize a multiplication away as
illustrated in Figure 3. In this case, by adding control ﬂow
faults to the fault policy, our tool also ﬁnds that faulting q(cid:48)
to 0 and doubling the number of loop iterations during this
ﬁnal call forces its result to zero. Indeed, in this case, after
the normal number of iterations, the shift-register initially
containing x contains zero and any further loop iteration
simply shifts a to the right, eventually forcing it to zero as
well. A much simpler, albeit much less elegant, control ﬂow
fault involves simply faulting the initial loop condition so no
computation is performed.
Finding “almost full” linear transforms of p or q. It
may not always be possible to skip the loop entirely, or to
ensure that the loop is run at least twice as many times as
expected. However, it may be easier to inject faults on loop
counters that consistently add a small (possibly unknown)
1024number of iterations. Our tool automatically ﬁnds that such
faults, when q(cid:48) is set to zero during the additional loop iter-
ations are in fact suﬃcient to guarantee the fault condition
from Proposition 3 using both Garner and CRT recombi-
nation. For each additional iteration, the size of the expo-
nentiation’s result is reduced by the size of a base b digit,
quickly leading to a result that can be exploited by the clas-
sic lattice-based attacks described in Section 3.1.
Alternatively, instead of faulting the control ﬂow and a
variable, our tool also ﬁnds that simply setting q(cid:48) and xj to
zero during the last iterations of the loop leads to a similarly
faulted signature, that fulﬁlls the desired fault condition.
Finding “almost full” linear combinations of p and q.
When given the fault condition from Proposition 2, our tool
ﬁnds that running the previous size-reducing attacks on both
half-exponentiations yields a suitable faulted signature when
using the classic CRT recombination rather than Garner’s.
The relative eﬃciency of the lattice-based attack from Sec-
tion 3.1 compared to the one from Section 3.1 may justify
the additional faults.
5.2 ECDSA signatures
We also run our tool on the ECDSA signature algorithm.
We consider an implementation where scalar multiplication
is computed using MSB-ﬁrst Double-and-Add (Figure 5).
The main challenge here is that the fault conditions we con-
sider are very precise, in the sense that they give a full func-
tional description of the result depending on some (faulted)
inputs. We therefore need not only to be able to ﬁnd the
faults, but also to be able to prove the functional correctness
of the non-faulted algorithms.
Faults on the randomness. We ﬁrst consider the fault con-
dition from Proposition 5, that we generalized from [36].
The tool ﬁnds that performing a zero-higher-order bit fault
on k after it is sampled is suﬃcient to guarantee the fault
condition (as we then have k = k (cid:29) (cid:96)). However, we do not
automatically ﬁnd more complex attacks (that use proposi-
tion 5) on the algorithms computing scalar multiplications
and ﬁeld element inversions. We believe that our tool would
in fact ﬁnd such attacks given precise enough implementa-
tions for these operations, and precise enough loop invariants
for their non-faulted versions.
Faults on scalar multiplication.. Fault condition (1) from
Proposition 4 allows our algorithm to quickly focus the fault
search on the computation of the scalar multiplication in
ECDSA. The tool discovers that exiting the loop early when
computing [k] · P , and letting all other computations oc-
cur normally, yields signatures (r, s) that fulﬁll fault condi-
tion 4(1).
The second fault condition (2) from Proposition 4 leads
to a slightly more ﬂexible overall attack, since it does not