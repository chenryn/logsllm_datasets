from a list of active, valid memory addresses that is main-
tained by the system. This task is accomplished via a sliding
window technique in which 8-byte (16-hexadecimal character)
segments of payload data are compared to the set of known
active, valid memory addresses, which are already saved by the
defensive system [7]. If the 8-byte payload segment matches
a signature in the active address list then a memory disclosure
has been discovered in the payload and mitigation steps are
initiated to prevent exploitation of the leaked address.
While many techniques and strategies exist to accomplish
payload inspection, an efﬁcient means is to employ the Bloom
ﬁlter probabilistic data structure [11]. In our model, a Bloom
ﬁlter is programmed (i.e., populated) by generating a set of k
hash values in the range [1, ..., m] from each of n active system
addresses. A vector of size m is initialized to null values. Each
valid, active memory address is then hashed k times, and the
corresponding location in the bit vector [1, ..., m] is set to 1.
When the decision is taken to inspect a given system
output, an 8-byte (16-character) window moves through the
data byte-by-byte (skipping every other character) through the
system output. Each 8-byte segment of payload data is hashed
using hash functions identical to those used for programming
the Bloom ﬁlter. If a 1 is discovered in every location queried,
then the payload data segment being inspected is deemed to
be an active system address (i.e., a memory disclosure). If, on
the other hand, a queried vector entry is 0, the data segment
is deemed not to contain a memory disclosure.
Strengths of a Bloom ﬁlter implementation of the search
algorithm include a compact representation of the data set to
be queried, and query times independent of the size of the
signature set used to program the Bloom ﬁlter [12]. The source
of these strengths is the simple representation of the data made
possible by the hashing operation. Inherent in the compact
data representation at the heart of a Bloom ﬁlter, however,
also comes a non-zero probability of false positives, given
approximately by the expression [12],
PF P = (1 − e−nk/m)k.
(8)
Here PF P is the probability of obtaining a false positive
indication in a given query of the Bloom ﬁlter, n is the number
of elements used to program the Bloom ﬁlter, k is the number
of hash functions used in the Bloom ﬁlter, and m is the size
of the Bloom ﬁlter’s bit vector.
For a given n and m, the probability of obtaining a false
positive result from querying the Bloom ﬁlter is minimized
when
k =
m
n
ln2
(9)
hash functions are used in the Bloom ﬁlter [12]. We use
these expressions to parameterize our model of system output
searching.
IV. REINFORCEMENT LEARNING
The intelligent, adaptive capability of our proposed control
architecture is accomplished via an agent architecture that
incorporates machine learning techniques. These methods al-
low the defender’s control system to automatically adjust its
operations to counter attacker attempts at compromise while
considering the performance ramiﬁcations of policy choices.
Many machine learning techniques rely on the existence on
labeled examples to learn optimal mappings between inputs
and outputs. This learning approach works well
in many
applications, but encounters difﬁculty in strategic situations of
even moderate complexity where the decision space quickly
grows to the point that any attempt by a human to label
the good and bad moves by hand becomes infeasible [13].
The class of learning algorithms that attempt to circumvent
this bottleneck by enabling agents to learn optimal courses
of action, or policies, through interaction with an unknown
environment that answers an agent’s actions with rewards and
punishments are termed reinforcement learning techniques.
Reinforcement learning [14] grew out of work on trial and
error learning [15], [16] and optimal control [17] and today
comprises a set of important and rapidly developing machine
learning techniques that are used across a number of industries
and academic ﬁelds [18]. Reinforcement learning comprises
a class of machine learning algorithms that allow an agent
to learn online and incrementally, proceeding without perfect
knowledge of the environment or the reward structure.
A. Q-Learning
Q-learning [19] is a reinforcement learning technique that
operates without needing a model of the environment. The goal
of Q-learning is for the agent to discover a policy, π, giving
an optimal mapping,
π : S → A,
(10)
between states (S), and actions (A).
As the Q-learning algorithm proceeds a set of action-
value, or Q, functions are incrementally learned. The Q values
represent the utility an agent expects to achieve by taking
the speciﬁed action in a given state and following an optimal
policy afterwards. The Q values are initialized arbitrarily at
the beginning of the learning process, and thereafter updated
according to the value-iteration update equation,
ΔQ(s, a) = α[R + γ maxbQ(s(cid:2), b) − Q(s, a)].
(11)
Here a is the action taken in state s, and b is the optimal
action taken from future state s(cid:2). The learning rate, 0 ≤ α ≤ 1,
determines how quickly the decision rules are updated based
on the latest information. The discount factor, 0 ≤ γ < 1,
weights the importance of future rewards, with low values
causing the Q-learning agent to focus on obtaining immediate
payoffs, and high values of γ encouraging behavior to optimize
future reward.
In Q-learning the agent estimates reward Q(s, a) based on
the estimate of future rewards Q(s(cid:2), b). This method has the
advantage of allowing the Q-learning agent to estimate future
rewards without having a knowledge of actual future actions
and rewards/punishments [18].
In our Q-learning instantiation we use the reward function
R to address the cost/beneﬁt problem the defender must
solve in devising a maximally effective set of search control
heuristics. The reward Ri for inspecting a given system output
i for possible memory disclosures is given by,
Ri = Bi − (Li,s + Li,m)
(12)
the beneﬁt
with Bi
the defender gets from discovering a
memory disclosure in system output i, Li,s the latency cost
the defender pays for searching a system output i for memory
disclosures, and Li,m the latency cost the defender pays for
mitigating a memory disclosure in payload i once if it has been
discovered. We note that Ri is the instantaneous feedback the
defender receives due to the chosen action on system output
i, while Q is the expected discounted reward the defender
forecasts for a given course of action in the current round
given that an optimal policy is followed afterwards.
We note that the beneﬁt term Bi can be interpreted in
two ways. In the ﬁrst, Bi represents an empirically derived
610
measure of inverse risk to the defended system of the attacker
obtaining a memory disclosure from the system. The second
interpretation of Bi
is as a tuning parameter the system
operator can use to tune the control architecture to a security
focused or a performance focused posture. We adopt this later
view of B throughout the present study.
In the implementation of Q-learning in this study the de-
fender has access to the past 3 decisions that have been made,
and whether or not the decision to inspect a system output
(if such a decision has occurred in the previous 3 rounds of
play) has resulted in a memory disclosure detection. Ancillary
studies revealed a 3-step memory horizon to be sufﬁciently
large to allow for fairly complex strategy formulation while
still remaining tractable for implementation. It is an item of
future work to extend this constrained memory horizon using
more scalable encoding schemes (e.g., neural networks). If the
defender has chosen not to inspect the system output in a given
round, the defender cannot know whether or not the passed
over system output contained a memory disclosure, though this
information will be reﬂected in the security score, of which the
defender is not aware. The 3 step memory horizon comprises
the state space (S) for our Q-learning defender agent. A single
state in this space is given by,
1 = {(an−1, rn−1), (an−2, rn−2), (an−3, rn−3)} ,
sn
(13)
with a the action on the n − x (x ∈ 1, 2, 3) step, and r the
result (i.e., discovery or no-discovery of a memory disclosure
in the system output’s payload) on the same step.
The set of all possible actions and consequent results makes
up the state space S,
S = {s1, s2, ...s27}
(14)
We note that the restriction that the Q-learning agent know only
of the presence or absence of memory disclosures in payloads
which the agent chooses to inspect reduces the state space from
64 to 27 states.
The action space for the defender (AD) consists of the
decision to inspect, and the decision to pass on a system output.
Alternatively, an agent might choose to explore the environ-
ment, ignoring what it has learned of its environment in the
past and choosing an action randomly from those available to
it, observing the reward or punishment that results and adjust-
ing its decision rules accordingly. Some adaptation techniques
implicitly account for this tradeoff, for example the mutation
rate in a genetic algorithm [20] drives the modeled population
to greater exploration when set to a high value. The Q-learning
algorithm makes the exploration/exploitation tradeoff explicit
for each individual learning agent by augmenting the basic Q-
update cycle with a scheme for allowing the agent to make
exploration moves at some rate during its interaction with the
environment.
In this study we address the action-selection dilemma by
employing the -greedy scheme [14] in which agent’s perform
random, non-optimal exploration actions with a probability 
(0 ≤  ≤ 1), and act greedily with probability (1 − ). Unless
otherwise indicated, at the start of a model run,  is set to
unity, and is then decreased uniformly on each subsequent
step such that  reaches zero on the ﬁnal simulation step,
thus maintaining at least a small probability of exploration
throughout the model runs. By initializing  to unity and then
decreasing its value as the simulation proceeds we cause the
agent to explore more often early on when the optimal policy
is still unknown, while still acting according to the optimal
policy with high probability later in the model run when the
optimal policy is more certain.
V. SYSTEM ARCHITECTURE FOR BALANCING SECURITY
AND PERFORMANCE
The system components described above are composed into
the overall architecture for balancing security and performance
depicted in Fig. 1. This architecture consists of a decision mod-
ule (represented by the blue diamond in Fig. 1) that instantiates
a set of inspection-decision heuristics for inspecting system
outputs according to the estimated probability that a system
output contains a memory disclosure given the recent history
of disclosures. System outputs stream through the decision
module to the inspection module, which either inspects the
system output or lets it pass uninspected depending on the
recommendation of the decision module. If the search is
executed and a memory disclosure is discovered, the mitigation
step is activated (indicated by the green box in Fig. 1).
AD = {Inspect, Pass}
(15)
Positive Reinforcement 
The policy (π) is the set of learned heuristics H over the
3-step memory horizon,
System 
Output 
Inspect 
? 
Yes 
Search 
Leak Found 
Re-
Randomize 
π = Hn ((an−1, rn−1), (an−2, rn−2), (an−3, rn−3)) .
(16)
An adaptive agent in unknown or changing environments
faces the exploration-exploitation dilemma [14]. An agent
might choose to greedily exploit their understanding of the
environment and take the action as expected to maximize
future reward,
Q(as) = maxaQ(a).
(17)
Leak Not Found 
No 
Negative Reinforcement 
Memory  
Disclosure  
Detector 
Fig. 1. Memory disclosure mitigation architecture for automatically discov-
ering defensive policies that balance system security and system performance.
This architecture is general, and can be applied to problems
across a broad spectrum of application areas where security
must be balanced against system performance. The particular
details of a given system enter the architecture in the form of
611
the latency costs, Li,s and Li,m, that paraemterize the reward
function R that determines the feedback (negative or positive)
to the inspection module and causes adaptation of the learned
heuristics to the latest observed attacker behavior.
The human operator exerts a measure of control over the
functioning of the architecture by selecting a value for the
beneﬁt term Bi in the reward function Ri. The value of Bi
relative to the latency terms (Li,s and Li,m) determines the
characteristic behavior, what we term bias, of the system. For
convenience, we deﬁne the normalized beneﬁt, BN , to be,
BN =
B
Ls + Lm
,
(18)
the ratio of the user determined beneﬁt term and the system-
determined latency terms.
VI. SIMULATION EXPERIMENTS
A. Model Parameterization
In order to quantitatively assess our framework for au-
tomatically balancing system security and performance we
develop a scenario involving the speciﬁc cyber moving target
technology known by the acronym TASR (Timely Address
Space Randomization) [7]. The TASR technology mitigates
memory leaks from the defended system by re-randomizing
system memory at runtime. The default operation mode of
TASR is to re-randomize memory just before processing an
input system call after there has been a system output gener-
ated by the defended system [7]. For parsimony, we do not
model input system calls, so TASR’s default operation mode
corresponds in our model to memory re-randomization after
each system output exists the system.
The TASR technology provides a convenient example of
a technology that executes a potentially costly mitigation in
response to a speciﬁc threat scenario that would potentially
beneﬁt from adaptively predictive techniques to achieve se-
curity goals without unduly impacting system performance. In
what follows, we use the TASR prototype as a surrogate system
to derive realistic model parameters when they are needed to
enable quantitative evaluation of our framework’s performance.
Our stand-in for mitigation in this study is the TASR pro-
totype’s runtime memory re-randomization, described in detail
in [7]. The performance of the C-based programs in the SPEC
CPU2006 benchmark with and without the TASR prototype
in operation were studied in [7] and we have evaluated the
execution time increases as measured in absolute time in the 10
programs examined. We normalized these times by the number
of re-randomizations executed by the TASR prototype in the
examined runs, and this analysis resulted in the time per re-
randomization event for each of the studied programs in the
SPEC benchmark. The median of this set is 0.1 seconds. We
use this median number as our value for the latency cost of
mitigation, Li,m.
To generate a plausible set of parameters for representing
the system output search task in our model we analyzed a
sampling of system outputs from the Iceweasel [21] web
browser application using a custom-built data capture tool [22].
This data sample gives a mean of 221 hexadecimal characters
for the average system output payload size. Due to space
constraints we defer extended reporting on the sensitivity of
our model to these precise parameter choices and note that
preliminary investigations reveal the model behavior and the
conclusions we draw from this model behavior to be robust
over a reasonable range of parameter values.
For the Iceweasel data sample we also obtained the list of
valid, active system addresses for analysis. We note that the
list of valid, active memory addresses is updated periodically
throughout the application’s operation. We ignore this dynamic
factor in our search model and assume that a Bloom ﬁlter
has been programmed prior to the start of the simulation, and
further that the valid list of active address signatures does not
change as the simulation proceeds. We calculate our n (i.e.,
number of address signatures used to program the Bloom ﬁlter)
and m (i.e., size of Bloom ﬁlter in memory) values at the start
of the data sample and hold these constant throughout our
simulation.
In this formulation the operator selects an acceptable value
for PF P , taking into consideration the tradeoff between PF P
and k under the assumption of optimal k value, shown in Fig. 2.
Unless otherwise indicated, we set the PF P parameter to a
value of 0.1 throughout this study, implying the use of k = 4.