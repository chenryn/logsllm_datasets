User:  Provide a solution for clickhouse cluster log error readonlyProvide a solution for clickhouse cluster log error readonly
Assistant: The clickhouse cluster mode data table may generate a readonly error, because the standard clickhouse cluster deployment uses zookeeper for cluster data synchronization. When the zookeeper data differs from the local metadata, the table will be read-only (readonly). The frequent process of table deletion and table creation may result in damage to zookeeper data or out-of-time synchronization, resulting in discrepancies between zookeeper data and local metadata, resulting in readonly tables. When the data difference between replicas synchronized by zookeeper is too large, the table will be read-only (readonly). When the writing pressure is too high, the zookeeper data synchronization will not be timely, and the difference between each copy is too large. When the maximum allowed difference value is exceeded, the clickhouse copy protection mechanism will be triggered to force the data table to be read-only. After the copy difference is normal Writing can be performed again. Solution: 1. Check the error message. The error message will contain the path of the data table. Log in to zookeeper to check whether the address exists and whether the metadata is consistent with the local one. If it is damaged or does not exist, you need to delete this copy and create it again. Delete the data table for this copy. After the deletion is complete, click Repair on the data table page, and the correct table will be automatically created and the data will be automatically synchronized. Connect to zk: cd /data/app/zookeeper/bin ./zkCli.sh -server 10.0.0.1:18127 > addauth digest yzh:Zook_123_yzh > ls /clickhouse/tables/3/dola/test_table Delete data table: drop table shard_3. dola_test_table_replica; 2. Excessive writing pressure. If there is no difference between the zk data and the clickhouse data, it may be that the writing pressure of the clickhouse is too high. The long-term solution needs to expand the clickhouse, and the short-term solution needs to stop the relevant pipeline and wait for the table to recover automatically It can be written again after normal operation.