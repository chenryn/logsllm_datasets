# Title: Performance Analysis of a Consensus Algorithm Combining Stochastic Activity Networks and Measurements

## Authors:
- Andrea Coccoli
- P'eter Urb'an
- Andrea Bondavalli

### Abstract
Agreement protocols are essential building blocks for fault-tolerant distributed applications. While numerous such protocols have been published, their performance analysis remains largely unexplored. This paper initiates a performance study by focusing on the consensus problem, which is closely related to other agreement problems. We analyze the latency of a consensus algorithm designed for an asynchronous model with failure detectors, using both experiments on a cluster of PCs and simulations with Stochastic Activity Networks (SANs). The latency is evaluated under three scenarios: (1) no failures or suspicions, (2) failures without wrong suspicions, and (3) no failures but with wrong suspicions. We validate the SAN model by comparing experimental results with simulation outcomes, identifying limitations and suggesting new directions for evaluating agreement protocol performance.

**Keywords:** quantitative analysis, distributed consensus, failure detectors, Stochastic Activity Networks, measurements

## 1. Introduction
Agreement problems, such as atomic commitment, group membership, and total order broadcast, are fundamental for fault-tolerant distributed applications, including transactional and time-critical systems. These problems have been extensively studied, and many protocols have been developed. However, most analyses focus on safety and liveness properties, with little attention to performance. One reason is the complexity of these protocols, making analytical performance evaluation challenging. 

Some studies have attempted to address this gap. For example, [3] and [4] use discrete event simulation to analyze four total order broadcast algorithms, while [5] employs a contention-aware metric to compare the performance of four total order broadcast algorithms. [6] and [7] analyze atomic broadcast protocols for wireless networks, deriving assumption coverage and performance metrics. [8] presents a probabilistic verification approach for a synchronous round-based consensus protocol, and [9] evaluates the performability of a group-oriented multicast protocol. [10] compares the latency of a consensus algorithm under different failure detector implementations. However, most of these studies only consider failure-free runs, providing an incomplete understanding of the protocols' behavior.

To address this, we start with a detailed performance analysis of a consensus algorithm, extending the work in [10]. The consensus problem involves a set of processes that must agree on a common value proposed by one of them. It is known that consensus cannot be solved deterministically in an asynchronous model, necessitating a stronger system model. We choose the asynchronous model with unreliable failure detectors and analyze the Chandra-Toueg consensus algorithm based on the ✸S failure detector.

Our performance analysis combines experiments on a cluster of PCs and simulations using Stochastic Activity Networks (SANs). We evaluate the latency, defined as the time from the algorithm's start until the first process decides, under different scenarios: (1) no failures or suspicions, (2) failures without wrong suspicions, and (3) no failures but with wrong suspicions. We validate the SAN model by comparing it with experimental results, identifying its limitations and suggesting new directions for future research.

## 2. Context of Our Performance Analysis
This section provides the context for our performance analysis, including the algorithms, performance measures, and the environment.

### 2.1 The ✸S Consensus Algorithm
The consensus problem involves \( n \) processes, each starting with an initial value. The processes must decide on a common value, which is the initial value of one of the processes. We analyze the Chandra-Toueg consensus algorithm, which uses the ✸S failure detector. This algorithm requires a majority of correct processes and operates in an asynchronous system model with unreliable failure detectors. Each process has a local failure detector module that maintains a list of suspected processes. The ✸S failure detector ensures completeness (every crashed process is eventually suspected) and accuracy (eventually, some correct process is no longer suspected).

The algorithm follows a rotating coordinator paradigm, where each process proceeds through asynchronous rounds, and one process acts as the coordinator in each round. The coordinator attempts to impose a decision value, and if it fails, a new round with a new coordinator begins.

### 2.2 Failure Detection Algorithm
Each process has a failure detector module that manages a list of suspected processes. We use a push-style failure detector implemented with heartbeat messages. Each process periodically sends a heartbeat message to all other processes. If a process does not receive a message from another process within a timeout period \( T \), it suspects that process. The failure detector is parameterized with a timeout \( T \) and a heartbeat period \( Th \).

### 2.3 Latency as Our Performance Measure
Latency is a key performance measure, defined as the time from the start of the algorithm until the first process decides. This is relevant for fault-tolerant replicated services, where the first response received by a client is from the server that first decided in the consensus algorithm. Future work will include studying the throughput of the ✸S consensus algorithm.

### 2.4 Classes of Runs Considered
We consider the following classes of runs:
1. **No Failures or Suspicions:** All processes are correct, and the failure detectors do not suspect any process.
2. **Failures Without Wrong Suspicions:** One process is initially crashed, and the failure detectors correctly suspect the crashed process and do not suspect correct processes.
3. **No Failures but with Wrong Suspicions:** All processes are correct, but the failure detectors wrongly suspect some processes.

### 2.5 Hardware and Software Environment
The consensus algorithm is run on a cluster of PCs. The hardware and software environment is described in detail in the subsequent sections.

## 3. SAN Model of the Consensus Algorithm
This section describes the SAN model used to simulate the consensus algorithm and its environment. The model focuses on the control aspect of the algorithm, ignoring the data aspect. The failure detectors are modeled using QoS metrics, and communications are modeled to account for network and host contention.

## 4. Implementation and Measurements
This section discusses the implementation details and the measurement setup. We present the results of the experiments and simulations, and validate the SAN model by comparing it with the experimental results.

## 5. Results and Discussion
We present and discuss the results of our performance analysis, highlighting the latency under different scenarios and the validation of the SAN model. We also identify the limitations of the model and suggest new directions for future research.

## 6. Conclusion
In conclusion, this paper provides a detailed performance analysis of a consensus algorithm using a combination of experiments and simulations. We validate the SAN model and identify its limitations, suggesting new directions for evaluating the performance of agreement protocols.

---

This revised version aims to improve clarity, coherence, and professionalism, ensuring that the content is well-structured and easy to follow.