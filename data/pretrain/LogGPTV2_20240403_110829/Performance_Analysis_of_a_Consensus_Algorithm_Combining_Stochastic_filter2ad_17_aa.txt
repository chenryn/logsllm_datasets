title:Performance Analysis of a Consensus Algorithm Combining Stochastic
Activity Networks and Measurements
author:Andrea Coccoli and
P&apos;eter Urb&apos;an and
Andrea Bondavalli
Performance Analysis of a Consensus Algorithm
Combining Stochastic Activity Networks and Measurements∗
Andrea Coccoli
†
PI:EMAIL
‡
P´eter Urb´an
peter.urban@epﬂ.ch
‡
Andr´e Schiper
andre.schiper@epﬂ.ch
Andrea Bondavalli(cid:1)
PI:EMAILﬁ.it
†
CNUCE-CNR, Via Alﬁeri 1, I-56010 Ghezzano, Pisa, Italy
‡ ´Ecole Polytechnique F´ed´erale de Lausanne (EPFL), CH-1015 Lausanne, Switzerland
(cid:1)Universit`a di Firenze, Via Lombroso 6/17, 50134 Firenze, Italy
Abstract
Protocols which solve agreement problems are essential
building blocks for fault tolerant distributed applications.
While many protocols have been published, little has been
done to analyze their performance. This paper represents
a starting point for such studies, by focusing on the con-
sensus problem, a problem related to most other agreement
problems. The paper analyzes the latency of a consensus
algorithm designed for the asynchronous model with fail-
ure detectors, by combining experiments on a cluster of
PCs and simulation using Stochastic Activity Networks. We
evaluated the latency in runs (1) with no failures nor fail-
ure suspicions, (2) with failures but no wrong suspicions
and (3) with no failures but with (wrong) failure suspicions.
We validated the adequacy and the usability of the Stochas-
tic Activity Network model by comparing experimental re-
sults with those obtained from the model. This has led us to
identify limitations of the model and the measurements, and
suggests new directions for evaluating the performance of
agreement protocols.
Keywords: quantitative analysis, distributed consensus, fail-
ure detectors, Stochastic Activity Networks, measurements
1 Introduction
Agreement problems — such as atomic commitment,
group membership, or total order broadcast — are essential
building blocks for fault tolerant distributed applications,
including transactional and time critical applications. These
agreement problems have been extensively studied in vari-
ous system models, and many protocols solving these prob-
lems have been published [1, 2]. However, these protocols
have almost only been analyzed from the point of view of
their safety and liveness properties, and very little has been
done to analyze their performance. One of the reasons is
probably that agreement protocols are complex, typically
∗
Research supported by a grant from the CSEM Swiss Center for Elec-
tronics and Microtechnology, Inc., Neuchˆatel, and by a Short Term Mobil-
ity grant from CNR, the Italian National Research Council.
too complex for analytical approaches to performance eval-
uation. Nevertheless, a few papers have tried to analyze the
performance of agreement protocols: [3] and [4] analyze
quantitatively four different total order broadcast algorithms
using discrete event simulation; [5] uses a contention-aware
metric to compare analytically the performance of four total
order broadcast algorithms; [6, 7] analyze atomic broadcast
protocols for wireless networks, deriving assumption cov-
erage and other performance related metrics; [8] presents
an approach for probabilistically verifying a synchronous
round-based consensus protocol; [9] evaluates the performa-
bility of a group-oriented multicast protocol; [10] compares
the latency of a consensus algorithm by simulation, under
different implementations of failure detectors.1 In all these
papers, except for [10, 8, 9], the protocols are only analyzed
in failure free runs. This only gives a partial and incom-
plete understanding of their quantitative behavior. More-
over, in [3, 4] the authors model communication delays in
a way that completely ignores contention on the network
and the hosts: the communication delays are modeled us-
ing a distribution that was obtained independently from the
agreement protocols analyzed. This approach does not ac-
count for the fact that the transmission delay of messages is
greatly inﬂuenced by the message trafﬁc that the algorithm
generates itself. In fact, the transmission delay of messages
cannot realistically be assumed to be independent of the al-
gorithm that generates them.
A detailed quantitative performance analysis of agree-
ment protocols represents a huge work. Where should such
a work start? As most agreement problems are related to
the abstract consensus problem [11, 12, 13] it seems natural
to start by a performance analysis of a consensus algorithm,
and to extend the work of [10]. This is the goal of this paper,
which analyzes a consensus algorithm.
The consensus problem is deﬁned over a set of processes.
Informally, each process in this set proposes a value ini-
tially, and the processes must decide on the same value, cho-
sen among the proposed values [11]. It has been shown that
consensus cannot be solved deterministically in an asyn-
chronous model [14]. A stronger system model is thus re-
1The algorithm is the same as in this paper, but the failure detection
techniques and the deﬁnition of latency differ.
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:20:03 UTC from IEEE Xplore.  Restrictions apply. 
quired to solve consensus. We have chosen the asynchronous
model with unreliable failure detectors [11], and we have
decided to analyze the Chandra-Toueg consensus algorithm
based on the ✸S failure detector.
The paper evaluates the performance of the Chandra-
Toueg ✸S consensus algorithm by combining the follow-
ing two approaches: (1) experiments on a cluster of PCs
and (2) simulation. The simulation was conducted using
Stochastic Activity Networks (SANs), a class of timed Petri
nets. Only the control aspect of the consensus algorithm had
to be modeled for the simulation: the data aspect (e.g., the
content of messages) could be ignored. The failure detec-
tors were modeled in an abstract way, using the quality of
service (QoS) metrics proposed by Chen et al. [15]. Com-
munications were modeled in a way that takes contention
on the network and on the hosts into account. The goal
of our quantitative analysis was to determine the latency of
the consensus protocol, i.e., the time elapsed from the the
beginning of the algorithm until the ﬁrst process decides.
Moreover, we evaluated the latency in different classes of
runs: (1) runs with no failures nor failure suspicions, (2)
runs with failures but no wrong suspicions, and (3) runs
with no failures but with (wrong) failure suspicions. We
combined the results of the simulations and the measure-
ments: some measurement results have been used to deter-
mine input parameters for the simulation model. Further-
more, a validation of the adequacy and the usability of the
model has been made by comparing experimental results
with those obtained from the model. This validation activ-
ity led us to determine some limitations of the model and
new ideas for the experimental measurements, and suggests
us new directions for evaluating the performance of agree-
ment protocols.
The paper is structured as follows. Section 2 presents
the context of our performance analysis: the algorithms, the
performance measures, and the environment for running the
algorithm. Section 3 describes the SAN model of the con-
sensus algorithm and its environment. Section 4 mentions
interesting points of the implementation and the measure-
ments. We present and discuss our results in Section 5, and
conclude the paper in Section 6.
2 Context of our performance analysis
As stated in the previous section, the goal of the paper
is the performance analysis of a consensus algorithm.
In
Section 2.1, we start by describing the consensus algorithm
that we want to analyze. As this algorithm uses failure de-
tectors, we give in Section 2.2 the algorithm used to imple-
ment failure detectors. Then, in Section 2.3 we explain what
performance measures we want to obtain from our analysis.
Obviously, these measures vary from one run to another,
depending on the failure behavior of the processes, and the
output of the failure detectors.
In Section 2.4, we intro-
duce the different classes of runs for which we obtain per-
formance measures. Finally, in Section 2.5 we describe the
hardware and software environment in which our consensus
algorithm is supposed to run.
2.1 The ✸S consensus algorithm
The consensus problem is deﬁned over a set of n pro-
cesses p1, . . . , pn. Each process pi starts with an initial
value vi and the processes have to decide on a common
value v that is the initial value of one of the processes. We
consider in our study the consensus algorithm based on the
failure detector ✸S proposed by Chandra and Toueg [11].
The algorithm requires a majority of correct processes. We
describe below the algorithm, up to the level of detail sufﬁ-
cient to understand the experiments that we have conducted.
The algorithm assumes an asynchronous system model aug-
mented with (unreliable) failure detectors. In that model,
each process has a local failure detector module, which main-
tains a list of processes that are suspected to have crashed.
A process pi can query its local failure detector module to
learn whether some other process pj is currently suspected
or not. Roughly speaking, the ✸S failure detector ensures
that (1) every crashed process is eventually suspected for
ever by every correct process (completeness property), and
(2) eventually there exists a correct process that is no more
suspected by any correct process (accuracy property).
The ✸S consensus algorithm is based on the rotating co-
ordinator paradigm: each process proceeds in a sequence of
asynchronous rounds (i.e., not all processes necessarily ex-
ecute the same round at a given time t), and in each round
one process assumes the role of the coordinator (pi is the
coordinator for the rounds kn + i). All the processes have
an a priori knowledge of the identity of the coordinator of
a given round. Processes that are not coordinator in a given
round are called participants. In each round, every message
is sent either by the participants to the coordinator or by the
coordinator to the participants. The role of the coordinator
is to impose a decision value. If it succeeds, the consensus
algorithm terminates. It it fails, a new round with a new co-
ordinator starts, in which the new coordinator will in turn
try to impose a decision value, etc. Knowing the details of
the execution of one round is not necessary for understand-
ing this paper. We refer the interested reader to [11].
2.2 Failure detection algorithm
As explained above, each process has a failure detector
module, which manages a list of processes that are sus-
pected to have crashed. The failure detector modules are
unreliable, in the sense that they can make mistakes by in-
correctly suspecting a correct process and also by not sus-
pecting a crashed process.
A variety of techniques exist for implementing a failure
detector: they are usually qualiﬁed as push and pull tech-
In push techniques, a given process periodically
niques.
sends a message (called heartbeat message) to inform the
failure detector module of other processes that it is alive.
In pull techniques, the failure detector module periodically
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:20:03 UTC from IEEE Xplore.  Restrictions apply. 
sends a ping message to other processes, and waits for a
reply.
We chose a push-style failure detector implemented us-
ing heartbeat messages (Figure 1): each process periodi-
cally sends a heartbeat message to all other processes. Fail-
ure detection is parameterized with a timeout value T and
a heartbeat period Th. Process p starts suspecting process q
if it has not received any message from q (heartbeat or ap-
plication message) for a period longer than T . Process p
stops suspecting process q upon reception of any message
from q (heartbeat or application message). The reception of
any message from q resets the timer for the timeout T .
t=0
Th
Th
Th
heartbeat 
message
q
p
t
t
T
T
T
p suspects q
T
Figure 1. Heartbeat failure detection.
There exist heartbeat failure detectors with possibly bet-
ter characteristics [15]. Our choice has the advantage that
it is very simple to implement and control. In particular, it
does not rely on synchronized clocks.
2.3 Latency as our performance measure
Latency and throughput are meaningful measures of the
performance of algorithms. Roughly speaking, latency mea-
sures the time elapsed between the beginning and the end of
the execution of an algorithm, while throughput measures
the maximum number of times that a given algorithm can
be executed per second.
Our study focuses on the latency of the consensus pro-
tocol, deﬁned exactly as follows. We assume that all par-
ticipants propose values at the same time t0, and let t1 be
the time at which the ﬁrst process decides. We deﬁne the
latency as t1 − t0. This is a reasonable measure for the
following reason. Consider a service replicated for fault
tolerance using active replication [16]. Clients of this ser-
vice send their requests to the server replicas using Atomic
Broadcast [17] (which guarantees that all replicas see all re-
quests in the same order). Atomic Broadcast can be solved
by using a consensus algorithm [11]: a client request can
be delivered at a server si as soon as si decides in the con-
sensus algorithm. Once a request is delivered, the server
replica processes the client request, and sends back a reply.
The client waits for the ﬁrst reply, and discards the other
ones (identical to the ﬁrst one). If we assume that the time
to service a request is the same on all replicas, and the time
to send the response from a server to the client is the same
for all servers, then ﬁrst response received by the client is
the response sent by the server that has ﬁrst decided in the
consensus algorithm.
Studying the throughput of the ✸S consensus algorithm
will be one of the subjects of our future work. Through-
put should be considered in a scenario where a sequence
of consensus is executed, i.e., on each process, consensus
#(k+1) starts immediately after consensus #k has decided.
Note that, unlike in the deﬁnition of latency, not all pro-
cesses necessarily start consensus at the same time.
2.4 Classes of runs considered
The latency of a consensus algorithm varies for differ-
ent number of processes. However, given n, the latency can
also vary from one run to another, depending on (1) the dif-
ferent delays experienced by messages, (2) the failure pat-
tern of processes and (3) the failure detector history (i.e.,
the output of the failure detectors). We have considered the
following classes of runs:
1. All processes are correct, and the failure detectors are
accurate, i.e., they do not suspect any process.
This is the scenario that one expects to happen most
of the time. It assumes a failure detection mechanism
that does not incorrectly suspects correct processes.
There is a price to pay for the accuracy of the failure
detector, though: the failure detection timeout T must
be high to avoid wrong suspicions, thus detecting fail-
ures relatively slowly, or the heartbeat period Th must
be shortened thus increasing the network load.
2. One process is initially crashed, and the failure detec-
tors are complete and accurate: the crashed process
is suspected forever from the beginning, and correct
processes are not suspected. We have further distin-
guished the following cases: (i) the ﬁrst coordinator
is initially crashed, and (ii) another process is initially
crashed.
3. All processes are correct and the failure detectors are
not accurate, i.e., they wrongly suspect some pro-
cesses. We obtained the histories of the failure detec-
tors by implementing the failure detector algorithm
described in Section 2.2 and conducting measurements
for different values of the parameters Th and T . The
failure detector histories obtained this way allowed
us to estimate the failure detector quality of service
(QoS) metrics deﬁned in [15] (see Section 3.4). This