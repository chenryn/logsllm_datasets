title:CloudProphet: towards application performance prediction in cloud
author:Ang Li and
Xuanran Zong and
Srikanth Kandula and
Xiaowei Yang and
Ming Zhang
CloudProphet: Towards Application Performance
Prediction in Cloud
Ang Li, Xuanran Zong, Srikanth Kandula†, Xiaowei Yang, Ming Zhang†
Duke University
†Microsoft Research
Durham, NC
{angl,xrz,xwy}@cs.duke.edu
Redmond, WA
{srikanth,mzh}@microsoft.com
ABSTRACT
Choosing the best-performing cloud for one’s application is a crit-
ical problem for potential cloud customers. We propose Cloud-
Prophet, a trace-and-replay tool to predict a legacy application’s
performance if migrated to a cloud infrastructure. CloudProphet
traces the workload of the application when running locally, and
replays the same workload in the cloud for prediction. We discuss
two key technical challenges in designing CloudProphet, and some
preliminary results using a prototype implementation.
Categories and Subject Descriptors
C.4 [Performance of Systems]: General—measurement techniques,
performance attributes; C.2.4 [Computer-Communication Net-
works]: Distributed Systems—distributed applications
General Terms
Design, Performance, Measurement.
Keywords
Cloud computing, performance, prediction.
1.
INTRODUCTION
The public cloud computing market has grown dramatically in
the recent years. Many companies, including Amazon, Microsoft,
and Rackspace, have all released their own public cloud com-
puting infrastructures, such as Amazon AWS, Microsoft Azure,
and Rackspace CloudServers. These infrastructures, albeit offer-
ing similar services, can diverge signiﬁcantly in terms of perfor-
mance [3]. Hence, if a potential cloud customer is planning to mi-
grate her legacy application into cloud, it is critical to know: which
cloud does my application perform best on?
The most straightforward way is to actually migrate the appli-
cation to different clouds, so that the customer can ﬁeld test her
application’s performance inside the cloud infrastructures. How-
ever, migration is not cheap. For infrastructure providers, the whole
software stack of the application needs to be deployed on the cloud
VMs, incurring huge conﬁguration effort. The application data also
need to be migrated to ensure realistic benchmarking, raising the
data security concern.
In this work, we focus on predicting an application’s perfor-
mance running on a target cloud infrastructure, prior to any mi-
gration effort. Depending on accuracy, the prediction result can di-
Copyright is held by the author/owner(s).
SIGCOMM’11, August 15–19, 2011, Toronto, Ontario, Canada.
ACM 978-1-4503-0797-0/11/08.
rectly point the customer to the best-performing provider, or at least
limit the scope for actual migration and ﬁeld testing. There are two
common approaches for performance prediction. Standard bench-
marks [1, 3] can provide a baseline to compare the performance of
different providers. However, as the benchmarks are simple by de-
sign, it is challenging to map the complex and multi-tiered cloud
applications to the limited set of benchmarks. Modeling is another
widely used approach to predict the performance of simple com-
putation and I/O-intensive applications [5]. Yet it remains a chal-
lenging task to describe the complex and often distributed cloud
applications’ workload using concise model characteristics.
In this paper, we propose CloudProphet, a performance predic-
tion tool aiming to provide accurate and application-speciﬁc pre-
diction results. CloudProphet takes a trace-and-replay approach [4].
During tracing, CloudProphet records the detailed workload infor-
mation and the internal dependency of a representative application
run. During replaying, CloudProphet runs an agent in the target
cloud platform, which emulates the workload of the traced applica-
tion run by replaying the recorded workload and dependency. The
performance of the agent is then used to predict the application per-
formance after migration.
We choose this approach for several reasons. First, it does not
require any a priori knowledge of the performance characteristics
of the target cloud infrastructure, because the agent uses real work-
load to test the infrastructure’s efﬁciency. This is particularly suit-
able for the cloud environment, as the performance of a cloud can
change frequently due to interferences and equipment upgrades.
Second, the approach is less likely to be limited by the complexity
of the application. As long as we can trace the correct dependency,
the approach should work with applications with multiple compo-
nents and rich inter-component communication, which is essential
for the practicality of the tool.
In the following, we describe several key design challenges of
CloudProphet.
2. CHALLENGES
2.1 Non-deterministic Application Workload
For many multi-threaded applications, such as databases and sci-
entiﬁc computation tools [6], the workload on each thread depends
on the order of the synchronization events (e.g., locks), which in
turn depends on how the threads interleave with each other. Due to
the performance (e.g. CPU and I/O speed) difference in cloud, the
application threads may interleave differently if migrated, which
then leads to different workload. In this case, simply replaying the
workload events collected locally can result in poor prediction ac-
curacy.
426We ﬁrst illustrate the problem using a real example. Figure 1(a)
shows part of the request handling code of a ﬁle hosting application
UDDropBox, and (b) shows the lock and I/O events triggered by
two application threads during tracing. Note that thread T 2 fails to
acquire the lock and has to sleep for a while before retrying.
A naive replay mechanism simply replays all events in each
thread’s trace one after another. This may cause the replayed
events to diverge from the real events of the application if migrated.
For instance, if the cloud VM has faster disk I/O, T 1’s unlock
event may happen earlier than the ﬁrst trylock event of T 2 (Fig-
In this case, the ﬁrst trylock of T 2 would succeed,
ure 1(c)).
and the application would directly trigger open instead of usleep.
However, if we replay strictly according to the event trace, T 2 still
needs to replay usleep and another trylock before open. This
adds unnecessary overhead to T 2.
CloudProphet introduces a novel mechanism to address this.
During replay, CloudProphet detects any synchronization event that
occurs out-of-order compared to the order in the recorded trace.
An out-of-order synchronization event may cause the future work-
load events to diverge from the events in the current trace. If such
an event is detected, CloudProphet pauses the replay process, and
starts a new application run on the local machine to update the
workload events after the diverged point. Speciﬁcally, the new
run is steered by enforcing the new synchronization events order
occurred in cloud [2]. After the application run has reached the
diverged point, CloudProphet then collects the updated workload
events, resumes the replay process, and uses those events for future
replaying. The process is repeated if new out-of-order events are
detected.
Consider again the previous example. With the new mechanism
the cloud replayer will detect that the ﬁrst trylock of T 2 occurs
out-of-order, because the event happens before T 1’s unlock in
the trace, while during replay it occurs after. CloudProphet then
pauses the replay right after the trylock, and tries to update the
future events through a new run of the application. During the new
run, CloudProphet enforces the order between the trylock and the
unlock, so that the trylock always happens after the unlock. In
this way, the trylock is guaranteed to succeed, and therefore the
application will immediately trigger open and other I/O functions
afterwards. Finally, CloudProphet updates the event trace using the
new local events that do not contain the extra usleep and trylock,
and resumes the replay.
The mechanism does not make any assumption on the applica-
tion model, and therefore theoretically works for arbitrary applica-
tion. On the other hand, one practical limitation is that it requires
multiple runs of the application to obtain the right workload to re-
play. It is our ongoing work to adopt optimizations to reduce the
overhead.
2.2 Replay Computation Workload
To faithfully replay the computation (CPU and memory) work-
load of the original application, we need to trace the exact CPU
instructions executed and the memory footprint. However, this can
incur signiﬁcant tracing overhead and increase the replayer com-
plexity. CloudProphet instead adopts a simple linear model to map
the local computation workload to the one in cloud. The model
scales the CPU time measured locally by a constant factor cali-
brated by standard CPU benchmarks. The cloud replayer then uses
a busy-loop to emulate the scaled workload. The model works rea-
sonably well for most applications we have tested (the error rate
is smaller than 30% for most cases), including memory-intensive
applications in the SPLASH-2 benchmark [6].
...
while (!trylock(&lock)) {
  usleep(10000);
}
open("dropbox.sqlite");
... // more file operations
unlock(&lock);
...
(a) Code snippet
T1
.
.
.
T2
.
.
.
T1
.
.
.
T2
.
.
.
e
m
T
i
trylock
open
unlock
trylock
usleep
trylock
open
trylock
open
unlock
trylock
open
unlock
.
.
.
.
.
.
.
.
.
.
.
.
(b) Events collected
(c) Events replayed
locally
in cloud
Figure 1: (a) The UDDropBox code snippet that accesses the lock-
protected database ﬁle; (b) The events collected locally; (c) The events
replayed in the cloud by CloudProphet.
real processing time
CloudProphet
naive replayer
 3000
 2500
)
s
m
(
e
m
T
i
 2000
 1500
 1000
 500
 0
One thread Two threads Three threads
Figure 2: The prediction results of the UDDropBox application.
3. PRELIMINARY RESULTS
Figure 2 shows the prediction results of UDDropBox with 1, 2,
and 3 concurrent clients. Each client uploads ten 1MB ﬁles back-
to-back. We predict the total processing time on an Amazon AWS
m1.large instance. In comparison, we also show the real process-
ing time when actually running DropBox on the cloud VM, and
the prediction results using the naive replay mechanism. The pre-
diction result of CloudProphet closely matches the real processing
time. Moreover, with multiple threads the naive mechanism pre-
dicts almost ten-fold of the real processing time, suggesting that
the workload collected on our local machine diverges signiﬁcantly
from the real workload in cloud. The result shows our approach is
promising in predicting the performance of applications with non-
deterministic workload.
4. REFERENCES
[1] Cloudharmony benchmark results. http://cloudharmony.com/benchmarks.
[2] Jong-Deok Choi and Harini Srinivasan. Deterministic replay of Java
multithreaded applications. In Proceedings of the SIGMETRICS symposium on
Parallel and distributed tools - SPDT ’98, pages 48–59, New York, New York,
USA, 1998. ACM Press.
[3] Ang Li, Xiaowei Yang, Srikanth Kandula, and Ming Zhang. CloudCmp:
Comparing Public Cloud Providers. In ACM IMC, 2010.
[4] Michael P. Mesnier, Matthew Wachs, Raja R. Sambasivan, Julio Lopez, James
Hendricks, Gregory R. Ganger, and David O’Hallaron. //trace: parallel trace
replay with approximate causal events. In USENIX FAST, 2007.
[5] Mengzhi Wang, Kinman Au, Anastassia Ailamaki, Anthony Brockwell, Christos
Faloutsos, and Gregory R. Ganger. Storage device performance prediction with
cart models. In MASCOTS, pages 588–595, Washington, DC, USA, 2004.
[6] Steven Cameron Woo, Moriyoshi Ohara, Evan Torrie, Jaswinder Pal Singh, and
Anoop Gupta. The SPLASH-2 programs: characterization and methodological
considerations. In ACM ISCA, 1995.
427