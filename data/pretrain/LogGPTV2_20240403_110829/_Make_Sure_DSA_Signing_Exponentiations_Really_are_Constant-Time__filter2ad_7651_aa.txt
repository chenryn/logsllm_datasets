title:"Make Sure DSA Signing Exponentiations Really are Constant-Time"
author:Cesar Pereida Garc&apos;ıa and
Billy Bob Brumley and
Yuval Yarom
“Make Sure DSA Signing Exponentiations Really are
Constant-Time”
Cesar Pereida García
Department of Computer
Science
Aalto University, Finland
cesar.pereida@aalto.ﬁ
Billy Bob Brumley
Department of Pervasive
Computing
Tampere University of
Technology, Finland
billy.brumley@tut.ﬁ
Yuval Yarom
The University of Adelaide and
PI:EMAIL
Data61, CSIRO, Australia
ABSTRACT
TLS and SSH are two of the most commonly used proto-
cols for securing Internet traﬃc. Many of the implemen-
tations of these protocols rely on the cryptographic primi-
tives provided in the OpenSSL library. In this work we dis-
close a vulnerability in OpenSSL, aﬀecting all versions and
forks (e.g. LibreSSL and BoringSSL) since roughly October
2005, which renders the implementation of the DSA signa-
ture scheme vulnerable to cache-based side-channel attacks.
Exploiting the software defect, we demonstrate the ﬁrst pub-
lished cache-based key-recovery attack on these protocols:
260 SSH-2 handshakes to extract a 1024/160-bit DSA host
key from an OpenSSH server, and 580 TLS 1.2 handshakes
to extract a 2048/256-bit DSA key from an stunnel server.
Keywords
applied cryptography; digital signatures; side-channel anal-
ysis; timing attacks; cache-timing attacks; DSA; OpenSSL;
CVE-2016-2178
1.
INTRODUCTION
One of the contributing factors to the explosion of the
Internet in the last decade is the security provided by the
underlying cryptographic protocols. Two of those protocols
are the Transport Layer Security (TLS) protocol, which pro-
vides security to network communication and the more spe-
cialized Secure Shell (SSH), which provides secure login to
remote hosts.
Software implementations of these protocols often use the
cryptographic primitives’ implementations of the OpenSSL
cryptographic library. Consequently, the security of these
implementations depends on the security of OpenSSL.
In this paper we present a novel side-channel cache-timing
attack against OpenSSL’s DSA implementation. The attack
exploits a vulnerability in OpenSSL, which fails to use a side-
channel-secure implementation of modular exponentiation
— the core mathematical operation used in DSA signatures.
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
CCS’16 October 24-28, 2016, Vienna, Austria
c(cid:13) 2016 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-4139-4/16/10.
DOI: http://dx.doi.org/10.1145/2976749.2978420
Our attack builds upon several techniques to proﬁle the
cache memory and capture timing signals. The signals are
processed and converted into a sequence of square and mul-
tiplication (SM) operations from which we extract informa-
tion to create a lattice problem. The solution to the lattice
problem yields the secret key of digital signatures.
Flush+Reload [40] is a powerful technique to perform
cache-timing attacks. We adapt the Flush+Reload tech-
nique to OpenSSL’s implementation of DSA and, exploit-
ing properties of the Intel implementation of the x86 and
x64 processor architectures, our spy program probes rele-
vant memory addresses to create a signal trace.
We process the captured signal to get the SM sequence
performed by the sliding window exponentiation (SWE) al-
gorithm. Then we observe and analyze the number of bits
that can be extracted and used from each of those sequences.
Later, the variable amount of bits extracted from each trace
is used as input to a lattice attack that recovers the private
key.
To bridge the gap between the limited resolution of the
Flush+Reload technique [4] and the high-performance of
the OpenSSL code we apply the performance-degradation
technique of Allan et al. [4]. This technique slows the expo-
nentiation by an average factor of 20, giving a high resolution
trace and allowing us to extract up to 8 bits of information
from some of the traces.
Similar to previous works [9, 14, 21, 32], we perform a
lattice attack to recover the secret key. We use the lattice
construction of Benger et al. [9] and solve the resulting lat-
tice problem using the lattice reduction technique of Nguyen
and Shparlinski [28].
A unique feature of our work is that we target common
cryptographic protocols. Previous works that demonstrate
cache-timing key-recovery attacks only target the crypto-
graphic primitives, ignoring potential cache noise from the
protocol implementation.
In contrast, we present end-to-
end attacks on two common cryptographic protocols: SSH
and TLS. We are, therefore, the ﬁrst to demonstrate that
cache-timing attacks are a threat not only when executing
the cryptographic primitives but also in the presence of the
cache activity of the whole protocol suite.
Our contributions in this work are the following:
• We identify a security weakness in OpenSSL which
fails to use a side-channel safe implementation when
performing DSA signatures. (Section 3)
• We describe how to use a combination of the Flush+
Reload technique with a performance-degradation at-
tack to leak information from the unsafe SWE algo-
rithm. (Section 4)
• We present the ﬁrst key-recovery cache-timing attack
on the TLS and SSH cryptographic protocols. (Sec-
tion 5)
• We construct and solve a lattice problem with the side-
channel information and the digital signatures in order
to recover the secret key. (Section 6)
2. BACKGROUND
2.1 Memory Hierarchy
Accessing data and instructions from main memory is a
time consuming operation which delays the work of the fast
processors, for that reason the memory hierarchy includes
smaller and faster memories called caches. Caches improve
the performance by exploiting the spatial and temporal lo-
cality of the memory access.
In modern processors the hierarchy of caches is structured
as follows, higher-level caches, located closer to the processor
core, are smaller and faster than low-level caches, which are
located closer to main memory. Recent Intel architecture
typically has three levels of cache: L1, L2 and Last-Level
Cache (LLC).
Each core has two L1 caches, a data cache and an instruc-
tion cache, each 32 KiB in size with an access time of 4
cycles. L2 caches are also core-private and have an inter-
mediate size (256 KiB) and latency (7 cycles). The LLC is
shared among all of the cores and is a uniﬁed cache, con-
taining both data and instructions. Typical LLC sizes are
in megabytes and access time is in the order of 40 cycles.
The unit of memory and allocation in a cache is called
cache line. Cache lines are of a ﬁxed size B, which is typ-
ically 64 bytes. The lg(B) low-order bits of the address,
called line oﬀset, are used to locate the datum in the cache
line.
When a memory address is accessed, the processor checks
the availability of the address line in the top-level L1 cache.
If the data is there then it is served to the processor, a
situation referred to as a cache hit. In a cache miss, when
the data is not found in the L1 cache, the processor repeats
the search for the line in the next cache level and continues
through all the caches. Once the line is found, the processor
stores a copy in the cache for future use.
Most caches are set-associative. They are composed of S
cache sets each containing a ﬁxed number of cache lines. The
number of cache lines in a set is the cache associativity, i.e.,
a cache with W lines in each set is a W -way set-associative
cache.
Since the main memory is orders of magnitude larger than
the cache, more than W memory lines may map to the same
cache set. If a cache miss occurs and all the cache lines in
the matching cache set are in use, one of the cached lines
is evicted, freeing a slot for a new line to be fetched from
a lower-level memory. Several cache replacement policies
exist to determine the cache line to evict when a cache miss
occurs but the typical policy in use is an approximation to
the least-recently-used (LRU).
The last-level cache in modern Intel processors is inclusive.
Inclusive caches contain a superset of the contents of the
cache levels above them.
In the case of Intel processors,
the contents of the L1 and L2 caches is also stored in the
last-level cache. A consequence of the inclusion property is
that when data is evicted from the last-level cache it is also
evicted from all of the other levels of cache in the processor.
Intel architecture implements several cache optimizations.
The spatial pre-fetcher pairs cache lines and attempt to fetch
the pair of a missed line [17]. Consecutive accesses to mem-
ory addresses are detected and pre-fetched when the pro-
cessor anticipates they may be required [17]. Additionally,
when the processor is presented with a conditional branch,
speculative execution brings the data of both branches into
the cache before the branch condition is evaluated [35].
Page [30] noted that tracing the sequence of cache hits
and misses of software may leak information on the internal
working of the software, including information that may lead
to recovering cryptographic keys.
This idea was later extended and used for mounting sev-
eral cache-based side-channel attacks [10, 29, 31]. Other
attacks were shown against the L1-instruction cache [3], the
branch prediction buﬀer [1, 2] and the last-level cache [20,
22, 25, 40].
2.2 The Flush+Reload Attack
Our LLC-based attack is based on the Flush+Reload [20,
40] attack, which is a cache-based side-channel attack tech-
nique.
Unlike the earlier Prime+Probe technique [29, 31] that
detects activity in cache sets, the Flush+Reload technique
identiﬁes access to memory lines, giving it a higher resolu-
tion, a high accuracy and high signal-to-noise ratio.
Like Prime+Probe, Flush+Reload relies on cache shar-
ing between processes. Additionally, it requires data shar-
ing, which is typically achieved through the use of shared
libraries or using page de-duplication [6, 36].
A round of the attack, which identiﬁes victim access to
a shared memory line, consists of three phases. (See Algo-
rithm 1.) In the ﬁrst phase the adversary evicts the mon-
itored memory line from the cache.
In the second phase,
the adversary waits a period of time so the victim has an
opportunity to access the memory line. In the third phase,
the adversary measures the time it takes to reload the mem-
ory line. If during the second phase the victim accesses the
memory line, the line will be available in the cache and the
reload operation in the third phase will take a short time. If,
on the other hand, the victim does not access the memory
line then the third phase takes a longer time as the memory
line is loaded from main memory.
Algorithm 1: Flush+Reload Attack
Input: Memory Address addr.
Result: True if the victim accessed the address.
begin
flush(addr )
Wait for the victim.
time ← current_time()
tmp ← read(addr )
readTime ← current_time() - time
return readTime < threshold
The execution of the victim and the adversary processes
are independent of each other, thus synchronization of prob-
ing is important and several factors need to be considered
when processing the side-channel data. Some of those fac-
tors are the waiting period for the adversary between probes,
memory lines to be probed, size of the side-channel trace and
cache-hit threshold. One important goal for this attack is to
achieve the best resolution possible while keeping the error
rate low and one of the ways to achieve this is by targeting
memory lines that occur frequently during execution, such
as loop bodies. Several processor optimizations are in place
during a typical process execution and an attacker must be
aware of these optimizations to ﬁlter them during the anal-
ysis of the attack results. See [4, 39, 40] for discussions of
some of these parameters.
A typical implementation of the Flush+Reload attack
makes use of the clflush instruction of the x86 and x64
instruction sets. The clflush instruction evicts a speciﬁc
memory line from all the cache hierarchy and being an un-
privileged instruction, it can be used by any process.
The inclusiveness of the LLC is essential for the Flush+
Reload attack. Whenever a memory line is evicted from
the LLC, the processor also evicts the line from all of the L1
and L2 caches. On processors that do not have an inclusive
LLC, e.g., AMD processors, the attack does not work [40].
See, however, Lipp et al. [24] for a variant of the technique
that does not require an inclusive LLC.
2.3 The Digital Signature Algorithm (DSA)
A variant of the ElGamal signature scheme, DSA was ﬁrst
proposed by the U.S. National Institute of Standards and
Technology (NIST). DSA uses the multiplicative group of a
ﬁnite ﬁeld. We use the following notation for DSA.
Parameters: Primes p, q such that q divides (p − 1), a gen-
erator g of multiplicative order q in GF (p) and an approved
hash function h (e.g. SHA-1, SHA-256, SHA-512).
Private-Public key pairs: The private key α is an integer
uniformly chosen such that 0 < α < q and the corresponding
public key y is given by y = gα mod p. Calculating the
private key given the public key requires solving the discrete
logarithm problem and for correctly chosen parameters, this
is an intractable problem.
Signing: A given party, Alice, wants to send a signed mes-
sage m to Bob—the message m is not necessarily encrypted.
Using her private-public key pair (αA, yA), Alice performs
the following steps:
1. Select uniformly at random a secret nonce k such that
0 < k < q.
2. Compute r = (gk mod p) mod q and h(m).
3. Compute s = k−1(h(m) + αAr) mod q.
4. Alice sends (m, r, s) to Bob.
Verifying: Bob wants to be sure the message he received
comes from Alice—a valid DSA signature gives strong evi-
dence of authenticity. Bob performs the following steps to
verify the signature:
1. Reject the signature if it does not satisfy 0 < r < q
and 0 < s < q.
2. Compute w = s−1 mod q and h(m).
3. Compute u1 = h(m)w mod q and u2 = rw mod q.
4. Compute v = (gu1 yu2
A mod p) mod q.
5. Accept the signature if and only if v = r holds.
2.3.1 DSA in Practice
Putting it mildly, there is no consensus on key sizes, and
furthermore keys seen in the wild and used in ubiquitous
protocols have varying sizes—sometimes dictated by existing
and deployed standards. For example, NIST deﬁnes 1024-bit
p with 160-bit q as “legacy-use” and 2048-bit p with 256-bit
q as “acceptable” [8]. We focus on these two parameter sets.
SSH’s Transport Layer Protocol1 lists DSA key type ssh-
dss as “required” and deﬁnes r and s as 160-bit integers,
implying 160-bit q. In fact the OpenSSH tool ssh-keygen
defaults to 160-bit q and 1024-bit p for these key types, not
allowing the user to override that option, and using the same
parameters to generate the server’s host key.
It is worth
noting that recently as of version 7.0, OpenSSH disables
host server DSA keys by a conﬁgurable default option2, but
of course this does not aﬀect already deployed solutions.
As a countermeasure to previous timing attacks, Open-
SSL’s DSA implementation pads nonces by adding either q
or 2q to k—see details in Section 3.
For the DSA signing algorithm, Step 2 is the performance
bottleneck and the exponentiation algorithm used will prove