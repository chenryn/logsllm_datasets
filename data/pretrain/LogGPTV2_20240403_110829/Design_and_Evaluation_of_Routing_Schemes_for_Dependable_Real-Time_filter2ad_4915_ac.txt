k E N B ;  
f
+ 1. 
,
~
The minimum  hop  count  can be  calculated  using  the 
Dijkstra’s  algorithm  or  the  Bellman-Ford  distance- 
vector algorithm. The distance tables are updated only 
upon change of  the network topology. 
To  establish  a DR-connection  from  the  source i  to 
the  destination  j, the  source  initiates  the  bounded 
flooding of  a CDP, which  contains the following fields: 
P3 
Figure 3. Backup route selection for D3 
4.  Routing with Bounded  Flooding 
Link-state routing is easy to implement, but the ex- 
tended  link-state  packet  requires  a  larger  packet  size 
and introduces additional routing traffic.  To deal with 
this problem, we propose a different on-demand routing 
scheme  based  on  bounded  flooding,  which  was  origi- 
nally  proposed  by  Kweon  and  Shin  [9]  for  QoS  (not 
DOS) routing. 
Suppose  a destination (client) node  requests  a DR- 
connection service from the source  (server) node,  and 
specifies its  QoS requirement  by  indicating  the  mini- 
mum desired link bandwidth of  the connection.  In or- 
der  to establish  the  DR-connection,  the  source  node 
floods  a  special  channel-discovery  packet  (CDP)  to- 
wards  the  destination.  To  reduce  the  overhead,  the 
source node  limits the number  of  hops  each  CDP  can 
take before reaching the destination.  That is, when  an 
intermediate node  receives a CDP, it will  forward the 
packet to its neighbors only if  the minimum-hop  route 
via  that  neighbor  can  lead  the  CDP  to  the  destina- 
tion  within  the source-specified  hop-count  limit.  This 
scheme can be  viewed  as  bounded  flooding.  The des- 
tination node is responsible  for  selecting the best  pri- 
mary  and  backup  routes for  the  real-time  connection 
based  on  the  flooded  information.  Before proceeding 
with the proposed algorithm, we introduce the relevant 
data structures. 
4.1. Data Structures and Notation 
Each network node maintains a distance table (DT). 
Let  NBi  denote  the  set  of  node  i’s  neighbors.  Hop 
count  is  used  to  build  distance  tables,  although  any 
bandwidth 
srce-id  (dest-id): an integer which uniquely iden- 
tifies  the source (destination) node. 
conn-id:  a number  that  uniquely  identifies a DR- 
connection. 
hc-limit:  maximum  hop count  that the CDP can 
take before reaching the destination. 
hc-curr:  hop  count  of  the  route  taken  so  far  by 
the CDP to reach the current node. 
bw-req: 
connection. 
primary- f lag: one bit flag to indicate if  the route 
traversed  so far  by  the CDP can be  used  for  pri- 
It  is  1 if  total-bw  - (prime-bw + 
mary  route. 
spare-bur)  is larger than  the bur-req; 0 otherwise. 
list of  nodes  that  the  CDP has  traversed  so far. 
Every  time  the  CDP  is  forwarded,  the  current 
node  is  appended  to  list.  This  information  is 
needed for the destination to select the best routes 
for  the  primary  and  backup  channels  of  the  con- 
nection, and to guarantee loop-free flooding. 
the  DR- 
requested 
for 
The flooding bound of the CDP is specified by hc-limit, 
which is equal to p x Dj + p ,  where p  2 1 and p 2 0. In 
order to improve the chance of  granting the requested 
DR-connection, multiple routes must be given opportu- 
nities to run the connection over them.  Therefore, the 
values  of  p  and p  are determined  by  making  a trade- 
off  between  the routing overhead and the connection- 
acceptance probability. 
Each  node  maintains  a  “transient”  table,  Pending 
Connection  Table  (PCT). Each entry of  a P C T  repre- 
sents a connection  request  passed  through  this  node, 
and consists of  four fields: 
conn-id:  the connection identifier. 
bur-req:  the requested  bandwidth. 
290 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:00:49 UTC from IEEE Xplore.  Restrictions apply. 
0 
0 
min-dist: hop count of  the shortest route taken by 
CDPs to the current node. 
time-out  a real  number  which  specifies  this  en- 
try’s time  to live.  Upon  expiration  of  the timer, 
this  entry  is  no  longer  valid  and  thus  deleted 
from the PCT. In order to prevent false deletion, 
time-out must be no less than the average link de- 
lay multiplied by  the hop-count  limit. 
Besides PCT, each node maintains a set of candidate 
route tables (CRTs), one for each outstanding connec- 
tion  request  destined  for  this  node.  The function  of 
these  tables  is to allow  the  destination  to choose  the 
best  primary  and  backup  routes  among  those  routes 
which the CDPs have safely traversed to reach the des- 
tination.  Each  entry of  a CRT  represents one candi- 
date route  for  the  corresponding connection request, 
and contains primary-Slay,  hop-count  and list. 
4.2. Action by the Source Node 
The destination node initiates a connection request 
and uni-casts the request message to the source node. 
Upon receiving the request message, the source node i 
composes  a CDP m, and performs the following tests 
for each of  its neighbors k  E NBi: 
Distance  test: 
Bandwidth test: 
bw-req(m) 5 total-bw(i, k )  - prime-bw(i, k ) .  
(9) 
If node  k  passes  both  tests,  node  i  updates  and  for- 
wards the packet to node k .  The CDP is updated by re- 
calculating  primary-flag(m), increasing  hc-curr(m) 
by one, and appending i to list(m). 
4.3. Action by an Intermediate Node 
Upon  receiving a CDP, m, node i performs the fol- 
lowing tests for each of  its neighbors  k  E NBi: 
Distance  test: 
hc-curr(m) + D6est-id,k + 1 5 hc-Zimit(m). 
Loop-freedom test: 
k 4 Zist(m). 
Bandwidth  test: 
bw-req(m) 5 total-bw(i, k )  - prime-bw(i, k ) .  
If  node i has already received  at least one copy of  the 
CDP for the same DR-connection, node i performs an 
additional test on the incoming CDP before executing 
the above three tests: 
Valid-detour test: 
hc-curr(m) 5 (Y  x  min-dist(conn-id(m)) + p. 
By  using  this  additional  “valid-detour test,”  where  (Y 
and ,B  are two pre-determined  parameters,  we  further 
reduce the number of  CDPs.  If  node k  passes all these 
tests, node i updates and forwards the packet  to node 
k  and updates its PCT by  adding a new entry. 
(13) 
4.4. Action by the Destination Node 
When  the destination node i  receives  the  CDP, m, 
node i checks if conn-id(m) appears in one of its CRTs. 
If  yes, node i updates the CRT by  filling a new  entry 
based  on the information  provided  in  this  CDP.  0th- 
erwise,  node i  creates  a new  CRT for  this  connection 
request, and sets a timer which is no less than the aver- 
age link delay multiplied by the hop-count limit.  Upon 
expiration  of  the timer,  any  outstanding  CDP  corre- 
sponding to this  connection request is  no longer  valid 
and  has  been  discarded  by  some  intermediate  node, 
then node i starts the route selection process and then 
the route confirmation process. 
Among  all  the  candidate  routes  listed  in  a  CRT, 
only  those  with  primary-flag  = 1 might  be  selected 
as  the primary  channel  route.  The destination  node 
chooses the shortest route (i.e., the one with the small- 
est  hop-count  value)  to be the primary channel  route. 
All the remaining candidate routes in the CRT are eligi- 
ble to be the backup channel route, and the destination 
node chooses the shortest one that minimally  overlaps 
with the primary channel route.  The destination node 
starts the route confirmation process for both primary 
and backup channels simultaneously. 
5. Backup Multiplexing 
A  node’s  attempt  to  choose  a  backup  route  with- 
out  any  conflict  may  not  always  be  successful.  It  is 
therefore  possible  to  activate  more  than  one  backup 
when  a link  failure  occur.  For  example, let APLVl  = 
(0,1,2,1,2).  Then,  if  LJ  or  Ls  fails,  two  DR- 
connections  will  attempt  to  activate  their  backups 
through L1.  If  the spare resources  reserved on L1  can 
accommodate only one of the two DR-connections, one 
of  the two  will  fail  to activate its  backup.  To handle 
the case, the DR-connection  manager  responsible  for 
Li  has  to reserve  more  spare resources.  The conflict- 
ing backups  are not  multiplexed over  the  same spare 
resources. 
291 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:00:49 UTC from IEEE Xplore.  Restrictions apply. 
The  DR-connection  manager  for  a  link  checks  if 
more  spare  resources  need  to  be  reserved  using  the 
APLV  and  SC  of  the  link.  Since  all  DR-connections 
are assumed  to  require  an  identical  amount of  band- 
width,  SC,  can  be  calculated  by  dividing  the  total 
spare bandwidth  reserved  on  L, by  the bandwidth  of 
a  DR-connection.  If  any  element  of  APLV, is  larger 
than  SC,,  at least  two  conflicting backups  are multi- 
plexed  on the same spare resources.  In  this  case, it is 
necessary to reserve more spare resources. 
When  a  node  receives  a  backup-setup  request, 
the  DR-connection  manager  of  the  node  updates  the 
APLV  of  the link that  the backup traverses using the 
LSET included in the request of the corresponding pri- 
mary.  Using the new APLV, the DR-connection man- 
ager can determine if  it will multiplex  the new  backup 
on the current spare resources or if  it will reserve more 
resources. 
A  DR-connection  manager  may  not  be  able to in- 
crease spare resources due to the shortage of  resources, 
even  when  the  new  backup  has  conflicts  with  other 
backups.  In  such  a  case,  we  have  two  choices:  (1) 
reject  the  request,  or  (2)  multiplex  the  new  backup 
on  the  previously-reserved  spare resources  with  other 
backups  that  the new  backup  has  conflicts  with.  We 
opt to take the second approach. Although  multiplex- 
ing conflicting backups degrades fault-tolerance, there 
is a chance that one of  the conflicting backups  may be 
rejected,  or  one of  the primary  channels  on  the same 
link may terminate before a link failure that will trigger 
activation of  conflicting backups.  If  a primary  channel 
is released, its resources will be returned to the pool of 
free resources, and the DR-connection  managers assign 
these free resources to spare resources. 
6.  Simulation and Analysis 
We  have  conducted  an  in-depth  simulation  study 
to comparatively evaluate  the three proposed  routing 
schemes  in  terms  of  fault-tolerance  and  overhead.  In 
this study, we measured the probability of  successfully 
establishing a  DR-connection,  and the fault-tolerance 
of  established  connections  under  various  load  condi- 
tions  and  network  configurations.  We  also  evaluated 
the overhead of  discovering backup routes. 
6.1. The Simulation Model 
To  evaluate the performance  of  the proposed  rout- 
ing schemes under different network configurations, we 
selected networks with 60 nodes and the average node- 
degrees ( E )  of  3 and 4. The networks are generated by 
using the Waxman topology generator  [ll]. Each node 
acts  as  a  router  or  switch,  and  links  are assumed  to 
be  hi-directional, with  an identical  bandwidth  capac- 
ity  (C) in both directions. 
parameters 
1  value 
Table 1. The simulation parameters 
The simulation study uses two traffic patterns.  One, 
called  UT, is  uniform  random selection of  source and 
destination  nodes.  The  other,  NT,  is  random  pre- 
selection  of  10 nodes  as  destinations for  50% of  DR- 
connections.  For  simplicity,  we  assume  that  DR- 
connection  requests  arrive  as  a  Poisson  process  with 
rate  A.  Instead of  using  more  realistic traffic  models, 
we  only  consider  simple  traffic patterns,  because  our 
goal  is  to  comparatively  evaluate  the  proposed  rout- 
ing schemes, as opposed  to providing absolute  perfor- 
mance figures.  Moreover, we assume that each connec- 
tion  requires  a constant  bandwidth  (bw-req)  and  has 
a uniformly-distributed lifetime,  t-req, between  20 and 