100.0
100.0
100.0
Table 5: Hybrid time/direction perturbations on Var-CNN [3].
α,µ,σ,
20, 0, 5
100, 0, 10
500, 0, 20
1000, 0, 30
2000, 0, 50
BW Overhead (%) A:
0.04
2.04
11.11
25.0
66.66
SU-DU (%) Max ST-DU (#, %) Min ST-DU (#, %) Max SU-DT (#, %) Min SU-DT (#, %) Max ST-DT (#←#, %) Min ST-DT (#←#, %)
79.0
83.9
97.0
98.6
99.0
−,100.0
−,100.0
−,100.0
−,100.0
−,100.0
−,100.0
−,100.0
−,100.0
−,100.0
−,100.0
2,69.4
2,92.8
3,99.9
−,100.0
−,100.0
4,30.0
−,100.0
−,100.0
−,100.0
−,100.0
−,0
−,10.0
−,20.0
−,30.0
−,30.0
6,40.3
3,72.3
4,92.6
0,96.7
9,97.7
Algorithm 6 Our tailored adversarial defense
Figure 8: The accuracy
of DeepCorr with different
blind adversarial noises
Figure 9: The l2 distance be-
tween DeepCorr’s different
adversarial noises
Table 6: Evaluating various defenses against blind adversarial
perturbations (website ﬁngerprinting application)
Adversary Strength Original No Def Madry et al. [34]
α = 20
α = 100
α = 500
92%
92%
92%
60%
28%
8%
84%
48%
19%
IGR [48] RC [7] Our Defense
62%
23%
2%
84%
60%
24%
54%
23%
7%
Table 7: Evaluating various defenses against blind adversarial
perturbations (ﬂow correlation application). FP=10−4.
Adversary Strength Original No Def Madry et al. [34]
µ = 0,σ = 10
µ = 0,σ = 50
µ = 0,σ = 100
79%
79%
79%
63%
21%
13%
70%
25%
18%
IGR [48] RC [7] Our Defense
62%
23%
13%
74%
32%
23%
63%
22%
14%
proach in which the defender uses adversarial perturbations
crafted by our attack to make the target model robust against
the attacks. We assume the defender knows the objective
function and its parameters. We evaluate our defense when
the defender does not know if the attack is targeted or untar-
geted (for both source and destination). The defender trains
the model for one epoch, and then generates blind adversarial
perturbations from all possible settings using Algorithm 1.
Then, he extends the training dataset by including all of the
Randomly initialize network N
L f ← target model loss function
M ← domain remapping function
R ← domain regularizations function
G(z) ← initialize the blind adversarial perturbation model parameters (θG)
T ← epochs
Z ← []
for epoch t ∈ {1···T} do
// List of adversarial perturbations
Train the model N for one epoch on training dataset Dtr
Z ← generate adversarial perturbations using Algorithm 1 from all
possible targets and focus classes
end for
Dtr.extend(Dtr + Z)
return N
adversarial samples generated by the adversary and trains the
target model on the augmented train dataset. Algorithm 6
sketches our defense algorithm.
Comparing our defense vs. prior defenses: We compare
our defense with previous defenses borrowed from the image
classiﬁcation literature. Tables 6 and 7 compare the perfor-
mances of different defenses on DF and DeepCorr scenar-
ios, respectively. As we see, none of the prior defenses for
adversarial examples are robust against our blind adver-
sarial attacks, and in some cases, utilizing them even im-
proves the accuracy of the attack. However, the results show
that our tailored defense is more robust than prior defenses.
Since the attacker knows the exact attack mechanism, all de-
fense methods cannot perform well when the adversary uses
higher strengths in crafting adversarial perturbations. While
our defense is more robust against blind adversarial attacks,
it increases the training time of the target model by orders
of magnitude which makes it not scalable for larger models.
Therefore, designing efﬁcient defenses against blind adversar-
ial perturbations is an important future work.
2718    30th USENIX Security Symposium
USENIX Association
0.420.460.500.540.580.62True positive0.000.020.040.060.08Fraction2004006008001000l2 distance0100200300400500CountTable 8: Transferability of direction-
based perturbations (surrogate model:
DF [50], original model: [47])
Table 9: Transferability of timing per-
turbations (surrogate model: AlexNet,
original model: DeepCorr [37])
Table 10: Transferability of size per-
turbations: (surrogate model: AlexNet,
original model: DeepCorr [37])
Adversary Strength Transferability (%)
α = 100
α = 500
α = 1000
30.65
85.90
96.53
Adversary Strength Transferability (%)
µ = 0,σ = 20
µ = 20,σ = 20
µ = 50,σ = 20
46.24
76.14
88.51
Adversary Strength Transferability (%)
N = 10
N = 20
N = 50
75.32
83.11
90.24
9 Transferability
An adversarial perturbation scheme is called transferable
if the perturbations it creates for a target model can evade
other models as well. A transferable perturbation algorithm is
much more practical, as the adversary will not need to have
a whitebox access to its target model; instead, the adversary
will be able to use a surrogate (whitebox) model to craft its
adversarial perturbations, and then apply them to the original
blackbox target model.
In this section, we evaluate the transferability of our blind
adversarial perturbation technique. First, we train a surro-
gate model for our trafﬁc analysis application. Note that, the
original and surrogate models do not need to have the same
architecture, but they are trained for the same task (likely with
difference classiﬁcation accuracies). Next, we create a pertur-
bation generation function G(z) for our surrogate model (as
described before). We use this G(z) to generate perturbations,
and apply these perturbations on some sample ﬂows. Finally,
we feed the resulted perturbed ﬂows as inputs to the original
model (i.e., the target blackbox model) of the trafﬁc analysis
application. We measure transferability using a common met-
ric from [42]: we identify the input ﬂows that are correctly
classiﬁed by both original and surrogate models before ap-
plying the blind adversarial perturbation; then, among these
samples, we return the ratio of samples misclassiﬁed by the
original model over the samples misclassiﬁed by the surrogate
model as our transferability metric.
Direction-based technique To evaluate the transferability of
our direction-based perturbations, we use the DF system [50]
as the surrogate model and the WF system of Rimmer et
al. [47] as the original model. Note that the model proposed
by Rimmer et al. uses CNNs, however, it has a completely
different structure than DF. We train both models on DF’s
dataset [50], and generate blind adversarial perturbations for
the surrogate DF model. Then we test the original model
using these perturbations. Table 8 shows the transferability of
our direction-based attack with different noise strengths. As
can be seen, our direction-based attack is highly transferable.
Time-based technique For the transferability of the time-
based attack, we use DeepCorr [37] as the original model. We
use AlexNet [30] as the surrogate model, which has a com-
pletely different architecture. We train AlexNet on the same
dataset used by DeepCorr. Since the main task of AlexNet is
image classiﬁcation, we modify its hyper-parameters slightly
to make it compatible with the DeepCorr dataset. To calculate
transferability, we ﬁx the false positive rates of both surrogate
and original models to the same values (by choosing the right
ﬂow correlation thresholds). Table 9 shows high degrees of
transferability for the time-based attack with different blind
noise strengths (for a constant false positive rate of 10−4).
Size-based technique To evaluate the transferability of the
size-based perturbations, we use DeepCorr as the original
model and AlexNet as the surrogate model, and calculate
transferability as before. Table 10 shows the transferability of
the size-based technique with different blind noise strengths
for a false positive rate of 10−4.
To summarize, we show that blind adversarial pertur-
bations are highly transferable between different model
architectures, enabling their use by blackbox adversaries.
10 Limitations and Future Directions
As mentioned earlier, this work is focused on defeating DNN-
based trafﬁc analysis techniques that use raw trafﬁc features,
e.g., packet timing, sizes, and directions; this includes a large
corpora of prior trafﬁc analysis techniques implemented for
different scenarios [2, 3, 6, 37, 38, 53, 60, 60, 61]. However,
our attack can not be directly applied to content-based trafﬁc
analysis techniques (e.g., signature-based malware detection
algorithms), nor can it be applied trivially on trafﬁc analysis
techniques that use non-differentiable, irreversible functions
of trafﬁc features, e.g., hashes of the timestamps. Future work
can extend blind adversarial perturbations to such trafﬁc anal-
ysis techniques by fabricating tailored remapping functions
or approximating gradient functions.
Additionally, note that our use of adversarial perturbations
aim at defending “DNN-based” trafﬁc analysis mechanisms
only. Non-DNN trafﬁc analysis techniques, in particular ﬂow
watermarking techniques [23–25] and volume-based trafﬁc
classiﬁers [4], can not be protected by our defense. Future
work can look into combining defenses against such non-
DNN mechanisms with our defense.
To keep our adversarial perturbation process hidden from
the adversary, our perturbation generator function enforces
various constraints to make the perturbed connections seman-
tically and statistically indistinguishable from benign connec-
USENIX Association
30th USENIX Security Symposium    2719
tions. To enforce semantic indistinguishability, the perturber
needs to be aware of the semantics of the underlying network
protocol, e.g., it needs to know the format of Tor packets. To
enforce statistical indistinguishability, the perturber needs to
measure some statistical properties of the target trafﬁc, e.g.,
the network jitter of Tor trafﬁc. The lack of such information
to the perturbation entity will reduce the performance of our
technique (note that this is not an issue in the applications
evaluated in our work).
11 Conclusions
In this paper, we introduced blind adversarial perturbations, a
mechanism to defeat DNN-based trafﬁc analysis classiﬁers
which works by perturbing the features of live network con-
nections. We presented a systematic approach to generate
blind adversarial perturbations through solving speciﬁc op-