6
3
95
96
97
98
99
100
23
54
85
33
5
86
87
88
89
90
3
10
29
91
1
31
54
109 – 110
12 – 16
1 – 3
110 – 111
3 – 7
8 – 12
2 – 25
111 – 112
6 – 23
6 – 22
33 – 38
1 – 92
1 – 4
21 – 30
44 – 70
JtR Rule
-s x**
-c (?a c Q
-c l Q
-s-c x** /?u l
>6 ’6
>7 ’7 l
-c >6 ’6 /?u l
>5 ’5
/?d @?d >4
/?d @?d M @?A Q >4
/?d @?d >4 M [lc] Q
/?d @?d M @?A Q >4 M [lc] Q
@?D Q >4
/?d @?d >3 3 <* [lc] Q $[0-9] Q
A. Approach
We reorder rules in descending order of success density,
deﬁned as the ratio of a rule’s successful guesses (those
matching a password in evaluation set S) to the total number of
guesses. To avoid prioritizing rules whose successful guesses
overlap with those of a previously prioritized rule, we reorder
iteratively. We assume attackers ﬁrst guess all items in the
wordlist verbatim, which is often the best strategy [8]. We
then calculate the success density for each rule against the
evaluation set S, placing the rule with the highest success
density next. We remove all passwords guessed by that rule
from S, recalculating the success density for all remaining
rules. We repeat this process until all rules have been ordered.
In the case of ties, we prioritize the rule that made fewer
guesses. If the guesses made by rules are fully disjoint, this
strategy is provably optimal in maximizing the area under
the guessing curve. If the guesses are not disjoint, one can
construct pathological cases where this strategy is not optimal,
yet it is far more computationally tractable than alternatives.
(cid:20)(cid:25)(cid:26)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:40:17 UTC from IEEE Xplore.  Restrictions apply. 
d
e
s
s
e
u
g
t
n
e
c
r
e
P
BField(selfoptimized)
Clixsense
Brazzers
000webhost
Neopets
CSDN
OriginalOrdering
80%
70%
60%
50%
40%
107
109
1011
Guesses
1013
Fig. 1: The guessability of Battleﬁeld Heroes under JtR using
the PGS wordlist and the SpiderLabs rule list in its original
order, reordered artiﬁcially based on itself (BField), and re-
ordered based on each of the ﬁve other evaluation sets. Each
reordering substantially improved on the original ordering.
B. Evaluation Procedure and Results
Using each of our three wordlists, we applied this approach
to reorder each of the three JtR rule lists using each of the six
evaluation sets in turn. Comparing rules’ original positions to
their reordered position, we found a number of rules whose
positions after reordering diverged consistently from their
original position in these widely distributed rule lists.
As Table V demonstrates, our reordering process consis-
tently suggests that some rules that appear early in the 145-
rule John list likely belong late in the list, and vice versa. For
example, three of the ﬁrst ten rules never appear earlier than
the 85th position after reordering based on success density
for any of our six evaluation sets. Notably, John is widely
distributed in this non-optimal form as JtR’s default rule list.
We found trends that were similar, if not more stark, for the
other rule lists. For example, in the 5,146-rule SpiderLabs rule
list, none of the ﬁrst 23 rules still appear within the top 100
rules after reordering based on any of our six evaluation sets.
This is particularly notable because SpiderLabs was already
manually reordered based on a password-cracking expert’s
intuition [18]. Appendix E shows the original and reordered
positions of the ﬁrst 100 rules for two of the JtR rule lists.
Across the three rule lists, we observed that using any of our
four English-language evaluation sets with identical password-
composition policies – Battleﬁeld Heroes, Brazzers, Clixsense,
and Neopets – resulted in rule reorderings that were similar
to each other. We therefore group these four sets together in
our tables. The reorderings tended to be distinct, however, for
both the 000webhost set, whose composition policy required
a digit, and the Chinese-language CSDN set.
Crucially, we found that our reordering procedure enables
many passwords to be guessed much earlier in an attack for
both the SpiderLabs and John rule lists. Figure 1 shows the
guessability of the Battleﬁeld Heroes evaluation set using the
SpiderLabs rules in their original order (black line), artiﬁcially
reordered on itself as an upper bound (dashed red line), and
reordered based on each of the ﬁve other evaluation sets.
Reordering the SpiderLabs rule list based on any of the
four other English-language sets provides substantial improve-
ments in guessing. While rule reordering does not change
which passwords are guessed, a substantially larger fraction
of passwords are guessed earlier in the attack compared to
the original ordering. Notably, reordering based on any of the
other four English-language sets results in guessing success
approaching the artiﬁcial self-optimized ordering, highlighting
that reorderings generalize well across sets. While reordering
based on the Chinese-language CSDN set also provides sub-
stantial improvements over the original ordering, it performs
less well than any of the English-language sets. We observed
very similar trends for each of the ﬁve other evaluation sets.
We also observed similar trends reordering the much smaller
John rule list for all six evaluation sets. For the Megatron
rule list, however, we observed a different trend. As shown in
Figure 4a in Appendix E, the original ordering of the Megatron
rule list (black) is already relatively close to the artiﬁcial
self-optimized reordering (red). Guessing Battleﬁeld Heroes
passwords using any of the four other English-language eval-
uation sets results in an attack that performs very similarly to
the original ordering, while reordering based on the Chinese-
language CSDN set results in a less effective attack. While
Megatron’s original ordering already seems near optimal, our
approach let us demonstrate this scientiﬁcally.
IX. OPTIMIZATION 2: ORDERING WORDLISTS
Given the performance beneﬁts we observed reordering rule
lists for JtR, which guesses in rule-major order, we then
used our analytical tools to reorder wordlists for Hashcat,
which guesses in word-major order. We had hoped reordering
wordlists based on evaluation sets would improve guessing
performance on other sets, yet instead found this process to
worsen performance. Wordlists are typically already ordered
in descending frequency based on myriad prior password
leaks, and our data-driven optimization seemed to overﬁt. We
nonetheless describe our process because it might prove more
effective if extremely large evaluation sets were used.
A. Approach
We begin with a wordlist wlist containing words w. We
term wlist in its initial order wlistoriginal. Given an
evaluation set S, we run invert_rule on each password
pw in S to identify which passwords would be guessed, and
by which words. We split the wordlist wlist in two: one
wordlist, wlistsuccess, containing words wi that would guess
at least one password pw in S, and wlistf ailure, containing
the remaining words. We rearrange wlistsuccess in descend-
ing order of the number of passwords in S that each would
guess, breaking ties arbitrarily. We append wlistf ailure,
maintaining the order from the original wlist. This combined
wordlist, optimized on evaluation set S, is termed wlistS.
B. Evaluation Procedure and Results
We used this approach to reorder both the XATO wordlist,
which contains only passwords, and the PGS wordlist, which
contains passwords followed by natural language dictionaries.
We did so for each of the six evaluation sets. Compared
(cid:20)(cid:26)(cid:17)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:40:17 UTC from IEEE Xplore.  Restrictions apply. 
to the original, reordering a wordlist based on one evalua-
tion set decreased guessing performance substantially for all
other evaluation sets (Figure 4b in Appendix E). Data-driven
wordlist reordering for Hashcat appears to overﬁt and would
not be recommended, at least for small evaluation sets.
X. OPTIMIZATION 3: RULE LIST COMPLETENESS
Some prior work has automatically generated new rules
and then enumerated their guesses to test their effectiveness
cracking evaluation sets [38], [39]. We similarly generate new
rules, yet use our analytical tools to reason efﬁciently about
their effectiveness. Adding these potentially “missing” rules
enable more passwords to be guessed, as well as more quickly.
A. Approach
The space of possible rules is huge. By randomly or com-
prehensively generating rules, we extend a wordlist wlist
with rules it does not already contain. We then reorder this
extended list based on other evaluation sets as in Section VIII.
B. Evaluation Procedure and Results
Using each of our three wordlists in turn with the JtR
SpiderLabs rule list, we tested on the four English-language
evaluation sets with identical composition policies. First, we
reordered SpiderLabs based on the three other evaluation
sets (termed reordered). We then generated the 15,085 JtR
rules that consist of a single transformation and are both
invertible and countable. Adding those to the SpiderLabs rule
list (extended), we followed the same reordering procedure
and cut off guessing at the previous number of guesses.
This procedure identiﬁed rules that are both new and effec-
tive. Figure 2 shows the guessability of both Battleﬁeld Heroes
and Brazzers. Compared to the original ordering (light colors),
reordering based on other evaluation sets (mid colors) leads to
passwords being guessed more quickly, echoing Section VIII.
However, extending SpiderLabs and then reordering it (dark
colors) leads to passwords being guessed even more quickly,
as well as previously unguessed passwords being guessed.
Results were similar across the evaluation sets.
We then analyzed the new rules. Cutting off at the same
number of guesses as the original SpiderLabs with Battleﬁeld
Heroes as the evaluation set, we observed 3,495 new rules
having been executed in the extended attack. While the original
SpiderLabs contained only 5,146 rules, many of them utilized
JtR’s rule preprocessor to make a large number of guesses in
a single rule (e.g., appending a digit). We found that 178 of
the newly identiﬁed rules were strict subsets of an existing
SpiderLabs rule (e.g., appending a speciﬁc digit) that had a
higher success density than the superset rule. Another 115 new
rules were either contained verbatim in John or Megatron, or
they were strict subsets of a rule in those lists. The remaining
3,202 rules were completely new to our three JtR rule lists.
XI. OPTIMIZATION 4: WORDLIST COMPLETENESS
Our invert_rule process moves backwards from pass-
words to the preimages that, when transformed, guess that
d
e
s
s
e
u
g
t
n
e
c
r
e
P
90%
80%
70%
60%
50%
40%
Brazzers:Extended
Brazzers:Reordered
Brazzers:Original
BField:Extended
BField:Reordered
BField:Original
107
109
1011
Guesses
1013
Fig. 2: The guessability of Battleﬁeld Heroes and Brazzers
using the PGS wordlist and the JtR SpiderLabs rule list in its
original order (light), reordered based on the other sets (mid),
and extended with “missing” rules and then reordered (dark).
password. We modiﬁed this process to identify what we term
missing words, or words that perhaps should have been in the
wordlist based on a given evaluation set. The intuition is to
leverage “cache misses” to improve wordlist completeness.
A. Approach
Given an evaluation set S, a wordlist wlist, and a (re-
ordered) rule list rlist, we use invert_rule to invert
each password pw ∈ S to generate preimages pi. Each
preimage not in the wordlist (i.e., pi /∈ wlist) is a potential
missing word. To identify preimages likely to generalize, for
each unique password pw ∈ S, we assign a credit c ∈ [0, 1] to
each potential preimage pi /∈ wlist inversely proportional to
the rule’s position in rlist. A preimage identiﬁed with the
ﬁrst rule in rlist will receive c = 1, while one identiﬁed
with the middle rule will receive c = 0.5. This approach
prioritizes preimages used early in an attack. For a particular
password, credit for a particular preimage is given only once.
After following this process for all unique passwords in set
S, we rank preimages in descending order of credit summed
across passwords, keeping those above a threshold.
B. Evaluation Procedure and Results
We follow this procedure for all six evaluation sets using
the fully invertible SpiderLabs rule list and both the LinkedIn
and PGS wordlists. We use the rule ordering self-optimized for
each evaluation set (Section VIII). To emphasize new cracks,
we use only the passwords in each evaluation set that would
not otherwise be guessed by a given wordlist and rule list.
Table VI presents a manual thematic categorization of the
100 preimages with the highest credit for the 000webhost,
Battleﬁeld Heroes, and Neopets evaluation sets. This process
produces site-speciﬁc words (e.g., “bfheroes”), meaningful
strings unrelated to the site, and short (2–3 character) strings.
To understand whether this procedure results in more ef-
fective guessing in realistic scenarios (i.e., testing on sets
not used for optimization), we ﬁrst used the four English-
language evaluation sets with identical password policies to
identify words potentially missing from the PGS wordlist,
which includes both passwords and natural language. To test
the impact on guessing, we used a random sample of 500,000
(cid:20)(cid:26)(cid:18)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:40:17 UTC from IEEE Xplore.  Restrictions apply. 
TABLE VI: A manual categorization of the top 100 preimages
identiﬁed as potentially “missing” from the PGS wordlist.
Category
Set-speciﬁc
Meaningful
Short strings
Unidentiﬁed
bfheroes; ilovmyneopets”””
la la la; Son-gouku; MaSterBrain
a2; a23; 7a; b2; q2; 2k;
gawabint1; kur0=ud1
000webhost
Examples
7
73
6
14
3
68
3
26
9
34
18
39
Neopets
BField
passwords from each of the other three sets to generate the
top million words “missing” from PGS. We modeled an attack
using SpiderLabs rules and the missing words as the wordlist.
In each case, this attack made 1.7 × 1013 extra guesses,
successfully guessing 221 Clixsense passwords, 157 Battle-
ﬁeld Heroes passwords, 128 Brazzers passwords, and 118
Neopets passwords from our 25,000-password evaluation sets.
None of these passwords would have been guessed otherwise
by SpiderLabs. While the success density of such attacks is
low, they are appropriate at the end of an attack when high-
probability guesses have been exhausted. For comparison, the
ﬁnal 1.7×1013 guesses with the PGS wordlist and SpiderLabs
rule list (reordered on the three other sets combined) resulted
in zero successful guesses for any of the four test sets.
XII. COMPARISONS TO EXISTING ALGORITHMS / METERS
Our analytical techniques enable two primary applications:
proactive password checking and data-driven conﬁguration
(improvement) of transformation-based attacks. Here, we an-
alyze these applications relative to prior approaches.
First, our guess-number calculator enables real-time pass-
word checking, which is effectively a server-side password
meter. Here, we highlight two experiments comparing our
approach to meters using combinatoric estimates (zxcvbn) [22]
and Neural Networks [13]. Appendix C expands on both.
Following best practices in comparing meters [53], we
examined how meters’ guess numbers for a given password
were correlated with the number of times that password
appeared in an evaluation set. As shown in Table VII, our
analytical JtR and Hashcat approaches are better correlated
with the frequency counts than existing meters. Correlations
approaching 1 indicate better alignment with frequency counts.
For example, for the Brazzers set, JtR had a correlation of
0.734 and Hashcat had a correlation of 0.731, compared to
0.693 and 0.702 for zxcvbn and Neural Networks, respectively.
However, while existing meters estimate a guess number for
every password, our approach assigns the same large guess
number to any passwords unguessed by JtR or Hashcat.
Unlike any prior meter, ours is the ﬁrst to provide real-
time models of guessing attacks widely used in the wild.
To evaluate whether existing meters already fully captured
these attacks, we examined whether those meters made unsafe
errors, rating guessable passwords as strong. Reﬂecting real
attacks, we rated the 25% of each password set with the
lowest guess numbers (guessed ﬁrst) for Hashcat and JtR as
practically weak. As shown in Table VIII, all meters rated
at least some practically weak passwords among the 25%
of hardest-to-guess passwords. While this represents only a
TABLE VII: Coverage (%) and accuracy (rw, weighted Spear-
man correlation) of our approach and existing meters.
Meter
000webhost
Brazzers
Neopets
Hashcat (Sec. VII)
JtR: Extended (Sec. X)
Neural Network [13]
zxcvbn [22]
%
40.1
40.3
100.0
100.0
rw
0.512
0.515
0.507
0.437
%
83.3
79.4
100.0
100.0
rw
0.731
0.734
0.702
0.693
%
77.8
76.4
100.0
100.0