# Optimizing Password Guessing Strategies

## 1. Introduction
This document discusses various optimization techniques for improving the efficiency of password guessing attacks, particularly using John the Ripper (JtR) and Hashcat. The optimizations include reordering rules, reordering wordlists, generating new rules, and identifying missing words in wordlists.

## 2. Reordering JtR Rules

### 2.1 Approach
We reorder JtR rules in descending order of success density, defined as the ratio of a rule's successful guesses (matching a password in the evaluation set \( S \)) to the total number of guesses. This process is iterative to avoid prioritizing rules whose successful guesses overlap with those of previously prioritized rules. We assume attackers first guess all items in the wordlist verbatim, which is often the best strategy. We then calculate the success density for each rule against the evaluation set \( S \), placing the rule with the highest success density next. We remove all passwords guessed by that rule from \( S \), recalculating the success density for all remaining rules. This process is repeated until all rules are ordered. In case of ties, we prioritize the rule that made fewer guesses. If the guesses made by rules are fully disjoint, this strategy is provably optimal in maximizing the area under the guessing curve. However, if the guesses are not disjoint, this strategy may not be optimal, but it is more computationally tractable than alternatives.

### 2.2 Evaluation Procedure and Results
Using three wordlists, we applied this approach to reorder each of the three JtR rule lists using six evaluation sets. We found that some rules that appear early in the original 145-rule John list likely belong later, and vice versa. For example, three of the first ten rules never appear earlier than the 85th position after reordering based on success density. Similarly, in the 5,146-rule SpiderLabs rule list, none of the first 23 rules appear within the top 100 rules after reordering. This is notable because SpiderLabs was already manually reordered based on an expert's intuition. Using any of our four English-language evaluation sets (Battlefield Heroes, Brazzers, Clixsense, and Neopets) resulted in similar rule reorderings, while the 000webhost set (requiring a digit) and the Chinese-language CSDN set showed distinct reorderings.

Reordering the SpiderLabs rule list based on any of the four other English-language sets provided substantial improvements in guessing. While reordering does not change which passwords are guessed, a larger fraction of passwords are guessed earlier. Reordering based on the Chinese-language CSDN set also improved performance but less so than the English-language sets. Similar trends were observed for the smaller John rule list. For the Megatron rule list, the original ordering was already near-optimal, and reordering based on the Chinese-language CSDN set resulted in a less effective attack.

## 3. Reordering Wordlists for Hashcat

### 3.1 Approach
Given the performance benefits of reordering rule lists for JtR, we used our analytical tools to reorder wordlists for Hashcat, which guesses in word-major order. We hoped reordering wordlists based on evaluation sets would improve guessing performance, but instead, it worsened performance. Wordlists are typically already ordered in descending frequency based on prior password leaks, and our data-driven optimization seemed to overfit. We describe our process below:

- Begin with a wordlist \( wlist \) containing words \( w \).
- Term \( wlist \) in its initial order \( wlist_{\text{original}} \).
- Given an evaluation set \( S \), run `invert_rule` on each password \( pw \) in \( S \) to identify which passwords would be guessed and by which words.
- Split \( wlist \) into two: \( wlist_{\text{success}} \) containing words \( w_i \) that would guess at least one password in \( S \), and \( wlist_{\text{failure}} \) containing the remaining words.
- Rearrange \( wlist_{\text{success}} \) in descending order of the number of passwords in \( S \) that each would guess, breaking ties arbitrarily.
- Append \( wlist_{\text{failure}} \), maintaining the order from the original \( wlist \). This combined wordlist, optimized on evaluation set \( S \), is termed \( wlist_S \).

### 3.2 Evaluation Procedure and Results
We used this approach to reorder both the XATO wordlist (containing only passwords) and the PGS wordlist (containing passwords followed by natural language dictionaries) for each of the six evaluation sets. Compared to the original, reordering a wordlist based on one evaluation set decreased guessing performance substantially for all other evaluation sets. Data-driven wordlist reordering for Hashcat appears to overfit and is not recommended for small evaluation sets.

## 4. Generating New Rules

### 4.1 Approach
Some prior work has automatically generated new rules and enumerated their guesses to test their effectiveness. We similarly generate new rules and use our analytical tools to reason efficiently about their effectiveness. Adding these potentially "missing" rules enables more passwords to be guessed, as well as more quickly.

- The space of possible rules is huge. By randomly or comprehensively generating rules, we extend a wordlist \( wlist \) with rules it does not already contain.
- We then reorder this extended list based on other evaluation sets as in Section VIII.

### 4.2 Evaluation Procedure and Results
Using each of our three wordlists in turn with the JtR SpiderLabs rule list, we tested on the four English-language evaluation sets with identical composition policies. First, we reordered SpiderLabs based on the three other evaluation sets (termed "reordered"). We then generated 15,085 JtR rules consisting of a single transformation and are both invertible and countable. Adding these to the SpiderLabs rule list (extended), we followed the same reordering procedure and cut off guessing at the previous number of guesses.

This procedure identified rules that are both new and effective. Figure 2 shows the guessability of both Battlefield Heroes and Brazzers. Compared to the original ordering, reordering based on other evaluation sets leads to passwords being guessed more quickly. Extending SpiderLabs and then reordering it leads to even quicker guessing and previously unguessed passwords being guessed. Results were similar across the evaluation sets.

We analyzed the new rules. Cutting off at the same number of guesses as the original SpiderLabs with Battlefield Heroes as the evaluation set, we observed 3,495 new rules having been executed in the extended attack. While the original SpiderLabs contained only 5,146 rules, many utilized JtR’s rule preprocessor to make a large number of guesses in a single rule. We found that 178 of the newly identified rules were strict subsets of an existing SpiderLabs rule with a higher success density, and another 115 new rules were either contained verbatim in John or Megatron or were strict subsets of a rule in those lists. The remaining 3,202 rules were completely new to our three JtR rule lists.

## 5. Identifying Missing Words in Wordlists

### 5.1 Approach
Our `invert_rule` process moves backwards from passwords to the preimages that, when transformed, guess that password. We modified this process to identify what we term "missing words," or words that should have been in the wordlist based on a given evaluation set. The intuition is to leverage "cache misses" to improve wordlist completeness.

- Given an evaluation set \( S \), a wordlist \( wlist \), and a (reordered) rule list \( rlist \), we use `invert_rule` to invert each password \( pw \in S \) to generate preimages \( pi \).
- Each preimage not in the wordlist (i.e., \( pi \notin wlist \)) is a potential missing word.
- To identify preimages likely to generalize, for each unique password \( pw \in S \), we assign a credit \( c \in [0, 1] \) to each potential preimage \( pi \notin wlist \) inversely proportional to the rule’s position in \( rlist \). A preimage identified with the first rule in \( rlist \) will receive \( c = 1 \), while one identified with the middle rule will receive \( c = 0.5 \). This approach prioritizes preimages used early in an attack. For a particular password, credit for a particular preimage is given only once.
- After following this process for all unique passwords in set \( S \), we rank preimages in descending order of credit summed across passwords, keeping those above a threshold.

### 5.2 Evaluation Procedure and Results
We followed this procedure for all six evaluation sets using the fully invertible SpiderLabs rule list and both the LinkedIn and PGS wordlists. We used the rule ordering self-optimized for each evaluation set. To emphasize new cracks, we used only the passwords in each evaluation set that would not otherwise be guessed by a given wordlist and rule list.

Table VI presents a manual thematic categorization of the 100 preimages with the highest credit for the 000webhost, Battlefield Heroes, and Neopets evaluation sets. This process produces site-specific words, meaningful strings unrelated to the site, and short (2–3 character) strings.

To understand whether this procedure results in more effective guessing in realistic scenarios, we first used the four English-language evaluation sets with identical password policies to identify words potentially missing from the PGS wordlist, which includes both passwords and natural language. To test the impact on guessing, we used a random sample of 500,000 passwords from each of the other three sets to generate the top million words "missing" from PGS. We modeled an attack using SpiderLabs rules and the missing words as the wordlist. In each case, this attack made 1.7 × 10^13 extra guesses, successfully guessing 221 Clixsense passwords, 157 Battlefield Heroes passwords, 128 Brazzers passwords, and 118 Neopets passwords from our 25,000-password evaluation sets. None of these passwords would have been guessed otherwise by SpiderLabs. While the success density of such attacks is low, they are appropriate at the end of an attack when high-probability guesses have been exhausted. For comparison, the final 1.7 × 10^13 guesses with the PGS wordlist and SpiderLabs rule list (reordered on the three other sets combined) resulted in zero successful guesses for any of the four test sets.

## 6. Comparisons to Existing Algorithms/Meters

### 6.1 Proactive Password Checking
Our analytical techniques enable two primary applications: proactive password checking and data-driven configuration (improvement) of transformation-based attacks. Here, we analyze these applications relative to prior approaches.

- Our guess-number calculator enables real-time password checking, effectively a server-side password meter.
- We highlight two experiments comparing our approach to meters using combinatoric estimates (zxcvbn) and Neural Networks. Appendix C expands on both.

Following best practices in comparing meters, we examined how meters’ guess numbers for a given password were correlated with the number of times that password appeared in an evaluation set. As shown in Table VII, our analytical JtR and Hashcat approaches are better correlated with the frequency counts than existing meters. Correlations approaching 1 indicate better alignment with frequency counts. For example, for the Brazzers set, JtR had a correlation of 0.734 and Hashcat had a correlation of 0.731, compared to 0.693 and 0.702 for zxcvbn and Neural Networks, respectively.

However, while existing meters estimate a guess number for every password, our approach assigns the same large guess number to any passwords unguessed by JtR or Hashcat. Unlike any prior meter, ours is the first to provide real-time models of guessing attacks widely used in the wild.

To evaluate whether existing meters already fully captured these attacks, we examined whether those meters made unsafe errors, rating guessable passwords as strong. Reflecting real attacks, we rated the 25% of each password set with the lowest guess numbers (guessed first) for Hashcat and JtR as practically weak. As shown in Table VIII, all meters rated at least some practically weak passwords among the 25% of hardest-to-guess passwords.

## 7. Conclusion
This document outlines several optimization techniques for improving password guessing attacks, including reordering rules, reordering wordlists, generating new rules, and identifying missing words in wordlists. These techniques can significantly enhance the efficiency and effectiveness of password cracking efforts.