word strength meter that both accurately estimates the proba-
bilities from a relatively small sample, and is secure against
leakage of the locally stored data. Basically, for every pass-
word x which is added to the password list, we store the
hashed (and possibly salted) value Hash(x) as commonly
done. In addition, we determine the n-gram counts for the
password (i.e., frequencies of n consecutive characters in the
password), merge this information with previously stored
n-grams, to obtain the frequencies over the entire password
database, and add some noise to this data. These n-gram
frequencies can then be used to compute an estimate on the
probability of a fresh password.
4.1 Markov Models
Over the last years, Markov models have proven very
useful for computer security in general and for password
security in particular. For example, Narayanan et al. [19]
showed the effectiveness of Markov models to password
cracking.
The idea is that adjacent letters in human-generated pass-
words are not independently chosen, but follow certain reg-
ularities (e.g., the 2-gram th is much more likely than tq
and the letter e is very likely to follow th). In an n-gram
Markov model, one models the probability of the next char-
acter in a string based on a preﬁx of length n. Hence for a
given string c1, . . . , cm we can write
m(cid:89)
P (c1, . . . , cm) =
P (ci|ci−n+1, . . . , ci−1).
i=1
Our construction only keeps track of the n-gram counts
count(x1, . . . , xn), and the conditional probabilities can eas-
ily computed from these by the following formula:
P (ci|ci−n+1, . . . , ci−1)
(cid:80)
=
count(ci−n+1, . . . , ci−1, ci)
x∈Σ count(ci−n+1, . . . , ci−1, x)
Also note that the size of the password’s alphabet (Σ) is
quite important. We initially used an alphabet of size 96 to
estimate the password strength. However, we observed that
most of the characters are rarely used, leading to sparseness
problems. In the ﬁnal version of our scheme, we choose to
use the following alphabet composed of 38 distinct charac-
ters [a − z][0 − 9][U ][S], where U and S are two symbols
representing all upper-case letters and all special characters,
respectively. This leads to reduce sparseness in the dataset
and better probability estimations.
4.2 Our Construction
Our construction uses Markov Models [14] from the
previous section to estimate the strength of passwords. What
makes this construction interesting is that, by adding some
ﬁne-dosed amount of noise, we can actually prove that the
construction is secure against leakage of the n-gram database.
(For notational simplicity, we assume that all passwords have
00.10.20.30.40.501002003004005006007008009001000FractionofsharedpasswordsFirstkcommonpasswordsRockYouANDPhpBBRockYouANDHotmailRockYouANDMySpaceRockYouANDMySpaceANDPhpBBRockYouFaithwritersMySpace123456123456password112345writerabc123123456789jesus1fuckyoupasswordchristmonkey1iloveyoublessediloveyou1princessjohn316myspace11234567jesuschristfuckyou1rockyoupasswordnumber112345678heavenfootball1abc123faithwritersnicole11length m = l + n − 1, thus the total number of n-grams is
ln.)
4.2.1 N-gram Database Construction and Update (DCU
algorithm)
The n-gram database is constructed and updated as follows:
1. The algorithm keeps as state the n-gram counts,
count(x1, . . . , xn), for all x1, . . . , xn ∈ Σ. All these
counts are initialized to 0.
2. Whenever a password c = c1, . . . , cl+n−1 is added,
it is decomposed into n-grams. The database is then
updated by incrementing the counts corresponding to
each of the password’s n-grams by 1, as follows:
count(ci, . . . , ci+n−1) = count(ci, . . . , ci+n−1) + 1,
for each i = 1, . . . , l.
3. Additionally, noise is added to the database by increas-
ing each individual n-gram’s count, by one, with proba-
bility γ each (independently):
∀x1, . . . , xn :
count(x1, . . . , xn) = count(x1, . . . , xn) + 1
with probability γ.
4. Finally, the password is stored hashed with salt as usual,
e.g., choose salt ←R {0, 1}16 and store
hash(x (cid:107) salt) (cid:107) salt.
4.2.2 Password Strength Estimation (PSE algorithm)
The strength of a password c = c1, . . . , cm, where each char-
acter ci is chosen from alphabet Σ, is estimated as follows:
1. For i = 1, . . . , m, the following conditional probabili-
ties are computed:
P (ci|ci−n+1, . . . , ci−1)
count(ci−n+1, . . . , ci)
count(ci−n+1, . . . , ci−1)
=
=
(cid:80)
count(ci−n+1, . . . , ci)
x∈Σ count(ci−n+1, . . . , ci−1, x)
.
2. Finally, the strength estimate f (c) for the password c
is:
m(cid:89)
f (c) = −log2(
P (ci|ci−n+1, . . . , ci−1))
i=0
Example: The probability of the string password (with
n = 5) is computed as follows
P (password) = P (p)P (a|p)P (s|pa) . . . P (d|swor)
count(asswo)
Picking one of the elements as an example: p(o|assw) =
101485 = 0.97. This results in the overall
count(assw) = 98450
estimation P (password) = 0.0016, where the actual fre-
quency of this password in the RockYou database is 0.0018.
A short list of passwords as scored by our markov model
is included in Table 1. The example shows that the markov
meter is able to correctly recognize weak passwords and, in
fact, closely approximate the actual probability of the pass-
words. Furthermore, more complex and random passwords
are correctly estimated as stronger.
On the other hand, the NIST, Microsoft and Google pass-
word checkers fail to correctly gauge password strength. For
example, the Google meter gives a 0 score to a random
password (dkriouh) only because it does not match the
minimum length of 8 characters. The Microsoft checker
gives a very high score to P4ssw0rd and Password1,
while giving a lower score to the randomly generated pass-
word dkriouh. The NIST checker assigns a higher score
to Password1 than to 2GWapWis. Mistakes of this type
are mostly inevitable in heuristic password checkers and are
reﬂected in the poor accuracy of these as shown in the next
sections.
5 Security of our Construction
Under normal operation, the n-gram database will be
secret and not accessible to an attacker. However, best prac-
tices mandate that even when an adversary gets access to
the n-gram database (e.g., by breaking into the server), little
information about the plain-text passwords should be leaked.
In this section we will show that our system has this property,
by proving a strong bound on the (Shannon) entropy that
leaks when the n-gram database leaks. Consequently, the
remaining guessing entropy of the passwords in the database
remains high by the results from [16].
5.1 Security Deﬁnition
(If the numerator equals zero we use a small out-of-
dictionary probability, to account for unseen n-grams.
However, this will almost never happen, due to the
added noise.)
When deﬁning the security of an adaptive password
checker, it is necessary to consider the adversary’s prior
knowledge on the password distribution. An adversary can
Ideal Markov NIST MS Google
Password
password
9.09
password1 11.52
Password1 16.15
P4ssw0rd
22.37
naeemha
21.96
N/A
dkriouh
N/A
2GWapWis
N/A
Wp8E&NCc
9.25
11.83
17.08
21.67
28.42
42.64
63.67
67.15
21
22.5
28.5
27
19.5
19.5
21
27
1
2
3
3
1
1
3
3
1
1
1
1
0
0
4
4
Table 1. Scores (in bits) as computed by the
markov model and the ideal password meter
from Section 2.1. For the ideal meter the prob-
ability is the empirical frequency in the Rock-
You dataset. The last three passwords were
generated at random using, respectively, the
following rules [a-z]{7}, [a-zA-Z0-9]{8}
and [a-zA-Z0-9\special]{8}.
have access to statistics from lists such as language dictionar-
ies and leaked password lists, and knowledge about common
mangling rules (i.e., rules to derive more passwords from
such lists by appending numbers and special characters).
However, there are many more sources of information an
adversary has access to: their technical background, the
password policies enforced by the site, theme of the site, etc.
It is hard, if not impossible, to come up with a comprehen-
sive list of sources that the adversary is using. Therefore, in
this paper, we consider an adversary who knows the distribu-
tion of the service’s passwords. This automatically considers
all the above sources of information, and protects us against
any future improvements of password cracking software,
such as John the Ripper.
We explicitly note that in normal operation our password
strength meter hardly leaks information. (An exception is the
unavoidable leakage from the password strength meter itself;
as we cannot prevent the attacker from accessing the pass-
word strength meter f, we cannot prevent him from learning
a limited number of data points of this distribution.) Only
when the n-gram database is leaked, then the distribution, as
well as some bits of additional distribution about the actual
passwords in the database, leak.
One might argue that while the password distribution is
generally known, it is not known for very unlikely passwords,
and the n-gram database might leak these passwords. This is
incorrect since we are adding noise to all n-grams, including
rare ones. In addition, as explained previously, knowing
the n-gram frequencies does not help the adversary to break
unlikely, i.e., strong passwords.
The assumption that the password distribution is known
can be seen as an extension of Kerckhoffs’ principle [10].
We do not assume that the distribution is secret, but only the
password chosen with this distribution are. It also underlies
the deﬁnition of guessing entropy (see Equation (1)), which
also considers the optimal guessing strategy.
Finally, we argue that this assumption does not weaken
security since by enforcing a minimum password strength
(similar to [23]) we can still guarantee a minimal guessing
entropy of the passwords. In fact, if we assume that X is a
distribution on passwords with Pr (x) ≤ t for all passwords
x, then the Shannon entropy of X is bounded by
H(X) ≥ − log(t).
(2)
By using the results from [16] we can compute a lower
bound on guessing entropy as
G(X) ≥ 1
4
2H(X) ≥ 1
4t
.
(3)
Enforcing, e.g., a maximum probability of 2−20 yields a
lower bound on the guessing entropy of 218. In other words,
a strong password will remain strong, even if the password
distribution is known. The adversary will be able to compute
its guessing entropy from the distribution, but not more. On
the other hand, weak passwords might be easier to break
since the adversary will be able to optimize his guessing
strategy. However, such passwords should be prohibited in
most services.
5.2
Information and Entropy
The (noisy) n-gram database, when leaked to an adver-
sary, constitutes a noisy channel that transports information
about the stored passwords in the database. In this section
we introduce the notion of entropy and mutual information,
including some basic properties.
The information content of a random variable is measured
in terms of entropy. For two discrete random variables X, Y
with ﬁnite domain D = {d1, . . . , dn}, Conditional Shannon
entropy is deﬁned as
H(Y |X = x)
− n(cid:88)
def=
(4)
Pr (Y = di|X = x) log(Pr (Y = di|X = x))
i=1
and
H(Y |X)
def
=
(cid:88)
x∈X
P r(X = x)H(Y |X = x).
For independent random variables X, Y , a simple calculation
shows
H(X + Y |X) = H(Y ).
(5)
A central notion used to deﬁne the transport of informa-
tion on a (noisy) channel is the notion of mutual information.
The mutual information between the input X and the output
Y , where both X and Y are discrete random variables with
ﬁnite domain, is given by
I(X, Y )
def
= H(Y ) − H(Y |X).
(6)
When deﬁning channel capacity, one takes the maximum
of the mutual information over input distributions; in our
application the input distribution is ﬁxed by the construc-
tion, so the information ﬂow is given directly by the mutual
information I(X, Y ).
Computing a closed formula for the entropy of a given
distribution can be hard. However, for some distributions
such as the binomial distribution the literature gives closed
formulas. Let Bin(m, p) denote the binomial distribution
with m trials that follow a Bernoulli distribution with param-
eter p each, and write q = 1 − p. From [1] we obtain the
following estimates on the entropy of binomial variables:
H(Bin(m, p)) ≥
1
2
log(2πmpq) +
1
2
+
C (p)
1
m
2
C (p)
m2 +
C (p)
3
m3
+
and
(7)
(8)
H(Bin(m, p)) ≤
1
2
1
2
+
log(2πmpq) +
C (p)
4
m
2 = − 7
4 = 1
2
12 − 3 log(pq)
3 = − 1
− 5
6pq , C (p)
360, and C (p)
1 = 13
6(pq)2 , C (p)
for C (p)
3 + 4 log(pq) +
3pq − 1
6pq .
8
From [33] we learn that the entropy of the sum of two bi-
nomially distributed random variables with the same number
of trials can be estimated by the the entropy of a binomial
variable with double the number of trials and the average
parameter:
12 + log(pq)
2 + 1
(cid:19)
(cid:18)
(cid:19)
(cid:18)
≤ H
Bin(2m,
a + b
2
)
.
(9)
H
Bin(m, a) + Bin(m, b)
5.3 Leakage Estimation
To prove the security of the scheme we prove that the
total amount of information leaked (in terms of Shannon
entropy) is limited (A detailed version of this section can be
found in Appendix A).
In this section we use the following variable names: n
denotes the length of the n-grams, N = |Σ|n is the total
number of n-grams, l the number of n-grams per password,
= k · l the
k the number of passwords in the database, m def
total number of N-grams in the database, γ is the probabil-
ity of adding noise used in the construction, and L is the
information leaked from the database.
h), where χj(P i
h all have P r(Si,j
First, we consider the leakage of an individual n-gram