1.79
1.71
90%CI
s
(size=120)
1.62
1.54
1.12
1.01
1.76
3.42
1.36
2.11
1.27
1.25
1.37
(2.84, 3.32)
(1.68, 2.14)
(1.60, 1.93)
(1.30, 1.61)
(2.05, 2.58)
(3.30, 4.33)
(1.65, 2.05)
(3.15, 3.78)
(1.67, 2.05)
(1.60, 1.97)
(1.50, 1.91)
Fig. 11: Absolute frequency of log.entropy values and quantile-
quantile plot of three logs (sample size = 120).
TABLE III: Emulation scenarios
Attack phase as in [36]
Not applicable
Scenario
Regular operation (baseline)
Description: The system operates under misuse-free condition: regular operation is
accomplished by running the typical workload described in Section V.
Bruteforce SSH
Description: The attacker attempts to gain unauthorized access to the system: this is
emulated through repeated SSH connections, which aim to guess the root credentials
of the CWP node while running the workload.
Flights re-routing
Description: The attacker modiﬁes the data in the system: a ﬂight is re-routed every
ﬁve seconds and re-routing is done towards the same destination.
Flights removal
Description: The attacker modiﬁes the data in the system by removing sensitive
information: a ﬂight is removed every four seconds.
Flights creation
Description: The attackers starts misusing the system for personal gain: a new ﬂight
is created every second with the aim of slowing down the system response.
Data modiﬁcation (removal)
Data modiﬁcation
Penetration
Attack relay
size is 120. The quantile-quantile plots suggest that most of the
mass of log.entropy follows a normal distribution, i.e., normal
quantiles with respect to the quantiles of input sample are
distributed along a straight line.
B. Emulation of Regular Operation and Misuse (on-line)
Measurements have been done under regular and system
misuse conditions, in order to generate realistic interesting
activity in the logs, on purpose. Inspired by [36] we emulate
interesting activity, which resembles operations occurring un-
der different attack phases, i.e., penetration, data modiﬁcation
and attack relay. It should be noted that emulation experiments
do not aim to validate the proposal, which is addressed in
Section VII; we aim to induce changes into the behavioral
baseline in order to show how log.entropy measurements can
be used in practice to develop models that help in uncovering
relationships across different nodes, which can be subsequently
used to ﬁlter interesting activity. TABLE III summarizes the
emulation scenarios; each scenario is identiﬁed by a name, the
attack phase it pertains (if applicable) and a description, which
explains scope and how the emulation has been accomplished.
Each scenario has been run for 20 minutes, which allowed
collecting 120 chunks for each log source (i.e.,
total 600
log.entropy vectors over the emulation scenarios).
386
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:58:24 UTC from IEEE Xplore.  Restrictions apply. 
o m i t t e d
|
|
|
o m i t t e d
1
2 D02PAN  1 5 . 3 2
8
9
10
11
12
|
|
|
|
o m i t t e d
FP1NTT  0 . 4 0 : CREATION
MN1MNA  0 . 9 5 : REMOVAL
MN1MNA > 3 . 0 5 : RE−ROUTING
Fig. 14: Portions of the decision tree.
VII. VALIDATION
The log.entropy sampling task is tested under controlled
input conditions to gain insights into the characteristics of
the interesting activity that impact log.entropy-based measure-
ments. To this objective, we characterize a chunk through the
following parameters, such as are depicted by Fig. 15:
•
•
•
•
size (SIZE): total number of entries of the chunk;
signal to noise ratio (SNR): ratio between the number
of interesting (i) and regular (r) entries in the chunk.
Please note that SNR=(i/r) and SIZE=(i+r): the values
of i and r are uniquely determined by SIZE and SNR,
i.e., r=SIZE/(1+SNR) and i=(SIZE-r). For example, a
chunk with 100 entries and SNR=0.02 consists of 98
regular and 2 interesting entries. When SNR=0, the
chunk consists only of regular entries (r=SIZE, i=0).
verbosity (VERB): the total number of terms com-
posing an interesting entry;
dictionary (DICT): percentage of unknown (u) terms
(i.e., terms that are never experienced in the behavioral
baseline of the log) out of the total terms of an inter-
esting entry. We obtain u=VERB· DICT
100 ; for example,
if VERB=12 and DICT=50%, the interesting entry will
contain 6 unknown terms.
We synthesize chunks for a given log Li based on the values
of SNR, SIZE, VERB and DICT as follows. Regular entries
are randomly selected from the 120 chunks that represent the
behavioral baseline of Li; interesting entries are generated
by randomly selecting (i) regular terms from the behavioral
baseline of Li and (ii) unknown terms from a synthetic
list. Here follows an interesting entry with VERB=8 and
DICT=25%:
UNKNOWN-TERM-BC UNKNOWN-TERM-CL TRK: msg[*] CREATEBUFFER
clocknum MTMO converto
The log.entropy of a chunk is systematically evaluated under
different values and combinations of SIZE, SNR, VERB and
DICT in three logs, i.e., FP1LNR, MN1PAN and D02PAN.
Fig. 15: Composition of a synthetic chunk.
387
Fig. 12: Log.entropy of different logs in regular and misuse
condition (ﬂights re-routing scenario).
Fig. 13: Log.entropy of different logs in regular and misuse
condition (ﬂights creation scenario).
Fig. 12 and 13 show how log.entropy varies during the
emulation of the ﬂights re-routing and creation scenarios in
six logs. For each log we report the regular time series, i.e.,
the log.entropy samples obtained in regular operations, and
the misuse time series, i.e., the one obtained under misuse.
It should be noted that the log.entropy observations collected
during the misuse scenarios exhibited a deviation from the
regular operation one. More important,
the misuse jointly
affects different logs of the system.
Although beyond the aim of this paper, we run a decision-
tree model on the vectors collected during emulations to
formalize the relationships across the logs through a human-
actionable model. Fig. 14 shows few portions of the tree, which
is obtained for our data with F-Measure=0.948. For example,
interesting activity in a creation scenario is characterized by
the fact that log.entropy in D02PAN and FP1NTT remains
below 15.32 and 6.92, respectively, while FP1AFS exceeds
0.40. Similarly when D02PAN and MN1MNA exceed 15.32
and 3.05, it can be claimed interesting activity is due to a
re-routing; further involvement of DB1PAN within decision,
allows revealing the occurrence of removal episodes. These
relationships were unknown to the ATC provider: log.entropy
measurements across different streams can pave the way for
effective automated ﬁltering/detection approaches.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:58:24 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 16: For each log and level of VERB, the box plots summarize log.entropy median (−), mean (×), outliers ((cid:3)) and quartile
groups for each level of SIZE (x-axis) and DICT; SNR=LOW in all the cases; y-axis is the value of log.entropy.
TABLE IV: Values of each level by factor and log source; for
each log source DICT={25%,50%,75%,100%}.
reference values
DOE levels values
MEDIUM
x
97.9
11.4
-
-
-
139.1
15.1
859.3
14.5
s
57.7
-
3.5
49.3
-
8.7
448.6
-
9.1
SIZE
SNR
VERB
SIZE
SNR
VERB
SIZE
SNR
VERB
LOW
FP1LNR
50
0.02
8
MN1PAN
90
0.01
8
D02PAN
400
0.0025
8
100
0.04
12
140
0.02
16
800
0.005
16
HIGH
150
0.08
16
190
0.04
24
1,200
0.01
24
We use a Design of Experiments (DOE) approach [35]:
in the DOE terminology SNR, SIZE, VERB and DICT are
referred to as factors, while the values a factor can take are
called levels. In DOE-based analysis levels are usually catego-
rized in classes that can easily understood by practitioners [35],
such as LOW, MEDIUM, HIGH. TABLE IV shows the actual
values taken by SIZE, SNR, VERB at each level; SIZE and
VERB are tuned according to the actual sample mean (x) and
standard deviation (s) of size and verbosity measured in the
behavioral baseline chunks (i.e., reference values in TABLE
IV); SNR level are set in a way they return 1, 2, 4 interesting
entries, respectively, when the size of the chunk is LOW. Levels
of DICT are set to {25%,50%,75%,100%} for each log. Given
a log, we assess all the combinations in the cartesian prod-
uct SIZE×SNR×VERB×DICT, i.e., total 108 combinations;
for each combination we generate 30 independent synthetic
chunks and related log.entropy, i.e., total 3,240 observations,
achieving a full-factorial design with repetitions DOE.
388
Fig. 16 shows the results of our analysis. Given a log and
a level of VERB (indicated by the label in leftmost upper
corner), each subﬁgure reports the box plots that summarize
median (−), mean (×), outliers ((cid:3)) and quartile groups of
the 30 log.entropy observations for each level of SIZE (x-
axis) and DICT (i.e., the colored blue scale); SNR=LOW in
all the cases (we discuss here the worst-case SNR due to space
limitations). Box plots are arranged in groups of ﬁve. For
each group, the leftmost box plot represents the log.entropy
of the baseline chunks, i.e., no interesting activity at all (i.e.,
SNR=0); the rightmost four box plots are obtained at growing
levels of DICT, such as indicated at the bottom of Fig. 16.
Given a group, the overlap between (i) the baseline box plot
and (ii) any of the box plots obtained at a given level of
DICT≥25%, provides insights into the extent to what a chunk
containing a certain amount of interesting activity (i.e., in terms
of SNR, VERB, and DICT) can be actually discriminated from
a baseline chunk of a given SIZE. Intuitively, the smaller the
overlap, the higher the chance to pinpoint a chunk that carries
interesting entries. Fig. 16 suggests that the overlap decreases
as DICT increases; moreover, given a value of DICT, overlap
decreases much more faster when VERB and SIZE increase.
We measured how the ability to classify regular/interesting
chunks by means of log.entropy-based approach varies with
respect to the experimental factors. For each combination of
the factors SIZE, SNR, VERB, DICT we run a classiﬁcation
experiment between (i) the log.entropy observations obtained