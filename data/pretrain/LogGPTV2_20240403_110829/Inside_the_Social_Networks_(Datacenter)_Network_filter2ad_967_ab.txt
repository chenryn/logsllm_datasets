intra-cluster trafﬁc. Finally, CSWs also connect to aggrega-
tion switches for intra-site (but inter-datacenter) trafﬁc and
datacenter routers for inter-site trafﬁc.
The majority of Facebook’s current datacenters employ
this 4-post Clos design. Work is underway, however, to mi-
grate Facebook’s datacenters to a next-generation Fabric ar-
chitecture [9]. The analyses in this paper are based upon
data collected from machines in traditional 4-post clusters,
although Facebook-wide statistics (e.g., Table 3) cover hosts
in both traditional 4-post clusters and newer Fabric pods.
One distinctive aspect of Facebook’s datacenters is that
each machine typically has precisely one role: Web servers
(Web) serve Web trafﬁc; MySQL servers (DB) store user
data; query results are stored temporarily in cache servers
(Cache)—including leaders, which handle cache coherency,
and followers, which serve most read requests [15]; Hadoop
servers (Hadoop) handle ofﬂine analysis and data mining;
Multifeed servers (MF) assemble news feeds [31]. While
there are a number of other roles, these represent the major-
ity, and will be the focus of our study. In addition, a rel-
125Figure 2: How an HTTP request is served
Type
Web
Cache-l
Cache-f
Hadoop
Web Cache MF
15.2
5.9
-
-
63.1
86.6
5.8
-
88.7
-
-
-
SLB Hadoop Rest
16.1
5.6
7.5
-
-
5.5
0.2
-
99.8
-
-
-
Table 2: Breakdown of outbound trafﬁc percentages for four
different host types
atively small number of machines do not have a ﬁxed role
and are dynamically repurposed. Facebook’s datacenters do
not typically house virtual machines: each service runs on
a physical server. Moreover—and in contrast to previously
studied datacenters [12]—to ease provisioning and manage-
ment, racks typically contain only servers of the same role.
3.2 Constituent services
The organization of machines within a cluster—and even
a datacenter—is intimately related to the communication
patterns between the services they support. We introduce the
major services by brieﬂy describing how an HTTP request is
served by http://facebook.com, shown in Figure 2.
When an HTTP query hits a Facebook datacenter, it ar-
rives at a layer-four software load balancer (SLB) [37]. The
query is then redirected to one of the Web servers. Web
servers are largely stateless, containing no user data. They
fetch data from the cache tier [15].
In case of a cache
miss, a cache server will then fetch data from the database
tier. At the same time, the Web server may communi-
cate with one or more backend machines to fetch objects
such as news stories and ads. Table 2 quantiﬁes the rela-
tive trafﬁc intensity between different services by classify-
ing the outbound trafﬁc from four different servers—a Web
server, cache leader (cache-l), cache follower (cache-f), and
Hadoop—based upon the role of the destination host. (The
data is extracted from packet-header traces described in Sec-
tion 3.3.2.)
In contrast to most service tiers, Hadoop nodes are not
involved with serving end-user requests.
Instead, Hadoop
clusters perform ofﬂine analysis such as data mining. HDFS
and Hadoop MapReduce are the main applications running
on these servers.
3.3 Data collection
Due to the scale of Facebook’s datacenters, it is imprac-
Instead,
tical to collect complete network trafﬁc dumps.
Figure 3: Fbﬂow architecture
we consider two distinct sources of data. The ﬁrst, Fbﬂow,
constantly samples packet headers across Facebook’s entire
global network. The second, port mirroring, focuses on a
single machine (or rack) at a time, allowing us to collect
complete packet-header traces for a brief period of time at
particular locations within a single datacenter.
3.3.1 Fbﬂow
Fbﬂow is a production monitoring system that samples
packet headers from Facebook’s entire machine ﬂeet. Its ar-
chitecture, comprised of two main component types—agents
and taggers—is shown in Figure 3. Fbﬂow samples packets
by inserting a Netﬁlter nflog target into every machine’s
iptable rules. The datasets we consider in this paper
are collected with a 1:30,000 sampling rate. A user-level
Fbﬂow agent process on each machine listens to the nflog
socket and parses the headers, extracting information such as
source and destination IP addresses, port numbers, and pro-
tocol. These parsed headers—collected across all machines
in Facebook’s datacenters—along with metadata such as ma-
chine name and capture time, are streamed to a small number
of taggers using Scribe [2], a log aggregation system.
Taggers, running on a subset of machines, read a portion
of the packet-header stream from Scribe, and further anno-
tate it with additional information such as the rack and clus-
ter containing the machine where the trace was collected,
its autonomous system number, etc., by querying other data
sources. Taggers then convert each annotated packet header
into a JSON object and feed it into Scuba [3], a real-time
data analytics system. Samples are simultaneously stored
into Hive [38] tables for long-term analysis.
3.3.2 Port mirroring
While Fbﬂow is a powerful tool for network monitor-
ing and management, its sampling-based collection prohibits
certain types of data analysis. Speciﬁcally, in production
use, it aggregates statistics at a per-minute granularity. In
order to collect high-ﬁdelity data, we deploy a number of
special-purpose trace collection machines within the data-
center that collect packet-header traces over short intervals.
We deploy monitoring hosts in ﬁve different racks across
Facebook’s datacenter network, locating them in clusters
that host distinct services. In particular, we monitor a rack
of Web servers, a Hadoop node, cache followers and lead-
ers, and a Multifeed node.
In all but one (Web) instance,
we collect traces by turning on port mirroring on the RSW
SLB	
  WEB	
  WEB	
  WEB	
  Web	
  WEB	
  WEB	
  WEB	
  Cache	
  WEB	
  WEB	
  WEB	
  DB	
  Mul1feed,	
  Ads,	
  etc.	
  WEB	
  WEB	
  WEB	
  Hadoop	
  HTTP Request HTTP Reply Agent&nflog&Samples&Agent&Agent&Agent&Agent&Agent&Agent&Agent&Agent&Scribe&Tagger&Tagger&Tagger&Tagger&Scribe&Server&Info,&BGP,&etc.&Scuba&Hive&126(ToR) and mirroring the full, bi-directional trafﬁc for a sin-
gle server to our collection server. For the hosts we monitor,
the RSW is able to mirror the selected ports without loss. In
the case of Web servers, utilization is low enough that we are
able to mirror trafﬁc from a rack of servers to our collection
host. We did not measure database servers that include user
data in this study.
Recording the packet traces using a commodity server is
not entirely trivial, as tcpdump is unable to handle more
than approximately 1.5 Gbps of trafﬁc in our conﬁguration.
In order to support line-rate traces, we employ a custom ker-
nel module that effectively pins all free RAM on the server
and uses it to buffer incoming packets. Our kernel mod-
ule extracts the packets immediately after the Ethernet driver
hands the packets to the kernel to avoid any additional delay
or overhead. Once data collection is complete, the data is
spooled to remote storage for analysis. Memory restrictions
on our collection servers limit the traces we collect in this
fashion to a few minutes in length.
4. PROVISIONING
The appropriate design, scale, and even technology of a
datacenter interconnect depends heavily on the trafﬁc de-
mands of the services it hosts. In this section, we quantify
the trafﬁc intensity, locality, and stability across three dif-
ferent types of clusters inside Facebook datacenters; in par-
ticular, we examine clusters supporting Hadoop, Frontend
machines serving Web requests, and Cache.
Our study reveals that while Facebook’s Hadoop deploy-
ments exhibit behavior largely consistent with the literature,
the same cannot be said for clusters hosting Facebook’s other
In particular, most trafﬁc is not rack-local, yet
services.
locality patterns remain stable within and across both long
(multiple-day) and short (two-minute) time intervals. We
deﬁne stable trafﬁc as being close to constant (low devia-
tion from a baseline value) over a time interval, and slowly
changing across time intervals. Note that this deﬁnition is
dependent upon the length of the interval being considered;
accordingly, we examine several different timescales.
4.1 Utilization
Given that Facebook has recently transitioned to 10-Gbps
Ethernet across all of their hosts, it is not surprising that
overall access link (i.e., links between hosts and their RSW)
utilization is quite low, with the average 1-minute link uti-
lization less than 1%. This comports with the utilization lev-
els reported for other cloud-scale datacenters [12, 17]. De-
mand follows typical diurnal and day-of-the-week patterns,
although the magnitude of change is on the order of 2× as
opposed to the order-of-magnitude variation reported else-
where [12], Even the most loaded links are lightly loaded
over 1-minute time scales: 99% of all links are typically less
than 10% loaded. Load varies considerably across clusters,
where the average link utilization in the heaviest clusters
(Hadoop) is roughly 5× clusters with light load (Frontend).
As in other datacenters with similar structure [12, 13], uti-
lization rises at higher levels of aggregation. Focusing on the
links between RSWs and CSWs, median utilization varies
between 10–20% across clusters, with the busiest 5% of the
links seeing 23–46% utilization. These levels are higher
than most previously studied datacenters [12, Fig. 9], likely
due to the disproportionate increase in edge-link technology
(1→10 Gbps) vs. aggregation links (10→40 Gbps). The
variance between clusters decreases, with the heaviest clus-
ters running 3× higher than lightly loaded ones. Utilization
is higher still on links between CSWs and FC switches, al-
though the differences between clusters are less apparent be-
cause different clusters are provisioned with different num-
bers of uplinks depending on their demand. We examine link
utilization at ﬁner timescales in Section 6.
4.2 Locality and stability
Prior studies have observed heavy rack locality in data-
center trafﬁc. This behaviour seems in line with applications
that seek to minimize network utilization by leveraging data
locality, allowing for topologies with high levels of oversub-
scription. We examine the locality of Facebook’s trafﬁc from
a representative sampling of production systems across var-
ious times of the day.
Figure 4 shows the breakdown of outbound trafﬁc by des-
tination for four different classes of servers: a Hadoop server
within a Hadoop cluster, a Web server in a Frontend cluster,
and both a cache follower and a cache leader from within the
same Cache cluster. For each server, each second’s trafﬁc
is represented as a stacked bar chart, with rack-local trafﬁc
in cyan, the cluster-local trafﬁc in blue, the intra-datacenter
trafﬁc in red, and inter-datacenter trafﬁc in green.
Among the four server types, Hadoop shows by far the
most diversity—both across servers and time: some traces
show periods of signiﬁcant network activity while others do
not. While all traces show both rack- and cluster-level local-
ity, the distribution between the two varies greatly. In one
ten-minute-long trace captured during a busy period, 99.8%
of all trafﬁc sent by the server in Figure 4 is destined to other
Hadoop servers: 75.7% of that trafﬁc is destined to servers in
the the same rack (with a fairly even spread within the rack);
almost all of the remainder is destined to other hosts within
the cluster. Only a vanishingly small amount of trafﬁc leaves
the cluster.
In terms of dispersion, of the inter-rack (intra-cluster) traf-
ﬁc, the Hadoop server communicates with 1.5% of the other
servers in the cluster—spread across 95% of the racks—
though only 17% of the racks receive over 80% of the
server’s trafﬁc. This pattern is consistent with that observed
by Kandula et al. [26], in which trafﬁc is either rack-local or
destined to one of roughly 1–10% of the hosts in the cluster.
Hadoop’s variability is a consequence of a combina-
tion of job size and the distinct phases that a Hadoop job
undergoes—any given data capture might observe a Hadoop
node during a busy period of shufﬂed network trafﬁc, or dur-
ing a relatively quiet period of computation.
By way of contrast, the trafﬁc patterns for the other server
classes are both markedly more stable and dramatically dif-
ferent from the ﬁndings of Kandula et al. [26]. Notably,
only a minimal amount of rack-local trafﬁc is present; even
127Figure 4: Per-second trafﬁc locality by system type over a two-minute span: Hadoop (top left), Web server (top right), cache
follower (bottom left) and leader (bottom right) (Note the differing y axes)
inter-datacenter trafﬁc is present in larger quantities. Fron-
tend cluster trafﬁc, including Web servers and the atten-
dant cache followers, stays largely within the cluster: 68%
of Web server trafﬁc during the capture plotted here stays
within the cluster, 80% of which is destined to cache sys-
tems; the Multifeed systems and the SLB servers get 8%
each. While miscellaneous background trafﬁc is present, the
volume of such trafﬁc is relatively inconsequential.
Cache systems, depending on type, see markedly different
localities, though along with Web servers the intra-rack lo-
cality is minimal. Frontend cache followers primarily send
trafﬁc in the form of responses to Web servers (88%), and
thus see high intra-cluster trafﬁc—mostly servicing cache
reads. Due to load balancing (see Section 5.2), this trafﬁc
is spread quite widely; during this two-minute interval the
cache follower communicates with over 75% of the hosts in
the cluster, including over 90% of the Web servers. Cache
leaders maintain coherency across clusters and the backing
databases, engaging primarily in intra- and inter-datacenter
trafﬁc—a necessary consequence of the cache being a "sin-
gle geographically distributed instance." [15]
The stability of these trafﬁc patterns bears special men-
tion. While Facebook trafﬁc is affected by the diurnal trafﬁc
pattern noted by Benson et al. [12], the relative proportions
of the locality do not change—only the total amount of traf-
ﬁc. Over short enough periods of time, the graph looks es-