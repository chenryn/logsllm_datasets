nwn
kkkkkkckkk
mkdir
kk
mkmkmk
kkk
open64
eopeeeeneeopeope
enee
execl
ex
clcl
cll
exeeeccclll
lll
l
readlink
open
opeeen
opeeen
mkfifo
gettimeofday
putchar_unlocked
h
h
putctttt h
dirname
access
ac
aa
strchrnul
memrchr
strchr
trchr
trchr
strrchr
memchr
mem
mem
strtoull
strtoll
strtt
strtoul
t ltot l
strtol
/ŶƐƚƌƵĐƟŽŶƐ
ZĞŐŝƐƚĞƌƐ
>ŝďĐ&ƵŶĐƟŽŶĂůůƐ
regexec
fnmatch
ffnff maa
^ƚƌŝŶŐ^ĞĂƌĐŚ
strnlen
htons
htotto
ntohs
fnstsw
ϭϲͲďŝƚZĞŐŝƐƚĞƌƐ
r15w
r9w
di
diidi
d
dd
dd
d
dd
dd
r8w
r10w
rrrr1
1r111rr11
1
111111
ww
000000000www
ww
00www
ww
r11w
1w
wwwww
wwwwwwwwww
w
w
w
rrrwwwwww
w
w
rwwwww
1w
r111
11
1
1
r14w
1
1
si
ss
ss
bp
bppbx
b
bbxxxbbbb bbxxxbbb b
bbb b
bbb bb bbb b
bb bb b
b
bbb
bbb
bbb
bbb
bbbb133wbbb
bb1 wbb
r13w
r13www
133w13ww
r1 ww
133w1 w
1
cwde
ax
ax
x
xx
x
ccdx
dddx
dddx
cx
r12w
Figure 2: T-SNE clustering visualization of tokens appearing in assembly code. There are three categories of tokens: operation,
operand, and libc function call. Each token is represented as a 200-dimensional numeric vector. They are learned by Asm2Vec
on plain assembly code without any prior knowledge of the assembly language. The training assembly code does not contain
the libc callee functions’ content. For visualization, T-SNE reduces the vectors to two dimensions by nearest neighbor
approximation. A smaller geometric distance indicates a higher lexical semantic similarity.
2. Problem Deﬁnition
In the assembly clone search literature, there are four
types of clones [15], [16], [17]: Type I: literally identical;
Type II: syntactically equivalent; Type III: slightly modiﬁed;
and Type IV: semantically similar. We focus on Type IV
clones, where assembly functions may appear syntactically
different, but share similar functional logic in their source
code. For example, the same source code with and without
obfuscation, or a patched source code between different
releases. We use the following notions: function denotes an
assembly function; source function represents the original
function written in source code, such as C++; repository
function stands for the assembly function that is indexed in-
side the repository; and target function denotes the assembly
function query. Given an assembly function, our goal is to
search for its semantic clones from the repository RP. We
formally deﬁne the search problem as follows:
Deﬁnition 1. (Assembly function clone search) Given a
target function ft, the search problem is to retrieve the top-
k repository functions fs ∈ RP, ranked by their semantic
similarity, so they can be considered as Type IV clones.
3. Overall Workﬂow
Figure 3: The overall work ﬂow of Asm2Vec.
any prior knowledge. Step 2: After the training phase, the
model produces a vector representation for each repository
function. Step 3: Given a target function ft that was not
trained with this model, we use the model to estimate its
vector representation. Step 4: We compare the vector of ft
against the other vectors in the repository by using cosine
similarity to retrieve the top-k ranked candidates as results.
The training process is a one-time effort and is efﬁcient
to learn representation for queries. If a new assembly func-
tion is added to the repository, we follow the same procedure
in Step 3 to estimate its vector representation. The model can
be retrained periodically to guarantee the vectors’ quality.
4. Assembly Code Representation Learning
Figure 3 shows the overall workﬂow. There are four
steps: Step 1: Given a repository of assembly functions,
we ﬁrst build a neural network model for these functions.
We only need their assembly code as training data without
In this section, we propose a representation learning
model for assembly code. Speciﬁcally, our design is based
on the PV-DM model [20]. PV-DM model learns document
representation based on the tokens in the document. How-
(cid:21)(cid:24)(cid:21)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:47:52 UTC from IEEE Xplore.  Restrictions apply. 
ever, a document is sequentially laid out, which is different
than assembly code, as the latter can be represented as
a graph and has a speciﬁc syntax. First, we describe the
original PV-DM neural network, which learns a vectorized
representation of text paragraph. Then, we formulate our
Asm2Vec model and describe how it is trained on instruction
sequences for a given function. After, we elaborate how to
model a control ﬂow graph as multiple sequences.
4.1. Preliminaries
The PV-DM model is designed for text data. It is an
extension of the original word2vec model. It can jointly learn
vector representations for each word and each paragraph.
Figure 4 shows its architecture.
Figure 4: The PV-DM model.
Given a text paragraph which contains multiple sen-
tences, PV-DM applies a sliding window over each sentence.
The sliding window starts from the beginning of the sen-
tence and moves forward a single word at each step. For
example, in Figure 4, the sliding window has a size of 5.
In the ﬁrst step, the sliding window contains the ﬁve words
‘the’, ‘cat’, ‘sat’, ‘on’ and ‘a’. The word ‘sat’ in the middle is
treated as the target and the surrounding words are treated as
the context. In the second step, the window moves forward
a single word and contains ‘cat’, ‘sat’, ‘on’, ‘a’ and ‘mat’,
where the word ‘on’ is the target.
At each step, the PV-DM model performs a multi-class
prediction task (see Figure 4). It maps the current paragraph
into a vector based on the paragraph ID and maps each word
in the context into a vector based on the word ID. The
model averages these vectors and predicts the target word
from the vocabulary through a softmax classiﬁcation. The
back-propagated classiﬁcation error will be used to update
these vectors. Formally, given a text corpus T that contains
a list of paragraphs p ∈ T, each paragraph p contains a list
of sentences s ∈ p, and each sentence is a sequence of |s|
words wt ∈ s. PV-DM maximizes the log probability:
T(cid:2)
p(cid:2)
|s|−k(cid:2)
p
s
t=k
log P(wt|p, wt−k, ..., wt+k)
(1)
(cid:21)(cid:24)(cid:22)
The sliding window size is 2k + 1. The paragraph vector
captures the information that is missing from the context
to predict the target. It is interpreted as topics [20]. PV-
DM is designed for text data that is sequentially laid out.
However, assembly code carries richer syntax than plain-
text. It contains operations, operands, and control ﬂow that
are structurally different than plaintext. These differences
require a different model architecture design that cannot
be addressed by PV-DM. Next, we present a representation
learning model that integrates the syntax of assembly code.
4.2. The Asm2Vec Model
An assembly function can be represented as a control
ﬂow graph (CFG). We propose to model the control ﬂow
graph as multiple sequences. Each sequence corresponds
to a potential execution trace that contains linearly laid-out
assembly instructions. Given a binary ﬁle, we use the IDA
Pro2 disassembler to extract a list of assembly functions,
their basic blocks, and control ﬂow graphs.
t ∈ R
This section corresponds to Step 1 and 2 in Figure 3.
In these steps, we train a representation model and produce
a numeric vector for each repository function fs ∈ RP.
Figure 5 shows the neural network structure of the model.
It is different than the original PV-DM model.
First, we map each repository function fs to a vector
(cid:2)θfs ∈ R
2×d. (cid:2)θfs is the vector representation of function
fs to be learned in training. d is a user chosen parameter.
Similarly, we collect all the unique tokens in the repository
RP. We treat operands and operations in assembly code as
tokens. We map each token t into a numeric vector (cid:2)vt ∈ R
d
2×d. (cid:2)vt is the vector
and another numeric vector (cid:2)v(cid:2)
representations of token t. After training, it represents a
token’s lexical semantics. (cid:2)vt vectors are used in Figure 2 to
visualize the relationship among tokens. (cid:2)v(cid:2)
t is used for token
prediction. All (cid:2)θfs and (cid:2)vt are initialized to small random
value around zero. All (cid:2)v(cid:2)
t are initialized to zeros. We use
2 × d for fs since we concatenate the vectors for operation
and operands to represent an instruction.
We treat each repository function fs ∈ RP as multiple
sequences S(fs) = seq[1 : i], where seqi is one of them.
We assume that the order of sequences is randomized. A
sequence is represented as a list of instructions I(seqi) =
in[1 : j], where inj is one of them. An instruction inj
contains a list of operands A(inj) and one operation P(inj).
Their concatenation is denoted as its list of tokens T (inj) =
P(inj) || A(inj), where || denotes concatenation. Constants
tokens are normalized into their hexadecimal form.
For each sequence seqi in function fs, the neural net-
work walks through the instructions from its beginning. We
collect the current instruction inj, its previous instruction
inj−1, and its next instruction inj+1. We ignore the instruc-
tions that are out-of-boundary. The proposed model tries to
2. IDA Pro, available at: http://www.hex-rays.com/
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:47:52 UTC from IEEE Xplore.  Restrictions apply. 
Figure 5: The proposed Asm2Vec neural network model for assembly code.
maximize the following log probability across the repository
RP:
RP(cid:2)
S(fs)(cid:2)
I(seqi)(cid:2)
T (inj )(cid:2)
log P(tc|fs, inj−1, inj+1)
(2)
fs
seqi
inj
tc
It maximizes the log probability of seeing a token tc at
the current instruction, given the current assembly function
fs and neighbor instructions. The intuition is to use the
current function’s vector and the context provided by the
neighbor instructions to predict the current instruction. The
vectors provided by neighbor instructions capture the lexi-
cal semantic relationship. The function’s vector remembers
what cannot be predicted given the context. It models the
instructions that distinguish the current function from the
others.
For a given function fs, we ﬁrst
look-up its vector
representation (cid:2)θfs through the previously built dictionary.
To model a neighbor instruction in as CT (in) ∈ R
2×d, we
average the vector representations of its operands (∈ R
d)
and concatenate the averaged vector (∈ R
d) with the vector
representation of the operation. It can be formulated as:
A(in)(cid:2)
1
CT (in) = (cid:2)vP(in)||
|A(in)|
(3)
Recall that P(∗) denotes an operation and it is a single
token. By averaging fs with CT (inj − 1) and CT (inj + 1),
δ(in, fs) models the joint memory of neighbor instructions:
(cid:2)vtb
t
δ(inj, fs) =
1
3
((cid:2)θfs + CT (inj−1) + CT (inj+1))
(4)
+
Example 1. Consider a simple assembly code function
fs and one of its sequence in Figure 5. Take the third
instruction where j = 3 for example. T (in3) = {’push’,
’rbx’}. A(in3−1) = {’rbp’, ’rsp’}. P(in3−1) = {’mov’}. We
collect their respective vectors (cid:2)vrbp, (cid:2)vrsp, (cid:2)vmov and calculate
CT (in3−1) = (cid:2)vmov||((cid:2)vrbp + (cid:2)vrsp)/2. Following the same
(cid:21)(cid:24)(cid:23)
procedure, we calculate CT (in3+1). With Equation 4 and
(cid:2)θfs we have δ(in3, fs). (cid:2)
Given δ(in, fs), the probability term in Equation 2 can
be rewritten as follows:
P(tc|fs, inj−1, inj+1) = P(tc|δ(inj, fs))
(5)
Recall that we map each token into two vectors (cid:2)v and (cid:2)v(cid:2).
For each target token tc ∈ T (inj), which belongs to the
current instruction, we look-up its output vector (cid:2)v(cid:2)
tc. The
probability in Equation 5 can be modeled as a softmax multi-
class regression problem:
P(tc|δ(inj, fs)) = P((cid:2)v(cid:2)
f ((cid:2)v(cid:2)
(cid:3)D
d f ((cid:2)v(cid:2)
=
tc
f ((cid:2)v(cid:2)
tc , δ(inj, fs)) = U h(((cid:2)v(cid:2)
|δ(inj, fs))
tc , δ(inj, fs))
td , δ(inj, fs))
tc )T × δ(inj, fs))
D denotes the whole vocabulary constructed upon the repos-
itory RP. U h(·) denotes a sigmoid function applied to
each value of a vector. The total number of parameters
to be estimated is (|D| + 1) × 2 × d for each pass of the
softmax layout. The term |D| is too large for the softmax
classiﬁcation. Following [20], [21], we use the k negative
sampling approach to approximate the log probability as:
log P(tc|δ(inj, fs)) ≈ log f ((cid:2)v(cid:2)
|δ(inj, fs))
tc
(cid:4)
Etd(cid:2)Pn(tc)
k(cid:2)
i=1
(cid:2)td (cid:5)= tc(cid:3)log f (−1 × (cid:2)v(cid:2)
td , δ(inj, fs))
(cid:5)
(6)
(cid:2)·(cid:3) is an identity function. If the expression inside this
function is evaluated to be true, then it outputs 1; otherwise
0. For example, (cid:2)1 + 2 = 3(cid:3) = 1 and (cid:2)1 + 1 = 3(cid:3) = 0. The
negative sampling algorithm distinguishes the correct guess
tc with k randomly selected negative samples {td|td (cid:5)= tc}
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:47:52 UTC from IEEE Xplore.  Restrictions apply. 
using k + 1 logistic regressions. Etd(cid:2)Pn(tc) is a sampling
function that samples a token td from the vocabulary D
according to the noise distribution Pn(tc) constructed from
D. By taking derivatives, respectively on (cid:2)v(cid:2)
t and (cid:2)θfs, we
can calculate the gradients as follows.
∂
∂(cid:2)θfs
J(θ) =
k(cid:2)
1
3
× (cid:2)v(cid:2)
i
t
Etb(cid:2)Pn(tc)
(cid:4)
(cid:2)tb = tc(cid:3) − f ((cid:2)v(cid:2)
(cid:5)
t, δ(inj, fs))
J(θ) = (cid:2)t = tc(cid:3) − f ((cid:2)v(cid:2)
t, δ(inj, fs)) × δ(inj, fs)
∂
∂ (cid:2)v(cid:2)
t
(7)
By taking derivatives, respectively on (cid:2)vP(inj+1) and
{(cid:2)vtb
|tb ∈ A(inj+1)}, we can calculate their gradients as
follows. It will be the same equation for the previous
instruction inj−1, by replacing inj+1 with inj−1.
instructions are selectively expanded with the body of the
callee function. BinGo inlines all the standard library calls
for the purpose of semantic correctness. We do not inline
any library calls, since the lexical semantic among library
call
tokens have been well captured by the model (see
the visualization in Figure 2). BinGo recursively inlines
callee, but we only expand the ﬁrst-order callees in the call
graph. Expanding callee functions recursively will include
too many callees’ body into the caller, which makes the
caller function statically more similar to the callee.
The decoupling metric used by BinGo captures the ratio
of in-degree and out-degree of each callee function fc:
α(fc) = outdegree(fc)/(outdegree(fc)+indegree(fc)) (9)
We adopt the same equation, as well as the same threshold
value 0.01, to select a callee for expansion. Additionally,
we ﬁnd that if the callee function is longer than or has a
comparable length to the caller, the callee will occupy a too
large portion of the caller. The expanded function appears
similar to the callee. Thus, we add an additional metric to
ﬁlter out lengthy callees:
∂
∂(cid:2)vP(inj+1)
∂
J(θ) =
∂(cid:2)vtb
J(θ) =
(cid:5)
J(θ)
(cid:4) ∂
∂(cid:2)θfs
1
|A(inj+1)| × (cid:4) ∂
tb ∈ A(inj+1)
∂(cid:2)θfs
[0 : d − 1]
(cid:5)
J(θ)
[d : 2d − 1]
(8)
δ(fs, fc) = length(fc)/length(fs)
(10)
We expand a callee if δ is less than 0.6 or fs is shorter
than 10 lines of instructions. The second condition is to
accommodate wrapper functions.
After, we use back propagation to update the values of
the involved vectors. Speciﬁcally, we update (cid:2)θfs, all the
involved (cid:2)vt and involved (cid:2)v(cid:2)
t according to their gradients,
with a learning rate.
Example 2. Continue from Example 1, where the target
push|δ(inj, fs))
token tc is ’push’. Next, we calculate P((cid:2)v(cid:2)
using negative sampling (Equation A). After, we calculate
the gradients using Equation 7 and 8. We update all the
involved vectors in these two examples, according to their
respective gradient, with a learning rate. (cid:2)
4.3. Modeling Assembly Functions
In this section, we model an assembly function into mul-
tiple sequences. Formally, we treat each repository function
fs ∈ RP as multiple sequences S(fs) = seq[1 : i]. The
original linear layout of control ﬂow graph covers some
invalid execution paths. We cannot directly use it as a
training sequence. Instead, we model the control ﬂow graph
as edge coverage sequences and random walks.
4.3.1. Selective Callee Expansion. Function inlining is a
compiler optimization technique that replaces a function call
instruction with the body of the called function. It extends
the original assembly function and improves its performance
by removing call overheads. It signiﬁcantly modiﬁes the
control ﬂow graph and is a major challenge in assembly
clone search [12], [13].
BinGo [12] proposes to selectively inline callee functions
into the caller function in the dynamic analysis process.
We adopt this technique for static analysis. Function call