### 1. Introduction
The following text describes a series of experiments aimed at evaluating the effectiveness of mouse dynamics and touchpad data for user authentication. The experiments were designed to assess the impact of environmental variables, such as the type of pointing device, on the error rates of two different classification techniques: one based on neural networks (Ahmed) and another based on logistic regression (Gamboa).

### 2. Experiment 1: User Authentication with Mouse and Touchpad Data
In this experiment, each user's sessions were divided into training and testing sets. Specifically, the first two sessions were used for training, and the last three sessions were used for testing. For each user \( u \) (where \( 1 \leq u \leq n \)), two classification tasks were performed:

1. **Positive and Negative Training**:
   - A classifier was trained using \( u \)'s training sessions as positive examples and the training sessions of the other \( n-1 \) users as negative examples.
   - The classifier was then tested on \( u \)'s three test sessions.

2. **Cross-User Testing**:
   - For each of the other \( n-1 \) users \( v \) (where \( 1 \leq v \leq n \) and \( v \neq u \)), a classifier was trained using \( u \)'s training sessions and the training sessions of the other \( n-2 \) users (excluding \( v \)'s training sessions).
   - The classifier was then tested on both \( v \)'s and \( u \)'s test sessions.

**Error Calculation**:
- A test session was considered misclassified if the classifier output a score below a threshold \( t \) for positive sessions or above \( t \) for negative sessions.
- The False Acceptance Rate (FAR) was calculated as \( \text{FAR} = \frac{\text{FA}}{\text{TN}} \), where \( \text{TN} \) is the number of test sessions belonging to the \( n-1 \) other users, and \( \text{FA} \) is the number of those sessions with a classification score above \( t \).
- The False Rejection Rate (FRR) was calculated as \( \text{FRR} = \frac{\text{FR}}{\text{TP}} \), where \( \text{TP} \) is the number of test sessions belonging to \( u \), and \( \text{FR} \) is the number of those sessions with a classification score below \( t \).
- The threshold \( t \) was set independently for each user to equalize or minimize the difference between FAR and FRR.
- The resulting error rates for all \( n \) users were averaged to get the overall FAR and FRR for the experiment.

**Experimental Procedure**:
- The procedure was performed separately on mouse and touchpad datasets, and the results are presented in Table 1.

### 3. Experiment 2: Remote Access Scenario
This experiment aimed to determine whether data collected from one pointing device could be used to verify a user's identity based on data from the other pointing device.

**Procedure**:
- For each user \( u \), two classification tasks were performed:
  1. **Mouse to Touchpad**:
     - The classifier was trained using \( u \)'s two training sessions from the mouse dataset as positive examples and the two mouse training sessions of each of the other \( n-1 \) users as negative examples.
     - The classifier was then tested on \( u \)'s three test sessions from the touchpad dataset.
  2. **Touchpad to Mouse**:
     - The classifier was trained using \( u \)'s two training sessions from the touchpad dataset as positive examples and the two touchpad training sessions of each of the other \( n-1 \) users as negative examples.
     - The classifier was then tested on \( u \)'s three test sessions from the mouse dataset.

**Error Rates**:
- The thresholds for determining the error rates were set to be the same as those determined in Experiment 1.
- The results are shown in Table 2.

### 4. Experiment 3: Detecting Device Type
This experiment aimed to determine whether the two approaches could be used to identify the pointing device that generated a given session.

**Procedure**:
- The 17 users were divided into two groups: eight for training and nine for testing.
- All sessions in the training and test groups were labeled according to the pointing device.
- A classifier was trained on the labeled training sessions and tested on the test sessions.
- For Ahmed, a neural network approach was used, while for Gamboa, a logistic regression classifier was used due to time constraints and implementation inflexibility.
- Using a decision threshold of 0.5, Gamboa correctly identified the generating pointing device for 96.7% of the test instances, and Ahmed achieved a success rate of 97.8%.

### 5. Analysis of Results
- **Experiment 1**: The average error rates for both techniques were higher than those reported in the literature, likely due to the tight control of environmental variables and the omission of feature selection for Gamboa.
- **Experiment 2**: The average error rates rose substantially when training and testing on data from different pointing devices, suggesting that the hardware itself has a strong influence on mouse dynamics.
- **Experiment 3**: Both techniques were able to correctly determine the pointing device with almost perfect accuracy, indicating that the behavioral variations caused by the pointing device exhibit a distinct, user-independent pattern.

### 6. Limitations
- More training data could improve error rates, but we did not collect enough data to examine the impact of enrollment time.
- Collecting additional mouse data from each test subject on their personal computers would have provided a better baseline for the first experiment.

### 7. Avenues for Additional Work
- **Common Evaluation Data**: There is a need for a common, publicly available dataset to allow for the comparison of existing and future approaches.
- **Reduction of Verification Time**: Developing more effective ways to clean raw mouse data of extraneous noise could reduce verification time.
- **Minimizing False Rejections**: Combining mouse dynamics with other types of behavioral biometrics, such as keystroke dynamics, could improve authentication accuracy.
- **Offline Analysis Techniques**: Existing techniques could be used in offline forensics analysis to provide useful insights.
- **Effects of Environmental Variables**: Further exploration of the effects of environmental variables, such as screen resolution, mouse speed, and the psychological state of the user, is needed.

### 8. Conclusion
- The results confirmed that there are detectable behavioral differences among individuals, but loosely controlled environmental variables in past evaluations likely contributed to the low error rates.
- Continuous approaches generally require a significant amount of mouse data, raising questions about their practicality for continuous online authentication.
- When enrollment and verification data for the same user are collected under different pointing devices, existing techniques are not likely to accurately verify the user's identity, suggesting that mouse dynamics may not be suitable for web-based applications or remotely accessed resources.

### 9. Acknowledgments
We thank the students who volunteered their time for data collection and the anonymous peer reviewers for their helpful comments.

### 10. References
[References listed here]