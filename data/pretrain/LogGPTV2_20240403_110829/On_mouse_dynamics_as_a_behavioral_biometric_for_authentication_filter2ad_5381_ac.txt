the number of users. The (cid:12)ve sessions belonging to each user
were divided such that the (cid:12)rst two sessions were designated
for training and the last three for testing. For each user
u(1 ≤ u ≤ n), two classi(cid:12)cation tasks were performed. First,
a classi(cid:12)er was trained with u’s training sessions as positive
examples and the training sessions of the other n−1 users as
negative training examples. The classi(cid:12)er was then tested
on u’s three test sessions. Second, for each of the other
n − 1 users, v(1 ≤ v ≤ n; v ̸= u), in turn, a classi(cid:12)er is
trained using u’s training sessions and the training sessions
of the other n − 2 users (that is, excluding user v’s training
sessions), and then tested on v’s and u’s test sessions.
A given test session is considered misclassi(cid:12)ed for user u if
the classi(cid:12)er outputs a score below some threshold t, for pos-
itive test sessions, or above the threshold for negative test
sessions. The FAR is then calculated as F A=T N , where
T N is the number of test sessions belonging to the n − 1
other users and F A is the number of those test sessions for
which the classi(cid:12)cation score was above the threshold t. The
FRR is calculated as F R=T P where T P is the number of
test sessions belonging to u and F R is the number of those
test sessions for which the classi(cid:12)cation score was below t.
The threshold t is set independently for each user such that
the FAR and FRR are equal (or as close as possible). The
resulting error rates for all n users are averaged to get the
FAR and FRR for the entire experiment. The experimen-
tal procedure was performed separately on the mouse and
touchpad datasets and the results are given in Table 1.
6.3 Experiment 2: remote access scenario
The purpose of Experiment 2 was to investigate whether
data collected from a given user on one of the pointing de-
vices could be used to verify that user’s identity based on da-
ta collected from the same user on the other pointing device.
The procedure was as follows. For each user u(1 ≤ u ≤ n),
two classi(cid:12)cation tasks were performed. First, the classi-
480
Table 1: Results of Experiment 1
Ahmed
Gamboa
Mouse
Mouse
Touchpad
Touchpad
FAR FRR FAR FRR FAR FRR FAR FRR
Avg. 21% 21.5% 20% 21.5% 30.3% 37.1% 29.7% 34.3%
Std. 14.3% 13.4% 13.3% 13.4% 9.8% 17.7% 17.4% 24.2%
Table 2: Results of Experiment 2
Train on Mouse
Train on Touchpad
Gamboa
Ahmed
Gamboa
Ahmed
FAR FRR FAR FRR FAR FRR FAR FRR
Avg. 19.6% 56.9% 40.9% 46.7% 26.7% 56.9% 37.5% 56.9%
Std. 18.1% 38.7% 31.7% 35% 23.2% 30.7% 29.2% 31.8%
(cid:12)er was trained with user u’s two training sessions from
the mouse data set as positive examples and the two mouse
training sessions of each of the other n − 1 users as negative
training examples; the classi(cid:12)er was then tested on user u’s
three test sessions from the touchpad data set. Next, for each
of the other users, v(1 ≤ v ≤ n; v ̸= u), user v’s training ex-
amples are excluded from the training set and the classi(cid:12)er
is retrained. The retrained classi(cid:12)er is then tested both on
user v’s and user u’s own test sessions from the touchpad
dataset. This whole procedure is repeated a second time
using the touchpad data set for training and the mouse da-
ta set for testing. The thresholds for determining the error
rates in this experiment were set to be the same as those
determined for the same user in Experiment 1. The results
of this experiment are shown in Table 2.
6.4 Experiment 3: detecting device type
In the (cid:12)nal experiment, we sought to determine whether
the two approaches could be used to identify the pointing
device that generated a given session. We (cid:12)rst divided the 17
users into two groups with eight users for training and nine
for testing, such that no user was represented in both the
training and testing set. We then labeled all of the sessions
in the training and test groups according to pointing device.
A classi(cid:12)er was then trained on the labeled training sessions
and subsequently tested on the test sessions. For Ahmed, we
used the same neural network approach for classi(cid:12)cation as
in the earlier experiments. For Gamboa, however, we used a
logistic regression classi(cid:12)er, due to time constraints and the
in(cid:13)exibility of our implementation. Using the same session
length as in the previous experiments, and using a decision
threshold of 0.5, Gamboa was able to correctly identify the
generating pointing device for 96.7% of the test instances,
while Ahmed achieved a success rate of 97.8%.
6.5 Analysis of Results
In Table 1, we see that the average error rates for both
techniques are signi(cid:12)cantly higher than those originally re-
ported in the literature. We suspect that these results are
due, in part, to the tight control of environmental variables
in our experiment, but they are also likely due, in part, to:
(1) omitting the feature selection step for Gamboa, and to
(2) using less enrollment data than was used in the pre-
vious experiments reported in the literature; however, we
believe that the amount of data used in our experiments
represents a practical enrollment time. The thing to note
from these results is that although the error rates obtained
481
under the conditions of this experiment were not especially
good, they were well below 0.5 for both techniques, which
suggests that there are indeed detectable behavioral di(cid:11)er-
ences among users, even when environmental variables are
tightly controlled.
Using the results from the (cid:12)rst experiment as a baseline, it
can be observed from the results of Experiment 2 (see Table
2) that the average error rates rose substantially for both
techniques when training and testing on data from di(cid:11)erent
pointing devices. For nearly all of the users, either the FRR
or FAR rose above 50%. These results suggest that pointing
device hardware itself exhibits an in(cid:13)uence on mouse dy-
namics strong enough to overshadow the unique behavioral
patterns of most users. The results of Experiment 3 pro-
vide further evidence of this in(cid:13)uence. In the experiment,
both techniques were able to correctly determine, with al-
most perfect accuracy, which of the two pointing devices
generated a given data session. The results further indicate
that the behavioral variations caused by the pointing device
hardware exhibit a distinct, user-independent pattern.
6.6 Limitations of our Experiments
In practice it is reasonable to assume that, over time, more
training data could be collected and incorporated into the
system than was available in our experiments; this would
likely improve error rates. We did not collect enough data
to examine the impact of enrollment time on the accuracy
of the techniques.
In hindsight, Experiment 1 could have been improved by
additionally collecting mouse data from each test subject as
they performed the 30 minute task on their personal com-
puters. This would have provided a better baseline for the
(cid:12)rst experiment, allowing us to make stronger claims about
the extent of the impact of environmental variables on the
two techniques we tested.
7. AVENUES FOR ADDITIONAL WORK
We believe that there are still signi(cid:12)cant issues to be re-
solved before MDA systems are truly ready to be deployed
in practice. In this section we discuss important directions
for new research in this area.
7.1 Creation of Common Evaluation Data
There is critical need for the creation of a common, pub-
licly available data set for use by researchers in this area.
Not only would such a data set allow for the comparison of
existing and future approaches, it would signi(cid:12)cantly reduce
the overhead for new researchers in this (cid:12)eld.
7.2 Reduction of Veriﬁcation Time
The practicality of MDA systems depends not only on low
error rates, but also depends critically on achieving a reason-
ably short veri(cid:12)cation time. However, existing approaches
tend to require a considerable amount of data to be collected
before a reasonably accurate authentication decision can be
made, which for many systems is more than enough time for
an attacker to achieve her goal. One approach to decrease
veri(cid:12)cation time might be to develop more e(cid:11)ective ways to
clean the raw mouse data of extraneous noise; with higher
quality data, it might be possible to make authentication
decisions over smaller quantities of data.
7.3 Strategies for Minimizing False Rejection-
s
Strategies for minimizing or handling false rejections in a
graceful manner might also increase the practicality of M-
DA systems. One possibility might be to combine mouse
dynamics with other types of behavioral biometrics, such as
keystroke dynamics ([9]) to improve authentication accura-
cy. Multimodal biometric systems have been proposed in
the past to address low authentication accuracy caused by
noisy data or intra-class variations [13].
7.4 Ofﬂine Analysis Techniques
Existing techniques could potentially be used in o(cid:15)ine
forensics analysis to provide useful insights; for example,
determining whether multiple distinct individuals have been
using a single account or whether a single user has been using
more than one account. To our knowledge, this has not yet
been explored.
7.5 Effects of Environmental Variables
We have shown in this paper that certain environmental
variables (e.g. pointing device type) may signi(cid:12)cantly im-
pact mouse behavior if the state of these variables is di(cid:11)erent
at enrollment time than at veri(cid:12)cation time. Other variables
that could be explored include the following: software-level
variables, such as screen resolution, mouse speed and accel-
eration settings in the OS, mouse polling rate, perceptual
delays caused by high CPU load, and etc; properties of the
surface on which the mouse is being used; and the psycholog-
ical state of the user (e.g the user may be fatigued, distracted
or distressed). The e(cid:11)ects of such variables on one’s mouse
behavior are generally unexplored.
8. CONCLUSION
In this paper, we pointed out that past evaluations of M-
DA techniques did not carefully control environmental vari-
ables across test subjects. We conducted an experiment on
mouse data collected from 17 volunteer subjects to try to
answer the question of whether the low error rates reported
in the literature were due to strongly detectable behavioral
di(cid:11)erences among test subjects, or instead due to di(cid:11)erences
in their computing environments. The results of that exper-
iment con(cid:12)rmed that there are detectable behavioral di(cid:11)er-
ences among individuals but that the loosely controlled en-
vironmental variables in past evaluations likely contributed
to the low error rates.
We also pointed out that existing continuous approaches
generally require a signi(cid:12)cant amount of mouse data to be
collected before an authentication decision can be reached.
Consequently, we question whether mouse dynamics is really
practical for continuous online authentication.
Finally, we showed that when enrollment data and veri-
(cid:12)cation data for the same user are collected under two dif-
ferent pointing devices, existing techniques are not likely to
be able to accurately verify the user’s identity. This (cid:12)nding
suggests that mouse dynamics may not be a good choice for
authentication in web-based applications or other remotely
accessed resources.
9. ACKNOWLEDGMENTS
We wish to thank the students who volunteered their time
to participate in our data collection. We would also like to
482
thank the anonymous peer reviewers for their helpful com-
ments.
10. REFERENCES
[1] A. A. E. Ahmed and I. Traore. Anomaly intrusion
detection based on biometrics. In IEEE Workshop on
Information Assurance and Security, pages 452{453,
2005.
[2] A. A. E. Ahmed and I. Traore. A new biometric
technology based on mouse dynamics. IEEE
Transactions on Dependable and Secure Computing,
4(3):165{179, 2007.
[3] P. Bours and C. J. Fullu. A login system using mouse
dynamics. In Fifth International Conference on
Intelligent Information Hiding and Multimedia Signal
Processing, pages 1072{1077, 2009.
[4] H. Gamboa and A. Fred. An identity authentication
system based on human computer interaction
behaviour. In 3rd International Workshop on Pattern
Recognition on Information Systems, pages 46{55,
2003.
[5] H. Gamboa and A. Fred. A behavioural biometric
system based on human computer interaction. In
SPIE 5404 - Biometric Technology for Human
Identi(cid:12)cation, pages 381{392, Orlando, FL USA, 2004.
[6] H. Gamboa, A. L. N. Fred, and A. K. Jain.
Webbiometrics: User veri(cid:12)cation via web interaction.
In Proceedings of Biometrics Symposium, pages 1{6,
2007.
[7] I. Guyon and A. Elissee(cid:11). An introduction to variable
and feature selection. The Journal of Machine
Learning Research, 3:1157{1182, 2003.
[8] S. Hashia, C. Pollett, and M. Stamp. On using mouse
movements as a biometric. In Proceeding in the
International Conference on Computer Science and its
Applications, volume 1, 2005.
[9] F. Monrose and A. D. Rubin. Keystroke dynamics as
a biometric for authentication. Future Generation
Computer Systems, 1:351{359, 2000.
[10] M. Pusara and C. E. Brodley. User re-authentication
via mouse movements. In C. E. Brodley, P. Chan,
R. Lippman, and W. Yurcik, editors, Workshop on
Visualization and Data Mining for Computer Security
(VizSEC/DMSEC 2004), 29 October 2004,
Washington DC, USA, pages 1{8. ACM, 2004.
[11] R. J. Quinlan. C4.5: Programs for Machine Learning.
Morgan Kaufmann, 1993.
[12] K. Revett, H. Jahankhani, S. T. de Magalhes, and
H. M. D. Santos. A survey of user authentication
based on mouse dynamics. In Proceedings of 4th
International Conference on Global E-Security,
volume 12 of Communications in Computer and
Information Science, pages 210{219, London, UK,
June 2008. Springer Berlin Heidelberg.
[13] A. Ross and A. Jain. Multimodal biometrics: An
overview. In 12th European Signal Processing
Conference, pages 1221{1224, Vienna, Austria, 2004.
[14] D. A. Schulz. Mouse curve biometrics. In Biometric
Consortium Conference, 2006 Biometrics Symposium,
pages 1{6, 2006.