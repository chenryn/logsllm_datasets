In particular, the range of
[0, 100) exhibits a signiﬁcant peak for malicious domains.
3.4 Domain Name-Based Features
Benign services usually try to choose domain names that
can be easily remembered by users. For example, a bank
called “The Iceland Bank” might have a domain name such
as “www.icelandbank.com”. In contrast, attackers are not
concerned that their domain names are easy to remember.
This is particularly true for domain names that are generated
by a DGA.
The main purpose of DNS is to provide human-readable
names to users as they often cannot memorize IP addresses
of servers. Therefore, benign Internet services tend to
choose easy-to-remember domain names. In contrast, hav-
ing an easy-to-remember domain name is not a concern for
people who perform malicious activity. This is particularly
true in cases where the domain names are generated by a
DGA. To detect such domains, we extracted two features
from the domain name itself: First, the ratio of the numer-
ical characters to the length of the domain name (Feature
14), and second, the ratio of the length of the longest mean-
ingful substring (i.e., a word in a dictionary) to the length
of the domain name (Feature 15).
Note that there exist popular domains such as yahoo.com
and google.com that do not necessarily include “meaning-
ful” words.
In order to gain a higher conﬁdence about a
domain, we query Google and check to see if it returns a
hit-count for a domain that is above a pre-deﬁned threshold.
When analyzing a domain, we only focus on the second
level domains (i.e., SLD). For example, for x.y.server.com,
we would take server.com. To detect domain names that
have been possibly automatically generated, we calculate
the percentage of numerical characters (Feature 14) and the
ratio of the length of the longest meaningful substring to the
total length of the SLD (Feature 15). To extract all possible
meaningful substrings from an SLD, we check the English
dictionary.
As some benign domains in China and Russia consist
of combinations of alphabetical and numerical characters,
Feature 15 produces a high positive rate. However, when
Features 14 and 15 are combined, the false positives de-
crease. Also, for domains that are determined to be suspi-
cious, we check how many times it is listed by Google. The
reasoning here is that sites that are popular and benign will
have higher hit counts.
4 Building Detection Models
4.1 Constructing the Training Set
The quality of the results produced by a machine learn-
ing algorithm strongly depends on the quality of the train-
ing set [35]. Our goal is to develop a classiﬁer that is able
to label domains as being benign, or malicious. Thus, we
require a training set that contains a representative sample
of benign and malicious domains. To this end, we studied
several thousand malicious and benign domains, and used
them for constructing our training set.
We collected malicious domains from multiple sources.
Speciﬁcally, we obtained malicious domains from malware-
domains.com [19], the Zeus Block List [26], Malware Do-
mains List [25], Anubis [13] reports, a list of domains
that are extracted from suspected to be malicious URLs
analyzed by Wepawet [18], and Phishtank [31]. We also
used the list of domains that are generated by the DGAs of
the Conﬁcker [32] and Mebroot [34] (i.e., Torpig) botnets.
These malicious domain lists represent a wide variety of
malicious activity, including botnet command and control
servers, drive-by download sites, phishing pages, and scam
sites that can be found in spam mails.
Note that we are conservative when constructing the ma-
licious domain list. That is, we apply a preliminary check
before labeling a domain as being malicious and using it
in our training set. Malicious domain sources such as
Wepawet and Phishtank operate on URLs that have been
submitted by users. Hence, while most URLs in these
repositories are malicious, not all of them are. Also, while
some third level domains (3LD) of a domain extracted from
a URL may behave maliciously, the rest may not (e.g.,
a.x.com might be malicious, while x.com might be benign).
Assuming that a domain that is suspected to be mali-
cious either by Wepawet or Phishtank has ttotal possible
3LDs (number of distinct 3LD recorded by EXPOSURE
during the analysis period) and tmal 3LDs are thought to
be malicious, we choose the domain to be representative
for a malicious behavior only if tmal/ttotal is greater than
0.75 (i.e., only if 75% of the 3LDs have been reported to
be involved in malicious activities). The initial malicious
domain list that we generated consists of 3500 domains.
As discussed in detail in Section 5.1, we assume that all
of the Alexa top 1000 domains and domains that we have
observed on our sensors that are older than one year are
benign. Therefore, we construct our initial benign domain
list using these domains. However, to ensure that our benign
domain list does not include any domain that might have
been involved in malicious activity, we perform a two-step
veriﬁcation process.
First, we compare all the domains in the benign domain
list with the malicious domain list and with the tools that test
domains for their maliciousness, speciﬁcally with McAffee
Site Advisor and Norton Safe Web. Second, we also cross-
check the benign domain with the list provided by the Open
Directory Project (ODP – a large, human-edited directory of
the web constructed and maintained by volunteer editors).
Our initial benign domain list consists of 3000 domains.
4.2 The Initial Period of Training
By experimenting with different values, we determined
that the optimal period of initial training for our system was
seven days. This period is mainly required for us to be able
to use the time-based features that we described in Section
3. During this time, we can observe the time-based behav-
ior of the domains that we monitor and can accurately take
decisions on their maliciousness.
After the initial one week of training, we are able to re-
train the system every day, hence, increasing detection ac-
curacy.
4.3 The Classiﬁer
Our classiﬁer is built as a J48 decision tree algo-
rithm (J48). J48 [40] is an implementation of the C4.5 algo-
rithm [33] that is designed for generating either pruned or
unpruned C4.5 decision trees. It constructs a decision tree
from a set of labeled training set by using the concept of
information entropy (i.e., the attribute values of the training
set).
The J48 algorithm leverages the fact that the tree can
be split into smaller subtrees with the information obtained
from the attribute values. Whenever the algorithm encoun-
ters a set of items that can clearly be separated from the
other class by a speciﬁc attribute, it branches out a new leaf
according to the value of the attribute. Each time a decision
needs to be taken, the attribute with the highest normalized
gain is chosen. Among all possible values of the attributes,
if there is any value for which there is no ambiguity, the
branch is terminated and the appropriate label is assigned
to it. The splitting procedure stops when all instances in all
subsets belong to the same class.
We use a decision tree classiﬁer because these algorithms
have shown to be efﬁcient while producing accurate results
[33]. As the decision tree classiﬁer builds a tree during the
training phase, the features that are best in separating the
malicious and the benign domains can be clearly seen.
Recall that we divided the 15 features that we use into
four different classes according to the type of information
used: Features that are extracted from the time series anal-
ysis (F1, Time-Based Features), the DNS answer analy-
sis (F2, DNS Answer-Based Features), the TTL value anal-
ysis (F3, TTL Value-Based Features), and the analysis of
the domain name (F4, Domain Name-Based Features).
To ﬁnd the combination of features that produce the min-
imum error rate, we trained classiﬁers using different com-
binations of feature sets and compared the results. Figure 2
shows the percentage of the number of miss-classiﬁed items
with three different training schemes: 10-fold cross valida-
tion, 66% percentage split, and training on the whole train-
ing set. Note that the smallest error rates were produced by
F1. Therefore, while experimenting with different combi-
nations of feature sets, we excluded the combinations that
do not include F1 (i.e., F23, F24, F34 and F234). The high-
est error rates are produced by F3 and F4. However, when
all features are combined (i.e., F-all), the minimum error
rate is produced. Hence, we use the combination of all the
features in our system.
5 Evaluation
5.1 DNS Data Collection for Ofﬂine Experiments
Our sensors for the SIE DNS feeds receive a large vol-
ume of trafﬁc (1 million queries per minute on average).
Therefore, during our ofﬂine experimental period of two
and a half months, we monitored approximately 100 bil-
lion DNS queries. Unfortunately, tracking, recording and
post-processing this volume of trafﬁc without applying any
ﬁltering was not feasible in practice. Hence, we reduced
the volume of trafﬁc that we wished to analyze to a more
manageable size by using two ﬁltering policies. The goal of
these policies was to eliminate as many queries as possible
that were not relevant for us. However, we also had to make
sure that we did not miss relevant, malicious domains.
The ﬁrst policy we used whitelisted popular, well-known
domains that were very unlikely to be malicious. To create
this whitelist, we used the Alexa Top 1000 Global Sites [4]
list. Our premise was that the most popular 1000 websites
on the Internet would not likely be associated with domains
that were involved in malicious activity. These sites typi-
cally attract many users, and are well-maintained and mon-
itored. Hence, a malicious popular domain cannot hide its
malicious activities for long. Therefore, we did not record
the queries targeting the domains in this whitelist. The do-
mains in the whitelist received 20 billion queries during two
and a half months. By applying this ﬁrst ﬁltering policy, we
were able to reduce 20% of the trafﬁc we were observing.
The second ﬁltering policy targeted domains that were
older than one year. The reasoning behind this policy was
that many malicious domains are disclosed after a short
period of activity, and are blacklisted. As a result, some
miscreants have resorted to using domain generation algo-
rithms (DGA) to make it more difﬁcult for the authorities to
blacklist their domains. For example, well-known botnets
such as Mebroot [34] and Conﬁcker [32] deploy such algo-
rithms for connecting to their command and control servers.
Typically, the domains that are generated by DGAs and reg-
istered by the attackers are new domains that are at most
several months old. In our data set, we found 45.000 do-
mains that were older than one year. These domains re-
ceived 40 billion queries. Hence, the second ﬁltering policy
reduced 50% of the remaining trafﬁc, and made it manage-
able in practice.
Clearly, ﬁltering out domains that do not satisfy our age
requirements could mean that we may miss malicious do-
mains for the training that are older than one year. How-
ever, our premise is that if a domain is older than one year
and has not been detected by any malware analysis tool,
it is not likely that the domain serves malicious activity.
To verify the correctness of our assumption, we checked
if we had ﬁltered out any domains that were suspected to
be malicious by malware analysis tools such as Anubis and
Wepawet. Furthermore, we also queried reports produced
by Alexa [4], McAfee Site Advisor [8], Google Safe Brows-
ing [6] and Norton Safe Web [9]. 40 out of the 45, 000 ﬁl-
tered out domains (i.e., only 0.09%) were reported by these
external sources to be “risky” or “shady”. We therefore be-
lieve that our ﬁltering policy did not miss a signiﬁcant num-
ber of malicious domains because of the pre-ﬁltering we
performed during the ofﬂine experiments.
5.2 Evaluation of the Classiﬁer
To evaluate the accuracy of the J48 DecisionTree Clas-
siﬁer, we classiﬁed our training set with 10-fold cross-
validation and percentage split, where 66% of the training
set is used for training, and the rest is used to check the cor-
t
%
(
e
a
r
r
o
r
r
)
E
 20
 15
 10
 5
 0
Error rate / Feature Sets
Full-Set
Cross-Validation
Percentage-Split
F1
F2
F3
F4
F12
F13
F14
F123
F134
F124
Fall
Feature Sets
Figure 2: Percentage of miss-classiﬁed instances
rectness. Table 3 reports the results of the experiment. The
Area Under the ROC curve [15] for the classiﬁer is high for
both methods.
Note that the false positive rate is low (i.e., around 1% for
both methods). After investigating the reasons for the miss-
classiﬁcations, we saw that the classiﬁer had identiﬁed 8
benign domains as being malicious. The reason for the mis-
classiﬁcation was that these domains were only requested a
small number of times during the two and half months of
experiments (i.e., making the classiﬁer conclude that they
were short-lived) and because they exhibited TTL behav-
ior that looked anomalous (e.g., possibly because there was
a conﬁguration error, or because the site maintainers were
experimenting to determine the optimal TTL value).
5.3 Experiments with the Recorded Data Set
During the two and a half month ofﬂine experimental pe-
riod, we recorded and then analyzed 4.8 million distinct do-
main names that were queried by real Internet users. Note
that a domain that only receives a few requests cannot pro-
duce a time series that can then be used for the time-based
features we are analyzing. This is because a time series
analysis produces accurate results only when the sampling
count is high enough. In order to ﬁnd the threshold for the
minimum number of queries required for each domain, we
trained our known malicious and benign domain list with
differing threshold values. Figure 4 shows the detection
and false positive rates for the threshold values we tested.
Based on these empirical results, we set the threshold to 20
queries, and excluded the 4.5 million domains from our ex-
periments that received less than 20 requests in the two and
a half months duration of our monitoring.
e
t
a
r
P
F
P
T
/
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
Minimum Requests Targetting a Domain
FalsePositives
TruePositives
 10
 20
 30
 40
 50
Threshold
Figure 4: The effect of the minimum request count on de-
tection rate
For further experiments, we then focused on the remain-
ing 300,000 domains that were queried more than 20 times.
EXPOSURE decided that 17,686 out of the 300,000 do-
mains were malicious (5.9%).
5.3.1 Evaluation of the Detection Rate
The percentage split and cross-validation evaluations on the
training set show that the detection rate of our classiﬁer is
around 98%. Since our goal is to be able to detect unknown
malicious domains that have not been reported by any mali-
cious domain analyzer, our evaluation of the classiﬁer needs
to show that we are able to detect malicious domains that
do not exist in our training set. To this end, we used mal-
Full data
10-folds Cross-Validation
66% Percentage Split
AUC Detection Rate
99.5%
0.999
98.5%
0.987