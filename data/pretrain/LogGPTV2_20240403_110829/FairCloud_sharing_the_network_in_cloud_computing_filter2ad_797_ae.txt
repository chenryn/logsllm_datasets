additional (cid:24)400 lines of C++ for the Click elements. To achieve
high performance, we deploy Click in kernel mode and approximate
CSFQ using (cid:277)xed point operations. Unless otherwise speci(cid:277)ed, the
presented results are obtained in simulation.
Since Per-Flow’s allocation can be arbitrary, depending on the
number of (cid:280)ows, we only consider a maximum of one (cid:280)ow between
any source-destination pair. In this case, Per-SD and Per-Flow are
equivalent, and therefore, we omit Per-SD from the results. For ease
of exposition, we assume all VMs to have a unit weight. e high-
lights of our (cid:277)ndings are as follows:
(cid:15) e tradeoﬀ between proportionality and bandwidth guaran-
tee is also evident at the network level. While PS-P is able to
provide the highest guarantee, PS-N exhibits maximum pro-
portionality in network allocation.
(cid:15) e relative behaviors of these policies scale to large-scale
clusters. PS-N is very close to achieving network propor-
tionality for the small MapReduce jobs, and both PS-P and
PS-N improve the minimum bandwidth allocations of the
tasks of individual jobs and therefore result in an approximate
speed-up of (cid:24)15(cid:2) in the shuﬄe time of the small jobs.8
5.1 Link-Level Scenarios
For the (cid:277)rst set of experiments, we focus on a single congested
link and evaluate multiple allocation policies by varying workloads.
Since PS-N would provide the same allocation as PS-L on a single
link (assuming that the VMs in our experiments only communicate
over that congested link), we omit it from the following discussion.
Figure 8 depicts a scenario similar to the one in Figure 2, where
two VMs, A1 and B1, are collocated on the same physical host; A1
communicates with VM A2, while B1 communicates with N other
VMs. We assume VMs have high demands and the link capacity is
1Gbps. Figure 8 plots the bandwidth allocation of tenant A for in-
creasing values of N. Figure 8(a) presents simulation results, while
Figure 8(b) presents actual allocations achieved on the testbed. Re-
member that Per-Source is asymmetric, and so we present both in-
coming and outgoing allocations for it.
We make four observations from these results.
(cid:15) First, PS-P maintains the same throughput for tenant A re-
gardless of N. is is because A1 is guaranteed a minimum
bandwidth of half its access link capacity; the same is true for
the outgoing bandwidth allocated by Per-Source.
8During the shuﬄe phase, mappers of a MapReduce job transfer
intermediate data to the reducers over the network.
Strategy
Per-Flow
Per-Source
PS-L
PS-P
PS-N
Full Bisection BW
Exp
6.99
1.00
4.03
0.99
Sim
7.00
1.00
4.00
1.00
1.00 WFQ CSFQ
1.15
1.00
4(cid:2) Oversubscribed
Sim
18.98
6.99
7.00
7.00
5.29 WFQ CSFQ
5.55
Exp
18.91
6.75
7.20
7.13
5.30
Table 4: Simulation vs. Experimental Results (Ratio of Aggregate
Bandwidth of the tenants, B=A)
We assume that tenant A communicates using a pairwise one-to-one
communication pattern between its VMs (i.e., Ai $ Ai+4, where
i = f1; : : : 4g), while tenant B communicates all-to-all (i.e., Bi com-
municates with all Bj, where j ̸= i). For simplicity, we assume all the
VMs have equal weights and in(cid:277)nite bidirectional demands. From
the network proportionality point of view, an ideal allocation would
provide both tenants the same bandwidth.
Figure 10(b) presents the bandwidth allocation for the two ten-
ants in Figure 10(a) when the core is fully provisioned (i.e., the net-
work has full bisection bandwidth). In this case, the access links are
the congestion points. We see from Figure 10(b) that PS-P, PS-N
and Per-Source policies are able to match the desirable equal allo-
cation. However, PS-L provides B twice as much bandwidth as A
on each access link, since B competes with four VMs on each access
link against only two VMs of A(cid:0)we use the version of PS-L propor-
tional the number of active tenant VMs on the link. Consequently,
at the network level, Per-Flow and PS-L favor dense communication
patterns such as all-to-all over sparse communication patterns.
Figure 10(c) presents the allocations for tenants A and B when the
core is under-provisioned by a factor of 4(cid:2) (each of the aggregation
and core layers being oversubscribed by 2(cid:2)). Given our setup, core
links are the bottlenecks for tenant A. In this case, PS-P, PS-L and
Per-Source policies allocate the core bandwidth equally between the
two tenants, while allowing tenant B to fully utilize the aggregation-
level bandwidth for free. However, PS-N penalizes tenant B for uti-
lizing the aggregation-level bandwidth because half the weights of
each of the tenant B’s VMs is utilized in aggregation-level commu-
nication, and it provides twice as much core bandwidth to tenant A.
us, PS-N provides the best proportionality at the network level
(see Table 4 for quantitative results).
We validate the simulation results presented in the above two
cases through experiments on a testbed with the same topology.
Table 4 presents the comparison between the simulation and the
experimental results. e experimental results closely match those
from the simulation, with an error margin lower than than 4%.
In the next two experiments, we focus on highlighting the tradeoﬀ
between proportionality and bandwidth guarantees at the network
level. First, consider a scenario, where two tenants A and B have the
same number of VMs (64) deployed on a fully provisioned tree net-
work (cid:0)we use a 32 server cluster, each server containing one VM
of each tenant. VMs of tenant A communicate in a (random) one-
to-one fashion. B runs a MapReduce job, and his VMs are divided
in two sets M and R, such that all VMs in M communicate to all in
R. For ease of exposition, we consider the communications to be
bidirectional.9
Figure 11 presents the ratio between the aggregate bandwidths of
B and A for PS-P and PS-N in this setting.10 e fully proportional
9 Otherwise, congestion will not appear uniformly, and the results
will become more diﬃcult to understand.
10We did not include the other policies since they would oﬀset the
chart, making it signi(cid:277)cantly harder to visualize.
(a) Simulation results
(b) Experimental results
Figure 8: Link level bandwidth allocation of tenant A, while tenant
B has an increasingly larger number of VMs (similar to the scenario
in Figure 2). PS-P maintains a constant throughput for tenant A.
(a) Simulation results
(b) Experimental results
Figure 9: Network allocation of tenant B on a congested link, while
tenant A varies the number of mappers (senders). Both tenants
use the many-to-many communication pattern commonly found in
MapReduce shuﬄes.
(cid:15) Second, PS-L provides tenant A its proportional share.
(cid:15) ird, Per-Flow and Per-Source policies provide neither pro-
portionality nor guarantees.
(cid:15) Lastly, our implementation matches the simulation results,
modulo the fact that the link capacities available on DETER-
lab are smaller (the available bandwidth is (cid:24)900Mbps instead
of advertised 1Gbps).
In Figure 9, we consider two MapReduce jobs A and B (jobs play
the role of tenants in this discussion), each with 10 VMs. While job
B has 5 mappers and 5 reducers communicating over the congested
link under consideration, we vary the ratio between the number of
mappers (M) and reducers (R) of job A while keeping (M+R) = 10.
As before, we show both simulation and testbed results.
We observe that PS-L achieves a proportional allocation that is
not aﬀected by the change in M, while PS-P’s allocation increases
as M
M+5 (5 is the number of mappers of tenant B). We also notice
that Per-Flow achieves proportionality only when the distribution of
mappers and receivers is the same between the two jobs (at M = 5),
since only at this point both the jobs have equal number of (cid:280)ows;
remember that the number of (cid:280)ows is M (cid:2) R.
5.2 Network-Level Scenarios
We now turn our attention to the network-wide allocation under
diﬀerent scenarios. For these experiments we include PS-N, because
its network-wide allocations will be diﬀerent than those of PS-L.
We start oﬀ with the simple scenario illustrated in Figure 10(a).
In this particular example, we have a small tree with eight servers
and two tenants A and B, each with one VM in each of the servers.
051015202530# Destination VMs100200300400500Aggregate Bandwidth (Mbps)Per-FlowPer-Source (Out)Per-Source (In)PS-LPS-P051015202530# Destination VMs100200300400500Aggregate Bandwidth (Mbps)Per-FlowPer-Source (Out)Per-Source (In)PS-LPS-P0246810# Mappers100200300400500600700Aggregate Bandwidth (Mbps)Per-FlowPer-Source (Out)Per-Source (In)PS-LPS-P0246810# Mappers100200300400500600700Aggregate Bandwidth (Mbps)Per-FlowPer-Source (Out)Per-Source (In)PS-LPS-P(a) Experiment
(b) Full bisection bandwidth network
(c) 4(cid:2) Oversubscribed network
Figure 10: Simple scenario with two tenants having equal number of VMs and a uniform deployment. Tenant A communicates using a pairwise
one-to-one communication pattern and B all-to-all. (b) PS-P, PS-L and Per-Source policies achieve perfect proportionality in presence of full
bisection bandwidth; (c) PS-N provides the best allocation ratio among the compared policies when the network is oversubscribed.
Figure 11: PS-N is more proportional than PS-P.
Figure 12: PS-P gives the highest bandwidth guarantee.
allocation would allocate equal network shares to both A and B, and
would appear as a horizontal line at y = 1. However, this allocation
is not always possible, since the MapReduce communication pat-
tern is limited by min(M; R)(cid:2)C, where C is the access link capacity.
Figure 11 shows that PS-P provides direct proportionality between
min(M; R) and tenant B’s bandwidth: each VM is guaranteed its fair
share of the network up to the core, and MapReduce will commu-
nicate with an aggregate throughput of min(M; R) (cid:2) Bw, where Bw
is the per-VM bandwidth guarantee (Bw = C
2 in this case). PS-N,
on the other side, provides better proportionality than PS-P, i.e., the
ratio of allocated bandwidths is closer to 1:0.
Proportionality, however, comes at a cost as expected from the
tradeoﬀs presented in §2; the minimum bandwidth allocation of a
VM provided by PS-N can become arbitrarily small in practice. Fig-
ure 12 presents the minimum guarantee provided by PS-N and PS-P
for a scenario similar to the one shown in Figure 7 but with a slightly
diﬀerent communication pattern. In addition to the N-to-one com-
munication pattern employed by A (i.e., all of A’s VMs send to A1),
each of the N VMs sending traﬃc to A1 also communicates with
another of the N VMs, so that they can utilize the rest of the links.
Without this additional (cid:280)ow from each sender, the throughput of
A’s VMs would be limited by the traﬃc going into A1, and the min-
imum bandwidth would be aﬀected by the hose model rather than
the allocation scheme. We study the variation of minimum of the
bandwidth allocations in such a scenario with increasing network
size. Figure 12 shows that PS-P can maintain the same minimum
guarantee regardless of the network size (for the same oversubscrip-
tion ratio), while PS-N’s minimum bandwidth decreases with the
network size given this “bad” communication pattern.
5.3 Trace-driven Simulations
So far we have evaluated the proposed and existing network al-
location policies in synthetic, small-scale scenarios. To understand
their large-scale implications, we performed some experiments us-
ing MapReduce traces from a 3200-node Facebook production data
center [12]. Our goal is to identify the network shares that diﬀerent
MapReduce jobs would achieve given diﬀerent network allocation
techniques.
We consider a one hour window in the trace and observe the
number of jobs involved in active shuﬄe at a minute’s interval in that
hour. In our trace, an average number of 73 jobs were in active shuf-
(cid:280)e at any given time, with a maximum of 100 jobs and a minimum
of 49. We then create a snapshot in time at the point when most jobs
are in active shuﬄe, generate corresponding traﬃc matrices and ob-
serve the allocation of diﬀerent policies. We infer the network posi-
tion of each mapper and reducer based on the IP addresses and the
names in the log (cid:277)les. e inference shows a heterogeneous cluster
with an average of four active tasks per server, with some servers
having up to 12 tasks. Due to the lack of knowledge about the over-
subscription factor, we consider both full bisection bandwidth and
4(cid:2) oversubscribed topologies with 20 aggregate switches and 160
racks. Note that the trace does not contain information about the
processing times of each job; hence, we do not try to emulate entire
running times of the jobs.
Figure 13(a) presents the network allocations for all the active
jobs based on their size (measured by the number of mapper and re-
ducer tasks) for the full bisection bandwidth case. e total network
utilization was the same since all evaluated allocations are work con-
serving disciplines, but the distribution of bandwidth among jobs is
diﬀerent. During the shuﬄe phase, a MapReduce job with M map-
pers and R reducers can launch M (cid:2) R (cid:280)ows, one from each of the
mappers to each of the reducers. In such a scenario, a Per-Flow al-
location can potentially give a network share proportional to M(cid:2) R
to a job whose payment was proportional to M + R (the number of
map and reduce slots it was allotted). us network allocations can
be quadratic making allocations signi(cid:277)cantly unfair to small jobs.
A2 B2 A1 B1 A3 B3 A4 B4 A5 B5 A6 B6 A7 B7 A8 B8 Core  Bandwidth Agg. Bandwidth Rack Bandwidth Per-FlowPer-SourcePS-LPS-PPS-N020040060080010001200Aggregate Bandwidth (Mbps)ABABABABABRackAggCorePer-FlowPer-SourcePS-LPS-PPS-N020040060080010001200Aggregate Bandwidth (Mbps)ABABABABABRackAggCore0.20.30.40.50.60.70.80.91.0M=R0.20.30.40.50.60.70.80.91.0Ratio of BandwidthPS-PPS-N020406080100120# Hosts01020304050Min Bandwidth (Mbps)Per-FlowPer-SourcePS-LPS-PPS-N(a) All Jobs
(b) Small Jobs Only
Figure 13: Network allocation of MapReduce jobs on a 3200-node Facebook production cluster under diﬀerent policies.