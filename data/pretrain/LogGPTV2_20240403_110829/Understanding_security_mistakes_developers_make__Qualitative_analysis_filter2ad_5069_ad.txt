of encryption and/or integrity checks in the SL and SC prob-
lems: use of an algorithm without any secret component, i.e.,
a key (P=8, V=8), weak algorithms (P=4, V=5), or homemade
encryption (P=2, V=2). As an example of a weak algorithm,
SL-69 simply XOR’d key-length chunks of the text with the
user-provided key to generate the ﬁnal ciphertext. Therefore,
the attacker could simply extract two key-length chunks of
the ciphertext, XOR them together and produce the key.
The next issue identiﬁes a weak access-control design for
the MD problem, which could not handle all use cases (P=5,
V=6). For example, MD-14 implemented delegation improp-
erly. In the MD problem, a default delegator may be set by the
administrator, and new users should receive the rights this del-
egator has when they are created. However, MD-14 granted
rights not when a user was created, but when they accessed
particular data. If the default delegator received access to data
between time of the user’s creation and time of access, the
user would be incorrectly provided access to this data.
The ﬁnal issue (potentially) applies to all three problems:
use of libraries that could lead to memory corruption. In this
case, team SL-81 chose to use strcpy when processing user
input, and in one instance failed to validate it, allowing an
overﬂow. Rather than code this as Mistake, we considered it a
bad choice because a safe function (strlcpy) could have been
used instead to avoid the security issue.
4.2.2 Conceptual Error
Teams that chose a secure design often introduced a vulner-
ability in their implementation due to a conceptual misun-
derstanding (rather than a simple mistake). This Conceptual
Error sub-type manifested in six ways.
Most commonly, teams used a ﬁxed value when an ran-
dom or unpredictable one was necessary (P=26, V=26). This
included using hardcoded account passwords (P=8, V=8), en-
cryption keys (P=3, V=3), salts (P=3, V=3), or using a ﬁxed
IV (V=12, N=12).
[] byte ) ( err error ) {
var box [] byte
var nonce [24] byte
1 var nextNonce uint64 = 1337
2 ...
3 func sendMessage ( conn * net . Conn , message
4
5
6
7
8
9
10
11
12
13
14
15
16
17 }
nextNonce ++
writer := * conn
err = binary . Write ( writer , byteOrder , packet )
...
byteOrder . PutUint64 ( nonce [:] , nextNonce )
box = secretbox . Seal (box , message , & nonce ,
var packet = Packet { Size : uint64 ( len ( box )) ,
& sharedSecret )
Nonce : nextNonce }
Listing 1: SC-76 Used a hardcoded IV seed.
Sometimes chosen values were not ﬁxed, but not sufﬁ-
ciently unpredictable (P=7, V=8). This included using a
timestamp- based nonce, but making the accepted window too
large (P=3, V=3); using repeated nonces or IVs (P=3, V=4);
or using predictable IVs (P=1, V=1). As an example, SC-76
attempted to use a counter-based IV to ensure IV uniqueness.
Listing 1 shows that nonce nextNonce is incremented after
each message. Unfortunately, the counter is re-initialized ev-
ery time the client makes a new transaction, so all messages
to the server are encrypted with the same IV. Further, both the
client and server initialize their counter with the same number
(1337 in Line 1 of Listing 1), so the messages to and from the
server for the ﬁrst transaction share an IV. If team SC-76 had
maintained the counter across executions of the client (i.e., by
persisting it to a ﬁle) and used a different seed for the client
and server, both problems would be avoided.
Other teams set up a security mechanism correctly, but only
protected a subset of necessary components (P=9, V=10). For
example, Team SL-66 generated a MAC for each log entry
separately, preventing an attacker from modifying an entry,
but allowing them to arbitrarily delete, duplicate, or reorder
log entries. Team SC-24 used an HTTP library to handle
client-server communication, then performed encryption on
each packet’s data segment. As such, an attacker can read or
116    29th USENIX Security Symposium
USENIX Association
manipulate the HTTP headers; e.g., by changing the HTTP
return status the attacker could cause the receiver to drop a
legitimate packet.
In three cases, the team passed data to a library that failed
to handle it properly (P=3, V=3). For example, MD-27 used
an access-control library that takes rules as input and returns
whether there exists a chain of delegations leading to the
content owner. However, the library cannot detect loops in
the delegation chain. If a loop in the rules exists, the library
enters an inﬁnite loop and the server becomes completely
unresponsive. (We chose to categorize this as a Conceptual
Error vulnerability instead of a Mistake because the teams vi-
olate the library developers’ assumption as opposed to making
a mistake in their code.)
1 self . db = self . sql . connect ( filename , timeout =30)
2 self . db . execute ( ’ pragma key =" ’ + token + ’"; ’)
3 self . db . execute ( ’ PRAGMA kdf_iter = ’
4
5 self . db . execute ( ’ PRAGMA cipher_use_MAC = OFF ; ’)
6 ...
+ str ( Utils . KDF_ITER ) + ’; ’)
Listing 2: SL-22 disabled automatic MAC in SQLCipher
library.
Finally, one team simply disabled protections provided
transparently by the library (P=1, V=1). Team SL-22 used
the SQLCipher library to implement their log as an SQL
database. The library provides encryption and integrity checks
in the background, abstracting these requirements from the
developer. Listing 2 shows the code they used to initialize the
database. Unfortunately, on line 5, they explicitly disabled the
automatic MAC.
4.3 Mistake
Finally, some teams attempted to implement the solution cor-
rectly, but made a mistake that led to a vulnerability. The
mistake type is composed of ﬁve sub-types. Some teams did
not properly handle errors putting the program into an observ-
ably bad state (causing it to be hung or crash). This included
not having sufﬁcient checks to avoid a hung state, e.g., in-
ﬁnite loop while checking the delegation chain in the MD
problem, not catching a runtime error causing the program to
crash (P=5, V=9), or allowing a pointer with a null value to be
written to, causing a program crash and potential exploitation
(P=1, V=1).
# First we check for tiemstamp delta
dateTimeStamp = datetime . strptime ( timestamp ,
deltaTime = datetime . utcnow () - dateTimeStamp
if deltaTime . seconds > MAX_DELAY :
raise Exception (" ERROR : Expired nonce ")
’%Y -%m -% d %H :% M :% S .% f ’)
1 def checkReplay ( nonce , timestamp ):
2
3
4
5
6
7
8
9
10
11
# The we check if it is in the table
global bank
if ( nonce in bank . nonceData ):
raise Exception (" ERROR : Reinyected package ")
Listing 3: SC-80 forgot to save the nonce.
Other mistakes led to logically incorrect execution behav-
iors. This included mistakes related to the control ﬂow logic
(P=5, V=10) or skipping steps in the algorithm entirely. List-
ing 3 shows an example of SC-80 forgetting a necessary step
in the algorithm. On line 10, they check to see if the nonce
was seen in the list of previous nonces (bank.nonceData)
and raise an exception indicating a replay attack. Unfortu-
nately, they never add the new nonce into bank.nonceData,
so the check on line 10 always returns true.
5 Analysis of Vulnerabilities
This section considers the prevalence (RQ1) of each vulner-
ability type as reported in Table 2 along with the attacker
control (RQ2), and exploitability (RQ3) of introduced types.
Overall, we found that simple implementation mistakes (Mis-
take) were far less prevalent than vulnerabilities related to
more fundamental lack of security knowledge (No Imple-
mentation, Misunderstanding). Mistakes were almost always
exploited by at least one other team during the Break It phase,
but higher-level errors were exploited less often. Teams that
that were careful to minimize the footprint of security-critical
code were less likely to introduce mistakes.
5.1 Prevalence
To understand the observed frequencies of different types
and sub-types, we performed planned pairwise comparisons
among them. In particular, we use a Chi-squared test—
appropriate for categorical data [32]—to compare the number
of projects containing vulnerabilities of one type against the
projects with another, assessing the effect size (φ) and signiﬁ-
cance (p-value) of the difference. We similarly compare sub-
types of the same type. Because we are doing multiple com-
parisons, we adjust the results using a Benjamini-Hochberg
(BH) correction [11]. We calculate the effect size as the mea-
sure of association of the two variables tested (φ) [22, 282-
283]. As a rule of thumb, φ ≥ 0.1 represents a small effect,
≥ 0.3 a medium effect, and ≥ 0.5 a large effect [21]. A p-
value less than 0.05 after correction is considered signiﬁcant.
Teams often did not understand security concepts. We
found that both types of vulnerabilities relating to a lack of se-
curity knowledge—No Implementation (φ = 0.29, p < 0.001)
and Misunderstanding (φ = 0.35, p < 0.001)—were signiﬁ-
cantly more likely (roughly medium effect size) to be intro-
duced than vulnerabilities caused by programming Mistakes.
We observed no signiﬁcant difference between No Implemen-
tation and Misunderstanding (φ = 0.05, p = 0.46). These
results indicate that efforts to address conceptual gaps should
USENIX Association
29th USENIX Security Symposium    117
Variable
Value
Problem
Min Trust
Popularity
LoC
SC
MD
SL
False
True
C (91.5)
1274.81
Log
Estimate
CI
–
6.68
0.06
–
0.36
1.09
0.99
–
–
[2.90, 15.37]
[0.01, 0.43]
[0.17, 0.76]
[1.02, 1.15]
[0.99, 0.99]
p-value
–
< 0.001*
0.006*
–
0.007*
0.009*
0.006*
*Signiﬁcant effect
– Base case (Log Estimate deﬁned as 1)
Table 3: Summary of regression over Mistake vulnerabilities.
Pseudo R2 measures for this model were 0.47 (McFadden)
and 0.72 (Nagelkerke).
be prioritized. Focusing on these issues of understanding, we
make the following observations.
Unintuitive security requirements are commonly skipped.
Of the No Implementation vulnerabilities, we found that the
Unintuitive sub-type was much more common than its All
Intuitive (φ = 0.44, p < 0.001) or Some Intuitive (φ = 0.37,
p < 0.001) counterparts. The two more intuitive sub-types
did not signiﬁcantly differ (φ = 0.08, p = 0.32) This indicates
that developers do attempt to provide security — at least when
incentivized to do so — but struggle to consider all the unin-
tuitive ways an adversary could attack a system. Therefore,
they regularly leave out some necessary controls.
Teams often used the right security primitives, but did
not know how to use them correctly. Among the Misunder-
standing vulnerabilities, we found that the Conceptual Error
sub-type was signiﬁcantly more likely to occur than Bad
Choice (φ = 0.23, p = .003). This indicates that if developers
know what security controls to implement, they are often able
to identify (or are guided to) the correct primitives to use.
However, they do not always conform to the assumptions of
“normal use” made by the library developers.
Complexity breeds Mistakes. We found that complexity
within both the problem itself and also the approach taken by
the team has a signiﬁcant effect on the number of Mistakes
introduced. This trend was uncovered by a poisson regression
(appropriate for count data) [15, 67-106] we performed for
issues in the Mistakes type.6
Table 3 shows that Mistakes were most common in the
MD problem and least common in the SL problem. This is
shown in the second row of the table. The log estimate (E) of
6.68 indicates that teams were 6.68× more likely to introduce
Mistakes in MD than in the baseline secure communication
6We selected initial covariates for the regression related to the language
used, best practices followed (e.g., Minimal Trusted Code), team character-
istics (e.g., years of developer experience), and the contest problem. From
all possible initial factor combinations, we chose the model with minimum
Bayesian Information Criteria—a standard metric for model ﬁt [63]. We
include further details of the initial covariates and the selection process in
Appendix C, along with discussion of other regressions we tried but do not
include for lack of space.
case. In the fourth column, the 95% conﬁdence interval (CI)
provides a high-likelihood range for this estimate between
2.90× and 15.37×. Finally, the p-value of < 0.001 indicates
that this result is signiﬁcant. This effect likely reﬂects the fact
that the MD problem was the most complex, requiring teams
to write a command parser, handle network communication,
and implement nine different access control checks.
Similar logic demonstrates that teams were only 0.06×
as likely to make a mistake in the SL problem compared to
the SC baseline. The SL problem was on the other side of
the complexity spectrum, only requiring the team to parse
command-line input and read and write securely from disk.
Similarly, not implementing the secure components mul-
tiple times (Minimal Trusted Code) was associated with an
0.36× decrease in Mistakes, suggesting that violating the
“Economy of Mechanism” principle [68] by adding unnec-
essary complexity leads to Mistakes. As an example of this
effect, MD-74 reimplemented their access control checks four
times throughout the project. Unfortunately, when they real-
ized the implementation was incorrect in one place, they did
not update the other three.
Mistakes are more common in popular languages. Teams
that used more popular languages are expected to have a
1.09× increase in Mistakes for every one unit increase in pop-
ularity over the mean Popularity7 (p = 0.009). This means,
for example, a language 5 points more popular than average
would be associated with a 1.54× increase in Mistakes. One
possible explanation is that this variable proxies for experi-
ence, as many participants who used less popular languages
also knew more languages and were more experienced.
Finally, while the LoC were found to have a signiﬁcant
effect on the number of Mistakes introduced, the estimate is
so close to one as to be almost negligible.
No signiﬁcant effect observed for developer experience or
security training. Across all vulnerability types, we did not