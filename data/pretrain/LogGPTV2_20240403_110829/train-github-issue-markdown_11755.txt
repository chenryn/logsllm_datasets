For training in translation and summarization tasks, `decoder_input_ids` should be provided. If `decoder_input_ids` is not provided, the model will automatically generate this tensor by shifting the `input_ids` to the right, following the denoising pre-training method described in the relevant paper.

However, if `labels` are provided, the model will create the `decoder_input_ids` tensor by shifting the `labels` instead. This behavior is implemented in the BART model's source code, specifically in the `modeling_bart.py` file, as shown in the following lines:

```python
if labels is not None:
    if decoder_input_ids is None and decoder_inputs_embeds is None:
        decoder_input_ids = shift_tokens_right(
            labels, self.config.pad_token_id, self.config.decoder_start_token_id
        )
```

For more details, refer to the Hugging Face documentation:
- [BartForConditionalGeneration.forward.decoder_input_ids](https://huggingface.co/docs/transformers/v4.16.2/en/model_doc/bart#transformers.BartForConditionalGeneration.forward.decoder_input_ids)
- [Source Code](https://github.com/huggingface/transformers/blob/db7d6a8/transformers/src/transformers/models/bart/modeling_bart.py#L1320-L1324)