DenseNet
Fully Connected
Fully Connected
Train Acc.
100.0%
Test Acc.
39.8%
100.0%
95.2%
100.0%
64.3%
48.6%
77.5%
Distinguish D from DΔ
Distinguish D from ¯D Distinguish DΔ from ¯D
62.1%
63.3%
58.4%
64.4%
75.4%
74.6%
68.4%
73.8%
71.3%
71.5%
67.2%
71.2%
Member instances
Non-member instances
m
r
o
n
t
n
e
i
d
a
r
G
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0
20
40
60
Training epoch
80
Fig. 3: Gradient norms of the last layer during learning epochs for
member and non-member instances (for Purchase100).
underlying distribution, but he does not know their member
and non-member labels. In this case, the attacker classiﬁes the
tested records into two clusters as described in Section II-D.
We implemented our attack and compared its performance
to Shadow models of Shokri et al. [6] introduced earlier. We
train our unsupervised models on various datasets based on the
training and test dataset sizes in Table II. We train a single
Shadow model on each of Texas100 and Purchase100 datasets
using training sizes according to Table II. The training sets of
the Shadow models do no overlap with the training sets of
the target models. For the CIFAR100 dataset, however, our
Shadow model uses a training dataset that overlaps with the
target model’s dataset, as we do not have enough instances
(we train each model with 50,000 instances out of the total
60,000 available records).
After the training, we use the Spectral clustering algo-
rithm [14] to divide the input samples into two clusters. As
shown earlier (Figure 4), the member instances have smaller
gradient norm values. Therefore, we assign the member label
to the cluster with a smaller average gradient norm, and the
non-member label to the other cluster.
Table VII compares the accuracy of our unsupervised attack
with shadow training [6] on various datasets and architectures.
We see that our approach offers a noticeably higher accuracy.
The intuition behind our attack working is that the encoded
values of our unsupervised algorithm present different distri-
butions for member and non-member samples. This can be
seen in Figure 5 for various datasets and architectures.
C. Stand-Alone Setting: Attacking Fine-Tuned Models
We investigate privacy leakage of ﬁne-tuned target models.
In this scenario, the victim trains a model with dataset D, then
he uses a dataset DΔ to ﬁne-tune the trained model to improve
its performance. Hence, the attacker has two snapshots of the
trained model, one using only D, and one for the same model
which is ﬁne-tuned using DΔ. We assume the attacker has
access to both of the trained models (before and after ﬁne-
tuning). We are interested in applying the membership infer-
ence attack in this scenario, where the goal of the adversary is
to distinguish between the members of D, DΔ, and ¯D, which
is a set of non-members.
We use the same training dataset as in the previous experi-
ments (Table II); we used 60% of the train dataset as D and the
rest for DΔ. Table IX shows the train, test, and attack accuracy
for different scenarios. As can be seen, the attacker is able to
distinguish between members (in D or DΔ) and non-members
( ¯D) with accuracies similar to previous settings. Additionally,
the attacker can also distinguish between the members of D
and DΔ with reasonably high accuracies.
D. Federated Learning Settings: Passive Inference Attacks
Table XI shows the dataset sizes used in our federated
attack experiments. For the CIFAR100 experiment with a
(cid:24)(cid:21)(cid:25)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:51:43 UTC from IEEE Xplore.  Restrictions apply. 
n
o
i
t
c
a
r
F
0.8
0.6
0.4
0.2
0.0
Member
Non-member
n
o
i
t
c
a
r
F
1.0
0.8
0.6
0.4
0.2
0.0
Member
Non-member
n
o
i
t
c
a
r
F
0.6
0.4
0.2
0.0
Member
Non-member
0
500
1500
2000
1000
Gradient norm
2500
3000
0
50
150
100
200
Gradient norm
250
300
0
50
150
200
100
Gradient norm
250
300
(a) CIFAR100-Alexnet
(b) CIFAR100-Densenet
(c) CIFAR100-Resnet
Fig. 4: The distribution of gradient norms for member and non-member instances of different pretrained models.
n
o
i
t
c
a
r
F
1.0
0.8
0.6
0.4
0.2
0.0
Non-member
Member
0
100 200 300 400 500
Encoded value
n
o
i
t
c
a
r
F
1.0
0.8
0.6
0.4
0.2
0.0
Non-member
Member
0
100 200 300 400 500
Encoded value
n
o
i
t
c
a
r
F
0.6
0.4
0.2
0.0
Non-member
Member
0
500 1000 1500 2000 2500
Encoded value
(a) DenseNet-CIFAR100
(b) ResNet-CIFAR100
(c) AlexNet-CIFAR100
Fig. 5: The distribution of the encoded values (i.e., the attack output) for the member and non-member instances of our
unsupervised algorithm are distinguishable. This is the intuition behind the high accuracy of our unsupervised attack.
e
v
i
t
i
s
o
P
e
u
r
T
1.0
0.8
0.6
0.4
0.2
0.0
0.0
0.2
Small uncert diff (0.25)
Medium uncert diff (0.36)
Large uncert diff (0.59)
Random guess
0.4
0.6
False Positive
0.8
1.0
Fig. 6: Attack’s ROC for three different classes of data with
large, medium, and small prediction uncertainty values (pre-trained
CIFAR100-Alextnet model in the stand-alone scenario).
local attacker, each participant uses 30,000 instances to train,
which overlaps between various participants due to non-
sufﬁcient number of instances. For all the other experiments,
the participants use non-overlapping datasets. In the following,
we present the attack in various settings.
The Passive Global Attacker:
In this scenario, the attacker
(the parameter aggregator) has access to the target model’s
parameters over multiple training epochs (see Section II-B).
Thus, he can passively collect all
the parameter updates
from all of the participants, at each epoch, and can perform
the membership inference attack against each of the target
participants, separately.
Due to our limited GPU resources, our attack observes each
target participant during only ﬁve (non-consecutive) training
epochs. Table XII shows the accuracy of our attack when
it uses different sets of training epochs (for the CIFAR100
dataset with Alexnet). We see that using later epochs, sub-
stantially increases the attack accuracy. Intuitively, this is
because the earlier training epochs contain information of the
generic features of the dataset, which do not leak signiﬁcant
membership information, however, the later epochs contain
more membership information as the model starts to learn the
outliers in such epochs [15].
Table X presents the results of this attack on different
datasets. For the Purchase100 and Texas100 datasets we use
the [40, 60, 80, 90, 100] training epochs, and for the CIFAR100
(cid:24)(cid:21)(cid:26)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:51:43 UTC from IEEE Xplore.  Restrictions apply. 
TABLE X: Attack accuracy in the federated learning setting. There are 4 participants. A global attacker is the central parameter aggregator,
and the local attacker is one of the participants. The global attacker performs the inference against each individual participant, and we report
the average attack accuracy. The local attacker performs the inference against all other participants. The passive attacker follows the protocol
and only observes the updates. The active attacker changes its updates, or (in the case of a global attack) isolates one participant by not
passing the updates of other participants to it, in order to increase the information leakage.
Target Model
Global Attacker (the parameter aggregator)
Local Attacker (a participant)
Passive
Active
Dataset
CIFAR100
CIFAR100
Texas100
Purchase100
Architecture
Alexnet
DenseNet
Fully Connected
Fully Connected
Passive
85.1%
79.2%
66.4%
72.4%
Gradient Ascent
88.2%
82.1%
69.5%
75.4%
Active
Isolating
89.0%
84.3%
69.3%
75.3%
Isolating Gradient Ascent
Gradient Ascent
92.1%
87.3%
71.7%
82.5%
73.1%
72.2%
62.4%
65.8%
76.3%
76.7%
66.4%
69.8%
m
r
o
n
t
n
e
i
d
a
r
G
5
4
3
2
1
0
Target members
Non-target members
5
4
3
2
1
0
m
r
o
n
t
n
e
i
d
a
r
G
Target non-members
Non-target non-members
m
r
o
n
t
n
e
i
d
a
r
G
5
4
3