we extract a frequency distribution for each to generate empirical
cumulative distribution functions (CDF). At this stage, we have
the choice of either stopping with the empirical distributions
or performing curve-ﬁtting to analytical equations [31]. For this
work, we choose the former approach for simplicity and because
it accurately represents observed data (for instance, capturing
outliers), and leave the derivation and use of analytic distributions
to future work.
3.2 Extracting network characteristics
Given models of individual ﬂows crossing a target network
link, we next require an understanding of the characteristics of the
network links responsible for transmitting data to and from the
target link for the hosts communicating across the link. Of course,
we can only approximate the dynamically changing bandwidth,
latency and loss rate characteristics of all links that carry ﬂows that
eventually arrive at our target from a single packet trace. While we
developed a number of techniques independently and while it is
impossible to determine the extent to which our approach differs
from techniques in the literature (where important details may be
omitted and source code is often unavailable), we do not necessarily
innovate in our ability to passively extract wide-area network
conditions from an existing packet trace. Rather, our contribution
is to show that it is possible to both capture and replay these
network conditions with sufﬁcient ﬁdelity to reproduce essential
characteristics of the original trace. § 5 quantiﬁes the extent to
which we are successful. Likewise, we assume that the modeled
parameters (CDFs) are stationary for the duration of the trace.
Augmenting our models to account for the changing nature [45]
of these (for instance, changing bandwidth) is part of ongoing
work. For the traces we consider, non-stationarity has not been a
signiﬁcant obstacle.
We extract network characteristics as follows. For each host
(unique IP address) communicating across the target link we wish
to measure the delays, capacities and loss rates of the set of links
connecting the host to the target link as shown in Figure 1. For
simplicity, we aggregate all links from a source to the target and the
links from the target to the destination into single logical links with
aggregate capacity, loss rate, and latency corresponding to the links
that make up this aggregate. Thus, in our model we employ four
separate logical links (assuming assymetric link characteristics)
responsible for carrying trafﬁc to and from the target link for all
communicating hosts. For cases where sufﬁcient information is
not available—for instance if we do not see ACKs in the reverse
direction—we approximate link characteristics for the host as the
mean of the observed values for all other hosts on its side of the
target link for the same application.
Link delays: Consider a ﬂow from a client C initiating a TCP
connection to a server S as shown in Figure 1. We use each ﬂow in
the underlying trace between these hosts as samples for the four sets
of links responsible for carrying trafﬁc between the ﬂow endpoints.
We record four quantities for packets arriving at the target link
in both directions. First, we record the time difference between a
SYN (from C) and the corresponding SYN+ACK (from S) as a
sample to estimate the sum of link delays l2 and l3 (Figure 2).
Next, we measure the difference between the SYN+ACK and the
corresponding ACK packet as samples to estimate the sum of
delays l4 and l1 (not shown). We use the difference between a
response packet and its corresponding ACK (from C) to estimate
the sum of delays l4 and l1 as shown in Figure 3. Finally, we
measure the time between a data packet and its corresponding ACK
(from S) as further samples for l2 + l3 (not shown).
For this analysis, we only consider hosts that have 5 or more
sample values in the trace. We use the median (per host) of the
sample values to approximate the sum of link delays (l1+l4 or
l2+l3). We chose the median because in our current conﬁguration
we assign static latency values to the links in our topology and the
median should be representative of the time it takes for a packet
to reach the target link once it leaves hosts on either end. One
assumption behind our work is that ﬂows follow symmetric paths
in the forward and reverse direction, allowing us to assign values
for l1 and l4 from samples of l1 + l4. Figure 5 for instance shows
the two-way delay for hosts on either side of the target link in the
Auck trace (§5).
CLIENT   C
C1,l1,p1
C4, l4, p4
TARGET
LINK
SERVER  
  p 2
C 2 ,
l 2 ,
  p 3
l 3 ,
C 3 ,
CLIENT   C
SYN
SERVER  
TARGET
S Y N / A C K
LINK
CLIENT   C
   ACK
SERVER   S
TARGET
LINK
R S P
Figure 1: Approximating wide-area characteristics.
Path from C to the target link approximated as a single
link with capacity C1, delay l1 and loss rate p1.
Figure 2: Time difference between a SYN
and the corresponding SYN+ACK is an
estimate of l2+l3.
Figure 3: Time difference between a re-
sponse packet and the corresponding ACK
is an estimate of l4+l1.
 HTTP
GENERATOR
P2P
FTP
GENERATOR
UDP
LISTENER
 HTTP
LISTENER
10 Mbps, 100 ms,
       2% loss rate
SMTP
LISTENER
 HTTP
LISTENER
 HTTP
GENERATOR
P2P
UDP
GENERATOR
FTP
LISTENER
TARGET LINK
100 Mbps, 20 ms delay
1% link loss rate
1Gbps, 1ms delay
0.1 % loss rate
SMTP
GENERATOR
FTP
LISTENER
Figure 4: Target link modeled as a Dumb-bell.
Link Capacities: We employ the following variant of packet-
pair techniques to estimate link capacities. We extract consecutive
data packets not separated by a corresponding ACK from the other
side. The time difference between these packet-pairs gives an esti-
mate [11, 31] of the time for the packets to traverse the bottleneck
link from the host to the target link. It is then straightforward to
calculate the bottleneck capacity using the formula:
LinkCapacity * TimeDifference = PacketSize.
With widespread use of delayed ACKs in TCP, half of the
packets are sent as packet-pairs. To account for this shortcoming,
we sort the packet-pair time separation values in ascending order
and ignore the bottom half. Of course, it is known that packet-pair
techniques overestimate the bottleneck capacity [11] and moreover
doing passive estimation means that we cannot control which
packets are actually sent in pairs. To account for this, we use the
50th percentile of the sample values to approximate path capacity
from a given host to the target link. For instance, in Figure 3, we can
estimate c1 and c3 in this fashion. We assume the incoming link to
a host has capacity at least as large as the outgoing link and hence
we approximate c4 and c2 to the values c1 and c3 respectively.
However, post bottleneck link queueing may reduce packet
spacing, inﬂating our capacity estimates. Likewise, packets sent in
pairs might not arrive at the bottleneck in pairs [19]. Improving
our capacity estimates based on these recent studies is part of our
ongoing work.
Loss Rates: We measure loss rates using retransmissions and
a simple algorithm based on [5]. The algorithm starts by arranging
TCP sequence numbers corresponding to packets of a ﬂow, based
on the timestamps. Next, if there is a missing sequence number in
the timestamp sequence, it usually means that the corresponding
packet was lost en-route to the target link. This is established if
the packet (and the corresponding TCP sequence number) is seen
at a later time indicating retransmission. Such a missing sequence
number is called a “hole” and we count it as a loss event for
estimating p1 in Figure 1. Likewise, if there is an out-of-order TCP
packet (retransmission) with no corresponding hole, it is then likely
that the packet arrived at the target but was lost en-route to the
destination. We use this loss event to estimate p2. p3 and p4 are
estimated in a similar manner when we see ﬂows in the opposite
direction.
Of course the naive algorithm described above needs to
be improved to account for spurious timeouts, correlated losses,
routing changes, etc., and we refer the reader to [5] for a detailed
explanation.
Figure 7 shows the loss rate values extracted for both direc-
tions of trafﬁc of a trace. We assume that losses experienced by a
ﬂow are typically not caused by the trafﬁc we measure directly,
i.e., losses on upstream and downstream links are caused by
ambient congestion elsewhere in the network and not at some other
congestion point shared by multiple hosts from our trace. Using this
assumption, we assign distinct loss rates for links connecting each
host to the target rather than attempting to account for such shared
congestion.
3.3 Generating Swing packet traces
Given application and network models, we are now in position
to generate the actual packet trace based on these characteristics.
Our strategy is to generate live communication among endpoints
exchanging packets across an emulated network topology. We im-
plement custom generators and listeners and pre-conﬁgure each to
initiate communication according to the application characteristics
extracted in the previous step. We also conﬁgure the network topol-
ogy to match the bandwidth, latency, and loss rate characteristics
of the original trace as described below. We designate a single link
in the conﬁgured topology as the target. Our output trace then is
simply a tcpdump of all packets traversing the target during the
duration of a Swing experiment. We run Swing for 5 minutes more
than the duration of the target trace and then ignore the ﬁrst 5
minutes to focus on steady state characteristics.
For each application in the original trace, we generate a list
of sessions and session start times according to the distribution
of inter-session times measured in the original trace. We then
randomly assign individual sessions to generators to seed the
corresponding conﬁguration ﬁle. For each session, we set values
for number of RREs, inter-RRE times, number of connections per
RRE, etc., from the distributions measured in the original trace.
At startup, each generator reads a conﬁguration ﬁle that
speciﬁes: i) its IP address, ii) a series of relative time-stamps
designating session initiation times (as generated above), iii) the
number of RREs for each session, iv) the destination address
and communication pattern for each connection within an RRE,
and v) the packet size distribution for each connection. For our
target scenarios, we typically require thousands of generators, each
conﬁgured to generate trafﬁc matching the characteristics of a
single application/host pair. We create the necessary conﬁguration
ﬁles for all generators according to our extracted application
characteristics. We similarly conﬁgure all listeners with their IP
addresses and directives for responding to individual connections,
e.g., how long to wait before responding (server think time) and the
size of the response.
We run the generators and listeners on a cluster of commodity
workstations, multiplexing multiple instances on each workstation
depending on the requirements of the generated trace. To date,
we have generated traces up to approximately 200Mbps. For the
experiments in this paper, we use 11 2.8 Ghz Xeon processors
running Linux 2.6.10 (Fedora Core 2) with 1GB memory and
integrated Gigabit NICs. For example, for a 200Mbps trace, assum-
ing an even split between generators and listeners (ﬁve machines
each), each generator would be responsible for accurately initiating
ﬂows corresponding to 40Mbps on average. Each machine can
comfortably handle the average case though there are signiﬁcant
bursts that make it important to “over-provision” the experimental
infrastructure.
Critical to our methodology is conﬁguring each of the ma-
chines to route all packets to a single ModelNet [39] core re-
sponsible for emulating the hop-by-hop characteristics of a user-
speciﬁed wide-area topology. The source code for ModelNet is
publicly available and we run version 0.99 on a machine identical
to those hosting the generators and listeners. Brieﬂy, ModelNet
subjects each packet to the per-hop bandwidth, delay, queueing,
and loss characteristics of the target topology. ModelNet operates
in real time, meaning that it moves packets from queue to queue
in the target topology before forwarding it on to the destination
machine (one of the 11 running generators/listeners) assuming that
the packet was not dropped because it encountered a full queue
or a lossy link. Earlier work [39] validates ModelNet’s accuracy
using a single core at trafﬁc rates up to 1Gbps (we can generate
higher speed traces in the future by potentially running multiple
cores [43]). Simple conﬁguration commands allow us to assign
multiple IP addresses (for our experiments, typically hundreds) to
each end host.
3.3.1 Creating an emulation topology
The ﬁnal question revolves around generating the network
topology with appropriate capacity, delay, and loss rate values
to individual links in the topology. We begin with a single bi-
directional link that represents our target link (Figure 4). We assign
a bandwidth and latency (propagation delay) to this link based
on the derived characteristics of the original traced link. The next
step is to add nodes on either side of this target to host generators
and listeners. Ideally, we would create one node/edge in our target
topology for every unique host in the original trace. However,
depending on the size of the trace and the capacity of our emulation
environment, it may be necessary to collapse multiple sources from
the original trace onto a single IP address/generator in our target
topology 1. For the experiments described in the rest of the paper,
we typically collapse up to 10,000 endpoints (depending on the
size of the trace) from the original trace onto 1,000 endpoints in
our emulation environment. Our mapping process ensures that the
generated topology reﬂects characteristics of the most active hosts.
We base the number of generators we assign to each appli-
cation on the bytes contributed by each application in the original
trace. For instance, if 60% of the bytes in the original trace is HTTP,
then 60% of our generators (each with a unique IP address) in the
emulation topology will be responsible for generating HTTP trafﬁc.
1Such a collapsing impacts the IP address distribution of ﬂows
crossing the target link (i.e., it reduces the number of unique IP
addresses in our generated trace relative to the original trace).
However, as shown in § 5, this does not affect aggregate trace
characteristics, such as bandwidth, packet inter-spacing etc.
We discuss the limitations of this approach in § 6. Next, we assign
hosts to both sides of the target link based on the number of bytes
ﬂowing in each direction in the original trace for that application.
For instance, if there is twice as much HTTP trafﬁc ﬂowing in
Figure 4 from left to right as there is from right to left, we would
then have twice as many HTTP hosts on the left as on the right in
the emulated topology.
3.3.2 Assigning link characteristics to the topology
Given baseline graph interconnectivity consisting of an un-
balanced dumb-bell, we next assign bandwidth and latency values
to these links. We proceed with the distributions measured in the
original trace (§ 3.2) and further weigh these distributions with the
amount of trafﬁc sent across those links in the original trace. Thus,
if a particular HTTP source in the original trace were responsible
for transmitting 20% of the total HTTP bytes using a logical link
with 400 kbps of bandwidth and 50 ms of latency, a randomly
chosen 20% of the links (corresponding to HTTP generators) in
our target topology would be assigned bandwidth/latency values
accordingly. We also assign per-link MTUs to each link based on
distributions from the original trace.
The topology we have at the end of this stage is not one
that accurately represents the total number of hosts with the same
distribution of wide-area characteristics as in the original trace, but
one that is biased towards hosts that generate the most trafﬁc in the
original trace. One alternative is to assign sessions to generators
based on the network characteristics of the sources in the original
trace. We have implemented this strategy as well and it produces
better results with respect to matching trace characteristics, but we
found that it did not offer sufﬁcient randomness in our generated
traces, i.e., the resulting traces too closely matched the charac-
teristics of the original trace making it less interesting from the
perspective of exploring a space or extrapolating to alternative
scenarios.
4. VALIDATION
We now describe our approach for extracting and validating
our parameter values for our model from a number of available
packet traces. In particular, we focus on Mawi [26] traces, a trans-
Paciﬁc line (18Mbps Committed Access Rate on a 100Mbps link),
CAIDA [6] traces from a high-speed OC-48 MFN (Metropolitan
Fibre Network) Backbone 1 link (San Jose to Seattle) as well as
OC3c ATM link traces from the University of Auckland, New
Zealand [2]. These traces come from different geographical loca-
tions and demonstrate variation in application mix and individual
application characteristics (Table 2).
For each trace 2, we ﬁrst extract distributions of user, network
and application using the methodology outlined in § 3. Next we
generate trafﬁc using Swing and during the live emulation record
each packet that traverses our target link. From the generated
Swing-trace we re-extract parameter values and compare them
to the original values in Table 3. Speciﬁcally, we compare: i)
application breakdown, ii) aggregate bandwidth consumption, iii)
packet and byte arrival burstiness, iv) per-application bandwidth
consumption, and v) distributions for our model’s parameter values.
To compare distributions we use various techniques ranging from
visual tests to comparing the median values and Inter-Quartile 3
ranges.
To determine whether we capture the burstiness characteristics
of the original trace, we employ wavelet-based multi-resolution
2UDP trafﬁc (∼ 10% by bytes) was ﬁltered out.
3IQR is the difference in the 75th and 25th percentile values.
Table 2: Comparing aggregate bandwidth (Mbps) and packets per second (pps) (Trace/Swing) for Auck, Mawi and CAIDA traces.
Trace ↓ Length
Secs
599
899
299
300
3.33 / 3.24
9.90 / 9.04
13.88 / 12.24
134.93 / 127.56
591 / 509
1209 / 1101
1779 / 1567
17404 / 14625
2001-06-11
2004-09-23
2004-03-18
2003-04-24
58 / 57
720 / 609
972 / 890
5382 / 4523
Application 2
Mbps
0.55 / 0.55
5.58 / 4.96
7.51 / 7.63
49.24 / 45.97
Application 1 - HTTP
pps
Mbps
Name
SQUID