title:DeepSteal: Advanced Model Extractions Leveraging Efficient Weight
Stealing in Memories
author:Adnan Siraj Rakin and
Md Hafizul Islam Chowdhuryy and
Fan Yao and
Deliang Fan
2022 IEEE Symposium on Security and Privacy (SP)
DeepSteal: Advanced Model Extractions Leveraging Efﬁcient
Weight Stealing in Memories
Adnan Siraj Rakin*1, Md Haﬁzul Islam Chowdhuryy*2, Fan Yao2, and Deliang Fan1
1 School of Electrical, Computer, and Energy Engineering, Arizona State University, Tempe, AZ
2 Department of Electrical and Computer Engineering, University of Central Florida, Orlando, FL
∗ Co-ﬁrst Authors with Equal Contributions
3
4
7
3
3
8
9
.
2
2
0
2
.
4
1
2
6
4
P
S
/
9
0
1
1
.
0
1
:
I
O
D
|
E
E
E
I
2
2
0
2
©
0
0
.
1
3
$
/
2
2
/
9
-
6
1
3
1
-
4
5
6
6
-
1
-
8
7
9
|
)
P
S
(
y
c
a
v
i
r
P
d
n
a
y
t
i
r
u
c
e
S
n
o
m
u
i
s
o
p
m
y
S
E
E
E
I
2
2
0
2
Abstract—Recent advancements in Deep Neural Networks
(DNNs) have enabled widespread deployment
in multiple
security-sensitive domains. The need for resource-intensive train-
ing and the use of valuable domain-speciﬁc training data have
made these models the top intellectual property (IP) for model
owners. One of the major threats to DNN privacy is model
extraction attacks where adversaries attempt to steal sensitive in-
formation in DNN models. In this work, we propose an advanced
model extraction framework DeepSteal that steals DNN weights
remotely for the ﬁrst time with the aid of a memory side-channel
attack. Our proposed DeepSteal comprises two key stages. Firstly,
we develop a new weight bit information extraction method,
called HammerLeak, through adopting the rowhammer-based
fault technique as the information leakage vector. HammerLeak
leverages several novel system-level techniques tailored for DNN
applications to enable fast and efﬁcient weight stealing. Secondly,
we propose a novel substitute model training algorithm with
Mean Clustering weight penalty, which leverages the partial
leaked bit information effectively and generates a substitute
prototype of the target victim model. We evaluate the proposed
model extraction framework on three popular image datasets
(e.g., CIFAR-10/100/GTSRB) and four DNN architectures (e.g.,
ResNet-18/34/Wide-ResNet/VGG-11). The extracted substitute
model has successfully achieved more than 90% test accuracy
on deep residual networks for the CIFAR-10 dataset. Moreover,
our extracted substitute model could also generate effective
adversarial input samples to fool the victim model. Notably, it
achieves similar performance (i.e., ∼1-2% test accuracy under
attack) as white-box adversarial input attack (e.g., PGD/Trades).
Index Terms—rowhammer, model extraction, bit leakage, ad-
versarial attack
I. INTRODUCTION
Recent developments in deep learning technologies have
made machine learning an integral part of our daily life.
This widespread application of Deep Neural Networks (DNNs)
includes but not limited to image classiﬁcation [1], image
detection [2] and speech recognition [3], many of which
are deployed in security-sensitive settings [4]. DNN models
typically take a tremendous amount of resources to train,
and in many cases the training relies on the use of valuable
domain-speciﬁc data. As a result, DNN models are regarded
as the top intellectual properties (IP) for machine learning
(ML) service providers and model owners [5]. With the rapid
development of system and hardware level attack vectors
that can compromise and tamper computing systems [6]–[10],
understanding and investigating the security of deep learning
systems has become imperative.
the exact
Model extraction attacks aim to infer or steal critical in-
formation from DNN models to achieve certain malicious
goals [11]. Particularly, active learning is a popular approach
in recovering the performance of a victim DNN model [12]–
[18]. These methods primarily use input and output scores
to recover the victim model’s performance. It is challenging
to extract
internal decision boundary only with
input-output scores, which is especially the case for DNNs
with complex and deep structures [11], [19], [20]. These
techniques typically come with tremendous training overhead
and substantial attack costs because of heavy model queries.
Recent advances in hardware-based exploitation have shown
that adversaries can leverage side channel attacks to gain sensi-
tive information in computing systems [21]–[24]. Particularly,
several works have demonstrated successful information leak-
age manipulation in DNN models by leveraging microarchi-
tecture attacks [25], power/electromagnetic side channels [26],
[27] and bus snooping attacks [28], [29]. Hardware-based
attacks can be extremely dangerous as they allow adversaries
to directly gain internal knowledge about the victim DNN
models. However, existing hardware-based DNN attacks either
only extract high-level model structures (e.g., model archi-
tectures) or require physical access to the target machines to
gain ﬁne-grained model information, which is not applicable
to remote victims (e.g., in the cloud). Recent work reveals
that rowhammer attacks can be harnessed as a side channel
exploit in a remote setting [7]. However, it remains uncertain
whether accurate information about model weights (i.e., the
core information of DNN models) can be effectively exﬁltrated
via rowhammer-based side channels. Note that acquisition of
such information can potentially further extend DNN adver-
sarial capabilities,
including constructing substitute models
with high accuracy, reproducing model ﬁdelity (i.e., identical
prediction as to the victim model), and bolstering adversarial
attacks [18], [30]. In this paper, we focus on answering the
following question: Is it possible for an adversary to perform
model extractions through remotely stealing weight parameters
using rowhammer-based side channels?
While obtaining model weights can be useful intuitively,
there are several major challenges from the attacker’s perspec-
tive to practically capture and effectively utilize such infor-
mation. First, although ﬁne-grained secret leakage has been
widely shown to be plausible in many non-ML applications
(e.g., through microarchitecture attacks [22], [31]–[35]), such
© 2022, Adnan Siraj Rakin. Under license to IEEE.
DOI 10.1109/SP46214.2022.00122
1157
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:59:25 UTC from IEEE Xplore.  Restrictions apply. 
attacks fail to exﬁltrate detailed model weights due to the lack
of distinguishable control and data-ﬂow dependencies in DNN
applications. Second, DNN models are often extremely large
(with millions of parameters); even with a hardware-based
attack that can recover certain model weight information, it
is typically impractical to assume that the entire weights can
be exﬁltrated in practical settings. Moreover, prior works [36],
[37] have shown that variations on only tens out of millions
of weight parameters will completely malfunction a DNN
model. In this case, whether partial information of model
weights can be effectively leveraged to build a stronger model
extraction attack is uncertain. The ﬁnal challenge involves
how to design highly optimized ML techniques based on
the obtained unique and partial model weight knowledge for
different attack objectives.
In this paper, we present DeepSteal, an advanced model
extraction attack framework using efﬁcient model weight
stealing with the aid of rowhammer-based side channels. The
objective of our attack is to recover (partial) weight parameters
of a target DNN model (i.e., victim model), which will be har-
nessed to build useful substitute models using novel learning
schemes. At a high level, DeepSteal consists of two key stages.
To address the ﬁrst aforementioned challenge, in the ﬁrst stage,
we develop a novel rowhammer-based side channels that can
exﬁltrate partial bit information of model weight parameters,
called HammerLeak. Particularly, we leverage the well-known
rowhammer fault attack [9] as the information leakage attack
vector. Our exploitation is motivated by prior studies showing
that the occurrence of rowhammer-induced fault in a memory
cell highly depends on the data pattern from its neighboring
cells [7], [37]. While such a phenomenon was ﬁrst used in the
prior work [7] to successfully steal crypto keys, we note that
such a technique is ineffective in stealing secrets in bulk and
also cannot be applied in the context of DNNs. Therefore,
we propose a set of system-level rowhammer optimization
schemes that enable fast and efﬁcient exﬁltration of partial
model weights tailored for DNN application platforms. After
recovering the partial information, the weight search space of
a victim model still remains high. For instance, even after
recovering 90% of the bits in a large model
like VGG-
11 [38] (i.e., 1056 Million bits for an 8-bit model), the attacker
still needs to train the recovered model with limited data
to restore the remaining 10% bits (i.e., 105.6 million bits).
To address the additional challenges, in the second stage,
we propose a novel substitute model training algorithm with
Mean Clustering weight penalty. The purpose of such a loss
penalty term is to utilize the recovered partial weight bits for
effectively guiding the substitute model training. Subsequently,
DeepSteal produces a substitute model that achieves similar
accuracy as the victim model with high ﬁdelity. Moreover, the
trained substitute model could help mount strong adversarial
input attacks on the victim model. We summarize the major
contributions of our work as follows:
1) In this work, we investigate an advanced model extraction
attack with the exploitation of a remote side channel that
for the ﬁrst time can exﬁltrate ﬁne-grained information
from DNN model weights.
2) We develop HammerLeak, a multi-round rowhammer-
based information leakage technique that can exﬁltrate
secretive data from computing systems in bulk with high
accuracy. To make the rowhammer side channel appli-
cable and efﬁcient for attacking DNNs, HammerLeak
integrates a set of novel system-level techniques such
as the use of more ﬂexible rowhammer memory layouts,
effective DNN inference anchoring and batched victim
page releasing.
3) We propose a novel training algorithm to build a substi-
tute model using the leaked weight bits as constraints. It
ﬁrst conducts a data ﬁltering process of the leaked bits
to construct a proﬁle consisting of projected searching
space for each weight. Then, a Mean Clustering penalty
term is added to the cross-entropy loss during training,
penalizing each weight to converge near the mean of the
projected range, for achieving a substitute prototype of
the victim DNN model with comparable accuracy and
high ﬁdelity.
4) We build an end-to-end DeepSteal attack in real sys-
tems and demonstrate its efﬁcacy on four popular DNN
architectures (e.g., ResNet-18/34, Wide-ResNet-28, and
VGG-11). We evaluate our attack on three standard image
classiﬁcation datasets (e.g., CIFAR-10, CIFAR-100, and
GTSRB). For example, the extracted substitute model has
successfully achieved more than 90% test accuracy on
deep residual networks for the CIFAR-10 dataset.
5) Finally, we demonstrate the superior performance of
leveraging the recovered model weight bits (i.e., MSBs)
to build more powerful substitute model (as compared to
the use of model architecture information only) for adver-
sarial input attacks. With our DeepSteal, it can generate
effective adversarial examples with similar attack efﬁcacy
as a white-box attack (e.g., 0.05%).
II. BACKGROUND AND RELATED WORKS
A. Model Extraction
Model extraction is an emerging class of attacks in deep
learning applications. It jeopardizes the privacy of the de-
ployed victim model by leaking conﬁdential information (e.g.,
model architecture, weights and biases). An ideal model
extraction attack would extract the exact copy of the victim
model. For a task, the input and output pair data (X, Y ) ∈ R
can be drawn from the true distribution DA to train a DNN
model Mθ with parameters θ. In this work, we designate this
model Mθ as the victim model. To extract the exact model,
the attacker will attempt to recover a theft model ˆMθ such
that Mθ = ˆMθ. However, such an identical model (i.e., same
architecture and parameters) stealing is practically challenging
if not impossible [11].
Algorithm-based Model Extraction. Prior works [11], [12],
[39] have deﬁned several potential approaches to extract DNN
model information. In Table I, we summarize the prior DNN
model extraction works into three major categories. First, in
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:59:25 UTC from IEEE Xplore.  Restrictions apply. 
1158
direct recovery method, the attacker attempts to reconstruct
the victim DNN model using DNN output scores and gradi-
ent information. These works [11], [19], [20], [40] leverage
layer-wise mathematical formulation and internal functional
representation to recover weights. In this setting, the goal of
the attacker is to create a functionally equivalent model which
is given an input x ∈ X, the recovered model ˆMθ should
follow: Mθ(x) = ˆMθ(x). This objective is a weaker version
of the exact model extraction method. But it remains a difﬁcult
route to succeed in model extraction, as prior works [11], [19],
[20] have failed to show a successful attack for over 2-layer