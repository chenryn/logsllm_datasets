threats (with the possible exception of cookies) to install
such spooﬁng extensions. As a rough metric, consider that
the most popular extension for Mozilla Firefox is Adblock
Plus [34] that, at the time of this writing, is installed by
ﬁfteen million users, 25 times more users than UserAgent
Switcher, the most popular extension in Table IV.
We characterize the extension-problem as an iatrogenic 8
one. The users who install these extensions in an effort
8iatrogenic - Of or relating to illness caused by medical examination or
treatment.
552
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:53:33 UTC from IEEE Xplore.  Restrictions apply. 
to hide themselves in a crowd of popular browsers, install
software that actually makes them more visible and more
distinguishable from the rest of the users, who are using
their browsers without modiﬁcations. As a result, we advice
against the use of user-agent-spooﬁng extensions as a way
of increasing one’s privacy. Our ﬁndings come in direct
antithesis with the advice given by Yen et al. [18], who
suggest that user-agent-spooﬁng extensions can be used, as
a way of making tracking harder. Even though their study
focuses on common identiﬁers as reported by client-side
HTTP headers and the client’s IP address, a server capable
of viewing these can respond with JavaScript code that will
uncover the user-agent-spooﬁng extension, using any of the
aforementioned techniques.
VI. DISCUSSION
Given the intrusive nature of web-based device ﬁnger-
printing and the current inability of browser extensions to
actually enhance a user’s privacy, in this section, we ﬁrst
discuss possible ways of reducing a user’s ﬁngerprintable
surface and then brieﬂy describe alternative uses of ﬁnger-
printing which may become more prevalent in the future.
A. Reducing the ﬁngerprintable surface
Flash. As described in Section II, Adobe Flash was
utilized by all three ﬁngerprinting libraries that we studied,
due to its rich API that allow SWF ﬁles to access information
not traditionally available through a browser’s API. In all
cases, the SWF ﬁle responsible for gathering information
from the host was hidden from the user, by either setting
the width and height of the  tag to zero, or
placed into an iframe of zero height and width. In other
words, there was no visible change on the web page that
included the ﬁngerprinting SWF ﬁles. This observation can
be used as a ﬁrst line of defense. All modern browsers
have extensions that disallow Flash and Silverlight to be
loaded until explicitly requested by the user (e.g., through
a click on the object
itself). These hidden ﬁles cannot
be clicked on and thus, will never execute. While this is
a straightforward solution that would effectively stop the
Flash-part of the ﬁngerprint of all three studied companies,
a circumvention of this countermeasure is possible. By
wrapping their ﬁngerprinting code into an object of the ﬁrst-
party site and making that object desirable or necessary for
the page’s functionality, the ﬁngerprinting companies can
still execute their code. This, however, requires much more
integration between a ﬁrst-party website and a third-party
ﬁngerprinting company than the current model of “one-size-
ﬁts-all” JavaScript and Flash.
In the long run, the best solution against ﬁngerprinting
through Flash should come directly from Flash. In the past,
researchers discovered that Flash’s Local Shared Objects,
i.e., Flash’s equivalent of browser cookies, were not deleted
when a user exited her browser’s private mode or even when
she used the “Clear Private Data” option of her browser’s
UI [36]. As a result, in the latest version of Flash, LSOs
are not stored to disk but simply kept in memory when
the browser’s private mode is utilized [37]. Similarly, when
a browser enters private mode, Flash could provide less
system information, respect any browser-set HTTP proxies
and possibly report only a standard subset of a system’s
fonts, to protect a user’s environment from ﬁngerprinting.
JavaScript. There are multiple vendors involved in the
development of JavaScript engines, and every major browser
is equipped with a different engine. To unify the behavior of
JavaScript under different browsers, all vendors would need
to agree not only on a single set of API calls to expose to
the web applications, but also to internal implementation
speciﬁcs. For example, hash table implementations may
affect the order of objects in the exposed data structures
of JavaScript, something that can be used to ﬁngerprint
the engine’s type and version. Such a consensus is difﬁcult
to achieve among all browser vendors, and we have seen
diversions in the exposed APIs of JavaScript even in the
names of functions that offer the same functionality, e.g.,
execScript and eval. Also, based on the fact
that
the vendors battle for best performance of their JavaScript
engines, they might be reluctant to follow speciﬁc design
choices that might affect performance.
At the same time, however, browsers could agree to sac-
riﬁce performance when “private-mode” is enabled, where
there could be an attempt to expose a uniﬁed interface.
B. Alternative uses of ﬁngerprinting
the attackers can decide, at
Although, in this paper, we have mostly focused on ﬁnger-
printing as a fraud-detection and web-tracking mechanism,
there is another aspect
that requires attention. Drive-by
downloads and web attacks in general use ﬁngerprinting
to understand if the browser that they are executing on is
vulnerable to one of the multiple available exploits. This
way,
the server-side, which
exploit to reveal to the client, exposing as little as they
can of their attack capabilities. There are three different
architectures to detect drive-by downloads: low-interaction
honeypots, high-interaction honeypots and honeyclients. In
all three cases, the browser is either a specially crafted one,
so that it can instrument the pages visited, or a browser
installation that was never used by a real user. Given the
precise, browser-revealing, ﬁngerprinting techniques that we
described in this paper, it is possible to see in the future these
mechanisms being used by attackers to detect monitoring
environments and circumvent detection.
VII. RELATED WORK
To the best of our knowledge, this paper is the ﬁrst that
attempts to study the problem of web-based ﬁngerprinting
from the perspectives of all the players involved, i.e., from
the perspective of the ﬁngerprinting providers and their
553
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:53:33 UTC from IEEE Xplore.  Restrictions apply. 
ﬁngerprinting methods, the sites utilizing ﬁngerprinting, the
users who employ privacy-preserving extensions to combat
ﬁngerprinting, and the browser’s internals and how they
relate to its identity.
Eckersley conducted the ﬁrst large-scale study showing
that various properties of a user’s browser and plugins
can be combined to form a unique ﬁngerprint [12]. More
precisely, Eckersley found that from about 500,000 users
who visited panopticlick.eff.org and had Flash or
Java enabled, 94.2% could be uniquely identiﬁed, i.e., there
was no other user whose environment produced the same ﬁn-
gerprint. His study, and surprisingly accurate identiﬁcation
results, prompted us to investigate commercial ﬁngerprinting
companies and their approach. Yen et al. [18] performed
a ﬁngerprinting study, similar to Eckersley’s, by analyzing
month-long logs of Bing and Hotmail. Interestingly, the
authors utilize a client’s IP address as part of their tracking
mechanism, which Eckersley explicitly avoids dismissing
it as “not sufﬁciently stable.” As a way of protecting
oneself, the authors advocated the use of user-agent-spooﬁng
extensions. As we discussed in Section V, this is actually
counter-productive since it allows for more ﬁngerprinting
rather than less.
Mowery et al. [13] proposed the use of benchmark ex-
ecution time as a way of ﬁngerprinting JavaScript imple-
mentations, under the assumption that speciﬁc versions of
JavaScript engines will perform in a consistent way. Each
browser executes a set of predeﬁned JavaScript benchmarks,
and the completion-time of each benchmark forms a part
of the browser’s performance signature. While their method
correctly detects a browser-family (e.g., Chrome) 98.2% of
the time, it requires over three minutes to fully execute.
According to a study conducted by Alenty [38], the average
view-time of a web page is 33 seconds. This means that,
with high likelihood, the benchmarks will not be able to
completely execute and thus, a browser may be misclassi-
ﬁed. Moreover, the reported detection rate of more speciﬁc
attributes, such as the browser-version, operating system and
architecture, is signiﬁcantly less accurate.
Mowery and Shacham later proposed the use of rendering
text and WebGL scenes to a  element as another
way of ﬁngerprinting browsers [39]. Different browsers will
display text and graphics in a different way, which, however
small, can be used to differentiate and track users between
page loads. While this method is signiﬁcantly faster than
the execution of browser benchmarks, these technologies are
only available in the latest versions of modern browsers,
thus they cannot be used to track users with older versions.
Contrastingly, the ﬁngerprinting techniques introduced in
Section IV can be used to differentiate browsers and their
versions for any past version.
Olejnik et al. [40] show that web history can also be
used as a way of ﬁngerprinting without the need of addi-
tional client-side state. The authors make this observation
by analyzing a corpus of data from when the CSS-visited
history bug was still present in browsers. Today, however,
all modern browsers have corrected this issue and thus,
extraction of a user’s history is not as straightforward,
especially without user interaction [41]. Olejnik et al. claim
that large script providers, like Google, can use their near-
ubiquitous presence to extract a user’s history. While this
is true [42], most users have ﬁrst-party relationships with
Google, meaning that they can be tracked accurately, without
the need of resorting to history-based ﬁngerprinting.
VIII. CONCLUSION
In this paper, we ﬁrst investigated the real-life implemen-
tations of ﬁngerprinting libraries, as deployed by three pop-
ular commercial companies. We focused on their differences
when compared to Panopticlick and discovered increased use
of Flash, backup solutions for when Flash is absent, broad
use of Internet Explorer’s special features, and the existence
of intrusive system-ﬁngerprinting plugins.
Second, we created our own ﬁngerprinting script, us-
ing multiple novel features that mainly focused on the
differences between special objects, like the navigator
and screen, as implemented and handled by different
browsers. We identiﬁed that each browser deviated from
all the rest in a consistent and measurable way, allowing
scripts to almost instantaneously discover the true nature
of a browser, regardless of a browser’s attempts to hide
it. To this end, we also analyzed eleven popular user-agent
spooﬁng extensions and showed that, even without our newly
proposed ﬁngerprinting techniques, all of them fall short of
properly hiding a browser’s identity.
The purpose of our research was to demonstrate that when
considering device identiﬁcation through ﬁngerprinting,
user-privacy is currently on the losing side. Given the
complexity of fully hiding the true nature of a browser,
we believe that this can be efﬁciently done only by the
browser vendors. Regardless of
their complexity and
sophistication, browser-plugins and extensions will never
be able to control everything that a browser vendor can.
At the same time, it is currently unclear whether browser
vendors would desire to hide the nature of their browsers,
thus the discussion of web-based device ﬁngerprinting, its
implications and possible countermeasures against it, must
start at a policy-making level in the same way that stateful
user-tracking is currently discussed.
Acknowledgments: We want to thank our shepherd and
the anonymous reviewers for their valuable comments. For
KU Leuven, this research was performed with the ﬁnancial
support of the Prevention against Crime Programme of the
European Union (B-CCENTRE), the Research Fund KU
Leuven, the EU FP7 projects NESSoS and WebSand, as
well as the IWT project SPION. For UCSB,
this work
was supported by the Ofﬁce of Naval Research (ONR)
554
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:53:33 UTC from IEEE Xplore.  Restrictions apply. 
under grant N000140911042, and by the National Science
Foundation (NSF) under grants CNS-0845559 and CNS-
0905537, and in part by Secure Business Austria.
REFERENCES
[1] The New York Times - John Schwartz, “Giving the Web
a Memory Cost Its Users Privacy,” http://www.nytimes.com/
2001/09/04/technology/04COOK.html.
[2] B. Krishnamurthy, “Privacy leakage on the Internet,” pre-
sented at IETF 77, March 2010.
[3] B. Krishnamurthy and C. E. Wills, “Generating a privacy
footprint on the Internet,” in Proceedings of the 6th ACM
SIGCOMM Conference on Internet Measurement, ser. IMC
’06, New York, NY, USA, 2006, pp. 65–70.
[4] F. Roesner, T. Kohno, and D. Wetherall, “Detecting and de-
fending against third-party tracking on the web,” in NSDI’12:
Proceedings of the 9th USENIX conference on Networked
Systems Design and Implementation. Berkeley, CA, USA:
USENIX Association, 2012, pp. 12–12.
[5] The Wall Street Journal, “What They Know,” http://blogs.wsj.
com/wtk/.
[6] J. Turow, J. King, C. J. Hoofnagle, A. Bleakley, and M. Hen-
nessy, “Americans Reject Tailored Advertising and Three
Activities that Enable It,” 2009.
[7] B. Ur, P. G. Leon, L. F. Cranor, R. Shay, and Y. Wang,
“Smart, useful, scary, creepy: perceptions of online behavioral
advertising,” in Proceedings of the Eighth Symposium on
Usable Privacy and Security, ser. SOUPS ’12. New York,
NY, USA: ACM, 2012, pp. 4:1–4:15.
[8] comScore, “The Impact of Cookie Deletion on Site-Server
and Ad-Server Metrics in Australia,” January 2011.
[9] “Ghostery,” http:wwww.ghostery.com.
[10] “Collusion: Discover who’s tracking you online,” http://www.
[11] J. R. Mayer, “Any person... a pamphleteer,” Senior Thesis,
mozilla.org/en-US/collusion/.
Stanford University, 2009.
[12] P. Eckersley, “How Unique Is Your Browser?” in Proceed-
ings of the 10th Privacy Enhancing Technologies Symposium
(PETS), 2010.
[13] K. Mowery, D. Bogenreif, S. Yilek, and H. Shacham, “Fin-
gerprinting information in JavaScript implementations,” in
Proceedings of W2SP 2011, H. Wang, Ed.
IEEE Computer
Society, May 2011.
[14] C. Kolbitsch, B. Livshits, B. Zorn, and C. Seifert, “Rozzle:
De-cloaking internet malware,” in IEEE Symposium on Secu-
rity and Privacy, May 2012.
[15] E. Mills, “Device identiﬁcation in online banking is privacy
threat, expert says,” CNET News (April 2009).
[16] “Opt out of being tracked,” http://www.bluecava.com/
preferences/.
[17] J. R. Mayer, “Tracking the Trackers: Early Results — Center
for Internet and Society,” http://cyberlaw.stanford.edu/node/
6694.
[18] T.-F. Yen, Y. Xie, F. Yu, R. P. Yu, and M. Abadi, “Host
Fingerprinting and Tracking on the Web: Privacy and Security
Implications,” in Proceddings of the 19th Annual Network and
Distributed System Security Symposium (NDSS), 2012.
[19] J. R. Mayer and J. C. Mitchell, “Third-party web tracking:
Policy and technology,” in IEEE Symposium on Security and
Privacy, 2012, pp. 413–427.
[20] G. Cluley, “How to turn off Java on your browser - and why
you should do it now,” http://nakedsecurity.sophos.com/2012/
08/30/how-turn-off-java-browser/.
[21] B. Krebs, “How to Unplug Java from the Browser,” http://
krebsonsecurity.com/how-to-unplug-java-from-the-browser.
[22] D. Jang, R. Jhala, S. Lerner, and H. Shacham, “An empirical
study of privacy-violating information ﬂows in JavaScript
Web applications,” in Proceedings of CCS 2010, Oct. 2010.
[23] “Torbutton: I can’t view videos on YouTube and other
ﬂash-based sites. Why?” https://www.torproject.org/torbutton/
torbutton-faq.html.en#noﬂash.
[24] “Anubis: Analyzing Unknown Binaries,” http://anubis.iseclab.
org/.
[25] “VirusTotal - Free Online Virus, Malware and URL Scanner,”
https://www.virustotal.com/.
[26] G. Pierson and J. DeHaan, “Patent US20080040802 - NET-
WORK SECURITY AND FRAUD DETECTION SYSTEM
AND METHOD.”
[27] M. Cova, C. Kruegel, and G. Vigna, “Detection and analysis
of drive-by-download attacks and malicious javascript code,”
in Proceedings of the 19th International Conference on World
Wide Web (WWW), 2010, pp. 281–290.
[28] “ECMAScript Language Speciﬁcation, Standard ECMA-262,
Third edition.”
[29] M. Zalewski, The Tangled Web: A Guide to Securing Modern
Web Applications. No Starch Press, 2011.
[30] A. Andersen, “History of the browser user-agent string,” http:
//webaim.org/blog/user-agent-string-history.
[31] “Web Tracking Protection,” http://www.w3.org/Submission/
2011/SUBM-web-tracking-protection-20110224/.
[32] P.
Eckersley,
“Panopticlick — Self-Defense,”
https://panopticlick.eff.org/self-defense.php.
[33] J. Scott, “How many Firefox users have add-ons
in-
stalled? 85%!” https://blog.mozilla.org/addons/2011/06/21/
ﬁrefox-4-add-on-users/.
[34] “Adblock plus - for annoyance-free web surﬁng,” http://
adblockplus.org.
Fingerprinting,”
[35] A. Klein, “How Fraudsters are Disguising PCs to Fool
Device
http://www.trusteer.com/blog/
how-fraudsters-are-disguising-pcs-fool-device-ﬁngerprinting.
[36] A. Soltani, S. Canty, Q. Mayo, L. Thomas, and C. J. Hoofna-
gle, “Flash Cookies and Privacy,” in SSRN preprint (August
2009).
[37] J. Xu and T. Nguyen, “Private browsing and Flash Player
http://www.adobe.com/devnet/ﬂashplayer/articles/
10.1,”
privacy mode fp10 1.html.
[38] J.-L. Gass´ee and F. Filloux, “Measuring Time Spent
On A Web Page,” http://www.cbsnews.com/2100-215
162-5037448.html.
[39] K. Mowery and H. Shacham, “Pixel perfect: Fingerprint-
ing canvas in HTML5,” in Proceedings of W2SP 2012,
M. Fredrikson, Ed.
IEEE Computer Society, May 2012.
[40] Ł. Olejnik, C. Castelluccia, and A. Janc, “Why Johnny Can’t
Browse in Peace: On the Uniqueness of Web Browsing
History Patterns,” in the 5th workshop on Hot Topics in
Privacy Enhancing Technologies (HOTPETS 2012).
[41] Z. Weinberg, E. Y. Chen, P. R. Jayaraman, and C. Jackson, “I
still know what you visited last summer: Leaking browsing
history via user interaction and side channel attacks,” in
Proceedings of the 2011 IEEE Symposium on Security and
Privacy, ser. SP ’11, 2011, pp. 147–161.
[42] N. Nikiforakis, L. Invernizzi, A. Kapravelos, S. V. Acker,
W. Joosen, C. Kruegel, F. Piessens, and G. Vigna, “You
Are What You Include: Large-scale Evaluation of Remote
JavaScript Inclusions,” in Proceedings of the ACM Conference
on Computer and Communications Security (CCS), 2012.
555
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:53:33 UTC from IEEE Xplore.  Restrictions apply.