SPECsfs has a default NFS request mix, where 12% of
the requests update the ﬁle system and the remaining re-
quests are read-only. The ﬁle update logging overhead is
more pronounced when the proportion of write requests is
high. To stress-test the CDP schemes, we varied the write
request percentage from 12% to 96%. The distribution
among different types of write requests, and the distribution
among different types of read-only requests remain ﬁxed
throughout the experiments. We also varied the input load,
and set the initial working set size to be proportional to the
input load, for example, 7GB for the load of 700 ops/sec
and 5GB for the load of 500 ops/sec.
The measured peak throughput of a vanilla unprotected
NFS server under SPECsfs decreases as the write percent-
age increases. At 12%, it is about 700 ops/sec and goes
down to 500 ops/sec at 96%. Figure 5 shows that all four
continuous data protection schemes yield almost the same
throughput when the percentage of write requests in the
input workload is less than 12%. UCDP-A performs the
same as the vanilla NFS server because the logging server
in UCDP-A is not the system bottleneck, and the protected
NFS server in UCDP-A is identical to the vanilla NFS
server. As the write request percentage increases, UCDP-
O is limited by the logging server due to the expensive
three-step ﬁle update logging procedure (Section 1). Sur-
prisingly, even with all the extra processing due to version-
ing, UCDP-I and UCDP-K actually out-perform the vanilla
NFS server in throughput by around 7%. In this case, the
disk is the system bottleneck, and the advantage of the non-
overwrite strategy in processing random write requests, as
discussed in Section 3.1, is signiﬁcant enough that it re-
sults in a small but distinct overall performance improve-
ment. Another block-level versioning system, Clotho [17],
reported similar performance gain due to the use of a non-
overwrite strategy. Because the kernel module in UCDP-K
has no effects on disk access efﬁciency, UCDP-K performs
roughly the same as UCDP-I in all cases.
Figure 6 shows the average per-request latency of the
vanilla NFS server, UCDP-I, and UCDP-K. Each latency
number represents the average of ten measurements. The
upper three curves correspond to a SPEC load of 500
ops/sec, while the lower three curves correspond to a SPEC
load of 200 ops/sec. The latencies of UCDP-O and UCDP-
A are similar to that of the vanilla NFS because the reply to
each NFS request actually comes from their primary NFS
server. The results for UCDP-I and UCDP-K are similar,
because the kernel module has no signiﬁcant impact on la-
tency. When the write request percentage is no more than
36%, the average per-request latency of UCDP-I/UCDP-K
is similar to that of the vanilla NFS server. As the write re-
quest percentage increases further, the per-request latency
of UCDP-I/UCDP-K becomes higher than that of vanilla
NFS server, and the latency gap also increases. In the worst
case, when the write request percentage is at 96% of the
SPEC load of 200 ops/sec, the gap is about 7 msec, which
represents a 200% latency overhead. However, this latency
gap decreases as the load increases. For example, the ad-
ditional latency overhead is reduced to 10% to 30% at load
500.
The latency overhead of UCDP-I/UCDP-K when com-
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:53:28 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007)
%
(
n
o
i
t
a
z
i
l
i
t
U
U
P
C
30
25
20
15
10
5
0
0
SPEC load 500
UCDP-I
UCDP-K
NFS
24
12
84
Percentage of Update Request (%)
48
36
60
72
96
Figure 7. CPUutilizationcomparisonasthepercent-
ageofwriterequestsintheinputworkloadvaries.
pared with the vanilla NFS server comes from the extra
processing associated with ﬁle update logging. With the
non-overwrite strategy, to serve a client request, multiple
requests may need to be issued to the local NFS daemon,
such as reassembling read/write requests, reading the be-
fore image when a write request is not aligned, checking
out the ﬁle type of an object to be deleted, etc. These re-
quests do not impose much additional disk bandwidth re-
quirements, but they do increase the request processing la-
tency. As the write request percentage grows, more and
more blocks are overwritten and reside in overwrite pool,
more reassembling of read or write requests is needed,
the probability of issuing multiple local requests per client
request becomes higher, and eventually the per-request la-
tency of UCDP-I/UCDP-K increases.
Figure 7 compares the CPU utilization of NFS, UCDP-
I and UCDP-K. UCDP-O and UCDP-A are excluded be-
cause both of them require a separate server. When the in-
put SPECsfs load is 500 ops/sec, the throughputs of vanilla
NFS, UCDP-I and UCDP-K are comparable, but the CPU
utilization of UCDP-I and UCDP-K is about 170% and
85%, respectively, higher than that of vanilla NFS. These
results suggest that ﬁle update logging processing indeed
consumes additional CPU resource, and the kernel module
in UCDP-K effectively reduces the CPU consumption by
eliminating a large portion of context switching and mem-
ory copying overhead. When CPU is the system bottleneck,
UCDP-K should out-perform UCDP-I in terms of overall
throughput. To substantiate this claim, we modify the SPEC
workload so that it is read-only and has high buffer hit ratio,
and upgrade the network connection from a 100 Mbps to a
1000 Mbps network. As a result, disk and network are no
longer the system bottleneck. With an initial working set
size of 300MB and a SPECsfs input load of 7000 ops/sec,
the measured throughput of NFS, UCDP-I and UCDP-K
are 6560, 4166, 5441 respectively. UCDP-K indeed out-
performs UCDP-I by 30%.
4 Related Work
WAFL [12] is a general-purpose high performance ﬁle
system with snapshot support developed by Network Appli-
ance. WAFL is not optimized for ﬁne ﬁle update logging,
and allows only a limited number (32 originally) of snap-
shots. Each snapshot is taken at a coarse granularity and the
cost is amortized over hundreds of ﬁle updates.
Wayback [8] is a user-level comprehensive versioning
ﬁle system for Linux. For each data update, Wayback
uses an undo logging scheme similar to UCDP-O. For each
metadata update, Wayback incurs a higher overhead than all
of our logging schemes. For normal ﬁle system update, the
performance of Wayback is quite poor compared with tra-
ditional ﬁle systems. When compared with EXT3, the data
read/write overhead ranges from -2% to 70%, and the meta-
data update overhead ranges from 100% to 400%. In con-
trast, the performance of UCDP-I and UCDP-K are compa-
rable to generic NFS server running on top of EXT3.
CVFS [11] is a kernel-level comprehensive versioning
ﬁle system that is optimized for metadata logging efﬁciency.
Journal-based meta-data is used for inode/indirect-block
update and multiversion B-tree [10] is used for directory
update. S4 [16] is a secure network-attached object store
against malicious attacks. S4 logs every update and min-
imizes the space explosion. S4 uses a log-structured de-
sign to avoid overwriting of old data. S4 improves the
inode/indirect-block logging efﬁciency by encoding their
changes in a logging record. Although the overhead of S4’s
logging scheme is low, its cleaning cost can be as high as
50%.
Log structured ﬁle system [20] has been used to reduce
the disk access penalty of random small writes. Yet it shares
one common feature of non-overwrite with ﬁle update log-
ging. Logging systems do not overwrite to save old data.
Log structured ﬁle system writes data into new location in
big batch to improve write performance. In LFS, cleaning
is essential to keep the disk locality of ﬁle but the overhead
is often high. This issue is not pronounced in the proposed
user-level ﬁle update logging system because we use a base
image (Section 4.3.1) to maintain the disk locality and our
cleaning cycle is much longer hence the cost is amortized.
Elephant [13] is a kernel-level versioning ﬁle system that
creates a new version only when a ﬁle is closed. There-
fore, it does not distinguish updates between a ﬁle open
and ﬁle close operation. VersionFS [19] is a versioning
ﬁle system implemented using stackable ﬁle system tech-
nique [22]. Similar to Elephant [13], the version is based
on open-close session and the versioning policy is ﬂexible.
VersionFS also provides friendly interface for user to access
old versions and to customize the versioning policies. Ver-
sionFS still incurs non-negligible performance overhead -
about 100% when measured by Postmark benchmark.
Clotho [17] is a versioning system at the disk block level.
Compared with ﬁle system versioning, block-level version-
ing is less complex due to its simpler interface. However,
it is more difﬁcult for users to directly manage versions of
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:53:28 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007disk blocks. Usually another layer on top of block-level ver-
sioning is required to provide easy version access. Clotho
aggregates updates that take place within a period of time
(e.g. a minute) and create new versions for them, and there-
fore does not support CDP.
5 Conclusion
Continuous data protection is a critical building block for
quickly repairing damage to a ﬁle system due to malicious
attacks or innocent human errors. So far it has not been in-
corporated into mainstream ﬁle servers because of the con-
cern of additional storage requirements, performance over-
head, and the implementation complexity. Given the dra-
matic improvements in the per-byte cost of magnetic disk
technology, disk cost is no longer an issue. Measurements
on a real-world NFS trace shows that a $200 200GB disk
can easily support a one-month logging window for a large
NFS server whose size is 400GB and whose average load is
34 requests/sec [23]. The performance overhead and im-
plementation complexity associated with continuous data
protection, however, remain signiﬁcant barriers to its de-
ployment in practice. This paper describes a user-level con-
tinuous data protection architecture that is both efﬁcient
and portable, and thus completely eliminates these barriers.
We have implemented four variants of this user-level CDP
architecture, and compared their latency, throughput, and
CPU usage characteristics using standard benchmarks, NFS
traces, and synthetic workloads. The main lessons from this
implementation effort and performance study are
• User-level continuous data protection based on the
NFS protocol that is portable across multiple operat-
ing system platforms is feasible and relatively simple
to implement.
• UCDP-A incurs close to zero latency and through-
put penalty compared with an unprotected vanilla NFS
server, and is thus the best choice for IT environments
where performance is the main concern, mirroring ﬁle
system image is desirable, and minimum disruption to
the primary NFS server is important.
• User-level continuous data protection, when embedded
into an NFS server, can have comparable throughput
as the unprotected NFS server although it does incur
3∼5 msec of latency penalty when the write request
percentage is above 36%. The write request percent-
age in typical NFS sites, as speciﬁed in the SPECsfs
benchmark, is less than 12%.
• If portability can be slightly compromised, simple in-
kernel optimizations could signiﬁcantly decrease the
CPU overhead due to context switching and mem-
ory copying associated with user-level CDP. But they
do not produce noticeable latency and throughput im-
provement when the write request percentage in the in-
put workload is lower than 12%.
• Logging updates at a higher level of abstraction, such
as NFS requests and replies, tends to produce a much
more compact log than logging at a lower level of ab-
straction, such as disk accesses and responses, and is
also more portable and ﬂexible.
Acknowledgement
This research is supported by NSF awards SCI-0401777,
CNS-0410694 and CNS-0435373.
References
[1] The Advanced Maryland Automatic Network Disk Archiver.
(http://www.-
amanda.org/).
[2] Concurrent Versions System. (http://www.cvshome.org/).
[3] Enterprise Rewinder: Product Suite for Continuous Data Protection (CDP).
(http://www.xosoft.com/).
[4] RealTime - Near-Instant Recovery to Any Point
in Time.
(http://www.-
mendocinosoft.com/).
[5] System File Server Benchmark SPEC SFS97 R1 V3.0. Standard Performance
Evaluation Corporation. (http://www.specbench.org/sfs97r1/).
[6] NFS: Network ﬁle system protocol speciﬁcation. Sun Microsystems, Mar 1989.
[7] A. Chervenak, V. Vellanki, and Z. Kurmas. Protecting ﬁle systems: A survey
In Proceedings Joint NASA and IEEE Mass Storage
of backup techniques.
Conference, March 1998.
[8] Brian Cornell, Peter A. Dinda, and Fabin E. Bustamante. Wayback: A user-
level versioning ﬁle system for linux. In USENIX 2004 Annual Technical Con-
ference (Freenix).
[9] D. Ellard, J. Ledlie, P. Malkani, and M. Seltzer. Passive nfs tracing of email
and research workloads. In 2nd USENIX Conference on File and Storage Tech-
nologies, Mar 2003.
[10] B. Becker et al. An asymptotically optimal multiversion b-tree. Very Large
Data Bases Journal, 1996.
[11] C.A.N. Soules et al. Metadata efﬁciency in a comprehensive versioning ﬁle
In 2nd USENIX Conference on File and Storage Technologies, Mar
system.
2003.
[12] D. Hitz et al. File system design for an nfs ﬁle server appliance. In USENIX
winter 1994 conference, pages 235–246, Chateau Lake Louise, Banff, Canada,
1994.
[13] D. S. Santry et al. Deciding when to forget in the elephant ﬁle system. In Pro-
ceedings of the Seventeenth ACM Symposium on Operating Systems Principles,
pages 110–123, December 12-15, 1999.
[14] G. W. Dunlap et al. Revirt: Enabling intrusion analysis through virtual-machine
logging and replay. In Proceedings of 5th Symposium on Operating Systems
Design and Implementation, Dec 2002.
[15] Hugo Patterson et al. Snapmirror: ﬁle system based asynchronous mirroring
for disaster recovery. In Conference on File and Storage Technologies, pages
28–30, Monterey, CA, January 2002.
[16] J. Strunk et al. Self-securing storage: Protecting data in compromised systems.
In Proceedings of the 2000 OSDI Conference, October 2000.
[17] Michail D. Flouris and Angelos Bilas. Clotho: Transparent data versioning at
In 21st IEEE Conference on Mass Storage Systems and
the block i/o level.
Technologies, April 2004.
[18] D. Mazires. A toolkit for user-level ﬁle systems. In Proceedings of the 2001
USENIX Technical Conference, pages 261–274, June 2001.
[19] K. Muniswamy-Reddy, C. P. Wright, A. Himmer, and E. Zadok. A Versatile and
User-Oriented Versioning File System. In Proceedings of the Third USENIX
Conference on File and Storage Technologies (FAST 2004), pages 115–128,
San Francisco, CA, March/April 2004.
[20] M. Rosenblum and J. K. Ousterhout. The design and implementation of a log-
structured ﬁle system. In ACM Transactions on Computer Systems, 1991.
[21] Michael Rowan. Continuous data protection: A technical overview.
In
http://www.revivio.com/index.asp?p=tech white papers, 2004.
[22] E. Zadok and J. Nieh. FiST: A Language for Stackable File Systems.
In
Proceedings of the Annual USENIX Technical Conference, pages 55–70, June
2000.
[23] N. Zhu and T. Chiueh. Design, implementation, and evaluation of repairable
ﬁle service. In The International Conference on Dependable Systems and Net-
works, June 2003.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:53:28 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007