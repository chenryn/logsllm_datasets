### Optimized Text

SPECsfs employs a default NFS (Network File System) request mix, where 12% of the requests are for file updates, and the remaining 88% are read-only. The overhead associated with logging file updates becomes more significant as the proportion of write requests increases. To evaluate the performance of various Continuous Data Protection (CDP) schemes, we adjusted the percentage of write requests from 12% to 96%, while keeping the distribution of different types of write and read-only requests constant throughout the experiments. Additionally, we varied the input load, setting the initial working set size proportional to the input load; for example, 7GB for a load of 700 operations per second (ops/sec) and 5GB for a load of 500 ops/sec.

The peak throughput of an unprotected vanilla NFS server under SPECsfs decreases as the percentage of write requests increases. At 12% write requests, the throughput is approximately 700 ops/sec, which drops to 500 ops/sec at 96% write requests. Figure 5 illustrates that all four CDP schemes exhibit similar throughput when the write request percentage is below 12%. UCDP-A performs identically to the vanilla NFS server because the logging server in UCDP-A does not become a bottleneck, and the protected NFS server in UCDP-A is identical to the vanilla NFS server. As the write request percentage increases, UCDP-O's performance is limited by its logging server due to the expensive three-step file update logging procedure (Section 1). Surprisingly, even with the additional processing required for versioning, UCDP-I and UCDP-K outperform the vanilla NFS server by about 7% in throughput. In this scenario, the disk is the system bottleneck, and the non-overwrite strategy's advantage in handling random write requests, as discussed in Section 3.1, is significant enough to result in a small but noticeable overall performance improvement. Another block-level versioning system, Clotho [17], reported similar performance gains due to the use of a non-overwrite strategy. Since the kernel module in UCDP-K has no effect on disk access efficiency, UCDP-K performs similarly to UCDP-I in all cases.

Figure 6 presents the average per-request latency for the vanilla NFS server, UCDP-I, and UCDP-K. Each latency value represents the average of ten measurements. The upper three curves correspond to a SPEC load of 500 ops/sec, while the lower three curves correspond to a SPEC load of 200 ops/sec. The latencies of UCDP-O and UCDP-A are similar to those of the vanilla NFS server because the responses to NFS requests come from their primary NFS server. The results for UCDP-I and UCDP-K are similar, as the kernel module has no significant impact on latency. When the write request percentage is 36% or less, the average per-request latency of UCDP-I/UCDP-K is comparable to that of the vanilla NFS server. As the write request percentage increases further, the per-request latency of UCDP-I/UCDP-K exceeds that of the vanilla NFS server, and the latency gap widens. In the worst case, at 96% write requests and a SPEC load of 200 ops/sec, the gap is about 7 milliseconds, representing a 200% latency overhead. However, this latency gap decreases as the load increases. For instance, at a load of 500 ops/sec, the additional latency overhead is reduced to 10% to 30%.

The latency overhead of UCDP-I/UCDP-K compared to the vanilla NFS server arises from the extra processing required for file update logging. With the non-overwrite strategy, multiple requests may need to be issued to the local NFS daemon to serve a client request, such as reassembling read/write requests, reading the before image for unaligned write requests, and checking the file type of objects to be deleted. These requests do not significantly increase disk bandwidth requirements but do increase request processing latency. As the write request percentage grows, more blocks are overwritten and reside in the overwrite pool, increasing the need for reassembling read or write requests and the likelihood of issuing multiple local requests per client request, leading to higher per-request latency for UCDP-I/UCDP-K.

Figure 7 compares the CPU utilization of NFS, UCDP-I, and UCDP-K. UCDP-O and UCDP-A are excluded because they require a separate server. At an input SPECsfs load of 500 ops/sec, the throughputs of vanilla NFS, UCDP-I, and UCDP-K are comparable, but the CPU utilization of UCDP-I and UCDP-K is about 170% and 85% higher, respectively, than that of vanilla NFS. These results indicate that file update logging consumes additional CPU resources, and the kernel module in UCDP-K effectively reduces CPU consumption by eliminating a large portion of context switching and memory copying overhead. When the CPU is the system bottleneck, UCDP-K should outperform UCDP-I in terms of overall throughput. To validate this, we modified the SPEC workload to be read-only with a high buffer hit ratio and upgraded the network connection from 100 Mbps to 1000 Mbps. This change ensured that the disk and network were no longer the system bottlenecks. With an initial working set size of 300MB and a SPECsfs input load of 7000 ops/sec, the measured throughputs of NFS, UCDP-I, and UCDP-K were 6560, 4166, and 5441 ops/sec, respectively. UCDP-K indeed outperformed UCDP-I by 30%.

### Related Work

WAFL [12] is a general-purpose, high-performance file system with snapshot support developed by Network Appliance. WAFL is not optimized for fine-grained file update logging and allows only a limited number (originally 32) of snapshots. Each snapshot is taken at a coarse granularity, and the cost is amortized over hundreds of file updates.

Wayback [8] is a user-level, comprehensive versioning file system for Linux. For each data update, Wayback uses an undo logging scheme similar to UCDP-O. For metadata updates, Wayback incurs a higher overhead than all of our logging schemes. Compared to traditional file systems, Wayback's performance for normal file system updates is quite poor. When compared to EXT3, the data read/write overhead ranges from -2% to 70%, and the metadata update overhead ranges from 100% to 400%. In contrast, the performance of UCDP-I and UCDP-K is comparable to that of a generic NFS server running on top of EXT3.

CVFS [11] is a kernel-level, comprehensive versioning file system optimized for metadata logging efficiency. It uses journal-based metadata for inode/indirect-block updates and a multiversion B-tree [10] for directory updates. S4 [16] is a secure network-attached object store designed to protect against malicious attacks. S4 logs every update and minimizes space usage. It uses a log-structured design to avoid overwriting old data and improves inode/indirect-block logging efficiency by encoding changes in a logging record. Although S4's logging scheme has low overhead, its cleaning cost can be as high as 50%.

Log-structured file systems [20] have been used to reduce the disk access penalty of random small writes. They share a common feature with file update logging: non-overwrite to save old data. Log-structured file systems write data to new locations in large batches to improve write performance. In LFS, cleaning is essential to maintain disk locality, but the overhead is often high. This issue is not pronounced in the proposed user-level file update logging system because it uses a base image (Section 4.3.1) to maintain disk locality, and the cleaning cycle is much longer, thus amortizing the cost.

Elephant [13] is a kernel-level versioning file system that creates a new version only when a file is closed. Therefore, it does not distinguish updates between file open and close operations. VersionFS [19] is a versioning file system implemented using stackable file system techniques [22]. Similar to Elephant [13], VersionFS bases versions on open-close sessions and provides flexible versioning policies. VersionFS also offers a user-friendly interface for accessing old versions and customizing versioning policies. However, VersionFS still incurs a non-negligible performance overhead—about 100% when measured by the Postmark benchmark.

Clotho [17] is a versioning system at the disk block level. Compared to file system versioning, block-level versioning is less complex due to its simpler interface. However, it is more challenging for users to directly manage versions of disk blocks, typically requiring an additional layer to provide easy version access. Clotho aggregates updates that occur within a period (e.g., a minute) and creates new versions, but it does not support CDP.

### Conclusion

Continuous Data Protection (CDP) is a critical component for quickly repairing damage to a file system caused by malicious attacks or human errors. Despite its importance, CDP has not been widely adopted in mainstream file servers due to concerns about additional storage requirements, performance overhead, and implementation complexity. With the significant reduction in the per-byte cost of magnetic disk technology, storage cost is no longer a major issue. Real-world NFS trace measurements show that a $200 200GB disk can easily support a one-month logging window for a large NFS server with 400GB of data and an average load of 34 requests/sec [23]. However, the performance overhead and implementation complexity remain significant barriers to CDP deployment. This paper describes a user-level CDP architecture that is both efficient and portable, eliminating these barriers. We implemented four variants of this user-level CDP architecture and compared their latency, throughput, and CPU usage characteristics using standard benchmarks, NFS traces, and synthetic workloads. The main lessons from this implementation and performance study are:

- User-level continuous data protection based on the NFS protocol, which is portable across multiple operating system platforms, is feasible and relatively simple to implement.
- UCDP-A incurs almost no latency and throughput penalty compared to an unprotected vanilla NFS server, making it the best choice for IT environments where performance is a top priority, mirroring the file system image is desirable, and minimal disruption to the primary NFS server is important.
- User-level continuous data protection, when embedded into an NFS server, can achieve comparable throughput to an unprotected NFS server, although it incurs a 3-5 millisecond latency penalty when the write request percentage exceeds 36%. The write request percentage in typical NFS sites, as specified in the SPECsfs benchmark, is less than 12%.
- If portability can be slightly compromised, simple in-kernel optimizations can significantly decrease the CPU overhead due to context switching and memory copying associated with user-level CDP. However, these optimizations do not produce noticeable latency and throughput improvements when the write request percentage is below 12%.
- Logging updates at a higher level of abstraction, such as NFS requests and replies, tends to produce a more compact log than logging at a lower level of abstraction, such as disk accesses and responses. This approach is also more portable and flexible.

### Acknowledgement

This research was supported by NSF awards SCI-0401777, CNS-0410694, and CNS-0435373.

### References

[1] The Advanced Maryland Automatic Network Disk Archiver. (http://www.amanda.org/).

[2] Concurrent Versions System. (http://www.cvshome.org/).

[3] Enterprise Rewinder: Product Suite for Continuous Data Protection (CDP). (http://www.xosoft.com/).

[4] RealTime - Near-Instant Recovery to Any Point in Time. (http://www.mendocinosoft.com/).

[5] System File Server Benchmark SPEC SFS97 R1 V3.0. Standard Performance Evaluation Corporation. (http://www.specbench.org/sfs97r1/).

[6] NFS: Network file system protocol specification. Sun Microsystems, Mar 1989.

[7] A. Chervenak, V. Vellanki, and Z. Kurmas. Protecting file systems: A survey of backup techniques. In Proceedings Joint NASA and IEEE Mass Storage Conference, March 1998.

[8] Brian Cornell, Peter A. Dinda, and Fabian E. Bustamante. Wayback: A user-level versioning file system for Linux. In USENIX 2004 Annual Technical Conference (Freenix).

[9] D. Ellard, J. Ledlie, P. Malkani, and M. Seltzer. Passive NFS tracing of email and research workloads. In 2nd USENIX Conference on File and Storage Technologies, Mar 2003.

[10] B. Becker et al. An asymptotically optimal multiversion B-tree. Very Large Data Bases Journal, 1996.

[11] C.A.N. Soules et al. Metadata efficiency in a comprehensive versioning file system. In 2nd USENIX Conference on File and Storage Technologies, Mar 2003.

[12] D. Hitz et al. File system design for an NFS file server appliance. In USENIX winter 1994 conference, pages 235–246, Chateau Lake Louise, Banff, Canada, 1994.

[13] D. S. Santry et al. Deciding when to forget in the Elephant file system. In Proceedings of the Seventeenth ACM Symposium on Operating Systems Principles, pages 110–123, December 12-15, 1999.

[14] G. W. Dunlap et al. Revirt: Enabling intrusion analysis through virtual-machine logging and replay. In Proceedings of 5th Symposium on Operating Systems Design and Implementation, Dec 2002.

[15] Hugo Patterson et al. SnapMirror: File system-based asynchronous mirroring for disaster recovery. In Conference on File and Storage Technologies, pages 28–30, Monterey, CA, January 2002.

[16] J. Strunk et al. Self-securing storage: Protecting data in compromised systems. In Proceedings of the 2000 OSDI Conference, October 2000.

[17] Michail D. Flouris and Angelos Bilas. Clotho: Transparent data versioning at the block I/O level. In 21st IEEE Conference on Mass Storage Systems and Technologies, April 2004.

[18] D. Mazieres. A toolkit for user-level file systems. In Proceedings of the 2001 USENIX Technical Conference, pages 261–274, June 2001.

[19] K. Muniswamy-Reddy, C. P. Wright, A. Himmer, and E. Zadok. A Versatile and User-Oriented Versioning File System. In Proceedings of the Third USENIX Conference on File and Storage Technologies (FAST 2004), pages 115–128, San Francisco, CA, March/April 2004.

[20] M. Rosenblum and J. K. Ousterhout. The design and implementation of a log-structured file system. In ACM Transactions on Computer Systems, 1991.

[21] Michael Rowan. Continuous Data Protection: A Technical Overview. (http://www.revivio.com/index.asp?p=tech white papers, 2004).

[22] E. Zadok and J. Nieh. FiST: A Language for Stackable File Systems. In Proceedings of the Annual USENIX Technical Conference, pages 55–70, June 2000.

[23] N. Zhu and T. Chiueh. Design, implementation, and evaluation of repairable file service. In The International Conference on Dependable Systems and Networks, June 2003.