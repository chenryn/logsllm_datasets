Thus, the white box attacker is characterized by the knowl-
edge of (𝑓 , 𝑤). With that information, the attacker could ei-
ther run a basic exhaustive search, basic optimization strate-
gies, or more sophisticated approaches (especially solutions
that use the gradient signal from the attacked model).
• Black box (
,
), the attacker is aware of the general
detection scheme, but unaware of internal variables, architec-
ture and exact thresholds used in the classification. We note
that our black box attacker is different from the one defined
in [6], ( ˆ𝑓 , ˆ𝑤). Our attacker does not require the knowledge
of 𝑓 or its approximation ˆ𝑓 . In our case, the nature of the
environment imposes that the attacker cannot query the
system even in a black box manner to get feedback on the
provided labels or confidence scores (this is done for example
in [12, 14, 56, 61]), as this would mean potentially raising
the alarm. Thus, we consider that the only assumption of
the attacker concerning 𝑓 is that Deep Learning techniques
are used for detection.
Given this taxonomy, the attacker can be classified for example,
as unconstrained white box.
3.3 Example Constraint Scenarios
We argue our Constrained and Unconstrained attacks represent
a realistic threat model in the ICS setting and fit the taxonomy
of attacks in the AML literature. In particular, practical ICS are
typically composed of multiple stages, and each stage is controlled
by a different PLC (i.e., different brands/models). Moreover, the ICS
can be deployed in a physically distributed manner. For example,
in the case of water distribution networks, pumping stations are
typically located kilometers away from the water reservoir. In this
heterogeneous setting, an attacker can either gain control over a
limited set of resources as practically demonstrated in [17], or the
ACSAC2020,December7–11,2020,Austin,USAA.Erbaetal.Table1:Classificationofourattackermodelsbasedontrainingdataandfeatures,Datatuple(D,X)andalgorithmknowledgeandparameters.Access:(cid:32)=full,(cid:71)(cid:35)=partial.Forallwhiteboxattacks,theDefensetupleis(f,w),forallblackboxattacks,(AAˆf,Zˆw).Attacker’sXConstraintsDReadWriteUnconstrained§5.4(cid:32)(cid:32)(cid:32)XPartially§5.5(cid:32)(cid:32)(cid:71)(cid:35)XFully§5.5(cid:32)(cid:71)(cid:35)(cid:71)(cid:35)D§5.5(cid:71)(cid:35)(cid:32)(cid:32)theadversarialattackhastofindaperturbation(cid:174)δtominimizethereconstructionerrorbetweentheinput(cid:174)x+δandoutputˆ(cid:174)xoftheReconstruction-baseddetector.PleaserefertoSection4.1forfur-therdetailsonthetargetmodel.Inamathematicalnotation,itcanbewrittenasthefollowingconstrainedoptimizationprobleminEquation1:minimizeMSE=1nni=1(ˆxi−(xi+δi))2s.t.(cid:174)δ∈constraintspace(Section3.2.2)real-timeconstraintsimposedbyCPSy((cid:174)x+(cid:174)δ)=‘safe’(1)Wenotethattheattackswedemonstratearenotnecessarilyop-timal,astheconstraintsaresatisfiedwithnonuniquesolutions.Theattacksareconductedinreal-time(i.e.,inmillisecondspertimestep),notaposteriori(i.e.,appliedretrospectivelytoalongerse-quenceofsensorreadingsaftertheattackerfullyreceivesthem).3.2.2AttackerKnowledge.UsingtheadversariallearningnotationintroducedinSection2,aconcealmentattackischaracterizedbytheknowledgeoftheattackeraboutthetrainingdatasetD,featuresetX,learningalgorithmf,andtrainedparametersw.IntheICSsetting,theattackercanbecharacterizeddifferentlyaccordingtohisknowledgeoftheattackedsystem.Inordertoexplainourattackermodel,wesplitthetuple(D,X,f,w)intotwo:theDatatuple(D,X)andDefensetuple(f,w).Weassumetheattackertobeunconstrainedorconstrainedw.r.t.theDatatuple,i.e.,thesensorreadingsXthatshecanobserveandmanipulateandthedataDthatsheeavesdrops.Moreover,weclassifyattackersaswhitebox,blackbox,w.r.t.theDefensetuple,i.e.,theknowledgeoflearningalgorithmf,andtrainedparametersw.Table1,providesanoverviewoftheattacker’sconstraintsconsideredinthiswork.ConstraintsoverDataTuple.AccordingtotheDatatuple(D,X),weclassifytheattackeras:•Unconstrained(D,X),inwhichtheattackercanmanipulateallnfeaturesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetobeatmostn.•FeaturesPartiallyConstrained(D,ˆX),weassumethattheattackerisconstrainedtoperturbasubsetofkoutofnvariablesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetonotexceeddistancek.•FeaturesFully-Constrained(D,ˆX),weassumethattheat-tackerisconstrainedtoobserveandperturbasubsetofkoutofnvariablesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetonotexceeddistancek.•DataConstrained(ˆD,X),weassumethattheattackerisconstrainedtoeavesdropalimitedquantityofprocessdatathatareusedfortrainingitsattacks.SelectionofConstrainedFeaturesThesubsetoffeaturesthatcanbemodifiedishighlyuse-casedependent(i.e.,whichlinkisattacked,whichdevicewascompromised).Todemonstratethegeneralityofourfindings,weexploredtwotypesofconstraints:abest-casescenarioandatopology-basedscenario.Forthebest-casescenario,weassumethattheselectionofthekoutofnmanipulatedfeaturescanbemadebytheattackertomaximizetheattackimpact.Thisarguablyrepresentsabest-casescenariofortheconstrainedattacker(i.e.,anattackerconstrainedtofeaturesthathappentoberelativelyidealfortheattacker).Forthesecondscenario,constraintsarederivedfromthenetworktopology.Weassumethattheattackercancompromiseasinglesubstation(orPLC)inthenetwork,andtheselectionofkoutofnfeaturesisbasedonwhichsensorsareinterconnectedtothecompromisedsubstation.KnowledgeofDefenseTuple.WeclassifytheattackeraccordingtotheirknowledgeoftheDefensetuple(f,w),as:•Whitebox(f,w),theattackerknowstheexactsystemmodelanditsvariables(suchasthecurrentlyestimatedsystemstate),andtheexactthresholdsoftheclassificationsystem.Thus,thewhiteboxattackerischaracterizedbytheknowl-edgeof(f,w).Withthatinformation,theattackercouldei-therrunabasicexhaustivesearch,basicoptimizationstrate-gies,ormoresophisticatedapproaches(especiallysolutionsthatusethegradientsignalfromtheattackedmodel).•Blackbox(AAˆf,Zˆw),theattackerisawareofthegeneralde-tectionscheme,butunawareofinternalvariables,architec-tureandexactthresholdsusedintheclassification.Wenotethatourblackboxattackerisdifferentfromtheonedefinedin[6],(ˆf,ˆw).Ourattackerdoesnotrequiretheknowledgeofforitsapproximationˆf.Inourcase,thenatureoftheenvironmentimposesthattheattackercannotquerythesystemeveninablackboxmannertogetfeedbackontheprovidedlabelsorconfidencescores(thisisdoneforexamplein[12,14,56,61]),asthiswouldmeanpotentiallyraisingthealarm.Thus,weconsiderthattheonlyassumptionoftheattackerconcerningfisthatDeepLearningtechniquesareusedfordetection.Giventhistaxonomy,theattackercanbeclassifiedforexample,asunconstrainedwhitebox.3.3ExampleConstraintScenariosWeargueourConstrainedandUnconstrainedattacksrepresentarealisticthreatmodelintheICSsettingandfitthetaxonomyofattacksintheAMLliterature.Inparticular,practicalICSaretypicallycomposedofmultiplestages,andeachstageiscontrolledbyadifferentPLC(i.e.,differentbrands/models).Moreover,theICScanbedeployedinaphysicallydistributedmanner.Forexample,ACSAC2020,December7–11,2020,Austin,USAA.Erbaetal.Table1:Classificationofourattackermodelsbasedontrainingdataandfeatures,Datatuple(D,X)andalgorithmknowledgeandparameters.Access:(cid:32)=full,(cid:71)(cid:35)=partial.Forallwhiteboxattacks,theDefensetupleis(f,w),forallblackboxattacks,(AAˆf,Zˆw).Attacker’sXConstraintsDReadWriteUnconstrained§5.4(cid:32)(cid:32)(cid:32)XPartially§5.5(cid:32)(cid:32)(cid:71)(cid:35)XFully§5.5(cid:32)(cid:71)(cid:35)(cid:71)(cid:35)D§5.5(cid:71)(cid:35)(cid:32)(cid:32)theadversarialattackhastofindaperturbation(cid:174)δtominimizethereconstructionerrorbetweentheinput(cid:174)x+δandoutputˆ(cid:174)xoftheReconstruction-baseddetector.PleaserefertoSection4.1forfur-therdetailsonthetargetmodel.Inamathematicalnotation,itcanbewrittenasthefollowingconstrainedoptimizationprobleminEquation1:minimizeMSE=1nni=1(ˆxi−(xi+δi))2s.t.(cid:174)δ∈constraintspace(Section3.2.2)real-timeconstraintsimposedbyCPSy((cid:174)x+(cid:174)δ)=‘safe’(1)Wenotethattheattackswedemonstratearenotnecessarilyop-timal,astheconstraintsaresatisfiedwithnonuniquesolutions.Theattacksareconductedinreal-time(i.e.,inmillisecondspertimestep),notaposteriori(i.e.,appliedretrospectivelytoalongerse-quenceofsensorreadingsaftertheattackerfullyreceivesthem).3.2.2AttackerKnowledge.UsingtheadversariallearningnotationintroducedinSection2,aconcealmentattackischaracterizedbytheknowledgeoftheattackeraboutthetrainingdatasetD,featuresetX,learningalgorithmf,andtrainedparametersw.IntheICSsetting,theattackercanbecharacterizeddifferentlyaccordingtohisknowledgeoftheattackedsystem.Inordertoexplainourattackermodel,wesplitthetuple(D,X,f,w)intotwo:theDatatuple(D,X)andDefensetuple(f,w).Weassumetheattackertobeunconstrainedorconstrainedw.r.t.theDatatuple,i.e.,thesensorreadingsXthatshecanobserveandmanipulateandthedataDthatsheeavesdrops.Moreover,weclassifyattackersaswhitebox,blackbox,w.r.t.theDefensetuple,i.e.,theknowledgeoflearningalgorithmf,andtrainedparametersw.Table1,providesanoverviewoftheattacker’sconstraintsconsideredinthiswork.ConstraintsoverDataTuple.AccordingtotheDatatuple(D,X),weclassifytheattackeras:•Unconstrained(D,X),inwhichtheattackercanmanipulateallnfeaturesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetobeatmostn.•FeaturesPartiallyConstrained(D,ˆX),weassumethattheattackerisconstrainedtoperturbasubsetofkoutofnvariablesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetonotexceeddistancek.•FeaturesFully-Constrained(D,ˆX),weassumethattheat-tackerisconstrainedtoobserveandperturbasubsetofkoutofnvariablesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetonotexceeddistancek.•DataConstrained(ˆD,X),weassumethattheattackerisconstrainedtoeavesdropalimitedquantityofprocessdatathatareusedfortrainingitsattacks.SelectionofConstrainedFeaturesThesubsetoffeaturesthatcanbemodifiedishighlyuse-casedependent(i.e.,whichlinkisattacked,whichdevicewascompromised).Todemonstratethegeneralityofourfindings,weexploredtwotypesofconstraints:abest-casescenarioandatopology-basedscenario.Forthebest-casescenario,weassumethattheselectionofthekoutofnmanipulatedfeaturescanbemadebytheattackertomaximizetheattackimpact.Thisarguablyrepresentsabest-casescenariofortheconstrainedattacker(i.e.,anattackerconstrainedtofeaturesthathappentoberelativelyidealfortheattacker).Forthesecondscenario,constraintsarederivedfromthenetworktopology.Weassumethattheattackercancompromiseasinglesubstation(orPLC)inthenetwork,andtheselectionofkoutofnfeaturesisbasedonwhichsensorsareinterconnectedtothecompromisedsubstation.KnowledgeofDefenseTuple.WeclassifytheattackeraccordingtotheirknowledgeoftheDefensetuple(f,w),as:•Whitebox(f,w),theattackerknowstheexactsystemmodelanditsvariables(suchasthecurrentlyestimatedsystemstate),andtheexactthresholdsoftheclassificationsystem.Thus,thewhiteboxattackerischaracterizedbytheknowl-edgeof(f,w).Withthatinformation,theattackercouldei-therrunabasicexhaustivesearch,basicoptimizationstrate-gies,ormoresophisticatedapproaches(especiallysolutionsthatusethegradientsignalfromtheattackedmodel).•Blackbox(AAˆf,Zˆw),theattackerisawareofthegeneralde-tectionscheme,butunawareofinternalvariables,architec-tureandexactthresholdsusedintheclassification.Wenotethatourblackboxattackerisdifferentfromtheonedefinedin[6],(ˆf,ˆw).Ourattackerdoesnotrequiretheknowledgeofforitsapproximationˆf.Inourcase,thenatureoftheenvironmentimposesthattheattackercannotquerythesystemeveninablackboxmannertogetfeedbackontheprovidedlabelsorconfidencescores(thisisdoneforexamplein[12,14,56,61]),asthiswouldmeanpotentiallyraisingthealarm.Thus,weconsiderthattheonlyassumptionoftheattackerconcerningfisthatDeepLearningtechniquesareusedfordetection.Giventhistaxonomy,theattackercanbeclassifiedforexample,asunconstrainedwhitebox.3.3ExampleConstraintScenariosWeargueourConstrainedandUnconstrainedattacksrepresentarealisticthreatmodelintheICSsettingandfitthetaxonomyofattacksintheAMLliterature.Inparticular,practicalICSaretypicallycomposedofmultiplestages,andeachstageiscontrolledbyadifferentPLC(i.e.,differentbrands/models).Moreover,theICScanbedeployedinaphysicallydistributedmanner.Forexample,ACSAC2020,December7–11,2020,Austin,USAA.Erbaetal.Table1:Classificationofourattackermodelsbasedontrainingdataandfeatures,Datatuple(D,X)andalgorithmknowledgeandparameters.Access:ACSAC2020,December7–11,2020,Austin,USAA.Erbaetal.Table1:Classificationofourattackermodelsbasedontrainingdataandfeatures,Datatuple(D,X)andalgorithmknowledgeandparameters.Access:(cid:32)=full,(cid:71)(cid:35)=partial.Forallwhiteboxattacks,theDefensetupleis(f,w),forallblackboxattacks,(AAˆf,Zˆw).Attacker’sXConstraintsDReadWriteUnconstrained§5.4(cid:32)(cid:32)(cid:32)XPartially§5.5(cid:32)(cid:32)(cid:71)(cid:35)XFully§5.5(cid:32)(cid:71)(cid:35)(cid:71)(cid:35)D§5.5(cid:71)(cid:35)(cid:32)(cid:32)theadversarialattackhastofindaperturbation(cid:174)δtominimizethereconstructionerrorbetweentheinput(cid:174)x+δandoutputˆ(cid:174)xoftheReconstruction-baseddetector.PleaserefertoSection4.1forfur-therdetailsonthetargetmodel.Inamathematicalnotation,itcanbewrittenasthefollowingconstrainedoptimizationprobleminEquation1:minimizeMSE=1nni=1(ˆxi−(xi+δi))2s.t.(cid:174)δ∈constraintspace(Section3.2.2)real-timeconstraintsimposedbyCPSy((cid:174)x+(cid:174)δ)=‘safe’(1)Wenotethattheattackswedemonstratearenotnecessarilyop-timal,astheconstraintsaresatisfiedwithnonuniquesolutions.Theattacksareconductedinreal-time(i.e.,inmillisecondspertimestep),notaposteriori(i.e.,appliedretrospectivelytoalongerse-quenceofsensorreadingsaftertheattackerfullyreceivesthem).3.2.2AttackerKnowledge.UsingtheadversariallearningnotationintroducedinSection2,aconcealmentattackischaracterizedbytheknowledgeoftheattackeraboutthetrainingdatasetD,featuresetX,learningalgorithmf,andtrainedparametersw.IntheICSsetting,theattackercanbecharacterizeddifferentlyaccordingtohisknowledgeoftheattackedsystem.Inordertoexplainourattackermodel,wesplitthetuple(D,X,f,w)intotwo:theDatatuple(D,X)andDefensetuple(f,w).Weassumetheattackertobeunconstrainedorconstrainedw.r.t.theDatatuple,i.e.,thesensorreadingsXthatshecanobserveandmanipulateandthedataDthatsheeavesdrops.Moreover,weclassifyattackersaswhitebox,blackbox,w.r.t.theDefensetuple,i.e.,theknowledgeoflearningalgorithmf,andtrainedparametersw.Table1,providesanoverviewoftheattacker’sconstraintsconsideredinthiswork.ConstraintsoverDataTuple.AccordingtotheDatatuple(D,X),weclassifytheattackeras:•Unconstrained(D,X),inwhichtheattackercanmanipulateallnfeaturesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetobeatmostn.•FeaturesPartiallyConstrained(D,ˆX),weassumethattheattackerisconstrainedtoperturbasubsetofkoutofnvariablesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetonotexceeddistancek.•FeaturesFully-Constrained(D,ˆX),weassumethattheat-tackerisconstrainedtoobserveandperturbasubsetofkoutofnvariablesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetonotexceeddistancek.•DataConstrained(ˆD,X),weassumethattheattackerisconstrainedtoeavesdropalimitedquantityofprocessdatathatareusedfortrainingitsattacks.SelectionofConstrainedFeaturesThesubsetoffeaturesthatcanbemodifiedishighlyuse-casedependent(i.e.,whichlinkisattacked,whichdevicewascompromised).Todemonstratethegeneralityofourfindings,weexploredtwotypesofconstraints:abest-casescenarioandatopology-basedscenario.Forthebest-casescenario,weassumethattheselectionofthekoutofnmanipulatedfeaturescanbemadebytheattackertomaximizetheattackimpact.Thisarguablyrepresentsabest-casescenariofortheconstrainedattacker(i.e.,anattackerconstrainedtofeaturesthathappentoberelativelyidealfortheattacker).Forthesecondscenario,constraintsarederivedfromthenetworktopology.Weassumethattheattackercancompromiseasinglesubstation(orPLC)inthenetwork,andtheselectionofkoutofnfeaturesisbasedonwhichsensorsareinterconnectedtothecompromisedsubstation.KnowledgeofDefenseTuple.WeclassifytheattackeraccordingtotheirknowledgeoftheDefensetuple(f,w),as:•Whitebox(f,w),theattackerknowstheexactsystemmodelanditsvariables(suchasthecurrentlyestimatedsystemstate),andtheexactthresholdsoftheclassificationsystem.Thus,thewhiteboxattackerischaracterizedbytheknowl-edgeof(f,w).Withthatinformation,theattackercouldei-therrunabasicexhaustivesearch,basicoptimizationstrate-gies,ormoresophisticatedapproaches(especiallysolutionsthatusethegradientsignalfromtheattackedmodel).•Blackbox(AAˆf,Zˆw),theattackerisawareofthegeneralde-tectionscheme,butunawareofinternalvariables,architec-tureandexactthresholdsusedintheclassification.Wenotethatourblackboxattackerisdifferentfromtheonedefinedin[6],(ˆf,ˆw).Ourattackerdoesnotrequiretheknowledgeofforitsapproximationˆf.Inourcase,thenatureoftheenvironmentimposesthattheattackercannotquerythesystemeveninablackboxmannertogetfeedbackontheprovidedlabelsorconfidencescores(thisisdoneforexamplein[12,14,56,61]),asthiswouldmeanpotentiallyraisingthealarm.Thus,weconsiderthattheonlyassumptionoftheattackerconcerningfisthatDeepLearningtechniquesareusedfordetection.Giventhistaxonomy,theattackercanbeclassifiedforexample,asunconstrainedwhitebox.3.3ExampleConstraintScenariosWeargueourConstrainedandUnconstrainedattacksrepresentarealisticthreatmodelintheICSsettingandfitthetaxonomyofattacksintheAMLliterature.Inparticular,practicalICSaretypicallycomposedofmultiplestages,andeachstageiscontrolledbyadifferentPLC(i.e.,differentbrands/models).Moreover,theICScanbedeployedinaphysicallydistributedmanner.Forexample,=full,ACSAC2020,December7–11,2020,Austin,USAA.Erbaetal.Table1:Classificationofourattackermodelsbasedontrainingdataandfeatures,Datatuple(D,X)andalgorithmknowledgeandparameters.Access:(cid:32)=full,(cid:71)(cid:35)=partial.Forallwhiteboxattacks,theDefensetupleis(f,w),forallblackboxattacks,(AAˆf,Zˆw).Attacker’sXConstraintsDReadWriteUnconstrained§5.4(cid:32)(cid:32)(cid:32)XPartially§5.5(cid:32)(cid:32)(cid:71)(cid:35)XFully§5.5(cid:32)(cid:71)(cid:35)(cid:71)(cid:35)D§5.5(cid:71)(cid:35)(cid:32)(cid:32)theadversarialattackhastofindaperturbation(cid:174)δtominimizethereconstructionerrorbetweentheinput(cid:174)x+δandoutputˆ(cid:174)xoftheReconstruction-baseddetector.PleaserefertoSection4.1forfur-therdetailsonthetargetmodel.Inamathematicalnotation,itcanbewrittenasthefollowingconstrainedoptimizationprobleminEquation1:minimizeMSE=1nni=1(ˆxi−(xi+δi))2s.t.(cid:174)δ∈constraintspace(Section3.2.2)real-timeconstraintsimposedbyCPSy((cid:174)x+(cid:174)δ)=‘safe’(1)Wenotethattheattackswedemonstratearenotnecessarilyop-timal,astheconstraintsaresatisfiedwithnonuniquesolutions.Theattacksareconductedinreal-time(i.e.,inmillisecondspertimestep),notaposteriori(i.e.,appliedretrospectivelytoalongerse-quenceofsensorreadingsaftertheattackerfullyreceivesthem).3.2.2AttackerKnowledge.UsingtheadversariallearningnotationintroducedinSection2,aconcealmentattackischaracterizedbytheknowledgeoftheattackeraboutthetrainingdatasetD,featuresetX,learningalgorithmf,andtrainedparametersw.IntheICSsetting,theattackercanbecharacterizeddifferentlyaccordingtohisknowledgeoftheattackedsystem.Inordertoexplainourattackermodel,wesplitthetuple(D,X,f,w)intotwo:theDatatuple(D,X)andDefensetuple(f,w).Weassumetheattackertobeunconstrainedorconstrainedw.r.t.theDatatuple,i.e.,thesensorreadingsXthatshecanobserveandmanipulateandthedataDthatsheeavesdrops.Moreover,weclassifyattackersaswhitebox,blackbox,w.r.t.theDefensetuple,i.e.,theknowledgeoflearningalgorithmf,andtrainedparametersw.Table1,providesanoverviewoftheattacker’sconstraintsconsideredinthiswork.ConstraintsoverDataTuple.AccordingtotheDatatuple(D,X),weclassifytheattackeras:•Unconstrained(D,X),inwhichtheattackercanmanipulateallnfeaturesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetobeatmostn.•FeaturesPartiallyConstrained(D,ˆX),weassumethattheattackerisconstrainedtoperturbasubsetofkoutofnvariablesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetonotexceeddistancek.•FeaturesFully-Constrained(D,ˆX),weassumethattheat-tackerisconstrainedtoobserveandperturbasubsetofkoutofnvariablesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetonotexceeddistancek.•DataConstrained(ˆD,X),weassumethattheattackerisconstrainedtoeavesdropalimitedquantityofprocessdatathatareusedfortrainingitsattacks.SelectionofConstrainedFeaturesThesubsetoffeaturesthatcanbemodifiedishighlyuse-casedependent(i.e.,whichlinkisattacked,whichdevicewascompromised).Todemonstratethegeneralityofourfindings,weexploredtwotypesofconstraints:abest-casescenarioandatopology-basedscenario.Forthebest-casescenario,weassumethattheselectionofthekoutofnmanipulatedfeaturescanbemadebytheattackertomaximizetheattackimpact.Thisarguablyrepresentsabest-casescenariofortheconstrainedattacker(i.e.,anattackerconstrainedtofeaturesthathappentoberelativelyidealfortheattacker).Forthesecondscenario,constraintsarederivedfromthenetworktopology.Weassumethattheattackercancompromiseasinglesubstation(orPLC)inthenetwork,andtheselectionofkoutofnfeaturesisbasedonwhichsensorsareinterconnectedtothecompromisedsubstation.KnowledgeofDefenseTuple.WeclassifytheattackeraccordingtotheirknowledgeoftheDefensetuple(f,w),as:•Whitebox(f,w),theattackerknowstheexactsystemmodelanditsvariables(suchasthecurrentlyestimatedsystemstate),andtheexactthresholdsoftheclassificationsystem.Thus,thewhiteboxattackerischaracterizedbytheknowl-edgeof(f,w).Withthatinformation,theattackercouldei-therrunabasicexhaustivesearch,basicoptimizationstrate-gies,ormoresophisticatedapproaches(especiallysolutionsthatusethegradientsignalfromtheattackedmodel).•Blackbox(AAˆf,Zˆw),theattackerisawareofthegeneralde-tectionscheme,butunawareofinternalvariables,architec-tureandexactthresholdsusedintheclassification.Wenotethatourblackboxattackerisdifferentfromtheonedefinedin[6],(ˆf,ˆw).Ourattackerdoesnotrequiretheknowledgeofforitsapproximationˆf.Inourcase,thenatureoftheenvironmentimposesthattheattackercannotquerythesystemeveninablackboxmannertogetfeedbackontheprovidedlabelsorconfidencescores(thisisdoneforexamplein[12,14,56,61]),asthiswouldmeanpotentiallyraisingthealarm.Thus,weconsiderthattheonlyassumptionoftheattackerconcerningfisthatDeepLearningtechniquesareusedfordetection.Giventhistaxonomy,theattackercanbeclassifiedforexample,asunconstrainedwhitebox.3.3ExampleConstraintScenariosWeargueourConstrainedandUnconstrainedattacksrepresentarealisticthreatmodelintheICSsettingandfitthetaxonomyofattacksintheAMLliterature.Inparticular,practicalICSaretypicallycomposedofmultiplestages,andeachstageiscontrolledbyadifferentPLC(i.e.,differentbrands/models).Moreover,theICScanbedeployedinaphysicallydistributedmanner.Forexample,=partial.Forallwhiteboxattacks,theDefensetupleis(f,w),forallblackboxattacks,AAˆfZˆwAttacker’sXConstraintsDReadWriteUnconstrained§5.4(cid:32)(cid:32)(cid:32)XPartially§5.5(cid:32)(cid:32)(cid:71)(cid:35)XFully§5.5(cid:32)(cid:71)(cid:35)(cid:71)(cid:35)D§5.5(cid:71)(cid:35)(cid:32)(cid:32)theadversarialattackhastofindaperturbation(cid:174)δtominimizethereconstructionerrorbetweentheinput(cid:174)x+δandoutputˆ(cid:174)xoftheReconstruction-baseddetector.PleaserefertoSection4.1forfur-therdetailsonthetargetmodel.Inamathematicalnotation,itcanbewrittenasthefollowingconstrainedoptimizationprobleminEquation1:minimizeMSE=1nni=1(ˆxi−(xi+δi))2s.t.(cid:174)δ∈constraintspace(Section3.2.2)real-timeconstraintsimposedbyCPSy((cid:174)x+(cid:174)δ)=‘safe’(1)Wenotethattheattackswedemonstratearenotnecessarilyop-timal,astheconstraintsaresatisfiedwithnonuniquesolutions.Theattacksareconductedinreal-time(i.e.,inmillisecondspertimestep),notaposteriori(i.e.,appliedretrospectivelytoalongerse-quenceofsensorreadingsaftertheattackerfullyreceivesthem).3.2.2AttackerKnowledge.UsingtheadversariallearningnotationintroducedinSection2,aconcealmentattackischaracterizedbytheknowledgeoftheattackeraboutthetrainingdatasetD,featuresetX,learningalgorithmf,andtrainedparametersw.IntheICSsetting,theattackercanbecharacterizeddifferentlyaccordingtohisknowledgeoftheattackedsystem.Inordertoexplainourattackermodel,wesplitthetuple(D,X,f,w)intotwo:theDatatuple(D,X)andDefensetuple(f,w).Weassumetheattackertobeunconstrainedorconstrainedw.r.t.theDatatuple,i.e.,thesensorreadingsXthatshecanobserveandmanipulateandthedataDthatsheeavesdrops.Moreover,weclassifyattackersaswhitebox,blackbox,w.r.t.theDefensetuple,i.e.,theknowledgeoflearningalgorithmf,andtrainedparametersw.Table1,providesanoverviewoftheattacker’sconstraintsconsideredinthiswork.ConstraintsoverDataTuple.AccordingtotheDatatuple(D,X),weclassifytheattackeras:•Unconstrained(D,X),inwhichtheattackercanmanipulateallnfeaturesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetobeatmostn.•FeaturesPartiallyConstrained(D,ˆX),weassumethattheattackerisconstrainedtoperturbasubsetofkoutofnvariablesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetonotexceeddistancek.•FeaturesFully-Constrained(D,ˆX),weassumethattheat-tackerisconstrainedtoobserveandperturbasubsetofkoutofnvariablesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetonotexceeddistancek.•DataConstrained(ˆD,X),weassumethattheattackerisconstrainedtoeavesdropalimitedquantityofprocessdatathatareusedfortrainingitsattacks.SelectionofConstrainedFeaturesThesubsetoffeaturesthatcanbemodifiedishighlyuse-casedependent(i.e.,whichlinkisattacked,whichdevicewascompromised).Todemonstratethegeneralityofourfindings,weexploredtwotypesofconstraints:abest-casescenarioandatopology-basedscenario.Forthebest-casescenario,weassumethattheselectionofthekoutofnmanipulatedfeaturescanbemadebytheattackertomaximizetheattackimpact.Thisarguablyrepresentsabest-casescenariofortheconstrainedattacker(i.e.,anattackerconstrainedtofeaturesthathappentoberelativelyidealfortheattacker).Forthesecondscenario,constraintsarederivedfromthenetworktopology.Weassumethattheattackercancompromiseasinglesubstation(orPLC)inthenetwork,andtheselectionofkoutofnfeaturesisbasedonwhichsensorsareinterconnectedtothecompromisedsubstation.KnowledgeofDefenseTuple.WeclassifytheattackeraccordingtotheirknowledgeoftheDefensetuple(f,w),as:•Whitebox(f,w),theattackerknowstheexactsystemmodelanditsvariables(suchasthecurrentlyestimatedsystemstate),andtheexactthresholdsoftheclassificationsystem.Thus,thewhiteboxattackerischaracterizedbytheknowl-edgeof(f,w).Withthatinformation,theattackercouldei-therrunabasicexhaustivesearch,basicoptimizationstrate-gies,ormoresophisticatedapproaches(especiallysolutionsthatusethegradientsignalfromtheattackedmodel).•Blackbox(AAˆf,Zˆw),theattackerisawareofthegeneralde-tectionscheme,butunawareofinternalvariables,architec-tureandexactthresholdsusedintheclassification.Wenotethatourblackboxattackerisdifferentfromtheonedefinedin[6],(ˆf,ˆw).Ourattackerdoesnotrequiretheknowledgeofforitsapproximationˆf.Inourcase,thenatureoftheenvironmentimposesthattheattackercannotquerythesystemeveninablackboxmannertogetfeedbackontheprovidedlabelsorconfidencescores(thisisdoneforexamplein[12,14,56,61]),asthiswouldmeanpotentiallyraisingthealarm.Thus,weconsiderthattheonlyassumptionoftheattackerconcerningfisthatDeepLearningtechniquesareusedfordetection.Giventhistaxonomy,theattackercanbeclassifiedforexample,asunconstrainedwhitebox.3.3ExampleConstraintScenariosWeargueourConstrainedandUnconstrainedattacksrepresentarealisticthreatmodelintheICSsettingandfitthetaxonomyofattacksintheAMLliterature.Inparticular,practicalICSaretypicallycomposedofmultiplestages,andeachstageiscontrolledbyadifferentPLC(i.e.,differentbrands/models).Moreover,theICScanbedeployedinaphysicallydistributedmanner.Forexample,ACSAC2020,December7–11,2020,Austin,USAA.Erbaetal.Table1:Classificationofourattackermodelsbasedontrainingdataandfeatures,Datatuple(D,X)andalgorithmknowledgeandparameters.Access:ACSAC2020,December7–11,2020,Austin,USAA.Erbaetal.Table1:Classificationofourattackermodelsbasedontrainingdataandfeatures,Datatuple(D,X)andalgorithmknowledgeandparameters.Access:(cid:32)=full,(cid:71)(cid:35)=partial.Forallwhiteboxattacks,theDefensetupleis(f,w),forallblackboxattacks,(AAˆf,Zˆw).Attacker’sXConstraintsDReadWriteUnconstrained§5.4(cid:32)(cid:32)(cid:32)XPartially§5.5(cid:32)(cid:32)(cid:71)(cid:35)XFully§5.5(cid:32)(cid:71)(cid:35)(cid:71)(cid:35)D§5.5(cid:71)(cid:35)(cid:32)(cid:32)theadversarialattackhastofindaperturbation(cid:174)δtominimizethereconstructionerrorbetweentheinput(cid:174)x+δandoutputˆ(cid:174)xoftheReconstruction-baseddetector.PleaserefertoSection4.1forfur-therdetailsonthetargetmodel.Inamathematicalnotation,itcanbewrittenasthefollowingconstrainedoptimizationprobleminEquation1:minimizeMSE=1nni=1(ˆxi−(xi+δi))2s.t.(cid:174)δ∈constraintspace(Section3.2.2)real-timeconstraintsimposedbyCPSy((cid:174)x+(cid:174)δ)=‘safe’(1)Wenotethattheattackswedemonstratearenotnecessarilyop-timal,astheconstraintsaresatisfiedwithnonuniquesolutions.Theattacksareconductedinreal-time(i.e.,inmillisecondspertimestep),notaposteriori(i.e.,appliedretrospectivelytoalongerse-quenceofsensorreadingsaftertheattackerfullyreceivesthem).3.2.2AttackerKnowledge.UsingtheadversariallearningnotationintroducedinSection2,aconcealmentattackischaracterizedbytheknowledgeoftheattackeraboutthetrainingdatasetD,featuresetX,learningalgorithmf,andtrainedparametersw.IntheICSsetting,theattackercanbecharacterizeddifferentlyaccordingtohisknowledgeoftheattackedsystem.Inordertoexplainourattackermodel,wesplitthetuple(D,X,f,w)intotwo:theDatatuple(D,X)andDefensetuple(f,w).Weassumetheattackertobeunconstrainedorconstrainedw.r.t.theDatatuple,i.e.,thesensorreadingsXthatshecanobserveandmanipulateandthedataDthatsheeavesdrops.Moreover,weclassifyattackersaswhitebox,blackbox,w.r.t.theDefensetuple,i.e.,theknowledgeoflearningalgorithmf,andtrainedparametersw.Table1,providesanoverviewoftheattacker’sconstraintsconsideredinthiswork.ConstraintsoverDataTuple.AccordingtotheDatatuple(D,X),weclassifytheattackeras:•Unconstrained(D,X),inwhichtheattackercanmanipulateallnfeaturesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetobeatmostn.•FeaturesPartiallyConstrained(D,ˆX),weassumethattheattackerisconstrainedtoperturbasubsetofkoutofnvariablesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetonotexceeddistancek.•FeaturesFully-Constrained(D,ˆX),weassumethattheat-tackerisconstrainedtoobserveandperturbasubsetofkoutofnvariablesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetonotexceeddistancek.•DataConstrained(ˆD,X),weassumethattheattackerisconstrainedtoeavesdropalimitedquantityofprocessdatathatareusedfortrainingitsattacks.SelectionofConstrainedFeaturesThesubsetoffeaturesthatcanbemodifiedishighlyuse-casedependent(i.e.,whichlinkisattacked,whichdevicewascompromised).Todemonstratethegeneralityofourfindings,weexploredtwotypesofconstraints:abest-casescenarioandatopology-basedscenario.Forthebest-casescenario,weassumethattheselectionofthekoutofnmanipulatedfeaturescanbemadebytheattackertomaximizetheattackimpact.Thisarguablyrepresentsabest-casescenariofortheconstrainedattacker(i.e.,anattackerconstrainedtofeaturesthathappentoberelativelyidealfortheattacker).Forthesecondscenario,constraintsarederivedfromthenetworktopology.Weassumethattheattackercancompromiseasinglesubstation(orPLC)inthenetwork,andtheselectionofkoutofnfeaturesisbasedonwhichsensorsareinterconnectedtothecompromisedsubstation.KnowledgeofDefenseTuple.WeclassifytheattackeraccordingtotheirknowledgeoftheDefensetuple(f,w),as:•Whitebox(f,w),theattackerknowstheexactsystemmodelanditsvariables(suchasthecurrentlyestimatedsystemstate),andtheexactthresholdsoftheclassificationsystem.Thus,thewhiteboxattackerischaracterizedbytheknowl-edgeof(f,w).Withthatinformation,theattackercouldei-therrunabasicexhaustivesearch,basicoptimizationstrate-gies,ormoresophisticatedapproaches(especiallysolutionsthatusethegradientsignalfromtheattackedmodel).•Blackbox(AAˆf,Zˆw),theattackerisawareofthegeneralde-tectionscheme,butunawareofinternalvariables,architec-tureandexactthresholdsusedintheclassification.Wenotethatourblackboxattackerisdifferentfromtheonedefinedin[6],(ˆf,ˆw).Ourattackerdoesnotrequiretheknowledgeofforitsapproximationˆf.Inourcase,thenatureoftheenvironmentimposesthattheattackercannotquerythesystemeveninablackboxmannertogetfeedbackontheprovidedlabelsorconfidencescores(thisisdoneforexamplein[12,14,56,61]),asthiswouldmeanpotentiallyraisingthealarm.Thus,weconsiderthattheonlyassumptionoftheattackerconcerningfisthatDeepLearningtechniquesareusedfordetection.Giventhistaxonomy,theattackercanbeclassifiedforexample,asunconstrainedwhitebox.3.3ExampleConstraintScenariosWeargueourConstrainedandUnconstrainedattacksrepresentarealisticthreatmodelintheICSsettingandfitthetaxonomyofattacksintheAMLliterature.Inparticular,practicalICSaretypicallycomposedofmultiplestages,andeachstageiscontrolledbyadifferentPLC(i.e.,differentbrands/models).Moreover,theICScanbedeployedinaphysicallydistributedmanner.Forexample,=full,ACSAC2020,December7–11,2020,Austin,USAA.Erbaetal.Table1:Classificationofourattackermodelsbasedontrainingdataandfeatures,Datatuple(D,X)andalgorithmknowledgeandparameters.Access:(cid:32)=full,(cid:71)(cid:35)=partial.Forallwhiteboxattacks,theDefensetupleis(f,w),forallblackboxattacks,(AAˆf,Zˆw).Attacker’sXConstraintsDReadWriteUnconstrained§5.4(cid:32)(cid:32)(cid:32)XPartially§5.5(cid:32)(cid:32)(cid:71)(cid:35)XFully§5.5(cid:32)(cid:71)(cid:35)(cid:71)(cid:35)D§5.5(cid:71)(cid:35)(cid:32)(cid:32)theadversarialattackhastofindaperturbation(cid:174)δtominimizethereconstructionerrorbetweentheinput(cid:174)x+δandoutputˆ(cid:174)xoftheReconstruction-baseddetector.PleaserefertoSection4.1forfur-therdetailsonthetargetmodel.Inamathematicalnotation,itcanbewrittenasthefollowingconstrainedoptimizationprobleminEquation1:minimizeMSE=1nni=1(ˆxi−(xi+δi))2s.t.(cid:174)δ∈constraintspace(Section3.2.2)real-timeconstraintsimposedbyCPSy((cid:174)x+(cid:174)δ)=‘safe’(1)Wenotethattheattackswedemonstratearenotnecessarilyop-timal,astheconstraintsaresatisfiedwithnonuniquesolutions.Theattacksareconductedinreal-time(i.e.,inmillisecondspertimestep),notaposteriori(i.e.,appliedretrospectivelytoalongerse-quenceofsensorreadingsaftertheattackerfullyreceivesthem).3.2.2AttackerKnowledge.UsingtheadversariallearningnotationintroducedinSection2,aconcealmentattackischaracterizedbytheknowledgeoftheattackeraboutthetrainingdatasetD,featuresetX,learningalgorithmf,andtrainedparametersw.IntheICSsetting,theattackercanbecharacterizeddifferentlyaccordingtohisknowledgeoftheattackedsystem.Inordertoexplainourattackermodel,wesplitthetuple(D,X,f,w)intotwo:theDatatuple(D,X)andDefensetuple(f,w).Weassumetheattackertobeunconstrainedorconstrainedw.r.t.theDatatuple,i.e.,thesensorreadingsXthatshecanobserveandmanipulateandthedataDthatsheeavesdrops.Moreover,weclassifyattackersaswhitebox,blackbox,w.r.t.theDefensetuple,i.e.,theknowledgeoflearningalgorithmf,andtrainedparametersw.Table1,providesanoverviewoftheattacker’sconstraintsconsideredinthiswork.ConstraintsoverDataTuple.AccordingtotheDatatuple(D,X),weclassifytheattackeras:•Unconstrained(D,X),inwhichtheattackercanmanipulateallnfeaturesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetobeatmostn.•FeaturesPartiallyConstrained(D,ˆX),weassumethattheattackerisconstrainedtoperturbasubsetofkoutofnvariablesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetonotexceeddistancek.•FeaturesFully-Constrained(D,ˆX),weassumethattheat-tackerisconstrainedtoobserveandperturbasubsetofkoutofnvariablesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetonotexceeddistancek.•DataConstrained(ˆD,X),weassumethattheattackerisconstrainedtoeavesdropalimitedquantityofprocessdatathatareusedfortrainingitsattacks.SelectionofConstrainedFeaturesThesubsetoffeaturesthatcanbemodifiedishighlyuse-casedependent(i.e.,whichlinkisattacked,whichdevicewascompromised).Todemonstratethegeneralityofourfindings,weexploredtwotypesofconstraints:abest-casescenarioandatopology-basedscenario.Forthebest-casescenario,weassumethattheselectionofthekoutofnmanipulatedfeaturescanbemadebytheattackertomaximizetheattackimpact.Thisarguablyrepresentsabest-casescenariofortheconstrainedattacker(i.e.,anattackerconstrainedtofeaturesthathappentoberelativelyidealfortheattacker).Forthesecondscenario,constraintsarederivedfromthenetworktopology.Weassumethattheattackercancompromiseasinglesubstation(orPLC)inthenetwork,andtheselectionofkoutofnfeaturesisbasedonwhichsensorsareinterconnectedtothecompromisedsubstation.KnowledgeofDefenseTuple.WeclassifytheattackeraccordingtotheirknowledgeoftheDefensetuple(f,w),as:•Whitebox(f,w),theattackerknowstheexactsystemmodelanditsvariables(suchasthecurrentlyestimatedsystemstate),andtheexactthresholdsoftheclassificationsystem.Thus,thewhiteboxattackerischaracterizedbytheknowl-edgeof(f,w).Withthatinformation,theattackercouldei-therrunabasicexhaustivesearch,basicoptimizationstrate-gies,ormoresophisticatedapproaches(especiallysolutionsthatusethegradientsignalfromtheattackedmodel).•Blackbox(AAˆf,Zˆw),theattackerisawareofthegeneralde-tectionscheme,butunawareofinternalvariables,architec-tureandexactthresholdsusedintheclassification.Wenotethatourblackboxattackerisdifferentfromtheonedefinedin[6],(ˆf,ˆw).Ourattackerdoesnotrequiretheknowledgeofforitsapproximationˆf.Inourcase,thenatureoftheenvironmentimposesthattheattackercannotquerythesystemeveninablackboxmannertogetfeedbackontheprovidedlabelsorconfidencescores(thisisdoneforexamplein[12,14,56,61]),asthiswouldmeanpotentiallyraisingthealarm.Thus,weconsiderthattheonlyassumptionoftheattackerconcerningfisthatDeepLearningtechniquesareusedfordetection.Giventhistaxonomy,theattackercanbeclassifiedforexample,asunconstrainedwhitebox.3.3ExampleConstraintScenariosWeargueourConstrainedandUnconstrainedattacksrepresentarealisticthreatmodelintheICSsettingandfitthetaxonomyofattacksintheAMLliterature.Inparticular,practicalICSaretypicallycomposedofmultiplestages,andeachstageiscontrolledbyadifferentPLC(i.e.,differentbrands/models).Moreover,theICScanbedeployedinaphysicallydistributedmanner.Forexample,=partial.Forallwhiteboxattacks,theDefensetupleis(f,w),forallblackboxattacks,AAˆfZˆwAttacker’sXConstraintsDReadWriteUnconstrained§5.4(cid:32)(cid:32)(cid:32)XPartially§5.5(cid:32)(cid:32)(cid:71)(cid:35)XFully§5.5(cid:32)(cid:71)(cid:35)(cid:71)(cid:35)D§5.5(cid:71)(cid:35)(cid:32)(cid:32)theadversarialattackhastofindaperturbation(cid:174)δtominimizethereconstructionerrorbetweentheinput(cid:174)x+δandoutputˆ(cid:174)xoftheReconstruction-baseddetector.PleaserefertoSection4.1forfur-therdetailsonthetargetmodel.Inamathematicalnotation,itcanbewrittenasthefollowingconstrainedoptimizationprobleminEquation1:minimizeMSE=1nni=1(ˆxi−(xi+δi))2s.t.(cid:174)δ∈constraintspace(Section3.2.2)real-timeconstraintsimposedbyCPSy((cid:174)x+(cid:174)δ)=‘safe’(1)Wenotethattheattackswedemonstratearenotnecessarilyop-timal,astheconstraintsaresatisfiedwithnonuniquesolutions.Theattacksareconductedinreal-time(i.e.,inmillisecondspertimestep),notaposteriori(i.e.,appliedretrospectivelytoalongerse-quenceofsensorreadingsaftertheattackerfullyreceivesthem).3.2.2AttackerKnowledge.UsingtheadversariallearningnotationintroducedinSection2,aconcealmentattackischaracterizedbytheknowledgeoftheattackeraboutthetrainingdatasetD,featuresetX,learningalgorithmf,andtrainedparametersw.IntheICSsetting,theattackercanbecharacterizeddifferentlyaccordingtohisknowledgeoftheattackedsystem.Inordertoexplainourattackermodel,wesplitthetuple(D,X,f,w)intotwo:theDatatuple(D,X)andDefensetuple(f,w).Weassumetheattackertobeunconstrainedorconstrainedw.r.t.theDatatuple,i.e.,thesensorreadingsXthatshecanobserveandmanipulateandthedataDthatsheeavesdrops.Moreover,weclassifyattackersaswhitebox,blackbox,w.r.t.theDefensetuple,i.e.,theknowledgeoflearningalgorithmf,andtrainedparametersw.Table1,providesanoverviewoftheattacker’sconstraintsconsideredinthiswork.ConstraintsoverDataTuple.AccordingtotheDatatuple(D,X),weclassifytheattackeras:•Unconstrained(D,X),inwhichtheattackercanmanipulateallnfeaturesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetobeatmostn.•FeaturesPartiallyConstrained(D,ˆX),weassumethattheattackerisconstrainedtoperturbasubsetofkoutofnvariablesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetonotexceeddistancek.•FeaturesFully-Constrained(D,ˆX),weassumethattheat-tackerisconstrainedtoobserveandperturbasubsetofkoutofnvariablesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetonotexceeddistancek.•DataConstrained(ˆD,X),weassumethattheattackerisconstrainedtoeavesdropalimitedquantityofprocessdatathatareusedfortrainingitsattacks.SelectionofConstrainedFeaturesThesubsetoffeaturesthatcanbemodifiedishighlyuse-casedependent(i.e.,whichlinkisattacked,whichdevicewascompromised).Todemonstratethegeneralityofourfindings,weexploredtwotypesofconstraints:abest-casescenarioandatopology-basedscenario.Forthebest-casescenario,weassumethattheselectionofthekoutofnmanipulatedfeaturescanbemadebytheattackertomaximizetheattackimpact.Thisarguablyrepresentsabest-casescenariofortheconstrainedattacker(i.e.,anattackerconstrainedtofeaturesthathappentoberelativelyidealfortheattacker).Forthesecondscenario,constraintsarederivedfromthenetworktopology.Weassumethattheattackercancompromiseasinglesubstation(orPLC)inthenetwork,andtheselectionofkoutofnfeaturesisbasedonwhichsensorsareinterconnectedtothecompromisedsubstation.KnowledgeofDefenseTuple.WeclassifytheattackeraccordingtotheirknowledgeoftheDefensetuple(f,w),as:•Whitebox(f,w),theattackerknowstheexactsystemmodelanditsvariables(suchasthecurrentlyestimatedsystemstate),andtheexactthresholdsoftheclassificationsystem.Thus,thewhiteboxattackerischaracterizedbytheknowl-edgeof(f,w).Withthatinformation,theattackercouldei-therrunabasicexhaustivesearch,basicoptimizationstrate-gies,ormoresophisticatedapproaches(especiallysolutionsthatusethegradientsignalfromtheattackedmodel).•Blackbox(AAˆf,Zˆw),theattackerisawareofthegeneralde-tectionscheme,butunawareofinternalvariables,architec-tureandexactthresholdsusedintheclassification.Wenotethatourblackboxattackerisdifferentfromtheonedefinedin[6],(ˆf,ˆw).Ourattackerdoesnotrequiretheknowledgeofforitsapproximationˆf.Inourcase,thenatureoftheenvironmentimposesthattheattackercannotquerythesystemeveninablackboxmannertogetfeedbackontheprovidedlabelsorconfidencescores(thisisdoneforexamplein[12,14,56,61]),asthiswouldmeanpotentiallyraisingthealarm.Thus,weconsiderthattheonlyassumptionoftheattackerconcerningfisthatDeepLearningtechniquesareusedfordetection.Giventhistaxonomy,theattackercanbeclassifiedforexample,asunconstrainedwhitebox.3.3ExampleConstraintScenariosWeargueourConstrainedandUnconstrainedattacksrepresentarealisticthreatmodelintheICSsettingandfitthetaxonomyofattacksintheAMLliterature.Inparticular,practicalICSaretypicallycomposedofmultiplestages,andeachstageiscontrolledbyadifferentPLC(i.e.,differentbrands/models).Moreover,theICScanbedeployedinaphysicallydistributedmanner.Forexample,ACSAC2020,December7–11,2020,Austin,USAA.Erbaetal.Table1:Classificationofourattackermodelsbasedontrainingdataandfeatures,Datatuple(D,X)andalgorithmknowledgeandparameters.Access:ACSAC2020,December7–11,2020,Austin,USAA.Erbaetal.Table1:Classificationofourattackermodelsbasedontrainingdataandfeatures,Datatuple(D,X)andalgorithmknowledgeandparameters.Access:(cid:32)=full,(cid:71)(cid:35)=partial.Forallwhiteboxattacks,theDefensetupleis(f,w),forallblackboxattacks,(AAˆf,Zˆw).Attacker’sXConstraintsDReadWriteUnconstrained§5.4(cid:32)(cid:32)(cid:32)XPartially§5.5(cid:32)(cid:32)(cid:71)(cid:35)XFully§5.5(cid:32)(cid:71)(cid:35)(cid:71)(cid:35)D§5.5(cid:71)(cid:35)(cid:32)(cid:32)theadversarialattackhastofindaperturbation(cid:174)δtominimizethereconstructionerrorbetweentheinput(cid:174)x+δandoutputˆ(cid:174)xoftheReconstruction-baseddetector.PleaserefertoSection4.1forfur-therdetailsonthetargetmodel.Inamathematicalnotation,itcanbewrittenasthefollowingconstrainedoptimizationprobleminEquation1:minimizeMSE=1nni=1(ˆxi−(xi+δi))2s.t.(cid:174)δ∈constraintspace(Section3.2.2)real-timeconstraintsimposedbyCPSy((cid:174)x+(cid:174)δ)=‘safe’(1)Wenotethattheattackswedemonstratearenotnecessarilyop-timal,astheconstraintsaresatisfiedwithnonuniquesolutions.Theattacksareconductedinreal-time(i.e.,inmillisecondspertimestep),notaposteriori(i.e.,appliedretrospectivelytoalongerse-quenceofsensorreadingsaftertheattackerfullyreceivesthem).3.2.2AttackerKnowledge.UsingtheadversariallearningnotationintroducedinSection2,aconcealmentattackischaracterizedbytheknowledgeoftheattackeraboutthetrainingdatasetD,featuresetX,learningalgorithmf,andtrainedparametersw.IntheICSsetting,theattackercanbecharacterizeddifferentlyaccordingtohisknowledgeoftheattackedsystem.Inordertoexplainourattackermodel,wesplitthetuple(D,X,f,w)intotwo:theDatatuple(D,X)andDefensetuple(f,w).Weassumetheattackertobeunconstrainedorconstrainedw.r.t.theDatatuple,i.e.,thesensorreadingsXthatshecanobserveandmanipulateandthedataDthatsheeavesdrops.Moreover,weclassifyattackersaswhitebox,blackbox,w.r.t.theDefensetuple,i.e.,theknowledgeoflearningalgorithmf,andtrainedparametersw.Table1,providesanoverviewoftheattacker’sconstraintsconsideredinthiswork.ConstraintsoverDataTuple.AccordingtotheDatatuple(D,X),weclassifytheattackeras:•Unconstrained(D,X),inwhichtheattackercanmanipulateallnfeaturesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetobeatmostn.•FeaturesPartiallyConstrained(D,ˆX),weassumethattheattackerisconstrainedtoperturbasubsetofkoutofnvariablesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetonotexceeddistancek.•FeaturesFully-Constrained(D,ˆX),weassumethattheat-tackerisconstrainedtoobserveandperturbasubsetofkoutofnvariablesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetonotexceeddistancek.•DataConstrained(ˆD,X),weassumethattheattackerisconstrainedtoeavesdropalimitedquantityofprocessdatathatareusedfortrainingitsattacks.SelectionofConstrainedFeaturesThesubsetoffeaturesthatcanbemodifiedishighlyuse-casedependent(i.e.,whichlinkisattacked,whichdevicewascompromised).Todemonstratethegeneralityofourfindings,weexploredtwotypesofconstraints:abest-casescenarioandatopology-basedscenario.Forthebest-casescenario,weassumethattheselectionofthekoutofnmanipulatedfeaturescanbemadebytheattackertomaximizetheattackimpact.Thisarguablyrepresentsabest-casescenariofortheconstrainedattacker(i.e.,anattackerconstrainedtofeaturesthathappentoberelativelyidealfortheattacker).Forthesecondscenario,constraintsarederivedfromthenetworktopology.Weassumethattheattackercancompromiseasinglesubstation(orPLC)inthenetwork,andtheselectionofkoutofnfeaturesisbasedonwhichsensorsareinterconnectedtothecompromisedsubstation.KnowledgeofDefenseTuple.WeclassifytheattackeraccordingtotheirknowledgeoftheDefensetuple(f,w),as:•Whitebox(f,w),theattackerknowstheexactsystemmodelanditsvariables(suchasthecurrentlyestimatedsystemstate),andtheexactthresholdsoftheclassificationsystem.Thus,thewhiteboxattackerischaracterizedbytheknowl-edgeof(f,w).Withthatinformation,theattackercouldei-therrunabasicexhaustivesearch,basicoptimizationstrate-gies,ormoresophisticatedapproaches(especiallysolutionsthatusethegradientsignalfromtheattackedmodel).•Blackbox(AAˆf,Zˆw),theattackerisawareofthegeneralde-tectionscheme,butunawareofinternalvariables,architec-tureandexactthresholdsusedintheclassification.Wenotethatourblackboxattackerisdifferentfromtheonedefinedin[6],(ˆf,ˆw).Ourattackerdoesnotrequiretheknowledgeofforitsapproximationˆf.Inourcase,thenatureoftheenvironmentimposesthattheattackercannotquerythesystemeveninablackboxmannertogetfeedbackontheprovidedlabelsorconfidencescores(thisisdoneforexamplein[12,14,56,61]),asthiswouldmeanpotentiallyraisingthealarm.Thus,weconsiderthattheonlyassumptionoftheattackerconcerningfisthatDeepLearningtechniquesareusedfordetection.Giventhistaxonomy,theattackercanbeclassifiedforexample,asunconstrainedwhitebox.3.3ExampleConstraintScenariosWeargueourConstrainedandUnconstrainedattacksrepresentarealisticthreatmodelintheICSsettingandfitthetaxonomyofattacksintheAMLliterature.Inparticular,practicalICSaretypicallycomposedofmultiplestages,andeachstageiscontrolledbyadifferentPLC(i.e.,differentbrands/models).Moreover,theICScanbedeployedinaphysicallydistributedmanner.Forexample,=full,ACSAC2020,December7–11,2020,Austin,USAA.Erbaetal.Table1:Classificationofourattackermodelsbasedontrainingdataandfeatures,Datatuple(D,X)andalgorithmknowledgeandparameters.Access:(cid:32)=full,(cid:71)(cid:35)=partial.Forallwhiteboxattacks,theDefensetupleis(f,w),forallblackboxattacks,(AAˆf,Zˆw).Attacker’sXConstraintsDReadWriteUnconstrained§5.4(cid:32)(cid:32)(cid:32)XPartially§5.5(cid:32)(cid:32)(cid:71)(cid:35)XFully§5.5(cid:32)(cid:71)(cid:35)(cid:71)(cid:35)D§5.5(cid:71)(cid:35)(cid:32)(cid:32)theadversarialattackhastofindaperturbation(cid:174)δtominimizethereconstructionerrorbetweentheinput(cid:174)x+δandoutputˆ(cid:174)xoftheReconstruction-baseddetector.PleaserefertoSection4.1forfur-therdetailsonthetargetmodel.Inamathematicalnotation,itcanbewrittenasthefollowingconstrainedoptimizationprobleminEquation1:minimizeMSE=1nni=1(ˆxi−(xi+δi))2s.t.(cid:174)δ∈constraintspace(Section3.2.2)real-timeconstraintsimposedbyCPSy((cid:174)x+(cid:174)δ)=‘safe’(1)Wenotethattheattackswedemonstratearenotnecessarilyop-timal,astheconstraintsaresatisfiedwithnonuniquesolutions.Theattacksareconductedinreal-time(i.e.,inmillisecondspertimestep),notaposteriori(i.e.,appliedretrospectivelytoalongerse-quenceofsensorreadingsaftertheattackerfullyreceivesthem).3.2.2AttackerKnowledge.UsingtheadversariallearningnotationintroducedinSection2,aconcealmentattackischaracterizedbytheknowledgeoftheattackeraboutthetrainingdatasetD,featuresetX,learningalgorithmf,andtrainedparametersw.IntheICSsetting,theattackercanbecharacterizeddifferentlyaccordingtohisknowledgeoftheattackedsystem.Inordertoexplainourattackermodel,wesplitthetuple(D,X,f,w)intotwo:theDatatuple(D,X)andDefensetuple(f,w).Weassumetheattackertobeunconstrainedorconstrainedw.r.t.theDatatuple,i.e.,thesensorreadingsXthatshecanobserveandmanipulateandthedataDthatsheeavesdrops.Moreover,weclassifyattackersaswhitebox,blackbox,w.r.t.theDefensetuple,i.e.,theknowledgeoflearningalgorithmf,andtrainedparametersw.Table1,providesanoverviewoftheattacker’sconstraintsconsideredinthiswork.ConstraintsoverDataTuple.AccordingtotheDatatuple(D,X),weclassifytheattackeras:•Unconstrained(D,X),inwhichtheattackercanmanipulateallnfeaturesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetobeatmostn.•FeaturesPartiallyConstrained(D,ˆX),weassumethattheattackerisconstrainedtoperturbasubsetofkoutofnvariablesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetonotexceeddistancek.•FeaturesFully-Constrained(D,ˆX),weassumethattheat-tackerisconstrainedtoobserveandperturbasubsetofkoutofnvariablesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetonotexceeddistancek.•DataConstrained(ˆD,X),weassumethattheattackerisconstrainedtoeavesdropalimitedquantityofprocessdatathatareusedfortrainingitsattacks.SelectionofConstrainedFeaturesThesubsetoffeaturesthatcanbemodifiedishighlyuse-casedependent(i.e.,whichlinkisattacked,whichdevicewascompromised).Todemonstratethegeneralityofourfindings,weexploredtwotypesofconstraints:abest-casescenarioandatopology-basedscenario.Forthebest-casescenario,weassumethattheselectionofthekoutofnmanipulatedfeaturescanbemadebytheattackertomaximizetheattackimpact.Thisarguablyrepresentsabest-casescenariofortheconstrainedattacker(i.e.,anattackerconstrainedtofeaturesthathappentoberelativelyidealfortheattacker).Forthesecondscenario,constraintsarederivedfromthenetworktopology.Weassumethattheattackercancompromiseasinglesubstation(orPLC)inthenetwork,andtheselectionofkoutofnfeaturesisbasedonwhichsensorsareinterconnectedtothecompromisedsubstation.KnowledgeofDefenseTuple.WeclassifytheattackeraccordingtotheirknowledgeoftheDefensetuple(f,w),as:•Whitebox(f,w),theattackerknowstheexactsystemmodelanditsvariables(suchasthecurrentlyestimatedsystemstate),andtheexactthresholdsoftheclassificationsystem.Thus,thewhiteboxattackerischaracterizedbytheknowl-edgeof(f,w).Withthatinformation,theattackercouldei-therrunabasicexhaustivesearch,basicoptimizationstrate-gies,ormoresophisticatedapproaches(especiallysolutionsthatusethegradientsignalfromtheattackedmodel).•Blackbox(AAˆf,Zˆw),theattackerisawareofthegeneralde-tectionscheme,butunawareofinternalvariables,architec-tureandexactthresholdsusedintheclassification.Wenotethatourblackboxattackerisdifferentfromtheonedefinedin[6],(ˆf,ˆw).Ourattackerdoesnotrequiretheknowledgeofforitsapproximationˆf.Inourcase,thenatureoftheenvironmentimposesthattheattackercannotquerythesystemeveninablackboxmannertogetfeedbackontheprovidedlabelsorconfidencescores(thisisdoneforexamplein[12,14,56,61]),asthiswouldmeanpotentiallyraisingthealarm.Thus,weconsiderthattheonlyassumptionoftheattackerconcerningfisthatDeepLearningtechniquesareusedfordetection.Giventhistaxonomy,theattackercanbeclassifiedforexample,asunconstrainedwhitebox.3.3ExampleConstraintScenariosWeargueourConstrainedandUnconstrainedattacksrepresentarealisticthreatmodelintheICSsettingandfitthetaxonomyofattacksintheAMLliterature.Inparticular,practicalICSaretypicallycomposedofmultiplestages,andeachstageiscontrolledbyadifferentPLC(i.e.,differentbrands/models).Moreover,theICScanbedeployedinaphysicallydistributedmanner.Forexample,=partial.Forallwhiteboxattacks,theDefensetupleis(f,w),forallblackboxattacks,AAˆfZˆwAttacker’sXConstraintsDReadWriteUnconstrained§5.4(cid:32)(cid:32)(cid:32)XPartially§5.5(cid:32)(cid:32)(cid:71)(cid:35)XFully§5.5(cid:32)(cid:71)(cid:35)(cid:71)(cid:35)D§5.5(cid:71)(cid:35)(cid:32)(cid:32)theadversarialattackhastofindaperturbation(cid:174)δtominimizethereconstructionerrorbetweentheinput(cid:174)x+δandoutputˆ(cid:174)xoftheReconstruction-baseddetector.PleaserefertoSection4.1forfur-therdetailsonthetargetmodel.Inamathematicalnotation,itcanbewrittenasthefollowingconstrainedoptimizationprobleminEquation1:minimizeMSE=1nni=1(ˆxi−(xi+δi))2s.t.(cid:174)δ∈constraintspace(Section3.2.2)real-timeconstraintsimposedbyCPSy((cid:174)x+(cid:174)δ)=‘safe’(1)Wenotethattheattackswedemonstratearenotnecessarilyop-timal,astheconstraintsaresatisfiedwithnonuniquesolutions.Theattacksareconductedinreal-time(i.e.,inmillisecondspertimestep),notaposteriori(i.e.,appliedretrospectivelytoalongerse-quenceofsensorreadingsaftertheattackerfullyreceivesthem).3.2.2AttackerKnowledge.UsingtheadversariallearningnotationintroducedinSection2,aconcealmentattackischaracterizedbytheknowledgeoftheattackeraboutthetrainingdatasetD,featuresetX,learningalgorithmf,andtrainedparametersw.IntheICSsetting,theattackercanbecharacterizeddifferentlyaccordingtohisknowledgeoftheattackedsystem.Inordertoexplainourattackermodel,wesplitthetuple(D,X,f,w)intotwo:theDatatuple(D,X)andDefensetuple(f,w).Weassumetheattackertobeunconstrainedorconstrainedw.r.t.theDatatuple,i.e.,thesensorreadingsXthatshecanobserveandmanipulateandthedataDthatsheeavesdrops.Moreover,weclassifyattackersaswhitebox,blackbox,w.r.t.theDefensetuple,i.e.,theknowledgeoflearningalgorithmf,andtrainedparametersw.Table1,providesanoverviewoftheattacker’sconstraintsconsideredinthiswork.ConstraintsoverDataTuple.AccordingtotheDatatuple(D,X),weclassifytheattackeras:•Unconstrained(D,X),inwhichtheattackercanmanipulateallnfeaturesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetobeatmostn.•FeaturesPartiallyConstrained(D,ˆX),weassumethattheattackerisconstrainedtoperturbasubsetofkoutofnvariablesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetonotexceeddistancek.•FeaturesFully-Constrained(D,ˆX),weassumethattheat-tackerisconstrainedtoobserveandperturbasubsetofkoutofnvariablesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetonotexceeddistancek.•DataConstrained(ˆD,X),weassumethattheattackerisconstrainedtoeavesdropalimitedquantityofprocessdatathatareusedfortrainingitsattacks.SelectionofConstrainedFeaturesThesubsetoffeaturesthatcanbemodifiedishighlyuse-casedependent(i.e.,whichlinkisattacked,whichdevicewascompromised).Todemonstratethegeneralityofourfindings,weexploredtwotypesofconstraints:abest-casescenarioandatopology-basedscenario.Forthebest-casescenario,weassumethattheselectionofthekoutofnmanipulatedfeaturescanbemadebytheattackertomaximizetheattackimpact.Thisarguablyrepresentsabest-casescenariofortheconstrainedattacker(i.e.,anattackerconstrainedtofeaturesthathappentoberelativelyidealfortheattacker).Forthesecondscenario,constraintsarederivedfromthenetworktopology.Weassumethattheattackercancompromiseasinglesubstation(orPLC)inthenetwork,andtheselectionofkoutofnfeaturesisbasedonwhichsensorsareinterconnectedtothecompromisedsubstation.KnowledgeofDefenseTuple.WeclassifytheattackeraccordingtotheirknowledgeoftheDefensetuple(f,w),as:•Whitebox(f,w),theattackerknowstheexactsystemmodelanditsvariables(suchasthecurrentlyestimatedsystemstate),andtheexactthresholdsoftheclassificationsystem.Thus,thewhiteboxattackerischaracterizedbytheknowl-edgeof(f,w).Withthatinformation,theattackercouldei-therrunabasicexhaustivesearch,basicoptimizationstrate-gies,ormoresophisticatedapproaches(especiallysolutionsthatusethegradientsignalfromtheattackedmodel).•Blackbox(AAˆf,Zˆw),theattackerisawareofthegeneralde-tectionscheme,butunawareofinternalvariables,architec-tureandexactthresholdsusedintheclassification.Wenotethatourblackboxattackerisdifferentfromtheonedefinedin[6],(ˆf,ˆw).Ourattackerdoesnotrequiretheknowledgeofforitsapproximationˆf.Inourcase,thenatureoftheenvironmentimposesthattheattackercannotquerythesystemeveninablackboxmannertogetfeedbackontheprovidedlabelsorconfidencescores(thisisdoneforexamplein[12,14,56,61]),asthiswouldmeanpotentiallyraisingthealarm.Thus,weconsiderthattheonlyassumptionoftheattackerconcerningfisthatDeepLearningtechniquesareusedfordetection.Giventhistaxonomy,theattackercanbeclassifiedforexample,asunconstrainedwhitebox.3.3ExampleConstraintScenariosWeargueourConstrainedandUnconstrainedattacksrepresentarealisticthreatmodelintheICSsettingandfitthetaxonomyofattacksintheAMLliterature.Inparticular,practicalICSaretypicallycomposedofmultiplestages,andeachstageiscontrolledbyadifferentPLC(i.e.,differentbrands/models).Moreover,theICScanbedeployedinaphysicallydistributedmanner.Forexample,ACSAC2020,December7–11,2020,Austin,USAA.Erbaetal.Table1:Classificationofourattackermodelsbasedontrainingdataandfeatures,Datatuple(D,X)andalgorithmknowledgeandparameters.Access:ACSAC2020,December7–11,2020,Austin,USAA.Erbaetal.Table1:Classificationofourattackermodelsbasedontrainingdataandfeatures,Datatuple(D,X)andalgorithmknowledgeandparameters.Access:(cid:32)=full,(cid:71)(cid:35)=partial.Forallwhiteboxattacks,theDefensetupleis(f,w),forallblackboxattacks,(AAˆf,Zˆw).Attacker’sXConstraintsDReadWriteUnconstrained§5.4(cid:32)(cid:32)(cid:32)XPartially§5.5(cid:32)(cid:32)(cid:71)(cid:35)XFully§5.5(cid:32)(cid:71)(cid:35)(cid:71)(cid:35)D§5.5(cid:71)(cid:35)(cid:32)(cid:32)theadversarialattackhastofindaperturbation(cid:174)δtominimizethereconstructionerrorbetweentheinput(cid:174)x+δandoutputˆ(cid:174)xoftheReconstruction-baseddetector.PleaserefertoSection4.1forfur-therdetailsonthetargetmodel.Inamathematicalnotation,itcanbewrittenasthefollowingconstrainedoptimizationprobleminEquation1:minimizeMSE=1nni=1(ˆxi−(xi+δi))2s.t.(cid:174)δ∈constraintspace(Section3.2.2)real-timeconstraintsimposedbyCPSy((cid:174)x+(cid:174)δ)=‘safe’(1)Wenotethattheattackswedemonstratearenotnecessarilyop-timal,astheconstraintsaresatisfiedwithnonuniquesolutions.Theattacksareconductedinreal-time(i.e.,inmillisecondspertimestep),notaposteriori(i.e.,appliedretrospectivelytoalongerse-quenceofsensorreadingsaftertheattackerfullyreceivesthem).3.2.2AttackerKnowledge.UsingtheadversariallearningnotationintroducedinSection2,aconcealmentattackischaracterizedbytheknowledgeoftheattackeraboutthetrainingdatasetD,featuresetX,learningalgorithmf,andtrainedparametersw.IntheICSsetting,theattackercanbecharacterizeddifferentlyaccordingtohisknowledgeoftheattackedsystem.Inordertoexplainourattackermodel,wesplitthetuple(D,X,f,w)intotwo:theDatatuple(D,X)andDefensetuple(f,w).Weassumetheattackertobeunconstrainedorconstrainedw.r.t.theDatatuple,i.e.,thesensorreadingsXthatshecanobserveandmanipulateandthedataDthatsheeavesdrops.Moreover,weclassifyattackersaswhitebox,blackbox,w.r.t.theDefensetuple,i.e.,theknowledgeoflearningalgorithmf,andtrainedparametersw.Table1,providesanoverviewoftheattacker’sconstraintsconsideredinthiswork.ConstraintsoverDataTuple.AccordingtotheDatatuple(D,X),weclassifytheattackeras:•Unconstrained(D,X),inwhichtheattackercanmanipulateallnfeaturesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetobeatmostn.•FeaturesPartiallyConstrained(D,ˆX),weassumethattheattackerisconstrainedtoperturbasubsetofkoutofnvariablesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetonotexceeddistancek.•FeaturesFully-Constrained(D,ˆX),weassumethattheat-tackerisconstrainedtoobserveandperturbasubsetofkoutofnvariablesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetonotexceeddistancek.•DataConstrained(ˆD,X),weassumethattheattackerisconstrainedtoeavesdropalimitedquantityofprocessdatathatareusedfortrainingitsattacks.SelectionofConstrainedFeaturesThesubsetoffeaturesthatcanbemodifiedishighlyuse-casedependent(i.e.,whichlinkisattacked,whichdevicewascompromised).Todemonstratethegeneralityofourfindings,weexploredtwotypesofconstraints:abest-casescenarioandatopology-basedscenario.Forthebest-casescenario,weassumethattheselectionofthekoutofnmanipulatedfeaturescanbemadebytheattackertomaximizetheattackimpact.Thisarguablyrepresentsabest-casescenariofortheconstrainedattacker(i.e.,anattackerconstrainedtofeaturesthathappentoberelativelyidealfortheattacker).Forthesecondscenario,constraintsarederivedfromthenetworktopology.Weassumethattheattackercancompromiseasinglesubstation(orPLC)inthenetwork,andtheselectionofkoutofnfeaturesisbasedonwhichsensorsareinterconnectedtothecompromisedsubstation.KnowledgeofDefenseTuple.WeclassifytheattackeraccordingtotheirknowledgeoftheDefensetuple(f,w),as:•Whitebox(f,w),theattackerknowstheexactsystemmodelanditsvariables(suchasthecurrentlyestimatedsystemstate),andtheexactthresholdsoftheclassificationsystem.Thus,thewhiteboxattackerischaracterizedbytheknowl-edgeof(f,w).Withthatinformation,theattackercouldei-therrunabasicexhaustivesearch,basicoptimizationstrate-gies,ormoresophisticatedapproaches(especiallysolutionsthatusethegradientsignalfromtheattackedmodel).•Blackbox(AAˆf,Zˆw),theattackerisawareofthegeneralde-tectionscheme,butunawareofinternalvariables,architec-tureandexactthresholdsusedintheclassification.Wenotethatourblackboxattackerisdifferentfromtheonedefinedin[6],(ˆf,ˆw).Ourattackerdoesnotrequiretheknowledgeofforitsapproximationˆf.Inourcase,thenatureoftheenvironmentimposesthattheattackercannotquerythesystemeveninablackboxmannertogetfeedbackontheprovidedlabelsorconfidencescores(thisisdoneforexamplein[12,14,56,61]),asthiswouldmeanpotentiallyraisingthealarm.Thus,weconsiderthattheonlyassumptionoftheattackerconcerningfisthatDeepLearningtechniquesareusedfordetection.Giventhistaxonomy,theattackercanbeclassifiedforexample,asunconstrainedwhitebox.3.3ExampleConstraintScenariosWeargueourConstrainedandUnconstrainedattacksrepresentarealisticthreatmodelintheICSsettingandfitthetaxonomyofattacksintheAMLliterature.Inparticular,practicalICSaretypicallycomposedofmultiplestages,andeachstageiscontrolledbyadifferentPLC(i.e.,differentbrands/models).Moreover,theICScanbedeployedinaphysicallydistributedmanner.Forexample,=full,ACSAC2020,December7–11,2020,Austin,USAA.Erbaetal.Table1:Classificationofourattackermodelsbasedontrainingdataandfeatures,Datatuple(D,X)andalgorithmknowledgeandparameters.Access:(cid:32)=full,(cid:71)(cid:35)=partial.Forallwhiteboxattacks,theDefensetupleis(f,w),forallblackboxattacks,(AAˆf,Zˆw).Attacker’sXConstraintsDReadWriteUnconstrained§5.4(cid:32)(cid:32)(cid:32)XPartially§5.5(cid:32)(cid:32)(cid:71)(cid:35)XFully§5.5(cid:32)(cid:71)(cid:35)(cid:71)(cid:35)D§5.5(cid:71)(cid:35)(cid:32)(cid:32)theadversarialattackhastofindaperturbation(cid:174)δtominimizethereconstructionerrorbetweentheinput(cid:174)x+δandoutputˆ(cid:174)xoftheReconstruction-baseddetector.PleaserefertoSection4.1forfur-therdetailsonthetargetmodel.Inamathematicalnotation,itcanbewrittenasthefollowingconstrainedoptimizationprobleminEquation1:minimizeMSE=1nni=1(ˆxi−(xi+δi))2s.t.(cid:174)δ∈constraintspace(Section3.2.2)real-timeconstraintsimposedbyCPSy((cid:174)x+(cid:174)δ)=‘safe’(1)Wenotethattheattackswedemonstratearenotnecessarilyop-timal,astheconstraintsaresatisfiedwithnonuniquesolutions.Theattacksareconductedinreal-time(i.e.,inmillisecondspertimestep),notaposteriori(i.e.,appliedretrospectivelytoalongerse-quenceofsensorreadingsaftertheattackerfullyreceivesthem).3.2.2AttackerKnowledge.UsingtheadversariallearningnotationintroducedinSection2,aconcealmentattackischaracterizedbytheknowledgeoftheattackeraboutthetrainingdatasetD,featuresetX,learningalgorithmf,andtrainedparametersw.IntheICSsetting,theattackercanbecharacterizeddifferentlyaccordingtohisknowledgeoftheattackedsystem.Inordertoexplainourattackermodel,wesplitthetuple(D,X,f,w)intotwo:theDatatuple(D,X)andDefensetuple(f,w).Weassumetheattackertobeunconstrainedorconstrainedw.r.t.theDatatuple,i.e.,thesensorreadingsXthatshecanobserveandmanipulateandthedataDthatsheeavesdrops.Moreover,weclassifyattackersaswhitebox,blackbox,w.r.t.theDefensetuple,i.e.,theknowledgeoflearningalgorithmf,andtrainedparametersw.Table1,providesanoverviewoftheattacker’sconstraintsconsideredinthiswork.ConstraintsoverDataTuple.AccordingtotheDatatuple(D,X),weclassifytheattackeras:•Unconstrained(D,X),inwhichtheattackercanmanipulateallnfeaturesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetobeatmostn.•FeaturesPartiallyConstrained(D,ˆX),weassumethattheattackerisconstrainedtoperturbasubsetofkoutofnvariablesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetonotexceeddistancek.•FeaturesFully-Constrained(D,ˆX),weassumethattheat-tackerisconstrainedtoobserveandperturbasubsetofkoutofnvariablesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetonotexceeddistancek.•DataConstrained(ˆD,X),weassumethattheattackerisconstrainedtoeavesdropalimitedquantityofprocessdatathatareusedfortrainingitsattacks.SelectionofConstrainedFeaturesThesubsetoffeaturesthatcanbemodifiedishighlyuse-casedependent(i.e.,whichlinkisattacked,whichdevicewascompromised).Todemonstratethegeneralityofourfindings,weexploredtwotypesofconstraints:abest-casescenarioandatopology-basedscenario.Forthebest-casescenario,weassumethattheselectionofthekoutofnmanipulatedfeaturescanbemadebytheattackertomaximizetheattackimpact.Thisarguablyrepresentsabest-casescenariofortheconstrainedattacker(i.e.,anattackerconstrainedtofeaturesthathappentoberelativelyidealfortheattacker).Forthesecondscenario,constraintsarederivedfromthenetworktopology.Weassumethattheattackercancompromiseasinglesubstation(orPLC)inthenetwork,andtheselectionofkoutofnfeaturesisbasedonwhichsensorsareinterconnectedtothecompromisedsubstation.KnowledgeofDefenseTuple.WeclassifytheattackeraccordingtotheirknowledgeoftheDefensetuple(f,w),as:•Whitebox(f,w),theattackerknowstheexactsystemmodelanditsvariables(suchasthecurrentlyestimatedsystemstate),andtheexactthresholdsoftheclassificationsystem.Thus,thewhiteboxattackerischaracterizedbytheknowl-edgeof(f,w).Withthatinformation,theattackercouldei-therrunabasicexhaustivesearch,basicoptimizationstrate-gies,ormoresophisticatedapproaches(especiallysolutionsthatusethegradientsignalfromtheattackedmodel).•Blackbox(AAˆf,Zˆw),theattackerisawareofthegeneralde-tectionscheme,butunawareofinternalvariables,architec-tureandexactthresholdsusedintheclassification.Wenotethatourblackboxattackerisdifferentfromtheonedefinedin[6],(ˆf,ˆw).Ourattackerdoesnotrequiretheknowledgeofforitsapproximationˆf.Inourcase,thenatureoftheenvironmentimposesthattheattackercannotquerythesystemeveninablackboxmannertogetfeedbackontheprovidedlabelsorconfidencescores(thisisdoneforexamplein[12,14,56,61]),asthiswouldmeanpotentiallyraisingthealarm.Thus,weconsiderthattheonlyassumptionoftheattackerconcerningfisthatDeepLearningtechniquesareusedfordetection.Giventhistaxonomy,theattackercanbeclassifiedforexample,asunconstrainedwhitebox.3.3ExampleConstraintScenariosWeargueourConstrainedandUnconstrainedattacksrepresentarealisticthreatmodelintheICSsettingandfitthetaxonomyofattacksintheAMLliterature.Inparticular,practicalICSaretypicallycomposedofmultiplestages,andeachstageiscontrolledbyadifferentPLC(i.e.,differentbrands/models).Moreover,theICScanbedeployedinaphysicallydistributedmanner.Forexample,=partial.Forallwhiteboxattacks,theDefensetupleis(f,w),forallblackboxattacks,AAˆfZˆwAttacker’sXConstraintsDReadWriteUnconstrained§5.4(cid:32)(cid:32)(cid:32)XPartially§5.5(cid:32)(cid:32)(cid:71)(cid:35)XFully§5.5(cid:32)(cid:71)(cid:35)(cid:71)(cid:35)D§5.5(cid:71)(cid:35)(cid:32)(cid:32)theadversarialattackhastofindaperturbation(cid:174)δtominimizethereconstructionerrorbetweentheinput(cid:174)x+δandoutputˆ(cid:174)xoftheReconstruction-baseddetector.PleaserefertoSection4.1forfur-therdetailsonthetargetmodel.Inamathematicalnotation,itcanbewrittenasthefollowingconstrainedoptimizationprobleminEquation1:minimizeMSE=1nni=1(ˆxi−(xi+δi))2s.t.(cid:174)δ∈constraintspace(Section3.2.2)real-timeconstraintsimposedbyCPSy((cid:174)x+(cid:174)δ)=‘safe’(1)Wenotethattheattackswedemonstratearenotnecessarilyop-timal,astheconstraintsaresatisfiedwithnonuniquesolutions.Theattacksareconductedinreal-time(i.e.,inmillisecondspertimestep),notaposteriori(i.e.,appliedretrospectivelytoalongerse-quenceofsensorreadingsaftertheattackerfullyreceivesthem).3.2.2AttackerKnowledge.UsingtheadversariallearningnotationintroducedinSection2,aconcealmentattackischaracterizedbytheknowledgeoftheattackeraboutthetrainingdatasetD,featuresetX,learningalgorithmf,andtrainedparametersw.IntheICSsetting,theattackercanbecharacterizeddifferentlyaccordingtohisknowledgeoftheattackedsystem.Inordertoexplainourattackermodel,wesplitthetuple(D,X,f,w)intotwo:theDatatuple(D,X)andDefensetuple(f,w).Weassumetheattackertobeunconstrainedorconstrainedw.r.t.theDatatuple,i.e.,thesensorreadingsXthatshecanobserveandmanipulateandthedataDthatsheeavesdrops.Moreover,weclassifyattackersaswhitebox,blackbox,w.r.t.theDefensetuple,i.e.,theknowledgeoflearningalgorithmf,andtrainedparametersw.Table1,providesanoverviewoftheattacker’sconstraintsconsideredinthiswork.ConstraintsoverDataTuple.AccordingtotheDatatuple(D,X),weclassifytheattackeras:•Unconstrained(D,X),inwhichtheattackercanmanipulateallnfeaturesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetobeatmostn.•FeaturesPartiallyConstrained(D,ˆX),weassumethattheattackerisconstrainedtoperturbasubsetofkoutofnvariablesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetonotexceeddistancek.•FeaturesFully-Constrained(D,ˆX),weassumethattheat-tackerisconstrainedtoobserveandperturbasubsetofkoutofnvariablesin(cid:174)x,andherperturbationsarelimitedintermsofL0distancetonotexceeddistancek.•DataConstrained(ˆD,X),weassumethattheattackerisconstrainedtoeavesdropalimitedquantityofprocessdatathatareusedfortrainingitsattacks.SelectionofConstrainedFeaturesThesubsetoffeaturesthatcanbemodifiedishighlyuse-casedependent(i.e.,whichlinkisattacked,whichdevicewascompromised).Todemonstratethegeneralityofourfindings,weexploredtwotypesofconstraints:abest-casescenarioandatopology-basedscenario.Forthebest-casescenario,weassumethattheselectionofthekoutofnmanipulatedfeaturescanbemadebytheattackertomaximizetheattackimpact.Thisarguablyrepresentsabest-casescenariofortheconstrainedattacker(i.e.,anattackerconstrainedtofeaturesthathappentoberelativelyidealfortheattacker).Forthesecondscenario,constraintsarederivedfromthenetworktopology.Weassumethattheattackercancompromiseasinglesubstation(orPLC)inthenetwork,andtheselectionofkoutofnfeaturesisbasedonwhichsensorsareinterconnectedtothecompromisedsubstation.KnowledgeofDefenseTuple.WeclassifytheattackeraccordingtotheirknowledgeoftheDefensetuple(f,w),as:•Whitebox(f,w),theattackerknowstheexactsystemmodelanditsvariables(suchasthecurrentlyestimatedsystemstate),andtheexactthresholdsoftheclassificationsystem.Thus,thewhiteboxattackerischaracterizedbytheknowl-edgeof(f,w).Withthatinformation,theattackercouldei-therrunabasicexhaustivesearch,basicoptimizationstrate-gies,ormoresophisticatedapproaches(especiallysolutionsthatusethegradientsignalfromtheattackedmodel).•Blackbox(AAˆf,Zˆw),theattackerisawareofthegeneralde-tectionscheme,butunawareofinternalvariables,architec-tureandexactthresholdsusedintheclassification.Wenotethatourblackboxattackerisdifferentfromtheonedefinedin[6],(ˆf,ˆw).Ourattackerdoesnotrequiretheknowledgeofforitsapproximationˆf.Inourcase,thenatureoftheenvironmentimposesthattheattackercannotquerythesystemeveninablackboxmannertogetfeedbackontheprovidedlabelsorconfidencescores(thisisdoneforexamplein[12,14,56,61]),asthiswouldmeanpotentiallyraisingthealarm.Thus,weconsiderthattheonlyassumptionoftheattackerconcerningfisthatDeepLearningtechniquesareusedfordetection.Giventhistaxonomy,theattackercanbeclassifiedforexample,asunconstrainedwhitebox.3.3ExampleConstraintScenariosWeargueourConstrainedandUnconstrainedattacksrepresentarealisticthreatmodelintheICSsettingandfitthetaxonomyofattacksintheAMLliterature.Inparticular,practicalICSaretypicallycomposedofmultiplestages,andeachstageiscontrolledbyadifferentPLC(i.e.,differentbrands/models).Moreover,theICScanbedeployedinaphysicallydistributedmanner.Forexample,Constrained Concealment Attacks against Reconstruction-based detectors in ICS
ACSAC 2020, December 7–11, 2020, Austin, USA
whole plant (by compromising the network or central SCADA). We
capture those different capabilities of the attacker in the following
three scenarios:
• The Unconstrained Attacker can read and write any features
arbitrarily, e.g., by compromising the SCADA system, as
modeled by Mo et al. [39].
• The Partially Constrained attacker can read all traffic received
by the SCADA system (for example by a passive wiretap [55]
or by leveraging access control misconfigurations), but she is
only able to spoof sensor readings from a specific substation,
exploiting specific vulnerabilities of the substation or its
protocol to the SCADA (e.g., lack of authentication).
• The Fully Constrained attacker relates to a scenario where an
attacker compromised a specific substation, giving him read
and write access to features from this substation only [1, 17].
Similar assumptions were also considered in the BATADAL
dataset [53], where the attacker was assumed to perform con-
strained replay attacks to reduce the confidence of anomaly de-
tectors. In our contribution, we perform a concealment attack in a
systematic way to assess their impact over the anomaly detector.
A similar intuition is also used in the FAIL attacker model [49] for
AML. In particular, FAIL proposes to characterize attacker knowl-
edge over 4 dimension: Feature, Algorithm, Instance, Leverage.
While the first three have a counterpart in systematization by Big-
gio et al. [6], Leverage stands for the subset of features that the
adversary can modify (just like our constrained attacker). Thus,
our Unconstrained and Constrained attacks represent attacks with
full and limited Leverage.
3.4 Our Framework for Attack Computation
For both the white box and black box case, the attacker is assumed
to intercept and manipulate a Constrained or Unconstrained set of
sensor readings in real-time.
In the white box setting, we propose an iterative attack, able
to interactively query a classification oracle to determine which
features to manipulate, and the value to assign to those features. We
propose to compute the manipulations using an iterative algorithm.
This algorithm calculates solutions that are ‘safe’ from the detector
perspective. The algorithm is tunable, i.e., the attacker can act on
some algorithm parameters that impact over time the computation
and, consequently, the concealment efficacy. Again, this speeds up
computation but can impact the solution quality.
In the black box setting, we propose the use of a learning based
attack, specifically a Deep Neural Network that is capable of out-
putting concealed sensor readings, without the oracle’s feedback.
The attacker is adversarially training the neural network to learn
how the detector expects the ICS to behave. This trained neural
network then receives the traffic coming from the PLC. While the
attacker creates an anomaly over the physical process, the neural
network adjusts the anomalous data to resemble ‘safe’ data. This
manipulated version of sensor data is sent to the SCADA.
In order to avoid confusion, we point out explicitly that our iter-
ative attack can operate under the attacker white box assumption
as it requires to query an oracle of the anomaly detector, while the
learning based attack can operate under the black box assumption
as no query access is required to compute the adversarial sensor
readings. We compare iterative and learning based approaches with
replay attacks, as proposed in literature [39]. The attacker that
performs a replay attack can be categorized as black box.
4 DESIGN OF CONCEALMENT ATTACKS
We now present a detailed design for the three attacks that we
consider. We start with details on the (prior work) Reconstruction-
based attack detector, then introduce the replay attack (proposed
by [39]). We provide details on the iterative attack (white box knowl-
edge). We then conclude with the learning based approach (black
box knowledge), which leverages an online concealment method
without any prior knowledge about the physical process that gen-
erates the sensor readings and the detection scheme. Given these
premises, we note that, while adversarial examples found using the
iterative approach depend on the internal structure of the attacked
anomaly detector, examples crafted through the learning based
approach are independent from the addressed detection scheme
(see Section 5.6).
4.1 Background: Reconstruction-based Attack
Detector
In this work, we target anomaly detection systems proposed in
prior works [20, 32, 51], which share the same underlying idea,
reconstruction-based anomaly detection.
The anomaly detector consists of two parts, namely a Deep
Learning autoencoder model (with 𝑚 × 𝑛 features as input and 𝑛
output) trained over the normal operation sensor readings of an
ICS to optimize Mean Squared Error Loss, and a classifier function.
The idea is that the deep model has learned to reproduce the system
behavior under normal operating conditions with a low reconstruc-
tion error, so it reproduces a higher reconstruction error when
fed with anomalous sensor readings. The comparison between the
input and output of the deep model is used to decide if the system
is ‘safe’ or ‘under attack’. Reconstruction based classifiers represent
the state of the art for anomaly detection in ICS on a multi-fold
basis. First, they can overcome the problem of shortage of ‘under
attack’ samples that are hard to be gathered from the system with-
out damaging the plant. Second, they can capture interdependence
between sensor signals that helps localization of anomalies. Third,
they guarantee a low time of detection, which is a fundamental
property for ICS anomaly detectors.
As reference implementation, we use the general Autoencoder-
based anomaly detector framework proposed in [51] and available
as open source [46]. Moreover, we explore transferability of our
black box attack between different Deep Architectures (DA) in Sec-
tion 5. In particular we tested Long Short Term Memory (LSTM) [25]
as proposed in [20] and Convolutional Neural Networks (CNN) [33]
as proposed in [32]. The input to the DA is 𝑋 = [ (cid:174)𝑠𝑡−𝑚, . . . , (cid:174)𝑠𝑡],
representing 𝑚 + 1 time-steps of (cid:174)𝑠 = [𝑟1, 𝑟2, . . . , 𝑟𝑛], which is an
𝑛-dimensional vector of sensor readings. DA’s goal is to find 𝜙
parameters that minimize following the Mean-Squared Error opti-
mization problem:
E(𝑋 , (cid:174)𝑠𝑡)∼D(cid:2)∥𝐷𝐴(𝜙, 𝑋), (cid:174)𝑠𝑡 ∥2
2(cid:3)
min
𝜙
(2)
where DA outputs an n-dimensional vector (cid:174)𝑜 = [𝑣1, 𝑣2, ..., 𝑣𝑛],
and 𝑣𝑖 𝑠.𝑡 . 𝑖 ∈ {1, ..., 𝑛} represents the reconstructed value w.r.t. the
ACSAC 2020, December 7–11, 2020, Austin, USA
A. Erba et al.
input reading 𝑟𝑡