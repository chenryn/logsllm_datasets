position is out of the grid, it is referred to as an exit position.
The path computation ends with an exit position. The user
reports the answer associated with that exit position to ﬁn-
ish an authentication round. The answer is an integer from
[0, 3], and is randomly assigned to each exit position. Since
the same answer is assigned to multiple exit positions (i.e.
4 answers assigned to 18 exit positions), the adversary can-
not easily tell which the exact exit position is. For each exit
position, there are also many possible paths leading to it,
which further increases the difﬁculty for the adversary.
Since the default parameters are large (k = 30, n = 80),
brute force attack is infeasible for this scheme. The scheme
also follows Principle 3 to display all the candidate images
in each challenge so that the adversary cannot extract the
secret only by analyzing the challenges. However, each in-
dividual element is distinguishable in this scheme during
the decision process, as each element has different impact
on the transition of decision paths. One can use probabilis-
tic decision tree to recover the secret from the observations
of challenge-response pairs.
Each possible path leading to the observed response
forms a decision path in the probabilistic decision tree. The
probability of a decision path is decided by the movements
on this path. For example, a path X = ⟨DOWN, RIGHT,
RIGHT, DOWN⟩ means the ﬁrst and the fourth images be-
long to the secret set, while the second and third images do
not. The probability p(X) is p1·p0·p0·p1, where p1 = k/n
and p0 = 1 − p1.
Initially, we create a 1-element score
table. Given a response with the answer i, we enumerate
all consistent decision paths leading to this answer, and up-
date the score table according to the conditional probability
p(X|response = i).
For an 8 × 10 grid speciﬁed by the default parameters,
there are 43758 possible decision paths in total, with aver-
4.4 P5: Indistinguishable Correlation Principle
Principle 5: An LRPS system with secret leakage should
minimize the statistical difference in low-dimensional cor-
relations among each possible response.
This principle is complementary to Principle 4 to limit
the efﬁciency of counting-based statistical analysis. Al-
though counting-based statistical analysis is straightfor-
ward, it cannot be completely prevented without a secure
channel, as the user’s response is always statistically biased
towards his knowledge of the secret. In the extreme case,
the adversary is able to maintain a counting table to hold
every candidate for the root secret, and update the table ac-
cording to every available observation. Using these count-
ing tables, the statistical difference caused by the knowl-
edge of the secret is always identiﬁable even when the user
is asked to make intentional mistakes at a predeﬁned proba-
bility only known by the server. Its informal proof is given
in Appendix B. In this sense, the counting-based statistical
analysis is more powerful than brute force attack if sufﬁ-
cient resources are available to the adversary.
In reality, the resources available to the adversary are not
unbounded. The cost of maintaining t-element counting ta-
bles is O(nt), which increases exponentially with the num-
ber of elements t contained in a table entry, where n is the
number of total individual elements. If the adversary fails
to maintain a high-dimensional counting table, the correla-
tion information in these tables is safe from the adversary.
However, it is still possible for the adversary to exploit the
low-dimensional correlation to recover the secret. We use
SecHCI [20] as a counterexample to show how it works
while brute force and probabilistic decision tree are infeasi-
ble.
During registration of SecHCI, a user is assigned k icons
as his secret from a pool of n icons. In each authentication
round, the challenge is a window consisting of w icons. The
user is asked to count how many secret icons appearing in
the window. After getting the count value x, the user calcu-
lates r = ⌊(x mod 4)/2⌋. The ﬁnal response r is either 0
or 1. The challenge is designed so that each individual can-
didate has the same probability to appear in the window for
either response. Hence, it is impossible for the adversary
to extract useful information based on 1-element statistical
analysis.
Since the default parameters are large, k = 14, n = 140,
brute force attack is not applicable. Also because it is a
counting-based scheme, it is not subject to probabilistic de-
cision tree attack according to Principle 4. However, 2-
dimensional counting attack is still applicable. Compared to
decoy icons, there are 0.599 more pairs on average among
secret icons for response 0, and 0.599 less pairs on aver-
age among secret icons for response 1. So we can use two
2-element counting tables to recover its secret, one table
Figure 4. The average false positive rate de(cid:173)
creases for the high(cid:173)complexity CAS scheme.
age path length of 14.5539. For each candidate image, its
score is at a signiﬁcantly high level if it belongs to the secret
set after a sufﬁcient number of observations. Figure 4 shows
the false positive rate decreasing along with the increasing
number of observed authentication rounds. On average, it
is sufﬁcient to discover the exact secret within 640.8 rounds
(65 sessions), and discover 90% secret elements after 264.7
rounds (27 sessions). Although the required number of ses-
sion observations is larger, it is still possible for the adver-
sary to collect them using a key logger, and such security
strength is achieved only when the user is able to remember
30 independent secret images.
Probabilistic decision tree can also be applied to the low-
complexity CAS scheme [31], the CHC scheme [32], the
S3PAS scheme [35], and the PAS scheme [4]. All of them
are based on simple challenges with an enumerable candi-
date space for decision paths and the individual element has
different impact on the transition of decision paths.
From these counterexamples, we can see that it is nec-
essary to increase the number of candidate decision paths
if it is infeasible to make each individual element indis-
tinguishable in the probabilistic decision tree. The only
known designs that satisfy this indistinguishability require-
ment are the counting-based schemes [15, 20].
In those
schemes, there is no order or weight information associated
with each candidate element, which usually distinguishes
the elements in decision paths. The user is asked to count
their secret elements appearing in the challenge. The ﬁnal
response is based on the count value. For these schemes,
probabilistic decision tree attack does not apply, but they
may still subject to counting-based statistical analysis at-
tack.
05001000150020000.00.10.20.30.40.50.60.60.570.540.480.410.360.260.130.0510.00170False positive rateNumber of roundsAppendix C.
5 Usability Costs of Defense Principles
In this section, we provide a qualitative analysis for us-
ability costs of our defense principles. We show the relation
and tradeoff among the constraints imposed by our princi-
ples and the requirements on human capabilities. This sec-
tion aims to provide a high level understanding of the quan-
titative tradeoff analysis to be presented in the next section.
As deﬁned in Section 2, the common parameters of an
LRPS system is a tuple (D, k, n, d, w, s). All of the param-
eters except D (the expected authentication strength) are
affected by our principles. The principles related to brute
force attack mainly dictates the memory demand for the se-
cret, and the principles related to statistical attack mainly
increase the computation workload for each authentication
session. Their impacts are also interrelated.
Principles 1 and 2 require a large candidate set for the
root secret and the round secret. This implies that either k
increases or n increases. An increase in k requires the user
to memorize more elements as his secret. An increase in n
will not raise the memory demand, but will increase statis-
tical signiﬁcance of the secret in the whole candidate set,
which indirectly increases the computation workload as an-
alyzed later. Principle 2 also directly raises the computation
workload, as it indicates a challenge is not safe against brute
force attack if it can be solved by using a small number of
possible secret elements. In order to increase the candidate
space of the round secret, the round secret must be either
randomly selected from the root secret [20, 31, 32] or use
all elements in the root secret [15, 2]. The former choice
requires the user to recognize the current displayed secret
elements that change in every round; the latter requires the
user to recall a large number of secret elements that would
be difﬁcult when k is large. Finally, more elements appear-
ing in a challenge means more computation workload to ag-
gregate them into the correct response. This demands much
more effort compared to using a ﬁxed short round secret in
a traditional password system.
Principles 3, 4, and 5 have more impact on (d, w,
s). Principle 3 requires that the elements in the challenge
should be uniformly drawn from the candidate set. Due to
previous requirements of large secret space and our prefer-
ence of minimizing the memory demand for the secret, the
value of k is to be small and the value of n is to be large. The
consequence of this is that the average number of secret el-
ements displayed in a challenge window, w· k/n, cannot be
large enough if the window size w is not large. This restricts
the number of possible responses to a small value, which
raises the success rate d of guessing attack and increases
the round number required to achieve an expected authenti-
cation strength D. On the other hand, if the window size is
Figure 5. The pair(cid:173)based score distribution is
distorted for SecHCI. The ﬁrst 14 elements
are the secret icons, whose pair(cid:173)based scores
are distinguishable from the scores of other
icons.
for each response. We update the count value for each pair
displayed in each challenge and each response. The score
for each entry is calculated as the value difference between
these two tables. For each pair of candidate icons, the score
is at a signiﬁcantly high level if both of them belong to the
secret set after a sufﬁcient number of observations. Figure 5
shows the pair-based score distribution after 20000 authen-
tication rounds, from which the secret icons can be easily
distinguished. On average, it is sufﬁcient to recover the ex-
act secret with 14219.4 rounds (711 sessions), and recover
90% secret elements after 10799.8 rounds (540 sessions).
Since SecHCI follows most of our principles, these numbers
are much larger than the schemes we analyzed previously,
but it is still far less secure than it is claimed to be [20]. Its
security strength is achieved by imposing a high cognitive
workload where the user is asked to correctly examine 600
icons (30 icons per round × 20 rounds) one by one for each
authentication session.
The secret leakage on pair-based statistics for SecHCI
can be ﬁxed by changing its response function from r = ⌊(x
mod 4)/2⌋ to r = x mod 2, where x is the number
of secret icons in the challenge window, but this ﬁx will
make SecHCI subjects to algebraic attack based on Gaus-
sian elimination [20]. This is also the original motivation
of the scheme to use its current function. To further defend
against this algebraic attack, a user has to produce incor-
rect answers with a ﬁxed error probability to create noises
as suggested in [15]. This certainly further decreases the
scheme’s usability. Another design limitation on counting-
based scheme is that the response function cannot be in the
form of r = x mod q, where q is an integer larger than
2. The detailed explanation for this limitation is given in
large, the LRPS system is limited only for large screen de-
vices and it also increases the difﬁculty for the user to exam-
ine the elements in the challenge window. Regardless of the
window size, this principle imposes increased computation
workload and the error rate for the user. Principles 4 and 5
further rule out most schemes based on simple challenges.
Principle 4 states if a leakage-resistant challenge design is
not complex enough to aggregate a large number of secret
elements into a response, it leads to a counting problem.
Principle 5 further states that only 0 and 1 can be safely
used as the response for a counting problem if the modular
operation is the only operation used to generate the ﬁnal re-
sponse. Hence, the three possible choices for a challenge
are: 1) a complex challenge using many secret elements -
the round number will be small but the challenge will be
very difﬁcult for the user to respond (the average length s of
decision paths signiﬁcantly increased); 2) a counting-based
challenge using the modular operation - the round number
will be large and the challenge will be relatively easier to re-
spond; and 3) a counting-based challenge using a specially
designed response function that has a large number of pos-
sible responses and satisﬁes the correlation indistinguisha-
bility condition; however, it will be a challenge to design
such a function with acceptable usability. All of the three
choices impose a considerable burden on the user.
6 Quantitative Tradeoff Analysis
In this section, we establish a quantitative analysis
framework for evaluating the usability cost of typical exist-
ing LRPS systems. This framework decomposes the pro-
cess of human-computer authentication into atomic cog-
nitive operations in psychology. There are four types of
atomic cognitive operations commonly used: single/parallel
recognition, free/cued recall, single-target/multi-target vi-
sual search and simple cognitive arithmetic. Their deﬁni-
tions and performance models are given in Appendix A,
which characterizes the relation between experiment pa-
rameters and reaction time of an average human. These per-
formance models are used to evaluate the cognitive work-
load for typical existing LRPS systems. The result in this
section provides quantitative assessment of the tradeoff be-
tween security and usability of LRPS systems. According
to conventions in psychology literature, we will refer user
as subject in this section.
6.1 Quantitative Analysis Framework
There are two components in our quantitative analysis
framework, Cognitive Workload (C) and Memory Demand
(M). Cognitive workload is measured by the total reaction
time required by the involved cognitive operations. Long
reaction time for each authentication round implies that
it is difﬁcult for the subject to answer each challenge and
the overall error rate is also high. Long reaction time for
each authentication session implies that the overall cogni-
tive workload is high and the involvement of attention and
patience is also high. Memory demand is measured by the
number of elements that must be memorized by the subject,
which is the prerequisite of any password system. Since this
prerequisite process is independent from the authentication
process, we consider it as a separate component. Since the
precise relation between overall error rate and total reaction
time is difﬁcult to measure in controlled psychology experi-
ments, our framework provides lower bound estimation for
the usability of a human-computer authentication system.
The detailed calculation for both components is described
as follows.
For cognitive workload, the cost for each authentication
round is the sum of average reaction time for all involved
atomic cognitive operations. This cost represents the aver-
age thinking time of a subject required to answer a chal-
lenge. A typical authentication round consists of at least
a memory retrieval operation and a simple arithmetic oper-
ation. For the graphic-based scheme, visual search is also
common. According to the working memory capability the-
ory [25, 9, 30, 29], the average reaction time is not short-
ened by repetitive rehearsal, when the subject has to main-
tain more than 4(±1) items in his working memory. The
rehearsal only improves the accuracy, which represents an
inherent limitation of human capabilities. This limitation is
also applied to other non-memory operations such as visual
search when the item positions are shufﬂed in each chal-
lenge [33]. Overall, the cognitive workload of an authenti-
cation session is calculated as the product of the cognitive
workload of an authentication round and the round number
when the number of the secret items is larger than 5. For the
schemes [4, 32] with no more than 5 secret items, we only
count once for their memory retrieval operations, assuming
that the secret will not be ﬂushed out due to the limitation
of working memory capacity.
Besides the reaction time, other usability measurements
for cognitive workload (such as user frustration level, con-
centration load, and motivational effort) are usually col-
lected from standardized testing questionnaires. However,
these measurements are susceptible to many implementa-
tion and environmental factors, such as screen size, graphic
or text-based interface design, and the education back-
ground of subjects. In contrast, the inﬂuence of those un-
stable factors has been minimized in more than a century’s
development of experimental psychology. So the advantage
of using performance models of atomic cognitive operations
is that they are implementation-independent. This property
is necessary for a fair comparison between different LRPS
designs. Consequently, our estimation of cognitive work-
load is very consistent with the time costs reported in the
k
15
16
60
30
14
5
4