e
l
n
r
e
t
t
a
P
30000
25000
20000
15000
10000
5000
0
0
20
ClamAV
Snort
40
60
Number of patterns
80
100
120
140 >160
Fig. 4. ClamAV pattern length distribution
the array represents whether a match was found in the corresponding position.
We have chosen the bit array structure, since it is a compact representation of
the results, even in the worst case scenario where a match is found at every
position.
3.3 Optimized Memory Management
The two major tasks of DFA matching, is determining the address of the next
state in the state table, and fetching the next state from the device memory.
These memory transfers can take up to several hundreds of nanoseconds, de-
pending on the traﬃc conditions and congestion.
Our approach for hiding memory latencies is to run many threads in paral-
lel. Multiple threads can improve the utilization of the memory subsystem, by
overlapping data transfer with computation. To obtain the highest level of per-
formance, we tested GrAVity to determine how the computational throughput
is aﬀected by the number of threads. As discussed in Section 4.2 the memory
subsystem is best utilized when there is a large number of threads, running in
parallel.
Moreover, we have investigated storing the DFA state table both in the global
memory space, as well as in the texture memory space of the graphics card. The
texture memory can be accessed in a random fashion for reading, in contrast
to global memory, where the access patterns must be coalesced. This feature
can be very useful for algorithms like DFA matching, which exhibit irregular
access patterns across large data sets. Furthermore, texture fetches are cached,
increasing the performance when read operations preserve locality. As we will
see in Section 4.2, the usage of texture memory can boost the computational
throughput up to a factor of two.
3.4 Other Optimizations
In addition to optimizing the memory usage, we considered two other optimiza-
tions: the use of page-locked (or pinned) memory, and reducing the number of
transactions between the host and the GPU device.
The page-locked memory oﬀers better performance, as it does not get swapped
(i.e. non-pageable memory). Furthermore, it can be accessed directly by the GPU
88
G. Vasiliadis and S. Ioannidis
through Direct Memory Access (DMA). Hence, the usage of page-locked memory
improves the overall performance, by reducing the data transferring costs to and
from the GPU. The contents of the ﬁles are read into a buﬀer allocated from
page-locked memory, through the CUDA driver. The DMA then, transfers the
buﬀer from the physical memory of the host, to the texture memory of the GPU.
To further improve performance, we use a large buﬀer to store the contents of
many ﬁles, that is transferred to the GPU in a single transaction. The motivation
behind this feature, is that the matching results will be the same, whether we
scan each ﬁle individually or scanning several ﬁles back-to-back, all at once. This
results in a reduction of I/O transactions over the PCI Express bus.
4 Performance Evaluation
In this section, we evaluate our prototype implementation. First, we give a short
description of our experimental setup. We then present an overall performance
comparison of GrAVity and ClamAV, as well as detailed measurements to show
how it scales with the preﬁx length and the number of threads that are executing
on the GPU.
4.1 Experimental Environment
For our experiment testbed, we used the NVIDIA GeForce GTX295 graphics
card. The card consists of two PCBs (Printed Circuit Board), each of which is
equipped with 240 cores, organized in 30 multiprocessors, and 896MB of GDDR3
memory. Our base system is equipped with two Intel(R) Xeon(R) E5520 Quad-
core CPUs at 2.27GHz with 8192KB of L2-cache, and a total of 12GB of memory.
The GPU is interconnected using a PCIe 2.0 x16 bus.
We use the latest signatures set of ClamAV (main v.52, released on February
2010). The set consists of 60 thousand string and regular expression signatures.
As input data stream, we used the ﬁles under /usr/bin/ in a typical Linux
installation. The directory contains 1516 binary ﬁles, totalling about 132MB
of data. The ﬁles do not contain any virus, however they exercise most code
branches of GrAVity.
In all experiments we conducted, we disregarded the time spent in the initial-
ization phase for both ClamAV and GrAVity. The initialization phase includes
the loading of the patterns and the building of the internal data structures, so
there is no actual need to include this time in our graphs.
4.2 Microbenchmarks
Figure 5 shows the matching throughput for varying signature preﬁx lengths.
We explore the performance that diﬀerent types of memory can provide, by
using global device and texture memory respectively to store the DFA state
table. The horizontal axis shows the signature preﬁx length. We also repeated
the experiment using diﬀerent number of threads. As the number of threads
GrAVity: A Massively Parallel Antivirus Engine
89
)
c
e
s
/
s
t
i
B
G
(
t
u
p
h
g
u
o
r
h
T
50
40
30
20
10
0
2
3
4
5
6
9 10 11 12 13 14
8
7
Prefix length
)
c
e
s
/
s
t
i
B
G
(
t
u
p
h
g
u
o
r
h
T
50
40
30
20
10
0
2
3
4
5
6
9 10 11 12 13 14
8
7
Prefix length
Threads=256K
Threads=512K
Threads=1024K
Threads=2048K
Threads=4096K
Threads=8192K
Threads=256K
Threads=512K
Threads=1024K
Threads=2048K
Threads=4096K
Threads=8192K
(a)
(b)
Fig. 5. Sustained throughput for varying signature preﬁx. Higher number of threads
achieve higher performance as memory latencies are hidden. We demonstrate the eﬀect
of diﬀerent GPU memory types on performance. (a) uses global device memory to store
the DFA state table, where (b) uses texture memory.
’
)
s
0
0
0
1
(
s
e
a
t
t
s
f
o
r
e
b
m
u
N
400
300
200
100
2
3
4
5
6
9 10 11 12 13 14
8
7
Prefix length
Fig. 6. Memory requirements for the storage of the DFA as a function of the signature
preﬁx length
increases, the throughput sustained by the GPU also increases. When using eight
millions threads, which is the maximum acceptable number of threads for our
application, the computational throughput raises to a maximum of 40 Gbits/s.
Comparing the two types of memory available in the graphics card, we observe
that the texture memory signiﬁcantly improves the overall performance by a
factor of two. The irregularity of memory accesses that DFA matching exhibits,
can be partially hidden when using texture memory. Texture memory provides
a random access model for fetching data, in contrast with global memory where
access patterns have to be coalesced. Moreover, texture fetches are cached, which
oﬀers an additional beneﬁt.
The total memory requirements for storing the DFA, independently of the
memory type, is shown in Figure 6. We observe that the total number of states
of the DFA machine is growing linearly to the length of the preﬁx. Using a value
of 14 as a preﬁx length, results in a DFA machine that holds about 400 thousands
states. In our DFA implementation this is approximately 400MB of memory —
each state requires 1KB of memory.
90
G. Vasiliadis and S. Ioannidis
4.3 Application Performance
In this section, we evaluate the overall performance of GrAVity. Each experiment
was repeated a number of times, to ensure that all ﬁles were cached by the
operating system. Thus, no ﬁle data blocks were read from disk during our
experiments. We have veriﬁed the absence of I/O latencies using the iostat(1)
tool.
Throughput. In this experiment we evaluate the performance of GrAVity com-
pared to vanilla ClamAV. Figure 7 shows the throughput achieved for diﬀerent
preﬁx lengths. The overall throughput increases rapidly, raising at a maximum
of 20 Gbits/s. A plateau is reached for a preﬁx length of around 10.
As the preﬁx length increases, the number of potential matches decreases, as
shown in Figure 9. This results to lower CPU post-processing, hence the overall
application throughput increases. In the next section, we investigate in more
detail the breakdown of the execution time.
20
5
1
0.2
)
c
e
s
/
s
t
i
B
G
(
t
u
p
h
g
u
o
r
h
T
2
3
4
5
6
GrAVity
ClamAV (1x core)
ClamAV (8x cores)
9 10 11 12 13 14
8
7
Prefix length
Fig. 7. Performance of GrAVity and ClamAV. We also include the performance num-
ber for ClamAV running on 8 cores. The CPU-only performance is still an order of
magnitude less that the GPU-assisted. The numbers demonstrate that additional CPU
cores oﬀer less beneﬁt than that of utilizing the GPU.
Execution Time Breakdown. We measure the execution time for data trans-
fers, result transfers, CPU and GPU execution. We accomplish this by adding
performance counters before each task.
As expected, Figure 8 shows that for small preﬁx sizes most of the time is
dominated by the cost of the CPU, verifying the possible matches reported back
by the GPU. For example, for a preﬁx length equal to 2, approximately 95% of
the total execution time is spent on the CPU to validate the potential matches.
For a preﬁx length equal to 14, the corresponding CPU time results in just 20%
of the total execution time, and in actual time signiﬁes a reduction of three
orders of magnitude, while the GPU consumes 54% of the total execution. As
the preﬁx length increases, this overhead decreases and the GPU execution time
becomes the dominant factor. For veriﬁcation, in Figure 9 we plot the number
of potential matches reported in accordance with the signature preﬁx length.
GrAVity: A Massively Parallel Antivirus Engine
91
2000
1500
1000
500
0
2
3
4
5
Transfer Results
Transfer Data
GPU Search
CPU Post−process
9 10 11 12 13 14
8
7
6
Prefix length
)