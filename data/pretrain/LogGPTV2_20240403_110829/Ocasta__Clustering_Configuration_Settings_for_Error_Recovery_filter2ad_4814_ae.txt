### Application and Time Measurement

The time taken for the participant to select the screenshot was recorded. After the participant selected the screenshot, we noted whether they chose the 21-second sliding time window, a clustering threshold of 2, and the DFS (Depth-First Search) strategy. The time measurements for some manual fixes represent a lower bound, while the time measurements for Ocasta usage are precise. We selected errors that were relatively simple, making it easier to explain them to users who might be unfamiliar with the applications. Simple errors also make manual fixing easier, thus making it more challenging for Ocasta to have a significant advantage over manual searching.

### Related Work

#### Inferring Configuration Settings
Few previous studies have automatically inferred relationships among configuration settings. Zheng et al. [15] deduced dependencies by experimentally testing the impact of changing settings. In contrast, Ocastaâ€™s clustering algorithm avoids experimental overhead by using observed application accesses. Glean [5] infers relationships by analyzing the hierarchical structure of settings, whereas Ocasta does not require such a structure.

#### Diagnosing Configuration Errors
Among the work focused on diagnosing configuration errors, Ocasta is most closely related to Strider [4] and PeerPressure [3]. Both systems use a genebank of common configurations and statistical methods to identify potential errors. They assume homogeneity across machines and have privacy implications, as users must share their configurations. Ocasta, however, only requires locally collected information from the machine with the error, avoiding these drawbacks.

ConfAid [6] uses taint analysis to identify the setting causing an error, ranking settings that affect the path to the error as more likely to be the key. Another approach, Failure-Context-Sensitive analysis [16], extracts mappings between settings and affected source code lines. ConfDiagnoser [17] combines static analysis and execution profiling to rank settings that cause deviations from correct executions. These approaches require source code, while Ocasta treats applications as black boxes, requiring only access to the key-value store.

### User Study

We asked participants to select the correct screenshot and recorded the time taken. We also inquired about the number of screenshots they examined and rated the difficulty of finding the correct one. After resetting the system to its misconfigured state, participants were asked to fix the error manually, with full control of the computer and internet access. The test was cut off at 5 minutes. We recorded whether the participant fixed the error and the time taken. Finally, we asked if they had experienced the error before and the steps they took to fix or try to fix it.

Figure 4 compares the average time users took to create the witness and select the screenshot versus the average time for manual repair. Ocasta significantly reduces user effort. Only in case 16 did most participants fix the error manually, lowering the average time. On a difficulty scale of 1 to 5, participants rated the creation of the trial as 1 (74% of the time), 2 (21% of the time), and 3 (5% of the time). For selecting the correct screenshot, the ratings were 1 (80% of the time), 2 (11% of the time), 3 (8% of the time), and 4 (1% of the time).

### Sources of Bias

Our user study has several sources of bias. First, participant selection was not random, consisting of colleagues and acquaintances. Second, the study was single-blind, with the administrator knowing the correct answer. To minimize this, we minimized interaction and used written materials. Third, participants were cut off at 5 minutes for manual fixes, while no cutoff was used for Ocasta.

### Conclusion

Ocasta, a system for handling multi-configuration setting errors, identifies clusters of related settings using statistical clustering. Evaluated over several months on Windows and Linux, Ocasta accurately identified about 88.6% of clusters. It successfully fixed all 16 real-world configuration errors, with 5 requiring changes to multiple settings.

### Acknowledgments

We thank Ding Yuan for his suggestions on our user study and Tim Trant for setting up our trace collection infrastructure. We also thank the anonymous reviewers for their comments. This research was partially supported by an ORF-RE grant from the Ontario Ministry of Research and Innovation and an NSERC Discovery Grant.

### References

[References listed here as per the original text]

---

This optimized version aims to improve clarity, coherence, and professionalism, ensuring the content is well-structured and easy to follow.