application. The time taken for the participant
to select
the screenshot was also recorded. After the participant
selected the screenshot, we recorded whether they selected
21-second sliding time window, clustering threshold of 2, and DFS search
strategy
487
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:26:43 UTC from IEEE Xplore.  Restrictions apply. 
or selecting the screenshot. Thus, the time measurements
for some of the manual ﬁxes represent a lower-bound while
the time measurements for Ocasta usage are precise. Finally,
we selected errors that tended to be simple. This made it
easier to explain the errors to users who might be unfamiliar
with the applications. In addition, simple errors make manual
ﬁxing easier and thus make it more difﬁcult for Ocasta to
have a signiﬁcant advantage over manually searching for the
ﬁx.
VII. RELATED WORK
Inferring related conﬁguration settings: Few previous
studies automatically infer relations among conﬁguration
settings. Zheng et al. [15] deduce dependency among con-
ﬁguration settings by experimentally testing the impact of
changing conﬁguration settings. Ocasta’s clustering algo-
rithm avoids the overhead of experimental
tests by us-
ing observed application accesses to conﬁguration settings.
Glean [5] infers relations among conﬁguration settings by
analyzing hierarchical structure of conﬁguration settings,
while Ocasta’s clustering algorithm does not require the
existence of hierarchial structure for conﬁguration settings.
Diagnosing conﬁguration errors: Of the work that
focuses on diagnosing conﬁguration errors, Ocasta is most
closely related to Strider [4] and PeerPressure [3]. Both
PeerPressure and Strider use a genebank of common con-
ﬁgurations and apply statistical methods to determine where
the error might
lie. These systems assume homogeneity
across machines and also have privacy implications as users
must share their conﬁgurations with the genebank. Ocasta
only requires information collected locally from the machine
with the error and thus does not have the drawbacks of a
genebank.
ConfAid [6] takes a “white-box” approach by using taint-
analysis to try to identify the conﬁguration setting that
causes a conﬁguration error. ConfAid ranks conﬁguration
settings that affect the path taken to reach the conﬁguration
error as more likely to be conﬁguration keys that can ﬁx
the error. Another “white-box” approach, Failure-Context-
Sensitive analysis [16] extracts the mapping between conﬁg-
uration settings and the source code lines that can be affected
by these conﬁguration settings, from the source code of an
application. These mappings can be used to identify the
conﬁguration setting that causes conﬁguration errors, when
the source code lines of the errors are available, for example
from an application’s error message. More recent work,
ConfDiagnoser, combines static analysis of an application’s
source code and execution proﬁling to rank conﬁguration
settings that causes executions to deviate from pre-generated
correct executions [17]. Because these approaches are white-
box, they require application source code. In contrast, Ocasta
treats applications as black-boxes and only requires access
to the application’s key-value store.
Figure 4: Comparison of time required to ﬁx the error with
Ocasta versus manual ﬁxing from our user study.
the right one. We also asked the participant how many of
the screenshots they actually examined and to qualitatively
rate how difﬁcult it was to ﬁnd the screenshot.
We then reset the system back to its misconﬁgured state
and asked the participant to try to ﬁx the error manually.
The participant was given full control of the computer and
was allowed to use Internet to search for possible solutions
to the conﬁguration error. To keep the test short, we cut
the participants off at 5 minutes. We recorded whether the
participant was able to ﬁx the error manually or not and the
time it took for them to ﬁx the error. For each error, the
participant was ﬁnally asked whether they had experienced
the particular error themselves before and the steps they took
to ﬁx or try to ﬁx the error.
Figure 4 shows a comparison between the average time
users took to both create the witness and select the screen-
shot and the average time taken to manually repair each
conﬁguration error. If we use the time spent as an indicator
of the amount of user effort, we can see that Ocasta saves
users a signiﬁcant amount of effort to repair conﬁguration
errors. Only in case 16 were the majority of participants able
to ﬁx the conﬁguration error manually and this signiﬁcantly
lowered the average time for the a manual ﬁx. Qualitatively
on a difﬁculty scale of 1 to 5, with 1 being the easiest, across
the 4 errors, the participants rated the creation of the trial as
1 74% of the time, 2 21% of the time and and 3 5% of the
time. For selecting the correct screenshot, participants rated
the difﬁculty as 1 80% of the time, 2 11% of the time, 3
8% of the time and 4 1% of the time.
Our user study has several sources of bias. First, selection
of participants was not completely random, but consisted
of colleagues and acquaintances of the authors. Second,
the administration of the study was single blind and the
person administrating the test knew the correct answer. To
minimize this effect, we tried to minimize interaction with
the participant and communicated using written materials
as much as possible. Third, the participants were cut off
at 5 minutes when they tried to ﬁx the error manually,
while no cut off was used for generating the Ocasta trial
488
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:26:43 UTC from IEEE Xplore.  Restrictions apply. 
All above work focuses on identifying a single conﬁgura-
tion setting that causes conﬁguration errors. With the cluster-
ing provided by Ocasta, their techniques can be leveraged
to diagnose conﬁguration errors caused by more than one
conﬁguration settings.
Chronus [2] maintains a history of entire system states and
focuses on using binary search to ﬁnd the optimal recovery
point in an application’s history. Chronus logs at the disk
block layer and as a result, many of the historical states it
generates can corrupt ﬁle systems and thus cannot be used
for recovery.
Fixing conﬁguration errors: Kardo [18] and Auto-
bash [19] are both systems that take a human-generated
solution for a conﬁguration error, perform analysis on the
solution to ﬁnd the minimum set of actions that make up
the conﬁguration ﬁx and generalize it so it can be applied
to a wider set of machines. Ocasta does not require human-
generate solutions.
Detecting
errors:
conﬁguration
Like Ocasta,
CODE [14] analyzes the accesses patterns that applications
make to the Windows registry. CODE uses a rule learning
algorithm to identify normal key access patterns of an
application and ﬂags anomalous access patterns as possible
conﬁguration errors. CODE detects conﬁguration errors,
but unlike Ocasta, it does not ﬁx the errors, nor does it
try to identify relationships between keys other than the
access patterns. Conferr is a tool for quantifying system
manageability and resilience to conﬁguration errors [20],
[21]. It uses simulated human models to try to generate
realistic conﬁguration errors. Both CODE and Conferr can
be viewed as complementary to Ocasta.
Time travel and roll back: The concept of time travel
and roll back has been used for debugging and system
recovery from intrusions. Time-travel virtual machines [22]
enables deterministic replay of whole machines to simplify
OS debugging. Taser [23] and Retro [24] use system-level
tracking and perform selective recovery after an intrusion.
Rx [25] uses repeated roll backs to ﬁnd an execution
where bugs do not occur, but does not try to ﬁnd the root
cause or attempt to permanently ﬁx the bug. Like Ocasta,
these systems use roll back recovery but focus on ﬁxing
other types of faults while Ocasta focuses speciﬁcally on
conﬁguration errors.
Hierarchical clustering: Many previous studies have
used hierarchical clustering for software clustering [26],
[27], [28], including program comprehension, reverse en-
gineering, and software reengineering, cluster different lev-
els of abstractions of software artifacts, such as variables,
functions, and source ﬁles. Prior work has also used hier-
archical agglomerative clustering to improve the efﬁciency
of ﬁnding software failures during software testing [29] or
categorizing software failures [30]. They cluster proﬁles of
an application’s executions.
Ocasta uses the maximum linkage criterion, which as been
489
found by other prior work [31], [32] to provide better per-
formance than other linkage criterion. Ocasta augments the
hierarchical agglomerative algorithm to be able to partition
clusters using an adjustable clustering threshold, which is
more ﬂexible and intuitive for our purposes of clustering
conﬁguration settings.
VIII. CONCLUSION
We describe the design and implementation of Ocasta, a
system that enables conﬁguration recovery systems to handle
multi-conﬁguration setting errors by identifying clusters of
related conﬁguration settings using statistical clustering. We
have evaluated Ocasta over several months on both Windows
and Linux machines and ﬁnd that Ocasta’s clustering accu-
rately identiﬁes about 88.6% of clusters on average. Our
evaluation of Ocasta in ﬁxing conﬁguration errors shows
that Ocasta successfully ﬁxed all 16 real world conﬁguration
errors used in our evaluation, 5 of which require changing
more than one conﬁguration setting together to ﬁx, by utiliz-
ing the identiﬁed clusters of related conﬁguration settings,
ACKNOWLEDGMENTS
We thank Ding Yuan for his invaluable suggestions on
our user study and Tim Trant for helping us setting up our
trace collection infrastructure. We also thank the anonymous
reviewers for their helpful comments. This research was
partially supported by an ORF-RE grant from the Ontario
Ministry of Research and Innovation and by an NSERC
Discovery Grant.
REFERENCES
[1] A. Ganapathi, Y.-M. Wang, N. Lao, and J.-R. Wen, “Why pcs
are fragile and what we can do about it: a study of windows
registry problems,” in Dependable Systems and Networks,
2004 International Conference on, 2004, pp. 561–566.
[2] A. Whitaker, R. S. Cox, and S. D. Gribble, “Conﬁguration
debugging as search: Finding the needle in the haystack.”
in Proceedings of the 6th Symposium on Operating Systems
Design and Implementation, Dec. 2004, pp. 77–90.
[3] H. J. Wang, J. C. Platt, Y. Chen, R. Zhang, and Y.-M. Wang,
“Automatic misconﬁguration troubleshooting with PeerPres-
sure,” in Proceedings of the 6th Symposium on Operating
Systems Design and Implementation, Dec. 2004, pp. 245–258.
[4] Y.-M. Wang, C. Verbowski, J. Dunagan, Y. Chen, H. J.
Wang, C. Yuan, and Z. Zhang, “Strider: A black-box, state-
based approach to change and conﬁguration management and
support,” in Proceedings of the 17th Large Installation System
Administrator Conference, Jun. 2003, pp. 159–172.
[5] E. Kycyman and Y.-M. Wang, “Discovering correctness con-
straints for self-management of system conﬁguration,” in Pro-
ceedings of the 1st International Conference on Autonomic
Computing, May 2004, pp. 28–35.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:26:43 UTC from IEEE Xplore.  Restrictions apply. 
[6] M. Attariyan and J. Flinn, “Automating conﬁguration trou-
bleshooting with dynamic information ﬂow analysis,” in Pro-
ceedings of the 9th Symposium on Operating Systems Design
and Implementation, Oct. 2010, pp. 1–11.
[19] Y.-Y. Su, M. Attariyan, and J. Flinn, “Autobash: Improving
conﬁguration management with operating system causality
analysis,” in Proceedings of the 21st ACM Symposium on
Operating Systems Principles, Oct. 2007, pp. 237–250.
[7] Z. Yin, X. Ma, J. Zheng, Y. Zhou, L. N. Bairavasundaram,
and S. Pasupathy, “An empirical study on conﬁguration errors
in commercial and open source systems,” in Proceedings of
the 23rd ACM Symposium on Operating Systems Principles,
Oct. 2011, pp. 159–172.
[8] P.-N. Tan, M. Steinbach, V. Kumar, and editors, Introduction
to Data Mining. Pearson Addison Wesley, 2005.
[20] L. Keller, P. Upadhyaya, and G. Candea, “Conferr: A tool for
assessing resilience to human conﬁguration errors,” in Pro-
ceedings of the 2008 International Conference on Dependable
Systems and Networks, Jun. 2008, pp. 157–166.
[21] G. Candea, “Toward quantifying system manageability,” in
Proceedings of the 4th Workshop on Hot Topics in Systems
Dependability, Dec. 2008.
[9] L. Ingram, I. Popov, S. Setty, and M. Walﬁsh, “Repair from
a chair: Computer repair as an untrusted cloud service,” in
Proceedings of the 13th Workshop on Hot Topics in Operating
Systems, May 2011.
[22] S. T. King, G. W. Dunlap, and P. M. Chen, “Debugging
operating systems with time-traveling virtual machines,” in
Proceedings of the 2005 Annual Usenix Technical Confer-
ence, Apr. 2005, pp. 1–15.
[10] S. Lu, S. Park, C. Hu, X. Ma, W. Jiang, Z. Li, R. A.
Popa, and Y. Zhou, “Muvi: automatically inferring multi-
variable access correlations and detecting related semantic
and concurrency bugs,” in Proceedings of twenty-ﬁrst ACM
SIGOPS symposium on Operating systems principles,
ser.
SOSP ’07. New York, NY, USA: ACM, 2007, pp.
103–116.
[Online]. Available: http://doi.acm.org/10.1145/
1294261.1294272
[11] “Redis,” http://redis.io/, 2012.
[12] G. Hunt and D. Brubacher, “Detours: Binary interception of
Win32 functions,” in Proceedings of the 3rd Usenix Windows
NT Symposium, Jul. 1999.
[13] M. J. L. de Hoon, S. Imoto, J. Nolan, and S. Miyano, “Open
source clustering software,” Bioinformatics, vol. 20 (9), pp.
1453–1454, 2004.
[14] D. Yuan, Y. Xie, R. Panigrahy, J. Yang, C. Verbowski, and
A. Kumar, “Context-based online conﬁguration-error detec-
tion,” in Proceedings of the 2011 Annual Usenix Technical
Conference, Jun. 2011, pp. 36–41.
[15] W. Zheng, R. Bianchini, and T. D. Nguyen, “Automatic
conﬁguration of internet services,” in Proceedings of the 2Nd
ACM SIGOPS/EuroSys European Conference on Computer
Systems 2007, ser. EuroSys ’07. New York, NY, USA:
ACM, 2007, pp. 219–229.
[Online]. Available: http:
//doi.acm.org/10.1145/1272996.1273020
[16] A. Rabkin and R. Katz, “Precomputing possible conﬁguration
error diagnoses,” in Automated Software Engineering (ASE),
2011 26th IEEE/ACM International Conference on, 2011, pp.
193–202.
[17] S. Zhang and M. D. Ernst, “Automated diagnosis of
software conﬁguration errors,” in Proceedings of the 2013
International Conference on Software Engineering, ser. ICSE
’13.
Piscataway, NJ, USA: IEEE Press, 2013, pp. 312–
321. [Online]. Available: http://dl.acm.org/citation.cfm?id=
2486788.2486830
[18] N. Kushman and D. Katabi, “Enabling conﬁguration-indep-
endent automation by non-expert users,” in Proceedings of
the 9th Symposium on Operating Systems Design and Imple-
mentation, 2010, pp. 1–10.
[23] A. Goel, K. Po, K. Farhadi, Z. Li, and E. de Lara, “The Taser
intrusion recovery system,” in Proceedings of the 20th ACM
Symposium on Operating Systems Principles, Oct. 2005, pp.
163–176.
[24] T. Kim, X. Wang, N. Zeldovich, and M. F. Kaashoek, “Intru-
sion recovery using selective re-execution,” in Proceedings
of the 9th Symposium on Operating Systems Design and
Implementation, Oct. 2010, pp. 1–9.
[25] F. Qin, J. Tucek, J. Sundaresan, and Y. Zhou, “Rx: treating
bugs as allergies—a safe method to survive software failures,”
in Proceedings of the 20th ACM Symposium on Operating
Systems Principles, Oct. 2005, pp. 235–248.
[26] R. Schwanke, “An intelligent tool for re-engineering software
modularity,” in Software Engineering, 1991. Proceedings.,
13th International Conference on, 1991, pp. 83–92.
[27] A. van Deursen and T. Kuipers, “Identifying objects using
cluster and concept analysis,” in Software Engineering, 1999.
Proceedings of the 1999 International Conference on, 1999,
pp. 246–255.
[28] P. Andritsos and V. Tzerpos, “Information-theoretic software
clustering,” Software Engineering, IEEE Transactions on,
vol. 31, no. 2, pp. 150–165, 2005.
[29] W. Dickinson, D. Leon, and A. Fodgurski, “Finding fail-
ures by cluster analysis of execution proﬁles,” in Software
Engineering, 2001. ICSE 2001. Proceedings of
the 23rd
International Conference on, 2001, pp. 339–348.
[30] N. DiGiuseppe
and
J. A.
Jones,
“Concept-based
failure clustering,” in Proceedings of the ACM SIGSOFT
20th International Symposium on the Foundations of Software
New York, NY, USA:
Engineering,
ACM,
[Online]. Available:
http://doi.acm.org/10.1145/2393596.2393629
ser. FSE ’12.
29:1–29:4.
2012,
pp.
[31] N. Anquetil and T. Lethbridge, “Experiments with clustering
as a software remodularization method,” in Reverse Engineer-
ing, 1999. Proceedings. Sixth Working Conference on, 1999,
pp. 235–255.
[32] O. Maqbool and H. Babri, “Hierarchical clustering for soft-
IEEE
ware architecture recovery,” Software Engineering,
Transactions on, vol. 33, no. 11, pp. 759–780, 2007.
490
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:26:43 UTC from IEEE Xplore.  Restrictions apply.