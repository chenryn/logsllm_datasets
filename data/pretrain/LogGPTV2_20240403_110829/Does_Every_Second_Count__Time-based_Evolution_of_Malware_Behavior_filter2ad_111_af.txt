analyzing adversarial software. In particular, malware authors
can try to evade analysis environments by using several tech-
niques (see Section II-C). While this is true also for other sand-
boxes (and thus affects real-world malware analysis pipelines)
in Section V-A we describe how we tried to conservatively
ﬁlter out samples that did not run correctly in our environment.
However, we cannot rule out the possibility that some malware
detected our environment but then continued to execute its
code without showing its actual malicious behavior.
As our main contribution is to measure to what extent
malicious code can be analyzed within a certain execution
time, the generalizability of our results to other sandboxes is
very important. Therefore, both our sandbox as well as our
machine learning classiﬁer are designed to mimic the current
state of research as closely as possible. Our solution based on
an emulator is inherently more precise that most VM-based
sandboxes, but the precise set of collected events can differ
from one sandbox to another. Another thing that might differ
is the distribution of malware samples under observation. The
dataset we collected for our experiments is the largest ever used
for such a ﬁne-grained analysis and because of our sampling
strategy we are conﬁdent that it closely resembles the one that
might have been observed by other commercial sandboxes in
the same period of time.
14
B. Impact on Prior Work
In Section II-A, we showed that the execution time of
malware analysis experiments in academia greatly differs from
one paper to another, ranging from less than 30 seconds to one
hour. While none of these studies provided an explanation to
support the chosen threshold, every research paper is motivated
by different goals which can justify a longer or a shorter
analysis time.
Our results suggest that in most cases it is sufﬁcient to
execute a malware sample for two minutes to observe the
majority of its behavior. The data collected during this window
also led to the best accuracy for our machine learning classiﬁer.
Therefore, we now want to see how these results could have
impacted those studies that either executed samples for a very
short time (i.e., for less than two minutes according to Table
I), or for an unusually long amount of time (i.e., more than
ﬁve minutes according to our survey). We chose to focus
on these intervals because on the one hand execution times
shorter than two minutes might have compromised the results
of an experiment, while on the other hand the use of long
time thresholds was probably a sign of a sub-optimal use
of computational resources. This is also important because
the number of analyzed samples is often determined by the
amount of time at the disposal of the researchers. Thus, running
samples for longer than it
is needed likely results in the
choice of smaller datasets, which could affect the statistical
signiﬁcance of the results.
However, it is very important to understand that the exact
time threshold depends on the overhead introduced by the
analysis system (in our case between 1.4x and 3.7x). Moreover,
the fact that previous experiments were conducted often a
decade ago, therefore on much slower server infrastructures,
make an exact comparison very difﬁcult.
By looking at the papers in the ﬁrst category (short ex-
ecution time), we can ﬁnd mainly two different motivations
for the experiments performed by the authors. The ﬁrst is
to showcase a prototype of the proposed technique. In this
case, the focus of the work is on the design of the sandbox
itself. Examples in this category are papers that describe new
network-oriented sandboxes [44], [45], [108], [109] or those
describing the use of hypervisor techniques [56]. Overall, we
believe that for these papers the short execution time was
not critical for the overall contribution. However, the results
obtained by the experiments have to be handled with care. The
second motivation for papers in this category is to identify the
most suitable classiﬁer based on either system calls or API
calls (i.e., [46], [94]). In this case, the 30 seconds timeout
used by the authors cast some doubts about the ﬁndings of
these studies and it is unclear whether the same results would
hold for a longer execution time.
Among papers that executed samples for longer than ﬁve
minutes, several work aimed at classifying or detecting mal-
ware [10], [20], [25], [59], [89]. For such cases, our results
suggest that the long execution times adopted by the authors
were not required. The small case study by Canzanese et
al. [25] conﬁrms our results. Other papers in this category
aim at classifying Android malware by extracting behavioral
features [24], [36], [63]. It is particularly noteworthy that Cai
et al. [24] only include such apps that explore at least 50%
of their code during the experiments (an amount we rarely
observed in any of the samples in our dataset). Overall, we
cannot draw conclusions about their execution times as the
speciﬁcs of Android and Windows malware may differ. Finally,
the goal of Severi et al. [87] was to provide a collection of
malware traces in PANDA to enable further research. The
authors presented a use-case to estimate the global unique code
that is ever executed in a sandbox and one about a classiﬁcation
based on features extracted from the analysis. While our results
suggest that the execution time of 10 minutes is not required
in these scenarios, it can still be helpful to collect longer traces
for future experiments. Finally, a last aspect is the collection
of network packets from running malware [41], [82], [84],
typically to observe DNS and HTTP requests. Russow et al.
[82] found that only 23.6% of the endpoints which can be
observed within one hour have been discovered in the ﬁrst
ﬁve minutes. This suggests that it can be useful to execute
samples for a longer time. However, such a long analysis time
is out-of-scope in most scenarios.
C. Recommendations
Rather than revealing inadequate execution times in previ-
ous dynamic malware analysis experiments and sandboxes, the
main goal of our work is to provide insights to guide future
dynamic analysis experiments.
We suggest to run samples for two minutes (adjusted for
the actual overhead of the system) if the goal of the dynamic
analysis is to perform a classiﬁcation of the samples or simply
to build a behavioral report for manual inspection. This is the
case for most industrial analysis environments, whose appli-
cations are mostly classiﬁcation tasks of unknown programs
into different malicious or benign categories. Furthermore, we
suggest all industrial sandboxes to implement countermeasures
for basic timing attacks, usually deployed by malware authors
as a defence strategy to hide a part of the malicious behaviors.
Only few commercial sandboxes already handle the sleep
functions, but they should also be aware of the presence of
malware that implements alternative techniques to delay the
time (e.g., NtWaitForSingleObject).
Experiments conducted by researchers in academia might
need to tune the execution threshold to reﬂect the goal of
their study. However, our recommendation to the authors is to
include in the papers a justiﬁcation of the adopted threshold,
by using our experiments and results to guide their decision.
In any case, our measurements suggest that long execution
times (ﬁve minutes or more) are often unjustiﬁed and would
only increase the overall analysis time–which is often a costly
resource in academic studies. Therefore we believe that it is
better to analyze more samples for a shorter time than to reduce
the dataset size to accommodate longer executions.
VIII. KEY TAKEAWAYS
In this paper we argue for a data-driven approach to
malware analysis. As a ﬁrst step in this direction, we conducted
an extensive set of experiments to evaluate the impact of the
analysis time on the results collected by a malware analysis
sandbox. We can summarize our main ﬁndings along the
following ﬁve points:
15
1) Samples, both benign and malicious, tend to run either
for a short amount of time (less than two minutes in our
emulator) or for a long time (over ten). This is based
on samples analyzed on the ﬁrst day in which they were
ﬁrst collected. The analysis of old samples would probably
move the curve even further towards short executions, as
external components and needed infrastructure may not be
available anymore for the sample to continue its execution.
2) While new system calls can be collected on a regular basis
for the entire duration of a program, the code coverage
tends to plateau very fast, typically in the ﬁrst minute or
two of execution.
3) Stalling code is a very well known anti-analysis tech-
nique. However, its most common form (which relies on
invoking one of the sleep functions) only affected 2-
3% of our samples. Nevertheless, countermeasures are easy
to implement (and are in fact adopted by some of the
commercial sandboxes), allowing to properly analyze even
those samples.
4) A single execution of a binary inside a sandbox typically
exposes between 10% and 40% of its code. The actual
value, and the amount of time that each sample executes
for, depends on the family but it also varies greatly within
the same family.
5) Our experiments with a machine learning classiﬁer show
that not just the volume, but also the “quality” of the
collected data (in terms of how useful it is to classify the
sample), is mostly concentrated in the ﬁrst two minutes of
execution.
By considering all factors mentioned above, and based on
the data analyzed in our experiment, we therefore conﬁrm the
initial intuition of Willems et al. [101] that a threshold of two
minutes is sufﬁcient in the vast majority of the cases for the
analysis of freshly collected malware samples.
Of course, the actual value needs to take into considera-
tion the amount of available resources. Moreover, this value
is based on new samples submitted over a period of six
months, between November 2019 and May 2020. Therefore,
the threshold should be regularly updated to reﬂect changes
in the malware ecosystem or in the type of collected data.
Thus, we believe security companies should adopt a self-tuning
analysis infrastructure, where the parameters of the sandbox
are regularly re-tuned based on the recently-collected data.
ACKNOWLEDGMENT
This research was supported by the European Research
Council (ERC) under the Horizon 2020 research and inno-
vation program (grant agreement No 771844 – BitCrumbs).
REFERENCES
[1] Anubis sandbox. [Online]. Available: https://anubis.iseclab.org
[2] Cuckoo
Available:
[Online].
sandbox.
https://github.com/
cuckoosandbox/cuckoo
[3] Hybrid-analysis sandbox. [Online]. Available: https://www.hybrid-
analysis.com
[4] Panda – syscalls2 plugin. [Online]. Available: https://github.com/
panda-re/panda/tree/master/panda/plugins/syscalls2
[5] Panda plugin win7proc. [Online]. Available: https://github.com/panda-
re/panda/tree/panda1/qemu/panda plugins/win7proc
[6] Rekall forensics. [Online]. Available: http://www.rekall-forensic.com
16
[7] Smda. [Online]. Available: https://github.com/danielplohmann/smda
[8] Virus total. [Online]. Available: https://www.virustotal.com/
[9] Vmray sandbox. [Online]. Available: https://www.vmray.com/
[10] M. Abdelsalam, R. Krishnan, Y. Huang, and R. Sandhu, “Malware
detection in cloud infrastructures using convolutional neural networks,”
in 2018 IEEE 11th International Conference on Cloud Computing
(CLOUD).
IEEE, 2018, pp. 162–169.
[11] S. Ahmed, Y. Xiao, K. Z. Snow, G. Tan, F. Monrose, and D. D. Yao,
“Methodologies for quantifying (re-)randomization security and timing
under JIT-ROP,” in Proceedings of the 2020 ACM SIGSAC Conference
on Computer and Communications Security, 2020.
[12] B. Anderson and D. McGrew, “Machine learning for encrypted
malware trafﬁc classiﬁcation: accounting for noisy labels and non-
stationarity,” in Proceedings of the 23rd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, 2017.
[13] B. Anderson, D. Quist, J. Neil, C. Storlie, and T. Lane, “Graph-
based malware detection using dynamic analysis,” Journal in computer
Virology, vol. 7, no. 4, pp. 247–258, 2011.
[14] D. Andriesse, A. Slowinska, and H. Bos, “Compiler-agnostic function
detection in binaries,” in 2017 IEEE European Symposium on Security
and Privacy (Euro S&P), April 2017.
[15] K. Aoki, T. Yagi, M. Iwamura, and M. Itoh, “Controlling malware http
communications in dynamic analysis system using search engine,” in
2011 Third International Workshop on Cyberspace Safety and Security
(CSS).
IEEE, 2011, pp. 1–6.
[16] S. Attaluri, S. McGhee, and M. Stamp, “Proﬁle hidden markov models
in computer virology,
and metamorphic virus detection,” Journal
vol. 5, pp. 151–169, 2009.
[17] D. Balzarotti, M. Cova, C. Karlberger, C. Kruegel, E. Kirda, and
G. Vigna, “Efﬁcient detection of split personalities in malware,” in
In Proceedings of the Symposium on Network and Distributed System
Security (NDSS), 2010.
[18] T. Barabosch and E. Gerhards-Padilla, “Host-based code injection
attacks: A popular technique used by malware,” in 2014 9th In-
ternational Conference on Malicious and Unwanted Software: The
Americas (MALWARE).
IEEE, 2014.
[19] U. Bayer, P. M. Comparetti, C. Hlauschek, C. Kruegel, and E. Kirda,
“Scalable, behavior-based malware clustering.” in NDSS, 2009.
[20] U. Bayer, E. Kirda, and C. Kruegel, “Improving the efﬁciency of dy-
namic malware analysis,” in Proceedings of the 2010 ACM Symposium
on Applied Computing, 2010.
[21] M. Brengel and C. Rossow, “Memscrimper: Time-and space-efﬁcient
storage of malware sandbox memory dumps,” in DIMVA. Springer,
2018, pp. 24–45.
[22] D. Brumley, C. Hartwig, Z. Liang, J. Newsome, D. Song, and
H. Yin, Automatically Identifying Trigger-based Behavior in Malware.
Boston, MA: Springer US, 2008, pp. 65–88.
[23] P. Burnap, R. French, F. Turner, and K. Jones, “Malware classiﬁca-
tion using self organising feature maps and machine activity data,”
computers & security, vol. 73, pp. 399–410, 2018.
[24] H. Cai, N. Meng, B. Ryder, and D. D. Yao, “Droidcat: Uniﬁed dynamic
detection of Android malware,” Department of Computer Science,
Virginia Polytechnic Institute & State University, Tech. Rep., 2016.
[25] R. Canzanese, M. Kam, and S. Mancoridis, “Multi-channel change-
point malware detection,” in 2013 IEEE 7th International Conference
on Software Security and Reliability.
IEEE, 2013, pp. 70–79.
[26] Y. Cao, J. Liu, Q. Miao, and W. Li, “Osiris: A malware behavior
capturing system implemented at virtual machine monitor layer,” in
2012 Eighth International Conference on Computational Intelligence
and Security, 2012.
J. Chung, C¸ . G¨ulc¸ehre, K. Cho, and Y. Bengio, “Empirical evaluation
of gated recurrent neural networks on sequence modeling,” CoRR, vol.
abs/1412.3555, 2014.
[27]
[28] P. M. Comparetti, G. Salvaneschi, E. Kirda, C. Kolbitsch, C. Kruegel,
and S. Zanero, “Identifying dormant functionality in malware pro-
grams,” in 2010 IEEE Symposium on Security and Privacy.
IEEE.
[29] E. Cozzi, M. Graziano, Y. Fratantonio, and D. Balzarotti, “Understand-
IEEE, 2018, pp. 161–175.
ing linux malware,” in 2018 IEEE S&P.
[30]
J. R. Crandall, G. Wassermann, D. A. S. de Oliveira, Z. Su, S. F. Wu,
and F. T. Chong, “Temporal search: Detecting hidden malware time-
bombs with virtual machines,” in Proceedings of the 12th International
Conference on Architectural Support for Programming Languages and
Operating Systems, ser. ASPLOS XII. ACM, 2006, p. 2536.
[31] B. Dolan-Gavitt, J. Hodosh, P. Hulin, T. Leek, and R. Whelan,
“Repeatable reverse engineering with panda,” in Proceedings of the
5th Program Protection and Reverse Engineering Workshop, 2015.
[32] B. Dolan-Gavitt, T. Leek, J. Hodosh, and W. Lee, “Tappan zee (north)
bridge: Mining memory accesses for introspection,” in Proceedings of
the 2013 ACM SIGSAC CCS, ser. CCS ’13. ACM, 2013.
[33] B. Efron, T. Hastie, L. Johnstone, and R. Tibshirani, “Least angle
regression,” Annals of Statistics, vol. 32, pp. 407–499, 2004.
[34] M. Egele, T. Scholte, E. Kirda, and C. Kruegel, “A survey on
automated dynamic malware-analysis techniques and tools,” ACM