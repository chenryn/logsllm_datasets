sively be accessed from external hardware, no login creden-
tials are required to inject our hypervisor into its memory. To
access the memory of the target host, we ﬂash the PCILeech
ﬁrmware to the PCIe Screamer FPGA, and attach it to a free
PCIe slot on the target side. PCIe Screamer allows native
64-bit DMA operations, and thus access to the target’s entire
physical memory. Over USB3, PCIe Screamer is connected
to an analysis machine that is used to execute the controlling
agent software. The agent, written in Go, serves the analyst
as an interface for controlling our hypervisor through the
PCILeech host software.
3.1 Mode of Operation
For the installation and removal of our hypervisor, the agent
uses DMA to inject multiple code stages into the target mem-
ory. This is possible as x86-64 ensures cache coherency re-
garding DMA operations, preventing processors from retriev-
ing inconsistent data [24, chap. 11.3.2]. The stages are de-
signed to preserve memory and processor state, so that the tar-
get is not notably impacted by the injection (see Section 4.1).
Apart from this, we outsource most computational tasks to the
remote agent in order to further reduce target modiﬁcations.
Prior to the code injection, the agent needs to determine
the location of the target kernel. Due to the usage of Ker-
nel Address Space Randomization (KASLR), modern Linux
kernels are randomly placed in physical memory. Therefore,
the agent scans the entire physical memory until it matches a
pre-registered signature of the kernel’s ﬁrst code page. To cor-
rectly terminate the scanning process, the agent issues DMA
read operations to probe the amount of memory installed in
the target host. As certain memory ranges do not respond
to DMA (e.g., MMIO areas which are used to map certain
devices), a timeout is employed on each read operation to
prevent the agent from stalling.
Once the kernel is found, the agent writes special injection
stages (see Section 3.2) into the target’s physical memory.
These stages hijack the control ﬂow of the target kernel, and
subsequently install a thin hypervisor that virtualizes the sys-
tem at runtime. Despite being able to write arbitrary memory
over DMA, taking over the system’s operation is mandatory
to actually execute the injected code. For a stealthy operation,
we designed a lightweight, VMX-based custom hypervisor
that mostly avoids any interference with the target’s operation.
Limited to DMA, the agent cannot obtain contextual informa-
tion of the running system, as it is restricted to a physical view.
Therefore, the agent risks potentially corrupting the target by
overwriting the memory which is currently in use. To mini-
mize this risk, the agent searches for regions that are unlikely
to be used. During our research, we determined the ﬁrst two
KiBs of a Linux kernel’s code segment to be the perfect injec-
tion spot as it mostly consists of nop instructions which do not
have any functional purpose. As of that, modifying this nop
area does not corrupt the kernel’s execution. Prior to overwrit-
ing any memory, the agent stores the original content to the
analysis machine, so it can be restored in due time. Since the
nop area does not hold enough space for the actual hypervisor,
the stages request the target kernel to allocate further memory.
This is not an issue from a forensics perspective, as occupying
currently unused memory should not corrupt evidential data
in most cases. After receiving the necessary information, the
agent sets up the hypervisor’s memory layout, and copies the
appropriate page tables and the binary of the hypervisor to
the previously allocated memory. Withdrawing control from
the target leads the hypervisor to take over and virtualize the
running cores. At this point, the hypervisor is in full control,
ready to fulﬁll stealthy analysis tasks or perform memory
forensics. Eventually, the agent removes the injection stages,
restoring the original target memory.
Using DMA, an analyst can now instruct the agent to trans-
parently interact with the hypervisor, allowing to send com-
mands or receive data. Compared to conventional commu-
nication channels that rely on the ﬁle system or a network
card, exchanging data over DMA is both stealthier and less
intrusive to the target state. Especially for targets that might
be compromised, covert communication is an important re-
quirement for the integrity of the transferred data.
To deinstall our system, the agent overwrites parts of the
target’s kernel space with removal stages (see Section 3.3)
which have the purpose of transparently devirtualizing the
system. After that, the hypervisor signals the agent to restore
the overwritten memory including the hypervisor area. At this
point, the target is resumed without leaving any traces of our
system.
Injection
3.2
We designed the injection process in multiple injection stages,
so that no relevant data is corrupted during the virtualization
of the target system. We minimized dependencies on the target
kernel, as the system could have already been compromised,
and thus subvert the injection process. One exception is to
query the target kernel to allocate some memory for the hy-
pervisor area. This seems acceptable, however, as we doubt
that malware could forge the allocation without risking severe
system failures. Figure 2 gives an overview of the injection.
While stage I1 only consists of a few bytes overwriting a ker-
nel function to hijack the control ﬂow, I2 and I3 are written
to the previously mentioned nop area. In contrast, I4 and I5
are directly placed within the hypervisor area. Upcoming, we
shed light on the individual injection stages.
Stage I1: Control Flow Hijacking Once the target kernel’s
base address was found, an injection location which allows
to take over a processor’s execution is chosen. In contrast
168    23rd International Symposium on Research in Attacks, Intrusions and Defenses
USENIX Association
detour to a custom handler before returning to the originally
intended code. Therefore, we avoided to overwrite these ﬁrst
few bytes with our hook. Instead, the agent replaces the fol-
lowing bytes with a relative jump to stage I2. As we could
have chosen any other spot in the kernel code, monitoring the
idle loop is not a reliable method to generically detect our
approach. To virtualize every core, the hook must remain until
all processors have passed the residual stages. Our current im-
plementation relies on target kernel functionality to determine
the number of target cores (num_online_cpus). However,
enumerating the Non-Uniform Memory Access (NUMA) hier-
archy could possibly provide a more target independent way.
After entering the hook, the core jumps to stage I2.
Stage I2: Processor Serialization Until having allocated
further memory for the hypervisor, we use the kernel’s stack
to temporarily store register content that is about to be mod-
iﬁed, preventing a loss of processor state. As discussed in
Section 4.1, this appears to be insigniﬁcant from a forensics
perspective. The subsequent process is sequentialized by the
possession of a global lock which prevents the target cores
from concurrently entering the following stages. In case the
lock has already been occupied, a core immediately detours
to a speciﬁcally prepared trampoline that is responsible for
executing the instructions overwritten by our hook, before
resuming the original execution within the idle loop (dashed
arrow in Figure 2). Thus, no processor stalls while the lock is
held by another core. The trampoline mechanism is also used
for processors that were already virtualized, as these must be
prevented from reentering the subsequent stages. To deter-
mine the current processor’s virtualization state, I2 attempts
to force a VM exit which is only generated in VMX non-root
mode. In case of an already virtualized core, this forces a
context switch to the hypervisor which in turn informs I2
about the current processor’s operation mode. Whenever a
non-virtualized core acquires the lock, it is allowed to enter
the subsequent stages, eventually leading to its virtualization.
Stage I3: Hypervisor Setup To prevent the target from
disrupting the installation of our hypervisor, I3 temporarily
disables all interrupts until the processor reaches the end
of I4. Besides maskable interrupts, modern processors sup-
port NMIs for critical asynchronous event delivery like hard-
ware interrupts or watchdogs. These special kind of inter-
rupts cannot be trivially disabled with the cli instruction,
however. To temporarily deactivate NMI delivery, I3 recon-
ﬁgures each processor’s LAPIC, disabling the valid ﬂags of
its LVT registers (see Section 2.3). Additionally, it consults
the IA32_APIC_BASE MSR to determine the system’s current
APIC mode, as both xAPIC and x2APIC are supported by
modern processors. Consequently, our implementation offers
different ways to access a LAPIC. As NMIs must be disabled
before switching to the hypervisor’s memory layout, there
Figure 2: View of physical memory after the injection stages
I1 to I5 were written to the target memory. The hypervisor
area was vicariously allocated by the target kernel. Arrows
represent jumps to subsequent stages. The dashed arrow sym-
bolizes the abort of the injection, jumping back to the kernel’s
idle routine.
to the base of the kernel which differs after each boot due
to KASLR, offsets within its code segment only depend on
the actual kernel version. This is because the Linux kernel is
linearly mapped in both physical and virtual memory. Con-
sequently, addresses of arbitrary kernel symbols can be stati-
cally computed for a particular target by adding the respective
offsets to the previously determined kernel base address.
To actually hijack the target kernel, we chose to hook its
idle routine (intel_idle), as it is regularly executed by each
processor and allows to run code with ring 0 privileges which
are required for the virtualization. Other valid injection spots
include a system’s scheduler as well as its interrupt handlers.
For the later, special care has to be taken since the injected
code needs to run in interrupt context, however. To decrease
the latency a processor takes to enter our hook, even multiple
injection locations could be selected. As long as the hook
is placed within the target’s kernel space and is regularly
executed, almost any location could be used to hijack the
control ﬂow. This prevents the target system from mitigating
our injection by monitoring speciﬁc memory regions.
As soon as a core starts idling, intel_idle is invoked, en-
tering a sleeping state to save power until it is woken up again.
Before placing the hook, the agent saves the respective mem-
ory to be able to restore it later on. Acquiring memory over
non-atomic protocols like DMA introduces a race condition,
as the kernel might alter the area before it enters the hook. As
the kernel’s code segment should be mapped non-writable at
all times, it usually is never modiﬁed, however. Nevertheless,
a custom kernel might remap its code ranges to be writable.
Even mainline kernels might sometimes alter parts of their
code segments when using instrumentation frameworks like
Ftrace. When activated, Ftrace dynamically modiﬁes the ﬁrst
few bytes of a kernel function, so that subsequent invocations
USENIX Association
23rd International Symposium on Research in Attacks, Intrusions and Defenses    169
Kernel	Code	SegmentI2I3I1I4I5IdleRoutinenop		areaHypervisor	Areakmalloc SpacePhysical	Target	Memory5.1.2.2.3.4.is no way to map the LAPICs’ physical addresses. There-
fore, we decided to rely on the kernel’s APIC_BASE symbol
to make use of the already established kernel mapping. From
a forensics view, we do not expect this symbol to be a critical
target dependency, as it is deﬁned as a kernel constant.
For the memory of the actual hypervisor, I3 manually calls
the kernel’s kmalloc function to allocate additional space.
Our experiments revealed that a single two MiB page seems
to be a sufﬁcient size. Most of the hypervisor area serves for
dynamic memory allocations during the setup of the hyper-
visor. Other parts are used to store its code, data, stacks, and
page tables. The resulting base address of the hypervisor area
is then translated to its physical counterpart, and provided to
the agent using DMA. After receiving the information, the
agent copies relevant parts of the hypervisor to the newly
allocated area. These include a custom memory layout which
exclusively maps the hypervisor and the injection stages. De-
pending on the speciﬁc use case, mapping certain parts of
the guest’s address space might be conceivable. Eventually,
the processor’s TLBs are ﬂushed, the newly created memory
layout is enabled, and Process Context Identiﬁerss (PCIDs)
are deactivated to prevent caching leftovers.
Stage I4: NMI Handling Entering stage I4 ﬁrst sets up
a processor individual stack which is subsequently used to
store the state of the core. To prevent stack overﬂows from
corrupting any memory, the stacks are surrounded by non-
presently mapped pages.
Before NMIs can be safely reenabled, I4 registers a custom
handler which ensures NMIs that would otherwise be lost to
be reinjected into the guest. This additionally requires the
installation of both a custom Global Descriptor Table (GDT)
and Interrupt Descriptor Table (IDT). During the execution
of the hypervisor, the handler records upcoming NMIs within
a bitmap which indicates if a processor has an NMI pending.
To distinguish the running cores, and thus mark the correct
bit within the pending bitmap, the hypervisor’s NMI handler
compares the stack pointer of the current processor with the
hypervisor’s stack ranges. As every virtualized core has its
own stack which is known to the hypervisor, these informa-
tion can be used to distinguish the processors without the need
to consult the target kernel. Until properly acknowledged by
clearing the pending ﬂag in the respective LVT, an NMI is not
forwarded to the corresponding processor core. This, however,
prevents further NMIs of the same type from being delivered
independent of the valid ﬂag of the corresponding LVT regis-
ter. As a solution, the hypervisor consults the pending bitmap
to determine if an NMI must be reinjected to the current vir-
tualized core every time it is about to reenter the guest. This
is necessary, as watchdog tasks could potentially misbehave
due to missing NMIs, and thus corrupt the target state. To
inject an NMI into the guest, the hypervisor sets the valid
ﬂag of the VM-entry interruption-information ﬁeld within
the corresponding VMCS, and speciﬁes the interruption type
ﬁeld to indicate an NMI. This automatically invokes the guest
kernel’s NMI handler as soon as the guest is resumed. From
the perspective of the guest, it is not distinguishable if an NMI
appeared during its own operation or due to an injection from
our hypervisor. The guest’s NMI handler then clears the pend-
ing ﬂag in the respective LVT, allowing NMIs of the same
type to be delivered again. Lastly, I4 reenables all interrupts,
and jumps to the ﬁnal stage.
Stage I5: Processor Virtualization To preserve the full
register state of a processor, all previous stages had to be im-
plemented in plain assembly code. With a custom stack being
set up during the previous stage, I5 is implemented in the
high-level programming language C. The stage is responsible
for the setup of our hypervisor and the actual virtualization
of a processor. Consequently, it allocates relevant data struc-
tures like the VMCS within the hypervisor area. With all
the preparations done, the processor releases the global lock
before entering the virtualization process. Provided Intel Vir-
tualization Technology (VT-x) was not deactivated in the Uni-
ﬁed Extensible Firmware Interface (UEFI) or BIOS settings
(which is not the case in most modern machines), I5 enables
VMX operation, and copies the previously saved processor
state to the VMCS. Similar to a hypervisor rootkit [42, 52, 69],
this allows to transparently launch the target system inside a
hardware-assisted VM, and resume its original execution with
its current state. To remain stealthy, we conﬁgure the VMCS
so that the hypervisor intercepts only a minimal set of events.
Thus, hardware is directly passed to the guest without the
hypervisor’s interference. This increases the overall perfor-
mance of the guest while reducing detectable side channels.
Depending on the speciﬁc use case, it might be necessary to
conﬁgure the interception of additional events, however. Due
to the hypervisor area being allocated by the target kernel,
it should never be accessed by accident. To prevent the tar-
get from purposely accessing the hypervisor area, we set up
EPTs to redirect read accesses to a 4 KiB guard page. Write
accesses are conﬁgured to generate an EPT violation in order
to keep the originally stored memory on the analysis machine