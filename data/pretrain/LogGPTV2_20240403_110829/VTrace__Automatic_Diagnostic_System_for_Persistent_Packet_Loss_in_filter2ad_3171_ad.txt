the metric loc and the rearranged order of log data. Take the case
in Fig. 6 as an example. Firstly, SortBolt needs to find the first hop
of the flow, i.e., looking for the log containing dscp_mark=1 in
the set of logs that are the first one in each server. Knowing that
the location of the first hop in the flow is loc = in, Pack 1 in
Server 1 is quickly determined based on the metrics dscp_mark
and loc. Then, SortBolt can easily find its output packet Pack 2
in Server 1 whose loc = out. To obtain the next hop, we take
advantage of the fact that the out_dip and out_sip of the packet
will not change until it leaves the next hop. Therefore, SortBolt
uses (out_dip $ out_sip $ loc=in) as the matching rule to seek
for the next matched log. As a result, the Pack 1 in Server 2 is
found. Subsequently, SortBolt repeats the above steps until some
abnormalities appear or all the logs are sorted. Note that the log of
the last hop must have dscp_mark=2. In order to avoid reordering
the same log, each log item will be removed from the data group of
the corresponding server after being ranked.
3.6 DCA Implementation
DCA is implemented in the JStorm process engine, which is realized
with Java on several JStorm workers. The topology has been given
in Fig. 5. Specifically, in order to support multiple concurrent tasks,
a pool of VTraceTaskBolts is maintained. Each VTraceTaskBolt
needs to query for data from VTrace Task Database and Log Agents.
However, with the number of bolts rising, the two data sources are
unable to support too many queries if each bolt requests for data
separately, since some may query for a large amount of data (e.g.,
exceeding 10 megabytes per second). Certainly, such a busy query
could result in data loss. To deal with the scalability issue brought
by the excessive data reading with the increase of JStorm workers,
we make several optimizations on the JStorm topology during its
implementation.
38
Server 1: node_ip_1Pack1Pack2Pack3Pack4Server 2: node_ip_2Pack1Pack2Server 3: node_ip_3Pack1Pack2Pack3Pack4dscp_mark:1out_dip:0.0.0.1out_sip:0.0.0.0loc: indscp_mark:0out_dip:0.0.0.2out_sip:0.0.0.1loc: outdscp_mark:0out_dip:0.0.0.2out_sip:0.0.0.1loc: indscp_mark:0out_dip:0.0.0.3out_sip:0.0.0.3loc: outdscp_mark:0out_dip:0.0.0.3out_sip:0.0.0.3loc: indscp_mark:0out_dip:0.0.0.1out_sip:0.0.0.3loc: indscp_mark:2out_dip:0.0.0.4out_sip:0.0.0.1loc: outLogSpoutsVTraceTaskSort‚Ä¶ ‚Ä¶‚Ä¶ ‚Ä¶Spout-ShanghaiSpout-US-EastSpout-EU-CentralBoltBoltBoltBoltBoltBoltVTraceTask‚Ä¶ ‚Ä¶Bolt 1 (busy)Bolt 2 (idle)Bolt 3 (idle)VTraceTaskSpoutTask 1Task 1Task 2‚Ä¶VTrace
SIGCOMM ‚Äô20, August 10‚Äì14, 2020, Virtual Event, USA
Table 3: Experimental settings on vRouter.
Settings/Scenarios
No VTrace
No task
Ten tasks but zero
match
Ten tasks and full
match
Description
The VFD does not support VTrace
Support VTrace but no VTrace task is issued now
Ten tasks are issued but there is no matched packet for
any task
Ten tasks are issued and there are 100 target packets to
be tracked for each task
virtual flows in Fig. 1, we conduct experiments on packets of differ-
ent sizes (from 64 bytes to 1280 bytes) under different experimental
settings, as listed in Table 3. The results are presented in Fig. 8.
For packets over 256 bytes, there is almost no decrease in the
forwarding rate. However, the forwarding rate is declined by at
most 12% and 14% while performing experiments on the packet
of 64 bytes and 128 bytes, respectively. For the same forwarding
capability, the smaller the packet size is, the larger is the number of
packets per second, which results in the more frequent look of flow
table by the vRouter and then the decrease of the forwarding rate.
Given that the packets smaller than 256 bytes only occupy a very
tiny part of the total business traffic and appear in a short time, the
forwarding rate will not be influenced at most times. Besides, not all
VFDs need to perform 5-tuple matching and coloring, which shows
that the real impact brought by VTrace could be smaller. Thus, the
forwarding rate loss is acceptable. As for the CPU usage in Fig.
8(b), we find that matching the packet larger than 256 bytes will
incur at most 7% rise of average CPU usage on the server where the
vRouter resides, compared to the scenario "No VTrace". The extra
CPU resources consumed by the VFD to perform VTrace tasks are
affordable.
Influence on vSwitches: The tests are conducted on a vSwitch in
the test environment, which has the same capability as the vSwitch
in the production network and serves 3 ECSs (may belong to dif-
ferent tenants). Each ECS has an 8-cores Intel(R) Xeon(R) E5-2630
2.3GHz CPU and 16GB memory. The traffic crossing the vSwitch
reaches its maximum forwarding capability. In such a setting, we
conduct experiments, which are similar to the ones listed in Table
3, on vSwitches. The obtained results indicate at most 1% drop in
the forwarding rate compared to the VTrace-free scenario. Further,
we evaluate the performance when all packets traversing through
are processed by VTrace. Also compared to the VTrace-free case,
the forwarding rate under the "coloring" operation only is down
by 4.1%, whereas the complete version of VTrace on all packets
(i.e., coloring and logging) results in a significant performance drop
(18.1% down). In summary, when VTrace works in a flexible and
on-demand manner, it induces an affordable performance loss on
the vSwitch. However, it may be challenging to apply VTrace as an
always-on service due to the performance degradation issues.
4.2 Overhead
Storage: Since VFDs transmit logs to the Log Agent in the local
region without storing it and the real-time process engine does not
store those logs, the storage overhead is mainly due to the archive
of logs in Log Agents. Since VTrace is an on-demand tool, we
will next analyze the storage overhead when VTrace works at the
(a) The influence on the packet forwarding rate.
(b) The influence on the average CPU usage.
Figure 8: The performance of vRouters with VTrace.
maximum capacity. According to our deployment experience, the
maximum peak number of VTrace tasks issued by cloud operation
engineers per week is about 1200 (see Fig. 10 in Section 5.3). Given
the possibility of VTrace being open for tenants, here we consider
a larger concurrent task number (i.e., 5000) for overhead analysis.
Additionally, the average diameter in the overlay network is around
4 hops and 300 target packets per task are usually sufficient for
network diagnostics. In practice, the maximum size of the log is
about 600 bytes. Hence, the maximum storage at a time is about
600ùêµ √ó 2√ó 4(‚Ñéùëúùëù) √ó 300(ùëùùëéùëêùëòùëíùë°) √ó 5000(ùë°ùëéùë†ùëò) = 6.71 GB. Compared
to [24] and [26], the storage here is larger. But it is not a big issue
for current cloud storage, especially the storage will be freed when
VTrace tasks are completed.
Bandwidth: The bandwidth is consumed by VTrace when trans-
mitting log data. As the volume of the log data generated at max-
imum capacity is only 6.71 GB and VTrace provides on-demand
services, the bandwidth overhead is negligible.
4.3 Speed
We conduct experiments in both the test environment and the pro-
duction cloud network to illustrate the speed at which VTrace re-
sponds to VTrace tasks. The JStorm process engine is implemented
on servers with Intel (R) Xeon (R) 32-core, 2.6 GHz CPU. Five JS-
torm workers are virtualized in these servers, and each worker
possesses 1-core, 2.6GHz CPU and 2 GB memory. The task respond-
ing time includes the time of configuring VFDs, the trace_time, the
buffer_time, the time of preprocessing and path reconstruction.
In both test and production environments, the time of configuring
VFDs usually arranged between 600ms and 700ms. Besides, the
trace_time is specified by users. The buffer_time is fixed where
short_buffer_time = 12s and long_buffer_time = 24s in our ex-
periments. Next, we focus on presenting the time of preprocessing
and path reconstruction.
39
70758085909510010564 B128 B256 B512 B1280 BForwarding rate (%)No VTraceNo taskTen tasks but zero matchTen tasks and full match02040608010064 B128 B256 B512 B1280 BCPU usage (%)No VTraceNo taskTen tasks but zero matchTen tasks and full matchSIGCOMM ‚Äô20, August 10‚Äì14, 2020, Virtual Event, USA
C. Fang et al.
Test environment: The average time for each packet is used to
illustrate the speed. We issue 800 concurrent VTrace tasks in the
experiment to demonstrate the performance. In each task, it is
required to trace 100 target packets in a 4-hop flow. Our deployed
JStorm completes all tasks within 43.5 seconds. It takes averagely
0.54ms to process a packet trajectory.
Production cloud network: For the VTrace tasks issued in the
production cloud network, the parameters of packet_count in
tasks are different. Similarly, the average time for each packet is
also used here. According to the results returned by VTrace since
its deployment, it averagely consumes 5ms per packet for prepro-
cessing and path reconstruction.
In our production cloud system, most users set trace_time to
10 to 40 seconds or 500 to 600 seconds (see Fig. 9(b)). In summary,
VTrace tasks usually take a few minutes or a dozen minutes in total,
most of which is spent waiting for VTrace data generation. In our
experience, it typically costs network experts hours to diagnose
the root cause without VTrace. As a result, VTrace dramatically
simplifies the diagnosis of persistent packet loss.
5 DEPLOYMENT EXPERIENCES
We deployed VTrace in Alibaba Cloud network in May 2018, where
about 90% VFDs are VTrace-enabled. In fact, VTrace Application
maintains a whitelist for these VTrace-enabled VFDs such that a
VTrace task will not be issued to VFDs that do not support VTrace.
When a VTrace task is issued, VTrace is used to check whether
a persistent packet loss exists in the overlay network of this real-
world cloud system and present the root cause if so. In the following,
we share our deployment experiences with VTrace from several
aspects, including practical diagnosis cases of persistent packet loss
and other experiences learned, to demonstrate the effectiveness of
VTrace in practice.
5.1 Applicability of VTrace
Packet drops in the overlay network: In August 2018, a tenant
reported that an ECS in his own VPC can not be accessed from the
Internet. A lot of connection establishment requests were experi-
encing timeouts. Operations engineers suspected that there could
be a persistent packet loss in the cloud network. Both the virtual
and physical network teams are responsible for troubleshooting the
problem in their respective network. From the perspective of the
virtual network team, usually they identify the suspicious sVM and
dVM first and then in an orderly manner check all VFDs that may
transmit packets from the sVM to the dVM. Due to the existence
of SLBs, multiple flow paths need to be checked, which greatly
increases the inspection space. Even with the problematic node in
hand, it sometimes still takes hours for network experts to know
the root cause and then fix it. Such a job is really labour-intensive.
Therefore, VTrace was called for help. With a set of suspicious
sVM and dVM specified by the 5-tuple, VTrace started generating
and collecting relevant data and then performed reconstruction of
virtual flow paths. During the path reconstruction, several SortBolts
found that loc = in and loc = error among the logs generated
by a VFD (a vSwitch indeed). It implies that the culprit vSwitch had
problems with sending the traced packets. Through the metric msg
in the log, a fine-grained root cause is determined. That is, the error
code is tx_acl_error, which means that the ACL configuration for
forwarding the target packets is inappropriate or wrong. Actually,
it is a non-VFD fault since this ACL configuration is provided by
a tenant. Thus, operations engineers located the culprit VFD and
helped resolve the problem quickly. It is worth noting that VTrace
usually only takes a few minutes to identify the culprit node and
determine the root cause.
Packet drops in the physical network: Although VTrace is not
designed to diagnose the problem in the physical network, some-
times it can tell that a part of the physical network should be
responsible for it. Here is a practical case in point, which happened
in September, 2018. One of our tenants found that his applications
deployed in the region, the East of America, were suffering packet
dropping. In fact, it was a physical network issue. However, it is not
easy to determine that the overlay network is normal. Typically, the
virtual network team still needs to check all suspicious VFDs one by
one until the end or find a problematic VFD. Given that the overlay
network is actually fault-free, all relevant VFDs are demonstrated
normal after going through all machine logs of suspected VFDs.
Theoretically speaking, the team still needs a further inspection
to ensure that all tenant-related configurations are correct, which
is yet another quite time-consuming work. In this way, an "OK"
conclusion for the overlay network can be drawn manually.
By using VTrace, a result was returned quickly. When VTrace
finished the path reconstruction, it was found that the last hop
ùêøùëéùë†ùë°ùëÉùëéùëêùëò of several reconstructed flow paths is not the true last
hop of the flow (namely, dscp_mark‚â†2 and loc = out). It means
that the last VFD in the path has sent out the packet successfully
whereas the next VFD does not receive it. On this condition, there
must be a packet loss in a certain segment of the physical network
that is responsible for the packet transmission between the two
servers specified by the out_sip and out_dip in the last VFD‚Äôs logs.
With this information, the physical network team rapidly found
that the physical NIC of one of the two servers dropped packets, and
then fixed the problem. More specifically, both storage and network
traffic share the same physical NIC. Since the capability of the NIC
is limited, in this case study, a part of network traffic is discarded
when the storage throughput occupies a significant fraction of NIC
resources. In summary, for physical network issues, VTrace can
quickly release the virtual network team from debugging and may
give a useful direction for solving such problems. Even coarse-grain
diagnosis results from VTrace could help speed up locating the root
cause of the failure.
High latency in the overlay network: Since VFDs generate logs
when the target packet enters and leaves, the duration in each
VFD (or the server it resides) is obtained directly by computing
the discrepancy of the time. Generally, a VTrace task will trace
packet_count target packets. The average latency at each hop
can be obtained to avoid misjudgment induced by accidental high
latency, and the abnormal VFD can be intuitively recognized.
5.2 Make More Sense for non-VFD Faults
As mentioned above, VTrace is effective and efficient in diagnos-
ing persistent packet losses in the overlay network. Actually, for a
VFD fault, usually it will affect multiple tenants due to the cloud‚Äôs
40
VTrace
SIGCOMM ‚Äô20, August 10‚Äì14, 2020, Virtual Event, USA
Table 4: The distribution of root causes for non-VFD faults.
Type of cause
vSwitch (%)
vRouter (%)
Proportion (%)
(a) The CDF of packet_count.
(b) The CDF of trace_time.
Total
Rate limit
Tenant security policy blocking
Tenant configuration error
Unsupported actions
Unknown
4.6
52.4
5.3
3.9
0
66.2
5.4
3.8
18.5