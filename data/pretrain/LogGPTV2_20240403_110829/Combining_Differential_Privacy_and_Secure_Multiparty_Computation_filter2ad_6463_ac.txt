in Sec. 9. We create a bigger matrix
(cid:18) V
(cid:19)
A =
0
0 B
with rV + rB rows and cV + cB columns. Then we add to it two extra columns to
the left, the ﬁrst of which contains the provenance of the row of V or B contained
Algorithm 3 Joining two tables
Input: Matrices V and B
0
1
V
0
0 B
t
(cid:1)
A ←(cid:0) s
s ← the provenance column from V
t ← the provenance column from B
A ← sort A by the ﬁrst (provenance) column, breaking ties by the second column
the last cB columns of A ← propagateValuesBack(the last cB columns of A, the ﬁrst
column of A) using Alg. 4
randomly shuﬄe the rows of A
declassify the second column of A
V (cid:48) ← the rows corresponding to V (0 in the second column) from A
return V (cid:48) without the ﬁrst two columns
in this row of A. The second column contains 0 or 1 depending on whether the
corresponding row of A contains values from V or B, respectively.
Now we sort A by the ﬁrst column, breaking ties by the second column. Then
rows with the same provenance will appear sequentially in A, with the rows from
V appearing before the rows from B with the same provenance.
Now we apply the algorithm in Alg. 4 to the last cB columns of A. This
Algorithm 4 Propagating values back
Input: An n by c matrix M , an n-element vector p containing the provenance of each
row of M
propagateValuesBack(M ,p):
j ← 1
while j < n do
j ← j · 2
return M
for each i ∈ {1, . . . , n − j} (in parallel) do
Mi ← if pi = pi+j then Mi+j else Mi
algorithm takes a matrix M with rows sorted by provenance, and copies the
last row of each provenance to the previous rows of the same provenance. The
while loop does approximately log n iterations. After the ﬁrst iteration, there are
up to two copies of a value. After the second, up to 2, then 4, 8, and so on.
The algorithm makes approximately n log n equality tests. This technique can
be used also for other tasks where we have a matrix sorted by a certain column
(provenance) and we want to do something with each group of rows with the
same provenance. We will see an example later (Alg. 7).
If the provenance of each row of B is diﬀerent then after applying Alg. 4 to
the right part of A, the rows of A with 0 in the second column contain the join
of V and B. We may extract the join using a (linear-time) cut operation. This
operation is also described towards the end of Alg. 3, starting from the shuﬄing
of the rows of A. The declassiﬁcation that follows the shuﬄe does not increase
an adversary’s knowledge. Indeed, it produces a randomly ordered vector of rV
zeroes and rB ones, where both rV and rB are public information.
9
Implementing Personalized Diﬀerential Privacy
Our implementation supports both cases introduced in Sec. 5 but the overhead
is much higher in the second case.
We ﬁrst consider the simpler case. Here we add to the database table an
extra column, where we store the privacy budget of each row. There is another
column that contains the mask vector that shows which rows participate in the
query. When performing an -diﬀerentially private query, we check that each
of the rows participating in the query has at least  left in its privacy budget.
The rows that do not have enough budget are silently excluded from the query.
The other participating rows have their budgets reduced by . Then the query is
executed, using a modiﬁed mask vector, where some rows may have been silently
excluded. This algorithm is given in Alg. 5. The overhead here is n comparisons
(and n multiplications and n boolean operations, which are much cheaper than
comparisons).
Algorithm 5 Personalized Diﬀerential Privacy with in-place budgets
Input: The number of rows n in the table, privacy parameter , an -diﬀerentially
private query Q
m ← the mask column read from the database
b ← the budget column read from the database
for each i ∈ {1, . . . , n} (in parallel) do
ai ← mi ∧ bi ≥ 
i ← bi − ai · 
b(cid:48)
write b(cid:48) to the database as the new budget column
r ← output of Q with a as the mask vector
return r
Now we consider the more complicated case. Here we have in the database
a separate table (the budget table) that contains the privacy budget for each
provenance. The table containing the analyzed values (the value table) has an
extra column that now contains the provenance of that row instead of the budget.
Now the data needed for performing a diﬀerentially private query is in two
separate tables, thus before the query, we need to join those two tables by the
provenance columns, and after the query, we need to extract the updated budgets
from the joined table and write them to the budget table. Because there may
be r rows with a provenance p, the budget of the provenance p must be at least
r, otherwise all the r rows are silently dropped. If the provenance has enough
budget, the budget is reduced by r. Thus the reduction may be diﬀerent for
diﬀerent provenances. We use Alg. 6 for this case.
Algorithm 6 Personalized Diﬀerential Privacy with provenances
Input: Privacy parameter , an -diﬀerentially private query Q
(cid:1)
A ←(cid:0) s
V ← the value table read from the database
B ← the budget table read from the database
s ← the provenance column from V
t ← the provenance column from B
A ← sort A by the ﬁrst (provenance) column, breaking ties by the second column
m ← the mask column from A
b ← the budget column from A
p ← the provenance (ﬁrst) column from A
f ← the frequency table of p using Alg. 7
n ← the number of rows in A
for each i ∈ {1, . . . , n} (in parallel) do
0
1
V
0
0 B
t
hi ← bi ≥ fi · 
i ← bi − hi · fi · 
b(cid:48)
ai ← mi ∧ hi
h ← propagateValuesBack(h (as a 1-column matrix), p) using Alg. 4
for each i ∈ {1, . . . , n} (in parallel) do
randomly shuﬄe the rows of A
declassify the second column of A
B(cid:48) ← the rows corresponding to B (1 in the second column of A) from (p, b(cid:48))
write B(cid:48) to the database as the new budget table
(V (cid:48), m(cid:48)) ← the rows corresponding to V (0 in the second column of A) from (A, a)
r ← output of Q on V (cid:48) with m(cid:48) as the mask vector
return r
Algorithm 7 Frequency table
Input: A sorted vector v of length n
Output: A vector f , where fi is the number of values equal to vi before the ith position
in v
initialize f with zeros
j ← 1
while j < n do
for each i ∈ {j + 1, . . . , n} (in parallel) do
fi ← if vi = vi−j then fi−j + j else fi
j ← j · 2
return f
It uses the same elements as the join algorithm (Alg. 3)—the big sorted ma-
trix A, propagating values back, and cut (extracting certain rows of a matrix)—
but in a modiﬁed way, so we cannot use the join algorithm as a black box. In
addition, it computes (Alg. 7, which uses the same technique as Alg. 4) the
frequency table (the number of rows with each provenance) to determine how
much budget is needed for each provenance and which provenances have enough
budget (the vector h). Then the booleans in h are propagated back from the
rows corresponding B to the rows corresponding to V to ﬁnd the rows whose
provenance has enough budget.
Let nv and nb be the number of rows in the value table and the budget table,
respectively, and n = nv +nb. Then Alg. 6 uses O(n log n) comparisons for sorting
A (using quicksort) and at most a total of 2n log n equality checks for computing
the frequency table and propagating values back (actually, the comparison results
from computing the frequency table could be reused for propagating values back,
thus we only need n log n equality checks instead of 2n log n). The rest of the
algorithm is linear-time.
If we need to make several queries in a row on the same value table and
the same mask vector (but with possibly diﬀerent aggregation functions) then
we can reuse (if we modify Alg. 6 slightly) the results of the O(n log n) part of
the algorithm and need to repeat only the linear-time part for each query. If the
next query uses the same value table but a diﬀerent mask vector then we need to
recompute the frequency table and propagating values back. Sorting (the most
time-consuming part of Alg. 6) needs to be redone only when the next query
uses a diﬀerent value table.
10 Benchmarking Results
In Fig. 1, we give benchmarking results of our implemention for various ag-
gregation functions and for various forms of diﬀerential privacy (global budgets,
Personalized Diﬀerential Privacy with in-place budgets and provenance budgets)
and also for a non-diﬀerentially private (but still secret-shared) version. We have
skipped some of the larger tests whose running time would be predictable from
the running times of other (performed) tests. All tests were performed on a clus-
ter of three computers with 48 GB of RAM and a 12-core 3 GHz CPU with
Hyper Threading running Linux (kernel v.3.2.0-3-amd64), connected by an Eth-
ernet local area network with link speed of 1 Gbps.
If we compare the non-diﬀerentially private and the global-budget version
of count, the overhead is roughly constant (around 360 ms) independent of n
(the number of rows). This overhead is due to the ﬂoating-point operations of
generating a Laplace random value that is added to the ﬁnal result.
When comparing the non-diﬀerentially private and the global-budget version
of average, we have in addition to generating a Laplace random value, the over-
head of 2n comparisons and 3n multiplications. For n = 200000, the overhead
is 3427 ms, of which 2638 ms are comparisons, 368 ms are multiplications, and
395 ms are ﬂoating-point operations (mostly for the Laplace random value).
When comparing the non-diﬀerentially private and the global-budget version
of correlation, we see that the overhead depends mostly on (cid:96) and not much
on n because the number of slow ﬂoating-point operations is proportional to (cid:96).
For (cid:96) = 1000 the running time is about 8000 ms larger than for (cid:96) = 100. This
extra overhead is used mostly (7000 ms) for ﬂoating-point operations (square
root, division, etc.).
function
num. rows non-diﬀ. private
budgets:
global in-place provenance
6096
10672
1124
1241
1587
2257
3452
7234
13871
1475
1753
2531
3873
6483
15118
28995
2826
3157
4092
5525
8548
17895
33530
25961
41363
72436
1708
2131
3502
5312
9198
24175
34583
6598
11051
7663
12693
26833
54767
112300
6548
11786
count
average
correlation ((cid:96) = 100)
correlation ((cid:96) = 1000)
median ((cid:96) = 100)
10000
20000
50000
100000
200000
500000
1000000
10000
20000
50000
100000
200000
500000
1000000
10000
20000
50000
100000
200000
500000