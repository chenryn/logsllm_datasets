h
t
t
d
d
w
w
w
d
d
d
n
n
n
a
a
a
B
B
B
B
b 
0
T
1 
T
2 
Time 
T
	
T
11 
T
12 
T
21 
T
Time 
T
31 
22 
T
32 
T
	



	
Figure 2: TIVC models contain time-varying bandwidth speci-
Ô¨Åcations B(t).
demand. These two applications fall into the more general category
of a sequence of trafÔ¨Åc bursts of varying durations and of varying
intensities.
3. THE TIVC ABSTRACTION
Our key observations in the previous section clearly indicate the
need for a new networking abstraction that can express the time-
varying nature of application networking requirement. To this
end, we propose a novel network abstraction called temporally-
interleaved virtual cluster (TIVC) that captures the temporal varia-
tions in the network behavior of cloud applications.
The TIVC abstraction (shown in Figure 2) consists of a virtual
cluster of N nodes connected to a switch, via links of bandwidth
B, similar to the VC abstraction proposed in [11]. However, the
key difference is that the bandwidth for each link is a time-varying
function B(t) instead of a constant value in prior work. This al-
lows capturing the actual networking requirement of applications
much more precisely, which enables the cloud provider to achieve
better utilization of datacenter resources, and ultimately improves
the provider revenue and reduces the tenant cost without sacriÔ¨Åcing
the job performance, as we will show in our experiments.
3.1 Different TIVC Models
Given the networking requirement proÔ¨Åle of an application,
e.g., shown in Figure 1, one can potentially derive some compli-
cated function (e.g., high-order polynomials) to precisely model the
changing requirement over time. However, such smooth functions
signiÔ¨Åcantly complicate the process of allocating a TIVC in the
physical datacenter network, as well as provisioning of the contin-
uously changing bandwidth requirement in the physical network.
To strike a balance between modeling precision and implementa-
tion difÔ¨Åculties1, we choose to model the networking requirement
as simple pulse functions in this paper. We leave exploring other
tradeoffs in the spectrum as future work.
Since TIVC is a generalization of the VC abstraction, we call
the model with a Ô¨Åxed bandwidth B(t) = B as Type 0 as shown
in Figure 2. To capture the several general time-varying patterns
observed in our proÔ¨Åling study, we propose the following model
functions, also shown in Figure 2.
Type 1: Single peak. A Type 1 model captures the networking de-
mand of applications that only generate network trafÔ¨Åc in a certain
interval, and has a format of , where P =(T1, T2,
B). The bandwidth function is given as
B(t) =  Bb
: t ‚àà [0, T1] or [T2, T ]
B : t ‚àà [T1, T2]
1The Ô¨Åxed bandwidth speciÔ¨Åcation in [20, 11] can be viewed as
going to one extreme in this tradeoff, ease of implementation.
2011Gbps 
500Mbps 
Bandwidth 
500 
J1 
J2 
J3 
Time 
0     5    10   15   20   25  30 
Virtual Cluster Model 
Bandwidth 
500 
J1 
J2  J3  J4  J5 
Time 
0     5    10   15   20   25  30 
TIVC Model 
Figure 3: Simple tree with full-bisection bandwidth.
For example, the Sort application in Figure 1(a) would request for
a Type 1 TIVC with .
Type 2: Fixed-width peaks. The Type 2 TIVC model captures the
networking requirement of applications that have repeated trafÔ¨Åc
peaks. A request of this type has the format 2, where Pi = (Ti1, Ti2, B). The bandwidth function is
B(t) =  B : t ‚àà [Ti1, Ti2], i ‚àà [1, K]
: otherwise
Bb
The request speciÔ¨Åes K repeated peaks of bandwidth B and the
same width to be provisioned, i.e., the widths of the peaks (Ti2 ‚àí
Ti1) are the same, and the base bandwidth Bb during the rest of
time. For example, the Word Count application in Figure 1(b)
would request for a Type 2 TIVC with .
Type 3: Varying-width peaks. The Type 3 TIVC model is more
general than previous two types and captures the networking re-
quirement of applications that have varying-width trafÔ¨Åc peaks.
The request format and the bandwidth function are the same as
with Type 2, except the durations of the peaks (Ti2 ‚àí Ti1) can
differ. For example, Hive Join in Figure 1(c) would request for a
Type 3 TIVC with .
Type 4: Varying height and width peaks. The Type 4 TIVC
model reÔ¨Ånes the Type 3 model to allow varying heights for dif-
ferent peaks. The format is the same as in Type 3, except Pi =
(Ti1, Ti2, Bi), which speciÔ¨Åes a bandwidth cap of Bi is requested
from time Ti1 to Ti2.
The four models based on pulse functions are of increasing gen-
erality. While we do not claim they are universal, we Ô¨Ånd they
capture well the trafÔ¨Åc patterns of the applications we have stud-
ied (which are also widely used in previous datacenter networking
studies, e.g., [16, 36, 24, 28]).
3.2 Implication on Job Scheduling
By more precisely capturing the networking requirement of ap-
plications, TIVC enables the cloud provider to schedule more jobs
to run concurrently not only in over-subscribed networks, but also
in networks with full-bisection bandwidth, such as fat-trees [7, 19].
Consider the simple tree datacenter network with full-bisection
bandwidth shown in Figure 3. Consider a sequence of Sort-like
jobs, each requesting 4 VMs and access bandwidth of 500Mbps
during the Ô¨Årst half of their 10-second execution. Under the VC
model which reserves the constant 500Mbps bandwidth through-
out the job execution, only one job can be scheduled to run every
10 seconds. The four VMs have to be allocated on the four servers
since each server has an access bandwidth of 500Mbps. In con-
trast, under TIVC, after the Ô¨Årst job has run for 5 seconds and thus
Ô¨Ånished the networking phase, the second job can be scheduled to
2We enumerate the peaks in all types for consistency.
run on the other VM of each of the four servers. This results in
doubling the resource utilization and hence the job throughput of
the whole system.
4. TIVC MODEL GENERATION
In this section, we study the key challenge in using TIVC ab-
stractions in practice: How to automatically generate the TIVC
model for a given cloud application?
Since the quantitative
program behavior, e.g., networking requirement, is typically de-
pendent on the MapReduce framework conÔ¨Ågurations at runtime
(e.g., [22]) and potentially on input parameters, in this paper, we
propose a ‚Äúblack-box‚Äù approach to modeling the trafÔ¨Åc require-
ment of a cloud application. The general idea is to collect the trafÔ¨Åc
trace of the application during proÔ¨Åling runs,3 and use it in model
generation. We discuss the generality of our approach in ¬ß4.5.
Realizing the above proÔ¨Åling-based approach faces two immedi-
ate challenges. First, there exists a tradeoff between the bandwidth
cap and the execution time, since tightening the bandwidth con-
straint elongates the networking component of a job and affects the
completion time. The question then is what bandwidth cap should
be used during the proÔ¨Åling run? Second, given a trafÔ¨Åc proÔ¨Åle col-
lected from the proÔ¨Åling run, how to automatically derive the most
suitable TIVC model.
4.1 Impact of Bandwidth Capping
Under the TIVC abstraction, the cloud provider charges for both
the VMs and the network usage4. This raises the question of how to
balance the VM cost and the networking cost, e.g., in trying to min-
imize the total cost of a tenant‚Äôs job. Intuitively, a job can request
for a lower bandwidth limit which lowers the networking cost per
unit time and potentially the total networking cost, which however
may lead to longer networking time and hence longer job execu-
tion time, increasing the total VM cost. To our knowledge, this
important question has not been studied before, even in immedi-
ately relevant prior works such as Oktopus [11].
In the follow-
ing, we conduct experiments to characterize the Ô¨Årst-order impact
that bandwidth capping has on the cloud application execution time
from which we draw implications to TIVC model generation.
We repeat the proÔ¨Åling experiments in ¬ß2.2 while gradually re-
ducing the access bandwidth limit per VM. Figure 4 shows the
measured application execution times. We make two observations.
First, for each application, until the bandwidth cap is reduced to a
certain threshold (e.g., 300 Mbps for Hive Join), there is virtually
no impact on the application execution time and network through-
put. Second, once the cap crosses below the threshold, denoted as
the no-elongation threshold bandwidth, the execution time is elon-
gated monotonically. We empirically conÔ¨Årmed that the elongation
of the execution time is due to the slowdown of the application net-
working activities, as we measured the execution time slowdown
to be equal to the elongation on the network active periods, i.e., the
width of the pulses captured in the TIVC model shown in ¬ß4.2.
The main reason for the above behavior is that cloud applications
have mixed communication and computation even in network-
intensive phases. For example, we calculated the average through-
put per VM of Hive Join during each of the four high pulses in
Figure 1(c) which had a bandwidth cap of 800 Mbps to be only 160
Mbps. This indicates that it may be unnecessary to set the band-
width cap to be much higher than the average application trafÔ¨Åc
generation rate. However, we found that capping the bandwidth to
3We note proÔ¨Åling trafÔ¨Åc demand is required even in Oktopus [11]
to meaningful decide on the constant bandwidth parameter B.
4Designing bandwidth charging models is currently being stud-
ied [30, 10] and is beyond the scope of this paper.
202)
c
e
s
(
i
e
m
T
n
o
i
t
l
e
p
m
o
C
 1000
 800
 600
 400
 200
 0
Sort
Word Count
Hive Join
Hive Aggre.
Bcap
BW Profile
Base Bb1
Base Bb2
A1'
A2'
i
t
h
d
w
d
n
a
B
Bb2
1
2
3
4
6
8
571
2
3
5
1
2
3
4
6
8
1
2
3
4
6
8
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
Figure 5: Model Ô¨Åtting by varying the base bandwidth.
A1
A2
Bb1
T1
t1 t1'
t2'
t2
T2
Time
BW Cap (Mbps)
Figure 4: Application execution time under different band-
width caps.
be exactly the same as the application data generation rate is too
rigid, i.e., it actually slows down the application network-intensive
phases. This is shown in Figure 4 where Hive Join under 200 Mbps
runs 25 seconds longer than under 300 Mbps. The reason is mainly
due to applications‚Äô bursty networking behavior. SpeciÔ¨Åcally, we
observe that applications tend to interleave computation and data
transfer phases, i.e., it generates trafÔ¨Åc in bursts during data trans-
fer phases, at a rate higher than the average data generation rate.
Further, the computation has certain dependence on the data trans-
fer. For example, in MapReduce jobs, the data shufÔ¨Çing phase is
interleaved with map tasks which generate processed data. If the
bursty data transfer is slowed down due to bandwidth capping, it
pushes back the subsequent computation.
The above threshold behavior suggests that the tenant should al-
ways pick the bandwidth cap to be the same or lower than the no-
elongation threshold bandwidth, as higher capping wastes network-
ing cost without making the application run faster. However, below
the threshold, automatically deriving the bandwidth cap that opti-
mizes the total cost requires a precise modeling of the relationship
between the bandwidth cap and application execution time, which
is beyond the scope of this paper. Instead, in ¬ß5.1, we implement a
proÔ¨Åling-based approach to help the user to pick the bandwidth cap
that optimizes the total cost while meeting the performance goal.
The above study further helps us to draw an important impli-
cation in terms of automatic TIVC model generation. If the user
picks a bandwidth cap Bcap equal to or below the threshold value
in the proÔ¨Åling run to be used for model generation, in the gener-
ated TIVC model, the ceiling of the pulses should be conservatively
set to be the same as the bandwidth cap, as using a lower ceiling
will elongate the application execution time relative to the proÔ¨Åle
run. However this rule can be loosened for certain pulses which
will be become clear in ¬ß4.3.
4.2 Model Generation
R T
The model generation algorithm takes as input the trafÔ¨Åc proÔ¨Åle
of an application proÔ¨Åling run under bandwidth cap Bcap, and de-
rives the TIVC parameters that achieves the highest efÔ¨Åciency in the
following two steps. Here, we deÔ¨Åne efÔ¨Åciency of a TIVC model
as the ratio of the total application trafÔ¨Åc volume over the total traf-
Ô¨Åc volume under the bandwidth reserved by the TIVC model, i.e.,
0 B(t)dt.
Since TIVC models use pulse functions, i.e., square curves, the
main idea of automatic model generation is to derive square curves
to cap the continuous bandwidth demand curve from the proÔ¨Åling
run. There are potentially many ways of generating such bounding
square curves, and different bounding curves may have different
efÔ¨Åciencies. We note high efÔ¨Åciency Ô¨Åttings (and hence low total
bandwidth volume) may not necessarily translate into Ô¨Åtting more
jobs in the datacenter, as how well the TIVC models of competing
jobs complement each other also plays an important role. However,
we envision it is more practical that TIVC models are generated
ofÔ¨Çine, i.e., oblivious to the competing jobs at (future) scheduling
time, and hence set maximizing the efÔ¨Åciency as the main objective
in model generation.
We generate the most efÔ¨Åcient Type 4 model in two steps. First
we show how to generate the bounding square curves that maximize
the efÔ¨Åciency under Type 3 TIVC (which generalizes Type 1 and 2).
We then show the conditions when a Type 3 model is reÔ¨Åned into
Type 4 in ¬ß4.3. Recall a Type 3 TIVC model speciÔ¨Åes a base band-
width Bb and a list of Ô¨Åxed-height pulses Pi = (Ti1, Ti2, B). For a
Ô¨Åxed Bb value, to meet the application‚Äôs bandwidth need, all peri-
ods in which the bandwidth usage is observed to be greater than Bb
is conservatively rounded up to Bcap, for reasons discussed in ¬ß4.1.
Thus Bb effectively controls the relative amount of bandwidth vol-
ume under the base bandwidth periods and under the pulse periods.
In general, the higher the Bb value, the more area under the base
bandwidth, and the less area under the (narrower) pulses, as shown
in Figure 5, which shows two tentative square curve Ô¨Åttings with
base bandwidth Bb1 and Bb2, respectively. Now given an applica-
tion bandwidth proÔ¨Åle, the Type 3 TIVC parameters that maximize
the model efÔ¨Åciency can be found by searching through different
values of Bb.